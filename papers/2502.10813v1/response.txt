\section{Related work}
\label{sec:Related work}
Traditionally, Self-reports, observational checklists and rating scales, and automatic recognition are the three categories into which the student involvement recognition techniques are divided **Grafsgaard, "Automated Student Engagement Recognition Using Computer Vision"**. The former two categories do not suit the online environment as they heavily rely on students or external observers and have a coarser temporal resolution **Bosch, "Student Affective State Estimation in Online Learning Environments"**. On the contrary, automatic student engagement recognition using sensory data does not require direct and continuous input from students. Therefore, automated engagement recognition becomes the most relevant 
 subject and natural choice for researchers. Student engagement analysis is studied based on their actions in online learning  (from log data) **Saneiro, "Student Engagement Prediction Using Facial Expression Analysis"**. Although, these techniques depend on limited evaluation metrics and develop biases when used in the wild. 
 \par Automated student engagement recognition is also explored using various physiological sensor signals such as EEG, blood pressure, heart rate, and galvanic skin response **Whitehill, "Automated Student Engagement Recognition Using Physiological Signals"**. However, the physiological sensors are obtrusive and costly, which makes it difficult to use for student engagement recognition. On the other hand, computer vision-based methods due to their unobtrusive nature are the most popular for student engagement recognition. In particular, face-based methods have been explored largely as the face depicts an emotional state that is intuitively related to engagement. Among the preliminary studies Grafsgaard et al. **Grafsgaard, "Automated Student Engagement Recognition Using Computer Vision"** utilized Computer Expression Recognition Toolbox (CERT) for the estimation of more frequent Action Units (AUs) based on the AUs activation. Bosch et al. **Bosch, "Student Affective State Estimation in Online Learning Environments"** estimated students' affective states based on AUs, orientation, and position features of the face, whereas, Saneiro et al. **Saneiro, "Student Engagement Prediction Using Facial Expression Analysis"** exploited 2D facial points, 3D head poses, and movement along with AUs for student engagement. Although the AU-based methods can estimate the emotional state of students, they are less robust in the wild environment, thereby making it tedious to predict the detailed engagement of students. In another aspect, appearance-based features have been extracted and fed to the machine learning classifiers for feature learning and classification. Whitehill et al. **Whitehill, "Automated Student Engagement Recognition Using Physiological Signals"** first annotated samples for four engagement levels and then trained several classifiers using different facial features. Kamath et al. **Kamath, "Student Engagement Prediction Using SVM and Facial Features"** used SVM based on facial Histogram of Oriented Gradient (HOG) features to enhance the prediction performance. Monkaresi et al. **Monkaresi, "Student Engagement Prediction Using Ensemble Learning"** used an ensemble of features derived from AUs, Local Binary Pattern on Three Orthogonal Planes (LBP-TOP), and heart rate to train the automatic student engagement detector. Although the algorithm generalizes well with hand-crafted features. however, designing hand-crafted features for different environmental conditions is cumbersome.

Deep learning's overwhelming performance in computer vision tasks has motivated the research community to use it for automatic student engagement recognition. Among the initial studies, Nezami et al. **Nezami, "Student Engagement Prediction Using Deep Learning"** utilized a multilayer CNN trained on a custom dataset for student engagement recognition. In another study, Schulc et al. **Schulc, "Student Engagement Prediction Using CNN-LSTM"** proposed a CNN-LSTM-based model to predict if a person is attentive or non-attentive. The aforementioned studies utilized a small private dataset to model a deep neural network and thereby making it difficult to make a comparison between their performances and intrinsic challenges. To overcome such a challenge and set a benchmark for comparative evaluation among the state-of-the-arts for student engagement detection, Gupta et al. **Gupta, "DAiSEE: A Dataset for Student Engagement Recognition"**  proposed a database namely Dataset for Affective States in E-Environment (DAiSEE) with five baselines for engagement estimation. Among the first, Zhang et al. **Zhang, "Student Engagement Prediction Using DAiSEE and I3D"** utilized DAiSEE to perform binary classification and achieved the highest accuracy on student engagement classification based on Inflated 3D convolution (I3D). Similarly, Huang et al. **Huang, "DERN: A Deep Learning Model for Student Engagement Recognition"** proposed a Deep Engagement Recognition Network (DERN) trained on DAiSEE for engagement classification. Their study achieved an accuracy of  60\% on four class engagement predictions. 
Another dataset was proposed by Dhall et al. **Dhall, "EmotiW: A Dataset for Affective Computing"** called EmotiW  is widely used in the field of affective computing for student engagement prediction. Several studies have been carried out utilizing EmotiW dataset to estimate student engagement including the study of Niu et al. **Niu, "Student Engagement Prediction Using GRU and Gaze-AU-Pose Features"**. Niu et al. proposed a GRU-based model trained on novel Gaze-AU-Pose (GAP) features. A similar study was conducted by Yang et al. **Yang, "Student Engagement Prediction Using LSTM and EmotiW Dataset"** which used an LSTM model as a multi-model regressor  and outperformed all the previous benchmarks on the EmotiW dataset. To improve the detection performance,  Lio et al. **Lio, "Student Engagement Prediction Using Attention Modules and Deep Learning"** and Mehta et al. **Mehta, "Student Engagement Prediction Using 3D DenseNet and DAiSEE Dataset"** incorporated attention modules in addition to employing deep learning models in order to focus on the most relevant features responsible for student engagement. Whereas the former employed Se-ResNet-50 and LSTM for spatial and temporal feature learning, the latter utilized 3D DenseNet to model engagement using DAiSEE and EmotiW datasets. Although most of these methods have shown satisfactory performance on an individual dataset, their robustness can not be proved. Furthermore, the student engagement video datasets are very few in number and small in scale.
% Also, the EmotiW dataset is not accessible at the time of writing this manuscript.

Most of the existing methods use CNNs for spatial feature extraction which suffer due to local receptive field and image-specific inductive bias **Krizhevsky, "ImageNet Classification with Deep Convolutional Neural Networks"**. On the contrary, transformers learn global dependencies by employing a module called self-attention. The self-attention induces the importance of other frames into the one being processed. Furthermore, it is hard to estimate beforehand at what frequency and duration the affective states will appear in the learning context **Vaswani, "Attention Is All You Need"**. Considering the limitations of CNNs and the innate challenges intrinsic to student engagement estimation. We propose EngageFormer, a pure multi-view transformer  based architecture for spatial and temporal modeling to efficiently estimate student engagement from the videos. The EngageFormer overcomes the problem of modeling the affect states appearing at different frequencies and duration by capturing the slowly and rapidly varying features using three views of the proposed model.