\bibitem{mandia2022vision}Mandia, S., Singh, K. \& Mitharwal, R. Vision Transformer for Automatic Student Engagement Estimation. {\em 2022 IEEE 5th International Conference On Image Processing Applications And Systems (IPAS)}. pp. 1-6 (2022)
\bibitem{mahmood2021instructional}Mahmood, S. Instructional strategies for online teaching in COVID-19 pandemic. {\em Human Behavior And Emerging Technologies}. \textbf{3}, 199-203 (2021)
\bibitem{dias2020deeplms}Dias, S., Hadjileontiadou, S., Diniz, J. \& Hadjileontiadis, L. DeepLMS: a deep learning predictive model for supporting online learning in the Covid-19 era. {\em Scientific Reports}. \textbf{10}, 1-17 (2020)
\bibitem{dhawan2020online}Dhawan, S. Online learning: A panacea in the time of COVID-19 crisis. {\em Journal Of Educational Technology Systems}. \textbf{49}, 5-22 (2020)
\bibitem{adnan2020online}Adnan, M. \& Anwar, K. Online Learning amid the COVID-19 Pandemic: Students' Perspectives.. {\em Online Submission}. \textbf{2}, 45-51 (2020)
\bibitem{dewan2019engagement}Dewan, M., Murshed, M. \& Lin, F. Engagement detection in online learning: a review. {\em Smart Learning Environments}. \textbf{6}, 1-20 (2019)
\bibitem{fredricks2004school}Fredricks, J., Blumenfeld, P. \& Paris, A. School engagement: Potential of the concept, state of the evidence. {\em Review Of Educational Research}. \textbf{74}, 59-109 (2004)
\bibitem{jung2018learning}Jung, Y. \& Lee, J. Learning engagement and persistence in massive open online courses (MOOCS). {\em Computers & Education}. \textbf{122} pp. 9-22 (2018)
\bibitem{saurav2021emnet}Saurav, S., Saini, R. \& Singh, S. EmNet: a deep integrated convolutional neural network for facial emotion recognition in the wild. {\em Applied Intelligence}. \textbf{51}, 5543-5570 (2021)
\bibitem{calvo2010affect}Calvo, R. \& D'Mello, S. Affect detection: An interdisciplinary review of models, methods, and their applications. {\em IEEE Transactions On Affective Computing}. \textbf{1}, 18-37 (2010)
\bibitem{grafsgaard2013automatically}Grafsgaard, J., Wiggins, J., Boyer, K., Wiebe, E. \& Lester, J. Automatically recognizing facial expression: Predicting engagement and frustration. {\em Educational Data Mining 2013}. (2013)
\bibitem{whitehill2014faces}Whitehill, J., Serpell, Z., Lin, Y., Foster, A. \& Movellan, J. The faces of engagement: Automatic recognition of student engagementfrom facial expressions. {\em IEEE Transactions On Affective Computing}. \textbf{5}, 86-98 (2014)
\bibitem{liao2021deep}Liao, J., Liang, Y. \& Pan, J. Deep facial spatiotemporal network for engagement prediction in online learning. {\em Applied Intelligence}. \textbf{51}, 6609-6621 (2021)
\bibitem{gupta2016daisee}Gupta, A., D'Cunha, A., Awasthi, K. \& Balasubramanian, V. Daisee: Towards user engagement recognition in the wild. {\em ArXiv Preprint ArXiv:1609.01885}. (2016)
\bibitem{mehta2022three}Mehta, N., Prasad, S., Saurav, S., Saini, R. \& Singh, S. Three-dimensional DenseNet self-attention neural network for automatic detection of student’s engagement. {\em Applied Intelligence}. pp. 1-21 (2022)
\bibitem{huang2019fine}Huang, T., Mei, Y., Zhang, H., Liu, S. \& Yang, H. Fine-grained engagement recognition in online learning environment. {\em 2019 IEEE 9th International Conference On Electronics Information And Emergency Communication (ICEIEC)}. pp. 338-341 (2019)
\bibitem{abedi2021improving}Abedi, A. \& Khan, S. Improving state-of-the-art in Detecting Student Engagement with Resnet and TCN Hybrid Network. {\em 2021 18th Conference On Robots And Vision (CRV)}. pp. 151-157 (2021)
\bibitem{dosovitskiy2020image}Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. \& Others An image is worth 16x16 words: Transformers for image recognition at scale. {\em ArXiv Preprint ArXiv:2010.11929}. (2020)
\bibitem{arnab2021vivit}Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lučić, M. \& Schmid, C. Vivit: A video vision transformer. {\em Proceedings Of The IEEE/CVF International Conference On Computer Vision}. pp. 6836-6846 (2021)
\bibitem{yan2022multiview}Yan, S., Xiong, X., Arnab, A., Lu, Z., Zhang, M., Sun, C. \& Schmid, C. Multiview transformers for video recognition. {\em Proceedings Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition}. pp. 3333-3343 (2022)
\bibitem{devlin2018bert}Devlin, J., Chang, M., Lee, K. \& Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. {\em ArXiv Preprint ArXiv:1810.04805}. (2018)
\bibitem{d2006predicting}D'Mello, S., Craig, S., Sullins, J. \& Graesser, A. Predicting affective states expressed through an emote-aloud procedure from AutoTutor's mixed-initiative dialogue. {\em International Journal Of Artificial Intelligence In Education}. \textbf{16}, 3-28 (2006)
\bibitem{o2010development}O'Brien, H. \& Toms, E. The development and evaluation of a survey to measure user engagement. {\em Journal Of The American Society For Information Science And Technology}. \textbf{61}, 50-69 (2010)
\bibitem{5518758}Cocea, M. \& Weibelzahl, S. Disengagement Detection in Online Learning: Validation Studies and Perspectives. {\em IEEE Transactions On Learning Technologies}. \textbf{4}, 114-124 (2011)
\bibitem{aluja2019measuring}Aluja-Banet, T., Sancho, M. \& Vukic, I. Measuring motivation from the virtual learning environment in secondary education. {\em Journal Of Computational Science}. \textbf{36} pp. 100629 (2019)
\bibitem{monkaresi2016automated}Monkaresi, H., Bosch, N., Calvo, R. \& D'Mello, S. Automated detection of engagement using video-based estimation of facial expressions and heart rate. {\em IEEE Transactions On Affective Computing}. \textbf{8}, 15-28 (2016)
\bibitem{fairclough2006prediction}Fairclough, S. \& Venables, L. Prediction of subjective states from psychophysiology: A multivariate approach. {\em Biological Psychology}. \textbf{71}, 100-110 (2006)
\bibitem{khedher2019tracking}Khedher, A., Jraidi, I., Frasson, C. \& Others Tracking students’ mental engagement using EEG signals during an interaction with a virtual learning environment. {\em Journal Of Intelligent Learning Systems And Applications}. \textbf{11}, 1 (2019)
\bibitem{6681424}Grafsgaard, J., Wiggins, J., Boyer, K., Wiebe, E. \& Lester, J. Automatically Recognizing Facial Indicators of Frustration: A Learning-centric Analysis. {\em 2013 Humaine Association Conference On Affective Computing And Intelligent Interaction}. pp. 159-165 (2013)
\bibitem{bosch2015automatic}Bosch, N., D'Mello, S., Baker, R., Ocumpaugh, J., Shute, V., Ventura, M., Wang, L. \& Zhao, W. Automatic detection of learning-centered affective states in the wild. {\em Proceedings Of The 20th International Conference On Intelligent User Interfaces}. pp. 379-388 (2015)
\bibitem{saneiro2014towards}Saneiro, M., Santos, O., Salmeron-Majadas, S. \& Boticario, J. Towards emotion detection in educational scenarios from facial expressions and body movements through multimodal approaches. {\em The Scientific World Journal}. \textbf{2014} (2014)
\bibitem{kamath2016crowdsourced}Kamath, A., Biswas, A. \& Balasubramanian, V. A crowdsourced approach to student engagement recognition in e-learning environments. {\em 2016 IEEE Winter Conference On Applications Of Computer Vision (WACV)}. pp. 1-9 (2016)
\bibitem{he2020deformable}He, M., Zhang, J., Shan, S., Kan, M. \& Chen, X. Deformable face net for pose invariant face recognition. {\em Pattern Recognition}. \textbf{100} pp. 107113 (2020)
\bibitem{mohamad2020automatic}Mohamad Nezami, O., Dras, M., Hamey, L., Richards, D., Wan, S. \& Paris, C. Automatic recognition of student engagement using deep learning and facial expression. {\em Joint European Conference On Machine Learning And Knowledge Discovery In Databases}. pp. 273-289 (2020)
\bibitem{schulc2019automatic}Schulc, A., Cohn, J., Shen, J. \& Pantic, M. Automatic measurement of visual attention to video content using deep learning. {\em 2019 16th International Conference On Machine Vision Applications (MVA)}. pp. 1-6 (2019)
\bibitem{zhang2019novel}Zhang, H., Xiao, X., Huang, T., Liu, S., Xia, Y. \& Li, J. An novel end-to-end network for automatic student engagement recognition. {\em 2019 IEEE 9th International Conference On Electronics Information And Emergency Communication (ICEIEC)}. pp. 342-345 (2019)
\bibitem{dhall2018emotiw}Dhall, A., Kaur, A., Goecke, R. \& Gedeon, T. Emotiw 2018: Audio-video, student engagement and group-level affect prediction. {\em Proceedings Of The 20th ACM International Conference On Multimodal Interaction}. pp. 653-656 (2018)
\bibitem{dhall2019emotiw}Dhall, A. Emotiw 2019: Automatic emotion, engagement and cohesion prediction tasks. {\em 2019 International Conference On Multimodal Interaction}. pp. 546-550 (2019)
\bibitem{niu2018automatic}Niu, X., Han, H., Zeng, J., Sun, X., Shan, S., Huang, Y., Yang, S. \& Chen, X. Automatic engagement prediction with GAP feature. {\em Proceedings Of The 20th ACM International Conference On Multimodal Interaction}. pp. 599-603 (2018)
\bibitem{yang2018deep}Yang, J., Wang, K., Peng, X. \& Qiao, Y. Deep recurrent multi-instance learning with spatio-temporal features for engagement intensity prediction. {\em Proceedings Of The 20th ACM International Conference On Multimodal Interaction}. pp. 594-598 (2018)
\bibitem{hassani2021escaping}Hassani, A., Walton, S., Shah, N., Abuduweili, A., Li, J. \& Shi, H. Escaping the big data paradigm with compact transformers. {\em ArXiv Preprint ArXiv:2104.05704}. (2021)
\bibitem{vaswani2017attention}Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, Ł. \& Polosukhin, I. Attention is all you need. {\em Advances In Neural Information Processing Systems}. \textbf{30} (2017)
\bibitem{hendrycks2016gaussian}Hendrycks, D. \& Gimpel, K. Gaussian error linear units (gelus). {\em ArXiv Preprint ArXiv:1606.08415}. (2016)
\bibitem{ba2016layer}Ba, J., Kiros, J. \& Hinton, G. Layer normalization. {\em ArXiv Preprint ArXiv:1607.06450}. (2016)
\bibitem{loshchilov2017decoupled}Loshchilov, I. \& Hutter, F. Decoupled weight decay regularization. {\em ArXiv Preprint ArXiv:1711.05101}. (2017)
\bibitem{huang2016deep}Huang, G., Sun, Y., Liu, Z., Sedra, D. \& Weinberger, K. Deep networks with stochastic depth. {\em European Conference On Computer Vision}. pp. 646-661 (2016)
\bibitem{szegedy2016rethinking}Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. \& Wojna, Z. Rethinking the inception architecture for computer vision. {\em Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition}. pp. 2818-2826 (2016)
\bibitem{zhalehpour2016baum}Zhalehpour, S., Onder, O., Akhtar, Z. \& Erdem, C. BAUM-1: A spontaneous audio-visual face database of affective and mental states. {\em IEEE Transactions On Affective Computing}. \textbf{8}, 300-313 (2016)
\bibitem{ghoddoosian2019realistic}Ghoddoosian, R., Galib, M. \& Athitsos, V. A realistic dataset and baseline temporal model for early drowsiness detection. {\em Proceedings Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition Workshops}. pp. 0-0 (2019)
\bibitem{abtahi2014yawdd}Abtahi, S., Omidyeganeh, M., Shirmohammadi, S. \& Hariri, B. YawDD: A yawning detection dataset. {\em Proceedings Of The 5th ACM Multimedia Systems Conference}. pp. 24-28 (2014)
\bibitem{zhang2016joint}Zhang, K., Zhang, Z., Li, Z. \& Qiao, Y. Joint face detection and alignment using multitask cascaded convolutional networks. {\em IEEE Signal Processing Letters}. \textbf{23}, 1499-1503 (2016)
\bibitem{ekman1992argument}Ekman, P. An argument for basic emotions. {\em Cognition & Emotion}. \textbf{6}, 169-200 (1992)
\bibitem{pekrun2000social}Pekrun, R. A social-cognitive, control-value theory of achievement emotions.. (Elsevier Science,2000)
\bibitem{zhang2017learning}Zhang, S., Zhang, S., Huang, T., Gao, W. \& Tian, Q. Learning affective features with a hybrid deep model for audio–visual emotion recognition. {\em IEEE Transactions On Circuits And Systems For Video Technology}. \textbf{28}, 3030-3043 (2017)
\bibitem{ma2019audio}Ma, Y., Hao, Y., Chen, M., Chen, J., Lu, P. \& Košir, A. Audio-visual emotion fusion (AVEF): A deep efficient weighted approach. {\em Information Fusion}. \textbf{46} pp. 184-192 (2019)
\bibitem{pan2021multimodal}Pan, B., Hirota, K., Jia, Z., Zhao, L., Jin, X. \& Dai, Y. Multimodal emotion recognition based on feature selection and extreme learning machine in video clips. {\em Journal Of Ambient Intelligence And Humanized Computing}. pp. 1-15 (2021)
\bibitem{omidyeganeh2016yawning}Omidyeganeh, M., Shirmohammadi, S., Abtahi, S., Khurshid, A., Farhan, M., Scharcanski, J., Hariri, B., Laroche, D. \& Martel, L. Yawning detection using embedded smart cameras. {\em IEEE Transactions On Instrumentation And Measurement}. \textbf{65}, 570-582 (2016)
\bibitem{zhang2017driver}Zhang, W. \& Su, J. Driver yawning detection based on long short term memory networks. {\em 2017 IEEE Symposium Series On Computational Intelligence (SSCI)}. pp. 1-5 (2017)
\bibitem{zhang2015driver}Zhang, W., Murphey, Y., Wang, T. \& Xu, Q. Driver yawning detection based on deep convolutional neural learning and robust nose tracking. {\em 2015 International Joint Conference On Neural Networks (IJCNN)}. pp. 1-8 (2015)
\bibitem{bai2021two}Bai, J., Yu, W., Xiao, Z., Havyarimana, V., Regan, A., Jiang, H. \& Jiao, L. Two-stream spatial-temporal graph convolutional networks for driver drowsiness detection. {\em IEEE Transactions On Cybernetics}. (2021)
\bibitem{deng2019real}Deng, W. \& Wu, R. Real-time driver-drowsiness detection system using facial features. {\em Ieee Access}. \textbf{7} pp. 118727-118738 (2019)
\bibitem{ji2019fatigue}Ji, Y., Wang, S., Zhao, Y., Wei, J. \& Lu, Y. Fatigue state detection based on multi-index fusion and state recognition network. {\em IEEE Access}. \textbf{7} pp. 64136-64147 (2019)
\bibitem{ye2021driver}Ye, M., Zhang, W., Cao, P. \& Liu, K. Driver fatigue detection based on residual channel attention network and head pose estimation. {\em Applied Sciences}. \textbf{11}, 9195 (2021)
\bibitem{xiang2022driving}Xiang, W., Wu, X., Li, C., Zhang, W. \& Li, F. Driving Fatigue Detection Based on the Combination of Multi-Branch 3D-CNN and Attention Mechanism. {\em Applied Sciences}. \textbf{12}, 4689 (2022)
\bibitem{bian2019spontaneous}Bian, C., Zhang, Y., Yang, F., Bi, W. \& Lu, W. Spontaneous facial expression database for academic emotion inference in online learning. {\em IET Computer Vision}. \textbf{13}, 329-337 (2019)
\bibitem{ashwin2019unobtrusive}Ashwin, T. \& Guddeti, R. Unobtrusive behavioral analysis of students in classroom environment using non-verbal cues. {\em IEEE Access}. \textbf{7} pp. 150693-150709 (2019)
\bibitem{mandia2023recognition}Mandia, S., Singh, K. \& Mitharwal, R. Recognition of student engagement in classroom from affective states. {\em International Journal Of Multimedia Information Retrieval}. \textbf{12}, 18 (2023)
\bibitem{mandia2023automatic}Mandia, S., Mitharwal, R. \& Singh, K. Automatic student engagement measurement using machine learning techniques: A literature study of data and methods. {\em Multimedia Tools And Applications}. pp. 1-32 (2023)
