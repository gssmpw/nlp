@inproceedings{hordri2016deep,
  title={Deep learning and its applications: A review},
  author={Hordri, Nur Farhana and Yuhaniz, Siti Sophiayati and Shamsuddin, Siti Mariyam},
  booktitle={Conference on Postgraduate Annual Research on Informatics Seminar},
  pages={1--5},
  year={2016}
}
@book{bengio2017deep,
  title={Deep learning},
  author={Bengio, Yoshua and Goodfellow, Ian and Courville, Aaron},
  volume={1},
  year={2017},
  publisher={MIT press Cambridge, MA, USA}
}
@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{poirson2016fast,
  title={Fast single shot detection and pose estimation},
  author={Poirson, Patrick and Ammirato, Phil and Fu, Cheng-Yang and Liu, Wei and Kosecka, Jana and Berg, Alexander C},
  booktitle={2016 Fourth International Conference on 3D Vision (3DV)},
  pages={676--684},
  year={2016},
  organization={IEEE}
}
@article{kamilaris2018deep,
  title={Deep learning in agriculture: A survey},
  author={Kamilaris, Andreas and Prenafeta-Bold{\'u}, Francesc X},
  journal={Computers and electronics in agriculture},
  volume={147},
  pages={70--90},
  year={2018},
  publisher={Elsevier}
}
@article{gamboa2017deep,
  title={Deep learning for time-series analysis},
  author={Gamboa, John Cristian Borges},
  journal={arXiv preprint arXiv:1701.01887},
  year={2017}
}

@article{goodfellow2020generative,
  title={Generative adversarial networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Communications of the ACM},
  volume={63},
  number={11},
  pages={139--144},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{sarker2021machine,
  title={Machine learning: Algorithms, real-world applications and research directions},
  author={Sarker, Iqbal H},
  journal={SN computer science},
  volume={2},
  number={3},
  pages={160},
  year={2021},
  publisher={Springer}
}

@article{malhotra2023recent,
  title={Recent advances in deep learning models: a systematic literature review},
  author={Malhotra, Ruchika and Singh, Priya},
  journal={Multimedia Tools and Applications},
  volume={82},
  number={29},
  pages={44977--45060},
  year={2023},
  publisher={Springer}
}
@article{hatcher2018survey,
  title={A survey of deep learning: Platforms, applications and emerging research trends},
  author={Hatcher, William Grant and Yu, Wei},
  journal={IEEE access},
  volume={6},
  pages={24411--24432},
  year={2018},
  publisher={IEEE}
}
@article{assi2024unraveling,
  title={Unraveling Code Clone Dynamics in Deep Learning Frameworks},
  author={Assi, Maram and Hassan, Safwat and Zou, Ying},
  journal={arXiv preprint arXiv:2404.17046},
  year={2024}
}
@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}
@article{shin2021survey,
  title={A survey of automatic code generation from natural language},
  author={Shin, Jiho and Nam, Jaechang},
  journal={Journal of Information Processing Systems},
  volume={17},
  number={3},
  pages={537--555},
  year={2021}
}

@article{lyu2024top,
  title={Top Pass: Improve Code Generation by Pass@ k-Maximized Code Ranking},
  author={Lyu, Zhi-Cun and Li, Xin-Ye and Xie, Zheng and Li, Ming},
  journal={arXiv preprint arXiv:2408.05715},
  year={2024}
}

@article{wei2023magicoder,
  title={Magicoder: Source code is all you need},
  author={Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming},
  journal={arXiv preprint arXiv:2312.02120},
  year={2023}
}

@article{wang2021codet5,
  title={Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2109.00859},
  year={2021}
}

@article{feng2020codebert,
  title={Codebert: A pre-trained model for programming and natural languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},
  journal={arXiv preprint arXiv:2002.08155},
  year={2020}
}

@article{zhong2024ldb,
  title={LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step},
  author={Zhong, Li and Wang, Zilong and Shang, Jingbo},
  journal={arXiv preprint arXiv:2402.16906},
  year={2024}
}

@article{guo2024deepseek,
  title={DeepSeek-Coder: When the Large Language Model Meets Programming--The Rise of Code Intelligence},
  author={Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Y and Li, YK and others},
  journal={arXiv preprint arXiv:2401.14196},
  year={2024}
}


@inproceedings{lai2023ds,
  title={DS-1000: A natural and reliable benchmark for data science code generation},
  author={Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
  booktitle={International Conference on Machine Learning},
  pages={18319--18345},
  year={2023},
  organization={PMLR}
}
@inproceedings{agashe-etal-2019-juice,
    title = "{J}u{IC}e: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation",
    author = "Agashe, Rajas  and
      Iyer, Srinivasan  and
      Zettlemoyer, Luke",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1546",
    doi = "10.18653/v1/D19-1546",
    pages = "5436--5446",
    abstract = "Interactive programming with interleaved code snippet cells and natural language markdown is recently gaining popularity in the form of Jupyter notebooks, which accelerate prototyping and collaboration. To study code generation conditioned on a long context history, we present JuICe, a corpus of 1.5 million examples with a curated test set of 3.7K instances based on online programming assignments. Compared with existing contextual code generation datasets, JuICe provides refined human-curated data, open-domain code, and an order of magnitude more training data. Using JuICe, we train models for two tasks: (1) generation of the API call sequence in a code cell, and (2) full code cell generation, both conditioned on the NL-Code history up to a particular code cell. Experiments using current baseline code generation models show that both context and distant supervision aid in generation, and that the dataset is challenging for current systems.",
}

@inproceedings{hendrycks2measuring,
  title={Measuring Coding Challenge Competence With APPS},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)}
}

@article{austin2021program,
  title={Program Synthesis with Large Language Models},
  author={Jacob Austin and Augustus Odena and Maxwell Nye and Maarten Bosma and Henryk Michalewski and David Dohan and Ellen Jiang and Carrie J. Cai and Michael Terry and Quoc V. Le and Charles Sutton},
  journal={ArXiv},
  year={2021},
  volume={abs/2108.07732},
  url={https://api.semanticscholar.org/CorpusID:237142385}
}
@article{chen2021evaluating,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Pond{\'e} and Jared Kaplan and Harrison Edwards and Yura Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and David W. Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and Igor Babuschkin and Suchir Balaji and Shantanu Jain and Andrew Carr and Jan Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew M. Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.03374},
  url={https://api.semanticscholar.org/CorpusID:235755472}
}
@article{yu2018spider,
  title={Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task},
  author={Yu, Tao and Zhang, Rui and Yang, Kai and Yasunaga, Michihiro and Wang, Dongxu and Li, Zifan and Ma, James and Li, Irene and Yao, Qingning and Roman, Shanelle and others},
  journal={arXiv preprint arXiv:1809.08887},
  year={2018}
}
@inproceedings{du2024evaluating,
  title={Evaluating large language models in class-level code generation},
  author={Du, Xueying and Liu, Mingwei and Wang, Kaixin and Wang, Hanlin and Liu, Junwei and Chen, Yixuan and Feng, Jiayi and Sha, Chaofeng and Peng, Xin and Lou, Yiling},
  booktitle={Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
  pages={1--13},
  year={2024}
}

@article{shin2023good,
  title={The good, the bad, and the missing: Neural code generation for machine learning tasks},
  author={Shin, Jiho and Wei, Moshi and Wang, Junjie and Shi, Lin and Wang, Song},
  journal={ACM Transactions on Software Engineering and Methodology},
  volume={33},
  number={2},
  pages={1--24},
  year={2023},
  publisher={ACM New York, NY}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@article{wang2024survey,
  title={A survey on large language model based autonomous agents},
  author={Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and others},
  journal={Frontiers of Computer Science},
  volume={18},
  number={6},
  pages={186345},
  year={2024},
  publisher={Springer}
}

@inproceedings{arpteg2018software,
  title={Software engineering challenges of deep learning},
  author={Arpteg, Anders and Brinne, Bj{\"o}rn and Crnkovic-Friis, Luka and Bosch, Jan},
  booktitle={2018 44th euromicro conference on software engineering and advanced applications (SEAA)},
  pages={50--59},
  year={2018},
  organization={IEEE}
}

@article{jiang2024challenges,
  title={Challenges and practices of deep learning model reengineering: A case study on computer vision},
  author={Jiang, Wenxin and Banna, Vishnu and Vivek, Naveen and Goel, Abhinav and Synovic, Nicholas and Thiruvathukal, George K and Davis, James C},
  journal={Empirical Software Engineering},
  volume={29},
  number={6},
  pages={1--61},
  year={2024},
  publisher={Springer}
}

@inproceedings{xu2022systematic,
  title={A systematic evaluation of large language models of code},
  author={Xu, Frank F and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent Josua},
  booktitle={Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},
  pages={1--10},
  year={2022}
}
@inproceedings{yu2024codereval,
  title={Codereval: A benchmark of pragmatic code generation with generative pre-trained models},
  author={Yu, Hao and Shen, Bo and Ran, Dezhi and Zhang, Jiaxin and Zhang, Qi and Ma, Yuchi and Liang, Guangtai and Li, Ying and Wang, Qianxiang and Xie, Tao},
  booktitle={Proceedings of the 46th IEEE/ACM International Conference on Software Engineering},
  pages={1--12},
  year={2024}
}

@article{liu2024understanding,
  title={Understanding llms: A comprehensive overview from training to inference},
  author={Liu, Yiheng and He, Hao and Han, Tianle and Zhang, Xu and Liu, Mengyuan and Tian, Jiaming and Zhang, Yutong and Wang, Jiaqi and Gao, Xiaohui and Zhong, Tianyang and others},
  journal={arXiv preprint arXiv:2401.02038},
  year={2024}
}
@inproceedings{ngiam2011multimodal,
  title={Multimodal deep learning},
  author={Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={689--696},
  year={2011}
}

@inproceedings{shinde2018review,
  title={A review of machine learning and deep learning applications},
  author={Shinde, Pramila P and Shah, Seema},
  booktitle={2018 Fourth international conference on computing communication control and automation (ICCUBEA)},
  pages={1--6},
  year={2018},
  organization={IEEE}
}

@article{zhou2021multi,
  title={Multi-task learning for segmentation and classification of tumors in 3D automated breast ultrasound images},
  author={Zhou, Yue and Chen, Houjin and Li, Yanfeng and Liu, Qin and Xu, Xuanang and Wang, Shu and Yap, Pew-Thian and Shen, Dinggang},
  journal={Medical Image Analysis},
  volume={70},
  pages={101918},
  year={2021},
  publisher={Elsevier}
}

@article{waibel2021instantdl,
  title={InstantDL: an easy-to-use deep learning pipeline for image segmentation and classification},
  author={Waibel, Dominik Jens Elias and Shetab Boushehri, Sayedali and Marr, Carsten},
  journal={BMC bioinformatics},
  volume={22},
  pages={1--15},
  year={2021},
  publisher={Springer}
}

@article{zan2022cert,
  title={CERT: continual pre-training on sketches for library-oriented code generation},
  author={Zan, Daoguang and Chen, Bei and Yang, Dejian and Lin, Zeqi and Kim, Minsu and Guan, Bei and Wang, Yongji and Chen, Weizhu and Lou, Jian-Guang},
  journal={arXiv preprint arXiv:2206.06888},
  year={2022}
}

@article{chen2023effectiveness,
  title={On the effectiveness of large language models in domain-specific code generation},
  author={Chen, Meng and Zhang, Hongyu and Wan, Chengcheng and Wei, Zhao and Xu, Yong and Wang, Juhong and Gu, Xiaodong},
  journal={arXiv preprint arXiv:2312.01639},
  year={2023}
}

@article{zhang2023repocoder,
  title={Repocoder: Repository-level code completion through iterative retrieval and generation},
  author={Zhang, Fengji and Chen, Bei and Zhang, Yue and Keung, Jacky and Liu, Jin and Zan, Daoguang and Mao, Yi and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2303.12570},
  year={2023}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{lu2021codexglue,
  title={Codexglue: A machine learning benchmark dataset for code understanding and generation},
  author={Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and others},
  journal={arXiv preprint arXiv:2102.04664},
  year={2021}
}

@article{song2024good,
  title={The good, the bad, and the greedy: Evaluation of llms should not ignore non-determinism},
  author={Song, Yifan and Wang, Guoyin and Li, Sujian and Lin, Bill Yuchen},
  journal={arXiv preprint arXiv:2407.10457},
  year={2024}
}
@inproceedings{tran2019does,
  title={Does BLEU score work for code migration?},
  author={Tran, Ngoc and Tran, Hieu and Nguyen, Son and Nguyen, Hoan and Nguyen, Tien},
  booktitle={2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC)},
  pages={165--176},
  year={2019},
  organization={IEEE}
}
@article{chen2024using,
  title={Using Prompts to Guide Large Language Models in Imitating a Real Person's Language Style},
  author={Chen, Ziyang and Moscholios, Stylios},
  journal={arXiv preprint arXiv:2410.03848},
  year={2024}
}

@article{ouyang2023llm,
  title={LLM is Like a Box of Chocolates: the Non-determinism of ChatGPT in Code Generation},
  author={Ouyang, Shuyin and Zhang, Jie M and Harman, Mark and Wang, Meng},
  journal={arXiv preprint arXiv:2308.02828},
  year={2023}
}

@article{sahoo2024systematic,
  title={A systematic survey of prompt engineering in large language models: Techniques and applications},
  author={Sahoo, Pranab and Singh, Ayush Kumar and Saha, Sriparna and Jain, Vinija and Mondal, Samrat and Chadha, Aman},
  journal={arXiv preprint arXiv:2402.07927},
  year={2024}
}
@book{el2019deep,
  title={Deep learning pipeline: building a deep learning model with TensorFlow},
  author={El-Amir, Hisham and Hamdy, Mahmoud},
  year={2019},
  publisher={Apress}
}
@inproceedings{nam2024using,
  title={Using an llm to help with code understanding},
  author={Nam, Daye and Macvean, Andrew and Hellendoorn, Vincent and Vasilescu, Bogdan and Myers, Brad},
  booktitle={Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
  pages={1--13},
  year={2024}
}

@article{almarzouq2020mining,
  title={Mining GitHub for research and education: challenges and opportunities},
  author={AlMarzouq, Mohammad and AlZaidan, Abdullatif and AlDallal, Jehad},
  journal={International Journal of Web Information Systems},
  volume={16},
  number={4},
  pages={451--473},
  year={2020},
  publisher={Emerald Publishing Limited}
}
@article{shrivastava2023repofusion,
  title={Repofusion: Training code models to understand your repository},
  author={Shrivastava, Disha and Kocetkov, Denis and de Vries, Harm and Bahdanau, Dzmitry and Scholak, Torsten},
  journal={arXiv preprint arXiv:2306.10998},
  year={2023}
}

@article{kalliamvakou2016depth,
  title={An in-depth study of the promises and perils of mining GitHub},
  author={Kalliamvakou, Eirini and Gousios, Georgios and Blincoe, Kelly and Singer, Leif and German, Daniel M and Damian, Daniela},
  journal={Empirical Software Engineering},
  volume={21},
  pages={2035--2071},
  year={2016},
  publisher={Springer}
}

@inproceedings{dabic2021sampling,
  title={Sampling projects in github for MSR studies},
  author={Dabic, Ozren and Aghajani, Emad and Bavota, Gabriele},
  booktitle={2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)},
  pages={560--564},
  year={2021},
  organization={IEEE}
}
@article{madeja2021automating,
  title={Automating test case identification in java open source projects on github},
  author={Madeja, Matej and Porub{\"a}n, Jaroslav and Ba{\v{c}}{\'\i}kov{\'a}, Michaela and Sul{\'\i}r, Mat{\'u}{\v{s}} and Juh{\'a}r, J{\'a}n and Chodarev, Sergej and Gurb{\'a}l', Filip},
  journal={arXiv preprint arXiv:2102.11678},
  year={2021}
}

@inproceedings{tufano2022methods2test,
  title={Methods2Test: A dataset of focal methods mapped to test cases},
  author={Tufano, Michele and Deng, Shao Kun and Sundaresan, Neel and Svyatkovskiy, Alexey},
  booktitle={Proceedings of the 19th International Conference on Mining Software Repositories},
  pages={299--303},
  year={2022}
}

@inproceedings{fredriksson2020data,
  title={Data labeling: An empirical investigation into industrial challenges and mitigation strategies},
  author={Fredriksson, Teodor and Mattos, David Issa and Bosch, Jan and Olsson, Helena Holmstr{\"o}m},
  booktitle={International Conference on Product-Focused Software Process Improvement},
  pages={202--216},
  year={2020},
  organization={Springer}
}

@inproceedings{mohajer2024effectiveness,
author = {Mohajer, Mohammad Mahdi and Aleithan, Reem and Harzevili, Nima Shiri and Wei, Moshi and Belle, Alvine Boaye and Pham, Hung Viet and Wang, Song},
title = {Effectiveness of ChatGPT for Static Analysis: How Far Are We?},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664646.3664777},
doi = {10.1145/3664646.3664777},
abstract = {This paper conducted a novel study to explore the capabilities of ChatGPT, a state-of-the-art LLM, in static analysis tasks such as static bug detection and false positive warning removal. In our evaluation, we focused on two types of typical and critical bugs targeted by static bug detection, i.e., Null Dereference and Resource Leak, as our subjects. We employ Infer, a well-established static analyzer, to aid the gathering of these two bug types from 10 open-source projects. Consequently, our experiment dataset contains 222 instances of Null Dereference bugs and 46 instances of Resource Leak bugs.                                Our study demonstrates that ChatGPT can achieve remarkable performance in the mentioned static analysis tasks, including bug detection and false-positive warning removal.                                 In static bug detection, ChatGPT achieves accuracy and precision values of up to 68.37\% and 63.76\% for detecting Null Dereference bugs and 76.95\% and 82.73\% for detecting Resource Leak bugs, improving the precision of the current leading bug detector, Infer by 12.86\% and 43.13\% respectively.                                 For removing false-positive warnings, ChatGPT can reach a precision of up to 93.88\% for Null Dereference bugs and 63.33\% for Resource Leak bugs, surpassing existing state-of-the-art false-positive warning removal tools.},
booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software},
pages = {151–160},
numpages = {10},
keywords = {ChatGPT, Large language models, Static analysis},
location = {Porto de Galinhas, Brazil},
series = {AIware 2024}
}

@article{shiri2024history,
  title={History-Driven Fuzzing For Deep Learning Libraries},
  author={Shiri Harzevili, Nima and Mohajer, Mohammad Mahdi and Wei, Moshi and Pham, Hung Viet and Wang, Song},
  journal={ACM Transactions on Software Engineering and Methodology},
  publisher={ACM New York, NY}
}

@article{zapf2016measuring,
  title={Measuring inter-rater reliability for nominal data--which coefficients and confidence intervals are appropriate?},
  author={Zapf, Antonia and Castell, Stefanie and Morawietz, Lars and Karch, Andr{\'e}},
  journal={BMC medical research methodology},
  volume={16},
  pages={1--10},
  year={2016},
  publisher={Springer}
}

@inproceedings{shrivastava2023repository,
  title={Repository-level prompt generation for large language models of code},
  author={Shrivastava, Disha and Larochelle, Hugo and Tarlow, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={31693--31715},
  year={2023},
  organization={PMLR}
}
@article{tian2023test,
  title={Test-case-driven programming understanding in large language models for better code generation},
  author={Tian, Zhao and Chen, Junjie},
  journal={arXiv preprint arXiv:2309.16120},
  year={2023}
}
@article{hao2022aixbench,
  title={Aixbench: A code generation benchmark dataset},
  author={Hao, Yiyang and Li, Ge and Liu, Yongqiang and Miao, Xiaowei and Zong, He and Jiang, Siyuan and Liu, Yang and Wei, He},
  journal={arXiv preprint arXiv:2206.13179},
  year={2022}
}

@article{cassano2022multipl,
  title={Multipl-e: A scalable and extensible approach to benchmarking neural code generation},
  author={Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and Phipps-Costin, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q and others},
  journal={arXiv preprint arXiv:2208.08227},
  year={2022}
}

@article{chen2024decix,
  title={DeciX: Explain Deep Learning Based Code Generation Applications},
  author={Chen, Simin and Li, Zexin and Yang, Wei and Liu, Cong},
  journal={Proceedings of the ACM on Software Engineering},
  volume={1},
  number={FSE},
  pages={2424--2446},
  year={2024},
  publisher={ACM New York, NY, USA}
}

@article{liu2024your,
  title={Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation},
  author={Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{mu2024clarifygpt,
  title={ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification},
  author={Mu, Fangwen and Shi, Lin and Wang, Song and Yu, Zhuohao and Zhang, Binquan and Wang, ChenXue and Liu, Shichao and Wang, Qing},
  journal={Proceedings of the ACM on Software Engineering},
  volume={1},
  number={FSE},
  pages={2332--2354},
  year={2024},
  publisher={ACM New York, NY, USA}
}

@inproceedings{haluptzok2022language,
  title={Language Models Can Teach Themselves to Program Better},
  author={Haluptzok, Patrick and Bowers, Matthew and Kalai, Adam Tauman},
  booktitle={Deep Reinforcement Learning Workshop NeurIPS 2022}
}
@inproceedings{park2021facilitating,
  title={Facilitating knowledge sharing from domain experts to data scientists for building nlp models},
  author={Park, Soya and Wang, April Yi and Kawas, Ban and Liao, Q Vera and Piorkowski, David and Danilevsky, Marina},
  booktitle={Proceedings of the 26th International Conference on Intelligent User Interfaces},
  pages={585--596},
  year={2021}
}
@article{singaravel2020explaining, title={Explainable deep convolutional learning for intuitive model development by non–machine learning domain experts}, volume={6}, DOI={10.1017/dsj.2020.22}, journal={Design Science}, author={Singaravel, Sundaravelpandian and Suykens, Johan and Janssen, Hans and Geyer, Philipp}, year={2020}, pages={e23}} 


@inproceedings{xiong2017mining,
  title={Mining Developer Behavior Across GitHub and StackOverflow.},
  author={Xiong, Yunxiang and Meng, Zhangyuan and Shen, Beijun and Yin, Wei},
  booktitle={SEKE},
  pages={578--583},
  year={2017}
}

@Article{technologies2024Manakitsa,
AUTHOR = {Manakitsa, Nikoleta and Maraslidis, George S. and Moysis, Lazaros and Fragulis, George F.},
TITLE = {A Review of Machine Learning and Deep Learning for Object Detection, Semantic Segmentation, and Human Action Recognition in Machine and Robotic Vision},
JOURNAL = {Technologies},
VOLUME = {12},
YEAR = {2024},
NUMBER = {2},
ARTICLE-NUMBER = {15},
URL = {https://www.mdpi.com/2227-7080/12/2/15},
ISSN = {2227-7080},
DOI = {10.3390/technologies12020015}
}
@Article{Vinodkumar2023Survey,
AUTHOR = {Vinodkumar, Prasoon Kumar and Karabulut, Dogus and Avots, Egils and Ozcinar, Cagri and Anbarjafari, Gholamreza},
TITLE = {A Survey on Deep Learning Based Segmentation, Detection and Classification for 3D Point Clouds},
JOURNAL = {Entropy},
VOLUME = {25},
YEAR = {2023},
NUMBER = {4},
ARTICLE-NUMBER = {635},
URL = {https://www.mdpi.com/1099-4300/25/4/635},
ISSN = {1099-4300},

DOI = {10.3390/e25040635}
}


@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@misc{liu2018neural,
  title={Neural network methods for natural language processing},
  author={Liu, Yang and Zhang, Meng},
  year={2018},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{chen2016xgboost,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
  pages={785--794},
  year={2016}
}

@article{diederik2014adam,
  title={Adam: A method for stochastic optimization},
  author={Diederik, P Kingma},
  journal={(No Title)},
  year={2014}
}

@article{wen2020time,
  title={Time series data augmentation for deep learning: A survey},
  author={Wen, Qingsong and Sun, Liang and Yang, Fan and Song, Xiaomin and Gao, Jingkun and Wang, Xue and Xu, Huan},
  journal={arXiv preprint arXiv:2002.12478},
  year={2020}
}
@inproceedings{howard2019searching,
  title={Searching for mobilenetv3},
  author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1314--1324},
  year={2019}
}

@inproceedings{kirillov2019panoptic,
  title={Panoptic feature pyramid networks},
  author={Kirillov, Alexander and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6399--6408},
  year={2019}
}

@article{wu2020comprehensive,
  title={A comprehensive survey on graph neural networks},
  author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Philip, S Yu},
  journal={IEEE transactions on neural networks and learning systems},
  volume={32},
  number={1},
  pages={4--24},
  year={2020},
  publisher={IEEE}
}

@article{xie2024frontiers,
  title={Frontiers of Deep Learning: From Novel Application to Real-World Deployment},
  author={Xie, Rui},
  journal={arXiv preprint arXiv:2407.14386},
  year={2024}
}

@article{jimenez2023swe,
  title={Swe-bench: Can language models resolve real-world github issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2310.06770},
  year={2023}
}

@article{zhuo2024bigcodebench,
  title={Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions},
  author={Zhuo, Terry Yue and Vu, Minh Chien and Chim, Jenny and Hu, Han and Yu, Wenhao and Widyasari, Ratnadira and Yusuf, Imam Nur Bani and Zhan, Haolan and He, Junda and Paul, Indraneil and others},
  journal={arXiv preprint arXiv:2406.15877},
  year={2024}
}
@article{chan2024mle,
  title={Mle-bench: Evaluating machine learning agents on machine learning engineering},
  author={Chan, Jun Shern and Chowdhury, Neil and Jaffe, Oliver and Aung, James and Sherburn, Dane and Mays, Evan and Starace, Giulio and Liu, Kevin and Maksin, Leon and Patwardhan, Tejal and others},
  journal={arXiv preprint arXiv:2410.07095},
  year={2024}
}
@inproceedings{kisowski2015how,
author = {v. Kistowski, J\'{o}akim and Arnold, Jeremy A. and Huppler, Karl and Lange, Klaus-Dieter and Henning, John L. and Cao, Paul},
title = {How to Build a Benchmark},
year = {2015},
isbn = {9781450332484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668930.2688819},
doi = {10.1145/2668930.2688819},
abstract = {Standardized benchmarks have become widely accepted tools for the comparison of products and evaluation of methodologies. These benchmarks are created by consortia like SPEC and TPC under confidentiality agreements which provide little opportunity for outside observers to get a look at the processes and concerns that are prevalent in benchmark development. This paper introduces the primary concerns of benchmark development from the perspectives of SPEC and TPC committees. We provide a benchmark definition, outline the types of benchmarks, and explain the characteristics of a good benchmark. We focus on the characteristics important for a standardized benchmark, as created by the SPEC and TPC consortia. To this end, we specify the primary criteria to be employed for benchmark design and workload selection. We use multiple standardized benchmarks as examples to demonstrate how these criteria are ensured.},
booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
pages = {333–336},
numpages = {4},
keywords = {sert, spec, spec cpu, specpower_ssj2008, tpc},
location = {Austin, Texas, USA},
series = {ICPE '15}
}

@article{tambon2024bugs,
  title={Bugs in large language models generated code},
  author={Tambon, Florian and Dakhel, Arghavan Moradi and Nikanjam, Amin and Khomh, Foutse and Desmarais, Michel C and Antoniol, Giuliano},
  journal={arXiv preprint arXiv:2403.08937},
  year={2024}
}

@inproceedings{islam2019comprehensive,
  title={A comprehensive study on deep learning bug characteristics},
  author={Islam, Md Johirul and Nguyen, Giang and Pan, Rangeet and Rajan, Hridesh},
  booktitle={Proceedings of the 2019 27th ACM joint meeting on european software engineering conference and symposium on the foundations of software engineering},
  pages={510--520},
  year={2019}
}