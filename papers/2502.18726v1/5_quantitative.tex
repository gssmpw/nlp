% \section{Research Questions}
% \label{sec:rq}

% Introduce your research questions briefly
\setlength{\parskip}{0pt}

\section{Quantitaive Analysis}
\label{sec:study}

\input{tabs/RQ1_ACC}

\textbf{What are the performances of SOTA LLMs on DL/ML code generation tasks?}
In this analysis, we investigate how the existing ML code generation benchmark (DS-1000) and \tool evaluate SOTA LLMs.
Table~\ref{tab:RQ1_ACC} shows the pass@1 metric of SOTA LLMs on those benchmarks.
%specifically designed to test code generation capabilities. The dataset consists of carefully crafted prompts or doc-strings to minimize the risk of memorization by the LLM. 
To focus on 
%the purpose of this RQ is to 
demonstrating \tool's ability to evaluate existing LLMs, we intentionally avoided using specialized prompt strategies, opting instead for vanilla prompts to assess the model's baseline performance. However, the use of advanced prompt engineering strategies could yield different results and this will be interesting for future usage of \tool. 

Our analysis underscores that even the most advanced model such as GPT-4o struggles with ML/DL-specific code generation. Specifically, GPT-4o achieves only 60\% and 31\% Pass@1 scores on DS-1000 and \tool respectively. 
Also, when comparing between DS-1000 and \tool, our benchmark is more challenging as the SOTA LLM gets a much lower Pass@1 score, SOTA LLMs in other categories are behind GPT-4o with performance as low as 15\% for the tiny Mistral 7B model on \tool. The overall weak performance of these models highlights the ongoing challenges in generating reliable, executable ML/DL code which supports the need for more benchmarks in this domain.

%\hung{do we have any idea how our run of ds-1000 is much lower than their run? Was it because of pass@k where k is not 1? Or do they have special dependencies that we need to set up? It is kind of bad if they say they get 53\% but we get 20\%, to the reader, this will effect their confidence in our work.}
%\alireza{I used exactly run their scrips and build the environment as they set. I can mail them and ask}
%\hung{yes, for now I will just include their reported value.}

%Our analysis underscores that even the most advanced models struggle with code generation for deep learning %tasks. GPT-4, while performing best, still only achieves a Pass@1 score of 0.31 This reflects a broader trend—none of the models truly excel. Claude 3.5 and LLaMA 3.1, though slightly behind GPT-4o, also fail to meet expectations. Mistral, the weakest, falls significantly short of the benchmark. The overall weak performance of these models highlights the ongoing challenges in generating reliable, executable code.

% \hung{can we have the passing rate analysis there?}

% \hung{Also compilation rate or syntact error rate? This would be very interesting to discuss here.}

Overall, GPT-4o's pass@k is 31\%, but to further assess its performance, we calculated the average passing rate across the functions, which was higher at 40\%. This suggests that while only 31\% of functions pass all test cases, many pass some. With additional insights, these partially valid cases could be improved.

%the overall pass@k metric may appear lower, the model demonstrates improved performance in specific scenarios when evaluated on a case-by-case basis.

%For example, more advanced prompt techniques can be used to improve the result. Such techniques often require additional domain-specific information from deeper analyses that DS-1000 could not provide. In the later RQs, we will demonstrate how \tool can be used to conduct deep analyses so useful insights can be extracted and more sophisticated prompting techniques can be developed to better tackle the task of generating DL-specific code.

%To improve these outcomes, future experiments might explore optimized prompting strategies, such as prompt tuning or providing more detailed context. These approaches could potentially enhance the performance of GPT-4o in complex code generation tasks, particularly those within the AI domain. Our current findings suggest that while GPT-4o demonstrates some level of competence, its application in practical AI code generation remains limited without further optimization.

\begin{tcolorbox}[boxrule=0.5pt, colback=gray!10,  arc=4pt,left=3pt,right=3pt,top=3pt,bottom=3pt,boxsep=0pt
]
\textbf{Finding 1:} Our evaluation indicates that current LLMs struggle to generate correct, executable code for ML/DL tasks. Although GPT-4o is the strongest among the tested models, it still falls short of meeting practical standards (Pass@k score of only 31\%). 
%Prior benchmarks such as DS-1000 pose some challenges to the top SOTA LLM but are still not as challenging as \tool. 
A deeper analysis is needed (which \tool can provide) to extract insight into improving prompting techniques.
%This underscores the limitations of existing LLMs in AI code generation and highlights the need for optimized prompting strategies or further advancements to enhance their performance.
\end{tcolorbox}

% \begin{tcolorbox}[boxrule=0.5pt, colback=gray!10, arc=4pt,left=6pt,right=6pt,top=6pt,bottom=6pt,boxsep=0pt]
% \textbf{Finding 1:} Our findings suggest that task complexity, data variability, and the nature of the input are critical factors in determining how well an LLM can generate accurate code. These insights shed light on the current limitations of LLMs and provide direction for future research aimed at improving their performance in more complex domains, such as image segmentation.
% \end{tcolorbox}

\input{tabs/RQ2_ACC}

\textbf{Which stages in the ML pipeline pose a greater challenge for SOTA LLMs?}
We conduct an analysis of the challenges of generating DL code for specific stages in the pipeline (enabled by %Since 
\tool provided categorization)
%of the stage that each of our data points belongs to in the ML pipeline, we group the result into these specific stages. T
Table~\ref{tab:RQ2_MLStage} presents the Pass@1 scores that each LLM achieves for code in each %categorization of 
ML/DL pipeline stage.
Our result shows that the best SOTA LLM---GPT-4o---outperforms all of the other LLMs in all stages. Claude 3.5 sonnet is the closest second, where in the \textit{Inference} category, it is level with GPT-4o. 

%We tested our benchmark using SOTA LLMs, focusing on its ability to perform tasks across these different stages. By examining the model's performance in each category, we aim to identify the specific challenges and complexities involved in generating the various pipeline steps. This analysis will provide valuable insights into the strengths and limitations of different LLMs when applied to different tasks within the AI pipeline.

%\input{figs/accepted1}
\textit{Pre/post Processing} stages often require small but important data transformation tasks, thus such codes are the most available. Hence, 
%in AI and deep learning projects where 
\tool collected the most data in this category (210/520). %This is because many different tasks are performed on the data at the beginning (pre-processing) or at the end (post-processing) of the pipeline. 
Code in this category prepares and cleans the input data for the model and formats the model's output. 
%For example, the prompt and reference code in Fig~\ref{fig:accepted1} demonstrate a straightforward task of converting the format of the bounding boxes which is common in image-processing ML systems.
Our result shows that LLMs have the most success in generating code for 
%this case.
the Pre/post Processing stages. 
One possible reason for the higher Pass@1 scores is that the models might have more changes to learn from a vast set of samples in this category. 
%of such 
%pre/post processing category.
%tasks in their respective training sets. 

%The reason LLMs, usually perform better in pre- and post-processing might be to do with the higher number of related code samples to these tasks, in the training set. Pre- and post-processing involve common tasks(e.g., \ref{fig:accepted1} which GPT 4o generated correct code for that which is a simple pre/post-processing task that easily transforms bounding box coordinates) that are used in many projects, making them easier for LLMs to predict and generate accurately. In addition, the code for these stages is usually more repetitive, which helps the models handle them more effectively.

On the other hand, LLMs struggle to generate code for the \textit{Model Construction} stage with the
%showed the 
lowest Pass@1 scores.
%in our experiments. 
This is because the code for this stage is more complex, very project-specific, and often longer.
%is usually much longer and more complex. Unlike pre and post-processing, model construction is unique to each project, making it harder for GPT-4o to generate accurate code.

%\input{figs/rejected2}
%\input{figs/segmentation}

%For example, Fig~\ref{fig:rejected2} shows the prompt and the reference code corresponding to a model construction code that GPT-4o failed to generate code for. The main reason is that the code involves utilizing various libraries and constructing a model in a non-standardized way (i.e., with significant variations in structure and organization). Since these tasks appear less frequently in the development pipeline, LLMs are exposed to fewer examples, which diminishes their ability to generate code for these stages.

%, we see an example related to model construction where GPT-4o fails to generate the correct code. 
%This is primarily due to the complexity of the task, which involves utilizing various libraries and constructing a model in a non-standardized way. The intricacy of integrating multiple libraries adds an extra layer of difficulty, making it challenging for the model to produce accurate code.

%Moreover, the variability in how model construction is implemented across different projects adds to the challenge. The code for these tasks is often less standardized, with significant differences in structure and organization. This lack of uniformity, combined with the typically larger codebase for these steps, makes it harder for LLMs to handle them effectively. Since these tasks appear less frequently in the development pipeline compared to pre-processing and post-processing, LLMs are exposed to fewer examples, which diminishes their accuracy when generating code for these stages.

%\input{figs/rejected1}
%Fig~\ref{fig:rejected1} presents another case where the LLM struggled, this time with generating a function in the Training stage. Specifically, the forward function for each class can vary significantly, introducing further complexity. These variations make it difficult for the LLM to generalize and learn how to generate the correct code, as it encounters different implementations depending on the context. Perhaps, an interactive prompt technique can help clarify the context or some additional domain-specific information that could be added as a few-step learning to improve the models' performance.

%Our deeper analysis of GPT-4o shows that it achieves an average passing rate of 0.40\% for pre/post-processing tasks, and 0.32\% for model construction. The passing rates for training and inference are 38\% and 39\%, respectively. Notably, for evaluation and metrics, the model's performance jumps to 46\%, highlighting its strengths in that area.

\begin{tcolorbox}[boxrule=0.5pt, colback=gray!10, arc=4pt,left=3pt,right=3pt,top=3pt,bottom=3pt,boxsep=0pt
]
\textbf{Finding 2:} Our study shows that LLMs perform best (e.g., GPT-4o's Pass@1 score is 33\%) in Pre/post Processing stages because code in such stages involves common, repetitive tasks, making them easier to learn and generate. In contrast, the Model Construction stage has the lowest scores (e.g., GPT-4o's Pass@1 is 26\%) due to its complexity, variability across projects, and the need to integrate multiple libraries. 
\tool enables detailed analysis which helps future work develop prompting techniques to provide LLMs with the appropriate context and information, improving their performance.
\end{tcolorbox}


\input{tabs/RQ4_ACC}

\textbf{Are different ML task types easier or harder to generate code for?}
In this analysis, we demonstrate how \tool's categorization of ML task types can enable deeper analysis of LLMs code generation and may provide additional insight that can help research propose more accurate prompting techniques and models. Specifically, we made use of our categorization of ML/DL task types: \textit{Classification}, \textit{Regression}, \textit{Object Detection}, \textit{Image Segmentation}, \textit{Time Series Prediction}, \textit{Recommendation}, and \textit{General}.
%In our benchmark construction, we categorized various tasks into distinct 
%task types: 
%1. Classification, 2.Regression, 3. Recommendation, 4. Prediction, 5. Segmentation, 6. Detection
%These categories represent the broad range of tasks typically encountered in machine learning and artificial intelligence. The primary objective of this research is to evaluate the relative difficulty of generating code for each of these task types using large LLMs.

%Through our experiments, 
There is a significant disparity in the Pass@1 score of generated code across task types. Notably, scores for \textit{Recommendation} tasks were the highest among all LLMs, with the best score of 58\% with GPT-4o. On the other end of the scale, \textbf{Image Segmentation} tasks' scores are the lowest across all LLMs. These results indicate that each task type has characteristics LLMs can or not yet capture.
%is either suitable or not for LLMs' code generation.

%Specifically, we suspect that image segmentation code is harder to generate due to its close relation to image processing. Specifically, segmentation tasks involve dividing an image into distinct regions, which can vary greatly based on image type, number of channels, and image resolution which present significant challenges to even the top-of-the-line LLMs such as GPT-4o. 
%For example, Fig~\ref{fig:segmentation} shows an example where the generated code incorrectly assumes that cropping can be represented by a simple translation matrix.
%, leading to improper image processing.

%Recommendation codes, on the other hand, generally involve structured data, such as user-item interactions, which are more predictable compared to image data. The limited variation in data structures and patterns across different recommendation tasks makes it easier for GPT-4o to generate accurate code. Specifically, Recommendation systems typically rely on common algorithms such as collaborative filtering or matrix factorization, both of which follow relatively standardized patterns. This standardization likely enables GPT-4o to "memorize" these patterns (one example is shown in Fig~\ref{fig:accepted_structured}).
%that GPT-4o memorized the structure of data.

%\add{Our analysis of GPT-4o reveals varied strengths across tasks: while recommendation tasks showed no pass@k improvement, segmentation achieved a notable 28\% accuracy. Regression and classification tasks improved to 49\% and 40\% accuracy, respectively, with prediction and detection each reaching a strong 45\% pass@k accuracy.}

%Recommendation systems typically rely on common algorithms such as collaborative filtering or matrix factorization, both of which follow relatively standardized patterns. This standardization likely enables GPT-4o to "memorize" these patterns, allowing for a higher degree of accuracy when generating recommendation code. Furthermore, the structured nature of the data in recommendation tasks—usually in numerical or categorical form—presents fewer challenges than the complex, high-dimensional data involved in segmentation tasks.

%The disparity in accuracy between recommendation and segmentation tasks highlights the varying levels of difficulty GPT-4o face in different domains. Tasks such as segmentation, which involve complex image data and require nuanced understanding, are more challenging for LLMs and result in lower performance. Conversely, recommendation tasks, with their structured nature and repetitive patterns, are easier for LLMs to process.

% \hung{Some analysis of passing rate or syntax error would be interesting here.}

\begin{tcolorbox}[boxrule=0.5pt, colback=gray!10,  arc=4pt,left=3pt,right=3pt,top=3pt,bottom=3pt,boxsep=0pt
]
\textbf{Finding 3:} Different ML/DL tasks vary in complexity affecting LLMs' code generation abilities.
LLMs score highest (up to 58\% with GPT-4o) for recommendation code due to their predictable input structure and patterns.
However, they struggled with image segmentation tasks (up to only 21\%) since they require pixel-level understanding and generalization across variable inputs. \tool's categorization provides insights to improve DL code generation techniques.
\end{tcolorbox}
%\vspace{-8pt}

\input{tabs/RQ3_ACC}
%\input{figs/gray_rgb}


\textbf{Do the various required input data types have any effect on how LLMs generate DL code?}
This analysis aims to investigate if different input data types have different effects on how well LLMs generate code. \tool enables this analysis by providing a categorization of input data types: \textit{Image}, \textit{Text}, \textit{Structured Array}, and \textit{Others}. By comparing their performance across these input types, the study evaluates 
%seeks to provide a comprehensive understanding of 
the versatility and limitations of LLMs in dealing with varied data sources.
%and provide specific areas of improvement that future researchers can address.
%investigate the performance of LLMs across different input types, including image, text, and tabular data. The study will evaluate how effectively LLMs process and analyze each type of input, identifying strengths and weaknesses in their ability to handle diverse data formats. By comparing their performance across these input types, the study seeks to provide a comprehensive understanding of the versatility and limitations of LLMs in dealing with varied data sources within the AI pipeline.
%In this study, we examine the challenges of generating code for a range of tasks, with a particular focus on handling different input types. 
Table~\ref{tab:rq3_accuracy} shows the Pass@1 of all LLMs 
%under test 
across different types of input data.
Specifically, performance for image-related tasks is the lowest (only up to 25\% for GPT-4o).
%We suspect that 
This can be attributed to the inherent complexity and lack of consistent structure in image data, such as varying shapes, resolutions, and channel configurations (e.g., grayscale vs. RGB). 
%Fig~\ref{fig:gray_rgb} shows an example where GPT-4o failed to generate the correct type torch.unit8.
%of different image types. The problem with generated code is when image.dtype is torch.uint8, creating a tensor with dtype=image.dtype and then dividing by 255.0 leads to integer division before casting to float. This results in weights effectively becoming [0, 0, 0] because integer division truncates the decimal part.

%Our analysis shows that while the overall difficulty of these tasks is comparable, the performance varies significantly depending on the input type. Specifically, performance for image-related tasks is the lowest. This can be attributed to the inherent complexity and lack of consistent structure in image data, such as varying shapes, resolutions, and channel configurations (e.g., grayscale vs. RGB). These variations increase the complexity of code generation for image-based tasks compared to other data types, making it more challenging for models to generalize and produce accurate code.

%\input{figs/text_accept}
On the other hand, our results show that textual input data type in code generation exhibits better performance (up to 32\%). We assume that most textual input data type are tokenized and converted before being processed in the DL model, which makes functions that deal directly with textual input data types quite standard and easier to generate.
%Fig~\ref{fig:text_accept} shows an example of a text-related prompt where GPT-4o can generate the correct code. Specifically, the prompt asks LLMs to convert to n-grams from a sequence of tokenized text which belong to the pre-processing stage.
%This observation, as shown in Table~\ref{tab:rq3_accuracy} highlights that although image input type requires difficult levels of reasoning and technical handling during the code generation process, text input type needs less reasoning for finding best result. 
The structured array category shows slightly higher performance (up to 29\% with GPT-4o) compared to image data.
%might be due to
%since structured array are 
%which may be attributed to 
%the structured nature of such data type. 
This is because structured data inherently provides a clearer, more organized format, reducing the reasoning required by the model. As a result, the model more easily generates accurate code for table-related tasks, as opposed to the unstructured, abstract nature of image-based tasks. This suggests that structured data simplifies the generation process.
%, leading to marginally better outcomes.

%\input{figs/accepted_table}
%Fig~\ref{fig:accepted_structured} provides an example of structured array data where GPT successfully generates accurate code. The function \_adapted\_cohen\_kappa\_score, designed to extend Cohen's kappa score by handling the special case of perfect agreement and preventing a division by zero error, operates on two Numpy arrays y1 and y2. GPT can generate code for this example efficiently because the input data consists of two Numpy arrays, which are are commonly used in numerical computations and have a well-defined structure that is predictable for the model. Both y1 and y2 share the same Numpy structure, making it easier to generate logic based on well-known Numpy operations. 
%This predictability and consistency in the data format allow GPT to handle the code generation more effectively, avoiding the complications seen with less structured or more variable data types like images.

%\add{Our deeper analysis of GPT-4o reveals that, although the pass@k accuracy for text input data is 35\%, the average passing rate is 44\%. On the other hand, for image data, the average passing rate is 35\%, which demonstrates a different performance profile. Similarly, while the pass@k accuracy for tabular data stands at 29\%, the average passing rate for this data type improves to 42\%. These findings highlight that the model's performance varies across different input types, with notable differences between the pass@k metric and the average passing rate.}

\begin{tcolorbox}[boxrule=0.5pt, colback=gray!10,  arc=4pt,left=3pt,right=3pt,top=3pt,bottom=3pt,boxsep=0pt
]
\textbf{Finding 4:} This RQ demonstrates the detailed analysis possible with \tool. Our study shows image data had the lowest performance (Pass@1 score up to 25\%) due to complex input structures. In contrast, textual data tasks achieved higher performance (up to 32\%), likely because of more deterministic coding in the pre-processing stages.
\end{tcolorbox}
