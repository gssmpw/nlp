\section{Introduction}
In recent years, machine learning (ML) and deep learning (DL) have advanced significantly and
%and proliferated across a wide range of disciplines~\cite{malhotra2023recent, bengio2017deep}. Its versatile applications have attracted considerable attention from researchers~\cite{hatcher2018survey,  poirson2016fast, goodfellow2020generative}, prompting its 
have been integrated into various fields~\cite{hordri2016deep, kamilaris2018deep, gamboa2017deep}. DL coding has its challenges~\cite{arpteg2018software}, and because of its widespread use, many DL systems are developed by domain experts who are often not software developers~\cite{park2021facilitating, singaravel2020explaining}, which amplifies the problems even more.

%its widespread use, the coding aspect of deep learning is challenging~~\cite{arpteg2018software}, necessitating a solid foundational understanding. Large language models (LLM) have demonstrated their proficiency across a variety of tasks, including mathematical problem-solving, human reasoning~\cite{zellers2019hellaswag}, static analysis~\cite{mohajer2024effectiveness} and the adaptation of new information in real-world scenarios~\cite{wang2024survey}.

%Recent advancements in LLMs have notably enhanced the capability of code generation (CG)\cite{haluptzok2022language} using deep learning techniques combined with natural language processing~\cite{shin2021survey}. Also, LLMs specialized in programming languages like CodeT5~\cite{wang2021codet5}, CodeBERT~\cite{feng2020codebert}, and Magicoder~\cite{wei2023magicoder} have demonstrated significant efficacy in generating simple lines of code. Notably, state-of-the-art models such as the GPT-4~\cite{achiam2023gpt} have achieved an accuracy of at least 76.5\%~\cite{guo2024deepseek} on the HumanEval dataset~\cite{chen2021evaluating}. Furthermore, Zhang et al.~\cite{zhong2024ldb} reported an impressive 96.9\% accuracy on the same dataset. These findings underscore the potential of LLMs in generating Python code effectively.


Recently, with the rise of Large Language Models (LLMs) such as ChatGPT, LLMs are considered among the best solutions for coding tasks~\cite{wang2021codet5, feng2020codebert, achiam2023gpt}.
As shown in Table~\ref{tabs:other_benchmarks}, numerous code generation datasets~\cite{hendrycks2measuring,austin2021program,agashe-etal-2019-juice, lu2021codexglue, yu2018spider,du2024evaluating, zhuo2024bigcodebench} benchmark LLMs' code generation capabilities.
However, DS-1000~\cite{lai2023ds} is the only dataset offering a limited set of DL-specific code generation samples. Specifically, DS-1000 provides small code snippets, typically just a few lines, primarily focused on pre/post-processing tasks. Furthermore, they do not include categorizations such as DL phases (e.g., pre/post-processing, model construction, training, inference, evaluation) or input types (e.g., tabular, image, text), which could provide valuable insights for advancing DL code generation research.
%\hung{last time we talk about three things that we are better, do I miss something? \alireza{the granularity of our dataset is function level which reflects the real world scenario better}}.

%Despite the existence of benchmarks such as DS-1000~\cite{lai2023ds}, MBPP~\cite{hendrycks2measuring}, APPS~\cite{austin2021program}, JUICe~\cite{agashe-etal-2019-juice}, Spider~\cite{yu2018spider}, CoderEval~\cite{yu2024codereval} and ClassEval~\cite{du2024evaluating}, which serve various programming tasks, there remains a significant gap in the assessment of LLMs for generating DL-related code~\cite{jiang2024challenges, chen2024decix}. 

%The critical importance of deep learning tasks, combined with the challenges inherent in programming within this domain, necessitates the development of new benchmarks for DL-related code generation specifically tailored for evaluating LLMs. Moreover, although several studies have aimed to address this deficiency~\cite{shin2023good}, the absence of comprehensive test cases has impeded the establishment of meaningful metrics to measure LLM performance accurately\cite{liu2024your, mu2024clarifygpt}. 

To address this gap, we introduce \tool, a novel dataset designed to benchmark DL code generation at a functional level. \tool includes the code generation prompt, the ground-truth code at the function level, an extensive set of unit tests, and three categorizations (DL phases, ML tasks, and input data types). Unlike DS-1000, \tool provides a more comprehensive set of entries that cover the full DL pipeline, encompassing code examples for various machine learning tasks. Additionally, it includes DL functions with diverse input data types, including tabular, image, and text, making it a more diverse benchmark for DL code generation evaluation.
%including code from the pre-processing, model construction, training, post-processing, inference, and evaluation. Furthermore, \tool contains code examples for various machine learning tasks such as classification, regression, detection, recommendation, segmentation, and prediction. Finally, \tool collects DL functions with a wide range of inputs ranging from tabular, image, and text. 
Specifically, each entry of \tool dataset is categorized based on these three categories which allow a more in-depth evaluation of future DL code generation techniques:
(1)\textit{The DL/ML pipeline stages} (consist of pre/post-processing, model construction, training, inference, and evaluation), (2)\textit{The DL/ML tasks} (consist of classification, object detection, image segmentation, time-series prediction, recommendation, and regression), and (3)\textit{The input data types} (consist of text, image, and array).

%in this paper, we introduce a novel benchmark dataset named \tool specialized for DL-related code generation. The innovations presented in \tool address several limitations found in previous work, specifically those related to DL-related code generation. For example, while datasets like DS-1000~\cite{lai2023ds} provide a good amount of data for data science tasks, they fall short in covering a wide range of DL-related and ML-related tasks. 
 
%To fill this gap, \tool introduces a more comprehensive set of DL-related inputs, extending the dataset to better represent different categories within deep learning domain. This extension ensures that \tool reflects the complexities of deep learning and machine learning pipelines more accurately. 
%Also, \tool addresses the lack of comprehensive test cases, which has previously hindered the establishment of meaningful metrics, by incorporating both test cases and ground-truth code to enable a more accurate evaluation of code generation capabilities across different models. Furthermore, while previous studies have attempted to create benchmarks for code generation models~\cite{lai2023ds, shin2023good}, they often do so without clearly distinguishing between different stages of the DL/ML pipeline. To this mean, we have introduced a detailed categorization system in \tool to explore the challenges associated with generating code for each category~\cite{shinde2018review}. \tool contains three types of categorizations:
%\begin{enumerate}
%    \item Categorization based on the DL/ML pipeline stage~\cite{liu2024understanding}, which consists of pre-processing, post-processing, model construction, training, evaluation, and inference.
%    \item Categorization based on the data type~\cite{ngiam2011multimodal}, which consists of text, image, structured table and array categories.
%    \item Categorization based on the DL/ML task~\cite{waibel2021instantdl, zhou2021multi}, which consists of classification, detection, segmentation, and prediction.
%\end{enumerate}
%By doing so, we provide a framework to evaluate models across multiple dimensions, revealing areas where code generation models may struggle more than others. This allows for a more nuanced evaluation of how various stage of deep learning pipeline is challenging for code generation. 

%To demonstrate how useful \tool can be in evaluating DL code generation approaches, we applied GPT-4o~\cite{achiam2023gpt}, OpenAI's latest LLM, on \tool and analyze in-depth its DL code generation capability. 
Our study found that while GPT-4o demonstrated strong performance in various code generation tasks, it achieved only 31\% accuracy on \tool, significantly lower than the 60\% reported for DS-1000. We observed similar difficulty for other LLMs (e.g., 28\% vs. 54\% for Claude, 21\% vs. 41\% for LLaMA, and 15\% vs. 20\%  for Mistral) which indicates that \tool contains more challenging data points. Furthermore, the difficulty of generating code varies significantly across categories. For example, GPT-4o reaches an accuracy of 33\% for pre/post-processing tasks but only 26\% for model construction. Additionally, the accuracies vary even more among task types, ranging from 58\% for recommendation tasks to 21\% for segmentation tasks. These large gaps in performance across categories highlight the insights that \tool can bring to help improve the LLM DL code generation capability.

To further analyze the root causes of such a poor generation performance, we construct a bug taxonomy of the issues found in the generated DL code. Our qualitative analysis reveals that when compared to LLM-generated general code, LLM-generated DL code exhibits a higher frequency of \textit{deviation from the prompt} issues. Furthermore, \textit{arithmetic and logical errors} emerged as a new and frequently observed category in generated DL code.

%We conducted a study to categorize bug taxonomies in machine-generated code for DL. Our findings indicate that the most common bug pattern is deviation from the prompt(120), followed by arithmetic and logical errors(72). Additionally, we classified each bug pattern into several DL-specific subcategories. Based on our categorization, shape and dimension mismatches, API misuse, and algebraic errors account for the largest share of bugs.
\tool's data is available in our public repository\footnote{\href{https://anonymous.4open.science/r/DL-Bench-D65E/}{https://anonymous.4open.science/r/DL-Bench-D65E/}}.

\input{tabs/Others}

% (melika) you can add this sentence: "The low level of accuracy provides an opportunity for further improvements."

%Finally, we conduct experiments on \tool to evaluate the performance of code generation by an state-of-the-art LLM. For this means, we opted GPT-4o, OpenAI's latest LLM, as our subject of study. Thus, We analyze how well this model performs across different categories of \tool, demonstrating the difficulty of generating code for each categorization type. This allowed us to identify which parts of the dataset pose the greatest challenge for code generation and provide insights into areas where these models excel or struggle.

%This paper makes the following contributions:
%\begin{itemize}
%    \item \tool, a novel benchmark dataset for DL code generation which contains:
%    \begin{itemize}
%        \item High quality and diverse source code at the function level collected from GitHub
%        \item Three types of categorization: DL pipeline stages, data types, and ML task types
%    \end{itemize}
%   \item A study demonstrating \tool capability to perform an in-depth evaluation of the state-of-the-art LLMs such as GPT-4o, Claude3.5 Sonet, LLama 3.1 70B, and Mistral 7B.
%\end{itemize}



