\section{Qualitative Analysis}

\input{figs/taxonomy_graph}
\input{figs/tax_distribution}

\textbf{Taxonomy of Bugs in Generated DL Code:} GPT-4o achieves a pass@1 rate of only 31\% hence in most cases, it failed to generate the correct code. In this section, we build a taxonomy of common bug patterns and issues that arise in DL code generated by GPT-4o. This taxonomy is an expansion of Tambon et.al \cite{tambon2024bugs}'s bug taxonomy for LLM-generated regular code.

Following the same procedure as our labeling process, three authors manually investigate all GPT-4o failures and categorize them following Tambon et.al's taxonomy. At the same time, the annotators identify the DL-specific sub-categories for each failure. The result is the taxonomy presented in Fig~\ref{fig:taxonomy}. The appendix gives a detailed explanation of each bug type and sub-category.

%Specifically, we compare these patterns to those found in general Python code produced by LLMs and deep learning code generated by humans. To facilitate this analysis, we use Tambon et.al\cite{tambon2024bugs} work as a baseline for understanding bug patterns in general Python code generated by LLMs work as a baseline for understanding bug patterns in general Python code generated by LLMs, and  Islam et. al\cite{islam2019comprehensive} work for comparing bugs in deep learning code generated by LLMs versus that produced by humans.

%The section is structured as follows: First, we introduce and explain bug taxonomies. Next, we discuss the differences between bug patterns in general Python code versus deep learning-specific code. Finally, we compare the bug patterns found in human-generated deep learning code versus LLM-generated code.

\textbf{Failures in Generating the DL Code and General Code:} Tambon et. al\cite{tambon2024bugs} analyzed failures when CodeGen models generate code for the general tasks. Figures~\ref{fig:tax_distribution} show the distributions of the bug types when generating general code vs DL code. 
On the one hand, \textit{misinterpretation} (purple) is a common bug when generating both general code and DL code, however, due to more complex logic and arithmetic requirements, LLMs more often make this mistake when generating DL code. An example of this type of bug can be seen in Figure~\ref{fig:discussion2}. On the other hand, since GPT4o is much more capable compared to CodeGen models used by prior work, errors such as \textit{incomplete generation} (green), \textit{silly mistake} (dark gray), and \textit{syntax error} (yellow) occur at a much lower rate.

Furthermore, we have introduced several new categories of bugs that commonly arise in DL code generation. Firstly, \textit{errors in arithmetic and logical operations}(light blue) occur when incorrect calculations or flawed logical code are generated.
%, leading to unintended behaviours such as inaccurate model predictions or faulty algorithm implementations. 
Secondly, \textit{performance}(light brown) issues involve inefficient generated code with slow execution times, excessive memory consumption, or suboptimal utilization of resources.
%, which can hinder the scalability and practicality of deep learning applications. 
Lastly, \textit{prompt missing information}(light purple) when the prompts are missing details to fully address the problem at hand, resulting in incomplete or partially implemented solutions.
%such as missing functions, incomplete data preprocessing steps, or unfinished model architectures. 
These new categories identify important challenges that are unique to DL code generation.
%, facilitating more effective debugging, enhancing the quality of generated code, and ultimately contributing to the development of more robust and efficient deep learning models.

%Now, we will discuss the differences between bugs in general Python code generated by LLMs and those specific to deep learning code. Previous work, such as the analysis conducted by Tambon et. al\cite{tambon2024bugs}, provides an extensive evaluation of bugs in general Python code, with results summarized in Table~\ref{tab:general_bug}. While their findings reveal that many issues stemmed from deviations from the given prompt, they did not categorize bugs related to arithmetic or mathematical errors. This distinction is crucial because deep learning code often involves more algebraic and mathematical operations, increasing the likelihood of such errors. An example of this type of bug can be seen in Figure~\ref{fig:discussion2}.

%Additionally, their analysis focused on the CodeGen model, which is known to produce a higher percentage of syntax errors or incomplete code generations. In contrast, our analysis is based on GPT-4o, a more advanced and powerful model. The improved capabilities of GPT-4o reduce the likelihood of syntax errors and incomplete outputs, thereby providing a more robust baseline for analyzing deep learning-specific bugs. This shift highlights the importance of evaluating LLM-generated code within the specific context of the application, as deep learning code introduces unique challenges that are less prevalent in general Python code.

%On the other hand, we have introduced several new categories of bugs that commonly arise in code generation for deep learning systems. Firstly, \textbf{arithmetic and logical errors} occur when the generated code contains incorrect calculations or flawed logical operations, leading to unintended behaviours such as inaccurate model predictions or faulty algorithm implementations. Secondly, \textbf{performance issues} involve inefficiencies in the generated code that result in slow execution times, excessive memory consumption, or suboptimal utilization of computational resources, which can hinder the scalability and practicality of deep learning applications. Lastly, \textbf{incomplete problem resolution} refers to situations where the code generation prompt fails to fully address the problem at hand, resulting in incomplete or partially implemented solutions such as missing functions, incomplete data preprocessing steps, or unfinished model architectures. By categorizing these bugs, we aim to systematically identify and address the common challenges in automated code generation for deep learning, facilitating more effective debugging, enhancing the quality of generated code, and ultimately contributing to the development of more robust and efficient deep learning models.


\begin{tcolorbox}[boxrule=0.5pt, colback=gray!10,  arc=4pt,left=3pt,right=3pt,top=3pt,bottom=3pt,boxsep=0pt
]
\textbf{Observation 1:} \textit{Misinterpretation} is a common issue in both generated general code and DL code, however, due to more complex logic and arithmetic requirements, LLMs are more likely to make this mistake when generating DL code. \textit{Errors in arithmetic and logical operations}, \textit{performance}, and \textit{prompt missing information} emerged as new issues that are specific to DL code generation.
\end{tcolorbox}


\input{figs/discussion3}
\newpage
\textbf{Bugs in Human-Written vs. LLM-Generated DL Code:} On one hand, prior study~\cite{islam2019comprehensive} identified the most common types of bugs in human-written DL code which include logic errors, API misuse, and data-related issues. Among these, API misuse is the most prevalent bug pattern in human-written DL code when using TensorFlow, whereas data flow bugs are more common when using PyTorch. On the other hand, according to our analysis of LLM-generated DL code, although API misuse remains a frequent issue, data structural problems, such as tensor mismatches and dimensional errors, occupy more frequently. Figure~\ref{fig:discussion3} highlights an instance of dimensional mismatches in LLM-generated DL code. In this case, GPT-4o incorrectly assumes that each shift value can be applied directly to all pixels in the image channel, causing a shape mismatch.

%larger proportion of bugs. As seen in Table~\ref{tab:taxonomy}, these structural issues surpass other bug types in LLM-generated code. This trend is likely due to LLMs' inherent weaknesses in understanding shapes and numerical relationships, a limitation that occurs less frequently in human-generated code. 

\input{figs/discussion2}
%While there are meaningful differences between humans' and LLMs' code, notable similarities in bug patterns also exist. 
Similar to prior findings~\cite{islam2019comprehensive} of human-written DL code, LLM-generated DL code often contains  
%shows that a considerable proportion of bugs in human-written DL code are 
logic errors.
This similarity may stem from the fact that LLMs are trained on human-written code, thereby inheriting logical structures and concepts from human programmers. An example of such logic-related bugs is shown in Figure~\ref{fig:discussion2}, demonstrating how LLMs replicate logical reasoning errors that occur in human-written code. Here, GPT-4o applies \textit{scale\_x} only to the cosine whereas the scaling factors \textit{scale\_x} and \textit{scale\_y} should be applied uniformly to both the sine and cosine components of the rotation matrix. This results in improper scaling along the axes and triggers a test failure.

\input{figs/discussion1}
Additionally, API misuse is a common bug pattern occurred in both human-written and LLM-generated DL code.
%sources of code.
Figure~\ref{fig:discussion1} provides an example of API misuse in LLM-generated code where GPT-4o attempts to call \textit{torch.idct}, which is not implemented in PyTorch. One possible fix is to provide more context concerning third-party libraries. For example, one could hint to LLMs to use \textit{scipy} instead, resulting in \textit{scipy.fftpack.idct(x.numpy(), norm=norm)} instead.

\begin{tcolorbox}[boxrule=0.5pt, colback=gray!10,  arc=4pt,left=3pt,right=3pt,top=3pt,bottom=3pt,boxsep=0pt
]
\textbf{Observation 2:} Unlike human-written DL code, LLM-generated DL code contains more data structural problems, such as tensor mismatches and dimensional errors. 
Similar to prior findings of human-written DL code, LLM-generated DL code often contains logic errors.
Additionally, API misuse frequently occurs as a bug pattern in both human-written and LLM-generated DL code.
These overlaps suggest that while LLMs exhibit unique weaknesses, their reliance on human-generated training data also leads to shared bug patterns, particularly in logic and API misuse errors.

\end{tcolorbox}

%\subsection{Incorrect Usage of Third-Party Libraries}

%Third-party libraries (e.g., PyTorch, TensorFlow, and NumPy) are essential to writing ML/DL code as oftentimes it is not effective and efficient to write everything from scratch, especially with so many available popular ML/DL APIs. However, as illustrated in Fig~\ref{fig:discussion1}, LLMs sometimes fail to use these libraries correctly. In this case, the model attempts to call \textit{torch.idct}, which is not implemented in PyTorch. One possible fix is to provide more context concerning third-party libraries. For example, one could provide a hint to LLMs to use \textit{scipy} instead which would result in the use of \textit{scipy.fftpack.idct(x.numpy(), norm=norm)} instead.

%This benchmark is designed for ML and DL-related code, where the ground truth often relies on third-party libraries like PyTorch, TensorFlow, and NumPy. LLMs also utilize these libraries in their generated code. However, as illustrated in Fig~\ref{fig:discussion1}, LLMs sometimes fail to use these libraries correctly. For example, the model attempts to call \textbf{torch.idct}, which is not implemented in PyTorch. A simple fix would be to use \textbf{scipy.fftpack.idct(x.numpy(), norm=norm)} instead. Retrieval-augmented generation can improve LLM understanding of available APIs and help prevent such issues.


%\subsection{Incorrect Operations on Input Parameters}

%Not only do the API calls need to be correct, but the input parameters also need to be appropriately processed. For example, LLMs sometimes perform incorrect operations on input parameters. For example, in Fig~\ref{fig:discussion2}, GPT-4o applies \textbf{scale\_x} only to the cosine whereas the scaling factors \textbf{scale\_x} and \textbf{scale\_y} should be applied uniformly to both the sine and cosine components of the rotation matrix. This results in improper scaling along the axes and triggers a test failure.

%Even when LLMs use the correct APIs, they may perform the wrong operations on input parameters. Fig~\ref{fig:discussion2} demonstrates this type of error. In this example, the scaling factors \textbf{scale\_x} and \textbf{scale\_y} should be applied uniformly to both the sine and cosine components of the rotation matrix. However, the LLM incorrectly applies \textbf{scale\_x} only to the cosine and sine terms in the first column (x-axis) and \textbf{scale\_y} only to the sine and cosine terms in the second column (y-axis). This results in improper scaling along the axes.


%\subsection{Incorrect Assumptions About Input and Output Shapes}
%Input/output shapes are also hard for LLMs to handle. Specifically, as shown in Fig~\ref{fig:discussion3}, when shifting values \textit{(r\_shift, g\_shift, b\_shift}) are one-dimensional tensors with shape (N) they need to be broadcastable to the image channels with shape (N, H, W) in order for the \textit{+=} operation to work. The GPT-4o incorrectly assumes that each shift value can be applied directly to all pixels in the image channel, causing a shape mismatch. Additional few-steps learning specifically focused on shape correction would help in cases where the shift values are reshaped to (N, 1, 1) using \textit{.view(N, 1, 1).}

%\input{figs/example6}
%\input{figs/example9}