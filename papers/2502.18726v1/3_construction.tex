\section{Benchmark Construction}
\label{sec:BenchConstr}

\input{figs/pipeline}
\input{figs/codeprompt}
%\subsection{Overview}

\tool consists of 520 instances of AI and DL data points (filtered from over 2,000 raw data points). The data is curated from 30 GitHub repositories (selected from an initial pool of 160 related repositories).

%In this section, we will discuss the step-by-step process of constructing \tool as demonstrated in Figure~\ref{fig:pipeline}. 
The construction process of \tool consists of two main phases: The Raw Data Extraction and the Labeling Procedure. The raw data extraction involves six semi-automatic steps. Since \tool is designed to have diverse and realistic code samples, the first step \circled{1} is to construct \tool from code crawled from highly rated GitHub repositories (i.e., with the most stars) filtered using 30 DL-related terms such as ``neural-networks'',  ``pytorch'', ``computer-vision''. We then manually select (step \circled{2}) 160 high quality candidate DL projects 
%that are considered DL related 
%to make DL benchmark in code generation, as they 
(i.e., involve the integration of DL and AI-related frameworks, comprehensive test cases, clear and well-written docstrings, and detailed contribution guidelines). We then employed a bespoke utility to extract the test files and then test cases from each repository (step \circled{3} and \circled{4}). By performing static analysis, we were able to track and collect all of the functions under test in step \circled{5} to form the raw data that is the base of \tool. 

Once the raw data is extracted, the labeling procedure starts. To speed up the task of constructing the prompt for each code sample, we utilize LLM (i.e., GPT-4o) as a code-explanation tool\cite{nam2024using} to generate the first prompt candidate for each function under test (step \circled{7}). Four co-authors were then tasked with manually filtering each entry to ensure that each function is highly relevant (i.e., contributes to a DL task such as image recognition, utilizes at least one recognized DL framework, and implements a relatively advanced and sophisticated algorithm).
Finally, we conduct a manual labeling process involving four co-authors (step \circled{9}) to refine the prompt and label each code sample with the appropriate category from our three chosen types of categories: DL pipeline phases, ML task types, and input types.


% (melika) It's better to explain how you generate the prompts. You just mentioned that they are generated automatically.
%In summary, the steps needed for building \tool is as follows:
%\begin{enumerate}
%    \item We gathered 30 different GitHub tags related to deep learning, like ``neural-networks'' (details in Section~\ref{sec:rs}).
%    \item We crawled GitHub and extracted related repositories with those tags, filtering for those that were more recently updated and had more than 400 stars (details in Section~\ref{sec:rs}).
%    \item We manually cherry-picked 160 repositories for more detailed data extraction and cloned them (details in Section~\ref{sec:rs}).
%    \item We extracted test cases within each repository (details in Section~\ref{sec:fe}).
%    \item We then extracted functions that are called in the test suites, ensuring they were not third-party functions (details in Section~\ref{sec:fe}).
%    \item We crawled each repository to extract the function definitions, forming the first version of our raw data (details in Section~\ref{sec:fe}).
%    \item We used code-explanation tools, such as LLMs, to generate prompts based on the function definitions (details in Section~\ref{sec:pg}).
%    \item We manually checked each function definition and rated them for relevance to deep learning, involving 2-3 people in the review process (details in Section~\ref{sec:rd}).
%    \item Finally, we manually checked and modified each prompt, integrating the generated prompts with doc-strings within each function to ensure clarity and accuracy (details in Section~\ref{sec:lp}).
%\end{enumerate}
%These steps are demonstrated in Fig.~\ref{fig:pipeline}. In the following sections, we will provide a more detailed explanation of each step.

\subsection{Raw Data Extraction}

This phase consists of six semi-automatic steps that crawl data from GitHub repositories to generate a list of function definitions and their test cases.

%\subsubsection{}
%\label{sec:rs}

%One of the characteristics of \tool is the source of its origin. 
%Unlike other datasets, which may draw from limited or outdated sources, 
%We curated our data from the most starred DL-related GitHub repositories
%~\cite{shrivastava2023repofusion, jimenez2023swe}.
%ensuring
%that our dataset contains 
%the inclusion of high-quality and widely used Dl-related functions.
%that reflect current trends and best practices in DL. To do this, we follow two steps. 

\textbf{Repository Selection:} We curated our data from the top 1000 starred DL-related GitHub repositories to include high-quality and widely used DL-related functions.

In step \circled{1}, we filtered GitHub projects with one of 30 DL-related tags such as ``neural-networks'',  ``pytorch'', and ``computer-vision'' (we provided the complete list of tags in our repository). 
%To select the most rated project, we select our candidate to be in the top 1000 projects ranked by the number of stars.
%Using this amount of tags ensures that the tags remain highly relevant and focused on popular, well-defined DL subdomains.
%Also, using too many tags can introduce excessive complexity, diluting the dataset with less significant projects~\cite{xiong2017mining}. 
Specifically, we select the tags
%selection, we 
by collecting from DL and AI-related GitHub repositories and filtering the most relevant ones to get the final 30.
%related to AI and deep learning, collect their tags, and identify common tags across projects. From these, we select 30 final tags for future use.

In step \circled{2}, we select 160 most relevant projects for \tool and
%. Specifically, from the 1000 candidates, we review each project and
retain only projects that: 1) are DL related (i.e., use DL libraries, or perform DL tasks like segmentation or detection), 2) have sufficient test cases (averaging at least three per function), and 3) include thorough documentation, such as source code docstrings or README files. 
%This ensures our selected repositories are relevant, well-documented, and actively maintained, providing reliable ground-truth code for our benchmark \cite{almarzouq2020mining}. 
%In the end, we select 160 projects, focusing on repositories updated after GPT-4o release to minimize data leakage.

%The diversity and complexity of the functions in these repositories provide a richer and more realistic testing ground for evaluating code generation models. Additionally, \tool targets function-level data. Function-level code presents a more complex and generalizable challenge for code generation models, as it requires handling the intricacies of specific ML/DL tasks without the simplicity of complete script generation. This is in contrary to other datasets (e.g., DS-1000~\cite{lai2023ds}), which focuses on testing the ability of models to generate scripts. This shift from script-level to function-level testing enables a more granular evaluation of how well models can generate individual components of DL systems, offering deeper insights into their strengths and limitations in different stages of the development of such systems.

%For gathering our data, we begin by collecting relevant GitHub repositories~\cite{kalliamvakou2016depth} related to machine learning and deep learning. Each repository is manually evaluated to ensure relevancy and quality, and to verify that they include useful test cases for our analysis~\cite{dabic2021sampling}. Through this process, we identified and filtered 160 repositories that are highly relevant to machine learning and deep learning, ensuring that they are suitable for further investigation and use in building our dataset. Next, we focused on extracting test cases from these repositories~\cite{madeja2021automating, tufano2022methods2test}. We crawled through the repositories to find all available test cases, which were then analyzed to identify the specific functions each test case was designed to test. This step allowed us to map each test case to its corresponding function, ensuring that we had clear associations between the test cases and the code they were testing. This mapping is crucial for understanding how code generation models should approach specific functions in the deep learning pipeline~\cite{tian2023test}. Once we had this mapping, we generated prompts for each function. These prompts were generated automatically by utilizing the code and the doc-strings within the code~\cite{yu2024codereval}. We manually evaluated each prompt to ensure that it provided sufficient information to cover different aspects of the corresponding function. This step involved carefully reviewing the prompts to make sure they captured key elements of the code, such as input parameters, functionality, and context. We aimed to generate prompts that would fully represent the complexities of the functions they were tied to. Following the prompt generation, four independent reviewers examined each instance and categorized them based on several key dimensions: relevancy, the specific step in the deep learning pipeline (i.e., pre-processing, training, model-construction, evaluation, and inference), data type (i.e., text, image, structured data), and task (i.e., classification, detection, segmentation, regression, recommendation, and prediction). This categorization allowed us to better understand the nature of the dataset and its relevance to different aspects of machine learning and deep learning~\cite{fredriksson2020data}.

%To establish a robust benchmark for AI, it is imperative to source high-quality ground-truth codes related to machine learning, alongside test cases that these ground-truths can successfully pass. Addressing this necessity, we propose leveraging well-known GitHub repositories as the basis for our ground truth. As referenced in step 1 of Fig.~\ref{fig:pipeline}, We utilized 30 different tags related to AI, such as``NLP'', ``convolutional-neural-networks'',and ``PyTorch''. Subsequently, we crawled GitHub and extracted repository links that met our criteria: having at least 400 stars and being recently updated. The rationale behind selecting repositories with a minimum of 400 stars is that they are more likely to be recognized for their reliable and high-quality code, widely utilized by the developer community. This ensures that we are relying on well-vetted and reputable sources for our ground-truth code. 

%Moreover, focusing on recently updated repositories helps address the concern of language models potentially memorizing existing code. By using more recent code, we reduce the likelihood that these repositories have been extensively seen or used in the training data of current LLMs. This approach not only enhances the relevance and reliability of the benchmark but also mitigates the issue of overfitting or memorization by the models being evaluated. In summary, our method involves a careful selection process to ensure that the ground-truth codes and test cases used for benchmarking deep learnin are both highly reliable and up-to-date, providing a robust foundation for evaluating in the performance of deep learning models.

%By this method, we initially extracted 1000 repositories that met our specified conditions. Subsequently, As indicated by entry 2 in Fig ~ \ref{fig:pipeline}, we manually reviewed these repositories and cherry-picked 160 different repositories As referenced by entry 3 in Fig ~ \ref{fig:pipeline}. During this process, we assessed each repository to determine how closely their projects were related to deep learning. We removed repositories that did not provide sufficient test cases or that were not directly related to using deep learning codes but rather built tools that were not sufficient for our purpose. Additionally, repositories that lacked comprehensive documentation, had sparse code comments, or showed minimal community engagement were also excluded. By rigorously filtering out such repositories, we ensured that our selected repositories are highly relevant and robust, providing a reliable source of ground-truth code for our benchmark. This careful selection process guarantees that the repositories used in our benchmark are well-documented, widely recognized, and actively maintained, thereby enhancing the credibility and utility of our benchmark for evaluating deep learning models.

%In selecting repositories for our study, we carefully applied the following criteria to ensure the relevance, quality, and utility of the dataset:
%\begin{enumerate}
%    \item \textbf{Presence of Test Cases:} 
        %We evaluated whether the repository includes test cases, as repositories with test cases are preferred due to their usefulness for applying metrics in future studies. The presence of test cases reflects a mature and robust codebase, making it a more reliable source for evaluating software metrics. Repositories with comprehensive test coverage should be prioritized to ensure more accurate assessments and benchmarks in future research.

    %\item \textbf{Use of Deep Learning or deep learning-related Libraries:}
    %We assessed whether the repository uses deep learning or deep learning-related libraries, as selecting repositories with deep learning pipelines enhances our dataset with complex, relevant examples. This approach aligns with current trends in machine learning and deep learning research. Focus on repositories that significantly utilize popular AI frameworks like TensorFlow, PyTorch,or JAX as they likely contain advanced code structures, contributing to a diverse and challenging dataset.

    %\item \textbf{Strength of Documentation for Contribution:}
        %We evaluated the quality of the repository's contribution guidelines and documentation, as strong documentation is essential for simplifying the process of rebuilding and rerunning test cases. Well-documented repositories minimize ambiguity and support smoother collaboration and experimentation. Prioritize repositories with clear, comprehensive contribution guidelines, including detailed instructions for environment setup, running tests, and understanding the codebase, ensuring they are accessible and usable for future research.

    %\item \textbf{Use of Docstrings for Functions:}
    %\begin{itemize}
     %   We checked whether functions in the repository are documented with docstrings, as they provide valuable information for prompt generation. Docstrings not only improve documentation quality but also offer contextual data that can enhance input generation for models, improving dataset usability. Prioritize repositories where functions have informative and descriptive docstrings, as this aids in understanding the code and supports better prompt generation for future research.
    %\end{itemize}
%\end{enumerate}

%\subsubsection{Function Extraction}
%\label{sec:fe}
%After compiling a list of suitable repositories, we cloned them for subsequent analysis and extraction. 

\textbf{Function Extraction:} One of the main design choices of \tool is to include a set of reliable and robust test cases for each benchmark entry. This is because programming languages are different from natural languages. Specifically, generated code can fulfill all of the functional requirements but could have a low BLEU score when compared with the ground truth code\cite{tran2019does}. This means that using text similarity metrics such as BLEU score as evaluation metrics is not the best method to evaluate code generation techniques. Instead, test cases (functional and non-functional) passing rate should be used to reliably access a new code generation approach.

In step \circled{3}, we crawled selected repositories for test files using standard test file name patterns such as tests/test\_file\_name.py~\cite{madeja2021automating}. In step \circled{4}, for each test file, we extract test cases using common patterns in Python test suites, such as the @pytest decorator.
%indicating the following function is a test case.

%to extract the functions contained therein 
% As highlighted in point \circled{5} of Fig.~\ref{fig:pipeline}. 
Once we identified all test cases, in step \circled{5}, we performed call graph analysis to track and collect all functions under test (excluding third-party function calls). The definitions of each of those functions are then extracted in step \circled{6} to form the bases for our ground-truth code samples.

%We then analyzed each function to identify the function calls they made, specifically excluding third-party function calls to focus solely on the internal functions within the repository. 

%Following this, we performed a comprehensive crawl of each repository to extract the definitions of these internal functions As show in point \circled{6} of Fig.~\ref{fig:pipeline}. Each function definition was meticulously extracted and saved for further analysis. 
%This process ensures that we have a detailed and accurate representation of the code and its functionality, which is crucial for the development of a robust deep learning benchmark.

\subsection{Labeling Procedure}

The labeling procedure involves three semi-automatic steps to generate and refine a prompt and assign categorizations for each entry in our \tool dataset. To determine the best procedure and criteria for our manual process, we perform a small trial run of the manual process on a small sample of the data points. In this trial run, we ask each reviewer to provide feedback on the labeling criteria so that when we start our full run we have
%can improve the final criteria for the full run.
%used for our manual process are 
the most comprehensive and accurate manual process possible.

\textbf{Prompt Generation:} 
%Since the \tool is designed to be a benchmark for code generation with LLM, one of the most important components is the prompts associated with the collected source code.
In step \circled{7}, we utilize two sources of data to create the code generation prompts: 1) the doc-strings provided by developers, which describe the functionality and parameters of the code, and 2) the function definitions themselves, which can be used to generate candidate prompts. Specifically, We take advantage of the function definitions to explain the code, and by combining them with their respective doc-strings (when available), we generate the initial candidate prompt by querying GPT-4o with the template as described in Fig.~\ref{fig:code_prompt}.

% ``\todo{The query to generate prompt from doc string and source code ...}''. 

%As noted in step \circled{7} of Fig.~\ref{fig:pipeline}, after extracting the function definitions, we have two sources of data to use as prompts: 1) the doc-strings provided by developers, which describe the functionality and parameters of the code, and 2) the function definitions themselves, which can be used to generate prompts. 

%We utilize the function definitions to explain the code, and by combining them with their respective doc-strings (when available), we generate initial prompts. 


However, generated prompts require manual validation to ensure accuracy and relevance. This review process is essential to refine prompts and guarantee quality for subsequent use~\cite{shrivastava2023repository}. We further refine prompts based on the following criteria: (1) contain clear, sufficient information for code generation, (2) specify input and output format, and (3) cover error handling and boundary conditions. More details are in the appendix.

%\subsection{Revising prompts}
%\label{sec:rp}
%In the final step
%After completing the initial round of data selection and labelling, we conduct a thorough review of each instance by re-examining each prompt and its corresponding ground truth. This meticulous process ensures that every prompt is capable of addressing various components of the function, including essential sub-functions, potential ``ValueError'' and other exceptions, and whether the prompt is comprehensible to an expert tasked with generating the code. Below is a detailed list of the key areas we scrutinize during this review:
%\begin{enumerate}
%    \item \textbf{Comprehensive Coverage of Code Aspects:}
%   The objective is to ensure that the prompt comprehensively addresses the various boundaries and aspects of the code. We examine whether the prompt effectively captures the critical elements of the function, such as conditional logic (e.g., ``if'' statements) that dictate execution flow based on input values. This ensures that all relevant decision points and branches are covered for accurate code generation. Reviewers should focus on control flow elements like loops, conditional statements, and function calls, confirming that these are clearly and logically integrated into the prompt.
%    \item \textbf{Handling of Errors and Exceptions:}
%    The objective is to verify that the prompt includes references to the function's error-handling mechanisms. We specifically check whether the prompt accounts for handling exceptions such as ``ValueError'', ``TypeError'', or other domain-specific exceptions that the function might raise. Including these scenarios ensures that the generated code can handle unexpected inputs and edge cases, making it robust for real-world usage. Reviewers should examine the function's error-handling logic and ensure the prompt explicitly mentions these errors, clearly describing the conditions under which they may occur for accurate code generation.
%    \item \textbf{Human Understandability:}
%        The objective is to assess the clarity and comprehensibility of the prompt for a human expert. We evaluate whether the prompt is articulated in clear, unambiguous language that provides enough detail for a subject matter expert to generate the corresponding code without needing additional context. The prompt should balance being informative and concise, allowing the expert to focus on the task without unnecessary complexity. Reviewers should ensure the prompt conveys the function’s purpose and requirements without leaving key details open to interpretation.
%\end{enumerate}

If the prompt does not meet the
%pass any of the 
mentioned criteria, the annotators propose and agree on changes that 
%to the prompt to 
bring it up to the expected quality.
%level.
This reviewing process 
%helps us to 
produces prompts that are 
not only 
technically correct 
but also include details essential to
%practically useful, fostering better outcomes in 
code generation.
%tasks.

\textbf{Data Filtering and Validation:} After compiling all the data (i.e., the ground truth, test cases, and candidate prompts), in step \circled{8}, we manually evaluate each function meticulously, reading and modifying the prompts following a set of criteria. Specifically, we discard general codes (e.g., those for reading text files) that are not DL related.
%to ensure we retain only deep learning-related codes. 
%This thorough evaluation and labelling process ensures the relevance and quality of the data for building a robust DL benchmark.
%For each instance in our dataset, a team of three reviewers thoroughly examines its usability and relevance for inclusion in our benchmark. This collaborative review process ensures that each instance meets our standards for data quality and applicability. If the instance is deemed acceptable, the reviewers then label the data according to several specific criteria. The process is broken down as follows:
%\subsubsection{Review the Applicability and Data Validation}
%\label{sec:rd}
In this step, the annotators independently assess the
%validity of the generated prompts based on whether they provide enough information to enable a successful code generation. 
%This involves a detailed evaluation of the 
prompt’s clarity, relevance to DL-related tasks, and overall usability 
%The annotators 
with the following
%provided 
criteria: (1) serving key DL tasks, (2) utilization of popular DL frameworks, and (3) algorithms' relevancy and clarity.

\textbf{Labeling:} In step \circled{9}, we assign labels for each data point based on the role of the function in the ML pipeline (e.g., pre/post-processing, model construction), the ML tasks (e.g., classification, regression) it solves, and types of data (e.g., image, text) it operates on. For each data point, three co-authors thoroughly analyze and assign appropriate labels. We use a majority vote to finalize the labels and modify the prompts accordingly. Specifically, we assign the following labels when appropriate to each data point: Stage in the ML pipeline, ML task type, and Input data type.





%\subsubsection{Consensus and final label}
%\label{sec:cfl}
Once each reviewer completes their assessments, the team meets to discuss any discrepancies and reach a consensus on the final labels. Due to our detailed instructions and guidelines, we achieve a high inter-rater reliability of 0.83 measured by Krippendorff's alpha~\cite{zapf2016measuring}(measures of more than 0.8 indicating strong agreement.
%among annotators. 
% \add{However, there are still instances of disagreement, during which the team deliberates to ensure the final labels accurately reflect the code's purpose, structure, and usability, ultimately resolving the issue through a majority vote among the reviewers.}

%\subsubsection{Documentation and Archiving}
%\label{sec:doc}
The labeled data is carefully documented, including notes on the decision-making process for transparency and future reference. Instances are organized, with labels to ensure easy retrieval and analysis in later stages of research. To enable easier benchmark utilization (i.e., running test cases), the relevant projects are set up in virtual environments along with appropriate dependencies and ready-to-run testing scripts.

This rigorous review and labeling process ensures that each instance in the dataset is not only relevant and useful but also thoroughly understood and appropriately categorized, contributing to a robust and reliable benchmark. %for DL and AI research. 
%Our dataset is available for use on GitHub.





