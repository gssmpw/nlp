\section{Evaluation}

To demonstrate the potential of \tool and the depth of analysis that can be done on \tool, we perform a preliminary study evaluating the performance of state-of-the-art LLM on the \tool benchmark. We choose the top LLMs: 
%best paid---
GPT-4o, 
%, as a representative of the most advanced, best %conversational model  ---
Claude3.5 sonnet, 
%best open-source---
Llama 3.1 70B, and 
%best small---
Mistral 7B.
The LLMs are evaluated based on the commonly used pass@k, which measures the likelihood that at least one of the k-generated solutions passes all test cases~\cite{lyu2024top}. 
%It's a common metric for evaluating code generation models, indicating the proportion of successful executions among the top k attempts.

%We conducted experiments using our benchmark dataset with the state-of-the-art LLM models, GPT-4o, Claude3.5 sonet, LLama 3.1 70B, and Mistral 7B. For each test instance in the benchmark, we used the code generated by LLM to replace the ground-truth code and executed the relevant unit tests on the newly generated code. By doing this, we were able to assess the performance and correctness of the generated code in a real-world testing scenario. 

%\subsection{Experimental Settings}
To minimize non-determinism and improve reproducibility, we set the temperature to zero for LLMs \cite{bommasani2021opportunities}. This allowed us to accurately measure the %accuracy and %
quality of the generated code across various DL tasks, offering deeper insights into the LLM's strengths and weaknesses.

For each project, \tool provides 
%we created 
a
%isolated 
%virtual environment (
docker image that includes all required dependencies.
%to minimize package conflicts. 
These images
%then built the environments from scratch to 
ensure that test cases are runnable and the evaluations are
%. This approach makes the projects 
easily replicable on other systems.

%To ensure consistent and repeatable results, we set the model's temperature to 0, minimizing randomness and reducing the non-deterministic nature of the LLM’s outputs. This approach allowed us to evaluate the generated code in a controlled environment and reliably compare it against the benchmark’s expected outcomes. By carefully checking the results of the test cases, we measured the accuracy and quality of the code generated by LLM across various DL tasks, providing deeper insights into the LLM's strengths and weaknesses in code generation.

%\subsection{Research Questions}
%\label{sec:RQ}
%To systematically analyze the applicability of \tool, we investigate the following research questions:

%\begin{itemize}

%\item
%\textbf{RQ1---What are the performances of state-of-the-art (SOTA) LLMs on DL/ML code generation tasks?}
%This RQ aims to analyze how well various SOTA LLMs such as GPT-4o, Claude 3.5 sonet, LLama 3.1, and Mistral 7B perform on ML/DL code generation benchmarks (i.e., prior benchmark DS-1000 and our proposed \tool). For samples in \tool, we also analyzed the typical errors and test failures that generated code triggered. Such analysis provides insight into pitfalls that LLMs often make when performing DL-specific code generation.

%\item
%\textbf{RQ2---Which stages in the ML pipeline pose a greater challenge for SOTA LLMs?}
%In this RQ, we assess if code in different stages of the ML pipeline is harder or easier for the SOTA LLMs to generate. This analysis is made possible by our categorization of ML stages (i.e., pre/post-processing, model construction, training, inference, and evaluation \& metrics). Such metadata provides an opportunity to perform deeper analyses of the LLMs DL code generation performance.


%\item
%\textbf{RQ3---Are different ML task types easier or harder to generate code for?} 
%Different ML tasks present different coding challenges and this RQ aims to demonstrate our ML task categorization enables deeper analyses. E.g., by investigating the complexity of each type and examining how the LLMs perform across these diverse categories, we aim to demonstrate \tool's ability in providing valuable insights into areas where LLMs excel and where further improvements are needed.


%\item
%\textbf{RQ4---Do the various required input data types have any effect on how LLMs generate DL code?}
%This RQ aims to investigate the performance of LLMs across different input types, including image, text, and tabular data. Specifically, we evaluate how effectively LLMs process and analyze each specified type of input data and, in the process, identify strengths and weaknesses in their ability to handle diverse data formats.

%\end{itemize}
