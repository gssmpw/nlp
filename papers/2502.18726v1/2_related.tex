\section{Related Works}

\textbf{Code Generation for ML software}: Shin et. al.\cite{shin2023good} have explored the effectiveness of neural code generation models on ML tasks, which differ significantly from general programming tasks. A key study evaluated six state-of-the-art models across popular ML libraries, highlighting the need for domain-specific benchmarks and improvements tailored to the complexities of ML. Our work provides such benchmarks specifically for ML and DL software.
%, with reliable test cases and a structured approach to categorizing and evaluating different instances, offering a more focused assessment of code generation in these domains.

Recently, DS-1000\cite{lai2023ds} is the only benchmark designed for data science code generation. It contains 1000 problems sourced from 
%real-world scenarios like 
StackOverflow with
%It spans seven Python libraries, offering 
diverse, practical tasks. Similarly, MLE-bench~\cite{chan2024mle} targets evaluating ML engineering workflows in Kaggle competitions.
%that reflect everyday coding challenges. 
However, \tool differs in key aspects: 1) it focuses on ML/DL tasks rather than general data science and MLE, 2) we categorize the data by ML phases, task types, and data types, and 3) our granularity is at the function level rather than script or workflow level. Additionally, unlike the other datasets, our dataset is based on GitHub repositories with real code.

\textbf{Code Generation Benchmarks: } There have been multiple code generation benchmarks as shown in Table~\ref{tabs:other_benchmarks}.
Among them, HumanEval~\cite{chen2021evaluating} is the most popular
%for evaluating functional correctness in Python code generated from docstrings, 
with 164 hand-crafted programming problems.
Building on this foundation, AiXBench\cite{hao2022aixbench} extends the evaluation to Java, and MultiPL-E\cite{cassano2022multipl} expands
%upon HumanEval's scope 
by supporting 18 different programming languages.
%providing 175 automated samples and 161 manually assessed ones, introducing a new metric to measure code correctness. Meanwhile, MultiPL-E\cite{cassano2022multipl} expands upon HumanEval's scope by supporting 18 different programming languages, translating unit tests, doctests, and terminology to facilitate a broader multi-language evaluation for code generation.
Another prominent benchmark is MBPP~\cite{austin2021program} which offers 974 programming tasks solvable by entry-level programmers.
Additionally, the Spider benchmark~\cite{yu2018spider} provides 10,000 questions paired with 5,000 SQL queries. 
CoderEval~\cite{yu2024codereval} and APPS benchmark~\cite{hendrycks2measuring} assess code generation models in real-world programming scenarios in Python and Java with coding problems ranging from beginner to advanced competitive programming challenges. RepoEval~\cite{zhang2023repocoder}, meanwhile, assesses LLMs for repository-level code generation. All the above-mentioned benchmarks focus on general programming, unlike \tool, which focuses on DL/ML code generation problems.

%evaluating models' abilities to generate SQL queries from natural language, featuring over 10,000 questions paired with 5,000 SQL queries across 200 databases. Spider's emphasis on generalizing to unseen databases distinguishes it from other datasets. CoderEval\cite{yu2024codereval}, another benchmark, assesses code generation models in real-world programming scenarios, highlighting both standalone and integrated functions, especially in Python and Java. The APPS benchmark\cite{hendrycks2measuring}, with its 10,000 coding problems ranging from beginner to advanced competitive programming challenges, pushes models to generate syntactically correct and functionally sound code, testing their capacity to solve complex algorithmic tasks.

