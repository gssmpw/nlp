\appendix
\onecolumn

\noindent {\Large  \textbf{Appendix}}

Table of Contents: 
\begin{itemize}
    \item Appendix A: Dataset Statistics
    \item Appendix B: Candidate Prompt Filtering Criteria
    \item Appendix C: Final Data Filtering and Validation Criteria
    \item Appendix D: Data Categories
    \item Appendix E: LLM Bug Types and DL-specific Subtypes
    \item Appendix F: Distribution of Failures in Generated DL Code
    
\end{itemize}

%This appendix provides supplementary details on the methodology used for prompt filtering, dataset validation, labeling criteria, and taxonomy construction in our study. The sections included here outline the steps taken to ensure the clarity, completeness, and reliability of prompts, as well as the categorization framework for analyzing the generated DL code. By structuring the dataset with a well-defined taxonomy, we enhance the evaluation of code generation models across different ML pipeline stages, task types, and input data categories.

%We begin by detailing the prompt filtering process, which ensures that prompts contain essential information, specify input-output formats and account for error handling and boundary conditions. Next, we describe the criteria used to validate data relevance, particularly in alignment with key DL tasks and frameworks. Following this, we present the labeling strategy applied to categorize functions based on their role within the ML pipeline, their ML task type, and the nature of their input data. Lastly, we provide a comprehensive breakdown of failure types observed in generated code, including both general programming errors and DL-specific issues such as incorrect framework usage, tensor mismatches, and computational inefficiencies.

%The taxonomy and dataset statistics presented in this appendix serve as a foundation for benchmarking automated code generation models, offering structured insights into model performance and common failure patterns. These details supplement the main findings of our study, providing a more granular view of the dataset’s construction and evaluation methodology.

\section{Dataset Statistics}

% \input{pics/distribution}
%\input{tabs/statistics}
% \input{tabs/Datatype}
% \input{tabs/Tasks}

\input{figs/statistics_pie}

\tool consists of 520 instances of AI and DL data points (filtered from over 2,000 raw data points). The data is curated from 30 GitHub repositories (selected from an initial pool of 160 related repositories).
%a pool of \hung{...} related GitHub repositories.
%, meticulously gathered from an initial pool of over related repositories. 
%Following a thorough manual analysis, this pool was refined down to 160 repositories, which were further evaluated for quality and relevance. After multiple rounds of detailed evaluation and data selection, we finalized our dataset, which originates from 40 high-quality repositories.
%As mentioned in Section~\ref{sec:BenchConstr}, to ensure the accuracy and consistency of the labeling process, four separate annotators were involved.
%Each instance was analyzed and labeled independently by at least three annotators and a majority voting system was used to minimize bias. 
%The final dataset was carefully selected from a total of over 2,000 data points, ensuring that only the most relevant and accurate data was included for further analysis.
%The multi-step process of curation and labelling underscores the rigorous methodology employed in creating this high-quality dataset.
To ensure an accurate evaluation of code generation techniques under test,
%\tool was designed to gather test cases to evaluate the system's performance rigorously. 
each prompt instance in \tool is accompanied by
%, we utilized on average 
at least three test cases (six test cases on average).
%available within the corresponding GitHub repositories, ensuring that these test cases successfully passed when executed on the ground truth function. 
%This approach enhances the accuracy of our evaluation and allows for a more automated and consistent assessment process.
%The process for generating the dataset is illustrated with an example in Figure \ref{fig:pipeline}. Initially, we applied a filtering mechanism based on specific tags, such as ``deep-learning''. As an example, this filtering process returned the ``Kornia'' repository. Following this, we performed a targeted crawl of the repository, focusing on files related to test suites. Within these files, we extracted the functions being tested. We then expanded the crawl to cover the entire repository, enabling us to locate the definitions of these functions.
%Once the function definitions were identified, we undertook a manual labeling process. Each function was classified into different categories based on specific criteria. This involved organizing the data according to three key categories: the stage within the AI pipeline, the data type being used, and the general task the function addresses.
%As a result of this comprehensive process, we constructed a dataset containing 520 distinct entries. Each entry, on average, is associated with six test cases. The dataset was then meticulously categorized into the aforementioned labels, providing a structured and detailed representation of the data.
One of \tool's contributions is the categories that we assign for each data point. As mentioned in Section~\ref{sec:BenchConstr}, each data point is assigned a label for which stage of the ML pipeline it belongs to, a label for which ML task it helps solve, and a label for the type of input data. This information enables users of our benchmark to perform an in-depth analysis of their proposed technique with respect to multiple ML-specific aspects. We demonstrate this in our empirical study presented in Section~\ref{sec:study} later.

\input{figs/processing}
\input{figs/model_construction}

Fig~\ref{fig:statistics} represents the distribution of \tool's data in each categorization. In terms of the stages in the ML pipeline (Fig~\subref{fig:statistics_step}), our dataset well covers the five stages of the ML pipeline with the pre/post-processing stage having the most (210) representative samples. Fig~\ref{fig:preproprocessing} lists the prompt to generate 
a pre/post-processing ``draw\_point2d'' function that can be used to 
%modify output images after inference for purposes such as 
highlight key points of interest in output images. The model construction stage contains the second-most (119) samples such as the one shown in Fig~\ref{fig:model_construction}. This example shows the prompt to generate the
%initialization 
``\_\_init\_\_'' method for a fully connected neural network (FCNN). Other ML stages have an equal share of samples. This indicates a balanced dataset that covers all ML stages.

%Based on Table \ref{tabs:statistics}, we observed that the dataset contains 210 examples related to pre/post-processing, 58 examples of evaluation, 75 training examples, 58 examples of inference, and 119 examples related to model construction. Fig ~\ref{fig:preproprocessing} provides a concrete example of pre/post-processing, illustrating the use of the ``draw\_point2d'' function to modify output images after inference, such as for visualizing results or highlighting key points of interest. Similarly, Fig~\ref{fig:metrics} depicts an evaluation example, where the function evaluates the quality of a recommendation system by calculating how many relevant items (those that users liked) are included in the top-K recommendations, providing a measure of the model’s effectiveness after prediction. Fig~\ref{fig:training} showcases a training example, where the method computes the loss based on projections of transformed images. This loss function guides the model during the back-propagation phase of training, with the model improving as it minimizes the loss and learns more effective representations.

\input{figs/general_example}
\input{figs/example_classification}

%\input{figs/text_example}
%\input{figs/image_example}

Most of our data serve more than one ML task type, hence 328 (over 63\%) instances are labeled as \textit{General} as shown in Fig~\subref{fig:statistics_task}. For example, Fig~\ref{fig:example_general} shows \textbf{to\_image} function handles data type conversions and pre-processing to standardize image inputs, without performing any specific machine learning task. However, for the cases that serve a specific ML task, our dataset covers all ML tasks evenly with 14 to 60 instances each. Among these, the classification task has the most representative of 60 data points. For example, Fig~\ref{fig:example_classification} shows a classification task, calculating precision, recall, and F1 scores for both duplicate and non-duplicate file pairs to evaluate the performance of a classification model. On the other hand, The regression task is not as popular with only 14 data points.

%As shown in Table \ref{tabs:statistics}, we categorized the data into different task types: 60 examples of classification, 14 instances of regression, 24 examples of prediction, 40 examples of segmentation, 37 examples of detection, and 17 instances of recommendation. However, 328 amount of data is common or not specific to any one task type. For instance, as illustrated in \ref{fig:training}, the ``forward'' function in the ``NTXentLoss'' class cannot be easily categorized into a single task type. This function computes the contrastive cross-entropy loss and can be used for different purposes depending on the pipeline it is a part of, whether related to prediction, segmentation, or recommendation. Its functionality is general and flexible, making it difficult to categorize it strictly into one task type. Such examples highlight that certain functions are common across various processes and thus defy clear classification into traditional task types like prediction, regression, or segmentation.

Image data is the most popular input data type with 238 instances (nearly 46\%) as shown in Fig~\subref{fig:statistics_data}. In some cases where the input data to the function is missing or not the input to the model, we categorize them into the Others category which contains 168 instances. An example of such cases is presented in Fig~\ref{fig:model_construction}, where the initialization method constructs a new neural network model, however, information on the input type of such networks is not available. Textual data has the least instances since most of the time, textual data is tokenized and presented as either a data array or general tensor.

%Based on Table \ref{tabs:statistics}
%, our dataset contains 238 instances related to image data, 28 examples of text data, 84 instances of tabled or array-structured data, and 168 instances where the data type is not explicitly clear. By ``not obvious'' data type, we refer to cases where it is unclear what kind of data is being processed. As an example of such a case, consider the \ref{fig:model_construction}. In this case, while it is known that the task is related to model construction, there is no direct hint about the type of data being processed—whether it is images, text, or something else. The function simply sets up the architecture for a convolutional network, leaving the data type ambiguous.

\section{Candidate Prompt Filtering Criteria}
In this appendix, we describe the criteria of filtering and refining prompts to ensure clarity and completeness.

\begin{description}
\item[Contains clear sufficient information for the code to be generated]
This assessment aims to ensure the prompt’s clarity and comprehensibility for a human expert. Annotators check that the prompt includes all essential variables, functions, and definitions for high-quality code generation, providing enough information to clearly explain the problem. The human expert serves as the benchmark to set a high standard for future code generation. We also verify that the prompt provides sufficient guidance, including specific coding conventions or required components.

%In this phase, we examine whether the prompt contains all the essential variables, functions, and definitions necessary for a human expert to produce high-quality code. This ensures that the expert can leverage the information provided in the prompt to accurately generate the intended functionality. We focus on whether the prompt offers sufficient guidance, including specific coding conventions and necessary components, that are essential for the expert to follow.

\item[Specifies the input and output format]
Since our test cases require certain input and output formats, it is important to check such details in the candidate prompt to enable our test cases to function correctly\cite{sahoo2024systematic, chen2024using}. In other words, without precise definitions of the input and output specifications, the generated code might not align with the expected test parameters, resulting in false negative results during evaluation. Error and exception handling are also considered in this question. For example, we specifically check whether the prompt accounts for handling cases such as ``ValueError'', ``TypeError'', or other domain-specific exceptions that the function might raise. This will ensure that the code will be correctly evaluated given our extracted test cases.


%Even if a prompt appears to be complete, it may still require refinement, particularly in defining input and output formats. This is a critical aspect, as we rely on automated test cases to evaluate the performance of different code generation models. 
%The input and output specifications provide a clear framework for these tests, allowing them to function correctly. 
%Without precise definitions, the generated code might not align with the expected test parameters, resulting in erroneous outputs or failures during testing. Ensuring that the prompt includes explicit input and output formats guarantees that both the code and tests can execute effectively.

\item[Covers error handling and boundary conditions]
%Including error handling and boundary conditions in test parameters ensures the passing rate accurately reflects code generation performance. If prompts lack these criteria, reviewers suggest revisions to meet quality standards. This process produces prompts that are both technically accurate and practically useful, enhancing code generation outcomes.
Similar to input and output specification, error handling and boundary conditions are often part of the required testing parameters
%that the included test cases require. 
By ensuring that the prompt includes such details, we ensure that the passing rate truly reflects the performance of the code generation under test.
%Error handling is a vital component of robust code, as it ensures that the function can gracefully manage unexpected inputs or invalid states without crashing. In this phase, we ensure that the prompt explicitly addresses potential error-raising scenarios and contains sufficient detail to guide the implementation of error handling mechanisms. This includes specifying boundary conditions and input validation requirements, which are essential to create reliable and fault-tolerant code. By embedding these details into the prompt, we improve the likelihood that the generated code will be resilient to a wide range of inputs and will properly manage errors through appropriate exception handling. In summary, during this round of checks, we will focus on ensuring that the prompts contain all necessary elements for an expert to generate accurate code, that input and output formats are clearly defined to facilitate automatic testing, and that the prompt includes comprehensive guidance on error handling and boundary conditions.
%(melika) How the prompts ensure that error handling mechanisms are covered. You can add specific examples of error conditions (e.g., handling invalid input types)?
\end{description}

\section{Final Data Filtering and Validation Criteria}

This appendix outlines the criteria used to filter and validate data, ensuring alignment with key DL tasks, proper use of AI frameworks, and clarity in algorithm implementation.

\begin{description}
\item[\textbf{Serving key DL tasks}] The prompt and the associated function should be closely aligned with significant DL tasks such as image recognition, regression, item recommendation, object detection, label prediction, and natural language processing tasks. This criterion ensures that our dataset contains all important and relevant data points\cite{xie2024frontiers}.
%the data is relevant to current deep learning research and applications, while reviewers evaluate whether the function contributes to key deep learning tasks such as 

\item[\textbf{Utilization of popular DL frameworks}] The code should efficiently use widely recognized AI frameworks (when appropriate), such as TensorFlow, PyTorch, or Keras. This criterion ensures our dataset represents typical DL code with a heavy emphasis on reusability\cite{assi2024unraveling}.
%practical  This not only enhances the code's practical relevance but also ensures that it aligns with industry standards.

\item[\textbf{Algorithms' relevancy and clarity}] The code should implement DL-specific algorithms (e.g., edge detection algorithms, Principal component analysis, or Stochastic gradient descent). The code should also be well-documented and easy to understand. Complex algorithms must strike a balance between technical depth and clarity to ensure usability.
\end{description}

\section{Data Categories}
%Labeling is a crucial aspect of organizing and analyzing the dataset, allowing for a structured evaluation of generated code in DL applications. 
In this appendix, we provide details of three key sample categorizations: the stage in the ML pipeline, the ML task type, and the input data type. 
%The ML pipeline stage label categorizes code based on its role, such as pre-processing, model construction, training, inference, or evaluation. The ML task type label identifies the specific problem the code is addressing, including tasks like classification, regression, or object detection. Lastly, the input data type label specifies whether the function processes images, text, structured arrays, or other data formats. These labels provide a standardized framework for analyzing code, ensuring consistency in benchmarking and facilitating deeper insights into the characteristics of generated DL code.

\subsection{Stage in the ML pipeline}

    This label indicates the stage that the code is in within the ML pipeline: \textit{Pre/post Processing}, \textit{Model Construction}, \textit{Training}, \textit{Inference}, or \textit{Evaluation \& Metrics}.
    %The objective is to determine where the code fits within a typical deep-learning pipeline. 
    The annotators determine whether the function is related to a stage by analyzing the code and comment to find information that is related to the specific stage. For example, code that specifies a convolutional neural network (CNN) architecture with layers such as convolutions or pooling would fall under the Model Construction category. %More details are provided in our repository\footnote{...}.
    %We provide the specific details of this labeling process in 

\begin{description}
    \item[\textbf{Pre/Post Processing}] Code in the pre or post-processing stage often manipulates data (input or output). For example, pre-processing code cleans or augments input data, whereas post-processing code augments output data for visualization. Due to the ambiguity at the function level, we have a combined category for pre and post-processing code\cite{wen2020time}.

    \item[\textbf{Model Construction}]This stage defines the network architecture and sets up the computational graph for deep learning models, including defining layers, activation functions, and layer connections. Examples include defining CNN architectures and forward pass logic. Loss functions are part of this stage, but optimization steps are in the training phase\cite{howard2019searching}.

    \item[\textbf{Training}]The training stage optimizes the model's parameters using a loss function and optimization algorithm. This includes backpropagation and weight updates. Code for gradient descent using optimizers like Adam or SGD and looping over epochs and batches falls under this stage\cite{diederik2014adam}.

    \item[\textbf{Inference}]Inference code is used to generate labels based on a trained model. It processes new input data and outputs results, such as classifications or detections, without changing model parameters. This stage emphasizes speed and efficiency for real-time deployment\cite{kirillov2019panoptic}.

    \item[\textbf{Evaluation \& Metrics}]Code in this stage assesses the performance of a trained model using various metrics. It involves running the model on a validation/test dataset and comparing predictions to ground truth labels to measure accuracy, precision, recall, F1-score, etc.\cite{wu2020comprehensive}.

\end{description}

%    \begin{itemize}
%    \item \textbf{Pre/post Processing}---code in the pre or post-processing stage often manipulates data (input or output data). For example, pre-processing code would clean or augment input data whereas post-processing code would also augment output data for visualization. Since at the function level, it is often ambiguous if the code is for pre or post-processing, we decided to have a combined category for both pre and post-processing code\cite{wen2020time}.

%    \item \textbf{Model Construction}---Code in the model construction stage defines the network architecture and sets up the computational graph for deep learning models. This includes defining layers, activation functions, and connections between layers. For example, code that specifies a convolutional neural network (CNN) architecture with layers such as convolutions, pooling, and fully connected layers would fall under this category. Additionally, this stage involves defining loss functions and the forward pass logic. It typically excludes optimization steps, which are part of the training phase\cite{howard2019searching}.

%    \item \textbf{Training}---Code in the training stage is responsible for optimizing the model's parameters through iterative updates using a specified loss function and optimization algorithm. one of the main components of this stage is backpropagation, where the gradients are calculated and used to update the weights of the model to minimize the loss function. For example, code that performs gradient descent using optimizers like Adam or SGD, including the loop over epochs and batches, would be part of the training stage\cite{diederik2014adam}.

%    \item \textbf{Inference}---Code in the inference stage is used to make labels based on a trained model. During this phase, the model processes new input data and produces output, such as classifications, detections, or other tasks, without updating the model's parameters. This code is typically optimized for speed and efficiency, as it is often deployed in real-time systems\cite{kirillov2019panoptic}.

%    \item \textbf{Evaluation \& Metrics}---Code in the evaluation stage defines how the performance of a trained model is assessed using various metrics. This stage involves running the model on a validation or test dataset and comparing its predictions to the ground truth labels to measure accuracy, precision, recall, F1-score, or other domain-specific metrics\cite{wu2020comprehensive}.

%    \end{itemize}
    
    %pre-processing, post-processing, model construction, training, inference, or evaluation and metrics. They should thoroughly analyze the code to accurately label its role, such as pre-processing tasks like data cleaning or augmentation, or model construction tasks like defining a neural network's architecture.


\subsection{ML task type}
    
This label indicates the ML task\cite{sarker2021machine, Vinodkumar2023Survey, technologies2024Manakitsa} that the code is serving when applicable. The annotators examine the code to determine the type of task being solved, such as \textit{Time series Prediction}, \textit{Recommendation}, \textit{Image Segmentation}, \textit{Object Detection}, \textit{Regression}, \textit{Classification}, or \textit{General}. Specifically, the annotators look for patterns in the code corresponding to each task. For instance, code that outputs bounding boxes and class labels for objects falls under the Object Detection category. In cases where the code can be used for multiple ML tasks (i.e., does not exclusively belong to a specific ML task), we assigned a \textit{General} label.
    %More details are provided in our repository\footnote{...}.

\begin{description}

    \item [\textbf{Classification}]Classification tasks involve assigning input data to categories or classes. For example, models using softmax activation in the final layer for outputs like ``dog'' or ``cat'' fall under this category. Categorical cross-entropy loss is a common indicator.

    \item [\textbf{Regression}]Regression tasks predict continuous values. Code indicating regression tasks often has linear activation functions in the final layer.

    \item [\textbf{Object Detection}]Detection tasks identify objects and their locations within images. Code that outputs bounding boxes and class labels (e.g., YOLO, Faster R-CNN) and employs anchor boxes or non-maximum suppression is indicative of detection tasks.

    \item [\textbf{Image Segmentation}]Segmentation tasks assign labels to each pixel in an image. Code involving semantic or instance segmentation (e.g., U-Net, Mask R-CNN) where the output is a mask with pixel-level classifications is a common example.

    \item [\textbf{Time Series Prediction}]These tasks forecast future values using historical data. Code involving recurrent neural networks (RNNs), LSTM, GRU models, and loss functions like mean absolute error (MAE) or MSE is typical.

    \item [\textbf{Recommendation}]Recommendation tasks suggest items or actions based on user data. Code implementing collaborative or content-based filtering algorithms, matrix factorization, or deep learning-based models for recommendations falls into this category.

    \item [\textbf{General}]Code that is versatile and applicable to multiple ML tasks without being exclusive to a specific one is labeled as \textbf{General}.

\end{description}

%    \begin{itemize}
    
%    \item \textbf{Classification}---Categorical assignments indicate classification tasks. In these tasks, the model assigns input data to predefined categories or classes. For example, if the code is using softmax activation in the final layer of a neural network and outputs discrete class labels such as "dog" or "cat," it would be classified as a classification task. The presence of loss functions like categorical cross-entropy is also a key indicator.

%    \item \textbf{Regression}---Continuous output variables indicate regression tasks. In regression, the model predicts continuous values rather than discrete classes. The use of linear activation functions in the final layer is common in regression tasks.

%    \item \textbf{Object Detection}---Code in detection tasks typically identifies the presence and location of objects within images or sequences. For instance, object detection code that outputs bounding boxes and class labels for detected objects, such as YOLO or Faster R-CNN architectures, falls under this category. Using anchor boxes and non-maximum suppression techniques are strong indicators of detection tasks.

%    \item \textbf{Image Segmentation}---Segmentation tasks involve pixel-level classification, often used in image processing. Code performing semantic or instance segmentation, where each pixel of an image is assigned a label (e.g., identifying road, sky, or buildings in an image), would fall under this category. The use of U-Net or Mask R-CNN models, where the output is a mask with per-pixel classifications, is a common example.

%    \item \textbf{Time series Prediction}---Prediction tasks involve forecasting future values based on historical data. Code for time-series forecasting, such as predicting stock prices or weather trends, fits into this category. Look for recurrent neural networks (RNNs) or models like LSTM and GRU, as well as loss functions like mean absolute error (MAE) or MSE.

%    \item \textbf{Recommendation}---Recommendation tasks suggest items or actions based on user data. Code implementing collaborative filtering or content-based filtering algorithms, as well as models like matrix factorization or deep learning-based recommenders, falls under this category. For example, a recommendation system that suggests movies based on user ratings is classified as a recommendation task.

%    \item \textbf{General}---In cases where the code can be used for multiple ML tasks (i.e., does not exclusively belong to a specific ML task), we assigned a \textit{General} label.

%    \end{itemize}
    
    %The objective is to identify and label the task type, if applicable. Reviewers examine the code to determine the type of task being solved, such as Prediction, Recommendation, Segmentation, Detection, Regression, or Classification. They should look for patterns in the code that correspond to these tasks, such as continuous output variables indicating Regression or categorical assignments indicating Classification.


\subsection{Input data type}

This label indicates the input data type of the function. We focus on typical ML input data types such as \textit{Image}, \textit{Text}, \textit{Structured Array} (i.e., tabular), and \textit{Others}. The annotators analyze the processing flow of data to assign accurate labels. For example, techniques like flipping, cropping, or adding noise process image input. When the input data does not fit one of the typical types (image, text, structured array), we assign the Others label. 
    %More details for each category are provided in our repository
    %\footnote{\url{https://anonymous.4open.science/r/DL-Bench-D65E/category-explanation/readme.md}}

\begin{itemize}
    
    \item \textbf{Image}---Processing for image data includes steps like resizing, normalization, and data augmentation. Code that resizes images (e.g., 224$\times$224 for CNNs), normalizes pixel values, or applies augmentations (flipping, cropping, noise addition) typically signals image data\cite{krizhevsky2012imagenet}.

    \item \textbf{Text}---Text processing involves tokenization, n-gram generation, stemming, lemmatization, and embeddings. Code that handles these processes and converts text into vectors (e.g., using TF-IDF, Word2Vec, BERT) indicates text data\cite{liu2018neural}.

    \item \textbf{Structured Array}---Tabular data, where rows represent data points and columns represent features, is processed by normalization, one-hot encoding, or handling missing values. Code that reads CSVs into DataFrames and applies these techniques indicates structured array data, commonly used in regression or classification tasks\cite{chen2016xgboost}.

    \item \textbf{Others}---When input data does not match typical types (image, text, structured array), it is labeled as \textbf{Others}. This includes input such as model parameters or hyperparameters. For example, \verb|def __init__(self, weight, bias=None)| initializing model components without direct input data processing falls under this label.

\end{itemize}

%    \begin{itemize}
    
%    \item \textbf{Image}---Processing steps like resizing, normalization, and data augmentation are typical for image data. For example, code that processes image input by resizing to a fixed dimension (e.g., 224x224 pixels for CNN models) or applies normalization to scale pixel values (e.g., between 0 and 1) is indicative of image data. Techniques like flipping, cropping, or adding noise as part of data augmentation also signal image input. Models like CNNs are commonly associated with image data\cite{krizhevsky2012imagenet}.

%    \item \textbf{Text}---Typical steps for text data include tokenization, n-grams creation, stemming, lemmatization, and embeddings. For example, splitting text into tokens, generating n-grams for context, and converting text to vectors using TF-IDF, Word2Vec, or BERT embeddings indicate text data. Models like RNNs, LSTMs, and Transformers often process text inputs\cite{liu2018neural}.

%    \item \textbf{Structured Array}---Structured array input refers to tabular data, where each row represents a data point and each column represents a feature. Typical processing steps include normalization, one-hot encoding for categorical variables, or handling missing values. For example, code that reads a CSV file into a DataFrame and applies scaling or categorical encoding would be indicative of structured array data. Tabular input is often used in regression or classification tasks, with models like decision trees, random forests, or XGBoost\cite{chen2016xgboost}.

%    \item \textbf{Others}---When the input data does not fit one of the typical types (image, text, structured array), we assign the Others label. This includes cases where the input is model parameters, hyperparameters, or custom data types. For example, code like def \_\_init\_\_(self, weight, bias=None) that initializes model components but doesn’t directly process input data falls under this category. 

%    \end{itemize}
    
    %For example, resizing or normalization are typical processing steps for image data, and tokenization and embedding are typical processing steps for text data.
    %When the input data does not fit one of these types (e.g., \texttt{def \_\_init\_\_(self, weight, bias=None)}), we assign \textit{Others} label.
    %The objective is to determine and label the type of input data the function handles. Reviewers analyze the code to identify whether the input is image data, tabular data, text, or another format. They should also consider how the data is processed to label this part, such as resizing or normalization for image data or tokenization and embedding for text data.\end{itemize}


%\section{Categories of Code Stages and Tasks in Machine Learning}

%\section{Categories of Machine Learning Tasks}

%\section{Input Types in Machine Learning}

\section{LLM Bug Types and DL-specific Subtypes}

In this appendix, we provide details for the common types of errors in LLM-generated code as well as our DL-specific subtypes.
%, providing a structured analysis of typical failure modes. 
%These include misinterpretation of prompts, syntax errors, unnecessary or redundant code, failure to handle edge cases, and performance inefficiencies. By understanding these error categories—ranging from input type mismatches to hallucinated objects and incorrect constant values—we aim to identify patterns that can guide improvements in automated code generation.

\begin{description}

\item[Misinterpretation: \textit{Generated code deviates from the prompt intention}]  
The produced solution does not fulfill the user’s original requirements or strays from the specified goals. This often indicates that the LLM has misunderstood or incompletely parsed the prompt.
    
\begin{description}
\item[Incorrect DL Library or Framework Usage:] The generated code does not match the requested library or framework. For example, if the prompt asks for a TensorFlow implementation of a CNN, but the LLM generates the model using PyTorch instead, or if a user requests a NumPy-based neural network operation but the output code uses TensorFlow functions.

    \item[Shape and Dimension Mismatch:] The LLM produces code with incorrect tensor dimensions that do not follow the prompt specifications. For example, if the prompt requests a fully connected layer expecting an input of shape $(64, 128)$, but the generated code initializes it with an input shape of $(128, 64)$, leading to a mismatch in matrix operations.

    \item[Incorrect DL/ML Functionality:] The generated code does not implement the correct functionality as described in the prompt. For instance, if the prompt asks for a binary classification model using a sigmoid activation function, but the output code instead applies a softmax activation function intended for multi-class classification, altering the intended behavior.
    
    
\end{description}
    
\item[Syntax Error: \textit{Missing parenthesis, semicolon, or other syntax issues}]
Straightforward syntactic mistakes such as unclosed quotes, unmatched braces, or misplaced punctuation prevent the code from compiling or running properly.

\item[Silly Mistake: \textit{Redundant conditions, unnecessary casting}] 
Simple but avoidable errors, such as repeating the same condition twice or performing extra type conversions with no purpose. While these do not always break the code, they reduce readability and hint at confusion in the model's reasoning.

\item[Prompt-biased Code: \textit{Code overly relies on examples from the prompt}]
The LLM anchors too strongly to the examples provided in the prompt, resulting in a solution that works only for the specific inputs shown rather than generalizing the logic for broader applicability.

\item[Missing Corner Cases: \textit{Edge cases not handled}]
The generated solution neglects special scenarios such as empty inputs, boundary values, or invalid parameters, leading to unreliable behavior outside of typical inputs.

\begin{description}
    

    \item[Tensor Type and Value Edge Cases:] These bugs occur when operations fail due to unexpected tensor types or values. For example, using a tensor with \texttt{float32} data type in a function that expects integers or encountering issues when dividing by zero in a tensor.
    
    \item[Shape and Dimension Edge Cases:] Bugs of this type happen when operations fail because of unexpected edge-case shapes. For example, trying to perform a convolution on a tensor with a batch size of $0$ or a single dimension, such as $(1, 28, 28)$, when a shape like $(32, 28, 28)$ is expected.
\end{description}

\item[Wrong Input Type: \textit{Incorrect input type in function calls}]
The code passes incompatible data types to functions or methods (e.g., providing a string instead of a list), which causes runtime failures or nonsensical outputs.

\begin{description}
    \item[Tensor Shape Mismatch:] The generated code provides tensors with incorrect shapes to functions, leading to shape-related errors. For example, passing a 3D tensor of shape $(batch, height, width)$ to a function that expects a 4D tensor of shape $(batch, channels, height, width)$, causing a runtime error in deep learning frameworks like PyTorch or TensorFlow.
    
    \item[Incorrect ML/DL Function Library Arguments:] These occur when invalid arguments are passed to functions. For instance, using \texttt{stride=-1} in a convolution function, which is not logically or mathematically valid.

    \item[Type Mismatch Problem:] The generated code uses tensors with incompatible data types in operations. For example, passing a tensor with data type \texttt{float32} to a function that expects \texttt{int64}, or attempting to index a tensor with a floating-point value instead of an integer, leading to type-related execution failures.
\end{description}

\item[Hallucinated Object: \textit{Nonexistent or undefined objects used}] 
The LLM invents objects, classes, or modules that do not exist or have not been imported or defined. These errors result in runtime failures or developer confusion.
\begin{description}
    \item[Missing or Undefined DL Modules:] This happens when a model, layer, or module that hasn’t been properly defined or initialized is used. For example, attempting to forward-pass input through a neural network layer that hasn't been added to the model.
    
    \item[Incorrect Usage of DL Modules:]The generated code references deep learning modules, functions, or classes that do not exist or belong to the wrong framework. For example, calling \texttt{torch.nn.Dense()} instead of \texttt{torch.nn.Linear()}, or attempting to use \texttt{tensorflow.layers.Conv2D} instead of \texttt{tf.keras.layers.Conv2D}. These hallucinated module names cause import errors or incorrect function calls.
\end{description}

\item[Wrong Attribute: \textit{Incorrect/nonexistent attributes for objects or modules}]
The LLM references valid objects but assigns them invalid or incorrect attributes. These subtle errors often result from misunderstandings of library APIs or typos in the generated code.

\begin{description}
 \item[Wrong DL Module Import:] Bugs of this nature arise when modules are imported incorrectly. For example, importing \texttt{jax} functions when the rest of the code is written in PyTorch, leading to incompatibilities during execution.
    \item[Incorrect API Usage:] These bugs occur when a library API function is called incorrectly. For example, using the \texttt{train()} method instead of \texttt{fit()} for a Keras model or passing parameters in the wrong order to an optimizer.
\end{description}

\item[Non-Prompted Consideration: \textit{Non-requested features added}]
The LLM includes functionality unrelated to the requirements, often due to extraneous training data or contextual noise. This bloats the code and complicates its scope.

\item[Operation/Calculation Error: \textit{Errors in arithmetic or logical operations}]
The LLM makes errors in mathematical calculations or logical expressions, such as confusing addition with subtraction or mixing up operator precedence. These subtle mistakes produce incorrect results.

\begin{description}

    \item[Data Type Casting Issues:] These bugs occur when tensors or variables are cast into incompatible data types. For instance, casting a \texttt{float32} tensor into \texttt{int32} without considering the loss of precision, which may disrupt training.
    \item[Shape and Dimension Error in Operations:] The generated code performs mathematical operations on tensors with incompatible shapes or dimensions, leading to incorrect computations or runtime failures. For example, attempting to add two tensors of shapes $(32, 64)$ and $(64, 32)$ without proper broadcasting, or performing a matrix multiplication between tensors with mismatched inner dimensions, such as $(4, 3) \times (5, 4)$, causing a shape misalignment error.
    
    \item[Incorrect Algebraic Calculation:] These bugs refer to mathematical errors in computations. For instance, incorrectly normalizing data by dividing by the mean instead of the standard deviation, leading to improper scaling of input features.
    
    
\end{description}

\item[Performance Issue:]
This category includes inefficiencies in the generated code that impact runtime or resource usage. Examples include unnecessary nested loops, unoptimized algorithms, or excessive use of memory. While the code may produce correct results, its suboptimal implementation can make it impractical for large datasets or real-time applications. Performance issues often arise because the LLM generates a brute-force solution without understanding optimization principles.

\begin{description}
    \item[DL Performance Issues:] These bugs refer to inefficiencies in implementation that degrade model performance. For instance, not using GPU acceleration for operations or improper batching strategies leads to high memory consumption and slow training.
\end{description}

\item[Prompt Missing Information: \textit{Incomplete or unclear prompts}]
The bug arises due to insufficient detail or ambiguity in the input prompt, leading the LLM to make assumptions or guess certain details when generating the code. For example, if the prompt does not specify edge case handling or input constraints, the model may overlook these aspects entirely. This highlights the importance of crafting precise and comprehensive prompts when using LLMs for code generation.

\begin{description}
    \item[Not Defining the Correct DL Library in the Prompt:] This occurs when the prompt or instructions fail to specify the appropriate library or framework. For example, a user asks a language model to generate PyTorch code but does not explicitly state this, leading to TensorFlow code generation instead.
\end{description}

\item[Incorrect or Undefined Variable/Method References: \textit{Variables or methods that are not defined or incorrectly referenced}]
The LLM generates code that includes references to variables or methods that do not exist or are improperly used, leading to runtime errors such as NameError or AttributeError.

\item[Constant Value Error: \textit{Incorrect constant value assignment}]
The LLM assigns incorrect or miscalculated constant values, such as setting a time-out period to \texttt{10ms} instead of \texttt{1000ms}, leading to unexpected behavior.
\begin{description}
    \item[Incorrect Tensor Constant Value:] This type of bug arises when tensors are initialized with incorrect values, leading to flawed model behavior. For example, initializing weights or biases with all zeros instead of random values causes issues in training dynamics.
\end{description}

\end{description}

\newpage
\section{Distribution of Failures in Generated DL Code}

Table~\ref{tab:taxonomy} presents the distribution of bugs in LLM-generated DL code. The most prevalent issue is \textbf{deviation from the prompt}, accounting for the largest portion of errors. Unlike general LLM-generated code, DL code is more prone to \textbf{arithmetic and logical errors}, reflecting the complexity of numerical computations. Additionally, \textbf{incorrect input types in function calls} represent a significant share of the identified bugs, highlighting a common source of failures in generated DL code.

\input{tabs/Taxonomies}


%\section{Explanation of Table~\ref{tab:dl_bug}}
%\input{tabs/Taxonomy_prev2}
%Below is the categorization of bugs found in deep learning software generated by humans as shown in ~\ref{tab:dl_bug}:

%\begin{itemize}
%    \item \textbf{API Bug}: Issues caused by changes or incompatibilities in deep learning API versions or confusing documentation.
%    \item \textbf{Data Bug}: Problems arising from improperly formatted, cleaned, or structured input data before being fed into a model.
%    \item \textbf{Control and Sequence Bug}: Incorrect control flow (e.g., faulty if-else statements or loops) leading to crashes or unexpected behaviors.
%    \item \textbf{Data Flow Bug}: Mismatches in data type or shape after being fed to the model, causing inconsistencies in layers.
%    \item \textbf{Initialization Bug}: Parameters or functions improperly initialized, often degrading model performance without a crash.
%    \item \textbf{Logic Bug}: Incorrect logical organization of a model resulting in faulty outputs or runtime errors.
%    \item \textbf{Processing Bug}: Errors in algorithm selection or compatibility between layers due to data type inconsistencies.
%\end{itemize}

%As shown in Table~\ref{tab:dl_bug}, the most common bug category is API bugs, which are also among the most frequent DL-related bugs, as indicated in Table~\ref{tab:taxonomy}. On the other hand, dataflow bugs constitute a smaller portion of the overall bugs, which contrasts with our observations of LLM-generated code bugs. Additionally, logic bugs do not play a dominant role in this categorization, although a significant portion of the identified bugs fall into this category. This suggests that while there is some overlap between LLM-generated and human-generated DL code bugs, notable differences remain, likely due to LLMs' weaknesses in handling numerical and logical reasoning.
%\input{tabs/Taxonomy_prev1}
