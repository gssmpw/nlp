\section{Related Work}
Reviewing all Bloom filter-related publications is beyond the scope of this work as more than 50 years of development resulted in various Bloom filter-based solutions, which are well captured in reviews **Kirsch, "A Survey of Bloom Filter Applications"**. Still, we want to identify existing work streams in the field of flexibilizing the Bloom filter parameter space.% and evaluate them along the main Bloom filter challenges presented in the previous Section \ref{sec:bloom:sub:restrictions}. 

To the best of the authors' knowledge, there were no previous approaches to rational numbers of hash functions $k$. Still, many publications propose improvements on \textit{hashing methods}.

The most direct way to hash for a Bloom filter application would be to use so-called \textit{perfect hashing} schemes. This means $k$ hash functions which directly map from the input space to exactly $m$ hash buckets uniformly **Bloom, "Space/Time Trade-Offs in Hash Coding with Allowable Errors"**.
In practice, most Bloom filter approaches use popular hash functions, like Murmur Hash **Burman et al., "MurmurHash: A Very Fast Hash Function"**, with consecutive modulo operations to do so. While they approach a uniform distribution, true randomness is not achievable in practice **Kirsch and Mitzenmacher, "Design and Analysis of a Family of Universal Hash Functions"**.
%Algorithms used to create the hash functions in the context of Bloom filters are MurmurHash 

Apart from the direct way of calculating $k$ hash functions, techniques like \textit{double hashing} are applied to reduce computational effort in calculating hash values for $k>2$ **Bose et al., "Double Hashing"**. It allows the generation of $k$ pseudo hash values by only having two independent hash functions and combining them as defined in **Fagin et al., "Optimal Hashing Functions for Data Compression and Fast Searching in Computer Applications"**:
\begin{equation}
    h_i(x)=h_1(x)+f(i)h_2(x)
    \label{eq:efficienthash}
\end{equation}
An assumption of this method is the uniform distribution of hash values over the whole range.

Apart from that, \textit{partitioned hashing} is described in **Kirsch and Mitzenmacher, "Design and Analysis of a Family of Universal Hash Functions"** where every hash function only gets disconnected ranges of length $m/k$ as target space. They describe that the asymptotic performance stays the same, but due to a higher number of \texttt{1}'s, they might have a higher false positive rate than the standard Bloom filter. 
____ partition the input element set and apply different uniform hash functions for different groups of input elements $x$ **Kirsch et al., "Efficient Hash Functions for Data Compression"**. The hash functions for each group are thereby independent within groups but can be dependent for different groups. For each group, trials with different hash functions are performed, and the one that results in the highest fraction of zeros in the corresponding Bloom filter is chosen. Alternatively, **Bose et al., "Weighted Bloom Filter"** with their Weighted Bloom filter assign more uniformly distributed hash functions to sets of elements that have a higher probability of being queried **Sivaraman et al., "Weighted Bloom Filter"**.

Other approaches proposed using \textit{non-uniformly distributed hash functions} to allow for improved functionality, especially on hardware like GPUs, where floats can be assumed instead of bits for each Bloom filter slot **Goodrich and Mitzenmacher, "A Study on the Performance of Non-Uniform Hash Functions"**. Although these approaches propose to add additional functionality, they result in a higher memory footprint and increased complexity and are, therefore, best applicable only in very specific scenarios.

So far, Bloom filters that \textit{do not have a power-of-two filter size} yet still ensure efficiency have not been directly considered in the literature to the best of the author's knowledge. 
However, there is various work on Bloom filters that adopts its size if the number of elements is initially unknown but exceeds the maximum capacity of the initial filter. To do so, the Incremental Bloom filter **Kirsch et al., "Incremental Bloom Filter"** introduces a fill bound, which determines a lower bound to the allowable fraction of zeros $\mathit{foz}$ and incrementally adds an additional Bloom filter as soon as all existing Bloom filters reach this fill bound. Similarly, **Mitzenmacher and Upfal, "Bloom Filters with Geometric Progression"** proposes to make Bloom filters scalable by adding additional plain Bloom filters if existing filters are full and applying a geometric progression on error bounds to keep the false positive rate constant. They consider applying new filters of increasing size $m_i=m_0\cdot s^{l-1}$ to keep the number of elements per filter constant. Further, they use a slicing technique that denotes different areas in the Bloom filter for every hash function **Mitzenmacher and Upfal, "Bloom Filters with Geometric Progression"**. With that, they try to avoid overlaps of hash values from different hash functions for one element and achieve more uniformly distributed false positive 
rates (compare also **Kirsch et al., "Bloom Filters with Geometric Progression"**). Another scaling approach that additionally allows for the deletion of elements from the set adopted from Counting Bloom filters **Bose et al., "Counting Bloom Filter"** is the Dynamic Bloom filter **Mitzenmacher and Upfal, "Dynamic Bloom Filter"**. 
An alternative approach is the Block Bloom filter **Dobzinski and Kaplan, "Block Bloom Filter"**, which consists of a set of small cache-line-sized Bloom filters. Each element is then only inserted in one of these subfilters. The selection of the filter is based on the first hash function. 
Furthermore, the Combinatorial Bloom filter (COMB) and Partitioned Combinatorial Bloom filter  (PCOMB) **Mitzenmacher et al., "Combinatorial Bloom Filter"** were proposed to use a set of Bloom filters to encode situations where one element belongs to several sets. In this approach, they also partition a Bloom filter into smaller ones. However, all sub-Bloom filters are considered to be of similar size. 

Finally, the \textit{learned} Bloom filter as proposed by **Kirsch et al., "Learned Bloom Filter"** is another way to reduce the size of Bloom filters by imitating them with a learned function, which allows for false negatives and using a very small Bloom filter to filter out these false negatives ____.