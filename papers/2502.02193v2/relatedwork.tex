\section{Related Work}
Reviewing all Bloom filter-related publications is beyond the scope of this work as more than 50 years of development resulted in various Bloom filter-based solutions, which are well captured in reviews \cite{Abdennebi.2021, Putze.2009,Patgiri.2018, Gupta.2017}. Still, we want to identify existing work streams in the field of flexibilizing the Bloom filter parameter space.% and evaluate them along the main Bloom filter challenges presented in the previous Section \ref{sec:bloom:sub:restrictions}. 

To the best of the authors' knowledge, there were no previous approaches to rational numbers of hash functions $k$. Still, many publications propose improvements on \textit{hashing methods}.

The most direct way to hash for a Bloom filter application would be to use so-called \textit{perfect hashing} schemes. This means $k$ hash functions which directly map from the input space to exactly $m$ hash buckets uniformly \cite{Gupta.2017}.
In practice, most Bloom filter approaches use popular hash functions, like Murmur Hash \cite{Appleby.2008}, with consecutive modulo operations to do so. While they approach a uniform distribution, true randomness is not achievable in practice \cite{Estebanez.2014}. 
%Algorithms used to create the hash functions in the context of Bloom filters are MurmurHash \cite{Appleby.2008}

Apart from the direct way of calculating $k$ hash functions, techniques like \textit{double hashing} are applied to reduce computational effort in calculating hash values for $k>2$ \cite{Gupta.2017}. It allows the generation of $k$ pseudo hash values by only having two independent hash functions and combining them as defined in \cite{Kirsch.2006}:
\begin{equation}
    h_i(x)=h_1(x)+f(i)h_2(x)
    \label{eq:efficienthash}
\end{equation}
An assumption of this method is the uniform distribution of hash values over the whole range.

Apart from that, \textit{partitioned hashing} is described in \cite{Gupta.2017} where every hash function only gets disconnected ranges of length $m/k$ as target space. They describe that the asymptotic performance stays the same, but due to a higher number of \texttt{1}'s, they might have a higher false positive rate than the standard Bloom filter. 
\citeauthor{Hao.2007} partition the input element set and apply different uniform hash functions for different groups of input elements $x$ \cite{Hao.2007}. The hash functions for each group are thereby independent within groups but can be dependent for different groups. For each group, trials with different hash functions are performed, and the one that results in the highest fraction of zeros in the corresponding Bloom filter is chosen. Alternatively, \citeauthor{Bruck.2006} with their Weighted Bloom filter assign more uniformly distributed hash functions to sets of elements that have a higher probability of being queried \cite{Bruck.2006}.

Other approaches proposed using \textit{non-uniformly distributed hash functions} to allow for improved functionality, especially on hardware like GPUs, where floats can be assumed instead of bits for each Bloom filter slot \cite{Werner.2015b}. Although these approaches propose to add additional functionality, they result in a higher memory footprint and increased complexity and are, therefore, best applicable only in very specific scenarios.

So far, Bloom filters that \textit{do not have a power-of-two filter size} yet still ensure efficiency have not been directly considered in the literature to the best of the author's knowledge. 
However, there is various work on Bloom filters that adopts its size if the number of elements is initially unknown but exceeds the maximum capacity of the initial filter. To do so, the Incremental Bloom filter \cite{Hao.2008} introduces a fill bound, which determines a lower bound to the allowable fraction of zeros $\mathit{foz}$ and incrementally adds an additional Bloom filter as soon as all existing Bloom filters reach this fill bound. Similarly, \cite{Almeida.2007} proposes to make Bloom filters scalable by adding additional plain Bloom filters if existing filters are full and applying a geometric progression on error bounds to keep the false positive rate constant. They consider applying new filters of increasing size $m_i=m_0\cdot s^{l-1}$ to keep the number of elements per filter constant. Further, they use a slicing technique that denotes different areas in the Bloom filter for every hash function \cite{Chang.2004, Almeida.2007}. With that, they try to avoid overlaps of hash values from different hash functions for one element and achieve more uniformly distributed false positive 
rates (compare also \cite{Bose.2008}). Another scaling approach that additionally allows for the deletion of elements from the set adopted from Counting Bloom filters \cite{Fan.1998, Bonomi.2006} is the Dynamic Bloom filter \cite{Guo.2010}. 
An alternative approach is the Block Bloom filter \cite{Putze.2009}, which consists of a set of small cache-line-sized Bloom filters. Each element is then only inserted in one of these subfilters. The selection of the filter is based on the first hash function. 
Furthermore, the Combinatorial Bloom filter (COMB) and Partitioned Combinatorial Bloom filter  (PCOMB) \cite{Hao.2009} were proposed to use a set of Bloom filters to encode situations where one element belongs to several sets. In this approach, they also partition a Bloom filter into smaller ones. However, all sub-Bloom filters are considered to be of similar size. 

Finally, the \textit{learned} Bloom filter as proposed by \citeauthor{Mitzenmacher.2018} is another way to reduce the size of Bloom filters by imitating them with a learned function, which allows for false negatives and using a very small Bloom filter to filter out these false negatives \cite{Mitzenmacher.2018}.