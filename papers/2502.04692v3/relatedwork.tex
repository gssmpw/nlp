\section{Related Work}
\label{Related_Work}

\textbf{Reward Design in Reinforcement Learning.}
Reward design is a fundamental aspect of RL, directly influencing an agentâ€™s ability to learn effective policies. Traditional approaches rely heavily on hand-crafted rewards tailored to specific tasks \cite{mo2022managing}. While these methods provide clarity and control, they often lack adaptability across diverse environments and require significant domain expertise. To address these limitations, automated reward generation has emerged as a key area of research. For example, \cite{sarukkai2024automated} proposes an LLMs-driven framework that uses progress functions and intrinsic rewards to efficiently generate rewards for reinforcement learning tasks. \cite{katara2024gen2sim} introduces Gen2Sim, which automates 3D asset creation, tasks, and rewards to scale robot learning in simulation. However, these approaches primarily focus on low-dimensional tasks, and their applicability to high-dimensional systems, such as humanoid robot locomotion, remains constrained.

\textbf{Reinforcement Learning for Humanoid Robotics.}  
Humanoid robots pose significant challenges for reinforcement learning (RL) due to their high degrees of freedom and the necessity for dynamic stability. Early research on humanoid robot locomotion utilized model-based methods, such as the inverted pendulum approach \cite{nandula2021neurodynamic}, which proved insufficient for handling complex movements like sprinting. More recent advancements have employed RL to develop motor skills in humanoid robots. For instance, \cite{tang2024humanmimic} introduced a Wasserstein adversarial imitation learning system, enabling humanoid robots to replicate human locomotion and execute seamless transitions. Similarly, \cite{pei2024gait} proposed a DDPG-based control framework with optimized reward functions to achieve stable, high-speed, and humanlike bipedal walking. Additionally, \cite{figueroa2024reinforcement} integrated intrinsically stable model predictive control and whole-body admittance control into an RL framework to enhance bipedal walking stability under dynamic conditions. While these approaches mark significant progress, achieving sprint-level performance in humanoid robotics still demands more advanced reward engineering and sophisticated learning algorithms.

\textbf{Large Language Models in RL and Control Tasks.}
LLMs, such as GPT-4, have demonstrated exceptional capabilities in zero-shot reasoning, code generation, and contextual optimization. Recent studies have begun exploring their potential in RL reward design. For instance, \cite{li2024auto} introduces Auto MC-Reward, an LLM-driven system that designs and refines dense reward functions to improve reinforcement learning efficiency. \cite{beak2024chatpcg} proposes ChatPCG, an LLM-driven framework that automates reward design and content generation for multiplayer game AI development. However, most of these efforts focus on high-level planning tasks, with limited exploration in low-level control for high-dimensional systems. Integrating LLMs into RL pipelines for humanoid robotics remains a largely untapped area of research.

\textbf{Feedback-Driven Optimization in RL.}
Iterative feedback has proven to be a powerful tool in RL for refining policies and reward functions. Frameworks like Reinforcement Learning from Human Feedback (RLHF) have demonstrated success in incorporating human input to improve model performance. However, RLHF methods typically require manual feedback, making them resource-intensive and less scalable. Recent advancements, such as the adapting feedback-driven DRL algorithm \cite{pattnaik2021multitask}, the online data-driven model-based inverse RL technique \cite{self2022model} and the dynamic inverse RL method \cite{tan2024dynamic} have shown promise in automating this process, but they remain insufficiently explored in the context of humanoid robotics locomotion.

\textbf{Positioning of Our Work.}
Our work builds on these foundational efforts by introducing STRIDE, a framework that leverages LLMs for fully automated reward generation, training, and feedback-driven optimization. Unlike traditional methods that rely on task-specific templates or human intervention, STRIDE dynamically generates rewards tailored to complex tasks like humanoid sprinting. By integrating LLMs' generative capabilities with reinforcement learning, STRIDE bridges the gap between high-level planning and low-level control, outperforming human-engineered rewards and state-of-the-art frameworks such as Eureka. Furthermore, its gradient-free feedback mechanism ensures safer and higher-quality rewards, setting a new benchmark for humanoid robotics and RL reward engineering.