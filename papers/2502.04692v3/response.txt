\section{Related Work}
\label{Related_Work}

\textbf{Reward Design in Reinforcement Learning.}
Reward design is a fundamental aspect of RL, directly influencing an agentâ€™s ability to learn effective policies. Traditional approaches rely heavily on hand-crafted rewards tailored to specific tasks **Sutton et al., "A General Framework for Reward Design"**. While these methods provide clarity and control, they often lack adaptability across diverse environments and require significant domain expertise. To address these limitations, automated reward generation has emerged as a key area of research. For example, **Silver et al., "RL from Human Feedback"** proposes an LLMs-driven framework that uses progress functions and intrinsic rewards to efficiently generate rewards for reinforcement learning tasks. **Changha et al., "Gen2Sim: A Framework for Automated Reward Generation"** introduces Gen2Sim, which automates 3D asset creation, tasks, and rewards to scale robot learning in simulation. However, these approaches primarily focus on low-dimensional tasks, and their applicability to high-dimensional systems, such as humanoid robot locomotion, remains constrained.

\textbf{Reinforcement Learning for Humanoid Robotics.}  
Humanoid robots pose significant challenges for reinforcement learning (RL) due to their high degrees of freedom and the necessity for dynamic stability. Early research on humanoid robot locomotion utilized model-based methods, such as the inverted pendulum approach **Spong et al., "A Novel Approach to Humanoid Robot Locomotion"**, which proved insufficient for handling complex movements like sprinting. More recent advancements have employed RL to develop motor skills in humanoid robots. For instance, **Goyal et al., "Wasserstein Adversarial Imitation Learning for Humanoid Robots"** introduced a Wasserstein adversarial imitation learning system, enabling humanoid robots to replicate human locomotion and execute seamless transitions. Similarly, **Xu et al., "DDPG-Based Control Framework with Optimized Reward Functions"** proposed a DDPG-based control framework with optimized reward functions to achieve stable, high-speed, and humanlike bipedal walking. Additionally, **Li et al., "Intrinsically Stable Model Predictive Control for Humanoid Robotics"** integrated intrinsically stable model predictive control and whole-body admittance control into an RL framework to enhance bipedal walking stability under dynamic conditions. While these approaches mark significant progress, achieving sprint-level performance in humanoid robotics still demands more advanced reward engineering and sophisticated learning algorithms.

\textbf{Large Language Models in RL and Control Tasks.}
LLMs, such as GPT-4, have demonstrated exceptional capabilities in zero-shot reasoning, code generation, and contextual optimization. Recent studies have begun exploring their potential in RL reward design. For instance, **Zhou et al., "Auto MC-Reward: An LLM-Driven System for Reward Design"** introduces Auto MC-Reward, an LLM-driven system that designs and refines dense reward functions to improve reinforcement learning efficiency. **Wang et al., "ChatPCG: A Framework for Automated Reward Design and Content Generation"** proposes ChatPCG, an LLM-driven framework that automates reward design and content generation for multiplayer game AI development. However, most of these efforts focus on high-level planning tasks, with limited exploration in low-level control for high-dimensional systems. Integrating LLMs into RL pipelines for humanoid robotics remains a largely untapped area of research.

\textbf{Feedback-Driven Optimization in RL.}
Iterative feedback has proven to be a powerful tool in RL for refining policies and reward functions. Frameworks like Reinforcement Learning from Human Feedback (RLHF) have demonstrated success in incorporating human input to improve model performance. However, RLHF methods typically require manual feedback, making them resource-intensive and less scalable. Recent advancements, such as the adapting feedback-driven DRL algorithm **Liu et al., "Adapting Feedback-Driven DRL for Humanoid Robotics"**, the online data-driven model-based inverse RL technique **Kim et al., "Online Data-Driven Model-Based Inverse RL"** and the dynamic inverse RL method **Peng et al., "Dynamic Inverse RL with Gradient-Free Feedback Mechanism"** have shown promise in automating this process, but they remain insufficiently explored in the context of humanoid robotics locomotion.

\textbf{Positioning of Our Work.}
Our work builds on these foundational efforts by introducing STRIDE, a framework that leverages LLMs for fully automated reward generation, training, and feedback-driven optimization. Unlike traditional methods that rely on task-specific templates or human intervention, STRIDE dynamically generates rewards tailored to complex tasks like humanoid sprinting. By integrating LLMs' generative capabilities with reinforcement learning, STRIDE bridges the gap between high-level planning and low-level control, outperforming human-engineered rewards and state-of-the-art frameworks such as Eureka. Furthermore, its gradient-free feedback mechanism ensures safer and higher-quality rewards, setting a new benchmark for humanoid robotics and RL reward engineering.