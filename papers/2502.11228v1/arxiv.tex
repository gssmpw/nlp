\RequirePackage[l2tabu, orthodox]{nag}
\documentclass[11pt]{article}

\usepackage[T1]{fontenc}

\renewcommand*\ttdefault{lmvtt}

\usepackage{soul} %% to use \st{} 

%%% OPTION 1 - Fourier Math + New Century Schoolbook + ParaType Sans

\usepackage{garamond}  %%% so package mathdesign doesn't throw errors
\renewcommand{\rmdefault}{ugm}%%% so package mathdesign doesn't throw errors

\usepackage[bitstream-charter]{mathdesign}
\usepackage{amsmath}
\usepackage[scaled=0.92]{PTSans}


% %%% OPTION 3 - MTPRO 2 Math + Termes Times + ParaType Sans

% GEOMETRY
\usepackage[
  paper  = letterpaper,
  left   = 1.65in,
  right  = 1.65in,
  top    = 1.0in,
  bottom = 1.0in,
  ]{geometry}

% COLOR
\usepackage[usenames,dvipsnames]{xcolor}
\definecolor{shadecolor}{gray}{0.9}

% SPACING and TEXT
\usepackage[final,expansion=alltext]{microtype}
\usepackage[english]{babel}
\usepackage[parfill]{parskip}
\usepackage{afterpage}
\usepackage{framed}

%redefine the leftbar environment to accept a width and coloring options
\renewenvironment{leftbar}[1][\hsize]
{%
  \def\FrameCommand
  {%
    {\color{Gray}\vrule width 3pt}%
    \hspace{10pt}%
    %\hspace{0pt}\fboxsep=\FrameSep\colorbox{black!10}%
  }%
  \MakeFramed{\hsize#1\advance\hsize-\width\FrameRestore}%
}%
{\endMakeFramed}

% define a paragraph header function
\DeclareRobustCommand{\parhead}[1]{\textbf{#1}~}

% EDITING
% line numbering in left margin
\usepackage{lineno}
\renewcommand\linenumberfont{\normalfont
                             \footnotesize
                             \sffamily
                             \color{SkyBlue}}
% ragged paragraphs in right margin
\usepackage{ragged2e}
\DeclareRobustCommand{\sidenote}[1]{\marginpar{
                                    \RaggedRight
                                    \textcolor{Plum}{\textsf{#1}}}}
% paragraph counter in right margin
\newcommand{\parnum}{\bfseries\P\arabic{parcount}}
\newcounter{parcount}
\newcommand\p{%
    \stepcounter{parcount}%
    \leavevmode\marginpar[\hfill\parnum]{\parnum}%
}
% paragraph helper
\DeclareRobustCommand{\PP}{\textcolor{Plum}{\P} }

% COUNTERS
\renewcommand{\labelenumi}{\color{black!67}{\arabic{enumi}.}}
\renewcommand{\labelenumii}{{\color{black!67}(\alph{enumii})}}
\renewcommand{\labelitemi}{{\color{black!67}\textbullet}}

% FIGURES
\usepackage{graphicx}
\usepackage[labelfont=bf]{caption}

% TABLES
\usepackage{booktabs}
\usepackage{multirow}

% ALGORITHMS
\usepackage[algoruled]{algorithm2e}
\usepackage{listings}
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}

% BIBLIOGRAPHY
\usepackage{natbib}

% ACRONYMS
\usepackage[acronym,nowarn]{glossaries}
\makeglossaries


% HYPERREF
\definecolor{tangerine}{rgb}{0.95, 0.52, 0.0}
\definecolor{palebrown}{rgb}{0.6, 0.46, 0.33}
\definecolor{peru}{rgb}{0.8, 0.52, 0.25}
\usepackage[colorlinks,linktoc=all]{hyperref}
\usepackage[all]{hypcap}
\hypersetup{citecolor=peru}
\hypersetup{linkcolor=peru}
\hypersetup{urlcolor=peru}

% CLEVEREF must come after HYPERREF
\usepackage[nameinlink]{cleveref}
\creflabelformat{equation}{#1#2#3}
\crefname{equation}{eq.}{eqs.}  
\Crefname{equation}{Eq.}{Eqs.}

% COLOR DEFINITIONS
\newcommand{\red}[1]{\textcolor{BrickRed}{#1}}
\newcommand{\orange}[1]{\textcolor{BurntOrange}{#1}}
\newcommand{\green}[1]{\textcolor{OliveGreen}{#1}}
\newcommand{\blue}[1]{\textcolor{MidnightBlue}{#1}}
\newcommand{\gray}[1]{\textcolor{black!60}{#1}}

% LISTINGS DEFINTIONS
\lstdefinestyle{mystyle}{
    commentstyle=\color{OliveGreen},
    keywordstyle=\color{BurntOrange},
    numberstyle=\tiny\color{black!60},
    stringstyle=\color{MidnightBlue},
    basicstyle=\ttfamily,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}

% NICE TO DO'S
\usepackage[colorinlistoftodos,
           prependcaption,
           textsize=small,
           backgroundcolor=yellow,
           linecolor=lightgray,
           bordercolor=lightgray]{todonotes}



\DeclareRobustCommand{\parhead}[1]{\textbf{#1}~}

\usepackage{amsthm}  % for proofs

% SPACING and TEXT
\usepackage{bm}

% NICE TODOs
\usepackage[colorinlistoftodos,
           prependcaption,
           textsize=small,
           backgroundcolor=yellow,
           linecolor=lightgray,
           bordercolor=lightgray]{todonotes}

% FIGURES
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}

% TABLES
\usepackage{booktabs}
\usepackage{arydshln} % Dashed lines
\usepackage{multirow}
\usepackage{nicematrix}

% ALGORITHMS
\usepackage{listings}
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}

% HYPERREF

% CLEVEREF must come after HYPERREF
\usepackage[nameinlink]{cleveref}
\creflabelformat{equation}{#1#2#3}
\crefname{equation}{eq.}{eqs.}  
\Crefname{equation}{Eq.}{Eqs.}


% LISTINGS DEFINTIONS
\usepackage{listings}
\lstdefinestyle{alp_style}{
    commentstyle=\color{OliveGreen},
    numberstyle=\tiny\color{black!60},
    stringstyle=\color{BrickRed},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=none,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\usepackage{capt-of}

% COMMENTS
\DeclareRobustCommand{\fjrr}[1]{{\color{red}\textbf{fjrr:} #1}}
\providecommand{\dan}[1]{
    {\protect\color{teal}{[Dan: #1]}}
}

\newcommand{\ourmetric}{{\text{VS}}}

% DUMMY TEXT
\usepackage{lipsum}

\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[section]   %  B

\theoremstyle{remark}
\newtheorem*{lemma*}{Lemma}


\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}

\newcommand{\g}{\, | \,}
\newcommand{\prm}{\, ; \,}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\bp}{\bm{p}}
\newcommand{\bX}{\bm{X}}
\newcommand{\bK}{\bm{K}}
\newcommand{\bC}{\bm{C}}
\newcommand{\bu}{\bm{u}}
\newcommand{\bz}{\bm{z}}
\newcommand{\by}{\bm{y}}
\newcommand{\bv}{\bm{v}}
\newcommand{\br}{\bm{r}}
\newcommand{\bx}{\bm{x}}
\newcommand{\bs}{\bm{s}}
\newcommand{\bw}{\bm{w}}
\newcommand{\bI}{\bm{I}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bepsilon}{\bm{\epsilon}}

\usepackage{authblk}
\usepackage{enumitem} 
\usepackage{algorithmic}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage[T1]{fontenc}
\usepackage{multirow}
\usepackage{threeparttable}
\newcommand{\first}[1]{\textbf{\textcolor[RGB]{0,0,255}{#1}}} % Blue
\newcommand{\second}[1]{\underline{\textbf{\textcolor[RGB]{255,165,0}{#1}}}} % Orange


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% DOCUMENT STARTS HERE %%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{\textbf{Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs}}

\author[1, 3]{Mohammad R. Rezaei}
\author[2, 3]{Adji Bousso Dieng}
\affil[1]{Institute of Biomedical Engineering, University of Toronto}
\affil[2]{Department of Computer Science, Princeton University}
\affil[3]{\href{https://vertaix.princeton.edu/}{Vertaix}}


\makeglossary %%%added for the notation section

\begin{document}
\maketitle

\begin{abstract}
\noindent Retrieval-augmented generation (RAG) enhances large language models (LLMs) for domain-specific question-answering (QA) tasks by leveraging external knowledge sources. However, traditional RAG systems primarily focus on relevance-based retrieval and often struggle with redundancy, especially when reasoning requires connecting information from multiple sources. This paper introduces \textbf{Vendi-RAG}, a framework based on an iterative process that jointly optimizes retrieval diversity and answer quality. This joint optimization leads to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to promote semantic diversity in document retrieval.~It then uses an LLM judge that evaluates candidate answers, generated after a reasoning step, and outputs a score that the retriever uses to balance relevance and diversity among the retrieved documents during each iteration. Experiments on three challenging datasets---HotpotQA, MuSiQue, and 2WikiMultiHopQA---demonstrate Vendi-RAG's effectiveness in multi-hop reasoning tasks. The framework achieves significant accuracy improvements over traditional single-step and multi-step RAG approaches, with accuracy increases reaching up to +4.2\% on HotpotQA, +4.1\% on 2WikiMultiHopQA, and +1.3\% on MuSiQue compared to Adaptive-RAG, the current best baseline. The benefits of Vendi-RAG are even more pronounced as the number of retrieved documents increases. Finally, we evaluated Vendi-RAG across different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and observed consistent improvements, demonstrating that the framework's advantages are model-agnostic.\\

\noindent \textbf{Keywords:} RAG, LLMs, Question Answering, NLP, Diversity, Vendi Scoring 
\end{abstract}

\section{Introduction}

\begin{figure*}[!t]
\centering
\includegraphics[width=\linewidth]{figures/Vendi-RAG-schema.pdf}
\caption{The process begins with an initial retrieval step, where a diverse set of documents is retrieved using the Vendi Score, ensuring broad semantic coverage. Next, leveraging a reasoning step to construct a coherent path to the final answer, the LLM generates an answer, which then undergoes quality assessment by an LLM judge. Based on the answer quality, the retriever is adjusted to balance diversity and relevance: high-quality answers limit the emphasis on diversity, while low-quality answers prompt the retriever to prioritize diversity more heavily. This adjustment is controlled by an adaptive parameter, $s$, which is updated over iterations. The process continues until the answer quality reaches an optimal threshold, denoted by Thr. Finally, the highest-quality responses and documents are selected, ensuring both diversity and accuracy.}
\label{fig:vendi-rag}
\end{figure*}


Retrieval-augmented generation (RAG) has emerged as a transformative framework for enhancing the performance of large language models (LLMs) in domain-specific tasks such as question-answering (QA). By retrieving relevant information from external sources beyond the training set, RAG enables LLMs to answer specialized queries more effectively \citep{achiam2023gpt, team2023gemini, jiang2024mixtral}. This approach has been particularly successful in single-hop QA, where a question can be answered using information from a single document \cite{raiaan2024review, kwiatkowski2019natural}. For instance, answering a question such as "Which country is filmmaker Sembene Ousmane from?" only requires retrieving relevant information from a single document containing this fact.

However, multi-hop QA introduces significantly greater complexity. Finding the correct answer to queries in multi-hop QA requires reasoning across multiple sources \citep{press2022measuring, tang2024multihop}. For instance, answering "Which city is the capital of the African country where Mount Kilimanjaro is located?" necessitates first identifying that Mount Kilimanjaro is in Tanzania, and then determining that Dodoma is the capital of Tanzania. This process involves not only retrieving information from multiple documents but also synthesizing these different sources effectively to form an accurate answer, which greatly increases the complexity of both retrieval and reasoning and often leads to redundancy.

To address these challenges, iterative RAG pipelines have been developed. These pipelines refine the retrieval process through repeated modifications and re-querying of retrieved documents, aiming to resolve ambiguities and improve relevance. Notable examples include Adaptive-RAG~\citep{jeong2024adaptive}, Self-RAG~\citep{asai2023self}, and IROC~\citep{trivedi2022interleaving}. 

Despite their success, iterative RAG methods typically rely solely on relevance-based retrieval, focusing on the similarity between the query and dataset entries and overlooking diversity. However, effectively addressing more complex queries requires diverse retrieval. We therefore propose a novel retrieval method called \emph{Vendi retrieval} to address the limitation of existing retrieval pipelines. Vendi retrieval leverages the Vendi Score (VS) to enhance the diversity of retrieved documents while accounting for retrieval quality through a simple weighting mechanism. 

Building on Vendi retrieval, we propose an iterative RAG pipeline called Vendi-RAG that effectively balances diversity and quality. More specifically, an initial set of candidate documents is retrieved. Based on these retrieved documents, the system generates chain-of-thought (CoT) reasoning steps. Using these reasoning steps and retrieved documents, a backbone LLM then generates candidate answers. Another LLM, which we'll refer to as an \emph{LLLM judge}, then assesses these candidates for relevance, coherence, and completeness. The highest-scoring answer is selected as the final response. If the answer does not meet the quality threshold, the Vendi retrieval process dynamically adjusts the balance between diversity and relevance in document selection for the next iteration. This iterative refinement continues until a high-quality response is achieved. Figure \ref{fig:vendi-rag} provides a detailed overview of the Vendi-RAG framework. 

We evaluated the Vendi retrieval process and Vendi-RAG on three challenging multi-hop QA datasets, HotpotQA \citep{yang2018enhancing}, MuSiQue \citep{trivedi2022interleaving}, and 2WikiMultiHopQA \citep{ho2020constructing}. To assess the Vendi retrieval method we measured the diversity of retrieved documents on the three datasets using two different diversity metrics, the VS and the max pairwise distance (MPD). We found that the Vendi retrieval process yields more diverse documents compared to the baselines according to both metrics. Second, we evaluated Vendi-RAG in terms of several performance metrics, looking at both accuracy and diversity. The results showed that Vendi-RAG substantially improves response accuracy, outperforming the baselines. Using GPT-3.5 as the LLM backbone, Vendi-RAG demonstrated significant accuracy gains across all datasets, with accuracy increases reaching +4.2\% on HotpotQA, +4.1\% on 2WikiMultiHopQA, and +1.3\% on MuSiQue compared to Adaptive-RAG, the best baseline. Notably, the accuracy improvement remained consistent across different LLM backbones---GPT-4o, GPT-4o-mini, and GPT-3.5---indicating that Vendi-RAG's advantages are model-agnostic. Additionally, our experiments with varying numbers of retrieved documents---beyond the standard two-document setting---showed that Vendi-RAG maintained its superior performance, especially as the number of retrieved documents increased. This underscores the critical role of the Vendi retrieval process in handling complex retrieval scenarios. For instance, when retrieving ten documents from HotpotQA, Vendi-RAG outperformed Adaptive-RAG by 7.8\% in accuracy using GPT-4o-mini as the backbone LLM.

\section{Related Work}
\label{sec:related}

\parhead{Question answering.} There are three main approaches to QA: non-retrieval-based methods~\citep{petroni2019language}, single-step RAG~\citep{lewis2020retrieval}, and multi-step RAG~\citep{asai2023self}. Non-retrieval-based QA methods pass queries directly to an LLM and use its generated output as the answer, without consulting external sources. While efficient, these methods struggle with queries requiring external or up-to-date information and suffer from hallucinations on out-of-distribution queries~\citep{shuster2021retrieval}. Single-step RAG methods integrate external knowledge retrieved from a knowledge base (e.g., Wikipedia). These methods improve factual accuracy but are limited by retrieval noise and perform poorly in complex reasoning tasks~\citep{trivedi2022interleaving}. Multi-step RAG methods are designed for complex multi-hop queries~\citep{jeong2024adaptive,asai2023self,tang2024multihop}. 

Recent improvements in multi-hop QA focus on question decomposition~\citep{radhakrishnan2023question}, chain-of-thought reasoning~\citep{wei2022chain,liu2024much}, and iterative retrieval~\citep{jeong2024adaptive,shao2023enhancing,yu2024auto}. Methods like ReCite~\citep{sun2022recitation} and IRCoT~\citep{trivedi2022interleaving} refine retrieval with progressive reasoning, while Self-RAG~\citep{asai2023self} adapts retrieval strategies based on query complexity. Decomposed prompting~\citep{khot2022decomposed} further enhances retrieval for complex queries~\citep{zhang2024accelerating}. MultiHop-RAG~\citep{tang2024multihop} integrates decomposition and retrieval pipelines but remains constrained by the redundancy issue caused by relevance-based retrieval.  

\parhead{Vendi scoring.} The Vendi Score (VS)~\citep{friedman2023vendi} is a similarity-based diversity metric applied in machine learning~\citep{berns2023towards, pasarkar2023cousins, mousavi4924208vsi, nguyen2024quality, kannen2024beyond, jalali2024conditional, askari2024improving, rezaei2025alpha, bhardwaj2025robust}, chemistry~\citep{pasarkar2023vendi}, materials science~\citep{liu2024diversity}, and biology~\citep{pasarkar2025vendiscope}. Vendi-RAG integrates VS into retrieval, balancing diversity and quality beyond conventional ranking systems~\citep{carbonell1998use, slivkins2010learning}. Unlike standard relevance-based retrieval~\citep{guu2020retrieval}, this approach enhances robustness and accuracy in multi-hop QA by incorporating semantic diversity into document retrieval.

\section{Method}
\label{sec:method}

We now describe Vendi-RAG, including the novel retrieval process it uses.

\subsection{Vendi Retrieval}
Diversity in retrieved documents is essential for multi-hop QA, as it ensures broad semantic coverage, reduces redundancy, and incorporates multiple perspectives~\citep{sun2022recitation, carbonell1998use, thakur2021beir}. The most used methods for diverse retrieval are similarity search (SS)~\citep{thakur2021beir} and maximal marginal relevance (MMR)~\citep{carbonell1998use}. SS maximizes relevance to the query but retrieves highly similar documents, leading to redundancy. MMR balances relevance and novelty using pairwise comparisons but also struggles to account for global semantic diversity.

To overcome these limitations, we propose a novel retrieval method that leverages the VS~\citep{friedman2023vendi} to explicitly optimize retrieval diversity. Let $\mathcal{D} = \left\{d_1, \dots, d_n\right\}$ be a set of retrieved documents  and $k(\cdot, \cdot)$ a positive semi-definite similarity kernel such $k(d_i, d_i) =  1$ for all $i$. Denote by K the corresponding similarity matrix that is such that $K_{ij} = k(d_i, d_j)$. The VS is defined as 
\begin{align}
\text{VS}_k(\mathcal{D}) = \exp\left(-\sum_{i=1}^n \lambda_i \log \lambda_i\right),
\end{align}
where $\lambda_1, \dots, \lambda_n$ are the eigenvalues of the normalized kernel matrix $K / n$. As argued by \citet{friedman2023vendi}, the VS is the effective number of unique documents in $\mathcal{D}$, reaching its maximum value $n$ when all the documents are distinct and its minimal value $1$ when all the documents are the same. 

While accounting for diversity is good for retrieval, especially for complex queries, it shouldn't be the only criterion. Quality also matters. To balance these two criteria, the Vendi retrieval process uses a convex combination of the two,
\begin{align}
\text{VRS}= s \cdot \text{VS}_k(q, \mathcal{D}) + (1-s) \cdot \text{SS}(q, \mathcal{D}),
\end{align}
where VRS stands for \emph{Vendi retrieval score}. The similarity score $\text{SS}(q, \mathcal{D})$ is computed using dense vector representations of both the query and the documents. The document representations are used to provide meaningful context, ensuring that the retrieved documents are relevant to the query. Here $s \in [0, 1]$ is a tunable parameter controlling the trade-off between diversity and similarity. When handling complex queries, such as those with multiple possible answers, a higher diversity weight $s$ promotes the selection of a semantically diverse set of documents. In contrast, for simpler or more specific queries that require precise information, a smaller value of $s$ prioritizes similarity-based retrieval.

\subsection{Vendi-RAG}  
We integrate the Vendi retrieval process into a flexible RAG pipeline that balances diversity and relevance for improved performance on multi-hop QA. 

\parhead{1. Initial retrieval.} The process begins by retrieving a set of documents using Vendi retrieval. This first step prioritizes broad semantic coverage (we set $s = 0.8$ initially in all our experiments), ensuring that the retrieved documents capture multiple perspectives and to prevent recovering semantically redundant documents. This initial diversity is particularly critical for multi-hop QA, where synthesizing information from varied sources is essential to accurately answering the query.
    
\parhead{2. Reasoning generation.} Based on the retrieved documents, the system generates CoT reasoning steps. These intermediate reasoning steps help contextualize the retrieved information, building a coherent pathway to the final answer.

\parhead{3. Candidate answer generation.} Using the reasoning steps and retrieved documents, the LLM generates candidate answers. These proposed answers are evaluated to determine their quality and completeness.

\parhead{4.~Quality evaluation.} An LLM judge assesses the candidate answers. This evaluation considers factors such as coherence, relevance, and alignment with the query. A quality score $Q_t$ is produced at the end of this quality-check. Here $t$ is used to indicate the iteration step. 

\parhead{5. Dynamic adjustment of the VRS.} Based on the quality score $Q_t$, the parameter $s$ is adjusted dynamically. We denote by $s_t$ the value of the parameter $s$ at the $t^{\text{th}}$ iteration. It controls the trade-off between diversity (via VS) and relevance (via similarity search). If  $Q_t$ is low, $s_t$ should be increased, to prioritize greater diversity in the subsequent retrievals. This ensures broader semantic exploration, which is beneficial for refining answers in cases where the retrieved information is already relevant but lacks coverage. Conversely, if $Q_t$ is high, $s_t$ should be decreased to focus more on relevance, retrieving documents that are closely aligned with the query to address potential gaps in specificity. We therefore define $s_t$ as
\begin{align}
    s_t &= f(Q_{t-1})=1- \frac{Q_{t-1}}{\max(Q_{t-1})},
\end{align}
where $f$ is a simple linear function that maps \( Q_{t-1} \) to the interval \([0,1]\), ensuring that higher quality scores correspond to lower diversity scores. 

\parhead{6. Iterative refinement.} The retrieval and reasoning steps are repeated iteratively, with adjustments to $s$ dynamically balancing diversity and relevance at each stage. This process continues until the desired answer quality is reached, ensuring that the system converges on an optimal set of documents and reasoning steps.

\parhead{7. Final answer selection.} Once the iterative refinement process is complete, the final set of documents and answers are selected based on their quality scores. This ensures that the output reflects both broad semantic coverage and high-quality, relevant information. Algorithm~\ref{alg:vendi-rag} summarizes the procedure.

\parhead{Why Adjusting $s$ Matters:}
The dynamic adjustment of $s$ is critical for striking the right balance between diversity and relevance during the retrieval process. High diversity is essential for exploring various facets of a complex query, especially in multi-hop QA, where information from disparate sources must be synthesized. However, excessive diversity can dilute the relevance of retrieved documents, potentially introducing noise and reducing the quality of generated answers. On the other hand, overemphasizing relevance can lead to redundancy and failure to capture the breadth of information needed for comprehensive reasoning.

By reducing $s$ when the quality score is high, the Vendi-RAG pipeline encourages exploration of less-redundant, semantically diverse documents. This ensures that even if the current answer is sufficient, the model explores additional perspectives that may enhance the depth and breadth of the final response. Conversely, increasing $s$ when quality is low allows the system to focus on retrieving documents that are more closely aligned with the query, addressing gaps in specificity or relevance.

The strength of Vendi-RAG lies in this adaptive approach to document retrieval. Unlike traditional RAG systems that use fixed retrieval strategies, Vendi-RAG's dynamic adjustment of the diversity-relevance trade-off (the parameter $s$) allows it to respond to the specific requirements of each query and intermediate reasoning step. When the system detects that current retrievals are yielding high-quality but potentially narrow responses, it automatically shifts toward greater diversity, exploring complementary perspectives. Conversely, when responses lack precision, the system can focus on more closely related documents to improve specificity.

\parhead{Performance characteristics.}
In practice, Vendi-RAG exhibits distinctive performance patterns that reflect its sophisticated design. The system naturally adapts its computational effort to query complexity, requiring more iterations for intricate multi-hop queries while converging quickly for simpler ones. Though the computational overhead exceeds that of basic RAG systems, the improved retrieval quality often results in better final answers. The system maintains reasonable scalability with document corpus size, as the primary computational bottleneck—eigenvalue computation—depends on the number of retrieved documents rather than the total corpus size. These characteristics make Vendi-RAG particularly suitable for complex tasks such as multi-hop QA. 


\begin{algorithm}[t]
\DontPrintSemicolon
\caption{Vendi-RAG Inference Pipeline}
\label{alg:vendi-rag}
Inputs: Query $q$, knowledge base $\mathcal{D}$, \# iterations $N$, quality threshold $\tau$\;

Initialize query $q_1 \gets q$, initialize parameter $s_1 \gets 0.8$\;

\For{$i = 1, \dots, N$}{

    Compute scores: $\text{scores}_i = s_i \cdot \text{VS}_k(q, \mathcal{D}) + (1 - s_i) \cdot \text{SS}(q, \mathcal{D})$\;

    Retrieve documents: $D_i \gets \text{Vendi-Retriever}(q, \text{scores}_i; \mathcal{D})$\;
    
    Generate reasoning: $r_i \gets \text{CoT}(q, D_i)$\;
    
    Generate answers: $\hat{a}_i \gets \text{LLM}(q, D_i, r_{1:i})$\;
    
    Evaluate quality: $Q_i \gets \text{Grader}(\hat{a}_i)$\;

    If $Q_i \geq \tau$ then return $\hat{a}_i$\;

    Else update $q$ and s: $q_{i+1} \gets \text{RewriteQuery}(q_i, \hat{a}_i, r_i)$ and $s_{i+1} \gets f(Q_{i})$\;
}
\Return $\hat{a}^*$ \tcp*[f]{Return the best answer after $N$ iterations.}
\end{algorithm}

\section{Experiments}
\label{sec:experiments}

This section presents a comprehensive evaluation of Vendi-RAG on multi-hop QA datasets. First, we investigate the effectiveness of the Vendi retrieval process in enhancing retrieval diversity. Next, we evaluate the Vendi-RAG pipeline, demonstrating its ability to handle complex queries requiring multi-step reasoning compared to the baselines.

\begin{table*}[t]
\caption{Retrieval diversity as measured by the Vendi Score (VS) and Max Pairwise Distance (MPD) for Vendi-RAG, Adaptive-RAG, and Adaptive Retrieval across different datasets. Vendi-RAG achieves higher diversity than the baselines across all datasets and both metrics.}
\label{tab:vs_mpd_comparison}
\centering
\begin{tabular}{lcc|cc|cc}
\toprule
\multirow{2}{*}{Dataset} & \multicolumn{2}{c|}{Adaptive Retrieval} & \multicolumn{2}{c|}{Adaptive-RAG} & \multicolumn{2}{c}{Vendi-RAG} \\
\cmidrule{2-7}
& VS & MPD & VS & MPD & VS & MPD \\
\midrule
MuSiQue & 6.13 & 1.25 & 6.55 & 1.42 & \textbf{7.12} & \textbf{1.95} \\
HotpotQA & 4.95 & 1.10 & 5.21 & 1.31 & \textbf{6.82} & \textbf{1.89} \\
2WikiMultiHopQA & 5.34 & 1.32 & 5.81 & 1.45 & \textbf{6.68} & \textbf{1.78} \\
\bottomrule
\end{tabular}
\end{table*}

\parhead{Datasets.} Our experiments are conducted on three challenging benchmark multi-hop QA datasets: {MuSiQue} \citep{trivedi2022interleaving}, {HotpotQA} \citep{yang2018enhancing}, and {2WikiMultiHopQA} \citep{ho2020constructing}. 

MuSiQue evaluates a model’s ability to synthesize facts spread across across multiple document sources. It includes questions spanning diverse domains such as history, science, and culture, requiring logical reasoning and synthesis of interdependent information. Given its emphasis on multi-step comprehension, this dataset challenges models to accurately identify and integrate relevant information to generate correct answers to queries. This is the most challenging dataset among the three.

HotpotQA assesses reasoning and fact verification across various domains, including geography, entertainment, and history. Its questions necessitate reasoning over two or more interconnected documents linked via hyperlinks. Additionally, the dataset includes “comparison” questions that require juxtaposing information from multiple sources, making it a challenging benchmark for evaluating both retrieval quality and reasoning ability.

2WikiMultiHopQA leverages Wikipedia’s complex structure to pose complex reasoning challenges. Questions are derived from real-world knowledge graphs and require navigating reasoning paths across multiple documents. Topics span science, politics, and sports, emphasizing logical relationships such as cause-effect dependencies, making it an essential tool for evaluating structured knowledge reasoning. 

\parhead{Vendi retrieval improves document retrieval diversity.} To assess the impact of the Vendi retrieval process on retrieval diversity, we compared the diversity of outputs from Vendi-RAG against Adaptive-RAG and Adaptive Retrieval. We measured diversity using two different metrics, the VS and the max pairwise distance (MPD). Table~\ref{tab:vs_mpd_comparison} summarizes the results. Vendi-RAG achieves higher diversity compared to Adaptive Retrieval and Adaptive-RAG on all dataset, demonstrating its ability to retrieve documents that capture multiple perspectives relevant to the query. This is a crucial advancement, as increased diversity in retrieval directly correlates with improved robustness in multi-hop reasoning (see Table \ref{tab:main:gpt}). Adaptive-RAG, which incorporates iterative refinement but lacks explicit diversity control, shows moderate retrieval diversity improvement over Adaptive Retrieval. 

\parhead{Accuracy on multi-hop QA tasks.} We further evaluated the performance of the Vendi-RAG pipeline, to assess its ability to reason across multiple documents. The results in Table~\ref{tab:main:gpt} indicate that Vendi-RAG consistently outperforms other methods in response accuracy across all datasets, showcasing the efficacy of balancing retrieval diversity with quality. Additionally, Vendi-RAG achieves competitive performance in Exact Match (EM) and F1 score. These findings highlight Vendi-RAG's capability to enhance answer correctness for complex reasoning tasks through improved document retrieval.

\begin{table*}[t]
\caption{Performance on multi-hop QA datasets using GPT-3.5 (Turbo). Here we use three different flavors of accuracy: exact match (EM), F1-score (F1), and traditional accuracy (Acc). Vendi-RAG outperforms the baselines in all 3 datasets, except in terms of F1-score, where it performs similarly to Adaptive-RAG.}
\vspace{-0.1in}
\label{tab:main:gpt}
\small
\centering
\resizebox{\textwidth}{!}{
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{lllcccccccccccc}
\toprule
\midrule
& & & \multicolumn{3}{c}{ MuSiQue} & \multicolumn{3}{c}{ HotpotQA} & \multicolumn{3}{c}{ 2WikiMultiHopQA} \\
\cmidrule(l{2pt}r{2pt}){4-6} \cmidrule(l{2pt}r{2pt}){7-9} \cmidrule(l{2pt}r{2pt}){10-12}
{Steps} & {Types} &{Methods} & EM & F1 & Acc & EM & F1 & Acc & EM & F1 & Acc \\
\midrule

Single-step& Simple 
& {No Retrieval} & 20.40 & 31.30 & 24.40 & 37.40 & 51.04 & 43.20 & 37.00 & 48.50 & 43.40 \\
& & {Single-step Approach} & 16.40 & 26.70 & 23.60 & 39.60 & 50.44 & 45.60 & 46.80 & 57.69 & 52.60 \\
\noalign{\vskip 0.25ex}\cdashline{2-12}\noalign{\vskip 0.75ex}
Multi-step & Adaptive 
& {Adaptive Retrieval} & 18.80 & 30.30 & 24.80 & 38.60 & 50.70 & 43.20 & 44.20 & 55.11 & 50.60 \\
& & {Adaptive-RAG} & 21.80 & \textbf{32.60} & 29.60 & 40.40 & 52.56 & 47.00 & 46.60 & \textbf{60.09} & 56.80 \\
& & \textbf{Vendi-RAG} & \textbf{24.4} & 32.52 & \textbf{30.4} & \textbf{42.2} & \textbf{57.04} & \textbf{58.4} & \textbf{47.2} & 58.9 & \textbf{61.4} \\
\bottomrule
\end{tabular}
}
\end{table*}

\parhead{Impact of the number of retrieved documents on performance.} To further examine the impact of document size on retrieval effectiveness, we analyze the performance of Vendi-RAG and Adaptive-RAG across varying document sizes. Figure~\ref{fig:rag-doc_size} illustrates the relationship between document size and performance on the HotPotQA dataset. Vendi-RAG consistently outperforms Adaptive-RAG in accuracy for document sizes greater than two. As document size increases, accuracy improves for both methods, but the gain is notably higher for Vendi-RAG. Similar to accuracy, EM and F1 scores exhibit an increasing trend as document size grows. Vendi-RAG shows a more pronounced improvement, underscoring its capacity to retrieve more informative and relevant documents, thereby enhancing answer quality.

The VS also increases with document size. This is evidence that Vendi-RAG alleviates redundancy since the VS is known to be invariant under duplication~\citep{friedman2023vendi}. An increasing VS indicates less redundancy in the retrieved documents. By leveraging the VS in its retrieval process, Vendi-RAG avoids the redundancy issue that often plagues RAG pipelines. 

These results indicate that increasing document size enhances both retrieval diversity and answer correctness. However, the degree of improvement varies across methods, with Vendi-RAG achieving superior gains in all metrics. However, we are computationally bottlenecked primarily by the LLM's context window limitation and processing time. As the number of retrieved documents increases, we must either truncate documents to fit within the model's maximum context length or process documents in multiple batches, both of which have significant computational overhead. The bottleneck occurs specifically in the final stage of the pipeline where the LLM processes the retrieved documents to generate answers.

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\linewidth]{figures/per-doc.pdf}
\caption{Performance comparison of Vendi-RAG and Adaptive-RAG across different document sizes in terms of Exact Match, F1-score, Accuracy, and Vendi Score on HotPotQA. The backbone LLM here is GPT-4o-mini. Vendi-RAG consistently outperforms Adaptvie-RAG on all the metrics. In particular, performance improves as the number of documents increases.}
\label{fig:rag-doc_size}
\end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.9\columnwidth]{figures/vendiragllms.pdf}
\caption{Performance comparison of Vendi-RAG and Adaptive-RAG variants across the three datasets using three evaluation metrics: F1-score, Exact Match, and Accuracy. Results show that Vendi-RAG-4o consistently outperforms other variants across all datasets and metrics, with a particularly strong performance on HotpotQA.}
\label{fig:rag-llm-comparison}
\end{figure*}

\parhead{Performance for different LLM Backbones and retrieval strategies.} To evaluate the impact of different LLM backbones and retrieval strategies on the performance of the Vendi-RAG framework, we conducted experiments using three LLMs: GPT-4o, GPT-4o-mini, and GPT-3.5, across all the multi-hop QA datasets described above. The results, shown in Figure~\ref{fig:rag-llm-comparison}, highlight the effectiveness of Vendi-RAG compared to Adaptive-RAG, the best baseline, across all datasets and backbones, except for F1-score on the 2WikiMultiHopQA dataset. In general, larger LLM backbones, such as GPT-4o, achieve superior performance across all datasets, particularly for tasks requiring complex reasoning and synthesis across multiple documents. However, even with smaller models like GPT-4o-mini, the Vendi-RAG model maintains competitive performance, demonstrating its effectiveness. 

\section{Conclusion}
\label{sec:conclusion}

While retrieval-augmented generation (RAG) has proven effective in enhancing large language model (LLM) performance for domain-specific question-answering (QA) tasks, traditional RAG frameworks often struggle with redundancy, particularly in multi-hop reasoning tasks. To address this shortcoming, we introduce Vendi-RAG, a novel framework that jointly optimizes retrieval diversity and answer quality through an iterative refinement process. Vendi-RAG leverages the Vendi Score and an LLM judge to promote semantic diversity while maintaining relevance during retrieval. Our experiments on HotpotQA, MuSiQue, and 2WikiMultiHopQA demonstrate Vendi-RAG's effectiveness. Specifically, Vendi-RAG outperforms the best baseline by $+4.2\%$ on HotpotQA, $+4.1\%$ on 2WikiMultiHopQA, and $+1.3\%$ on MuSiQue. These gains become even more pronounced as the number of retrieved documents increases, highlighting the importance of retrieval diversity in complex reasoning tasks. Furthermore, we evaluated Vendi-RAG across multiple LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and observed consistent performance improvements, demonstrating that the framework is model-agnostic. These findings establish Vendi-RAG as an effective and adaptable solution for multi-hop QA. 

\parhead{Limitations.}
Vendi-RAG introduces computational overhead due to LLM-based quality scoring, which may limit scalability in real-time applications. Additionally, like all RAG approaches, its performance depends on the quality and completeness of external knowledge sources, making it susceptible to biases or gaps in the retrieved information.

\subsection*{Acknowledgements}
Adji Bousso Dieng acknowledges support from the National Science Foundation, Office of Advanced Cyberinfrastructure (OAC): \#2118201. She also acknowledges Schmidt Sciences for the AI2050 Early Career Fellowship.

\bibliographystyle{apa}
\bibliography{arxiv}

\end{document}
