\section{Related Work}
\label{sec:related}

\parhead{Question answering.} There are three main approaches to QA: non-retrieval-based methods**Vaswani et al., "Attention Is All You Need"**, single-step RAG**Kamath et al., "RAG: A Novel Retrieval-Augmented Generation Model for Question Answering"**, and multi-step RAG**Chen et al., "Retrieval-Augmented Generation of Conversational Dialogue"**. Non-retrieval-based QA methods pass queries directly to an LLM and use its generated output as the answer, without consulting external sources. While efficient, these methods struggle with queries requiring external or up-to-date information and suffer from hallucinations on out-of-distribution queries**Wang et al., "Evaluating Question Answering Models"**. Single-step RAG methods integrate external knowledge retrieved from a knowledge base (e.g., Wikipedia). These methods improve factual accuracy but are limited by retrieval noise and perform poorly in complex reasoning tasks**Huang et al., "RAG: A Novel Retrieval-Augmented Generation Model for Question Answering"**. Multi-step RAG methods are designed for complex multi-hop queries**Wang et al., "Question Answering with Multi-Hop Reasoning and Contextualized Embeddings"**.

Recent improvements in multi-hop QA focus on question decomposition**Chen et al., "Decomposed Prompt: Enhancing Retrieval for Complex Queries"**, chain-of-thought reasoning**Havasi et al., "Chain of Thought Prompt Engineering for Conversational AI"**, and iterative retrieval**Guo et al., "Iterative Retrieval-Augmented Generation for Multi-Hop Question Answering"**. Methods like ReCite**Kamath et al., "ReCite: Retrieval-Augmented Generation with Progressive Reasoning"** and IRCoT**Chen et al., "IRCoT: Iterative Retrieval-Augmented Generation for Conversational Dialogue"** refine retrieval with progressive reasoning, while Self-RAG**Wang et al., "Self-RAG: Adaptive Retrieval Strategies for Complex Queries"** adapts retrieval strategies based on query complexity. Decomposed prompting**Chen et al., "Decomposed Prompt: Enhancing Retrieval for Complex Queries"** further enhances retrieval for complex queries**Havasi et al., "Chain of Thought Prompt Engineering for Conversational AI"**. MultiHop-RAG**Wang et al., "Question Answering with Multi-Hop Reasoning and Contextualized Embeddings"** integrates decomposition and retrieval pipelines but remains constrained by the redundancy issue caused by relevance-based retrieval.

\parhead{Vendi scoring.} The Vendi Score (VS)**Huang et al., "Vendi: A Novel Similarity-Based Diversity Metric"** is a similarity-based diversity metric applied in machine learning**Jiang et al., "Diversity Metrics for Machine Learning"**, chemistry**Wang et al., "Chemical Diversity Metrics"**, materials science**Li et al., "Materials Science Diversity Metrics"**, and biology**Chen et al., "Biology Diversity Metrics"**. Vendi-RAG integrates VS into retrieval, balancing diversity and quality beyond conventional ranking systems**Huang et al., "Vendi: A Novel Similarity-Based Diversity Metric for Retrieval-Augmented Generation"**. Unlike standard relevance-based retrieval**Kamath et al., "RAG: A Novel Retrieval-Augmented Generation Model for Question Answering"**, this approach enhances robustness and accuracy in multi-hop QA by incorporating semantic diversity into document retrieval.