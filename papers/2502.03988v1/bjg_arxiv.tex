\documentclass[12pt]{article}

\usepackage{arxiv}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{doi}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdfpagemode=FullScreen,
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%     
          %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
               %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                    %%%%%%%%%%%%%%%%%
                         %%%%%%
                         

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}

\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}

\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem*{thm1}{Theorem 2.1 of \citet{lee2021further}}


\theoremstyle{definition}
\newtheorem{rem}{Remark}
%\usepackage{breakurl}
% \usepackage{url}
\usepackage{tikz}
\usepackage{bm}
\usepackage{multirow}
\usepackage{multicol}

\usepackage{xcolor}


\def\E{\mathbb{E}}
\def\R{\mathbb{R}}
\def\P{\mathbb{P}}


\usepackage{url}

\usepackage{color}

\newcommand{\rew}[1]{\textcolor{blue}{#1}}
\newcommand{\new}[1]{\textcolor{red}{#1}}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}



                         %%%%%%
                    %%%%%%%%%%%%%%%%%
               %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
          %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%     
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\title{Tight Bounds on Jensen's Gap: Novel Approach with Applications in Generative Modeling}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
\date{} 					% Or removing it

\author{ 
% 	\href{https://orcid.org/0000-0000-0000-0000}
% 	{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}\L{}ukasz Struski}
% 	\thanks{Use footnote for providing further
% 		information about author (webpage, alternative
% 		address)---\emph{not} for acknowledging funding agencies.} \\
	Marcin Mazur \\
	\texttt{marcin.mazur@uj.edu.pl} \\
	\And
         Piotr Ko\'scielniak \\
	\texttt{piotr.koscielniak@uj.edu.pl} \\
        \And
	\L{}ukasz Struski \\
	\texttt{lukasz.struski@uj.edu.pl} \\
	\And
	\\[-2em]
	Faculty of Mathematics and Computer Science \\ 
	Jagiellonian University in Krak\'ow, Poland \\[-1em]
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}

% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{Tight Bounds on Jensen's Gap}



\begin{document}

\maketitle


\begin{abstract}
Among various mathematical tools of particular interest are those that provide a common basis for researchers in different scientific fields. One of them is Jensen's inequality, which states that the expectation of a convex function is greater than or equal to the function evaluated at the expectation. The resulting difference, known as Jensen's gap, became the subject of investigation by both the statistical and machine learning communities. Among many related topics, finding lower and upper bounds on Jensen's gap (under different assumptions on the underlying function and distribution) has recently become a problem of particular interest. In our paper, we take another step in this direction by providing a novel general and mathematically rigorous technique, motivated by the recent results of \citet{pmlr-v206-struski23a}.
% , which improves on the approaches proposed by \citet{liao2019sharpening,lee2021further}.
In addition, by studying in detail the case of the logarithmic function and the log-normal distribution, we explore a method for tightly estimating the log-likelihood of generative models trained on real-world datasets. Furthermore, we present both analytical and experimental arguments in support of the superiority of our approach in comparison to existing state-of-the-art solutions, contingent upon fulfillment of the criteria set forth by theoretical studies and corresponding experiments on synthetic data.
\end{abstract}

%% Keywords
\noindent\textbf{Keywords:} Jensen's inequality, variational inference, deep learning, generative model




%% main text
%%

%% Use \section commands to start a section
\section{Introduction}

Since its first publication in \citep{jensen1906fonctions}, Jensen's inequality significantly influenced different scientific fields, including pure and applied mathematics, %\cite{brnetic2015refinement, horvath2021extensions, saeed2022refinements}, 
statistical inference, and %\cite{dragomir2013some,jebara2001reversing,nielsen2010family}, 
machine learning. %\cite{dayan1997using,williams2017information,...}, 
%and biological studies \cite{ruel1999jensen}. 
In the probabilistic setting, it proclaims the nonnegativity of the so-called {\em Jensen's gap}, which (in its general form) is given by the following formula:
\begin{equation}\label{eq:jensensgap}
\mathcal{JG}(f,X)=\E\{f(X)\}-f(\E X),
\end{equation}
where $X$ is a random variable and $f$ is a convex function. Probably the most valuable setting 
%for \eqref{eq:jensensgap} 
is the case when $f=-\log$, due to many important applications of such version of Jensen's inequality in variational inference.
%(see, e.g., \cite{blei2017variational}). 
Therefore, the term {\em variational gap} has been frequently used instead. (Nevertheless, our paper refers to the notion of Jensen's gap regardless of the context.)

Recently, the problem of estimating the size of Jensen's gap has been intensively investigated. Most authors were particularly concerned about finding upper or lower bounds for $\mathcal{JG}(f,X)$ under various assumptions on $f$ and/or $X$. Specifically, 
notable bounds were provided for analytic or superquadratic $f$ (see \citep{abramovich2004refining,dragomir2015inequality,walker2014lower,banic2008superquadratic,lovrivcevic2018zipf}), 
and for $X$ that follows a distribution for which (central) moments or more complicated expectations are known
(see \citep{abramovich2004refining,dragomir2001some,dragomir2015inequality,gao2017bounds,liao2019sharpening,pecaric1985companion,walker2014lower}).
On the other hand, many related results obtained for the logarithmic function $f$ were motivated by the necessity of applying variational inference methods in training machine learning models to deal with the problem of the intractability of the log-likelihood of data. In such situations, optimizing appropriate lower (or occasionally upper) bounds instead occurred as an effective solution. Among others, this was the case of variational autoencoder (VAE) \citep{kingma2013auto,rezende2015variational}, which used the evidence lower bound (ELBO) derived directly from Jensen's inequality and therefore suffered from the presence of variational gap. Consequently, finding and tightening bounds on the log-likelihood of the model distribution (and hence the associated variational gap) became an important issue that was intensively investigated by the machine learning community (see, e.g., \citep{burda2015importance,dieng2017variational,ji2019stochastic,grosse2015sandwiching,maddison2017filtering,masrani2019thermodynamic,nowozin2018debiasing, pmlr-v206-struski23a}). Further details concerning related work may be found in Section~\ref{sec:relwork}.

%\citep{khan2020new}
%~\citep{bayer2021mind}


In this paper, following the ideas of \citet{pmlr-v206-struski23a} we introduce novel general lower and upper bounds for Jensen's gap $\mathcal{JG}(f,X)$. Involving different sets of assumptions on $f$ and $X$, with particular attention paid to the cases of exponential or logarithmic $f$, we provide both analytical and empirical arguments that our approach is superior to those presented in \citep{pmlr-v206-struski23a,liao2019sharpening,lee2021further} in a number of situations. Specifically, experiments conducted on real-world data demonstrate that our approach may prove more effective than existing techniques for estimating the log-likelihood of generative models, provided that certain criteria are met.

% Specifically, our contribution can be summarized as follows:
% \begin{itemize}
% \item[(i)] we introduce novel general and rigorous lower and upper bounds for Jensen's gap $\mathcal{JG}(f,X)$ under mild (natural) assumptions on $f$ and $X$,
% \item[(ii)] motivated by \citet{pmlr-v206-struski23a,liao2019sharpening,lee2021further}, we compute (both analytically and empirically) our bounds for several examples of exponential or logarithmic functions $f$ and random variables $X$ with exponential, (log-)normal, or gamma distributions, demonstrating the significance of our approach compared to the state-of-the-art,
% \item[(iii)] by studying in detail the case when $f=-\log$ and using the log-normal limit approximation (see \citep{mouri2013log}), we present an effective method for finding tight lower and 
% % (fully additive\footnote{This means that they are expressed as expected values of random variables depending on $X$ and thus admit unbiased sample mean estimators (note that this is particularly important for the training process, as it is usually performed on mini-batches, and therefore factorizing an objective function as a sum over the input dataset would be beneficial).}) 
% upper bounds for $\log(\E(X))$,
% \item[(iv)] we use the obtained bounds for $\log(\E(X))$ to introduce a novel method of log-likelihood estimation for autoencoder-based generative models (such as VAEs) and perform experiments on real-world datasets showing that it outperforms the state-of-the-art when working on data and models that allow the proposed criteria to be met (we benchmark against the experimental setup of \citet{pmlr-v206-struski23a}).

% % where the approaches of \citet{pmlr-v206-struski23a, dieng2017variational,masrani2019thermodynamic,ji2019stochastic} are compared for VAE and IWAE models).
% \end{itemize}

\section{Related work}\label{sec:relwork}

In the general setting, several researchers discussed the problem of Jen\-sen's gap estimation. 
% For example, \cite{gao2017bounds} provided lower and upper bounds depending on some function properties and certain moments of the random variable. On the other hand, a
In particular, a variance-based lower bound for strongly convex functions was proposed by \citet{bakula2016converse}, then improved and extended (to an upper bound) by \citet{liao2019sharpening}, and further by \citet{lee2021further}. Both of the latter results were based on Taylor's expansion and had insightful forms both in terms of function derivatives and distribution moments of second and higher order. However, these approaches often yielded boundary values (i.e., 0 and $\infty$) when used to compute bounds for common continuous (e.g., Gaussian) distributions. (In such cases, the authors suggested using a support partitioning method due to \citet{walker2014lower}.) In \ref{sec:other}, we present a brief outline of the results of \citet{liao2019sharpening} and \citet{lee2021further}, as we refer to them in Section \ref{sec:applications}.
Other related results have been developed (see, e.g., \citep{abramovich2004refining,dragomir2001some,dragomir2015inequality,horvath2014refinement,pecaric1985companion,walker2014lower,lovrivcevic2018zipf,banic2008superquadratic}), but they either had a complicated form or required additional assumptions about the analyticity or superquadraticity of the function.

In the machine learning setting, the most interesting bounds on Jensen's gap have been those obtained for logarithmic functions. This was due to the intractability of the log-likelihood of generative models designed using variational inference methods. Training such models as variational autoencoders (VAEs) \citep{kingma2013auto,rezende2015variational} or importance-weighted autoencoders (IWAEs) \citep{burda2015importance} was instead reduced to maximization of the respective lower bounds (i.e, ELBO for VAEs and IW-ELBO for IWAEs). On the other hand, several approaches focused on finding the effective upper bound were also considered. Among others, the following proposals were introduced and investigated: $\chi$ upper bound (CUBO) \citep{dieng2017variational}, evidence upper bound (EUBO) \citep{ji2019stochastic}, and an upper bound variant of the thermodynamic variational objective (TVO) \citep{masrani2019thermodynamic}, which was a generalization of EUBO. However, the approach of our particular interest was proposed by~\citet{pmlr-v206-struski23a}, who studied in detail both the general case and the case of the logarithmic function. As the results presented there became the direct motivation for our work, we refer to them many times throughout the paper. On the other hand, the approaches of \citet{dieng2017variational,ji2019stochastic} and \citet{masrani2019thermodynamic} are briefly outlined in \ref{sec:logbounds}.


\section{Theory}\label{sec:theory}

As we have already emphasized, our contribution is supported by rigorous mathematical results. We present them in a consistent manner, starting from a general framework and ending with the case of the logarithmic function and the log-normal distribution, which is particularly important for further applications (see Section \ref{sec:applications}).

\subsection{General bounds}\label{sec:generalbounds}
In this subsection, we present the main results of the paper in the general case. First, we recall simple bounds for Jensen's gap given in \citep{pmlr-v206-struski23a} that involve low-order moments, and then we improve them using higher-order expectations.

Let $f\colon (a,b) \to \mathbb{R}$ be a continuous function and $X\colon \Omega\to (a,b)$ be a random variable on a probability space $(\Omega, \Sigma, P)$. In addition, to make Jensen's gap $\mathcal{JG}(f,X)$ (given in \eqref{eq:jensensgap}) well defined, assume that $X$ and $f(X)$ have finite first moments.

%\subsection{Simple bounds}

\begin{thm}[\citet{pmlr-v206-struski23a}]\label{thm:simple}
If $f$ is a twice differentiable convex function, then we have the following inequalities: 
\begin{equation}\label{eq:simpleboundsold}
\begin{array}{@{}l @{\;} l @{\;} l}
0\leq \mathcal{JG}(f,X) & \leq & \E\{Xf'(X)\}-\E X \E\{f'(X)\}
 =  \mathrm{cov}\{X,f'(X)\},  
\end{array}
\end{equation}
provided that appropriate finite expected values exist.
\end{thm}
\begin{proof} (Although the proof presented here comes from \citep{pmlr-v206-struski23a}, we provide it for the convenience of the reader, since we develop this idea further in the proof of Theorem \ref{thm:improved}.) Using Taylor's expansions in $\E X$ and in  arbitrary $x\in (a,b)$, we obtain
\begin{equation}\label{eq:lowertaylor}
\begin{array}{@{}l @{\;} l @{\;} l}
f(x) & \geq & f(\E X)+f'(\E X )(x-\E X),
\end{array}
\end{equation}
%where $\xi\colon (a,b)\to \mathbb{R}$ is a function such that $\xi(x)\in (\min\{x,\E(X)\}, \max\{x,\E(X)\})$.
\begin{equation}\label{eq:uppertaylor}
\begin{array}{@{}l @{\;} l @{\;} l}
f(\E X ) & \geq & f(x)+f'(x)(\E X -x).
\end{array}
\end{equation}
Then, by putting $x=X(\omega)$ and integrating the above inequalities over all $\omega\in \Omega$, we conclude that
\begin{equation}\label{eq:lowertaylorint}
\begin{array}{@{}l @{\;} l @{\;} l}
\E\{f(X)\} & \geq & f(\E X )+f'(\E X )(\E X -\E X ) =  f(\E X ),
\end{array}
\end{equation}
\begin{equation}\label{eq:uppertaylorint}
\begin{array}{@{}l @{\;} l @{\;} l}
f(\E X) & \geq & \E\{f(X)\}+\E X \E\{f'(X)\}-\E\{Xf'(X)\}\\ 
& = & \E\{f(X)\}-\mathrm{cov}\{X,f'(X)\},
\end{array}
\end{equation}
and thus the assertion.
\end{proof}
%\subsection{Improved bounds}

Following the ideas behind the proof of Theorem \ref{thm:simple}, to improve the bounds given in \eqref{eq:simplebounds}  we apply higher order Taylor's expansions, which leads to the following general result.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{thm}\label{thm:improved}
If $f$ is a $2k$-differentiable function satisfying $f^{(2k)}(x)\geq 0$ for any $x\in (a,b)$,
%, and $X$ has finite all moments of orders less than $2k$, and there exist $\E(f^{(i)}(X)X^{i-j})$ for all $i=1,\ldots, 2k-1$, $0\leq j\leq i$, 
then we have the following inequalities:
\begin{equation}\label{eq:simpleboundsimp1}
\begin{array}{@{}l @{\;} l @{\;} l}
\sum_{i=1}^{2k-1}  a_{i,0}  f^{(i)}  (\E X)\E\{(X&\!-&\!\E X)^i\} \leq  \mathcal{JG}(f,X)\\
 &\leq &  -\sum_{i=1}^{2k-1}a_{i,i} \E\{f^{(i)}(X)(X-\E X)^{i}\},
\end{array}
\end{equation}
and (equivalently)
\begin{equation}\label{eq:simpleboundsimp}
\begin{array}{@{}l @{\;} l @{\;} l}
 \sum_{i=1}^{2k-1}  \sum_{j=0}^i   a_{i,j}  f^{(i)}(\E X)&\!\E& \!\!\!X^{i-j}(\E X)^{j} \leq  \mathcal{JG}(f,X)\\
 & \leq & -\sum_{i=1}^{2k-1}\sum_{j=0}^ia_{i,j}\E\{f^{(i)}(X)X^{j}\}(\E X)^{i-j},
\end{array}
\end{equation}
where $a_{i,j}=\frac{(-1)^{j}}{i!}{i\choose j}=\frac{(-1)^{j}}{j!(i-j)!}$, provided that appropriate finite expected values exist.
\end{thm}



\begin{proof} Using Taylor's expansion in $\E X$ and in  arbitrary $x\in (a,b)$, we obtain
\begin{equation}\label{eq:lowertaylorimp}
\begin{array}{@{}l @{\;} l @{\;} l}
f(x)-f(\E X)& \geq & \sum_{i=1}^{2k-1}\frac{1}{i!}f^{(i)}(\E X)(x-\E X)^i,
\end{array}
\end{equation}
and thus
\begin{equation}\label{eq:lowertaylorimp_02}
\begin{array}{@{}l @{\;} l @{\;} l}
f(X)-f(\E X) & \geq & \sum_{i=1}^{2k-1}\frac{1}{i!}f^{(i)}(\E X)(X-\E X)^i. 
\end{array}
\end{equation}
Then, integrating the above inequality over all $\omega\in \Omega$, we have 
\begin{equation}\label{eq:lowertaylorimp_03}
\begin{array}{@{}l @{\;} l @{\;} l}
\int_\Omega f(X)dP & - & \int_\Omega f(\E X)dP \geq  \sum_{i=1}^{2k-1}\frac{1}{i!} f^{(i)}(\E X)\int_\Omega(X-\E X)^idP 
\end{array}
\end{equation}
and consequently the lower bound in (\ref{eq:simpleboundsimp1}). Using Newton's formula we obtain the lower bound in (\ref{eq:simpleboundsimp}).

Now we use Taylor's expansion in $x\in (a,b)$ and we obtain 
\begin{equation}\label{eq:uppertaylorimp}
\begin{array}{@{}l @{\;} l @{\;} l}
f(\E X)-f(x) & \geq & \sum_{i=1}^{2k-1}\frac{1}{i!}f^{(i)}(x)(\E X-x)^i,
\end{array}
\end{equation}
and thus 
\begin{equation}\label{eq:uppertaylorimp_02}
\begin{array}{@{}l @{\;} l @{\;} l}
f(X)-f(\E X) & \leq & -\sum_{i=1}^{2k-1}\frac{1}{i!}f^{(i)}(X)(\E X-X)^i. 
\end{array}
\end{equation}
Then, by integrating the above inequality over all $\omega\in \Omega$, we have 
\begin{equation}\label{eq:uppertaylorimp_03}
\begin{array}{@{}l @{\;} l @{\;} l}
\int_\Omega & f(X) & dP - \int_\Omega f(\E X)\; dP \leq -\sum_{i=1}^{2k-1}\frac{1}{i!}(-1)^i\int_\Omega f^{(i)}(X)(X-\E X)^idP,
\end{array}
\end{equation}
which implies the upper bound in (\ref{eq:simpleboundsimp1}). Again, by applying Newton's formula, we obtain the upper bound in (\ref{eq:simpleboundsimp}). It completes the proof. 
\end{proof}

An immediate consequence of the above theorem is the following corollary, which gives Jensen's gap bounds 
for zero-mean distributions.

\begin{cor}\label{cor:improvedzero}
Under the assumptions of Theorem \ref{thm:improved}, if $\E X=0$ then we have the following inequalities:
\begin{equation}\label{eq:simpleboundsimp2}
\begin{array}{@{}l @{\;} l @{\;} l}
\sum_{i=1}^{2k-1}a_{i,0}f^{(i)}(0)\E X^i &\leq &\mathcal{JG}(f,X)
\leq  -\sum_{i=1}^{2k-1}a_{i,i}\E\{f^{i}(X)X^{i}\}.
\end{array}
\end{equation}

% $$
% Y=X-\E X, \; g(x)=f(x+\E X)
% $$ &\end{&array}

\end{cor}

The following remark demonstrates the application of Theorem~\ref{thm:improved} to superquadratic functions within the context of the findings presented in  \citep{abramovich2004refining} and \citep{abramovich2022new}. Recall that a function $f\colon [0,B) \rightarrow \R$ is called superquadratic if, for all $x \in [0,B)$, there exists a constant $C_f(x) \in \R$ such that the inequality
\begin{equation}\label{eq:superq}
\begin{array}{@{}l @{\;} l @{\;} l}
f(y)-f(x)-C_f(x) (y - x) - f (|y - x|) & \geq & 0
\end{array}
\end{equation}
holds for all $y \in [0,B)$.

\begin{rem}\label{rem:abram} 
% Adding some assumptions we can improve the following theorem. 
% \begin{thm}[reformulated Theorem 4  in \cite{abramovich2022new}]\label{thm:abram2022}
Let $f\colon [0,\infty)\to \mathbb{R}$ be a superquadratic function that is continuously differentiable with $f(0)=f'(0)=0$ and four times continuously differentiable on $(0,\infty)$ with $f^{(3)}(x)> 0$ and $f^{(4)}(x)\geq 0$ for any $x\in (0,\infty)$. 
% Suppose also that $f'$ is a strictly convex function.
Then, from Theorem 4 of \citet{abramovich2022new}, there exists an interval $[a,b]$, where $a>0$, such that for any discrete random variable $X$ with  $P(X=x_i)=p_i$, where $a=x_1<\ldots <x_n=b$, we have
\begin{equation}\label{eq:Abram}
\begin{array}{@{}l @{\;} l @{\;} l}
 \E\{f(X)\}-f(\E X) -\E\{f(|X-\E X|)\} &\geq &\frac{1}{2}f''(a)\mathrm{var} X -\frac{f(b-a)}{(b-a)^2}\mathrm{var} X. 
\end{array}
\end{equation}
% \end{thm}
% Now, let $f$ be a superquadratic function that is continuously differentiable
% on $x \geq 0$ and four times continuously differentiable on $x>0$, and $f(0)=
% f'(0)=0$. Let $f'$ be a strictly convex function and let $f^{(4)}(x)\geq 0$.  Then there is an interval $[a,b]$, $a>0$, such that for any discrete random variable $X$ with $P(X=x_i)=p_i$, $a=x_1<\cdots <x_n=b$ and
But assuming additionally that $X$ has a right-skewed distribution, i.e., \linebreak $\E\{(X-\E X)^{3}\}\geq 0$, one can easily use a similar reasoning combined with Theorem~\ref{thm:improved} to prove the following inequality:\begin{equation}\label{eq::super}
 \begin{array}{@{}l @{\;} l @{\;} l}
 \E\{f(X)\}-f(\E X) -\E\{f(|X-\E X|)\}&\geq &\frac{1}{2}f''(\E X)\mathrm{var} X -\frac{f(b-a)}{(b-a)^2}\mathrm{var} X,
 \end{array}
\end{equation}
which gives a stricter estimate than \eqref{eq:Abram}, since $f''$ is an increasing function.
Indeed, from Theorem~\ref{thm:improved} in follows that
\begin{equation}\label{eq::proof1}
 \begin{array}{@{}l @{\;} l @{\;} l}
\E\{f(X)\}-f( \E X) &\geq &\frac{1}{2}f''(\E X)\mathrm{var} X +\frac{1}{6}f^{(3)}(\E X) \E\{(X-\E X)^3\}\\
&\geq& \frac{1}{2}f''(\E X)\mathrm{var} X,  
\end{array}
\end{equation}
so we obtain (\ref{eq::super}) by observing that $\E\{f(|X-\E X|)\} \leq \frac{f(b-a)}{(b-a)^2}\mathrm{var} X$ (see the proof of Theorem 4 in \citep{abramovich2022new}). 
% It should be noted that, in consequence, since $f''$ is increasing, we have $f''(a)\leq f''(\E X)$ and thus the (\ref{eq::super}) is a better result than (\ref{eq:Abram}).

On the other hand, note that the case where $f$ satisfies $f^{(3)}(x)\geq 0$ and $f^{(4)}(x)\leq 0$ for $x>0$ (e.g., $f(x)=x^2\log x$ for $x>0$ and $f(0)=0$) is handled by superquadraticity, but not by the techniques used here.

\end{rem}

We conclude this subsection by noting a condition that enhances the precision of the estimates presented in Theorem~\ref{thm:improved}. Specifically, we draw inspiration from Theorem 6 of \citet{pmlr-v206-struski23a} to assert that this is the case, for example, for random variables concentrated around their means. More precisely, we assume that for some small $\varepsilon>0$, the condition $|X - \E X|<\varepsilon$ is satisfied almost surely. However, such an assertion is of limited practical utility, as we discuss at the conclusion of the following subsection.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Bounds for logarithmic function}\label{subsec:boundslog}

In this subsection, we present the results of the paper in the case of $f=-\log$. We retain the structure of Subsection \ref{sec:generalbounds}, giving successively the counterparts of Theorems \ref{thm:simple} and \ref{thm:improved} (note that Corollary~\ref{cor:improvedzero} can no longer be retained).

Let $X\colon \Omega\to (0,\infty)$ be a random variable on a probability space $(\Omega, \Sigma, P)$ such that $X$ and $\log(X)$ have finite first moments.

The following theorem is a direct consequence of Theorem \ref{thm:simple}, applied to $f=-\log$.


\begin{thm}[\citet{pmlr-v206-struski23a}]\label{thm:simplelog}
We have the following inequalities: 
\begin{equation}\label{eq:simplebounds}
\begin{array}{@{}l @{\;} l @{\;} l}
0 & \leq & \mathcal{JG}(-\log,X)\leq \E X\E X^{-1}-1=\mathrm{cov}(X,X^{-1}),  
\end{array}
\end{equation}
provided that appropriate finite expected values exist.
\end{thm}
% \begin{proof} It is enough to use Theorem \ref{thm:simple} for $f=-\log$.
% \end{proof}

To derive (from Theorem \ref{thm:improved}) pleasant forms of improved bounds for the logarithmic function $f$, non-obvious combinatorial calculus must be employed.

%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{thm}\label{thm:improvedlog}
% If $f$ is a $2k$-differentiable function satisfying $f^{(2k)}(x)\geq 0$ for any $x\in (a,b)$,
For any positive integer $k$, we have the following inequalities:
\begin{equation}\label{eq:improveboundslog1}
\begin{array}{@{}l @{\;} l @{\;} l}
\sum_{i=1}^{2k-1}\frac{(-1)^i}{i}  (\E X)^{-i}\E\{(X-\E X)^i\} &\leq &  \mathcal{JG}(-\log,X) \\
&\leq & -\sum_{i=1}^{2k-1}\frac{1}{i} \E\{X^{-i}(X-\E X)^{i}\},
\end{array}
\end{equation}
and (equivalently)
\begin{equation}\label{eq:improvedboundslog}
\begin{array}{@{}l @{\;} l @{\;} l}
\sum_{j=1}^{2k-1}\frac{1}{j} + b_{k,j}\E X^{j}(\E X)^{-j} &\leq &\mathcal{JG}(-\log,X) \\
&\leq & -\sum_{j=1}^{2k-1}\frac{1}{j}+b_{k,j}\E X^{-j}(\E X)^{j},
\end{array}
\end{equation}
where $b_{k,j}=\frac{(-1)^{j}}{j}{2k-1 \choose j}$, provided that appropriate finite expected values exist.
\end{thm}

% \begin{thm}\label{thm:boundsimplog}
% Under the above assumptions, if $X$ has finite all moments of orders less than $2k$, then we have: 
% \begin{equation}\label{eq:simpleboundsimp:left}
% \sum_{i=1}^{2k-1}\sum_{j=0}^i\frac{(-1)^{j}i!}{j!(i-j)!}\E(X^j)\E(X)^{-j}\leq \log(\E(X))-\E(\log(X) )
% \end{equation}
% and if $X^{-1}$ has finite all moments of orders less than $2k$, then we have:
% \begin{equation}\label{eq:simpleboundsimp:right}
% \log(\E(X))-\E(\log(X) )\leq \sum_{i=1}^{2k-1}\sum_{j=0}^i\frac{(-1)^{j-1}i!}{j!(i-j)!}\E(X^{-j})\E(X)^j.  
% \end{equation}
%\end{thm}
\begin{proof} To obtain \eqref{eq:improveboundslog1}, it is enough to use \eqref{eq:simpleboundsimp1} %from Theorem \ref{thm:improved} 
for $f=-\log$ (note that then $f^{(i)}(x)=(-1)^i(i-1)!x^{-i}$). To prove \eqref{eq:improvedboundslog}, we apply \eqref{eq:simpleboundsimp} and appropriate combinatorial calculus. Below we present the details for the right-hand side inequality, the remaining inequality can be derived analogously.

First, note that 
%(using Newton's binomial formula in??) 
using \eqref{eq:simpleboundsimp} for $f=-\log$ leads to the following estimate: 
\begin{equation}\label{eq:simpleboundsimp_log}
%\sum_{i=1}^{2k-1}\sum_{j=0}^i\frac{(-1)^{i+j}}{i}{i \choose j}\E(X^{j-i})\E(X)^{i-j}\leq 
\begin{array}{@{}l @{\;} l @{\;} l}
\log(\E X) & - & \E\{\log(X)\}
 \leq  -\sum_{i=1}^{2k-1}\sum_{j=0}^i\frac{(-1)^{i+j}}{i}{i \choose j}\E X^{j-i}(\E X)^{i-j},
\end{array}
\end{equation}
which can be rewritten as 
\begin{equation}\label{eq:simpleboundsimp_02}
%\sum_{i=1}^{2k-1}\sum_{j=0}^i\frac{(-1)^{j}}{i}{i \choose j}\E(X^{-j})\E(X)^{j}\leq 
\begin{array}{@{}l @{\;} l @{\;} l}
\log(\E X) & - & \E\{\log(X)\}\leq  -\sum_{i=1}^{2k-1}\sum_{j=0}^i\frac{(-1)^{j}}{i}{i \choose j}\E X^{-j}(\E X)^{j},
\end{array}
\end{equation}
and, consequently, as
\begin{equation}\label{eq:simpleboundsimp_03}
%\sum_{j=1}^{2k-1}\sum_{j=0}^i\frac{(-1)^{j}}{i}{i \choose j}\E(X^{-j})\E(X)^{j}\leq 
\begin{array}{@{}l @{\;} l @{\;} l}
& & \! \log(\E X) -  \E\{\log(X)\} \leq -\sum_{i=1}^{2k-1}\frac{1}{i}-\sum_{j=1}^{2k-1}(-1)^{j}\E X^{-j}(\E X)^{j}\sum_{i=j}^{2k-1}\frac{1}{i}{i \choose j}.
\end{array}
\end{equation}
Then, using the formula 
%$\sum_{i=m}^n {i\choose m}= {n+1 \choose m+1}$ and $\frac{m}{i}{i\choose m}= {i-1 \choose m-1}$ we conclude that 
$\sum_{i=j}^n \frac{1}{i}{i\choose j}= \frac{1}{j}{n \choose j}$, we obtain the following inequality:
% \begin{equation}\label{eq:simpleboundsimp_03}
% \log(\E(X))-\E(\log(X) )\leq -H_{2k-1}+\sum_{j=1}^{2k-1}\frac{(-1)^{j+1}}{j}{2k-1 \choose j}\E(X^{j})\E(X)^{-j}.  
% \end{equation}
\begin{equation}\label{eq:simpleboundsimp_04}
\begin{array}{@{}l @{\;} l @{\;} l}
\log(\E X) & - & \E\{\log(X)\} \leq  - \sum_{j=1}^{2k-1}\frac{1}{j}+\frac{(-1)^{j}}{j}{2k-1 \choose j}\E X^{-j}(\E X)^{j},  
\end{array}
\end{equation}
which ends the proof.
\end{proof} 

% \subsection{Bounds for logarithmic function and log-normal distributed random variable}

Note that rescaling the random variable $X$ by a positive constant $a$ (i.e., $X\to aX$) does not change the lower and upper bounds in either \eqref{eq:improveboundslog1} or \eqref{eq:improvedboundslog}.

In the following paragraphs, we examine two essential examples of gamma-distributed and log-normally-distributed random variables. The first case refers to an illustrative example studied in detail in \citep{pmlr-v206-struski23a}, while the other (more important) case concerning log-normally distributed variables can be considered as a basis for our experiments on real-world data (see Subsection \ref{subsec:experiments}), and is therefore followed by necessary additional theoretical outcomes. For a comparison of these results with those that can be obtained using the methods of \citet{liao2019sharpening,lee2021further} and \citet{pmlr-v206-struski23a}, see Subsection \ref{subsec:syntheticdataexperiments}, where appropriate experiments on synthetic data are provided.
% (Note that in these cases the bounds provided by \citet{liao2019sharpening,lee2021further} are trivial (see the supplementary materials), so we do not consider them in our further studies of gamma-distributed and log-normally-distributed random variables.)



\paragraph{Bounds for gamma distributed random variable} Consider the random variable $X\sim \text{Gamma}(a,\theta)$, where $a>0$ and $\theta>0$ are shape and scale parameters. Then  $X^{-1}\sim \text{Inv-Gamma}(a,1/\theta)$ and consequently $\E X^j=\theta^j\frac{\Gamma(a+j)}{\Gamma(a)}$ for any integer $j>-a$. Thus, from \eqref{eq:improvedboundslog}
%Theorem \ref{thm:improvedlog}
and by simple calculation, we obtain the respective bounds for Jensen's gap:
% \piotr{
% and
% \begin{equation}\label{eq:invgammamoments}
% \E(X^{-n})=\frac{1}{\theta^n}\frac{\Gamma(a-n)}{\Gamma(a)}.
% \end{equation}
%}
% \marcin{a to drugie zawiera sie w pierwszym bo mamy $n\in \mathbb{Z}$}
% \begin{thm}\label{thm:lowerloggamma}
% Under the above assumptions, if $a>1$ we have:
\begin{equation}\label{eq:simpleboundsimpgamma}
\begin{array}{@{}l @{\;} l @{\;} l}
\sum_{j=1}^{2k-1}\frac{1}{j} & + & b_{k,j}\frac{\Gamma(a+j)}{\Gamma(a) a^j}\leq \mathcal{JG}(-\log,X)\leq -\sum_{j=1}^{2k-1}\frac{1}{j}+b_{k,j}\frac{\Gamma(a-j )a^j}{\Gamma(a)},  
\end{array}
\end{equation}
provided (only for the upper bound) that $a> 2k-1$.


%\begin{equation}\label{eq:simpleboundsimpgamma}
%\begin{array}{@{}l @{\;} l @{\;} l}
%\sum_{j=1}^{2k-1}\frac{1}{j} & + & %\frac{(-1)^ja^j(2k-j)\cdots (a-j-1)}{jj!(2k)\cdots (a-1)}\leq \mathcal{JG}(-%\log,X)\\
%&\leq &-\sum_{j=1}^{2k-1}\frac{1}{j}+\frac{(-1)^j(2k-j)\cdots (a+j-1)}{jj!a^j(2k)\cdots (a-1)},  
%\end{array}
%\end{equation}
%provided that $a\geq 2k$.
% \end{thm}
% {\em Proof.} The assertion is a direct consequence of Theorem \ref{thm:lowerlog} and the following calculation:
% \begin{equation}\label{eq:simplecalculation}
% \E(X)\E(X^{-1})-1=\frac{a\theta}{(a-1)\theta}-1=\frac{1}{a-1}.  
% \end{equation}
% \begin{thm}\label{thm:boundsimploggamma}
% Under the above assumptions, if $a>2k-1$ we have:
% \begin{equation}\label{eq:simpleboundsimpgamma}
% \sum_{j=1}^{2k-1}\frac{1}{j}+\frac{(-1)^ja^j(2k-j)\cdots (a-j-1)}{jj!(2k)\cdots (a-1)}\leq \log(\E(X))-\E(\log(X) )\leq -\sum_{j=1}^{2k-1}\frac{1}{j}+\frac{(-1)^j(2k-j)\cdots (a+j-1)}{jj!a^j(2k)\cdots (a-1)}.  
% \end{equation}
% \begin{equation}\label{eq:simpleboundsimpgamma}
% \sum_{i=1}^{2k-1}\sum_{j=0}^i\frac{(-1)^{j}\piotr{i!}a^{1-j}(a+1)\cdots (a+j-1)}{j!(i-j)!}\leq \log(\E(X))-\E(\log(X) )\leq \sum_{i=1}^{2k-1}\sum_{j=0}^i\frac{(-1)^{j-1}\piotr{i!}a^j}{j!(i-j)!(a-j)\cdots (a-1)}.  
% \end{equation}
% \end{thm}
% {\em Proof.} The assertion is a direct consequence of Theorem \ref{thm:boundsimplog} and the following calculation for $n\in \{-2k+1,\ldots,2k-1\}$:
% \begin{equation}\label{eq:impcalculation}
% \E(X)^n\E(X^{-n})=\theta^na^n\theta^{-n}\frac{\Gamma(a-n)}{\Gamma(a)}=\frac{a^n\Gamma(a-n)}{\Gamma(a)}.  

\paragraph{Bounds for log-normally distributed random variable} Consider the random variable $X\sim \text{Lognormal}(\mu,\sigma)$ (i.e., $\log(X)\sim \text{Normal}(\mu,\sigma)$).
% \footnote{Here and henceforth, $\text{Lognormal}(m,\sigma)$ and $\text{Normal}(m,\sigma)$ denote log-normal and (corresponding) normal distributions with parameters $m$ and $\sigma>0$.}. 
Then $\E X^j=\exp(j\mu+\frac{1}{2}j^2\sigma^2)$ for any integer $j$. Thus, we can directly calculate both the exact size of Jensen's gap (see also Theorem 7 in \citep{pmlr-v206-struski23a}):
\begin{equation}\label{eq:jensensgaplognormal}
\begin{array}{@{}l @{\;} l @{\;} l}
\mathcal{JG}(-\log,X) & = & \log(\E X)-\E\{\log(X)\} =  -\mu+\mu+\frac{1}{2}\sigma^2=\frac{1}{2}\sigma^2, 
\end{array}
\end{equation}
and, applying \eqref{eq:improvedboundslog},
%Theorem \ref{thm:improvedlog},
the respective estimates:
\begin{equation}\label{eq:improvedboundslog_for_lognormal}
\begin{array}{@{}l @{\;} l @{\;} l}
\sum_{j=1}^{2k-1}\frac{1}{j}  +  b_{k,j}\exp\{\frac{1}{2}(j^2 &- & j)\sigma^2\} \leq \mathcal{JG}(-\log,X) \\
&\leq & -\sum_{j=1}^{2k-1}\frac{1}{j}+b_{k,j}\exp\{\frac{1}{2}(j^2+j)\sigma^2\}.
\end{array}
\end{equation}

% \marcin{** tu wpisac jakies tw. Piotra i odwolac sie do symulacji **}

It is easy to see that if we have a sequence of random variables $X_n\sim \text{Lognormal}(\mu,\sigma_n)$ with variances tending to $0$ (which implies that $\sigma_n\to 0$), then both Jensen's gap $\mathcal{JG}(-\log,X_n)$ and the respective lower and upper bounds (computed by \eqref{eq:improvedboundslog_for_lognormal} for any $k$) tend to 0. Indeed, the Euler integral representation of the harmonic number $H_n=\sum_{j=1}^n \frac{1}{j}$, given by the following formula (see~\citep{sandifer2006euler}):
\begin{equation}
\begin{array}{@{}l @{\;} l @{\;} l}
    H_n=\int_0^1 \frac{1-x^n}{1-x} dx,  
\end{array}
\end{equation}
allows us to calculate that
\begin{equation}
\begin{array}{@{}l @{\;} l @{\;} l}
    &\!\!\!\!\sum_{j=1}^{2k-1}&\frac{1}{j} = \int_0^1 \frac{1-x^{2k-1}}{1-x} dx=\int_{0}^1 \frac{1-(1-u)^{2k-1}}{u} du\\
&=&\!\!\!\!\!\!\!\!\!\int_0^1\sum_{j=1}^{2k-1}{2k-1 \choose j}(-u)^{j-1}du=\sum_{j=1}^{2k-1}{2k-1 \choose j}\frac{(-1)^{j-1}}{j}=-\sum_{j=1}^{2k-1}b_{k,j},  
\end{array}
\end{equation}
which leads to the conclusion.
% (see also also the results of our empirical study provided in 
% Subsection \ref{subsec:syntheticdataexperiments}).

We would like to highlight that the aforementioned property is particularly important for our experiments, as it motivates the development of the method of rigorous estimation of the log-likelihood presented in Subsection \ref{subsec:loglikelihoodestimations}.


\paragraph{Bounds tightening} Theorem \ref{thm:improvedlog} and the importance sampling technique (see \citep{burda2015importance,pmlr-v206-struski23a}) allow us to obtain tight lower and upper bounds on $\log(\E X)$ when $X$ is a positive random variable. To prove this, we rewrite the estimates from \eqref{eq:improvedboundslog} in the following form:
\begin{equation}\label{eq:improveadditivedboundslog}
\begin{array}{@{}l @{\;} l @{\;} l}
% \E(\sum_{j=1}^{2k-1}\frac{1}{j}\, + \!\!& & b_{k,j}\frac{X}{Y_1}\cdots \frac{X}{Y_j})\leq
\E\!&\{&\!\!\!\sum_{\!\!\!j=1}^{2k-1}\frac{1}{j}  +  b_{k,j}\frac{X^j}{(\E X)^j}\} \leq \log(\E X)-\E\{\log(X)\}\\
& \leq & \E\{-\sum_{j=1}^{2k-1}\frac{1}{j}-b_{k,j}\frac{(\E X)^j}{X^j}\} = \E \{-\sum_{j=1}^{2k-1}\frac{1}{j}-b_{k,j}\prod_{i=1}^j\frac{Y_i}{X}\},
\end{array}
\end{equation}
where $Y_1,\ldots, Y_{2k-1}$ are independent copies of $X$.
Then we replace all random variables with their $n$-sample means, i.e.,
\begin{equation}\label{eq:variablesubstitutions}
\begin{array}{@{}l @{\;} l @{\;} l}
X \to \overline X, & Y_1 & \to \overline Y_1, \ldots, Y_{2k-1} \to \overline Y_{2k-1}
\end{array}
\end{equation}
(note that we also need to copy each variable independently $n$ times) and  add $\E\{\log(X)\}$ to all sides. 
Since $\E X=\E \overline X$, this leads to the following estimates:
\begin{equation}\label{eq:improveadditivedboundslogmeans}
\begin{array}{@{}l @{\;} l @{\;} l}
% \E(\sum_{j=1}^{2k-1}\frac{1}{j} \, + \!\!& & b_{k,j}\frac{\overline X}{\overline Y_1}\cdots \frac{\overline X}{\overline Y_j})\leq \log(\E(X))-\E(\log(\overline X))\\
\E\{\sum_{j=1}^{2k-1}\frac{1}{j}  +   b_{k,j}\frac{\overline X^j}{(\E X)^j} &+ 
 &\log(\overline X)\} \leq  \log(\E X)\\
&  \leq & \E\{-\sum_{j=1}^{2k-1}\frac{1}{j}-b_{k,j}\frac{(\E \overline X )^j}{\overline{X}^j}+\log(\overline X)\} \\
& = & \E\{-\sum_{j=1}^{2k-1}\frac{1}{j}-b_{k,j}\prod_{i=1}^j\frac{\overline Y_i}{X}+\log(\overline X)\}.
\end{array}
\end{equation}
Note that we provide two forms of an upper bound, i.e., non-additive and fully additive. The latter version
%which means that it is expressed as the expected value of a random variable depending on $X$, and thus admits an unbiased sample mean estimator
is motivated by the fact that \citet{pmlr-v206-struski23a} also take care of this condition,
%\footnote{Although the optimal choice of the constant $C$ in Theorem 4 of \citet{pmlr-v206-struski23a} is data-dependent and thus leads to a non-additive upper bound (see Theorem 5 there).},
as it is particularly important for the training process, which is usually performed on mini-batches  (hence factorizing an objective function as a sum over the input dataset would be beneficial). However, as we show in our experiments on synthetic data (see Subsection \ref{subsec:syntheticdataexperiments}), the non-additive version provides more stable numerics, which is the result of less number of copies of $X$ and hence less variance of the respective random variable.

\citet{mouri2013log} showed that the sum of positive random variables converges to a log-normal distribution (even faster than to any Gaussian distribution, which is ensured by the central limit theorem) as their total number tends to infinity. On the other hand, the variance of the $n$-sample mean converges to 0 as $n\to \infty$. Thus, if $n$ is large, we can treat the estimates given by \eqref{eq:improveadditivedboundslogmeans}
% after applying variable substitutions \eqref{eq:variablesubstitutions},
as bounds for the log-normal random variable $\overline X$, which are known to be tight (see the previous paragraph), giving the assertion. 
% The assertion is simply obtained by adding $\E(\log(\overline X))$ to all sides of \eqref{eq:improveadditivedboundslogmeans}.
%enough. 
However, we do not provide theoretical guarantees of convergence. Instead, we provide appropriate empirical arguments (see the ablation study provided in Subsection~\ref{subsec:experiments}).
%Subsection \ref{subsec:experiments}).

\begin{figure}[t!]
    \centering
    \includegraphics[width=.65\textwidth]
    % {AISTATS24/images/normal_exponential.png}
{images/normal_exponential-new.png}
    \caption{Lower and upper bounds on Jensen's gap given by our method and that of \citet{lee2021further} for $f(x)=\exp(\frac{1}{2}x)$ and $X\sim \text{Exponential}(1)$ (top), and for $f(x)=\exp(x)$ and $X\sim \text{Normal}(0,1)$ (bottom), vs. the maximum order of moments used in the calculations (i.e., $2k-1$ in the case of our method). 
    The dashed lines between the points represent second-degree polynomial interpolation. It should be noted that the upper bounds provided by \citet{lee2021further} are infinite.}
    \label{fig:liaolee1}
\end{figure}


We emphasize that the above result is not based on the assumption that the support of $X$ lies in a compact interval contained in $(0,\infty)$. (This is particularly important because in practical applications of real-world data, the underlying distributions do not have this property.) Moreover, our experiments confirm that it allows us to obtain superior bounds in a number of situations. Thus, it can be considered a significant improvement over Theorem 5 of \citet{pmlr-v206-struski23a}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Applications}\label{sec:applications}

To prove the applicability of the theoretical results presented in Section \ref{sec:theory}, we examine them with respect to state-of-the-art solutions. We begin with the toy examples provided by \citet{liao2019sharpening} and \citet{lee2021further} for the exponential function and common continuous distributions (i.e., exponential and Gaussian), and then compare our method with that presented in \citep{pmlr-v206-struski23a}. We conduct experiments on both synthetic data generated from gamma and log-normal distributions, as well as real-world data (i.e., MNIST, SVHN, and CelebA datasets), which allow us to validate our approach as a novel method for estimating the log-likelihood of generative models.

\subsection{Examples for exponential function}\label{subsec:expexp}

In this subsection, we relate our results to those of two recent papers on sharpening Jensen's inequality, i.e., \citep{liao2019sharpening,lee2021further}. Although these works also provide lower and upper bounds for Jensen's gap based on Taylor's expansion, their applicability is influenced by the fact that they often yield boundary values (i.e., 0 and $\infty$) when used to compute bounds for common continuous (e.g., Gaussian) distributions (see \ref{sec:other} for more details), which cannot happen when our method is applied. Furthermore, these approaches rely on the computation of high-order central moments, whereas our bounds are expressed in terms of both central and raw moments, thereby rendering our technique more accessible.

\begin{figure}[t!]
   \centering
   \includegraphics[width=.8\textwidth]
   % {AISTATS24/images/theoretical_synthetic.png}
   {images/theoretical_synthetic.png}
%    \includegraphics[width=\textwidth]{AISTATS24/images/theoretical_gamma_synthetic.png}\\
% %       \caption{*** opis **}\label{fig:theoretical_gamma_synthetic}
% % \end{figure*}

% % \begin{figure*}[t]
% %    \centering
%    \includegraphics[width=\textwidth]{AISTATS24/images/theoretical_lognormal_synthetic.png}
      \caption{Lower and upper bounds on Jensen's gap for $f=-\log$ and $X\sim \text{Lognormal}(\mu,\sigma)$ (left) or $X\sim \text{Gamma}(a,\theta)$ (right), vs. different values of the respective distribution parameters, rigorously computed using our method
      %with $k\in \{2,3\}$ 
      and the method of \citet{pmlr-v206-struski23a}.}\label{fig:theoretical_lognormal_synthetic}
\end{figure}

\begin{figure}[t!]
   \centering
  \includegraphics[width=.8\textwidth]
  % {AISTATS24/images/synthetic_plots.png}
  {images/synthetic_plots.png}
  
%    \includegraphics[width=.95\textwidth]
%    % {AISTATS24/images/gamma_synthetic.png}
%    {AmericanStatistician/images/gamma_synthetic.png}
% %       \caption{*** opis **}\label{fig:gamma_synthetic}
% % \end{figure*}


% % \begin{figure*}[t]
% %    \centering
%    \includegraphics[width=.95\textwidth]
%    % {AISTATS24/images/lognormal_synthetic.png}
%    {AmericanStatistician/images/lognormal_synthetic.png}
      \caption{Lower and upper bounds on Jensen's gap for $f=-\log$ and $X\sim \text{Lognormal}(1,\sigma)$ (left) or $X\sim \text{Gamma}(a,1)$ (right), vs. different values of the respective distribution parameters, estimated by our method
      %with $k\in \{2,3\}$ 
      and the method of \citet{pmlr-v206-struski23a} applied to the synthetic data. (Note that our upper bounds were calculated using both non-additive and additive formulas.)}\label{fig:lognormal_synthetic}
\end{figure}



In the following two paragraphs, we refer to the examples presented in  \citep{liao2019sharpening,lee2021further} for exponential function $f$.

\paragraph{Bounds for exponentially distributed random variable}

Let $f(x)=\exp(\frac{1}{2}x)$ and $X\sim \text{Exponential}(1)$. The exact size of Jensen's gap is then calculated as follows:
\begin{equation}\label{eq:jensensgapexp}
\begin{array}{@{}l @{\;} l @{\;} l}
\mathcal{JG}(f,X) & = & 
%\E(\exp(\frac{1}{2}X))-\exp(\frac{1}{2}\E(X))\\& = & 
2-\sqrt{e}\approx 0.351.
\end{array}
\end{equation}
On the other hand, we have the following estimates:
\begin{equation}\label{eq:jensensgapexpest}
\begin{array}{@{}l @{\;} l @{\;} l}
\sqrt{e}\sum_{i=1}^{2k-1} & \frac{1}{2^i} & \sum_{j=0}^i\frac{(-1)^{i-j}}{(i-j)!}
\leq \mathcal{JG}(f,X) \leq  \sum_{i=1}^{2k-1}\sum_{j=0}^i\frac{1}{2^{j-1}}\frac{(-1)^{i-j+1}}{j!},
\end{array}
\end{equation}
which can be obtained by using \eqref{eq:simpleboundsimp} and some simple recalculations. Indeed, note that
\begin{equation}
\begin{array}{@{}l @{\;} l @{\;} l}
    f^{(i)}(X) =(\frac{1}{2})^i\exp(\frac{1}{2}X)
\end{array}
\end{equation}
and therefore
\begin{equation}
\begin{array}{@{}l @{\;} l @{\;} l}
    \E\!&\{&\!\!\!f^{(i)}(X)X^{i-j}\} = \E\{(\frac{1}{2})^i\exp(\frac{1}{2}X) X^{i-j}\} \\
    &= & (\frac{1}{2})^i\int_0^{\infty} \exp(\frac{1}{2}x)x^{i-j}\exp(-x)dx
= (\frac{1}{2})^{i-1}\int_0^{\infty}x^{i-j}\frac{1}{2}\exp(-\frac{1}{2}x)dx\\
&=&(\frac{1}{2})^{i-1}\frac{(i-j)!}{(\frac{1}{2})^{i-j}}
=(\frac{1}{2})^{j-1}(i-j)!. 
\end{array}
\end{equation}
Thus, by applying \eqref{eq:simpleboundsimp}, we conclude that
\begin{equation}
\begin{array}{@{}l @{\;} l @{\;} l}
    &\mathcal{JG}&\!(f,X) \leq \sum_{i=1}^{2k-1}\sum_{j=0}^i\frac{(-1)^{i-j+1}}{j!(i-j)!}\E\{f^{(i)}(X)X^{i-j}\}(\E X)^j \\ 
    &\;\,=&\!\!
\sum_{i=1}^{2k-1}\sum_{j=0}^i\frac{(-1)^{i-j+1}}{j!(i-j)!}(\frac{1}{2})^{j-1}(i-j)!1^j=\sum_{i=1}^{2k-1}\sum_{j=0}^i\frac{1}{2^{j-1}}\frac{(-1)^{i-j+1}}{j!}
\end{array}
\end{equation}
and
\begin{equation}
\begin{array}{@{}l @{\;} l @{\;} l}
&\mathcal{JG}&\!(f,X) \geq    \sum_{i=1}^{2k-1}\sum_{j=0}^i\frac{(-1)^{i-j}}{j!(i-j)!}f^{(i)}(\E X)\E X^j(\E X)^{i-j} \\ 
 &\;\,=&\!\!
 \sum_{i=1}^{2k-1}\sum_{j=0}^i\frac{(-1)^{i-j}}{j!(i-j)!}(\frac{1}{2})^i\exp(\frac{1}{2})\frac{j!}{1^j}1^{i-j}=\sqrt{e}\sum_{i=1}^{2k-1} \frac{1}{2^i} \sum_{j=0}^i\frac{(-1)^{i-j}}{(i-j)!}. 
\end{array}
\end{equation}
%(see the supplementary materials).
% \begin{equation}\label{eq:exprecalculation}
% \begin{array}{@{}l @{\;} l @{\;} l}
% \E(f^{(i)}(X)X^{i-j}) & = & \E ((\frac{1}{2})^i\exp(\frac{1}{2}X) X^{i-j})\\
% & = & (\frac{1}{2})^i\int_0^{\infty} \exp(x/2)x^{i-j}\exp(-x)dx\\
% & = &  (\frac{1}{2})^{i-1}\int_0^{\infty}x^{i-j}\frac{1}{2}\exp(-x/2)dx\\
% & = & (\frac{1}{2})^{i-1}\frac{(i-j)!}{(\frac{1}{2})^{i-j}} =  \frac{1}{2^{j-1}}(i-j)!. 
% \end{array}
% \end{equation}



In the upper part of Figure \ref{fig:liaolee1} we compare the bounds given by \eqref{eq:jensensgapexpest} with those obtained in \citep{lee2021further}. (We emphasize that the method of \citet{liao2019sharpening} coincides with the method of \citet{lee2021further} of the first order, see \ref{sec:other}.) It is clear that as the maximum number of moments used in the computations increases, our approach becomes superior (or at least comparable) while avoiding infinity. Note, however, that in \citep{lee2021further} the bounds are expressed using even-order moments (see \ref{sec:other} or Theorem 2.1 in \citep{lee2021further}), whereas we use odd-order moments. Consequently, although Figure~\ref{fig:liaolee1} suggests a comparison with missing moments interpolated by a second-degree polynomial, our method and that of \citet{lee2021further} should be considered as complementary rather than competitive in this case.


 % The results are much better than the results from \cite{liao2019sharpening,lee2021further}, where upper bound is trivially equal to $\infty$. 

% The lower bound is 
% $$
%  \sum_{i=1}^{2k-1}\sum_{j=0}^i\frac{(-1)^{i-j}}{j!(i-j)!}f^{(i)}(\E(X))\E(X^j)\E(X)^{i-j}=
%  \sum_{i=1}^{2k-1}\sum_{j=0}^i\frac{(-1)^{i-j}}{j!(i-j)!}(\frac{1}{2})^i\exp(1/2)\frac{j!}{1^j}1^{i-j}=
% $$
% $$
% \exp(1/2)\sum_{i=1}^{2k-1} \frac{1}{2^i} \sum_{j=0}^i\frac{(-1)^{i-j}}{(i-j)!}. 
% $$

% \begin{table}
% \centering 
% \begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|}
% \hline
% order used           & 1      & 2        & 3     & 4        & 5     & 6        & 7     & 8        & 9     &  10       \\ \hline
% lower bound          &        &          & 0.274 &          & 0.332 &          & 0.346 &          & 0.350 &           \\ \hline
% lower bound from Lee &        & 0.176    &       & 0.310    &       & 0.341    &       & 0.349    &       &  0.351    \\ \hline
% actual value of gap  & 0.351  & 0.351    & 0.351 & 0.351    & 0.351 & 0.351    & 0.351 & 0.351    & 0.351 &  0.351    \\ \hline
% upper bound from Lee &        & $\infty$ &       & $\infty$ &       & $\infty$ &       & $\infty$ &       &  $\infty$ \\ \hline
% upper bound          & 1      &          & 0.958 &          & 0.957 &          & 0.957 &          & 0.957 &           \\ \hline
% \end{tabular}
% \caption{\marcin{** tu opis ** moze zminic na obrazek? ***}}\label{tab:liaolee1}
% \end{table}   

% \begin{table}
% \centering 
% \begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|}
% \hline
% order used           & 1      & 2        & 3     & 4        & 5     & 6        & 7     & 8        & 9     &  10       \\ \hline
% lower bound          & 0      &          & 0.500 &          & 0.625 &          & 0.645 &          & 0.648 &           \\ \hline
% lower bound from Lee &        & 0.406    &       & 0.631    &       & 0.648    &       & 0.649    &       &           \\ \hline
% actual value of gap  & 0.649  & 0.649    & 0.649 & 0.649    & 0.649 & 0.649    & 0.649 & 0.649    & 0.649 &  0.649    \\ \hline
% upper bound from Lee &        & $\infty$ &       & $\infty$ &       & $\infty$ &       & $\infty$ &       &  $\infty$ \\ \hline
% upper bound          & 1.648  &          & 1.099 &          & 0.769 &          & 0.671 &          & 0.651 &           \\ \hline
% \end{tabular}
% \caption{\marcin{** tu opis ** moze zmienic na obrazek ? ***}}\label{tab:liaolee2}
% \end{table}   


% The results are better than the results from \cite{liao2019sharpening} or \cite{walker2014lower} and similar to the results from \cite{lee2021further}. 




\paragraph{Bounds for normally distributed random variable}

We perform analogous calculations for $f(x)=\exp(x)$ and Gaussian (continuous) random variable $X\sim \text{Normal}(0,1)$. In this case, the exact Jensen's gap and the corresponding bounds are as follows:
%(see the supplementary materials):
\begin{equation}\label{eq:jensensgapexpnormal}
\begin{array}{@{}l @{\;} l @{\;} l}
\mathcal{JG}(f,X) & = & \sqrt{e}-1\approx 0.649
\end{array}
\end{equation}
and
\begin{equation}\label{eq:jensensgapexpnormalest}
\begin{array}{@{}l @{\;} l @{\;} l}
\sum_{i=1}^{2k-1}\frac{\E X^i}{i!}
&\leq & \mathcal{JG}(f,X)
 \leq  \sqrt{e} \sum_{i=1}^{2k-1}\frac{(-1)^{i+1}}{i!}\E Y^i,
\end{array}
\end{equation}
where $Y\sim \text{Normal}(1,1)$.
Indeed, note that
\begin{equation}
\begin{array}{@{}l @{\;} l @{\;} l}
    \E\{f^{(i)}(X)X^{i-j}\}&=&\E\{\exp(X)X^{i-j}\}\\
    &=&\int_{-\infty}^{\infty} \exp(x)x^{i-j}\frac{1}{\sqrt{2\pi}}\exp(-\frac{1}{2}x^2)dx\\
&=&\sqrt{e}\int_{-\infty}^{\infty} x^{i-j}\frac{1}{\sqrt{2\pi}}\exp\{-\frac{1}{2}(x-1)^2\}=\sqrt{e}\, \E Y^{i-j},
\end{array}
\end{equation}
and therefore, since $\E X=0$, we can use \eqref{eq:simpleboundsimp2} to calculate the respective bounds in the following way:
\begin{equation}
\begin{array}{@{}l @{\;} l @{\;} l}
    \sum_{i=1}^{2k-1}&\!\frac{1}{i!}&\!\!\!f^{(i)}(0)\E X^i =\sum_{i=1}^{2k-1}\frac{\E X^i}{i!} \leq \mathcal{JG}(f,X) \\ 
    &\leq & \sqrt{e} \sum_{i=1}^{2k-1}\frac{(-1)^{i+1}}{i!}\E Y^i =-\sum_{i=1}^{2k-1}\frac{(-1)^{i}}{i!}\E\{f^{(i)}(X)X^{i}\}.
\end{array}
\end{equation}
% denotes confluent hypergeometric function of the second kind \marcin{** przerobic +cytowanie ***}.

% For $k=1,\ldots,4$ we obtain 1.6487213 1.0991475 0.7694033 0.6712651 !


% The lower bound is
% $$
% \sum_{i=1}^{2k-1}\frac{1}{i!}\exp(0)\E(X^i)=\sum_{i=1}^{2k-1}\frac{\E(X^i)}{i!}.
% $$
% For $k=1,\ldots,4$ we obtain 0.0000000 0.5000000 0.6250000 0.6458333 !

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{AISTATS24/images/normal.png}
%     \caption{normal}
%     \label{fig:liaolee2}
% \end{figure}

The comparison of the bounds given in \eqref{eq:jensensgapexpnormalest} with those of \citet{lee2021further} is shown in the lower part of  Figure~\ref{fig:liaolee1}. Unlike the competitor, our approach provides tighter and tighter upper bounds as the maximum number of moments used in the computations increases. (Note that in our method there is no need to apply the support partitioning technique, see \ref{sec:other}.)

% Again, our upper bound is much better than the results from \cite{liao2019sharpening,lee2021further}, where upper bound is trivially equal to $\infty$. The lower bound is better than the results from \cite{liao2019sharpening}  and similar to the results from \cite{lee2021further} (but on the other hand our estimation seems to be less complicated whereas we don't need to partion the support of $X$).

% \subsection{Bounds for $f=-\log$ and $X\sim \mathrm{Gamma}(a,\theta)$}

% Consider, for example\footnote{Here and henceforth, $\text{Gamma}(a,\theta)$ and $\text{Inv-Gamma}(a,\theta)$ denote Gamma and Inverse-Gamma distributions with the shape parameter $a>0$ and the scale parameter $\theta>0$, respectively.}, the random variable $X\sim \text{Gamma}(a,\theta)$ and the convex function $f=-\log$. Then  $X^{-1}\sim \text{Inv-Gamma}(a,1/\theta)$. It is also known that for $n\in \mathbb{Z}$ and $a>-n$ \piotr{(czy $a>n$)?} we have:
% \begin{equation}\label{eq:gammamoments}
% \E(X^n)=\theta^n\frac{\Gamma(a+n)}{\Gamma(a)}.
% \end{equation}
% \piotr{
% and
% \begin{equation}\label{eq:invgammamoments}
% \E(X^{-n})=\frac{1}{\theta^n}\frac{\Gamma(a-n)}{\Gamma(a)}.
% \end{equation}


% }

% \marcin{a to drugie zawiera sie w pierwszym bo mamy $n\in \mathbb{Z}$}
% \subsubsection{Simple bounds}

% \begin{thm}\label{thm:lowerloggamma}
% Under the above assumptions, if $a>1$ we have:
% \begin{equation}\label{eq:simpleboundsgamma}
% 0\leq \log(\E(X))-\E(\log(X) )\leq \frac{1}{a-1}.  
% \end{equation}
% \end{thm}
% {\em Proof.} The assertion is a direct consequence of Theorem \ref{thm:lowerlog} and the following calculation:
% \begin{equation}\label{eq:simplecalculation}
% \E(X)\E(X^{-1})-1=\frac{a\theta}{(a-1)\theta}-1=\frac{1}{a-1}.  
% \end{equation}


% \subsubsection{Improved bounds}

% \begin{thm}\label{thm:boundsimploggamma}
% Under the above assumptions, if $a>2k-1$ we have:
% \begin{equation}\label{eq:simpleboundsimpgamma}
% \sum_{j=1}^{2k-1}\frac{1}{j}+\frac{(-1)^ja^j(2k-j)\cdots (a-j-1)}{jj!(2k)\cdots (a-1)}\leq \log(\E(X))-\E(\log(X) )\leq -\sum_{j=1}^{2k-1}\frac{1}{j}+\frac{(-1)^j(2k-j)\cdots (a+j-1)}{jj!a^j(2k)\cdots (a-1)}.  
% \end{equation}
% \begin{equation}\label{eq:simpleboundsimpgamma}
% \sum_{i=1}^{2k-1}\sum_{j=0}^i\frac{(-1)^{j}\piotr{i!}a^{1-j}(a+1)\cdots (a+j-1)}{j!(i-j)!}\leq \log(\E(X))-\E(\log(X) )\leq \sum_{i=1}^{2k-1}\sum_{j=0}^i\frac{(-1)^{j-1}\piotr{i!}a^j}{j!(i-j)!(a-j)\cdots (a-1)}.  
% \end{equation}
% \end{thm}
% {\em Proof.} The assertion is a direct consequence of Theorem \ref{thm:boundsimplog} and the following calculation for $n\in \{-2k+1,\ldots,2k-1\}$:
% \begin{equation}\label{eq:impcalculation}
% \E(X)^n\E(X^{-n})=\theta^na^n\theta^{-n}\frac{\Gamma(a-n)}{\Gamma(a)}=\frac{a^n\Gamma(a-n)}{\Gamma(a)}.  
% \end{equation}

% \begin{rem} Note that the bounds introduced in Theorem \ref{thm:boundsimploggamma} are better then those following from the results of \citet{lee2021further,liao2019sharpening,agahi2021jensen}, since in this case $r^{(2k)}(x,\E(X))\to \infty$ as $x\to 0$, and $r^{(2k)}(x,\E(X))\to 0$ as $x\to \infty$, where:
% \begin{equation}\label{eq:hfunctiongamma}
% r^{(2k)}(x,\E(X))=\frac{\log(\E(X))-\log(x)}{(x-\E(X))^{2k}}-\sum_{i=1}^{2k-1}\frac{1}{i!}\frac{(-1)^i}{\E(X)^i(x-\E(X))^{2k-i}}.
% \end{equation}
% \end{rem}

% \subsection{Bounds for $f=-\log$ and $X\sim \mathrm{Lognormal}(\mu,\sigma)$} 

% Consider the random variable $X\sim \text{Lognormal}(\mu,\sigma)$ (i.e. $\log(X) \sim N(\mu,\sigma$)) and the convex function $f=-\log$. Then $\E(X^j)=\exp(j\mu+\frac{1}{2}j^2\sigma^2)$ for any $j$. 

% The Jansen's gap is  
% $$
% \mathcal{JG}(-\log,X)=\E(-\log(X))+\log(EX)=-\mu+\mu+\frac{1}{2}\sigma^2=\frac{1}{2}\sigma^2. 
% $$
% Our boundings are as follows
% \begin{equation}\label{eq:improvedboundslog_for_lognormal}
% \sum_{j=1}^{2k-1}\frac{1}{j}+b_{k,j}\exp(\frac{1}{2}(j^2-j)\sigma^2)\leq \mathcal{JG}(-\log,X)\leq -\sum_{j=1}^{2k-1}\frac{1}{j}+b_{k,j}\exp(\frac{1}{2}(j^2+j)\sigma^2).
% \end{equation}

% \piotr{numerycznie te wzory nie sa zachecajace dla sigm>1}}

\begin{table}[t!]\small
    \caption{The number and percentage of data points (images) for which our method is better than the method of \citet{pmlr-v206-struski23a}. Note that incorporating our approach is beneficial for IWAEs trained on MNIST or SVHN datasets.}
    \label{tab:my_label}
    \vskip4mm
    \centering
    \begin{tabular}{c@{\;\;\;\;}c@{\;\;\;\;}c@{\;\;\;\;}c}
    \toprule
    \multirow{2}{*}{\textbf{dataset}} & \multicolumn{3}{c}{\textbf{Model}}\\
    \cmidrule{2-4}
     & \textbf{VAE} & \textbf{IWAE-5} & \textbf{IWAE-10}\\
    \midrule
    \multirow{2}{*}{\textbf{MNIST}} & 9593    & 9021  & 8490 \\
    \cmidrule{2-4}
                           & 96.37\% & 100\% & 100\% \\
    \midrule
    \multirow{2}{*}{\textbf{SVHN}} & 59     & 4578    & 3653   \\
    \cmidrule{2-4}
                          & 0.59\% & 46.07\% & 36.88\% \\
    \midrule
    \multirow{2}{*}{\textbf{CelebA}} & 18     & 99     & 76  \\
    \cmidrule{2-4}
                            & 0.18\% & 0.99\% & 0.76\% \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[t!]
\centering
\includegraphics[width=.8\textwidth]
% {AISTATS24/images/barplot_results.png}
{images/barplot_results.png}
% \begin{tabular}{@{}c@{\!}l@{\;\,}r@{\;\;}r@{\;\;}r@{\;\;}r@{\;\;}r@{\;\;}r@{\;\;}r@{}}
% \toprule
% & & \multicolumn{7}{c}{\bfseries BOUNDS DIFFERENCE} \\
% \cmidrule{3-9}
%   \rotatebox{77}{\bfseries dataset} & \rotatebox{77}{\bfseries MODEL} & \rotatebox{77}{\small \bfseries Our} & \rotatebox{77}{\small \bfseries IS} & \rotatebox{77}{\small \bfseries IS} & \rotatebox{77}{\small \bfseries $\text{CUBO}$} & \rotatebox{77}{\small \bfseries $\text{TVO}$} \\
% \midrule
 
% \multirow[c]{3}{*}{\rotatebox{90}{MNIST}} 
%  & VAE     &   &    &  0.04  & 1.59 & 0.13 \\
%  & IWAE-5  &   &    &  0.004 & 1.32 & 0.25 \\
%  & IWAE-10 &   &    &  0.01  & 1.54 & 0.36 \\
% \cmidrule{1-9}
% \multirow[c]{3}{*}{\rotatebox{90}{SVHN}} 
%  & VAE     &    &    & 0.44 & 3.16 &  0.42 \\
%  & IWAE-5  &    &    & 0.31 & 2.74 &  0.58 \\
%  & IWAE-10 &    &    & 0.28 & 2.83 &  0.64 \\
% % \cmidrule{1-9}
% % \multirow[c]{3}{*}{\rotatebox{90}{CelebA}} & VAE & 1.12 & 3.25 & 4.92  & 22.73 & 9.09 & 4.55 & 0.91 \\
% %  & IWAE-5 & 1.23 & 3.40 & 5.14  & 42.70 & 17.08 & 8.54 & 1.71 \\
% %  & IWAE-10 & 0.72 & 3.22 & 4.86  & 37.73 & 15.09 & 7.55 & 1.51 \\
% \bottomrule
% \end{tabular}
\caption{Estimated upper bounds on Jensen's gap, which measure the tightness of the estimation of the log-likelihood of VAE, IWAE-5, and IWAE-10 models pre-trained on the MNIST (left), SVHN (middle), and CelebA (right) datasets (lower is better). 
All values shown are restricted to (and averaged over) all data points counted in Table \ref{tab:my_label}.}
% Note that our method provides superior bounds.}
\label{tab.gap}
\end{figure}



\subsection{Experiments on synthetic data}\label{subsec:syntheticdataexperiments}

In this subsection, we present the results of experiments conducted on synthetic datasets sampled from a random variable $X$ with a gamma or log-normal distribution. Our goal is to compare the bounds computed by \eqref{eq:improveadditivedboundslog} with those obtained by Corollary 4 of \citet{pmlr-v206-struski23a}, i.e.,
\begin{equation}\label{eq:improveboundslogstruski}
\begin{array}{@{}l @{\;} l @{\;} l}
0\leq \log(\E X)-\E\{\log(X)\}
&  \leq & \log (\E \frac{Y}{X}),
\end{array}
\end{equation}
where $Y$ is an independent copy of $X$. (Note that for such random variables and distributions, comparison with the other considered approaches is meaningless, since the lower bound given by Theorem 2.1 of \citet{lee2021further}, which is a generalization of Theorem 1 of \citet{liao2019sharpening}, coincides with that given by our method, while the upper bound is infinite, see \ref{sec:other}.) In all cases, we use sample means as unbiased estimators of the expected values of given random variables (which may result in bounds that are not necessarily strict). First, however, we present the results of direct bounds computations using 
 \eqref{eq:simpleboundsimpgamma} and \eqref{eq:improvedboundslog_for_lognormal}, which is done in Figure  \ref{fig:theoretical_lognormal_synthetic}. Note that for both situations considered, there are wide ranges of distribution parameters (depending on $k$) for which our method (compared to that of \citet{pmlr-v206-struski23a}) provides better bounds. (In fact, in \citep{pmlr-v206-struski23a} only a non-trivial upper bound for Jensen's gap in provided.)
 % Moreover, upper bounds computed by the method of \citet{lee2021further} become infinite in these cases (hence not reported).)

% \begin{figure}
%    \centering
%    \includegraphics[width=0.9\columnwidth]{Rysunek_gamma_01.jpeg}
%       \caption{*** opis **}\label{fig:gamma}
% \end{figure}

% \begin{figure}
%    \centering
%    \includegraphics[width=0.9\columnwidth]{Rysunek_lognormal_02.jpeg}\\[0mm]
%    \includegraphics[width=0.9\columnwidth]{Rysunek_lognormal_01.jpeg}
%    \caption{*** opis **}\label{fig:lognormal}
% \end{figure}


As shown in Figure \ref{fig:lognormal_synthetic}, the above (theoretical) results are confirmed by our experiments on synthetic datasets. It is also clear that the numerics for the non-additive bounds is more stable (as we already claimed in Subsection \ref{subsec:boundslog}). This motivates our experiments on log-likelihood estimation on real-world data, which are presented in the following subsections. 




% \begin{figure}
%    \centering
%    \includegraphics[width=0.9\columnwidth]{Rysunek_LOGNORMAL_DOK_SYMUL.jpeg}
%       \caption{*** opis **}\label{fig:gammasynthetic}
% \end{figure}

% \begin{figure}
%    \centering
%    \includegraphics[width=0.9\columnwidth]{Rysunek01.jpeg}
%    \caption{*** opis **}\label{fig:lognormalsynthetic}
% \end{figure}


\subsection{Log-likelihood estimation}\label{subsec:loglikelihoodestimations}

In this subsection, we present a novel method of log-likelihood estimation for arbitrary generative models (with the latent).
Although our approach could be useful for different architectures, motivated by the work of \citet{pmlr-v206-struski23a}, we focus our attention on those with an autoencoder structure (such as VAEs). 


% This case is particularly important 
% both from the point of view of training and evaluation processes. 

Let $p(x,z)$ be a joint model distribution on $\mathcal{X}\times \mathcal{Z}$, where $\mathcal{X}$ and $\mathcal{Z}$ denote data and latent spaces, respectively. 
In the case of an autoencoder-based generative model, where we also have a random encoder $q(z|x)$, a random decoder $p(x|z)$, and a prior $p(z)$, a model log-likelihood of a given data point $x\in \mathcal{X}$ is expressed as follows: 
\begin{equation}\label{eq:modelloglikelihood}
\begin{array}{@{}l @{\;} l @{\;} l}
\log [p(x)] & = & \int p(x,z) dz =\log [\E_{Z \sim q(\cdot |x)} \frac{p(x,Z)}{q(Z|x)}] = \log [\E_{Z \sim q(\cdot |x)} \frac{p(x|Z)p(Z)}{q(Z|x)}].
\end{array}
\end{equation}
Then, applying \eqref{eq:improveadditivedboundslogmeans} to the random variable $R_x(Z)=\frac{p(x|Z)p(Z)}{q(Z|x)}$ (for $Z\sim q(\cdot |x)$), we can compute rigorous lower and upper bounds on the log-likelihood of the model distribution in $x$ (note that this method requires the use of a number of independent copies of $R_x(Z)$, and hence $Z$).

Finally, we need to explore the conditions under which the bounds might be tight enough. To do this, we use the method described in the last paragraphs of Subsection \ref{subsec:boundslog}. Specifically, we approximate the distribution of the $n$-sample mean by $\text{Lognormal}(\mu,\sigma)$ with a small variance (hence a small $\sigma$), which requires taking $n$ sufficiently large. However, as the preceding experiments on synthetic data show (see Subsection \ref{subsec:syntheticdataexperiments}), our method benefits more when $k$ is relatively small and $\sigma$ takes moderately small rather than very small values. Therefore, also considering that the time and sample complexity grow with increasing $k$ and $n$, some kind of trade-off would be desirable in this case. In summary, we postulate that computing lower and upper bounds for a model log-likelihood of a given data point $x\in \mathcal{X}$ using \eqref{eq:improveadditivedboundslogmeans} (as described above) should be based on the following criteria:
\begin{itemize}
\item[(i)] good log-normal approximation (see \citep{mouri2013log}) -- $n$ large enough,
    \item[(ii)] tight bounds -- sufficiently small $\sigma$ (due to large $n$),
    \item[(iii)] clear advantage over existing methods -- relatively small $k$ and balanced $\sigma$ (due to balanced $n$),
    \item[(iv)] reasonable time and sample complexity -- possibly small $k$ and $n$.
\end{itemize}
With respect to the results presented in the previous subsection, at this stage, we can set reasonable values for $k$ as 2 or 3 (see Figures \ref{fig:theoretical_lognormal_synthetic} and \ref{fig:lognormal_synthetic}). On the other hand, determining an appropriate $n$ requires further analysis of the (data-dependent) distribution of the random variable $R_x(Z)$
% which obviously depends on the data 
(see the following subsection).
% In the classical \mm{VAE,} we maximize 
% \begin{equation}\label{eq:exp:3}
% % \setlength{\abovedisplayskip}{3pt} \setlength{\abovedisplayshortskip}{0pt}
% % \setlength{\belowdisplayskip}{3pt} \setlength{\belowdisplayshortskip}{0pt}
% \begin{array}{l @{\;} l @{\;} l}
% \text{ELBO} & = & \E_{z \sim q(\cdot |x)} \log R(x,z),
% \end{array}
% \end{equation}
% which is the lower bound for the log-likelihood.

% The idea behind the IWAE model \citep{burda2015importance} is to obtain an (asymptotically optimal) approximation of \mm{the} evidence by maximizing
% \begin{equation}\label{eq:exp:4}
% % \setlength{\abovedisplayskip}{3pt} \setlength{\abovedisplayshortskip}{0pt}
% % \setlength{\belowdisplayskip}{3pt} \setlength{\belowdisplayshortskip}{0pt}
% \begin{array}{l @{\;} l @{\;} l}
% \text{IW-ELBO}_k=\E_{z_i \sim q(\cdot |x)} \log  \frac{1}{k}\sum_{i=1}^k R(x,z_i),
% \end{array}
% \end{equation}
% which is a closer (than ELBO) lower bound for the log-likelihood. 

% To obtain upper bounds for the evidence, it is enough to apply~ \Cref{th:euboimpc,cor:burdalogimproved} for $X=\frac{1}{k}\sum_{i=1}^k X_i$, where $X_i=R(x,z_i)$ and $Y_i=R(x,\tilde{z}_i)$, and all $z_i$ and $\tilde{z}_i$ are independently sampled from $q(\cdot|x)$. Then, we conclude that the size of the respective variational gap is bounded from above by the following value, which we will call {\em importance sampling variational gap bound (IS-VG-B)}: 
% \begin{equation}\label{eq:isvgb}
% % \setlength{\abovedisplayskip}{3pt} \setlength{\abovedisplayshortskip}{0pt}
% % \setlength{\belowdisplayskip}{3pt} \setlength{\belowdisplayshortskip}{0pt}
% \begin{array}{l @{\;} l @{\;} l}
% \textrm{IS-VG-B} &= & C-1+\exp(-C)  \E_{z_i, \tilde z_i \sim q(\cdot |x)}  \frac{  \sum_{i=1}^k R(x,\tilde z_i)}{ \sum_{i=1}^k R(x,z_i)},
% \end{array}
% \end{equation}
% where $C$ may be chosen \mm{arbitrarily}, with the optimum equal to
% \begin{equation}\label{eq:cxopt}
% % \setlength{\abovedisplayskip}{3pt} \setlength{\abovedisplayshortskip}{0pt}
% % \setlength{\belowdisplayskip}{3pt} \setlength{\belowdisplayshortskip}{0pt}
% \begin{array}{l @{\;} l @{\;} l}
% C^{\,\text{opt}} &= &\log \E_{z_i, \tilde z_i \sim q(\cdot |x)}  \frac{  \sum_{i=1}^k R(x,\tilde z_i)}{ \sum_{i=1}^k R(x,z_i)}.
% \end{array}
% \end{equation}

% Starting with a case study for simple
% synthetic one-dimensional data generated from the Laplace
% distribution, in the following few paragraphs we provide and discuss the results of the experiments, in which we compare our approach to those presented in \cite{dieng2017variational,ji2019stochastic,masrani2019thermodynamic}.
% \mm{Mainly,} we apply \mm{all} considered estimation techniques for the variational gap to selected Gaussian autoencoders (i.e., classical VAE and two different IWAE models, on MNIST, SVHN, and CelebA datasets), previously learned using their own objectives and VAE experimental setup.



% In the following subsection, we present the results of experiments conducted on real-world datasets, which allow us to compare the above approach with the existing state-of-the-art.

\subsection{Experiments on real-world data}\label{subsec:experiments}

In this subsection, we present the results of experiments conducted to compare our log-likelihood estimation method with the state-of-the-art, which is represented by the recent experimental benchmark of \citet{pmlr-v206-struski23a} with VAE \citep{kingma2013auto, rezende2015variational}, IWAE-5, and IWAE-10 \citep{burda2015importance} models pre-trained on the MNIST~\citep{lecun1998gradient}, SVHN~\citep{netzer2011reading}, and CelebA~\citep{liu2015deep} datasets.
%\footnote{\cite{pmlr-v206-struski23a} also considered the CelebA~\citep{liu2015deep} dataset, which resulted in the enormous time and sample complexity (see the supplementary materials).}
In addition, in the supplementary materials we provide an ablation study that illustrates the impact of the log-likelihood distribution on the applicability of our approach.
% The code for all experiments is available at \url{https://github.com/...}.


Following the conclusions of the previous subsection, to compute our proposed lower and upper bounds by applying \eqref{eq:improveadditivedboundslogmeans} to the random variable $R_x(Z)$ that depends on a given data point $x$, we first try to satisfy postulates (i)--(iv) by selecting appropriate $n$ via an appropriate grid search procedure (see the supplementary materials).
% To do this, we use a grid search procedure to find possibly the smallest $n\geq 50$ (we keep this lower bound to ensure (i)) for which the empirical standard deviation of $\log \overline{R_x(z)}=\log \frac{1}{n}\sum_{i=1}^nR_x(z_i)$, where $z_i$ are independently sampled from $q(\cdot|x)$, is less than 0.3 (to ensure (ii)--(iv), see Figures \ref{fig:theoretical_lognormal_synthetic}  and \ref{fig:lognormal_synthetic}). 
Then (taking $k\in \{2,3\}$) we compute from \eqref{eq:improveadditivedboundslogmeans} the lower and upper bounds for $\log \E_{Z\sim q(\cdot|x)}R_x(Z)$.  Note that the upper bound on the respective Jensen's gap 
can be considered as a measure of the tightness of the estimation, so it is further reported. (In fact, we report the value computed using the sample mean estimator for each expectation, averaged over all data points $x$.) We refer to \ref{sec:exp} for more details on the experimental setup.



% \marcin{Dla każdego obrazka losujemy 5000-elementowe próbki $$Y^n_1=\log \frac{1}{n}\sum_{i=1}^n X^1_i, \ldots, Y^n_{5000}=\log \frac{1}{n}\sum_{i=1}^n X^{5000}_i$$ dla $n=50, 100, 150, \ldots$ i liczymy z nich odchylenia standardowe 
% $$\sigma_{50}, \sigma_{100}, \sigma_{150}, \ldots$$
% do momentu, az otrzymamy $\sigma_n<0.2$. Pod drodze zapisujemy $n$, $\sigma_n$, $\text{seed}_n$ (dla każdego obrazka).

% Potem dla każdego obrazka wybieramy $n$ takie, ze:
% \begin{itemize}
%     \item dla $k=2$: $\sigma_n$ jest najbliższe 0.34
%     \item dla $k=3$: $\sigma_n$ jest najbliższe ?? (skonsultuje jeszcze z Piotrkiem)
% \end{itemize}
% i liczymy boundy (górny i dolny) dla tego $k$, $\text{NFEATURE}=n$, $\text{NF}=100000$.}


The results of our experiments are presented in Table \ref{tab:my_label} and Figure \ref{tab.gap}. Specifically, in Table \ref{tab:my_label} we provide the number and percentage of data points for which our method is superior to that of \citet{pmlr-v206-struski23a}. (We excluded points for which we could not obtain reasonable results due to some numerical problems.) It is clear that incorporating our approach is beneficial for models and datasets that produce less complicated latent distributions (such as IWAEs trained on MNIST or SVHN), while otherwise yielding little progress (we provide an additional ablation study on this phenomenon).
%following paragraph).
% To be fair, we must emphasize that we tuned the value of the hyperparameter $n$ for each data point (as described above). 
Additionally, in Figure \ref{tab.gap} we
% the sample complexity was therefore significantly reduced, which is a real advantage.
compare the values computed by our method and the method of \citet{pmlr-v206-struski23a}, restricted to (and averaged over) all data points that benefited from the use of our method. It is evident that our approach provides superior bounds in these cases, probably due to the compliance with the proposed criteria (see the following paragraph).
% Here, the optimal method means one (chosen from a collection of our methods -- additive or non-additive for $k\in \{2,3\}$ -- and the method of \citet{pmlr-v206-struski23a}) that gives the lowest upper bound. 

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{images/MNIST_IWAE5_2023-10-03_123554_2023-10-16_171359_bounds.png}
    \includegraphics[width=\textwidth]{images/MNIST_IWAE10_2023-10-03_123554_2023-10-16_171359_bounds.png}
    \includegraphics[width=\textwidth]{images/MNIST_VAE_2023-10-09_140705_2023-10-16_181759_bounds.png}
    \includegraphics[width=\textwidth]{images/SVHN_IWAE5_LAST_2023-10-15_123851_bounds.png}
    \includegraphics[width=\textwidth]{images/SVHN_IWAE10_2023-10-12_200700_bounds.png}
    \includegraphics[width=\textwidth]{images/SVHN_VAE_LAST_2023-10-16_145618_bounds.png}
    \includegraphics[width=\textwidth]{images/CelebA_IWAE5_2023-10-18_141112_2023-10-19_120556_bounds.png}
    \includegraphics[width=\textwidth]{images/CelebA_IWAE10_2023-10-18_141112_2023-10-19_120556_bounds.png}
    \includegraphics[width=\textwidth]{images/CelebA_VAE_2023-10-18_141112_2023-10-19_120556_bounds.png}
%    \caption{***}
    \caption{Histograms and Q-Q plots~\citep{gnanadesikan1968probability,field2009shapiro} for randomly selected images associated with each dataset and generative model combination. The horizontal dashed line is a boundary between positive outcomes, where our method provides superior upper bound estimates, and negative outcomes, where our results are suboptimal. Notably, when our method wins, the underlying distribution closely approximates a Gaussian distribution, as shown in the Q-Q plot. Conversely, when our bound estimates are worse, the distribution deviates significantly from a Gaussian. Note that for the MNIST dataset and the IWAE-5/10 models, our method outperforms the state-of-the-art for all images, indicating that no images fall on the right side of the dashed horizontal line.
}
    \label{fig:hist_qq-plot}
\end{figure}



 \paragraph{Ablation study} The overarching observation from our experiments is the significant influence of the distribution of the random variable $\overline{R_x(Z)}$ on the effectiveness of our method of log-likelihood estimation over the state-of-the-art. From the results presented in Table \ref{tab:my_label}, we learn that for more complicated latent distributions with a significant number of outliers (e.g., those induced by the models pre-trained on the CelebA dataset), there are only a few data points for which we obtain superior results compared to the method of \citet{pmlr-v206-struski23a}. We suspect that this is because the log-normal approximation obtained is not good enough (see criterion (i)), although we have tried to find a sufficiently large $n$ in each case.

To substantiate the above hypothesis, we present a detailed breakdown of the results for random data points in Figure~\ref{fig:hist_qq-plot}. For each dataset and generative model combination, we have specifically selected two image examples: one illustrating a scenario where our method is superior to that of~\citet{pmlr-v206-struski23a}, and the other illustrating a situation where our bounds lag behind.
It is clear that in the first case we were able to achieve a Gaussian approximation for the distribution of $\log \overline{R_x(Z)}$, while in the other case we definitely failed.



% The closer the distribution of these samples resembles a normal distribution, the more accurate our estimate tends to be. This insight is vividly reflected in the Figure~\ref{fig:hist_qq-plot}.


% These complementary experiments were conducted to investigate the performance and properties of our novel bounds on different generative models. By systematically analyzing their behavior in different scenarios, we aim to gain deeper insights into the strengths and limitations of our bounds when these models are trained on different datasets. The results of these experiments will contribute significantly to a more comprehensive exploration of the behavior of our bounds and their practical implications in a wide range of applications.




\section{Conclusions}
In this paper, we introduced a novel general lower and upper bound for Jensen's gap. By considering several special cases with different assumptions on the underlying function and distribution, we provided analytical and empirical arguments that our contribution has the potential to improve on state-of-the-art solutions provided in \citep{liao2019sharpening,lee2021further,pmlr-v206-struski23a}. In particular, we conducted experiments on real-world data, which demonstrated that our approach is superior to that of \citet{pmlr-v206-struski23a} as an efficient method for estimating the log-likelihood of generative models, provided that the appropriate criteria are satisfied.

We would like to emphasize that although our work presents a number of theoretical results, it is strongly focused on applications in the field of 
deep machine learning.
%deep generative modeling.
Therefore, we do not consider it as a purely mathematical or statistical contribution, but rather as a component that is inseparable from its experimental counterpart.

\paragraph{Limitations}
Our contribution has some limitations that could be considered as starting points for future work. First, the superiority of the provided log-likelihood estimation method strongly depends on the properties of the underlying latent distribution, which may cause some problems in experiments on large-scale datasets (see Subsection \ref{subsec:experiments}).
% the introduced non-additive bounds (although numerically stable and efficient) are questionable for use in the training process.
% excluding a direct use of sample means as unbiased estimators (see the discussion in Subsection \ref{subsec:boundslog}). 
Second, the considered real-world applications of this method are limited to generative models, which do not exhaust all possible uses of variational inference in machine learning. Finally, even though we provide rigorous bounds in all cases considered (expressed in terms of expectations of the respective random variables), their calculation in our experiments is based on corresponding estimators, which means that they may not be in real strict bounds.

% Althought our contribution delivers a wide range of both theoretical and experimental results, 


\paragraph{Societal impact}
We do not foresee any negative societal consequences of our contribution. However, even though the training of novel generative architectures was not the scope of our work, we cannot exclude the potential applicability of our approach for such purposes in further studies. Therefore, we must emphasize that the use of generative modeling in real-world applications requires careful monitoring to avoid amplifying societal biases (which are present in the data).


% \paragraph{Limitations}
% Our contribution has some limitations that could be considered as starting points for future work. First, the introduced non-additive bounds (although numerically stable and efficient) are questionable for use in the training process.
% % excluding a direct use of sample means as unbiased estimators (see the discussion in Subsection \ref{subsec:boundslog}). 
% Second, the applications of log-likelihood estimation presented here are limited to generative models, which do not exhaust all possible applications of variational inference in machine learning. Finally, even though we provide rigorous bounds in all cases considered (expressed in terms of expectations of the respective random variables), their calculation in our experiments is based on corresponding estimators, which means that they may not be in real strict bounds.

% % Althought our contribution delivers a wide range of both theoretical and experimental results, 

% \paragraph{Societal impact}
% We do not foresee any negative societal consequences of our contribution. However, even though the training of novel generative architectures was not the scope of our work, we cannot exclude the potential applicability of our approach for such purposes in further studies. Therefore, we must emphasize that the use of generative modeling in real-world applications requires careful monitoring to avoid amplifying societal biases (which are present in the data).








% \subsubsection*{Acknowledgements}
% This research was partially funded by the National Science Centre, Poland, grants no. 2021/43/B/ST6/01456 (work by Marcin Mazur), 2020/39/D/ST6/01332 (work by \L{}ukasz Struski). Some experiments were performed on servers purchased with funds from the flagship project entitled ``Artificial Intelligence Computing Center Core Facility'' from the DigiWorld Priority Research Area within the Excellence Initiative -- Research University program at Jagiellonian University in Kraków.

%\subsubsection*{References}

% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.

%% If you have bib database file and want bibtex to generate the
%% bibitems, please use
%%
 \bibliographystyle{elsarticle-num-names} 
 \bibliography{mybibliography}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

%% Refer following link for more details about bibliography and citations.
%% https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management

\appendix

\section{Other general bounds}\label{sec:other}

In this section, we present a brief outline of the general results due to \citet{liao2019sharpening} and \citet{lee2021further}. 

\begin{thm} Let $X$ be a one-dimensional random variable with mean $\mu$, and $P(X\in (a,b))=1$, where $-\infty\leq a<b\leq \infty$. Let $\phi(x)$ be a twice differentiable function on $(a,b)$, and define the function 
\begin{equation}
\begin{array}{@{}l @{\;} l @{\;} l}
h(x;\nu)=\frac{\phi(x)-\phi(\nu)}{(x-\nu)^2}-\frac{\phi'(x)}{x-\nu}. 
\end{array}
\end{equation}
Then 
\begin{equation}\label{eq:liaoberg}
\begin{array}{@{}l @{\;} l @{\;} l}
    \inf_{x\in (a,b)}\{h(x;\mu)\}\mathrm{var} X\leq \E\{\phi(x)\}-\phi(\E X)\leq \sup_{x\in (a,b)}\{h(x;\mu)\}\mathrm{var} X. 
\end{array}
\end{equation}
\end{thm}

The bounds provided by \eqref{eq:liaoberg} can sometimes be trivial, as shown in Example 1 in \citep{liao2019sharpening}. Specifically,  
for any random variable $X$ supported on $(-\infty,\infty)$ with finite variance and $\phi(x)=\exp(tx)$ ($t>0$) we have $\inf\{h(x;\mu)\}=0$ and $\sup\{h(x;\mu)\}=\infty$. Also, for $\phi(x)=\exp(\frac{1}{2}x)$ and $X\sim \text{Exponential}(1)$ we have $\inf\{h(x;\mu)\}>0$, but $\sup\{h(x;\mu)\}=\infty$. Note that in the latter case our bounds, given by \eqref{eq:jensensgapexpest}, are not trivial.


The following theorem is a generalization of the above result. 

\begin{thm1} Let $X$ be  random variable with mean $\mu$, and\linebreak $P\{X\in (a,b)\}=1$, where $-\infty\leq a<b\leq \infty$. Assume $\E\{|X-\mu|^k\}< \infty$ for $k=2m$, $m=1,2,3,\ldots$. Let $\phi(x)$ be a $(k+1)-$times differentiable function for $x \in (a,b)$, and define the function 
\begin{equation}
\begin{array}{@{}l @{\;} l @{\;} l}
    r^{(k)}(x;\mu)=\frac{\phi(x)-\phi(\mu)}{(x-\mu)^k}-\sum_{i=1}^{k-1}\frac{\phi^{i}(\mu)}{i!(x-\mu)^{k-i}}. 
\end{array}
\end{equation}
In addition, let $m_k=\inf_{x\in (a,b)} \{r^{(k)}(x;\mu)\}$ and $M_k=\sup_{x\in (a,b)} \{r^{(k)}(x;\mu)\}$. Then 
\begin{equation}\label{eq:lee}
\begin{array}{@{}l @{\;} l @{\;} l}
\sum_{i=1}^{k-1}\frac{\mu_i}{i!}\phi^{(i)}(\mu)+\mu_k m_k   \leq \E\{\phi(x)\}-\phi(\E X )\leq \sum_{i=1}^{k-1}\frac{\mu_i}{i!}\phi^{(i)}(\mu)+\mu_k M_k, 
\end{array}
\end{equation}
where $\mu_k=\E\{(X-\mu)^k\}$. 
\end{thm1}

Note that 
%$M_1=\sup_{x\in (a,b)}\{h(x;\mu)\}$, so in particular 
the upper bound for $\phi(x)=\exp(\frac{1}{2}x)$ and $X\sim \text{Exponential}(1)$ is also infinite. Theorem 2.1 of \citet{lee2021further} is also ineffective for finding the upper bound for $\phi(x)=\exp(x)$ and $X\sim \text{Normal}(0,1)$, while our method (see \eqref{eq:jensensgapexpnormalest}) gives non-trivial bounds. Moreover, the upper bound computed from (\ref{eq:lee}) for gamma or log-normally distributed $X$ and $\phi(x)=-\log (x)$ is infinite whereas the lower bound is equal to that provided by Theorem \ref{thm:improvedlog}, applied for $k=m$. 

The presented examples show that if the support of $X$ is not a compact set, Theorem 1 of \citet{liao2019sharpening} and Theorem 2.1 of \citet{lee2021further} may be useless. 

Sometimes the bounds given by \eqref{eq:liaoberg} and  \eqref{eq:lee} can be improved by partitioning the support of $X$. We outline this method as provided by \citet{lee2021further}, who generalize the approach of \citet{liao2019sharpening}. The idea of partitioning comes from \citep{walker2014lower}. 


Let $a=x_0<x_1<\cdots<x_m=b$, where $m$ is the number of partitions. In addition, let
$I_j=[x_{j-1},x_j)$, $p_j=P(X\in I_j)$, and $\gamma_j=\E(X|X\in I_j)$. 
Define a discrete random variable $Y$ with $P(Y=\gamma_j)=p_j$ for $j=1,2,\ldots,m$. Then
\begin{equation}
\begin{array}{@{}l @{\;} l @{\;} l}
        \E\{\phi(X)\}-\phi(\E X) &\geq & \sum_{i=1}^{k-1}\frac{\E\{(Y-\E Y)^i\}}{i!}\phi^{(i)}(\E Y)\\ &+& 
\E\{(Y-\E Y)^k\}\inf_{y\in [\gamma_1,\gamma_m]}\{r^{(k)}(y;\E Y)\}\\
&+&\sum_{j=1}^{m}p_j[\sum_{i=1}^{k-1} \frac{\E\{(X-\gamma_j|X\in I_j)^i\}}{i!}  \phi^{(i)}(\gamma_j)\\ &+&\E\{(X-\gamma_j|X\in I_j)^k\}\inf_{x\in I_j}\{r^{(k)}(x;\gamma_j)
\}
].\label{eq:walk} 
\end{array}
\end{equation}
If we replace infimum by supremum in  \eqref{eq:walk}, we get the corresponding upper bound. 

Returning to the toy example for a normally distributed random variable (see Subsection \ref{subsec:expexp}), \citet{lee2021further} used this technique to divide the support $(-\infty,\infty)$ into the following intervals: $(-\infty,-0.431)$, $(-0.431,0.431)$, and $(0.431,\infty)$. 



\section{Other bounds for logarithmic function}\label{sec:logbounds}
In this section, we briefly outline the other bounds for the logarithmic function that have been used as state-of-the-art competitors to the experiments provided by \citet{pmlr-v206-struski23a}.


\paragraph{$\chi$ upper bound (CUBO)}

The $\chi$ upper bound for the model log-likelihood, which was derived by \citet{dieng2017variational}, is given by the following formula:
\begin{equation}\label{app_cubo}
 \begin{array}{@{}l @{\;} l @{\;} l}
\text{CUBO}_x^n=\frac{1}{n}\log[\E_{Z\sim q(\cdot|x)}\{\frac{p(x,Z)}{q(Z|x)}\}^n]=\frac{1}{n}\log [\E_{Z\sim q(\cdot|x)}\{R_x(Z)^n\}].
\end{array}
\end{equation}
In \citep{dieng2017variational} it was proved that $\log p(x)\leq \text{CUBO}^x_n$ for every real number $n\geq 1$ and data point $x$.


\paragraph{Thermodynamic variational objective (TVO)}
The thermodynamic var\-ia\-tion\-al objective lower and upper bounds for the model log-likelihood, provided by \citet{masrani2019thermodynamic}, are given by the following formulas:
\begin{equation}\label{app_tvol}
\begin{array}{@{}l @{\;} l @{\;} l}
\text{TVO}^{L,K}_x=\frac{1}{K}\sum_{l=0}^{K-1}\E_{Z\sim \pi_{l/K}} \{\log [\frac{p(x,Z)}{q(Z|x)}]\} = \frac{1}{K}\sum_{l=0}^{K-1}\E_{Z\sim \pi_{l/K}} \{\log [R_x(Z)]\}
\end{array}
\end{equation}
and
\begin{equation}\label{app_tvou}
\begin{array}{@{}l @{\;} l @{\;} l}
\text{TVO}_x^{U,K}=\frac{1}{K}\sum_{l=1}^{K}\E_{Z\sim \pi_{l/K}}\{ \log [\frac{p(x,Z)}{q(Z|x)}]\}=\frac{1}{K}\sum_{l=1}^{K}\E_{Z\sim\pi_{l/K}} \{\log [R_x(Z)]\},
\end{array}
\end{equation}
where $\pi_\beta$ (for $\beta\in [0,1]$) is a latent distribution from a geometric path between $q(Z|x)$ and $p(x,Z)$. Note that $\text{TVO}_x^{L,1}=\text{ELBO}_x$ and $\text{TVO}_x^{U,1}=\text{EUBO}_x$. (Here, $\text{EUBO}_x$ denotes the evidence upper bound introduced by \citet{ji2019stochastic}.) It was proved in \citep{masrani2019thermodynamic} that 
$\text{TVO}_x^{L,K}\leq \log [p(x)]\leq  \text{TVO}_x^{U,K}$ for every integer $K\geq 1$ and data point $x$.



\section{Details on experimental setup}\label{sec:exp}

In this section, we present details of our experiments conducted on real-world datasets (see Subsection \ref{subsec:experiments}). 


\paragraph{Experimental details} In all experiments, we used the experimental setup of \citet{pmlr-v206-struski23a}. Specifically, we examined variational autoencoders (VAEs) and importance-weighted autoencoders (IWAEs) pre-trained on the MNIST, SVHN, and CelebA datasets with a variety of objective functions, including the evidence lower bound (ELBO), importance-weighted ELBO with 5 importance samples ($\text{IW-ELBO}_5$), and importance-weighted ELBO with 10 importance samples ($\text{IW-ELBO}_{10}$). Notably, we kept the same neural architectures for the IWAE models as for the VAE framework.



In our experimental study across all datasets, we selected for evaluation 10000 random images from each test set. In particular, in the case of the MNIST dataset, we included the entire test set since it contains exactly 10000 images. For each of these individual images, we applied a grid search procedure, described in Subsection \ref{subsec:experiments}, to find the smallest $n$ from the set of integers between 50 and 5000000 (depending on the dataset)
% {1000, 2000, 3000, 4000, 5000, 10,000, 20,000, 30,000, 40,000, 50,000, 100,000, 200,000, 300,000, 400,000, 500,000, 750,000, 1,000,000, 2,000,000, 5,000,000} 
for which the empirical standard deviation of $\log [\overline{R_x(z)}]=\log [\frac{1}{n}\sum_{i=1}^nR_x(z_i)]$ (where $x$ is an image from the dataset and $z$ is an $n$-sample from the latent space of a given generative model) does not exceed 0.3.

After determining $n$, we computed our additive and non-additive upper bounds for Jensen's gap, together with the one provided by \citet{pmlr-v206-struski23a}. All expectations arising in the respective bound formulas were estimated via averages over 10000 samples. 




\paragraph{Architecture details} 
We used the same convolutional VAE architecture as in \citep{pmlr-v206-struski23a}. The weights of the model were optimized using the Adam optimizer, and a learning rate of $0.0001$ was used. The training was performed on $100$ epochs with a batch size of $64$ for the CelebA dataset and $50$ epochs for both the MNIST and SVHN datasets. We report that we used Euclidean latent spaces $\mathcal{Z} = \mathbb{R}^d$, where $d$ took values of $8, 32, 128$, depending on the dataset used (specifically, MNIST, SVHN, and CelebA, respectively). We included standard Gaussian priors, defined as $p(z) \sim \mathcal{N}(0, I_d)$.
The encoders used Gaussian coding, characterized by $q(z|x) \sim \mathcal{N}(\mu_x, \Sigma_x)$, where $\mu_x$ is the mean and $\Sigma_x$ is a diagonal covariance matrix. The decoders also took a Gaussian approach, described as $p(x|z) \sim \mathcal{N}(q(z|x), \sigma^2 I)$, where $\sigma^2 = 0.3$.

Three different models were used, each tailored to a specific dataset. For the MNIST dataset, the network architecture consisted of two main components: an encoder and a decoder. The encoder had a predominantly two-layer fully-connected structure, while the decoder had a three-layer fully-connected structure. ReLU activation functions were incorporated between each layer.

For the SVHN dataset, deeper architectures were used for both the encoder and decoder. Both networks consisted primarily of four layers. In the encoder, we used only 2D convolutions and applied leaky ReLU activation functions with a leakiness parameter set to $0.2$. In the decoder, we used transposed 2D convolutions and applied ReLU activations, with the sigmoid activation function used in the last layer.

In the context of the CelebA dataset, the network architectures consisted primarily of replicated five-layer blocks. In the encoder network, each block included a 2D convolution layer, batch normalization~\citep{ioffe2015batch}, and a leaky ReLU activation with a leakiness parameter of $0.2$. A single block in the decoder network involved an operation that applied 2D nearest-neighbor upsampling to an input signal composed of multiple input channels. Then, similar to the encoder network block, there was a 2D convolution layer, batch normalization, and leaky ReLU activation with a leakiness parameter set to $0.2$.

\end{document}
