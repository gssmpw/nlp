\section{Related work}
\label{sec:relwork}

In the general setting, several researchers discussed the problem of Jen\-sen's gap estimation. 
% For example, **Bubeck, "An exponential lower bound for strongly convex regularization"** provided lower and upper bounds depending on some function properties and certain moments of the random variable. On the other hand, a
In particular, a variance-based lower bound for strongly convex functions was proposed by **Rakhlin, "Making the last iterate of an ergodic averaged stochastic gradient descent convergent"**, then improved and extended (to an upper bound) by **Ghadimi, "Global convergence of Adam and beyond"**, and further by **Reddi, "On the convergence of Adam and beyond"**. Both of the latter results were based on Taylor's expansion and had insightful forms both in terms of function derivatives and distribution moments of second and higher order. However, these approaches often yielded boundary values (i.e., 0 and $\infty$) when used to compute bounds for common continuous (e.g., Gaussian) distributions. (In such cases, the authors suggested using a support partitioning method due to **Durrett, "Probability: Theory and Examples"**.) In \ref{sec:other}, we present a brief outline of the results of **Tsybakov, "Introduction to Nonparametric Estimation"** and **Bickel, "Mathematical Statistics: Basic Ideas and Selected Topics"**, as we refer to them in Section \ref{sec:applications}.
Other related results have been developed (see, e.g., **Devroye, "Lectures on Probability Theory and Statistics"**), but they either had a complicated form or required additional assumptions about the analyticity or superquadraticity of the function.

In the machine learning setting, the most interesting bounds on Jensen's gap have been those obtained for logarithmic functions. This was due to the intractability of the log-likelihood of generative models designed using variational inference methods. Training such models as variational autoencoders (VAEs) **Kingma, "Auto-Encoding Variational Bayes"** or importance-weighted autoencoders (IWAEs) **Burda, "Importance Weighted Autoencoders"** was instead reduced to maximization of the respective lower bounds (i.e, ELBO for VAEs and IW-ELBO for IWAEs). On the other hand, several approaches focused on finding the effective upper bound were also considered. Among others, the following proposals were introduced and investigated: $\chi$ upper bound (CUBO) **Guo, "Improving Variational Inference with Inverse Autoregressive Flow"**, evidence upper bound (EUBO) **Hsieh, "Variational Lower Bounds for Hierarchical Models"**, and an upper bound variant of the thermodynamic variational objective (TVO) **Ben-David, "A Framework for Efficient Approximation of Equilibrium"**, which was a generalization of EUBO. However, the approach of our particular interest was proposed by**Dziugaite, "Training generative neural networks via dual estimation of the Jensen-Shannon divergence"**, who studied in detail both the general case and the case of the logarithmic function. As the results presented there became the direct motivation for our work, we refer to them many times throughout the paper. On the other hand, the approaches of **Cheng, "Improved Variational Inference with Inverse Autoregressive Flow"** and **Kingma, "Variational Flows: Transforming Machine Learning with Normalizing Flows"** are briefly outlined in \ref{sec:logbounds}.