\section{Related work}
\label{sec:relwork}

In the general setting, several researchers discussed the problem of Jen\-sen's gap estimation. 
% For example, \cite{gao2017bounds} provided lower and upper bounds depending on some function properties and certain moments of the random variable. On the other hand, a
In particular, a variance-based lower bound for strongly convex functions was proposed by \citet{bakula2016converse}, then improved and extended (to an upper bound) by \citet{liao2019sharpening}, and further by \citet{lee2021further}. Both of the latter results were based on Taylor's expansion and had insightful forms both in terms of function derivatives and distribution moments of second and higher order. However, these approaches often yielded boundary values (i.e., 0 and $\infty$) when used to compute bounds for common continuous (e.g., Gaussian) distributions. (In such cases, the authors suggested using a support partitioning method due to \citet{walker2014lower}.) In \ref{sec:other}, we present a brief outline of the results of \citet{liao2019sharpening} and \citet{lee2021further}, as we refer to them in Section \ref{sec:applications}.
Other related results have been developed (see, e.g., \citep{abramovich2004refining,dragomir2001some,dragomir2015inequality,horvath2014refinement,pecaric1985companion,walker2014lower,lovrivcevic2018zipf,banic2008superquadratic}), but they either had a complicated form or required additional assumptions about the analyticity or superquadraticity of the function.

In the machine learning setting, the most interesting bounds on Jensen's gap have been those obtained for logarithmic functions. This was due to the intractability of the log-likelihood of generative models designed using variational inference methods. Training such models as variational autoencoders (VAEs) \citep{kingma2013auto,rezende2015variational} or importance-weighted autoencoders (IWAEs) \citep{burda2015importance} was instead reduced to maximization of the respective lower bounds (i.e, ELBO for VAEs and IW-ELBO for IWAEs). On the other hand, several approaches focused on finding the effective upper bound were also considered. Among others, the following proposals were introduced and investigated: $\chi$ upper bound (CUBO) \citep{dieng2017variational}, evidence upper bound (EUBO) \citep{ji2019stochastic}, and an upper bound variant of the thermodynamic variational objective (TVO) \citep{masrani2019thermodynamic}, which was a generalization of EUBO. However, the approach of our particular interest was proposed by~\citet{pmlr-v206-struski23a}, who studied in detail both the general case and the case of the logarithmic function. As the results presented there became the direct motivation for our work, we refer to them many times throughout the paper. On the other hand, the approaches of \citet{dieng2017variational,ji2019stochastic} and \citet{masrani2019thermodynamic} are briefly outlined in \ref{sec:logbounds}.