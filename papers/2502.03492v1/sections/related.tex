\section{Related Work}
\paragraph{Self-Improvement of LLMs.}
Recent work has explored various approaches for LLMs to improve their outputs autonomously, including self-critique~\cite{madaan2024self,shinn2024reflexion}, debates~\cite{irving2018ai,michael2023debate,khan2024debating}, and training models to self-correct~\cite{welleck2022generating,kumar2024training}. However, \citet{huang2023large} demonstrates that without appropriate external feedback, such self-improvement loops may lead to performance degradation.  Our work addresses these challenges by learning specialized models that can provide effective feedback for improvement.

\paragraph{LLM Critics.}
Several approaches have been proposed to train LLMs as critics for various purposes, including generative reward models~\cite{ankner2024critique,xiong2024llava} and scalable oversight~\cite{saunders2022self,kenton2024scalable}.
These approaches either learn from human feedback~\cite{wang2023shepherd,mcaleese2024llm} or much more capable models' outputs~\cite{xi2024enhancing}, with recent work exploring reinforcement learning to improve feedback generation~\cite{akyurek2023rl4f,yao2023retroformer}.
Our approach differs in three key aspects: (1) leveraging execution feedback and model reasoning to synthesize high-quality critiques, (2) introducing variance reduction techniques to stabilize training, and (3) requiring only single-round critique-revision interactions.
Additional discussion on related work is provided in \cref{appendix:related}.

\input{tables/timeout}

\paragraph{Scaling Test-Time Compute.}
Recent work has explored various approaches to improve model performance at test time without fine-tuning~\cite{snell2024scaling}. While existing approaches focus on techniques like repeated sampling with proper selection mechanisms~\cite{brown2024large} and more sophisticated modular frameworks with existing models~\cite{saad2024archon}, we instead investigate test-time scaling through a decoupled critic model trained to provides targeted feedback to guide solution improvements.
Notably, while \citet{saad2024archon} demonstrates that strong models can serve as effective critics, their approach struggles with code generation tasks.





