\section{Experiments}
We conduct extensive experiments to evaluate our method's effectiveness across multiple benchmarks.
Our evaluation focuses on two key aspects: (1) the accuracy of the critic in identifying solution correctness, and (2) the quality improvement achieved through critique-guided revisions.

\subsection{Setup}
\paragraph{Training Data.}
We use TACO \cite{li2023taco}, a dataset containing 26,443 programming problems collected from competitive programming platforms like CodeForces and LeetCode. Each problem includes a natural language description and multiple test cases.
Due to noise in the original dataset (malformed test cases and contaminated problems), we filter the dataset to 18,820 problems for training, with details presented in \cref{appendix:training_details}.

\paragraph{Models.}
We base our critic model on the open-source Qwen2.5-Coder-Ins~\cite{hui2024qwen2} model.
During training, we fix the generator model to be Qwen2.5-Coder-Ins itself.
For evaluation, we assess the trained critic's performance by pairing it with various generator models for initial solution generation and subsequent revision, comparing against other LLM critics such as GPT-4o.

\paragraph{Benchmarks.} We evaluate our approach on three programming benchmarks and one general-domain benchmark:
(1) CodeContests~\cite{li2022competition}, a collection of challenging competitive programming problems;
(2) LiveCodeBench (24.08-24.11)~\cite{jain2024livecodebench}, a curated set of recent programming challenges designed to minimize data contamination;
(3) MBPP+~\cite{liu2024your}, an extension of the MBPP benchmark~\cite{austin2021program} focused on fundamental programming tasks; and
(4) JudgeBench~\cite{tan2024judgebench}, where we evaluate the model's effectiveness as a generative reward model for comparing solution pairs.

\paragraph{Metrics.}
To evaluate critiquing ability, we use three metrics: Pass@1 measures the success rate of the final solutions, $\Delta_\uparrow$ represents the fraction of initially incorrect solutions that become correct after revision, and $\Delta_\downarrow$ represents the fraction of initially correct solutions that become incorrect after revision. For discrimination ability, we employ F1 score when evaluating single solutions, and accuracy when comparing paired solutions in Judgebench, as the latter involves binary decisions between two alternatives.


\paragraph{Execution Sandbox.} We employ SandboxFusion~\cite{liu2024fullstack} as our execution environment, which provides a unified interface for evaluating solutions across training data and benchmarks through both function-based and standard input-output formats.



\subsection{Evaluating Critics for Iterative Critique-revisions}\label{sec:exp_critique_revision}
To evaluate the effectiveness of {\ours}, we present a comprehensive analysis of critique-revision strategies with different feedback mechanisms on CodeContests in \cref{tab:cc_res}.
The discrimination performance of critics is shown in \cref{tab:cc_discrimination}, while results across different benchmarks and generators are presented in \cref{tab:main}.

\paragraph{RL Significantly Boosts Critiquing Ability.}
Table~\ref{tab:cc_res} shows that our RL-trained critic significantly outperforms baseline approaches, achieving a 11.76\% pass@1 rate compared to 7.88\% with zero-shot generation. This substantial improvement builds upon a much reduced regression rate $\Delta_\downarrow$ than its SFT counterpart (0.85\% vs. 3.03\%).

\paragraph{{\ours} Enables Test-time Scaling.}
As shown in Table~\ref{tab:cc_res}, our approach enables test-time scaling through iterative critique-revisions. Notably, despite training exclusively on single-turn critiquing tasks, {\ours} generalizes to multi-turn settings. By increasing the number of iterations from one to three (Critique$\times3$ w/ {\ours}), we further improve the Pass@1 rate from 11.76\% to 15.15\% while maintaining a low regression rate $\Delta_\downarrow$ of 0.85\%. This demonstrates that our critic provides consistently reliable feedback across multiple revision iterations, unlike baseline approaches that accumulate errors, as discussed below.

\paragraph{{\ours} Mitigates Compounding Errors.}
Figure~\ref{fig:compounding_error} further illustrates this stability advantage - while both Qwen2.5-Coder and GPT-4o show increasing error compounding rates over iterations, {\ours} maintains a significantly lower rate, enabling reliable multi-round improvements.

\paragraph{{\ours} Generalizes to Different Generators and Tasks.}
While we train the critic model with Qwen2.5-Coder as the generator, as shown in \cref{tab:main}, our approach generalizes well across different programming tasks.
Notably, a weak critic model trained against itself can assist stronger model (GPT-4o), providing evidence for scalable oversight~\cite{christiano2018supervising,kenton2024scalable}.



\input{figs/compound_error}

\input{figs/difficulty}
\paragraph{Performance Scaling with Problem Difficulty.}
As shown in \cref{fig:difficulty}, our critique-revision approach demonstrates increasingly substantial relative gains as both iteration and  problem difficulty increases, revealing that {\ours} is particularly effective for complex tasks, where iterative refinement through targeted critique and revision yields the most significant benefits compared to zero-shot generation.

\subsection{Evaluating Critics as Generative Reward Models}\label{sec:judgebench_exp}
\input{figs/judgebench_v2}
One advantage of unifying textural feedback is to balance discrimination and critiquing abilities.
To assess our critics' discrimination capabilities, we evaluate them on JudgeBench~\cite{tan2024judgebench}, a comprehensive benchmark containing 350 GPT-4o completions across categories spanning general knowledge, reasoning, mathematics, and coding.
This setup presents a challenging out-of-distribution test in two aspects: \textbf{(1)} our critics must evaluate outputs from a more capable model than their training distribution, and \textbf{(2)} they need to generalize to broader domains beyond coding tasks.
This evaluation scenario is particularly interesting as it examines whether relatively weaker models can be effectively trained to judge outputs from more powerful models.

As shown in Figure~\ref{fig:judgebench}, {\ours} critic achieves competitive performance compared to stronger models such as Claude-3.5-Sonnet. Notably, while our critic is specifically trained on programming tasks, it maintains comparable overall accuracy (64.3\%) while demonstrating superior performance on coding-specific evaluations. This suggests that our {\ours} enables effective discrimination capabilities that generalize beyond the training domain.




\subsection{Analysis}
To better understand how {\ours} boosts iterative refinement, we further conduct analyses on the similarity between original and revised solutions, execution time changes, and critique characteristics. Our findings reveal several key patterns in how different critique methods influence the process of critique-revision.

\input{tables/generator_critic_matrix} 

\paragraph{The Effect of Generator Ability.}
As a preliminary analysis before finetuning experiments, we examine how model sizes affect critique-revision performance using Qwen2.5-Coder-Ins models (7B, 14B, and 32B) in an \emph{inference-only} setting, comparing zero-shot generation against critique-revision with critiques generated by another critic model conditioned on execution feedback.
\cref{tab:matrix} reveals that critic capability significantly influences improvement potentialâ€”while smaller critics (7B) often lead to performance degradation, larger critics (32B) consistently yield better outcomes, achieving up to 50\% improvement when paired with similarly-sized generators. The results also highlight the importance of critic-generator size relationships, as critics less capable than their generators typically degrade performance. These findings motivate us to focus our subsequent finetuning experiments with {\ours} on 32B models to maximize the benefits of critique-revision.



\paragraph{{\ours} Prevents Similar Revisions.}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figs/similarity_comparison.pdf}
    \vspace{-5mm}
    \caption{Comparison of solution similarities between original and revised code guided by {\ours} on CodeContests. Left: Distribution of similarity scores for self-critique and our {\ours} method. Right: Box plot showing the statistical distribution of similarity scores. Lower scores indicate more substantial revisions.}
    \label{fig:similarity}
\end{figure}

We analyze how different critique methods influence solution revisions by measuring code similarity scores between original and revised solutions, as described in \cref{appendix:evaluation_details}. As shown in Figure~\ref{fig:similarity}, self-critique tends to make conservative modifications with higher similarity scores (mean 0.482), while our {\ours} method proposes more substantial changes (mean 0.313). This suggests {\ours} is more willing to recommend major structural revisions when needed, rather than just local optimizations, which may explain its superior performance in improving solution quality.

\paragraph{{\ours} Trade-offs between Accuracy and Efficiency.}
While our critique-revision approach improves solution accuracy on LiveCodeBench, we observe a notable increase in timeout rates. Solutions guided by {\ours} exhibit a timeout rate of 16.61\%, higher than both zero-shot (10.54\%) and GPT-4o critic (8.93\%). However, even with more timeouts, {\ours} still achieves better overall Pass@1 accuracy. This suggests that our approach tends to generate more comprehensive solutions\,---\,while these may take longer to execute, the solution quality is guaranteed.













