\section{Conclusion}
We present {\ours}, a reinforcement learning framework for training critic LLMs to provide effective feedback for iterative refinement. Our trained critic demonstrates significant improvements over baselines across multiple benchmarks and enables efficient test-time scaling through iterative critique-revisions\,---\,notably, even when guiding stronger generators. While this work focuses on improving pass rates, future directions include optimizing for efficiency and safety, and extending our training pipeline towards multi-turn critique revision. We hope this work inspires further research into scalable LLM self-improvement through reinforcement learning.















\section*{Impact Statement}
This work aims to advance the field of Machine Learning by introducing a framework for training LLM critics. While this research has the potential to improve the reliability and robustness of AI systems, we have not identified any immediate societal concerns requiring specific attention. However, as with any AI technology, careful consideration should be given to its broader deployment and potential misuse.


