

\documentclass[table]{article}

\usepackage{markdown}
\usepackage{longtable}
\usepackage{listings}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %
\usepackage{amssymb}%
\usepackage{pifont}%
\usepackage{xspace}
\usepackage{bbm}
\usepackage{tcolorbox}
\tcbuselibrary{breakable,skins,theorems}
\usepackage{makecell}
\usepackage{tikz}
\usepackage{xcolor} %

\usetikzlibrary{positioning}
\usetikzlibrary{calc}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}
\usepackage{multirow}

\newcommand{\lpk}[1]{\textcolor{blue}{\bf \small [#1 --lpk]}}

\icmltitlerunning{Teaching Language Models to Critique via Reinforcement Learning}

\begin{document}

\input{notation}

\twocolumn[
\icmltitle{Teaching Language Models to Critique via Reinforcement Learning}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Zhihui Xie}{equal,hku}
\icmlauthor{Jie Chen}{equal,seed}
\icmlauthor{Liyu Chen}{seed}
\icmlauthor{Weichao Mao}{seed}
\icmlauthor{Jingjing Xu}{seed}
\icmlauthor{Lingpeng Kong}{hku}
\end{icmlauthorlist}

\icmlaffiliation{hku}{The University of Hong Kong}
\icmlaffiliation{seed}{Bytedance, Seed}

\icmlcorrespondingauthor{Zhihui Xie}{zhxieml@gmail.com}

\icmlkeywords{Machine Learning}

\vskip 0.3in
]

\noindent\begin{minipage}{\textwidth}
    \centering
    \vspace{-1.3cm}
    \href{https://critic-rl.github.io}{\texttt{https://critic-rl.github.io}}
\end{minipage}



\printAffiliationsAndNotice{\icmlEqualContribution}  %


\begin{abstract}
Teaching large language models (LLMs) to critique and refine their outputs is crucial for building systems that can iteratively improve, yet it is fundamentally limited by the ability to provide \emph{accurate judgments} and \emph{actionable suggestions}. In this work, we study LLM critics for code generation and propose {\ours}, a framework for \texttt{C}ritic \texttt{T}raining via \texttt{R}einforcement \texttt{L}earning, which trains a critic model to generate feedback that maximizes correction performance for a fixed generator model without human supervision. Our results demonstrate that critics trained with {\ours} significantly enhance pass rates and mitigate compounding errors across both base and stronger generator models.
Furthermore, we show that these critic models act as accurate generative reward models and enable test-time scaling through iterative critique-revision, achieving up to 106.1\% relative improvements across challenging code generation benchmarks.


\end{abstract}

\section{Introduction}
\input{figs/scaling}

\input{figs/illustration}

Recent advances in Large Language Models (LLMs) have sparked interest in their potential for self-improvement through iterative feedback mechanisms~\cite{pan2023automatically}. Methods like Reflexion \citep{shinn2024reflexion} and Self-Refine \citep{madaan2024self} demonstrate that LLMs can, in principle, critique their own outputs and generate refined responses. This self-improvement paradigm offers a promising direction toward more autonomous AI systems that can learn from their mistakes.

However, the effectiveness of such self-improvement mechanisms remains challenging in practice.
\citet{huang2023large} demonstrate that without appropriate external feedback, such self-improvement loops may lead to performance degradation.
To address this, existing approaches primarily rely on reward models~\cite{sun2023salmon,yuan2024self} or automated verification tools~\cite{gou2023critic,chen2023teaching}. However, these mechanisms often fail to provide actionable guidance\,---\,reward models compress complex evaluation criteria into simplified numerical signals~\cite{gao2023scaling,pan2024spontaneous}, while verification tools generate low-level execution traces that do not directly translate to high-level fixes~\cite{zhong2024ldb}.
Even in domains like code generation~\cite{li2022competition,sun2024survey} where such feedback mechanisms are readily available, previous work~\citep{zheng2024makes} as well as our experiment (\cref{tab:cc_res}) reveal that such feedback alone struggles to drive meaningful improvements. At the heart of this issue lies the \textit{feedback bottleneck}: feedback needs to both accurately discriminate the correctness of solutions and provide informative yet actionable suggestions for improvement. 


To address these challenges, we propose {\ours} (\texttt{C}ritic
\texttt{T}raining via \texttt{R}einforcement \texttt{L}earning), a framework that decouples the critic model from the task-performing model (e.g., GPT-4o) and focus on developing a specialized critic that can effectively drive the task-performing model toward optimal solution generation through iterative critique-revisions (\cref{fig:illustration}). This decomposition naturally introduces a well-defined \emph{proxy task} for training the critic model: while directly evaluating the quality of generated critiques remains challenging, the effectiveness of a critic can be measured by its ability to drive the task-performing model toward correct outputs. Though such indirect optimization signals lead to a large space of possible critiques and therefore high variance during training, we address this through a two-stage pipeline: first synthesizing high-quality critiques using execution feedback for supervised finetuning, then optimizing the critic through Group Relative Policy Optimization (GRPO; \citealt{shao2024deepseekmath}).





Through extensive evaluations on diverse benchmarks including CodeContests~\cite{li2022competition}, LiveCodeBench~\cite{jain2024livecodebench}, MBPP+~\cite{liu2024your}, and JudgeBench~\citep{tan2024judgebench}, we demonstrate that training with {\ours} significantly outperforms both  self-critique approaches and methods using stronger critic models. Notably, we observe remarkable generalization capabilities of the decoupled critic LLM across different problem domains and model scales. Our experiments demonstrate that relatively weaker critic models can effectively guide stronger task-performing models such as GPT-4o (\cref{tab:main}), exhibiting a similar phenomenon to weak-to-strong generalization \citep{christiano2018supervising,burns2023weak}, where weaker models can be trained to effectively supervise more capable ones.

Furthermore, {\ours} enables efficient test-time scaling (\cref{fig:scaling}). By providing targeted and actionable feedback, our critic significantly reduces the number of revision iterations needed, leading to both lower token consumption and higher success rates. Our empirical analysis (\cref{fig:compounding_error}) demonstrates that this efficiency stems from reduced error compoundingâ€”the critic effectively identifies and corrects mistakes early, guiding the model toward more direct solution paths without compromising solution quality.



Our work makes four key contributions: (1) We propose {\ours}, a novel framework that decouples critic LLMs from task-performing models and trains them through two-stage GRPO to guide code improvement. (2) Through extensive evaluation on programming benchmarks, we demonstrate that {\ours} significantly outperforms both self-critique methods and approaches using stronger critic models. (3) We establish that relatively weaker critic models can effectively guide stronger task-performing models, demonstrating a promising weak-to-strong generalization phenomenon in LLM guidance. (4) We show that a trained critic enables test-time scaling through iterative critique-revisions, achieving up to 106.1\% and 23.5\% relative Pass@1 improvements on the challenging CodeContests benchmark when paired with its base model and a stronger model, respectively.



\input{sections/preliminary_v3}
\input{sections/method}
\input{sections/exp}
\input{sections/related}
\input{sections/conclusion}


\bibliography{ref}
\bibliographystyle{icml2025}

\clearpage

\appendix
\onecolumn
\input{sections/appendix}

\end{document}
