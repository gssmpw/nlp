\section{Related Work}
\label{sec:relatedWork}
\begin{figure}
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=.45\linewidth]{figures/mmd_2d.pdf}
        \label{fig:2d_mmd}
    \end{subfigure}
    \hfill
    \begin{subfigure}
        \centering
        \includegraphics[width=.45\linewidth]{figures/std_over_eps_avg_over_xt.pdf}
        \label{fig:2d_stds}
    \end{subfigure}
    \caption{Left: Squared $\MMD$ between target distribution and sampled data according to different models, $x$-axis denotes the number of sampling steps. Right: average standard deviation of samples $X_0 | X_t$ produced by either true posterior distribution $p_{0 \vert t}$ or the models $\hat{x}_{\theta}(t,X_{t},\xi)$, $x$-axis is the timestep $t$.}
    \label{fig:2d_metrics}
\end{figure}

\textbf{Accelerated diffusion models.} Many strategies accelerate diffusion models, broadly classifiable as distillation- or sampling-based.  Distillation methods ____ train a student model to mimic a teacher diffusion model, leveraging consistency losses ____, adversarial losses ____, or noise coupling ____. See ____ for a detailed discussion.  While distillation is prevalent, other work focuses on improved samplers for larger step sizes ____.  Our work differs from both, neither training a student nor proposing new samplers.

\textbf{Discrepancy and diffusion.} We modify the diffusion model training loss to learn the conditional distribution $p_{0|t}(x_0|x_t)$.  The importance of approximating the covariance this distribution was noted in ____ and exploited in ____.  Closest to our approach is ____, which uses a GAN to better learn $p_{0|t}(x_0|x_t)$ for fewer sampling steps.  While sharing the same motivation, our method avoids adversarial training and discriminator training, offering a simple loss modification that encompasses standard diffusion models.  Other loss modifications, like using the $\ell_1$ loss ____, aim to improve output quality, not reduce sampling steps.  ____ can be seen as dual to our work: they learn discriminative kernel features with a flow at different noise levels, while we focus solely on learning a generator.

\textbf{Energy distances, MMD, and generative modeling.}
The MMD ____, of which energy distances ____ are a special case ____, has been widely used as a distributional loss in generative modeling. GANs have used MMDs with fixed kernels as critics to distinguish generated from reference samples ____. MMDs ____ and energy distances ____ have also been defined on adversarially trained discriminative neural net features.
The conditional generalized energy score of \eqref{eq:energyScoreConditional} has been used in learning conditional distributions by the DISCONet approach of ____,  and by the engression approach of ____ 
(which corresponds to the special case of $\lambda,\beta=1$). Recently, energy distances have been applied to speech synthesis ____, normalizing flows ____, neural SDEs ____, and other generative models ____.

To our knowledge, such losses have not previously been used to train diffusion models.
It is possible, however, to use an adaptation of  ____  in combination with the approach of ____, in order to incorporate conditional GAN-style generation into the reverse process of a diffusion model.  This idea was proposed by X. Shen  (personal communication, 6th November 2024) in an open  discussion following a presentation at Google Deepmind, as a future research direction of interest. The main idea of ____ is retained, namely to use a standard forward diffusion  process, and a distributional loss for the reverse process: however the adaptive-kernel MMD loss for the reverse process is replaced with a fixed-kernel energy score; and this score is used to train a sequence of conditional GAN generators to approximate $p(x_{t_{k-1}} |x_{t_{k}})$, rather than using particle diffusion directly. The approach indeed represents a promising line of work ____ for discretizing the reverse diffusion process, as distinct from the DDIM-style approach adopted in  \cref{sec:method}. The approach can further be understood as a non-adversarial formulation of  ____, which uses an adaptive conditional GAN critic in place of the fixed-kernel energy score.

\begin{figure}
    \centering
        \includegraphics[width=1\linewidth]{figures/engression_2d.png}
    \caption{Samples from true posterior $p_{0 \vert t}(\cdot | x_t)$ (light blue) for a sample $x_t$ (black cross) from $p_ {t|0}(x_t|x_0)$ for a specific $x_0$ (orange cross) and samples $\hat{x}_{\theta}(t,x_{t},\xi) \sim p^\theta_{0|t}(\cdot|x_t)$ (green dots) for $\beta=0.1,\lambda=1$.  Top/bottom row: $x_0$ is the mean of the left/right Gaussian. Samples from $\hat{x}_{\theta}(t,x_{t},\xi)$ (red) with $\beta=2,\lambda=0$ concentrate around $\mathbb{E}[X_0 | x_t]$ as expected.}
    \label{fig:trajectories_2d}
\end{figure}