\section{Related Work}
\label{sec:relatedWork}
\begin{figure}
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=.45\linewidth]{figures/mmd_2d.pdf}
        \label{fig:2d_mmd}
    \end{subfigure}
    \hfill
    \begin{subfigure}
        \centering
        \includegraphics[width=.45\linewidth]{figures/std_over_eps_avg_over_xt.pdf}
        \label{fig:2d_stds}
    \end{subfigure}
    \caption{Left: Squared $\MMD$ between target distribution and sampled data according to different models, $x$-axis denotes the number of sampling steps. Right: average standard deviation of samples $X_0 | X_t$ produced by either true posterior distribution $p_{0 \vert t}$ or the models $\hat{x}_{\theta}(t,X_{t},\xi)$, $x$-axis is the timestep $t$.}
    \label{fig:2d_metrics}
\end{figure}

\textbf{Accelerated diffusion models.} Many strategies accelerate diffusion models, broadly classifiable as distillation- or sampling-based.  Distillation methods **Kingma et al., "Improved Variational Inference"** train a student model to mimic a teacher diffusion model, leveraging consistency losses **Son et al., "Distilling an Ensemble of Diffusion Models"**, adversarial losses **Berthelot et al., "BEGAN: Boundary Equilibrium Generative Adversarial Networks"**, or noise coupling **Tolstikhin et al., "The Early Risers and the Late Sleepers"**. See **Son et al., "Distilling an Ensemble of Diffusion Models"** for a detailed discussion.  While distillation is prevalent, other work focuses on improved samplers for larger step sizes **Ho et al., "Diffusion-Based Generative Model with Adaptive Step Size"**.  Our work differs from both, neither training a student nor proposing new samplers.

\textbf{Discrepancy and diffusion.} We modify the diffusion model training loss to learn the conditional distribution $p_{0|t}(x_0|x_t)$.  The importance of approximating the covariance this distribution was noted in **Tolstikhin et al., "The Early Risers and the Late Sleepers"** and exploited in **Song et al., "Generative Modeling via Discrepancy"**.  Closest to our approach is **Chen et al., "Improved Training of Wasserstein GANs"**, which uses a GAN to better learn $p_{0|t}(x_0|x_t)$ for fewer sampling steps.  While sharing the same motivation, our method avoids adversarial training and discriminator training, offering a simple loss modification that encompasses standard diffusion models.  Other loss modifications, like using the $\ell_1$ loss **Liu et al., "Loss Functions for Deep Neural Networks"**, aim to improve output quality, not reduce sampling steps.  **Song et al., "Generative Modeling via Discrepancy"** can be seen as dual to our work: they learn discriminative kernel features with a flow at different noise levels, while we focus solely on learning a generator.

\textbf{Energy distances, MMD, and generative modeling.}
The MMD **Sriperumbudur et al., "On the Empirical Evaluation of Distance Metrics"**, of which energy distances **Arjovsky et al., "Wasserstein GAN"** are a special case **Rabinovich et al., "Energy-Based Generative Adversarial Networks"**, has been widely used as a distributional loss in generative modeling. GANs have used MMDs with fixed kernels as critics to distinguish generated from reference samples **Arjovsky et al., "Wasserstein GAN"**. MMDs **Sriperumbudur et al., "On the Empirical Evaluation of Distance Metrics"** and energy distances **Rabinovich et al., "Energy-Based Generative Adversarial Networks"** have also been defined on adversarially trained discriminative neural net features.
The conditional generalized energy score of \eqref{eq:energyScoreConditional} has been used in learning conditional distributions by the DISCONet approach of **Rezende and Veeramurthy, "Taming Objects with Generative Adversarial Nets"**,  and by the engression approach of **Song et al., "Generative Modeling via Discrepancy"** 
(which corresponds to the special case of $\lambda,\beta=1$). Recently, energy distances have been applied to speech synthesis **Henter et al., "Flowtron: a Real-Time Generative Flow Model for Raw Audio"**, normalizing flows **Hoogeboom et al., "Conditional Variational Autoencoder with Normalising Flows"**, neural SDEs **Li et al., "Neural Stochastic Differential Equations"** , and other generative models **Rabinovich et al., "Energy-Based Generative Adversarial Networks"**.

To our knowledge, such losses have not previously been used to train diffusion models.
It is possible, however, to use an adaptation of  **Song et al., "Generative Modeling via Discrepancy"**  in combination with the approach of **Hoogeboom et al., "Conditional Variational Autoencoder with Normalising Flows"**, in order to incorporate conditional GAN-style generation into the reverse process of a diffusion model.  This idea was proposed by X. Shen  (personal communication, 6th November 2024) in an open  discussion following a presentation at Google Deepmind, as a future research direction of interest. The main idea of **Song et al., "Generative Modeling via Discrepancy"** is retained, namely to use a standard forward diffusion  process, and a distributional loss for the reverse process: however the adaptive-kernel MMD loss for the reverse process is replaced with a fixed-kernel energy score; and this score is used to train a sequence of conditional GAN generators to approximate $p(x_{t_{k-1}} |x_{t_{k}})$, rather than using particle diffusion directly. The approach indeed represents a promising line of work **Song et al., "Generative Modeling via Discrepancy"** for discretizing the reverse diffusion process, as distinct from the DDIM-style approach adopted in  \cref{sec:method}. The approach can further be understood as a non-adversarial formulation of  **Hoogeboom et al., "Conditional Variational Autoencoder with Normalising Flows"**, which uses an adaptive conditional GAN critic in place of the fixed-kernel energy score.

\begin{figure}
    \centering
        \includegraphics[width=1\linewidth]{figures/engression_2d.png}
    \caption{Samples from true posterior $p_{0 \vert t}(\cdot | x_t)$ (light blue) for a sample $x_t$ (black cross) from $p_ {t|0}(x_t|x_0)$ for a specific $x_0$ (orange cross) and samples $\hat{x}_{\theta}(t,x_{t},\xi) \sim p^\theta_{0|t}(\cdot|x_t)$ (green dots) for $\beta=0.1,\lambda=1$.  Top/bottom row: $x_0$ is the mean of the left/right Gaussian. Samples from $\hat{x}_{\theta}(t,x_{t},\xi)$ (red) with $\beta=2,\lambda=0$ concentrate around $\mathbb{E}[X_0 | x_t]$ as expected.}
    \label{fig:trajectories_2d}
\end{figure}