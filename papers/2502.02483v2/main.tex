%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{upgreek}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{listings}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}
\usepackage[capitalise]{cleveref}
\usepackage{autonum} 
\usepackage[inline]{enumitem}
\usepackage{algpseudocode}
\usepackage{algorithm}



\usepackage[oxfordcolors,fancytheorems]{fancy_theorems}


% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\Id}{\mathrm{Id}}
\newcommand{\argmin}{\mathrm{argmin}}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2024}


% if you use cleveref..

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\argmax}{\mathrm{argmax}}
\newcommand{\pnc}{\overset{\leftrightarrow}{p}}
\newcommand{\pc}{\overset{\rightarrow}{p}}
\newcommand\lnc{\overset{\leftrightarrow}{l}\vphantom{l}}
%\newcommand{\hnc}[1]{\overset{\leftrightarrow}{h^{#1}}}
\newcommand\hnc{\overset{\leftrightarrow}{h}\vphantom{h}}
\newcommand\hc{\overset{\rightarrow}{h}\vphantom{h}}
\newcommand{\lc}{\overset{\rightarrow}{l}\vphantom{l}}
\newcommand{\x}{\bm{x}}
\newcommand{\pdata}{p_{\text{data}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\z}{\bm{z}}
\newcommand{\ptp}{p_{\theta, \phi}}
\newcommand{\rset}{\mathbb{R}}
\newcommand{\bfX}{\mathbf{X}}
\newcommand{\bfZ}{\mathbf{Z}}
\newcommand{\bfY}{\mathbf{Y}}
\newcommand{\bfB}{\mathbf{B}}
\newcommand{\vareps}{\varepsilon}
\newcommand{\rmd}{\mathrm{d}}
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\MMD}{\mathrm{MMD}}
\newcommand{\SNR}{\mathrm{SNR}}
\newcommand{\rmi}{\mathrm{i}}
\newcommand{\simG}{\mathcal{G}}
\newcommand{\rbf}{\texttt{rbf}}
\newcommand{\imq}{\texttt{imq}}
\newcommand{\expk}{\texttt{exp}}
\newcommand{\msx}{\mathsf{X}}
\newcommand{\nset}{\mathbb{N}}
\newcommand{\joint}{\mathrm{joint}}
\newcommand{\marginal}{\mathrm{marginal}}

\definecolor{customred}{HTML}{d1221d} % Define your custom red color
\definecolor{customblue}{HTML}{3c77df}
\definecolor{custompurple}{HTML}{bb1a4a}


\newcommand{\alex}[1]{{\color{blue}[Alex: #1]}}
\newcommand{\arthur}[1]{{\color{red}[Arthur: #1]}}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Distributional Diffusion Models}

\begin{document}

\twocolumn[
\icmltitle{Distributional Diffusion Models with Scoring Rules}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Valentin De Bortoli}{equal,gdm}
\icmlauthor{Alexandre Galashov}{equal,gdm,ucl}
\icmlauthor{J. Swaroop Guntupalli}{gdm}
\icmlauthor{Guangyao Zhou}{gdm}
\icmlauthor{Kevin Murphy}{gdm}
\icmlauthor{Arthur Gretton}{gdm,ucl}
\icmlauthor{Arnaud Doucet}{gdm}
\end{icmlauthorlist}

\icmlaffiliation{ucl}{Gatsby UCL}
\icmlaffiliation{gdm}{Google DeepMind}
%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Valentin De Bortoli}{vdebortoli@google.com}
\icmlcorrespondingauthor{Alexandre Galashov}{agalashov@google.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Diffusion models generate high-quality synthetic data. They operate by defining a continuous-time forward process which gradually adds Gaussian noise to  data until fully corrupted. The corresponding reverse process progressively ``denoises" a Gaussian sample into a sample from the data distribution. However, generating high-quality outputs requires many discretization steps to obtain a faithful approximation of the reverse process. This is expensive and has motivated the development of many acceleration methods. We propose to accomplish sample generation by learning the posterior {\em distribution} of  clean data samples  given their noisy versions, instead of only the mean of this distribution. This allows us to sample from the probability transitions of the reverse process on a coarse time scale, significantly accelerating inference with minimal degradation of the quality of the output. This is  accomplished by replacing the standard regression loss used to estimate conditional means with a  scoring rule.
We validate our method on  image and robot trajectory generation, where we consistently outperform standard diffusion models at few discretization steps.
\end{abstract}

% noncausal motivation through being in mask diffusion but still want to do speculative

\section{Introduction}

Diffusion models \citep{ho2020denoising,song2019generative,sohl2015deep} have demonstrated remarkable success in synthesizing high-quality data across various domains, including images \citep{saharia2022photorealistic}, videos \citep{ho2022video}, and 3D \citep{poole2022dreamfusion}. These models proceed as follows: First, a forward diffusion process is defined where Gaussian noise is progressively introduced to corrupt the data. This allows us to learn a denoiser at a continuum of noise levels. Next, to generate new samples, we sample from the time-reversed diffusion process, leveraging the learned denoiser. Despite their impressive capabilities, diffusion models often require a large number of steps to faithfully approximate  the time-reversal so as to obtain high-fidelity samples. 
To overcome this limitation, different solutions have been explored. One approach focuses on the development of improved numerical integrators (e.g. \citep{karras2022elucidating,lu2022dpm,zheng2023dpm}). Another common strategy involves the use of distillation techniques  (e.g. \cite{luhman2021knowledge,song2023consistency,luo2023comprehensive,salimans2024multistep}), which requires training a smaller, more computationally efficient model to emulate the behavior of a larger, pre-trained diffusion model. Finally, parallel simulation methods have also been explored (e.g. \citep{shih2023parallel,chen2024accelerating}) and use more memory to avoid slow sequential processing.

We depart from these earlier approaches in 
simply proposing to sample from the generative process on a coarser time scale. However, naively using denoisers obtained from the training of diffusion models significantly degrades the quality of the outputs, as these denoisers do not capture the full posterior distribution of the clean data given its noisy version, but rather its conditional mean. We review the relevant background on diffusions and their limitations in \Cref{{sec:diffusion_models}}.
In earlier works, it has been proposed by \citet{xiao2021tackling,xu2024ufogen} to use Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} to learn such conditional distributions. We show here how to bypass adversarial training by relying on generalized scoring rules \cite{GneRaf07}, which we describe in \Cref{sec:proper_scoring_rules}. This yields a simple loss which interpolates between the classical regression loss used in diffusion models and \emph{distributional} losses.  We present our new class of {\bf Distributional Diffusion Models} in \Cref{sec:method}, and provide theoretical grounding for our choice of distributional loss, as well as details of the interpolation to the regression loss, in \Cref{sec:theory}.  We review related work in \Cref{sec:relatedWork}.

In our experimental evaluation of  \Cref{sec:experimentsMain}, we demonstrate that our approach allows us to produce high-quality samples with  significantly fewer denoising steps. We observe substantial benefits across a range of tasks, including image-generation tasks in pixel space and latent space, and in robotics applications.
All proofs are given in the supplementary material and a table of notation is included in \Cref{sec:notation} for easy reference.

\section{Background \& Motivation}
\label{sec:background}

\subsection{Diffusion models}
\label{sec:diffusion_models}
We follow the Denoising Diffusion Implicit Models (DDIM) framework of \citet{song2020denoisingimplicit}. Let $p_0$ be a target data distribution on $\rset^d$. Consider $X_{t_0}\sim p_0$ and the process $X_{t_1:t_N}:=(X_{t_1},...,X_{t_N})$ distributed according to
\begin{equation}\label{eq:forward}
   p(x_{t_1:t_N}|x_{t_0})= p(x_{t_1}|x_{t_0}) \prod_{k=1}^{N-1} p(x_{t_{k}} | x_{t_0}, x_{t_{k+1}}) ,
\end{equation}
with $0 = t_0 < \dots < t_N = 1$. For $0\leq s<t\leq 1$, we define $p(x_{s} |x_0, x_{t})=\mathcal{N}(x_{s} ; \mu_{s,t}(x_0, x_{t}), \Sigma_{s,t})$ to ensure that for any $t \in [0,1]$, 
\begin{equation}
\label{eq:interpolation}
p(x_t | x_0) = \mathcal{N}(x_t ; \alpha_t x_0, \sigma_t^2 \Id) ,
\end{equation}
for some schedule $\alpha_t,\sigma_t$ chosen such that $\alpha_0 =\sigma_1 =1$ and $\alpha_1 = \sigma_0 = 0$. This ensures in particular that $p(x_1|x_0) = \mathcal{N}(x_1 ; 0,\Id)$. One possible popular schedule is given by the \emph{flow matching} noise schedule \citep{lipman2022flow,albergo2023stochastic,gao2025diffusionmeetsflow}, i.e., 
\begin{equation}
    \textstyle
    \alpha_t=1-t, \qquad \sigma_t = t . 
    \label{eq:flow_matching_noise_schedule}
\end{equation}
The mean and covariance of $p(x_{s} |x_0, x_{t})$ are given by
\begin{align}
    &\mu_{s,t}(x_0, x_t) = \vareps^2 r_{1,2}(s,t) + (1 - \vareps^2) r_{0,1}) x_{t} \\
    & \quad + \alpha_{s}(1 - \vareps^2 r_{2,2}(s,t) - (1 - \vareps^2) r_{1,1}(s,t)) x_0 ,
\\
& \Sigma_{s,t} =  \sigma_{s}^2 (1 - (\vareps^2 r_{1,1}(s,t) + (1-\vareps^2))^2) \Id, \label{eq:mean_sigma}
\end{align}
with $r_{i,j}(s,t) = \left(\alpha_t/\alpha_s\right)^i \left(\sigma_s^2/\sigma_t^2\right)^j $; see \citep{song2020denoisingimplicit} and \Cref{sec:ddim_sde} for a discussion. In \eqref{eq:mean_sigma}, $\varepsilon \in [0,1]$ is a \emph{churn} parameter which interpolates between a \emph{deterministic} process ($\varepsilon=0$) and a \emph{stochastic} one ($\varepsilon=1$).

We generate data $X_{t_0}\sim p_0$ by sampling $X_{t_N}\sim \mathcal{N}(0,\Id)$ and 
$X_{t_k}\sim p(\cdot|X_{t_{k+1}})$ for $k=N-1,...,0$, where for $0\leq s<t\leq 1$
\begin{equation}
\label{eq:backward_kernel}
    p(x_s|x_t) = \int_{\rset^d} p(x_s|x_0, x_t) p(x_0 | x_t) \rmd x_0 .
\end{equation}
It can be easily checked that this process has indeed the same marginal distributions, denoted $p_{t_k}(x_{t_k})$, as the process defined by $p_0(x_{t_0})p(x_{t_1:t_N}|x_{t_0})$ (see \eqref{eq:forward}) so that in particular $X_{t_0}\sim p_0$; see e.g. \citep[Appendix E]{shi2024diffusion}.

Subsequently, to avoid  ambiguity, we  write $p_{s|t}(x_s|x_t)$ for $p(x_s|x_t)$ and $p_{s,t}(x_s,x_t)$ for $p(x_s,x_t)$ for any $0\leq s<t \leq 1$. Usually $p_{0|t}(x_0 | x_t)$ in \eqref{eq:backward_kernel} is approximated for any $t$ by $\updelta_{\hat{x}_{\theta}(t,x_t)}$, where $\hat{x}_{\theta}(t, x_t)\approx \mathbb{E}[X_{0}|X_t=x_t]$ is a neural network denoiser trained using a regression loss; i.e. 
\begin{equation}\label{eq:diffusionloss}
    \mathcal{L}_{\text{diff}}(\theta)=\int^1_0 w_t \mathbb{E}[||X_0-\hat{x}_\theta(t,X_t)||^2]\rmd t,
\end{equation}
for a weighting function $w_t$ \cite{kingma2021variational}. 
Approximating $p_{0|t}(x_0|x_t)$ with a Dirac mass located at $\hat{x}_\theta(t,x_t)$ seems crude at first. However, as $N\rightarrow \infty$ with $t_{k+1} \to t_k$, the resulting \emph{discrete} time process converges to a \emph{continuous} time one which recovers the data distribution if $\hat{x}_\theta(t,X_t)=\mathbb{E}[X_0|X_t]$; see e.g. \cite{song2020score}.

When performing a coarse time discretization, this result no longer holds, and the Dirac approximation is poor. 
We propose here to learn a generative network to sample approximately from $p_{0|t}(x_0|x_t)$. 

\subsection{Scoring rules}
\label{sec:proper_scoring_rules}

We next recall the framework of scoring rules, as reviewed
by \citet{GneRaf07}, and then describe the specific scoring rules
used in this work. Consider two probability distributions $p,q$. A \emph{scoring rule} $S(p,x)$ indicates
the quality of a prediction $p$ when the event $Y=y$ is
observed. The \emph{expected score} is its expectation under $q$, 
\[
S(p,q):=\mathbb{E}_{q}[S(p,Y)].\label{eq:expected_score}
\]
A scoring rule is \emph{proper} when 
$S(q,q)\ge S(p,q)$. 
It is called \emph{strictly proper} with respect to a class of
distributions $\mathcal{P}$ when the equality holds if and only if  $p=q,$ for all  $p,q\in\mathcal{P}$.
In this work we focus on the class of proper scoring rules called \emph{kernel scores} introduced by \citet[Section 5.1]{GneRaf07} which take the form
\[
S_{\rho}(p,y)=\frac{1}{2} \mathbb{E}_{p\otimes p}
[\rho(X,X')]-\mathbb{E}_{p}[\rho(X,y)],\label{eq:proper_score}
\]
where $\rho$ is a continuous negative definite kernel. 
Similarly to \citet[Section 3.2]{bouchacourt2016disco}, we will also consider \emph{generalized kernel scores} of the form 
\[
S_{\lambda, \rho}(p,y)=\frac{\lambda}{2} \mathbb{E}_{p\otimes p}
[\rho(X,X')]-\mathbb{E}_{p}[\rho(X,y)],\label{eq:generalized_kernel_score} 
\]
with $\lambda \in [0,1]$. 
One score of particular interest is the \emph{energy score} \citep[eq. 22]{GneRaf07} denoted $S_{\beta}$,
which uses 
$\rho(x,x')=\|x-x'\|^{\beta}$ with $\beta \in (0,2)$. We denote $S_{\lambda, \beta}$, its generalized version, called the \emph{generalized energy score}. 
We will also employ $\rho = -k$ with $k$ a continuous positive definite kernel, see e.g. \cite{BerTho04} and \Cref{sec:notation} for a definition. Similarly to \eqref{eq:expected_score}, we define the \emph{(generalized) expected energy score} and \emph{(generalized) expected kernel score}. 


Positive definite kernels are said to be characteristic to $\mathcal{P}$
when $S_{\rho}$ yields a strictly proper scoring rule on $\mathcal{P}$
\citep{SriGreFukLanetal10,SriFukLan11}. While the exponentiated quadratic
(\rbf) kernel satisfies this property, we will also investigate other kernels such as the inverse multiquadratic (\imq) kernel and the exponential kernel (\expk),
\begin{subequations} \label{eq:kernels_specific}
\begin{align}
k_{\mathrm{\imq}}(x,x') & =(\|x-x'\|^{2}+c)^{-1/2},\label{eq:imq_k}\\
k_{\rbf}(x,x') & =\mathrm{exp}\left[-\|x-x'\|^2/2\sigma^2\right],\label{eq:rbf_k} \\
k_{\expk}(x,x') & =\mathrm{exp}\left[-\|x-x'\|/\sigma\right].\label{eq:exp_k}
\end{align}
\end{subequations}
Any proper scoring rule $S_\rho$ defines a divergence 
\begin{equation}\label{eq:divergence}
D_\rho(p,q) := S(q,q) - S(p,q).
\end{equation}
When $S=S_\rho$ and $\rho(x,x') = -k(x,x')$ with $k$ a positive definite kernel, we recover the squared Maximum Mean Discrepancy ($\MMD^2$) \cite{gretton2012kernel}. For $\rho(x,x')=\|x-x'\|^{\beta}$, we obtain the \emph{energy distance} \cite{rizzo2016energy,sejdinovic2013equivalence}. 


Letting $\beta \to 2$ for the energy distance, we obtain $D_\rho(p,q) = \| \mathbb{E}_p[X] - \mathbb{E}_q[X] \|^2,$ resembling the integrand of \eqref{eq:diffusionloss}. Indeed, up to a constant $C$ independent of $\theta$, letting $p=\updelta_{\hat{x}_\theta(t,X_t)}$ and $q=p_{0|t}(\cdot|X_t)$, we get
\begin{equation}
\label{eq:recover_diffusion}
    \mathbb{E}_{p_t}[D_\rho(p,q)] = \mathbb{E}[\| X_0 - \hat{x}_{\theta}(t, X_t) \|^2] + C ,
\end{equation}
see \Cref{sec:correspondence_between_discrepancy_and_diffusion} for a proof. 
In the case of $\rho=-k$ with $k$ given by \eqref{eq:imq_k} or \eqref{eq:rbf_k}, the relation between $D_{\rho}$ and the diffusion loss \eqref{eq:diffusionloss} is less immediate
than for the energy distance. However the connection becomes apparent
by keeping the leading terms in the Taylor expansions of the kernels,
as shown in \Cref{prop:diffusion_compatible}. 


\section{Distributional Diffusion Models}
\label{sec:method}

We now introduce the Distributional Diffusion Models. At a high level, we replace the \emph{regression}
loss \eqref{eq:diffusionloss}, used to learn approximation of the conditional mean
$\mathbb{E}[X_{0}|X_t]$
in diffusion models, with a  loss based on \emph{scoring rules}, to learn an approximation $p_{0\vert t}^{\theta}(x_{0}\vert x_{t})$ of $p_{0|t}(x_0|x_t)$. This is achieved by learning a generative network $\hat{x}_{\theta}(t,x_{t},\xi)$ which aims to produce samples $X_{0} \sim p_{0\vert t}^{\theta}(\cdot \vert x_{t})$. This model takes as input a time $t \in [0,1]$, a noisy sample $x_t$ and a Gaussian noise sample $\xi\sim\mathcal{N}(0,\Id)$. Sampling from the noise $\xi$ allows us to \emph{sample} from this model (as in GANs).
The model's objective is to approximate the full distribution $p_{0|t}(x_0|x_t)$, i.e., $\hat{x}_{\theta}(t,X_{t},\xi) \overset{d}{=} X_0 | X_t, t$ and $\xi \sim \mathcal{N}(0, \Id)$.
Whenever the model $\hat{x}_{\theta}(t,X_{t},\xi)$ is used, we refer to such a method as \emph{distributional}. Using this model allows us to sample from diffusion models with fewer steps than classical diffusion models, leveraging \eqref{eq:backward_kernel}.

\textbf{Conditional Generalized Energy Score.} We base our training loss on \emph{generalized energy scores} as introduced in \Cref{sec:proper_scoring_rules}. For each $t,x_0,x_t$, we consider a \emph{conditional} generalized energy score with $\beta \in (0,2]$ and $\lambda \in [0,1]$
\begin{align}
\textstyle
S_{\lambda, \beta}(p_{0\vert t}^{\theta}(\cdot|x_t), x_0) &=  
\frac{\lambda}{2} \mathbb{E}_{p_{0\vert t}^{\theta}(\cdot|x_t)\otimes p_{0\vert t}^{\theta}(\cdot|x_t)}[\|X-X'\|^{\beta}] \\
 & \quad -\mathbb{E}_{p_{0\vert t}^{\theta}(\cdot|x_t)}[\|X-x_0\|^{\beta}] .
 \label{eq:energyScoreConditional}
 \end{align}
The conditional energy score was first proposed for training conditional generative models by \citet{bouchacourt2016disco}, and its use in extrapolation and model estimation was established by \citet{shen2024engression} in the case $\lambda=\beta=1$.
We emphasize that \eqref{eq:energyScoreConditional} is \emph{conditional}, since it compares the conditional distribution $p_{0\vert t}^{\theta}(\cdot|x_t)$ to $x_0$. We recall that this scoring rule is strictly proper when $\lambda=1$ and $\beta \in (0,2)$. The hyperparameter $\lambda$ allows us to control the trade-off between diversity (first ``interaction" term)
and accuracy (second ``confinement" term).

\textbf{Energy diffusion loss.} Our final expected loss on $\theta$  integrates the \emph{conditional} \emph{generalized} energy score over both the noise level $t\in[0,1],$ \emph{and} the samples from the dataset, $X_0 \sim p_0$, together with noisy samples $X_t \sim p_{t\vert 0}(\cdot \vert X_0)$ so that $(X_0,X_t)\sim p_{0,t}$. This gives the \emph{energy diffusion loss}
\begin{equation}\label{eq:energyloss}
\mathcal{L}(\theta)=-\int_{0}^{1} w_{t} \mathbb{E}_{p_{0,t}} \left[ S_{\lambda, \beta}(p_{0\vert t}^{\theta}(\cdot|X_t),X_0) \right]\rmd t,
\end{equation}
where $w_{t}$ is a user-defined weighting function.
The loss~\eqref{eq:energyloss} has the following remarkable property: when we set $\beta \to 2$ and $\lambda \to 0$, we recover the classical regression diffusion loss \eqref{eq:diffusionloss}.
When $\beta \in (0, 2)$ and $\lambda=1$ we obtain an
(integrated) strictly proper expected \emph{conditional} energy score.
By selecting $\lambda \in (0, 1)$ and $\beta \in (0,2]$, we can interpolate between these two cases. 
Our approach shares with \citep{bouchacourt2016disco,gritsenko2020spectral,chen2024generative,Pacchiardi2024,shen2024engression} the conditional energy score \eqref{eq:energyScoreConditional}, but differs in that these earlier works learned only a {\em single} conditional distribution, whereas we integrate over many different noise levels."

\textbf{Empirical energy diffusion loss.} 
For $n$ samples $\{X^i_0\}_{i=1}^n \overset{\mathrm{i.i.d.}}{\sim} p_0$ from the training dataset and corresponding $n$ noise levels $\{t_{i}\}_{i=1}^n \overset{\mathrm{i.i.d.}}{\sim}\mathcal{U}[0,1]$, we sample noisy data points $X_{t_{i}}^{i} | X_{0}^{i}, t_i \sim p_{t_i | 0}(\cdot | X_0^i)$ from the forward diffusion process~\eqref{eq:interpolation} with noise schedule~\eqref{eq:flow_matching_noise_schedule}. Then, for each pair $(X_{0}^{i},X_{t_{i}}^{i})$, we sample $m$ Gaussian noise samples $\xi^j_i\sim\mathcal{N}(0,\Id)$, $j\in[m]$ which allows us to compute $\hat{x}_{\theta}(t_{i},X_{t},\xi_i^j)$. We then obtain the following empirical energy diffusion loss
\begin{align}\label{eq:lossengression}
 & \mathcal{L}_{n,m}(\theta){ =\frac{1}{nm}\sum_{i,j=1}^{n,m}w_{t_{i}}\Bigg[\left\| X_{0}^{i}-\hat{x}_{\theta}(t_{i},X_{t},\xi_i^j)\right\| ^{\beta}}\\
 & \ \frac{-\lambda}{2(m-1)}\sum_{j'\neq j,}^{m}\left\Vert \hat{x}_{\theta}(t_{i},X_{t_{i}},\xi_{i}^{j})-\hat{x}_{\theta}(t_{i},X_{t_{i}},\xi_{i}^{j'})\right\Vert ^{\beta}\Bigg].
\end{align}

In Section~\ref{sec:theory}, we discuss an alternative training loss using the \emph{joint} \emph{generalized} energy score which would focus on $p_{0, t}^{\theta}$ and show that its empirical version suffers from much higher variance.  See also Appendix~\ref{app_sec:different_learning_objectives} for a longer discussion.

\textbf{Kernel diffusion loss.}
Similarly, we can define a \emph{kernel diffusion loss} by replacing $\| x-x' \|^\beta$ with a characteristic kernel $-k(x,x')$, such as \eqref{eq:imq_k} or \eqref{eq:rbf_k}, in~\eqref{eq:energyScoreConditional} and \eqref{eq:energyloss}. As for the energy score, we can  recover the diffusion loss \eqref{eq:diffusionloss} for $k_{\mathrm{\imq}}$ and $k_{\mathrm{\rbf}}$ by identifying the leading terms in the Taylor expansion of these kernels.  We refer the reader to \Cref{sec:diffusion_compatible} for more details.

\begin{figure}
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=.48\linewidth]{figures/isocurve_f.png}
        \label{fig:beta_evolution}
    \end{subfigure}
    \hfill
    \begin{subfigure}
        \centering
        \includegraphics[width=.48\linewidth]{figures/variance_gaussian.png}
        \label{fig:lambda_evolution}
    \end{subfigure}
    \caption{Left: reduction factor $f(\lambda, \beta)$ as a function of $\beta \in [0,2]$ and $\lambda \in [0,1]$. Right: evolution of the covariance during the sampling of distributional  diffusion model for a Gaussian target $p_0 = \mathcal{N}(0, 4 \Id)$ for $20$ of $N \in \{5, 10, \dots, 95, 100\}$, $\lambda=0.5$. As $N$ increases, the output variance gets closer to $4$.}
    \label{fig:overall}
        \label{fig:stepsize_matters}
\end{figure}

\textbf{Architecture and training.}
Unlike standard diffusion models, the neural network $\hat{x}_{\theta}(t,x_{t},\xi)$ takes not only $t$ and $x_t$ as input, but also $\xi \sim \mathcal{N}(0, \Id)$ to obtain an approximate sample from $p_{0|t}(x_0|x_t)$. For simplicity, in image generation, we concatenate $[x_t, \xi]$ along the channel dimension without modifying the rest of the architecture, see \Cref{app_sec:experimental_details} for more details, and
\Cref{alg:training_diffusion} for the full training algorithm.

\begin{algorithm}
\caption{Distributional Diffusion Model (training)}\label{alg:training_diffusion}
\begin{algorithmic}
\Require $M$ training steps, schedule $(\alpha_t, \sigma_t)$,  distribution $p_0$, weights $\theta_0$, batch size $n$, population size $m$
\For{$k=1:M$}
\State Sample $t_i \sim \mathrm{Unif}([0,1])$~ for $i\in[n]$
\State Sample $X_0^{i}  \overset{\textup{i.i.d.}}{\sim}  p_0$~ for $i\in[n]$
\State Sample $X_{t_i}^{i} \sim p_{t_i \vert 0}(\cdot | X_0^{i})$~for $i\in[n]$ using~\eqref{eq:interpolation}
\State Sample $\xi_{i,j} \sim \mathcal{N}(0, \Id)$ for $i\in[n],j\in[m]$
\State Set $\theta_{k} = \theta_{k-1} - \delta \nabla \mathcal{L}_{n,m}(\theta)|_{\theta_{k-1}}$ using \eqref{eq:lossengression}
\EndFor
\State \textbf{Return:} $\theta_{M}$
\end{algorithmic}
\end{algorithm}

Once again, we emphasize that setting $\lambda = 0$ and $\beta = 2$ in \Cref{alg:training_diffusion} recovers the classical diffusion model loss. 

\textbf{Sampling.}
Once we have trained our model $\hat{x}_{\theta}$, we generate samples using \Cref{alg:sampling_diffusion}. This procedure is very similar to DDIM \citep{song2020denoisingimplicit}.  The only difference is that now $\hat{x}_{\theta}$ outputs an approximate \emph{sample} from $p_{0|t}(x_0|x_t)$ instead of an approximation of $\mathbb{E}[X_0|X_t=x_t]$.

\begin{algorithm}
\caption{Distributional Diffusion Model (sampling)}\label{alg:sampling_diffusion}
\begin{algorithmic}
\Require $\{t_k\}_{k=0}^N$ with $t_0 = 0 < \dots < t_N = 1$, churn parameter $\vareps$ 
\State Sample $X_{t_N} \sim \mathcal{N}(0, \Id)$
\For{$k \in \{N-1, \dots, 0\}$}
\State Sample $\xi \sim \mathcal{N}(0, \Id)$
\State Sample $Z \sim \mathcal{N}(0, \Id)$
\State Set $\hat{X}_0 = \hat{x}_{\theta}(t_{k+1}, X_{t_{k+1}}, \xi)$
\State Compute $\mu_{t_k, t_{k+1}}(\hat{X}_0, X_{t_{k+1}})$,  $\Sigma_{t_k, t_{k+1}}^{1/2}$ using~\eqref{eq:mean_sigma}
\State Set $X_{t_k} = \mu_{t_k, t_{k+1}}(\hat{X}_0, X_{t_{k+1}}) + \Sigma_{t_k, t_{k+1}}^{1/2} Z$
\EndFor
\State \textbf{Return:} $X_0$
\end{algorithmic}
\end{algorithm}

\section{Theoretical analysis}
\label{sec:theory}

In this section, we provide some theoretical understanding of the generalized scoring rules introduced in \Cref{sec:method}.

\subsection{Gaussian analysis}
\label{sec:gaussian}
We shed more light on the two main hyperparameters of the generalized kernel score introduced in \Cref{sec:proper_scoring_rules}  
\begin{enumerate*}[label=(\roman*)]
    \item the kernel parameter ($\beta$ in the case of the energy score)
    \item the trade-off term $\lambda$.
\end{enumerate*}
We focus here on the case of  \emph{generalized energy score} defined in \eqref{eq:generalized_kernel_score}.
\begin{proposition}{}{reduction_variance}
Assume that $p = \mathcal{N}(\mu, \sigma^2 \Id)$ for $\mu \in \rset^d$ and $\sigma >0$. Then, for any $\lambda \in [0,1]$ and $\beta \in [0,2]$, we have that  $q^\star_{\lambda, \beta} = \mathcal{N}(\mu_\star, \sigma_\star^2 \Id)$ maximizes $S_{\lambda, \beta}(p, q)$ defined by~\eqref{eq:generalized_kernel_score} among all Gaussian distributions $q$, with parameters
 \begin{equation}
    \mu_\star = \mu , \qquad \sigma_\star^2 = (2 \lambda^{-2 / (2 - \beta)} - 1)^{-1} \sigma^2 . 
\end{equation}
\end{proposition}
%Refer to Appendix~\ref{sec:gaussian_setting} for proofs.

A notable outcome of maximizing the generalized energy score is that 
% A notable property of the generalized energy distance discrepancy is that
$\mu^\star = \mu$ no matter the value of $\lambda$ and $\beta$. With $\lambda=1$ and $\beta < 2$, we also recover the correct variance, which is to be expected as $S_{\lambda, \beta}$ is a strictly proper energy score. As soon as $\lambda <1$, the variance $\sigma^2$ is underestimated by a factor $f(\lambda, \beta) = (2 \lambda^{-2 / (2 - \beta)} - 1)^{-1}$; see \Cref{fig:overall}. 

We now consider the effect of these hyperparameters when sampling from the corresponding generative model using \Cref{alg:sampling_diffusion}.  For simplicity, we let the churn parameter $\vareps=1$ but our analysis also applies to $\vareps \in [0,1)$. We consider a Gaussian target $p_0 = \mathcal{N}(\mu, \sigma^2 \Id)$. In that case, $p_{0|t}(x_0|x_t)$ is also Gaussian for any $t \in [0,1]$. Therefore if this density was estimated using the generalized energy score, the results of \Cref{prop:reduction_variance} apply. In particular, this implies that the induced estimate of the Gaussian distribution $p_{s|t}$ obtained by plugging our approximation of $p_{0|t}$ into \eqref{eq:backward_kernel} would have the correct mean but an incorrect variance for $\lambda<1$.

As expected, differences between classical diffusion models and distributional diffusion models arise when considering larger discretization stepsizes, where the Dirac approximation of $p_{0|t}(x_{0}|x_t)$ is no longer valid, see \Cref{fig:stepsize_matters}.

\subsection{Joint or conditional scoring rules}\label{subsec:jointvsconditionaltraining}
Given two unbiased estimators $\hat{A}$ and $\hat{B}$ of $A$ and $B$, we say that $A$ is easier to approximate than $B$ if $\SNR(\hat{A}) \geq \SNR(\hat{B})$ where, for an unbiased estimate $\hat{C}$, $\SNR(\hat{C}): = \mathbb{E}[\hat{C}]^2 / \mathrm{Var}(\hat{C})$; i.e. the inverse relative variance of $\hat{C}$.
We investigate here one alternative to the loss \eqref{eq:energyloss} and show that its empirical version exhibits lower SNR than the empirical version of \eqref{eq:energyloss} under the simplifying assumption that  $\theta$ is chosen so that $p^\theta_{0|t}=p_{0|t}$. While the loss \eqref{eq:energyloss} leverages generalized energy score on the \emph{conditional} distribution $p_{0|t}$, we can instead leverage the generalized energy score on the \emph{joint} distribution $p_{0,t}$ to define a new loss
\begin{equation}
\label{eq:energylossjoint}
\mathcal{L}_{\mathrm{joint}}(\theta)=-\int_{0}^{1} w_{t} \mathbb{E}_{p_{0,t}} \left[ S_{\lambda, \beta}(p_{0\vert t}^{\theta} p_t,(X_0, X_t)) \right]\rmd t,
\end{equation}
where $p^\theta_{0|t}p_t$ denotes $p^\theta_{0|t}(x_0|x_t) p_t(x_t)$.
In the case of the \emph{conditional} loss \eqref{eq:energyloss}, an empirical  interaction estimate is 
\begin{equation}
    \mathcal{I}_{n,m} = \sum_{i=1}^n \sum_{j\neq j'}^{m}  \frac{\lambda}{2n(m-1)} \Delta_{i,j}  
\end{equation}
where $\Delta_{i,j} = \| \hat{x}_{\theta}(t_{i},X_{t_{i}},\xi_{i}^{j})-\hat{x}_{\theta}(t_{i},X_{t_{i}},\xi_{i}^{j'})\| ^{\beta}$ where $X_{t_i}\sim p_{t_i}, \xi^j_i \sim \mathcal{N}(0,\Id)$ for all $i,j$. In the case of the \emph{joint} loss \eqref{eq:energylossjoint}, an empirical  interaction estimate is
\begin{align}
    \mathcal{I}_{n,m, \mathrm{joint}} &= \sum_{i=1}^n \sum_{j\neq j'}^{m}  \frac{\lambda}{2n(m-1)} \left[ \| X_{t_{i}}^j - X_{t_{i}}^{j'}\| + \Delta_{i,j}' \right] ,
\end{align}
where $\Delta_{i,j}' = \| \hat{x}_{\theta}(t_{i},X_{t_{i}}^j,\xi_{i}^{j})-\hat{x}_{\theta}(t_{i},X_{t_{i}}^{j'},\xi_{i}^{j'})\|^{\beta}$. Here we have
 $X^j_{t_i}\sim p_{t_i}, \xi^j_i \sim \mathcal{N}(0,\Id)$ for all $i,j$.
We then ask whether $\SNR(\mathcal{I}_{n,m}) \geq \SNR(\mathcal{I}_{n,m, \mathrm{joint}})$.  Unfortunately, we cannot answer this in the general case. However, if $p_0= \mathcal{N}(0, \sigma^2 \Id)$, and leveraging results from U-statistics, we have the following result.
\begin{proposition}{}{variance_snr}
Let $U \sim p_U$ and $V \sim p_V$ where $p_U \propto w_t / (1 + \tfrac{\alpha_t^2 \sigma^2}{\sigma_t^2})^{1/2}$ and $p_V \propto w_t (\sigma + (\alpha_t^2 \sigma^2 + \sigma_t^2)^{1/2})$ are two distributions on $[0,1]$. Then, we have that 
\begin{align}
    &\SNR(\mathcal{I}_{n}) := \lim_{m \to + \infty} \SNR(\mathcal{I}_{n,m}) = n\SNR(U) , \\
    &\SNR(\mathcal{I}_{n, \mathrm{joint}}) :=  \lim_{m \to +\infty} \SNR(\mathcal{I}_{n, m, \mathrm{joint}}) = n \SNR(V). 
\end{align}
\end{proposition}
In practice, we consider the sigmoid weighting scheme $w_t = (1 + \exp[b-\log(\alpha_t^2 / \sigma_t^2)])^{-1}$ \citep{kingma2021variational,hoogeboom2023simple}, where $b \in \rset$ is some bias. 
The $\SNR$ of $U,V$ can easily be computed and we observe that, across a large range of values of $\sigma$ and bias values $b$, we indeed have $\SNR(\mathcal{I}_{n,m}) \geq \SNR(\mathcal{I}_{n,m, \mathrm{joint}})$ in the large $m$ limit, see \Cref{fig:snr_study_fig}.

\begin{figure}
    \centering
        \centering
        \includegraphics[width=.55\linewidth]{figures/snr_study.png}
    \caption{$ \SNR(\mathcal{I}_{n})$ (red) and $\SNR(\mathcal{I}_{n, \mathrm{joint}})$ (blue) w.r.t. $\sigma^2$ ($x$-axis) for 3 possible bias levels $b\in \{0,1,2\}$ (from dark to light). Vertically, we plot  $(\sigma_b^\star)^2$ such that for the bias $b$, we have $\SNR(\mathcal{I}_{n}) \geq \SNR(\mathcal{I}_{n,\mathrm{joint}})$ for $\sigma \geq \sigma_b^\star$.}
    \label{fig:snr_study_fig}
\end{figure}

\subsection{From kernel scores to diffusion losses}
\label{sec:diffusion_compatible}
We have shown in eq. \eqref{eq:recover_diffusion} of  \Cref{sec:proper_scoring_rules} that the energy diffusion loss \eqref{eq:lossengression} recovers the diffusion loss \eqref{eq:diffusionloss} when $\beta \to 2$. In that case, we say that the scoring rule is \emph{diffusion compatible}. A natural question to ask is whether other scoring rules are diffusion compatible: i.e., given $\rho_c$ with $c \in \rset$ a hyperparameter of the kernel, that there exists $c^\star \in [-\infty, +\infty]$ and $f: \rset \to \rset$ such that 
\begin{equation}
    \lim_{c \to c^\star} f(c) D_{\rho_c}(p, q) = \| \mathbb{E}_p[X] - \mathbb{E}_q[X] \|^2 . 
\end{equation}
The following result shows that $k_{\imq}$ and $k_{\rbf}$ can also recover the diffusion loss.
\begin{proposition}{}{diffusion_compatible}
Assume that $\rho = -k$ with $k$ given by \eqref{eq:imq_k} or \eqref{eq:rbf_k}. Then the scoring rule is diffusion compatible. 
\end{proposition}
This proposition justifies using scoring rules other than the energy one, which also allow recovering diffusion models in the limit. We compare the performance of these different kernels in \Cref{sec:image_experiments}.  Our experiments suggest that diffusion compatibility, as demonstrated by the energy score, $k_{\imq}$, and $k_\rbf$, is sufficient for defining a loss function that leads to high-quality sample generation when used to train a model.  However, we have identified other kernels, such as $k_{\expk}$, that do not satisfy the diffusion compatibility requirement but still result in models with desirable properties.

\section{Related Work}
\label{sec:relatedWork}
\begin{figure}
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=.45\linewidth]{figures/mmd_2d.pdf}
        \label{fig:2d_mmd}
    \end{subfigure}
    \hfill
    \begin{subfigure}
        \centering
        \includegraphics[width=.45\linewidth]{figures/std_over_eps_avg_over_xt.pdf}
        \label{fig:2d_stds}
    \end{subfigure}
    \caption{Left: Squared $\MMD$ between target distribution and sampled data according to different models, $x$-axis denotes the number of sampling steps. Right: average standard deviation of samples $X_0 | X_t$ produced by either true posterior distribution $p_{0 \vert t}$ or the models $\hat{x}_{\theta}(t,X_{t},\xi)$, $x$-axis is the timestep $t$.}
    \label{fig:2d_metrics}
\end{figure}

\textbf{Accelerated diffusion models.} Many strategies accelerate diffusion models, broadly classifiable as distillation- or sampling-based.  Distillation methods \citep{luhman2021knowledge,salimans2022progressive,luo2023comprehensive,salimans2024multistep,dieleman2024distillation,liu2022flow,meng2023distillation,song2023consistency,franceschi2024unifying,huang2024flow,sauer2025adversarial} train a student model to mimic a teacher diffusion model, leveraging consistency losses \cite{song2023consistency}, adversarial losses \citep{franceschi2024unifying,xu2024ufogen, sauer2025adversarial}, or noise coupling \cite{huang2024flow}. See \citep{dieleman2024distillation} for a detailed discussion.  While distillation is prevalent, other work focuses on improved samplers for larger step sizes \citep{jolicoeur2021gotta,lu2022dpm,zheng2023dpm}.  Our work differs from both, neither training a student nor proposing new samplers.

\textbf{Discrepancy and diffusion.} We modify the diffusion model training loss to learn the conditional distribution $p_{0|t}(x_0|x_t)$.  The importance of approximating the covariance this distribution was noted in \citep{nichol2021improved} and exploited in \citep{ho2020denoising,nichol2021improved,bao2022estimating,bao2022analytic,ou2024diffusion}.  Closest to our approach is \citep{xiao2021tackling}, which uses a GAN to better learn $p_{0|t}(x_0|x_t)$ for fewer sampling steps.  While sharing the same motivation, our method avoids adversarial training and discriminator training, offering a simple loss modification that encompasses standard diffusion models.  Other loss modifications, like using the $\ell_1$ loss \citep{chen2020wavegrad,saharia2022palette}, aim to improve output quality, not reduce sampling steps.  \citep{galashov2024deep} can be seen as dual to our work: they learn discriminative kernel features with a flow at different noise levels, while we focus solely on learning a generator.

\textbf{Energy distances, MMD, and generative modeling.}
The MMD \citep{gretton2012kernel}, of which energy distances \citep{szekely2013energy,rizzo2016energy} are a special case \citep{sejdinovic2013equivalence}, has been widely used as a distributional loss in generative modeling. GANs have used MMDs with fixed kernels as critics to distinguish generated from reference samples \citep{li2015generative,dziugaite2015training,Unterthiner2018coulomb}. MMDs \citep{li2017mmdGan,binkowski2018demystifying} and energy distances \citep{energy-distance-gan,bellemare2017cramerdistancesolutionbiased,salimans2018improvinggansusingoptimal} have also been defined on adversarially trained discriminative neural net features.
The conditional generalized energy score of \eqref{eq:energyScoreConditional} has been used in learning conditional distributions by the DISCONet approach of \citet{bouchacourt2016disco},  and by the engression approach of \citet{shen2024engression} 
(which corresponds to the special case of $\lambda,\beta=1$). Recently, energy distances have been applied to speech synthesis \cite{gritsenko2020spectral}, normalizing flows \cite{si2023semi}, neural SDEs \cite{issa2024non}, and other generative models \citep{chen2024generative,Pacchiardi2024}.

To our knowledge, such losses have not previously been used to train diffusion models.
It is possible, however, to use an adaptation of  \citep{galashov2024deep}  in combination with the approach of \citet{bouchacourt2016disco,shen2024engression}, in order to incorporate conditional GAN-style generation into the reverse process of a diffusion model.  This idea was proposed by X. Shen  (personal communication, 6th November 2024) in an open  discussion following a presentation at Google Deepmind, as a future research direction of interest. The main idea of \citet{galashov2024deep} is retained, namely to use a standard forward diffusion  process, and a distributional loss for the reverse process: however the adaptive-kernel MMD loss for the reverse process is replaced with a fixed-kernel energy score; and this score is used to train a sequence of conditional GAN generators to approximate $p(x_{t_{k-1}} |x_{t_{k}})$, rather than using particle diffusion directly. The approach indeed represents a promising line of work \citep[since developed in][]{shen2025reversemarkovlearningmultistep} for discretizing the reverse diffusion process, as distinct from the DDIM-style approach adopted in  \cref{sec:method}. The approach can further be understood as a non-adversarial formulation of  \citet{cheng2024conditionalganenhancingdiffusion}, which uses an adaptive conditional GAN critic in place of the fixed-kernel energy score.

\begin{figure}
    \centering
        \includegraphics[width=1\linewidth]{figures/engression_2d.png}
    \caption{Samples from true posterior $p_{0 \vert t}(\cdot | x_t)$ (light blue) for a sample $x_t$ (black cross) from $p_ {t|0}(x_t|x_0)$ for a specific $x_0$ (orange cross) and samples $\hat{x}_{\theta}(t,x_{t},\xi) \sim p^\theta_{0|t}(\cdot|x_t)$ (green dots) for $\beta=0.1,\lambda=1$.  Top/bottom row: $x_0$ is the mean of the left/right Gaussian. Samples from $\hat{x}_{\theta}(t,x_{t},\xi)$ (red) with $\beta=2,\lambda=0$ concentrate around $\mathbb{E}[X_0 | x_t]$ as expected.}
    \label{fig:trajectories_2d}
\end{figure}

\section{Experiments}\label{sec:experimentsMain}

In this section, we validate the performance of our approach in 2D, image generation, and robotics settings.
The main experiments are presented here, while additional experiments and ablations are given in~\Cref{app_sec:additional_experiments}. All the experimental details are described in Appendix~\ref{app_sec:experimental_details}.

\subsection{2D experiments}
\label{sec:2d_experiments}
Consider a target given by a mixture of two Gaussians, i.e. $p_0=0.5\mathcal{N}(\mu_1, \sigma^2 \Id) + 0.5\mathcal{N}(\mu_2, \sigma^2 \Id)$, where $\mu_1=(3,3)$, $\mu_2=(-3,3)$ and $\sigma=0.5$.
We train an unconditional \emph{distributional} model $\hat{x}_{\theta}$ with $\beta=0.1$ and $\lambda=1$ using Algorithm~\ref{alg:training_diffusion}. Additionally, we train a baseline unconditional diffusion model by optimizing~\eqref{eq:diffusionloss} and the model $\hat{x}_{\theta}$ with $\beta=2$ and $\lambda=0$ using Algorithm~\ref{alg:training_diffusion}, i.e. a classical diffusion model using the same architecture as the \emph{distributional} models. More details are provided in Appendix~\ref{app_sec:2d_experimental_details}.


To measure the quality of the samples produced, we use the MMD squared given by $D_\rho$~\eqref{eq:divergence} with $\rho(x,x')=-k_{\rbf}(x,x')$ ~\eqref{eq:rbf_k}  for $\sigma=1$. In Figure~\ref{fig:2d_metrics} (left), we observe
that with few denoising steps, the \emph{distributional} variant has a smaller $D_\rho$ compared to other models, and achieves similar performance as the diffusion model using a large number of steps. In \Cref{fig:2d_metrics} (right), for each $X^i_t$, we produce $8$ samples $\xi \sim \mathcal{N}(0, \Id)$ and we compute the standard deviation (std) of $\hat{x}_{\theta}(t,x^i_{t},\xi)$ over $\xi$. We then average the std over all the $x^i_{t}$. While classical diffusion models cannot model the variance of the posterior, \emph{distributional} diffusion models have std close to that of the true posterior. 

To further our analysis, we visualize the samples from the \emph{distributional} model in Figure~\ref{fig:trajectories_2d}. For  given $x_t$ values, we sample from the true posterior $p_{0|t}(x_0|x_t)$ which is  available in closed form. We also sample $ \hat{x}_{\theta}(t,x_t,\xi)\sim p^\theta_{0|t}(\cdot|X^i_t)$ where $\xi \sim \mathcal{N}(0,\Id)$ for a model trained using $\beta=0.1,\lambda=1$, showing that our generating network is able to learn a good approximation of the posterior. We also display samples from $\hat{x}_{\theta}(t,x_t,\xi)$ trained with $\beta=2,\lambda=0$ which are concentrated around $\mathbb{E}[X_0|x_t]$, as expected.

\subsection{Image experiments}
\label{sec:image_experiments}

\begin{figure}
    \centering
        \includegraphics[width=.9\linewidth]{figures/image_results_conditional.pdf}
    \caption{
    Conditional image generation. $x$-axis denotes number of denoising steps, $y$-axis represents FID.
    % Whenever $\beta < 2$ is written, $\beta$ is selected such to achieve minimal FID.
    Black dashed line denotes the performance of diffusion model at $100$ steps.}
    \label{fig:image_results_conditional}
\end{figure}

\textbf{Main results.} We train conditional pixel-space models on CIFAR-10 (32x32x3) and on CelebA (64x64x3), as well as unconditional pixel-space models on LSUN Bedrooms (64x64x3). We  further use an autoencoder trained on CelebA-HQ (256x256x3) producing latent codes of shape 64x64x3, which are then used to build latent unconditional models. For each dataset, we train a diffusion model by optimizing~\eqref{eq:diffusionloss} as well as a \emph{distributional} model  (Algorithm~\ref{alg:training_diffusion}) for different $\lambda \in [0,1]$ and $\beta \in [0.001, 2]$. Models are evaluated using the FID~\citep{heusel2017gans} score. See~\Cref{app_sec:image_experimental_details} for details.

\begin{figure}
    \centering
        \includegraphics[width=.9\linewidth]{figures/image_results_unconditional.pdf}
    \caption{
    Unconditional image generation. $x$-axis denotes number of denoising steps, $y$-axis represents FID.
    % Whenever $\beta < 2$ is written, $\beta$ is selected such to achieve minimal FID.
    Black dashed line denotes the performance of the diffusion model at $100$ steps.}
    \label{fig:image_results_unconditional}
\end{figure}

In Figure~\ref{fig:image_results_conditional}, we show results of conditional image generation on CIFAR-10 and CelebA, and in Figure~\ref{fig:image_results_unconditional} results of unconditional image generation on LSUN Bedrooms and latent CelebA-HQ. Whenever we report performance of \emph{distributional} model and write $\beta < 2$, we select a parameter $\beta$ given one fixed $\lambda$ which minimizes the FID. More detailed results are given in~\Cref{app_sec:image_additional_results}, which include FIDs for different numbers of steps for every combination of $(\beta,\lambda)$.

\textbf{Main takeaways.} The results suggest that whenever the number of diffusion steps is low, then \emph{distributional} models with $\lambda=1,\beta<2$ achieve better performance than classical diffusions. This confirms that the diffusion model approximation is poor in the "few steps" regime, and can be improved with better modeling of $p_{0|t}$ as explained in~\Cref{sec:diffusion_models}. As the number of diffusion steps increases, the Dirac approximation becomes more accurate and diffusion models achieve the best performance. In that scenario, we observe that \emph{distributional} models with $\lambda=1,\beta<2$ yield worse performance. Our hypothesis is that since \emph{distributional} models $\hat{x}_{\theta}$ only approximate the posterior distribution $p_{0|t}$, this could lead to an increased amount of noise at sampling time and consequently to error accumulation.
However, we observe that \emph{distributional} model variants with $\lambda \in (0, 1)$ still yield good performance as the number of diffusion steps increases. This highlights the trade-off between \emph{distributional} and diffusion models controlled by the parameter $\lambda$. As argued in~\Cref{sec:gaussian},  $\lambda < 1$ leads to underestimation of the target variance which, for denoising diffusions, implies that the variance of $\hat{x}_{\theta}$ is more concentrated around $p_{0|t}$ than would occur for posterior samples. Remarkably, we also found that using $\beta < 2,\lambda=0$ led to comparable or even slightly better performance compared to classical diffusion models. This suggests that it could be valuable to train diffusion models using a different loss than Mean Square Error (MSE) regression.

\begin{figure}
    \centering
        \includegraphics[width=.95\linewidth]{figures/best_params.pdf}
    \caption{Parameters $\lambda$ and $\beta$ minimizing the FID for a given number of diffusion steps. $x$-axis indicates the number of diffusion steps and legend color indicates the dataset.}
    \label{fig:best_lambda_and_beta}
\end{figure}



\textbf{Conditional vs unconditional generation.} We observe slightly different behavior of \emph{distributional} models in the  conditional image generation case (Figure~\ref{fig:image_results_conditional}), and in the unconditional case (Figure~\ref{fig:image_results_unconditional}). For a small number of diffusion steps, we notice that the gap in performance between the model trained with $\lambda=1,\beta<2$ and a diffusion model, is much smaller in the unconditional case (1.5x) compared to the conditional one (3x). In Figure~\ref{fig:best_lambda_and_beta} we visualize the best parameters $\lambda,\beta$ selected to achieve the lowest FID, as a function of diffusion steps. More detailed results are presented in~\Cref{app_sec:image_additional_results}. We observe that overall $\lambda$ follows a downward trend, decreasing as the number of steps increases. However,  $\beta$ behaves differently in each setting, increasing monotonically as a function of diffusion steps in the conditional case, while remaining at a minimal value in the unconditional case. We hypothesise that this is because learning unconditional distributions is a much harder task since the data is spread out more, leading to a much higher variance in the distribution of $\hat{x}_{\theta}$.

\textbf{Using different kernels.}
As discussed in~\Cref{sec:proper_scoring_rules} and in~\Cref{sec:method}, we can also use the kernel diffusion loss. We train $\hat{x}_{\theta}$ with kernels~\eqref{eq:imq_k},~\eqref{eq:rbf_k},~\eqref{eq:exp_k}, varying kernel parameters and $\lambda \in [0,1]$; see Figure~\ref{fig:kernel_results} for results on conditional image generation for CIFAR-10. We observe similar behavior to the energy diffusion loss, achieving the best results when $\lambda=1$ for few diffusion steps and better performance with $\lambda < 1$ when number of steps increases; see \Cref{app_sec:image_additional_results} for detailed results.

\begin{figure}
    \centering
        \includegraphics[width=0.95\linewidth]{figures/kernel_results.pdf}
    \caption{Conditional generation on CIFAR-10 with different kernels. $x$-axis is the number of diffusion steps. $y$-axis is the FID.}
    \label{fig:kernel_results}
\end{figure}

\subsection{Robotics experiments}
\label{sec:robotics_experiments}

\textbf{Experimental setup.}
We demonstrate here the effectiveness of our \emph{distributional} diffusion models for robotics applications. We experiment with diffusion policies on the Libero \cite{LIBERO} benchmark, a life-long learning benchmark that consists of 130 language-conditioned robotic manipulation tasks. In our experiments we focus on Libero-Long, the most challenging suite with 10 tasks that features long-horizon tasks with diverse object interactions. Our multi-task diffusion policy is conditioned on the encoded visual and proprioceptive observations, and the task descriptions and generates a chunk of 8 7-dimensional actions to execute; see \Cref{app_sec:robotics_experimental_details} for additional details on the architecture.

\textbf{Distributional diffusion model.} We train the \emph{distributional} diffusion model with Algorithm~\ref{alg:training_diffusion} using the energy diffusion loss with population size $m=16$ and varying $\beta$ and $\lambda$. We use a diffusion model to model a sequence of $8$ actions $a = (a_1,\ldots,a_8)$, where $a_i \in \rset^{7}$. We use the noise $\xi \in \rset^{8,2}$ which we concatenate with $a$ on the last dimension. At sampling time, we follow Algorithm~\ref{alg:sampling_diffusion} to sample a sequence of actions from a diffusion policy and we execute it. We sweep over a number $\{2, 16, 50\}$ of diffusion steps.

\textbf{Results.} Similar to the results on image experiments, $\lambda=0$ works best when more diffusion steps $(50)$ are used during evaluation. Using $\lambda > 0$ gives best results when fewer diffusion steps are used $(2, 16)$. We note that the performance slightly deteriorates with $\lambda > 0$ when the number of diffusion steps is large, see \Cref{fig:libero10_results}.

\begin{figure}
    \centering
        \includegraphics[width=.5\linewidth]{figures/libero10_results_beta1.pdf}
    \caption{Performance in Libero10 as function of diffusion steps and $\lambda$. We plot the maximum success rate from best checkpoint during training. $\lambda=0$ works best when more diffusion steps are used, and  $\lambda > 0$ gives best results when fewest diffusion steps are used. $\lambda > 0$ also reduces the performance by a little for 50 diffusion steps (see 0.5 vs 0.0).}
    \label{fig:libero10_results}
\end{figure}

\vspace{-.1cm}
\section{Discussion}
We present a novel approach for training diffusion models, by learning the full conditional distributions $p_{0 \vert t}$ using scoring rules.  This approach achieves good performance in image generation and robotics tasks, outperforming standard diffusion models in the "few-step" regime with minimal degradation of output quality.   Our experimental results further suggest that standard diffusion models can also be trained with objectives other than classical MSE regression.

So far, our generative networks approximating $p_{0 \vert t}$ incorporate the noise $\xi$ only by concatenating it to the $x_t$ along the channel dimension.
We believe that other architectures could significantly improve sample quality, and will explore this in future work.
Another promising direction is in learning the kernel $\rho$  and  the parameter $\lambda$ to maximize generative performance in the few-step regime. Combining our approach with powerful distillation techniques such as \citep{salimans2024multistep} is a further avenue for future work. 

Finally kernel scores allow for more general domains than $\R^d$, by way of characteristic kernels on groups  (e.g. the group $\mathrm{SO}(3)$ of 3d  rotations) and semigroups ($d$-dimensional histograms) \citep{NIPS2008_d07e70ef}; and graphs \citep{JMLR:v11:vishwanathan10a}. It is thus possible to extend our methodology to these scenarios.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

\bibliography{references}
\bibliographystyle{icml2024}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section*{Organization of the Appendix}

The appendix is organized as follows.
In \Cref{sec:notation}, we present a notation table, to help the reader. 
In \Cref{sec:correspondence_between_discrepancy_and_diffusion}, we prove that diffusion compatible scoring rules can be linked to diffusion model losses as claimed in \Cref{sec:proper_scoring_rules}.
In \Cref{sec:gaussian_setting}, we prove the results regarding the minimization of generalized expected energy score in a Gaussian setting presented in \Cref{sec:gaussian}. 
In \Cref{app_sec:different_learning_objectives}, we expand on the discussion of \Cref{subsec:jointvsconditionaltraining} regarding joint or conditional scoring rules. 
In \Cref{app:connection_to_mmd}, we present the results of \Cref{sec:diffusion_compatible} regarding diffusion compatible kernels. 
In \Cref{sec:ddim_sde}, we present a new perspective on the DDIM updates derived in \citep{song2020denoisingimplicit}. More precisely, we show that these updates can be recovered using a Stochastic Differential Equation perspective. This justifies the form of the DDIM mean and covariance updates presented in \eqref{eq:mean_sigma}. 
We present some pseudocode to compute our loss function in \Cref{sec:pseudocode_training}.
Experimental details are provided in \Cref{app_sec:experimental_details}.
We present an extended related work in \Cref{sec:extended_related}.
In \Cref{app_sec:additional_experiments}, we present additional experimental results including ablation studies. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Notation}
\label{sec:notation}

We recall that a kernel defined on a space $\msx$ is a symmetric function $k: \ \msx \times \msx \to \rset$, i.e., for any $x,y \in \msx$, $k(x,y) = k(y,x)$.
A kernel $k$ is said to be \emph{positive semi-definite} if for any $n \in \nset$, $\{x_1, \dots, x_n \} \in \msx^n$ and $\{c_1, \dots, c_n\} \in \rset^n$ we have
\begin{equation}
    \sum_{i=1}^n \sum_{j=1}^n c_i c_j k(x_i,x_j) \geq 0 . \label{eq:positive_semi_definite}
\end{equation}
The kernel $k$ is  \emph{positive definite} if equality in \eqref{eq:positive_semi_definite} occurs if and only $c_i = 0$ for any $i \in \{1, \dots, n\}$. We refer the reader to \citep{BerTho04,SteChr08,gretton2013introduction} for an overview on kernel theory. 

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
 Name & Notation & Definition & First appearance \\
\hline \hline
\rule{0pt}{15pt} $\cdot$ & $\rho$ & Continuous negative definite kernel & \Cref{sec:proper_scoring_rules} \\ % Added padding
\hline
\rule{0pt}{15pt} $\cdot$ & $k$ & Continuous positive definite kernel & \Cref{sec:proper_scoring_rules} \\ % Added padding
\hline
\rule{0pt}{15pt} Scoring rule & $S(p, y)$ & $\cdot$ & \Cref{sec:proper_scoring_rules} \\ % Added padding
\hline
\rule{0pt}{15pt} Expected score & $S(p,q)$ & $S(p,q) = \mathbb{E}_q[S(p,Y)]$ & \Cref{sec:proper_scoring_rules} \\ % Added padding
\hline
\rule{0pt}{15pt} Kernel score & $S_\rho(p,y)$ & $S_\rho(p,y) = \frac{1}{2}\mathbb{E}[\rho(X,X')] - \mathbb{E}[\rho(X,y)]$ & \Cref{sec:proper_scoring_rules} \\ % Added padding
\hline
\rule{0pt}{15pt} Energy score & $S_\beta(p,y)$ & $S_\beta(p,y) = S_\rho(p,y)$ with $\rho(x,x') = \| x - x'\|^\beta$ & \Cref{sec:proper_scoring_rules} \\ % Added padding
\hline
\rule{0pt}{15pt} Expected Kernel score & $S_\rho(p,q)$ & $S_\rho(p,q) = \mathbb{E}_q[S_\rho(p,Y)]$ & \Cref{sec:proper_scoring_rules} \\ % Added padding
\hline
\rule{0pt}{15pt} Expected Energy score & $S_\beta(p,q)$ & $S_\beta(p,q) = S_\rho(p,q)$ with $\rho(x,x') = \| x - x'\|^\beta$ & \Cref{sec:proper_scoring_rules}\\ % Added padding
\hline
\rule{0pt}{15pt} Generalized Kernel score & $S_{\lambda, \rho}(p,y)$ & $S_{\lambda, \rho}(p,y) = \frac{\lambda}{2} \mathbb{E}_{p\otimes p}[\rho(X,X')] - \mathbb{E}[\rho(X,y)]$ & \Cref{sec:proper_scoring_rules} \\ % Added padding
\hline
\rule{0pt}{15pt} Generalized Energy score & $S_{\lambda, \beta}(p,y)$ & $S_{\lambda, \beta}(p,y) = S_{\lambda, \rho}(p,y)$ with $\rho(x,x') = \| x - x' \|^\beta$ & \Cref{sec:proper_scoring_rules} \\ % Added padding
\hline
\rule{0pt}{15pt} Expected Generalized Kernel score & $S_{\lambda, \rho}(p,q)$ & $S_{\lambda, \rho}(p,q) = \mathbb{E}_q[S_{\lambda, \rho}(p,Y)]$ & \Cref{sec:proper_scoring_rules} \\ % Added padding
\hline
\rule{0pt}{15pt} Expected Generalized Energy score & $S_{\lambda, \beta}(p,q)$ & $S_{\lambda, \beta}(p,q) = S_{\lambda, \rho}(p,q)$ with $\rho(x,x') = \| x - x'\|^\beta$ & \Cref{sec:proper_scoring_rules} \\ % Added padding
\hline
\rule{0pt}{15pt} Squared Maximum Mean Discrepancy & $D_\rho(p,q)$ & $D_\rho(p,q) =  S_\rho(q,q) - S_\rho(p,q)$ & \Cref{sec:proper_scoring_rules} \\ % Added padding
\hline 
\rule{0pt}{15pt} Energy Distance & $D_\beta(p,q)$ & $D_\beta(p,q) = D_\rho(p,q)$ with $\rho(x,x') = \|x-x'\|^\beta$ & \Cref{sec:proper_scoring_rules} \\ % Added padding
\hline
\rule{0pt}{15pt} Diffusion Loss & $\mathcal{L}_{\text{diff}}(\theta)$ & $\mathcal{L}_{\text{diff}}(\theta)=\int^1_0 w_t \mathbb{E}_{p_{0,t}}\left[||X_0-\hat{x}_\theta(t,X_t)||^2\right]\rmd t$ & \Cref{sec:diffusion_models} \\ % Added padding
\hline 
\rule{0pt}{15pt} Energy Diffusion Loss & $\mathcal{L}(\theta)$ & $\mathcal{L}(\theta)=-\int_{0}^{1} w_{t} \mathbb{E}_{p_{0,t}} \left[ S_{\lambda, \beta}(p_{0\vert t}^{\theta}(\cdot|X_t),X_0) \right]\rmd t$ & \Cref{sec:method}   \\ % Added padding
\hline
\rule{0pt}{15pt} Kernel Diffusion Loss & $\mathcal{L}(\theta)$ & $\mathcal{L}(\theta)=-\int_{0}^{1} w_{t} \mathbb{E}_{p_{0,t}} \left[ S_{\lambda, \rho}(p_{0\vert t}^{\theta}(\cdot|X_t),X_0) \right]\rmd t$ & \Cref{sec:method}   \\ % Added padding
\hline
Joint Energy Diffusion Loss & $\mathcal{L}_{\mathrm{joint}}(\theta)$ & $\mathcal{L}_{\mathrm{joint}}(\theta) =-\int_{0}^{1} w_{t} \mathbb{E}_{p_{0,t}} \left[ S_{\lambda, \beta}(p_{0\vert t}^{\theta} p_t,(X_0, X_t)) \right]\rmd t$ & \Cref{subsec:jointvsconditionaltraining} \\
\hline
\end{tabular}
\end{table}

\section{Correspondence between Discrepancy and Diffusion Loss}
\label{sec:correspondence_between_discrepancy_and_diffusion}

In this section, we demonstrate the connection between diffusion-compatible scoring rules and diffusion model losses, as stated in Section \ref{sec:proper_scoring_rules}. More precisely, we prove the following result. 

\begin{proposition}{}{correspondence_with_diffusion}
Assume that $D_\rho(p,q) = \|\mathbb{E}_p[X] - \mathbb{E}_q[X]\|^2$ and that $p = \updelta_{\hat{x}_\theta(t, X_t)}$ and $q = p_{0|t}(\cdot|X_t)$. 
Then, there exists a constant $C \in \rset$, independent of $\theta$, such that 
\begin{equation}
    \mathbb{E}_{p_t}[D_\rho(p,q)] = \mathbb{E}_{p_{0,t}}[\| X_0 - \hat{x}_\theta(t, X_t) \|^2] + C,
\end{equation}
with $C = - \mathbb{E}_{p_{0,t}}[\| X_0 - \mathbb{E}[X_0|X_t] \|^2]$. 
\end{proposition}

\begin{proof}
First we have 
\begin{align}
    &\mathbb{E}_{p_t}[D_\rho(p,q)] = \mathbb{E}_{p_t}[\|\hat{x}_\theta(t, X_t) - \mathbb{E}[X_0|X_t] \|^2] \\
    &\qquad = \mathbb{E}_{p_{0,t}}[\|\hat{x}_\theta(t, X_t) - X_0 + X_0 - \mathbb{E}[X_0|X_t] \|^2] \\
    &\qquad = \mathbb{E}_{p_{0,t}}[\|\hat{x}_\theta(t, X_t) - X_0 \|^2] + \mathbb{E}_{p_{0,t}}[\| X_0 - \mathbb{E}[X_0|X_t] \|^2] + 2 \mathbb{E}_{p_{0,t}}[\langle \hat{x}_\theta(t, X_t) - X_0, X_0 - \mathbb{E}[X_0|X_t] \rangle] \\ 
    &\qquad = \mathbb{E}_{p_{0,t}}[\|\hat{x}_\theta(t, X_t) - X_0 \|^2] + \mathbb{E}_{p_{0,t}}[\| X_0 - \mathbb{E}[X_0|X_t] \|^2] \\
    &\qquad \quad + 2 \mathbb{E}_{p_{0,t}}[\langle \hat{x}_\theta(t, X_t) , X_0 - \mathbb{E}[X_0|X_t] \rangle] - 2 \mathbb{E}_{p_{0,t}}[\langle X_0, X_0 - \mathbb{E}[X_0|X_t] \rangle] \\
    &\qquad = \mathbb{E}_{p_{0,t}}[\|\hat{x}_\theta(t, X_t) - X_0 \|^2] + \mathbb{E}_{p_{0,t}}[\| X_0 - \mathbb{E}[X_0|X_t] \|^2] \\
    &\qquad \quad - 2 \mathbb{E}_{p_{0,t}}[\langle X_0, X_0 - \mathbb{E}[X_0|X_t] \rangle] . \label{eq:intermediate_uno}
\end{align}
In addition, we have 
\begin{equation}
    \mathbb{E}_{p_{0,t}}[\| X_0 - \mathbb{E}[X_0|X_t] \|^2] = \mathbb{E}_{p_0}[ \| X_0 \|^2] - \mathbb{E}_{p_0}[\| \mathbb{E}[X_0|X_t] \|^2] . \label{eq:intermediate_duo}
\end{equation}
Similarly, we have
\begin{equation}
    \mathbb{E}_{p_{0,t}}[\langle X_0,  X_0 - \mathbb{E}[X_0|X_t] \rangle] = \mathbb{E}_{p_0}[ \| X_0 \|^2] - \mathbb{E}_{p_0}[\| \mathbb{E}[X_0|X_t] \|^2] . \label{eq:intermediate_tertio}
\end{equation}
Combining \eqref{eq:intermediate_uno}, \eqref{eq:intermediate_duo} and \eqref{eq:intermediate_tertio}, we get 
\begin{equation}
    \mathbb{E}_{p_t}[D_\rho(p,q)] = \mathbb{E}_{p_{0,t}}[\|X_0-\hat{x}_\theta(t, X_t)\|^2] - \mathbb{E}_{p_{0,t}}[\| X_0 - \mathbb{E}[X_0|X_t] \|^2] ,
\end{equation}
which concludes the proof. 
\end{proof}


\section{Generalized Energy Score for Gaussian Targets}
\label{sec:gaussian_setting}
We first investigate the properties of the generalized energy score for a single Gaussian target in \Cref{sec:learninggaussiandistributiongeneralizedenergyscore} (\Cref{prop:reduction_variance}) and then apply these results to diffusion models in  \Cref{sec:Gaussianapplitodiffusionmodels} 
\subsection{Learning a Gaussian distribution}\label{sec:learninggaussiandistributiongeneralizedenergyscore}

For any $\beta \in (0, 2]$ and $\lambda \in [0,1]$ we recall that $S_{\lambda, \beta}$ is the generalized expected energy score given for any distributions $p, q$ on $\rset^d$ with $\beta$ moments by
\begin{equation}
\label{eq:generalized_energy_discrepancy}
    S_{\beta, \lambda}(p, q) = - \mathbb{E}_{(X,Y) \sim p \otimes q}[\|X - Y\|^\beta] + \frac{\lambda}{2} \mathbb{E}_{(X,X') \sim p \otimes p}[\|X - X'\|^\beta] .
\end{equation}
We refer the reader to \Cref{sec:notation} for a remainder on the notation used throughout the paper. 
For any $\mu \in \rset^d$ and $\sigma > 0$, let $p_{\mu, \sigma} = \mathcal{N}(\mu, \sigma^2 \Id)$ and denote by $\mathfrak{G}$ the space of Gaussian distributions with scalar covariance, i.e.,
\begin{equation}
    \mathfrak{G} = \{ \mathcal{N}(\nu, \gamma^2 \Id) \ , \ \nu \in \rset^d, \ \gamma \geq 0 \} . 
\end{equation}

We consider the following minimization problem: for any $\mu \in \rset^d$ and $\sigma >0$ solve
\begin{equation}
    p_{\mu_\star, \sigma_\star} = \argmax_{q \in \mathfrak{G}} S_{\beta, \lambda}(q, p_{\mu, \sigma}) . 
\end{equation}
When $\lambda = 1$ and $\beta \in (0,2)$, $S_{\lambda, \beta}(p, q)$ is a proper scoring rule \citep{rizzo2016energy,GneRaf07} so we have $p_{\mu_\star, \sigma_\star} = p_{\mu, \sigma}$. For $\lambda \in [0,1]$, we have the following result. This is a simple restatement of \Cref{prop:reduction_variance}.

\begin{proposition}{}{}
For any $\beta \in (0,2)$ and $\lambda \in (0,1)$, we have that $p_{\mu_\star, \sigma_\star} \in \argmax_{q \in \mathfrak{G}} S_{\lambda, \beta}(q, p_{\mu, \sigma})$ with 
\begin{equation}
    \mu_\star = \mu , \qquad \sigma_\star = (2 \lambda^{-2 / (2 - \beta)} - 1)^{-1} \sigma^2 . 
\end{equation}
\end{proposition}

We start with the following lemma.

\begin{lemma}{}{lemmamin}
Assume that $X \sim \mathcal{N}(0, \sigma^2 \Id)$ for $\sigma >0$. Then for any $\beta > 0$, we have $0 \in \argmin_{c \in \rset^d} \mathbb{E}[\| X - c \|^\beta]$. 
\end{lemma}

\begin{proof}
For any $\beta > 0$, let $f_\beta(c) = \mathbb{E}[\| X - c \|^\beta]$.
For any $\beta > 0$, we have that $f_\beta$ is continuous, using the dominated convergence theorem, and coercive, using Fatou's lemma, hence admits at least one minimizer. 
If $\beta \geq 1$, then $f_\beta$ is convex. Assume that $c^\star$ is a minimizer. Then $-c^\star$ is also a minimizer and by convexity, we have that $0$ is also a minimizer. 
If $\beta \in (0,1)$ then $f_\beta$ is strictly concave. Assume that $c^\star$ is a minimizer. Then $-c^\star$ is also a minimizer. If $0$ is not a minimizer of $f_\beta$, then $f_\beta(0) > f_\beta(c^\star)$. Hence, by strict concavity, we have that $\lim_{t \to +\infty} f_\beta(t c^\star) = -\infty$ which is absurd. Hence, $0$ is also a minimizer. 
\end{proof}

\begin{proof}
First, we have that for any $q \in \mathfrak{G}$ with mean $\tilde{\mu}$ and covariance $\tilde{\sigma}^2 \Id$
\begin{align}
    S_{\lambda, \beta}(q, p_{\mu, \sigma}) = - \mathbb{E}[\| \mu - \tilde{\mu} + (\sigma^2 + \tilde{\sigma}^2)^{1/2} Z \|^\beta ] + \frac{\lambda}{2} \mathbb{E}[\| \sqrt{2} \tilde{\sigma} Z \|^\beta] ,
\end{align}
where  $Z \sim \mathcal{N}(0, \Id)$. 
Maximizing with respect to $\tilde{\mu}$ first, we get that $\mu^\star = \mu$ thanks to  \Cref{lemma:lemmamin}. In addition, if $\tilde{\mu} = \mu$, we have 
\begin{align}
    S_{\lambda, \beta}(q, p_{\mu, \sigma}) &= - \mathbb{E}[\| \mu - \tilde{\mu} + (\sigma^2 + \tilde{\sigma}^2)^{1/2} Z \|^\beta ] + \frac{\lambda}{2} \mathbb{E}[\| \sqrt{2} \tilde{\sigma} Z \|^\beta] \\
    &= -(\sigma^2 + \tilde{\sigma}^2)^{\beta/2} \mathbb{E}[ \| Z \|^\beta ] + \frac{\lambda}{2} 2^{\beta/2} \tilde{\sigma}^{\beta} \mathbb{E}[\|  Z \|^\beta] \\
    &= -\mathbb{E}[\|  Z \|^\beta] ((\sigma^2 + \tilde{\sigma}^2)^{\beta/2} + \frac{\lambda}{2} 2^{\beta/2}  \tilde{\sigma}^{\beta}) .
\end{align}
Maximizing with respect to $\tilde{\sigma}$, we get that 
\begin{equation}
    \sigma_\star^2 = (2 \lambda^{-2 / (2 - \beta)} - 1)^{-1} \sigma^2 . 
\end{equation}
\end{proof}

So the effect of the introduction of $\lambda < 1$ is that $\sigma_\star$ underestimates $\sigma$, i.e. the model is too concentrated around its mean. The effect is stronger as $\beta \to 2$.  In what follows, we denote 
\begin{equation}
\label{eq:definition_reduction}
    f(\lambda, \beta) = (2 \lambda^{-2 / (2 - \beta)} - 1)^{-1} . 
\end{equation}
\subsection{Application to diffusion sampling}\label{sec:Gaussianapplitodiffusionmodels}
Consider a Gaussian target distribution $p_0$. We investigate the effect of using the generalized energy score to learn the Gaussian conditionals $p_{0|t}$ on the sampling updates  used at inference time, i.e. when substituting our approximation of $p_{0|t}$ within  \eqref{eq:backward_kernel} to obtain an approximation of $p_{s|t}$.

We first recall a few useful lemmas. Using \eqref{eq:interpolation}, the forward model is given by 
\begin{equation}
\label{eq:interpolation_appendix}
    \bfX_t = \alpha_t \bfX_0 + \sigma_t \bfZ , 
\end{equation}
with $\bfZ \sim \mathcal{N}(0, \Id)$ and $\bfX_0 \sim p_{\mu, \sigma}$, the target density. The diffusion $(\bfX_t)_{t \in [0,1]}$ given by 
\begin{equation}
\label{eq:forward_process_appendix}
    \rmd \bfX_t = f_t \bfX_t \rmd t + g_t \rmd \bfB_t ,
\end{equation}
where $(\bfB_t)_{t \in [0,1]}$ is a $d$-dimensional Brownian motion has the same marginal distributions as \eqref{eq:interpolation_appendix} for
\begin{equation}
    f_t = \partial_t \log(\alpha_t) , \qquad g_t^2 = 2 \alpha_t \sigma_t \partial_t (\sigma_t / \alpha_t ) . 
\end{equation}
We refer to \Cref{sec:ddim_sde} for a derivation of this fact, see also \citep{song2020score}.


\begin{lemma}{}{joint}
For any $t \in [0,1]$ we have that 
\begin{equation}
    (\bfX_0, \bfX_t) \sim \mathcal{N}\left( \left( \begin{matrix} \mu \\
    \alpha_t \mu \end{matrix} \right), \left( \begin{matrix} \sigma^2 \Id & \alpha_t \sigma^2 \Id \\
    \alpha_t \sigma^2 \Id & (\alpha_t^2 \sigma^2 + \sigma_t^2) \Id \end{matrix} \right) \right) . 
\end{equation}
\end{lemma}

\begin{proof}
This is a direct consequence of \eqref{eq:interpolation_appendix}. 
\end{proof}

In particular, we also have the following lemma.

\begin{lemma}{}{x0_from_xt}
For any $t \in [0,1]$ we have that 
\begin{equation}
    \bfX_0 | \bfX_t \sim \mathcal{N}(r(t) \bfX_t + (1-r(t)) \mu, \sigma^2(1 - r_{2,2}(t)) \Id ) ,
\end{equation}
where 
\begin{equation}
    r(t) = \frac{\alpha_t \sigma^2}{\alpha_t^2 \sigma^2 + \sigma_t^2} , \qquad r_{2,2}(t) = \frac{\alpha_t^2 \sigma^2}{\alpha_t^2 \sigma^2 + \sigma_t^2} . 
\end{equation}
\end{lemma}

\begin{proof}
This is a combination of \Cref{lemma:joint} and the formula for computing Gaussian posteriors. 
\end{proof}

\begin{lemma}{}{}
For $s, t \in [0,1]$ with $t \geq s$ we have that 
\begin{equation}
    \bfX_t | \bfX_s \sim \mathcal{N}\left(\frac{\alpha_t}{\alpha_s} \bfX_s, \left(\sigma_t^2 - \left(\frac{\sigma_s \alpha_t}{\alpha_s}\right)^2\right) \Id \right) .  
\end{equation}
\end{lemma}

\begin{proof}
This is a direct consequence of \eqref{eq:forward_process_appendix}.
\end{proof}

\begin{lemma}{}{xs_from_xt_and_x0}
For $s, t \in [0,1]$ with $t \geq s$ we have that

\begin{equation}
    \bfX_s | \bfX_0, \bfX_t \sim \mathcal{N}\left( r(s,t) \bfX_t + \alpha_s (1 - r_{2,2}(s,t)) \bfX_0, \sigma_s^2 (1 -r_{2,2}(s,t)) \Id \right) ,
\end{equation}
where 
\begin{equation}
    r(s,t) = \frac{\alpha_t \sigma_s^2}{\alpha_s \sigma_t^2} , \qquad r_{2,2}(s,t) = \frac{\alpha_t^2 \sigma_s^2}{\alpha_s^2 \sigma_t^2} .
\end{equation}
\end{lemma}

\begin{proof}
Using that $\bfX_t | \bfX_s$ is the same as $\bfX_t | \bfX_s, \bfX_0$ by the Markov property, we get the distribution of $(\bfX_s, \bfX_t) | \bfX_0$. We conclude upon using the formula for computing Gaussian posteriors. 
\end{proof}

Note that in \Cref{sec:ddim_sde}, we will establish \Cref{lemma:xs_from_xt_and_x0} in a more general setting, i.e., we will introduce a churn parameter $\vareps$ and draw further connection with DDIM \citep{song2020denoisingimplicit}. At this stage, \Cref{lemma:xs_from_xt_and_x0} recovers \citep{ho2020denoising} which corresponds to setting the churn parameter to $\vareps = 1$. 
Now, combining \eqref{eq:definition_reduction}, \Cref{lemma:x0_from_xt} and \Cref{lemma:xs_from_xt_and_x0} we get the following proposition.

\begin{proposition}{}{general_update_appendix}
For $s, t \in [0,1]$ with $t \geq s$ we have that
\begin{equation}
    \bfX_s | \bfX_t = \mathcal{N}(\hat{\mu}_{s|t}, \hat{\sigma}_{s|t}^2 \Id) , 
\end{equation}
with 
\begin{align}
    &\hat{\mu}_{s|t} = [r(s,t)  + \alpha_s (1 - r_{2,2}(s,t)) r(t) ] \bfX_t + \alpha_s (1 - r_{2,2}(s,t))  (1 -r(t)) \mu ,  \\
    &\hat{\sigma}_{s|t}^2 = \sigma_s^2 (1 -r_{2,2}(s,t)) + f(\lambda, \beta) \sigma^2 \alpha_s^2 (1-r_{2,2}(s,t))^2 (1-r_{2,2}(t)) .
\end{align}
In particular, denoting $\bfX_t \sim \mathcal{N}(\hat{\mu}_t, \hat{\sigma}_t^2 \Id)$ we get that 
\begin{align}
\label{eq:update_equation_appendix}
    &\hat{\mu}_s = [r(s,t)  + \alpha_s (1 - r_{2,2}(s,t)) r(t)] \hat{\mu}_t + \alpha_s (1 - r_{2,2}(s,t)) (1 -\alpha_t r(t)) \mu, \\
    &\hat{\sigma}_s^2 = \sigma_s^2 (1 -r_{2,2}(s,t)) + f(\lambda, \beta) \sigma^2 \alpha_s^2 (1-r_{2,2}(s,t))^2 (1-r_{2,2}(t)) + \hat{\sigma}_t^2 [r(s,t)  + \alpha_s (1 - r_{2,2}(s,t)) r(t)]^2 .
\end{align}
\end{proposition}

In particular, the update on the mean in \eqref{eq:update_equation_appendix} is not dependent on $\lambda$ and $\beta$. Therefore we estimate correctly the mean of $p_{s|t}$ but incorrectly the variance for $\lambda <1$.

The curves presented in \Cref{fig:stepsize_matters} are obtained using \Cref{prop:general_update_appendix}. 

\section{Different learning objectives}
\label{app_sec:different_learning_objectives}

First in 
\Cref{sec:conditional_versus_dirac} we compare the energy diffusion loss \eqref{eq:energyloss} with a similar loss based on Maximum Mean Discrepancy and show that they only differ by a term which does not depend on $\theta$. 
In \Cref{sec:conditional_versus_joint}, we compare the \emph{conditional} loss and the \emph{joint} loss, as discussed in \Cref{subsec:jointvsconditionaltraining}.
In \Cref{sec:joint_and_marginal}, we consider a third possible option, i.e., we compare the \emph{conditional} loss with a \emph{marginal} loss. We highlight the limitations of the \emph{marginal} loss in that case. 
We derive the empirical versions of the conditional and joint losses in \Cref{sec:empirical_version}.
Finally in \Cref{sec:sample_efficiency}, we prove the main result of \Cref{subsec:jointvsconditionaltraining}, i.e., we prove \Cref{prop:variance_snr} and compare the $\SNR$ of the interaction terms of the joint and conditional losses. 

\subsection{Energy diffusion loss and Maximum Mean Discrepancy diffusion loss}
\label{sec:conditional_versus_dirac}

In this section, we first compare the energy diffusion loss \eqref{eq:energyloss} and another loss based on Maximum Mean Discrepancy (MMD), denoted MMD diffusion loss.
We show that these two losses are equal up to a constant which does not depend on the network parameters $\theta$. However, this additional term appearing in the MMD diffusion loss is untractable, even though it could be potentially estimated using important sampling techniques. We consider the case where the kernel $k$ is given by $k(x,x') = -\|x-x'\|$. While our discussion can be extended to other settings, we also focus on the case where $\lambda = 1, \beta = 1$ and $w_t=1$ for simplicity.  

First, we consider the following MMD diffusion loss 
\begin{equation}
        \mathcal{L}_{\MMD}(\theta) = \int_0^1 \int_{\rset^d} p_{t}(x_t) D_\rho(p_{0|t}^\theta, p_{0|t})  \rmd x_t \rmd t . \label{eq:mmdloss}
\end{equation}
We recall that the squared $\MMD$ is given by $D_\rho(p,q) = S_\rho(q,q) - S_\rho(p,q)$ with $\rho = -k$ 
and
\begin{equation}
    \mathcal{L}(\theta) = - \int_0^1 \int_{\rset^d \times \rset^d} p_{0,t}(x_0,x_t) S_\rho(p_{0|t}^\theta, x_0) \rmd x_0 \rmd x_t \rmd t . \label{eq:energyloss_appendix}
\end{equation}
First, we highlight the following result which draws a connection between kernel scores and squared MMD.

\begin{proposition}{}{connection_mmd}
Let $x\in \rset^d$ be such that $k(x,x) = 0$, then we have $D_\rho(p, \updelta_x) = - S_\rho(p, x)$. 
\end{proposition}



\begin{proof}
For any distribution $q$, we have $S_\rho(q,q) = \frac{1}{2} \mathbb{E}_{q \otimes q}[k(X,X')]$ and therefore $S_\rho(\updelta_x, \updelta_x) =\frac{1}{2}k(x,x)= 0$.
Similarly, we have that 
\begin{equation}
    S_\rho(p, \updelta_x) = -\frac{1}{2}\mathbb{E}_{p \otimes p}[k(X,X')] + \mathbb{E}_{p \otimes \updelta_x}[k(X,Y)] = -\frac{1}{2}\mathbb{E}_{p \otimes p}[k(X,X')] + \mathbb{E}_{p}[k(X,x)] = S_\rho(p, x) . 
\end{equation}
Therefore, we have that 
\begin{align}
    D_\rho(p, \updelta_x) &= S_\rho(\updelta_x, \updelta_x) - S_\rho(p, \updelta_x) = - S_\rho(p, x) .
\end{align}
\end{proof}

Using \Cref{prop:connection_mmd}, \eqref{eq:energyloss_appendix} can be rewritten as 
\begin{equation}
    \mathcal{L}(\theta) = \int_0^1 \int_{\rset^d \times \rset^d} p_{0,t}(x_0,x_t) D_\rho(p_{0|t}^\theta, \updelta_{x_0}) \rmd x_0 \rmd x_t \rmd t . \label{eq:energyloss_appendix_mmd}
\end{equation}
Therefore, our energy diffusion loss can be expressed in terms of squared $\MMD$. We observe that both \eqref{eq:mmdloss} and \eqref{eq:energyloss_appendix_mmd} involve an integrated version of the squared $\MMD$. However, in \eqref{eq:energyloss_appendix_mmd} we target $\updelta_{x_0}$ while in \eqref{eq:mmdloss} we target $p_{0|t}$. In the rest of this section, we show that these two losses actually only differ by a term which does not depend on the network parameters $\theta$. 


Let us start with the \emph{energy diffusion loss} $\mathcal{L}$ given in \eqref{eq:energyloss_appendix} . We have 
\begin{align}
    \mathcal{L}(\theta) &= -\frac{1}{2} \int_0^1  \int_{(\rset^d)^4} p_{0,t}(x_0,x_t) p_{0|t}^\theta(x_0^1|x_t) p_{0|t}(x_0^2|x_t) \| x_0^1 - x_0^2 \| \rmd x_0 \rmd x_0^1 \rmd x_0^2 \rmd x_t \rmd t \\
    & \qquad + \int_0^1  \int_{(\rset^d)^3} p_{0,t}(x_0,x_t)p_{0|t}^\theta(x_0^1|x_t) \| x_0 - x_0^1 \| \rmd x_0 \rmd x_0^1  \rmd x_t \rmd t\\
    &= - \frac{1}{2} \int_0^1  \int_{(\rset^d)^3} p_{t}(x_t) p_{0|t}^\theta(x_0^1|x_t) p_{0|t}^\theta(x_0^2|x_t) \| x_0^1 - x_0^2 \| \rmd x_0^1 \rmd x_0^2  \rmd x_t \rmd t \\
    & \qquad + \int_0^1  \int_{(\rset^d)^3} p_t(x_t)p_{0|t}(x_0|x_t) p_{0|t}^\theta(x_0^1|x_t) \| x_0 - x_0^1 \|  \rmd x_0 \rmd x_0^1 \rmd x_t \rmd t . 
\end{align}
For the $\MMD$ diffusion loss \eqref{eq:mmdloss}, we have 
\begin{align}
    \mathcal{L}_{\MMD}(\theta) &= - \frac{1}{2} \int_0^1  \int_{(\rset^d)^3} p_{t}(x_t) p_{0|t}(x_0^1|x_t) p_{0|t}(x_0^2|x_t) \| x_0^1 - x_0^2 \| \rmd x_0^1 \rmd x_0^2 \rmd x_t \rmd t \\ 
    & \qquad - \frac{1}{2}\int_0^1  \int_{(\rset^d)^3} p_{t}(x_t) p_{0|t}^\theta(x_0^1|x_t) p_{0|t}^\theta(x_0^2|x_t) \| x_0^1 - x_0^2 \| \rmd x_0^1 \rmd x_0^2 \rmd x_t \rmd t \\
    & \qquad + \int_0^1 \int_{(\rset^d)^3} p_t(x_t)p_{0|t}(x_0|x_t) p_{0|t}^\theta(x^1_0|x_t)  \| x_0 - x_0^1 \| \rmd x_0 \rmd x_0^1 \rmd x_t \rmd t .
\end{align}
To summarize, in this section we have shown that the kernel score losses that we have defined in \Cref{sec:method} can be expressed in terms of squared $\MMD$. More precisely, we have shown the following result in the specific case where $k(x,x') = -\| x- x'\|$ (but the result remains true for every positive definite kernel).

\begin{proposition}{}{from_energy_loss_to_mmd_loss}
We have that \begin{align}
    \mathcal{L}(\theta) &=  \mathcal{L}_{\MMD}(\theta)+C,
\end{align}
with $C \in \rset$ a constant independent of $\theta$.  
\end{proposition}
This means that $p_{0|t}^\theta$ also minimizes $\mathcal{L}(\theta)$. Note that we could also have derived this result from the properties of the \emph{strictly proper} scoring rules. 

When optimizing with respect to the parameters of the kernel, here the kernel $\rho(x,x')=||x-x'||$ does not depend on any parameter, the losses $\mathcal{L}(\theta)$ and $\mathcal{L}_{\MMD}(\theta)$ differ since now we cannot neglect the additional term appearing in $\mathcal{L}_{\MMD}(\theta)$. Unfortunately, this term recalled below is computationally problematic
\begin{align}
    &\int_0^1 \int_{(\rset^d)^3} p_t(x_t) p_{0|t}(x_0|x_t) p_{0|t}(x_0^1|x_t) \| x_0 - x_0^1 \| \rmd x_0 \rmd x_0^1 \rmd x_t \rmd t . 
\end{align}
Indeed an estimate of this integral requires sampling $x_0^1$ from the conditional distribution $p_{0|t}(x_0|x_t)$. However this can be approximated by using the following self-normalized importance sampling approximation of this distribution 
\begin{equation}\label{eq:SNIS}
    p_{0|t}(x_0|x_t)=\frac{p_{t|0}(x_t|x_0)p_0(x_0)}{p_t(x_t)} \approx \sum_{i=1}^n w^i \delta_{X^i_0}(x_0),\quad w^i \propto p_{t|0}(x_t|X^i_0),~~\sum_{i=1}^n w^i=1,
\end{equation}
where $X^i_0 \overset{\textup{i.i.d.}}{\sim} p_0$.
Such approximation was considered by \citet{xu2023stable} to derive low variance regression targets in diffusion models. 
However, for $t$ small, the distribution of the weights $(w_i)_{i=1}^N$ is very degenerate and the approximation of $p_{0|t}(x_0|x_t)$ will be poor.

\subsection{Joint Diffusion Energy Loss}
\label{sec:conditional_versus_joint}

Now that we have established the connection between $\MMD$ diffusion losses and energy diffusion losses in \Cref{prop:from_energy_loss_to_mmd_loss}, we are going to discuss other possible learning objectives. We recall the \emph{joint diffusion energy loss} introduced in \eqref{eq:energylossjoint}
\begin{equation}
%\textstyle
\label{eq:energylossjoint_appendix}
\mathcal{L}_{\mathrm{joint}}(\theta)=-\int_{0}^{1} w_{t} \mathbb{E}_{p_{0,t}} \left[ S_{\lambda, \beta}(p_{0\vert t}^{\theta} p_t,(X_0, X_t)) \right]\rmd t .
\end{equation}
and the $\mathcal{L}_{\MMD, \joint}$ loss which is defined using \emph{joint} distributions as follows
\begin{equation}
    \mathcal{L}_{\MMD, \joint}(\theta) = \int_0^1 D_\rho(p_{t} p_{0|t}^\theta, p_{0,t}) \rmd t . \label{eq:mmdloss_joint}
\end{equation}
We emphasize again that $D_\rho$ appearing in $\mathcal{L}_{\MMD, \joint}$ is defined over $\rset^d \times \rset^d$ and not $\rset^d$ as in $\mathcal{L}_{\MMD}$.

Similarly to \Cref{prop:from_energy_loss_to_mmd_loss}, we can show the following result. Recall that we consider $w_t=1$ to simplify presentation.

\begin{proposition}{}{from_energy_loss_to_mmd_loss_joint}Let $\rho((x_0,x_t),(x_0',x_t')) = \rho(x_0,x_0') + \rho(x_t,x_t')$. We have that \begin{align}
    \mathcal{L}_{\mathrm{joint}}(\theta) =\mathcal{L}_{\MMD, \joint}(\theta)+C,
\end{align}
with $C \in \rset$ a constant independent of $\theta$. 
\end{proposition}

Developing $\mathcal{L}_{\MMD, \joint}(\theta)$, we obtain
\begin{align}\label{eq:mmdloss_jointdev}
    \mathcal{L}_{\MMD, \joint}(\theta) &= -\frac{1}{2}\int_0^1 \int_{(\rset^d)^4} p_t(x_t^1) p_{0|t}(x_0^1|x_t^1) p_t(x_t^2) p_{0|t}(x_0^2|x_t^2) [ \| x_0^1 - x_0^2 \| + \| x_t^1 - x_t^2 \| ] \rmd x_0^1 \rmd x_0^2 \rmd x_t^1 \rmd x_t^2 \rmd t \\
    & \qquad -\frac{1}{2}\int_0^1 \int_{(\rset^d)^4} p_t(x_t^1) p_{0|t}^\theta(x_0^1|x_t^1) p_t(x_t^2) p_{0|t}^\theta(x_0^2|x_t^2) [ \| x_0^1 - x_0^2 \| + \| x_t^1 - x_t^2 \| ] \rmd x_0^1 \rmd x_0^2 \rmd x_t^1 \rmd x_t^2 \rmd t \\
     & \qquad + \int_0^1 \int_{(\rset^d)^4} p_t(x_t^1) p_{0|t}^\theta(x_0^1|x_t^1) p_t(x_t^2) p_{0|t}(x_0^2|x_t^2) [ \| x_0^1 - x_0^2 \| + \| x_t^1 - x_t^2 \| ] \rmd x_0^1 \rmd x_0^2 \rmd x_t^1 \rmd x_t^2 \rmd t .
\end{align}

We recall that the term which does not depend on $\theta$ in $\mathcal{L}_\MMD$ is difficult to approximate, the proposed approximation using importance sampling \eqref{eq:SNIS} is indeed expected to perform poorly. On the contrary the term which does not depend on $\theta$ in $\mathcal{L}_{\MMD, \joint}$ is very easy to approximate. This suggests that if one would optimize the loss with respect to the parameters of the kernel, then the choice  $\mathcal{L}_{\MMD, \joint}$ is more suitable. However, in the case where the parameters are fixed, as it is the case in our framework, then we will show in \Cref{sec:sample_efficiency} that  $\mathcal{L}_\MMD$ is more sample efficient. 

\subsection{Marginal Diffusion Energy Loss}
\label{sec:joint_and_marginal}

Finally, we introduce a last discrepancy $\mathcal{L}_{\MMD, \marginal}$ given by
\begin{align}
    \mathcal{L}_{\MMD, \marginal}(\theta) &= \int_0^1 D_\rho(p_0^{\theta,t}, p_0) \rmd t , \label{eq:mmdloss_marginal}
\end{align}
where
\begin{equation}
    p_0^{\theta, t}(x_0) = \int_{\rset^d} p_t(x_t) p_{0|t}^\theta(x_0|x_t) \rmd x_t . 
\end{equation}
After a few simplifications, we obtain 
\begin{align}
    \mathcal{L}_{\MMD, \marginal}(\theta) &= -\frac{1}{2}\int_0^1 \int_{(\rset^d)^4} p_t(x_t^1) p_{0|t}(x_0^1|x_t^1) p_t(x_t^2) p_{0|t}(x_0^2|x_t^2) \| x_0^1 - x_0^2 \|  \rmd x_0^1 \rmd x_0^2 \rmd x_t^1 \rmd x_t^2 \rmd t \\
    & \qquad -\frac{1}{2}\int_0^1 \int_{(\rset^d)^4} p_t(x_t^1) p_{0|t}^\theta(x_0^1|x_t^1) p_t(x_t^2) p_{0|t}^\theta(x_0^2|x_t^2) \| x_0^1 - x_0^2 \| \rmd x_0^1 \rmd x_0^2 \rmd x_t^1 \rmd x_t^2 \rmd t \\
     & \qquad + \int_0^1 \int_{(\rset^d)^4} p_t(x_t^1) p_{0|t}^\theta(x_0^1|x_t^1) p_t(x_t^2) p_{0|t}(x_0^2|x_t^2) \| x_0^1 - x_0^2 \| \rmd x_0^1 \rmd x_0^2 \rmd x_t^1 \rmd x_t^2 \rmd t .
\end{align}
Note that while the minimizers of the conditional and joint losses are unique and given by $p_{0|t}$, this is not the case for the marginal loss. Indeed, we simply have that a minimizer of the marginal loss should satisfy 
\begin{equation}
    p_0(x_0) = \int_{\rset^d} p_{0|t}^\theta(x_0|x_t) p_t(x_t) \rmd x_t . 
\end{equation}
This simply says that we should transport $p_t$ to $p_0$ and there is an infinite number of such transports.

\subsection{Empirical versions of MMD and MMD Joint diffusion losses}
\label{sec:empirical_version}

In this section we justify the form of \eqref{eq:lossengression}, i.e., the empirical version of $\mathcal{L}$ given by \eqref{eq:energyloss}. For completeness, here we present the empirical version of $\mathcal{L}_\MMD$ which can be expressed as $\mathcal{L}_\MMD(\theta) = \mathcal{L}(\theta) + C$, where $C$ is a term which does not depend on $\theta$, according to \Cref{prop:from_energy_loss_to_mmd_loss}

We recall that we have 
\begin{align}
    \mathcal{L}_\MMD(\theta) &= -\frac{1}{2} \int_0^1  \int_{(\rset^d)^3} p_{t}(x_t) p_{0|t}(x_0^1|x_t) p_{0|t}(x_0^2|x_t) \| x_0^1 - x_0^2 \| \rmd x_0^1 \rmd x_0^2 \rmd x_t \rmd t \\ 
    & \qquad - \frac{1}{2}\int_0^1  \int_{(\rset^d)^3} p_{t}(x_t) p_{0|t}^\theta(x_0^1|x_t) p_{0|t}^\theta(x_0^2|x_t) \| x_0^1 - x_0^2 \| \rmd x_0^1 \rmd x_0^2 \rmd x_t \rmd t \\
    & \qquad + \int_0^1 \int_{(\rset^d)^3} p_t(x_t) p_{0|t}(x_0|x_t) p_{0|t}^\theta(x_0^1|x_t) \| x_0 - x_0^1 \| \rmd x_0 \rmd x_0^1 \rmd x_t \rmd t 
\end{align}

\textbf{Confinement term.} We first focus on the last term of the loss, known as \emph{confinement term} 
\begin{equation}
    \mathcal{C}(\theta) = \int_0^1 \int_{(\rset^d)^3} p_{0,t}(x^1_0,x_t^1) p_{0|t}^\theta(x_0^2|x_t^1) \| x_0^1 - x_0^2 \| \rmd x_0^1 \rmd x_0^2 \rmd x_t^1 \rmd t . 
\end{equation}
This can be estimated unbiasedly by 
\begin{equation}
    \mathcal{C}_{n, m} = \frac{1}{n} \sum_{i=1}^n \frac{1}{m} \sum_{j=1}^{m} \| X_0^{1,i} - X_0^{2,i,j} \| , \label{eq:population_confinement_appendix}
\end{equation}
where $t_i \sim \mathrm{Unif}([0,1])$, $(X^{1,i}_0,X_{t_i}^i) \sim p_{0,t_i}$ for $i\in[n]$ and  $X_0^{2,i,j} \sim p_{0|t}^\theta(\cdot|X_{t_i}^i)$ for $j\in[m]$. 

\textbf{Prediction Interaction term.} We now focus on the \emph{prediction interaction term}
\begin{equation}
    \mathcal{I}= \int_0^1  \int_{(\rset^d)^3} p_{t}(x_t) p_{0|t}^\theta(x_0^1|x_t) p_{0|t}^\theta(x_0^2|x_t) \| x_0^1 - x_0^2 \| \rmd x_0^1 \rmd x_0^2 \rmd x_t \rmd t . 
\end{equation}
This can be approximated unbiasedly by 
\begin{equation}
    \mathcal{I}_{n,m} = \frac{1}{n} \sum_{i=1}^n \frac{1}{m(m-1)} \sum_{j,j'=1}^{m} \| X_0^{i,j} - X_0^{i,j'} \| .
\end{equation}
where $t_i \sim \mathrm{Unif}([0,1])$, $X_{t_i}^i \sim p_{t_i}$ for $i\in[n]$ and $X_0^{i,j} \sim p_{0|t}^\theta(\cdot|X_{t_i}^i)$ for $j\in [m]$. 

\paragraph{Target interaction term.} We finally focus on the \emph{target interaction term}. This term is harder to describe and implement. However, we emphasize that it is not needed in order to optimize $\mathcal{L}$ given by \eqref{eq:energyloss}. It is only needed when computing \eqref{eq:mmdloss}. We have 
\begin{equation}
    \mathcal{I}^t= \int_0^1  \int_{(\rset^d)^3} p_{t}(x_t) p_{0|t}(x_0^1|x_t) p_{0|t}(x_0^2|x_t) \| x_0^1 - x_0^2 \| \rmd x_0^1 \rmd x_0^2 \rmd x_t \rmd t . 
\end{equation}
As explained before it can also be written as 
\begin{equation}
    \mathcal{I}^t= \int_0^1  \int_{(\rset^d)^3} p_{0,t}(x_0^1, x_t^1)  \frac{p_{t|0}(x_t^1|x_0^2)p_0(x_0^2)}{\int_{\rset^d}p_{t|0}(x_t^1|x_0^3) p_0(x_0^3) \rmd x_0^3} \| x_0^1 - x_0^2 \| \rmd x_0^1 \rmd x_0^2 \rmd x_t^1 \rmd t . 
\end{equation}
One estimate of this objective is given by 

\begin{equation}
    \mathcal{I}^t_{n,m} = \frac{1}{n} \sum_{i=1}^n  \sum_{j=1, j\neq i}^{n} \frac{p_{t_i|0}(X_{t_i}^{i}|X_0^{j})}{\sum_{k=1, k\neq i}^{n} p_{t_i|0}(X_{t_i}^{i}|X_0^{k})} \| X_0^{i} - X_0^{j} \|,
\end{equation}
where $t_i \sim \mathrm{Unif}([0,1])$, $X^i_0,X^i_{t_i}\sim p_{0,t_i}$.

\subsection{Sample efficiency}
\label{sec:sample_efficiency}
Previous subsections assume $w_t=1$ for ease of presentation. We consider a general weighting function $w_t$ here. 
Finally, in this section, we discuss the sample efficiency of the empirical version of the \emph{energy diffusion loss} \eqref{eq:energyloss} and the \emph{joint energy diffusion loss} \eqref{eq:energylossjoint}. In particular, we prove \Cref{prop:variance_snr}. 

\paragraph{U-statistics. }
We need first to recall a few basic results about $U$-statistics of order 2. Consider a symmetric function $h: (\rset^d)^2 \to \rset$ and let 
\begin{equation}
    U_n = {{n}\choose{2}}^{-1} \sum_{1\leq i_1<i_2\leq n} h(X_{i_1}, X_{i_2}),\qquad  X_i \overset{\textup{i.i.d.}}{\sim} p.
\end{equation}

We recall Hoeffding's theorem on the variance of $U$-statistics, see \citep{serfling2009approximation} for instance.
\begin{proposition}{Hoeffding's theorem}{hoeffding}
We have 
\begin{equation}
    \mathrm{Var}(U_n) = \frac{2}{n(n-1)} [2(n-2) \mathrm{Var}(\mathbb{E}[h(X_1,X_2) \ | \ X_1]) + \mathrm{Var}(h(X_1,X_2))]. 
\end{equation}
\end{proposition}
This implies that
\begin{equation}
    \lim_{n \to + \infty} n \mathrm{Var}(U_n) = 4 \mathrm{Var}(\mathbb{E}[h(X_1,X_2) \ | \ X_1]) . 
\end{equation}
Another classical result is the law of total variance.
\begin{lemma}{}{variance_conditional}
We have 
\begin{equation}
    \mathrm{Var}(Y) = \mathrm{Var}(\mathbb{E}[ Y \ | \ X]) + \mathbb{E}[\mathrm{Var}(Y \ | \ X)] . 
\end{equation}
\end{lemma}

Combining those two results, we obtain the following proposition which is central to the rest of the study.

\begin{proposition}{}{appendix_main_variance}
Let $X_i \overset{\textup{i.i.d.}}{\sim} p$ for $i\in\{1,...,n\}$ and $n \in \nset$.  Define also $\{U_i\}_{i=1}^n$ such that for each $i \in \{1, \dots, n\}$, $U_i$ is independent from $X_j$ for $j \neq i$,  $\mathbb{E}[U_i|X_i]$ are i.i.d. and $U_i$ is a $U$-statistics of order $2$ with $m$ samples.  We define 
\begin{equation}
    U = \frac{1}{n} \sum_{i=1}^n U_i . 
\end{equation}
Then, we have that 
\begin{equation}
    \mathrm{Var}(U) = \frac{1}{n}\mathrm{Var}(\mathbb{E}[U_1 \ | \ X_1]) + \frac{1}{n} \mathbb{E}[\mathrm{Var}(U_1 \ | \ X_1)] . 
\end{equation}
In particular, we have that 
\begin{equation}
    \lim_{m \to + \infty} \mathrm{Var}(U) = \frac{1}{n}\mathrm{Var}(\mathbb{E}[U_1 \ | \ X_1]) . 
\end{equation}
\end{proposition}

\paragraph{Conditional and joint interaction terms.}

We consider two interaction terms. Recall that we assume that we are working here under the strong assumption that we are at parameter $\theta$ such that $p^{\theta}_{0|t}=p_{0|t}$. The first one is given by the empirical interaction term in the \emph{joint energy diffusion loss}

\begin{equation}
    \mathcal{I}_{n,m, \joint} = \frac{1}{n}\sum_{i=1}^n \frac{1}{m(m-1)} \sum_{j,j'=1}^m w_{t_i} \{ \| X_0^{i,j} - X_0^{i,j'}\| + \| X_{t_i}^{i,j} - X_{t_i}^{i,j'}\| \} ,
\end{equation}
with $t_i  \overset{\textup{i.i.d.}}{\sim}  \mathrm{Unif}([0,1])$, $\{X_0^{i,j}\}_{i,j=1}^{n,m} \overset{\textup{i.i.d.}}{\sim} p_0$ and $X_{t_i}^{i,j} \sim p_{t_i|0}(\cdot | X_0^{i,j})$. 
The second one is given by the empirical interaction term in the \emph{conditional} loss:
\begin{equation}
    \mathcal{I}_{n,m} = \frac{1}{n}\sum_{i=1}^n \frac{1}{m(m-1)} \sum_{j,j'=1}^m w_{t_i} \{ \| X_0^{i,j} - X_0^{i,j'}\|\} ,
\end{equation}
where $t_i  \overset{\textup{i.i.d.}}{\sim}  \mathrm{Unif}([0,1])$ and for any $i \in [n]$, $X_{t_i} \sim p_{t_i}$. In addition, $\{X_0^{i,j}\}_{i,j=1}^{n,m}$ are conditionally independent, i.e. $X_0^{i,j} \sim p_{0|t_i}(\cdot | X_{t_i})$.

We start with the following proposition giving the mean of $\mathcal{I}_{n,m}$ and $\mathcal{I}_{n,m, \joint}$.

\begin{proposition}{}{mean_interaction_term}
We have that 
\begin{equation}
    \mathbb{E}[\mathcal{I}_{n,m, \joint}] = \int_0^1 \int_{(\rset^d)^4} w_t \{ \| x_0 - x_0' \|  + \| x_t - x_t' \| \} p_{0,t}(x_0, x_t) p_{0,t}(x_0', x_t') \rmd x_0 \rmd x_t \rmd x_0' \rmd x_t' \rmd t . 
\end{equation}
Similarly, we have that
\begin{equation}
    \mathbb{E}[\mathcal{I}_{n,m}] = \int_0^1 \int_{(\rset^d)^3} w_t \| x_0 - x_0' \| p_t(x_t) p_{0|t}(x_0|x_t) p_{0|t}(x_0'|x_t) \rmd x_0 \rmd x_0' \rmd x_t \rmd t .  
\end{equation}
\end{proposition}

Next, we can obtain the asymptotic variance of those random variables.
\begin{proposition}{}{}
We have that 
\begin{align}
   & \lim_{m \to + \infty}\mathrm{Var}(\mathcal{I}_{n,m, \joint}) \\
   & \qquad = \frac{1}{n}  \mathrm{Var}_{t \sim \mathrm{Unif}([0,1])} \left( \int_{(\rset^d)^4} w_t \{ \| x_0 - x_0' \| + \| x_t - x_t' \|\} p_{0,t}(x_0,x_t) p_{0,t}(x_0'x_t') \rmd x_0 \rmd x_t \rmd x_0' \rmd x_t' \right) . 
\end{align}
Similarly, we have that
\begin{equation}
    \lim_{m \to + \infty} \mathrm{Var}(\mathcal{I}_{n,m}) = \frac{1}{n} \mathrm{Var}_{t \sim \mathrm{Unif}([0,1]),X_t \sim p_t} \left( \int_{(\rset^d)^2} w_t \| x_0 - x_0' \| p_{0|t}(x_0|x_t) p_{0|t}(x_0'|x_t) \rmd x_0 \rmd x_0' \right) . 
\end{equation}

\end{proposition}

\begin{proof}
This is an application of \Cref{prop:appendix_main_variance}, with $X=t \sim \mathrm{Unif}([0,1])$ in the case of $\mathcal{I}_{n,m, \joint}$ and $X=(t,X_t)\sim  \mathrm{Unif}([0,1]) \otimes p_t$ in the case of $\mathcal{I}_{n,m}$. 
\end{proof}

In order to compare the sample efficiency of these different terms, we compute their Signal-to-Noise Ratio (SNR).
In particular, we compare 
\begin{align}
    &\SNR(\mathcal{I}_{n, \joint}) := \lim_{m \to + \infty}  \SNR(\mathcal{I}_{n, m, \joint}) = \lim_{m \to + \infty} \frac{\mathbb{E}[\mathcal{I}_{n,m, \joint}]^2}{\mathrm{Var}(\mathcal{I}_{n,m, \joint})} , \\
    &\SNR(\mathcal{I}_n) := \lim_{m \to + \infty}  \SNR(\mathcal{I}_{n, m}) = \lim_{m \to + \infty} \frac{\mathbb{E}[\mathcal{I}_{n,m}]^2}{\mathrm{Var}(\mathcal{I}_{n,m})} . 
\end{align}
The larger the SNR the better. In particular, if $\SNR(\mathcal{I}_n) \geq \SNR(\mathcal{I}_{n, \joint})$ then we claim that the conditional version is more sample efficient than the joint version. Note that both $\SNR(\mathcal{I}_n)$ and $\SNR(\mathcal{I}_{n, \joint})$ are linear functions of $N$. Therefore if $\SNR(\mathcal{I}_n) \geq \SNR(\mathcal{I}_{n, \joint})$ then we can interpret this as one sample used in the conditional version is worth $\SNR(\mathcal{I}_n) / \SNR(\mathcal{I}_{n, \joint})$ used in the joint version. 

While the general expression for these $\SNR$ can be hard to obtain, we are going to analyze the specific case where the target is Gaussian. In that case the posterior is known and explicit expressions can be derived. 

\paragraph{Gaussian case.} We assume that $p_0 \sim \mathcal{N}(0, \sigma^2 \Id)$ and consider $p_{t|0}(x_t|x_0) = \mathcal{N}(x_t;\alpha_t x_0, \sigma_t^2 \Id)$. In that case it can be shown that 
\begin{equation}
    p_{0|t}(x_0|x_t) = \mathcal{N}(x_0;\alpha_t \sigma^2 / (\alpha_t^2 \sigma^2 + \sigma_t^2) x_t , \sigma^2 (1 - \alpha_t^2 \sigma^2 / (\alpha_t^2 \sigma^2 + \sigma_t^2)) \Id) .
\end{equation}
This can be rewritten as 
\begin{equation}
    p_{0|t}(x_0|x_t) = \mathcal{N} \left(x_0; \frac{x_t}{1 + u_t}, \frac{\sigma^2 u_t }{1 + u_t}\Id \right) , 
\end{equation}
where $u_t = \sigma_t^2 / \alpha_t^2 \sigma^2$. Note that $\lim_{t \to 0} u_t = 0$ and $\lim_{t \to 1} u_t = +\infty$. 
First, we compute the mean in both cases.

\begin{proposition}{}{mean_gaussian_mmd}
We have that
\begin{equation}
    \mathbb{E}[\mathcal{I}_{n,m, \joint}] = \sqrt{2}  \mathbb{E}[\| Z \|] \int_0^1 w_t  (\sigma  +  (\alpha_t^2 \sigma^2 + \sigma_t^2)^{1/2}) \rmd t . 
\end{equation}
In addition, we have that 
\begin{equation}
    \mathbb{E}[\mathcal{I}_{n,m}] = \sqrt{2}  \sigma \mathbb{E}[\| Z \|] \int_0^1 w_t \left(\frac{u_t}{1+u_t}\right)^{1/2} \rmd t .
\end{equation}
\end{proposition}

Then, we compute the variance terms.
\begin{proposition}{}{variance_gaussian_mmd}
We have that 
\begin{equation}
    \lim_{m \to + \infty}\mathrm{Var}(\mathcal{I}_{n,m, \joint}) =  \frac{2 \mathbb{E}[\|Z\|]^2}{n} \left[  \int_0^1 w_t^2 (\sigma + (\alpha_t^2 \sigma^2 + \sigma_t^2)^{1/2})^2 \rmd t -  \left(\int_0^1 w_t ( \sigma +  (\alpha_t^2 \sigma^2 + \sigma_t^2)^{1/2}) \rmd t\right)^2 \right] .
\end{equation}
Similarly, we have that
\begin{equation}
    \lim_{m \to + \infty} \mathrm{Var}(\mathcal{I}_{n,m}) =  \frac{2 \mathbb{E}[\|Z\|]^2 \sigma^2}{n} \left[ \int_0^1 w_t^2 \frac{ u_t}{1+u_t} \rmd t - \left( \int_0^1 w_t \left(\frac{u_t}{1+u_t}\right)^{1/2} \rmd t \right)^2\right] . 
\end{equation}

\end{proposition}

Combining \Cref{prop:mean_gaussian_mmd} and \Cref{prop:variance_gaussian_mmd}, we get the following result.

\begin{proposition}{}{snr_gaussian_mmd}
We have that 
\begin{equation}
    \SNR(\mathcal{I}_{n, \joint}) = n \frac{\left(\int_0^1 w_t  (\sigma  +  (\alpha_t^2 \sigma^2 + \sigma_t^2)^{1/2}) \rmd t\right)^2}{\int_0^1 w_t^2 (\sigma + (\alpha_t^2 \sigma^2 + \sigma_t^2)^{1/2})^2 \rmd t -  \left(\int_0^1 w_t ( \sigma +  (\alpha_t^2 \sigma^2 + \sigma_t^2)^{1/2}) \rmd t\right)^2} . 
\end{equation}
Similarly, we have 
\begin{equation}
    \SNR(\mathcal{I}_{n}) = n \frac{\left( \int_0^1 w_t \left( \frac{u_t}{1+u_t}\right)^{1/2} \rmd t \right)^2}{\int_0^1 w_t^2 \frac{u_t}{1+u_t} \rmd t - \left( \int_0^1 w_t \left( \frac{u_t}{1+u_t}\right)^{1/2} \rmd t \right)^2} . 
\end{equation}
\end{proposition}

This concludes the proof of \Cref{prop:variance_snr}. In \Cref{fig:snr_study_fig}, we show that in a setting where the weighting is chosen to follow the sigmoid one of \citep{kingma2021variational,hoogeboom2023simple}, we obtain that $\SNR(\mathcal{I}_{n, \joint}) \leq \SNR(\mathcal{I}_{n})$ for a larger range of $\sigma$.


\section{Diffusion compatible kernels}
\label{app:connection_to_mmd}

In this section, we prove the results of \Cref{sec:diffusion_compatible}. In particular, we prove \Cref{prop:diffusion_compatible}. 
We recall that for a continuous negative kernel $\rho_c$ with $c \in \rset$ a hyperparameter of the scoring rule, we say that $\rho_c$ is diffusion compatible, if there exist $c^\star \in [-\infty, +\infty]$ and $f: \rset \to \rset$ such that 
\begin{equation}
    \lim_{c \to c^\star} f(c) D_{\rho_c}(p, q) = \| \mathbb{E}_p[X] - \mathbb{E}_q[X] \|^2 . 
\end{equation}
We also recall some of the kernels we use.
\begin{subequations} \label{eq:kernels_specific_appendix}
\begin{align}
k_{\mathrm{\imq}}(x,x') & =(\|x-x'\|^{2}+c)^{-1/2},\label{eq:imq_k_appendix}\\
k_{\rbf}(x,x') & =\mathrm{exp}\left[-\|x-x'\|^2/\sigma\right],\label{eq:rbf_k_appendix} \\
k_{\expk}(x,x') & =\mathrm{exp}\left[-\|x-x'\|/\sigma\right].\label{eq:exp_k_appendix} 
\end{align}
\end{subequations}

In particular, we prove the following proposition.

\begin{proposition}{}{diffusion_compatible_appendix}
    Assume that $\rho = -k$ with $k$ given by \eqref{eq:imq_k_appendix} or \eqref{eq:rbf_k_appendix}. Then the scoring rule is diffusion compatible. In particular, if $k$ is given by \eqref{eq:imq_k_appendix}, we have that $c^\star = + \infty$ and $f(c) = 2c$. If $k$ is given by \eqref{eq:rbf_k_appendix}, we have that $c^\star = + \infty$ and $f(c) = 2c$.
\end{proposition}

\begin{proof}
First, we have that for any continuous positive kernel $k$, with $\rho=-k$, we get
\begin{align}
    D_\rho(p,q) &= S(q,q) - S(p,q) \\
    &= \mathbb{E}_{q \otimes q}[-k(Y,Y')] -  \frac{1}{2} \mathbb{E}_{q \otimes q}[-k(Y,Y')] - \mathbb{E}_{p \otimes q}[-k(X,Y)] +  \frac{1}{2} \mathbb{E}_{p \otimes p}[-k(X,X')] \\
    &= \mathbb{E}_{p\otimes q}[k(X,Y)] - \frac{1}{2} \mathbb{E}_{p \otimes p}[k(X,X')]  -  \frac{1}{2} \mathbb{E}_{q \otimes q}[k(Y,Y')]  .
\end{align}
In addition, we have that for any $u, v \in \rset$ 
\begin{equation}
    u D_\rho(p,q) = \mathbb{E}_{p\otimes q}[u (k(X,Y) -  v)] - \frac{1}{2} \mathbb{E}_{p \otimes p}[u(k(X,X') - v)]  -  \frac{1}{2} \mathbb{E}_{q \otimes q}[u(k(Y,Y') - v)].
\end{equation}
Let $k$ be given by \eqref{eq:imq_k_appendix}, i.e., for any $x, x' \in \rset^d$, we have that $k_{\mathrm{\imq}}(x,x')  =(\|x-x'\|^{2}+c)^{-1/2}$. 
Let $c>0$, $u = 2c$ and $ v = c^{-1/2}$ and consider $(c_n)_{n \in \nset}$ such that $c_n \to + \infty$. 
We have that for any $x, x'$ and $c > 0$
\begin{align}
    | u(k(x,x') - v) | &= 2c^{1/2} - 2c(\|x-x'\|^2 + c)^{-1/2} \\
    &=\frac{2c^{1/2}(\|x-x'\|^2 + c)^{1/2} - 2c}{(\|x-x'\|^2 + c)^{1/2}} \\
    &\leq \frac{2 c^{1/2} \| x - x' \| }{(\|x-x'\|^2 + c)^{1/2}} \\
    &\leq \frac{2 \| x - x' \| }{(\|x-x'\|^2/c + 1)^{1/2}} \leq 2 \| x - x' \| . 
\end{align}
In addition, we have that for any $x, x'$ and $n \in \nset$
\begin{equation}
    2c_n^{1/2} - 2c_n(\|x-x'\|^2 + c_n)^{-1/2} = \frac{2 - 2(\|x-x'\|^2/c_n + 1)^{1/2}}{(\|x-x'\|^2/c_n + 1)^{1/2}} . 
\end{equation}
Therefore, letting $n \to + \infty$ we have that 
\begin{equation}
    \lim_{n \to +\infty} 2c_n^{1/2} - 2c_n(\|x-x'\|^2 + c_n)^{-1/2} = \| x- x'\|^2 . 
\end{equation}
Hence, setting $f(c) = 2c$, and using the dominated convergence theorem, we get that 
\begin{align}
    \lim_{c \to + \infty} f(c) D_\rho(p,q) &= \mathbb{E}_{p\otimes q}[\| X - Y \|^2] - \frac{1}{2} \mathbb{E}_{p \otimes p}[\| X - X' \|^2]  -  \frac{1}{2} \mathbb{E}_{q \otimes q}[\| Y - Y' \|^2] \\
    &= \| \mathbb{E}_p[X] - \mathbb{E}_q[X] \|^2 ,
\end{align}
which concludes the first part of the proof. Next, we consider $k$ be given by \eqref{eq:rbf_k_appendix}, i.e., for any $x, x' \in \rset^d$, we have that $k_{\mathrm{\exp}}(x,x') =\exp[-\| x - x'\|^2 / \sigma^2]$. Let $u = 2\sigma^2$ and $v = 1$. Then, we have that 
\begin{align}
     | u(k(x,x') - v) |  = 2 \sigma^2 (1 - \exp[-\| x - x'\|^2 / 2 \sigma^2]) \leq \| x - x'\|^2 . 
\end{align}
Now consider $(c_n)_{n \in \nset}$ such that $c_n \to + \infty$. We have that 
\begin{equation}
    \lim_{n \to + \infty} 2 c_n^2 (1 - \exp[-\| x - x'\|^2 / 2 c_n^2]) = \| x - x' \|^2 . 
\end{equation}
Hence, setting $f(c) = 2c$, and using the dominated convergence theorem, we get that 
\begin{align}
    \lim_{c \to + \infty} f(c) D_\rho(p,q) &= \mathbb{E}_{p\otimes q}[\| X - Y \|^2] - \frac{1}{2} \mathbb{E}_{p \otimes p}[\| X - X' \|^2]  -  \frac{1}{2} \mathbb{E}_{q \otimes q}[\| Y - Y' \|^2] \\
    &= \| \mathbb{E}_p[X] - \mathbb{E}_q[X] \|^2 ,
\end{align}
which concludes the second part of the proof. 
\end{proof}

Finally, we highlight that while the exponential kernel \eqref{eq:exp_k_appendix} is not diffusion compatible it satisfies another compatibility rule. More precisely, we can show the following result. The proof is similar to the proof of \Cref{prop:diffusion_compatible_appendix}, in the case of $k_\rbf$. 

\begin{proposition}{}{}
Assume that $\rho = -k$ with $k$ given by \eqref{eq:exp_k_appendix}, i.e., $k(x,x') = \exp[-\| x - x' \| / \sigma]$. In that case, upon setting $f(c) = c$ and $c^\star = +\infty$, we get that 
\begin{equation}
    \lim_{c \to + \infty} f(c) D_{\rho_c}(p,q) = D_{\rho^\star}(p,q) ,
\end{equation}
where $\rho^\star(x,x') = \| x - x'\|$, i.e. we recover the energy distance with $\beta = 1$. 
\end{proposition}

\section{DDIM updates from a Stochastic Differential Equation perspective}
\label{sec:ddim_sde}

In this section, we show that one can \emph{exactly} recover the DDIM updates from the stochastic process point of view with a careful choice of forward process. 



We first show that difference choices of forward process yields different interpolation densities $p_{s|0,t}$ in \Cref{sec:generalinterpolationprocesses}.
With a careful choice of the forward process and appropriate parameters, we show that we can recover DDIM updates \citep{song2020denoisingimplicit} in \Cref{sec:connection_with_ddim_section}.

\subsection{General forward processes}\label{sec:generalinterpolationprocesses}
\paragraph{Interpolation and forward process.} Consider the following interpolation
\begin{equation}
\label{eq:initial_interpolation}
    \bfX_t = \alpha_t \bfX_0 + \sigma_t \bfZ , \qquad \bfX_0 \sim p_0 . 
\end{equation}
where $\bfZ \sim \mathcal{N}(0, \Id)$ and the equality holds in distribution. 
Next, we associate a forward process with this interpolation. 
First, note that one possible process is given by  $\bfX_0 \sim p_0$ and 
\begin{align}
    \rmd \bfX_t &= [\partial_t \alpha_t \bfX_0 + \partial_t \sigma_t \bfZ] \rmd t \\
     &= \left[\partial_t \log(\alpha_t)\bfX_t + \left\lbrace \partial_t \sigma_t -\partial_t \alpha_t \frac{\sigma_t}{\alpha_t}\right\rbrace \bfZ\right] \rmd t ,
\end{align}
where we have used \eqref{eq:initial_interpolation}. 
In addition, one can show that another valid sampler is given by
\begin{align}\label{eq:ODE}
    \rmd \bfX_t = \left[\partial_t \log(\alpha_t) \bfX_t + \left\lbrace \partial_t \sigma_t -\partial_t \alpha_t \frac{\sigma_t}{\alpha_t}\right\rbrace ((1-\vareps^2) \bfZ +\vareps^2 \mathbb{E}[\bfZ | \bfX_t]) \right] \rmd t ,
\end{align}
with $\vareps \in [0,1]$. Note that this is a mean-field equation, in the sense that \eqref{eq:ODE} does not depend only on $\bfX_t$ but also on its distribution. More precisely, we have the following result.
\begin{proposition}{}{}
Let $\vareps \in [0,1]$. Let $\bfZ \sim \mathcal{N}(0, \Id)$ and $\bfX_0 \sim p_0$ and assume that the following mean-field ODE admits a solution
\begin{equation}
    \rmd \bfX_t = \left[\partial_t \log(\alpha_t) \bfX_t + \left\lbrace \partial_t \sigma_t -\partial_t \alpha_t \frac{\sigma_t}{\alpha_t}\right\rbrace \left((1-\vareps_t^2) z +\vareps_t^2 \frac{\int_{\rset^d} \tilde{z} p_t(\bfX_t|\tilde{z})p(\tilde{z})r \rmd \tilde{z}}{\int_{\rset^d} p_t(\bfX_t|\tilde{z}) p(\tilde{z}) \rmd \tilde{z}}\right) \right] \rmd t , \label{eq:ODE_correct}
\end{equation}
where $p(z) = \mathcal{N}(z; 0 , \Id)$ and $p_t(\cdot|z)$ is the density of $\bfX_t$ conditionally to $\bfZ=z$. Denote $p_t(x_t) = \int_{\rset^d} p_t(x_t|\tilde{z}) p(\tilde{z}) \rmd \tilde{z}$, where $p(\tilde{z}) = \mathcal{N}(\tilde{z} ; 0, \Id)$. Then, we have that for any $t \in [0,1]$, $\alpha_t \bfX_0 + \sigma_t \bfZ \sim p_t$.
\end{proposition}

Now, we recall that using Tweedie's identity we have
\begin{equation}
    \nabla \log p_t(x_t) = \frac{1}{\sigma_t^2} \mathbb{E}[\alpha_t \bfX_0 - \bfX_t | \bfX_t = x_t] = -\frac{1}{\sigma_t} \mathbb{E}[\bfZ | \bfX_t = x_t].
\end{equation}
The following non-Markov diffusion admits the same marginals as \eqref{eq:ODE_correct} 
\begin{align}\label{eq:ODE+Langevin}
    \rmd \bfX_t &= \left[\partial_t \log(\alpha_t) \bfX_t + \left\lbrace \partial_t \sigma_t -\partial_t \alpha_t \frac{\sigma_t}{\alpha_t}\right\rbrace ((1-\vareps^2) \bfZ +\vareps^2\mathbb{E}[\bfZ | \bfX_t]) + \frac{\vareps_t^2}{2} \nabla \log p_t(\bfX_t) \right] \rmd t + \vareps_t \rmd \bfB_t \\
    &= \left[\partial_t \log(\alpha_t) \bfX_t + \left\lbrace \partial_t \sigma_t -\partial_t \alpha_t \frac{\sigma_t}{\alpha_t}\right\rbrace ((1-\vareps_t^2) \bfZ + \vareps^2 \mathbb{E}[\bfZ | \bfX_t]) - \frac{\vareps_t^2}{2\sigma_t} \mathbb{E}[\bfZ| \bfX_t] \right] \rmd t + \vareps_t \rmd \bfB_t,
\end{align}
as we have simply added a Langevin term of invariant distribution $p_t$ to \eqref{eq:ODE}. Next, we define 
\begin{equation}
    \vareps_t^2 = 2 \vareps^2 \alpha_t \sigma_t \partial_t (\sigma_t / \alpha_t) = \vareps^2 g_t^2 . 
\end{equation}
In that case, \eqref {eq:ODE+Langevin} simplifies to
\begin{equation}
\label{eq:forward_churn}
    \rmd \bfX_t = \left[ \partial_t \log(\alpha_t) \bfX_t + \frac{g_t^2}{2 \sigma_t} (1- \vareps^2) \bfZ \right] \rmd t + \vareps g_t \rmd \bfB_t . 
\end{equation}
In \eqref{eq:forward_churn}, we identify different forward processes which are all non-Markov except in the case where $\vareps = 1$. The remarkable property of these forward processes is that they all recover the interpolation \eqref{eq:initial_interpolation}. This means that the forward trajectories might be very different but they all admit the same \emph{marginal} distributions. More formally, we get the following result.

\begin{proposition}{}{forward_process_epsilon}
For any $t \in [0,1]$, let $\vareps_t \in [0,1]$. Let $\bfZ \sim \mathcal{N}(0, \Id)$ and $\bfX_0 \sim p_0$. Additionally, assume that the following SDE admits a solution
\begin{equation}
\label{eq:forward_churn_prop}
    \rmd \bfX_t = \left[ \partial_t \log(\alpha_t) \bfX_t + \frac{g_t^2}{2 \sigma_t} (1- \vareps^2) \bfZ \right] \rmd t + \vareps g_t \rmd \bfB_t ,
\end{equation}
where $(\bfB_t)_{t \in [0,1]}$ is a $d$-dimensional Brownian motion, $p(z) = \mathcal{N}(z; 0 , \Id)$ and $p_t(\cdot|z)$ is the density of $\bfX_t$ conditionally to $\bfZ=z$. Denote $p_t(x_t) = \int_{\rset^d} p_t(x_t|z) p(z) \rmd z$. We have that for any $t \in [0,1]$, $\alpha_t \bfX_0 + \sigma_t \bfZ \sim p_t$.
\end{proposition}


\paragraph{Explicit integration.} We can solve exactly \eqref{eq:forward_churn} and its solution is given for any $t \geq s$ by  
\begin{equation}
    \bfX_t = \frac{\alpha_t}{\alpha_s} \bfX_s + (1 - \vareps^2) \left(\sigma_t - \frac{\alpha_t}{\alpha_s} \sigma_s \right) \bfX_1 + \vareps \left(\sigma_t^2 - \frac{\alpha_t}{\alpha_s} \sigma_s^2 \right)^{1/2} \tilde{\mathbf{Z}} ,
 \end{equation}
 with $\tilde{\mathbf{Z}} \sim \mathcal{N}(0, \Id)$. This can also be rewritten as 
 \begin{align}
    \bfX_t -\alpha_t \bfX_0 &= -\alpha_t \bfX_0 +  \frac{\alpha_t}{\alpha_s} \bfX_s + (1 - \vareps^2) \left(\sigma_t - \frac{\alpha_t}{\alpha_s} \sigma_s \right) \frac{\bfX_s - \alpha_s \bfX_0}{\sigma_s} + \vareps \left(\sigma_t^2 - \frac{\alpha_t}{\alpha_s} \sigma_s^2 \right)^{1/2} \tilde{\mathbf{Z}}  \\
    &= \left[\vareps^2 \frac{\alpha_t}{\alpha_s} + (1- \vareps^2) \frac{\sigma_t}{\sigma_s}\right] (\bfX_s - \alpha_s \bfX_0)  + \vareps \left(\sigma_t^2 - \frac{\alpha_t}{\alpha_s} \sigma_s^2 \right)^{1/2} \tilde{\mathbf{Z}} . \label{eq:integration_sde}
 \end{align}
We can now compute the probability distribution $p_{s,t|0}$. This will allow us to compute $p_{s|0,t}$ using Gaussian posteriors. Using \eqref{eq:integration_sde}, we have 
\begin{equation}
    (\bfX_s, \bfX_t) | \bfX_0 \sim \mathcal{N}\left( \left(\begin{matrix} \alpha_s \bfX_0 \\ \alpha_t \bfX_0 \end{matrix} \right), \left( \begin{matrix} \sigma_s^2 \Id & C_t   \\
    C_t & \sigma_t^2 \Id
    \end{matrix} \right)\right) , 
\end{equation}
with 
\begin{align}
    C_t = \left[\vareps^2 \frac{\alpha_t}{\alpha_s} + (1- \vareps^2) \frac{\sigma_t}{\sigma_s}\right] \sigma_s^2 \Id =\left[\vareps^2 \frac{\alpha_t}{\alpha_s}\sigma_s^2 + (1- \vareps^2) \sigma_t \sigma_s\right]  \Id . 
\end{align}
Now, let us compute the obtained posterior.
\begin{align}
    \mathbb{E}[\bfX_s | \bfX_0, \bfX_t] &= \alpha_s \bfX_0 + \left[\vareps^2 \frac{\alpha_t}{\alpha_s}\frac{\sigma_s^2}{\sigma_t^2} + (1- \vareps^2) \frac{\sigma_s}{\sigma_t} \right] (\bfX_t - \alpha_t \bfX_0) \\
    &= \left[\vareps^2 \frac{\alpha_t}{\alpha_s}\frac{\sigma_s^2}{\sigma_t^2} + (1- \vareps^2) \frac{\sigma_s}{\sigma_t} \right]  \bfX_t + \alpha_s \left(1 - \left[\vareps^2 \frac{\alpha_t^2}{\alpha_s^2}\frac{\sigma_s^2}{\sigma_t^2} + (1- \vareps^2) \frac{\sigma_s}{\sigma_t}\frac{\alpha_t}{\alpha_s} \right]\right) \bfX_0 . 
\end{align}
Finally, we have  that 
\begin{align}
    \Cov(\bfX_s | \bfX_t, \bfX_0) &= \sigma_s^2 \Id - \left[\vareps^2 \frac{\alpha_t}{\alpha_s}\sigma_s^2 + (1- \vareps^2) \sigma_t \sigma_s\right]^2 \frac{1}{\sigma_t^2} \Id \\
    &= \sigma_s^2 \left(1 - \left[\vareps^2 \frac{\alpha_t}{\alpha_s}\frac{\sigma_s}{\sigma_t} + (1- \vareps^2)\right]^2 \right) .
\end{align}
Note that in the special case where $\vareps = 0$, we recover a deterministic sampler. To summarize, we have the following result which justifies \eqref{eq:mean_sigma}.


\begin{proposition}{}{}
Let $\vareps \in [0,1]$ and $(\bfX_t)_{t \in [0,1]}$ given by \Cref{prop:forward_process_epsilon}. Then, we have that for any $s, t \in [0,1]$ with $s \leq t$,
\begin{align}
    \bfX_s &= (\vareps^2 r_{1,2}(s,t) + (1 - \vareps^2) r_{0,1}) \bfX_t \\
    & \qquad +  \alpha_s(1 - \vareps^2 r_{2,2}(s,t) - (1 - \vareps^2) r_{1,1}(s,t)) \bfX_0 \\
    & \qquad \qquad + \sigma_s (1 - (\vareps^2 r_{1,1}(s,t) + (1-\vareps^2))^2)^{1/2} \tilde{\bfZ} , 
    \label{eq:final_update_sde_prop}
\end{align}
with $\tilde{\bfZ} \sim \mathrm{N}(0, \Id)$ and where 
\begin{equation}
    r_{i,j}(s,t) = \frac{\alpha_t^i}{\alpha_s^i}\frac{\sigma_s^j}{\sigma_t^j} . 
\end{equation}
\end{proposition}



%
\paragraph{Limiting behavior. } Looking at \eqref{eq:final_update_sde_prop} one can wonder if we recover the SDE framework when we let $s \to t$. This is indeed the case as established below. We will make use of the following result
\begin{equation}
    \lim_{s \to t } \frac{r_{i,j}(s,t) - 1}{t-s}  = i \partial_t \log(\alpha_t) - j \partial_t \log(\sigma_t) . 
\end{equation}
Therefore, we get that taking the limit in \eqref{eq:final_update_sde_prop} we have
\begin{align}
    \rmd \bfX_t &= \left[ (\vareps^2 (\partial_t \log(\alpha_t) - 2 \partial_t \log(\sigma_t))  - (1 - \vareps^2) \partial_t \log(\sigma_t)) \bfX_t  \right. \\
    &\qquad \left. - \alpha_t (\vareps^2 (2 \partial_t \log(\alpha_t) - 2 \partial_t \log(\sigma_t)) + (1-\vareps^2) (\partial_t \log(\alpha_t) - \partial_t \log(\sigma_t))) \bfX_0 \right] \rmd t \\
    &\qquad \qquad + \sigma_t [-2\vareps^2(\partial_t \log(\alpha_t) - \partial_t \log(\sigma_t))]^{1/2} \rmd \bfB_t . 
\end{align}
It can easily be shown that 
\begin{equation}
\label{eq:key_relationship}
    -2\sigma_t^2 (\partial_t \log(\alpha_t) - \partial_t \log(\sigma_t)) = 2 \alpha_t \sigma_t \partial_t (\sigma_t / \alpha_t) = g_t^2 . 
\end{equation}
In addition, we have 
\begin{align}
    \rmd \bfX_t &= \left[ -\partial_t \log(\alpha_t) \bfX_t +  (\vareps^2 (2\partial_t \log(\alpha_t) - 2 \partial_t \log(\sigma_t))  + (1 - \vareps^2) (\partial_t \log(\alpha_t) - \partial_t \log(\sigma_t))) \bfX_t  \right. \\
    &\qquad \left. - \alpha_t (\vareps^2 (2 \partial_t \log(\alpha_t) - 2 \partial_t \log(\sigma_t)) + (1-\vareps^2) (\partial_t \log(\alpha_t) - \partial_t \log(\sigma_t))) \bfX_0 \right] \rmd t  \\
    &\qquad \qquad + \sigma_t [-2\vareps^2(\partial_t \log(\alpha_t) - \partial_t \log(\sigma_t))]^{1/2} \rmd \bfB_t \\
    &= \left[ -\partial_t \log(\alpha_t) \bfX_t + (\vareps^2 (2\partial_t \log(\alpha_t) - 2 \partial_t \log(\sigma_t))  \right. \\
    & \qquad \left. + (1 - \vareps^2) (\partial_t \log(\alpha_t) - \partial_t \log(\sigma_t))) (\bfX_t - \alpha_t \bfX_0) \right] \rmd t + g_t \rmd \bfB_t \\
    &= \left[ -\partial_t \log(\alpha_t) \bfX_t + (1 + \vareps^2) (\partial_t \log(\alpha_t) - \partial_t \log(\sigma_t))) (\bfX_t - \alpha_t \bfX_0) \right] \rmd t + \vareps g_t \rmd \bfB_t . 
\end{align}
Now, combining this result and \eqref{eq:key_relationship} we get that 
\begin{equation}
    \rmd \bfX_t = \left[ - \partial_t \log(\alpha_t) +g_t^2 \frac{1 + \vareps^2}{2} \frac{\alpha_t \bfX_0 - \bfX_t }{\sigma_t^2} \right] \rmd t + \vareps g_t \rmd \bfB_t . 
\end{equation}
Taking the conditional expectation in this expression, we recover the score $\nabla \log p_t$ and therefore 
\begin{equation}
    \rmd \bfX_t = \left[ - \partial_t \log(\alpha_t) +g_t^2 \frac{1 + \vareps^2}{2} \nabla \log p_t(\bfX_t) \right] \rmd t + \vareps g_t \rmd \bfB_t . 
\end{equation}
Therefore, we recover the usual sampler derived from the stochastic process point of view by taking the limit $s \to t$ in \eqref{eq:final_update_sde_prop}. 

\subsection{Connection with DDIM}
\label{sec:connection_with_ddim_section}

We recall that in DDIM \citep{song2020denoisingimplicit}, the authors assume that the schedule $(\alpha_t, \sigma_t)$ satisfies $\alpha_t^2 + \sigma_t^2 = 1$. In this case, they consider 
\begin{equation}
    p(x_s|x_0, x_t) = \mathcal{N}(x_s; \hat{\mu}_{s,t}(x_0,x_t), \hat{\Sigma}_{s,t}) ,
\end{equation}
with 
\begin{align}
    &\hat{\mu}_{s,t}(x_0,x_t) = \sqrt{\frac{1 - \alpha_s^2 - \eta_{s,t}^2}{1-\alpha_t^2}} x_t + \left[ \alpha_s - \alpha_t \sqrt{\frac{1 - \alpha_s^2 - \eta_{s,t}^2}{1-\alpha_t^2}} \right] x_0 , \\
    &\hat{\Sigma}_{s,t} = \eta_{s,t}^2 \Id . \label{eq:mean_sigma_ddim_appendix}
\end{align}

Note that in \citep{song2020denoisingimplicit}, we $\alpha_t^2$ is replaced with $\alpha_t$ and $\eta_{s,t}$ is denoted $\sigma_t$.
Recall that in our introduction of diffusion models in \Cref{sec:diffusion_models}, see \eqref{eq:mean_sigma} we have 
\begin{align}
    &\mu_{s,t}(x_0, x_t) = \vareps^2 r_{1,2}(s,t) + (1 - \vareps^2) r_{0,1}) x_{t} \\
    & \quad + \alpha_{s}(1 - \vareps^2 r_{2,2}(s,t) - (1 - \vareps^2) r_{1,1}(s,t)) x_0 ,
\\
& \Sigma_{s,t} =  \sigma_{s}^2 (1 - (\vareps^2 r_{1,1}(s,t) + (1-\vareps^2))^2) \Id .  \label{eq:mean_sigma_appendix}
\end{align}

The following result proves that the updates of DDIM \eqref{eq:mean_sigma_ddim_appendix} and the ones we present \eqref{eq:mean_sigma_appendix} are identical upon identification of one parameter. 

\begin{proposition}{}{identification_with_ddim}
Assume that $\eta_{s,t}^2 = \sigma_{s}^2 (1 - (\vareps^2 r_{1,1}(s,t) + (1-\vareps^2))^2)$. Then, we have that $\Sigma_{s,t} = \hat{\Sigma}_{s,t}$ and $\mu_{s,t}(x_0,x_t) = \hat{\mu}_{s,t}(x_0,x_t)$.
\end{proposition}

\begin{proof}
First, we have that 
\begin{align}
    \sqrt{\frac{1 - \alpha_s^2 - \eta_{s,t}^2}{1-\alpha_t^2}} &= \sqrt{\frac{1 - \alpha_s^2 - \sigma_{s}^2 (1 - (\vareps^2 r_{1,1}(s,t) + (1-\vareps^2))^2)}{1-\alpha_t^2}} \\
    &= \sqrt{\frac{1 - \alpha_s^2 - (1 - \alpha_s^2) (1 - (\vareps^2 r_{1,1}(s,t) + (1-\vareps^2))^2)}{1-\alpha_t^2}} \\
    &= \sqrt{r_{0,2}(s,t) (1 - (1 - (\vareps^2 r_{1,1}(s,t) + (1-\vareps^2))^2))} \\
    &= r_{0,1}(s,t)(\vareps^2 r_{1,1}(s,t) + (1-\vareps^2)) \\
    &= \vareps^2 r_{1,2}(s,t) + (1-\vareps^2) r_{0,1}(s,t) . 
\end{align}
Hence, we get that 
\begin{align}
    \hat{\mu}_{s,t}(x_0,x_t) &= \sqrt{\frac{1 - \alpha_s^2 - \eta_{s,t}^2}{1-\alpha_t^2}} x_t + \left[ \alpha_s - \alpha_t \sqrt{\frac{1 - \alpha_s^2 - \eta_{s,t}^2}{1-\alpha_t^2}} \right] x_0 \\
    &= \sqrt{\frac{1 - \alpha_s^2 - \eta_{s,t}^2}{1-\alpha_t^2}} x_t + \alpha_s \left[ 1 - r_{1,0}(s,t) \sqrt{\frac{1 - \alpha_s^2 - \eta_{s,t}^2}{1-\alpha_t^2}} \right] x_0 \\
    &= (\vareps^2 r_{1,2}(s,t) + (1-\vareps^2) r_{0,1}(s,t)) x_t + \alpha_s \left[ 1 - \vareps^2 r_{2,2}(s,t) - (1-\vareps^2) r_{1,1}(s,t)\right] x_0 = \mu_{s,t}(x_0,x_t) ,
\end{align}
which concludes the proof. 
\end{proof}

In our work, we call $\vareps$ the \emph{churn} parameter. In \citep[Equation (16)]{song2020denoisingimplicit}, we have that 
\begin{equation}
    \eta_{s,t}^2 = \eta^2 \frac{1 - \alpha_s^2}{1 - \alpha_t^2}\left( 1 - \frac{\alpha_t^2}{\alpha_s^2}\right)  = \eta^2 (r_{0,2}(s,t) - r_{2,2}(s,t)) .  
\end{equation}
In that case $\eta$ is another churn parameter. However, we can write the following relation between those two parameters.
In particular, in both cases we have that we recover if DDPM if $\vareps = 1$ (or equivalently if $\eta =1$) and DDIM if $\vareps = 0$ (or equivalently if $\eta = 0$). 

\begin{proposition}{}{churn_param_relation}
We have that 
\begin{equation}
    \eta^2 = \frac{\sigma_{s}^2 (1 - (\vareps^2 r_{1,1}(s,t) + (1-\vareps^2))^2)}{r_{0,2}(s,t) - r_{2,2}(s,t)} . 
\end{equation}
Similarly, we have that 
\begin{equation}
    \vareps^2 = \frac{1 - \left[ 1 - \eta^2 \frac{r_{0,2}(s,t) - r_{2,2}(s,t)}{\sigma_s^2}\right]^{1/2}}{1 - r_{1,1}(s,t)} . 
\end{equation}
Remarkably, we have that $\vareps =0$ if and only if $\eta = 0$ and $\vareps = 1$ if and only if $\eta = 1$.
\end{proposition}

\begin{proof}
Proving that  $\vareps =0$ if and only if $\eta = 0$ is straightforward and left to the reader. Now assume that 
$\vareps = 1$, we are going to prove that $\eta = 1$. 
\begin{align}
    \eta^2 &= \frac{\sigma_{s}^2 (1 - (\vareps^2 r_{1,1}(s,t) + (1-\vareps^2))^2)}{r_{0,2}(s,t) - r_{2,2}(s,t)} \\
    &= \frac{\sigma_s^2 - \sigma_s^2 r_{2,2}(s,t)}{r_{0,2}(s,t) - r_{2,2}(s,t)} \\
    &= \frac{\frac{(1-\alpha_s^2)(1-\alpha_t^2)\alpha_s^2 - (1-\alpha_s^2)^2\alpha_t^2}{\alpha_s^2 (1-\alpha_t^2)}}{r_{0,2}(s,t) - r_{2,2}(s,t)} \\
    &= \frac{\frac{(1 - \alpha_s^2)(\alpha_s^2 - \alpha_t^2)}{\alpha_s^2 (1-\alpha_t^2)}}{r_{0,2}(s,t) - r_{2,2}(s,t)} \\
    &= \frac{r_{0,2}(s,t) (1 - r_{2,0}(s,t)) }{r_{0,2}(s,t) - r_{2,2}(s,t)} = 1 .
\end{align}
Similarly, we get that $\eta = 1$ implies that $\vareps = 1$. 
\end{proof}


\section{Pseudo-code for loss function}
\label{sec:pseudocode_training}

\subsection{Some replication utils}

Given a batch of $[x^1, x^2, x^4, x^5]$, assuming that $n=4$ and $m=2$, this function will output (before the reshape operation)

\begin{equation}
    \hat{x} = \left( \begin{matrix} 
    x^1 & x^1 \\
    x^2 & x^2 \\
    x^3 & x^3 \\
    x^4 & x^4 
    \end{matrix}\right) .
\end{equation}


\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[language=Python]
def replicate_fn(n: int, m: int, x: chex.Array) -> chex.Array:
  batch_size, data_shape = x.shape[0], x.shape[1:]
  x = x[:n]
  x = jnp.reshape(x, (n, 1, *data_shape))
  x = jnp.tile(x, (1, m) + (1,) * len(data_shape))
  x = jnp.reshape(x, (n * m, *data_shape))
  return x
\end{lstlisting}
\end{minipage}


We also assume that we are provided a function \texttt{split\_fn} such that given $x$ with shape $(nm, ...)$ the output of \texttt{split\_fn} has shape $(n, m, ...)$.

\subsection{Loss function}

We assume that we are given two functions \texttt{compute\_rho\_fn} and \texttt{compute\_rho\_diagonal\_fn} such that given $x$ and $y$ with shape $(n, m, ...)$
the output of \texttt{compute\_rho\_diagonal\_fn} is $\tfrac{1}{n}\sum_{i=1}^n \tfrac{1}{m} \sum_{j=1}^m \rho(x_{i,j}, y_{i,j})$ and the output of 
\texttt{compute\_rho\_fn} is $\tfrac{1}{n}\sum_{i=1}^n \tfrac{1}{m(m-1)} \sum_{j, j'=1, j \neq j'}^m \rho(x_{i,j}, y_{i,j'})$. Function $\rho$ is consistent with the notation defined in~\Cref{sec:proper_scoring_rules}.

In addition, we assume that we have access to a function \texttt{add\_noise\_fn} such that given $t$ of shape $(n,)$ and $x_0$ of shape $(n, ...)$ it outputs $x_t = \alpha_t x_0 + \sigma_t z$ where $z$ has the same shape as $x$ and has independent Gaussian entries $\mathcal{N}(0, 1)$. 

Finally, we assume that we have access to a function \texttt{apply\_fn} such that given $t$ of shape $(n,)$ and $x_t$ of shape $(n, ...)$ it outputs $\hat{x}_\theta(t, x_t)$. 

\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[language=Python]
def loss_fn(t: jnp.Array, x0: jnp.Array) -> jnp.Array:

  # add noise and replicate
  xt = add_noise_fn(key=key, t=t, x0=x0)
  key, _ = jax.random.split(key)
  x0_population = replicate_fn(n=n, m=m, x=x0)
  t_population = replicate_fn(n=n, m=m, x=t)
  xt_population = replicate_fn(n=n, m=m, x=xt)
  
  # compute prediction
  eps_population = jax.random.normal(key=key, shape=xt_population.shape)
  key, _ = jax.random.split(key)
  output_population = apply_fn(t=t_population, xt=xt_population, eps=eps_population)
  
  # split the populations
  x0_population = split_fn(n=n, m=m, x=x0_population)
  output_population = split_fn(n=n, m=m, x=output_population)

  # compute confinement term 
  confinement = compute_rho_diagonal_fn(x=x0_population, y=output_population)

  # compute interaction term for predictions
  interaction_prediction = compute_rho_fn(x=output_population, y=output_population)
  
  # Generalized kernel score
  score = 0.5 * lbda * interaction_prediction - confinement
  
  # Our aim to maximize the score, so the loss is negative score
  loss = -score
  
  return loss
  
\end{lstlisting}
\end{minipage}

In this function, we have focused on the $x_0$-prediction and have omitted the weighting for simplicity. 


\section{Extended related work}
\label{sec:extended_related}

\paragraph{Diffusion and GANs.} In \citep{xiao2021tackling}, the authors propose to replace the conditional mean estimator of diffusion models with a GAN. This allows to model bigger steps in the denoising process. Recall that, seeing diffusion models as hierarchical VAEs, the ELBO is given by 
\begin{equation}
    \log p_\theta(x) \geq - \sum_{k=1}^N \KL(p(x_{t_{k-1}}|x_{t_k}) | p_\theta(x_{t_{k-1}}|x_{t_k})) + C, 
\end{equation}
where $C$ is a constant. In \citep{xiao2021tackling}, the relative entropy $\KL(q(x_{t-1}|x_t) | p_\theta(x_{t-1}|x_t))$ is replaced by an adversarial loss. Namely, the authors consider 
\begin{equation}
    \mathcal{L}(\theta, \phi) = \sum_{k=1}^N \left(\mathbb{E}_{p(x_{t_{k-1}}|x_{t_k})}[-\log (D_\phi(t, x_{t_{k-1}}, x_{t_k)})] + \mathbb{E}_{p_\theta(x_{t_{k-1}}|x_{t_k})}[-\log (1 - D_\phi(t, x_{t_{k-1}}, x_{t_k)})]\right)  .
\end{equation}
In that case, $D_\phi$ is a \emph{discriminator} and $p_\theta$ is a \emph{generator}. The main differences with our setting is that in our case, we do not train a discriminator. In our setting the discriminative parameters are only given by the kernel parameters, i.e., in the case of the energy distance, the parameters are given by $\lambda$ and $\beta$ in \eqref{eq:energyloss} and are \emph{fixed}. In this regards our parameterization is much more lightweight. In addition, while the transition $p_\theta(x_{t_{k-1}}|x_{t_k})$ is estimated in a similar way as in our paper, i.e., 
\begin{equation}
    p_\theta(x_{t_{k-1}}|x_{t_k}) = \int_{\rset^d} p_\theta(x_0|x_{t_k}) p(x_{t_{k-1}} | x_0, x_{t_k}) \rmd x_0 ,
\end{equation}
the discriminator is not applied on the $x_0$ variable conditionally to $x_{t_k}$ but on the couple $(x_{t_k}, x_{t_{k-1}})$. This is similar to the differences we highlighted in \Cref{sec:sample_efficiency} when comparing joint loss and conditional loss. Note that our approach could be extended to consider kernels not on $p_{0|s}$ but instead leveraging the characterisation

\begin{equation}
    p(x_s|x_t) = \int_{\rset^d} p(x_u|x_t) p(x_s|x_u,x_t) \rmd x_u ,
\end{equation}
for $u \leq \min(s, t)$. In that case, choosing $u = 0$ is simply a special case.
%Another key difference is that $p_\theta$ models the transitions $t_{k-1}$ to $t_k$ whereas we model the transition $t$ to $0$. MIGHT CHANGE. 

\paragraph{Modeling the covariance. } The importance of modeling $p_{0|t}$ in the low step regime has been highlighted in numerous papers, see \citep{ho2020denoising,nichol2021improved,bao2022estimating,bao2022analytic,ou2024diffusion}. A number of approaches aim at learning/approximating the covariance of $p_{s|t}$ or $p_{0|t}$, see \citep{ho2020denoising,nichol2021improved,bao2022estimating,bao2022analytic,ou2024diffusion,rozet2024learning}. Most of the time in order to obtain a tractable approximation of this covariance they consider diagonal approximations leveraging the Hutchinson trace estimator \cite{hutchinson1989stochastic}. In contrast, we do not assume a Gaussian form for $p_{0|t}$ or $p_{s|t}$. Doing so we 
\begin{enumerate*}[label=(\roman*)]
    \item  model more complex distributions than Gaussian transitions
    \item avoid the additional complexity of having to model a covariance matrix.
 \end{enumerate*}
 


\section{Experimental details}
\label{app_sec:experimental_details}

\subsection{2D experiments}
\label{app_sec:2d_experimental_details}

We consider a target distribution $p_0$ given by a mixture of two Gaussians, $p_0=0.5\mathcal{N}(\mu_1, \sigma^2 \Id) + 0.5\mathcal{N}(\mu_2, \sigma^2 \Id)$, where $\mu_1=(3,3)$, $\mu_2=(-3,3)$ and $\sigma=0.5$. We create a dataset from this distribution by sampling $102400$ points.

We train diffusion model and \emph{distributional} diffusion models for $100$k steps with batch size $128$ with learning rate $1e-3$ using Adam optimizer with a cosine warmup for first $100$ iterations. We use $b_1 = 0.9$, $b_2=0.999$, $\epsilon=1e-8$ in Adam optimizer. On top of that, we clip the updates by their global norm (with the maximal norm being $1$). We use EMA decay of $0.99$. We use the \emph{flow matching} noise schedule~\eqref{eq:flow_matching_noise_schedule} and we use safety epsilon $1e-2$. When training the model, we use sigmoid loss weighting as in~\citep{kingma2021variational}. The time is encoded via sinusoidal embedding into the dimension $2048$ followed by $2$ layer MLP where hidden dimension is $2048$ and the output dimension is $2048$, using \texttt{gelu} activation. As a backbone architecture, we use a $9$-layers MLP with \texttt{gelu} activation and with hidden dimension of $64$. The MLP is applied as follows. First, we apply $4$ preprocessing MLP layers to the embedding of time $t$ and separately to the $x_t$. After that, we concatenate these two on the last dimension and pass through $4$ MLP layers which is then followed by one output MLP layer of dimension $2$.  When we use the \emph{distributional} diffusion model, the noise $\xi$ has the same dimensionality as $x_t$ and we concatenate the two along the last dimension. After passing it through the MLP, which produces a vector of dimension $4$, we ignore first $2$ dimension to get the output $\hat{x}_{\theta}(t,x_{t},\xi)$ of dimension 2. We use the population size $m=32$ (see Algorithm~\ref{alg:training_diffusion}) and we use $\beta=0.1,\lambda=1$ for \emph{distributional} model and $\beta=2,\lambda=0$ for diffusion-like variant.

For evaluation, we sample $4096$ samples $X^i_0 \sim p_0$ as well as samples $X^i_t | X^i_0$ from the forward process~\eqref{eq:flow_matching_noise_schedule}. For Figure~\ref{fig:snr_study_fig}, right, we use the MMD squared given by $D_\rho$~\eqref{eq:divergence} with $\rho(x,x')=-k_{\rbf}(x,x')$ (see~\eqref{eq:rbf_k}) for $\sigma=1$.

For each $X^i_t$, we produce $8$ samples $\xi \sim \mathcal{N}(\xi, 0, \Id)$ and we compute standard deviation of $\hat{x}_{\theta}(t,x^i_{t},\xi)$ over $\xi$. Then, we average the standard deviation over all the $x^i_{t}$. On top of that, since we know the posterior $p(X_0|X_t)$, which is given by
\begin{align}
    p(x_0 | x_t) &= w_1 \mathcal{N}(x_0;\nu_1, \Sigma) + w_2 \mathcal{N}(x_0;\nu_2, \Sigma), \\
    w_{k} & \propto \mathcal{N}(\alpha_t\mu_k, (\alpha^2_t \sigma^2 + \sigma^2_t) \Id), \\
    \nu_k &= \Sigma \left( \frac{\alpha_t}{\sigma^2_t} X_t + \frac{1}{\sigma^2} \mu_k \right), \\
    \Sigma &= \left( \frac{\alpha^2_t}{\sigma^2_t} + \frac{1}{\sigma^2} \right)^{-1} \Id,
\end{align}
where $\mu_k$ with $k=1,2$ are the means of $p_0$ and $\sigma^2$ is the variance. The values of $\alpha_t$ and $\sigma_t$ are given by the schedule~\eqref{eq:flow_matching_noise_schedule}. We produce $8$ samples $X_0 \sim p(\cdot|X^i_t)$ and compute their standard deviation and then average over $X^i_t$. We report this metric in Figure~\ref{fig:2d_metrics}, right. In order to produce Figure~\ref{fig:trajectories_2d}, we fix $x_0$ to be equal to either $\mu_1$ or to $\mu_2$. We then produce $X_t | x_0$ for each $t$ (one sample for each $t$ and $x_0$). After that, we visualize $\hat{x}_{\theta}(t,X^i_{t},\xi)$ for $n=4096$ samples of  $\xi \sim \mathcal{N}(0, \Id)$.

\subsection{Image space experiments}
\label{app_sec:image_experimental_details}

\paragraph{Architecture.}
In our image space experiments we consider a $U$-net architecture \citep{ronneberger2015u}. We consider a channel of size $256$ and apply the following multiplier for the channel at each level of the $U$-net, $(1,2,2,2)$. At each level we consider $2$ residual blocks. We apply an attention layer at the second level of the $U$-net (resolution 16 in the case of CIFAR-10 and resolution 32 in the case of CelebA and LSUN Bedrooms). We use an attention block in the bottleneck block of the $U$-net. Each residual block consists of normalizing the input, applying a $3\times3$ convolution block and applying a non-linearity. This is then followed by a dropout layer and a $3\times3$ convolution block. Finally we add the input (processed through a $1\times1$ convolutional block) of the residual block to this output (this is the residual connection). We use the $\mathrm{RMSnorm}$ for the normalization layer and $\mathrm{GELU}$ for the non-linearity. When using an attention layer, this is done after the convolutional residual block just described. We use a multi-head attention mechanism with $8$ heads. We process the time with a sinusoidal embedding followed by a MLP layer. In the case of CelebA, we use a linear embedding layer to process the conditioning vector of shape $(40,)$ while in the case of CIFAR10 we consider an embedding matrix to process the class information. Once the time conditioning and the (optional) other conditioning have been obtained we sum them. The conditioning is used in the model by replacing the $\mathrm{RMSnorm}$  with an adaptive $\mathrm{RMSnorm}$ layer, i.e., the scale and bias of the normalization layers are obtained by passing the conditioning vector through a MLP layer. Similarly to the two dimensional setting, we concatenate $x_t$ of shape $(b, h, w, c)$ and $\xi $ of shape $(b, h, w, c)$ along the last dimension in order to get an input of shape of shape $(b, h, w, 2c)$. The output of the $U$-net is of shape of shape $(b, h, w, 2c)$ and we drop half of the channels to get a sample of shape $(b, h, w, c)$. 


For latent CelebA-HQ, we follow the recipe of \citep{rombach2022highresolutionimagesynthesislatent} to train the autoencoder. We train the autoencoder with a similar architecture as the autoencoder used in LDM-4 (see~\citep{rombach2022highresolutionimagesynthesislatent}). It encodes CelebA-HQ of shape $(256,256,3)$ into latent code of shape $(64,64,3)$. Then, for latent diffusion model and \emph{distributional} latent model, we use a similar architecture as LDM-4 except that we concatenate the latent $x_{t}$ and $\xi$ over the last dimension. More precisely, we consider a channel of size $256$ and apply the following multiplier for the channel at each level of the $U$-net, $(1,2,2,2)$. At each level we consider $2$ residual blocks. We apply an attention layer at the foruth level of the $U$-net. The structure of the residual block is identical as in image space. Similarly, we use the same type of normalization layer and non-linearity. We use a multi-head attention mechanism with $8$ heads.

\paragraph{Training and evaluation details.}
We train all the pixel-space models for $1e6$ steps with batch size $256$ for CIFAR-10, batch size of $64$ for CelebA, LSUN Bedrooms. The latent space model consists of two-stage training regime, where we first train latent autoencoder for $2e6$ steps with batch size $16$. It is then followed by training a latent diffusion model for $2e6$ steps with batch size $16$.

For all the experiments, we use the Adam optimizer with additional norm clipping (maximal norm equal to 1) with the learning rate equal to $1e-4$ and an additional cosine warmup for $100$ first iterations.

Our autoencoder on CelebA-HQ is trained with $0.1$ coefficient on adversarial loss, $100$ coefficient on generator loss and $100$ coefficient on codebook, from the  VQ-VAE~\citep{oord2018neuraldiscreterepresentationlearning} loss. We use a $\beta$-VAE with $\beta = 1e-6$.

For all the models, we sweep over weighting $w_t$ (see~\eqref{eq:diffusionloss},~\eqref{eq:energyloss}), where we either do not use any weighting or we use sigmoid weighting, see~\citep{kingma2021variational},  weighting with bias parameters $\{-2, -1, 0, 1, 2\}$. When we train classical diffusion model, we always use velocity as a prediction target, i.e., the target is $X_0 - X_1$. When we train \emph{distributional} model, we use $X_0$ as prediction target. For \emph{distributional} models, we additionally sweep over $\lambda \in \{0, 0.1, 0.5, 1.0\}$, $\beta \in \{0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 1.5, 1.99, 2.0\}$. When we use kernel diffusion losses, see
~\Cref{sec:proper_scoring_rules} and~\Cref{sec:method}, for (\imq) kernel~\eqref{eq:imq_k}, we sweep over $c \in \{0.01, 0.1, 0.3, 0.5, 1.0, 1.5, 2.0, 10.0, 50.0, 100.0\}$. For for~(\rbf) kernel~\eqref{eq:rbf_k} and for~(\expk) kernel~\eqref{eq:exp_k}, we firstly compute median $m^2 = ||x-x'||^2_2$ for~(\rbf) kernel and $m = ||x-x'||_2$ for~(\expk) kernel, on a subset of $100$ batches of size $512$ from the dataset. Then, for~(\rbf) kernel, we define $\sigma^2 = m^2 \gamma$ and we sweep over $\gamma \in \{0.01, 0.1, 0.3, 0.5, 1.0, 1.5, 2.0, 10.0\}$. For~($\exp$) kernel, we define $\sigma = m \gamma$ and we sweep over $\gamma \in \{ 0.01, 0.1, 0.3, 0.5, 1.0, 1.5, 2.0, 10.0 \}$. Since we only use kernel diffusion losses on CIFAR-10, we only report the value of the medians in this case. We have that $m^2 = 1357.8127$ and $m = 36.9$.

In order to produce samples, we follow \Cref{alg:sampling_diffusion} for \emph{distributional} models and \texttt{SDE} sampler for classical diffusion models. To evaluate performance, we use FID~\citep{heusel2017gans} metric. As we train the models, every $10000$ steps we produce samples with $10$ denoising steps for \emph{distributional} models and with $100$ denoising steps for classical diffusion model. We then evaluate the performance of the models by computing FID on a subset of $2048$ datapoints from the original dataset. Then, we use this metric in order to select the best hyperparameters and the best checkpoints for each best hyperparameter. After training is done, for every combination of $\lambda, \beta$ (or $c$, $\sigma$, $\sigma^2$ in case of other kernels), we select the best checkpoint based on their FID on this small subset of data. The corresponding checkpoints are then used for evaluation when sampling with $10,30,50,70,90,100$ denoising steps. To compute the final FIDs, we use $50000$ samples for CIFAR-10, LSUN Bedrooms and CelebA. We use $30000$ samples for latent CelebA-HQ.

\subsection{Robotics experiments}
\label{app_sec:robotics_experimental_details}
We used the Libero benchmark \cite{LIBERO}, a life-long learning benchmark that consists of 130 language-conditioned robotic manipulation tasks. There are 5 suites in the Libero benchmark, Libero-Spatial, Libero-Object, Libero-Goal, Libero-Long and Libero-90. Each suite contains 10 tasks, except Libero-90 which has 90 tasks. In our experiments we focus on Libero-Long, the most challenging suite with 10 tasks that features long-horizon tasks with diverse object interactions.
We train language-conditioned multitask diffusion policies for the 10 tasks of Libero10 suite. and evaluate the success rates of the policies in simulation. We encode the visual observations using a ResNet \cite{ResnetHe2015-ak}, and encode the language instruction using a pretrained and frozen Bert \cite{BERTDevlin2018-fn} encoder. Our multitask diffusion policy does action chunking \cite{DiffpolicyChi2023-gp}, and predicts and executes a chunk of 8 7-dimensional actions conditioned on the language instruction, the current visual observations and proprioceptive states. For the denoiser, we follow Aloha unleashed \cite{AlohaZhao2024-bc} and use a cross-attention based transformer denoiser.

We follow the original Libero paper for training and evaluating our multitask diffusion policy. As a loss function for training the diffusion policy, we use the Mean Absolute Error(MAE), i.e., $\ell_1$ loss. See~\Cref{app_sec:additional_robotics} for results with Mean Squared Error (MSE). Libero10 suite has 138090 steps of training data and we use a batch size of 32 to train our policies for 250k steps (roughly 50 epochs). Evaluation is on simulated environment for 50 episodes per task following the Libero benchmark. We evaluate our policy every 25k steps (roughly 5 epochs). We take the median success rate of the policy across all tasks in the suite and repeat the experiment over 3 seeds. We report the median success rates over all tasks and best checkpoint during training.

\textbf{Energy distance diffusion model.} For the energy distance diffusion variant, we concatenated 2 noise dimensions, sampled from $\mathcal{N}(0; \Id)$ to the action dimension of the chunks. For training we used population size $m=16$ per data sample, and computed the loss with varying $\beta$s and $\lambda$s. For evaluation, we use a single sample and execute the predicted action chunk from the diffusion policy using a varying number of diffusion steps $(2, 16, 50)$.

\section{Additional experiments}
\label{app_sec:additional_experiments}

\subsection{Additional image experiments}
\label{app_sec:image_additional_results}

In this section, we provide additional results for image space experiments.

\paragraph{Impact of parameters $\beta$ and $\lambda$ on FID.} We report detailed performance of \emph{distributional} models $\hat{x}_{\theta}$ based on chosen $\beta$ and $\lambda$ for different number of diffusion steps. We present results for conditional image generation on CIFAR-10 in Figure~\ref{fig:heatmap_energy_cifar10} and on CelebA in Figure~\ref{fig:heatmap_energy_celeba}. The unconditional image generation for LSUN Bedrooms are given in Figure~\ref{fig:heatmap_lsun} and for latent CelebA-HQ in Figure~\ref{fig:heatmap_latent_celeba}. In practice, we noticed that performance of models for some $\lambda$ and $\beta$ led to very high FIDs. In order to better visually inspect the results, we mask the corresponding blocks if FID reaches a certain threshold. We used following thresholds: $9.8$ for CIFAR-10, $20$ for  CelebA and LSUN Bedrooms, $300$ for latent CelebA HQ. The results on CIFAR-10 and CelebA suggest that for a small number of steps, the best values of parameters are located in the bottom right corner, i.e. $\lambda$ close to $1.0$ and $\beta$ small. As we increase the number of steps,  better values appear in the top left corner, i.e., $\lambda$ close or equal to $0$ and $\beta \approx 2$. In case of unconditional generation, we noticed generally that using the minimal possible $\beta$ led to the best results, as can be seen from \Cref{fig:heatmap_lsun} and \Cref{fig:heatmap_latent_celeba}, where the bottom row leads to the best performance.

\begin{figure}
    \centering
        \includegraphics[width=.95\linewidth]{figures/heatmap_energy_cifar10.pdf}
    \caption{Heatmap of FIDs for conditional image generation on CIFAR-10. $x$-axis corresponds to $\lambda$, $y$-axis corresponds to $\beta$, the color corresponds to FID going from minimal (blue) to maximal (red) values. Different columns denote number of diffusion steps. Black squares correspond to the cases where performance reaches FID higher than specified threshold.}
    \label{fig:heatmap_energy_cifar10}
\end{figure}

\begin{figure}
    \centering
        \includegraphics[width=.95\linewidth]{figures/heatmap_energy_celeba.pdf}
    \caption{Heatmap of FIDs for conditional image generation on CelebA. $x$-axis corresponds to $\lambda$, $y$-axis corresponds to $\beta$, the color corresponds to FID going from minimal (blue) to maximal (red) values. Different columns denote number of diffusion steps. Black squares correspond to the cases where performance reaches FID higher than specified threshold.}
    \label{fig:heatmap_energy_celeba}
\end{figure}

\begin{figure}
    \centering
        \includegraphics[width=.95\linewidth]{figures/heatmap_lsun.pdf}
    \caption{Heatmap of FIDs for unconditional image generation on LSUN Bedrooms. $x$-axis corresponds to $\lambda$, $y$-axis corresponds to $\beta$, the color corresponds to FID going from minimal (blue) to maximal (red) values. Different columns denote number of diffusion steps. Black squares correspond to the cases where performance reaches FID higher than specified threshold.}
    \label{fig:heatmap_lsun}
\end{figure}

\begin{figure}
    \centering
        \includegraphics[width=.95\linewidth]{figures/heatmap_latent_celeba.pdf}
    \caption{Heatmap of FIDs for unconditional image generation on Latent CelebA-HQ. $x$-axis corresponds to $\lambda$, $y$-axis corresponds to $\beta$, the color corresponds to FID going from minimal (blue) to maximal (red) values. Different columns denote number of diffusion steps. Black squares correspond to the cases where performance reaches FID higher than specified threshold.}
    \label{fig:heatmap_latent_celeba}
\end{figure}




\paragraph{Detailed results with different kernels.} We present results on conditional image generation on CIFAR-10 with different kernels. First, in Figure~\ref{fig:kernel_results_best_params}, we show the value of the $\lambda$ for every kernel which achieves the lowest FID for the given number of diffusion steps. Overall, we observe that $\lambda$ follows a downward trend as we increase the number of diffusion steps, which is consistent with out experiments in the main paper. Furthermore, we show the corresponding value of the kernel parameter which achieves the lowest FID for given number of diffusion steps. Below, we also show the detailed figures of FID for every $\lambda$ and kernel parameter combination.
\begin{figure}
    \centering
        \includegraphics[width=.95\linewidth]{figures/kernel_results_best_params.pdf}
    \caption{Best parameters $\lambda$ and corresponding kernel parameters for each kernel, achieving the lowest FID for the given number of diffusion steps.}
    \label{fig:kernel_results_best_params}
\end{figure}

On top of that, we compare performance of different kernels for a fixed value of $\lambda$. We also report the performance of \emph{energy} kernel, which is defined by $\rho(x,x')=\|x-x'\|^{\beta}$, see also~\Cref{sec:proper_scoring_rules}. The results are given in Figure~\ref{fig:kernel_performance_comparison}. We see that overall using different (to \emph{energy}) kernels leads to similar results overall.

\begin{figure}
    \centering
        \includegraphics[width=.95\linewidth]{figures/kernel_performance_comparison.pdf}
    \caption{Performance of different kernels on conditional image generation on CIFAR-10, for different values of $\lambda$ (different columns). The kernel parameters are chosen to minimize FID for the given $\lambda$ condition. We also report performance of the diffusion model (red) and show in the black dashed line the performance of diffusion model with $100$ steps.}
    \label{fig:kernel_performance_comparison}
\end{figure}


Finally, we present detailed results of FIDs for different kernels with different parameters $\lambda$ and corresponding kernel parameters. We report the corresponding heatmaps. Whenever the value of FID is larger than a threshold, we put black rectangles. For Inverse MQ, we use threshold $13.3$. For RBF kernel, we use threshold $10$. For Exponential kernel, we use threshold $10$. For Inverse MQ, the results are given in Figure~\ref{fig:heatmap_inverse_mq}. We observe that overall the best FIDs are achieved for larger values of parameter $c$ for all three kernels we investigate. Moreover, when number of diffusion steps increases, the region with the best FIDs moves from $\lambda \approx 0$ to $\lambda \approx 1$. The results for RBF kernel are given in Figure~\ref{fig:heatmap_gaussian}. We observe that in case of small number of diffusion steps, the region of the best values is around $\lambda = 1$ and smaller values of $\sigma^2$. As we increase the number of diffusion steps, we generally observe that many values of $\lambda$ and $\sigma^2$ yield similar reasonable results. Results for Exponential kernel are given in Figure~\ref{fig:heatmap_gaussian_abs}. When the number of steps is low, the best values are achieved for $\lambda$ close to $1$. As we increase the value of $\lambda$, the best region moves closer to smaller values of $\lambda$. This is coherent with our theoretical and methodological motivation, i.e., $\lambda \approx 1$ is beneficial in the low number of diffusion steps regime, and confirms our experiments when using the energy kernel $\rho(x,x') = -\|x -x'\|^\beta$, see \Cref{fig:heatmap_energy_cifar10}. 

\begin{figure}
    \centering
        \includegraphics[width=.95\linewidth]{figures/heatmap_inverse_mq.pdf}
    \caption{Heatmap of FIDs for conditional image generation on CIFAR-10 for Inverse MQ~\eqref{eq:imq_k} kernel. $x$-axis corresponds to $\lambda$, $y$-axis corresponds to $c$, the color corresponds to FID going from minimal (blue) to maximal (red) values. Different columns denote number of diffusion steps. Black squares correspond to the cases where performance reaches FID higher than specified threshold.}
    \label{fig:heatmap_inverse_mq}
\end{figure}



\begin{figure}
    \centering
        \includegraphics[width=.95\linewidth]{figures/heatmap_gaussian.pdf}
    \caption{Heatmap of FIDs for conditional image generation on CIFAR-10 for RBF~\eqref{eq:rbf_k} kernel. $x$-axis corresponds to $\lambda$, $y$-axis corresponds to $\sigma^2$, the color corresponds to FID going from minimal (blue) to maximal (red) values. Different columns denote number of diffusion steps. Black squares correspond to the cases where performance reaches FID higher than specified threshold.}
    \label{fig:heatmap_gaussian}
\end{figure}

\begin{figure}
    \centering
        \includegraphics[width=.95\linewidth]{figures/heatmap_gaussian_abs.pdf}
    \caption{Heatmap of FIDs for conditional image generation on CIFAR-10 for Exponential~\eqref{eq:exp_k} kernel. $x$-axis corresponds to $\lambda$, $y$-axis corresponds to $\sigma$, the color corresponds to FID going from minimal (blue) to maximal (red) values. Different columns denote number of diffusion steps. Black squares correspond to the cases where performance reaches FID higher than specified threshold.}
    \label{fig:heatmap_gaussian_abs}
\end{figure}

\subsection{Additional robotics experiments}
\label{app_sec:additional_robotics}

In the setting of \Cref{sec:robotics_experiments}, we only consider the case $\beta = 1$ and compared our results for several values of $\lambda \in [0,1]$, namely $\lambda \in \{0.0, 0.1, 0.5\}$. 
In \Cref{fig:libero10_results_beta2}, we present a similar study as \Cref{fig:libero10_results} but in the case where $\beta = 2$. Note that the diffusion baseline is not the same in \Cref{fig:libero10_results_beta2} and in \Cref{fig:libero10_results}. In \Cref{fig:libero10_results}, the diffusion baseline is trained with the Mean Absolute Error (MAE) regression loss, i.e., we replace the $\ell_2^2$ loss by the $\ell_1$ loss in the diffusion model loss. However, in \Cref{fig:libero10_results_beta2}, the diffusion model baseline is trained with the classical MSE regression loss. 

\begin{figure}
    \centering
        \includegraphics[width=.4\linewidth]{figures/libero10_results_beta2.pdf}
        \hfill
        \includegraphics[width=.4\linewidth]{figures/libero10_results_beta1.pdf}
    \caption{\textbf{Left}. Performance in Libero10 as function of diffusion steps and $\lambda$, same as \Cref{fig:libero10_results} but for $\beta=2$. \textbf{Right}. we reproduce \Cref{fig:libero10_results} for ease of read.}
    \label{fig:libero10_results_beta2}
\end{figure}


\subsection{Additional samples}

For CIFAR-10, samples from the \emph{distributional} model $\hat{x}_{\theta}$ trained with $\beta=0.1,\lambda=1$ and with $10$ sampling steps are shown in Figure~\ref{fig:cifar10_10_steps_samples}, left. Samples from the one trained with $\beta=2,\lambda=0$ and with $100$ sampling steps are shown in Figure~\ref{fig:cifar10_100_steps_samples}, leeft. The samples from the diffusion model with $10$ steps are shown in Figure~\ref{fig:cifar10_10_steps_samples}, right and with $100$ steps are shown in Figure~\ref{fig:cifar10_100_steps_samples}, right.

For CelebA, samples from the \emph{distributional} model trained with $\beta=0.001,\lambda=1$ and with $10$ sampling steps, are shown in Figure~\ref{fig:celeba_10_steps_samples}, left. Samples from the one trained with $\beta=2,\lambda=0$ and with $100$ sampling steps, are shown in Figure~\ref{fig:celeba_100_steps_samples}, left. Samples from diffusion model with $10$ steps are shown in Figure~\ref{fig:celeba_10_steps_samples}, right and with $100$ steps are shown in Figure~\ref{fig:celeba_100_steps_samples}, right.

For LSUN Bedrooms, samples from the \emph{distributional} model trained with $\beta=0.0001,\lambda=1$ and with $10$ sampling steps, are shown in Figure~\ref{fig:lsun_10_steps}, left. Samples from the one trained with $\beta=0.0001,\lambda=0$ and with $100$ sampling steps, are shown in Figure~\ref{fig:lsun_100_steps}, left. Samples from the diffusion model with $10$ steps are shown in Figure~\ref{fig:lsun_10_steps}, right and with $100$ steps are shown in Figure~\ref{fig:lsun_100_steps}, right.

For latent CelebA HQ, samples from the \emph{distributional} model trained with $\beta=0.01,\lambda=0.5$ and with $10$ sampling steps, are shown in Figure~\ref{fig:latente_celeba_hq_10_steps}, left. The samples from the one trained with $\beta=0.01,\lambda=0.1$ and with $100$ sampling steps, are shown in Figure~\ref{fig:latente_celeba_hq_100_steps}, left. Samples from the diffusion model with $10$ steps are shown in Figure~\ref{fig:latente_celeba_hq_10_steps}, right and with $100$ steps are shown in Figure~\ref{fig:latente_celeba_hq_100_steps}, right.

\begin{figure}
    \centering
    \begin{subfigure}
    \centering
        \includegraphics[trim=0 138 138 0, clip, 
        width=.45\linewidth]{figures/cifar10_engression_10_steps.png}
    \end{subfigure}
    \hfil
    \begin{subfigure}
    \centering
        \includegraphics[trim=0 138 138 0, clip,  width=.45\linewidth]{figures/cifar10_diffusion_10_steps.png}
    \end{subfigure}
    \caption{\textbf{Left}, samples from \emph{distributional} model trained on CIFAR-10 and sampled with $10$ steps. $\beta=0.1,\lambda=1$. \textbf{Right}, samples from diffusion model trained on CIFAR-10 and sampled with $10$ steps.}
    \label{fig:cifar10_10_steps_samples}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}
    \centering
        \includegraphics[trim=0 138 138 0, clip, 
        width=.45\linewidth]{figures/cifar10_engression_100_steps.png}
    \end{subfigure}
    \hfil
    \begin{subfigure}
    \centering
        \includegraphics[trim=0 138 138 0, clip,  width=.45\linewidth]{figures/cifar10_diffusion_100_steps.png}
    \end{subfigure}
    \caption{\textbf{Left}, samples from \emph{distributional} model trained on CIFAR-10 and sampled with $100$ steps. $\beta=2,\lambda=0$. \textbf{Right}, samples from diffusion model trained on CIFAR-10 and sampled with $100$ steps.}
    \label{fig:cifar10_100_steps_samples}
\end{figure}




\begin{figure}
    \centering
    \begin{subfigure}
    \centering
        \includegraphics[trim=0 138 138 0, clip, 
        width=.45\linewidth]{figures/celeba_engression_10_steps.png}
    \end{subfigure}
    \hfil
    \begin{subfigure}
    \centering
        \includegraphics[trim=0 138 138 0, clip,  width=.45\linewidth]{figures/celeba_diffusion_10_steps.png}
    \end{subfigure}
    \caption{\textbf{Left}, samples from \emph{distributional} model trained on CelebA and sampled with $10$ steps. $\beta=0.001,\lambda=1$. \textbf{Right}, samples from diffusion model trained on CelebA and sampled with $10$ steps.}
    \label{fig:celeba_10_steps_samples}
\end{figure}


\begin{figure}
    \centering
    \begin{subfigure}
    \centering
        \includegraphics[trim=0 138 138 0, clip, 
        width=.45\linewidth]{figures/celeba_engression_100_steps.png}
    \end{subfigure}
    \hfil
    \begin{subfigure}
    \centering
        \includegraphics[trim=0 138 138 0, clip,  width=.45\linewidth]{figures/celeba_diffusion_100_steps.png}
    \end{subfigure}
    \caption{\textbf{Left}, samples from \emph{distributional} model trained on CelebA and sampled with $100$ steps. $\beta=2,\lambda=1$. \textbf{Right}, samples from diffusion model trained on CelebA and sampled with $100$ steps.}
    \label{fig:celeba_100_steps_samples}
\end{figure}


\begin{figure}
    \centering
    \begin{subfigure}
    \centering
        \includegraphics[trim=0 138 138 0, clip, 
        width=.45\linewidth]{figures/lsun_engression_10_steps.png}
    \end{subfigure}
    \hfil
    \begin{subfigure}
    \centering
        \includegraphics[trim=0 138 138 0, clip,  width=.45\linewidth]{figures/lsun_diffusion_10_steps.png}
    \end{subfigure}
    \caption{\textbf{Left}, samples from \emph{distributional} model trained on LSUN Bedrooms and sampled with $10$ steps. $\beta=0.0001,\lambda=1$. \textbf{Right}, samples from diffusion model trained on LSUN Bedrooms and sampled with $10$ steps}
    \label{fig:lsun_10_steps}
\end{figure}


\begin{figure}
    \centering
    \begin{subfigure}
    \centering
        \includegraphics[trim=0 138 138 0, clip, 
        width=.45\linewidth]{figures/lsun_engression_100_steps.png}
    \end{subfigure}
    \hfil
    \begin{subfigure}
    \centering
        \includegraphics[trim=0 138 138 0, clip,  width=.45\linewidth]{figures/lsun_diffusion_100_steps.png}
    \end{subfigure}
    \caption{\textbf{Left}, samples from \emph{distributional} model trained on LSUN Bedrooms and sampled with $100$ steps. $\beta=0.0001,\lambda=0$. \textbf{Right}, samples from diffusion model trained on LSUN Bedrooms and sampled with $100$ steps}
    \label{fig:lsun_100_steps}
\end{figure}




\begin{figure}
    \centering
    \begin{subfigure}
    \centering
        \includegraphics[trim=0 138 138 0, clip, 
        width=.45\linewidth]{figures/latent_celeba_hq_engression_10_steps.png}
    \end{subfigure}
    \hfil
    \begin{subfigure}
    \centering
        \includegraphics[trim=0 138 138 0, clip,  width=.45\linewidth]{figures/latent_celeba_hq_diffusion_10_steps.png}
    \end{subfigure}
    \caption{\textbf{Left}, samples from \emph{distributional} model trained on latent CelebA HQ  and sampled with $10$ steps. $\beta=0.01,\lambda=0.5$. \textbf{Right}, samples from diffusion model trained on latent CelebA HQ and sampled with $10$ steps.}
    \label{fig:latente_celeba_hq_10_steps}
\end{figure}


\begin{figure}
    \centering
    \begin{subfigure}
    \centering
        \includegraphics[trim=0 138 138 0, clip, 
        width=.45\linewidth]{figures/latent_celeba_hq_engression_100_steps.png}
    \end{subfigure}
    \hfil
    \begin{subfigure}
    \centering
        \includegraphics[trim=0 138 138 0, clip,  width=.45\linewidth]{figures/latent_celeba_hq_diffusion_100_steps.png}
    \end{subfigure}
    \caption{\textbf{Left}, samples from \emph{distributional} model trained on latent CelebA HQ  and sampled with $100$ steps. $\beta=0.01,\lambda=0.1$. \textbf{Right}, samples from diffusion model trained on latent CelebA HQ and sampled with $100$ steps.}
    \label{fig:latente_celeba_hq_100_steps}
\end{figure}


\end{document}