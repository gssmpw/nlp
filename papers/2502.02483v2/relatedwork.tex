\section{Related Work}
\label{sec:relatedWork}
\begin{figure}
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=.45\linewidth]{figures/mmd_2d.pdf}
        \label{fig:2d_mmd}
    \end{subfigure}
    \hfill
    \begin{subfigure}
        \centering
        \includegraphics[width=.45\linewidth]{figures/std_over_eps_avg_over_xt.pdf}
        \label{fig:2d_stds}
    \end{subfigure}
    \caption{Left: Squared $\MMD$ between target distribution and sampled data according to different models, $x$-axis denotes the number of sampling steps. Right: average standard deviation of samples $X_0 | X_t$ produced by either true posterior distribution $p_{0 \vert t}$ or the models $\hat{x}_{\theta}(t,X_{t},\xi)$, $x$-axis is the timestep $t$.}
    \label{fig:2d_metrics}
\end{figure}

\textbf{Accelerated diffusion models.} Many strategies accelerate diffusion models, broadly classifiable as distillation- or sampling-based.  Distillation methods \citep{luhman2021knowledge,salimans2022progressive,luo2023comprehensive,salimans2024multistep,dieleman2024distillation,liu2022flow,meng2023distillation,song2023consistency,franceschi2024unifying,huang2024flow,sauer2025adversarial} train a student model to mimic a teacher diffusion model, leveraging consistency losses \cite{song2023consistency}, adversarial losses \citep{franceschi2024unifying,xu2024ufogen, sauer2025adversarial}, or noise coupling \cite{huang2024flow}. See \citep{dieleman2024distillation} for a detailed discussion.  While distillation is prevalent, other work focuses on improved samplers for larger step sizes \citep{jolicoeur2021gotta,lu2022dpm,zheng2023dpm}.  Our work differs from both, neither training a student nor proposing new samplers.

\textbf{Discrepancy and diffusion.} We modify the diffusion model training loss to learn the conditional distribution $p_{0|t}(x_0|x_t)$.  The importance of approximating the covariance this distribution was noted in \citep{nichol2021improved} and exploited in \citep{ho2020denoising,nichol2021improved,bao2022estimating,bao2022analytic,ou2024diffusion}.  Closest to our approach is \citep{xiao2021tackling}, which uses a GAN to better learn $p_{0|t}(x_0|x_t)$ for fewer sampling steps.  While sharing the same motivation, our method avoids adversarial training and discriminator training, offering a simple loss modification that encompasses standard diffusion models.  Other loss modifications, like using the $\ell_1$ loss \citep{chen2020wavegrad,saharia2022palette}, aim to improve output quality, not reduce sampling steps.  \citep{galashov2024deep} can be seen as dual to our work: they learn discriminative kernel features with a flow at different noise levels, while we focus solely on learning a generator.

\textbf{Energy distances, MMD, and generative modeling.}
The MMD \citep{gretton2012kernel}, of which energy distances \citep{szekely2013energy,rizzo2016energy} are a special case \citep{sejdinovic2013equivalence}, has been widely used as a distributional loss in generative modeling. GANs have used MMDs with fixed kernels as critics to distinguish generated from reference samples \citep{li2015generative,dziugaite2015training,Unterthiner2018coulomb}. MMDs \citep{li2017mmdGan,binkowski2018demystifying} and energy distances \citep{energy-distance-gan,bellemare2017cramerdistancesolutionbiased,salimans2018improvinggansusingoptimal} have also been defined on adversarially trained discriminative neural net features.
The conditional generalized energy score of \eqref{eq:energyScoreConditional} has been used in learning conditional distributions by the DISCONet approach of \citet{bouchacourt2016disco},  and by the engression approach of \citet{shen2024engression} 
(which corresponds to the special case of $\lambda,\beta=1$). Recently, energy distances have been applied to speech synthesis \cite{gritsenko2020spectral}, normalizing flows \cite{si2023semi}, neural SDEs \cite{issa2024non}, and other generative models \citep{chen2024generative,Pacchiardi2024}.

To our knowledge, such losses have not previously been used to train diffusion models.
It is possible, however, to use an adaptation of  \citep{galashov2024deep}  in combination with the approach of \citet{bouchacourt2016disco,shen2024engression}, in order to incorporate conditional GAN-style generation into the reverse process of a diffusion model.  This idea was proposed by X. Shen  (personal communication, 6th November 2024) in an open  discussion following a presentation at Google Deepmind, as a future research direction of interest. The main idea of \citet{galashov2024deep} is retained, namely to use a standard forward diffusion  process, and a distributional loss for the reverse process: however the adaptive-kernel MMD loss for the reverse process is replaced with a fixed-kernel energy score; and this score is used to train a sequence of conditional GAN generators to approximate $p(x_{t_{k-1}} |x_{t_{k}})$, rather than using particle diffusion directly. The approach indeed represents a promising line of work \citep[since developed in][]{shen2025reversemarkovlearningmultistep} for discretizing the reverse diffusion process, as distinct from the DDIM-style approach adopted in  \cref{sec:method}. The approach can further be understood as a non-adversarial formulation of  \citet{cheng2024conditionalganenhancingdiffusion}, which uses an adaptive conditional GAN critic in place of the fixed-kernel energy score.

\begin{figure}
    \centering
        \includegraphics[width=1\linewidth]{figures/engression_2d.png}
    \caption{Samples from true posterior $p_{0 \vert t}(\cdot | x_t)$ (light blue) for a sample $x_t$ (black cross) from $p_ {t|0}(x_t|x_0)$ for a specific $x_0$ (orange cross) and samples $\hat{x}_{\theta}(t,x_{t},\xi) \sim p^\theta_{0|t}(\cdot|x_t)$ (green dots) for $\beta=0.1,\lambda=1$.  Top/bottom row: $x_0$ is the mean of the left/right Gaussian. Samples from $\hat{x}_{\theta}(t,x_{t},\xi)$ (red) with $\beta=2,\lambda=0$ concentrate around $\mathbb{E}[X_0 | x_t]$ as expected.}
    \label{fig:trajectories_2d}
\end{figure}