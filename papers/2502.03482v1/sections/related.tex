\section{Related work}




\para{Human-AI decision making.}
There is a growing interest in the research community to augment human decision making with AI assistance~\cite{lai2023towards}. 
Typically, the tasks of interest are situated in high-stakes domains such as medicine, law, and finance, where AI-assisted decisions can have significant consequences. 
However, due to constraints related to resources and the simplicity of participant recruitment, the majority of empirical studies in this area are conducted with crowdworkers or laypeople without expertise.  
For instance, instead of involving real judges, researchers have explored recidivism prediction as a testbed for Human-AI decision making using crowdworkers~\cite{binns2018s, lai2019human, green2019disparate}.
Similarly, in the medical domain, experiments on disease diagnosis have been conducted with laypeople, such as students~\cite{lakkaraju2016interpretable}. 
In finance, studies have utilized crowdworkers for tasks like income prediction~\cite{zhang2020effect}, loan approval~\cite{green2019disparate}, and sales forecasting~\cite{dietvorst2015algorithm}. 
In some cases, researchers have substituted real-world tasks with entirely artificial ones to facilitate experimentation with crowdworkers, such as alien medicine recommendation~\cite{lage2019evaluation}.

While crowdworkers offer a convenient participant pool, it remains unclear if findings based on these populations generalize to domain experts in real cases. In our work, we work directly with domain experts. 


\para{Human-AI decision making with experts in the clinical context.}
There have been several studies with healthcare professionals in the clinical context, but experiments focused on human-AI complementary performance remain limited.
While several studies have shown that AI assistance can improve diagnostic accuracy~\cite{steiner2018impact,sim2020deep,jain2021development,seah2021effect,mcduff2023towards}, the experts behavior in human-AI collaboration are underexamined.
Existing research also reveals complex performance trade-offs: some studies reveal important trade-offs, such as improved sensitivity at the cost of reduced specificity~\cite{kiani2020impact,park2019deep}.
Some studies explicitly demonstrated that the performance of human-AI performance falls short of AI alone~\cite{rajpurkar2020chexaid,kim2020changes}.
To the best of our knowledge, the only work that achieves complementary performance is \citet{steiner2018impact}, which demonstrated that algorithm-assisted pathologists outperformed both the algorithm and pathologists in detecting breast cancer metastasis. However, human specificity is 100\% on that task, suggesting a relatively easy task for domain experts. 



In summary, human-AI decision making with domain experts, especially for complementary performance, remains underexplored. In light of this gap, our study aims to provide an in-depth analysis of both human+AI team performance and domain expert behavior in a difficult, real-world clinical setting.




































