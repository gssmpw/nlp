\subsection{Human-AI Decision Making Interface}

\begin{figure}[t]
    \centering
    \begin{minipage}[b]{0.5\textwidth}
        \begin{subfigure}[b]{\textwidth}
            \includegraphics[width=\textwidth]{figs/interface/study1.pdf}
            \caption{Patient case review interface.}
            \label{fig:study1_interface}
        \end{subfigure}
        \begin{subfigure}[b]{\textwidth}
            \includegraphics[width=\textwidth]{figs/interface/annotate.pdf}
            \caption{Lesion annotation panel.}
            \label{fig:annotate}
        \end{subfigure}
    \end{minipage}
    \begin{subfigure}[b]{0.465\textwidth}
        \includegraphics[width=\textwidth, trim={500 100 500 150}, clip]{figs/interface/feedback.pdf}
        \caption{Performance feedback and attention check page.}
        \label{fig:feedback}
    \end{subfigure}
    \caption{
    Screenshots of the webapp interface for our human study.
    (a) \cref{fig:study1_interface} presents a user interface for patient case evaluation.
    An AI lesion prediction is highlighted with a red contour in the T2W sequence. On the right, the user's current prediction is shown as ``No Cancer," and they are at the stage of evaluating the AI prediction to make a final diagnosis. 
    (b) \cref{fig:annotate} shows the user interface of the \ap.
    The screenshot shows a current annotation of the user.
    The user can clear the annotation or add new annotations on the canvas.
    (c) \cref{fig:feedback} illustrates an example performance feedback page presented to a user before proceeding to Study 2. 
    The page provides a summary of the total number of cases, including counts of correct and incorrect cases, the number of decision changes influenced by AI advice, and whether those changes were correct or incorrect. It also highlights key performance metrics such as accuracy, sensitivity, and specificity, derived from Study 1.
    To ensure users review the information carefully, they are required to answer attention check questions.
    }
\end{figure}

We developed a webapp to conduct the human-study.
Participants can log in with their name and email.
They will see a consent page when they log in for the first time.
Once they give the consent, they will enter the study and see our study interface. A screenshot of the consent page can be found in appendix \cref{fig:consent-form}. Our human study is pre-registered and approved by the Institutional Review Board (IRB).

\para{Study interface.}
Our study interface has three major components: the \vp on the left, the \cp on the right, and the \ap as a pop-up in the center of the screen. 
The interface is shown in \cref{fig:study1_interface}.
In the \vp, we display three image sequences (T2W, ADC, BWI) from the MRI scans of the current case.
In the \cp, participants are informed about the current study (study 1 or 2) and provided with control buttons to make decisions or proceed to the next steps. 
Binary case-level AI predictions are also presented in this panel.  Participants make their own predictions by clicking the buttons (`Annotate Cancer" for positive cases and ``No Cancer'' for negative cases) and indicate their confidence level using a sliding bar.
If a participant believes the case is positive, they click the "Annotate Cancer" button, which triggers a pop-up window (\ap) displaying enlarged images from the T2W sequence of the current case, allowing participants to annotate the suspicious lesion areas. 
Participants can annotate any suspicious lesions by freely drawing on any image slice, using the sidebar to navigate between slices. 
The annotation interface is illustrated in \cref{fig:annotate}.

\para{Performance feedback.}
In Study 2, the first page after the login page will be the performance feedback page, as shown in \cref{fig:feedback}.
This page provides detailed individual feedback on their performance from Study 1.
The feedback includes both case counts and performance metrics. Specifically, we present the total number of cases completed by the participant, the number of cases where their prediction disagreed with the AI's prediction, and the number of times they changed their decision after viewing the AI's advice. Among these decision changes, we further highlight how many were correct and how many were mistaken after incorporating the AI's input.
For performance metrics, we provide accuracy, sensitivity, and specificity. These metrics are shown for the participant’s diagnoses before and after reviewing AI predictions, as well as for the AI’s performance alone. This breakdown allows participants to see the impact of the AI on their decision-making and compare their independent performance with AI.
At the bottom of the feedback page, we ask an attention check question to ensure participants review the information carefully.
The attention question is a single-answer multiple-choice question that asks for the value of one of the performance metrics displayed on the page.

\para{Exit survey.}
As the final step in both studies, participants are required to complete an exit survey. The survey for Study 1 collects demographic information and participants' opinions on AI. The survey for Study 2 gathers their thoughts on the performance feedback provided and revisits their opinions on AI. Screenshots of these surveys are included in the appendix \cref{fig:exit-survey-1} and \cref{fig:exit-survey-2}.



