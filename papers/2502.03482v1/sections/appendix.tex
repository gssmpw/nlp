
\section{Model Impementation Details}
\label{appen:model}

\para{Training configurations}
We use the established nnU-Net implementation\footnote{\url{https://github.com/DIAGNijmegen/picai_baseline}} for image segmentation.
The framework was configured to handle dataset preprocessing, augmentation, and training pipeline generation automatically. The training process utilized a batch size of 8 and a learning rate of $0.001$, optimized using the AdamW optimizer. Training was performed over 1000 epochs on one NVIDIA A40 GPU. nnU-Net's default data augmentation techniques, such as random cropping, flipping, and intensity scaling, were employed to improve generalization. For lesion-level prediction, we set the threshold to 0.5. The framework's automatic hyperparameter tuning ensured optimal performance, and we monitored model training using AUROC and average precision on the validation set.
A detailed performance is shown in table~\cref{tab:AI-results}.




\begin{table}[ht]
\begin{tabular}{@{}lcccccccc@{}}
\toprule
                   & \multicolumn{4}{c}{Training (n=1211)}        & \multicolumn{4}{c}{Testing (n=200)}         \\         \cmidrule(lr){2-5} \cmidrule(l){6-9}
                   & AUROC  & AP     & Accuracy & F1     & AUROC  & AP     & Accuracy & F1     \\
                   \midrule
Per-patient & 0.910 & 0.737 & 0.847   & 0.725 & 0.799 & 0.624 & 0.735    & 0.644 \\
Per-lesion       & 0.940 & 0.682 & 0.948   & 0.664 & 0.824 & 0.484 & 0.911   & 0.531 \\ \bottomrule
\end{tabular}
\label{tab:AI-results}
\caption{AI model performance.}
\end{table}




\section{Demographics}
\label{app:human_demographics}
We recruit 8 practicing radiologists, aged 29 to 52 years (mean: 38.4 years). Respondents were primarily from the United States (n=4), Turkey (n=3), and Italy (n=1). Most participants (n=5) reported advanced or expert-level experience with prostate MRI, whilte the others reported intermediate (n=2). One participant did not answer this question.  

\section{Exit Survey Results}

\subsection*{Study 1 Results}
In Study 1, participants were highly familiar with the AI tool (mean familiarity: 5/5), though its accuracy received a lower mean rating of 2.4/5. Usefulness and trust in the system were rated moderately, both averaging 3/5. 
In open-ended feedback, practitioners reported that the AI tool was most helpful in ambiguous cases and increased confidence in detecting lesions in challenging locations such as the anterior, apical, and transition zones. Concerns included oversensitivity in non-cancerous areas and missed lesions, with suggestions for improvement focusing on providing malignancy probability scores, separate reporting of T2 and DWI/ADC scores, and better performance in transitional zone lesions.

\subsection*{Study 2 Results}
In Study 2, the AI tool's helpfulness was rated moderately (mean: 2.9/5), with accuracy ratings remaining low to moderate (mean: 2.1/5). Trust in the AI also averaged 2.5/5. Despite moderate satisfaction, respondents expressed a high likelihood of future AI use (mean: 3.75/5). In open-ended feedback, the AI was perceived as useful in ambiguous cases, with one practitioner noting it reinforced decisions to call studies negative. 
They also pointed out key challenges such as poor performance in transitional zone lesions, overreliance on diffusion restriction, and limitations in segmenting prostate versus non-prostate tissue. Participants' recommendations for improvement included adopting the PI-RADS classification system, enhancing segmentation capabilities, and improving detection of small lesions. Image quality issues were a significant limitation, with practitioners noting that humans outperform AI in evaluating non-diagnostic images, particularly for diffusion-weighted imaging.




\section{Fine-grained analysis}
\label{app:fine_grained}

\cref{tab:study1_analysis} and \cref{tab:study2_analysis} provide an overview of the subgroup analysis of human-AI agreement and disagreement in Studies 1 and 2, respectively. The results indicate that performance metrics are significantly better in subgroups where human and AI decisions align compared to those with disagreement.

For a detailed breakdown, individual-level performance for the different agreement and disagreement subgroups is presented. In Study 1, the results are available in \cref{tab:study1-fine-1}, \cref{tab:study1-fine-2}, \cref{tab:study1-fine-3}, and \cref{tab:study1-fine-4}, each focusing on specific subcategories of agreement or disagreement. Similarly, Study 2 individual-level results are provided in \cref{tab:study2-fine}, offering finer granularity of the analysis.

\input{tables/fine-grained-tables-main}
\input{tables/fine-grained-tables}


\section{Ensemble on Common-50 Cases}

\cref{tab:ensemble-common-cases} presents a detailed performance comparison among AI, Human, Human-ensemble, Human+AI, and Human+AI ensemble (Study 1 and Study 2) for the common 50-case subset. While the results highlight that the Human-ensemble consistently outperforms individual human performance, the advantage of any ensemble method over AI alone is less significant.

\input{tables/ensemble_common}




\section{More Screenshots on User Interface Design}
\label{sec:ui-design}

We show screenshots of a login page (\cref{fig:login}), a consent form (\cref{fig:consent-form}), a toy demonstration example page (\cref{fig:demo}), and two exit surveys (\cref{fig:exit-survey-1}, \cref{fig:exit-survey-2}) for study 1 and study 2 respectively.

 


\begin{figure}[ht]
    \centering
    \includegraphics[width=.9\linewidth]{figs/interface/login.pdf}
    \caption{Login page.}
    \label{fig:login}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figs/interface/consent.png}
    \caption{Consent page.}
    \label{fig:consent-form}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figs/interface/demo.pdf}
    \caption{Toy demonstration example page.}
    \label{fig:demo}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=.55\linewidth]{figs/interface/survey1.pdf}
    \caption{Exit survey for study 1.}
    \label{fig:exit-survey-1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=.55\linewidth]{figs/interface/survey2.pdf}
    \caption{Exit survey for study 2.}
    \label{fig:exit-survey-2}
\end{figure}