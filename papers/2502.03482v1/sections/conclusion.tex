\section{Conclusion}

While there is a growing interest in evaluating AI assistance with human decision makers, only a handful of previous works have attempted to evaluate AI systems directly with domain experts, and even fewer have achieved complementary performance or investigated human behavior. 
We contribute a comprehensive study with domain experts about how a clinical AI tools might be integrated in practice with two realistic design of workflows. Our findings suggest that while human-AI teams consistently outperform humans alone, they still underperform compared to AI due to under-reliance.
More importantly, we look beyond accuracy and investigate human behavioral patterns in human-AI interaction. 
Even when domain experts are informed about their performance, the gap to AI performance, and their previous AI-assisted performance, it remains challenging for them to effectively calibrate their reliance and trust in AI tools. 
While complementary performance falls short in our work---as in previous works---our results on the ensemble performance of human-AI teams are promising.
This highlights exciting opportunities to improve human-AI decision-making. 



\para{Limitations.} 
Several issues remain unresolved and present opportunities for future research. 
While our study show that upfront AI assistance can encourage greater adoption among radiologists, it remains unclear what factors positively contribute to complementary performance.
Additionally, our research is limited to particular clinical setting and disease, which may not be generalizable to other domains or environments. 
Despite these limitations, we hope that our study will inspire and support the broader research community to further investigate the complexities of human-AI decision-making in relevant real-world tasks.
