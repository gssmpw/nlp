
\section{Results}




We organize our findings into two parts: 1) the effect of AI assistance on the performance of human-AI decision making; 2) how AI assistance changes behavioral patterns such as reliance and decision efficiency.
Overall, for (1), we observe a performance trend in order of \textbf{Human alone < Human+AI < AI}, with occasional instances of individual radiologists achieving complementary performance. It is also worth noting the ensemble of human+AI could outperform AI, i.e., complementary performance. 
For (2), we find that the different workflow does not significantly impact human performance. Radiologists are generally reluctant to adopt AI suggestions after making their own diagnosis. In contrast, providing upfront AI input increases the adoption of AI advice among experts. However, under-reliance on AI persists, preventing human+AI team from achieving complementary performance. 

\subsection{Performance of Human vs. AI vs. Human+AI Team (Q1)}
We evaluate both the baseline performance of humans and their performance after receiving AI assistance.
\cref{tab:main-results} presents an overview of performance metrics from both studies, including per-patient and per-lesion results. 

\para{\halone < AI.}
The workflow of Study 1 allows us to compare the baseline performance of humans and AI on the same set of patient cases. 
As shown in \cref{tab:main-results}, 
AI consistently outperforms humans across most metrics, with statistically significant advantages in AUROC, accuracy, specificity, and PPV/precision($p<0.05$).
At the lesion level, the AI also shows significant gains in accuracy, sensitivity, and PPV. Moreover, we find that for identified positive lesions, AI is less likely to miss the biopsy confirmed lesions, compared with human radiologists. \cref{fig:human-wrong-lesion} provides an example of this.
These findings suggest that the AI is better than human radiologists in predicting csPCa, especially in identifying true negative cases and true positive lesions.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/human_wrong_lesion.png}
    \caption{An example of lesion-level annotation comparing human experts (red contour), AI (yellow contour), and expert annotation from the dataset (green contour). In this case, the AI successfully detected a lesion which corresponded to a clinically significant prostate cancer in the dataset; our human radiologist did not identify this lesion, and instead annotated a lesion in the transition zone.}
    \label{fig:human-wrong-lesion}
\end{figure}

\input{tables/common-50-table}

 
\para{\halone < \hai.}
In Study 1, human+AI outperformed human radiologists alone, with statistical significance in AUROC, accuracy, specificity, and PPV/precision ($p<0.05$), as shown in \cref{tab:main-results}. This highlights the potential positive utility of AI assistance.


While study 2 did not include a direct human-alone baseline, we conducted two statistical analysis to evaluate the impact of AI assistance. First, we performed an unpaired statistical test, comparing human-alone performance from Study 1 (75 cases) against human+AI performance from Study 2 (100 cases). This analysis shows statistically significant improvements in both AUROC and accuracy, from \cref{tab:main-results}. Second, to further validate these findings with a common set of patient cases, we investigate specifically the 50 common cases shared between both studies to perform a paired statistical analysis. By referencing the human-alone performance from Study 1 on these exact same cases, we found that human+AI  outperformed human-alone in both studies, as shown in \cref{tab:common-cases}.


Overall, our findings suggest that AI assistance consistently improves radiologists' performance. 


\para{\hai < AI.}
Although the Human + AI team outperforms humans alone, it consistently underperforms AI alone in AUROC, accuracy, specificity, and PPV/precision ($p<0.05$) in Study 2, while showing no significant evidence of inferiority to AI in Study 1.
This trend becomes more salient when focusing on the common 50-case subset, as shown in \cref{tab:common-cases}, where all metrics except specificity show statistically significant differences in both studies.
This is somewhat justified, as human radiologists in practice tend to be more cautious to avoid missing any suspicious cases (i.e., identifying true negative cases). They are inclined to send suspicious cases for biopsy.  
For lesion level analysis, it is more prominent that AI outperformed \hai in identifying positive lesions, with statistical significance in accuracy, sensitivity, and precision in Study 1. 


\para{Individual human radiologists can occasionally achieve complementary performance.}
In the common cases between Study 1 and Study 2, we evaluate individual radiologists and AI-assisted radiologists against AI model using both receiver operating characteristic (ROC) and precision-recall (PR) curves. 
As shown in~\cref{fig:two_side_by_side}, and consistent with prior discussions, the AI curve generally outperforms individual radiologists (represented by blue dots).
Additionally, AI-assisted radiologists in both studies (red and orange dots) are generally positioned above individual radiologists (blue dots) in both figures, indicating that AI assistance helps improve radiologistsâ€™ performance.
We highlight that there are cases where AI-assisted radiologists outperform the AI curve, as shown by the red and orange dots above the AI curve. This is a promising finding as it suggests that AI assistance could augment human to achieve complementary performance (Human+AI > human and Human+AI > AI).



\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/roc_curve.pdf}
    \caption{ROC Curve}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/prc_curve.pdf}
    \caption{PR Curve}
  \end{subfigure}
  
  \caption{Individual radiologists performance compared with the AI model. The model achieves higher performance than all of the radiologists without AI assistance (blue dots). However, with AI assistance, some individual radiologists outperformed the AI model (red and orange dots that are above the curve).
  }
  \label{fig:two_side_by_side}
\end{figure}




\input{tables/ensemble}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/mean_ensemble.pdf}
    \caption{
        Mean performance of \halone, \hai, \hensem, \haiensem, and AI in Study 1.
        AUROC, accuracy, specificity, NPV, and PPV are significantly better in \hensem than in \halone.
        In \haiensem, AUROC, accuracy, and PPV are significantly better than that of AI.
    }
    \label{fig:ensemble}
\end{figure}

\para{Ensemble of human outperforms human but not AI, ensemble of Human+AI could outperform AI.} We compiled an ensemble of results from the human radiologists' predictions in \cref{tab:ensemble} and \cref{fig:ensemble}.
For each test case, we do a majority vote among the predictions from the eight radiologists.
If there is a tie among the radiologists, i.e. four cancer predictions versus four non-cancer predictions), we calculate the weighted prediction based on the radiologists' reported confidence.
Performance of \hensem is significantly improved over \halone, especially with precision/PPV increasing from $44.7\%$ to $48.7\%$ ($4\%$) and specificity rising from $56.5\%$ to $61.5\%$ ($5\%$).
This improvement closes the gap between humans and AI.
Moreover, \haiensem has the highest performance among all conditions, gaining significantly better AUROC ($0.771$), accuracy ($73.3\%$), and precision/PPV ($54.1\%$) than AI.
Sensitivity also reaches $87.0\%$, indicating a strong performance. 
This suggests that, with the help of AI, a group of experts can surpass either themselves or AI, achieving complementary performance.



\subsection{Behavioral Analysis on Human-AI collaboration (Q2)}


We now focus on the impact of different interventions, specifically the effect of performance feedback in Study 2 and the effect of providing AI assistance after humans have made their decisions.

\para{The different workflow does not significantly change human performance --- comparison of common-50 subset results of study 1 and 2.} In study 2, we share with each participant their own individual performance, the AI's performance, and their performance after reviewing AI predictions. 
A sample screenshot of the performance feedback provided to an individual radiologist is shown in \cref{fig:feedback}. 
To ensure radiologists understood their relative performance compared to the AI and whether AI assistance improved their results from Study 1, they were required to answer an attention check question before proceeding with the study.
We investigate how this performance feedback affects human decision making behavior, particularly whether they tended to incorporate AI advice more, less, or without significant change.
By learning about their past performance, the AI's performance, and the previous \hai team performance, radiologists were better informed before making new decisions in Study 2.

We hypothesized that radiologists would adjust their trust and reliance on AI if they realized that AI was more accurate overall. 
To test this, we analyze the performance of the 50 common test cases across study 1 and study 2. 
Despite the introduction of performance feedback, Human+AI team still does not surpass AI alone and achieves results that are relatively similar to or only slightly better than Human+AI in Study 1.
Moreover, there is no statistical significance in any of the metrics comparing \hai[2] with \hai[1]. 
As none of the metrics showed statistical significance, we defer the full details of the common-set results to Appendix \cref{tab:ensemble_common}. 
In conclusion, our findings suggest that performance feedback did not lead to significant improvements in the Human+AI accuracy.





\begin{figure}[t]
  \centering
  \begin{subfigure}[c]{0.6\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/study-1-ai-prostate-confusion-matrix.pdf}
    \caption{Study 1 (total cases n=75; radiologists N=8). Top-2 frequent groups are aligned and overrule AI. When there is a disagreement in the initial decision, radiologists are more likely to overrule AI predictions. However, Accuracy in the follow-AI group is higher than the `Aligned' and `Overrule AI' groups ($p=0.04^*$).
    }
    \label{fig:study1}
  \end{subfigure}
  \hspace{0.05\textwidth} %
  \begin{subfigure}[c]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/study2-human-agreement-disagreement-heatmap.pdf}
    \caption{Study 2 (total cases n=100; radiologists n=8). Compared with study 1, radiologists are more likely to follow AI when the AI is shown directly without them making their own initial decision. Accuracy is also higher in the `follow AI' group compared with `overrule AI' ($p=0.00^*$). }
    \label{fig:study2}
  \end{subfigure}
  \caption{Comparison of Human-AI Decision Alignment and Accuracy. 
  Blue shading indicates frequency of cases for each scenarios; percentages showing diagnostic accuracy for scenario. 
  Accuracy is the highest in the follow-AI group for both studies. 
  }
  \label{fig:comparison}
\end{figure}




\para{Radiologists are reluctant in adopting AI assistance after they made their own independent diagnosis.}
In Study 1, radiologists first make diagnostic decisions before being shown the AI's predictions. This allows us to observe how likely they are to incorporate AI suggestions. 
The results indicate that radiologists tend to maintain their initial diagnostic decisions even when presented with contradicting AI predictions.
From \cref{fig:study1}, the initial agreement between human and AI is about 52.4 (69.9\%) vs. 22.6 (30.1\%). For 52.4 cases (initial agreement), human rarely changes their decision as their decision is confirmed by AI. 
When the AI disagrees with their initial assessment (22.6/75 average cases), radiologists change their diagnosis in only 4.6 (20.4\%) of cases.
This reluctance to revise initial decisions persists even in cases where their own accuracy is low (44.4\%), suggesting a significant barrier to incorporating AI assistance.

\input{tables/conf_time}
\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/perperson/mean_confidence.pdf}
    \caption{Mean confidence of each participant.}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/perperson/median_time.pdf}
    \caption{Median time spent of each participant.}
  \end{subfigure}
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/perperson/mean_sensitivity.pdf}
    \caption{Mean sensitivity of each participant.}
  \end{subfigure}
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/perperson/mean_npv.pdf}
    \caption{Mean negative predictive value of each participant.}
  \end{subfigure}
  \caption{
  The average confidence score, time spent, sensitivity, and NPV on the common 50-case subset for each participant.
  Statistics are calculated on data bootstrapped in the same way as the results in \cref{tab:common-cases}.
  Performance metrics other than sensitivity and NPV are excluded due to insignificance on the comparison between \halone and \hai[2].
  All significant comparisons are annotated with corresponding $p$-values with green indicating increasing and red indicating decreasing.
  Means are plotted with 95\% confidence Intervals.
  }
\end{figure}
  

\para{Upfront AI input and performance feedback increase AI adoption.}
In Study 2, performance feedback was shown to human radiologists at the very beginning of the study to help them gain a sense of their performance compared with AI from Study 1. When they diagnose each patient cases, AI predictions/assistance are shown upfront without them making their own initial diagnosis. 
As shown in \cref{fig:study2}, the results indicate that performance feedback and upfront AI assistance leads to higher rate of human-AI agreement (78.4\% ``follow AI'' vs. 75.5\% final human-AI agreement from study 1). 
In addition, ``follow AI'' group shows higher accuracy (87.3\%) compared with ``overrule AI'' group (35.3\%), as well as sensitivity (92.1\% vs. 36.6\%), and specificity (72.4\% vs. 34.8\%). For a complete results with more metrics, please refer to \cref{tab:study2_analysis} in the Appendix.
This slightly higher adoption rate, however, was insufficient to bridge the gap between \hai teams and AI significantly.















\para{Mixed Effects on Diagnostic Confidence and Time Efficiency.}
In addition to the diagnoses and annotations on the test cases, we also ask radiologists for a confidence score for each of their diagnoses on the case-level.
We design the confidence score to be on a scale of one to five (from ``Not certain at all'', ``Slightly certain'', ``Moderately certain'', ``Highly certain'', to ``Extremely certain'').
We observe no significant difference between the overall mean confidence scores of \halone and \hai[1].
However in \hai[2] radiologists report significantly higher confidence scores than in \halone ($p<0.05$), along with higher sensitivity and NPV as shown in \cref{tab:common-cases}.



We also tracked how long radiologists spent on each case in seconds. Because the \hai[1] diagnosis is an update of \halone, its recorded time includes the entire decision process from \halone. To mitigate outliers, we focus on median times: $123.11s$ for \halone, $144.65s$ for \hai[1], and $115.89s$ for \hai[2]. On average, radiologists used about 21 seconds more in \hai[1] (a statistically significant increase) and about 7 seconds less in \hai[2] (not statistically significant).
On the individual level, we did not observe a consistent ``time versus performance'' trade-off among all radiologists.  
Some spent less time and improved (P2) or maintained (P3, P5, P7) their diagnostic performance, while others lost sensitivity (P1). 
Others who took the same or more time either saw no change in performance (P5) or increased their sensitivity (P6, P8) or negative predictive values (P8).
These individual variations suggest that the relationship between processing time and diagnostic performance is complex and participant-specific.

