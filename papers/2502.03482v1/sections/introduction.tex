
\section{Introduction}

AI holds promise for improving human decision making in a wide range of domains~\citep{lai2023towards,kleinberg2018human,reverberi2022experimental,scherer2019artificial,alon2023human}. 
Radiology is a representative example as 
AI outperforms or shows comparable performance with experts~\cite{hosny2018artificial,wu2019deep,rodriguez2019stand,rauschecker2020artificial,SahaBosmaTwilt2024,kromrey2024navigating,pauwels2021artificial,mckinney2020international}.
Rather than complete automation, there is growing consensus that AI's optimal role in the near future will serve as an assistance tool for human radiologists in clinical decision making~\cite{langlotz2019will, agarwal2023combining, norden2022ai, harvey2020fda}.
On the one hand, legal and regulatory challenges stand in the way of full automation. 
On the other hand, human AI collaboration has the potential to achieve \textit{complementary performance}, where human experts can leverage their contextual knowledge and expertise to correct AI mistakes in ways that could surpass either human or AI performance alone. 

However, the actual utility of integrating AI assistance tools in clinical settings remain poorly understood. 
In particular, very few studies examine the effectiveness of AI assistance in real clinical decision-making with domain experts~\cite{ouanes2024effectiveness, armando2023clinical}.
In this work, we conduct an in-depth collaboration with radiologists and focus on the case of prostate cancer diagnosis.
Prostate cancer diagnosis with\han{Aritrick: multiparametric} magnetic resonance imaging (MRI) remains one of the most difficult tasks for radiologists—even experienced ones—and inter-reader variability is high~\cite{de2014accuracy,chatterjee2025prospective}. Such complexity makes prostate MRI an ideal testbed for studying how AI assistance may complement human expertise. If AI can help reduce radiologists' mistakes here, it is plausible that similar technology could be effective in other radiology tasks as well.








We run human studies with domain experts to directly understand AI tool integration in radiology workflow, particularly for challenging diagnoses like prostate cancer. We investigate two key questions:
\begin{enumerate}[label=\textbf{Q\arabic*:}, itemsep=0.2em]
    \item \emph{Can AI-assistance help humans achieve higher diagnostic accuracy 
    than either human experts or AI systems alone?}
    \item \emph{How does AI-assistance shape human decision making beyond decision accuracy?}
\end{enumerate}

 
To answer these questions, we conducted pre-registered human subject experiments with domain experts, specifically board-certified radiologists (N=8), focusing on prostate cancer diagnosis with AI assistance.
We first trained a state-of-the-art AI model~\cite{isensee2021nnu} for prostate cancer detection from MRI scans.
The AI model is able to provide both diagnostic predictions and lesion annotation maps for positive cases as assistance for radiologists. 
To simulate real-world clinical practice, we designed and implemented two distinct workflows, see \cref{fig:study_overview} for an overview of the design of our human studies. 
Building on existing tools for teaching prostate cancer diagnosis, we also developed a web-based diagnostic platform that enables radiologists to review MRI scans and annotate suspicious cancer lesions seamlessly. 

In Study 1, radiologists each evaluated 75 cases in a three-step process.
For each case, they first made independent diagnoses, which helped us to establish baseline human performance.\han{Aritrick: For each case, they first made independent diagnoses using the PIRADS v2.1 guidelines to mimic standard of clinical care, which helps us to establish baseline human performance in the diagnosis of clinically significant cancers (Gleason $\geq$3+4).}
Then, they were shown the AI's predictions. In the final step, they are asked to finalize their decisions after reviewing AI predictions.
In Study 2, we introduced a novel element: before starting their evaluations, radiologists first received detailed individual performance feedback from Study 1, as shown in the screenshot in \cref{fig:feedback}.
This feedback included various metrics of their own performance, AI's performance, and their AI-assisted performance. 
To ensure engagement with this feedback, participants completed attention checks about their performance metrics before proceeding with new cases. This design allowed us to systematically examine how performance awareness influences radiologists' interaction with AI assistance.
Moreover, for each case diagnosis, AI assistance was provided directly to radiologists without them making independent diagnosis. 

These two distinct workflows represent common scenarios in the deployment of AI assistance tools in clinical practice and their evolution over time. 
Study 1 simulates an approach often regarded as responsible, as it allows radiologists to form independent opinions before consulting AI predictions. 
This approach may be particularly relevant during early deployments,
since radiologists may prefer minimal intervention to exercise caution.
Over time, the performance information will become available in a local scenario that retains the same distribution of doctors and patients as in the earlier integration of AI tools. 
Through the design of Study 2, we can investigate how both the timing of AI assistance and awareness of comparative performance metrics influence diagnostic accuracy and radiologists' integration of AI recommendations. \han{...diagnostic accuracy and the extent to which radiologists integrate AI recommendations / the way radiologists integrate AI recommendations?}


Our findings are consistent with prior studies on human-AI decision making.
Human+AI outperforms human alone, showcasing the positive utility of AI assistance.
However, Human+AI underperforms AI alone, largely driven by under-reliance.
Although performance feedback and upfront AI assistance nudged radiologists to incorporate AI predictions more frequently, we did not observe statistically significant improvements in metrics such as area under the receiver operating characteristic curve (AUROC/AUC) or accuracy.
We further investigate the effect of ensembling decisions.
A promising finding is that the majority vote of Human-AI teams can outperform AI alone, achieving complementary performance.
This observation points to exciting opportunities to identify insights into optimal ways to facilitate human-AI decision making.
\han{This observation highlights exciting opportunities to uncover insights into optimal strategies for facilitating human-AI decision making.}


To summarize, we make the following contributions:
\begin{itemize}[nosep]
    \item We conduct an in-depth collaboration with domain experts and design two experiments to study the effect of AI-assistance on expert decision making.
    \item We demonstrate that while human+AI outperforms human alone, they fall short of AI alone, similar to prior studies with crowdworkers.
    \item We present potential opportunities in leveraging the collective wisdom of human-AI teams.
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/prostate-AI-slides.pdf}
    \caption{Overview of our experiments with radiologists.
    In study 1, participant radiologists (N=8) reviewed 75 cases in three steps: initial independent diagnosis, review of AI predictions, and final diagnosis. In study 2, we introduce performance feedback to communicate individual radiologist's performance collected from study 1 before the study. Then they reviewed 100 cases with direct AI assistance without independent diagnosis. 
    }
    \label{fig:study_overview}
\end{figure}
