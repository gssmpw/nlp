
\subsection{Experimental Design}



To evaluate the effectiveness of AI assistance, we conduct two studies with practicing radiologists ($N=8$). An overview of our experimental workflow is shown in~\cref{fig:study_overview}. 


Participant demographics, including experience levels, are detailed in Appendix~\ref{app:human_demographics}. Participants are recruited through interest forms distributed at the annual conference of RSNA (Radiological Society of North America), one of the largest radiology conferences in the world. 
We also use snowball recruiting, where participants refer colleagues and peers in their network. All participants are practicing radiologists and come from different regions (US and Europe), and all US-based participants are board-certified.





\para{Study conditions.}
Our experiments include three main conditions to evaluate radiologist performance:

\begin{itemize}[nosep]
    \item Human-only (Study 1): Independent diagnosis without AI assistance.
    \item \hai (Study 1): Diagnosis made after independent diagnosis and reviewing AI predictions.
    \item \hai (Study 2): Diagnosis made with AI predictions shown upfront, with prior feedback on individual performance metrics at the beginning of the study.
\end{itemize}



In Study 1, participants complete 75 test cases.
After logging in and signing the consent form, we provide a toy case to familiarize participants with the interface and workflow.
For each of the test cases, participants first make an independent diagnosis (human-only condition).
Then they review the AI prediction and annotations.
Participants have a chance to update and finalize their diagnosis before moving on to the next case (\hai condition for Study 1).



Between Study 1 and Study 2, we set a minimum memory wash-out period of 30 days to eliminate any recall effects. 
The actual period varies because participants complete the study at their own pace.



In Study 2, participants begin by reviewing a summary of their performance metrics from the \hai condition in Study 1. This feedback includes key metrics and interaction statistics to encourage reflection on their interaction with AI. To ensure engagement, participants answer an attention check question about the feedback before proceeding.
Study 2 consists of 100 cases, 50 randomly sampled from Study 1 and 50 new cases from a separate test pool.
Different from Study 1, AI predictions and annotations are shown upfront, and participants either accept the AI diagnosis or make modifications (\hai condition for Study 2).

Both studies conclude with an exit survey.



\subsection{Metrics and Statistical Testing Methods}

\para{Patient level metrics.}
We evaluate the performance using AUROC, accuracy, sensitivity/recall, specificity, negative predictive value (NPV), and positive predictive value (PPV)/precision, based on the predictions of Cancer vs. Non-Cancer for each case.
NPV is the proportion of cases predicted as Non-Cancer that are correctly classified. PPV/precision is the proportion of cases predicted as Cancer that are truly cancerous.

\para{Lesion level metrics.}
Note that lesion-level analysis focuses only on identified lesions (i.e., no true negatives), only accuracy, sensitivity, and PPV can be calculated at that level.
Prostate MRI consists of 3-D images, where lesions may span across multiple slices (images). For each 3-D connected lesion, we calculate lesion-level hits or misses based on a $10\%$ overlap between predicted annotations vs. groundtruth annotations, for both AI and human alike. 

\para{Statistical testing methods.}
We perform bootstrapped $z$-tests on the mean differences of metrics.
For each condition, bootstrapping is conducted by resampling with replacement over 10,000 iterations, using a sample size of 400 for population-level analysis and 50 for participant-level analysis.
We calculate the $95\%$ confidence intervals and $z$-statistics from the bootstrapped samples to conduct hypothesis testing.
Paired testing is performed when the data involve the same participants and cases; otherwise, unpaired testing is used.
We compute and report one-tailed $p$-values, applying a significance threshold of $\alpha=0.05$.

