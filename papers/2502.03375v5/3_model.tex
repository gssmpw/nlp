
\section{Problem Formulation}
In this section, we formulate interactive personalized visualization recommendation as a contextual combinatorial bandit problem with bias. 
We start with the traditional contextual combinatorial bandit scenario where an agent recommends a combination of items to a user and receives feedback for the combination. We then discuss the limitations of models mentioned in the related works, and propose a new hierarchical structure and bias term to improve the recommendation in PVisRec setting.

\subsection{Contextual Combinatorial Bandit}
Contextual combinatorial bandit algorithms are applied in various settings. In previous work done by Qin et al.~\cite{qin_contextual_2014}, the combination is assumed to have a reward that is a linear combination of rewards from related items, which is incompatible with visualization recommendation. 
Furthermore, previous works on contextual combinatorial bandits do not consider recommending items from more than one category, which is a critical aspect of the visualization recommendation problem.

To overcome these issues, we formulate a novel contextual bandit problem that allows the recommender agent to choose 3 items from 2 different categories: configuration, attribute of the x-axis and attribute of the y-axis. Different from the former contextual bandit settings~\cite{qin_contextual_2014}, the agent has to select exactly one item from each category. 
We consider the stochastic k-armed contextual bandit problem from the 2 different categories, where the total number of rounds $T$ is known. 
For each round $t$, user plays action $a_t=(a_{c,t},a_{x,t},a_{y,t})$ , picking one configuration and two attributes. Picking the configuration is noted as $a_{c,t}$, picking x-axis attribute is noted as $a_{x,t}$ and picking y-axis attribute is noted as $a_{y,t}$.

For the category of configuration, we assume there are $n$ items. We denote the feature vector of configuration in round $t$ as $\mathbf x_{c,t}$. 
The feature vector is generated based on the historical data of visualization recommendation using collaborative filtering. 

For the category of attributes, we assume there are $m$ items and we need to select two items (x-axis and y-axis) each round. We denote the feature vector of x-axis and y-axis in round $t$ as $\mathbf x_{x,t}, \mathbf x_{y,t}$.
The feature vector is generated based on the statistical values extracted from the data points of this attribute, as done in~\cite{ML-based-Vis-Rec}. 

Following the definition of contextual bandits and the denotation above, in round $t$, the reward of configuration $r_{C,t}$, the reward of attribute $r_{A,t}$ and the reward of visualization $r_{V,t}$ is modeled as:
\begin{align}
  r_{C,t} &= \langle \theta_{C,t}, \mathbf x_{c,t} \rangle + \epsilon_{t,a_t}\\
  r_{A,t} &= \langle \theta_{A,t}, \mathbf x_{x,t} \rangle + \langle \theta_{A,t}, \mathbf x_{y,t} \rangle + \epsilon_{t,a_t}\\
  r_{V,t} &= r_{C,t}+r_{A,t}
  \label{equ:comb}
\end{align}
where $\epsilon_{t,a_t}$ is a noise term with sub-Gaussian distribution and zero mean. We follow conventional assumption in bandit problems ~\cite{vakili2021optimal} that assume the inconsistent user feedback can be simplified with a sub-Gaussian distribution with zero mean. $\theta_{A,t}$ and $\theta_{C,t}$ are learnable parameters for the estimation of users' preference for attributes and configurations. Due to the hierarchical structure, the feature vectors of configurations have smaller dimensions than the feature vectors of attributes, so we use $\theta_{C,t}$ to separately denote user preference for configurations. 

To reduce calculation, we use the same $\theta_{A,t}$ to denote user preference for x-axis and y-axis attributes as they have same feature vector dimensions. Using the same set of parameter will not affect the estimation. According to the contextual bandit setting, we assume the reward of visualization is a linear combination of the configuration reward, x-axis reward, y-axis reward and bias term. If we use different parameters $\theta_{x,t},\theta_{y,t}$ for x-axis and y-axis, their sum would be $<\theta_{x,t}, \mathbf x_{x,t}>+<\theta_{y,t}, \mathbf x_{y,t}>$. But we can always concatenate $\theta_{x,t},\theta_{y,t}$ to be $\theta_{A,t}={\theta_{x,t}^{(1)},\theta_{x,t}^{(2)},...,\theta_{x,t}^{(n)},\theta_{y,t}^{(1)},\theta_{y,t}^{(2)},...,\theta_{y,t}^{(n)}}$, which has double number of dimension compared to the parameter of x-axis or y-axis. Meanwhile, we concatenate zero vector to $\mathbf x_{x,t}, \mathbf x_{y,t}$ so that the new vector $\mathbf x_{x,t}^\prime, \mathbf x_{y,t}^\prime$ would be ${x_1,x_2,...,x_n,0,0,...,0}$ and ${0,0,...,0,y_1,y_2,...,y_n}$. Calculating the inner value between $\theta_{A.t}$ and $\mathbf x_{x,t}^\prime, \mathbf x_{y,t}^\prime$ would be the same as using two parameters $<\theta_{x,t}, \mathbf x_{x,t}>+<\theta_{y,t}, \mathbf x_{y,t}>$.

We further represent the parameters $\theta_{c,t},\theta_{x,t},\theta_{y,t}$ with matrix $V_{C,t},V_{A,t}$ and vector $b_{C,t},b_{A,t}$. They are initialized with:
\begin{align}
    \theta_{C,t}&=V_{C,t}+b_{C,t}=\mathbf I_d+\mathbf 0_d\\ 
    \theta_{A,t}&=V_{A,t}+b_{A,t}=\mathbf I_d+\mathbf 0_d \label{eqn:theta_def}  
\end{align}
In each turn $\theta_{C,t},\theta_{A,t}$ is updated by updating $V_{C,t},b_{C,t}$ and $V_{A,t},b_{A,t}$.
\begin{align}
    V_{C,t}&=V_{C,t-1}+\mathbf x_{c,t-1}\mathbf x_{c,t-1}^T\\
    V_{A,t}&=V_{A,t-1}+(\mathbf x_{x,t-1}+\mathbf x_{y,t-1})(\mathbf x_{x,t-1}^T+\mathbf x_{y,t-1}^T)\\
    b_{C,t}&=b_{C,t-1}+ r_{C,t-1}\mathbf x_{c,t-1}\\
    b_{A,t}&=b_{A,t-1}+ r_{A,t-1}\mathbf (\mathbf x_{x,t-1}+\mathbf x_{y,t-1})
   \label{eqn:theta_update}
\end{align}


The goal of the agent is to minimize the cumulative regret and maximize the average reward in T rounds through repeated item combination recommendations. With the reward defined above, we want to optimize a cumulative regret defined as follows:
\begin{equation}
    Reg(T)=\sum_{t=1}^T(r_{t}^* - r_{t,a_t})
    \label{oldReg}
\end{equation}
where $r_t^*$ refers to optimal reward in round $t$.

In contextual combinatorial bandit, the reward of target combination is assumed to be a linear combination of reward of its items, which is unrealistic in the PVisRec setting. For the finite-dimensional feature vectors of attributes, it is impossible to find parameter $\theta$ that ensures $\epsilon$ is a zero-mean random variable~\cite{peng2019practical}.
More specifically, in the PVisRec problem user may like both the attributes and configuration but not their combination. To allow the agent to be aware of this difference, we need to have a different set of parameter $\theta_A$ for the attribute pair $A_{1,2}$, which is a fundamental contradiction in the contextual bandit setting~\cite{qin_contextual_2014}.

 
\subsection{Bias Term in PVisRec} 
We introduce a learnable bias term in the definition of reward function. It is an additional term in the reward function that is learnable and represents the relation between configuration and attributes. Based on Eq.~\ref{equ:comb} above, we rewrite the reward function of visualization in round $t$ with action $a_t$ as:
\begin{equation}
    r_V(t,a_t) = \langle \theta_{C,t}, \mathbf x_{c,t} \rangle +\langle \theta_{A,t}, \mathbf x_{x,t} \rangle + \langle \theta_{A,t}, \mathbf x_{y,t} \rangle + r_{\gamma,t}+\epsilon_{t,a_t}\\
\end{equation}
where $\gamma$ is an arm-specific bias term and $\epsilon_{t,a_t}$ sub-Gaussian noise term with zero mean. We model the bias term as a simple multi-armed bandit that has the same action space as the visualization given configuration $C_k$, attribute pair $A_i,A_j$. We assume the reward of bias term $r_{\gamma,t}$ as a function of visualization reward, configuration reward and attribute reward:
    \begin{equation}
        r_{\gamma,t}=f(r_{V,t},r_{C,t},r_{A,t})
    \end{equation}
There could be various non-linear functions $f(r_V,r_C,r_A)$, and the rewards in this equation follow the definition of Bernoulli reward $r_{C,t},r_{A,t},r_{V,t} \in \lbrace 0,1 \rbrace$.

\subsection{Hierarchical Structure in PVisRec}
In PVisRec problem, we derive a hierarchical structure both in the interaction with users and the learning process of the agent. The user is first asked to determine whether visualization $V_t$ is favorable by giving a Bernoulli feedback $r_{V,t}\in \lbrace 0,1 \rbrace$. 
If the answer is positive, the reward of configuration is $r_{C,t}=1$ and the reward of the attribute pair is $r_{A,t}=1$.
However, if the answer is negative, then the user will be further queried for the preference of configuration and attributes to get $r_{C,t},r_{A,t}$. 
In this system, the user provides feedback for both the combinatorial contextual bandits and the bias term bandits without degrading the user experience.
By evaluating the bias term with the feedback on the entire visualization, we avoid the cascading assumption that would otherwise harm the user experience.

The agent in our algorithm is also implemented with a hierarchical structure. In each round $t$ the agent will first decide the optimal action in configuration $a_{c,t}$, and then decides the $a_{x,t},a_{y,t},\gamma_t$ based on the summed upper confidence bound of configuration reward, attribute reward and the bias. Since the configuration usually has a smaller arm pool as studied in Fig. ~\ref{fig:preprocess}, the agent only needs to estimate the reward of attributes and the bias term for the majority of time. With such breakdown the agent is free from $O(mn^2)$ action space in the bias term, and can thus work with lower regrets.