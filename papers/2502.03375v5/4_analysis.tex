
\section{Algorithm \& Theoretical Analysis}
In this section, we propose our combinatorial bandit algorithm for interactive personalized visualization recommendation, called Hierarchical Semi UCB (Hier-SUCB).

\subsection{Hier-SUCB}

Inspired by SPUCB~\cite{peng2019practical}, we develop a combinatorial contextual semi-bandit with a learnable bias term and a hierarchical structure. The structure includes a hierarchical agent to optimize the exploration on biased combinatorial setting and a hierarchical interaction system to get detailed user feedback without hurting user experience. The algorithm maintains two sets of upper confidence bounds (UCB) including the UCB of configurations $U(c)$ and visualizations $U(c,x,y)$ with a given configuration $c$. More formally, let $U(c)$ and $U(v)=U(c,x,y)$ be defined as:
\begin{equation}
    U(c_t)=\theta_{C,t}^T \mathbf x_{c,t}+\rho_{c,t}
\label{eqn:ucfg}
\end{equation}
\begin{equation}
    U(v_t)=\theta_{C,t}^T \mathbf x_{c,t}+\theta_{A,t}^T (\mathbf x_{x,t}+\mathbf x_{y,t}) + \gamma_t + \rho_{c,t} +\rho_{a,t} +\rho_{\gamma,t}
\label{eqn:uvis}
\end{equation}
The confidence radius of attribute and configuration $\rho_a,\rho_c$ is defined as:
\begin{equation}
    \rho_{k,t}=\sqrt{\mathbf x_{k,t}^T(\mathbf I_d+\mathbf x_{k,t} \mathbf x_{k,t}^T) \mathbf x_{k,t}}, k\in\lbrace a,c \rbrace
    \label{eqn:rhoca}
\end{equation}
where $\mathbf{x}_{k,t}$ is the embedding vector of attribute or configuration in round $t$ and $\mathbf I_d$ refers to identity matrix with the same dimension $d$ as $\mathbf x_t$. According to UCB~\cite{auer2010ucb}, the confidence radius of bias $\rho_\gamma$ defined as
\begin{equation}
    \rho_{\gamma,t}=\sqrt{2ln(T) / t_\gamma}
    \label{eqn:rhobias}
\end{equation}
where $t_\gamma$ is the time that bias $\gamma$ has been played.

In each turn, the agent first computes the UCB of all configurations with Eq.~\ref{eqn:ucfg} and then selects the configuration with optimal UCB. 
Afterward, the agent evaluates the upper confidence bounds of all visualizations with Eq.~\ref{eqn:uvis} to select the optimal. Then, the agent will ask for user feedback on the recommended visualization: 
if it is positive, the agent will automatically take the configuration and attributes as positive; otherwise, it will further ask for user feedback on the configuration and attributes separately. 

Intuitively, adding a bias term in the estimation of visualization reward can improve the accuracy of recommendation, because in the worst case we can assume it is the visualization reward and explore in a large action space. By designing appropriate reward function for the bias term, the bias term can serve as a correction term for cases that user likes the configuration and attributes but not the visualization. With the hierarchical structure of our agent, we further narrow down the large action space of the bias term. The agent quickly converges in configuration bandit with less item pool, so that it can have more exploration of attribute and bias terms with larger item pools.

\begin{algorithm}
    \SetAlgoLined
        Initialize $\theta_{C,t},\theta_{A,t}, \gamma_t$ (Eq. ~\ref{eqn:theta_def})\;
	\For{$t=1,2,...T$}{
	
        \For{$a_{c,t}=1,2,...n$}{
        Compute UCB $U(c_t)$ (Eq. ~\ref{eqn:ucfg})\;
        }
        Select $c_t=\textbf{argmax}(U(c_t))$\;
        \For{$a_{x,t}=1,2,...m$}
            {
            \For{$a_{y,t}=1,2,...m$}
                {
                Compute UCB $U(c_t,x_t,y_t)$ (Eq. ~\ref{eqn:uvis})\;
                }      
            }
        Select $V_{t}=\textbf{argmax}(U(c_t,x_t,y_t))$\;
        \uIf{$r_{V,t}==1$}
        {$r_{C,t}\leftarrow 1,r_{A,t}\leftarrow 1$\;}
        \Else{ask for $r_{C,t},r_{A,t}$\;}
        Update $\theta_{C,t},\theta_{A,t}, \gamma_t$ (Eq. ~\ref{eqn:theta_update})\;
        Update $\rho_{c,t},\rho_{a,t}, \rho_{\gamma,t}$ (Eq. ~\ref{eqn:rhoca},~\ref{eqn:rhobias})\;
        }
\caption{Hier-SUCB}

\end{algorithm}

\subsection{Regret Analysis}
% When the user provides negative feedback for the visualization, we consider the regret of round $t$ in four cases:
The regret of Hier-SUCB comes from the exploration of preferred configuration, attribute pair and learning the bias term. The exploration of preferred configuration and attribute pair can be reduced to general combinatorial bandit problem. Learning bias term can be viewed as a general bandit problem with constraints. For more detailed analysis of regret bound, we consider the regret of round $t$ under four cases when the user provides negative feedback to the visualization:
\begin{enumerate}
    \item Like configuration $c_t$ and attribute pair $\lbrace x_t,y_t \rbrace$ 
    \item Like attribute pair $\lbrace x_t,y_t \rbrace$ but not configuration $c_t$
    \item Like configuration $c_t$ but not attribute pair $\lbrace x_t,y_t \rbrace$
    \item Dislike configuration $c_t$ and attribute pair $\lbrace x_t,y_t \rbrace$
\end{enumerate}

For case (1), we provide the regret bound by analyzing the bias term converges in certain rounds.
\begin{lemma}
    The reward gap between optimal and sub-optimal bias $\gamma$ is bounded with the overall round $T$ and the time $t_\gamma$ that $\gamma$ has been played for.
    \begin{equation}
        \Delta_\gamma \leq \sqrt{\frac{ln(T)}{t_\gamma}}
    \end{equation}
    \label{lem:case1}
\end{lemma}
\begin{proof}
    having a positive configuration and attributes while negative visualization implies:
% Having positive configuration and attributes while negative visualization implies:
\begin{equation}
    U(c,a)\geq U(c^\ast,a^\ast)
\end{equation}
where $c^\ast, a^\ast$ refers to configuration and attributes of preferred visualization. Notably, $c,a$ may also receive positive feedback from user, but their combination is not preferred. In such case, $\Delta_c=\Delta_a=0$, and we can bound the regret with bias:
\begin{equation}
    \Delta_{bias} \leq \rho_{c,t} +\rho_{a,t}+ \rho_{\gamma,t} - \rho_{c,t}^\ast -\rho_{a,t}^\ast -\rho_{t,\gamma}^\ast
\end{equation}
The round that $\gamma^\ast$ is updated given $c,a$ should be less than either $t_a^\ast$ or $t_c^\ast$, we define $t_{max}^\ast=max(t_a^\ast,t_c^\ast)\geq t_{min}^\ast=min(t_a^\ast,t_c^\ast)\geq t_\gamma^\ast$. Using the definition of UCB, we can bound the gap of bias by
\begin{align}
     \Delta_{bias} &\leq  3\sqrt{\frac{2ln(T)}{t_\gamma}}-3\sqrt{\frac{2ln(T)}{t^\ast_{max}}}\leq  3\sqrt{\frac{2ln(T)}{t_\gamma}} \\
     t_\gamma &\leq 18ln(T) \frac{1}{\Delta_{bias}^2}
\end{align}
\renewcommand\qedsymbol{}
\end{proof}

With ~\ref{lem:case1}, we can bound the regret bound of case (1) by:
\begin{equation}
    Reg_1=\mathbb E\lbrack t_\gamma \rbrack \Delta_{bias}\leq 18ln(T)/\Delta_{bias} = O(ln(T))
\end{equation}

Notably, cases (2) and (4) are bounded by the rapid convergence of the confidence radius of configurations, thus, we consider when the agent recommends configuration. We derive the following lemma with $s^c_t$ representing the time that the configuration arm of action $a_t$ in round $t$ has been played. 
\begin{lemma}
Following the proof in LinUCB ~\cite{chu2011contextual}, we can bound The gap between optimal and sub-optimal reward is bounded by the following equation with probability $1-\delta/T$:
 \begin{equation}
\label{eqn:semi}
    |r_{t}^*- r_{t,a_t}| \leq \alpha \sqrt{-2log(\delta/2)/s^c_t}
\end{equation}
\end{lemma}

By summing Equation ~\ref{eqn:semi} with the expectation of round $T$, we derive the regret for case (2) and (4) as
\begin{equation}
    Reg_{2,4}=O(\sqrt{Tln^3(m^2Tln(T))})
    \label{eqn:reg_comb}
\end{equation}

For case (3), we first evaluate how many rounds the agent needs to recommend a positive configuration.
% For the case (3), we first evaluate how many rounds the agent needs to recommend positive configuration.
\begin{lemma}
With overall round $T$, the expected round for attribute exploration is 
\begin{equation}
    T-\frac{k}{\Delta_c^2}ln(T) 
    \label{eqn:tbound}
\end{equation}
\end{lemma}
\begin{proof}
The rounds to reach a positive configuration depend on the expectation of rounds that recommends a negative configuration. 
Thus by following the definition of UCB, we have:
\begin{equation}
    \mathbb{E}\lbrack t \rbrack= k\frac{ln(T)}{\Delta_c^2}
\end{equation}    
\renewcommand\qedsymbol{}
\end{proof}

To calculate the regret bound of case (3), we apply the upper bound of round $t$ derived in Equation ~\ref{eqn:semi} and get:
\begin{equation}
    Reg_3=O(\sqrt{(T-ln(T))ln^3(m^2(T-ln(T))ln(T-ln(T))}))
\end{equation}


Therefore, we can get the overall regret by summing up the regret of each case:
\begin{theorem}
The regret of Hier-SUCB can be bounded as:
\begin{align}
    Reg&=Reg_1+Reg_3+Reg_{2,4}\\
    &=O(\sqrt{Tln^3(m^2T ln(T))})
\end{align}
\end{theorem}


Notably, we reduce the original semi-bandit by improving $O(nm^2)$ to $O(m^2)$ by adding a hierarchical structure and decompose the combinatorial problem to multi-arm bandits and contextual semi-bandits. Regular combinatorial contextual bandit will apply Eq.~\ref{eqn:reg_comb} to all the attributes and configurations, where the term $m^2$ would be $nm^2$ in this case. With a hierarchical structure, the regret of the configuration is bounded by a contextual bandit. 
The regret of attribute pairs can be bounded with combinatorial contextual bandits as long as the configuration is preferred.
For the case (4) where attributes and configuration are preferred but their combination is not, we model an independent bias as multi-armed bandits  whose regret bound is $O(ln(T))$.

We also model the relation between the configuration and attribute with an extra bias as an individual bandit.
This helps improve the final accuracy of the personalized visualization recommendation, which we demonstrate later in the experiments using real-world datasets. 