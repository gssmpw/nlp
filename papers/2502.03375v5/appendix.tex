\documentclass[letterpaper]{article}
% DO NOT CHANGE THIS
\usepackage[submission]{aaai24} % DO NOT CHANGE THIS
\usepackage{times} % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier} % DO NOT CHANGE THIS
\usepackage[hyphens]{url} % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
% \usepackage{subfigure}
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm} % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\usepackage{natbib} % DO NOT CHANGE THIS
\usepackage{caption} % DO NOT CHANGE THIS
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\newtheorem{lemma}{Lemma}
\usepackage{subcaption}
\usepackage{paralist}
\usepackage{appendix}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\frenchspacing % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% Keep the \pdfinfo as shown here. Thereâ€™s no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}





% \title{Interactive Personalized Visualization Recommendation}
% \date{February 2023}

\begin{document}
\appendixpage

\section{Detailed Regret Analysis}
We consider the regret of round $t$ under four cases when the user provides negative feedback to the visualization:
\begin{enumerate}
    \item Like configuration $c_t$ and attribute pair $\lbrace x_t,y_t \rbrace$ 
    \item Like attribute pair $\lbrace x_t,y_t \rbrace$ but not configuration $c_t$
    \item Like configuration $c_t$ but not attribute pair $\lbrace x_t,y_t \rbrace$
    \item Dislike configuration $c_t$ and attribute pair $\lbrace x_t,y_t \rbrace$
\end{enumerate}

For case (1), we provide the regret bound by analyzing the bias term converges in certain rounds.
\begin{lemma}
    The reward gap between optimal and sub-optimal bias $\gamma$ is bounded with the overall round $T$ and the time $t_\gamma$ that $\gamma$ has been played for.
    \begin{equation}
        \Delta_\gamma \leq \sqrt{\frac{ln(T)}{t_\gamma}}
    \end{equation}
    \label{lem:case1}
\end{lemma}
\begin{proof}
    having a positive configuration and attributes while negative visualization implies:
% Having positive configuration and attributes while negative visualization implies:
\begin{equation}
    U(c,a)\geq U(c^\ast,a^\ast)
\end{equation}
where $c^\ast, a^\ast$ refers to configuration and attributes of preferred visualization. Notably, $c,a$ may also receive positive feedback from user, but their combination is not preferred. In this case, $\Delta_c=\Delta_a=0$ and we bound the regret with bias.
\begin{equation}
    \Delta_{bias} \leq \rho_{c,t} +\rho_{a,t}+ \rho_{\gamma,t} - \rho_{c,t}^\ast -\rho_{a,t}^\ast -\rho_{t,\gamma}^\ast
\end{equation}
The round that $\gamma^\ast$ is updated given $c,a$ should be less than either $t_a^\ast$ or $t_c^\ast$, we define $t_{max}^\ast=max(t_a^\ast,t_c^\ast)\geq t_{min}^\ast=min(t_a^\ast,t_c^\ast)\geq t_\gamma^\ast$. Using the definition of UCB, we can bound the gap of bias by
\begin{align}
     \Delta_{bias} &\leq  3\sqrt{\frac{2ln(T)}{t_\gamma}}-3\sqrt{\frac{2ln(T)}{t^\ast_{max}}}\leq  3\sqrt{\frac{2ln(T)}{t_\gamma}} \\
     t_\gamma &\leq 18ln(T) \frac{1}{\Delta_{bias}^2}
\end{align}
\
\end{proof}

With \ref{lem:case1}, we can bound the regret bound of case (1) by:
\begin{equation}
    Reg_1=\mathbb E\lbrack t_\gamma \rbrack \Delta_{bias}\leq 18ln(T)/\Delta_{bias} \leq O(ln(T))
\end{equation}

Notably, cases (2) and (4) are bounded by the rapid convergence of the confidence radius of configurations, thus, we consider when the agent recommends configuration. We derive the following lemma with $s^c_t$ representing the time that the configuration arm of action $a_t$ in round $t$ has been played. 
\begin{lemma}
Following the proof in LinUCB \cite{chu2011contextual}, we can bound The gap between optimal and sub-optimal reward is bounded by the following equation with probability $1-\delta/T$:
 \begin{equation}
\label{eqn:semi}
    |r_{t}^*- r_{t,a_t}| \leq \alpha \sqrt{-2log(\delta/2)/s^c_t}
\end{equation}
\end{lemma}

By summing Equation \ref{eqn:semi} with the expectation of round $T$, we derive the regret for case (2) and (4) as
\begin{equation}
    Reg_{2,4}=O(\sqrt{Tln^3(m^2Tln(T))})
    \label{eqn:reg_comb}
\end{equation}

For case (3), we first evaluate how many rounds the agent needs to recommend a positive configuration.
% For the case (3), we first evaluate how many rounds the agent needs to recommend positive configuration.
\begin{lemma}
With overall round $T$, the expected round for attribute exploration is 
\begin{equation}
    T-\frac{k}{\Delta_c^2}ln(T) 
    \label{eqn:tbound}
\end{equation}
\end{lemma}
\begin{proof}
The rounds to reach a positive configuration depend on the expectation of rounds that recommends a negative configuration. 
Thus by following the definition of UCB we have,
\begin{equation}
    \mathbb{E}\lbrack t \rbrack= k\frac{ln(T)}{\Delta_c^2}
\end{equation}    
\end{proof}

To calculate the regret bound of case (3), we apply the upper bound of round $t$ derived in Equation \ref{eqn:semi} and get 
\begin{equation}
    Reg_3=O(\sqrt{(T-ln(T))ln^3(m^2(T-ln(T))ln(T-ln(T))}))
\end{equation}

Therefore, we can get the overall regret by summing up the regret of each case:
% Briefly, we summarize the overall regret with:
\begin{align}
    Reg&=Reg_1+Reg_3+Reg_{2,4}\\
    &=O(\sqrt{Tln^3(m^2T ln(T))})
\end{align}

\section{Hier-SUCB Algorithm}
\subsection{Pseudo-code}
\begin{algorithm}
    \SetAlgoLined
        Initialize $\theta_{c,t},\theta_{a,t}, \gamma_t$ (Eq. \ref{eqn:theta_def})\;
	\For{$t=1,2,...T$}{
	
        \For{$a_{c,t}=1,2,...n$}{
        Compute UCB $U(c_t)$ (Eq. \ref{eqn:ucfg})\;
        }
        Select $c_t=\textbf{argmax}(U(c_t))$\;
        \For{$a_{x,t}=1,2,...m$}
            {
            \For{$a_{y,t}=1,2,...m$}
                {
                Compute UCB $U(c_t,x_t,y_t)$ (Eq. \ref{eqn:uvis})\;
                }      
            }
        Select $V_{t}=\textbf{argmax}(U(c_t,x_t,y_t))$\;
        \uIf{$r_{V,t}==1$}
        {$r_{C,t}\leftarrow 1,r_{A,t}\leftarrow 1$\;}
        \Else{ask for $r_{C,t},r_{A,t}$\;}
        Update $\theta_{c,t},\theta_{a,t}, \gamma_t$ (Eq. \ref{eqn:theta_update},\ref{eqn:bias_update})\;
        Update $\rho_{c,t},\rho_{a,t}, \rho_{\gamma,t}$ (Eq. \ref{eqn:rhoca},\ref{eqn:rhobias})\;
        }
\caption{Hier-SUCB}
\end{algorithm}

\subsection{Related Equations}
\begin{align}
    \theta_{c,t}&=V_{c,t}+b_{c,t}=\mathbf I_d+\mathbf 0_d\\ 
    \theta_{a,t}&=V_{a,t}+b_{a,t}=\mathbf I_d+\mathbf 0_d \label{eqn:theta_def}  
\end{align}
\begin{equation}
    U(c_t)=\theta_{C,t}^T \mathbf x_{c,t}+\rho_{c,t}
\label{eqn:ucfg}
\end{equation}
\begin{equation}
    U(c_t,x_t,y_t)=\theta_{C,t}^T \mathbf x_{c,t}+\theta_{A,t}^T \mathbf x_{y,t}+\theta_{A,t}^T \mathbf x_{x,t} + \gamma_t + \rho_{c,t} +\rho_{a,t} +\rho_{\gamma,t}
\label{eqn:uvis}
\end{equation}
\begin{align}
    V_{c,t}&=V_{c,t-1}+\mathbf x_{c,t-1}\mathbf x_{c,t-1}^T\\
    V_{a,t}&=V_{a,t-1}+\mathbf x_{a,t-1}\mathbf x_{a,t-1}^T\\
    b_{c,t}&=b_{c,t-1}+ r_{c,t-1}\mathbf x_{c,t-1}\\
    b_{a,t}&=b_{a,t-1}+ r_{a,t-1}\mathbf x_{a,t-1}
   \label{eqn:theta_update}
\end{align}
\begin{equation}
    \gamma_t=\frac{t-1}{t}\gamma_{t-1}+\frac{1}{t}r_{\gamma,t}
    \label{eqn:bias_update}
\end{equation}
\begin{equation}
    \rho=\sqrt{\mathbf x_t^T(\mathbf I_d+\mathbf x_t \mathbf x_t^T) \mathbf x_t}
    \label{eqn:rhoca}
\end{equation}
\begin{equation}
    \rho_{\gamma,t}=\sqrt{\frac{2ln(T)}{t_\gamma}}
    \label{eqn:rhobias}
\end{equation}
\section{Long Tail Effect}
The distribution of the number of attribute in Plot.ly-1k and Plot.ly-full is shown in Fig.\ref{fig:hist}:
\begin{figure}
	\centering
	\begin{subfigure}{0.48\columnwidth}
		\centering
		\includegraphics[width=\textwidth]{image/hist.pdf}
		\caption{Plot.ly-full}
	\end{subfigure}
	\begin{subfigure}{0.48\columnwidth}
    	\centering
    	\includegraphics[width=\textwidth]{image/hist_1k.pdf}
    	\caption{Plot.ly-1k}
	\end{subfigure}
	\caption{The distribution of attribute number in the datesets of Plot.ly-full Plot.ly-1k} 
	\label{fig:hist}
\end{figure}

\bibliography{ref.bib}
\end{document}
