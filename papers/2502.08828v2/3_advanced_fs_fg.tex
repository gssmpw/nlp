\section{Reinforcement Learning Perspective for Data-Centric AI}

Reinforcement learning (RL) guides feature selection and generation through a reward mechanism. Its exploratory search strategy closely aligns with the iterative process used by human experts to refine features, enabling it to effectively discover the most discriminative feature combinations (see \textbf{Figure~\ref{fig:feat}}).

\subsection{RL for Feature Selection}
RL formulates feature selection as a sequential decision process, where an agent selects informative features to maximize a reward based on model performance and feature relevance. Existing works can be categorized into three-fold: \textbf{Multi-Agent RL Frameworks}, \textbf{Single-Agent RL Frameworks}, and \textbf{Hybrid and Specialized RL Approaches}. 

The multi-agent framework enables more efficient large-scale feature selection tasks through parallel exploration. \cite{liu2019automating} proposed a framework where each feature is assigned an independent agent, leveraging advanced state representations, such as statistical summaries and Graph Convolutional Networks (GCNs), to improve feature selection intelligence. Furthermore, \cite{liu2021automated} optimized the reward mechanism to improve collaboration among agents. To further reduce computational resource requirements, \cite{fan2021autogfs} cluster similar features, and each group is managed by a single agent. This approach maintains scalability while significantly reducing computational overhead.

However, the multi-agent framework faces the challenge of high computational costs. To address this issue, \cite{zhao2020simplifying} proposed a single-agent approach, integrating all decisions into a single agent to enable sequential feature exploration, thereby reducing computational overhead. \cite{liu2021efficient} further improved efficiency by combining the single-agent framework with Monte Carlo methods and early stopping. Additionally, \cite{wang2024knockoff} introduced knockoff features to guide exploration, achieving more robust and effective feature selection.

Hybrid and specialized RL methods integrate RL with other techniques to adapt it to specific domains. \cite{fan2020autofs} incorporated an external trainer within the RL framework to guide RL agents, enhancing the breadth of exploration. \cite{xiao2025} leveraged biological domain knowledge to guide biomarker identification. \cite{fan2021interactive} utilized decision tree feedback to model the hierarchical structure of features, improving state representation and personalizing feature selection.

In conclusion, RL-driven feature selection provides a novel approach to tackling the challenges of high-dimensional and complex feature spaces. Through multi-agent collaboration, single-agent optimization, and hybrid integrations, these methods showcase RL's adaptability in refining feature selection processes, enhancing efficiency, and ultimately boosting machine learning model performance.


\begin{figure}[t]
  \centering
  \subfigure[\small{Feature Selection}]{
    \includegraphics[width=0.227\textwidth]{figures/Feature-Selection.png}
  }
  \hfill
  \subfigure[\small{Feature Generation}]{
    \includegraphics[width=0.227\textwidth]{figures/Feature-Generation.png}
  }
  \vspace{-0.3cm}
  \caption{Feature selection and feature generation are formulated as RL problems.}
  \vspace{-0.3cm}
  \label{fig:feat}
\end{figure}

\subsection{RL for Feature Generation}
RL-based feature generation models feature construction as a sequential decision process, where an agent selects existing features, applies mathematical transformations, and optimizes the generated features for downstream tasks. Existing works can be categorized into \textbf{cascading frameworks}, \textbf{graph-based exploration}, and \textbf{hybrid and domain-adapted techniques}.
The cascaded RL framework formulates the feature generation process as a sequence of dependent decision steps, where each step builds upon the previous one.
\cite{wang2022group} introduced a three-agent cascaded framework, where two agents select features and one agent chooses an operator, iteratively constructing new features. Expanding on this idea, \cite{hu2024reinforcement} applied cascaded RL to polymer property prediction, using this structured approach to generate meaningful descriptors.

Graph-based methods leverage feature relationships to optimize feature generation. \cite{huang2024enhancing} constructs a feature state transition graph to track valuable transformations, enhancing the RL exploration strategy. \cite{ying2024topology} extracts core data and encodes features using graph neural networks, ensuring that latent structural information within the data is captured, thereby improving feature transformation capabilities.

Hybrid methods integrate RL with other relevant techniques to address domain-specific challenges. \cite{ying2023self} proposed a hierarchical reinforcement feature interaction approach, incorporating feature discretization, hashing, and descriptive summaries to enhance feature generation quality. \cite{xiao2022self} employed a self-optimizing framework to mitigate the issue of overestimated Q-values. \cite{zhang2024tfwt} leveraged a Transformer-based attention to capture feature dependencies, reducing data redundancy.


In conclusion, RL-based feature generation introduces a dynamic and adaptive approach to constructing features. By integrating cascaded decision-making, structural learning, and hybrid methodologies, these frameworks showcase RL's ability to automate and optimize feature engineering, paving the way for more efficient and insightful data representations.





\section{Generative Perspective for Data-Centric AI}
Generative AI, with its ability to encapsulate knowledge within a latent space, offers a systematic approach to compress feature information and generate enhanced feature representations. It can learn hidden patterns from high-dimensional, sparse, or complex non-linear tabular data, producing more expressive features than traditional methods.

\begin{figure}[t]
    \centering
\includegraphics[width=1.0\linewidth]{figures/Generative-AI.png}
    \vspace{-0.3cm}
    \caption{Generative AI captures feature knowledge in a continuous embedding space, enabling gradient-based search to identify optimal feature sets.}
    \label{fig:generative AI}
    \vspace{-0.3cm}
\end{figure}

\subsection{Generative AI for Feature Selection} 
Generative AI redefines feature selection by transforming it from a discrete search problem into an optimization problem in continuous space. As shown in \textbf{Figure~\ref{fig:generative AI}}, it embeds empirical feature selection behaviors as knowledge into a continuous space, and then generates selection decisions based on this space, thereby constructing a more effective feature selection framework.

~\cite{GAINS} introduces an encoder-decoder-evaluator framework that leverages reinforcement learning to automatically collect observed feature selection experiences, then uses an encoder to embed them into a continuous space, and finally generates new feature selection results by a decoder. This framework ensures the diversity of observed data and enhances the effectiveness of feature selection tasks using generative models.

~\cite{VTFS} redefines feature selection as a sequential token generation task based on the encoder-decoder-evaluator paradigm. This framework incorporates a transformer-based variational autoencoder (VAE), effectively capturing complex dependencies among features while mitigating model overfitting and sensitivity to noise.

Building upon this, ~\cite{FNFS} further investigates feature redundancy in high-dimensional feature spaces. This framework evaluates feature orthogonality and integrates it into the generative framework, identifying a more compact but more precise feature subset while minimizing redundancy in the selection results.

Moreover, generative AI-based feature selection has been applied to specific domains. ~\cite{GERBIL} identifies key biomarkers in high-dimensional, low-sample-size biological gene datasets, enhancing the effectiveness of early disease detection through genomic analysis. This highlights the transformative potential of generative AI in healthcare, where precise feature selection can directly improve disease prediction and reduce healthcare costs.

In conclusion, generative AI methods significantly enhance the effectiveness, stability, and generalization capability of feature selection while demonstrating substantial potential across various application domains. These advancements pave the way for broader research and applications in high-dimensional data environments.






\subsection{Generative AI for Feature Generation}
Generative AI constructs an optimal feature space through the embedding-optimization-generation paradigm. The core idea is to embed generated feature sets as observed feature knowledge into a continuous space to learn feature interactions, optimize within this space, and ultimately generate an optimal set of features. This paradigm has demonstrated significant potential in feature generation tasks, effectively improving model performance while reducing manual intervention.

~\cite{zhu2022difer} randomly generates generated feature sets as observed feature knowledge to guide feature generation. However, the poor performance of randomly generated observations leads to ineffective embeddings, while the independent handling of embedding and generation processes introduces noise and errors. Additionally, manually setting the number of generated features and using a greedy search strategy results in suboptimal generation outcomes.

Building on this, ~\cite{NIPS@MOAT} incorporates RL into the framework. RLâ€™s exploration-exploitation mechanism is used to automatically generate high-quality observation data, which is encoded using postfix expressions to optimize the construction of the embedding space. Within this space, gradients are used to locate the optimal embeddings, enabling the generation of superior feature generations. However, this framework heavily relies on feedback from downstream predictive tasks to search for optimal results, making it time-consuming and limiting its applicability in unsupervised learning scenarios.

To address this limitation, ~\cite{KDD@NEAT} proposes an unsupervised feature generation framework. This approach represents each observed generated feature set as a feature-feature similarity graph and employs graph contrastive learning and multi-objective fine-tuning to optimize the embedding space, thereby decoupling feature generation from downstream predictive tasks. However, the effectiveness of this framework is constrained by graph augmentation strategies and manually designed unsupervised utility evaluation metrics.

Recently, advanced methods leveraging large language models (LLMs) have expanded the possibilities of feature generation. ~\cite{zhang2024dynamic} introduces a dynamic and adaptive automatic feature generation process using LLMs. This method reconstructs the feature space based on feedback from downstream tasks, utilizing LLMs' context learning and reasoning capabilities to dynamically adjust and optimize the generated features across various machine learning scenarios.

Furthermore, ~\cite{gong2024evolutionary} proposes an evolutionary LLM framework for automated feature generation. This method formulates feature generation as a sequence generation task, integrating an RL-based data collector to construct a multi-population database. The LLM leverages few-shot learning to generate improved feature generation sequences, while evolutionary algorithms (EAs) further refine the search path, enhancing both the quality and generalizability of the feature generations.

In conclusion, generative AI has achieved significant advancements in feature generation, substantially improving the efficiency and quality of feature modeling. These methods address the inefficiencies of discrete search spaces and enhance feature interaction modeling, enabling AI to exhibit greater adaptability. Future research directions could explore smarter adaptive optimization strategies, integrate multi-modal data (e.g., text, images, time series) for feature generation, and further enhance the automation and generalization capabilities of feature engineering.





