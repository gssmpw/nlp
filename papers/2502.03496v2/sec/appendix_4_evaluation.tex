\section{Experimental details}
\label{appendix:setting}
Three open-sourced text-to-video models are used as the base models for evaluation: They are AnimateDiff~\citep{guo2023animatediff}, ModelScope~\citep{wang2023modelscope,VideoFusion}, and VideoCrafter~\citep{chen2023videocrafter1}.
\begin{itemize}
    \item For AnimateDiff, we use mm-sd-v15\_v2 motion module along with realisticVisionV20\_v20 dreambooth LoRA~\footnote{https://huggingface.co/ckpt/realistic-vision-v20/blob/main/realisticVisionV20\_v20.safetensors}, sampling 16 frames of at a resolution of $512\times512$ at 8 FPS with a guidance scale of 7.5.
    \item For ModelScope, we utilize the modelscope-damo-text-to-video-synthesis version, sampling 16 frames at a resolution of $256 \times 256$ at 8 FPS, with a guidance scale of 9.
    \item For VideoCrafter, we employ the VideoCrafter-v1 base text-to-video model, sampling 16 frames at a resolution of  $320 \times 320$ at 10 FPS, with a guidance scale of 12.
\end{itemize}


\section{Evaluation metrics}
\label{appendix:vbench}
% vbench 
We employ VBench~\citep{huang2023vbench} for evaluation, a comprehensive benchmark designed with tailored prompts and evaluation dimensions specifically aimed at assessing video generation performance.
A key feature of VBench is its incorporation of human preference annotations, ensuring alignment with human perception.
VBench uses a hierarchical and disentangled scoring system, breaking the overall \textbf{\textit{total score}} into two main components: \textbf{\textit{quality score}} and \textbf{\textit{semantic score}}.
It covers 16 evaluation dimensions, with 7 contributing to \textbf{\textit{quality score}} and 9 contributing to \textbf{\textit{semantic score}}. 
Each dimension is assessed using a specially designed approach, ensuring precise and meaningful evaluation of the generated videos. The assessments involve various off-the-shelf models~\citep{caron2021dino, ruiz2023dreambooth, radford2021clip, li2023amt, teed2020raft, laion2022aesthetic,ke2021MUSIQ,wu2022GRiT, li2023umt,huang2023t2i-compbench,huang2024tagtext, wang2024internvid},  and the score for each dimension is normalized on a 0 to 100 scale, based on empirical minimum and maximum values.
\begin{itemize}
    \item \textbf{\textit{Quality score}} is calculated as the weighted average of seven dimensions: {\it subject consistency}, {\it background consistency}, {\it temporal flickering}, {\it motion smoothness}, {\it dynamic degree}, {\it aesthetic quality}, and {\it imaging quality}. The weight for {\it aesthetic quality} is set to 2, while the other dimensions carry a weight of 1. 
    % The rationale behind this is that there are several dimensions related to temporal consistency, while \textit{dynamic degree} is the only dimension that specifically measures motion dynamics.
    \item \textbf{\textit{semantic score}} is calculated as the weighted average of nine dimensions: {\it object class}, {\it multiple objects}, {\it human action}, {\it color}, {\it spatial relationship}, {\it scene},{\it appearance style}, {\it temporal style}, and {\it overall consistency}, with each dimension equally weighted 1.
\end{itemize}
 
After calculating \textbf{\textit{quality score}} and \textbf{\textit{semantic score}}, \textbf{\textit{total score}} is calculated as follows:
\begin{equation}
    \mathrm{Total} = \frac{w_q}{w_q+w_s}\mathrm{Quality}+\frac{w_s}{w_q+w_s}\mathrm{Semantic},
\end{equation}
where $w_q$ and $w_s$ are $4$ and $1$ respectively by default.

\section{Visualization results}
\paragraph{More qualitative results.} 
Additional qualitative results are presented in Figure~\ref{fig:appendix_vis}. 
The videos generated using our noise prior exhibit superior video quality, in terms of imaging details, aesthetic aspects, and semantic coherence.
\paragraph{Visualization of the influence of timestep \boldmath$t$.} 
As illustrated in Figure~\ref{fig:ablation_timestep},  the first three rows of frames, which correspond to different timesteps $t$, are almost the same.
In the fourth case, there are some differences in the representation of the grape stem, highlighted by a red box. The stem is absent at the timestep of 321. 
At the timestep of 321, the stem is missing. 
The final case demonstrates more notable differences; as the timestep $t$ increases, the ice cream appears to melt, and the changes are observable on the table.
The visualizations suggest two key points: first, in most instances, the timestep 
$t$ has minimal impact on the overall generation results; second, although the content and layout of the video frames remain largely unchanged, the timestep can indeed influence the finer imaging details.
In general, the differences are quite minor, which means we can save much time by diffusing the latent at intermediate timestep during the noise refinement stage without compromising the quality of generation results.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figure/ablation_timestep.pdf}
    \caption{ \textbf{Visualization results of the influence of timestep.} We present five cases where other settings are fixed to isolate the effects of varying timestep $t$. Overall, timestep $t$ has minimal impact on the generation outcomes. However, it does exert some influences on the imaging details occasionally. For the fourth and fifth cases, the red boxes highlight the differences.}
    \label{fig:ablation_timestep}
\end{figure}

\section{Limitations}
While our method enhances consistency and smoothness in videos generated from Gaussian noise, it can occasionally result in unnatural smoothness that does not align with the laws of physics. 
Additionally, although our approach improves overall performance, it may alter the content layout of video frames compared to Gaussian noise. For real images, low-frequency signals typically dictate layouts; however, this is not always true for the noise prior in diffusion models. 
Our method refines the Gaussian noise prior using a novel frequency filtering technique, which usually preserves the structural similarity to the original Gaussian noise. Nonetheless, in some cases, the generated videos can differ significantly. During filtering, high-frequency components from other Gaussian noise may subtly change the structure of the Gaussian noise prior, resulting in variations in the content and layouts of the generated videos.


\section{Broader impacts}
\label{appendix:impacts}
This work aims to propose a novel prior by refining initial Gaussian noise to enhance the quality of video generation. 
Text-to-video diffusion models hold the potential to revolutionize media creation and usage. While these models offer vast creative opportunities, it is crucial to address the risks of misinformation and harmful content. 
Before deploying these models in practice, it is essential to thoroughly investigate their design, intended applications, safety aspects, associated risks, and potential biases.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figure/visualization_appendix.pdf}
    \caption{{\bf More qualitative results.}}
    \label{fig:appendix_vis}
\end{figure}
