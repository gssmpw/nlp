\section{Related work}
\label{sec:realted}
\paragraph{Video generative models}
In the field of video generation, previous work has explored a range of methods, including VAEs~\citep{diederik2014vae,Hsieh2018DDPAE,sarthak2020guassianVae}, 
GANs~\citep{goodfellow2014gan,tian2021mocogan-hd,brooks2022dynamicSenes,Skorokhodov2022stylegan-v}, and auto-regressive models~\citep{wu2021godiva,chenfei2022nuwa,Songwei2022longvideo,wenyi2023cogvideo}. 
Recently, diffusion models~\citep{ho2020denoising,song2021scorebased,jascha2015nonequilibrium,dhariwal2021diffusionBeatsGan} have showcased great abilities in image synthesis~\citep{Rombach2022SD,Saharia2022imagen,Alexander2022glide}, and pave the way towards video generation~\citep{ho2022vdmodels,he2022latentVD,voleti2022mcvd}.
Many recent works~\citep{ho2022imagenvideo,Blattmann2023align,ge2023PYoCo,guo2023animatediff,wang2023modelscope,chen2023videocrafter1} on video synthesis are text-to-video diffusion paradigm with text as a highly intuitive and informative instruction.
Both ModelScope~\citep{wang2023modelscope,VideoFusion} and VideoCrafter~\citep{chen2023videocrafter1} are built upon on the UNet~\citep{Olaf2015unet} architecture.
VideoCrafter adds a temporal transformer after a spatial transformer in each block, while in ModelScopoe each block comprises spatial and temporal convolution layers, along with spatial and temporal attention layers.
AnimateDiff~\citep{guo2023animatediff} generates videos by integrating Stable Diffusion~\citep{Rombach2022SD} with motion modules. 

\paragraph{Noise prior for diffusion models}
Given inherent high correlations within video data, several studies~\citep{ge2023PYoCo,qiu2023freenoise,chang2024warp,gu2023reuse,mao2024lottery,wu2023freeinit} have delved into the realm of noise prior within diffusion models.
Both FreeNoise~\citep{qiu2023freenoise} and VidRD~\citep{gu2023reuse} focus on initialization strategies for long video generation, with FreeNoise employing a shuffle strategy to create noise sequences with long-range relationships, while VidRD utilizes the latent feature of the initial video clip.
$\int$-noise prior interprets noise as a continuously integrated noise field rather than discrete pixel values~\citep{chang2024warp}. However, it focuses on low-level features, making it more suitable for tasks such as video restoration and video editing. 
\citet{mao2024lottery} identifies that some pixel blocks of initial noise correspond to certain concepts, enabling semantic-level generation. Nevertheless, collecting these blocks for different concepts is time-consuming, which limits its practical application.
Motivated by correlations in the noise maps corresponding to different frames, PYoCo~\citep{ge2023PYoCo} carefully designs mixed noise prior and progressive noise prior. 
FreeInit~\citep{wu2023freeinit} identifies signal leakage in the low-frequency domain and uses Fourier transform to refine the noise, making the initial exploration of frequency operations in the noise prior.
However, noise is essentially different from signals, making the classic frequency filtering method unsuitable. As a result, 
the generated videos lack motion dynamics and imaging details due to the variance decay issue.
To address these limitations, we propose a novel prior to enhance the overall quality of generated videos.
