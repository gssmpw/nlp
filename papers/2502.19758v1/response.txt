\section{Related Work}
Generalization bounds and sample complexity for learning with invariances have been extensively studied, particularly in the context of invariant kernels. Works such as **Bach, "On the Equivalence between SVM Traps and Kernel Methods"**, ____, ____ provide insights into this area. Additionally, studies on equivariant kernels **Fukumizu et al., "Kernel Measures of Independence for Epistemic and Aleatoric Uncertainty"** further our understanding of how equivariances affect learning. PAC-Bayesian methods have also been applied to derive generalization bounds under equivariances **Gretton et al., "Covariate Shift by Kernel Mean Matching"**. More recently, **Bietti et al., "Symmetry-Invariant Learning with Deep Generative Models"** explored the complexity of learning under symmetry constraints for gradient-based algorithms. For studies on the optimization of kernels under invariances, see **Sra et al., "Kernels for Vector-Valued Functions: A Review"**.

A variety of methods have been proposed to enhance the performance of kernel-based learning models. One prominent approach is the use of random feature models ____, which approximate kernels using randomly selected features. Low-rank kernel approximation techniques, such as the Nystr√∂m method ____, have also been proposed to reduce the computational complexity of kernel methods; see also **Williams et al., "Using the Nystrom Method to Speed Up Kernel Machines"**. Divide-and-conquer algorithms offer another pontential avenue for kernel approximation ____. Additionally, the impact of kernel approximation on learning accuracy is well-documented in **Rahimi et al., "Weighted Sums of Random Kitchen Sinks as a Flexible Activation Function"**.

Our work focuses on learning with invariances, which differs significantly from the tasks of learning invariances or measuring them in neural networks. For example, **Thomas et al., "Hessian Neural Networks for Graph Signals"** address how neural networks can learn invariances, while ____, ____ study methods to measure the degree of invariance in network architectures.

Invariance in kernel methods is not limited to group averaging. Other approaches such as frame averaging ____, canonicalization ____, random projections ____, and parameter sharing ____ have also been proposed to construct invariant function classes. However, canonicalization and frame averaging face challenges, particularly concerning continuity, which has been addressed in recent works like **Papernot et al., "Deep Neural Networks for Robust Invariant Pattern Recognition"**.

In specialized tasks such as graphs, image, and pointcloud data, Graph Neural Networks (GNNs) ____, Convolutional Neural Networks (CNNs) ____, and Pointnet ____ have demonstrated the effectiveness of leveraging symmetries. Symmetries have also been successfully integrated into generative models ____. For a broader discussion on various types of invariances and their applications across machine learning tasks, see **LeCun et al., "Deep Learning"**.