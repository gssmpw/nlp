%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%NEW PACKAGES


% \documentclass[11pt]{article}
% \usepackage{palatino}
% \usepackage[margin = 1in]{geometry}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for 

% \newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{tcolorbox}

\usepackage[textsize=tiny]{todonotes}




\usepackage{enumitem}  
% \usepackage[numbers]{natbib}

\usepackage{bbm}
\usepackage{thmtools,thm-restate}

%\input{notation}

\usepackage{booktabs}
\usepackage{threeparttable}

\definecolor{darkblue}{rgb}{0.0,0.0,0.65}
\definecolor{darkred}{rgb}{0.68,0.05,0.0}
\definecolor{darkgreen}{rgb}{0.0,0.29,0.29}
\definecolor{darkpurple}{rgb}{0.47,0.09,0.29}
 
\usepackage[backref=page]{hyperref}
\hypersetup{
   colorlinks = true,
   citecolor  = darkblue,
   linkcolor  = darkred,
   filecolor  = darkblue,
   urlcolor   = darkblue,
 }
\usepackage[capitalize,noabbrev]{cleveref}


\usepackage{cleveref}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\declaretheorem[name=Theorem,style=plain]{thm}


\input{notation}
\input{math_commands}


% macros

\newcommand{\defeq}[0]{\overset{\mbox{\normalfont\tiny def}}{=}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[arxiv]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2025}
 


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\theoremstyle{plain}
%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{corollary}[theorem]{Corollary}
%\theoremstyle{definition}
%\newtheorem{definition}[theorem]{Definition}
%\newtheorem{assumption}[theorem]{Assumption}
%\theoremstyle{remark}
%\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Learning with Exact Invariances in Polynomial Time}

\begin{document}

\twocolumn[
\icmltitle{Learning with Exact Invariances in Polynomial Time}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Ashkan Soleymani}{equal,qqq}
\icmlauthor{Behrooz Tahmasebi}{equal,yyy}
\icmlauthor{Stefanie Jegelka}{zzz,yyy}
\icmlauthor{Patrick Jaillet}{qqq}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{zzz}{School of CIT, MCML, and MDSI, Technical University of Munich (TUM)}
\icmlaffiliation{yyy}{MIT EECS and MIT CSAIL}
\icmlaffiliation{qqq}{MIT EECS and MIT LIDS}

% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Ashkan Soleymani}{ashkanso@mit.edu}
\icmlcorrespondingauthor{Behrooz Tahmasebi}{bzt@mit.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
 



\begin{abstract}
  We study the statistical-computational trade-offs for learning with exact invariances (or symmetries) using kernel regression. Traditional methods, such as data augmentation, group averaging, canonicalization, and frame-averaging, either fail to provide a polynomial-time solution or are not applicable in the kernel setting. However, with oracle access to the geometric properties of the input space, we propose a polynomial-time algorithm that learns a classifier with \emph{exact} invariances. Moreover, our approach achieves the same excess population risk (or generalization error) as the original kernel regression problem. To the best of our knowledge, this is the first polynomial-time algorithm to achieve exact (not approximate) invariances in this context. Our proof leverages tools from differential geometry, spectral theory, and optimization. A key result in our development is a new reformulation of the problem of learning under invariances as optimizing an infinite number of linearly constrained convex quadratic programs, which may be of independent interest. 
\end{abstract}


\section{Introduction}



While humans can readily observe symmetries or invariances in systems, it is generally challenging for machines to detect and exploit these properties from data. The objective of machine learning with invariances is to develop approaches that enable models to be trained and utilized under the symmetries inherent in the data. This framework is broadly applicable across various domains in the natural sciences and physics, including atomistic systems \citep{grisafi2018symmetry}, molecular wavefunctions and electronic densities \citep{unke2021se}, interatomic potentials \citep{batzner20223}, and beyond \citep{batzner2023advancing}. While many applications involve Euclidean symmetries \citep{smidt2021euclidean}, the scope of such methods extends well beyond them to other geometries \citep{bronstein2017geometric}.

Learning with invariances has a longstanding history in machine learning \citep{hinton1987learning,kondor2008group}. In recent years, there has been significant interest in the development and analysis of learning methods that account for various types of invariances. This surge in interest is strongly motivated by many models showing considerable success in practice. Empirical evidence suggests the existence of algorithms that can effectively learn under invariances while exhibiting strong generalization and computational efficiency. However, from a theoretical perspective, much of the focus has been on the expressive power of models, generalization bounds, and sample complexity. There remains a relative lack of understanding regarding the statistical-computational trade-offs in learning under invariances, even in foundational settings such as kernel regression.

% Kernels, which have been among popular learning approaches, offer both statistical and computational efficiency \citep{scholkopf2018learning}. Symmetries can be included in kernel learning in various ways. An immediate solution for learning with invariances seems to be \emph{data augmentation} over the elements of the group. Moreover, most kernel-based approaches to learning with invariances rely on \emph{group averaging}, a technique that involves summing over group elements. However, the typically large size of the group can make both of these approaches computationally prohibitive, even super-exponential in the dimension of input data. Alternative approaches, such as \emph{canonicalization} and \emph{frame averaging}, also suffer from issues like discontinuities and scalability challenges \citep{dym2024equivariant}. 

Symmetries can be incorporated into learning in multiple ways. An immediate solution for learning with invariances seems to be \emph{data augmentation} over the elements of the group. Moreover, some approaches to learning with invariances rely on \emph{group averaging}, a technique that involves summing over group elements. However, the typically large size of the group can make both of these approaches computationally prohibitive, even super-exponential in the dimension of input data. Alternative approaches, such as \emph{canonicalization} and \emph{frame averaging}, also suffer from issues like discontinuities and scalability challenges \citep{dym2024equivariant}.  

This paper seeks to address the following question:

% \begin{tcolorbox} Is it possible to obtain an invariant estimator for the kernel regression problem that exhibits both strong generalization capabilities and computational efficiency? \end{tcolorbox}

\begin{tcolorbox} Can we obtain an invariant estimator for learning with invariances that achieves both strong generalization and computational efficiency? \end{tcolorbox}





The first contribution of this work is a detailed study of the problem of learning with invariances in the context of kernel methods.
Kernels, which have been among popular learning approaches, offer both statistical and computational efficiency \citep{scholkopf2018learning}. We argue that while group averaging fails to produce exactly invariant estimators within a computationally efficient time frame, alternative algorithms can generate invariant estimators for the kernel regression problem in time that is polylogarithmic in the size of the group. In other words, we demonstrate that it is possible to achieve an invariant estimator that is both computationally efficient and exhibits strong generalization. At first glance, this result may seem counterintuitive and even impossible, since it implies that enumerating all possible invariances is not required to design statistically efficient learning algorithms. This provides theoretical support for the empirical observation that computational efficiency and strong generalization are attainable in learning with invariances.
To the best of our knowledge, this is the first algorithm that is both statistically and computationally efficient for learning with invariances in the kernel setting, 







Learning with invariances can be formulated as a \emph{nonconvex optimization} problem, which is not tractable directly. To design an efficient algorithm, we leverage the spectral theory of the Laplace-Beltrami operator on manifolds. Notably, since this operator commutes with all (isometric) group actions on the manifold, it is possible to find an orthonormal basis of Laplacian eigenfunctions such that each group action on the manifold acts on the eigenspaces of the Laplacian via orthogonal matrices. This theoretical framework allows us to reformulate the original problem of learning with invariances on manifolds as an \emph{infinite collection of finite-dimensional convex quadratic programs}--one for each eigenspace--each \emph{constrained by linear conditions}. By truncating the number of quadratic programs solved, we can efficiently approximate solutions to the primary nonconvex optimization problem, thereby approximating kernel solutions to the problem of learning with invariances. This reformulation not only enables us to derive a polynomial-time algorithm for kernel regression under invariances, but it may also have broader applications, the exploration of which we defer to future research.

Finally, we emphasize again that this work is centered on achieving \emph{exact} invariance, as many applications—especially neural networks with strong empirical performance—are explicitly designed to incorporate exact invariances by construction. In summary, this paper makes the following  contributions:

\vspace{-0.2cm}
\begin{itemize} 
\setlength{\itemindent}{-5pt}
\setlength\itemsep{-0.1em}
\item We initiate the exploration of statistical-computational trade-offs in the context of learning with exact invariances, focusing specifically on kernel regression.

\item We reformulate the problem of learning under invariances in kernel methods, leveraging differential geometry and spectral theory, and cast it as infinitely many convex quadratic programs with linear constraints, for which we derive an efficient solution in terms of time complexity. We trade off computational and statistical complexity by controlling the number of convex quadratic programs solved to obtain the estimator.

\item We introduce the first polynomial algorithm for learning with invariances in the general setting of kernel regression over manifolds. 

\end{itemize}




\section{Related Work}


Generalization bounds and sample complexity for learning with invariances have been extensively studied, particularly in the context of invariant kernels. Works such as \citet{elesedy2021provably}, \citet{bietti2021sample}, \citet{tahmasebi2023exact}, and \citet{mei2021learning} provide insights into this area. Additionally, studies on equivariant kernels \citep{elesedy2021provablyinv, petrache2023approximation} further our understanding of how equivariances affect learning. PAC-Bayesian methods have also been applied to derive generalization bounds under equivariances \citep{behboodi2022pac}. More recently, \citet{kiani2024on} explored the complexity of learning under symmetry constraints for gradient-based algorithms. For studies on the optimization of kernels under invariances, see \citet{teo2007convex}.

A variety of methods have been proposed to enhance the performance of kernel-based learning models. One prominent approach is the use of random feature models \citep{rahimi2007random}, which approximate kernels using randomly selected features. Low-rank kernel approximation techniques, such as the Nyström method \citep{williams2000using, drineas2005nystrom}, have also been proposed to reduce the computational complexity of kernel methods; see also \citet{bach2013sharp, cesa2015complexity}. Divide-and-conquer algorithms offer another pontential avenue for kernel approximation \citep{zhang2013divide}. Additionally, the impact of kernel approximation on learning accuracy is well-documented in \citet{cortes2010impact}.

Our work focuses on learning with invariances, which differs significantly from the tasks of learning invariances or measuring them in neural networks. For example, \citet{benton2020learning} address how neural networks can learn invariances, while \citet{goodfellow2009measuring} study methods to measure the degree of invariance in network architectures.

Invariance in kernel methods is not limited to group averaging. Other approaches such as frame averaging \citep{puny2021frame}, canonicalization \citep{kaba2023equivariance, ma2024canonization}, random projections \citep{dym2024low}, and parameter sharing \citep{ravanbakhsh2017equivariance} have also been proposed to construct invariant function classes. However, canonicalization and frame averaging face challenges, particularly concerning continuity, which has been addressed in recent works like \citet{dym2024equivariant}.

In specialized tasks such as graphs, image, and pointcloud data, Graph Neural Networks (GNNs) \citep{scarselli2008graph, xu2018powerful}, Convolutional Neural Networks (CNNs) \citep{krizhevsky2012imagenet, li2021survey}, and Pointnet \citep{qi2017pointnet, qi2017pointnetplus} have demonstrated the effectiveness of leveraging symmetries. Symmetries have also been successfully integrated into generative models \citep{bilovs2021scalable, niu2020permutation, kohler2020equivariant}. For a broader discussion on various types of invariances and their applications across machine learning tasks, see \citet{bronstein2017geometric}.





 \section{Background and Problem Statement}

\textbf{Notation.} We begin by establishing some frequently used notation. Let \(\mathcal{M}\) be a smooth, compact, and boundaryless \(d\)-dimensional Riemannian manifold.
 The uniform distribution over the manifold is the normalized volume element corresponding to its metric. We denote the space of square-integrable functions over \(\mathcal{M}\) by \(L^2(\mathcal{M})\) and the space of continuous functions by \(C(\mathcal{M})\). Furthermore, \(H^s(\mathcal{M})\) represents the Sobolev space of functions on \(\mathcal{M}\) with parameter \(s\), defined as the set of functions with square-integrable derivatives up to order \(s\). Larger values of \(s\) correspond to greater smoothness, and it holds that \(H^s(\mathcal{M}) \subseteq C(\mathcal{M})\) if and only if \(s > d/2\), a condition we will assume throughout this paper. For each \(n \in \mathbb{N}\), we define \([n] \coloneqq  \{1, 2, \ldots, n\}\). We use $\log$ to denote the logarithm with base 2. We refer to \Cref{app:manifold,app:functional_manifold} for a quick review of Riemannian manifolds.

\textbf{Problem statement.} We consider a general learning setup on a smooth, compact, and boundaryless Riemannian manifold \(\mathcal{M}\) of dimension \(d\). Our objective is to identify an estimator \(\widehat{f} \in \mathcal{F}\) from a feasible space of estimators \(\mathcal{F} \subseteq L^2(\mathcal{M})\), based on \(n\) independent and uniformly\footnote{We assume uniformity for simplicity, with results extending to non-uniform sampling with bounded density.} distributed labeled samples \(\mathcal{S} = \{(x_i, y_i) : i \in [n]\} \subseteq (\mathcal{M} \times \mathbb{R})^n\) drawn from the manifold. Here, the labels \(y_i\) for \(i \in [n]\) are produced based on the (unknown) ground truth regression function \(f^\star \in C(\mathcal{M})\), meaning that \(y_i = f^\star(x_i) + \epsilon_i\), for each $i \in [n]$,  where \(\epsilon_i\), $i \in [n]$, is a sequence of independent zero-mean random variables with variance bounded by \(\sigma^2\).
The population risk (or generalization error) of an estimator \(\widehat{f} \in L^2(\mathcal{M})\), which quantifies the quality of the estimation, is defined as:
\[
    \mathcal{R}(\widehat{f})\coloneqq 
    \mathbb{E}\left[\|\widehat{f} - f^\star\|^2_{L^2(\mathcal{M})}\right],
\]
where the expectation is taken over the randomness of the data and labels.

Given a dataset of size \(n\), finding estimators with minimal population risk can be quite complex, often requiring the resolution of non-convex optimization objectives. However, in scenarios where \(f^\star \in \mathcal{H}\), with \(\mathcal{H} \subseteq L^2(\mathcal{M})\) being a Reproducing Kernel Hilbert Space (RKHS), it is feasible to compute kernel-based estimators with low risk efficiently. Specifically, the Kernel Ridge Regression (KRR) estimator for the RKHS \(\mathcal{H} = H^s(\mathcal{M})\), denoted as \(\widehat{f}_{\operatorname{KRR}}\), achieves a population risk of \(\mathcal{R}(\widehat{f}_{\operatorname{KRR}}) = \mathcal{O}\left(n^{-s/(s + d/2)}\right)\) while being computable in time \(\mathcal{O}(n^3)\), assuming access to an oracle that computes the kernel associated with the space. Note that Sobolev spaces \(H^s(\mathcal{M})\) with \(s > d/2\) are RKHS. We refer the reader to \cref{app:sob_kernel,app:krr} for a detailed review of the KRR estimator and related topics on Sobolev spaces.

\textbf{Learning with invariances.} We assume that a finite group \(G\) acts smoothly and isometrically\footnote{The assumption of isometric action is made for simplicity; the proof can be extended to non-isometric actions using standard techniques in the literature, as discussed in \cite{tahmasebi2023exact}. 
} on the manifold \(\mathcal{M}\), represented by a smooth function \(\theta: G \times \mathcal{M} \to \mathcal{M}\) mapping the product manifold \(G \times \mathcal{M}\) to \(\mathcal{M}\). We employ the notation \(\theta(g, x)\) as \(gx\) for any \(g \in G\) and \(x \in \mathcal{M}\). In a scenario of learning under invariances, the regression function \(f^\star\) is invariant under the action of the group \(G\), satisfying \(f^\star(gx) = f^\star(x)\) for each \(g \in G\) and \(x \in \mathcal{M}\). Thus, learning under invariances introduces an additional requirement: not only must we compute an estimator with minimal population risk efficiently, but \(\widehat{f}\) must also be invariant with respect to \(G\). This additional condition is often satisfied in neural network applications by constructing networks that are invariant \emph{by design}, such as graph neural networks.

In the context of learning with Sobolev kernels, the KRR estimator \(\widehat{f}_{\operatorname{KRR}}\) is \emph{not} \(G\)-invariant (see \cref{app:back} for more details). Consequently, the KRR estimator cannot provide a solution for learning under invariances. However, with a shift-invariant Positive Definite Symmetric (PDS) kernel\footnote{A kernel \(K: \mathcal{M} \times \mathcal{M} \to \mathbb{R}\) is termed shift-invariant with respect to group \(G\) if and only if \(K(gx_1, gx_2) = K(x_1, x_2)\) for each \(g \in G\) and \(x_1, x_2 \in \mathcal{M}\). Shift-invariant kernels are \emph{not} necessarily \(G\)-invariant. For example, one can show that \(H^s(\mathcal{M})\) adopts a shift-invariant kernel while still producing non-invariant functions in its RKHS. We cover the details in \cref{app:krr}.} \(K: \mathcal{M} \times \mathcal{M} \to \mathbb{R}\), one can utilize \emph{group averaging} to derive a new kernel and a new RKHS holding only \(G\)-invariant functions:
\[
    K_{\operatorname{inv}}\big(x_1, x_2\big) \coloneqq \frac{1}{|G|}\sum_{g \in G} K\big(gx_1, x_2\big).
\]
Given that the Sobolev space \(H^s(\mathcal{M})\) adopts a shift-invariant PDS kernel (\cref{app:sob_kernel}), one can apply the above method to construct and compute a \(G\)-invariant kernel (assuming access to evaluating its original kernel). This indicates that the KRR estimator on \(K_{\operatorname{inv}}\) yields an invariant estimator for \(f^\star\) with a desirable population risk (see \cite{tahmasebi2023exact} for a comprehensive study).

However, in terms of computational complexity, this method requires \(\Omega(n^2 |G|)\) time to compute the new kernel between pairs of input data. In many practical scenarios, \(|G|\) can be intolerably large. For instance, for the permutation group \(P_d\), we have \(|G| = d! \sim \sqrt{d}(\frac{d}{e})^d \) which is super-exponential in $d$. Consequently, the group averaging method cannot provide an efficient algorithm for learning with exact invariances. We emphasize "exact invariance" here, as the sum involved in \(K_{\operatorname{inv}}\) can be approximated by summing over a number of random group transformations. However, this does not guarantee exact invariance, which is the primary goal of this paper.

Other traditional approaches for achieving learning under invariances include data augmentation, canonicalization, and frame averaging. For data augmentation, we need to increase our dataset size by a multiplicative factor of \(|G|\), which is often impractical within efficient time constraints. This is because, for any datapoint \( x_i \in \mathcal{S} \), data augmentation requires adding a new datapoint \( g x_i \) for each group element \( g \in G \) to ensure invariance of the underlying learning procedure in a black-box manner, leading to a complexity of \( \Omega(n |G|) \). 
Canonicalization involves mapping data onto the quotient space of the group action and subsequently finding an estimator (e.g., a KRR estimator) on the reduced input space. However, this method is also infeasible for kernels due to the unavoidable discontinuities and non-smoothness of the canonicalization maps, which violate RKHS requirements \citep{dym2024equivariant}.  
Finally, frame averaging is analogous to canonicalization, but it remains unclear how to address continuity issues for efficient frame sizes. Moreover, it requires careful design of frames tailored to the specific problem at hand, making it unsuitable for a general-purpose algorithm. Thus, motivated by these observations, we pose the following question:

\begin{tcolorbox}
Is it possible to obtain a \( G \)-invariant estimator for \( f^\star \in H^s(\mathcal{M}) \) with a desirable population risk (similar to the case without invariances) in $\operatorname{poly}(n,d,\log(|G|))$ time?
\end{tcolorbox}

In the next section, we answer this question affirmatively. This is surprising, as it suggests that enumerating the set 
$G$ is not required to find statistically efficient 
$G$-invariant estimators.

\textbf{Oracles.} To characterize computational complexity, first we need to specify the type of oracle access provided for the estimation. Before doing so, we briefly review the spectral theory of the Laplace-Beltrami operator on manifolds. For further details, we refer the reader to \cref{app:back}.

The Laplace-Beltrami operator generalizes the Laplacian operator to Riemannian manifolds. It has a basis of smooth eigenfunctions $\phi_{\lambda,\ell} \in L^2(\mathcal{M})$, which serve as an orthonormal basis for $L^2(\mathcal{M})$. The index $\lambda$ represents the eigenvalue corresponding to the eigenfunction $\phi_{\lambda,\ell}$, and $\ell \in [m_{\lambda}]$ runs over the multiplicity of $\lambda$, denoted by $m_{\lambda}$. The eigenvalues can be ordered such that $0=\lambda_0 < \lambda_1 \leq \lambda_2 \leq \dots \to \infty$. For example, in the case of the sphere $S^{d-1}$, the spherical harmonics, which are homogeneous harmonic polynomials, are a natural choice of eigenfunctions.

The sequence of eigenfunctions and their corresponding eigenvalues provide critical information about the geometry of the manifold. In this work, we make use of the following two types of oracles:
\begin{itemize}
\setlength{\itemindent}{-5pt}
\setlength\itemsep{-0.1em}
    \item The ability to evaluate any eigenfunction $\phi_{\lambda,\ell}(x)$ at a given point $x \in \mathcal{M}$.
    \item The ability to compute the $L^2(\mathcal{M})$ inner product between a shifted eigenfunction $\phi_{\lambda,\ell}(gx)$ and another eigenfunction $\phi_{\lambda,\ell'}(x)$ for any group element $g \in G$.
\end{itemize}

For both oracles, we assume free access as long as $D_{\lambda}\coloneqq\sum_{\lambda' \leq \lambda} m_{\lambda'} = \operatorname{poly}(n,d)$,  where \( D_\lambda \) denotes the number of eigenfunctions with eigenvalues less than or equal to \( \lambda \). 
This assumption is motivated by the case of $S^{d-1}$, where spherical harmonics can be efficiently evaluated or multiplied in low dimensions (in such cases, only a few monomials need to be processed, making the task simple\footnote{This extends to the Stiefel manifold and tori.}). 
The first oracle handles the geometric structure of the manifold, while the second oracle captures the relationship between the group action and the manifold’s spectrum.  















\section{Main Result}


In this section, we address the question raised in the previous section by presenting the primary result of the paper, which is encapsulated in the following theorem.

\begin{restatable}[Learning with exact invariances in polynomial time]{thm}{main}
\label{thrm:main}
Consider the problem of learning with invariances with respect to a finite group \( G \) using a labeled dataset of size \( n \) sampled from a manifold of dimension \( d \). Assume that the optimal regression function belongs to the Sobolev space of functions of order \( s \), i.e., \( f^\star \in H^s(\mathcal{M}) \) for some \( s > d/2 \) and let $\alpha\coloneqq 2s/d$. Then, there exists an algorithm that, given the data, produces an exactly invariant estimator \( \widehat{f} \) such that:
 \vspace{-0.15in}
    \begin{itemize}
    \setlength{\itemindent}{-5pt}
\setlength\itemsep{-0.1em}
        \item It runs in time \( \mathcal{O}\big(\log^3(|G|) n^{3/(1+\alpha)} + n^{(2+\alpha)/(1+\alpha)}\big) \);
        \item It achieves an excess population risk (or generalization error) of \( \mathcal{R}(\widehat{f})=\mathcal{O}\big(n^{-s/(s+d/2)}\big) \);
        \item It requires \( \mathcal{O}\big( \log(|G|) n^{2/(1+\alpha)}  + n^{(2+\alpha)/(1+\alpha)}\big) \) oracle calls to construct the estimator;
        \item For any \( x \in \mathcal{M} \), the estimator \( \widehat{f}(x) \) can be computed in time \( \mathcal{O}\big( n^{1/(1+\alpha)}\big) \) using \( \mathcal{O}\big( n^{1/(1+\alpha)} \big) \) oracle calls.
    \end{itemize} 
\end{restatable}

% \begin{theorem}[Learning with exact invariances in polynomial time]\label{thrm:main} 
%     Consider the problem of learning with invariances with respect to a finite group \( G \) using a labeled dataset of size \( n \) sampled from a manifold of dimension \( d \). Assume that the optimal regression function belongs to the Sobolev space of functions of order \( s \), i.e., \( f^\star \in H^s(\mathcal{M}) \) for some \( s > d/2 \). Then, there exists an algorithm that, given the data, produces an estimator \( \widehat{f} \) such that:
%     \begin{itemize}
%         \item It operates in time \( \mathcal{O}\big(n^{1.5}d^{3} \log^3(|G|)\big) \).
%         \item It achieves an excess population risk (or generalization error) of \( \mathcal{R}(\widehat{f})=\mathcal{O}\big(n^{-s/(s+d/2)}\big) \).
%         \item It requires \( \mathcal{O}\big(n \log(|G|)\big) \) oracle calls to construct the estimator.
%         \item For any \( x \in \mathcal{M} \), the estimated label \( \widehat{f}(x) \) can be computed in time \( \mathcal{O}\big(\sqrt{n}\big) \) using \( \mathcal{O}\big(\sqrt{n}\big) \) oracle calls.
%     \end{itemize} 
% \end{theorem}

The full proof of Theorem \ref{thrm:main} is presented in \cref{app:main}, while a detailed proof sketch is provided in \cref{sec:ps}, and the algorithm is outlined in \cref{alg:alg}.



Let us interpret the above theorem. Note that without any invariances, the Kernel Ridge Regression (KRR) estimator (details are given in \cref{app:krr}) provides an estimator $\widehat{f}_{\operatorname{KRR}}$ for the Sobolev space $H^s(\mathcal{M})$ that is computed in time $\mathcal{O}\big(n^3\big)$ and achieves the risk $\mathcal{R}(\widehat{f}_{\operatorname{KRR}})=\mathcal{O}\big(n^{-s/(s+d/2)}\big)$, which is minimax optimal. Here, while KRR cannot guarantee an exactly invariant estimator, we propose another estimator which is both exactly invariant and also converges with the same rate $\mathcal{O}\big(n^{-s/(s+d/2)}\big)$. As a result, we achieve exact invariances with statistically desirable risk (or sample complexity). In other words, the population risk is the same as the optimal case without invariances, which shows that the algorithm introduces no loss in statistical performance while enforcing group invariances.  

We thus come to the following conclusion:

\begin{tcolorbox}
The problem of learning with exact invariances can be efficiently solved in time $\operatorname{poly}(n,d, \log(|G|))$ and with excess population risk (or generalization error) $\mathcal{O}\big(n^{-s/(s+d/2)}\big)$  which is the same statistical performance as for learning without invariances. 

\end{tcolorbox}



It is worth mentioning that, according to the theorem, the proposed estimator $\widehat{f}$ is not only efficiently achievable but also efficiently computes new predictions on unlabeled data. 




\begin{remark}
    We notice that in the proof of \cref{thrm:main}, the actual  time and sample complexity depends only on the size of the minimum generating set of the group $G$, denoted by $\rho(G)$, instead of $\log(|G|)$. We use the logarithm in the theorem just to make the improvement from the naive approach clearer. The actual proof allows to achieve the tighter result with $\rho(G)\le \log(|G|)$, which holds for any finite group (see \cref{prop:minial} in \cref{app:minimal}). Note that for some cases (such as cyclic groups) $\rho(G) \ll \log(|G|)$.
\end{remark}

\section{Algorithm and Proof Sketch}\label{sec:ps}

In this section, we provide a proof sketch for \cref{thrm:main}, introducing several new notations and concepts necessary for achieving the reduction in time complexity.

We begin with the most natural optimization program for obtaining an estimator: the Empirical Risk Minimization (ERM), which proposes the following estimator:
\[
    \widehat{f}_{\operatorname{ERM}} \coloneqq \argmin_{f \in H^s(\mathcal{M})} \left\{ \frac{1}{n} \sum_{i=1}^n \left( f(x_i) - y_i \right)^2 \right\},
\]
where \(\mathcal{S} = \{(x_i, y_i) : i \in [n]\} \subseteq (\mathcal{M} \times \mathbb{R})^n\) denotes the sampled (labeled) dataset.

However, as discussed, this method does not necessarily produce an estimator that is exactly invariant. A natural idea is to introduce group invariances as constraints into the above optimization, leading to the following constrained ERM solution:
\[
    \widehat{f}_{\operatorname{ERM-C}} \coloneqq \argmin_{f \in H^s(\mathcal{M})} & \left\{ \frac{1}{n} \sum_{i=1}^n \left( f(x_i) - y_i \right)^2 \right\} \\
    \textrm{s.t.} \quad & \forall (g,x) \in G \times \mathcal{M}: f(gx) = f(x).
\]
While this formulation ensures exact invariance, it introduces \(|G|\) functional equations. This is problematic for two reasons: first, \(|G|\) constraints are prohibitively many, and second, these constraints require solving functional equalities, which are not easily achievable. Moreover, the functional equations involve non-linear (pointwise) constraints on the estimator function, which at first glance appear intractable due to nonconvexity of the contraints $f(g x) = f(x)$ for general choice of $g$.

Therefore, it is necessary to reformulate the above optimization program. The goals of the reformulation are to reduce the number of constraints and encode the functional equations into more tractable constraints, ideally linear ones.

\textbf{Reducing the number of constraints.} We begin by using the following basic property (based on the group law):
\[
    \Big( \forall g \in \{g_1, g_2\},& \forall x \in \mathcal{M}: f(gx) = f(x) \Big) \\&\implies \Big( \forall x \in \mathcal{M}: f(g_1g_2x) = f(x) \Big).
\]
This observation allows us to eliminate many unnecessary constraints. Specifically, we only need constraints over a small subset of \(G\) if this subset can generate any group element through arbitrary group multiplications. To formalize this, we introduce the following definition:

\begin{definition}
    A finite group \(G\) is said to be generated by a subset \(S \subseteq G\) if for every \(g \in G\), there exists a sequence of elements \(s_1, s_2, \ldots, s_k\) such that for each $i \in [k]$, either $s_i \in S$ or $s_i^{-1}\in S$ and  \(g = s_1 s_2 \cdots s_k\). The minimum size of such a subset \(S\) is denoted by \(\rho(G)\).
\end{definition}

Clearly, \(\rho(G) \leq |G|\). However, it can be shown (see \cref{app:minimal}) that \(\rho(G) \leq \log(|G|)\), which represents an exponential improvement over the trivial upper bound.

Thus, we can reformulate the constrained ERM as:
\[
    \widehat{f}_{\operatorname{ERM-C}} \coloneqq \argmin_{f \in H^s(\mathcal{M})} & \left\{ \frac{1}{n} \sum_{i=1}^n \left( f(x_i) - y_i \right)^2 \right\} \\
    \textrm{s.t.} \quad & \forall (g,x) \in S \times \mathcal{M}: f(gx) = f(x),
\]
where \(|S| \leq \log(|G|)\). This way we reduce the number of constraints from \(|G|\) to \(\log(|G|)\) by leveraging the concept of minimal group generators. Note that this fact cannot be directly used in data augmentation, group averaging, or canonicalization techniques.













\textbf{Optimization in the spectral domain.} The constrained ERM formulation presented above, while advantageous in terms of reducing the number of constraints, involves optimizing over the infinite-dimensional space \(H^s(\mathcal{M})\), which is computationally intractable. One way to make this problem tractable is to parametrize the estimator and search for the optimal parameters. To achieve this, we utilize the spectral theory of the Laplace-Beltrami operator over manifolds. While a detailed discussion of spectral theory is provided in \cref{app:back}, we summarize the relevant concepts here.

As mentioned earlier, the Laplace-Beltrami operator yields a sequence of orthonormal eigenfunctions \(\phi_{\lambda,\ell} \in L^2(\mathcal{M})\), where \(\lambda \in \{\lambda_0, \lambda_1, \ldots\} \subseteq [0, \infty)\) represents the eigenvalue corresponding to the eigenfunction \(\phi_{\lambda,\ell}\), and \(\ell \in [m_{\lambda}]\) indexes the multiplicity of \(\lambda\), denoted by \(m_{\lambda}\). Therefore, any estimator \(f \in L^2(\mathcal{M})\) can be expressed as:
\[
    f(x) = \sum_{\lambda} \sum_{\ell=1}^{m_{\lambda}} f_{\lambda,\ell} \phi_{\lambda,\ell}(x), \quad f_{\lambda,\ell} \coloneqq \langle f, \phi_{\lambda, \ell} \rangle_{L^2(\mathcal{M})}.
\]
The idea is to parametrize the problem by finding the best coefficients \(f_{\lambda,\ell}\). However, since there are infinitely many eigenvalues, there are infinitely many parameters to estimate, which is not feasible in finite time. Fortunately, we know that \(f^\star \in H^s(\mathcal{M})\). From the definition of Sobolev spaces (see \cref{app:sob_kernel}), we have:
\[
    \|f^\star\|^2_{H^s(\mathcal{M})} \coloneqq \sum_{\lambda} \sum_{\ell=1}^{m_{\lambda}} (f^\star_{\lambda,\ell})^2 D_{\lambda}^{\alpha},
\]
where \(D_{\lambda} = \sum_{\lambda' \leq \lambda} m_{\lambda'}\), and \(\alpha \coloneqq \frac{2s}{d} > 1\).

Thus, we conclude that:
\[
    \sum_{\lambda: D_\lambda > D} \sum_{\ell=1}^{m_{\lambda}} (f^\star_{\lambda,\ell})^2 \leq D^{-\alpha} \|f^\star\|^2_{H^s(\mathcal{M})} = \mathcal{O}(D^{-\alpha}),
\]
for any \(D > 0\). This shows that for Sobolev regression functions \(f^\star \in H^s(\mathcal{M})\), we can truncate the estimation of coefficients at a certain cutoff frequency \(\lambda\), which allows the problem to be parametrized with finitely many parameters. Although this introduces bias into the estimation (since higher-frequency eigenfunctions will not be captured), the bias is bounded by the above inequality for Sobolev spaces.

Interestingly, this spectral approach yields a more meaningful optimization problem when considering the population risk function rather than ERM. The population risk, which is the primary objective in regression, is given by:
\[
    \mathcal{R}(f) = \mathbb{E}_{\mathcal{S}}\left[\|f - f^\star\|^2_{L^2(\mathcal{M})}\right] = \sum_{\lambda} \sum_{\ell=1}^{m_{\lambda}} \mathbb{E}[(f_{\lambda,\ell} - f^\star_{\lambda,\ell})^2].
\]




\textbf{Constrained spectral method.} To review, we introduced an efficient way to impose the constraints related to group invariances in the ERM objective and later presented spectral methods for obtaining estimators. The last step here is to combine these to achieve exact invariances via a constrained spectral method. We use an important property of the Laplace-Beltrami operator to introduce the algorithm.

Let $\Delta_{\mathcal{M}}$ denote the Laplace-Beltrami operator on the manifold $\mathcal{M}$, and let $G$ be a group acting isometrically on $\mathcal{M}$. Define the linear operator $T_g : f(x) \mapsto f(gx)$ for each group element $g \in G$ and any smooth function $f$ on the manifold. Then, we have
\[
    \Delta_{\mathcal{M}}(T_g \phi) = T_g (\Delta_{\mathcal{M}}(\phi)),
\]
for any smooth function $\phi$ on the manifold (for a formal proof, please refer to \cref{app:commute}).

This identity tells us that the Laplace-Beltrami operator $\Delta_{\mathcal{M}}$ commutes with the operator $T_g$ for each $g$. Since both operators are linear, spectral theory implies that the commutativity shows the eigenspaces of $\Delta_{\mathcal{M}}$ are \emph{preserved} under the action of the group $G$, meaning the operators can be simultaneously diagonalized. Specifically, for any $\lambda,\ell$, and any $g \in G$, the function $\phi_{\lambda,\ell}(gx)$ is a linear combination of eigenfunctions $\phi_{\lambda,\ell'}$, $\ell' \in [m_\lambda]$. In particular, the group $G$ acts via orthogonal matrices on the eigenspace $V_\lambda \coloneqq \operatorname{span}(\phi_{\lambda,\ell} : \ell \in [m_\lambda])$ for each $\lambda$.

Let $D^{\lambda}(g)$ denote the $m_\lambda \times m_\lambda$ orthogonal matrix corresponding to the action of an element $g \in G$ on $V_\lambda$ for each $\lambda$. Then, a function 
\[
f(x) = \sum_{\lambda} \sum_{\ell=1}^{m_\lambda} f_{\lambda,\ell} \phi_{\lambda,\ell}(x)
\]
is $G$-invariant if and only if
\[
D^\lambda(g) f_{\lambda} = f_{\lambda}, \quad \forall g \in G ~\forall \lambda \in \{\lambda_0, \lambda_1, \ldots\},
\]
where $f_\lambda \coloneqq (f_{\lambda,\ell})_{\ell \in [m_\lambda]} \in \mathbb{R}^{m_\lambda}$ for each $\lambda$. We can further reduce the number of conditions by passing $G$ to a generator set, which gives only $\log(|G|)$ conditions.

Thus, the commutativity of the Laplace-Beltrami operator and any isometric group action allows us to introduce only linear constraints on the spectral method to achieve exact invariances. This leads to the following optimization program:
\[
\min_{f_{\lambda,\ell}}~ &\sum_{\lambda} \sum_{\ell=1}^{m_\lambda} \mathbb{E}[(f_{\lambda,\ell} - f^\star_{\lambda,\ell})^2], \\
\text{s.t.} &\quad \forall g \in S ~ \forall \lambda \in \{\lambda_0, \lambda_1, \ldots\}: D^\lambda(g) f_\lambda = f_\lambda.
\]
Here, $f^\star_{\lambda,\ell} = \mathbb{E}_{x}[f^\star(x)\phi_{\lambda,\ell}(x)] = \mathbb{E}_{x,y}[y\phi_{\lambda,\ell}(x)]$ is not known a priori; only $n$ samples $(x_i, y_i) \in \mathcal{M} \times \mathbb{R}$, $i \in [n]$, are given. Furthermore, the constraints are independent for different eigenspaces (i.e., different $\lambda$), and the objective is a sum over eigenspaces. This means we can decompose the problem into a set of linearly constrained optimization programs, one for each eigenspace $V_\lambda$:
\[
\min_{f_{\lambda,\ell}}~ &\sum_{\ell=1}^{m_\lambda} \mathbb{E}[(f_{\lambda,\ell} - f^\star_{\lambda,\ell})^2], \\
\text{s.t.} &\quad \forall g \in S: D^\lambda(g) f_\lambda = f_\lambda.
\]

This reformulation allows us to propose efficient estimators for the problem.

\textbf{Empirical estimator.} In this paper, we suggest the following auxiliary empirical mean estimator from the data for the above optimization program on $V_\lambda$:
\[
\widetilde{f}_{\lambda,\ell} = \frac{1}{n} \sum_{i=1}^n y_i \phi_{\lambda,\ell}(x_i), \quad \forall \ell \in [m_\lambda]. \numberthis{eq:emp}
\]
Moreover, we stop estimation and set $\widetilde{f}_{\lambda,\ell} = 0$ when $D_\lambda > D$, where $D$ is a hyperparameter. To obtain a $G$-invariant estimator from our primary estimator, we solve the following quadratic program to find a solution satisfying the constraints for each $V_\lambda$ with $D_\lambda \leq D$:
\[
\widehat{f}_{\lambda, \ell} \coloneqq \argmin\limits_{f_{\lambda,\ell}} &~ \sum_{\ell=1}^{m_\lambda} (f_{\lambda,\ell} - \widetilde{f}_{\lambda,\ell})^2, \\
\text{s.t.} &\quad \forall g \in S: D^\lambda(g) f_\lambda = f_\lambda.
\]

This optimization problem is a convex quadratic program with linear constraints that can be solved iteratively using the rich convex optimization machinery. Additionally, it  has a closed-form solution as noted in \Cref{prop:projection} in \Cref{app:project}. Let $B^\lambda \in \mathbb{R}^{|S| m_\lambda \times m_\lambda}$ be defined as the augmented matrix resulting from concatenating $D^\lambda(g) - I$ for all $g \in S$, %on top of each other, 
i.e., $B^\lambda = [(D^\lambda(g_1) - I)^\top, (D^\lambda(g_2) - I)^\top, \dots, (D^\lambda(g_{|S|}) - I)^\top]^\top$. Then,
\[
\widehat{f}_{\lambda,\ell} = \widetilde{f}_{\lambda,\ell} - {B^\lambda}^\top (B^\lambda {B^\lambda}^\top)^\dagger (B^\lambda \widetilde{f}_{\lambda})[\ell],
\]
where $\dagger$ denotes Moore–Penrose inverse.

The final estimator of the algorithm is given by
\[
    \widehat{f}(x) = \sum_{\lambda : D_\lambda \leq D} \sum_{\ell=1}^{m_\lambda} \widehat{f}_{\lambda,\ell} \phi_{\lambda,\ell}(x).
\]
This meta approach to design a $G$-invariant estimator $\widehat{f}$ from any primary estimtor $\widetilde{f}$ is novel and may be of independent interest. Pseudocode for the method is presented in \cref{alg:alg}. Since the invariance is imposed in the spectral representation, we refer to our proposed algorithm as Spectral Averaging (\texttt{Spec-Avg}).

\begin{algorithm}[h]
\caption{Learning with Exact Invariances by Spectral Averaging (\texttt{Spec-Avg})}
\begin{algorithmic}[1]\label{alg:alg}
\REQUIRE  $\mathcal{S} = \{(x_i, y_i) : i \in [n]\}$ and $\alpha = 2s/d \in (1,\infty)$.
\ENSURE  $\widehat{f}(x)$.
\STATE Initialize $D \gets n^{1/(1+\alpha)}$.
\FOR{each $\lambda$ such that $D_\lambda \leq D$}
    \FOR{each $\ell \in [m_\lambda]$}
        \STATE $\widetilde{f}_{\lambda,\ell} \gets \frac{1}{n} \sum_{i=1}^n y_i \phi_{\lambda,\ell}(x_i)$.
    \ENDFOR
\ENDFOR
\FOR{each $\lambda$ such that $D_\lambda \leq D$}
\STATE Solve the following linearly constrained quadratic program over $m_\lambda$ variables:
\[
    \widehat{f}_{\lambda, \ell} \gets \argmin_{f_{\lambda,\ell}} &~ \sum_{\ell=1}^{m_\lambda} (f_{\lambda,\ell} - \widetilde{f}_{\lambda,\ell})^2, \\
    \text{s.t.} &\quad \forall g \in S: D^\lambda(g) f_\lambda = f_\lambda.
\]
\ENDFOR
\STATE \textbf{Return:} $\widehat{f}(x) = \sum_{\lambda : D_\lambda \leq D} \sum_{\ell=1}^{m_\lambda} \widehat{f}_{\lambda,\ell} \phi_{\lambda,\ell}(x)$.
\end{algorithmic}
\end{algorithm}


We conclude this section by reviewing how we apply the results from \cref{alg:alg} to the two following important examples.
\begin{example}
    Consider the problem of learning under invariances over the unit sphere $S^{d-1}\coloneqq \{x \in \mathbb{R}^d : \|x\|_2 = 1\}$, where the group $G$ is the group of all permutations of coordinates. Note that $|G| = d!$, which is prohibitively large for data augmentation or group averaging. However, this group is generated by only two elements: $\sigma_1 = (1 \, 2)$ and $\sigma_2 = (1 \, 2 \, \ldots \, d)$. Here, $\sigma_1$ swaps the first and second coordinates, while $\sigma_2$ is a cycle that maps $1 \to 2$, $2 \to 3$, and so on, cyling with $d \to 1$.


    The eigenspaces $V_{\lambda}$ for the sphere are precisely the sets of homogeneous harmonic polynomials of degree $k$, where $\lambda = k(k+d-2)$. The permutation group acts on $V_{\lambda}$ by permuting the variables of the polynomials. This action is clearly linear, and the matrices $D^\lambda(g)$ can be efficiently computed (using tensor products) as long as $k$ is small. Moreover, homogeneous polynomials of degree $k$ can also be computed efficiently for small $k$. This shows that the oracles considered in this paper align perfectly with what we observe in the important case of spheres and polynomial regression. In \cref{alg:alg}, we first compute the coefficients of each polynomial for degree $k$, up to a small $k$, and then solve a quadratic program with only two linear constraints to obtain an exactly invariant polynomial solution.
\end{example}

\begin{example}
    Consider the same setup as the previous example but assume $d = 2$, i.e., the manifold is the unit circle. In this case, each eigenspace $V_{\lambda}$ is spanned by $\sin(k\theta)$ and $\cos(k\theta)$, where $\lambda = k^2$. Let us assume our task is to find an estimator invariant with respect to rotations by integer multiples of $\frac{2\pi}{|G|}$. This group is cyclic and is generated by only one element $g_0 = \frac{2\pi}{|G|}$. Thus, we have only one constraint for each eigenspace. Indeed, one can observe that $D^\lambda(g_0) = R(k\frac{2\pi}{|G|})$, where $R(.) \in \mathbb{R}^{2 \times 2}$ is the two-dimensional rotation matrix. Thus, this example further illustrates how our oracles are defined to solve the problem.
\end{example}
 




\section{Experiments}\label{exper}

In this section, we provide complementary experiments to support our theoretical results. We first show that, in practice, Kernel Ridge Regression (\texttt{KRR}) is not a $G$-invariant estimator. Then, we demonstrate that our algorithm (\texttt{Spec-Avg}) achieves the same rate of population risk as \texttt{KRR}, while enjoying exact invariance properties.

\subsection{Problem Statement}
We consider the input space (manifold) $\mathbb{T}^d  = [-1, 1)^d$, which represents a flat $d$-dimensional torus. Additionally, we consider the group of sign-invariances $G = \{\pm 1\}^d$, acting on this space via coordinate-wise sign inversions.
The dataset is generated as $n$ independent and identically distributed (i.i.d.) samples drawn uniformly from this space, with the target function defined as:
$
f^*(x) = \frac{1}{d}\sum_{i=1}^d i x_i^2.
$

Clearly, this function is invariant w.r.t. group action $G$.
To analyze estimation via kernels in this setup, we consider a periodic kernel on the torus $\mathbb{T}^d$, specifically the \textit{von Mises Kernel}~\citep{von1918ganzzahligkeit,mardia2009directional}, defined as:
$
K_{\eta}(x, y) = \exp\left(\eta \cos(\pi (x - y))\right),
$
where $\eta$ is a positive parameter associated with kernel bandwidth. This kernel function is particularly useful for circular and directional statistics.


Moreover, the kernel admits the following sign-invariant eigenfunctions:
$
\phi_{\ell_1, \ell_2, \dots, \ell_d}(x) = \prod_{i=1}^d \cos(\pi \ell_i x_i),
$
where $\ell_i \in \mathbb{N} \cup \{0\}$. 
The corresponding eigenvalues can be computed as  
$\lambda = \pi \sum_{i=1}^d \ell_i^2,$ 
derived from the partial differential equation  
$
\Delta \phi_{\ell_1, \ell_2, \dots, \ell_d} + \lambda \phi_{\ell_1, \ell_2, \dots, \ell_d} = 0.
$
This formulation facilitates the analysis of \texttt{KRR} and \texttt{Spec-Avg} under symmetry constraints, ensuring their compatibility with the underlying group structure. It is worth noting that, in this setting, $|G| = 2^d$. Consequently,  group averaging is computationally inefficient due to the exponential growth of the group size with the dimensionality $d$.

\subsection{Settings}
We conduct our experiments for $d = 10$. The trained models are evaluated on a test dataset of size $100$. Both the test and train datasets are generated uniformly from the interval $[-1, 1]^d$, independently and identically distributed. Each point in our plots represents an average over $10$ different random seeds (from $1$ to $10$) to account for the randomness in the data generation process. 

\subsection{Results}

The results of the experiments are depicted in \Cref{fig:exp1} and \Cref{fig:exp2} in \cref{app:exp}. While our algorithm (\texttt{Spec-Avg}) is $G$-invariant by construction, there is no theoretical guarantee for Kernel Ridge Regression (\texttt{KRR}) to be $G$-invariant. In \Cref{fig:exp1} in \cref{app:exp}, we demonstrate that this is indeed the case in practice, as the estimator \texttt{KRR} is not $G$-invariant. We define the following measure of Invariance Discrepancy:
\[
\operatorname{ID}(\widehat{f}) \defeq \sup_{x \in \mathcal{X}, g \in G} |\widehat{f}(x) - \widehat{f}(g x)|,
\]
where $\widehat{f}$ is the estimator. We report this value for \texttt{KRR} across different choices of the regularization parameter $\lambda$. It is worth noting that $\operatorname{ID}(\widehat{f})$ is zero for the \texttt{Spec-Avg} estimator, as it is $G$-invariant by design.

In \Cref{fig:exp2} in \cref{app:exp}, we present the empirical excess population risk of \texttt{KRR} and \texttt{Spec-Avg} for different hyperparameters $\lambda$ and $D$, respectively. As expected, it is demonstrated that with an appropriate choice of hyperparameters, \texttt{KRR} and \texttt{Spec-Avg} achieve the same order of test error. Higher values of the regularization parameter $\lambda$ for \texttt{KRR} correspond to lower values of the sparsity parameter $D$ for \texttt{Spec-Avg}, both of which act as mechanisms for regularizing the norm of the estimator. It can be observed that \texttt{Spec-Avg} with $D = 176$ achieves the same order of performance as \texttt{KRR} with $\lambda = 50$.



\section{Conclusion}


In this paper, we explore the statistical-computational trade-offs in learning with invariances, focusing  on kernel regression. We observe that while the Kernel Ridge Regression (KRR) estimator can address this problem, it is not invariant without group averaging, and since group averaging is  costly for large groups, we ask whether it is possible to develop statistically sound estimators with efficient time complexity. Our findings show that by reformulating the problem and reducing the number of constraints using group laws, we can express it as solving an infinite series of quadratic optimization programs under linear constraints. We conclude with an algorithm that achieves an exactly invariant estimator with polynomial time complexity and highlight several additional questions for future research.




% \section*{Acknowledgements}

 

\section*{Impact Statement}
% This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.

The primary focus of this work is on theoretical problems in machine learning, specifically learning with invariances. As such, it does not have direct societal implications or ethical concerns.


\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn



\section{Discussion and Future Directions}

We initiated the study on computational-statistical trade-offs in learning with exact invariances. We designed an algorithm that shows achieving the desirable population risk (the same as kernel regression without invariances) in $\operatorname{poly}(n,d,\log(|G|))$ time for the task of kernel regression with invariances on general manifolds. We note that, for simplicity, we have focused on boundaryless manifolds and isometric group actions. However, using standard techniques, the theory can be extended to more general cases as well\footnote{See e.g., \citet{tahmasebi2023exact}.}. While the proposed spectral algorithm is computationally efficient, it does not offer any improvement in sample complexity over the baseline $\mathcal{R}(\widehat{f}) = \mathcal{O}\big(n^{-s/(s+d/2)}\big)$. It has been observed that without computational constraints, better convergence rates are possible for learning with invariances \citep{tahmasebi2023exact}, which are minimax optimal. Thus, it remains open whether those improved rates are achievable in $\operatorname{poly}(n,d,\log(|G|))$ time.

We note that the oracle access we assumed is primarily motivated by the case of the sphere, where polynomials can be evaluated, multiplied, composed by group elements, and integrated efficiently when they are of relatively low degree. We believe this is the most natural oracle access for this problem, as it aligns well with applications involving polynomials. An interesting future work could be to investigate the statistical-computational trade-offs using alternative oracles, e.g., similar to the kernel trick, how to design computationally efficient algorithms that have only access to the inner product of the RKHS. Another interesting future direction is to find whether random feature models as approximations for kernels can significantly improve the statistical-computational trade-off of learning with invariances. At present, our theory does not apply to random feature models.


We also observe that the spectral algorithm used in this paper does not employ the kernel trick, as it requires access to the entire set of features, rather than just their inner products. An interesting question is whether it is possible to utilize kernel tricks and find an alternative (polynomial-time) algorithm for learning under invariances. This approach could potentially improve the statistical efficiency of the spectral algorithm. In the end, we would like to note that capturing computational-statistical trade-offs in other estimation problems with invariances such as density estimation \citep{chen2023sample, tahmasebisample} could serve as a compelling avenue for future research.




\section{Background}\label{app:back}

\subsection{Riemannian Manifolds} \label{app:manifold}

In this section, we review some fundamental definitions from differential geometry and refer the reader to \citet{lee2006riemannian,petersen2006riemannian,lee2012introduction} for further details.

\begin{definition}[Manifold]
    A topological \emph{manifold} $\mathcal{M}$ of dimension $\operatorname{dim}(\mathcal{M})$ is a completely separable Hausdorff space that is locally homeomorphic to an open subset of Euclidean space of the same dimension, specifically $\mathbb{R}^{\operatorname{dim}(\mathcal{M})}$. More formally, for each point $x \in \mathcal{M}$, there exists an open neighborhood $ U \subseteq \mathcal{M} $ and a homeomorphism $\phi: U \to \widehat{U}$, where $\widehat{U} \subseteq \mathbb{R}^{\operatorname{dim}(\mathcal{M})}$.
\end{definition}

The value $\dim(\mathcal{M})$ is referred to as the \emph{dimension} of the manifold. Examples of manifolds include tori, spheres, $\mathbb{R}^d$, and graphs of continuous functions. \emph{Manifolds with boundaries} differ from boundaryless manifolds in that they may have neighborhoods that locally resemble open subsets of closed \emph{$\operatorname{dim}(\mathcal{M})$-dimensional upper half-spaces}, denoted as $\mathbb{H}^{\operatorname{dim}(\mathcal{M})} \subseteq \mathbb{R}^{\operatorname{dim}(\mathcal{M})}$, defined as follows:
\[
\mathbb{H}^{d} = \{(x_1, x_2, \dots, x_d) \in \mathbb{R}^d \; |\: x_d \geq 0\}.
\]

\begin{definition}[Local Coordinates]
    Given a \emph{chart} $(U, \phi)$—a pair consisting of a local neighborhood $U$ and the corresponding homeomorphism $\phi: U \to \widehat{U}$—on a manifold $\mathcal{M}$ with dimension $d$, we define \emph{local coordinates} $(x^1, x^2, \dots, x^d)$ such that 
    \[
    \phi (p) = (x^1(p), x^2(p), \dots, x^d(p)),
    \]
    for each point $p \in U$.
\end{definition}

\begin{definition}[Tangent Space]
    At each point $x \in \mathcal{M}$, the \emph{tangent space} $T_x \mathcal{M}$ is defined as the vector space formed by the tangent vectors to the manifold $\mathcal{M}$ at $x$. A tangent vector \( v \in T_x \mathcal{M} \) can be represented as the derivative of a smooth curve $\gamma(t): (-\epsilon, \epsilon) \to \mathcal{M}$ defined on the manifold with the property that $\gamma(0) = x$. It is expressed as 
    \[
    \nu = \frac{d}{dt} \gamma(t) \bigg|_{t=0}.
    \]
\end{definition}

The tangent space $T_x \mathcal{M}$ is a real vector space with dimension $\operatorname{dim}(\mathcal{M})$.

\begin{definition}[Riemannian Metric Tensor]
    A \emph{Riemannian metric tensor} $ g $\footnote{This notation differs from $g$, which denotes group elements.} on a manifold $ \mathcal{M} $ is a smooth inner product defined on the tangent space $ T_x \mathcal{M} $ at each point $ x \in \mathcal{M} $. For any two tangent vectors $ u, v \in T_x \mathcal{M} $, the metric assigns a real number $ g_x(u, v) \in \mathbb{R}$.
\end{definition}

\begin{definition}[Riemannian Manifold]
    A \emph{Riemannian manifold} is defined as a pair $ (\mathcal{M}, g) $, where $ \mathcal{M} $ is a manifold and $g$ is a \emph{Riemannian metric tensor} defined on the tangent space $T_x \mathcal{M}$ at each point $x \in \mathcal{M}$.
\end{definition}

A Riemannian metric tensor provides essential tools for the study of manifolds, which we formalize below. It enables the following:

\begin{itemize}
    \item the definition of the geodesic distance $d(x,y)$ between any two points $x,y \in \mathcal{M}$ on the manifold, 
    \item  a volume element $d \operatorname{vol}_g(x)$ over the manifold, serving as the measure for the Borel sigma-algebra over open subsets of the manifold $\mathcal{M}$, and 
    \item the measurement of the angle between any two tangent vectors $u, v \in T_x \mathcal{M}$, which in turn provides the size of tangent vectors.
\end{itemize}



\begin{definition}[Geodesic Distance]
    The \emph{geodesic distance} $d_{\mathcal{M}}(x, y)$ between any two points $x, y \in \mathcal{M}$ on the manifold is defined as the infimum length among all smooth curves $\gamma: [0, 1] \to \mathcal{M}$ connecting $x$ to $y$ ($\gamma(0) = x, \gamma(1) = y$). The length of a curve $\gamma$ is defined as 
    \[
        L(\gamma) = \int_0^1 \sqrt{g_{\gamma(t)}\left( \dot{\gamma}, \dot{\gamma} \right)} \, dt,
    \]
    where $\dot{\gamma}$ denotes the derivative $\frac{d\gamma}{dt}$.
\end{definition}

\begin{definition}[Volume Element]
    The \emph{volume element} $ d\mathrm{vol}_g(x) $ on a Riemannian manifold $ (\mathcal{M}, g) $ is defined as 
    \[
    d\mathrm{vol}_g = \sqrt{\det(g_{ij})} \, dx^1 \wedge dx^2 \wedge \cdots \wedge dx^n,
    \]
    where $ g_{ij} $ are the components of the Riemannian metric tensor, $(x^1, x^2, \ldots, x^n)$ are the local coordinates, and $\wedge$ denotes the exterior product. 
\end{definition}


The volume element provides a way to compute the volume of subsets of $ \mathcal{M}$ by integrating functions over $\mathcal{M}$. Moreover, a Borel measure $\mu$ on open subsets of $\mathcal{M}$ can be derived form the volume element to form probability measure space, e.g., uniformly over the manifold.

\begin{definition}[Smooth Map]
    A maping $f: \mathcal{M} \to \mathcal{N}$ is a smooth map if for any charts $(U, \phi)$ on $\mathcal{M}$, and $(V, \psi)$ on $\mathcal{N}$, the composition function $\psi \circ f \circ \phi^{-1}: \mathbb{R}^{\dim(\mathcal{M})} \to \mathbb{R}^{\dim(\mathcal{N})}$ is infinitely differentiable. 
\end{definition}

\begin{definition}[Pullback of the metric tensor]
    Given Riemannian manifolds $\mathcal{M}$, $(\mathcal{N}, g)$ and $\varphi: \mathcal{M} \to \mathcal{N}$ 
    a smooth map between them. The \emph{pullback of the metric tensor} $g$ by $\phi$, denoted by $\varphi^* g$ is the Riemannian metric tensor on manifold $\mathcal{M}$ defined by,
    \[
    (\varphi^* g)_x(u, v) = g_{\varphi(x)}(d\varphi_x(u), d\varphi_x(v)), \textrm{ for all points } x \in \mathcal{M} \textrm{ and all }  u, v \in T_x \mathcal{M},
    \]
    where $d\varphi_x: T_x \mathcal{M} \to T_{\varphi(x)} \mathcal{N}$ is the differential of the map $\varphi$ at point $x$.
\end{definition}

Thus, the pullback metric $\varphi^* g$ on $\mathcal{M}$ captures the relation between tangent vectors of $\mathcal{M}$ in terms of how they are mapped to the manifold $\mathcal{N}$ via $\varphi$.

\begin{definition}[Connected Manifold]
    A manifold $\mathcal{M}$ is \emph{connected} if for any two points $x, x^\prime \in \mathcal{M}$, there is a smooth curve $\gamma: [0, 1] \to \mathcal{M}$ such that $\gamma(0) = x$ and $\gamma(1) = x^\prime$.
\end{definition}

Throughout this paper, we focus on smooth, connected, compact and boundaryless Riemannian manifolds $(M, g)$ unless stated otherwise. For a Riemannian manifolds $(M, g)$, we denoted the dot product induced by the metric tensor $g$ as $\langle u, v \rangle_{g_x} = g_x(u, v)$ for all $u, v \in T_x$. We drop the subscript $x$ whenever it is clear from the context.

\subsection{Functional Spaces over Manifolds} \label{app:functional_manifold}

Now equipped with probability measures on manifold discussed in \Cref{app:manifold}, we are ready to define functional spaces $L^p(\mathcal{M})$ and Sobolev spaces $\mathcal{H}^s(\mathcal{M})$ on manifold $\mathcal{M}$ analogously to their Euclidean counterparts in the following,

\begin{definition}[Functional Spaces on Manifolds]
The Lebesgue functional spaces $ L^p(\mathcal{M}) $ for $p \in [1, \infty] $, and the Sobolev spaces $H^s(\mathcal{M})$ for $s \geq 0$ on a smooth Manifold $\mathcal{M}$, are defined as follows:
\begin{itemize}
    \item The \emph{Lebesgue space} \( L^p(\mathcal{M}) \) consists of measurable functions \( f: \mathcal{M} \to \mathbb{R} \) such that $\| f \|_{L^p(\mathcal{M})} < \infty$ where,
    \[
    \| f \|_{L^p(\mathcal{M})} = \begin{cases}
        \left( \int_{\mathcal{M}} |f(x)|^p \, d\mu(x) \right)^{1/p} & \textrm{if } p \in [1, \infty) \\
        \operatorname*{ess\,sup}_{x \in \mathcal{M}} |f(x)| < \infty. & \textrm{if } p = \infty
    \end{cases},
    \]
    where $\mu$ is the uniform measure over the manifold $\mathcal{M}$.
    \item The \emph{Sobolev space} \( H^s(\mathcal{M}) \) consists of measurable functions whose derivatives up to order \( s \) are in \( L^2(\mathcal{M}) \), i.e.,
    \[
    H^s(\mathcal{M}) = \left\{ f \in L^2(\mathcal{M}) \mid D^\alpha f \in L^2(\mathcal{M}) \text{ for all multi-indices } \alpha \text{ with } |\alpha| \leq s \right\}.
    \]
\end{itemize}
\end{definition}

\subsection{Lie Group of Isometry Maps}

In this section, first we state basic definition of isometric mappings over manifolds and then wrap up by characterizing the isometry group over the manifold.

\begin{definition}[Isometry Map]
    A bijective mapping $\tau: \mathcal{M} \to \mathcal{M}$ is an \emph{isometry} on the manifold ($\mathcal{G}$, g) if $d(\tau(x), \tau(x^\prime)) = d(x, x^\prime) $.
\end{definition}

We also state a brief definition of Lie groups for completeness.

\begin{definition}[Lie group]
    A group $G$ is a \emph{Lie group} with smooth group operations (multiplication and inversion) if it is additionally a smooth manifold.
\end{definition}

The space of bijective Riemannian isometries defined on the manifold $(\mathcal{M}, g)$, denoted by $\operatorname{ISO}(\mathcal{M}, g)$ constitutes a group with composition operation. The celebrated Myers–Steenrod theorem states that any isometry map $\tau \in \operatorname{ISO}(\mathcal{M}, g)$ between connected manifolds is an isometry \citep{myers1939group,palais1957differentiability}. \citet{myers1939group} took it a step further and proved that isometry group of a Riemannian manifold  $(\mathcal{M}, g)$ is a Lie group.

Alternatively, $\operatorname{ISO}(\mathcal{M}, g)$ can be charecterized by the pullback of the metric tensor. In terms, $\tau \in \operatorname{ISO}(\mathcal{M}, g)$ if and only if $g = \tau^* g$ \citep{petersen2006riemannian}.

\subsection{Laplacian on Manifolds}

In this section, we reiterate over definition of Laplace-Beltrami operator on manifolds (which is the generalization of the Laplacian operator $\Delta = \partial^2_1 + \partial^2_2 + \dots + \partial^2_d$ defined on the Euclidean space $\mathbb{R}^d$) and state a several interesting properties that will utilize later. We refer to \citet{chavel1984eigenvalues} for additional details.

\begin{definition}[Laplace-Beltrami operator]
    Given a Riemannian manifold $(\mathcal{M}, g)$, the \emph{Laplace-Beltrami} operator $\Delta_g : \mathcal{H}^s (\mathcal{M}) \to \mathcal{H}^{s - 2} (\mathcal{M})$ acts on a smooth function $f: \mathcal{M} \to \mathbb{R}$ by 
    \[
    \Delta_g f = \operatorname{div}_g (\operatorname{grad}_g (f)).
    \]
\end{definition}

Moreover, $\Delta_g f$ has an equivalent weak formulation \citep{evans2022partial}, as the unique continuous linear operator $\Delta_g : \mathcal{H}^s (\mathcal{M}) \to \mathcal{H}^{s - 2} (\mathcal{M})$ which is a solution to the equation,
\[
\int_{\mathcal{M}} \psi(x) \Delta_g \phi(x) d\operatorname{vol}_g(x) + \int_{\mathcal{M}} \langle \nabla_g \psi (x), \nabla_g \phi(x) \rangle_g  d\operatorname{vol}_g(x) = 0, \forall \phi,\psi \in \mathcal{H}^s(\mathcal{M}). \numberthis{eq:identity_laplacian}
\]

The Laplace-Beltrami operator $\Delta_g$ is self-adjoint, eliptic and diagonalizable in $L^p(\mathcal{M})$ \citep{chavel1984eigenvalues,evans2022partial}, yielding a sequence of orthonormal eigenfunctions \(\phi_{\lambda,\ell} \in L^2(\mathcal{M})\), where \(\lambda \in \{\lambda_0, \lambda_1, \ldots\} \subseteq [0, \infty)\) represents the eigenvalue corresponding to the eigenfunction \(\phi_{\lambda,\ell}\), and \(\ell \in [m_{\lambda}]\) indexes the multiplicity of \(\lambda\), denoted by \(m_{\lambda}\) such that $\Delta_g \phi_{\lambda_i, \ell} + \lambda_\ell \phi_{\lambda_i, \ell} = 0$ for all $\ell \in \{1, \dots, m_{\lambda_i}\}$. Note that the basis starts with the constant function $\phi_0 \equiv 1$ and $\lambda_0 = 0$. Hence, one can write $\Delta_g f = - \sum_{i=0}^{\infty} \sum_{\ell = 1}^{{m_\lambda}_i} \lambda_i \langle f, \phi_{\lambda_i, \ell} \rangle \phi_{\lambda_i, \ell}$.

\begin{lemma}
    For any function $f \in L^2(\mathcal{M})$, such that $f$ is decomposed into the basis $\{\phi_{\lambda, \ell} \}_{\lambda = 1}^{\infty}$ as $f = \sum_{i=0}^{\infty} \sum_{\ell = 1}^{{m_\lambda}_i} \langle f, \phi_{\lambda_i, \ell} \rangle_{L^2(\mathcal{M})} \phi_{\lambda, \ell}$, we know that
    \[
        \|\nabla_g f\|_{L^2(\mathcal{M})}^2 = \sum_{i=0}^{\infty} \sum_{\ell = 1}^{{m_\lambda}_i} \lambda_i \langle f, \phi_{\lambda_i, \ell} \rangle_{L^2(\mathcal{M})}^2,
    \]
    for convergent summations.
\end{lemma}
\begin{proof}
    By \Cref{eq:identity_laplacian},
    \[
    \|\nabla_g f\|_{L^2(\mathcal{M})}^2 & = \int_{\mathcal{M}} \langle \nabla_g f(x), \nabla_g f(x) \rangle_g \, d\operatorname{vol}_g(x) \\
    & = - \int_{\mathcal{M}} f(x) \Delta_g f(x) \, d\operatorname{vol}_g(x) \\
    & = \sum_{i=0}^{\infty} \sum_{\ell = 1}^{{m_\lambda}_i} \lambda_i \langle f, \phi_{\lambda_i, \ell} \rangle_{L^2(\mathcal{M})}^2.
    \]
\end{proof}

\subsection{Commutativity of Laplacian and Isometric Group Actions} \label{app:commute}

Let $G$ be a group acting isometrically on a compact, smooth, boundaryless manifold $\mathcal{M}$. As we stated in the main body of the paper, we have $\Delta_{\mathcal{M}}(T_g\phi) = T_g(\Delta_{\mathcal{M}}(\phi))$ for each smooth function $\phi$ on manifold $\mathcal{M}$, where $T_g\phi = \phi(gx)$. To see how,  note that by \Cref{eq:identity_laplacian}, this is equivalent to 
\begin{align}
    \int_{\mathcal{M}} h \Delta_{\mathcal{M}}(T_g\phi) d\operatorname{vol}_g(x) = 
    \int_{\mathcal{M}} h T_g(\Delta_{\mathcal{M}}(\phi)) d\operatorname{vol}_g(x),
\end{align}
for each smooth function $h$ on manifold $\mathcal{M}$. By changing the variables in the integrable and noting that $dx = d(gx)$ from isometry, we have
\begin{align}
        \int_{\mathcal{M}} h \Delta_{\mathcal{M}}(T_g\phi) d\operatorname{vol}_g(x) &= 
    -\int_{\mathcal{M}}  \langle \nabla h, \nabla T_g \phi\rangle_g d\operatorname{vol}_g(x) \\
    & =-\int_{\mathcal{M}}  \langle \nabla T_{g^{-1}} h, \nabla  \phi\rangle_g d\operatorname{vol}_g(x)\\
    & = \int_{\mathcal{M}} T_{g^{-1}}h \Delta_{\mathcal{M}}(\phi) d\operatorname{vol}_g(x)\\
    & = \int_{\mathcal{M}} h T_g(\Delta_{\mathcal{M}}(\phi)) d\operatorname{vol}_g(x).
\end{align}

\subsection{Weyl's Law under Invariances} \label{app:weyl}

Weyl’s law characterizes the asymptotic distribution of the eigenvalues in a closed-form formula \citep{hormander1968spectral,sogge1988concerning,canzani2013analysis}. Let us denote dimension of the space spanned by the eigenvectors corresponding to eigenvalue of the Laplace-Beltrami operator up to $\lambda$ as
\[
D_\lambda \coloneqq \sum_{\lambda^\prime \leq \lambda} m_{\lambda^\prime}.
\]


\begin{theorem}[Weyl’s law \citep{hormander1968spectral,sogge1988concerning,canzani2013analysis}]
Let $(\mathcal{M}, g)$ be a compact, boundaryless $d$-dimensional Riemannian manifold. The asymptotic behavior of dimension count $D_\lambda$ follows
\[
D_\lambda = \frac{\omega_d \, \operatorname{vol}(\mathcal{M})}{(2\pi)^d} \lambda^{d/2} + \mathcal{O}(\lambda^{(d-1)/2}),
\]
where $ \omega_d = \frac{\pi^{d/2}}{\Gamma\left(\frac{d}{2} + 1\right)} $ is the volume of the unit $d$-dimensional ball in $\mathbb{R}^d$, $\text{vol}(\mathcal{M}) $ is the Riemannian volume of $ \mathcal{M}$, and $\mathcal{O}(\lambda^{(d-1)/2})$ represents the error term.
\end{theorem}

Define $D_{\lambda, G}$ as the dimension of the space induced by projection of the corresponding eigenspaces of $D_\lambda$ into the space of $G$-invariant functions. \citet{tahmasebi2023exact} proved the following characterization over this dimension as $\lambda \to \infty$. 

\begin{theorem}[Dimension counting \citep{tahmasebi2023exact}]
Let $(\mathcal{M}, g)$ be a compact, boundaryless $d$-dimensional Riemannian manifold, and $G$ be a compact finite Lie group acting isometrically on $(\mathcal{M}, g)$. Then.
\[
D_{\lambda, G} = \frac{\omega_d \, \operatorname{vol}(\mathcal{M}/G)}{(2\pi)^d} \lambda^{d/2} + \mathcal{O}(\lambda^{(d-1)/2}),
\]
as $\lambda \to \infty$, where again $ \omega_d$ is the volume of the unit $d$-dimensional ball in $\mathbb{R}^d$.
\end{theorem}

\subsection{Sobolev Spaces on Manifolds}

The ordinary definition of Sobolev spaces on manifolds deals with having square-integrable derivatives up to an order $s$. Here, since our focus is on the spectral approach, we present the spectral definition of Sobolev spaces. 
\begin{definition}[Sobolev spaces] The Sobolev space of functions $H^s(\mathcal{M})$ on a compact, smooth, boundaryless Riemannian manifold $\mathcal{M}$ is defined as:
\[
    H^s(\mathcal{M})\coloneqq \Big \{
            f = \sum_{\lambda} \sum_{\ell=1}^{m_\lambda} f_{\lambda, \ell}\phi_{\lambda,\ell}(x) : 
            \|f\|^2_{H^s(\mathcal{M})}\coloneqq \sum_{\lambda} \sum_{\ell = 1}^{m_\lambda} D_{\lambda} ^{\alpha}f_{\lambda,\ell}^2 < \infty
    \Big\},
\]
    where $\alpha \coloneqq 2s/d$. 
\end{definition}
Note that the above definition is equivalent to the other definition of the Sobolev spaces that involves considering $\lambda^{s}$ instead of $D_\lambda^\alpha$ above. Using Weyl's law (see \Cref{app:weyl}), one can show that both definitions are equivalent. 




\subsection{Sobolev Kernels} \label{app:sob_kernel}
Sobolev spaces are RKHS when $s>d/2$. Indeed, the Sobolev kernel can be defined as:
\[
    K_{H^s(\mathcal{M})}(x,y) \coloneqq \sum_{\lambda} \sum_{\ell=1}^{m_\lambda}  D_\lambda^{-\alpha} \phi_{\lambda, \ell}(x) \phi_{\lambda, \ell}(y).
\]
Note that any group $G$ that acts isometrically on the manifold, also acts on the eigenspace of Laplacian via orthogonal matrices. Since orthogonal matrices preserve the inner product we conclude that
\[
    K_{H^s(\mathcal{M})}(gx,gy) = K_{H^s(\mathcal{M})}(x,y),
\]
for any $g \in G$, which means that the Sobolev kernel is shift-invariant. However, this is clearly not $G$-invariant since it produces small bump functions, which need not be invariant.













\subsection{Kernel Ridge Regresion (KRR)} \label{app:krr}

Consider a Positive-Definite Symmetric (PDS) kernel $K(.,.)$ on a smooth, compact, boundaryless manifold with $H$ denoting its RKHS. The objective of  Kernel Ridge Regression (KRR) estimator is to introduce the RKHS norm to the ERM objective to make sure of finding smooth interpolators:
\begin{align}
    \min_{f \in H} \Big \{ \frac{1}{n}\sum_{i=1}^n(f(x_i ) - y_i)^2 + \eta\|f\|^2_{H}\Big\},
\end{align}
where $\eta$ denotes the regularization parameter that balances the bias and variance terms. Here, the objective function takes a closed-form solution to the represented theorem for kernels. This gives an efficient estimator, which is termed KRR in the literature. 

However, this estimator need not be $G$-invariant even when trained on invariant data. To see why, note that as long as the space $H$ includes non-invariant functions, there is a chances that we find a non-invariant function optimizing the above objective due to the observation noise. Thus, the only way to make sure that the KRR estimator is $G$-invariant is to impose the assumption of having $G$-invariant kernels, which translated to group averaging over the Sobolev kernel:
\begin{align}
    K^G_{H^s(\mathcal{M})}(x,y)\coloneqq \frac{1}{|G|}\sum_{g \in G} K_{H^s(\mathcal{M})}(gx,y).
\end{align}
This method is unfortunately not computationally feasible, even though it achieves minimax optimal generalization bounds for learning under invariances with kernels \citep{tahmasebi2023exact}.












\section{Proofs} \label{app:proofs}


\subsection{Minimal generating set of a Group} \label{app:minimal}

Here, we restate and prove the following lemma on the size of the minimal generating set in group theory for completeness.
\begin{proposition} \label{prop:minial}
    The minimal generating set $S$ of a finite group $G$, has a size $\rho(G) \coloneqq |S| \leq \log_2 (|G|)$.
\end{proposition}
\begin{proof}
    Consider the minimal generating set $S = \{ g_1, g_2, \dots, g_{|S|} \}$ of the finite group $G$. For each $k \in \{1, 2, \dots, |S|\}$, define $G_k = \langle g_1, g_2, \dots, g_k \rangle$. 
    
    The identity $e$ is not equal to any of $g_k$, and hence cannot be a member of any $G_n$, since it can always be produced by combining an element with its inverse. Moreover, for all $k \in \{1, 2, \dots, |S|\}$, we know that $g_{k+1} \notin G_k$, since otherwise $\langle g_1, g_2, \dots, g_{k}, g_{k+2}, \dots g_{|S|} \rangle = G $ which contradicts the minimality of the generating set $S$ for the group $G$. Therefore, $g_{n + 1} G_n$ the left coset of $G_n$ is disjoint from $G_n$. Additionally, by definition, we know that $g_{n + 1} G_n \bigcup G_n \subseteq G_{n + 1}$. Hence, $|G_{n + 1}| \geq |g_{n + 1} G_n | + | G_n | = 2 |G_n| $. By induction, $2^{|S|} = 2^{|S|} |G_1| \leq |G_{|S|}| = |G| $ which establishes the claim.
\end{proof}


\subsection{Constrained Optimization} \label{app:project}

In this section, we preset a detailed analysis of the constrained quadratic optimization problem that is used in \cref{alg:alg}.

\begin{proposition}[Projection into invariant subspace of eigenspaces]\label{prop:projection}
    The optimization problem,
    \[
    \widehat{f}_{\lambda} \coloneqq \argmin_{f_{\lambda}} &~ \sum_{\ell=1}^{m_\lambda} (f_{\lambda,\ell} - \widetilde{f}_{\lambda,\ell})^2, \numberthis{eq:project_appendix} \\
    \textrm{s.t.} & \quad \forall g \in S: D^\lambda(g) f_\lambda = f_\lambda,
    \]
    with $|S| = m$, has a closed form solution,
    \[
    \widehat{f}_{\lambda} = \widetilde{f}_{\lambda} - {B^\lambda}^\top (B^\lambda {B^\lambda}^\top)^\dagger (B^\lambda \widetilde{f}_{\lambda}),
    \]
    where
    \[
    B^\lambda = \begin{bmatrix}
    D^\lambda(g_1) - I \\
    D^\lambda(g_2) - I \\
    \vdots \\
    D^\lambda(g_m) - I
    \end{bmatrix},
    \]
    and $\dagger$ denotes Moore–Penrose inverse.
\end{proposition}
\begin{proof}
    For better readability, we define $B(g_i) \coloneqq D^\lambda(g_i) - I$, where $I \in \mathbb{R}^{m_\lambda \times m_\lambda}$ is the identity matrix of size $m_\lambda$, then,
    \[
    B^\lambda = \begin{bmatrix}
    B(g_1) \\
    B(g_2) \\
    \vdots \\
    B(g_m)
    \end{bmatrix}.
    \]
    For ease of notation, let $a \coloneqq \widetilde{f}_{\lambda} \in \mathbb{R}^{m_\lambda}$ and $a^* \coloneqq \widehat{f}_{\lambda} \in \mathbb{R}^{m_\lambda} $, then the optimization problem (\ref{eq:project_appendix}) can be rewritten as,
    \[
    a^* = \min_{a^{\prime}} \frac{1}{2} \| a - a^{\prime} \|^2 \textrm{ subject to } B^\lambda a^{\prime} = 0.\numberthis{eq:proj_opt_problem}
    \]
    Now, we need to show that the projection of $a$ onto the subspace defined by $\{a^{\prime} \: | \; Ba^{\prime} = 0\}$ has the following analytical form,
    \[
    a^* = a - {B^\lambda}^\top (B^\lambda {B^\lambda}^\top)^\dagger (B^\lambda a).
    \]
    We form the Lagrangian,
    \[
    \mathcal{L}(a^{\prime}, \lambda) = \frac{1}{2} \| a - a^{\prime} \|^2 + \xi^\top B^\lambda a,
    \]
    where $\xi \in \mathbb{R}^{m_\lambda} $ is the vector of Lagrange multipliers. By taking gradients,
    \[
    \frac{\partial \mathcal{L}}{\partial a^{\prime}} = (a^{\prime} - a) + {B^\lambda}^\top \xi = 0, 
    \]
    thus,
    \[
    a^* = a - {B^\lambda}^\top \xi \numberthis{eq:astar}.
    \]
    Substituting back into the constraint $B^\lambda a^* = 0$,
    \begin{align}
    B^\lambda ( a - {B^\lambda}^\top \xi) = B^\lambda a - B^\lambda {B^\lambda}^\top \xi = 0. \label{eq:linear_sys}
    \end{align}
    Hence, $\xi$ satisfies the above linear system which may have infinite number of solutions. We claim that the choice of $\xi^* = (B^\lambda {B^\lambda}^\top)^\dagger B^\lambda a$ leads to the optimal solution of optimization problem~(\ref{eq:proj_opt_problem}). The objective of optimization~(\ref{eq:proj_opt_problem}) is $\| a - a^*\|^2 = \| {B^\lambda}^\top \xi \|_2^2 $. Any solution $\xi$ to the linear system~(\ref{eq:linear_sys}), can be decomposed as $\xi = \xi^* + \xi_0 $ where $\xi_0$ is in the nullspace of $B^\lambda {B^\lambda}^\top$. Hence,
    \[
     \| {B^\lambda}^\top \xi \|_2^2 =  \| {B^\lambda}^\top (\xi^* + \xi_0) \|_2^2 
      \overset{\text{(I)}}{=} \| {B^\lambda}^\top \xi^* \|_2^2 + \| {B^\lambda}^\top \xi_0 \|_2^2 \geq \| {B^\lambda}^\top \xi^* \|_2^2.
    \]
    (I) follows since ${B^\lambda}^\top \xi^*$ and ${B^\lambda}^\top \xi_0$ are orthogonal w.r.t. each other. Placing $\xi^* = (B^\lambda {B^\lambda}^\top)^\dagger B^\lambda a$ in \Cref{eq:astar} concludes the proof.
\end{proof}

\begin{remark}[Time complexity of optimization in each eigenspace]\label{remark:time}
    We arbitrarily chose to use the closed-form solution of the optimization problem (\ref{eq:project_appendix}) instead of iterative approaches. In the closed form solution, we need to calculate the pseuedoinverse of matrix $B^\lambda {B^\lambda}^\top \in \mathbb{R}^{|S| m_\lambda \times |S| m_\lambda}$ which can be done via singular value decomposition (SVD) in $O(|S|^3 m_\lambda^3) $. The other operations are matrix multiplications that are dominated by this part in terms of computational complexity.
\end{remark}

\subsection{Main Theorem} \label{app:main}

\main* 

% This command restates the main theorem of the paper
\begin{proof}
    To prove \cref{thrm:main}, we use \cref{alg:alg}. Let us start by calculating the time and oracle complexity of the algorithm. Given a dataset $\mathcal{S}$ of size $n$, we first compute
    \begin{align}
        \widetilde{f}_{\lambda,\ell} = \frac{1}{n} \sum_{i=1}^n y_i \phi_{\lambda,\ell}(x_i),
    \end{align}
    for each $\lambda$ such that $D_\lambda \leq D = n^{1/(1+\alpha)}$, and each $\ell \in [m_\lambda]$. This requires $\mathcal{O}(n^{1 + 1/(1+\alpha)})$ oracle calls and can be accomplished in time 
    $\mathcal{O}(n^{1 + 1/(1+\alpha)})$. 

    Next, we solve the following constrained quadratic program:
    \begin{align}
        \widehat{f}_{\lambda, \ell} \gets \argmin_{f_{\lambda,\ell}} &~ \sum_{\ell=1}^{m_\lambda} (f_{\lambda,\ell} - \widetilde{f}_{\lambda,\ell})^2, \\
        \text{s.t.} &\quad \forall g \in S: D^\lambda(g) f_\lambda = f_\lambda.
    \end{align}
    This is done for each $\lambda$ such that $D_\lambda \leq n^{1/(1+\alpha)}$. Note that to even set up this program, we need $\mathcal{O}(|S| m_{\lambda}^2)$ oracle calls to find the constraints. We have 
    \begin{align}
        (D^\lambda(g))_{\ell, \ell'} = \langle \phi_{\lambda, \ell}(x), \phi_{\lambda, \ell'}(gx)\rangle_{L^2(\mathcal{M})}
    \end{align}
    for each $\ell, \ell' \in [m_{\lambda}]$. 

    Therefore, the total oracle complexity of the proposed algorithm is
    \begin{align}
        \mathcal{O}\left(\sum_{\lambda: D_\lambda \leq n^{1/(1+\alpha)}} |S| m_{\lambda}^2 + n^{(2+\alpha)/(1+\alpha)}\right).
    \end{align}
    We have already shown in \cref{prop:minial} that one can use a generator set with $\rho(G) \leq \log(|G|)$. Moreover, note that 
    \begin{align}
        \sum_{\lambda: D_\lambda \leq n^{1/(1+\alpha)}} m_{\lambda}^2 = \mathcal{O}(n^{2/(1+\alpha)}). 
    \end{align}
    Therefore, the oracle complexity is
    \begin{align}
        \mathcal{O}\left(\log(|G|) n^{2/(1+\alpha)} + n^{(2+\alpha)/(1+\alpha)}\right).
    \end{align}

Let us now calculate the time complexity of finding the estimator. We have already established that we can compute the empirical estimation in time $\mathcal{O}(n^{1 + 1/(1+\alpha)})$. Next, we need to solve the constrained quadratic program with $\log(|G|)$ constraints and $m_{\lambda}$ variables for each $\lambda$ such that $D_\lambda \leq n^{1/(1+\alpha)}$. Using the proposed algorithm in \cref{app:project} and also \cref{remark:time}, we can solve each of these constrained quadratic programs in time $\mathcal{O}(\log^3(|G|) m_\lambda^3)$. Therefore, the total time complexity of this step is bounded by
\begin{align}
    \mathcal{O}\left(\sum_{\lambda: D_\lambda \leq n^{1/(1+\alpha)}} \log^3(|G|) m_\lambda^3\right) = \mathcal{O}\left(\log^3(|G|) n^{3/(1+\alpha)}\right).
\end{align}
This proves that the total time complexity of \cref{alg:alg} is 
\begin{align}
    \mathcal{O}\left(\log^3(|G|) n^{3/(1+\alpha)} + n^{(2+\alpha)/(1+\alpha)}\right).
\end{align}

Finally, note that given $\widehat{f}$, one can evaluate it on new unlabeled data $x \in \mathcal{M}$ using the formula:
\begin{align}
    \widehat{f}(x) = \sum_{\lambda : D_\lambda \leq D} \sum_{\ell=1}^{m_\lambda} \widehat{f}_{\lambda,\ell} \phi_{\lambda,\ell}(x),
\end{align}
with $D = n^{1/(1+\alpha)}$, which requires both time and oracle complexity of $\mathcal{O}(n^{1/(1+\alpha)})$.

To complete the proof, we need to study the convergence of the population risk of the proposed estimator. We first note that
\begin{align}
    \mathcal{R}(\widehat{f}) = \mathbb{E}[\| \widehat{f} - f^\star\|^2_{L^2(\mathcal{M})}] \le 
        2 \mathbb{E}[\| \widehat{f} - f^\star_{\leq D}\|^2_{L^2(\mathcal{M})}]
        +  2 \mathbb{E}[\| f^\star_{>D}\|^2_{L^2(\mathcal{M})}],
\end{align}
where $f^\star_{\leq D}$ denotes the orthogonal projection of the function $f^\star$ onto the space of eigenfunctions with eigenvalues satisfying $D_\lambda \leq D$. Moreover, $f^\star_{>D} = f^\star - f^\star_{\leq D}$. 

First, let us upper bound the second term. Note that, according to the assumption, $f^\star \in H^s(\mathcal{M})$. Thus, 
\begin{align}
    \mathbb{E}[\| f^\star_{>D}\|^2_{L^2(\mathcal{M})}] &= \sum_{\lambda: D_\lambda > D} \sum_{\ell=1}^{m_\lambda} (f^\star_{\lambda,\ell})^2 \\
    & = \sum_{\lambda: D_\lambda > D} \sum_{\ell=1}^{m_\lambda} D_\lambda^{-\alpha} D_\lambda^{\alpha}(f^\star_{\lambda,\ell})^2 \\
    & \leq D^{-\alpha} \sum_{\lambda: D_\lambda > D} \sum_{\ell=1}^{m_\lambda} D_\lambda^{\alpha} (f^\star_{\lambda,\ell})^2 \\
    & \leq D^{-\alpha} \sum_{\lambda} \sum_{\ell=1}^{m_\lambda} D_\lambda^{\alpha} (f^\star_{\lambda,\ell})^2 \\
    & = D^{-\alpha} \|f^\star\|_{H^s(\mathcal{M})}^2.
\end{align}
Now we focus on the first term. Note that
\begin{align}
    \mathbb{E}[\| \widehat{f} - f^\star_{\leq D}\|^2_{L^2(\mathcal{M})}] = \sum_{\lambda: D_\lambda \leq D} \sum_{\ell = 1}^{m_\lambda} \mathbb{E}[|\widehat{f}_{\lambda,\ell} - f^\star_{\lambda,\ell}|^2]. 
\end{align}
According to the definition, we have 
\begin{align}
    f^\star_{\lambda,\ell} = \mathbb{E}_x[f^\star(x)\phi_{\lambda, \ell}(x)] = \mathbb{E}_{x,y}[y\phi_{\lambda, \ell}(x)],
\end{align}
for each $\lambda,\ell$. Moreover, $\widetilde{f}_{\lambda,\ell}$ is the empirical estimation obtained from data:
\begin{align}
    \widetilde{f}_{\lambda,\ell} = \frac{1}{n} \sum_{i=1}^n y_i \phi_{\lambda,\ell}(x_i).
\end{align}
Thus, we obtain
\begin{align}
    \mathbb{E}[|\widetilde{f}_{\lambda,\ell} - f^\star_{\lambda,\ell}|^2] &= \frac{1}{n} \mathbb{E}\left[| y \phi_{\lambda,\ell}(x) - \mathbb{E}[y\phi_{\lambda,\ell}(x)] |^2\right] \\
    & = \frac{1}{n} \mathbb{E}\left[| \epsilon \phi_{\lambda,\ell}(x) + f^\star(x) \phi_{\lambda,\ell}(x) - \mathbb{E}[f^\star(x)\phi_{\lambda,\ell}(x)] |^2\right] \\
    & = \frac{1}{n} \left( \sigma^2 \mathbb{E}[\phi_{\lambda,\ell}^2] + \mathbb{E}\left[|f^\star(x) \phi_{\lambda,\ell}(x) - \mathbb{E}[f^\star(x)\phi_{\lambda,\ell}(x)] |^2\right] \right) \\
    & \leq \frac{1}{n} \left( \sigma^2 + \mathbb{E}[f^\star(x)^2 \phi_{\lambda,\ell}^2(x)] \right) \\
    & \leq \frac{1}{n} \left( \sigma^2 + \|f^\star\|^2_{L^{\infty}(\mathcal{M})} \right),
\end{align}
where we used the orthonormality of the eigenfunctions $\phi_{\lambda,\ell}$. Then, summing this up to dimension $D$ gives:
\begin{align}
    \mathbb{E}[\| \widetilde{f} - f^\star_{\leq D}\|^2_{L^2(\mathcal{M})}] \leq \frac{D}{n} \left( \sigma^2 + \|f^\star\|^2_{L^{\infty}(\mathcal{M})} \right).
\end{align}
Note that, by definition, $\widehat{f} = P_G \widetilde{f}$, where $P_G: L^2(\mathcal{X}) \to L^2(\mathcal{X})$ is the orthogonal projection operator onto the invariant functions. Therefore, we have
\begin{align}
    \mathbb{E}[\| \widehat{f} - f^\star_{\leq D}\|^2_{L^2(\mathcal{M})}]  &= 
    \mathbb{E}[\| P_G\widetilde{f} - f^\star_{\leq D}\|^2_{L^2(\mathcal{M})}]\\
    & = \mathbb{E}[\|P_G \widetilde{f} - P_Gf^\star_{\leq D}\|^2_{L^2(\mathcal{M})}] 
    \\& \le  \mathbb{E}[\|\widetilde{f} - f^\star_{\leq D}\|^2_{L^2(\mathcal{M})}] \\
    &\le  \frac{D}{n} \left( \sigma^2 + \|f^\star\|^2_{L^{\infty}(\mathcal{M})} \right),
\end{align}
where the penultimate step follows from $P_G f^\star_{\leq D} = f^\star_{\leq D}$.




Therefore, we can combine the two terms to derive the following population risk bound:
\begin{align}
     \mathcal{R}(\widehat{f}) = \mathbb{E}[\| \widehat{f} - f^\star\|^2_{L^2(\mathcal{M})}] \leq \frac{D}{n} \left( \sigma^2 + \|f^\star\|^2_{L^{\infty}(\mathcal{M})} \right) + D^{-\alpha} \|f^\star\|_{H^s(\mathcal{M})}^2.
\end{align}
We can now specify the above bound to \(D = n^{1/(1+\alpha)}\), which is used in the algorithm, to get:
\begin{align}
    \mathcal{R}(\widehat{f}) = \mathbb{E}[\| \widehat{f} - f^\star\|^2_{L^2(\mathcal{M})}] \leq n^{-\alpha/(1+\alpha)} \left( \sigma^2 + \|f^\star\|^2_{L^{\infty}(\mathcal{M})} \right) + n^{-\alpha/(1+\alpha)} \|f^\star\|_{H^s(\mathcal{M})}^2,
\end{align}
which is equivalent to 
\begin{align}
    \mathcal{R}(\widehat{f}) = \mathcal{O}(n^{-\alpha/(1+\alpha)}).
\end{align}
This completes the proof.

\begin{remark}
    Note that other choices of \(D\) may or may not yield better bounds depending on the sparsity of the solution. For this sparsity-unaware upper bound that we use, such a choice of \(D\) is optimal. Additionally, since we focus on polynomial time algorithms, we cannot choose exponentially large \(D\) even if they deliver gains in sample complexity.
\end{remark}

\end{proof}

\section{Experimental Results}\label{app:exp}

The plots for the experiments discussed in \cref{exper} are presented below.



\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/non-invariant.png}
    \caption{Invariance Discrepancy measure of Kernel Ridge Regression (\texttt{KRR}) for various choices of the regularization parameter $\lambda$. The resulting estimator, \texttt{KRR}, is not invariant with respect to the group $G$ of sign averages $\{\pm 1\}^d$, whereas \texttt{Spec-Avg} is $G$-invariant by construction. Each point in the plot represents an average over 10 different random seeds. The Invariance Discrepancy measure used for this plot is defined as  $ \sup_{x \in \mathcal{X}, g \in G} |\widehat{f}(x) - \widehat{f}(g x)|,$
    where $\widehat{f}$ is the estimator. The set $\mathcal{X}$ consists of 100 points uniformly sampled from the interval $[-1, 1]^d$, independently and identically distributed.
    }
    \label{fig:exp1}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/population_risk.png}
    \caption{Test error (empirical excess population risk) of \texttt{KRR} for different choices of the regularization parameter $\lambda$ and \texttt{Spec-Avg} for different choices of the sparsity parameter $D$. Conceptually, higher values of $\lambda$ and lower values of $D$ encourage sparser representations for the estimators \texttt{KRR} and \texttt{Spec-Avg}, respectively. As suggested by our theory, it can be observed that test error rates of the same order can be achieved by \texttt{Spec-Avg} and \texttt{KRR} with appropriate choices of hyperparameters. Note that the test errors are shown on a log scale. Their almost linear behavior implies that they are polynomial functions of the number of training samples with comparable orders. We note that each point in the plot represents an average over 10 different random seeds.}
    \label{fig:exp2}
\end{figure}


\end{document}

