\section{Related Work}
Generalization bounds and sample complexity for learning with invariances have been extensively studied, particularly in the context of invariant kernels. Works such as \citet{elesedy2021provably}, \citet{bietti2021sample}, \citet{tahmasebi2023exact}, and \citet{mei2021learning} provide insights into this area. Additionally, studies on equivariant kernels \citep{elesedy2021provablyinv, petrache2023approximation} further our understanding of how equivariances affect learning. PAC-Bayesian methods have also been applied to derive generalization bounds under equivariances \citep{behboodi2022pac}. More recently, \citet{kiani2024on} explored the complexity of learning under symmetry constraints for gradient-based algorithms. For studies on the optimization of kernels under invariances, see \citet{teo2007convex}.

A variety of methods have been proposed to enhance the performance of kernel-based learning models. One prominent approach is the use of random feature models \citep{rahimi2007random}, which approximate kernels using randomly selected features. Low-rank kernel approximation techniques, such as the Nystr√∂m method \citep{williams2000using, drineas2005nystrom}, have also been proposed to reduce the computational complexity of kernel methods; see also \citet{bach2013sharp, cesa2015complexity}. Divide-and-conquer algorithms offer another pontential avenue for kernel approximation \citep{zhang2013divide}. Additionally, the impact of kernel approximation on learning accuracy is well-documented in \citet{cortes2010impact}.

Our work focuses on learning with invariances, which differs significantly from the tasks of learning invariances or measuring them in neural networks. For example, \citet{benton2020learning} address how neural networks can learn invariances, while \citet{goodfellow2009measuring} study methods to measure the degree of invariance in network architectures.

Invariance in kernel methods is not limited to group averaging. Other approaches such as frame averaging \citep{puny2021frame}, canonicalization \citep{kaba2023equivariance, ma2024canonization}, random projections \citep{dym2024low}, and parameter sharing \citep{ravanbakhsh2017equivariance} have also been proposed to construct invariant function classes. However, canonicalization and frame averaging face challenges, particularly concerning continuity, which has been addressed in recent works like \citet{dym2024equivariant}.

In specialized tasks such as graphs, image, and pointcloud data, Graph Neural Networks (GNNs) \citep{scarselli2008graph, xu2018powerful}, Convolutional Neural Networks (CNNs) \citep{krizhevsky2012imagenet, li2021survey}, and Pointnet \citep{qi2017pointnet, qi2017pointnetplus} have demonstrated the effectiveness of leveraging symmetries. Symmetries have also been successfully integrated into generative models \citep{bilovs2021scalable, niu2020permutation, kohler2020equivariant}. For a broader discussion on various types of invariances and their applications across machine learning tasks, see \citet{bronstein2017geometric}.