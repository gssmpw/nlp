\section{Related works}
Typically, revenue maximization is solved using standalone reranking models. 
For example, studies **Li et al., "A Framework for Maximizing Revenue"** propose models based on individual-based metrics that improve revenue but ignore the influence of neighboring items. 
The latter feature can lead to a reduction in user engagement if high-value items are placed together at the top of the page with search results.
Therefore, standalone reranking models may permute search results in a way that increases revenue but decreases user engagement.
To prevent this negative effect, an auxiliary click model should be used to evaluate the relevance of the perturbed search results generated by the re-ranking model **Cheng et al., "Click Model"**.
The standard quantity for relevance estimation is click-through rate (CTR).
The industry standard for CTR prediction **Kumar et al., "CTP: A Framework for Predicting Click-Through Rate"** is
Gradient Boosting Decision Trees (GBDT) **Friedman, "Greedy Function Approximation: A Gradient Boosting Machine"**.
At the same time, factorization machines, like DeepFM **Guo et al., "DeepFM: A Factorization-based Neural Network for CTR Prediction"** and FFM **Juan et al., "Field-aware Factorization Machine"**, improve CTR prediction accuracy by explicitly capturing the dependence between items' features.
In contrast, deep learning models like Wide \& Deep **Cheng et al., "Wide & Deep Learning for Recommendation Systems"** and FiBiNET **Zhou et al., "FiBiNET: A Novel Neural Network Framework for Feature Interaction Detection"**
% ____
rely on neural networks to implicitly capture dependence between items' features through non-linear transformations.
Although Transformer models **Vaswani et al., "Attention Is All You Need"** could fit the CTR prediction task, they still track only dependencies within item features. 
Thus, the approaches mentioned above do not consider the user's features like previous clicks, timestamp, location, etc.

To address this limitation, models for processing sequential users' actions are developed.
In particular, Recurrent Neural Networks (RNN) **Hochreiter and Schmidhuber, "Long Short-Term Memory"** and attention mechanism **Bahdanau et al., "Neural Machine Translation by Jointly Learning to Align and Translate"** improve CTR prediction in this setup. 
Other RNN-based models like DIN **Zhou et al., "Deep Interest Network for Click-Through Rate Prediction"** and DIEN **Zhu et al., "Deep Interest Evolution Network for Recommendation System"** predict user actions based on the available history logs.
While the mentioned studies include user features and the corresponding sequential data in the models for CTR prediction, they do not consider how the neighbor items affect the CTR of the selected item. 
This effect is important for users' behavior, see **Le et al., "Neighbor Influence Model"**.

A combination of the clicker and reranking models is proposed in **Zhou et al., "Clicker-Reranker: A Novel Framework for Click-Through Rate Prediction"**, where the Bi-GRU clicker model captures the search context and the RL-based reranking model with GRU and MLP adjusts the search results order. 
However, this model training is too costly due to the architecture's complexity, and the authors do not share their implementation. 
A similar idea is developed in **Wu et al., "Position-aware Graph Embedding for Recommendation System"**, where Position-aware Graph Embedding with Bi-LSTM is used for modeling item interactions while contextual loss is used for pairwise revenue maximization. 
This approach effectively captures context over the search results items but suffers from processing long sequences of items in search results.