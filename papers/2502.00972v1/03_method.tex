\section{Method}
\label{sec:method}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.93\textwidth]{figs/hth.pdf}
  \vspace{-5pt}
  \caption{Illustration of our diffusion Hybrid Hydra (\ours{}) model for image and video generation. The architecture consists of $N$ stacked blocks, each comprising a cross-attention layer, a token mixer, and a feed-forward network. (a) The token mixer can be implemented as either the Hydra state space model or self-attention. (b) For image data, we use horizontal and vertical bidirectional raster scans on tokens, and for video data, an additional bidirectional temporal scan is applied.}
  \label{fig:hth_model}
  \vspace{-7pt}
\end{figure*}


We present our Hydra-Transformer Hybrid model (\ours{}), a diffusion framework for text-guided image and video generation (Figure~\ref{fig:hth_model}). Similar to the Diffusion Transformer~\citep{peebles2023dit}, we consider the simplest network components and architecture to establish a general and highly scalable large model. Inspired by recent study~\citep{waleffe2024empirical}, we effectively combine the Hydra state space model and self-attention by simply switching the token mixer across layers to allow our \ours{} to benefit from both efficient processing and global interaction. We specify the network architecture of \ours{} in \S\ref{subsec:hth} and \S\ref{subsec:integration}, and the adaptation of T2I to T2V training in \S\ref{subsec:adapt}.

\subsection{\ours{} Architecture}
\label{subsec:hth}

\paragraph{Data Encoders} 
The framework starts with textual and visual encoders for preprocessing the text prompt and its corresponding image or video. Similar to existing approaches~\citep{ho2022imagenvideo,kondratyuk2023videopoet,sun2024autoregressive,FLUX2024}, we adopt the T5 language model~\citep{raffel2020exploringt5}, specifically FLAN-T5-XXL~\citep{chung2024scalingt5}, to acquire textual representations. We compress raw visual input into a low-dimensional continuous latent space using a 3D VAE adapted from MAGVIT-v2~\citep{yu2023magvitv2}. The VAE can encode 2D images and incorporate motion encoding from video frames. Both VAEs produce 12-channel latent representations with an 8$\times$ spatial compression and $\frac{16}{5}\times$ temporal compression.


\paragraph{Denoising Model}
As visualized in the top-left of Figure~\ref{fig:hth_model}, the denoising model in \ours{} is formed by stacking $N$ blocks of residual-connected and pre-normed~\citep{xiong2020onlayer,lei2016layernorm} cross-attention, token mixer, and feed-forward layers. Starting with the cross-attention layer, it attends the input token sequence (\textit{i.e.}, the noised image/video latents) to the textual representations to acquire language guidance. Then, the token mixer layer, either Hydra or self-attention, models and coordinates the interactions among the visual latents, followed by a feed-forward layer to incorporate general knowledge. We choose Hydra among the SSM variants due to its efficient implementation of a non-causal model with superior expressiveness~\citep{hwang2024hydra}. We refer to Appendix~\ref{appsec:token_mixers} for comparison between unidirectional SSM, bidirectional SSM, the state-sharing VSSD~\citep{shi2024vssd}, self-attention, and Hydra.


\begin{figure*}[t]
  \centering
  \includegraphics[width=0.80\textwidth]{figs/hybrid_formula.pdf}
  \vspace{-7pt}
  \caption{Illustration of the model adaptation from state 1 (T2I) to state 2 (T2V). For each set of 11 blocks in our \ours{} model, we change the scanning pattern of certain Hydra layers from spatial-major to temporal-major scan when processing video data.}
  \label{fig:hybrid_formula}
  \vspace{-10pt}
\end{figure*}


\paragraph{Scanning Patterns} Due to the bi-directional sequential nature of Hydra, the processed tokens are biased towards a higher correspondence to the adjacent tokens in the flattened 1D sequence, which contradicts the structure continuity of 2D and 3D visual data. To enlarge the receptive field of tokens, we alternate the scanning pattern between each of the two consecutive Hydra blocks. As shown in Figure~\ref{fig:hth_model}, we only consider two bi-directional raster scans, 
% from top-left to bottom-right, 
via either a horizontal or a vertical path. While previous research has explored other scanning patterns in various domains~\citep{zhang2024surveymamba,xu2024surveymamba}, their impact on large-scale diffusion SSMs remains unclear. Experimenting with different scanning patterns falls outside the scope of this paper and is left for future work.

\paragraph{Conditioning Mechanisms}
We leverage simple but effective methods to incorporate the diffusion and textual conditions. For the learnable diffusion timestep encoding, we add it directly to the noised latents once, before passing them to the first \ours{} layer. In contrast to the Adaptive LayerNorm applied in DiT~\citep{peebles2023dit}, this approach significantly saves the parameters and computation of the model. For the text prompts, we use a cross-attention layer to introduce semantic guidance from the text tokens to the visual tokens. Exploring other possible design choices, such as formulating in-context multimodal SSM (\textit{i.e.}, prepending text tokens to visual tokens, and initiating scanning from the text tokens~\citep{hu2024zigma}), is out of the scope of this paper. We leave this exploration for future work.

\subsection{Hydra and Self-Attention Integration}
\label{subsec:integration}
Inspired by existing works that combine SSM layers and Transformer layers~\citep{zuo2022augmented,waleffe2024empirical,lieber2024jamba,fei2024dimba,ziwen2024longlrm,wang2024mambainllama} as well as other linear hybrid RNN networks~\citep{botev2024recurrentgemma,de2024griffin}, we create our hybrid model that enjoys the efficiency advantage and global interaction from SSM and self-attention, respectively. As shown in Figure~\ref{fig:hybrid_formula}(a), our \ours{} model consists of a total $N{=}33$ blocks, with each set of 11 blocks containing 10 Hydra blocks followed by a final self-attention block. We formulate \ours{} to have a high Hydra-to-Transformer ratio ($30{:}3$) to exploit the modeling capacity of SSMs and preserve efficiency.


\subsection{T2I to T2V Adaptation}
\label{subsec:adapt}
Following the common multi-stage training strategy~\citep{hong2022cogvideo,blattmann2023align,girdhar2023emuvid,bar2024lumiere}, we first train a base T2I model before applying video data, allowing the T2V \ours{} to inherit the rich semantics and high visual quality learned from abundant images. However, video data introduces an extra dimension that significantly increases the modeling distance between spatially or temporally adjacent tokens and demands additional temporal-scanning SSMs to model the frame-wise consistency and evolution. Our early empirical results indicate that neither directly using the stage 1 spatial-major scanning (Figure~\ref{fig:hybrid_formula}(a)) nor adding new temporal-major scanning Hydra blocks is effective for video data; we hypothesize that the former results in a weak temporal receptive field for the tokens, while the latter introduces excessive noise, hindering learning. 

Through extensive experiments, we found a simple but surprisingly effective method -- revising the scanning patterns of certain spatial-major scanning blocks to temporal-major scanning. As illustrated in Figure~\ref{fig:hth_model}(b) and Figure~\ref{fig:hybrid_formula}(b), we directly change every two Hydra blocks with horizontal and vertical scans ($\texttt{H}$ and $\texttt{V}$) to temporal-horizontal and temporal-vertical scans ($\texttt{TH}$ and $\texttt{TV}$, \textit{i.e.}, the SSMs scan all tokens at the same temporal position before moving to a horizontal or vertical adjacent token at the first frame), while having the other blocks to perform spatial-temporal frame-by-frame scanning ($\texttt{HT}$ and $\texttt{VT}$). Note that for image data, temporal scans are absent, so all layers function as in stage 1. Therefore, this approach actually forces 40\% of the Hydra blocks to learn additional temporal-major modeling.

