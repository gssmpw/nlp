\section{Experiments}
\label{sec:experiments}

We provide the implementation details of our model, along with the results of text-to-image and text-to-video generation at different resolutions.

\paragraph{Implementation Details} 
As mentioned in \S\ref{subsec:integration}, our \ours{} model comprises 33 blocks, with every 10 Hydra blocks followed by a self-attention block. The model uses a hidden dimension of 3072 throughout, with Hydra configured to have an internal state dimension of 256, \texttt{ngroups} set to 1, a head dimension of 64, an expansion factor of 2$\times$, and a convolution window size of 7. For both cross-attention and self-attention layers, the KV-dimension matches the model dimension (3072), utilizing 32 attention heads and incorporating RMSNorm~\citep{zhang2019rmsnorm} for QK-Normalization~\citep{henry2020qknorm}. The MLP in each block applies an expansion ratio of 2$\times$ and consists of two nonlinear layers. Together, the above configurations result in the final 5B-parameter \ours{} model.
As illustrated in Figure~\ref{fig:hth_model}, residual connections are employed at the layer level, linking features preceding each LayerNorm~\citep{lei2016layernorm} to the output features of the subsequent layer.
We use a patch size of 2$\times$2 to further compress the spatial dimension of latents produced by the VAE. To incorporate positional information, we use a non-learnable absolute sinusoidal positional embedding~\citep{vaswani2017attention}, which is directly added to the input tokens.

\paragraph{Training} 
We adopt a multi-stage training strategy to enable high-resolution image and video generation. Initially, we train a base T2I model on 256p internal data for 200K iterations with a batch size of 1024, followed by an additional 286K iterations with an increased batch size of 3072. The starting learning rate is set to 1e-4 and decays according to a cosine annealing schedule. Next,
\begin{itemize}
    \item \textbf{1K T2I}. We apply a fast adaptation training to extend the model for high-resolution image generation. Specifically, we fine-tune the 256p base model on 1K-resolution images using a batch size of 576, training for only 10,000 iterations with a learning rate that rapidly decays from 5e-5 to 1e-6.
    \item \textbf{180p T2V}. To obtain a base video generation model, we continue training the T2I base model on a mixture of 256p images and 180p videos (characterized by a smaller number of frames on average and varying FPS) over 154K iterations. 
    We adopt a stepwise learning rate reset approach, where the starting learning rate of the cosine annealing schedule is reset whenever the validation loss plateaus. In our experiment, the learning rate is reset four times: from 1e-4 to 5e-5, then to 3e-5, and finally to 1e-5.
    This training stage enables the model to synthesize video data while preserving the learned semantics and visual quality. 
    \item \textbf{360p T2V}. Finally, we tune the 180p video model using 360p 128-frame video data with a batch size of 288. Leveraging a similar fast adaptation training strategy, we train the model for only 10K iterations with a learning rate that decays from 1e-5 to 1e-6.
\end{itemize} 
We apply the diffusion noise scheduling and loss introduced in InstaFlow~\citep{liu2023instaflow} and train the model following standard diffusion model training procedure~\citep{ho2020ddpm} with classifier-free guidance~\citep{ho2022classifier}.

\paragraph{Inference} 
At inference, we apply DDIM~\citep{song2020ddim} to denoise the latent representations. The denoised latents are then decoded by our internal VAE, with the resulting outputs presented and evaluated in this paper without any post-processing, such as super-resolution.


\subsection{Results and Discussions}

\subsubsection{Visualization} We present several 1K$+$ images and 360p video frames generated by our \ours{} model in Figure~\ref{fig:visualize_1} and Figure~\ref{fig:visualize_2}, respectively. Additional results, including playable videos, are provided in Appendix~\ref{appsec:visualization} and the supplemental materials. The visualizations demonstrate that the model can produce high-fidelity outputs across diverse styles consistent with the given text prompts. Notably, for high-res image generation, despite being trained on mixed-aspect ratios 1K data (with extreme \texttt{Height:Width} ratios of 512:2048 and 2048:512), the model is capable of zero-shot generating around 3.5$\times$ larger 2K images, including resolutions up to 1920$\times$1920, 1440$\times$2560, and 2560$\times$1440. We refer readers to Appendix~\ref{appsec:highres_zeroshot} for more details about the zero-shot experiments.


\begin{table}[t]
  \caption{Comparison of 256$\times$256 T2I generation.}
  \vspace{3pt}
  \begin{center}
  \resizebox{0.67\columnwidth}{!}{
    \begin{tabular}{lc}
        \hline \hline & \\[-2.0ex]
        \multicolumn{1}{c}{\multirow{2}{*}{Models}} & \multicolumn{1}{c}{MSCOCO-30K} \\
        \cline{2-2} & 
        \multicolumn{1}{c}{FID$\downarrow$}\Tstrut\\
        \hline \hline & \\[-2.0ex]
        LDM~\citep{rombach2022ldm}              & 12.63 \\
        DALL-E 2~\citep{ramesh2022dalle2}       & 10.39 \\
        Imagen~\citep{saharia2022imagen}        & 7.27  \\
        eDiff-I~\citep{balaji2022ediff}         & 6.95  \\
        Transfusion~\citep{zhou2024transfusion} & 6.78  \\
        DeepFloyd~\citep{deepfloyd2024}         & 6.66  \\
        RAPHAEL~\citep{xue2024raphael}          & 6.61  \\
        \hline & \\[-2.0ex]
        DiT (Ours, baseline)  & 12.53 \\
        HTH (Ours)  & 11.85 \\
        \hline \hline
      \end{tabular}
        }
  \end{center}
  \label{tab:eval_256p}
  \vspace{-15pt}
\end{table}


\begin{table}[t]
  \caption{Comparison of 1024$\times$1024 T2I generation.}
  \vspace{-5pt}
  \begin{center}
  \resizebox{\columnwidth}{!}{
    \begin{tabular}{lccc}
        \hline \hline & \\[-2.0ex]
        \multicolumn{1}{c}{\multirow{2}{*}{Models}} & \multicolumn{2}{c}{MJHQ-30K} & \multicolumn{1}{c}{GenEval} \\
        \cline{2-4} & 
        \multicolumn{1}{c}{FID$\downarrow$} & \multicolumn{1}{c}{CLIP-Score$\uparrow$} &
        \multicolumn{1}{c}{Overall$\uparrow$} \Tstrut\\
        \hline \hline & \\[-2.0ex]
        SDXL~\citep{podell2023sdxl}      &  8.76 & 28.60  & 0.55 \\
        PixArt-$\Sigma$~\citep{chen2024pixartsigma} &  6.34 & 27.62  & 0.54 \\
        Playground v2.5~\citep{li2024playground} &  6.84 & 29.39  & 0.56 \\
        SD3-medium~\citep{esser2024sd3}  & 11.92 & 27.83  & 0.62 \\
        DALL-E 3~\citep{dalle3}          &    -- &    --  & 0.67 \\
        FLUX-dev~\citep{FLUX2024}        & 10.15 & 27.47  & 0.67 \\
        FLUX-schnell~\citep{FLUX2024}    &  7.94 & 28.14  & 0.71 \\
        \hline & \\[-2.0ex]
        DiT (Ours, baseline)      &  6.90 & 27.37  & 0.63 \\
        \ours{} (Ours)  &  6.52 & 27.26  & 0.58 \\
        \hline \hline
      \end{tabular}
        }
  \end{center}
  \label{tab:eval_1k}
  \vspace{-5pt}
\end{table}


\begin{table*}[t]
  \caption{Comparison with existing text-to-video models on VBench~\citep{huang2024vbench}. The exact resolutions of our 180p and 360p videos are 192$\times$320 and 384$\times$640, respectively. We choose 12 evaluation dimensions. Higher metric values indicate better performance.}  % 12 out of 16
  \vspace{-5pt}
  \begin{center}
  \resizebox{\textwidth}{!}{
  \begin{tabular}{lcccccccccccc}
    \hline \hline & \\[-2.0ex]
    Models & \begin{tabular}{@{}c@{}}Subject \\ Consistency\end{tabular} & \begin{tabular}{@{}c@{}}Background \\ Consistency\end{tabular} & \begin{tabular}{@{}c@{}}Aesthetic \\ Quality\end{tabular} & \begin{tabular}{@{}c@{}}Object \\ Class\end{tabular} & Scene & \begin{tabular}{@{}c@{}}Human \\ Action\end{tabular} & \begin{tabular}{@{}c@{}}Apperance \\ Style\end{tabular} & \begin{tabular}{@{}c@{}}Temporal \\ Style \end{tabular} & \begin{tabular}{@{}c@{}}Temporal \\ Flickering\end{tabular} & \begin{tabular}{@{}c@{}}Motion \\ Smoothness \end{tabular} & \begin{tabular}{@{}c@{}}Dynamic \\ Degree\end{tabular} & \begin{tabular}{@{}c@{}}Overall \\ Consistency\end{tabular} \\ & \\[-2.0ex]
    \hline \hline & \\[-2.0ex]
    ModelScope       & 89.87 & 95.29 & 56.39 & 82.25 & 39.26 & 92.40 & 23.39 & 25.37 & 98.28 & 95.79 & 66.39 & 25.67 \\    
    LaVie            & 91.41 & 97.47 & 54.94 & 91.82 & 52.69 & 96.80 & 23.56 & 25.93 & 98.30 & 96.38 & 49.72 & 26.41 \\
    Show-1           & 95.53 & 98.02 & 57.35 & 93.07 & 47.03 & 95.60 & 23.06 & 25.28 & 99.12 & 98.24 & 44.44 & 27.46 \\
    OpenSora-v1.2    & 96.75 & 97.61 & 56.18 & 82.22 & 42.44 & 91.20 & 23.95 & 24.55 & 99.47 & 98.50 & 42.39 & 26.85 \\
    VideoCrafter-2.0 & 96.85 & 98.22 & 63.13 & 92.55 & 55.29 & 95.00 & 25.13 & 25.84 & 98.41 & 97.73 & 42.50 & 28.23 \\
    T2V-Turbo (VC2)  & 96.28 & 97.02 & 63.04 & 93.96 & 55.58 & 95.20 & 24.42 & 25.51 & 97.48 & 97.34 & 49.17 & 28.16 \\
    CogVideoX-5B     & 96.23 & 96.52 & 61.98 & 85.23 & 53.20 & 99.40 & 24.91 & 25.38 & 98.66 & 96.92 & 70.97 & 28.23 \\
    Pika             & 96.94 & 97.36 & 62.04 & 88.72 & 49.83 & 86.20 & 22.26 & 24.22 & 99.74 & 99.50 & 47.50 & 25.94 \\
    Kling (2024-07)  & 98.33 & 97.60 & 61.21 & 87.24 & 50.86 & 93.40 & 19.62 & 24.17 & 99.30 & 99.40 & 46.94 & 26.42 \\
    Gen3             & 97.10 & 96.62 & 63.34 & 87.81 & 54.57 & 96.40 & 24.31 & 25.33 & 98.61 & 99.23 & 60.14 & 26.69 \\
    ARLON            & 93.41 & 97.10 & 61.01 & 89.80 & 54.43 & --    & --    & --    & 99.37 & 98.92 & 52.77 & 27.27 \\
    Emu3             & 95.32 & 97.69 & 59.64 & 86.17 & 37.11 & 77.71 & 20.92 & 23.26 & 98.57 & 98.93 & 79.27 & 24.79 \\
    \hline & \\[-2.0ex]
    \ours{}-180p (Ours)   & 95.71 & 98.53 & 55.58 & 91.61 & 50.12 & 97.60 & 25.03 & 25.34 & 98.92 & 98.87 & 79.72 & 27.36 \\
    \ours{}-360p (Ours)   & 91.36 & 95.34 & 62.71 & 86.08 & 46.28 & 96.80 & 25.09 & 25.10 & 97.00 & 98.75 & 84.44 & 27.41 \\
    \hline \hline
  \end{tabular}}
  \end{center}
  \label{tab:eval_vbench}
  \vspace{-5pt}
\end{table*}


\subsubsection{Benchmark Evaluation}
We compare our generated images and videos with existing approaches on the widely applied benchmarks. Specifically, we evaluate 256$\times$256 images produced by our base image model on MSCOCO-30K~\citep{lin2014mscoco}, 1024$\times$1024 images produced by our 1K-tuned image model on MJHQ-30K~\citep{li2024playground} and GenEval~\citep{ghosh2024geneval}, as well as the videos produced by our 180p and 360p T2V models on VBench~\citep{huang2024vbench}, respectively. Our model follows the standard guidelines when generating the results without tricks like prompt rewriting. All image models we compared with are diffusion-based models.


\paragraph{256$\times$256 \& 1024$\times$1024 T2I.} 
As shown in Table~\ref{tab:eval_256p} and Table~\ref{tab:eval_1k}, we compare our generation results against a wide range of recent approaches. Our \ours{} model demonstrates strong performance, achieving results highly comparable to other state-of-the-art methods in terms of FID~\citep{heusel2017fid} and CLIP-Score~\citep{radford2021clip} when generating high-resolution images. Notably, our model, built on the DiT framework, surpasses DiT on both the MSCOCO and MJHQ benchmarks.
However, our model underperforms on GenEval, an object-centric evaluation metric. Upon analyzing the evaluation dimensions and text-image pairs, we observe that compared to the DiT baseline, the SSM-based solution exhibits a higher failure rate in generating structure-coherent objects and more struggles with creating co-occurring objects that align with the text prompt (score 0.67 \textit{vs.} 0.75). We hypothesize that this limitation stems from the inherent constraints of SSM-dense models, particularly their insufficient capacity to model long-range global dependencies effectively.


\paragraph{180p \& 360p T2V.} We compare our generated videos with ModelScope~\citep{wang2023modelscope}, LaVie~\citep{wang2023lavie}, Show-1~\citep{zhang2024show1}, OpenSora-v1.2~\citep{opensora2024v1p2}, VideoCrafter-2.0~\citep{chen2023videocrafter1}, T2V-Turbo (VC2)~\citep{li2024t2vturbo}, CogVideoX-5B~\citep{yang2024cogvideox}, Pika~\citep{pika2023}, Kling~\citep{kling2024}, Gen3~\citep{gen3_2024}, ARLON~\citep{li2024arlon}, and Emu3~\citep{wang2024emu3} on the VBench benchmark~\citep{huang2024vbench}. Emu3 is the only autoregressive model on the list, while all other approaches are diffusion-based. Our \ours{} model demonstrates highly comparable performance to existing methods across all evaluation dimensions. In particular, the 360p model excels in \textit{Aesthetic Quality}, as well as \textit{Appearance and Temporal Style}. Additionally, it generates accurate and realistic \textit{Human Actions} while producing videos with a significantly higher \textit{Dynamic Degree} and a good \textit{Overall Consistency}.


\subsubsection{Efficiency}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\columnwidth]{figs/speed_curve.pdf}
  \vspace{-7pt}
  \caption{Denoiser inference speed comparison between HTH and Transformer (our DiT baseline).}
  \label{fig:speed_curve}
  \vspace{-10pt}
\end{figure}

In Figure~\ref{fig:speed_curve}, we compare the wall-time inference speed between a Transformer-based denoiser and our proposed HTH denoiser for processing varying numbers of tokens. Both models have 5B parameters and execute 50 (denoising) steps, with all self-attention layers employing the Flash-Attention 2 implementation~\citep{dao2023flashattention2}.
At a token length of 128$^{2}$, corresponding to the largest 2K images we tested with our HTH model, the speed difference is minor, with HTH being only 10 seconds faster. However, as the resolution increases, the performance gap widens exponentially. This observation is consistent with the quadratic computational complexity associated with global self-attention. A token length of 384$^{2}$ roughly corresponds to an 8K image or a 6-second 1K video at 24 FPS under our $\times$8 spatial and $\frac{16}{5}\times$ temporal compression rates and a patch size of 2; this suggests that the efficiency benefits of the SSM-based model become more pronounced for very long visual sequences.
The current implementation of bidirectional SSMs, like Hydra, presents significant potential for hardware (CUDA) optimization, including kernel operation integration. We intend to pursue these optimizations in future work.
