\section{Limitations and Future Directions}
\label{appsec:limitations}
Due to the extensive scope of this work and constraints in computational resources, we identify and share several limitations of our system that we believe are valuable for future research:
\begin{itemize}
    \item \textbf{SSM-friendly pipeline}. Our model is built upon the existing DiT~\citep{peebles2023dit} pipeline, which is specifically tailored and optimized for Transformer-based models and global attention operations. However, certain critical components, such as positional encoding, may be suboptimal for informing SSMs about spatial continuity. Improving these aspects is essential to better align the pipeline with SSM-based architectures.
    \item \textbf{Conditioning mechanisms}. The sequential nature of SSMs imposes constraints on how conditions are introduced to latent representations. In this work, we employ simple cross-attention to incorporate textual conditions, but the results generally exhibit inferior semantic understanding (\textit{e.g.}, worse entity composition and structural coherence) compared to Transformer-based models. Developing more effective conditioning mechanisms for SSMs is a promising direction for future exploration.  
    \item \textbf{SSM optimization}. The formulation and hardware optimization of SSMs remain at an early stage. Further research is needed to design SSMs better suited for processing N-dimensional visual data while maintaining high computational efficiency. 
\end{itemize}


\section{Additional Results}

\subsection{Comparison of Token Mixers}
\label{appsec:token_mixers}
As shown in our small-scale experiment (Figure~\ref{fig:scan_compare}), we compared the generation quality across unidirectional SSM, bidirectional SSM, the state-sharing VSSD~\citep{shi2024vssd}, self-attention, and Hydra. We constructed 1B-parameter models with the different token mixers above while keeping all other components identical and trained each model for 300K iterations. Hydra demonstrated clearly superior visual quality than the other SSM-based models, particularly in structural coherence and correctness, while achieving better semantic alignment with the text prompt. Its performance is also highly comparable to the model employing self-attention.

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.90\textwidth]{figs/scan_comparison.pdf}
  \vspace{-5pt}
  \caption{Example 256$\times$256-resolution T2I generation results of different token mixers. SSMs and Hydra models are based on Mamba-2~\citep{dao2024mamba2}. The Bidirectional SSM and Hydra apply interleaved horizontal and vertical raster scans across model blocks. None of the models in this small-scale experiment is hybrid (\textit{i.e.}, all blocks use the same token mixer).}
  \label{fig:scan_compare}
\end{figure*}


\subsection{Higher-Res Zero-shot Generation}
\label{appsec:highres_zeroshot}
We further compare the zero-shot performance of our SSM-dominant architecture with pure Transformer models, as shown in Figure~\ref{fig:zeroshot_compare}. All models are of the same scale and are trained exclusively on 256p images, but are tasked with generating 512p images. The Transformer-based model with absolute positional embedding (APE)~\citep{vaswani2017attention} struggles to generalize to higher resolutions, resulting in severe checkerboard artifacts. While models using relative positional encoding, such as RoPE~\citep{su2024roformer}, achieve better results, noticeable inconsistency and noise persist. In contrast, our proposed \ours{} model naturally generalizes to synthesize $\times$4 times larger images, benefiting from the strong locality properties inherent to the state space model.

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.75\textwidth]{figs/zero_shot_comparison.pdf}
  \vspace{-5pt}
  \caption{Zero-shot comparison at higher resolution. We compare our \ours{} model with the same-scale Transformer-based models (SA: Self-Attention) with Absolute Positional Embedding (APE) or Rotary Positional Embedding (RoPE). All models are trained on 256p image data and evaluated on 416$\times$608 images in a zero-shot manner.}
  \label{fig:zeroshot_compare}
\end{figure*}


\subsection{Visualization}
\label{appsec:visualization}
The following pages provide additional visualization of generated images in various aspect ratios up to 2560$\times$2560 resolution, and some extracted frames from generated 360p 128-frame videos. Please also refer to the Supplementary Materials for more playable videos.

\begin{figure*}[h!]
  \centering
  \includegraphics[width=0.95\textwidth]{figs/visualize_3.pdf}
  \vspace{-8pt}
  \caption{Text-to-360p 128 frames video generation results produced by our Hydra-Transformer Hybrid model. Please zoom in for a clearer visualization.}
  \label{fig:visualize_3}
  % \vspace{-15pt}
\end{figure*}


\begin{figure*}[p]
  \centering
  \includegraphics[width=0.88\textwidth]{figs/visualize_96x168_168x168.pdf}
  \vspace{-5pt}
  \caption{Text-to-1K image generation results produced by our Hydra-Transformer Hybrid model. The images are in resolutions 768$\times$1344 and 1344$\times$1344. Please zoom in for clearer visualization.}
  \label{fig:visualize_96x168_168x168}
\end{figure*}


\begin{figure*}[p]
  \centering
  \includegraphics[width=0.86\textwidth]{figs/visualize_168x96.pdf}
  \vspace{-5pt}
  \caption{Text-to-1K image generation results produced by our Hydra-Transformer Hybrid model. The images are in resolutions 1344$\times$768. Please zoom in for clearer visualization.}
  \label{fig:visualize_168x96}
\end{figure*}


\begin{figure*}[p]
  \centering
  \includegraphics[width=0.89\textwidth]{figs/visualize_180x320.pdf}
  \vspace{-5pt}
  \caption{Text-to-2K image generation results produced by our Hydra-Transformer Hybrid model. The images are in resolutions 1440$\times$2560. Please zoom in for clearer visualization.}
  \label{fig:visualize_180x320}
\end{figure*}


\begin{figure*}[tp!]
  \centering
  \includegraphics[width=0.88\textwidth]{figs/visualize_320x180.pdf}
  \vspace{-5pt}
  \caption{Text-to-2K image generation results produced by our Hydra-Transformer Hybrid model. The images are in resolutions 2560$\times$1440. Please zoom in for clearer visualization.}
  \label{fig:visualize_320x180}
\end{figure*}


\begin{figure*}[tp!]
  \centering
  \includegraphics[width=0.88\textwidth]{figs/visualize_240x240.pdf}
  \vspace{-5pt}
  \caption{Text-to-2K+ image generation results produced by our Hydra-Transformer Hybrid model. The images are in resolutions 2560$\times$2560. Please zoom in for clearer visualization.}
  \label{fig:visualize_240x240}
\end{figure*}

\section{Prompts of Figure~\ref{fig:visualize_1}}

We provide the prompts for generating the images presented in Figure~\ref{fig:visualize_1} here. In a raster scan order: 

\begin{enumerate}
    \item A dog that has been meditating all the time.
    \item In a surreal office setting during a sunlit day, a man in a bizarre jelly mask, reminiscent of an oyster shell, occupies a full shot. He is seated at an office desk, his melancholic expression in contrast to the elaborate facades of yttrium yellow and zircon blue surrounding him.
    \item Full-body photograph of a beautiful female cyborg, suspended in a worn-out, cassette-futurism science lab. The scene exudes a gritty retro sci-fi aesthetic from the 70s and 80s, with outdated technology and weathered machinery. Dust particles float in the light, enhancing the vintage, worn-down atmosphere. The cyborgâ€™s synthetic skin is torn from her down to her, revealing a complex but aged, metallic structure beneath. Her damaged breasts and partially exposed torso show signs of wear, with cracks and rust adding to the dystopian feel. Her body hangs lifeless from flickering, illuminated cords connected to her back.
    \item A bouquet of roses made of pastel ice crystals.
    \item A cat drinking a beer.
    \item Paper artwork, layered paper, colorful Chinese dragon surrounded by clouds.
    \item Beautiful neon lights forming the words ``Hydra'', glowing vibrantly.
    \item Filmic photo of a group of three women on a street downtown, they are holding their hands up the camera.
    \item A pink bunny girl in the cyber world, with beautiful long hair, wearing a pink skirt, pink bunny ears, pink headphones. Neon lights, and a sense of technological future.
    \item Half human, half robot, repaired human, human flesh warrior, mech display, man in mech, cyberpunk.
    \item Elephant amigurumi walking in savanna, a professional, blurry background.
    \item Polychrome particles and powders surrounding a whimsical child figure silhouette emerging out of a portal.
    \item A very cute little Shamrock bird in a mossy forest.
\end{enumerate}
