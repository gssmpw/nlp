\section{Background}
\label{sec:preliminaries}

\subsection{Selective State Space Models}
The recently proposed selective state space models (S4, \textit{e.g.}, Mamba~\citep{gu2023mamba,dao2024mamba2}) formulate a sequential transformation that maps a sequence $x_{t} \in \mathbb{R}^{d} \mapsto y_{t} \in \mathbb{R}^{d}$ through an implicit latent state $h_{t} \in \mathbb{R}^{N}$. Specifically, S4 models are defined with four input-dependent parameters, $\Delta_{t}$, $\mathbf{A}_{t}$, $\mathbf{B}_{t}$, and $\mathbf{C}_{t}$, which their discrete form can be generally expressed as 
\begin{equation}
\begin{aligned}
    h_{t} &= \overline{\mathbf{A}}_{t}h_{t-1} + \overline{\mathbf{B}}_{t}x_{t} \quad & 
    y_{t} &= \mathbf{C}^{\top}_{t}h_{t}
\end{aligned}
\end{equation}
where $\Delta_{t}$ controls the preservation of the current state, and $\overline{\mathbf{A}}_{t}$ and $\overline{\mathbf{B}}_{t}$ are produced by a discretization rule such as via the Zero-Order Hold as
\begin{equation}
\begin{aligned}
    \overline{\mathbf{A}}_{t} &= \text{exp}(\Delta_{t}\mathbf{A}_{t}) \\
    \overline{\mathbf{B}}_{t} &= (\Delta_{t}\mathbf{A}_{t})^{-1}(\text{exp}(\Delta_{t}\mathbf{A}_{t})-\mathbf{I})\cdot\Delta_{t}\mathbf{B}_{t}
\end{aligned}
\end{equation}
Referring to the state-space duality~\citep{dao2024mamba2,hwang2024hydra}, SSMs present a matrix transformation form for a $({\overline{\mathbf{A}},\overline{\mathbf{B}},\mathbf{C}})$-parameterized semiseparable matrix $\mathbf{M}_{\theta} \in \mathbb{R}^{(T,T)}$, where $T$ indicates the length of input sequence $\mathbf{X} \in \mathbb{R}^{(T,d)}$. Concisely, we can view the sequential transition
\begin{equation}
\begin{aligned}
    y_{t} &= \Sigma^{t}_{s=0}\mathbf{C}^{\top}_{t}\overline{\mathbf{A}}^{\times}_{t:s}\overline{\mathbf{B}}_{s}x_{s} \\
    \overline{\mathbf{A}}^{\times}_{i:j}&=\begin{cases}
    \prod^{i}_{k=j+1}\overline{\mathbf{A}}_{k},     & i>j \\
    1, & i=j
\end{cases}
\end{aligned}
\end{equation}
as
\begin{equation}
    \mathbf{Y}=\mathbf{M}_{\theta}\mathbf{X}
\end{equation}
where $m_{ij}=\mathbf{c}^{\top}_{i}\overline{\mathbf{A}}_{i}\cdot\cdot\cdot\overline{\mathbf{A}}_{j+1}\overline{\mathbf{b}}_{j}$ are the $(i,j)$-entries that fill the lower-triangular part of $\mathbf{M}_{\theta}$.


\subsection{Hydra: Bidirectional SSM}
A core limitation of the above SSM formulation lies in its inherent causality, which constrains its ability to model N-dimensional visual data effectively. To address this issue while allowing the model to possess strong expressiveness and leverage sub-quadratic matrix multiplication algorithms, the Hydra bidirectional SSM is introduced~\citep{hwang2024hydra}. Hydra employs a quasiseparable matrix as the token mixer and is implemented using the Mamba-2 framework~\citep{dao2024mamba2}.

Specifically, the quasiseparable matrix (\textit{QS}) in Hydra is formed by combining two semiseparable matrices (\textit{SS}) corresponding to forward and reverse sequence modeling:
\begin{equation}
    QS(\mathbf{X})=\hat{s}(SS(\mathbf{X}))+\hat{f}(\hat{s}(SS(\hat{f}(\mathbf{X}))))+\mathbf{D}\mathbf{X}
\end{equation}
where $\textbf{D}$ denotes the learnable diagonal parameters of the quasiseparable matrix. $\hat{f}$ and $\hat{s}$ are two non-parameterized operations; $\hat{f}$ reverses the input sequence, $\hat{s}$ shifts the sequence right by one position and pads the beginning with zero. As claimed and empirically justified in the Hydra paper, the above formulation of a quasiseparable matrix is strictly more expressive than existing addition-based bidirectional SSMs. We refer readers to the original paper for proof of the derivation~\citep{hwang2024hydra}.



