\section{Introduction}
\label{sec:intro}

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.86\textwidth]{figs/visualize_1.pdf}
  \vspace{-5pt}
  \caption{Text-to-1K/2K+ image generation results of our Hydra-Transformer Hybrid model. The resolution of each sample is displayed in the bottom-right corner. Text prompts and additional results are provided in the Appendix. Please zoom in for a clearer visualization.}
  \label{fig:visualize_1}
\end{figure*}

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.95\textwidth]{figs/visualize_2.pdf}
  \vspace{-8pt}
  \caption{Text-to-360p 128 frames video generation results produced by our Hydra-Transformer Hybrid model. More results are provided in the Appendix. Please zoom in for a clearer visualization.}
  \label{fig:visualize_2}
  \vspace{-12pt}
\end{figure*}

In recent years, we have witnessed remarkable advancements in industrial-scale visual generation, spanning images~\citep{esser2021taming,dhariwal2021diffusion,rombach2022ldm,saharia2022imagen,peebles2023dit,chang2023muse,ruiz2023dreambooth,podell2023sdxl}, videos~\citep{yan2021videogpt,ho2022imagenvideo,blattmann2023align,girdhar2023emuvid,kondratyuk2023videopoet,bar2024lumiere}, 3D assets~\citep{hong2023lrm,li2023instant,xu2023dmv3d,wang2024dust3r,watson2024controlling}, embodied interactions~\citep{wu2023gr1,yang2023UniSim,du2024learning,cheang2024gr2,yang2024cogvideox}, and more. Central to many of these approaches is Transformer architecture~\citep{vaswani2017attention}, which serves as the core model backbone. Transformers, powered by the self-attention token mixing mechanism, effectively capture the relationships among visual tokens to produce realistic visual content and have proven highly scalable~\citep{dosovitskiy2020vit,dehghani2023scaling}. However, the quadratic computational complexity of Transformer models poses significant efficiency challenges, leading to dramatic slowdowns when processing long token sequences and making them impractical to common users~\citep{kitaev2020reformer,wang2020linformer,beltagy2020longformer}.

Meanwhile, there has been a surge in linear complexity models (LCM)~\citep{katharopoulos2020transformers,choromanski2020performers,shen2021efficient,qin2022cosformer,yang2023gated}; large models built based on RetNet~\citep{sun2023retentive}, RWKV~\citep{peng2023rwkv}, GLA~\citep{yang2023gated}, and Mamba~\citep{gu2023mamba,dao2024mamba2} have demonstrated performance comparable to Transformers across a variety of natural language processing tasks while achieving superior inference time speed. This success inspires recent research such as building linear complexity visual generative models~\citep{yan2023difussm,hu2024zigma,fei2024dimba,yang2023gated,zhu2024dig} and exploring Transformer-to-LCM distillation~\citep{wang2024mambainllama,bick2024mohawk,liu2024linfusion}. Despite their efficiency advantages, these methods often fail to produce visual outputs with the same quality as Transformer models. An essential reason behind this is that most of the LCMs are formulated as a stateful model that enforces causality on the input (tokens), which does not align with the non-causal N-dimensional nature of visual data. As a result, many recent generative visual LCMs implement multi-directional scans and preserve self-attention~\citep{gao2024matten,zhang2024motion,fei2024dimba,hu2024zigma,chen2024maskmamba,yi2024mvgamba}, which enlarges the receptive field of each token across layers.
% \footnote{To be precise, many of these are sub-quadratic computations. \label{footnote_subquadratic}}.
However, those models and experiments mostly only deal with short visual sequences (\textit{e.g.}, low-res images), which cannot reflect the memory limitation or justify the expressiveness of LCMs~\citep{jelassi2024repeat,merrill2024illusion}. On the other hand, several works attempt to build non-casual LCMs by squeezing all visual tokens into the fix-sized state at once~\citep{liu2024linfusion,xie2024sana} or establishing a state representation that shares the same information with all the tokens~\citep{shi2024vssd}; despite encouraging results on image generation, our preliminary experiments show that these methods struggle with complex text prompts or video generation, where the latter requires learning much more complicated spatiotemporal patterns.

In light of the above, this paper takes an empirical approach to explore the boundary of State Space Models (SSM) in long-sequence visual generation with diffusion. Specifically, we apply the best-performing bi-directional SSM, Hydra, and follow the previous research to build a Hydra-and-Transformer Hybrid model (\textbf{\ours{}}). We consider the simplest horizontal and vertical raster scan patterns while focusing on scaling the model to 5B parameters for generating up to 2K images (1440$\times$2560) and 360p 8-second videos (16 FPS), respectively. Compared to 2D images, video data has an extra dimension which introduces large scanning complexity to the SSM. Here, to adapt the image-trained \ours{} base model for video generation, we propose a simple but effective method that directly changes the scanning order of some layers to temporal-first, forcing these layers to learn the temporal evolution of tokens at the same 2D position. 
% \yicong{Additionally, we optimize the original Hydra implementation by fusing CUDA kernels and improving memory allocation, allowing such bidirectional SSM to run on par forward and back-propagation speed as a uni-directional Mamba-2, and ? times faster than the same-sized Transformer model (5B) with Flash Attention-2 on producing ?k tokens.} 
Our final results reveal that the \ours{} model effectively generates faithful outputs that align with complex text prompts while producing temporally consistent and highly dynamic videos, highlighting the potential of sequential linear complexity models in visual generation.
