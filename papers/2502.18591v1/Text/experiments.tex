\section{Experiments}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\subsection{Setting}
\label{subsec:setting}

All models are trained with a base solver operating on a $64\times64$ grid with reference data coming from a $2048\times2048$ discretization. To facilitate comparability  of models, we strictly follow the data generation procedures presented in \citeauthor{kochkov2021machine}. 

The training data is the result of simulations of a 2D Kolmogorov flow in a box of side-length $2\pi$ on a structured grid with periodic boundary conditions. Furthermore, Reynolds number $\RN=1000$, density $\rho=1$, dynamic viscosity $\mu=0.001$, explicit time stepping with $\Delta t = 7.0125 \cdot 10^{-3}$ and forcing $f = \sin (4y) \vec{e}_1 - 0.1 \vec{U}$ (\textit{forced turbulence}) are used, where $\vec{e}_1$ is the basis vector in $x$-direction and $\vec{U}$ is the velocity vector. We generate 50 trajectories resulting from different random initial conditions. Of those, two are kept for the validation set and 16 for the test set. We also test our model on the three generalization cases from \citeauthor{kochkov2021machine} (\citeyear{kochkov2021machine}): a \textit{larger domain} with increased side-length of $4\pi$, a \textit{more turbulent} Kolmogorov flow with $\RN=4,000$, and a \textit{decaying turbulence} case where the forcing is removed.
 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\subsection{Training}
\label{subsec:training}

The networks are trained by consecutively minimizing the loss functions 
{\footnotesize
\begin{align}
 L_T = &\sum_{j=0}^{T/N} \mathsf{MSE}\left( \textbf{enc}_{\theta}(\vec{U}_{1 + jN}^{\text{exact}}), \vec{H}_{1 + jN} \right) 
    \notag\\
    &+ 
    \sum_{i=1}^T \mathsf{MSE}\left(\vec{U}_i^{\text{exact}}, \vec{U}_i\right)
    \label{eq:lossfn} \quad \text{for} \;\, T\in \{16,32,64\}
\end{align}}

\noindent
The second term is the standard mean square error between the down-sampled high resolution velocity field $\vec{U}_i^{\text{exact}}$ and the velocity field $\vec{U}_i$ obtained from the ML-augmented solver (see \citeauthor{kochkov2021machine} for more details on the down-sampling scheme). In practice, we found that starting the training on trajectories composed of 16 time steps, and then increasing its length to 32 and finally to 64 time steps worked best for the TMN approach. These chunks are obtained by splitting up longer trajectories of $4,800$ sequential time steps. 

The first term computes the difference between the hidden state vector after $N$ time steps and the output of the encoder taking as input the velocity at this same $N$\textsuperscript{th} time step (of the chunk of trajectory). $\textbf{enc}_{\theta}(\vec{U}_{1 + jN}^{\text{exact}})$ denotes the output of encoder when taking as input the velocity (of the reference model\footnote{We also explored using the velocity produced by the corrected model but did not observe any difference in model performance.}) at time $t_{1 + jN}$, and $\vec{H}_{1 + jN}$ is the hidden state vector at the same time step, i.e., after $jN$ hidden state updates of the initial encoded value $\textbf{enc}_{\theta}(\vec{U}_{1}^{\text{exact}})$. This term ensures that the hidden state that is encoded at the end of a chunk will be similar to the one that is encoded at the beginning of the next chunk (in the same trajectory) and thus enforce stability during long roll-outs.

The initial learning rate of 0.001 is exponentially decreased with a decay rate of 0.5 and 50,000 transition steps. A batch size of 8 chunks is used. At each chunk length, 400 training epochs are performed producing converging loss curves. 

The error is computed on trajectories, instead of single time steps, which means the correction is applied auto-regressively, and the gradients used in the optimization procedure take into account the effect of the correction not only on the current time step but also on all the subsequent ones in the trajectories, see \citeauthor{melchers2023comparison} for an in-depth explanation. 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\subsection{Evaluation Metrics}
\label{subsec:metric}

To compare the performances of the models, we use the same metrics as in \citeauthor{kochkov2021machine}. As a metric for point-wise accuracy, we evaluate the Pearson correlation of the vorticity field $\nabla\times\vec{U}$ produced by the models with the one obtained from reference high resolution solvers. To facilitate comparison across models, we look in particular at the time at which the correlation falls below $0.95$. 

For statistical accuracy, we compute the energy spectrum $E(k) = 0.5 |\vec{U}(k)|^2$, where $k$ is the wavenumber, and evaluate the spectral error defined as the average absolute error of $E(k)k^5$ between the reference solution and the respective model output.

For all metrics, the numbers reported are taken from an average over 16 different trajectories. We also tested the robustness of each model by varying the random initialization of the parameters of the neural networks with 4 different random seeds. As baselines we report the performances of uncorrected base solvers on intermediates grid sizes.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\subsection{Effect of the Input Stencil Size}
\label{sec:stencil_study}

As mentioned in Section \ref{sec:LC_CNN_models}, the original LC model in \citeauthor{kochkov2021machine} (\citeyear{kochkov2021machine}) uses information coming from a $15\times15$ stencil around a given cell to compute the correction for that same cell. To identify the influence of the stencil size on model performance, we track the accuracy of a series of models with varying stencil size. The models consist of a variable number of convolution layers with $3\times3$ stencils, followed by two layers with $1\times1$ kernel that act as final, local Multilayer Perceptron (MLP). The stencil size is then equal to $2N + 1$ where $N$ is the number of convolution layers with $3\times3$ kernels (see Appendix \ref{sec:nets_architecture} for details).

The time until the correlation falls below 0.95 for the various stencil sizes are reported in Figure \ref{fig:stencil_study} for the \textit{forced turbulence} case, with the performance of the model from \citeauthor{kochkov2021machine} added as reference. As expected the model performance improves with larger stencil, with a plateau achieved for stencils above $13\times13$.

\begin{figure}[h!]
         \centering
         \includegraphics[width=0.9\columnwidth]{Figures/plot_stencil_study.pdf}
     \caption{Simulation time until correlation drops below 0.95 for models with different input stencil sizes and 4 random initializations, vertical lines indicate the multiples of cells required by base solver to achieve the respective timings.}
     \label{fig:stencil_study}
\end{figure}

In particular, the model with a $3\times3$ stencil (i.e. the one using only information coming from the direct neighbours) shows considerably worse performance compared to the best performing models. 

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\subsection{Benchmarking the TMN approach}
\label{sec:latent_study}

\paragraph{Accuracy:} 
In Figure \ref{fig:results_vort_corr}  we report the results of the TMN model for different sizes of the hidden state vector and for the various cases: \textit{forced turbulence} (same setting as the training data), \textit{decaying turbulence}, \textit{larger domain} and \textit{more turbulent} (see Section \ref{subsec:setting} for more details). Figure \ref{fig:corr_time_force_turb} shows the correlations over time for the \textit{forced turbulence} case.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\columnwidth]{Figures/plot_timings_all_cases.pdf}
    \caption{Simulation time until correlation drops below 0.95 for models with number of hidden states (hs) and 4 random initializations. For more details on the simulation settings, c.f., Section \ref{subsec:setting}. }
    \label{fig:results_vort_corr}  
\end{figure}

\begin{figure}[h!]
    \centering
         \includegraphics[width=0.9\columnwidth]{Figures/vort_corr_forced_turb.pdf}
    \caption{Simulation time until correlation drops below 0.95 for for direct simulations (DS) with different discretizations as well as models with varying number of hidden states (hs) for \textit{forced turbulence}.}
    \label{fig:corr_time_force_turb}
\end{figure}

While on average the model accuracy does improve with the number of hidden states, the trend is not as pronounced as in the case of the stencil size (see Section \ref{sec:stencil_study}). Overall, a hidden state vector of dimension 8 is enough to obtain comparable results to the full CNN architecture, but even models with as few as 4 hidden states remain competitive in most cases.

Figure \ref{fig:energy_forc_turb} shows the energy spectrum (scaled by $k^5$ where $k$ is the wave number) for the various baselines, for the TMN models with 2 and 10 hidden states and for the model from \citeauthor{kochkov2021machine} (\citeyear{kochkov2021machine}). The learned models agree well with the high resolution baseline solution except at $k=1$. Further, Figure \ref{fig:energy_forc_turb} depicts the spectral error (see Section \ref{subsec:metric}) for the 2 to 10 hidden states TMN models and 4 different random initialization each. When satisfied with statistical accuracy, 2 hidden states is enough to reach a comparable accuracy as the original LC model in \citeauthor{kochkov2021machine} (\citeyear{kochkov2021machine}). Using more than 4 hidden states does not seem to bring further improvement for this metric.

\begin{figure}[h!]
     \centering
     \begin{subfigure}[b]{0.49\columnwidth}
         \centering
         \includegraphics[width=0.99\columnwidth]{Figures/energy_forc_turb.pdf}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\columnwidth}
         \centering
         \includegraphics[width=0.99\columnwidth]{Figures/plot_energy_forced_turb.pdf}
     \end{subfigure}
     \caption{Left, the energy spectrum scaled by $k^5$ over wavelength is shown for direct simulations (DS) and learned models for \textit{forced turbulence}. Right, the spectral error is depicted for learned models (dots) for different random initializations, vertical lines indicate multiple of required cells by the base solver,}
     \label{fig:energy_forc_turb}
\end{figure}

\paragraph{Speed:} 
In Table \ref{tab:models_speed} we report the runtime per time unit (i.e., how long does it take to compute one second worth of simulated flow) as a function of the number of hidden states, with the model from \citeauthor{kochkov2021machine} added for reference. All models were run on a single Nvidia A10G GPU.

%\todo{We need to better present acceleration potential, i.e., specifically compare to today's "standard" solver.}

\begin{table} [h!] 
\small
    \centering
        \begin{tabular}{@{\,}ccccccc|c@{\,}} 
        \toprule
        \# Hidden states &  2 & 4 & 6 & 8 & 10 & 12 & \citeauthor{kochkov2021machine}\\
        \hline 
        Speedup   &  
        99 & 95 & 93 & 90 & 88 & 85 & 86 \\
        \bottomrule
    \end{tabular}
    \caption{Speedup of simulation for different sizes of the hidden state vector. The simulation times of the corresponding hybrid methods on a $64\times64$ grid are compared with the direct simulation on a $512\times512$ grid, which has a comparable accuracy (i.e., accuracy correlation as shown in Figure \ref{fig:corr_time_force_turb}).}
    \label{tab:models_speed}
\end{table}

%\begin{table} [h!] 
%\small
%    \centering
%        \begin{tabular}{@{\,}ccccccc|c@{\,}} 
%        \toprule
%        \# hidden states &  2 & 4 & 6 & 8 & 10 & 12 & \citeauthor{kochkov2021machine}\\
%        \hline 
%        timings   &  2.31 & 2.40 & 2.46 & 2.54 & 2.60&  2.69 & 2.65\\  
%        \bottomrule
%    \end{tabular}
%    \caption{Runtime per time unit ($\times10^{-2}$ real time/simulation time) for different sizes %of the hidden state vector \label{tab:models_speed}}
%\end{table}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.6\columnwidth]{Figures/timings.pdf}
%     \caption{Runtime for different sizes of the latent vector. Black line represents the timing of the model from \cite{kochkov2021machine}.}
%     \label{fig:models_speed}
% \end{figure}

These timings demonstrate that the TMN architecture is computationally faster for all models up to 10 hidden states compared with the CNN model from \citeauthor{kochkov2021machine}. However, we would like to point out that we do not perform any optimization regarding the latency of the TMN models. Thus potential further speed up can be expected with more carefully chosen implementations and network architectures. Last but not least, we expect additional speedup going from 2D to 3D due to cubic scaling of compute effort in 3D versus quadratic scaling in 2D.

We also note that, as reported in \cite{mcgreivy2024weak}, in the original study of \citeauthor{kochkov2021machine} the LC and LI models were only evaluated against an (uncorrected) FV solvers, and not the stronger baselines of Pseudo-Spectral \cite{shen2011spectral} and DG methods \cite{cockburn2012discontinuous}, which would perform significantly better in the benchmarks selected here. However, in our case we are concerned with industrial solvers which primarily use FV methods. Thus it is a fair baseline.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\subsection{Transport of Hidden State Vector}
\label{sec:latent_discussion}

Some interesting insight can be gained by inspecting the values and evolution of the hidden states during a trajectory. Figure \ref{fig:vort_and_lat} show the evolution of the first two components of the long term state $\vec{H}$ for one of the trajectories of the test set in the forced turbulence case. Also, the vorticity of the velocity field is pictured. 

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=15cm]{Figures/placeholder_vorticityscalars.png}
%     \caption{Vorticity (first row) and first 3 latent components (row 2-4).}
%     \label{fig:vort_and_lat}
% \end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\columnwidth]{Figures/latent_component3.pdf}
    \caption{Vorticity (row 1) and first 2 hidden states (rows 2-3) show remarkable resemblance.}
    \label{fig:vort_and_lat}
\end{figure}

As can be seen from the plots, the hidden state components are highly correlated with the vorticity. This suggest that the update
{\footnotesize
\begin{align}
    \vec{H}_{i} = \textbf{up}_{\theta} (\vec{U}_i, \vec{H}_{i-1} )
    \label{eq:latent_update}
\end{align}}

\noindent 
effectively learns a transport equation (the vorticity itself is transported with the flow and any transported quantity would follow a similar pattern). We stress that during training the model never sees the vorticity directly, but only the velocity components, see Equation (\ref{eq:lossfn}).

As an alternative to the direct update of Equation (\ref{eq:latent_update}), and taking inspiration from classical turbulence model, we tried to explicitly transport the hidden states and only learn a source term with different variation of the update:
{\footnotesize
\begin{align}
    \vec{H}_{i} = \textbf{trsprt}\left(\vec{U}_i, \vec{H}_{i-1}\right) + \textbf{s}_{\theta} (\vec{U}_i, \vec{H}_{i-1} )
    \label{eq:latent_transport}
\end{align}
}

\noindent
with $\textbf{trsprt}$ the transport of quantity $\vec{H}_{i-1}$ by the velocity field $\vec{U}_i$ given by the base solver and $\textbf{s}_{\theta}$ a learnable source component. However this approach with a physics prior by explicitly modelling transport negatively impacts performance (see 
Appendix \ref{sec:stuff_tried} for more details).
