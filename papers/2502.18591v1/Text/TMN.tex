\section{Transported Memory Networks}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\subsection{Motivation}

While using CNNs is straightforward and efficient for CFD solvers based on Cartesian grids, their application in industrial high-end CFD solvers remains impractical. Most solvers adopt a matrix-free approach, which efficiently accesses information only for nearest neighbors and a limited number of previous time steps. While replacing CNN with Graph Neural Networks may seem like a natural extension to handle unstructured meshes \cite{shankar2023differentiableturbulenceii}, this does not address the issue of having to access information beyond the immediate neighbours. 

Thus the question is: \textit{Is it possible, through a change in the ML model, to compute the learned correction by only using information coming from direct neighbours and still achieve the desirable performance?}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\subsection{Transported Memory Network Architecture}
\label{sec:TMN_arch}

Within this paper, we propose a new architecture, that is compatible with small computational stencils while still achieving the desired level of performance, both in term of accuracy and model efficiency. Inspired by various recurrent neural network architectures \cite{hochreiter1997long, gers2000recurrent, cho2014learning} our approach takes into account, in addition to the short term physical states $\vec{U}(t)$, a long term state (memory) encoded by a hidden state vector $\vec{H}(t)$, both defined cell / discretization point-wise (see Figure \ref{fig:overview} and \ref{fig:cell}). That is, the hidden states are latent variables that can be inferred only indirectly. Given the Lagrangian nature, the hidden state vector is effectively transported with the flow, as we show in Section \ref{sec:latent_discussion}. We thus refer to the architecture as Transported Memory Networks (TMN). It can be considered as a fusion of simulation algorithms formulated in a Eulerian coordinate system and recurrent neural network architectures.

Concretely, we use the FV approach on a staggered grid \cite{griebel1998numerical} based on the JAX-CFD code of \citeauthor{kochkov2021machine} (\citeyear{kochkov2021machine}), where in addition to the velocity components carried by each cell we add a hidden state vector defined at cell center that encodes history information. In practice, at each time step (with constant size $\Delta t = t_i - t_{i-1}$), a new intermediate velocity field is produced by the base solver, i.e., $\vec{U}^*_i =  \textbf{solver} ( \vec{U}_{i-1} )$, which is then corrected by the velocity corrector network using as input the current intermediate velocity (of the direct neighbours only) plus the hidden state vector (of the direct neighbours only), i.e., $\vec{U}_i = \vec{U}^*_i + \textbf{corr}_{\theta}(\vec{U}^*_i, \vec{H}_{i-1} )$. The hidden state vector is then itself updated, using its own update network taking as input the previous values and the current corrected velocity, i.e, $\vec{H}_{i} = \textbf{up}_{\theta} (\vec{U}_i, \vec{H}_{i-1} )$. Additionally we have also explored adding a physics prior by introducing an explicit transport of $\vec{H}$, but this has been less effective (see Appendix \ref{sec:stuff_tried}).

For initializing the hidden state vector we use a separate encoder network which computes the initial condition for the hidden state vector from the initial condition of the velocity field, i.e, $\vec{H}_0 = \textbf{enc}_{\theta}( \vec{U}_0 )$. Here, the encoder is non-local, with 7 layers of convolution with $3 \times 3$ kernel, meaning an "input stencil" of $15 \times 15$ as in the original model of \citeauthor{kochkov2021machine} (\citeyear{kochkov2021machine}). At encoding we do not have access to any previous snapshots in time and use the spatial information instead to encode the "history" of the flow\footnote{In classical turbulence model the additional quantities (like turbulence kinetic energy) also need to be initialized but this can be done based on their definition and the initial velocity field.}. The encoder is used only once at the beginning of the simulation, leaving the locality during solver iterations intact. 

The whole algorithm is summarized in Algorithm \ref{alg:LC_hidden states} (where the subscript $\theta$ denote the learnable functions) and further details of the architecture is given in Appendix \ref{sec:nets_architecture}. Albeit we assume a constant time step an extension to adaptive time steps, i.e., more general time integration schemes, is possible, e.g., \cite{chen2018neural,kang2023learning}.

\begin{algorithm}[t]
  \caption{Learned correction with hidden states}\label{alg:LC_hidden states} 
  \begin{algorithmic}
    \STATE Set initial conditions $\vec{U}_0$
    \STATE Encode initial hidden states $\vec{H}_0 = \textbf{enc}_{\theta}( \vec{U}_0 )$
    \WHILE{$1 \leq i \leq N$}    
      \STATE Update velocity with base solver $\vec{U}^*_i =  \textbf{solver} ( \vec{U}_{i-1} )$
      \STATE Correct velocity $\vec{U}_i = \vec{U}^*_i + \textbf{corr}_{\theta}(\vec{U}^*_i, \vec{H}_{i-1} )$
      \STATE Update hidden states $\vec{H}_{i} = \textbf{up}_{\theta} (\vec{U}_i, \vec{H}_{i-1} )$
    \ENDWHILE
  \end{algorithmic}
\end{algorithm} 

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %



