\section{Related Work}
\label{se:relwork}




In this section, we start by discussing key early works that looked at the underlying theme of using iterated supervised learning to solve reinforcement learning problems in section~\ref{sec:rw:rwr}.
We then look at works associated with the more specific goal-conditioned reinforcement learning in section~\ref{sec:rw:gcrl}.
After, we tackle the most relevant works to this article---those directly relating to upside-down reinforcement learning---in section~\ref{sec:rw:udrl}.
We end this section by discussing critical work associated with the transformer architecture used by decision transformers and online decision transformers in section~\ref{sec:rw:transformers}.


\subsection{Reward-Weighted Regression}
\label{sec:rw:rwr}

The idea of using \emph{iterated supervised learning to solve reinforcement learning problems} was previously investigated by \citet{peters2007reinforcement} in the form of Reward-Weighted Regression (RWR),
which itself built on the Expectation-Maximization framework for RL by \citet{dayan1997using}.
However, the setting explored in their works is very limited; the work of \citet{dayan1997using} considered only a bandit scenario
(only one state and a finite number of actions) and
\citet{peters2007reinforcement} restricted their work to immediate reward
problems.
The extension of RWR to consider a full episodic setting (considering not only immediate rewards but rather full returns) was done by \citet{wierstra2008episodic}
and \citet{kober2011policy}.
Efficient off-policy schemes in the context of RWR were later discussed by \citet{hachiya2009efficient,hachiya2011reward}.
Finally, the use of deep neural network approximators was introduced to RWR by \citet{peng2019advantage}.

Theoretical studies of RWR are more sparse, with the monotonicity of RWR proved by~\citet{dayan1997using} and~\citet{peters2007reinforcement} in their respective settings. Both of the aforementioned monotonicity proofs build on the Expectation-Maximization paradigm.
More than a decade after the work on monotonicity, RWR's convergence to a global optimum was
proved by \citet{strupl2021reward} for compact state and action spaces (assuming an infinite number of samples and no function approximation).

RWR remains particularly relevant for this work because \eUDRL{} on $\Seg^{\diag}$
coincides with RWR (as discussed in section~\ref{se:recrewrites}).
This link motivated our approach to the continuity proofs
of \eUDRL{} generated quantities at deterministic kernels.


\subsection{Goal-conditioned Reinforcement Learning Without Relabeling}
\label{sec:rw:gcrl}

To our knowledge, the first work that could be considered goal-conditional RL was published
by 
\citet{schmidhuber1991learning,schmidhuber1990learning} in the context of learning selective attention.
In this work, there are extra goal-defining input patterns that encode various tasks so that the RL machine knows which task to execute next.
The rewards are granted at the end of each task.
The need to propagate learning signals through a non-differentiable
environment is elevated by a separate environment model network.
Essentially, it is model-based RL with sparse rewards, which is equivalent to SL except samples are collected online.
However, note that, in contrast to UDRL, this work and also all other works in this paragraph are missing segment/trajectory relabeling.
The work by \citet{schmidhuber1991learning}
also contributes to a body of literature on attention which provides grounds for today transformer architectures.
The options framework of \citet{sutton1999between} could also be considered as goal-conditioned RL (an option determines a policy).
Providing goals to a policy network provides possibility of generalization to unseen goals when representing the policy by deep network (similarly for value functions).
Building on the work of \citet{sutton2011horde}, the work of \citet{schaul2015universal} proposes convenient approximators of goal-conditioned value functions.
For an example of a more recent work on goal-conditioned RL, we can cite the paper by \cite{faccio2023goal} which introduced goal-conditioned policy generators for deep policy networks.%


\subsection{UDRL}
\label{sec:rw:udrl}

UDRL \citep{schmidhuber2019reinforcement} extends the \emph{iterated SL} idea by
learning command-conditioned policies using
trajectory segments relabeled with the achieved goals/returns.
This was motivated by the desire to obtain
generalization across goals/returns (when representing the UDRL policy by
a deep neural network), and by the desire for
more efficient data usage (there are more segments in a given trajectory than there are states).
Trajectory relabeling appeared already before in Hindsight Experience Replay (HER) by \citet{andrychowicz2017hindsight} and we relate to this work in more detail below.

\citet{schmidhuber2019reinforcement} designed the specific algorithm
that we denote here as \eUDRL{} only for deterministic environments
(though, some speculation was done by \citet{schmidhuber2019reinforcement} on variants for stochastic environments).

However, it is clear that there are many useful stochastic
environments which are close to deterministic.
With this motivation (a return-reaching variant of) \eUDRL{} was
demonstrated by \cite{srivastava2019training} to solve successfully many non-deterministic environments (e.g. MuJoCo tasks) featuring transition kernels exhibiting only small non-determinism.
This was afterwards followed by \cite{ghosh2021learning}
in context of GCSL (essentially \eUDRL{} restricted to fixed horizon and state reaching task) with further impressive benchmark performance.
Further concurrently there appeared also work of \cite{kumar2019reward}
which is again essentially \eUDRL{}.

The idea of combining \eUDRL{} with transformer architectures \cite{schmidhuber1992learningFWP,vaswani2017attention,schlag2021linear} as policy approximator was successfully demonstrated in the Decision Transformer (DT) architecture proposed by \citet{chen2021decision}.
\citet{chen2021decision}, however, only looked at the offline RL setting (equivalent to a single \eUDRL{} iteration).
Shortly after the DT architecture was introduced, a generalized version of it was proposed by \citet{furuta2021generalized}, which led to a non-trivial performance improvement.

\citet{strupl2022upsidedown} and \citet{paster2022cant}
described the problems and causes of \eUDRL{} convergence issues 
in environments with stochastic transition dynamics.
While the work of \cite{strupl2022upsidedown} rewrote the recursion in \eUDRL{} to identify the convergence issues,
the work of \cite{paster2022cant} directly proposed a solution to this issue in the context of DTs.

Several later works were proposed to fix the issues with \eUDRL{} stochastic environments, such as the work by \citet{yang2022dichotomy} in context of DT and 
the work of \citet{faccio2023goal} in context of \eUDRL{}.

More relevant to this work than much of the above is some of the theoretical work that has been done with regard to \eUDRL{} and related algorithms.
Here, the most related practical and theoretical works are those of
\citet{brandfonbrener2023does},
\citet{ghosh2021learning},
\citet{kumar2019reward},
\citet{andrychowicz2017hindsight}, and
\cite{paster2022cant}.
We provide a comprehensive treatment of each of these works below.

\paragraph{\citet{brandfonbrener2023does}}
discusses \eUDRL{} (and other related algorithms)
in context of offline RL, i.e.,
it investigates just one iteration of \eUDRL{} aiming for
sample bounds. In contrast, we are interested in developing continuity results and bounds for both any finite number of iterations and for the asymptotic case where there is an infinite sample size.
So, there is an overlap between this work and that of \citet{brandfonbrener2023does} in some trivial results (namely, the continuity of expected return (goal-reaching objective in our case) for the first iteration as shown by theorem 1 of \citet{brandfonbrener2023does}). However, note that the continuity for the
first iteration is trivial to obtain, as the initial policies do not depend on $\tkernel$. The main difficulty
comes from policies being functions of $\tkernel$. This leads to a
discontinuity of policies in $\tkernel$
at $\tkernel_0$ (a deterministic kernel) in $\theinterestingstates$ for the second and later iterations.\footnote{We refer the reader to the discussion at the end of appendix \ref{ap:interiorcont} which justifies this statement.} To overcome this difficulty, one has to utilize some weaker notion of continuity.
Here, we used the notion of relative continuity (see the induction step
in our proof of theorem~\ref{le:detcont}).
Developing asymptotic results is yet another level of complexity (see sections \ref{se:continfty}
and \ref{se:regrec}).

\paragraph{\citet{ghosh2021learning}}
introduces the following lower bound on the goal-reaching objective (see theorem 3.1 of \citet{ghosh2021learning}):
$$
J^{\pi} \geq J_{\mathrm{GCSL}}(\pi) - 4T(T-1)\alpha^2 + C,
$$
where $J^{\pi}$ denotes the goal-reaching objective
for a policy $\pi$, $J_{\mathrm{GCSL}}(\pi)$
denotes a GCSL objective for the policy $\pi$
(the same as \eUDRL{} objective \eref{eq:objective} for trailing segments), $T$ denotes a fixed horizon, $\alpha' := \max_{\bar{s}} D_{\mathrm{TV}}(\pi(\cdot|\bar{s})|\pi_{\mathrm{old}}(\cdot|\bar{s}))$
denotes total variation distance of the policy $\pi$ from
policy $\pi_{\mathrm{old}}$ which was used to collect trajectories, and $C$ is a constant on $\pi$. There are (at least) two problems when
one tries to adopt this result in our work to, for example, assess the continuity of the goal-reaching objective (or the $\delta$-dependent error bounds for the goal-reaching objective).

The first of the aforementioned problems is that to use the bound in a meaningful way, one has to minimize $\alpha'$. This could be done by assuming that a sequence of GCSL (\eUDRL{}) policies has a limit (then $\alpha' \rightarrow 0$).
This is, however, difficult to prove.
To maintain a certain level of rigor, we carefully avoid the limit assumption and instead investigate all the possible accumulation points.
From this perspective, this result is not useful as, for a finite number of iteration, $\alpha'$ could be large.
Nor is it useful for the asymptotic case as we could
not afford the limit assumption.

The second of the aforementioned problems is that the dependence on $\delta$ (the distance of the transition kernel $\tkernel$ to a deterministic kernel
$\tkernel_0$)
is not considered, as, in order to use the bound
(e.g. for assessing continuity in $\tkernel$ at $\tkernel_0$),
one has to determine the dependence on $\delta$
for all three terms on the right side.
This seems to be more challenging than determining this 
dependence for $J^{\pi}$ itself (as we do
here) and would most likely still be much less precise (see the point above).

\paragraph{\citet{kumar2019reward}} introduced an algorithm for learning return-conditioned policies that is essentially \eUDRL{} on $\Seg_{\trail}$ with fixed horizon.
However, while the authors do include some theoretical discussion on the properties of \eUDRL{}, this discussion does not touch on the distance of an MDP transition kernel to a deterministic kernel (the role played by deterministic kernels is not investigated at all). 
Therefore, with regards to the continuity of the transition kernel we are particularly interested in here, this work has little to offer.

\paragraph{\cite{andrychowicz2017hindsight}} introduced Hindsight Experience Replay (HER), the popular extension of experience replay that has be applied to many off-policy RL algorithms, such as DQN and DDPG.
HER is closely related to \eUDRL{} in some aspects.
Like \eUDRL{}, HER deals with goals, a goal map, and goal dependent policies (note that a dependence on the time step/remaining horizon could be accounted for by including it into the state representation).
HER is formulated for a fixed horizon with discounting (no discounting case is included).
The principle of the HER extension relies in populating the replay buffer not only with the trajectory that the agent actually encountered, but also with its \lq\lq{}relabeled\rq\rq{} version, where the original goals are replaced with the actually achieved final state and the corresponding rewards.
HER then uses an off-policy algorithm to learn value functions and a policy using data in the replay buffer.

The main distinguishing characteristics of  \eUDRL{} with respect to HER are that
(1) HER uses not just the relabeled trajectories but also the original trajectories, i.e., with the original intended goal;
and, (2) HER does not fit the next policy directly to some action conditional (at least when DQN or DDPG is used as the offline algorithm as in the original HER paper), e.g., DQN aims to rather learn the critic and derive the policy as an epsilon-greedy
policy using the critic.
These are the main problems when one wants to connect HER and \eUDRL{} recursion.
The second problem could be elevated by using a convenient RL algorithm (e.g. RWR) after which, because the substantial part of the data is not relabeled, we are left with some crossover between RWR and \eUDRL{} recursions.
This observation could already hint some properties of an  HER extension of RWR.
Note that some of the problems HER shares with \eUDRL{} were already reported, analyzed, and/or fixed in the literature. We refer the interested reader to the works of \citet{lanka2018archer} and \citet{schramm2023usher}.

\paragraph{\citet{paster2022cant}} describes the causes behind DT divergence in stochastic environments and proposes a fix.
To resolve the issues with convergence in stochastic domains, instead of conditioning on return, \citet{paster2022cant} propose to condition on a statistics
$I(\tau)$ (where $\tau$ stands for a trajectory) which is independent of the stochasticity of the environment. In their work, $I$ is represented by NN and learned using an adversarial scheme. Trajectories are then clustered according to $I$ values with $I$ values subsequently mapped to the average return of their corresponding cluster.
The result is then that by conditioning the policy on $I$ values, the corresponding 
average return can be reached consistently, i.e., in expectation.
This is in contrast to when conditioning on a statistics which is generally dependent on the environment's stochasticity, such as the trajectory return (or some general abstract goals like in \eUDRL{}).
For a detailed description of what it means to be \lq\lq{}independence of the environment stochasticity\rq\rq{}
please refer to the original article by \citet{paster2022cant}.

\subsection{Transformers}
\label{sec:rw:transformers}

Transformers with unnormalized linearized self attention (also known as Unnormalized Linear Transformers, or ULTRAs), appeared at least as early as the 1990s in works by \cite{schmidhuber1992learningFWP,schmidhuber1991learningFWP} under name fast-weight programmers~\citep{schlag2021linear}.
The goal of FWPs was to obtain a more storage-efficient alternative to recurrent networks by processing each input with both a slow and a fast network, where the output of the slow network provided changes to the weights of the fast network.
The attention terminology was introduced shortly after by \citet{schmidhuber1993reducing}.
The modern transformer architecture was introduced by \citet{vaswani2017attention} and scaled quadratically in input size.
More recently, variants of the \citet{vaswani2017attention} architecture have reverted to using linearized attention (e.g., see the work by \citet{katharopoulos2020transformers} and \citet{choromanski2020rethinking}), as this results in a linear scaling.