\section{The Segment Distribution and its Factorization}
\label{ap:SegmentDist}
Since we assume a deterministic reward function, rewards are completely determined by state-action-state transitions and can be omitted from segments or trajectories. Once a CE has reached an absorbing state in $\bar{\mathcal{S}}_A$, the policy becomes irrelevant as all actions lead to the same absorbing state. For this reason, it is sufficient to restrict the discussion to all policies $\pi$ that deterministically select a unique action $a_A$ in all absorbing states $\bar{\mathcal{S}}_A$, i.e.,
$\forall \bar{s} \in \bar{\mathcal{S}}_A:\:\pi(a_A|\bar{s}) = 1$. In this section, we assume that all trajectories are generated by using one fixed policy $\pi$.
The length $l(\cdot)$ of a trajectory is defined as the number of transitions
until an absorbing state is entered for the first time.
Regarding trajectories, we can consider just prefixes of length $N$ because the
CE MDP bounds the remaining horizon component by $N$. Further, we can
restrict ourselves to the subspace $\Traj \subset (\bar{\mathcal{S}}\times\mathcal{A})^N\times\bar{\mathcal{S}}$ allowed
by remaining horizon/goal dynamics of CE, i.e., for a trajectory
$\tau = (\bar{s}_0,a_0,\ldots,\bar{s}_N) \in \Traj$, $\bar{s}_t = (s_t,h_t,g_t)$, $0\leq t\leq N$,
the remaining horizon $h_t$ decreases by 1
from its initial value $h_0=l(\tau)$ till 0 when entering to
an absorbing state. The goal $g_t=g_0$ remains unchanged.
Thus $\tau$ is fully determined by the initial horizon $h_0=l(\tau)$, the initial goal $g_0$, the
states $s_0,\ldots,s_{l(\tau)}$, and actions $a_0,\ldots,a_{l(\tau)-1}$, i.e.,
$\tau = ((s_0,h_0,g_0),a_0,(s_1,h_0-1,g_0),a_1,\ldots,(s_{l(\tau)},0,g_0),a_A,\ldots,(s_{l(\tau)},0,g_0))$.
The probability of $\tau$ is given as:
$$
\prob( \mathcal{T} = \tau;\pi)
=
\left( \prod_{t=1}^{l(\tau)}
\tkernel(s_t | a_{t-1}, s_{t-1})
\right)
\times
\left( \prod_{t=0}^{l(\tau)-1}
\pi(a_t|\bar{s}_t)
\right)
\times
\bar{\mu}(\bar{s}_0),
$$
where $\mathcal{T}:\Omega\rightarrow\Traj$, $\mathcal{T} = ((S_0,H_0,G_0),A_0,\ldots,(S_N,H_N,G_N))$ is the trajectory random variable map.

\paragraph{Segment Distribution:}
Segments are assumed to be continuous chunks of trajectories in $\Traj$,
so they respect the CE horizon/goal dynamics.
We assume them to be always contained within the length of a trajectory.
Notice that \eUDRL{} is learning actions just in a state which happens to be the first state of a segment.
Since learning actions in absorbing states is not meaningful, we will assume the first state of a segment to be transient (i.e., $\in \bar{S}_T$)\footnote{One could likewise additionally assume the original MDP component of this state to be transient for the similar reason.}.
Thus, segments $\sigma$ are fully determined by the
segment length, denoted by $l(\sigma)$ (the number of transitions), by the
remaining horizon and goal at the segment beginning
$h_0^{\sigma}$,$g_0^{\sigma}$, and by $l(\sigma)+1$ states and $l(\sigma)$
actions. Without loss of generality we will identify $\sigma$ with such a tuple
$\sigma=(l(\sigma),s_0^{\sigma},h_0^{\sigma},g_0^{\sigma},a_0^{\sigma},s_1^{\sigma},a_1^{\sigma},\ldots,s_{l(\sigma)}^{\sigma})$.
The space of all such tuples will be denoted $\Seg$.
Formally, we will assume that there exists a random variable map $\Sigma: \Omega \rightarrow \Seg$, $\Sigma = (l(\Sigma),\stag{S}_0,\stag{H}_0,\stag{G}_0,\stag{A}_0,\stag{S}_1,\stag{A}_1,\ldots,\stag{S}_{l(\Sigma)})$
with distribution $d_{\Sigma}^{\pi}$ given
below.

We construct the segment distribution $d_{\Sigma}^{\pi}$ in a similar way as the state visitation distribution---summing across appropriate trajectory distribution marginals.
This causes the result to be un-normalized (among other restrictions, e.g., on the first state of the segment), therefore we have to include a normalization constant $c$. ($\forall \sigma \in \Seg$):
$$
\begin{aligned}
&(d_{\Sigma}^{\pi_n}(\sigma) :=)\:
\prob(\Sigma=\sigma;\pi)
=
\\
&= c^{-1}\sum_{t \leq N -l(\sigma)}
\prob(S_t=s_0^{\sigma},H_t=h_0^{\sigma},G_t=g_0^{\sigma},A_t=a_0^{\sigma}, \ldots, S_{t+l(\sigma)}=s_{l(\sigma)}^{\sigma} ;\pi)
\\
&=
c^{-1}
\sum_{t \leq N -l(\sigma)}
\left( \prod_{i=1}^{l(\sigma)}
\tkernel(s_{i}^{\sigma} | a_{i-1}^{\sigma}, s_{i-1}^{\sigma})
\right)
\cdot
\left( \prod_{i=0}^{l(\sigma)-1}
\pi(a_i^{\sigma}|\bar{s}_i^{\sigma})
\right)
\cdot
\prob(S_t=s_0^{\sigma},H_t=h_0^{\sigma},G_t = g_0^{\sigma};\pi)
\\
&=
c^{-1}
\left( \prod_{i=1}^{l(\sigma)}
\tkernel(s_{i}^{\sigma} | a_{i-1}^{\sigma}, s_{i-1}^{\sigma})
\right)
\cdot
\left(\prod_{i=0}^{l(\sigma)-1}
\pi(a_i^{\sigma}|\bar{s}_i^{\sigma})
\right)
\cdot
\sum_{t \leq N -l(\sigma)}
\prob(S_t=s_0^{\sigma},H_t=h_0^{\sigma},G_t = g_0^{\sigma};\pi)
,
\end{aligned}
$$
where
$$
c := \sum_{\sigma\in \Seg}
\sum_{t \leq N -l(\sigma)}
\prob(S_t=s_0^{\sigma},H_t=h_0^{\sigma},G_t=g_0^{\sigma},A_t=a_0^{\sigma}, \ldots, S_{t+l(\sigma)}=s_{l(\sigma)}^{\sigma} ;\pi) > 0
.
$$
Since we constrained $N\geq 1$, $\supp \bar{\mu} \subset \bar{S}_T$ (which prevents 
degenerate cases), there must be a non-zero probability assigned to some segments
leading to the full summation in the definition of $c$ being positive. 
This means that $c^{-1}$ and the segment distribution are defined correctly.
Given the factorized form of the segment distribution we can conclude the following (through computing marginals and calculating conditional probability ratios):
\begin{equation*}
\prob(l(\Sigma)=k,\stag{S}_0=s_0,\stag{H}_0=h_0,\stag{G}_0=g;\pi)
=
c^{-1}
\sum_{t \leq N -k}
\prob(S_t=s_0,H_t=h_0,G_t = g;\pi)
\end{equation*}
\begin{multline*}
\prob(\stag{A}_0=a_0,\stag{S}_1=s_1,\stag{A}_1=a_1,\ldots,\stag{S}_k=s_k
|\stag{S}_0=s_0,\stag{H}_0=h_0,\stag{G}_0=g_0,l(\Sigma)=k
;\pi)
\\
=
\left( \prod_{i=1}^{k}
\tkernel(s_{i} | a_{i-1}, s_{i-1})
\right)
\times
\prod_{i=0}^{k-1}
\pi(a_i|\bar{s}_i)
\end{multline*}
and
$(\forall 0 \leq i \leq k)$:
\begin{multline}
\prob(\stag{A}_i=a_i|\stag{S}_i=s_i,\ldots,\stag{S}_0=s_0,\stag{H}_0=h_0,\stag{G}_0=g,l(\Sigma)=k;\pi)
\nonumber
\\
\begin{aligned}
&=\pi(a_i|s_i,h_0-i,g)
\\
&=\prob(\stag{A}_i=a_i|\stag{S}_i=s_i,\stag{H}_0=h_0,\stag{G}_0=g;\pi), 
\end{aligned}
\tag{\ref{eq:segactions}}
\end{multline}
\begin{multline}
\prob(\stag{S}_i=s_i|\stag{S}_{i-1}=s_{i-1},\stag{A}_{i-1}=a_{i-1},\ldots,\stag{S}_0=s_0,\stag{H}_0=h_0,\stag{G}_0=g_0,l(\Sigma)=k;\pi)
\\
\begin{aligned}
&=
\tkernel(s_i|s_{i-1},a_{i-1})
\\
&=
\prob(\stag{S}_i=s_i|\stag{S}_{i-1}=s_{i-1},\stag{A}_{i-1}=a_{i-1})
\end{aligned}
\tag{\ref{eq:segtransitions}}
\end{multline}
After defining $\stag{H}_i := \stag{H}_0-i,\stag{G}_i := \stag{G}_0$, we can write
$\prob(\stag{A}_i=a_i|\stag{S}_i=s_i,\stag{H}_i=h_i,\stag{G}_i=g_i;\pi)= \pi(a_i|s_i,h_i,g_i)
$.

\paragraph{Bounding $c$:}
For further investigation it is useful to bound the normalizing constant
$c$. We can write
\begin{equation*}
\begin{aligned}
0 < c &= \sum_{\sigma \in \Seg}
\sum_{t \leq N -l(\sigma)}
\prob(S_t=s_0^{\sigma},H_t=h_0^{\sigma},G_t=g_0^{\sigma},A_t=a_0^{\sigma}, \ldots, S_{t+l(\sigma)}=s_{l(\sigma)}^{\sigma} ;\pi)
\\
&=
\sum_{k=1}^{N}
\sum_{t \leq N - k}
\sum_{\sigma : l(\sigma)=k}
\prob(S_t=s_0^{\sigma},H_t=h_0^{\sigma},G_t=g_0^{\sigma},A_t=a_0^{\sigma}, \ldots, S_{t+l(\sigma)}=s_{l(\sigma)}^{\sigma} ;\pi)
\\
&\leq
\sum_{k=1}^{N}
\sum_{t \leq N - k}
1
=
\frac{N(N+1)}{2}
.
\end{aligned}
\tag{\ref{eq:cupperbound}}
\end{equation*}



\section{Motivating Examples}
\label{ap:examples}

We begin this section by demonstrating discontinuities of \eUDRL{} generated quantities in the transition kernel given specific example of compatible families of MDPs.
In order to demonstrate a discontinuity at a specific point $\tkernel_0 \in (\Delta \mathcal{S})^{\mathcal{S}\times\mathcal{A}}$
we take limits with respect to two distinct rays meeting at $\tkernel_0$ and showing that they disagree. Apart of providing plots
depicting how the value of investigated quantity depends on the ray parameter we also provide exact computation of the limits.
During the computation we will use the form of \eUDRL{} recursion
introduced by \citet{strupl2022upsidedown}
($\forall a\in \mathcal{A},(s,h,g)\in \supp \den_{\tkernel,\pi_n}$):
\begin{equation}
\begin{aligned}
\pi_{n+1}(a|s,h,g)
&=
\prob_{\tkernel}(\stag{A}_0=a|\stag{S}_0=s,l(\Sigma)=h,\rho(\stag{S}_{l(\Sigma)}) = g; \pi_n)
\\
&=
\frac{
Q_{A}^{\pi_n,g}(s,h,a)
\pi_{A,n} (a|s,h)
}
{
\sum_{a\in\mathcal{A}} Q_{A}^{\pi_n,g}(s,h,a) \pi_{A,n} (a|s,h)
}
,
\end{aligned}
\label{eq:recursion}
\end{equation}
where $Q_{A}$ denotes an ``average" $Q$-value and $\pi_{A,n}$ denotes
an ``average" policy
\begin{equation}
\begin{aligned}
Q_{A}^{\pi_n,g}(s,h,a) &= 
\prob_{\tkernel}(\rho (\stag{S}_{l(\Sigma)})=g | \stag{A}_0=a, \stag{S}_0=s,l(\Sigma)=h; \pi_n)
\\
\pi_{A,n} (a|s,h) &=
\sum_{h'\geq h,g'\in\mathcal{G}}
\pi_n (a|h',g',s)
\prob_{\tkernel}( \stag{H}_0=h', \stag{G}_0=g' |\stag{S}_0=s,l(\Sigma)=h; \pi_n)
.\label{eq:apolicy}
\end{aligned}
\end{equation}
The derivation of the above \eUDRL{} recursion rewrite uses the same techniques as the proof of lemma \ref{le:recrewrites}.%

At points $\tkernel$ lying on a boundary of $(\Delta \mathcal{S})^{\mathcal{S}\times\mathcal{A}}$ the continuity can break. Therefore, we cannot prove continuity on the whole
boundary.
This is illustrated in the following counter-example.

\begin{example} (non-removable discontinuity of goal reaching objective at a boundary point)
\label{ex:boundarypoint}
Consider an MDP $\mathcal{M} = (\mathcal{S},\mathcal{A},\tkernel,\mu,r)$
with three states $\mathcal{S} = \{0,1,2\}$ and three actions
$\mathcal{A} = \{0,1,2\}$. The state 0 is the only initial state.
We consider its CE $\bar{M}$ with maximum horizon $N=1$, $\mathcal{G}:=\mathcal{S}$, $\rho := \mathrm{id}_{\mathcal{S}}$. The CE's initial distribution fixes the initial remaining horizon at $H_0=1$ and initial goals
are distributed as $\prob(G_0=0) = \prob(G_0=2) = \frac{1}{2}$ (so $\prob(G_0=1) = 0$).
It follows that we are left with only two transient states $(0,1,0)$, $(0,1,2) \in \mathcal{S}\times\mathbb{N}_0\times\mathcal{G}$ differing just by the goal component which are actually visited. Thus it suffices
to provide policies and values only for these states. 
Since we have $N=1$
and the only initial state $S_0 = 0$ it suffice to give the transition kernel only
from this state.
We split the computation into three parts and label them by letters A,B and C for later reference.

\vspace{1em}
\noindent\textbf{A} First we define the parametric transition kernel (with parameter $\alpha \in [0,1]$)
$$
\begin{tabular}{c|ccc}
$\tkernel_{\alpha}(g|a)$ & $g=0$ & $g=1$ & $g=2$ \\
\hline
$a=0$  & $1-\alpha$          & $\frac{\alpha}{4}$ & $\frac{3\alpha}{4}$ \\
$a=1$  & $\frac{3\alpha}{4}$ & $1-\alpha$         & $\frac{\alpha}{4}$  \\
$a=2$  & $\frac{1}{2}$       & $\frac{1}{2}$      & $0$ \\
\end{tabular}
$$
Which we identify with the matrix
$$
\underset{a\in \mathcal{A}, g \in \mathcal{G}}{[\tkernel_{\alpha}(g|a)]} =
\left(
\begin{array}{ccc}
1-\alpha          & \frac{\alpha}{4} & \frac{3\alpha}{4} \\
\frac{3\alpha}{4} & 1-\alpha         & \frac{\alpha}{4}  \\
\frac{1}{2}       & \frac{1}{2}      & 0 \\
\end{array}
\right)
\xrightarrow{\alpha\rightarrow 0+}
\left(
\begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0  \\
\frac{1}{2} & \frac{1}{2} & 0 \\
\end{array}
\right)
=
\underset{a\in \mathcal{A}, g \in \mathcal{G}}{[\tkernel_{0+}(g|a)]}.
$$
We will be
interested how values (policies) behave at and around the point $\alpha = 0$.
For computing the \eUDRL{} policies we will use the recursion
\eqref{eq:recursion}.
We will note argument names inside kernels and value functions
(in case they were substituted by actual values) so it is not necessary
to always look for how we fixed individual argument positions.\footnote{
This is only to improve readability of the example by helping the reader to keep track of substitutions. We use different color (gray)
for these extra notes.
}
Using \eref{eq:apolicy} we get
$Q_{A}^{\pi_n,g} (\subnote{s=}0,\subnote{h=}1,a) = \tkernel(g|a,\subnote{s=}0)$.
Since the condition $\supp \bar{\mu} \subset \bar{\mathcal{S}}_T$ and fixed horizon 1 the segments coincide with trajectories. Therefore, the distribution of the first extended state of a segment is the same
as the initial distribution $\bar{\mu}$:
$\prob(\stag{H}_0=1,\stag{G}_0=g'| \stag{S}_0=0,l(\Sigma)=1 ; \pi_n )
= \prob(G_0=g')$.
Thus we get the following relation for the ``average" policy:
$\pi_{A,n}(a|\subnote{s=}0,\subnote{h=}1) = \sum_{g' \in \mathcal{G}} \pi_n(a|\subnote{s=}0,\subnote{h'=}1,g') \prob(G_0=g')$.
The recursion \eqref{eq:recursion} can then be rewitten
\begin{align*}
\pi_{n+1}(a|\subnote{s=}0,\subnote{h=}1,g) &\propto
Q_{A}^{\pi_n,g} (\subnote{s=}0,\subnote{h=}1,a)
\pi_{A,n}(a|\subnote{s=}0,\subnote{h=}1)
\\
&=
\tkernel(g|a,\subnote{s=}0)
\pi_{A,n}(a|\subnote{s=}0,\subnote{h=}1)
.
\end{align*}
Assuming we start with uniform initial condition $\pi_0$ we obtain also
uniform $\pi_{A,0} = \frac{1}{3}$ leaving 
$\pi_{1}(a|\subnote{s=}0,\subnote{h=}1,g) \propto \tkernel(g|a,\subnote{s=}0)
\frac{1}{3} \propto \tkernel(g|a,\subnote{s=}0)$.
Thus we get
$$
\underset{a\in \mathcal{A}, g \in \mathcal{G}}{[\pi_{1,\alpha}(a|g)]} =
\left(
\begin{array}{ccc}
\frac{1-\alpha}{1-\alpha + \frac{3\alpha}{4} +\frac{1}{2}} &
\frac{\frac{\alpha}{4}}{\frac{\alpha}{4} + 1-\alpha + \frac{1}{2}} &
\frac{3}{4}
\\
\frac{\frac{3\alpha}{4}}{1-\alpha + \frac{3\alpha}{4} +\frac{1}{2}} &
\frac{1-\alpha}{\frac{\alpha}{4} + 1-\alpha + \frac{1}{2}} &
\frac{1}{4}
\\
\frac{\frac{1}{2}}{1-\alpha + \frac{3\alpha}{4} +\frac{1}{2}} &
\frac{\frac{1}{2}}{\frac{\alpha}{4} + 1-\alpha + \frac{1}{2}} &
0
\end{array}
\right)
\xrightarrow{\alpha \rightarrow 0+}
\left(
\begin{array}{ccc}
\frac{2}{3} &
0 &
\frac{3}{4}
\\
0 &
\frac{2}{3} &
\frac{1}{4}
\\
\frac{1}{3} &
\frac{1}{3} &
0
\end{array}
\right)
=
\underset{a\in \mathcal{A}, g \in \mathcal{G}}{[\pi_{1,0+}(a|g)]}
.
$$
This gives us
$$
\underset{a\in \mathcal{A}}{[\pi_{A,1,0+}]}
=
\underset{a\in \mathcal{A}, g \in \mathcal{G}}{[\pi_{1,0+}(a|g)]}
\underset{g \in \mathcal{G}}{[\prob(G_0=g)]}
=
\underset{a\in \mathcal{A}, g \in \mathcal{G}}{[\pi_{1,0+}(a|g)]}
\left(\begin{array}{c}\frac{1}{2}\\0\\\frac{1}{2}\end{array}\right)
= 
\left(\begin{array}{c}\frac{17}{24}\\\frac{1}{8}\\\frac{1}{6}\end{array}\right)
$$
We deduce from
$\pi_{2}(a|\subnote{s=}0,\subnote{h=}1,g) \propto \tkernel(g|a,\subnote{s=}0)
\pi_{A,1}(a)$ that 
$$
\lim_{\alpha \rightarrow 0+}\pi_{2}(a|\subnote{s=}0,\subnote{h=}1,g)
= \frac{ \lim_{\alpha \rightarrow 0+} \tkernel(g|a,\subnote{s=}0) \cdot  \lim_{\alpha \rightarrow 0+} \pi_{A,1}(a) }{ \sum_{a\in \mathcal{A}} \lim_{\alpha \rightarrow 0+} \tkernel(g|a,\subnote{s=}0) \cdot  \lim_{\alpha \rightarrow 0+} \pi_{A,1}(a)}
$$
provided all limits exists and division makes sense. Fortunately we will be interested
just in the column for $g=0$ and $g=1$
$$
\underset{a\in \mathcal{A}}{[\pi_{2,0+}(a|\subnote{g=}0)]} = \left(
\begin{array}{c}
\frac{\frac{17}{24}1}{\frac{17}{24}1 + \frac{1}{8}0 + \frac{1}{6}\frac{1}{2} }
\\
\frac{\frac{1}{8}0}{\frac{17}{24}1 + \frac{1}{8}0 + \frac{1}{6}\frac{1}{2} }
\\
\frac{\frac{1}{6}\frac{1}{2}}{\frac{17}{24}1 + \frac{1}{8}0 + \frac{1}{6}\frac{1}{2}}
\end{array}
\right)
=
\left(
\begin{array}{c}
\frac{\frac{17}{24}}{\frac{19}{24}}
\\
0 
\\
\frac{\frac{1}{12}}{\frac{19}{24}}
\end{array}
\right)
=
\left(
\begin{array}{c}
\frac{17}{19}
\\
0 
\\
\frac{2}{19}
\end{array}
\right)
,
\quad
\underset{a\in \mathcal{A}}{[\pi_{2,0+}(a|\subnote{g=}1)]} = \left(
\begin{array}{c}
0
\\
\frac{3}{5}
\\
\frac{2}{5}
\end{array}
\right).
$$
Finally using 
$$
V_{0+}^{\pi_2}(\subnote{s=}0,\subnote{h=}1,g) = \sum_{a\in \mathcal{A}}\tkernel_{0+}(g|\subnote{s=}0,a) \pi_{2,0+}(a|g)$$
and
$$J_{\tkernel,\pi_{2},0+} = \sum_{g\in \mathcal{G}} V_{0+}^{\pi_2}(\subnote{s=}0,\subnote{h=}1,g) \bar{\mu}(\subnote{s=}0,\subnote{h=}1,g) =
\sum_{g\in \mathcal{G}}
V_{0+}^{\pi_2}(\subnote{s=}0,\subnote{h=}1,g) \prob(G_0=g)
$$
the values
$V_{0+}^{\pi_2}(\subnote{s=}0,\subnote{h=}1,g=0) = \frac{18}{19}$, $V_{0+}^{\pi_2}(\subnote{s=}0,\subnote{h=}1,g=2) = 0$ and the goal reaching objective
$J_{\tkernel,\pi_{2},0+}= \frac{9}{19}$ can be computed.

\vspace{1em}
\noindent{}\textbf{B}
Now we aim to re-compute everything for $\alpha = 0$. Since $\tkernel$ is continuous in $\tkernel$ we get
$\tkernel_0 = \tkernel_{0+}$. Further since $\tkernel_0(g=2|\cdot) = 0$ we
get $\pi_{1,0}(\cdot|g=2) = \frac{1}{\mathcal{A}} = \frac{1}{3}$. Thus
$$
\underset{a\in \mathcal{A}, g \in \mathcal{G}}{[\pi_{1,0}(a|g)]} =
\left(
\begin{array}{ccc}
\frac{2}{3} &
0 &
\frac{1}{3}
\\
0 &
\frac{2}{3} &
\frac{1}{3}
\\
\frac{1}{3} &
\frac{1}{3} &
\frac{1}{3}
\end{array}
\right)
,\quad
\underset{a\in \mathcal{A}}{[\pi_{A,1,0}]}
=
\underset{a\in \mathcal{A}, g \in \mathcal{G}}{[\pi_{1,0}(a|g)]}
\left(\begin{array}{c}\frac{1}{2}\\0\\\frac{1}{2}\end{array}\right)
= 
\left(\begin{array}{c}\frac{1}{2}\\\frac{1}{6}\\\frac{1}{3}\end{array}\right)
$$



$$
\underset{a\in \mathcal{A}}{[\pi_{2,0}(a|\subnote{g=}0)]}
=
\underset{a\in \mathcal{A}} {[\frac{\tkernel_0(a|\subnote{g=}0)) 
\pi_{A,1,0}(a)}{\sum_{a\in \mathcal{A}} \tkernel_0(a|\subnote{g=}0))\pi_{A,1,0}(a)}]}
=
\left(
\begin{array}{c}
\frac{\frac{1}{2}1}{\frac{1}{2}1 + \frac{1}{6}0 + \frac{1}{3}\frac{1}{2}}
\\
\frac{\frac{1}{6}0}{\frac{1}{2}1 + \frac{1}{6}0 + \frac{1}{3}\frac{1}{2}}
\\
\frac{\frac{1}{3}\frac{1}{2}}{\frac{1}{2}1 + \frac{1}{6}0 + \frac{1}{3}\frac{1}{2}}
\end{array}
\right)
=
\left(
\begin{array}{c}
\frac{\frac{1}{2}}{\frac{4}{6}}
\\
0 
\\
\frac{\frac{1}{6}}{\frac{4}{6}}
\end{array}
\right)
=
\left(
\begin{array}{c}
\frac{3}{4}
\\
0 
\\
\frac{1}{4}
\end{array}
\right),
$$
similarly
$$
\underset{a\in \mathcal{A}}{[\pi_{2,0}(a|\subnote{g=}1)]}
=
\left(
\begin{array}{c}
0
\\
\frac{1}{2}
\\
\frac{1}{2}
\end{array}
\right)
.
$$
Finally we get
$$
V_{0}^{\pi_2}(\subnote{s=}0,\subnote{h=}1,g=0) = \frac{7}{8},\quad V_{0}^{\pi_2}(\subnote{s=}0,\subnote{h=}1,g=2) = 0,\quad
J_{\tkernel,\pi_{2},0}= \frac{7}{16}
,
$$
which means that we have a discontinuity at $\alpha = 0$: a boundary point.

\vspace{1em}
\noindent{}\textbf{C}
Moreover, this discontinuity could not be removed as can be seen
by computing the limit with respect to another ray, e.g.,
$$
\underset{a\in \mathcal{A}, g \in \mathcal{G}}{[\tkernel_{\alpha}'(g|a)]} =
\left(
\begin{array}{ccc}
1-\alpha          & \frac{3\alpha}{4} & \frac{\alpha}{4} \\
\frac{\alpha}{4} & 1-\alpha         & \frac{3\alpha}{4}  \\
\frac{1}{2}       & \frac{1}{2}      & 0 \\
\end{array}
\right)
.
$$
Notice that $\tkernel_{0+}' = \tkernel_{0}' = \tkernel_{0+} = \tkernel_{0}$. 
Following the same procedure as in part A we obtain the following results:
$$
\begin{gathered}
\underset{a\in \mathcal{A}, g \in \mathcal{G}}{[\pi_{1,0+}'(a|g)]} =
\left(
\begin{array}{ccc}
\frac{3}{2} & 0            & \frac{1}{4} \\
0           & \frac{2}{3}  & \frac{3}{4}  \\
\frac{1}{3} & \frac{1}{3}  & 0 \\
\end{array}
\right)
,\quad
\underset{a\in \mathcal{A}}{[\pi_{A,1,0+}'(a)]} =
\left(
\begin{array}{c}
\frac{11}{24} \\
\frac{3}{8} \\
\frac{1}{6}  \\
\end{array}
\right)
,
\\
\underset{a\in \mathcal{A}}{[\pi_{2,0+}'(a|g=0)]} =
\left(
\begin{array}{c}
\frac{11}{13} \\
0 \\
\frac{2}{13}  \\
\end{array}
\right)
,\quad
\underset{a\in \mathcal{A}}{[\pi_{2,0+}'(a|g=1)]} =
\left(
\begin{array}{c}
0 \\
\frac{9}{11} \\
\frac{2}{11}  \\
\end{array}
\right),
\\
V_{0+}^{'\pi_2}(g=0) = \frac{12}{13},\quad
V_{0+}^{'\pi_2}(g=2) = 0,\quad
J_{0+}^{'\pi_2} = \frac{6}{13}.
\end{gathered}
$$
\end{example}

\textbf{Discussing alternative definitions of \eUDRL{} recursion outside of $\supp \den_{\tkernel,\pi_n}$:} 
Since the computation of the limits in \textit{\textbf{A}} and \textit{\textbf{C}} does not depend
on the way we defined the \eUDRL{} recursion outside of $\supp \den_{\tkernel,\pi_n}$ in \eref{eq:recursionUsingNumeratorDenominator},
the presented non-removable discontinuities of $\pi_2$ and $J^{\pi_2}$
at $\tkernel_0$ cannot vanish by employing any alternative definition
of the recursion outside of $\supp \den_{\tkernel,\pi_n}$. This means there is no suitable alternative definition of \eUDRL{} recursion (outside of $\supp \den_{\tkernel,\pi_n}$) which would remove the present discontinuity, e.g. in the goal-reaching objective. Therefore, the concrete
definition of the recursion in \eref{eq:recursionUsingNumeratorDenominator} outside of $\supp \den_{\tkernel,\pi_n}$ does not matter much from this point of view (the discontinuities will always be there). The same reasoning applies also for discontinuities presented in the next example.

The next example illustrates non-removable discontinuity of
\eUDRL{} generated policies (for $n \geq 2$)
at deterministic points. However,
while we will see the limits with respect to different rays
disagree, they are all optimal policies. This could motivate us
to prove relative continuity (some weaker notion of continuity)
for these policies, as was successfully achieved in theorem \ref{le:detcont}.
\begin{example}
\label{ex:detpoint}
(non-removable discontinuity of policy at deterministic point)
Let the MDP $\mathcal{M}$ be the same as in example \ref{ex:boundarypoint} except that we vary the parametric
transition kernels to illustrate the behavior around (and at) the specific deterministic transition kernel. Further, we change the distribution of the initial goals to the uniform distribution, i.e., $\prob(G_0 = 0) = \prob(G_0 = 1) = \prob(G_0 = 2) = \frac{1}{3}$.
Since the computation proceeds as in the example \ref{ex:boundarypoint} we just introduce the new kernels and summarize
the results.

\vspace{1em}
\noindent{}\textbf{A}
$$
\underset{a\in \mathcal{A}, g \in \mathcal{G}}{[\tkernel_{\alpha}(g|a)]} =
\left(
\begin{array}{ccc}
1-\alpha  & \alpha   & 0 \\
0         & 1-\alpha & \alpha  \\
\alpha    & 1-\alpha & 0 \\
\end{array}
\right)
\xrightarrow{\alpha\rightarrow 0+}
\left(
\begin{array}{ccc}
1  & 0  & 0 \\
0  & 1  & 0 \\
0  & 1  & 0 \\
\end{array}
\right)
=
\underset{a\in \mathcal{A}, g \in \mathcal{G}}{[\tkernel_{0+}(g|a)]}
(=
\underset{a\in \mathcal{A}, g \in \mathcal{G}}{[\tkernel_{0}(g|a)]}
)
.
$$
Notice continuity at $\alpha = 0$ ($\tkernel_{0+} = \tkernel_{0}$).
The results follow:
$$
\begin{gathered}
\underset{a\in \mathcal{A}, g \in \mathcal{G}}{[\pi_{1,0+}(a|g)]} =
\left(
\begin{array}{ccc}
1 & 0            & 0 \\
0           & \frac{1}{2}  & 1  \\
0 & \frac{1}{2}  & 0 \\
\end{array}
\right)
,\quad
\underset{a\in \mathcal{A}}{[\pi_{A,1,0+}(a)]} =
\left(
\begin{array}{c}
\frac{1}{3} \\
\frac{1}{2} \\
\frac{1}{6}  \\
\end{array}
\right)
,
\\
\underset{a\in \mathcal{A}}{[\pi_{2,0+}(a|g=0)]} =
\left(
\begin{array}{c}
1 \\
0 \\
0  \\
\end{array}
\right)
,\quad
\underset{a\in \mathcal{A}}{[\pi_{2,0+}(a|g=1)]} =
\left(
\begin{array}{c}
0 \\
\frac{3}{4} \\
\frac{1}{4}  \\
\end{array}
\right)
,\\
V_{0+}^{\pi_2}(g=0) = 1,\quad
V_{0+}^{\pi_2}(g=1) = 1,\quad
V_{0+}^{\pi_2}(g=2) = 0,\quad
J_{0+}^{\pi_2} = \frac{2}{3}.
\end{gathered}
$$

\vspace{1em}
\noindent{}\textbf{B}
$$
\begin{gathered}
\underset{a\in \mathcal{A}, g \in \mathcal{G}}{[\pi_{1,0}(a|g)]} =
\left(
\begin{array}{ccc}
1 & 0            & \frac{1}{3} \\
0 & \frac{1}{2}  & \frac{1}{3} \\
0 & \frac{1}{2}  & \frac{1}{3} \\
\end{array}
\right)
,\quad
\underset{a\in \mathcal{A}}{[\pi_{A,1,0}(a)]} =
\left(
\begin{array}{c}
\frac{4}{9} \\
\frac{5}{18} \\
\frac{5}{18}  \\
\end{array}
\right)
,\quad
\underset{a\in \mathcal{A},g\in \mathcal{G}}{[\pi_{2,0}(a|g)]} =
\left(
\begin{array}{ccc}
1 & 0 & \frac{1}{3} \\
0 & \frac{1}{2} & \frac{1}{3} \\
0 & \frac{1}{2} & \frac{1}{3} \\
\end{array}
\right)
,\\
V_{0}^{\pi_2}(g=0) = 1,\quad
V_{0}^{\pi_2}(g=1) = 1,\quad
V_{0}^{\pi_2}(g=2) = 0,\quad
J_{0}^{\pi_2} = \frac{2}{3}.
\end{gathered}
$$

\vspace{1em}
\noindent{}\textbf{C}
$$
\underset{a\in \mathcal{A}, g \in \mathcal{G}}{[\tkernel_{1,\alpha}'(a|g)]} :=
\left(
\begin{array}{ccc}
1-\alpha & 0            & \alpha \\
\alpha   & 1-\alpha  & 0  \\
0        & 1-\alpha  & \alpha \\
\end{array}
\right).
$$
Notice that $\tkernel_{1,0+}' = \tkernel_{1,0}' = \tkernel_{1,0} = \tkernel_{1,0+}$.
$$
\begin{gathered}
\underset{a\in \mathcal{A}, g \in \mathcal{G}}{[\pi_{1,0+}'(a|g)]} =
\left(
\begin{array}{ccc}
1 & 0            & \frac{1}{2} \\
0 & \frac{1}{2}  & 0  \\
0 & \frac{1}{2}  & \frac{1}{2} \\
\end{array}
\right)
,\quad
\underset{a\in \mathcal{A}}{[\pi_{A,1,0+}'(a)]} =
\left(
\begin{array}{c}
\frac{1}{2} \\
\frac{1}{6} \\
\frac{1}{3}  \\
\end{array}
\right)
,\quad
\underset{a\in \mathcal{A},g\in \mathcal{G}}{[\pi_{2,0+}'(a|g)]} =
\left(
\begin{array}{ccc}
1 & 0           & \frac{3}{5} \\
0 & \frac{1}{3} & 0 \\
0 & \frac{2}{3} & \frac{2}{5}  \\
\end{array}
\right)
,\\
V_{0+}^{'\pi_2}(g=0) = 1,\quad
V_{0+}^{'\pi_2}(g=1) = 1,\quad
V_{0+}^{'\pi_2}(g=2) = 0,\quad
J_{0+}^{'\pi_2} = \frac{2}{3}.
\end{gathered}
$$
\end{example}
The dependence of the goal reaching objective and the specific 
$\pi_2$ component on $\alpha$ is depicted in figure \ref{fig:discont}
(for both examples \ref{ex:boundarypoint} and \ref{ex:detpoint}).
While policies are clearly discontinuous at $\alpha = 0$ in both
examples, we see continuity of a goal-reaching objective (on A and C rays) at the deterministic kernel
(example \ref{ex:detpoint}). This hints at a possible 
continuity of goal reaching objectives and relative continuity
of policies at deterministic points (kernels), as was later 
proved in this paper. Unfortunately, we cannot hope for
the similar (general) result in a case of non-deterministic boundary points (kernels) which was clearly demonstrated in example \ref{ex:boundarypoint} and figures \ref{fig:discont:c} and \ref{fig:discont:d}. In addition to the examples above, we include a discussion describing the causes that lead to discontinuities of \eUDRL{}-generated quantities at the end of section~\ref{ap:interiorcont}. 

In the following example, we want to illustrate (relative) continuity of \eUDRL{} generated policies and goal reaching objectives for a finite number of iterations, which was proved in section \ref{se:contfinite}. 
\begin{example}
\label{ex:oscillations} (example featuring random walk on $\mathbb{Z}_3$)
In this example, we consider an MDP $\mathcal{M} = (\mathcal{S},\mathcal{A},\tkernel,\mu,r)$ with state space $\mathcal{S} = \mathbb{Z}_3$ and action space $\mathcal{A}=\{0,1\}$.
The transition kernel depends on the parameter $\alpha \in [0,1]$ that regulates the
stochasticity.
When action $0$ is selected, the MDP stays in the current state $s\in\mathcal{S}$ with probability $1-\alpha$ and transits to the state $s+1$ with probability $\alpha$.
When action $1$ is selected, the MDP stays in the current state $s \in \mathcal{S}$ with probability $\alpha$ and transits to the state $s+1$ with probability $1-\alpha$, i.e., the states we are transiting stay the same; just the probabilities are swapped. For both actions, the transition kernels are essentially
different random walks on $\mathcal{S} = \mathbb{Z}_3$.
We consider $\mathcal{M}$'s CE $\bar{\mathcal{M}}$ with remaining horizon $N=8$,
$\mathcal{G}:=\mathcal{S}$, $\rho :=\mathrm{id}_{\mathcal{S}}$.
The CE's initial distribution is a uniform distribution over all extended states.
\end{example}
The figures \ref{fig:oscillationsPI} and \ref{fig:oscillations} show the dependence
of $\min_{\bar{s}\in\theinterestingstates} \pi_{n,\tkernel}(\oactions(\bar{s}|\bar{s}))$ and the goal reaching objective respectively on iteration $n$.
Continuity in the transition kernel is hinted
at by these figures as the plots for different $\delta := \|\tkernel-\tkernel_0\|_1$
approach the plot for $\delta = 0 $ (i.e., $\tkernel_0$ plot) smoothly as
$\delta \rightarrow 0$. Plots in this particular example are interesting
as they exhibit significant oscillations in the first few iterations.
A more detailed plot for the oscillating goal reaching objective (for one specific $\tkernel$ and one specific initial condition) is shown in \ref{fig:oscillations:b}. Here we see that the best results are obtained
for either iteration 2 or 3. Furthermore, the performance of the goal reaching objective deteriorates
when \eUDRL{} is continued.
The general non-monotonicity of a goal reaching objective could raise the question of
whether or not it makes sense to continue \eUDRL{} iterations in order to reach some
\lq\lq{}optimal\rq\rq{} behavior. These questions motivate our theoretical investigation
of the asymptotic properties of \eUDRL{} iteration in section \ref{se:continfty}.

In order to compare bounds derived for all special cases from section
\ref{se:continfty}, we include a simple bandit example below (which
complies with both of the conditions $(\forall \bar{s}\in \theinterestingstates): |\oactions(\bar{s})|=1$ and $\theinterestingstates \subset \supp \bar{\mu}$).
\begin{example}\label{ex:bandit} (a simple bandit)
We consider an MDP $\mathcal{M} = (\mathcal{S},\mathcal{A},\tkernel,\mu,r)$ with a state space $\mathcal{S} = \{0,1\}$ and an action space $\mathcal{A}=\{0,1\}$.
The transition kernel depends on parameter $\alpha \in [0,1]$ which regulates
stochasticity.
When action $0$ is selected, the MDP stays in the current state $s\in\mathcal{S}$ with probability $1-\alpha$ and transitions to the other state with probability $\alpha$.
When action $1$ is selected, the MDP stays in the current state $s \in \mathcal{S}$ with probability $\alpha$ and transitions to the other state with probability $1-\alpha$.
We consider $\mathcal{M}$'s CE $\bar{\mathcal{M}}$ with remaining horizon $N=1$,
$\mathcal{G}:=\mathcal{S}$, $\rho :=\mathrm{id}_{\mathcal{S}}$.
The CE's initial distribution is a uniform distribution over states with the original MDP state component $s=0$. Since the initial horizon can be just 1, there are just two initial states in the CE which differ in the goal component: $(0,1,0)$ and $(0,1,1)$.
\end{example}
For detailed plots concerning the bandit example, see figures \ref{fig:bandit}, \ref{fig:banditxu}, \ref{fig:ebandit} and \ref{fig:banditap}.
\begin{figure}
    \begin{subfigure}{0.48\linewidth}
		\includegraphics[width=\linewidth]{exportedsvg/banditpitraj_svg-tex.pdf}
		\caption{Dependency on iteration.}
        \label{fig:banditap:a}
	\end{subfigure}
     \begin{subfigure}{0.48\linewidth}
		\includegraphics[width=\linewidth]{exportedsvg/banditpilimmap_svg-tex.pdf}
		\caption{Dependency on distance to determin.~kernel.}
        \label{fig:banditap:b}
	\end{subfigure}\\    
    \caption{Illustration of behavior of $\min_{\bar{s}\in\theinterestingstates}\pi_n(\oactions(\bar{s})|s)$ for \eUDRL{} recursion in a example~\ref{ex:bandit}. 
    Plot (a) shows the dependency on the iteration $n$ for varying distances to a deterministic kernel highlighted by different colors and varying initial policy. Plot (b) contains the same information as (a) but it is organized to reveal the dependency on $\delta$, where varying numbers of iteration are highlighted by different colors.}
    \label{fig:banditap}
\end{figure}

Next, we include an example featuring a tiny grid world domain.
In this example, we wanted to demonstrate $x^*$ bound on \eUDRL{} policy
accumulation points introduced in section \ref{se:continfty}.
In order to satisfy the assumption $\theinterestingstates \subset \supp \bar{\mu}$ required by this bound, we chosen the uniform initial distribution for the associated CE.
\begin{example}\label{ex:gridworld} (a tiny grid world domain)
The domain is described by a 3 by 3 map (see figure \ref{fig:gridworld:d}).
The gray color depicts a wall. Other positions correspond to the MDP
states. Thus, we have a domain 
$\mathcal{M} = (\mathcal{S},\mathcal{A},\tkernel,\mu,r)$
with 8 states ($|\mathcal{S}| = 8$) and 4 actions 
($\mathcal{A}=\{\text{``right"},\text{\lq\lq{}left\rq\rq{}},\text{\lq\lq{}up\rq\rq{}},\text{\lq\lq{}down\rq\rq{}}\}$).
The transition kernel is parameterized by the parameter $\alpha \in [0,1]$,
regulating the distance from a deterministic kernel (for $\alpha =0$ the kernel
is deterministic and we denote it $\tkernel_0$).
In case of $\alpha = 0$, the kernel transits as indicated by the name of an action if possible. If it is not possible (one would move out of the grid or into a wall), the MDP stays in its current state.
In case of non-deterministic kernels ($\alpha >0$), the exact kernel
depends on \lq\lq{}available states\rq\rq{} which is union (over all actions) of all states the deterministic kernel can move in one step. If given an action $a$
the deterministic kernel would transit to $s'$ and \lq\lq{}available states\rq\rq{}$\setminus \{s'\} \neq \emptyset$, the kernel transits to 
$s'$ with probability $1-\alpha$ and remaining probability $\alpha$
is uniformly distributed between all states in \lq\lq{}available states\rq\rq{}$\setminus \{s'\}$. If \lq\lq{}available states\rq\rq{}$\setminus \{s'\} = \emptyset$, then 
the kernel transits to $s'$ with probability 1.
We consider $\mathcal{M}$'s CE $\bar{\mathcal{M}}$ with the remaining horizon $N=4$,
$\mathcal{G}:=\mathcal{S}$, $\rho :=\mathrm{id}_{\mathcal{S}}$.
The CE's initial distribution is the uniform distribution over all CE states.
\end{example}
The bunch of example accumulation points, together with the lower bound $x^*(\gamma_N)$ on the kernel distance $\delta:=\|\tkernel-\tkernel_0\|_1$,
is plotted in the figure \ref{fig:gridworld:a}. The map
of the grid world domain is shown in figure \ref{fig:gridworld:d}.

Further, we give an example which features the $x_u$ bound on the \eUDRL{}
policy accumulation points introduced in section \ref{se:continfty} together
with ODT recursion. In order to do this, we followed the approach
explained in the background section which assumes fixed horizon scenario with the reward at the end. This approach employs first a state space transformation (which consists of forming a $K$-tuples of states)
and then builds CE on top of the new state space.
The fixed horizon and the optimal policy uniqueness (just on $\theinterestingstates$) requirements are handled by a convenient choice
of the CE initial distribution and the goal map. Finally, we utilized the
fact that ODT recursion becomes just \eUDRL{} recursion (on the
described CE with transformed states) on the restricted segment space $\Seg^{\trail}$.
\begin{example}\label{ex:ODTgridworld} (a tiny grid world domain with ODT recursion)
The underlying MDP is the same as in the previous example \ref{ex:gridworld}
with just the initial distribution having restricted support (we will comment on it more precisely when detailing the associated CE).
Then we form a new MDP by considering $K$-tuples of the underlying 
MDP states with $K=3$ and extend the kernel accordingly.
Over this new MDP, we form the CE. The CE's maximal horizon was set to $N=4$
and the goal space was defined $\mathcal{G} :=\{0,1\}$ with the goal map
mapping $K$-tuples finishing with position $(0,2)$ to $1$ and others
to $0$. We pick just one initial state for this CE corresponding
to position $(2,2)$ with a remaining horizon $4$ and goal $1$ (see the grid world map on \ref{fig:ODTgridworld:d}).
This setting ensured the fixed horizon requirement (horizon was fixed to 4 by the choice of initial state) and the requirement for the uniqueness of the optimal policy on $\theinterestingstates$.
Now, note that the positions in the right column are not included in $\theinterestingstates$. This is because they are not in
$\supp \den_{\tkernel_0,\pi_0}$ ($\pi_0 >0$), i.e., there is no trajectory
which would connect the initial state in position $(2,2)$ and the goal 1
(the position $(0,2)$) of length $N=4$ going through these states.
Therefore, $\theinterestingstates$ spans over $K$-tuples finishing with
the remaining positions. Moreover, since there is only one trajectory
connecting $(2,2)$ with $(0,2)$ in 4 steps, there is only one remaining
horizon available for each position in $\theinterestingstates$.
This leads to the uniqueness of the optimal policy on $\theinterestingstates$.
So we can employ the $x_u$ bound for this domain.
\end{example}
The example's accumulation points of $\min_{\bar{s}\in \theinterestingstates} \pi_{n,\tkernel} (\oactions(\bar{s})|\bar{s})$ of ODT recursion together with the lower bound $x_u(\delta)$ are shown in figure \ref{fig:ODTgridworld:a}.

Finally, we give an example of a grid word which does not fall in the special cases investigated in section~\ref{se:continfty}. On this example we will showcase ODT recursion with regularization (using uniform distribution). This recursion (in the specific setting of the example) can be understood as a special case of $\epsilon$-\eUDRL{} which was investigated in section~\ref{se:regrec}.
\begin{example}\label{ex:eODTgridworld} (a tiny grid world domain with ODT recursion allowing for a non-deterministic optimal policy on $\theinterestingstates$)
This example is exactly the same as the previous example \ref{ex:ODTgridworld} except that we change the initial state
position to $(2,0)$. This change allows for non-determinism of
an optimal policy on $\theinterestingstates$ (see the map in the figure \ref{fig:eODTgridworld:b}). Thus, both
special conditions we have introduced for bounding the accumulation points
of \eUDRL{} (ODT without regularization) recursion fail.
However, we can still bound regularized \eUDRL{} (ODT) recursions.
\end{example}
The example accumulation points of $\min_{\bar{s}\in \theinterestingstates} \pi_{n,\tkernel} (\oactions(\bar{s})|\bar{s})$ of the ODT recursion together with the $\epsilon$-\eUDRL{} based lower bound $\min_M x^*(\gamma_N,\epsilon,\delta)$ are plotted in figure \ref{fig:eODTgridworld:a}.


\section{Continuity at Interior Kernels for a Finite Number of Iterations}
\label{ap:interiorcont}

In this section, we aim to prove continuity of \eUDRL{}-generated policies and related quantities at the interior points of the set of all transition kernels $(\Delta\mathcal{S})^{\mathcal{S}\times\mathcal{A}}$.
Note that an interior point $\tkernel \in ((\Delta\mathcal{S})^{\mathcal{S}\times\mathcal{A}})^{\circ} = ((\Delta\mathcal{S}^{\circ})^{\mathcal{S}\times\mathcal{A}})$ can be equivalently characterized as a transition kernel satisfying $\tkernel > 0$.
The following lemma, which could be understood as the interior point counterpart of the lemma~\ref{le:suppstab},
discusses the stability properties of the set $\supp \den_{\tkernel,\pi}$ subject to changes in the transition kernel $\tkernel>0$ and the policy $\pi$.
\begin{lemma}(support properties)
\label{le:suppprop}
Let $\{\mathcal{M}_{\tkernel} : \tkernel\in (\Delta\mathcal{S})^{\mathcal{S}\times\mathcal{A}} \}$ and $\{\bar{\mathcal{M}}_{\tkernel} : \tkernel\in (\Delta\mathcal{S})^{\mathcal{S}\times\mathcal{A}}\}$ be compatible families of MDPs.
Let $\tkernel, \tkernel' \in (\Delta\mathcal{S})^{\mathcal{S}\times\mathcal{A}}$ be transition kernels with $\tkernel, \tkernel' > 0$ and $\pi,\pi' \in (\Delta \mathcal{A})^{\mathcal{S}}$ be policies. Then it holds that
$$
\supp \den_{\tkernel,\pi} = \supp \den_{\tkernel',\pi'}.
$$
    
\end{lemma}

\begin{proof}
Assume $\bar{s} = (s,h,g) \in \supp \den_{\tkernel,\pi}$. This means
that there exist a trajectory $\tau$ and $0 \leq t \leq N$ with $\prob_{\tkernel,\pi}(\mathcal{T}=\tau) >0$,
$l(\tau) \geq t+h$, $s_t^{\tau} = s$, $\rho(s_{t+h}^{\tau})=g$.
Let us construct a new trajectory $\tau'$ from $\tau$ by altering
actions so that $a_{t'}^{\tau'} \in \supp \pi'(\cdot|\bar{s}_{t'}^{\tau})$
for all $0\leq t'\leq N$. Since $\tkernel'>0$, we also get
$\prob_{\tkernel',\pi'}(\mathcal{T}=\tau') >0$ causing
$\bar{s}  \in \supp \den_{\tkernel',\pi'}$.



\end{proof}
Lemma~\ref{le:suppprop} asserts
that the set $\supp \den_{\tkernel,\pi}$ is constant over the interior of the set of all transition kernels and all policies.
This is in striking difference with the behavior of $\supp \den_{\tkernel,\pi}$ on the neighborhood of a deterministic kernel (or generally a kernel located at the boundary of the set of all transition kernels), as discussed in lemma~\ref{le:suppstab} and example~\ref{ex:detpoint} (or example~\ref{ex:boundarypoint} for boundary points), where on every neighborhood the $\supp \den_{\tkernel,\pi}$ could change abruptly.
In the following lemma we will exploit this property to deliver a much simpler continuity proof than in the case of a deterministic kernel (cf. theorem~\ref{le:detcont}).


\begin{lemma}(Continuity of \eUDRL{} policies and values at interior points)
\label{le:intcont}
Let $\{\mathcal{M}_{\tkernel} : \tkernel\in (\Delta\mathcal{S})^{\mathcal{S}\times\mathcal{A}} \}$ and $\{\bar{\mathcal{M}}_{\tkernel} : \tkernel\in (\Delta\mathcal{S})^{\mathcal{S}\times\mathcal{A}}\}$ be compatible families of MDPs.
Let $(\pi_{n,\tkernel})$, $\pi_0 \in (\Delta \mathcal{A})^{\mathcal{S}}$
be \eUDRL{} generated sequence of policies.
Let $\tkernel_0>0$ be a transition kernel, i.e., $\tkernel_0$ is  an interior point $\tkernel_0 \in ((\Delta \mathcal{S})^{\mathcal{S}\times\mathcal{A}})^{\circ}$.
Then for all $n\geq 0$ it holds that:
\begin{enumerate}
\item
For all $\bar{s} = (s,h,g) \in \bar{\mathcal{S}}_T$ the policy
$\pi_{n+1,\tkernel}(\cdot|\bar{s})$ is continuous in  $\tkernel$ at $\tkernel_0$.
\item
For all $\bar{s} = (s,h,g) \in \bar{\mathcal{S}}_T$
the values $V_{\lambda}^{\pi_n}(\bar{s})$ and $Q_{\lambda}^{\pi_n}(\bar{s},\cdot)$
are continuous in $\tkernel$ at the point $\tkernel_0$.
In addition, the goal reaching objective $J_{\tkernel}^{\pi_n}$ is continuous in $\tkernel$ at the point $\tkernel_0$. (Here we view $V_{\lambda}^{\pi_n}$, $Q_{\lambda}^{\pi_n}$, and $J_{\tkernel}^{\pi_n}$ as functions of a single parameter $\tkernel$ resulting from composition with $\pi_{n,\tkernel}$.)
\end{enumerate}
\end{lemma}
\begin{proof}
Since the interior $((\Delta \mathcal{S})^{\mathcal{S}\times\mathcal{A}})^{\circ}$ is an open set, we can fix $\delta >0$
so that $U_{\delta}(\tkernel_0)$ is contained in the interior.

1.
The proof proceeds by induction on $n$.

\emph{Base case $n=0$:}
$\pi_0$ is continuous in $\tkernel$ as it is constant in $\tkernel$.

\emph{Induction:} Assume the statement holds for $n\geq0$, we aim to prove it for $n+1$.
Let us fix $\bar{s} \in \bar{\mathcal{S}}_T$.
First, assume $\bar{s} \notin \supp \den_{\tkernel_0,\pi_n}$ then from lemma~\ref{le:suppprop} it holds that
$\bar{s} \notin \supp \den_{\tkernel,\pi_n}$ for all 
$\tkernel\in U_{\delta}(\tkernel_0)$.
This in turn means that, as defined by \eref{eq:recursionUsingNumeratorDenominator}, $\pi_{n+1,\tkernel}(\cdot|\bar{s})=\frac{1}{|\mathcal{A}|}$ for all $\tkernel\in U_{\delta}(\tkernel_0)$, and therefore
$\pi_{n+1,\tkernel}(\cdot|\bar{s})$ is continuous in $\tkernel$ at point $\tkernel_0$.
Finally, assume $\bar{s}\in\supp \den_{\tkernel_0,\pi_n}$.
Then it holds that $\bar{s}\in\supp \den_{\tkernel,\pi_n}$ for all $\tkernel\in U_{\delta}(\tkernel_0)$.
This means that $\pi_{n+1,\tkernel}(\cdot|\bar{s})$ is defined as
$$
\pi_{n+1,\tkernel}(a|\bar{s}) = \frac{\num_{\tkernel,\pi_n}(\bar{s},a)}
{\den_{\tkernel,\pi_n}(\bar{s})}
$$
for all $\tkernel \in U_{\delta}(\tkernel_0)$.
From point 4 of lemma~\ref{le:contoptbound}, both $\num_{\tkernel,\pi_n}(\bar{s},\cdot)$ and $\den_{\tkernel,\pi_n}(\bar{s})$ are continuous in $(\tkernel,\pi_n)$ as segment distribution marginals. Further, from the induction assumption, $\pi_{n,\tkernel}(\cdot|\bar{s}')$ is continuous in $\tkernel$ at $\tkernel_0$ for all $\bar{s}' \in \bar{\mathcal{S}}_T$. Therefore both $\num_{\tkernel,\pi_n}(\bar{s},\cdot)$ and $\den_{\tkernel,\pi_n}(\bar{s})$,
when compounded with $\pi_{n,\tkernel}$, are continuous
in $\tkernel$ at $\tkernel_0$.
This together with $\den_{\tkernel_0,\pi_n}(\bar{s}) >0$ implies that $\pi_{n+1,\tkernel}(a|\bar{s})$ is continuous in $\tkernel$ at $\tkernel_0$.


2.
Assume $n>0$. First, we prove the statement about values. Let us fix $\bar{s}\in \bar{\mathcal{S}}_T$.
The values $V_{\tkernel}^{\pi_n}(\bar{s})$ and $Q_{\tkernel}^{\pi_n}(\bar{s},a)$ are both continuous
in $(\tkernel,\pi_n)$ from point 1 of lemma~\ref{le:contoptbound}. Since
$\pi_{n,\tkernel}$ are continuous in $\tkernel$ at $\tkernel_0$ from point 1.\, both $V_{\tkernel}^{\pi_n}(\bar{s})$ and $Q_{\tkernel}^{\pi_n}(\bar{s},a)$, when compounded with $\pi_{n,\tkernel}$,
are continuous in $\tkernel$ at $\tkernel_0$.
Finally, the continuity of the goal-reaching objective
$$
J_{\tkernel}^{\pi_n}
=
\sum_{\bar{s}\in \bar{S}_T}
V_{\tkernel}^{\pi_n}(\bar{s})
\bar{\mu}(\bar{s})
$$
in $\tkernel$ at $\tkernel_0$
follows from the continuity of $V_{\tkernel}^{\pi_n}$.



\end{proof}


\paragraph{When and why discontinuities arise at boundary points:}
Now, when we introduced the proof of continuity of \eUDRL{}-generated
quantities at the interior points (of $\Delta\mathcal{S}^{\mathcal{S}\times\mathcal{A}}$---the set of all transition kernels), we are at the right position
to investigate when this type of proof would break if we replace
an interior point with a boundary point.
In order to discuss general boundary points (not only deterministic ones) we will need the following two generalizations\footnote{Note that these can be omitted if one is interested solely in deterministic points.}. 
First, we need to generalize the notion of $\theinterestingstates$ for general transition kernels $\tkernel_0\in\Delta\mathcal{S}^{\mathcal{S}\times\mathcal{A}}$ (allowing also for non-deterministic $\tkernel_0$) which is trivial. Second, we will need to generalize  lemma~\ref{le:suppstab} to a general kernel $\tkernel_0$.

\begin{lemma}\label{le:suppstabgeneral}(Stability of $\theinterestingstates$) The conclusions of lemma~\ref{le:suppstab} remain valid under following changes: We replace ``a deterministic transition kernel $\tkernel_0$" with ``a transition kernel $\tkernel_0$". We replace quantification in points 1., 2.\ and 3.\ ``for all $n\geq0$ and all $\tkernel\in U_2(\tkernel_0)$" with ``There exists $2> \delta > 0$ such that for all $n\geq0$ and all $\tkernel\in U_{\delta}(\tkernel_0)$".     
\end{lemma}
\begin{proof}
The proof remains the same as in the original lemma~\ref{le:suppstab} except that we have to find a convenient $\delta>0$ for points 1., 2.\ and 3.,
i.e., we have to find $\delta>0$ so that the following variation of \eref{eq:kernelInclusion} holds
$$
(\forall \tkernel \in U_{\delta}(\tkernel_0)) :
\supp \tkernel_0 \subset \supp \tkernel.
$$
But this is just a consequence of the continuity of the identity map $\tkernel \mapsto \tkernel$.
\end{proof}

Now we will follow the proof of lemma~\ref{le:intcont} in the first two iterations of \eUDRL{} while assuming $\tkernel_0$ to be a boundary point and see when it fails.
First, we fix $\delta>0$ so that conclusions of points 1., 2.\ and 3.\ of lemma~\ref{le:suppstabgeneral} hold.

\emph{Iteration $n=0$:}
Here, $\pi_0$ is assumed constant in $\tkernel$ and therefore continuous in $\tkernel$ at $\tkernel_0$. Similarly, from point 1.\ of lemma~\ref{le:contoptbound}, the values $V_{\tkernel}^{\pi_0}(\bar{s})$ and $Q_{\tkernel}^{\pi_0}(\bar{s},a)$ (after composing with $\pi_0$) are continuous in $\tkernel$ at $\tkernel_0$ for all $\bar{s}\in\bar{S}_T$. Finally, the goal-reaching objective $J_{\tkernel}^{\pi_0}$ (cf. \eref{eq:goalreachingobj}) is continuous in  $\tkernel$ at $\tkernel_0$ due to the described continuity of values $V_{\tkernel}^{\pi_0}(\bar{s})$. 

\emph{Iteration $n=1$:}
Let us fix $\bar{s}\in\bar{S}_T$.
First assume $\bar{s}\notin \supp\den_{\tkernel_0,\pi_0}$.
For any neighborhood $U_{\delta'}(\tkernel_0)$, $0<\delta'<\delta$ we can find an interior point $\tkernel' \in U_{\delta'}(\tkernel_0)$, i.e., we can find a sequence of interior points $(\tkernel_{k}')_{k\geq 0}$ converging to $\tkernel_0$. From lemma~\ref{le:suppprop}, only one of the following cases can happen: either $\bar{s}\in \supp\den_{\tkernel_{k}',\pi_0}$ for all $k$ or $\bar{s}\notin \supp\den_{\tkernel_{k}',\pi_0}$ for all $k$.
If the first case is true, $\pi_{1,\tkernel}(\cdot|\bar{s})$ can be discontinuous at $\tkernel_0$ due to discontinuity of $\supp \den_{\tkernel,\pi_0}$ at $\tkernel_0$, as evidenced by the sequence $(\tkernel_{k}')_{k\geq 0}$ (the definition of $\pi_{1,\tkernel}(\cdot|\bar{s})$, cf. \ref{eq:recursionUsingNumeratorDenominator}, changes abruptly from $\num/\den$ ratio to $1/|\mathcal{A}|$).
This behavior is illustrated in example \ref{ex:boundarypoint} (of non-deterministic boundary point $\tkernel_0$) in the appendix (see $\pi_{1,0+}$, $\pi_{1,0}$, $\pi_{1,0+}'$ in the third column ($g=2$)),
and also in example \ref{ex:detpoint} (of deterministic $\tkernel_0$) in the appendix (see again the third column).



In case $\bar{s}\in\supp\den_{\tkernel_0,\pi_0}$,
it holds that $\bar{s}\in\supp\den_{\tkernel_0,\pi_0} \subset \supp\den_{\tkernel,\pi_0}$ for all $\tkernel\in U_{\delta}(\tkernel_0)$ by point 2.\ of the lemma~\ref{le:suppstabgeneral}.
This means that $\pi_{1,\tkernel}(\cdot|\bar{s})$ is defined on the whole $U_{\delta}(\tkernel_0)$, using the ratio $\num_{\tkernel,\pi_0}(\bar{s},a)/\den_{\tkernel,\pi_0}(\bar{s})$. Following the reasoning in point 1.\ of lemma~\ref{le:intcont}, utilizing the continuity of $\pi_0$, point 4.\ of the lemma~\ref{le:contoptbound}, and $\den_{\tkernel_0,\pi_0}(\bar{s})>0$, we conclude that $\pi_{1,\tkernel}(\cdot|\bar{s})$ is continuous in $\tkernel$ at $\tkernel_0$.

\emph{Iteration $n=2$:}
Apart from the discontinuities of $\pi_{2,\tkernel}(\cdot|\bar{s})$ (in $\tkernel$ at $\tkernel_0$) which can arise due to $\bar{s}\notin\supp\den_{\tkernel_0,\pi_1}$, in the same way as was already described for $n=1$, there can be, additionally, also discontinuities for states in the set $\theinterestingstates$. In order to discuss these, let us fix $\bar{s}\in\theinterestingstates$. Since $\theinterestingstates\subset \supp\den_{\tkernel,\pi_1}\cap\supp\nu_{\tkernel,\pi_1}$ (by point 2.\ of lemma~\ref{le:suppstabgeneral}) the policy $\pi_{2,\tkernel}(\cdot|\bar{s})$ is defined by the ratio
$$
\pi_{2,\tkernel}(\cdot|\bar{s})
=
\frac{\num_{\tkernel,\pi_1}(\bar{s},\cdot)}{\den_{\tkernel,\pi_1}(\bar{s})}
$$
for all $\tkernel\in U_{\delta}(\tkernel_0)$.
However, since $\pi_{1,\tkernel}$ can be already discontinuous for some $\bar{s}'\in\bar{S}_T\setminus \supp\den_{\tkernel_0,\pi_0}$ (as we have described in paragraph for $n=1$), this discontinuity can propagate to $\num_{\tkernel,\pi_1}(\bar{s},\cdot)$, $\den_{\tkernel,\pi_1}(\bar{s})$ 
causing a discontinuity of $\pi_{2,\tkernel}(\cdot|\bar{s})$
defined via the above ratio.

This behavior is illustrated in example \ref{ex:boundarypoint} in the appendix (see $\pi_{2,0+}$, $\pi_{2,0}$, $\pi_{2,0+}'$ first column ($g=0$)),
and also in example \ref{ex:detpoint} in the appendix (see the second column and figure \ref{fig:discont:a}).


\emph{Summary:}
To sum up, policies can become discontinuous already at
iteration $n=1$, but this occurs outside of $\theinterestingstates$. At iteration $n=2$, we start also observing discontinuities on $\theinterestingstates$,
which is more problematic.
While at deterministic kernels the policies
on $\theinterestingstates$ are relatively continuous,
causing the goal-reaching objective to be continuous
(cf. theorem~\ref{le:detcont} and corollary~\ref{le:detJcont}). In
non-deterministic boundary points this is not the case
and policy discontinuities (for $n\geq 2$) propagates through values
to goal-reaching objective (cf. figure \ref{fig:discont:d}).





\section{Regularized Recursion --- Lemmas and Proofs}
\label{ap:regrec}

Here we introduce and prove the various lemmas and theorems used in section \ref{se:regrec}.

\subsection{Preliminary Lemmata}
The following lemma is the $\epsilon$-\eUDRL{} version of lemma \ref{le:suppstab}.

\begin{lemma}\label{le:esuppstab} ($\epsilon$-\eUDRL{} version of support stability)
Let $\{\mathcal{M}_{\tkernel} : \tkernel \in (\Delta \mathcal{S})^{\mathcal{S}\times\mathcal{A}}\}$
and $\{\bar{\mathcal{M}}_{\tkernel} : \tkernel \in (\Delta \mathcal{S})^{\mathcal{S}\times\mathcal{A}}\}$ be compatible families.
Let $(\pi_{n,\tkernel,\epsilon})_{n\geq 0}$ be a sequence of 
policies generated by the $\epsilon$-\eUDRL{} iteration given a transition kernel $\tkernel\in (\Delta S)^{\mathcal{S}\times\mathcal{A}}$ and  an initial condition $\pi_0$ (that does not depend on $\tkernel$) and a regularization parameter $1> \epsilon >0$.
Fix a deterministic transition kernel $\tkernel_0 \in \Delta \mathcal{S}^{\mathcal{S}\times\mathcal{A}}$.
Then for all initial conditions $\pi_0 > 0$ it
holds:
\begin{enumerate}
    \item For all $n\geq 0$ and all $\tkernel \in U_{\delta}(\tkernel_0)$ we have that $\supp \num_{\tkernel_0,\pi_0} \cap ( \mathcal{A} \times \supp \nu_{\tkernel_0,\pi_0} ) \subset \supp \num_{\tkernel,\pi_{n,\epsilon}} \cap ( \mathcal{A} \times \supp \nu_{\tkernel,\pi_{n,\epsilon}} )$,
    where the inclusion becomes equality for $\tkernel = \tkernel_0$.
    \item For all $n\geq 0$ and all $\tkernel \in U_{\delta}(\tkernel_0)$ we have that $\theinterestingstates \subset \supp \den_{\tkernel,\pi_{n,\epsilon}} \cap \supp \nu_{\tkernel,\pi_{n,\epsilon}}$,
    where the inclusion becomes an equality for $\tkernel = \tkernel_0$.
    \item For all $n\geq 0$ and all $\tkernel \in U_{\delta}(\tkernel_0)$ we have that $\prob_\tkernel (\stag{S}_0=s, l(\Sigma)=h, \rho(\stag{S}_h)=g, \stag{H}_0=h, \stag{G}_0=g; \pi_{n,\epsilon} ) > 0$ for all $(s,h,g)\in \theinterestingstates$.   
\end{enumerate}
The points 4. and 5. of the original lemma \ref{le:suppstab} hold also for $\epsilon$-\eUDRL{}.
\end{lemma}

\begin{proof}
The equation \eref{eq:kernelInclusion} is proved exactly the same as in the proof of the original lemma \ref{le:suppstab}.

1.
There is an easier alternative proof
for this point which we present now.
Since $\pi_{n,\epsilon} > 0$ for $n\geq 0$
(because of the regularization and the assumption that $\pi_0 > 0$) we
trivially have $\supp \pi_{n,\epsilon} = \supp \pi_0$.
Now fix $\tkernel \in U_{2}(\tkernel_0)$.
From $\supp \tkernel_0 \subset \supp \tkernel$ (cf. equation \eref{eq:kernelInclusion}) and just
stated fact about policy supports it follows:
\begin{equation}
\supp \nu_{\tkernel_0,\pi_0} \subset
\supp \nu_{\tkernel,\pi_{n,\epsilon}},
\quad
\supp \num_{\tkernel_0,\pi_0} \subset
\supp \num_{\tkernel,\pi_{n,\epsilon}}.
\end{equation}
Now we take the first inclusion and perform the cartesian product
with $\mathcal{A}$ and intersection with $\supp \num_{\tkernel_0,\pi_0}$. Further we take the second inclusion
and perform intersection with $\supp \nu_{\tkernel,\pi_{n,\epsilon}} \times \mathcal{A}$.
We obtain the following chain of inclusions:
\begin{align*}
( \supp \nu_{\tkernel_0,\pi_0}  \times \mathcal{A} )
\cap \supp \num_{\tkernel_0,\pi_0}
&\subset
(\supp \nu_{\tkernel,\pi_{n,\epsilon}}  \times \mathcal{A} )
\cap \supp \num_{\tkernel_0,\pi_0}
\\
&\subset
(\supp \nu_{\tkernel,\pi_{n,\epsilon}}  \times \mathcal{A} )
\cap \supp \num_{\tkernel,\pi_{n,\epsilon}},
\end{align*}
which leaves the result. The equality is proved exactly the same
as in the original proof.

2. The proof is unchanged.

3. The proof can be simplified but otherwise repeated with minimal changes as the new version of \eref{eq:taupositive} trivially holds (since $\pi_{n,\epsilon}>0$ for all $n \geq 0$).

The remaining proof of 4. and 5. is unchanged.
\end{proof}


The following lemma is the $\epsilon$-\eUDRL{} version of the lemma \ref{le:detopt}.
\begin{lemma}\label{le:edetopt}(optimality of $\epsilon$-\eUDRL{} policies for deterministic transition kernel)
Let $\tkernel_0$ be
a deterministic transition kernel and let $\mathcal{M}=(\mathcal{S},\mathcal{A},\tkernel_0,\mu,r)$ be a respective MDP with CE $\bar{\mathcal{M}}=(\bar{\mathcal{S}},\mathcal{A},\bar{\tkernel}_0,\bar{\mu},\bar{r},\rho)$. 
Assume $\pi_0 > 0$ and let $(\pi_{n,\epsilon})_{n\geq 0}$ be the policy sequence generated by the $\epsilon$-\eUDRL{} iteration given $\pi_0$, $\tkernel_0$ and regularisation parameter $\epsilon$. Then it holds:
\begin{enumerate}
\item For all $n \geq 0$ the policy $\pi_{n+1,\epsilon}$ has the form (on $\theinterestingstates$) 
$$
\pi_{n+1,\epsilon} = (1-\epsilon) \pi_{n+1}^* + \frac{\epsilon}{|\mathcal{A}|},
$$
where 
$$
\pi_{n+1}^* = \frac{\num_{\tkernel_0,\pi_{n,\epsilon}}(a,s,h,g)}{\den_{\tkernel_0,\pi_{n,\epsilon}}(s,h,g)}
$$
is an optimal policy on $\theinterestingstates$.
\item For all $n\geq 1$ and all $\bar{s}=(s,h,g)\in \theinterestingstates$ it holds
$$
V^{\pi_{n,\epsilon}}(\bar{s}) \geq (1-\epsilon)^h \geq (1-\epsilon)^N.
$$
\item For all $n\geq 1$ and all $\bar{s}=(s,h,g)\in \theinterestingstates$ it holds
\begin{align*}
Q^{\pi_{n,\epsilon}}(\bar{s},a) &\geq (1-\epsilon)^{h-1} \geq (1-\epsilon)^N \quad \text{for} \:  a \in \oactions(\bar{s}),
\\
&= 0 \quad \text{otherwise.} 
\end{align*}
\end{enumerate}
\end{lemma}

\begin{proof}
1.
The equations is just a rewrite of
the $\epsilon$-\eUDRL{} recursion.
The easiest way to prove
optimality of $\pi_{n}^*$ is to realize that
$\pi_{n}^*$ is defined similarly as $\pi_n$ (in the original lemma)
except that it uses $\epsilon$-version of \lq\lq{}$\num/\den$\rq\rq{}-ratio.
Since the proof of point 1. (in the original lemma) is
essentially just a statement about the supports and these
do not change (due to points 1. and 2. of lemma \ref{le:suppstab} and points 1. and 2. of
lemma \ref{le:esuppstab}), i.e.,

\begin{align*}
( \supp \nu_{\tkernel_0,\pi_n}  \times \mathcal{A} )
\cap \supp \num_{\tkernel_0,\pi_n}
&=
( \supp \nu_{\tkernel_0,\pi_0}  \times \mathcal{A} )
\cap \supp \num_{\tkernel_0,\pi_0}
\\
&=
( \supp \nu_{\tkernel_0,\pi_{n,\epsilon}}  \times \mathcal{A} )
\cap \supp \num_{\tkernel_0,\pi_{n,\epsilon}},
\\
\supp \den_{\tkernel_0,\pi_n}
\cap \supp \nu_{\tkernel_0,\pi_n}
&=
\theinterestingstates
=
\supp \den_{\tkernel_0,\pi_{n,\epsilon}}
\cap \supp \nu_{\tkernel_0,\pi_{n,\epsilon}}
\end{align*}
the $\pi_n^*$ has to be optimal too.

2.\ \& 3.
The proof follows by induction on the remaining horizon $h$.
For $h=1$ the $Q$-values are independent of a policy
and therefore are optimal, i.e., for all 
$\bar{s} = (s,1,g) \in \theinterestingstates$, it holds that
$Q^{\pi_{n,\epsilon}}((s,1,g),a) = 1$ for $a\in \oactions(s,1,g)$
and $Q^{\pi_{n,\epsilon}}((s,1,g),a) = 0$ otherwise.
Now assume that 2. and 3. holds for a fixed horizon $h$.
We obtain (for all $\bar{s} = (s,h,g) \in \theinterestingstates$)
\begin{align*}
V^{\pi_{n,\epsilon}}(\bar{s})
&=
\sum_{a\in \mathcal{A}} \pi_{n,\epsilon}(a|\bar{s})
Q^{\pi_{n,\epsilon}}(\bar{s},a)
\\
&\geq
\sum_{a\in \oactions(\bar{s})} \pi_{n,\epsilon}(a|\bar{s})
(1-\epsilon)^{h-1}
=
(1-\epsilon)^{h-1} \pi_{n,\epsilon}(\oactions(\bar{s})|\bar{s})
\\
&=
(1-\epsilon)^{h-1} ((1-\epsilon) \pi_n^*(\oactions(\bar{s})|\bar{s})
+\frac{\epsilon |\oactions(\bar{s})|}{|\mathcal{A}|})
=
(1-\epsilon)^{h-1} (1-\epsilon(1-\frac{|\oactions(\bar{s})|}{|\mathcal{A}|}) )
\\
&\geq
(1-\epsilon)^{h},
\end{align*}
where we used the induction assumption and the point 1.
Further for all $\bar{s} = (s,h+1,g) \in \theinterestingstates$ it holds
\begin{align*}
Q^{\pi_{n,\epsilon}}(\bar{s},a)
&=
\sum_{s'\in \mathcal{S}} \tkernel_0(s'|s,a)
V^{\pi_{n,\epsilon}}(s',h,g)
=
\tkernel_0(s''|s,a)
V^{\pi_{n,\epsilon}}(s'',h,g) \geq (1-\epsilon)^{h} \quad \text{for}\; a \in \oactions(\bar{s}),
\\
&= 0 \quad \text{otherwise,}
\end{align*}
where for $a\in \oactions(\bar{s})$ we used that the kernel
is deterministic and therefore there exists a $s'' \in \mathcal{S}$
such that $\tkernel_0(s''|s,a)=1$ and, further, that from point 4.\ of lemma
\ref{le:esuppstab}, $(s'',h,g) \in \theinterestingstates$.
The statement for $a\notin \oactions(\bar{s})$ is just point 5.\ of lemma \ref{le:esuppstab}.
This completes the induction.
\end{proof}

The class of policies $\Pi_{\tkernel_0,\epsilon}^* := \{(1-\epsilon) \pi_{\tkernel_0}^* + \frac{\epsilon}{|\mathcal{A}|} \mid \pi_{\tkernel_0}^*\; \text{is an optimal policy for}\; \tkernel_0 \;\text{on}\; \theinterestingstates \}$
which was discussed in the previous lemma 
will be of great importance in the
following text because (given $\bar{s}\in \theinterestingstates$) it constitutes
the limit relative to $\oactions(\bar{s})$ we would like to achieve when proving
relative continuity of $\epsilon$-\eUDRL{} policies in $\tkernel$ at $\tkernel_0$.
Unlike the optimal action-value function
$Q_{\tkernel_0}^*$, which is constant on 
$\oactions(\bar{s})$ (and therefore factors through a quotient map relative to $\oactions(\bar{s})$), there are many $Q_{\tkernel_0}^{\pi_{\epsilon}^*}$ depending on $\pi_{\tkernel_0,\epsilon}^*\in \Pi_{\tkernel_0,\epsilon}^*$ and which are generally non-constant on $\oactions(\bar{s})$.
Note that during the proof of points 2.\ and 3.\ of the above lemma 
we exclusively used policy properties following from $\pi_{n,\epsilon} \in \Pi_{\tkernel_0,\epsilon}^*$ for $n>0$. Therefore,
the points 2.\ and 3.\ describe
lower bounds on the values $Q_{\tkernel_0}^{\pi_{\epsilon}^*}$ and $V_{\tkernel_0}^{\pi_{\epsilon}^*}$, respectively, where $\pi_{\tkernel_0,\epsilon}^* \in \Pi_{\tkernel_0,\epsilon}^*$.



The following lemma is the $\epsilon$-\eUDRL{} version of the lemma \ref{le:alpha}.
\begin{lemma}
\label{le:ealpha} ($\epsilon$-\eUDRL{} version of the lower bound on visitation probabilities)
Let 
$\{\mathcal{M}_{\tkernel} : \tkernel \in (\Delta \mathcal{S})^{\mathcal{S}\times\mathcal{A}}\}$
and $\{\bar{\mathcal{M}}_{\tkernel} : \tkernel \in (\Delta \mathcal{S})^{\mathcal{S}\times\mathcal{A}}\}$ be compatible families.
Let $\tkernel_0$ be a deterministic kernel. Let $(\pi_{n,\tkernel,\epsilon})_{n\geq 0}$ denotes $\epsilon$-\eUDRL{} generated sequence given
the initial condition $\pi_0>0$, the transition kernel $\tkernel$, and the regularization parameter $\epsilon$.
Then for all $n>0$ all $\bar{s} = (s,h,g)\in\theinterestingstates$ and all $\tkernel\in U_2(\tkernel_0)$ it holds
$$
\prob_{\tkernel}(\stag{H}_0=h, \stag{G}_0=g | \stag{S}_0=s, l(\Sigma)=h; \pi_{n,\epsilon}) \geq \alpha(\delta,\epsilon),
$$
where
$$
\alpha(\delta,\epsilon) = 
\frac{2}{N(N+1)}
(\min_{\bar{s}'\in \supp \bar{\mu}}\bar{\mu}(\bar{s}'))
(\frac{\epsilon}{|\mathcal{A}|})^N
(1-\frac{\delta}{2})^N
> 0.
$$
Notice that
$
\alpha(\delta,\epsilon) \rightarrow \alpha(0,\epsilon_0)$ as $(\delta,\epsilon) \rightarrow (0,\epsilon_0)$,
where we assume an $\epsilon_0 \geq 0$, and where
$$
\alpha(0,\epsilon_0) = 
\frac{2}{N(N+1)}
(\min_{\bar{s}'\in \supp \bar{\mu}}\bar{\mu}(\bar{s}'))
\frac{1}{|\mathcal{A}|^N}\epsilon_0^N > 0
$$
for $\epsilon_0 > 0$.
\end{lemma}

\begin{proof}
We can bound the $\epsilon$-\eUDRL{} policy
$$
(\forall n >0, \bar{s} \in \theinterestingstates):
\pi_{n,\tkernel,\epsilon}(\cdot|\bar{s}) \geq \frac{\epsilon}{|\mathcal{A}|} =: \alpha' >0.
$$
Further, we continue similarly as in theorem \ref{le:limdetcontM1} \lq\lq{}Bounding the visitation term\rq\rq{} except we utilize the
above $\alpha'$ bound for lower bounding the policy terms.
In short, from $\bar{s}\in\theinterestingstates$ we have $\bar{s}\in \supp \nu_{\tkernel_0,\pi_0}$ meaning there exists a prefix
$\bar{s}_0,a_0,\ldots,\bar{s}_t = \bar{s}$ with positive probability
$\prob_{\tkernel_0}(\bar{S}_t=\bar{s},\ldots,A_0 = a_0,\bar{S}_0=\bar{s}_0;\pi_0)>0$.
Rewriting this probability as a product of $\tkernel_0$ terms and $\pi_0$ terms
then replacing the $\tkernel_0$ with $\tkernel$ and $\pi_0$
with $\pi_{n,\tkernel,\epsilon}$ doing exactly the same bounding of $\tkernel$
terms as in theorem \ref{le:limdetcontM1} and bounding $\pi_{n,\tkernel,\epsilon}$ using the $\alpha'$ bound above we obtain
$$
\prob_{\tkernel}(\bar{S}_t=\bar{s};\pi_{n,\tkernel,\epsilon})
\geq
\left(\min_{\bar{s}'\in \supp\bar{\mu}} \bar{\mu}(\bar{s}')\right)
(\alpha')^N
(1-\frac{\delta}{2})^N
$$
the rest of the proof is the same as in theorem \ref{le:limdetcontM1}
leading to the $\alpha$ bound above.
\end{proof}
Note that the bound could be easily extended to $n\geq 0$ by putting
$$
\alpha':= \min\{ \min_{\bar{s}'\in \theinterestingstates, a \in \mathcal{A}} \pi_0(a|\bar{s}'), \frac{\epsilon}{|\mathcal{A}|} \}.
$$
Although we will suffice with the above simpler version.

We will call a policy $\pi$ \emph{$\epsilon$-regular}
if and only if, for all $\bar{s} \in \theinterestingstates$ and $a \in \mathcal{A}$, it holds that $\pi(a|\bar{s}) > \frac{\epsilon}{|\mathcal{A}|}$.
The following remark describes equivalent statements about the distance of an $\epsilon$-regular policy to $\Pi_{\tkernel_0,\epsilon}^*$.

\begin{remark}
\label{re:epsreg}
(distance of $\epsilon$-regular policy to $\Pi_{\tkernel_0,\epsilon}^*$)
Assume $\delta > 0$ and $\pi_{\epsilon}$ is an
$\epsilon$-regular policy, then the following statements are equivalent:
\begin{description}
    \item[(a)] 
    $2(1-\epsilon(1-\frac{|\oactions(\bar{s})|}{|\mathcal{A}|})-\pi_{\epsilon}(\oactions(\bar{s}')|\bar{s}')) < \delta$
    \item[(b)] 
$
(\exists \pi_{\epsilon}^{*} \in \Pi_{\tkernel_0,\epsilon}^{*} ):
\quad
\|\pi_{\epsilon}^* - \pi_{\epsilon}\|_1 < \delta
\quad
\wedge
\quad
(
(\forall \bar{s}\in \theinterestingstates, \forall a \in \oactions(\bar{s})):
\pi_{\epsilon}^{*}(a|\bar{s}) \geq \pi_{\epsilon}(a|\bar{s})
)
$
    \item[(c)] 
$
(\exists \tilde{\pi}_{\epsilon}^{*} \in \Pi_{\tkernel_0,\epsilon}^{*} ):
\quad
\|\tilde{\pi}_{\epsilon}^* - \pi_{\epsilon}\|_1 < \delta
$
\end{description}
\end{remark}
Note that $\epsilon$-\eUDRL{} generated policies are $\epsilon$-regular, as well as policies from $\Pi_{\tkernel_0,\epsilon}^*$.
A policy from $\Pi_{\tkernel_0,\epsilon}^*$ can be characterized by
being $\epsilon$-regular and putting maximum mass on $\oactions(\bar{s})$
for all $\bar{s}\in \theinterestingstates$, where this 
maximum is $1-\epsilon(1-\frac{|\oactions(\bar{s})|}{|\mathcal{A}|})$.
\begin{proof} (of the remark \ref{re:epsreg})
We begin by the proof of 
(c) $\implies$ (b).
Assume (c).
Since $\pi_{\epsilon}$ might not be in $\Pi_{\tkernel_0,\epsilon}^*$,
we have $\pi_{\epsilon}(\oactions(\bar{s})|\bar{s}) \leq 1-\epsilon(1-\frac{|\oactions(\bar{s})|}{|\mathcal{A}|}) \;( =  \tilde{\pi}_{\epsilon}^*(\oactions(\bar{s})|\bar{s}))$.
We can easily construct $\pi_{\epsilon}^* \in \Pi_{\tkernel_0,\epsilon}^*$ satisfying the second condition in (b), e.g., by
$$
\pi_{\epsilon}^*(a|\bar{s}) =
\begin{cases}
\pi_{\epsilon}(a|\bar{s}) 
+
(
1-\epsilon(1-\frac{|\oactions(\bar{s})|}{|\mathcal{A}|})
-
\pi_{\epsilon}(\oactions(\bar{s})|\bar{s}) 
)/|\oactions(\bar{s})|
\quad \text{for}\; a \in \oactions(\bar{s})
\\
\frac{\epsilon}{|\mathcal{A}|}
\quad \text{otherwise}.
\end{cases}
$$
Since $\tilde{\pi}_{\epsilon}^*$ might not satisfy the
second condition in (b), we have $\delta > \|\tilde{\pi}_{\epsilon}^*-\pi_{\epsilon}\|_1 \geq \|\pi_{\epsilon}^*-\pi_{\epsilon}\|_1$. This concludes the proof of (c) $\implies$ (b).

The implication (b) $\implies$ (a) is trivial:
\begin{align*}
\delta > \|\pi_{\epsilon}^*-\pi_{\epsilon}\|_1
&= 
\pi_{\epsilon}^*(\oactions(\bar{s})|\bar{s})
- 
\pi_{\epsilon}(\oactions(\bar{s})|\bar{s})
+
\pi_{\epsilon}(\mathcal{A}\setminus \oactions(\bar{s})|\bar{s})
-
\pi_{\epsilon}^*(\mathcal{A}\setminus \oactions(\bar{s})|\bar{s}) 
\\
&=
2(\pi_{\epsilon}^*(\oactions(\bar{s})|\bar{s})
- 
\pi_{\epsilon}(\oactions(\bar{s})|\bar{s}))
\\
&=
2(
1-\epsilon(1-\frac{|\oactions(\bar{s})|}{|\mathcal{A}|})
- 
\pi_{\epsilon}(\oactions(\bar{s})|\bar{s}))
),
\end{align*}
where we used $\pi_{\epsilon}^*(\mathcal{A}|\bar{s}) = \pi_{\epsilon}(\mathcal{A}|\bar{s}) = 1$.

The implication (a) $\implies$ (c) is proved as follows.
Assuming (a) we can construct $\tilde{\pi}_{\epsilon}^* \in \Pi_{\tkernel_0,\epsilon}^*$
in the same way as we were constructing $\pi_{\epsilon}^*$
in the proof of (c) $\implies$ (b).
\end{proof}


In the following lemma, we will aim to show that when $\pi_{\epsilon}$ (we will consider just $\epsilon$-regularized policies) is close to $\Pi_{\tkernel_0,\epsilon}^*$ and 
$\tkernel$ is close to $\tkernel_0$, then $Q_{\tkernel}^{\pi_{\epsilon}}$ is close to the set $\{Q_{\tkernel_0}^{\pi_{\epsilon}^*} \mid \pi_{\tkernel_0,\epsilon}^* \in  \Pi_{\tkernel_0,\epsilon}^* \}$.
The following lemma is the $\epsilon$-\eUDRL{} version of the lemma \ref{le:contQfac}.
\begin{lemma}
\label{le:econtQfac}
($\epsilon$-\eUDRL{} version of the continuity of action-values in quotient topology)
Let $\{\mathcal{M}_{\tkernel} : \tkernel \in (\Delta \mathcal{S})^{\mathcal{S}\times\mathcal{A}}\}$
and $\{\bar{\mathcal{M}}_{\tkernel} : \tkernel \in (\Delta \mathcal{S})^{\mathcal{S}\times\mathcal{A}}\}$ be compatible families.
Let $\tkernel_0$ be a deterministic kernel.
For all $\epsilon' >0$ there exists $\delta >0$ such that if $\tkernel\in U_{\delta}(\tkernel_0)$ and the $\epsilon$-regular policy $\pi_\epsilon$ satisfies,
for 
all $\bar{s}=(s,h,g)\in \theinterestingstates$,
$$
2(1-\epsilon(1-\frac{|\oactions(\bar{s})|}{|\mathcal{A}|})-\pi_{\epsilon}(\oactions(\bar{s})|\bar{s})) < \delta,
$$
then
$|Q^{\pi_\epsilon}_{\tkernel}(\bar{s},\cdot) - Q^{\pi_{\epsilon}^*}_{\tkernel_0}(\bar{s},\cdot)| < \epsilon'$ for all $\bar{s}=(s,h,g)\in \theinterestingstates$ for some $\pi_{\tkernel_0,\epsilon}^* \in \Pi_{\tkernel_0,\epsilon}^*$.
The statement can be made explicit by a recursive estimate:
for all $\bar{s} = (s,h,g) \in \theinterestingstates$, all $h\geq 2$, and all $a\in \oactions(\bar{s})$ it holds that
\begin{align*}
|Q_{\tkernel}^{\pi_{\epsilon}}(\bar{s},a)-Q_{\tkernel_0}^{\pi_{\epsilon}^*}(\bar{s},a) |
&\leq
\|\tkernel(\cdot|s,a)-\tkernel_0(\cdot|s,a)\|_1
\\
&\quad+
\max_{\bar{s}'=(s',h-1,g) \in\theinterestingstates}
2(1-\epsilon(1-\frac{|\oactions(\bar{s}')|}{|\mathcal{A}|})-\pi_{\epsilon}(\oactions(\bar{s}')|\bar{s}'))
\\
&\quad+
\max_{\bar{s}'=(s',h-1,g) \in\theinterestingstates, a' \in \oactions(\bar{s}')}
|Q_{\tkernel}^{\pi_{\epsilon}}(\bar{s}',a')-Q_{\tkernel_0}^{\pi_{\epsilon}^*}(\bar{s}',a')|.
\end{align*}
\end{lemma}

\begin{proof}
The proof follows as in lemma \ref{le:contQfac}
by induction on the remaining horizon.
In the induction step, we simply show that $|Q_{\tkernel}^{\pi_{\epsilon}}(\bar{s},a)-Q_{\tkernel_0}^{\pi_{\epsilon}^*}(\bar{s},a) | \rightarrow 0$
for $\delta \rightarrow 0$ by bounding it by $\delta$
and using the induction assumption. The trick is to use
the implication (a) $\implies$ (b) of the remark \ref{re:epsreg} to select a
convenient $\pi_{\epsilon}^*$ for a given $\pi_{\epsilon}$.
\end{proof}

The following lemma is the $\epsilon$-\eUDRL{} version of lemma \ref{le:f}.
\begin{lemma}
\label{le:z}
Let $\gamma,\epsilon > 0$, $1 > \gamma +\epsilon$, $|\mathcal{A}| \geq M > 0$ and $z_{\gamma,\epsilon,M}:[0,1] \rightarrow [0,1]$ be defined as
$$
z_{\gamma,\epsilon,M}(x) =
(1-\epsilon)\frac{x}
{x + \gamma}
+ \epsilon \frac{M}{|\mathcal{A}|}.
$$
Then the following assertions hold
\begin{description}
\item{1.} $z_{\gamma,\epsilon,M}$ is increasing.
\item{2.} $z_{\gamma,\epsilon,M}$ has a unique fixed point $x^*(\gamma,\epsilon,M)$
$$
x^*(\gamma,\epsilon,M) = \frac{\hat{x}^* + \sqrt{(\hat{x}^*)^2+\frac{4\gamma\epsilon M}{|\mathcal{A}|}}}{2},
$$
where $\hat{x}^* = 1-\epsilon(1-\frac{M}{\mathcal{|A|}})-\gamma$, $\hat{x}^* \leq x^*(\gamma,\epsilon,M)$ and
$1> x^*(\gamma,\epsilon,M) >0$. Further,
$$
x^*(\gamma,\epsilon,M)
\rightarrow
x^*(0,\epsilon_0,M) =  1-\epsilon_0(1-\frac{M}{|\mathcal{A}|}) \quad\text{as}\quad
(\gamma,\epsilon)  \rightarrow (0,\epsilon_0),
$$
for any  $1>\epsilon_0 \geq 0$, and it holds that
$$
\begin{aligned}
z_{\gamma,\epsilon,M}(x) &> x \quad \text{for}\; x < x^*(\gamma,\epsilon,M),
\\
z_{\gamma,\epsilon,M}(x) &< x \quad \text{for}\; x > x^*(\gamma,\epsilon,M).
\end{aligned}
$$
\item{3.} For all $x\in [0,1]$ the iterated application of $z_{\gamma,\epsilon,M}$ converges pointwise, $z_{\gamma,\epsilon,M}^{\circ n} (x) \xrightarrow{n\rightarrow\infty} x^*(\gamma,\epsilon,M)$.
\item{4.} Assume a sequence $(y_n)$,$y_n \in [0,1]$ such that for all $n\geq 0$ we have 
$y_{n+1} \geq z_{\gamma,\epsilon,M}(y_n)$.
Then $y_n \geq z_{\gamma,\epsilon,M}^{\circ n}(y_0)$ and $\liminf_n y_n \geq x^*(\gamma,\epsilon,M)$.

\end{description}
\end{lemma}

\begin{proof}
1. The derivative of $z$ is positive.

2. We follow the same approach as in lemma \ref{le:f} except the resulting
equality/inequality is now quadratic (instead of linear). However there is just one
root in $[0,1]$ (the other one is always negative) which is also the
only fixed point $x^*(\gamma,\epsilon,M)$.
From $\frac{4\gamma\epsilon M}{|\mathcal{A}|} >0$ follows the inequality 
$\hat{x}^* \leq x^*$.
To prove the bounds $1>x^*(\gamma,\epsilon,M)>0$ is trivial. 

3.\ \&\ 4. The rest follows as in lemma \ref{le:f} except it is somewhat more simplified
as there are no complications with the point $x=0$.
\end{proof}

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{exportedsvg/dynz_svg-tex.pdf}
    \caption{The map $z_{\gamma,\epsilon,M}$ as a dynamical system:
    the dependence of $z_{\gamma,\epsilon,M}^{\circ n}(x)$ (repeated application of $z_{\gamma,\epsilon,M}$ given initial condition $x$) on iteration $n$ for various
    initial conditions $x$. The parameters are set with 
    $|\mathcal{A}| = 4$, $M = 1$, and $\gamma = 0.4$. The regularization parameter is set to  $\epsilon = 0.1$ for black trajectories
    and to $\epsilon = 0.3$ for green trajectories, respectively.    
}
    \label{fig:z}
\end{figure}
The figure \ref{fig:z} illustrates the dynamical system induced by $z_{\gamma,\epsilon,M}$.
The lemma shows some of its properties (e.g., the fixed point). 

\subsection{The Main Theorem}
The following theorem is the $\epsilon$-\eUDRL{} version of theorem \ref{le:limdetcont}.
\elimdetcont*
At the core of the proof is the analysis of dynamical systems and convergence
induced by iterative application of the rational function $z_{\gamma,\epsilon,M}(x) = (1-\epsilon)\frac{x}{x+\gamma} +\epsilon\frac{M}{|\mathcal{A}|}$ (see lemma \ref{le:z}). Indeed, we will show that for all horizons $N \geq h \geq 1$ and all $\beta,\tilde{\beta},\epsilon \in (0,1)$ satisfying $\gamma+\epsilon<1$ (with $\gamma$ as in point 2. above), 
there exists $n_0$ and $\delta >0$ such that, for all initial conditions
$\pi_0>0$
all $\tkernel \in U_{\delta}(\tkernel_0)$, $n>n_1\geq n_0$,
and all $\bar{s}\in\theinterestingstates$ with remaining horizon $h$, it holds that
\begin{equation}
\pi_{n,\tkernel,\epsilon}(\oactions(\bar{s})|\bar{s}) \geq
z_{\gamma,\epsilon,|\oactions(\bar{s})|}^{\circ (n-n_1)} (\frac{\epsilon|\oactions(\bar{s})|}{|\mathcal{A}|})
.
\label{eq:eforMainTheorem}
\end{equation}
Then, by lemma \ref{le:z}, the right-hand side converges to $x^*(\gamma,\epsilon,|\oactions(\bar{s})|)$, which implies point~2.\ of the theorem (see the proof for details). Furthermore, point~1.\ is a consequence of point~2.\ The argument for this is the same as in section \ref{ssse:specmutheorem}.
\begin{proof}
The proof is analogous to the proof of theorem \ref{le:limdetcont} thus we will concentrate on
discussing the small differences (of course, with the lemmas introduced in this appendix instead of their original versions).

Since the lower bound $\pi_{n,\epsilon}(\oactions(\bar{s})) \geq \frac{\epsilon|\oactions(\bar{s})|}{|\mathcal{A}|}\geq \frac{\epsilon}{|\mathcal{A}|} > 0$ (for all $\bar{s}\in \theinterestingstates$) coming from $\epsilon$-regularity of $\epsilon$-\eUDRL{}-generated policies is used in \eref{eq:eforMainTheorem}, we do not need to prove any analogies to \eref{eq:pisep}.

In point 1., $(\beta,\tilde{\beta},\alpha(\delta,\epsilon),\epsilon) \rightarrow (0,0,\alpha(0,\epsilon_0),\epsilon_0)$
(where $\epsilon_0 >0$ and $\alpha(0,\epsilon_0) >0$ from lemma \ref{le:z})
implies $\gamma \rightarrow 0$ (continuity of $\gamma$ at point $(0,0,\alpha(0,\epsilon_0),\epsilon_0)$). Further,
$(\gamma,\epsilon) \rightarrow (0,\epsilon_0)$ causes
$x^* \rightarrow 1-\epsilon_0(1-\frac{|\oactions(\bar{s})|}{|\mathcal{A}|})$, according to lemma \ref{le:z}.

The proof proceeds by showing implications 2.$\implies$1. and \eref{eq:eforMainTheorem}$\implies$2. similarly as in the original proof.
Here we will focus on the proof of \eref{eq:eforMainTheorem}.
Fixing $2>\delta>0$ and $\epsilon >0$,
 lemma \ref{le:esuppstab} asserts that $\theinterestingstates \subset \supp \den_{\tkernel,\pi_{n,\epsilon}} \cap \supp \nu_{\tkernel,\pi_n}$
for all $n\geq 0$,$\pi_0 \geq 0$, $\tkernel \in U_{\delta}(\tkernel_0)$.
The policy $\pi_{n+1,\tkernel,\epsilon}$ is then well defined by
$$
\pi_{n+1,\tkernel,\epsilon}(\cdot|\bar{s}) =
\frac{\num_{\tkernel,\pi_{n,\epsilon}}(\cdot|\bar{s})}
{\den_{\tkernel,\pi_{n,\epsilon}}(\cdot|\bar{s})}
+ \frac{\epsilon}{|\mathcal{A}|}
\label{eq:epirec}
$$
for all $\bar{s} \in \theinterestingstates$.
Analogous to \eref{eq:piM}, we can rewrite the above equation similarly
as in theorem \ref{le:limdetcont} using $Q_{\tkernel}^{\pi_{n,\epsilon},g,h}$
and $v_{\tkernel,\pi_{n,\epsilon}}$.
The proof proceeds by induction on the remaining horizon $h$.

\emph{Base case $(h=1)$:}
Now we fix $\beta,\tilde{\beta} \in (0,1)$ arbitrary so that
$1> \gamma + \epsilon$, where $\gamma = \frac{\tilde{\beta}}{((1-\epsilon)^N-\beta)\alpha(\delta,\epsilon)}$.
Note that we will be restricting (decreasing) $\delta>0$
several times in the proof.
Since $\alpha(\delta,\epsilon)$ is deceasing in $\delta$, the decrease in $\delta$ will cause an increase in $\alpha$, which further causes a decrease in $\gamma$ leaving the constraint $1> \gamma + \epsilon$ in place.
According to 
point 1.\ of lemma \ref{le:contoptbound} (continuity of $Q^*_{\tkernel}$),
point 5.\ of lemma \ref{le:esuppstab},
 lemma \ref{le:econtQfac}, and lemma \ref{le:edetopt}, we can fix $2>\delta >0$
so that for all $\tkernel \in U_{\delta}(\tkernel_0)$,
all $\bar{s} = (s,1,g) \in \theinterestingstates$, and all $n\geq 1$ it holds that
$$
\begin{aligned}
\tilde{\beta}  &> Q_{\tkernel}^*(\bar{s},a)\quad \text{for}\; a\notin \oactions(\bar{s}),
\\
(1-\epsilon)^N - \beta &< Q_{\tkernel}^{\pi_{n,\epsilon}}(\bar{s},a) \quad \text{otherwise.}
\end{aligned}
$$
Further, we lower bound the recursion for $\pi_{n+1,\tkernel,\epsilon}(\oactions(\bar{s})|\bar{s})$ in exactly
the same way as in theorem \ref{le:limdetcont} using the above $\tilde{\beta}$ and $\beta$ bounds, and also the $\alpha$ bound on the 
state visitation terms leaving $(\forall n \geq 0, \forall \pi_0 >0, \forall \tkernel \in U_{\delta}(\tkernel_0),\forall \bar{s} = (s,1,g) \in \theinterestingstates )$
\begin{align}
\pi_{n+1,\tkernel,\epsilon}(\oactions(\bar{s})|\bar{s})
&\geq
\frac{((1-\epsilon)^N-\beta)\alpha \pi_{n,\tkernel,\epsilon}(\oactions(\bar{s})|\bar{s})}
{((1-\epsilon)^N-\beta)\alpha \pi_{n,\tkernel,\epsilon}(\oactions(\bar{s})|\bar{s}) + \tilde{\beta}}
+
\frac{\epsilon |\oactions(\bar{s})|}{|\mathcal{A}|}
\nonumber\\
&=
z_{\gamma,\epsilon,\oactions(\bar{s})} (\pi_{n,\tkernel,\epsilon}(\oactions(\bar{s})|\bar{s})).
\label{eq:epimf1}
\end{align}

\emph{Induction step:} Now we aim to prove that the statement holds for a fixed $h$ with $h>1$, while working under the assumption that it holds for a smaller $h$ (induction assumption).
Fix $\beta,\tilde{\beta} \in (0,1)$ arbitrary so that
$1 > \gamma + \epsilon$, where $\gamma = \frac{\tilde{\beta}}{((1-\epsilon)^N -\beta)\alpha(\delta,\epsilon)}$.
By lemma \ref{le:econtQfac} there exists $\delta'>0$ such that
if the following two conditions are met
$$
\begin{gathered}
\tkernel \in U_{\delta'}(\tkernel_0),
\\
(\forall \bar{s}' = (s',h',g') \in \theinterestingstates,h' < h ):
\quad
1 - \epsilon(1 - \frac{\oactions(\bar{s}')}{|\mathcal{A}|}) - \pi_{\epsilon}(\oactions(\bar{s}')|\bar{s}')
< \frac{\delta'}{2}
\end{gathered}
$$
then it holds that
$$
(\forall \bar{s} = (s,h,g) \in \theinterestingstates, \forall a \in \oactions(\bar{s}), \forall \tkernel\in U_{\delta'}(\tkernel_0) ):
\quad
Q_{\tkernel}^{\pi_{\epsilon}}(\bar{s},a) \geq
(1-\epsilon)^N - \beta.
$$
The first condition is met by the choice $0<\delta < \delta'<2$.
To meet the second condition, we will use the induction assumption
choosing $\beta',\tilde{\beta}' >0$ such that $\gamma'+\epsilon < 1$
(where $\gamma' = \frac{\tilde{\beta}'}{((1-\epsilon)^N-\beta')\alpha(\delta,\epsilon)}$) and $x^*(\gamma',\epsilon,|\oactions(\bar{s}')|) > 1-\epsilon(1-\frac{|\oactions(\bar{s}')|}{|\mathcal{A}|})-\delta'/2$
(this can be done since $x^*(\gamma',\epsilon,|\oactions(\bar{s}')|) \rightarrow 1 - \epsilon(1-\frac{|\oactions(\bar{s}')|}{|\mathcal{A}|})$ for $(\beta',\tilde{\beta}') \rightarrow 0$). 
Applying the induction assumption requires a restriction on $\delta$. From the induction assumption, it follows that there exists $n_0'$ such that,
for all $n>n_0', \tkernel \in U_{\delta}(\tkernel_0)$, it holds that
$\pi_{n,\tkernel,\epsilon}(\oactions(\bar{s}')| \bar{s}') > z_{\gamma',\epsilon,|\oactions(\bar{s}')|}^{n-n_0'}(\frac{\epsilon|\oactions(\bar{s}')|}{|\mathcal{A}|})$.
As $z_{\gamma',\epsilon,|\oactions(\bar{s}')|}^{n-n_0'}(\frac{\epsilon|\oactions(\bar{s}')|}{|\mathcal{A}|}) \rightarrow x^*(\gamma',\epsilon,|\oactions(\bar{s}')|)$ there exists $n_0$ such that
$\pi_{n,\tkernel,\epsilon}(\oactions(\bar{s}')|\bar{s}') > 1 -\epsilon(1-\frac{|\oactions(\bar{s}')|}{|\mathcal{A}|}) - \delta'/2$
for $n>n_0$, $\tkernel \in U_{\delta}(\tkernel_0)$.

In addition, by restricting $\delta > 0$ accordingly, we force the bound $Q_{\tkernel}^*(\bar{s},a) <\tilde{\beta}$ for all $\bar{s} \in \theinterestingstates$ and all $a \notin \oactions(\bar{s})$. Finally, we can apply
$\beta$, $\tilde{\beta}$ and state visitation bounds as we
did before (for $h=1$) to get the desired result.
\end{proof}


\subsection{Extending the Continuity Results to Other Segment Sub-Spaces}
As before we need to modify theorem \ref{le:elimdetcont} to cover algorithms like ODT with regularization restricting the recursion to $\Seg^{\diag}$ or $\Seg^{\trail}$. First, we will introduce $\Seg^{\diag/\trail}$ variants of lemmas \ref{le:esuppstab} and \ref{le:edetopt}.
Note that equations \eref{eq:isthesame}, \eref{eq:saisthesame} and \eref{eq:optathesame}
depends only on $\pi_0$ and are therefore applicable also in $\Seg^{\diag/\trail}$.
Following the same reasoning as in section \ref{sse:suppstabDiagTrail},
one can extend the results in lemma \ref{le:esuppstab} also to $\Seg^{\diag/\trail}$.
\begin{lemma}\label{le:esuppstabDiagTrail} ($\epsilon$-\eUDRL{} version of support stability for $\Seg^{\diag/\trail}$) The lemma \ref{le:esuppstab} remains valid under renaming $\pi_{n,\epsilon}\rightarrow \pi_{n,\epsilon}^{\diag/\trail}$.
\end{lemma}
Similarly we can derive the variant of lemma \ref{le:edetopt} for $\Seg^{\diag/\trail}$.
\begin{lemma}\label{le:edetoptDiagTrail} (Optimality of $\epsilon$-\eUDRL{} policies for deterministic transition kernel in $\Seg^{\diag/\trail}$) The lemma \ref{le:edetopt} remains valid under renaming $\pi_{n,\epsilon}\rightarrow \pi_{n,\epsilon}^{\diag/\trail}$.
\end{lemma}
The proof follows the same lines as in lemma \ref{le:edetopt} except in point 1.\ we use lemma \ref{le:esuppstabDiagTrail} instead of lemma \ref{le:esuppstab} and equations \eref{eq:isthesame} and \eref{eq:saisthesame}. Finally, we can state the $\Seg^{\diag/\trail}$ version of the theorem \ref{le:elimdetcont}.
\elimdetcontDiagTrail*
The proof follows similarly as the proof of theorem \ref{le:elimdetcont} except of small differences which resembles those described already in section \ref{ssse:extendingfinite}.
First, to assert that $\epsilon$-\eUDRL{}-generated policies are in $\Pi_{\tkernel_0,\epsilon}^*$ at deterministic points one applies lemma \ref{le:edetoptDiagTrail} instead of lemma \ref{le:edetopt}. Second, to prove that $\pi_{n+1,\epsilon}^{\diag/\trail}$ is
well defined through equation \eref{eq:epirec}, one uses equation \eref{eq:isthesame} and point 2.\ of lemma \ref{le:esuppstabDiagTrail} instead of lemma \ref{le:esuppstab}. Third, the application of point 1.\ of lemma \ref{le:recrewrites} has to be replaced with points 2.\ and 3.\ of the lemma respectively. To bound the $\epsilon$-\eUDRL{} recursion, one proceeds along the same lines yielding the exactly the same result as in \eref{eq:epimf1}. The same reasoning applies to the induction step. Finally, making use of lemma \ref{le:recrewrites}, which allows us to rewrite the $\epsilon$-\eUDRL{} recursion using only $\Seg$-space-related quantities, shows that there are no further differences even for $\pi_{n+1,\epsilon}^{\diag/\trail}$.
The same approach also applies to the forthcoming corollary of the theorem \ref{le:elimdetcont}.


\subsection{Estimating the Location of Accumulation Points}
The following corollary is the $\epsilon$-\eUDRL{} version of the corollary \ref{le:limitbounds}.
\elimitbounds*
\begin{proof}
The proof follows the same as the proof of corollary \ref{le:limitbounds} except for slight variations in the corollary statement which has to be addressed. However, the main arguments are the same and we will not repeat them here. Again we rather try to
concentrate on differences. Of course, one has to use lemmas introduced in this appendix instead of their original versions.
The definition of $\alpha$ comes from lemma \ref{le:ealpha}.
We decided to drop $\delta,\epsilon$ arguments for convenience because
in this corollary proof there are no any frequent readjustments of
$\delta$ in opposite to the previous theorem.

\emph{The claim} has now a sightly different form $(\forall  N \geq h \geq 1, \forall \pi_0 >0 , \forall \tkernel \in U_{\delta}(\tkernel_0)):$
\begin{align*}
&
(\exists (\pi_{n,\tkernel_0,\epsilon}^*), \pi_{n,\tkernel_0,\epsilon}^*\in \Pi_{\tkernel_0,\epsilon}^*):
\;
\limsup_n
\max_{\bar{s} = (s,h',g) \in\theinterestingstates,h'=h, a \in \mathcal{A}}
|Q_{\tkernel}^{\pi_{n,\epsilon}}(\bar{s},a) - Q_{\tkernel_0}^{\pi_{n,\epsilon}^*}(\bar{s},a)| \leq \beta_h,
\\
&\limsup_n
\max_{\bar{s} = (s,h',g) \in\theinterestingstates,h'=h}
2 (1-\epsilon(1-\frac{|\oactions(\bar{s})|}{|\mathcal{A}|})-\pi_{n,\epsilon}(\oactions(\bar{s})|\bar{s}))
\leq \nu_h
\end{align*}
Notice that even before we were using convenient $\pi_{\tkernel_0}^*$
to a policy $\pi$ in the original version of lemma \ref{le:econtQfac}. The difference is that now
we have to mention the chosen sequence $(\pi_{n,\tkernel_0,\epsilon}^*)$ explicitly in the claim
statement, because $Q_{\tkernel_0}^{\pi_{n,\epsilon}^*}$ depends on the particular choice of $\pi_{n,\epsilon}^* \in \Pi_{\tkernel_0,\epsilon}^*$ (unlike the optimal value $Q_{\tkernel_0}^*$).

Points 1.\ and 2.\ follows again directly from the claim.
points 3., 4., and 5.\ are proved in exactly the same way as in the original corollary. We comment only on point 5.\ here. 
We chose the sequence $(\pi_{n,\epsilon}^{*})_{n\geq 0}$ so that $\pi_{n,\epsilon}^*(a\mid\bar{s}) \geq \pi_{n,\epsilon}(a\mid\bar{s})$, for all $\bar{s}\in\theinterestingstates$ and all $a\in\oactions(\bar{s})$, which implies 
$\|
\pi_{n,\epsilon}(\cdot|\bar{s})
- \pi_{n,\epsilon}^{*} (\cdot|\bar{s}) 
\|_1
= 2(1-\epsilon(1-\frac{|\oactions(\bar{s})|}{|\mathcal{A}|})-\pi_{n,\epsilon}(\oactions(\bar{s})|\bar{s}))
$.
We conclude with the computation of
the convergence rate. 
After applying Heine and L'Hospital
theorems (in exactly the same way as in the original proof)
one is left with
\begin{align*}
\lim_{n\rightarrow \infty} \frac{y_{n+1}-y_L}{y_n-y_L}
&=
\lim_{x\rightarrow x^*(\gamma',\epsilon,|\oactions(\bar{s})|)}
z_{\gamma',\epsilon,|\oactions(\bar{s})|}'
\\
&=
\lim_{x\rightarrow x^*(\gamma',\epsilon,|\oactions(\bar{s})|)}
\frac{(1-\epsilon)\gamma'}{(x+\gamma')^2}
=
\frac{(1-\epsilon)\gamma'}{(x^*(\gamma',\epsilon,|\oactions(\bar{s})|)+\gamma')^2}
\\
&\leq
\frac{(1-\epsilon)\gamma'}{(\hat{x}^*(\gamma',\epsilon,|\oactions(\bar{s})|)+\gamma')^2}
=
\frac{(1-\epsilon)\gamma'}{(1-\epsilon(1-\frac{|\oactions(\bar{s})|}{|\mathcal{A}|}))^2}
\\
&\leq
\frac{\gamma'}{(1-\epsilon)}
=
\gamma' + \frac{\epsilon\gamma'}{(1-\epsilon)}
\leq
\gamma' + \frac{\epsilon\gamma'}{\gamma'}
=
\gamma'+\epsilon
<
1.
\end{align*}
\end{proof}





