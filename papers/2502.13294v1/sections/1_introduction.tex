\section{Introduction}
As the use of Artificial Intelligence (AI) to automate or support critical tasks proliferates, a wide range of interested parties have created principles, frameworks, and tools to ensure that AI systems are aligned with stakeholders' values.
In spite of the growing amount of interest in Responsible AI (RAI) and AI governance, we continue to see AI systems produce unintended outputs and fail to adhere to ethical principles.
For example, researchers have found that AI tools to detect COVID-19 have been ineffective, and potentially even harmful~\cite{heaven2021hundreds}.
% https://www.technologyreview.com/2021/07/30/1030329/machine-learning-ai-failed-covid-hospital-diagnosis-pandemic/
Faulty facial recognition software has led to the wrongful arrest of three men in the US~\cite{johnson2022how}.
% https://www.wired.com/story/wrongful-arrests-ai-derailed-3-mens-lives/
New York City's AI chatbot has encouraged businesses to break the law~\cite{lecher2024nyc}.
% https://themarkup.org/news/2024/03/29/nycs-ai-chatbot-tells-businesses-to-break-the-law

One barrier to translating the principles, frameworks, and tools to practice is that many are too general, focusing on high-level ethical principles, such as autonomy and justice, but providing little guidance on how to promote the principles in action~\cite{Morley2019, schiff2020principles}.
On the other hand, some tools are too granular. For example, exclusively focusing on one technical definition of fairness during model testing does not necessarily guarantee fairness during AI system deployment~\cite{Kaye2023}.
Furthermore, many principles, frameworks, and tools have not been empirically tested for usability---the extent to which a RAI principle, framework, or tool is easy to adopt by practitioners---nor effectiveness---the extent to which using a principle, framework, or tool results in the claimed benefit in the real-world~\cite{mittelstadt2019ai, Kaye2023, berman2024scoping, cartwright2009thing}. 
% \hhcomment{We need to be clear what we mean by terms such as validation.}

More broadly, effective AI governance remains an inherently complex issue due to the numerous stakeholders and AI lifecycle stages involved.
First, AI systems have multiple stakeholders, including technical stakeholders (e.g., developers), business stakeholders (e.g., organizational leaders), and members of the general public.
These stakeholders differ in educational background, their approach to framing problems, and norms for communication~\cite{schiff2020principles}. Consequently, not all RAI frameworks and tools are usable by every stakeholder.
% Furthermore, the inclusion of numerous stakeholders in an AI system makes it difficult to clearly assign responsibilities, credit, or blame to a specific actor~\cite{schiff2020principles}.
% https://dl.acm.org/doi/10.1145/3531146.3533150
Second, preventing and mitigating AI harms requires understanding their source in the AI lifecycle---the process by which an AI model is imagined, designed, developed, evaluated, and integrated into broader decision-making processes and workflows. 
Thus, despite the growing collection of frameworks and tools for AI governance, \textbf{it remains unclear \emph{who} within an organization should conduct \emph{what} task at different stages of the AI lifecycle, and \emph{how}.} In particular, stakeholders are frequently unaware or uncertain about which principles, frameworks, and tools they can utilize to discharge their responsibilties. %\emph{concrete} and \emph{validated} framework or tool applies. 
This reality, in turn, makes it difficult for organizations to effectively govern AI. 

Past work has recognized this need for concrete RAI tools~\cite{Morley2019, Prem2023}.
There have been previous classifications of RAI tools by the stages of the AI life cycle where the tool is supposed to be used~\cite{Morley2019, Prem2023, Ayling2022} and, less commonly, by the stakeholders who can use a given tool and apply its output~\cite{Ayling2022}.
However, there is a lack of a \emph{comprehensive} review of current tools that classifies artifacts into the \emph{intersection} of stakeholders and AI lifecycle stages. This gap makes it difficult for each stakeholder in an AI system to know what tools are available to them at each stage of the AI lifecycle.
Furthermore, previous work does not check whether different artifacts were tested for usability or effectiveness.
This also makes it difficult for researchers and practitioners who develop AI governance tools to determine where additional research and development is needed.

% \xhdr{The Present Work} 
In this paper, we address the above gaps by conducting a systematic review of the literature of more than 220 AI governance and RAI tools that provide concrete guidance towards operationalizing RAI. We analyze these tools, categorizing them according to the specific stakeholder group and the AI lifecycle stage they address (Figure~\ref{fig:roles_stages_sbs}). We also assess whether each tool has been validated in any way for usability or effectiveness (for example, through a case study, experiment, or a pilot program). The resulting matrices provides an overview of available resources, and offers practitioners a valuable resource to consult in deciding how to comply with RAI values in different elements of their work. In addition, it provides the RAI research community an empirical account of the gaps in the field and outlines impactful avenues for future work.


% \hhcomment{We need to clearly state what we mean by tool vs. framework and use the terms consistently throughout.}
\textbf{A note on key concepts and terminology:} We differentiate RAI ``tools,'' ``artifacts,'' and ``processes'' from ``principles'' and ``frameworks'' in two main ways. First, we differentiate by functional scope. In this respect, frameworks are conceptual structures or overarching guidelines that often define principles, directives, and high-level strategies for RAI. On the other hand, tools are specific, practical instruments, techniques, or processes that implement or operationalize certain aspects of a framework. The second differentiation pertains to the level of the focus in each approach. Frameworks are abstract and intended to guide thinking and high-level strategizing about RAI. They often deal with the ``what'' and ``why'' perspective, focusing on desired principles. In contrast, tools are applied, hands-on resources that assist in the implementation of a framework's principles in practice. They deal with the ``how'' by offering concrete steps and methods to achieve the desired goals of a framework. For the rest of this study, we focus our efforts on tools as opposed to frameworks. 
% Henceforth, we use the term RAI ``tool,'' ``artifact,'' or ``process'' to denote concrete guidance that is neither too high-level or too granular in the ways we described above.
%\Rachelcomment{unsure if it's necessary to explain this much given that we talked about the principles to practices gap already. There's a shorter version commented out in the Overleaf, but it might be too succinct.}



% \begin{figure}[]
% \centering
% \includegraphics[width=\textwidth]{figs/roles_stages_validated.png}
% \caption{Co-occurrence of roles and stages for validated tools}
% \label{fig:roles_stages_validated}
% \end{figure}

% \begin{figure}[]
% \centering
% \begin{minipage}{.4\textwidth}
%   \centering
%   \includegraphics[width=\linewidth]{figs/roles_stages_validated.png}
%   \captionof{figure}{}
%   \label{fig:roles_stages_validated}
% \end{minipage}%
% \begin{minipage}{.4\textwidth}
%   \centering
%   \includegraphics[width=\linewidth]{figs/roles_stages.png}
%   \captionof{figure}{}
%   \label{fig:roles_stages}
% \end{minipage}
% \caption{Summarization and so what}
% \end{figure}

\begin{figure}
     \centering
     \includegraphics[width=1\linewidth]{figs/role_stage_cooccurrence.pdf}
     \caption{Validated and non-validated co-occurrence matrix for stakeholders and stages}
     \label{fig:roles_stages_sbs}
\end{figure}

% \TODO{Summary of main findings, after we have a draft of the discussion.}
Through our analysis of existing tools, we find that existing RAI tools are primarily developed for designers and developers in the data collection, data processing, statistical modeling, and testing stages.
Concrete tools for leaders, end-users, and impacted communities, in value proposition and problem formulation are comparatively lacking. 
Additionally, only 36.3\% of the RAI tools in our literature search mentioned any evidence that the tool creators checked the tool's usability or effectiveness.
Moreover, there are very few validated tools that look end-to-end across the AI lifecycle and engage with end-users and impacted communities along with other stakeholder groups.
% \TODO{Rachel: rewrite a shorter summary of what the findings mean}
% Our findings surrounding the over-represented stakeholders and stages suggest that current approaches to AI governance are mostly technical.
% Furthermore, the under-representation of leaders, end-users, and impacted communities suggests that current efforts toward effective AI governance underestimate the role of organizational culture and incentives, end-user experiences, and downstream consequences on ensuring responsible AI. 
% The relative lack of tools for value proposition and problem formulation indicates that responsible AI is often seen as an afterthought; ethical values are usually embedded after an AI system is created, failing to consider whether it is beneficial or even moral to create a AI system for a particular use case in the first place.
% This finding suggests the need for \emph{proven} tools for AI governance.
% \hhcomment{The following is your positive contribution. You should lead with this.}
% \Rachelcomment{Is this referring to the sentence that begins with ``overall''?}
%In summary, our systematic literature review provides stakeholders with a view of the tools available to them at each stage of the AI lifecycle, which tools are validated, and the broader RAI community with recommendations to help ensure responsible AI in practice.
We argue that without validated RAI tools, many tools may be improperly used and create a false sense of assurance. 
Furthermore, without tools that provide a holistic view across AI lifecycle stages and stakeholders, organizations risk employing a fragmented approach to AI governance. 
In line with these problems, we make three recommendations for practitioners and researchers: validating existing and new RAI tools, approaching AI governance holistically, and using the stakeholder-stage matrix as a blueprint for developing a custom approach to RAI depending on organizational and stakeholders' needs.

% Second, we call for the validation of existing and new RAI tools. Such validations could occur in the form of a hypothetical case study (as done in~\cite{dAlessandro2017, Moon2019, Vidgen2019}), testing code (as done in~\cite{Tyler2024, Ryffel2018, Friedler2019}), or a real-world pilot (as done in~\cite{IBM2024, Ballard2019, Gebru2021}).
% Lastly, we call for the creation \emph{actionable} guidance for leaders, end-users, and impacted communities that address the value proposition and problem formulation stages.

% - make tools for leaders, end-users, and impacted communities, and value proposition, that offer actionable guidance
% - make sure that tools and frameworks offer actionable guidance.
% - make sure that tools are validated. validations could be in the form of case studies, that demonstrate how a tool can be applied to an AI system, checking code, or through piloting a framework with an organization to see whether people find it usable, etc.

%%% Blaine's Notes
% \subsection{Motivation}
% - Growing importance of responsible AI governance.\\
% \subsection{Problem Statement}
% - Lack of a unified framework that connects AI governance roles with lifecycle stages.\\
% - Need for actionable insights into who and where existing tools are effective and where gaps remain.\\
% - this, in turn, makes it difficult for researchers and practitioners who develop these tools to determine where the gaps are

% - The aim of this survey is to explore and analyze existing AI development frameworks, with a specific emphasis on their approach to stakeholder engagement in responsible AI and AI governance across the AI lifecycle. 
% % \subsection{Objectives}
% - Develop a roles-lifecycle matrix.\\
% - Identify tools, gaps, and opportunities within this matrix.\\
% - Provide recommendations for advancing the field.\\
% - The objective of this survey is to systematically examine existing responsible AI frameworks across the AI lifecycle, evaluate the effectiveness of those approaches regarding stakeholder engagement, and provide recommendations/a solution(?) for improving engagement strategies to ensure ethical and responsible AI development. 

%%% Rachel's Notes
% \xhdr{The Present Work}
% - Systematic review of literature.\\
% - Comprehensive analysis of governance and assurance frameworks.\\
% - Identifying series of tools for each role and stage, or gaps if those tools don't exist\\
% - Proposed roadmap for future research.

%%%
% \subsection{The AI Lifecycle} 
% The use of AI in high-stakes domains is often justified by its potential for efficiency gains, lower costs, offering a certain form of transparency and consistency, and preventing certain human biases and prejudices from impacting decisions. However, in recent years, we have witnessed numerous cases of algorithmic harms, such as unfairness,  discrimination, lack of human legibility and recourse, undue surveillance, violation of privacy, and negative impacts on the environment, the economy, and society at large. To prevent and mitigate such harms, we need to understand their source in the AI lifecycle-the process by which an AI model is imagined, designed, developed, evaluated, and integrated into broader decision-making processes and workflows. 

% Referencing previous frameworks that outline the AI lifecycle, such as PwC’s governance framework on the AI lifecycle, PwC’s nine-step responsible AI lifecycle process, and NIST's AI Risk Management Framework, we break down the AI lifecycle into 9 major stages (Value Proposition, Problem Formulation, Data Collection, Data Processing, Statistical Modeling, Testing, Validation, Deployment, and Monitoring), which are briefly explained below.

% \subsection{Major Roles and Stakeholders}
% An important step in designing responsible AI is to identify the relevant players, actors, or stakeholders who have an interest, impact, or influence on the AI project and the corresponding lifecycle. We identify a set of key stakeholders below and delineate their key responsibilities at various stages of the AI lifecycle. This enables organizations to more easily define and execute tasks. 

% % - comprehensive
% - intersection between stakeholders and stages
% - checking for validity
% - tool applicability 

% other tools are too granular
% - furthermore, many tools are not validated
% - importance of broader organization in ensuring responsible AI, and addressing the multiple stakeholders that are involved in an AI system (use text from below)
% - importance of addressing all the lifecycle stages (use text from below)
% - Challenges in addressing diverse roles and lifecycle stages effectively.

% - previous works have addressed the lifecycle stages where a tool should be used
% - have also discussed principles that different AI tools address, and taxonomized tool types
% - however, it still remains unclear for any stakeholder, what exactly is available to them at each stage of the lifecycle, and what tools address their concerns wrt a specific lifecycle stage

% - many organizations and other bodies have come up with frameworks and tools on dealing with RAI
% - in spite of this, we continue to see AI systems produce harmful outputs (list examples) and fail to adhere to ethical principles