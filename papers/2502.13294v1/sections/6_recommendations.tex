\section{Recommendations}
% Immediate Next Steps: Areas requiring urgent research focus, Suggestions for new tools tailored to underserved roles or lifecycle stages.\\
% Long-term Goals: Work with organizations to develop holistic approach
% \TODO{write a summary of the recommendations}
We provide three recommendations towards realizing RAI in practice: validating existing and new RAI tools, approaching AI governance in a holistic and end-to-end manner, and using the stakeholder-stage matrix (Figure~\ref{fig:roles_stages_sbs}) as a blueprint for RAI.
We elaborate on these below.
% To improve RAI tools and processes, researchers and practitioners must explore ways to validate tools in real world contexts while involving a broad range of stakeholders throughout the AI lifecycle to ensure that tools meet technical standards and societal needs. Additionally, research should be conducted to find ways to integrate RAI practices and tools throughout the AI lifecycle rather than retrofitted at the end. Also, researchers should find effective ways to foster collaboration between domain experts, technical professionals, and impacted community members. Future research should focus on exploring and creating a holistic blueprint  with flexible responsibility allocations for organizations. 

\xhdr{Validation of existing and new RAI tools} 
%%% Call for the validation of RAI tools
% Validation should be centered in real-world contexts
% Tools should be evaluated for effectiveness, rather than just usability
% In such evaluations, we look back to the stakeholder-stage matrix we used to classify RAI tools
% Like RAI tools themselves, evaluations of RAI tools can also benefit from this framework of thinking
% Echoing recent work surrounding RAI, RAI tools themselves should also be tested, validated, and monitored
% Resources for monitoring CITE
% They should also work with stakeholders to determine what effectiveness means
% The stakeholders that are meant to use the tool, and the stakeholders that are meant to use the toolâ€™s output
% The exact mechanisms that it will take to achieve this in practice is still a nascent area. One idea is to borrow from some of the RAI tools we have explored, for instance, community jury
% What does it mean for an RAI tool targeted towards impacted communities to be effective?
%%% Recognition of the resource constraints of people that  are building RAI tools
% Organizational constraints
% Financial incentives for users needed for users to audit
% RAI stifles innovation
% However, we see this as an important step to ensure that we see RAI in practice
% In a world where we continue to have difficulties with seeing RAI in practice despite the growing interest in this area
% Through our systematic review of the literature, 
We discovered that most RAI tools lack any evidence of validation. For the few RAI tools that are validated, the validation method often fails to demonstrate whether the RAI tool will be effective in the real world.
Thus, we join previous calls~\cite{berman2024scoping} to validate RAI tools to ensure that they are effective in practice, that is, that using a given RAI tool allows real-world AI systems to achieve the claimed ethical values~\cite{cartwright2009thing}. 
We further recommend that RAI tool developers document both the validation process and results as part of the tool itself.

How can we validate RAI tools to ensure their effectiveness in the real world? 
Berman et al.~\cite{berman2024scoping} propose developing an ``effectiveness evaluation framework,'' outlining design desiderata for such a framework and providing guidelines for ensuring evaluation validity.
We argue that the stakeholder-stage matrix we used to classify RAI tools in this paper (Figure~\ref{fig:roles_stages_sbs}) is useful for thinking about the necessary components of RAI tool \emph{validations} as well.
To illustrate, RAI tools, like AI systems, should also go through stages such as testing (i.e. assessing the tool's effectiveness in driving desired changes in an organization's AI design, development, and deployment processes), validation (i.e. evaluating how well the tool generalized to different AI systems), and monitoring (i.e. ensuring the tool remains effective amidst evolving AI development practices).
% Note that there are other stages that should be considered in RAI tool validation; we use testing, validation, and monitoring as illustrative examples.
Furthermore, like AI systems, RAI tools validations should involve a variety of stakeholder groups. 
This involvement can help address specific outcomes that might otherwise be overlooked.
For example, organizational leaders could be involved to answer the question: what does it mean for an RAI tool to be effective for the organization? Similarly, impacted community members could be involved to answer the question: what does it mean for an RAI tool to be effective for their community?
% Similar to how previous research has recognized the need for multiple stakeholders to be involved in RAI efforts \TODO{because of xyz, Rachel cite}, we argue that the involvement of multiple stakeholders in RAI tool validations is critical for many of the same reasons.
This diverse stakeholder approach aligns with prior research emphasizing the importance of broad stakeholder involvement in RAI efforts~\cite{kaminski2018binary, kawakami2024responsible}. Similarly, we argue that involving multiple stakeholders in RAI tool validation is essential for ensuring their relevance and effectiveness. 

Achieving a multi-stage and multi-stakeholder RAI tool validation process presents significant challenges.
First, RAI tool developers have many resource constraints~\cite{berman2024scoping}.
Additionally, organizational practices and culture are often barriers to RAI~\cite{rakova2021responsible, varanasi2023currently}. Previous work has also shown that end-users require financial incentives to participate in auditing~\cite{deng2023understanding}
Furthermore, a narrow focus on validation can stifle innovation~\cite{greenberg2008usability}.
Nonetheless, in a world where we continue to have difficulties with seeing RAI in practice despite the growing interest in this area, we see RAI tool validations as a productive step forward.
As next steps, we recommend RAI tool developers to test, validate, and monitor the tools they create, consider multiple stakeholder groups, and document these processes within the RAI tool itself.
We also recommend researchers conduct further investigation into RAI tool evaluation practices, developing methods and frameworks to support effective validation.

%%% Recognition of the resource constraints of people that  are building RAI tools
% Organizational constraints
% Financial incentives for users needed for users to audit
% RAI stifles innovation
% However, we see this as an important step to ensure that we see RAI in practice
% In a world where we continue to have difficulties with seeing RAI in practice despite the growing interest in this area

% practical next steps

%%% Old Draft (January 21)
% Good validation practices are crucial not only to ensure that tools are fit for their intended purpose, but also to assess whether the assumptions built into the tools are valid in different (real-world) contexts. 
% Without clear and reliable benchmarks for validation, researchers and organizations risk misapplying these tools in ways that fail to account for real-world complexity. 
% Validation can ensure that tools are technically sound and appropriately tuned to meet the needs of a wide variety of use cases. 
% It is also important to focus research on including diverse stakeholders in the design, testing, and implementation of RAI tools. Because these systems have the potential to impact vast subsets of the population, it is crucial to involve representatives from stakeholder groups that are not frequently included given the current status quo. 
% This may help to address specific problematic outcomes and concerns such as value and problem alignment that might otherwise be overlooked. Without a diverse range of perspectives, even validated RAI tools may unintentionally overlook key issues or exclude critical considerations. By including many stakeholder groups throughout the lifecycle, researchers can develop tools that are better suited to address these problems, creating more equitable solutions that better serve society as a whole. 
% Lastly, the real world effectiveness of RAI tools must be a central focus of ongoing research. Many tools may look promising in controlled environments, but their true efficacy can only be determined in real world, dynamic settings. Research in this area should aim to verify that these tools actually achieve their intended goals. Conducting large scale field studies and evaluations is essential for determining whether these tools actually address real world challenges effectively. 

\xhdr{A holistic, end-to-end approach to AI governance}
We discussed that the current landscape of RAI tools encourages an approach to RAI that is fragmented across both AI lifecycle stages and stakeholder groups, posing a barrier to realizing RAI in practice.
Thus, we join previous calls~\cite{Raji2020} to incorporate and develop an end-to-end approach to AI governance across lifecycle stages.
As illustrative examples of RAI tools that consider all stages of the AI lifecycle, we point to GAO's Accountability Framework~\cite{GAO2021} and Microsoft's AI Fairness Checklist~\cite{Madaio2020}.
We also recommend that approaches to AI governance be holistic not only across the AI lifecycle stages, but also across stakeholder groups.
For example, it may be beneficial to research architectures for collaborative lifecycle models in which designers, developers, and impacted communities jointly define the goals, constraints, and risks of AI systems.
In line with these recommendations, we call on organizational leaders to implement holistic processes within their workflows and researchers to focus their efforts on developing and testing the effectiveness of such processes.

% As we look forward, it is crucial to explore whether the vision of a more integrated and proactive approach to responsible AI is possible in real world scenarios. Implementing these ideas requires a deeper understanding of how these tools and processes can be embedded throughout the AI lifecycle. Although there will be challenges in creating a more responsible AI ecosystem, these will be vastly outweighed by the benefits of a holistic approach. Emerging tools that emphasize responsible stakeholder collaboration throughout the AI lifecycle may provide the necessary foundation for societally aligned AI systems that are transparent, fair, and accountable. By aligning RAI practices with the operational needs of organizations and the safe keeping of the community, we can move from theoretical models to actionable solutions. For instance, tools like Model Cards for Model Reporting \cite{Mitchell2019} provide structured documentation to improve transparency and understanding of AI systems \cite{Nunes2022}, while Datasheets for Datasets \cite{Gebru2021} emphasize ethical considerations in data curation and usage, promoting fairness and accountability across AI workflows \cite{Boyd2021}.

% Many researchers and organizations are still researching how to implement a responsible AI lifecycle. Some current approaches attempt to add existing tools to the models that already exist such as adding a fairness auditing tool, explainable AI algorithm, or bias detection model after the fact, but we have demonstrated that these efforts are often fragmented and reactive when implemented alone. For instance, fairness tools like AI Fairness 360 \cite{Bellamy2019} have been applied to assess bias retrospectively but often lack the ability to influence upstream decisions, such as those made during data collection. Similarly, explainability tools like LIME~\cite{lime} and SHAP~\cite{lundberg2017unified} are frequently used without adequate consideration of how their outputs are interpreted or acted upon by non-technical stakeholders. We have also shown that there are few if any fully integrated RAI development pipelines, with many approaches treating these considerations as afterthoughts rather than an integral part of the lifecycle. For example, retrofitted fairness interventions in automated hiring models have often resulted in ethical challenges, such as reinforcing biases in historical data or failing to address systemic issues, which undermines the trust and effectiveness of the solutions they aim to improve~\cite{mihaljevic2024}.

% Future research should attempt to explore ways to embed responsible tools and practices earlier in the AI lifecycle. For example, it may be greatly beneficial to research architectures for collaborative lifecycle models where technical experts, domain specialists, and members of impacted communities jointly define the goals and constraints of AI systems. Successful instances of such collaboration, such as community-based participatory design approaches in AI for social good projects~\cite{kawakami2024situate}, highlight the potential for co-creating ethical AI solutions. Additionally, researchers could examine architectures for integrating ethical considerations into the earliest stages of AI development, such as through value-sensitive design (VSD) frameworks~\cite{Friedman2006}. 

% \xhdr{Future Work: Envisioning a Blueprint for Real World Use Cases} 
\xhdr{A blueprint for RAI}
Ensuring strong accountability mechanisms is a difficult problem in RAI~\cite{cooper2022accountability, schiff2020principles}.
One of the reasons behind this difficulty is the ``many hands'' problem: there are many stages in the AI lifecycle, and many actors involved each stage~\cite{cooper2022accountability, nissenbaum1996accountability}
% The ``many hands'' problem makes 
Consequently, it is difficult to determine both who should be responsible for problems, but also where these problems stem from.

Our stakeholder-stage matrix (Figure~\ref{fig:roles_stages_sbs}) provides a structure not only for determining for each stakeholder what RAI tools are available when, but also for assigning clearer responsibilities for each of the stakeholders in an AI system across the AI lifecycle.
This is reminiscent of the responsibility assignment matrix, which is a widely-used project management tool encouraging an end-to-end view of a project and benefiting organizations with multiple moving parts~\cite{wittenberg2024everything}.
Indeed, one of the RAI tools in our literature review~\cite{Johnson2023} incorporates the responsibility assignment matrix to clearly delineate tasks to different stakeholder groups. 
Using a responsibility assignment matrix for AI systems can help disaggregate the extent of each stakeholder's involvement in each stage of the AI lifecycle and therefore serve as a potential remedy to the ``many hands'' problem.

Thus, we encourage organizational leaders to consider the stakeholder-stage matrix as a blueprint for AI governance within their organization.
We also recommend that researchers explore the utility of the responsibility assignment matrix as a framework for ensuring RAI. 
For example, researchers could use the matrix to understand current organizational practices, determine whether responsibility allocations need to be adjusted depending on the specific AI use case, and establish the extent to which the responsibility assignment matrix is both standardized and customizable.


% One key area for future research is whether responsibility allocations in RAI need to adjust depending on the specific domain or application. For instance, in healthcare, issues of fairness and equity are particularly pressing, as demonstrated by the biases inherent in predictive models that influence patient outcomes~\cite{Obermeyer2019}. In contrast, sectors like finance may focus more on transparency and explainability to mitigate risks such as algorithmic discrimination. For example, implementing tools in cases where credit scoring models have denied loans to individuals from underrepresented groups based on opaque decision-making processes is likely to improve outcomes~\cite{BERTRAND2021}.  The diversity of AI applications may require a nuanced approach to responsibility allocation, where both the ethical challenges and stakeholder needs are carefully considered. RAI strategies should be flexible enough to adapt to these differences, ensuring that ethical practices are tailored to the unique requirements of each domain while still upholding overarching principles of fairness and accountability.

% To move forward, researchers must build on these early efforts and create a blueprint to guide organizations in implementing RAI effectively. The blueprint should include tools to address concerns and problems at every stage while fostering collaboration across stakeholder groups. Researching questions such as ``is it possible to implement these collaborative systems?'', ``do responsibility allocations shift or adjust for a given domain or application?'', and ``what practices are organizations currently following to implement responsible AI across the lifecycle?'' will be crucial in refining these practices.  


%%% Recommendations moved from Discussion (January 19)
% Prioritizing these early stages can unlock novel approaches to embedding ethical considerations and stakeholder alignment from the outset, fostering AI systems that are both innovative and accountable.
% This oversight not only stifles innovation, but also increases the likelihood of addressing critical risks too late in the lifecycle, where remediation is more complex and costly. 
% Rebuilding this trust demands meaningful participation from these stakeholders across the AI lifecycle, ensuring their voices are reflected in the design, deployment, and oversight of AI systems.
% Bridging this gap requires aligning technical solutions with broader governance structures and fostering collaboration among diverse stakeholders to address RAI challenges comprehensively.

%%% Old Draft (January 19)
% Our findings have several implications for researchers, practitioners, and policymakers aiming to address these gaps. First, interdisciplinary efforts are essential for addressing underrepresented roles and stages. Collaboration between technical experts, ethicists, policymakers, and community representatives can help ensure that tools and frameworks account for diverse perspectives and responsibilities. For example, involving impacted communities in the value proposition stage can help identify societal risks early, leading to more robust and inclusive AI systems.

% Second, researchers should prioritize developing tools that are both actionable and validated. While many existing frameworks and tools emphasize principles, there is a critical need for practical guidance on how these principles can be operationalized by specific roles at each stage of the AI lifecycle. Validation of these tools, through real-world application and iterative refinement, is equally important to ensure their efficacy and usability.

% For practitioners, it is important to recognize that technical solutions alone are insufficient for achieving RAI. Effective governance requires aligning technical tools with organizational structures, incentives, and accountability mechanisms. For instance, developers should not be expected to assume responsibility for ethical considerations without adequate training, resources, and support from leadership.

% Finally, policymakers should focus on creating enforceable standards that go beyond transparency. While transparency is a cornerstone of accountability, it should be accompanied by mechanisms for enforceability and meaningful participation. Standards should also promote the integration of RAI practices across the entire lifecycle, emphasizing early and late stages as much as technical stages.

% Organizational leaders must adopt enforceable RAI policies by investing in tools that enable consistent oversight across the lifecycle, such as compliance dashboards or risk assessment processes. These tools should integrate real-time monitoring capabilities to track adherence to ethical guidelines and identify potential risks proactively. Additionally, leaders should establish mechanisms for auditing and reporting on RAI practices, leveraging technologies like automated documentation generation and risk visualization systems to ensure transparency and accountability.

% Designers should prioritize inclusive design practices, such as co-creation workshops with impacted communities during early-stage development. These workshops can be supplemented with tools for participatory prototyping, enabling iterative feedback from diverse stakeholders. Furthermore, adopting human-centered design frameworks that incorporate ethical guidelines, such as differential privacy for sensitive data handling or bias detection algorithms in interface design, can help mitigate risks early in the process.

% Developers should focus on integrating RAI principles directly into technical workflows. This can include developing modular fairness testing frameworks that are easy to plug into existing pipelines, as well as adopting explainability-by-design approaches to ensure transparency at each stage of the system. Leveraging version-controlled ethical checklists and automated bias detection tools can streamline the process, reducing the burden of retrofitting ethical considerations post-development. Developers should also consider utilizing continuous integration pipelines that include RAI metrics as standard evaluation criteria.

% Deployers should use deployment tools that incorporate real-time monitoring and adaptive risk management capabilities. For instance, integrating predictive analytics to forecast potential unintended outcomes during deployment can improve responsiveness to emerging challenges. Additionally, deployers should ensure that rollback mechanisms are in place, allowing for rapid system reconfiguration or deactivation when ethical or functional issues arise.

% End-users should be engaged during testing and validation stages to provide feedback on system usability and alignment with real-world needs. Tools such as usability testing platforms with embedded RAI evaluation criteria or feedback aggregation systems can streamline this process. Gamified testing environments or interactive tutorials can also be employed to encourage end-user participation while collecting meaningful insights about system performance and user experience.

% Impacted communities should participate in value proposition stages to surface potential societal risks and ensure systems align with collective goals. Community engagement platforms, such as online forums with facilitated discussions or AI-assisted sentiment analysis tools, can be used to gather diverse perspectives efficiently. Additionally, scenario-based workshops supported by ethical simulation tools can help impacted communities visualize potential outcomes and provide informed input on system design and objectives.

% Immediate next steps should prioritize areas requiring urgent research focus. This includes developing new tools tailored to underserved roles, such as organizational leadership and impacted communities, and lifecycle stages, particularly value proposition and problem formulation. Such tools should offer actionable guidance and be designed to address the specific challenges these roles and stages encounter. For example, frameworks that integrate community perspectives during early stages can better anticipate societal risks and foster alignment with ethical standards. Additionally, longitudinal studies on the adoption and impact of RAI tools in organizational contexts could help identify best practices and barriers to effective implementation.

% There is a need for more participatory approaches to RAI research and practice, and in the long term, the goal should be to collaborate with organizations to develop a holistic approach to RAI. Engaging underrepresented roles, such as end-users and impacted communities, in co-design processes can help bridge the gap between principles and practice, ensuring that RAI tools and frameworks are both inclusive and effective. This involves creating systems where RAI practices are embedded throughout the lifecycle and supported by comprehensive governance structures. Partnerships between academia, industry, and impacted communities can help align incentives, ensure accountability, and build trust in AI systems. By addressing immediate gaps and focusing on sustainable solutions, researchers and practitioners can pave the way for more inclusive and effective RAI practices.