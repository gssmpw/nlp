\section{Discussion}\label{sec:discussion}
% Practical guidance for researchers and practitioners to address gaps.\\
% Importance of interdisciplinary efforts to fill underrepresented areas.

% Despite organizations increasing eagerness to adopt AI---the percentage of organizations surveyed by McKinsey adopting AI jumped from 55\% in 2023 to 72\%~\cite{}---leaders are still largely AI illiterate.
% % https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai
% Notably, previous research shows low AI literacy among executives, with only 3\% of executives at S\&P 500 companies viewed as AI literate~\cite{}
% % https://sloanreview.mit.edu/article/why-executives-cant-get-comfortable-with-ai/

There are several implications for RAI development that can be drawn from our findings. They center around improper use of RAI tools and fragmented approaches to RAI. We elaborate on these below. 
% \TODO{write a summary of the discussion}

% Organizational leaders may struggle to enforce ethical standards because of their low AI literacy~\cite{} and the lack of actionable tools to operationalize governance. In turn, the overall accountability of AI systems is undermined. 
% This gap can result in a disconnect between high-level principles and their practical implementation, leaving ethical considerations as aspirational rather than actionable. Leaders may find it challenging to translate organizational values into concrete practices, particularly in complex, fast-moving development cycles. This lack of operational clarity can create inconsistencies in decision-making and accountability, increasing the risk of ethical oversights. Furthermore, without tools to monitor and measure the impact of their governance strategies, leaders are left unable to assess or refine their approaches, perpetuating a cycle of ineffective oversight that compromises trust and responsibility within AI-driven organizations. 

\xhdr{Improper use of RAI tools}
% The lack of validated processes for AI governance means that organizations searching for RAI artifacts to use cannot be sure of the usability, applicability, or validity of what is available. 
% If an organization decides to employ an unvalidated process, they may encounter issues with the usability of the process, finding it difficult to apply the process to their particular AI use case.
% Even worse, an organization employing a process that is inapplicable or invalid for their use case can lead to a false sense of confidence in adhering to ethical values.
% To illustrate, previous work has shown that developers can over-rely on explainability and interpretability tools~\cite{kumar2021shapley, kaur2020interpreting}.
Many RAI tools are not validated, which can result in publicly available artifacts that are unusable, prone to misuse, or ineffective, consequently providing a false sense of assurance surrounding AI systems.
% In Section~\ref{sec:tool_validation}, we reported that many RAI tools are not validated.
By a tool ``not being validated'', we mean that validation is not mentioned \emph{within the tool's documentation}, although researchers and third parties may have done validation studies later~\citep[e.g.,][]{deng2022exploring, kaur2020interpreting, nunes2022using}.
However, given the already-existing barriers to organizations adopting RAI practices, such as a fast-paced work environment~\cite{berman2024scoping, kaur2020interpreting, Tenney2020}, we hypothesize that organizations are unlikely to search for additional evidence of RAI tool validation if it is not already mentioned within the tool itself. 
Consequently, an organization considering an unvalidated RAI tool likely has no information regarding a tool’s effectiveness, that is, whether using the RAI tool delivers the claimed benefit in practice~\cite{cartwright2009thing}.
% Finding --> many tools are not validated
% Show no evidence of validation in the documentation of the tool (although researchers may have done validation studies later)
% This means that an organization looking at a tool has no idea about a tool’s effectiveness → does the tool achieve what it says it will achieve 

Attempting to use unvalidated RAI tools poses a myriad of issues.
First, an unvalidated RAI tool can be difficult to use~\cite{lee2021landscape}.
Furthermore, it is hard to pinpoint \emph{why} there are difficulties with using the tool. For example, the difficulty may be rooted in the tool's design, but could also be the result of a poor onboarding process and a lack of guidance for practitioners~\cite{berman2024scoping}.
Conversely, while an unvalidated RAI tool may be highly usable, without comprehensive checks on how the tool should and should not be used, practitioners can easily become prone to \emph{over-relying} on it. For example, previous work has shown that developers can over-rely on explainability and interpretability tools~\cite{kumar2021shapley, kaur2020interpreting}.
Additionally, an unvalidated RAI tool may not be suitable for existing organizational practices.
There are already many barriers to RAI adoption including resource constraints~\cite{berman2024scoping} and organizational culture~\cite{rakova2021responsible, varanasi2023currently}.
If a tool causes too much friction with already-present workflows, it may be more difficult to incentivize stakeholders to adopt the tool.
% This poses a myriad of issues, for instance
% Are tools easy to use by the stakeholders?
% On one hand, a tool might not be usable
% Difficulty of pinpointing where the source of unusability (?) is
% Design flaw of the tool?
% Onboarding / lack of guidance on how to use the tool?
% On the other hand, it might be very usable, but consequently cause practitioners to misuse it
% Ex. overreliance on interpretability tools
% Are tools suitable for existing organizational practices?
% Already there are barriers to adopting RAI (CITE)
% If tools are not designed for organizational workflows, it might be harder to incentivize people to adopt them

Even when we consider validated RAI tools, we find that many of the validations are removed from real-world contexts.
For example, many RAI tools are validated through a hypothetical case study~\citep[e.g.][]{dAlessandro2017, Moon2019, Vidgen2019} that illustrates how the tool should be used.
However, such forms of validation leave the critical question---what role can the RAI tool can play in the real-world?---unanswered. 
For example, Model Cards for Model Reporting~\cite{Mitchell2019} are validated through two hypothetical case studies; however, recent work has shown that the emphasis on accuracy-based metrics in Model Cards results in other downstream harms being overlooked, although the latter could be more beneficial for impacted communities~\cite{kawakami2024responsible}.
This finding is particularly critical, as it is mentioned in the documentation of Model Cards that they are meant to serve impacted communities. 
Overall, unvalidated RAI tools can be unusable, misused, or incompatible with existing practices, while among the few validated tools, many of the checks are removed from the real-world contexts where these tools are meant to be applied.
% Even with tools with existing evaluations, a lot of them are hypothetical – case studies or testing code
% This leaves a critical question unanswered – the role that RAI tools can play in the real world
% Case studies
% For instance --> model cards shows case studies
% However, impacted communities might have more domain specific needs (kawakami2024responsible)


% This misplaced confidence can have cascading consequences. Unvalidated or misapplied RAI tools may result in flawed AI systems being deployed in sensitive domains including healthcare, finance, and criminal justice. Decisions informed by these tools can perpetuate systemic harms. For instance, unchecked use of these tools alone may reinforce existing biases, create inequitable access to services, or unjustly impact individuals' lives. Historically, we have seen reliance on unvalidated or poorly validated fairness tools give the appearance of compliance while embedding discriminatory practices into decision making \TODO{CITE - stop and frisk or facial rec or financial race proxies}. In other instances, we have also found AI systems that prioritize interpretability or explainability without adequate oversight and validation leading to poorly informed decisions based on misleading insights \TODO{CITE - Propublica}. 

% I think we also can have an argument that bad or non existent validation can lead to erosions of trust, and we can pull that back in
% In addition, the ongoing use of poorly executed validation for RAI tools can erode public trust in AI systems. This misplaced confidence can have significant impacts on society, especially when RAI tools fail to include the perspectives of end users and impacted community members. 
% The underrepresentation of end-users and impacted communities in decision-making processes could lead to AI systems that fail to align with individual and societal needs, ultimately eroding public trust. 
%For example, when these groups are excluded from early stages like value proposition or problem formulation, their perspectives on usability and risks remain unaddressed. 
% Similarly, if end-users and impacted communities are excluded from later stages like monitoring, they are may not be able to voice their concerns about an algorithm until preventable harms are already impacting those with the least power to make any change. 

% Over time, this exclusion can result in systems that unintentionally overlook essential requirements, propagate harm, or reinforce societal inequities. 
% Essential requirements are overlooked when end-users and impacted communities are not given the opportunity to participate in the early stages of an AI system's development, resulting in individual experiences being overlooked in favour of organizational objectives.
% Harms are propagated when end-users and impacted communities are only given the means to participate in an AI system in the monitoring stage, once the harm has already begun to manifest.
% Societal inequities are reinforced when the perspectives of end-users and impacted communities, often the least powerful stakeholder group, are further diminished in the RAI tool landscape.
% Over time, repeated instances of misalignment, harm, and inequity can create the perception that AI technologies are opaque, unaccountable, or even adversarial to the public good.

% Such trends stem from organizations using RAI tools without a complete understanding of their strengths and limitations, a situation exacerbated by the absence of validations in current RAI tools.

\xhdr{Fragmented approach to RAI} 
% Overemphasis on technical roles and stages could perpetuate a piecemeal approach to RAI, where solutions address isolated issues rather than systemic challenges. 
% This fragmented governance framework risks creating a disconnect between technical efforts and overarching ethical or organizational objectives, making it difficult to ensure coherence across the AI lifecycle. 
% Without a more holistic approach, systemic issues such as bias, opacity, or accountability gaps may persist, 
% % or misalignment with societal values may persist, 
% % undermining trust and the long-term sustainability of AI systems.
% continuing to make responsible AI difficult to realize in practice.
There are few validated RAI tools that address the AI lifecycle end-to-end 
% (Section~\ref{sec:lifecycle_stages}) 
and also address all the stakeholders that are involved in an AI system.
% (Section~\ref{sec:stakeholders}).
The lack of comprehensive and cohesive efforts for RAI means that many organizations are using a piecemeal approach to RAI and AI governance. 
Consequently, ethical issues at different stages of the AI lifecycle are being addressed in isolation.
However, this fragmented view of AI governance makes it difficult to have a holistic view of an AI system.
For example, AI Fairness 360~\cite{Bellamy2019} addresses fairness issues in data processing, statistical modeling, and testing.
However, ensuring fairness in these stages does not necessarily mean that fairness will translate into deployment. 
To illustrate, Cheng and Chouldechova~\cite{cheng2022heterogeneity} found that different end-users make different decisions with the same algorithmic output. 
In this example, while the algorithmic output may be fair, the end-user's decisions following the output may render the AI system unfair.
Approaching fairness in isolation can prevent organizations from understanding the interactions between all the different stages of the AI lifecycle where ethical issues can arise.

Furthermore, there is currently an absence of RAI tools that bring different stakeholders together. In particular, there are few tools that bring end-users and impacted communities together with other stakeholder groups.
Due to the wide-ranging impacts of AI systems, many people in RAI have advocated for involving, consulting with, and collaborating across diverse stakeholders~\cite{rakova2021responsible}.
However, there are few concrete tools to do so in the context of RAI, giving rise to difficulties in recognizing and reconciling different stakeholders' perspectives.
For instance, developers may benefit most from concrete, quantitative metrics for fairness, but impacted communities may benefit more from descriptions of potential downstream instances and implications of unfairness.
Without RAI tools bringing these two stakeholder groups together, developers may not understand impacted communities' needs, while impacted communities may have trouble understanding how an algorithm was tested for fairness and what the results were.
% model cards center accuracy-based metrics, which might be useful for developers but less useful for impacted communities, who would benefit more from information that shows potential downstream harms

% Additionally, there are a limited number of tools that address the early stages of the AI lifecycle, such as value proposition and problem formulation.
% Rather, researchers and practitioners often focus their efforts on the later, more technical stages. 
% These trends limit opportunities for proactive risk identification,
% %and creative problem-solving, 
% constraining the potential for truly responsible AI, a problem recognized in previous work~\cite{kawakami2024situate, coston2023validity, passi2019problem, raji2022fallacy}.
% The present-day reactive approach fails to address potential ethical concerns until problems show up in later stages when they are often more difficult, expensive, and complex to fix. By then, organizations may have already faced reputational damage and failures in innovation. Moreover, marginalized communities may have to bear the grunt of those negative consequences. 
% This gap in tool applicability further strengthens the reactive and fragmented governance approach, often inhibiting the integration of responsible AI principles from the very beginning. 


%%% OLD (January 22)
% Many people in the field have been vocal about bringing together stakeholders, since AI has wide-reaching impacts on people
% However, there are little tools to do this in the context of organizational RAI
% Clashes in what goals we want to achieve…

% Finding → there is a lack of validated RAI tools that address the ML pipeline end-to-end and also address all the actors that might be involved in an AI system, including end-users and impacted communities

% What does this mean about the real world → attempts to ensure RAI are fragmented. AI governance is fragmented
% This further means that, in the current state, with regards to the stages of the AI lifecycle...
% Different actors addressing different issues in isolation
% Different stages being addressed in isolation
% Ex. ensuring fairness in statistical modeling and testing does not mean that fairness will translate into deployment
% Homelessness algorithm example

% This further means that, in the current state, with regards to the actors in the ML pipeline, there are a lack of tools that bring different stakeholders together, especially end-users and impacted communities with other stakeholders
% Many people in the field have been vocal about bringing together stakeholders, since AI has wide-reaching impacts on people
% However, there are little tools to do this in the context of organizational RAI
% Clashes in what goals we want to achieve…

% In addition to this fragmentation, RAI is currently mostly reactive rather than proactive


% Neglecting early stages, such as value proposition and problem formulation, limits opportunities for proactive risk identification and creative problem-solving, constraining the potential for truly responsible AI. 
% These stages are pivotal for setting the foundation of an AI system's alignment with ethical and societal goals. 
% When decisions made in these stages are flawed, the consequences can be significant and irreversible. To illustrate, the development of an AI model that infers ``criminality'' from a person's face~\cite{raji2022fallacy}
% % https://www.wired.com/story/algorithm-predicts-criminality-based-face-sparks-furor/
% is a direct result of a failure to properly define the value proposition of AI use in sensitive areas, perpetuating harmful stereotypes and reinforcing racial bias. An algorithm should not have been developed for such tasks in the first place.
% Moreover, the existence of such algorithms would not provide a net-benefit to any stakeholder group, instead perpetuating the long-debunked and harmful notion that people's characteristics can be inferred from their faces.~\cite{raji2022fallacy} 
% Another real-world example, this time regarding a decision in the problem formulation stage that resulted in fairness issues is the choice to use healthcare costs as a proxy to predict patient healthcare needs.  Using costs as a proxy to predict patient needs has led to racially biased outcomes, with Black patients being unfairly disadvantaged due to historical inequalities in healthcare spending, resulting in less money being spent on black patients than on white patients who are equally ill~\cite{Obermeyer2019}.
% https://www.science.org/doi/10.1126/science.aax2342

% Despite the clear risks associated with neglecting the early stages of AI development, most notably value proposition and problem formulation, researchers and practitioners often focus their efforts on the later, more technical stages. This reactive approach fails to address potential ethical concerns until problems show up in later stages when they are often more difficult, expensive, and complex to fix. By then, organizations may have already faced reputational damage and failures in innovation. Additionally, marginalized communities may have to bear the grunt of those negative consequences. Also, the lack of well-developed tools that are specifically designed to incorporate early stage considerations and engage diverse stakeholder groups, compounds this problem. The existing tools tend to focus on later lifecycle stages and apply to only a small and repetitive set of stakeholder groups. 
% This gap in tool applicability further strengthens the reactive and fragmented governance approach, often inhibiting the integration of responsible AI principles from the very beginning. 

% These notable instances of AI failures that occurred as a result of decisions made in the value proposition or problem formulation are some of the many examples that indicate that the early stages are critical to defining project objectives and identifying potential risks. 
% Yet, their frequent omission by the RAI researchers and practitioners perpetuates a reactive approach to RAI. 
% This reactive approach to RAI leaves critical ethical considerations unaddressed until problems manifest in later stages.
% However, in later stages, problems are more complex and costly to resolve, organizations may have faced additional damages like slower innovation or a declining reputation, and negative consequences may already have been perpetuated in impacted communities.

% \xhdr{Erosion of trust} The underrepresentation of end-users and impacted communities in decision-making processes could lead to AI systems that fail to align with individual and societal needs, ultimately eroding public trust. 
% When these groups are excluded from early stages like value proposition or problem formulation, their perspectives on usability and risks remain unaddressed. 
% When end-users and impacted communities are excluded from later stages like monitoring, they are not given the means to voice their concerns about an algorithm. 

% This exclusion can result in systems that unintentionally overlook essential requirements, propagate harm, or reinforce societal inequities. 
% Essential requirements are overlooked when end-users and impacted communities are not given the opportunity to participate in the early stages of an AI system's development, resulting in individual experiences being overlooked in favour of organizational objectives.
% Harms are propagated when end-users and impacted communities are only given the means to participate in an AI system in the monitoring stage, once the harm has already begun to manifest.
% Societal inequities are reinforced when the perspectives of end-users and impacted communities, often the least powerful stakeholder group, are further diminished in the RAI tool landscape.
% Over time, repeated instances of misalignment, harm, and inequity can create the perception that AI technologies are opaque, unaccountable, or even adversarial to the public good.

% \xhdr{Ineffective organizational leadership} 
% The lack of RAI tools for organizational leaders indicates that they may have a difficult time understanding AI systems.
% Previous research aligns with this finding, showing low AI literacy among executives, with only 3\% of executives at S\&P 500 companies viewed as AI literate~\cite{}. 
% % https://sloanreview.mit.edu/article/why-executives-cant-get-comfortable-with-ai/
% The lack of artifacts designed for leaders may contribute to this statistic.
% Despite low AI literacy among organizational leaders, organizations are increasingly eager to adopt AI: a survey by McKinsey shows that the percentage of organizations adopting AI increased from 55\% in 2023 to 72\% in 2024~\cite{}. 
% All in all, these findings mean that many organizations are choosing to adopt AI without full knowledge of its capabilities and risks.

% Leaders are a powerful stakeholder group, and the trend of haphazardly using AI leads to a myriad of downstream effects. 
% First, the lack of RAI tools for leaders implies that many organizations have difficulty assessing how AI adoption supports higher-level organizational goals. Leaders can find it challenging to translate organizational values into concrete practices, leaving ethical considerations to be aspirational rather than actionable. 
% Second, many of the responsibilities in ensuring ethical standards may trickle downstream to designers and developers. While these stakeholder groups play a critical role within any AI system, in a top-down organization, designers and developers have little opportunity to voice concerns or change an organization's higher-level direction. This power structure can result in designers and developers being forced to attempt to implement AI solutions when, for example, the data may be insufficient or inappropriate. 
% Third, without robust RAI processes, leaders may assign unclear or inconsistent accountabilities to the different stakeholders within their organization. 
% It is already inherently difficult to assign accountabilities within AI systems~\cite{};
% % accountability in an algorithmic society
% the lack of RAI processes for leaders thus increases the risk of ethical oversights.

% \subsection{Blaine's Impressions}
% There are many papers that identify the need for diverse stakeholders, lifecycle interaction (not just after the fact), or governance in general. Not many actually provide both who should be doing these things, and how they should be done. (Cite)

% Many papers that claim to provide a framework for achieving governance or assurance are instead lists of principles or practices upon further examination. Few dive past the principles to explain and explore the who and the how. (Cite)

% A lot of highlighting key aspects of governance or RAI: auditing, benchmarking, confidence and trust, explainability and interpretability, for example, but most end up with no detailed insight. Many pointing to tools for XAI or calls for greater transparency. 

% Several papers imply who should be taking these actions - almost always developers end up required to wear many hats at once, acting as developers, with the expectation that they are familiar with and follow RAI practices innately. (Cite)

% A greater focus recently has been on tools for principles rather than for stages. Tools/checklists for fairness or XAI for example are abundant. This again brings the above expectation that developers know about and want to implement these, often without explicit incentive to do so. (Cite)

% A ton of roadmaps and frameworks for future research, etc. - Legal, regulatory, and ethical frameworks for development of standards in artificial intelligence (AI) and autonomous robotic surgery, 

% Many papers are very normatively oriented - “a system should be fair/transparent, a system should allow for opting in/out”

% Multiple calls for XAI as accountability, or XAI as auditability, highlights a potential misalignment between technical tools for XAI as justifying a prediction, often as a bolted on afterthought, and a more traceable series of design/modeling choices that led to this being the outcome. 

% Most standards for accountability, often focus on variations of ‘transparency’, and remain silent on enforceability and participation, basically enabling this as voluntary opt-in with little power to wield. 

% \subsection{Rachel's Impressions}

% Tools and frameworks are often said to have been created with expert input. But that's not enough. There still needs to be validation of the tool. Even if it was created with expert input, that isn't necessarily a guarantee that the tool is functional.

% There aren't that many tools for impacted communities anyway, but few that exist mostly center around monitoring. Although it makes sense that impacted communities can't really contribute to something super technical like statistical modeling, they should contribute to value proposition more, for example.

% \subsection{Interpretation of Findings}\label{sec:interpretation_of_findings}
% % Why certain roles or lifecycle stages are overrepresented.\\
% % Potential causes for underrepresentation in specific matrix cells.
% We discuss the key observations presented in Section~\ref{sec:results} and explain our hypotheses surrounding the potential causes of our findings.
% Overall, we find that existing RAI frameworks and tools are primarily developed for designers and developers in the data collection, data processing, statistical modeling, and testing stages.
% Concrete tools for leaders, end-users, and impacted communities, in value proposition and problem formulation are comparatively lacking. 
% Our findings surrounding the over-represented stakeholders and stages suggest that current approaches to AI governance are mostly technical.
% Furthermore, the under-representation of leaders, end-users, and impacted communities suggests that current efforts toward effective AI governance underestimate the role of organizational culture and incentives, end-user experiences, and downstream consequences on ensuring responsible AI. 
% The relative lack of tools for value proposition and problem formulation indicates that responsible AI is often seen as an afterthought; ethical values are usually embedded after an AI system is created, failing to consider whether it is beneficial or even moral to create a AI system for a particular use case in the first place.
% Furthermore, only 33\% of the RAI tools in our literature search mentioned any evidence that the tool creators checked the tool's validity, applicability, or usability. 
% % This finding suggests the need for \emph{proven} tools for AI governance.

% \xhdr{Overrepresentation of data collection, data processing, statistical modeling, and testing}
% AI lifecycle stages including data collection, data processing, statistical modeling, and testing, are overrepresented in our systematic literature review.
% This overrepresentation can be attributed to the technical nature of these tasks and the strong emphasis on technical tools in responsible AI research, a trend also found previously by Ojewale et al.~\cite{Ojewale2024} and Black et al.~\cite{black2023toward}. 
% In addition, we hypothesize that there is a general perception that these stages are critical for achieving measurable outcomes such as fairness, transparency, and precision, which constitute some of the key ethical values of RAI. 
% Tools for these stages often target tasks such as fairness auditing~\cite{Weerts2023, Bellamy2019, Saleiro2018}, explainable AI~\cite{Nori2019, h2o2024, Tenney2020}, and benchmarking~\cite{Rauber2017}.
% %, reinforcing their prominence in the literature.

% \xhdr{Underrepresentation of value proposition and problem formulation}
% Value proposition and problem formulation are stages that have been notably neglected in current RAI tools.
% The comparative neglect of these early AI lifecycle stages in our systematic literature review suggests a broader systemic issue: RAI is often retrofitted rather than embedded from the start. 
% We hypothesize that the promises of AI---increasing economic efficiency, boosting productivity, and allowing for human capital reinvestment---are both implicitly assumed and perceived to be enough to provide a ``green light'' for a project to move through subsequent stages. 
% Ethics concerns are often seen as something that can and should be addressed later.

% \xhdr{Overrepresentation of designers and developers}
% Tools for designers and developers are overrepresented in our collection of AI tools.
% We suspect that this overrepresentation is a reflection of the fact that these stakeholder groups are often perceived to be the main actors in AI development. 
% As the main technical stakeholders, it is unsurprising that they most often coexist with the technical stages discussed above. 
% These pairings of, and the emphasis on, technical roles and stages could be leading to the stakeholder-stage overrepresentation that we have identified. 

% \xhdr{Underrepresentation of leaders, end-users, and impacted communities}
% On the other hand, the underrepresentation of roles such as organizational leadership, end-users, and impacted communities reflects a gap in recognizing both the technical and organizational responsibilities to ensure RAI.
% Organizational leaders are a powerful stakeholder group, setting overarching goals, making strategic decisions, and establishing the working culture.
% The high-level choices made by leaders perpetuate down to the stakeholder groups that interact more directly with AI systems, such as designers, developers, and deployers.
% The lack of RAI tools for leaders suggest that they are choosing to use AI within their organizations without fully understanding its benefits or risks. 
% The detailed steps required to ensure good AI governance then becomes the responsibility of the designers and developers.
% % These statistics, along with the lack of suggest that organizational leaders are choosing to use AI without fully understanding its benefits or risks. 
% % The lack of RAI tools for leaders makes it difficult for leaders to understand the downstream consequences of their decision
% % - puts the burden of ensuring good AI governance to 
% % Due to the top-down structure of many organizations, designers and developers may be left to build the AI system, with little avenues to provide feedback to leadership on the decision to use AI for a particular use case despite their comparatively higher AI literacy.
% % Leadership often sets high-level governance and ethical standards but lacks actionable tools to enforce these standards across the lifecycle. 

% End-users and impacted communities are also underrepresented in our systematic literature review.
% In particular, there are little tools available to them in the earlier stages of the AI lifecycle.
% Perhaps most notably, there are \emph{no} validated tools that encompass both end-users and value proposition, and \emph{one} validated tool that encompasses both impacted communities and value proposition (Figure~\ref{fig:roles_stages_sbs}).
% Current RAI tools do not empower end-users and impacted communities to voice whether an AI system would be useful for them (in the case of end-users) want an algorithm to affect a certain aspect of their life (in the case of impacted communities).
% This finding suggests that the value of an AI system is primarily perceived through the lens of higher-level organizational goals rather than individual experiences.
% As Kawakami et al.~\cite{} also argue, many organizations developing AI systems treat end-users and impacted communities as an afterthought.
% % https://ojs.aaai.org/index.php/AIES/article/view/31669/33836
% % - why is this happening. end-users and impacted communities are an afterthought for an organization developing AI systems. focus on higher-level, organizational level goals rather than the individual experience with an algorithm.
% % - 
% % This finding suggests that current RAI tools do not empower end-users and impacted communities to voice whether an AI system would be useful for them (in the case of end-users) want an algorithm to affect a certain aspect of their life.
% % The organizational structures around AI systems further reinforce pre-existing inequalities: end-users and impacted communities, who are often the stakeholder groups with the least power, are given no say in whether an algorithm is valuable to them.

% The previous claim is further supported by the fact that of the small number of RAI tools available to end-users and impacted communities, many are focused on monitoring, especially for impacted communities. (Figure~\ref{fig:roles_stages_sbs}).
% \Rachelmargincomment{Should somehow refer to these as Figure (a) and (b)}
% This means that current tools only help end-users and impacted communities take action towards RAI \emph{after} a model has been deployed. 
% After an AI system's deployment, however, undesired effects on end-users and impacted communities may have already begun to take place.
% To illustrate, two tools for end-users and impacted communities in the monitoring stage include a black-box method to detect whether a certain user's texts were used to train a model~\cite{Song2019} and a website that informs people about the user-tracking technologies of a given URL~\cite{Blacklight2020}. 
% If end-users and impacted communities discover through these tools that their information has been unwillingly used in model training or that they have visited a website that tracks them, the harm has already occurred: the individual's privacy has already been violated.
% We suspect that end-users and impacted communities are the least powerful stakeholder groups within the AI lifecycle, resulting in them being given a voice only after harm has already occurred, rather than being actively involved in efforts to prevent harm from the outset.

% % - no tools for value proposition
% % => no consultation as to whether an AI system would be useful (in the case of end-users), or whether you want an algo to affect your life (in the case of impacted communities).
% % - what is the problem with this? only allow 

% % end users and impacted communities are the stakeholder groups with least power
% % current tools do not empower them 

% % most tools are focused on monitoring
% % end-users and impacted communities can only take action towards RAI after a model has been deployed
% % very retroactive approach. face harmful consequences before.
% %%% example. ~\cite{Blacklight2020} ~\cite{Song2019}

% % little tools on the earlier stage of value proposition

% % but they differ from organizational leaders in that they are often the least powerful stakeholders in an AI system.


% % not often involved in technical stages, 
% % limiting their input to later stages such as monitoring, which may be too late to address systemic issues.

% \xhdr{Lack of validation} \hhcomment{This seems misplaced}
% Of the RAI tools in our systematic literature review, only 35\% of the tools contain any evidence that the tool creators checked the tool's validity, applicability, or usability.
% Examples of validation include a hypothetical case study~\citep[e.g.][]{dAlessandro2017, Moon2019, Vidgen2019}, testing code~\citep[e.g.][]{Tyler2024, Ryffel2018, Friedler2019}, or a real-world pilot~\citep[e.g.][]{IBM2024, Ballard2019, Gebru2021}.
% Some of the unvalidated RAI tools are built by RAI experts~\citep[e.g.][]{ODI2021, CDT2017, DC2021}, include consultations and collaborations with relevant people and groups ~\citep[e.g.][]{ACLU2020, ECP2019, UNICEF2024}, and provide mechanisms for tool-users to give feedback~\citep[e.g.][]{DD2022, GovEx2018, Doteveryone2019}.
% Builders of such tools may not feel the need to undergo a validation process.
% % - why do we think there is a lack of validation
% %%% - one reasoning could be there many tools are build with expert consultation. this could be implicit in the organization that built the tool, or consultation with stakeholders
% %%% - limited usability of the tools. might be seen as enough. however, it is not (take an example of a use case)

