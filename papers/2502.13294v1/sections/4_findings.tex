\section{Findings}\label{sec:findings}

We first analyze the AI lifecycle stages and stakeholders addressed by the RAI tools in our systematic literature review.
Then, we explore the intersection between stakeholders and stages, quantifying how well each intersection is covered in the literature both when considering all tools and when filtering by tools that have been validated.
Finally, we explore whether and how existing tools are validated.
% Our findings highlight roles and lifecycle stages that are overrepresented and, conversely, areas that are underrepresented and may benefit from further research.
% Furthermore, we showcase the need for a more comprehensive validation of the efficacy and reliability of frameworks and tools.
% Additionally, we explored the existing relations between roles and lifecycle stages. 

\subsection{Lifecycle Stages} \label{sec:lifecycle_stages}
Figure~\ref{fig:stages_sbs} (left) contains the lifecycle stages addressed by the tools from our systematic literature review.
The most frequently represented lifecycle stages include ``Data Collection'' (50.0\% of all validated tools), ``Data Processing'' (53.7\% of all validated tools), ``Statistical Modeling'' (59.8\% of all validated tools), and ``Testing'' (47.6\% of all validated tools), and ``Validation'' and (51.2\% of all validated tools). 
Tools for these stages often target tasks such as fairness auditing~\citep[e.g.,][]{Weerts2023, Bellamy2019, Saleiro2018}, explainable AI~\citep[e.g.,][]{Nori2019, h2o2024, Tenney2020}, and benchmarking~\citep[e.g.,][]{Rauber2017}.
This overrepresentation can be attributed to the technical nature of these tasks and the strong emphasis on technical tools in responsible AI research, a trend also found previously by Ojewale et al.~\cite{Ojewale2024} and Black et al.~\cite{black2023toward}. 
% In addition, we hypothesize that there is a general perception that these stages are critical to achieving measurable outcomes such as fairness, transparency, and precision, which constitute some of the key ethical values of RAI. 

Conversely, 
% ``Problem Formulation'', ``Deployment'', and ``Monitoring'' were all less well represented with 35.4\%, 26.8\%, and 29.3\% respectively.
``Value Proposition'' and ``Problem Formulation'' were much less represented, covering only 17.1\% and 32.9\% of all validated tools, respectively.
The comparative neglect of these early AI lifecycle stages in our systematic literature review suggests a broader systemic issue discussed by many RAI researchers and practitioners~\cite{kawakami2024situate, coston2023validity, passi2019problem, raji2022fallacy}: many RAI challenges originate from the early stages of AI development, but RAI is often retrofitted rather than embedded from the start. 
% We hypothesize that the promises of AI---increasing economic efficiency, boosting productivity, and allowing for human capital reinvestment---are both implicitly assumed and perceived to be enough to provide a ``green light'' for a project to move through subsequent stages. 
Ethics concerns are often seen as something that can and should be addressed later.

Additionally, ``Deployment'' and ``Monitoring'' are underrepresented, with 23.2\%, and 26.8\% of all validated tools addressing these stages. 
This gap may point to a broader issue within the RAI landscape: the lack of tools that address the ongoing lifecycle needs of AI systems after their deployment. 
% Unlike data collection or modeling, deployment and monitoring involve continuous oversight and adaptation, yet they remain significantly underserved by the current tool ecosystem.
As a result, many present-day deployment and monitoring practices are not standardized, instead relying on the perspectives and expertise of the actors involved in these stages~\cite{cheng2022heterogeneity, eslami2017careful}. 
However, this can result in heterogeneous decision-making---for example, senior call-workers are more likely to screen in referrals in the context of child abuse hotline screening~\cite{cheng2022heterogeneity}.
This variance can have downstream implications, especially in high-stakes domains.

% \begin{figure}[H]
% \centering
% \begin{minipage}{.4\textwidth}
%   \centering
%   \includegraphics[width=\linewidth]{figs/stages.png}
%   \captionof{figure}{Distribution of Stages}
%   \label{fig:stages}
% \end{minipage}%
% \begin{minipage}{.4\textwidth}
%   \centering
%   \includegraphics[width=\linewidth]{figs/stages_stages.png}
%   \captionof{figure}{Co-occurrence of Stages}
%   \label{fig:stages_stages}
% \end{minipage}
% \end{figure}

Finally, we count how many RAI tools address each possible pair of stages (Figure~\ref{fig:stages_sbs}, right). 
We observe that while there are many RAI tools that cover pairs of stages in the middle of the AI lifecycle, there are comparatively less processes that cover the AI lifecycle from end-to-end. Notably, there are only three validated tools that cover both value proposition and monitoring. Of these three tools, two cover \emph{all} of the AI lifecycle stages in Table~\ref{tab:stage_table}.
The first tool is the Government Accountability Office's (GAO's) Accountability Framework~\cite{GAO2021} contain key questions and processes for auditors.
The second tool is Microsoft's AI Fairness Checklist~\cite{Madaio2020} contain questions for organizations to consider to ensure that their AI systems are fair.

\begin{figure}[]
     \centering
     \includegraphics[width=1\linewidth]{figs/stages_side_by_side.pdf}
     \caption{The distribution of stages present for validated tools (left) and the co-occurrence of pairs of stages present for validated tools (right).}
     \label{fig:stages_sbs}
\end{figure}

\subsection{Stakeholders} \label{sec:stakeholders}
Our analysis uncovered that some stakeholders and stages within AI governance were overrepresented and others were underexplored in the literature. We highlight the results in Figure~\ref{fig:roles_sbs} (left).
Overall, ``Developers'' were the most frequently addressed role, with 78.0\% of all validated tools addressing aspects of their responsibilities. ``Designers'' were also well represented, with 53.7\% of all validated artifacts highlighting tools and processes for designers. 
We suspect that this overrepresentation is a reflection of the fact that these stakeholder groups are often perceived to be the main actors in AI development. 
As the main technical stakeholders, it is unsurprising that they most often coexist with the technical stages discussed above. 
These pairings of, and the emphasis on, technical roles and stages could be leading to the stakeholder-stage overrepresentation that we have identified. 

Conversely, ``Organizational Leaders'' were relatively underrepresented with only addressed with 18.3\% of all validated artifacts providing tools and processes for their roles. 
Organizational leaders are a powerful stakeholder group who have the ability to set overarching goals, make strategic decisions, and establish the working culture.
The high-level choices made by organizational leaders are perpetuated down to the stakeholder groups that interact more directly with AI systems, such as designers, developers, and deployers.
The lack of RAI tools for organizational leaders suggests that they are choosing to use AI within their organizations without fully understanding its benefits or risks. 
% The detailed steps required to ensure good AI governance then becomes the responsibility of the designers and developers.
On the other hand, ``End-users'' and ``Impacted Communities'' were vastly underrepresented when compared to the top two roles, with only 11.0\% and 9.8\% of all validated tools addressing those roles, respectively.
This statistic shows that similar to the findings by Kawakami et al.~\cite{kawakami2024responsible}, the landscape of RAI tools currently treats end-users and impacted communities as an afterthought.

Additionally, we count the number of RAI tools that are designed for each possible pair of stakeholders present in Table~\ref{tab:role_table}. The co-occurrences are shown in Figure~\ref{fig:roles_sbs} (right).
We find that there are many tools that address the combination of ``Designer'' and ``Developer'', and ``Developer'' and ``Deployer''. 
There are also a moderate number of tools that address the combination of ``Organizational Leader'' and ``Designer'', ``Organizational Leader'' and ``Developer'', and ``Designer'' and ``Deployer''.
However, there is a lack of tools that are designed for ``End-users'' and ``Impacted Communities'' with other stakeholder groups.
One prominent validated tool that addresses leaders, end-users, and impacted communities is Model Cards~\cite{Mitchell2019}.

% \Rachelmargincomment{What is the main takeaway from the co-occurrence matrix? If it isn't that interesting, maybe we should remove it.}

% \begin{figure}[H]
% \centering
% \begin{minipage}{.4\textwidth}
%   \centering
%   \includegraphics[width=\linewidth]{figs/roles.png}
%   \captionof{figure}{Distribution of Roles}
%   \label{fig:roles}
% \end{minipage}%
% \begin{minipage}{.4\textwidth}
%   \centering
%   \includegraphics[width=\linewidth]{figs/roles_roles.png}
%   \captionof{figure}{Co-occurrence of Roles}
%   \label{fig:roles_roles}
% \end{minipage}
% \end{figure}

\begin{figure}[]
     \centering
     \includegraphics[width=1\linewidth]{figs/roles_side_by_side.pdf}
     \caption{The distribution of roles present for validated tools (left), and the co-occurrence of pairs of roles present for validated tools (right).}
     \label{fig:roles_sbs}
\end{figure}

\subsection{Stakeholders and Stages}
To better understand the interactions between roles and lifecycle stages, we analyzed their co-occurrences within the dataset.
Figure~\ref{fig:roles_stages_sbs} (right) represents all the tools from our systematic literature review, while Figure~\ref{fig:roles_stages_sbs} (left) represents only the tools that have been validated in some form.
% The heatmaps reveal key patterns in pairings and also in the lack of pairings. 
We see that the most frequent pairings involved developers with ``Data Collection'' and ``Statistical Modeling'', as well as designers with ``Problem Formulation''. These findings align with the roles and stages traditionally emphasized in technical aspects of AI development. 

Conversely, intersections involving ``Impacted Communities'' and any of the lifecycle stages were notably sparse. 
A similar pattern exists for ``End-users'' at all stages, albeit at a slightly higher frequency. 
In particular, there are little tools available to these stakeholder groups in the earlier stages of the AI lifecycle.
Perhaps most notably, there are \emph{no} validated tools that encompass both end-users and value proposition, and \emph{one} validated tool (Community Jury~\cite{MSCJ2022}) that encompasses both impacted communities and value proposition (Figure~\ref{fig:roles_stages_sbs}, left).
Current RAI tools do not empower end-users and impacted communities to voice whether an AI system would be useful for them (in the case of end-users) want an algorithm to affect a certain aspect of their life (in the case of impacted communities).
This finding highlights that the value of an AI system is primarily perceived through the lens of higher-level organizational goals rather than individual experiences, end-users' and impacted communities' have a lack of power.
As Kawakami et al.~\cite{kawakami2024responsible} also argue, many organizations developing AI systems treat end-users and impacted communities as an afterthought.
% https://ojs.aaai.org/index.php/AIES/article/view/31669/33836
% - why is this happening. end-users and impacted communities are an afterthought for an organization developing AI systems. focus on higher-level, organizational level goals rather than the individual experience with an algorithm.
% - 
% This finding suggests that current RAI tools do not empower end-users and impacted communities to voice whether an AI system would be useful for them (in the case of end-users) want an algorithm to affect a certain aspect of their life.
% The organizational structures around AI systems further reinforce pre-existing inequalities: end-users and impacted communities, who are often the stakeholder groups with the least power, are given no say in whether an algorithm is valuable to them.

The previous claim is further supported by the fact that of the small number of RAI tools available to end-users and impacted communities, many are focused on monitoring, especially for impacted communities. (Figure~\ref{fig:roles_stages_sbs}, left).
This means that current tools only help end-users and impacted communities take action towards RAI \emph{after} a model has been deployed. 
However, after a AI system is deployed, undesired effects on end-users and impacted communities may have already begun to take place.
To illustrate, two tools for end-users and impacted communities in the monitoring stage include a black-box method to detect whether a certain user's texts were used to train a model~\cite{Song2019} and a browser extension that informs people about the entities that might be targeting political ads towards them~\cite{WTM2017}. 
If end-users and impacted communities discover through these tools that their information has been unwillingly used in model training or that they have been unknowingly targeted by a political entity, the harm has already occurred: the individual's privacy has already been violated or the individual has not been provided the transparency they deserve.
End-users and impacted communities often the least powerful stakeholder groups within the AI lifecycle~\cite{bondi2021envisioning}, which can result in them being given a voice only after harm has already occurred, rather than actively participating in efforts to prevent harm from the outset.
% This suggests that while technical and managerial roles are well-covered, tools that engage external stakeholders or consider the long-term societal impact of AI systems are 
% % less incentivized and therefore 
% lacking. 


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=1\linewidth]{figs/roles_stages.png}
%     \caption{Co-occurrence of roles and stages, filtered by tools that have been validated}
%     \label{fig:roles_stages}
% \end{figure}

\subsection{Tool Table}
In Appendix Table~\ref{tab:validated_tools}, we have constructed a tool table that includes the available, validated tools and processes for each lifecycle stage and stakeholder intersection. Additionally, we have provided Table~\ref{tab:ex_tools} of example tools for each stakeholder-stage intersection.

\begin{landscape}
\begin{table}[]
\centering
% \renewcommand{\arraystretch}{1.5} 
% \setlength{\tabcolsep}{5pt}  
% \footnotesize
% \scriptsize
\tiny
\caption{Examples of validated tools for each lifecycle stage and stakeholder}
\begin{tabular}
{p{0.075\linewidth}p{0.075\linewidth}p{0.075\linewidth}p{0.075\linewidth}p{0.075\linewidth}p{0.075\linewidth}p{0.075\linewidth}p{0.075\linewidth}p{0.075\linewidth}p{0.075\linewidth}}
\toprule
 & \textbf{Value Proposition}
 & \textbf{Problem Formulation}
 & \textbf{Data Collection}
 & \textbf{Data Processing}
 & \textbf{Statistical Modeling}
 & \textbf{Testing}
 & \textbf{Validation}
 & \textbf{Deployment} 
 & \textbf{Monitoring}\\ \midrule
 
% \textbf{Leaders} 
% & An Approach to Organizational Deployment of Inscrutable Artificial Intelligence Systems \cite{Asatiani2021} \newline \emph{Assesses organizational AI readiness} 
% & A Lifecycle Approach for Artificial Intelligence Ethics in Energy Systems \cite{El-Haber2024} \newline \emph{Identifies ethical issues in translating problems}
% & Guidance on the Assurance of Machine Learning in Autonomous Systems \cite{Hawkins2021} \newline \emph{Ensures ML assurance in safety-critical systems}
% & Model Cards \cite{Mitchell2019}  \newline \emph{Summarizes key model details for leaders}
% & Aequitas \cite{Saleiro2018} \newline \emph{Measures group fairness in models}
% &  Assured AI reference architecture \cite{Tyler2024} \newline \emph{Defines AI  architecture for integration}
% & From ethical AI frameworks to tools \cite{Prem2023} \newline \emph{Links ethical principles to actionable tools} 
% & Corporate digital responsibility \cite{Lobschat2021} \newline \emph{Process for aligning corporate AI use with digital responsibility goals}
% & A Framework for Evaluating and Disclosing the ESG Related Impacts of AI with the SDGs \cite{Saetra2021} \newline \emph{Monitoring and Evaluation of ESG impacts with SDG alignment}\\ \midrule

% \textbf{Designers} 
% & ALTAI \cite{EUAI2020} \newline \emph{Guides ethical self-assessments}
% & Prescriptive and Descriptive Approaches to Machine-Learning Transparency \cite{Adkins2022} \newline \emph{Clarifies machine learning model transparency for various stakeholders}
% & Decision Provenance \cite{Singh2019}  \newline \emph{Tracks decision origins to improve accountability}
% &  AI Fairness 360 \cite{Bellamy2019} \newline \emph{Toolkit to evaluate and mitigate algorithmic bias}
% & Hybrid human-machine analyses for characterizing system failure \cite{Nushi2018} \newline \emph{Combines human insights with ML analysis to prevent failure}
% & VerifAI \cite{Dreossi2019} \newline \emph{Simulates and tests AI models under diverse conditions}
% & Behavioral Use Licensing for Responsible AI \cite{Contractor2022} \newline \emph{Introduces licensing approaches for responsible AI usage}
% & Model AI Governance Framework \cite{PDPC2020} \newline \emph{Comprehensive process for AI governance, including risk assessments}
% & Artificial Intelligence Ethics and Safety \cite{Corra2021} \newline \emph{principles and practices for ethical AI deployment and monitoring}\\ \midrule

% \textbf{Developers} 
% & Co-Designing Checklists \cite{Madaio2020} \newline \emph{Co-created checklists addressing AI fairness and accountability}
% & FactSheets \cite{Arnold2019} \newline \emph{Summarizes AI system properties, such as performance and ethical considerations}
% & Datasheets for datasets \cite{Gebru2021} \newline \emph{Document templates to describe dataset motivation, composition, and uses}
% & Conscientious Classification \cite{dAlessandro2017} \newline \emph{Offers practical guidance for improving classification fairness}
% & FairLens \cite{Panigutti2021} \newline \emph{Identifies data-related sources of unfairness}
% & LiFT \cite{Vasudevan2020} \newline \emph{Tests fairness and explainability of AI models}
% & Fairness-enhancing Interventions \cite{Friedler2019} \newline \emph{Provides interventions to improve fairness outcomes}
% & IBM Watson Studio \cite{IBM2024} \newline \emph{Provides infrastructure for building AI workflows}
% & The medical algorithmic audit \cite{Liu2022} \newline \emph{Audits algorithms for biases}\\ \midrule

% \textbf{Deployers} 
% & Corporate digital responsibility \cite{Lobschat2021} \newline \emph{Aligns corporate AI strategy with ethical frameworks}
% & TensorFlow Responsible AI \cite{TF2025} \newline \emph{Offers tools for mitigating bias in TensorFlow workflows}
% &  Decentralized Big Data Auditing \cite{Yu2019} \newline \emph{Facilitates transparency and trust in decentralized systems}
% & FairPy \cite{Viswanath2023} \newline \emph{Enables fairness-aware data preprocessing}
% & Fairtest \cite{Tramer2017} \newline \emph{Validates models under diverse scenarios for bias detection}
% & The Language Interpretability Tool \cite{Tenney2020} \newline \emph{Provides visual explanations for model behavior}
% & From ethical AI frameworks to tools \cite{Prem2023} \newline \emph{Maps ethical AI principles to implementable tools}
% &  PyRIT \cite{Munoz2024} \newline \emph{Open-source library for tracking responsible AI practices}
% & FairTest \cite{Tramer2015} \newline \emph{Identifies failures and biases in AI systems using audits}\\ \midrule

% \textbf{End Users} 
% & \cite{}  
% & Model Cards \cite{Mitchell2019}  \newline \emph{Facilitates understanding of AI systems through user-friendly descriptions} 
% & Decision Provenance \cite{Singh2019} \newline \emph{Explains AI-driven decisions through provenance tracking}  
% & Towards Intersectional Feminist and Participatory ML \cite{Suresh2022} \newline \emph{Advocates participatory design to integrate marginalized perspectives}  
% & A maturity assessment framework for conversational AI development platforms \cite{Aronsson2021} \newline  \emph{Evaluating conversational AI usability and ethical compliance}
% & Hybrid human-machine analyses for characterizing system failure \cite{Nushi2018} \newline \emph{System supports joint human-AI problem-solving for complex errors}
% & Model Cards \cite{Mitchell2019} \newline \emph{Guides users on interpreting model summaries and impacts} 
% & A Lifecycle Approach for Artificial Intelligence Ethics \cite{El-Haber2024}  \newline \emph{Provides actionable ethics recommendations for end-user systems}
% &  Discovering and Validating AI Errors With Crowdsourced
% Failure Reports \cite{Cabrera2021}  \newline \emph{Collects user-generated error reports to refine AI models} \\ \midrule

% \textbf{Impacted Communities}  
% & Microsoft Community jury \cite{MSCJ2022} \newline \emph{Uses participatory methods to gather community input on AI systems} 
% & Practical fundamental rights impact assessments \cite{Janssen2022}  \newline \emph{Provides step-by-step impact assessments for rights-based concerns}
% & A Lifecycle Approach for Artificial Intelligence Ethics \cite{El-Haber2024} \newline \emph{Evaluates AI ethics using a lifecycle perspective to ensure societal benefits} 
% & Model Cards \cite{Mitchell2019} \newline \emph{Summarizes models with accessible, community-centered insights} 
% & UnBias Fairness Toolkit \cite{Lane2018} \newline \emph{Toolkit for understanding and addressing algorithmic biases}
% & Testing Concerns about Technology’s Behavioral Impacts \cite{Matias2022}  \newline \emph{Investigates technology’s behavioral impacts on communities}
% & Model Cards \cite{Mitchell2019} \newline \emph{Accessible model documentation for diverse audiences}
% & A Lifecycle Approach for Artificial Intelligence Ethics \cite{El-Haber2024} \newline 
% & Auditing Data Provenance in Text-Generation Models \cite{Song2019} \newline \emph{Audits data lineage to identify potential ethical issues}\\ \bottomrule

\textbf{Leaders} 
& \raggedright An Approach to Organizational Deployment of Inscrutable Artificial Intelligence Systems \cite{Asatiani2021} 
\newline \newline \emph{Provides a framework for assessing organizational readiness and capability to deploy AI responsibly} 
& \raggedright  A Lifecycle Approach for Artificial Intelligence Ethics in Energy Systems \cite{El-Haber2024} 
\newline \newline \emph{Outlines a comprehensive approach for incorporating ethical considerations into AI projects}
& \raggedright Guidance on the Assurance of Machine Learning in Autonomous Systems \cite{Hawkins2021} 
\newline \newline \emph{Offers specific guidance for certifying ML systems in high-stakes applications}
& \raggedright Model Cards \cite{Mitchell2019}  
\newline \newline \emph{Presents a template for summarizing key details of AI models, including intended use and potential risks}
& \raggedright Aequitas \cite{Saleiro2018} 
\newline \newline \emph{Evaluates fairness across groups and ensures equitable outcomes using metrics and dashboards}
&  \raggedright Assured AI reference architecture \cite{Tyler2024} 
\newline \newline \emph{Defines a reference architecture to ensure AI systems are trustworthy, explainable, and aligned with organizational goals}
& \raggedright From ethical AI frameworks to tools \cite{Prem2023} 
\newline \newline \emph{Maps high-level ethical AI principles to actionable tools, enabling practical implementation of ethical AI} 
& \raggedright Corporate digital responsibility \cite{Lobschat2021} 
\newline \newline \emph{Aligns AI deployment with corporate digital responsibility initiatives to support sustainable and ethical practices}
& A Framework for Evaluating and Disclosing the ESG Related Impacts of AI with the SDGs \cite{Saetra2021} 
\newline \newline \emph{Evaluates AI’s environmental, social, and governance (ESG) impacts through alignment with Sustainable Development Goals }\\ \midrule

\textbf{Designers} 
& \raggedright ALTAI \cite{EUAI2020} 
\newline \newline \emph{Guides designers through ethical self-assessment and understanding AI's societal impact via interactive tools}
& \raggedright Prescriptive and Descriptive Approaches to Machine-Learning Transparency \cite{Adkins2022} 
\newline \newline \emph{Provides transparency guidelines and case studies to help designers articulate and address AI opacity}
& \raggedright Decision Provenance \cite{Singh2019}  
\newline \newline \emph{Tracks and explains the origins of decisions in ML pipelines, ensuring transparency for stakeholders}
&  \raggedright AI Fairness 360 \cite{Bellamy2019} 
\newline \newline \emph{TOffers algorithms and tools to reduce bias in data preprocessing and model training}
& \raggedright Hybrid human-machine analyses for characterizing system failure \cite{Nushi2018} 
\newline \newline \emph{Combines human oversight with ML results to detect failure points and refine system performance}
& \raggedright VerifAI \cite{Dreossi2019} 
\newline \newline \emph{Simulates diverse real-world scenarios to evaluate robustness and safety of AI models}
& \raggedright Behavioral Use Licensing for Responsible AI \cite{Contractor2022} 
\newline \newline \emph{Introduces licensing frameworks that enforce ethical AI behaviors in deployment}
& \raggedright Model AI Governance Framework \cite{PDPC2020} 
\newline \newline \emph{Provides actionable steps for organizations to build robust AI governance practices}
& Artificial Intelligence Ethics and Safety \cite{Corra2021} 
\newline \newline \emph{Suggests ethical principles for AI projects in the design phase, focusing on community benefit}\\ \midrule

\textbf{Developers} 
& \raggedright Co-Designing Checklists \cite{Madaio2020} 
\newline \newline \emph{Facilitates co-creation of development checklists to address fairness, bias, and accountability issues}
& \raggedright FactSheets \cite{Arnold2019} 
\newline \newline \emph{Summarizes AI system specifications, including intended use, limitations, and safety measures}
& \raggedright Datasheets for datasets \cite{Gebru2021} 
\newline \newline \emph{Documents datasets with metadata on collection methods, ethical considerations, and intended uses}
& \raggedright Conscientious Classification \cite{dAlessandro2017} 
\newline \newline \emph{Proposes techniques for classifying data with fairness constraints, balancing accuracy and equity}
& \raggedright FairLens \cite{Panigutti2021} 
\newline \newline \emph{Identifies sources of bias in datasets, aiding in debiasing efforts during feature engineering}
& \raggedright LiFT \cite{Vasudevan2020} 
\newline \newline \emph{Tests fairness and robustness of ML models using automated workflows and visualization tools}
& \raggedright Fairness-enhancing Interventions \cite{Friedler2019} 
\newline \newline \emph{Provides fairness-enhancing interventions, including pre- and post-processing methods}
& \raggedright IBM Watson Studio \cite{IBM2024} 
\newline \newline \emph{Enables responsible development workflows through prebuilt tools and assessment metrics}
& The medical algorithmic audit \cite{Liu2022} 
\newline \newline \emph{Audits algorithmic outputs, identifying disparities in decision-making}\\ \midrule

\textbf{Deployers} 
& \raggedright Corporate digital responsibility \cite{Lobschat2021} 
\newline \newline \emph{Aligns deployment strategies with organizational ethics and corporate responsibility goals}
& \raggedright TensorFlow Responsible AI \cite{TF2025} 
\newline \newline \emph{Offers Responsible AI tools integrated with TensorFlow for bias mitigation and transparency}
& \raggedright Decentralized Big Data Auditing \cite{Yu2019} 
\newline \newline \emph{Implements decentralized auditing processes for ensuring the security and accuracy of data pipelines}
& \raggedright FairPy \cite{Viswanath2023} 
\newline \newline \emph{Provides preprocessing algorithms to identify and reduce potential biases in datasets}
& \raggedright Fairtest \cite{Tramer2017} 
\newline \newline \emph{Tests ML models for fairness using specific tools that simulate diverse user interactions}
& \raggedright The Language Interpretability Tool \cite{Tenney2020} 
\newline \newline \emph{Visualizes model interpretability, offering insights into decision boundaries and errors}
& \raggedright From ethical AI frameworks to tools \cite{Prem2023} 
\newline \newline  \emph{Links organizational ethical frameworks to specific tools, aiding in post-deployment validation}
& \raggedright PyRIT \cite{Munoz2024} 
\newline \newline \emph{Tracks AI practices and analyzes responsible development actions across lifecycle stages}
& FairTest \cite{Tramer2015} 
\newline \newline \emph{Identifies critical algorithmic errors during deployment, focusing on reducing harm}\\ 
\midrule

\textbf{End-users}     
& \raggedright \cite{}  
& \raggedright Model Cards \cite{Mitchell2019}  
\newline \newline \emph{Simplifies understanding of AI models by providing key details on risks, performance, and scope} 
& \raggedright Decision Provenance \cite{Singh2019} 
\newline \newline \emph{Offers explanations of AI-driven decisions, increasing trust and usability for end-users}  
& \raggedright Towards Intersectional Feminist and Participatory ML \cite{Suresh2022} 
\newline \newline \emph{Promotes participatory development, incorporating diverse perspectives into ML design}  
& \raggedright A maturity assessment framework for conversational AI development platforms \cite{Aronsson2021} 
\newline \newline  \emph{Evaluates usability of conversational AI systems to improve accessibility and user experience }
& \raggedright Hybrid human-machine analyses for characterizing system failure \cite{Nushi2018} 
\newline \newline \emph{Combines human judgment and machine analysis to understand and address system errors}
& \raggedright Model Cards \cite{Mitchell2019} 
\newline \newline \emph{Summarizes AI models in ways that are interpretable and actionable for end-users} 
& \raggedright A Lifecycle Approach for Artificial Intelligence Ethics \cite{El-Haber2024}  
\newline \newline \emph{Guides ethical recommendations for AI usage through a lifecycle framework, with an emphasis on end-user systems}
& Discovering and Validating AI Errors With Crowdsourced Failure Reports \cite{Cabrera2021}  
\newline \newline \emph{Crowdsources user feedback to identify and validate AI failures during post-deployment} \\ 
\midrule

\textbf{Impacted Communities}  
& \raggedright Microsoft Community jury \cite{MSCJ2022} 
\newline \newline \emph{Incorporates community voices into AI decisions through jury-based participatory approaches} 
& \raggedright Practical fundamental rights impact assessments \cite{Janssen2022}  
\newline \newline \emph{Provides frameworks for assessing fundamental rights impacts of AI systems on vulnerable groups}
& \raggedright A Lifecycle Approach for Artificial Intelligence Ethics \cite{El-Haber2024} 
\newline \newline \emph{Ensures AI projects align with societal values and promote positive outcomes for communities} 
& \raggedright Model Cards \cite{Mitchell2019} 
\newline \newline \emph{Communicates essential model details, such as intended use and limitations, to the public } 
& \raggedright UnBias Fairness Toolkit \cite{Lane2018} 
\newline \newline \emph{Engages communities in understanding fairness and addressing biases in algorithms}
& \raggedright Testing Concerns about Technology’s Behavioral Impacts \cite{Matias2022}  
\newline \newline \emph{Examines potential community harms of technology and offers mitigation strategies}
& \raggedright Model Cards \cite{Mitchell2019} 
\newline \newline \emph{Documents AI systems accessibly, ensuring impacted groups can understand and question use cases}
& \raggedright A Lifecycle Approach for Artificial Intelligence Ethics \cite{El-Haber2024} 
\newline \newline  \emph{Provides a structure to enable discussion for ensuring AI systems align with ethical standards and societal values.}
& Auditing Data Provenance in Text-Generation Models \cite{Song2019} 
\newline \newline \emph{Audits data provenance in generative text systems to evaluate risks of misinformation}\\ \bottomrule
\end{tabular}
\label{tab:ex_tools}
\end{table}
\end{landscape}

\subsection{Tool Validation} \label{sec:tool_validation}
Our analysis found that a significant portion of the tools and practices identified in the literature lack validation of any kind (151 of 226, 63.7\%) and only a small portion presented evidence of validation (82 of 226, 36.3\%). 
Among those that were validated, validation methods include a hypothetical case study~\citep[e.g.][]{dAlessandro2017, Moon2019, Vidgen2019}, testing code~\citep[e.g.][]{Tyler2024, Ryffel2018, Friedler2019}, or a real-world pilot~\citep[e.g.][]{IBM2024, Ballard2019, Gebru2021}.
Some of the unvalidated RAI tools are built by RAI experts~\citep[e.g.][]{ODI2021, CDT2017, DC2021}, include consultations and collaborations with relevant people and groups ~\citep[e.g.][]{ACLU2020, ECP2019, UNICEF2024}, and provide mechanisms for tool-users to give feedback~\citep[e.g.][]{DD2022, GovEx2018, Doteveryone2019}.
Consequently, builders of such tools may not feel the need to undergo a validation process given the rigorous development process and existing monitoring strategies.

% However, we found that many tools did not provide sufficient evidence of practical application, indicating a need for more rigorous evaluation methodologies in this domain.
% Of the RAI tools in our systematic literature review, only 35\% of the tools contain any evidence that the tool creators checked the tool's validity, applicability, or usability.
% Examples of validation include a hypothetical case study~\citep[e.g.][]{dAlessandro2017, Moon2019, Vidgen2019}, testing code~\citep[e.g.][]{Tyler2024, Ryffel2018, Friedler2019}, or a real-world pilot~\citep[e.g.][]{IBM2024, Ballard2019, Gebru2021}.
% Some of the unvalidated RAI tools are built by RAI experts~\citep[e.g.][]{ODI2021, CDT2017, DC2021}, include consultations and collaborations with relevant people and groups ~\citep[e.g.][]{ACLU2020, ECP2019, UNICEF2024}, and provide mechanisms for tool-users to give feedback~\citep[e.g.][]{DD2022, GovEx2018, Doteveryone2019}.
% Builders of such tools may not feel the need to undergo a validation process.
