\section{Background and Related Work}
% - Overview of existing AI governance frameworks, approaches, philosophies.\\
% - Discussion of related methodologies for assessing governance practices.\\
% - Prior attempts to address roles or lifecycle stages individually.\\
% - Positioning this work in the context of FAccT’s mission: Create best practices for AI ethics; Advance research and dialogue on creating fair, accountable, and transparent systems; Foster collaboration between data and computer scientists, community organizers, advocates, and policy actors; Address issues related to fairness, justice, accountability, transparency, ethics, and adverse impacts of computational systems.

Our work builds upon previous literature reviews and collections of responsible AI frameworks and tools. This section highlights previous work by government agencies, organizations, and researchers to develop responsible AI frameworks and tools, and past reviews and collections of these artifacts.

\subsection{Existing Responsible AI Frameworks and Tools}

The rise of artificial intelligence (AI) in high-stakes domains has led to multiple approaches designed to address responsible AI practices.
Despite the growing body of work surrounding such principles, frameworks, and tools, many challenges remain in ensuring responsible AI \emph{in practice}. 
One challenge is that many guidelines, such as the European Commission's Ethical Guidelines for Trustworthy AI~\cite{highlevel2019ethics}, are too general~\cite{floridi2021establishing}.
There is a lack of methods that translate high-level principles into low-level requirements~\cite{mittelstadt2019ai}.
In fact, 78\% of technology workers wanted more guidance on dealing with ethical issues~\cite{doteveryone2019people}; relatedly, past work has called for moving from the ``what'' to the ``how'' of AI ethics~\cite{Morley2019}.

A set of tools has been developed with the goal of providing very concrete and low-level guidance, such as the four-fifths rule~\cite{us2023select}, Local Interpretable Model-agnostic Explanations (LIME)~\cite{lime}, and Shapley Additive Explanations (SHAP)~\cite{lundberg2017unified}.
However, there are concerns surrounding the narrow focus of the tools in terms of the AI lifecycle stages covered~\cite{Kaye2023, Ojewale2024}, type of methodology used~\cite{Ojewale2024}, and integrity in achieving the higher-level ethical principles~\cite{Ojewale2024}.
These concerns have highlighted the need for AI governance frameworks and tools to be both broad and flexible~\cite{Ojewale2024, schiff2020principles}.

AI governance principles, frameworks, and tools should also be validated for usability and effectiveness.
However, empirically proven methods for responsible AI are lacking~\cite{mittelstadt2019ai}.
Many AI auditing tools do not obviously consult practitioners or affected stakeholders~\cite{Ojewale2024}.
Another big limitation of existing principles, frameworks, and tools is the lack of knowledge surrounding when a certain artifact is appropriate and when it is not~\cite{Kaye2023}.
Consequently, there are calls for principle, framework, and tool creators to follow a quality assurance process~\cite{Kaye2023}.

In our work, we focus on tools that provide concrete yet not overly granular guidance towards RAI, finding the artifacts that are most useful in achieving good AI governance \emph{in practice}. 
Moreover, we label each artifact with whether or not there was validation of the applicability and usability of the framework or tool.

\subsection{Reviews and Collections of Responsible AI Frameworks and Tools}

There is prior research on collecting, reviewing, and analyzing responsible AI frameworks and tools for all stages of the AI development lifecycle.
Morley et al.~\cite{Morley2019} and Prem~\cite{Prem2023} focus specifically on tools that are meant for developers, 
while Ojewale et al.~\cite{Ojewale2024} focus on auditing.
Some work~\cite{Johnson2023} only provides a collection of existing frameworks and tools available to organizations developing AI systems;
other work~\cite{Ortega2024, Morley2019, Prem2023} classifies the frameworks and tools according to the ethical principles that are addressed, including autonomy, beneficence, explainability, justice, non-maleficence, and governance.
Prem~\cite{Prem2023} and Ojewale et al.~\cite{Ojewale2024} also label each framework and tool with their type: for example, whether a tool is designed to identify relevant standards.
More related to our paper, there have been classifications~\cite{Ayling2022, Ortega2024, Morley2019, Prem2023} of existing frameworks and tools into the AI lifecycle stages to which the framework or tool should be applied.
Perhaps most similar to our work is the work of Ayling and Chapman~\cite{Ayling2022}, who label 39 AI ethics frameworks and tools with the stages of the AI lifecycle when the framework should be applied, the stakeholders who apply the tool, and the stakeholders who use the tool output.

Our work expands on these reviews by conducting a systematic literature review; we reviewed all of the frameworks and tools explored in each of these previous literature reviews. 
We consider the intersection of stakeholders and lifecycle stages and ask what tools are available to each stakeholder that address which lifecycle stages. Different from previous work, we focus on the stages that are \emph{addressed} by a framework or a tool rather than the stages that the framework or tool should be \emph{used in}.
Through answering this question, we aim to help various stakeholders across an organization determine exactly which practical tools are available to them at every stage of the lifecycle.

%%% Rachel's rough notes below

% - as interest in RAI, ethics increases, there have been more attempts to create principles, frameworks,  tools to ensure that systems are aligned to ethical values
% - principles to practices gap
% - validating things in practice
% - our work
% - bridge the principles to practices gap --- only 
% - bridge the gap between 

% the principles to practices gap
% ~\cite{Morley2019}
% --- people have been too focused on the principles rather than practices
% - https://doteveryone.org.uk/report/workersview/
% --- 78\% of tech workers want more guidance on dealing with ethical issues
% - https://www.nature.com/articles/s42256-019-0055-y
% --- european commissions ‘Ethics guidelines for trustworthy AI' too general
% --- need to move from ``what'' to ``how''
% - mittelstadt paper

% tools that are too technical
% ---~\cite{Ojewale2024} concerned about methodological integrity of performance analysis tools that are usually quantitative metrics designed to evaluate and explain model behaviour
% --- tools need to be more expansive~\cite{Ojewale2024}, focusing not only on technical fairness and explainability methods in academic literature, but also qualitative tools
% - https://www.worldprivacyforum.org/wp-content/uploads/2023/12/WPF_Risky_Analysis_December_2023_fs.pdf
% --- focusing on one stage of the lifecycle is risky. if fairness is achieved in statistical modeling, does not mean that there is fairness in deployment
% - principles to practice paper
% --- need for broad and flexible tools~\cite{Ojewale2024}

% validation
% - https://www.worldprivacyforum.org/wp-content/uploads/2023/12/WPF_Risky_Analysis_December_2023_fs.pdf
% --- big limitation is that there is a lack of knowledge about which contexts are and are not appropriate for teh use fo a particular tool
% --- call for those creating tools and frameworks to do a quality assurance and standardization
% ---~\cite{Ojewale2024} find that many of the tools did not obviously consult practitioners or affected stakeholders

% considering the intersection of stakeholders and lifecycle stages and what tools are available for each stakeholder that address which lifecycle stages, and performing an additional analysis of whether tools were validated or not.


% including the stages of the AI lifecycle when the framework should be applied, who uses the tool output, who applies the tool


% - outline
% --- there's been prior literature reviews on ethical ai tools and frameworks, and collections of tools and frameworks~\cite{Johnson2023}
% --- classification on the principles that frameworks and tools address~\cite{Ortega2024, Morley2019, Prem2023}, including autonomy, beneficence, explainability, justice, non-maleficence, governance
% --- classification based on lifecycle stages~\cite{Ayling2022, Ortega2024, Morley2019, Prem2023}
% --- type of approach~~\cite{Prem2023, Ojewale2024}
% --- ~\cite{Ojewale2024} focus on auditing tools, supplementing their review of the tools with interviews to determine what is actually used, and what AI practitioners actually need
% --- ~\cite{Morley2019, Prem2023} focus on tools that are meant for developers, while ~\cite{Ojewale2024} focus on auditing
% --- perhaps most similar to our work is ~\cite{Ayling2022}, who classify 39 ethical AI frameworks / tools labelled with a variety of things, including the stages of the AI lifecycle when the framework should be applied, who uses the tool output, who applies the tool
% --- our work expands on these reviews by conducting a systematic review, considering the intersection of stakeholders and lifecycle stages and what tools are available for each stakeholder that address which lifecycle stages, and performing an additional analysis of whether tools were validated or not.

% ~\cite{Ayling2022}
% - probably most similar to our work
% - look at practical guidance as well
% - 39 proposed ethics tool, labelled with a variety of things, including the stages of the AI lifecycle when the framework should be applied, who uses the tool output, who applies the tool, 
% - build on this work to perform a more comprehensive review, (ex. including repos for explainability and not just fairness)
% - also build on this work by considering the stages addressed in the tools rather than when it should be used

% ~\cite{Ortega2024}
% - lifecycle stages and AI principles (autonomy, beneficence, explainability, justice, non-maleficence, governance)
% - our results slightly differ. they find that there is a lot of work in data processing, model training, and model implementation
% - no filtering of high-level principles

% ~\cite{Morley2019}
% - we differ from this in that we focus on a more systematic review, that is meant to be complete
% - we also exclude very specific technical tools, LIME, etc.
% - stages x principles (autonomy, beneficence, explainability, justice, non-maleficence, governance)
% - main arguments derived from findings -- need more of a focus on usability of the tools
% - usability and concreteness is what we focus on for our paper

% ~\cite{Prem2023}
% - more comprehensive AI lifecycle than~\cite{Morley2019}
% - types of approaches (checklist, algo, metric, etc)
% - ethical issues / principles addressed, then a 2x2 on checklist / algo, etc.

% ~\cite{Ojewale2024}
% - categorize tools for auditing specifically
% - look at ``we also  manually labeled each tool with several tags describing the tool’s  documentation and function: license (open-source or proprietary);  organization type (for-profit, non-profit, government, or academic);  intended audit target (automated decision system, online platforms,  large pre-trained online platforms autonomous vehicles, and/or  other); intended user (internal and/or external); and format (e.g.  API, software product, code/data repository, white paper, and/or  other).''
% - additionally -- interviews to determine what is actually used, and what AI practitioners actually need

% ~\cite{Johnson2023}
% - provides a list of approximately 70 tools, whether they're for genAI or standard AI models 