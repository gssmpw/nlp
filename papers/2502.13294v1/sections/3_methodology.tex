\section{Methodology}
\subsection{Literature Review}
\xhdr{Terminology Selection and Refinement} The literature selection began with a focused list of key terms chosen to cover a breadth of AI governance framework descriptions. These included common words and phrases such as AI / artificial intelligence, ML / machine learning, framework, responsible, governance, stakeholder. Using these terms, we conducted an initial search on Google Scholar and Scopus, yielding a limited but targeted set of papers. From this initial collection, we identified synonyms and related terms to expand the search. 
Additional terms included transparency, assurance, auditing, risk management, regulation, ethics / ethical. This expanded list was used to generate combinations of terms to comprehensively capture the relevant literature. 
Examples of combinations included ``AI framework'', ``responsible AI'', ``AI governance'', ``AI risk management'', ``responsible AI framework'', ``responsible  AI stakeholder'', ``responsible AI auditing'', ``AI assurance'', ``responsible AI governance'', ``ethical AI framework''.
% \Rachelcomment{Unsure how many examples of combinations we need. Personally I feel like less could suffice.}

These term combinations were applied two times, first using AI/artificial intelligence, then replacing the term with ML/machine learning. 
% to cover the broadest set of terms possible.
We then collected the top 100 papers from Scopus and Google Scholar for each combination. The process resulted in overlapping results, which were deduplicated by removing articles that matched either DOI number or title, resulting in 1313 unique papers.
Five of these papers~\cite{Ayling2022, Ortega2024, Prem2023, Ojewale2024, Morley2019} were also reviews of existing AI frameworks and tools; we worked through each of the frameworks and tools contained in these papers and added them to the initial list as well.
It is worth noting that this broad selection approach resulted in finding several papers that use reversed versions of the terms in specific technologies or use cases, for example using AI for financial auditing rather than processes or tools for auditing AI. These application-specific papers did not apply to our current research and were excluded from the final selection, reducing the number of applicable papers to 416. 

\xhdr{Inclusion and Exclusion Criteria}
To further refine the dataset, we assessed each paper for applicability using the inclusion and exclusion criteria found in Table~\ref{tab:inclusion_exclusion_criteria}.
After applying these criteria, we finalized a set of 226 tools for this analysis.

\begin{table}[]
\footnotesize
    \centering
    \caption{Inclusion and exclusion criteria for the literature review.}
    \begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}}
         \toprule
         \multicolumn{1}{c}{\textbf{Inclusion Criteria}} & \multicolumn{1}{c}{\textbf{Exclusion Criteria}} \\
         \midrule
\begin{itemize}
    \item Concrete and explicit references: We selected works that clearly stated the roles (e.g., developers, leaders) and/or lifecycle stages (e.g., data collection, model validation) addressed, for example, ``this tool is for developers to use during data collection.''
    \item Explicit methodology: We chose to focus on papers describing concrete implementation methods, such as code, questionnaires, case studies, or detailed guides.
    \item Clear responsible AI-related purpose: We selected works that were specifically designed and created for responsible AI.
    \item Language: We only selected works that were published in English.
\end{itemize}
         & 
\begin{itemize}
    \item Purely conceptual papers: We excluded works focusing solely on abstract principles or ethics without actionable insights.
    \item High-level guidance: We excluded works that relied on normative statements or recommendations lacking specific implementation details, for example, ``organizations should consider fairness.''
    \item Overly granular guidance: We excluded works that are too specific in focus, for example, those that are meant for single type of model in one stage of the AI lifecycle, a single organization, or a single method for explainability.
    \item Accessibility: We excluded papers not publicly available or tools only accessible upon special request. 
    \item Ambiguous roles: We excluded works using vague language such as ``AI practitioners'' or failing to identify the ``who'' and ``how'' of tool usage, if there was no clearly interpretable role or stage involved. 
    \item Roles external to organizational perspective: We excluded works that are not intended for leaders, designers, developers, deployers, end-users, or impacted communities (for example, works meant for governments or policy-makers). 
    % \Rachelmargincomment{Unsure if this is the best name}
\end{itemize} \\ 
         \bottomrule
    \end{tabular}
    \label{tab:inclusion_exclusion_criteria}
\end{table}

\subsection{The AI Lifecycle and Stakeholders}

Following previous work~\cite{black2023toward, NIST2023, Ortega2024} we considered the following stages of designing, developing, and deploying an AI system (Table~\ref{tab:stage_table}).
%Here is the link to the visualization slide, I am still messing with it. https://docs.google.com/presentation/d/16CKZeEffnZj98NqwQAAyibxPk0nl6WctMuiiylzCRqI/edit?usp=sharing 
Furthermore, we borrowed from the AI actors included in the NIST Risk Management Framework (NIST AI RMF)~\cite{NIST2023} to consider the AI system stakeholder groups found in Table \ref{tab:role_table}.

% \begin{figure}[]
% \includegraphics[width=\linewidth]{figs/stage_fig.png}
% \centering
% \caption{Lifecycle Stage Definitions}
% \label{fig:stage_fig}
% \end{figure}

\begin{table}[]
\centering
% \renewcommand{\arraystretch}{1.5} 
% \setlength{\tabcolsep}{5pt}  
\footnotesize
\caption{Definitions for major stages in the AI Lifecycle}
\begin{tabular}
{cp{0.75\linewidth}}
\toprule
\textbf{Stage}
 & \textbf{Definition} \\ \midrule
\textbf{Value Proposition}           &  Value Proposition refers to a series of early investigations into the problem the AI system is designed to solve, what the business and policy requirements are, and whether including an AI component within the broader decision-making system leads to net-benefit over the status quo. \\ \midrule
\textbf{Problem Formulation}         &  Problem Formulation refers to the translation of business needs and requirements to technical choices, what types of methodologies will be used, and how the overall system will incorporate the AI system.   \\ \midrule
\textbf{Data Collection}        &  Data collection involves selecting, collecting, or compiling data to train and/or validate an AI model, and it involves making choices---or implicitly accepting previously made choices---about how to sample, label, and link data.  \\ \midrule
\textbf{Data Processing}         &  Data processing refers to the steps taken to make data usable by the ML model---for example, cleaning, standardizing, or normalizing data, including feature engineering. \\ \midrule
\textbf{Statistical Modeling}    &  Statistical Modeling consists of deciding what type of statistical model(s) to fit to the data, and how to perform the fitting. The latter requires choosing the learning rule and loss function, regularizers, hyperparameters' values, model selection methodology, and model selection metrics to be used. \\ \midrule
\textbf{Testing} & Testing refers to the process of verifying that the AI system functions correctly and produces outputs as expected. This stage involves ensuring that the code is implemented properly, free of critical bugs, and capable of handling various input scenarios without errors. Testing focuses on the technical robustness of the system, such as confirming that the model generates predictions, processes data as intended, and operates reliably in different environments.   \\ \midrule
\textbf{Validation }        &  Validation goes beyond testing to assess whether the system's performance meets specific criteria or thresholds necessary for deployment. This process evaluates the model's accuracy, fairness, bias mitigation, and alignment with ethical and regulatory standards. \\ \midrule
\textbf{Deployment}         &  Deployment refers to the process of deploying the model into a larger decision system.  \\ \midrule
\textbf{Monitoring}         &  Monitoring refers to how a modelâ€™s behavior is recorded and responded to over time to ensure there is no significant or unexpected degradation in performance and use over time.  \\ \bottomrule
\end{tabular}
\label{tab:stage_table}
\end{table}

\begin{table}[]
\centering
% \renewcommand{\arraystretch}{1.5} 
% \setlength{\tabcolsep}{5pt}  
\caption{Role definitions for major stakeholders in the AI lifecycle}
\footnotesize
\begin{tabular}
{cp{0.75\linewidth}}
\toprule
\textbf{Role}
 & \textbf{Definition} \\ \midrule
\textbf{Leaders}           &  This role makes strategic decisions about the goals, objectives, and missions of the organization and the high-level policies to implement those goals. Typically, leaders are at the top of the organizational hierarchy or structure and include roles such as C-suite executives, and leaders of units or teams who are delegated specific responsibilities. In addition to C-suite executives, these roles can include senior management, unit or group leaders, policy leaders, or team leaders.  \\ \midrule
\textbf{Designers}         &  This role oversees the reframing of business and policy goals into a technical problem, aligning these problems with stakeholder needs in a particular context of use, and envisioning the user experience. Example roles include domain experts, AI designers, product managers, compliance experts, human factors experts, UX designers, and others who are familiar with doing cross-functional work.  \\ \midrule
\textbf{Developers}        &  This role uses programming or other technical knowledge to prepare, curate, and engineer data and to train AI models to perform the specified task. Example roles include data engineers, modelers, model engineers, data scientists, AI developers, software engineers, and systems engineers.  \\ \midrule
\textbf{Deployers}         &  The deployer role verifies and validates the model beyond the training and test data, pilots the model, checks compatibility with existing systems, and collaborates with designers to evaluate the user experience. Roles include testing and evaluation experts, auditors, impact assessors, and system integrators.   \\ \midrule
\textbf{End-users}         &  This role represents any individual who utilize the output of the AI model to contribute to an organizational goal (for example, making decisions or automating workflows and practices).  \\ \midrule
\textbf{Impacted Communities} & This stakeholder group represents any individual or community impacted directly or indirectly by the AI systemâ€™s operation and their advocates and representatives. Roles include groups that may be harmed by the model, advocacy groups, and the broader public.    \\ \bottomrule
\end{tabular}
\label{tab:role_table}
\end{table}

% \subsubsection{Organizational leaders.} This role makes strategic decisions about the goals, objectives, and missions of the organization and the high-level policies to implement those goals. Typically, leaders are at the top of the organizational hierarchy or structure and include roles such as C-suite executives, and leaders of units or teams who are delegated specific responsibilities. In addition to C-suite executives, these roles can include senior management, unit or group leaders, policy leaders, or team leaders.
% \Rachelmargincomment{We can cut back on the examples if we have space issues. Applies for other stakeholder groups as well.}

% \subsubsection{Designers.} This role oversees the reframing of business and policy goals into a technical problem, aligning these problems with stakeholder needs in a particular context of use, and envisioning the user experience. Example roles include domain experts, AI designers, product managers, compliance experts, human factors experts, UX designers, and others who are familiar with doing cross-functional work.

% \subsubsection{Developers.} This role uses programming or other technical knowledge to prepare, curate, and engineer data and to train AI models to perform the specified task. Example roles include data engineers, modelers, model engineers, data scientists, AI developers, software engineers, and systems engineers.

% \subsubsection{Deployers.} The deployer role verifies and validates the model beyond the training and test data, pilots the model, checks compatibility with existing systems, and collaborates with designers to evaluate the user experience. Roles include testing and evaluation experts, auditors, impact assessors, and system integrators. 
% \Rachelmargincomment{I removed the description of operators for now, since we don't include it in our matrix}

% The operator role oversees and monitors the application of the model in deployment and its relationship to the larger organizational system. Often, operators combine human expertise with AI technology as they apply their skills to optimize operations and enhance the efficiency of the model. They may or may not have a technical background. 

% \subsubsection{End-users.} This role represents any individual who utilize the output of the AI model to contribute to an organizational goal (for example, making decisions or automating workflows and practices). 
% In some cases, data may be collected about this actorâ€™s behavior to train or refine the AI model.

% \subsubsection{Impacted communities.} This stakeholder group represents any individual or community impacted directly or indirectly by the AI systemâ€™s operation and their advocates and representatives. Roles include groups that may be harmed by the model, advocacy groups, and the broader public.  
% Responsible use of AI should ensure that impacted individuals and communities are not subjected to undue harm and risksâ€”which should be defined concerning the use case at hand.

% \subsection{Categorization}
%  This process highlights both the abundance and the dearth of tools associated with a given stage and stakeholder role.
% This matrix serves as a key tool for researchers and practitioners to assess whether each stage of the AI lifecycle has tools and processes available for the major decisions that must be made in a way that promotes responsible AI principles and practices, namely, the safe, effectively, reliable, and equitable functioning of the AI system. 
% This process allowed us to begin exploring the available tools and frameworks for every intersection. From this we identify possible gaps in the literature and areas where there was an abundance of available tools and resources, both for roles and associated stages. 

\subsection{Exploratory Data Analysis}
After applying the inclusion and exclusion criteria, we began to assess the final set of papers according to the roles and lifecycle stages they addressed. 
Note that we chose to focus on the lifecycle stages that are \emph{addressed} rather than the stages where the tool is \emph{used}; for instance, although model cards~\cite{Mitchell2019} are relevant to end-users after deployment, they address ``statistical modeling.''
Taking the stakeholder groups as rows and the stages as columns, we constructed a matrix, organizing the RAI tools into the rows and columns they address.
Our goal was to understand the relative abundance or lack of lifecycle stages and roles addressed in responsible AI governance. 
% To that end, we began categorizing the papers by developing a table of the roles and lifecycles stages that were addressed by each paper.

We were also interested in exploring whether the tools introduced in the literature were validated in any way for usability or effectiveness before publishing. Specifically, we evaluated tools and processes to determine whether they included hypothetical case studies,
experimental results, pilot programs, or even reports of results in real-world use.

To ensure consistency in labeling, two of the co-authors independently labeled a small subset of the dataset, focusing on lifecycle stages, roles, and validation criteria. Overall, this resulted in a Krippendorff's alpha score of .883 for stage labels and .824 for role labels, indicating substantial agreement. With these values, we were confident in our alignment and we moved forward with labeling the rest of the data individually, maintaining frequent check-ins to address any uncertainties.

\subsection{Limitations}
% Possible biases in literature collection.\\
% Constraints in defining roles or lifecycle stages.
Despite its contributions, the methodology used in this literature review has some limitations. First, potential biases in the literature collection may have influenced our findings. Our categorization of papers and tools relies on the availability and accessibility of published work, which may overrepresent well-funded research areas or regions. Similarly, our definitions of lifecycle stages and roles, while informed by existing literature
%, are necessarily constrained by our frameworkâ€™s scope and 
may not capture the full complexity of real-world AI development practices.

Second, the boundaries between stages and roles can be fluid, complicating efforts to assign responsibilities and tools to specific cells in our matrix. For example, developers often contribute to problem formulation and validation, even if these are not traditionally considered part of their primary responsibilities. Similarly, stages such as deployment and monitoring often overlap, particularly in iterative development processes.