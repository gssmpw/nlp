\section{Related Work}
\paragraph{LGT and HGT Detection.}
Past efforts to identify LLM-generated texts (LGTs) often relied on binary classification systems that distinguish human-generated texts (HGTs) from LGTs using surface-level features. While these methods were initially effective, they are prone to errors when encountering adversarial attacks or domain shifts, which limit their overall robustness~\cite{bhattacharjee2024fighting, dugan2024raid}. To address these limitations, researchers have explored strategies that integrate external knowledge, such as combining internal and external factual structures, to boost detection against diverse content and styles~\cite{ideate2024}. Recent studies also highlight the promise of using the LLMs themselves for text detection: approaches like self-detection and mutual detection can outperform traditional classifiers, as illustrated by GPT-4's success in tasks like plagiarism detection~\cite{plagbench2024}. Notably, smaller models sometimes excel in zero-shot scenarios, offering adaptable solutions across varying architectures~\cite{smallerLLM2024}. Furthermore, \citet{lee2023do} demonstrated that LLMs can reliably identify their own outputs, providing a more nuanced framework for content verification. Despite these advances, the continuing challenges of domain adaptation and adversarial resistance underscore the need for more versatile and robust detection systems.



\textbf{Explainability in Detection Models.}
Recent work on LGT detection has focused on improving explainability. \citet{zhou2024humanizing} proposed to incorporate factual consistency into detection models to enhance their interpretability, while~\citet{weng2024understanding} explored mixed-initiative approaches that combine human expertise with automated models for better detection. These studies have made significant contributions to the field; however, they either depend heavily on expert input~\cite{weng2024understanding} or lack integration of explanation generation within the model itself~\cite{zhou2024humanizing}. Our approach, in contrast, enables LLMs to autonomously generate both predictions and detailed explanations, making it a more scalable and transparent solution for detecting machine-generated content.

\textbf{Ternary Classification.}
Traditional binary classification methods, distinguishing between LGTs and HGTs, face limitations when texts exhibit characteristics that are ambiguous or could plausibly originate from either source. In such cases, adding an ``Undecided'' category provides a more nuanced framework for detection. This approach has several advantages over previous methods, which typically focus on mixed human-machine texts and fail to account for uncertainties in text origin. For instance, the work by~\citet{lee2024llm} revealed that current detectors struggle to identify ``mixed text'', particularly in cases where subtle modifications and style adaptability blur the distinction. Similarly, the Turing test conducted on online reviews~\cite{turingtest2024} showed that even humans often cannot reliably distinguish between human-written and AI-generated content, pointing to the inherent difficulties in binary classification. Moreover, studies such as~\cite{turingtest2024} and \cite{coling2024} further underscore the complexity in distinguishing between the two, suggesting that an ``Undecided'' class could allow for better handling of such ambiguity. Incorporating this third category not only enhances detection accuracy but also improves explainability, offering clearer insights into the decision-making process.