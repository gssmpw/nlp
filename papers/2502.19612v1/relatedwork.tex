\section{Related Work}
In recent years, NLP and large language models have improved hate speech detection. LLMs have been contributing to enhancing both performance and interpretability in hate speech detection. Models employ different techniques like rationale extraction to identify and highlight elements, i.e., derogatory words or contextual reasoning behind classifications \cite{nirmal2024towards}.

Different prompting strategies, e.g., general prompts, definition-based prompts, few-shot learning, and chain-of-thought (CoT) were explored for detecting hate speech. Study shows prompting strategy played a critical role in reasoning and significantly improved contextual understanding and classification accuracy \cite{guo2023investigation}. With these strategies, LLMs can effectively use their extensive pre-trained knowledge, even when fine-tuning is not possible. CoT reasoning, in particular, has been shown to enhance the modelâ€™s ability to detect implicit hate speech by breaking down complex tasks into logical steps \cite{yang2023hare}. Zero-shot learning approaches using instruction-tuned LLMs have demonstrated equal or better performance compared to the fine-tuned models in low-resource settings. \cite{plaza2023respectful} It suggests the careful selection of verbalizers and prompts helps the models to work with a wide range of datasets and languages when the labeled data is scarce. 
\cite{yang2023hare} emphasize the importance of explainability techniques in the context of LLM-based hate speech detection. CoT reasoning offers detailed, step-by-step rationales for predictions. By combining explanations generated by machines with annotations made by humans, models are able to achieve better alignment with the requirements that are uniquely associated with the dataset. This hybrid approach shows improved classification accuracy and interpretability.
\cite{roy2023probing} 


Recently, researchers have focused on fine-tuning LLMs with smaller parameters, which shows promising results on the English hate speech dataset \cite{sen2024hatetinyllmhatespeech}. Vision-based models have also been used to detect hate speech from images and their captions, keeping the cultural context in mind \cite{bui2024multi3hatemultimodalmultilingualmulticultural}. However, though the current research provides significant insights into hate speech detection, minimal attention has been given to the evaluation of LLMs on hate speech detection, keeping the geographical context in mind. Our study aims to bridge the gap in LLM's hate speech evaluation while incorporating geographical information in our evaluation process. This focus will ensure the future directions and challenges in addressing hate speech detection with LLMs across multiple languages and regions.