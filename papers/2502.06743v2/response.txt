\section{Related Work}
In the context of network traffic prediction, FL allows distributed training between multiple clients, with each client having access to the data of a different operator**McMahan et al., "Communication-Efficient Learning of Deep Networks from Decentralized Data"**. This enables the creation of a centralized predictive traffic demand matrix (e.g., formed in a cloud orchestrator), enabling the proactive (re)optimization of shared network resources**with increased privacy and security**. In fact, distributed learning**Konečnỳ et al., "Federated Learning: Strategies for Improving Communication Efficiency"** and traditional FL, is previously considered in**Li et al., "Federated Learning"**, where the ability of FL to achieve performance accuracies comparable to the centralized learning paradigm is demonstrated. However, in those works, federated datasets are assumed to be independent and identically distributed (iid), largely ignoring the fact that the various traffic sources may be heterogeneous.% Such an attribute is very important in the DT context, as diverse operators supporting different services and applications may generate non-iid datasets. 

In the presence of heterogeneous traffic (i.e., imbalanced and non-iid datasets), traditional FL fails to achieve sufficiently high accuracy across all clients, since it focuses on minimizing a global (aggregated) loss function across participating clients, which may lead to a disproportionate advantage (or disadvantage) for some of the clients. In other words, traditional FL does not inherently account for achieving fair training accuracy across the different clients; rather, its target is to maximize the aggregated accuracy over all clients.

%Thus, a challenge in traditional FL is ensuring fairness—both in terms of collaborative training contributions from heterogeneous clients and in mitigating bias in model predictions with respect to sensitive attributes.

To address this challenge, a fair FL approach is considered that introduces a mechanism to enhance fairness by focusing on the performance of clients with higher empirical losses. In general, fairness-aware ML has recently attracted growing attention**Dwork et al., "A Study of Bias in Machine Learning Models"**. Common approaches include pre-processing the data to remove sensitive information or post-processing the model by adjusting the prediction thresholds after training**Hardt et al., "Equality of Opportunity in Supervised Learning"**. Another set of works focuses on optimizing the learning objective under fairness constraints during training**Zafar et al., "Fairness-Aware Classifier with Priorities"**.