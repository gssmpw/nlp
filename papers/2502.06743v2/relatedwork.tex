\section{Related Work}
In the context of network traffic prediction, FL allows distributed training between multiple clients, with each client having access to the data of a different operator~\cite{sepasgozar2022fed,li2024core}. This enables the creation of a centralized predictive traffic demand matrix (e.g., formed in a cloud orchestrator), enabling the proactive (re)optimization of shared network resources~\cite{9748600} with increased privacy and security. In fact, distributed learning~\cite{Chen:19} and traditional FL, is previously considered in~\cite{behera2024federated,drainakis2023centralized,9306745,9748424,asad2021federated,9605846}, where the ability of FL to achieve performance accuracies comparable to the centralized learning paradigm is demonstrated. However, in those works, federated datasets are assumed to be independent and identically distributed (iid), largely ignoring the fact that the various traffic sources may be heterogeneous.% Such an attribute is very important in the DT context, as diverse operators supporting different services and applications may generate non-iid datasets. 

In the presence of heterogeneous traffic (i.e., imbalanced and non-iid datasets), traditional FL fails to achieve sufficiently high accuracy across all clients, since it focuses on minimizing a global (aggregated) loss function across participating clients, which may lead to a disproportionate advantage (or disadvantage) for some of the clients. In other words, traditional FL does not inherently account for achieving fair training accuracy across the different clients; rather, its target is to maximize the aggregated accuracy over all clients.

%Thus, a challenge in traditional FL is ensuring fairnessâ€”both in terms of collaborative training contributions from heterogeneous clients and in mitigating bias in model predictions with respect to sensitive attributes.

To address this challenge, a fair FL approach is considered that introduces a mechanism to enhance fairness by focusing on the performance of clients with higher empirical losses. In general, fairness-aware ML has recently attracted growing attention~\cite{ezzeldin2023fairfed,salazar2023fair,li2019fair}. Common approaches include pre-processing the data to remove sensitive information or post-processing the model by adjusting the prediction thresholds after training~\cite{hardt2016equality,calmon2017optimized}. Another set of works focuses on optimizing the learning objective under fairness constraints during training \cite{agarwal2018reductions,hashimoto2018fairness,cotter2019optimization}. 

%\vspace{-0.15in}