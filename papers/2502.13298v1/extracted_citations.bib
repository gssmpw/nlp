@inproceedings{AutoDroid,
author = {Wen, Hao and Li, Yuanchun and Liu, Guohong and Zhao, Shanhui and Yu, Tao and Li, Toby Jia-Jun and Jiang, Shiqi and Liu, Yunhao and Zhang, Yaqin and Liu, Yunxin},
title = {AutoDroid: LLM-powered Task Automation in Android},
year = {2024},
isbn = {9798400704895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636534.3649379},
doi = {10.1145/3636534.3649379},
abstract = {Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smartphones. However, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or endusers. The recent advance of large language models (LLMs) in language understanding and reasoning inspires us to rethink the problem from a model-centric perspective, where task preparation, comprehension, and execution are handled by a unified language model. In this work, we introduce AutoDroid, a mobile task automation system capable of handling arbitrary tasks on any Android application without manual efforts. The key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis. The main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection techniques that augment the app-specific domain knowledge of LLM, and a multi-granularity query optimization module that reduces the cost of model inference. We integrate AutoDroid with off-the-shelf LLMs including online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a new benchmark for memory-augmented Android task automation with 158 common tasks. The results demonstrated that AutoDroid is able to precisely generate actions with an accuracy of 90.9\%, and complete tasks with a success rate of 71.3\%, outperforming the GPT-4-powered baselines by 36.4\% and 39.7\%.},
booktitle = {Proceedings of the 30th Annual International Conference on Mobile Computing and Networking},
pages = {543–557},
numpages = {15},
keywords = {task automation, large language models, app analysis},
location = {Washington D.C., DC, USA},
series = {ACM MobiCom '24}
}

@INPROCEEDINGS{EckertUser,
  author={Eckert, W. and Levin, E. and Pieraccini, R.},
  booktitle={1997 IEEE Workshop on Automatic Speech Recognition and Understanding Proceedings}, 
  title={User modeling for spoken dialogue system evaluation}, 
  year={1997},
  volume={},
  number={},
  pages={80-87},
  keywords={Speech analysis;Speech recognition;Performance evaluation;Manuals;Stochastic systems;System testing;Engineering management;Art;Signal generators;Optimal control},
  doi={10.1109/ASRU.1997.658991}}

@article{LEE2009466,
title = {Example-based dialog modeling for practical multi-domain dialog system},
journal = {Speech Communication},
volume = {51},
number = {5},
pages = {466-484},
year = {2009},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2009.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167639309000107},
author = {Cheongjae Lee and Sangkeun Jung and Seokhwan Kim and Gary Geunbae Lee},
keywords = {Example-based dialog modeling, Generic dialog modeling, Multi-domain dialog system, Domain identification},
abstract = {This paper proposes a generic dialog modeling framework for a multi-domain dialog system to simultaneously manage goal-oriented and chat dialogs for both information access and entertainment. We developed a dialog modeling technique using an example-based approach to implement multiple applications such as car navigation, weather information, TV program guidance, and chatbot. Example-based dialog modeling (EBDM) is a simple and effective method for prototyping and deploying of various dialog systems. This paper also introduces the system architecture of multi-domain dialog systems using the EBDM framework and the domain spotting technique. In our experiments, we evaluate our system using both simulated and real users. We expect that our approach can support flexible management of multi-domain dialogs on the same framework.}
}

@article{Mi_Wang_Li_2022, title={CINS: Comprehensive Instruction for Few-Shot Learning in Task-Oriented Dialog Systems}, volume={36}, url={https://ojs.aaai.org/index.php/AAAI/article/view/21356}, DOI={10.1609/aaai.v36i10.21356}, abstractNote={As the labeling cost for different modules in task-oriented dialog (ToD) systems is high, a major challenge is to learn different tasks with the least amount of labeled data. Recently, pre-trained language models (PLMs) have shown promising results for few-shot learning in ToD. To better utilize the power of PLMs, this paper proposes Comprehensive Instruction (CINS) that exploits PLMs with extra task-specific instructions. We design a schema (definition, constraint, prompt) of instructions and their customized realizations for three important downstream tasks in ToD, ie. intent classification, dialog state tracking, and natural language generation. A sequence-to-sequence model (T5) is adopted to solve these three tasks in a unified framework. Extensive experiments are conducted on these ToD tasks in realistic few-shot learning scenarios with small validation data. Empirical results demonstrate that the proposed CINS approach consistently improves techniques that finetune PLMs with raw input or short prompt.}, number={10}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Mi, Fei and Wang, Yasheng and Li, Yitong}, year={2022}, month={Jun.}, pages={11076-11084} }

@inproceedings{NEURIPS2022_82ad13ec,
    author = {Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
    pages = {20744--20757},
    publisher = {Curran Associates, Inc.},
    title = {WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents},
    url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/82ad13ec01f9fe44c01cb91814fd7b8c-Paper-Conference.pdf},
    volume = {35},
    year = {2022}
}

@inproceedings{SiddiqueTOD,
author = {Siddique, A.B. and Maqbool, M.H. and Taywade, Kshitija and Foroosh, Hassan},
title = {Personalizing Task-oriented Dialog Systems via Zero-shot Generalizable Reward Function},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557417},
doi = {10.1145/3511808.3557417},
abstract = {Task-oriented dialog systems enable users to accomplish tasks using natural language. State-of-the-art systems respond to users in the same way regardless of their personalities, although personalizing dialogues can lead to higher levels of adoption and better user experiences. Building personalized dialog systems is an important, yet challenging endeavor, and only a handful of works took on the challenge. Most existing works rely on supervised learning approaches and require laborious and expensive labeled training data for each user profile. Additionally, collecting and labeling data for each user profile is virtually impossible. In this work, we propose a novel framework, P-ToD, to personalize task-oriented dialog systems capable of adapting to a wide range of user profiles in an unsupervised fashion using a zero-shot generalizable reward function. P-ToD uses a pre-trained GPT-2 as a backbone model and works in three phases. Phase one performs task-specific training. Phase two kicks off unsupervised personalization by leveraging the proximal policy optimization algorithm that performs policy gradients guided by the zero-shot generalizable reward function. Our novel reward function can quantify the quality of the generated responses even for unseen profiles. The optional final phase fine-tunes the personalized model using a few labeled training examples. We conduct extensive experimental analysis using the personalized bAbI dialogue benchmark for five tasks and up to 180 diverse user profiles. The experimental results demonstrate that P-ToD, even when it had access to zero labeled examples, outperforms state-of-the-art supervised personalization models and achieves competitive performance on BLEU and ROUGE metrics when compared to a strong fully-supervised GPT-2 baseline.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
pages = {1787–1797},
numpages = {11},
keywords = {dialog systems, personalization, reinforcement learning, zero-shot learning},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{UnravelingChatGPT,
  author={Tiziano Labruna and Sofia Brenna and Andrea Zaninello and Bernardo Magnini},
  title={Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations},
  year={2023},
  cdate={1672531200000},
  pages={151-171},
  url={https://doi.org/10.1007/978-3-031-47546-7_11},
  booktitle={AI*IA}
}

@article{WILLIAMS2007393,
title = {Partially observable Markov decision processes for spoken dialog systems},
journal = {Computer Speech \& Language},
volume = {21},
number = {2},
pages = {393-422},
year = {2007},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2006.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0885230806000283},
author = {Jason D. Williams and Steve Young},
keywords = {Spoken dialog system, Dialog management, Planning under uncertainty, User modelling, Markov decision processes, Decision theory},
abstract = {In a spoken dialog system, determining which action a machine should take in a given situation is a difficult problem because automatic speech recognition is unreliable and hence the state of the conversation can never be known with certainty. Much of the research in spoken dialog systems centres on mitigating this uncertainty and recent work has focussed on three largely disparate techniques: parallel dialog state hypotheses, local use of confidence scores, and automated planning. While in isolation each of these approaches can improve action selection, taken together they currently lack a unified statistical framework that admits global optimization. In this paper we cast a spoken dialog system as a partially observable Markov decision process (POMDP). We show how this formulation unifies and extends existing techniques to form a single principled framework. A number of illustrations are used to show qualitatively the potential benefits of POMDPs compared to existing techniques, and empirical results from dialog simulations are presented which demonstrate significant quantitative gains. Finally, some of the key challenges to advancing this method – in particular scalability – are briefly outlined.}
}

@article{bai2024digirl,
    title={DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning},
    author={Bai, Hao and Zhou, Yifei and Cemri, Mert and Pan, Jiayi and Suhr, Alane and Levine, Sergey and Kumar, Aviral},
    journal={arXiv preprint arXiv:2406.11896},
    year={2024}
}

@inproceedings{chen-etal-2019-semantically,
    title = "Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention",
    author = "Chen, Wenhu  and
      Chen, Jianshu  and
      Qin, Pengda  and
      Yan, Xifeng  and
      Wang, William Yang",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1360/",
    doi = "10.18653/v1/P19-1360",
    pages = "3696--3709",
    abstract = "Semantically controlled neural response generation on limited-domain has achieved great performance. However, moving towards multi-domain large-scale scenarios are shown to be difficult because the possible combinations of semantic inputs grow exponentially with the number of domains. To alleviate such scalability issue, we exploit the structure of dialog acts to build a multi-layer hierarchical graph, where each act is represented as a root-to-leaf route on the graph. Then, we incorporate such graph structure prior as an inductive bias to build a hierarchical disentangled self-attention network, where we disentangle attention heads to model designated nodes on the dialog act graph. By activating different (disentangled) heads at each layer, combinatorially many dialog act semantics can be modeled to control the neural response generation. On the large-scale Multi-Domain-WOZ dataset, our model can yield a significant improvement over the baselines on various automatic and human evaluation metrics."
}

@inproceedings{cheng-etal-2022-multiwoz,
    title = "Is {M}ulti{WOZ} a Solved Task? An Interactive {TOD} Evaluation Framework with User Simulator",
    author = "Cheng, Qinyuan  and
      Li, Linyang  and
      Quan, Guofeng  and
      Gao, Feng  and
      Mou, Xiaofeng  and
      Qiu, Xipeng",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.90/",
    doi = "10.18653/v1/2022.findings-emnlp.90",
    pages = "1248--1259",
    abstract = "Task-Oriented Dialogue (TOD) systems are drawing more and more attention in recent studies.Current methods focus on constructing pre-trained models or fine-tuning strategies while the evaluation of TOD is limited by a policy mismatch problem.That is, during evaluation, the user utterances are from the annotated dataset while these utterances should interact with previous responses which can have many alternatives besides annotated texts.Therefore, in this work, we propose an interactive evaluation framework for TOD. We first build a goal-oriented user simulator based on pre-trained models and then use the user simulator to interact with the dialogue system to generate dialogues.Besides, we introduce a sentence-level and a session-level score to measure the sentence fluency and session coherence in the interactive evaluation. Experimental results show that RL-based TOD systems trained by our proposed user simulator can achieve nearly 98{\%} inform and success rates in the interactive evaluation of MultiWOZ dataset and the proposed scores measure the response quality besides the inform and success rates.We are hoping that our work will encourage simulator-based interactive evaluations in the TOD task."
}

@inproceedings{chung-etal-2023-instructtods,
    title = "{I}nstruct{TODS}: Large Language Models for End-to-End Task-Oriented Dialogue Systems",
    author = "Chung, Willy  and
      Cahyawijaya, Samuel  and
      Wilie, Bryan  and
      Lovenia, Holy  and
      Fung, Pascale",
    editor = "Chen, Kehai  and
      Ku, Lun-Wei",
    booktitle = "Proceedings of the Second Workshop on Natural Language Interfaces",
    month = nov,
    year = "2023",
    address = "Bali, Indonesia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.nlint-1.1/",
    doi = "10.18653/v1/2023.nlint-1.1",
    pages = "1--21"
}

@article{davidson2023user,
  title={User simulation with large language models for evaluating task-oriented dialogue},
  author={Davidson, Sam and Romeo, Salvatore and Shu, Raphael and Gung, James and Gupta, Arshit and Mansour, Saab and Zhang, Yi},
  journal={arXiv preprint arXiv:2309.13233},
  year={2023}
}

@inproceedings{feng-etal-2023-schema,
    title = "Schema-Guided User Satisfaction Modeling for Task-Oriented Dialogues",
    author = "Feng, Yue  and
      Jiao, Yunlong  and
      Prasad, Animesh  and
      Aletras, Nikolaos  and
      Yilmaz, Emine  and
      Kazai, Gabriella",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.116/",
    doi = "10.18653/v1/2023.acl-long.116",
    pages = "2079--2091",
    abstract = "User Satisfaction Modeling (USM) is one of the popular choices for task-oriented dialogue systems evaluation, where user satisfaction typically depends on whether the user`s task goals were fulfilled by the system. Task-oriented dialogue systems use task schema, which is a set of task attributes, to encode the user`s task goals. Existing studies on USM neglect explicitly modeling the user`s task goals fulfillment using the task schema. In this paper, we propose SG-USM, a novel schema-guided user satisfaction modeling framework. It explicitly models the degree to which the user`s preferences regarding the task attributes are fulfilled by the system for predicting the user`s satisfaction level. SG-USM employs a pre-trained language model for encoding dialogue context and task attributes. Further, it employs a fulfillment representation layer for learning how many task attributes have been fulfilled in the dialogue, an importance predictor component for calculating the importance of task attributes. Finally, it predicts the user satisfaction based on task attribute fulfillment and task attribute importance. Experimental results on benchmark datasets (i.e. MWOZ, SGD, ReDial, and JDDC) show that SG-USM consistently outperforms competitive existing methods. Our extensive analysis demonstrates that SG-USM can improve the interpretability of user satisfaction modeling, has good scalability as it can effectively deal with unseen tasks and can also effectively work in low-resource settings by leveraging unlabeled data. Code is available at \url{https://github.com/amzn/user-satisfaction-modeling}."
}

@inproceedings{fereidouni-etal-2024-grounded,
    title = "Grounded Language Agent for Product Search via Intelligent Web Interactions",
    author = "Fereidouni, Moghis  and
      Mosharrof, Adib  and
      Siddique, A.b.",
    editor = "Kumar, Sachin  and
      Balachandran, Vidhisha  and
      Park, Chan Young  and
      Shi, Weijia  and
      Hayati, Shirley Anugrah  and
      Tsvetkov, Yulia  and
      Smith, Noah  and
      Hajishirzi, Hannaneh  and
      Kang, Dongyeop  and
      Jurgens, David",
    booktitle = "Proceedings of the 1st Workshop on Customizable NLP: Progress and Challenges in Customizing NLP for a Domain, Application, Group, or Individual (CustomNLP4U)",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.customnlp4u-1.7/",
    doi = "10.18653/v1/2024.customnlp4u-1.7",
    pages = "63--75",
    abstract = "Recent research has focused on developing agents powered by large language models (LLMs) to accomplish complex high-level user intents. However, employing LLMs with billions of parameters (e.g., GPT-4) may incur substantial costs on top of handcrafting extensive prompts. To address this, we introduce a Grounded Language Agent for Intelligent Web Interactions, named GLAINTEL. GLAINTEL employs Flan-T5 as its backbone and is flexible in training in various settings: unsupervised learning, supervised learning, and unsupervised domain adaptation. Specifically, we tackle both the challenge of learning without human demonstrations and the opportunity to leverage human demonstrations effectively when those are available. Additionally, we explore unsupervised domain adaptation for cases where demonstrations are limited to a specific domain. Experimental evaluations across diverse setups demonstrate the effectiveness of GLAINTEL in unsupervised settings, outperforming in-context learning-based approaches that employ larger models with up to 540 billion parameters. Surprisingly, behavioral cloning-based methods that straightforwardly use human demonstrations do not outperform unsupervised variants of GLAINTEL. Additionally, we show that combining human demonstrations with reinforcement learning-based training yields results comparable to methods utilizing GPT-4. The code is available at: https://github.com/MultifacetedNLP/Web-Agents-Unsupervised"
}

@inproceedings{gao-etal-2023-adaptive,
    title = "An Adaptive Prompt Generation Framework for Task-oriented Dialogue System",
    author = "Gao, Jun  and
      Xiang, Liuyu  and
      Wu, Huijia  and
      Zhao, Han  and
      Tong, Yiqi  and
      He, Zhaofeng",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.76/",
    doi = "10.18653/v1/2023.findings-emnlp.76",
    pages = "1078--1089",
    abstract = "The de facto way of utilizing black-box large language models (LLMs) to perform various downstream tasks is prompting. However, obtaining suitable prompts for specific tasks is still a challenging problem. While existing LLM-based methods demonstrate promising performance in task-oriented dialogue (TOD) task, they often require manual adjustment in prompt selection, or focus solely on dialogue understanding or generation. To address these issues, we propose an adaptive prompt generation framework to fully unleash the potential of LLMs for the comprehensive TOD system. Firstly, we design a trainable slot generator (TSG) that can generate domain and slot information in the belief state, which serves as prior knowledge for subsequent prompt generation. Next, we propose an adaptive prompt generator (APG) that utilizes the prior knowledge to generate prompts for the LLM, deriving the belief state and system response of the dialogue for evaluation. Finally, we evaluate our framework on the MultiWOZ 2.0 dataset. Extensive experiments demonstrate that our method outperforms existing methods. Our code and data will be released."
}

@inproceedings{gupta-etal-2022-show,
    title = "Show, Don`t Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue",
    author = "Gupta, Raghav  and
      Lee, Harrison  and
      Zhao, Jeffrey  and
      Cao, Yuan  and
      Rastogi, Abhinav  and
      Wu, Yonghui",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.336/",
    doi = "10.18653/v1/2022.naacl-main.336",
    pages = "4541--4549",
    abstract = "Building universal dialogue systems that operate across multiple domains/APIs and generalize to new ones with minimal overhead is a critical challenge. Recent works have leveraged natural language descriptions of schema elements to enable such systems; however, descriptions only indirectly convey schema semantics. In this work, we propose Show, Don`t Tell, which prompts seq2seq models with a labeled example dialogue to show the semantics of schema elements rather than tell the model through descriptions. While requiring similar effort from service developers as generating descriptions, we show that using short examples as schema representations with large language models results in state-of-the-art performance on two popular dialogue state tracking benchmarks designed to measure zero-shot generalization - the Schema-Guided Dialogue dataset and the MultiWOZ leave-one-out benchmark."
}

@article{hosseini2020simple,
  title={A simple language model for task-oriented dialogue},
  author={Hosseini-Asl, Ehsan and McCann, Bryan and Wu, Chien-Sheng and Yavuz, Semih and Socher, Richard},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20179--20191},
  year={2020}
}

@inproceedings{hudecek-dusek-2023-large,
    title = "Are Large Language Models All You Need for Task-Oriented Dialogue?",
    author = "Hude{\v{c}}ek, Vojt{\v{e}}ch  and
      Dusek, Ondrej",
    editor = "Stoyanchev, Svetlana  and
      Joty, Shafiq  and
      Schlangen, David  and
      Dusek, Ondrej  and
      Kennington, Casey  and
      Alikhani, Malihe",
    booktitle = "Proceedings of the 24th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
    month = sep,
    year = "2023",
    address = "Prague, Czechia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.sigdial-1.21/",
    doi = "10.18653/v1/2023.sigdial-1.21",
    pages = "216--228",
    abstract = "Instruction-finetuned large language models (LLMs) gained a huge popularity recently, thanks to their ability to interact with users through conversation. In this work, we aim to evaluate their ability to complete multi-turn tasks and interact with external databases in the context of established task-oriented dialogue benchmarks. We show that in explicit belief state tracking, LLMs underperform compared to specialized task-specific models. Nevertheless, they show some ability to guide the dialogue to a successful ending through their generated responses if they are provided with correct slot values. Furthermore, this ability improves with few-shot in-domain examples."
}

@inproceedings{imrattanatrai-fukuda-2023-end,
    title = "End-to-End Task-Oriented Dialogue Systems Based on Schema",
    author = "Imrattanatrai, Wiradee  and
      Fukuda, Ken",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.645/",
    doi = "10.18653/v1/2023.findings-acl.645",
    pages = "10148--10161",
    abstract = "This paper presents a schema-aware end-to-end neural network model for handling task-oriented dialogues based on a dynamic set of slots within a schema. Contrary to existing studies that proposed end-to-end approaches for task-oriented dialogue systems by relying on a unified schema across domains, we design our approach to support a domain covering multiple services where diverse schemas are available. To enable better generalizability among services and domains with different schemas, we supply the schema`s context information including slot descriptions and value constraints to the model. The experimental results on a well-known Schema-Guided Dialogue (SGD) dataset demonstrated the performance improvement by the proposed model compared to state-of-the-art baselines in terms of end-to-end modeling, dialogue state tracking task, and generalization on new services and domains using a limited number of dialogues."
}

@inproceedings{kim-etal-2024-prospector,
    title = "Prospector: Improving {LLM} Agents with Self-Asking and Trajectory Ranking",
    author = "Kim, Byoungjip  and
      Jang, Youngsoo  and
      Logeswaran, Lajanugen  and
      Kim, Geon-Hyeong  and
      Kim, Yu Jin  and
      Lee, Honglak  and
      Lee, Moontae",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.879/",
    doi = "10.18653/v1/2024.findings-emnlp.879",
    pages = "14958--14976",
    abstract = "Large language models (LLMs) have shown the ability to solve complex decision-making tasks beyond natural language processing tasks. LLM agents based on few-shot in-context learning (ICL) achieve surprisingly high performance without training. Despite their simplicity and generalizability, ICL-based agents are limited in their ability to incorporate feedback from an environment. In this paper, we introduce Prospector, an LLM agent that consists of two complementary LLMs, an Actor and a Critic. To elicit better instruction-aligned actions from the LLM agent, we propose AskAct prompting that performs an additional self-asking step such as goal and progress checking before generating an action. Furthermore, to implicitly incorporate the environment feedback, we propose Trajectory Ranking that orders generated trajectories by predicting the expected total reward. Prospector encourages the LLM Actor to generate diverse (creative) trajectories, and harnesses the LLM Critic to select the most rewarding trajectory. On representative decision-making benchmark environments such as ALFWorld and WebShop, we empirically demonstrate that Prospector can considerably increase the success rate of given tasks, while outperforming recent advancements such as ReAct and Reflexion."
}

@inproceedings{lee-2013-structured,
    title = "Structured Discriminative Model For Dialog State Tracking",
    author = "Lee, Sungjin",
    editor = "Eskenazi, Maxine  and
      Strube, Michael  and
      Di Eugenio, Barbara  and
      Williams, Jason D.",
    booktitle = "Proceedings of the {SIGDIAL} 2013 Conference",
    month = aug,
    year = "2013",
    address = "Metz, France",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W13-4069/",
    pages = "442--451"
}

@article{lee2023explore,
  title={Explore, select, derive, and recall: Augmenting llm with human-like memory for mobile task automation},
  author={Lee, Sunjae and Choi, Junyoung and Lee, Jungjae and Wasi, Munim Hasan and Choi, Hojun and Ko, Steven Y and Oh, Sangeun and Shin, Insik},
  journal={arXiv preprint arXiv:2312.03003},
  year={2023}
}

@inproceedings{lei-etal-2018-sequicity,
    title = "{S}equicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures",
    author = "Lei, Wenqiang  and
      Jin, Xisen  and
      Kan, Min-Yen  and
      Ren, Zhaochun  and
      He, Xiangnan  and
      Yin, Dawei",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1133/",
    doi = "10.18653/v1/P18-1133",
    pages = "1437--1447",
    abstract = "Existing solutions to task-oriented dialogue systems follow pipeline designs which introduces architectural complexity and fragility. We propose a novel, holistic, extendable framework based on a single sequence-to-sequence (seq2seq) model which can be optimized with supervised or reinforcement learning. A key contribution is that we design text spans named belief spans to track dialogue believes, allowing task-oriented dialogue systems to be modeled in a seq2seq way. Based on this, we propose a simplistic Two Stage CopyNet instantiation which emonstrates good scalability: significantly reducing model complexity in terms of number of parameters and training time by a magnitude. It significantly outperforms state-of-the-art pipeline-based methods on large datasets and retains a satisfactory entity match rate on out-of-vocabulary (OOV) cases where pipeline-designed competitors totally fail."
}

@inproceedings{lin-etal-2020-mintl,
    title = "{M}in{TL}: Minimalist Transfer Learning for Task-Oriented Dialogue Systems",
    author = "Lin, Zhaojiang  and
      Madotto, Andrea  and
      Winata, Genta Indra  and
      Fung, Pascale",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.273/",
    doi = "10.18653/v1/2020.emnlp-main.273",
    pages = "3391--3405",
    abstract = "In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data. MinTL is a simple yet effective transfer learning framework, which allows us to plug-and-play pre-trained seq2seq models, and jointly learn dialogue state tracking and dialogue response generation. Unlike previous approaches, which use a copy mechanism to {\textquotedblleft}carryover{\textquotedblright} the old dialogue states to the new one, we introduce Levenshtein belief spans (Lev), that allows efficient dialogue state tracking with a minimal generation length. We instantiate our learning framework with two pre-trained backbones: T5 and BART, and evaluate them on MultiWOZ. Extensive experiments demonstrate that: 1) our systems establish new state-of-the-art results on end-to-end response generation, 2) MinTL-based systems are more robust than baseline methods in the low resource setting, and they achieve competitive results with only 20{\%} training data, and 3) Lev greatly improves the inference efficiency."
}

@inproceedings{lin-etal-2021-domain,
    title = "Domain-independent User Simulation with Transformers for Task-oriented Dialogue Systems",
    author = "Lin, Hsien-chin  and
      Lubis, Nurul  and
      Hu, Songbo  and
      van Niekerk, Carel  and
      Geishauser, Christian  and
      Heck, Michael  and
      Feng, Shutong  and
      Gasic, Milica",
    editor = "Li, Haizhou  and
      Levow, Gina-Anne  and
      Yu, Zhou  and
      Gupta, Chitralekha  and
      Sisman, Berrak  and
      Cai, Siqi  and
      Vandyke, David  and
      Dethlefs, Nina  and
      Wu, Yan  and
      Li, Junyi Jessy",
    booktitle = "Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue",
    month = jul,
    year = "2021",
    address = "Singapore and Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.sigdial-1.47/",
    doi = "10.18653/v1/2021.sigdial-1.47",
    pages = "445--456",
    abstract = "Dialogue policy optimisation via reinforcement learning requires a large number of training interactions, which makes learning with real users time consuming and expensive. Many set-ups therefore rely on a user simulator instead of humans. These user simulators have their own problems. While hand-coded, rule-based user simulators have been shown to be sufficient in small, simple domains, for complex domains the number of rules quickly becomes intractable. State-of-the-art data-driven user simulators, on the other hand, are still domain-dependent. This means that adaptation to each new domain requires redesigning and retraining. In this work, we propose a domain-independent transformer-based user simulator (TUS). The structure of TUS is not tied to a specific domain, enabling domain generalization and the learning of cross-domain user behaviour from data. We compare TUS with the state-of-the-art using automatic as well as human evaluations. TUS can compete with rule-based user simulators on pre-defined domains and is able to generalize to unseen domains in a zero-shot fashion."
}

@inproceedings{lin-etal-2022-gentus,
    title = "{G}en{TUS}: Simulating User Behaviour and Language in Task-oriented Dialogues with Generative Transformers",
    author = "Lin, Hsien-chin  and
      Geishauser, Christian  and
      Feng, Shutong  and
      Lubis, Nurul  and
      van Niekerk, Carel  and
      Heck, Michael  and
      Gasic, Milica",
    editor = "Lemon, Oliver  and
      Hakkani-Tur, Dilek  and
      Li, Junyi Jessy  and
      Ashrafzadeh, Arash  and
      Garcia, Daniel Hern{\'a}ndez  and
      Alikhani, Malihe  and
      Vandyke, David  and
      Du{\v{s}}ek, Ond{\v{r}}ej",
    booktitle = "Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue",
    month = sep,
    year = "2022",
    address = "Edinburgh, UK",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.sigdial-1.28/",
    doi = "10.18653/v1/2022.sigdial-1.28",
    pages = "270--282",
    abstract = "User simulators (USs) are commonly used to train task-oriented dialogue systems via reinforcement learning. The interactions often take place on semantic level for efficiency, but there is still a gap from semantic actions to natural language, which causes a mismatch between training and deployment environment. Incorporating a natural language generation (NLG) module with USs during training can partly deal with this problem. However, since the policy and NLG of USs are optimised separately, these simulated user utterances may not be natural enough in a given context. In this work, we propose a generative transformer-based user simulator (GenTUS). GenTUS consists of an encoder-decoder structure, which means it can optimise both the user policy and natural language generation jointly. GenTUS generates both semantic actions and natural language utterances, preserving interpretability and enhancing language variation. In addition, by representing the inputs and outputs as word sequences and by using a large pre-trained language model we can achieve generalisability in feature representation. We evaluate GenTUS with automatic metrics and human evaluation. Our results show that GenTUS generates more natural language and is able to transfer to an unseen ontology in a zero-shot fashion. In addition, its behaviour can be further shaped with reinforcement learning opening the door to training specialised user simulators."
}

@inproceedings{liu-etal-2022-generative,
    title = "A Generative User Simulator with {GPT}-based Architecture and Goal State Tracking for Reinforced Multi-Domain Dialog Systems",
    author = "Liu, Hong  and
      Cai, Yucheng  and
      Ou, Zhijian  and
      Huang, Yi  and
      Feng, Junlan",
    editor = "Ou, Zhijian  and
      Feng, Junlan  and
      Li, Juanzi",
    booktitle = "Proceedings of the Towards Semi-Supervised and Reinforced Task-Oriented Dialog Systems (SereTOD)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, Beijing (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.seretod-1.10/",
    doi = "10.18653/v1/2022.seretod-1.10",
    pages = "85--97",
    abstract = "Building user simulators (USs) for reinforcement learning (RL) of task-oriented dialog systems (DSs) has gained more and more attention, which, however, still faces several fundamental challenges. First, it is unclear whether we can leverage pretrained language models to design, for example, GPT-2 based USs, to catch up and interact with the recently advanced GPT- 2 based DSs. Second, an important ingredient in a US is that the user goal can be effectively incorporated and tracked; but how to flexibly integrate goal state tracking and develop an end-to-end trainable US for multi-domains has remained to be a challenge. In this work, we propose a generative user simulator (GUS) with GPT-2 based architecture and goal state tracking towards addressing the above two challenges. Extensive experiments are conducted on MultiWOZ2.1. Different DSs are trained via RL with GUS, the classic agenda-based user simulator (ABUS) and other ablation simulators respectively, and are compared for crossmodel evaluation, corpus-based evaluation and human evaluation. The GUS achieves superior results in all three evaluation tasks."
}

@inproceedings{madotto-etal-2018-mem2seq,
    title = "{M}em2{S}eq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems",
    author = "Madotto, Andrea  and
      Wu, Chien-Sheng  and
      Fung, Pascale",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1136/",
    doi = "10.18653/v1/P18-1136",
    pages = "1468--1478",
    abstract = "End-to-end task-oriented dialog systems usually suffer from the challenge of incorporating knowledge bases. In this paper, we propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to address this issue. Mem2Seq is the first neural generative model that combines the multi-hop attention over memories with the idea of pointer network. We empirically show how Mem2Seq controls each generation step, and how its multi-hop attention mechanism helps in learning correlations between memories. In addition, our model is quite general without complicated task-specific designs. As a result, we show that Mem2Seq can be trained faster and attain the state-of-the-art performance on three different task-oriented dialog datasets."
}

@article{mosharrof2023zero,
  title={Zero-Shot Generalizable End-to-End Task-Oriented Dialog System using Context Summarization and Domain Schema},
  author={Mosharrof, Adib and Maqbool, Muhammad Hasan and Siddique, AB},
  journal={arXiv preprint arXiv:2303.16252},
  year={2023}
}

@inproceedings{peng-etal-2020-shot,
    title = "Few-shot Natural Language Generation for Task-Oriented Dialog",
    author = "Peng, Baolin  and
      Zhu, Chenguang  and
      Li, Chunyuan  and
      Li, Xiujun  and
      Li, Jinchao  and
      Zeng, Michael  and
      Gao, Jianfeng",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.17/",
    doi = "10.18653/v1/2020.findings-emnlp.17",
    pages = "172--182",
    abstract = "As a crucial component in task-oriented dialog systems, the Natural Language Generation (NLG) module converts a dialog act represented in a semantic form into a response in natural language. The success of traditional template-based or statistical models typically relies on heavily annotated data, which is infeasible for new domains. Therefore, it is pivotal for an NLG system to generalize well with limited labelled data in real applications. To this end, we present FewshotWOZ, the first NLG benchmark to simulate the few-shot learning setting in task-oriented dialog systems. Further, we develop the SC-GPT model. It is pre-trained on a large set of annotated NLG corpus to acquire the controllable generation ability, and fine-tuned with only a few domain-specific labels to adapt to new domains. Experiments on FewshotWOZ and the large Multi-Domain-WOZ datasets show that the proposed SC-GPT significantly outperforms existing methods, measured by various automatic metrics and human evaluations."
}

@inproceedings{sridhar2023hierarchical,
  title={Hierarchical Prompting Assists Large Language Model on Web Navigation},
  author={Abishek Sridhar and Robert Lo and Frank F. Xu and Hao Zhu and Shuyan Zhou},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258841249}
}

@inproceedings{su-etal-2022-multi,
    title = "Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System",
    author = "Su, Yixuan  and
      Shu, Lei  and
      Mansimov, Elman  and
      Gupta, Arshit  and
      Cai, Deng  and
      Lai, Yi-An  and
      Zhang, Yi",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.319/",
    doi = "10.18653/v1/2022.acl-long.319",
    pages = "4661--4676",
    abstract = "Pre-trained language models have been recently shown to benefit task-oriented dialogue (TOD) systems. Despite their success, existing methods often formulate this task as a cascaded generation problem which can lead to error accumulation across different sub-tasks and greater data annotation overhead. In this study, we present PPTOD, a unified plug-and-play model for task-oriented dialogue. In addition, we introduce a new dialogue multi-task pre-training strategy that allows the model to learn the primary TOD task completion skills from heterogeneous dialog corpora. We extensively test our model on three benchmark TOD tasks, including end-to-end dialogue modelling, dialogue state tracking, and intent classification. Experimental results show that PPTOD achieves new state of the art on all evaluated tasks in both high-resource and low-resource scenarios. Furthermore, comparisons against previous SOTA methods show that the responses generated by PPTOD are more factually correct and semantically coherent as judged by human annotators."
}

@article{terragni2023context,
  title={In-context learning user simulators for task-oriented dialog systems},
  author={Terragni, Silvia and Filipavicius, Modestas and Khau, Nghia and Guedes, Bruna and Manso, Andr{\'e} and Mathis, Roland},
  journal={arXiv preprint arXiv:2306.00774},
  year={2023}
}

@inproceedings{tseng-etal-2021-transferable,
    title = "Transferable Dialogue Systems and User Simulators",
    author = "Tseng, Bo-Hsiang  and
      Dai, Yinpei  and
      Kreyssig, Florian  and
      Byrne, Bill",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.13/",
    doi = "10.18653/v1/2021.acl-long.13",
    pages = "152--166",
    abstract = "One of the difficulties in training dialogue systems is the lack of training data. We explore the possibility of creating dialogue data through the interaction between a dialogue system and a user simulator. Our goal is to develop a modelling framework that can incorporate new dialogue scenarios through self-play between the two agents. In this framework, we first pre-train the two agents on a collection of source domain dialogues, which equips the agents to converse with each other via natural language. With further fine-tuning on a small amount of target domain data, the agents continue to interact with the aim of improving their behaviors using reinforcement learning with structured reward functions. In experiments on the MultiWOZ dataset, two practical transfer learning problems are investigated: 1) domain adaptation and 2) single-to-multiple domain transfer. We demonstrate that the proposed framework is highly effective in bootstrapping the performance of the two agents in transfer learning. We also show that our method leads to improvements in dialogue system performance on complete datasets."
}

@article{wang2024mobile,
  title={Mobile-agent: Autonomous multi-modal mobile device agent with visual perception},
  author={Wang, Junyang and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},
  journal={arXiv preprint arXiv:2401.16158},
  year={2024}
}

@article{wen2023droidbot,
  title={Droidbot-gpt: Gpt-powered ui automation for android},
  author={Wen, Hao and Wang, Hongming and Liu, Jiaxuan and Li, Yuanchun},
  journal={arXiv preprint arXiv:2304.07061},
  year={2023}
}

@inproceedings{xu-etal-2024-rethinking,
    title = "Rethinking Task-Oriented Dialogue Systems: From Complex Modularity to Zero-Shot Autonomous Agent",
    author = "Xu, Heng-Da  and
      Mao, Xian-Ling  and
      Yang, Puhai  and
      Sun, Fanshu  and
      Huang, Heyan",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.152/",
    doi = "10.18653/v1/2024.acl-long.152",
    pages = "2748--2763",
    abstract = "Task-oriented dialogue (TOD) systems are predominantly designed to be composed of several functional modules (e.g. dialogue state tracker, dialogue policy, natural language generation) whether they are pipeline or end-to-end architectures. However, this modular design not only heavily relies on massive fully-annotated data, but also suffers from many intrinsic drawbacks, such as serious error accumulation, poor generalization ability, high customization cost, and low fault tolerance rate. In this paper, we rethink the architecture of the task-oriented dialogue systems and propose a novel fully zero-shot autonomous TOD agent, named AutoTOD, where all the delicate modules in traditional TOD systems are deprecated and all it needs is a general-purpose instruction-following language model (e.g. GPT-4). AutoTOD only leverages a simple instruction schema consisting of the description of tasks and external APIs, and can autonomously decide what to do at each dialogue turn, including asking for information, calling APIs, summarizing API results, and correcting previous mistakes. Moreover, we propose a simulation-based evaluation framework to better validate the abilities of TOD models in real-life scenarios. Extensive experiments conducted on the MultiWOZ and SGD datasets show the superior task completion ability and flexible language skills of AutoTOD."
}

@inproceedings{xu2024rethinking,
  title={Rethinking Task-Oriented Dialogue Systems: From Complex Modularity to Zero-Shot Autonomous Agent},
  author={Xu, Heng-Da and Mao, Xian-Ling and Yang, Puhai and Sun, Fanshu and Huang, He-Yan},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2748--2763},
  year={2024}
}

@inproceedings{zhao-etal-2023-anytod,
    title = "{A}ny{TOD}: A Programmable Task-Oriented Dialog System",
    author = "Zhao, Jeffrey  and
      Cao, Yuan  and
      Gupta, Raghav  and
      Lee, Harrison  and
      Rastogi, Abhinav  and
      Wang, Mingqiu  and
      Soltau, Hagen  and
      Shafran, Izhak  and
      Wu, Yonghui",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.1006/",
    doi = "10.18653/v1/2023.emnlp-main.1006",
    pages = "16189--16204",
    abstract = "We propose AnyTOD, an end-to-end, zero-shot task-oriented dialog (TOD) system capable of zero-shot adaptation onto unseen tasks or domains. We view TOD as a program executed by a language model (LM), where program logic and ontology is provided by a designer as a schema. To enable generalization to unseen schemas and programs without prior training, AnyTOD adopts a neuro-symbolic approach. A neural LM keeps track of events that occur during a conversation, and a symbolic program implementing dialog policy is executed to recommend actions AnyTOD should take. This approach drastically reduces data annotation and model training requirements, addressing the enduring challenge of rapidly adapting a TOD system to unseen tasks and domains. We demonstrate state-of-the-art results on STAR, ABCD and SGD benchmarks. We also demonstrate strong zero-shot transfer ability in low-resource settings, such as zero-shot transfer onto MultiWOZ. In addition, we release STARv2, an updated version of the STAR dataset with richer annotations, for benchmarking zero-shot task transfer for end-to-end TOD models."
}

