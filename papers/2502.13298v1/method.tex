
% We introduce an interactive, real-world TOD framework, called {\ours}, that eliminates the need for fine-tuning while seamlessly generalizing to new domains. 
% {\ours} leverages instruction-tuned LLM (e.g., LLaMA) and require only a single dialog example from any domain to generalize infinitely many domains using their schemas.
% At the core of {\ours} is prompt chaining and a fine-grained feedback mechanism, enabling it to autonomously request users for task-related information, generate natural responses, execute API calls, and complete complex, multi-turn tasks -- all without any domain-specific customization.
% Specifically, prompt chaining automatically generates example dialogs in new domains by transforming a given source domain dialog into a corresponding target domain example using the new domain's schema. This generated example is then passed forward, allowing the model to leverage in-context learning for task completion in the target domain without relying on human-curated demonstrations.
% To further enhance accuracy, {\ours} integrates a fine-grained feedback mechanism from an API parser, which dynamically corrects errors and fills in missing parameters in API calls, resulting in reliable execution of actions and improved task completion rates.



% \begin{figure}[t!]
% \centering
% \begin{tikzpicture}[node distance=1cm]

%     % Nodes
%     \node (task) [startstop] {\textbf{Task Description }};
%     \node (schema) [data, below of=task] {\textbf{Schema} \( \Gamma \in D \)};

%     \node (sample) [decision, below of=schema] {\textbf{Sample Conversation} \( \mathcal{T} \in D \)};
%     \node (guidelines) [process, below of=sample] {\textbf{Guidelines to Follow}};
%     \node (history) [process, below of=guidelines, fill=green!60] {\textbf{Conversation History} \( H_t \)};
    
%     % Arrows
%     \draw [arrow] (task.south) -- (schema.north);
%     \draw [arrow] (schema.south) -- (sample.north);
%     \draw [arrow] (sample.south) -- (guidelines.north);
%     \draw [arrow] (guidelines.south) -- (history.north);

% \end{tikzpicture}
% \caption{Prompt Outline of {\ours} Model}
% \label{fig:realtod_prompt}
% \end{figure}

\section{Proposed Framework: {\ours}}
\label{sec:realtod}

We introduce {\ours}, an interactive, real-world TOD framework that eliminates the need for fine-tuning while seamlessly scaling to new domains. {\ours} leverages instruction-tuned LLMs (e.g., \LlamaShortName) and requires only a single dialog example from any domain to generalize across infinitely many domains using their schemas.
At the core of {\ours} is a two-stage prompt chaining and a fine-grained feedback mechanism, enabling it to autonomously request task-related information from the user, generate natural responses, execute API calls, and complete complex, multi-turn tasks -- all without domain-specific customization of the LLM. The first stage of prompt chaining generates an example dialog in a new domain by transforming a given source domain dialog into a corresponding target domain example while maintaining task-specific consistency with the target domain schema. The second stage then uses this generated example as an in-context demonstration, enabling the model to adapt to the target domain without requiring additional human-curated dialogs.
Additionally, {\ours} integrates a fine-grained feedback mechanism via an API parser that verifies the correctness of API calls. If any errors are detected, the parser provides fine-grained feedback to the LLM for correction, resulting in reliable execution of actions and improved task completion rates.


\subsection{Problem Formulation} \label{sec:problem_formulation} 

We formulate multi-turn task completion as a conditional sequence generation problem, where the LLM produces natural language responses or API calls to help users achieve their goals across relevant domains. 
% Specifically, the input to the LLM in {\ours} consists of five key elements: \myNum{i}~task description, \myNum{ii}~ domain schema, \myNum{iii}~sample dialog, \myNum{iv}~system guidelines and \myNum{v}~dialog history.
% The output of the LLM at each turn can be categorized into two types: \myNum{i}~a natural language response to the user -- providing requested information, seeking more information to fulfill the task, or chit-chat, and \myNum{ii}~an API call to retrieve pertinent information from an external system or execute an action. 
Each API call includes method name, a dictionary of parameter names and their corresponding values.


Formally, a domain \( d_x \in D \) is characterized by a domain schema, which consists of a set of user intents \( \mathcal{I}_{d_x} \). 
An intent represents a specific goal the user aims to achieve in the domain. For example, in the ``Flights'' domain, an intent might be ``Book a Flight''.
Each intent \( i \in \mathcal{I}_{d_x} \) is associated with a set of slots \( \mathcal{S}_i \), where each slot $s$ captures relevant constraints to fulfilling the intent. For example, the intent of ``Book a Flight'' may involve slots such as ``departure city'' and ``destination city''. 
We define a slot \( s \) as a tuple:
\[
s = (\texttt{name}(s), \texttt{is\_required}(s), \texttt{values}(s))
\]
where  \( \texttt{name}(.) \) specifies the slot's name (e.g., ``departure city''), \( \texttt{is\_required}(.) \) is a boolean flag indicating whether the slot is mandatory, and \( \texttt{values}(.) \) specifies a predefined set of possible values for categorical slots (e.g., ``business class'', ``economy class'' in the ``Flights'' domain). If the slot accepts free-form inputs, this field remains empty.
For brevity, we will refer to the name of a slot \( \texttt{name}(s) \) as \(s_m\).
Formally, the schema for a domain \( d_x \) is represented as:
\[
\Sigma_{d_x} = (d_x, \mathcal{I}_{d_x}, \{ \mathcal{S}_i \mid i \in \mathcal{I}_{d_x} \}).
\]


In addition to generating natural language responses, the model may need to retrieve information from external systems or execute actions via API calls to accurately fulfill a user's goal. Each API call corresponds to a specific intent in a domain and a set of specified constraints, represented as slot-value pairs.
Formally, an API call $a_n$ is defined as:  
\(
a_n = \texttt{API}(\texttt{method} = i,  \texttt{parameters} = \{(s_m , v), (\cdots) \mid s_m \in \mathcal{S}_i\}),
\)
where  \( i \) is the intent,  \( s_m \) is the slot name, and \( v \) is its assigned value.   
For instance, in the ``Flights'' domain, an API call for booking a flight may look like:  
\(
\texttt{API}(\texttt{method} = \text{``Book\_a\_Flight''}, \texttt{parameters} =\{ (\text{``departure\_city''}, \text{``New York''}), (\text{``destination''},\\ \text{``London''}), (\cdots) \}).
\)  


A dialog session in a domain \( d_x \) consists of a sequence of user utterances and system responses across multiple turns. 
We define a session \( \mathcal{T}_{d_x} \) of up to \( T \) turns as:  
\[
\mathcal{T}_{d_x} = \bigl((u_1, r_1), (u_2, r_2), \dots, (u_T, r_T)\bigr)
\]
where \( u_t \) is the user's utterance at turn \( t \), \( r_t \) is the system's response at turn \( t \), which can either be a natural language reply or an API call. 
The dialog history up to turn \( t \), denoted as \( H_t \), consists of all previous exchanges up to and including the current user utterance:  
\(
H_t = \{(u_1, r_1), (u_2, r_2), \dots, (u_{t-1}, r_{t-1}), u_t\}.
\)




\subsection{Prompt Chaining} \label{sec:prompt-chain}


To enable seamless generalization across domains without any additional example dialogs in each target domain, {\ours} employs a two-stage prompt chaining mechanism, which consists of two sequential prompting phases: \myNum{i}~example dialog generation that transforms an example dialog from a source domain into a target domain while maintaining task-specific consistency; and \myNum{ii}~task adaptation that leverages the generated example dialog for in-context learning in the target domain.

\vspace{5pt}
\stitle{Example Dialog Generation.}
The first phase constructs an example dialog in the target domain by leveraging the schema mapping between the source domain and target domain. 
Formally, the inputs to LLM in this phase include the source domain schema \(\Sigma_{d_x}\), an example dialog \( \mathcal{T}_{d_x} \) in the source domain, an instruction prompt \( P_1 \) specifying the transformation process, and the target domain schema  \(\Sigma_{d_y}\). The output is a new example dialog \( \mathcal{T}_{d_y} \) that aligns with the intents \(\mathcal{I}_{d_x} \) and associated slots \( \mathcal{S}_{i_x} \) in the target domain \( d_y \). 


\vspace{5pt}
\stitle{Task Adaptation.}
Once the example dialog \( \mathcal{T}_{d_y} \) in target domain \( d_y \) is generated, the second phase leverages this as an in-context learning example to enhance the model's adaptation in the target domain. 
At each dialog turn \( t \), the inputs to the LLM include the target domain schema \( \Sigma_{d_y} \), the generated example dialog \( \mathcal{T}_{d_y} \), the dialog history up to turn \( t \) (denoted as \( H_t \)), and an instruction prompt \( P_2 \) that guides the response generation process.

The LLM then produces the system response \( r_t \), which can be either a natural language reply or an API call, depending on the current task context.
Since a single dialog may span multiple domains, we can denote the set of target domains involved in a dialog session as \( \{d_1, d_2, \dots, d_m\} \subseteq D \), and extend to the formulation to condition on all relevant domain schemas
\( \{\Sigma_{d_j}\}_{j=1}^{m} \).
  
\vspace{5pt}
\stitle{Instruction Prompt.} 
The prompt \(P_1\) begins with a task description on generating a dialog from a schema, then presents {domain\_X}'s schema and its sample conversation. It instructs the LLM to analyze this structure, apply it to {domain\_Y}, and generate a corresponding conversation. (For the full prompt \(P_1\), see Appendix \ref{sec:prompt1}.) The instruction prompt \(P_2\) consists of two main parts: a task description and general guidelines. It directs the system to collect required slot values before API calls and use search results for accurate responses. The guidelines emphasize limiting slot requests per turn and confirming user inputs before invoking the API call. (For the full prompt \(P_2\), see Appendix \ref{sec:prompt2}.)
%The prompt defines the LLM's role as an expert chat assistant in a specified domain. It instructs the model to leverage domain schemas, sample dialogs, and dialog history to understand user intent and generate coherent responses that continue the conversation. Additionally, the LLM must extract both required and optional slots (if provided by the user) from the conversation before executing an API call. API parameters must strictly follow schema-defined column names. Once an API call is executed, the retrieved results will be incorporated into the dialog history in the next turn, and the LLM must integrate them accurately to generate grounded responses, preventing hallucinations.
%Additionally, the prompt emphasizes to incrementally collecting slot values, confirming them before API execution, and strictly adhering to the predefined API structure. 
%The assistant must not fabricate search results but instead integrate the retrieved ones while avoiding information overload. Moreover, it should assist users by resolving ambiguities and providing relevant suggestions, ensuring clarity and maintaining contextual accuracy.

\subsection{Fine-Grained Feedback}

Even SOTA LLMs can make errors when generating API calls. 
To minimize these errors and ensure successful API execution, {\ours} integrates a fine-grained feedback mechanism via a generic API parser. Given a domain schema \( \Sigma_{d_x} \) and an API call \(a\), the parser verifies the correctness of the request before execution. 
If the API call conforms to the schema, it is passed for execution; otherwise, the parser provides fine-grained feedback to the LLM for correction.  
The verification process identifies three types of errors: \myNum{i}~incorrect method name, where the API method does not match any intent \( i \not\in \mathcal{I}_d \); \myNum{ii}~incorrect slot name, where a provided slot is not defined in the schema \( s_m \not\in \mathcal{S}_i \) for the given intent; and \myNum{iii}~missing required slots, where required slots \( s_m \) with \( \texttt{is\_required}(s_m) = \text{True} \) are absent in the API parameters. 
Upon detecting an error, the parser returns fine-grained feedback specifying the issue, allowing the LLM to refine its response. 



% The task description defines the LLM’s role as an expert chat assistant within a specified domain. In the task description, the LLM understands that it needs to utilize structured schema, sample dialog, and the dialog history conversation to understand user intent and provide the next turn response continuing the dialog history. Moreover, in the task description, the llm learns that it needs to acquire the required and optional slots from the conversation before executing an API call. The assistant must restrict responses to relevant slots and use schema-defined column names as API parameters. After executing the API call, the search results will be provided to the LLM in the next turn (on dialog history), and the LLM learns to integrate the retrieved database results to generate accurate and grounded responses, preventing hallucinations.  In addition to the task description, we provided a system guideline as well. 

% In addition to the task description, system guidelines refine the assistant’s interaction strategy, ensuring a natural and user-friendly conversation flow. They emphasize collecting slot values incrementally, confirming them before API calls, and strictly following the predefined API structure. The assistant must not generate search results but integrate the provided ones while preventing information overload. It should also assist users through confusion with relevant suggestions, maintaining clarity and contextual accuracy.








% These errors typically fall into three categories:  
% \begin{itemize}[leftmargin=1.2em, labelindent=-1pt, itemsep=-1pt]

%     \item \emph{Incorrect method name generation} – e.g., generating `BookRestaurants` instead of `ReserveRestaurant`.
%     \item \emph{Incorrect slot name generation} – e.g., using `city` instead of `location`. While they may have similar meanings, the database may only accept `city`, leading to a mismatch.  
%     \item \emph{Missing required slots} – e.g., for reserving a restaurant, the API must include `restaurant\_name`, `city`, and `time`. If any of these are missing, the API call is considered invalid.  

% \end{itemize}
% To mitigate these errors, we propose a retry mechanism. Whenever an API generation error occurs, we provide the LLM with feedback about the issue and prompt it to regenerate the API call while considering the correction. The model is given up to three attempts to produce a valid API call. If it fails after three retries, we terminate the conversation and end the dialog, as querying the database with an incorrect API is not feasible.


%In our formulation, at each turn \( t \), the model generates its response \( r_t \) by conditioning on the relevant domain schema \( \Sigma_d \), an in-context dialog example \(\mathcal{T}_d \), the current dialog history \( H_t \), and any additional prompts. 



% We formulated our task-oriented dialog (TOD) system to either generate natural language responses for the user or produce API calls to interact with external tools. To achieve this, the system must effectively leverage the schema provided for each domain.

% We begin by formally defining the domain and its corresponding schema. Each schema, denoted as \( \Gamma \), corresponds to a domain \( d \in D \) (e.g., the \textit{Restaurants} domain). Within a given domain, there exists a set of intents \( I_d \), where each intent represents a specific goal the user aims to achieve (e.g., \textit{FindRestaurants}).


% For each intent \( i_d \in I_d \), there is an associated set of slots \( S_{i_d} \), which can be either required or optional. Each slot \( s_{i_d} \in S_{i_d} \) may have a predefined set of possible values. For example, in the \textit{Restaurant} domain, the slot \textit{cuisine} may be required, with possible values such as \textit{Mexican, Chinese, Indian, American}, and \textit{Italian}. We formally represent each slot as a tuple:
% \vspace{-4pt}
% \[
% s_{i_d} = (\text{name}(s_{i_d}), \text{is\_required}(s_{i_d}), \text{values}(s_{i_d}))
% \]
% \vspace{-4pt}
% Finally, we define the whole schema as:

% \[
% \Gamma = (d_\Gamma, I_d, \{ S_{i_d} \mid i_d \in I_d \})
% \]

% A task-oriented dialog system must effectively initiate and sustain conversations while adhering to a predefined schema. To formalize this interaction, we define a dialog conversation $\mathcal{T}$ as a temporal sequence of user and system exchanges. At the turn $t$, the user produces an utterance $u_t \in U_\mathcal{T}$, where $U_\mathcal{T}$ denotes the set of all possible user utterances in the conversation $\mathcal{T}$. Correspondingly, the system generates a response $r_t \in R_\mathcal{T}$, with $R_\mathcal{T}$ representing the set of all system responses. This iterative exchange forms the dialog, structured over multiple turns. Formally, a dialog conversation spanning $T$ turns can be expressed as:

% \[
% \mathcal{T} = ((u_1, r_1), (u_2, r_2), \ldots, (u_T, r_T))
% \]

% Moreover, the dialog history up to turn $t$, denoted as $H_t$, can be defined as:
% \[
% H_t = ((u_1, r_1), (u_2, r_2), \ldots, (u_{t-1}, r_{t-1}), u_t)
% \]


% Here, $\mathcal{T}$ and $H_t$ may encompass $m \geq 1$ domains or schemas, allowing the system to navigate various topics and tasks dynamically.

% As discussed earlier, the system response at turn \( t \), denoted as \( r_t \), can either be a natural language utterance or an API call. To formally represent API calls within the dialog, we define the set of all API calls invoked during the conversation \( \mathcal{T} \) as \( API_\mathcal{T} \). 

% Each API call corresponds to a specific intent \( i_d \in I_d \) within a given domain and operates on a set of slot-value pairs. Formally, we define the \( n \)-th API call, \( api_n \in API_\mathcal{T} \), as:

% \[
% api_n = \text{API}(\text{method} = i_d, \{(s_{i_d}, v_{i_d}) \mid s_{i_d} \in S_{i_d} \})
% \]

% where \( i_d \) specifies the intent associated with the API call, and each slot \( s_{i_d} \) is assigned a corresponding value \( v_{i_d} \).

% Moreover, after each $api_n$, there is a list of slots that the user should request (e.g. whether the restaurant has live music or not). We denote this as $ur_n \in UR_\mathcal{T}$, where $UR_\mathcal{T}$ includes all the user equests in dialog conversation $\mathcal{T}$.


%\subsection{RealTOD}
%5. user simulator




%{\ours} is an interactive, real-world task-oriented dialog (TOD) system powered by instruction-following large language models (LLMs). Unlike traditional modular TOD systems, which rely on separate components such as Dialog State Tracking, {\ours} adopts a fully end-to-end framework. Users can communicate their needs directly, and the system responds seamlessly, providing the necessary information in an integrated manner.




%To improve reliability, we introduce a novel retry mechanism that refines the LLM’s output when an API call is incorrectly generated. The following sections will delve deeper into this mechanism and the input components.



% \stitle{Schema.}
% The schema of the $m$ domain(s) for the current ongoing dialog conversation, denoted $\{ \Gamma_{d_j} \}_{j=1}^{m}$, is also provided to the LLM. This schema guides the LLM in generating the appropriate API call by providing the following information: \myNum{i}~ Available method names within the current domain(s) which can be taken from \( I_d \). \myNum{ii}~ Required and optional slots along with their optional values provided as \( S_{i_d} \) that will be used in API as parameters name. This structured schema enables the LLM to generate accurate and contextually appropriate API calls.

% \stitle{Sample Conversation and Dialog History.}
% To enhance response generation, we retrieve a sample conversation from the dataset that matches the domain(s) of the current dialog. This serves as a reference for guiding the model's responses. Additionally, the dialog history up to that point, $H_t$, is provided as input to the LLM, enabling it to generate the next turn based on the accumulated history, the retrieved sample conversation, and the available schema.


% \subsection{User Simulator}
%A good user simulator needs to first inform its needs (e.g restaurant\_name or restaurant\_city) accurately to the system and then optionally requests some slots to know more about entities (e.g. the phone number of the restaurant). To create such user simulator, we use the ground truth dialog conversation $\mathcal{T}$ along with $API_\mathcal{T}$ and $UR_\mathcal{T}$.


%Our proposed user simulator is a fine-tuned Flan-T5 model designed to guide {\ours} toward generating the desired API call. It strategically provides information in a way that influences {\ours} to generate the intended APIs. Additionally, the user simulator is fine-tuned to determine whether it should request additional information (e.g., phone number or address) for a given entity at each turn.

%To achieve this functionality, we fine-tuned the model with the following inputs:
%\begin{itemize}[leftmargin=1.2em, labelindent=-1pt, itemsep=-1pt]
%\item The next API call that {\ours} should request.  
%\item The next additional slots that the user simulator should inquire about (e.g., requesting a phone number).  
%\item The previously called API.  
%\item The additional slots that have already been requested.  
%\item The dialog history so far.  
%\end{itemize}

%Notably, the dialog history provided to the user simulator excludes any retrieved database results from previous turns and does not retain information about past API calls.

%To ensure adaptability, the user simulator dynamically updates its next API call and follow-up questions, along with the previously called API and prior follow-up questions, whenever {\ours} generates a new API request. This design helps maintain a concise input context, ensuring the user simulator receives only the essential information at each time step.

\section{User Simulator}

Ideally, a TOD system should interact with real users in order to evaluate its effectiveness. However, engaging real users is often costly and time-consuming. To address this challenge, we develop a user simulator. An effective user simulator must first accurately convey its needs by specifying the required slot values (e.g., `departure city'') before optionally requesting information (e.g., the flight's arrival time'') from the TOD system. 
To construct such a simulator, we utilize dialog data \( \mathcal{T}_{d_x} \)  consisting of user goals, expressed through API calls \( A = [a_1, a_2, \dots, a_n] \), and the request slots  \( R =  [s_1, s_2, \dots, s_m] \) that the user should request. 
To train the user simulator, we optimize an instruction-finetuned model as:
\vspace{-4pt}
\[
\mathcal{L} = - \sum_{k=1}^{|u_t|} \log p(w_k \mid w_{<k}, H_t, A, R),
\]
where \( w_k \) denotes the \( k \)-th token in the user utterance \( u_t \) at turn \(t\), and \( w_{<k} \) represents all preceding tokens in the same utterance. 
The simulator learns to express user goals in natural language by providing values for requested slots, and request information from the TOD system, conditioned on the set of API calls \(A\), request slots \(R\), and dialog context \(H_t\).
To conduct an interactive session between a trained user simulator and the TOD system, the simulator initiates the conversation by retrieving the first user goal \( a_1 \) from \(A\) and associated request slot \( s_1 \) from \(R\).
This process continues iteratively until all user goals in 
A and their associated request slots in R have been processed.
% Once the retrieved goal is fulfilled, the simulator iterates over all goals in \(A\) and associated request slots \(R\).

%Once training is complete, a new conversation is initiated by sampling \( a_1 \) and \( s_1 \).

%While \( a_i \) and \( s_i \) correspond to the next API call and the next requested slot, respectively. Additionally, \( a_{i-1} \) and \( s_{i-1} \) represent the previous API call and requested slot. %By conditioning on both past interactions and predicted actions, the model effectively learns to generate user utterances that align with realistic query sequences.

%The user simulator dynamically updates \( a_i \), \( s_i \), \( a_{i-1} \), and \( s_{i-1} \). An API pointer, initialized to 0, increments whenever a new API call occurs. The relevant values are retrieved as \( a_i = A[\text{pointer} + 1] \), \( s_i = R[\text{pointer} + 1] \), \( a_{i-1} = A[\text{pointer}] \), and \( s_{i-1} = R[\text{pointer}] \), ensuring a structured and context-aware interaction flow.


%\subsection{Interactive Session}  

%Our method operates interactively, simulating a dynamic user-system exchange. The session starts with the fine-tuned Flan-T5 user simulator generating an initial user utterance based on its predefined goals. This utterance, structured according to the dialog history \( H_t \), is fed into {\ours}, which responds with either a natural language reply or an API call.

%Each system response \( r_t \) is appended to the dialog history and provided to the user simulator, which generates the next utterance \( u_{t+1} \). This exchange continues until the simulator outputs the termination token \texttt{<|endofdialog|>}, signaling that all required API calls \( API_\mathcal{T} \) have been executed and all necessary slot values from \( UR_\mathcal{T} \) have been successfully requested and resolved.

%This interactive process ensures the system efficiently gathers and processes information while adhering to the predefined schema \( \Gamma \).
