\section{Related Works}
%% TODO Writing :Sajid

\stitle{Fine-Tuned Task-Oriented Dialog Systems.} % Needs work
TOD systems are typically classified into pipeline-based and end-to-end approaches. Pipeline-based methods **Vinyals, "Conditional Neural Skills"**__**Li, "Deep Reasoning"** decompose the system into modular components—natural language understanding, dialog state tracking, policy learning, and natural language generation—allowing independent optimization of each module. In contrast, end-to-end approaches **Wang, "Task-Oriented Dialogue Systems"**__**Liu, "End-to-End Task-Oriented Dialogues"** generate responses directly, bypassing these modules. A major drawback of these fine-tuned methods is their reliance on high-quality labeled data, which can be a significant limitation.

%In recent years, task-oriented dialog (TOD) systems have been an active area of research in natural language processing (NLP), evolving significantly over the past few decades. These systems can be divided into two approaches, pipeline-based methods eng et al., 2021; Eric et al., 2020 Feng et al., 2022b; Ye et al., 2022; Su et al., 2022; Heck et al., and end-to-end methods ____.

%The pipeline approach has significant drawbacks, such as error propagation. Mistakes made in the earlier phases can transfer over and negatively impact the later modules in the pipeline, making the entire system less reliable.

%In recent times, \emph{end-to-end} TOD systems emerged and have been popular among researchers, offering a unified model that integrates all modules and is trained on fully annotated dialog datasets ____). These methods have also been trained from scratch or fine-tuned with fully annotated dialog datasets. While this approach simplifies the architecture more than the previous pipeline architectures, it relies heavily on large-scale, annotated dialog data.

%In an effort to reduce the dependence on large-scale training datasets, researchers have begun to explore zero-shot approaches for TOD systems. For instance, AnyTOD ____ leverages a neuro-symbolic approach to unseen tasks without the need for additional training or fine-tuning. Similarly, ZS-ToD ____ employs domain schemas in a zero-shot end-to-end model to achieve better performance across unseen domains.

%Despite these explorations, these approaches still operate within the classical modular or end-to-end paradigms. They are not entirely free from the need for training data or fine-tuning with domain-specific training, and their overall performance remains significantly is heavily influenced by the quality of the data collected.

\stitle{LLM-Powered Systems.} The rise of LLMs has led to the development of various intelligent systems, which can be broadly categorized into three classes. The first class includes Web Agents, which facilitate online interactions for information retrieval and task execution _____. The second class consists of Mobile Agents, which focus on optimizing LLM-based decision-making for performing diverse tasks on mobile applications _____. The third and most relevant class to our work is LLM-powered TOD Systems _____. Specifically, AutoTOD ____ shares similarities with our approach; however, AutoTOD does not account for the possibility of LLMs making errors in generating API calls and lacks proper evaluation of API accuracy.


\stitle{User Simulators.}
One of the earliest data-driven user simulators is **Li, "Data-Driven User Simulation"**_, where user actions are generated probabilistically based on system actions. In addition to this, there have been many advancements in data-driven user simulation. For instance, recent advancements leverage transformer-based architectures for domain-independent simulation ____ and GPT-based models integrating goal state tracking _____. Reinforcement learning has also been applied to fine-tune generative simulators ____.
More recently, in-context learning (ICL) with LLMs has enabled user simulation without fine-tuning, _____. Similar to _____, our user simulator employs transformer-based architectures.

% as seen in few-shot prompting ____ and ICL-based evaluation _____. Similar to _____, our user simulator employs transformer-based architectures.


%One of the earliest data-driven user simulators is ____, where user actions are generated probabilistically based on system actions. In addition to this, there have been many advancements in data-driven user simulation. For instance, transformer-based user simulators such as ____ and ____ aim to achieve domain independence. Other approaches, like _____, leverage GPT-based architectures to generate both user actions and utterances, integrating goal state tracking to improve interaction quality. Additionally, reinforcement learning has been used to fine-tune generative user simulators, such as in ____ and ____.
%Recent work explores in-context learning (ICL) for user simulation with large language models (LLMs), eliminating the need for fine-tuning. ____ generates user utterances via few-shot prompting, while ____ uses ICL-based simulators to evaluate task-oriented dialogue systems. Similar to ____, our user simulator utilizes transformer-based architectures and a mechanism similar to goal state tracking.

% Early data-driven user simulators, such as ____, generate user actions probabilistically based on system actions. Transformer-based approaches ____ focus on domain independence, while GPT-based models ____ generate both user actions and utterances with goal state tracking. Reinforcement learning has also been used to fine-tune generative simulators ____. More recently, in-context learning (ICL) with large language models (LLMs) ____ enables and evaluates user simulation without fine-tuning.


%% TODO Writing 
% \stitle{Prompt-based Approaches.} In recent times in TOD systems, there has been an emergence of prompt-based language models, such as ChatGPT (OpenAI, 2022), Gemini, Claude, DeepSeek, Llama 3.3 (Touvron et al., 2023), has opened a new era of intelligent assistants. These new models have the potential to understand a user's intent on any domain and can create human-like responses from various topics (OpenAI, 2023a). This approach eliminates the need for fine-tuning. Researchers such as Labruna et al. (2023), Hudevcek and Dusek (2023), Dingliwal et al. (2021), Madotto and Liu (2020), Li et al. (2022), and Madotto et al. (2021) have explored this innovative approach.

%Recently AutoTOD ____  

%Most of the recent studies in this field depend on turn-level annotated data for training task-oriented dialog (TOD) models and if the model was trained for general tasks, for TOD systems we need to fine-tune them with our tasks and datasets to get better results. This is one of the major limitations of previous task-oriented dialog systems. Our work here focuses on in-context learning with instruction and example-oriented dialog generation. Instead of training/file-tuning LLMs, we leverage the intelligence of recent LLMs for Task-oriented dialog systems.