\section{Limitations}

%{\ours} is among the first real-world task-oriented dialog (TOD) systems to leverage prompt-based large language models (LLMs). While our approach demonstrates the feasibility of using LLMs in real-world TOD settings, several limitations present opportunities for future work. 

%Expanding the evaluation to cover a more diverse set of LLMs and real-world datasets would help assess the generalization and robustness of {\ours}. Although we included DeepSeek V3 in our experiments, the growing adoption and positive response of DeepSeek R1 suggest that further evaluation is needed with these new LLMs.

We restricted our experiments to four popular LLMs (\GPTLongName, \ClaudeLongName, \DeepSeekLongName, and \LlamaLongName) due to time and computational constraints. Given the rapid pace of model development -- exemplified by emerging systems such as DeepSeek R1 \cite{guo2025deepseek} and OpenAI o3-mini \cite{o3}, Qwen-2.5 Max \cite{qwen2024max} -- it remains an open question how RealTOD would perform on these newer LLMs.

While the user simulators fine-tuned in this study perform reasonably well, they are not perfect and may occasionally struggle to respond accurately to {\ours}.  Further details on their performance can be found in Appendix~\ref{sec:User_Simulator_Performance}. % As future work, we plan to evaluate {\ours} using existing user simulators (e.g. GenTUS \cite{lin-etal-2022-gentus}), real human users, or another LLM as a more robust user simulator.
%\textcolor{red}{More limitations?}


%Additionally, our experiments found that sometimes, these LLMs fail to generate conversations on specific domains and dialogs. The model falls into a loop in the middle of completing the dialog: the complex dialog structure and lack of fine-tuning cause this. In such cases, building a better retry mechanism for those domains could be a way to fix these errors. Another popular option is exploring domain-specific fine-tuning or incorporating more effective prompt engineering strategies that could improve response quality. Fine-tuning large-scale models on underperforming domains or tailoring prompts to specific conversational patterns may enhance the systemâ€™s ability to handle diverse user queries.

% Beyond these technical considerations, RealTOD exhibits some common challenges associated with LLM-driven TOD systems. All these models operate as a black box, making it difficult to interpret their responses, which limits transparency and error correction. Furthermore, biases inherent in pre-trained LLMs may affect response fairness, particularly in sensitive domains. While careful dataset curation and fine-tuning can somewhat mitigate these biases, eliminating them remains an ongoing challenge.

% Finally, deploying LLM-based Task-Oriented Dialog (TOD) systems in real-world applications raises ethical and privacy concerns. Ensuring transparency with data privacy regulations and preventing system misuse requires continuous monitoring and updates. Future work should explore mechanisms for enhancing data privacy, increasing interpretability, and refining safety measures to create more reliable and responsible task-oriented dialog systems.




