\section{Experiments}

% Response accurately
% Api completions 
% Inform accuracy 

%The main objective of our experiments is to evaluate the performance of popular LLMs as Task-Oriented dialog (TOD) systems compared to the baseline SOTA Task-Oriented dialog System models. To achieve this, we simulate conversations between a user simulator and TOD systems. In our experiments, we use a fine-tuned FLAN-T5 Turbo as our user simulator.

%In this section, we are going to evaluate the performance of {\ours} powered by large language modes (LLMS) with other SOTA task-oriented dialog system.

\subsection{Datasets}
We conduct our experiments using two datasets: the Schema-Guided dialog (SGD) dataset \cite{rastogi2020towards} and the Bilingual Task-Oriented dialog (BiToD) dataset \cite{lin2021bitod}. Since BiToD includes dialogs in both Chinese and English, we retain only the English dialogs for our analysis. Both datasets provide domain-specific schemas along with corresponding dialog conversations, which are essential for baseline models. A comparative summary of key statistics for both datasets is presented in Table \ref{tab:sgd_bitod_comparison}.

% We use two traditional datasets: The schema-guided dialog (SGD) dataset, and Bilingual TaskOriented Dialog (BiToD) dataset for our experiment. These datasets provide a schema for describing the domain and the conversational data contains API calls to communicate with external sources. Table 1 displays detailed statistics about the datasets.

\begin{table}[t]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{adjustbox}{max width=0.40\textwidth}
        \begin{tabular}{lcc}
            \toprule
            \textbf{Statistic} & \textbf{SGD} & \textbf{Bitod} \\
            \midrule
            Total Dialogs & 4,201 & 352 \\
            Total Dialogs (Single-domain) & 1,331 & 111 \\
            Total Dialogs (Multi-domain) & 2,870 & 241 \\ 
            Total API Calls & 13,239 & 1,005 \\
            Total API Calls (Single-domain) & 2,188 & 127 \\
            Total API Calls (Multi-domain) & 11,051 & 878 \\
            Total Turns & 89,428 & 6,979 \\ 
            Total User Req. Slots & 8,271 & 500 \\ \hline
            Avg. API calls per dialog & 3.15 & 2.85 \\
            Avg. API calls (Single-domain) & 1.64 & 1.14 \\
            Avg. API calls (Multi-domain) & 3.85 & 3.64 \\
            Avg. turns per dialog & 21.28 & 19.82 \\
            Avg. User Req. Slots & 1.96 & 1.42 \\
            Avg. parameters per API call & 2.96 & 3.51\\ \hline
            Total Unique API methods & 34 & 7\\ %TODO: recheck
            Total Unique API parameters & 88 & 20\\ %TODO: recheck
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{Test Dataset Statistics for SGD and BiTOD.
    }
    \label{tab:sgd_bitod_comparison}
    \vspace{-10pt}
\end{table}

\subsection{Experimental Setup} 

We integrated four LLMs in {\ours}: two open-source models, \DeepSeekLongName\ \cite{liu2024deepseek} and \LlamaLongName\ \cite{dubey2024llama}, and two proprietary models, \GPTLongName\ \cite{achiam2023gpt} and \ClaudeLongName\ \cite{anthropic2023claude}. For \GPTLongName, we accessed the model via the official OpenAI API\footnote{\url{https://openai.com/api/}}, while \ClaudeLongName\ was queried using the official Anthropic API\footnote{\url{https://docs.anthropic.com/claude}}.
%We trained two separate user simulators, one for each dataset, both based on the Flan-T5 model \cite{Chung2022}. 

We fine-tune Flan-T5 model~\cite{Chung2022}  to act as a user simulator for each dataset. Specifically, we use the "google/flan-t5-base" model, which consists of 250 million parameters.
During fine-tuning, we set the warm-up steps to 100 and applied early stopping based on the evaluation loss, with patience of three. The models were trained for 10 epochs.

% \subsection{Prompts} We have used two separate prompts for single and multi-domain to conduct all of our experiments. The prompt includes detailed instructions, a sample API call, a full sample conversation between a real user and a system, and a schema for that particular domain. If the dialog turns to contain multiple domains, the examples will be multiple for schema and sample conversations. A detailed structured prompt is shown in Appendix C. 


\subsection{Evaluation Metrics}
To comprehensively evaluate the performance of {\ours} and baseline models, we assess the following: \myNum{i}~Dialog-Level System Response, \myNum{ii}~Inform Accuracy, \myNum{iii}~API Call, and \myNum{iv}~Dialog Success Rate.

%We individually assess the subtask's information and responses to evaluate the performance of the popular LLM models for Task-Oriented dialog Systems. This way of experiment gives us a more step-by-step assessment of the system's performance and future improvements.

\stitle{Dialog-Level System Response.}
To assess the quality of the responses generated by {\ours}, we removed all user responses produced by our user simulator, retaining only system responses. We then concatenated all system turns into a single text containing only system-generated outputs. The same process was applied to the ground truth dialog, keeping and concatenating only the system turns. Finally, we evaluated system response quality at the dialog level by comparing the generated responses to the ground truth using BERTScore~\cite{zhang2019bertscore}, a metric that measures semantic similarity between texts. Furthermore, we utilize "microsoft/mpnet-base" as the foundational model for computing BERTScore.

%We report BERTScores (Zhang et al., 2019a) for overall system response instead any traditional metrics like BLUE-4 or GLUE generally used for task-oriented dialog systems. 

\stitle{Inform Accuracy.}
To evaluate how effectively {\ours} informs the user about the requested slots, we implemented a regex-based system. First, we identify the slots requested by the user and extract their corresponding values from the search results. Then, we use regex matching to determine whether the systemâ€™s subsequent responses include those extracted values. If a system turn contains the requested slot values, we consider the system to have successfully provided the required information.


\stitle{API Calls.}
% As mentioned in , an API call is defined as:
%\vspace{-8pt}
%\[
%api_n = \text{API}(\text{method} = i, \{(s_{i_d}, v_{i_d}) \mid s_{i_d} \in S_{i_d} \})
%\]
%We introduced a custom metric to asses different components of the API Call. API Call format looks like this \begin{quote} APICall(method = \textit{method\_name}, parameters = \(\{(s_i, v_i)_{i=1}^n\}\)) \end{quote} The parameter attribute is a list of key, value pair from slots, where si denotes slot name and vi denotes slot value. We use regular expression to extract the key, and value pairs from the System generated API Calls so that we can calculate its accuracy. 
%We defined the API call in Section~\ref{sec:problem_formulation}.
To evaluate the quality of API Calls, we first extract the key-value pairs $ (name(s_k),v_k)_{k=1}^n $, along with method name $i$ from the generated API call using regular expressions. \emph{Method Accuracy} evaluates whether the generated API call uses the correct method name, assessed using exact matching. \emph{Parameter Name Accuracy} determines whether all ground truth key names are included in the generated API call, using fuzzy matching. \emph{Parameter Value Accuracy} verifies if the value associated with a correctly predicted key matches the ground truth, also using fuzzy matching. Notably, this metric is computed only when the corresponding \emph{Parameter Name} is correctly predicted. \emph{Operator Accuracy} applies specifically to the BiToD dataset, where API calls include operators (e.g., ``at\_least'', ``one\_of''). We assess this using fuzzy matching. \emph{Full API Accuracy} measures whether the entire API call -- including the method, parameter, values, and, for BiToD, the operator -- matches the ground truth.


% \textbf{Method Accuracy} determines whether the system appropriately picks the method name or not. 

% \textbf{Key Value Accuracy} checks whether the {key, value} pair from the API call has been correctly extracted or not. We use regular expression and fuzzy matching logic to extract {key, value} from the API Call.

% \textbf{Operation Accuracy} only available on the BiToD dataset, where API Call has operators (e.g., <>!= >=). We use fuzzy matching and exact matching for numbers to verify the operators from the API Call.

% \textbf{API Call Accuracy} evaluates whether the full API call is correct and matches everything (key, value, method names) with the ground truth API Calls. 

\stitle{Dialog Success Rate.} This metric measures the percentage of dialogs in which all API calls achieve 100\% \emph{Full API Accuracy}. In other words, it represents the proportion of dialogs where every generated API call matches the ground truth, ensuring complete correctness throughout the dialog.

\subsection{Baseline Methods}


%We pick several state-of-the-art baselines for evaluating our model's ability towards Task-Oriented dialog Systems. All these models except AutoTOD use annotated data for training/ fine-tuning, and existing methods do not report any metrics to evaluate API Calls. We have implemented the metrics and reported them. To the best of our knowledge; this is the first paper to do so.

We compare {\ours} against several strong baseline models. % Specifically, we evaluate four SOTA models using our proposed metrics. %to ensure a comprehensive analysis.

% \textbf{SimpleTOD} \cite{hosseini2020simple} is a task-oriented dialog (TOD) system that formulates dialog modeling as a sequence-to-sequence (seq2seq) problem using a single transformer-based language model. By using the dialog context and retrieved database search results, generates all outputs from a single, causal language model. Unlike traditional pipeline-based TOD systems, SimpleTOD unifies natural language understanding (NLU), dialog state tracking (DST), and response generation into a single end-to-end model.

\textbf{SimpleTOD} \cite{hosseini2020simple} treats task-oriented dialog as a single sequence generation problem, using a causal language model to predict dialog state, actions, and responses auto regressively. 

% \textbf{SOLOIST}\cite{peng2021soloist} is a task-oriented dialog (TOD) system that takes advantage of the pretraining-fine-tuning paradigm for building task-oriented dialog systems. Integrates dialog modules (e.g. state tracker, dialog policy, response generator), dialog state tracking (DST), and response generation into a single end-to-end model, reducing the need for manually designed dialog components.

\textbf{SOLOIST} \cite{peng2021soloist} is a Transformer-based task-oriented dialog system that unifies multiple dialog modules into a single pre-trained model. It leverages transfer learning and machine teaching, allowing adaptation to new tasks with minimal labeled data.


% \textbf{ZS-TOD} \cite{mosharrof2023zero} (Zero-Shot Task-Oriented dialog) is an end-to-end dialog system introduced by Mosharrof et al. in 2023. Designed to address the challenges of scaling task-oriented dialog systems to new domains without the need for large annotated labeled data, ZS-TOD emphasizes robust generalization to unseen domains through the use of domain schemas and effective dialog context summarization.

\textbf{ZS-TOD} \cite{mosharrof2023zero} is a zero-shot task-oriented dialog system that generalizes to unseen domains using domain schemas instead of memorizing task-specific patterns. It replaces full dialog history with a concise summary (previous dialog state), reducing context complexity.

% \textbf{AutoTOD} \cite{xu-etal-2024-rethinking} is a task-oriented dialog (TOD) system that departs from traditional architectures relying on multiple functional modules, such as dialog state tracking, policy management, and natural language generation. Instead, AutoTOD adopts a zero-shot autonomous approach, leveraging a general-purpose instruction-following language model, such as GPT-4, to manage the entire dialog process without the need for extensive annotated data or predefined modules.

\textbf{AutoTOD} \cite{xu-etal-2024-rethinking} is a zero-shot task-oriented dialog agent that eliminates traditional modules, relying only on instruction-following LLMs like GPT-4. It requires no task-specific training and autonomously decides actions, queries APIs, and generates responses.