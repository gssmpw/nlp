% Results and Analysis Section

% API Metric Table 3
\begin{table*}[t!]
\centering
\resizebox{\textwidth}{!}{ 
\begin{tabular}{l|l|ccc|ccc|ccc|ccc|ccc}
\toprule

\textbf{Dataset} & \textbf{LLM Model} & 
\multicolumn{3}{c|}{\makecell{\textbf{Method} \\ \textbf{Accuracy}}} & 
\multicolumn{3}{c|}{\makecell{\textbf{Param Names} \\ \textbf{Accuracy}}} & 
\multicolumn{3}{c|}{\makecell{\textbf{Param Values} \\ \textbf{Accuracy}}} & 
\multicolumn{3}{c|}{\makecell{\textbf{Operator} \\ \textbf{Accuracy}}} & 
\multicolumn{3}{c}{\makecell{\textbf{Full API} \\ \textbf{Accuracy}}} \\ 
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14} \cmidrule(lr){15-17}

& & \makecell{\textbf{Single}} & \makecell{\textbf{Multi}} & \makecell{\textbf{Both}} & 
\makecell{\textbf{Single}} & \makecell{\textbf{Multi}} & \makecell{\textbf{Both}} & 
\makecell{\textbf{Single}} & \makecell{\textbf{Multi}} & \makecell{\textbf{Both}} & 
\makecell{\textbf{Single}} & \makecell{\textbf{Multi}} & \makecell{\textbf{Both}} & 
\makecell{\textbf{Single}} & \makecell{\textbf{Multi}} & \makecell{\textbf{Both}} \\ 
\midrule

\multirow{8}{*}{\textbf{SGD}}
 & SOLOIST                & 61.56 & 65.10 & 64.51 & 44.85 & 47.50 & 47.06 & 42.96 & 45.60 & 45.16 &  &  &  & 24.50 & 26.89 & 26.50 \\ 
 & SimpleTOD              & 53.52 & 59.46 & 58.48 & 44.44 & 50.07 & 49.14 & 41.97 & 47.35 & 46.46 & N/A & N/A & N/A & 17.05 & 21.86 & 21.07 \\ 
 & ZS-TOD                 & 74.36 & 50.00 & 54.26 & 64.74 & 41.54 & 45.60 & 62.82 & 39.23 & 43.35 &  &  &  & 35.90 & 16.3 & 19.73 \\ 
 & AutoTOD                & 56.67 & 62.47 & 61.52 & 58.49 & 64.82 & 63.77 & 54.76 & 61.32 & 60.23 &  &  &  & 41.96 & 47.91 & 46.92 \\ \cline{2-17}
 & {\ours}-{\small \GPTShortName}                 & 80.60 & \underline{71.26} & \underline{72.81} & 84.72 & \underline{73.54} & \underline{75.40} & 81.44 & \underline{70.57} & \underline{72.38} &  &  &  & \underline{68.71} & \underline{57.89} & \underline{59.69} \\ 
 & {\ours}-{\small \ClaudeShortName}             & \textbf{88.29} & \textbf{81.74} & \textbf{82.83} & \textbf{88.91} & \textbf{80.59} & \textbf{81.97} & \textbf{85.66} & \textbf{77.08} & \textbf{78.51} & N/A & N/A & N/A & \textbf{72.58} & \textbf{63.04} & \textbf{64.63} \\ 
 &  {\ours}-{\small \DeepSeekShortName}            & \underline{81.11} & 69.25 & 71.22 & 83.78 & 70.80 & 72.96 & 79.42 & 66.42 & 68.58 &  &  &  & 63.27 & 51.67 & 53.59 \\ 
 & {\ours}-{\small \LlamaShortName}              & 80.32 & 69.60 & 71.39 & \underline{86.65} & 71.46 & 73.99 & \underline{82.36} & 67.56 & 70.02 &  &  &  & 63.23 & 52.58 & 54.35 \\ 
\midrule

\multirow{8}{*}{\textbf{BiTOD}}
 & SOLOIST                & 39.39 & 63.48 & 60.95 & 21.06 & 57.51 & 53.69 & 21.06 & 57.01 & 53.24 & 20.15 & 54.69 & 51.07 & 15.15 & 46.10 & 42.86 \\ 
 & SimpleTOD              & 25.00 & 59.54 & 56.21 & 25.00 & 58.94 & 55.66 & 25.00 & 58.81 & 55.55 & 24.59 & 56.58 & 53.49 & 21.43 & \underline{50.00} & \underline{47.24} \\ 
 & ZS-TOD                 & 23.08 & 33.45 & 32.59 & 23.08 & 32.46 & 31.68 & 20.83 & 30.98 & 30.14 & 23.08 & 31.68 & 30.96 & 15.38 & 19.86 & 19.49 \\ 
 & AutoTOD                & 64.29 & 48.42 & 49.84 & 41.73 & 23.84 & 25.44 & 38.45 & 21.84 & 23.33 & 31.48 & 21.15 & 22.08 & 17.86 & 14.04 & 14.38 \\ \cline{2-17}
 & {\ours}-{\small \GPTShortName}                & 82.81 & \underline{73.82} & 74.97 & 82.81 & \underline{71.20} & 72.69 & 82.31 & \textbf{66.90} & \underline{68.88} & 79.56 & 70.55 & 71.71 & \underline{68.75} & \textbf{50.17} & \textbf{52.56} \\ 
 & {\ours}-{\small \ClaudeShortName}            & \textbf{94.49} & 66.74 & \textbf{83.07} & \textbf{91.47} & 69.06 & \textbf{79.15} & \textbf{90.05} & 60.19 & \textbf{73.34} & \textbf{90.81} & \textbf{76.38} & \textbf{78.22} & \textbf{71.65} & 47.30 & 50.40 \\ 
 & {\ours}-{\small \DeepSeekShortName}            & \underline{90.55} & \textbf{75.92} & \underline{77.79} & \underline{86.93} & \textbf{72.02} & \underline{73.92} & \underline{84.69} & \underline{65.12} & 67.62 & \underline{85.33} & \underline{71.35} & \underline{73.13} & 62.20 & 44.47 & 46.73 \\ 
 & {\ours}-{\small \LlamaShortName}             & 85.16 & 61.78 & 64.81 & 83.46 & 60.84 & 63.77 & 82.43 & 54.16 & 57.82 & 79.27 & 59.19 & 61.78 & 66.41 & 35.88 & 39.83 \\ 
\bottomrule

\end{tabular}%
}
\caption{API Call Accuracy breakdown across all models on the SGD and BiTOD datasets. Accuracy is reported across multiple metrics, including method, parameter name, parameter value, operator, and overall full API accuracy. Results are shown for single-domain, multi-domain, and both domains.}
\label{table_api}
\vspace{-10pt}
\end{table*}

\section{Results and Analysis}
\subsection{Evaluating the Quality of API Calls}
Table~\ref{table_api} presents the API call accuracy results on both the SGD and BiToD datasets. 

\stitle{Comparing \ours{} Performance to Baselines.} 
A key observation is that across both datasets, nearly all variants of {\ours} outperform the baseline models across all evaluation metrics, including Method Accuracy, Param Names Accuracy, Param Values Accuracy, Operator Accuracy (for BiToD), and Full API Accuracy. Notably, when focusing on Full API Accuracy, we see substantial gains of {\ours} over baselines. For instance, \ClaudeShortName\ surpasses AutoTOD, the strongest baseline, by 37.74\% in Full API Accuracy on the SGD dataset. Similarly, on BiToD, \GPTShortName\ outperforms SimpleTOD, the best baseline model, by 11.26\%, highlighting the robustness of our approach. Moreover, to view the dialogs generated by {\ours}, please refer to Appendix~\ref{sec:Example_Dialog_Responses}.

\stitle{Open-Source vs.\ Proprietary Models.} Another notable trend in the Table~\ref{table_api} is the consistent superiority of proprietary models (\GPTShortName, \ClaudeShortName) over open-source models (\DeepSeekShortName, \LlamaShortName) in Full API Accuracy across both datasets. For example, on the SGD dataset, \ClaudeShortName\ achieves an 18.91\% higher Full API Accuracy than \LlamaShortName, highlighting the performance gap between proprietary and open-source LLMs.

\stitle{Model-Specific Observations.} Interestingly, when comparing \LlamaShortName\ and \DeepSeekShortName\ in the Table~\ref{table_api}, their relative performance depends on the dataset. While \LlamaShortName\ yields higher accuracies in most metrics on SGD, the trend reverses in BiToD, where \DeepSeekShortName\ significantly outperforms \LlamaShortName. We attribute this to \DeepSeekShortName’s closer alignment with Chinese data, which proves advantageous for BiToD’s English subset that still contains Chinese references (e.g., restaurant names). This shows that LLM performance in TOD tasks depends on alignment with the dataset's language and domain.



%Table \ref{table_api} presents the API call accuracy results. A key observation is that across both datasets, nearly all variants of {\ours} outperform baseline models across all evaluation metrics. Notably, \ClaudeShortName\ exceeds AutoTOD by 37.74\% in Full API Accuracy, while in BiTOD, \GPTShortName\ outperforms SimpleTOD—a traditional model—by 11.26\%, demonstrating the effectiveness of our approach. Another notable trend is the consistent superiority of {\ours} proprietary models over {\ours} open-source models in Full API Accuracy across both datasets. For example, in the SGD dataset, \ClaudeShortName\ achieves an 18.91\% higher Full API Accuracy than \LlamaShortName, highlighting the performance gap between proprietary and open-source models. Moreover, an interesting contrast emerges when comparing \LlamaShortName\ and \DeepSeekShortName. While \LlamaShortName\ achieves higher accuracies in almost all API metrics in the SGD dataset, the trend reverses in BiTOD, where \DeepSeekShortName\ significantly outperforms \LlamaShortName. We attribute this to the fact that \DeepSeekShortName, being a Chinese model, is better aligned with BiTOD’s dataset, which includes a substantial number of Chinese references, such as restaurant names. 

% Another notable point is that the Full API Accuracy of {\ours} on BiTOD dataset is not as good as SGD data

%Another notable trend is that alough auto tod is performing good in SGD, it does not show great perfromance on Bitod.

\subsection{Dialog-Level System Response} 
\stitle{Dialog‐Level System Response.}
From Table \ref{table_score_inform}, we observe that fine‐tuned models such as SOLOIST and SimpleTOD generally yield higher BERTScores than LLM‐powered models (including our {\ours} and AutoTOD). This is most evident on the SGD dataset, where SOLOIST achieves the highest BERTScores and SimpleTOD likewise surpasses LLM‐based methods. This suggests that supervised fine‐tuning enables closer alignment with reference responses, as measured by semantic overlap (BERTScore).

\stitle{Inform Accuracy.}
Despite lower BERTScores, LLM‐powered models excel at Inform Accuracy, which measures whether the correct slot values are returned to the user. On SGD, our {\ours} approach consistently attains the highest Inform Accuracy across Single/Multi/Both domains. Notably, {\ours} demonstrates an 82.93\% Inform Accuracy in Single‐domain settings, substantially higher than SOLOIST (44.54\%). Similarly, on BiToD, AutoTOD shows particularly strong Inform Accuracy (reaching over 90\% in Single‐domain settings), outperforming SOLOIST and SimpleTOD. These results confirm that LLMs, especially when guided by a dedicated system architecture (e.g., {\ours}), tend to be more precise in providing the requested slot values -- even if their surface‐level similarity to the reference text is lower.

%As it can be seen in Table \ref{table_score_inform}, across both datasets, fine-tuned models such as SOLOIST and SimpleTOD generally achieve higher BERT scores compared to LLM-powered approaches ({\ours} and AutoTOD). This is expected, as fine-tuned models undergo supervised optimization on task-specific data. Notably, SOLOIST achieves the highest BERT scores across all settings (single, multi, and both domains), reinforcing its strong alignment with reference responses. contrastingly, LLM-powered models ({\ours} and AutoTOD) outperform fine-tuned models in our proposed Inform Accuracy. On the SGD dataset, {\ours} achieves the best Inform Accuracy scores across all domains. On the BiTOD dataset, AutoTOD demonstrates the highest Inform Accuracy.




% We evaluate {\ours} on multiple TOD benchmarks and demonstrate that it improves task completion rates and API call accuracy while maintaining strong generalization across diverse domains. Table \ref{table_api} presents a comparative analysis of Method Accuracy, Param Names Accuracy, Param Values Accuracy, Operator Accuracy, and Full API Accuracy for each model setting. Table ~\ref{tab:table_3} provides \textbf{BERT Score (System F1)} and \textbf{Inform Accuracy}. , while Figure~\ref{fig:realtod_architecture} illustrates the \textbf{successful dialog rate trends} across different models.

% From table \ref{table_api}, we can see that {\ClaudeLongName} outperformed with a large margin even the fine-tuned baseline models for the \textbf{SGD dataset}. An explanation of the performance could be our \textbf{Prompt Chaining} mechanism effectively transfers knowledge to new domains, allowing for effective generalization without additional fine-tuning. However, the \textbf{BiTOD dataset} shows a different trend in the multi-domain setting. Sometimes, {\DeepSeekShortName} and {\GPTShortName} perform better in \textbf{parameter value accuracy and Full API Accuracy}, which suggests that different models exhibit strengths in handling various structured API components. One explanation of the results could the nature of API operators. BiTOD introduces more complex operators such as \texttt{equal\_to}, \texttt{at\_least}, \texttt{one\_of}, and \texttt{not}. These additional logical operators make the multi-domain task significantly more challenging, as the model must correctly interpret the API parameters before making API calls. Due to this complexity, additional fine-tuning or a more robust approach could be applied for the Bitod multi-domain dialogs. This complexity might explain why no single model consistently leads across all BiTOD metrics. 

% Despite these variations, the incorporation of \textbf{Prompt Chaining} and the \textbf{Fine-grained Feedback system} appears to have a stabilizing effect on the results. While the BiTOD dataset presents additional challenges, {\ours} still demonstrates its ability to generalize without requiring domain-specific fine-tuning, providing a scalable approach to TOD.


%% Table RealTOD Model Metrics Comparison with the Baseline
\begin{table}[t!]
\centering

\resizebox{1.0\linewidth}{!}{ 
\begin{tabular}{l|l|ccc|ccc}
\toprule

\textbf{Dataset} & \textbf{LLM Model} & 
\multicolumn{3}{c|}{\makecell{\textbf{BERTScore} \\ \textbf{System (F1)}}} &  
\multicolumn{3}{c}{\makecell{\textbf{Inform Accuracy}}} \\ 
\cmidrule(lr){3-5} \cmidrule(lr){6-8} 

& & 
\makecell{\textbf{Single}} & \makecell{\textbf{Multi}} & \makecell{\textbf{Both}} & 
\makecell{\textbf{Single}} & \makecell{\textbf{Multi}} & \makecell{\textbf{Both}} \\ 
\midrule

\multirow{8}{*}{\textbf{SGD}}
 & SOLOIST  & \textbf{0.7132}   & \textbf{0.7096}   & \textbf{0.7107}    & 44.54  & 54.42   & 52.66  \\ 
 & SimpleTOD  & \underline{0.6753}  & \underline{0.6758}   & \underline{0.6756}     & 32.15  & 51.11   & 47.74   \\ 
 & ZS-TOD  & 0.5119  & 0.5139  & 0.5136   & 12.32  & 11.20  & 11.40    \\ 
 & AutoTOD   & 0.5716    & 0.5937   & 0.5867    & 76.34   & \underline{75.84}  & 75.93    \\  
 & {\ours}-{\small \GPTShortName}  & 0.6547 & 0.6544 &  0.6545  & \textbf{82.93} & \textbf{76.89} & \textbf{78.03} \\ 
 & {\ours}-{\small \ClaudeShortName}   & 0.6552 & 0.6694 & 0.6649  &  79.44 & 71.37 & 72.85 \\ 
 & {\ours}-{\small \DeepSeekShortName}   & 0.6345 &  0.6384   &  0.6372   & \underline{82.81} &  75.45   &  \underline{76.88} \\ 
 & {\ours}-{\small \LlamaShortName}    & 0.6019 & 0.5979 & 0.5992 & 73.49 &  70.26 & 70.88 \\ 
\midrule

\multirow{8}{*}{\textbf{BiTOD}} 
 & SOLOIST   & 0.5479   & \underline{0.6977}   & \underline{0.6572}  & 80.0  & 69.42  & 70.62   \\ 
 & SimpleTOD     & 0.5292  & \textbf{0.7103}   & \textbf{0.6636}   & 85.0   & 71.33 & 72.88   \\ 
 & ZS-TOD   & 0.5729   & 0.655   & 0.6319  & 60.0  & 66.24  & 65.53   \\ 
 & AutoTOD    & 0.5064   & 0.5358   & 0.5277  & \textbf{97.50}   & \textbf{84.39}  & \textbf{85.87}   \\  
 & {\ours}-{\small \GPTShortName} & \textbf{0.6543} &  0.6447 & 0.6477  & \underline{85.18} &  \underline{79.78} & \underline{80.26} \\ 
 & {\ours}-{\small \ClaudeShortName} & \underline{0.6523} & 0.6392 &  0.6434  & 64.70 & 63.51 & 63.61 \\ 
 & {\ours}-{\small \DeepSeekShortName}   & 0.6454 & 0.6278 & 0.6334 & 86.66 & 79.25 & 79.93 \\ 
 & {\ours}-{\small \LlamaShortName}   & 0.5927 & 0.5694 & 0.5769 & 64.28 & 65.06 & 65.00 \\ 
\bottomrule

\end{tabular}%
}
\caption{Comparison of {\ours} with baseline models for inform accuracy and BERTScore for system response on SGD and BiTOD datasets.}
\vspace{-12pt}
\label{table_score_inform}
\end{table}


%\subsection{Error Analysis}
%Despite the advancements in LLM-based dialog modeling, all models show a {performance drop in Operator Accuracy in the multi-domain dialogs}, indicating challenges in accurately predicting API-related logical operations in complex domain settings for the BiTOD dataset. Since most of the chat instruct LLMs are trained on equality-based API calls, it is still challenging to determine the operators while generating the queries for the user to find/ complete the tasks. It might be the case that similar models like Claude-3.5-Sonnt did not perform well as single-domain dialogs. Additionally, {zero-shot models such as ZS-TOD and AuToTOD} achieve considerably lower scores across \textbf{all metrics}, indicating the importance of fine-tuning domain-specific data for better performance.


% Among all the models evaluated, Claude-3.5-Sonnet consistently outperforms all the baseline models and other LLMs of our experiments. In both datasets, Claude-3.5-Sonnet achieves the highest {Method Accuracy} of \textbf{88.29\%} and \textbf{BiTOD (94.49\%)}, surpassing \textbf{GPT-4o (80.60\%} and \textbf{82.81\%}, respectively) Similarly, {Claude-3.5-Sonnet} excels in other metrics {Param Names Accuracy, Param Values Accuracy}, and {Operator Accuracy}, demonstrating its capability of correctly extracting information from the Systrm turns for completing tasks. 

% On the other hand, {DeepSeek-V3 }and {DeepSeek-V3} records \textbf{75.92\%} {Method Accuracy } and \textbf{77.79\% Full API Accuracy}, while {\LlamaLongName } maintains a strong baseline across all metrics.

% \subsection{Fine Grained Model Comparisons}

% \textbf{Claude-3.5 vs. GPT-4o:} Claude-3.5-Sonnet outperforms GPT-4o almost all categories, with a {substantial margin in Full API Accuracy} (\textbf{72.58\% vs. 68.71\%} on SGD, \textbf{71.65\% vs. 68.75\%} on BiTOD). This suggests that Claude-3.5 performs better in handling full API prediction tasks.

% \textbf{DeepSeek-V3:} DeepSeek-V3 here performs second best before GPT-4o, especially in {Value Accuracy} and {Full API Accuracy} for {BiTOD}, by achiving \textbf{73.92\% and 62.20\%} accuracy, respectively. However, it lags behind in {SGD} dataset in {Key Accuracy} and {Operator Accuracy}.

% \textbf{\LlamaShortName\ as a Baseline:} While \textbf{\LlamaShortName} achieves considerably better accuracy than all the baseline models across different metrics, it consistently ranks \textbf{lower} than Claude-3.5-Sonnet and GPT-4o. Despite not being trained on the two datasets we use for our experiments, it outperforms all the fine-tuned baseline models regarding task completion and goals. Its {Full API Accuracy of \textbf{54.35\%} (SGD) and \textbf{39.83\%} (BiTOD)} highlights its limitations in handling complex API prediction in System generated dialogs.



\subsection{Ablation Study}
To assess the effectiveness of our proposed components -- Fine-Grained Feedback and Prompt Chaining -- we conducted an ablation study using 100 dialog conversations sampled from the SGD dataset (50 from multi-domain and 50 from single-domain). We evaluated all four variants of {\ours} (\GPTShortName, \ClaudeShortName, \LlamaShortName, and \DeepSeekShortName) under four different settings: one without either of the components, one with Fine-Grained Feedback only, one with Prompt Chaining only, and one with both components. Moreover, we used the Full API Accuracy as our comparison metric. This experimental design allowed us to isolate the impact of each component and determine their individual and combined contributions to performance. The results are provided in the Table \ref{tab:ablation_study}. As it can be seen in Table~\ref{tab:ablation_study}, adding Fine-Grained Feedback alone leads to moderate improvements, indicating its role in refining APIs. Prompt Chaining, on the other hand, provides a more substantial boost. The combination of both components yields the highest accuracy, demonstrating their complementary nature.

%% ablation study table
\begin{table}[t!]
    \centering
    \resizebox{1.0\linewidth}{!}{ 
    \begin{tabular}{cc|cccc}
        \toprule
        \makecell{\textbf{Fine-Grained} \\ \textbf{Feedback}} & \makecell{\textbf{Prompt} \\ \textbf{Chaining}} & \textbf{\GPTShortName} & \textbf{\ClaudeShortName} & \textbf{\DeepSeekShortName} & \textbf{\LlamaShortName} \\
        \midrule
        \xmark & \xmark & 51.89 & 29.92 & 45.83 & 49.62 \\
        \cmark & \xmark & 56.06 & 36.74 & 48.10& 57.19 \\
        \xmark & \cmark & \underline{64.01} & \underline{70.45} &  \underline{54.92} & \underline{59.84} \\
        \cmark & \cmark & \textbf{66.66} & \textbf{72.34} &  \textbf{59.46} & \textbf{63.63} \\

        \bottomrule
    \end{tabular}
    }
    \caption{Ablation study of full API accuracy on the SGD dataset to evaluate the impact of fine-grained feedback and prompt chaining across different LLMs.}
    \label{tab:ablation_study}
\end{table}


\begin{figure}[t!]
   \centering
   \includegraphics[width=1.0\linewidth]{Figures/overall_scores.pdf}
   \vspace{-16pt}
   \caption{
       Human evaluation results on SGD and BiTOD. Human evaluators were asked to rate the generated conversations on a scale of 1 to 5 across three key aspects: informativeness, naturalness, and task completion.
   }
   \label{fig:human_evaluation}
   \vspace{-6pt}
\end{figure}

\subsection{Human Evaluation}

We conducted a human evaluation using Amazon Mechanical Turk to assess the performance of our models. We used two baseline models to compare with all four variants of {\ours}. For our evaluation, we sampled 100 dialogs from the test sets of our chosen datasets (SGD and BiToD), with 50 each from single and multi-domain tasks. We asked the human evaluators to rate the generated conversations on a scale of 1 to 5 across three key aspects: \emph{Informativeness}, \emph{Naturalness}, and \emph{Task Completion Rate}. Figure \ref{fig:human_evaluation} shows the human evaluation results, where all four variants of {\ours} outperformed the baseline models (SOLOIST and AutoToD), supporting the reliability of our evaluation metrics. Moreover, the \ClaudeShortName\ variant of {\ours} achieved the highest average scores across all three aspects, albeit by a slight margin, further verifying the accuracy of our metrics.

%% Figure SGD Dialog Success Rate Trend
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/combined_trend_chart.pdf}
    \caption{Trend in dialog success rate across models: we notice a decline in dialog success rate as the number of API calls increases for different models across the SGD and BiTOD datasets. This trend highlights the challenge of error propagation in LLM-powered TOD systems, where mistakes in the earlier part of the dialog negatively impact subsequent interactions.}
    \label{fig:trend}
    \vspace{-10pt}
\end{figure}


\subsection{Dialog Success Rate} 
To rigorously assess the quality of the generated dialogs, we conducted an experiment to measure the dialog success rate as the number of API calls within a dialog increases. As shown in Figure \ref{fig:trend}, all variants of {\ours} exhibit a declining trend in dialog success rate as the number of API calls increases. This trend is consistent across both the SGD and BiTOD datasets. The primary reason for this decline is the interdependence of API calls. For example, when a user books a restaurant at a particular destination, the same location is often referenced for booking a taxi or later searching for nearby hotels. Any errors in earlier API calls can propagate, making subsequent calls more prone to failure. The Figure \ref{fig:trend} highlights a limitation of LLM-powered TOD systems, suggesting that they are still far from achieving perfect performance. Further research is needed to enhance their ability to handle these scenarios more effectively.