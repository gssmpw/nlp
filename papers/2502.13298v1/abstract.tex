\begin{abstract}

Task-oriented dialog (TOD) systems facilitate users in accomplishing complex, multi-turn tasks through natural language. While traditional approaches rely on extensive fine-tuning and annotated data for each domain, instruction-tuned large language models (LLMs) offer a more flexible alternative. However, LLMs struggle to reliably handle multi-turn task completion, particularly with accurately generating API calls and adapting to new domains without explicit demonstrations. 
To address these challenges, we propose {\ours}, a novel framework that enhances TOD systems through prompt chaining and fine-grained feedback mechanisms. Prompt chaining enables zero-shot domain adaptation via a two-stage prompting strategy, eliminating the need for human-curated demonstrations. Meanwhile, the fine-grained feedback mechanism improves task completion by verifying API calls against domain schemas and providing precise corrective feedback when errors are detected.
We conduct extensive experiments on the SGD and BiTOD benchmarks using four LLMs. {\ours} improves API accuracy, surpassing AutoTOD by 37.74\% on SGD and SimpleTOD by 11.26\% on BiTOD. Human evaluations further confirm that LLMs integrated with {\ours} achieve superior task completion, fluency, and informativeness compared to existing methods.
\footnote{Source code will be released upon acceptance.}


% We conduct extensive experiments on two benchmark TOD datasets, SGD and BiTOD, using four LLMs.
% Our results show that {\ours} significantly improves API accuracy, outperforming AutoTOD by 37.74\% on SGD and SimpleTOD by 11.26\% on BiTOD. Further validation through human studies confirms that all four LLMs integrated with {\ours} achieve superior task completion, fluency, and informativeness compared to competing methods. \footnote{Source code will be made public upon paper's acceptance.}
%â€”two proprietary and two open-source. 
%To overcome these limitations, we introduce {\ours}, a novel framework that enhances TOD systems via prompt chaining and fine-grained feedback mechanisms. Prompt chaining enables zero-shot domain adaptation through a two-stage prompting scheme, alleviating the need for human-curated demonstrations in each domain. 
%First, a source domain dialog is transformed into an example dialog in the target domain by aligning it with the target domain schema while preserving task-specific consistency. This generated example is used as an in-context demonstration to guide response generation in the target domain, enabling adaptation without additional human-curated dialog.
%The Fine-grained feedback mechanism further improves task reliability by verifying API calls against domain schemas and providing precise feedback for corrections when errors are detected. 
%{\ours} ensures robust task completion and seamless domain generalization while eliminating the need for per-domain customization. 


%{\ours} with \ClaudeShortName\ as the top performer.


\end{abstract}

