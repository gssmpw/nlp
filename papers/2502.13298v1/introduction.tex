\section{Introduction}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/overview.pdf} 
    \vspace{-12pt}
\caption{Overview of {\ours}: With only a single example dialog from one domain, {\ours} scales to infinitely many domains through prompt chaining, removing the need for human-curated dialogs for each domain. A fine-grained feedback loop from the API parser further improves API call accuracy.
}
\vspace{-8pt}
\label{fig:realtod_architecture}
\end{figure*}

Task-oriented dialog (TOD) systems enable users to accomplish multi-turn tasks, such as booking flights, making restaurant reservations, and managing appointments, through multi-turn, natural language interactions \cite{he-etal-2022-space}. These systems must understand user intent, retrieve relevant information from external systems via API calls, and generate coherent responses to guide users toward task completion. 
Traditional TOD systems rely on extensive domain-specific fine-tuning and manually annotated datasets, which limit their scalability to new domains and increase deployment costs~\cite{xu-etal-2024-rethinking, Mi_Wang_Li_2022}. As a result, developing TOD systems that generalize across diverse domains without extensive supervision remains an open challenge.

%  AttentionisAllYouNeed, NEURIPS2020_1457c0d6, zhang2023instruction
Recent advances in instruction-tuned large language models~(LLMs)~\cite{Chung2022, shu2024rewritelm} have significantly improved performance across a wide range of natural language processing (NLP) tasks, including text classification~\cite{sun-etal-2023-text, wang2023large, zhang2024pushing}, summarization~\cite{Pu2023SummarizationI, zhang-etal-2024-benchmarking, van2023clinical}, and response generation~\cite{Radford2019LanguageMA, brown2020language}. These models, trained on diverse instruction-following datasets, can generate fluent and contextually relevant responses in dialog settings \cite{Thoppilan2022LaMDALM, dubey2024llama, achiam2023gpt}, making them promising candidates for TOD systems. 
However, integrating LLMs into TOD remains challenging.



%However, applying LLMs to TOD poses unique challenges~\cite{chung-etal-2023-instructtods}. 
Unlike single-step NLP tasks, TOD systems require multi-turn interactions to collect task-specific information, retrieve external data, and execute user requests while adhering to domain constraints~\cite{chung-etal-2023-instructtods}. While LLMs demonstrate strong generalization in open-ended response generation~\cite{naveed2023comprehensive}, they struggle with task completion -- measured through API call accuracy \cite{10.5555/3666122.3666499, jain2024mitigating, song2025callnavi}. 
Common issues include hallucinated search results, incorrect API method names, invalid slot-value pairs, and missing required parameters, all of which can lead to execution failures and incomplete task fulfillment. Addressing these limitations is critical to unlock the full potential of LLM-based TOD systems.

To overcome these challenges, we propose {\ours}, a novel framework that enhances LLM-based TOD systems through prompt chaining and fine-grained feedback. Prompt chaining enables zero-shot domain adaptation by using a two-stage prompting process: first, an example dialog from a source domain is transformed into a target domain dialog by aligning it with the domain schema while maintaining task consistency. This generated example serves as a demonstration, allowing LLMs to generalize to new domains without human-curated examples. Meanwhile, fine-grained feedback improves API execution reliability by systematically verifying API calls against domain schemas, detecting errors (e.g., incorrect method names, invalid slot assignments, missing parameters), and providing precise corrective signals. This iterative feedback mechanism enables real-time error correction, ultimately enhancing task completion rates.




We evaluate {\ours} on two benchmark datasets -- SGD~\cite{rastogi2020towards} and BiTOD~\cite{lin2021bitod} -- using four LLMs: two proprietary models (GPT-4o and Claude) and two open-source models (DeepSeek and LLaMA). We measure task completion using full API Call Accuracy, which assesses how often the generated API calls exactly match the ground truth. Even minor errors -- such as incorrect method names, missing parameters, or invalid slot-value pairs -- can lead to execution failures, resulting in failed tasks. The quality of natural language responses is evaluated using BERTScore~\cite{zhang2019bertscore}, which measures semantic similarity between generated and reference responses using contextual embeddings, offering a more reliable assessment than traditional n-gram-based metrics.



Our results show that {\ours} significantly improves API accuracy across all models and datasets. On SGD, it surpasses AutoTOD~\cite{hosseini2020simple} by 37.74\% in full API accuracy, while on BiTOD, it outperforms supervised fine-tuned SimpleTOD~\cite{xu-etal-2024-rethinking} by 11.26\%. 
Human evaluations further confirm that LLMs integrated with {\ours} generate more fluent, informative, and effective task completions than baseline models. 
Our ablation study demonstrates that both prompt chaining and fine-grained feedback contribute to improved multi-turn dialog quality and reliable task completion.


% We evaluate {\ours} on the SGD \cite{rastogi2020towards} and BiTOD \cite{lin2021bitod} datasets, demonstrating its superior accuracy in generating API calls. {\ours} outperforms state-of-the-art task-oriented dialog (TOD) systems, achieving a 37.74\% improvement in Full API accuracy over AutoTOD on SGD and an 11.26\% gain over SimpleTOD on BiTOD. Furthermore, our human study confirms that all four LLMs integrated into {\ours} outperform baseline models in task completion, fluency, and informativeness, while our ablation study highlights the effectiveness of prompt chaining and fine-grained feedback. By eliminating domain-specific fine-tuning and improving task reliability, {\ours} sets a new benchmark for scalable, high-performance TOD systems.

