\section{Related Work}
\label{sec:lit-rev}


    % \MDnote{240521}{Maybe frame this first chunk of material with paragraph heading ``DP Conversion Frameworks'', and move ``The main work that ours follows \ldots'' to the end of it.  Again, per reviewers' comments, we want to emphasise that the idea of converting between std and metric DP is broadly applicable and not yet done, and that our specific Gauss vs VMF in SGD is just the specific instance we're looking at in this work.}
    

    
    %   \PSnote{240128}{Here is some stuff about works enabling us to carry out the theoretical comparisons.}

\emph{Converting between frameworks via R\'enyi differential privacy}
For a theoretical comparison of the VMF and Gaussian mechanisms in terms of privacy versus utility performance, we need a fair method for the conversion of their mechanism parameters into common privacy or utility metrics. 
A promising and versatile framework for such conversion is  R\'enyi differential privacy~\citep{mironov:2017:CSF}. It is capable of handling important implementation aspects such as privacy amplification through sub-sampling~\citep{ mironov2019r, zhu2019poission,balle2018privacy,wang2019subsampled} and composition through multiple epochs ~\citep{abadi-etal:2016:CCS,Shahab_ISIT_2020:100rounds} of the SGD algorithm. Ultimately, the R\'enyi differential privacy computations will need to be translated back to the traditional $(\epsilon,\delta)$ differential privacy domain, where the first approach was provided in~\citep{mironov:2017:CSF}. Later works such as~\citep{Canonne_Kamath_Steinke_2022,Shahab_ISIT_2020:100rounds} have improved the conversion in the sense that tighter achievable $(\epsilon,\delta)$ tuples can potentially be obtained from the same R\'enyi differential privacy metric. For the Gaussian mechanism, these methods have been heavily studied~\citep{abadi-etal:2016:CCS, mironov2019r,Shahab_ISIT_2020:100rounds}. In this work, we use similar techniques to present how the R\'enyi differential privacy for the VMF mechanism is useful in characterising its privacy performance and how it enables a fair comparison of the Gaussian and VMF mechanisms. 

Another work that compares Gaussian and VMF mechanisms is \cite{faustini2023directional} in which the authors investigate this, like us, in the context of DP-SGD. Our work differs from theirs in that we focus on proposing a foundational framework to compare the different notions of formal privacy guarantees that these mechanisms provide rather than simply comparing them empirically via experiments. 
As far as we know, this is the only work that has performed this comparison; in general, there is a lack of studies comparing how different noise distributions affect the privacy/utility trade-off and how noise distributions other than isotropic ones can be used for training neural networks. 

\emph{Gradient-based reconstruction attacks.} For our experimental work, where we evaluate the effect on privacy, like~\cite{faustini2023directional}, we use gradient-based reconstruction attacks to calibrate how well each of our two mechanisms can provide protection against leaking of the original data, rather than Membership Inference Attacks (MIAs). This is because $a)$ reconstruction attacks are generally agreed in the literature to be more consequential breaches of privacy~\citep{melis-etal:2019:IEEE-SP, balle-etal:2022:IEEE-SP} and $b)$ \citep{faustini2023directional} showed that, although MIAs have been used for calibrating among standard DP variants \citep{DBLP:conf/uss/Jayaraman019}, they are not for calibrating between Gaussian and VMF mechanisms, while reconstruction attacks are.

The reconstruction attack we work with is a gradient-based one, where the attacker has access to these gradients.  
    Reference~\citet{zhu-etal:2019:NeurIPS} first developed the Deep Leakage from Gradients (DLG) attack, which can recover private data from neural network architectures which are twice differentiable. The attacker creates fake inputs, and by minimising the Euclidean distance between their gradients and the gradients received from another client, the fake input moves towards the real one.  An improved version was presented by \citet{DBLP:journals/corr/abs-2001-02610}.
    
    A subsequent method that improved over these was the Inverting Gradients method (\IGA) of \citet{geiping-etal:2020:NeurIPS}, which maximises the cosine similarity between gradients. It was proved in~\citet{geiping-etal:2020:NeurIPS}  that this reconstruction attack is guaranteed to be effective against Multi-Layer Perceptrons (MLPs). 
    
There has been much work since then on developing new gradient-based attacks and on understanding the conditions under which they apply or can be mitigated, e.g., see \citep{huang-etal:2021:NeurIPS,wu-etal:2023:UAI:learning-invert,wang-etal:2023:AISTATS:reconstructing-provably}, with a useful survey provided by \citet{du2024sok}.
For this paper, we use \IGA of \citet{geiping-etal:2020:NeurIPS} as a well-established method that is known to apply to the neural network architecture we use for our experiments.

\emph{Alternative measures for privacy.} The information theory and security communities have long used leakage measures to understand the security properties of systems modelled as information-theoretic channels. Recent work in these communities has converged on common measures based on the $g$-leakage framework of Smith~\cite{Smith09,issa2019operational}. The Bayes' capacity measure emerges in both communities as a robust leakage measure for both security and privacy~\cite{Alvim20:Book,sibson1969information,issa2019operational}.~\footnote{The logarithm of the Bayes' capacity is also known as the Sibson mutual information of order $\alpha = \infty$.} 
 
 Reference~\citet{fernandes2022explaining} compared the Bayes' capacity measure to the $\epsilon$ of local differential privacy, showing that both provide robust upper bounds on leakage but describe different adversarial scenarios. Our work validates these differences experimentally.

Reference~\citet{cherubinclosed} used a Bayes' security measure --based on the existing measure of \emph{advantage}-- to measure the leakage of DP-SGD. They show that the Bayes' security is bounded by the total variation distance between the distributions of the gradients. They use this measure to evaluate the Gaussian mechanism's efficacy at protecting against membership inference attacks and attribute inference attacks. Their work differs from ours in that our measures are different (we use the Bayes' capacity measure), and our applications are different (we use our measure to compare different mechanisms against the same reconstruction attack).

Reference~\citet{guo2022bounding} proposed Fisher Information Leakage as a better measure for privacy leakage against reconstruction attacks than $\epsilon$-DP, which they note has been identified as relevant to membership inference attacks. Similar to our work they focus on semantic guarantees relevant to reconstruction. However, our work differs significantly from theirs in that our focus is on the comparison of disparate mechanisms. 


The closest work to ours is that of~\citet{hayes2023bounding}, who also propose an alternative measure for protection against reconstruction threats using a probability measure that measures the success of a reconstruction adversary. Their conclusion aligns with ours -- that $(\epsilon, \delta)$ is not a strong predictor of reconstruction accuracy. Our work differs from theirs in that our focus is on comparing different mechanisms, whereas they compare the same mechanism across different algorithms.