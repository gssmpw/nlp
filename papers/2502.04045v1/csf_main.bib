%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Natasha Fernandes at 2024-12-17 10:48:31 +0800 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{fernandes2024explaining,
	author = {Fernandes, Natasha and McIver, Annabelle and Sadeghi, Parastoo},
	booktitle = {2024 IEEE 37th Computer Security Foundations Symposium (CSF)},
	date-added = {2024-12-17 10:46:43 +0800},
	date-modified = {2024-12-17 10:48:29 +0800},
	organization = {IEEE},
	pages = {419--432},
	title = {Explaining epsilon in Local Differential Privacy Through the Lens of Quantitative Information Flow},
	year = {2024}}

@article{desfontaines2020sok,
	author = {Desfontaines, Damien and Pej{\'o}, Bal{\'a}zs},
	date-added = {2024-04-30 19:53:39 +1000},
	date-modified = {2024-04-30 19:53:39 +1000},
	journal = {Proceedings on Privacy Enhancing Technologies},
	title = {SoK: Differential privacies},
	year = {2020}}

@incollection{Bengio+chapter2007,
	author = {Bengio, Yoshua and LeCun, Yann},
	booktitle = {Large Scale Kernel Machines},
	publisher = {MIT Press},
	title = {Scaling Learning Algorithms Towards {AI}},
	year = {2007}}

@article{fernandes2022explaining,
	author = {Fernandes, Natasha and McIver, Annabelle and Sadeghi, Parastoo},
	journal = {arXiv preprint arXiv:2210.12916},
	title = {Explaining epsilon in differential privacy through the lens of information theory},
	year = {2022}}

@article{cherubinclosed,
	author = {Cherubin, Giovanni and K{\"o}pf, Boris and Paverd, Andrew and Tople, Shruti and Wutschitz, Lukas and Zanella-B{\'e}guelin, Santiago},
	title = {Closed-Form Bounds for DP-SGD against Record-level Inference Attacks}}

@article{sibson1969information,
	author = {Sibson, Robin},
	date-added = {2022-09-03 17:53:54 +1000},
	date-modified = {2022-09-03 17:53:54 +1000},
	journal = {Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und verwandte Gebiete},
	number = {2},
	pages = {149--160},
	publisher = {Springer},
	title = {Information radius},
	volume = {14},
	year = {1969}}

@article{issa2019operational,
	author = {Issa, Ibrahim and Wagner, Aaron B and Kamath, Sudeep},
	date-added = {2022-04-29 09:48:37 +1000},
	date-modified = {2022-04-29 09:48:37 +1000},
	journal = {IEEE Transactions on Information Theory},
	number = {3},
	pages = {1625--1657},
	publisher = {IEEE},
	title = {An operational approach to information leakage},
	volume = {66},
	year = {2020}}

@inproceedings{Smith09,
	author = {Geoffrey Smith},
	booktitle = {FOSSACS},
	date-added = {2022-05-13 21:50:20 +1000},
	date-modified = {2022-05-13 21:50:20 +1000},
	pages = {288--302},
	publisher = {Springer},
	series = {LNCS},
	title = {On the {F}oundations of {Q}uantitative {I}nformation {F}low},
	volume = {5504},
	year = {2009}}

@inproceedings{hayes2023bounding,
	author = {Jamie Hayes and Borja Balle and Saeed Mahloujifar},
	booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
	title = {Bounding training data reconstruction in {DP}-{SGD}},
	url = {https://openreview.net/forum?id=7LZ4tZrYlx},
	year = {2023},
	bdsk-url-1 = {https://openreview.net/forum?id=7LZ4tZrYlx}}

@inproceedings{guo2022bounding,
	author = {Guo, Chuan and Karrer, Brian and Chaudhuri, Kamalika and van der Maaten, Laurens},
	booktitle = {International Conference on Machine Learning},
	organization = {PMLR},
	pages = {8056--8071},
	title = {Bounding training data reconstruction in private (deep) learning},
	year = {2022}}

@book{Alvim20:Book,
	author = {Alvim, M{\'a}rio S. and Chatzikokolakis, Konstantinos and McIver, Annabelle and Morgan, Carroll and Palamidessi, Catuscia and Smith, Geoffrey},
	date-added = {2022-04-29 09:44:15 +1000},
	date-modified = {2022-04-29 09:44:15 +1000},
	hal_id = {hal-01971490},
	hal_version = {v1},
	publisher = {{Springer}},
	title = {{The Science of Quantitative Information Flow}},
	x-int.-audience = {yes},
	x-scientific-popularization = {no},
	x-toappear_pubdate = {yes},
	year = {2020},
	bdsk-url-1 = {https://hal.inria.fr/hal-01971490}}

@article{DBLP:journals/corr/abs-2109-12298,
	author = {Ashkan Yousefpour and Igor Shilov and Alexandre Sablayrolles and Davide Testuggine and Karthik Prasad and Mani Malek and John Nguyen and Sayan Ghosh and Akash Bharadwaj and Jessica Zhao and Graham Cormode and Ilya Mironov},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2109-12298.bib},
	eprint = {2109.12298},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Thu, 26 Jan 2023 15:24:59 +0100},
	title = {Opacus: User-Friendly Differential Privacy Library in PyTorch},
	url = {https://arxiv.org/abs/2109.12298},
	volume = {abs/2109.12298},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/2109.12298}}

@article{Hinton06,
	author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
	journal = {Neural Computation},
	pages = {1527--1554},
	title = {A Fast Learning Algorithm for Deep Belief Nets},
	volume = {18},
	year = {2006}}

@book{goodfellow2016deep,
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
	publisher = {MIT Press},
	title = {Deep learning},
	volume = {1},
	year = {2016}}

@article{jagielski2020auditing,
	author = {Jagielski, Matthew and Ullman, Jonathan and Oprea, Alina},
	date-added = {2022-12-06 09:50:07 +1100},
	date-modified = {2022-12-06 09:50:07 +1100},
	journal = {Advances in Neural Information Processing Systems},
	pages = {22205--22216},
	title = {Auditing differentially private machine learning: How private is private sgd?},
	volume = {33},
	year = {2020}}

@article{Rahman2018MembershipIA,
	author = {Md.Atiqur Rahman and Tanzila Rahman and Robert Lagani{\`e}re and Noman Mohammed},
	journal = {Trans. Data Priv.},
	pages = {61-79},
	title = {Membership Inference Attack against Differentially Private Deep Learning Model},
	volume = {11},
	year = {2018}}

@article{Xiao2017FashionMNISTAN,
	author = {Han Xiao and Kashif Rasul and Roland Vollgraf},
	journal = {ArXiv},
	title = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
	volume = {abs/1708.07747},
	year = {2017}}

@article{Mukherjee2021privGANPG,
	author = {Sumit Mukherjee and Yixi Xu and Anusua Trivedi and Nabajyoti Patowary and Juan M. Lavista Ferres},
	journal = {Proceedings on Privacy Enhancing Technologies},
	pages = {142 - 163},
	title = {privGAN: Protecting GANs from membership inference attacks at low cost to utility},
	volume = {2021},
	year = {2021}}

@inproceedings{Yu2020CloudLeakLD,
	author = {Honggang Yu and Kaichen Yang and Teng Zhang and Yun-Yun Tsai and Tsung-Yi Ho and Yier Jin},
	booktitle = {NDSS},
	title = {CloudLeak: Large-Scale Deep Learning Models Stealing Through Adversarial Examples},
	year = {2020}}

@inproceedings{10.1145/3447548.3467268,
	abstract = {In differentially private stochastic gradient descent (DPSGD), gradient clipping and random noise addition disproportionately affect underrepresented and complex classes and subgroups. As a consequence, DPSGD has disparate impact: the accuracy of a model trained using DPSGD tends to decrease more on these classes and subgroups vs. the original, non-private model. If the original model is unfair in the sense that its accuracy is not the same across all subgroups, DPSGD exacerbates this unfairness. In this work, we study the inequality in utility loss due to differential privacy, which compares the changes in prediction accuracy w.r.t. each group between the private model and the non-private model. We analyze the cost of privacy w.r.t. each group and explain how the group sample size along with other factors is related to the privacy impact on group accuracy. Furthermore, we propose a modified DPSGD algorithm, called DPSGD-F, to achieve differential privacy, equal costs of differential privacy, and good utility. DPSGD-F adaptively adjusts the contribution of samples in a group depending on the group clipping bias such that differential privacy has no disparate impact on group accuracy. Our experimental evaluation shows the effectiveness of our removal algorithm on achieving equal costs of differential privacy with satisfactory utility.},
	address = {New York, NY, USA},
	author = {Xu, Depeng and Du, Wei and Wu, Xintao},
	booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
	doi = {10.1145/3447548.3467268},
	isbn = {9781450383325},
	keywords = {stochastic gradient descent, fairness, differential privacy},
	location = {Virtual Event, Singapore},
	numpages = {9},
	pages = {1924--1932},
	publisher = {Association for Computing Machinery},
	series = {KDD '21},
	title = {Removing Disparate Impact on Model Accuracy in Differentially Private Stochastic Gradient Descent},
	url = {https://doi.org/10.1145/3447548.3467268},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3447548.3467268}}

@inproceedings{10.1145/3469877.3490594,
	abstract = {While deep learning has proved success in many critical tasks by training models from large-scale data, some private information within can be recovered from the released models, leading to the leakage of privacy. To address this problem, this paper presents a differentially private deep learning paradigm to train private models. In the approach, we propose and incorporate a simple operation termed grouped gradient clipping to modulate the gradient weights. We also incorporated the smooth sensitivity mechanism into differentially private deep learning paradigm, which bounds the adding Gaussian noise. In this way, the resulting model can simultaneously provide with strong privacy protection and avoid accuracy degradation, providing a good trade-off between privacy and performance. The theoretic advantages of grouped gradient clipping are well analyzed. Extensive evaluations on popular benchmarks and comparisons with 11 state-of-the-arts clearly demonstrate the effectiveness and genearalizability of our approach.},
	address = {New York, NY, USA},
	articleno = {14},
	author = {Liu, Haolin and Li, Chenyu and Liu, Bochao and Wang, Pengju and Ge, Shiming and Wang, Weiping},
	booktitle = {ACM Multimedia Asia},
	doi = {10.1145/3469877.3490594},
	isbn = {9781450386074},
	keywords = {Deep learning, grouped gradient clipping., differential privacy},
	location = {Gold Coast, Australia},
	numpages = {7},
	publisher = {Association for Computing Machinery},
	series = {MMAsia '21},
	title = {Differentially Private Learning with Grouped Gradient Clipping},
	url = {https://doi.org/10.1145/3469877.3490594},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3469877.3490594}}

@book{GoodBengCour16,
	address = {Cambridge, MA, USA},
	author = {Ian J. Goodfellow and Yoshua Bengio and Aaron Courville},
	note = {\url{http://www.deeplearningbook.org}},
	publisher = {MIT Press},
	title = {Deep Learning},
	year = {2016}}

@article{1284395,
	author = {Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
	doi = {10.1109/TIP.2003.819861},
	journal = {IEEE Transactions on Image Processing},
	number = {4},
	pages = {600-612},
	title = {Image quality assessment: from error visibility to structural similarity},
	volume = {13},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/TIP.2003.819861}}

@inproceedings{torkzadehmahani2019dp,
	author = {Torkzadehmahani, Reihaneh and Kairouz, Peter and Paten, Benedict},
	booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
	pages = {0--0},
	title = {DP-CGAN: Differentially Private Synthetic Data and Label Generation},
	year = {2019}}

@article{8636556,
	author = {Xu, Chugui and Ren, Ju and Zhang, Deyu and Zhang, Yaoxue and Qin, Zhan and Ren, Kui},
	doi = {10.1109/TIFS.2019.2897874},
	journal = {IEEE Transactions on Information Forensics and Security},
	number = {9},
	pages = {2358-2371},
	title = {GANobfuscator: Mitigating Information Leakage Under GAN via Differential Privacy},
	volume = {14},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TIFS.2019.2897874}}

@article{DBLP:journals/corr/abs-2001-02610,
	author = {Bo Zhao and Konda Reddy Mopuri and Hakan Bilen},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2001-02610.bib},
	eprint = {2001.02610},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Mon, 13 Jan 2020 12:40:17 +0100},
	title = {iDLG: Improved Deep Leakage from Gradients},
	url = {http://arxiv.org/abs/2001.02610},
	volume = {abs/2001.02610},
	year = {2020},
	bdsk-url-1 = {http://arxiv.org/abs/2001.02610}}

@inproceedings{7780459,
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	doi = {10.1109/CVPR.2016.90},
	pages = {770-778},
	title = {Deep Residual Learning for Image Recognition},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/CVPR.2016.90}}

@inbook{bb72cefb6cc34854965b753d1ce10cbd,
	author = {Yann Lecun},
	booktitle = {Connectionism in perspective},
	editor = {R. Pfeifer and Z. Schreter and F. Fogelman and L. Steels},
	language = {English (US)},
	publisher = {Elsevier},
	title = {Generalization and network design strategies},
	year = {1989}}

@incollection{NEURIPS2019_9015,
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	booktitle = {Advances in Neural Information Processing Systems 32},
	pages = {8024--8035},
	publisher = {Curran Associates, Inc.},
	title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	year = {2019},
	bdsk-url-1 = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}}

@inproceedings{yousefpour2021opacus,
	author = {Ashkan Yousefpour and Igor Shilov and Alexandre Sablayrolles and Davide Testuggine and Karthik Prasad and Mani Malek and John Nguyen and Sayan Ghosh and Akash Bharadwaj and Jessica Zhao and Graham Cormode and Ilya Mironov},
	booktitle = {NeurIPS 2021 Workshop Privacy in Machine Learning},
	title = {Opacus: User-Friendly Differential Privacy Library in PyTorch},
	url = {https://openreview.net/forum?id=EopKEYBoI-},
	year = {2021},
	bdsk-url-1 = {https://openreview.net/forum?id=EopKEYBoI-}}

@inproceedings{abadi-etal:2016:CCS,
	abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
	address = {New York, NY, USA},
	author = {Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
	booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
	doi = {10.1145/2976749.2978318},
	isbn = {9781450341394},
	keywords = {deep learning, differential privacy},
	location = {Vienna, Austria},
	numpages = {11},
	pages = {308--318},
	publisher = {Association for Computing Machinery},
	series = {CCS '16},
	title = {Deep Learning with Differential Privacy},
	url = {https://doi.org/10.1145/2976749.2978318},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1145/2976749.2978318}}

@techreport{Huang07labeledfaces,
	author = {Gary B. Huang and Marwan Mattar and Tamara Berg and Erik Learned-Miller},
	title = {Labeled faces in the wild: A database for studying face recognition in unconstrained environments},
	year = {2007}}

@article{deng2012mnist,
	author = {Deng, Li},
	journal = {IEEE Signal Processing Magazine},
	number = {6},
	pages = {141--142},
	publisher = {IEEE},
	title = {The mnist database of handwritten digit images for machine learning research},
	volume = {29},
	year = {2012}}

@techreport{Krizhevsky09learningmultiple,
	author = {Alex Krizhevsky},
	title = {Learning multiple layers of features from tiny images},
	year = {2009}}

@article{DBLP:journals/corr/abs-2108-01624,
	author = {Rohan Anil and Badih Ghazi and Vineet Gupta and Ravi Kumar and Pasin Manurangsi},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2108-01624.bib},
	eprint = {2108.01624},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Thu, 05 Aug 2021 14:27:08 +0200},
	title = {Large-Scale Differentially Private {BERT}},
	url = {https://arxiv.org/abs/2108.01624},
	volume = {abs/2108.01624},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/2108.01624}}

@inproceedings{tramer2021differentially,
	author = {Florian Tramer and Dan Boneh},
	booktitle = {International Conference on Learning Representations},
	title = {Differentially Private Learning Needs Better Features (or Much More Data)},
	url = {https://openreview.net/forum?id=YTWGvpFOQD-},
	year = {2021},
	bdsk-url-1 = {https://openreview.net/forum?id=YTWGvpFOQD-}}

@inproceedings{li-etal:2022:ICLR,
	author = {Xuechen Li and Florian Tramer and Percy Liang and Tatsunori Hashimoto},
	booktitle = {International Conference on Learning Representations},
	title = {Large Language Models Can Be Strong Differentially Private Learners},
	url = {https://openreview.net/forum?id=bVuP3ltATMz},
	year = {2022},
	bdsk-url-1 = {https://openreview.net/forum?id=bVuP3ltATMz}}

@inproceedings{fredrikson-etal:2015:CCS,
	abstract = {Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al., adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown. We develop a new class of model inversion attack that exploits confidence values revealed along with predictions. Our new attacks are applicable in a variety of settings, and we explore two in depth: decision trees for lifestyle surveys as used on machine-learning-as-a-service systems and neural networks for facial recognition. In both cases confidence values are revealed to those with the ability to make prediction queries to models. We experimentally show attacks that are able to estimate whether a respondent in a lifestyle survey admitted to cheating on their significant other and, in the other context, show how to recover recognizable images of people's faces given only their name and access to the ML model. We also initiate experimental exploration of natural countermeasures, investigating a privacy-aware decision tree training algorithm that is a simple variant of CART learning, as well as revealing only rounded confidence values. The lesson that emerges is that one can avoid these kinds of MI attacks with negligible degradation to utility.},
	address = {New York, NY, USA},
	author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
	booktitle = {Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
	doi = {10.1145/2810103.2813677},
	isbn = {9781450338325},
	keywords = {attacks, machine learning, privacy},
	location = {Denver, Colorado, USA},
	numpages = {12},
	pages = {1322--1333},
	publisher = {Association for Computing Machinery},
	series = {CCS '15},
	title = {Model Inversion Attacks That Exploit Confidence Information and Basic Countermeasures},
	url = {https://doi.org/10.1145/2810103.2813677},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1145/2810103.2813677}}

@article{dwork-roth:2014,
	abstract = {The problem of privacy-preserving data analysis has a long history spanning multiple disciplines. As electronic data about individuals becomes increasingly detailed, and as technology enables ever more powerful collection and curation of these data, the need increases for a robust, meaningful, and mathematically rigorous definition of privacy, together with a computationally rich class of algorithms that satisfy this definition. Differential Privacy is such a definition.After motivating and discussing the meaning of differential privacy, the preponderance of this monograph is devoted to fundamental techniques for achieving differential privacy, and application of these techniques in creative combinations, using the query-release problem as an ongoing example. A key point is that, by rethinking the computational goal, one can often obtain far better results than would be achieved by methodically replacing each step of a non-private computation with a differentially private implementation. Despite some astonishingly powerful computational results, there are still fundamental limitations --- not just on what can be achieved with differential privacy but on what can be achieved with any method that protects against a complete breakdown in privacy. Virtually all the algorithms discussed herein maintain differential privacy against adversaries of arbitrary computational power. Certain algorithms are computationally intensive, others are efficient. Computational complexity for the adversary and the algorithm are both discussed.We then turn from fundamentals to applications other than queryrelease, discussing differentially private methods for mechanism design and machine learning. The vast majority of the literature on differentially private algorithms considers a single, static, database that is subject to many analyses. Differential privacy in other models, including distributed databases and computations on data streams is discussed.Finally, we note that this work is meant as a thorough introduction to the problems and techniques of differential privacy, but is not intended to be an exhaustive survey --- there is by now a vast amount of work in differential privacy, and we can cover only a small portion of it.},
	address = {Hanover, MA, USA},
	author = {Dwork, Cynthia and Roth, Aaron},
	doi = {10.1561/0400000042},
	issn = {1551-305X},
	issue_date = {August 2014},
	journal = {Found. Trends Theor. Comput. Sci.},
	month = {aug},
	number = {3--4},
	numpages = {197},
	pages = {211--407},
	publisher = {Now Publishers Inc.},
	title = {The Algorithmic Foundations of Differential Privacy},
	url = {https://doi.org/10.1561/0400000042},
	volume = {9},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1561/0400000042}}

@inproceedings{song-etal:2013,
	author = {Song, Shuang and Chaudhuri, Kamalika and Sarwate, Anand D.},
	booktitle = {2013 IEEE Global Conference on Signal and Information Processing},
	doi = {10.1109/GlobalSIP.2013.6736861},
	pages = {245-248},
	title = {Stochastic gradient descent with differentially private updates},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/GlobalSIP.2013.6736861}}

@inproceedings{wang-etal:2018,
	abstract = {Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.},
	address = {Brussels, Belgium},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
	booktitle = {Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}},
	doi = {10.18653/v1/W18-5446},
	month = nov,
	pages = {353--355},
	publisher = {Association for Computational Linguistics},
	title = {{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
	url = {https://aclanthology.org/W18-5446},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/W18-5446},
	bdsk-url-2 = {https://doi.org/10.18653/v1/W18-5446}}

@inproceedings{weggenmann-kerschbaum:2021:CCS,
	abstract = {Directional data is an important class of data where the magnitudes of the data points are negligible. It naturally occurs in many real-world scenarios: For instance, geographic locations (approximately) lie on a sphere, and periodic data such as time of day, or day of week can be interpreted as points on a circle. Massive amounts of directional data are collected by location-based service platforms such as Google Maps or Foursquare, who depend on mobility data from users' smartphones or wearable devices to enable their analytics and marketing businesses. However, such data is often highly privacy-sensitive and hence demands measures to protect the privacy of the individuals whose data is collected and processed. Starting with the von Mises-Fisher distribution, we therefore propose and analyze two novel privacy mechanisms for directional data by combining directional statistics with differential privacy, which presents the current state-of-the-art for quantifying and limiting information disclosure about individuals. As we will see, our specialized privacy mechanisms achieve a better privacy-utility trade-off than ex post adaptions of established mechanisms to directional data.},
	address = {New York, NY, USA},
	author = {Weggenmann, Benjamin and Kerschbaum, Florian},
	booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
	doi = {10.1145/3460120.3484734},
	isbn = {9781450384544},
	keywords = {data anonymization, directional data, differential privacy},
	location = {Virtual Event, Republic of Korea},
	numpages = {18},
	pages = {1205--1222},
	publisher = {Association for Computing Machinery},
	series = {CCS '21},
	title = {Differential Privacy for Directional Data},
	url = {https://doi.org/10.1145/3460120.3484734},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3460120.3484734}}

@inproceedings{szegedy-etal:2014:ICLR,
	author = {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian J. Goodfellow and Rob Fergus},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/SzegedyZSBEGF13.bib},
	booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
	editor = {Yoshua Bengio and Yann LeCun},
	timestamp = {Thu, 25 Jul 2019 14:35:25 +0200},
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	year = {2014},
	bdsk-url-1 = {http://arxiv.org/abs/1312.6199}}

@inproceedings{andres2013geo,
	author = {Andr{\'e}s, Miguel E and Bordenabe, Nicol{\'a}s E and Chatzikokolakis, Konstantinos and Palamidessi, Catuscia},
	booktitle = {Proceedings of the 2013 ACM SIGSAC conference on Computer \& communications security},
	organization = {ACM},
	pages = {901--914},
	title = {Geo-indistinguishability: Differential privacy for location-based systems},
	year = {2013}}

@inproceedings{mcmahan-etal:2018:ICLR,
	author = {H. Brendan McMahan and Daniel Ramage and Kunal Talwar and Li Zhang},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/iclr/McMahanRT018.bib},
	booktitle = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher = {OpenReview.net},
	timestamp = {Thu, 25 Jul 2019 14:25:48 +0200},
	title = {Learning {Differentially Private} Recurrent Language Models},
	url = {https://openreview.net/forum?id=BJ0hF1Z0b},
	year = {2018},
	bdsk-url-1 = {https://openreview.net/forum?id=BJ0hF1Z0b}}

@inproceedings{zhu-etal:2019:NeurIPS,
	author = {Ligeng Zhu and Zhijian Liu and Song Han},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/nips/ZhuLH19.bib},
	booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada},
	editor = {Hanna M. Wallach and Hugo Larochelle and Alina Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and Emily B. Fox and Roman Garnett},
	pages = {14747--14756},
	timestamp = {Mon, 16 May 2022 15:41:51 +0200},
	title = {Deep Leakage from Gradients},
	url = {https://proceedings.neurips.cc/paper/2019/hash/60a6c4002cc7b29142def8871531281a-Abstract.html},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2019/hash/60a6c4002cc7b29142def8871531281a-Abstract.html}}

@inproceedings{geiping-etal:2020:NeurIPS,
	author = {Jonas Geiping and Hartmut Bauermeister and Hannah Dr{\"{o}}ge and Michael Moeller},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/nips/GeipingBD020.bib},
	booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual},
	editor = {Hugo Larochelle and Marc'Aurelio Ranzato and Raia Hadsell and Maria{-}Florina Balcan and Hsuan{-}Tien Lin},
	timestamp = {Tue, 19 Jan 2021 15:57:14 +0100},
	title = {Inverting Gradients - How easy is it to break privacy in federated learning?},
	url = {https://proceedings.neurips.cc/paper/2020/hash/c4ede56bbd98819ae6112b20ac6bf145-Abstract.html},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2020/hash/c4ede56bbd98819ae6112b20ac6bf145-Abstract.html}}

@inproceedings{wei-etal:2020:ESORICS,
	author = {Wenqi Wei and Ling Liu and Margaret Loper and Ka Ho Chow and Mehmet Emre Gursoy and Stacey Truex and Yanzhao Wu},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/esorics/WeiLLCGTW20.bib},
	booktitle = {Computer Security - {ESORICS} 2020 - 25th European Symposium on Research in Computer Security, {ESORICS} 2020, Guildford, UK, September 14-18, 2020, Proceedings, Part {I}},
	doi = {10.1007/978-3-030-58951-6\_27},
	editor = {Liqun Chen and Ninghui Li and Kaitai Liang and Steve A. Schneider},
	pages = {545--566},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	timestamp = {Fri, 02 Jul 2021 20:12:27 +0200},
	title = {A Framework for Evaluating Client Privacy Leakages in Federated Learning},
	url = {https://doi.org/10.1007/978-3-030-58951-6\_27},
	volume = {12308},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-58951-6%5C_27}}

@inproceedings{scheliga-etal:2022:WACV,
	author = {Daniel Scheliga and Patrick M{\"{a}}der and Marco Seeland},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/wacv/ScheligaMS22.bib},
	booktitle = {{IEEE/CVF} Winter Conference on Applications of Computer Vision, {WACV} 2022, Waikoloa, HI, USA, January 3-8, 2022},
	doi = {10.1109/WACV51458.2022.00366},
	pages = {3605--3614},
	publisher = {{IEEE}},
	timestamp = {Thu, 17 Feb 2022 14:51:17 +0100},
	title = {{PRECODE} - {A} Generic Model Extension to Prevent Deep Gradient Leakage},
	url = {https://doi.org/10.1109/WACV51458.2022.00366},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/WACV51458.2022.00366}}

@article{dwork-etal:2019,
	author = {Cynthia Dwork and Nitin Kohli and Deirdre K. Mulligan},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/jpc/DworkKM19.bib},
	doi = {10.29012/jpc.689},
	journal = {J. Priv. Confidentiality},
	number = {2},
	timestamp = {Wed, 12 May 2021 17:24:22 +0200},
	title = {Differential Privacy in Practice: Expose your Epsilons!},
	url = {https://doi.org/10.29012/jpc.689},
	volume = {9},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.29012/jpc.689}}

@inproceedings{lee-clifton:2011,
	author = {Jaewoo Lee and Chris Clifton},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/isw/LeeC11.bib},
	booktitle = {Information Security, 14th International Conference, {ISC} 2011, Xi'an, China, October 26-29, 2011. Proceedings},
	doi = {10.1007/978-3-642-24861-0\_22},
	editor = {Xuejia Lai and Jianying Zhou and Hui Li},
	pages = {325--340},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	timestamp = {Tue, 14 May 2019 10:00:53 +0200},
	title = {How Much Is Enough? Choosing {\(\epsilon\)} for Differential Privacy},
	url = {https://doi.org/10.1007/978-3-642-24861-0\_22},
	volume = {7001},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-642-24861-0%5C_22}}

@book{pejo-desfontaines:2022,
	author = {Bal{\'{a}}zs Pej{\'{o}} and Damien Desfontaines},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/series/sbcs/PejoD22.bib},
	doi = {10.1007/978-3-030-96398-9},
	isbn = {978-3-030-96397-2},
	publisher = {Springer},
	series = {Springer Briefs in Computer Science},
	timestamp = {Mon, 11 Apr 2022 17:28:57 +0200},
	title = {Guide to {Differential Privacy} Modifications - {A} Taxonomy of Variants and Extensions},
	url = {https://doi.org/10.1007/978-3-030-96398-9},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-96398-9}}

@inproceedings{chatzikokolakis-etal:2013:PETS,
	author = {Konstantinos Chatzikokolakis and Miguel E. Andr{\'{e}}s and Nicol{\'{a}}s Emilio Bordenabe and Catuscia Palamidessi},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/pet/ChatzikokolakisABP13.bib},
	booktitle = {Privacy Enhancing Technologies - 13th International Symposium, {PETS} 2013, Bloomington, IN, USA, July 10-12, 2013. Proceedings},
	doi = {10.1007/978-3-642-39077-7\_5},
	editor = {Emiliano De Cristofaro and Matthew K. Wright},
	pages = {82--102},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	timestamp = {Fri, 09 Apr 2021 18:42:13 +0200},
	title = {Broadening the Scope of Differential Privacy Using Metrics},
	url = {https://doi.org/10.1007/978-3-642-39077-7\_5},
	volume = {7981},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-642-39077-7%5C_5}}

@article{wang-etal:2019,
	author = {Zhibo Wang and Jiahui Hu and Ruizhao Lv and Jian Wei and Qian Wang and Dejun Yang and Hairong Qi},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/tmc/WangHLWWYQ19.bib},
	doi = {10.1109/TMC.2018.2861393},
	journal = {{IEEE} Trans. Mob. Comput.},
	number = {6},
	pages = {1330--1341},
	timestamp = {Thu, 29 Jul 2021 14:12:45 +0200},
	title = {Personalized Privacy-Preserving Task Allocation for Mobile Crowdsensing},
	url = {https://doi.org/10.1109/TMC.2018.2861393},
	volume = {18},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TMC.2018.2861393}}

@article{wang-etal:2017,
	author = {Yu Wang and Zhenqi Huang and Sayan Mitra and Geir E. Dullerud},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/tcns/WangHMD17.bib},
	doi = {10.1109/TCNS.2017.2658190},
	journal = {{IEEE} Trans. Control. Netw. Syst.},
	number = {1},
	pages = {118--130},
	timestamp = {Thu, 09 Apr 2020 17:10:56 +0200},
	title = {Differential Privacy in Linear Distributed Control Systems: Entropy Minimizing Mechanisms and Performance Tradeoffs},
	url = {https://doi.org/10.1109/TCNS.2017.2658190},
	volume = {4},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TCNS.2017.2658190}}

@article{zhao-etal:2022:TKDE,
	author = {Zhao, Ying and Yuan, Dong and Du, Jia Tina and Chen, Jinjun},
	doi = {10.1109/TKDE.2022.3192360},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	pages = {1-11},
	title = {Geo-Ellipse-Indistinguishability: Community-Aware Location Privacy Protection for Directional Distribution},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TKDE.2022.3192360}}

@article{yang-etal:2022,
	author = {Yang, Mengmeng and Tjuawinata, Ivan and Lam, Kwok-Yan},
	doi = {10.1109/TIFS.2022.3189532},
	journal = {IEEE Transactions on Information Forensics and Security},
	pages = {2524-2537},
	title = {K-Means Clustering With Local d-Privacy for Privacy-Preserving Data Analysis},
	volume = {17},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TIFS.2022.3189532}}

@inproceedings{xiao-xiong:2015:CCS,
	author = {Yonghui Xiao and Li Xiong},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/ccs/XiaoX15.bib},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSAC} Conference on Computer and Communications Security, Denver, CO, USA, October 12-16, 2015},
	doi = {10.1145/2810103.2813640},
	editor = {Indrajit Ray and Ninghui Li and Christopher Kruegel},
	pages = {1298--1309},
	publisher = {{ACM}},
	timestamp = {Fri, 09 Apr 2021 18:39:44 +0200},
	title = {Protecting Locations with Differential Privacy under Temporal Correlations},
	url = {https://doi.org/10.1145/2810103.2813640},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1145/2810103.2813640}}

@inproceedings{chatzikokolakis-etal:2014:PETS,
	author = {Konstantinos Chatzikokolakis and Catuscia Palamidessi and Marco Stronati},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/pet/ChatzikokolakisPS14.bib},
	booktitle = {Privacy Enhancing Technologies - 14th International Symposium, {PETS} 2014, Amsterdam, The Netherlands, July 16-18, 2014. Proceedings},
	doi = {10.1007/978-3-319-08506-7\_2},
	editor = {Emiliano De Cristofaro and Steven J. Murdoch},
	pages = {21--41},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	timestamp = {Sat, 09 Apr 2022 12:39:29 +0200},
	title = {A Predictive Differentially-Private Mechanism for Mobility Traces},
	url = {https://doi.org/10.1007/978-3-319-08506-7\_2},
	volume = {8555},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-319-08506-7%5C_2}}

@inproceedings{Chatzi:2019,
	author = {Konstantinos Chatzikokolakis and Natasha Fernandes and Catuscia Palamidessi},
	booktitle = {Proc. CSF},
	date-modified = {2021-02-17 11:48:07 +1100},
	publisher = {IEEE Press},
	title = {Comparing Systems: Max-case Refinement Orders and Application to Differential Privacy},
	year = {2019}}

@inproceedings{fernandes-etal:2019:POST,
	author = {Natasha Fernandes and Mark Dras and Annabelle McIver},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/post/FernandesDM19.bib},
	booktitle = {Principles of Security and Trust - 8th International Conference, {POST} 2019, Held as Part of the European Joint Conferences on Theory and Practice of Software, {ETAPS} 2019, Prague, Czech Republic, April 6-11, 2019, Proceedings},
	doi = {10.1007/978-3-030-17138-4\_6},
	editor = {Flemming Nielson and David Sands},
	pages = {123--148},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	timestamp = {Sun, 25 Oct 2020 23:08:54 +0100},
	title = {Generalised Differential Privacy for Text Document Processing},
	url = {https://doi.org/10.1007/978-3-030-17138-4\_6},
	volume = {11426},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-17138-4%5C_6}}

@inproceedings{feyisetan-etal:2019,
	author = {Oluwaseyi Feyisetan and Tom Diethe and Thomas Drake},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/icdm/FeyisetanDD19.bib},
	booktitle = {2019 {IEEE} International Conference on Data Mining, {ICDM} 2019, Beijing, China, November 8-11, 2019},
	doi = {10.1109/ICDM.2019.00031},
	editor = {Jianyong Wang and Kyuseok Shim and Xindong Wu},
	pages = {210--219},
	publisher = {{IEEE}},
	timestamp = {Tue, 26 Jan 2021 13:51:21 +0100},
	title = {Leveraging Hierarchical Representations for Preserving Privacy and Utility in Text},
	url = {https://doi.org/10.1109/ICDM.2019.00031},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/ICDM.2019.00031}}

@inproceedings{feyisetan-etal:2020,
	author = {Oluwaseyi Feyisetan and Borja Balle and Thomas Drake and Tom Diethe},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/wsdm/FeyisetanBDD20.bib},
	booktitle = {{WSDM} '20: The Thirteenth {ACM} International Conference on Web Search and Data Mining, Houston, TX, USA, February 3-7, 2020},
	doi = {10.1145/3336191.3371856},
	editor = {James Caverlee and Xia (Ben) Hu and Mounia Lalmas and Wei Wang},
	pages = {178--186},
	publisher = {{ACM}},
	timestamp = {Fri, 24 Jan 2020 12:03:51 +0100},
	title = {Privacy- and Utility-Preserving Textual Analysis via Calibrated Multivariate Perturbations},
	url = {https://doi.org/10.1145/3336191.3371856},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3336191.3371856}}

@inproceedings{weggenmann-etal:2022:WWW,
	author = {Benjamin Weggenmann and Valentin Rublack and Michael Andrejczuk and Justus Mattern and Florian Kerschbaum},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/www/WeggenmannRAMK22.bib},
	booktitle = {{WWW} '22: The {ACM} Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022},
	doi = {10.1145/3485447.3512232},
	editor = {Fr{\'{e}}d{\'{e}}rique Laforest and Rapha{\"{e}}l Troncy and Elena Simperl and Deepak Agarwal and Aristides Gionis and Ivan Herman and Lionel M{\'{e}}dini},
	pages = {721--731},
	publisher = {{ACM}},
	timestamp = {Thu, 23 Jun 2022 19:54:34 +0200},
	title = {{DP-VAE:} Human-Readable Text Anonymization for Online Reviews with Differentially Private Variational Autoencoders},
	url = {https://doi.org/10.1145/3485447.3512232},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3485447.3512232}}

@inproceedings{qu-etal:2021:CIKM,
	author = {Chen Qu and Weize Kong and Liu Yang and Mingyang Zhang and Michael Bendersky and Marc Najork},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/cikm/QuKY0BN21.bib},
	booktitle = {{CIKM} '21: The 30th {ACM} International Conference on Information and Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021},
	doi = {10.1145/3459637.3482281},
	editor = {Gianluca Demartini and Guido Zuccon and J. Shane Culpepper and Zi Huang and Hanghang Tong},
	pages = {1488--1497},
	publisher = {{ACM}},
	timestamp = {Tue, 16 Aug 2022 23:04:38 +0200},
	title = {Natural Language Understanding with Privacy-Preserving {BERT}},
	url = {https://doi.org/10.1145/3459637.3482281},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3459637.3482281}}

@inproceedings{mironov:2017:CSF,
	author = {Ilya Mironov},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/csfw/Mironov17.bib},
	booktitle = {30th {IEEE} Computer Security Foundations Symposium, {CSF} 2017, Santa Barbara, CA, USA, August 21-25, 2017},
	doi = {10.1109/CSF.2017.11},
	pages = {263--275},
	publisher = {{IEEE} Computer Society},
	timestamp = {Wed, 16 Oct 2019 14:14:49 +0200},
	title = {R{\'{e}}nyi Differential Privacy},
	url = {https://doi.org/10.1109/CSF.2017.11},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/CSF.2017.11}}

@inproceedings{fernandes:22:CSF,
	author = {N. Fernandes and A. McIver and C. Palamidessi and M. Ding},
	booktitle = {35th {IEEE} Computer Security Foundations Symposium, {CSF} 2022, Haifa, Israel},
	doi = {10.1109/CSF54842.2022.00022},
	pages = {332-347},
	publisher = {{IEEE} Computer Society},
	title = {Universal Optimality and Robust Utility Bounds for Metric Differential Privacy},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/CSF54842.2022.00022}}

@inproceedings{bassily-etal:2014:FOCS,
	author = {Raef Bassily and Adam D. Smith and Abhradeep Thakurta},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/focs/BassilyST14.bib},
	booktitle = {55th {IEEE} Annual Symposium on Foundations of Computer Science, {FOCS} 2014, Philadelphia, PA, USA, October 18-21, 2014},
	doi = {10.1109/FOCS.2014.56},
	pages = {464--473},
	publisher = {{IEEE} Computer Society},
	timestamp = {Wed, 16 Oct 2019 14:14:54 +0200},
	title = {Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds},
	url = {https://doi.org/10.1109/FOCS.2014.56},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/FOCS.2014.56}}

@article{gong-etal:2020,
	author = {Gong, Maoguo and Xie, Yu and Pan, Ke and Feng, Kaiyuan and Qin, A.K.},
	doi = {10.1109/MCI.2020.2976185},
	journal = {IEEE Computational Intelligence Magazine},
	number = {2},
	pages = {49-64},
	title = {A Survey on Differentially Private Machine Learning [Review Article]},
	volume = {15},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/MCI.2020.2976185}}

@misc{de-etal:2022,
	author = {De, Soham and Berrada, Leonard and Hayes, Jamie and Smith, Samuel L. and Balle, Borja},
	copyright = {Creative Commons Attribution 4.0 International},
	doi = {10.48550/ARXIV.2204.13650},
	keywords = {Machine Learning (cs.LG), Cryptography and Security (cs.CR), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Unlocking High-Accuracy Differentially Private Image Classification through Scale},
	url = {https://arxiv.org/abs/2204.13650},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2204.13650},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2204.13650}}

@inproceedings{li-liu:2020,
	author = {Yinan Li and Fang Liu},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/isnn/LiL20.bib},
	booktitle = {Advances in Neural Networks - {ISNN} 2020 - 17th International Symposium on Neural Networks, {ISNN} 2020, Cairo, Egypt, December 4-6, 2020, Proceedings},
	doi = {10.1007/978-3-030-64221-1\_16},
	editor = {Min Han and Sitian Qin and Nian Zhang},
	pages = {176--189},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	timestamp = {Thu, 16 Sep 2021 11:02:30 +0200},
	title = {Adaptive Gaussian Noise Injection Regularization for Neural Networks},
	url = {https://doi.org/10.1007/978-3-030-64221-1\_16},
	volume = {12557},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-64221-1%5C_16}}

@inproceedings{li-liu:2021,
	abstract = {We propose Noise-Augmented Privacy-Preserving Empirical Risk Minimization (NAPP-ERM) that solves ERM with differential privacy (DP) guarantees. Existing privacy-preserving ERM approaches may be subject to over-regularization with the employment of a {\$}{\$}l{\_}2{\$}{\$}l2term to achieve strong convexity on top of the target regularization. NAPP-ERM improves over the current approaches and mitigates over-regularization by iteratively realizing the target regularization through appropriately designed noisy augmented data and delivering strong convexity via a single adaptively weighted dual-purpose {\$}{\$}l{\_}2{\$}{\$}l2regularizer. When the target regularization is for variable selection, we propose a new regularizer that achieves privacy and sparsity guarantees simultaneously. Finally, we propose a strategy to retrieve the privacy budget when the strong convexity requirement is met, which can be returned to users such that DP is guaranteed at a lower privacy cost than originally planned, or be recycled to the ERM optimization procedure to reduce the magnitude of injected DP noise and improve the utility of DP-ERM. From an implementation perspective, NAPP-ERM can be achieved by optimizing a non-perturbed object function given noise-augmented data and can thus leverage existing tools for non-private ERM optimization. We illustrate through extensive experiments the mitigation effect of the over-regularization and private budget retrieval by NAPP-ERM on variable selection and outcome prediction.},
	address = {Cham},
	author = {Li, Yinan and Liu, Fang},
	booktitle = {Intelligent Computing},
	editor = {Arai, Kohei},
	isbn = {978-3-031-10467-1},
	pages = {660--681},
	publisher = {Springer International Publishing},
	title = {Noise-Augmented Privacy-Preserving Empirical Risk Minimization with Dual-Purpose Regularizer and Privacy Budget Retrieval and Recycling},
	year = {2022}}

@inproceedings{phan-etal:2017:ICDM,
	author = {NhatHai Phan and Xintao Wu and Han Hu and Dejing Dou},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/icdm/PhanWHD17.bib},
	booktitle = {2017 {IEEE} International Conference on Data Mining, {ICDM} 2017, New Orleans, LA, USA, November 18-21, 2017},
	doi = {10.1109/ICDM.2017.48},
	editor = {Vijay Raghavan and Srinivas Aluru and George Karypis and Lucio Miele and Xindong Wu},
	pages = {385--394},
	publisher = {{IEEE} Computer Society},
	timestamp = {Fri, 09 Apr 2021 18:44:32 +0200},
	title = {Adaptive Laplace Mechanism: Differential Privacy Preservation in Deep Learning},
	url = {https://doi.org/10.1109/ICDM.2017.48},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/ICDM.2017.48}}

@inproceedings{phan-etal:2019:IJCAI,
	author = {NhatHai Phan and Minh N. Vu and Yang Liu and Ruoming Jin and Dejing Dou and Xintao Wu and My T. Thai},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/ijcai/PhanVLJDWT19.bib},
	booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, {IJCAI} 2019, Macao, China, August 10-16, 2019},
	doi = {10.24963/ijcai.2019/660},
	editor = {Sarit Kraus},
	pages = {4753--4759},
	publisher = {ijcai.org},
	timestamp = {Fri, 09 Apr 2021 18:53:49 +0200},
	title = {Heterogeneous Gaussian Mechanism: Preserving Differential Privacy in Deep Learning with Provable Robustness},
	url = {https://doi.org/10.24963/ijcai.2019/660},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.24963/ijcai.2019/660}}

@misc{nilsson-akenine:2020,
	author = {Nilsson, Jim and Akenine-M{\"o}ller, Tomas},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2006.13846},
	keywords = {Image and Video Processing (eess.IV), Graphics (cs.GR), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences, I.3.6; C.4},
	publisher = {arXiv},
	title = {Understanding SSIM},
	url = {https://arxiv.org/abs/2006.13846},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2006.13846},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2006.13846}}

@article{lu-etal:2022:TIFS,
	author = {Lu, Zhigang and Asghar, Hassan Jameel and Kaafar, Mohamed Ali and Webb, Darren and Dickinson, Peter},
	doi = {10.1109/TIFS.2022.3169911},
	journal = {IEEE Transactions on Information Forensics and Security},
	pages = {2151-2165},
	title = {A Differentially Private Framework for Deep Learning With Convexified Loss Functions},
	volume = {17},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TIFS.2022.3169911}}

@inproceedings{DBLP:conf/uss/Jayaraman019,
	author = {Bargav Jayaraman and David Evans},
	booktitle = {28th {USENIX} Security Symposium, {USENIX} Security 2019, Santa Clara, CA, USA, August 14-16, 2019},
	editor = {Nadia Heninger and Patrick Traynor},
	pages = {1895--1912},
	publisher = {{USENIX} Association},
	title = {Evaluating Differentially Private Machine Learning in Practice},
	year = {2019}}

@article{papernot-etal:2021:AAAI,
	abstractnote = {Because learning sometimes involves sensitive data, machine learning algorithms have been extended to offer differential privacy for training data. In practice, this has been mostly an afterthought, with privacy-preserving models obtained by re-running training with a different optimizer, but using the model architectures that already performed well in a non-privacy-preserving setting. This approach leads to less than ideal privacy/utility tradeoffs, as we show here. To improve these tradeoffs, prior work introduces variants of differential privacy that weaken the privacy guarantee proved to increase model utility. We show this is not necessary and instead propose that utility be improved by choosing activation functions designed explicitly for privacy-preserving training. A crucial operation in differentially private SGD is gradient clipping, which along with modifying the optimization path (at times resulting in not-optimizing a single objective function), may also introduce both significant bias and variance to the learning process. We empirically identify exploding gradients arising from ReLU may be one of the main sources of this. We demonstrate analytically and experimentally how a general family of bounded activation functions, the tempered sigmoids, consistently outperform the currently established choice: unbounded activation functions like ReLU. Using this paradigm, we achieve new state-of-the-art accuracy on MNIST, FashionMNIST, and CIFAR10 without any modification of the learning procedure fundamentals or differential privacy analysis. While the changes we make are simple in retrospect, the simplicity of our approach facilitates its implementation and adoption to meaningfully improve state-of-the-art machine learning while still providing strong guarantees in the original framework of differential privacy.},
	author = {Papernot, Nicolas and Thakurta, Abhradeep and Song, Shuang and Chien, Steve and Erlingsson, {\~A}{\v s}lfar},
	doi = {10.1609/aaai.v35i10.17123},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	month = {May},
	number = {10},
	pages = {9312-9321},
	title = {Tempered Sigmoid Activations for Deep Learning with Differential Privacy},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17123},
	volume = {35},
	year = {2021},
	bdsk-url-1 = {https://ojs.aaai.org/index.php/AAAI/article/view/17123},
	bdsk-url-2 = {https://doi.org/10.1609/aaai.v35i10.17123}}

@inproceedings{wainakh-etal:2022:PETS,
	author = {Aidmar Wainakh and Fabrizio Ventola and Till M{\"u}{\ss}ig and Jens Keim and Carlos Garcia Cordero and Ephraim Zimmer and Tim Grube and Kristian Kersting and Max M{\"u}hlh{\"a}user},
	booktitle = {Proceedings on Privacy Enhancing Technologies},
	pages = {227--244},
	title = {User-Level Label Leakage from Gradients in Federated Learning},
	year = 2022}

@inproceedings{DBLP:conf/cvpr/Li0L022,
	author = {Zhuohang Li and Jiaxin Zhang and Luyang Liu and Jian Liu},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/cvpr/Li0L022.bib},
	booktitle = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition, {CVPR} 2022, New Orleans, LA, USA, June 18-24, 2022},
	doi = {10.1109/CVPR52688.2022.00989},
	pages = {10122--10132},
	publisher = {{IEEE}},
	timestamp = {Wed, 05 Oct 2022 16:31:19 +0200},
	title = {Auditing Privacy Defenses in Federated Learning via Generative Gradient Leakage},
	url = {https://doi.org/10.1109/CVPR52688.2022.00989},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/CVPR52688.2022.00989}}

@inproceedings{DBLP:conf/csfw/YeomGFJ18,
	author = {Samuel Yeom and Irene Giacomelli and Matt Fredrikson and Somesh Jha},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/csfw/YeomGFJ18.bib},
	booktitle = {31st {IEEE} Computer Security Foundations Symposium, {CSF} 2018, Oxford, United Kingdom, July 9-12, 2018},
	doi = {10.1109/CSF.2018.00027},
	pages = {268--282},
	publisher = {{IEEE} Computer Society},
	timestamp = {Wed, 16 Oct 2019 14:14:49 +0200},
	title = {Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting},
	url = {https://doi.org/10.1109/CSF.2018.00027},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/CSF.2018.00027}}

@article{Shokri2016MembershipIA,
	author = {R. Shokri and Marco Stronati and Congzheng Song and Vitaly Shmatikov},
	journal = {2017 IEEE Symposium on Security and Privacy (SP)},
	pages = {3-18},
	title = {Membership Inference Attacks Against Machine Learning Models},
	year = {2016}}

@inproceedings{DBLP:conf/ndss/Salem0HBF019,
	author = {Ahmed Salem and Yang Zhang and Mathias Humbert and Pascal Berrang and Mario Fritz and Michael Backes},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/ndss/Salem0HBF019.bib},
	booktitle = {26th Annual Network and Distributed System Security Symposium, {NDSS} 2019, San Diego, California, USA, February 24-27, 2019},
	publisher = {The Internet Society},
	timestamp = {Mon, 01 Feb 2021 08:42:22 +0100},
	title = {ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models},
	url = {https://www.ndss-symposium.org/ndss-paper/ml-leaks-model-and-data-independent-membership-inference-attacks-and-defenses-on-machine-learning-models/},
	year = {2019},
	bdsk-url-1 = {https://www.ndss-symposium.org/ndss-paper/ml-leaks-model-and-data-independent-membership-inference-attacks-and-defenses-on-machine-learning-models/}}

@inproceedings{pmlr-v139-choquette-choo21a,
	abstract = {Membership inference is one of the simplest privacy threats faced by machine learning models that are trained on private sensitive data. In this attack, an adversary infers whether a particular point was used to train the model, or not, by observing the model's predictions. Whereas current attack methods all require access to the model's predicted confidence score, we introduce a label-only attack that instead evaluates the robustness of the model's predicted (hard) labels under perturbations of the input, to infer membership. Our label-only attack is not only as-effective as attacks requiring access to confidence scores, it also demonstrates that a class of defenses against membership inference, which we call ``confidence masking'' because they obfuscate the confidence scores to thwart attacks, are insufficient to prevent the leakage of private information. Our experiments show that training with differential privacy or strong L2 regularization are the only current defenses that meaningfully decrease leakage of private information, even for points that are outliers of the training distribution.},
	author = {Choquette-Choo, Christopher A. and Tramer, Florian and Carlini, Nicholas and Papernot, Nicolas},
	booktitle = {Proceedings of the 38th International Conference on Machine Learning},
	editor = {Meila, Marina and Zhang, Tong},
	month = {18--24 Jul},
	pages = {1964--1974},
	pdf = {http://proceedings.mlr.press/v139/choquette-choo21a/choquette-choo21a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Label-Only Membership Inference Attacks},
	url = {https://proceedings.mlr.press/v139/choquette-choo21a.html},
	volume = {139},
	year = {2021},
	bdsk-url-1 = {https://proceedings.mlr.press/v139/choquette-choo21a.html}}

@inproceedings{10.1007/978-3-030-81242-3_2,
	abstract = {Attacks that aim to identify the training data of neural networks represent a severe threat to the privacy of individuals in the training dataset. A possible protection is offered by anonymization of the training data or training function with differential privacy. However, data scientists can choose between local and central differential privacy, and need to select meaningful privacy parameters . A comparison of local and central differential privacy based on the privacy parameters furthermore potentially leads data scientists to incorrect conclusions, since the privacy parameters are reflecting different types of mechanisms.Instead, we empirically compare the relative privacy-accuracy trade-off of one central and two local differential privacy mechanisms under a white-box membership inference attack. While membership inference only reflects a lower bound on inference risk and differential privacy formulates an upper bound, our experiments with several datasets show that the privacy-accuracy trade-off is similar for both types of mechanisms despite the large difference in their upper bound. This suggests that the upper bound is far from the practical susceptibility to membership inference. Thus, small &nbsp;in central differential privacy and large &nbsp;in local differential privacy result in similar membership inference risks, and local differential privacy can be a meaningful alternative to central differential privacy for differentially private deep learning besides the comparatively higher privacy parameters.},
	address = {Berlin, Heidelberg},
	author = {Bernau, Daniel and Robl, Jonas and Grassal, Philip W. and Schneider, Steffen and Kerschbaum, Florian},
	booktitle = {Data and Applications Security and Privacy XXXV: 35th Annual IFIP WG 11.3 Conference, DBSec 2021, Calgary, Canada, July 19--20, 2021, Proceedings},
	doi = {10.1007/978-3-030-81242-3_2},
	isbn = {978-3-030-81241-6},
	keywords = {Anonymization, Membership inference, Neural networks},
	location = {Calgary, AB, Canada},
	numpages = {21},
	pages = {22--42},
	publisher = {Springer-Verlag},
	title = {Comparing Local and Central Differential Privacy Using Membership Inference Attacks},
	url = {https://doi.org/10.1007/978-3-030-81242-3_2},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-81242-3_2}}

@inproceedings{8962136,
	author = {Irolla, Paul and Ch{\^a}tel, Gr{\'e}gory},
	booktitle = {2019 12th CMI Conference on Cybersecurity and Privacy (CMI)},
	doi = {10.1109/CMI48017.2019.8962136},
	pages = {1-7},
	title = {Demystifying the Membership Inference Attack},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/CMI48017.2019.8962136}}

@inproceedings{8049725,
	author = {Mironov, Ilya},
	booktitle = {2017 IEEE 30th Computer Security Foundations Symposium (CSF)},
	doi = {10.1109/CSF.2017.11},
	pages = {263-275},
	title = {R{\'e}nyi Differential Privacy},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/CSF.2017.11}}

@inproceedings{10.5555/3294996.3295074,
	abstract = {Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.},
	address = {Red Hook, NY, USA},
	author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
	booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
	isbn = {9781510860964},
	location = {Long Beach, California, USA},
	numpages = {9},
	pages = {3149--3157},
	publisher = {Curran Associates Inc.},
	series = {NIPS'17},
	title = {LightGBM: A Highly Efficient Gradient Boosting Decision Tree},
	year = {2017}}

@inproceedings{9577318,
	author = {Rezaei, Shahbaz and Liu, Xin},
	booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	doi = {10.1109/CVPR46437.2021.00780},
	pages = {7888-7896},
	title = {On the Difficulty of Membership Inference Attacks},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/CVPR46437.2021.00780}}

@inproceedings{watson2022on,
	author = {Lauren Watson and Chuan Guo and Graham Cormode and Alexandre Sablayrolles},
	booktitle = {International Conference on Learning Representations},
	title = {On the Importance of Difficulty Calibration in Membership Inference Attacks},
	url = {https://openreview.net/forum?id=3eIrli0TwQ},
	year = {2022},
	bdsk-url-1 = {https://openreview.net/forum?id=3eIrli0TwQ}}

@misc{mia,
	author = {Bogdan Kulynych and Mohammad Yaghini},
	doi = {10.5281/zenodo.1433744},
	month = sep,
	title = {{mia: A library for running membership inference attacks against ML models-}},
	url = {https://doi.org/10.5281/zenodo.1433744},
	year = 2018,
	bdsk-url-1 = {https://doi.org/10.5281/zenodo.1433744}}

@article{Ulrich1984ComputerGO,
	author = {Gary Ulrich},
	journal = {Journal of The Royal Statistical Society Series C-applied Statistics},
	pages = {158-163},
	title = {Computer Generation of Distributions on the MSphere},
	volume = {33},
	year = {1984}}

@article{Wood1994SimulationOT,
	author = {Andrew T. A. Wood},
	journal = {Communications in Statistics - Simulation and Computation},
	pages = {157-164},
	title = {Simulation of the von mises fisher distribution},
	volume = {23},
	year = {1994}}

@article{hu-etal:2022,
	abstract = {Machine learning (ML) models have been widely applied to various applications, including image classification, text generation, audio recognition, and graph data analysis. However, recent studies have shown that ML models are vulnerable to membership inference attacks (MIAs), which aim to infer whether a data record was used to train a target model or not. MIAs on ML models can directly lead to a privacy breach. For example, via identifying the fact that a clinical record that has been used to train a model associated with a certain disease, an attacker can infer that the owner of the clinical record has the disease with a high chance. In recent years, MIAs have been shown to be effective on various ML models, e.g., classification models and generative models. Meanwhile, many defense methods have been proposed to mitigate MIAs. Although MIAs on ML models form a newly emerging and rapidly growing research area, there has been no systematic survey on this topic yet. In this article, we conduct the first comprehensive survey on membership inference attacks and defenses. We provide the taxonomies for both attacks and defenses, based on their characterizations, and discuss their pros and cons. Based on the limitations and gaps identified in this survey, we point out several promising future research directions to inspire the researchers who wish to follow this area. This survey not only serves as a reference for the research community but also provides a clear description for researchers outside this research domain. To further help the researchers, we have created an online resource repository, which we will keep updated with future relevant work. Interested readers can find the repository at https://github.com/HongshengHu/membership-inference-machine-learning-literature.},
	address = {New York, NY, USA},
	articleno = {235},
	author = {Hu, Hongsheng and Salcic, Zoran and Sun, Lichao and Dobbie, Gillian and Yu, Philip S. and Zhang, Xuyun},
	doi = {10.1145/3523273},
	issn = {0360-0300},
	issue_date = {January 2022},
	journal = {ACM Comput. Surv.},
	keywords = {Membership inference attacks, deep leaning, privacy risk, differential privacy},
	month = {sep},
	number = {11s},
	numpages = {37},
	publisher = {Association for Computing Machinery},
	title = {Membership Inference Attacks on Machine Learning: A Survey},
	url = {https://doi.org/10.1145/3523273},
	volume = {54},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3523273}}

@misc{dwork-rothblum:2016,
	archiveprefix = {arXiv},
	author = {Cynthia Dwork and Guy N. Rothblum},
	eprint = {1603.01887},
	primaryclass = {cs.DS},
	title = {{Concentrated Differential Privacy}},
	year = {2016}}

@inproceedings{bun-steinke:2016,
	abstract = {``Concentrated differential privacy'' was recently introduced by Dwork and Rothblum as a relaxation of differential privacy, which permits sharper analyses of many privacy-preserving computations. We present an alternative formulation of the concept of concentrated differential privacy in terms of the R{\'e}nyi divergence between the distributions obtained by running an algorithm on neighboring inputs. With this reformulation in hand, we prove sharper quantitative results, establish lower bounds, and raise a few new questions. We also unify this approach with approximate differential privacy by giving an appropriate definition of ``approximate concentrated differential privacy''.},
	address = {Berlin, Heidelberg},
	author = {Bun, Mark and Steinke, Thomas},
	booktitle = {Theory of Cryptography},
	editor = {Hirt, Martin and Smith, Adam},
	isbn = {978-3-662-53641-4},
	pages = {635--658},
	publisher = {Springer Berlin Heidelberg},
	title = {Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds},
	year = {2016}}

@inproceedings{ye-etal:2022:CCS,
	abstract = {How much does a machine learning algorithm leak about its training data, and why? Membership inference attacks are used as an auditing tool to quantify this leakage. In this paper, we present a comprehensivehypothesis testing framework that enables us not only to formally express the prior work in a consistent way, but also to design new membership inference attacks that use reference models to achieve a significantly higher power (true positive rate) for any (false positive rate) error. More importantly, we explainwhy different attacks perform differently. We present a template for indistinguishability games, and provide an interpretation of attack success rate across different instances of the game. We discuss various uncertainties of attackers that arise from the formulation of the problem, and show how our approach tries to minimize the attack uncertainty to the one bit secret about the presence or absence of a data point in the training set. We perform adifferential analysis between all types of attacks, explain the gap between them, and show what causes data points to be vulnerable to an attack (as the reasons vary due to different granularities of memorization, from overfitting to conditional memorization). Our auditing framework is openly accessible as part of thePrivacy Meter software tool.},
	address = {New York, NY, USA},
	author = {Ye, Jiayuan and Maddi, Aadyaa and Murakonda, Sasi Kumar and Bindschaedler, Vincent and Shokri, Reza},
	booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
	doi = {10.1145/3548606.3560675},
	isbn = {9781450394505},
	keywords = {privacy auditing, membership inference, indistinguishability game},
	location = {Los Angeles, CA, USA},
	numpages = {14},
	pages = {3093--3106},
	publisher = {Association for Computing Machinery},
	series = {CCS '22},
	title = {Enhanced Membership Inference Attacks against Machine Learning Models},
	url = {https://doi.org/10.1145/3548606.3560675},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3548606.3560675}}

@inproceedings{nasr-etal:2018:SP,
	author = {Nasr, Milad and Shokri, Reza and Houmansadr, Amir},
	booktitle = {Proceedings of the 2019 IEEE Symposium on Security and Privacy (SP)},
	pages = {1--15},
	title = {Comprehensive privacy analysis of deep learning},
	year = {2018}}

@inproceedings{kumar-shokri:2020,
	author = {Sasi Kumar and Shokri, Reza},
	booktitle = {Workshop on Hot Topics in Privacy Enhancing Technologies (HotPETs)},
	maintitle = {The 20th Privacy Enhancing Technologies Symposium},
	title = {ML Privacy Meter: Aiding regulatory compliance by quantifying the privacy risks of machine learning},
	year = {2020}}

@inproceedings{zhang-etal:2018:CVPR,
	author = {Richard Zhang and Phillip Isola and Alexei A. Efros and Eli Shechtman and Oliver Wang},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/cvpr/ZhangIESW18.bib},
	booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
	doi = {10.1109/CVPR.2018.00068},
	pages = {586--595},
	publisher = {Computer Vision Foundation / {IEEE} Computer Society},
	timestamp = {Fri, 24 Mar 2023 00:02:56 +0100},
	title = {The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
	url = {http://openaccess.thecvf.com/content\_cvpr\_2018/html/Zhang\_The\_Unreasonable\_Effectiveness\_CVPR\_2018\_paper.html},
	year = {2018},
	bdsk-url-1 = {http://openaccess.thecvf.com/content%5C_cvpr%5C_2018/html/Zhang%5C_The%5C_Unreasonable%5C_Effectiveness%5C_CVPR%5C_2018%5C_paper.html},
	bdsk-url-2 = {https://doi.org/10.1109/CVPR.2018.00068}}

@article{ghildyal-liu:2023:TMLR,
	author = {Abhijay Ghildyal and Feng Liu},
	issn = {2835-8856},
	journal = {Transactions on Machine Learning Research},
	note = {Featured Certification},
	title = {Attacking Perceptual Similarity Metrics},
	url = {https://openreview.net/forum?id=r9vGSpbbRO},
	year = {2023},
	bdsk-url-1 = {https://openreview.net/forum?id=r9vGSpbbRO}}

@inproceedings{gu-etal:2020:ECCV,
	author = {Jinjin Gu and Haoming Cai and Haoyu Chen and Xiaoxing Ye and Jimmy S. Ren and Chao Dong},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/eccv/GuCCYRD20.bib},
	booktitle = {Computer Vision - {ECCV} 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part {XI}},
	doi = {10.1007/978-3-030-58621-8\_37},
	editor = {Andrea Vedaldi and Horst Bischof and Thomas Brox and Jan{-}Michael Frahm},
	pages = {633--651},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	timestamp = {Wed, 07 Dec 2022 23:10:23 +0100},
	title = {{PIPAL:} {A} Large-Scale Image Quality Assessment Dataset for Perceptual Image Restoration},
	url = {https://doi.org/10.1007/978-3-030-58621-8\_37},
	volume = {12356},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-58621-8%5C_37}}

@article{murakonda-shokri:2020,
	author = {Sasi Kumar Murakonda and Reza Shokri},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2007-09339.bib},
	eprint = {2007.09339},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Tue, 28 Jul 2020 14:46:12 +0200},
	title = {{ML} Privacy Meter: Aiding Regulatory Compliance by Quantifying the Privacy Risks of Machine Learning},
	url = {https://arxiv.org/abs/2007.09339},
	volume = {abs/2007.09339},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2007.09339}}

@inproceedings{10.1145/3548606.3560675,
	abstract = {How much does a machine learning algorithm leak about its training data, and why? Membership inference attacks are used as an auditing tool to quantify this leakage. In this paper, we present a comprehensivehypothesis testing framework that enables us not only to formally express the prior work in a consistent way, but also to design new membership inference attacks that use reference models to achieve a significantly higher power (true positive rate) for any (false positive rate) error. More importantly, we explainwhy different attacks perform differently. We present a template for indistinguishability games, and provide an interpretation of attack success rate across different instances of the game. We discuss various uncertainties of attackers that arise from the formulation of the problem, and show how our approach tries to minimize the attack uncertainty to the one bit secret about the presence or absence of a data point in the training set. We perform adifferential analysis between all types of attacks, explain the gap between them, and show what causes data points to be vulnerable to an attack (as the reasons vary due to different granularities of memorization, from overfitting to conditional memorization). Our auditing framework is openly accessible as part of thePrivacy Meter software tool.},
	address = {New York, NY, USA},
	author = {Ye, Jiayuan and Maddi, Aadyaa and Murakonda, Sasi Kumar and Bindschaedler, Vincent and Shokri, Reza},
	booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
	doi = {10.1145/3548606.3560675},
	isbn = {9781450394505},
	keywords = {indistinguishability game, membership inference, privacy auditing},
	location = {Los Angeles, CA, USA},
	numpages = {14},
	pages = {3093--3106},
	publisher = {Association for Computing Machinery},
	series = {CCS '22},
	title = {Enhanced Membership Inference Attacks against Machine Learning Models},
	url = {https://doi.org/10.1145/3548606.3560675},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3548606.3560675}}

@inproceedings{fredrikson-etal:2014:Usenix,
	abstract = {We initiate the study of privacy in pharmacogenetics, wherein machine learning models are used to guide medical treatments based on a patient's genotype and background. Performing an in-depth case study on privacy in personalized warfarin dosing, we show that suggested models carry privacy risks, in particular because attackers can perform what we call model inversion: an attacker, given the model and some demographic information about a patient, can predict the patient's genetic markers.As differential privacy (DP) is an oft-proposed solution for medical settings such as this, we evaluate its effectiveness for building private versions of pharmacogenetic models. We show that DP mechanisms prevent our model inversion attacks when the privacy budget is carefully selected. We go on to analyze the impact on utility by performing simulated clinical trials with DP dosing models. We find that for privacy budgets effective at preventing attacks, patients would be exposed to increased risk of stroke, bleeding events, and mortality. We conclude that current DP mechanisms do not simultaneously improve genomic privacy while retaining desirable clinical efficacy, highlighting the need for new mechanisms that should be evaluated in situ using the general methodology introduced by our work.},
	address = {USA},
	author = {Fredrikson, Matthew and Lantz, Eric and Jha, Somesh and Lin, Simon and Page, David and Ristenpart, Thomas},
	booktitle = {Proceedings of the 23rd USENIX Conference on Security Symposium},
	isbn = {9781931971157},
	location = {San Diego, CA},
	numpages = {16},
	pages = {17--32},
	publisher = {USENIX Association},
	series = {SEC'14},
	title = {Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing},
	year = {2014}}

@inproceedings{yin-etal:2021:CVPR,
	author = {Hongxu Yin and Arun Mallya and Arash Vahdat and Jose M. Alvarez and Jan Kautz and Pavlo Molchanov},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/cvpr/YinMVAKM21.bib},
	booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2021, virtual, June 19-25, 2021},
	doi = {10.1109/CVPR46437.2021.01607},
	pages = {16337--16346},
	publisher = {Computer Vision Foundation / {IEEE}},
	timestamp = {Mon, 18 Jul 2022 16:47:41 +0200},
	title = {See Through Gradients: Image Batch Recovery via GradInversion},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Yin\_See\_Through\_Gradients\_Image\_Batch\_Recovery\_via\_GradInversion\_CVPR\_2021\_paper.html},
	year = {2021},
	bdsk-url-1 = {https://openaccess.thecvf.com/content/CVPR2021/html/Yin%5C_See%5C_Through%5C_Gradients%5C_Image%5C_Batch%5C_Recovery%5C_via%5C_GradInversion%5C_CVPR%5C_2021%5C_paper.html},
	bdsk-url-2 = {https://doi.org/10.1109/CVPR46437.2021.01607}}

@inproceedings{huang-etal:2021:NeurIPS,
	author = {Huang, Yangsibo and Gupta, Samyak and Song, Zhao and Li, Kai and Arora, Sanjeev},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {7232--7241},
	publisher = {Curran Associates, Inc.},
	title = {Evaluating Gradient Inversion Attacks and Defenses in Federated Learning},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/3b3fff6463464959dcd1b68d0320f781-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/3b3fff6463464959dcd1b68d0320f781-Paper.pdf}}

@inproceedings{Dwork-etal:2006:DP,
	abstract = {We continue a line of research initiated in [10,11]on privacy-preserving statistical databases. Consider a trusted server that holds a database of sensitive information. Given a query function f mapping databases to reals, the so-called true answer is the result of applying f to the database. To protect privacy, the true answer is perturbed by the addition of random noise generated according to a carefully chosen distribution, and this response, the true answer plus noise, is returned to the user.},
	address = {Berlin, Heidelberg},
	author = {Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
	booktitle = {Theory of Cryptography},
	editor = {Halevi, Shai and Rabin, Tal},
	isbn = {978-3-540-32732-5},
	pages = {265--284},
	publisher = {Springer Berlin Heidelberg},
	title = {Calibrating Noise to Sensitivity in Private Data Analysis},
	year = {2006}}

@article{mironov2019r,
	author = {Mironov, Ilya and Talwar, Kunal and Zhang, Li},
	journal = {arXiv preprint arXiv:1908.10530},
	title = {R$\backslash$'enyi differential privacy of the sampled gaussian mechanism},
	year = {2019}}

@inproceedings{Shahab_ISIT_2020:100rounds,
	author = {Asoodeh, Shahab and Liao, Jiachun and Calmon, Flavio P. and Kosut, Oliver and Sankar, Lalitha},
	booktitle = {2020 IEEE International Symposium on Information Theory (ISIT)},
	doi = {10.1109/ISIT44484.2020.9174015},
	pages = {920-925},
	title = {A Better Bound Gives a Hundred Rounds: Enhanced Privacy Guarantees via f-Divergences},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/ISIT44484.2020.9174015}}

@article{kitagawa2022mises,
	author = {Kitagawa, Toru and Rowley, Jeff},
	journal = {arXiv preprint arXiv:2202.05192},
	title = {von Mises-Fisher distributions and their statistical divergence},
	year = {2022}}

@article{Canonne_Kamath_Steinke_2022,
	abstractnote = {&amp;lt;p&amp;gt;A key tool for building differentially private systems is adding Gaussian noise to the output of a function evaluated on a sensitive dataset. Unfortunately, using a continuous distribution presents several practical challenges. First and foremost, finite computers cannot exactly represent samples from continuous distributions, and previous work has demonstrated that seemingly innocuous numerical errors can entirely destroy privacy. Moreover, when the underlying data is itself discrete (e.g., population counts), adding continuous noise makes the result less interpretable.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;With these shortcomings in mind, we introduce and analyze the discrete Gaussian in the context of differential privacy. Specifically, we theoretically and experimentally show that adding discrete Gaussian noise provides essentially the same privacy and accuracy guarantees as the addition of continuous Gaussian noise. We also present an simple and efficient algorithm for exact sampling from this distribution. This demonstrates its applicability for privately answering counting queries, or more generally, low-sensitivity integer-valued queries.&amp;lt;/p&amp;gt;},
	author = {Canonne, Clement and Kamath, Gautam and Steinke, Thomas},
	doi = {10.29012/jpc.784},
	journal = {Journal of Privacy and Confidentiality},
	month = {Jul.},
	number = {1},
	title = {The Discrete Gaussian for Differential Privacy},
	url = {https://journalprivacyconfidentiality.org/index.php/jpc/article/view/784},
	volume = {12},
	year = {2022},
	bdsk-url-1 = {https://journalprivacyconfidentiality.org/index.php/jpc/article/view/784},
	bdsk-url-2 = {https://doi.org/10.29012/jpc.784}}

@article{balle2018privacy,
	author = {Balle, Borja and Barthe, Gilles and Gaboardi, Marco},
	journal = {Advances in neural information processing systems},
	title = {Privacy amplification by subsampling: Tight analyses via couplings and divergences},
	volume = {31},
	year = {2018}}

@article{composition:theorems,
	author = {Kairouz, Peter and Oh, Sewoong and Viswanath, Pramod},
	doi = {10.1109/TIT.2017.2685505},
	journal = {IEEE Transactions on Information Theory},
	number = {6},
	pages = {4037-4049},
	title = {The Composition Theorem for Differential Privacy},
	volume = {63},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TIT.2017.2685505}}

@inproceedings{sun-etal:2023:NeurIPS,
	author = {Xiaoxiao Sun and Nidham Gazagnadou and Vivek Sharma and Lingjuan Lyu and Hongdong Li and Liang Zheng},
	booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
	title = {Privacy Assessment on Reconstructed Images: Are Existing Evaluation Metrics Faithful to Human Perception?},
	url = {https://openreview.net/forum?id=cx9a4Xvb3l},
	year = {2023},
	bdsk-url-1 = {https://openreview.net/forum?id=cx9a4Xvb3l}}

@inproceedings{zhu2019poission,
	author = {Zhu, Yuqing and Wang, Yu-Xiang},
	booktitle = {International Conference on Machine Learning},
	organization = {PMLR},
	pages = {7634--7642},
	title = {Poission subsampled r{\'e}nyi differential privacy},
	year = {2019}}

@inproceedings{wang2019subsampled,
	author = {Wang, Yu-Xiang and Balle, Borja and Kasiviswanathan, Shiva Prasad},
	booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics},
	organization = {PMLR},
	pages = {1226--1235},
	title = {Subsampled r{\'e}nyi differential privacy and analytical moments accountant},
	year = {2019}}

@misc{faustini2023directional,
	archiveprefix = {arXiv},
	author = {Pedro Faustini and Natasha Fernandes and Shakila Tonni and Annabelle McIver and Mark Dras},
	eprint = {2211.04686},
	note = {Accepted for presentation at the 5th AAAI Workshop on Privacy-Preserving Artificial Intelligence},
	primaryclass = {cs.LG},
	title = {Directional Privacy for Deep Learning},
	url = {https://ppai-workshop.github.io/},
	year = {2023},
	bdsk-url-1 = {https://ppai-workshop.github.io/}}

@inproceedings{melis-etal:2019:IEEE-SP,
	author = {Luca Melis and Congzheng Song and Emiliano De Cristofaro and Vitaly Shmatikov},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/sp/MelisSCS19.bib},
	booktitle = {2019 {IEEE} Symposium on Security and Privacy, {SP} 2019, San Francisco, CA, USA, May 19-23, 2019},
	doi = {10.1109/SP.2019.00029},
	pages = {691--706},
	publisher = {{IEEE}},
	timestamp = {Wed, 16 Oct 2019 14:14:51 +0200},
	title = {Exploiting Unintended Feature Leakage in Collaborative Learning},
	url = {https://doi.org/10.1109/SP.2019.00029},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/SP.2019.00029}}

@inproceedings{balle-etal:2022:IEEE-SP,
	author = {Borja Balle and Giovanni Cherubin and Jamie Hayes},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/sp/BalleCH22.bib},
	booktitle = {43rd {IEEE} Symposium on Security and Privacy, {SP} 2022, San Francisco, CA, USA, May 22-26, 2022},
	doi = {10.1109/SP46214.2022.9833677},
	pages = {1138--1156},
	publisher = {{IEEE}},
	timestamp = {Thu, 21 Sep 2023 15:57:28 +0200},
	title = {Reconstructing Training Data with Informed Adversaries},
	url = {https://doi.org/10.1109/SP46214.2022.9833677},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/SP46214.2022.9833677}}

@inproceedings{wu-etal:2023:UAI:learning-invert,
	author = {Ruihan Wu and Xiangyu Chen and Chuan Guo and Kilian Q. Weinberger},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/uai/WuCGW23.bib},
	booktitle = {Uncertainty in Artificial Intelligence, {UAI} 2023, July 31 - 4 August 2023, Pittsburgh, PA, {USA}},
	editor = {Robin J. Evans and Ilya Shpitser},
	pages = {2293--2303},
	publisher = {{PMLR}},
	series = {Proceedings of Machine Learning Research},
	timestamp = {Mon, 28 Aug 2023 17:23:08 +0200},
	title = {Learning To Invert: Simple Adaptive Attacks for Gradient Inversion in Federated Learning},
	url = {https://proceedings.mlr.press/v216/wu23a.html},
	volume = {216},
	year = {2023},
	bdsk-url-1 = {https://proceedings.mlr.press/v216/wu23a.html}}

@inproceedings{wang-etal:2023:AISTATS:reconstructing-provably,
	abstract = {Understanding when and how much a model gradient leaks information about the training sample is an important question in privacy. In this paper, we present a surprising result: Even without training or memorizing the data, we can fully reconstruct the training samples from a single gradient query at a randomly chosen parameter value. We prove the identifiability of the training data under mild assumptions: with shallow or deep neural networks and wide range of activation functions. We also present a statistically and computationally efficient algorithm based on tensor decomposition to reconstruct the training data. As a provable attack that reveals sensitive training data, our findings suggest potential  severe threats to privacy, especially in federated learning.},
	author = {Wang, Zihan and Lee, Jason and Lei, Qi},
	booktitle = {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
	editor = {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
	month = {25--27 Apr},
	pages = {6595--6612},
	pdf = {https://proceedings.mlr.press/v206/wang23g/wang23g.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Reconstructing Training Data from Model Gradient, Provably},
	url = {https://proceedings.mlr.press/v206/wang23g.html},
	volume = {206},
	year = {2023},
	bdsk-url-1 = {https://proceedings.mlr.press/v206/wang23g.html}}

@article{croft-etal:2022:FGCS,
	abstract = {From smartphones owned by the majority of teenage and adult populations to omnipresent closed-circuit television systems, the ubiquity of image-capturing devices in our everyday lives ensures that digital images of individuals are taken in the hundreds of millions on a daily basis. Many of these images capture individuals{\^a}{\texttrademark} faces which, through facial recognition techniques, identify the individuals and thus represent a major privacy concern. Many countries and companies require facial obfuscation to conform to privacy laws or policies. Since images should be useful and look realistic, a trade-off arises between privacy and utility. The task is therefore to find a method of obfuscation that offers a formal privacy guarantee while preserving visual quality and maintaining facial attributes deemed acceptable for release (e.g., the pose of the head, gender, etc.). We address this task by proposing facial identity obfuscation through the application of differential privacy to image encodings in a generative adversarial network. We provide details on the design of the model architecture and training process that allow for the generation of photo-realistic obfuscated images. Through the use of principal component analysis, we control the application of noise to the model encodings in order to achieve a favourable trade-off between privacy and utility. We demonstrate the effectiveness of our approach through an experimental comparison against other methods of obfuscation which also offer a formal guarantee of privacy.},
	author = {William L. Croft and J{\~A}rg-R{\~A}diger Sack and Wei Shi},
	doi = {https://doi.org/10.1016/j.future.2021.11.032},
	issn = {0167-739X},
	journal = {Future Generation Computer Systems},
	keywords = {Privacy protection, Facial obfuscation, Differential privacy, Generative adversarial networks},
	pages = {358-379},
	title = {Differentially private facial obfuscation via generative adversarial networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X21004763},
	volume = {129},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167739X21004763},
	bdsk-url-2 = {https://doi.org/10.1016/j.future.2021.11.032}}

@inproceedings{croft-etal:2019,
	abstract = {The pervasiveness of camera technology in every-day life begets a modern reality in which images of individuals are routinely captured on a daily basis. Although this has enabled many benefits, it also infringes on personal privacy. To mitigate the loss of privacy, researchers have investigated methods of facial obfuscation in images. A promising direction has been the work in the k-same family of methods which employ the concept of k-anonymity from database privacy. However, there are a number of deficiencies of k-anonymity which carry over to the k-same methods, detracting from their usefulness in practice. In this paper, we first outline several of these deficiencies and discuss their implications in the context of facial obfuscation. We then develop the first framework to apply the formal privacy guarantee of differential privacy to facial obfuscation in generative machine learning models for images. Next, we discuss the theoretical improvements in the privacy guarantee which make this approach more appropriate for practical usage. Our approach provides a provable privacy guarantee which is not susceptible to the outlined deficiencies of k-same obfuscation and produces photo-realistic obfuscated output. Finally, while our approach provides a stronger privacy guarantee, we demonstrate through experimental comparisons that it can achieve comparable utility to k-same approaches in the context of preservation of demographic information in the images. The preservation of such information is of particular importance for enabling effective data mining on the obfuscated images.},
	address = {Cham},
	author = {Croft, William L. and Sack, J{\"o}rg-R{\"u}diger and Shi, Wei},
	booktitle = {Machine Learning and Knowledge Extraction},
	editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
	isbn = {978-3-030-29726-8},
	pages = {229--249},
	publisher = {Springer International Publishing},
	title = {Differentially Private Obfuscation of Facial Images},
	year = {2019}}

@article{pan-etal:2024:Neurocomputing,
	abstract = {The widespread adoption of deep learning is facilitated in part by the availability of large-scale data for training desirable models. However, these data may involve sensitive personal information, which raises privacy concerns for data providers. Differential privacy has been thought of as a key technique in the privacy preservation field, which has drawn much attention owing to its capability of providing rigorous and provable privacy guarantees for training data. Training deep learning models in a differentially private manner is a topic that is gaining traction as this alleviates the reconstruction and inference of sensitive information effectively. Taking this cue, in this paper, we present here a comprehensive and systematic study on differentially private deep learning from the facets of privacy attack and privacy preservation. We explore a new taxonomy to analyze the privacy attacks faced in deep learning and then survey the type of privacy preservation based on differential privacy to tackle such privacy attacks in deep learning. Finally, we propose the first probe into the real-world application of differentially private deep learning, and then conclude with several potential future research avenues. This survey provides promising directions for protecting sensitive information in training data via differential privacy during deep learning model training.},
	author = {Ke Pan and Yew-Soon Ong and Maoguo Gong and Hui Li and A.K. Qin and Yuan Gao},
	doi = {https://doi.org/10.1016/j.neucom.2024.127663},
	issn = {0925-2312},
	journal = {Neurocomputing},
	keywords = {Deep learning, Differential privacy, Privacy attack, Privacy preservation},
	pages = {127663},
	title = {Differential privacy in deep learning: A literature survey},
	url = {https://www.sciencedirect.com/science/article/pii/S092523122400434X},
	volume = {589},
	year = {2024},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S092523122400434X},
	bdsk-url-2 = {https://doi.org/10.1016/j.neucom.2024.127663}}

@inproceedings{klymenko-etal-2022-differential,
	abstract = {As the tide of Big Data continues to influence the landscape of Natural Language Processing (NLP), the utilization of modern NLP methods has grounded itself in this data, in order to tackle a variety of text-based tasks. These methods without a doubt can include private or otherwise personally identifiable information. As such, the question of privacy in NLP has gained fervor in recent years, coinciding with the development of new Privacy- Enhancing Technologies (PETs). Among these PETs, Differential Privacy boasts several desirable qualities in the conversation surrounding data privacy. Naturally, the question becomes whether Differential Privacy is applicable in the largely unstructured realm of NLP. This topic has sparked novel research, which is unified in one basic goal how can one adapt Differential Privacy to NLP methods? This paper aims to summarize the vulnerabilities addressed by Differential Privacy, the current thinking, and above all, the crucial next steps that must be considered.},
	address = {Seattle, United States},
	author = {Klymenko, Oleksandra and Meisenbacher, Stephen and Matthes, Florian},
	booktitle = {Proceedings of the Fourth Workshop on Privacy in Natural Language Processing},
	doi = {10.18653/v1/2022.privatenlp-1.1},
	editor = {Feyisetan, Oluwaseyi and Ghanavati, Sepideh and Thaine, Patricia and Habernal, Ivan and Mireshghallah, Fatemehsadat},
	month = jul,
	pages = {1--11},
	publisher = {Association for Computational Linguistics},
	title = {Differential Privacy in Natural Language Processing The Story So Far},
	url = {https://aclanthology.org/2022.privatenlp-1.1},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.privatenlp-1.1},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.privatenlp-1.1}}

@misc{du2024sok,
	archiveprefix = {arXiv},
	author = {Jiacheng Du and Jiahui Hu and Zhibo Wang and Peng Sun and Neil Zhenqiang Gong and Kui Ren},
	eprint = {2404.05403},
	primaryclass = {cs.CR},
	title = {SoK: Gradient Leakage in Federated Learning},
	year = {2024}}

@article{biswasprivic2023,
	author = {Biswas, Sayan and Palamidessi, Catuscia},
	journal = {Proceedings on Privacy Enhancing Technologies},
	number = {1},
	pages = {582--596},
	title = {PRIVIC: A privacy-preserving method for incremental collection of location data},
	volume = {2024},
	year = {2023}}

@inproceedings{OyaMILocationPrivacy2017,
	acmid = {3134004},
	author = {Oya, Simon and Troncoso, Carmela and P{\'e}rez-Gonz\'{a}lez, Fernando},
	booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
	date-modified = {2019-06-16 20:39:03 +0200},
	doi = {10.1145/3133956.3134004},
	isbn = {978-1-4503-4946-8},
	keywords = {location privacy, mechanism design, mechanism evaluation, quantifying privacy},
	location = {Dallas, Texas, USA},
	numpages = {14},
	pages = {1959--1972},
	publisher = {ACM},
	title = {Back to the Drawing Board: Revisiting the Design of Optimal Location Privacy-preserving Mechanisms},
	url = {http://doi.acm.org/10.1145/3133956.3134004},
	year = {2017},
	bdsk-url-1 = {http://doi.acm.org/10.1145/3133956.3134004},
	bdsk-url-2 = {https://dx.doi.org/10.1145/3133956.3134004}}

@inproceedings{galli2022group,
	author = {Filippo Galli and Sayan Biswas and Kangsoo Jung and Tommaso Cucinotta and Catuscia Palamidessi},
	booktitle = {Workshop on Federated Learning: Recent Advances and New Challenges (in Conjunction with NeurIPS 2022)},
	title = {Group privacy for personalized federated learning},
	url = {https://openreview.net/forum?id=R45g30SnwsR},
	year = {2022},
	bdsk-url-1 = {https://openreview.net/forum?id=R45g30SnwsR}}

@article{galli2023advancing,
	author = {Galli, Filippo and Jung, Kangsoo and Biswas, Sayan and Palamidessi, Catuscia and Cucinotta, Tommaso},
	journal = {SN Computer Science},
	number = {6},
	pages = {831},
	publisher = {Springer},
	title = {Advancing Personalized Federated Learning: Group Privacy, Fairness, and Beyond},
	volume = {4},
	year = {2023}}

@article{UgurPrivacyPreservingEVs2024,
	author = {Atmaca, Ugur Ilker and Biswas, Sayan and Maple, Carsten and Palamidessi, Catuscia},
	doi = {10.1109/OJVT.2024.3360302},
	journal = {IEEE Open Journal of Vehicular Technology},
	keywords = {Privacy;Data privacy;Quality of service;Standards;Data models;Charging stations;Trajectory;Charging station;electric vehicle;geo-indistinguishability;location privacy;privacy-utility trade-off},
	pages = {262-277},
	title = {A Privacy-Preserving Querying Mechanism with High Utility for Electric Vehicles},
	volume = {5},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1109/OJVT.2024.3360302}}


@inproceedings{m2012measuring,
  title={Measuring information leakage using generalized gain functions},
  author={M'rio, S Alvim and Chatzikokolakis, Kostas and Palamidessi, Catuscia and Smith, Geoffrey},
  booktitle={2012 IEEE 25th Computer Security Foundations Symposium},
  pages={265--279},
  year={2012},
  organization={IEEE}
}

@article{biswas2024bayes,
  title={Bayes' capacity as a measure for reconstruction attacks in federated learning},
  author={Biswas, Sayan and Dras, Mark and Faustini, Pedro and Fernandes, Natasha and McIver, Annabelle and Palamidessi, Catuscia and Sadeghi, Parastoo},
  journal={arXiv preprint arXiv:2406.13569},
  year={2024}
}