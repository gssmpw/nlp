\section{Related Work}
\label{sec:lit-rev}


    % \MDnote{240521}{Maybe frame this first chunk of material with paragraph heading ``DP Conversion Frameworks'', and move ``The main work that ours follows \ldots'' to the end of it.  Again, per reviewers' comments, we want to emphasise that the idea of converting between std and metric DP is broadly applicable and not yet done, and that our specific Gauss vs VMF in SGD is just the specific instance we're looking at in this work.}
    

    
    %   \PSnote{240128}{Here is some stuff about works enabling us to carry out the theoretical comparisons.}

\emph{Converting between frameworks via R\'enyi differential privacy}
For a theoretical comparison of the VMF and Gaussian mechanisms in terms of privacy versus utility performance, we need a fair method for the conversion of their mechanism parameters into common privacy or utility metrics. 
A promising and versatile framework for such conversion is  R\'enyi differential privacy**Mironov, "Renyi Differential Privacy"**, It is capable of handling important implementation aspects such as privacy amplification through sub-sampling**Bun et al., "Essentially Optimal Composable On-Differential Privacy"** and composition through multiple epochs **Nissim et al., "Differential Privacy via Polynomial Regression"**  of the SGD algorithm. Ultimately, the R\'enyi differential privacy computations will need to be translated back to the traditional $(\epsilon,\delta)$ differential privacy domain, where the first approach was provided in**Cheu et al., "Privacy for the 21st Century: Saving Data Privacy from the Shadow of Legacy"**. Later works such as **Dwork et al., "Our Data, Ourselves: Privacy Via Decentralized Differential Privacy"** have improved the conversion in the sense that tighter achievable $(\epsilon,\delta)$ tuples can potentially be obtained from the same R\'enyi differential privacy metric. For the Gaussian mechanism, these methods have been heavily studied**Dwork et al., "Privacy in the Age of Machines (Dagstuhl Seminar 17281)"**. In this work, we use similar techniques to present how the R\'enyi differential privacy for the VMF mechanism is useful in characterising its privacy performance and how it enables a fair comparison of the Gaussian and VMF mechanisms. 

Another work that compares Gaussian and VMF mechanisms is **Bun et al., "Differentially Private Empirical Risk Minimization"** in which the authors investigate this, like us, in the context of DP-SGD. Our work differs from theirs in that we focus on proposing a foundational framework to compare the different notions of formal privacy guarantees that these mechanisms provide rather than simply comparing them empirically via experiments. 
As far as we know, this is the only work that has performed this comparison; in general, there is a lack of studies comparing how different noise distributions affect the privacy/utility trade-off and how noise distributions other than isotropic ones can be used for training neural networks. 

\emph{Gradient-based reconstruction attacks.} For our experimental work, where we evaluate the effect on privacy, like**Liu et al., "Stealing Hyperparameters with Models"**, we use gradient-based reconstruction attacks to calibrate how well each of our two mechanisms can provide protection against leaking of the original data, rather than Membership Inference Attacks (MIAs). This is because $a)$ reconstruction attacks are generally agreed in the literature to be more consequential breaches of privacy**Nasr et al., "Comprehensive Privacy Analysis of Deep Learning: Passive and Active Attacker Models"** and $b)$ **Hitaj et al., "Deep Leakage from Gradients"** showed that, although MIAs have been used for calibrating among standard DP variants ____, they are not for calibrating between Gaussian and VMF mechanisms, while reconstruction attacks are.

The reconstruction attack we work with is a gradient-based one, where the attacker has access to these gradients.  
    Reference**Hitaj et al., "Deep Leakage from Gradients"** first developed the Deep Leakage from Gradients (DLG) attack, which can recover private data from neural network architectures which are twice differentiable. The attacker creates fake inputs, and by minimising the Euclidean distance between their gradients and the gradients received from another client, the fake input moves towards the real one.  An improved version was presented by **Fredrikson et al., "Join the Club: A System for Federated Learning"**.
    
    A subsequent method that improved over these was the Inverting Gradients method (\IGA) of **Bonabi and Fredrikson, "Gradient Leaks vs. Membership Inference Attacks in Machine Learning as a Service (MLaaS)"**, which maximises the cosine similarity between gradients. It was proved in**Fredrikson et al., "Join the Club: A System for Federated Learning"  that this reconstruction attack is guaranteed to be effective against Multi-Layer Perceptrons (MLPs). 
    
There has been much work since then on developing new gradient-based attacks and on understanding the conditions under which they apply or can be mitigated, e.g., see **Fredrikson et al., "Join the Club: A System for Federated Learning"**, with a useful survey provided by **Borgel and Fredrikson, "Gradient Leaks vs. Membership Inference Attacks in Machine Learning as a Service (MLaaS)"**.
For this paper, we use \IGA of **Bonabi and Fredrikson, "Gradient Leaks vs. Membership Inference Attacks in Machine Learning as a Service (MLaaS)"** as a well-established method that is known to apply to the neural network architecture we use for our experiments.

\emph{Alternative measures for privacy.} The information theory and security communities have long used leakage measures to understand the security properties of systems modelled as information-theoretic channels. Recent work in these communities has converged on common measures based on the $g$-leakage framework of Smith**Smith, "Information-Theoretic Upper Bounds on Differential Privacy"**. The Bayes' capacity measure emerges in both communities as a robust leakage measure for both security and privacy**Bun et al., "Differentially Private Empirical Risk Minimization"**.~\footnote{The logarithm of the Bayes' capacity is also known as the Sibson mutual information of order $\alpha = \infty$.} 
 
 Reference**Smith, "Information-Theoretic Upper Bounds on Differential Privacy"** compared the Bayes' capacity measure to the $\epsilon$ of local differential privacy, showing that both provide robust upper bounds on leakage but describe different adversarial scenarios. Our work validates these differences experimentally.

Reference**Cheng et al., "Bayesian Inference for Differential Privacy"** used a Bayes' security measure --based on the existing measure of \emph{advantage}-- to measure the leakage of DP-SGD. They show that the Bayes' security is bounded by the total variation distance between the distributions of the gradients. They use this measure to evaluate the Gaussian mechanism's efficacy at protecting against membership inference attacks and attribute inference attacks. Their work differs from ours in that our measures are different (we use the Bayes' capacity measure), and our applications are different (we use our measure to compare different mechanisms against the same reconstruction attack).

Reference**Cheng et al., "Fisher Information Leakage"** proposed Fisher Information Leakage as a better measure for privacy leakage against reconstruction attacks than $\epsilon$-DP, which they note has been identified as relevant to membership inference attacks. Similar to our work they focus on semantic guarantees relevant to reconstruction. However, our work differs significantly from theirs in that our focus is on the comparison of disparate mechanisms. 


The closest work to ours is that of**Cheng et al., "Fisher Information Leakage"**, who also propose an alternative measure for protection against reconstruction threats using a probability measure that measures the success of a reconstruction adversary. Their conclusion aligns with ours -- that $(\epsilon, \delta)$ is not a strong predictor of reconstruction accuracy. Our work differs from theirs in that our focus is on comparing different mechanisms, whereas they compare the same mechanism across different algorithms.