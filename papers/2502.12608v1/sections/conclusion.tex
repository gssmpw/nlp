\section{Related Work}
\label{sec: rw}

    % Graph Neural Networks (GNNs) have been extensively studied from various theoretical perspectives, including expressive power (Xu et al., 2019; Morris et al., 2019), generalization properties (Keriven & Peyré, 2019; Verma & Zhang, 2019), and training stability (Oono & Suzuki, 2020). While these works provide valuable insights, they primarily focus on spectral properties and convergence behavior, with less emphasis on the broader optimization landscape of GNNs.
    
    % Another key direction in understanding GNNs involves analyzing their feature propagation and aggregation mechanisms (Wu et al., 2020; Loukas, 2020). These studies examine how information is diffused across graph structures and how different aggregation functions affect learning. However, they do not explore the connectivity between different trained GNN models in the parameter space, which is essential for understanding optimization dynamics and generalization.
    
    % Despite significant progress in analyzing GNNs from spectral and convergence perspectives, the geometric structure of their optimization landscape remains largely unexplored. Unlike CNNs, where feature interactions are localized, GNNs rely on message passing, which introduces complex dependencies between parameters and may result in a loss landscape with distinct connectivity properties. Understanding whether and how mode connectivity emerges in GNNs remains an open question, motivating further investigation into their optimization dynamics.
    
    % \czk{Papers studying the training dynamics of GNN in the over-parameterized regime~\cite{yanggraph} and mean-field regime~\cite{aminian2024generalization}}
    Graph Neural Networks (GNNs) have been extensively studied from various theoretical perspectives, including expressive power~\citep{maron, xupowerful}, generalization properties~\citep{verma2019stability, aminian2024generalization}, and training stability~\citep{oono2019graph}. While these works provide valuable insights, they mainly focus on spectral properties and convergence behavior, leaving open questions about the structure of solutions found by different training runs. A thorough survey for relevant literature is in \citet{jegelka2022theory, huang2024foundations}. Another key research direction involves analyzing how GNNs propagate and aggregate information across graph structures~\citep{loukas2019graph, wu2019simplifying}. %These studies examine the impact of different aggregation functions and diffusion mechanisms on learning dynamics.
    However, a fundamental question remains unexplored: how do independently trained GNNs relate to each other in the parameter space? This study aims to bridge this gap.
    
  %  In other architectures, such as CNNs and transformers, trained models are often connected by low-loss paths, a property known as mode connectivity. Whether similar connectivity emerges in GNNs, given their unique reliance on message passing and graph-dependent computations, remains an open problem.

    % The study of neural network optimization landscapes has provided deep insights into training dynamics, generalization, and robustness. Early works (Freeman & Bruna, 2016; Draxler et al., 2018; Garipov et al., 2018) demonstrated that local optima in deep networks are often connected by low-loss paths, a phenomenon known as mode connectivity. This insight has led to practical applications in model ensembling, adversarial robustness, and fine-tuning.
    
    % A particularly important variant, linear mode connectivity (LMC), occurs when such connections are nearly linear (Frankle et al., 2020; Entezari et al., 2021). LMC has been extensively studied in convolutional networks and transformers, with extensions to text classification (Juneja et al., 2022), pre-trained language models (Qin et al., 2022), and vision-language models (Abdollahpour Rostam et al., 2024). Additionally, researchers have explored mode connectivity in input space representations (Vrabel et al., 2024) and extended connectivity analysis from curves to higher-dimensional surfaces (Anonymous, 2024).
    
    % Despite these advances, most prior studies have focused on image and text domains, leaving the connectivity of GNN loss landscapes largely unexplored. Given the fundamental differences between GNNs and traditional architectures—especially the graph-dependent parameter sharing and message-passing mechanisms—it remains unclear whether trained GNN models exhibit similar connectivity properties. This gap in understanding motivates our systematic study of mode connectivity in GNNs, where we investigate its existence, its dependence on graph structure, and its implications for generalization and training dynamics.
    Mode connectivity refers to the existence of low-loss paths between different trained neural networks, demonstrating that solutions found by independent runs of training are not necessarily isolated~\citep{freeman2016topology, garipov2018loss, draxler2018essentially}. %This phenomenon has significant implications for generalization, robustness, and optimization, as it suggests that different trained models can be smoothly interpolated without performance degradation.
    A particularly important variant, linear mode connectivity (LMC), occurs when such connections are nearly linear~\citep{frankle2020linear, entezari2021role}. LMC has been widely observed in CNNs and transformers, with applications in model ensembling, adversarial robustness, and transfer learning~\citep{juneja2022linear, qin2022exploring, abdollahpourrostam2024unveiling}. Recent studies have also extended connectivity analysis to input space representations~\citep{vrabel2024input} and explored higher-dimensional connectivity structures~\citep{anonymous2024revisiting}. However, mode connectivity in GNNs remains largely unexplored. % In this work, we extend mode connectivity to GNNs, systematically analyzing its existence, its dependence on graph structure, and its implications for deep graph learning.

\section{Conclusion}
\label{sec: conclusion}
In this paper, we systematically study the mode connectivity behavior of GNNs and reveal that they exhibit distinct non-linear mode connectivity, primarily driven by graph structure rather than model architecture. This discovery deepens our understanding of GNN training dynamics and its link to improved generalization, paving the way for refined training methodologies and cross-domain adaptation techniques. Meanwhile, our research on mode connectivity has primarily focused on the node classification task. A promising future direction is to investigate the mode connectivity exhibited by GNNs on link prediction and graph-level tasks.



