\section{Introduction}


% GNNs have demonstrated outstanding performance in learning node representations~\citep{kipf2016semi, velickovic2017graph}, with abundant applications in social networks, recommendation systems, and traffic networks~\citep{ma2021deep}. Despite the empirical success, the understanding of GNN's training dynamics and loss landscapes remains limited, with only preliminary results under NTK~\citep{yanggraph} or mean-field regimes~\citep{aminian2024generalization}, still presents a gap to real-world cases. Such limitation in understanding prevents the further development of model robustness and explainability. 
% \czk{same paragraph or two?}

% Meanwhile, mode connectivity~\citep{garipov2018loss, draxler2018essentially}, which studies the relationship between different local minima of the same model obtained under different training trajectories, acts as an important lens to understand the geometric properties of model's loss landscape. 






Graph Neural Networks (GNNs) have emerged as a dominant paradigm for processing graph-structured data, achieving state-of-the-art results across diverse applications, from social network analysis to bioinformatics and traffic prediction~\citep{ma2021deep,wang2020traffic}. Despite their empirical success, the fundamental understanding of GNN optimization dynamics and the intricate geometry of their loss landscapes remains limited, hindering progress in model interpretability and robustness~\cite{jin2020graph}. Meanwhile, the concept of mode connectivity~\citep{garipov2018loss, draxler2018essentially}, which refers to the relationships between disparate local minima obtained from varied training runs, offers a powerful lens for examining the loss landscape. Mode connectivity has been shown closely related to model robustness~\citep{zhao2020bridging} and generalization capabilities~\citep{vrabel2024input}. Standing apart from other theoretical tools like Neural Tangent Kernels~\citep{yanggraph} or Mean Field Theory~\citep{aminian2024generalization} that often rely on assumptions not readily applicable to real-world scenarios, mode connectivity allows for the study of practical models under realistic conditions.

While mode connectivity has been extensively explored in fully-connected networks (FCNs), convolutional neural networks (CNNs), and transformers \citep{garipov2018loss, draxler2018essentially, qin2022exploring, entezari2021role}, a systematic investigation into GNN mode connectivity is, to the best of our knowledge, absent.  GNNs, operating on non-Euclidean graph data, introduce unique data-structure interactions that may fundamentally alter training dynamics compared to models processing independent and identically distributed (i.i.d.) data. Understanding whether GNN mode connectivity deviates from that of other architectures is crucial. Dissimilar behavior could unveil unique properties of the GNN loss landscape, while similarities would allow us to leverage existing theoretical frameworks from other domains to explain the GNN optimization.  Therefore, exploring GNN mode connectivity is a critical step towards demystifying these powerful models, motivating our central inquiry: \textit{How does mode connectivity manifest in GNNs, and how is it influenced by the inherent structure of the input graph?}

This work provides an initial, systematic answer to these questions through a comprehensive controlled study encompassing 12 diverse graphs from various domains.  Our key observation is that, in contrast to FCNs and CNNs, \textbf{GNNs exhibit a distinct non-linear mode connectivity, accurately characterized by a polynomial curve such as a quadratic BÃ©zier curve}. This phenomenon strongly suggests that the non-i.i.d. nature of graph data profoundly impacts GNN training dynamics.  Furthermore, dissecting the influence of model architecture versus data characteristics, we find that \textbf{GNN architectures have minimal impact on mode connectivity, whereas graph structure emerges as the dominant factor}. This relationship is further corroborated by a strong correlation between specific graph properties and the observed mode connectivity. We complement these empirical findings with a theoretical analysis providing rigorous justification for our observations.

Beyond characterizing GNN mode connectivity, our empirical evidence reveals a compelling link between mode connectivity and generalization performance. For instance, models trained on graphs exhibiting enhanced feature separability and higher homophily levels demonstrate simultaneously superior predictive accuracy and more robust mode connectivity.  This motivates the use of mode connectivity as a diagnostic tool for GNN generalization.  We introduce a generalization bound based on loss barriers, a key metric derived from mode connectivity analysis. Extending our investigation across diverse graph domains, we further construct a Wasserstein distance based on mode connectivity, offering a novel geometric perspective for domain alignment and providing a potential explanation for the efficacy of existing model-based knowledge transfer. In summary, we present a systematic study on the mode connectivity of GNNs and uncover novel insights into their optimization behavior.

% Our key contributions are summarized as follows:
% \begin{itemize}
%     \item \textbf{First systematic study of GNN mode connectivity.} We empirically analyze mode connectivity in GNNs across diverse graph datasets, revealing a fundamentally distinct, non-linear mode connectivity pattern compared to FCNs and CNNs.
%     \item \textbf{Graph structure as the dominant factor.} Our controlled experiments demonstrate that mode connectivity in GNNs is primarily dictated by the graph structure rather than the choice of model architecture, highlighting the impact of structural properties on optimization behavior.
%     \item \textbf{Theoretical justification of GNN mode connectivity.} We provide rigorous theoretical analysis explaining why GNN mode connectivity deviates from other architectures, grounding our empirical findings in mathematical insights.
%     \item \textbf{Practical implications for generalization and transferability.} We introduce a generalization bound based on loss barriers and propose a Wasserstein distance metric for mode connectivity, offering a novel perspective on domain alignment and model-based knowledge transfer in GNNs.
% \end{itemize}

% In this work, we present a systematic study on the mode connectivity of GNNs and uncover novel insights into their optimization behavior. Our key contributions are summarized as follows:

% establish a relationship between mode connectivity and graph domain, offering a potential explanation for the efficacy of model-based knowledge transfer~\citep{cao2023autotransfer} and cross-domain adaptation strategies in GNNs~\citep{wu2020unsupervised, zhang2019dane, mao2021source}.

% In summary, this work delivers novel empirical insights into GNN mode connectivity in node classification and lays a rigorous theoretical foundation for analyzing training dynamics, distribution alignment, and generalization mechanisms in graph-based learning.  Our findings offer valuable perspectives for refining GNN training methodologies and developing effective cross-domain adaptation techniques. 


% The paper is structured as follows: Section \ref{sec: rw} provides background and reviews related work. Section \ref{sec: pre} introduces relevant concepts and notation systems. Section \ref{sec: main} investigates the mode connectivity behavior, and Section \ref{sec: general} discusses the implication of mode connectivity for GNN generalization. Section \ref{sec: conclusion} summarizes the whole work and discusses future directions. 

% \textbf{Implications for GNN generalization and cross-domain alignment. (Section 5)}  

% and it allow us to study practical models unlike other theoretical tools such as neural tangent kernel~\citep{yanggraph} or mean field theory~\citep{aminian2024generalization} which presents a gap to real-world cases. 


% In deep learning, \jt{are these studies about model connectivity??/} prior works \jt{add as references?} (e.g., Garipov et al., 2018; Draxler et al., 2018) have shown that local optima obtained under different random initializations and training trajectories are often connected by low-loss paths. This phenomenon reveals the geometric properties of high-dimensional loss landscapes and provides a theoretical foundation for improving model ensembling and robustness \jt{do previous works all focus on CNNs?} . Compared to conventional convolutional neural networks (CNNs), GNNs exhibit fundamental differences in information propagation, parameter sharing, and aggregation mechanisms. These differences result in a more complex and dynamic loss landscape, thereby influencing the connectivity of different solutions.

% \jt{the following is not well organzied. we present what we have done but why we do these and what are their relations are unclear. We may start with a few research questions we try to investigate which correspond to what we want to study and claim}

% This work systematically investigates \jt{the problem of mode connectivity is not well defined, is it to study our understanding of the optimization dynamics, loss landscape, and generalization mechanisms of GNNs?? we may define this problem at the first paragraph??} the mode connectivity of GNNs in node classification tasks from multiple perspectives. First, we empirically validate whether GNN models trained under different random initializations and data orders can be connected via low-loss paths. Our experiments reveal that as the proportion of training data increases, the loss barrier along the connection paths decreases, sometimes leading to nearly linear connections, suggesting an increasing alignment between the training and test distributions. Second, on multiple CSBM datasets, we observe that mode connectivity is closely related to node classification performance and is significantly influenced by graph properties such as node connectivity, heterophily, and intra-class feature distributions. These findings provide empirical evidence for understanding how graph structures affect model optimization and generalization.

% Furthermore, we explore the role of data selection in regulating mode connectivity. Our experiments indicate that jointly training on different data subgroups substantially improves node classification performance, with performance gains being strongly correlated with enhanced mode connectivity. In cross-domain graph scenarios, we extend the notion of loss barriers to explain the success of domain alignment techniques such as UDA-GNN, showing that variations in loss barriers across different data partitions reflect improvements in distributional alignment.

% To support our empirical observations, we conduct extensive experiments on real-world datasets using state-of-the-art GNN models, including GCN, GraphSAGE, and GAT. We construct connection paths along different training trajectories and extend the traditional definition of loss barriers to better compare model connectivity across different data partitions. Building on these findings, we derive a new Lipschitz upper bound theorem for CSBM and its heterophilic variant (CSBM-H), theoretically linking loss barriers, mode connectivity, and model generalization.

