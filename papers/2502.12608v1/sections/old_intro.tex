Foundation models (FMs), which are pre-trained on vast corpora and demonstrate strong generalization capabilities on unseen data during inference, have emerged as a dominant paradigm in modern machine learning. 
With their ability to process inputs from diverse datasets and tackle various downstream tasks, FMs have achieved remarkable success in both computer vision (CV) and natural language processing (NLP). 
Despite the success, the development of graph foundation models (GFMs) has encountered distinct challenges. A major issue is the feature heterogeneity problem. 
As a ubiquitous data structure, graphs can represent a wide range of data across different domains, such as academia, e-commerce, and biology. 
Consequently, graphs from different sources often contain node feature vectors with varying semantic meanings and dimensions. 
This discrepancy makes it difficult for a single model to handle data from multiple graph domains effectively.

Recently, there has been a growing trend to integrate large language models (LLMs) into graph foundation models to unify the node feature space. 
The approach typically involves extracting raw text from node features and encoding these texts into language embeddings via LLMs, which are then used as the new node features. 
This transformation allows text-attributed graphs to be mapped into a unified embedding space. 
Most current studies follow this preprocessing strategy, pre-training a graph foundation model on multiple datasets.

However, despite unifying the node feature space, graph foundation models still encounter \jt{I think we may use negative transfer? we may use "heterogeneity issues", the first bullet is about the remaining feature heterogeneity issue and the second one is structural heterogeneity.  } negative transfers for several reasons: 
(1) Even within the same language embedding space, feature distribution shifts exist among different datasets, potentially causing negative transfer. 
(2) It remains difficult for a single model to effectively capture the diverse structural patterns present in different graphs. These issues limit the \jt{not just generalization but also knowledg in the pretraining data is hardly captured. } generalization capability of current graph foundation models when applied to downstream tasks.  \jt{That would be the reason, we still can observe GFMs cannot help or even negative transfer...}



In contrast to the traditional approach of training a single model for all graph data, we propose a "one-to-one" pipeline to \jt{I do not think to addess negative transfer, I think we still try to handle the heterogeneity problem?? } address negative transfer in cross-dataset graph learning.  \jt{we can high-levely introduce our design}This pipeline comprises two components: an expert model pre-trained on a source graph and a gate module that evaluates the similarity of the input to the source graphs.  \jt{after high-levely mentioning our design, we can discuss what are the advantages of this design and new challenges introduced. }
Both components feature simple structures, avoiding the complexity of previous designs.


\jt{embracing both opportunities and challenges, we design our framework, the following is too detailed and I think we can still briefly introduce it.  }

Specifically, the expert module consists of a set of non-parametric SGCs to capture information from different hops and an attention mechanism to fuse the information.
After pre-training the expert module, we would fix its parameters and train a post-hoc gate module on the same graph.
The gate is an MLP model and is deployed to score the input data by its likeness to the source data with which the gate is trained.
Thus, given a set of $N$ source graphs, the pipeline will train an expert and a gate on each graph, resulting in a library of $N$ experts and $N$ gates.
During the inference stage, every gate will give an importance score for the test sample, and experts with top-k high corresponding scores will be fused to predict the labels of the downstream data.
This enables the pipeline to adaptively choose the experts whose knowledge can be generalized to the downstream labels, thus mitigating the negative transfer problems caused by data heterogeneity.

Our contributions in this work can be summarized as follows:
\begin{itemize}[leftmargin=*]
\item We propose a new paradigm for cross-dataset graph learning. Unlike existing methods that apply one model to all datasets, our design could adaptively choose transferrable experts for the test samples, minimizing the negative transfer.
\item Our design makes it easier to incorporate new pre-training datasets, requiring only the training of a new expert and gate for the new dataset, without needing to repeat the entire pre-training process.
\item We show the superiority of our design on both zero-shot and few-shot learning settings.  
\end{itemize}