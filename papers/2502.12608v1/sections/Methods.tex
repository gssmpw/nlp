

\section{Preliminaries}
\label{sec: pre}

In this section, we introduce the graph neural network (GNN) model used for node classification (Section~\ref{sec: gnn}) and the Contextual Stochastic Block Model (CSBM) as the underlying data model for theoretical analysis (Section~\ref{sec: csbm}). We then formally define mode connectivity in GNNs (Section~\ref{sec: mc}) and discuss different experimental evaluation methods used to study it.

\subsection{Graph Neural Networks}
\label{sec: gnn}

In this work, we consider a graph neural network (GNN) trained for node classification task on a graph represented as $\mathcal{G} = (\mathbf{A}, \mathbf{X}, Y)$. The adjacency matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ encodes the graph structure, where $n$ is the number of nodes. The node feature matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$ contains $d$-dimensional features for each node. The label matrix $Y \in \mathbb{R}^{n \times C}$ represents the ground-truth labels for $C$ classes.


\noindent\textbf{GNN Forward Propagation:}  
In this work, we focus on Graph Convolutional Networks (GCNs)\cite{kipf2016semi}, the classic GNN architecture, which propagates node representations iteratively as:
A GNN with $L$ layers propagates node representations iteratively as:
\begin{equation}
    \mathbf{H}^{(l)} = \sigma\Bigl(\hat{\mathbf{A}} \mathbf{H}^{(l-1)} \mathbf{W}^{(l)}\Bigr), \quad l = 1, \dots, L,
\end{equation}
where $\mathbf{H}^{(l)} \in \mathbb{R}^{n \times d_l}$ denotes node embeddings at layer $l$, with $d_l$ the embedding dimension of that layer. $\hat{\mathbf{A}}$ is the normalized adjacency matrix, $\mathbf{W}^{(l)}$ is the trainable weight matrix, and $\sigma(\cdot)$ is a non-linear activation function. The initial representation is $\mathbf{H}^{(0)} = \mathbf{X}$.

\noindent\textbf{Final Node Embeddings:}  
After $L$ layers of message passing, the final node representations are: $\mathbf{Z} = \mathbf{H}^{(L)} \mathbf{W}^{(L)}$,
where $\mathbf{Z} \in \mathbb{R}^{n \times C}$ represents the node embeddings before classification.

\noindent\textbf{Output Layer (Softmax Probability Distribution):}  
The predicted class probabilities are obtained using the softmax function:$\mathcal{P}^{\mathbf{Z}} = \text{softmax}(\mathbf{Z})$, where $\mathcal{P}^{\mathbf{Z}} \in \mathbb{R}^{n \times C}$ is the probability distribution over $C$ classes.

% \jt{why we have loss on test set?? we may define both losses, but it will confuse people since we may think both losses will be used for training? }

\noindent\textbf{Loss Function:}  
The model is trained by minimizing the cross-entropy loss over the labeled nodes in the training set $\mathcal{V}_{\text{train}}$:  
\begin{equation}
    \mathcal{L}_{\text{Tr}} = - \sum_{i \in \mathcal{V}_{\text{train}}} \sum_{c=1}^{C} Y_{i,c} \log \mathcal{P}_{i,c}^{\mathbf{Z}},
\end{equation}
where $Y_{i,c}$ denotes the one-hot ground-truth label. For evaluation purposes, we also define the test loss on $\mathcal{V}_{\text{test}}$ as:  
\begin{equation}
    \mathcal{L}_{\text{Te}} = - \sum_{i \in \mathcal{V}_{\text{test}}} \sum_{c=1}^{C} Y_{i,c} \log \mathcal{P}_{i,c}^{\mathbf{Z}}.
\end{equation}
\noindent\textbf{Accuracy Metric:}  
Classification accuracy is measured as the proportion of correctly classified nodes: 

\begin{equation}
    \text{Acc}_{\text{Tr}} = \frac{1}{|\mathcal{V}_{\text{train}}|} \sum_{i \in \mathcal{V}_{\text{train}}} \mathbb{I}(\hat{y}_i = \arg\max_c Y_{i,c}),
\end{equation}

\begin{equation}
    \text{Acc}_{\text{Te}} = \frac{1}{|\mathcal{V}_{\text{test}}|} \sum_{i \in \mathcal{V}_{\text{test}}} \mathbb{I}(\hat{y}_i = \arg\max_c Y_{i,c}),
\end{equation}
where $\hat{y}_i = \arg\max_c \mathcal{P}_{i,c}^{\mathbf{Z}}$ is the predicted label, and $\mathbb{I}(\cdot)$ is the indicator function.
% \jt{we need to add a few sentences why we want to define the following gaps}

\noindent\textbf{Generalization Gap~\cite{neyshabur2017exploring}:}  
 The generalization gap is defined as the difference between the loss on test set and that on training set:
\[
\Delta_{\text{gen}} = \mathcal{L}_{\text{Te}} - \mathcal{L}_{\text{Tr}}.
\]

\noindent\textbf{Domain Performance Gap~\cite{ben2010theory}:}  
Assume that the model is evaluated on two different domains, a source domain $\mathcal{G}_S $ and a target domain$\mathcal{G}_T$, with corresponding losses \(\mathcal{L}_S\) and \(\mathcal{L}_T\). The domain adaptation gap is defined as:
\[
\Delta_{\text{da}} = \mathcal{L}_T - \mathcal{L}_S.
\]
\subsection{Contextual Stochastic Block Model (CSBM)}
\label{sec: csbm}

To theoretically understand the behavior of GNNs, we employ Contextual Stochastic Block Model (CSBM)~\cite{deshpande2018contextual} as the underlying data model for graphs, which has been widely adopted for node-level task~\cite{Ma2021IsHA, baranwal2021graph, wang2022augmentation, baranwal2023effects, wang2022can, fountoulakis2022graph} due to its ability to simultaneously capture both node features and graph structures.

\begin{definition}[Contextual Stochastic Block Model (CSBM)]
A CSBM with two classes defines a graph \( \mathcal{G} = (\mathcal{V}, \mathcal{E}, X, Y) \), where  
\(|V|=n\) denotes the number of nodes nodes and \( \mathcal{V} \) belong to two disjoint classes \( \mathcal{C}_1 \) and \( \mathcal{C}_2 \), with labels \( Y \in \{c_1, c_2\} \). Each node \( v \) with label \( c_v \) has a feature vector \( x_v \sim \mathcal{N}(\mu_i, \sigma I) \), where \( \mu_i \in \mathbb{R}^d \) is the class-wise mean, and $\sigma$ quantifies the separability of node features. Edges form independently with probability  
\[
P((u,v) \in \mathcal{E}) =
\begin{cases}
    p_{in}, & \text{if } Y_u = Y_v, \\
    p_{out}, & \text{if } Y_u \neq Y_v,
\end{cases}
\]
where \( p_{in} > p_{out} > 0 \) denote intra-class and inter-class connection probabilities, respectively.
\end{definition}
\paragraph{Remark.}  
The CSBM model describes a graph where edges form based on class memberships, and node features are drawn from class-specific distributions. The probability sum \( p_{in} + p_{out} \) reflects the overall edge density, influencing how well information propagates in the network. The ratio \( \frac{p_{in}}{p_{in} + p_{out}} \) measures the tendency of nodes to connect within their class, affecting the homophily of the graph. The feature variance \( \sigma \) controls how tightly node features cluster around their class means, impacting the difficulty of classification. Additionally, the number of nodes \( n \) determines the scale of the graph, which can influence statistical properties and computational complexity.

% \begin{remark}
    
% \end{remark}
% CSBM can be used as a graph generation model to conduct a controlled study by adjusting hyperparameters to generate graphs with different properties. The degree of nodes and homophily are two important properties of a graph. The former can be measured as the sum of the in-degree and out-degree of a node, represented as \( p_{in} + p_{out} \), while the latter is denoted as \( h(G) \), which quantifies the similarity of labels among neighbors. 



\subsection{Mode Connectivity}
\label{sec: mc}

Mode connectivity~\cite{garipov2018loss,draxler2018essentially} studies the relationship between different parameter space modes in neural networks. Despite the abundant research about mode connectivity in FCN, CNN, and transformer~\citep{draxler2018essentially, garipov2018loss, entezari2021role, qin2022exploring}, there's yet no existing research on mode connectivity in GNN. So, we aim to extend existing definitions of mode connectivity to GNNs. 
\noindent\textbf{Parameter Space Mode.}  
A \textit{mode} in the parameter space refers to a set of parameters $\theta$ that minimizes the loss function $\mathcal{L}$ for a given graph $\mathcal{G}$. Formally, an optimal mode is defined as:
\begin{equation}
\theta^* = \operatorname*{argmin}_\theta \mathcal{L}(f(\mathcal{G};\theta), Y),
\end{equation}
where $f(\mathcal{G}; \theta) = \mathcal{P}^{\mathbf{Z}}$ represents the probability distribution predicted by the GNN model, and $Y$ is the ground-truth label matrix. For an \( L \)-layer GNN, the parameters are given by \( \theta = \{W^{l}\} \) for \( l = 0, \dots, L \).
Different modes arise due to variations in training conditions such as random initialization and data sampling.

Given two independently trained GNN models under different training configurations $M_a$ and $M_b$, we obtain two sets of parameters $\theta_a$ and $\theta_b$, each corresponding to a local minimum of $\mathcal{L}$. 
% A fundamental property of mode connectivity is the ability to connect these minima with a low-loss path, where linear interpolation path and bezier curve interpolation path are two commonly considered ones. 
A fundamental property of mode connectivity is the ability to connect these minima with a low-loss path~\citep{draxler2018essentially, garipov2018loss}. Two commonly considered paths are linear interpolation and Bézier curve interpolation, both of which are used to fit the minima of the trained model. A good fit suggests that the modes lie along a simple, low-loss manifold, providing further insights into the structure of the loss landscape.


\noindent\textbf{Linear Interpolation Path.}  
\citet{frankle2020linear,qin2022exploring} demonstrate that neural networks initialized similarly often exhibit a linear low-loss connection between minima. A linear interpolation path can be used to examine this property, which can be defined as continuous curve $\phi(\alpha): [0, 1] \to \mathbb{R}^{|\theta|}$ connecting $\theta_a$ and $\theta_b$:
\begin{equation}
    \phi(\alpha) = (1 - \alpha) \theta_a + \alpha \theta_b, \quad \alpha \in [0,1].
\end{equation}

Even if a fully linear path between two minima results in a high loss, a low-loss non-linear path may still exist. A quadratic Bézier curve is often adopted~\cite{garipov2018loss,gotmare2018using,draxler2018essentially} to examine such property.

\noindent\textbf{Bezier Curve Interpolation Path.} A bezier curve with  two modes $\theta_{a}$ and $\theta_{b}$ as endpoints can be given as 
\begin{equation}
\phi(\alpha) = (1-\alpha)^2 \theta_{a} + 2\alpha(1-\alpha) \theta + \alpha^2 \theta_{b},
\end{equation}
where $\theta$ is a learnable parameter. Specifically, mode connectivity can be analyzed through  
(i) the smoothness of the loss and accuracy along $\phi(\alpha)$, and  
(ii) the \textit{loss barrier}, which captures the maximum increase in loss along the interpolation.


\begin{definition}[Loss Barrier]
For two given models $\theta_a$ and $\theta_b$, the loss barrier is defined as:
\begin{equation}
    B(\theta_a, \theta_b) := \max_{\alpha \in [0,1]} 
    \Big( \mathcal{L}(\phi(\alpha)) - \mathcal{L}_{\text{lin}}(\alpha) \Big),
\end{equation}
where $\mathcal{L}_{\text{lin}}(\alpha)$ is the linearly interpolated loss: $\mathcal{L}_{\text{lin}}(\alpha) = (1-\alpha) \mathcal{L}(\theta_a) + \alpha \mathcal{L}(\theta_b). $
\end{definition}

The loss barrier measures the deviation from a perfect linear interpolation in the loss landscape. Taking linear interpolation path as an example, a higher $B(\theta_a, \theta_b)$ suggests weaker connectivity, whereas $B(\theta_a, \theta_b) \approx 0$ indicates linear mode connectivity.  

% In experiments, we compute the loss and accuracy at evenly spaced points along $\phi(\alpha)$ to assess both the interpolation smoothness and the magnitude of the loss barrier. These analyses provide insights into whether and when GNNs exhibit strong linear mode connectivity, shedding light on the underlying optimization landscape.
\begin{figure*}[!ht]\label{fig:mc1}
    \centering
    % 第一行
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/init/init/cora_gcn_plot.png}
        \caption{Cora}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/init/init/citeseer_gcn_plot.png}
        \caption{Citeseer}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/init/init/pubmed_gcn_plot.png}
        \caption{Pubmed}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/init/init/wikics_gcn_plot.png}
        \caption{Wiki-CS}
    \end{subfigure}
    % 加粗的 Figure 1 说明
    \vspace{5pt}
    \label{fig: figure1}
    % 这里加一个换行
    \newline
    % 第二行
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/init/init/coauthor-cs_gcn_plot.png}
        \caption{Coauthor-CS}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/init/init/coauthor-physics_gcn_plot.png}
        \caption{Coauthor-Physics}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/init/init/squirrel_gcn_plot.png}
        \caption{Squirrel}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/init/init/chameleon_gcn_plot.png}
        \caption{Chameleon}
    \end{subfigure}


    % 加粗的 Figure 2 说明
    % \textbf{Figure 1:} The performance of linear interpolations between two minima trained with different training data order.
    \caption{Performance of linear interpolations between two minima on eight real-world datasets. The x-axis represents the interpolation coefficient $\alpha$, and the y-axis shows train accuracy, test accuracy, train loss, and test loss.}
    \vspace{-0.2in}
    \label{fig: figure1}
\end{figure*}
\section{Main Analysis}
\label{sec: main}

In this section, we systematically study mode connectivity of GNNs. We begin by exploring the manifestation of mode connectivity in GNNs~(Section~\ref{sec:lmc}). Considering the non-Euclidean nature of graphs and the intertwined roles of structure and features, we proceed to investigate how these graph properties impact the mode connectivity of GNNs~(Section~\ref{sec:gp}). Finally, we develop a theoretical framework to explain the empirically observed phenomena (Section~\ref{sec:theory}) and provide further insights.  






\begin{figure*}[!ht]\label{fig:Bezier}
    \centering
    % 第一行
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/Bezier/Bezier/cora_bezier_fitted_plot.png}
        \caption{Cora}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/Bezier/Bezier/citeseer_bezier_fitted_plot.png}
        \caption{Citeseer}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/Bezier/Bezier/pubmed_bezier_fitted_plot.png}
        \caption{Pubmed}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/Bezier/Bezier/wikics_bezier_fitted_plot.png}
        \caption{Wiki-CS}
    \end{subfigure}
    % 加粗的 Figure 1 说明
    \vspace{5pt}
    % 这里加一个换行
    \newline
    % 第二行
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/Bezier/Bezier/coauthor-cs_bezier_fitted_plot.png}
        \caption{Coauthor-CS}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/Bezier/Bezier/coauthor-physics_bezier_fitted_plot.png}
        \caption{Coauthor-Physics}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/Bezier/Bezier/squirrel_bezier_fitted_plot.png}
        \caption{Squirrel}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/Bezier/Bezier/chameleon_bezier_fitted_plot.png}
        \caption{Chameleon}
    \end{subfigure}
    \caption{Performance of Quadratic Bézier Curve Interpolations Between Two Minima. This figure shows train/test loss and accuracy curves for quadratic Bézier curve interpolations across eight datasets. Compared to linear interpolation (Figure~\ref{fig: figure1}), Bézier curves better bypass loss barriers, indicating that GNN minima, while not always linearly connected, often lie on a smooth low-loss manifold.}
    \label{fig: figure2}
\vspace{-0.2in}
\end{figure*}

\subsection{Does Mode Connectivity Exist in Graph Neural Networks?}\label{sec:lmc}

While prior works have studied the mode connectivity for fully connected and convolutional neural network~\cite{garipov2018loss, draxler2018essentially}, the graph-dependent message-passing mechanisms of GNNs may influence the loss landscape and thus mode connectivity, which motivates us to first study the the existence and characteristics of mode connectivity in GNNs.

% In this subsection, we systematically investigate the existence and characteristics of mode connectivity in Graph Neural Networks (GNNs) under varying training conditions. While prior work has established mode connectivity in fully connected and convolutional networks (Garipov et al., 2018; Draxler et al., 2018), the graph-dependent message-passing mechanisms of GNNs introduce unique challenges to their optimization landscape. Specifically, we examine how training data order, parameter initialization, aggregation mechanisms, and non-linear connectivity pathways influence the connectivity of different minima in GNNs.

\noindent\textbf{Investigation protocol.} We adopt node classification as our research subject and utilize one of the most commonly adopted GNNs, GCN~\cite{Kipf2016SemiSupervisedCW}, as the backbone models, while also studying the influence of different model architectures. In terms of data selection, we consider $12$ graphs with different properties, including those exhibiting homophilic and heterophilic~\cite{Ma2021IsHA} characteristics.  Following the settings in ~\cite{luo2024classic}, we fix all hyperparameters except one, which we vary to generate different modes. Specifically, we investigate the impact of different initialization strategies, data orders (Appendix~\ref{app: A}), and model selections, as explored in \citet{garipov2018loss,qin2022exploring}. 
% Additionally, we examine the influence of various aggregation mechanisms unique to GNNs.  
After generating these modes, we follow \citet{garipov2018loss} to fit the corresponding performance data points using linear interpolation and Bézier curve interpolation, where train accuracy, test accuracy, train loss, and test loss are measured. To ensure statistical reliability, all experiments are repeated three times with different random seeds, and we report the average performance. Additional details and supplementary results are provided in Appendix~\ref{app: A}.

\noindent\textbf{Observation 1: The GNN minima are usually connected but not always linearly connected.} 
After fitting the performance data points of different modes using linear interpolation, we observe a significant increase in both the training loss and the test loss (see Figure\ref{fig: figure1}). This indicates that although linear mode connectivity is a common phenomenon in fully connected networks (such as MLPs and CNNs)~\citep{frankle2020linear, yunis2022convexity}, it does not always hold true for GNNs. \textbf{First}, as shown in Figure\ref{fig: figure1}, 
linear interpolation often results in noticeable loss barriers. For example, the interpolation curves for datasets such as Cora and Citeseer are relatively smooth with only slight increases in loss, which suggests that the minima obtained during training are relatively well-connected in the parameter space. However, in datasets such as Coauthor-CS and Squirrel, we observe significant loss barriers - linear interpolation leads to a dramatic drop in model performance. This phenomenon indicates that for some datasets, the solutions obtained from training GNNs are more isolated and cannot be connected by a simple linear path. \textbf{Second}, although there is a lack of a linear path, most modes can still be connected by a simple polynomial curve, such as a quadratic Bézier curve. We use a quadratic Bézier curve to fit the performance data of different modes and observe that the loss does not increase significantly (see Figure \ref{fig: figure2}). This indicates that although GNNs may lack linear mode connectivity (LMC), in most cases their minima are not completely isolated but are distributed on a well-structured low-loss manifold and can be connected via learnable nonlinear paths.

\textbf{Further probing.} To further understand this phenomenon, we visualize the loss landscapes on different datasets (Figure \ref{fig:contour}). The loss contour map for the Pubmed dataset shows that its minima typically lie within the same smooth local loss basin, implying that the training process tends to converge to a connected low-loss region, allowing different minima to be connected via a low-loss path. In contrast, the Squirrel dataset exhibits a significantly more rugged loss landscape, with steeper boundaries for the local loss basins and minima that may be distributed across different disjoint basins. This explains why, in some datasets, the mode connectivity of GNNs is poor and why they cannot be effectively connected via linear interpolation.


\textbf{Implications for lottery ticket hypothesis and pruning.} It is noteworthy that the observed phenomenon is related to studies on linear mode connectivity (LMC), the Lottery Ticket Hypothesis, and pruning~\citep{frankle2020linear}. When studying the LMC of fully connected neural networks, pruning is shown to generally improve mode connectivity, making network weights easier to interpolate along a low-loss path~\citep{frankle2020linear}. However, our observations suggest that the pruning capability of GNNs may not be universal: on some datasets (e.g., Cora), the model weights are relatively smooth in the loss landscape and are easy to connect, whereas on other datasets (e.g., Squirrel) the loss landscape is more complex, and the minima may reside in different local basins, leading to pruned modes that are still difficult to connect. This finding emphasizes the role of the GNN model structure and its optimization strategy in mode connectivity and suggests that the existing LMC theory might not be directly applicable to all GNN architectures. This observation can potentially explain why the heterophilic graph is seldom considered in model-based graph condensation~\citep{jin2021graph, zheng2024structure}. 

% \noindent\textbf{Observation 2.} \textbf{Better model test performance generally implies better mode connectivity.} 

% \textbf{Implications for the generalization behavior of GNNs.} 


\noindent\textbf{Observation 2.} \textbf{The interpolation curves present similar patterns for graphs coming from similar domains.} 
We then look into the detailed pattern difference among interpolation curves of different graphs. Surprisingly, we observe that datasets from different domains exhibit distinct patterns, and those from similar domains tend to have more similar curves. In particular, citation networks (such as Cora, CiteSeer, and PubMed) display a smoother interpolation process compared to co-authorship networks (such as Coauthor-CS and Coauthor-Physics), which may relate to the nature of each graph. In citation networks, papers typically cite other papers within the same or similar research fields, which results in a stronger clustering effect among nodes of the same class. In contrast, due to cross-domain collaborations, the local structure in co-authorship networks is more complex; certain nodes (researchers) might belong to multiple communities simultaneously, leading to blurred class boundaries. 

% We further analyze the phenomenon appeared on the Coauthor-CS dataset in \jt{please index the appendix} Appendix XX, whose interpolation curve looks alike the combination of one of Coauthor-Physics and Wiki-CS. 

\textbf{Implications for measuring domain discrepancy of different graphs.} This observation suggests that mode connectivity can be effectively employed to gauge the domain discrepancy among different graphs, which is important for applications like graph foundation models~\citep{chen2024textspace, maoposition}. While traditional measures such as Maximum Mean Discrepancy (MMD)~\citep{JMLR:v13:gretton12a} or degree distribution capture only partial aspects of a graph's feature distribution or structural properties, model-based mode connectivity offers a far more nuanced and detailed assessment. The advantages of this approach and its applications are discussed further in Section~\ref{subsec:graph_similarity}. 

% The existence of non-linear mode connectivity for GNN inspires us to further analyze the \textit{geometric properties of the GNN loss landscape}. Following \citet{garipov2018loss}, we use a contour plot to visualize loss surface surrounding the minima, and the result reveals: 

% \noindent\textbf{Observation 2: GNN minima reside in the same loss basin and exhibit local convexity.}  We find most GNN minima are located within the same loss basin, reinforcing the hypothesis that \textit{they are not entirely independent solutions but variations within a shared optimization region}. Moreover, for most minima within the same basin, their convex combination also remains within the basin, suggesting that the loss landscape exhibits \textit{local convexity} in parameter space.  
% Such observation indicates the close relationship between the mode connectivity and generalization behavior of GNNs, which we give a detailed discussion in Section~\ref{sec: general}.

\begin{figure}[ht]
    \centering
    % 第一行
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/basin/Basin/loss_landscape_fixed_pubmed.png}
        \caption{Pubmed}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/basin/Basin/loss_landscape_fixed_squirrel.png}
        \caption{Squirrel}
    \end{subfigure}
    % 加粗的 Figure 1 说明
    \vspace{-0.1in}
    \caption{Contour visualization of loss basin} 
    \vspace{-0.2in}
    \label{fig:contour}
    \end{figure}

% \czk{this seems an extension experiment after mode connectivity, need some motivation first to say why we need to look at this}

% \czk{Not very sure, can we say From two-dimensional to three-dimensional here?}
% \lbh{i also feel conflicted. so changed.}

% \textbf{Observation 2: Minima Reside in the Same Loss Basin and Exhibit Local Convexity.}  

% To further analyze the \textit{geometric properties of the GNN loss landscape}, we generate \textit{contour visualizations} of the loss surface surrounding the minima in Figure 4. Our key findings include:  

% - Most GNN minima are located within the same loss basin, reinforcing the hypothesis that \textit{they are not entirely independent solutions but variations within a shared optimization region}. 

% - For most minima within the same basin, their convex combination also remains within the basin, suggesting that the loss landscape exhibits \textit{local convexity} in parameter space.  

% \czk{need some transition and motivation here to say why we study training ratio}

% \czk{we may use the semi-supervised setting in GNN to motivate this, in GML the training ratio is usually low, and more training ratio can close the gap }

% \czk{for prior explanations, add citation }

% \czk{\textbf{Comparison to the phenomenon of MLP.} Visualizations of mode connectivity and the loss landscape reveal a significant difference between GNNs and MLPs. To quantitatively analyze this distinction, we compare the loss barriers between the minima of GNNs and MLPs. As illustrated in Figure 5, while MLPs consistently exhibit near-linear mode connectivity across datasets, GNNs display marked non-linearity, suggesting that their minima are not as easily traversable.

% Contrary to earlier explanations attributing mode connectivity differences to optimizer bias, our findings indicate that the inductive biases introduced by graph convolutions fundamentally reshape the loss landscape. This effect likely stems from the relational dependencies inherent in graph data, which constrain optimization trajectories and reinforce local structures in the parameter space. Consequently, mode connectivity in GNNs is not merely a byproduct of training dynamics but is intrinsically determined by graph topology. In the next section, we further investigate how graph structures and features influence mode connectivity through the lens of graph properties. 

% } 

In the previous study, we restrict the model backbone to GCN and explore the mode connectivity of the same model across different graphs. To further investigate the impact of GNN model architectures on mode connectivity, we replace GCN with MLP, GraphSAGE~\citep{hamilton2017inductive} and GAT~\citep{velickovic2017graph} models and study their mode connectivity. We observe
\noindent\textbf{Observation 3: Message passing significantly affects the mode connectivity while different aggregation design puts little influence.} 
As shown in Figure~\ref{fig:obs3}, when using the loss barrier to assess the mode connectivity of each model, there is a significant gap between the mode connectivity of the MLP and the GCN. In contrast, for GNNs with varying architectural designs, the loss barrier values remain similar (see Appendix~\ref{app: conv}). This phenomenon suggests that while message passing has a substantial impact on mode connectivity, the effect of the specific model design is minimal. Consequently, we are motivated to focus on investigating the influence of the graph data itself on mode connectivity, and in the next section, we will examine how graph properties affect mode connectivity.
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{fig/GNN-MLP/GNN-MLP.png}
    \vspace{-0.1in}
    \caption{Comparison between the loss barrier of GNN and MLP.}
    \label{fig:obs3}
    \vspace{-0.2in}
\end{figure}



% Our results demonstrate that GNNs exhibit substantially larger loss barriers between minima compared to MLPs with the same parameter space dimensionality (Figure 5). While MLPs consistently achieve near-linear mode connectivity across datasets, GNNs display pronounced non-linearity, suggesting that their minima are not as easily traversable.

% Contrary to prior explanations that attribute mode connectivity differences to optimizer bias, our findings indicate that the inductive biases introduced by graph convolutions fundamentally alter the loss landscape. This effect likely arises from the relational dependencies in graph data, which constrain optimization trajectories and reinforce local structures in parameter space. Consequently, mode connectivity in GNNs is not merely a product of training dynamics but is inherently shaped by graph topology.

% \textbf{Observation 3: Loss Barrier Systematically Decreases with More Training Data.}

% Our experiments reveal a systematic decrease in the loss barrier as the proportion of training data increases. This suggests that mode connectivity in GNNs is not static but evolves with the amount of available labeled data. Specifically, in some cases, we observe that the mode connectivity becomes nearly linear, indicating that the independently trained models converge to better-aligned minima as the training data increases.

% To quantitatively validate this trend, we conduct experiments on multiple datasets, progressively increasing the proportion of labeled nodes while keeping all other settings fixed. The results consistently show:

% \textit{As training data increases, the loss barrier systematically decreases.}
% \textit{When sufficient training data is available, mode connectivity becomes nearly linear}, indicating that different minima align closely.
% \textit{In graphs with high homophily or strong connectivity, this effect is even more pronounced}, reinforcing the role of graph structure in mode connectivity.
% Figure~4 presents the loss barrier curves for varying training set sizes. Notably, the transition from \textit{nonlinear to nearly linear mode connectivity} occurs at a critical data proportion threshold, suggesting that there exists a \textbf{minimum required data volume for stable optimization in GNNs}.

% Observation 2: Graph-Induced Relational Bias Disrupts Mode Connectivity in GNNs

% Our analysis reveals a fundamental distinction in mode connectivity between GNNs and MLPs with identical parameter space dimensionality. As shown in Figure 5, while MLPs exhibit near-linear mode connectivity across all datasets, GNNs consistently encounter substantial loss barriers between minima. This suggests that GNN loss landscapes are significantly more fragmented, preventing smooth transitions between solutions.

% Contrary to conventional explanations that attribute mode connectivity differences to optimizer bias or instability, our results indicate that the graph convolution operation itself imposes a strong inductive bias that disrupts connectivity. This effect is likely driven by the relational nature of graph data, which constrains optimization trajectories and induces sharper local minima. These findings suggest that mode connectivity in GNNs is inherently limited by graph structure, highlighting a fundamental difference in how neural architectures interact with their input data.

% This observation has critical implications:  
% - It supports interpolation-based model ensembling, as interpolated models do not suffer from severe loss degradation.  
% - It indicates that GNN mode connectivity is influenced by dataset structure, with connectivity being stronger in some graph types than others.  

% Given that \textbf{non-linear mode connectivity} is the predominant behavior in GNNs, an important question arises: \textit{Under what conditions does linear mode connectivity emerge?} A strong linear connection between minima implies a highly smooth optimization landscape, which is desirable for robust training and efficient ensembling.  

% To investigate this, we analyze the impact of:  
% % 1. \textit{Graph structure} (homophily vs. heterophily, edge density, connectivity).  
% 1. \textit{Architectural choices} (residual connections, normalization techniques).  
% 2. \textit{Dataset split} (labeled data proportion).  

% Our results suggest that \textbf{linear mode connectivity is more likely to emerge when:}  

% - \textbf{Residual connections and normalization techniques (BatchNorm, LayerNorm) are applied}, particularly in \textit{dense graphs}, as they mitigate over-smoothing and stabilize gradients.  

% - \textbf{A sufficient number of labeled nodes is available}, especially in \textit{sparse graphs}, as it provides additional optimization anchors, reducing the risk of models converging to isolated minima.  

% Thus, while \textbf{non-linear mode connectivity is the default in GNNs}, proper architectural and data-driven factors \textbf{can enable near-linear mode connectivity}, reinforcing the effectiveness of recent advancements in GNN model design.  
% \subsection{Extra observations}

\subsection{How Do Graph Properties Influence Mode Connectivity?}\label{sec:gp}
\label{sec: gp}
% \begin{figure*}[!ht]
%     \centering
%     % 第一行
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{fig/CSBM/CSBM/node.png}
%         \caption{Node Number}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{fig/CSBM/CSBM/density.png}
%         \caption{Density}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{fig/CSBM/CSBM/heterophily.png}
%         \caption{Heterophily}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{fig/CSBM/CSBM/sigma.png}
%         \caption{$\sigma$}
%     \end{subfigure}
%     % 加粗的 Figure 1 说明
%     \vspace{5pt}
%     \textbf{Figure 3:} {Graph property}
%     \label{fig:gp}
%     \end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{fig/CSBM/CSBM/CSBM.png}
    \vspace{-0.1in}
    \caption{The trend of mode connectivity, as measured by the barrier, with changes in graph properties.}
    \label{fig:gp}
    \vspace{-0.2in}
\end{figure*}
% \czk{add some transition here. behavior of GCN and FCN is different, why, its because of graph's non-iid nature. In this section, we dig it deeper to see what graph properties affect the mode behavior. }

In this section, we further study how relational dependencies inherent in graphs affect mode connectivity of GNNs through the lens of graph properties. 

\noindent\textbf{Study protocol.} To come up with a controlled study in terms of graph properties, we first adopt a synthetic graph generative model CSBM. [add more experimental details here] We further consider the following key graph properties reflecting the graph structures and feature relationships 
\begin{itemize}[leftmargin=0pt, nosep]
    \item \textbf{Density of the graph} \textbf{{\(p = (p_{\mathrm{in}} + p_{\mathrm{out}})\)}},  which affects the connectivity of a graph. A denser graph implies a stronger structural relationship across different nodes. 
    \item \textbf{Homophily} \textbf{$h(G)= \frac{p_{in}}{p_{in} + p_{out}} $}, which
    refers to the tendency of nodes (vertices) with the same or similar labels to be more likely to connect to each other than to nodes with different labels.
    \item \textbf{Feature separability} \textbf{{\(\sigma\)}} reflects the difficulty of GNNs distinguishing the two different classes, and it can imply the influence of feature distribution on mode connectivity.
\end{itemize}

After fitting different GNNs on generated graphs, we measure the correlation between mode connectivity and corresponding graph properties, where we use loss barriers as a metric for the mode connectivity. As shown in Figure~\ref{fig:gp}, we observe:

\noindent\textbf{Observation 4.} \textbf{Both structural and feature properties of graphs affect the mode connectivity of GNNs.} As shown in Figure~\ref{fig:gp}, we find that 
\begin{enumerate}[nosep, leftmargin=20pt]
    \item \textit{Models trained on denser graphs present lower loss barriers, which implies better mode connectivity}.  Our experiments show that denser graphs—with higher intra-class connection probability \( p \)—exhibit lower loss barriers, indicating smoother transitions between minima. This supports the intuition that well-connected graphs enable stable feature propagation and reduce optimization variance. Conversely, sparse graphs lead to fragmented loss landscapes with higher barriers, particularly when \( p \) is very low, resulting in isolated subgraphs and less alignment across training runs.
    \item \textit{models trained on graphs with either high homophily or high heterophily exhibit better mode connectivity compared to graphs with medium homophily level.} This phenomenon arises from how class information propagates in different structural regimes. In strongly homophilic graphs, message passing reinforces class boundaries, yielding stable and aligned representations. In highly heterophilic graphs, despite inter-class edges, distinct feature distributions keep embeddings separable. However, intermediate homophily introduces label ambiguity, increasing optimization instability and resulting in rugged loss landscapes—a trend linked to the "mid homophily pitfalls" noted in \citet{luan2023graph}.
    \item \textit{Models trained on graphs with greater feature separability demonstrate improved mode connectivity.} Increasing \( \rho \) enhances mode connectivity by making same-class nodes more distinguishable and yielding consistent local optima. However, high intra-class variance or outliers raise loss barriers, indicating that unstable feature aggregation undermines training alignment. This underscores the interplay between graph topology and feature smoothness in optimization stability.
\end{enumerate}

% \textit{Controlling for other factors, models trained on denser graphs present lower loss barriers, which implies better mode connectivity.} Our experiments indicate that denser graphs, characterized by higher intra-class connection probability \( p \), tend to exhibit lower loss barriers, suggesting smoother transitions between minima. This aligns with the intuition that a well-connected graph facilitates more stable feature propagation, thereby reducing variance in optimization trajectories. In contrast, highly sparse graphs display increased fragmentation in their loss landscapes, resulting in higher barriers that impede smooth connectivity. This effect is particularly evident when \( p \) is sufficiently low, leading to isolated subgraphs where optimization proceeds independently, thereby reducing alignment across different training runs.  

% \noindent\textbf{Observation 4.} \textit{Controlling for other factors, models trained on graphs with either high homophily or high heterophily exhibit lower loss barriers, suggesting better mode connectivity compared to graphs with moderate homophily levels.} This phenomenon can be attributed to the way class information propagates in different structural regimes. In strongly homophilic graphs, message passing reinforces class boundaries, leading to more stable and aligned representations across training runs. Similarly, in highly heterophilic graphs, despite the presence of inter-class edges, node embeddings remain separable due to distinct feature distributions. In contrast, intermediate homophily levels introduce ambiguity in label consistency, thereby increasing optimization instability and leading to more rugged loss landscapes, which can be attributed to the ``mid homophily pitfalls'' observed in \citet{luan2023graph}. 

% \noindent\textbf{Observation 5.} \textit{ Controlling for other factors, models trained on graphs with greater feature separability (i.e., larger inter-class mean feature distance \( \rho = \|\mu_1 - \mu_2\| \) demonstrate lower loss barriers, indicating improved mode connectivity.} Our results suggest that increasing \( \rho \) improves mode connectivity, as nodes within the same class become more distinguishable, leading to more consistent local optima. However, when intra-class feature variance is high or when feature outliers are introduced, loss barriers increase significantly, implying that unstable feature aggregation weakens alignment between different training trajectories. This highlights the intricate interplay between graph topology and feature smoothness in determining optimization stability.  

% \noindent\textbf{Observation 6.} Controlling for other factors, models trained on graphs with stronger spectral connectivity (i.e., a higher Cheeger constant 
% \( h(G) \)) show lower loss barriers, implying better mode connectivity. Our empirical analysis reveals that graphs with larger \( h(G) \) tend to exhibit lower loss barriers, suggesting that stronger spectral connectivity facilitates better alignment between different optima. This observation is consistent with prior studies on spectral clustering, reinforcing the hypothesis that well-connected graph structures lead to more stable optimization landscapes. 

\noindent\textbf{Relationship to real-world graphs.} The phenomenon observed in CSBM graphs provides valuable insights into the mode connectivity of GNNs trained on real-world graphs. Specifically, we observe that graphs characterized by stronger feature separability and higher homophily levels, such as Cora, Citeseer, and Pubmed, are associated with GNNs displaying lower loss barriers, further indicating enhanced mode connectivity. These findings suggest that graph density, feature separability, and homophily play significant roles in shaping the mode connectivity landscape of GNNs. In general, we find the phenomenons observed in synthetic graphs match ones observed in real-world graphs.


% \subsection{Why does mode connectivity happen? General Bound for GNN}

% Despite the empirical investigation of mode connectivity in the previous sections, we still lack a rigorous understanding of why this phenomenon occurs in Graph Neural Networks (GNNs). In this section, we address this gap by presenting a theoretical analysis that includes a generalization bound.

%  We formally analyze the loss barrier between two GNN models trained from different initialization. 
% \begin{definition}[Spectral Gap]
% \label{def:spectral-gap}
% Let \(\mathbb{E}[\hat{\mathbf{A}}]\) be the expected normalized adjacency matrix with eigenvalues
% \[
% \lambda_1(\mathbb{E}[\hat{\mathbf{A}}]) \ge \lambda_2(\mathbb{E}[\hat{\mathbf{A}}]) \ge \cdots \ge \lambda_n(\mathbb{E}[\hat{\mathbf{A}}]).
% \]
% We define the \emph{spectral gap} as
% \[
% \Delta \triangleq 1 - \max_{i\ge 2} |\lambda_i(\mathbb{E}[\hat{\mathbf{A}}])|.
% \]
% \end{definition}

% \begin{theorem}[General Bound of the Loss Barrier in \(L\)-layer GNN]
% \label{thm:loss-barrier-GNN}
% Let \(\theta_a\) and \(\theta_b\) be two sets of GNN parameters obtained under different initializations. Suppose the graph aggregation operator \(\hat{\mathbf{A}}\) has an  effective propagation factor \(\lambda_{\mathrm{eff}}\) (related to its spectral gap) and let
% \[
% N_l \triangleq \min\Bigl\{ \prod_{j=l+1}^{L} \|W_a^{(j)}\|,\,\prod_{j=l+1}^{L} \|W_b^{(j)}\| \Bigr\}.
% \]
% Then,  the loss barrier satisfies
% \begin{equation}
% \label{eq:final-bound}
% \begin{aligned}
%     B(\theta_a,\theta_b) &\le \max_{\lambda \in [0,1]} \Bigg\{ 
%     (1-\lambda) C_L + \lambda\,L_\ell\,\lambda_{\mathrm{eff}}\,\|X\|\,\sum_{l=0}^{L} N_l  \|W_a^{(l)}-W_b^{(l)}\| 
%     \Bigg\}
% \end{aligned}
% \end{equation}
% where \(C_L\) is a constant reflecting the higher-order (curvature) effects in the loss landscape, and \(L_\ell\) is the fixed Lipschitz constant of the loss function .
% \end{theorem}
% \begin{remark}[Implications for Mode Connectivity]
% Theorem~\ref{thm:loss-barrier-GNN} reveals that the loss barrier is directly affected by three main sources of variability in training: differences in initialization, weight norm, and graph property within both structure and feature. In particular:
% \begin{itemize}
%     \item \textbf{Initialization Variability:} The term 
%     \(
%     \|W_a^{(l)}-W_b^{(l)}\|
%     \)
%     quantifies the discrepancies in the network weights at layer \(l\) arising from different initialization.  
%     \item \textbf{Weight Norm:} The term \( N_l \) reflects the gradient propagation trend in deep GNN training, where a larger \( N_l \) may lead to gradient explosion, increasing the loss barrier \( B(\theta_a, \theta_b) \) and affecting training stability. Inter-layer normalization (e.g., BatchNorm, LayerNorm) helps control \( N_l \), mitigate gradient explosion, and stabilize optimization.
%     \item \textbf{Graph Property Contribution:} \(\lambda_{\mathrm{eff}}\) (which depends on the graph aggregation operator \(\hat{\mathbf{A}}\)) as well as the norm of the input features \(\|X\|\). 
% \end{itemize}
% \end{remark}


% Based on this generalization bound, we can deduce a generalization bound in terms of graph properties, and thus explain the phenomenon we observe in Section~\ref{sec: gp}
% \begin{corollary}[Graph Property perspective of Loss barrier]
% \label{cor:loss-barrier-bound-simplified}
% Let \(\theta_a\) and \(\theta_b\) be two sets of GNN parameters obtained under different initializations and/or mini-batch orders. Suppose that the graph aggregation operator \(\hat{\mathbf{A}}\) satisfies
% \(
% \lambda_{\mathrm{eff}} = 1 - \Delta + C_2 \sqrt{\frac{\log n}{d_{\min}}},
% \)
% with the spectral gap
% \(
% \Delta \triangleq 1 - \max_{i\ge 2} |\lambda_i(\mathbb{E}[\hat{\mathbf{A}}])|,
% \)
% Then, the loss barrier satisfies
% \[
% % \boxed{
% B(\theta_a,\theta_b) \le O\Bigl( L_\ell\, \lambda_{\mathrm{eff}}\, \|X\| \Bigr)
% = O\Biggl( L_\ell\|X\|\cdot \left[1 - \Delta + C_2 \sqrt{\frac{\log n}{d_{\min}}}\right]  \Biggr).
% % }
% \]
% \end{corollary}

% \begin{proposition}[Loss Barrier of node classification in CSBM datasets]
% \label{prop:loss-barrier-community}
% Let \(\theta_a\) and \(\theta_b\) be two sets of GNN parameters trained under different conditions. Suppose the graph follows the Community Stochastic Block Model (CSBM) with intra-community and inter-community connection probabilities \(p_{\mathrm{in}}\) and \(p_{\mathrm{out}}\), respectively.  The loss barrier then satisfies the upper bound:
% \begin{equation}
%     \begin{aligned}
% \label{eq:loss-barrier-community}
% B(\theta_a,\theta_b) &\le O\Biggl( \sigma \sqrt{d \log n}\cdot\Bigl[ C_2 \sqrt{\frac{\log n}{d_{min}}} - \frac{1}{C_1}(h(G)-\frac{1}{2})^2\cdot(p_{\mathrm{in}} + p_{\mathrm{out}})  \Bigr] \Biggr).
%     \end{aligned}
% \end{equation}
% Here, \(C_1,C_2\) are constants.
% \end{proposition}

% The theoretical bound aligns well with our empirical observations. Our experiments demonstrate that graphs with stronger homophilic or heterophilic (higher \(h(G)\)) exhibit lower loss barriers, supporting the claim that mode connectivity improves under these conditions. The derived upper bound quantitatively explains this phenomenon by linking the spectral gap \(\Delta\) to key graph properties.

% Furthermore, the presence of the sparsity correction term \((p_{\mathrm{in}}+p_{\mathrm{out}})\) suggests that increasing graph density reduces the loss barrier. This is consistent with our experimental findings that training on denser graphs leads to smoother optimization landscapes. This theoretical insight provides a principled explanation for why well-clustered and well-connected graphs facilitate optimization stability and generalization in GNNs.

% \czk{

\subsection{Why Does Mode Connectivity Occur? A General Bound for GNNs}
\label{sec:theory}

While previous sections empirically investigated mode connectivity in GNNs, a rigorous theoretical understanding of why this phenomenon occurs remains elusive. In this section, we bridge this gap by establishing a theoretical framework that quantifies mode connectivity through a general bound on the loss barrier. 
Our analysis effectively validates previous experimental observations based on our understanding. We begin by formalizing the role of the graph structure in mode connectivity. One key factor is the spectral gap of the expected adjacency matrix, which controls the effective propagation of information in GNNs.
\begin{definition}[Spectral Gap~\cite{chung1997spectral}]
\label{def:spectral-gap}
Let \(\mathbb{E}[\hat{\mathbf{A}}]\) be the expected normalized adjacency matrix with eigenvalues
\[
\lambda_1(\mathbb{E}[\hat{\mathbf{A}}]) \ge \lambda_2(\mathbb{E}[\hat{\mathbf{A}}]) \ge \cdots \ge \lambda_n(\mathbb{E}[\hat{\mathbf{A}}]).
\]
The \emph{spectral gap} is defined as:
\[
\Delta \triangleq 1 - \max_{i\ge 2} |\lambda_i(\mathbb{E}[\hat{\mathbf{A}}])|.
\]
\end{definition}
 A larger spectral gap \(\Delta\) implies faster information propagation across the graph and better separability of node representations, which is crucial for stable optimization~\cite{chung1997spectral,abbe2018community}.

\begin{theorem}[General Bound of the Loss Barrier in \(L\)-Layer GNNs]
\label{thm:loss-barrier-GNN}
Let \(\theta_a\) and \(\theta_b\) be two sets of GNN parameters obtained under different initializations. Suppose the graph aggregation operator \(\hat{\mathbf{A}}\) has an effective propagation factor \(\lambda_{\mathrm{eff}}\) (related to the spectral gap) and let:
\[
N_l \triangleq \min\Bigl\{ \prod_{j=l+1}^{L} \|W_a^{(j)}\|,\,\prod_{j=l+1}^{L} \|W_b^{(j)}\| \Bigr\}.
\]
Then, the loss barrier satisfies:
\begin{equation}
\label{eq:final-bound}
\begin{aligned}
    B(\theta_a,\theta_b) &\le \max_{\lambda \in [0,1]} \Bigg\{ 
    (1-\lambda) C_L + \lambda\,L_\ell\,\lambda_{\mathrm{eff}}\,\|X\|\,\sum_{l=0}^{L} N_l  \|W_a^{(l)}-W_b^{(l)}\| 
    \Bigg\}
\end{aligned}
\end{equation}
where \(C_L\) captures higher-order curvature effects in the loss landscape, and \(L_\ell\) is the Lipschitz constant of the loss function.(Proof in Appendix \ref{the:3.2})
\end{theorem}


\begin{remark}[Implications for Mode Connectivity]
Theorem~\ref{thm:loss-barrier-GNN} formally establishes that the loss barrier is influenced by three fundamental factors:
\begin{itemize}
    \item \textbf{Initialization Variability:} The term 
    \(
    \|W_a^{(l)}-W_b^{(l)}\|
    \)
    quantifies differences in learned parameters at layer \(l\), arising from different initializations. A lower initialization variability leads to smaller barriers, facilitating smoother connectivity.
    \item \textbf{Weight Norm and Propagation:} The term \( N_l \) reflects how gradients propagate through layers. Larger \( N_l \) increases the loss barrier \( B(\theta_a, \theta_b) \), making the optimization landscape sharper. Techniques such as BatchNorm and LayerNorm help regulate \( N_l \), improving stability.~\cite{luo2024classic}
    \item \textbf{Graph Structural Influence:} The effective propagation factor \(\lambda_{\mathrm{eff}}\), which depends on the spectral gap \(\Delta\), controls how node features spread. A smaller \(\lambda_{\mathrm{eff}}\) (caused by a small spectral gap) results in less smooth minima transitions, increasing the loss barrier.
\end{itemize}
\end{remark}


By leveraging Theorem~\ref{thm:loss-barrier-GNN}, we establish a direct connection between the graph structure and mode connectivity.

\begin{corollary}[Graph Property Perspective of the Loss Barrier]
\label{cor:graph-property}
Let \(\theta_a\) and \(\theta_b\) be two sets of GNN parameters obtained under different initializations and/or mini-batch orders. Suppose the graph aggregation operator \(\hat{\mathbf{A}}\) satisfies:
\(
\lambda_{\mathrm{eff}} = 1 - \Delta + C_2 \sqrt{\frac{\log n}{d_{\min}}},
\)
where the spectral gap is:
\(
\Delta \triangleq 1 - \max_{i\ge 2} |\lambda_i(\mathbb{E}[\hat{\mathbf{A}}])|.
\)
Then, the loss barrier satisfies:
\[
B(\theta_a,\theta_b) \le O\Bigl( L_\ell\, \lambda_{\mathrm{eff}}\, \|X\| \Bigr)
= O\Biggl( L_\ell\|X\|\cdot \left[1 - \Delta + C_2 \sqrt{\frac{\log n}{d_{\min}}}\right]  \Biggr).
\]
\end{corollary}

This result suggests that a larger spectral gap \(\Delta\) leads to a smaller loss barrier, facilitating better mode connectivity and generalization.

\begin{proposition}[Loss Barrier in Node Classification on CSBM Datasets]
\label{prop:loss-barrier-CSBM1}
Let \(\theta_a\) and \(\theta_b\) be two sets of GNN parameters trained under different conditions. Suppose the graph follows the Community Stochastic Block Model (CSBM) with intra-class and inter-class connection probabilities \(p_{\mathrm{in}}\) and \(p_{\mathrm{out}}\), respectively. The loss barrier then satisfies the upper bound:
\begin{equation}
    \begin{aligned}
\label{eq:loss-barrier-community}
B(\theta_a,\theta_b) &\le O\Biggl( \sigma \sqrt{d \log n}\cdot\Bigl[ C_2 \sqrt{\frac{\log n}{d_{min}}} - \frac{(h(G)-\frac{1}{2})^2}{C_1}\cdot(p_{\mathrm{in}} + p_{\mathrm{out}})  \Bigr] \Biggr).
    \end{aligned}
\end{equation}
Here, \(C_1,C_2\) are constants.
\end{proposition}

Detailed proof is in Appendix \ref{pro:3.5}.
The theoretical results align well with empirical observations. Experiments show that graphs with strong homophily or heterophily (higher \(h(G)\)) exhibit lower loss barriers, supporting the hypothesis that mode connectivity is enhanced under these conditions. Corollary~\ref{cor:graph-property} provides a quantitative explanation for this by linking the spectral gap to loss landscape smoothness. Furthermore, Proposition~\ref{prop:loss-barrier-CSBM1} suggests that increasing graph density (higher \( p_{\mathrm{in}} + p_{\mathrm{out}} \)) reduces the loss barrier, leading to smoother optimization landscapes. This is empirically validated by observing that denser graphs result in lower barriers and more stable training. These findings establish a principled connection between graph structure and GNN optimization, offering valuable insights into how structural properties influence generalization and mode connectivity.


\section{Implications for the generalization of GNNs}
\label{sec: general}

After thoroughly investigating GNN mode connectivity, and with Observations 2 and 4 highlighting its relationship to GNN generalization behavior across both single and diverse graphs, we now leverage mode connectivity to derive a generalization bound for GNNs and, subsequently, to propose a mode connectivity-based metric for quantifying graph domain discrepancy.


% \czk{transition, relationship between MC and convexity and we study how it influences the generalization behavior of GNNs. We start by giving a generalization bound of GNNs based on loss barriers, and extending it to xxx across groups and across graphs. We showcase the xxx of our theories on UDA. }
\subsection{How Does Mode Connectivity Reflect Generalization Performance?}
Understanding generalization in GNNs remains a fundamental challenge, as their performance depends on both graph topology and node features. Specifically, while previous studies~\cite{keskar2016large,tatro2020optimizing,juneja2022linear} in deep learning suggest that flatter minima lead to better generalization, it remains unclear whether this holds for GNNs due to their reliance on relational structures. To address this, we establish a formal connection between mode connectivity and generalization by analyzing the role of mode connectivity geometry in GNN training. The barrier can quantify the degree of sharpness between minima, where a higher barrier suggests a more isolated minimum with a steeper surrounding manifold.

\begin{theorem}[Generalization Bound via Loss Barrier]  
\label{thm:generalization-barrier}  
 Let \(\theta^T_a,\theta^T_b\) be the model parameter obtained after \(T\) training iterations under different initialization, with \(m\) labeled and \(n-m\) unlabeled data samples . Then, for any \(\delta \in (0,1)\), with probability at least \(1 - \delta\), the generalization gap satisfies  
\begin{equation}
\Delta_{\text{gen}} \leq O \bigg( 8 B(\theta_a, \theta_b)\cdot \frac{n^{\frac{3}{2}}}{m(n-m)} \cdot \log(c(T)) T^\rho \log \frac{1}{\delta} \bigg).
\end{equation}
\end{theorem}
Detailed proof is in Appendix \ref{the:4.1}.
This bound formally establishes that the generalization gap is controlled by the mode connectivity barrier \( B(\theta_a, \theta_b) \), along with dataset-dependent terms and the training dynamics. Several key insights are suggested from this result:
1. The bound directly links the loss barrier to generalization, indicating that a smaller barrier leads to stronger generalization. 2. The dependence on the labeling ratio \( (m+u)^{3/2} / (mu) \) highlights the importance of labeled data in GNN generalization.

% \noindent\textbf{Empirical Validation.} To validate Theorem~\ref{thm:generalization-barrier}, we measure the correlation between the barrier value and the generalization gap. It is important to note that we adopt the barrier of training accuracy here to ensure a fair comparison with validation accuracy. To generate statistically significant and generalizable results, we train the same model (using GCN as the backbone) with different training trajectories across $12$ datasets with varying levels of homophily and supervision. For each dataset, we consider three labeling ratio 0.3-0.4, 0.4-0.6, and 0.6-0.9. As shown in Figure~\ref{fig:lbhpearson}, we observe that the barrier exhibits a positive Pearson correlation with generalization performance across all scenarios. Notably, in heterophilous and weakly supervised settings (the first bar of each bar group)—where traditional training loss fails to reliably predict generalization—the barrier remains robust. This indicates that the sharpness of the loss landscape is directly linked to overfitting, further supporting our theoretical findings.

To validate the theorem, we measure the correlation between the barrier value and the generalization gap. Also, we compare the correlation between the training accuracy barrier and the generalization gap to that between the validation accuracy and the generalization gap, where the latter is one of the most commonly used metrics for measuring overfitting and selecting model checkpoints. As illustrated in Figure~\ref{fig:compare}, where Each data point represents a trained model, we find that the training accuracy barrier demonstrates a stronger correlation with the generalization gap compared to the widely used validation accuracy. This suggests its potential utility as an indicator for selecting model checkpoints and mitigating overfitting. These findings collectively highlight that mode connectivity serves as a fundamental characteristic of GNN generalization, independent of specific architectures or hyperparameters.

% The results reveal a strong positive correlation, particularly in heterophilous and weakly supervised settings, where traditional training loss fails to reliably predict generalization. This suggests that the sharpness of the loss landscape is directly linked to overfitting, further reinforcing our theoretical findings.

% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{fig/Application/lbh_Pearson.png}
%     \caption{Correlation between generalization performance and accuracy barrier. The accuracy barrier positively correlates with generalization performance, even with low labeling ratios or non-homophilous graphs. Each bar represents one labeling ratio.} 
%     \label{fig:lbhpearson}
% \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{fig/Application/lbh1_generalization.png}
    \vspace{-0.1in}
    \caption{Train accuracy barrier presents a stronger correlation to generalization gap compared to the commonly used validation accuracy. $\rho_p$ means Pearson correlation coefficient, $\rho_s$ means Spearman's rank correlation, while $R^2$ means Coefficient of determination. }
    \label{fig:compare}
     \vspace{-0.3in}
\end{figure}


% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{fig/Application/domain_sim.png}
%     \caption{Caption}
%     \label{fig:enter-label}
% \end{figure}
\subsection{Inter-Domain Graph Similarity via Mode Connectivity}
\label{subsec:graph_similarity}

Inspired from {Observation 3}, where the way trained minima are connected in the {parameter space} may implicitly capture {domain-specific properties}, we study how to come up with a mode connectivity-based metric to measure domain discrepancy. Measuring similarity between different {graph domains} is a fundamental problem in transfer learning, where an appropriate similarity metric helps guide domain adaptation. Prior works~\cite{JMLR:v13:gretton12a, lee2019domain} have proposed various {structure-based} and {feature-based} metrics, yet these are often {heuristic and predefined}, lacking validation from the model’s training dynamics. In contrast, mode connectivity can reflect complex training dynamics and simultaneously consider both features and graph structures. We first introduce the metrics we design and then validate its effectiveness. 

{\bf Mode Connectivity Distance.}
\label{subsubsec:mc_distance} Consider two graphs, $G^1$ and $G^2$, trained under the same GNN, leading to minima $\theta^1_a, \theta^1_b$ for $G^1$ and $\theta^2_a, \theta^2_b$ for $G^2$. The {mode connectivity curve} between two minima is 
\(
    \mathcal{L}(\alpha) = \mathcal{L}(\phi(\alpha)), \quad \text{where } \phi(\alpha) = (1 - \alpha) \theta_a + \alpha \theta_b, \quad \alpha \in [0,1].
\)
So we treat $\mathcal{L}(\alpha)$ as a {distribution} over interpolation values $\alpha$, capturing the smoothness of the loss landscape between minima. The discrepancy between two graphs can then be quantified by the \textbf{Wasserstein-1 distance}~\citep{ramdas2017wasserstein} between their respective loss distributions:
\begin{equation}
    d_{\text{MC}}(G^1, G^2) = W_1\big(\mathcal{L}^{G^1}(\alpha), \mathcal{L}^{G^2}(\alpha)\big).
\end{equation}

{\bf Theoretical Justification: Upper Bounding Transferability Gap.}
\label{subsubsec:theory_transfer} We then theoretically show that $d_{\text{MC}}(G^1, G^2)$ provides an {upper bound} on the transferability performance gap between the two domains:
\begin{equation}
    \Delta_{da} \leq C \cdot O\bigg(d_{\text{MC}}(G^1, G^2)\bigg),
\end{equation}
for some constant $C$ dependent on model complexity and domain properties. Crucially, this result holds \textbf{independent of graph size}, indicating that a smaller mode connectivity distance implies {higher transferability}. The proof is shown in Appendix~\ref{the:final}.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{fig/Application/domain.png}
\caption{Correlation between source/target performance gap and our proposed $d_{\text{MC}}(G^1, G^2)$. Cases 1 and 3 depict transfer learning across subgroups, while Cases 2 and 4 illustrate transfer learning across full graphs with different .}
    \label{fig:domainada}
    \vspace{-0.2in}
\end{figure}

{\bf Empirical Verification.}
\label{subsubsec:empirical_results} To empirically validate our theoretical insights, we investigate unsupervised graph domain adaptation~\citep{wu2020unsupervised}. Specifically, we train a GNN model on a source graph and then transfer it to a target graph. We evaluate several common graph domain adaptation methods, including vanilla transfer, DANN~\citep{ganin2016domain}, DANE~\citep{song2020domain}, UDAGCN~\citep{wu2020unsupervised}, JHGDA~\citep{shi2023improving}, and SpecReg~\citep{you2023graph}. We then analyze the correlation between the source/target performance gap and our proposed $d_{\text{MC}}$ metric. To generate statistically significant results for vanilla transfer, we consider transferring between $7$ subgroups of the source and target graphs. For other baseline methods, we directly consider transfer learning on the full graphs. The results are shown in Figure~\ref{fig:domainada}, and  key observations include:
\begin{itemize}[nosep, leftmargin=20pt]
    \item \textbf{Strong Correlation}: $d_{\text{MC}}(G^1, G^2)$ exhibits a \textbf{positive correlation} with the empirical performance gap across all scenarios. 
    \item \textbf{Effect of Domain Alignment}: Applying \textbf{domain alignment} techniques demonstrably lowers $d_{\text{MC}}$, visually demonstrating their effectiveness in enhancing knowledge transfer through the reshaping of the parameter space geometry.
\end{itemize}

These results underscore the {geometric perspective} of domain adaptation and suggest that {mode connectivity analysis} provides a principled approach to quantify and enhance {transfer learning across graph domains}.
