\section{Related Works}
SSL trains a feature extractor $\theta: \mathcal{X} \rightarrow \mathcal{F}$ to map inputs \(x \in \mathcal{X}\) to latent representations \(z \in \mathcal{F}\). Training involves pretext tasks on unlabeled data, while the evaluation is usually conducted with linear probing on downstream tasks.
We focus on \textit{instance discrimination} SSL methods, where the pretext task aligns two augmented views of the same sample in feature space via contrastive loss \cite{chen2020simclr}, additional predictor head \cite{chen2020simsiam}, clustering \cite{caron2021swav} or redundancy reduction \cite{bardes2022vicreg}.\\
In OCL \cite{soutifcormerais2023comprehensive}, the model faces a non-stationary sequence of data $\mathcal{D} = (\mathcal{D}_1, \mathcal{D}_2, \ldots)$ where each $\mathcal{D}_i$ is composed by a very small number of examples (e.g., usually from $1$ to around $10$). We consider class-incremental data streams \cite{rebuffi2017icarl}, where drifts between a given $\mathcal{D}_i$ and $\mathcal{D}_{i+1}$ introduce examples sampled from unseen classes. Interestingly, in OCL drifts do not occur after each $\mathcal{D}_i$ and the model does not know \emph{when} the drift occurs (boundary-free stream). This contrasts with many SSL methods for CL that require to know in advance when a drift is introduced \cite{gomezvilla2022pfr}. In addition, OCL approaches (both with and without SSL) usually employ replay to increase the amount of examples available at each training iteration and to mitigate forgetting \cite{purushwalkam2022minred, soutifcormerais2023comprehensive, yu2023scale, purushwalkam2022minred}.\\
Our approach is replay-free and works without access to boundaries by leveraging the idea of building multiple patches from a single example. This idea is already present in BagSSL \cite{chen2022bagssl} and EMP-SSL \cite{tong2023emp} but it has not been applied to CL, yet.
EMP-SSL loss enforces similarity between each patch latent representation and their average. EMP-SSL also uses the Total Coding Rate $\mathcal{L}_\textit{TCR}$ (Section \ref{sec:method}) to avoid the collapse of latent representations into a single point.

% \JW{Shall we add a paragraph on existing continual learning methods for self-supervised learning CASSLE etc? Concluding that these are not online.}