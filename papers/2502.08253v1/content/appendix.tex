% If you have textual supplementary material
\glsresetall
\renewcommand{\contentsname}%{\centering\MakeUppercase{Appendices}}
\tableofcontents
% Insert the Appendix TOC entry
\addtocontents{toc}{\protect\vspace{10pt}}            % Adjust the vertical space as needed
\addtocontents{toc}{\protect\setcounter{tocdepth}{3}} % Set tocdepth to 1, only display sections
\vspace{.3in}

\newpage

\section{Next-Gen SM kernel and Universal Bochner's Theorem}
\subsection{Bivariate SM kernel}
\label{app:generized_sm_kernel}
The development of the spectral mixture (\MakeUppercase{sm}) kernel is based on the fundamental Bochner's theorem \citep{wilson2013gaussian}, which suggests that any stationary kernel
and its spectral density are Fourier duals. However, the stationarity assumption limits the kernel's learning capacity, especially when dealing with multi-view datasets, where some views may exhibit non-stationary characteristics. To model the non-stationarity, the \MakeUppercase{b}ivariate \MakeUppercase{sm} (\MakeUppercase{bsm}) kernel was introduced in \citep{remes2017non,samo2015generalized}, based on the following generalized Bochner's theorem:
% Specifically, this non-stationary kernel is expressed as \( k(\x_1, \x_2) \in \mathbb{R} \) for inputs \( \x_1, \x_2 \in \mathbb{R}^D \), and it is characterized by its bivariate gaussian mixture spectral density \( p(\w_1, \w_2) \) over frequencies \( \w_1, \w_2 \in \mathbb{R}^D \). The relationship between the kernel and its spectral density is established through a generalized Fourier transform:
\begin{theorem}[Generalized Bochner's Theorem] 
\label{theo:Gen_Bochner_old}
A complex-valued bounded continuous kernel \( k\left(\x_1, \x_2\right) \) on \( \mathbb{R}^D \) is the covariance function of a mean square continuous complex-valued random process on  \( \mathbb{R}^D \) if and only if
\begin{equation}
\label{eq:gen_bochener_old}
\begin{aligned}
& k\left(\x_1, \x_2\right) =  \int
 \exp(i\left(\w_1^{\top} \x_1 - \w_2^{\top} \x_2\right)) m\left(\mathrm{d} \w_1, \mathrm{d} \w_2\right),
\end{aligned}
\end{equation}
where \( m \) is the Lebesgue-Stieltjes measure associated with some positive semi-definite (PSD) function \( S\left(\w_1, \w_2\right) \). 
\end{theorem}

According to Theorem~\ref{theo:Gen_Bochner_old}, one can approximate the function $S(\w_1, \w_2)$, and implement the inverse Fourier transform shown in Eq.~\eqref{eq:gen_bochener_old} to obtain the associated kernel function. The bivariate spectral mixture (\MakeUppercase{bsm}) kernel adopts this concept and approximates $S(\w_1, \w_2)$ as a bivariate Gaussian mixture as follows:
\begin{equation}
\label{eq:spectral of BSM kernel}
% \setlength{\abovedisplayskip}{3pt}
% \setlength{\belowdisplayskip}{3pt}
\begin{aligned}
&S\left(\w_1, \w_2\right) = \sum_{q=1}^Q \alpha_q s_q\left(\w_{q1}, \w_{q2}\right), 
\end{aligned}
\end{equation}
where $s_q$ represents the $q$-th component of a bivariate Gaussian distribution, formally defined as \citep{remes2017non}:
\begin{equation}
\begin{aligned}
s_q\left(\w_{q1}, \w_{q2}\right) = \frac{1}{8}\sum_{\boldsymbol{\mu}_q \in \pm \{\boldsymbol{\mu}_{q1}, \boldsymbol{\mu}_{q2}\}^2}   \mathcal{N} \left[ \left.\binom{\w_{q1}}{\w_{q2}} \right\rvert\,\binom{\boldsymbol{\mu}_{q1}}{\boldsymbol{\mu}_{q2}}, \underbracket{\left(\begin{array}{cc}
\operatorname{diag}(\bm{\sigma}_{q1}^2) & \rho_q \operatorname{diag}(\bm{\sigma}_{q1})\operatorname{diag}(\bm{\sigma}_{q2})  \\
\rho_q \operatorname{diag}(\bm{\sigma}_{q1})\operatorname{diag}(\bm{\sigma}_{q2})  & \operatorname{diag}(\bm{\sigma}_{q2}^2)
\end{array}\right)}_{\triangleq \bm{\Sigma}_q} \right].
\end{aligned}
\end{equation}
Here $\pm \{\boldsymbol{\mu}_{q1}, \boldsymbol{\mu}_{q2}\}^2$ represents:
\begin{align}
\{ (\boldsymbol{\mu}_{q1}, \boldsymbol{\mu}_{q2}), (\boldsymbol{\mu}_{q1}, \boldsymbol{\mu}_{q1}), (\boldsymbol{\mu}_{q2}, \boldsymbol{\mu}_{q2}), (\boldsymbol{\mu}_{q2}, \boldsymbol{\mu}_{q1}), \notag (-\boldsymbol{\mu}_{q1}, -\boldsymbol{\mu}_{q2}), (-\boldsymbol{\mu}_{q1}, -\boldsymbol{\mu}_{q1}), (-\boldsymbol{\mu}_{q2}, -\boldsymbol{\mu}_{q2}), (-\boldsymbol{\mu}_{q2}, -\boldsymbol{\mu}_{q1})\}.
\end{align}

The terms $\bm{\sigma}_{q1}^2$ and $\bm{\sigma}_{q2}^2 \in \mathbb{R}^D$ represent the variances of the $q$-th bivariate Gaussian component, while $\rho_q$ denotes the correlation between $\w_{q1}$ and $\w_{q2}$;  the vectors $\boldsymbol{\mu}_{q1}$ and $\boldsymbol{\mu}_{q2} \in \mathbb{R}^D$ specify the means of the $q$-th Gaussian component. The mixture weight is denoted by $\alpha_q$, and $Q$ is the total number of mixture components. Taking the inverse Fourier transform, we can obtain the following kernel function:
\begin{align}
\label{eq:kernel_BSM_1}
k\left(\x_1, \x_2\right) = \sum_{q=1}^Q \alpha_q \exp\left(-\frac{1}{2} \boldsymbol{\tilde{x}}^{\top} \bm \Sigma_q \boldsymbol{\tilde{x}}\right) \Psi_{q}(\x_1)^\top \Psi_{q}\left(\x_2\right),
\end{align}
where
$$
\Psi_{q}(\x) = 
\begin{bmatrix}
\cos(\boldsymbol{\mu}_{q1}^\top \x) + \cos(\boldsymbol{\mu}_{q2}^\top \x) \\
\sin(\boldsymbol{\mu}_{q1}^\top \x) + \sin(\boldsymbol{\mu}_{q2}^\top \x)
\end{bmatrix} \in \mathbb{R}^2,
$$
% and $\bm \Sigma_q \in \mathbb{R}^{2D \times 2D}$ represents the covariance matrix of the $q$-th bivariate Gaussian component,
and $\tilde{\x} = (\x_1, -\x_2) \in \mathbb{R}^{2D}$ is a stacked vector.

The \MakeUppercase{bsm}  kernel overcomes the stationarity limitation in the \MakeUppercase{sm} kernel.
However, it still has two major issues.
\begin{itemize}
    \item First, the assumption of identical variance of $\w_1$ and $\w_2$ limits the approximation flexibility of the mixture of Gaussian, which in turn diminishes the generalization capacity of the kernel. 
    \item Second, the RFF kernel approximation technique cannot be directly applied, as the closed-form expression of the feature map is hard to derive (see explaination below).
\end{itemize}
The first limitation arises from the requirement that the spectral density must be PSD. To ensure the symmetry of the \MakeUppercase{bsm} kernel function (i.e., $k(\mathbf{x}_1, \mathbf{x}_2) = k(\mathbf{x}_2, \mathbf{x}_1)$), the \MakeUppercase{bsm} kernel assumes that $\bm{\sigma}_{q1} = \bm{\sigma}_{q2}, \, \forall q$.

In addition, we highlight the challenges of deriving a closed-form feature map for RFF when directly applying Theorem \ref{theo:Gen_Bochner_old}. According to Theorem \ref{theo:Gen_Bochner_old}, the kernel can be approximated via \MakeUppercase{mc} as follows:
\begin{equation}
\begin{aligned}
k\left(\x_1, \x_2\right) &  = \frac{1}{4} \int \exp(i(\w_1^{\top} \x_1 \! - \! \w_2^{\top} \x_2))  u(\mathrm{d}\w_1, \mathrm{d}\w_2) \\
&= \frac{1}{4} \mathbb{E}_u \left( \exp(i(\w_1^{\top} \x_1 - \w_2^{\top} \x_2)) \right)\\
&\approx \frac{1}{2 L} \sum_{l=1}^{L/2} \exp(i(\w_1^{(l)\top} \x_1 - \w_2^{(l)\top} \x_2))  \\
&= \frac{1}{2 L} \sum_{l=1}^{L/2} \left(\cos(\w_{1}^{(l)\top} \x_1)\cos(\w_{2}^{(l)\top} \x_2) + \sin(\w_{1}^{(l)\top} \x_1)\sin(\w_{2}^{(l)\top} \x_2) \right).
\end{aligned}
\end{equation}
Since \(\mathbf{w}_1\) and \(\mathbf{w}_2\) are distinct, it is hard to derive a closed-form  RFF approximation for $k(\x_1, \x_2)$. More specifically, it is challenging to explicitly define $\phi_{\w_1}(\cdot), \phi_{\w_2}(\cdot)$, the feature maps of the kernel function, such that 
\begin{equation}
    k(\x_1, \x_2) \approx \phi_{\w_1}(\x_1)^\top \phi_{\w_2}(\x_2) = \phi_{\w_1}(\x_2)^\top \phi_{\w_2}(\x_1)
\end{equation}
Thus, the inversion of the \MakeUppercase{bsm} kernel matrix retains high computational complexity, rendering it unsuitable for multi-view data. 
 % This not only results in reduced computational efficiency but also complicates theoretical derivations. 
 % For instance, it becomes impossible to project the original data into a linear space through the feature map for performing linear regression.

% These two issues limit the performance of the \MakeUppercase{bsm} kernel. 
\begin{remark}
    To enhance the kernel capacity, this paper proposes the Universal Bochnerâ€™s Theorem (Theorem \ref{theo:Uni_Bochner}) and the NG-SM kernel.  The main contribution of Theorem \ref{theo:Uni_Bochner} is that it relaxes the PSD assumption of the spectral density, thus the induced NG-SM kernel can mitigate the constraint of identical spectral variance.
\end{remark}

\begin{remark} 
    To derive a closed-form feature map for any kernel, one potential approach is to decompose the spectral density \( S(\w_1, \w_2) \) into some density functions \( p(\w_1, \w_2) \) \citep{ton2018spatial, samo2015generalized}, such as:
    \begin{equation}
    \begin{aligned}
    \label{eq:S_decompose1}
    S(\w_1, \w_2) = \frac{1}{4}(p(\w_1, \w_2) + p(\w_2, \w_1) + p(\w_1)\delta(\w_2 - \w_1) + p(\w_2)\delta(\w_1 - \w_2)),
    \end{aligned}
    \end{equation}
    where  \( p(\w_1) \) and \( p(\w_2) \) are the marginal distributions of \( p(\w_1, \w_2) \), and \( \delta(x) \) denotes the Dirac delta function. Subsequently, \MakeUppercase{mc} integration can be applied to \( S(\w_1, \w_2) \) to derive the closed-form feature map, see details in Appendix \ref{app:rff_ng_sm}. 
    % However, finding a suitable \( p(\w_1, \w_2) \) for an arbitrary \( S(\w_1, \w_2) \) and directly sampling from \( S(\w_1, \w_2) \) can both be challenging. 
    % Instead, our method starts directly with \( p(\w_1, \w_2) \) and samples from it to obtain RFF via \MakeUppercase{mc} integration, bypassing the decomposition step and simplifying the sampling process. We will demonstrate the application of the RFF technique to the NG-SM kernel using Theorem \ref{theo:Uni_Bochner}, as detailed in Appendix \ref{app:rff_ng_sm}.
\end{remark}

\subsection{Proof of Theorem~\ref{theo:Uni_Bochner}}
\label{app:new_bochner_theorem}


($\Longrightarrow$) 
Suppose there exists a continuous kernel \( k(\x_1, \x_2) \) on \(\mathbb{R}^D\). By the Theorem \ref{theo:Gen_Bochner_old}, this kernel can be represented as:
\[
k(\x_1, \x_2) = \int \exp\left(i(\w_1^{\top} \x_1 - \w_2^{\top} \x_2)\right) \, m(\mathrm{d}\w_1, \mathrm{d}\w_2),
\]

where \( m \) is the Lebesgue-Stieltjes measure associated with some PSD function \( S(\w_1, \w_2) \) of bounded variation.

To ensure that the kernel function is exchangeable and PSD, we design the spectral density \( S(\w_1, \w_2) \) as follows: 
\begin{equation}
\begin{aligned}
\label{eq:S_decompose}
S(\w_1, \w_2) = \frac{1}{4}(p(\w_1, \w_2) + p(\w_2, \w_1) + p(\w_1)\delta(\w_2 - \w_1) + p(\w_2)\delta(\w_1 - \w_2)),
\end{aligned}
\end{equation}
where $\delta(x)$ represents the Dirac delta function, and $p$ is a certain density function that can be decomposed from $S$ as Eq.~\eqref{eq:S_decompose}. Additionally, \( p(\w_1) \) and \( p(\w_2) \) are the marginal distributions of \( p(\w_1, \w_2) \).

The resulting kernel is 
\begin{equation}
% \setlength{\abovedisplayskip}{3pt}
% \setlength{\belowdisplayskip}{3pt}
\begin{aligned}
k\left(\x_1, \x_2\right) = &\frac{1}{4} \left( \int \exp\left(i\left(\w_1^{\top} \x_1 - \w_2^{\top} \x_2\right)\right) p\left(\w_1, \w_2\right) \, \mathrm{d} \w_1 \, \mathrm{d} \w_2 + \int \exp\left(i\left(\w_2^{\top} \x_1 - \w_1^{\top} \x_2\right)\right) p\left(\w_2, \w_1\right) \, \mathrm{d} \w_2 \, \mathrm{d} \w_1  \right. \\
& \! + \! \int \exp\left(i\w_1^{\top}( \x_1\! -\! \x_2)\right) p\left(\w_1\right)\delta(\w_2\! -\! \w_1) \, \mathrm{d} \w_1  \! +\!  \left. \int \exp\left(i\w_2^{\top}(\x_1 \! -\! \x_2)\right) p\left(\w_2\right)\delta(\w_1\! -\! \w_2) \, \mathrm{d} \w_2 \right) \\
= & \frac{1}{4} \left( \int \exp\left(i\left(\w_1^{\top} \x_1 - \w_2^{\top} \x_2\right)\right) u(\mathrm{d}\w_1, \mathrm{d}\w_2)  + \int \exp\left(i\left(\w_2^{\top} \x_1 - \w_1^{\top} \x_2\right)\right) u(\mathrm{d}\w_1, \mathrm{d}\w_2)   \right. \\
& + \left. \int \exp\left(i\left(\w_1^{\top} \x_1 - \w_1^{\top} \x_2\right)\right) u(\mathrm{d}\w_1)  + \int \exp\left(i\left(\w_2^{\top} \x_1 - \w_2^{\top} \x_2\right)\right) u(\mathrm{d}\w_2)  \right),
\end{aligned}
\end{equation}
where \( u \) is the Lebesgue-Stieltjes measure associated with the density function \( p(\w_1, \w_2) \).

Note that we can disregard the $\delta$ functions in the last two terms of the expression, as the integrands in these terms depend solely on $\w_1$ or $\w_2$. Consequently, we can only integrate over the single variable, while setting the other variable to be equal to the one being integrated.

Finally, we can express the kernel $k\left(\x_1, \x_2\right)=$
\begin{equation}
% \setlength{\abovedisplayskip}{3pt}
% \setlength{\belowdisplayskip}{3pt}
\begin{aligned}
\frac{1}{4} \int( \exp\left(i\left(\w_1^{\top} \x_1 \!-\! \w_2^{\top} \x_2\right)\right) \!+\! \exp\left(i\left(\w_2^{\top} \x_1 \!-\! \w_1^{\top} \x_2\right)\right) \!+\! \exp\left(i\w_1^{\top} (\x_1 \!-\! \x_2)\right) \!+\! \exp\left(i\w_2^{\top} (\x_1 \!-\! \x_2)\right)) u(\mathrm{d}\w_1, \mathrm{d}\w_2),
\end{aligned}
\end{equation}
which is the expression shown in Theorem \ref{theo:Uni_Bochner}. 

\vspace{.2in}
($\Longleftarrow$) 
Given the function $k\left(\x_1, \x_2\right) = $
\begin{equation}
% \setlength{\abovedisplayskip}{3pt}
% \setlength{\belowdisplayskip}{3pt}
\begin{aligned}
\frac{1}{4} \int (
 \exp(i\w_1^{\top} \x_1\! -\! i\w_2^{\top} \x_2)\! +\! \exp(i\w_2^{\top} \x_1\! -\! i\w_1^{\top} \x_2)\! +\! \exp(i\w_1^{\top}( \x_1\! - \! \x_2)) \!+\! \exp(i\w_2^{\top} (\x_1\! -\! \x_2)) )
u(\mathrm{d}\w_1, \mathrm{d} \w_2),
\end{aligned}
\end{equation}
% where \( u \) is the Lebesgue-Stieltjes measure associated with a density function \( p(\w_1, \w_2) \), 
we have the condition that:

\begin{equation}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\begin{aligned}
S(\w_1, \w_2) = \frac{1}{4}(p(\w_1, \w_2) + p(\w_2, \w_1) + p(\w_1)\delta(\w_2 - \w_1) + p(\w_2)\delta(\w_1 - \w_2)),
\end{aligned}
\end{equation}

is a PSD function. 
% Here, \( p(\w_1) \) and \( p(\w_2) \) are the marginal distributions of \( p(\w_1, \w_2) \), and \( \delta(x) \) is the Dirac delta function. 
We aim to demonstrate that \(k(\x_1, \x_2)\) is a valid kernel function. 
Specifically, we need to show that \(k\) is both symmetric and PSD.

First, it is straightforward to show that \(k(\x_1, \x_2) = k(\x_2, \x_1)\), confirming that \(k\) is symmetric.

The next step is to establish that \(k\) is PSD. Consider that \(k(\x_1, \x_2) = \)

\begin{equation}
\begin{aligned}
&\  \frac{1}{4} \left( \int \exp(i(\w_1^{\top} \x_1 \!-\! \w_2^{\top} \x_2)) p(\w_1, \w_2)\,\mathrm{d}\w_1\mathrm{d}\w_2 \!+\! \int \exp(i(\w_2^{\top} \x_1 \!-\! \w_1^{\top} \x_2)) p(\w_1, \w_2)\,\mathrm{d}\w_1\mathrm{d}\w_2 \right. \\
&\quad + \left. \int \exp(i(\w_1^{\top} \x_1 \!-\! \w_1^{\top} \x_2)) p(\w_1, \w_2)\,\mathrm{d}\w_1\mathrm{d}\w_2 \!+\! \int \exp(i(\w_2^{\top} \x_1 \!-\! \w_2^{\top} \x_2)) p(\w_1, \w_2)\,\mathrm{d}\w_1\mathrm{d}\w_2 \right) \\
& = \frac{1}{4} \left( \int \exp(i(\w_1^{\top} \x_1 \!-\! \w_2^{\top} \x_2)) p(\w_1, \w_2)\,\mathrm{d}\w_1\mathrm{d}\w_2 + \int \exp(i(\w_1^{\top} \x_1 \!-\! \w_2^{\top} \x_2)) p(\w_2, \w_1)\,\mathrm{d}\w_1\mathrm{d}\w_2 \right. \\
& \quad + \left. \int \exp(i(\w_1^{\top} \x_1 \!-\! \w_2^{\top} \x_2)) p(\w_1)\delta(\w_2 \!-\! \w_1)\,\mathrm{d}\w_1\mathrm{d}\w_2 + \int \exp(i(\w_1^{\top} \x_1 \!-\! \w_2^{\top} \x_2)) p(\w_2)\delta(\w_1 \!-\! \w_2)\,\mathrm{d}\w_1\mathrm{d}\w_2 \right) \\
&= \int \exp(i(\w_1^{\top} \x_1 \!-\! \w_2^{\top} \x_2)) \frac{1}{4}(p(\w_1, \w_2) + p(\w_2, \w_1) + p(\w_1)\delta(\w_2 - \w_1) + p(\w_2)\delta(\w_1 - \w_2))\,\mathrm{d}\w_1\mathrm{d}\w_2\\
&= \int \exp(i(\w_1^{\top} \x_1 \!-\! \w_2^{\top} \x_2)) m(\mathrm{d}\w_1\mathrm{d}\w_2),
\end{aligned}
\end{equation}
where \( m \) is the Lebesgue-Stieltjes measure associated with the PSD density function \( S\left(\w_1, \w_2\right) \). Thus, by Theorem \ref{theo:Gen_Bochner_old}, $k(\w_1,\w_2)$ is PSD. 
%Thus, \(k(\x_1, \x_2)\) is a valid kernel function. 
\qed

\subsection{Derivation of Next-Gen SM kernel}
\label{app:ng-sm-kernel}
The \MakeUppercase{bsm} kernel based on Theorem \ref{theo:Gen_Bochner_old} is constrained by the requirement of identical variances for \(\w_1\) and \(\w_2\), and is incompatible with the RFF approximation technique. In this section, we derive the NG-SM kernel based on Theorem \ref{theo:Uni_Bochner}, which effectively resolves these limitations.

The spectral density of the NG-SM kernel is designed as:
\begin{equation}
    % \setlength{\abovedisplayskip}{2.5pt}
    % \setlength{\belowdisplayskip}{2.5pt}
    p_{\text{ngsm}} \left(\mathbf{w}_1, \mathbf{w}_2\right)=\sum_{q=1}^{Q} \alpha_q s_q\left(\mathbf{w}_1, \mathbf{w}_2\right) ,
    \label{eq:ngsm_spectral}
\end{equation}
with each 
\begin{equation}
s_q\left(\mathbf{w}_1, \mathbf{w}_2\right)=
\begin{aligned}
  & \frac{1}{2} ~  \mathcal{N}\left(\left.\binom{\mathbf{w}_{1}}{\mathbf{w}_{2}} \right\rvert\,\binom{\boldsymbol{\mu}_{q1}}{\boldsymbol{\mu}_{q2}}, \begin{bmatrix}
      \bm \Sigma_{1}\!&\!\bm \Sigma_{\text{c}}^{\top} \\
    \bm \Sigma_{\text{c}}\!&\!\bm \Sigma_2
  \end{bmatrix} \right) +  \frac{1}{2} ~ \mathcal{N}\left(\left.\binom{-\mathbf{w}_{1}}{-\mathbf{w}_{2}} \right\rvert\,\binom{\boldsymbol{\mu}_{q1}}{\boldsymbol{\mu}_{q2}}, \begin{bmatrix}
      \bm \Sigma_{1}\!&\!\bm \Sigma_{\text{c}}^{\top} \\
    \bm \Sigma_{\text{c}}\!&\!\bm \Sigma_2
  \end{bmatrix} \right).
\end{aligned}
\end{equation}
We simplify the notation by omitting the index \(q\) from the sub-matrices \(\bm \Sigma_1 = \operatorname{diag}(\bm{\sigma}_{q1}^2)\), \(\bm \Sigma_2 = \operatorname{diag}(\bm{\sigma}_{q2}^2)\), and \(\bm \Sigma_{\mathrm{c}} = \rho_q \operatorname{diag}(\bm{\sigma}_{q1}) \operatorname{diag}(\bm{\sigma}_{q2})\), where \(\bm{\sigma}_{q1}^2, \bm{\sigma}_{q2}^2 \in \mathbb{R}^D\) and \(\rho_q\) represents the correlation between \(\w_1\) and \(\w_2\). These terms together define the covariance matrix for the \(q\)-th bivariate Gaussian component. Additionally, the vectors \(\bm{\mu}_{q1}\) and \(\bm{\mu}_{q2} \in \mathbb{R}^D\) serve as the mean of the \(q\)-th bivariate Gaussian component. 

Relying on Theorem \ref{theo:Uni_Bochner}, we derive the NG-SM kernel as following:

% We define the spectral density \( S(w_1, w_2) \) as a mixture of \( m \) bivariate components.
% \begin{equation}
% S\left(w_1, w_2\right)=\sum_{i=1}^m \alpha_i s_i\left(w_{i1}, w_{i2}\right)
% \end{equation}
% where $\alpha_i$ is the weight of bivariate components.To ensure that the kernel function is exchangeable, it must satisfy \( s_i(w_1, w_2) = s_i(w_2, w_1) \) and have sufficient diagonal components \( s_i(w_1, w_1) \) and \( s_i(w_2, w_2) \). Additionally, to obtain a real-valued kernel, symmetry with respect to negative frequencies is required, i.e., \( s_i(w_1, w_2) = s_i(-w_1, -w_2) \). Therefore, We define the spectral density components \( s_i(w_1, w_2) \) as a mixture of 8 bivariate Gaussian components.
% \begin{equation}
% \begin{aligned}
% s_i\left(w_{i1}, w_{i2}\right)= \frac{1}{8}&\Bigg[ \mathcal{N}\left(\left.\binom{w_{i1,1}}{w_{i2,1}} \right\rvert\,\binom{u_{i1}}{u_{i2}},\left[\begin{array}{cc}
% \sigma_{i1}^2 & \rho_i \sigma_{i1} \sigma_{i2} \\
% \rho_i \sigma_{i1} \sigma_{i2} & \sigma_{i2}^2
% \end{array}\right]\right) 
% + \mathcal{N}\left(\left.\binom{w_{i1,2}}{w_{i2,2}}\right\rvert\,\binom{u_{i2}}{u_{i1}},\left[\begin{array}{cc}
% \sigma_{i2}^2 & \rho_i \sigma_{i1} \sigma_{i2} \\
% \rho_i \sigma_{i1} \sigma_{i2} & \sigma_{i1}^2
% \end{array}\right]\right) \\
% & + \mathcal{N}\left(\left.\binom{w_{i1,3}}{w_{i2,3}} \right\rvert\,\binom{u_{i1}}{u_{i1}},\left[\begin{array}{cc}
% \sigma_{i1}^2 & \sigma_{i1}^2 \\
% \sigma_{i1}^2 & \sigma_{i1}^2
% \end{array}\right]\right) 
% + \mathcal{N}\left(\left.\binom{w_{i1,4}}{w_{i2,4}} \right\rvert\,\binom{u_{i2}}{u_{i2}},\left[\begin{array}{cc}
% \sigma_{i2}^2 & \sigma_{i2}^2 \\
% \sigma_{i2}^2 & \sigma_{i2}^2
% \end{array}\right]\right)  \\
% & + \mathcal{N}\left(\left.\binom{w_{i1,1}}{w_{i2,1}} \right\rvert\,\binom{-u_{i1}}{-u_{i2}},\left[\begin{array}{cc}
% \sigma_{i1}^2 & \rho_i \sigma_{i1} \sigma_{i2} \\
% \rho_i \sigma_{i1} \sigma_{i2} & \sigma_{i2}^2
% \end{array}\right]\right) 
% + \mathcal{N}\left(\left.\binom{w_{i1,2}}{w_{i2,2}}\right\rvert\,\binom{-u_{i2}}{-u_{i1}},\left[\begin{array}{cc}
% \sigma_{i2}^2 & \rho_i \sigma_{i1} \sigma_{i2} \\
% \rho_i \sigma_{i1} \sigma_{i2} & \sigma_{i1}^2
% \end{array}\right]\right) \\
% & + \mathcal{N}\left(\left.\binom{w_{i1,3}}{w_{i2,3}} \right\rvert\,\binom{-u_{i1}}{-u_{i1}},\left[\begin{array}{cc}
% \sigma_{i1}^2 & \sigma_{i1}^2 \\
% \sigma_{i1}^2 & \sigma_{i1}^2
% \end{array}\right]\right) 
% + \mathcal{N}\left(\left.\binom{w_{i1,4}}{w_{i2,4}} \right\rvert\,\binom{-u_{i2}}{-u_{i2}},\left[\begin{array}{cc}
% \sigma_{i2}^2 & \sigma_{i2}^2 \\
% \sigma_{i2}^2 & \sigma_{i2}^2
% \end{array}\right]\right) \Bigg] \\
% = & \frac{1}{8} \sum_{j=1}^8 \mathcal{N}\left(\left.\binom{w_{i1,j}}{w_{i2,j}} \right\rvert\, \mu_j, \Sigma_j\right)
% \end{aligned}
% \end{equation}
% with parameterization involving the correlation \(\rho_i\), means \(\mu_{i1}\) and \(\mu_{i2}\), and variances \(\sigma_{i1}^2\) and \(\sigma_{i2}^2\).
%Next, we derive the closed-form expression for the generalized Fourier transform by exploiting Gaussian integral identities.
\begin{equation}
\begin{aligned}
k_{\text{ngsm}}\left(\x_1, \x_2\right) = &\frac{1}{4} \int p_{\text{ngsm}} \left(\mathbf{w}_1, \mathbf{w}_2\right) \bigg(
\exp(i\w_1^{\top} \x_1 - i\w_2^{\top} \x_2) + \exp(i\w_2^{\top} \x_1 - i\w_1^{\top} \x_2) \nonumber \\
&\quad + \exp(i\w_1^{\top} \x_1 - i\w_1^{\top} \x_2) + \exp(i\w_2^{\top} \x_1 - i\w_2^{\top} \x_2) \bigg)
\mathrm{d}\w_1 \mathrm{d}\w_2 \\
= &\frac{1}{4} \sum_{q=1}^Q \alpha_q \int \frac{1}{2}(\Phi_q(\w_1,\w_2) + \Phi_q(-\w_1,-\w_2)) \bigg(
\exp(i\w_1^{\top} \x_1 - i\w_2^{\top} \x_2)  \nonumber\\
&\quad \!+\! \exp(i\w_2^{\top} \x_1 \!-\! i\w_1^{\top} \x_2) \!+\! \exp(i\w_1^{\top} \x_1 \!-\! i\w_1^{\top} \x_2) \!+\! \exp(i\w_2^{\top} \x_1 \!-\! i\w_2^{\top} \x_2) \bigg)
\mathrm{d}\w_1 \mathrm{d}\w_2,
\end{aligned}
\end{equation}
where 
\begin{equation}
\begin{aligned}
\Phi_q(\w_1,\w_2) = \mathcal{N}\left(\left.\binom{\mathbf{w}_{1}}{\mathbf{w}_{2}} \right\rvert\,\binom{\boldsymbol{\mu}_{q1}}{\boldsymbol{\mu}_{q2}}, \begin{bmatrix}
      \bm \Sigma_{1}\!&\!\bm \Sigma_{\text{c}}^{\top} \\
    \bm \Sigma_{\text{c}}\!&\!\bm \Sigma_2
  \end{bmatrix} \right).
\end{aligned}
\end{equation}
We focus solely on the real part of the kernel function. Since the real part of the integrand is a cosine function, and both $\Phi_q$ and the cosine function are even functions, we can therefore simplify the expression as follows.
\begin{equation}
\begin{aligned}
= &\frac{1}{4} \sum_{q=1}^Q \alpha_q \int \Phi_q(\w_1,\w_2)  \bigg(\exp(i\w_1^{\top} \x_1 - i\w_2^{\top} \x_2) + \exp(i\w_2^{\top} \x_1 - i\w_1^{\top} \x_2) \\
 &\quad\quad\quad\quad+ \exp(i\w_1^{\top} \x_1 - i\w_1^{\top} \x_2) + \exp(i\w_2^{\top} \x_1 - i\w_2^{\top} \x_2) \bigg)
\mathrm{d}\w_1 \mathrm{d}\w_2 \\
= &\frac{1}{4} \sum_{q=1}^Q \alpha_q \bigg(\underbracket{\int \Phi_q(\w_1,\w_2)\exp(i\w_1^{\top} \x_1 \!-\! i\w_2^{\top} \x_2)\mathrm{d}\w_1 \mathrm{d} \w_2}_{\text{Term~(1)}}
\!+\! \underbracket{\int \Phi_q(\w_2,\w_1)\exp(i\w_2^{\top} \x_1 \!-\! i\w_1^{\top} \x_2)\mathrm{d}\w_1 \mathrm{d}\w_2}_{\text{Term~(2)}} \\  &
\quad\quad\quad\quad +\underbracket{\int \Phi_q(\w_1)\exp(i\w_1^{\top} \x_1 \!-\! i\w_1^{\top} \x_2)\mathrm{d}\w_1}_{\text{Term~(3)}}
+\underbracket{\int \Phi_q(\w_2)\exp(i\w_2^{\top} \x_1 \!-\! i\w_2^{\top} \x_2) \mathrm{d} \w_2}_{\text{Term~(4)}}\bigg),
\end{aligned}
\end{equation}
where $\Phi_q(\w_1)$ and $\Phi_q(\w_2)$ are marginal distributions of $\Phi_q(\w_1,\w_2)$. Next, we will derive the closed forms for each term. First, Term (1) is given by:
\begin{equation}
\begin{aligned}
\text{Term (1)} = & \int \mathcal{N}\left(\w \mid \bm{\mu}_{q}, \bm{\Sigma}_q\right) e^{ \w^{\top}\tilde{\x}} \mathrm{d} \w \\
= & \frac{1}{(2 \pi)^2 |\bm{\Sigma}_q|} \int \exp \left(-\frac{1}{2}\left(\w - \bm{\mu}_{q} \right)^{\top} \bm{\Sigma}_q^{-1}\left(\w-\bm{\mu}_{q} \right)+ \w^{\top} \tilde{\x}\right) \mathrm{d} \w \\
= & 
\frac{1}{(2 \pi)^2 |\bm{\Sigma}_q|} \int \exp \left(-\frac{1}{2} \mathbf{w}^{\top} \bm{\Sigma}_q^{-1} \mathbf{w}+\mathbf{w}^{\top}\left(\tilde{\x} + \bm{\Sigma}_q^{-1} \bm{\mu}_{q}\right)  -\frac{1}{2} \bm{\mu}_{q}^{\top} \bm{\Sigma}_q^{-1} \bm{\mu}_{q}\right) \mathrm{d} \mathbf{w}\\
=& \exp \left(\frac{1}{2}\left(\tilde{\x}+\bm{\Sigma}_q^{-1} \bm{\mu}_{q}^{\top}\right)^{\top} \bm{\Sigma}_q \left(\tilde{\x}+\bm{\Sigma}_q^{-1} \bm{\mu}_{q}^{\top}\right)\right) \exp \left(-\frac{1}{2} \bm{\mu}_{q}^{\top} \bm{\Sigma}_q^{-1} \bm{\mu}_{q}\right)\\
=&\exp \left(\frac{1}{2} \tilde{\x}^{\top} \bm{\Sigma}_q \tilde{\x} + \bm{\mu}_{q}^{\top} \tilde{\x}\right),
\end{aligned}
\end{equation}
where we defined $\tilde{\x}=\left(i\x_1,-i\x_2\right)$ and $\mathbf{w}=\left(\w_{1}, \w_{2}\right)$. In addition, $\bm{\mu}_{q} = (\bm{\mu}_{q1},\bm{\mu}_{q2})$ and 
\[
\bm{\Sigma}_q = \begin{bmatrix}
    \bm{\Sigma}_1 & \bm{\Sigma}_{\text{c}}^{\top} \\
    \bm{\Sigma}_{\text{c}} & \bm{\Sigma}_2
\end{bmatrix}.
\]
The first term of the kernel mixture is then given by:
\begin{equation}
\begin{aligned}
 \exp \left(\frac{1}{2} \tilde{\x}^{\top} \bm{\Sigma}_q \tilde{\x} + \bm{\mu}_q^{\top} \tilde{\x}\right) 
= & \exp \left(-\frac{1}{2}\left(\x_1^\top \bm{\Sigma}_1 \x_1 - 2 \rho_q \x_1^\top   \bm{\Sigma}_c \x_2+ \x_2^\top \bm{\Sigma}_2 \x_2\right)\right) \exp \left(i\left(\boldsymbol{\mu}_{q1}^\top  \x_1- \boldsymbol{\mu}_{q2}^\top \x_2 \right)\right) \\
= &  \exp \left(-\frac{1}{2}\left(\x_1^\top  \bm{\Sigma}_1 \x_1 - 2 \rho_q \x_1^\top   \bm{\Sigma}_c \x_2 + \x_2^\top  \bm{\Sigma}_2 \x_2\right)\right) \cos \left(\boldsymbol{\mu}_{q1}^\top  \x_1 - \boldsymbol{\mu}_{q2}^\top  \x_2\right).
\end{aligned}
\end{equation}
By swapping \(\x_1\) and \(\x_2\) in Term (1), the closed form of Term (2) can be easily obtained as below:
\begin{equation}
\begin{aligned}
\text{Term (2)} = \exp \left(-\frac{1}{2}\left(\x_2 \bm{\Sigma}_1 \x_2^\top - 2 \rho_q \x_1  \bm{\Sigma}_c \x_2^\top + \x_1 \bm{\Sigma}_2 \x_1^\top\right)\right) \cos \left(\boldsymbol{\mu}_{q1} \x_2^\top - \boldsymbol{\mu}_{q2} \x_1^\top\right).
\end{aligned}
\end{equation}
Term (3) of the kernel is then given by:
\begin{equation}
\begin{aligned}
\text{Term (3)} = & \int \mathcal{N}\left(\w_1 \mid \boldsymbol{\mu}_{q1}, \bm{\Sigma}_1\right) \exp(i\w_1^{\top}(\x_1-\x_2) ) \mathrm{d} \w_1 \\
= & \frac{1}{(2 \pi)^2 |\bm{\Sigma}_1|} \int \exp \left(-\frac{1}{2}\left(\w_1 - \boldsymbol{\mu}_{q1} \right)^{\top}\bm{\Sigma}_1^{-1}\left(\w_1-\boldsymbol{\mu}_{q1} \right)  + i\w_1^{\top} (\x_1-\x_2) \right) \mathrm{d} \w_1 \\
= & \frac{1}{(2 \pi)^2 |\bm{\Sigma}_1|} \int \exp \left(-\frac{1}{2} \w_1^{\top} \bm{\Sigma}_1^{-1} \w_1 + \w_1^{\top}\left(i(\x_1-\x_2) + \bm{\Sigma}_1^{-1} \boldsymbol{\mu}_{q1}\right)  - \frac{1}{2} \boldsymbol{\mu}_{q1}^{\top}\bm{\Sigma}_1^{-1} \boldsymbol{\mu}_{q1}\right)  \mathrm{d} \w_1 \\
= & \exp \left(\frac{1}{2} \left(i(\x_1-\x_2) + \bm{\Sigma}_1^{-1} \boldsymbol{\mu}_{q1}\right)^{\top}\bm{\Sigma}_1 \left(i(\x_1-\x_2) + \bm{\Sigma}_1^{-1} \boldsymbol{\mu}_{q1}\right)\right)  \exp \left(-\frac{1}{2} \boldsymbol{\mu}_{q1}^{\top} \bm{\Sigma}_1^{-1} \boldsymbol{\mu}_{q1}\right) \\
= & \exp \left(\frac{1}{2} i(\x_1-\x_2)^{\top}\bm{\Sigma}_1 i(\x_1-\x_2) + \boldsymbol{\mu}_{q1}^{\top} i(\x_1-\x_2) \right) \\
= & \exp \left(-\frac{1}{2} \left( \x_1-\x_2)^{\top} \bm{\Sigma}_1 (\x_1-\x_2) \right) \right) \exp \left( i\boldsymbol{\mu}_{q1}^{\top} (\x_1-\x_2) \right) \\
= & \exp \left(-\frac{1}{2} \left( \x_1-\x_2)^{\top}  \bm{\Sigma}_1 (\x_1-\x_2)\right) \right) \cos \left( \boldsymbol{\mu}_{q1}^{\top} (\x_1 -  \x_2 )\right).
\end{aligned}
\end{equation}
The 4'th term of the kernel can be derived in a manner similar to the 3'rd term.
\begin{equation}
\begin{aligned}
\text{Term (4)} =  \exp \left(-\frac{1}{2} \left( \x_1-\x_2)^{\top} \bm{\Sigma}_2 (\x_1-\x_2) \right) \right) \cos \left( \boldsymbol{\mu}_{q2}^{\top} (\x_1 -  \x_2 )\right).
\end{aligned}
\end{equation}
Thus, NG-SM kernel takes the form:
\begin{equation}
\label{eq:ng-sm-kernel}
\begin{aligned}
k(\x_1, \x_2) = &\frac{1}{4} \sum_{q = 1}^{Q} \alpha_q \left[ \exp \left(-\frac{1}{2}\left(\x_1^\top \bm{\Sigma}_1 \x_1 - 2 \rho_q \x_1^\top  \bm{\Sigma}_c \x_2 + \x_2^\top \bm{\Sigma}_2 \x_2\right)\right) \cos \left(\boldsymbol{\mu}_{q1}^\top \x_1 - \boldsymbol{\mu}_{q2}^\top \x_2\right) \right.\\
&+ \exp \left(-\frac{1}{2}\left(\x_2^\top \bm{\Sigma}_1 \x_2 - 2 \rho_q \x_1^\top  \bm{\Sigma}_c \x_2 + \x_1^\top \bm{\Sigma}_2 \x_1\right)\right) \cos \left(\boldsymbol{\mu}_{q1}^\top \x_2 - \boldsymbol{\mu}_{q2}^\top \x_1\right)\\ 
&+ \exp \left(-\frac{1}{2} \left( \x_1-\x_2)^\top \bm{\Sigma}_1 (\x_1-\x_2)\right) \right) \cos \left( \boldsymbol{\mu}_{q1}^\top (\x_1 -  \x_2 )\right) \\
&+  \exp \left(-\frac{1}{2} \left( \x_1-\x_2)^\top \bm{\Sigma}_2 (\x_1-\x_2)\right) \right) \cos \left( \boldsymbol{\mu}_{q2}^\top (\x_1 -  \x_2 )\right) \left] \right..
\end{aligned}
\end{equation}


\section{Auto-differentiable Next-Gen SM Kernel using RFF Approximation}
\subsection{Random Fourier feature for Next-Gen SM Kernel}
\label{app:rff_ng_sm}

% We will present a direct derivation of the RFF based on our proposed Theorem \ref{theo:Uni_Bochner} to approximate any kernel, 

Our proposed Theorem \ref{theo:Uni_Bochner} establishes the following duality: 
$k\left(\x_1, \x_2\right)$
\begin{equation}
\begin{aligned}
&= \frac{1}{4} \mathbb{E}_u \left( \exp(i(\w_1^{\top} \x_1 \!-\! \w_2^{\top} \x_2)) \!+\! \exp(i(\w_2^{\top} \x_1 \!-\! \w_1^{\top} \x_2))  \!+\! \exp(i(\w_1^{\top} \x_1 \!- \!\w_1^{\top} \x_2)) \!+\! \exp(i(\w_2^{\top} \x_1 \!-\! \w_2^{\top} \x_2)) \right). \nonumber
\end{aligned}
\end{equation}

By estimating this expectation with the \MakeUppercase{mc} estimator using spectral points $\{\mathbf{w}_1^{(l)}; \w_2^{(l)} \}_{l=1}^{L/2}$ sampled from $p(\w_1, \w_2)$, we can drive 
\begin{equation}
\begin{aligned}
k\left(\x_1, \x_2\right) &\approx \frac{1}{2 L} \sum_{l=1}^{L/2} \left( \exp(i(\w_1^{(l)\top} \x_1 - \w_2^{(l)\top} \x_2)) + \exp(i(\w_2^{(l)\top} \x_1 - \w_1^{(l)\top} \x_2)) \right. \\
& \quad \left. + \exp(i(\w_1^{(l)\top} \x_1 - \w_1^{(l)\top} \x_2)) + \exp(i(\w_2^{(l)\top} \x_1 - \w_2^{(l)\top} \x_2)) \right) \\
&= \frac{1}{2 L} \sum_{l=1}^{L/2} \left( \cos(\w_{1}^{(l)\top} \x_1)\cos(\w_{1}^{(l)\top} \x_2) + \cos(\w_{1}^{(l)\top} \x_1)\cos(\w_{2}^{(l)\top} \x_2) \right.\\
&\quad + \cos(\w_{2}^{(l)\top} \x_1)\cos(\w_{1}^{(l)\top} \x_2) + \cos(\w_{2}^{(l)\top} \x_1)\cos(\w_{2}^{(l)\top} \x_2) \\
&\quad + \sin(\w_{1}^{(l)\top} \x_1)\sin(\w_{1}^{(l)\top} \x_2) + \sin(\w_{1}^{(l)\top} \x_1)\sin(\w_{2}^{(l)\top} \x_2) \\
&\quad + \sin(\w_{2}^{(l)\top} \x_1)\sin(\w_{1}^{(l)\top} \x_2) + \sin(\w_{2}^{(l)\top} \x_1)\sin(\w_{2}^{(l)\top} \x_2) \bigg) \\
% &= \sum_{l=1}^{L/2} \phi^{(l)}\left(\x_1\right)^{\top} \phi^{(l)}\left(\x_2\right),
&= \phi(\x_1)^{\top} \phi(\x_2) 
\end{aligned}
\end{equation}
where 
% \begin{equation}
% \begin{aligned}
% \phi^{(l)}\left(\x\right)=\sqrt{\frac{1}{2L}}\!\begin{bmatrix}
%     \cos \left( \mathbf{w}_{1}^{(l)\top}  \x \right)\!+\!\cos \left(  \mathbf{w}_{2}^{(l)\top} \x \right) \\
%     \sin \left( \mathbf{w}_{1}^{(l)\top}\x  \right)\!+\!\sin \left(  \mathbf{w}_{2}^{(l)\top} \x \right) 
%     \end{bmatrix}.
% \end{aligned}
% \end{equation}
% In addition, $\{\mathbf{w}_1^{(l)}; \w_2^{(l)} \}_{l=1}^{L/2}$, and are independent and identically distributed (i.i.d) random vectors drawn from the spectral density  $p(\w_1, \w_2)$. Furthermore, we can concatenate $\phi^{(l)}(\mathbf{x})$ for $l = 1, \dots, L/2$ column-wise.
\begin{equation}
\begin{aligned}
\phi\left(\x\right)=\sqrt{\frac{1}{2L}}\!\begin{bmatrix}
    \cos \left( \mathbf{w}_{1}^{(1:L/2)\top}  \x \right)\!+\!\cos \left(  \mathbf{w}_{2}^{(1:L/2)\top} \x\right) \\
    \sin \left( \mathbf{w}_{1}^{(1:L/2)\top}\x  \right)\!+\!\sin \left(  \mathbf{w}_{2}^{(1:L/2)\top} \x \right)  
    \end{bmatrix}\!\in\!\mathbb{R}^{L}.
\end{aligned}
\end{equation}
Here the superscript $1:L/2$ indicates that the cosine plus cosine or sine plus sine function is repeated $L/2$ times, with each element corresponding to the one entry of \( \{\w^{(l)}_1; \w^{(l)}_2 \}_{l=1}^{L/2} \). If we specify the spectral density as \(p_{\text{ngsm}}(\w_1, \w_2)\) (Eq.~\eqref{eq:ngsm_spectral}), the estimator of the kernel \(k_{\text{ngsm}}(\mathbf{x}_1, \mathbf{x}_2)\) can be formulated as:
\begin{equation}
% \setlength{\abovedisplayskip}{4.5pt}
% \setlength{\belowdisplayskip}{4.5pt}
k_{\text{ngsm}}(\x_1, \x_2) \approx \phi(\x_1)^\top \phi(\x_2),    
\end{equation}
where random features $\phi(\x_1)$ and $\phi(\x_2)$ are constructed using spectral points sampled from \(p_{\text{ngsm}}(\w_1, \w_2)\). 

\subsection{ELBO Derivation and Evaluation}
\label{app:ELBO_deriviations}

The term (a) of \MakeUppercase{elbo} is handled numerically with \MakeUppercase{mc} estimation as below:
\begin{subequations} \label{eq:evaluation_term_1}
% \setlength{\abovedisplayskip}{3.5pt}
% \setlength{\belowdisplayskip}{3.4pt}
\begin{align}
   \text{(a)} & =\sum_{m=1}^{M_v} \mathbb{E}_{q(\vx,\vw)} \left[ \log p(\y_{:,m}^v \vert \vx, \vw^v) \right] \\
    & \approx \sum_{m=1}^{M_v}   \frac{1}{{I}}\sum_{i=1}^{I} \log \mathcal{N}(\y_{:,m}^v \vert \bm{0}, \tilde{{\vk}}_{\mathrm{ngsm}}^{v(i)} + \sigma_v^2 \mathbf{I}_N),
\end{align}
\end{subequations}
where ${I}$ denotes the number of \MakeUppercase{mc} samples drawn from $q(\vx,\vw)$. Additionally, $\tilde{{\vk}}_{\mathrm{ngsm}}^{v(i)} = (\bm \Phi_{x}^{v} \bm \Phi_{x}^{v \top})^{(i)}$ is the NG-SM kernel gram matrix approximation, where $\Phi_{x}^{v} \in \mathbb{R}^{N \times L}$.

The term (b) of \MakeUppercase{elbo} can be evaluated analytically due to the Gaussian nature of the distributions. More specific, we have 
\begin{subequations} \label{eq:evaluation_term_2}
\begin{align}
   \text{(b)} & =  \sum_{n=1}^N\operatorname{KL}(q(\x_n) \| p(\x_n)) \\
    & =  \frac{1}{2} \sum_{n=1}^{N} \Big[ \operatorname{tr}(\mathbf{S}_{n}) + \boldsymbol{\mu}_n^{\top} \boldsymbol{\mu}_n -\log |\mathbf{S}_{n}| - D  \Big],
\end{align}
\end{subequations}
where \(D\) represents the dimensionality of \(\mathbf{x}_n\), and \(\mathbf{S}_n\) is commonly assumed to be a diagonal matrix. Consequently, the \MakeUppercase{elbo} can be expressed as follows:
 % \mathbb{E}_{q(\vx, \vw)} \! \left[\log \frac{p(\vx) \prod_{v=1}^{V} p(\vw^v) p(\vy^v \vert \vx, \vw^v)}{q(\vx)\prod_{v=1}^{V} p(\vw^v)} \right] \\  
 %    & = \sum_{v=1}^V \underbracket{\mathbb{E}_{q(\cdot, \cdot)} \left[ \log p(\vy^v \vert \vx,\!\vw^v) \right]}_{\text{(a): reconstruction}}\!-\!\underbracket{ \operatorname{KL}(q(\vx) \| p(\vx))}_{\text{(b): regularization}}\!\!\! 

\begin{equation}
    \begin{aligned}
            \MakeUppercase{elbo} & = \mathbb{E}_{q(\vx,\vw)} \left[ \frac{p(\vy, \vx ; \mathbf{W})}{q(\vx,\vw)}\right] \\
            & =  \mathbb{E}_{q(\vx, \vw)} \! \left[\log \frac{p(\vx) \prod_{v=1}^{V} p(\vw^v) p(\vy^v \vert \vx, \vw^v)}{q(\vx)\prod_{v=1}^{V} p(\vw^v)} \right] \nonumber \\
            & = \sum_{v=1}^V \underbracket{\mathbb{E}_{q(\cdot, \cdot)} \left[ \log p(\vy^v \vert \vx,\!\vw^v) \right]}_{\text{(a): reconstruction}}\!-\!\underbracket{ \operatorname{KL}(q(\vx) \| p(\vx))}_{\text{(b): regularization}}\!\!\! \\
            & \approx \sum_{v=1}^V \sum_{m=1}^{M_v}   \frac{1}{{I}}\sum_{i=1}^{I} \log \mathcal{N}(\y_{:,m}^v \vert \bm{0}, \tilde{{\vk}}_{\mathrm{ngsm}}^{v(i)} + \sigma^2 \mathbf{I}_N) -  \frac{1}{2} \sum_{n=1}^{N} \Big[ \operatorname{tr}(\mathbf{S}_{n}) + \boldsymbol{\mu}_n^{\top} \boldsymbol{\mu}_n -\log |\mathbf{S}_{n}| - D  \Big] \\
            & = \sum_{v=1}^V \sum_{m=1}^{M_v} \frac{1}{{I}}\sum_{i=1}^{I}  \left\{ -\frac{N}{2} \log 2 \pi - \frac{1}{2} \log \left|    \tilde{{\vk}}_{\mathrm{ngsm}}^{v(i)} + \sigma_v^2 \mathbf{I}_N  \right| - \frac{1}{2} \y_{:, m}^{v\top}  \left(  \tilde{{\vk}}_{\mathrm{ngsm}}^{v(i)} + \sigma_v^2 \mathbf{I}_N \right)^{-1} \y_{:, m}^v \right\}  \! \\ 
            & ~~~ - \!  \frac{1}{2} \sum_{n=1}^{N} \Big[ \operatorname{tr}(\mathbf{S}_{n}) + \boldsymbol{\mu}_n^{\top} \boldsymbol{\mu}_n -\log |\mathbf{S}_{n}| - D  \Big].
    \end{aligned}
\end{equation}
%, and 
% $$
% \log \mathcal{N}(\y_j \vert \bm{0}, \hat{\vk}_{\text{sm}}^{(i)}+\sigma^2 \mathbf{I}_N) = -\frac{N}{2} \log 2 \pi - \frac{1}{2} \log \left|    \hat{\vk}_{\text{sm}}^{(i)} + \sigma^2 \mathbf{I}_N  \right| - \frac{1}{2} \y_j^{\top}  \left(  \hat{\vk}_{\text{sm}}^{(i)} + \sigma^2 \mathbf{I}_N \right)^{-1} \y_j. 
% $$

When $N \gg L$, both the determinant and the inverse of $\tilde{\vk}_{\text{ngsm}}^{v (i)} + \sigma_v^2 \mathbf{I}_N$ can be computed efficiently by the following two lemma \citep{williams2006gaussian}.
\begin{lemma}
Suppose $\mathbf{A}$ is an invertible $n$-by-$n$ matrix and $\mathbf{U}, \mathbf{V}$ are $n$-by-$m$ matrices. Then the following determinant equality holds.
$$
\left|\mathbf{A}+\mathbf{U} \mathbf{V}^{\top}\right|=\left|\mathbf{I}_{\mathrm{m}}+\mathbf{V}^{\top} \mathbf{A}^{-1} \mathbf{U}\right| \left|\mathbf{A}\right|.
$$
\end{lemma}
\begin{lemma}[Woodbury matrix identity]
\label{Lemma:Woodbury_matrix_identity}
    Suppose $\mathbf{A}$ is an invertible $n$-by-$n$ matrix and $\mathbf{U}, \mathbf{V}$ are $n$-by-$m$ matrices. Then
$$
\left(\mathbf{A}+\mathbf{U} \mathbf{V}^{\top}\right)^{-1}=\mathbf{A}^{-1} - \mathbf{A}^{-1}\mathbf{U} (\mathbf{I}_{\mathrm{m}} + \mathbf{V}^\top \mathbf{U})^{-1} \mathbf{V}^\top.
$$
\end{lemma}
We can reduce the computational complexity for evaluating the \MakeUppercase{elbo} from the original $\mathcal{O}(N^3)$ to $\mathcal{O}(NL^2)$ as below:
\begin{align}
    & \left| \tilde{\vk}_{\text{ngsm}}^{v (i)} + \sigma_v^2 \mathbf{I}_N \right| = \sigma_v^{2N} \left| \mathbf{I}_{L} + \frac{1}{\sigma_v^2} (\bm \Phi_{x}^{v} \bm \Phi_{x}^{v \top})^{(i)}\right|, \\
    & \left( \tilde{\vk}_{\text{ngsm}}^{v (i)} + \sigma_v^2 \mathbf{I}_N \right)^{-1} = \frac{1}{\sigma_v^2} \left[ \mathbf{I}_N - \bm \Phi_{x}^{v(i)} \left( \mathbf{I}_{L} + (\bm \Phi_{x}^{v} \bm \Phi_{x}^{v \top})^{(i)} \right)^{-1} \bm \Phi_{x}^{v(i) \top} \right].
\end{align}
% (\bm \Phi_{x}^{v(i)} \bm \Phi_{x}^{v\top})^{(i)} \!+\! \sigma^2_{v} \mathbf{I}

\subsection{Treating $\mathbf{W}$ variationally}
\label{app:set_of_w}

% We propose employing variational inference to treat both the spectral frequencies \(\mathbf{W}\) and \(\mathbf{X}\) in a variational framework. Additionally, to ensure that the kernel remains consistent with the NG-SM kernel, we assume the variational distribution of \(\mathbf{W}\) follows a Gaussian mixture. Notably, the joint distribution of the spectral frequencies is given by \(p(\w; \bm{\theta}) = \prod_{v=1}^{V} p(\w^v) = \prod_{v=1}^{V} \prod_{l=1}^{L/2} p_{\mathrm{ngsm}}^v(\w_1, \w_2)\). In this case, Eq.~\eqref{eq:variational_dis} becomes: $$q(\vx,\vw) = q(\vw;\bm{\eta})q(\vx).$$ 

Alternatively, we define the variational distributions as 
$$q(\vx,\vw) = q(\vw;\bm{\eta})q(\vx),$$
where the variational distribution $q(\vw; \bm{\eta})$ is also a bivariate Gaussian mixture that is parameterized by \(\bm{\eta}\). By combining these variational distributions with the joint distribution defined in Eq.~\eqref{eq:model_joint}, we derive the following \MakeUppercase{elbo}: 
\begin{align}
    \MakeUppercase{elbo} & = \mathbb{E}_{q(\vx,\vw)} \left[ \frac{p(\vy, \vx , \vw )}{q(\vw;\bm{\eta})q(\vx)}\right] \nonumber \\
            & = \mathbb{E}_{q(\vx, \vw)} \! \left[\log \frac{p(\vx) \prod_{v=1}^{V} p(\vw^v;\bm{\theta}_{\text{ngsm}}) p(\vy^v \vert \vx, \vw^v)}{q(\vx)\prod_{v=1}^{V} p(\vw^v;\bm{\eta})} \right] \nonumber \\ 
            % & = \mathbb{E}_{q(\vx,\vw)} \left[\log \frac{p(\vw;\bm{\theta}_{\text{ngsm}}) \prod_{n=1}^{N} p(\x_n) \prod_{v=1}^{V} \prod_{m=1}^{M_v} p(\y_{:,m}^v \vert \vx, \vw^v)}{q(\vw;\bm{\eta}) \prod_{n = 1}^N q(\x_n) } \right] \nonumber \\
            & = \sum_{v=1}^V \underbracket{\mathbb{E}_{q(\cdot, \cdot)} \left[ \log p(\vy^v \vert \vx,\!\vw^v) \right]}_{\text{(a): reconstruction}}\!-\!\underbracket{ \operatorname{KL}(q(\vx) \| p(\vx))}_{\text{(b): regularization of $\vx$}} -\underbrace{ \operatorname{KL}(q(\vw;\bm{\eta}) \| p(\vw;\btheta_{\text{ngsm}}))}_{\text{(c): regularization of $\vw$}},  
            \label{eq:elbo_variational_w}
\end{align}
where we redefine the prior distribution $p(\vw)$ as $p(\vw;\bm{\theta}_{\text{ngsm}})$ to maintain notational consistency. 
When maximizing the \MakeUppercase{elbo}, we obtain \( q(\vw; \bm{\eta}) = p(\vw; \btheta_{\text{ngsm}}) \), as the optimization variable \(\btheta_{\text{ngsm}}\) only affects term \(c\). Consequently, term \((c)\) becomes zero, and Eq.~\eqref{eq:elbo_variational_w} aligns with the optimization objective outlined in the main text. 



\subsection{Proof of Proposition \ref{prop:Two-step reparameterization trick}}
\label{app:proposition_1}

% In this section, we present a detailed proof of Proposition \ref{prop:Two-step reparameterization trick}, which introduces a two-step reparameterization approach for sampling bivariate Gaussian random variables. This method leverages sequential simulation, where the goal is to sample from a joint distribution \( p(\mathbf{w}_1, \mathbf{w}_2) \neq p(\mathbf{w}_1)p(\mathbf{w}_2) \). Specifically, we first sample \(\mathbf{w}_1\) from \( p(\mathbf{w}_1) \), followed by \(\mathbf{w}_2\) from the conditional distribution \( p(\mathbf{w}_2 | \mathbf{w}_1) \) \citep{gomez1994theory}. In the following, we provide the formal proof of Proposition \ref{prop:Two-step reparameterization trick}.

\begin{proof}

The proposed two-step reparameterization trick leverages sequential simulation, where \(\mathbf{w}_{q1}\) is first sampled from \( p(\mathbf{w}_{q1}) \), followed by \(\mathbf{w}_{q2}\) drawn from the conditional distribution \( p(\mathbf{w}_{q2} \vert \mathbf{w}_{q1}) \).

\underline{1) Sample from $p(\w_{q1})$:} 
Given that $\mathbf{w}_{q1}$ follows a normal distribution $\mathcal{N}(\bm{\mu}_{q1}, \operatorname{diag}(\bm{\sigma}_{q1}^2))$,  we can directly use the reparameterization trick to sample from it, i.e., 
\[ \mathbf{w}_{q1}^{(l)} = \bm{\mu}_{q1} + \bm{\sigma}_{q1} \circ  \bm{\epsilon}_1, \]
where $\bm{\epsilon}_1 \sim \mathcal{N}(\bm{0}, \mathbf{I})$. 

% To be specific, for a given mean $\bm{\mu}_{q1}$ and standard deviation $\bm{\sigma}_{q1}$, using the reparameterization trick, we can generate a normal random variable $\mathbf{w}_{q1}^{(l)}$ using a standard normal variable $\bm{\epsilon}_1$:
% \[ \mathbf{w}_{q1}^{(l)} = \bm{\mu}_{q1} + \bm{\sigma}_{q1} \circ  \bm{\epsilon}_1, \]
% where $\bm{\epsilon}_1 \sim \mathcal{N}(\bm{0}, \bm{I})$. 
% Thus, $\mathbf{w}_{q1}^{(l)}$ follows a normal distribution $\mathcal{N}(\bm{\mu}_{q1}, \operatorname{diag}(\bm{\sigma}_{q1}^2))$.

\underline{2) Sample from $p(\w_{q2} \vert \w_{q1})$:}  Given $\mathbf{w}_{q1}^{(l)}$, we use the given correlation parameter $\rho_q$ and a new standard normal variable $\bm{\epsilon}_2$ to generate $\mathbf{w}_{q2}^{(l)}$:
   \[ \mathbf{w}_{q2}^{(l)} = \bm{\mu}_{q2} + \rho_q \bm{\sigma}_{q2} \backslash \bm{\sigma}_{q1} \circ (\mathbf{w}_{q1}^{(l)} - \bm{\mu}_{q1}) + \sqrt{1 - \rho_q^2}  \bm{\sigma}_{q2} \circ \bm{\epsilon}_2. \]

Now, we need to proof that the generated $\mathbf{w}_{q1}^{(l)}$ and $\mathbf{w}_{q2}^{(l)}$ follow the bivariate Gaussian distribution $s_q(\mathbf{w}_1, \mathbf{w}_2)$. To this ends, we compute the mean and variance of $\mathbf{w}_{q2}^{(l)}$, and the covariance between $\mathbf{w}_{q1}^{(l)}$ and $\mathbf{w}_{q2}^{(l)}$ below. 

   - Mean of $\mathbf{w}_{q2}^{(l)}$:
     \[
     \mathbb{E}[\mathbf{w}_{q2}^{(l)}] = \mathbb{E}\left[\bm{\mu}_{q2} + \rho_q \bm{\sigma}_{q2} \backslash \bm{\sigma}_{q1} \circ  (\mathbf{w}_{q1}^{(l)} - \bm{\mu}_{q1}) + \sqrt{1 - \rho_q^2} \bm{\sigma}_{q2} \circ \bm{\epsilon}_2 \right].
     \]
     Since $\mathbb{E}[\bm{\epsilon}_2] = 0$ and $\mathbb{E}[\mathbf{w}_{q1}^{(l)}] = \bm{\mu}_{q1}$, we have:
     \[
     \mathbb{E}[\mathbf{w}_{q2}^{(l)}] = \bm{\mu}_{q2} + \rho_q \bm{\sigma}_{q2} \backslash \bm{\sigma}_{q1} \circ  (\mathbb{E}[\mathbf{w}_{q1}^{(l)}] - \bm{\mu}_{q1}) + \sqrt{1 - \rho_q^2}  \bm{\sigma}_{q2} \circ \mathbb{E}[\bm{\epsilon}_2]= \bm{\mu}_{q2}.
     \]

   - Variance of $\mathbf{w}_{q2}^{(l)}$:
     \[
     \text{Var}(\mathbf{w}_{q2}^{(l)}) = \text{Var}\left(\rho_q \bm{\sigma}_{q2} \backslash \bm{\sigma}_{q1} \circ  (\mathbf{w}_{q1}^{(l)} - \bm{\mu}_{q1}) + \sqrt{1 - \rho_q^2}  \bm{\sigma}_{q2} \circ \bm{\epsilon}_2\right).
     \]
     Since $\mathbf{w}_{q1}^{(l)} - \bm{\mu}_{q1}$ and $\bm{\epsilon}_2$ are independent, and $\text{Var}(\bm{\epsilon}_2) = \mathbf{I} $, we have:
     \[
     \text{Var}(\mathbf{w}_{q2}^{(l)}) = \rho_q^2 \left(\bm{\sigma}_{q2} \backslash \bm{\sigma}_{q1}\right)^2 \circ \text{Var}(\mathbf{w}_{q1}^{(l)}) + (1 - \rho_q^2) \bm{\sigma}_{q2}^2 = \rho_q^2 \bm{\sigma}_{q2}^2 + (1 - \rho_q^2) \bm{\sigma}_{q2}^2 = \bm{\sigma}_{q2}^2.
     \]

   - Covariance between $\mathbf{w}_{q1}^{(l)}$ and $\mathbf{w}_{q2}^{(l)}$:
   \begin{equation}
       \text{Cov}(\mathbf{w}_{q1}^{(l)}, \mathbf{w}_{q2}^{(l)}) = \text{Cov}\left(\bm{\mu}_{q1} + \bm{\sigma}_{q1} \circ \bm{\epsilon}_1, \bm{\mu}_{q2} + \rho_q \bm{\sigma}_{q2} \backslash \bm{\sigma}_{q1} \circ  (\mathbf{w}_{q1}^{(l)} - \bm{\mu}_{q1}) + \sqrt{1 - \rho_q^2}  \bm{\sigma}_{q2} \circ \bm{\epsilon}_2\right). 
       \label{eq:2-step-cov}
   \end{equation}
     Since $\bm{\mu}_{q1}$ and $\bm{\mu}_{q2}$ are constants, the covariance only depends on $\bm{\sigma}_{q1} \circ \bm{\epsilon}_1$ and $\rho_q \bm{\sigma}_{q2} \circ \bm{\epsilon}_1 + \sqrt{1 - \rho_q^2}  \bm{\sigma}_{q2} \circ \bm{\epsilon}_2$. Thus we can reformulate Eq.~\eqref{eq:2-step-cov} as  
     \[
     \text{Cov}(\bm{\sigma}_{q1} \circ \bm{\epsilon}_1, \rho_q \bm{\sigma}_{q2} \circ \bm{\epsilon}_1 + \sqrt{1 - \rho_q^2} \bm{\sigma}_{q2} \circ \bm{\epsilon}_2)  = \bm{\sigma}_{q1} \rho_q \circ \bm{\sigma}_{q2} \text{Cov}(\bm{\epsilon}_1, \bm{\epsilon}_1) + \bm{\sigma}_{q1} \sqrt{1 - \rho_q^2} \circ \bm{\sigma}_{q2} \text{Cov}(\bm{\epsilon}_1, \bm{\epsilon}_2).
     \]
    Given that $\bm{\epsilon}_1$ and $\bm{\epsilon}_2$ are independent, $\text{Cov}(\bm{\epsilon}_1, \bm{\epsilon}_2) = \bm{0}$, and $\text{Cov}(\bm{\epsilon}_1, \bm{\epsilon}_1) = \mathbf{I}$, this covariance is equal to:
     \[
     \bm{\sigma}_{q1} \rho_q \circ\bm{\sigma}_{q2} \circ \mathbf{I} + \bm{\sigma}_{q1} \sqrt{1 - \rho_q^2} \circ \bm{\sigma}_{q2} \circ \bm{0} = \rho_q \bm{\sigma}_{q1} \circ \bm{\sigma}_{q2}.
     \]

Therefore, the generated $\mathbf{w}_{q1}^{(l)}$ and  $\mathbf{w}_{q2}^{(l)}$ follow the bivariate Gaussian distribution $s_q(\mathbf{w}_1, \mathbf{w}_2)$. 
\end{proof}
The two-step reparameterization trick simplifies the sampling process of the bivariate Gaussian distribution. Specifically, the traditional sampling method \citep{mohamed2020monte} requires inverting the \(2D\)-dimensional covariance matrix, resulting in a computational complexity of \( \mathcal{O}(D^3) \). In contrast, our method achieves a computational complexity of \( \mathcal{O}(D)\) for sampling from \(p(\w_{q1})\), while sampling from \(p(\w_{q2} \vert \w_{q1})\) also maintains a complexity of \( \mathcal{O}(D)\). This results in a total computational cost of \( \mathcal{O}(D) \). 

% In contrast, the traditional reparameterization approach necessitates the inversion of the \( 2D \)-dimensional covariance matrix associated with the joint distribution of \( \w_1 \) and \( \w_2 \), leading to \( \mathcal{O}(D^3) \) complexity.






\subsection{Proof of Theorem \ref{prop_NGSM_RFF_approx}}
\label{app:proof_theorem_3}
% In this section, we aim to prove that the RFF defined for the NG-SM kernel provide an unbiased estimator of the kernel function. 
\begin{proof}
With the RFF feature map defined in Eq.~\eqref{eq:RFF_NGSM}, we can express the inner product of the feature maps as follows:
\begin{equation}
\begin{aligned}
&\varphi \left( \x_1 \right)^\top \varphi \left( \x_2 \right) = \sum_{q=1}^Q \alpha_q \sum_{l=1}^{L/2} \frac{1}{2 L}A_{q}^{(l)},
\end{aligned}
\end{equation}
where,
\begin{equation}
\begin{aligned}
& A_{q}^{(l)} = \left( \cos(\w_{q1}^{(l)\top} \x_1) \cos(\w_{q1}^{(l)\top} \x_2) + \cos(\w_{q1}^{(l)\top} \x_1) \cos(\w_{q2}^{(l)\top} \x_2) \right.\\
&\quad + \cos(\w_{q2}^{(l)\top} \x_1) \cos(\w_{q1}^{(l)\top} \x_2) + \cos(\w_{q2}^{(l)\top} \x_1) \cos(\w_{q2}^{(l)\top} \x_2) \\
&\quad + \sin(\w_{q1}^{(l)\top} \x_1) \sin(\w_{q1}^{(l)\top} \x_2) + \sin(\w_{q1}^{(l)\top} \x_1) \sin(\w_{q2}^{(l)\top} \x_2) \\
&\quad + \sin(\w_{q2}^{(l)\top} \x_1) \sin(\w_{q1}^{(l)\top} \x_2) + \sin(\w_{q2}^{(l)\top} \x_1) \sin(\w_{q2}^{(l)\top} \x_2) \bigg),
\end{aligned}
\end{equation}
where \(\{\mathbf{\w}_1^l,\mathbf{\w}_2^l\}_{l=1}^{L/2}\) are independently and identically distributed (i.i.d.) spectral points drawn from the density function \(s_q(\mathbf{\w}_1, \mathbf{\w}_2)\) using the two step reparameterization trick. Taking the expectation with respect to \( p_{\text{ngsm}}\left(\mathbf{\w}_1, \mathbf{\w}_2\right) = \prod_{q=1}^Q \prod_{l=1}^{L/2} s_q(\mathbf{\w}_1, \mathbf{\w}_2) \), we obtain:
\begin{equation}
    \label{eq:proof_RFF_SM}
    \begin{aligned}
        &\mathbb{E}_{p_{\text{ngsm}}\left(\mathbf{\w}_1, \mathbf{\w}_2\right)}  \left[\varphi\left( \x_1\right)^\top \varphi \left( \x_2 \right)\right] 
        =  \mathbb{E}_{p_{\text{ngsm}}\left(\mathbf{\w}_1, \mathbf{\w}_2\right)} \left[ \sum_{q=1}^Q \alpha_q \sum_{l = 1}^{L/2} \frac{1}{2 L} A_q^{(l)} \right] &   \\
        & =  \sum_{q=1}^Q \alpha_q \mathbb{E}_{s\left(\mathbf{\w_{q1}}^{1:L/2},\mathbf{\w_{q2}}^{1:L/2}\right)} \left[\sum_{l = 1}^{L/2} \frac{1}{2 L} A_q^{(l)} \right], &(\text{linearity of expectation})
 \end{aligned}
\end{equation}
% we can simplify \(A_q^{(l)}\) as \(A_q\) as the expectation is already taken over all \(l\), allowing it to be removed for simplicity.
\begin{equation}
    \begin{aligned}
        \ & =  \sum_{q=1}^Q \alpha_q  \frac{1}{4}\mathbb{E}_{s_q(\w_1,\w_2)} \left[ A_q \right] &(\text{i.i.d. of } \w^{(q)}_l) \\
        & =  \sum_{q=1}^Q \alpha_q  \frac{1}{4} \mathbb{E}_{s_q(\w_1,\w_2)} \left[ \exp(i(\w_1 \x_1^\top - \w_2 \x_2^\top)) + \exp(i(\w_2 \x_1^\top - \w_1 \x_2^\top)) \right. \\
        & \quad\quad +  \exp(i(\w_1 \x_1^\top - \w_1 \x_2^\top)) + \exp(i(\w_2 \x_1^\top - \w_2 \x_2^\top)) \bigg] & (\text{Eulerâ€™s identity}) \\
        & = \sum_{q=1}^Q \alpha_q k_q(\x_1, \x_2)  &  \\
        & = k_{\text{ngsm}}(\x_1, \x_2). & (\text{NG-SM kernel definition})
    \end{aligned}
\end{equation}
Thus, we conclude that \(\varphi\left( \mathbf{x}_1 \right)^\top \varphi\left( \mathbf{x}_2 \right)\) provides an unbiased estimator for the NG-SM kernel.

\end{proof}
\vspace{.2in}

\subsection{Proof of Theorem \ref{thm:NGSM_RFF_approx}}
\label{app:proof_theorem_4}
\begin{proof}
We primarily rely on the Matrix Bernstein inequality \citep{tropp2015introduction} to establish Theorem \ref{thm:NGSM_RFF_approx}, with the proof outline depicted in Figure \ref{fig:Flowchart for Proving Theorem 4}. 
    \!
    \begin{lemma}[Matrix Bernstein Inequality] \label{lemma:matrix_bernstein}
    Consider a finite sequence $\left\{\mathbf{E}_i\right\}$ of independent, random, Hermitian matrices with dimension $N$. Assume that
    $$
    \mathbb{E} [\mathbf{E}_i] =\mathbf{0} \text {  and  }  \left\|\mathbf{E}_i\right\|_2 \leq H \text {  for each index } i,
    $$ where $\|\cdot\|_2$ denotes the matrix spectral norm.  Introduce the random matrix
    $
    \mathbf{E}=\sum_i \mathbf{E}_i,
    $ 
    and let $v(\mathbf{E})$ be the matrix variance statistic of the sum:
    $$
    v(\mathbf{E})=\left\|\mathbb{E} [\mathbf{E}^2]\right\|=\left\|\sum_i \mathbb{E} [\mathbf{E}_i^2]\right\| .
    $$
    
    Then we have
    \begin{equation}
        \mathbb{E} \left[\|\mathbf{E}\|_2 \right] \leq \sqrt{2 v(\mathbf{E}) \log N}+\frac{1}{3} L \log N .
    \end{equation}
    
    Furthermore, for all $\epsilon \geq 0$.
    \begin{equation}
        {P}\left\{ \|\mathbf{E}\|_2 \geq \epsilon \right\} \leq N \cdot \exp \left(\frac{-\epsilon^2 / 2}{v(\mathbf{E})+H \epsilon / 3}\right).
    \end{equation}
    \end{lemma}
    \begin{proof}
        The proof of Lemma \ref{lemma:matrix_bernstein} can be found in {Theorem 6.6.1} in \cite{tropp2015introduction}. 
    \end{proof}
    \vspace{.1in}

% Next, we will prove Theorem \ref{thm:NGSM_RFF_approx} following the approach outlined in Figure \ref{fig:Flowchart for Proving Theorem 4}.

\begin{figure}[t!]
    \vspace{-.1in}
    \centering
    %{ \small \textbf{channel bar chart}}
    %\\[0.5em]
    \includegraphics[width=\linewidth]{img/flowchart.pdf} 
    \caption{Flowchart for proving Theorem \ref{thm:NGSM_RFF_approx} }
    \vspace{-0.2in}
    \label{fig:Flowchart for Proving Theorem 4}
\end{figure}


\paragraph{Step 1:}With the constructed NG-SM kernel matrix approximation, $\hat{\mathbf{K}}_{\mathrm{ngsm}} =  \Phi_{\mathrm{ngsm}}(\vx) \Phi_{\mathrm{ngsm}}(\vx)^\top$, where the random feature matrix $\Phi_{\mathrm{ngsm}}(\vx)\!=\!\left[\varphi\left(\x_1\right),  \ldots , \varphi\left(\x_N\right)\right]^\top \!\in \! \mathbb{R}^{N \times QL}$, we have the following approximation error matrix:
\begin{equation}
   \mathbf{E} = \hat{\mathbf{K}}_{\mathrm{ngsm}} - {\mathbf{K}}_{\mathrm{ngsm}}.
\end{equation}
We are going to show that $\mathbf{E}$ can be factorized as 
\begin{equation}
    \mathbf{E} = \sum_{q=1}^Q \sum_{l=1}^{L/2} \mathbf{E}_{q}^{(l)},
\end{equation}
where $\mathbf{E}_{q}^{(l)}$ is a sequence of independent, random, Hermitian matrices with dimension $N$.

Sample \( \w_{q1}^{(l)}, \w_{q2}^{(l)} \) from \(s_q(\w_{1}, \w_{2})\),
and we can show that 
\begin{equation}
% \setlength{\abovedisplayskip}{3.5pt}
% \setlength{\belowdisplayskip}{3.4pt}
\begin{aligned}
& [\hat{\mathbf{K}}_{\mathrm{ngsm}}]_{h,g} = \sum_{q=1}^Q \frac{\alpha_q}{2L} \sum_{l=1}^{L/2} A_{q}^{(l)}(h,g), \\
& \text{where} \\
& A_{q}^{(l)}(h,g) = \left( \cos(\w_{q1}^{(l)\top} \x_h) \cos(\w_{q1}^{(l)\top} \x_g) + \cos(\w_{q1}^{(l)\top} \x_h) \cos(\w_{q2}^{(l)\top} \x_g) + \cos(\w_{q2}^{(l)\top} \x_h) \cos(\w_{q1}^{(l)\top} \x_g) \right. \\
& \quad + \cos(\w_{q2}^{(l)\top} \x_h) \cos(\w_{q2}^{(l)\top} \x_g) + \sin(\w_{q1}^{(l)\top} \x_h) \sin(\w_{q1}^{(l)\top} \x_g) + \sin(\w_{q1}^{(l)\top} \x_h) \sin(\w_{q2}^{(l)\top} \x_g) \\
& \quad + \sin(\w_{q2}^{(l)\top} \x_h) \sin(\w_{q1}^{(l)\top} \x_g) + \sin(\w_{q2}^{(l)\top} \x_h) \sin(\w_{q2}^{(l)\top} \x_g) \bigg).
\end{aligned}
\end{equation}
Thus, we have $\hat{\mathbf{K}}_{\mathrm{ngsm}} = \sum_{q=1}^Q \sum_{l=1}^{L/2} \frac{\alpha_q}{2L} A_{q}^{(l)}$. Based on this factorization and Eq.~\eqref{eq:proof_RFF_SM} in Proposition \ref{prop_NGSM_RFF_approx}, we have that $${\mathbf{K}}_{\mathrm{ngsm}} = \sum_{q=1}^Q \sum_{l=1}^{L/2} \frac{\alpha_q}{2 L} \mathbb{E}[A_{q}^{(l)}].$$
    Therefore, the approximation error matrix $\mathbf{E}$ can be factorized as
    $
        \mathbf{E} = \sum_{q=1}^Q \sum_{l=1}^{L/2} \mathbf{E}_{q}^{(l)} 
    $
    where 
    \begin{equation}
    % \setlength{\abovedisplayskip}{2.5pt}
    % \setlength{\belowdisplayskip}{2.4pt}
         \mathbf{E}_{q}^{(l)} = \frac{\alpha_q}{2 L} \left( A_{q}^{(l)} - \mathbb{E}[A_{q}^{(l)}]\right)
    \end{equation}
    is a sequence of independent, random, Hermitian matrices with dimension $N$ that satisfy the condition of $\mathbb{E} [ \mathbf{E}_{q}^{(l)}] = \mathbf{0}$.
    




\paragraph{Step 2:}We can bound the $\|\mathbf{E}_{q}^{(l)}\|_2$ by following.
        \begin{subequations}
        % \setlength{\abovedisplayskip}{3.5pt}
        % \setlength{\belowdisplayskip}{3.4pt}
        \label{eq:upper_bound_e_l_q}
            \begin{align}
            \|\mathbf{E}_{q}^{(l)}\|_2 & =\frac{\alpha_q}{2L} \left\|A_{q}^{(l)}-\mathbb{E} \left[A_{q}^{(l)}\right]\right\|_2 \\
            & \leq \frac{\alpha_q}{2L}\left(\left\|A_{q}^{(l)}\right\|_2+\left\|\mathbb{E}\left[A_{q}^{(l)}\right]\right\|_2\right) \quad \quad \text{ (triangle inequality)}\\
            & \leq \frac{\alpha_q}{2L}\left(\left\|A_{q}^{(l)}\right\|_2+ \mathbb{E} \left[ \left\| A_{q}^{(l)} \right\|_2 \right] \right)   \quad \quad \text{  (Jensenâ€™s inequality)}\\
            & \leq \frac{C}{2L}\left( 8N + 8N\right) \label{subeq:step2_d}\\
            & = \frac{C}{2L} 16 N = \frac{8CN}{L},
            \end{align}
        \end{subequations}
where $ C = \sqrt{\sum_{q=1}^Q \alpha_q^2}$ and 

\begin{equation}
% \setlength{\abovedisplayskip}{3.5pt}
% \setlength{\belowdisplayskip}{3.4pt}
    \begin{aligned}
        & \bm{c}_{qk}^{(l)}  =\left[\cos \left( \w_{qk}^{(l)\top} \x_1\right), \ldots, \cos \left(\w_{qk}^{(l)\top} \x_N\right)\right]^\top \in \mathbb{R}^{N \times 1}, \\
        & \bm{s}_{qk}^{(l)}  =\left[\sin \left( \w_{qk}^{(l)\top} \x_1\right) , \ldots, \sin \left(\w_{qk}^{(l)\top} \x_N\right)\right]^\top \in \mathbb{R}^{N \times 1},\\
        & A_{q}^{(l)} = \bm{c}_{q1}^{(l)}\bm{c}_{q1}^{(l)\top}
        +\bm{s}_{q1}^{(l)}\bm{s}_{q1}^{(l)\top} + \bm{c}_{q2}^{(l)}\bm{c}_{q2}^{(l)\top}
        +\bm{s}_{q2}^{(l)}\bm{s}_{q2}^{(l)\top} +\bm{c}_{q1}^{(l)}\bm{c}_{q2}^{(l)\top}
        +\bm{s}_{q1}^{(l)}\bm{s}_{q2}^{(l)\top} +\bm{c}_{q2}^{(l)}\bm{c}_{q1}^{(l)\top}
        +\bm{s}_{q2}^{(l)}\bm{s}_{q1}^{(l)\top}.   
    \end{aligned}
\end{equation}
We use the fact that $\left\|A_{q}^{(l)}\right\|_2=$
\begin{equation}
\setlength{\abovedisplayskip}{3.5pt}
\setlength{\belowdisplayskip}{3.4pt}
\begin{aligned}
& \sup _{\|\bm{v}\|_2^2=1} \bm{v}^\top\left(\bm{c}_{q1}^{(l)}\bm{c}_{q1}^{(l)\top}
        +\bm{s}_{q1}^{(l)}\bm{s}_{q1}^{(l)\top} + \bm{c}_{q2}^{(l)}\bm{c}_{q2}^{(l)\top}
        +\bm{s}_{q2}^{(l)}\bm{s}_{q2}^{(l)\top} +\bm{c}_{q1}^{(l)}\bm{c}_{q2}^{(l)\top}
        +\bm{s}_{q1}^{(l)}\bm{s}_{q2}^{(l)\top} +\bm{c}_{q2}^{(l)}\bm{c}_{q1}^{(l)\top}      +\bm{s}_{q2}^{(l)}\bm{s}_{q1}^{(l)\top}\right) \bm{v}
% &\leq \left\|\bm{c}_{q1}^{(l)}\right\|_2^2+\left\|\bm{s}_{q1}^{(l)} \right\|_2^2 + \left\|\bm{c}_{q2}^{(l)}\right\|_2^2+\left\|\bm{s}_{q2}^{(l)} \right\|_2^2 +
% \left\|\bm{c}_{q1}^{(l)}\bm{c}_{q2}^{(l)\top}\right\|_2^2+\left\|\bm{s}_{q1}^{(l)}\bm{s}_{q2}^{(l)\top} \right\|_2^2+
% \left\|\bm{c}_{q2}^{(l)}\bm{c}_{q1}^{(l)\top}\right\|_2^2+\left\|\bm{s}_{q2}^{(l)}\bm{s}_{q1}^{(l)\top} \right\|_2^2
 \leq 8 N,
\end{aligned}
\end{equation}
to obtain and the inequality in Eq.~\eqref{subeq:step2_d}.


\paragraph{Step 3 :}We first have the following bound:
\begin{subequations}
\setlength{\abovedisplayskip}{8pt}
\setlength{\belowdisplayskip}{8pt}
\begin{align}
&\frac{4 L^2}{\alpha_q^2} \mathbb{E}\left[\left(\mathbf{E}_q^{(l)}\right)^2\right] =\mathbb{E}\left[(A_{q}^{(l)})^2\right]-\left(\mathbb{E}\left[A_{q}^{(l)}\right]\right)^2 
\\
& \preccurlyeq \mathbb{E}\left[(A_{q}^{(l)})^2\right]  \label{eq:first_inqu_}\\
& = \mathbb{E}\left[\sum_{k_1 = 1}^2 \sum_{k_2 = 1}^2\sum_{k_3 = 1}^2\sum_{k_4 = 1}^2 \left( \bm{c}_{qk_1}^{(l)}\bm{c}_{qk_2}^{(l)\top}\bm{c}_{qk_3}^{(l)}\bm{c}_{qk_4}^{(l)\top}
\!+\!\bm{c}_{qk_1}^{(l)}\bm{c}_{qk_2}^{(l)\top}\bm{s}_{qk_3}^{(l)}\bm{s}_{qk_4}^{(l)\top} \!+\! \bm{s}_{qk_1}^{(l)}\bm{s}_{qk_2}^{(l)\top}\bm{c}_{qk_3}^{(l)}\bm{c}_{qk_4}^{(l)\top}
\!+\!\bm{s}_{qk_1}^{(l)}\bm{s}_{qk_2}^{(l)\top}\bm{s}_{qk_3}^{(l)}\bm{s}_{qk_4}^{(l)\top}   \right)\right] \\
& = \mathbb{E}\left[\sum_{k_1 = 1}^2 \sum_{k_2 = 1}^2\sum_{k_3 = 1}^2\sum_{k_4 = 1}^2 \left( (\bm{c}_{qk_2}^{(l)\top}\bm{c}_{qk_3}^{(l)})\bm{c}_{qk_1}^{(l)}\bm{c}_{qk_4}^{(l)\top} + (\bm{s}_{qk_2}^{(l)\top}\bm{s}_{qk_3}^{(l)})\bm{s}_{qk_1}^{(l)}\bm{s}_{qk_4}^{(l)\top} +
(\bm{s}_{qk_2}^{(l)\top}\bm{c}_{qk_3}^{(l)})(\bm{s}_{qk_1}^{(l)}\bm{c}_{qk_4}^{(l)\top} + \bm{c}_{qk_1}^{(l)}\bm{s}_{qk_4}^{(l)\top})\right)\right] \\
& =\sum_{k_1 = 1}^2 \sum_{k_2 = 1}^2\sum_{k_3 = 1}^2\sum_{k_4 = 1}^2 \left(\mathbb{E}\left[ (\bm{c}_{qk_2}^{(l)\top}\bm{c}_{qk_3}^{(l)})\bm{c}_{qk_1}^{(l)}\bm{c}_{qk_4}^{(l)\top} + (\bm{s}_{qk_2}^{(l)\top}\bm{s}_{qk_3}^{(l)})\bm{s}_{qk_1}^{(l)}\bm{s}_{qk_4}^{(l)\top} +
(\bm{s}_{qk_2}^{(l)\top}\bm{c}_{qk_3}^{(l)})(\bm{s}_{qk_1}^{(l)}\bm{c}_{qk_4}^{(l)\top} + \bm{c}_{qk_1}^{(l)}\bm{s}_{qk_4}^{(l)\top})\right] \right)\\
& \preccurlyeq \sum_{k_1 = 1}^2 \sum_{k_2 = 1}^2\sum_{k_3 = 1}^2\sum_{k_4 = 1}^2 \left( N \mathbb{E}\left[\bm{c}_{qk_1}^{(l)}\bm{c}_{qk_4}^{(l)\top}+\bm{s}_{qk_1}^{(l)}\bm{s}_{qk_4}^{(l)\top}\right]+\mathbb{E}\left[(\bm{s}_{qk_2}^{(l)\top}\bm{c}_{qk_3}^{(l)})(\bm{s}_{qk_1}^{(l)}\bm{c}_{qk_4}^{(l)\top} + \bm{c}_{qk_1}^{(l)}\bm{s}_{qk_4}^{(l)\top})\right] \right) \label{eq:second_inqu_} \\
&= 4 N \mathbb{E}\left[A_q^{(l)}\right] + \sum_{k_1 = 1}^2 \sum_{k_2 = 1}^2\sum_{k_3 = 1}^2\sum_{k_4 = 1}^2 \left( \mathbb{E}\left[(\bm{s}_{qk_2}^{(l)\top}\bm{c}_{qk_3}^{(l)})(\bm{s}_{qk_1}^{(l)}\bm{c}_{qk_4}^{(l)\top} + \bm{c}_{qk_1}^{(l)}\bm{s}_{qk_4}^{(l)\top})\right] \right),
\end{align}
\end{subequations}


\begin{comment}
\begin{equation}
\begin{aligned}
&\bm{c}_{i1}^{(l)}\bm{c}_{i1}^{(l)\top}\bm{c}_{i1}^{(l)}\bm{c}_{i1}^{(l)\top}
+\bm{c}_{i1}^{(l)}\bm{c}_{i1}^{(l)\top}\bm{s}_{i1}^{(l)}\bm{s}_{i1}^{(l)\top}
+\bm{c}_{i1}^{(l)}\bm{c}_{i1}^{(l)\top}\bm{c}_{i2}^{(l)}\bm{c}_{i2}^{(l)\top}
+\bm{c}_{i1}^{(l)}\bm{c}_{i1}^{(l)\top}\bm{s}_{i2}^{(l)}\bm{s}_{i2}^{(l)\top} + \\
&\bm{c}_{i1}^{(l)}\bm{c}_{i1}^{(l)\top}\bm{c}_{i1}^{(l)}\bm{c}_{i2}^{(l)\top}
+\bm{c}_{i1}^{(l)}\bm{c}_{i1}^{(l)\top}\bm{s}_{i1}^{(l)}\bm{s}_{i2}^{(i)\top}
+\bm{c}_{i1}^{(l)}\bm{c}_{i1}^{(l)\top}\bm{c}_{i2}^{(l)}\bm{c}_{i1}^{(l)\top}
+\bm{c}_{i1}^{(l)}\bm{c}_{i1}^{(l)\top}\bm{s}_{i2}^{(l)}\bm{s}_{i1}^{(i)\top} + \\ 
& \bm{s}_{i1}^{(l)}\bm{s}_{i1}^{(l)\top}\bm{c}_{i1}^{(l)}\bm{c}_{i1}^{(l)\top}
+\bm{s}_{i1}^{(l)}\bm{s}_{i1}^{(l)\top}\bm{s}_{i1}^{(l)}\bm{s}_{i1}^{(i)\top}
+\bm{s}_{i1}^{(l)}\bm{s}_{i1}^{(l)\top}\bm{c}_{i2}^{(l)}\bm{c}_{i2}^{(l)\top}
+\bm{s}_{i1}^{(l)}\bm{s}_{i1}^{(l)\top}\bm{s}_{i2}^{(l)}\bm{s}_{i2}^{(i)\top} + \\
&\bm{s}_{i1}^{(l)}\bm{s}_{i1}^{(l)\top}\bm{c}_{i1}^{(l)}\bm{c}_{i2}^{(l)\top}
+\bm{s}_{i1}^{(l)}\bm{s}_{i1}^{(l)\top}\bm{s}_{i1}^{(l)}\bm{s}_{i2}^{(i)\top}
+\bm{s}_{i1}^{(l)}\bm{s}_{i1}^{(l)\top}\bm{c}_{i2}^{(l)}\bm{c}_{i1}^{(l)\top}
+\bm{s}_{i1}^{(l)}\bm{s}_{i1}^{(l)\top}\bm{s}_{i2}^{(l)}\bm{s}_{i1}^{(i)\top} + \\ 


& = \mathbb{E}\left[\sum_{k_1 = 1}^2 \sum_{k_2 = 1}^2\sum_{k_3 = 1}^2\sum_{k_4 = 1}^2 \left( \bm{c}_{ik_1}^{(l)}\bm{c}_{ik_2}^{(l)\top}\bm{c}_{ik_3}^{(l)}\bm{c}_{ik_4}^{(l)\top}
+\bm{c}_{ik_1}^{(l)}\bm{c}_{ik_2}^{(l)\top}\bm{s}_{ik_3}^{(l)}\bm{s}_{ik_4}^{(l)\top} + \bm{s}_{ik_1}^{(l)}\bm{s}_{ik_2}^{(l)\top}\bm{c}_{ik_3}^{(l)}\bm{c}_{ik_4}^{(l)\top}
+\bm{s}_{ik_1}^{(l)}\bm{s}_{ik_2}^{(l)\top}\bm{s}_{ik_3}^{(l)}\bm{s}_{ik_4}^{(l)\top}   \right)\right] \\
& = \mathbb{E}\left[\sum_{k_1 = 1}^2 \sum_{k_2 = 1}^2\sum_{k_3 = 1}^2\sum_{k_4 = 1}^2 \left( (\bm{c}_{ik_2}^{(l)\top}\bm{c}_{ik_3}^{(l)})\bm{c}_{ik_1}^{(l)}\bm{c}_{ik_4}^{(l)\top} + (\bm{s}_{ik_2}^{(l)\top}\bm{s}_{ik_3}^{(l)})\bm{s}_{ik_1}^{(l)}\bm{s}_{ik_4}^{(l)\top} +
(\bm{s}_{ik_2}^{(l)\top}\bm{c}_{ik_3}^{(l)})(\bm{s}_{ik_1}^{(l)}\bm{c}_{ik_4}^{(l)\top} + \bm{c}_{ik_1}^{(l)}\bm{s}_{ik_4}^{(l)\top})\right)\right] \\
& =\sum_{k_1 = 1}^2 \sum_{k_2 = 1}^2\sum_{k_3 = 1}^2\sum_{k_4 = 1}^2 \left(\mathbb{E}\left[ (\bm{c}_{ik_2}^{(l)\top}\bm{c}_{ik_3}^{(l)})\bm{c}_{ik_1}^{(l)}\bm{c}_{ik_4}^{(l)\top} + (\bm{s}_{ik_2}^{(l)\top}\bm{s}_{ik_3}^{(l)})\bm{s}_{ik_1}^{(l)}\bm{s}_{ik_4}^{(l)\top} +
(\bm{s}_{ik_2}^{(l)\top}\bm{c}_{ik_3}^{(l)})(\bm{s}_{ik_1}^{(l)}\bm{c}_{ik_4}^{(l)\top} + \bm{c}_{ik_1}^{(l)}\bm{s}_{ik_4}^{(l)\top})\right] \right)

\\
& \preccurlyeq \sum_{k_1 = 1}^2 \sum_{k_2 = 1}^2\sum_{k_3 = 1}^2\sum_{k_4 = 1}^2 \left( N \mathbb{E}\left[\bm{c}_{ik_1}^{(l)}\bm{c}_{ik_4}^{(l)\top}+\bm{s}_{ik_1}^{(l)}\bm{s}_{ik_4}^{(l)\top}\right]+\mathbb{E}\left[(\bm{s}_{ik_2}^{(l)\top}\bm{c}_{ik_3}^{(l)})(\bm{s}_{ik_1}^{(l)}\bm{c}_{ik_4}^{(l)\top} + \bm{c}_{ik_1}^{(l)}\bm{s}_{ik_4}^{(l)\top})\right] \right) \label{eq:second_inqu_} \\
&= 4 N \mathbb{E}\left[A_i^{(l)}\right] + \sum_{k_1 = 1}^2 \sum_{k_2 = 1}^2\sum_{k_3 = 1}^2\sum_{k_4 = 1}^2 \left( \mathbb{E}\left[(\bm{s}_{ik_2}^{(l)\top}\bm{c}_{ik_3}^{(l)})(\bm{s}_{ik_1}^{(l)}\bm{c}_{ik_4}^{(l)\top} + \bm{c}_{ik_1}^{(l)}\bm{s}_{ik_4}^{(l)\top})\right] \right)

\\
& \bm{c}_{i2}^{(l)}\bm{c}_{i2}^{(l)\top}\bm{c}_{i1}^{(l)}\bm{c}_{i1}^{(l)\top}
+\bm{c}_{i2}^{(l)}\bm{c}_{i2}^{(l)\top}\bm{s}_{i1}^{(l)}\bm{s}_{i1}^{(l)\top} 
+\bm{c}_{i2}^{(l)}\bm{c}_{i2}^{(l)\top}\bm{c}_{i2}^{(l)}\bm{c}_{i2}^{(l)\top}
+\bm{c}_{i2}^{(l)}\bm{c}_{i2}^{(l)\top}\bm{s}_{i2}^{(l)}\bm{s}_{i2}^{(l)\top}+\\
&\bm{c}_{i2}^{(l)}\bm{c}_{i2}^{(l)\top}\bm{c}_{i1}^{(l)}\bm{c}_{i2}^{(l)\top}
+\bm{c}_{i2}^{(l)}\bm{c}_{i2}^{(l)\top}\bm{s}_{i1}^{(l)}\bm{s}_{i2}^{(l)\top} 
+\bm{c}_{i2}^{(l)}\bm{c}_{i2}^{(l)\top}\bm{c}_{i2}^{(l)}\bm{c}_{i1}^{(l)\top}
+\bm{c}_{i2}^{(l)}\bm{c}_{i2}^{(l)\top}\bm{s}_{i2}^{(l)}\bm{s}_{i1}^{(l)\top} + \\ 
& \bm{c}_{i1}^{(l)}\bm{c}_{i1}^{(l)\top}
+ \bm{s}_{i1}^{(l)}\bm{s}_{i1}^{(l)\top} 
+\bm{c}_{i2}^{(l)}\bm{c}_{i2}^{(l)\top}
+\bm{s}_{i2}^{(l)}\bm{s}_{i2}^{(l)\top} +\bm{c}_{i1}^{(l)}\bm{c}_{i2}^{(l)\top}
+\bm{s}_{i1}^{(l)}\bm{s}_{i2}^{(l)\top} +\bm{c}_{i2}^{(l)}\bm{c}_{i1}^{(l)\top}
+\bm{s}_{i2}^{(l)}\bm{s}_{i1}^{(l)\top} + \\ 
& \bm{c}_{i1}^{(l)}\bm{c}_{i1}^{(l)\top}
+ \bm{s}_{i1}^{(l)}\bm{s}_{i1}^{(l)\top} 
+\bm{c}_{i2}^{(l)}\bm{c}_{i2}^{(l)\top}
+\bm{s}_{i2}^{(l)}\bm{s}_{i2}^{(l)\top} +\bm{c}_{i1}^{(l)}\bm{c}_{i2}^{(l)\top}
+\bm{s}_{i1}^{(l)}\bm{s}_{i2}^{(l)\top} +\bm{c}_{i2}^{(l)}\bm{c}_{i1}^{(l)\top}
+\bm{s}_{i2}^{(l)}\bm{s}_{i1}^{(l)\top} + \\ 
& \bm{c}_{i1}^{(l)}\bm{c}_{i1}^{(l)\top}
+ \bm{s}_{i1}^{(l)}\bm{s}_{i1}^{(l)\top} 
+\bm{c}_{i2}^{(l)}\bm{c}_{i2}^{(l)\top}
+\bm{s}_{i2}^{(l)}\bm{s}_{i2}^{(l)\top} +\bm{c}_{i1}^{(l)}\bm{c}_{i2}^{(l)\top}
+\bm{s}_{i1}^{(l)}\bm{s}_{i2}^{(l)\top} +\bm{c}_{i2}^{(l)}\bm{c}_{i1}^{(l)\top}
+\bm{s}_{i2}^{(l)}\bm{s}_{i1}^{(l)\top} + \\ 
& \bm{c}_{i1}^{(l)}\bm{c}_{i1}^{(l)\top}
+ \bm{s}_{i1}^{(l)}\bm{s}_{i1}^{(l)\top} 
+\bm{c}_{i2}^{(l)}\bm{c}_{i2}^{(l)\top}
+\bm{s}_{i2}^{(l)}\bm{s}_{i2}^{(l)\top} +\bm{c}_{i1}^{(l)}\bm{c}_{i2}^{(l)\top}
+\bm{s}_{i1}^{(l)}\bm{s}_{i2}^{(l)\top} +\bm{c}_{i2}^{(l)}\bm{c}_{i1}^{(l)\top}
+\bm{s}_{i2}^{(l)}\bm{s}_{i1}^{(l)\top} + \\  
& \bm{c}_{i1}^{(l)}\bm{c}_{i1}^{(l)\top}
+ \bm{s}_{i1}^{(l)}\bm{s}_{i1}^{(l)\top} 
+\bm{c}_{i2}^{(l)}\bm{c}_{i2}^{(l)\top}
+\bm{s}_{i2}^{(l)}\bm{s}_{i2}^{(l)\top} +\bm{c}_{i1}^{(l)}\bm{c}_{i2}^{(l)\top}
+\bm{s}_{i1}^{(l)}\bm{s}_{i2}^{(l)\top} +\bm{c}_{i2}^{(l)}\bm{c}_{i1}^{(l)\top}
+\bm{s}_{i2}^{(l)}\bm{s}_{i1}^{(l)\top} + \\ 
\end{aligned}
\end{equation}
\end{comment}

where the notation $\mathbf{A} \preccurlyeq \mathbf{B}$ denotes that $\mathbf{B} - \mathbf{A}$ is a PSD matrix, and the inequality in Eq.~\eqref{eq:first_inqu_} holds due to the fact that $\left(\mathbb{E}\left[A_{q}^{(l)}\right]\right)^2$ is a PSD matrix. The inequality in Eq.~\eqref{eq:second_inqu_} holds because 
\begin{equation}
\setlength{\abovedisplayskip}{8pt}
\setlength{\belowdisplayskip}{8pt}
    \begin{aligned}
        & N \mathbb{E}\left[\bm{c}_{qk_1}^{(l)}\bm{c}_{qk_4}^{(l)\top}+\bm{s}_{qk_1}^{(l)}\bm{s}_{qk_4}^{(l)\top}\right] - \mathbb{E}\left[(\bm{c}_{qk_2}^{(l)\top}\bm{c}_{qk_3}^{(l)})\bm{c}_{qk_1}^{(l)}\bm{c}_{qk_4}^{(l)\top} + (\bm{s}_{qk_2}^{(l)\top}\bm{s}_{qk_3}^{(l)})\bm{s}_{qk_1}^{(l)}\bm{s}_{qk_4}^{(l)\top}\right] \\
        & = \mathbb{E}\left[(\bm{s}_{qk_2}^{(l)\top}\bm{s}_{qk_3}^{(l)})\bm{c}_{qk_1}^{(l)}\bm{c}_{qk_4}^{(l)\top} + (\bm{c}_{qk_2}^{(l)\top}\bm{c}_{qk_3}^{(l)})\bm{s}_{qk_1}^{(l)}\bm{s}_{qk_4}^{(l)\top} \right]  \\ &\footnotesize{\left(\text{due to }  \bm{c}_{qk_2}^{(l)}\bm{c}_{qk_3}^{(l)\top}+\bm{s}_{qk_2}^{(l)}\bm{s}_{qk_3}^{(l)\top} = \sum_{j=1}^N(\cos(\w_{qk_2}^{(l)\top} - \w_{qk_3}^{(l)\top})\x_j)
        \preccurlyeq N \right)}
    \end{aligned}
\end{equation}
is a PSD matrix.

Then we are able to bound the variance, $\left\| \sum_{q=1}^Q \sum_{l=1}^{L/2} \mathbb{E}[ (\mathbf{E}_{q}^{(l)})^2 ] \right\|_2$, as
\begin{equation}
% \setlength{\abovedisplayskip}{3.5pt}
% \setlength{\belowdisplayskip}{3.4pt}
\label{eq:variance_bound}
    \begin{aligned}
    & ~~~ \left\| \sum_{q=1}^Q \sum_{l=1}^{L/2} \mathbb{E}[ (\mathbf{E}_{q}^{(l)})^2 ] \right\|_2 \\ 
    & \leq\left\|\sum_{q=1}^Q \sum_{l=1}^{L/2} \frac{\alpha_q^2}{4 L^2}\left(4 N \mathbb{E}\left[A_q^{(l)}\right] + \sum_{k_1 = 1}^2 \sum_{k_2 = 1}^2\sum_{k_3 = 1}^2\sum_{k_4 = 1}^2 \left( \mathbb{E}\left[(\bm{s}_{qk_2}^{(l)\top}\bm{c}_{qk_3}^{(l)})(\bm{s}_{qk_1}^{(l)}\bm{c}_{qk_4}^{(l)\top} + \bm{c}_{qk_1}^{(l)}\bm{s}_{qk_4}^{(l)\top})\right] \right)\right)\right\|_2 \\
    & \leq \frac{2 C}{L}\left\|\sum_{q=1}^Q \alpha_q\left(\frac{N}{4} \mathbb{E}\left[A_q^{(l)}\right] + \frac{1}{16}\sum_{k_1 = 1}^2 \sum_{k_2 = 1}^2\sum_{k_3 = 1}^2\sum_{k_4 = 1}^2 \left( \mathbb{E}\left[(\bm{s}_{qk_2}^{(l)\top}\bm{c}_{qk_3}^{(l)})(\bm{s}_{qk_1}^{(l)}\bm{c}_{qk_4}^{(l)\top} + \bm{c}_{qk_1}^{(l)}\bm{s}_{qk_4}^{(l)\top})\right] \right)\right)\right\|_2 \\
    & \leq \frac{2 C}{L}\left(N\left\|\mathbf{K}_{\mathrm{ngsm}}\right\|_2\!+\!\frac{1}{16}\sum_{k_1 = 1}^2 \sum_{k_2 = 1}^2\sum_{k_3 = 1}^2\sum_{k_4 = 1}^2\sum_{q=1}^Q \alpha_q\left\|\mathbb{E}\left[(\bm{s}_{qk_2}^{(l)\top}\bm{c}_{qk_3}^{(l)})(\bm{s}_{qk_1}^{(l)}\bm{c}_{qk_4}^{(l)\top} \!+\! \bm{c}_{qk_1}^{(l)}\bm{s}_{qk_4}^{(l)\top})\right]\right\|_2\right)  \text{ (triangle inequality)}\\
    & \leq \frac{2C}{L}\left(N\left\|\mathbf{K}_{\mathrm{ngsm}}\right\|_2\!+\!\frac{1}{16}\sum_{k_1 = 1}^2 \sum_{k_2 = 1}^2\sum_{k_3 = 1}^2\sum_{k_4 = 1}^2\sum_{q=1}^Q \alpha_q \mathbb{E}\left[\left\|(\bm{s}_{qk_2}^{(l)\top}\bm{c}_{qk_3}^{(l)})(\bm{s}_{qk_1}^{(l)}\bm{c}_{qk_4}^{(l)\top} \!+\! \bm{c}_{qk_1}^{(l)}\bm{s}_{qk_4}^{(l)\top})\right\|_2\right]\right) \text{ (Jensenâ€™s inequality)}\\
    & \leq \frac{2C}{L}\left(N\left\|\mathbf{K}_{\mathrm{ngsm}}\right\|_2+ \frac{1}{16}\sum_{k_1 = 1}^2 \sum_{k_4 = 1}^2 4 * \frac{N}{2} \sum_{q=1}^Q \alpha_q \mathbb{E}\left[\left\|(\bm{s}_{qk_1}^{(l)}\bm{c}_{qk_4}^{(l)\top} + \bm{c}_{qk_1}^{(l)}\bm{s}_{qk_4}^{(l)\top})\right\|_2\right]\right) \qquad \qquad   \left( | (\bm{s}_{qk_2}^{(l)\top}\bm{c}_{qk_3}^{(l)}) | \le \frac{N}{2} \right) \\
    & \leq \frac{2C N}{L}\left(\left\|\mathbf{K}_{\mathrm{ngsm}}\right\|_2+\frac{1}{16} * \frac{N}{2} * 16  C \sqrt{Q} \right), 
    \end{aligned}
\end{equation}
where the last inequality is because that 
\begin{equation}
% \setlength{\abovedisplayskip}{3.5pt}
% \setlength{\belowdisplayskip}{3.4pt}
    \mathbb{E}\left[\left\|(\bm{s}_{qk_1}^{(l)}\bm{c}_{qk_4}^{(l)\top} + \bm{c}_{qk_1}^{(l)}\bm{s}_{qk_4}^{(l)\top})\right\|_2\right] = \sup_{\|\bm{v}\|_2^2 =1} \mathbb{E}\left[ \left\| \bm{v}^\top (\bm{s}_{qk_1}^{(l)}\bm{c}_{qk_4}^{(l)\top} + \bm{c}_{qk_1}^{(l)}\bm{s}_{qk_4}^{(l)\top}) \bm{v} \right\|_2\right] \le N, 
\end{equation}
and $\sum_{q=1}^Q \alpha_q \le C \sqrt{Q}$ by the Cauchyâ€“Schwarz inequality.

\paragraph{Step 4 :}We can now apply the derived upper bounds, given by Eqs.~\eqref{eq:upper_bound_e_l_q} and \eqref{eq:variance_bound}, to \( H \) and \( v(\mathbf{E}) \) in Lemma \ref{lemma:matrix_bernstein}, 
\begin{equation}
% \setlength{\abovedisplayskip}{3.5pt}
% \setlength{\belowdisplayskip}{3.4pt}
\begin{aligned}
    & {P} \left(\left\|\hat{\mathbf{K}}_{\mathrm{ngsm}} - \mathbf{K}_{\mathrm{ngsm}} \right\|_2 \geq \epsilon\right) \leq N \exp \left(\frac{-3 \epsilon^2 L }{2N C \left(6\left\| \mathbf{K}_{\mathrm{ngsm}} \right\|_2 + 3 N C \sqrt{Q}+8 \epsilon\right)}\right),
\end{aligned}
\end{equation}
which completes the proof of Theorem \ref{thm:NGSM_RFF_approx}.

\end{proof}

% \textbf{Step 3:} Compute the upper bound of the variance of the sum of of $\mathbf{E}_q^{(l)}$.

\clearpage
\section{Experimental Details}
\label{app:exp_detail}

\subsection{Dataset Description}
This section presents a detailed overview of the datasets used in our experiments, covering the generation process of synthetic data, the description of real-world data, and the applied preprocessing steps.

\subsubsection{Synthetic Data}
\label{app:Synthetic_Data}

The datasets are generated using a 2-view MV-GPLVM with an $S$-shaped latent variable, employing two different kernel configurations: (1) both views use the \MakeUppercase{rbf} kernel, and (2) one view uses the \MakeUppercase{rbf} kernel while the other uses the Gibbs kernel. Detailed descriptions of the kernels are provided below. 

- \textbf{RBF Kernel}: The kernel function is expressed as: 
  \[
  k(\x_1, \x_2) = \ell_o \exp\left(-\frac{\|\x_1 - \x_2 \|^2}{2\ell_l^2}\right)
  \]
  where \(\ell_o = 1\) denotes the outputscale, and \(\ell_l = 1\) represents the lengthscale.

- \textbf{Gibbs Kernel}: As a non-stationary kernel, the kernel function is formulated as: 
  \[
  k(\x_1, \x_2) = \sqrt{\frac{2 \ell_{\x_1} \ell_{\x_2}}{\ell_{\x_1}^2 + \ell_{\x_2}^2}} \exp\left(-\frac{\|\x_1 - \x_2\|^2}{\ell_{\x_1}^2 + \ell_{\x_2}^2}\right)
  \]
  In this context, \(\ell_\x\) is dynamic length scales derived from the positions of the input point \(\x\), specifically defined as:
  \[
  \ell_{\x} = \exp\left(-0.5 \cdot \|\x\|\right).
  \]


\subsubsection{Real-World Data}
\label{app:real_dataset_description}


\begin{table*}[h!]
\caption{Description of real-world datasets. \vspace{-.1in}}
\label{table:dataset_description}
\centering
% 
\setlength{\tabcolsep}{4.0mm}
{
\scalebox{0.71}{
\begin{tabular}{c cccccccc}

DATASET & \# SAMPLES (N) & \# DIMENSIONS (M) & \# LABELS \\ \midrule 
\rowcolor[HTML]{f2f2f2} 
BRIDGES    & 2,000 & 2 & 2   \\

CIFAR   & 60,000 & 32 $\times$ 32  & 10   \\
\rowcolor[HTML]{f2f2f2} 
R-CIFAR   & 2,000 & 20 $\times$ 20 & 5   \\

MNIST     & 70,000  & 28 $\times$ 28 & 10   \\
\rowcolor[HTML]{f2f2f2} 
R-MNIST    & 1,000 & 28 $\times$ 28 & 10   \\

NEWSGROUPS & 20,000 & 19  & 3   \\
\rowcolor[HTML]{f2f2f2} 
YALE       & 165 & 32 $\times$ 32  & 15   \\
CHANNEL       & 20,000 & 64  & 10   \\
\rowcolor[HTML]{f2f2f2} 
BRENDAN       & 2,000 & 20 $\times$ 28 & - 
\\ \midrule 

\end{tabular}
}
} 
\end{table*}


The detailed preprocessing steps and descriptions of the real-world datasets are provided below. For convenience, key information about the datasets is summarized in Table \ref{table:dataset_description}.

\begin{itemize}
    \item{
        1) \underline{BRIDGES:} This dataset is a collection of data documenting the daily bicycle counts crossing four East River bridges in New York City\footnote{\url{https://data.cityofnewyork.us/Transportation/Bicycle-Counts-for-East-River-Bridges/gua4-p9wg}} (Brooklyn, Williamsburg, Manhattan, and Queensboro). We classify the data into weekdays and weekends, treating these as binary labels. This classification aims to examine the variations in bicycle counts on weekdays versus weekends, exploring whether significant differences exist in the traffic patterns between the two.
    }

     \item{
        2) \underline{CIFAR:}  This dataset comprises 60,000 color images with a resolution of 32Ã—32 pixels, categorized into 10 distinct classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. We convert the color image into grayscale image for training.
    }

     \item{
        3) \underline{R-CIFAR:} We sample this dataset from the CIFAR dataset, specifically selecting five categories: airplane, automobile, bird, cat, and deer. For each category, 400 images are sampled, and each 32Ã—32 pixel image is converted into a 20Ã—20 pixel image.
    }

     \item{
        4) \underline{MNIST:} It is a classic handwritten digit recognition dataset. It consists of 70,000 grayscale images with a resolution of 28Ã—28 pixels, divided into 60,000 training samples and 10,000 testing samples. Each image represents a handwritten digit ranging from 0 to 9.
    }

     \item{
        5) \underline{R-MNIST:} We select 1,000 randomly handwritten digit images from the classic MNIST dataset.
    }

     \item{
        6) \underline{NEWSGROUPS:} It is a dataset for text classification, containing articles from multiple newsgroups\footnote{\url{http://qwone.com/~jason/20Newsgroups/}}. We restrict the vocabulary to words that appear within a document frequency range of 10\% to 90\%. For our analysis, we specifically select text from three classes: comp.sys.mac.hardware, sci.med, and alt.atheism.

    }

    \item{
        7) \underline{YALE:} The Yale Faces Dataset\footnote{\url{http://vision.ucsd.edu/content/yale-face-database}} consists of face images from 15 different individuals, captured under various lighting conditions, facial expressions, and viewing angles.      
    }

    
    \item{
        8) \underline{BRENDAN:} The dataset contains 2,000 photos of Brendan's face\footnote{\url{https://cs.nyu.edu/~roweis/data/frey_rawface.mat}}.      
    }

\end{itemize}


\subsubsection{Channel Data}
\label{app:channel_data}
For generating the channel data, the parameter settings of the channel simulatorâ€”QUAsi Deterministic RadIo channel GenerAtor (QUADRIGA)\footnote{\url{https://quadriga-channel-model.de}}, are shown in Table \ref{table:channel_settings}.

\begin{table*}[t]
\caption{Parameters settings of QUADRIGA.}
\label{table:channel_settings}
\centering
% 
\setlength{\tabcolsep}{4.0mm}
{
\scalebox{0.71}{
\begin{tabular}{c c}

PARAMETER DESCRIPTION  & VALUES    \\ \midrule 

\multicolumn{2}{c}{\textbf{Overall Setup}} \\ \midrule
\rowcolor[HTML]{f2f2f2} 
\# samples &  1000 \\
\# user equipment (UE) &  10 \\
\rowcolor[HTML]{f2f2f2} 
\# receive antennas &  1 \\
Moving speed (km/h) & 30 \\
\rowcolor[HTML]{f2f2f2} 
Proportion of indoor UEs &  1  \\
Time sampling interval (seconds) & 5e-3 \\
\rowcolor[HTML]{f2f2f2} 
Total duration of sampling (seconds) & 5 \\
Channel type & 3GPP\_3D\_UMa \\

\midrule 
\multicolumn{2}{c}{\textbf{Channel Configuration}} \\ \midrule
\rowcolor[HTML]{f2f2f2} 
Center frequency (\MakeUppercase{h}z) & 1.84e9 \\
Use random initial phase &  False  \\
\rowcolor[HTML]{f2f2f2} 
Use geometric polarization & False  \\ 
Use spherical waves &  False  \\
\rowcolor[HTML]{f2f2f2} 
Show progress bars & False  \\

\midrule 
\multicolumn{2}{c}{\textbf{Base Station (BS) Antenna Configuration}} \\ \midrule
\rowcolor[HTML]{f2f2f2} 
\# vertical elements per antenna & 4 \\
\# horizontal elements per antenna & 1 \\
\rowcolor[HTML]{f2f2f2} 
\# rows in the antenna array & 2 \\
\# columns in the antenna array & 8 \\
\rowcolor[HTML]{f2f2f2} 
Electrical tilt angle (degrees) & 7 \\
\# carriers & 1 \\
\rowcolor[HTML]{f2f2f2} 
\# transmit antennas & 32 \\

\midrule 
\multicolumn{2}{c}{\textbf{UE Antenna Configuration}} \\ \midrule
\rowcolor[HTML]{f2f2f2} 
UE antenna array & omni \\
\# subcarriers & 1 \\
\rowcolor[HTML]{f2f2f2} 
Subcarrier spacing (\MakeUppercase{h}z) & 1e6 \\
\# loops (simulations) & 1 \\
\rowcolor[HTML]{f2f2f2} 
Minimum UE distance from BS (meters) & 35 \\ 
Maximum UE distance from BS (meters) & 300 \\

\midrule 
\multicolumn{2}{c}{\textbf{Layout Configuration}} \\ \midrule
\rowcolor[HTML]{f2f2f2} 
\# base stations & 1 \\
Base station position (x, y, z) (meters) & (0, 0, 30) \\
\rowcolor[HTML]{f2f2f2} 
UE movement path length (meters) & 41.667 \\

\end{tabular}
}
}\vspace{-.06in}
\end{table*}



\subsection{Benchmark Methods}
\label{app:implementatation}

In Table \ref{table:benchmark_methods}, we provide a description of the benchmark methods, including their corresponding references and implementation code, to enhance reproducibility. 

\begin{table*}[t]
\caption{Descriptions of benchmark methods.\vspace{-.1in}}
\label{table:benchmark_methods}
\centering
% 
\setlength{\tabcolsep}{4.0mm}
{
\scalebox{0.71}{
\begin{tabular}{c c c}

METHOD & REFERENCE & IMPLEMENTATION CODE \\ \midrule 
\rowcolor[HTML]{f2f2f2} 
PCA    & \cite{wold1987principal} & Using the \texttt{scikit-learn} library \citep{sklearn_api}. \\

LDA   & \cite{blei2003latent} & Using the \texttt{scikit-learn} library \citep{sklearn_api}.  \\
\rowcolor[HTML]{f2f2f2} 
ISOMAP   & \cite{balasubramanian2002isomap}  & Using the \texttt{scikit-learn} library \citep{sklearn_api}.   \\

HPF     & \cite{gopalan2015scalable}  & \url{https://github.com/david-cortes/hpfrec}   \\

\rowcolor[HTML]{f2f2f2} 
BGPLVM    & \cite{titsias2010bayesian} & \url{https://github.com/SheffieldML/GPy}   \\

GPLVM-SVI & \cite{lalchand2022generalised} & \url{https://github.com/vr308/Generalised-GPLVM}   \\
\rowcolor[HTML]{f2f2f2} 
VAE       & \cite{kingma2013auto} & \url{https://github.com/pytorch/examples/blob/main/vae/main.py}   \\

NBVAE       & \cite{zhao2020variational} & \url{https://github.com/ethanheathcote/NBVAE} 
\\ \rowcolor[HTML]{f2f2f2} 
DCA       & \cite{eraslan2019single} & \url{https://github.com/theislab/dca}   \\

CVQ-VAE   & \cite{zheng2023online} & \url{https://github.com/lyndonzheng/CVQ-VAE}   \\
\rowcolor[HTML]{f2f2f2} 
RFLVM    & \cite{zhang2023bayesian,gundersen2021latent} & \url{https://github.com/gwgundersen/rflvm}   \\ 
DGPLVM    & \cite{salimbeni2017doubly} & \url{https://github.com/UCL-SML/Doubly-Stochastic-DGP}   \\ 
\rowcolor[HTML]{f2f2f2} 
ARFLVM    & \cite{li2024preventing} & \url{https://github.com/zhidilin/advisedGPLVM}   \\ 
MVAE    & \cite{wu2018multimodal} & \url{https://github.com/mhw32/multimodal-vae-public}   \\ 
\rowcolor[HTML]{f2f2f2} 
MMVAE    & \cite{shi2019variational,mao2023multimodal} & \url{https://github.com/OpenNLPLab/MMVAE-AVS} \\
\midrule 
\end{tabular}
}
}\vspace{-.06in}
\end{table*}



\subsection{Hyperparameter Settings}
\label{app:hyperparameter_settings}

\revise{Figure \ref{fig:hyperparameter} depicts the latent manifold learning outcomes of NG-RFLVM for varying values of $Q$ and $L$ on synthetic single-view data. Figure \ref{fig:heatmap} shows a heatmap of $R^2$ scores, quantifying the similarity between the learned and ground truth latent variables, with values closer to 1 indicating better alignment \citep{li2024preventing}. The results demonstrate that larger values of $Q$ and $L$ typically improve model performance, albeit at the cost of higher computational complexity. To balance computational efficiency with latent representation quality, we select $Q = 2$ and $L = 50$. The default hyperparameter settings are summarized in Table \ref{table:hyperparameter_settings}.} 


\begin{table*}[t]
\caption{Default Hyperparameter Settings.}
\label{table:hyperparameter_settings}
\centering
\setlength{\tabcolsep}{4.0mm}
{
\scalebox{0.71}{
\begin{tabular}{c c}
\toprule
PARAMETER DESCRIPTION & VALUES \\ 
\midrule 

\multicolumn{2}{c}{\textbf{NG-SM Kernel Setup}} \\ 
\midrule
\rowcolor[HTML]{f2f2f2} 
\# Mixture densities ($Q$)      & 2 \\
Dim. of random feature ($L/2$)    & 50 \\
\rowcolor[HTML]{f2f2f2} 
Dim. of latent space ($D$)      & 2 \\

\midrule 
\multicolumn{2}{c}{\textbf{Optimizer Setup (Adam)}} \\ 
\midrule
\rowcolor[HTML]{f2f2f2} 
Learning rate                 & 0.01 \\
Beta                          & (0.9, 0.99) \\
\rowcolor[HTML]{f2f2f2} 
\# Iterations                 & 10000 \\ 
\bottomrule
\end{tabular}
}
}\vspace{-.02in}
\end{table*}




\subsection{Wall-Time Comparison}
\label{app:wall_time}

\revise{Figure \ref{fig:wall_time} presents the wall-time for model learning across various benchmark methods with increasing sample size $N$ on the MNIST dataset. Mean and standard deviation of wall-time are computed over five experiments. The results highlight the computational efficiency of our approach, which significantly outperforms MV-GPLVM due to the incorporation of RFFs. While MV-DGPLVM benefits from the inducing point method to reduce the computational cost, its multi-layer \MakeUppercase{gp} structure still results in higher wall-time compared to our method. Additionally, our approach achieves computational efficiency comparable to the \MakeUppercase{mvae}, which is well-known for its scalability in comparison to \MakeUppercase{gp}-based models.} 



\begin{figure}[t!]
    \centering
    % ç¬¬ä¸€å¼ å›¾ç‰‡
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/heatmap.pdf}
        \caption{Heatmap of $R^2$ in latent manifold learning.}
        \label{fig:heatmap}
    \end{minipage}
    \hfill % å¢žåŠ æ°´å¹³é—´è·
    % ç¬¬äºŒå¼ å›¾ç‰‡
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/wall_time.pdf}
        \caption{Wall-time for model fitting.}
        \label{fig:wall_time}
    \end{minipage}
\end{figure}



\subsection{Additional Experiments on Multi-View Datasets}
\label{app:mv_experiments}

\revise{To explore more general scenarios, we conducted experiments on synthetic datasets and multi-view MNIST, both containing a large number of views.} 

\subsubsection{Multi-View Synthetic Data}
\label{app:mv_synthetic}

\revise{As the number of views increases, Figures \ref{fig:mv_synthetic} and \ref{fig:r2_vs_view} present the unified latent representations generated by our method and the corresponding $R^2$ scores, respectively. These results show that with more views, the unified latent representations learned by our model progressively align more closely with the ground truth.} 

\begin{figure}[t!]
    \centering
    % ç¬¬ä¸€å¼ å›¾ç‰‡
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/mv_toy.pdf}
        \caption{Latent manifold learning results with multiple views.}
        \label{fig:mv_synthetic}
    \end{subfigure}
    \hfill % å¢žåŠ æ°´å¹³é—´è·
    % ç¬¬äºŒå¼ å›¾ç‰‡
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/r2_vs_view.pdf}
        \caption{$R^2$ against the number of views.}
        \label{fig:r2_vs_view}
    \end{subfigure}
    \vspace{-0.1in} % è°ƒæ•´å›¾åƒå’Œæ–‡å­—ä¹‹é—´çš„åž‚ç›´é—´è·
    \caption{Comparison of latent manifold learning and $R^2$ against the number of views.}
    \label{fig:combined_figure}
\end{figure}


\subsubsection{Multi-View MNIST}
\label{app:mv_mnist}

\revise{We generated a four-view dataset derived from the MNIST dataset using a rotation operation, as illustrated on the left-hand side of Figure \ref{fig:mv_mnist}, alongside reconstruction results obtained with our method. The right-hand side of Figure \ref{fig:mv_mnist} displays the KNN accuracy evaluated using the latent variables learned by various methods on \MakeUppercase{mv-mnist}. These results highlight that our approach not only achieves superior performance in downstream classification tasks but also effectively reconstructs data form each view through the shared latent space.} 


\begin{figure*}[t!]
    \centering
    \caption{(\textbf{Left}) \MakeUppercase{mv-mnist} reconstruction task. (\textbf{Right}) Classification accuracy (\%) evaluated using KNN classifier with five-fold cross-validation. Mean and standard deviation of the accuracy is computed over five experiments. 
    \vspace{-.15in}
    }
    \centering
    %{ \small \textbf{channel bar chart}}
    %\\[0.5em]
    \includegraphics[width=.99\linewidth]{img/mv_mnist.pdf} 
    \vspace{-0.1in}
    \label{fig:mv_mnist}
\end{figure*}




\subsection{Missing Data Imputation}
\label{app:miss_data}


In this section, we evaluate our model's capability for missing data imputation using the single-view datasets MNIST and BRENDAN. We randomly set various proportions of the observed data \(\vy\) to zero (denoted as \(\vy_{\text{obs}}\)), ranging from 0\% to 60\%. Using the incomplete datasets \(\vy_{\text{obs}}\), our model estimates the underlying latent variable \(\vx\), and the missing values are imputed as \(\hat{\vy}_{\text{miss}} = \mathbb{E}[\vy_{\text{miss}} \mid \vx, \vy_{\text{obs}}]\). Figures \ref{fig:missing_data_mnist} and \ref{fig:missing_data_brendan} illustrate the reconstruction tasks on the MNIST and BRENDAN datasets with varying proportions of missing values, showing superior ability of our model to restore missing pixels.\footnote{In the future, we particularly interested in expressing a factorized latent space where each view is paired with an additional private space, alongside a shared space to capture unaligned variations across different views \citep{damianou2012manifold, damianou2021multi}. } 


% \begin{table*}[t]
% \caption{\MakeUppercase{mse} between reconstructed missing data and ground truth on the MNIST and BRENDAN datasets.
% \label{table:missing_data}}
% \vspace{-2ex}
% \small
% \setlength{\tabcolsep}{1.05mm}{
%     \scalebox{.92}{
%         \begin{tabular}{c|| cccc| cccc| cccc| cccc}
%         \toprule
%         \multirow{2}{*}{\MakeUppercase{dataset}}   
%         & \multicolumn{4}{c|}{\MakeUppercase{ours}}  
%         & \multicolumn{4}{c|}{\MakeUppercase{arflvm}}      
%         & \multicolumn{4}{c|}{DGPLVM}  
%         & \multicolumn{4}{c}{\MakeUppercase{vae}}   
%         \\  
%         \cmidrule(lr){2-5}
%         \cmidrule(lr){6-9}
%         \cmidrule(lr){10-13}
%         \cmidrule(lr){14-17}
%         &0\% &10\% &30\% &60\%   &0\% &10\% &30\% &60\%   &0\% &10\% &30\% &60\%   &0\% &10\% &30\% &60\%
%         \\ \midrule  \midrule
%         \multirow{1}{*}{MNIST}  
%         & \textbf{0.024}& \textbf{0.028} & \textbf{0.038} & \textbf{0.068} &0.025 & 0.028 &0.039& 0.068  &0.084& 0.045 &  0.40 & 0.68 & 0.035 & 0.038 &  0.045 & 0.068
%         \\ \midrule \midrule
%         \multirow{1}{*}{BRENDAN}  
%         & \textbf{0.003}& \textbf{0.009} & 0.046 &  0.152 & 0.003 & 0.009 & 0.045 & 0.152&0.010& 0.014 &   0.046 &  0.152 &0.005 & 0.009 &  \textbf{0.043} & \textbf{0.150} 
%         \\ \bottomrule
%         \end{tabular}
%     }
% }
% \vspace{-1ex}
% \end{table*}
\newpage
\begin{figure}[t!]
    \vspace{-.1in}
    \centering
    %{ \small \textbf{channel bar chart}}
    %\\[0.5em]
     \caption{Latent manifold learning results with  different Q and L.}
    \includegraphics[width=0.85\linewidth]{img/hyperparameter.pdf} 
   
     %\vspace{-0.2in}
    \label{fig:hyperparameter}
\end{figure}
\newpage

\begin{figure}[t!]
    \vspace{-.1in}
    \centering
    \includegraphics[width=1.0\linewidth]{img/missing_data_mnist.pdf}
    \caption{MNIST reconstruction task.}
    \vspace{-0.2in}
    \label{fig:missing_data_mnist}
    
    \vspace{1.0in} % è°ƒæ•´ä¸¤ä¸ªå›¾ä¹‹é—´çš„åž‚ç›´è·ç¦»
    \includegraphics[width=1.0\linewidth]{img/missing_data_brendan.pdf}
    \caption{BRENDAN reconstruction task.}
    \vspace{-0.2in}
    \label{fig:missing_data_brendan}
\end{figure}


