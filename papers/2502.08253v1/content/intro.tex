\vspace{-0.08in}
\section{Introduction}
\label{sec:introduction}
\vspace{-0.05in}

Multi-view representation learning 
% \footnote{In typical setups, multi-view depict different aspects of objects within the same modality. In contrast, our multi-view approach is more general.} 
aims to construct a unified latent representation by integrating multiple modalities and aspects of the observed data  \citep{li2018survey, wang2015deep}. The learned representation captures %comprehensive 
inter-view correlations within observations. By sharing information between each view of the data, we can obtain a much richer latent representation of the data compared to modeling each view independently which is crucial for
%without losing view-specific information, ultimately allowing for 
handling complex datasets \citep{zhang2018generalized, wei2022self, lu2019see}. For example, modeling video data which involves both visual frames and audio signals  \citep{hussain2021comprehensive}, or developing clinical diagnostic systems that incorporates the patients' various medical records \citep{yuan2018multi}. %Thus, multi-view fusion is crucial for recovering underlying patterns when we observe multiple aspects of the observed data.  

%\vspace{-0.02in}
Two approaches have emerged as standard for multi-view learning: neural network-based methods, exemplified by multi-view variational auto-encoders (MV-VAEs) \citep{wu2018multimodal, mao2023multimodal}, and Gaussian process-based methods, represented by multi-view Gaussian process latent variable models (MV-GPLVMs) \citep{li2017shared, sun2020multi}. However, MV-VAEs are susceptible to posterior collapse, where the encoder fails to learn meaningful unified latent representations. In contrast, MV-GPLVMs often lack the kernel flexibility needed to capture such representations \citep{li2017shared} and are computationally demanding to train, particularly for large-scale datasets commonly encountered in multi-view scenarios \citep{sun2020multi}. To overcome these issues, we propose an expressive and efficient multi-view oriented GPLVM. Our contributions are:
%\vspace{-0.1in}
% Nevertheless, the development of MV-GPLVMs still lags behind that of GPLVMs. Nowadays, several efforts have been directed towards improving both the flexibility and scalability of GPLVM \citep{lalchand2022generalised, li2024preventing, zhang2023bayesian}. One of the representative works is \citep{li2024preventing}, namely \textsl{advised}RFLVM (a.k.a. \MakeUppercase{arflvm}). This method integrates the spectral mixture (\MakeUppercase{sm}) kernel to enhance model flexibility, in conjunction with a differentiable random Fourier feature (RFF) kernel approximation to ensure scalability. Table \ref{table:comparison} compares our proposed models with the key related works. 

% However, despite significant advancements in MV-GPLVMs and GPLVMs, these models still fall short in handling complex multi-view representational learning tasks that must not only manage relatively large-scale data, as it is collected from multiple views, but also able to capture each view-specific properties to avoid degrading the quality of the shared/unified latent representations. To be specific, MV-GPLVMs \citep{sun2020multi, li2017shared} suffer from high computational costs due to the lack of advanced \MakeUppercase{gp} approximation techniques such as RFF. On the other hand, GPLVMs \citep{lalchand2022generalised, li2024preventing, zhang2023bayesian} experience inadequate kernel expressiveness caused by the stationary assumption, which will overlook view-specific properties such as long-range correlations. To overcome those issues, we propose a multi-view oriented GPLVM in this paper. Our contributions are summarized as follows. 
% \vspace{-0.02in}
% \textbf{Contributions:}
% \vspace{-0.15in}
%\vspace{-0.13in}
\begin{itemize}
    \item We establish a novel duality between the spectral density and the kernel function, deriving the expressive and generic \MakeUppercase{n}ext-\MakeUppercase{g}en \MakeUppercase{s}pectral \MakeUppercase{m}ixture (NG-SM) kernel, which, by modeling spectral density as dense Gaussian mixtures, can approximate any continuous kernel with arbitrary precision given enough mixture components. Building on this, we design a novel MV-GPLVM for multi-view scenarios, capable of capturing the unique characteristics of each view, leading to an informative unified latent representation.  
    \vspace{-0.02in}
    \item To enhance the computational efficiency, we design a unique unbiased RFF approximation for the NG-SM kernel that is differential w.r.t. kernel hyperparameters. By integrating this RFF approximation with an efficient two-step reparameterization  trick,  we enable efficient and scalable learning of kernel hyperparameters and unified latent representations within the variational inference framework \citep{kingma2013auto}, making the proposed model well-suited for multi-view scenarios. 
    \vspace{-0.02in}
    \item We validate our model on a range of cross-domain multi-view datasets, including synthetic, image, text, and wireless communication data. The results show that our model consistently outperforms various state-of-the-art (\MakeUppercase{sota}) MV-VAEs, MV-GPLVMs, and multi-view extensions of \MakeUppercase{sota} GPLVMs in terms of generating informative unified latent representations. 
\end{itemize}
% \vspace{-0.02in}
% \vspace{-0.13in}
% In the following sections, we provide the necessary background in \S~\ref{sec:background}, present our proposed methodology for developing a novel MV-GPLVM in \S~\ref{sec:method}, and conduct extensive simulations based on this method in \S~\ref{sec:experiments}. Finally, \S~\ref{sec:conclusion} summarizes the conclusions drawn from our work. 
% \vspace{0.05in}

% \textcolor{red}{add the remaining paper organization description here?}