\vspace{-0.09in}
\section{Experiments}
\vspace{-0.09in}
\label{sec:experiments}

In \S~\ref{subsec:single_view}, we demonstrate the capability of our method in representation learning using various single-view real-world datasets. Subsequently, we validate the superior performance of our model on multiple cross-domain multi-view datasets, including synthetic data (\S~\ref{subsec:toy_example}), image and text data (\S~\ref{subsec: image_text_data}), and wireless communication data (\S~\ref{subsec: wireless}). Further experimental details are provided in App.~\ref{app:exp_detail}, with specific focus on benchmark implementations in App.~\ref{app:implementatation} and the hyperparameter selection process in App.~\ref{app:hyperparameter_settings}. Additionally, we present supplementary simulation results to highlight the scalability (App.~\ref{app:wall_time}) and robustness of our method in handling a large number of views (App.~\ref{app:mv_experiments}) and missing data imputation (App.~\ref{app:miss_data}).

%%%%%%%%%%%%%%%%%%%%%%%%%
\input{table/multi_view}
%%%%%%%%%%%%%%%%%%%%%%%%%%


\vspace{-0.09in}
\subsection{Single-View Data}
\label{subsec:single_view}
\vspace{-0.09in}
% This section demonstrates our method's superior capability to capture the underlying structure in various single-view real-world data sets, serving as the foundation for multi-view learning.
%We first demonstrate our model's superior capability to capture the underlying structure in various single-view real-world data sets before further analyzing the case of multi-view learning.
In this section, we first examine the following single-view data types: images (\MakeUppercase{r-mnist}, \MakeUppercase{yale}, \MakeUppercase{r-cifar}), text (\MakeUppercase{newsgroups}), and structured data (\MakeUppercase{bridges}). 
To accommodate the high computational costs of RFLVM, the dataset sizes for \MakeUppercase{cifar} and \MakeUppercase{mnist} are reduced and denoted with the prefix `\MakeUppercase{r}-' (see details in Appendix~\ref{app:real_dataset_description}).



%\vspace{-0.09in}
The results\footnote{%See detailed implementation setups in App.~\ref{app:real_dataset_description}.
Comparison benchmarks include various classic dimensionality reduction approaches \citep{wold1987principal, gopalan2015scalable, blei2003latent, bach2002kernel}, GPLVM-based approaches \citep{lalchand2022generalised, li2024preventing, zhang2023bayesian, titsias2010bayesian}, and \MakeUppercase{vae}-based models \citep{kingma2013auto, zhao2020variational,eraslan2019single,zheng2023online}. See further details in Table~\ref{table:benchmark_methods}.} in Table~\ref{table:KNN_single_view} present the mean and standard deviation of classification accuracy from a five-fold cross-validated K-nearest neighbor (KNN) classification, performed on the learned latent variables for each dataset and method. 

Based on this experiment, our method is capable of capturing informative latent representations across various datasets due to its superior modeling and learning capacities. %On the contrary, 
The inferior performance of classic approaches is primarily due to their insufficient learning capacity. The GPLVM variants perform slightly inferior to our approach, primarily due to the assumption of kernel stationarity.%, which may be unsuitable for certain complex data patterns. 

While DGPLVM addresses the modeling limitation by incorporating deep structures, its performance is hindered by more complex model optimization processes \citep{dunlop2018deep}. Similarly, \MakeUppercase{vae}-based methods inherently suffer from posterior collapse, making them prone to generating uninformative latent representations, partly due to overfitting \citep{sonderby2016ladder, bowman2016generating}.

% Consequently, their performance on diverse data sets is inferior to ours.

\begin{figure}[t!]
    \vspace{-.1in}
    \centering
    \includegraphics[width=0.98\linewidth]{img/mv_toy_example.pdf} 
    \vspace{-.05in}
    \caption{
    Comparison of learned latent variables and kernel matrices with the ground truth for two-view datasets. Each dashed box contains three rows: the first row shows the latent variables, while the second and third rows represent the kernel matrices for the two views. 
    \textbf{Top}: Both views are generated using an \MakeUppercase{rbf} kernel. \textbf{Bottom}: The first view is generated using an \MakeUppercase{rbf} kernel, and the second view is generated using a Gibbs kernel.
    \vspace{-.1in}
    }
     
    \label{fig:MV_toy_example}
\end{figure}
%

%%%%%%%%%-------------------------------------------------------------------------
\begin{figure*}[t!]
    \centering
    \caption{(\textbf{Left}) Classification accuracy , expressed as a percentage (\%), is evaluated using KNN, Logistic Regression (LR) and SVM classifiers with five-fold cross-validation. Mean and standard deviation are computed over five experiments. (\textbf{Right}) The average mean squared error (\MakeUppercase{mse}) of the reconstructed channel data compared to the original channel data across different latent variable dimensions in five experiments.
    \vspace{-.15in}
    }
    \centering
    %{ \small \textbf{channel bar chart}}
    %\\[0.5em]
    \includegraphics[width=.99\linewidth]{img/channel.pdf} 
    \vspace{-0.09in}
    \label{fig:channel_bar_chart}
\end{figure*}
%
\vspace{-0.09in}
\subsection{Synthetic Data}
\label{subsec:toy_example}
\vspace{-0.09in}
%%%%%%%%%-------------------------------------------------------------------------
We further demonstrate the impact of kernel expressiveness on manifold learning and highlight the expressive power of the proposed NG-SM kernel. 
To this end, we synthesized two datasets using a two-view MV-GPLVM with the $S$-shape $\vx$, based on different kernel configurations: (1) both views using the stationary \MakeUppercase{rbf} kernel, and (2) one view using the \MakeUppercase{rbf} kernel and the other using the non-stationary Gibbs kernel \citep{williams2006gaussian}. {See more details in App.~\ref{app:Synthetic_Data}.}. For benchmark methods, we use the MV-DGPLVM \citep{sun2020multi} and the multi-view extension of the SOTA \MakeUppercase{arflvm} \citep{li2024preventing}, namely, \MakeUppercase{mv-arflvm}.

%\vspace{-0.09in}
The manifold learning and kernel learning results across different methods for the two datasets are presented in Figure~\ref{fig:MV_toy_example}. The results for \MakeUppercase{mv-arflvm} indicate that if the model fails to capture the non-stationary features of one view, then significant distortions arise in the unified latent variables. In turn this degrades the modelâ€™s ability to learn stationary features from other views. %that it would otherwise be able to capture. 
In contrast, %it can be observed that 
both the latent variables and kernel matrices learned by our method are closer to the ground truth compared to the benchmark methods, especially in non-stationary kernel setting. Thus, these results demonstrate the importance of kernel flexibility for the MV-GPLVM. %Those results highlight the significant importance of kernel expressiveness in MV-GPLVM variants. 
% \vspace{-0.09in}
% For \MakeUppercase{mv-arflvm}, the kernel learning results suggest that the significant performance degradation in manifold learning can be attributed to the limited kernel expressiveness in the \MakeUppercase{sm} kernel \citep{wilson2013gaussian}, which struggles to capture the kernel matrix with a non-stationary Gibbs kernel. This result highlights the importance of kernel expressiveness in MV-GPLVM variants, as any deviation of the learned mapping function from the underlying one for any view can lead to significant distortion in the final unified latent variables.
% \vspace{-0.09in} {\color{blue} what is the kernel matrix of DGPLVM in Fig~1?}

For the MV-DGPLVM, we select the best latent representations from all \MakeUppercase{gp} layers and plot the corresponding kernel Gram matrix. However, the kernel learning result may be less meaningful, as the flexibility of model stems from using deep GPs rather than the kernel choice. The deep architecture of MV-DGPLVM exhibits a notable capability to capture non-stationary components, yielding a relatively ``high-quality'' latent representations; however, this comes at the cost of significant computational complexity. In contrast, our method achieves superior performance by adopting a more efficient architecture, significantly reducing computational overhead (see Figure~\ref{fig:wall_time} for wall-time comparison). Moreover, by integrating advanced kernel learning techniques, our approach retains considerable flexibility in modeling non-stationary patterns.

% The deep architecture enables MV-DGPLVM to capture non-stationary elements.%, offering the potential to model the underlying mapping for each view. 
% Unfortunately, due to the large number of parameters and complex hierarchical structure \citep{dunlop2018deep}, MV-DGPLVM often suffers from computational and overfitting issues in practice--resulting in inferior performance, as demonstrated in Figure~\ref{fig:MV_toy_example}.

%%%%%%%%%-------------------------------------------------------------------------
\vspace{-0.09in}
\subsection{Multi-View Image and Text Data}
\vspace{-0.09in}
\label{subsec: image_text_data}
%This subsection extends \S~\ref{subsec:single_view} and 
We further illustrate the capability of our model to capture unified latent representations in various multi-view image and text datasets. Specifically, we follow the setup of \citet{wu2018multimodal} and treat the single-view data in \S~\ref{subsec:single_view} and its label as two distinct views. %For comprehensive comparisons, 
\revise{We compare the \MakeUppercase{sota} \MakeUppercase{mv}-GPLVM variants: \MakeUppercase{mv-arflvm}, MV-DGPLVM, MV-GPLVM with the Gibbs kernel (\MakeUppercase{mv-ngplvm}) and the \MakeUppercase{sota} MV-VAE variants: \MakeUppercase{mvae} \citep{wu2018multimodal} and \MakeUppercase{mmvae} \citep{mao2023multimodal}.} After inferring the unified latent variable $\vx$, we perform five-fold cross-validation using two types of classifiers: KNN and support vector machine (\MakeUppercase{svm}). The mean classification accuracy based on various classifiers for each dataset is reported in Table~\ref{table:mv_learning}. 

%\vspace{-0.09in}
From Table~\ref{table:mv_learning}, we can see that our method demonstrates superior performance in estimating the %unified 
latent represntations. For the MV-VAE variants, the %fluctuating and 
inferior performance can be attributed to: (1) optimizing the huge number of neural network parameters leads to model instability, and (2) the posterior collapse issue in the \MakeUppercase{vae} results in an uninformative latent space. %causes the unified latent variable to tend toward being uninformative. 
Both factors are partly due to overfitting, a problem that can be naturally addressed by the MV-GPLVM variants.

%\vspace{-0.09in}
Consequently, the performance of the MV-GPLVM variants is generally superior to that of the MV-VAE. %variants in generating informative unified latent variables. 
\revise{Specifically, the performance improvement of our method compared to \MakeUppercase{mv-arflvm} and \MakeUppercase{mv-ngplvm} is due to our proposed NG-SM kernel.} MV-DGPLVM exhibits high variance in experimental results, reflecting its inherent instability. This is likely due to its large number of parameters, which increases susceptibility to overfitting and sensitivity to initialization.



% Although the deep structure of MV-DGPLVM could allow it to model complex datasets,
% %intricate data patterns and theoretically possesses enough expressiveness to capture complex unified latent variables, 
% in practice it struggles to realize this potential due to the computational complexity of training the model \citep{dunlop2018deep}. %Thus, it performs poorly in generating informative unified latent variables.

\vspace{-0.09in}
\subsection{Wireless Communication Data}
\label{subsec: wireless}
\vspace{-0.09in}

For further evaluations, %the capability of our model in producing informative unified latent representations and its effectiveness in practical applications, 
we test the model in a channel compression task. Specifically, we generate wireless communication channel datasets using a high-fidelity channel simulatorâ€”\MakeUppercase{qua}si \MakeUppercase{d}eterministic \MakeUppercase{r}ad\MakeUppercase{i}o channel \MakeUppercase{g}ener\MakeUppercase{a}tor (\MakeUppercase{quadriga})\footnote{https://quadriga-channel-model.de}. The %testing wireless communication 
environment we considered consists of a base station (\MakeUppercase{bs}) with 32 antennas serving 10 single-antenna user equipments (\MakeUppercase{ue}s), each moving at a speed of 30 km/h. We sample $1,000$ complex-valued channel vectors for each \MakeUppercase{ue} at intervals of $2.5$ ms. The real and imaginary parts of each complex channel vector are treated as two distinct views, with the identifier of the \MakeUppercase{ue} as the label, resulting in a two-view dataset\footnote{See App.~\ref{app:channel_data} for more details regarding the wireless communication scenario settings.} with $N = 10,000$ and $M_v = 32$ for $v = 1, 2$.

\vspace{-0.09in}
To reduce communication overhead, the \MakeUppercase{ue} typically transmits a compressed channel vector to the \MakeUppercase{bs} instead of the full vector. This compressed vector must retain sufficient information to accurately reconstruct the ground-truth channel vectors. %To this end, 
We use the unified latent variables as the compressed channel vector and evaluate both their classification performance and reconstruction capability. The mean and standard error of the classification accuracy, along with the mean square error (\MakeUppercase{mse}) between the reconstructed and ground-truth channel vectors, are shown in Figure~\ref{fig:channel_bar_chart}. \revise{
The results indicate that the KNN accuracy achieved by our method consistently surpasses that of competing methods, while the \MakeUppercase{mse} is consistently lower. These findings highlight the superior performance of our model in channel compression tasks compared to other benchmark approaches.}



% We generate unified latent variables using various methods and perform 5-round classification tasks with three classifiers: KNN, \MakeUppercase{lr}, and \MakeUppercase{svm}. The mean and standard error of classification accuracy are presented in Figure~\ref{fig:channel_bar_chart}. Moreover, 
% The results demonstrate the superior consistency and generalization of our approach in multi-view representation learning, outperforming benchmark methods across multiple domains, including wireless communications, text, and images.





%%%%%%%%%-------------------------------------------------------------------------

% \subsection{More experiments in Appendix}
% \begin{itemize}
%     \item[1.] A lot of figures with missing data; 
%     \item[2.] Parameter selections; 
% \end{itemize}
