
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{table/method_summary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.05in}
\section{Background}\label{sec:background}
\vspace{-0.05in}
% \vspace{-0.02in}
Multi-view learning first emerged in early works with techniques like canonical correlation analysis (\MakeUppercase{cca}) \citep{hotelling1992relations} and its kernelized extensions \citep{bach2002kernel, hardoon2004canonical}, yet their capability is limited in capturing latent representations of complex datasets. \revise{Recent works have incorporated various view-specific variational auto-encoders (\MakeUppercase{vae}s) to address multi-view representation learning \citep{wu2018multimodal, mao2023multimodal,kingma2013auto, shi2019variational, xu2021multi}.} However, these approaches suffer from the posterior collapse, which are inherent in \MakeUppercase{vae}s  \citep{wang2021posterior}, where the encoder collapses to the prior on the latent variables--thus failing to effectively capture the underlying structure of the data.

%\vspace{-0.02in}
The cause of posterior collapse has several hypothesized sources. %\cite{higgins2022beta} suggested that the prior influence on the latent variables is too strong and proposed a beta-VAE that limits the bias of the prior. \cite{he2019lagging} demonstrated that one potential cause is from ``inference lag'' %where the initial training stages of the VAE fails to effectively model the posterior, 
%and proposed a solution where they train the encoding network first for several iterations before training the decoding network.  
One particularly well-known factor that contributes to posterior collapse is the overfitting of the decoder network \citep{bowman2016generating, sonderby2016ladder}. Alternatively, placing regularization on the function space of the decoder may help the latent variable model learn useful representations of the data.

\vspace{-0.12in}
%The probabilistic nature of GPs in
\paragraph{Gaussian Processes.} Probabilistic models like the Gaussian process latent variable model (GPLVM) \citep{lawrence2005probabilistic} introduces a regularization effect via a GP-distributed prior that helps prevent overfitting and thus improves generalization from limited samples. 
%\vspace{-0.02in}
%\begin{definition}
A GP $f(\cdot)$ is defined as a real-valued stochastic process defined over the input set $\mathcal{X} \subseteq \mathbb{R}^D$, such that for any finite subset of inputs $\!\vx\!=\!\{\x_n\}_{n=1}^N\!\subset\!\mathcal X$, the random variables $\f=\{f(\x_n)\}_{n=1}^N$ follow a joint Gaussian distribution  \citep{williams2006gaussian}. A common prior choice for a GP-distributed function is:
\begin{equation}
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
   \f \mid \vx = \mathcal{N}(\f \mid \bm{0}, \vk),  
\end{equation}
where $\mathbf{K}$ denotes the covariance/kernel matrix evaluated on the finite input $\vx$ with the kernel function $k(\x_1, \x_2)$, i.e., $[\mathbf{K}]_{i,j} \!=\! k({{\x}}_i, {\x}_j), i, j \in (1, ..., N)$. 

%\vspace{-0.02in}
Consequently, the GPLVM has laid the groundwork for several advancements in multi-view representation learning. One of the early works by \citet{li2017shared} straightforwardly assumes that each view in observations is a projection from a shared latent space using a GPLVM, referred to as multi-view GPLVM (MV-GPLVM). 
\vspace{-0.05in}
\paragraph{Multi-View Gaussian Process Latent Variable Model.} 
A MV-GPLVM assumes that the relationship between each view $v \! \in \! (1, ..., V)$ of observed data $\mathbf{Y}^{v} \!=\! [\mathbf{y}_{:, 1}^{v}, \mathbf{y}_{:, 2}^{v}, ..., \mathbf{y}_{:, M_v}^{v}]\!\in\!\mathbb{R}^{N \times M_v}$ and the shared/unified latent variables $\mathbf{X} \!=\!\! [\x_1, \x_2, ..., \x_N]^\top \!\!\in \! \mathbb{R}^{N \times D}$ is modeled by a GP. That is, in each dimension $m \!\in\! (1, ..., M_v)$ and view $v$, MV-GPLVM is defined as follows:
%\vspace{-0.12in}
\begin{subequations}
\label{eq:gplvm}
\begin{align}
    \setlength{\abovedisplayskip}{4.5pt}
    \setlength{\belowdisplayskip}{4.5pt}
        & \y_{:, m}^{v} \mid f_m^{v}(\vx)  \sim \mathcal{N}( f_m^{v}(\vx), \sigma^{2}_{v} \vi),  \\ 
        & f_m^{v}(\vx) \sim \mathcal{N}(\mathbf{0}, \mathbf{K}^{v}),  \\ 
        & \x_n \sim  \mathcal{N}(\mathbf{0}, \mathbf{I}),  \label{eq:gplvm_prior_x} 
\end{align}
\end{subequations}
%\vspace{-0.38in}
where $\sigma_v^{2}$ represents the noise variance and $f_m^{v}(\mathbf{X})\!=\!\left[f_m^{v} \left(\mathbf{x}_1\right) \ldots f_m^{v}\left(\mathbf{x}_N\right)\right]^{\top}\!\in\!\mathbb{R}^{N}$. The stationary radial basis function (\MakeUppercase{rbf}) is  typically the `default' choice of the kernel function. Due to the conjugacy between the Gaussian likelihood\footnote{Extending to other likelihoods is straightforward, as guided by the GP literature \citep{zhang2023bayesian,lalchand2022generalised}.} and the GP, we can integrate out each $f^{v}_m(\cdot)$ and get the marginal likelihood formed as
%\vspace{-0.12in}
\begin{align}
    &\y_{:, m}^{v}\sim \mathcal{N} (\bm 0, \mathbf{K}^{v} + \sigma^2_v \mathbf{I}). 
    \label{eq:gplvm_mle}
\end{align}
%\vspace{-0.38in}
Based on the marginal likelihood and the prior of $\vx$ (Eq.~\eqref{eq:gplvm_prior_x}),  the \textit{maximum a posteriori} (MAP) estimation of $\vx$ can be obtained, with $\mathcal{O}(N^3)$ computational complexity due to the inversion of the kernel matrix. \revise{\cite{eleftheriadis2013shared} extends the MV-GPLVM with a discriminative prior over the shared latent space for adapted to classification tasks.} Later, \citet{sun2020multi} incorporates deep GPs \citep{damianou2013deep} into MV-GPLVM for model flexibility, named MV-DGPLVM. However, existing MV-GPLVMs still fall short in handling practical multi-view datasets, that are often large-scale and exhibit diverse patterns across views. This limitation arises from either (1) the high computational complexity of fitting a deep GP or (2) limited kernel expressiveness caused by the stationary assumption. 
Particularly, a stationary kernel may fail to model input-varying correlations \citep{remes2017non}, particularly in domains like video analysis and clinical diagnosis that exhibit complex, time-varying dynamics. 

% \textcolor{red}{limited kernel expressiveness caused by the stationary property, which may struggle to manage data pattern diversity as it typically forces models to ignore view-specific properties, such as varying correlations within the input spaceâ€”an issue that is particularly problematic in contexts like video analysis and clinical diagnosis \michael{What about this type of data makes this problematic?}}.  

%Despite significant advancements in MV-GPLVMs and GPLVMs, these models still fall short in handling complex multi-view representational learning tasks that must not only manage relatively large-scale data, as it is collected from multiple views, but also able to handle data pattern diversity across views.
%; failure to account for view-specific properties can degrade the quality of the shared/unified latent representations. 
%To be specific, MV-GPLVMs \citep{sun2020multi} suffer from high computational costs due to the usage of deep GP structure. On the other hand, GPLVMs \citep{lalchand2022generalised, li2024preventing, zhang2023bayesian} may struggle to model complex data sets due to the inadequate kernel expressiveness caused by the stationary assumption. %, which leads to overlooking view-specific properties, such as long-range correlations. 
%\vspace{-0.02in}
%More specifically, a GP is defined as 
% defines a distribution over functions $f(\x)$, which is characterized by its mean function $m(\x) \triangleq \mathbb{E}(f(\x))$, typically assumed $m(\x)=\mathbf{0}$, and its covariance function, a.k.a., kernel $k\left(\x, \x^{\prime} \right)$. Mathematically, we have

%\vspace{0.02in}
To address those issues, we review the recent work of GPLVM \citep{lalchand2022generalised, li2024preventing, zhang2023bayesian}, that we may extend to deal with the multi-view scenario (See detailed comparisons in Table \ref{table:comparision}). One potential solution is the \textsl{advised}RFLVM (named ARFLVM for short) \citep{li2024preventing}. This method integrates the stationary spectral mixture (\MakeUppercase{sm}) kernel to enhance kernel flexibility in conjunction with a scalable random Fourier feature (RFF) kernel approximation. However, it remains limited by the stationary assumption. 

\vspace{-0.05in}
\paragraph{Random Fourier Features.}
Bochner's theorem  \citep{bochner1959lectures} states that any continuous stationary kernel and its spectral density $p(\w)$ are Fourier duals, i.e., \( k(\x_1\!-\!\x_2)\!=\!\int p(\w) \exp \! \left({i \w^{\top} (\x_1\!-\!\x_2)}\right)\!\mathrm{~d} \w \). Built upon this duality, \citet{rahimi2008random} approximates the stationary kernel \( k(\x_1 - \x_2) \) using an unbiased \MakeUppercase{m}onte \MakeUppercase{c}arlo (\MakeUppercase{mc}) estimator with \( L/2 \) spectral points \( \{\w^{(l)}\}_{l=1}^{L/2} \) sampled from \( p(\w) \), given by
\begin{equation}
    % \setlength{\abovedisplayskip}{5pt}
    % \setlength{\belowdisplayskip}{5pt}
    k(\x_1 - \x_2) \approx \phi(\x_1)^{\top} \phi(\x_2),
    \label{eq:basic_rff}
\end{equation}
where the random feature \( \phi(\x) = \)
\begin{equation}
    % \setlength{\abovedisplayskip}{5pt}
    % \setlength{\belowdisplayskip}{5pt}
    \sqrt{\frac{2}{L}} \left[\cos \left(2\pi \x^{\top} \w^{1:L/2} \right), \sin \left(2 \pi \x^{\top} \w^{1:L/2} \right)\right]^{\top} \in \mathbb{R}^{L}.
    \nonumber
\end{equation}
Here the superscript $1:L/2$ indicates that the cosine or sine function is repeated $L/2$ times, with each element corresponding to the one entry of  \( \{\w^{(l)}\}_{l=1}^{L/2} \). 
Consequently, the kernel matrix \( \mathbf{K} \) can be approximated by \( \Phi(\x) \Phi(\x)^{\top} \), with \( \Phi(\x) = \left[\phi(\x_1); \ldots; \phi(\x_N)\right]^{\top} \). By employing the RFF approximation, the kernel matrix inversion can be computed using the Woodbury matrix identity \citep{woodbury1950inverting}, thereby reducing the computational complexity to $\mathcal{O}(NL^2)$. 



% {\color{black}Later, we provide the necessary background by presenting the definition of MV-GPLVM (\S~\ref{subsec:mv-gplvm}) and briefly introducing RFF (\S~\ref{subsec:rff}).}

% \vspace{-0.02in}
% \subsection{Spectral Mixture Kernel}
% \label{subsec: sm_kernel}
% \vspace{-0.02in}

% In GP
%  \citep{williams2006gaussian}, the kernel serves as a key ingredient due to its influential role in shaping the characteristics and functions that the GP can model. To design stationary kernels and avoid the challenging explicit construction of positive definite kernel functions,  \citet{wilson2013gaussian} proposes to design the spectral density first and then transform it into a kernel function using the inverse Fourier transform. The theoretical foundation supporting this construction is provided by the Bochner's theorem  \citep{bochner1959lectures} which states that any continuous stationary kernel and its spectral density are Fourier duals, i.e., \( k(\x_1\!-\!\x_2)\!=\!\int p(\w) \exp \! \left({i \w^{\top} (\x_1\!-\!\x_2)}\right)\!\mathrm{~d} \w. \)
% % \begin{equation}
% % \setlength{\abovedisplayskip}{4.5pt}
% % \setlength{\belowdisplayskip}{4.5pt}
% % \label{eq:bachner_theorem}
% %    k(\x - \x^{\prime})=\int_{\mathbb{R}^{Q}} p(\w) \exp \! \left({i \w^{\top} (\x - \x^{\prime})}\right) \mathrm{~d} \w. 
% % \end{equation}
% In this sense, the \MakeUppercase{sm} kernel approximates the underlying spectral density by a Gaussian mixture 
% \vspace{-0.02in}
% \begin{equation}
% \label{eq:SM_density}
% \setlength{\abovedisplayskip}{4.5pt}
% \setlength{\belowdisplayskip}{4.5pt}
% \begin{aligned}
%     & p_{\mathrm{sm}}(\mathbf{w})= \sum_{q=1}^{Q} \alpha_q s_q(\w), \\
%     \!\!\!  & s_q(\w) \!=\! \frac{\mathcal{N}(\mathbf{w} \vert \bm{\mu}_q, \operatorname{diag}(\bm{\sigma}_q^2)) \!+\! \mathcal{N}(-\mathbf{w} \vert \bm{\mu}_q, \operatorname{diag}(\bm{\sigma}_q^2))}{2}, \nonumber
% \end{aligned}
% \end{equation}
% where $\alpha_q$ represents the mixture weight, $\bm{\mu}_q \!\in\! \mathbb{R}^D$ and $\bm{\sigma}_q^2\!\in\! \mathbb{R}^D$ are the mean and variance of the \(q\)-th Gaussian component, and $m$ denotes the number of mixture components, with the kernel hyperparameters collectively defined as $\bm{\theta}_{\mathrm{sm}} \!=\! \{\alpha_q, \bm{\mu}_q, \bm{\sigma^2_q}\}_{q=1}^{Q}$. Relying on Bochner's theorem, we can derive the \MakeUppercase{sm} kernel, $ k_{\mathrm{sm}}(\x_1, \x_2)=$
% % \vspace{-0.02in}
% \begin{equation}
% \label{eq:sm_kernel}
% \setlength{\abovedisplayskip}{4.5pt}
% \setlength{\belowdisplayskip}{4.5pt}
%  \sum_{q=1}^{Q}\!\alpha_q\!\exp\!\left(\!- 2 \pi^2 \|\boldsymbol{\sigma}_q^\top (\x_1\!-\!\x_2) \|^2 \right)\!\cos\!\left( 2\pi \boldsymbol \mu_q^{\top}\!\left(\x_1\!-\!\x_2\right)\right). \nonumber
% \end{equation}
% % Since the Gaussian mixture is dense, the \MakeUppercase{sm} kernel can approximate any continuous stationary kernel arbitrarily well \citep{tobar2015learning}. 
% Due to the universal function approximation property of the mixtures of Gaussians, the \MakeUppercase{sm} kernel can approximate continuous stationary kernels to an arbitrary precision given sufficient spectral components, as demonstrated in \citep{tobar2015learning}.

% % \vspace{-0.02in}
% \vspace{-0.1in}


% In this way, the matrix inversion can be calculated using the Woodbury matrix identity  \citep{woodbury1950inverting}, which reduces the computational complexity from \( O(N^3) \) to \( O(NL^2) \).

% To overcome the cubic computational complexity associated with kernel matrix inversion, 

% \citet{rahimi2008random} approximates the stationary kernel \( k(\x_1 - \x_2) \) using a \MakeUppercase{m}onte \MakeUppercase{c}arlo (\MakeUppercase{mc}) estimator with \( L/2 \) spectral points \( \{\w^{(l)}\}_{l=1}^{L/2} \) sampled from \( p(\w) \), given by
% \begin{equation}
%     \setlength{\abovedisplayskip}{4.5pt}
%     \setlength{\belowdisplayskip}{4.5pt}
%     k(\x_1 - \x_2) = \phi(\x_1)^{\top} \phi(\x_2),
% \end{equation}
% where the random feature \( \phi(\x) = \)
% \begin{equation}
%     \setlength{\abovedisplayskip}{4.5pt}
%     \setlength{\belowdisplayskip}{4.5pt}
%     \sqrt{\frac{2}{L}} \left[\cos \left(2\pi \x^{\top} \w^{1:L/2} \right), \sin \left(2 \pi \x^{\top} \w^{1:L/2} \right)\right]^{\top} \in \mathbb{R}^{L}, 
%     \nonumber
% \end{equation}
% where the subscript $1:L/2$  indicates that the cosine or sine function is repeated $L/2$ times, with each element corresponding to the one entry of  \( \{\w^{(l)}\}_{l=1}^{L/2} \). Consequently, the kernel matrix \( \mathbf{K} \) can be approximated by \( \Phi(\x) \Phi(\x)^{\top} \), with the random feature matrix \( \Phi(\x) = \left[\phi(\x_1); \ldots; \phi(\x_N)\right]^{\top} \). In this way, the matrix inversion can be calculated using the Woodbury matrix identity  \citep{woodbury1950inverting}, which reduces the computational complexity from \( O(N^3) \) to \( O(NL^2) \).


%\vspace{-0.02in}
%The primary aim of the multi-view learning task is to computationally effectively learn a informative unified latent representation from multiple views of observations. In practice, observed data collected from different views are often large-scale and diverse in patterns, which requires not only efficient inference approaches but also the model capability to capture each view-specific properties to avoid degrading the quality of the unified latent representations. 




%\vspace{-0.02in}



