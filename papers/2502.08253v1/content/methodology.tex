% \newpage
\vspace{-0.08in}
\section{Methodology}
\label{sec:method}
\vspace{-0.08in}
In \S~\ref{subsec:NG_kernel}, we introduce our expressive kernel function for multi-view representation learning. In \S~\ref{subsec:scalable_VI}, we propose a sampling-based variational inference algorithm to efficiently learn both kernel hyperparameters and latent representations. The scalable RFF approximation and the associated efficient reparameterization trick are detailed in \S~\ref{subsec:differentialble_rff}.

\vspace{-0.08in}
\subsection{Next-Gen SM (NG-SM) Kernel}
\label{subsec:NG_kernel}
\vspace{-0.08in}
As mentioned in \S~\ref{sec:background}, limited kernel expressiveness in the MV-GPLVM may hinder its ability to capture informative latent representations, potentially neglecting crucial view-specific characteristics like time-varying correlations in the data\footnote{For an exploration of the impact of limited kernel expressiveness in manifold learning, see \S~\ref{subsec:toy_example}.}. This problem necessitates a more flexible kernel design.
%Among the various efforts in kernel methods, one of the most notable is the series of works on 

The Bivariate SM kernel (BSM) is one notable development for improving kernel flexibility \citep{chen2024compressing, remes2017non, chen2021gaussian}.
%initially proposed by \citet{remes2017non}. 
Grounded in the duality that relates any continuous kernel function to a dual density from the generalized Bochner’s theorem \citep{yaglom1987correlation}, this method first models the dual density using a mixture of eight bivariate Gaussian components and then transforms it into the BSM kernel. By removing the restrictions of stationarity, the generalized Bochner’s theorem enables the BSM kernel to capture time-varying correlations \citep{remes2017non}.

%\vspace{-0.08in}
However, the BSM kernel faces several limitations imposed by the generalized Bochner’s theorem: (1) To guarantee the positive semi-definite (PSD) spectral density, the two variables in the bivariate Gaussian must have equal variances, which limits the flexibility of the Gaussian mixture and consequently reduces the kernel’s expressiveness. 2) According to the duality in the generalized Bochner’s theorem, it is not possible to derive a closed form expression of random features\footnote{See App.~\ref{app:generized_sm_kernel} for a more detailed discussion.}, such as $\phi(\x)$ in Eq.~\eqref{eq:basic_rff}.

Thus, the inversion of the BSM kernel matrix retains high computational complexity, rendering it unsuitable for multi-view datasets. Consequently, we introduce a duality that enables the derivation of the expressive NG-SM kernel. Additionally, it facilitates the derivation of its corresponding RFF estimator, thereby mitigating the high computational burden. 

% This method \cred{manually incorporates non-stationarity} into the frequencies and mixture weights of the SM kernel \citep{wilson2013gaussian} to account for time-varying correlations. \cred{However, its manual design to ensure positive definiteness limits the ability of generalised SM kernel to explore the space of continuous kernels \citep{remes2017non}. 

% \vspace{-0.08in}
% In response, it is imperative to design a generic kernel function with automatic construction of positive definiteness.} One potential solution is to establish a duality between \textbf{any continuous kernel function} and a relatively simple dual density, allowing the design of the dual density first, which can then be transformed into a kernel function. \cred{Although this approach is theoretically grounded in the generalised Bochner’s theorem \citep{yaglom1987correlation}, it typically results in a complex kernel form, making a RFF estimator impractical 
% and, therefore, unable to reduce the computational complexity of the inversion of kernel Gram matrix, rendering it unsuitable for the multi-view case (see more detailed discussions in App.~\ref{app:generized_sm_kernel}).
% }
%%\vspace{-0.08in}
\begin{theorem}[Universal Bochner's Theorem] 
\label{theo:Uni_Bochner}
A complex-valued bounded continuous kernel \( k\left(\x_1, \x_2\right) \) on \( \mathbb{R}^D \) is the covariance function of a mean square continuous complex-valued random process on  \( \mathbb{R}^D \) if and only if $k\left(\x_1, \x_2\right)=$
\begin{equation}
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5.5pt}
\begin{aligned}
& \frac{1}{4}\!\int 
 \exp(i\w_1^{\top} \x_1- i\w_2 ^{\top}\x_2)\!+\!\exp(i\w_2^{\top} \x_1 - i\w_1^{\top} \x_2) + \nonumber \\ &
 \exp(i\w_1^{\top}\x_1\!\!-\!i \w_1^{\top}\x_2)\!+\!\exp(i\w_2^{\top}\x_1\!\!-\!i\w_2^{\top}\x_2) u(\mathrm{d}\w_1,\!\mathrm{d} \w_2)
\end{aligned}
\end{equation}
where \( u \) is the Lebesgue-Stieltjes measure associated with some function \( p\left(\w_1, \w_2\right) \). When \(\w_1 = \w_2\), this theorem reduces to Bochner's theorem. 
\vspace{-.1in}
\end{theorem}
\begin{proof}
The proof can be found in App.~\ref{app:new_bochner_theorem}.
\end{proof}
\vspace{-0.15in}
The duality established in Theorem \ref{theo:Uni_Bochner} implies that a bivariate spectral density entirely determines the properties of a continuous kernel function. In this sense, we approximate the underlying bivariate spectral density by a Gaussian mixture: 
%\vspace{-0.08in}
\begin{equation}
    \label{eq:spectral_density_of_Next-Gen_SM} 
    % \setlength{\abovedisplayskip}{3pt}
    % \setlength{\belowdisplayskip}{3pt}
    p_{\text{ngsm}} \left(\mathbf{w}_1, \mathbf{w}_2\right)=\sum_{q=1}^{Q} \alpha_q s_q\left(\mathbf{w}_1, \mathbf{w}_2\right) 
\end{equation}
%\vspace{-0.2in}
with each symmetric density $s_q\left(\mathbf{w}_1, \mathbf{w}_2\right)$ 
\begin{equation}
% \setlength{\abovedisplayskip}{6pt}
% \setlength{\belowdisplayskip}{6pt}
\begin{aligned}
  \!\!s_q(\mathbf{w}_1, \mathbf{w}_2)\!= & \frac{1}{2} ~  \mathcal{N}\left( \! \left.\binom{\mathbf{w}_{1}}{\mathbf{w}_{2}} \right\rvert\,\binom{\boldsymbol{\mu}_{q1}}{\boldsymbol{\mu}_{q2}}, \begin{bmatrix}
      \bm \Sigma_{1}\!&\!\bm \Sigma_{\text{c}}^{\top} \\
    \bm \Sigma_{\text{c}}\!&\!\bm \Sigma_2
  \end{bmatrix} \right) + \\ 
 & \frac{1}{2} ~ \mathcal{N}\left(\!\left.\binom{-\mathbf{w}_{1}}{-\mathbf{w}_{2}} \right\rvert\,\binom{\boldsymbol{\mu}_{q1}}{\boldsymbol{\mu}_{q2}}, \begin{bmatrix}
      \bm \Sigma_{1}\!&\!\bm \Sigma_{\text{c}}^{\top} \\
    \bm \Sigma_{\text{c}}\!&\!\bm \Sigma_2
  \end{bmatrix} \right) 
\end{aligned}
\label{eq:s_q}
\end{equation}
in order to explore the space of continuous kernels. To simplify the notation, we omit the index \(q\) from the sub-matrices $\bm \Sigma_1$ \(\!=\!\operatorname{diag}(\bm{\sigma}_{q1}^2)\), \(\bm \Sigma_2\!=\! \operatorname{diag}(\bm{\sigma}_{q2}^2)\), and \(\bm \Sigma_{\mathrm{c}}\!=\!\rho_q \operatorname{diag}(\bm{\sigma}_{q1}) \operatorname{diag}(\bm{\sigma}_{q2})\), where \(\bm{\sigma}_{q1}^2, \bm{\sigma}_{q2}^2\in\mathbb{R}^D\) and the scalar \(\rho_q\) denotes the correlation between \(\w_1\) and \(\w_2\). These components collectively form the covariance matrix of the \(q\)-th bivariate Gaussian component. Furthermore, the vectors \(\boldsymbol{\mu}_{q1}\) and \(\boldsymbol{\mu}_{q2} \in \mathbb{R}^D\) constitute the mean of the \(q\)-th bivariate Gaussian component. 

%\vspace{-0.08in}
{\color{black}Based on Theorem \ref{theo:Uni_Bochner}, we derive the NG-SM kernel, \(k_{\text{ngsm}}(\mathbf{x}_1, \mathbf{x}_2)\) (see Eq.~\eqref{eq:ng-sm-kernel} in App.~\ref{app:ng-sm-kernel}), with kernel hyperparameters  $\bm{\theta}_{\mathrm{ngsm}} \!=\! \{\alpha_q, \bm{\mu}_{q1},\!\revise{\bm{\mu}_{q2}}, \bm{\sigma}_{q1}^2, \bm{\sigma}_{q2}^2, \rho_q\}_{q=1}^Q$.} As the PSD assumption is relaxed and the mixtures of Gaussians become dense \citep{plataniotis2017gaussian}, the duality ensures that the NG-SM kernel can approximate any continuous kernel arbitrary well. Next, we will demonstrate that a general closed-form RFF approximation can be derived for all kernels based on our duality, which we specify for the NG-SM kernel below. 

% \cred{For computational efficiency, we design an unbiased RFF-based kernel estimation method for the NG-SM kernel below.} 
%%\vspace{-0.08in}
\begin{theorem}
\label{theo:Non_stationary RFF}
Let $\phi(\x)$ be the randomized feature map of $k_{\text{ngsm}}(\x_1, \x_2)$, defined as follows:
\begin{equation}
    % \setlength{\abovedisplayskip}{5pt}
    % \setlength{\belowdisplayskip}{5pt}
    \!\sqrt{\frac{1}{2L}}\!\begin{bmatrix}
    \cos \left( \mathbf{w}_{1}^{(1:L/2) \top}  \x \right)\!+\!\cos \left(  \mathbf{w}_{2}^{(1:L/2)\top} \x \right) \\
    \sin \left( \mathbf{w}_{1}^{(1:L/2)\top}\x  \right)\!+\!\sin \left(  \mathbf{w}_{2}^{(1:L/2)\top} \x \right)  
    \end{bmatrix}\!\!\in\!\mathbb{R}^{L},\!\!
    \label{eq:rff_def}
\end{equation}
where the vertically stacked vectors $\{ [\mathbf{w}_1^{(l)}; \w_2^{(l)} ]\}_{l=1}^{L/2} \in \mathbb{R}^{L/2 \times 2D}$ are independent and identically distributed (i.i.d) random vectors drawn from the spectral density $p_{\text{ngsm}}(\w_1, \w_2)$. Then, the unbiased estimator of the kernel $k_{\text{ngsm}}(\x_1,\x_2)$ using RFFs is given by:
%%\vspace{-0.08in}
\begin{equation}
% \setlength{\abovedisplayskip}{4.5pt}
% \setlength{\belowdisplayskip}{4.5pt}
k_{\text{ngsm}}(\x_1, \x_2) \approx \phi(\x_1)^\top \phi(\x_2).   \label{eq:kernel_app}
\end{equation} 
\end{theorem}
%\vspace{-0.22in}
\begin{proof}
    {\color{black}The proof can be found in App.~\ref{app:rff_ng_sm}.} 
\end{proof}
%\vspace{-0.2in}
By integrating this unbiased estimator into the framework of MV-GPLVM, we derive the next-gen multi-view GPLVM: 
%following \MakeUppercase{n}ext-\MakeUppercase{g}en SM kernel-embedded MV-GPLVM(\MakeUppercase{ng}-MV-GPLVM):
%%\vspace{-0.08in}
\begin{subequations}
% \setlength{\abovedisplayskip}{4.5pt}
% \setlength{\belowdisplayskip}{4.5pt}
\label{eq:Next-Gen SM in MV-GPLVM}
\begin{align}
& \y_{:, m}^{v} \sim \mathcal{N}(\mathbf{0}, \bm \Phi_x^v \bm \Phi_x ^{v \top} + \sigma^2_v \mathbf{I}),  \\ 
& (\mathbf{w}^{(l)}_1)^{v},\!(\mathbf{w}^{(l)}_2)^v\!\!\sim\! p_{\text{ngsm}}^v(\mathbf{w}_1^v,\!\mathbf{w}_2^v),  \\
& \mathbf{x}_n \sim \mathcal{N}\left(\mathbf{0}, \mathbf{I} \right),
\end{align} 
\end{subequations}
where the random feature matrix for each view is denoted as \( \bm{\Phi}_x^{v} = \left[\phi(\mathbf{x}_1); \ldots; \phi(\mathbf{x}_N)\right]^{\top}\), with the superscript of each feature map omitted for simplicity of notation. Furthermore, we collectively denote $\bm \sigma^2 = \{\sigma^2_{v}\}_{v=1}^{V}$ and spectral points $\vw \triangleq \{ \vw^{v} \}_{v=1}^{V}$, with each 
\begin{equation}
    % \setlength{\abovedisplayskip}{4.5pt}
    % \setlength{\belowdisplayskip}{4.5pt}
    \vw^{v} \triangleq \{ [ (\mathbf{w}_1^{(l))^{v}}; (\w_2^{(l)})^{v} ]\}_{l=1}^{L/2} \in \mathbb{R}^{L/2 \times 2D}. 
\end{equation}

The following subsections will illustrate how to infer the parameters of the \MakeUppercase{ng-mv-gplvm} through a unified sampling-based variational inference framework \citep{kingma2013auto}. 
\vspace{-0.02in}
\subsection{Sampling-based Variational Inference} 
\label{subsec:scalable_VI}  
\vspace{-0.02in}
%For \MakeUppercase{ng-mv-gplvm}, 
We employ variational inference \citep{blei2017variational} to jointly and efficiently estimate the posterior and hyperparameters \(\boldsymbol{\theta} = \{\boldsymbol{\theta}_{\text{ngsm}}, \bm \sigma^2\}\). %and infer the posteriors of model variables. 
Variational inference reformulates Bayesian inference task as a deterministic optimization problem by approximating the true posterior $p(\vw, \vx \vert \vy)$ using a surrogate distribution, %that is identified from pre-selected variational distributions 
$q(\vw, \vx)$, indexed by variational parameters $\bm \xi$. The variational parameters are typically estimated \revise{by maximizing} the evidence lower bound (\MakeUppercase{elbo}) which is equivalent to minimizing the \MakeUppercase{k}ullback-\MakeUppercase{l}eibler (\MakeUppercase{kl}) divergence \(\mathrm{KL}(q(\vw, \vx) \| p(\vw, \vx \vert \vy))\) between the surrogate and the true posterior. 

%\vspace{-0.08in}
To construct the \MakeUppercase{elbo} for \MakeUppercase{ng-mv-gplvm}, we first obtain joint distribution of the model: 
%\vspace{-0.08in}
\begin{equation}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{2pt}
    \begin{aligned}
        p(\vy, \vx, \vw) &= p(\vx) \prod_{v=1}^{V} p(\vw^v) p(\vy^v \vert \vx, \vw^v)
        % &=  \prod_{i=1}^{N} p(\x_i) \prod_{v=1}^{V} \prod_{j=1}^{M_v} p(\y_{:,j}^v \vert \vx, \mathbf{w}^v),
    \end{aligned}
    \label{eq:model_joint}
\end{equation}
and then define the variational distributions $q(\vw, \vx)$ as
%%\vspace{-0.08in}
\begin{equation}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{2pt}
    q(\vx, \vw) \triangleq q(\vx) p(\vw) = q(\vx) \prod_{v=1}^{V} p(\vw^v), 
    \label{eq:variational_dis}
\end{equation}
%\vspace{-0.15in}
where $q(\vx)\!=\!\prod_{n=1}^N \mathcal{N}(\x_n \vert \bm{\mu}_n,\bm{S}_n)$, and $\bm \xi\!\triangleq\!\{ \bm{\mu}_n\!\in\!\mathbb{R}^D$, $\bm{S}_n\!\in\!\mathbb{R}^{D \times D} \}_{n=1}^{N}$ are the free variational parameters. The variational distribution $q(\vw)$ is set to its prior $p(\vw)$ as this is essentially equal to assuming that $q(\vw)$ follows a Gaussian mixtures (See the justification in App.~\ref{app:set_of_w}). Consequently, the optimization problem is:
%\vspace{-0.1in}
\begin{equation}
\begin{aligned}
\setlength{\abovedisplayskip}{4pt}
\setlength{\belowdisplayskip}{3pt}
    \nonumber
    \max_{\bm \theta, \bm \xi} \ & \mathbb{E}_{q(\vx, \vw)} \! \left[\log \frac{p(\vx) \prod_{v=1}^{V} p(\vw^v) p(\vy^v \vert \vx, \vw^v)}{q(\vx)\prod_{v=1}^{V} p(\vw^v)} \right] \\  
    & = \sum_{v=1}^V \underbracket{\mathbb{E}_{q(\cdot, \cdot)} \left[ \log p(\vy^v \vert \vx,\!\vw^v) \right]}_{\text{(a): reconstruction}}\!-\!\underbracket{ \operatorname{KL}(q(\vx) \| p(\vx))}_{\text{(b): regularization}}\!\!\! 
\end{aligned}
\end{equation}
%\vspace{-0.2in}
which jointly learns the model hyperparameters $\bm \theta$ and infers the variational parameters $\bm \xi$. Term (a) encourages reconstructing the observations using any samples drawn from \(q(\vx, \vw)\) and term (b) serves as a regularization to prevent \(\vx\) from deviating significantly from the prior distribution. 
%\vspace{-0.08in}

To address this optimization problem using the sampling-based variational inference \citep{kingma2013auto}, we first derive the analytical form of term (b), the \MakeUppercase{kl} divergence between two Gaussian distributions, and expand term (a), approximating the expectation via \MakeUppercase{mc} estimation,
%\vspace{-0.08in}
\begin{equation}
    % \setlength{\abovedisplayskip}{4.5pt}
    % \setlength{\belowdisplayskip}{4.5pt}
    \sum_{v=1}^{V} \sum_{m=1}^{M_v} \frac{1}{I} \sum_{i=1}^I \log \mathcal{N}\left(\mathbf{y}_{:, m}^{v} \vert \mathbf{0}, (\bm \Phi_{x}^{v} \bm \Phi_{x}^{v \top})^{(i)} \!+\! \sigma^2_{v} \mathbf{I}\right),  
    \label{eq:gaussian_elbo_estimate}
\end{equation}
where $I$ denotes the number of \textbf{differentiable} \MakeUppercase{mc} samples drawn from $q(\mathbf{X})$ and $p(\mathbf{W})$ with respect to $\bm \theta$ and $\bm \xi$; See the further computational details in App.~\ref{app:ELBO_deriviations}. Then, modern optimization techniques, such as Adam \citep{kingma2015adam}, can be directly applied to solve the problem. However, this raises a question: \textit{How can differentiable \MakeUppercase{mc} samples be efficiently generated from the mixture bivariate Gaussian distribution, \(p(\vw^{v})\), that implicitly involves discrete variables?} 


\vspace{-0.02in}
\subsection{Sampling in Mixture Bivariate Gaussians}
\label{subsec:differentialble_rff}
\vspace{-0.02in}

In other words, it is essential to both generate differentiable samples from the mixture bivariate Gaussian and ensure high sampling efficiency, which is particularly beneficial in the multi-view case. However, the typical sampling process hinders us from achieving this goal \citep{mohamed2020monte, graves2016stochastic}. 
% \textcolor{red}{Specifically, the difficulty arises in first generating an index $q$ from the discrete distribution, $P(q)\!=\!\alpha_q / \sum_{j=1}^{Q} \alpha_j, q\!=\!1, ..., Q$, since state-of-the-art reparameterization approximations for discrete distributions, such as Gumbel-Softmax, have been shown to be unstable and highly sensitive to hyperparameter choices \citep{potapczynski2020invertible}.} 
A primary difficulty stems from first generating an index $q$ from the discrete distribution, $P(q)\!=\!\alpha_q / \sum_{j=1}^{Q} \alpha_j, q\!=\!1, ..., Q$, as it is inherently non-reparameterizable w.r.t. the mixture weights. Although the Gumbel-Softmax method provides an approximation, it remains unstable and highly sensitive to hyperparameter choices \citep{potapczynski2020invertible}. Additionally, directly performing joint sampling from \( s_q(\w_1, \w_2) \) (see Eq.~\eqref{eq:s_q}) incurs a computational complexity of \(\mathcal{O}(D^3)\), further complicating the sampling process.
% Then, the concatenation vector \([\mathbf{w}_1^{(l)};\mathbf{w}_2^{(l)}]\) is sampled from the \(q\)-th Gaussian component $s_{q}(\w_1, \w_2)$ with reparameterization trick. 
% However, jointly sampling from $s_{q}(\w_1, \w_2)$ (see Eq.~\eqref{eq:s_q}) results in $\mathcal{O}(D^3)$ computational complexity.  %\michael{Is the 8 necessary?}. 
Next, we will demonstrate how to address both issues. To simplify the notation, we omit the superscript $v$ in this section.

\vspace{0.1in}
\noindent \textbf{1) \underline{Two-step reparameterization trick}}  
% \vspace{-0.08in}

We first propose a sequential sampling method within the reparameterization framework. This approach reduces the computational complexity from \(\mathcal{O}(D^3)\) to \(\mathcal{O}(D)\), as detailed below \citep{mohamed2020monte}.
% Instead of using the one-liner sample step, which has a computational complexity of \(\mathcal{O}(8D^3)\) \citep{mohamed2020monte}, we propose a two-step reparameterization trick by leveraging sequential sampling method within reparametrization trick [xxx], which reduces complexity to \(\mathcal{O}(D)\), as detailed below.
% %\vspace{-0.08in}
\begin{proposition}[\MakeUppercase{t}wo-step reparameterization trick]
\label{prop:Two-step reparameterization trick}

We can sample \(\mathbf{w}_{1}^{(l)}, \mathbf{w}_{2}^{(l)}\) from a bivariate Gaussian distribution \(s_q(\mathbf{w}_1, \mathbf{w}_2)\) using the following steps:

%\vspace{-0.08in}
1. \!\!$\mathbf{w}_{1}^{(l)} \!=\! \bm \mu_{q1} + \bm \sigma_{q1}\!\circ \bm \epsilon_1$, 

%\vspace{-0.08in}
2. \!\!$\mathbf{w}_{2}^{(l)}\!=\!\bm \mu_{q2}+\rho_q(\bm \sigma_{q2} \backslash \bm \sigma_{q1}) \circ (\mathbf{w}_{1}^{(l)} \!-\! \bm \mu_{q1})\!+\!\sqrt{1\!-\! \rho_q^2} \bm \sigma_{q2} \circ \bm \epsilon_2$,

%\vspace{-0.08in}
where \(\bm \epsilon_2\), \(\bm \epsilon_1\) are standard normal random variables and $\circ \text{ and } \backslash$ represents element-wise multiplication and division, respectively. \vspace{-.15in}
\end{proposition} 
\begin{proof}
The proof and complexity analysis can be found in App.~\ref{app:proposition_1}.
\vspace{-0.1in}
\end{proof}



\begin{algorithm}[t!]
    \caption{\underline{\MakeUppercase{ng-mv-rflvm}}: Next-Gen
     spectral mixture kernel-embedded Multi-View RFLVM
    }
    \label{alg:GPLVM_Next-Gen_SM_RFF}
    \textbf{Input:} Dataset $\vy$;  Maximum iterations $T$.
    
    \textbf{Initialize:} Iteration count $t = 0$; Initialized model hyperparameters $\btheta$ and variational parameters $\bm \xi$.
    
    \While{$t < T$ \textbf{or} Not Converged}{
        Sample $\mathbf{X}$ from $q(\vx) = \prod_{n = 1}^N \mathcal{N}(\x_n \vert \bm{\mu}_n, \bm{S}_n)$ using the reparameterization trick. \\ 
        
        For each view, sample $\mathbf{W}^v$ from $p(\vw^v)\!=\!\prod_{q=1}^Q \prod_{l=1}^{L / 2} s_q(\w_1^v, \w_2^v)$ via the two-step reparameterization trick. \\
        
        For each view, construct ${\bm \Phi}^v_{\text{ngsm}}(\vx)$ using the sampled $\mathbf{X}$ and $\mathbf{W}^v$. \\
        
        Evaluate data reconstruction term of  \MakeUppercase{elbo} through Eq.~\eqref{eq:gaussian_elbo_estimate}.\\
        
        Evaluate regularized term of  \MakeUppercase{elbo} analytically.\\
        
        Maximize \MakeUppercase{elbo} and update $\btheta$, $\bm \xi$ using Adam \citep{kingma2015adam}.\\ 
        
        Increment $t = t + 1$. \\
    }
    \textbf{Output:} $\btheta$, $\bm \xi$. 
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{table/knn_acc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.1in}
\noindent\textbf{2) \underline{Unbiased differential RFF estimator}}
%\vspace{-0.08in}

Let spectral points \(\vw \triangleq \{ [\w_{q1}^{(l)}; \w_{q2}^{(l)}] \}_{q=1, l=1}^{Q, L/2}\), be sampled from \(p(\vw) = \prod_{q=1}^{Q} \prod_{l=1}^{L / 2} s_q(\w_1, \w_2)\) using the two-step reparameterization trick. Inspired by previous work \citep{li2024preventing,jung2022efficient},  we first use the spectral points from the $q$-th mixture component to construct the following feature map:
%\vspace{-0.08in}
\begin{equation}
\setlength{\abovedisplayskip}{4.5pt}
\setlength{\belowdisplayskip}{4.5pt}
 \varphi_q(\mathbf{x}) \triangleq \sqrt{\alpha_q} \cdot \phi(\mathbf{x}; \{\mathbf{w}_{q1}^{(l)}\}_{l=1}^{L/2}, \{\mathbf{w}_{q2}^{(l)}\}_{l=1}^{L/2}).
\end{equation}
Based on the feature maps $\{\varphi_q(\mathbf{x})\}_{q=1}^Q$, we can formulate our RFF estimator for the NG-SM kernel as follows.
%\vspace{-0.2in}
\begin{theorem}
Stacking the feature maps $\varphi_q(\x), q = 1, \ldots, Q$, yields the final form of the RFF approximation for the NG-SM kernel, \(\varphi(\mathbf{x})\):
%%\vspace{-0.08in}
\begin{equation}
% \setlength{\abovedisplayskip}{4.5pt}
% \setlength{\belowdisplayskip}{4.5pt}
\varphi\left(\x\right)\!=\!\left[\varphi_1(\mathbf{x})^{\top}, \varphi_2(\mathbf{x})^{\top}, \ldots, \varphi_m(\mathbf{x})^{\top}\right]^{\top}\!\in\!\mathbb{R}^{Q L}.  
\label{eq:RFF_NGSM}
\end{equation}
Built upon this mapping, our unbiased RFF estimator for the NG-SM kernel is reformulated as follows:
%\footnote{Alternatively, \citet{tokui2016reparameterization} propose another way to solve this issue.}:  
\begin{equation}
% \setlength{\abovedisplayskip}{4.5pt}
% \setlength{\belowdisplayskip}{4.5pt}
\mathbb{E}_{p(\vw)}\left[ \varphi(\mathbf{x}_1)^{\top} \varphi(\mathbf{x}_2)\right] = k_{\mathrm{ngsm}}(\mathbf{x}_1, \mathbf{x}_2).
\end{equation}
\label{prop_NGSM_RFF_approx}
\end{theorem}
\vspace{-0.30in}
\begin{proof}
    The proof can be found in App.~\ref{app:proof_theorem_3}. 
\end{proof}
\vspace{-0.1in}
Moreover, given inputs $\vx$ and the RFF feature map $\varphi(\x)$, we can establish the approximation error bound for the NG-SM kernel gram matrix approximation, \(\hat{\mathbf{K}}_{\mathrm{ngsm}}\!\triangleq\! \Phi_{\mathrm{ngsm}}(\mathbf{X}) \Phi_{\mathrm{ngsm}}(\mathbf{X})^{\top}\) below, where \(\Phi_{\mathrm{ngsm}}(\mathbf{X})\) \(=\left[\varphi\left(\mathbf{x}_1\right), \ldots, \varphi\left(\mathbf{x}_N\right)\right]^{\top} \in \mathbb{R}^{N \times Q L}\).

\vspace{0.02in}
\begin{theorem}
\label{thm:NGSM_RFF_approx}
Let $C=(\sum_{q=1}^Q \alpha_q^2)^{1/2}$, then for a small $\epsilon>0$, the approximation error between the true NG-SM kernel matrix $\mathbf{K}_{\mathrm{ngsm}}$ and its RFF approximation $\hat{\mathbf{K}}_{\mathrm{ngsm}}$ is bounded as follows:
%\vspace{-0.08in}
\begin{equation}
    \nonumber
    \begin{aligned}
    % \setlength{\abovedisplayskip}{4.5pt}
    % \setlength{\belowdisplayskip}{4.5pt}
    & P\left(\left\|\hat{\mathbf{K}}_{\mathrm{ngsm}}-\mathbf{K}_{\mathrm{ngsm}}\right\|_2 \geq \epsilon\right) \leq \\
    & N \exp \left(\frac{-3 \epsilon^2 L}{2 NC \left(6\left\|\mathbf{K}_{\mathrm{ngsm}}\right\|_2+ 3 N C \sqrt{Q}+8 \epsilon\right)}\right),
    \end{aligned}
\end{equation}
where $\|\cdot\|_2$ is the matrix spectral norm.  
\end{theorem}
\vspace{-0.08in}
\begin{proof}
    The proof can be found in App.~\ref{app:proof_theorem_4}. 
\end{proof}
\vspace{-0.08in}
% Intuitively, this implies that the approximation error can be reduced in a probabilistic sense as the number of the sampled spectral points $L$ increases.  
% %\vspace{-0.08in}
The feature map \(\varphi(\x)\) not only provides theoretical guarantees for the approximation but also eliminates the need to generate differential samples from the mixture bivariate Gaussian distribution by directly introducing differentiability into the optimization objective w.r.t. the mixture weights \(\alpha_q\). 

Furthermore, the two-step reparameterization trick uses the correlations between \(\w_1\) and \(\w_2\), for efficient sampling. %, that is particularly helpful in multi-view scenarios. 
Consequently, the aforementioned optimization problem is solvable with standard algorithms like Adam \citep{kingma2013auto}. We summarize the pseudocode in Algorithm \ref{alg:GPLVM_Next-Gen_SM_RFF} which illustrates the implementation of our proposed method, named \MakeUppercase{n}ext-\MakeUppercase{g}en SM kernel-embedded \MakeUppercase{mv} \MakeUppercase{r}andom \MakeUppercase{f}eature-based GPLVM  (\MakeUppercase{ng}-\MakeUppercase{mv-rflvm}).  



% In addition to the theoretical assurance provided by the approximation, the new feature map in Eq.~\eqref{eq:featrue_map_of_Next-Gen_SM} offers a significant benefit: it makes the variational lower bound $\mathcal{L}$ differentiable with respect to the mixture weights $\alpha_i$. This allows for the easy use of automatic differentiation tools in optimizing hyperparameters. 

% Notably, when $N \gg mL$, the computational complexity per iteration of training is $\mathcal{O}(N(mL)^2)$, as detailed in Appendix \ref{appendix:ELBO_deriviations}.
