%\documentclass{uai2025} % for initial submission
\documentclass[accepted]{uai2025} % after acceptance, for a revised version; 
% also before submission to see how the non-anonymous paper would look like 
                        
%% There is a class option to choose the math font
% \documentclass[mathfont=ptmx]{uai2025} % ptmx math instead of Computer
                                         % Modern (has noticeable issues)
% \documentclass[mathfont=newtx]{uai2025} % newtx fonts (improves upon
                                          % ptmx; less tested, no support)
% NOTE: Only keep *one* line above as appropriate, as it will be replaced
%       automatically for papers to be published. Do not make any other
%       change above this note for an accepted version.

%% Choose your variant of English; be consistent
\usepackage[american]{babel}
% \usepackage[british]{babel}

\usepackage[table,xcdraw]{xcolor} % 支持表格着色和 HTML 格式的颜色代码

%% Some suggested packages, as needed:
\usepackage{natbib} % has a nice set of citation styles and commands
%     \bibliographystyle{plainnat}
%     \renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} % amsmath with fixes and additions
% \usepackage{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage{tikz} % nice language for creating drawings and diagrams

%% Provided macros
% \smaller: Because the class footnote size is essentially LaTeX's \small,
%           redefining \footnotesize, we provide the original \footnotesize
%           using this macro.
%           (Use only sparingly, e.g., in drawings, as it is quite small.)

%% Self-defined macros
\newcommand{\swap}[3][-]{#3#1#2} % just an example

\usepackage{graphicx}

\input{config/packages.tex}
\input{config/acronyms.tex}
\input{config/math.tex}

% align footnote
%\usepackage[hang, flushmargin]{footmisc}

\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}

\makeatletter
\renewcommand*\env@matrix[1][c]{\hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols #1}}
\makeatother

% \usepackage{algpseudocode}  % For algorithmic pseudocode
% \usepackage{graphicx}       % For including graphics
% \usepackage{float}          % For float placement
% \usepackage[ruled,vlined]{algorithm2e}
 

\newcommand{\michael}[1]{{\color{blue}\textbf{Michael: #1}}}
\newcommand{\cred}[1]{{\color{red}#1}}
\newcommand{\cblue}[1]{{\color{blue}#1}}

\newcommand{\revise}[1]{{\color{black}#1}}




\title{Multi-View Oriented GPLVM: Expressiveness and Efficiency}

% The standard author block has changed for UAI 2025 to provide
% more space for long author lists and allow for complex affiliations
%
% All author information is authomatically removed by the class for the
% anonymous submission version of your paper, so you can already add your
% information below.
%
% Add authors
\author[1]{Zi~Yang$^{\star}$}
\author[1]{Ying~Li$^{\star}$}
\author[2]{\href{mailto:<zhidilin@nus.edu.sg>?Subject=Your UAI 2025 paper}{Zhidi~Lin$^{\dagger}$}}
\author[1]{\href{mailto:<mzhang18@hku.hk>?Subject=Your UAI 2025 paper}{Michael~Minyi~Zhang$^{\dagger}$}}
\author[3]{Pablo M. Olmos}


% Add affiliations after the authors
\affil[1]{%
    School of Computing and Data Science \\
    University of Hong Kong\\
    Hong Kong, China
}
\affil[2]{%
    Department of Statistics and Data Science\\
    National University of Singapore\\
    Singapore
}
\affil[3]{%
    Department of Signal Theory and Communications \\
    Universidad Carlos III de Madrid\\
    Madrid, Spain
}

% Add footnotes for equal contribution and correspondence author
\affil[ ]{%
    $^{\star}$: Equal contribution \\
    $^{\dagger}$: Correspondence author.
}
  
\begin{document}
\maketitle


\begin{abstract} 
    The multi-view Gaussian process latent variable model (MV-GPLVM) aims to learn a unified representation from multi-view data but is hindered by challenges such as limited kernel expressiveness and low computational efficiency. 
    %
    To overcome these issues, we first introduce a new duality between the spectral density and the kernel function.  
    %
    By modeling the spectral density with a bivariate Gaussian mixture, we then derive a generic and expressive kernel termed Next-Gen Spectral Mixture (NG-SM) for MV-GPLVMs.
    % which can approximate any continuous kernel with arbitrary precision, %given enough mixture components, 
    % thus enhancing kernel expressiveness.
    %
    To address the inherent computational inefficiency of the NG-SM kernel, we propose a random Fourier feature approximation. 
    %
    Combined with a tailored reparameterization trick, this approximation enables scalable variational inference for both the model and the unified latent representations.
    %
    Numerical evaluations across a diverse range of multi-view datasets demonstrate that our proposed method consistently outperforms state-of-the-art models in learning meaningful  latent representations.
\end{abstract}

\input{content/intro}

\input{content/background}

\input{content/methodology}

\input{content/experiments}

\input{content/conclusion}

% \section{Future Work}

% An intriguing direction for future research is \citep{damianou2012manifold, damianou2021multi}.    

% \revise{An intriguing direction for future research is the incorporation of more expressive priors for the latent space, such as energy-based priors, instead of the standard Gaussian prior. These flexible priors help minimize the gap between the ground-truth posterior and the approximated variational distribution \citep{yuan2024learning}, potentially enhancing our model's performance in generating meaningful latent representations.}  

\newpage
% References
%\bibliography{reference.bib}

\begin{thebibliography}{}
\bibitem[Bach and Jordan, 2002]{bach2002kernel}
Bach, F.~R. and Jordan, M.~I. (2002).
\newblock Kernel independent component analysis.
\newblock {\em Journal of Machine Learning Research}, 3(Jul):1--48.

\bibitem[Balasubramanian and Schwartz, 2002]{balasubramanian2002isomap}
Balasubramanian, M. and Schwartz, E.~L. (2002).
\newblock The {I}somap algorithm and topological stability.
\newblock {\em Science}, 295(5552):7--7.

\bibitem[Blei et~al., 2017]{blei2017variational}
Blei, D.~M., Kucukelbir, A., and McAuliffe, J.~D. (2017).
\newblock Variational inference: {A} review for statisticians.
\newblock {\em Journal of the American Statistical Association}, 112(518):859--877.

\bibitem[Blei et~al., 2003]{blei2003latent}
Blei, D.~M., Ng, A.~Y., and Jordan, M.~I. (2003).
\newblock Latent {D}irichlet allocation.
\newblock {\em Journal of Machine Learning Research}, 3(Jan):993--1022.

\bibitem[Bochner et~al., 1959]{bochner1959lectures}
Bochner, S. et~al. (1959).
\newblock {\em Lectures on {F}ourier integrals}, volume~42.
\newblock Princeton University Press.

\bibitem[Bowman et~al., 2016]{bowman2016generating}
Bowman, S.~R., Vilnis, L., Vinyals, O., Dai, A., Jozefowicz, R., and Bengio, S. (2016).
\newblock Generating sentences from a continuous space.
\newblock In {\em SIGNLL Conference on Computational Natural Language Learning}.

\bibitem[Buitinck et~al., 2013]{sklearn_api}
Buitinck, L., Louppe, G., Blondel, M., Pedregosa, F., Mueller, A., Grisel, O., Niculae, V., Prettenhofer, P., Gramfort, A., Grobler, J., Layton, R., VanderPlas, J., Joly, A., Holt, B., and Varoquaux, G. (2013).
\newblock {API} design for machine learning software: {E}xperiences from the scikit-learn project.
\newblock In {\em ECML PKDD Workshop: Languages for Data Mining and Machine Learning}, pages 108--122.

\bibitem[Cao et~al., 2023]{cao2023variational}
Cao, J., Kang, M., Jimenez, F., Sang, H., Schaefer, F.~T., and Katzfuss, M. (2023).
\newblock {Variational sparse inverse {C}holesky approximation for latent {G}aussian processes via double {K}ullback-{L}eibler minimization}.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Casale et~al., 2018]{casale2018gaussian}
Casale, F.~P., Dalca, A., Saglietti, L., Listgarten, J., and Fusi, N. (2018).
\newblock Gaussian process prior variational autoencoders.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Chang et~al., 2023]{chang2023memory}
Chang, P.~E., Verma, P., John, S., Solin, A., and Khan, M.~E. (2023).
\newblock Memory-based dual {G}aussian processes for sequential learning.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Chen et~al., 2021]{chen2021gaussian}
Chen, K., van Laarhoven, T., and Marchiori, E. (2021).
\newblock Gaussian processes with skewed {L}aplace spectral mixture kernels for long-term forecasting.
\newblock {\em Machine Learning}, 110:2213--2238.

\bibitem[Chen et~al., 2024]{chen2024compressing}
Chen, K., van Laarhoven, T., and Marchiori, E. (2024).
\newblock Compressing spectral kernels in {G}aussian process: {E}nhanced generalization and interpretability.
\newblock {\em Pattern Recognition}, page 110642.

\bibitem[Cheng et~al., 2022]{cheng2022rethinking}
Cheng, L., Yin, F., Theodoridis, S., Chatzis, S., and Chang, T.-H. (2022).
\newblock {Rethinking {B}ayesian learning for data analysis: {T}he art of prior and inference in sparsity-aware modeling}.
\newblock {\em IEEE Signal Processing Magazine}, 39(6):18--52.

\bibitem[Cutajar et~al., 2017]{cutajar2017random}
Cutajar, K., Bonilla, E.~V., Michiardi, P., and Filippone, M. (2017).
\newblock Random feature expansions for deep {G}aussian processes.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Damianou et~al., 2012]{damianou2012manifold}
Damianou, A., Ek, C.~H., Titsias, M., and Lawrence, N. (2012).
\newblock Manifold relevance determination.
\newblock In {\em International Conference on Machine Learning}, pages 145--152.

\bibitem[Damianou and Lawrence, 2013]{damianou2013deep}
Damianou, A. and Lawrence, N. (2013).
\newblock Deep {G}aussian processes.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}.

\bibitem[Damianou et~al., 2021]{damianou2021multi}
Damianou, A., Lawrence, N.~D., and Ek, C.~H. (2021).
\newblock Multi-view learning as a nonparametric nonlinear inter-battery factor analysis.
\newblock {\em Journal of Machine Learning Research}, 22(86):1--51.

\bibitem[De~Souza et~al., 2021]{de2021learning}
De~Souza, D., Mesquita, D., Gomes, J.~P., and Mattos, C.~L. (2021).
\newblock Learning {GPLVM} with arbitrary kernels using the unscented transformation.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}.

\bibitem[Dunlop et~al., 2018]{dunlop2018deep}
Dunlop, M.~M., Girolami, M.~A., Stuart, A.~M., and Teckentrup, A.~L. (2018).
\newblock How deep are deep {G}aussian processes?
\newblock {\em Journal of Machine Learning Research}, 19(54):1--46.

\bibitem[Eleftheriadis et~al., 2013]{eleftheriadis2013shared}
Eleftheriadis, S., Rudovic, O., and Pantic, M. (2013).
\newblock Shared {G}aussian process latent variable model for multi-view facial expression recognition.
\newblock In {\em International Symposium on Visual Computing}, pages 527--538. Springer.

\bibitem[Eraslan et~al., 2019]{eraslan2019single}
Eraslan, G., Simon, L.~M., Mircea, M., Mueller, N.~S., and Theis, F.~J. (2019).
\newblock Single-cell {RNA}-seq denoising using a deep count autoencoder.
\newblock {\em Nature Communications}, 10(1):390.

\bibitem[Feng et~al., 2014]{feng2014cross}
Feng, F., Wang, X., and Li, R. (2014).
\newblock Cross-modal retrieval with correspondence autoencoder.
\newblock In {\em ACM International Conference on Multimedia}.

\bibitem[G{\'o}mez-Hern{\'a}ndez and Cassiraga, 1994]{gomez1994theory}
G{\'o}mez-Hern{\'a}ndez, J.~J. and Cassiraga, E.~F. (1994).
\newblock Theory and practice of sequential simulation.
\newblock In {\em Geostatistical Simulations: Proceedings of the Geostatistical Simulation Workshop, Fontainebleau, France, 27--28 May 1993}, pages 111--124. Springer.

\bibitem[Gopalan et~al., 2015]{gopalan2015scalable}
Gopalan, P., Hofman, J.~M., and Blei, D.~M. (2015).
\newblock Scalable recommendation with hierarchical {P}oisson factorization.
\newblock In {\em Conference on Uncertainty in Artificial Intelligence}.

\bibitem[Graves, 2016]{graves2016stochastic}
Graves, A. (2016).
\newblock Stochastic backpropagation through mixture density distributions.
\newblock {\em arXiv preprint arXiv:1607.05690}.

\bibitem[Gundersen et~al., 2021]{gundersen2021latent}
Gundersen, G., Zhang, M., and Engelhardt, B. (2021).
\newblock Latent variable modeling with random features.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 1333--1341. PMLR.

\bibitem[Hardoon et~al., 2004]{hardoon2004canonical}
Hardoon, D.~R., Szedmak, S., and Shawe-Taylor, J. (2004).
\newblock Canonical correlation analysis: {A}n overview with application to learning methods.
\newblock {\em Neural Computation}, 16(12):2639--2664.

\bibitem[He et~al., 2019]{he2019lagging}
He, J., Spokoyny, D., Neubig, G., and Berg-Kirkpatrick, T. (2019).
\newblock Lagging inference networks and posterior collapse in variational autoencoders.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Higgins et~al., 2022]{higgins2022beta}
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., and Lerchner, A. (2022).
\newblock beta-{VAE}: {L}earning basic visual concepts with a constrained variational framework.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Hotelling, 1992]{hotelling1992relations}
Hotelling, H. (1992).
\newblock Relations between two sets of variates.
\newblock In {\em Breakthroughs in Statistics: Methodology and Distribution}, pages 162--190. Springer.

\bibitem[Hussain et~al., 2021]{hussain2021comprehensive}
Hussain, T., Muhammad, K., Ding, W., Lloret, J., Baik, S.~W., and De~Albuquerque, V. H.~C. (2021).
\newblock A comprehensive survey of multi-view video summarization.
\newblock {\em Pattern Recognition}, 109:107567.

\bibitem[Hwang et~al., 2021]{hwang2021multi}
Hwang, H., Kim, G.-H., Hong, S., and Kim, K.-E. (2021).
\newblock Multi-view representation learning via total correlation objective.
\newblock In {\em Advances in Neural Information Processing Systems}, volume~34, pages 12194--12207.

\bibitem[Jordan et~al., 1999]{jordan1999introduction}
Jordan, M.~I., Ghahramani, Z., Jaakkola, T.~S., and Saul, L.~K. (1999).
\newblock An introduction to variational methods for graphical models.
\newblock {\em Machine Learning}, 37:183--233.

\bibitem[Jung et~al., 2022]{jung2022efficient}
Jung, Y., Song, K., and Park, J. (2022).
\newblock Efficient approximate inference for stationary kernel on frequency domain.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Kingma and Ba, 2014]{kingma2015adam}
Kingma, D.~P. and Ba, J. (2014).
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}.

\bibitem[Kingma and Welling, 2013]{kingma2013auto}
Kingma, D.~P. and Welling, M. (2013).
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}.

\bibitem[Lalchand et~al., 2022]{lalchand2022generalised}
Lalchand, V., Ravuri, A., and Lawrence, N.~D. (2022).
\newblock Generalised {GPLVM} with stochastic variational inference.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}.

\bibitem[Lawrence, 2005]{lawrence2005probabilistic}
Lawrence, N. (2005).
\newblock Probabilistic non-linear principal component analysis with {G}aussian process latent variable models.
\newblock {\em Journal of Machine Learning Research}, 6(11).

\bibitem[Lawrence and Quinonero-Candela, 2006]{lawrence2006local}
Lawrence, N.~D. and Quinonero-Candela, J. (2006).
\newblock Local distance preservation in the {GP-LVM} through back constraints.
\newblock In {\em International Conference on Machine learning}.

\bibitem[Li et~al., 2017]{li2017shared}
Li, J., Zhang, B., and Zhang, D. (2017).
\newblock Shared autoencoder {G}aussian process latent variable model for visual classification.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems}, 29(9):4272--4286.

\bibitem[Li et~al., 2024]{li2024preventing}
Li, Y., Lin, Z., Yin, F., and Zhang, M.~M. (2024).
\newblock Preventing model collapse in {G}aussian process latent variable models.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Li et~al., 2018]{li2018survey}
Li, Y., Yang, M., and Zhang, Z. (2018).
\newblock A survey of multi-view representation learning.
\newblock {\em IEEE Transactions on Knowledge and Data Engineering}, 31(10):1863--1883.

\bibitem[Lopez-Paz et~al., 2014]{lopez2014randomized}
Lopez-Paz, D., Sra, S., Smola, A., Ghahramani, Z., and Sch{\"o}lkopf, B. (2014).
\newblock Randomized nonlinear component analysis.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Lu et~al., 2019]{lu2019see}
Lu, X., Wang, W., Ma, C., Shen, J., Shao, L., and Porikli, F. (2019).
\newblock See more, know more: Unsupervised video object segmentation with co-attention siamese networks.
\newblock In {\em IEEE/CVF Conference on Computer Vision and Pattern Recognition}.

\bibitem[Mao et~al., 2023]{mao2023multimodal}
Mao, Y., Zhang, J., Xiang, M., Zhong, Y., and Dai, Y. (2023).
\newblock Multimodal variational auto-encoder based audio-visual segmentation.
\newblock In {\em IEEE/CVF International Conference on Computer Vision}, pages 954--965.

\bibitem[Marmin and Filippone, 2022]{marmin2022deep}
Marmin, S. and Filippone, M. (2022).
\newblock Deep gaussian processes for calibration of computer models (with discussion).
\newblock {\em Bayesian Analysis}, 17(4):1301--1350.

\bibitem[Mohamed et~al., 2020]{mohamed2020monte}
Mohamed, S., Rosca, M., Figurnov, M., and Mnih, A. (2020).
\newblock {M}onte {C}arlo gradient estimation in machine learning.
\newblock {\em Journal of Machine Learning Research}, 21(132):1--62.

\bibitem[Paszke et~al., 2019]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et~al. (2019).
\newblock Py{T}orch: An imperative style, high-performance deep learning library.
\newblock {\em Advances in Neural iInformation Processing Systems}.

\bibitem[Pearson, 1901]{pearson1901liii}
Pearson, K. (1901).
\newblock {LIII}. on lines and planes of closest fit to systems of points in space.
\newblock {\em The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science}, 2(11):559--572.

\bibitem[Plataniotis and Hatzinakos, 2000]{plataniotis2017gaussian}
Plataniotis, K.~N. and Hatzinakos, D. (2000).
\newblock Gaussian mixtures and their applications to signal processing.
\newblock {\em Advanced Signal Processing Handbook}, pages 89--124.

\bibitem[Potapczynski et~al., 2020]{potapczynski2020invertible}
Potapczynski, A., Loaiza-Ganem, G., and Cunningham, J.~P. (2020).
\newblock Invertible {G}aussian reparameterization: Revisiting the {G}umbel-softmax.
\newblock {\em Advances in Neural Information Processing Systems}, 33:12311--12321.

\bibitem[Rahimi and Recht, 2007a]{rahimi2008random}
Rahimi, A. and Recht, B. (2007a).
\newblock Random features for large-scale kernel machines.
\newblock {\em Advances in Neural Information Processing Systems}.

\bibitem[Rahimi and Recht, 2007b]{rahimi2007random}
Rahimi, A. and Recht, B. (2007b).
\newblock Random features for large-scale kernel machines.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Remes et~al., 2017]{remes2017non}
Remes, S., Heinonen, M., and Kaski, S. (2017).
\newblock Non-stationary spectral kernels.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Salimbeni and Deisenroth, 2017]{salimbeni2017doubly}
Salimbeni, H. and Deisenroth, M. (2017).
\newblock Doubly stochastic variational inference for deep {G}aussian processes.
\newblock {\em Advances in neural information processing systems}, 30.

\bibitem[Samo and Roberts, 2015]{samo2015generalized}
Samo, Y.-L.~K. and Roberts, S. (2015).
\newblock Generalized spectral kernels.
\newblock {\em arXiv preprint arXiv:1506.02236}.

\bibitem[Sanabria et~al., 2018]{sanabria2018how2}
Sanabria, R., Caglayan, O., Palaskar, S., Elliott, D., Barrault, L., Specia, L., and Metze, F. (2018).
\newblock How2: a large-scale dataset for multimodal language understanding.
\newblock {\em arXiv preprint arXiv:1811.00347}.

\bibitem[Shi et~al., 2019]{shi2019variational}
Shi, Y., Paige, B., Torr, P., et~al. (2019).
\newblock Variational mixture-of-experts autoencoders for multi-modal deep generative models.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[S{\o}nderby et~al., 2016]{sonderby2016ladder}
S{\o}nderby, C.~K., Raiko, T., Maal{\o}e, L., S{\o}nderby, S.~K., and Winther, O. (2016).
\newblock Ladder variational autoencoders.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Stergiopoulos, 2000]{stergiopoulos2000gaussian}
Stergiopoulos, S. (2000).
\newblock Gaussian mixtures and their applications to signal processing.
\newblock {\em Advanced Signal Processing Handbook}, pages 89--124.

\bibitem[Sun et~al., 2020]{sun2020multi}
Sun, S., Dong, W., and Liu, Q. (2020).
\newblock Multi-view representation learning with deep {G}aussian processes.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 43(12):4453--4468.

\bibitem[Sutter et~al., 2020]{sutter2020multimodal}
Sutter, T., Daunhawer, I., and Vogt, J. (2020).
\newblock Multimodal generative learning utilizing {J}ensen-{S}hannon-divergence.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Sutter et~al., 2021]{sutter2021generalized}
Sutter, T.~M., Daunhawer, I., and Vogt, J.~E. (2021).
\newblock Generalized multimodal {ELBO}.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Theodoridis, 2020]{theodoridis2020machine}
Theodoridis, S. (2020).
\newblock {\em Machine Learning: {A Bayesian} and Optimization Perspective}.
\newblock Academic Press, 2nd edition.

\bibitem[Titsias and Lawrence, 2010]{titsias2010bayesian}
Titsias, M. and Lawrence, N.~D. (2010).
\newblock {Bayesian {G}aussian process latent variable model}.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}.

\bibitem[Tobar et~al., 2015]{tobar2015learning}
Tobar, F., Bui, T.~D., and Turner, R.~E. (2015).
\newblock Learning stationary time series using {G}aussian processes with nonparametric kernels.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Tokui et~al., 2016]{tokui2016reparameterization}
Tokui, S. et~al. (2016).
\newblock Reparameterization trick for discrete variables.
\newblock {\em arXiv preprint arXiv:1611.01239}.

\bibitem[Ton et~al., 2018]{ton2018spatial}
Ton, J.-F., Flaxman, S., Sejdinovic, D., and Bhatt, S. (2018).
\newblock Spatial mapping with {G}aussian processes and nonstationary {F}ourier features.
\newblock {\em Spatial Statistics}, 28:59--78.

\bibitem[Tropp, 2015]{tropp2015introduction}
Tropp, J.~A. (2015).
\newblock An introduction to matrix concentration inequalities.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning}, 8(1-2):1--230.

\bibitem[Wang et~al., 2015]{wang2015deep}
Wang, W., Arora, R., Livescu, K., and Bilmes, J. (2015).
\newblock On deep multi-view representation learning.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Wang et~al., 2021]{wang2021posterior}
Wang, Y., Blei, D., and Cunningham, J.~P. (2021).
\newblock Posterior collapse and latent variable non-identifiability.
\newblock {\em Advances in Neural Information Processing Systems}.

\bibitem[Wei et~al., 2022]{wei2022self}
Wei, Q., Sun, H., Lu, X., and Yin, Y. (2022).
\newblock Self-filtering: A noise-aware sample selection for label noise with confidence penalization.
\newblock In {\em European Conference on Computer Vision}.

\bibitem[Williams and Rasmussen, 2006]{williams2006gaussian}
Williams, C.~K. and Rasmussen, C.~E. (2006).
\newblock {\em Gaussian processes for machine learning}.
\newblock MIT press Cambridge, MA.

\bibitem[Wilson and Adams, 2013]{wilson2013gaussian}
Wilson, A. and Adams, R. (2013).
\newblock Gaussian process kernels for pattern discovery and extrapolation.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Wold et~al., 1987]{wold1987principal}
Wold, S., Esbensen, K., and Geladi, P. (1987).
\newblock Principal component analysis.
\newblock {\em Chemometrics and Intelligent Laboratory Systems}, 2(1-3):37--52.

\bibitem[Woodbury, 1950]{woodbury1950inverting}
Woodbury, M.~A. (1950).
\newblock {\em Inverting modified matrices}.
\newblock Department of Statistics, Princeton University.

\bibitem[Wu and Goodman, 2018]{wu2018multimodal}
Wu, M. and Goodman, N. (2018).
\newblock Multimodal generative models for scalable weakly-supervised learning.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Xu et~al., 2021]{xu2021multi}
Xu, J., Ren, Y., Tang, H., Pu, X., Zhu, X., Zeng, M., and He, L. (2021).
\newblock {Multi-VAE}: Learning disentangled view-common and view-peculiar visual representations for multi-view clustering.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 9234--9243.

\bibitem[Yaglom, 1987]{yaglom1987correlation}
Yaglom, A.~M. (1987).
\newblock {\em Correlation Theory of Stationary and Related Random Functions, Volume I: {B}asic Results}.
\newblock Springer.

\bibitem[Yuan et~al., 2024]{yuan2024learning}
Yuan, S., Cui, J., Li, H., and Han, T. (2024).
\newblock Learning multimodal latent generative models with energy-based prior.
\newblock {\em arXiv preprint arXiv:2409.19862}.

\bibitem[Yuan et~al., 2018]{yuan2018multi}
Yuan, Y., Xun, G., Jia, K., and Zhang, A. (2018).
\newblock A multi-view deep learning framework for {EEG} seizure detection.
\newblock {\em IEEE Journal of Biomedical and Health Informatics}, 23(1):83--94.

\bibitem[Zadeh et~al., 2016]{zadeh2016mosi}
Zadeh, A., Zellers, R., Pincus, E., and Morency, L.-P. (2016).
\newblock Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos.
\newblock {\em arXiv preprint arXiv:1606.06259}.

\bibitem[Zhang et~al., 2018]{zhang2018generalized}
Zhang, C., Fu, H., Hu, Q., Cao, X., Xie, Y., Tao, D., and Xu, D. (2018).
\newblock Generalized latent multi-view subspace clustering.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 42(1):86--99.

\bibitem[Zhang et~al., 2023]{zhang2023bayesian}
Zhang, M.~M., Gundersen, G.~W., and Engelhardt, B.~E. (2023).
\newblock Bayesian non-linear latent variable modeling via random {F}ourier features.
\newblock {\em arXiv preprint arXiv:2306.08352}.

\bibitem[Zhao et~al., 2020]{zhao2020variational}
Zhao, H., Rai, P., Du, L., Buntine, W., Phung, D., and Zhou, M. (2020).
\newblock Variational autoencoders for sparse and overdispersed discrete data.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}.

\bibitem[Zhao et~al., 2017]{zhao2017multi}
Zhao, J., Xie, X., Xu, X., and Sun, S. (2017).
\newblock Multi-view learning overview: {R}ecent progress and new challenges.
\newblock {\em Information Fusion}, 38:43--54.

\bibitem[Zheng and Vedaldi, 2023]{zheng2023online}
Zheng, C. and Vedaldi, A. (2023).
\newblock Online clustered codebook.
\newblock In {\em IEEE/CVF International Conference on Computer Vision}.

\end{thebibliography}





\newpage

\onecolumn
\title{Multi-View Oriented GPLVM: Expressiveness and Efficiency\\(Supplementary Material)}

\begin{flushleft}
    \centering % 居中 % 上横线，粗的
    \rule[0.5ex]{\textwidth}{4pt} % 宽度为全文宽度，粗度为1pt
    \\
    \vspace{1.2em} % 上横线和下面的内容之间的空白
    \Large  
    \textbf{Multi-View Oriented GPLVM: Expressiveness and Efficiency \\ (Supplementary Material)} % 标题内容
    \vspace{1.2em} % 标题和横线之间的空白
    \\
    % 下横线，细的
    \rule[1.5ex]{\textwidth}{1.5pt} % 宽度为全文宽度，粗度为0.5pt
\end{flushleft}

\appendix

% 清除之前的目录记录
\vspace{-0.2in}
\setcounter{tocdepth}{-1} % 停止正文记录
\renewcommand{\contentsname}{\centering\textsc{Appendices}} % 设置附录目录标题
\tableofcontents
\setcounter{tocdepth}{3} % 恢复附录目录深度


\input{content/appendix}


\vfill

\end{document}








