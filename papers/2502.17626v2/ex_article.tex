% SIAM Article Template
\documentclass[hidelinks,onefignum,onetabnum]{siamart220329}
\usepackage{mathdots}
\usepackage{todonotes}
\usepackage{booktabs}
% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{ex_shared}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={
  Preconditioned normal equations for solving discretised partial differential equations
  %Some more comments on the normal equations: With a focus on discretisation of partial differential equations
  },
  pdfauthor={L. Lazzarino, Y. Nakatsukasa, and U. Zerbinati}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

\externaldocument[][nocite]{ex_supplement}

\newtheorem{example}{Example}
\newtheorem{nt}{Notion}
\let\vec\mathbf
\newcommand{\mat}[1]{\underline{\underline{#1}}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
This paper explores preconditioning the normal equation for non-symmetric square linear systems arising from PDE discretization, focusing on methods like CGNE and LSQR. The concept of {``normal''} preconditioning is introduced and a strategy to construct preconditioners studying the associated ``normal'' PDE is presented. Numerical experiments on convection-diffusion problems demonstrate the effectiveness of this approach in achieving fast and stable convergence.
\end{abstract}

% REQUIRED
\begin{keywords}
Preconditioning, normal equation, CGNE, advection-diffusion
\end{keywords}

% REQUIRED
\begin{MSCcodes}
65F08, 65N22, 65H10, 65N30, 65N06, 65N55
\end{MSCcodes}
\section{Introduction}
We are here interested in studying a linear system of the form
\begin{equation}
    \label{eq:linearsystem}
    A\vec{x} = \vec{b}, \qquad A \in \mathbb{R}^{n \times n}, \vec{x} \in \mathbb{R}^n, \vec{b} \in \mathbb{R}^n,
\end{equation}
where $A$ is invertible and not symmetric, i.e., $A \neq A^T$, and $n\gg 1$. In this case, iterative methods, and in particular Krylov subspace methods, are the most viable option.
Given the non-symmetric nature of the problem, arguably the most common Krylov subspace method, is the generalized minimal residual (GMRES) method, \cite{Saad}. GMRES is based on minimizing the 2-norm of the residual in increasingly larger Krylov subspaces $\mathcal{K}_k(A,b) \coloneq \text{span}(b,Ab, \dots , A^kb)$. In the literature, GMRES is often presented as an obvious choice, albeit there is a wide range of applications in which other approaches can have significant advantages.
We here argue for conjugate gradient (CG) like methods \cite{Saad2,Trefethen,Trefethen2,Paige}. Much is known about these methods, such as their convergence behaviour. Hence, we can obtain fast and stable (as we discuss below)
solutions for linear systems of the form \eqref{eq:linearsystem}. To this end, we formulate a linear system equivalent to \eqref{eq:linearsystem}, i.e.~the normal equations
\begin{equation}
    \label{eq:normal}
    B\vec{x} = A^T\vec{b}, \qquad B = A^TA.
\end{equation}
Here, the matrix of coefficients $B$, often called Gram matrix, is a symmetric positive definite (SPD) matrix. Thus, the system \eqref{eq:normal} can be solved by CG. 
The strategy of considering the normal equation and applying CG is well known for solving least-squares problems, and various algorithms have been proposed in this context to implement it. The best known and used are CG normal equation (CGNE) \cite{Saad2}, where the CG method is directly applied to the linear system \eqref{eq:normal}, and LSQR \cite{Paige}, a reformulation of the CGNE algorithm equivalent in exact arithmetic but with better finite precision properties.
Both methods inherit the known analysis of CG, with the only difference that we now obtain results starting from the matrix $B = A^TA$, instead of $A$. In other words, at each iteration we are minimizing the $A^TA$-norm of the forward error in an increasingly larger Krylov subspace, i.e. at the $k$-th iteration, we have
\begin{equation}
	x_k = \text{arg}\min_{x\in \mathcal{K}_k(A^TA,A^Tb)} \|x - x^*\|_{A^TA},
\end{equation}
where $x^*$ is the exact solution of the linear system. This is equivalent to minimize the $2$-norm of the residual in $\mathcal{K}_k(A^TA,A^Tb)$. Note that this implies that at each iteration we are minimizing the same quantity we would minimize in GMRES, but in a different subspace. Moreover, by the convergence analysis of CG, it can be shown that the convergence of CGNE and LSQR is fully determined by the spectral properties of the matrix ${B}$ \cite{Trefethen},
\begin{equation}
	\label{eq:CGbound}
	\frac{\|x^*-x_k\|_B}{\|x^*-x_0\|_B} \leq \underset{p\in P_k}{\text{inf}} \underset{\lambda \in \Lambda(B)}{\text{max}} |p(\lambda)| \leq 2 \left(\frac{\sqrt{\kappa(B)}-1}{\sqrt{\kappa(B)}+1} \right)^k = 2 \left(\frac{\kappa(A)-1}{\kappa(A)+1} \right)^k,
\end{equation}
where $P_k$ is the space of polynomials $p$ of degree less or equal than $n$ and unitary lowest order coefficient, $\Lambda(B)$ is the spectrum of $B$.
The convergence behaviour is not fully characterized by the condition number of $B$, but by its spectral distribution, and that an adaptive bound as in \cite{Strakos2} would be more descriptive of the convergence behaviour of the method. However, bound \eqref{eq:CGbound} indicates that if the condition number of the matrix is very close to 1, then the method will converge very fast independently of the spectral distribution of $B$, and this is enough for our discussion.
The GMRES method is very different from this point of view. Indeed, its convergence is determined by the minimal polynomial of the matrix ${A}$ \cite{Trefethen}, and the spectral properties of $A$ are not enough to fully characterize the convergence of the method. In fact, it can be proven that any nonincreasing GMRES convergence curve is possible for any eigenvalue distribution, \cite{Greenbaum, Strakos}. This implies that preconditioning strategies for this method are often derived by heuristic processes. It is worth mentioning that if the matrix $A$ is nearly orthogonally diagonalizable the convergence of GMRES it is well understood in terms of the spectrum of $A$, \cite[Theorem 35.2]{Trefethen}.

Methods like CGNE and LSQR appear to have been sometimes overlooked in the context of non-symmetric square systems mainly because of the need of accessing the action of $A^T$ and because using the Gram matrix squares the condition number of the problem. However, in the context of PDEs discretisation, the action of the adjoint operator is often easily accessible. Moreover, we argue that studying the normal equation, we can devise new preconditioners that are competitive in the iteration count in spite of the squared condition number. In addition, as shown in \cite{epperly2025,Maike}, it is possible to obtain a backward stable solution by the preconditioned CGNE or LSQR (when carefully implemented) simply by introducing a small number of iterative refinement steps in the algorithm. In other words, if a good preconditioner is employed, we can find a fast and backward stable solution to the non-symmetric square linear system by the preconditioned CGNE/LSQR method. 

In this paper, we first discuss what it means for a preconditioner to be good or ideal in this context, stressing the differences with the GMRES framework.
We revisit a notion of good preconditioner from the least-squares community to introduce the notion of ``normal'' preconditioners for square systems, which characterizes a bigger class of ideal preconditioners. Once equipped with a new notion of good preconditioner, we take into account the physical intuition behind the discretisation of a PDE to construct preconditioners for the normal equation. In particular, the choice of preconditioner can be guided by discretisation of a “normal” PDE associated with the continuous problem; more details are provided in Section \ref{sec:funcanal}.
All numerical experiments here presented are performed with CGNE for ease of implementation. We recall that CGNE and LSQR are equivalent in exact arithmetic and, thus, all formal derivations apply to both. 
In this paper we mainly focus on CGNE, even though LSQR is sometimes observed to have better numerical stability. This is because we often discuss the Gram matrix $A^TA$ (or its appropriate variant of the form $A^TTA$) directly rather than the matrix $A$, and the two methods are both provenly stable with iterative refinement~\cite{epperly2025}, provided a careful implementation of the underlying Lanczos iteration~\cite{musco2018stability} is used.
%Nevertheless, in Section \ref{sec:conclusion}, we discuss the differences one may encounter in the implementation of the presented preconditioner strategies when LSQR is chosen, together with future work.


\section{Preconditioning the normal equations}
In this section, we discuss the use of a preconditioner to improve the convergence of the CGNE method \cite{WathenNormal}.
The most straightforward approach to precondition the normal equation \eqref{eq:normal} is to consider a preconditioned CG method, where the preconditioner is a symmetric positive definite matrix ${G}$.
Such scheme is equivalent either to solving the systems
\begin{equation}
    \label{eq:normalPrecond}
    {G}^{-1}{B}\,\vec{x} = {G}^{-1}{A}^T\vec{b},\text{ or } {B}\,{G}^{-1}y = {A}^T\vec{b}, \text{ with } \vec{x} = {G}^{-1}\vec{y},
\end{equation}
the first system correspond to a left preconditioned CGNE method, while the second to a right preconditioned CGNE method.
Equation \eqref{eq:CGbound} suggests that, if we can find a preconditioner ${G}$ such that ${G}^{-1}{B}$ or ${B}{G}^{-1}$ have $k$ distinct eigenvalues, then the preconditioned CGNE method will converge in at most $k$ iterations. 
In practice, often a preconditioner $G$ is chosen such that ${G}^{-1}{B}$ and ${B}{G}^{-1}$ have clustered eigenvalues.
More often than not, if the matrix ${A}$ arises from the discretisation of a PDE, we have a good physical intuition of what the operator ${A}$ represents.
Using this intuition, lots of preconditioners have been developed \cite{Elman,Wathen,Benzi,Farrell}.

Assuming that we have a good preconditioner ${P}$, can we use ${G} = {P}^T{P}$ as a preconditioner for the CGNE method?
This question has been addressed by D.~Braess and P.~Peisker \cite{Braess} and S.~Graton et al. \cite{Gratton}, and in the recent work by A.~Wathen \cite{WathenNormal}.
First, we would have to agree on what it means for ${P}$ to be a good preconditioner for ${A}$. As mentioned above, an often used notion is the following:
\begin{nt}
    \label{nt:classical}
    $P$ is a good preconditioner for $A$ if ${P}^{-1}{A}$ has clustered eigenvalues.
\end{nt}
Such preconditioners can be shown to be good for (left-preconditioned) GMRES under some assumptions~\cite{Saad}.
Unfortunately, such notion of goodness does not imply that ${P}^T{P}$ is a good preconditioner for the normal equation, since 
a phenomenon called matrix squaring in~\cite{WathenNormal} can cause ${P}^T{P}$ to be an arbitrary bad preconditioner for the normal equation, as shown in \cite{Wathen}.

For this reason, in \cite{Wathen} the matrix ${T}:= I - {A}{P}^{-1}$ is considered, and it is observed that 
    \begin{equation}
        \label{eq:specEquiv}
        {G}^{-1}{B} = {P}^{-1}{P}^{-T}{A}^T{A}\sim {P}^{-T}{A}^T{A}{P}^{-1} = {I} - {T} - {T}^T + {T}^T{T}.
    \end{equation}
    Since ${G}^{-1}{B}$ is similar to a symmetric positive definite matrix its eigenvalues are real. Furthermore, since for any symmetric matrix ${S}$ we have $\Lambda({S})\subset [-\norm{{S}}_2, \norm{{S}}_2]$, we can easily see that
    \begin{equation}
        -1 - 2\norm{{T}}_2 - \norm{{T}}_2^2 \leq \lambda \leq 1 + 2\norm{{T}}_2 + \norm{{T}}_2^2.
    \end{equation}
    Thus, we see that ${P}$ is a good preconditioner for ${A}$ if $\norm{I-{A}{P}^{-1}}_2$ is small. This notion was first introduced by S. Graton et al. \cite{Gratton}.
\section{{Normal preconditioning}}
We begin this section giving a third notion of goodness of a preconditioner $P$, which will highlights a counterintuitive phenomenon related to the side we decide to precondition \eqref{eq:normal}.
Recall that, from \eqref{eq:CGbound}, the convergence of CGNE is related to the spectrum of $G^{-1}B$, in the context of left preconditioning, and to the spectrum of $BG^{-1}$ in the context of right preconditioning.
For the moment, let us focus our attention on the left preconditioned normal equation \eqref{eq:normalPrecond} and observe that from \eqref{eq:specEquiv} we know
\begin{equation}
    G^{-1}B \sim (AP^{-1})^T(AP^{-1}).
\end{equation}
It is immediate to see that if $AP^{-1}$ is an orthogonal matrix, then $G^{-1}B$ reduces to the identity and the left preconditioned normal equation in \eqref{eq:normalPrecond} can be solved by means of the CGNE method {in exact arithmetic in exactly one iteration}. 
\begin{nt}
    \label{nt:crosseyed}
$P$ is a good left preconditioner for the normal equation in \eqref{eq:normalPrecond} if $AP^{-1}$ has clustered singular values. In this case, we will refer to $P$ as a good {normal preconditioner}. 
\end{nt}
{We highlight the ``cross'' effect of the preconditioner action, i.e. in order to have a good left preconditioner for the normal equation, we need a good right preconditioner for $A$, where here the term good refers to the fact that $AP^{-1}$ has clustered singular values rather than eigenvalues.}
Note that this change in the preconditioning side is needed to define this new concept of goodness. Indeed, if $P^{-1}A$ has clustered singular values, $P$ is not necessarily a good preconditioner for CGNE \cite{Wathen}. 
The notion of clustering singular values is widely adopted for example in the randomised linear algebra community, for example Blendenpik~\cite{avron2010blendenpik}, a popular randomised solver for least-squares problems, is based on clustering singular values of $AP^{-1}$. 


We illustrate the idea of {normal preconditioning} for discretised PDEs in the following example.
\begin{example}[{Normal preconditioning}]
    \label{ex:cross-eyed}
    We consider the classical convection diffusion ODE in one dimension, i.e.
    \begin{equation}
        \label{eq:advectionDiffusion1D}
        -\nu \ddot{u} + \beta \dot{u} = f \text{ in }(a,b)\subset\mathbb{R}, \text{ and } u(a)=A,\; u(b)=B,\; \nu,\beta \in \mathbb{R}_{\geq 0}.
    \end{equation}
   We consider neither diffusion nor advection dominated regimes, i.e. $\nu\approx \beta$, and a centred finite difference discretisation over an equi-spaced mesh of step-size $h$.
    Such discretisation result in the linear system
    \begin{equation}
        \label{eq:linsysAdvDiff1D}
        A\vec{x} = \vec{b}, \qquad A=\text{tridiag}\left(-\frac{\nu}{h^2}- \frac{\beta}{2h}, \frac{2\nu}{h^2}, -\frac{\nu}{h^2}+\frac{\beta}{2h}\right),\; \vec{b}_j=f(a+jh).
    \end{equation}
    We are now interested in solving \eqref{eq:linsysAdvDiff1D} via the left preconditioned CGNE method.
    Notice that since the matrix $A$ is tridiagonal, we could solve the linear system \eqref{eq:linsysAdvDiff1DUpWind} in $O(n)$ operations, yet we are here interested in the performance of CGNE, for illustrative purposes.
    We will compare different choices of preconditioners $P$ and always choose $G=P^TP$.
    In particular, since we are interested in highlighting the cross phenomenon, we consider as $P$ the $R$-factor in both the QR and RQ factorisation of $A$, and the left and right polar factors $(A^TA)^{1/2}$ and $(AA^T)^{1/2}$.
    The results for different mesh of $(0,1)$ with step-size $h$, expressed in terms of degrees of freedom $n$, are shown in Table \ref{tab:cross-eyed} for $\nu=\beta=1$ and $A=0$, $B=1$.
    \begin{table}
        %python -i advection.py -n 1000 -exact exp -b 1 -wind 1. -uRHS 1.0 -nu 1 -ksp_type cgne -pc_type svd -ksp_view -ksp_atol 1e-16 -ksp_rtol 1e-20 -ksp_monitor -precond QR
        \centering
        \caption{Comparison of the number of iterations for different preconditioners for the left preconditioned normal equation in \eqref{eq:normalPrecond}.
        The CGNE method was terminated when the absolute residual was less than $10^{-10}$.
        If the method did not converge in $1000$ iterations, we marked the number of iterations with a dash.
        {We marked with a star the cases where we expect the number of iterations in exact arithmetic to be $1$, but due to round-off errors the method converge in $2$ iterations.}
        }
        {
        \label{tab:cross-eyed}
        \begin{tabular}{c|c|c|c|c}
            \toprule
            $n$ & QR & RQ & $Q(A^TA)^{1/2}$ & $(AA^T)^{1/2}Q$ \\
            \midrule
            10  & 1  & 12 & 1 & 3\\
            100 & 1  & -  & 1 & 3\\
            1000& 1  & -  & 2* & 5\\
            \bottomrule
        \end{tabular}
        }
    \end{table}
\end{example}
The previous example highlights another crucial difference between preconditioning \eqref{eq:normal} and \eqref{eq:linearsystem}, i.e.~the ideal preconditioner for the normal equation is not unique. 
In fact, while clustering the eigenvalues of $AP^{-1}$ to a single point, in the ideal scenario, results in $P$ being $A^{-1}$ up to scaling, the same is not true for the normal equation.
Clustering singular values of $AP^{-1}$ to a single point, in the ideal scenario, amounts to $AP^{-1}$ being an orthogonal matrix up to scaling.
The space of orthogonal matrices is a manifold of dimension $n(n-1)/2$~\cite{edel98, epperly2025}. Hence, the space of ideal normal preconditioners for the normal equation is much larger than the space of ideal preconditioners for the linear system.
To demonstrate this, in the previous example, we constructed two different ideal preconditioners for the normal equation, i.e.~the $R$-factor in the QR factorisation of $A$ and the polar factor $(A^TA)^{1/2}$.

Now, we would like to take advantage of the physical intuition behind the discretisation of a PDE to construct preconditioners for the normal equations.
Once again, we would like to begin by studying the following toy example.
\begin{example}[Upwind convection diffusion]
    \label{ex:upwind}
    Let us consider \eqref{eq:advectionDiffusion1D} in the advection dominated regime, i.e.~$\nu \ll \beta$.
    In this framework, it is common to consider a slightly different discretisation with respect to the one in Example \ref{ex:cross-eyed}.
    In fact, the formation of a boundary layer in the solution of \eqref{eq:advectionDiffusion1D} might cause numerical instability if we choose to use centred finite differences to approximate $\dot{u}$.
    For this reason, we consider the upwind discretisation of the advection term \cite{leVeque}, i.e.
    \begin{equation}
        \label{eq:linsysAdvDiff1DUpWind}
        A\vec{x} = \vec{b}, \qquad A=\text{tridiag}\left(-\frac{\nu}{h^2}- \frac{\beta}{h}, \frac{2\nu}{h^2}+\frac{\beta}{h}, -\frac{\nu}{h^2}\right),\; \vec{b}_j=f(a+jh).
    \end{equation}
    While the up-winded discretisation is more stable, we need to keep in mind that we pay the price of having a first order approximation of the advection term, which causes the scheme to be only first order accurate.
    Let us now consider the normal equation \eqref{eq:normal} associated with \eqref{eq:linsysAdvDiff1DUpWind}.
    We would now like to compare the performance, in the advection dominated regime, of the CGNE method with an ideal preconditioner for the normal equation with the performance of GMRES with the corresponding ideal preconditioner.
    We consider as preconditioner,
    \begin{equation}
        P = \text{tridiag}\left(-\frac{\beta}{h}, \frac{\beta}{h},0\right).
    \end{equation}
    Notice that the ``normal'' PDE  associated with the continuous problem is
    \begin{equation}
        -\nu^2 u^{(4)} - \beta^2 \ddot{u} = g, \text{ in }(a,b)\subset\mathbb{R}.
    \end{equation}
    In the limit $\nu\to0$, the ``normal'' PDE can be discretised using a centred finite difference scheme, which, in this case, is approximated by the matrix $P^TP$.
    The results for different discretisations over the unit interval for different values of $\nu$, $\beta=1$ and $A=0$, $B=1$ are shown in Table \ref{tab:upwind} and Table \ref{tab:centered}.
    The number of degrees of freedom has been fixed to $10^4$ for all the experiments.
    \begin{table}
        %python -i fdm_advection.py -n 10000 -exact exp -b 1 -wind 1. -uRHS 1.0 -ksp_type cgne -pc_type lu -ksp_view -ksp_atol 1e-5 -ksp_rtol 1e-20 -ksp_monitor -precond PQR -nu 1e-4 -upwind False  -plot
        \centering
        \caption{Comparison of the number of iterations for different preconditioners for the left preconditioned normal equation in \eqref{eq:normalPrecond}, with $A$ as in \eqref{eq:linsysAdvDiff1DUpWind}.
        The CGNE and GMRES methods were terminated when the absolute residual $\norm{A\vec{x}-\vec{b}}_2$ was less than $10^{-5}$.
        }
        \label{tab:upwind}
        \begin{tabular}{c|c|c|c}
            \toprule
            $\nu$ & P (GMRES) & $R^TR$ (CGNE) & $P^TP$ (CGNE) \\
            \midrule
            $1 \cdot 10^{-2}$  & 1428 & 2105  & 2113 \\
            $5\cdot 10^{-3}$   & 692  & 1052  & 1054 \\
            $1\cdot 10^{-3}$   & 130  & 212  &  209\\
            $5\cdot 10^{-4}$   & 63   & 104  &  102\\
            $1\cdot 10^{-4}$   & 10   & 17  &  17\\
            \bottomrule
        \end{tabular}
    \end{table}
    \begin{table}
        %python -i advection.py -n 1000 -exact exp -b 1 -wind 1. -uRHS 1.0 -nu 1e-3 -ksp_type cgne -pc_type svd -ksp_view -ksp_atol 1e-6 -force -ksp_rtol 1e-20 -ksp_monitor -ksp_lsqr_set_standard_error -ksp_lsqr_exact_mat_norm -precond PTP -plot
        \centering
        \caption{
        Comparison of the number of iterations for different preconditioners for the left preconditioned normal equation in \eqref{eq:normalPrecond}, with $A$ as in \eqref{eq:linsysAdvDiff1D}.        The CGNE and GMRES methods were terminated when the absolute residual $\norm{A\vec{x}-\vec{b}}_2$ was less than $10^{-5}$.
        }
        \label{tab:centered}
        \begin{tabular}{c|c|c|c}
            \toprule
            $\nu$ & P (GMRES) & $R^TR$ (CGNE) & $P^TP$ (CGNE) \\
            \midrule
            $1 \cdot 10^{-2}$  &1435& 2118 & 2117 \\
            $5\cdot 10^{-3}$   & 699& 1073 & 1056 \\
            $1\cdot 10^{-3}$   & 136&  218 &  217\\
            $5\cdot 10^{-4}$   & 69 &  112 &113 \\
            $1\cdot 10^{-4}$   & 16 &   27 &27 \\
            $5\cdot 10^{-5}$   & 2  &    7 &7 \\
            $1\cdot 10^{-5}$   & 5  &    8 &8 \\
            $5\cdot 10^{-6}$   & 4  &    7 &7 \\
            $1\cdot 10^{-6}$   & 3  &    4 &4  \\
            \bottomrule
        \end{tabular}
    \end{table}
\end{example}
One might think that $P^TP$ is a bad preconditioner for the normal equation.
This is not the case in Example \ref{ex:upwind}, where we can see that $P^TP$ is indeed a good preconditioner for the normal equation.
In fact, it is well known that the QR factorisation can be computed from the Cholesky factorisation of $A^TA$, in particular
\begin{equation}
    \label{eq:QR}
    Q = AP^{-1}, \text{ with } B = A^TA \approx P^TP \text{ as } \nu \to 0.
\end{equation}
While one might think that, since the Cholesky factorisation of $A^TA$ is not a stable way to compute the QR factorisation of $A$ \cite{Yuji}, as $n$ grows $P^TP$ becomes a worse and worse preconditioner for the normal equation.
This is not the case as we can see from Table \ref{tab:upwind} and \ref{tab:centered} where we compare $P^TP$ and $R^TR$ as preconditioners, where $R$ is the $R$-factor in the QR factorisation of $P$.

The conclusion that we can draw from the previous examples is that the ideal preconditioner for the normal equation is not unique, and that there might be a vast choice of preconditioners that are good for the normal equation.
In particular, the choice of preconditioner can be guided by discretisations of the ``normal'' PDE associated with the continuous problem, as we will explore in the next section.

\section{Functional Analytic Perspective}
\label{sec:funcanal}
As stated at the end of the previous section, the choice of preconditioner for the normal equation can be guided by the discretisation of the ``normal'' PDE associated with the continuous problem.
We explore the ideas presented above in this section, with a focus on singularly perturbed PDEs {\cite{Verhulst}}, the prototypical example being the convection diffusion equation in the advection dominated regime.
We already discussed the discretisation of the convection diffusion equation in one dimension using a centred finite difference scheme in Example \ref{ex:upwind}.
Consider the two-dimensional advection diffusion equation, i.e.~ 
\begin{equation}
    \label{eq:advectionDiffusion2D}
    \mathcal{L}u := -\nu \Delta u + \vec{\beta}\cdot \nabla u = f \text{ in }\Omega\subset\mathbb{R}^d, \text{ and } u = g \text{ on }\partial\Omega, \text{ with } \nu\ll \norm{\beta},\;\nabla\cdot \beta =0. 
\end{equation}
In particular, we will focus on a finite element discretisation.
To this end, we fix a discrete space $V_h\subset H^1_0(\Omega)$ and consider the weak formulation of \eqref{eq:advectionDiffusion2D} in the discrete space $V_h$, i.e.~find $u_h\in V_h$ such that 
\begin{equation}
    \label{eq:weakForm}
    (\hat{\mathcal{L}}u_h,v_h)\coloneqq\nu (\nabla u_h, \nabla v_h)_{L^2(\Omega)} + (\beta\cdot \nabla u_h,v_h)_{L^2(\Omega)} = (f,v_h)_{L^2(\Omega)}\text{ for any } v_h\in V_h.
\end{equation}
Given a fixed basis $\{\varphi_i\}_{i=1}^{n}$ for $V_h$, the weak formulation introduced above can be expressed as linear system:
\begin{equation}
	A\vec{x}=\vec{b}, \text{ with } A_{ij}=(\hat{\mathcal{L}}\varphi_i,\varphi_j)_{L^2(\Omega)} \text{ and } b_j = (f,\varphi_j)_{L^2(\Omega)}.
\end{equation}
From the previous expression we can work out what PDE is represented by $A^T$, in fact
\begin{equation}
	A^T_{ij} = A_{ji} = (\hat{\mathcal{L}}\varphi_j,\varphi_i)_{L^2(\Omega)} = (\varphi_j, \hat{\mathcal{L}}^*\varphi_i)_{L^2(\Omega)} = (\hat{\mathcal{L}}^*\varphi_j,\varphi_i)_{L^2(\Omega)},
\end{equation}
where $\hat{\mathcal{L}}^*:H^1(\Omega)\to H^1(\Omega)$ is the Hilbert adjoint of $\hat{\mathcal{L}}$, with respect to the $L^2(\Omega)$ inner product, i.e.
\begin{equation}
	(\hat{\mathcal{L}}u,v)_{L^2(\Omega)} =  (u,\hat{\mathcal{L}}^*v)_{L^2(\Omega)} \text{ for any } u,v\in H^1(\Omega).
\end{equation}
{Let us denote with $V_h^\prime$ the dual space of $V_h$}. It is important to notice that $A:V_h\to V_h^\prime$ and $A^T:V_h\to V_h^\prime$, in fact the matrix $A^T$ is neither the Hilbert adjoint of $A$ nor the Banach adjoint of $A$, but it is the Galerkin approximation of the Hilbert adjoint of the operator discretised by $A$.

Due to the range of $A$ being $V_h^\prime$ while the domain of $A^T$  is $V_h$, we observe that even if the normal equation \eqref{eq:normal} makes sense from the finite dimensional linear algebra point of view, it does not have any meaning from the functional analysis point of view, in that it is performing operations on inconsistent function spaces.
For this reason, we need to be careful when considering the normal equation in the context of the discretisation of partial differential equations.
Let us consider a Riesz map $\tau:V_h^\prime\to V_h$ discretised by the matrix $T$. 
{The appropriate choice of the Riesz map is problem dependent. An in depth discussion of an appropriate choice for the advection diffusion equation can be found in Example \ref{ex:L2InnerProduct} and \ref{ex:H1InnerProduct}.}
Consider the normal equation
\begin{equation}
    \label{eq:normalPDE}
    A^T TA\vec{x} = A^T T\vec{b}.
\end{equation}
Given a fixed scalar product $(\cdot,\cdot)$ compatible with the space $V_h$, i.e.~such that there exists a Hilbert space $\big(V,(\cdot,\cdot))$ such that $V_h\subset V$ and $(\cdot,\cdot)|_{V_h\times V_h} = (\cdot,\cdot)$, the Riesz map $\tau$ is elliptic, and therefore the operator $T$ is symmetric positive definite, \cite{Kirby}.
Since the operator $T$ is symmetric positive definite, we can consider the Cholesky factorisation of $T$, i.e.~$T = C^TC$, and rewrite \eqref{eq:normalPDE} as
\begin{equation}
    \label{eq:normalPDECh}
    (CA)^T(CA)\vec{x} = (CA)^TC\vec{b}.
\end{equation}
Thus, \eqref{eq:normalPDE} is the normal equation associated with the linear system
\begin{equation}
    \label{eq:linSysPDE}
    CA\vec{x}= C\vec{b}.
\end{equation}
While at first this manipulation of the normal equation might seem an unnecessary complication, \eqref{eq:normalPDE} has an associated ``normal'' PDE, contrary to \eqref{eq:normal}.

Analogously to \eqref{eq:specEquiv} we can show the following spectral equivalence
\begin{equation}
	\label{eq:specEquiv2}
	\begin{aligned}
	(P^TTP)^{-1}A^TTA & = P^{-1}T^{-1}P^{-T}A^TTA\\
			  & \sim T^{-1}P^{-T}A^TATP^{-1}  \\
			  & =  T^{-1}(AP^{-1})^TT(AP^{-1}).
\end{aligned}
\end{equation}
Therefore, an ideal preconditioner is such that $(AP^{-1})^TT(AP^{-1}) = T$, i.e. $AP^{-1}$ is orthogonal with respect to the inner product induced by $T$. More generally, as stated by the following theorem, this implies that the convergence behaviour of preconditioned CG applied to \eqref{eq:normalPDE} is governed by the singular values of $AP^{-1}$ with respect to the inner product induced by $T$, or equivalently, the singular values of the matrix $CAP^{-1}$.
\begin{theorem}
	\label{thm:conv}
The relative error at the $k$-th iteration of the CG method applied to the preconditioned equation
\begin{equation}
	\label{eq:PrecnormalPDE}
	G_T^{-1}B_T\vec{x} = G_T^{-1}A^TT\vec{b}, \quad G_T = P^TTP, \quad B_T = A^TTA,
\end{equation} 
is such that
\begin{equation}
	\frac{\|x^* - x_k \|_{G_T^{-1}B_T}}{\|x^* - x_0 \|_{G_T^{-1}B_T}} \leq \underset{p\in P_k}{\text{inf}} \underset{\,\,\sigma \in \Sigma_T(AP^{-1})}{\text{max}} |p(\sigma^2)| \leq 2 \left( \frac{\kappa_T(AP^{-1}) -1}{\kappa_T(AP^{-1}) +1} \right)^k,
\end{equation}
where $x^*$ is the exact solution of \eqref{eq:PrecnormalPDE}, $P_k$ is the space of polynomials $p$ of degree 
less than or equal to $k$ and unitary lowest order coefficient, 
\begin{equation}
	\begin{aligned}
		\Sigma_T(AP^{-1}) = \{\sigma \in \text{diag}(\Sigma_T) :& \quad AP^{-1} = U\Sigma_TV^\circ, \\
									& \quad \Sigma_T \text{ non-negative diagonal,} \\
									& \quad U^\circ U = I, V^\circ V = I\},
\end{aligned}
\end{equation}
where for a matrix $M$ we denote by $M^\circ$ the $T$-transpose of $M$, i.e. the matrix such that for any vectors $u,v$, we have $\langle Mv,u\rangle_T = \langle v, M^\circ u\rangle_T$, and $\kappa_T(AP^{-1}) = \sigma_{T,max}(AP^{-1})/\sigma_{T,min}(AP^{-1})$.
\end{theorem}
\begin{proof}
	Note that for a matrix $M$, it holds that $M^\circ = T^{-1}M^TT$. Then, the Gram matrix of $AP^{-1}$ with respect to the inner product induced by $T$ is
	\begin{equation}
		(AP^{-1})^\circ(AP^{-1}) = T^{-1}(AP^{-1})^TT(AP^{-1}).
	\end{equation}
	Moreover, it holds the following relation between the $T$-singular values of $AP^{-1}$ and the eigenvalues of its Gram matrix, 
	\begin{equation}
		\label{eq:singVsEige}
		\sigma^2_{T,i}\left(AP^{-1}\right) = \lambda_i\left(T^{-1}(AP^{-1})^TT(AP^{-1})\right) = \lambda_i\left((P^TTP)^{-1}A^TTA\right),
	\end{equation}
	where the last equality follows from the spectral equivalence in \eqref{eq:specEquiv2}. Thus, applying \eqref{eq:CGbound} to $G_T^{-1}B_T$, we obtain:
\begin{equation}
	\begin{aligned}
		\frac{\|x^* - x_k \|_{G_T^{-1}B_T}}{\|x^* - x_0 \|_{G_T^{-1}B_T}} & \leq \underset{p\in P_k}{\text{inf}} \underset{\,\,\lambda \in \Lambda(G_T^{-1}B_T)}{\text{max}} |p(\lambda)| \\
											      & = \underset{p\in P_k}{\text{inf}} \underset{\,\,\lambda \in \Lambda((AP^{-1})^\circ (AP^{-1}))}{\text{max}} |p(\lambda)|\\ 
											      & = \underset{p\in P_k}{\text{inf}} \underset{\,\,\sigma \in \Sigma_T(AP^{-1})}{\text{max}} |p(\sigma^2)|
	\end{aligned}
\end{equation}
Analogously, by \eqref{eq:CGbound} and \eqref{eq:singVsEige}, we have
\begin{equation}
	\begin{aligned}
		\frac{\|x^* - x_k \|_{G_T^{-1}B_T}}{\|x^* - x_0 \|_{G_T^{-1}B_T}} & \leq 2\left( \frac{\sqrt{\kappa(G_T^{-1}B_T)} -1}{\sqrt{\kappa(G_T^{-1}B_T)} +1} \right)^k\\
										  & = 2\left( \frac{\kappa_T(AP^{-1}) -1}{\kappa_T(AP^{-1}) +1} \right)^k.
	\end{aligned}
\end{equation}
\end{proof}

The role of different inner products in determining the convergence of preconditioned minimal residual methods has been investigated in \cite{PestanaWathen}.
\begin{remark}
	Using the notation introduced in Theorem~\ref{thm:conv}, we can rewrite equation \eqref{eq:normalPDE} as 
	\begin{equation}
        \label{eq:Tnormaleq}
		A^\circ A \vec{x} = A^\circ \vec{b},
	\end{equation}
	and \eqref{eq:PrecnormalPDE} as
	\begin{equation}
		(P^\circ P)^{-1}A^\circ A \vec{x} = (P^\circ P)^{-1} A^\circ \vec{b}.
	\end{equation}
	In other words, we are still considering a (preconditioned) normal equation, but with respect to the inner product induced by $T$.
\end{remark}
\begin{remark}
	When applying the CG method to equation \eqref{eq:normalPDE} we are minimizing the residual in the $T$-norm. Indeed,
	\begin{equation}
		\|x^* -x_k\|_{A^TTA} = (x^*-x_k)^TA^T T A(x^* - x_k) = \|A(x^* - x_k)\|_T = \|r_k\|_T.
	\end{equation}
	Thus, at each iteration CG on \eqref{eq:normalPDE} is minimizing the same quantity as GMRES on \eqref{eq:linearsystem} but in different Krylov subspaces and in different norms.
\end{remark}
\begin{remark}
    We have already pointed out that the choice of ideal preconditioner for the normal equation is not unique, and much larger than the choice of ideal preconditioner for the original linear system, due to the goal being orthogonality of $AP^{-1}$, with respect to the inner product induced  $T$, rather than reducing $AP^{-1}$ to an identity.
    The same observation holds true also for the normal equation \eqref{eq:Tnormaleq}. Furthermore, once we have chosen a Riesz map $\tau$ any absolutely continuous measure with respect to the Lebesgue measure will induce another valid Riesz map.
    Therefore, varying the choice of the Riesz map $\tau$ the possible choices of ideal preconditioners for the normal equation becomes even richer.
\end{remark}

We have already explored the benefits of studying the normal equation associated with the discretisation of a PDE in the context of a simple finite difference discretisation in Example \ref{ex:upwind}. In the next examples, we will explore the same idea in the context of \eqref{eq:weakForm}.
\begin{example}[Preconditioning via anisotropic diffusion]
    \label{ex:L2InnerProduct}
    In Example \ref{ex:upwind} we considered the discretisation of the convection diffusion equation in the advection dominated regime and observed that in the limit $\nu\to 0$ the ``normal'' PDE is a Laplace equation, with diffusion coefficient equal to $\beta^2$.
    In particular, if an upwind discretisation is used, the normal equation in the limit $\nu\to 0$ is equivalent to the discretisation of the Laplace equation with a centred finite difference scheme.
    We might be tempted to see if also in the context of a finite element discretisation, we can find a Laplace equation as the ``normal'' PDE associated to \eqref{eq:weakForm}, in the limit $\nu\to 0$.
    To this end, we consider the Riesz map $\tau:V_h^\prime\to V_h$ associated with the $L^2(\Omega)$ inner product, i.e.~for any $f_h\in V_h^\prime$ we have
    \begin{equation}
        \label{eq:RieszL2}
        (\tau(f_h),v_h)_{L^2(\Omega)} = \langle f_h,v_h\rangle \text{ for any } v_h\in V_h.
    \end{equation}
The question that we need to address is what PDE is the $A^TTA$ discretising. To this end, we consider the continuous analogue of the weak formulation \eqref{eq:weakForm} and \eqref{eq:RieszL2}, and observe that $TA$ is the Galerkin approximation of the following weak formulation
\begin{equation}
	(w,v)_{L^2(\Omega)} = (\hat{\mathcal{L}}u,v)_{L^2(\Omega)} = \nu (\nabla u, \nabla v)_{L^2(\Omega)} + (\beta\cdot \nabla u,v)_{L^2(\Omega)}.
\end{equation}
In the limit $\nu\to 0$ the previous equation reduces to
\begin{equation}
    (w,v)_{L^2(\Omega)} = (\beta\cdot \nabla u,v)_{L^2(\Omega)}.
\end{equation}
Hence, we can conclude that $w = \beta\cdot \nabla u$ and we can compute the weak formulation approximated by $A^TTA$, i.e.
\begin{equation}
    \label{eq:weakFormNormalL2}
    (\mathcal{L}^*w,v)_{L^2(\Omega)} = -\Big(\nabla\cdot(\beta (\beta\cdot \nabla u)),v\Big)_{L^2(\Omega)} =( \beta\otimes \beta \nabla u, \nabla v)_{L^2(\Omega)}.
\end{equation}
These relatively simple calculations suggest the use of a discretisation of the anisotropic diffusion $-\nabla\cdot (\beta\otimes \beta \nabla u)$ as preconditioner for the normal equation $\eqref{eq:normalPDE}$.
\begin{table}
        %python -i advection.py -n 1000 -exact exp -b 1 -wind 1. -uRHS 1.0 -nu 1e-3 -ksp_type cgne -pc_type svd -ksp_view -ksp_atol 1e-6 -force -ksp_rtol 1e-20 -ksp_monitor -ksp_lsqr_set_standard_error -ksp_lsqr_exact_mat_norm -precond PTP -plot
        \caption{Comparison of the number of iterations for the CGNE method preconditioned by the anisotropic diffusion operator $-\nabla\cdot (\beta\otimes \beta \nabla u)$ for different values of $\nu$. The mesh is fixed over the unit square to be $128\times 128$, $\beta = (1,0)$ and as right-hand side we consider the function $f(x,y) \equiv 1$.
        The CGNE method was terminated when the absolute residual was less than $10^{-5}$.}
        \label{tab:L2InnerProduct}
        \centering
        \begin{tabular}{c|c}
            \toprule
            $\nu$ & CGNE Iterations  \\
            \midrule
            $1\cdot 10^{-2}$    & 4231 \\
            $5\cdot 10^{-3}$    & 3803 \\
            $2.5\cdot 10^{-3}$  & 3327\\ 
            $1.25\cdot 10^{-3}$ & 2419\\
            \bottomrule
        \end{tabular}
    \end{table}
Unfortunately, since $A:H^1_0(\Omega)\to H^{-1}(\Omega)$, $A^T:H^1_0(\Omega)\to H^{-1}(\Omega)$ and $T:L^2(\Omega)^\prime\to L^2(\Omega)$ and $L^2(\Omega)^\prime\subset H^{-1}(\Omega)$ the choice of the Riesz map $\tau$ is not consistent with the function spaces of the continuous problem. Thus, as we can see from Table \ref{tab:L2InnerProduct}, the CGNE method is not well preconditioned by the discretisation of the anisotropic diffusion operator.
\end{example}
As we discussed in Example \ref{ex:L2InnerProduct}, the choice of the Riesz map $\tau$ determines the ``normal'' PDE associated with normal equation \eqref{eq:normalPDE}.
Yet, not every choice of $\tau$ is consistent with the function spaces involved in the formulation of the continuous problem. For example, while the choice of the Riesz map $\tau$ in \eqref{eq:RieszL2} leads to a two-dimensional analogue of the ``normal'' PDE discussed in Example \ref{ex:upwind}, the CGNE method was not well preconditioned because the Riesz map $\tau$ was not consistent with the function spaces of the continuous problem.
Hence, we suggest first to choose a consistent Riesz map $\tau$ and then to compute the normal PDE as discussed in Example \ref{ex:H1InnerProduct}.
\begin{remark}
    It is worth noticing that $T$ is a dense matrix hence for an efficient implementation of the CGNE method, we suggest using a matrix free implementation of $A^TTA$.
\end{remark}
\begin{example}[Preconditioning via reaction diffusion]
    \label{ex:H1InnerProduct}
    Once again, we consider the weak formulation of the convection diffusion equation in the advection dominated regime, i.e.~\eqref{eq:weakForm}.
    As Riesz map $\tau$ we consider the one associated with the $H^1_0(\Omega)$ inner product, i.e.
    \begin{equation}
        \label{eq:RieszH1}
        (\nabla \tau(f_h),\nabla v_h)_{L^2(\Omega)} = \nu^{-1}\langle f_h, v_h\rangle \text{ for any } v_h \in V_h.
    \end{equation}
    Once again we consider the continuous analogue of the weak formulation \eqref{eq:weakForm} and \eqref{eq:RieszH1}, and observe that $TA$ is the Galerkin approximation of the following weak formulation
    \begin{align}
        \nu(\nabla w,\nabla v)_{L^2(\Omega)} &= \nu (\nabla u, \nabla v)_{L^2(\Omega)} + (\beta\cdot \nabla u,v)_{L^2(\Omega)}\\
        &= (\nu \nabla u - \beta u,\nabla v)_{L^2(\Omega)}.
    \end{align}
    Hence, we can conclude that $\nabla w = \nabla u - \nu^{-1}\Pi_{\nabla} \beta u$, where $\Pi_{\nabla}$ is the $L^2(\Omega)$ projection onto the space of gradients of functions in $H^1_0(\Omega)$.
    Using the previous result, we can compute the weak formulation approximated by $A^TTA$, i.e.
    \begin{align}
        \label{eq:weakFormNormalH1}
	(\hat{\mathcal{L}}^*w,v)_{L^2(\Omega)} &= \nu(\nabla w,\nabla v)_{L^2(\Omega)} - (\nabla\cdot(\beta w),v)_{L^2(\Omega)} \\
        &= \nu ( \nabla u, \nabla v)_{L^2(\Omega)} + \nu^{-1}(\Pi_{\nabla}\beta u,\beta v)_{L^2(\Omega)} \\
        &- \nu^{-1} (\Pi_{\nabla}\beta u,\nabla v)_{L^2(\Omega)} - \nu^{-1}(\nabla u ,\beta v)_{L^2(\Omega)}\\
        &= \nu ( \nabla u, \nabla v)_{L^2(\Omega)} + \nu^{-1}(\Pi_{\nabla}\beta u,\Pi_{\nabla}\beta v)_{L^2(\Omega)}.
    \end{align}
    Notice that we have used the fact that $(\Pi_{\nabla}\beta u,\nabla v)_{L^2(\Omega)} = -(\nabla u,\Pi_{\nabla}\beta v)_{L^2(\Omega)}$ and that $(\Pi_{\nabla}\beta u,\beta v)_{L^2(\Omega)} = (\Pi_{\nabla}\beta u,\Pi_{\nabla}\beta v)_{L^2(\Omega)}$.
    Therefore, we can precondition the normal equation \eqref{eq:normalPDE} using a discretisation of the variational problem: find $u_h\in V_h$ such that
    \begin{equation}
        \label{eq:discreteReactionDiffusion}
        \nu ( \nabla u_h, \nabla v_h)_{L^2(\Omega)} + \nu^{-1}(\Pi_{\nabla}\beta u_h,\Pi_{\nabla}\beta v_h)_{L^2(\Omega)}, \text{ for any } v_h\in V_h. 
    \end{equation}
    \begin{table}
        \centering
        \caption{Comparison of the number of iterations for the CGNE method preconditioned by the inversion via an LU factorisation of \eqref{eq:discreteReactionDiffusion}, for different values of $\nu$ and different mesh sizes.
        The wind is fixed to $\beta = (1,0)$ and as right-hand side we consider the function $f(x,y) \equiv 1$.
        The CGNE method was terminated when the absolute residual was less than $10^{-5}$.}
        \label{tab:reactionDiffusionLU}
        \begin{tabular}{c|c|c|c}
            \toprule
            $\nu$ & $32\times 32$ & $64\times 64$ & $128\times 128$ \\
            \midrule
            $1\cdot 10^{-2}$    & 2 & 2 & 2\\
            $5\cdot 10^{-3}$    & 3 & 3 & 3\\
            $2.5\cdot 10^{-3}$  & 3 & 3 & 3\\ 
            $1.25\cdot 10^{-3}$ & 3 & 3 & 3\\
            \bottomrule
        \end{tabular}
    \end{table}
    In Table \ref{tab:reactionDiffusionLU} we can see that the CGNE method is well preconditioned by the discretisation of the reaction diffusion operator \eqref{eq:discreteReactionDiffusion}.
    In order to make discretisation robust on coarser meshes, we stabilised the discretisation using a streamline diffusion with parameter $\delta \approx 10^{-4}$. 
    Due to the fact that we adopted a streamline diffusion, we don't expect CGNE to converge in a single iteration. Yet, we can see that the number of iterations is constant as the mesh size grows and the value of $\nu$ decreases.
    Unfortunately, the mass matrix $\nu^{-1}(\Pi_{\nabla}\beta u_h,\Pi_{\nabla}\beta v_h)_{L^2(\Omega)}$ is singular, hence as $\nu$ vanishes it becomes harder and harder to invert it via a multigrid method.
    For this reason, we opted to use as preconditioner the discretisation of the reaction diffusion operator corresponding to the variational problem: find $u_h\in V_h$ such that
    \begin{equation}
        \label{eq:discreteReactionDiffusion2}
        \nu ( \nabla u_h, \nabla v_h)_{L^2(\Omega)} + \nu^{-1}(\beta u_h,\beta v_h)_{L^2(\Omega)}, \text{ for any } v_h\in V_h,
    \end{equation}
    and inverting the preconditioner using PETSc GAMG \cite{Adams}, a smoothed aggregation algebraic multigrid implementation. The results are shown in Table \ref{tab:reactionDiffusionAMG} and Table \ref{tab:reactionDiffusionAMG2}.
    \begin{table}
        \centering
        \caption{Comparison of the number of iterations for the CGNE method preconditioned by the inversion via PETSc GAMG of \eqref{eq:discreteReactionDiffusion2}, for different values of $\nu$ and different mesh sizes.
        The wind is fixed to $\beta = (1,0)$ and as right-hand side we consider the function $f(x,y) \equiv 1$.
        The CGNE method was terminated when the absolute residual was less than $10^{-5}$.}
        \label{tab:reactionDiffusionAMG}
        \begin{tabular}{c|c|c|c|c|c}
            \toprule
            $\nu$ & $32\times 32$ & $64\times 64$ & $128\times 128$ & $256\times 256$&$512\times 512$\\
            \midrule
            $1\cdot 10^{-2}$    &  9 & 14 & 21 & 24 & 26\\
            $5\cdot 10^{-3}$    & 13 & 13 & 19 & 28 & 33\\
            $2.5\cdot 10^{-3}$  & 19 & 17 & 17 & 25 & 37\\ 
            $1.25\cdot 10^{-3}$ & 27 & 24 & 21 & 22 & 33\\
            \bottomrule
        \end{tabular}
        \vspace{1cm}
    \end{table}
    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.074]{figures/u_1e-2.png}
        \includegraphics[scale=0.074]{figures/u_5e-3.png}
        \includegraphics[scale=0.074]{figures/u_2.5e-3.png}
        \includegraphics[scale=0.074]{figures/u_1.25e-3.png}
        
        \vspace{0.5cm}

        \includegraphics[scale=0.074]{figures/div_1e-2.png}
        \includegraphics[scale=0.074]{figures/div_5e-3.png}
        \includegraphics[scale=0.074]{figures/div_2.5e-3.png}
        \includegraphics[scale=0.074]{figures/div_1.25e-3.png}
        \caption{The discrete solution $u_h$ of the reaction diffusion equation \eqref{eq:weakForm}, with $\vec{\beta}=(1,0)$, for different value of $\nu$ at the finest mesh size $512\times 512$, together with $exp(-\lvert\nabla\cdot \beta u_h\lvert^2)$.}
        \label{fig:reactionDiffusionAMG}
    \end{figure}
    \begin{table}
        \centering
        \caption{Comparison of the number of iterations for the CGNE method preconditioned by the inversion via PETSc GAMG of \eqref{eq:discreteReactionDiffusion2}, for different values of $\nu$ and different mesh sizes.
        The wind is fixed to $\norm{\beta}\beta = (1,1)$ and as right-hand side we consider the function $f(x,y) \equiv 1$.
        The CGNE method was terminated when the absolute residual was less than $10^{-5}$.}
        \label{tab:reactionDiffusionAMG2}
        \begin{tabular}{c|c|c|c|c}
            \toprule
            $\nu$ & $32\times 32$ & $64\times 64$ & $128\times 128$ & $256\times 256$\\
            \midrule
            $1\cdot 10^{-2}$    & 10 & 15 & 20 & 23 \\
            $5\cdot 10^{-3}$    & 11 & 15 & 22 & 30\\
            $2.5\cdot 10^{-3}$  & 17 & 16 & 21 & 32\\ 
            $1.25\cdot 10^{-3}$ & 26 & 24 & 23 & 30\\
            \bottomrule
        \end{tabular}
    \end{table}
    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.074]{figures/diag_u_1e-2.png}
        \includegraphics[scale=0.074]{figures/diag_u_5e-3.png}
        \includegraphics[scale=0.074]{figures/diag_u_2.5e-3.png}
        \includegraphics[scale=0.074]{figures/diag_u_1.25e-3.png}
        
        \vspace{0.5cm}

        \includegraphics[scale=0.074]{figures/diag_div_1e-2.png}
        \includegraphics[scale=0.074]{figures/diag_div_5e-3.png}
        \includegraphics[scale=0.074]{figures/diag_div_2.5e-3.png}
        \includegraphics[scale=0.074]{figures/diag_div_1.25e-3.png}
        \caption{The discrete solution $u_h$ of the reaction diffusion equation \eqref{eq:weakForm}, with $\sqrt{2}\vec{\beta}=(1,1)$, for different value of $\nu$ at the finest mesh size $512\times 512$, together with $exp(-\lvert\nabla\cdot \beta u_h\lvert^2)$.}
        \label{fig:reactionDiffusionAMG2}
    \end{figure}
    \begin{figure}[h]
        \centering
        \begin{tikzpicture}[scale=0.75]
            \begin{groupplot}[%
                group style={%
                group size=2 by 2,
                horizontal sep=1.5cm,
                vertical sep=2cm,
                },
            ymajorgrids=true,
            grid style=dashed,
            %ymin = 1e-7, ymax = e2,
            ]       
            \nextgroupplot[width=8cm,height=6cm,domain=2:5,ymode=log, xlabel={Krylov steps}, ylabel={$\norm{Ax-b}_{2}$}, title={$32\times 32$}, %cycle list name=paulcolors, 
            legend style={legend columns=3, draw=none,nodes={scale=.8}}, 
            legend to name=name]
            \addplot+[line width=1.5pt,mark=None] table [x=step, y=res, col sep=comma] {data/fem_advection_mass_normal_eq_diagflow_0.00125_32.0.csv};
            \addplot+[line width=1.5pt,mark=None, dashed] table [x=step, y=res, col sep=comma] {data/fem_advection_mass_normal_eq_diagflow_0.0025_32.0.csv};
            \addplot+[line width=1.5pt,mark=None, dotted] table [x=step, y=res, col sep=comma] {data/fem_advection_mass_normal_eq_diagflow_0.01_32.0.csv};
            \nextgroupplot[width=8cm,height=6cm,domain=2:5,ymode=log, xlabel={Krylov steps}, ylabel={$\,$}, title={$64\times 64$}, %cycle list name=paulcolors, 
            legend style={legend columns=3, draw=none,nodes={scale=.8}}, 
            legend to name=named]
            \addplot+[line width=1.5pt,mark=None] table [x=step, y=res, col sep=comma] {data/fem_advection_mass_normal_eq_diagflow_0.00125_64.0.csv};
            \addplot+[line width=1.5pt,mark=None, dashed] table [x=step, y=res, col sep=comma] {data/fem_advection_mass_normal_eq_diagflow_0.0025_64.0.csv};
            \addplot+[line width=1.5pt,mark=None, dotted] table [x=step, y=res, col sep=comma] {data/fem_advection_mass_normal_eq_diagflow_0.01_64.0.csv};
            \nextgroupplot[width=8cm,height=6cm,domain=2:5,ymode=log, xlabel={Krylov steps}, ylabel={$\norm{Ax-b}_{2}$}, title={$128\times 128$}, %cycle list name=paulcolors, 
            legend style={legend columns=3, draw=none,nodes={scale=.8}}, 
            legend to name=named]
            \addplot+[line width=1.5pt,mark=None] table [x=step, y=res, col sep=comma] {data/fem_advection_mass_normal_eq_diagflow_0.00125_128.0.csv};
            \addplot+[line width=1.5pt,mark=None, dashed] table [x=step, y=res, col sep=comma] {data/fem_advection_mass_normal_eq_diagflow_0.0025_128.0.csv};
            \addplot+[line width=1.5pt,mark=None, dotted] table [x=step, y=res, col sep=comma] {data/fem_advection_mass_normal_eq_diagflow_0.01_128.0.csv};
            \nextgroupplot[width=8cm,height=6cm,domain=2:5,ymode=log, xlabel={Krylov steps}, ylabel={$\,$}, title={$256\times 256$}, %cycle list name=paulcolors, 
            legend style={legend columns=3, draw=none,nodes={scale=.8}}, 
            legend to name=named]
            \addplot+[line width=1.5pt,mark=None] table [x=step, y=res, col sep=comma] {data/fem_advection_mass_normal_eq_diagflow_0.00125_256.0.csv};
            \addplot+[line width=1.5pt,mark=None, dashed] table [x=step, y=res, col sep=comma] {data/fem_advection_mass_normal_eq_diagflow_0.0025_256.0.csv};
            \addplot+[line width=1.5pt,mark=None, dotted] table [x=step, y=res, col sep=comma] {data/fem_advection_mass_normal_eq_diagflow_0.01_256.0.csv};
            \legend{$\nu = 1.25 \cdot 10^{-3}$,$\nu = 2.5 \cdot 10^{-3}$,$\nu = 1 \cdot 10^{-2}$}
            \end{groupplot}
        \end{tikzpicture}
        \pgfplotslegendfromname{named}
        \caption{Comparison of convergence history for the CGNE method preconditioned by the inversion via PETSc GAMG of \eqref{eq:discreteReactionDiffusion2}, for different values of $\nu$ and different mesh sizes.
        The wind is fixed to $\norm{\beta}\beta = (1,1)$ and as right-hand side we consider the function $f(x,y) \equiv 1$.
        The CGNE method was terminated when the absolute residual was less than $10^{-5}$.}
        \label{eq:convergenceHistory}
    \end{figure}
    Observing Table \ref{tab:reactionDiffusionAMG} and Table \ref{tab:reactionDiffusionAMG2}, we might think that for a sufficiently small value of $\nu$ using an algebraic multigrid preconditioner over \eqref{eq:discreteReactionDiffusion2} is mesh-independent.
    Yet, once we refine the mesh enough, we lose the mesh independence of the preconditioner. This is because the mass matrix $\nu^{-1}(\Pi_{\nabla}\beta u_h,\Pi_{\nabla}\beta v_h)_{L^2(\Omega)}$ is equivalent to the mass matrix of the reaction diffusion operator, when $\nabla\cdot (\beta u) = 0$.
    As we can see from Figure \ref{fig:reactionDiffusionAMG} and Figure \ref{fig:reactionDiffusionAMG2}, the smaller the value of $\nu$, the more we need to refine the mesh to have a significant number of elements where $\nabla\cdot (\beta u) = 0$. Since the boundary layer in Figure \ref{fig:reactionDiffusionAMG2} is twice as large as in Figure \ref{fig:reactionDiffusionAMG} \cite{Elman} the above described phenomenon appears on one level of refinement ahead. 
    \begin{table}
        \centering
        \caption{Comparison of the number of iterations for the CGNE method preconditioned by geometric multigrid with SOR smoothing, for different values of $\nu$ and different mesh sizes.
        The wind is fixed to $\sqrt{2}\beta = (1,1)$ and as right-hand side we consider the function $f(x,y) \equiv 1$.
        The CGNE method was terminated when the absolute residual was less than $10^{-5}$.}
        \begin{tabular}{c|c|c|c}
            \toprule
            $\nu$ & $32\times 32$ & $64\times 64$ & $128\times 128$\\
            \midrule
            $1\cdot 10^{-2}$    & 4 & 5 & 8  \\
            $5\cdot 10^{-3}$    & 4 & 5 & 7\\
            $2.5\cdot 10^{-3}$  & 5 & 5 & 7 \\ 
            $1.25\cdot 10^{-3}$ & 7 & 7 & 7 \\
            \bottomrule
        \end{tabular}
        \label{tab:reactionDiffusionGMG}
    \end{table}
\end{example}
In this section, we have shown that the choice of the Riesz map $\tau$ determines the ``normal'' PDE associated with the normal equation, and that the choice of the Riesz map $\tau$ can be guided by the functional spaces involved in the continuous problem.
Once the Riesz map $\tau$ is chosen, we can compute the normal PDE associated with the normal equation and use it as a preconditioner for the normal equation, yielding a well preconditioned CGNE method.
The strategy presented here is general and can be applied to other PDEs, in particular to singularly perturbed PDEs.
\begin{remark}[Relations with LSQ-FEM]
    There is an intimate relation between \eqref{eq:normalPDE} and the Least SQuares Finite Element Method (LSQ-FEM) \cite{Bochev}.
    In fact, \eqref{eq:normalPDE} can be recast as a minimization problem, i.e 
    \begin{equation}
        \vec{x} =  \underset{\vec{w}\in V_h}{\text{argmin}}\; J(\vec{w};\vec{b}), \qquad J(\vec{w};\vec{b})\coloneqq \norm{A\vec{w} - \vec{b}}_T^2.
    \end{equation}
    This effectively reduces the discretisation assosciated with the normal equation \eqref{eq:normalPDE} to a rather non-standard LSQ-FEM discretisation. We stress that this is non-standard because it is typical in LSQ-FEM to view the least square functional as a residual associated with the strong form of the PDE or with one of its first order reformulation.
    Yet a key distinction between the strategy discussed here and a non-standard least square finite elements formulation lies in the fact that while a LSQ-FEM method construct a variational formulation by selecting appropriate norms, function spaces and residual, the strategy here seeks to identify the continuous PDE discretised by the operator \( A^T T A \) for different choices of inner product $T$ and pick the inner product $T$ resulting in the most convenient PDE to precondition.
\end{remark}

All codes used to generate the results presented in this section can be found in the repository \url{https://github.com/UZerbinati/normalPreconditioning}.
\section{Conclusions}
\label{sec:conclusion}
We have presented the idea of {normal preconditioner}, a notion of goodness of a preconditioner for normal equation. As a consequence, we have illustrated the non uniqueness of an ideal {normal preconditioner}. We believe that the large number of ideal {normal preconditioners} already makes a significant case as to why we should regard the problem of preconditioning the normal equation as tamable as preconditioning the original linear system. 

Additionally, we gave practical examples of situations where the normal equation arising from PDEs can be preconditioned to obtain an iteration count comparable to classical preconditioner for the original linear system.  We named the general framework devised to construct such preconditioner: normal preconditioning.
The normal preconditioning framework can be summarised as follows:
\begin{enumerate}
    \item identify the correct class of Riesz maps $\tau:V^\prime\to V$ from the function space involved in the continuous PDE we have discretised;
    \item consider a PDE such that its discretisation $P$ is a good {normal preconditioner} for the normal equation;
    \item consider as preconditioner for the normal equation the matrix $P^TTP$.
\end{enumerate}
In particular, we investigated the advection diffusion equation discretized with both finite difference and finite elements.
In Example \ref{ex:L2InnerProduct}, we highlighted the importance of choosing a consistent Riesz map $\tau$ in \textit{step 1}.
In Example \ref{ex:H1InnerProduct}, following \textit{step 2}, we picked as $P$ the non-stabilised discretisation of the reaction diffusion operator, while we considered as $A$ a streamline diffusion stabilised discretisation of the advection diffusion equation.
We then showed that the preconditioner $P^TTP$ can easily be inverted via multigrid strategies, since it is a reaction-diffusion operator.
Lastly, we showed that this strategy yields a preconditioner that is mesh-independent for a sufficiently small value of $\nu$ and that has a number of iterations comparable to the one obtained with classical preconditioners for the original linear system itself.

We would like to conclude by discussing some questions that we plan to investigate further in the future.
\begin{enumerate}
%	\item \textbf{Solvers for normal equation} have to be investigated. In the above, we have chosen the CGNE algorithm to solve the normal linear system. However, other strategies are available that, even if equivalent in exact arithmetic, lead to speed up in finite precision and have better stability properties. In particular, the LSQR method has been proven to perform better compared to CGNE in finite precision. Moreover, it has been proven that with two cycles of iterative refinement, the preconditioned LSQR method is backward stable, \cite{Maike}. Therefore, adapting the ideas previously described to such method would lead to have a fast and stable method to solve non-symmetric square linear systems. 

\item \textbf{Mixed reformulations} of \eqref{eq:normalPDE} can be used to escape the fact that $A^TTA$ is a dense matrix. Specifically, we aim to study this problem from the point of view of its Schur complement \cite{Wathen}. Such reformulation has many similarities to the discontinuous Petrov--Galerkin and least squares finite element methods \cite{Demkowicz, Ita,Bochev}, which we plan to highlight in future work.

\item \textbf{Different PDEs} can be tackled with the normal preconditioning framework, especially singularly perturbed problems. We aim to extend the presented ideas to the Oseen equation \cite{Elman}, the Helmholtz equation, and the Helmotz--Korteweg equation \cite{Farrell2}.

\end{enumerate}
\section*{Acknowledgements}
The authors would like to thank P.~Brubeck, P.~E.~Farrell, J.~M\'{a}lek, C.~Parker, V.~Simoncini, A.~J.~Wathen, and S.~Zampini for the fruitful discussions and invaluable suggestions, while writing this paper.
\bibliographystyle{siamplain}
\bibliography{references}
\end{document}
