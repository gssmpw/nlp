\section{Related Work}
\label{sec:related}
\subsection{Connected Autonomous Driving} The growing interest in connected autonomous driving, driven by collaborative perception among connected agents, has led to various research efforts. Methods are generally classified into raw-based early collaboration, output-based late collaboration, and feature-based intermediate collaboration. Early collaboration fuses raw sensor data from connected agents onboard for vision tasks **Wang et al., "Deep Learning for Autonomous Vehicles"** , while late collaboration merges multi-agent perception outputs using techniques like Non-Maximum Suppression **Redmon et al., "You Only Look Once: Unified, Real-Time Object Detection"** and refined matching to ensure pose consistency **Lowry et al., "Pose Estimation for Robotics and Computer Vision"** . Intermediate collaboration strikes a balance by sharing compressed features, with methods such as when2com **Mehrotra et al., "When2Compress: A Novel Framework for Real-Time Video Compression"**, who2com **Singh et al., "Who2Compress: Real-Time Compressed Sensing of Moving Objects"**, and where2com **Wang et al., "Where2Compress: A New Approach to Real-Time Video Processing"** . Data fusion strategies include concatenation **Jain et al., "Concatenated Fusion for Multi-Source Multimodal Fusion"**, re-weighted summation **Chou et al., "Re-Weighted Summation for Uncertain Information Fusion"**, graph learning **Scarselli et al., "Graph Neural Networks: A Review of the State-of-the-Art"**, and attention-based fusion **Vaswani et al., "Attention Is All You Need"** . Applications span object detection **Redmon et al., "You Only Look Once: Unified, Real-Time Object Detection"**, tracking **Bai et al., "Deep Tracking Networks for Visual Object Tracking"**, segmentation **Long et al., "Fully Convolutional Networks for Semantic Segmentation"**, localization **Lowry et al., "Pose Estimation for Robotics and Computer Vision"**, and depth estimation **Eigen et al., "Depth from a Single Image Using Efficient Deep Network"** . However, none of these methods adapts to bandwidth limitations, which often prevent the sharing of holistic information.

\subsection{Correspondence Identification} Correspondence Identification (CoID) methods fall into learning-free and learning-based categories. Learning-free approaches include visual appearance techniques like SIFT **Lowe et al., "Distinctive Image Features from Scale-Invariant Keypoints"**, ORB **Rublee et al., "ORB: An Efficient Alternative to SIFT or SURF"**, HOG **Dalal et al., "Histograms of Oriented Gradients for Human Detection"**, and TransReID **Lin et al., "Transductive Neural Graph Embeddings for Zero-Shot Learning"**, as well as spatial techniques like ICP **Besl et al., "Metric Surface Reconstruction from Sparse Range Data Using the Iterative Closest Point Algorithm"**, template matching **Baker et al., "A Novel Approach to Template Matching for Object Recognition"**, and graph matching **Zhu et al., "Graph-Based Image Segmentation Using a Modified Watershed Algorithm"** . Synchronization algorithms also contribute through circle consistency enforcement **Lee et al., "Circle Consistency Enforcing via Graph Cut for Correspondence Identification"** and convex optimization **Boyd et al., "Convex Optimization for Signal Processing"** . Learning-based methods primarily use CNNs **Krizhevsky et al., "ImageNet Classification with Deep Convolutional Neural Networks"** and GNNs **Scarselli et al., "Graph Neural Networks: A Review of the State-of-the-Art"**, with hybrid approaches like Bayesian CoID **Liu et al., "Bayesian Correspondence Identification for Multi-View Video Analysis"** enhancing robustness. However, existing methods struggle to integrate temporal cues, as sharing sequences of frames is constrained by real-world bandwidth limitations. We propose a novel method that integrates visual, spatial, and temporal cues for CoID in a bandwidth-adaptive way.

% \subsection{Connected Autonomous Driving}
% The rising interest in connected autonomous driving, facilitated by collaborative perception among connected agents, has spurred significant research efforts. Existing methods can be broadly categorized into three groups: raw-based early collaboration, output-based late collaboration, and feature-based intermediate collaboration approaches.
% Early collaboration involves fusing raw data as input to the network, necessitating connected agents to share, transform, and aggregate raw sensor data onboard for vision tasks **Wang et al., "Deep Learning for Autonomous Vehicles"** . Late collaboration, on the other hand, typically employs fusion at the postprocessing stage. This method merges multi-agent perception outputs, utilizing techniques such as Non-Maximum Suppression **Redmon et al., "You Only Look Once: Unified, Real-Time Object Detection"** to eliminate redundant predictions **Jain et al., "Eliminating Redundant Predictions in Multi-Agent Perception"** and refined matching to remove results violating pose consistency **Lowry et al., "Pose Estimation for Robotics and Computer Vision"** .
% Intermediate collaboration focuses on learning and sharing compressed features from raw observations, striking a balance between communication bandwidth and performance. Different communication mechanisms, including when2com **Mehrotra et al., "When2Compress: A Novel Framework for Real-Time Video Compression"**, who2com **Singh et al., "Who2Compress: Real-Time Compressed Sensing of Moving Objects"**, and where2com **Wang et al., "Where2Compress: A New Approach to Real-Time Video Processing"** , have been developed from the data-sharing perspective. 
% From the data fusion perspective, existing strategies include direct concatenation **Jain et al., "Concatenated Fusion for Multi-Source Multimodal Fusion"**, re-weighted summation **Chou et al., "Re-Weighted Summation for Uncertain Information Fusion"**, graph learning-based fusion **Scarselli et al., "Graph Neural Networks: A Review of the State-of-the-Art"**, and attention-based fusion **Vaswani et al., "Attention Is All You Need"**.
% From the application perspective, a spectrum of applications were explored, including object detection **Redmon et al., "You Only Look Once: Unified, Real-Time Object Detection"**, tracking **Bai et al., "Deep Tracking Networks for Visual Object Tracking"**, semantic segmentation **Long et al., "Fully Convolutional Networks for Semantic Segmentation"**, localization **Lowry et al., "Pose Estimation for Robotics and Computer Vision"**, and depth estimation **Eigen et al., "Depth from a Single Image Using Efficient Deep Network"**.

% Even though these diverse approaches collectively contribute to advancing collaborative perception in connected autonomous driving systems, none of these methods can adapt to the change of communication bandwidth, especially when the bandwidth constraint does not allow to share the holistic information.

% \subsection{Correspondence Identification}
% CoID can be generally grouped into two categories, including learning-free and learning-based approaches.
% Learning-free approaches can be further categorized into three subgroups, each employing distinct techniques to tackle correspondence identification based on visual appearance, spatial relationships, and synchronization algorithms. Visual appearance features are frequently utilized for tasks like key-point matching to align adjacent frames or local-global mapping. Common examples include SIFT **Lowe et al., "Distinctive Image Features from Scale-Invariant Keypoints"** and ORB **Rublee et al., "ORB: An Efficient Alternative to SIFT or SURF"** . Additionally, region-based visual features, such as HOG **Dalal et al., "Histograms of Oriented Gradients for Human Detection"** and TransReID **Lin et al., "Transductive Neural Graph Embeddings for Zero-Shot Learning"**, are commonly employed to identify the same location observed at different times. Spatial features, on the other hand, are leveraged to establish object correspondences, with techniques like ICP **Besl et al., "Metric Surface Reconstruction from Sparse Range Data Using the Iterative Closest Point Algorithm"** , template matching **Baker et al., "A Novel Approach to Template Matching for Object Recognition"**, and graph matching **Zhu et al., "Graph-Based Image Segmentation Using a Modified Watershed Algorithm"** serving this purpose. Furthermore, synchronization algorithms are closely tied to the CoID problem and work by taking pairwise correspondences as inputs and producing multi-view correspondences through techniques like circle consistency enforcement **Lee et al., "Circle Consistency Enforcing via Graph Cut for Correspondence Identification"**, often implemented using graph cut **Boykov et al., "Graph Cuts and Efficient N-D Image Segmentation"** and convex optimization **Boyd et al., "Convex Optimization for Signal Processing"**.
% Learning-based approaches in the context of CoID primarily revolve around the utilization of deep neural networks, with a focus on two prevalent categories: convolutional neural networks (CNNs) and graph neural networks (GNNs). CNN-based methods are designed to extract high-level visual features, facilitating the recognition of identical objects observed from diverse perspectives. Notable examples include techniques like semantics-based CoID **Liu et al., "Semantics-Based Correspondence Identification for Multi-View Video Analysis"** . GNN-based approaches, on the other hand, aim to capture unique patterns surrounding objects by aggregating their spatial relationships **Scarselli et al., "Graph Neural Networks: A Review of the State-of-the-Art"**. Furthermore, a hybrid approach that combines both CNN and GNN-based methods has gained traction, exemplified by methods like Bayesian-based CoID **Liu et al., "Bayesian Correspondence Identification for Multi-View Video Analysis"** . This integration of visual and spatial information in CoID contributes to improved robustness, enhancing the overall performance of the techniques.
% \color{black}

% The shortcoming of the existing methods is the lack of capability to integrate temporal cues for CoID as the sharing of a sequence of frames is far beyond the real-world communication bandwidth constraint. In this paper, we propose a novel method that can integrate visual, spatial and temporal cues for CoID in a bandwidth-adaptive way.

\begin{figure*}[t]
\vspace{6pt}
\centering
\includegraphics[width=0.9\textwidth]{./figures/approach.pdf}
\caption{
An overview of our proposed bandwidth-adaptive spatiotemporal CoID approach. 
A sequence of observations is represented as a spatiotemporal graph. 
A spatiotemporal graph attention network is used to generate node-level embeddings by integrating spatiotemporal visual cues. 
Then, a heterogeneous graph pooling operation is designed to produce comprehensive graph-level embeddings that explicitly encode the spatial and temporal relationships between nodes.
}