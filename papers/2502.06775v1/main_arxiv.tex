\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{ulem}
\usepackage{pgfplots}
\usepackage{graphicx,lipsum}
\usepackage{float}    % For tables and other floats
\usepackage{verbatim} % For comments and other
\usepackage{amsmath, amssymb, amsthm, mathtools}  % For math
\usepackage{subfigure}   % For subfigures
\usepackage[colorlinks=true,citecolor=blue,linkcolor=blue,urlcolor=blue]{hyperref}
\usepackage{bookmark}
\usepackage{fullpage}
\usepackage{enumerate}
\usepackage{paralist}
\usepackage{optidef}
\usepackage{xspace}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{bbm}
\usepackage{bm}
\usepackage{url}

\usepackage{lipsum}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{color}
\usepackage{microtype}
\usepackage[numbers]{natbib}
\usepackage[section]{placeins}
\usepackage{lscape}
\usepackage{multirow}
\usepackage{todonotes}
\usepackage{cleveref}




\newcommand{\vertiii}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1
		\right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\newcommand{\vect}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\x}{\vect{x}}
\newcommand{\rvbeta}{\vect{\beta}}
\newcommand{\rvzero}{\vect{0}}
\newcommand{\w}{\vect{w}}
\newcommand{\bu}{\vect{u}}
\newcommand{\bv}{\vect{v}}
\newcommand{\X}{\vect{X}}
\newcommand{\Y}{\vect{Y}}
\newcommand{\y}{\vect{y}}
\newcommand{\z}{\vect{z}}
\newcommand{\M}{\vect{M}^{\star}}
\newcommand{\U}{\vect{U}}
\newcommand{\V}{\vect{V}}
\newcommand{\Z}{\vect{Z}}
\newcommand{\D}{\vect{D}}
\newcommand{\bd}{\vect{d}}
\DeclareMathOperator{\id}{I}
\providecommand{\tabularnewline}{\\}
\newcommand{\Deltatsecond}{\Delta_t}
\DeclareMathOperator{\Aug}{Aug}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\Grad}{Grad}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\ReLU}{\textsf{ReLU}}
\DeclareMathOperator{\PAC}{\textsf{PAC}}
\DeclareMathOperator{\PB}{\textsf{PB}}

\DeclareMathOperator{\supp}{supp}

\providecommand{\SF}[1]{{\protect\color{blue}{[SF: #1]}}}


\newcommand{\smoothing}{s}

\newcommand{\err}{\epsilon}

\newcommand{\radius}{{\err_0}}

\newcommand{\rank}{\mathrm{rank}}

\newcommand{\proj}{\cP}

\newcommand{\subG}{\mathbf{subG}}

\newcommand{\inner}[2]{\left\langle #1, #2 \right\rangle}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}


\newcommand{\tr}{{\operatorname{tr}}}


%%%%%%%%%%%%%%%%%% Note %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\bigbracket}[1]{\left(#1\right)}
\newcommand{\bigsquare}[1]{\left[#1\right]}
\newcommand{\tmD}{\Tilde{\mD}}
\newcommand{\tf}{\Tilde{f}}
\newcommand{\trvd}{\Tilde{\rvd}}
\newcommand{\hrvd}{\widehat{\rvd}}
\newcommand{\whm}{\widehat{m}}
\newcommand{\var}{\mathrm{Var}}





\newenvironment{note}[1]{\medskip\noindent \textbf{#1:}}%
{\medskip}

\newenvironment{proofsketch}{\noindent{\bf Proof Sketch.}}%
{\hspace*{\fill}$\Box$\par\vspace{4mm}}
\newenvironment{proofof}[1]{\smallskip\noindent{\bf Proof of #1.}}%
{\hspace*{\fill}$\Box$\par}




%%%%%%%%%%%%%%%%%% characters %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\measurementnumber}{m}
%\newcommand{\samples}{{\measurementnumber}} 

\newcommand{\bA}{\mathbb{A}}
\newcommand{\bB}{\mathbb{B}}
\newcommand{\bC}{\mathbb{C}}

\newcommand{\bE}{\mathbb{E}}
\newcommand{\bF}{\mathbb{F}}
\newcommand{\bG}{\mathbb{G}}
\newcommand{\bH}{\mathbb{H}}
\newcommand{\bI}{\mathbb{I}}
\newcommand{\bJ}{\mathbb{J}}
\newcommand{\bK}{\mathbb{K}}
\newcommand{\bL}{\mathbb{L}}
\newcommand{\bM}{\mathbb{M}}
\newcommand{\bN}{\mathbb{N}}
\newcommand{\bO}{\mathbb{O}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\bQ}{\mathbb{Q}}
\newcommand{\bR}{\mathbb{R}}
\newcommand{\bS}{\mathbb{S}}
\newcommand{\bT}{\mathbb{T}}
\newcommand{\bU}{\mathbb{U}}
\newcommand{\bV}{\mathbb{V}}
\newcommand{\bW}{\mathbb{W}}


\newcommand{\bZ}{\mathbb{Z}}
\newcommand{\mle}{\mathrm{MLE}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}

\newcommand{\clip}{\mathrm{CLIP}}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{notation}{Notation}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}
\newtheorem{assumption}{Assumption}
\newtheorem{claim}{Claim}
\newtheorem{fact}{Fact}

\input{math_commands.tex}



\newcommand{\td}{\Tilde{d}}
\newcommand{\bbracket}[1]{\left(#1\right)}
\newcommand{\bip}[1]{\left\langle#1\right\rangle}
\newcommand{\bnorm}[1]{\left\|#1\right\|}
\title{Enhancing Performance of Explainable AI Models with Constrained Concept Refinement}

\author{
Geyu Liang\\
University of Michigan\\
\texttt{lianggy@umich.edu}\\
\and 
Senne Michielssen\\
Princeton University\\ 
\texttt{senne@princeton.edu }
\and
Salar Fattahi\\
University of Michigan\\
\texttt{fattahi@umich.edu}
}

\begin{document}

\maketitle

\begin{abstract}
    The trade-off between accuracy and interpretability has long been a challenge in machine learning (ML). This tension is particularly significant for emerging \textit{interpretable-by-design} methods, which aim to redesign ML algorithms for trustworthy interpretability but often sacrifice accuracy in the process. In this paper, we address this gap by investigating the impact of deviations in concept representations—an essential component of interpretable models—on prediction performance and propose a novel framework to mitigate these effects. The framework builds on the principle of optimizing concept embeddings under constraints that preserve interpretability. Using a generative model as a test-bed, we rigorously prove that our algorithm achieves zero loss while progressively enhancing the interpretability of the resulting model. Additionally, we evaluate the practical performance of our proposed framework in generating explainable predictions for image classification tasks across various benchmarks. Compared to existing explainable methods, our approach not only improves prediction accuracy while preserving model interpretability across various large-scale benchmarks but also achieves this with significantly lower computational cost.
\end{abstract}


\section{Introduction}
ML algorithms are often caught in a dilemma between interpretability and performance. Models such as linear regression \cite{hastie2009elements} and decision trees \cite{quinlan1986induction} provide straightforward interpretability through parameter weights and rule-based predictions. However, they frequently underperform on complex tasks. On the other hand, high-performing models, such as deep neural networks \cite{lecun2015deep} and large language models \cite{vaswani2017attention}, are notoriously opaque, given their large parameter spaces and intricate architectures. 

Numerous methods have been proposed to extract interpretability from complex models \cite{baehrens2010explain, simonyan2013deep, zeiler2014visualizing, 
shrikumar2017learning, selvaraju2017grad, smilkov2017smoothgrad, kolek2020rate, 
subramanya2019fooling}. However, these approaches typically adopt post-hoc strategies, utilizing sensitivity analysis to identify the key parameters that influence predictions. Consequently, these methods lack guarantees of providing explanations for random prediction or ensuring their trustworthiness \cite{adebayo2018sanity, rudin2019stop, 
kindermans2019reliability, ghorbani2019interpretation, slack2020fooling}. 

An alternative solution is to develop models that are \textit{interpretable by design}. These models intrinsically integrate transparency into their architecture, providing explanations directly tied to their predictions. Concept Bottleneck Models (CBMs, \cite{koh2020concept}) exemplify this approach by mapping input data to an intermediate representation of human-defined concepts, which is then used for prediction. While concept-based models 
are promising, they require datasets annotated with concept scores, limiting their applicability. Extensions of CBMs aim to improve the approach by utilizing pretrained encoders like CLIP \cite{yuksekgonul2022post, oikarinen2023label}. A recent line of research \cite{chattopadhyay2022interpretable, chattopadhyay2023variational, chattopadhyay2024information} has introduced a decision-tree-based architecture with enhanced explainability. The predictions of the model are generated through a sequence of queries, each closely associated with human-specified concepts.

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/flowchart.png}
    \caption{The red arrows represent the backpropagation training process for 
    classic explainable AI models. This paper extends the training process to refine concept 
    embeddings with constraints on their deviation from initial embeddings, represented by 
    green arrows and box.}
    \label{fig:flowchart}
\end{figure*}

A high level design paradigm of expainable AI is illustrated in 
\Cref{fig:flowchart}. Within these models, concept embeddings play a pivotal role: they not only influence prediction generation but also serve as the source of interpretability. However, recent studies have raised several questions against its reliability. These challenges are centered around its ambiguity \cite{margeloiu2021concept, watson2022conceptual, kim2023probabilistic, marconato2023interpretability}, fragility \cite{furby2024can}, and low accuracy \cite{zarlenga2022concept, chowdhury2024adacbm}. For embeddings generated with pretrained models, their encoders are reported to face issues such as domain adaptation \cite{shao2023investigating, gondal2024domain, feng2024rethinking, wang2024attention}, biases in pretraining data \cite{hamidieh2024identifying}, 
and sensitivity to encoding errors \cite{ranjan2024unveiling}.


Efforts to address these challenges include introducing unsupervised modules to enhance concept expressiveness \cite{sawada2022concept}, training residual learning modules in parallel with concept encoders \cite{yuksekgonul2022post}, learning dual embeddings per concept \cite{zarlenga2022concept}, and adding specialized data encoders to address domain shifts \cite{chowdhury2024adacbm}. However, these approaches all involve introducing additional black-box learning modules, which undermines the purpose of explainable AI. Moreover, theoretical guarantees for such approaches, which is of particular interest for safety-critical domains, such as medical imaging \cite{barragan2021artificial} and autonomous vehicles \cite{bensalem2023indeed}, remain largely unexplored. 

To address these challenges, this paper proposes a novel framework termed 
\textit{Constrained Concept Refinement (CCR)}. Unlike existing methods that introduce additional learning modules, our approach directly optimizes concept embeddings within a constrained parameter space. By restricting embeddings to a small neighborhood around their initialization, our framework offers a tunable trade-off between performance and interpretability, and in certain settings, it can simultaneously improve both. Specifically, we present the following contributions:

\begin{itemize}
    \item \textbf{Theoretical necessity.} We rigorously demonstrate the necessity of refining concept embeddings by establishing a non-vanishing worst-case lower bound on model performance when such refinements are not applied to the concepts (\Cref{theorem: formal necessity}). This highlights and motivates the idea of concept embedding refinement, which is at the crux of our proposed method. 
    \item \textbf{Accuracy and interpretability guarantees.} We show that CCR overcomes the aforementioned challenge by eliminating performance degradation through the appropriate refinement of concepts (\Cref{theorem: vanishing L convergence}). Furthermore, to quantify the performance of CCR, we consider a generative model as a test-bed, where both ``interpretability'' and ``accuracy'' admit crisp mathematical definitions. Under this model, we demonstrate that CCR achieves zero training loss while progressively enhancing interpretability (\Cref{theorem: columnwise convergence}).
    \item \textbf{Application in interpretable image classification.} We demonstrate the practical efficacy of CCR on multiple image classification benchmarks including CIFAR 10/100 \cite{krizhevsky2009learning}, ImageNet \cite{deng2009imagenet}, CUB200 \cite{wah2011caltech} and Places365 \cite{zhou2017places}. In all benchmarks except CUB-200, CCR outperforms two recently developed explainable methods for visual classification tasks—CLIP-IP-OMP \cite{chattopadhyay2024information} and label-free CBM (lf-CBM) \cite{oikarinen2023label}—in terms of prediction accuracy while preserving interpretability, achieving this with a tenfold reduction in runtime.
\end{itemize}

% \subsection{Notations}
\paragraph{Notations.} For a matrix $\mA$, we denote its spectral norm by $\|\rmA\|_2$, Frobenius norm by $\|\rmA\|_F$, and maximum column-wise $\ell_2$-norm by $\|\rmA\|_{1,2}$. Define $[n] = \{1, 2, \dots, n\}$. A matrix $\mA\in \mathbb{R}^{n\times m}$ is called column-orthogonal if $\mA^\top\mA = \mI_{m\times m}$. Events with probability at least $1-n^{-\omega(1)}$ are said to occur with high probability.

\section{Revisiting IP-OMP}\label{subsection: setup}
We consider a classical \textit{interpretable-by-design} setting in the literature \cite{chattopadhyay2022interpretable,chattopadhyay2024information}, namely the task of predicting a random variable by sequentially selecting a list of related random variables, termed \textit{queries}, and checking their values. The problem is defined as follows:
\begin{definition}[Single Variable Prediction by Query Selection]\label{definition: single variable prediction by query selection}
Given an \textit{input feature} $\rvx\in \mathbb{R}^d$, a set of {\it queries} $\{q_i\}_{i=1}^n\subset \mathbb{R}$, and their associated {\it query features} (also referred to as \textit{concept embeddings}) $\{\rvd_i\}_{i=1}^n\subset \mathbb{R}^d$, the objective is to predict the {\it target random variable} $y\in \mathbb{R}$ by selecting at most $k$ queries from $\{q_i\}_{i=1}^n$, leveraging the query features $\{\rvd_i\}_{i=1}^n$ to guide the selection process. Specifically, the query set $\{q_i\}_{i=1}^n$ consists of random variables that are correlated with $y$, whose observed values can contribute to predicting $y$. 
\end{definition}
\Cref{definition: single variable prediction by query selection} encompasses a broad spectrum of ML problems with applications in animal identification \cite{lampert2009learning}, medical diagnosis \cite{graziani2018regression,clough2019global}, visual question-answering \cite{yi2018neural}, image retrieval \cite{bucher2019semantic}, and so on.

One illustrative example is the game played among friends, where the target random variable refers to the answer to the question {``what am I thinking now?''}. In this context, the queries $\{q_i\}_{i=1}^n$ are binary random variables representing ``yes/no'' answers to specific questions like ``{Is it an animal?}''. Here, the query feature is a vector embedding of each question, encapsulating the information relevant to that question. Additionally, the participants are limited to asking at most  $k$ questions. 

In \cite{geman1996active}, the authors introduced a classic greedy-style algorithm termed {\it Information Pursuit} (IP) to address the task outlined in \Cref{definition: single variable prediction by query selection}. Specifically, IP iteratively selects the most informative query based on the observed values of previously selected queries. We provide a formal definition of IP for subsequent discussion:
\begin{definition}[Information Pursuit]\label{definition: information pursuit}
At every iteration $t=1,...,k$, IP selects $\pi(t)$-th query according to:
\begin{align*}
    \pi(t) = \argmax_{1\le i\le n} &\ \Big\{I(q_i,y\mid q_{\pi(1)}=r_{\pi(1)},\\
    &q_{\pi(2)}=r_{\pi(2)},...,q_{\pi(t-1)}=r_{\pi(t-1)})\Big\}.
\end{align*}
Here $I(\cdot,\cdot)$ denotes mutual information, $\pi(\cdot):[k]\rightarrow [n]$ is an injective map representing the selection of queries, and $r_i$ denotes the observed value for $q_{i}$. The final output of IP is defined as the maximum likelihood estimator given all observed queries:
\begin{align}\label{eq::MLE}
    y_{\mathrm{IP}} = \argmax_{\tilde{y}}&\ \Big\{ \mathbb{P}(y=\tilde{y}\mid q_{\pi(1)}=r_{\pi(1)},\nonumber\\
    &q_{\pi(2)}=r_{\pi(2)},...,q_{\pi(k)}=r_{\pi(k)})\Big\}.
\end{align}
\end{definition}
In \cite{chattopadhyay2023variational}, IP is demonstrated to achieve highly interpretable predictions with competitive accuracies. However, a significant limitation of IP is its substantial computational expense, as mutual information depends not only on the set \(\{\rvd_i\}_{i=1}^n\) but also on previous observations, leading to an exponentially large input space. \cite{chattopadhyay2024information} addresses this complexity by imposing additional assumptions on the underlying generative model. The first quantifies the connection between \(y\) and \(q_i\), while the second defines their connections with \( \rvd_i\).


\begin{assumption}[Generative model for IP-OMP]\label{assumption: generative model for ip-omp}
There exists an unknown random vector $\rvz\in \mathbb{R}^d$ drawn from a standard Normal distribution that satisfies $y = \langle \rvx, \rvz \rangle$. Moreover, the queries satisfy $q_i = \langle \rvv_i, \rvz \rangle$, where $\rvv_i$ is a {\it latent feature vector} associated with query $i$.
\end{assumption}
Under the above assumption, the target random variable and the queries are correlated through the unknown random variable $\rvz$. In this setting, a natural way to define the query features is by setting $\rvd_i = \rvv_i$. This entails an exact prior knowledge of the latent feature vectors.
\begin{assumption}[Prior knowledge of latent feature vectors]\label{assumption: accurate observation}
The latent feature vectors $\{\rvv_i\}_{i=1}^n$ are precisely observed.
\end{assumption}
Under \Cref{assumption: generative model for ip-omp} and \ref{assumption: accurate observation}, \cite{chattopadhyay2024information} establish a connection between IP and Orthogonal Matching Pursuit (OMP) \cite{pati1993orthogonal}, a seminal algorithm in the field of sparse coding.

\begin{theorem}[IP-OMP \cite{chattopadhyay2024information}]\label{theorem: ip-omp equivalency}
    Under \Cref{assumption: generative model for ip-omp} and \ref{assumption: accurate observation} and upon setting $\rvd_i = \rvv_i$ for $i=1,\dots, n$, the selection rule \(\pi(\cdot)\) defined in \Cref{definition: information pursuit} admits a closed-form expression given by:
    \begin{align*}
        \pi(t) = \argmax_{1 \leq i \leq n} \frac{|\langle \Pi^\perp_{\mD^{(t-1)}} \rvd_i, \Pi^\perp_{\mD^{(t-1)}} \rvx \rangle|}{\|\Pi^\perp_{\mD^{(t-1)}} \rvd_i\|_2 \|\Pi^\perp_{\mD^{(t-1)}} \rvx\|_2},
    \end{align*}
    where \(\mD^{(t-1)} \!=\! \begin{bmatrix}
        \rvd_{\pi(1)} & \rvd_{\pi(2)} & \cdots & \rvd_{\pi(t-1)}
    \end{bmatrix}\). 
    This selection criterion, referred to as IP-OMP, differs from OMP solely by the inclusion of the normalization term $\|\Pi^\perp_{\mD_{t-1}} \rvd_i\|_2 \|\Pi^\perp_{\mD_{t-1}} \rvx\|_2$.
\end{theorem}

The main theoretical contribution of \Cref{theorem: ip-omp equivalency} lies in its ability to transform the computation of mutual information into that of a linear projection, thereby significantly reducing computational complexity. Although the equivalence is derived under simplifying assumptions, IP-OMP demonstrates remarkable generalization capabilities across various benchmark experiments, outperforming models based on the classical IP algorithm \cite{chattopadhyay2024information}.
\subsection{A Major Hurdle: Inaccurate Query Features}\label{subsection: motivation}
As discussed in the previous section, \Cref{theorem: ip-omp equivalency}, and as a result, the success of IP-OMP is contingent upon correctly choosing the query features $\{\rvd_i\}_{i=1}^n$.

For learning algorithms, the effective utilization of \(\{\rvd_i\}_{i=1}^n\) is crucial for the accurate prediction of \(y\). On the other hand, humans derive comprehension from \(\{\rvd_i\}_{i=1}^n\) as these elements are intended to embed concepts that are interpretable by humans. In practical applications, \(\{\rvd_i\}_{i=1}^n\) are either embedded and learned from predefined datasets where concepts are explicitly labeled \cite{koh2020concept} or generated by pretrained multimodal models, with CLIP being the most popular example \cite{oikarinen2023label}.

However, these learned embeddings are often misaligned or inaccurate due to the inherent ambiguity and noise in the training of the models used to generate. For example, unlike carefully curated datasets, CLIP relies on large-scale, noisy image-text pairs scraped from the internet. These pairs often include mislabeled data, vague descriptions, or culturally biased associations, resulting in inconsistencies in representation, as recently reported by~\cite{dutta2023estimating}.

This suggests that \Cref{assumption: accurate observation} may be overly optimistic. In this section, we will illustrate how the violation of \Cref{assumption: accurate observation}—specifically, deviations of the available query features from the latent feature vectors—results in a proportional degradation in the performance of IP-OMP. For the remainder of this paper, we denote the observed query feature set compactly by the matrix \(\mD = \begin{bmatrix}
    \rvd_1 & \rvd_2 & \cdots & \rvd_n
\end{bmatrix} \in \mathbb{R}^{d \times n}\). This matrix \(\mD\) is also referred to as a \textit{feature query matrix}. We also refer to \(\mD^* = \begin{bmatrix}
    \rvv_1 & \rvv_2 & \cdots & \rvv_n
\end{bmatrix} \in \mathbb{R}^{d \times n}\) as the ground truth {\it latent feature matrix}. \Cref{assumption: accurate observation} can now be interpreted as assuming $\mD=\mD^*$, stating that we are using the ground truth latent feature vectors as the feature query matrix. This matrix can then be utilized to determine which queries should be observed.

Let $f_{\mD}(\rvx, \{r_{\pi(i)}\}_{i=1}^k)$ be the maximum likelihood estimator of $y$, defined as \Cref{eq::MLE}, when the selection rule $\pi(\cdot)$ is obtained by executing IP-OMP using $\mD$ as the query feature set in \Cref{theorem: ip-omp equivalency}. For brevity, when the context is clear, we refer to this maximum likelihood estimator simply as $f_{\mD}$. Moreover, define \( \mathcal{L}(y_{\mathrm{pred}}) \) as the squared population loss of any estimator \( y_{\mathrm{pred}} \):
\begin{align*}
        \mathcal{L}(y_{\mathrm{pred}}) = \mathbb{E}_{\rvz \sim \mathcal{N}(\mathbf{0}, \mI_{d \times d})}\left[(y - y_{\mathrm{pred}})^2\right].
    \end{align*}

where \(\rvz \sim \mathcal{N}(\mathbf{0}, \mI_{d \times d})\) is the unknown random vector used in the generative model described in~\Cref{assumption: generative model for ip-omp}. 

To showcase the effect of deviation in the query features, we consider a scenario where the input $\rvx$ can be written as a linear combination of $k$ latent feature vectors (corresponding to $k$ columns of $\mD^*$). This assumption is prevalent in the sparse coding literature \cite{arora2015simple,agarwal2016learning,liang2022simple} and ensures that using \(\mD^*\) as the query feature set guarantees good prediction performance. However, even in this ideal scenario, we demonstrate that even a small column-wise deviation from $\mD^*$ can lead to performance degradation of IP-OMP.
\begin{theorem}\label{theorem: formal necessity}
Suppose that \Cref{assumption: generative model for ip-omp} holds. Furthermore, suppose that \(\mD^*\) is column-orthogonal and $\rvx = \mD^*\rvbeta$ for some $k$-sparse vector $\rvbeta$ with \(\|\rvbeta\|_0 = k\). Additionally, assume that the non-zero entries in \(\rvbeta\) have absolute values bounded below by \(\gamma\) and above by \(\Gamma\).
Then, for any \(\epsilon \in \left(0, \frac{1}{\sqrt{1+16\Gamma^2/\gamma^2}}\right)\), there exists another orthonormal matrix \(\tmD \in \mathbb{R}^{d \times n}\) such that \(\|\tmD - \mD^*\|_{1,2} \leq \epsilon\) and
    \begin{align}\label{eq: formal necessity}
        \mathcal{L}(f_{\tmD}) - \mathcal{L}(f_{\mD^*}) \geq \frac{81(k-1)\epsilon^2\gamma^2}{200}.
    \end{align}
\end{theorem}

We present the complete proof of \Cref{theorem: formal necessity} in \Cref{section: proof theorem 1}, which proceeds as follows: (1) we derive the closed form solution for $\mathcal{L}(f_{\mD})$ using the column-orthogonality of $\mD$ and the distribution of $\rvz$; (2) we then construct the example that satisfies \Cref{eq: formal necessity} by rotating columns of $\mD^*$ alongside the subspace spanned by itself.  

When $\Gamma/\gamma$ is bounded by a constant, we have $\|\rvx\|^2_2 = \|\rvbeta\|^2_2 = \mathbf{\Theta}(k\gamma^2)$ which simplifies \Cref{eq: formal necessity} to $\mathcal{L}(f_{\tmD}) - \mathcal{L}(f_{\mD^*}) =\Omega(\epsilon^2\|\rvx\|^2_2)$. This indicates that the perturbation $\epsilon$ is fully captured by the resulting gap in the squared loss, scaled by a factor of $\|\rvx\|^2_2$. 

\section{Our Proposed Framework}\label{subsection: convergence}
Building upon our discussion in the preceding section, it is pertinent to consider the following question:

\textit{Since an inaccurate $\mD$ can adversely affect the performance of IP-OMP, can $\mD$ be optimized to mitigate this effect while preserving—or even enhancing—interpretability?}

There are two critical challenges to address before answering this question. First, modifying $\mD$ may diminish the encoded information that is interpretable by humans, thereby compromising the \textit{interpretable-by-design} principle of information pursuit. Second, treating the query features as variables introduces significant complexity in deriving the optimal decision rule, as the closed-form expression of IP-OMP presented in \Cref{theorem: ip-omp equivalency} is no longer valid.

To overcome these challenges, we propose obtaining a \textit{correction} $\Delta\mD$ that minimizes $\cL(\tf_{\mD+\Delta\mD})$, where $\tf_\mD$ serves as a differentiable surrogate of the original estimator produced by IP. To preserve interpretability, we constrain the corrected query feature matrix $\mD+\Delta\mD$ to remain within a small neighborhood around $\mD$, where $\mD$ represents the potentially inaccurate initial query feature matrix (e.g., the one obtained from CLIP). This leads to the following constrained optimization problem:
\begin{equation}
    \begin{aligned}
        \min_{\norm{\Delta\mD}_{1,2}\leq \rho} \quad& \cL(\tf_{\mD+\Delta\mD})
        % \text{subject to} \quad& \|\mD - \mD^{\mathrm{init}}\|_{1,2} \le \rho
    \end{aligned}
    \label{eq: objective}
\end{equation}
% $\cL_m$ denotes the finite sample version of $\cL$, with $m$ representing the sample size, and 
Indeed, the choice of the correction radius $\rho$ is crucial as it controls the trade-off between prediction accuracy and interpretability. Setting $\rho=0$ recovers the IP-OMP, and therefore, may suffer from the aforementioned performance degradation. Conversely, choosing a large value for $\rho$ can mitigate the performance degradation of IP-OMP, but at the cost of compromising the interpretability. 

Our meta-algorithm for Problem~\eqref{eq: objective}, called \textit{constrained concept refinement} (CCR), is presented in \Cref{alg: meta algorithm}. 
\begin{algorithm}[H]
   \caption{Constrained Concept Refinement}\label{alg: meta algorithm}
   \begin{algorithmic}[1]  % The "[1]" option numbers each line
      \STATE \textbf{Input}: Initial query feature matrix $\mD$, correction radius $\rho$, training dataset $\cD$.
      \STATE Initialize $\Delta\mD^{(0)} = 0_{d\times n}$
      \WHILE{$t = 1,2,\dots, T$}
         \STATE {\bf Forward propagation:} calculate $\cL(\tf_{\mD+\Delta\mD^{(t-1)}})$.
         \STATE {\bf Backward propagation:} update $\Delta\mD^{(t)}$ using the gradient $\partial \cL/\partial \Delta\mD^{(t-1)}$.
         \STATE Perform projection to ensure $\|\Delta\mD^{(t)}\|_{1,2} \le \rho$.
      \ENDWHILE
      \STATE Return $\tf_{\mD+\Delta\mD^{(T)}}$ 
   \end{algorithmic}
\end{algorithm}
Below, we provide a description of its various steps.

\paragraph{Choice of the surrogate estimator.}
To motivate the choice of the surrogate estimator $\tilde f_{\mD}$, let us revisit the generative model in \Cref{assumption: generative model for ip-omp}, and additionally assume that the ground truth latent feature matrix $\mD^*$ is column-orthogonal. 

By assuming $\mD^*$ to be column-orthogonal, we effectively assume an ideal scenario in which queries are mutually independent. Specifically, it follows that $\langle \rvd^*_i, \rvz \rangle$ is independent of $\langle \rvd^*_j, \rvz \rangle$ when $\rvd^*_i \perp \rvd^*_j$ and $\rvz \sim \cN(\rvzero, \mI_{d \times d})$. The primary motivation for focusing on an column-orthogonal $\mD^*$ is that, under this assumption, the estimator $f_{\mD}$ obtained from IP-OMP becomes inherently differentiable with respect to $\mD$:




\begin{lemma}\label{lemma: introducing tf}
    Assuming that $\mD=\mD^*$ and $\mD^*$ is column-orthogonal, we have
    \begin{align*}
        f_\mD\bigbracket{\rvx,\{r_{\pi(i)}\}_{i=1}^k} = 
        \sum_{i \in S} \langle \rvd_i, \rvx \rangle r_i,
    \end{align*}
    where $S \!=\! \argmax_{T \subseteq [n], |T| \le k} \sum_{i \in T} |\langle \rvd_i, \rvx \rangle|$ corresponds to the indices of the top-$k$ largest values of $\{|\langle \rvd_i, \rvx \rangle|\}_{i=1}^n$.
\end{lemma}
The proof of \Cref{lemma: introducing tf} is deferred to \Cref{section: proof to lemma introducing tf}. Assuming a small initial error, i.e., $\|\mD^* - \mD\|\leq \rho$ for some small $\rho>0$, the iterates $\mD^{(t)} = \mD+\Delta\mD^{(t)}$ of \Cref{alg: meta algorithm} stay approximately column-orthogonal. This consideration motivates the introduction of the following surrogate estimator $\tf$:
\begin{align}\label{eq: definition for tf}
    \tf_\mD\bigbracket{\rvx,\{r_{\pi(i)}\}_{i=1}^k} &= 
        \sum_{i \in S} \langle \rvd_i, \rvx \rangle r_i, \nonumber\\ \text{where} \quad S &= \argmax_{T \subseteq [n], |T| \le k} \sum_{i \in T} |\langle \rvd_i, \rvx \rangle|.
\end{align}
It is important to note that we do not require $\tf$ to be differentiable everywhere; rather, differentiability is required only almost surely along the trajectory of \Cref{alg: meta algorithm}, which we will rigorously demonstrate. Alternative differentiable surrogates for $f$ include the task-driven dictionary learning method \cite{mairal2011task} and the unrolled dictionary learning method \cite{malezieux2021understanding,tolooshams2021stable}, which can be used in place of \Cref{eq: definition for tf}. 

\paragraph{Backward propagation and projection} For the backward propagation and projection steps, we propose to adopt gradient descent updates, followed by a projection step based on $\ell_2$-norm. 
\begin{equation*}\label{eq: update rule}
        \Delta\mD^{(t+1)} = \argmin_{\|\Delta\mD\|_{1,2}\le \rho}\!\left\|\Delta\mD\!-\!\bigbracket{\Delta\mD^{(t)}\!-\!\eta\frac{\partial\cL(\tf_{\mD+\Delta\mD^{(t)}})}{\partial \Delta\mD^{(t)}}}\right\|_2,
\end{equation*}

We note that the specific instantiation of the proposed meta-algorithm and its steps is inherently task-dependent. For example, while we have only discussed the squared loss, a more common choice in image classification tasks is the cross-entropy loss. Additionally, our choice of the surrogate estimator relies on the assumption that the ground truth latent feature matrix is column-orthogonal. When this assumption is not met, it becomes necessary to enforce it via \textit{concept dispersion}. Both of these modifications are discussed extensively within the context of image classification in \Cref{section: experiments}.

However, a more fundamental question has remained unanswered: how can ``interpretability"—a concept inherently meaningful only to humans—be measured and quantified? One approach is to assess interpretability in an ad-hoc manner by running the candidate method and relying on human judgment to determine whether the predictions are interpretable for different individual samples. This has been the de facto approach adopted in nearly all previous works. An alternative way---which we seek in this work---is to study a simple test-bed for investigation, where ``interpretability'' admits a crisp mathematical formulation.

\subsection{Accuracy and Interpretability Guarantees}
Consider the following probabilistic generative model.
\begin{assumption}[Probabilistic generative model]\label{assumption: probabilistic generative model of x}
Suppose that the target random variable $y$ and queries are generated according to Assumption~\ref{assumption: generative model for ip-omp}. Moreover, suppose that ground truth latent feature matrix $\mD^*$ is column-orthogonal and the input $\rvx$ satisfies \(\rvx = \mD^* \rvbeta\), where: (1) the support of $\rvbeta$, denoted as $S^*$, is drawn uniformly from the set of $k$-element subsets of $[n]$; and (2) the non-zero elements of $\rvbeta$ are i.i.d. and satisfy $\bE[\rvbeta_i]=0$, $\var[\rvbeta_i]=\sigma^2$, and $\gamma\le |\rvbeta_i| \le\Gamma$.
\end{assumption}

The generative model in \Cref{assumption: probabilistic generative model of x} is akin to those explored in the sparse coding literature \cite{arora2015simple, agarwal2016learning, ravishankar2020analysis, liang2022simple}. Under this model, prediction error can be evaluated using the squared loss, while interpretability can be assessed by the proximity of the query feature matrix $\mD^{(T)}$, produced by CCR, to the ground truth latent feature matrix $\mD^*$. 

Our next theorem shows that CCR effectively resolves the challenge faced by IP-OMP, as outlined in Theorem~\ref{theorem: formal necessity}.\footnote{We note that the result of \Cref{theorem: formal necessity} also holds for the generative model described in \Cref{assumption: probabilistic generative model of x}.} Let $\mD^{(t)} = \mD+\Delta\mD^{(t)}$ be the corrected query feature matrix generated by CCR at iteration $t$.
\begin{theorem}\label{theorem: vanishing L convergence}
    Suppose that \Cref{assumption: probabilistic generative model of x} holds. Moreover, suppose that that the initial query feature matrix $\mD$ satisfies $\|\mD-\mD^*\|_{1,2}= \rho \le\frac{\gamma}{8\sqrt{k}\Gamma}$ and the step-size $\eta$ satisfies $0<\eta<\frac{1}{2\|\rvx\|^2_2}$. \Cref{alg: meta algorithm} with surrogate estimator~\eqref{eq: definition for tf} and update rule \eqref{eq: update rule} satisfies:
    \begin{align*}
        \cL(\tf_{\mD^{(t+1)}})\le \left(1-2\eta\|\rvx\|^2_2\right)^2\cL(\tf_{\mD^{(t)}}).
    \end{align*}
    % where $\tau = |1-2\eta\|\rvx\|^2_2|$. 
\end{theorem}

While \Cref{theorem: vanishing L convergence} addresses the performance limitations of IP-OMP, it does not guarantee improved interpretability. This outcome is unsurprising, since the reliance of the method on only {\it a single} input $\rvx$ ensures that only the columns of $\mD$ with indices in $S^*$ can be improved, essentially leaving the columns outside $S^*$ unmodified. Moreover, if $\mD^*$ is not full-rank, our analysis shows that $\mD$ may deviate from $\mD^*$ in directions column-orthogonal to the column space of $\mD^*$; these deviations cannot be eliminated by \Cref{eq: update rule}. This observation is further supported by numerical experiments (see \Cref{fig: synthetic plot}).

To tackle these challenges, we assume that $\mD^*$ is full-rank. Moreover, it is essential to use a sufficient number of i.i.d. input samples to ensure that each column of $\mD^*$ contributes to at least one input sample. This aligns more closely with practical scenarios, such as image classification, where multiple samples are typically used during the training phase. We denote the i.i.d. input samples and their corresponding target random variables, generated according to \Cref{assumption: probabilistic generative model of x}, as $\{\rvx^h\}_{h=1}^m$ and $\{y^h\}_{h=1}^m$, respectively. For any estimator $\rvy_{\mathrm{pred}} = \{y_{\mathrm{pred}}^h\}_{h=1}^m$, the aggregated squared loss is defined as
\begin{align}\label{eq::Lm}
        \mathcal{L}_m(\rvy_{\mathrm{pred}}) = \frac{1}{m}\sum_{h=1}^m \mathbb{E}_{\rvz^h \sim \mathcal{N}(\mathbf{0}, \mI_{d \times d})}\left[(y^h - y^h_{\mathrm{pred}})^2\right].
    \end{align}

\begin{theorem}\label{theorem: columnwise convergence}
    Suppose that $\{\rvx^h\}^m_{h=1}$ and $\{y^h\}^m_{h=1}$ are i.i.d. samples generated from \Cref{assumption: probabilistic generative model of x} with a fixed full-rank column-orthogonal $\mD^*$. Suppose that $m=\Omega\bigbracket{\frac{n^6}{\sigma^2 k^5}}$. Moreover, suppose that the initial query feature matrix $\mD$ satisfies $\|\mD-\mD^*\|_{1,2}= \rho \le\frac{\gamma}{8\sqrt{k}\Gamma}$ and the step-size $\eta$ satisfies $\eta=O\bigbracket{\frac{1}{\sigma^2}}$. With high probability, \Cref{alg: meta algorithm} with surrogate estimator~\eqref{eq: definition for tf} and update rule \eqref{eq: update rule} applied to the aggregated squared loss~\eqref{eq::Lm} satisfies:
    \begin{itemize}
        \item {\bf (Interpretability)} $\|\mD^{(t+1)}-\mD^*\|_{1,2}\le \tau\|\mD^{(t)}-\mD^*\|_{1,2}$, where $\tau = \sqrt{1-\frac{k(k-1)\sigma^2}{2n^2}\eta}$.
    \item {\bf (Accuracy)} $\!\cL_m(\tf_{\mD^{(t)}})\!\le\!\frac{k\sum_{h=1}^m\|\rvx^h\|^2_2}{m}\|\mD^{(t)}\!-\!\mD^*\|^2_{1,2}$.
    \end{itemize}
\end{theorem}
According to \Cref{theorem: columnwise convergence}, by leveraging multiple samples, CCR achieves the best of both worlds: it guarantees convergence to the ground truth latent feature matrix $\mD^*$, progressively enhancing interpretability, while simultaneously driving \(\cL_m\) to zero, thereby achieving perfect accuracy.

Contrary to traditional analyses in dictionary learning \cite{arora2015simple,liang2022simple}, the proofs of~\Cref{theorem: vanishing L convergence} and~\Cref{theorem: columnwise convergence} require a careful alignment of the updates from gradient descent with the direction of the true solution. In the setting of \Cref{theorem: columnwise convergence}, this is even more challenging because the finite sample size \(m\) inevitably causes deviations from the population-level behavior. Moreover, we must further control the deviation introduced by the projection step. The detailed proofs of these theorems are provided in Appendix~\ref{section: proofs}. We also include a detailed discussion on both theorems in \Cref{section: discussion on theorems} and experiments that validate them in \Cref{section: detailed synthetic}.
\section{Application: Interpretable Image Classification}\label{section: experiments}
In this section, we showcase the performance of our proposed CCR framework from \Cref{alg: meta algorithm} on interpretable image classification task. The Python implementation of algorithm can be found  here: 
{\href{https://github.com/lianggeyuleo/CCR.git}{github.com/lianggeyuleo/CCR.git}.} 

The image classification setting differs slightly from the setting provided in \Cref{definition: single variable prediction by query selection}. Specifically, we are given a dataset $\{\rva^i,\rvy^i\}_{i=1}^m$ consisting of $m$ image ($\rva$)-label ($\rvy$) pairs, where the goal is to predict the correct label for a given image. Here, $\rvy$ represents a one-hot label vector. Additionally, this setting provides access to a concept set $\{\rvc_i\}_{i=1}^n$, which consists of $n$ key concepts that can aid in classifying an image. These concepts are typically textual; for example, in the case of animal images, the concept set might include descriptors like “stocky body” or “black stripes”. 

\begin{algorithm}
   \caption{CCR for Interpretable Image Classification}
   \label{alg: constrained concept update for image classification}
   \begin{algorithmic}[1]
      \STATE \textbf{Input}: Dataset of image-label pairs $\{\rva^i ,\rvy^i\}_{i=1}^m$, concept set $\{\rvc_i\}_{i=1}^n$.
      \STATE Use CLIP to embed the images $\{\rva^i\}_{i=1}^m$ into input features $\{\rvx^i\}_{i=1}^m$, and the concepts $\{\rvc_i\}_{i=1}^n$ into query features $\{\rvd_i\}_{i=1}^n$.
      % \STATE Generate image embeddings $\{\rvx^i\}_{i=1}^m$ using CLIP.
      % \STATE Generate query features $\{\rvd_i\}_{i=1}^n$ by embedding concepts $\{\rvc_i\}_{i=1}^n$ using CLIP.
      \STATE Initialize $\mD^{(0)}$ via concept dispersion on $\{\rvd_i\}_{i=1}^n$. 
      \STATE Initialize $\rvs^{(0)}_i = \mathrm{HT}_\lambda\left({\mD^{(0)}}^\top \rvx^i\right)$ for $i=1,\dots,m$.
      \STATE Initialize $\mL^{(0)}$ with random values.
      \WHILE{$t=1,2,\dots,T$}
         % \STATE Calculate $\rvs^i = \mathrm{HT}_\lambda(\mD^\top \rvx^i)$ for $i=1,\dots,m$.
         \STATE Calculate $\cL_m = \sum_i \mathrm{CE}\left(\mL \rvs^{(t-1)}_i,\rvy^i\right)$.
         \STATE Update $\mD^{(t)} = \mD^{(t-1)} - \eta_\mD \,\partial \cL/\partial \mD^{(t-1)}$.\label{step: gd}
         \STATE Normalize and project $\mD^{(t)}$.
         \STATE Update $\mL^{(t)} = \mL^{(t-1)} - \eta_\mL \,\partial \cL/\partial \mL^{(t-1)}$.
         \STATE Update $\rvs^{(t)}_i = \mathrm{HT}_\lambda\left({\mD^{(t)}}^\top \rvx^i\right)$ for $i=1,\dots,m$.
         % \STATE Perform \Cref{alg: atom projection} with $\mD$, $\mD_{\mathrm{init}}$, $\rho$.
      \ENDWHILE
      \STATE \textbf{Return} $\mD^{(T)}$, $\mL^{(T)}$, and $\{\rvs^{(T)}_i\}_{i=1}^m$.
   \end{algorithmic}
\end{algorithm}

We formally introduce our algorithm in  \Cref{alg: constrained concept update for image classification}. Prior to comparing our approach with other interpretable AI methods, we provide a detailed explanation of the key components in \Cref{alg: constrained concept update for image classification}. 

\textbf{CLIP embedding.} We employ the multi-modal model CLIP \cite{radford2021learning}---a recently
introduced large Vision-Language Model---to embed both images and concepts into the same latent space.

\textbf{Concept dispersion.} After obtaining the CLIP embeddings, the query features tend to cluster too closely, making the query feature matrix $\mD^{(0)}$ far from column-orthogonal. To address this, we introduce a {\it concept dispersion} procedure (see \Cref{alg: atom dispersion} in the appendix). This heuristic approach enhances the mutual orthogonality of query features by increasing their relative angles, thereby improving the orthogonality of $\mD^{(0)}$. Notably, this is achieved while preserving interpretability by maintaining the relative positions of features in the embedded space. Further details on the dispersion step are provided in \Cref{section: detailed real}.

\textbf{Hard-thresholding.} In \Cref{eq: definition for tf}, $\tf$ is computed in two steps: (1) constructing the sparse code $\rvs$ by keeping only the entries with the top-$k$ absolute values of $\mD^\top \rvx$; and (2) setting $\tf$ as the inner product of $\rvs$ with $\rvr = [r_1,\dots,r_i]^\top \in \bR^n$. Here, we replace the top-$k$ selection with the entry-wise hard-thresholding operator:
\begin{align*}
    \mathrm{HT}_\lambda(x) = \begin{cases}
        x, & \text{if} \quad |x|\ge\lambda,\\
        0, & \text{if} \quad |x|<\lambda.
    \end{cases}
\end{align*}
This approach allows for parallelization and reduced computational cost, although the exact sparsity level $k$ can no longer be directly specified; instead, $k$ is determined implicitly by $\lambda$. Nonetheless, adjusting $\lambda$ provides similar control over sparsity.

\textbf{Linear Layer.} To compute $\tilde{f}_{\mD}$ as defined in \Cref{eq: definition for tf}, access to $\rvr$ is required, which is unavailable in the image classification setting. To address this, we train a linear classifier that takes the sparse code $\rvs$ as input and outputs a weight vector over the possible labels. The label with the highest weight is selected as the prediction. The loss function is defined as the cross-entropy between the linear layer's output and the true label $\rvy$.

\textbf{Embedding normalization and projection.} After each iteration, we perform a projection step similar to \Cref{eq: update rule} for each updated concept embedding. Additionally, we normalize each embedding to ensure that it remains on the unit ball (see \Cref{alg: atom projection} in \Cref{section: detailed real}).

\subsection{Performance}
We compare the performance of \Cref{alg: constrained concept update for image classification} against two recently proposed explainable AI methods, CLIP-IP-OMP \cite{chattopadhyay2024information} and label-free CBM (lf-CBM) \cite{oikarinen2023label}, as well as the CCR baseline without the concept refinement step, in the context of explainable image classification.

For the baseline version of CCR, we set $\eta_{\mD} = 0$ at Step 8 of \Cref{alg: constrained concept update for image classification}, ensuring that the comparison isolates the effect of concept refinement. The two methods we compare against represent state-of-the-art explainable AI approaches, particularly in terms of scalability. Lf-CBM was the first CBM-type model to be scaled to datasets as large as ImageNet, while CLIP-IP-OMP further improves computational efficiency while maintaining competitive accuracy. For a detailed introduction to both methods, we refer to \Cref{section: review}.

The evaluation is conducted across five image classification benchmarks: CIFAR-10, CIFAR-100 \cite{krizhevsky2009learning}, ImageNet \cite{deng2009imagenet}, CUB-200 \cite{wah2011caltech}, and Places365 \cite{zhou2017places}. 

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figs/benchmark_comparison_ver.pdf}
    \caption{Prediction accuracy of CCR and its baseline across iterations, with the final test accuracy of CLIP-IP-OMP and lf-CBM indicated for reference.}
    \label{fig:benchmarks}
\end{figure}

To ensure a fair comparison, all methods use the same concept set, which is generated by GPT-3 \cite{brown2020language}. A detailed description of the generation process can be found in \cite{oikarinen2023label}.
For the CIFAR-10/100 and CUB-200 datasets, we tune CLIP-IP-OMP to match the average sparsity level of $\rvs$, also referred to as the {\it explanation length} or $k$, used in CCR. For ImageNet and Places365, we report the best accuracy achieved by CLIP-IP-OMP across all explanation lengths. Since Lf-CBM does not allow tuning of its explanation length, we directly report its accuracy. The constraint parameter $\rho$ for CCR is fixed at $0.1$ for all experiments.

As shown in \Cref{fig:benchmarks}, CCR consistently outperforms its baseline, CLIP-IP-OMP, and lf-CBM across all benchmarks except CUB-200. This exception is expected, as lf-CBM was built on a ResNet-18 backbone specifically trained on CUB-200,  whereas both CLIP-IP-OMP and CCR rely on CLIP as their encoder, which was trained on more diverse and general datasets.

Additionally, we report the average explanation length (AEL, $k$), average sparsity ratio (ASR, $k/n$), and average concept embedding deviation (ACED, $\frac{1}{n}\sum_{i=1}^n \|\rvd_i - \rvd^{(0)}_i\|_2$) of CCR in the following table:

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
 & AEL & ASR & ACED \\ \hline
CIFAR10  & 11.09 & 8.66\% & 0.056\\ \hline
CIFAR100  & 19.67 & 2.39\% & 0.061\\ \hline
CUB200  & 27.52 & 13.2\% & 0.062\\ \hline
ImageNet  & 48.97 & 1.08\% & 0.097\\ \hline
Places365  & 44.43 & 2.01\% & 0.041\\ \hline
\end{tabular}
\caption{Average explanation length (AEL), average sparsity ratio(ASR) and average concept embedding deviation (ACED) for CCR.}
\label{tab: table}
\end{table}

Thanks to the parallelization enabled by hard thresholding and the computational efficiency of backpropagation, CCR significantly reduces computational costs compared to CLIP-IP-OMP and lf-CBM, particularly when applied to large-scale datasets such as ImageNet ($\approx 1.2$ million images) and Places365 ($\approx 1.8$ million images). As reported in \cite{chattopadhyay2024information}, training lf-CBM on ImageNet requires $\approx 50$ hours on an NVIDIA RTX A5000 GPU. Under the same experimental conditions, CLIP-IP-OMP, with an average explanation length of $\approx 50$, incurs a computational cost of $\approx 40$ hours. In our computational environment, using an NVIDIA Tesla V100 GPU, CLIP-IP-OMP remains comparably expensive, requiring $\approx 33$ hours for $k=50$.
In contrast, CCR (with $k \approx 49$, as shown in \Cref{tab: table}) processes ImageNet in only $\approx 2$ hours (corresponding to 200 iterations) while achieving even higher accuracy, demonstrating a substantial improvement in computational efficiency. To make CLIP-IP-OMP more computationally feasible, one can reduce $k$; however, for $k=10$, while the processing time drops to $\approx 6$ hours, it comes at the cost of a significant accuracy decline to $\approx 63\%$.

\subsection{Interpretability}\label{section: interpretability}
To illustrate the interpretability of \Cref{alg: constrained concept update for image classification}, we follow the methodology outlined in \cite{chattopadhyay2024information} and present the most significant coefficients and weights from the algorithm on different samples from the Places365 dataset.
As shown in \Cref{fig:interpretability}, we highlight the top 10 concepts in $\rvs$ with the highest values (left) along with their corresponding weights in the linear layer $\rmL$ for the predicted label (right). The selected concepts are semantically relevant to the image, and the linear layer effectively assigns substantial weight to key concepts such as “designated fairways and greens”, enabling accurate prediction while appropriately disregarding concepts that, although relevant, may be misleading, such as “a flag” or “a country”.
For additional case studies on other datasets, we refer the reader to \Cref{sec: more interpretability}.

\section*{Acknowledgements}
This research is supported, in part, by NSF CAREER Award CCF-2337776, NSF Award DMS-2152776, and ONR Award N00014-22-1-2127.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figs/interpretability_combined.png}
    \caption{
The first example illustrates a \textit{simple case} where CCR successfully learns the correct concepts. The second example represents a {\it misleading case}, where the image contains concepts like ``a flag'' that, while relevant and visually apparent, could potentially mislead classification. However, CCR effectively extracts both useful and misleading information, assigning appropriate weights to ensure the correct prediction.}
    \label{fig:interpretability}
\end{figure}



\newpage
\bibliographystyle{alpha}
\bibliography{ref}
\newpage
\appendix
\section{Related Literature}\label{section: review}
In this section, we provide a more comprehensive literature review. 

\paragraph{Explainable AI.} In the main body of the paper, we provided an overview of related methods in explainable AI. Here, we focus on two approaches that are closely related to and comparable with our proposed method. The first is the CLIP-IP-OMP method introduced by \cite{chattopadhyay2024information}, which leverages CLIP embeddings to greedily solve sparse coding problems for each input, subsequently passing the results through a linear layer to generate final predictions. Our method builds upon the same theoretical framework established in \cite{chattopadhyay2024information}. Notably, the baseline model (without concept refinement) considered in \Cref{subsection: convergence} and \Cref{section: experiments} serves as a differentiable surrogate for CLIP-IP-OMP. The second approach is the label-free CBM (lf-CBM) proposed by \cite{oikarinen2023label}, which employs the inner product between CLIP embeddings to achieve state-of-the-art results in interpretable models. Specifically, lf-CBM was the first CBM model scaled to ImageNet. A detailed comparison of the performance of our method against these two approaches is provided in \Cref{section: experiments}.

\paragraph{Sparse coding and dictionary learning.} Sparse Coding \cite{olshausen1997sparse} refers to the process of representing a given signal (or vector) as a sparse linear combination of fixed signals, collectively referred to as a \textit{dictionary}. Dictionary Learning \cite{aharon2006k} extends Sparse Coding to a bi-variable setting, allowing the dictionary itself to be optimized. The objective is to identify a dictionary capable of generating effective sparse codes for a specific signal distribution. \cite{chattopadhyay2024information} establishes a novel connection between sparse coding and explainable AI, treating concept embeddings as the dictionary. Our proposed method directly optimizes concept embeddings, which draws parallels to dictionary learning, although the two approaches address fundamentally different objectives. Among works on dictionary learning, \cite{mairal2011task} is particularly relevant, as it optimizes dictionaries for downstream tasks. However, our method distinguishes itself through the constrained concept requirement. Interestingly, \cite{mairal2011task} aligns with our framework as a differentiable learning module, and integrating this approach with our method presents an intriguing avenue for future work. Another promising direction for future research is leveraging the {\it personalized} dictionary learning framework proposed by~\cite{liang2024personalized} to extend our approach to Explainable AI in the context of heterogeneous datasets.


\textbf{Theoretical guarantees.} To the best of our knowledge, our work is the first to establish a convergence guarantee in the setting where dictionary atoms are updated for downstream tasks. The most closely related work is \cite{chattopadhyay2024information}, which introduced IP-OMP. However, the connection between IP-OMP and column-orthogonal matching pursuit lacks rigor due to the presence of normalization terms (see \Cref{theorem: ip-omp equivalency}), making it difficult to prove that IP-OMP minimizes the loss $\cL$ or $\cL_m$ using the optimality conditions of OMP \cite{tropp2004greed}.
Another related approach is task-driven dictionary learning \cite{mairal2011task}, which establishes the differentiability of the objective function but does not provide a convergence guarantee.

\section{Further Discussion}\label{section: discussion on theorems}

In this section, we provide further discussion on \Cref{theorem: vanishing L convergence} and \Cref{theorem: columnwise convergence}.

\textbf{Tolerance, Sample Size, and Convergence Speed.} 
Both theorems establish that the population loss converges to zero, provided that the initial dictionary estimate is within \(	\frac{1}{\sqrt{k}}\) in the column-wise \(\|\cdot\|_{1,2}\)-norm from the ground-truth dictionary \(\mD^*\). Assuming that a constant fraction of the queries in \(\mD^*\) is utilized to generate each \(
\rvx^h\), i.e., \(k=\Omega(n)\), \Cref{theorem: columnwise convergence} indicates that each column of \(\mD^*\) can be accurately recovered with a sample size of \(m=O(n)\) and a convergence rate of \(\alpha=1-\Omega(1)\). Notably, the linear sample size and linear convergence rate is consistent with the best-known results in dictionary learning \cite{arora2015simple,liang2022simple}.

\textbf{Sparsity Level.} 
It is well established in the dictionary learning literature that higher sparsity levels in ground-truth sparse codes (i.e., larger values of \(k\) in \Cref{assumption: probabilistic generative model of x}) present greater challenges for recovery; see \cite{arora2015simple}. Indeed, \cite{arora2015simple} asserts that sparsity levels beyond \(k = \Omega\bigbracket{	\frac{n}{\log n}}\) rarely succeed in practice, even though several approaches have been proposed to handle cases where \(k=\Omega\bigbracket{n}\) \cite{sun2016complete,liang2022simple}. In contrast, our results reveal that the sparsity level \(k\) plays a nuanced role when optimizing \(\mD\) for the downstream task \(\cL\). On the one hand, a larger \(k\) imposes a more stringent initial error bound for both theorems. On the other hand, a larger \(k\) in \Cref{theorem: columnwise convergence} improves the convergence rate \(	\tau\) and reduces the required sample size \(m\). The former phenomenon is consistent with findings in dictionary learning literature, where exact support recovery becomes more feasible for sparser generative models. A high-level explanation for the latter is that a denser generative model for \(\rvbeta\) provides each \(\rvx\) with more information, thereby expediting the training process.

\section{Proof of Theorems}\label{section: proofs}
\subsection{Proof of \Cref{theorem: formal necessity}}\label{section: proof theorem 1}
Our proof strategy follows two main steps:
\begin{itemize}
    \item {\bf Step 1:} We prove that $\cL(f_{\mD^*})=0$. To do so, we first derive an explicit form of $f_{\mD^*}$, and then show $\cL(f_{\mD^*})=0$ for this explicit form.
    \item {\bf Step 2:} We explicitly design a column-orthogonal $\tilde\mD$ such that \(\|\tmD - \mD^*\|_{1,2} \leq \epsilon\) and $\mathcal{L}(f_{\tmD})\geq \frac{2(k-1)\epsilon^2\gamma^2}{5}.$
\end{itemize}
To streamline the presentation, we denote the $i$-th column of $\mD^*$ as $\rvd_i^*$. Note that $\rvd_i^*$ is identical to $\rvv_i$ used in the paper.

$\bullet$ \textbf{Step 1: Establishing $\cL(f_{\mD^*})=0$.} We first provide an explicit characterization of $f_{\mD^*}$.
\begin{lemma}\label{lemma: closed form f_D}
    We have
    \begin{align*}
        f_{\mD^*}\bigbracket{\rvx,\{r_{\pi(i)}\}_{i=1}^k} = 
        \sum_{i\in S} \langle\rvd^*_i,\rvx\rangle r_i,
    \end{align*}
    where $S = \{\pi(1),\pi(2),...,\pi(k)\}$ is the index set selected by IP-OMP applied to $\rvx$ and $\mD^*$. 
\end{lemma}
Our next goal is to provide an explicit characterization of $S$, introduced in the above lemma.
\begin{lemma}\label{lemma: column-orthogonal IP-OMP results}
For any column-orthogonal $\mD$, the index set $S = \{\pi(1),\pi(2),...,\pi(k)\}$ selected by IP-OMP applied to $\rvx$ and $\mD$ corresponds to the indices of the top-$k$ largest values of $\{|\langle \rvd_i, \rvx \rangle|\}_{i=1}^n$:
\begin{align*}
    S = \argmax_{T\subseteq[n],|T|\le k} \sum_{i\in T} |\langle \rvd_i, \rvx \rangle|.
\end{align*}
Moreover, if $\mD = \mD^*$, we have $S = S^*$, where $S^*$ is the support of $\beta$ defined in \Cref{theorem: formal necessity}.
\end{lemma} 
We defer the proofs of \Cref{lemma: closed form f_D} and \Cref{lemma: column-orthogonal IP-OMP results} to \Cref{section: proof to lemma closed form f_D} and \Cref{section: prooof of column-orthogonal ip omp results}, respectively. Combining the above two lemmas, we obtain:
\begin{align*}
        f_{\mD^*}\bigbracket{\rvx,\{r_{\pi(i)}\}_{i=1}^k} = 
        \sum_{i\in S} \langle\rvd^*_i,\rvx\rangle r_i, \ \text{where}\ S = \argmax_{T\subseteq[n],|T|\le k} \sum_{i\in T} |\langle \rvd^*_i, \rvx \rangle|
    \end{align*}

Given this explicit form of $f_{\mD^*}$ generated by IP-OMP, we are now ready to investigate the population loss $\cL$ of that estimator. 

For any column-column-orthogonal $\mD$, one can write
\begin{equation}\label{eq: def L f_D part 1}
    \begin{aligned}
    \quad \cL\bigbracket{f_\mD} 
    &=\bE_{\rvz \sim \mathcal{N}(\mathbf{0}, \mI_{d \times d})}\bigsquare{\bigbracket{\sum_{i\in S} \langle\rvd_i,\rvx\rangle r_i - \langle \rvx,\rvz\rangle}^2}\\
    &= \bE_{\rvz \sim \mathcal{N}(\mathbf{0}, \mI_{d \times d})}\bigsquare{\bigbracket{\sum_{i\in S} \langle\rvd_i,\rvx\rangle \langle\rvd^*_i,\rvz\rangle - \langle \rvx,\rvz\rangle}^2}\\
    &= \bE_{\rvz \sim \mathcal{N}(\mathbf{0}, \mI_{d \times d})}\bigsquare{\bigbracket{\sum_{i\in S} \langle\rvd^*_i\rvd_i^\top \rvx,\rvz\rangle - \langle \rvx,\rvz\rangle}^2}\\
    &= \bE_{\rvz \sim \mathcal{N}(\mathbf{0}, \mI_{d \times d})}\bigsquare{\bigbracket{\langle \mD^*_S\mD_S^\top \rvx,\rvz\rangle - \langle \rvx,\rvz\rangle}^2}\\
    &= \bE_{\rvz \sim \mathcal{N}(\mathbf{0}, \mI_{d \times d})}\bigsquare{\bigbracket{\langle \mD^*_S\mD_S^\top \rvx -\rvx,\rvz\rangle}^2}
    \end{aligned}
\end{equation}
It is easy to verify that random variable $\langle \mD^*_S\mD_S^\top \rvx -\rvx,\rvz\rangle$ is distributed as $\cN(0,\|\mD^*_S\mD_S^\top \rvx -\rvx\|^2_2)$. 
As a result, we can conclude that 
\begin{align}\label{eq::Lf}
    \cL(f_\mD) = \|\mD^*_S\mD_S^\top \rvx -\rvx\|^2_2 \quad \text{for any column-orthogonal $\mD$.}
\end{align}
Given \Cref{lemma: column-orthogonal IP-OMP results}, we have $S=S^*$. We can subsequently conclude that:
\begin{align*}
   \mD^*_S\mD^{*\top}_S \rvx= \mD^*_{S^*}\mD^{*\top}_{S^*} \rvx =\mD^*_{S^*}\mD^{*\top}_{S^*}\mD^*_{S^*}\rvbeta_{S^*}= \mD^*_{S^*}\rvbeta_{S^*}  = \rvx,
\end{align*}
This leads to $\cL(f_{\mD^*}) = \|\mD^*_S\mD^{*\top}_S \rvx -\rvx\|^2_2 = 0$, thereby completing the proof of our first step.

$\bullet$ \textbf{Step 2: Constructing $\tilde\mD$.}
Without loss of generality, let us assume $S^* = [k]$, where $k$ to be even. The case where $k$ is odd is easily proved by following the argument below and replacing $k$ with $k-1$. Consider the following explicit form for $\tmD$:
\begin{align*}
    \begin{cases}
        \trvd_i = \cos\theta \rvd^*_i + \sin\theta \rvd^*_{i+k/2}, & 1\le i \le k/2,\\
        \trvd_{i+k/2} = -\sin\theta \rvd^*_i + \cos\theta \rvd^*_{i+k/2} & 1\le i \le k/2,\\
        \trvd_i=\rvd^*_i & i>k,
    \end{cases}
\end{align*}
where $\theta$ is an angle to be determined later.

Intuitively, $\tmD$ is constructed by iteratively selecting a pair of columns in $\mD^*_S$ and rotating them by an angle $\theta>0$ within the two-dimensional subspace spanned by each pair. Based on its definition, we can establish the following properties for $\tmD$:
\begin{lemma}\label{lemma: D property}
    $\tmD$ has the following properties:
    \begin{itemize}
        \item $\tmD$ is column-orthogonal.
        \item $\|\tmD-\mD^*\|_{1,2} = 2\sin \frac{\theta}{2}$.
    \end{itemize}
\end{lemma}
The proof of \Cref{lemma: D property} is presented in \Cref{section: proof to lemma D property}

Our next step is to calculate $\cL(f_{\tmD})$. Note that $\tmD$ is column-orthogonal according to \Cref{lemma: D property}. Therefore, according to \Cref{eq::Lf}, we have $\cL(f_{\tmD}) = \|\mD^*_S\tmD_S^\top \rvx -\rvx\|^2_2$. 
According to \Cref{lemma: column-orthogonal IP-OMP results}, in order to obtain the index set $S$ selected by IP-OMP applied to $\rvx$ and $\tilde\mD$, we need to find the indices of the top-$k$ largest values of $|\langle \trvd_i, \rvx \rangle|$. For $1 \le i \le k/2$, we have:
\begin{align*}
    |\langle \trvd_i,\rvx\rangle| & = \left|\left\langle \cos\theta \rvd^*_i + \sin\theta \rvd^*_{i+k/2},\sum_{j=1}^k \beta_j\rvd^*_j\right\rangle\right|\\ 
    & = |\cos\theta\beta_i + \sin\theta\beta_{i+k/2}|
\end{align*}
Upon choosing $\theta$ such that $\tan \theta< \gamma/\Gamma$, we have 
\begin{align*}
    |\cos\theta\beta_i| \ge \left|\frac{\sin\theta \gamma}{\tan \theta}\right| > |\sin\theta\Gamma| \ge |\sin\theta\beta_{i+k/2}|,
\end{align*}
which guarantees that $|\langle \trvd_i,\rvx\rangle|>0$. Similarly, for $1+k/2\le i \le k$, we have:
\begin{align*}
    |\langle \trvd_i,\rvx\rangle| 
    & = \left|\left\langle \cos\theta \rvd^*_i - \sin\theta \rvd^*_{i-k/2},\sum_{j=1}^k \beta_j\rvd^*_j\right\rangle\right|\\ 
    & = |\cos\theta\beta_i - \sin\theta\beta_{i-k/2}|\\
    & > 0.\\
\end{align*}
For $i>k$, we have $|\langle \trvd_i,\rvx\rangle|=|\langle \rvd^*_i,\rvx\rangle| = 0$. As a result, when $\tan \theta < \gamma/\Gamma$, \Cref{lemma: column-orthogonal IP-OMP results} guarantees that the index set selected by IP-OMP is $S=[k]=S^*$. Given this fact, we can calculate $\cL(f_{\tmD})$ as:
\begin{equation}
\begin{aligned}
    \cL(f_{\tmD}) &= \|\mD^*_S\tmD_S^\top \rvx -\rvx\|^2_2\\
    &= \|\mD^*_{S^*}\tmD_{S^*}^\top \rvx -\rvx\|^2_2\\
    &= \|\mD^*_{S^*}\tmD_{S^*}^\top \sum_{i=1}^k \beta_i\rvd^*_i -\sum_{i=1}^k \beta_i\rvd^*_i\|^2_2\\
    &\overset{(a)}{=} \left\|\sum_{i=1}^{k/2} \bigbracket{\cos\theta\beta_i+\sin\theta\beta_{i+k/2}}\rvd^*_i + \sum_{i=1+k/2}^{k} \bigbracket{\cos\theta\beta_i-\sin\theta\beta_{i-k/2}}\rvd^*_i - \sum_{i=1}^k \beta_i\rvd^*_i \right\|^2_2 \\
    &\overset{}{=} \left\|\sum_{i=1}^{k/2} \bigbracket{(\cos\theta-1)\beta_i+\sin\theta\beta_{i+k/2}}\rvd^*_i + \sum_{i=1+k/2}^{k} \bigbracket{(\cos\theta-1)\beta_i-\sin\theta\beta_{i-k/2}}\rvd^*_i\right\|^2_2\\
    &\overset{(b)}{=} \sum_{i=1}^{k/2} \bigbracket{(\cos\theta-1)\beta_i+\sin\theta\beta_{i+k/2}}^2 + \ \sum_{i=1+k/2}^{k} \bigbracket{(\cos\theta-1)\beta_i-\sin\theta\beta_{i-k/2}}^2,
\end{aligned}
\label{eq: L y tilde D}
\end{equation}
where (a) is based on the construction of $\tmD$, and (b) is due to the assumption that $\mD^*$ is column-orthogonal. When $\sin\frac{\theta}{2}\le \frac{1}{\sqrt{1+16\Gamma^2/\gamma^2}}$, we have for $1\le i \le k/2$:
\begin{align*}
    \sin\theta &= 2\sin\frac{\theta}{2}\cos\frac{\theta}{2} \\
    & = 2\sin\frac{\theta}{2}\sqrt{1-\sin^2\frac{\theta}{2}} \\
    & \ge 2\sin\frac{\theta}{2} \cdot \frac{4\Gamma}{\gamma} \frac{1}{\sqrt{1+16\Gamma^2/\gamma^2}} \\
    & \ge \frac{8\Gamma}{\gamma} \sin^2\frac{\theta}{2} \\
    & = \frac{4\Gamma}{\gamma} (1-\cos\theta), 
\end{align*}
which leads to
\begin{align*}
    &\quad\bigbracket{(\cos\theta-1)\beta_i+\sin\theta\beta_{i+k/2}}^2\\
    &= (\cos\theta-1)^2\beta^2_i + 2(\cos\theta-1)\sin\theta\beta_i\beta_{i+k/2} + \sin^2\theta\beta^2_{i+k/2} \\
    &\ge 2(\cos\theta-1)\sin\theta\beta_i\beta_{i+k/2} + \sin^2\theta\beta^2_{i+k/2}\\
    &\ge  - \left|\frac{\gamma}{2\Gamma}\sin^2\theta\beta_i\beta_{i+k/2}\right| + \sin^2\theta\beta^2_{i+k/2} \\
    &\ge \frac{1}{2}\sin^2\theta\beta^2_{i+k/2}. 
\end{align*}
With identical arguments, we have for $1+k/2\le i \le k$:
\begin{align*}
    \bigbracket{(\cos\theta-1)\beta_i+\sin\theta\beta_{i-k/2}}^2 \ge \frac{1}{2}\sin^2\theta\beta^2_{i-k/2}.
\end{align*}
As a result, \Cref{eq: L y tilde D} reduces to:
\begin{align*}
    \cL(f_{\tmD}) & \ge \sum_{i=1}^{k/2} \frac{1}{2}\sin^2\theta\beta^2_{i+k/2} + \ \sum_{i=1+k/2}^{k} \frac{1}{2}\sin^2\theta\beta^2_{i-k/2} \\
    & \ge \frac{k}{2}\sin^2\theta\gamma^2
\end{align*}
Finally, we need to perform a change of variable by setting $\epsilon = 2\sin \frac{\theta}{2}$. Elementary calculation gives $\sin\theta = \frac{\epsilon\sqrt{4-\epsilon^2}}{2}$ and $\tan\theta = \frac{\epsilon\sqrt{4-\epsilon^2}}{2-\epsilon^2}$. When $\epsilon<\frac{1}{2}$, we have $\sin\theta \ge 9\epsilon/10$ and $\tan\theta \le 6\epsilon/5$. Recall that our proof requires $\tan \theta < \gamma/\Gamma$ and $\sin\frac{\theta}{2}\le \frac{1}{\sqrt{1+16\Gamma^2/\gamma^2}}$. As a result, when $\epsilon = 2\sin \frac{\theta}{2} \le \min\bigbracket{\frac{1}{2},\frac{5\gamma}{6\Gamma},\frac{1}{\sqrt{1+16\Gamma^2/\gamma^2}}} = \frac{1}{\sqrt{1+16\Gamma^2/\gamma^2}}$, we have 
\begin{align*}
    \cL(f_{\tmD}) - \cL(f_{\mD^*}) = \cL(f_{\tmD}) \ge \frac{81k\epsilon^2\gamma^2}{200}.
\end{align*}
As we mentioned, for an odd value of $k$, an analogous argument can be made to arrive at a similar bound of 
\begin{align*}
    \cL(f_{\tmD}) - \cL(f_{\mD^*}) = \cL(f_{\tmD}) \ge \frac{81(k-1)\epsilon^2\gamma^2}{200},
\end{align*}
This completes the proof of \Cref{theorem: formal necessity}.
$\hfill\qed$
\subsection{Proof of \Cref{theorem: vanishing L convergence}}\label{section: proof theorem 2}
To prove the convergence of $\cL\bigbracket{\tf_{\mD^{(t)}}}$, we use an inductive approach where at each iteration, we will subsequently prove: (1) $\tf$ recovers the exact support $S^*$; (2) gradient descent will make progress towards one of the optimal minimizers which will be explicitly defined later; (3) $\cL\bigbracket{\tf_{\mD^{(t)}}}$ will decrease linearly. 

Recall that $\mD^{(t)} = \mD+\Delta\mD^{(t)}$. The equations below rewrite \Cref{eq: update rule} for each column $i\in [n]$ of $\mD^{(t)}$. 
\begin{align}
    \rvd^{(t+0.5)}_i &= \rvd^{(t)}_i - \eta\frac{\partial\cL(\tf_{\mD^{(t)}})}{\partial \rvd_i^{(t)}}, \label{eq: gd step}\\
    \rvd^{(t+1)}_i &= \argmin_{\rvd:\|\rvd - \rvd^{(0)}_i\|_2\le \rho} \|\rvd - \rvd^{(t+0.5)}_i\|_2. \label{eq: projection step}
\end{align}
It is easy to verify that the above update rules are equivalent to \Cref{eq: update rule}.
We will use induction to prove this theorem. In particular, define
\begin{align*}
    A(t):&\quad \cL(\tf_{\mD^{(t)}})\le \tau^2 \cL(\tf_{\mD^{(t-1)}}), \text{where $\tau = 1-2\eta\|\rvx\|^2_2$}.
\end{align*}
To prove this we define an auxiliary feature vector which will play an important role in our arguments:
\begin{align}\label{eq: hat d definition}
    \hrvd_i = \rvd^{(0)}_i + \frac{\beta_i - \rvx^\top\rvd^{(0)}_i}{\|\rvx\|^2_2}\rvx, \qquad \forall i\in [n].
\end{align}
Based on this auxiliary feature vector, we define the following event:
\begin{align*}
    B(t):&\quad \hrvd_i - \rvd^{(t)}_i = \frac{\beta_i - \rvx^\top\rvd^{(t)}_i}{\|\rvx\|^2_2}\rvx, \qquad \forall i\in [n].
\end{align*}
Now, our proof strategy is to show that, for $t=0,1,\dots$:
\begin{itemize}
    \item $B(t)$ implies $B(t+1)$,
    \item $B(t)$ implies $A(t+1)$.
\end{itemize}
This two statements combined will establish the correctness of \Cref{theorem: vanishing L convergence}.
Indeed, the base case $A(0)$ is trivially satisfied due to \Cref{eq: hat d definition}.

$\bullet$ \textbf{Establishing $B(t)\implies B(t+1)$.}
To begin with, we use the following lemma to show that $\tf$ is able to find the correct support when $\mD^{(t)}$ is close enough to $\mD$: 
\begin{lemma}\label{lemma: support recovery}
    For any $\mD$ such that $\|\mD-\mD^*\|_{1,2}\le2\rho<\frac{\gamma}{4\sqrt{k}\Gamma}$,we have 
    \begin{align*}
        S = \argmax_{T \subseteq [n], |T| \le k} \sum_{i \in T} |\langle \rvd_i, \rvx \rangle| = S^*.
    \end{align*}
\end{lemma}
We defer the proof of \Cref{lemma: support recovery} to \Cref{section: proof to lemma support recovery}. To invoke \Cref{lemma: support recovery}, we have
\begin{align*}
    \|\mD^{(t)} - \mD^*\|_{1,2} \le \|\mD^{(t)} - \mD^{(0)}\|_{1,2} + \|\mD^{(0)} - \mD^*\|_{1,2} \le \rho + \rho = 2\rho.
\end{align*}
The bound on $\|\mD^{(t)} - \mD^{(0)}\|_{1,2}$ follows from the projection step in \Cref{eq: projection step} while the bound on $\|\mD^{(0)} - \mD^*\|_{1,2} $ follows from the initial error bound. We can then conclude that $S=S^*$ at every iteration $t$, which means that \Cref{eq: gd step} and \Cref{eq: projection step} will only change $i\in S^*$, while for other $i\not\in S^*$, we have $\rvd^{(t+1)}_i = \rvd^{(t)}_i$. This immediately implies that $B(t)$ is trivially satisfied for $\forall i\not\in S^*$ and $\forall t$. Now, the goal is to show that $B(t)\implies B(t+1)$ for $\forall i\in S^*$.

Based on \Cref{eq::Lf}, we have
\begin{align*}
    \cL(\tf_{\mD^{(t)}}) = \|\mD^*_{S^*}\mD^{(t)\top}_{S^*} \rvx -\rvx\|^2_2,
\end{align*}
Subsequently, the gradient of $\cL$ at $\mD^{(t)}_{S^*}$ can be written as:
\begin{align*}
    \frac{\partial\cL(\tf_{\mD^{(t)}})}{\partial \mD^{(t)}_{S^*}} &= 2\rvx\bigbracket{\mD^*_{S^*}\mD^{(t)\top}_{S^*} \rvx -\rvx}^\top \mD^*_{S^*}\\
    &=2\rvx\bigbracket{\mD^*_{S^*}\mD^{(t)\top}_{S^*} \rvx -\mD^*_{S^*}\rvbeta_{S^*}}^\top \mD^*_{S^*}\\
    &= 2\rvx\bigbracket{\mD^{(t)\top}_{S^*} \rvx -\rvbeta_{S^*}}^\top \mD^{*\top}_{S^*}\mD^*_{S^*}\\
    &= 2\rvx\bigbracket{\mD^{(t)\top}_{S^*} \rvx -\rvbeta_{S^*}}^\top.
\end{align*}
For $i\in S^*$, we have
\begin{align*}
    \frac{\partial\cL(\tf_{\mD^{(t)}})}{\partial \rvd^{(t)}_i} = 2(\rvd^{(t)\top}_i\rvx - \rvbeta_i)\rvx.
\end{align*}
This implies that:
\begin{align*}
    \hrvd_i - \rvd^{(t+0.5)}_i &= \hrvd_i - \rvd^{(t)}_i + 2\eta (\rvd^{(t)\top}_i\rvx - \rvbeta_i)\rvx \\
    &\overset{A(t)}{=}\frac{\beta_i - \rvx^\top\rvd^{(t)}_i}{\|\rvx\|^2_2}\rvx+ 2\eta (\rvd^{(t)\top}_i\rvx - \rvbeta_i)\rvx \\
    &=\frac{\beta_i - \rvx^\top\bigbracket{\rvd^{(t+0.5)}_i+2\eta (\rvd^{(t)\top}_i\rvx - \rvbeta_i)\rvx}}{\|\rvx\|^2_2}\rvx+ 2\eta (\rvd^{(t)\top}_i\rvx - \rvbeta_i)\rvx \\
    &= \frac{\beta_i - \rvx^\top\rvd^{(t+0.5)}_i}{\|\rvx\|^2_2}\rvx - 2\eta (\rvd^{(t)\top}_i\rvx - \rvbeta_i)\rvx +2\eta (\rvd^{(t)\top}_i\rvx - \rvbeta_i)\rvx \\
    &= \frac{\beta_i - \rvx^\top\rvd^{(t+0.5)}_i}{\|\rvx\|^2_2}\rvx.
\end{align*}
In other words, $B(t)$ implies $B(t+0.5)$. 

Suppose $\|\rvd^{(t+0.5)}_i - \rvd^{(0)}_i\|_2\le\rho$, then we have $\rvd^{(t+1)}_i = \rvd^{(t+0.5)}_i$, which readily implies $B(t+1)$. On the other hand, if $\|\rvd^{(t+0.5)}_i - \rvd^{(0)}_i\|_2>\rho$, we have:
\begin{equation}\label{eq: d^(t+0.5) - d^(0)}
    \begin{aligned}
    \rvd^{(t+0.5)}_i - \rvd^{(0)}_i
    &= \bigbracket{\rvd^{(t+0.5)}_i-\hrvd_i } - \bigbracket{\rvd^{(0)}_i-\hrvd_i } \\
    &\overset{A(t+0.5),A(0)}{=} -\frac{\beta_i - \rvx^\top\rvd^{(t+0.5)}_i}{\|\rvx\|^2_2}\rvx + \frac{\beta_i - \rvx^\top\rvd^{(0)}_i}{\|\rvx\|^2_2}\rvx \\
    &=  \frac{\rvx^\top\rvd^{(t+0.5)}_i - \rvx^\top\rvd^{(0)}_i}{\|\rvx\|^2_2}\rvx,
\end{aligned}
\end{equation}
Therefore, when $\|\rvd^{(t+0.5)}_i - \rvd^{(0)}_i\|_2>\rho$, we have:
\begin{equation}\label{eq: d^(t+1) definition}
    \begin{aligned}
    \rvd^{(t+1)}_i & = \argmin_{\rvd:\|\rvd - \rvd^{(0)}_i\|_2\le \rho} \|\rvd - \rvd^{(t+0.5)}_i\|_2 \\
    &= \rvd^{(0)}_i + \rho\frac{\rvd^{(t+0.5)}_i - \rvd^{(0)}_i}{\|\rvd^{(t+0.5)}_i - \rvd^{(0)}_i\|_2}\\
    &= \rvd^{(0)}_i + \rho\sign\bigbracket{\rvx^\top\rvd^{(t+0.5)}_i - \rvx^\top\rvd^{(0)}_i}\frac{\rvx}{\|\rvx\|_2}.
\end{aligned}
\end{equation}
This in turn implies
% We are then ready to show $A(t+1)$ when $\|\rvd^{(t+0.5)}_i - \rvd^{(0)}_i\|_2>\rho$:
\begin{align*}
    &\quad \ \hrvd_i - \rvd^{(t+1)}_i \\
    &= \hrvd_i - \rvd^{(0)}_i - \rho\sign\bigbracket{\rvx^\top\rvd^{(t+0.5)}_i - \rvx^\top\rvd^{(0)}_i}\frac{\rvx}{\|\rvx\|_2} \\
    &\overset{A(0)}{=}\frac{\beta_i - \rvx^\top\rvd^{(0)}_i}{\|\rvx\|^2_2}\rvx - \rho\sign\bigbracket{\rvx^\top\rvd^{(t+0.5)}_i - \rvx^\top\rvd^{(0)}_i}\frac{\rvx}{\|\rvx\|_2}\\
    &=\frac{\beta_i - \rvx^\top\bigbracket{\rvd^{(t+1)}_i-\rho\sign\bigbracket{\rvx^\top\rvd^{(t+0.5)}_i - \rvx^\top\rvd^{(0)}_i}\frac{\rvx}{\|\rvx\|_2}}}{\|\rvx\|^2_2}\rvx-\rho\sign\bigbracket{\rvx^\top\rvd^{(t+0.5)}_i - \rvx^\top\rvd^{(0)}_i}\frac{\rvx}{\|\rvx\|_2} \\
    &= \frac{\beta_i - \rvx^\top\rvd^{(t+1)}_i}{\|\rvx\|^2_2}\rvx,
\end{align*}
establishing $B(t+1)$, as desired. 

$\bullet$ \textbf{Establishing $B(t)\implies A(t+1)$.}
First, we establish $\|\hrvd_i - \rvd^{(t+1)}_i\|_2\le \tau\|\hrvd_i - \rvd^{(t)}_i\|_2$. We do so in two steps. First, we prove that $\|\hrvd_i - \rvd^{(t+0.5)}_i\|_2\le \tau\|\hrvd_i - \rvd^{(t)}_i\|_2$. Then, we show that $\|\hrvd_i - \rvd^{(t+1)}_i\|_2\le \|\hrvd_i - \rvd^{(t+0.5)}_i\|_2$. 

To establish $\|\hrvd_i - \rvd^{(t+1)}_i\|_2\le \tau\|\hrvd_i - \rvd^{(t)}_i\|_2$, one can write:
\begin{align*}
    \|\hrvd_i - \rvd^{(t+0.5)}_i\|_2 &= 
    \|\hrvd_i - \rvd^{(t)}_i+ 2\eta (\rvd^{(t)\top}_i\rvx - \rvbeta_i)\rvx\|_2\\
    &\overset{A(t)}{=} 
    \left\|\hrvd_i - \rvd^{(t)}_i- 2\eta \|\rvx\|^2_2\bigbracket{\hrvd_i - \rvd^{(t)}_i} \right\|_2\\
    &\le \left|1-2\eta\|\rvx\|^2_2\right|\|\hrvd_i - \rvd^{(t)}_i\|_2\\
    &= \tau\|\hrvd_i - \rvd^{(t)}_i\|_2.
\end{align*}

Next, we show that $\|\hrvd_i - \rvd^{(t+1)}_i\|_2\le \|\hrvd_i - \rvd^{(t+0.5)}_i\|_2$. To do so, we need the following elementary lemma:
\begin{lemma}\label{lemma: elementary position}
    Given a fixed vector $\rvv$ and $\rva = a\rvv$, $\rvb = b\rvv$, $\rvc = c\rvv$, where $\rva$, $\rvb$ and $\rvc$ are vectors and $a$, $b$, $c$ are scalars, if $|c|\ge|b|\ge a$ and $bc>0$, we have $\|\rvc-\rva\|_2\ge \|\rvb-\rva\|_2$.
\end{lemma}
We will prove \Cref{lemma: elementary position} in \Cref{section: proof to lemma elementary position}. Now we invoke \Cref{lemma: elementary position} with:
\begin{align*}
    \rvv &= \frac{\rvx}{\|\rvx\|_2},\\
    a &= \frac{\rvbeta_i - \rvx^\top\rvd^{(0)}_i}{\|\rvx\|_2},\\
    b &= \rho \sign\bigbracket{\rvx^\top\rvd^{(t+0.5)}_i - \rvx^\top\rvd^{(0)}_i},\\
    c &= \frac{\rvx^\top\rvd^{(t+0.5)}_i - \rvx^\top\rvd^{(0)}_i}{\|\rvx\|_2}.
\end{align*}
Here $bc>0$ is given by their definition. Moreover $|c|\ge|b|$ is established as:
\begin{align*}
    |c|=& \left|\frac{\rvx^\top\rvd^{(t+0.5)}_i - \rvx^\top\rvd^{(0)}_i}{\|\rvx\|_2}\right| \\
    =& \left\| \frac{\rvx^\top\rvd^{(t+0.5)}_i - \rvx^\top\rvd^{(0)}_i}{\|\rvx\|^2_2}\rvx \right\|_2 \\
    \overset{\eqref{eq: d^(t+0.5) - d^(0)}}{=}&\|\rvd^{(t+0.5)}_i - \rvd^{(0)}_i\|_2 \\
    \ge & \rho \\
    = & |b|.
\end{align*}
Finally, $|b|>a$ is given by:
\begin{equation}\label{eq: a<b}
    \begin{aligned}
    a &= \frac{\rvbeta_i - \rvx^\top\rvd^{(0)}_i}{\|\rvx\|_2}\\
    &=\frac{\rvx^\top\rvd^*_i - \rvx^\top\rvd^{(0)}_i}{\|\rvx\|_2} \\
    &\le \frac{\|\rvd^*_i-\rvd^{(0)}_i\|_2\|\rvx\|_2}{\|\rvx\|_2}\\
    &\le \|\rmD^*-\rmD^{(0)}\|_{1,2} \\
    &= \rho \\
    &= |b|.
\end{aligned}
\end{equation}

Now, upon substituting these parameters in \Cref{lemma: elementary position}, we have $\|\rvc-\rva\|_2 \ge \|\rvb-\rva\|_2$, which can be rewritten as:
\begin{align*}
    % \|\rvc-\rva\|_2 &\ge \|\rvb-\rva\|_2 \\
    \left\|\frac{\rvx^\top\rvd^{(t+0.5)}_i - \rvx^\top\rvd^{(0)}_i}{\|\rvx\|^2_2}\rvx - \frac{\beta_i - \rvx^\top\rvd^{(0)}_i}{\|\rvx\|^2_2}\rvx\right\|_2 &\ge \left\|\rho \sign\bigbracket{\rvx^\top\rvd^{(t+0.5)}_i - \rvx^\top\rvd^{(0)}_i}\frac{\rvx}{\|\rvx\|_2}- \frac{\beta_i - \rvx^\top\rvd^{(0)}_i}{\|\rvx\|^2_2}\rvx\right\|_2\\
    \left\|\bigbracket{\rvd^{(t+0.5)}_i - \rvd^{(0)}_i}-\bigbracket{\hrvd_i-\rvd^{(0)}_i}\right\|_2&\overset{(a)}{\ge}\left\|\bigbracket{\rvd^{(t+1)}_i - \rvd^{(0)}_i}-\bigbracket{\hrvd_i-\rvd^{(0)}_i}\right\|_2\\
    \|\rvd^{(t+0.5)}_i-\hrvd_i\|_2 &\ge \|\rvd^{(t+1)}_i-\hrvd_i\|_2
\end{align*}
Here inequality (a) is obtained by \Cref{eq: d^(t+0.5) - d^(0)}, \Cref{eq: hat d definition}, and \Cref{eq: d^(t+1) definition}. We can now conclude that:
\begin{align}\label{eq: d^(t+1) < d^(t)}
    \|\rvd^{(t+1)}_i-\hrvd_i\|_2 \le \|\rvd^{(t+0.5)}_i-\hrvd_i\|_2 \le \tau \|\rvd^{(t)}_i-\hrvd_i\|_2.
\end{align}
Finally, we are ready to establish $A(t+1)$. We rewrite $\cL(\tf_{\mD^{(t)}})$ as:
\begin{align*}
    \cL(\tf_{\mD^{(t)}}) & = \|\mD^*_{S^*}\mD^{(t)\top}_{S^*} \rvx -\rvx\|^2_2 \\
    &= \|\mD^*_{S^*}\mD^{(t)\top}_{S^*} \rvx -\mD^*_{S^*}\rvbeta_{S^*}\|^2_2 \\
    &= \|\mD^{(t)\top}_{S^*} \rvx -\rvbeta_{S^*}\|^2_2\\
    &= \sum_{i\in S^*} \bigbracket{\rvx^\top\rvd^{(t)}_i-\rvbeta_i}^2 \\
    &= \sum_{i\in S^*}\left\|\frac{\beta_i - \rvx^\top\rvd^{(t)}_i}{\|\rvx\|^2_2}\rvx\right\|^2_2 \|\rvx\|^2_2\\
    &\overset{B(t)}{=}\sum_{i\in S^*}\left\|\rvd^{(t)}_i-\hrvd_i\right\|^2_2 \|\rvx\|^2_2
\end{align*}
On the other hand,
\begin{align*}
    \cL(\tf_{\mD^{(t+1)}}) = \sum_{i\in S^*}\left\|\rvd^{(t+1)}_i-\hrvd_i\right\|^2_2 \|\rvx\|^2_2
\end{align*}
can be established by identical arguments. Finally, we have
\begin{align*}
    \cL(\tf_{\mD^{(t+1)}}) = \sum_{i\in S^*}\left\|\rvd^{(t+1)}_i-\hrvd_i\right\|^2_2 \|\rvx\|^2_2 \overset{\eqref{eq: d^(t+1) < d^(t)}}{\le} \tau^2\sum_{i\in S^*}\left\|\rvd^{(t)}_i-\hrvd_i\right\|^2_2 \|\rvx\|^2_2 = \cL(\tf_{\mD^{(t)}}),
\end{align*}
which is exactly $B(t+1)$. As a result, we have proved \Cref{theorem: vanishing L convergence}. 

\subsection{Proof of \Cref{theorem: columnwise convergence}}\label{section: proof theorem 3}

To prove the statements of this theorem, we first establish the convergence of $\|\rvd^{(t)}_i - \rvd^{*}_i\|_2$ for every $i\in [n]$. Then, we provide the desired upper bound on $\cL_m\bigbracket{\tf_{\mD^{(t)}}}$ in terms of $\|\mD^{(t)}\!-\!\mD^*\|^2_{1,2}$.

We will follow the same index convention defined in \Cref{eq: gd step} and \Cref{eq: projection step}, with $\cL$ replaced by $\cL_m$. For each $h\in [m]$, we use $\rvbeta^h$ and $S^{h*}$ to denote the corresponding variables in \Cref{assumption: probabilistic generative model of x}. We define the set $Q_i$ as the index set of $h$ such that $i$ is in the support of $\rvbeta^h$:
\begin{align*}
    Q_i :=\left\{h\in[m]\mid i\in S^{h*} \right\}.
\end{align*}

Given \Cref{lemma: support recovery}, we have:
\begin{align*}
    \frac{\partial\cL_m(\tf_{\mD^{(t)}})}{\partial \rvd^{(t)}_i} = \frac{1}{m}\sum_{h\in Q_i} 2(\rvd^{(t)\top}_i\rvx^h - \rvbeta^h_i)\rvx^h, \qquad \forall i\in [n].
\end{align*}
To ensure that $\frac{\partial\cL_m(\tf_{\mD^{(t)}})}{\partial \rvd^{(t)}_i}$ is indeed aligned with $\rvd^{(t)}_i-\rvd^*_i$, we first need the following lemma:
\begin{lemma}
% [\cite{motwani1996randomized}]
\label{lemma: align condition}
    Suppose that $m=\Omega\bigbracket{\frac{n^6}{\sigma^2 k^5}}$. With probability at least $1-2\exp\{\log n-n\}-2\exp\left\{\log n-\frac{km}{8n}\right\}$, for all $i\in [n]$, 
    \begin{align*}
        \frac{1}{m}\sigma_d\bigbracket{\sum_{h\in Q_i}\rvx^h\rvx^{h\top}} &\ge \frac{k(k-1)\sigma^2}{4n^2},\\
        \frac{1}{m}\sigma_1\bigbracket{\sum_{h\in Q_i}\rvx^h\rvx^{h\top}} &\le \frac{4k\sigma^2}{n}.
    \end{align*}
\end{lemma}
We defer the proof of \Cref{lemma: align condition} to \Cref{section: proof to lemma align condition}. Now, we can bound $\|\rvd^{(t+0.5)}_i-\rvd^*_i\|^2_2$ as:
\begin{align}
    &\|\rvd^{(t+0.5)}_i-\rvd^*_i\|^2_2 = \left\|\rvd^{(t)}_i-\frac{\eta}{m}\sum_{h\in Q_i} 2(\rvd^{(t)\top}_i\rvx^h - \rvbeta^h_i)\rvx^h-\rvd^*_i\right\|^2_2\nonumber\\
    &= \|\rvd^{(t)}_i-\rvd^*_i\|^2_2 - 4 \eta\left\langle\rvd^{(t)}_i-\rvd^*_i,\frac{1}{m}\sum_{h\in Q_i} (\rvd^{(t)\top}_i\rvx^h - \rvbeta^h_i)\rvx^h\right\rangle + 4\eta^2\left\|\frac{1}{m}\sum_{h\in Q_i} (\rvd^{(t)\top}_i\rvx^h - \rvbeta^h_i)\rvx^h\right\|^2_2.\label{eq: break squre of d - d^*}
\end{align}
For the second term on the right hand side, we have:
\begin{align*}
    &\left\langle\rvd^{(t)}_i-\rvd^*_i,\frac{1}{m}\sum_{h\in Q_i} (\rvd^{(t)\top}_i\rvx^h - \rvbeta^h_i)\rvx^h\right\rangle \\
    =\quad & \left\langle\rvd^{(t)}_i-\rvd^*_i,\frac{1}{m}\sum_{h\in Q_i} (\rvd^{(t)\top}_i\rvx^h - \rvd^{*\top}_i\rvx^h)\rvx^h\right\rangle \\
    =\quad & \left\langle\rvd^{(t)}_i-\rvd^*_i,\frac{1}{m}\sum_{h\in Q_i}\rvx^h\rvx^{h\top}(\rvd^{(t)}_i- \rvd^{*}_i)\right\rangle \\
    \ge \quad & \frac{1}{m}\sigma_d\bigbracket{\sum_{h\in Q_i}\rvx^h\rvx^{h\top}}\left\|\rvd^{(t)}_i- \rvd^{*}_i\right\|^2_2 \\
    \overset{\Cref{lemma: align condition}}{\ge} \quad &\frac{k(k-1)\sigma^2}{4n^2}\left\|\rvd^{(t)}_i- \rvd^{*}_i\right\|^2_2.
\end{align*}
For the third term on the right hand side of \Cref{eq: break squre of d - d^*}, we have:
\begin{align*}
    &\left\|\frac{1}{m}\sum_{h\in Q_i} (\rvd^{(t)\top}_i\rvx^h - \rvbeta^h_i)\rvx^h\right\|^2_2\\
    =\quad & \left\|\frac{1}{m}\sum_{h\in Q_i}\rvx^h\rvx^{h\top}(\rvd^{(t)}_i- \rvd^{*}_i)\right\|^2_2 \\
    \le\quad & \frac{1}{m^2}\sigma^2_1\bigbracket{\sum_{h\in Q_i}\rvx^h\rvx^{h\top}}\left\|\rvd^{(t)}_i- \rvd^{*}_i\right\|^2_2 \\
    \overset{\Cref{lemma: align condition}}{\le} \quad &\frac{16k^2\sigma^4}{n^2}\left\|\rvd^{(t)}_i- \rvd^{*}_i\right\|^2_2.
\end{align*}
When $\eta<\frac{k-1}{128k\sigma^2}$, the above two inequalities can be combined with \Cref{eq: break squre of d - d^*} to arrive at:
\begin{equation}\label{eq: d t+0.5 - d^*}
    \begin{aligned}
    \|\rvd^{(t+0.5)}_i-\rvd^*_i\|^2_2 &\le \left(1-\frac{k(k-1)\sigma^2}{n^2}\eta + \frac{64k^2\sigma^4}{n^2}\eta^2\right)\|\rvd^{(t)}_i-\rvd^*_i\|^2_2 \\
    &\le \left(1-\frac{k(k-1)\sigma^2}{2n^2}\eta \right)\|\rvd^{(t)}_i-\rvd^*_i\|^2_2 \\
    &\le \tau^2 \|\rvd^{(t)}_i-\rvd^*_i\|^2_2.
\end{aligned}
\end{equation}

Next, we aim to show that $\|\rvd^{(t+1)}_i-\rvd^*_i\|^2_2\le \|\rvd^{(t+0.5)}_i-\rvd^*_i\|^2_2$. Recall that:
\begin{equation*}
    \rvd^{(t+1)}_i = \argmin_{\rvd\in D} \|\rvd - \rvd^{(t+0.5)}_i\|^2_2 \quad\text{where}\quad D=\{\rvd:\|\rvd-\rvd^{(0)}_i\|_2\le \rho\}.
\end{equation*}
It is obvious that $D$ is convex and $\|\rvd - \rvd^{(t+0.5)}_i\|^2_2$ is strongly convex with respect to $\rvd$. As a result, first order stationary condition requires that:
\begin{align*}
    \left\langle \nabla_{\rvd=\rvd^{(t+1)}_i} \|\rvd - \rvd^{(t+0.5)}_i\|^2_2,\ \trvd-\rvd^{(t+1)}_i\right\rangle=2\left\langle \rvd^{(t+1)}_i-\rvd^{(t+0.5)}_i,\trvd-\rvd^{(t+1)}_i\right\rangle \ge 0,
\end{align*}
for any $\trvd \in D$. Given the initial error bound $\|\mD^{(0)}-\mD^*\|_{1,2}\le \rho$, we have $\rvd^* \in D$, which results in:
\begin{align*}
    \left\langle \rvd^{(t+1)}_i-\rvd^{(t+0.5)}_i,\rvd^*-\rvd^{(t+1)}_i\right\rangle \ge 0.
\end{align*}
We subsequently have:
\begin{align}
    &\|\rvd^*_i-\rvd^{(t+0.5)}_i\|^2_2 =\left\|\left(\rvd^{(t+1)}_i-\rvd^{(t+0.5)}_i\right)+\left(\rvd^*-\rvd^{(t+1)}_i\right)\right\|^2_2\nonumber\\
    =\quad & \|\rvd^{(t+1)}_i-\rvd^{(t+0.5)}_i\|^2_2+2\left\langle \rvd^{(t+1)}_i-\rvd^{(t+0.5)}_i,\rvd^*-\rvd^{(t+1)}_i\right\rangle+\|\rvd^*_i-\rvd^{(t+1)}_i\|^2_2\nonumber\\
    \ge\quad &\|\rvd^*_i-\rvd^{(t+1)}_i\|^2_2,\label{eq: d^t+1 - d^* < d^t+0.5 - d^*}
\end{align}
where the last inequality follows from the fact that both $\|\rvd^{(t+1)}_i-\rvd^{(t+0.5)}_i\|^2_2$ and $\left\langle \rvd^{(t+1)}_i-\rvd^{(t+0.5)}_i,\rvd^*-\rvd^{(t+1)}_i\right\rangle$ are non-negative. Combining \Cref{eq: d t+0.5 - d^*} and \Cref{eq: d^t+1 - d^* < d^t+0.5 - d^*}, we have 
\begin{align*}
    \|\rvd^{(t+1)}_i-\rvd^*_i\|^2_2\le \|\rvd^{(t+0.5)}_i-\rvd^*_i\|^2_2 \le \tau^2 \|\rvd^{(t)}_i-\rvd^*_i\|^2_2.
\end{align*}

Finally, note that
\begin{align*}
    \|\mD^{(t+1)}-\mD^*\|_{1,2}
    = \max_{i\in [n]}\{\|\rvd^{(t+1)}_{i}-\rvd^*_{i}\|_2\} \le \tau\max_{i\in [n]}\{\|\rvd^{(t)}_{i}-\rvd^*_{i}\|_2\}\le  \tau\|\mD^{(t)}-\mD^*\|_{1,2},
\end{align*}

Next, we show that $\cL_m\bigbracket{\mD^{(t)}}$ is upper bounded by $\|\mD^{(t)}-\mD^*\|_{1,2}$:
\begin{align*}
    \cL_m\bigbracket{\tf_{\mD^{(t)}}} &= \frac{1}{m}\sum_{h=1}^m\|\mD^*_{S^{h*}}\mD^{(t)\top}_{S^{h*}} \rvx^h -\rvx^h\|^2_2 \\
    &= \frac{1}{m}\sum_{h=1}^m\|\mD^{(t)\top}_{S^{h*}} \rvx^h -\rvbeta^h_{S^{h*}}\|^2_2 \\
    &= \frac{1}{m}\sum_{h=1}^m\sum_{i\in S^{h*}}\bigbracket{\rvx^{h\top}\rvd^{(t)}_i-\rvx^{h\top}\rvd^*_i}^2\\
    &\le \frac{1}{m}\sum_{h=1}^m\sum_{i\in S^{h*}}\|\rvx^h\|^2_2\|\rvd^{(t)}_i-\rvd^*_i\|^2_2\\
    &\le \frac{1}{m}\sum_{h=1}^mk\|\rvx^h\|^2_2\|\mD^{(t)}-\mD^*\|^2_{1,2}\\
    &= \frac{k\sum_{h=1}^m\|\rvx^h\|^2_2}{m}\|\mD^{(t)}-\mD^*\|^2_{1,2}.
\end{align*}

To complete the proof of \Cref{theorem: columnwise convergence}, we need to examine the success probability which the above linear convergence occurs with. The only probability statement we use is \Cref{lemma: align condition} and we only invoke it once. So the total success probability is at least:
\begin{align*}
    1 -2\exp\{\log n-n\}-2\exp\left\{\log n-\frac{km}{8n}\right\},
\end{align*}
which is $1-n^{-\omega(1)}$ when $m=\Omega\bigbracket{\frac{n^6}{\sigma^2 k^5}}$. This completes the proof of \Cref{theorem: columnwise convergence}. $\hfill\qed$

\subsection{Proof of Lemmas}
\subsubsection{Proof of \Cref{lemma: introducing tf}}\label{section: proof to lemma introducing tf}
\Cref{lemma: introducing tf} directly follows from \Cref{lemma: closed form f_D} and \Cref{lemma: column-orthogonal IP-OMP results}.

\subsubsection{Proof of \Cref{lemma: closed form f_D}}\label{section: proof to lemma closed form f_D}
We can decompose $y$ as follows:
\begin{align*}
    y = \langle \rvx, \rvz \rangle = \langle \Pi_{\mD^*_S}\rvx, \rvz \rangle + \langle \Pi^\perp_{\mD^*_S}\rvx, \rvz \rangle
\end{align*}
Note that $\rvz\sim \mathcal{N}(\mathbf{0},\mI_{d\times d})$. This implies that $\Pi^\perp_{\mD^*_S}\rvz$ is independent of $\Pi_{\mD^*_S}\rvz$ \cite{anderson1958introduction}, which in turn entails that $ \langle \Pi^\perp_{\mD^*_S}\rvx, \rvz \rangle$ is independent of $\langle \Pi_{\mD^*_S}\rvx, \rvz \rangle$ and the events $\{\langle \rvd^*_i, \rvz \rangle = r_i \quad \forall i \in S\}$. Therefore, we have:
\begin{align*}
    \mle\bigbracket{\langle \rvx, \rvz \rangle \mid \langle \rvd^*_i, \rvz \rangle = r_i \quad \forall i \in S} =\quad & \mle\bigbracket{\langle \Pi_{\mD^*_S}\rvx, \rvz \rangle \mid \langle \rvd^*_i, \rvz \rangle = r_i \quad \forall i \in S}\\
    &+ \mle\bigbracket{\langle \Pi^\perp_{\mD^*_S}\rvx, \rvz \rangle }.
\end{align*}
It turns out that the first term on the right hand side is deterministic:
\begin{align*}
    \langle \Pi_{\mD^*_S}\rvx, \rvz \rangle &=
    \langle \mD^*_S\mD^{*\top}_S \rvx,\rvz\rangle \\
    &= \langle \sum_{i\in S} \langle\rvd^*_i,\rvx\rangle\rvd^*_i,\rvz\rangle \\ 
    &= \sum_{i\in S} \langle\rvd^*_i,\rvx\rangle \langle\rvd^*_i,\rvz\rangle \\
    &= \sum_{i\in S} \langle\rvd^*_i,\rvx\rangle r_i.
\end{align*}
For the last equality we used $q_i=r_i$. For the second term, we have:
\begin{align*}
    \mle\bigbracket{\langle \Pi^\perp_{\mD^*_S}\rvx, \rvz \rangle }
    &{=} \mle\bigbracket{\langle \Pi^\perp_{\mD^*_S}\rvx, \Pi^\perp_{\mD^*_S} \rvz \rangle}\\
    &\overset{(a)}{=}0.
\end{align*}
The equality $(a)$ is due to the observation that $\Pi^\perp_{\mD^*_S} \rvz$ is indeed distributed as $\mathcal{N}(\mathbf{0},\mI_{(d-k)\times (d-k)})$ after a simple dimensionality reduction \cite{anderson1958introduction}. The resulting random variable $\langle \Pi^\perp_{\mD^*_S}\rvx, \Pi^\perp_{\mD^*_S} \rvz \rangle$ is distributed as $\mathcal{N}(0,\|\Pi^\perp_{\mD^*_S}\rvx\|_2^2)$, leading to the equality $(a)$. To sum up, we have 
\begin{align*}
    \mle\bigbracket{\langle \rvx, \rvz \rangle \mid \langle \rvd^*_i, \rvz \rangle = r_i \quad \forall i \in S} = \sum_{i\in S} \langle\rvd^*_i,\rvx\rangle r_i.
\end{align*}$\hfill\qed$


\subsubsection{Proof of \Cref{lemma: column-orthogonal IP-OMP results}}\label{section: prooof of column-orthogonal ip omp results}
Let us define 
\begin{align*}
    S_k = \argmax_{T\subseteq[n],|T|\le k} \sum_{i\in T} |\langle \rvd_i, \rvx \rangle|.
\end{align*}
We will use induction to show that $S_k$ corresponds to the indices selected by IP-OMP after $k$ iterations. At the first iteration, according to \cite{chattopadhyay2024information}, IP-OMP simply selects the index $i=\arg\max_{j\in[n]}|\langle \rvd_j, \rvx \rangle|/\|\rvd_j\|_2\|\rvx\|_2 =  \arg\max_{j\in[n]}|\langle \rvd_j, \rvx \rangle|$, where the second equality is due to $\|\rvd_j\|_2=1$ for all $j\in [n]$. As a result we have $S_1 = \{i\}$ and the base case is established.

Suppose that the statement holds for $S_{k-1} = \allowbreak \arg\max_{T\subseteq[n],|T|\le k-1} \allowbreak\sum_{i\in T}\allowbreak |\langle \rvd_i, \rvx \rangle|$. Let $\mD_{S_{k-1}}$ be the submatrix of $\mD$ which consists of columns indexed by $S_{k-1}$. Then IP-OMP will select
\begin{align*}
    i = \argmax_{j\in[n],j\not\in S_{k-1}}\frac{|\langle \Pi^\perp_{\mD_{S_{k-1}}}\rvd_j, \Pi^\perp_{\mD_{S_{k-1}}}\rvx \rangle|}{\|\Pi^\perp_{\mD_{S_{k-1}}}\rvd_j\|_2\|\Pi^\perp_{\mD_{S_{k-1}}}\rvx\|_2}.
\end{align*}
As $\mD$ is column-orthogonal, we have $\Pi^\perp_{\mD_{S_{k-1}}}\rvd_j = \rvd_j$ and $\Pi_{\mD_{S_{k-1}}}\rvd_j = \mathbf{0}$ $\forall j \not\in S_{k-1}$. Therefore, for $\forall j \not\in S_{k-1}$, we have
\begin{align*}
    \langle \rvd_j,\rvx \rangle &= \langle \Pi^\perp_{\mD_{S_{k-1}}}\rvd_j,\Pi^\perp_{\mD_{S_{k-1}}}\rvx \rangle + \langle \Pi_{\mD_{S_{k-1}}}\rvd_j,\Pi_{\mD_{S_{k-1}}}\rvx \rangle\\
    &= \langle \Pi^\perp_{\mD_{S_{k-1}}}\rvd_j,\Pi^\perp_{\mD_{S_{k-1}}}\rvx \rangle.
\end{align*}
As a result, we can rewrite the selection rule of IP-OMP as  
\begin{align*}
    i &= \argmax_{j\in[n],j\not\in S_{k-1}}\frac{|\langle \Pi^\perp_{\mD_{S_{k-1}}}\rvd_j, \Pi^\perp_{\mD_{S_{k-1}}}\rvx \rangle|}{\|\Pi^\perp_{\mD_{S_{k-1}}}\rvd_j\|_2\|\Pi^\perp_{\mD_{S_{k-1}}}\rvx\|_2}\\
    &= \argmax_{j\in[n],j\not\in S_{k-1}} \frac{|\langle \rvd_j,\rvx \rangle|}{\|\rvd_j\|_2\|\rvx\|_2}\\
    &=\argmax_{j\in[n],j\not\in S_{k-1}}|\langle \rvd_j,\rvx \rangle|.
\end{align*}
The last equality is due to $\|\rvd_j\|_2=1$ and the fact that $\|\rvx\|_2$ is the same for different $j$. This implies that, at iteration $k$, IP-OMP will select the index $j\not\in S_{k-1}$ with the largest $|\langle \rvd_j,\rvx \rangle|$, which completes the proof of our induction.

Finally, when $\mD=\mD^*$, one can verify that $\langle \rvd_i,\rvx \rangle = \langle \rvd^*_i,\mD^*\rvbeta \rangle = 0 $ for $i\not\in S^*$. This implies that $S_k=S^*$.
$\hfill\qed$

\subsubsection{Proof of \Cref{lemma: D property}}\label{section: proof to lemma D property}
    To prove $\tmD$ is column-orthogonal, we first observe that, for $i\le k$, we have $\|\trvd_i\|^2_2 = \cos^2\theta + \sin^2\theta = 1$. Moreover, for any pair $i<j\le k/2$, we have
    \begin{align*}
        \langle \trvd_i,\trvd_j \rangle = \langle \cos\theta \rvd^*_i + \sin\theta \rvd^*_{i+k/2},\cos\theta \rvd^*_j + \sin\theta \rvd^*_{j+k/2} \rangle = 0,
    \end{align*}
    which follows since $\langle\rvd^*_i,\rvd^*_j\rangle=\langle\rvd^*_{i+k/2},\rvd^*_j\rangle=\langle\rvd^*_i,\rvd^*_{j+k/2}\rangle=\langle\rvd^*_{i+k/2},\rvd^*_{j+k/2}\rangle=0$. A similar argument can be made for any $i<j\le k$ and $j\ne i+k/2$. For any $i\leq k$ and $j=i+k/2$, we have 
    \begin{align*}
        \langle \trvd_i,\trvd_j \rangle = \langle \cos\theta \rvd^*_i + \sin\theta \rvd^*_{i+k/2},-\sin\theta \rvd^*_i + \cos\theta \rvd^*_{i+k/2} \rangle = -\sin\theta\cos\theta +  \sin\theta\cos\theta = 0.
    \end{align*}
    Finally, for any $j>i>k$ we trivially have $\langle \trvd_i,\trvd_j \rangle = 0$. This completes the proof of column-orthogonality of $\tilde\mD$. 

To prove the second statement, note that $\|\trvd_i-\rvd^*_i\|_2 = 0$ for every $i>k$. Moreover, for every $i\le k$, we have 
    \begin{align*}
        \|\trvd_i-\rvd^*_i\|_2 = \|(\cos\theta-1) \rvd^*_i + \sin\theta \rvd^*_{i+k}\|_2 = \sqrt{(\cos\theta-1)^2+\sin^2\theta} = \sqrt{2-2\cos\theta} = 2\sin \frac{\theta}{2}.
    \end{align*}
    This completes the proof.$\hfill\qed$


\subsubsection{Proof of \Cref{lemma: support recovery}}\label{section: proof to lemma support recovery}
The proof strategy we adopt here is similar with the proof of Lemma 3.1 in \cite{liang2022simple}. However, we consider a different generative model in this paper, so we present the full proof for the purpose of completeness. We will show that for $i\in S^*$, $|\langle \rvd_i,\rvx\rangle|>\gamma/2$ and for $i\not\in S^*$, $|\langle \rvd_i,\rvx\rangle|<\gamma/2$, which will immediately result in $S = \argmax_{T \subseteq [n], |T| \le k} \sum_{i \in T} |\langle \rvd_i, \rvx \rangle| = S^*$, which proves \Cref{lemma: support recovery}.

We decompose $\langle \rvd_i,\rvx\rangle$ as:
\begin{equation}\label{eq: cA_i cB_i decomposition}
    \begin{aligned}
        \langle \rvd_i,\rvx\rangle & = 
        \langle \rvd_i,\sum_{j\in S^*} \rvd^*_j \beta_j\rangle \\
        & = \underbrace{\langle \rvd_i,\rvd^*_i\rangle\beta_i}_{:=\cA_i} + \underbrace{\sum_{j\ne i, j\in S^*} \langle \rvd_i,\rvd^*_j\rangle\beta_j}_{:=\cB_i}
    \end{aligned}
\end{equation}
For $\cA_i$, we have:
\begin{align*}
    \cA_i = \langle \rvd_i,\rvd^*_i\rangle\beta_i
    = \langle \rvd^*_i,\rvd^*_i\rangle\beta_i + 
    \langle \rvd_i - \rvd^*_i ,\rvd^*_i\rangle\beta_i = \beta_i + \langle \rvd_i - \rvd^*_i ,\rvd^*_i\rangle\beta_i.
\end{align*}
When $i\in S^*$, we have
\begin{align*}
   | \beta_i + \langle \rvd_i - \rvd^*_i ,\rvd^*_i\rangle\beta_i|
    &\ge (1-\|\rvd_i - \rvd^*_i\|_2)|\beta_i|\\
    &\ge (1-\|\mD - \mD^*\|_{1,2})|\beta_i| \\
    &\ge (1-2\rho)|\beta_i|.
\end{align*}
Given that $2\rho<\frac{\gamma}{4\sqrt{k}\Gamma}\le\frac{1}{4}$, we can conclude
\begin{align}\label{eq: bound on cA_i}
    |\cA_i| \begin{cases}
        \ge \frac{3\gamma}{4} & \text{if}\quad i \in S^*\\
        =0 & \text{if}\quad i \not\in S^*
    \end{cases},
\end{align}
given that  $\beta_i = 0$ when $i\not\in S^*$. For $\cB_i$, we have:
\begin{equation}
    \label{eq: bound on cB_i}
    \begin{aligned}
        \left|\cB_i\right| &=\left|\sum_{j\ne i, j\in S^*} \langle \rvd_i,\rvd^*_j\rangle\beta_j\right| \\
        &= \left |\sum_{j\ne i, j\in S^*} \langle \rvd^{*}_i+(\rvd_i-\rvd^{*}_i),\rvd^*_j\rangle\beta_j\right|\\
        &= \left |\sum_{j\ne i, j\in S^*} \langle \rvd_i-\rvd^{*}_i,\rvd^*_j\rangle\beta_j\right|\\
        &\le \sum_{j\ne i, j\in S^*} \langle |\rvd_i-\rvd^{*}_i,\rvd^*_j\rangle||\beta_j|\\
        &\le  \Gamma \|\rvd_i-\rvd^{*}_i\|_1 \\
        &\le \sqrt{k}\Gamma \|\rvd_i-\rvd^{*}_i\|_2\\
        &\le 2\sqrt{k}\Gamma \rho\\
        &\le \gamma/4
    \end{aligned}
\end{equation}

Combining \Cref{eq: bound on cA_i} and \Cref{eq: bound on cB_i}, we have that for all $i$:
\begin{equation}\label{eq: put cA_i cB_i together}
    \begin{aligned}
    |\langle \rvd_i,\rvx\rangle| = |\cA_i+\cB_i|
    \begin{cases}
        > \frac{3\gamma}{4} - \frac{\gamma}{4}=\frac{\gamma}{2}& \text{if}\quad i \in S^*\\
        <\frac{\gamma}{4} & \text{if}\quad i \not\in S^*
    \end{cases},
\end{aligned}
\end{equation}
which proves the exact support recovery.

\subsubsection{Proof of \Cref{lemma: elementary position}}\label{section: proof to lemma elementary position}
It is easy to see that $\|\rvc-\rva\|_2\ge \|\rvb-\rva\|_2$ is equivalent to $|c-a|\ge |b-a|$. 

If $c>0$, by $bc>0$ we must have $b>0$. Then $|c|\ge|b|\ge a$ becomes $c\ge b \ge a$, which gives $|c-a|\ge |b-a|$.

If $c<0$, by $bc>0$ we must have $b<0$. Then $|c|\ge|b|\ge a$ becomes $a\ge b \ge c$, which gives $|c-a|\ge |b-a|$ as well.

\subsubsection{Proof of \Cref{lemma: align condition}}\label{section: proof to lemma align condition}
The proof of \Cref{lemma: align condition} consists of two steps:
\begin{enumerate}
    \item We first use classic concentration inequalities from covariance estimation literature to bound $\sigma_d$ and $\sigma_1$ given $|Q_i|$.
    \item Then we will show that when $m$ is large enough, with high probability, we have $|Q_i|$ bounded above and below.
\end{enumerate}
Combining these two steps and taking the union bound will complete the proof of \Cref{lemma: align condition}. 

For the fist step, we first notice that:
\begin{align*}
    \sigma_d\bigbracket{\sum_{h\in Q_i}\rvx^h\rvx^{h\top}} &= \sigma_d\bigbracket{\sum_{h\in Q_i}\mD^*\rvbeta^h\rvbeta^{h\top}\mD^{*\top}} \\
    &= \sigma_d\bigbracket{\sum_{h\in Q_i}\rvbeta^h\rvbeta^{h\top}}
\end{align*}
The last equation is due to the fact that $\mD^*$ is a full-rank orthogonal matrix. Consider the vector $\rvbeta^h$ for $h\in Q_i$. According to our generative model and the definition of $Q_i$, we have $\rvbeta^h_i\ne 0$, and its support outside $i$ is selected uniformly over all $(k-1)$-element subsets of $[n]\backslash\{i\}$. Each nonzero entry of $\rvbeta^h$ has a zero mean, variance of $\sigma^2$, and an absolute value bounded between $\gamma$ and $\Gamma$. Therefore, its covariance matrix defined as $\mSigma := \bE\left[\rvbeta^h{\rvbeta^h}^\top\right]$
is diagonal, with its $(i,i)$-th entry corresponding to $\sigma^2$, and its $(j,j)$-th entry (with $j\not=i$) corresponding to $\frac{(k-1)\sigma^2}{n}$. We claim that when $|Q_i|$ is sufficiently large, $\sum_{h\in Q_i}\rvx^h\rvx^{h\top}$ will concentrate around $|Q_i|\mSigma$. Specifically, we will use the following well-established result:
\begin{theorem}[\cite{vershynin2018high}]\label{theorem: covariance estimation}
 Let $\rvw$ be a zero-mean sub-Gaussian random vector in $\mathbb{R}^d$ with covariance matrix $\mSigma$, such that
\begin{equation}\nonumber
    \|\langle \rvw,\rvq \rangle\|_{\psi_2}\le K\bigbracket{\bE[\langle \rvw,\rvq \rangle^2]}^{1/2}\quad\text{for any }\rvq\in\mathbb{R}^d,
\end{equation}
for some $K\ge 1$. Here $\|\cdot\|_{\psi_2}$ denotes the sub-Gaussian norm of a random variable. Let $\mW\in\mathbb{R}^{d\times \whm}$ be a matrix whose columns have identical and independent distribution as $\rvw$. Then, for any $u\ge 0$ and with probability at least $1-2\exp{(-u)}$, we have
\begin{equation}\nonumber
    \left\|\frac{1}{\whm}\mW \mW^\top- \mSigma\right\|_2 \le CK^2\left(\sqrt{\frac{d+u}{\whm}}+\frac{d+u}{\whm}\right)\|\mSigma\|_2
\end{equation} 
for some universal constant $C$.
\end{theorem}
To invoke \Cref{theorem: covariance estimation}, we note that $\rvbeta^h$ is indeed zero-mean and sub-Gaussian. Therefore, upon setting $\rvw = \rvbeta^h$, we notice that for any unit-norm $\rvq$:
\begin{align*}
    \|\langle \rvbeta^h,\rvq \rangle\|^2_{\psi_2} \le \sum_{j=1}^d \rvq_j^2\|\rvbeta^h_j\|^2_{\psi_2} \le C_0 \sigma^2,
\end{align*}
for some universal constant $C_0$. We also have 
\begin{align*}
    \bE\left[\left\langle \rvbeta^h,\rvq \right\rangle^2\right] = \rvq^\top\Sigma\rvq\ge \frac{(k-1)\sigma^2}{n}.
\end{align*}
By setting $K^2 = \frac{C_0 n}{k-1}$, we can invoke \Cref{theorem: covariance estimation} with $u=n$ and conclude that, with probability at least $1-2\exp\{-n\}$:
\begin{align}
    \sigma_d\bigbracket{\sum_{h\in Q_i}\rvbeta^h\rvbeta^{h\top}}
    &\ge |Q_i|\sigma_d\bigbracket{\mSigma} - \frac{Cn\sigma}{k-1}\sqrt{2n|Q_i|} = \frac{|Q_i|(k-1)\sigma^2}{n} - \frac{Cn\sigma}{k-1}\sqrt{2n|Q_i|},\label{eq: sig d lower bound with Q_i}\\
    \sigma_1\bigbracket{\sum_{h\in Q_i}\rvbeta^h\rvbeta^{h\top}}
    &\le |Q_i|\sigma_1\bigbracket{\mSigma} + \frac{Cn\sigma}{k-1}\sqrt{2n|Q_i|} = |Q_i|\sigma^2 + \frac{Cn\sigma}{k-1}\sqrt{2n|Q_i|}, \label{eq: sig 1 upper bound with Q_i}
\end{align}
for some universal constant $C$, which concludes the first step. Here we recall that $d=n$ in the full-rank and orthogonal setting.

For the second step, consider one specific $i$ from $[n]$. The random variable $|Q_i|$ follows a binomial distribution with $m$ trials and $\frac{k}{n}$ success rate. We next recall the Chernoff bound:
\begin{theorem}[\cite{motwani1996randomized}]\label{theorem: chernoff}
Let $X_1, X_2, \ldots, X_N$ be independent Bernoulli random variables with
$\bP(X_i = 1) = p_i$, and let
\[
   X \;=\; \sum_{i=1}^N X_i
   \quad \text{and} \quad
   \mu \;=\; \mathbb{E}[X] \;=\; \sum_{i=1}^N p_i.
\]
Then for any $0 < \delta < 1$, we have
\[
   \bP\bigl(X \le (1-\delta)\,\mu\bigr)
   \;\;\le\;\;
   \exp\bigl\{-\tfrac{\delta^2\,\mu}{2}\bigr\},
\]
and
\[
   \bP \bigl(X \ge (1+\delta)\,\mu\bigr)
   \;\;\le\;\;
   \exp\bigl\{-\tfrac{\delta^2\,\mu}{2}\bigr\}.
\]
\end{theorem}
We can then invoke \Cref{theorem: chernoff} with $\delta = \frac{1}{2}$ to obtain 
\begin{align}\label{eq: Q_i bound}
    \frac{km}{2n} \le |Q_i| \le \frac{2km}{n}, 
\end{align}
with probability at least $1-2\exp\left\{-\frac{km}{8n}\right\}$. Finally, by combining \Cref{eq: sig d lower bound with Q_i} and \Cref{eq: Q_i bound}, we have
\begin{align}
    \frac{1}{m}\sigma_d\bigbracket{\sum_{h\in Q_i}\rvbeta^h\rvbeta^{h\top}}
    &\ge \frac{|Q_i|(k-1)\sigma^2}{n m} - \frac{Cn\sigma}{(k-1)m}\sqrt{2n|Q_i|} \nonumber\\
    &\ge \frac{k(k-1)\sigma^2}{2n^2} - \frac{Cn\sigma}{(k-1)}\sqrt{\frac{4k}{m}} \label{eq: sig d last step}.
\end{align}
When $m\ge \frac{64C^2n^6}{\sigma^2k(k-1)^4}$, we have 
\begin{align*}
    \frac{Cn\sigma}{(k-1)}\sqrt{\frac{4k}{m}} \le \frac{k(k-1)\sigma^2}{4n^2},
\end{align*}
which reduces \Cref{eq: sig d last step} to:
\begin{align*}
    \frac{1}{m}\sigma_d\bigbracket{\sum_{h\in Q_i}\rvbeta^h\rvbeta^{h\top}} \ge \frac{k(k-1)\sigma^2}{2n^2} - \frac{k(k-1)\sigma^2}{4n^2} = \frac{k(k-1)\sigma^2}{4n^2}.
\end{align*}
With a similar argument, we can combine \Cref{eq: sig 1 upper bound with Q_i} and \Cref{eq: Q_i bound} to get:
\begin{align*}
    \frac{1}{m}\sigma_1\bigbracket{\sum_{h\in Q_i}\rvbeta^h\rvbeta^{h\top}}\le |Q_i|\sigma^2 + \frac{Cn\sigma}{k-1}\sqrt{2n|Q_i|} \le \frac{4k\sigma^2}{n}.
\end{align*}
Finally, we take the union bound for all $i \in [n]$, leading to the overall probability of $1-2n\exp\{-n\}-2n\exp\left\{-\frac{km}{8n}\right\}$. This completes the proof. $\hfill\qed$

\section{Detailed Experiments}
\subsection{Experiments on Generative Model}\label{section: detailed synthetic}
All experiments reported in this section were performed in Python~3.9 on a MacBook~Pro (14-inch,~2021) equipped with an Apple~M1~Pro chip.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/synthetic_plot.pdf}
    \caption{Results on synthetic dataset.}
    \label{fig: synthetic plot}
\end{figure}

We generate samples according to the described generative model in \Cref{assumption: probabilistic generative model of x}, 
we construct the input data $\rvx$ or $\{\rvx^h\}_{h=1}^m$ by sampling each non-zero entry from a uniform distribution over the interval $[\gamma,\Gamma]$. The matrix $\mD^*$ is chosen to be a randomly generated orthonormal matrix, and we construct $\mD_{\mathrm{init}}$ by setting $\mD_{\mathrm{init}} = \mD^* + \mE$, where each column of $\mE$ is uniformly drawn from the set $\{\rve\mid\|\rve\|_2\le\rho\}$. For the results shown in \Cref{fig: synthetic plot}, we set $d=10$, $k=5$, $\rho=0.2$, $\gamma=0.5$, and $\Gamma=1$. The first column of \Cref{fig: synthetic plot} illustrates the scenario corresponding to \Cref{theorem: vanishing L convergence}, in which only a single input feature $\rvx$ is available. Here, we choose $n=8$ and $\eta=10^{-2}$. The plot showing $\cL(\tf_{\mD^{(t)}})$ confirms the linear convergence of the loss function to zero, in agreement with \Cref{theorem: vanishing L convergence}. However, as previously noted, this setting does not ensure exact recovery for each queried feature. Indeed, as shown in the right panel of the first row, $\|\mD^{(t)}-\mD^*\|_{1,2}$ does not converge to zero. Even if we only consider columns that are activated at each iteration, $\|\mD^{(t)}_S-\mD^*_S\|_{1,2}$ also remains nonzero, indicating that optimizing $\cL$ with a single $\rvx$ is insufficient to recover the ground-truth dictionary $\mD^*$.

In the second column of \Cref{fig: synthetic plot}, we apply projected gradient descent to minimize $\cL_m$, as defined in \Cref{eq::Lm}, under the assumption that $\mD^*$ is rank-deficient. Specifically, we set $n=8 < d$ and $\eta=10^{-1}$. Evidently, $\|\mD^{(t)}-\mD^*\|_{1,2}$ does not converge to zero because the component of $\mD^{(t)}-\mD^*$ orthogonal to the column space of $\mD^*$ remains unaffected throughout the iterations.
In contrast, the third column of \Cref{fig: synthetic plot} corresponds to the setting $n=d$, which guarantees the full-rankness of $\mD^*$ is full rank. In this case, $\|\mD^{(t)}_S-\mD^*_S\|_{1,2}$ converges to zero, thus supporting the conclusion of \Cref{theorem: columnwise convergence}.


\subsection{More Details on \Cref{alg: constrained concept update for image classification}}\label{section: detailed real}
Here we provide a detailed description for the \textbf{concept dispersion} step and the \textbf{embedding normalization and projection} step in \Cref{alg: constrained concept update for image classification}. In \Cref{alg: atom dispersion}, we present the pseudo-code for concept dispersion. The algorithm first calculates the mean concept embedding for the given $\{\rvd_i\}_{i=1}^n$ and calculates the angle between each $\rvd$ and the mean. Then, \Cref{alg: atom dispersion} increases these angles by a constant factor $r$.
\begin{algorithm}
   \caption{Concept dispersion}\label{alg: atom dispersion}
   \begin{algorithmic}[1]
      \STATE \textbf{Input}: $\{\rvd_i\}_{i=1}^n$, dispersion factor $r$.
      \STATE Calculate the mean concept embedding $\bar{\rvd} = \sum_{i=1}^n\rvd_i/\|\sum_{i=1}^n\bd_i\|_2$.
      \FOR{each $\bd_i$}
         \STATE Decompose $\bd_i$ as $\bd_i = \cos \alpha_i \,\bar{\bd} + \sin \alpha_i \,\rve_i$, where $\alpha_i\in [0,\pi/2]$, $\|\rve_i\|_2 = 1$, and $\rve_i \perp \bar{\bd}$.
         \STATE Calculate the dispersed query feature $\bd^{\mathrm{new}}_i = \cos (r\alpha_i)\,\bar{\bd} + \sin (r\alpha_i)\,\rve_i$.
         \STATE Add $\bd^{\mathrm{new}}_i$ as the $i$th column of $\mD^{\mathrm{init}}$.
      \ENDFOR
      \STATE \textbf{Return} $\mD^{\mathrm{init}}$.
   \end{algorithmic}
\end{algorithm}

\Cref{alg: atom projection} presents the pseudo-code for the projection and normalization step in \Cref{alg: atom projection}.
\begin{algorithm}[H]
   \caption{Embedding normalization and projection}\label{alg: atom projection}
   \begin{algorithmic}[1]
      \STATE \textbf{Input}: current query feature matrix $\mD$, initial query feature matrix $\mD^{\mathrm{init}}$, and radius $\rho$.
      \FOR{each query feature $\bd_i$ in $\mD$}
         \STATE Normalize $\bd_i$ as $\bd_i = \bd_i / \|\bd_i \|_2$.
         \IF{$\|\bd_i-\bd^{\mathrm{init}}_i\|_2\ge \rho$}
            \STATE $\bd_i = \arg\min_{\bd: \|\bd-\bd^{\mathrm{init}}_i\|_2\leq \rho,\|\bd\|_2 = 1} \|\bd - \bd_i\|_2$.
         \ENDIF
      \ENDFOR
      \STATE \textbf{Return} $\mD$.
   \end{algorithmic}
\end{algorithm}

\subsection{More Experiments on Interpretability}\label{sec: more interpretability}
In this section, we expand on the discussion of CCR's interpretability from \Cref{section: interpretability} by conducting a comprehensive case study across all five datasets used to evaluate CCR. For each dataset, we select three representative image-label pairs and present the ten highest-ranking concept scores on the left, alongside their corresponding weights associated with the predicted label on the right. Based on our case study, we derive several key observations:

\begin{itemize}
    \item \textbf{Semantic correlation.} In most cases, concepts with higher concept scores exhibit a strong semantic correlation with the input images. This finding substantiates the reliability of our algorithm as an interpretable AI model. A notable example is illustrated in \Cref{fig:200-b} and \Cref{fig:200-c}, where the algorithm effectively identifies the primary distinguishing features between two visually similar bird species—their coloration—leading to an accurate classification.
    
    \item \textbf{Weight distribution.} In instances where misleading elements are present in an image, certain concepts unrelated to the ground truth label may receive high concept scores. Nevertheless, the linear layer appropriately assigns a small weight to such concepts. These weights encapsulate the algorithm’s interpretation of the label, thereby rendering it comprehensible to human users. Examples of such cases include ``a beak" in \Cref{fig:10-b}, ``a plow" in \Cref{fig:100-a}, ``a seagull"/``a bird" in \Cref{fig:100-c}, ``a highlighter" in \Cref{fig:i-a}, ``iridescent" in \Cref{fig:i-b}, and ``abandoned buildings"/``a sunset" in \Cref{fig:365-a}.
    
    \item \textbf{Debugging capability for incorrect predictions.} Inevitably, our framework makes incorrect predictions for some samples. A key application of explainable AI is to help human experts understand the reasoning behind these errors. Our findings indicate that the proposed algorithm effectively captures interpretable misconceptions introduced by encoders. For instance, in \Cref{fig:365-b}, concepts such as ``a display case" and ``exhibits" suggest that the primary misleading factor causing the model to predict ``natural history museum" instead of the correct label ``shopping mall" is the manner in which the store displays its goods. This insight is valuable for human intervention and model correction.
    
    \item \textbf{Reliance on the richness of the concept set.} The effectiveness of the algorithm is inherently influenced by the quality of the concept set, a phenomenon that aligns with the fundamental motivation behind explainable AI. As demonstrated in \Cref{fig:365-c}, the concept set lacks distinctive features that differentiate between ``bus interior" and ``train interior". Consequently, the model is unable to distinguish between these two labels.
\end{itemize}







\begin{figure}[htbp]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.75\textwidth]{figs/interpretability/car.png}
        \caption{CIFAR-10 (a)}
        \label{fig:10-a}
    \end{subfigure}
    
    \begin{subfigure}
        \centering
        \includegraphics[width=0.75\textwidth]{figs/interpretability/plane.png}
        \caption{CIFAR-10 (b)}
        \label{fig:10-b}
    \end{subfigure}
    
    \begin{subfigure}
        \centering
        \includegraphics[width=0.75\textwidth]{figs/interpretability/ship.png}
        \caption{CIFAR-10 (c)}
        \label{fig:10-c}
    \end{subfigure}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.75\textwidth]{figs/interpretability/pickup.png}
        \caption{CIFAR-100 (a)}
        \label{fig:100-a}
    \end{subfigure}
    
    \begin{subfigure}
        \centering
        \includegraphics[width=0.75\textwidth]{figs/interpretability/couch.png}
        \caption{CIFAR-100 (b)}
        \label{fig:100-b}
    \end{subfigure}
    
    \begin{subfigure}
        \centering
        \includegraphics[width=0.75\textwidth]{figs/interpretability/shark.png}
        \caption{CIFAR-100 (c)}
        \label{fig:100-c}
    \end{subfigure}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.75\textwidth]{figs/interpretability/cowbird.png}
        \caption{CUB-200 (a)}
        \label{fig:200-a}
    \end{subfigure}
    
    \begin{subfigure}
        \centering
        \includegraphics[width=0.75\textwidth]{figs/interpretability/alba.png}
        \caption{CUB-200 (b)}
        \label{fig:200-b}
    \end{subfigure}
    
    \begin{subfigure}
        \centering
        \includegraphics[width=0.75\textwidth]{figs/interpretability/white_alba.png}
        \caption{CUB-200 (c)}
        \label{fig:200-c}
    \end{subfigure}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.75\textwidth]{figs/interpretability/chame.png}
        \caption{ImageNet (a)}
        \label{fig:i-a}
    \end{subfigure}
    
    \begin{subfigure}
        \centering
        \includegraphics[width=0.75\textwidth]{figs/interpretability/goldfish.png}
        \caption{ImageNet (b)}
        \label{fig:i-b}
    \end{subfigure}
    
    \begin{subfigure}
        \centering
        \includegraphics[width=0.75\textwidth]{figs/interpretability/tissue.png}
        \caption{ImageNet (c)}
        \label{fig:i-c}
    \end{subfigure}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.75\textwidth]{figs/interpretability/campus.png}
        \caption{Places365 (a)}
        \label{fig:365-a}
    \end{subfigure}
    
    \begin{subfigure}
        \centering
        \includegraphics[width=0.75\textwidth]{figs/interpretability/shop.png}
        \caption{Places365 (b)}
        \label{fig:365-b}
    \end{subfigure}
    
    \begin{subfigure}
        \centering
        \includegraphics[width=0.75\textwidth]{figs/interpretability/bus.png}
        \caption{Places365 (c)}
        \label{fig:365-c}
    \end{subfigure}
\end{figure}

\end{document}