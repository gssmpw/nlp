\begin{figure*}[!t]
\center
\begin{minipage}[t]{.45\linewidth}
\textbf{Full-matrix (Original)}
\begin{algorithmic}[1]
    \STATE
    \footnotesize  Compute gradient $\vg\coloneqq\nabla \ell(\vmu)$  \\
     \scalebox{0.8}{
     $
    {\vS}  \leftarrow (1-\gamma \stepsize_2)\vS + \stepsize_2  (\vg\vg^T+ \lambda \vI )
     $
     } \\
     \scalebox{0.72}{   $
     \color{blue}\vB \mathrm{Diag}(\vd) \vB^T = \mathrm{Eigen}(\vS)$
     }
    \STATE
  \scalebox{0.8}{    $
  \vmu \leftarrow \vmu  - \stepsize_1 \vB \mathrm{Diag}(\vd^{-1/p})\vB^T \vg
$}
            %
        \end{algorithmic}
\end{minipage}
    \begin{minipage}[t]{.48\linewidth}
\textbf{Ours \scalebox{0.8}{   ($\vS = \vB\mathrm{Diag}(\vd)\vB^T$)} }
\begin{algorithmic}[1]
%
    \STATE
    \footnotesize  Compute gradient $\vg\coloneqq\nabla \ell(\vmu)$  \\
     \scalebox{0.8}{
     $ {\vd}  \leftarrow (1-\gamma\stepsize_2)\vd + \stepsize_2 \big( \mathrm{diag}(\vB^T  \vg\vg^T  \vB ) + \lambda \big)  $
     } \\
     \scalebox{0.72}{   $ 
     \vB \leftarrow \vB\mathrm{Cayley}(\frac{\stepsize_2}{2}( \mathrm{Skew}({\mathrm{Tril}}(\vU))) )
     $
     }
    \STATE
  \scalebox{0.8}{    $
  \vmu \leftarrow \vmu  - \stepsize_1 \vB \mathrm{Diag}(\vd^{-1/p})\vB^T \vg
$}
            %
        \end{algorithmic}
\end{minipage}
 \vspace{-0.1cm}   \caption{
Comparison between the original full-matrix update scheme (e.g., full-matrix RMSprop when $\gamma=1$) and our update scheme when the exponential map is truncated.  We can see that our update scheme is a decomposition-free version of the adaptive method. Here, we can use the first-order truncation of the exponential map (see the top box of  Fig.~\ref{fig:kronecker} for the map).
This is possible because we leverage the positive semi-definiteness of the GOP. Thanks to the 
positive semi-definitness, the update of $\vd$ is always non-negative.
In practice, we introduce a damping term $\lambda$ so that $\vd$ is always positive.
}\label{fig:full_updates}
\end{figure*}

\begin{figure*}[!t]
\center
\begin{minipage}[t]{.49\linewidth}
\textbf{ 
Kronecker-based 
\citep{lincan2024}} 
        \begin{algorithmic}[1]
            %
    \STATE
    \footnotesize  Compute gradient $\vG\coloneqq \mathrm{Mat}(\nabla \ell(\vmu))$  \\
     \scalebox{0.8}{ $             \vS^{(l)} = (1-\gamma\stepsize_2){\vS}^{(l)}  +\frac{\stepsize_2}{k^{(l)}} \big(\vQ^{(l)} + \Lambda^{(l)}\vI\big)$ } \\
     \scalebox{0.8}{  \color{blue} $ \vB^{(l)} \mathrm{Diag}(\vd^{(l)}) (\vB^{(l)})^T = \mathrm{Eigen}(\vS^{(l)}) $ }  \\
     \scalebox{0.8}{
    $
    \big( \vS^{(l)}\big)^{-1/p} = \vB^{(l)} \mathrm{Diag}\big(( \vd^{(l)})^{-1/p} \big)  (\vB^{(l)})^T
    $ 
     }\\
    \STATE
  \scalebox{0.8}{    $
  \mathrm{Mat}(\vmu) \leftarrow \mathrm{Mat}(\vmu) - \stepsize_1 ({\vS}^{(C)})^{-1/p}\vG ({\vS}^{(K)})^{-1/p}
$}
            %
        \end{algorithmic}
\end{minipage}
    \begin{minipage}[t]{.48\linewidth}
\textbf{Ours (\scalebox{0.6}{ $\vS = {\alpha } [\vB^{(C)} \mathrm{Diag}(\vd^{(C)}) (\vB^{(C)})^T] \otimes [\vB^{(K)} \mathrm{Diag}(\vd^{(K)}) (\vB^{(K)})^T]$
})}

\begin{algorithmic}[1]
    \STATE
    \footnotesize  Compute gradient $\vG\coloneqq \mathrm{Mat}(\nabla \ell(\vmu))$  \\
     \scalebox{0.8}{ $             \vn^{(l)} = (1-\gamma\stepsize_2)\vd^{(l)} + \frac{\stepsize_2}{ {\alpha} k^{(l)}}\big(   \mathrm{diag}((\vB^{(l)})^T \vQ^{(l)} \vB^{(l)}) + \Lambda^{(l)}\big)$ } \\
     \scalebox{0.8}{   $ \vB^{(l)} \leftarrow \vB^{(l)} \mathrm{Cayley}(\frac{\stepsize_2}{2 {\alpha} k^{(l)} }\mathrm{Skew}( \mathrm{Tril}(\vU^{(l)})  ) ) $ } \\
     \scalebox{0.8}{   $ \vd^{(l)}  \leftarrow  \exp\big[\log(\vn^{(l)}) {\color{red} - \mathrm{mean}(\log(\vn^{(l)}))
     }\big] $} \\
     \scalebox{0.8}{   $\color{red}     \alpha \leftarrow \alpha  \exp\big[ \mathrm{mean}(\log(\vn^{(C)}))/2 + \mathrm{mean}(\log(\vn^{(K)}))/2\big]    $ } \\
    \STATE
  \scalebox{0.8}{    $
  \mathrm{Mat}(\vmu) \leftarrow \mathrm{Mat}(\vmu)  - \stepsize_1 \big({ \alpha^{-1/p}} \big)({\vS}^{(C)})^{-1/p}\vG ({\vS}^{(K)})^{-1/p}
$}
            %
        \end{algorithmic}
\end{minipage}
 \vspace{-0.1cm} \caption{
 Comparison between the Kronecker-based update scheme \citep{lincan2024} with $p=1$ and our update scheme when the exponential map is truncated.  
 We define  
\scalebox{0.8}{ $\vQ^{(C)}:=\vG ({\vS}^{(K)})^{-1}\vG^T$,
 $\vQ^{(K)}:=\vG^T ({\vS}^{(C)})^{-1}\vG$}, and \scalebox{0.8}{ $\vW^{(l)}=(\vB^{(l)})^T \vQ^{(l)} \vB^{(l)}$}, 
 where 
 \scalebox{0.8}{
$\vS^{(l)}:=\vB^{(l)}\mathrm{Diag}(\vd^{(l)}) \big(\vB^{(l)}\big)^T$}.
 Unlike \citet{lincan2024}, we consider a unique representation of preconditioner $\vS$ by introducing a scalar $\alpha$ and a determinant constraint on $\vd^{(l)}$.
 The lines highlighted in red are used to ensure the constraints.
 We can use the first-order truncation of the exponential map (see the bottom box of  Fig.~\ref{fig:kronecker} for the map) because we exploit the positive semi-definiteness of the curvature information $\vQ$ used in \citet{lincan2024}.
 Thus, $\vn^{(l)}$ is non-negative even when using the truncation.
 To make $\vn^{(l)}$ positive,  we use an adaptive damping term, \scalebox{0.8}{
  $\Lambda^{(l)} := \frac{\lambda \mathrm{Tr}\big((\vS^{(C)})^{-1}\big)\mathrm{Tr}\big((\vS^{(K)})^{-1}\big)}{\mathrm{Tr}\big((\vS^{(l)})^{-1}\big)} =
  \frac{\lambda k^{(l)} \mathrm{mean}\big((\vd^{(C)})^{-1}\big)\mathrm{mean}\big((\vd^{(K)})^{-1}\big)}{\mathrm{mean}\big((\vd^{(l)})^{-1}\big)}
 $}, as suggested by \citet{lincan2024}.
We can even use a truncated Cayley map discussed in Sec.~\ref{sec:practical_NN} for low-precision training. In this case, we update $\vn^{(l)}$ using a common $\stepsize_2$ while updating $\vB^{(l)}$ using a block-specific  
nonconstant $\stepsize_2^{(l)}:=\frac{ \bar{ \stepsize}_2 \alpha \kappa^{(l)} }{ \| 
  \mathrm{Skew}( \mathrm{Tril}(\vU^{(l)})  ) 
 \|_{\text{Frob}} }$ required by the truncation, where scalar $\bar{
\stepsize_2}$ remains constant for all blocks $\vB^{(l)}$.
Similar to \citet{vyas2024soap} and \citet{lincan2024}, our update scheme only introduces one additional scalar hyperparameter and supports momentum and weight decay.
}\label{fig:kron_updates}
\end{figure*}

%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%


%



\begin{comment}
  \begin{claim}
    \label{claim:eig_preconditioner_invarinace}
    Our NGD update scheme is invariant to
    permutations on
    Eigen parameterizations for preconditioning matrices.
  \end{claim}

  \begin{claim}
    \label{claim:eig_weight_invarinace}
    Our NGD update scheme is invariant to
    permutations on weights $\vmu$ when $p=1$.
  \end{claim}

  \begin{claim}
    \label{claim:eig_scaling_invarinace}
    Our NGD update scheme is invariant to scaling a loss when $p=1,2$.
  \end{claim}
\end{comment}


\begin{comment}
\subsection{ Spectral Coordinates for Kronecker Structures
}
\label{sec:kron_invar}
\vspace{-0.2cm}
Now, we propose spectral parametrizations and local coordinates for Kronecker structured matrices. 
Kronecker factorization introduces an additional redundancy that makes
the exact Fisher-Rao metric singular and non-block-diagonal in Kronecker structured coordinate. We construct local coordinates to remove this redundancy and simplify the inverse metric computation.  Furthermore, removing this redundancy makes our update scheme invariant to all equivalent  Kronecker factorizations.  

%
%
%
%
%
%
%
%
%
%


%
%


%
%
%
%
%

%
{\bf
Procedure 
for Kronecker-structured   Spectral Parametrizations
}
%
We use a similar procedure to obtain our update schemes in a structured case. We briefly describe the procedure and highlight the difference in this case. 

{\bf
Procedure 
for Kronecker-structured   Spectral Parametrizations
}
%
We use a similar procedure to obtain our update schemes in a structured case. We briefly describe the procedure and highlight the difference in this case. 

{\bf Step 1}\, We solve a Gaussian problem similar to \eqref{eq:ref_opt} using a Kronecker factorized  eigenparametrization, \scalebox{0.8}{ $\vS=\alpha [\vB^{(C)}\mathrm{Diag}(\vd^{(C)})\big(\vB^{(C)}\big)^T] \otimes [\vB^{(K)}\mathrm{Diag}(\vd^{(K)})\big(\vB^{(K)}\big)^T]$}, for the inverse covariance with extra constraints:\scalebox{0.8}{ $\mathrm{det}(\mathrm{Diag}(\vd^{(C)}))=\mathrm{det}(\mathrm{Diag}(\vd^{(K)}))=1$} and $\alpha>0$.

{\bf Step 2}\, We construct a local coordinate at each iteration, perform RGD in the local coordinate, and translate the change using a transformation map in Eq.~\eqref{eq:local_eig_kron}. See Eq.~\eqref{eq:kron_trans_map} in the appendix for details.


{\bf Step 3}\, We obtain the root-free update scheme in Fig.~\ref{fig:kronecker} by   simplifying this RGD step.
The simplification is straightforward because the metric evaluated at the origin in the local coordinate is block diagonal (c.f., Claim~\ref{claim:fisher_kron}).
\end{comment}


\section{
Leveraging 
Positive Semi-definiteness 
for Simplification via Truncating the Exponential Map
}
\label{app:connections}

Our update scheme supports generic curvature information to estimate a spectral-factored SPD matrix $\vS$. When the curvature information is 
positive semi-definite, we can further simplify our scheme by using a truncated exponential map.
We consider the following cases for GOP-based curvature information.

\subsection{Diagonal Case}
\label{app:connection_diag}
Here, we connect our scheme with  $\gamma=1$ to the RMSprop method.
Similarly, we can connect our scheme to AdaGrad by setting $\gamma=0$.
Observe that eigenvalues are diagonal entries of a diagonal preconditioning matrix (i.e., $\vd=\mathrm{diag}(\vS)$).
Because $\vB$ is now a diagonal and orthogonal matrix, its diagonal entries can only be 1 or -1.
Using this result, we can further simplify our update scheme with $p=2$ and recover the RMSprop update rule when using a first-order truncation of the exponential map.
\begin{align}
  \vd & \leftarrow  \vd \odot \exp\{ \stepsize_2\, \vd^{-1} \odot [ -\gamma \vd +    \mathrm{diag}(\vB^T  \vg\vg^T \vB )  ]  \} \approx \vd + \stepsize_2 [ -\gamma \vd +    \mathrm{diag}( \vB^T \vg\vg^T \vB )  ]   = (1-\stepsize_2\gamma) \vd + \stepsize_2 \mathrm{diag}( \vg\vg^T )   \nonumber \\
  \vB & \leftarrow \vB\mathrm{Cayley}(\frac{\stepsize_2}{2}\, \mathrm{Skew}({ \color{red}\mathrm{Diag}(\vU)} ) ) = \vB \mathrm{Cayley}(\mathbf{0}) =\vB \nonumber \\
  \vmu &\leftarrow \vmu  - \stepsize_1\, \vB \mathrm{Diag}(\vd^{-\nicefrac{1}{p}})\vB^T \vg =
         \vmu  - \stepsize_1\, \mathrm{Diag}(\vd^{-\nicefrac{1}{p}}) \vg
         \label{eq:root_free_diag},
\end{align} where $\mathrm{diag}(\vB^T  \vg\vg^T \vB )=\mathrm{diag}(  \vg\vg^T )$, $\vB \mathrm{Diag}(\vd^{-\nicefrac{1}{p}})\vB^T=\mathrm{Diag}(\vd^{-\nicefrac{1}{p}})$, and we use a first-order truncation of the exponential map $\vd \odot \exp(\vd^{-1} \odot \vn) \approx \vd \odot (\mathbf{1} + \vd^{-1} \odot \vn)= \vd + \vn$.
%
Due to the skew-symmetrization, $\mathrm{Skew}({ \mathrm{Diag}(\vU)} )$ is always a zero matrix. Thus, according to our update scheme, $\vB$ remains unchanged in diagonal cases because $\mathrm{Cayley}(\mathbf{0})=\vI$.
%


%
%
%
%

\subsection{Full-matrix Case}
We consider a truncated exponential map to bring our update schemes closer to the original ones.
Similar to the diagonal case, we use the first-order truncation of the exponential  map and obtain the update in
the rightmost box of Fig.~\ref{fig:full_updates}.
From Fig.~\ref{fig:full_updates}, we can observe the similarity between the original scheme  and ours.

\subsection{Kronecker-based Case}
We can further simplify our update scheme in Fig.~\ref{fig:kronecker} when truncating the exponential map.
To see that, we give Claims \ref{claim:approx1} and \ref{claim:approx2}.
According to Claim~\ref{claim:approx2},  our update scheme in Fig.~\ref{fig:kronecker} can be merged and reexpressed as
\begin{align*}
\alpha^2 \mathrm{Diag}(\vd^{(C)}) \otimes \mathrm{Diag}(\vd^{(K)}) &= \mathrm{Diag}(\alpha \vd^{(C)}) \otimes \mathrm{Diag}(\alpha \vd^{(K)}) \\
& \approx  ( \alpha \exp \big[\mathrm{mean}( \log(\vn^{(C)}) )/2 + \mathrm{mean}( \log(\vn^{(K)}) )/2 \big]  )^2 
\mathrm{Diag}( \exp \big(\vv^{(C)}\big)) \otimes 
\mathrm{Diag}(
\exp\big(\vv^{(K)}\big)),
\end{align*} where $\vv^{(l)}:=\log(\vn^{(l)}) - \mathrm{mean}( \log(\vn^{(l)}) )$ and
$\vn^{(l)}:=
(1-\gamma \stepsize_2)\vd^{(l)} +  \frac{\stepsize_2}{\alpha k^{(l)}} \mathrm{diag}\big(\vW^{(l)} \big)
$.
We then split the above   update into each individual factor  as 
\begin{align*}
   \vd^{(l)} & \leftarrow \exp(\vv^{(l)}) = \exp( \log(\vn^{(l)}) - \mathrm{mean}(\log(\vn^{(l)})) ) \\
   \alpha & \leftarrow \alpha \exp(
   \mathrm{mean}(\log(\vn^{(C)}))/2 +
   \mathrm{mean}(\log(\vn^{(K)}))/2
   ).
\end{align*} 
This simplified update scheme is summarized in the rightmost box of  Fig.~\ref{fig:kron_updates}.

\begin{claim}
\label{claim:approx1}
In our Kronecker-based update 
scheme (see Fig.~\eqref{fig:kronecker}),
we have the following identity for each factor $l \in \{C,K\}$. 
\begin{align*}
 \frac{1}{\alpha k^{(l)}}   \mathrm{mean}\big( (\vd^{(l)})^{-1} \odot  \mathrm{diag}(\vW^{(l)})\big) = \frac{1}{m n\alpha} \mathrm{Tr}\big[ (\vS^{(C)})^{-1}\vG  (\vS^{(K)})^{-1}\vG^T \big],
\end{align*} where  $\vS^{(l)}:=\vB^{(l)}\mathrm{Diag}(\vd^{(l)})\big(\vB^{(l)}\big)^T$, $k^{(C)}:=m$, $k^{(K)}:=n$, $\vS^{(C)} \in \mathcal{R}^{ n \times n }$,
and $\vS^{(K)} \in \mathcal{R}^{ m \times m }$.
\end{claim}

\begin{proof}
We will prove the case when $l=C$. Likewise, we can prove the case when $l=K$.
Recall that $\vW^{(C)}$ is defined as $\vW^{(C)}:=(\vB^{(C)})^T \vQ^{(C)} \vB^{(C)}= (\vB^{(C)})^T\vG (\vS^{(K)})^{-1}\vG^T (\vB^{(C)})$. We have the following expression.
\begin{align*}
    \frac{1}{\alpha k^{(l)}}\mathrm{mean}\big[ ( \vd^{(C)}  )^{-1} \odot \mathrm{diag}(\vW^{(C)}) \big] & =  \frac{1}{mn\alpha}\mathrm{Tr}\big[\mathrm{Diag}(\vd^{(C)})^{-1}  \vW^{(C)}\big] \\
    &= \frac{1}{mn\alpha}\mathrm{Tr}\big[
    \mathrm{Diag}(\vd^{(C)})^{-1}
    (\vB^{(C)})^T\vG (\vS^{(K)})^{-1}\vG^T \vB^{(C)}
    \big] \\
    &= \frac{1}{mn\alpha}\mathrm{Tr}\big[
 \underbrace{    \vB^{(C)}  \mathrm{Diag}(\vd^{(C)})^{-1} (\vB^{(C)})^T}_{= ( \vS^{(C)})^{-1} } \vG (\vS^{(K)})^{-1}\vG^T
    \big] 
\end{align*}

\end{proof}

\begin{claim}
\label{claim:approx2}
In our Kronecker-based update 
scheme (see Fig.~\eqref{fig:kronecker}), the updates  of $\alpha$ and $\vd^{(l)}$ can be merged and approximated as 
\begin{align*}
 \alpha \vd^{(l)} \leftarrow
(\alpha \vd^{(l)})\odot \exp(\stepsize_2 \vm^{(l)} ) & \approx \alpha \overbrace{\big[ (1-\gamma \stepsize_2)\vd^{(l)} +  \frac{\stepsize_2}{\alpha k^{(l)}} \mathrm{diag}\big(\vW^{(l)} \big) \big]}^{:= \vn^{(l)}} \\
& =  \alpha \exp\big[\mathrm{mean}( \log(\vn^{(l)}) )\big] \exp\big[\log(\vn^{(l)}) - \mathrm{mean}( \log(\vn^{(l)}) ) \big]
\end{align*} where $l \in \{C,K\}$.
\end{claim}
\begin{proof}
   According to the update scheme in Fig.~\ref{fig:kronecker} and Claim \ref{claim:approx1}, we can reexpress the update in $\alpha$ as
 \begin{align*}
\alpha \leftarrow \alpha \exp( \frac{\stepsize_2}{2 } [\mathrm{mean}(\vm^{(C)}) + \mathrm{mean}(\vm^{(K)})] ) & = \alpha \exp( \frac{\stepsize_2}{2 } [\mathrm{mean}(\vm^{(C)}) + \mathrm{mean}(\vm^{(K)})] ) \\
&= \alpha\exp\big[\frac{\stepsize_2}{2 } \big(-2\gamma + \frac{2}{\alpha mn} \mathrm{Tr}( (\vS^{(C)})^{-1}\vG  (\vS^{(K)})^{-1}\vG^T ) \big)\big] \\
&= \alpha\exp\big[\stepsize_2 \big(-\gamma + \frac{1}{\alpha mn} \mathrm{Tr}( (\vS^{(C)})^{-1}\vG  (\vS^{(K)})^{-1}\vG^T ) \big)\big]
\end{align*} where 
$ \vm^{(l)} = (\vd^{(l)})^{-1} \odot \big[ - \gamma\vd^{(l)} + \frac{1}{ {\alpha} k^{(l)}}   \mathrm{diag}(\vW^{(l)} ) \big] 
$.
Similarly, we can reexpress the update of $\vd^{(l)}$ as
\begin{align*}
\vd^{(l)} & \leftarrow 
\vd^{(l)} \odot \exp\big[ \stepsize_2 [\vm^{(l)} - \mathrm{mean}(\vm^{(l)})] \big] \\
& = \vd^{(l)} \odot  \exp\big[\stepsize_2 \big(  (\vd^{(l)})^{-1} \odot \big( \frac{1}{ {\alpha} k^{(l)}}   \mathrm{diag}(\vW^{(l)} ) \big) - \frac{1}{mn\alpha}\mathrm{Tr}((\vS^{(C)})^{-1}\vG  (\vS^{(K)})^{-1}\vG^T) \big) \big]
\end{align*}
We can merge these updates and truncate the experiential map as
\begin{align*}
    \alpha \vd^{(l)} & \leftarrow \alpha \vd^{(l)} \exp\big[ 
    \stepsize_2 \big( -\gamma + (\vd^{(l)})^{-1} \odot \big( \frac{1}{ {\alpha} k^{(l)}}   \mathrm{diag}(\vW^{(l)} ) \big)  \big)
    \big]  \\
    & \approx 
    \alpha \vd^{(l)} \odot \big( (1-\stepsize_2 \gamma) + (\vd^{(l)})^{-1} \odot \big[ \frac{1}{ {\alpha} k^{(l)}}   \mathrm{diag}(\vW^{(l)} ) \big] \big) \\
    & = \alpha \big[(1-\stepsize_2 \gamma) \vd^{(l)} + \frac{\stepsize_2}{ {\alpha} k^{(l)}}   \mathrm{diag}(\vW^{(l)} )\big]
\end{align*}
\end{proof}




\section{Additional Results for the Empirical Validation}
\label{app:extra_empirical_valida}

\subsection{Full-matrix Update Scheme}
\begin{description}
 \item[Fixed-point matching]\,
 The ground truth in this setting is a fixed-point solution, $\vS_{*} = E[\vg\vg^T]=\vSigma$, to the default update scheme as $\vS_{*}=(1-\stepsize)\vS_{*} + \stepsize \vg_k\vg_k^T $, where
  $\vg_k$ is independently generated from a Gaussian distribution $\vg_k \sim \gauss(\mathbf{0}, \vSigma)$ at each iteration $k$.
We evaluate each scheme at iteration $k$ by comparing its current estimate denoted by $\vS_k^{\text(est)}$ to the fixed point.
We use a relative Frobenius norm 
\scalebox{0.8}{
$\frac{ \|\vS_* - \vS_k^{\text{(est)}}\|_\text{Frob}}{\|\vS_*\|_\text{Frob}}$ }.
and the Wasserstein-2 distance for positive-definite matrices to measure the difference.


\item[\bf Iterate matching]\,
The ground truth is a sequence of matrices $\{\vS_1^{\text{(true)}},\dots,\vS_T^{\text{(true)}}\}$
generated by the default scheme when applying the scheme to the gradient sequence.
We want to match the iterate that the default scheme generates at every step. 
We use a relative Frobenius norm 
\scalebox{0.8}{
$\frac{ \|\vS_k^{\text{(true)}} - \vS_k^{\text{(est)}}\|_\text{Frob}}{\|\vS_k^{\text{(true)}}\|_\text{Frob}}$}.
and the Wasserstein-2 distance to measure the discrepancy between an update scheme and the default update scheme at every iteration $k$.
\end{description}

\subsection{Kronecker-based Update Scheme}
\begin{description}
 \item[Fixed-point matching]\,
 The ground truth is an unstructured fixed-point solution, $\vS_{*} = E[\vg\vg^T]=\vSigma$.
We evaluate a Kronecker-structured scheme in every iteration $k$ by comparing its current
structured estimate to the fixed point. We measure the difference using the same metrics
considered previously.


\item[\bf Iterate matching]
The ground truth is a sequence of unstructured matrices generated by the default
scheme. Our goal is to match the iterate that the default scheme generates using Kronecker
structured approximations We use the same metrics to measure the difference.

\end{description}

\begin{center}
\begin{figure*}
  \centering
 \includegraphics[width=\linewidth]{figs/fig_toy_full.pdf}
  \vspace{-2ex}
  \caption{
  Empirical validation of our full-matrix update scheme on estimating preconditioning matrix $\vS \in \real^{100 \times 100}$.
  The first two figures on the left show that our update scheme converges to a fixed-point solution as fast as the default update scheme in $\vS$ and the Cholesky-based scheme. 
  The last two figures illustrate how closely our update scheme matches the iterates generated by the default update scheme at each iteration.
  Our update scheme and the Cholesky-based scheme perform similarly for matching the preconditioner estimates generated by the default scheme.
  }
 \label{fig:full_mat_toy}
\end{figure*}
\end{center}


\begin{center}
\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{figs/fig_toy_kron.pdf}
  \vspace{-2ex}
  \caption{
   Empirical validation of our structured update scheme on estimating a preconditioner $\vS$ using a Kronecker product $\vS^{(C)} \otimes \vS^{(K)}$, where $\vS^{(C)} \in \real^{9 \times 9} $ and $ \vS^{(K)} \in \real^{11 \times 11}$.
  The first two figures on the left show that our update scheme gives a structural approximation of
  a fixed-point solution obtained by the default full-matrix update scheme. %
  Our scheme converges as fast as Kronecker-structured baseline methods, including the impractical projection-based method.
  The last two figures illustrate how closely our  scheme matches the unstructured iterates generated by the default  scheme at each iteration. All update schemes perform similarly due to the structural  approximation  gap.
  }
  \label{fig:kron_toy}
\end{figure*}

\end{center}

\section{Results on Gradient-free Evolutionary Problems}
\label{app:bb_opt}


We consider five test problems:
the Ackley function $\ell_\text{Ackley}(\vw)=20-20 \exp(-0.2\sqrt{\frac{1}{d}\sum_{i=1}^d w_i^2}) + e - \exp(\frac{1}{d}\sum_{i=1}^d \cos(2\pi w_i)) $, 
the Rosenbrock function $\ell_\text{Rosenbrock}(\vw)=\sum_{i=1}^{d-1} ( 100 (w_{i+1}-w_i^2)^2 +(w_i-1)^2 )$,
the Bohachevsky function $\ell_\text{Bohachevsky}(\vw)=\sum_{i=1}^{d-1}(w_i^2 + 2 w_{i+1}^2 - 0.3 \cos(3\pi w_i) - 0.4 \cos(4\pi w_{i+1}) + 0.7 )$, the Schaffer function $\ell_\text{Schaffer}(\vw)= \sum_{i=1}^{d-1} (w_i^2 + w_{i+1}^2)^{0.25} [ \sin^2 (50 (w_i^2 + w_{i+1}^2)^{0.1} ) +1 ] $ and the
Griewank function $\ell_\text{Griewank}(\vw)  =\frac{1}{4000} \sum_{i=1}^{d} w_i^2 - \prod_{i=1}^d \cos(\frac{w_i}{\sqrt{i}})+1 $. These problems represent diverse optimization settings, including multimodality and narrow curvature. 
From Fig.~\ref{fig:extra_results}, we can see that our method performs similarly to the baseline methods and demonstrates the efficacy of our update scheme.




\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{figs/nes_res.pdf}
  \vspace{-2ex}
  \caption{
  Experiments demonstrate the
  efficacy of our update schemes for gradient-free optimization problems defined at Appx.~\ref{app:bb_opt}.
 These problems represent diverse optimization settings, including multimodality and narrow curvature.
  We can see that our method performs similarly to Riemannian baseline methods.
  }
  \label{fig:extra_results}
\end{figure*}



\section{Additional Details and Results for Low-precision NN training}

We consider two vision transformer-based (ViT) models to demonstrate the effectiveness of our methods in half precision.
We train ViTs for 210 epochs with mini-batch size 128 for ImageWoof-10 and 256 for ImageNet-25. 
The ImageWoof-10 dataset is obtained from \url{https://github.com/fastai/imagenette}. The ImageNet-25 dataset is a subset of the ImageNet-1k dataset by randomly sampling 25 classes from the 1000 classes.
We use data augmentation techniques such as MixUp 
\citep{zhang2017mixup} and CutMix \citep{yun2019cutmix} during training. 
We use a cosine learning rate schedule suggested by \citet{chen2023symbolic}.
We use PyTorch’s built-in AdamW and official implementation for SOAP (\url{https://github.com/nikhilvyas/SOAP}) and Muon (\url{https://github.com/KellerJordan/modded-nanogpt}).
For SOAP and our method, we update their matrix preconditioners at each 2 iterations.
We tune each optimizer's available hyperparameters (HPs) using
random search \citep{choi2019empirical}. We use 80 runs for the search.
We use Pytorch's mixed precision with BFP-16 for NN training.
All methods except SOAP can use
BFP-16 data types and perform updates in BFP-16. SOAP has to internally cast BFP-16 gradients into FP-32 and performs matrix decomposition  (e.g., QR and eigendecomposition) in FP-32.
This is because matrix decomposition is numerically unstable in half-precision and, therefore, unsupported/unavailable in PyTorch, JAX, Julia, Scipy/Numpy, and LAPACK.
Figs.~\ref{fig:imagewoof} and \ref{fig:imagenet25} illustrate the performance of our Kronecker-based update scheme. 





\label{app:extra_nn_training}
\begin{figure*}
  \centering
 \includegraphics[width=\linewidth]{figs/exact_vs_truncated_results.pdf}
  \vspace{-2ex}
  \caption{
  Comparison of our Kronecker-based update schemes with $p=2$ (see the bottom box of Fig.~\ref{fig:kronecker}) using the exact Cayley and the truncated Cayley maps.
  We use nonconstant $\stepsize_2^{(l)}:=\frac{ \bar{ \stepsize}_2 \alpha \kappa^{(l)} }{ \| 
  \mathrm{Skew}( \mathrm{Tril}(\vU^{(l)})  ) 
 \|_{\text{Frob}} }$  and use the same hyperparameters in both cases, where $\kappa^{(C)}=m$, $\kappa^{(K)}=n$, $\vU^{(C)} \in \mathcal{R}^{n \times n}$, and  $\vU^{(K)} \in \mathcal{R}^{m \times m}$. Note that $\bar{\stepsize_2}$ is a constant hyperparameter.
We can see that both maps work equally well in terms of iterations/epochs. However, using the exact Cayley map is much slower regarding running time because it uses matrix inversion.
  }
 \label{fig:exact_vs_truncated}
\end{figure*}


\begin{figure*}
  \centering
 \includegraphics[width=\linewidth]{figs/imagenet10_results.pdf}
  \vspace{-2ex}
  \caption{
  Comparison of the methods on the ImageWoof-10 dataset in terms of iteration efficiency and wall-clock time.
  These plots demonstrate the efficiency of our update schemes for low-precision NN training compared to state-of-the-art methods such as Muon and SOAP. 
  }
 \label{fig:imagewoof}
\end{figure*}

\begin{figure*}
  \centering
 \includegraphics[width=\linewidth]{figs/imagenet25_results.pdf}
  \vspace{-2ex}
  \caption{
  Comparison of the methods on the ImageNet-25 dataset in terms of iteration efficiency and wall-clock time.
  These plots demonstrate the efficiency of our update schemes for low-precision NN training compared to state-of-the-art methods such as Muon and SOAP. 
  }
 \label{fig:imagenet25}
\end{figure*}





\section{
Handling Redundancy due to Repeated Entries of $\vd$ }
\label{app:handling_repated_d}

Recall that eigendecomposition is not unique when having repeated eigenvalues.
Our spectral parametrization is also not unique when $\vd$ has duplicate entries. 
In this case, the Fisher-Rao metric (coordinate representation) is singular.
We allow $\vd$ to have repeated entries and address the singularity using the Moore–Penrose inversion \citep{van2023invariance}. %
Computing this inversion is easy because we diagonalize the metric at evaluation points. 
We also use this inversion to improve numerical stability if $\vd$ has very close entries.
%




\section{Proof of Claim \ref{claim:linear_invariance}}
\label{app:proof_lemma_linear}
{\bf (Formal) Claim 1}
Consider an unconstrained parameterization $\vtau \in \mathcal{R}^d$ and a linear reparametrization $\vlambda \in \mathcal{R}^p$ so that  $\vtau:= \vU \vlambda $, where 
the metric $\vF_\tau$ in coordinate $\vtau$ is nonsingular (and thus SPD) everywhere, $\vU \in \mathcal{R}^{d \times p}$ is a constant matrix with rank $d$, and $p \geq d$. Then performing (unconstrained) RGD (see \eqref{eq:rgd}) in coordinate $\vtau$ is equivalent 
to performing RGD in coordinate $\vlambda$ (i.e., $\vtau_k = \vU \vlambda_k$ for each iteration $k$)

{\bf Remark 1:}
If a linear reparametrization is written as $\vlambda = \vA \vtau$, we can define $\vU$ as follows, where $\vA \in \mathcal{R}^{p \times d}$ with rank $d$.
We define $\vU := \vV \begin{bmatrix}
    \vD^{-1} & \mathbf{0}
\end{bmatrix} \vK^T$, where  $\vV$, $\vD$, and $\vK$ are obtained via
the singular value decomposition of $\vA$ so that $\vA = \vK \begin{bmatrix}
    \vD \\ \mathbf{0}
\end{bmatrix}  \vV^T$. 
Recall that $\vK \in \mathcal{R}^{p \times p}$ and $\vV \in \mathcal{R}^{d \times d} $ are orthogonal matrices. Moreover, $\vD \in \mathcal{R}^{d \times d}$ is a non-singular diagonal matrix because of the rank of $\vA$.
It is easy to verify that
$\vU$ is well-defined because 
\begin{align*}
\vU \vlambda & = \vU \vA \vtau \\
   & =\vV \begin{bmatrix}
    \vD^{-1} & \mathbf{0}
\end{bmatrix} \underbrace{\vK^T 
 \vK}_{=\vI_p} \begin{bmatrix}
    \vD \\ \mathbf{0}
\end{bmatrix}  \vV^T \vtau \\
&= \underbrace{\vV \vI_d \vV^T}_{=\vI_d} \vtau = \vtau.
\end{align*}  



{\bf Remark 2:}
Recall that by the definition of the Riemannian metric, we have $\vF_\lambda = \vU^T \vF_\tau \vU$.
Moreover, we have $\vg_\tau:=\nabla_\tau \mathcal{L}$ and $\vg_\lambda = \nabla_\lambda
\mathcal{L}= \vU^T \vg_\tau$ by the chain rule.
Note that when $p > d$, $\vlambda$ is in a linear over-parametrized space.
The claim implies that the over-parametrized $\vlambda$ preserves the invariance when $\vtau = \vU\vlambda$.
Notably, we can see that the metric $\vF_\lambda \in \mathcal{R}^{p \times p}$ in coordinate $\vlambda$ is singular when $p > d$. This is due to over-parametrization.

\begin{proof}
We will use proof by induction. 
We initialize $\vlambda$ so that $\vtau_0 = \vU\vlambda_0$. Thus, the base case holds.
At iteration $k$, by induction, we have $\vtau_k = \vU \vlambda_k  $. Now, we show that this relationship holds at iteration $k+1$.

By \eqref{eq:rgd}, $\tau_{k+1}$ is updated by following this expression:
\begin{align*}
    \vtau_{k+1} = \vtau_k -\stepsize [ \vF_\tau (\vtau_k)]^{-1} \vg_\tau(\vtau_k) 
\end{align*}


Since $\vF_\lambda$ is singular, we have to use  the Moore-Penrose inverse and it is computed as
\begin{align*}
    [\vF_\lambda]^{-1} &=[ \vU^T  \vF_\tau \vU]^{-1} \\
    &=[ \underbrace{\vU^T  \vL}_{:=\vC} 
 \underbrace{\vL^T \vU}_{:=\vD}]^{-1} \\
 &= \vD^T (\vD\vD^T)^{-1}(\vC\vC^T)^{-1}\vC^T \,\,\,\text{(according to Eq. 224 of \citet{petersen2008matrix})}\\
 &= \vU^T \vL (\vL^T\vU\vU^T\vL)^{-1}(\vL^T\vU\vU^T\vL)^{-1} \vL^T \vU \\
 &= \vU^T (\vU\vU^T)^{-1} \vF_\tau^{-1} (\vU\vU^T)^{-1} \vU,
\end{align*} where we use the Cholesky decomposition for $\vF_\tau = \vL\vL^T$ since $\vF_\tau$ is SPD.

Likewise, $\lambda_{k+1}$ is updated by RGD.
We have the following this expression:
\begin{align*}
\vU \{ 
    \vlambda_{k+1} \} &= \vU\{\underbrace{ \vlambda_k -\beta [ \vF_\lambda (\vlambda_k)]^{-1} \vg_\lambda(\vlambda_k)}_{\text{by the definition of RGD}} \} \\
& =\vU\{ \vlambda_k -\stepsize[ \vF_\lambda (\vlambda_k)]^{-1} \big(\underbrace{\vU^T \vg_\tau(\vtau_k)}_{\text{by the chain rule}}\big) \} \\
   & = \vU \{ \vlambda_k -\stepsize \big[ \vU^T (\vU\vU^T)^{-1} \vF_\tau^{-1} (\vU\vU^T)^{-1} \vU  \big] \big( \vU^T \vg_\tau(\vtau_k)\big)\}
 \,\,\,\text{(using the Moore-Penrose inverse)} \\
 &= \vU\{ \vlambda_k - \stepsize \vU^T (\vU\vU^T)^{-1} \vF_\tau^{-1} \vg_\tau(\vtau_k)\} \\
 &= \underbrace{ \vU \vlambda_k}_{=\vtau_k} - \stepsize 
 \vF_\tau^{-1} \vg_\tau(\vtau_k) \,\,\, \text{(by assumption)} \\
 &= \vtau_{k+1}
\end{align*}

Thus, by induction, we show the claim holds.
\end{proof}


\section{Proof of Claim \ref{claim:eigen_full_mat_case}}
\label{app:proof_lemma_full}
{\bf (Formal) Claim 2}
Our update scheme for $\vS$ in the top box of Fig~\ref{fig:kronecker} is equivalent to the scheme in Eq.~\eqref{eq:root_free} up to first-order in terms of $\stepsize_2$ (i.e., $\vB_k \mathrm{Diag}(\vd_k)\vB_k^T = \vS_k + O(\stepsize_2^2)$) when $\vd$ does not have repeated entries and using the same sequence of GOPs $\{\mathcal{H}_1, \dots, \mathcal{H}_T \}$, where $\mathcal{H}_k:=\vg_k\vg_k^T$ is a GOP.


\begin{proof}
    

%
We will use proof by induction.
%
When $k=0$, we initialize  $\vB$ and $\vd$, and define $\bar{\vS}_0$ so that $\bar{\vS}_0 := \vB_0\mathrm{Diag}(\vd_0)\vB_0^T = \vS_0$. Thus, the base case holds.

By induction, the claim holds at iteration $k$. That is $ \bar{\vS}_k:=\vB_k \mathrm{Diag}(\vd_k)\vB_k^T =\vS_k+O(\stepsize_2)$
We want to show that the claim also holds $k+1$ at iteration.  
Recall that we update the spectral factors using the following rule at iteration $k$.
\begin{align}
     \vd_{k+1} & \leftarrow  \vd_k \odot \exp\{ \stepsize_2\, \vd_k^{-1} \odot [ -\gamma \vd_k +    \mathrm{diag}(\vB_k^T  \mathcal{H}_k \vB_k )  ]  \} \nonumber \\
  \vB_{k+1} & \leftarrow \vB_k\mathrm{Cayley}(\frac{\stepsize_2}{2}\, \mathrm{Skew}({\mathrm{Tril}(\vU)}) ), 
\end{align} where $\mathcal{H}_k=\vg_k\vg_k^T$ and 
the $(i,j)$-th entry of $\vU$ is $[U]_{ij}:= -[\vB_k^T \mathcal{H}_k \vB_k]_{ij}/(d_i - d_j) $, where $\vd$ has no repeated entries by our assumption.
We want to show that the above update scheme is equivalent to the default update scheme in \eqref{eq:root_free} up to first order in terms of $O(\beta_2)$. 
\begin{align}
    \vS_{k+1} \leftarrow  (1-\stepsize_2\gamma) \vS_k + \stepsize_2 \underbrace{\mathcal{H}_k}_{ =\vg_k \vg_k^T}
\end{align}


Let $\vQ_k := \vB_k^T  \mathcal{H}_k \vB_k$.
Recall that the Cayley map is defined as $\mathrm{Cayley}(\vN)=(\vI +\vN)(\vI-\vN)^{-1}$.
Using the first-order approximation of $(\vI-\vN)^{-1}$, we have $(\vI-\stepsize_2\vN)^{-1} = \vI +\stepsize_2 \vN + O(\stepsize_2^2) $.
Thus, we have $\vB_{k+1} =\vB_k \mathrm{Cayley}(\frac{\stepsize_2}{2}\, \mathrm{Skew}({\mathrm{Tril}(\vU)}) = \vB_k (\vI+\frac{\stepsize_2}{2}\vN) (\vI+\frac{\stepsize_2}{2}\vN + O(\stepsize_2^2))= \vB_k (\vI+ \stepsize_2 \vN + O(\stepsize_2^2) )$, where $\vN:=\mathrm{Skew}({\mathrm{Tril}(\vU)})$.
Similarly, we have
$\vd_{k+1} = \vd_k \odot [ 1 + \stepsize_2 \vd_k^{-1} \odot (-\gamma \vd_k + \mathrm{diag}(\vQ_k) ) + O(\stepsize_2^2)] = \vd_k + \stepsize_2 \vw_k+O(\stepsize_2^2)$
by using the first-order truncation of the exponential map, where 
$\vw_k:=-\gamma \vd_k+\mathrm{diag}(\vQ_k)$.
Notice that
\begin{align*}
    &\bar{\vS}_{k+1} \\ 
    :=& \mathbf{B}_{k+1} \mathrm{Diag}(\vd_{k+1}) \mathbf{B}_{k+1}^{T} \\
    = &\mathbf{B}_{k} \Big[ (\vI+\stepsize_2 \vN + O(\stepsize_2^2)) \mathrm{Diag}( \vd_k +\stepsize_2 \vw_k + O(\stepsize_2^2)) \Big]
    (\vI+\stepsize_2 \vN + O(\stepsize_2^2))^T 
 \vB_k^T \\
    =& \vB_k \Big[ \vD_k  +\stepsize_2 \vN \vD_k + \stepsize_2\vW_k + O(\stepsize_2^2) \Big] (\vI+\stepsize_2 \vN + O(\stepsize_2^2))^T\vB_k^T\\
    =& \vB_k \Big[ \vD_k  +\stepsize_2 \vN \vD_k + \stepsize_2\vW_k + \stepsize_2 \vD_k \vN^T + O(\stepsize_2^2) \Big]\vB_k^T \\
=& \vB_k \vD_k \vB_k^T + \stepsize_2 \vB_k ( \vN\vD_k + \vW_k + \vD_k\vN^T) \vB_k^T + O(\stepsize_2^2) \\
=& \bar{\vS}_k + \stepsize_2 \vB_k (\vN\vD_k + \vD_k\vN^T) \vB_k^T + \stepsize_2 \vB_k \vW_k \vB_k^T +O(\stepsize_2^2), \, (\vN \text{ is skew-symmetric})\\
=&\bar{\vS}_k + \stepsize_2 \vB_k (\vN\vD_k - \vD_k\vN) \vB_k^T + \stepsize_2 \vB_k \vW_k \vB_k^T+O(\stepsize_2^2)
\end{align*} where $\vD_k:=\mathrm{Diag}(\vd_k)$ and $\vW_k:=\mathrm{Diag}(\vw_k)$


Observation (1): Since $\vW_k=\mathrm{Diag}(-\gamma \vd_k + \mathrm{diag}(\vB_k^T \mathcal{H}_k\vB_k))=-\gamma \vD_k+  \mathrm{Diag}( \mathrm{diag}(\vB_k^T \mathcal{H}_k \vB_k) ) $, we have
\begin{align}
   \vB_k \vW_k \vB_k^T = -\gamma \vB_k \vD_k \vB_k^T + \vB_k \mathrm{Ddiag}(\vQ_k) \vB_k^T
\end{align} where $ \mathrm{Ddiag}(\vQ_k) $ denotes the  diagonal part of $\vQ_k=\vB_k^T \mathcal{H}_k \vB_k$

Observation (2): Since $\vN = \mathrm{Skew}(\mathrm{Tril}(\vU))$ and $[\vU]_{ij}=  -[\vB_k^T \mathcal{H} \vB_k]_{ij}/(d_i - d_j)=-[\vQ_k]_{ij}/(d_i-d_j)$, we can show that $\vN\vD_k - \vD_k\vN$ is indeed a symmetric matrix with zero-diagonal entries.
Moreover,  
the low-triangular half ($i>j$) of the matrix can be expressed as
\begin{align}
    [\vN \vD_k -\vD_k \vN]_{ij} = (d_j-d_i) [\vU]_{ij} = [\vQ_k]_{ij}.
\end{align} where $d_j \neq d_i$ since $\vd$ has no repeated entries.
Thus, we have 
$\vN\vD_k - \vD_k\vN = \vQ_k - \mathrm{Ddiag}(\vQ_k)$.

Using Observations (1) and (2), we have
\begin{align*}
    &\bar{\vS}_{k+1} \\ 
    =& \vB_k \Big[ \vD_k  +\stepsize_2 \vN \vD_k + \stepsize_2\vW_k + \stepsize_2 \vD_k \vN^T + O(\stepsize_2^2) \Big]\vB_k^T \\
    =& \vB_k\vD_k\vB_k^T +\stepsize_2 \Big[ \vB_k \Big(\vQ_k-\mathrm{Ddiag}(\vQ_k)\Big) \vB_k^T  -\gamma \vB_k\vD_k\vB_k^T + \vB_k \mathrm{Ddiag}(\vQ_k) \vB_k^T \Big] + O(\stepsize_2^2) \\
    =& (1-\stepsize_2\gamma)  \vB_k\vD_k\vB_k^T + \stepsize_2 \vB_k \Big[ \vQ_k \Big] \vB_k^T + O(\stepsize_2^2),\, (\text{Note: } \vQ_k=\vB_k^T \mathcal{H}_k\vB_k)\\
    =& (1-\stepsize_2\gamma)\bar{\vS}_k + \stepsize_2 \mathcal{H}_k + O(\stepsize_2^2) = (1-\stepsize_2\gamma)\vS_k + \stepsize_2 \mathcal{H}_k + O(\stepsize_2^2).
\end{align*}  
By induction, we can show the claim holds.
%
\end{proof}

\section{Proof of Claim 
\ref{claim:eig_constraint2}}
{\bf Claim 3}
The map in \eqref{eq:translation_map} satisfies the constraints in \eqref{eq:eig_opt}.

\begin{proof}
    
Now, we show the map satisfies the spectral parameter constraints in \eqref{eq:eig_opt}.
Since $\vmu$ is unconstrained, we only consider the update on $\vd$ and $\vB$.

Recall that the current point  $(\vd_k,\vB_k)$ is in the spectral coordinate. Thus, we have $\vd_k >0$ and $\vB_k$ is orthogonal.
According to the map \eqref{eq:translation_map}, it is easy to see that $\vd(\vm)=\vd_k \odot \exp(\vm) >0$ because $\vd_k>0$. Thus, $\vd(\vm)$ satisfies the parameter constraints.
Now, we show that $\vB(\vM)$ is also orthogonal. 
Notice that a product of two orthogonal matrices is also orthogonal. Because $\vB_k$ is orthogonal, we only need to show that
the output of the Cayley map, $\mathrm{Cayley}(\mathrm{Skew}({\color{red} \mathrm{Tril}(\vM)}))$,  is orthogonal. 
Let $\vN:=\mathrm{Skew}({\color{red} \mathrm{Tril}(\vM)})$. We know that $\vN$ is skew-symmetric.
We can verify that the Cayley map satisfies the orthogonal constraint.
Consider the following expression:
\begin{align*}
\big( \mathrm{Cayley}(\vN) \big)^T \mathrm{Cayley}(\vN) & =   (\vI - \vN)^{-T} (\vI+\vN)^T (\vI+\vN) (\vI - \vN)^{-1} \\
&=  (\vI - \vN)^{-T} (\vI-\vN) (\vI+\vN) (\vI - \vN)^{-1}  \\
&= (\vI - \vN)^{-T} (\vI+\vN) (\vI-\vN) (\vI - \vN)^{-1} \\
&= (\vI - \vN)^{-T} (\vI-\vN)^{T} (\vI-\vN) (\vI - \vN)^{-1}  = \vI
\end{align*} where we use the fact that $\vN$ is skew-symmetric such as $\vN^T=-\vN$.

Likewise, we can show 
$\mathrm{Cayley}(\vN)  \big( \mathrm{Cayley}(\vN) \big)^T =\vI$. 
Thus, the output of the Cayley map is a square orthogonal matrix.

\end{proof}

%

%
%



\begin{comment}

\section{Proof of Claim 
\ref{claim:eig_constraint}}
\label{app:claim4_proof}

{\bf Claim 4}
The map in \eqref{eq:translation_map} is one-to-one, which is like the eigendecomposition, is unique
if $\vd$ has no repeated entries, where $\vM$ is a lower-triangular matrix with zero diagonal entries.


{\bf Remark} Recall that we use a sorted eigenvalue vector $\vd$ to make the factorization unique in the eigendecomposition. 
In our spectral parametrization, we use the lower-triangular restriction denoted by $\mathrm{Tril}(\cdot)$ to make our factorization unique. This is because we cannot guarantee $\vd$ to be sorted when updating $\vd$ on the fly.



\begin{proof}
It is easy to show that the map is one-to-one/injective for $\vdelta$.

Now, we show that the map is also injective for $\vm$ and $\vM$.
Let  $\vB(\vM) :=
\vB_{k} \mathrm{Cayley}(\mathrm{Skew}(\mathrm{Tril}(\vM))) $ and 
$\vd(\vm):= \vd_{k} \odot \exp( \vm )$

We want to show  $\vB(\vM_1)\mathrm{Diag}(\vd_1)\vB(\vM_1)^T = \vB(\vM_2)\mathrm{Diag}(\vd_2)\vB(\vM_2)^T$ implies $\vM_1=\vM_2$ and $\vd_1=\vd_2$.

Since $\vB(\vM_1)$ and $\vB(\vM_2)$ are orthogonal, we define $\vQ:=[\vB(\vM_2)]^{-1}\vB(\vM_1)$, which is also orthogonal.

We have
\begin{align*}
\vB(\vM_1)\mathrm{Diag}(\vd_1)\vB(\vM_1)^T &=
    \vB(\vM_2)\mathrm{Diag}(\vd_2)\vB(\vM_2)^T \\
    &=
    \vB(\vM_2) \underbrace{\vQ \vQ^T}_{=\vI} \mathrm{Diag} (\vd_2)\underbrace{\vQ \vQ^T}_{=\vI} \vB(\vM_2)^T \\
    &=\vB(\vM_1) [\vQ^T \mathrm{Diag} (\vd_2)\vQ ] \vB(\vM_1)^T,
\end{align*} which implies
$\mathrm{Diag} (\vd_1) = \vQ^T \mathrm{Diag} (\vd_2)\vQ $.

Because of the diagonal structure, we know that $\vQ$ must be a permutation matrix (up to sign changes).
Notice that $\vd_1$ and $\vd_2$ are vectors with positive entries. 
Thus, $\vQ$ is a permutation matrix (no sign change). 

Now, we show that $\vQ$ must be an identity matrix.
Let $\vK(\vM_i):= \mathrm{Cayley}(\mathrm{Skew}(\mathrm{Tril}(\vM_i)))$ for $i \in \{1,2\}$. 
By the definition of $\vQ$, we have $\vB(\vM_1) = \vB(\vM_2)\vQ$, which implies $\vK(\vM_1)=\vK(\vM_2) \vQ$.
By our assumption, $\vM_1$ and $\vM_2$ are low-triangular matrices with zero diagonal entries. 
Thus,  $\vK(\vM_1)$ and $\vK(\vM_2)$ are also low-triangular due to the definition of the Cayley map. 
Notice that $\vQ= (\vK(\vM_2))^{-1}\vK(\vM_1)$.
Thus, $\vQ$ is also a lower-triangular matrix. 
Because $\vQ$ is a permutation matrix and a lower-triangular matrix, $\vQ$ must be an identity matrix.



Since $\vQ=\vI$, we have $\vd_1=\vd_2$  
because  of
$\mathrm{Diag} (\vd_1) = \vQ^T \mathrm{Diag} (\vd_2)\vQ= \mathrm{Diag} (\vd_2) $.



Notice that $\vK(\vM_i)=\mathrm{Cayle}(\mathrm{Skew}(\vM_i))$ becomes the  Cayley map when $\vM_i$ is a lower-triangular matrix with zero diagonal entries. In other words $\vM_i  = \mathrm{Tril}(\vM_i)$.
Moreover, $\vK(\vM_1)=\vK(\vM_2)\vQ=\vK(\vM_2)$.

Since the Cayley map is injective w.r.t. to its skew-symmetric input (see Claim ~\ref{claim:cayley_map}), we have $\mathrm{Skew}(\vM_1) =\mathrm{Skew}(\vM_2)$. 
$\mathrm{Skew}(\vM_1) =\mathrm{Skew}(\vM_2)$ implies $\vM_1=\vM_2$ since $\vM_1$ and $\vM_2$ are 
lower-triangular matrices with zero diagonal entries.

Thus, the map in \eqref{eq:translation_map} is injective.
\end{proof}






\end{comment}



\section{Proof of Claim 
\ref{claim:fisher_full}
}
\label{app:claim_fisher_full}

{\bf Claim 5}
    The exact Fisher-Rao metric $\vF_\eta(\veta_\text{cur})$ (for a full-matrix Gaussian) evaluated at the origin \scalebox{0.8}{ $\veta_\text{cur}\equiv\mathbf{0}$} is \emph{diagonal} and has the following  closed-form expression:
\begin{align}
    \vF_\eta(\vm, \mathrm{vecTril}(\vM), \vdelta)\big|_{ \vdelta=\mathbf{0}, \vm=\mathbf{0}, \vM =\mathbf{0} } = \begin{bmatrix}
          \vF_{mm} & \mathbf{0} &\mathbf{0}  \\
         \mathbf{0}  & \vF_{MM}& \mathbf{0} \\
     \mathbf{0} & \mathbf{0} & \vF_{\delta\delta}   \\
    \end{bmatrix},
    \label{eq:fim_full_local}
\end{align} 
where  $\vF_{\delta\delta}=\vI$,
$\vF_{mm}=\half \vI$,
$\vF_{MM}= \mathrm{Diag}( \mathrm{vecTril}(\vC) )$, $\mathrm{vecTril}(\vC)$ represents the vectorization of the low-triangular half of $\vC$ excluding diagonal entries and its $(i,j)$-th entry is $[\vC]_{ij}=4( \frac{d_i}{d_j} + \frac{d_j}{d_i} -2 )\geq 0$ and $d_i$ denotes the $i$-th entry of $\vd_k$.
The metric becomes singular when $\vd$ has repeated entries (i.e., $d_i =d_j$ for $i \neq j$). In this case, we use the Moore-Penrose inversion to inverse the metric.

To verify this statement, we can analytically compute the Fisher-Rao metric according to the following expression.  
\begin{align*}
    \vF_\eta(\veta) &= E_{w\sim q}[\nabla_\eta \log q(\vw;\veta)\nabla_\eta^\top \log q(\vw;\veta)]  \\
    &= - E_{w\sim q}[\nabla_\eta^2 \log q(\vw;\veta)],
\end{align*} where we use the expression in the last line to compute the metric and obtain the simplified expression in Eq.~\eqref{eq:fim_full_local}.

\section{Proof of Claim 
\ref{claim:kron_unique}
}
\label{app:claim6_proof}
{\bf Claim 6}
    A Kronecker-structured positive-definite matrix $\vS$ can be uniquely expressed as  
    \scalebox{0.8}{ $\vS=\alpha [\vS^{(C)} \otimes \vS^{(K)}]$ } with constraints  \scalebox{0.8}{$\mathrm{det}(\vS^{(C)})=\mathrm{det}(\vS^{(K)})=1$ } and \scalebox{0.8}{$\alpha>0$}. 


\begin{proof}
We will show that 
$\vS_1:=\alpha_1 [\vS_1^{(C)} \otimes \vS_1^{(K)}] =\vS_2:= \alpha_2 [\vS_2^{(C)} \otimes \vS_2^{(K)}] $ implies $\alpha_1=\alpha_2$,
and $\vS_1^{(l)}=\vS_2^{(l)}$ for $l \in \{C,K\}$, 
where $\vS_1^{(l)}$ and $\vS_2^{(l)}$ are assumed to have the same shape.

Thanks to the determinant constraint, we have
$\alpha_1=\alpha_2$ because of $\mathrm{det}(\vS_1)=\mathrm{det}(\vS_2)$. 

Since $\vS_1=\vS_2$ is SPD, we have
\begin{align*}
    \vI & = \vS_1^{-1} \vS_2 \\
   &=  \{ (\vS_1^{(C)})^{-1} \vS_2^{(C)} \} \otimes
    \{ (\vS_1^{(K)})^{-1} \vS_2^{(K)} \}
\end{align*}

Notice that 
$ \mathrm{det} ( (\vS_1^{(l)})^{-1} \vS_2^{(l)} ) = 1
$ due to the determinant constraint for $l \in \{C,K\}$.
By the definition of the Kronecker structure, the above expression only holds when  $(\vS_1^{(K)})^{-1} \vS_2^{(K)}=\vI^{(K)}$ and  $(\vS_1^{(C)})^{-1} \vS_2^{(C)}=\vI^{(C)}$ for regardless of the shapes of $\vS^{(C)}$ and $\vS^{(K)}$.

Thus, we have $\vS_1^{(l)}=\vS_2^{(l)}$ for $l \in \{C,K\}$.
\end{proof}

\begin{comment}
\section{Proof of Claim 
\ref{claim:local_kron_unique}
}
{\bf Claim 7}
The transformation map (see Eq.~\eqref{eq:kron_trans_map}) is one-to-one if $\vd^{(l)}$ has no repeated entries for each Kronecerk factor $l$.

\begin{proof}

It is easy to see that the map is injective for the local coordinate for $\alpha$.
The remaining proof is similar to the proof of Claim~\ref{claim:eig_constraint} in Appx.~\ref{app:claim4_proof}. 
\end{proof}
\end{comment}

\section{Proof of Claim 
\ref{claim:fisher_kron}
}
\label{app:kron_fim}
In a Kronecker case, we consider
this spectral factorization $\vS = \alpha [ (\vB^{(C)}\mathrm{Diag}(\vd^{(C)})(\vB^{(C)})^T) \otimes (\vB^{(K)}\mathrm{Diag}(\vd^{(K)})(\vB^{(K)})^T) ]$.
At iteration $k$, we create a local coordinate $\veta:=(\vdelta,n, \vm^{(C)},\vM^{(C)}, \vm^{(K)},\vM^{(K)})$ at the current point $\vtau_k:=(\vmu_k,\alpha_k, \vd_k^{(C)}, \vB_k^{(C)},\vd_k^{(K)}, \vB_k^{(K)})$ and use this local transformation map
\begin{align}
\vtau(\veta;\vtau_k) :=
\begin{bmatrix}
\alpha(n;\vtau_k) \\
\vd^{(C)} (\vm^{(C)};\vtau_k)  \\
\vB^{(C)} (\vM^{(C)};\vtau_k) \\
\vd^{(K)} (\vm^{(K)};\vtau_k)  \\
\vB^{(K)} (\vM^{(K)};\vtau_k) \\
\vmu (\vdelta;\vtau_k) 
\end{bmatrix} = \begin{bmatrix}
    \alpha_{k}  \exp( n ) \\
    \vd_{k}^{(C)} \odot \exp( \vm^{(C)} ) \\
\vB_{k}^{(C)} \mathrm{Cayley}(\mathrm{Skew}(\mathrm{Tril}(\vM^{(C)}))) \\
    \vd_{k}^{(K)} \odot \exp( \vm^{(K)} ) \\
\vB_{k}^{(K)} \mathrm{Cayley}(\mathrm{Skew}(\mathrm{Tril}(\vM^{(K)}))) \\
    \vmu_k + (\vB_k^{(C)}\otimes \vB_k^{(K)}) ( \mathrm{Diag} (\vd_k^{(C)}) \otimes \mathrm{Diag}( \vd_k^{(K)}) )^{-1/2} \vdelta 
\end{bmatrix},
\label{eq:kron_trans_map}
\end{align} where $\vm^{(C)}=[m_1^{(C)}, m_2^{(C)}, \dots, m_{l-1}^{(C)}, -\sum_{i}^{l-1} m_i^{(C)}]$ has $l$ entries but only $(l-1)$ free variables since $\sum(\vm^{(C)})=0$.


To verify this statement, we can analytically compute the Fisher-Rao metric according to its definition. 

\begin{align}
 &   \vF_\eta(n, \mathrm{Free}(\vm^{(C)}), \mathrm{vecTril}(\vM^{(C)}), \mathrm{Free}(\vm^{(K)}), \mathrm{vecTril}(\vM^{(k)}), \vdelta
 )\big|_{ \veta=\mathbf{0}} \\
 = & \begin{bmatrix}
      \vF_{\alpha\alpha} & \mathbf{0} & \mathbf{0} & \mathbf{0}& \mathbf{0} & \mathbf{0}\\
          \mathbf{0} & \vF_{m^{(C)}m^{(C)}} & \mathbf{0} & \mathbf{0}& \mathbf{0} & \mathbf{0} \\
         \mathbf{0}  & \mathbf{0}& \vF_{M^{(C)}M^{(C)}}& \mathbf{0}& \mathbf{0} & \mathbf{0} \\
         \mathbf{0}  & \mathbf{0} & \mathbf{0} & \vF_{m^{(K)}m^{(K)}}& \mathbf{0} & \mathbf{0}\\
         \mathbf{0}  & \mathbf{0} & \mathbf{0} & \mathbf{0}& \vF_{M^{(K)}M^{(K)}} & \mathbf{0}\\
     \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{0}& \mathbf{0} & \vF_{\delta\delta}
    \end{bmatrix}
    \label{eq:fim_kron_local}
\end{align} where
$\mathrm{vecTril}(\vC)$ represents the low-triangular half of $\vC$ excluding diagonal entries and $\mathrm{Free}(\vm)$ extracts free variables from $\vm$.

We can see that the Fisher-Rao is block diagonal with six blocks.

The first two blocks are
$\vF_{\delta\delta}=\vI$ and $\vF_{\alpha\alpha}=\half$.
For each Kronecker factor, we have two blocks. 
For notation simplicity, 
we drop the factor index $C$ in $\vF_{m^{(C)}m^{(C)}}$ and $\vF_{M^{(C)}M^{(C)}}$.

For each Kronecker factor,  
$\vF_{MM}= \mathrm{Diag}( \mathrm{vecTril}(\vW) )$, $\mathrm{vecTril}(\vW)$ represents the low-triangular half of $\vW$ excluding diagonal entries and its $(i,j)$-th entry is $[W]_{ij}=4( \frac{d_i}{d_j} + \frac{d_j}{d_i} -2 )\geq 0$ and $d_i$ denotes the $i$-th entry of $\vd_k$ for the factor.
The $\vF_{mm}$ is non-diagonal but its inverse can be computed as 
$\vF_{mm}^{-1}= 2
\begin{bmatrix}
\frac{l-1}{l} & \frac{1}{l} & \dots & \frac{1}{l}  \\
\frac{1}{l} & \frac{l-1}{l} & \dots & \frac{1}{l} \\
\cdot & \cdot & \dots & \cdot \\
\frac{1}{l} & \frac{1}{l} & \dots & \frac{l-1}{l} 
\end{bmatrix}  \in \real^{(l-1) \times (l-1)}
$ for the $(l-1)$ free variables in $\vm$ denoted by $\mathrm{Free}(\vm)$.
Furthermore, the natural-gradient w.r.t. $\vm$ can also be simplified. 

\section{Complete Derivation for the Full-matrix Update}
\label{sec:deri_full}
According to Claim \ref{claim:fisher_full},  the Fisher-Rao metric under this  local coordinate system is  diagonal as
\begin{align}
    \vF_\eta(\vm, \mathrm{vecTril}(\vM),\vdelta)\big|_{ \vdelta=\mathbf{0}, \vm=\mathbf{0}, \vM =\mathbf{0} } = \begin{bmatrix}
          \vF_{mm} & \mathbf{0} &\mathbf{0} \\
         \mathbf{0}  & \vF_{MM} & \mathbf{0}\\
      \mathbf{0} & \mathbf{0} & \vF_{\delta\delta}  \\
    \end{bmatrix}
\end{align} where  $\vF_{\delta\delta}=\vI$,
$\vF_{mm}=\half \vI$,
$\vF_{MM}= \mathrm{Diag}( \mathrm{vecTril}(\vC) )$, $\mathrm{vecTril}(\vC)$ represents the low-triangular half of $\vC$ excluding diagonal entries and its $(i,j)$-th entry is $[C]_{ij}=4( \frac{d_i}{d_j} + \frac{d_j}{d_i} -2 )\geq 0$ and $d_i$ denotes the $i$-th entry of $\vd_k$.




Use the approximation  in Eq.~\eqref{eq:delta_approx}
 \begin{align}
    \vg_\mu & :=   \partial_\mu \mathcal{L} \mystein E_{w \sim q}[ \nabla_w \ell] \mydel \nabla_\mu \ell =\vg\\
       2\vg_{S^{-1}} &:= 2\partial_{S^{-1}}\mathcal{L} \mystein E_{w\sim q}[\nabla_w^2 \ell ] - \vS \mydel \nabla_\mu^2 \ell -\vS  \approx\mathcal{H} -\vS.
    \end{align}
 where $\vg := \nabla_\mu \ell(\vmu)$ is the gradient of $\ell$, $\mathcal{H}:= \vg\vg^T$ is a Hessian approximation. 
 %
%
%

The Euclidean gradient w.r.t local coordinate $(\vdelta,\vm,\vM)$ are
\begin{align}
 \vg_\delta \big|_{\delta=0} &= \vD_k^{-1/2} \vB_k^T \vg_\mu \\
    \vg_m \big|_{m=0} &= - \vd_k^{-1} \odot \mathrm{diag}(\vB_k^T \vg_{S^{-1}} \vB_k) \\
     \vg_{\mathrm{vecTril}(M)} \big|_{M=0} &= 4 \mathrm{vecTril}(\vB_k^T \vg_{S^{-1}} \vB_k \vD_k^{-1} - \vD_k^{-1} \vB_k^T  \vg_{S^{-1}} \vB_k)
\end{align} where $\vD_k:= \mathrm{Diag}(\vd_k)$. Recall that we use a gradient outer product $\mathcal{H}=\vg\vg^T$ as a Hessian approximation. %


The metric can still be singular 
when $\vd$ has repeated entries (i.e., $d_i = d_j$ for $i \neq j$) since $\vF_{MM}$ can be singular.
%
We can use the Moore-Penrose inverse when computing the inverse.
Thanks to this coordinate system,
$\vF_{MM}$ is indeed a diagonal matrix.
%


Thus, we can simplify the RGD update as
\begin{align}
     \begin{bmatrix}
        \vm \\
        \mathrm{vecTril}(\vM) \\
        \vdelta 
    \end{bmatrix} \leftarrow
       \begin{bmatrix}
         \mathbf{0} - \stepsize_2 \vF_{mm}^{-1} \vg_m \big|_{m=0} \\
         \mathbf{0} - \stepsize_2 \vF_{MM}^{-1}\vg_{\mathrm{vecTril}(M)} \big|_{M=0} \\
        \mathbf{0}  - \stepsize_1  \vF_{\mu\mu}^{-1}\vg_\delta 
    \end{bmatrix} =
      \begin{bmatrix}
         \mathbf{0} - 2 \stepsize_2 \vg_m \big|_{m=0} \\
         \mathbf{0} - \stepsize_2 \vF_{MM}^{-1}\vg_{\mathrm{vecTril}(M)} \big|_{M=0} \\ 
        \mathbf{0} - \stepsize_1 \mathrm{Diag}(\vd)^{-1/2} \vB^T  \vg_\mu 
    \end{bmatrix},
\end{align}
where we introduce another 
learning rate $\stepsize_2$ when updating $\vd$ and $\vB$.

Note that when $d_i \neq d_j$ for $i \neq j$, the $(i,j)$-th entry of the natural gradient w.r.t. $\vM$ can be expressed as
\begin{align}
   [  \vF_{MM}^{-1}\vg_{\mathrm{vecTril}(M)} \big|_{M=0} ]_{ij}=
    (\vB_k^T \vg_{S^{-1}} \vB_k)_{ij} / (d_i - d_j).
\end{align}
When $d_i = d_j$, we simply set the corresponding entry to be zero due to the
Moore-Penrose inverse.

Finally, we can re-express the above update as:
\begin{align}
    \begin{bmatrix}
        \vd_{k+1} \\
        \vB_{k+1}\\
        \vmu_{k+1} 
    \end{bmatrix} \leftarrow
        \begin{bmatrix}
        \vd_k  \odot \exp[  0 + 2 \stepsize_2  \vd_k^{-1} \odot \mathrm{diag}(\vB_k^T \vg_{S^{-1}} \vB_k) ] \\
        \vB_k \mathrm{Cayley}( \mathrm{Tril}(\vU)  - [\mathrm{Tril}(\vU)]^T ) \\
        \vmu_k - \stepsize_1 \vB_k \mathrm{Diag}(\vd_{k})^{-1} \vB_k^T  \vg_\mu\\
    \end{bmatrix}
\end{align} where the $(i,j)$ entry of $\vU$ is $[\vU]_{ij}  =  0 - \stepsize_2 [\vB_k^T \vg_{S^{-1}} \vB_k]_{ij} / (d_i - d_j)  $ for $i \neq j$ and $[\vU]_{ij}=0$ when $d_i = d_j$ thanks to the Moore-Penrose inverse.



\begin{claim}
\label{claim:cayley_map}
   The Cayley map $\mathrm{Cayley}(\vN)=(\vI+\vN)(\vI-\vN)^{-1}$ is well-defined for skew-symmetric $\vN$. Moreover, this map is injective.  
\end{claim}

\begin{proof}
To show the map is well-defined, we want to show  $(\vI-\vN)$ is non-singular.
Suppose not,  we have $\mathrm{det}(\vI-\vN)=0$. Thus, $\vN$ has an eigenvalue with $1$. By the definition of the eigenvalue, there exists a non-zero vector $\vx\neq \mathbf{0}$ so that $\vN \vx = \vx$.
Notice that
Given that $\vN$ is skew-symmetric, we have
\begin{align}
\vN + \vN^T = \mathbf{0}
\end{align} and
\begin{align}
  0 =  \vx^T (\vN+\vN^T) \vx = \vx^T (\vN \vx) + (\vx^T \vN^T) \vx = \vx^T \vx + \vx^T \vx 
\end{align} 
The above expression implies  $\vx=\mathbf{0}$, which is a contradiction.
Thus, $\mathrm{det}(\vI-\vN)\neq 0$ and $(\vI-\vN)$ is non-singular. 
%


Now, we show that the Cayley map is injective if $\vN$ is skew-symmetric. 
Let $\vQ = \mathrm{Cayley}(\vN)$. 
We first assume $(\vQ+\vI)$ is non-singular, and then we prove it. Given $(\vQ+\vI)$ is non-singular, we have 
\begin{align*}
   \vQ (\vI - \vN ) = (\vI+\vN) \iff \vQ -\vI = (\vQ +\vI) \vN \iff (\vQ+\vI)^{-1}(\vQ-\vI) = \vN,
\end{align*} This implies the map is injective and its inverse is 
\begin{align}
\vN=\mathrm{Cayley}^{-1}(\vQ) :=(\vQ+\vI)^{-1}(\vQ-\vI) \label{eq:inv_cayley}
\end{align}

Now, we show that $(\vQ+\vI)$ is non-singular. We use proof by contradiction.
If not, there exists a non-zero vector $\vv$ \citep{cayley-notes} so that
\begin{align}
    \vQ \vv = - \vv &\iff (\vI +\vN) (\vI-\vN)^{-1} \vv = -\vv\\
    &\iff (\vI-\vN)^{-1} (\vI+\vN)\vv = -\vv\\
    &\iff (\vI+\vN)\vv = -(\vI-\vN)\vv \\
    &\iff \vv = -\vv,\, (\text{another contradiction since } \vv \neq 0) 
\end{align} where we use the following identity in the second step in the above expression.
\begin{align}
   (\vI +\vN )(\vI-\vN)^{-1} &= -( -2\vI + \vI -\vN) (\vI-\vN)^{-1} =2 (\vI-\vN)^{-1} - \vI\\
   &= (\vI-\vN)^{-1}(2\vI - (\vI-\vN)) = (\vI-\vN)^{-1} (\vI+\vN)
\end{align}

\end{proof}
