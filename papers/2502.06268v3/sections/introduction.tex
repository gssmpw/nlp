%
%
\vspace{-0.2cm}
Symmetric positive-definite (SPD) curvature learning is useful for designing training methods. 
Given the neural net (NN) weights $\vmu$, the gradient $\vg$ and a SPD curvature estimate $\vS \succ 0$, many training methods apply $\smash{\vmu \leftarrow \vmu - \beta_1 \vS^{-1/p}}\vg$ using a step size $\beta_1$ and subjecting the curvature estimation to a fractional $p$-root before inversion.
For example, adaptive methods like 
Adam(W)~\citep{kingma2014adam,loshchilov2017decoupled}  employ the gradient outer product~\citep[GOP,][]{duchi2011adaptive,kingma2014adam,agarwal2019efficient} to estimate a diagonal matrix $\vS$ and apply a $2$-root (i.e., $p=2$) before preconditioning.
Other well-known methods, like natural-gradient methods, use $\vS$ to estimate the Fisher information   matrix~\citep{amari1998natural,roux2007topmoumoute} as another type of curvature and apply a $1$-root  before preconditioning.
These methods highlight the need for an SPD curvature learning scheme for $\vS$ that supports generic curvature information and root computation.

Existing works \citep{martens2015optimizing,zhang2018noisy,anil2020scalable,shi2023distributed,lincan2024,vyas2024soap} have demonstrated the great potential of Kronecker-based non-diagonal methods for training large neural networks because using Kronecker structures significantly reduces the memory consumption of non-diagonal methods.
However, applying fractional roots for Kronecker-based methods such as Shampoo~\citep{gupta18shampoo,anil2020scalable,shi2023distributed} and its variants \citep{vyas2024soap} remains computationally and numerically challenging. 
This is because computing a \emph{matrix} fractional root is computationally intensive and must be done in high precision to avoid numerical instabilities~\citep{anil2020scalable,shi2023distributed}, preventing those methods from using fast, low-precision arithmetic~\citep{micikevicius2017mixed}. 
Because of this bottleneck, non-diagonal methods are often limited to
a specific case of the root and particular types of curvature information.
%
%
%
Making the matrix root computation for generic curvature learning fast and stable not only enables non-diagonal methods for modern low-precision training but also facilitates the investigation of new non-diagonal methods.
Thus, it is essential to have a flexible and efficient approach that can (i) apply arbitrary fractional roots, (ii) circumvent the numerical instabilities of the root computation, and (iii) support generic curvature information. 

%
We address this instability and inefficiency and present an update scheme to directly and dynamically adapt the spectral factorization $\smash{\vB \mathrm{Diag}(\vd) \vB^\top}$ of a SPD estimation $\vS$ for generic curvature information. 
We call this parameterization a \emph{spectral parameterization}, due to its connection to the spectral decomposition of symmetric matrices. 
Thanks to this parametrization,  we can efficiently apply any matrix fractional root to $\vS$ through elementwise operations on the eigenvalues $\vd$.
Our approach directly adapts eigenfactors and maintains the factorization without performing matrix decomposition.  
This makes our scheme amenable to running in low precision because we do not rely on unstable matrix decomposition algorithms.
%
However, directly optimizing the factors involves nontrivial constraints (i.e., $\vB$ is orthogonal and $\vd$ is positive) that must be dealt with.

We overcome challenges and make these contributions:
\vspace{-0.3cm}
\begin{itemize}
%

\item
%
We propose an update scheme to dynamically adapt the spectral parameterization \scalebox{0.8}{ $\vB \mathrm{Diag}(\vd) \vB^\top$} of a SPD matrix $\vS$ for generic curvature information by extending the Riemannian idea
\citep{glasmachers2010exponential,lin2023simplifying,lincan2024} for estimating $\vS$.
Surprisingly, 
our scheme coincides with both diagonal and full-matrix AdaGrad and RMSprop up to first-order (Claim \ref{claim:eigen_full_mat_case} and Sec.~\ref{sec:diag_case_rgd}) when using the GOP as curvature information.


\item 
We then extend our scheme to Kronecker-structured spectral factorizations, i.e.\,\scalebox{0.8}{$\vS = (\vS^{(C)} \otimes \vS^{(K)})$}, where $\otimes$ denotes a Kronecker product, and \scalebox{0.8}{$\vS^{(l)}:=  \vB^{(l)} \mathrm{Diag}(\vd^{(l)}) {\vB^{(l)}}^\top$} for \scalebox{0.8}{ $l \in \{C,K\}$}, which are crucial to scale the approach.
The Kronecker structure introduces
additional challenges, which we resolve by introducing an improved factorization
\scalebox{0.8}{$\vS = \alpha (\vS^{(C)} \otimes \vS^{(K)})$} with
%
constraints \scalebox{0.8}{ $\det(\mathrm{Diag}(\vd^{(l)})) = 1$}.


%
%
%



\item 
Empirically, we demonstrate the efficacy and versatility of our approach for generic curvature learning in SPD matrix optimization \citep{absil2009optimization} and gradient-free optimization \citep{wierstra2008natural}, and showcase its efficiency and numerical stability in low-precision NN training.
Our experiments highlight the effectiveness of our scheme as a spectral-factorized adaptive method for NN training (See Fig.~\ref{fig:results}).

%
\end{itemize}

%


%
%
%
 %
 %
%
%
%
%


%
%
%
%
