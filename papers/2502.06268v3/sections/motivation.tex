\vspace{-0.15cm}
To train an NN model,
we solve an unconstrained optimization problem. The objective function of the problem is often expressed as a sum of cost functions with $N$ observations:

\vspace{-0.35cm}
\resizebox{\linewidth}{!}{
\begin{minipage}{1.1\linewidth}
\begin{align}
    \textstyle
    \min_{\mu }\, \ell (\vmu):=\sum_{i=1}^{N} c(f(\vx_i; \vmu), y_i),
    \label{eq:org_opt}
\end{align}
\end{minipage}
}

where $\vx_i$ and $y_i$ are features and a label for the $i$-th observation, respectively,
$f(\cdot;\vmu)$ is an NN with learnable weights $\vmu$,  and $c(\cdot, y_i)$ is a cost function (e.g., cross-entropy loss) to measure the mismatch between the NN's output and $y_i$.


We consider adaptive methods to solve this problem, where we estimate a preconditioning matrix using a GOP. For many well-known adaptive methods such as RMSprop \citep{tieleman2012rmsprop} (see Eq.~\eqref{eq:rmsprop} with  $\gamma=1$) and AdaGrad \citep{duchi2011adaptive} (see  Eq.~\eqref{eq:rmsprop} with  $\gamma=0$), a square root (i.e., $p=2$) is introduced before inversion.
%

\vspace{-0.4cm}
\resizebox{\linewidth}{!}{
\begin{minipage}{1.1\linewidth}
\begin{align}
\text{Diagonal}:  \vS &  \leftarrow  (1-\stepsize_2\gamma ) \vS + \stepsize_2 \mathrm{Diag} (\mathrm{diag}(\vg\vg^T)),\nonumber \\
   \vmu & \leftarrow \vmu - \stepsize_1 \vS^{-1/p} \vg, \label{eq:rmsprop}
\end{align}
\end{minipage}
}

\vspace{-0.05cm}
where 
$\diag(\cdot)$ returns a vector that contains diagonal entries
of its input,  $\mathrm{Diag}(\cdot)$ turns its input into a diagonal matrix,
$\vS$ is a diagonal matrix, $\vmu$ is the learnable weight vector,  $\vg=\nabla_\mu \ell$ is a gradient vector, and $\vg\vg^T$ is the GOP. We often estimate $\vg$ using a mini-batch of observations.
Other works improve the performance of adaptive methods on CNNs using other roots of $\vS$, for instance $p=4$ in \citet{chen2021closing} and $p=1$ in \citet{lincan2024}.
\citet{duchi2011adaptive} consider a full-matrix version of AdaGrad (i.e., $p=2$) update scheme while \citet{lincan2024} consider a variant of  AdaGrad with a $1$-root (i.e., $p=1$).

\vspace{-0.4cm}
\resizebox{\linewidth}{!}{
\begin{minipage}{1.1\linewidth}
\begin{align}
  \text{Full-matrix}:  \vS &\leftarrow  (1-\beta_2\gamma ) \vS + \beta_2 \vg\vg^T=  \vS - \beta_2 (\gamma \vS-\vg\vg^T),\nonumber\\
   \vmu &\leftarrow \vmu - \beta_1 \vS^{-1/p} \vg, \label{eq:root_free}
\end{align}
\end{minipage}
}
where a fractional matrix $p$-root is needed.


%
Kronecker-based adaptive methods, like Shampoo \citep{gupta18shampoo}, 
also include a matrix root (e.g, $p=4$) in their update rule to update matrix weights.
%
%
%
%
%
%


{\bf Challenges of Computing Matrix Roots}\,
Computing a matrix root (e.g., a square root, $p=2$) requires matrix decomposition, such as eigendecomposition. As a numerical linear algebra algorithm, matrix decomposition can be numerically unstable in \emph{discrete, finite-precision} arithmetic. Matrix decomposition is slow in single precision and unstable in half precision.
Using low-precision data types \citep{micikevicius2017mixed} is essential for training large NNs because it boosts training speed and lowers memory consumption.
Existing methods like Shampoo \citep{gupta18shampoo,shi2023distributed} and its variants \citep{vyas2024soap} often perform the matrix decomposition in single (rather than half) precision and ameliorate the cost by performing the decomposition less frequently. However, an infrequent update scheme can significantly degrade the curvature estimation  \citep{vyas2024soap}, often necessitating specific mitigations. For example, Shampoo often uses a pre-tuned Adam(W) to stabilize its infrequent update \citep{shi2023distributed,vyas2024soap} via grafting \citep{agarwallearning}. 
Iterative methods for the root computation \citep{anil2020scalable} also have been proposed
for single precision but have not yet been used in half-precision due to numerical instability \citep{shi2023distributed}.
For the Shampoo-based curvature information, 
\citet{muon2024} propose the Newton-Schulz iteration based 
on spectral norm descent \citep{bernstein2024old}.
However, this method is limited to a specific matrix root (i.e., $p=4$) and is not easy to generalize to curvature information other than the Shampoo-based curvature.


\vspace{-0.25cm}
\subsection{Riemannian Idea for Generic Curvature Learning}
\label{sec:rgd_approach}
\vspace{-0.1cm}
Although recent non-diagonal methods primarily focus on the Shampoo-based approximation, it is equally important to explore other types of curvature information to develop new non-diagonal methods. 
%
Inspired by \citet{lincan2024}, we will propose an efficient update scheme that supports generic curvature information and arbitrary matrix roots.

We first review the approach  of \citet{lincan2024}. They propose an update scheme for generic SPD  curvature estimation of $\vS$. %
When using the GOP as curvature information, they show that the scheme in \eqref{eq:root_free} with $p=1$ is a simplified version of Riemannian gradient descent (RGD) on a Gaussian manifold \citep{amari2016information}, where $\vmu$ and $\vS$ in \eqref{eq:root_free} become Gaussian's mean and inverse covariance, respectively.
Concretely, they consider a Gaussian approximation problem and use the following procedure to obtain the scheme in \eqref{eq:root_free}.

{\bf Step 1}\, They first reformulate the original problem in \eqref{eq:org_opt} as a variational Gaussian approximation problem:

\vspace{-0.35cm}
\resizebox{\linewidth}{!}{
  \begin{minipage}{1.1\linewidth}
\begin{align}
    \min_{\mu, S \succ 0}\, \mathcal{L} (\vmu,\vS):= E_{w \sim q(w;\mu,S)}[ \ell(\vw) ] - \gamma \mathcal{Q}_q,
    \label{eq:ref_opt}
\end{align}
\end{minipage}
}

\vspace{-0.25cm}
where $\gamma \in \{0,1\}$ is defined in Eq.~\eqref{eq:root_free}, $\ell(\cdot)$ is the loss function in Eq.~\eqref{eq:org_opt},  a new symbol $\vw$ is used to denote the weights of the NN   because they are no longer learnable,  $q(\vw;\vmu,\vS)$ is a Gaussian with mean $\vmu$ and covariance $\vS^{-1}$, and \scalebox{0.8}{$\mathcal{Q}_q:=E_{w \sim q}[-\log q(\vw;\vmu,\vS)]=-\half \log \mathrm{det}(\vS)$} is the Gaussian's differential entropy.


{\bf Step 2}\, They then suggest performing RGD in this  parameter space  $\vtau:=\{\vmu,\vS\}$ of the Gaussian.

\vspace{-0.4cm}
\resizebox{\linewidth}{!}{
  \begin{minipage}{1.1\linewidth}
\begin{align}
    \mathrm{RGD}: \vtau \leftarrow \vtau - \stepsize [\vF_{\tau}]^{-1} \nabla_\tau \mathcal{L},\label{eq:rgd}
\end{align} 
\end{minipage}
}

\vspace{-0.1cm}
where \scalebox{0.8}{
$\vF_{\tau}:=E_{w\sim q}[\nabla_\tau \log q(\vw;\vtau) \nabla_\tau^\top \log q(\vw;\vtau)] \in \real^{ (l+l^2)\times (l+l^2)}$ } is the Fisher-Rao metric in coordinate $\vtau$  and  $l$ is the number of NN weights.
This metric is high-dimensional. The Moore-Penrose inverse is used 
in Eq.~\eqref{eq:rgd} if $\vF_{\tau}$ is singular.
%
%

{\bf Step 3}\, Simplifying the RGD step %

\vspace{-0.2cm}
\resizebox{0.9\linewidth}{!}{
  \begin{minipage}{1.1\linewidth}
\begin{align}
    \mathrm{RGD}: &\begin{bmatrix}
         \vS \\ \vmu
    \end{bmatrix}  \leftarrow
     \begin{bmatrix}
        \vS \\ \vmu
    \end{bmatrix}  - \stepsize
    \begin{bmatrix}
 -2 \frac{\partial S }{\partial S^{-1} } & \mathbf{0}\\
 \mathbf{0} & \vS^{-1} \\
    \end{bmatrix}
    \begin{bmatrix}
\partial_S \mathcal{L} \\
\partial_\mu \mathcal{L} 
    \end{bmatrix} \nonumber\\ 
     = &
      \begin{bmatrix}
     \vS +\stepsize (2\partial_{S^{-1}} \mathcal{L}) \\
        \vmu -\stepsize \vS^{-1} \partial_\mu \mathcal{L}
    \end{bmatrix}  \approx
     \begin{bmatrix}
     \vS -\stepsize ( \gamma\vS- \vg\vg^\top ) \\
        \vmu -\stepsize \vS^{-1}  \vg
    \end{bmatrix},
    \label{eq:rgd_ada}
\end{align}
\end{minipage}
}

\vspace{-0.1cm}
gives rise to the scheme in \eqref {eq:root_free} with \scalebox{0.8}{$p=1$}, where they use (i) the analytical inverse  metric
\scalebox{0.8}{
$[\vF_{\tau}]^{-1} = \begin{bmatrix}
 -2 \frac{\partial S }{\partial S^{-1} } & \mathbf{0} \\
 \mathbf{0} & \vS^{-1},
\end{bmatrix}$}, (ii) 
Stein's estimator for the Gaussian \citep{opper2009variational}, and (iii) the GOP as a Hessian approximation \citep{lincan2024} \scalebox{0.8}{$\nabla_\mu^2 \ell \approx   \vg\vg^T$}  
with a delta evaluation at mean $\vmu$:

\vspace{-0.5cm}
\resizebox{\linewidth}{!}{
  \begin{minipage}{0.92\linewidth}
\begin{align}
& 2\partial_{S^{-1}}\mathcal{L} \mystein E_{w\sim q}[\nabla_w^2 \ell ] -\gamma  \vS \mydel \nabla_\mu^2 \ell -\gamma\vS  \approx \vg\vg^T -\gamma\vS,\nonumber \\
&     \partial_\mu \mathcal{L} \mystein E_{w \sim q}[ \nabla_w \ell] \mydel \nabla_\mu \ell =\vg. 
\label{eq:delta_approx}
\end{align}
\end{minipage}
}

\vspace{-0.35cm}
\paragraph{
Other Types of Curvature Information
}
This approach supports other types of curvature information.
For example, the RGD step becomes Newton's method \citep{khan18a}  if setting \scalebox{0.8}{$\stepsize=1$} and \scalebox{0.8}{$\gamma=1$} and using the Hessian curvature  information as  \scalebox{0.8}{$2\partial_{S^{-1}}\mathcal{L} \approx \nabla_\mu^2 \ell - \vS$}.
%
\citet{lin2021tractable} shows that this step recovers covariance adaptation evolution strategy \citep{wierstra2008natural} for gradient-free optimization when setting \scalebox{0.8}{$\gamma=0$} and using the curvature information obtained  from the REINFORCE  \citep{williams1992simple} estimation.
%
When using curvature information obtained from 
Bayesian inference,
 SPD matrix optimization, and inverse problems, this update scheme corresponds to Kalman filtering  \citep{khan2023bayesian},
RGD \citep{lin2023simplifying},
and ensemble Kalman filtering \citep{chen2024efficient}, respectively.
This step also supports the reparametrization trick \citep{lin2020handling} and Gauss-Newton approximation \citep{osawa2019practical} 
as other types of curvature information for training Bayesian NNs.
%

\vspace{-0.35cm}
\paragraph{Reparametrization Invariance} 
The Gaussian reformulation reveals a Riemannian structure hidden in  Eq.~\eqref{eq:root_free} and provides a new way to learn a decomposition of $\vS$ on the fly by 
reparametrizing $\vS$ and exploiting the reparametrization invariance of RGD.
Claim~\ref{claim:linear_invariance}—proof in Appx.~\ref{app:proof_lemma_linear}—establishes the invariance for linear unconstrained reparametrization.

\begin{claim}
\label{claim:linear_invariance}
RGD is linearly invariant in smooth, unconstrained parameter spaces, including over-parametrized spaces. 
\end{claim}

\vspace{-0.2cm}

{\bf Limitations of Existing Methods}
Generally, it is unclear if 
the reparametrization invariance holds for nonlinear and constrained reparametrizations.
\citet{lincan2024} present a positive case for directly learning a (nonlinear) Cholesky factor of \scalebox{0.8}{$\vS^{-1/p}$} with \scalebox{0.8}{$p=1$} on the fly.
However, their approach is not easy to extend to other factional 
roots (i.e., \scalebox{0.8}{$p\neq 1$}). 
Other approaches, like \citet{khan18a,
lin2020handling,
lin2021tractable,
tran2021variational,tan2021analytic,godichon2024natural}, also have difficulty efficiently computing matrix roots without matrix decomposition.
%
Moreover, most works do not preserve or demonstrate the invariance.
%

{\bf Challenges of Learning Constrained  Parametrizations via RGD}\, 
Performing RGD in a constrained coordinate is nontrivial because we have to satisfy parameter/coordinate constraints while taking a Riemannian metric into account.
For example, consider learning a spectral parameterization of $\vS=\vB \mathrm{Diag}(\vd)\vB^T$, where $\vB$ is an orthogonal matrix and $\vd$ is a vector with positive entries.
The RGD step in Eq.~\eqref{eq:rgd} does not
guarantee that these constraints are satisfied.
%
%
%
Many Riemannian approaches do not apply to the Gaussian problem because they are limited to certain constraints and tied to specific metrics.
For example, existing methods \citep{li2020efficient,kong2022momentum} only consider the orthogonal constraint for canonical metrics \citep{tagare2011notes} studied in the Riemannian optimization literature.
%
On the other hand, the Gaussian problem induces a non-standard metric for the orthogonal matrix $\vB$ and the vector $\vd$ jointly. 
The metric is necessary as it allows us to leverage the reparameterization invariance, enabling the design of efficient and stable adaptive methods.
Other methods like retraction-based methods \citep{boumal2014manopt} either use matrix decomposition to handle constraints or stick to the canonical parametrization $\vS$ for the SPD constraint, which contradicts our goal of solving the  problem with any $p$-root efficiently and stably.
%


%

{\bf Challenges of Efficiently Computing Riemannian Gradients
and  Handling a Singular Metric
}
Recall that the simplification step (i.e., Step 3) turns a computationally expensive RGD step in Eq.~\eqref{eq:rgd} involving the high-dimensional metric inversion into a more efficient adaptive update scheme.
Without an analytical metric inversion, we cannot simplify or explicitly express the RGD step as an adaptive update scheme.
When changing coordinates, the metric representation has to be changed accordingly \citep{lee2018introduction}.
%
However, an over-parameterized reparametrization can lead to a singular metric and thus complicate the metric inversion. 
For example, the metric is changed 
from block-diagonal and invertible  \scalebox{0.8}{$\vF_\tau = E_{w\sim q}[\nabla_\tau \log q(\vw;\vtau)\nabla_\tau^\top \log q(\vw;\vtau)] $} to non-block-diagonal and singular  \scalebox{0.8}{$\vF_\eta = E_{w\sim q}[\nabla_\eta \log q(\vw;\veta) \nabla_\eta^\top \log q(\vw;\veta)] $}, where \scalebox{0.8}{$\vtau=\{\vmu,\vS\}$} is a canonical coordinate and \scalebox{0.8}{$\veta=\{\vmu,\vd,\vB\}$} is a spectral coordinate.
The singularity occurs when $\vd$ has repeated entries.
Consequently, simplifying the (Moore-Penrose) inverse of this non-block-diagonal and singular metric is nontrivial. 
In other words, 
the inverse metric no longer admits an \emph{explicit}, simple, and analytical form.
%
%
The simplification step is more challenging in Kronecker-factorized cases because ambiguity (see  Sec.~\ref{sec:mat_gauss_nn}) arising from Kronecker factorization renders the metric always singular and complicates the simplification step.
Other approaches, such as implicit computation via automatic differentiation (auto-diff) \citep{salimbeni2018natural}, can result in implicit and inefficient schemes for structured cases, 
as auto-diff fails to leverage structures in both Kronecker-based $\vS$ and GOP-structured curvature for efficient computation.
%



%
%
%
%
