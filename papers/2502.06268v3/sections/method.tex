

\vspace{-0.2cm}
Our goal is to propose training schemes that support
generic curvature information and
offer the flexibility to apply arbitrary matrix roots and other operations needed for other applications, such as log-det, at a low cost.
To do so, we propose directly learning $\vB$ and $\vd$ in a spectral parametrization of \scalebox{0.8}{ $\vS = \vB \mathrm{Diag}(\vd) \vB^{\top}$} \emph{without} storing $\vS$, where $\vB$ is an orthogonal square matrix and $\vd$ is a vector with positive entries.
%
Our learnable spectral factor allows us to easily compute any fractional root and inversion \scalebox{0.8}{ $\smash{\vS^{-\nicefrac{1}{p}}=\vB \mathrm{Diag}(\vd^{-\nicefrac{1}{p}}) \vB^\top}$} through elementwise roots on $\vd$, instead of matrix roots on $\vS$.
We then extend our approach to Kronecker cases to support large-scale applications. Our approach is efficient as updating $\vB$ and $\vd$ does not involve any matrix decomposition.

Our starting point is that the adaptive method in \eqref{eq:root_free} is the Riemannian solution to the Gaussian problem discussed in Sec.~\ref{sec:rgd_approach}.
Because RGD is invariant under coordinate transformations (see  Claims \ref{claim:linear_invariance}, \ref{claim:eigen_full_mat_case}), our idea is to change coordinates so that the Riemannian solution becomes an adaptive update rule for spectral coordinates.
We propose using local coordinates to overcome the challenges of using spectral-constrained coordinates (see  Sec.~\ref{sec:rgd_approach}).

{\bf Constraint Satisfaction and Metric Diagonalization  via \emph{Local} Coordinate Transformations}
Our local coordinates are inspired by generalized  Riemannian (normal) local coordinates \citep{glasmachers2010exponential,lin2023simplifying} and  Fermi coordinates \citep{manasse1963fermi}.
The main idea is to construct local coordinates that handle constraints and facilitate the analytical metric inversion needed for the simplification. 
Using a local coordinate transformation can \emph{iteratively} diagonalize the metric at a \emph{single} evaluation. 
Given a global coordinate, such as a spectral coordinate, we construct, use, and discard a local coordinate system and its coordinate transformation map to the global coordinate at every iteration.
In our approach, the coordinate and its transformation map satisfy these conditions: (i) The map is differentiable 
and satisfies the constraints.
(ii) The local coordinate has no coordinate constraint for
performing RGD  and its origin represents a current iterate in the (global) spectral coordinate.
(iii) The metric evaluated at the origin is an easy-to-inverse matrix, such as an identity \citep{lin2023simplifying} or a diagonal matrix \citep{glasmachers2010exponential}.

{\bf Our Technical Contributions}\,
Existing local coordinates neither account for spectral constraints nor simplify the inverse of the \emph{singular} metric for spectral parameterizations.
We propose new local coordinates for spectral parameterization and demonstrate how they enable spectral-factorized adaptive schemes by satisfying spectral constraints and simplifying the inverse metric computation.
Moreover, we demonstrate that our (nonlinear) scheme preserves the reparametrization invariance (see Claim~\ref{claim:eigen_full_mat_case}).
We also extend our schemes for Kronecker cases.

{\bf Versatility}\,
While we focus on NN training using the GOP, our schemes are suitable for other curvature and applications mentioned in Sec.~\ref{sec:rgd_approach}. For example, our schemes (see  Sec.~\ref{sec:expriments}) can solve gradient-free optimization \citep{wierstra2008natural} and SPD matrix optimization \citep{absil2009optimization}.


\vspace{-0.05cm}
{\bf Notations}
To handle the orthogonal constraint in $\vB$, we use this Cayley map \scalebox{0.8}{$\mathrm{Cayley}(\vN):=(\vI+\vN)(\vI-\vN)^{-1}$}.
Claim~\ref{claim:cayley_map} in the appendix shows why the inversion in the Cayley map always exists.
%
We introduce a map denoted by \scalebox{0.8}{ $\mathrm{Skew}(\vM):=\vM-\vM^\top$} to make its input $\vM$ skew-symmetric as required by the Cayley map and a lower-triangular restriction denoted by \scalebox{0.8}{$\mathrm{Tril}(\vC)$} to make its input lower-triangular.



\begin{figure*}[!t]
\center
\begin{minipage}[t]{.5\linewidth}
\fbox{	
\begin{minipage}[t]{0.9\linewidth}
		\textbf{Full-matrix  \scalebox{0.8}{   ($\vS = \vB\mathrm{Diag}(\vd)\vB^T$)} }

        \begin{algorithmic}[1]
        %
        \STATE
        \footnotesize  Compute gradient $\vg\coloneqq\nabla \ell(\vmu)$  \\
             \scalebox{0.8}{
             $
            {\vd}  \leftarrow \vd \odot \exp\{ \stepsize_2 \vd^{-1} \odot [ -\gamma \vd +    \mathrm{diag}(\vB^T  \vg\vg^T \vB )  ]  \}
             $
             } \\
             \scalebox{0.72}{   $
             \vB \leftarrow \vB\mathrm{Cayley}(\frac{\stepsize_2}{2}( \mathrm{Skew}(\mathrm{Tril}(\vU))) )
             $
             }
            \STATE
          \scalebox{0.8}{    $
          \vmu \leftarrow \vmu  - \stepsize_1 \vB \mathrm{Diag}(\vd^{-1/p})\vB^T \vg
 $}
					%
				\end{algorithmic}
            \end{minipage}
}

\vspace{0.25cm}
\fbox{
\begin{minipage}[t]{0.9\linewidth}
\textbf{Kronecker (\scalebox{0.6}{$\vS = {\alpha } [\vB^{(C)} \mathrm{Diag}(\vd^{(C)}) (\vB^{(C)})^T] \otimes [\vB^{(K)} \mathrm{Diag}(\vd^{(K)}) (\vB^{(K)})^T]$
   })}	
   \vspace{-0.4cm}
   \begin{algorithmic}[1]
            \STATE
            \footnotesize  Compute gradient \scalebox{0.8}{$\vG\coloneqq \mathrm{Mat}(\nabla \ell(\vmu)) \in \mathcal{R}^{n \times m}$ } \\
             \scalebox{0.78}{   $     \vm^{(l)} = (\vd^{(l)})^{-1} \odot [ - \gamma\vd^{(l)} + \frac{1}{ {\alpha} k^{(l)}}   \mathrm{diag}(\vW^{(l)} )  ]$ } \\
         \scalebox{0.78}{   $ \vd^{(l)}  \leftarrow \vd^{(l)} \odot \exp\{
             \stepsize_2 [\vm^{(l)} - \mathrm{mean}(\vm^{(l)})]
             \} $} \\
             \scalebox{0.8}{ $ \vB^{(l)} \leftarrow \vB^{(l)} \mathrm{Cayley}(\frac{\stepsize_2}{ {2\alpha} k^{(l)} }\mathrm{Skew}( \mathrm{Tril}(\vU^{(l)})  ) ) $ } \\
             \scalebox{0.8}{   $
             \alpha \leftarrow \alpha \exp( \frac{\stepsize_2}{2 } [\mathrm{mean}(\vm^{(C)}) + \mathrm{mean}(\vm^{(K)})] )
             $ } \\
            \STATE
          \scalebox{0.8}{    $\mathrm{Mat}(\vmu)
           \leftarrow \mathrm{Mat}(\vmu) - \stepsize_1 \big({ \alpha^{-1/p}} \big)(\vS^{(C)})^{-1/p}\vG (\vS^{(K)})^{-1/p}
 $}
					%
				\end{algorithmic}
        \end{minipage}
}    
\end{minipage}
\begin{minipage}[t]{.45\linewidth}
\vspace{-0.6cm}
 \caption{
Adaptive update schemes for full-matrix and Kronecker-based spectral factorizations. 
{\bf Full-matrix scheme:} \scalebox{0.8}{ $\mathrm{Tril}(\vU)$} is a lower-triangular matrix with the $(i,j)$-th entry \scalebox{0.8}{ $[\vU]_{ij}:= -[\vB^T \vg\vg^T \vB]_{ij}/(d_i - d_j) $ } when $d_i \neq d_j$ and $0$ otherwise. {\bf  Kronecker-based scheme: } 
%
We assume that NN weights take a matrix form:\scalebox{0.8}{ $\vM:=\mathrm{Mat}(\vmu) \in \real^{n \times m}$}, where \scalebox{0.8}{$\mathrm{Mat}(\cdot)$} is a matrix representation of vector $\vmu$.
We define  \scalebox{0.78}{$\vW^{(K)} :=(\vB^{(K)})^T \vG^T { (\vS^{(C)})^{-1} } \vG \vB^{(K)}$,  $\vW^{(C)} :=(\vB^{(C)})^T \vG { (\vS^{(K)})^{-1} } \vG^T \vB^{(C)}$,} \scalebox{0.8}{
$k^{(K)}:=n$ and 
$k^{(C)}:=m$ }.
  where \scalebox{0.8}{ $\vS^{(l)} := \vB^{(l)} \mathrm{Diag}(\vd^{(l)}) (\vB^{(l)})^T $ } for \scalebox{0.8}{$l \in \{C,K\}$}.
%
We define a lower-triangular matrix \scalebox{0.8}{$\mathrm{Tril}(\vU^{(l)})$ }  with its $(i,j)$-th entry \scalebox{0.8}{ $[{\vU^{(l)} }]_{ij}:= -[W^{(l)}]_{ij}/(d^{(l)}_i - d^{(l)}_j) $} if \scalebox{0.8}{$d^{(l)}_i\neq d^{(l)}_j $} and $0$ otherwise, where $d^{(l)}_i$ denotes the $i$-th entry of vector $\vd^{(l)}$.  
For numerical stability, we set \scalebox{0.8}{$[\vU^{(l)}]_{ij}=0$} if \scalebox{0.8}{$|d^{(l)}_i - d^{(l)}_j|$} is near $0$.
%
See Fig~\ref{fig:kron_updates} in Appx.~\ref{app:connections} for a simplified version.
}\label{fig:kronecker}
\end{minipage}
\vspace{-0.5cm}
\end{figure*}



\vspace{-0.3cm}
\subsection{Full-matrix Schemes via Full Gaussian Approx.}
\label{sec:full_mat_update}
\vspace{-0.15cm}
We present our scheme in the context of full-matrix preconditioners and use the GOP as curvature information. While full-matrix methods are generally impractical for modern NNs, this will serve to illustrate the core ideas that we later apply to structured cases.
Our scheme is given in the top box of Fig.~\ref{fig:kronecker}.
It is a spectral-factorized adaptive scheme for solving problem \eqref{eq:org_opt} without matrix decomposition.
Thus, our scheme enables more efficient and stable root computation than the original one in Eq.~\eqref{eq:root_free}.
Surprisingly, our scheme coincides with the original one (up to first order):

\vspace{-0.05cm}
\begin{claim}
\label{claim:eigen_full_mat_case}
 \textbf{Reparametrization Invariance:} 
  Our update scheme for $\vS$ in the top box of Fig~\ref{fig:kronecker} is equivalent to the scheme in Eq.~\eqref{eq:root_free} up to first-order in terms of $\beta_2$ when $\vd$ does not have repeated entries (proof in Appx.~\ref{app:proof_lemma_full}).
\end{claim}

\vspace{-0.2cm}
To obtain the update scheme (top box of Fig.~\ref{fig:kronecker}), we consider the following procedure to convert an RGD step into an adaptive scheme.
Our procedure follows similar steps in Sec.~\ref{sec:rgd_approach} except for using local coordinates.

\vspace{-0.05cm}
{\bf Step 1}\, We consider a Gaussian problem in \eqref{eq:ref_opt} using 
a learnable spectral parametrization, \scalebox{0.8}{
 $\vS=\vB\mathrm{Diag}(\vd)\vB^T$}, of the inverse covariance $\vS$, where \scalebox{0.8}{$\mathcal{Q}_q=-\half \log \mathrm{det}(\vS)=-\half \sum_i (\log d_i)$}.
 
\vspace{-0.4cm}
\resizebox{0.85\linewidth}{!}{
  \begin{minipage}{\linewidth}
\begin{align}
    \min_{\mu, d, B }\, & \mathcal{L} (\vmu,\vd,\vB):= E_{w \sim q(w;\mu,d,B)}[ \ell(\vw) ] - \gamma \mathcal{Q}_{q}, \nonumber \\
    \textrm{s.t.}\, & \vB\vB^T = \vB^T\vB =\vI\, \text{ and } \vd >0 .
    \label{eq:eig_opt}
\end{align}
\end{minipage}
}

{\bf Step 2}\, We construct a new local coordinate at each iteration to remove the (spectral) constraints in Eq.~\eqref{eq:eig_opt}  when performing RGD.
We then take an unconstrained RGD step in this local coordinate, translate the update to the spectral coordinate, and discard the local coordinate. This coordinate generation process shares the same spirit as Cartan's method of moving frames \citep{ivey2003cartan}.


{\bf Step 2.1}\, Concretely, at iteration $k$, we create a local coordinate $\veta:=(\vdelta,\vm,\vM)$ at the current point $\vtau_k:=(\vmu_k,\vd_k, \vB_k)$ and use this local transformation map

 \vspace{-0.3cm}
\resizebox{0.95\linewidth}{!}{
  \begin{minipage}{1.2\linewidth}
\begin{align}
\vtau(\veta;\vtau_k) :=
\begin{bmatrix}
\vd (\vm;\vtau_k)  \\
\vB (\vM;\vtau_k) \\
\vmu (\vdelta;\vtau_k) \\
\end{bmatrix} = \begin{bmatrix}
    \vd_{k} \odot \exp( \vm ) \\
\vB_{k} \mathrm{Cayley}(\mathrm{Skew}(\mathrm{Tril}(\vM))) \\
    \vmu_k + \vB_k \mathrm{Diag}(\vd_k^{-1/2}) \vdelta 
\end{bmatrix},
\label{eq:translation_map}
\end{align}
\end{minipage}
}

\vspace{-0.2cm}
to translate the change from the local coordinate to the spectral coordinate (see  Claim~\ref{claim:eig_constraint2}), where $\vtau_k=(\vmu_k,\vd_k,\vB_k)$ is considered as a constant in this map and $\odot$ denotes the elementwise product. %
Inspired by the Riemannian normal coordinates \citep{lin2023simplifying},
we construct the map \eqref{eq:translation_map} so that Claims 
\ref{claim:eig_constraint2}
and 
\ref{claim:fisher_full} are satisfied.
%

\begin{claim}
\label{claim:eig_constraint2}
The map in \eqref{eq:translation_map} satisfies the constraints in \eqref{eq:eig_opt}.
\end{claim}
\vspace{-0.1cm}

\begin{comment}
\begin{claim}
\label{claim:eig_constraint}
{\bf Uniqueness of the Local Coordinates}
The map in  \eqref{eq:translation_map}, which is like the eigendecomposition,
is one-to-one if $\vd$ has no repeated entries
(see Appx.~\ref{app:claim4_proof} for a proof).
\end{claim}
\end{comment}

{\bf Step 2.2}\,
We then take an (unconstrained) RGD step in the local coordinate $\veta$,

 \vspace{-0.5cm}
\resizebox{0.9\linewidth}{!}{
  \begin{minipage}{\linewidth}
\begin{align}
    \mathrm{RGD}: \, \veta_{\text{new}}& \leftarrow \veta_{\text{cur}} - \stepsize [\vF_\eta(\veta_\text{cur}) ]^{-1} \nabla_\eta \mathcal{L}\big|_{\eta:=\eta_{\text{cur}}}, \nonumber \\
    & =\mathbf{0} - \stepsize [\vF_\eta(\mathbf{0}) ]^{-1} \nabla_\eta \mathcal{L}\big|_{\eta:=0} ,
    \label{eq:ngd_local_full}
\end{align}
\end{minipage}
}

\vspace{-0.2cm}
and translate the change $\veta_{\text{new}}$
from the local coordinate

 \vspace{-0.45cm}
\resizebox{0.95\linewidth}{!}{
  \begin{minipage}{\linewidth}
\begin{align}
%
    \vtau_{k+1} \leftarrow \vtau(\veta_{\text{new}};\vtau_k),
    \label{eq:ngd_global_full}
\end{align}
\end{minipage}
}

to the spectral coordinate, where
the Fisher-Rao metric $\vF_\eta(\veta_\text{cur})$ evaluated at $\veta_{\text{cur}}$ is diagonal  (see  Claim~\ref{claim:fisher_full}) in the local coordinate and the origin
$\veta_{\text{cur}} \equiv \mathbf{0}$ represents the current \scalebox{0.9}{ $\vtau_k=\vtau(\veta_{\text{cur}};\vtau_k)\equiv\vtau(\mathbf{0};\vtau_k)$} in the spectral coordinate.
Notatbly, evaluating the metric at the origin can greatly simplify the metric computation. 

%



{\bf Step 3}\, We obtain the scheme in
the top box of Fig.~\ref{fig:kronecker} with $p=1$
by simplifying RGD in \eqref{eq:ngd_local_full}-\eqref{eq:ngd_global_full}, and making the approximations in \eqref{eq:delta_approx}
(see  Appx.~\ref{sec:deri_full}).
Notably, this approximation can be changed to include other types of curvature information while keeping the root computation efficient.
Our approach efficiently computes  other $p$-roots while existing works \citep{lin2021tractable,
lin2023simplifying,
lincan2024,
tran2021variational,
tan2021analytic,godichon2024natural} are limited to the $1$-root.

Our procedure satisfies parameter constraints (see  Claim \ref{claim:eig_constraint2}) and simplifies the metric inversion (see  Claim  \ref{claim:fisher_full}). 
The simplification is easy because the metric
\scalebox{0.8}{
$\mathbf{F}_\eta(\veta_\text{cur}):=\mathbf{F}_\eta(\veta)\big|_{\eta=\eta_\text{cur}}$} evaluated at the origin (in the local coordinate) is diagonal. 
The metric diagonalization allows us to simplify the inverse metric computation in Eq.~\eqref{eq:ngd_local_full}
even when the metric is singular (see Appx.~\ref{app:handling_repated_d} for a discussion).
Moreover, the gradient 
\scalebox{0.8}{
$\nabla_\eta \mathcal{L}\big|_{\eta=\eta_{\text{cur}}} $}
%
required by RGD is easy to compute via the chain rule and has an analytical expression (see  Appx.~\ref{sec:deri_full}). 
Furthermore, our update scheme supports other types of curvature information arising from applications, such as gradient-free optimization and SPD matrix optimization (see Sec.~\ref{sec:expriments}).


\begin{claim}
\label{claim:fisher_full}
{\bf{Closed-form Metric Diagonalization}}
    The exact Fisher-Rao metric $\vF_\eta(\veta_\text{cur})$ (for a full-matrix Gaussian) evaluated at the origin \scalebox{0.8}{ $\veta_\text{cur}\equiv\mathbf{0}$} is \emph{diagonal} and has a closed-form expression   (see Eq.~\eqref{eq:fim_full_local} in  Appx.~\ref{app:claim_fisher_full}).
\end{claim}

\begin{figure*}
  \centering
  \includegraphics[width=0.92\linewidth]{figs/fig_toy_merged.pdf}
  \vspace{-3ex}
  \caption{
  Empirical validation of our update schemes for SPD curvature learning.
{\bf Full-matrix Scheme: } 
  The first plot on the left shows that the scheme converges to a fixed-point solution as fast as the default scheme in Eq.~\eqref{eq:root_free} (with $\gamma=1$) to update \scalebox{0.8}{ $\vS\in \real^{100 \times 100}$} and the Cholesky-based scheme. 
  The second plot illustrates how closely our scheme matches the iterates generated by the default update scheme at each iteration.
  %
{\bf Kronecker-based Scheme: } 
  The third plot shows that our update scheme gives a structural approximation \scalebox{0.8}{ $ \vS^{(C)} \otimes \vS^{(K)}$} of a fixed-point solution obtained by the default full-matrix update scheme for \scalebox{0.8}{$\vS \in \mathcal{R}^{99 \times 99}$}, where \scalebox{0.8}{$\vS^{(C)} \in \real^{9 \times 9} $} and \scalebox{0.8}{ $ \vS^{(K)} \in \real^{11 \times 11}$}.
  Our scheme converges as fast as Kronecker-structured baseline methods, including the impractical projection-based method.
  The last plot illustrates how closely our scheme matches the unstructured iterates generated by the default one at each iteration. 
   See Figs.~\ref{fig:full_mat_toy}-\ref{fig:kron_toy} for more results.
  }
 \label{fig:full_merged}
  \vspace{-0.5cm}
\end{figure*}

\begin{comment}
\vspace{-0.2cm}
{\bf  Leveraging the GOP structure}
Since the GOP is semi-definite and $\stepsize_2$ is small, we obtain a linear update for $\vd$:
%
\scalebox{0.85}{
$\vd \odot \exp( \stepsize_2 \vd^{-1} \odot \vh ) \approx  (1-\stepsize_2\gamma)\vd + \stepsize_2 \mathrm{diag}(\vB^T \vg\vg^T \vB)$
} by truncating the exponential map 
\scalebox{0.9}{ $
\vd \odot \exp(\stepsize_2 \vd^{-1} \odot \vh )$} \scalebox{0.9}{$  \approx \vd \odot (1+ \stepsize_2 \vd^{-1} \odot \vh) = \vd + \stepsize_2 \vh
$ }
while keeping $\vd$ positive, where \scalebox{0.9}{ $\vh:=-\gamma \vd + \mathrm{diag}(\vB^T \vg\vg^T \vB)$}.
This makes our approach more similar to the original update in Eq.~\eqref{eq:root_free}. 
\end{comment}


\vspace{-0.35cm}
\subsection{
Connections to Diagonal Adaptive  Methods}
\label{sec:diag_case_rgd}
\vspace{-0.2cm}
Our full-matrix scheme in Fig.~\ref{fig:kronecker} also applies in diagonal cases by forcing $\vB$ to be a diagonal matrix. 
We achieve that by changing map $\mathrm{Tril}(\cdot)$ to $\mathrm{Diag}(\cdot)$ in the update rule. Consequently, $\vB$ becomes a constant identity matrix (up to sign changes) that can be ignored.
Similar to the full matrix case, we can obtain this scheme through a diagonal Gaussian approximation.
When truncating the exponential map, our scheme becomes the 
$1$-root variant of RMSprop and AdaGrad  \citep{lincan2024} for $p=1$.
If applying other fractional $p$-roots, our scheme also recovers RMSprop and AdaGrad for $p=2$ and the fractional diagonal method \citep{chen2021closing} for $p=4$. See Appx.~\ref{app:connection_diag} for the details.




\vspace{-0.35cm}
\subsection{Kronecker Schemes via Matrix Gaussian Approx.}
\label{sec:mat_gauss_nn}
\vspace{-0.2cm}
Using Kronecker-structured preconditioners \citep{martens2015optimizing,li2017preconditioned,
gupta18shampoo} is necessary for large models, as a full-matrix preconditioner is too large to store.
Many Kronecker-based methods \citep{zhang2018noisy,ren2021tensor,lin2023simplifying,lincan2024} are based on a (matrix) Gaussian family with Kronecker-structured inverse covariance \scalebox{0.8}{$\vS=\vS^{(C)} \otimes \vS^{(K)}$}.
Thus, we want to extend the procedure in Sec.~\ref{sec:full_mat_update} to Kronecker cases. 
However, the Fisher-Rao metric for a Kronecker structure is singular
and non-block-diagonal because the Kronecker factorization is not unique.
Thus, we cannot use the procedure in Sec.~\ref{sec:full_mat_update} to obtain an adaptive scheme.
%
%
%
Existing works such as \citet{zhang2018noisy,
lin2019fast,
lincan2024} make an additional approximation for the metric to tackle this.
Instead, we overcome this without approximation by imposing a determinant constraint on each Kronecker factor and introducing a learnable scalar $\alpha$ to make the factorization unique (see  Claim~\ref{claim:kron_unique}) and simplify the inverse metric computation.


\vspace{0.05cm}
\begin{claim}
\label{claim:kron_unique}
    A Kronecker-structured positive-definite matrix $\vS$ can be uniquely expressed as  
    \scalebox{0.8}{ $\vS=\alpha [\vS^{(C)} \otimes \vS^{(K)}]$ } with constraints  \scalebox{0.8}{$\mathrm{det}(\vS^{(C)})=\mathrm{det}(\vS^{(K)})=1$ } and \scalebox{0.8}{$\alpha>0$}. 
    (see Appx.~\ref{app:claim6_proof})
\end{claim}

%
%

\vspace{-0.2cm}
We then propose a spectral parametrization and local coordinates for each Kronecker factor 
 \scalebox{0.8}{
$\vS^{(l)}=\vB^{(l)}\mathrm{Diag}(\vd^{(l)})(\vB^{(l)})^T$}
for  \scalebox{0.8}{$l \in \{C,K\}$}, where 
 \scalebox{0.8}{
$\mathrm{det}(\mathrm{Diag}(\vd^{(l)}))=1$} is  the  determinant constraint.
Thanks to our local coordinates, we can again  simplify the inverse metric computation (see  Claim~\ref{claim:fisher_kron}).
%
%
We then follow a similar procedure in Sec.~\ref{sec:full_mat_update} to obtain an update scheme for Kronecker cases (see bottom box of Fig.~\ref{fig:kronecker}).

\vspace{-0.05cm}
{\bf Handling New Constraints via \emph{Local} Transformations}
For the positive scalar $\alpha$, we introduce a local coordinate $n$ and use an exponential map in the coordinate transformation: $\alpha(n; \alpha_k) = \alpha_k \exp(n)$ at each iteration $k$.
 We drop the Kronecker factor index $l$ for simplicity. The coordinate transformation map for each factor is similar to the map in full-matrix cases, expect that for vector 
\scalebox{0.9}{
$\vd(\vm; \vd_k
    )  = \vd_{k} \odot \exp( \vm )$}, 
we require  \scalebox{0.9}{$\mathrm{sum}(\vm)=0$} to satisfy the determinant constraint (i.e.,
\scalebox{0.8}{$\mathrm{det}(\mathrm{Diag}(\vd(\vm)))=1$}), where $j$ is the length of vector $\vd_k$ and the local coordinate 
\scalebox{0.8}{
$\vm := [m_1,\dots,m_{j-1}, -\sum_{i}^{j-1}m_i]$} has only \scalebox{0.8}{$(j-1)$} free variables.


\begin{claim}
\label{claim:fisher_kron}
{\bf{Closed-form Metric Block Diagonalization}}
    The Fisher-Rao metric \scalebox{0.8}{ $\vF_\eta(\veta_\text{cur})$} (for a  matrix Gaussian) 
 and its inverse evaluated at \scalebox{0.8}{$\veta_\text{cur}\equiv\mathbf{0}$} are block-diagonal and have closed form (see Eq.~\eqref{eq:fim_kron_local} in Appx.~\ref{app:kron_fim}).
\end{claim}

\begin{comment}
\vspace{-0.25cm}
{\bf  Leveraging the GOP structure}
Similar to the full-matrix case, we obtain the following update by leveraging the GOP structure while satisfying the constraints in $\vd^{(l)}$ and $\alpha$.

\vspace{-0.45cm}
\resizebox{0.92\linewidth}{!}{
  \begin{minipage}{\linewidth}
\begin{align*}
    \vm^{(l)} & = (1-\stepsize_2\gamma ) \vd^{(l)} +  \frac{\stepsize_2}{ {\alpha} k^{(l)}}  \mathrm{diag}(\vW^{(l)} ) \\
   \vd^{(l)} & \leftarrow \exp( \log( \vm^{(l)} ) -\mathrm{mean}(\log( \vm^{(l)} )) )\\
  \alpha & \leftarrow \exp\big(  \mathrm{mean}(\log( \vm^{(C)} ))/2 + \mathrm{mean}(\log( \vm^{(K)} ))/2
  \big) 
\end{align*}
\end{minipage}
}

where  \scalebox{0.85}{$k^{(l)}$} and  \scalebox{0.85}{$\vW^{(l)}$} are defined at the caption of Fig.~\ref{fig:kronecker} and \scalebox{0.85}{$l \in \{C,K\}$}.
See Appx.xxx for more details.  \todo{add the appendix}
\end{comment}

%
%


\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{figs/other_res.pdf}
  \vspace{-6ex}
  \caption{
  Experiments showcase the efficacy and versatility of our approach for generic curvature learning.
   Our update scheme matches the equivalent Riemannian baselines, empirically illustrating the reparametrization invariance.
  {\bf SPD Matrix Optimization:}
  The first two plots on the left show the performance of our full-matrix update scheme for learning SPD matrices. Our update scheme matches the baselines, as our scheme is RGD in local coordinates.
  {\bf Gradient-free Optimization:}
  The last three plots show the performance of our scheme for gradient-free optimization problems on Ackley (multimode), Rosenbrok (flat valley), and Griewank (multimode) functions.
  See Fig.~\ref{fig:extra_results} in the appendix for more results.
  }
  \label{fig:other_res}
  \vspace{-0.5cm}
\end{figure*}

\vspace{-0.35cm}
\subsection{Practical Considerations for NN Training}
\label{sec:practical_NN}
\vspace{-0.2cm}
{\bf Using a Truncated Cayley Map
for Low-precision Training
}
Recall that our schemes in Fig.~\ref{fig:kronecker} use the Cayley map that involves matrix inversion.
To  work with half-precision and further reduce the computational cost, 
we use a truncated Cayley map, similar to \citet{  liu2021orthogonal,li2020efficient,qiu2023controlling}, 
for NN problems. 
Our truncation is based on a Neumann series for the matrix inversion \citep{krishnan2017neumann,lorraine2020optimizing,qiu2023controlling}. This is possible because we can approximate the matrix inversion in the Cayley map
%
\scalebox{0.8}{
 $ \mathrm{Cayley}(\stepsize \vN) = (\vI+\stepsize \vN) (\vI-\stepsize\vN)^{-1}
 = (\vI+\stepsize \vN) \prod_{l=0}^{\infty} (\vI+ (\stepsize\vN)^{2^l}) $} \scalebox{0.8}{ $ \approx (\vI+\stepsize \vN)^2 (\vI+ (\stepsize \vN)^2)(\vI+ (\stepsize \vN)^4)
 $}
%
based on a convergent Neumann series,  when $\stepsize$ is small enough so that the Frobenius norm $\|\stepsize \vN\|_{\text{Frob}}<1$.
Consequently, we use a nonconstant step size, similar to \citet{lincan2024}, to keep the norm bounded as required by the truncation.
Fig.~\ref{fig:exact_vs_truncated} in the appendix shows that using the truncation significantly reduces running time while maintaining performance.



\vspace{-0.4cm}
\paragraph{
Stabilizing Training using Preconditioned Gradient Clipping or Grafting}
For training transformers, it is common practice to use preconditioned gradient clipping \citep{liu2023sophia,shen2024variational} or grafting \citep{agarwal2019efficient,shi2023distributed,vyas2024soap} so that
the norm of their descent direction is either bounded layerwisely \citep{zhang2024transformers} or bounded as the norm of Adam \citep{shi2023distributed,vyas2024soap}.
This is needed because of heavy-tailed noise \citep{zhang2020adaptive}. We use the preconditioned gradient clipping in our experiments to train transformers.
Other approaches like sign descent \citep{chen2023symbolic}, normalized gradient descent \citep{cutkosky2020momentum}, and 
spectral norm descent
\citep{bernstein2024old,muon2024}
also implicitly bound the norm of their descent direction.



%

%
%
%
%
