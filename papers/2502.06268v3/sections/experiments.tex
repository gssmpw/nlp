


%


%


\vspace{-0.2cm}
\subsection{SPD Curvature Learning for the GOP}
\vspace{-0.15cm}
{\bf Empirical Validation of the Full-matrix Update Scheme}
First, we empirically evaluate our scheme for
using the GOP as curvature information to demonstrate how our update scheme relates to the default scheme (see Eq.~\eqref{eq:root_free}) by indirectly learning
\scalebox{0.8}{$\vS^{\text{(spectral)}}:=\vB\mathrm{Diag}(\vd)\vB^T$}.
We compare our scheme to the default one on $\vS$ as
\scalebox{0.8}{$ \vS_{k+1}^{\text{(default)}} \leftarrow (1-\stepsize)  \vS_{k}^{\text{(default)}} + \stepsize \vg_k\vg_k^T$}, and the inverse-free scheme \citep{lincan2024} for a learnable Cholesky factor \scalebox{0.8}{$\vC$} of \scalebox{0.8}{$\vS^{-1}$} (i.e., \scalebox{0.8}{ $\vS^{\text{(cholesky)}}:=(\vC\vC^T)^{-1}$}).
%
We focus on the curvature estimation of $\vS$ based on a fixed gradient sequence $\{\vg_1,\dots,\vg_T\}$ and initialized by the same $\vS_0$. 
We consider two scenarios:
(1) fixed-point matching and (2) iterate matching. 
%

\vspace{-0.3cm}
\begin{description}
 \item[Fixed-point matching]\,
 The ground truth  is a fixed-point solution, \scalebox{0.8}{ $\vS_{*} = E[\vg\vg^T]=\vSigma$}, to the default update scheme as \scalebox{0.8}{ $\vS_{*}=(1-\stepsize)\vS_{*} + \stepsize \vg_k\vg_k^T $}, where
  $\vg_k$ is independently generated from a Gaussian distribution \scalebox{0.8}{$\vg_k \sim \gauss(\mathbf{0}, \vSigma)$} at each iteration $k$.
We evaluate each scheme at iteration $k$ by comparing its current estimate denoted by \scalebox{0.8}{$\vS_k^{\text(est)}$} to the fixed point.
We use a relative Frobenius norm 
\scalebox{0.9}{
$\frac{ \|\vS_* - \vS_k^{\text{(est)}}\|_\text{Frob}}{\|\vS_*\|_\text{Frob}}$ }
to measure the difference.


\vspace{-0.2cm}
\item[\bf Iterate matching]
The ground truth is a sequence of matrices \scalebox{0.8}{ $\{\vS_1^{\text{(true)}},\dots,\vS_T^{\text{(true)}}\}$}
generated by the default scheme when applying the scheme to the gradient sequence.
We want to match the iterate the default scheme generates at every step. 
We use a relative Frobenius norm 
\scalebox{0.9}{
$\frac{ \|\vS_k^{\text{(true)}} - \vS_k^{\text{(est)}}\|_\text{Frob}}{\|\vS_k^{\text{(true)}}\|_\text{Frob}}$} 
to measure the discrepancy between an update scheme and the default one at every iteration $k$.
\end{description}

\vspace{-0.4cm}
From the first two plots in  Fig.~\ref{fig:full_merged}, we can see that our update scheme performs similarly to the default update scheme in the two scenarios. Together with Claim~\ref{claim:eigen_full_mat_case}, these results show that our scheme
coincides with the default one theoretically and empirically.
%
See Appx.~\ref{app:extra_empirical_valida} and Fig.~\ref{fig:full_mat_toy} for more details.


\vspace{-0.2cm}
\paragraph{Empirical Evaluation of the Kronecker-based Update Scheme}
We compare our Kronecker scheme to the default full-matrix scheme on $\vS$: 
$ \vS_{k+1} \leftarrow (1-\stepsize)  \vS_{k} + \stepsize \vg_k\vg_k^T$. %
As baselines, we consider the curvature estimation used in the structured Cholesky factorization \citep{lincan2024}, and an impractical projection-based method~\citep{van1993approximation}: \scalebox{0.8}{ $(\vS_{k+1}^{(C)}, \vS_{k+1}^{(K)}) \leftarrow \mathrm{Proj}( (1-\beta)( \vS_k^{(C)} \otimes \vS_k^{(K)}) + \beta \vg_k\vg_k^T ) $}.
We use a similar experimental setup and consider two similar scenarios
discussed in the full-matrix case.
Here, we initialized all update schemes by a Kronecker structured matrix $\vS_0$ to remove the difference introduced by initialization.

From the last two plots in  Fig.~\ref{fig:full_merged}, we can see that our structural scheme performs as well as existing structural baselines. Our approach performs similarly to the impractical method that requires storing a full matrix and solving a projection optimization problem at every iteration. This illustrates the effectiveness of our approach in Kronecker cases.
See Appx.~\ref{app:extra_empirical_valida} and Fig.~\ref{fig:kron_toy} for more details.

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth]{figs/nn_results.pdf}
  \vspace{-5ex}
  \caption{
  Experiments demonstrate the efficiency of our update schemes for low-precision NN training. 
  The plots show the performance of our Kronecker-based scheme for training vision transformers with half precision. 
  All models are trained for 210 epochs, including 10 epochs for warmup.
  For SOAP and our method, we update
  their preconditioners every two iterations.
  SOAP performs much slower than the other methods because it has to run in single precision to use matrix decomposition. 
  Using a different matrix root can affect the performance.
  Our method not only matches Muon's performance but also
  opens the door to using curvature information and matrix roots beyond Muon.
  See  Figs.~\ref{fig:imagewoof} and \ref{fig:imagenet25} in Appx.~\ref{app:extra_nn_training} for a comparison of the methods based on iteration efficiency and wall-clock time. 
 \vspace{-0.4cm} 
  }
  \label{fig:results}
\end{figure*}


\vspace{-0.2cm}
\subsection{Full-matrix Case: SPD Matrix Optimization}
\vspace{-0.15cm}
We consider SPD matrix optimization problems to demonstrate the versatility of our scheme.
%
%
We aim to learn an SPD matrix $\vS$ from another type of curvature information that can be negative-definite. Therefore, the (linear) original scheme of $\vS$ in  Eq.~\eqref{eq:root_free}  is unsuitable in this setting because the original scheme assumes the curvature (i.e., the GOP) is positive-semi-definite.
In Riemannian optimization, we introduce a non-linear retraction map
in the original scheme so that the updated $\vS$ is guaranteed to be SPD. This is known as RGD with retraction \citep{absil2009optimization,boumal2014manopt}.
%
We consider this retraction-based RGD \citep{boumal2014manopt} and the Cholesky-based RGD \citep{lin2023simplifying} as baselines. We consider a metric nearness problem \citep{brickell2008metric}\scalebox{0.8}{ $\min_{S \succ 0} \ell(\vS):= \frac{1}{2N}\sum_{i=1}^{N} \|\vS\vQ\vx_i-\vx_i\|_2^2$} 
and a log-det optimization problem \citep{han2021riemannian} \scalebox{0.8}{ $\min_{S \succ 0} \ell(\vS):= \mathrm{Tr}(\vS\vQ) - \log \mathrm{det}(\vS)$}
, where $\vQ \in \real^{d \times d}$ is a known SPD matrix and only a subset of $\vx_i \in \real^d$ are observed at each iteration. The optimal solution is $\vS_*=\vQ^{-1}$ for these two problems. We measure the difference between an estimate $\vS_\text{est}$ and the ground truth $\vS_*$ using \scalebox{0.8}{ $\ell(\vS_\text{est})-\ell(\vS_*)$} as the evaluation loss.
We consider a case for $d=60$ and generate $\vQ$ and $\vx_i$.
%
%
As we can see from the first two plots in Fig.~\ref{fig:other_res}, our method performs as well as the baseline methods. Our approach works well for estimating SPD matrices even when the curvature information is not SPD. This shows that our approach can  incorporate other types of curvature information for learning $\vS$.

\vspace{-0.3cm}
\subsection{Full-matrix Case: Gradient-free Optimization}
\vspace{-0.15cm}
We consider classic gradient-free problems to demonstrate the flexibility of our approach for preconditioning (i.e., learning $\vmu$ and $\vS$).
We learn $\vS$ from a new type of curvature information to solve an original problem $\min_{\mu}\ell(\vmu)$, where we only allow to
evaluate the value of $\ell(\vmu)$.  We consider natural evolution strategies (NES) \citep{wierstra2008natural} and the Cholesky-based NES \citep{glasmachers2010exponential,fukushima2011proposal,lin2021tractable} as our baselines. Like these methods, we consider a Gaussian problem in Eq.~\eqref{eq:ref_opt} with $\gamma=0$ to approximate the original problem.
We can solve this Gaussian problem using the procedure in Sec.~\ref{sec:full_mat_update},
where we use the $1$-root and the REINFORCE estimator in Eq.~\eqref{eq:delta_approx}, which evaluates the function value of $\ell(\vw)$ and uses samples $\vw$ generated from the Gaussian $q$. We use the techniques of  \citet{wierstra2008natural} and \citet{fukushima2011proposal} to reduce the number of Monte Carlo samples and function evaluations.
We use $\ell(\vmu)$ as the evaluation loss.
From the last three plots of Fig.~\ref{fig:other_res}, we can observe that our update scheme is competitive with existing Riemannian methods. This further illustrates the efficacy and versatility of our approach. See Appx.~\ref{app:bb_opt} and Fig.~\ref{fig:extra_results} for more details.

  


\vspace{-0.2cm}
\subsection{Kronecker-based Case: Low-precision NN Training}
\vspace{-0.2cm}
Now, we examine our Kronecker-based update scheme for training transformers in half-precision (BFP16). We use our update scheme to train vision transformers with half-precision from scratch. 
Training transformers in half-precision allows us to
evaluate the numerical stability of our approach because matrix inversion and decomposition are unstable and thus unavailable in half precision.  
We then show the effectiveness and efficiency of our approach by comparing our method to strong baselines like AdamW and two recently proposed Shampoo variants, SOAP \citep{vyas2024soap} and Muon \citep{muon2024}.
Both SOAP and Muon are limited to a specific matrix root and Shampoo-based curvature information. In contrast, our approach supports arbitrary matrix roots and generic types of curvature information.
We consider training two vision transformers, PlainViT (85.6M parameters) \citep{beyer2022better} and HieraViT (50.7M parameters) \citep{ryali2023hiera}, on ImageWoof-10 and ImageNet-25 datasets. 
These ViT models only contain transformer modules except for their embedding layers.
All training methods except for SOAP support half-precision training. SOAP has to use single precision (FP32) data types due to matrix decomposition.
To demonstrate the efficiency of our approach, we update the preconditioners of our method and SOAP every two iterations.
We use random search \citep{choi2019empirical} to tune all available hyperparameters for each method on each dataset using 80 runs. See Appx.~\ref{app:extra_nn_training} for the details of the experimental setup.
From the plots in  Fig.~\ref{fig:results},  we can see that our method effectively trains transformers in half-precision and matches the performance of state-of-the-art methods like Muon.  
Compared to SOAP, we can clearly see the computational advantages of
fast root computation without matrix decomposition.
The plots also show that using a different root can affect the performance of matrix methods.
Finally, these results highlight the benefits of non-diagonal methods and underscore the need for further development.
See  Figs.~\ref{fig:imagewoof} and \ref{fig:imagenet25}
 %
 for more results.



%
%
%
%
