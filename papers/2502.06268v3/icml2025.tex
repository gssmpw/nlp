%
\documentclass{article}
\newcommand\mydel{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily delta}}}{\, \approx \,}}}

\newcommand\mystein{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily Stein}}}{\, = \,}}}

\newcommand\myneumann{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily Neumann}}}{\, = \,}}}

\input{preamble/icml2024.tex}
\input{preamble/custom.tex}
\input{preamble/defns.tex}

\begin{document}

\input{preamble/metadata.tex} %

\begin{abstract}
Many training methods, such as Adam(W) and Shampoo, learn a positive-definite curvature matrix and apply an inverse root before preconditioning. Recently, non-diagonal training methods, such as Shampoo, have gained significant attention; however, they remain computationally inefficient and are limited to specific types of curvature information due to the costly matrix root computation via matrix decomposition.
To address this, we propose a Riemannian optimization approach that dynamically adapts spectral-factorized positive-definite curvature estimates, enabling the efficient application of arbitrary matrix roots and generic curvature learning.
We demonstrate the efficacy and versatility of our approach in positive-definite matrix optimization and covariance adaptation for gradient-free optimization, as well as its efficiency in curvature learning for neural net training.

\vspace{-0.2cm}
\end{abstract}

%
\vspace{-0.65cm}
\section{Introduction}
\input{sections/introduction.tex}

\vspace{-0.5cm}
\section{Background}
\label{sec:background}
\input{sections/motivation.tex}

\vspace{-0.4cm}
\section{
Spectral-factorized Curvature Learning
}\label{sec:fast-fngd}
\input{sections/method.tex}

\vspace{-0.4cm}
\section{Experiments}
\label{sec:expriments}
\input{sections/experiments.tex}

\vspace{-0.35cm}
\section{Conclusion}
\input{sections/conclusion.tex}




%
%



%
%

\bibliography{refs}
\bibliographystyle{icml2024}

\newpage
\appendix
\onecolumn
\input{appendix/appx}

\end{document}
%
%
%
%
