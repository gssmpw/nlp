\section{Experimental Setup}

\subsection{Dataset}
\label{sec:dataset}

We conduct our experiments on the Mu-SHROOM dataset \cite{vazquez-etal-2025-mu-shroom}, which consists of outputs generated by various LLMs in response to specific input questions. 
Each output is annotated by human annotators to identify spans that correspond to hallucinations.

The dataset includes multiple languages, and for our study, we focus on the following nine languages:  Arabic (AR), Czech (CS), German (DE), English (EN), Spanish (ES), Basque (EU), Finnish (FI), French (FR), and Italian (IT).
This multilingual diversity enables a comprehensive evaluation of our method across diverse linguistic contexts.

Each data point in the dataset contains the language identifier, the input question posed to the LLM, the model name, the generated output text, and its token-level probabilities. 
Additionally, binary annotations specify the start and end indices of hallucinated spans, marking each such span as a hallucination.


\subsection{Evaluation Metric}

To evaluate the performance of our hallucination detection method, we adopt the IoU metric, a standard measure for span-based evaluation.

Given the set of character indices predicted as hallucinations, $\mathcal{H}_{pred}$, and the set of character indices labeled as hallucinations in the gold reference, $\mathcal{H}_{gold}$, the IoU is calculated as:

\begin{equation}
\mathrm{IoU} = \frac{|\mathcal{H}_{pred} \cap \mathcal{H}_{gold}|}{|\mathcal{H}_{pred} \cup \mathcal{H}_{gold}|}
\end{equation}

This metric quantifies the overlap between the predicted and ground truth hallucinated spans. To handle cases where both $\mathcal{H}_{pred}$ and $\mathcal{H}_{gold}$ are empty (i.e., no hallucinations are present in either prediction or reference), we define $\mathrm{IoU} = 1.0$ to signify perfect agreement.


\subsection{Baseline Models}

\paragraph{Token-level Hallucination Classifier (XLM-R)}
We employ a token-level hallucination classifier \cite{liu-etal-2022-token} based on XLM-RoBERTa (XLM-R) \cite{conneau-etal-2020-unsupervised}, a multilingual transformer model. 
The model is fine-tuned to perform binary classification at the token level, where each token is labeled as either hallucinated or non-hallucinated. 
% By leveraging XLM-R's pre-trained multilingual representations, the classifier can effectively capture semantic discrepancies across different languages. 

\paragraph{FAVA}

We also include FAVA \cite{mishra2024finegrained-FAVA} as a baseline model. 
FAVA is a retrieval-augmented language model designed to detect and correct hallucinations in outputs generated by LLMs. 
The model is built upon Llama2-Chat 7B \cite{Touvron2023Llama2} and employs a two-step process: retrieval and editing.
To detect hallucinations in text, we compare the edited text produced by FAVA with the original text and get the span of $\mathcal{H}_{pred}$. 


\subsection{Implementation Details}

The retriever $\mathcal{R}$ used to retrieve context for REFIND and FAVA employs a hybrid approach, combining sparse and dense retrieval methods.  Initially, a Wikipedia corpus is preprocessed for each language, including chunking, to serve as the retrieval corpus. The retriever first retrieves the top 10 relevant documents using BM25 \cite{Robertson2009BM25}. Subsequently, a document reranking step is performed using a pre-trained language model to select the final 5 documents to $\mathcal{D}$. To maintain consistency across the multilingual setting, we utilize \texttt{multilingual-e5-large}\footnote{\url{https://huggingface.co/intfloat/multilingual-e5-large}} \cite{Wang2024multilingual-e5} for the reranking process.

When calculating $p_{\theta}(t_i \mid q, t_{<i})$ in REFIND, we utilize the token probabilities of the LLM's output response provided in the Mu-SHROOM dataset. The computation of $p_{\theta}(t_i \mid \mathcal{D}, q, t_{<i})$ is performed using PyTorch 2 \cite{Jason2024PyTorch2}. The specific prompt template employed for REFIND is illustrated in Figure \ref{fig:REFIND_Prompt} (Appendix \ref{sec:appendix_prompt_details}).
More details for baselines will be discussed in Appendix \ref{sec:appendix_implementation_details}.
