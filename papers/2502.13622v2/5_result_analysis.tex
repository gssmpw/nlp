\section{Result and Analysis}

\subsection{Performance Comparison}

Table \ref{tab:result} presents the evaluation results of our proposed method, alongside the baseline models, XLM-R and FAVA, on the Mu-SHROOM dataset. The results are reported across nine languages (AR, CS, DE, EN, ES, EU, FI, FR, IT) and averaged to provide an overall assessment of performance.

REFIND outperforms the baseline models in terms of average IoU scores.
The improvements are particularly notable in low-resource languages such as Arabic, Finnish, and French, where REFIND achieves IoU scores of 0.3743, 0.5061, and 0.4734, respectively, compared to significantly lower scores from the baselines. This indicates that REFIND effectively leverages retrieval-augmented information to enhance hallucination detection in diverse linguistic settings.


\subsection{Baseline Comparison}

The XLM-R-based token classifier performs poorly on average, with an IoU of 0.0345. Its reliance solely on intrinsic model knowledge without leveraging external context limits its ability to identify hallucinated spans accurately, particularly in low-resource languages.

FAVA exhibits better performance than XLM-R, with an average IoU of 0.2787. This improvement can be attributed to its use of retrieval-augmented information for detecting and editing hallucinated text. However, FAVA's two-step process introduces complexity and potential inaccuracies in aligning the edited text with the original output.

REFIND outperforms both baselines with an average IoU of 0.3633, highlighting its superior ability to integrate retrieved context directly into the token generation process for hallucination detection. 
This streamlined approach ensures accurate and efficient identification of hallucinated spans.

\subsection{Analysis of Multilingual Performance}

REFIND demonstrates robust performance across both high-resource and low-resource languages. This indicates the generalizability of its retrieval-augmented approach to varying linguistic contexts.  Notably, performance varies considerably across languages for all methods; for instance, XLM-R and FAVA struggle significantly with low-resource languages like Arabic, Finnish, and French. In contrast, REFIND's integration of external retrieval with the LLM's internal knowledge helps mitigate performance drops in these settings.


\input{figure/threshold_analysis}

\subsection{Analysis of Threshold Sensitivity}

Figure~\ref{fig:threshold_analysis} illustrates the performance of REFIND across varying threshold values (0.1-0.4) for nine languages.
Most languages exhibit consistent IoU scores, indicating robustness to threshold changes. High-resource languages like English and German maintain stable scores around 0.35, while low-resource languages such as Arabic and Finnish show slightly larger variations, especially at lower thresholds. 
This suggests that the choice of threshold may have a more significant impact on low-resource languages, potentially due to their inherent linguistic challenges and data scarcity. 
Overall, these findings emphasize REFIND's ability to maintain reliable performance across a range of threshold values while highlighting potential areas for optimization in low-resource scenarios.


\subsection{Case Study}
\label{sec:case_study}

Figure~\ref{fig:case_study} illustrates REFIND's ability to detect hallucinations by utilizing retrieved evidence. The question asks about Chance the Rapper's debut year. The LLM's output contains a hallucinated span ("\underline{\hl{2011}}"), which is inconsistent with the retrieved documents. By comparing the generated output with external knowledge, REFIND effectively identifies spans that deviate from factual information. 
