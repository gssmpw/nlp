\section{Introduction}

Detecting hallucinated information in responses generated by large language models (LLMs) has emerged as a critical challenge in the field of natural language generation \cite{Ji2023HallucinationSurvey, Zhang2023Sirens_Song_Hallucination}. 
Hallucination, in this context, refers to the generation of content that is factually incorrect or lacks grounding in verifiable sources \cite{li-etal-2024-dawn}. 
This issue is particularly pronounced in knowledge-intensive tasks that demand high factual accuracy, such as question answering \cite{Lee2022Factuality, sun-etal-2024-towards-verifiable}.
The consequences of unmitigated hallucination are significant, ranging from the propagation of misinformation to a decline in trust in AI systems, underscoring the need for effective hallucination detection for the development of safe and trustworthy AI.

\input{figure/overview}

Prior research has explored various approaches for hallucination detection. 
Token-level classifiers, for example, leveraging pre-trained language models like RoBERTa \cite{Liu2019RoBERTa}, have been employed for binary classification, labeling individual tokens as either factual or hallucinated \cite{liu-etal-2022-token}. 
However, these models often exhibit limitations when applied to low-resource languages and tend to rely heavily on internal knowledge without effectively utilizing external evidence, which can hinder their performance. 
Extrinsic methods, such as retrieval-augmented models, aim to mitigate hallucinations by integrating external knowledge.  
Nevertheless, existing retrieval-augmented approaches, such as FAVA \cite{mishra2024finegrained-FAVA}, can potentially lead to inaccuracies in aligning the modified responses with the original LLM output, due to their multi-step processes involving retrieval, comparison, and editing.

To address these limitations, we introduce \textbf{REFIND} (\textbf{RE}trieval-augmented \textbf{F}actuality halluc\textbf{IN}ation \textbf{D}etection), a novel framework specifically designed to identify hallucinated spans within LLM-generated text.  REFIND achieves this by quantifying the context sensitivity of each token at the token level.  
By leveraging retrieved documents, REFIND calculates a Context Sensitivity Ratio (CSR) for each token in the LLM's response, measuring the token's dependence on external contextual information. 
Tokens exhibiting high CSR values are identified as likely hallucinations, offering a more direct and efficient approach to factuality verification.


Our contributions can be summarized as follows:

\vspace{-0.075in}

\begin{itemize}[itemsep=0.3mm, parsep=1pt, leftmargin=*]

    \item We present REFIND, a novel framework for detecting hallucinated spans in LLM responses by leveraging an external retriever and calculating the CSR at the token level.
    % \item We present REFIND, a novel framework for detecting hallucinations in LLM outputs by leveraging an external retriever and calculating the Context Sensitivity Ratio (CSR) at the token level.
    \item We conduct a comprehensive evaluation of REFIND using the SemEval 2025 Task 3: Mu-SHROOM dataset \cite{vazquez-etal-2025-mu-shroom}, a multilingual benchmark for detecting hallucinated spans.  REFIND is rigorously tested across nine diverse languages – Arabic, Czech, German, Spanish, Basque, Finnish, French, Italian, and English – demonstrating its robustness in both high- and low-resource settings.
    \item Experimental results demonstrate that REFIND significantly outperforms baseline models such as token-level classifiers and FAVA, achieving superior Intersection-over-Union (IoU) scores. This highlights the efficacy of the CSR in accurately identifying hallucinated content.
\end{itemize}