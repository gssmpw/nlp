\section{Related Work}
Recent forecasting benchmarks focus on event prediction but largely overlook confidence calibration. OpenForecast____ introduces a large-scale dataset for open-ended, multi-step event forecasting but does not assess model confidence. ForecastBench____ evaluates binary (Yes/No) forecasting by prompting models for direct probability estimates, but since it queries each option separately, the assigned probabilities do not necessarily sum to 1, leading to potential inconsistencies. Neither benchmark systematically evaluates confidence calibration, a crucial aspect for reliable forecasting in real-world applications.

Beyond forecasting, several benchmarks assess language models' reasoning and inference capabilities. COPA____ evaluates causal reasoning by presenting a premise and two alternatives, requiring models to select the more plausible cause or effect. HellaSwag____ challenges models with sentence completion tasks that demand commonsense reasoning, where models must choose the most sensible continuation of a given scenario. PRobELM____ assesses models' capacity to rank scenarios by plausibility, bridging the gap between factual accuracy and world knowledge. While these benchmarks provide insights into models' reasoning abilities, they do not address the challenges of forecasting future events.