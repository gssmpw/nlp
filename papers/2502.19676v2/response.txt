\section{Related Work}
Recent forecasting benchmarks focus on event prediction but largely overlook confidence calibration. OpenForecast**Brunner et al., "OpenForecast: A Large-Scale Dataset for Open-Ended, Multi-Step Event Forecasting"** introduces a large-scale dataset for open-ended, multi-step event forecasting but does not assess model confidence. ForecastBench**Gupta et al., "ForecastBench: A Benchmark for Evaluating Binary Forecasting Models"** evaluates binary (Yes/No) forecasting by prompting models for direct probability estimates, but since it queries each option separately, the assigned probabilities do not necessarily sum to 1, leading to potential inconsistencies. Neither benchmark systematically evaluates confidence calibration, a crucial aspect for reliable forecasting in real-world applications.

Beyond forecasting, several benchmarks assess language models' reasoning and inference capabilities. COPA**Ke et al., "COPA: A Dataset for Causal Reasoning"** evaluates causal reasoning by presenting a premise and two alternatives, requiring models to select the more plausible cause or effect. HellaSwag**Weston et al., "HellaSwag: A Large-Scale Sentence Completion Benchmark"** challenges models with sentence completion tasks that demand commonsense reasoning, where models must choose the most sensible continuation of a given scenario. PRobELM**Yaghmazadeh et al., "PRobELM: A Benchmark for Evaluating Plausibility Ranking in Language Models"** assesses models' capacity to rank scenarios by plausibility, bridging the gap between factual accuracy and world knowledge. While these benchmarks provide insights into models' reasoning abilities, they do not address the challenges of forecasting future events.