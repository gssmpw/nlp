\begin{table}[ht]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
Model & N & Accuracy ($\uparrow$) & F1 ($\uparrow$) & Brier ($\downarrow$) \\
\midrule
GPT2              & 401 & 0.58 & 0.37 & 0.42 \\
GPT2-XL           & 401 & 0.67 & 0.32 & 0.45 \\
Pythia-14m        & 343 & 0.59 & 0.14 & 0.62 \\
Pythia-160m       & 343 & 0.61 & 0.26 & 0.53 \\
Pythia-2.8b       & 343 & 0.55 & 0.39 & 0.44 \\
Bloom-560m        & 263 & 0.49 & 0.41 & 0.45 \\
Bloom-7b1         & 263 & 0.64 & 0.33 & 0.32 \\
Llama-7b          & 226 & 0.57 & 0.34 & 0.52 \\
OLMo-1B           & 188 & 0.23 & 0.16 & 0.75 \\
OLMo-7B           & 188 & 0.21 & 0.17 & 0.82 \\
OLMo-7B-Instruct  & 188 & 0.65 & 0.14 & 0.38 \\
OLMo-2-7B         & 145 & 0.51 & 0.24 & 0.44 \\
OLMo-2-7B-Instruct & 145 & 0.59 & 0.30 & 0.41 \\
\bottomrule
\end{tabular}%
}
\caption{Performance of forecasting models on Boolean questions in \textsc{FOReCAst}. Reported metrics include the number of evaluated questions (N), accuracy, F1 score (both higher is better), and the Brier score (lower is better).}
\label{tab:binary_forecasting}
\end{table}
