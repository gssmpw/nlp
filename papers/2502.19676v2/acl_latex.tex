% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xcolor, colortbl}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%


% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{The \textbf{F}uture \textbf{O}utcome \textbf{R}easoning and \textbf{C}onfidence \textbf{A}ssessment Benchmark}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Zhangdie Yuan \\
  University of Cambridge \\
  \texttt{zy317@cam.ac.uk} \\\And
  Zifeng Ding \\
  University of Cambridge \\
  \texttt{zd320@cam.ac.uk} \\\And
  Andreas Vlachos \\
  University of Cambridge \\
  \texttt{av308@cam.ac.uk} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle

\begin{abstract}
Forecasting is an important task in many domains, such as 
technology and economics. However existing forecasting benchmarks largely lack comprehensive confidence assessment, focus on limited question types, and often consist of artificial questions that do not align with real-world human forecasting needs. To address these gaps, we introduce \textsc{FOReCAst} (\textbf{F}uture \textbf{O}utcome \textbf{R}easoning and \textbf{C}onfidence \textbf{A}ssessment), a benchmark that evaluates models' ability to make predictions and their confidence in them. \textsc{FOReCAst} spans diverse forecasting scenarios involving Boolean questions, timeframe prediction, and quantity estimation, enabling a comprehensive evaluation of both prediction accuracy and confidence calibration for real-world applications.
\end{abstract}

\input{tables/examples}

\section{Introduction}
Recent advances in large language models (LLMs) have significantly improved their performance across a wide range of natural language processing (NLP) tasks. Alongside these developments, various benchmarks and datasets have been introduced to effectively assess the capabilities of LLMs, particularly in terms of knowledge and reasoning \citep{zellers-etal-2019-hellaswag, guo2023evaluating}.  Fact-based benchmarks, like TruthfulQA~\citep{lin-etal-2022-truthfulqa} evaluate LLMs based on factual correctness, focusing on tasks like retrieving and verifying facts that are known. 

Forecasting is a crucial yet challenging task across various domains, including technology, economics, and public policy. Unlike tasks that rely on retrieving and verifying existing knowledge, forecasting requires predicting plausible outcomes for future events, often under uncertainty and incomplete information. This makes  forecasting particularly difficult, as models must infer trends, assess probabilities, and adapt to new information. Several datasets have been introduced to evaluate LLMs' forecasting capabilities. 
ForecastQA \citep{jin2020forecastqa} uses a multiple-choice format where models predict future outcomes, but it lacks confidence assessment. AutoCast \citep{zou2022forecasting} incorporates confidence intervals, however its confidence estimates are not designed for forecasting. Other datasets such as ExpTime~\citep{yuan2024back} are artificially generated from structured data, focusing on  
explainable event forecasting based on temporal knowledge graphs.

All of these aforementioned benchmarks ignore a crucial aspect of forecasting: confidence evaluation. Confidence plays a central role in forecasting, as predictions about unresolved events inherently lack definitive correctness at the time of evaluation. 
Predictions made with absolute certainty are undesirable, even if they ultimately prove to be correct, because they fail to account for the uncertain nature of future events. Moreover, miscalibrated confidence can lead to poor decision-making: overconfident yet incorrect forecasts may result in costly errors, while underconfident but accurate predictions can erode trust in the model. Therefore, well-calibrated confidence scores are as crucial as the accuracy of the predictions themselves.

To address these gaps, we present \textsc{FOReCAst}: \textbf{F}uture \textbf{O}utcome \textbf{R}easoning and \textbf{C}onfidence \textbf{A}ssessment. \textsc{FOReCAst} focuses on three distinct types of forecasting questions, shown in~\autoref{tab:example_questions}: (1) \textit{Boolean questions}, such as "Will there be a Frontier AI lab in China before 2026?"; (2) \textit{Timeframe Prediction}, such as "When will OpenAI announce GPT-5?"; and (3) \textit{Quantity Estimation}, such as "How many spacecrafts will land on the moon in each of the following years?" We conduct  experiments using a range of models differing in size, training objectives, and cutoff times, and explore multiple methods for estimating model confidence. Our results reveal that forecasting remains highly challenging for current LLMs, particularly in confidence evaluation, with no direct correlation between prediction performance and confidence calibration, and while larger models sometimes improve performance, the effect is inconsistent. 


\section{\textsc{FOReCAst}: Problem Formulation}
\label{sec:for}


System responses in  \textsc{FOReCAst} consist of (1) a prediction answering a question given the available information and (2) a confidence score in the prediction. This 

ensures a 
 comprehensive assessment of forecasting, accounting for both correctness and confidence calibration.  

Questions belong to three types. (1) \textit{Boolean Questions}, which ask yes/no questions about the occurrence of future events (sometimes within a certain timeframe). Boolean questions are simple to evaluate, and they can still be surprisingly challenging
~\citep{clark2019boolq}. (2) \textit{Timeframe Prediction}, which requires predicting a specific timeframe for an event, and are essential for applications where knowing whether an event will happen or not without a timeframe is insufficient. (3) \textit{Quantity Estimation}, which involves providing numerical estimates related to future events, e.g.,\ economic indicators or trends. 

Formally, let $Q$ represent a question about a future event, and let $M$ denote a system with access to information up to time $t_{train}$ (e.g., the system's knowledge cutoff point). The objective is for $M$ to produce an answer $A$ in $\mathcal{A}$ and an associated confidence score $C$, where:
\begin{equation}
    M(Q) \to (A, C), \quad C \in [0,1].
\end{equation}
\begin{equation}
    A = \arg\max_{a \in \mathcal{A}} P(X = a | Q, \mathcal{K}(t_{train})).
\end{equation}
Here, $\mathcal{K}(t_{train})$ represents the knowledge accessible to the model up to time $t_{train}$. The answer space $\mathcal{A}$ depends on the type of forecasting question: for \textit{Boolean Questions}, $\mathcal{A} = \{\text{Yes}, \text{No}\}$; for \textit{Timeframe Prediction}, $\mathcal{A}$ consists of a single date in the \texttt{YYYY-MM-DD} format; and for \textit{Quantity Estimation}, $\mathcal{A} = \mathbb{R}$, representing real numbers.  


\section{Evaluating Predictions and Confidence} 



\paragraph{Evaluating Boolean Questions}

For Boolean questions, where the answer space is $\mathcal{A} = \{\text{Yes}, \text{No}\}$, prediction performance is evaluated using standard classification metrics, including accuracy and F1-score.

Confidence calibration is assessed using a modified version of the Brier score~\citep{brier1950verification}, which measures the mean squared error between predicted confidence and the gold confidence provided in the dataset, which we assume is provided, and

represents the likelihood of an event occurring. The modified Brier score is defined as:

\begin{equation}
    \text{Brier} = \frac{1}{N} \sum_{i=1}^{N} (C^{\text{pred}}_i - C^{\text{gold}}_i)^2,
\end{equation}

where $C^{\text{pred}}_i$ is the model's predicted confidence and $C^{\text{gold}}_i$ is the gold confidence. 
This modification ensures that models are evaluated based on their ability to match the likelihood of an event. Lower Brier scores indicate better calibration, reflecting how well the predicted confidence aligns with the likelihood of the event.

\paragraph{Evaluating Timeframe Prediction}

For timeframe prediction, where the answer space consists of specific dates in the \texttt{YYYY-MM-DD} format, predictive accuracy is measured using absolute day error (ADE). Given a predicted date $D^{\text{pred}}_i$ and the gold date $D^{\text{gold}}_i$, we compute the normalized error as:
\begin{equation}
    E_i^{\text{ADE}} = \frac{2}{1 + e^{-\alpha |D^{\text{pred}}_i - D^{\text{gold}}_i|}} - 1,
\label{ade}
\end{equation}
where $\alpha$ is a scaling factor that controls how sharply large errors are penalized. This transformation ensures that extreme deviations do not disproportionately dominate the evaluation. 

For confidence calibration, rely on the Continuous Ranked Probability Score (CRPS) \citep{matheson1976cprs}, which is a generalisation of the Mean Absolute Error to probabilistic forecasts, and extend it to compare the predicted probability distribution with a gold distribution. Specifically, we assume that both the predicted and the gold confidence predictions follow Gaussian distributions, namely  $\mathcal{N}(D^{\text{pred}}_i, \sigma^{\text{pred}}_i)$ and $\mathcal{N}(D^{\text{gold}}_i, \sigma^{\text{gold}}_i)$ respectively, where the standard deviations are computed as:
\begin{equation}
    \sigma^{\text{pred}}_i = \sigma_{\max} \cdot (1 - C^{\text{pred}}_i) + \sigma_{\min} \cdot C^{\text{pred}}_i,
\label{crps_pred}
\end{equation}
\begin{equation}
    \sigma^{\text{gold}}_i = \sigma_{\max} \cdot (1 - C^{\text{gold}}_i) + \sigma_{\min} \cdot C^{\text{gold}}_i.
\label{crps_gold}
\end{equation}
Here, $C^{\text{pred}}_i$ is the model's predicted confidence for the $i$th question, and $C^{\text{gold}}_i$ is the corresponding gold confidence provided in our dataset. The parameters $\sigma_{\max}$ and $\sigma_{\min}$ define the upper and lower bounds for the standard deviation. Intuitively, when confidence is low ($C \approx 0$), uncertainty is high, leading to $\sigma \approx \sigma_{\max}$, while when confidence is high ($C \approx 1$), uncertainty is low, resulting in $\sigma \approx \sigma_{\min}$.
%
We then compute the CRPS as the integrated squared difference between the cumulative distribution functions (CDFs) of the predicted and gold distributions:
%
\begin{equation}
    \text{CRPS} = \frac{1}{N} \sum_{i=1}^{N} \int
    \left(F^{\text{pred}}_i(d) - F^{\text{gold}}_i(d)\right)^2 \, \mathrm{d}d,
\end{equation}
%
where $F^{\text{pred}}_i$ and $F^{\text{gold}}_i$ denote the CDFs of the predicted and gold Gaussian distributions, respectively.
%
A lower CRPS indicates better calibration, as it reflects a closer match between the predicted uncertainty and the uncertainty as specified by the gold confidence.

\paragraph{Evaluating Quantity Estimation}

For quantity estimation, where the answer space consists of non-negative real numbers ($\mathcal{A} = \mathbb{R}_{\geq 0}$), we evaluate prediction performance using two error metrics: absolute percentage error (APE) and mean absolute error (MAE). Given a predicted quantity $Q^{\text{pred}}_i$ and the gold quantity $Q^{\text{gold}}_i$, the normalized errors are computed as:
\begin{equation}
    E^{\text{APE}}_i = \frac{2}{1 + e^{-\alpha \frac{|Q^{\text{pred}}_i - Q^{\text{gold}}_i|}{Q^{\text{gold}}_i + \epsilon}}} - 1,
\label{ape}
\end{equation}
\begin{equation}
    E^{\text{MAE}}_i =  \frac{2}{1 + e^{-\alpha |Q^{\text{pred}}_i - Q^{\text{gold}}_i|}} - 1.
\label{mae}
\end{equation}
Here, $\epsilon$ is a small constant to prevent division by zero, and $\alpha$ is a scaling factor that controls how sharply large errors are penalized, similar to the timeframe prediction evaluation. 
%
Confidence calibration is assessed using  CRPS, following the same Gaussian assumption as in timeframe prediction. The predicted quantity is modeled as a Gaussian distribution $\mathcal{N}(Q^{\text{pred}}_i, \sigma^{\text{pred}}_i)$, and the gold quantity as $\mathcal{N}(Q^{\text{gold}}_i, \sigma^{\text{gold}}_i)$. The standard deviations $\sigma^{\text{pred}}_i$ and $\sigma^{\text{gold}}_i$ are computed using the same formulation as in timeframe prediction. 

\section{\textsc{FOReCAst} Construction}
\label{sec:forecast_construction}

\subsection{Data Source and Question Selection}

\textsc{FOReCAst} is constructed from Metaculus,\footnote{\url{www.metaculus.com}. Examples in \autoref{app:metaculus_example}.} an online forecasting platform where forecasters submit probabilistic predictions to questions across various domains. Metaculus aggregates individual forecasts into a continuously updated community prediction, which is finalized just before resolution. Each question has predefined resolution criteria, ensuring verifiable outcomes.
%
To ensure dataset reliability, we include only questions with a definitive resolution and at least 100 forecasts to maintain statistical reliability. Ambiguous or subjectively resolved questions are excluded, and we remove those whose outcomes depend on arbitrary or uncontrollable factors. 
%
These steps ensure that \textsc{FOReCAst} consists of high-quality, well-formed forecasting questions with verifiable outcomes.

\subsection{Extracting Confidence from Crowdsourced Forecasts}
Forecasting distinguishes between physical probabilities—objective likelihoods derived from statistical or scientific models—and human beliefs~\citep{sanders1963subjective} about future events. While physical probabilities can be useful,  
they are often unavailable, 
particularly for questions involving human behavior, economics, or sociopolitical outcomes. Instead, collective human forecasts offer a more practical 
confidence estimate, integrating expert reasoning, contextual knowledge, and evolving evidence. 
For instance, predicting a technological breakthrough depends more on expert assessment and current trends than on rigid probabilistic models. 
%
Therefore, confidence in \textsc{FOReCAst} is derived from Metaculus community forecasts, which aggregate predictions from a diverse pool of forecasters. While human predictions are sometimes incorrect, they still serve as a valuable proxy for uncertainty, as they reflect the best available reasoning given the information at the time. 

\paragraph{Formal Definition of Gold Confidence.}  
Gold confidence in \textsc{FOReCAst} is derived from the final Metaculus community prediction before resolution. Instead of directly using the predicted probability for the correct outcome, we compute a log score relative to a uniform baseline, ensuring that confidence reflects how much the forecast deviates from random guessing. This transformation prevents extreme probabilities in inherently uncertain scenarios and makes confidence scores more comparable across different forecasting tasks. The final score is mapped to $(0,1)$ using a sigmoid function.

For Boolean questions, where the human-forecasted probability for the correct outcome is $P^{\text{gold}}$, gold confidence is computed as:
\begin{equation}
    C^{\text{gold}} = \sigma\left(\frac{\ln P^{\text{gold}} - \ln 0.5}{\ln 2} \right),
\end{equation}
where $\sigma(x)$ is the sigmoid function.

Similarly, for timeframe prediction and quantity estimation, where the human-forecasted probability density function (PDF) assigns probability to a continuous outcome $x^{\text{gold}}$, gold confidence is computed as:
\begin{equation}
    C^{\text{gold}} = \sigma\left(\frac{\ln f(x^{\text{gold}}) - \ln f_{\text{uniform}}}{2} \right),
\end{equation}
where $f(x^{\text{gold}})$ is the forecasted probability density at the resolved outcome, and $f_{\text{uniform}}$ is the uniform baseline density over the valid range of values. The denominator 2 ensures numerical stability and scales confidence appropriately.

\subsection{Dataset Statistics and Comparison}

\textsc{FOReCAst} consists of 2256 forecasting questions, spanning domains such as politics, economics, science, and technology. Each question includes a resolved outcome, a gold confidence score, and a final Metaculus community forecast before resolution. To facilitate model development and evaluation, we split the dataset into 65\% training, 10\% validation, and 25\% test. The full dataset statistics is shown in \autoref{app:dataset_stats}.

\autoref{tab:benchmark_variants} provides a comparison between \textsc{FOReCAst} and existing forecasting benchmarks. Compared to prior datasets, \textsc{FOReCAst} uniquely emphasizes both forecasting accuracy and confidence calibration, includes a diverse set of forecasting tasks, and is constructed from a well-established crowdsourced platform with rigorous resolution criteria.

\input{tables/comparison1}




\section{Experiments on \textsc{FOReCAst}}

\subsection{Experimental Setup}

\paragraph{Models.}  
We evaluate a diverse set of large language models (LLMs) with varying training data cutoffs, model sizes, and instruction tuning. To analyze the impact of knowledge recency, we group models by family and assume the cutoff date is the 1st of the stated month. The models used are shown in \autoref{tab:models}.

\input{tables/models}


\paragraph{Inference.}  
We use 1-shot in-context learning to provide models with a structured example of how to answer forecasting questions. For instruction-tuned models, we add an extra line of instruction to align with their training paradigm. To ensure fair comparison, baseline prompts are kept minimal while maintaining clarity. Confidence is estimated using logit-based normalized heuristics. Full prompts and hyperparameters are provided in~\autoref{prompt} and ~\autoref{hyperparameters}.



\input{tables/finding_1_result_binary}

\subsection{Results and Findings}

Our experiments on the \textsc{FOReCAst} dataset reveal that forecasting remains highly challenging for current LLMs, particularly in confidence estimation. While models achieve reasonable accuracy in point predictions, their uncertainty estimates—captured by Brier score and CRPS—vary significantly. This suggests that confidence evaluation must be treated as a separate challenge from prediction assessment.  


\paragraph{Boolean Questions:}  
\autoref{tab:binary_forecasting} shows that while some models achieve reasonable accuracy, their calibration remains inconsistent. For example, \texttt{GPT2-XL} achieves an accuracy of 0.67 but has a relatively high Brier score of 0.45, while \texttt{OLMo-7B-Instruct} achieves similar accuracy (0.65) with a lower Brier score of 0.38, suggesting better confidence estimation. Notably, models with later training cutoffs do not always outperform older ones; for instance, \texttt{LLaMA-7B} has lower accuracy (0.57) and a worse Brier score (0.52) than some earlier models. This indicates that forecasting accuracy depends on multiple factors beyond knowledge recency.


\input{tables/finding_1_result_timeframe}

\input{tables/finding_1_result_quantity}

\paragraph{Timeframe Prediction:}  
\autoref{tab:timeframe_prediction} reveals that most models struggle with predicting event timing, as both ADE and CRPS remain near the worst-case scenario of 1.0. Across all models, only one achieves a CRPS below 0.9: \texttt{OLMo-2-7B-Instruct} (CRPS = 0.82), which still indicates substantial uncertainty. Even models with more recent training data, such as \texttt{OLMo-2-7B} (CRPS = 0.99), fail to make well-calibrated temporal forecasts. These results suggest that even when models correctly anticipate whether an event will happen, quantifying when it will occur remains a major challenge.

\paragraph{Quantity Estimation:}  
\autoref{tab:quantity_estimation} highlights that models differ widely in their ability to estimate numerical values. Some achieve relatively low APE, such as \texttt{GPT2-XL} (APE = 0.03) and \texttt{Pythia-160M} (APE = 0.02), yet their confidence calibration, CRPS, does not always align with their point prediction performance. For instance, \texttt{GPT2-XL} has a CRPS of 0.72, while \texttt{Pythia-160M} has a slightly higher CRPS of 0.76, despite achieving lower APE. In contrast, \texttt{OLMo-2-7B-Instruct}, which has a relatively higher APE of 0.08, achieves the lowest CRPS (0.57) among all models. These results indicate that prediction quality and confidence calibration do not necessarily improve together, reinforcing the complexity of numerical forecasting.

\paragraph{Impact of Model Size on Forecasting Performance}  
Larger models do not consistently improve forecasting performance. Within the Pythia family, \texttt{Pythia-2.8b} shows occasional gains in accuracy and calibration over \texttt{Pythia-14m} and \texttt{Pythia-160m}, but the improvements are not uniform across all metrics. Similarly, while \texttt{OLMo-2-7B} variants sometimes achieve lower CRPS and MAE in quantity estimation, these benefits often come with trade-offs in point prediction. These results suggest that model size alone is not a reliable predictor of forecasting performance. This highlights the importance of developing task-specific techniques rather than relying on ever-larger models to solve the forecasting problem.



\input{tables/finding_2_base_vs_instruct}

\paragraph{Impact of Instruction Tuning on Forecasting Performance}  
\autoref{tab:olmo_instr_effect} compares base and instruct-tuned variants of OLMo-7B and OLMo-2-7B across Boolean, timeframe, and quantity forecasting tasks. For Boolean questions, \texttt{OLMo-7B-Instruct} achieves higher accuracy (0.65 vs.\ 0.21) and a lower Brier score (0.38 vs.\ 0.82), indicating better confidence calibration. In timeframe prediction, the instruct-tuned \texttt{OLMo-2-7B-Instruct} improves uncertainty estimation, with an ADE of 0.87 and CRPS of 0.82, compared to 0.9981 and 0.9970 for the base \texttt{OLMo-2-7B}. For quantity estimation, instruct-tuned models have slightly higher APE but lower MAE and CRPS, suggesting better uncertainty calibration.  These results indicate that instruction tuning enhances confidence estimation, even if it does not always improve point prediction accuracy. This suggests a trade-off where instruct-tuned models prioritize more reliable uncertainty quantification.

\input{tables/finding_3}

\paragraph{Impact of Aggregation Methods on Forecasting Performance}  
\autoref{tab:ablation_aggregation} compares different aggregation methods for deriving the final prediction and confidence estimate from the top 10 outputs of \texttt{Llama-7B}. Bayesian Aggregation achieves the highest accuracy (0.5796) and F1 score (0.3485), suggesting it is the most effective at identifying correct outcomes. However, Weighted Average yields a significantly lower Brier score (0.2914), indicating superior confidence calibration compared to other methods.  

Majority Vote, Highest Confidence, and Logit Mean Probability produce comparable accuracy and F1 scores but have noticeably higher Brier scores, suggesting weaker uncertainty estimation. These results highlight that even when point prediction performance is similar, aggregation methods substantially impact confidence reliability. The challenge remains in developing techniques that optimize both accuracy and calibration simultaneously, emphasizing the importance of uncertainty-aware forecasting.



\section{Related Work}

Recent forecasting benchmarks focus on event prediction but largely overlook confidence calibration. OpenForecast~\citep{wang2025openforecast} introduces a large-scale dataset for open-ended, multi-step event forecasting but does not assess model confidence. ForecastBench~\citep{karger2024forecastbench} evaluates binary (Yes/No) forecasting by prompting models for direct probability estimates, but since it queries each option separately, the assigned probabilities do not necessarily sum to 1, leading to potential inconsistencies. Neither benchmark systematically evaluates confidence calibration, a crucial aspect for reliable forecasting in real-world applications.

Beyond forecasting, several benchmarks assess language models' reasoning and inference capabilities. COPA~\citep{roemmele2011choice} evaluates causal reasoning by presenting a premise and two alternatives, requiring models to select the more plausible cause or effect. HellaSwag~\cite{zellers-etal-2019-hellaswag} challenges models with sentence completion tasks that demand commonsense reasoning, where models must choose the most sensible continuation of a given scenario. PRobELM~\citep{yuan2024probelm} assesses models' capacity to rank scenarios by plausibility, bridging the gap between factual accuracy and world knowledge. While these benchmarks provide insights into models' reasoning abilities, they do not address the challenges of forecasting future events.


\section{Conclusion}

We introduce \textsc{FOReCAst}, a benchmark for evaluating both forecasting accuracy and confidence calibration in language models. Unlike existing datasets, \textsc{FOReCAst} explicitly assesses confidence alongside predictions. Our results show that current models struggle with both prediction and well-calibrated confidence, underscoring the need for improved uncertainty estimation and confidence calibration.

\section*{Limitations}

While \textsc{FOReCAst} represents a significant step toward evaluating forecasting accuracy and confidence calibration, there are inherent limitations that we view as opportunities for future research rather than fundamental shortcomings of our work. First, our dataset is constructed solely from Metaculus, which may not fully represent the global diversity of forecasting practices or question domains. Second, our method for deriving gold confidence relies on community forecasts and heuristic transformations that might not capture all nuances of human uncertainty. Lastly, our focus on English-language forecasts limits the benchmark’s applicability across different languages and cultural contexts. Addressing these issues is part of our future work agenda.


\section*{Ethical Statement}

\textsc{FOReCAst} is built from data sourced exclusively from Metaculus, an English-language forecasting platform. As a result, the dataset may embody the linguistic, cultural, and social biases inherent in its user community. These biases could affect both question selection and confidence judgments. We acknowledge these concerns and stress that our benchmark is intended as an initial step toward more inclusive forecasting evaluations. Future efforts should aim to incorporate data from a broader range of platforms and languages to mitigate these biases.

\section*{Acknowledgements}
All authors of this paper are supported by the ERC grant AVeriTeC (GA 865958). Andreas Vlachos receives further support from the DARPA program SciFy. 

\bibliography{custom}

\newpage

\appendix
\section{Example Metaculus Questions}
\label{app:metaculus_example}

To illustrate how human forecasts evolve over time, we present two questions from different domains: Q1 is in the business and geopolitics domain and Q2 is in the technology domain.

\begin{quote}
\small
\textit{Q1: Will TikTok become available in the US on both the App Store and Google Play before April 5, 2025?}

\textit{Q2: When will a SpaceX Starship reach orbit?}
\end{quote}

 For Q1, \autoref{fig:metaculus_example} shows how community forecasts changed over time, while \autoref{fig:metaculus_histogram} presents the histogram of the final forecast distribution.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/metaculus_example.png}
    \caption{Community prediction trend for a Metaculus question on TikTok’s availability in the US.}
    \label{fig:metaculus_example}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/metaculus_example_histogram.png}
    \caption{Histogram of final community forecasts.}
    \label{fig:metaculus_histogram}
\end{figure}

For Q2, \autoref{fig:metaculus_spacex} tracks forecast updates, while \autoref{fig:metaculus_pdf} shows the final probability density function (PDF) of predicted launch dates.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/metaculus_spacex.png}
    \caption{Community prediction trend for SpaceX Starship's first orbital launch.}
    \label{fig:metaculus_spacex}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/metaculus_spacex_pdf.png}
    \caption{Probability density function of final community forecasts for SpaceX Starship reaching orbit.}
    \label{fig:metaculus_pdf}
\end{figure}

\section{Dataset Statistics}
\label{app:dataset_stats}

\input{tables/stats}

\autoref{tab:forecast_stats} presents detailed dataset statistics, including the total number of questions and their distribution across Boolean Questions, Timeframe Prediction, and Quantity Estimation tasks.



\section{Prompts}
\label{prompt}

To ensure a fair and consistent evaluation across models, we use simple one-shot prompts with structured outputs in JSON format. For instruction-tuned models, we provide an additional instruction line specifying the task. The prompts are designed for three forecasting question types: Boolean Questions (Yes/No), Timeframe Prediction (YYYY-MM-DD), and Quantity Estimation (numeric values).

\onecolumn  
\setlength{\parindent}{0pt}  

\subsection{Instruction-Tuned Models}
For models containing "Instruct" in their name, we use the following prompts:

\noindent\textbf{Quantity Estimation}

\begin{verbatim}
You are an AI assistant providing precise numerical forecasts. 
Answer the following question with a single numeric value in JSON format.

Example:
Q: How much global photovoltaic energy generation was deployed 
   by the end of 2020?  
A: { "value": 738 }  

Q: $question  
A: { "value": "
\end{verbatim}


\noindent\textbf{Timeframe Prediction}

\begin{verbatim}
You are an AI assistant providing precise date forecasts. 
Answer the following question with a single date in YYYY-MM-DD format in JSON.

Example:
Q: When did an AI system achieve a significant victory against 
   a professional human in Starcraft 2?  
A: { "value": "2019-01-24" }

Q: $question  
A: { "value": "
\end{verbatim}


\noindent\textbf{Boolean Questions}

\begin{verbatim}
You are an AI assistant providing binary (Yes/No) answers. 
Answer the following question with "Yes" or "No" in JSON format.

Example:
Q: Will we confirm evidence for megastructures orbiting the 
   star KIC 8462852?  
A: { "value": "No" }

Q: $question  
A: { "value": "
\end{verbatim}


\subsection{Base Models}
For non-instruction-tuned models, we use the same examples but without additional instructions:

\noindent\textbf{Quantity Estimation}

\begin{verbatim}
Q: How much global photovoltaic energy generation was deployed 
   by the end of 2020?  
A: { "value": 738 }  

Q: $question  
A: { "value": "
\end{verbatim}


\noindent\textbf{Timeframe Prediction}

\begin{verbatim}
Q: When did an AI system achieve a significant victory against 
   a professional human in Starcraft 2?  
A: { "value": "2019-01-24" }

Q: $question  
A: { "value": "
\end{verbatim}


\noindent\textbf{Boolean Questions}

\begin{verbatim}
Q: Will we confirm evidence for megastructures orbiting the 
   star KIC 8462852?  
A: { "value": "No" }

Q: $question  
A: { "value": "
\end{verbatim}


\twocolumn  

\section{Hyperparameter Settings}
\label{hyperparameters}

\subsection{Generation Hyperparameters}
We generate responses using temperature-based sampling with the following hyperparameters:

\begin{itemize}
    \item \texttt{max\_length} = 200
    \item \texttt{do\_sample} = \texttt{True}
    \item \texttt{top\_k} = 50
    \item \texttt{top\_p} = 0.9
\end{itemize}

Among the generated outputs, we select the one with the highest confidence as the final prediction. All experiments are conducted using full precision on an NVIDIA RTX 8000 GPU.

\subsection{Evaluation Hyperparameters}
The scaling factor $\alpha$ in \autoref{ade}, \autoref{ape}, and \autoref{mae} is set to 0.05. For \autoref{crps_pred} and \autoref{crps_gold}, we set $\sigma_{\max}$ to 30 and $\sigma_{\min}$ to 1 for Timeframe Prediction, while for Quantity Estimation, $\sigma_{\max}$ is 20 and $\sigma_{\min}$ is 1. These values ensure that evaluation metrics appropriately scale errors and confidence calibration.


\section{Additional Results}
\label{app:results}

This section provides extended results categorized by the training data cutoff date of each model. Forecasting performance depends on model architecture, scale, and knowledge recency, so we evaluate models with different cutoff dates to examine how access to more recent information influences prediction accuracy and confidence calibration.  

Models trained after certain event resolutions may have indirectly encountered outcome-related information, potentially affecting evaluation fairness. This should be considered when interpreting results.  

Detailed model-specific performance metrics for Boolean Questions, Timeframe Prediction, and Quantity Estimation are presented in \autoref{tab:combined_2017-12-01} to \autoref{tab:combined_2023-12-01}.

\input{tables/appendix_tables/2017-12-01}
\label{tab:combined_2017-12-01}

\input{tables/appendix_tables/2020-03-01}
\label{tab:combined_2020-03-01}

\input{tables/appendix_tables/2021-12-01}
\label{tab:combined_2021-12-01}

\input{tables/appendix_tables/2022-08-01}
\label{tab:combined_2022-08-01}

\input{tables/appendix_tables/2023-03-01}
\label{tab:combined_2023-03-01}

\input{tables/appendix_tables/2023-12-01}
\label{tab:combined_2023-12-01}

These results highlight trends in forecasting accuracy and confidence calibration across models with different knowledge recency.



\end{document}
