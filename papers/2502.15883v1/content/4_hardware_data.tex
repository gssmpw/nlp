\section{Data Collection and Processing}
Considering the need for a complete record of both the strokes and the brushstoke process \textbf{(DC1, DC4)}, we design a data collection methodology that utilizes cameras to capture the brushstoke process and sensors to collect brush posture and finger pressure \textbf{(DC2)}. Algorithmic techniques are then applied to align the brush parameters with the stroke positions \textbf{(DC2)}. After discussions with experts, the specific collection parameters and their significance were finalized, as shown in the Table \ref{tab:calligraphy-measurements}. The overall data workflow of the system is illustrated in Figure \ref{fig:overview}, with the detailed design and corresponding descriptions discussed in the following sections.

















\subsection{Image Data Collection}
In this step, the goal is to capture raw video footage of the writing process, laying the groundwork for subsequent data processing. 
\subsubsection{Equipment Overview}
To minimize user reliance on specific equipment, we use widely available smartphones as the primary data collection tool. We have set up two recording devices, referred to as \textbf{Device A} and \textbf{Device B} (Figure \ref{fig:device A and B}). \textbf{Device A} is placed parallel to the table surface to monitor the contact between the brush tip and the paper, detecting writing motions. \textbf{Device B} is fixed above the table, covering the entire writing area to capture the full brushstroke process.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.48\textwidth]{design_fig/whole_system.png}
    \caption{The data collection setup includes pressure sensors, inertial sensors, and cameras. The sensors are connected via Arduino to capture data, which is then synchronized and processed on the computer.}
    \label{fig:whole system}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.4\textwidth]{design_fig/Device_A_and_B.png}
    \caption{Capture the brushstoke process using two smartphones from different angles: Device A and Device B}
    \label{fig:device A and B}
\end{figure}

\subsubsection{Image Preprocessing}
After \textbf{Device B} recording the whole brushstoke process, the videos are sent to a data preprocessing stage. To correct the perspective distortion captured by the overhead camera, we use the perspective transformation algorithm from the OpenCV library~\cite{opencv_library}. This algorithm corrects image distortion by defining source and destination points, calculating a perspective transformation matrix, and remapping the image to a top-down view (Figure \ref{fig:view_correct}). This step ensures that the writing trajectory is presented at the correct scale and angle in the video, accurately restoring the original writing result during subsequent stroke visualization \textbf{(DC1)}. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{design_fig/image_correct.png}
    \caption{Stroke Images Before and After Correction}
    \label{fig:view_correct}
\end{figure}



\subsection{Sensor Data Collection}
In this step, the inertial sensor and pressure sensor are used to capture the brush's posture and the hand's applied force, respectively (Figure \ref{fig:whole system}). 
 
\subsubsection{Brush posture collection}
The MPU6050 inertial sensor was used to measure the brush's tilt and rotation, and it was attached to the brush head. The sensor's built-in Digital Motion Processor (DMP) module can directly process data from the accelerometer and gyroscope to calculate the object's orientation. This not only reduces the computational load on external processors but also provides real-time orientation data~\cite{widagdo2017limb}. 

\subsubsection{Finger force collection}
Previous research inferred hand force by detecting arm muscle activity~\cite{10269740, 10340786}, but we required more precise finger force data due to the critical role of the fingers in writing. Therefore, we selected a circular resistive pressure sensor (short tail RP-C7.6-ST-LF2), which offers high sensitivity, small size, quick response, and low cost. Since the brushstoke process requires rotating the brush, we opted to attach the sensors to the fingers rather than directly to the brush shaft. This allows users to practice with any standard brush without the need for modifications. We initially considered wrapping a strip-shaped sensor around the shaft, but due to the slender nature of the brush handle, the excessive curvature of the sensor could lead to measurement errors.
\begin{figure}[t!]
  \centering
  \includegraphics[width=0.4\textwidth]{design_fig/finger.png}
  \caption{Design sketch: the pressure each finger applied to the brush}
  \label{fig: finger}
\end{figure}
Based on the posture of holding the brush handle, we attached four sensors to the thumb, index finger, middle finger, and ring finger using finger sleeves to measure the pressure each finger applied to the brush handle (Figure~\ref{fig: finger}). However, during subsequent iterations, expert suggested that the force exerted by the hand is primarily applied to the brush shaft, indirectly altering the contact between the brush tip and the paper, thus affecting the quality of the strokes. Such details are usually not emphasized in typical teaching settings, where only the overall force application needs to be monitored. Additionally, monitoring the force of multiple fingers might confuse beginners, leading them to believe they need to intentionally replicate different force applications from all four fingers, thereby creating unnecessary learning challenges.

As a result, we simplified the original four sensors to a single one, placed on the thumb. The thumb, being positioned opposite the other three fingers, serves as the primary point of force during writing and can largely represent the overall force exerted by the hand on the brush.


\subsection{Time Alignment and Skeleton Extraction}
 \begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{design_fig/iswriting.png}
    \caption{Determine if the brush is in writing mode based on the distance between the brush tip and the paper}
    \label{fig: Pen up and Down}
\end{figure*}
\subsubsection{Time Alignment}
In order to match the collected strokes with the sensor data, we need to assign a timestamp to each stroke point and align the stroke's time labels with the sensor data. The most straightforward approach is to record the time when the ink appears, thus corresponding the ink with the timeline, as shown in Figure~\ref{fig: TimeAlignment}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{design_fig/timeallign.png}
    \caption{In the figure, each point of the handwriting is annotated with the corresponding timestamp information. The entire time frame is assumed to be 500 units, providing context for the timing of each point. }
    \label{fig: TimeAlignment}
\end{figure}

Once the stroke's timestamps are assigned, we perform stroke segmentation to analyze the writing process of each stroke in detail \textbf{(DC2)}. Traditional stroke segmentation methods typically handle flat Chinese character images~\cite{10.1145/3548608.3559239, YAO2004631}. However, the rules for segmenting strokes vary between different calligraphy styles. For example, in the character ``Min'', the horizontal and vertical strokes in Clerical Script are made with two separate strokes, while in Regular Script, they are completed in one stroke (Figure \ref{fig: image_of_min_charactor}a).

To enhance the system's adaptability to different calligraphy styles, we adopt an image-based method, segmenting strokes based on the start and end of each brushstroke in the writing process. Specifically, a side-view camera (Device A) combined with OpenCV's CSRT-based tracker is used to detect the contact between the brush tip and the paper (Figure~\ref{fig: Pen up and Down}). The time points of each brush's lifting and landing, combined with the stroke's time data, allow for stroke-by-stroke classification while maintaining the stroke order (Figure~\ref{fig: strokepointsgroup}). Taking the character "Yong" as an example, it is divided into five strokes based on the state of contact between the brush and the paper (Figure \ref{fig: image_of_min_charactor}b).



% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\linewidth]{design_fig/Min.png}
%     \caption{A Chinese character can have different stroke segmentation methods.}
%     \label{fig: image_of_min_charactor}
% \end{figure}


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{group.png}
%     \caption{Pixel points are assigned to brush lift/drop intervals, isolating a stroke in the character ``Yong''.}
%     \label{fig: strokepointsgroup}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.9\linewidth]{design_fig/Stroke_splitting.png}
%     \caption{The character ``Yong'' is segmented into different individual strokes.}
%     \label{fig: Stroke splitting}
% \end{figure}

\begin{figure}[!htbp]
\centering
    \subfloat[A Chinese character can have different stroke segmentation methods.]{ \includegraphics[width=0.8\linewidth]{design_fig/Min.png}}\\
    \subfloat[The character ``Yong'' is segmented into different individual strokes.]{\includegraphics[width=0.9\linewidth]{design_fig/Stroke_splitting.png}}
    \caption{Segmentation of Calligraphy Strokes.}
    \label{fig: image_of_min_charactor}
\end{figure}


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{group.png}
    \caption{Pixel points are assigned to brush lift/drop intervals, isolating a stroke in the character ``Yong''.}
    \label{fig: strokepointsgroup}
\end{figure}





\subsubsection{Skeleton Extraction}
Chinese calligraphy is inherently an art of lines. To clearly highlight key positions in visualizations for students, extracting the skeleton of Chinese characters became a key focus of our work. 
We apply a centroid-based algorithm on the incremental ink deposition to extract the skeleton (Figure \ref{fig: Ink increment}). By connecting centroids over time, a smooth skeleton structure is obtained (Figure \ref{fig: stroke2skeleton}). Ultimately, the writing speed is represented by the offset of the ink's center point in each time frame.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{design_fig/Ink_increment.png}
    \caption{By tracking the added ink traces, time frames are assigned to different positions of the character. The pixel positions from each frame are averaged to generate central axis points, which are then connected to form the character's central axis line.}
    \label{fig: Ink increment}
\end{figure}



\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{design_fig/stroke2skeleton.png}
    \caption{Skeleton extraction of each stroke in the character ``Yong''}
    \label{fig: stroke2skeleton}
\end{figure}




















