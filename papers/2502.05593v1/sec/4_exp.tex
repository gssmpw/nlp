\section{Experiments}
\label{sec:method}

\input{tables/grdbench}


\subsection{Datasets}

The proposed method is evaluated using the GRDBench dataset \cite{dgdr2023}. 
GRDBench, introduced by Che~\etal~\cite{dgdr2023}, serves as a benchmark for domain generalization and consists of eight popular datasets along with two evaluation settings. 
Due to redistribution restrictions, our experiments focus on four of these datasets: IDRID~\cite{IDRID}, APTOS~\cite{APTOS}, RLDR~\cite{RLDR}, and DEEPDR~\cite{DEEPDR}. 
The datasets used are APTOS (3662 samples), DEEPDR (1999 samples), IDRID (516 samples), and RLDR (1593 samples).
\Cref{fig:samples} shows samples from each dataset, highlighting the diversity and complexity of the images.
Although our method was specifically evaluated on these datasets, its principles of semantic data augmentation and invariant risk minimization suggest potential applicability to other medical image analysis tasks.

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{images/dataset_samples.pdf}
    \caption{Samples of different center dataset of GRDBench~\cite{dgdr2023}.}
    \label{fig:samples}
\end{figure}

\subsection{Implementation Details}

For the hyperparameters, the learning rate is set to 0.0001 and the batch size to 256 for all experiments, except for GDRNet~\cite{dgdr2023}, where a batch size of 128 is used. 
This is due to GDRNet’s augmentation strategy, which feeds both the original and augmented images in each iteration, effectively doubling the input size. 
Hence, a batch size of 128 in GDRNet corresponds to processing 256 images per iteration.

The best learning rate and batch size were selected from \{ 0.001, 0.0001, 0.0005, 0.00005, 0.00001\} and \{16, 32, 64, 128, 256\}, respectively, based on the validation set.
The Adam optimizer~\cite{kingma2014adam} is used with a weight decay of 0.00001. 
All experiments were run for 100 epochs, with the learning rate decayed by a factor of 0.1 at epochs 50 and 75. 
The backbone network used for all experiments is ResNet-18~\cite{he2016deep}, with an input size of $224 \times 224$ for the GRDBench dataset. 
PyTorch is used for the deep learning framework, and all experiments were conducted on an NVIDIA RTX 3090 GPU.


\subsection{Experiment on GRDBench}
\label{sec:grdbench}

We conducted an extensive evaluation to compare the performance of our proposed method with several state-of-the-art approaches on the GRDBench dataset~\cite{dgdr2023}. 
The comparison includes a vanilla baseline method (ERM)\cite{vapnik1999nature}, along with advanced techniques from various domains such as ophthalmic disease diagnosis (Fishr, GDRNet)\cite{rame2021ishr, krueger2021out}, 
domain generalization (DGT) methods like IRM~\cite{arjovsky2019invariant} and VREx~\cite{krueger2021out}, and feature representation learning methods (GREEN, CABNet)\cite{liu2020green, he2020cabnet}. 
These methods were selected based on their relevance to domain generalization and their adaptability to medical image tasks, with minimal to no modifications required for application to our dataset. 
For all methods, we used the standard domain generalization augmentation pipeline\cite{dgdr2023} except DRGen, which employed a default augmentation strategy~\cite{dgdr2023}.


\input{tables/ablation}


\textbf{Results:}
The results, shown in \cref{tab:comparison}, demonstrate that our method outperforms all other approaches across multiple metrics (AUC, ACC, F1) on most test domains, particularly on DEEPDR, APTOS, and RLDR. 
Notably, our method achieved the highest AUC in these domains, surpassing other methods by a significant margin. 
While some methods like Fishr~\cite{rame2021ishr} and GDRNet~\cite{dgdr2023} performed well in certain tests, our method consistently outperformed them in key areas, such as the average F1 score and AUC across the four test domains. 
The results also reveal that domain generalization methods (such as IRM~\cite{arjovsky2019invariant} and VREx~\cite{krueger2021out}) show significant improvements over the baseline ERM, but our approach further refines generalization by learning more robust features that preserve diagnostic patterns while increasing intra-class variation. 
Furthermore, compared to the feature representation learning methods like GREEN~\cite{liu2020green} and CABNet~\cite{he2020cabnet}, our method shows notable improvements in both accuracy and robustness, making it a superior choice for domain generalization in medical image tasks.
These results underscore the effectiveness of our approach in addressing domain shifts and improving generalization performance across multiple medical imaging datasets.


\subsection{Analysis via OTDD}
\label{sec:ootd}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{images/ootd_heatmap.pdf}
    \caption{
        Heatmap of the Optimal Transport Dataset Distance (OTDD)\cite{alvarez2020geometric} for different domain pairs in the GRDBench dataset~\cite{dgdr2023}.
    }
    \label{fig:ootd_heatmap}
\end{figure}

\subsection{Ablation Study}
\label{sec:ablation}



\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/vis_virm_class2.pdf}
        \caption{VIRM}
        \label{fig:vis_aug_dir_virm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/vis_medvirm_class2.pdf}
        \caption{Ours}
        \label{fig:vis_aug_dir_ours}
    \end{subfigure}
    \caption{
      Visualizing deep features on GRDBench dataset with two domains using UMAP\cite{mcinnes2018umap}.
      For easier demonstration of the effectiveness of the proposed method, we visualized data from only two domains, although the original setting included four domains. 
      As shown in the green box in \cref{fig:vis_aug_dir_ours}, the proposed method consistently enhances features towards the target domain. 
      In contrast, the VIRM method (as shown in \cref{fig:vis_aug_dir_virm}) produces many random and meaningless directions, especially in the small-batch data scenario, leading to less stable feature representations.
          }
    \label{fig:vis_aug_direction}
  \end{figure*}

  

The goal of the direction selector is to make domain A move in the direction of domain B, the
This can be achieved by optimizing the direction selector by calculating the difference between the augmented features and other domain features. 
We consider two potential methods for direction selectors, including maximum mean difference (MMD~\cite{li2018domain}), covariance of the different source domains (Cov~\cite{wang2024inter}), and domain discriminator~\cite{bai2021decaug}.
Additionally, we consider potential two different settings for the direction selector: \textbf{Soft} and \textbf{Hard}.
\textbf{Soft} indicates that $\mathbb{d} \in [0, 1]$ and \textbf{Hard} indicates that $\mathbb{d} \in \{0, 1\}$.
\Cref{tab:ablation} shows the results of the ablation study, where we compare the performance of our method with different direction selectors and settings.




To further evaluate the proposed method’s ability to reduce domain discrepancies, we employed the Optimal Transport Dataset Distance (OTDD)\cite{alvarez2020geometric}. 
OTDD measures the distance between two datasets using optimal transport, where smaller values indicate reduced domain differences. 
Alvarez\etal~\cite{alvarez2020geometric} demonstrated a linear relationship between OTDD and transferability performance, showing a strong and significant correlation, which highlights OTDD’s predictive power for transferability across datasets.

We innovatively apply OTDD to assess the generalization between two distinct domains within the same dataset. 
As shown in \cref{fig:samples}, our method, excluding the GREEN method~\cite{liu2020green}, achieves the lowest average OTDD across domain pairs, indicating superior generalization across diverse domains.
The GREEN method~\cite{liu2020green} performs better in OTDD due to its use of graph convolutional networks (GCNs) to model inter-class dependencies. 
Since OTDD evaluates label-to-label distances, this results in improved OTDD performance for GREEN. 
However, our method exhibits significantly better classification performance compared to GREEN, highlighting its practicality in classification tasks.
  
Another key observation is in the last two rows, emphasizing the importance of incorporating invariant risk constraints. 
This also indirectly supports the perspective presented in VIRM. 
Furthermore, when compared to VIRM, our approach achieves better OTDD results, reinforcing its enhanced generalization capability across domain pairs.


\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/vis_tsne_kde/class_0.pdf}
        \caption{Class 0}
        \label{fig:vis_kde_0}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/vis_tsne_kde/class_1.pdf}
        \caption{Class 1}
        \label{fig:vis_kde_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/vis_tsne_kde/class_2.pdf}
        \caption{Class 2}
        \label{fig:vis_kde_2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/vis_tsne_kde/class_3.pdf}
        \caption{Class 3}
        \label{fig:vis_kde_3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/vis_tsne_kde/class_4.pdf}
        \caption{Class 4}
        \label{fig:vis_kde_4}
    \end{subfigure}
    \caption{
    %   Visualizing deep features using UMAP\cite{mcinnes2018umap}.
    KDE-Based Visualization of Deep Features Using UMAP\cite{mcinnes2018umap}.
          }
    \label{fig:vis_kde}
\end{figure*}


\subsection{Visualization}
\label{sec:visualization}


\subsubsection{Compare with VIRM}

To further emphasize the core advantages of our method, we compared its feature space visualization with that of the VIRM\cite{zhu2024enlarging} method. As shown in the green box in \cref{fig:vis_aug_dir_virm}, VIRM employs a strategy of randomly selecting augmentation directions, which results in a more dispersed feature space visualization. 
This suggests that the feature representations learned by VIRM are less stable and compact in the feature space. While this approach may suffice for large datasets that can cover most of the feature space, it becomes less robust for small-batch datasets.

In small-batch settings, the randomness in selecting augmentation directions can lead to meaningless transformations, exacerbating the dispersion of feature space representations. In contrast, as illustrated in the green box in \cref{fig:vis_aug_dir_ours}, our method introduces a domain-covariance-guided augmentation direction selector. 
This ensures that features are consistently enhanced toward the target domain, resulting in a tighter and more stable feature space overlap. 
As a result, the feature space visualization of our method is more compact and stable.

Zhu \etal~\cite{zhu2024enlarging} proposed the use of semantic data augmentation to improve IRM-based methods by enhancing feature overlap through semantic diversity, thereby improving model generalization in large-scale datasets. 
Similarly, our method demonstrates superior performance on small-batch medical datasets by guiding feature enhancement toward the target domain. This ensures more stable and compact feature space representations, which facilitate better generalization.

While \cref{fig:meaningless} provides an idealized model, \cref{fig:vis_aug_direction} presents the visualization of our method applied to real-world datasets. 
These results further validate the effectiveness of our approach and, more specifically, the augmentation direction selector we proposed.



\subsubsection{KDE Visualization}

Feature support overlap is a crucial determinant of successful generalization\cite{ahuja2021invariance}. 
Furthermore, it reflects the extent of dataset diversity shifts along the feature dimensions. 
To evaluate this overlap, we employed kernel density estimation (KDE)\cite{wkeglarczyk2018kernel}, which provides a clear visualization of the feature space overlap. 
As shown in \Cref{fig:vis_kde}, our method demonstrates notable overlap in the GRDBench dataset, offering an intuitive representation of its effectiveness in reducing domain differences and enhancing generalization.



\subsubsection{Feature Visualization}
% Similarly, we visualized the distribution of features for data from all domains within the same class in the feature space.
% As illustrated in \cref{fig:vis_class_0}, the features enhanced by our method consistently move towards directions aligned with other domains rather than random directions. 
% This alignment results in a more compact feature space overlap, which is beneficial for improving the generalization performance of the model.

We also visualized the distribution of features from all domains within the same class in the feature space. 
As illustrated in \cref{fig:vis_class_0}, the features enhanced by our method consistently move toward directions that align with other domains, rather than following random directions. 
This alignment results in a more compact feature space overlap, which significantly contributes to improving the generalization performance of the model.

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{images/vis_class_0_embedding_with_domains.pdf}
    \caption{
        UMAP\cite{mcinnes2018umap}. Visualization of Class 0 Deep Features on the GRDBench Dataset\cite{dgdr2023} (Original Features Only).
    }
    \label{fig:vis_class_0}
\end{figure}



% \subsection{Hyper-Parameters}
% \label{sec:parameters}

% The selection of hyperparameters plays a crucial role in determining the performance of a model. 
% Therefore, we conducted a detailed analysis of the key hyperparameters in our proposed method to evaluate their impact on performance.

% \subsubsection{Impact of Batch Size}
% We conducted experiments to evaluate the impact of different batch sizes on the performance of our proposed method. 
% As shown in \cref{fig:hyperparameters}, the batch size has a noticeable effect on all three evaluation metrics: AUC, ACC, and F1 scores. 
% The results indicate that the optimal performance for AUC is achieved with a batch size of 256, where the model reaches an AUC of 0.782. For ACC and F1 scores, the model performs best with a batch size of 128, where it achieves the highest ACC (0.397) and F1 score (0.272).
% The batch size of 16 and 64 yielded relatively lower performance compared to larger batch sizes, particularly in terms of F1 score. 
% These findings suggest that larger batch sizes improve generalization and contribute to better overall performance.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.45\textwidth]{images/params.pdf}
%     \caption{
%         Impact of batch size and learning rate on model performance.
%     }
%     \label{fig:hyperparameters}
% \end{figure}


% \subsubsection{Impact of Learning Rate}

% \Cref{fig:hyperparameters} summarizes the effect of various learning rates on the performance of our method. 
% The learning rate plays an important role in determining the convergence speed and stability of the model. 
% For this experiment, a learning rate of 0.0001 resulted in the highest F1 score (0.265), which indicates the best balance between precision and recall. 
% Meanwhile, the AUC and ACC values remained relatively stable across different learning rates, with the 0.0001 rate also achieving the highest AUC (0.782) and maintaining solid ACC (0.358). 
% Other learning rates, such as 0.00005, 0.0005, and 0.001, did not provide significant improvements in performance, and the F1 score was lower in those cases. 
% These results underscore the importance of selecting an appropriate learning rate to optimize model performance.


