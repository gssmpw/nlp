\section{Method}
\label{sec:method}

\subsection{Problem Definition}
\label{sec:medvirm}

Deep learning models excel at linearizing features\cite{upchurch2017deep}, which enables them to generate features semantically different from the original ones by performing linear translations on deep features while preserving the original label\cite{ISDA}. 
Zhu \etal \cite{zhu2024bayesian} redefined the paradigm of semantic data augmentation by decomposing it into two components: semantic direction and semantic strength. 
The semantic direction specifies the features to be altered, enabling the transformation into different semantics, while the semantic strength controls the magnitude of the shift, as excessive augmentation can alter the label. 
Therefore, the medical semantic data augmentation module proposed in this paper is defined as follows:
\begin{equation}
	\begin{aligned}
		\tilde{\mathbf{z}}_i = \mathbf{z}_i + \mathbf{d} \odot \mathbf{\xi},
	\end{aligned}
	\label{eq:sda}
\end{equation}
where $\mathbf{d}$ is the augment direction, $\mathbf{\xi}$ is the augment magnitude, and $\odot$ denotes the element-wise product.

Given feature $\mathbf{z}_i = \mathbf{\Phi}(\mathbf{x}_i)$ there exists a distribution $p(\mathbf{m}|\mathbf{z}_i)$ 
such that $\tilde{\mathbf{z}}_i =\mathbf{z}_i + \mathbf{\xi}_j, \forall \mathbf{\xi}_j \in p(\mathbf{\xi}|\mathbf{z}_i)$ with label $\tilde{y}_i$.

Vicinal Invariant Risk Minimization (VIRM)\cite{zhu2024enlarging}  is a domain generalization method that aims to minimize the expected loss across multiple training domains by learning domain-invariant features.
VIRM introduces Bayesian semantic data augmentation\cite{zhu2024bayesian} strategy to enhance domain overlap and improve invariant feature learning.
VIRM is formulated as the following optimization problem:


\begin{definition}[VIRM]
	\label{def:virm}
	Given data representation $\mathbf{\Phi}: \mathcal{X} \rightarrow \mathcal{H}$, classifier $\mathbf{w}: \mathcal{H} \rightarrow \mathcal{Y}$,
	the VIRM objective minimizes the expected loss, as defined below:
	\begin{equation}
		\begin{aligned}
			&\min_{\mathbf{\Phi, \mathbf{w}}}
			\sum_{e \in  \mathcal{E}_{\mathrm{tr}}} \sum_{\mathbf{x}_i \in e} \ell_e(\mathbf{w} (\mathbf{z}_i + \mathbf{d} \cdot \mathbf{\xi}), y_i), \\
			&\mathrm{s.t.} \quad \mathbf{w} \in \underset{\bar{\mathbf{w}}:\mathcal{H}\to\mathcal{Y}}{\operatorname*{\arg\min}} \, \ell_e(\bar{\mathbf{w}}\circ\mathbf{\Phi}),\forall e \in \mathcal{E}_{\mathrm{tr} },
			\end{aligned}
	\end{equation}
	where $\mathbf{\xi}$ is the translation magnitude sampled from the augmentable distribution $p(\mathbf{\xi} | \mathbf{z}_i)$, $\mathbf{d}$ is augment direction.
\end{definition}

Thus, the proposed method including two parts: the invariant risk and the medical semantic data augmentation (MedSDA) module.


\subsection{MedSDA Module}
\label{sec:medsda}
In detail, the MedSDA contains two components: the director (\cref{sec:director}) and the estimator (\cref{sec:estimator}) as shown in \cref{fig:framework}.
The director is responsible for selecting the augment direction $\mathbf{d}$,
while the estimator is responsible for estimating the augmentable distribution $p(\mathbf{\xi}|\mathbf{z}_i)$.
The director and the estimator are trained jointly to minimize the loss function of the MedSDA module.
The MedSDA module is designed to enhance the feature overlap between domains,
thus improving the generalization performance of the model.
Therefore, $\tilde{\mathbf{z}}_i$ is generated using the \cref{eq:sda}
% \begin{equation}
% 	\tilde{\mathbf{z}} = \mathbf{z} + \mathbf{d} \odot \mathbf{\xi},
% 	\label{eq:z_tilde}
% \end{equation}


\subsection{Director}
\label{sec:director}
We employed inter-domain covariance as the guiding principle for selecting augmentation directions.
Let $x \in \mathcal{X}$ be the input features with domains $\mathcal{D}$, where $x$ has shape $(N, \text{out\_features})$ and $\text{domains} \in \mathcal{D}$ has shape $(N,)$. The covariance between the domains is computed as:
\begin{equation}
	C_{e} = \text{Cov}(\mathbf{x} | e), \quad \forall e \in \mathcal{E}_{\mathrm{tr} }.
\end{equation}

The covariance difference for each domain is given by:
\begin{equation}
	\Delta C_e = C_e - \mathbb{E}_{\mathcal{D}}[C], \quad \forall e \in \mathcal{E}_{\mathrm{tr} },
\end{equation}
where $\mathbb{E}_{\mathcal{D}}[C]$ represents the mean covariance across all domains.

The direction of the maximum difference for each domain is:
\begin{equation}
	\mathbf{d}_e = \arg\max_{d \in \mathcal{H}} |\Delta C_e[d]|.
\end{equation}

The binary direction selection is:
\begin{equation}
	\mathbf{d}_{e}^* = 
	\begin{cases}
		1, & \text{if} \; |\Delta C_e[d]| > \mathbb{E}[\Delta C_e], \\
		0, & \text{otherwise.}
	\end{cases}
\end{equation}

\subsection{Estimator}
\label{sec:estimator}
In the VIRM implementation, we need to estimate the augmentable distribution $p(\mathbf{\xi} |\mathbf{z}_i)$ while ensuring that $\tilde{y}_i = y_i$.
We introduce model $q_{\phi_e}(\mathbf{\xi}|\mathbf{\mathbf{z}}_i)$ to approximate the distribution $p(\mathbf{\xi} |\mathbf{z}_i)$ of domain $e$.
The Kullback-Leibler (KL) divergence measures the similarity between these two distributions, aiming to make $q_{\phi_e}(\mathbf{\xi} |\mathbf{z}_i)$ closely match $p(\mathbf{\xi} |\mathbf{z}_i)$ by maximizing the KL divergence.
Thus, our optimization goal of the SDA module is defined as:
\begin{equation}
	\label{eq:bsda}
	\phi_e = \underset{\phi_e}{\arg\max} D_{KL}(q_{\phi_e}(\mathbf{\xi}|\mathbf{\mathbf{z}}_i) || p(\mathbf{\xi}|\mathbf{\mathbf{z}}_i)).
\end{equation}

Another challenge with \cref{eq:bsda} arises when domain-specific estimators are introduced, as they prevent feature sharing across domains, hindering the transformation of domain-invariant features. 
Although using shared estimators across domains can address this issue, it also increases the optimization complexity of the model. 
Thus, our domain-shared SDA module is defined as follows:
\begin{equation}
	\label{eq:sdsda}
	\phi = \underset{\phi}{\arg\max} D_{KL}(q_{\phi}(\mathbf{\xi}|\mathbf{\mathbf{z}}_i) || p(\mathbf{\xi}|\mathbf{\mathbf{z}}_i)).
\end{equation}

While \cref{eq:sdsda} allows us to estimate the distribution  $p(\mathbf{\xi}|\mathbf{\mathbf{z}}_i)$, additional constraints on the augmented features are essential to maintain label consistency.
Specifically, consistency constrains as follows:
\begin{equation}
	\label{eq:label_consistency}
	\min_{\mathbf{\Phi, \mathbf{w}}, \mathbf{\phi}} \sum_{\mathbf{x}_i \in \mathcal{E}}  \ell (\mathbf{w}(\tilde{\mathbf{z}}_i), y_i).
\end{equation}


\subsection{Loss Function}
\label{sec:loss}
Our method is formulated as the following optimization problem:
\begin{equation}
	\label{eq:virm}
	\begin{aligned}
		\mathcal{L}_{\text{VIRM}}(\Phi, \mathbf{w}, \phi) 
		& = \mathcal{L}_{\text{IRM}} + \alpha \mathcal{L}_{\phi}.
	\end{aligned}
\end{equation}

\Cref{eq:virm} consists of two components, which are the invariant risk and the loss function of the module of domain-shared SDA module $\mathcal{L}_{\phi}$.
The loss function of the domain-shared SDA module is defined as follows:
\begin{equation}
	\begin{aligned}
	& \mathcal{L}_{\phi} = 
	& - \frac{1}{2}\sum_{i=0}^n(1 + \log(\boldsymbol{\sigma}^2) - \boldsymbol{\sigma}^2 ) + 
	\frac{1}{2n}\sum_{l=1}^n (\mathbf{\hat{z}_i} - \mathbf{z}_i)^2.
  \label{eq:loss_dssda}
  \end{aligned}
\end{equation}

For first term $\mathcal{L}_{\text{IRM}}$ in \cref{eq:virm}, there are various approaches to implementing IRM, and our experiments demonstrate that combining VRM with the VREx~\cite{krueger2021out} method yields superior performance. 
VREx improved the constraints of IRMv1~\cite{arjovsky2019invariant} by introducing a penalty term for the risk variance, 
which induces the model to equalize the treatment of risk across multiple training domains, thus enhancing its robustness to changes in the underlying distribution.

The optimization objectives of VREx are defined as follows:
\begin{equation}
	\label{eq:vrex}
	\mathcal{L}_{\text{VREx}}(\Phi, \mathbf{w}) = \min_{\Phi, \mathbf{w}}  \beta \cdot \text{Var}\left(\{\ell_e(\Phi, \mathbf{w})\}_{e=1}^{M}\right) + \sum_{e=1}^{M} \ell_e(\Phi, \mathbf{w}),
\end{equation}
where $ \theta $ denotes the model parameters, 
$\ell_e(\theta) $ is defined as the loss (or risk) of the model on the $e$-th training domain, 
$ M$ is the total number of training domains, 
$ \beta $ is a positive weighting parameter, 
and $ \text{Var} $ denotes the variance.


% \todo{Add the algorithm for MedVIRM}

% \begin{algorithm}[H]
% 	\caption{The MedVIRM algorithm}  \label{algorithm:BSDA}
%   \begin{algorithmic}
%   \STATE 
%   \STATE \textbf{Input:} $\mathcal{D}$
%   \STATE Randomly initialize $\Theta$, $\phi_{\mathbf{a}}$, and $\phi_{\mathbf{m}}$
%   \STATE \textbf{for} $t=0$ \textbf{to} $T$
% 		\STATE \hspace{0.5cm} Sample a mini-batch $\{ \mathbf{x}_i, \mathbf{y}_i \}_{i=1}^B$ from $\mathcal{D}$;
% 		\STATE \hspace{0.5cm} Compute features $\mathbf{a}_i = G(\mathbf{x}_i)$;
% 		\STATE \hspace{0.5cm} Estimate variance of magnitude distribution $\boldsymbol{\sigma}_i$;
% 		\STATE \hspace{0.5cm} Compute magnitude $\mathbf{m}_i = \boldsymbol{\sigma}_i \odot \boldsymbol{\epsilon}_i$;
% 		\STATE \hspace{0.5cm} Compute \emph{augmented feature $\tilde{\mathbf{a}}_i$} according to \cref{eq:sda};
% 		\STATE \hspace{0.5cm} Compute reconstructed feature $\mathbf{\hat{a}}_i$;
% 		\STATE \hspace{0.5cm} Compute $\mathcal{L}$ according to \cref{eq:loss};
% 		\STATE \hspace{0.5cm} Update $\Theta$, $\phi_{\mathbf{a}}$, and $\phi_{\mathbf{m}}$;
%   \STATE \textbf{end for} 
%   \end{algorithmic}
%   \label{algorithm}
%   \end{algorithm}
