\section{Method}
\label{sec:method}

\subsection{Problem Definition}
\label{sec:medvirm}

Deep learning models excel at linearizing features \cite{upchurch2017deep}, enabling them to generate semantically different features by performing linear transformations on deep features while preserving the original label \cite{ISDA}. Zhu \etal \cite{zhu2024bayesian} redefined semantic data augmentation by decomposing it into two components: semantic direction and semantic strength. The semantic direction specifies the features to be altered, enabling transformation into different semantics, while semantic strength controls the magnitude of the shift, as excessive augmentation can alter the label.
Therefore, the medical semantic data augmentation module proposed in this paper is defined as:
\begin{equation}
	\begin{aligned}
		\tilde{\mathbf{z}}_i = \mathbf{z}_i + \mathbf{d} \odot \mathbf{\xi},
	\end{aligned}
	\label{eq:sda}
\end{equation}
where $\mathbf{z}_i = \mathbf{\Phi}(\mathbf{x}_i)$, $\mathbf{d}$ is the augment direction, $\mathbf{\xi} \sim p(\mathbf{\xi}|\mathbf{z}_i)$ is the augment magnitude, and $\odot$ denotes the element-wise product.

Vicinal Invariant Risk Minimization (VIRM) \cite{zhu2024enlarging} is a domain generalization method aiming to minimize the expected loss across multiple training domains by learning domain-invariant features. VIRM introduces a Bayesian semantic data augmentation \cite{zhu2024bayesian} strategy to enhance domain overlap and improve invariant feature learning. VIRM is formulated as the following optimization problem:

\begin{definition}[VIRM]
	\label{def:virm}
	Given data representation $\mathbf{\Phi}: \mathcal{X} \rightarrow \mathcal{H}$, classifier $\mathbf{w}: \mathcal{H} \rightarrow \mathcal{Y}$, the VIRM objective minimizes the expected loss, as defined below:
	\begin{equation}
		\begin{aligned}
			&\min_{\mathbf{\Phi, \mathbf{w}}}
			\sum_{e \in  \mathcal{E}_{\mathrm{tr}}} \sum_{\mathbf{x}_i \in e} \ell_e(\mathbf{w} (\mathbf{z}_i + \mathbf{d} \odot \mathbf{\xi}), y_i), \\
			&\mathrm{s.t.} \quad \mathbf{w} \in \underset{\bar{\mathbf{w}}:\mathcal{H}\to\mathcal{Y}}{\operatorname*{\arg\min}} \, \ell_e(\bar{\mathbf{w}}\circ\mathbf{\Phi}),\forall e \in \mathcal{E}_{\mathrm{tr} },
		\end{aligned}
	\end{equation}
	where $\mathbf{\xi}$ is the translation magnitude sampled from the augmentable distribution $p(\mathbf{\xi} | \mathbf{z}_i)$, and $\mathbf{d}$ is the augment direction.
\end{definition}

Thus, the proposed method includes two parts: the invariant risk and the medical semantic data augmentation (MedSDA) module.

\subsection{MedSDA Module}
\label{sec:medsda}
The MedSDA module consists of two components: the director (\cref{sec:director}) and the estimator (\cref{sec:estimator}) as shown in \cref{fig:framework}. The director is responsible for selecting the augment direction $\mathbf{d}$, while the estimator estimates the augmentable distribution $p(\mathbf{\xi}|\mathbf{z}_i)$. These components are jointly trained to minimize the loss function of the MedSDA module. The MedSDA module is designed to enhance the feature overlap between domains, improving the generalization performance of the model. Therefore, $\tilde{\mathbf{z}}_i$ is generated using \cref{eq:sda}.

\subsection{Director}
\label{sec:director}
We employ inter-domain covariance as the guiding principle for selecting augmentation directions. Let $x \in \mathcal{X}$ be the input features with domains $\mathcal{D}$, where $x$ has shape $(N, \text{out\_features})$ and $\text{domains} \in \mathcal{D}$ has shape $(N,)$. The covariance between the domains is computed as:
\begin{equation}
	C_{e} = \text{Cov}(\mathbf{x} | e), \quad \forall e \in \mathcal{E}_{\mathrm{tr} }.
\end{equation}

The covariance difference for each domain is given by:
\begin{equation}
	\Delta C_e = C_e - \mathbb{E}_{\mathcal{D}}[C], \quad \forall e \in \mathcal{E}_{\mathrm{tr} },
\end{equation}
where $\mathbb{E}_{\mathcal{D}}[C]$ represents the mean covariance across all domains.

The direction of the maximum difference for each domain is:
\begin{equation}
	\mathbf{d}_e = \arg\max_{d \in \mathcal{H}} |\Delta C_e[d]|.
\end{equation}

The binary direction selection is:
\begin{equation}
	\mathbf{d}_{e}^* = 
	\begin{cases}
		1, & \text{if} \; |\Delta C_e[d]| > \mathbb{E}[\Delta C_e], \\
		0, & \text{otherwise.}
	\end{cases}
\end{equation}

\subsection{Estimator}
\label{sec:estimator}
In VIRM implementation, we estimate the augmentable distribution $p(\mathbf{\xi} |\mathbf{z}_i)$ while ensuring $\tilde{y}_i = y_i$. We introduce model $q_{\phi_e}(\mathbf{\xi}|\mathbf{\mathbf{z}}_i)$ to approximate the distribution $p(\mathbf{\xi} |\mathbf{z}_i)$ of domain $e$. The Kullback-Leibler (KL) divergence measures the similarity between these two distributions, aiming to make $q_{\phi_e}(\mathbf{\xi} |\mathbf{z}_i)$ closely match $p(\mathbf{\xi} |\mathbf{z}_i)$ by maximizing the KL divergence. Thus, our optimization goal of the SDA module is defined as:
\begin{equation}
	\label{eq:bsda}
	\phi_e = \underset{\phi_e}{\arg\max} D_{KL}(q_{\phi_e}(\mathbf{\xi}|\mathbf{\mathbf{z}}_i) || p(\mathbf{\xi}|\mathbf{\mathbf{z}}_i)).
\end{equation}

Another challenge with \cref{eq:bsda} arises when domain-specific estimators are introduced, as they prevent feature sharing across domains, hindering the transformation of domain-invariant features. Although using shared estimators across domains can address this issue, it increases the model's optimization complexity. Thus, our domain-shared SDA module is defined as:
\begin{equation}
	\label{eq:sdsda}
	\phi = \underset{\phi}{\arg\max} D_{KL}(q_{\phi}(\mathbf{\xi}|\mathbf{\mathbf{z}}_i) || p(\mathbf{\xi}|\mathbf{\mathbf{z}}_i)).
\end{equation}

While \cref{eq:sdsda} estimates the distribution $p(\mathbf{\xi}|\mathbf{\mathbf{z}}_i)$, additional constraints on the augmented features are essential to maintain label consistency. Specifically, consistency is constrained as:
\begin{equation}
	\label{eq:label_consistency}
	\min_{\mathbf{\Phi, \mathbf{w}}, \mathbf{\phi}} \sum_{\mathbf{x}_i \in \mathcal{E}}  \ell (\mathbf{w}(\tilde{\mathbf{z}}_i), y_i).
\end{equation}

\subsection{Loss Function}
\label{sec:loss}
Our method is formulated as the following optimization problem:
\begin{equation}
	\label{eq:virm}
	\begin{aligned}
		\mathcal{L}_{\text{VIRM}}(\Phi, \mathbf{w}, \phi) 
		& = \mathcal{L}_{\text{IRM}} + \alpha \mathcal{L}_{\phi}.
	\end{aligned}
\end{equation}

\Cref{eq:virm} consists of two components: the invariant risk and the loss function of the domain-shared SDA module $\mathcal{L}_{\phi}$. The loss function of the domain-shared SDA module is defined as:
\begin{equation}
	\begin{aligned}
	& \mathcal{L}_{\phi} = 
	& - \frac{1}{2}\sum_{i=0}^n(1 + \log(\boldsymbol{\sigma}^2) - \boldsymbol{\sigma}^2 ) + 
	\frac{1}{2n}\sum_{l=1}^n (\mathbf{\hat{z}_i} - \mathbf{z}_i)^2.
  \label{eq:loss_dssda}
  \end{aligned}
\end{equation}

For the first term $\mathcal{L}_{\text{IRM}}$ in \cref{eq:virm}, there are several methods to implement IRM. However, our experiments align with the results of the VIRM framework, where VREx~\cite{krueger2021out} was adopted as the specific implementation for IRM. VREx improves upon previous IRM formulations by introducing a penalty term for the risk variance, thus encouraging the model to reduce risk disparities across domains, enhancing its robustness to changes in the underlying distribution.
