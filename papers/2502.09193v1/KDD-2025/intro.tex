\section{Introduction}
\label{sec:intro}
% Overfitting is a well-known problem in ML and, mostly, DL
One of the key challenges in machine learning is developing models that can generalize their predictions to new, unseen data beyond the scope of the training set. 
When the predictive accuracy of a model on the training set far exceeds that on the test set, this indicates a phenomenon known as \textit{overfitting}.
In general, the impact of overfitting is more pronounced for highly complex models like recent deep neural networks with billions of parameters. 
To compensate for the risk of overfitting, these models require massive amounts of training data, which may not always be feasible. 
%For instance, Gemini 1.5, the latest multimodal large language model (LLM) developed by Google, has been trained on a wide array of data extracted from the vast collection of Google services and platforms, including search, email, maps, news, photos, and videos~\cite{reid2024gemini}.


% Traditional techniques to combat overfitting
Therefore, several strategies have been proposed in the literature to mitigate the problem of model overfitting. For example, \textit{early stopping} interrupts the training phase before the model starts learning the noise in the data rather than the actual underlying input/output relationship.
Furthermore, \textit{data augmentation} is a technique that artificially increases the training set. For example, in the context of image data, this can include applying translation, flipping, and rotation transformations to input samples.
Finally, \textit{regularization} is a collection of training/optimization techniques that try to eliminate irrelevant factors by assessing the importance of features, preventing minor input changes from causing significant output variations.

% Intuition behind the relationship between overfitting and the ability to find counterfactual examples (insert picture)
In this work, we offer an entirely new perspective on model overfitting, establishing a connection with the ability to generate \textit{counterfactual examples}~\cite{wachter2017hjlt}. The notion of counterfactual examples has been successfully used, for instance, to attach post-hoc explanations for predictions of individual instances in the form: ``\textit{If A had been different, B would \textbf{not} have occurred}''~\citep{stepin2021survey}. 
Generally, finding the counterfactual example for an instance resorts to searching for the minimal perturbation of the input that crosses the decision boundary induced by a trained model. This task reduces to solving a constrained optimization problem.

In the presence of a strong degree of model overfitting, the decision boundary learned becomes a highly convoluted surface, up to the point where it perfectly separates every training input sample. 
Therefore, each data point, on average, is ``closer'' to the decision boundary, making it easier to find the best counterfactual example. 
The intuitive explanation of this claim is illustrated in Figure~\ref{fig:intuition}.

\begin{figure}
\centering
\begin{subfigure}[htb!]{.4\linewidth}
\includegraphics[width=.9\linewidth]{img/no-overfitting}
\caption{No overfitting}
\end{subfigure}
\qquad
\begin{subfigure}[h]{.4\linewidth}
\includegraphics[width=.9\linewidth]{img/overfitting}
\caption{Overfitting}
\end{subfigure}%
\caption{Distance between an input data point ($\inst$) and its counterfactual example ($\cfinst$): On average, this may be higher for a well-trained model (a) than an overfitted model (b).}
\label{fig:intuition}
\end{figure}

% Novel contributions of this work
Following this idea, we introduce a novel \textit{counterfactual regularization} term in the training loss that controls overfitting by enforcing a margin between each instance and its hypothetical counterfactual.
Below, we summarize the primary novel contributions of our work:
\begin{itemize}
\item[$(i)$] We are the first to explore the relationship between generalizability and counterfactual explanations.
\item[$(ii)$] We show that counterfactual examples can effectively guide and enhance the model training process.
\item[$(iii)$] We propose a counterfactual regularizer (CF-Reg) that outperforms existing regularization techniques.
\item[$(iv)$] We cast our regularization method in a flexible and extensible framework that is compatible with any differentiable counterfactual example generator. The source code for our method is available at the following GitHub repository: \url{https://anonymous.4open.science/r/COCE-80B4/README.md}.
\end{itemize}
% Firstly, it can be used to quantify the degree of overfitting of a model.
% Secondly, we can \textit{simultaneously} learn both the base model and its associated counterfactual explainer. 
% Thirdly, we can use the entire training set without the need to hold out samples for testing.

% Paper's structure
The remainder of this paper is structured as follows. In Section~\ref{sec:related}, we summarize related work. Section~\ref{sec:background} reviews background and preliminary concepts, useful for our problem formulation in Section~\ref{sec:problem}. In Section~\ref{sec:method}, we describe our method that is validated through extensive experiments in Section~\ref{sec:experiments}. 
We discuss the limitations of our method in Section~\ref{sec:discussion}.
Finally, Section~\ref{sec:conclusion} concludes our work.
