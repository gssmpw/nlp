\section{Background and Preliminaries}
\label{sec:background}
% STANDARD PREDICTIVE MODEL TRAINED UNDER SUPERVISION
Let $\model: \X \mapsto \Y$ denote a predictive model parameterized by a set of (learnable) weights $\params \in \paramspace$ that takes an input $\inst\in \X\subseteq \R^n$ and maps it to an output $y\in \Y$. 
% I'D MOVE THE FOLLOWING PARAGRAPH IN THE SECTION BELOW
% Furthermore, we define $\model(x) = l(\scoringf(x))$, where $\scoringf: \X \mapsto \R$ is a scoring function (e.g., logits) and $l: \R \mapsto \Y$ is a function that maps the output logit scores to discrete labels.
In the standard supervised learning setting, the optimal weights ($\params^*$) are found by minimizing a specific loss function ($\Loss$) computed on a training set ($\dataset$) of $m$ i.i.d. labeled instances, $\dataset=\{(\inst_i, y_i)\}_{i=1}^m$.
\begin{equation}
\label{eq:train}
\params^* = \argmin_{\params}\Big\{\Loss(\params;\dataset)\Big\} = \argmin_{\params}\Bigg\{\frac{1}{|\dataset|} \sum_{i=1}^m\loss(f_{\params}(\inst_i),y_i)\Bigg\}.
\end{equation}
Here, $\loss$ represents an instance-level error between the model's prediction for a given input ($\inst_i$) and its corresponding actual label ($y_i$), such as cross-entropy (for classification tasks) or mean squared error (for regression tasks).

% COUNTERFACTUAL EXPLANATION GENERATOR
Furthermore, we assume to have available a counterfactual generator model $\cfmodel: \X \mapsto \X$, for the predictive model $\model$, that takes as input a data point $\inst$ and produces as output its corresponding (optimal) counterfactual $\cfinst^*$. We stress that the counterfactual generator model $\cfmodel$ depends on the predictive model under consideration, since it typically resorts to solving a constrained objective as follows:
\begin{equation}
\label{eq:cf-train}
\begin{aligned}
\cfmodel(\inst) = \cfinst^* = \argmin_{\cfinst\in \X}  &\delta(\inst, \cfinst)\\
\text{ s.t.: } &\model(\inst) \neq \model(\cfinst).
\end{aligned}
\end{equation}

Here, $\delta:\X\times \X \mapsto \R_{\geq 0}$ is a function that measures the distance between the original input data point and its counterfactual. In practice, typically, $\delta$ resorts to computing the $L_1$- or $L_2$-norm of the resulting displacement vector between the original and the counterfactual example.

% I'D REMOVE THE FOLLOWING PARAGRAPH
% It is important to note that finding the counterfactual example typically involves solving an instance-level optimization task rather than learning a generalizable mapping from data. In other words, the counterfactual generator $\cfmodel$ is \textit{not} treated as a parameterized model trained as in Eq.~(\ref{eq:cf-train}). Instead, it can be simply denoted as $g$ (without any learnable parameters) and finds the optimal counterfactual example when input with a specific query sample $\inst$.

In general, for a given input $\inst$, there could be several, possibly infinitely many \textit{valid} counterfactuals: $\cfinst_1, \cfinst_2, \ldots$ In this work, a valid counterfactual is considered as any instance $\cfinst$ where $\model(\inst) \neq \model(\cfinst)$. More refined notions of validity are also conceivable, such as those entailing the \textit{plausibility} of the generated counterfactual, which assesses whether the counterfactual example is indeed realistic. For example, suppose we have trained an image classifier to distinguish between birds and rabbits. In this context, a plausible (and valid) counterfactual for a rabbit must be an image showing a bird's beak.
However, for the purpose of this work, we are only interested in characterizing the ability to find a valid counterfactual and relate this to the degree of model overfitting, regardless of its plausibility.

\begin{definition}[$\varepsilon$-Valid Counterfactual Example -- $\varepsilon$-VCE] Let $\inst \in \X$ be an input sample, $\model$ a trained model, $\delta:\X\times \X \mapsto \R_{\geq 0}$ a distance function, and $\varepsilon \in \R_{>0}$ a fixed threshold. Any $\cfinst\neq \inst$, such that $\model(\inst) \neq \model(\cfinst)$ and $\delta(\inst, \cfinst) \leq \varepsilon$ is an $\varepsilon$-valid counterfactual example for $\inst$.
\end{definition}

