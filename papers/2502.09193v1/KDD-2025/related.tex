\section{Related Work}
\label{sec:related}

\subsection{Model Overfitting}
\label{subsec:overfitting}
The challenge of model overfitting has been a well-known issue in machine learning. It refers to the inability of a model to generalize effectively from the training dataset to new, unseen test data.
%
% OPTIONAL
% Several factors can contribute to this phenomenon. For instance, if the training dataset is too small or lacks representative data, the resulting model may inadvertently learn noise rather than the genuine underlying patterns of interest. Moreover, when the hypothesis space is too complex (i.e., highly expressive), the model may achieve high accuracy on the training set but exhibit lower consistency. Consequently, models trained on different datasets may vary significantly from each other.
% This issue motivates the search for the right balance between bias and variance of a model.
%
Various strategies have been proposed in the literature to reduce the effects of overfitting, which can be broadly categorized as follows.

\smallskip
\noindent \textit{\textbf{Early-Stopping.}} This strategy aims to mitigate the phenomenon known as ``learning speed slow-down.'' This issue occurs when the accuracy of a model ceases to improve or even worsens due to noise-learning. The concept has a long history, dating back to the 1970s in the context of the Landweber iteration~\citep{raskutti2011ccc}. It has since been widely adopted in iterative algorithms, particularly for training deep neural networks in combination with backpropagation~\citep{caruana2000neurips}.

\smallskip
\noindent \textit{\textbf{Data Augmentation.}} The more complex the model, the higher the number of parameters that need to be learned. Therefore, the size of the training set must be adequate to avoid overfitting. Data augmentation techniques play a crucial role in enhancing model generalization across various domains, including pattern recognition and image processing. These techniques aim to expand existing datasets to generate additional data. Typically, four main approaches are employed~\citep{sun2014cvpr,karystinos2000tnn,yip2008bio}: \textit{(i)} acquiring new data, \textit{(ii)} introducing random noise to the existing dataset, \textit{(iii)} reprocessing existing data to produce new instances, and \textit{(iv)} sampling new data based on the distribution of the existing dataset.

\smallskip
\noindent \textit{\textbf{Regularization.}} An overfitting model tends to incorporate all available features into its decision-making process, even those with minimal impact or that are simply noise. To address this issue, there are two main approaches: \textit{(i)} \textit{Feature selection}, where only the most relevant features are retained, discarding those deemed irrelevant; \textit{(ii)} \textit{Feature regularization}, which involves minimizing the influence of less important features by reducing their weights in the model.
However, since identifying useless features can be challenging, regularization techniques work by applying a penalty term, or ``regularizer'', to the objective function. This penalizes complex models, encouraging simpler and more generalizable solutions. For example, L1 (``Lasso'') and L2 (``Ridge'') regularization, which add the $L_1$-norm and $L_2$-norm of the learned parameter as a penalties, respectively, are commonly adopted in linear regression. Furthermore, \textit{Dropout} is a popular technique to contrast overfitting in neural networks. This approach randomly drops units and relevant connections from the neural network during training to prevent co-adaptation~\citep{warde-farley2014iclr}.

Unlike existing regularization techniques, which either aim to reduce the magnitude of the learned model's weights (such as Lasso and Ridge) or randomly deactivate certain parameters (such as Dropout), our penalty term is inherently \textit{data-driven}. In other words, the counterfactual regularization we introduce (CF-Reg) is computed directly from the training observations, making it highly tailored to the specific dataset under consideration.

Following a similar strategy, \citeauthor{lin2023neurips} (\citeyear{lin2023neurips}) recently proposed a novel regularizer, called \textit{Abnormal Adversarial Examples Regularization} (AAER), which eliminates catastrophic overfitting by suppressing the generation of abnormal adversarial examples. 

Although adversarial and counterfactual examples are closely related, AAER addresses a different challenge. Specifically, AAER is designed to mitigate catastrophic overfitting, whereas CF-Reg aims to enhance overall model generalizability. 
% Second, our method does not need to distinguish between abnormal and normal counterfactual examples. Instead, it leverages the ability to find \textit{any} counterfactual as a signal of potential overfitting. Therefore, while AAER depends on both the adversarial example generation and the abnormal adversarial example detection method, CF-Reg depends only on the counterfactual generator method.

%Nonetheless, in this work, we compare the performance of the counterfactual regularization penalty proposed against (an adaptation of) AAER.
% \gabri{Potential related work: \url{https://papers.nips.cc/paper\_files/paper/2023/file/d65befe6b80ecf7f180b4def503d7776-Paper-Conference.pdf}}

% \msofia{Maybe interesting : \url{https://arxiv.org/pdf/2306.08805}
% \url{https://arxiv.org/pdf/2301.11113}}




\subsection{Counterfactual Examples}
\label{subsec:cf-examples}
Counterfactual examples have gained significant attention in machine learning due to their utility in various applications, including model \textit{explainability}, \textit{fairness}, and \textit{robustness}.

%Interpretability and Explainability
\smallskip
\noindent \textit{\textbf{Explainability.}} Counterfactual explanations offer valuable insights into model predictions by providing alternative scenarios under which the prediction would change~\citep{wachter2017hjlt}. 
Several studies have explored the use of counterfactual examples to enhance the interpretability of complex machine learning models, such as ensembles of decision trees~\citep{tolomei2017kdd,tolomei2021tkde,lucic2022aaai} and deep neural networks (DNNs)~\citep{le2020kdd}, including graph neural networks (GNNs)~\citep{lucic2022aistats}. Counterfactual instances may elucidate the underlying decision-making process of black-box models, enhancing trust and transparency~\citep{guidotti2018arxiv,karimi2020pmlr,chen2022cikm}.


%Fairness and Bias Mitigation
\smallskip
\noindent \textit{\textbf{Fairness.}} Counterfactual reasoning has been employed to address issues of fairness and bias in machine learning systems~\citep{kusner2017neurips}. By generating counterfactual examples, researchers can identify instances of discrimination or bias in model predictions and mitigate these effects~\citep{kamiran2012kis}. Various techniques have been proposed to generate counterfactual instances that satisfy fairness constraints, such as demographic parity and equalized odds~\citep{wachter2017hjlt}.

%Robustness and Adversarial Defense
\smallskip
\noindent \textit{\textbf{Robustness.}} Counterfactual examples have also been leveraged to enhance the robustness of machine learning models against adversarial attacks~\citep{brown2018arxiv}. Indeed, there is a strict relationship between adversarial and counterfactual examples~\citep{freiesleben2022mm}, although their primary goals are divergent. While both adversarial and counterfactual examples involve perturbing input data to influence model predictions, adversarial examples are crafted to jeopardize the model, whereas counterfactual examples are generated to understand the model's behavior.
By generating instances that are semantically similar to the original input but induce different model predictions, \citet{he2019acsac} aim to improve model robustness against adversarial perturbations. These counterfactual examples serve as natural adversaries, enabling the model to learn more robust decision boundaries against malicious attacks.

% %Causal Inference and Treatment Effects
% In the realm of causal inference, counterfactual examples play a crucial role in estimating treatment effects and causal relationships from observational data [@rubin1974estimating]. By comparing the outcomes of observed instances with counterfactual outcomes under different treatment assignments, researchers can estimate causal effects and infer causal relationships between variables [@pearl2009causality]. Counterfactual reasoning provides a principled framework for understanding causal mechanisms and making informed decisions based on observational data [@hernan2018causal].

%Generative Models and Data Augmentation
Recently, generative models have been employed to generate realistic counterfactual instances for data augmentation and model improvement~\citep{ganin2016jmlr,hjelm2019iclr}. 
% By learning the underlying data distribution, generative models can produce diverse and plausible counterfactual examples, which can be used to augment the training data~\citep{hjelm2019iclr}. 
These approaches offer promising avenues for leveraging counterfactual reasoning in generative modeling tasks~\citep{lopez2020msb}.
However, to the best of our knowledge, this study is the first to directly link counterfactual examples to model generalizability and leverage them as the cornerstone of a novel regularization method.
