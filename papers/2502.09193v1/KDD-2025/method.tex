\section{Counterfactual Regularization}
\label{sec:method}
%\gabri{The following assumes that our claim above is verified! :)}
\subsection{A New Training Loss}
\label{subsec:new-loss}
We exploit the correlation between overfitting and the ability to find counterfactual examples, as highlighted in the previous section, to define a new regularized training loss as follows.
\begin{equation}\label{eq:regularized_loss}
\params^* = \argmin_{\params}\Big\{\Loss_{\text{emp}}(\params;\dataset) - \alpha \Loss_{\text{cf}}(\params;\dataset,\varepsilon)\Big\}.
\end{equation}
% \begin{equation}\label{eq:regularized_loss}
% \params^* = \argmin_{\params}\Big\{\Loss_{\text{emp}}(\params;\dataset) - \alpha \Loss_{\text{cf}}(\params;\dataset)\Big\}.
% \end{equation}
The first term ($\Loss_{\text{emp}}$) is the standard empirical risk, while the second term ($\Loss_{\text{cf}}$) is our proposed \textit{counterfactual regularization} component, with $\alpha \in \R_{\geq 0}$ serving as a hyperparameter to weigh its contribution. 
More specifically, the counterfactual regularization component can be defined as follows:
\begin{equation}\label{eq:cf_loss}
\Loss_{\text{cf}}(\params;\dataset,\varepsilon) = \varphi(\{d(\inst_i, \cfmodel(\inst_i)),\, \inst_i \in \mathcal{D} \};\varepsilon),
\end{equation}
% \begin{equation}\label{eq:cf_loss}
% \Loss_{\text{cf}}(\params;\dataset) = \varphi(\{d(\inst_i, \cfmodel(\inst_i)),\, \inst_i \in \mathcal{D} \}),
% \end{equation}
where $\varphi (\cdot;\varepsilon)$ -- parameterized by $\varepsilon$ -- acts as an aggregation function applied to the set of distances $d(\inst_i, \cfmodel(\inst_i))$ between each sample $\inst_i$ and its corresponding counterfactual generated by the model, $\cfmodel(\inst_i)$.
Intuitively, the closer an instance is to its counterfactual, the greater the penalty imposed by the newly introduced loss. In other words, optimizing the objective defined in (\ref{eq:regularized_loss}) aims to ensure a sufficient margin between each training point and its corresponding counterfactual. 
Moreover, we would like this penalty to be stronger for training points where finding a counterfactual is easier -- i.e., those closer to the decision boundary.
To achieve this goal, each distance $d(\inst_i, \cfmodel(\inst_i))$ can be assigned a weight $w_i^{\varepsilon}$ that depends on $\varepsilon$ during aggregation. For example, this weight may be set as the $\varepsilon$-valid counterfactual probability associated with each training instance $\inst_i$ (i.e., $w_i^{\varepsilon} = p_i^{\varepsilon}$). 

Note that, for each data point $\inst_i$, its corresponding $p_i^{\varepsilon}$ can be estimated as described in Section~\ref{subsec:epsilon-VCF}. However, since $p_i^{\varepsilon}$ depends on the shape of the decision boundary, which may evolve dynamically across training epochs, its estimation should ideally be updated at \textit{every} epoch. However, this could introduce a significant computational overhead in the training process, which we can mitigate by instead updating our estimates $\{p_i^{\varepsilon}\}_{i=1}^m$ periodically, for example, every $t$ epochs.
We conjecture that a trade-off exists between the effectiveness of the counterfactual regularization term 
 -- impacted by the accuracy of each estimate $p_i^{\varepsilon}$ -- and the computational cost of the training process.
%\fabiano{Non so se ha senso mantenere il blocco precedente ``Note that, for each data point $\inst_i$, its corresponding $p_i^{\varepsilon}$ can be estimated as described in Section~\ref{subsec:epsilon-VCF}. However, since $p_i^{\varepsilon}$ depends on the shape of the decision boundary, which may evolve dynamically across training epochs, its estimation should ideally be updated at \textit{every} epoch. However, this could introduce a significant computational overhead in the training process, which we can mitigate by instead updating our estimates $\{p_i^{\varepsilon}\}_{i=1}^m$ periodically, for example, every $t$ epochs.
%We conjecture that a trade-off exists between the effectiveness of the counterfactual regularization term 
% -- impacted by the accuracy of each estimate $p_i^{\varepsilon}$ -- and the computational cost of the training process."}

%\gabri{Ho reso le equazioni 4-5 ancora pi√π generali, includendo un altro parametro $\varepsilon$ in $\Loss_{\text{cf}}(\params;\dataset, \varepsilon)$. In questo modo, posso definire l'aggregazione $\varphi$ ``pesata'' sulla base della $\varepsilon$-VCP ($p_i^{\varepsilon}$)}

It is worth noting that our counterfactual regularizer -- hereinafter referred to as CF-Reg -- is flexible enough to support any choice of aggregation function ($\varphi$), distance ($d$), and counterfactual generator ($\cfmodel$). However, the optimization problem in (\ref{eq:regularized_loss}) can be efficiently solved using standard gradient-based methods, provided that $\varphi$, $d$, and $\cfmodel$ are all differentiable with respect to $\params$.

In this work, we set $\varphi$ as the mean, $d$ as the Euclidean distance (i.e., the $L_2$-norm of the displacement vector resulting from the difference between the original sample and its corresponding counterfactual). 
For $\cfmodel$, we adopt the \textit{score counterfactual explanation} method proposed by~\citet{wachter2017hjlt}, and we discuss the rationale behind this choice below.
Alternative formulations of these components are possible and will be explored in future studies.

\subsection{Counterfactual Example Generator}
A key component of CF-Reg is the counterfactual generator $\cfmodel$, as this is used to compute the optimal counterfactual example for each training data point.
To incorporate this step into the regularized training process, the counterfactual generator must satisfy two essential requirements. First, it must be differentiable with respect to the predictive model weights $\params$. Second, it must be highly efficient to ensure the overall feasibility of our method. Among the various counterfactual generation approaches in the literature, we have selected the score counterfactual explanation method proposed by \cite{wachter2017hjlt}, which is defined as follows.
\begin{definition}[Score Counterfactual Explanation] Let $\inst \in \X$ be an input sample, $\model$ a trained model, $\delta:\X\times \X \mapsto \R_{\geq 0}$ a distance function, $\beta \in \R_{\geq 0}$ a controlling hyperparameter, and $s \in \R$ a target score. Thus, the score counterfactual explanation $\cfinst$ for $\inst$ is the result of the following optimization problem: 
\begin{equation}
\label{eq:sce}
        \cfinst^* = \underset{\cfinst \in \X}{\text{arg min}} \, (\model(\cfinst) - s)^2 + \beta \delta(\inst, \cfinst).
    \end{equation}
\end{definition}

With a slight abuse of notation, in (\ref{eq:sce}), we treat the output of the predictive model $\model$ as a continuous value, even for classification tasks. 
To accommodate this requirement, we can, for instance, assume access to the logits, which are then passed through the appropriate activation function, such as sigmoid or softmax.

Note that the resulting counterfactual generator satisfies the two requirements mentioned above. In particular, by carefully selecting the distance function $\delta$ in (\ref{eq:sce}), the resulting objective becomes differentiable with respect to the predictive model weights 
$\params$. Furthermore, when using the score-based counterfactual explanation approach, determining the optimal counterfactual perturbation vector $\perturb = \inst - \cfinst^* = \inst - \cfmodel(\inst)$ -- i.e., the displacement vector between the original instance and its optimal counterfactual -- admits a closed-form solution under specific assumptions. Specifically, if $\model$ is a linear function and $\cfinst^* = \cfmodel(\inst)$ is the optimal counterfactual example for the input $\inst$, \citet{pawelczyk2022ijcai} proved that:
\begin{equation}
\label{eq:opt-delta}
    \perturb = \frac{t}{\beta + \|\params\|^2} \cdot \params,
\end{equation}
where $t = s - \model(\inst)$.
This makes the method highly efficient and well-suited for incorporation into our regularized training loss.
% \gabri{ci serve $m$? non potremmo direttamente scrivere $s-\model(\inst)$ al numeratore? Tra l'altro $m$ potrebbe confondere un po' visto che lo usiamo anche per il numero di training point quando calcoliamo la media poco sotto... Quindi, in ogni caso, userei un'altra lettera!}
%\fabiano{dare intuitivamente la motivazione del perch\'e $\delta$ tende a 0 se troviamo un risultato sulla training dynamics}

From (\ref{eq:opt-delta}), we can, therefore, compute the $L_2$-norm of the perturbation vector $\perturb_i$, for each training instance $\inst_i$. This value is then used as $d(\inst_i, \cfinst^*_i)$, as required by the proposed regularized training loss in (\ref{eq:cf_loss}).
Finally, the $L_2$-norms of all perturbation vectors are aggregated using the function $\varphi(\cdot;\varepsilon)$, resulting in the following (weighted) average: 
\begin{equation}
\label{eq:agg-delta}
\overline{||\perturb||} = \frac{1}{m}\sum_{i=1}^m w_i^{\varepsilon}*||\perturb_i||.
\end{equation}
In the simplest case, the weights are uniformly set as $w_i^{\varepsilon} = 1~\forall i\in \{1,\ldots,m\}$, effectively reducing the aggregation to a standard average. Alternatively, more sophisticated strategies can be employed, such as the approach outlined in Section~\ref{subsec:new-loss}, where each training point is assigned a weight corresponding to its estimated $\varepsilon$-VCP, i.e., $w_i^{\varepsilon} = p_i^{\varepsilon}$.

Overall, this transforms our counterfactual regularized loss as follows:
\begin{equation}\label{eq:regularized_loss2}
\params^* = \argmin_{\params}\Big\{\Loss_{\text{emp}}(\params;\dataset) - \alpha \overline{||{\perturb}||} \Big\}.
\end{equation}

Finally, the objective in (\ref{eq:regularized_loss2}) can be solved using standard gradient-based optimization methods.

% \fabiano{Usiamo il simbolo $||\cdot||$ o $||\cdot||_2$ per indicare la norma 2?}
% \gabri{Entrambi vanno bene, a patto di essere consistenti. Come gusto personale, trovo meno "verbosa" e pi√π leggera la notazione senza pedice.} 