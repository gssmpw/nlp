\section{Impact of Counterfactual Examples on Model Generalizability}
\label{sec:problem}

\subsection{Intuition}
We consider a training set $\dataset = \{(\inst_i, y_i)\}_{i=1}^m$ of $m$ i.i.d. labeled instances, as introduced above. Then, suppose this dataset is used to train a sequence of $k$ different models $(f_{\params_0}, f_{\params_1}, \ldots, f_{\params_{k-1}})$. Each model $f_{\params_i}$ is associated with a training accuracy $\alpha_i$ (calculated on $\dataset$), such that $\alpha_0 < \alpha_1 < \ldots < \alpha_{k-1}$, where $f_{\params_0}$ indicates the random baseline model and $f_{\params_{k-1}}$ is the dummy model that simply memorizes the entire training set and, therefore, achieves the perfect training accuracy ($\alpha_{k-1} = 1$).

Informally, we claim that for a fixed positive threshold $\varepsilon > 0$, the expected number of training points for which we can find an $\varepsilon$-valid counterfactual example is positively correlated with the training accuracy of the model. %\footnote{Or, analogously, negatively correlated with the training loss.}
In other words, given two trained models $f_{\params_i}$ and $f_{\params_j}$, where $i \neq j$, whose training accuracies are $\alpha_i > \alpha_j$, we expect to find, on average, more $\varepsilon$-valid counterfactual examples for $f_{\params_i}$.

This intuition stems from the idea that a model with a higher training accuracy tends to have a more intricate decision boundary surface, which captures all the nuances of the training data more precisely. Consequently, data points are, on average, closer to such a convoluted decision boundary, making it easier for them to find valid counterfactual examples within a distance of $\varepsilon$.

In essence, a model for which finding counterfactual examples is ``too easy'' may indicate a risk of overfitting. Thus, a trade-off must exist between the model's generalizability and its counterfactual explainability, which we aim to investigate further in this study.

% For a given model $f_{\params_i}$ and for each training point $\inst \in \dataset$, we extract the set of its candidate counterfactuals, namely the subset of data points $\cfdataset^{\inst} \subseteq \dataset$ whose predicted label is different from the original training point, such that $\cfdataset^{\inst} = \{\inst' \in \dataset ~|~f_{\params_i}(\inst') \neq f_{\params_i}(\inst)\}$. 
% Then, we measure the distance between each training point $\inst$ and the optimal (closest) counterfactual among selected candidates $\cfdataset^{\inst}$. Finally, we compute the average of such distance across all the training points. 
% Our claim is that the average distance between an input data point and its nearest counterfactual decreases as overfitting increases.
% In essence, a model for which finding counterfactual examples is "too easy" may indicate a risk of overfitting. Thus, there exists a trade-off between the model's generalizability and its (counterfactual) explainability.

% In Fig.~\ref{fig:avg-dist-cf}, we plot the average distance from the optimal counterfactual example ($y$-axis) versus the training accuracy ($x$-axis).

% A different, yet similar, perspective.

% The density of data points "around"/"proximal" to the decision boundary is higher for overfitted models. 
% Let us assume the input space is $\X \subseteq \{0,1\}^n$. We define the $\varepsilon$-neighborhood of a datapoint $\inst$ as $\mathcal{N}(\inst;\varepsilon) = \{\inst' \in \dataset~|~d(\inst,\inst') < \varepsilon\}$. Moreover, let $\mathcal{N}_{\text{CF}}(\inst;\varepsilon) = \{\inst'\in \mathcal{N}(\inst;\varepsilon)~|~\model(\inst') \neq \model(\inst)\}$.
% Then, we can estimate the following:
% \[
% \Prob[d(\inst, \cfinst) < \varepsilon] \approx \frac{|\mathcal{N}_{\text{CF}}(\inst;\varepsilon)|}{|\mathcal{N}(\inst;\varepsilon)|}
% \]

% \gabri{Can we establish a relationship between the left-hand side of the equation above and the degree of overfitting (e.g., in terms of training accuracy?) It would be nice if we could somewhat relate $|\mathcal{N}_{\text{CF}}(\inst;\varepsilon)|$ to the training accuracy of $\model$.}

\subsection{$\varepsilon$-Valid Counterfactual Probability Estimation}
\label{subsec:epsilon-VCF}
To verify our claim, we need to characterize better what we mean by the ease of finding an $\varepsilon$-valid counterfactual example for a given data point and, therefore, for a full training set of data points.

Let us consider the generic predictive model $\model$ trained on $\dataset = \{(\inst_i, y_i)\}_{i=1}^m$.
Suppose we associate to each training data point $\inst_i$ a binary random variable $X_i\in \{0,1\}$, which indicates whether there exists an $\varepsilon$-valid counterfactual example $\cfinst_i$ for $\inst_i$. 
Therefore, each $X_i$ follows a Bernoulli distribution, i.e., $X_i \sim \text{Bernoulli}(p_i^{\varepsilon})$, whose probability mass function is defined as below.
\begin{equation}
\Prob(X_i = k) = p_{X_i}(k;p_i^{\varepsilon}) = \begin{cases} p_i^{\varepsilon} &\text{, if }k=1
\\ 
1-p_i^{\varepsilon} &\text{, if } k=0.
\end{cases}
\end{equation}
Furthermore, let $\bar{X}$ define the average number of training points for which we can find an $\varepsilon$-valid counterfactual example. More formally, $\bar{X}=\frac{1}{m}\sum_{i=1}^m X_i$ is the average of $m$ Bernoulli random variables. Thus, we can calculate $\E[\bar{X}] = \E\Big[ \frac{1}{m}\sum_{i=1}^m X_i\Big] = \frac{1}{m}\sum_{i=1}^m \E[X_i]$ due to the linearity of expectation, where $\E[X_i] = p_i^{\varepsilon}$. 
Note that the random variable $Z=\sum_{i=1}^m X_i$ follows a Poisson-Binomial distribution since $X_i$'s are independent but not necessarily identically distributed.
Overall, $\bar{X} = \frac{Z}{m}$, which scales the Poisson-Binomial random variable $Z$ by $1/m$, namely $\bar{X} \sim \text{Poisson-Binomial}(\lambda)$, where $\lambda= \frac{1}{m} \sum_{i=1}^m p_i^{\varepsilon}$.

For a fixed $\varepsilon > 0$, we claim $\mathbb{E}[\bar{X}]$ to be positively correlated with the training accuracy of the model. Simply put, as the training accuracy of $\model$ increases, we foresee a corresponding increase in the expected average number of training data points for which a valid counterfactual example can be found within a distance of $\varepsilon$.

Ultimately, the task aims to estimate each $p_i^{\varepsilon}$, which we call the (sample-level) $\varepsilon$-valid counterfactual probability.

\begin{definition}[$\varepsilon$-Valid Counterfactual Probability -- $\varepsilon$-VCP]
Let $\model$ be a trained model, $\inst \in \X\subseteq \R^n$ a generic training point, and $\varepsilon \in \R_{>0}$ a positive real number. Consider the $\varepsilon$ $n$-ball as the $n$-dimensional hypersphere of radius $\varepsilon$ centered around $\inst$. Let $V_{\inst}^{\varepsilon}\subseteq \R^n$ be the total hypervolume of such hypersphere and let $V_{\cfinst}^{\varepsilon} \subseteq V_{\inst}^{\varepsilon}$ denote the portion of the total hypervolume falling within the counterfactual region delimited by the decision boundary induced by $\model$. Therefore, we can estimate the $\varepsilon$-\textit{valid counterfactual probability} ($\varepsilon$-VCP) for $\inst$, as $p^{\varepsilon} = V_{\cfinst}^{\varepsilon}/V_{\inst}^{\varepsilon}$. 
\end{definition}
It is worth noticing that, in practice, $\inst$ can be seen as an embedding from an autoencoding process, mapping each raw training point into a dense, lower-dimensional latent manifold within the original, high-dimensional space. 

Thus, our focus is to evaluate how easily a counterfactual generation method can identify a counterfactual example within $\varepsilon$, given an input sample $\inst$.
Intuitively, given a fixed $\inst$ and two different models having different decision boundaries, it will generally be much easier to find an $\varepsilon$-valid counterfactual if the hypervolume $V_{\cfinst}^{\varepsilon}$ is larger. %, even though data might not be uniformly distributed within such space. 
This intuition is depicted in Figure~\ref{fig:cf-probability}, which illustrates how the probability of finding an $\varepsilon$-valid counterfactual for a $2$-dimensional data point may increase with model overfitting.
 \begin{figure}
 \centering
 \begin{subfigure}[htb!]{.4\linewidth}
 \includegraphics[width=.9\linewidth]{img/cf-probability-low}
 \caption{Low $\varepsilon$-valid counterfactual probability}
 \end{subfigure}
 \qquad
 \begin{subfigure}[h]{.4\linewidth}
 \includegraphics[width=.9\linewidth]{img/cf-probability-high}
 \caption{High $\varepsilon$-valid counterfactual probability}
 \end{subfigure}%
 \caption{The $\varepsilon$-\textit{valid counterfactual probability} for a sample $\inst \in \R^2$ can be estimated as the ratio of the area of the circle centered in $\inst$ with radius $\varepsilon$ that falls behind the decision boundary (in red).}
 \label{fig:cf-probability}
 \end{figure}

%\begin{figure}
%\centering
%\begin{subfigure}[htb!]{.7\linewidth}
%\centering
%\includegraphics[width=.7\linewidth]%{img/cf-probability-low}
%\caption{Low $\varepsilon$-valid counterfactual probability}
%\end{subfigure}
%\\
%\begin{subfigure}[h]{.7\linewidth}
%\centering
%\includegraphics[width=.7\linewidth]{img/cf-probability-high}
%\caption{High $\varepsilon$-valid counterfactual probability}
%\end{subfigure}%
%\caption{The $\varepsilon$-\textit{valid counterfactual probability} ($\varepsilon$-VCP) for a sample $\inst \in \R^2$ can be estimated as the ratio of the area of the circle centered in $\inst$ with radius $\varepsilon$ that falls behind the decision boundary (in red). We expect $\varepsilon$-VCP to be lower for a model that generalizes well (a) and higher for a model that suffers from overfitting (b).}
%\label{fig:cf-probability}
%\end{figure}

To estimate $V_{\cfinst}^{\varepsilon}$, we can apply standard Monte Carlo integration, a well-established technique that uses random draws to numerically compute a definite integral.
Furthermore, using Monte Carlo integration will provide valuable knowledge on the shape of a model's decision boundary as a by-product. 
The estimate outlined above assumes that data points are uniformly distributed around $\inst$ in the latent space, which may be reasonable for appropriate values of $\varepsilon$.
More accurate estimates could be obtained by sampling the $\varepsilon$-neighborhood from the manifold using advanced strategies (e.g., see \citet{chen2022cikm}). However, this lies beyond the primary scope of this step and will be explored in future work.

Finally, if we extend the same reasoning above to all training points, we should observe that the expected average number of training points for which we find $\varepsilon$-valid counterfactual examples ($\E[\bar{X}]$) should be higher for overtrained models.

\subsection{Empirical Assessment}
To investigate the impact of overfitting on $\varepsilon$-VCP, we trained two Multi-Layer Perceptron (MLP) models on the \textit{Water Potability} dataset \cite{kadiwal2020waterpotability} for a binary classification task. Both models used the same 5-layer architecture, but only one applied dropout regularization with a dropout rate of $0.5$. The models were trained over 500 epochs using the Adam optimizer with a learning rate of $\eta = 0.001$. Finally, the $\varepsilon$-VCP was estimated through Monte Carlo integration as discussed in Section~\ref{subsec:epsilon-VCF}. Specifically, we calculated it as $p^{\varepsilon} = V_{\cfinst}^{\varepsilon}/V_{\inst}^{\varepsilon}$, where $\varepsilon = 1.5$ and $V_{\cfinst}^{\varepsilon}$ was estimated using 100 random samples for each training point $\inst$.

It is important to note that the choice of $\varepsilon$ depends on the underlying dataset. A good practice is to empirically determine this value to ensure a meaningful assessment of counterfactual validity while maintaining consistency with the dataset structure. Specifically, since $\varepsilon$ represents the maximum norm of the perturbation vector applied to each sample, it serves as a threshold that defines the allowable search space for counterfactuals.  
A value that is too small would overly restrict counterfactual perturbations, limiting the ability to assess meaningful changes in predictions. Conversely, a value that is too large could result in unrealistic perturbations that deviate from the underlying data distribution.  
In this experiment with the \textit{Water Potability} dataset, we found that setting $\varepsilon=1.5$ provides a suitable trade-off, allowing us to capture a diverse yet interpretable range of counterfactual examples without introducing excessive modifications to the original training points.


Figure~\ref{fig:evcp} illustrates the evolution of the average $\varepsilon$-VCP, calculated over all training points, as a function of the models' training accuracy.
\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{img/p_x_comparison_acc.png}
    \caption{The average $\varepsilon$-VCP ($y$-axis) as a function of the model's training accuracy ($x$-axis). \textit{Plain}$_{\varepsilon\text{-VCP}}$ is the ``vanilla'' MLP, while \textit{Regularized}$_{\varepsilon\text{-VCP}}$ is the same MLP yet with a dropout rate of $0.5$.}
    \label{fig:evcp}
\end{figure}
From this plot, three key insights emerge.
First, the average $\varepsilon$-VCP increases alongside training accuracy for both models, confirming our hypothesis that the likelihood of finding valid counterfactuals rises as models tend to overfit.
%Second, the plain, unregularized MLP shows higher $\varepsilon$-VCP values, indicating that the more intricate decision boundary the MLP learns facilitates the identification of valid counterfactual examples.
Second, the plain, unregularized MLP exhibits higher $\varepsilon$-VCP values, suggesting that its more complex decision boundary makes it easier to identify valid counterfactual examples.
Third, the trend persists while the MLP trained with dropout regularization exhibits a less pronounced increase (i.e., smaller $\varepsilon$-VCP values). This suggests that although dropout regularization smooths the decision boundary to some extent, it may not sufficiently counteract this phenomenon, leaving room for further improvement.



% These findings align with the theoretical intuition that overtrained models tend to develop more complex and convoluted decision boundaries, making counterfactual generation easier but less reliable. Conversely, regularization smooths the decision boundary, reducing $\varepsilon$-VCP while enhancing stability.


% \gabri{We should show the claim above with an experiment, where we take a dataset (e.g., MNIST), we train $k$ distinct classifiers (e.g., $k=10$), we fix a value for $\varepsilon$ (e.g., $\varepsilon \in \{0.05, 0.10, 0.25, 0.50\}$), we randomly sample $m$ training points (e.g., $m=1,000$) and we run Monte Carlo integration to estimate $p^{\varepsilon}_i~\forall i \in \{1,\ldots, 1,000\}$, and therefore, $\mathbb{E}[Z]$. Should we also show some statistical significance tests? The null hypothesis $H_0$ for us would be something like: ``The expected value ($\E[Z]$) of training points for which we can find an $\varepsilon$-valid counterfactual example does \textit{not} depend on the training accuracy ($\alpha$) of the model ($\model$).'' Or should we run some regression tests to see if we can predict $\E[Z]$ from $\alpha$?}

%define the counterfactual probability for the specific data point The hypervolume of such hypersphere may be Monte Carlo integration.



% \begin{itemize}
% \item Consider the training data points only $\frac{|\mathcal{N}_{\text{CF}}(\inst_i;\varepsilon)|}{|\mathcal{N}(\inst_i;\varepsilon)|}$;
% \item Use some sampling strategy and refine the estimate above using the same approach;
% \item Relax the assumption on $p_i^{\varepsilon} = p_j^{\varepsilon}$ iff $i=j$, and simply let $p_i^{\varepsilon} = p^{\varepsilon} \forall i\in \{1, \ldots, m\}$. This can be in turn estimated with the validity of a given counterfactual generator $\cfmodel$.
% \end{itemize}

% \subsection{Random perturbation counterfactual density}
% In order to define the model overfitting in terms of counterfactual explainabilty we can use a perturbation-based counterfactual methods. The intuition behind it is the following: Given a set of samples $D=\{d_1, d_2, \dots, d_n\}$ with $d_i \in \mathcal{R}^k$ and an oracle $\Phi$ trained on $D$ we can draw a sample $d_i$ from $D$. We define a $k$-dimensional sphere with radius $r$ centered in sample $d_i$. Then we perturb sample $d_i$ with a perturbation vector $p_i$ such that its norm is smaller or equal than $r$. In such a way we should ensure that the new perturbed sample is inside the hypersphere. We sample uniformly at random (We could use Poisson Disk Sampling or uniform random sampling) $m$ perturbation vectors $p_i$. We can now use the oracle $Phi$ to classify all the perturbed points $d_i + p_i$. Our hypothesis is the following: the more the oracle $\Phi$ is trained on $D$ the higher the number of perturbed samples $d_i + p_i$ will be classified with a class that is the opposite of the classification for $d_i$. We can define a term $\eta$ that is the ratio between the number of samples classified as counterfactuals and the number of total samples. This term $\eta$ can be defined as counterfactual probability. This probability increases with the training level of $\Phi$ and can be used as a proxy estimator for model overfitting. In fact the higher the overfitting the closer the decision boundary to the samples $d_i$. We can then use this factor to regularize the model training, introducing a penalty positive corralated with the counterfactual probability $\eta$.
% \begin{algorithm}
%     \caption{\texttt{\textsc{AvgDistance2ClosestCF}}}\label{alg:intuition}
%     \begin{algorithmic}[1]
%         \Require A training set $\dataset = \{(\inst_i, y_i)\}_{i=1}^m$ of $m$ labeled instances; a model $\model$ trained on $\dataset$.
%         \Ensure The average distance between each data point ($\inst$) and its closest counterfactual ($\cfinst)$.
%  \Procedure{\texttt{AvgDistance2ClosestCF}}{$\model, \dataset$}
%         \State $tot\_dist \gets 0$
%         \ForEach {$\inst \in \dataset$}
%         \State $\cfdataset^{\inst} \gets \{\inst' \in \dataset ~|~\model(\inst') \neq \model(\inst)\}$
%         \Comment{The set of candidate CF examples for $\inst$}
%         \State $min\_dist \gets \infty$
%         \ForEach {$\inst' \in \cfdataset^{\inst}$}
%         \State $dist \gets \texttt{\textsc{ComputeDistance}}(\inst, \inst')$
%         \If {$dist < min\_dist$}
%         \State $min\_dist \gets dist$
%         \EndIf
%         \EndFor
%         \State $tot\_dist \gets tot\_dist + min\_dist$
%         %\State $\dataset \setminus \{\inst\}$
%         \EndFor
%         \State \textbf{return} $tot\_dist/m$
%         %$2*tot\_dist/(m*(m-1))$
%         %$2*tot\_dist/(tot\_samples*(tot\_samples-1))$
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}
