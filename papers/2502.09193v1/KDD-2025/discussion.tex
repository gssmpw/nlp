\section{Current Limitations}
\label{sec:discussion}
In this section, we discuss the current limitations of CF-Reg and identify potential areas for improvement. It is important to note that, in this initial iteration, our primary goal was to minimize the impact of our method on the overall training process. As a result, we prioritized efficiency, which may have come at the cost of potentially higher accuracy.

First, the framework proposed in Section~\ref{subsec:new-loss} is designed to accommodate \textit{any} counterfactual generator $\cfmodel$. However, this flexibility may come at the cost of increased computational overhead, as each training point requires querying $\cfmodel$ to generate the corresponding counterfactual example. The feasibility of the training process thus depends on the efficiency of $\cfmodel$. For instance, when using a relatively simple generator, such as the score-based counterfactual model adopted in this work, the impact on overall training time is minimal, as shown in the previous section, though this may come at the expense of regularization performance. Conversely, employing a more sophisticated $\cfmodel$ could improve generalization but at the cost of significantly increased training time. This highlights an inherent trade-off between the complexity of the counterfactual generation process and the effectiveness of the regularization it provides. The exploration of more complex counterfactual generators is, therefore, a promising direction for future research.

Second, even when using the score-based counterfactual generator, we might not fully exploit the closed-form solution for calculating the optimal perturbation vector as shown in (\ref{eq:opt-delta}). This analytical solution is valid under the assumption that the predictive model $\model$ is a linear function of the input parameters. However, when $\model$ is a deep neural network, this assumption is typically violated, and we approximate it using the first-order Taylor expansion, as in (\ref{eq:taylor}). This approximation, while useful, could lead to suboptimal counterfactuals, as it may not fully capture the non-linearity of the model's behavior.

Third, the results presented are based on a straightforward weighting scheme for CF-Reg, where the contribution $w_i^{\varepsilon}$ of each training point $\inst_i$ is uniformly set as $w_i^{\varepsilon} = 1~\forall i\in \{1,\ldots,m\}$. A more fine-grained strategy could potentially enhance the quality of the regularization, although it would likely increase the training time.