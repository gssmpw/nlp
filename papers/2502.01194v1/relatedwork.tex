\section{Related work}
\textbf{Veracity prediction for OOC images}\\
Providing a veracity label for OOC images has received significant attention in automated fact-checking (AFC) research \citep{akhtar-etal-2023-multimodal}. It is a binary classification task with labels \{accurate, OOC\}. Synthetic datasets have been created by replacing entities in the true caption \citep{10.1145/3240508.3240707,10.1145/3372278.3390670}   or by mismatching image-caption pairs from news corpora  \citep{luo-etal-2021-newsclippings}. 
Smaller datasets based on real-world fact-checks have recently been proposed \citep{aneja2023cosmos,papadopoulos2024verite,pham2024ookpik}. Models that leverage external evidence outperform those that use only the image and the caption as input \citep{luo-etal-2021-newsclippings,zhang2023interpretable}. \citet{Abdelnabi_2022_CVPR} collects text evidence with reverse image search and image evidence by querying a search engine with the caption. The performance can further be improved by predicting the stance of the evidence towards the caption \citep{yuan-etal-2023-support} or ranking the most relevant evidence \citep{papadopoulos2023red}. Unlike prior methods, ECENet provides an explanation in the form of a summary of the most relevant text evidence \citep{10.1145/3581783.3612183}. Recent work leverages multimodal LLMs (MLLMs) with instruction-tuning \citep{Qi_2024_CVPR}, external tools \citep{braun2024defamedynamicevidencebasedfactchecking}, or multi-agent debate \citep{lakara2024madsherlockmultiagentdebatesoutofcontext}.  
\citet{papadopoulos2024similarity} showed that simple classifiers like random forest trained on top of the image, caption, and image and text evidence embeddings achieve SOTA performance. Their results show that veracity can often be predicted based on shallow heuristics, highlighting the need to assess the progress in the field from other perspectives \citep{papadopoulos2024similarity}, such as context prediction.\\

\noindent \textbf{Context prediction for OOC images}\\
Context prediction, or image contextualization, has received less attention in AFC. \citet{tonglet-etal-2024-image} formulate it as a question-answering (QA) task where each context item is predicted given the corresponding question and a set of evidence which may include the image, the caption, or pieces of information derived from them. \citet{tahmasebi2025verifyingcrossmodalentityconsistency} formulate it as a true/false classification task where candidate \textit{people}, \textit{locations}, and \textit{events} are verified with a MLLM. 5Pils is a real-world dataset \citep{tonglet-etal-2024-image} which  contains context labels based on human fact-checking practices \citep{urbani2020verifying,bellingcat2021,10017287,khan2024debunking}. \citet{tonglet-etal-2024-image} proposed a baseline for context prediction that retrieves text evidence with reverse image search and answers context questions with a (M)LLM and the image and the evidence as input.\\


In this work, we fill an important gap by introducing the first method that performs both tasks sequentially, leveraging the generated context for veracity prediction.