\section{Related Work}
\paragraph{Unsupervised Object-Centric Learning}
%
Object-centric representation methods aim to parse in an unsupervised manner an image or video into a set of $\NumSlots$ latent vectors called slots, where each of them binds to a different object in the scene~\cite{Greff_OnTheBindingProblemInNeuralNetworks_2020, Locatello_ChallengingAssumptionsInLearningOfDissentangledRepresentations_2019}.
%
Early slot-based methods aimed to learn object representations from synthetic images~\cite{Locatello_ObjectCentricLearningWithSlotAttention_2020, Singh_SLATE_2021, Biza_InvariantSlotAttention_2023} or videos~\cite{Kipf_ConditionalObjectCentricLearningFromVideo_2022, Creswell_UnsupervisedObjectBasedTransitionModelsFor3DPartiallyObservableEnvironments_2021, Singh_STEVE_2022} by minimizing a reconstruction objective.
%
To learn meaningful representations from real data, recent slot-based methods leverage weak supervision~\cite{Elsayed_SAVi++TowardsEndToEndObjectCentricLearningFromRealWorldVideos_2022, Bao_ObjectDiscoverMotion_2023}, large pretrained transformers~\cite{Seitzer_BridgingTheGapToRealWorldObjectCentricLearning_2023, Aydemir_SelfSupervisedObjectCentricLearningForVideos_2023, Zadaianchuk_VideoSaur_2024}, or diffusion models~\cite{Jiang_ObjectCentricSlotDiffusion_2023, Wu_SlotDiffusion_2023}
%
These object-centric representations benefit multiple downstream tasks such as reinforcement learning for robotic manipulation~\cite{Mosbach_SOLDReinforcementLearningSlotObjectCentricLatentDynamics, Ferraro_Focus_2023} or visual-question-answering~\cite{Mamaghan_ObjectCentricRepsForVQA_2024}.




\paragraph{Object-Centric Video Prediction}

Object-centric video prediction aims to model the object dynamics and interactions in a video sequence with the goal of forecasting future object states and video frames.
%
Several methods address this task using different architectural priors, including RNNS~\cite{Zoran_PARTS_2021, Assouel_VariationIdependentModulesVP_2022}, transformers~\cite{Villar_OCVP_2023, Wu_SlotFormer_2022, Daniel_DDLP_2024, Meo_ObjectCentricTemporalConsistency_2024} or state-space models~\cite{Jiang_SlotSSM_2024}, attaining a remarkable prediction accuracy on synthetic datasets.
%
Recently, some methods improve the controllability of object-centric video prediction models by conditioning the prediction process on actions~\cite{Mosbach_SOLDReinforcementLearningSlotObjectCentricLatentDynamics} or language captions~\cite{Wang_TIVDiffusion_2024}.
%
However, forecasting future object states without supervision in complex environments still remains an open challenge.




% main fig
\input{imgs/main_fig.tex}




\paragraph{Learning Latent Actions from Unlabeled Videos:}
%

Videos provide abundant information about dynamics and activities, but often lack the action labels necessary for learning behaviors from video.
%
To address this challenge, some methods train a latent policy directly from observations by learning a discrete latent action space and sampling the actions that minimize a reconstruction error~\cite{Edwards_ImitatingLatentPoliciesFromObservation_2019, Struckmeier_ILPI_2023}.
%
Another group of methods, to which \Method{} belongs, learns inverse dynamics from unlabeled videos by predicting latent actions given pairs of observations, and uses them for learning behaviors for video games and robot simulations~\cite{Ye_BecomeAProficientPlayerByWatchingPureVideos_2022, Brandfonbrener_InverseDynamicsPretrainingLearnsGoodRepresentatiosForImitation_2024, Schmidt_LAPOLearningToActWithoutActions_2024}, as pretraining for Vision-Language-Action models~\cite{Ye_LatentActionPretrainingFromVideos_2024} or for learning robot policies~\cite{Cui_DynaMoDynamcisPretrainingForVisuoMotorControl_2024}.
%

Latent action models have also been used for conditional video prediction.
%
The most similar method to ours is \emph{CADDY}~\cite{Menapace_PlayableVideoGeneration_2021, Menapace_PlayableEnvironments_2022}, which learns latent actions from a collection of unlabeled videos from a single domain and uses the latent actions as conditioning signal for predicting future frames.
%
At inference time, CADDY maps user inputs to the latent space for playable video generation.
%
Building upon this same principle, \emph{Genie}~\cite{Bruce_GenieGenerativeInteractiveEnvironments_2024} proposes a foundation world model for playable video generation on diverse environments.
%
However, both CADDY and Genie operate on holistic scene representations, which are limited for tasks that require relational reasoning, often struggle to model object relationships and interactions, and require human supervision to generalize to scenes with multiple moving agents.









%%%%%%%%%%%%%%%%%%
%% Method %%
%%%%%%%%%%%%%%%%%%