\documentclass{article}
\usepackage{hyperref}



\usepackage[accepted]{icml2025}


\input{macros}



\icmltitlerunning{PlaySlot: Learning Inverse Latent Dynamics for Controllable Object-Centric Video Prediction and Planning}


\begin{document}



\twocolumn[

\icmltitle{{PlaySlot: Learning Inverse Latent Dynamics for Controllable \\ Object-Centric Video Prediction and Planning}}



\icmlsetsymbol{equal}{*}


\begin{icmlauthorlist}
\icmlauthor{Angel Villar-Corrales}{ais}
\icmlauthor{Sven Behnke}{ais}

\end{icmlauthorlist}


\icmlaffiliation{ais}{Autonomous Intelligent Systems, Computer Science Institute VI – Intelligent Systems and Robotics, Center for Robotics and the Lamarr Institute for Machine Learning and Artificial Intelligence}
%
\icmlcorrespondingauthor{Angel Villar-Corrales}{villar@ais.uni-bonn.de}
%
\icmlkeywords{Object-Centric Video Prediction, Object-Centric Learning, Video Prediction, Representation Learning, Transformers}

\vskip 0.3in
]

\printAffiliationsAndNotice{}



\begin{abstract}
%
Predicting future scene representations is a crucial task for enabling robots to understand and interact with the environment.
%
However, most existing methods rely on video sequences and simulations with precise action annotations, limiting their ability to leverage the large amount of available unlabeled video data.
%
To address this challenge, we propose \emph{$\COCVP$}, an object-centric video prediction model that infers object representations and latent actions from unlabeled video sequences. It then uses these representations to forecast future object states and video frames.
%
\COCVP{} allows to generate multiple possible futures conditioned on latent actions, which can be inferred from video dynamics, provided by a user, or generated by a learned action policy, thus enabling versatile and interpretable world modeling.
%
Our results show that \COCVP{} outperforms both stochastic and object-centric baselines for video prediction across different environments.
%
Furthermore, we show that our inferred latent actions can be used to learn robot behaviors sample-efficiently from unlabeled video demonstrations.
%
Videos and code are available at \url{https://play-slot.github.io/PlaySlot/}.
%
\end{abstract}




\begin{figure}[t!]
	\centering
	\includegraphics[width=\linewidth]{imgs/teaser_v3.png}
	\vspace{-0.6cm}
	\caption{
		\Method{} parses an image $\ImageT{1}$ into its object components $\SlotsT{1}$.
		It then predicts multiple future object states and frames with an object-centric video prediction module (\CondPred) conditioned on latent actions $\LatentBig$, which can be inferred from a reference video with our $\InverseDynamics$ module, provided as input, or generated by a learned action policy.
	}
	\label{fig: teaser}
\end{figure}



\vspace{-0.5cm}
\section{Introduction}

% Motivation for world models
Accurate and flexible world models are crucial for autonomous systems to reason about
their surroundings, predict possible future outcomes, and plan their actions effectively.
%
Such models require a structured representation of the world that supports generalization, robustness, and controllability, even in complex and dynamic scenarios.


% Motivating the why for object-centric learning
Humans naturally achieve such understanding by parsing their environment into a \emph{background} and multiple separate \emph{objects}, which can interact with each other and can be recombined to form more complex entities~\cite{Johnson_ObjectPerception_2018, Kahneman_ReviewingOfObjectFiles_1992}.
%
Neural networks equipped with such compositional inductive biases have the ability to learn structured object-centric representations with desirable properties such as robustness~\cite{Bengio_RepresentationLearningReview_2013, Dittadi_GeneralizationAndRobustnessImplicationsInObjectCentricLearning_2022}, generalization to novel compositions~\cite{Greff_OnTheBindingProblemInNeuralNetworks_2020}, transferability to novel tasks~\cite{Zhang_IsAnObject-CentricVideoRepresentationBeneficialForTransfer_2022}, and sample efficiency~\cite{Mosbach_SOLDReinforcementLearningSlotObjectCentricLatentDynamics}, among others.


% Motivation and Limitations of Object-Centric Video Prediction
Building on these foundations, the field of object-centric learning has made great advances in recent years, progressing from learning object representations in simple synthetic images~\cite{Locatello_ObjectCentricLearningWithSlotAttention_2020, Burgess_MonetUnsupervisedSceneDecompositionRepresentation_2019} and videos~\cite{Kipf_ConditionalObjectCentricLearningFromVideo_2022, Elsayed_SAVi++TowardsEndToEndObjectCentricLearningFromRealWorldVideos_2022}, towards more complex real-world scenes~\cite{Seitzer_BridgingTheGapToRealWorldObjectCentricLearning_2023, Zadaianchuk_VideoSaur_2024}.
%
Recently, the field of object-centric video prediction combines these learned object representations with forward-dynamics models and has shown great promise for multiple downstream applications such as modeling object dynamics~\cite{Villar_OCVP_2023, Wu_SlotFormer_2022} or action planning~\cite{Yoon_InvestigationObjectCentricRepresentationsReinforcementLearning_2023, Mosbach_SOLDReinforcementLearningSlotObjectCentricLatentDynamics}.
%
However, such models are currently limited to deterministic environments or rely on videos and simulations with precise action labels to forecast scene dynamics, limiting their ability to leverage unlabeled video data and serve as world models for robotic applications.


% Our proposed method
In this work, we propose \emph{\Method}, a novel method for controllable video prediction using object-centric representations.
%
\Method{} learns in a self-supervised manner from video to infer object representations, called slots, and latent action embeddings, which are computed using our proposed \emph{$\InverseDynamics$} module to capture the scene dynamics.
%
\Method{} then predicts future video frames conditioned on the inferred object slots and latent actions.
%
At inference time, as illustrated on \Figure{fig: teaser}, \Method{} parses the observed environment into a set of object slots, each of them representing a different object in the image.
%
Then, \Method{} forecasts future object states and frames conditioned on past object slots and latent actions, which can be inferred from a video sequence using our proposed $\InverseDynamics$ module, provided by a human, or generated by a learned action policy.


% our experiments
In our experiments, we demonstrate that $\Method${} learns a rich and semantically meaningful action space, enabling accurate video prediction while providing high levels of controllability and interpretability.
%
We show how $\Method${} effectively captures precise robot actions and seamlessly scales to scenes with multiple moving objects or to real-world robotics data, outperforming several controllable video prediction baselines.
%
Moreover, we show that the latent actions inferred by $\Method${} enable sample-efficient learning of robot behaviors from unlabeled demonstrations.
%




In summary, our contributions are as follows:
\begin{itemize}[itemsep=1pt, topsep=0.3pt]
	\item We propose \emph{$\COCVP$} -- an object-centric video prediction model that infers object representations and latent actions from unlabeled videos, and uses them to forecast future object states and video frames.

	\item $\COCVP$ outperforms several video prediction models across diverse robotic environments, while showing superior interpretability and control capabilities.

	\item The object representations and latent actions inferred by $\COCVP$ can be used to learn robot behaviors from unlabeled video demonstrations sample efficiently.
\end{itemize}





%%%%%%%%%%%%%%%%%%
%% Related Work %%
%%%%%%%%%%%%%%%%%%
\section{Related Work}



\paragraph{Unsupervised Object-Centric Learning}
%
Object-centric representation methods aim to parse in an unsupervised manner an image or video into a set of $\NumSlots$ latent vectors called slots, where each of them binds to a different object in the scene~\cite{Greff_OnTheBindingProblemInNeuralNetworks_2020, Locatello_ChallengingAssumptionsInLearningOfDissentangledRepresentations_2019}.
%
Early slot-based methods aimed to learn object representations from synthetic images~\cite{Locatello_ObjectCentricLearningWithSlotAttention_2020, Singh_SLATE_2021, Biza_InvariantSlotAttention_2023} or videos~\cite{Kipf_ConditionalObjectCentricLearningFromVideo_2022, Creswell_UnsupervisedObjectBasedTransitionModelsFor3DPartiallyObservableEnvironments_2021, Singh_STEVE_2022} by minimizing a reconstruction objective.
%
To learn meaningful representations from real data, recent slot-based methods leverage weak supervision~\cite{Elsayed_SAVi++TowardsEndToEndObjectCentricLearningFromRealWorldVideos_2022, Bao_ObjectDiscoverMotion_2023}, large pretrained transformers~\cite{Seitzer_BridgingTheGapToRealWorldObjectCentricLearning_2023, Aydemir_SelfSupervisedObjectCentricLearningForVideos_2023, Zadaianchuk_VideoSaur_2024}, or diffusion models~\cite{Jiang_ObjectCentricSlotDiffusion_2023, Wu_SlotDiffusion_2023}
%
These object-centric representations benefit multiple downstream tasks such as reinforcement learning for robotic manipulation~\cite{Mosbach_SOLDReinforcementLearningSlotObjectCentricLatentDynamics, Ferraro_Focus_2023} or visual-question-answering~\cite{Mamaghan_ObjectCentricRepsForVQA_2024}.




\paragraph{Object-Centric Video Prediction}

Object-centric video prediction aims to model the object dynamics and interactions in a video sequence with the goal of forecasting future object states and video frames.
%
Several methods address this task using different architectural priors, including RNNS~\cite{Zoran_PARTS_2021, Assouel_VariationIdependentModulesVP_2022}, transformers~\cite{Villar_OCVP_2023, Wu_SlotFormer_2022, Daniel_DDLP_2024, Meo_ObjectCentricTemporalConsistency_2024} or state-space models~\cite{Jiang_SlotSSM_2024}, attaining a remarkable prediction accuracy on synthetic datasets.
%
Recently, some methods improve the controllability of object-centric video prediction models by conditioning the prediction process on actions~\cite{Mosbach_SOLDReinforcementLearningSlotObjectCentricLatentDynamics} or language captions~\cite{Wang_TIVDiffusion_2024}.
%
However, forecasting future object states without supervision in complex environments still remains an open challenge.




% main fig
\input{imgs/main_fig.tex}




\paragraph{Learning Latent Actions from Unlabeled Videos:}
%

Videos provide abundant information about dynamics and activities, but often lack the action labels necessary for learning behaviors from video.
%
To address this challenge, some methods train a latent policy directly from observations by learning a discrete latent action space and sampling the actions that minimize a reconstruction error~\cite{Edwards_ImitatingLatentPoliciesFromObservation_2019, Struckmeier_ILPI_2023}.
%
Another group of methods, to which \Method{} belongs, learns inverse dynamics from unlabeled videos by predicting latent actions given pairs of observations, and uses them for learning behaviors for video games and robot simulations~\cite{Ye_BecomeAProficientPlayerByWatchingPureVideos_2022, Brandfonbrener_InverseDynamicsPretrainingLearnsGoodRepresentatiosForImitation_2024, Schmidt_LAPOLearningToActWithoutActions_2024}, as pretraining for Vision-Language-Action models~\cite{Ye_LatentActionPretrainingFromVideos_2024} or for learning robot policies~\cite{Cui_DynaMoDynamcisPretrainingForVisuoMotorControl_2024}.
%

Latent action models have also been used for conditional video prediction.
%
The most similar method to ours is \emph{CADDY}~\cite{Menapace_PlayableVideoGeneration_2021, Menapace_PlayableEnvironments_2022}, which learns latent actions from a collection of unlabeled videos from a single domain and uses the latent actions as conditioning signal for predicting future frames.
%
At inference time, CADDY maps user inputs to the latent space for playable video generation.
%
Building upon this same principle, \emph{Genie}~\cite{Bruce_GenieGenerativeInteractiveEnvironments_2024} proposes a foundation world model for playable video generation on diverse environments.
%
However, both CADDY and Genie operate on holistic scene representations, which are limited for tasks that require relational reasoning, often struggle to model object relationships and interactions, and require human supervision to generalize to scenes with multiple moving agents.









%%%%%%%%%%%%%%%%%%
%% Method %%
%%%%%%%%%%%%%%%%%%
\section{\Method}


We propose \emph{\Method}, a novel framework for controllable object-centric video prediction from unlabeled video sequences.
%
\Figure{fig: main}a) illustrates the training process in \Method, as well as its main four components.
%
Namely, given $\NumFrames$ video frames $\ImageRange{1}{\NumFrames}$, our model employs as \emph{Scene Parsing module}
that decomposes these images into object representations, called slots, $\SlotsMany{1}{\NumFrames} = (\SlotsT{1}, ..., \SlotsT{\NumFrames})$, where $\SlotsT{t}=(\SingleSlotsT{t}{1}, ..., \SingleSlotsT{t}{\NumSlots}) \in \R^{\NumSlots \times \SlotDim}$ is the set of $\SlotDim$-dimensional object slots parsed from frame $\ImageT{t}$.
%
For each consecutive pair of frames, \Method{} employs an \emph{Inverse Dynamics} (\InverseDynamics) module (\Section{section: inverse dynamics}) in order to estimate latent action embeddings
$\PredLatentT{t}$ that encode the actions taken by the agents in the scene between every consecutive pair of frames.
%
The \emph{Conditional Object-Centric Predictor} (\CondPred) (\Section{section: predictor}) forecasts future object states conditioned on past slots and latent actions estimated by $\InverseDynamics$.
%
Finally, the \emph{object rendering} module decodes the object slots to render object images and masks, which can be combined via a weighted sum to render video frames.


At inference time, as shown in \Figure{fig: main}b), \Method{} autoregressively predicts multiple possible sequence continuations conditioned on the initial object slots and latent action embeddings, which can be estimated by \InverseDynamics, provided by human, or generated by a learned action policy.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SAVi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Object-Centric Representation Learning}
\label{section: savi}

\Method{} employs SAVi~\cite{Kipf_ConditionalObjectCentricLearningFromVideo_2022},
a recursive encoder-decoder model with a structured bottleneck of $\NumSlots$ permutation-equivariant object slots,
to parse a sequence of video frames $\ImageRange{1}{\NumFrames}$ into their object components $\SlotsMany{1}{\NumFrames}, \SlotsT{t} \in \R^{\NumSlots \times \SlotDim}$.
%
The slots $\SlotsT{0}$ are sampled from a learned distribution and recursively refined to bind to the objects in the video frames.
%
At time step $t$, SAVi encodes the corresponding input into feature maps $\FeatureMaps \in \R^{\NumLocs \times \DimFeats}$,
%
which are fed to Slot Attention~\cite{Locatello_ObjectCentricLearningWithSlotAttention_2020} to
iteratively refine the previous slot representations conditioned on the current features.
%
Slot Attention performs cross-attention between the image features and slots with the attention weights normalized over the slot dimension, encouraging competition between slots so as to represent each feature location:
%
\vspace{-0.1cm}
\begin{align}
	& \Attention = \softmax_{\NumSlots} \left( \frac{q(\SlotsT{t-1})\cdot k(\FeatureMaps_t)^T}{\sqrt{\SlotDim}} \right) \in \R^{\NumSlots \times \NumLocs},
	\label{eq: slot attention}
\end{align}
\vspace{-0.1cm}
%
where $q$ and $k$ are linear projections. % to a common dimension.
%
The slots are then independently updated via a shared Gated Recurrent Unit~\cite{Cho_GRU_2014} (GRU) followed by a residual MLP:
%
\begin{align}
	& \SlotsT{t} = \GRU(\Attention \cdot v(\FeatureMaps_t), \SlotsT{t-1})
	\; , \;\; \Attention_{n,l} = \frac{\Attention_{n,l}}{\sum_{i=0}^{\NumLocs-1}\Attention_{n,i}},
	\label{eq: slot update}
\end{align}
%
where $v$ is a linear projection. The steps described in \Equations{eq: slot attention}{eq: slot update} can be repeated multiple times with shared
weights to iteratively refine the slots and obtain an accurate object-centric
representation of the scene.

To map the object representations back to images, SAVi independently decodes each slot in $\SlotsT{t}$ with a Spatial Broadcast Decoder~\cite{Watters_SpatialBroadcastDecoder_2019} ($\SlotDecoder$) to render an object image and mask, which can be normalized and combined via a weighted sum to render the reconstructed frame:
%
\begin{align}
	& \SlotObject{t}{n}, \SlotMask{t}{n} = \SlotDecoder(\SingleSlotsT{t}{n}), \\
%	& \SlotObject{t}{n}, \SlotMask{t}{n} = \text{Decoder}(\SingleSlotsT{t}{n}), \\
	& \PredImageT{t} = \sum_{n=1}^{\NumSlots} \SlotObject{t}{n} \cdot \ProcessedMask{t}{n} \;\; \text{with} \; \ProcessedMask{t}{n} = \softmax_{\NumSlots}(\SlotMask{t}{n}).
\end{align}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Learning Inverse Dynamics
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Learning Inverse Dynamics}
\label{section: inverse dynamics}


In general, future frames depend not only on previous observations, but also on other variables, such as robot actions.
%
We propose an inverse dynamics module (\InverseDynamics) that estimates, given the object slots from two consecutive time steps, latent action embeddings $\PredLatent \in \R^\DimLatent$ that encode the actions taken by the agents between such time steps:
%
\begin{align}
	& \PredLatentT{t} = \InverseDynamics(\SlotsT{t}, \SlotsT{t+1}).
\end{align}




% Action Parameterization
\subsubsection{Action Parameterization}
%
The parameterization of the latent actions determines the complexity of the transitions that can be modeled, as well as the degree of control that we have over the predictions.
%
On the one hand, learning a finite set of latent actions allows for controllable video prediction while limiting the complexity of the dynamics that such actions can explain.
%
On the other hand, continuous latent vectors can model complex transitions between frames with the drawback of less interpretability and control.

As a compromise between these two approaches, inspired by~\citet{Menapace_PlayableVideoGeneration_2021}, we propose a hybrid approach to parameterize the latent actions $\PredLatentT{t}$ with a discrete component $\ActionProtoT{t}$ denoted as \emph{action prototype}, which determines the high-level action taking place (e.g. move left, go up), and a continuous \emph{action variability} $\ActionVariabilityT{t}$, which captures non-deterministic dynamics in the environment and enables to interpolate between action prototypes.
%
This combination allows for modeling complex frame transitions effectively.



% Architectures
\subsubsection{\InverseDynamics~Module}
\label{sec: inverse dynamics}


We propose two variants of our inverse dynamics module.
%
{$\InverseDynamicsSingle$} processes object slots $\SlotsT{t}$ along with an additional token~\ActToken~using a transformer encoder $\ActionEmbPred$. It outputs a single latent action $\PredLatentT{t}$ that captures the agent's action, making it well-suited for single-agent environments.
%
In contrast, {$\InverseDynamicsMany$} processes each slot with a shared MLP, producing $\NumSlots$ latent action embeddings $\PredLatentBigT{t}=\{\PredLatentBigSingleT{t}{1}, ..., \PredLatentBigSingleT{t}{\NumSlots}\}$, each representing the action of a specific object in the scene.
%
Below we explain the process for computing latent actions using $\InverseDynamicsSingle$, which follows a similar procedure to that of $\InverseDynamicsMany${}.


Following \citet{Menapace_PlayableVideoGeneration_2021}, we adopt a probabilistic formulation where $\InverseDynamics$ predicts the posterior distribution of scene dynamics, modeled as Gaussian:
%
\begin{align}
	\MeanDynT{t}, \VarDynT{t} = \ActionEmbPred(\SlotsT{t}, \ActToken).
\end{align}
%
We then model the distribution of latent actions $\PredLatentT{t}$ as the difference between the distributions of dynamics embeddings from two consecutive time steps:
%
\begin{align}
	& \PredLatentT{t} \sim \mathcal{N}(\MeanActT{t}, \VarActT{t}) \;\; \text{with} \;\;
	\begin{cases}
		\MeanActT{t} = \MeanDynT{t+1} - \MeanDynT{t}, \\
		\VarActT{t} = \VarDynT{t+1} + \VarDynT{t},
	\end{cases},
\end{align}
%
from which we can sample the latent actions $\PredLatentT{t}$.


To prevent the model from simply encoding the target scene into $\PredLatentT{t}$, we regularize the latent action space by enforcing an information bottleneck.
%
Specifically, we constrain the latent action space to be low dimensional, i.e., $\DimLatent << \SlotDim$.
%
Furthermore, we parameterize the latent actions as the sum of a discrete action prototype $\ActionProtoT{t}$ and an action variability embedding $\ActionVariabilityT{t}$,
%
where $\ActionProtoT{t}$ is obtained by vector-quantizing the latent actions $\PredLatentT{t}$, i.e. $\ActionProtoT{t} = \text{VQ}(\PredLatentT{t})$.
%
We empirically verify that the information bottleneck enforced by vector quantization achieves comparable performance to the one proposed by~\citep{Menapace_PlayableVideoGeneration_2021}, while requiring significantly fewer hyper-parameters.


This latent action parameterization ensures that our $\InverseDynamics$ module encodes only the essential dynamics, effectively capturing the agent’s interaction with the scene while learning semantically meaningful action prototypes.
%
Moreover, this hybrid factorization improves the controllability and interpretability of the prediction process while maintaining the ability to model complex scene dynamics.
%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Predictor
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Conditional Object-Centric Prediction}
\label{section: predictor}
%

We employ a transformer-based~\cite{Vaswani_AttentionIsAllYouNeed_2017} module to autoregressively predict future object slots
conditioned on past object states and latent actions.

Our proposed predictor, \CondPred, is a transformer encoder with $\NumPredLayers$ layers.
%
At each time step $t$, \CondPred~takes as input all previous slots $\SlotsMany{1}{t}$, action prototypes $\ActionProtosMany{1}{t}$ and variability embeddings $\ActionVariabilityMany{1}{t}$, all of which are first linearly projected into a shared token dimensionality.
%
The slots are then conditioned by adding them with the corresponding projected action prototype and variability embeddings.
%
Additionally, we incorporate sinusoidal positional encodings such that all slots from the same time step receive the same encoding, thus preserving the inherent permutation equivariance of the objects.

\CondPred~forecasts the future slots $\PredSlotsT{t+1}$ by jointly modeling the object dynamics and interactions from the past object slots conditioned on the inferred latent actions.
%
This process is summarized as:
%
\begin{align}
	& \PredSlotsT{t+1} = \CondPred(\LinearSlots(\SlotsMany{1}{t}) + \LinearProto(\ActionProtoT{1:t}) + \LinearVar(\ActionVariabilityT{1:t})),
\end{align}
%
where $\LinearSlots, \LinearProto~\text{and}~\LinearVar$ are learned linear layers.


The prediction process can be initiated from the slots of a single reference frame $\SlotsT{1}$ and the corresponding inferred latent actions $\PredLatentT{1}$.
%
This process is repeated autoregressively, with the predicted slots being appended to the input at each subsequent time step, allowing the generation of future object representations for a desired number of time steps $\NumPreds$.




\subsection{Learning Behaviors from Unlabeled Videos}
\label{sec: behavior learning}


Using a trained \InverseDynamics~module, we aim to learn a policy from unlabeled video expert demonstrations without the need for action or reward information.
%
For this purpose, we compute with $\InverseDynamics$ a sequence of latent actions that explain the dynamics of the expert demonstrations, and then train a policy model $\PolicyModel$ to
regress such latent actions using the object slots from the corresponding time step.

At inference time, starting with a single observation $\ImageT{1}$, \Method{} computes the corresponding object slots $\SlotsT{1}$ and uses the policy to estimate a latent action $\PredLatentT{1}$, which is decomposed into an action prototype  $\ActionProtoT{1} = \text{VQ}(\PredLatentT{1})$ and variability embedding $\ActionVariabilityT{1} = \PredLatentT{1} - \ActionProtoT{1}$.
%
These representations are fed to $\CondPred$ to forecast subsequent slots $\PredSlotsT{2}$.
%
This process is repeated autoregressively, allowing the learned behavior to unfold within the model's latent imagination.



To map the latent actions generated by the policy $\PolicyModel$ to the real action space, we introduce an action decoder $\ActionDecoder$.
This module, implemented as a three-layer MLP, is trained to translate the latent actions inferred by $\InverseDynamics$~into the real-world actions using a small action-labeled dataset.


This approach shares similarities with \citet{Schmidt_LAPOLearningToActWithoutActions_2024}.
%
However, whereas their method learns policies for simple games with a small discrete set of actions, our flexible action representation and conditional object-centric decoder enable us to learn more complex robot behaviors.








\subsection{Training}

We differentiate three different training stages in \Method.
%
We first train SAVi to parse video frames into object-centric representations by minimizing a reconstruction loss:
%
\begin{align}
	\Loss_{\text{SAVi}} &=  %\frac{1}{\NumFrames}
	\sum_{t=1}^{\NumFrames}|| \SlotDecoder(\ImageEncoder(\ImageT{t})) - \ImageT{t} ||_2^2, \label{eq: loss savi}
\end{align}
%
where $\ImageEncoder$~and $\SlotDecoder$~correspond to the scene parsing and object rendering modules, respectively.

Second, given the pretrained SAVi model, we jointly train $\InverseDynamics$ and $\CondPred$ by minimizing a combined loss:
%
%
\begin{align}
	\Loss_{{\Method}} &= %\frac{1}{\NumPreds}
	\sum_{t=2}^{\NumPreds+1} \lambda_{\text{Img}}
	\Loss_{\text{Img}} + \lambda_{\text{Slot}} \Loss_{\text{Slot}} + \lambda_{\text{VQ}} \Loss_{\text{VQ}},  \label{eq: full loss}  \\
	%
	\Loss_{\text{Img}} &=  || \PredImageT{t} - \ImageT{t} ||_2^2 \label{eq: loss img}, \\
	%
	\Loss_{\text{Slot}} &=  || \PredSlotsT{t} - \ImageEncoder(\ImageT{t}) ||_2^2,
	\label{eq: loss slot}   \\
	%
	\Loss_{\text{VQ}} &= ||\texttt{sg}[\PredLatentT{t}] - \ActionProtoT{t}|| +
	0.25 \cdot || \PredLatentT{t}- \texttt{sg}[\ActionProtoT{t}] ||,  \label{eq: loss vq}
	%
\end{align}
%
%
where \texttt{sg} is the stop-gradient operator. $\Loss_{\text{Img}}$ measures the future frame prediction error, $\Loss_{\text{Slot}}$ aligns the predicted object slots with the actual object-centric representations, and $\Loss_{\text{VQ}}$ encourages the learning of meaningful action prototypes while regularizing the latent actions to align with their prototypes~\cite{VanOord_NeuralDiscreteRepresentationLearning_2017}.
%
We do not employ teacher forcing, enabling the predictor model to learn to handle its own imperfect predictions.


Finally, the policy model $\PolicyModel$ and action decoder $\ActionDecoder$ are trained
to regress the inferred latent actions $\PredLatent$ and ground truth actions $\Action$, respectively:
%
\vspace{-0.1cm}
\begin{align}
	\Loss_{\PolicyModel} &=  %\frac{1}{\NumFrames}
		\sum_{t=1}^{\NumFrames}
		|| \PolicyModel(\SlotsT{t}) - \PredLatentT{t} ||, \\
	%
	\Loss_{\ActionDecoder} &= %\frac{1}{\NumFrames}
	\sum_{t=1}^{\NumFrames} ||\ActionDecoder(\PredLatentT{t}) - \ActionT{t}||.
\end{align}
\vspace{-0.0cm}




%%%%%%%%%%%%%%%%%%
%% Experiments %%
%%%%%%%%%%%%%%%%%%



\input{./tables/main_table.tex}





\section{Experiments}




\subsection{Experimental Setup}




\subsubsection{Datasets}

We evaluate our method on three environments with distinct characteristics.
%
Further details are provided in \Appendix{app: dataset}.


\noindent \textbf{\ButtonPress} This environment, based  on \emph{MetaWorld}~\cite{Yu_MetaworldDataset_2020}, features a Sawyer robot arm that must press a red button.
%
This environment depicts a non-object centric task involving complex shapes and textures.


\noindent\textbf{\RobotDB}: This environment, inspired by~\cite{Li_TowardsPracticalMultiObjectManipulationRelationalRL_2020}, features a robot arm and a table with multiple uni-colored cubes of different colors.
%
The robot must push the block of distinct color into the location specified by a red target.
%
This task evaluates the capabilities of an agent to reason about object relations and to model object collisions.


\noindent\textbf{GridShapes:} This dataset features two simple 2D shapes moving in grid-like patterns, restricted to up, down, left, or right directions on a colored background.
%
The shapes randomly change direction with a predefined probability, introducing stochasticity to their motion.
%
%We include multiple variants of this dataset, each with a different number of moving objects, ranging from one to five.
%
This simple dataset serves as benchmark to evaluate a model's ability to jointly predict the motion of multiple moving agents in the scene.




\input{./imgs/qual_comb_v3.tex}




\subsubsection{Implementation Details}

All our models are implemented in PyTorch~\cite{Paszke_AutomaticDifferneciationInPytorch_2017} and trained on a single NVIDIA A100 GPU.
%
\Method{} uses SAVi~\cite{Kipf_ConditionalObjectCentricLearningFromVideo_2022} with 128-dimensional object slots, as well as a convolutional encoder and spatial broadcast decoder as scene parsing and rendering modules, respectively.
%
The conditional predictor and inverse dynamics modules are transformer encoders with four layers and a token dimension of 256.
%
For the \ButtonPress~and \RobotDB~datasets, we use the $\InverseDynamicsSingle$ variant with eight different 16-dimensional action prototypes, whereas for GridShapes we use $\InverseDynamicsMany$ with five distinct eight-dimensional action prototypes.
%
Further implementation details are provided in \Appendix{app: implementation details}.







\subsection{Video Prediction}

We evaluate \Method{} for video prediction and compare it with different baselines, including the object-centric video prediction models SlotFormer~\cite{Wu_SlotFormer_2022} and OCVP-Seq~\cite{Villar_OCVP_2023}, the stochastic video prediction model SVG-LP~\cite{Denton_StochasticVideoGenerationWithALearnedPrior_2018} and the playable video generation model CADDY~\cite{Menapace_PlayableVideoGeneration_2021}.
%
For a fair comparison, all models are trained with six seed frames to predict the subsequent eight, and evaluated for 15 predictions.
%
For CADDY, $\Method${} and SVG, we predict future frames conditioned on latent actions or vectors inferred from the ground truth sequence.
%
Additionally, on the \RobotDB~and \ButtonPress~datasets all models are trained using sequences with random exploration policies, and evaluated on expert demonstrations.

We evaluate the quality of the predicted frames using standard metrics: PSNR, SSIM~\cite{Wang_SSIM_2004} and LPIPS~\cite{Zhang_TheUnreasonableEffectivenessOfDeepFeaturesLPIPS_2018}.
%
A quantitative comparison of the methods is presented in \Table{table:quantitative comparison}.
%
As expected, deterministic object-centric models (i.e. SlotFormer and OCVP) perform poorly, as they cannot infer the agent's actions and simply average over multiple possible futures.
%
Our proposed method outperforms all other models on both the \RobotDB~and GridShapes datasets, demonstrating \Method's superior ability to forecast future video frames in environments
involving multiple object interactions and moving agents, respectively.
%

\Figure{fig: qual_01} depicts a qualitative comparison of the best performing methods on the \ButtonPress~and \RobotDB~datasets, respectively.
%
On the \ButtonPress~dataset, as shown in \Figure{fig: qual_01}a), all methods accurately model the motion of the robot arm.
%
However, on the more complex \RobotDB~task, depicted in \Figure{fig: qual_01}b), SVG and CADDY fail to model the object collisions, leading to blurriness and vanishing objects.
%
In contrast, \Method{} maintains sharp object representations and correctly models interactions between objects, leading to accurate frame predictions.
%
Further qualitative evaluations are provided in \Appendix{app: extra results}.


\input{./imgs/fig_gridshapes.tex}



\subsection{Model Analysis}


\noindent \textbf{Impact of Number of Moving Objects:}
%
We evaluate the performance of our method for different number of moving objects.
%
For this purpose, we train two \Method{} variants with the $\InverseDynamicsSingle$ and $\InverseDynamicsMany$ inverse dynamics modules, respectively, and compare them with the SVG and CADDY baselines on several variants of the GridShapes dataset featuring a different number of objects, ranging from one to five moving shapes.
%
The results are depicted in \Figure{fig: gridshapes}.
%
CADDY and \Method{} with $\InverseDynamicsSingle$, which encode scene dynamics using a single latent action, perform strongly when jointly forecasting one or two objects, but experience a sharp drop in performance as the number of objects increases.
%
SVG scales to multiple moving objects but encodes the dynamics of all objects into a single distribution, limiting its flexibility and control over the predictions.
%
In contrast, \Method{} with $\InverseDynamicsMany$ uses a latent action per object, allowing to scale seamlessly to a large number of moving agents by individually modeling the motion of each object, thus outperforming all baselines.





\noindent \textbf{Action Representation:}
%
In \Table{table: ablation} we compare the hybrid latent action representation used in \Method{} with three different variants that use continuous latent actions, a discrete set of latent actions, and an oracle variant with access to ground truth actions.
%
Additionally, we evaluate the models on two different \RobotDB~variants, including
a set with random exploration sequences, similar to the training distribution, and another set featuring expert demonstrations.

On the random exploration set, \Method{} with a hybrid latent action performs comparably to the oracle, highlighting the ability of our $\InverseDynamics$ module to infer latent dynamics from unlabeled sequences.
%
However, \Method's video prediction performance drops on the expert demonstrations, where the variant using unconstrained continuous latent actions outperforms it.
%
We attribute this to the large discrepancy between the action distribution of expert sequences and the training distribution, which challenges the generalization of the learned action prototypes.





% ABLATTION OF ACTION REPRESENTATIONS
\input{./tables/ablations.tex}





% EFFECT OF THE ACTIONS
\input{./imgs/action_fig_00.tex}

\noindent \textbf{Learned Actions:}
%
\Figure{fig: action_figs robot} depicts the effect of different action prototypes learned by \Method{} on the \RobotDB~dataset.
%
Given a single seed frame, we forecast 15 frames by repeatedly conditioning the predictor on the same action prototype.
%
Furthermore, we visualize the predictions obtained using the latent actions inferred by our inverse dynamics module from the ground truth sequence; as well as the instance segmentation maps obtained by assigning a different color to each slot mask.
%
$\Method${} learns to infer precise robot actions from visual observations and the physics of interacting objects, while distinctly representing each object in a different slot.
%
Additionally, \Figure{fig: action_figs robot} shows that \Method{} learns consistent and semantically meaningful actions, such as moving the robot towards the right (action 2), left (action 7), or upwards (action 4).
%







% REAL World Robotics Data
\input{./imgs/qual_sketchy_00.tex}

\noindent \textbf{Real-World Robotic Videos:}
%
We validate the applicability of \Method{} to real-world robotic videos using the Sketchy~\cite{Cabi_ScalingDataDrivenRoboticsRewardSketching_2020} dataset, which features a robotic gripper interacting with diverse objects.
%
\Figure{fig: sketchy00} shows a qualitative result of \Method{} on Sketchy.
%
Our model accurately infers the scene's inverse dynamics, reconstructing the ground truth sequence from a single reference frame.
%
Furthermore, \Method{} learns semantically meaningful and consistent action prototypes, capturing diverse behaviors such as opening the gripper (action 6), moving the robot downwards (action 2), or upwards (action 4).






\input{./imgs/beh_button_00.tex}

\subsection{Learning Behaviors from Expert Demonstrations}
\label{sec: behavior}


We evaluate the quality of $\Method$'s object-centric representations and inferred latent actions for a downstream behavior learning task.
%
To this end, we train a policy model and an action decoder, as described in~\Section{sec: behavior learning}, to imitate \ButtonPress~and~\RobotDB~behaviors from a limited set of expert demonstrations.
%
\Figure{fig: behavior}. illustrates the learned latent policies on the (a) \ButtonPress~and (b) \RobotDB~environments, respectively.
%
The top row in each sequence (labeled \emph{Latent Pred.}) shows predicted trajectories within \Method's latent imagination, where the model, starting from a single reference frame, autoregressively generates latent actions using the policy model and predicts future scene states in the latent space.
%
The bottom row (labeled \emph{Sim. Actions}) depicts the simulated execution of the decoded latent actions in the corresponding environment.
%
\Method{} learns to solve both tasks within its latent imagination, successfully reasoning about object properties and generating a precise sequence of latent actions, which can be decoded into executable motions.
%
Further results can be found in \Appendix{app: learned behaviors}.






\section{Conclusion}

We introduced $\Method$, a novel framework for controllable object-centric video prediction. $\Method$~parses video frames into object slots, infers the scene's inverse dynamics, and predicts future object states and video frames by modeling the object dynamics and interactions, conditioned on inferred latent actions.
%
Through extensive experiments, we demonstrated that $\Method$ learns a semantically rich and meaningful action space, allowing for accurate video frame predictions.
%
Our method outperforms several baseline models, offering superior controllability and interpretability.
%
Moreover, we demonstrated that the learned object representations and latent actions inferred by $\Method$ can be utilized to predict future frames with precise robot control, while also enabling the model to learn complex robot behaviors from unlabeled video demonstrations.
%
This versatility makes \Method{} a powerful and interpretable world model suitable for various tasks in autonomous systems.















\newpage

\section*{Acknowledgment}

This work was funded by grant BE 2556/16-2 (Research Unit FOR 2535 Anticipating Human Behavior) of the German Research Foundation (DFG)


\section*{Impact Statement}

This paper presents work whose goal is to advance the field of machine learning by introducing \Method{} -- an object-centric video prediction model that forecasts future video frames conditioned on inferred object-centric representations and latent actions, enhancing its interpretability and controllability, as well as learning representations that can be used for action planning.
%
This advancement is particularly relevant for domains such as robotics and autonomous systems, where understanding and predicting complex environments are essential for correct and safe operation.
%
However, the deployment of such models in real systems requires careful consideration for ethical and safety challenges, such as biases in the training data or lack of transparency on the decision-making process.

%




\bibliography{referencesAngel}
\bibliographystyle{icml2025}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix

\onecolumn  % optional


\section{Limitations \& Future Work}

We recognize two main limitations that currently limit the scope of our \Method~framework to simple tabletop robotic scenarios, preventing it from generalizing to more complex domains.


\paragraph{Limited Decomposition Model} The first limitation arises from the SAVi object-centric decomposition model used at the core of our framework.
%
SAVi achieves a great decomposition performance on datasets with objects of simple shapes and textures. However, it fails to generalize to complex real-world robotic scenarios, hence limiting the current scope of \Method{} to robotic tabletop simulations, or simple real-world environments as in Sketchy.


\paragraph{Single Latent Action} The second limitation lies at the representational capability of our latent actions and action prototypes.
%
In robotic scenarios, several actions often happen simultaneously, such as moving and rotating the robot, as well as opening or closing the gripper.
%
Our current latent action representation jointly models the scene's inverse dynamics, encoding together all actions into a single latent space.
%
This entangled representation limits our ability to control the agents in the scene with greater precision.


\paragraph{Future Work} In future work, we plan to extend our proposed \Method{} framework with more capable decomposition models, such as DINOSAUR~\cite{Seitzer_BridgingTheGapToRealWorldObjectCentricLearning_2023} or SOLV~\cite{Aydemir_SelfSupervisedObjectCentricLearningForVideos_2023}, as well as scale our inverse dynamics and predictor models.
%
Furthermore, we can employ factorized latent action vectors, which represent in a disentangled manner different actions that happen simultaneously, such as moving the robot arm and opening the gripper.
%
We believe that this architectural modifications will enable us to use \Method{} on more complex robotic simulations and perform real-world robotic experiments.







\section{Implementation Details}
\label{app: implementation details}

In this section, we describe the network architecture and training details for each of the components in our \Method{} framework.
%
Our models are implemented in PyTorch~\cite{Paszke_AutomaticDifferneciationInPytorch_2017} and are trained on a single NVIDIA A100 GPU.



\subsection{Object-Centric Learning}

We closely follow~\citet{Kipf_ConditionalObjectCentricLearningFromVideo_2022} for the implementation of the SAVi object-centric decomposition model, which we employ as scene parsing and object rendering modules.
%
We strictly adhere to the architecture of their proposed CNN-based image encoder $\ImageEncoder$, slot decoder $\SlotDecoder$, transformer-based dynamics transition module, and Slot Attention corrector.
%
We use a variable number of 128-dimensional object slots, depending of the dataset.
%
Namely, we employ eight object slots on \RobotDB, four object slots on \ButtonPress, and three object slots on GridShapes.
%
On all datasets, we sample the initial object slots $\SlotsT{0}$ from a Gaussian distribution with learned mean and covariance.
%
Furthermore, we use three Slot Attention iterations for the initial video
frame to obtain a good initial object-centric decomposition, and a single iteration for subsequent frames, which suffices to recursively update the slot representation state given the newly observed image features.
%



\subsection{Inverse Dynamics Module}

%
We propose two variants of our \InverseDynamics~module.

\noindent \textbf{$\InverseDynamicsSingle$:} $\InverseDynamicsSingle$~jointly processes the objects slots from a single time step $\SlotsT{t}$ along with an additional
token \ActToken~using a transformer encoder.
%
We use a four-layer transformer encoder with a 256-dimensional tokens, four 64-dimensional heads and hidden dimension of 1024.
%
This module aggregates information from the object slots into the \ActToken~token, and outputs a single latent action $\PredLatentT{t}$  that captures the agent’s action, making it well-suited for single-agent environments.


\noindent \textbf{$\InverseDynamicsMany$:} $\InverseDynamicsMany$~independently processes each object slot with a shared MLP, thus generating $\NumSlots$ latent action embeddings, each representing the action of a specific object in the scene.
%
This design makes $\InverseDynamicsMany$~well-suited for environments with multiple moving agents.
%
We employ a two-layer MLP, featuring a ReLU nonlinear activation and layer normalization.


As described in~\Section{sec: inverse dynamics}, the generated latent actions $\PredLatentT{t}$ are vector-quantized to assign them to their corresponding action prototype $\ActionProtoT{t}$.
%
On the \ButtonPress, \RobotDB, and Sketchy datasets, we use the $\InverseDynamicsSingle$ variant with eight different 16-dimensional action prototypes,
whereas for GridShapes we use $\InverseDynamicsMany$ with five distinct eight-dimensional action prototypes.
%
Following common practice, we update the action prototypes using the exponential moving average updates of cluster assignment counts~\cite{VanOord_NeuralDiscreteRepresentationLearning_2017}.



\subsection{Conditional Object-Centric Predictor}

Our conditional object-centric predictor (\CondPred) is inspired by the transformer-based SlotFormer~\cite{Wu_SlotFormer_2022} architecture.
%
Our \CondPred~module features four layers, 256-dimensional tokens, eight 64-dimensional attention heads, and hidden dimension of 512.

To enable predictions conditioned on the inferred latent actions, \CondPred~maps the action prototypes $\ActionProtoT{1:t}$, variability embeddings $\VarActT{1:t}$ and object slots into the token dimensionality.
%
The slots are then conditioned by adding them with the projected action prototype and variability embedding from the corresponding time step.
%
Furthermore, following~\citet{Wu_SlotFormer_2022}, we augment these representations with a temporal sinusoidal positional encoding.




\subsection{Policy Model and Action Decoder}

The policy model $\PolicyModel$ follows a similar architecture to $\InverseDynamicsSingle$.
%
$\PolicyModel$ is a four layer transformer that jointly processes the objects slots from a single time step $\SlotsT{t}$ and an additional token \ActToken in order to regress a latent action.

The action decoder is a three-layer MLP with a hidden dimension of 128 that maps latent action vectors into real-world actions.



\subsection{Training Details}


\noindent \textbf{SAVi Training:}  SAVi is trained for object-centric decomposition using the Adam optimizer~\cite{Kingma_Adam_2014}, a batch size of 64, sequences of length eight frames, and a base learning rate of $10^{-4}$, which is linearly warmed-up for the first 4000 steps, followed by cosine annealing for the remaining of the training process.
%
Moreover, we clip the gradients to a maximum norm of 0.05.



\noindent \textbf{$\InverseDynamics$~and \CondPred~Training:} We jointly train our $\InverseDynamics$~and \CondPred modules given a pretrained SAVi decomposition model.
%
These modules are trained with the Adam optimizer~\cite{Kingma_Adam_2014}, batch size of 64, and a base learning rate of $2 \times 10^{-4}$, which decreases during training with a cosine annealing schedule.
%
To stabilize the training, we clip the gradients to a maximum norm of 0.05.
%
We set the loss weights to $\lambda_{\text{Img}} = 1$, $\lambda_{\text{Slot}} = 1$, and $\lambda_{\text{VQ}} = 0.25$.


\noindent \textbf{$\PolicyModel$~and $\ActionDecoder$~Training:} We train the $\PolicyModel${} and $\ActionDecoder${} modules given pretrained and frozen SAVi, $\InverseDynamics${} and \CondPred modules.
%
These modules are trained with the Adam optimizer~\cite{Kingma_Adam_2014}, batch size of 64, and a fixed learning rate of $2 \times 10^{-4}$.

%



\section{Dataset Details}
\label{app: dataset}


\subsection{\RobotDB} This environment, inspired by~\citet{Li_TowardsPracticalMultiObjectManipulationRelationalRL_2020} and simulated using MuJoCo~\cite{todorov2012mujoco}, features a robot arm on a tabletop interacting with multiple uni-colored blocks.
%
We use two different dataset variants.

The first variant consists of a robot controlled by a random exploration policy, moving in the environment with minimal meaningful object interactions.
%
This easily simulated dataset includes 20,000 training sequences and 2,000 validation sequences.
%
We use this dataset variant for training SAVi, as well as for jointly training our inverse dynamics and conditional predictor modules.

The second variant contains a smaller subset of expert demonstrations where the robot is tasked to push the block of distinct color to a target location marked with a red sphere.
%
This task evaluates the capabilities of an agent to reason about object relations and model object collisions.
%
We collect 5,000 expert demonstrations using a pretrained policy~\cite{Mosbach_SOLDReinforcementLearningSlotObjectCentricLatentDynamics} with a success rate of $\approx 80\%$ for this pushing task.
%
We split this dataset into 4,500 training sequences, which are used for training the policy model and action decoder; and 500 evaluation sequences that are used for benchmarking the prediction models.





\subsection{\ButtonPress}

This environment, based on MetaWorld~\cite{Yu_MetaworldDataset_2020} features a Sawyer robot arm tasked with pressing a red button.
%
Unlike \RobotDB, it involves a non-object-centric task with complex shapes and textures.
As before, we use two different dataset variants.

The first variant contains 10,000 sequences, split into 9,000 training and 1,000 validation videos, with the robot controlled by a random exploration policy.
%
We use this dataset variant to train SAVi, as well as our inverse dynamics and conditional predictor modules.

The second variant includes a small subset of expert demonstrations where the robot successfully presses the red button.
%
We collect 1,000 expert demonstrations using an expert policy, from which 900 are used for training the policy model and action decoder, and 100 demonstrations are used for benchmarking the prediction models.
%


\subsection{GridShapes}

This dataset features one or more simple 2D shapes moving in a grid-like pattern on top of a colored background.
%
The shapes can be either a ball, triangle or square, and have a random color. These shapes can move up, down, left, right or remain still, and revert their motion when an image boundary is reached, thus emulating a bouncing effect.
%
To introduce some stochasticity into the motion, the shapes randomly change direction with a predefined probability of 0.25.
%
We train and evaluate the models on several variants of the GridShapes dataset featuring different number of objects, ranging from one single moving shape, to five objects moving independently in the same sequence.
%
This simple dataset serves as benchmark to evaluate a model’s ability to jointly predict the motion of multiple moving agents in the scene.











\section{Baselines}

In our experiments, we compare our approach with different baseline models, including the stochastic and playable video prediction models SVG~\cite{Denton_StochasticVideoGenerationWithALearnedPrior_2018} and CADDY~\cite{Menapace_PlayableVideoGeneration_2021}, as well as the object-centric video prediction models SlotFormer~\cite{Wu_SlotFormer_2022} and OCVP~\cite{Villar_OCVP_2023}.


\subsection{SVG}

SVG~\cite{Denton_StochasticVideoGenerationWithALearnedPrior_2018} is a generative model for video prediction that captures both deterministic dynamics and stochastic variations in video sequences.
%
It combines a variational autoencoder (VAE) with recurrent neural networks (RNNs) to model stochastic temporal dynamics.
%
SVG represents a probabilistic framework, where the next frame $\PredImageT{t+1}$ is generated based on the previous frame $\ImageT{t}$ and a latent sample $\PredLatentT{t}$ drawn from a latent distribution.
%
In our experiments, we adapt the original implementation\footnote{\url{https://github.com/edenton/svg}} and use an SVG variant with a learned prior, VGG-like encoder and decoders, and two recurrent predictor layers.
%

\textbf{Main Difference with \Method:}
%
SVG operates with holistic scene representations, whereas our proposed method employs a structured object-centric representation.
%
Furthermore, SVG encodes the stochastic scene dynamics into a single continuous latent vector. In contrast, \Method{} follows a hybrid approach in which the latent action vectors are composed of an action prototype and action variability embeddings, making the prediction process more controllable and interpretable.



%SVG approximates the real posterior distribution over the latent space conditioned on the target frames up to the time-step $t$ $\text{p}(\PredLatentT{t-1}|\ImageRange{1}{t})$ using a so-called inference network. To prevent the posterior network from learning to simply copy the target frame $\ImageT{t+1}$,  without encoding any useful information into $\PredLatentT{t-1}$, the learned distribution is forced to be close to a prior distribution $\text{p}(\PredLatentT{t-1}|\ImageRange{1}{t-1})$. Both prior and posterior distributions are parameterized as Gaussian distributions.




\subsection{CADDY}
%
CADDY~\cite{Menapace_PlayableEnvironments_2022} is a recurrent encoder-decoder  model designed for playable video generation, enabling user-controllable future video prediction.
%
CADDY infers latent actions that encode the agent's actions between consecutive pairs of frames.
%
These latent actions are parameterized with a discrete \emph{one-hot action label}, which determines the high-level action taking place; and a high-dimensional \emph{action variability embedding}, which describes the variability of each action and captures the possible non-determinism in the environment.
%
We adapt the original implementation\footnote{\url{https://github.com/willi-menapace/PlayableVideoGeneration}} for our experiments.


\textbf{Main Difference with \Method:}
%
CADDY operates with holistic scene representations, i.e. CNN features, whereas our proposed method employs a structured object-centric representation.
%
Moreover, despite both methods using a hybrid parameterization of the latent actions, they differ in their implementation.
%
CADDY learns a discrete one-hot action label by minimizing multiple regularization objectives, including an action matching loss and several Kullback–Leibler  (KL) divergences losses.
%
In contrast, \Method{} employs a discrete set of high-dimensional action prototypes, which are learned via vector-quantization of the latent space.
%
We empirically verify that both approaches achieve comparable performance.
However, our vector quantization approach requires significantly fewer hyper-parameters and is easier to tune.




%


\subsection{SlotFormer}
%
SlotFormer~\cite{Wu_SlotFormer_2022} is an object-centric video prediction model that builds upon slot-based representations.
%
First, it uses Slot Attention to decompose an image into object-centric latent representations, called slots.
%
SlotFormer then employs a transformer-based autoregressive predictor module, which jointly processes all input slots in order to forecast future object representations.
%
Finally, the predicted slots are decoded into object images and video frames.
%
In our experiments, we adapt the original implementation\footnote{\url{https://github.com/pairlab/SlotFormer}}.


\textbf{Main Difference with \Method:} SlotFormer forecasts future slots in an unconditional manner, thus not being able to model stochastic environments or agents such as robots.
%
\Method{} addresses this challenge by inferring the scene inverse dynamics and using them to condition the prediction process.



\subsection{OCVP}
%
OCVP~\cite{Villar_OCVP_2023}, similar to SlotFormer, is a slot-based object-centric video prediction model.
%
Differing from SlotFormer, OCVP leverages to specialized decoupled attention mechanisms, \emph{relation} and \emph{temporal} attention, which model the object interactions and dynamics, respectively.
%
In our experiments, we use the OCVP-Seq variant with default settings adapted from original implementation\footnote{\url{https://github.com/AIS-Bonn/OCVP-object-centric-video-prediction}}.


\textbf{Main Difference with \Method:} OCVP forecasts future slots in an unconditional manner, thus not being able to model stochastic environments or agents such as robots.
%
\Method{} addresses this challenge by inferring the scene inverse dynamics and using them to condition the prediction process.





\section{Additional Results}
\label{app: extra results}



\subsection{Effect of the Number of Actions}


In \Table{table: num_actions} we evaluate on the \RobotDB~dataset multiple \Method{} variants using a different number of learned action prototypes.
%
We show that using eight learned action prototypes achieves the best video prediction performance, while learning a concise semantically meaningful set of action prototypes.
%

\input{./imgs/supplementary/table_num_actions.tex}




\subsection{Learned Behaviors from Expert Demonstrations}
\label{app: learned behaviors}


We evaluate the ability of \Method{} to learn robot behaviors from a small set of expert demonstrations.
%
As outlined in~\Section{sec: behavior}, we train a policy model and an action decoder to imitate \ButtonPress~and \RobotDB~behaviors from a small set of expert demonstrations.
%
We compare \Method{} to oracle baselines that have direct access to expert action labels.
%
This comparison allows us to assess the effectiveness of object-centric representations and inferred latent actions for downstream robotic tasks.



\noindent \textbf{\ButtonPress~Behavior:}
%
We quantitatively compare our learned policy model, which imitates the \ButtonPress~behavior,
against oracle baselines.
%
These models have access to all available expert demonstrations and directly regress the ground-truth actions from object-centric representations (Slot Oracle), ResNet feature maps (ResNet Oracle), and from feature maps output by the CADDY encoder (CADDY Oracle), respectively.
%
Unlike these oracles, \Method{} autoregressively predicts latent actions within its latent imagination before decoding them into executable actions.
%
Additionally, we compare with a modified CADDY variant, which includes a small policy model and action decoder designed output latent actions from observed feature maps and decode them into real-world actions, respectively.


The results, shown in \Figure{fig: quant beh}a), demonstrate that \Method{} consistently outperforms CADDY across all training regimes.
%
With as few as 200 demonstrations, \Method{} achieves comparable performance to the CADDY Oracle, which has direct access to more expert demonstrations and ground-truth actions.
%
As the number of demonstrations increases, \Method{} continues to improve, approaching the performance of the oracle models, while CADDY struggles to generalize.
%
These findings highlight the effectiveness of \Method's object-centric representations in capturing meaningful action dynamics, enabling efficient behavior learning from limited expert data.
%
Additionally, we observe that slot-based object-centric models outperform their holistic counterparts, further emphasizing the advantage of structured object-centric representations for learning robot behaviors.



% FIGURE DESCRIPTION
\Figure{fig: button beh 00} illustrates \Method's learned behavior in the \ButtonPress~environment.
%
The top row in each sequence (labeled as \emph{Latent Behavior}) shows predicted trajectories within \Method's latent imagination, where the model autoregressively generates actions using the policy model and predicts future scene states in the latent space.
%
The bottom row (labeled \emph{Sim. Actions}) depicts the simulated execution of the decoded latent actions in the environment.
%
\Method{} learns to solve the task within its latent imagination, successfully reasoning about the required action sequences before translating its latent actions into executable motions.
%
\Figure{fig: button beh 00}b) shows a failure case where \Method{} predicts within its latent imagination a trajectory that leads to successfully pressing the button. However, accumulated errors during action decoding cause the simulated execution to miss the button.




\input{./imgs/supplementary/fig_quant_beh.tex}

\input{./imgs/supplementary/behaviors_button/comb_fig.tex}




\noindent \textbf{\RobotDB~Behavior:}
%
We quantitatively evaluate \Method{} on the challenging \RobotDB~task, which requires reasoning about specific object properties.
%
In this task, our learned policy model imitates the behavior based on expert demonstrations, with only approximately 80\% of these demonstrations being successful.
%
The imperfect nature of the expert data adds an extra layer of difficulty to the learning process.


We compare \Method{} against two model-based reinforcement learning models with holistic (DreamerV3~\cite{hafner2023mastering}) and object-centric (SOLD~\cite{Mosbach_SOLDReinforcementLearningSlotObjectCentricLatentDynamics}) latent spaces, both of which learn the robot behavior by interacting with the environment.
%
The results are shown in \Figure{fig: quant beh}b).
%
As the number of demonstrations increases, \Method{} closes the gap with the baseline models, demonstrating its ability to generalize effectively from limited data.


% QUAL FIGURE
\Figure{fig: robot beh 00} illustrates \Method's learned behavior in the \RobotDB~environment.
%
The top row in each sequence (labeled as \emph{Latent Behavior}) shows predicted trajectories within \Method's latent imagination, where the model autoregressively generates actions using the policy model and predicts future scene states in the latent space.
%
The bottom row (labeled \emph{Sim. Actions}) depicts the simulated execution of the decoded latent actions in the environment.
%
\Method{} learns to solve the task within its latent imagination, successfully reasoning about object properties and generating a precise sequence of latent actions, which can be decoded into executable motions.
%
\Figure{fig: robot beh 00}c) shows a failure case where \Method{} controls the robot to interact with the correct block, but fails to place it in the target location.










\subsection{Qualitative Results}


\subsubsection{Comparison with Baselines}


\Figure{fig: qual_00} depicts a qualitative comparison between SVG, CADDY and $\Method${} 
on the \ButtonPress{} and \RobotDB{} datasets, respectively.
%
On the  \ButtonPress{} dataset, as shown in \Figure{fig: qual_00}a), all methods accurately model the trajectory of the robot arm.
%
However, on the complex \RobotDB{} task, depicted in \Figure{fig: qual_00}b), SVG and CADDY fail to model the
object collisions, failing to move the block of distinct color to the target location, and leading disappearing objects.
%
In contrast, $\Method${} maintains sharp object representations and correctly models interactions between objects, leading to accurate frame predictions.


\input{./imgs/qual_comb_v2.tex}



\input{./imgs/supplementary/behaviors_robotdb/comb_fig.tex}






\subsubsection{\RobotDB~Dataset}


\Figure{fig: robot qual} shows two qualitative comparisons between \Method, CADDY and SVG on the \RobotDB~dataset.
%
Our proposed method, which explicitly models object interactions, preserves sharp object representations and accurately predicts future frames.
%
In contrast, SVG and CADDY, which rely on holistic scene features for forecasting, struggle to model object collisions, resulting in blurry predictions and disappearing objects.




\Figure{fig: sup robot objs 00} shows the predicted video frames, slot masks, and objects representations on a \RobotDB~sequence.
%
\Method{} parses the scene into precise object images and masks, which can be assigned a unique color to obtain a segmentation of the scene.
%
Our approach decomposes the \RobotDB~environment using eight object slots, where one slot represents the background, five slots to different blocks, one slot to the red target, and one slot to the robot arm.
%
The sharp object images and masks demonstrate that \Method{} encodes into each slot features from the corresponding object.
%
This allows our method to directly reason about object properties, dynamics and interactions, allowing for accurate future frame predictions.




\Figure{fig: sup acts robot 00} depicts the effect each action prototype learned by \Method{} on the
\RobotDB~dataset.
%
\Method{} learns consistent semantically meaningful action prototypes that control the robot arm to move
in a specific direction.
%
We note that some actions prototypes, e.g, action 5 and 7, perform semantically similar actions but with different velocities.



\input{./imgs/supplementary/qual_robot_comb.tex}

\input{./imgs/supplementary/qual_robot_00/objs_00.tex}

\input{./imgs/supplementary/qual_robot_00/acts_00.tex}





\subsubsection{\ButtonPress~Dataset}

\Figure{fig: sup button qual} shows a comparisons between \Method, CADDY and SVG on the \ButtonPress~dataset.
All methods successfully predict the ground truth sequence by inferring and modeling the robot's dynamics,


\Figure{fig: sup button objs 00} shows \Method's~predictions and object representations on a \ButtonPress~sequence.
%
We visualize the ground truth sequence, the predicted frames, segmentation obtained by assigning a different color to each slot mask, as well as the object reconstructions for four slots.
%
\Method~assigns one slot to the background, one slot for box and red button, and two slots for different parts of the robot arm.


\Figure{fig: sup acts button 00}  depicts the effect each action prototype learned by \Method on the \ButtonPress~dataset.




\input{./imgs/supplementary/qual_button/qual_00.tex}

\input{./imgs/supplementary/qual_button/objs_00.tex}


\input{./imgs/supplementary/qual_button/act_00.tex}



\subsubsection{GridShapes Dataset}

\Figure{fig: sup acts gridshapes 00} depicts the effect each action prototype learned by \Method{} on a variant of GridShapes with three shapes.
%
\Method{} successfully captures the five possible object movements—up, down, left, right, and stay—predicting future frames by modeling the motion of each object independently.
%
However, we observe that \Method{} sometimes generates artifacts when shapes reach the image boundaries.
%
We attribute this to the training data, where objects change direction upon reaching the boundary mimicking a bouncing effect.
%
This leads to poor prediction performance when conditioning the model to predict outside its training distribution.



\input{./imgs/supplementary/gridshapes_00/act_00.tex}



\subsubsection{Sketchy Dataset}

\Figure{fig: sup sketchy objs 00} shows \Method's~predictions and object representations on a Sketchy sequence.
%
We visualize the ground truth sequence, the predicted frames, segmentation obtained by assigning a different color to each slot mask, as well as the object reconstructions for four slots.
%
\Method~assigns two slots to the workspace and background, two slots for each part of the robot gripper, and two slots for different objects present in the scene.
%


\input{./imgs/supplementary/sketchy_00/objs_00.tex}








\end{document}
