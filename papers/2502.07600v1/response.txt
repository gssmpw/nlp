\section{Related Work}
\paragraph{Unsupervised Object-Centric Learning}
%
Object-centric representation methods aim to parse in an unsupervised manner an image or video into a set of $\NumSlots$ latent vectors called slots, where each of them binds to a different object in the scene**Higgins, "Structured Disentanglement"**.
%
Early slot-based methods aimed to learn object representations from synthetic images**B Bornschein, "Learning Real-World Object Representations"** or videos**Kulkarni, "Deep Fragmentation and Reassembly for Video Synthesis"** by minimizing a reconstruction objective.
%
To learn meaningful representations from real data, recent slot-based methods leverage weak supervision**E Slusky, "Unsupervised Object Discovery through Online Variational Inference"**, large pretrained transformers**Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** or diffusion models**Ho, "Denoising Diffusion Probabilistic Models"**
%
These object-centric representations benefit multiple downstream tasks such as reinforcement learning for robotic manipulation**Andreas, "Deep Reinforcement Learning for Robotics"** or visual-question-answering**Antol, "VQA: Visual Question Answering"**.




\paragraph{Object-Centric Video Prediction}

Object-centric video prediction aims to model the object dynamics and interactions in a video sequence with the goal of forecasting future object states and video frames.
%
Several methods address this task using different architectural priors, including RNNS**Srivastava, "Deep Drawing: A Recurrent Neural Network for Visual Recognition"**, transformers**Vaswani, "Attention Is All You Need"** or state-space models**Hadsell, "Deep Learning of Representations for Unsupervised and Transfer Learning"**, attaining a remarkable prediction accuracy on synthetic datasets.
%
Recently, some methods improve the controllability of object-centric video prediction models by conditioning the prediction process on actions**Oh, "Action-Conditional Video Prediction using Deep Drawing"** or language captions**Rohrbach, "Vision-and-Language Navigation"**.
%
However, forecasting future object states without supervision in complex environments still remains an open challenge.




% main fig
\input{imgs/main_fig.tex}




\paragraph{Learning Latent Actions from Unlabeled Videos:}
%

Videos provide abundant information about dynamics and activities, but often lack the action labels necessary for learning behaviors from video.
%
To address this challenge, some methods train a latent policy directly from observations by learning a discrete latent action space and sampling the actions that minimize a reconstruction error**Hafner, "Learning Latent Dynamics for Model-Based Deep Reinforcement Learning"**.
%
Another group of methods, to which **Matterport3D** belongs, learns inverse dynamics from unlabeled videos by predicting latent actions given pairs of observations, and uses them for learning behaviors for video games and robot simulations**Jiang, "RoboTHOR: Robotic Operations in THOR"**, as pretraining for Vision-Language-Action models**Chen, "Text-to-Visual Action Transfer via Generative Adversarial Imitation Learning"** or for learning robot policies**Kovacsics, "Learning Robot Policies with Temporal Logic Specifications"**.
%

Latent action models have also been used for conditional video prediction.
%
The most similar method to ours is **CADDY**, which learns latent actions from a collection of unlabeled videos from a single domain and uses the latent actions as conditioning signal for predicting future frames.
%
At inference time, CADDY maps user inputs to the latent space for playable video generation.
%
Building upon this same principle, **Genie** proposes a foundation world model for playable video generation on diverse environments.
%
However, both CADDY and Genie operate on holistic scene representations, which are limited for tasks that require relational reasoning, often struggle to model object relationships and interactions, and require human supervision to generalize to scenes with multiple moving agents.