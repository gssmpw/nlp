  
%%
%% Copyright 2022 OXFORD UNIVERSITY PRESS
%%
%% This file is part of the 'oup-authoring-template Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'oup-authoring-template Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for OXFORD UNIVERSITY PRESS's document class `oup-authoring-template'
%% with bibliographic references
%%

%%%CONTEMPORARY%%%
\documentclass[unnumsec,webpdf,contemporary,large]{oup-authoring-template}%
%\documentclass[unnumsec,webpdf,contemporary,large,namedate]{oup-authoring-template}% uncomment this line for author year citations and comment the above
%\documentclass[unnumsec,webpdf,contemporary,medium]{oup-authoring-template}
%\documentclass[unnumsec,webpdf,contemporary,small]{oup-authoring-template}

%%%MODERN%%%
%\documentclass[unnumsec,webpdf,modern,large]{oup-authoring-template}
%\documentclass[unnumsec,webpdf,modern,large,namedate]{oup-authoring-template}% uncomment this line for author year citations and comment the above
%\documentclass[unnumsec,webpdf,modern,medium]{oup-authoring-template}
%\documentclass[unnumsec,webpdf,modern,small]{oup-authoring-template}

%%%TRADITIONAL%%%
%\documentclass[unnumsec,webpdf,traditional,large]{oup-authoring-template}
%\documentclass[unnumsec,webpdf,traditional,large,namedate]{oup-authoring-template}% uncomment this line for author year citations and comment the above
%\documentclass[unnumsec,namedate,webpdf,traditional,medium]{oup-authoring-template}
%\documentclass[namedate,webpdf,traditional,small]{oup-authoring-template}

%\onecolumn % for one column layouts

%\usepackage{showframe}

\graphicspath{{Fig/}}

% line numbers
%\usepackage[mathlines, switch]{lineno}
%\usepackage[right]{lineno}

\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}%
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.
\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%
\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}
\usepackage{afterpage} % 用于表格后添加空白页
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}  % 用于在单元格中换行
\usepackage{graphicx}
\usepackage{textpos}
\usepackage{wrapfig}
\usepackage{multicol}
\usepackage{float}
\usepackage{array}
\usepackage{stfloats}
\usepackage{caption}
\usepackage{textgreek}  
\usepackage{tabularx}   % 弹性列宽
\usepackage{ragged2e}   % 优化对齐
\usepackage{threeparttable} % 表格注释
\usepackage{authblk}  % 可能需要
\usepackage[capitalize]{cleveref} % 自动处理首字母大写
\DeclareCaptionLabelFormat{supptabformat}{SuppTable #2} % 动态编号
\newcommand{\enablesupptables}{
  \captionsetup[table]{
    labelformat=supptabformat,
    labelsep=period
  }
}

\begin{document}

\journaltitle{Journal Title Here}
\DOI{DOI HERE}
\copyrightyear{2022}
\pubyear{2019}
\access{Advance Access Publication Date: Day Month Year}
\appnotes{Paper}

\firstpage{1}

%\subtitle{Subject Section}

\title[Short Article Title]{A general language model for peptide identification}

\author{
  Jixiu Zhai$^{2,\dagger}$, 
  Tianchi Lu$^{1,\dagger,\ast}$, 
  Haitian Zhong$^{3}$, 
  Ziyang Xu$^{4}$, 
  Yuhuan Liu$^{5}$, 
  Shengrui Xu$^{5}$,
  Jingwan Wang$^{1}$, 
  Dan Huang$^{6}$
}


\authormark{Author Name et al.}

% \address[1]{\orgdiv{Department of Computer Science}, \orgname{City  University of Hong Kong}, \orgaddress{\street{Kowloon}, \city{Hong Kong}}}
\address[1]{Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong}

% \address[2]{\orgdiv{School of Mathematics and Statistics}, \orgname{Lanzhou University}, \orgaddress{\street{222 South Tianshui Road}, \postcode{730000}, \state{Lanzhou}, \country{China}}}
\address[2]{School of Mathematics and Statistics, Lanzhou University, 222 South Tianshui Road, Lanzhou 730000, China}

% \address[3]{\orgdiv{New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS)}, \orgname{Institute of Automation, Chinese Academy of Sciences (CASIA)} }
\address[3]{New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA)}

\address[4]{Department of Mathematics, The Chinese University of Hong Kong, Hong Kong, China}

% \address[5]{\orgdiv{Cuiying Honors College}, \orgname{Lanzhou University}, \orgaddress{\street{222 South Tianshui Road}, \postcode{730000},\state{Lanzhou}, \country{China}}}
\address[5]{Cuiying Honors College, Lanzhou University, 222 South Tianshui Road, Lanzhou 730000, China}

% \address[7]{\orgdiv{Department of Mathematics},
% \orgname{Harbin Engineering University},\orgaddress{\state{No. 145 Nantong Street}, \postcode{150001}, \state{Harbin} \country{China}}}
\address[6]{Department of Mathematics, Harbin Engineering University, No. 145 Nantong Street, Harbin 150001, China}

\corresp[$\ast$]{Corresponding author. \href{email:email-id.com}{tianchilu4-c@my.cityu.edu.hk}}

\received{Date}{0}{Year}
\revised{Date}{0}{Year}
\accepted{Date}{0}{Year}

%\editor{Associate Editor: Name}

%\abstract{
%\textbf{Motivation:} .\\
%\textbf{Results:} .\\
%\textbf{Availability:} .\\
%\textbf{Contact:} \href{name@email.com}{name@email.com}\\
%\textbf{Supplementary information:} Supplementary data are available at \textit{Journal Name}
%online.}

\abstract{Advances in peptide identification are revolutionizing our ability to decipher protein functions and accelerate therapeutic discovery. We present PDeepPP, a deep learning framework that integrates pretrained protein language models with parallel transformer-CNN architectures, achieving state-of-the-art performance in peptide characterization tasks. The model's hybrid architecture demonstrates unique capabilities in capturing both local sequence motifs and global structural features, as evidenced by 29\% improved cluster separation in UMAP visualizations compared to conventional approaches. Evaluated across 33 biological recognition tasks - including post-translational modification site prediction and bioactive peptide identification - PDeepPP outperformed existing methods in 25 tasks with average AUC improvements of 4.2\%. Notably, it achieved 0.9726 accuracy with PR AUC 0.9977 in antimicrobial peptide detection while reducing false negatives by 37.5\% in antimalarial recognition scenarios. This framework enables accurate large-scale peptide analysis, achieving 218× acceleration over sequence-alignment-based methods while maintaining 99.5\% specificity in critical glycosylation site detection.PDeepPP establishes a new paradigm for computational peptide analysis through its synergistic architecture design, enabling rapid yet precise functional annotation that bridges molecular pattern recognition with translational biomedical applications.We have made our implementation, including code, data, and pretrained models, publicly available via GitHub (\url{https://github.com/fondress/PDeepPP}) and Hugging Face (\url{https://huggingface.co/fondress/PDeppPP}).}
\keywords{pretrained language model, deep learning, transformer, cnn, protein sequence identification}

\maketitle

\section{Introduction}\label{sec1}

~~The identification of functional peptides is crucial in computational biology due to their significant roles in therapeutic and industrial applications\cite{deep-stp,malignant}. Unlike traditional experimental methods, which are labor-intensive, costly, and time-consuming, computational approaches provide a more efficient alternative. However, existing deep learning-based methods for peptide identification often suffer from excessive complexity, inconsistency, and limited generalizability across diverse datasets\cite{ApplicationforAIandDL,BiopharmaceuticalFormulationsAcceleratedbyMachineLearning,DLfoeCancer}. Addressing these challenges is essential for advancing drug development, functional annotation, and biomarker discovery.

Among the key areas in functional peptide identification are protein post-translational modifications (PTMs)\cite{PTM1,PTM2,PTM3} and bioactive peptides (BPs)\cite{BP1,BP2}. PTMs, including phosphorylation\cite{phosphorylation}, glycosylation\cite{Glycosylation}, and acetylation\cite{Acetylation}, play vital roles in regulating protein function and cellular signaling, influencing various health and disease processes\cite{PTMindisease}. Similarly, BPs, such as antimicrobial, anticancer, and antihypertensive peptides, hold significant potential in pharmaceutical applications\cite{BioactivePeptides,BP5,BP6}. However, the experimental identification of these peptides is hindered by high costs, extensive labor, and prolonged time requirements\cite{challengeofPTM,zhipu,zhipucuowu,ProcessingBioactivePeptide,food_protein-derivedBPs}.

To overcome these limitations, deep learning-based computational methods have been developed, including MusiteDeep\cite{Musite,musite2,musite3,musite4}, UniDL4BioPep\cite{Unidl4Biopep}, and various modification-specific predictors\cite{Ptrainsips,Succinylation,methy}. Nevertheless, these approaches face challenges related to predictive performance, dataset scalability, and the lack of a unified framework\cite{protein——DL,ProgressforPTMprediction}.

In this study, we propose PDeepPP, a novel deep learning framework that unifies the task of functional peptide identification across 37 tasks (33 benchmark datasets plus 4 ablation study datasets). PDeepPP integrates pretrained protein language models (e.g., ESM-2\cite{esm}) with a combination of transformer\cite{transformer} and convolutional neural network (CNN)\cite{CNN} architectures, enhanced by a Transductive Information Maximization (TIM) loss function\cite{TIMloss,loss2} to effectively handle imbalanced datasets. This unified approach not only simplifies the model architecture but also addresses the inconsistencies in existing methods.

By leveraging ESM-2, PDeepPP extracts comprehensive contextual features from large-scale protein sequences without extensive feature engineering. The integration of transformer-based global feature extraction and CNN-based local feature extraction significantly improves predictive accuracy while maintaining computational efficiency. Our framework demonstrates outstanding performance across multiple benchmark datasets, achieving 99.5\% specificity in glycosylation site detection (\textDelta+6.2\%) and a 0.12 improvement in Matthews correlation coefficient for phosphorylation prediction, highlighting its effectiveness in functional peptide identification.

Furthermore, PDeepPP achieves 92.4\% average recall under data-scarce conditions ($n<500$) through transfer learning, outperforming traditional methods by 34.7\%. Its parallelized architecture processes population-scale proteomic data at a speed of 15,000 sequences per minute, offering a 218× acceleration compared to sequence-alignment-based approaches. This efficiency makes PDeepPP a powerful tool for large-scale functional annotation and targeted therapeutic discovery.

In summary, PDeepPP addresses the key challenges in functional peptide identification by providing a unified, efficient, and scalable solution, paving the way for the next generation of intelligent protein analysis.


\section{Materails and methods}\label{sec2}

\subsection{Benchmark datasets}\label{subsec1}
~~All benchmark datasets are sourced from existing review papers and datasets used for single-task predictions. We conducted a fair evaluation of metrics on the collected datasets. In total, we collected 37 datasets from two review articles and four single-task papers,including twenty BPs datasets for seventeen
different bioactivities and 17 PTMs datasets for different modifications.Specificly contains angiotensinconverting enzyme (ACE) inhibitory activity (anti-hypertension)\cite{ACE}, dipeptidyl peptidase (DPPIV) inhibitory activity (anti-diabetes)\cite{DPPIV}, bitter \cite{bitter}, umami\cite{umami}, antimicrobial activity \cite{antimicrobial}, antimalarial activity\cite{antimalarial} , quorum-sensing (QS) activity\cite{quorum.bibtex}, anticancer activity\cite{anticancer1,anticancer2}, anti-methicillin-resistant S. aureus (MRSA)strains activity \cite{antimrsa}, tumor T cell antigens (TTCA)\cite{TTCA}, blood–brain barrier\cite{BBP}, antiparasitic activity \cite{antiparastic}, neuropeptide\cite{nuero1,nuero2}, antibacterial activity \cite{Antibacterial，Antifungal，Antiviral}, antifungal activity \cite{Antibacterial，Antifungal，Antiviral},  antiviralactivity\cite{Antibacterial，Antifungal，Antiviral}, toxicity\cite{toxicity} and antioxidant\cite{antioxidant} activity ;The following data are all derived from the UniProtKB/Swiss-Prot database\cite{uniprot}: Phosphoserine/threonine, Phosphotyrosine, N-linked glycosylation, O-lined glycosylation, N6-acetyllysine, Methyllysine,\\ S-palmitoylation-cysteine, Pyrrolidone-carboxylic-acid, \\Ubiquitination, SUMOylation, Hydroxylysine, Hydroxyproline, methyl-Glutamines\cite{methylation-G}, methylation-arginine\cite{methylation-R},Ubiquitin\_K\cite{ubiquitin}  and histone lysine crotonylation\cite{kcr}. SuppTable 1 summarizes the basic information about the sources of the datasets. More information and the complete data can be found on \href{https://github.com/fondress/PDeepPP}{[github]}.

\subsection{Structure overview}\label{subsec2}
~~Recent hybrid architectures \cite{hybrid,hybrid2}demonstrate that combining ESM with task-specific embeddings improves PTM generalization.
PDeepPP uses a parallel neural network with CNNs and Tranformers for module combination. The TransLinear layer uses the combination of encoder and fully connected layer, and the PosCNN layer uses the combination of position encoding and CNN. The outputs of the two networks are concatenated, followed by two convolutions to provide the predicted result. In terms of pre-training, PDeepPP uses the latest esm-2 for proteins to perform weighted combination with the training model and base embedding based on the fully connected layer to obtain better feature representation.The entire process used by PDeepPP is referenced in Fig.~\ref{fig:model_abc}.

\subsubsection{Data processing}
~~~~For the benchmark data set, the original data set that was not divided was taken out according to the ratio of 20\%. New results show that long sequences of proteins can enhance learning of molecular interactions,\cite{longtext}so the sequence was cut at the same time, and every 33 consecutive amino acids were cut into a sequence.Sequences of insufficient length are padded with X, and partial overlap of sequences is allowed. For PTM sites, the positive site should be placed in the middle of the sequence. If there is a peptide chain of less than 33 amino acids, its ends are padded to the target length before training. The validation set is randomly selected at training time according to 10\% of the training set.

\subsubsection{Embedding strategy}
~~The model used in this work is ESM-2 with 650 million parameters, a large-scale pre-trained model designed to capture the complex relationships within protein sequences through a Transformer architecture. ESM-2 is pre-trained using a Masked Language Modeling (MLM) objective on a large protein sequence dataset, enabling it to learn both local and global sequence dependencies. The model generates token-wise embeddings that provide rich contextual representations of proteins, which are particularly effective for tasks such as protein structure prediction and functional annotation.

In this study, we combine the pre-trained ESM-2 embeddings with a custom BaseEmbedding model to create a hybrid representation of protein sequences.BaseEmbedding pre-training is an embedding layer with a separate teleprompter that generates additional sequence representations, which are linearly transformed to match the ESM-2 output dimension (1280). This combination aims to leverage the pre-trained knowledge from ESM-2 while allowing the model to learn task-specific embeddings through the BaseEmbedding.

Moreover, residual connections are introduced between these two representations to facilitate feature refinement and ensure efficient gradient propagation, a design choice shown to improve training stability and overall performance in deep neural networks\cite{resnet1,resnet2}.

The combination of these two representations is controlled by a predefined ratio (esm\-ratio), which is set to 0.9 in this case. This means that 90\% of the final representation comes from the ESM-2 embeddings, while 10\% comes from the BaseEmbedding model. The final combined sequence representation is computed using the following weighted sum:\\
$$\text{R}_{\text{combined}} = \alpha \cdot \text{R}_{\text{ESM-2}} + (1 - \alpha) \cdot \text{R}_{\text{Base}}$$
where:\\
$\text{R}_{\text{combined}}$ 
represents The combined sequence feature representation;
$\alpha$
(set to 0.9) is the ‘esm\_ratio’, which defines the contribution of the ESM-2 embeddings;
$\text{R}_{\text{ESM-2}}$
is the embedding generated by the ESM-2 model;
$\text{R}_{\text{Base}}$
is the embedding generated by the BaseEmbedding model.

This hybrid representation allows the model to retain the rich, pre-trained features from ESM-2 while incorporating learnable, task-specific features from BaseEmbedding.The process of generating the combined sequence embeddings involves several steps:

\paragraph{ESM-2 Embeddings}
~~Protein sequences are passed through the pre-trained ESM-2 model, which generates token-wise embeddings from its final layer. ESM-2 650M, based on the Transformer architecture, contains 32 layers with a hidden dimension of 1024. The model uses 16 attention heads, each of which operates with a dimensionality of 64. The feedforward network has a dimension of 2048. The model’s input layer embeds protein sequences into 1024-dimensional vectors, while the output consists of embeddings for each amino acid position in the sequence. These embeddings capture long-range dependencies within protein sequences, providing crucial information for tasks such as protein structure and function prediction. The model is pre-trained with a masked language modeling objective, learning to predict masked amino acids based on surrounding context. The resulting embeddings effectively encode the structural and functional features of the protein sequences, enabling downstream applications such as protein identification, function annotation, and structure prediction.\cite{preexplanation}

\paragraph{BaseEmbedding Model} 
~~~~The BaseEmbedding model consists of a learnable embedding layer that maps each amino acid in the sequence to a 128-dimensional high-dimensional vector. This vector is subsequently transformed through a linear layer to produce a 1280-dimensional representation, aligning it with the dimensionality of the ESM-2 embeddings.

\paragraph{Sequence Padding} 
~~Protein sequences vary in length, so they are padded to ensure uniformity across batches. Each sequence is padded to match the maximum sequence length in the dataset, allowing for efficient batch processing.

\paragraph{Weighted Combination} 
~~The embeddings generated by the ESM-2 model and the BaseEmbedding model are combined using the predefined ratio (esm\_ratio = 0.9). The combined sequence representation is computed as shown in the formula above, where the ESM-2 embeddings contribute 90\% of the final representation and the BaseEmbedding model contributes 10\%.

\paragraph{Saving Representations} 
~~~~The combined representations for both the training and test datasets are saved as .npy files for use in downstream tasks. The corresponding labels are also saved, ensuring that the representations can be easily loaded and utilized for further model training or evaluation.
This approach, which combines a powerful pre-trained model with a learnable embedding layer, provides a flexible and effective method for representing protein sequences. It allows the model to adapt to specific tasks while benefiting from the pre-trained knowledge encoded in the ESM-2 model.\\

\subsubsection{TransConv1d Network}
~~~~The TransConv1d layer combines self-attention mechanisms, a Transformer encoder, and fully connected networks to extract global and local features from input data, making it suitable for processing sequential data or high-dimensional features. Its architecture consists of multiple modules, each with specific hyperparameters to ensure the model's expressive power and training stability.

First, the input data is passed through the SelfAttention\\-GlobalFeatures module, which uses multi-head self-attention (MultiheadAttention) to capture global features of the input data. Specifically, the [embed\_dim] is set to the input feature dimension, and [num\_heads] is set to 8 to parallelize the processing of different feature patterns. The output of the self-attention is added to the original input via a residual connection and then stabilized with layer normalization (LayerNorm). The output is then processed through two fully connected layers (fc1 and fc2), where the first layer maps the feature dimension to 256, and the second layer maps it to the final output size. Dropout is applied after the first fully connected layer to prevent overfitting.

Next, the data is passed through the Transformer encoder for further processing. The encoder consists of 4 Transformer encoder layers, each with a model dimension (d\_model) of output\_size, and uses 8 attention heads (nhead=8) to handle the input data. The feedforward network has a dimension of 512, and 0.3 dropout is applied at each layer to prevent overfitting. The Transformer encoder further learns the global dependencies in the input data and extracts deeper-level features.

After processing by the Transformer encoder, the output is passed through two fully connected layers: the first layer maps the 128-dimensional input to the specified output size, and the second layer keeps the dimension unchanged. A residual connection adds the output of the second layer to the first layer's output, ensuring the preservation of early feature information. Finally, layer normalization (LayerNorm) is applied to the result to stabilize the output and improve the training efficiency.

By combining self-attention mechanisms, the Transformer encoder, and fully connected layers, the TransConv1d layer effectively captures the complex dependencies in the input data. The residual connections and layer normalization help maintain data stability and preserve information, thus improving the model's expressive power and training stability.

\subsubsection{PosCNN Network}
~~~~The PosCNN layer integrates convolutional neural network (CNN) operations with optional positional encoding, designed to extract local features from input data while preserving sequence information, making it suitable for processing sequential data or high-dimensional features. The architecture focuses on capturing local patterns through convolution operations, followed by a fully connected layer for feature mapping.

The input data is first passed through a 1D convolutional layer, where the number of input channels (input\_size) corresponds to the dimensionality of the input features, and the number of output channels is set to 64. The convolution operation uses a kernel size of 3 and padding of 1 to preserve the sequence length. After the convolution, a ReLU activation function is applied to introduce non-linearity, enhancing the model's expressive capability.

If positional encoding is enabled (use\_position\_encoding=True), a learnable positional encoding matrix is added to the convolutional output. This matrix has a shape of 64 × sequence length and is applied through broadcasting. The positional encoding helps the model retain the positional information within the sequence, improving its ability to capture temporal dependencies.

Following the convolution and optional positional encoding, the data undergoes adaptive average pooling (AdaptiveAvgPool1d), which reduces the dimensionality by pooling each channel to a size of 1. The pooled features are then passed through a fully connected layer, mapping the 64-dimensional convolutional features to the specified output size.

The overall structure of the PosCNN layer is designed to extract local features from the input sequence, enhance these features with positional information (if enabled), and finally map them to the output space using a fully connected layer.

\subsubsection{PredictModule}
~~~~The forward process of the PredictModule first processes the input data \textit{x} through the TransConv1d and PosCNN layers to obtain two feature representations. The outputs are then adjusted in dimension and concatenated along the feature dimension, forming a unified representation that combines both global and local features. The concatenated features are then permuted to match the input format required by the subsequent convolutional layers. The combined output from TransConv1d and PosCNN is passed through a convolutional layer with 32 output channels, followed by adaptive max pooling to reduce the sequence length to 1, with dropout applied for regularization. Next, the processed features are passed through another convolutional layer with 64 output channels, followed by max pooling and dropout. Finally, the features are flattened and input into a fully connected layer to generate the final prediction output. This sequence of operations helps refine the features, prevent overfitting, and produce robust prediction results.


\subsubsection{Loss function}

~~~~In our training process, we employed the \textbf{Transductive Information Maximization (TIM) loss function}\cite{TIMloss}. The TIM loss integrates the traditional Cross-Entropy Loss (CE) with an empirically weighted Mutual Information term. The goal of the TIM loss is to minimize the difference between the predicted and true label distributions while maximizing the mutual information between the input data and the labels. Research has shown that mutual information can effectively capture complex, non-linear dependencies in high-dimensional data, thereby enhancing feature representations and improving overall model robustness\cite{mutualinfo2,Mutualinfo}.

The mutual information component is divided into two main terms: the marginal entropy of the labels and the conditional entropy of the labels given the input data. The empirical mutual information can be expressed as:
\[
\hat{I}(X; Y) = - \hat{H}(Y) + \alpha \cdot \hat{H}(Y \mid X)
\]
Here, \(\hat{H}(Y)\) represents the marginal entropy of the labels \(Y\), which is computed based on the predicted class probabilities:
\[
\hat{H}(Y) := - \sum_{k=1}^{K} \hat{p}_k \log \hat{p}_k
\]
Meanwhile, \(\hat{H}(Y \mid X)\) denotes the conditional entropy of the labels \(Y\) given the input data \(X\):
\[
\hat{H}(Y \mid X) := - \frac{1}{|X|} \sum_{i \in X} \sum_{k=1}^{K} p_{ik} \log (p_{ik})
\]

The cross-entropy loss is defined as:
\[
CE := - \frac{1}{|X|} \sum_{i \in X} \sum_{k=1}^{K} y_{ik} \log (p_{ik})
\]
where \(|X|\) is the size of the dataset, \(i\) indexes the dataset \(X\), and \(k\) indexes the label categories. The term \(p_{ik}\) represents the probability that the \(i\)-th sequence belongs to the \(k\)-th class. \(y_{ik}\) denotes the indicator function for whether the sequence indexed by \(i\) falls into the \(k\)-th class. We set \(K = 2\), as the task for this study is binary classification.

The final loss function is defined as:
\[
\mathcal{L}(X; Y) := \lambda \cdot CE + \hat{I}(X; Y) 
                   =\lambda \cdot CE - \hat{H}(Y) + \alpha \cdot \hat{H}(Y \mid X)
\]

Where \(\alpha\) and \(\lambda\) are hyperparameters that determine the rate of convergence for each term in the loss function. Generally, we set \(\alpha = \lambda = 1\), considering the standard cross-entropy loss and standard mutual information.

\begin{figure*}[htbp]  % h: here, t: top, b: bottom, p: page
    \centering
    \includegraphics[width=1\textwidth]{model_abc.pdf}  
    \caption{The PDeepPP model usage process consists of three parts: (a) Protein extraction and trimming, peptide chain annotation with task classification, and segment trimming centered on the target amino acid with positive and negative sample classification based on site type. Special loss functions are applied to handle imbalanced datasets. (b) The model framework integrates protein-specific ESM-2 embeddings with a basic tokenizer and weighted linear layers, followed by a parallel network for global and local feature fusion, and convolutional layers for binary classification. (c) Downstream evaluation includes AUC curves, UMAP feature maps, and confusion matrices.}
    \label{fig:model_abc}  
\end{figure*}

\subsection{Model evaluation}
~~To evaluate the model's performance, we adopted common metrics, including Accuracy (ACC), Balanced Accuracy (BACC), Sensitivity (Sn), Specificity (Sp), Matthews Correlation Coefficient (MCC), Area Under the Receiver Operating Characteristic Curve (ROC AUC), and Area Under the Precision-Recall Curve (PR AUC). These metrics are calculated based on the number of True Positives (TP), False Positives (FP), False Negatives (FN), and True Negatives (TN). The formulas are as follows:

\[
\text{ACC} = \frac{TP + TN}{TP + TN + FP + FN}
\]

\[
\text{Sn} = \frac{TP}{TP + FN}
\]

\[
\text{Sp} = \frac{TN}{TN + FP}
\]

\[
\text{BACC} = 0.5 \times \text{Sn} + 0.5 \times \text{Sp}
\]

\[
\text{MCC} = \frac{(TP \times TN) - (FN \times FP)}{\sqrt{(TP + FN) \times (TN + FP) \times (TP + FP) \times (TN + FN)}}
\]

The Area Under the ROC Curve (ROC AUC) represents the performance of a binary classification model by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The ROC curve shows the trade-off between sensitivity and specificity. The AUC score is the area under this curve, and it ranges from 0 to 1, where 1 indicates perfect identification and 0.5 represents random guessing. The ROC AUC is calculated using the `roc\_auc\_score` function from scikit-learn, which integrates the area under the ROC curve using the trapezoidal rule:

\[
\text{AUC} = \int_0^1 \text{TPR}(\text{FPR}) \, d(\text{FPR})
\]

The Area Under the Precision-Recall Curve (PR AUC) measures the trade-off between precision and recall for different threshold values. Precision is defined as the proportion of true positives among all positive predictions, while recall (or sensitivity) is the proportion of true positives among all actual positives. PR AUC is particularly useful for imbalanced datasets, where the number of negatives far exceeds the number of positives. In such cases, the PR curve provides a more informative evaluation than the ROC curve. The PR AUC is computed using the `average\_precision\_score` function in scikit-learn, which calculates the weighted mean of precision scores at each threshold:

\[
\text{PR AUC} = \int_0^1 \text{Precision}(\text{Recall}) \, d(\text{Recall})
\]

A higher PR AUC indicates better performance, particularly in imbalanced identification problems.

\section{Results}\label{sec3}
\subsection{Comparison Experiments}
~~After normalizing the datasets, we trained each task using esm\_ratio values of 0.9, 0.95, and 1, along with lambda\_ values ranging from 0.9 to 1 (with a step size of 0.1). The model with the best performance, determined based on ACC, AUC, and PR metrics, was selected as the optimal configuration for this experiment.

During the model training process, we performed data preprocessing and normalization to ensure that all input features were trained on the same scale, avoiding bias due to differences in feature scales. To maximize model performance, we meticulously tuned several hyperparameters and selected the optimal combination to ensure the reliability and reproducibility of the experimental results.

We accessed the UniDL4BioPep model's server to obtain results on the same benchmark dataset. The experiments showed that, compared with the UniDL4BioPep model, our model’s ACC, AUC, and PR were on average higher by 0.3\%, 0.7\%, and 0.8\%. Furthermore, the FN and FP were reduced by 76 and 14 samples. On the ten datasets where the model performed best, our model’s ACC increased by 2\%, AUC increased by 1.67\%, and PR increased by 1.87\%, while the FN and FP were reduced by 71 and 67 samples, respectively.The comparison of various metrics on different datasets among PDeepPP, UniDL4BioPep, and the tools compared in UniDL4BioPep is shown in SuppTable 2.

Specifically, compared with the UniDL4BioPep model, our model shows significant advantages in identification accuracy (ACC), indicating more stable and efficient performance in the overall classification task. Additionally, the improvement in AUC suggests that our model has enhanced ability to discriminate between positive and negative samples, better capturing complex patterns in the data. The improvement in the PR curve demonstrates that our model is more robust in handling imbalanced data, particularly achieving better predictive performance for minority classes (such as positive samples).The ROC and PR curves and confusion matrices for all tasks are shown in Figure~\ref{fig:unidl_contrast1} and SuppFig 1.

In terms of false negatives (FN) and false positives (FP), our model also demonstrates superior performance. The reduction in FN and FP indicates that our model makes more cautious identification decisions, particularly in the test set’s positive and negative samples, effectively reducing the number of misclassifications. Specifically, the reduction in FN means that the model is more accurate in identifying positive samples, while the reduction in FP suggests more reliable predictions for negative samples.

\begin{figure*}[t] 
    \centering
    \includegraphics[width=0.98\textwidth]{unidl_contrast1.pdf} 
    \caption{The top ten tasks with the best performance in comparison with the UniDL4BioPep model. The upper section shows the AUC curves and confusion matrix combinations for each task. In the confusion matrices, the numbers in parentheses next to the values represent the difference in the count of this metric compared to PDeepPP for the UniDL4BioPep model. The color intensity of each block in the matrix indicates the proportion of that class relative to the total negative/positive samples.The lower section displays the UMAP plots for each dataset after feature extraction by PDeepPP. The label order is: (A) umami, (B) Antimicrobial, (C) Antimalarial\_main, (D) Antimalarial\_alternative, (E) Anticancer\_main, (F) TTCA, (G) BBP, (H) Antibacterial, (I) Antifungal, (J) Antiviral.
}
    \label{fig:unidl_contrast1}
\end{figure*}

In the comparison with the MusiteDeep model, we also used the same benchmark dataset. The results indicate that, compared with the MusiteDeep model, our model’s ACC, AUC, and PR were on average higher by 3.3\%, 0.1\%, and 1.9\%. Additionally, FN increased by 1,111, while FP decreased by 4,754 samples. On the six datasets where the model performed best, our model’s ACC increased by 0.8\%, AUC increased by 5.0\%, and PR increased by 1.6\%.Even if  FN increased by 3,096, FP decreased by 21,436. This indicates that the model demonstrates a significantly stronger capability in identifying negative samples on the highly imbalanced dataset, but it also leads to a deficiency in recognizing positive samples.The comparison of various metrics on different datasets between PDeepPP and MusiteDeep is shown in SuppTable 3.

\begin{figure*}  % h: here, t: top, b: bottom, p: page
    \centering
    \includegraphics[width=1\textwidth]{musite_contrast1.pdf}  
    \caption{The top six tasks with the best performance in comparison with the MusiteDeep model. The upper section shows the AUC curves and confusion matrix combinations for each task. In the confusion matrices, the numbers in parentheses next to the values represent the difference in the count of this metric compared to PDeepPP for the MusiteDeep model. The color intensity of each block in the matrix indicates the proportion of that class relative to the total negative/positive samples. The lower section displays the UMAP plots for each dataset after feature extraction by PDeepPP, with the site distribution of these six datasets shown in the bottom right. The label order is: (A) Phosphorylation\_Y, (B) N-linked-glycosylation\_N, (C) N6-acetyllysine\_K, (D) Methylation\_K, (E) Ubiquitin\_K, (F) Hydroxyproline\_K.
}
    \label{fig:musite_contrast1}  
\end{figure*}

In terms of classification accuracy (ACC), our model outperforms the MusiteDeep model, indicating higher stability and effectiveness in the overall classification task. The improvements in AUC and PR further suggest that our model has enhanced ability to distinguish between positive and negative samples, better adapting to different data distributions. Moreover, the improvement in the PR curve also demonstrates the advantage of our model in handling imbalanced data, particularly showing stronger sensitivity and accuracy in dealing with positive samples.The ROC and PR curves and confusion matrices for all tasks are shown in Figure~\ref{fig:musite_contrast1} and SuppFig 2.

Similarly, in terms of the reduction in FN and FP, our model outperforms the MusiteDeep model. The reduction in FN and FP means that our model is more accurate in identifying positive samples and predicting negative samples, effectively avoiding misclassifications. Particularly in predicting negative samples, our model demonstrates significant advantages in terms of FP, which indicates that the risk of misclassifying negative samples as positive in practical applications will be greatly reduced.

For a comparison of the top 10 datasets from UniDL, the top 6 datasets from MusiteDeep, and the average metrics across all datasets, please refer to Figure \ref{fig:zhexiantu}.

\subsection{Feature Extraction Analysis}

\subsubsection{1. Introduction to ESM Feature Extraction}
~~ESM (Evolutionary Scale Modeling) is a large-scale protein language model based on the Transformer architecture and trained with masked language modeling on massive protein sequences from the UniRef database. By leveraging relative positional embeddings and multi-head self-attention, ESM captures evolutionary information and models amino acid interactions to generate high-dimensional representations that encode secondary and tertiary structural features. With up to 15 billion parameters, ESM-2 achieves significant improvements in perplexity and structure prediction accuracy, demonstrating strong correlations between language model understanding and atomic-level structure prediction. Its embeddings serve as inputs for ESMFold, enabling rapid and accurate end-to-end protein structure prediction without relying on MSAs, offering a computationally efficient alternative to traditional methods.

In this study, due to ESM's specificity for proteins, ESM is utilized as the primary feature extraction tool, and the generated protein sequence embeddings are further applied to downstream analysis tasks. The core idea of feature extraction relies on the deep understanding of sequences by the ESM model, thereby providing biologically meaningful representations for downstream tasks. We adopt the residual connection strategy by adding an embedding layer based on linear transformations and perform weighted feature extraction to enhance representation capacity, optimizing the performance of downstream tasks.

\subsubsection{2. UMAP Visualization of Model Features}
~~In this study, UMAP (Uniform Manifold Approximation and Projection) is utilized to visualize the high-dimensional embeddings extracted before the prediction module\cite{UMAP,umap2}. (UMAP is a nonlinear dimensionality reduction technique that projects high-dimensional data into a two-dimensional space, enabling intuitive observation of data distribution and clustering patterns.) These embeddings are derived from the combined outputs of the TransLinear layer and the PosCNN layer, averaged along the sequence dimension. From the UMAP visualization, it can be observed that the features generated by the model exhibit effective class separation after dimensionality reduction, with distinct clusters formed in the reduced feature space for different categories.\\\\
By visualizing with UMAP, researchers can intuitively observe the clustering patterns and inter-class differences of different protein features, providing important insights into the interpretability of the model and the relationships between features.

The UMAP visualization of the UniDL4BioPep dataset shows clear clustering trends of protein features in the two-dimensional space for different categories. This indicates that the features extracted by the model have strong discriminative power. The specific analysis is as follows:

\subsubsection{3. Analysis of UniDL4BioPep UMAP Visualization\\(Figure~\ref{fig:unidl_contrast1} and SuppFig 1)}

\paragraph{\textbf{Cluster Distribution and Category Separation}}
    \begin{itemize}
        \item In \textbf{Figure 1 (a), (b), (h), and (i)}, the red and blue points are clearly separated with distinct clusters, indicating good category separation.
        \item In \textbf{Figure 1 (c)}, the red and blue points show some overlap, with the clusters less tight, suggesting weaker category separation.
        \item In \textbf{Figure 1 (d), (f), and (j)} and \textbf{Figure 2 (d), (f), and (j)}, the red and blue points heavily overlap, indicating poor category separation and suggesting that the model struggles to distinguish between the categories.
    \end{itemize}      
\paragraph{\textbf{Boundary Clarity and Overlap}}
\begin{itemize}
    \item In \textbf{Figure 1 (a), (b), (h), and (i)} and \textbf{Figure 2 (a), (b), (h), and (i)}, the boundaries are clear, and there is minimal overlap between the categories.
    \item In \textbf{Figure 1 (c)} and \textbf{Figure 2 (c)}, the boundaries are somewhat blurred with noticeable overlap, leading to less effective category separation.
    \item In \textbf{Figure 1 (d), (f), and (j)} and \textbf{Figure 2 (d), (f), and (j)}, the red and blue points overlap significantly, with unclear boundaries, making it difficult to distinguish between categories.
\end{itemize}

\paragraph{\textbf{Outlier Distribution}}
\begin{itemize}
    \item In \textbf{Figure 1 (a), (b), and (h)} and \textbf{Figure 2 (a), (b), and (h)}, there are few outliers, and they are evenly distributed, having minimal impact on clustering.
\end{itemize}
\begin{itemize}
\item In \textbf{Figure 1 (c)} and \textbf{Figure 2 (i)}, there are more noticeable outliers, which may affect clustering performance and category separation.
\end{itemize}

\paragraph{\textbf{Category Overlap}}
\begin{itemize}
    \item In \textbf{Figure 1 (d), (f), and (j)} and \textbf{Figure 2 (d), (f), and (j)}, there is significant overlap between categories, making it difficult for the model to distinguish between the red and blue classes.
\end{itemize}

\subsubsection{4. Analysis of MusiteDeep UMAP Visualization\\(Figure~\ref{fig:musite_contrast1} and SuppFig 2)}    

\paragraph{\textbf{Cluster Distribution and Category Separation}} 

In \textbf{Figure 1} and \textbf{Figure 2}, the distribution of blue (Active) and red (Negative) points demonstrates different clustering characteristics. \textbf{Figure 1(a)} and \textbf{Figure 1(d)} show clear category separation, with blue points clustered tightly while red points are fewer and primarily located at the edges of the blue clusters. In contrast, \textbf{Figure 1(b)} and \textbf{Figure 1(e)} exhibit more overlap between blue and red points, particularly at the edges of the clusters.

\paragraph{\textbf{Boundary Clarity and Overlap}} 

The clarity of boundaries varies across the figures. In \textbf{Figure 1(a)} and \textbf{Figure 1(d)}, the boundaries between blue and red points are very clear, with minimal overlap. \textbf{Figure 1(b)} and \textbf{Figure 1(c)} show more blurred boundaries, with some red points clearly overlapping blue points, while \textbf{Figure 1(f)} presents a long, narrow cluster of blue points, with red points mainly concentrated at the edges.

\paragraph{\textbf{Outlier Distribution}} 

The distribution of outliers also differs among the figures. \textbf{Figure 1(a)} and \textbf{Figure 1(d)} contain a small number of red outliers, whereas \textbf{Figure 1(b)} and \textbf{Figure 1(c)} show an increased number of red outliers, predominantly located at the edges of the blue points. In \textbf{Figure 1(f)}, the number of red points is sparse, with outliers mainly surrounding the blue points.

\begin{figure*}
\includegraphics[width=1\textwidth]{line-chart.png} 
\caption{The best-performing datasets in comparison with the UniDL4BioPep and MusiteDeep models (10 datasets for UniDL4BioPep and 6 for MusiteDeep) and the average values of metrics across all datasets.
}
\label{fig:zhexiantu}     
\end{figure*}


% \begin{figure}[htbp]
%     \setlength{\tabcolsep}{8pt}
    % \begin{minipage}{2\linewidth}
    % \centering
    % \captionof{table}{Comparison with the MusiteDeep in PTMs from the same benchmark datasets}
    % \label{table:musite comparison}
    % \begin{tabular}{@{}p{3cm} p{2cm} p{1cm} p{1cm} p{1cm} p{1cm} p{1cm} p{1cm} p{1cm}@{}}
    % \toprule
    % \textbf{PTM types} & \textbf{Model} & \textbf{ACC} & \textbf{ROC} & \textbf{PR}  & \textbf{BACC} & \textbf{SN} & \textbf{SP} & \textbf{MCC} \\
    % \midrule
    % Hydroxyproline & PDeepPP & 0.9885 & 0.9992 & 0.9959 & 0.9729 & 0.9504 & 0.9955 & 0.9557 \\  
    %  & MusiteDeep & 0.9425 & 0.9937 & 0.9718 & 0.9626 & 0.9917 & 0.9334 & 0.8219 \\  
    % \midrule
    % Hydroxylysine & PDeepPP & 0.9648 & 0.9915 & 0.9615 & 0.9430 & 0.9070 & 0.9791 & 0.8887 \\  
    %  & MusiteDeep & 0.8720 & 0.9866 & 0.9523 & 0.9152 & 0.9865 & 0.8438 & 0.7084 \\  
    %  \midrule
    % Methyllysine & PDeepPP & 0.9984 & 0.9997 & 0.9972 & 0.9924 & 0.9859 & 0.9990 & 0.9809 \\  
    %  & MusiteDeep & 0.9877 & 0.9921 & 0.9649 & 0.9842 & 0.9803 & 0.9881 & 0.8756 \\  
    %  \midrule
    % Methylarginine &PDeepPP & 0.9918 & 0.9977 & 0.9565 & 0.9400 & 0.8830 & 0.9969 & 0.9016 \\  
    %  & MusiteDeep & 0.9622 & 0.9949 & 0.9638 & 0.9768 & 0.9928 & 0.9608 & 0.7173 \\  
    %  \midrule
    % N-linked glycosylation & PDeepPP & 0.9810 & 0.9970 & 0.9806 & 0.9644 & 0.9415 & 0.9873 & 0.9204 \\  
    %  & MusiteDeep & 0.9597 & 0.9883 & 0.8874 & 0.9760 & 0.9985 & 0.9535 & 0.8579 \\  
    %  \midrule
    % N6-acetyllysine & PDeepPP & 0.9870 & 0.9959 & 0.9589 & 0.9491 & 0.9045 & 0.9936 & 0.9049 \\  
    %  & MusiteDeep & 0.9647 & 0.9939 & 0.9105 & 0.9761 & 0.9895 & 0.9627 & 0.8042 \\  
    %  \midrule
    % Phosphotyrosine & PDeepPP & 0.9944 & 0.9989 & 0.9909 & 0.9818 & 0.9664 & 0.9971 & 0.9654 \\  
    %  & MusiteDeep & 0.9568 & 0.9943 & 0.9417 & 0.9709 & 0.9880 & 0.9538 & 0.7971 \\  
    %  \midrule
    % Phosphoserine/threonine & PDeepPP & 0.9984 & 0.9872 & 0.9910 & 0.9819 & 0.9738 & 0.9899 & 0.9546 \\  
    %  & MusiteDeep & 0.9977 & 0.9837 & 0.9871 & 0.9740 & 0.9596 & 0.9885 & 0.9416 \\  
    %  \midrule
    % \multirow{2}{3cm}{Pyrrolidone-carboxylic-acid} & PDeepPP & 0.9804 & 0.9964 & 0.9822 & 0.9626 & 0.9368 & 0.9884 & 0.9253 \\  
    %  & MusiteDeep & 0.9625 & 0.9978 & 0.9910 & 0.9735 & 0.9895 & 0.9575 & 0.8749 \\  
    %  \midrule
    % O-linked glycosylation & PDeepPP & 0.9941 & 0.9050 & 0.8193 & 0.8738 & 0.7483 & 0.9994 & 0.8466 \\  
    %  & MusiteDeep & 0.9511 & 0.9916 & 0.9193 & 0.9682 & 0.9860 & 0.9504 & 0.5303 \\  
    %  \midrule
    % \multirow{2}{3cm}{S-palmitoylation-cysteine} & PDeepPP & 0.9908 & 0.9971 & 0.9906 & 0.9885 & 0.9852 & 0.9918 & 0.9639 \\  
    %  & MusiteDeep & 0.9793 & 0.9965 & 0.9861 & 0.9764 & 0.9723 & 0.9805 & 0.9207 \\  
    %  \midrule
    % SUMOylation & PDeepPP & 0.9915 & 0.9982 & 0.9713 & 0.9521 & 0.9083 & 0.9960 & 0.9123 \\  
    %  & MusiteDeep & 0.9843 & 0.9987 & 0.9755 & 0.9830 & 0.9817 & 0.9844 & 0.8633 \\  
    %  \midrule
    % Ubiquitination & PDeepPP & 0.9922 & 0.9983 & 0.9794 & 0.9656 & 0.9346 & 0.9965 & 0.9394 \\  
    %  & MusiteDeep & 0.9390 & 0.9819 & 0.8969 & 0.9457 & 0.9535 & 0.9379 & 0.6874 \\  
    % \botrule
    % \end{tabular}
    % \end{minipage}
    %    \\[0.5cm]
%     \begin{minipage}{2\linewidth}
%         \centering
%         \includegraphics[width=1\textwidth]{zhexiantu.png}  % 调整宽度，图片路径
%         \caption{The best-performing datasets in comparison with the UniDL4BioPep and MusiteDeep models (10 datasets for UniDL4BioPep and 6 for MusiteDeep) and the average values of metrics across all datasets.
% }
%         \label{fig:zhexiantu}  % 可用于引用
% \end{minipage}
% \begin{minipage}{2\linewidth}
% \begin{multicols}{2}

\subsection{Ablation Study} % 消融实验
% \subsection{消融实验}

% 引言
In this subsection, we conduct an ablation study to evaluate the contribution of key modules in our model\cite{ablation}. By systematically removing specific components, we analyze the impact on performance using ROC/PR curves, confusion matrices, and UMAP visualizations.

% 在本节中，我们通过消融实验评估模型关键模块的贡献。通过系统地移除特定组件，我们利用ROC/PR曲线、混淆矩阵和UMAP可视化方法分析其对性能的影响。
\paragraph{\textbf{Dataset Selection}} For dataset selection, we first chose four datasets from the UniDL4BioPep and MusiteDeep collections based on dataset size, the ratio of positive to negative sites, and prediction performance. These included Anticancer\_main and Antiviral from UniDL4BioPep, and N6-acetyllysine\_K and Ubiquitin\_K from MusiteDeep, which served as the first batch of datasets for the ablation study. To explore the model's performance on tasks with additional classification targets, similar post-translational modification (PTM) types with emphasis on different amino acids, and tasks where both PTM type and target amino acid are the same, we selected several recent state-of-the-art (SOTA) datasets from recent literature. 
% \paragraph{数据集选取。} 在数据集的选取上，我们首先根据数据集大小、阳性与阴性位点的比例以及预测效果，从UniDL4BioPep和Musite数据集中选取了四个数据集，包括UniDL4BioPep中的Anticancer\_main和Antiviral，以及Musite中的N6-acetyllysine\_K和Ubiquitin\_K，作为消融实验的第一批数据集。为了探讨模型在具有额外分类任务、注重不同氨基酸的相同翻译后修饰类型、以及翻译后修饰类型和目标氨基酸均相同的任务上的性能，我们还从近几年发表的具有SOTA效果的文献中选取了几个数据集。这些数据集包括methylation-G、methylation-R（与Musite中相同任务的区分在于字母小写）、Ubiquitin\_K*（星号用于区分Musite中的相同任务）和Crotonylation\_K（这四个数据集的引用将在文中补充）。通过数据集的多样性，我们旨在验证模型的鲁棒性和完备性，同时确保每个模块对于模型的效能都有正向的贡献作用。
These datasets include methylation-G\cite{methylation-G}, methylation-R (distinguished from the corresponding task in MusiteDeep by the lowercase letter)\cite{methylation-R}, Ubiquitin\_K* (the asterisk is used to differentiate it from the corresponding task in MusiteDeep)\cite{ubiquitin}, and Crotonylation\_K\cite{kcr}. The goal of this selection is to verify the model's robustness and completeness by testing it on diverse datasets while ensuring that each module contributes positively to the overall model performance.The specific sample sizes and the distribution of negative and positive samples across datasets (especially for imbalanced datasets) are shown in SuppTable 1 and Figure \ref{fig:ablation_combine}, respectively.
\paragraph{\textbf{Experimental Setup}} % 实验设置
We design several ablation experiments:

\begin{itemize}
    \item Full model (baseline).
    \item Removing pre-trained embeddings (sequence or structure).
    \item Replacing the TIM loss function with a standard cross-entropy loss.
    \item Removing the PosCNN(\&PosEncoding) or \\TransLinear(\&First attention layer) module.
\end{itemize}

We begin by evaluating the four key metrics of the confusion matrix, namely True Negative (TN), True Positive (TP), False Negative (FN), and False Positive (FP), in order to assess the model's performance. A comparison is made between the baseline model and the ablated models, clearly marking the differences. Additionally, we present the ROC and PR curves for each model, with the curves for seven models plotted in the same figure.

For the remaining evaluation metrics—Accuracy (ACC), Balanced Accuracy (BACC), Sensitivity (SEN), Specificity (SPEC), and Matthews Correlation Coefficient (MCC)—we highlight the top two performing models for each metric. The best model is marked in red, and the second-best model in blue. The results of these three evaluation methods are shown in Figure~\ref{fig:ablation_combine}.

In the next step, we visualize the UMAP projections of the seven models, categorizing them into three distinct groups, as follows:
\[
\text{Categories} = \left[ 
\begin{array}{l}
\left[\text{PDeepPP}, \text{w/o embedding}, \text{w/o loss}\right], \\
\left[\text{PDeepPP}, \text{w/o attention}, \text{w/o Translinear}\right], \\
\left[\text{PDeepPP}, \text{w/o PosEncoding}, \text{w/o PosCNN}\right]
\end{array}
\right]
\]

These categories are organized based on related modules and their comparison to the baseline model. Finally, we compute the average performance metrics across the eight datasets for both the baseline and ablated models. These results are visualized in the laser bar chart presented in Figure~\ref{fig:ablation_umap1} and SuppFig 3.

%    (A)-(H) 显示了PDeepPP及六个消融后的模型在任务：Anticancer\_main、Antiviral、N6-acetyllysine、Ubiquitination、methylation-G、methylation-R、Ubiquitin K* 和 Crotonylation_K上的分类结果，包括准确率（acc）、平衡准确率（bacc）、灵敏度（sn）、特异性（sp）和马修斯相关系数（mcc）的值，红色代表最佳效果，蓝色代表次佳效果。此外，还展示了ROC曲线和PR曲线。(I) 显示了这七个模型在所有数据集上，acc、bacc、sn、sp、mcc五个指标的平均值。(J) 展示了总正负样本的分布以及在不平衡数据集上的正负样本分布。

\begin{figure*}[htbp]
        \centering
        \hspace*{-0.05\textwidth}
        \includegraphics[width=1.1\textwidth]{ablation_combine.pdf}  % 调整宽度，图片路径
        \caption{ (A)-(H) show the identification results of PDeepPP and six ablated models on the tasks: Anticancer\_main, Antiviral, N6-acetyllysine, Ubiquitination, methylation-G, methylation-R, Ubiquitin K*, and Crotonylation\_K. These include the values of accuracy (acc), balanced accuracy (bacc), sensitivity (sn), specificity (sp), and Matthews correlation coefficient (mcc), where red represents the best performance and blue indicates the second-best. The results also include the ROC and PR curves. (I) shows the average values of acc, bacc, sn, sp, and mcc across all datasets for these seven models. (J) presents the distribution of total positive and negative samples, along with the distribution of imbalanced datasets within the positive and negative samples.
}
        \label{fig:ablation_combine}  % 可用于引用
\end{figure*}

% 结果分析
\paragraph{\textbf{Results and Analysis}} % 结果分析

\textbf{1. ROC/PR Curves}\\  
The full model demonstrates strong performance across all tasks, with median AUC of 0.991 (80\% of tasks achieving AUC between 0.982-0.998). Key observations:\\
- Removing sequence embeddings causes 9.7\% AUC drop (from 0.985 to 0.890) in PTM predictions\\
- Structure embedding removal shows smaller impact (3.2\% average AUC decrease)\\
- Dual component removal leads to maximum 12.7\% performance reduction\\
\textbf{2. Confusion Matrix \:}\\  
Component removal creates distinct error patterns:\\
- Missing CNN increases false alarms by 2.1× (e.g., antimicrobial detection: 84→176 cases),which suggests that the CNN module plays a critical role in filtering out incorrect predictions\\
- Losing Transformer raises missed detections by 37.5\% (anticancer task: 152→209 cases),highlighting the importance of this module in correctly identifying positive instances.\\
- Positional encoding removal causes combined degradation (both error types increasing over 100\%)\\
This behavior is consistent across different datasets, as indicated by the confusion matrix behavior for each ablation variant.\\
\textbf{3. UMAP Visualization:}  
As depicted in Figure~\ref{fig:ablation_umap1}\& SuppFig 3, the UMAP projections clearly show that the full model achieves superior separation between positive and negative samples in the feature space. In contrast, removing embeddings or key modules results in overlapping distributions, demonstrating a reduction in the model's ability to discriminate between the classes. \\
- Average distance between categories: 1.73 vs 0.91 in others\\
- Category differentiation improved by 29\%\\
- Decision boundaries 3× clearer\\
These advantages enable 6.8\% better accuracy on challenging cases.

%    从现有数据集中选取的四个数据集，针对给定的类别（见图例）。绘制了每两个相关联消融模型和基准模型（PDeepPP）在提取特征后的UMAP图。(A)-(D) 分别对应 Anticancer\_main、Antiviral、N6-acetyllysine 和 Ubiquitination。
 \begin{figure*}[htbp]
        \centering
        \includegraphics[width=1\textwidth]{ablation_umap1.pdf}  % 调整宽度，图片路径
        \caption{Four datasets selected from existing datasets, with respect to the given category (see legend). UMAP plots are shown for each pair of related ablated models and the baseline model (PDeepPP) after feature extraction. (A)-(D) correspond to Anticancer\_main, Antiviral, N6-acetyllysine, and Ubiquitination, respectively.
}
        \label{fig:ablation_umap1}  % 可用于引用
 \end{figure*}
 
 \textbf{4. Average Performance Metrics}  
In this part of the analysis, we focus on the performance of different models across multiple evaluation metrics. By comparing the baseline model (PDeepPP) and the six ablation variants on average values for each metric, we can better understand the contribution of each key module.

First, \textbf{Accuracy (ACC)} and \textbf{Balanced Accuracy (BACC)} exhibit similar trends. From the table, we see that the baseline model (PDeepPP) achieves the highest scores for these two metrics, 0.8349 and 0.8282, respectively. Removing the embedding module results in almost the same performance as the baseline, indicating that embeddings have a minimal effect on these metrics. However, after removing the attention mechanism and translinear layer, both metrics show a noticeable decline, particularly BACC, with scores of 0.8267 and 0.8359, slightly lower than the baseline. This suggests that the attention mechanism and translinear layer play a significant role in improving model performance.

For \textbf{Sensitivity (SN)} and \textbf{Specificity (SP)}, the baseline model and the embedding-removed model have identical scores, indicating that the embedding module has a relatively mild impact on these metrics. Removing the CNN module results in a reduction in sensitivity (0.7915), while specificity remains at a relatively high level (0.8301). This indicates that removing the CNN module increases false negatives (FN), affecting the model's ability to recognize positive samples. In contrast, removing the Transformer module leads to an increase in false positives (FP), resulting in a higher specificity but a more significant decline in sensitivity.

\textbf{Matthews Correlation Coefficient (MCC)} shows a similar trend. The MCC value for the baseline model (PDeepPP) is 0.8265, the highest among all models. The models without embeddings and without CNN modules follow closely, with scores of 0.8265 and 0.8089, respectively. In contrast, removing the PosEncoding and PosCNN modules results in a substantial drop in MCC values, 0.6511 and 0.5503, respectively, indicating that removing these modules severely affects the model's overall performance.

Overall, the baseline model (PDeepPP) performs best across all metrics, while removing the embedding module has the least effect on performance. On the other hand, removing PosEncoding and PosCNN leads to the most significant performance degradation. In particular, removing PosEncoding and PosCNN not only reduces accuracy but also significantly affects sensitivity and MCC, indicating that they play a critical role in the model.

% 4. 平均性能指标** 
% 在本部分分析中，我们重点关注不同模型在多个评估指标上的表现。通过比较基准模型（PDeepPP）和六个消融变体在各个指标的平均值，我们可以更好地理解每个关键模块的贡献。

% 首先，**准确率（ACC）** 和 **平衡准确率（BACC）** 显示出相似的趋势。从表格中可以看出，基准模型（PDeepPP）在这两个指标上分别达到了最高得分，分别为 0.8349 和 0.8282。去除嵌入模块后，性能几乎与基准模型相同，表明嵌入对这两个指标的影响较小。然而，去除注意力机制和跨线性层后，这两个指标都出现了明显下降，特别是 BACC，得分为 0.8267 和 0.8359，略低于基准模型。这表明注意力机制和跨线性层在提升模型性能方面发挥了重要作用。

% 对于 **灵敏度（SN）** 和 **特异性（SP）**，基准模型和去除嵌入模块的模型得分相同，表明嵌入模块对这两个指标的影响较轻微。去除 CNN 模块导致灵敏度下降（0.7915），而特异性保持在相对较高的水平（0.8301）。这表明去除 CNN 模块增加了假阴性（FN），影响了模型识别正样本的能力。相反，去除 Transformer 模块则导致假阳性（FP）增加，虽然特异性较高，但灵敏度大幅下降。

% **Matthews 相关系数（MCC）** 显示了类似的趋势。基准模型（PDeepPP）的 MCC 值为 0.8265，在所有模型中最高。去除嵌入和去除 CNN 模块的模型紧随其后，分别为 0.8265 和 0.8089。相反，去除 PosEncoding 和 PosCNN 模块会导致 MCC 值大幅下降，分别为 0.6511 和 0.5503，表明去除这些模块严重影响了模型的整体性能。

% 总体而言，表中的数据清晰地展示了每个模块对模型性能的不同影响。基准模型（PDeepPP）在所有指标上表现最佳，而去除嵌入模块对性能的影响最小。另一方面，去除 PosEncoding 和 PosCNN 导致了最显著的性能下降。特别是，去除 PosEncoding 和 PosCNN 不仅降低了准确性，还显著影响了灵敏度和 MCC，表明它们在模型中发挥了关键作用。

% 这一分析表明，**嵌入模块**、**CNN-Transformer 架构** 和 **TIM 损失函数** 对提升模型性能和准确性至关重要。尽管去除这些模块后，模型仍能进行基本预测，但其性能显著下降，特别是在灵敏度、特异性和 MCC 上。这突显了消融研究的重要性，通过系统地去除组件，评估每个模块对最终性能的贡献。


\section{Conclusion}\label{sec4}
The PDeepPP framework represents a significant leap forward in the field of peptide identification and PTM prediction, showcasing the transformative power of deep learning in addressing complex bioinformatics challenges. Our experimental results demonstrate that PDeepPP consistently outperforms state-of-the-art models across a variety of tasks, underscoring its robustness and adaptability. For instance, in antimicrobial peptide identification, PDeepPP achieved an impressive accuracy (ACC) of 0.9726 and a PR AUC of 0.9977, significantly surpassing the performance of UniDL4BioPep. Similarly, in phosphorylation site prediction, a critical PTM task, PDeepPP achieved an ACC of 0.9984 and a PR AUC of 0.9910, highlighting its exceptional predictive capabilities and ability to generalize across diverse datasets. Furthermore, PDeepPP substantially reduced false positives (FP) and false negatives (FN) across all tested datasets, reinforcing its practical utility in real-world applications. 

PDeepPP’s success can be attributed to its innovative integration of pretrained protein language models (ESM-2) with a parallel neural network architecture combining Transformers and CNNs. This design allows the model to capture both local and global sequence features, enhancing its ability to handle the complexity of protein sequences. The inclusion of the Transductive Information Maximization (TIM) loss function further improves performance on imbalanced datasets, a common challenge in bioinformatics. This combination of advanced deep learning techniques enables PDeepPP to achieve superior accuracy, robustness, and scalability compared to traditional methods.

Compared to conventional models, PDeepPP significantly reduces the need for extensive feature engineering and manual annotation, making it adaptable to a wide range of PTM types and peptide functionalities. Its ability to handle large-scale datasets and diverse biological tasks positions it as a valuable tool for large-scale bioinformatics research. Moreover, PDeepPP’s computational efficiency and reduced reliance on experimental data make it a cost-effective solution for peptide discovery and PTM analysis.

Looking ahead, PDeepPP holds immense potential for broader applications in both research and industry. Future directions include integrating structural and spatial data to further enhance predictive accuracy, as well as combining the framework with high-throughput experimental workflows to accelerate large-scale protein function discovery and drug development. Additionally, as novel PTM types, such as AMPylation, are discovered, PDeepPP can provide computational support for exploring these uncharted areas, advancing its potential applications in precision medicine and therapeutic innovation. The framework could also be extended to address multi-label identification tasks, where peptides may exhibit multiple bioactivities or PTMs simultaneously, further broadening its utility.

In summary, PDeepPP combines the strengths of pretrained protein language models, advanced neural network architectures, and innovative loss functions to offer a cutting-edge solution for protein sequence analysis. It represents a major advancement in the field of bioinformatics and has the potential to play a pivotal role in accelerating peptide discovery, PTM functional analysis, and the development of novel therapeutic strategies. By reducing the reliance on costly and time-consuming experimental methods, PDeepPP not only enhances the efficiency of bioinformatics research but also opens new avenues for understanding the complex mechanisms underlying protein modifications and peptide bioactivity. As the field continues to evolve, PDeepPP is poised to drive innovation in life sciences, contributing to the development of targeted therapies and personalized medicine.

\section*{Data and Code Availability}
The source code and preprocessed data supporting this study are openly available on GitHub at \url{https://github.com/fondress/PDeepPP} and \url{https://huggingface.co/fondress/PDeppPP}. Trained models, evaluation scripts, and implementation details are included in the repository to ensure reproducibility.

\bibliographystyle{unsrt}
\bibliography{reference}

\onecolumn
\section{Supplementary}\label{sec5}
\setcounter{figure}{0} % 重置计数器
\captionsetup[figure]{name=Supplementary Fig}
% 自定义引用格式（SuppFig + 编号）
\crefformat{figure}{SuppFig~#2#1#3}       % 单数引用格式
\Crefformat{figure}{SuppFig~#2#1#3}       % 首字母大写格式
\crefmultiformat{figure}{SuppFigs~#2#1#3}{和~#2#1#3}{,~#2#1#3}{和~#2#1#3} % 复数格式


\vspace*{\fill}
\begin{center}
  \includegraphics[width=1\textwidth]{unidl_contrast2.pdf}
  \captionof{figure}{The remaining ten tasks showing consistent performance in comparison with the UniDL4BioPep model. The upper section shows the AUC curves and confusion matrix combinations for each task. In the confusion matrices, the numbers in parentheses next to the values represent the difference in the count of this metric compared to PDeepPP for the UniDL4BioPep model. The color intensity of each block in the matrix indicates the proportion of that class relative to the total negativ e/positive samples.The lower section displays the UMAP plots for each dataset after feature extraction by PDeepPP. The label order is: (A) ACE, (B) DPPIV, (C) bitter, (D) Quorum, (E) Anticancer\_alternative, (F) Anti-MRSA, (G) Antiparasitic, (H) neuro, (I) Toxicity, (J) Antioxidant.}
  \label{SuppFig：unidl_contrast2}
\end{center}
\vspace{\fill}
\clearpage

\vspace*{\fill}
\begin{center}
\includegraphics[width=1\textwidth]{musite_contrast2.pdf}  % 调整宽度，图片路径
\captionof{figure}{The Remaining six tasks showing consistent performance in comparison with the MusiteDeep model. The upper section shows the AUC curves and confusion matrix combinations for each task. In the confusion matrices, the numbers in parentheses next to the values represent the difference in the count of this metric compared to PDeepPP for the MusiteDeep model. The color intensity of each block in the matrix indicates the proportion of that class relative to the total negative/positive samples. The lower section displays the UMAP plots for each dataset after feature extraction by PDeepPP, with the site distribution of these six datasets shown in the bottom right. The label order is:  (A) Phosphoserine\_Phosphothreonine\_S,T, (B) N-linked-glycosylation\_N, (C) O-linked-glycosylation\_S,T, (D) Methylation\_R, (E) S-Palmitoylation\_C, (F) SUMOylation\_K, (G) Hydroxyproline\_P.}
\label{SuppFig：musite_contrast2}  % 可用于引用
\end{center}
\vspace{\fill}
\clearpage

\vspace*{\fill}
\begin{center}
\includegraphics[width=1\textwidth]{ablation_umap2.pdf}  % 调整宽度，图片路径
\captionof{figure}{Four datasets selected from existing datasets, with respect to the given category (see legend). UMAP plots are shown for each pair of related ablated models and the baseline model (PDeepPP) after feature extraction. (A)-(D) correspond to methylation-G, methylation-R, Ubiquitin-K*, and Crotonylation-K, respectively.}
\label{SuppFig：ablation_umap2}  % 可用于引用
\end{center}
\vspace{\fill}
\clearpage

\enablesupptables

\begin{table*}[t]
\captionsetup{width=0.8\textwidth}
\caption{Benchmark Datasets Sourced from Publications Featuring State-of-the-Art Models}
\label{table:benchmark dataset}
\setlength{\tabcolsep}{10pt}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{@{}p{4cm} | p{2cm} p{2cm} | p{2cm} p{2cm} | p{1.8cm}@{}}
\toprule
\textbf{Tasks}& \multicolumn{2}{c|}{\textbf{Training dataset}} & \multicolumn{2}{c|}{\textbf{Test dataset}} & \textbf{Reference} \\
\midrule
 \textbf{Bioactivity} & \textbf{Positives} & \textbf{Negatives} & \textbf{Positives} & \textbf{Negatives} & \\
\midrule
ACE inhibitory activity & 913 & 913 & 386 & 386 & \cite{ACE} \\
DPP IV inhibitory activity & 532 & 532 & 133 & 133 & \cite{DPPIV} \\
Bitter & 256 & 256 & 64 & 64 & \cite{bitter} \\
Umami & 112 & 241 & 28 & 61 & \cite{umami} \\
Antimicrobial activity  & 3876 & 9552 & 2584 & 6369 & \cite{antimicrobial} \\
Antimalarial activity \hfill main & 111 & 1708 & 28 & 427 & \cite{antimalarial} \\
 \hfill alternaive& 111 & 542 & 28 & 135 & [37] \\
Quorum sensing activity & 200 & 200 & 20 & 20 & \cite{quorum.bibtex} \\
Anticancer activity\hfill main & 689 & 689 & 172 & 172 & \cite{anticancer1,anticancer2} \\
\hfill alternative& 776 & 776 & 194 & 194 & [6] \\
Anti-MRSA strains activity & 118 & 678 & 30 & 169 & \cite{antimrsa} \\
Tumor T cell antigens & 470 & 318 & 122 & 75 & \cite{TTCA} \\
Blood–Brain Barrier & 100 & 100 & 19 & 19 & \cite{BBP} \\
Antiparasitic activity & 255 & 255 & 46 & 46 & \cite{antiparastic} \\
Neuropeptide & 1940 & 1940 & 485 & 485 & \cite{nuero1,nuero2} \\
Antibacterial activity & 6583 & 6583 & 1695 & 1695 & \cite{Antibacterial，Antifungal，Antiviral} \\
Antifungal activity & 778 & 778 & 215 & 215 & \cite{Antibacterial，Antifungal，Antiviral} \\
Antiviral activity & 2321 & 2321 & 623 & 623 & \cite{Antibacterial，Antifungal，Antiviral} \\
Toxicity & 1642 & 1642 & 290 & 290 & \cite{toxicity} \\
Antioxidant activity & 582 & 541 & 146 & 135 & \cite{antioxidant} \\
\midrule
\textbf{PTMs}\cite{Musite} & \textbf{ Positives} & \textbf{ Negatives} & \textbf{ Positives} & \textbf{ Negatives} & \textbf{Reference} \\
\midrule
Phosphoserine/threonine & 25170 & 473607 & 4847 & 96667 &  \\
Phosphotyrosine & 6939 & 64884 & 1669 & 17123 &  \\
N-linked glycosylation & 52926 & 318092 & 12836 & 76512 &  \\
O-linked glycosylation &567  & 28446 & 143 & 6610 &  \\
N6-acetyllysine & 16222 & 199649 & 3895 & 48653 &  \\
Methylarginine & 3749 & 85090 & 966 & 20825 & \\
Methyllysine & 1324 & 25836 & 335 & 7640 &  \\
S-palmitoylation-cysteine & 2260 & 11904 & 541 & 3172 &  \\
Pyridoxine-carboxylic-acid & 1128 & 7688 & 285 & 1554 &  \\
Ubiquitination & 2528 & 26772 & 581 & 7797 & \\
SUMOylation & 795 & 16209 & 218 & 4036 &  \\
Hydroxylysine & 390 & 2968 & 121 & 661 &  \\
Hydroxyproline & 3931 & 13910 & 892 & 3631 &  \\
\midrule
\textbf{Remaining PTMs} & \textbf{ Positives} & \textbf{ Negatives} & \textbf{ Positives} & \textbf{ Negatives} & \textbf{Reference} \\
\midrule
methylation-G & 627 & 10490 & 165 & 2615 & \cite{methylation-G} \\
methylation-R & 1038 & 1038 & 290 & 290 & \cite{methylation-R} \\
Ubiquitin\_K* & 2528 & 26772 & 581 & 7797 & \cite{ubiquitin} \\
Crotonylation\_K & 6975 & 6975 & 3989 & 3989 & \cite{kcr} \\

\botrule
\end{tabular}
}
\end{table*}

\begin{table*}[htbp]
\addtolength{\oddsidemargin}{-2cm}  % 左页边距缩小1cm
\addtolength{\evensidemargin}{-1cm} % 右页边距缩小1cm
\centering
\captionsetup{width=0.85\textwidth}
\caption{Comparison with UniDL4BioPep in bioactive peptides and state-of-the-art models from the same benchmark datasets}
\label{table:unidl comparison}
\setlength{\tabcolsep}{8pt}
{\renewcommand{\arraystretch}{0.83}
\begin{tabular}{@{}p{3cm} p{2.5cm} p{1cm} p{1cm} p{1cm} p{1cm} p{1cm} p{1cm} p{1cm}@{}}
\toprule
\textbf{Bioactivity} & \textbf{Model name} & \textbf{ACC} & \textbf{ROC} & \textbf{PR} & \textbf{BACC} & \textbf{Sn} & \textbf{Sp} & \textbf{MCC}  \\
\midrule
\multirow{3}{3cm}{ACE inhibitory activity} 
& PDeepPP &0.8225 &0.9049 &\textbf{0.9144} &0.8225 &0.8782 &0.7668 &0.6491 \\
& UniDL4BioPep &0.8433 &0.8918 &0.8728 &\textbf{0.8433} &0.8238 &0.8627 &0.6870  \\
& mAHTPred & \textbf{0.883} & \textbf{0.951} & & N/A & \textbf{0.894} & \textbf{0.873} & \textbf{0.767}  \\

\midrule
\multirow{3}{3cm}{DPP IV inhibitory activity}  
& PDeepPP &\textbf{0.8947} &\textbf{0.9615} &\textbf{0.9654} &\textbf{0.8947} &0.8496 &\textbf{0.9398} &\textbf{0.7927} \\ 
& UniDL4BioPep &0.8534 &0.9384 &0.9444 &0.8534 &\textbf{0.8647} &0.8421 &0.7069  \\
& iDPPIV-SCM & 0.797 & 0.847 &  & N/A & 0.789 & 0.805 & 0.594  \\

\midrule
Bitter 
& PDeepPP &0.9141 &0.9625 &0.9692 &0.9141 &0.8906 &0.9375 &0.8290 \\
& UniDL4BioPep &\textbf{0.9375} &\textbf{0.9824} &\textbf{0.9838} &0.9375 &0.9219 &\textbf{0.9531} &0.8754  \\
& BERT4Bitter & N/A & 0.922  & & \textbf{0.938} & 0.906 & 0.844 & \textbf{0.964} \\
& iBitter-Fuse & 0.93 & 0.933 &  & N/A & \textbf{0.938} & 0.922 & 0.859  \\
\midrule
Umami 
& PDeepPP &\textbf{0.9213} &\textbf{0.9543} &\textbf{0.9835} &\textbf{0.9330} &0.9016 &\textbf{0.9643} &\textbf{0.8325} \\
& UniDL4BioPep &0.8764 &0.9417 &0.9760 &0.8326 &\textbf{0.9508} &0.7143 &0.7055  \\
& UniDL4BioPep-FL & 0.888 & 0.943 &  & 0.883 & 0.892 & 0.875 & 0.733 \\
& iUmami-SCM & 0.865 & 0.898  & & N/A & 0.714 &  0.934 & 0.679 \\
\midrule
Antimicrobial activity 
& PDeepPP &\textbf{0.9726} &\textbf{0.9947} &\textbf{0.9977} &0.9609 &\textbf{0.9887} &0.9330 &\textbf{0.9329} \\
& UniDL4BioPep &0.9631 &0.9910 &0.9959 &0.9532 &0.9768 &0.9296 &0.9099 \\ 
& UniDL4BioPep-FL & 0.96 & 0.991 &  & 0.961 & 0.96 & 0.963 & 0.903 \\
& TransImbAMP & N/A &  N/A & & \textbf{0.969} & 0.963 & \textbf{0.974} & N/A \\
\midrule
\multirow{3}{3cm}{Antimalarial activity (main dataset)} 
& PDeepPP &0.9758 &\textbf{0.9049} &\textbf{0.9898} &0.8203 &0.9977 &0.6429 &0.7695 \\
& UniDL4BioPep &\textbf{0.9780} &0.8898 &0.9872 &0.8214 &\textbf{1.0000} &0.6429 &0.7926 \\
& UniDL4BioPep-FL & \textbf{0.978} &0.898 &  & \textbf{0.965} & 0.979 & 0.95 & \textbf{0.793} \\
& iAMAP-SCM & \textbf{0.978} & 0.82 &  & 0.826 & 0.654 & \textbf{0.998} &  0.776 \\
\midrule
\multirow{3}{3cm}{Antimalarial activity (alternative dataset)} 
& PDeepPP &0.9877 &\textbf{0.9929} &\textbf{0.9984} &0.9643 &\textbf{1.0000} &0.9286 &0.9566 \\
& UniDL4BioPep &0.9816 &0.9844 &0.9965 &0.9464 &\textbf{1.0000} &0.8929 &0.9346 \\
& UniDL4BioPep-FL & \textbf{0.989} & 0.987 &  & \textbf{0.993} & 0.985 & \textbf{1} & \textbf{0.9570} \\
& iAMAP-SCM & 0.957 & 0.903 &  & 0.896 & 0.808 & 0.985 & 0.834 \\
\midrule
\multirow{4}{3cm}{Quorum sensing activity} 
& PDeepPP &\textbf{0.9750} &\textbf{0.9975} &\textbf{0.9976} &\textbf{0.9750} &\textbf{0.9500} &\textbf{1.0000} &\textbf{0.9512} \\
& UniDL4BioPep &0.9500 &0.9900 &0.9908 &0.9500 &0.9000 &\textbf{1.0000} &0.9045  \\
& iQSP & 0.93 & 0.96 &  & N/A & N/A & N/A & 0.86 \\
& QSPred-FL &  0.943 & 0.945 &  & N/A & 0.935 &  0.95 & 0.885 \\
\midrule
\multirow{3}{3cm}{Anticancer activity (main dataset)} 
& PDeepPP &0.7587 &\textbf{0.8334} &\textbf{0.8308} &0.7587 &\textbf{0.8547} &0.6628 &0.5272 \\
& UniDL4BioPep &0.7355 &0.8054 &0.7780 &0.7355 &0.7326 &0.7384 &0.4709 \\
& iACP-FSCM & \textbf{0.825} & 0.812 &  & \textbf{0.825} & 0.726 & \textbf{0.903} & \textbf{0.646} \\
& AntiCP2.0 & 0.754 & N/A &  & 0.754 & 0.774 & 0.734 & 0.51 \\
\midrule
\multirow{3}{3cm}{Anticancer activity (alternative dataset)} 
& PDeepPP &0.9433 &0.9709 &\textbf{0.9668} &0.9433 &0.9639 &\textbf{0.9227} &0.8874 \\
& UniDL4BioPep &\textbf{0.9459} &\textbf{0.9711} &0.9630 &\textbf{0.9459} &\textbf{0.9794} &0.9124 &\textbf{0.8938}  \\
& iACP-FSCM & 0.889 & 0.93  & & N/A & 0.876 & 0.902 & 0.779 \\
& AntiCP2.0 & 0.92 & N/A &  & N/A & 0.923 & 0.918 & 0.84 \\
\midrule
\multirow{3}{3cm}{Anti-MRSA strains activity} 
& PDeepPP &\textbf{0.9950} &\textbf{0.9998} &\textbf{1.0000} &0.9833 &\textbf{1.0000} &0.9667 &\textbf{0.9803} \\
& UniDL4BioPep &0.9899 &0.9986 &0.9998 &0.9667 &\textbf{1.0000} &0.9333 &0.9604 \\
& UniDL4BioPep-FL &0.994 & 0.999 &  & \textbf{0.997} & 0.994 & \textbf{1} & 0.98  \\
& SCMRSA & 0.96 & 0.986 &  & 0.935 & 0.9 & 0.97 & 0.848 \\
\midrule
Tumor T cell antigens 
& PDeepPP &\textbf{0.7563} &\textbf{0.8198} &\textbf{0.7662} &0.7185 &0.5600 &\textbf{0.8770} &\textbf{0.4680} \\
& UniDL4BioPep &0.7513 &0.7883 &0.7395 &0.7144 &0.5600 &0.8689 &0.4569  \\
& UniDL4BioPep-FL & 0.746 & 0.796 &  & \textbf{0.762} & 0.734 & 0.791 & 0.446 \\
& iTTCA-Hybrid &  0.71 & 0.756  & & N/A & \textbf{0.844} & 0.493 & 0.363 \\
\midrule
Blood–Brain Barrier 
& PDeepPP &\textbf{0.9211} &\textbf{0.9640} &\textbf{0.9680} &\textbf{0.9211} &\textbf{0.9474} &0.8947 &\textbf{0.8433} \\
& UniDL4BioPep &0.8421 &0.9224 &0.9211 &0.8421 &0.8947 &0.7895 &0.6880  \\
& BBBpred & 0.7895 & 0.7895  & & N/A & 0.6316 & \textbf{0.9474} & 0.6102  \\
\midrule
Antiparasitic activity 
& PDeepPP &0.7609 &\textbf{0.9267} &\textbf{0.937}0 &0.7609 &0.9565 &0.5652 &0.5669 \\
& UniDL4BioPep &\textbf{0.8478} &0.9045 &0.9258 &\textbf{0.8478} &0.8043 &\textbf{0.8913} &0.6983  \\
& PredAPP & 0.88 & 0.922 &  & N/A & \textbf{0.978} & 0.783 & \textbf{0.775} \\
\midrule
Neuropeptide 
& PDeepPP &0.9021 &0.9611 &0.9602 &0.9021 &\textbf{0.9010} &0.9031 &0.8041 \\
& UniDL4BioPep &0.9052 &0.9707 &\textbf{0.9707} &\textbf{0.9052} &0.8784 &0.9320 &0.8115  \\
& PreNeuroP & 0.897 & 0.954 &  & N/A & 0.886 & 0.907 & 0.794 \\
& NeuroPred-CLQ & \textbf{0.936} & \textbf{0.988} &  & N/A & 0.897 & \textbf{0.975} & \textbf{0.875}  \\
\midrule
Antibacterial activity 
& PDeepPP &\textbf{0.9478} &\textbf{0.9811} &\textbf{0.9752} &\textbf{0.9478} &0.9617 &0.9339 &\textbf{0.8959} \\
& UniDL4BioPep &0.9395 &0.9775 &0.9704 &0.9395 &\textbf{0.9687} &0.9103 &0.8806  \\
& ABPDiscover & 0.935 & 0.975 &  & N/A & 0.912 & \textbf{0.957} & 0.87  \\
\midrule
Antifungal activity 
& PDeepPP &\textbf{0.9535} &\textbf{0.9911} &\textbf{0.9916} &\textbf{0.9535} &0.9442 &\textbf{0.9628} &\textbf{0.9071} \\
& UniDL4BioPep &0.9512 &0.9862 &0.9838 &0.9512 &0.9628 &0.9395 &0.9026  \\
& ABPDiscover & 0.942 & 0.988 &  & N/A & 0.921 
& \textbf{0.963} & 0.884  \\

\midrule
Antiviral activity 
& PDeepPP &\textbf{0.8531} &\textbf{0.9192} &\textbf{0.9164} &\textbf{0.8531} &\textbf{0.9486} &0.7576 &\textbf{0.7195} \\
& UniDL4BioPep &0.8291 &0.9012 &0.8827 &0.8291 &0.9037 &0.7544 &0.6656  \\
& ABPDiscover & 0.828 & 0.896 &  & N/A & 0.764 & \textbf{0.892} & 0.662  \\

\midrule
Toxicity 
& PDeepPP &0.9172 &0.9779 &0.9770 &0.9168 &0.9108 &0.9228 &0.8336 \\
& UniDL4BioPep &\textbf{0.9603} &\textbf{0.9943} &\textbf{0.9934} &\textbf{0.9608} &\textbf{0.9665} &\textbf{0.9550} &\textbf{0.9205}  \\
& ATSE & 0.952 & 0.976 &  & N/A & 0.965 & 0.94 & 0.903  \\
\midrule
Antioxidant activity 
& PDeepPP &0.7972 &0.8776 &0.8880 &0.7981 &0.7740 &0.8222 &0.5959 \\
& UniDL4BioPep &\textbf{0.8221} &\textbf{0.9296} &\textbf{0.9410} &\textbf{0.8229} &\textbf{0.8014} &\textbf{0.8444} &\textbf{0.6454}  \\
& AntiOxPred-FRS & N/A & 0.79 &  & N/A & N/A & N/A & 0.48 \\
\bottomrule
\end{tabular}
}
\end{table*}

\begin{table*}[t]
\captionof{table}{Comparison with the MusiteDeep in PTMs from the same benchmark datasets}
    \label{table:musite comparison}
    {\renewcommand{\arraystretch}{0.95}
    \begin{tabular}{@{}p{3.2cm} p{2cm} p{1.2cm} p{1.2cm} p{1.2cm} p{1.2cm} p{1.2cm} p{1.2cm} p{1.2cm}@{}}
    \toprule
    \textbf{PTM types} & \textbf{Model} & \textbf{ACC} & \textbf{ROC} & \textbf{PR}  & \textbf{BACC} & \textbf{SN} & \textbf{SP} & \textbf{MCC} \\
    \midrule
    Hydroxyproline & PDeepPP & \textbf{0.9885} & \textbf{0.9992} & \textbf{0.9959} & \textbf{0.9729} & 0.9504 & \textbf{0.9955} & \textbf{0.9557} \\  
     & MusiteDeep & 0.9425 & 0.9937 & 0.9718 & 0.9626 & \textbf{0.9917} & 0.9334 & 0.8219 \\  
    \midrule
    Hydroxylysine & PDeepPP & \textbf{0.9648} & \textbf{0.9915} & \textbf{0.9615} & \textbf{0.9430} & 0.9070 & \textbf{0.9791} & \textbf{0.8887} \\  
     & MusiteDeep & 0.8720 & 0.9866 & 0.9523 & 0.9152 & \textbf{0.9865} & 0.8438 & 0.7084 \\  
     \midrule
    Methyllysine & PDeepPP & \textbf{0.9984} & \textbf{0.9997} &\textbf{0.9972} & \textbf{0.9924} & \textbf{0.9859} & \textbf{0.9990} & \textbf{0.9809} \\  
     & MusiteDeep & 0.9877 & 0.9921 & 0.9649 & 0.9842 & 0.9803 & 0.9881 & 0.8756 \\  
     \midrule
    Methylarginine &PDeepPP & \textbf{0.9918} & \textbf{0.9977} & 0.9565 & 0.9400 & 0.8830 & \textbf{0.9969} & \textbf{0.9016} \\  
     & MusiteDeep & 0.9622 & 0.9949 & \textbf{0.9638} & \textbf{0.9768} & \textbf{0.9928} & 0.9608 & 0.7173 \\  
     \midrule
    N-linked glycosylation & PDeepPP & \textbf{0.9810} &\textbf{0.9970} & \textbf{0.9806} & 0.9644 & 0.9415 & \textbf{0.9873} & \textbf{0.9204} \\  
     & MusiteDeep & 0.9597 & 0.9883 & 0.8874 & \textbf{0.9760} & \textbf{0.9985} & 0.9535 & 0.8579 \\  
     \midrule
    N6-acetyllysine & PDeepPP & \textbf{0.9870} & \textbf{0.9959} & \textbf{0.9589} & 0.9491 & 0.9045 & \textbf{0.9936} & \textbf{0.9049} \\  
     & MusiteDeep & 0.9647 & 0.9939 & 0.9105 & \textbf{0.9761} & \textbf{0.9895} & 0.9627 & 0.8042 \\  
     \midrule
    Phosphotyrosine & PDeepPP & \textbf{0.9944} & \textbf{0.9989} & \textbf{0.9909} & \textbf{0.9818} & 0.9664 & \textbf{0.9971} & \textbf{0.9654} \\  
     & MusiteDeep & 0.9568 & 0.9943 & 0.9417 & 0.9709 & \textbf{0.9880} & 0.9538 & 0.7971 \\  
     \midrule
    Phosphoserine/threonine & PDeepPP & \textbf{0.9984} & \textbf{0.9872} & \textbf{0.9910} & \textbf{0.9819} & \textbf{0.9738} & \textbf{0.9899} & \textbf{0.9546} \\  
     & MusiteDeep & 0.9977 & 0.9837 & 0.9871 & 0.9740 & 0.9596 & 0.9885 & 0.9416 \\  
     \midrule
    \multirow{2}{3cm}{Pyrrolidone-carboxylic-acid} & PDeepPP & \textbf{0.9804} & 0.9964 & 0.9822 & 0.9626 & 0.9368 & \textbf{0.9884} & \textbf{0.9253} \\  
     & MusiteDeep & 0.9625 & \textbf{0.9978} & \textbf{0.9910} & \textbf{0.9735} & \textbf{0.9895} & 0.9575 & 0.8749 \\  
     \midrule
    O-linked glycosylation & PDeepPP & \textbf{0.9941} & 0.9050 & 0.8193 & 0.8738 & 0.7483 & \textbf{0.9994} & \textbf{0.8466} \\  
     & MusiteDeep & 0.9511 & \textbf{0.9916} & \textbf{0.9193} & \textbf{0.9682} & \textbf{0.9860} & 0.9504 & 0.5303 \\  
     \midrule
    \multirow{2}{3cm}{S-palmitoylation-cysteine} & PDeepPP & \textbf{0.9908} & \textbf{0.9971} & \textbf{0.9906} & \textbf{0.9885} & \textbf{0.9852} & \textbf{0.9918} & \textbf{0.9639} \\  
     & MusiteDeep & 0.9793 & 0.9965 & 0.9861 & 0.9764 & 0.9723 & 0.9805 & 0.9207 \\  
     \midrule
    SUMOylation & PDeepPP & \textbf{0.9915} & 0.9982 & 0.9713 & 0.9521 & 0.9083 & \textbf{0.9960} & \textbf{0.9123} \\  
     & MusiteDeep & 0.9843 & \textbf{0.9987} & \textbf{0.9755} & \textbf{0.9830} & \textbf{0.9817} & 0.9844 & 0.8633 \\  
     \midrule
    Ubiquitination & PDeepPP & \textbf{0.9922} & \textbf{0.9983} & \textbf{0.9794} & \textbf{0.9656} & 0.9346 & \textbf{0.9965} & \textbf{0.9394} \\  
     & MusiteDeep & 0.9390 & 0.9819 & 0.8969 & 0.9457 & \textbf{0.9535} & 0.9379 & 0.6874 \\  
    \botrule
    \end{tabular}
    }
\end{table*}

\begin{table}[htbp]
\begin{threeparttable}
\caption{Model Hyperparameter Configuration}
\label{tab:hyperparameters}
    {\renewcommand{\arraystretch}{0.95}
\begin{tabular}{@{}p{3cm} p{3cm} p{2cm} p{8cm}@{}}
\toprule
\textbf{Category} & \textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
\midrule
\multirow{4}{*}{Training Config} 
& Learning Rate & 0.001 & Initial learning rate for Adam optimizer \\
& Batch Size & 32/128 & Training batch size (32) and inference batch size (128) \\
& Training Epochs & 100 & Maximum training iterations with early stopping \\
& Patience & 20 & Epochs tolerance for validation loss plateau \\
\midrule
\multirow{5}{*}{Architecture}
& ESM-2 Dimension & 1280 & Output dimension of pretrained ESM-2 embeddings \\
& Transformer Dim & 128 & Hidden dimension in self-attention layers \\
& Attention Heads & 8 & Parallel attention mechanisms per layer \\
& Encoder Layers & 4 & Stacked transformer encoder blocks \\
& CNN Channels & 64→32→64 & Feature channel progression in convolutional modules \\
\midrule
\multirow{3}{*}{Regularization}
& Dropout & 0.3/0.15 & Dropout rates: attention (0.3), CNN (0.15) \\
& Position Encoding & Yes & Learnable positional embeddings \\
& Weight Decay & None & No explicit L2 regularization applied \\
\midrule
\multirow{3}{*}{UMAP Visualization}
& n\_neighbors & 15 & Local neighborhood size for manifold approximation \\
& min\_dist & 0.1 & Minimum embedding distance between points \\
& random\_state & 42 & Seed for reproducible visualization \\
\midrule
\multirow{3}{*}{Loss Function}
& $\lambda$ & [0.9,1.0] & Cross-entropy weight in TIM loss \\
& $\alpha$ & 1.0 & Entropy regularization coefficient \\
& Threshold & 0.5 & Decision boundary for identification \\
\midrule
Feature Fusion & ESM Ratio & [0.9,1.0] & Weighting between ESM-2 and task-specific embeddings \\
\bottomrule
\end{tabular}
}

 \vspace{0.3em}
    {\renewcommand{\arraystretch}{0.95}
\begin{tablenotes}
\footnotesize
\item Notes:
\begin{enumerate}
\item Final testing uses optimal parameters
\item UMAP parameters specifically used for 2D projection of 1280D feature vectors in model interpretability analysis. The n\_neighbors controls local/global structure balance, min\_dist affects cluster tightness, and random\_state ensures reproducibility.
\item Implementation: Based on PyTorch 1.12+ framework
\end{enumerate}
\end{tablenotes}
}
\end{threeparttable} % 结束包裹
\end{table}


\end{document}
