\section{Related Work}
\subsection{Topic Modeling Approaches and Their Evolution} % 1 paragraph: 5-6 sentences
The field of topic modeling has evolved significantly, advancing from early matrix factorization techniques to modern probabilistic and neural architectures. One foundational work, Latent Semantic Indexing (LSI) \cite{papadimitriou1998LSI},
utilized singular value decomposition to uncover conceptual associations between words and documents through co-occurrence patterns, though it lacked a fully generative probabilistic model.
Building on these concepts, Probabilistic Latent Semantic Indexing (pLSI) \cite{hofmann1999pLSI}
introduced a probabilistic framework where words are generated from a mixture of topics. This probabilistic perspective paved the way for LDA \cite{blei2003LDA}, 
a seminal contribution that introduced a fully generative Bayesian model capable of inferring a corpus-wide set of latent topics and their associated per-document distributions. LDA’s flexible yet tractable variational inference techniques (as well as alternative inference algorithms like Gibbs sampling, as demonstrated in \cite{Griffiths2004Gibbs})
solidified it as a cornerstone in topic modeling research.


In more recent years, researchers have embraced neural variational inference frameworks—inspired by VAE \cite{Kingma2014VAE}
—to develop neural topic models (NTMs) such as 
NVDM \cite{miao2016NVDM}, 
GSM/GSB/RSB
\cite{miao2017GSM-GSB-RSB}, 
ProdLDA/AVITM \cite{srivastava2017ProdLDA-AVITM}, 
and ETM \cite{dieng2020ETM}. 
These models employ continuous latent representations and deep neural networks to capture richer semantic structures and often rely on embeddings to represent words and documents. 
Another emerging direction leverages contextualized embeddings and large language models (LLMs) as building blocks or alternatives to traditional topic models. Methods such as clustering-based approaches \cite{sia2020tired-cluster},
CombinedTM \cite{bianchi2021CombinedTM}, and 
BERTopic \cite{grootendorst2022bertopic}
have demonstrated that directly clustering embeddings of documents and words can yield coherent, diverse topics. More recently, research efforts like
\cite{pham2024topicgpt, mu2024llmtm, mu2024llmtm-granularity-hallucination} propose employing LLMs directly to generate and refine topics, showing promise for overcoming certain limitations of classical topic modeling frameworks.
\subsection{Evaluation of Topic Models: Methods and Limitations}
Topic model evaluation has shifted from basic statistical measures to methods that capture human interpretability. Early work used held-out likelihood, introduced in \cite{blei2003LDA} 
and consistently employed by  
\cite{blei2007CTM},
measure how well a trained model can predict unseen data, although it can be computationally demanding. Similarly, log probability 
\cite{newman2009distributed} 
aggregates observed words and documents likelihood, providing an intuitive gauge of model fit but potentially favoring complexity over interpretability. Perplexity normalized held-out likelihood \cite{blei2003LDA, blei2007CTM, wang2008cDTM, newman2009distributed, Hinton2009UndirectedTM, srivastava2017ProdLDA-AVITM, ding2018coherenceNTM, card2018SCHOLAR, Zhang2018WHAI, dieng2020ETM}.
However, low perplexity does not necessarily translate to coherent or meaningful topics\cite{chang2009HumanTeaLeaves}, 
highlighting a fundamental disconnect between statistical quality and human interpretability.
To bridge this gap, human-centered evaluations like word intrusion and topic intrusion tasks \cite{chang2009HumanTeaLeaves}
were introduced, where human judges identify out-of-place words or topics. Human-rated coherence, first explored in
\cite{newman2010automaticHuman} 
and later adopted by 
\cite{mimno2011optimizing, aletras2013TC, stammbach2023revisitingLLM} 
asking annotators score topics on an ordinal scale. Although these methods closely reflect human understanding, their reliance on manual annotation constrains scalability and efficiency.
In response, automated metrics were then introduced to approximate human judgments. Topic words-based coherence metrics measure how strongly top-ranked topic words co-occur in the underlying data. Early approaches, such as coherence $C_{\text{UCI}}$ \cite{Newman2010TMforDL}
and coherence $C_{\text{UMass}}$ \cite{mimno2011optimizing},
rely on word co-occurrence frequencies and statistics derived from the training corpus. More refined metrics, like NPMI \cite{bouma2009NPMI}, normalize mutual information coherence scores $C_{\text{NPMI}}$ to better align with human judgments \cite{aletras2013TC, lau2014machineTeaLeaves, srivastava2017ProdLDA-AVITM, ding2018coherenceNTM, card2018SCHOLAR, wang2019ATM, wang2020BAT, grootendorst2022bertopic, wu2023ECRTM},
while coherence $C_{\text{v}}$ \cite{roder2015CV} 
uses a variation of $C_{\text{NPMI}}$ to compute topic coherence over a sliding window of size and adds a weight to assign more strength to more related words. Moreover, embedding-based coherence used by \cite{schnabel2015EmbeddingEvalTM, Nikolenko2016EmbeddingEvalTM, ding2018coherenceNTM, bianchi2021CombinedTM} further improves the match to human judgement \cite{Nikolenko2016EmbeddingEvalTM}.
Other metrics assess different aspects of topic quality beyond coherence. Diversity metrics ensure that the discovered topics are distinctive, not redundant or overlapping. For instance, topic diversity \cite{dieng2020ETM} 
counts the proportion of unique top words across topics, while topic redundancy \cite{Burkhardt2019TopicRedundancy} 
and topic uniqueness \cite{nan2019TopicUniqueness} 
measure how frequently top words appear across multiple topics. Similarly, inverted ranked-biased overlap \cite{bianchi2021CombinedTM} 
and embedding-based diversity metrics \cite{bianchi2021EmbedingCentroidTD, Terragni2021EmbedingTD} 
compare ranked word lists or semantic distances to ensure substantial topic variety. 
Document-level evaluations measure how well topics capture document's content. \cite{bhatia2017DocumentTopicEval} 
ask annotators to rate each topic’s relevance to a given document.
\cite{Korencic2018DocumenCoherence} vectorize documents associated with the selected topic and calculate a coherence score based on the document vectors.
\cite{bhatia2018topicIntrusionEval} 
tests whether an outlier topic can be identified given a document and a few topics.
Supervised coverage-based methods 
\cite{korenvcic2021CoverageEval}
match model-generated topics to a fixed, human-defined topics, though these methods are resource intensive.
More recently, LLM-based evaluations have emerged as a promising new paradigm. Studies \cite{stammbach2023revisitingLLM, rahimi2024contextualizedCoherence}
demonstrate that large language models can simulate human reasoning, providing nuanced judgments of topic coherence, word intrusions. 
\cite{yang2024llm} proposes a set of metrics to quantify the agreement between keywords generated from documents using LLM and topic words generated from documents by a topic model.
By leveraging LLMs’ extensive world knowledge and contextual reasoning, this approach overcomes the limitations of statistical co-occurrence, embedding similarities that often fail to capture semantic quality, and avoids the resource intensity of human-centric evaluations.
However, current LLM-based methods often focus on a single aspect, such as coherence. In contrast, our approach integrates multiple LLM-based metrics—including coherence, repetitiveness, diversity, and topic–document alignment—into a unified framework. Rather than simply measuring co-occurrence, our framework provides interpretable evidence (e.g., flagged outlier words and identified duplicate concepts) that explains topic flaws. Following \cite{chang2009HumanTeaLeaves, Blei2012PLDA}, topic model evaluations should assess the model's practical capabilities rather than rely on legacy metrics detached from its intended usage. Our novel topic–document alignment metrics explicitly reveal discrepancies between topic words and document content, which is crucial for applications like recommendation, summarization, and classification.
By integrating document-level and topic words-based assessments, our comprehensive, adversarially validated framework bridges the gap between statistical measures and human-centered evaluations, offering actionable insights to improve topic model performance.