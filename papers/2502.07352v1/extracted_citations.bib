@article{Blei2012PLDA,
author = {Blei, David M.},
title = {Probabilistic topic models},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/2133806.2133826},
doi = {10.1145/2133806.2133826},
abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
journal = {Commun. ACM},
month = apr,
pages = {77–84},
numpages = {8}
}

@article{Burkhardt2019TopicRedundancy,
  author  = {Sophie Burkhardt and Stefan Kramer},
  title   = {Decoupling Sparsity and Smoothness in the Dirichlet Variational Autoencoder Topic Model},
  journal = {Journal of Machine Learning Research},
  year    = {2019},
  volume  = {20},
  number  = {131},
  pages   = {1--27},
  url     = {http://jmlr.org/papers/v20/18-569.html}
}

@article{Griffiths2004Gibbs,
  author        = {Thomas L. Griffiths  and Mark Steyvers },
  title         = {Finding scientific topics},
  journal       = {Proceedings of the National Academy of Sciences},
  volume        = {101},
  number        = {suppl\_1},
  pages         = {5228-5235},
  year          = {2004},
  doi           = {10.1073/pnas.0307752101},
  URL           = {https://dyurovsky.github.io/learning-humans-machines/class/24-class/papers/griffiths2004.pdf},
}

@inproceedings{Hinton2009UndirectedTM,
 author = {Hinton, Geoffrey E and Salakhutdinov, Russ R},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Replicated Softmax: an Undirected Topic Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/31839b036f63806cba3f47b93af8ccb5-Paper.pdf},
 volume = {22},
 year = {2009}
}

@inproceedings{Kingma2014VAE,
  title = {{Auto-Encoding Variational Bayes}},
  added-at = {2020-10-15T14:36:56.000+0200},
  author = {Kingma, Diederik P. and Welling, Max},
  biburl = {https://www.bibsonomy.org/bibtex/242e5be6faa01cba2587f4907ac99dce8/annakrause},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  eprint = {http://arxiv.org/abs/1312.6114v10},
  eprintclass = {stat.ML},
  eprinttype = {arXiv},
  file = {:http\://arxiv.org/pdf/1312.6114v10:PDF;:KingmaWelling_Auto-EncodingVariationalBayes.pdf:PDF},
  interhash = {a626a9d77a123c52405a08da983203cb},
  intrahash = {42e5be6faa01cba2587f4907ac99dce8},
  keywords = {cs.LG stat.ML vae},
  timestamp = {2021-02-01T17:13:18.000+0100},
  year = 2014
}

@article{Korencic2018DocumenCoherence,
title = {Document-based topic coherence measures for news media text},
journal = {Expert Systems with Applications},
volume = {114},
pages = {357-373},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.07.063},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418304883},
author = {Damir Korenčić and Strahil Ristov and Jan Šnajder},
keywords = {Topic models, Topic coherence, Topic model evaluation, Text analysis, News text, Exploratory analysis},
abstract = {There is a rising need for automated analysis of news text, and topic models have proven to be useful tools for this task. However, as the quality of the topics induced by topic models greatly varies, much research effort has been devoted to their automated evaluation. Recent research has focused on topic coherence as a measure of a topic’s quality. Existing topic coherence measures work by considering the semantic similarity of topic words. This makes them unfit to detect the coherence of transient topics with semantically unrelated topic words, which abound in news media texts. In this paper, we introduce the notion of document-based topic coherence and propose novel topic coherence measures that estimate topic coherence based on topic documents rather than topic words. We evaluate the proposed measures on two datasets containing topics manually labeled for document-based coherence, on which the proposed measures outperform a strong baseline as well as word-based coherence measures. We also demonstrate the usefulness of document-based coherence measures for automated topic discovery from news media texts.}
}

@inproceedings{Newman2010TMforDL,
    author = {Newman, David and Noh, Youn and Talley, Edmund and Karimi, Sarvnaz and Baldwin, Timothy},
    title = {Evaluating topic models for digital libraries},
    year = {2010},
    isbn = {9781450300858},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1816123.1816156},
    doi = {10.1145/1816123.1816156},
    abstract = {Topic models could have a huge impact on improving the ways users find and discover content in digital libraries and search interfaces through their ability to automatically learn and apply subject tags to each and every item in a collection, and their ability to dynamically create virtual collections on the fly. However, much remains to be done to tap this potential, and empirically evaluate the true value of a given topic model to humans. In this work, we sketch out some sub-tasks that we suggest pave the way towards this goal, and present methods for assessing the coherence and interpretability of topics learned by topic models. Our large-scale user study includes over 70 human subjects evaluating and scoring almost 500 topics learned from collections from a wide range of genres and domains. We show how scoring model -- based on pointwise mutual information of word-pair using Wikipedia, Google and MEDLINE as external data sources - performs well at predicting human scores. This automated scoring of topics is an important first step to integrating topic modeling into digital libraries},
    booktitle = {Proceedings of the 10th Annual Joint Conference on Digital Libraries},
    pages = {215–224},
    numpages = {10},
    keywords = {evaluation, topic models, topic quality, user studies},
    location = {Gold Coast, Queensland, Australia},
    series = {JCDL '10}
}

@inproceedings{Nikolenko2016EmbeddingEvalTM,
author = {Nikolenko, Sergey I.},
title = {Topic Quality Metrics Based on Distributed Word Representations},
year = {2016},
isbn = {9781450340694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911451.2914720},
doi = {10.1145/2911451.2914720},
abstract = {Automated evaluation of topic quality remains an important unsolved problem in topic modeling and represents a major obstacle for development and evaluation of new topic models. Previous attempts at the problem have been formulated as variations on the coherence and/or mutual information of top words in a topic. In this work, we propose several new metrics for evaluating topic quality with the help of distributed word representations; our experiments suggest that the new metrics are a better match for human judgement, which is the gold standard in this case, than previously developed approaches.},
booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1029–1032},
numpages = {4},
keywords = {text mining, topic modeling, topic quality},
location = {Pisa, Italy},
series = {SIGIR '16}
}

@InProceedings{Terragni2021EmbedingTD,
author="Terragni, Silvia
and Fersini, Elisabetta
and Messina, Enza",
editor="M{\'e}tais, Elisabeth
and Meziane, Farid
and Horacek, Helmut
and Kapetanios, Epaminondas",
title="Word Embedding-Based Topic Similarity Measures",
booktitle="Natural Language Processing and Information Systems",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="33--45",
abstract="Topic models aim at discovering a set of hidden themes in a text corpus. A user might be interested in identifying the most similar topics of a given theme of interest. To accomplish this task, several similarity and distance metrics can be adopted. In this paper, we provide a comparison of the state-of-the-art topic similarity measures and propose novel metrics based on word embeddings. The proposed measures can overcome some limitations of the existing approaches, highlighting good capabilities in terms of several topic performance measures on benchmark datasets.",
isbn="978-3-030-80599-9"
}

@article{Zhang2018WHAI,
  title={WHAI: Weibull hybrid autoencoding inference for deep topic modeling},
  author={Zhang, Hao and Chen, Bo and Guo, Dandan and Zhou, Mingyuan},
  journal={arXiv preprint arXiv:1803.01328},
  year={2018}
}

@inproceedings{aletras2013TC,
    title = "Evaluating Topic Coherence Using Distributional Semantics",
    author = "Aletras, Nikolaos  and
      Stevenson, Mark",
    editor = "Koller, Alexander  and
      Erk, Katrin",
    booktitle = "Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Long Papers",
    month = mar,
    year = "2013",
    address = "Potsdam, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W13-0102",
    pages = "13--22",
}

@inproceedings{bhatia2017DocumentTopicEval,
    title = "An Automatic Approach for Document-level Topic Model Evaluation",
    author = "Bhatia, Shraey  and
      Lau, Jey Han  and
      Baldwin, Timothy",
    editor = "Levy, Roger  and
      Specia, Lucia",
    booktitle = "Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K17-1022",
    doi = "10.18653/v1/K17-1022",
    pages = "206--215",
    abstract = "Topic models jointly learn topics and document-level topic distribution. Extrinsic evaluation of topic models tends to focus exclusively on topic-level evaluation, e.g. by assessing the coherence of topics. We demonstrate that there can be large discrepancies between topic- and document-level model quality, and that basing model evaluation on topic-level analysis can be highly misleading. We propose a method for automatically predicting topic model quality based on analysis of document-level topic allocations, and provide empirical evidence for its robustness.",
}

@inproceedings{bhatia2018topicIntrusionEval,
    title = "Topic Intrusion for Automatic Topic Model Evaluation",
    author = "Bhatia, Shraey  and
      Lau, Jey Han  and
      Baldwin, Timothy",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1098",
    doi = "10.18653/v1/D18-1098",
    pages = "844--849",
    abstract = "Topic coherence is increasingly being used to evaluate topic models and filter topics for end-user applications. Topic coherence measures how well topic words relate to each other, but offers little insight on the utility of the topics in describing the documents. In this paper, we explore the topic intrusion task {---} the task of guessing an outlier topic given a document and a few topics {---} and propose a method to automate it. We improve upon the state-of-the-art substantially, demonstrating its viability as an alternative method for topic model evaluation.",
}

@inproceedings{bianchi2021CombinedTM,
    title = "Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence",
    author = "Bianchi, Federico  and
      Terragni, Silvia  and
      Hovy, Dirk",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.96",
    doi = "10.18653/v1/2021.acl-short.96",
    pages = "759--766",
    abstract = "Topic models extract groups of words from documents, whose interpretation as a topic hopefully allows for a better understanding of the data. However, the resulting word groups are often not coherent, making them harder to interpret. Recently, neural topic models have shown improvements in overall coherence. Concurrently, contextual embeddings have advanced the state of the art of neural models in general. In this paper, we combine contextualized representations with neural topic models. We find that our approach produces more meaningful and coherent topics than traditional bag-of-words topic models and recent neural models. Our results indicate that future improvements in language models will translate into better topic models.",
}

@inproceedings{bianchi2021EmbedingCentroidTD,
    title = "Cross-lingual Contextualized Topic Models with Zero-shot Learning",
    author = "Bianchi, Federico  and
      Terragni, Silvia  and
      Hovy, Dirk  and
      Nozza, Debora  and
      Fersini, Elisabetta",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.143",
    doi = "10.18653/v1/2021.eacl-main.143",
    pages = "1676--1683",
    abstract = "Many data sets (e.g., reviews, forums, news, etc.) exist parallelly in multiple languages. They all cover the same content, but the linguistic differences make it impossible to use traditional, bag-of-word-based topic models. Models have to be either single-language or suffer from a huge, but extremely sparse vocabulary. Both issues can be addressed by transfer learning. In this paper, we introduce a zero-shot cross-lingual topic model. Our model learns topics on one language (here, English), and predicts them for unseen documents in different languages (here, Italian, French, German, and Portuguese). We evaluate the quality of the topic predictions for the same document in different languages. Our results show that the transferred topics are coherent and stable across languages, which suggests exciting future research directions.",
}

@article{blei2003LDA,
  title         = {Latent dirichlet allocation},
  author        = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal       = {Journal of machine Learning research},
  volume        = {3},
  number        = {Jan},
  pages         = {993--1022},
  year          = {2003},
  URL           = {https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf?ref=http://githubhelp.com}
}

@article{blei2007CTM,
  title={A correlated topic model of Science},
  author={David M. Blei and John D. Lafferty},
  journal={The Annals of Applied Statistics},
  year={2007},
  volume={1},
  pages={17-35},
  url={https://projecteuclid.org/journals/annals-of-applied-statistics/volume-1/issue-1/A-correlated-topic-model-of-Science/10.1214/07-AOAS114.full}
}

@article{bouma2009NPMI,
  title={Normalized (pointwise) mutual information in collocation extraction},
  author={Bouma, Gerlof},
  journal={Proceedings of GSCL},
  volume={30},
  pages={31--40},
  year={2009},
  publisher={Potsdam}
}

@inproceedings{card2018SCHOLAR,
    title = "Neural Models for Documents with Metadata",
    author = "Card, Dallas  and
      Tan, Chenhao  and
      Smith, Noah A.",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1189",
    doi = "10.18653/v1/P18-1189",
    pages = "2031--2040",
    abstract = "Most real-world document collections involve various types of metadata, such as author, source, and date, and yet the most commonly-used approaches to modeling text corpora ignore this information. While specialized models have been developed for particular applications, few are widely used in practice, as customization typically requires derivation of a custom inference algorithm. In this paper, we build on recent advances in variational inference methods and propose a general neural framework, based on topic models, to enable flexible incorporation of metadata and allow for rapid exploration of alternative models. Our approach achieves strong performance, with a manageable tradeoff between perplexity, coherence, and sparsity. Finally, we demonstrate the potential of our framework through an exploration of a corpus of articles about US immigration.",
}

@inproceedings{chang2009HumanTeaLeaves,
 author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Boyd-graber, Jordan and Blei, David},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Reading Tea Leaves: How Humans Interpret Topic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/f92586a25bb3145facd64ab20fd554ff-Paper.pdf},
 volume = {22},
 year = {2009}
}

@article{dieng2020ETM,
    title = "Topic Modeling in Embedding Spaces",
    author = "Dieng, Adji B.  and
      Ruiz, Francisco J. R.  and
      Blei, David M.",
    editor = "Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.29",
    doi = "10.1162/tacl_a_00325",
    pages = "439--453",
}

@inproceedings{ding2018coherenceNTM,
    title = "Coherence-Aware Neural Topic Modeling",
    author = "Ding, Ran  and
      Nallapati, Ramesh  and
      Xiang, Bing",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1096",
    doi = "10.18653/v1/D18-1096",
    pages = "830--836",
    abstract = "Topic models are evaluated based on their ability to describe documents well (i.e. low perplexity) and to produce topics that carry coherent semantic meaning. In topic modeling so far, perplexity is a direct optimization target. However, topic coherence, owing to its challenging computation, is not optimized for and is only evaluated after training. In this work, under a neural variational inference framework, we propose methods to incorporate a topic coherence objective into the training process. We demonstrate that such a coherence-aware topic model exhibits a similar level of perplexity as baseline models but achieves substantially higher topic coherence.",
}

@article{grootendorst2022bertopic,
  title={BERTopic: Neural topic modeling with a class-based TF-IDF procedure},
  author={Grootendorst, Maarten},
  journal={arXiv preprint arXiv:2203.05794},
  year={2022}
}

@inproceedings{hofmann1999pLSI,
  title         = {Probabilistic latent semantic indexing},
  author        = {Hofmann, T},
  booktitle     = {Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval},
  year          = {1999},
  URL           = {https://sigir.org/wp-content/uploads/2017/06/p211.pdf}
}

@article{korenvcic2021CoverageEval,
  title={A topic coverage approach to evaluation of topic models},
  author={Koren{\v{c}}i{\'c}, Damir and Ristov, Strahil and Repar, Jelena and {\v{S}}najder, Jan},
  journal={IEEE access},
  volume={9},
  pages={123280--123312},
  year={2021},
  publisher={IEEE}
}

@inproceedings{lau2014machineTeaLeaves,
    title = "Machine Reading Tea Leaves: Automatically Evaluating Topic Coherence and Topic Model Quality",
    author = "Lau, Jey Han  and
      Newman, David  and
      Baldwin, Timothy",
    editor = "Wintner, Shuly  and
      Goldwater, Sharon  and
      Riezler, Stefan",
    booktitle = "Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2014",
    address = "Gothenburg, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E14-1056",
    doi = "10.3115/v1/E14-1056",
    pages = "530--539",
}

@InProceedings{miao2016NVDM,
  title = 	 {Neural Variational Inference for Text Processing},
  author = 	 {Miao, Yishu and Yu, Lei and Blunsom, Phil},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1727--1736},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/miao16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/miao16.html},
  abstract = 	 {Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks.}
}

@InProceedings{miao2017GSM-GSB-RSB,
  title = 	 {Discovering Discrete Latent Topics with Neural Variational Inference},
  author =       {Yishu Miao and Edward Grefenstette and Phil Blunsom},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2410--2419},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/miao17a/miao17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/miao17a.html},
}

@inproceedings{mimno2011optimizing,
    title = "Optimizing Semantic Coherence in Topic Models",
    author = "Mimno, David  and
      Wallach, Hanna  and
      Talley, Edmund  and
      Leenders, Miriam  and
      McCallum, Andrew",
    editor = "Barzilay, Regina  and
      Johnson, Mark",
    booktitle = "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland, UK.",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D11-1024",
    pages = "262--272",
}

@inproceedings{mu2024llmtm,
    title = "Large Language Models Offer an Alternative to the Traditional Approach of Topic Modelling",
    author = "Mu, Yida  and
      Dong, Chun  and
      Bontcheva, Kalina  and
      Song, Xingyi",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.887",
    pages = "10160--10171",
    abstract = "Topic modelling, as a well-established unsupervised technique, has found extensive use in automatically detecting significant topics within a corpus of documents. However, classic topic modelling approaches (e.g., LDA) have certain drawbacks, such as the lack of semantic understanding and the presence of overlapping topics. In this work, we investigate the untapped potential of large language models (LLMs) as an alternative for uncovering the underlying topics within extensive text corpora. To this end, we introduce a framework that prompts LLMs to generate topics from a given set of documents and establish evaluation protocols to assess the clustering efficacy of LLMs. Our findings indicate that LLMs with appropriate prompts can stand out as a viable alternative, capable of generating relevant topic titles and adhering to human guidelines to refine and merge topics. Through in-depth experiments and evaluation, we summarise the advantages and constraints of employing LLMs in topic extraction.",
}

@misc{mu2024llmtm-granularity-hallucination,
      title={Addressing Topic Granularity and Hallucination in Large Language Models for Topic Modelling}, 
      author={Yida Mu and Peizhen Bai and Kalina Bontcheva and Xingyi Song},
      year={2024},
      eprint={2405.00611},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.00611}, 
}

@inproceedings{nan2019TopicUniqueness,
    title = "Topic Modeling with {W}asserstein Autoencoders",
    author = "Nan, Feng  and
      Ding, Ran  and
      Nallapati, Ramesh  and
      Xiang, Bing",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1640",
    doi = "10.18653/v1/P19-1640",
    pages = "6345--6381",
    abstract = "We propose a novel neural topic model in the Wasserstein autoencoders (WAE) framework. Unlike existing variational autoencoder based models, we directly enforce Dirichlet prior on the latent document-topic vectors. We exploit the structure of the latent space and apply a suitable kernel in minimizing the Maximum Mean Discrepancy (MMD) to perform distribution matching. We discover that MMD performs much better than the Generative Adversarial Network (GAN) in matching high dimensional Dirichlet distribution. We further discover that incorporating randomness in the encoder output during training leads to significantly more coherent topics. To measure the diversity of the produced topics, we propose a simple topic uniqueness metric. Together with the widely used coherence measure NPMI, we offer a more wholistic evaluation of topic quality. Experiments on several real datasets show that our model produces significantly better topics than existing topic models.",
}

@article{newman2009distributed,
  title={Distributed algorithms for topic models.},
  author={Newman, David and Asuncion, Arthur and Smyth, Padhraic and Welling, Max},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={8},
  year={2009}
}

@inproceedings{newman2010automaticHuman,
    title = "Automatic Evaluation of Topic Coherence",
    author = "Newman, David  and
      Lau, Jey Han  and
      Grieser, Karl  and
      Baldwin, Timothy",
    editor = "Kaplan, Ron  and
      Burstein, Jill  and
      Harper, Mary  and
      Penn, Gerald",
    booktitle = "Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    month = jun,
    year = "2010",
    address = "Los Angeles, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N10-1012",
    pages = "100--108",
}

@inproceedings{papadimitriou1998LSI,
  title={Latent semantic indexing: A probabilistic analysis},
  author={Papadimitriou, Christos H and Tamaki, Hisao and Raghavan, Prabhakar and Vempala, Santosh},
  booktitle={Proceedings of the seventeenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems},
  pages={159--168},
  year={1998}
}

@inproceedings{pham2024topicgpt,
    title = "{T}opic{GPT}: A Prompt-based Topic Modeling Framework",
    author = "Pham, Chau  and
      Hoyle, Alexander  and
      Sun, Simeng  and
      Resnik, Philip  and
      Iyyer, Mohit",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.164",
    doi = "10.18653/v1/2024.naacl-long.164",
    pages = "2956--2984",
    abstract = "Topic modeling is a well-established technique for exploring text corpora. Conventional topic models (e.g., LDA) represent topics as bags of words that often require {``}reading the tea leaves{''} to interpret; additionally, they offer users minimal control over the formatting and specificity of resulting topics. To tackle these issues, we introduce TopicGPT, a prompt-based framework that uses large language models (LLMs) to uncover latent topics in a text collection. TopicGPT produces topics that align better with human categorizations compared to competing methods: it achieves a harmonic mean purity of 0.74 against human-annotated Wikipedia topics compared to 0.64 for the strongest baseline. Its topics are also more interpretable, dispensing with ambiguous bags of words in favor of topics with natural language labels and associated free-form descriptions. Moreover, the framework is highly adaptable, allowing users to specify constraints and modify topics without the need for model retraining. By streamlining access to high-quality and interpretable topics, TopicGPT represents a compelling, human-centered approach to topic modeling.",
}

@inproceedings{rahimi2024contextualizedCoherence,
    title = "Contextualized Topic Coherence Metrics",
    author = "Rahimi, Hamed  and
      Mimno, David  and
      Hoover, Jacob  and
      Naacke, Hubert  and
      Constantin, Camelia  and
      Amann, Bernd",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2024",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-eacl.123",
    pages = "1760--1773",
    abstract = "This article proposes a new family of LLM-based topic coherence metrics called Contextualized Topic Coherence (CTC) and inspired by standard human topic evaluation methods. CTC metrics simulate human-centered coherence evaluation while maintaining the efficiency of other automated methods. We compare the performance of our CTC metrics and five other baseline metrics on seven topic models and show that CTC metrics better reflect human judgment, particularly for topics extracted from short text collections by avoiding highly scored topics that are meaningless to humans.",
}

@inproceedings{roder2015CV,
    author = {R\"{o}der, Michael and Both, Andreas and Hinneburg, Alexander},
    title = {Exploring the Space of Topic Coherence Measures},
    year = {2015},
    isbn = {9781450333177},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2684822.2685324},
    doi = {10.1145/2684822.2685324},
    abstract = {Quantifying the coherence of a set of statements is a long standing problem with many potential applications that has attracted researchers from different sciences. The special case of measuring coherence of topics has been recently studied to remedy the problem that topic models give no guaranty on the interpretablity of their output. Several benchmark datasets were produced that record human judgements of the interpretability of topics. We are the first to propose a framework that allows to construct existing word based coherence measures as well as new ones by combining elementary components. We conduct a systematic search of the space of coherence measures using all publicly available topic relevance data for the evaluation. Our results show that new combinations of components outperform existing measures with respect to correlation to human ratings. nFinally, we outline how our results can be transferred to further applications in the context of text mining, information retrieval and the world wide web.},
    booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
    pages = {399–408},
    numpages = {10},
    keywords = {topic model, topic evaluation, topic coherence},
    location = {Shanghai, China},
    series = {WSDM '15}
}

@inproceedings{schnabel2015EmbeddingEvalTM,
    title = "Evaluation methods for unsupervised word embeddings",
    author = "Schnabel, Tobias  and
      Labutov, Igor  and
      Mimno, David  and
      Joachims, Thorsten",
    editor = "M{\`a}rquez, Llu{\'\i}s  and
      Callison-Burch, Chris  and
      Su, Jian",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1036",
    doi = "10.18653/v1/D15-1036",
    pages = "298--307",
}

@inproceedings{sia2020tired-cluster,
  title         = "Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too!",
  author        = "Sia, Suzanna  and
      Dalmia, Ayush  and
      Mielke, Sabrina J.",
  editor        = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
  booktitle     = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
  month         = nov,
  year          = "2020",
  address       = "Online",
  publisher = "Association for Computational Linguistics",
  url           = "https://aclanthology.org/2020.emnlp-main.135",
  doi           = "10.18653/v1/2020.emnlp-main.135",
  pages         = "1728--1736",
  abstract      = "Topic models are a useful analysis tool to uncover the underlying themes within document collections. The dominant approach is to use probabilistic topic models that posit a generative story, but in this paper we propose an alternative way to obtain topics: clustering pre-trained word embeddings while incorporating document information for weighted clustering and reranking top words. We provide benchmarks for the combination of different word embeddings and clustering algorithms, and analyse their performance under dimensionality reduction with PCA. The best performing combination for our approach performs as well as classical topic models, but with lower runtime and computational complexity.",
}

@inproceedings{srivastava2017ProdLDA-AVITM,
    title={Autoencoding Variational Inference For Topic Models},
    author={Akash Srivastava and Charles Sutton},
    booktitle={International Conference on Learning Representations},
    year={2017},
    url={https://openreview.net/forum?id=BybtVK9lg}
}

@inproceedings{stammbach2023revisitingLLM,
    title = "Revisiting Automated Topic Model Evaluation with Large Language Models",
    author = "Stammbach, Dominik  and
      Zouhar, Vil{\'e}m  and
      Hoyle, Alexander  and
      Sachan, Mrinmaya  and
      Ash, Elliott",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.581",
    doi = "10.18653/v1/2023.emnlp-main.581",
    pages = "9348--9357",
    abstract = "Topic models help us make sense of large text collections. Automatically evaluating their output and determining the optimal number of topics are both longstanding challenges, with no effective automated solutions to date. This paper proposes using large language models (LLMs) for these tasks. We find that LLMs appropriately assess the resulting topics, correlating more strongly with human judgments than existing automated metrics. However, the setup of the evaluation task is crucial {---} LLMs perform better on coherence ratings of word sets than on intrustion detection. We find that LLMs can also assist us in guiding us towards a reasonable number of topics. In actual applications, topic models are typically used to answer a research question related to a collection of texts. We can incorporate this research question in the prompt to the LLM, which helps estimating the optimal number of topics.",
}

@inproceedings{wang2008cDTM,
author = {Wang, Chong and Blei, David and Heckerman, David},
title = {Continuous time dynamic topic models},
year = {2008},
isbn = {0974903949},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {In this paper, we develop the continuous time dynamic topic model (cDTM). The cDTM is a dynamic topic model that uses Brownian motion to model the latent topics through a sequential collection of documents, where a "topic" is a pattern of word use that we expect to evolve over the course of the collection. We derive an efficient variational approximate inference algorithm that takes advantage of the sparsity of observations in text, a property that lets us easily handle many time points. In contrast to the cDTM, the original discrete-time dynamic topic model (dDTM) requires that time be discretized. Moreover, the complexity of variational inference for the dDTM grows quickly as time granularity increases, a drawback which limits fine-grained discretization. We demonstrate the cDTM on two news corpora, reporting both predictive perplexity and the novel task of time stamp prediction.},
booktitle = {Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence},
pages = {579–586},
numpages = {8},
location = {Helsinki, Finland},
series = {UAI'08}
}

@article{wang2019ATM,
title = {ATM: Adversarial-neural Topic Model},
journal = {Information Processing \& Management},
volume = {56},
number = {6},
pages = {102098},
year = {2019},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2019.102098},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319300500},
author = {Rui Wang and Deyu Zhou and Yulan He},
keywords = {Generative adversarial net, Neural-based topic model, Open domain event extraction, Topic modeling},
abstract = {Topic models are widely used for thematic structure discovery in text. But traditional topic models often require dedicated inference procedures for specific tasks at hand. Also, they are not designed to generate word-level semantic representations. To address the limitations, we propose a neural topic modeling approach based on the Generative Adversarial Nets (GANs), called Adversarial-neural Topic Model (ATM) in this paper. To our best knowledge, this work is the first attempt to use adversarial training for topic modeling. The proposed ATM models topics with dirichlet prior and employs a generator network to capture the semantic patterns among latent topics. Meanwhile, the generator could also produce word-level semantic representations. Besides, to illustrate the feasibility of porting ATM to tasks other than topic modeling, we apply ATM for open domain event extraction. To validate the effectiveness of the proposed ATM, two topic modeling benchmark corpora and an event dataset are employed in the experiments. Our experimental results on benchmark corpora show that ATM generates more coherence topics (considering five topic coherence measures), outperforming a number of competitive baselines. Moreover, the experiments on event dataset also validate that the proposed approach is able to extract meaningful events from news articles.}
}

@inproceedings{wang2020BAT,
    title = "Neural Topic Modeling with Bidirectional Adversarial Training",
    author = "Wang, Rui  and
      Hu, Xuemeng  and
      Zhou, Deyu  and
      He, Yulan  and
      Xiong, Yuxuan  and
      Ye, Chenchen  and
      Xu, Haiyang",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.32",
    doi = "10.18653/v1/2020.acl-main.32",
    pages = "340--350",
    abstract = "Recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text, since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as Latent Dirichlet Allocation (LDA). However, these models either typically assume improper prior (e.g. Gaussian or Logistic Normal) over latent topic space or could not infer topic distribution for a given document. To address these limitations, we propose a neural topic modeling approach, called Bidirectional Adversarial Topic (BAT) model, which represents the first attempt of applying bidirectional adversarial training for neural topic modeling. The proposed BAT builds a two-way projection between the document-topic distribution and the document-word distribution. It uses a generator to capture the semantic patterns from texts and an encoder for topic inference. Furthermore, to incorporate word relatedness information, the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT. To verify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are used in our experiments. The experimental results show that BAT and Gaussian-BAT obtain more coherent topics, outperforming several competitive baselines. Moreover, when performing text clustering based on the extracted topics, our models outperform all the baselines, with more significant improvements achieved by Gaussian-BAT where an increase of near 6{\%} is observed in accuracy.",
}

@inproceedings{wu2023ECRTM,
  title={Effective neural topic modeling with embedding clustering regularization},
  author={Wu, Xiaobao and Dong, Xinshuai and Nguyen, Thong Thanh and Luu, Anh Tuan},
  booktitle={International Conference on Machine Learning},
  pages={37335--37357},
  year={2023},
  organization={PMLR}
}

@article{yang2024llm,
  title         ={LLM Reading Tea Leaves: Automatically Evaluating Topic Models with Large Language Models},
  author        ={Yang, Xiaohao and Zhao, He and Phung, Dinh and Buntine, Wray and Du, Lan},
  journal       ={arXiv preprint arXiv:2406.09008},
  year          ={2024}
}

