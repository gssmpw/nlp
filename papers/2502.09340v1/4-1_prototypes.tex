\subsection{\texorpdfstring{\colorbox[RGB]{0, 191, 192}{Prototypes}}{Prototypes}}
\label{ssec:chall:proto}
The interpretability of \ppms depends on the number and quality of prototypes, i.e., how many prototypes are there in total and can humans understand what a prototype represents. 

\subsubsection{\texorpdfstring{\colorbox[RGB]{128, 223, 223}{Number}}{Number} of Prototypes}
\label{sssec:chall:proto:number}
In the \ppm architecture, the number of prototypes determines the size of the bottleneck layer.\footnote{While some architectures use more complex building blocks than single layers, the main argument still holds.} Finding the optimal number of prototypes is therefore similar to finding an optimal neural architecture~\cite{Elsken2019_survey-neural-arch-search}.

Early prototype models fixed the number of prototypes as a multiple of the number of classes (e.g., \citealt{Chen_2019_ThisLooksThat,Li_2024_ImprovingPrototypicalVisual}). This results in many redundant prototypes~\citep{Davoodi_2023_interpretabilitypartprototypebased} (see Figure \ref{fig:overview}f), because some object parts may discriminate between sets of classes rather than single classes. 
Therefore, extensions aim to reduce duplicate prototypes by pooling~\citep{Rymarczyk_2022_InterpretableImageClassification}, pruning~\citep{Rymarczyk_2021_ProtoPSharePrototypicalParts}, hierarchical ordering~\citep{Nauta_2021_NeuralPrototypeTrees}, or regularization on the number of prototypes~\citep{Nauta_2023_PIPNetPatchBasedIntuitive}.
While these approaches address the problem of prototype redundancy, they do not identify the optimal number of prototypes. A very small number of prototypes is easier for humans to interpret, but a larger number allows the model to learn more semantically meaningful prototypes~\cite{Davoodi_2023_interpretabilitypartprototypebased} (see Figure \ref{fig:overview}e).

\textit{
Reducing the number of prototypes and determining the optimal number of prototypes remain open challenges~\cite{Song_2024_MorphologicalPrototypingUnsupervised}. Solutions need to balance interpretability (less prototypes) and predictive performance (more prototypes)}.

\subsubsection{\texorpdfstring{\colorbox[RGB]{128, 223, 223}{Quality}}{Quality} of Prototypes}
\label{sssec:chall:proto:quality}
The quality of prototypes has two  aspects: \texorpdfstring{\colorbox[RGB]{191, 239, 239}{Semantics}}{Semantics}, i.e., whether a prototype is relevant to the task and understandable by humans, and \texorpdfstring{\colorbox[RGB]{191, 239, 239}{Similarity}}{Similarity}, i.e., whether the mapping from the image part to the prototype makes sense to humans.

\paragraph{\texorpdfstring{\colorbox[RGB]{191, 239, 239}{Semantics}}{Semantics}.}
\label{ssssec:chall:proto:quality:sem}
The interpretability of \ppms relies largely on the quality of the prototypes, currently hindered by the following semantic challenges:

\textbf{Prototype Interpretability:} 
\ppms classify an input image by comparing patches of the image to learned prototypes (see Prototype Matching in Figure \ref{fig:overview}). This matching does not specify what the model looks at (shape, color, texture) to assess similarity~\citep{Nauta_2021_NeuralPrototypeTrees}.
One approach to this problem is representing a prototype as multiple images parts (patches) instead of one~\citep{Ma_2023_ThisLooksThose}.
While insightful, this approach still relies on the user's own cognitive skills to infer which concept is being compared. Similarly, PIP-Net~\citep{Nauta_2023_PIPNetPatchBasedIntuitive} learns prototypes that represent semantically meaningful concepts and align with human intuition, but explicit semantics remain missing.


\textbf{Prototype Information:} 
Early models tend to learn prototypes that focus on background information, due to the inadvertently learned dataset biases~\citep{Chen_2019_ThisLooksThat,Nauta_2021_NeuralPrototypeTrees}.
Later work used humans-in-the-loop to adjust the learned prototype post-hoc, by removing confounded prototypes upon human inspection~\citep{Bontempelli_2023_ConceptlevelDebuggingPartPrototype}. 
Similarly,~\citet{Li_2024_ImprovingPrototypicalVisual} re-weight, re-select and re-train prototypes using a reward model that is trained on human preference feedback. However, both methods require the model to learn unambiguous concepts because ambiguous prototypes (i.e., prototypes that represent multiple concepts at once) make human evaluation difficult. 
In medical applications, \ppms struggle to accurately capture the region of interest\footnote{The most relevant concepts in an image needed for prediction.}~\citep{Pathak_2024_PrototypebasedInterpretableBreast}. This manifests in many irrelevant prototypes (see Figure~\ref{fig:overview}d), few pure\footnote{\citet{Nauta_2023_PIPNetPatchBasedIntuitive} defines purity as ``the fraction of image patches of a prototype that have overlap with the same ground-truth object part''.} and unique prototypes, and a spatial misalignment between the input image and visualization of prototypes (see Figure~\ref{fig:overview}b).

\textbf{Prototype Visualization:} 
A prototype is a vector in latent space. It is visualized by the rectangular patches from the training set, whose representations are close to the prototype. However, using rectangular patches to visualize prototypes does not always favor interpretability, 
as it encompasses multiple visual concepts, which can cause confusion in understanding exactly what the model is highlighting and introduces the risk of a confirmation bias~\cite{Alpherts2024_facct_perceptive-visual-urban-analytics}.

\paragraph{\texorpdfstring{\colorbox[RGB]{191, 239, 239}{Similarity}}{Similarity}.}
\label{ssssec:chall:proto:quality:sim}
\ppms infer similarity between part of the input and a prototype by comparing vector representations in latent space and calculating a similarity score. However, such a score lacks explicit and clear semantics for the inferred similarity~\citep{Hong_2023_ProtoryNetInterpretableText} (see Figure~\ref{fig:overview}c).
In particular, when visualized in the input space, prototypes may seem to activate on input parts that are semantically dissimilar for humans~\citep{Donnelly_2022_DeformableProtoPNetInterpretable,Hong_2023_ProtoryNetInterpretableText}. This gap in similarity perception is quantified by~\citet{Kim_2022_HIVEEvaluatingHuman}, revealing a clear misalignment in judgment between humans and \ppms.


\textit{The quality of prototypes is a major challenge, potentially limiting the broader usage of \ppms. High-quality prototypes should be diverse, represent semantically meaningful concepts that humans understand, and activate on the part of the input that humans perceive as semantically similar.}
