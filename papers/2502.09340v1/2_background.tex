\section{Background on Part-Prototype Models}
\label{sec:background}
We introduce part-prototype models (\ppms) using the seminal work on ProtoPNet~\cite{Chen_2019_ThisLooksThat} as example.
In ProtoPNet, the decision-making process follows the principle of ``this looks like that,'' where the input image is compared to learned prototypes
and the final decision is based on the prototypes only (cf. Figure~\ref{fig:overview}, top row).


\paragraph{ProtoPNet.}
ProtoPNet consists of a convolutional neural network (CNN) to encode the input, a prototype layer to match the input with the prototypes, and a fully connected layer to compute the final decision. 

The CNN is trained to map the input image onto a latent space of dimensionality $W\times H\times D$. The prototype layer acts as a bottleneck and consists of $K$ prototype representations, each of dimensionality $1\times 1\times D$. 
The representations of all $K$ prototypes are compared to the representations at each of the $W\times H$ locations of the CNN's latent grid.
These latent grid locations can be mapped back to pixel regions (patches) in the input image. 
Thus, the prototype layer outputs similarity scores that reflect the similarity of all input patches to all prototypes.
The highest similarity score per prototype (corresponding to a particular input patch) is passed to the final layer, which computes the classification output.
The weights of the final layer are constrained to be positive numbers, resulting in a decision that is a positive linear combination of prototype similarities, acting as a scoring sheet.

ProtoPNet is trained iteratively in three steps: i) stochastic gradient descent (SGD) of the CNN and the prototype layer (the final layer is frozen), ii) projection of prototypes to training image patches, and iii) convex optimization of the final layer with the CNN and prototype layer frozen. The training loss minimizes the classification loss and encourages representations of latent patches to be close to prototype of their class, and far from prototypes of other classes.
The projection in step ii) sets the prototype representation to the representation of the training patch closest in latent space.

ProtoPNet is considered inherently interpretable, because (i) the decision layer is a simple linear model that is easy to analyze, and (ii) the decision is based solely on the prototypes, which in turn (iii) reflect meaningful and representative parts of the data (see Figure~\ref{fig:overview}).

\paragraph{Extensions.}
Multiple extensions of ProtoPNet have been introduced, e.g., to further improve the interpretability of prototypes by ensuring their disentanglement~\citep{Wang_2021_InterpretableImageRecognition}, making them context aware~\citep{Donnelly_2022_DeformableProtoPNetInterpretable} or grouping them in the latent space~\citep{Ma_2023_ThisLooksThose}. 
Also with the goal to ease interpretability,~\citealt{Rymarczyk_2021_ProtoPSharePrototypicalParts,Nauta_2021_NeuralPrototypeTrees,Rymarczyk_2022_InterpretableImageClassification} focus on limiting the number of prototypes through adopting class-agnostic prototypes. 
Further extensions include the integration of prototypes into transformer architectures~\citep{Xue_2024_ProtoPFormerConcentratingPrototypical,Ma_2024_Interpretableimage} and the application to other modalities beyond vision (e.g., \citealt{Wang_2023_PROMINETPrototypebasedMultiView}).

