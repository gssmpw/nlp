\section{Research Directions}
\label{sec:directions}
Based on our analysis of the open challenges we synthesized five main research directions for future work (see Figure~\ref{fig:directions}) on part-prototype models (\ppms).
We describe each direction, outline some ideas and note which challenges they address. 

\begin{figure}
 \centering
         \vspace{1em}
        \includegraphics[width=1.0\linewidth,trim={0 0 0 0},clip]{figures/future-directions-drawing.pdf}
        \vspace{1em}
    \caption{Principle directions for future research. In addition to technical and theoretical research (top row), it is important to address human-centered issues (bottom row).}
    \label{fig:directions}
\end{figure}

\subsection{Performance Competitive to Black Boxes}
\label{ssec:directions:performance}
\ppms are limited in their expressiveness because they make decisions based on a fixed number of fixed-size localized image features (prototypes) and do not model their interrelationships. To increase the expressiveness of the model, future work should focus on relaxing these constraints. For example, multiple layers of the backbone CNN can be used to obtain prototypes corresponding to different types of visual features, such as color, shape, and higher-level object parts~\cite{Wang_2024_MCPNetInterpretableClassifier}. The prototypes should also be of different sizes and shapes as shown in previous work~\cite{Donnelly_2022_DeformableProtoPNetInterpretable}. 
Other research directions could focus on modeling spatial relationships between prototypes, e.g. using graph neural networks, and modeling hierarchical relationships to recognize objects even when they are partially occluded. Finally, the number of prototypes can be automatically optimized during training by adding and removing prototypes using ideas from pruning techniques.

Research in these directions addresses \texorpdfstring{\colorbox[RGB]{156,198,246}{Performance}}{Performance} 
and \texorpdfstring{\colorbox[RGB]{128, 223, 223}{Number}}{Number} of prototypes, %
and would make \ppms more likely to be adopted by stakeholders who prioritize performance over interpretability in ML systems. 


\subsection{Novel Architectures Grounded in Theory}
\label{ssec:directions:architectures}
\ppms are not intrinsically interpretable in practice, because prototypes are learned in an unsupervised manner, and therefore lack human-understandable semantics.
An important direction for future research is to clearly define prototypes for all modalities. The most concrete definition is available for vision tasks: prototypes are commonly defined as ``visual concepts'' corresponding to any part of the image to which humans could assign a label (e.g., feather). However, there is no clear definition of a visual concept with respect to the optimal level of granularity (e.g., a barb is part of a feather is part of a wing is part of a bird). Generally, the definition is that a prototypical concept is an element that is ``a reasonably small and sufficiently large part of the input that has some meaning to humans'' and is hardly actionable.
Rigorous definitions of prototypes can be derived from findings in human information processing, such as human visual perception, linguistics~\cite{panther2008prototype}, and cognitive science. Based on these definitions, we could then design novel \ppm architectures that reflect the human part-of relation, and guide the model (e.g., with few annotations) to learn the correct type of prototypes for different modalities.
\ppms for modalities other than vision are currently rare (cf. table at the top of Figure~\ref{fig:chart}). Hence, more guidelines on what constitutes a prototype for a certain modality could foster the development of not only single-modal models for other modalities but also multi-modal models. One of such models, recently proposed by \cite{DeSanti2024_xailbr_pip3d-alzheimer}, successfully combines prototypes for structured patient data (age) and 3D imaging data (brain computer tomography) into a multi-modal \ppm.

A clearer definition of prototypes would not only address the current criticism that prototypes are not interpretable (\texorpdfstring{\colorbox[RGB]{191, 239, 239}{Semantics}}{Semantics})
and ground models in theory (\texorpdfstring{\colorbox[RGB]{156,198,246}{Theoretical Foundation}}{Theoretical Foundation}) %
but also potentially improve \texorpdfstring{\colorbox[RGB]{156,198,246}{Performance}}{Performance},
and make models more trustworthy and applicable in a wider range of scenarios ({\texorpdfstring{\colorbox[RGB]{255,199,166}{Tasks}}{Tasks}},
\texorpdfstring{\colorbox[RGB]{242,96,119}{Safety and Use in Practice}}{Safety and Use in Practice}).


\subsection{Frameworks for Human-AI Collaboration}
\label{ssec:directions:interactive}
\ppms are not always useful in practice, either due to common ML problems (e.g., OOD generalization, shortcut learning~\citep{Geirhos2020_ShortcutLearningDeep}) or due to learning features that are counterintuitive or irrelevant for humans. 
An important direction is to extend the work on interactive \ppms (prior work~\cite{Bontempelli_2023_ConceptlevelDebuggingPartPrototype,Li_2024_ImprovingPrototypicalVisual}) to allow domain experts to adjust the reasoning process of the models. This includes adding relevant prototypes, removing irrelevant or redundant prototypes, removing prototypes corresponding to shortcuts, and adjusting the weights of prototypes.
It would also be interesting to inject domain knowledge prior to training to provide the model with a priori guidance on what is (or is not) important for the task from a human perspective.


We believe that adaptable \ppms would increase user trust (\texorpdfstring{\colorbox[RGB]{242,96,119}{Safety and Use in Practice}}{Safety and Use in Practice})
since they would align more with human reasoning (\texorpdfstring{\colorbox[RGB]{191, 239, 239}{Semantics}}{Semantics}, %
\texorpdfstring{\colorbox[RGB]{255,199,166}{Assumptions}}{Assumptions}),
help keep models up-to-date, improve out-of-distribution generalization, and potentially improve overall model \texorpdfstring{\colorbox[RGB]{156,198,246}{Performance}}{Performance}.

\subsection{Model Alignment}
\label{ssec:directions:alignment}

For humans, some prototypes and activations do not look similar (see Section~\ref{ssssec:chall:proto:quality:sim}, and Figure~\ref{fig:overview}). The similarity of prototypes is computed in latent space, which does not guarantee fidelity~\cite{Xu-Darme_2023_Sanitycheckspatch}), and it remains unclear what the similarity score represents: Is the color important or the shape? Or maybe they both should be ignored?

Future research should focus on metrics beyond cosine similarity, which is an unreliable metric for similarity~\cite{Steck_2024_CosineSimilarityEmbeddingsReally}. 
To explain similarities of an input to a learned prototype, natural language explanations (e.g., ``this is a bird's head with orange and blue coloring'') can be generated using vision-language models~\cite{feldhus_2023_SaliencyMaps}).
However, those are post-hoc rationals, not necessarily faithful to the model's internal decision process, and only show what the vision-language model ``sees'' in a prototype.
An alternative option that is faithful to the model is to use controlled perturbations in the input space (e.g., changing the color) and assess whether the prototype would still be activated~\cite{Nauta_2021_ThisLooksThat}, or derive visualizations that capture the essence of the prototypes, such as sketching, background removal, or improved localization). Furthermore, disentangling the different visual features (color, shape, and texture) and learning prototypes that represent each information separately could improve the clarity of explanations~\cite{Pach_2024_LucidPPNUnambiguousPrototypical}.
Finally, an interesting direction would be to combine the interpretability of semantic features as used in concept-bottleneck models~\citep{Koh2000_concept-bottleneck-models} and the ``part-of'' idea of \ppms.

Research in this direction aims to improve the model's alignment with humans (\texorpdfstring{\colorbox[RGB]{191, 239, 239}{Similarity}}{Similarity}) %
and to increase trust for using them safely in applications (\texorpdfstring{\colorbox[RGB]{242,96,119}{Safety and Use in Practice}}{Safety and Use in Practice}).
Developing human-aligned similarity metrics would have implications beyond \ppms.

\subsection{Metrics and Benchmarks}
\label{ssec:directions:benchmarks}
\ppms claim to be human understandable. However, human understanding depends on the expertise and background knowledge of specific stakeholders and is inherently difficult to measure~\cite{Boogert_2018_Measuringunderstandingindividual}. This is especially true given that XAI, as a relatively young field of research, does not have an established evaluation methodology~\cite{Nauta2023_csur_evaluating-xai-survey}.
Therefore, an important next step is to consolidate evaluation metrics. This comprises the comparison, revision and adaption of existing metrics including standard XAI evaluation metrics~\cite{Nauta2023_csur_evaluating-xai-survey}, specific metrics for \ppms~\cite{Nauta2023_wcxai_co-12-for-prototype-models,Huang_2023_EvaluationImprovementInterpretability}, and domain-specific metrics (e.g., ~\cite{Pathak_2024_PrototypebasedInterpretableBreast}).
Specifically designed synthetic benchmarks (such as FunnyBirds for fine-grained image recognition~\cite{Hesse_2023_FunnyBirdsSyntheticVision}), and additional benchmark datasets for other machine learning tasks and input modalities could provide insight into \ppms' failure modes and support model improvement. 
For faster research cycles, metrics and datasets should be integrated into well-established evaluation framework~\citep{Le2023_ijcai_benchmarking-xai}.

Research in this direction directly addresses the challenge of evaluating \ppms (\texorpdfstring{\colorbox[RGB]{156,198,246}{Benchmark and Evaluation}}{Benchmark and Evaluation}),
and contributes to validating and improving \texorpdfstring{\colorbox[RGB]{156,198,246}{Performance}}{Performance}. %
Moreover, domain-specific and problem-centric metrics and benchmarks would make \ppms more trustworthy and safer to use in applications (\texorpdfstring{\colorbox[RGB]{242,96,119}{Safety and Use in Practice}}{Safety and Use in Practice}).
