\subsection{\texorpdfstring{\colorbox[RGB]{58,142,237}{Methodology}}{Methodology}}
\label{ssec:chall:method}
From training and inference to evaluation and theoretical analysis, the development of \ppms poses multiple methodological challenges for the community.

\subsubsection{\texorpdfstring{\colorbox[RGB]{156,198,246}{Theoretical Foundation}}{Theoretical Foundation}}
\label{sssec:chall:method:tf}
Recent work on \ppms contains less theoretical analysis than earlier papers, such as~\citep{Chen_2019_ThisLooksThat}. Even when theory does appear, it is limited to properties of prototypical part representations, after the projection phase.
\citet{Hong_2023_ProtoryNetInterpretableText} calls for a mathematical formalization and enforcement of well-established requirements for linguistic prototypes, but we consider it to be a generally relevant desideratum for all kinds of prototypes. In the example of a linguistic prototype, according to the conditions proposed by~\citet{panther2008prototype}, the prototype must be an affirmative declarative sentence, where the subject is in the nominative case, the verb in the active voice and in the indicative mood.

\textit{The theoretical understanding  should be significantly deepened, using the theory-rich requirements per modality to increase interpretability and, perhaps performance.}

\subsubsection{Training and Inference \texorpdfstring{\colorbox[RGB]{156,198,246}{Performance}}{Performance}}
\label{sssec:chall:method:perf}

Despite their advantages in terms of interpretability, \ppms suffer from practical limitations that affect training and inference.
Training consists of several steps in which different parts of these models are trained~\citep{Zhang_2022_ProtGNNSelfExplainingGraph}. It often uses a large number of hyperparameters~\citep{Ruis_2021_IndependentPrototypePropagation}, which require careful tuning~\citep{Rymarczyk_2022_InterpretableImageClassification}. Therefore, training may take longer~\citep{Alpherts2024_facct_perceptive-visual-urban-analytics} and be less stable than in the case of black box models. In addition, inadequate regularization~\citep{Bontempelli_2023_ConceptlevelDebuggingPartPrototype} can lead to overfitting and poor generalization~\citep{Carmichael_2024_ThisProbablyLooks}.
During inference, these models often process input more slowly than black box models~\citep{Fauvel_2023_LightweightEfficientExplainablebyDesign}. Moreover, their performance degrades in the presence of noise or image transformations~\citep{Patricio2024_acm-csur_XAI-medical-image-classification,Nauta2023_wcxai_co-12-for-prototype-models}.
Finally, due to the randomness in training, there are significant inconsistencies in explanations generated across different runs (i.e., different prototypes are found across different runs) ~\citep{Nauta2023_wcxai_co-12-for-prototype-models}.

\textit{\ppms have weaknesses in training and inference performance, and lack stability and robustness.}


\subsubsection{\texorpdfstring{\colorbox[RGB]{156,198,246}{Benchmark and Evaluation}}{Benchmark and Evaluation}}
\label{sssec:chall:method:bench}
\ppms are evaluated in two aspects. The first is performance, as with standard models, and the second is explainability.
In terms of performance, \ppms are typically compared on well-formed (balanced, IID) benchmarks using accuracy as a performance measure, which lacks the realistic challenges, such as out-of-distribution data, for which overconfident estimates are often obtained~\citep{Nauta2023_wcxai_co-12-for-prototype-models}.
The second aspect, explainability, is assessed both quantitatively and qualitatively. Quantitative analysis assesses various aspects of explanations through automated proxy measures, such as their consistency across images and robustness~\citep{Huang_2023_EvaluationImprovementInterpretability}, or spatial misalignment~\citep{Sacha_2024_Interpretabilitybenchmarkevaluating}. However, these simplified proxy measures do not generalize well to complex setups, such as comparing prototype representations of two different models or their ensembles~\citep{Keswani_2022_Proto2ProtoCanyou}, as well as comparing with competing approaches.
Overall, there is no consensus in the community on how to evaluate the quality of explanations derived from \ppms~\citep{Nauta2023_wcxai_co-12-for-prototype-models,Nauta2023_csur_evaluating-xai-survey}. 
Evaluation with domain experts on real target tasks is considered the ``best way'' to assess explanations~\citep{doshi2017towards}, but is rarely used by the community due to its prohibitive cost and effort~\citep{Nauta2023_csur_evaluating-xai-survey}.
Finally, the lack of attribute datasets (with object part annotations) hinders both qualitative and quantitative evaluation of these architectures~\citep{Ruis_2021_IndependentPrototypePropagation}.

\textit{To effectively evaluate \ppms, it is essential to extend performance metrics beyond accuracy and establish consistent criteria for assessing explainability.}
