The growing interest in eXplainable Artificial Intelligence (XAI) has prompted research into models with built-in interpretability, the most prominent of which are part-prototype models.
Part-Prototype Models (PPMs) make decisions by comparing an input image to a set of learned prototypes, providing human-understandable explanations in the form of ``this looks like that''.
Despite their inherent interpretability, \ppms are not yet considered a valuable alternative to post-hoc models.
In this survey, we investigate the reasons for this and provide directions for future research.
We analyze papers from 2019 to 2024, and derive a taxonomy of the challenges that current \ppms face. 
Our analysis shows that the open challenges are quite diverse. The main concern is the quality and quantity of prototypes. Other concerns are the lack of generalization to a variety of tasks and contexts, and general methodological issues, including non-standardized evaluation. 
We provide ideas for future research in five broad directions: improving predictive performance, developing novel architectures grounded in theory, establishing frameworks for human-AI collaboration, aligning models with humans, and establishing metrics and benchmarks for evaluation.
We hope that this survey will stimulate research and promote intrinsically interpretable models for application domains.
Our list of surveyed papers is available at \url{https://github.com/aix-group/ppm-survey}.









