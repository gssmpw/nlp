\section{Experimental Results}
\label{sec:results}
\subsection{Experiment Setup}
We implemented PICBench in Python.
The quality of the given PIC design was evaluated using the open-source simulation tool SAX, which specializes in S-parameter-based circuit simulations.
We constructed the S-parameters for essential devices, including waveguides, couplers, MMIs, MZIs, MRRs, and phase shifters, to simulate the frequency-domain response of the specified PIC over the wavelength range of 1510 to 1590 nm.

PICBench is compatible with a wide range of LLMs as long as they provide a Python API. 
In our experiment, we extensively evaluated the capabilities of five notable LLMs developed by leading companies, using PICBench: 
\begin{itemize}
    \item \textbf{GPT-4}: The free commercial model developed by OpenAI.
    \item \textbf{GPT-4o}: The flagship commercial model that is widely recognized for its versatility and high accuracy across various domains.
    \item \textbf{GPT-o1-mini}: A smaller GPT-based model optimized for STEM reasoning, with a focus on mathematical and technical problem-solving.
    \item \textbf{Gemini 1.5 Pro}: Developed by Google, an emerging model that integrates hybrid architectures to improve code generation and context handling.
    \item \textbf{Claude 3.5 Sonnet}: Developed by Anthropic, emphasizing significant improvement in graduate-level reasoning, knowledge acquisition, and coding abilities.
\end{itemize}

We adopted the widely used Pass@$k$ metric \cite{chen2021evaluating} to measure code generation correctness.
A problem is considered solved if any of the $k$-generated samples passes the corresponding unit tests.
For each task, n samples (default $n=5$) are generated, of which c samples pass, and an unbiased estimator of Pass@k is computed as:
    \begin{equation}
        \text{pass@k} := \mathbb{E}_{\text{Problems}} \left[ 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}} \right].
    \label{eq:passk}
    \end{equation}
    
To analyze the impact of error feedback (EF), we queried the selected LLMs both without and with feedback for $n$ iterations where we set $n=1$ and $n=3$.   

\subsection{Results}

\begin{table*}[h!]
\caption{The Syntax and Functionality (Func.) evaluation for different LLMs. EF denotes the error feedback.}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccccccccc}
\toprule
\multirow{4}{*}{\textbf{LLM}} & \multicolumn{6}{c}{\textbf{Pass@1}} & \multicolumn{6}{c}{\textbf{Pass@5}} \\ 
\cmidrule(lr){2-7} \cmidrule(lr){8-13}
& \multicolumn{2}{c}{\textbf{Without EF}} & \multicolumn{2}{c}{\textbf{With 1 EFs}} & \multicolumn{2}{c}{\textbf{With 3 EFs}} 
& \multicolumn{2}{c}{\textbf{Without EF}} & \multicolumn{2}{c}{\textbf{With 1 EFs}} & \multicolumn{2}{c}{\textbf{With 3 EFs}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
& Syntax & Func. & Syntax & Func. & Syntax & Func. 
& Syntax & Func. & Syntax & Func. & Syntax & Func. \\
\midrule

GPT-4 & \bf{16.67} & 6.67 & 34.17 & 6.67 & 54.17 & 10.83 & \bf{41.67} & 12.50 & \bf{70.83} & 16.67 & \bf{100.00} & 29.17 \\
GPT-o1-mini & 8.33 & 4.17 & 33.33 & 15.00 & 63.33 & 23.33 & 29.17 & \bf{16.67} & 66.67 & \bf{25.00} & 91.67 & 33.33 \\
GPT-4o & 14.17 & 4.17 & \bf{40.83} & 15.00 & 59.17 & 20.00 & 37.50 & 4.17 & \bf{70.83} & \bf{25.00} & 87.50 & \bf{41.67} \\
Claude 3.5 Sonnet & 13.33 & 1.67 & 35.83 & 14.17 & \bf{75.83} & \bf{24.17} & 20.83 & 8.33 & \bf{70.83} & 20.83 & \bf{100.00} & 37.50 \\
Gemini 1.5 pro & 9.17 & \bf{8.33} & 33.33 & \bf{16.67} & 50.00 & 20.83 & 16.67 & 12.50 & 66.67 & 20.83 & 87.50 & 33.33 \\

\bottomrule
\end{tabular}}
\label{tab:wo_restrictions}
\end{table*}


\begin{table*}[h!]
\caption{The Syntax and Functionality (Func.) correctness evaluation for different LLMs with our proposed restrictions. EF denotes the error feedback.}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccccccccc}
\toprule
\multirow{4}{*}{\textbf{LLM}} & \multicolumn{6}{c}{\textbf{Pass@1}} & \multicolumn{6}{c}{\textbf{Pass@5}} \\ 
\cmidrule(lr){2-7} \cmidrule(lr){8-13}
& \multicolumn{2}{c}{\textbf{Without EF}} & \multicolumn{2}{c}{\textbf{With 1 EFs}} & \multicolumn{2}{c}{\textbf{With 3 EFs}} 
& \multicolumn{2}{c}{\textbf{Without EF}} & \multicolumn{2}{c}{\textbf{With 1 EFs}} & \multicolumn{2}{c}{\textbf{With 3 EFs}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
& Syntax & Func. & Syntax & Func. & Syntax & Func. 
& Syntax & Func. & Syntax & Func. & Syntax & Func. \\
\midrule

GPT-4 + restrictions & 20.00 & 4.17 & 38.33 & 17.50 & 71.67 & 30.00 & 58.33 & 12.50 & 58.33 & 20.83 & \bf{100.00} & 45.83 \\
GPT-o1-mini + restrictions & 13.33 & 9.17 & 50.00 & 22.50 & 84.17 & 33.33 & 25.00 & 12.50 & 79.17 & 33.33 & \bf{100.00} & 50.00 \\
GPT-4o + restrictions & 60.83 & 20.00 & 86.67 & 31.67 & \bf{95.00} & 36.67 & 87.50 & \bf{37.50} & \bf{100.00} & 37.50 & \bf{100.00} & 50.00 \\
Claude 3.5 Sonnet + restrictions & 54.17 & 15.83 & 80.83 & 29.00 & 90.00 & \bf{43.33} & 87.50 & \bf{37.50} & \bf{100.00} & \bf{45.83} & \bf{100.00} & \bf{62.50} \\
Gemini 1.5 pro + restrictions & \bf{64.17} & \bf{21.67} & \bf{88.33} & \bf{32.50} & \bf{95.00} & 38.33 & \bf{91.67} & \bf{37.50} & \bf{100.00} & \bf{45.83} & \bf{100.00} & 54.17 \\

\bottomrule
\end{tabular}}
\label{tab:w_restrictions}
\end{table*}

\subsubsection{Impact of error feedback}
\label{sec:error_feedback}

\Cref{tab:wo_restrictions} presents the comprehensive evaluation results for both syntax and functionality across all five selected LLMs, using 0, 1, and 3 error feedback iterations with PICBench, evaluated through the Pass@$k$ metric.

Without correction feedback, GPT-4 demonstrates the highest syntax accuracy, with 16.67\% for Pass@1 and 41.67\% for Pass@5, establishing its strength in pattern recognition and abstraction capabilities.

When error feedback is incorporated, there is a clear improvement in both syntax and functionality scores across Pass@1 and Pass@5 evaluations.
Claude 3.5 Sonnet demonstrated the most significant improvement in both syntax performance and functionality in response to feedback, highlighting its excellent self-correction and adaptive learning capabilities. 
At Pass@1, its syntax score increased from 13.33\% without feedback to 35.83\% with one feedback iteration, and further soared to 75.83\% with three iterations. 
Similarly, its functionality success rate improved from 1.67\% to 24.17\%.

Moreover, even one round of feedback can raise Pass@1 metrics beyond the Pass@5 results obtained without any feedback.
This further emphasizes the impact of feedback on model performance. 
For example, Gemini 1.5 pro achieves a syntax score of 33.33\% with one feedback iteration at Pass@1, which is notably higher than its Pass@5 syntax score without feedback (16.67\%).

Overall, the feedback method, aided by simulator diagnostics, rapidly accelerates debugging and enables iterative improvements in code generation. 
By revealing each model’s evolving logic and capacity for self-refinement, this approach delivers notable gains in both code quality and reliability.

\subsubsection{Impact of restrictions}
To investigate the impact of the restrictions introduced in \Cref{sec:Error_class}, we queried the selected LLMs using the same approach described in \Cref{sec:error_feedback}.

As shown in the \Cref{tab:w_restrictions}, the application of restrictions greatly enhances the performance of LLMs across all evaluated conditions, with notable improvements in both syntax and functionality. 
While all models benefit, the degree of improvement varies, with Gemini 1.5 pro showing the most dramatic enhancements across both syntax and functionality dimensions. 
For instance, Gemini 1.5 pro’s syntax score improves from 9.17\% to 64.17\% in Pass@1 and from 16.67\% to 91.67\% in Pass@5, while its functionality score increases from 8.33\% to 21.67\% in Pass@1 and from 12.50\% to 37.50\% in Pass@5 with restrictions applied, demonstrating its robust in-context reasoning and real-time adaptability.
The results highlight the potential for further optimization by leveraging the in-context learning ability of LLMs, enhanced with high-definition circuit knowledge.




