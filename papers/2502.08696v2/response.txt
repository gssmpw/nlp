\section{Related Work}
\label{sec:rel}
\textbf{Neural Optimization:} %Neural network-based optimization is a widely studied field. Here, a neural network is used to approximate the optimal solution $\eta$ that minimizes an objective function, expressed as $\min_\eta  O(\eta)$, where $\eta$ can represent either a function or a variable. Typically, this optimization process is conducted without relying on training data.
Besides their predominance in supervised and unsupervised learning tasks, neural networks become an increasingly popular choice for a wide range of data-free optimization tasks, i.e.~scenarios where an objective function can be explicitly expressed rather than implicitly via data samples.
In Physics Informed Neural Networks \textbf{Raissi, "Physics-Informed Neural Architectures in the Prediction of Chaotic Dynamical Systems"} models are trained to represent the solutions of differential equations. Here the loss function measures the adherence of the solution quality.
Similarly, \textbf{Sirignano, "DGM: A Deep Learning Framework for Solving Nonlinear PDEs"} propose a neural optimization approach for generating shapes under geometric constraints. %, where a geometry is represented by a neural network. %In the context of UCO, $\eta \in \{ 0,1 \}^N$ is defined as a vector of binary variables. As outlined in Sec.~\ref{Sec:problem_desc}, this is often conceptualized as a Neural Probabilistic Optimization (NPO) problem, wherein the objective is to approximate a target distribution using a neural network.
%Various applications of NPO have been explored in recent literature. \textbf{Huang et al., "A Deep Learning Framework for Solving Nonlinear PDEs"} employ autoregressive models to approximate discrete probability distributions associated with Ising models.
Recently, there has been increasing interest in using probabilistic generative models to generate solutions to neural optimization. Here the learned models do not directly represent a solution but rather a probability distribution over the solution space. We refer to this endeavor as Neural Probabilistic Optimization (NPO). In the following, we discuss two important NPO application areas in discrete domains.



%Recently diffusion models have been applied in continuous NPO problems that arise in various computational physics and statistics applications \textbf{Liu et al., "Diffusion-based Generative Models for Solving Nonlinear PDEs"}.

\textbf{Neural Combinatorial Optimization:} Neural CO aims at generating high-quality solutions to CO problems time-efficiently during inference time.  The goal is to train a generative model to generate solutions to a given CO problem instance on which it is conditioned. Supervised CO \textbf{Li et al., "Conditional Generative Models for Combinatorial Optimization"} typically involves training a conditional generative model using a training dataset that includes solutions obtained from classical solvers like Gurobi \textbf{Gurobi}. However, as noted by \textbf{Bengio}, these supervised approaches face challenges due to expensive data generation, leading to increased interest in unsupervised CO (UCO). In UCO the goal is to train models to solve CO problems without relying on labeled training data but only by evaluating the quality of generated solutions \textbf{Rubinstein}. These methods often utilize exact likelihood models, such as mean-field models \textbf{Hinton}. The calculation of expectation values in UCO is particularly convenient with mean-field models due to mathematical simplification arising from their assumption of statistical independence among modeled random variables. However, \textbf{Rombach} demonstrate that the statistical independence assumption in mean-field models limits their performance on particularly challenging CO problems. They show that more expressive exact likelihood models, like autoregressive models, offer performance benefits, albeit at the cost of high memory requirements and longer sampling times, which slow down the training process.
These limitations can be addressed by combining autoregressive models with RL methods to reduce memory requirements and accelerate training as it is done in \textbf{Ho et al.} and \textbf{Liu}. \textbf{Rombach} additionally introduce Subgraph Tokenization to mitigate slow sampling and training in autoregressive models. \textbf{Tolstikhin} utilize GFlow networks \textbf{Tolstikhin}, implementing autoregressive solution generation in UCO. \textbf{Anand et al.} introduce a general framework that allows for the application of diffusion models to UCO and demonstrate their superiority on a range of popular CO benchmarks.

%\textbf{Diffusion Models in Reinforcement Learning:}
%____ introduce denoising policy optimization, where they formulate a policy gradient algorithm as a multi-step Markov decision problem and optimize it with diffusion models. They apply their method to reinforcement learning with human feedback. In contrast to us, they do not include intermediate noise distributions in the reward function.

\textbf{Unbiased Sampling:}
In this work, unbiased sampling refers to the task of calculating unbiased expectation values via samples from an approximation of the target distribution. Corresponding methods rely so far primarily on exact likelihood models, i.e.~models that provide exact likelihoods for samples. Unbiased sampling plays a central role in a wide range of scientific fields, including molecular dynamics \textbf{Zhang et al.}, path tracing \textbf{Parekh et al.}, and lattice gauge theory \textbf{LÃ¼scher}. These applications in continuous domains are suitable for using exact likelihood models like normalizing flows which are a popular model class in these domains. More recently approximate likelihood models became increasingly important in these applications since their increased expressivity yields superior results \textbf{Huang et al.}. In discrete domains, unbiased sampling arises as a key challenge in the study of spin glasses \textbf{Tessera},  many-body quantum physics \textbf{Carleo}, and molecular biology \textbf{Mezard}. In these settings, autoregressive models are the predominant model class. We are not aware of works that explore the applicability and performance of approximate likelihood models like diffusion models for unbiased sampling on discrete problem domains.