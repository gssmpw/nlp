\documentclass[journal,twoside,web]{ieeecolor}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{tmi}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{comment}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,  
    urlcolor=blue,
    citecolor=blue,
    }

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
% \markboth{\journalname, VOL. XX, NO. XX, XXXX 2020}
% {Author \MakeLowercase{\textit{et al.}}: Preparation of Papers for IEEE TRANSACTIONS ON MEDICAL IMAGING}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\redb}[1]{\textcolor{red}{\small{\textbf{[#1]}}}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\blueb}[1]{\textcolor{blue}{\small{\textbf{[#1]}}}}
\newcommand{\cyan}[1]{\textcolor{cyan}{#1}}
\newcommand{\cyanb}[1]{\textcolor{cyan}{[#1]}}

\newcommand{\py}[1]{{\textbf{\color{red}{\small{[PY: #1]}}}}}

\newcommand{\zf}[1]{{\textbf{\color{blue}{\small{[ZF: #1]}}}}}

\newcommand{\jj}[1]{{\textbf{\color{purple}{\small{[JJ: #1]}}}}}

\usepackage[normalem]{ulem}
% \usepackage{xcolor}
% \newcommand{\ins}[1]{\textcolor{orange}{#1}}
% \newcommand{\rev}[2]{\textcolor{orange}{\sout{#1} {#2}}}
% \newcommand{\del}[1]{\textcolor{orange}{\sout{#1}}}
% \newcommand{\com}[1]{\textcolor{orange}{[XX: #1]}}

\begin{document}

\title{Chest X-ray Foundation Model with Global and Local Representations Integration}

\author{Zefan Yang, Xuanang Xu, Jiajin Zhang, Ge Wang, \IEEEmembership{Fellow, IEEE}, \\Mannudeep K. Kalra, and Pingkun Yan, \IEEEmembership{Senior Member, IEEE}
\thanks{Z. Yang, X. Xu, G. Wang, and P. Yan* are with the Department of Biomedical Engineering and Center for Biotechnology and Interdisciplinary Studies, Rensselaer Polytechnic Institute, Troy, NY, USA.}
\thanks{J. Zhang was with the Department of Biomedical Engineering and Center for Biotechnology and Interdisciplinary Studies, Rensselaer Polytechic Institute, when he contributed to this work.}
\thanks{M. K. Kalra is with the Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA.}
\thanks{This research was funded in part by  the NSF CAREER award 2046708 and the NIA Predoctoral Training Program For Alzheimerâ€™s Disease At The Interface Of Data Science, Engineering And Biology Training Program T32AG078123.}
\thanks{*Corresponding author: yanp2@rpi.edu}
}
% \py{Please get the official name of the training grant.}

\maketitle

\begin{abstract}
% CXR (CXR) is one of the most commonly-ordered imaging test worldwide.
% Clinical CXR interpretation involves a wide range of tasks, from detecting thoracic diseases to monitoring postoperative recovery.
% With the vast array of tasks, training specialized classification models for each task has limitations. 
% These models are only effective for limited pathologies and have inferior generalizability to out-of-distribution datasets.
% They also requires a large collection of cost-prohibitive labels to learn meaningful representations.
% To address these limitations, we introduce CheXFound, a self-supervised vision foundation model containing robust CXR representations that can generalize to a wide range of downstream tasks.
Chest X-ray (CXR) is the most frequently ordered imaging test, supporting diverse clinical tasks from thoracic disease detection to postoperative monitoring. However, task-specific classification models are limited in scope, require costly labeled data, and lack generalizability to out-of-distribution datasets. To address these challenges, we introduce CheXFound, a self-supervised vision foundation model that learns robust CXR representations and generalizes effectively across a wide range of downstream tasks.
We pretrain CheXFound on a curated CXR-1M dataset, comprising over one million unique CXRs from publicly available sources. We propose a Global and Local Representations Integration (GLoRI) module for downstream adaptations, by incorporating disease-specific local features with global image features for enhanced performance in multilabel classification.
Our experimental results show that CheXFound outperforms state-of-the-art models in classifying 40 disease findings across different prevalence levels on the CXR-LT 24 dataset and exhibits superior label efficiency on downstream tasks with limited training data.
Additionally, CheXFound achieved significant improvements on new tasks with out-of-distribution datasets, including opportunistic cardiovascular disease risk estimation and mortality prediction.
These results highlight CheXFound's strong generalization capabilities, enabling diverse adaptations with improved label efficiency. The project source code is publicly available at \url{https://github.com/RPIDIAL/CheXFound}.
\end{abstract}

\begin{IEEEkeywords}
Chest X-ray, Foundation Model, Knowledge Distillation, Self-supervised Learning, Pretraining.
% Enter about five key words or phrases in alphabetical order, separated by commas.
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}

\IEEEPARstart{C}{hest} X-ray (CXR) is one of the most commonly-ordered imaging tests worldwide \cite{raoof2012interpretation}.
Clinical CXR interpretation encompasses a broad spectrum of tasks, including detecting diseases associated with the lungs, heart, blood vessel, and bones, as well as monitoring postoperative recovery and the positioning of support devices. 
With advancements in computer-aided diagnosis, these tasks now extend even further to include opportunistic disease risk assessment, such as cardiovascular disease\cite{Weiss24-CVD-CXR,yang2024cardiovascular}, mortality risk\cite{weiss2023deep}, and diabetes\cite{pyrros2023opportunistic}, among other factors not directly quantifiable by human eyes.
% \cite{kamel2021prediction, ueda2023artificial}
Training specialized classification models for each task from scratch poses significant limitations. 
Such models are typically effective only within a narrow scope of pathologies and struggle to generalize to out-of-distribution datasets.
Furthermore, developing these models requires requires extensive labeled datasets, which are both cost-prohibitive and inefficient. These challenges underscore the need for self-supervised models that can learn robust representations and demonstrate superior generalization capabilities across diverse tasks.
% since it is cost prohibitive to have radiologists annotate a large\ins{-}scale dataset.
% Small-scale, task-specific training can only generalize to a limited scope of disease findings and has performance discrepancies between development and deployment due to data distribution shifts.

Recent advancements in the field of computer vision\cite{chen2021empirical, he2022masked, caron2021emerging, zhou2021ibot} demonstrate that self-supervised vision models can produce task-agnostic and semantic-rich image representations that achieve improved performance on a broad spectrum of downstream tasks.
% and computational pathology\cite{chen2024towards, wang2024pathology}
% \rev{general-purpose}
% transcend previous models
% predominantly
Such models are called foundation models because of their superior capabilities to adapt to diverse downstream tasks when pretrained on large-scale data. 
% the large-scale pretraining and 
% they are pretrained with large-scale data
% This generalization capability is heavily reliant on the massive size and diversity of pretraining data.
Recent works in self-supervised learning for CXR interpretation adopt a series of advanced training strategies to learn high-quality image representations, including contrastive learning \cite{azizi2023robust}, masked image modeling (MIM) \cite{yao2024eva}, and self-distillation \cite{perez2024rad}. Research further use CXRs and their clinical reports to perform contrastive language-image pretraining \cite{tiu2022expert}.
However, these studies have two major limitations. First, they only evaluate model performance for classifying a narrow range of disease findings, without considering the long-tail nature of pathologies in CXR and the opportunistic CXR interpretation tasks, such as cardiovascular disease (CVD) risk estimation and mortality prediction. Second, these studies simply rely on the global image features for disease classification, overlooking the use of CXR representations learned by the foundation models to provide disease-specific local features to enhance performance.
% reduce ambiguities.
% arisen from merely using the global representation.
% in multilabel classification.
% the model capabilities for 
% Second, previous studies focused on detecting disease findings recorded in radiology reports and did not evaluate the generalization capability of the learned representations on tasks that extend radiologists ability, including cardiovascular disease risk, coronary calcium scores, ejection fraction, all-cause mortality, etc. 
Addressing these limitations is pivotal to the development of the CXR foundation models towards clinical applications which often involve interpreting a wide range of disease findings. It also has broader implications by enabling CVD risk estimation and mortality prediction with a routine CXR.
% in risk estimation of diseases that radiologists \rev{cannot}{may not} see.

In this work, we introduce CheXFound, a vision foundation model specialized for CXR image analysis that learns high-quality CXR representations and generalizes effectively across a wide range of thoracic disease classification and opportunistic risk estimation tasks. 
% To pretrain our CheXFound model, we curate CXR-1M, a large-scale CXR dataset containing
We pretrain CheXFound on a curated CXR-1M dataset, comprising more than one million unique CXRs from 13 publicly available datasets, including MIMIC-CXR \cite{johnson2019mimic}, CheXpert \cite{irvin2019chexpert}, PadChest \cite{bustos2020padchest}, CXR14 \cite{wang2017chestx}, BRAX \cite{reis2022brax}, VinDr-CXR \cite{nguyen2022vindr}, and CANDID-PTX \cite{feng2021curation}, among others.
Our CheXFound model is pretrained via DINOv2 \cite{oquab2023dinov2}, a state-of-the-art self-supervised learning method with strong off-the-shelf linear probe performance.
 % released by a range of institutes
% \ins{Our CheXFound model is} pretrained \del{using the vision transformer architecture (ViT-Large)} on a large cohort of \ins{[one million]} CXRs from \rev{multiple}{[13]} institutions \com{Explicitly show the numbers of CXRs and institutes involved in this study to impress the reviewers. BTW, I think our data curation (CXR-1M) is also a major contribution in this work. We can emphasize it here.} \rev{. In the pretraining stage, we use}{via} a self-supervised learning method named DINOv2\cite{oquab2023dinov2}, which uses self-distillation techniques achieving strong image representations for downstream prediction tasks.
For downstream adaptation, we propose a Global and Local Representations Integration (GLoRI) module. 
GLoRI is trained on top of the frozen CheXFound model. It uses the attentional principle to compute disease-specific local features and integrates them with the global image features to improve the multilabel classification performance.
 % from the \texttt{[CLS]} token
 % in CXR interpretation
% GLoRI computes attention-pooled CXR representations for each disease finding and integrates the global features from the [CLS] token to address the multilabel classification problem.
% \py{What is after pretraining? Should also mention Glori here.}
% Contributions on data curation

We assess CheXFound's performance on two tiers of CXR interpretation tasks, including thoracic disease classification and opportunistic risk estimation.
CheXFound outperforms previous state-of-the-art models such as RAD-DINO \cite{perez2024rad}, EVA-X \cite{yao2024eva}, and CheXzero \cite{tiu2022expert} across 40 disease findings at different prevalence levels on the CXR-LT 24 dataset \cite{peng_2024_10991413}. 
Besides, CheXFound demonstrates superior label efficiency, which achieves best-performing results on the Shenzhen, Montgomery, and JSRT datasets with limited training data.
% of disease  
% \rev{We evaluate ChestFound on in total}{For the long-tail thoracic disease detection tier,} 40 disease findings provided by CXR-LT Challenge\cite{holste2024towards} with different levels of disease prevalence \ins{are involved}. 
% Results show that ChestFound achieves top-3 performance on the official leaderboard with an unseen radiologist-annotated gold standard test set.
% and outperforms previous models including CheXzero\cite{tiu2022expert} and ConvNeXt\cite{liu2022convnet} Fig. \ref{fig:performance}.
We also find that CheXFound achieves significant performance increases compared with its comparisons for the out-of-distribution tasks, including opportunistic CVD risk estimation and mortality prediction on the PLCO dataset \cite{hocking2010lung}. 
Overall, we demonstrate CheXFound's strong generalization capabilities across a wide range of downstream tasks on in-distribution and out-of-distribution datasets. CheXFound's strong representation quality can enable diverse downstream adaptations with improved label efficiency.
% We further validate ChestFound's capability in cardiovascular disease risk detection and survival prediction based on a routine chest radiograph. 
% Results demonstrate that ChestFound can achieve competitive performance compared with models using computed tomography (CT) for risk prediction, highlighting the potential of ChestFound as a foundation model for a full spectrum of chest radiograph analysis tasks.

\section{Related works}
\subsection{Self-supervised Visual Representation Learning}
Our study is mostly related to self-supervised visual representation learning. After the success of masked language modeling in language domain, masked autoencoder (MAE) \cite{he2022masked} and BEiT \cite{bao2021beit} translate the idea into visual representation learning, which assume the pretext task of recovering masked pixels can train networks to learn visual information and context. Another family of self-supervised learning methods (SimCLR \cite{chen2020simple} and MoCov3\cite{chen2021empirical}) apply contrastive learning objectives, assuming augmentation invariance of image representations and aiming to learn contrastive class representations. These methods have been reported to achieve inferior linear probe performance and require fine-tuning backbone features \cite{oquab2023dinov2}. They also do not translate well into medical applications \cite{chen2024towards}. Beyond the above methods, another family of self-supervised learning methods rely on a knowledge distillation framework first introduced by BYOL \cite{grill2020bootstrap}, which bootstraps latent features of a teacher network to train a student network. DINO \cite{caron2021emerging} applies self-distillation with the Transformer architecture and enforce similarity of categorical distributions. iBOT \cite{zhou2021ibot} extends the framework with masked image modeling. DINOv2 \cite{oquab2023dinov2} carefully curates pretraining data with deduplication and further makes modifications to improve training. Overall, self-distillation methods excel at linear probe evaluation and have demonstrated generalizability in medical application \cite{perez2024rad, chen2024towards}. Our study follows this methodology to train CheXFound with strong representation quality.

% SimSiam \cite{chen2021exploring}
 % \cite{}\com{missing citation}
% \begin{itemize}
%     \item Pretext tasks: masked autoencoder (MAE), contrastive learning (SimCLR, MoCov3); limitations: fine-tuning features, weak linear probe performance, does not translate well into medical domain
%     \item Knowledege distillation: BYOL introduce teacher-student network, DINO self-distillation with [CLS] token alignment, iBOT masked image modeling, the same style as BERT
% \end{itemize}
% \subsubsection{Exponential Moving Averaged Network as Online Tokenizer}
% \subsubsection{Masked Image Modeling as Knowledge Distillation}
% \subsubsection{Self-distillation via Local-to-global Alignment}
\subsection{Foundation Models for Medical Applications}
The surge in available data and computational resources have enabled the large-scale pretraining of foundation models. Studies have demonstrated that scaling foundation models in data and model sizes can achieve performance increases across a wide array of downstream tasks \cite{he2022masked, oquab2023dinov2, chen2024towards}. In medical domain, research works have developed multiple categories of foundation models differing in technical approaches and data modalities. Our study is related to vision-centric foundation models. RAD-DINO \cite{perez2024rad} and EVA-X \cite{yao2024eva} are two foundation models in CXR domain. Compared to CheXFound with ViT-L pretrained on CXR-1M, these models are limited in model and data scales. 
% In computational pathology, UNI \cite{chen2024towards} demonstrates generalization capabilities across downstream tasks, even for rare diseases. 
Another category of foundation models incorporate vision and text data for multimodal pretraining. CheXzero \cite{tiu2022expert}, BiomedCLIP \cite{zhang2023biomedclip}, PubMedCLIP \cite{eslami2021does} use contrastive vision-language pretraining, which is effective in zero-shot classification. Further development of vision-language models takes advantage of instruction-tuning to improve reasoning and detailed description capabilities \cite{chen2024chexagent, li2024llava, yang2024advancing}. Overall, research empirically finds that vision-language models achieve inferior performance than vision-centric foundation models in CXR classification \cite{perez2024rad}. In this study, we focus on the vision-centric foundation model and investigate its capability for extensitve CXR classification tasks.
To the best of our knowledge, our work employs the largest-scale self-supervised pretraining with over 1 million unique CXRs.
% \py{How do you distinguish your work from other works listed above?}
% surge, scaling
% , CONCH \cite{lu2024visual}, and PILP \cite{huang2023visual}

% \begin{itemize}
% \item Vision-centric foundation models; CXR: RAD-DINO, EVA-X; DINO-related vision foundation model: UNI;
% \item Vision language foundation models, contrastive vision-language pretraining: CheXzero, BiomedCLIP, PubMedCLIP, CONCH; Instruct-tuned models: CheXagent, BiomedGPT, PathChat, LLaVA-Med, Med-Gemini; versatile models: BiomedParse
% \end{itemize}

\section{Materials and Methods}

\begin{figure*}[th]
    \centering
    \includegraphics[width=\textwidth]{figs/pretrain_cropped2.pdf}
    \caption{Overview of self-supervised pretraining of CheXFound, using publicly available CXRs from multiple institutions with a masked image modeling objective and a \texttt{[CLS]} token alignment objective.}
    \label{fig:pretrain}
\end{figure*}


\begin{table}[th]
    \setlength{\tabcolsep}{5pt}
    \setlength{\extrarowheight}{2pt}
    \centering
    \caption{Curation of CXR-1M with publicly available datasets from diverse institutions for self-supervised pretraining. CXR-1M is subsetted into CXR-207K and CXR-744K to evaluate the data scalability of self-supervised models.}
    % \com{The caption is inconsistent with the statement described in main text. It sounds like 'CXR-1M'='CXR-207K'+'CXR-744K'. BTW, I adjusted the table structure a bit. Pls review it.}
    \label{tab:ptdata}
    \begin{tabular}{l | c | c | r }
    \toprule
     \multicolumn{1}{c|}{\textbf{Datasets}} & \textbf{View} & \textbf{Findings} & \textbf{image \#} \\ \midrule
     MIMIC-CXR\cite{johnson2019mimic} & Frontal, Lateral & 14 diseases & 207,096\\
     \midrule
     \multicolumn{4}{r}{\textbf{Total number of images in CXR-207K: 207,096}}\\
     \midrule
     CheXpert\cite{irvin2019chexpert} & Frontal, Lateral & 14 diseases & 223,648\\
     PadChest\cite{bustos2020padchest} & Frontal, Lateral & 193 diseases & 160,861\\
     CXR14\cite{wang2017chestx} & Frontal & 14 diseases & 112,120\\
     BRAX\cite{reis2022brax} & Frontal, Lateal & 14 diseases & 40,967\\ 
     \midrule
     \multicolumn{4}{r}{\textbf{Total number of images in CXR-744K: 744,692}}\\
     \midrule
     VinDr-CXR\cite{nguyen2022vindr} & Frontal & 28 diseases & 18,000 \\
     CANDID-PTX\cite{feng2021curation} & Frontal & Pneumothorax & 19,237 \\
     SIIM-ACR\cite{siim-acr} & Frontal & Pneumothorax & 18,499 \\
     Object-CXR\cite{objectcxr} & Frontal & Foreign objects & 9,000\\
     COVID-19\cite{lakhani20232021} & Frontal & COVID-19 & 7,597 \\
     COVIDx CXR-4\cite{wu2023covidx} & Frontal & COVID-19 & 84,818 \\
     MIDRC COVIDx\cite{midrc-covidx} & Frontal & COVID-19 & 23,001 \\
     BIMCV COVID+\cite{vaya2020bimcv} & Frontal, Lateral & COVID-19 & 80,889 \\
     \midrule
     \multicolumn{4}{r}{\textbf{Total number of images in CXR-1M: 1,005,733}}\\
     \bottomrule
    \end{tabular}
\end{table}

%\subsection{Data Curation for CXR-1M}
\subsection{CXR-1M for Pretraining CheXFound}

As detailed in Table \ref{tab:ptdata}, we curated the CXR-1M dataset for self-supervised pretraining by retrieving in total 1,005,733 unique CXRs from 13 publicly available datasets \cite{johnson2019mimic, irvin2019chexpert, bustos2020padchest, wang2017chestx, wu2023covidx, reis2022brax, midrc-covidx, vaya2020bimcv, nguyen2022vindr, lakhani20232021, objectcxr, feng2021curation, siim-acr} that were released for various downstream tasks, including disease diagnosis, abnormality detection, foreign objection detection, and segmentation.
%by retrieving CXRs from ins{13} publicly available datasets collected for various objectives, including disease diagnosis (MIMIC-CXR \cite{johnson2019mimic}, CheXpert \cite{irvin2019chexpert}, PadChest \cite{bustos2020padchest}, CXR14 \cite{wang2017chestx}, COVIDx CXR-4 \cite{wu2023covidx}, BRAX \cite{reis2022brax}, MIDRC COVIDx \cite{midrc-covidx}, BIMCV COVID+ \cite{vaya2020bimcv}), abnormality detection (VinDr-CXR \cite{nguyen2022vindr}, COVID-19 \cite{lakhani20232021}), foreign objection detection (Object-CXR \cite{objectcxr}), and segmentation (CANDID-PTX \cite{feng2021curation}, SIIM-ACR \cite{siim-acr}), totaling 1,005,733 CXRs, as shown in Table \ref{tab:ptdata}. 
To learn comprehensive representations for multiview CXR analysis, both frontal-view in PA (posterior-anterior) or AP (anterior-posterior) and lateral-view CXRs were included into CXR-1M. 
%To avoid data leakage during downstream evaluation, we excluded \py{missing some words here?} CXRs in the test set from CXR-1M. To be specific, CXR-LT 24 release a subset of MIMIC-CXR images with labels.\com{Do we really need to mention the 'CXR-LT 24' dataset here? Since it is one of our evaluation datasets, which are excluded in the training data mentioned above. I saw the evaluation datasets are introduced in the following experimantal sections (Sec. IV-A and B) now. Or we can move the introduction of evaluation datasets to here.} We divide CXR-LT 24 into training and test sets and only incorporate the training set into CXR-1M. For the evaluation on CheXpert, we include only its training and validation sets into CXR-1M and keep its test set unseen during pretraining. For data preprocessing, since CXRs have high resolution which can \rev{lower}{impact} the efficiency of large-scale pretraining, we resize images in CXR-1M \rev{into}{to a uniform size of} 512$^2$ pixels and store them in JPEG format.

% Datasets such as MIMIC-CXR \cite{johnson2019mimic}, CheXpert \cite{irvin2019chexpert} and CXR14 \cite{wang2017chestx} have been combined to pretrain a vision foundation model \blue{in the previous work by Yao et al.} \cite{yao2024eva}. RAD-DINO\cite{perez2024rad} additionally added PadChest \cite{bustos2020padchest} and BRAX \cite{reis2022brax} datasets for foundation model pretraining. 
% In this study, to further evaluate the data scaling capabilities of self-supervised vision encoders, we incorporated CXRs from diverse cohorts released for a variety of objectives, including abnormality detection \cite{nguyen2022vindr, lakhani20232021}, foreign object detection \cite{objectcxr}, segmentation \cite{feng2021curation, siim-acr}, diagnosis \cite{johnson2019mimic, irvin2019chexpert, wang2017chestx, wu2023covidx, bustos2020padchest, reis2022brax, midrc-covidx, vaya2020bimcv}, totaling 1,005,733 CXR images, as shown in Table \ref{tab:ptdata}. 
% To learn the useful representations of CXRs in multiple views, both frontal-view in PA (posterior-anterior) or AUPRC (anterior-posterior) and lateral-view CXRs were included into the dataset.

% ViT-Large architecture: patch size 16, 24 layers, embedding dimension 1024, num heads for multihead attention 16, hidden dimension in feed-forward layer 5472, feed-forward layer activation function SiL

To evaluate the data scalability of self-supervised pretraining, we further created CXR-207K and CXR-744K, denoting two subsets of CXR-1M, as shown in Table~\ref{tab:ptdata}. CXR-207K contains approximately 207K CXRs from MIMIC-CXR. CXR-744K contains around 744K CXRs from five datasets: MIMIC-CXR, CheXpert, PadChest, CXR14, and BRAX.

We used DINOv2 \cite{oquab2023dinov2}, a state-of-the-art self-supervised learning method, to pretrain CheXFound on CXR-1M.
DINOv2 inherits designs from DINO \cite{caron2021emerging} and iBOT \cite{zhou2021ibot} and incorporates two self-distillation objectives: the masked image modeling loss $\mathcal{L}_{\texttt{MIM}}$ and the \texttt{[CLS]} token alignment loss $\mathcal{L}_{\texttt{[CLS]}}$. It uses a teacher-student knowledge distillation architecture as shown in  Fig. \ref{fig:pretrain} to learn CXR representations. Masked image modeling uses the teacher network as an online tokenizer, which generates patch tokens from intact images to guide the student network in reconstructing masked patch tokens. This approach enables the student network to learn both visual features and contextual information effectively.
On the other hand, the \texttt{[CLS]} token alignment loss $\mathcal{L}_{\texttt{[CLS]}}$ enforces similarity between \texttt{[CLS]} tokens output by the teacher and student networks. This approach aims to train the network to learn high-level class representations with off-the-shelf linear probe capabilities.


\begin{comment}

\subsection{Large-scale Self-supervised Pretraining}

% \py{Shall we put this section into related works? Instead, we should introduce the architecture of ChexFound. I don't see it yet.}

We used DINOv2 \cite{oquab2023dinov2}, a state-of-the-art self-supervised learning method, to pretrain CheXFound on CXR-1M.
DINOv2, inheriting designs from DINO \cite{caron2021emerging} and iBOT \cite{zhou2021ibot}, incorporates two self-distillation objectives: masked image modeling loss $\mathcal{L}_{\texttt{MIM}}$ and \texttt{[CLS]} token alignment loss $\mathcal{L}_{\texttt{[CLS]}}$. It then uses a teacher-student knowledge distillation architecture as shown in  Fig. \ref{fig:pretrain} to learn CXR representations. Masked image modeling uses the teacher network as an online tokenizer, generating patch tokens from intact images to guide the student network in reconstructing masked patch tokens. This method enables the student network to learn both visual features and contextual information effectively.
%Masked image modeling treats the teacher network as an online tokenizer, which outputs patch tokens of intact images to guide the recovery of masked patch tokens produced by the student network. This approach trains the network to learn visual features and contextual information. 
The $\mathcal{L}_{\texttt{MIM}}$ loss is formulated as:
\begin{equation}
\mathcal{L}_{\texttt{MIM}} = - \sum_{i=1}^{N} \mathbf{m}_i \cdot P^{\texttt{Patch}}_{\boldsymbol{\theta}^\prime}(\mathbf{u}_i)^T \log P^{\texttt{Patch}}_{\boldsymbol{\theta}}(\tilde{\mathbf{u}}_i),
\end{equation}
where $\mathbf{u} = \{\mathbf{u}_i\}_{i=1}^N$ is the patch token sequence of length $N$ for the distorted input image $\mathbf{x}$. $\tilde{\mathbf{u}}$ is the masked token sequence. $\mathbf{m} = \{0, 1\}^N$ indicates masked patch tokens. The parameters of the teacher network $\boldsymbol{\theta}^\prime$ are the exponential moving average of the student parameters $\boldsymbol{\theta}$. $P^{\texttt{Patch}}_{\boldsymbol{\theta}(\cdot)}$ and $P^{\texttt{Patch}}_{\boldsymbol{\theta^\prime}(\cdot)}$ compute the predictive categorical distributions of dimension $K$ for patch tokens. Overall, $\mathcal{L}_{\texttt{MIM}}$ minimizes the cross-entropy between the categorical distributions of unmasked and masked patch tokens. $\mathcal{L}_{\texttt{MIM}}$ is applied multiple times depending on the number of distorted views.

On the other hand, the \texttt{[CLS]} token alignment loss $\mathcal{L}_{\texttt{[CLS]}}$ enforces similarity between \texttt{[CLS]} tokens output by the teacher and student networks. This approach aims to train the network to learn high-level class representations with off-the-shelf linear probe capabilities. To be specific, $\mathcal{L}_{\texttt{[CLS]}}$ is formulated as:

\begin{equation}
\mathcal{L}_{\texttt{[CLS]}} = - \sum_{i=1}^{N} P^{\texttt{[CLS]}}_{\boldsymbol{\theta}^\prime}(\mathbf{u}_i)^T \log P^{\texttt{[CLS]}}_{\boldsymbol{\theta}}(\tilde{\mathbf{v}}_i),
\end{equation}
where $\mathbf{u}$ and $\mathbf{v}$ are the patch token sequences for two distorted views of the input image $\mathbf{x}$. $\tilde{\mathbf{v}}$ is the masked patch token sequence. $P^{\texttt{[CLS]}}_{\boldsymbol{\theta}}(\cdot)$ and $P^{\texttt{[CLS]}}_{\boldsymbol{\theta}^\prime}(\cdot)$ compute the categorical distributions of dimension $K$ for the \texttt{[CLS]} tokens. $\mathcal{L}_{\texttt{[CLS]}}$ is complemented with a symmetric term between $\mathbf{v}$ and $\tilde{\mathbf{u}}$ and a term between the global and local crops. DINOv2 self-supervised training also includes KoLeo regularization to improve the diversity and spread of token embeddings and Sinkhorn-Knopp centering to avoid model collapse, a critical problem in self-supervised learing \cite{grill2020bootstrap, caron2021emerging}.

\end{comment}


% \begin{itemize}
%     \item We use DINOv2 for self-supervised pretraining, teacher-student network, two self-distillation objectives: masked image modeling loss $\mathcal{L}_{\texttt{MIM}}$ and [CLS] token alignment loss $\mathcal{L}_{\texttt{[CLS]}}$.
%     \item The purpose of two objectives: Masked image modeling to learn visual features and contextual information; [CLS] token alignment to learn high-level class representations with off-the-shelf linear probe capabilities. 
%     \item More training details: KoLeo regularization to improve the diversity and spread of token embeddings, Sinkhorn-Knopp centering to avoid model collapse,
%     \item Notations: student network that output distribution for [CLS] token $P^{\texttt{[CLS]}}_\theta(\cdot)$, teacher network $P_\theta^\prime(\cdot)$, input image $\mathbf{x}$, distorted global crops $\mathbf{u}$, $\mathbf{v}$, masked global crops $\tilde{\mathbf{u}}$, $\tilde{\mathbf{v}}$, 
% \end{itemize}

\subsection{Global and Local Representation Integration for Multilabel Classification}
For the downstream evaluation of CheXFound, the linear probe classifier is a pivotal tool to evaluate the quality of pretrained representations. However, the linear probe classifier has limited capability to address the multilabel classification problem commonly seen in CXR interpretation, since it generally relies on the global image features from a single \texttt{[CLS]} token for classifying a wide range of pathologies and lacks essential local details to support the predictions. In contrast, patch tokens from our pretrained CheXFound contain rich CXR representations and high-level contextual information learned via masked image modeling, which can provide disease-specific local features to substantially reduce ambiguities arisen from using the \texttt{[CLS]} token for classifying multiple pathologies. To take advantage of both local and global features for disease classification, we introduce GLoRI (Fig. \ref{fig:glori}), which utilizes a cross-attention layer with disease queries to summarize patch token features and a skip-connection to integrate the $\texttt{[CLS]}$ token towards final prediction.

To be specific, GLoRI receives output patch tokens $\mathbf{u}^\texttt{Patch} \in \mathbb{R}^{N \times D_\text{model}}$ from the frozen CheXFound backbone as input, where $D_\text{model}$ is the backbone embedding dimension. Since there can be a dimension mismatch between the backbone and GLoRI, we use a linear embedding layer to project $\mathbf{u}^\texttt{Patch}$ to the GLoRI dimensional space:

\begin{equation}
\mathbf{u}^{\prime\texttt{Patch}} = \text{ReLU}(\text{Linear}^\text{embed}(\mathbf{u}^\texttt{Patch})),
\end{equation}
where $\mathbf{u}^{\prime\texttt{Patch}} \in \mathbb{R}^{N\times D_\text{GLoRI}}$ is the projected by the linear embedding layer $\text{Linear}^\text{embed}(\cdot)$ to the patch token sequence with dimension $D_\text{GLoRI}$. In GLoRI, to extract disease-specific local features using the cross-attention layer, we initialize $M$ disease queries corresponding to $M$ disease findings, denoted as $\mathbf{q} \in \mathbb{R}^{M\times D_\text{key}}$. The keys $\mathbf{k} \in \mathbb{R}^{M\times D_\text{key}}$ and values $\mathbf{q} \in \mathbb{R}^{M\times D_\text{value}}$ for the cross-attention layer are from $\mathbf{u}^{\prime\texttt{Patch}}$, which provides rich CXR representations. A scaled dot-product attention module is used to compute attention-pooled features relevant to the query diseases:

\begin{equation}
\mathbf{q}^\prime = \text{Softmax} \left( \frac{\text{Linear}^\text{query}(\mathbf{q})\text{Linear}^\text{key}(\mathbf{k})^T}{\sqrt{D_\text{key}}} \right) \text{Linear}^\text{value}(\mathbf{v}),
\end{equation}
% \jj{The equation of attention mechanism is not correct. (1) the linear layers are different for extracting k, q, and v. (2) the input of k and q layers should be the same. (3) Explaining the definition of Dkey may be better.} 
where $\mathbf{q}^\prime \in \mathbb{R}^{M\times D_\text{key}}$ denotes the output disease queries, which we consider contain disease-specific local features.
$D_\text{key}$ is the dimension of the key tokens.
Last, we concatenate $\mathbf{q}^\prime$ with the \texttt{[CLS]} token to construct the GLoRI output token sequence. For multilabel classification, each GLoRI output token is projected by a linear classifier supervised by a binary cross-entropy loss.

Overall, GLoRI applies the attentional principle, which shares similarity with previous works, such as Perceiver \cite{jaegle2021perceiver} and DETR \cite{carion2020end}, for CheXFound's downstream evaluation. We empirically demonstrate that GLoRI extracts local features relevant to disease abnormalities in CXRs in Section \ref{sec:intepret}.

% \begin{itemize}
% \item The linear probe classifier is a pivotal tool to evaluate pretrained representation quality; However, the linear probe classifier has limited capability to address the multilabel classification problem since it generally relies on the global features from a single [CLS] token for classifying a wide range of pathologies. 
% \item In contrast, patch tokens from the pretrained encoders contains rich CXR representations and high-level contextual information learned via masked image modeling, which can provide disease-specific local features to substaintially reduce ambiguities arised from using the [CLS] token for multilabel classification.
% \item To take advantage of global, local
% \item Notation: output patch tokens $\mathbf{u}^\texttt{Patch}$, $\mathbb{R}^{N \times D_\text{model}}$ matrix; output [CLS] token $\mathbf{u}$, $\mathbb{R}^{D}$ vector; disease queries, $\mathbf{q}$, $\mathbb{R}^{M\times D_\text{query}}$; keys and values $\mathbf{k}$, $\mathbf{v}$, matrix $\mathbb{R}^{N\times D_\text{key}}$, $\mathbb{R}^{N \times D_\text{value}}$; global and local integration, $\text{Concat}(\mathbf{q}^\prime, \mathbf{u}^\texttt{[CLS]})$
% , which contains global features generally useful for classifying a wide range of pathologies but lacks disease-specific local features to multilabel classification problem. 
% \item Discussion: share attentional principle with previous works Perceiver, DETR; quality of attention maps in Fig. \ref{fig:glori-attns}
% \end{itemize}
% \subsubsection{Bootstrapped Representations for Downstream Evaluation}
% \subsubsection{Multilabel Cross-entropy Loss}
% Lightweight
% Perceiver, DETR

% \py{Put in your figures to the paper. So far there is figure in your paper!}

% \begin{itemize}
%     \item Aligning global crops [CLS] token with [CLS] tokens from local crops and masked global crops.
%     \item Bootstrapping a frozen image encoder
% \end{itemize}
% \py{Please merge panels b and c into one single figure.}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/glori2_cropped.pdf}
    \caption{Global and Local Representations Integration (GLoRI) for evaluating CheXFound on downstream tasks. GLoRI is appended on top of the frozen CheXFound backbone. GLoRI uses disease queries to compute attention-pooled CXR representations and integrates the global image features from [CLS] token for disease finding classification. For the opportunistic CXR interpretation, additional disease queries are created for GLoRI to retrieve representations related to the CVD risk estimation and mortality prediction.}.
    \label{fig:glori}
\end{figure}

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.8\textwidth]{figs/framework.pdf}
%     \caption{Overview of CheXFound pretraining and GLoRI. \textbf{a}. CheXFound is pretraiend on publicly available CXRs from a range of institutions using a self-supervised training algorithm that consists of a masked image modeling objective and an alignment objective. \textbf{b}. For the evaluation of CheXFound on downstream tasks, GLoRI is appended on top of the frozen CheXFound backbone. GLoRI uses query tokens to compute attention-pooled CXR representations and integrates the global features from [CLS] token for disease finding classification. \textbf{c}. For the opportunistic CXR interpretation, additional query tokens are created for GLoRI to retrieve representations related to the estimation of CVD risk and all-cause mortality. \py{Please merge panels b and c into one single figure.}}
%     \label{fig:framework}
% \end{figure*}

% \section{Experiments and results}
\section{Experimental Design}

\subsection{Implementation Details}
\subsubsection{Self-supervised pretraining details}
We conducted self-supervised pretraining of CheXFound on our curated CXR-1M dataset (Table \ref{tab:ptdata}) using the ViT-L architecture with a patch size of 16$\times$16 pixels. 
% The categorical distribution dimension $K$ is set to 131,072 for both $\mathcal{L}_\texttt{[CLS]}$ and $\mathcal{L}_\texttt{MIM}$ objectives.
% \com{There is no 'K' defined in Equation 1 and 2. What is its definition? And the value of K looks weird. Is it a typo?} 
The loss weights for $\mathcal{L}_\texttt{[CLS]}$ and $\mathcal{L}_\texttt{MIM}$ were set to 1.0 and 3.0, respectively. The momentum to compute the exponential moving average of the student network was set to 0.994. We varied the global and local crop sizes to pretrain CheXFound at a rang of resolutions. Specifically, we set the global and local crop size pairs to be (512, 144), (448, 128), (336, 128), and (224, 96). We set the number of global and local crops to 2 and 8, respectively. For masked image modeling, we set the proportion of masked patches to the range (0.1, 0.5). We trained CheXFound for 100 epochs with an epoch length of 2,500 iterations and a batch size of 14 per graphics processing unit (GPU). We used the AdamW optimizer with an initial learning rate of 2e-4. We applied a Cosine annealing schedule for learning rate decay and a warm up period of 10 epochs. 
We pretrained CheXFound on a DGX-1 server with $8\times$ NVIDIA A100 40GB GPUs. 
% \rev{8$\times$40 GB}
Depending on the image resolutions, the pretraining processes take around 48 to 96 hours. The project source code is publicly available at \url{https://github.com/RPIDIAL/CheXFound}.

\subsubsection{Downstream evaluation details}
In downstream evaluation, we trained GLoRI with feature representations from the frozen CheXFound. We took the concatenated representations from the last 4 layers of CheXFound as the input to GLoRI. We set the embedding dimension of GLoRI to 768. Disease queries were randomly initialized with a standard normal distribution. For the cross-attention layer, we used the multihead attention mechanism and set the number of heads to 8. To train GLoRI, we used the AdamW optimizer and conduct a thorough learning rate search in \{1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3\} to obtain the best-performing learning rate on the validation set. We then combined the training and validation sets for a second round of training using the best learning rate. To maximize the number of images that the GLoRI module processes during downstream adaptation, we trained GLoRI for 10 epochs on CXR-LT 24, CheXpert, and PLCO and 100 epochs on Shenzhen, Mongomery, and JSRT. We set the batch size to 16 for Montgomery and JSRT and 256 for CXR-LT 24, CheXpert, Shenzhen, and PLCO. 
% \py{Why different epochs and batch sizes? Explain here.}
% \begin{itemize}
    % \item Pretraining hyperparameters: network architecture, patch size, categorical distribution dimension or the number of prototypes K, teacher momentum, fully sharded data parallel training, [CLS] token alignment loss weight, MIM loss weight, global crop size, local crop size, masked ratio, number of epoch, number of iterations per epoch, AdamW optimizer, initial learning rate (2e-4), 
    % \item GLoRI hyperparameters: n last layer embeddings, embedding dimensions, query token initialization, the number of multi-head attention heads, 
    % \item learning rate search, training and validation combination
% \end{itemize}
% \subsection{Datasets and evaluation metrics}
% \subsubsection{Network architectures and pretraining protocol}
% \begin{itemize}
%     \item Vision Transformers with prototype heads for MIM and DINO alignment
%     \item Pretraining hyperparameters: iBOT loss weight, DINO loss weight, local crop size, global crop size, maximum masked ratio, etc.
% \end{itemize}
% \subsubsection{Data curation}
% Datasets such as MIMIC-CXR \cite{johnson2019mimic}, CheXpert \cite{irvin2019chexpert} and CXR14 \cite{wang2017chestx} have been combined to pretrain a vision foundation model \blue{in the previous work by Yao et al.} \cite{yao2024eva}. RAD-DINO\cite{perez2024rad} additionally added PadChest \cite{bustos2020padchest} and BRAX \cite{reis2022brax} datasets for foundation model pretraining. In this study, to further evaluate the data scaling capabilities of self-supervised vision encoders, we incorporated CXRs from diverse cohorts released for a variety of objectives, including abnormality detection \cite{nguyen2022vindr, lakhani20232021}, foreign object detection \cite{objectcxr}, segmentation \cite{feng2021curation, siim-acr}, diagnosis \cite{johnson2019mimic, irvin2019chexpert, wang2017chestx, wu2023covidx, bustos2020padchest, reis2022brax, midrc-covidx, vaya2020bimcv}, totaling 1,005,733 CXR images, as shown in Table \ref{tab:ptdata}. To learn the useful representations of CXRs in multiple views, both frontal-view in PA (posterior-anterior) or AUPRC (anterior-posterior) and lateral-view CXRs were included into the dataset.

% To perform pretraining at different scales, we derived subsets of the entire dataset containing $\sim$207K images from MIMIC-CXR and $\sim$744K images from MIMIC-CXR, CheXpert, PadChest, CXR14, and BRAX, respectively. During the preprocessing, images were resized and stored in the size of 512$\times$512 pixels.
% % \begin{itemize}
% %     \item Data from different centers: MIMIC-CXR, CheXpert, PadChest, BRAX, VinDr, etc.
% % \end{itemize}

% \begin{table}[th]
%     \setlength{\tabcolsep}{5pt}
%     \centering
%     \caption{Datasets from diverse institutions for self-supervised pretraining. Datasets were stratified into three cohorts at different scales to evaluate the data scaling law.}
%     \label{tab:ptdata}
%     \begin{tabular}{l | c | c | c }
%     \toprule
%      \multicolumn{1}{c|}{\textbf{Datasets}} & \textbf{\# images} & \textbf{View} & \textbf{Findings} \\ \midrule
%      MIMIC-CXR\cite{johnson2019mimic} & 207,096 & Frontal, Lateral & 14 diseases\\ \midrule
%      CheXpert\cite{irvin2019chexpert} & 223,648 & Frontal, Lateral & 14 diseases \\
%      PadChest\cite{bustos2020padchest} & 160,861 & Frontal, Lateral & 193 diseases \\
%      CXR14\cite{wang2017chestx} & 112,120 & Frontal & 14 diseases \\
%      BRAX\cite{reis2022brax} & 40,967 & Frontal, Lateal & 14 diseases \\ \midrule
%      VinDr-CXR\cite{nguyen2022vindr} & 18,000 & Frontal & 28 diseases \\
%      CANDID-PTX\cite{feng2021curation} & 19,237 & Frontal & Pneumothorax \\
%      SIIM-ACR\cite{siim-acr} & 18,499 & Frontal & Pneumothorax \\
%      Object-CXR\cite{objectcxr} & 9,000 & Frontal & Foreign objects\\
%      COVID-19\cite{lakhani20232021} & 7,597 & Frontal & COVID-19 \\
%      COVIDx CXR-4\cite{wu2023covidx} & 84,818 & Frontal & COVID-19 \\
%      MIDRC COVIDx\cite{midrc-covidx} & 23,001 & Frontal & COVID-19 \\
%      BIMCV COVID+\cite{vaya2020bimcv} & 80,889 & Frontal, Lateral & COVID-19 \\ \midrule
%      \textbf{Total} & 1,005,733 & \\
%      \bottomrule
%     \end{tabular}
% \end{table}
% \subsubsection{Prediction head training details}
% \begin{itemize}
%     \item Learning rates, 
% \end{itemize}

%\subsection{Data splits}

\subsection{Experimental Design}

% \py{Did other foundation models split dataset into training/val/test and test on a portion only? If the foundation models are frozen, the computation is fairly light. Why not perform cross validation? I guess you used the training set also for pretraining. You should mention it to clarify it. How about the val set, used in pretraining? If not, it is possible to do cross validation on val+test. What is your justification?}

% in-distribution and out-of-distribution datasets
To rigorously evaluate CheXFound's in-distribution and out-of-distribution performance, we employed an extensive classification benchmark, consisting of in-distribution datasets (CXR-LT 24 \cite{peng_2024_10991413} and CheXpert \cite{irvin2019chexpert}) and out-of-distribution datasets (Shenzhen \cite{jaeger2014two}, Montgomery\cite{jaeger2014two}, Japanese Society of Radiological Technology (JSRT) \cite{shiraishi2000development}, and Prostate, Lung, Colorectal, and Ovarian (PLCO) Cancer Screening Trial \cite{hocking2010lung}).
Following recent works in vision-centric foundation models \cite{perez2024rad, chen2024towards}, we split the evaluation datasets into training, validation, and test splits. To avoid any potential data contamination, we included only the training set in CXR-LT 24 and the training and validation sets in CheXpert for self-supervised pretraining, while keeping the test set unseen. For the out-of-distribution datasets (Shenzhen, Montgomery, and JSRT), none of the images in the training, validation, and test sets were used for self-supervised pretraining.


% \com{Do we need to add these benchmarking datasets to Table I?}
Although the MIMIC-CXR dataset is the common benchmark to evaluate the performance of CXR interpretation models, its labels contain only 14 findings.
%its CXR images were released with 14 binary labels indicating the existence of pathologies. 
To assess the generalizability of foundation models across diverse disease types, we conducted experiments on the CXR-LT 24 dataset which includes the annotations of 40 disease findings at different levels of prevalence (Fig. \ref{fig:prevalence_cxrlt}a). %by extracting labels from the MIMIC-CXR radiology reports. CXR-LT 24 released labels for a total of 258,871 CXRs. 
% of 258,871 CXRs 
To evaluate the model performance, we divided CXR-LT 24 into a training set of 207,096 images and a test set of 51,775 images. 
% [Be consistent about the name, either 24 or 2024, but not both]

To evaluate the performance of the foundation models against the annotations on five selected pathologies (atelectasis, cardiomegaly, consolidation, edema, and pleural effusion) by board-certificated radiologists, we incorporated the CheXpert dataset. It was divided into 191,027 frontal-view images in the training set,  202 images in the validation set, and 518 images in the test set. 

To assess the out-of-distribution generalization capabilities of the foundation models, we also performed evaluation on the Shenzhen and Montgomery datasets for tuberculosis detection and the JSRT database for lung nodule detection. 
% We note that none of these images was used in the pretraining.
The Shenzhen, Montgomery, and JSRT datasets were divided into training, validation, and test splits with a ratio of 70:10:20.
Shenzhen contains 463 training images, 65 validation images, and 134 test images. Montgomery contains 96 training images, 14 validation images, and 28 test images. JSRT contains 171 training images, 24 validation images, and 50 test images.

To evaluate the extended predictive power of the foundation models, we obtained the lung screening CXRs from the PLCO trial and extracted the all-cause mortality and cardiovascular disease mortality labels from the up to 25-year follow-up data. The PLCO CXRs were divided into training, validation, and test sets of 133,543 images, 19,099 images, and 38,058 images respectively.

% We split the Shenzhen dataset into 463 images for training, 65 images for validation, and 134 images for test. The Montgomery dataset was split into 96 images for training, 14 images for validation, and 28 images for test. The JSRT database composes of 171 training images, 24 validation images, and 50 test images.
% frontal view CXR
% chexpert, frontal view, radiologist-annotated test set
% \begin{itemize}
%     \item Information on data splits for CXR-LT, CheXpert, Shenzhen, Montgomery
%     \item Data splits information and CVD mortality and all-cause mortality statistics for PLCO (orientation correction)
% \end{itemize}

% \subsection{Evaluation methods}
% Since the CXR interpretation problem often involves severe class imbalance, we employed two metrics to evaluate model performance: mean average precision (AUPRC) and area under the receiver operating characteristic curve (AUROC). AUPRC is a valuable metric for evaluating classification models, especially when applied to imbalanced datasets on which AUROC tends to be inflated.
% The AUPRC score summarizes the precision-recall trade-off across various thresholds and is calculated for each class by computing the area under the precision-recall curve. The AUPRC score takes all classes under consideration by computing the mean of the AUPRC scores across classes: 
% \begin{equation}
% \text{AUPRC} = \frac{1}{C} \sum_{c=0}^{C-1}\sum_{n=1}^{N-1} (R_{c,n} - R_{c,n-1})P_{c,n},
% \end{equation}
% where $R_{c, n}, P_{c,n}$ are the recall and precision for class $c$ at the threshold index $n$, $C$ is the total number of classes, and $N$ is the number of thresholds. AUROC is a performance metric for classification models that indicates how well a model distinguishes between classes. The receiver operating characteristic (ROC) curve is the plot of the true positive rate (TRP) against the false positive rate (FPR) at various classification thresholds. The TPR measures the proportion of correctly identified positive samples, while the FPR measures the proportion of incorrectly identified negative samples. AUROC computes the area under the ROC curve, representing the model's overall ability to separate classes.

Since the CXR interpretation problem often involves severe class imbalance, we employed two metrics to evaluate model performance: the area under the precision-recall curve (AUPRC) and the area under the receiver operating characteristic curve (AUROC). 
For the multilabel classification problem, we computed the average of the metrics over disease findings.
% mean average precision (AUPRC)
% \py{Is AUPRC equivalent to AUPRC? If so, you should just use the terms of AUPRC and AUROC, which is not only easy to describe but also more commonly used in medical data analysis.}
We estimated the 95\% confidence intervals of the model performance in AUPRC and AUROC over 1,000 bootstrapped samples. To test statistical significance, we used a two-sided paired permutation test with 1,000 permutations to assess the observed performance differences of the two models for disease findings.
% \begin{itemize}
%     \item Mean average precision (AUPRC) metric  % useful for imbalanced datasets
%     \item Area under the receiver operating characteristic curve (AUROC) metric
%     \item Statistical significant test 
% \end{itemize}

\section{Experimental Results}

\begin{table*}
    \caption{Comparison CheXFound and other foundation models when using linear probe and GLoRI for classification. %presented in mean average precision (AUPRC) and area under the receiver operating characteristic curve (AUROC) over 1,000 bootstrapped samples. 
    Values inside the parentheses indicate the 95\% confidence intervals. Values in \textbf{bold} indicate the best results.}
    % \py{Pls use multirow to group `Linear probe's together. Similarly for `GLoRI head'.}
    % \py{Where do you introduce bootstrapping? I couldn't find it in the paper.}
    \label{tab:overall}
    \setlength{\tabcolsep}{1.2pt}
    \setlength{\extrarowheight}{2pt}
    \centering
    \begin{scriptsize}
    \begin{tabular}{c | c | c c | c c | c c | c c | c c}
    \toprule
    \textbf{Classifier} & \textbf{Foundation} & \multicolumn{2}{c}{\textbf{CXR-LT 24}} & \multicolumn{2}{c}{\textbf{CheXpert}} & \multicolumn{2}{c}{\textbf{Shenzhen}} & \multicolumn{2}{c}{\textbf{Montgomery}} & \multicolumn{2}{c}{\textbf{JSRT}} \\ \cmidrule(lr){3-12}
    \textbf{methods} & \textbf{models} & \textbf{AUPRC} & \textbf{AUROC} & \textbf{AUPRC} & \textbf{AUROC} & \textbf{AUPRC} & \textbf{AUROC} & \textbf{AUPRC} & \textbf{AUROC} & \textbf{AUPRC} & \textbf{AUROC} \\ \midrule
     \multirow{6}{*}{Linear probe} & PubMedCLIP \cite{eslami2021does} & 0.089\tiny(.088--.089) & 0.561\tiny(.554--.568) & 0.277\tiny(.244--.311) & 0.595\tiny(.564--.628) & 0.857\tiny(.786--.913) & 0.814\tiny(.738--.887) & 0.565\tiny(.306--.817) & 0.534\tiny(.310--.750) & 0.785\tiny(.630--.912) & 0.685\tiny(.511--.833) \\
     & BiomedCLIP \cite{zhang2023biomedclip} & 0.117\tiny(.116--.118) & 0.643\tiny(.636--.649) & 0.557\tiny(.504--.607) & 0.841\tiny(.822--.860) & 0.903\tiny(.843--.949) & 0.885\tiny(.827--.934) & 0.925\tiny(.791--1.0) & 0.929\tiny(.822--1.0) & 0.589\tiny(.427--.755) & 0.432\tiny(.265--.601) \\
     & CheXzero \cite{tiu2022expert} & 0.112\tiny(.111--.112) & 0.552\tiny(.545--.558) & 0.468\tiny(.424--.509) & 0.778\tiny(.754--.798) & 0.929\tiny(.884--.962) & 0.906\tiny(.851--.950) & 0.964\tiny(.869--1.0) & 0.970\tiny(.898--1.0) & 0.759\tiny(.597--.885) & 0.620\tiny(.453--.777) \\
     & EVA-X \cite{yao2024eva} & 0.114\tiny(.113--.115) & 0.596\tiny(.590--.602) & 0.468\tiny(.422--.512) & 0.788\tiny(.763--.812) & 0.840\tiny(.738--.921) & 0.824\tiny(.746--.898) & 0.577\tiny(.320--.809) & 0.509\tiny(.278--.749) & 0.641\tiny(.472--.797) & 0.490\tiny(.312--.660) \\
     & RAD-DINO \cite{perez2024rad} & 0.114\tiny(.113--.114) & 0.557\tiny(.570--.583) & 0.463\tiny(.422--.503) & 0.746\tiny(.715--.778) & 0.883\tiny(.818--.933) & 0.861\tiny(.798--.916) & 0.637\tiny(.369--.842) & 0.561\tiny(.316--.788) & 0.747\tiny(.585--.888) & 0.623\tiny(.456--.778) \\
     & CheXFound & 0.209\tiny(.204--.214) & 0.799\tiny(.794--.804) & 0.620\tiny(.630--.727) & 0.876\tiny(.860--.892) & 0.974\tiny(.949--.992) & 0.967\tiny(.935--.990) & 0.988\tiny(.939--1.0) & 0.990\tiny(.952--1.0) & 0.918\tiny(.826--.975) & 0.856\tiny(.741--.948) \\ \midrule
     \multirow{6}{*}{GLoRI head} & PubMedCLIP \cite{eslami2021does} & 0.116\tiny(.115--.117) & 0.649\tiny(.643--.655) & 0.501\tiny(.450--.552) & 0.804\tiny(.778--.828) & 0.897\tiny(.832--.946) & 0.867\tiny(.807--.927) & 0.628\tiny(.363--.866) & 0.694\tiny(.484--.872) & 0.652\tiny(.483--.810) & 0.493\tiny(.330--.654) \\
     & BiomedCLIP \cite{zhang2023biomedclip} & 0.122\tiny(.121--.123) & 0.643\tiny(.636--.649) & 0.552\tiny(.506--.593) & 0.829\tiny(.809--.847) & 0.921\tiny(.870--.957) & 0.897\tiny(.840--.944) & 0.900\tiny(.738--1.0) & 0.898\tiny(.744--1.0) & 0.634\tiny(.470--.789) & 0.505\tiny(.291--.704) \\
     & CheXzero \cite{tiu2022expert} & 0.131\tiny(.130--.132) & 0.671\tiny(.665--.677) & 0.599\tiny(.551--.647) & 0.888\tiny(.868--.905) & 0.912\tiny(.852--.954) & 0.894\tiny(.838--.942) & 0.726\tiny(.451--.901) & 0.653\tiny(.396--.882) & 0.640\tiny(.467--.807) & 0.485\tiny(.323--.662) \\
     & EVA-X \cite{yao2024eva} & 0.149\tiny(.147--.150) & 0.679\tiny(.672--.685) & 0.614\tiny(.571--.659) & 0.870\tiny(.853--.888) & 0.928\tiny(.881--.964) & 0.896\tiny(.835--.945) & 0.977\tiny(.909--1.0) & 0.979\tiny(.918--1.0) & 0.866\tiny(.760--.936) & 0.748\tiny(.615--.864) \\
     & RAD-DINO \cite{perez2024rad} & 0.173\tiny(.171--.176) & 0.723\tiny(.717--.729) & 0.639\tiny(.593--.687) & 0.884\tiny(.869--.898) & 0.909\tiny(.854--.952) & 0.885\tiny(.823--.936) & 0.909\tiny(.747--1.0) & 0.911\tiny(.782--1.0) & 0.683\tiny(.503--.861) & 0.615\tiny(.440--.783) \\
     & CheXFound & \textbf{0.252}\tiny(.247--.258) & \textbf{0.830}\tiny(.826--.834) & \textbf{0.679}\tiny(.630--.727) & \textbf{0.908}\tiny(.894--.921) & \textbf{0.983}\tiny(.960--.996) & \textbf{0.978}\tiny(.951--.995) & \textbf{1.000}\tiny(1.0--1.0) & \textbf{1.000}\tiny(1.0--1.0) & \textbf{0.986}\tiny(.956--1.0) & \textbf{0.975}\tiny(.931--1.0) \\
    \bottomrule
    \end{tabular}
    \end{scriptsize}
\end{table*}

\begin{table*}
    \caption{Comparison of CheXFound with the end-to-end trained model and vision-language foundation models.
    %presented in mean average precision (AUPRC) and area under the receiver operating characteristic curve (AUROC) over 1,000 bootstrapped samples. 
    Values inside the parentheses indicate the 95\% confidence intervals. Values in \textbf{bold} indicate the best results.}
    % \py{Pls use multirow to group `Img-text align's together.}
    \label{tab:compare_vlm}
    \setlength{\tabcolsep}{1.2pt}
    \setlength{\extrarowheight}{2pt}
    \centering
    \begin{scriptsize}
    \begin{tabular}{c | c | c c | c c | c c | c c | c c}
    \toprule
    \textbf{Classifier} & \textbf{Foundation} & \multicolumn{2}{c}{\textbf{CXR-LT 24}} & \multicolumn{2}{c}{\textbf{CheXpert}} & \multicolumn{2}{c}{\textbf{Shenzhen}} & \multicolumn{2}{c}{\textbf{Montgomery}} & \multicolumn{2}{c}{\textbf{JSRT}} \\ \cmidrule(lr){3-12}
     \textbf{methods} & \textbf{models} & \textbf{AUPRC} & \textbf{AUROC} & \textbf{AUPRC} & \textbf{AUROC} & \textbf{AUPRC} & \textbf{AUROC} & \textbf{AUPRC} & \textbf{AUROC} & \textbf{AUPRC} & \textbf{AUROC} \\ \midrule
      End-to-end & ConvNeXt \cite{liu2022convnet} & 0.170\tiny(.168--.173) & 0.761\tiny(.756--.766) & 0.657\tiny(.608--.704) & 0.886\tiny(.867--.903) & 0.923\tiny(.870--.963) & 0.891\tiny(.823--.945) & 0.622\tiny(.362--.862) & 0.670\tiny(.458--.872) & 0.750\tiny(.586--.886) & 0.608\tiny(.456--.761) \\ \midrule
      \multirow{3}{*}{Img-text align.} & PubMedCLIP \cite{eslami2021does} & 0.068\tiny(.068--.069) & 0.531\tiny(.524--.537) & 0.255\tiny(.221--.294) & 0.582\tiny(.550--.618) & 0.577\tiny(.457--.700) & 0.540\tiny(.437--.637) & 0.440\tiny(.246--.672) & 0.426\tiny(.216--.658) & 0.596\tiny(.426--.758) & 0.403\tiny(.244--.572) \\
      & BiomedCLIP \cite{zhang2023biomedclip} & 0.071\tiny(.071--.072) & 0.539\tiny(.533--.545) & 0.356\tiny(.317--.393) & 0.653\tiny(.626--.681) & 0.795\tiny(.705--.878) & 0.760\tiny(.680--.836) & 0.786\tiny(.567--.936) & 0.714\tiny(.487--.912) & 0.692\tiny(.516--.859) & 0.554\tiny(.397--.722) \\
      & CheXzero \cite{tiu2022expert} & 0.134\tiny(.133--.136) & 0.668\tiny(.662--.674) & 0.646\tiny(.600--.692) & 0.888\tiny(.868--.905) & 0.875\tiny(.804--.933) & 0.849\tiny(.776--.911) & 0.967\tiny(.870--1.0) & 0.969\tiny(.889--1.0) & 0.708\tiny(.532--.863) & 0.530\tiny(.378--.698) \\ \midrule
      GLoRI head & CheXFound & \textbf{0.252}\tiny(.247--.258) & \textbf{0.830}\tiny(.826--.834) & \textbf{0.679}\tiny(.630--.727) & \textbf{0.908}\tiny(.894--.921) & \textbf{0.983}\tiny(.960--.996) & \textbf{0.978}\tiny(.951--.995) & \textbf{1.000}\tiny(1.0--1.0) & \textbf{1.000}\tiny(1.0--1.0) & \textbf{0.986}\tiny(.956--1.0) & \textbf{0.975}\tiny(.931--1.0) \\
    \bottomrule
    \end{tabular}
    \end{scriptsize}
\end{table*}

\subsection{Overall performance comparison}
% independent heads for each datasets
% ablation study that combine the datasets (CXR-LT, CheXpert) for training
% \begin{itemize}
%     \item Perform plug-and-play bootstrapping from multiple frozen image encoder
% \end{itemize}
A pivotal characteristic of foundation models lies in their capability to achieve improved performance on a wide range of downstream datasets. To evaluate the capability of foundation models, we compared CheXFound, which uses ViT-L pretrained on CXR-1M, with publicly available pretrained encoders, including RAD-DINO \cite{perez2024rad}, EVA-X \cite{yao2024eva}, CheXzero \cite{tiu2022expert}, BiomedCLIP \cite{zhang2023biomedclip}, PubmedCLIP \cite{eslami2021does}, and ConvNeXt \cite{liu2022convnet}. 
RAD-DINO was pretrained on a combined dataset comprising MIMIC-CXR, CheXpert, PadChest, CXR14, and BRAX, using the DINOv2 framework. EVA-X was pretrained on MIMIC-CXR, CheXpert, and CXR14 using EVA \cite{fang2023eva} technique with contrastive vision features for masked image modeling. CheXzero, a vision-language foundation model, was pretrained on MIMIC-CXR using contrastive language-image pretraining (CLIP) \cite{radford2021learning}. BiomedCLIP was pretrained on PMC-15M with image-text pairs collected from scientific articles. PubmedCLIP was initialized with the CLIP model and finetuned on PubMed articles. Finally, ConvNeXt was pretraiend on the ImageNet-22K dataset. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/cxrlt_radarplot3.pdf}
    \caption{Detailed Performance for 40 disease findings in AUROC on the CXR-LT 24 dataset. Our CheXFound is compared with the vision-centric foundation models (EVA-X and RAD-DINO), the vision-language pretrained foundation models (CheXzero, BiomedCLIP, and PubMedCLIP), and the end-to-end trained model (ConvNeXt) with ImageNet-22K pretraining.}
    \label{fig:radar_cxrlt}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/cxrlt_prevalence_ap.pdf}
    \caption{Model performance under high, medium and low disease prevalence. \textbf{a}, The number of labels for the 40 disease findings on the CXR-LT 24 dataset \cite{peng_2024_10991413}. \textbf{b}, Model performance in AUPRC stratified by high, medium and low disease prevalence. Error bars indicate the 95\% confidence intervals of AUPRC over 1,000 bootstrapped samples.}
    \label{fig:prevalence_cxrlt}
\end{figure}

% We investigated the capabilities of CheXFound on five publicly available CXR datasets, provided by different institutions. These include CXR-LT 24 \cite{peng_2024_10991413}, which captures 40 disease findings with a long-tail distribution as shown in Fig. \ref{fig:prevalence_cxrlt}a; CheXpert \cite{irvin2019chexpert}, containing five radiologist-annotated pathologies (atelectasis, cardiomegaly, consolidation, edema, and pleural effusion); Shenzhen and Montgomery \cite{jaeger2014two} for tuberculosis detection; and JSRT \cite{shiraishi2000development} designed for lung nodule detection. \py{This paragraph is repeating the content under experimental design. Please merge into there.}

To evaluate the effectiveness and generalizability of the representations extracted by the foundation models, we evaluated the linear probe performance across the five datasets (CXR-LT 24, CheXpert, Shenzhen, Montgomery, and JSRT). While linear probe provides a straightforward approach to evaluate the  quality of representation, it primarily relies on global image features from the \texttt{[CLS]} token, often resulting in suboptimal performance. Hence, we also evaluated the pretrained encoders using GLoRI, which incorporates attention-pooled local features in addition to the global image features from the \texttt{[CLS]} token. For vision-language pretained encoders, we further validated the quality of their vision representations by examing their correlation with text features for disease finding classification.

% \py{Table 2 is hard to interpret, its organization doesn't match the description below. For example, if you want to talk about linear probe first, then your Table 2 should show the performance under linear probing together. Otherwise, readers have to scan the table multiple times to collect the needed information when reading this paragraph.}
Across all the five classification tasks (CXR-LT 24, CheXpert, Shenzhen, Montgomery, and JSRT), CheXFound consistently outperformed other foundation models in the linear probe setting as shown in Table \ref{tab:overall}. 
% \py{Is frozen backbone equal to pretrained encoder as in the above paragraph? Keep the terms consistent.} 
On the multilabel, long-tailed classification task (CXT-LT 24), CheXFound achieved an AUPRC of 0.209, outperforming the next best-performing model (either RAD-DINO or EVA-X) by 9.5\% ($p<0.001$, two-sided paired permutation test).
On the five-class multilabel classification task (CheXpert), CheXFound outperformed the next best-performing model (EVA-X) by 8.8\%  ($p<0.001$) in AUROC. On single-class classification tasks (Shenzhen, Montgomery, and JSRT) with limited amounts of training data, CheXFound similarly outperformed the next best-performing models in AUROC by 11.7\% ($p<0.001$), 42.9\% ($p<0.001$), and 23.3\% ($p<0.001$), respectively.
% \py{What are these tasks? Specify them to avoid confusion.}
% \py{Is 0.209 good or bad? Give readers a context, for example the range of AUPRC, in what range a method would be considered good.}
% \py{what are these tasks?} across the three institutions

% \py{Alternatively, change how you describe the results to match Table 2. Ask yourself the question: What is the most important thing to illustrate through Table 2? That should be at the highest level. Then the second priority, etc.}
We further evaluated the performance of the foundation models using GLoRI across five classification tasks (Table \ref{tab:overall}). The foundation models (CheXFound, RAD-DINO, EVA-X, CheXzero, BiomedCLIP, and PubMedCLIP) with GLoRI generally outperformed their linear probe baselines. Specifically, CheXFound with GLoRI outperformed its linear probe baseline in AUROC by 3.1\%, 3.2\%, 1.1\%, 1.0\%, and 11.9\% on CXR-LT 24, CheXpert, Shenzhen, Montgomery, and JSRT, respectively. CheXFound with GLoRI also outperformed other foundation models, including CLIP-based models (CheXzero, BioMedCLIP, and PubMedCLIP) under image-text alignment, as well as end-to-end trained ConvNeXt (Table \ref{tab:compare_vlm}). 

% \py{Justify to readers why do you need the comparison below. Use your figure well to convey more information, but not just nice to have.}
To show the detailed performance of CheXFound over 40 disease findings, we illustrated CheXFound performance in AUROC against its comparisons in Fig. \ref{fig:radar_cxrlt}. CheXFound consistently outperformed other methods across the 40 disease findings in AUROC. We also compared performance in AUPRC over disease findings with high, medium, and low prevalence in Fig. \ref{fig:radar_cxrlt}b. CheXFound outperformed its comparisons under all levels of prevalence, even for underrepresented pathologies in the low prevalence category.


% start with linear probe performance compared with vision encoders (linear probe performance) and vision language encoders (linear probe and image-text alignment)
% continue with GLoRI compared with linear probe performance, compared with GLoRI performance across pretrained encoders, and compared with ConvNeXt performance.

% on five datasets, including CXR-LT 24 \cite{peng_2024_10991413}, CheXpert \cite{irvin2019chexpert}, Shenzhen \cite{jaeger2014two}, Montgomery \cite{jaeger2014two}, and JSRT \cite{shiraishi2000development}.

% \begin{table*}[!h]
%     \caption{\cyan{Performance of CheXFound and other foundation models on five CXR classification datasets presented in mean average precision (AUPRC) and area under the receiver operating characteristic curve (AUROC) over 1,000 bootstrapped samples.} \py{Where do you introduce bootstrapping? I couldn't find it in the paper.} Values inside the parentheses indicate the 95\% confidence intervals. Values in \textbf{bold} indicate the best results.}
%     \label{tab:overall}
%     \setlength{\tabcolsep}{1.2pt}
%     \setlength{\extrarowheight}{3pt}
%     \centering
%     \begin{scriptsize}
%     \begin{tabular}{c | c | c c | c c | c c | c c | c c}
%     \toprule
%     \textbf{Foundation} & \textbf{Classifier} & \multicolumn{2}{c}{\textbf{CXR-LT 24}} & \multicolumn{2}{c}{\textbf{CheXpert}} & \multicolumn{2}{c}{\textbf{Shenzhen}} & \multicolumn{2}{c}{\textbf{Montgomery}} & \multicolumn{2}{c}{\textbf{JSRT}} \\ \cmidrule(lr){3-12}
%      \textbf{models} & \textbf{methods} &  \textbf{AUPRC} & \textbf{AUROC} & \textbf{AUPRC} & \textbf{AUROC} & \textbf{AUPRC} & \textbf{AUROC} & \textbf{AUPRC} & \textbf{AUROC} & \textbf{AUPRC} & \textbf{AUROC} \\ \midrule
%      ConvNeXt \cite{liu2022convnet} & End-to-end & 0.170\tiny(.168--.173) & 0.761\tiny(.756--.766) & 0.657\tiny(.608--.704) & 0.886\tiny(.867--.903) & 0.923\tiny(.870--.963) & 0.891\tiny(.823--.945) & 0.622\tiny(.362--.862) & 0.670\tiny(.458--.872) & 0.750\tiny(.586--.886) & 0.608\tiny(.456--.761) \\ \midrule
%      \multirow{3}{*}{PubMedCLIP \cite{eslami2021does}} & Img-text align. & 0.068\tiny(.068--.069) & 0.531\tiny(.524--.537) & 0.255\tiny(.221--.294) & 0.582\tiny(.550--.618) & 0.577\tiny(.457--.700) & 0.540\tiny(.437--.637) & 0.440\tiny(.246--.672) & 0.426\tiny(.216--.658) & 0.596\tiny(.426--.758) & 0.403\tiny(.244--.572) \\
%      & Linear probe & 0.089\tiny(.088--.089) & 0.561\tiny(.554--.568) & 0.277\tiny(.244--.311) & 0.595\tiny(.564--.628) & 0.857\tiny(.786--.913) & 0.814\tiny(.738--.887) & 0.565\tiny(.306--.817) & 0.534\tiny(.310--.750) & 0.785\tiny(.630--.912) & 0.685\tiny(.511--.833) \\
%      & GLoRI head & 0.116\tiny(.115--.117) & 0.649\tiny(.643--.655) & 0.501\tiny(.450--.552) & 0.804\tiny(.778--.828) & 0.897\tiny(.832--.946) & 0.867\tiny(.807--.927) & 0.628\tiny(.363--.866) & 0.694\tiny(.484--.872) & 0.652\tiny(.483--.810) & 0.493\tiny(.330--.654) \\ \midrule
%      \multirow{3}{*}{BiomedCLIP \cite{zhang2023biomedclip}} & Img-text align. & 0.071\tiny(.071--.072) & 0.539\tiny(.533--.545) & 0.356\tiny(.317--.393) & 0.653\tiny(.626--.681) & 0.795\tiny(.705--.878) & 0.760\tiny(.680--.836) & 0.786\tiny(.567--.936) & 0.714\tiny(.487--.912) & 0.692\tiny(.516--.859) & 0.554\tiny(.397--.722) \\
%      & Linear probe & 0.117\tiny(.116--.118) & 0.643\tiny(.636--.649) & 0.557\tiny(.504--.607) & 0.841\tiny(.822--.860) & 0.903\tiny(.843--.949) & 0.885\tiny(.827--.934) & 0.925\tiny(.791--1.0) & 0.929\tiny(.822--1.0) & 0.589\tiny(.427--.755) & 0.432\tiny(.265--.601) \\
%      & GLoRI head & 0.122\tiny(.121--.123) & 0.643\tiny(.636--.649) & 0.552\tiny(.506--.593) & 0.829\tiny(.809--.847) & 0.921\tiny(.870--.957) & 0.897\tiny(.840--.944) & 0.900\tiny(.738--1.0) & 0.898\tiny(.744--1.0) & 0.634\tiny(.470--.789) & 0.505\tiny(.291--.704) \\ \midrule
%      \multirow{3}{*}{CheXzero \cite{tiu2022expert}} & Img-text align. & 0.134\tiny(.133--.136) & 0.668\tiny(.662--.674) & 0.646\tiny(.600--.692) & 0.888\tiny(.868--.905) & 0.875\tiny(.804--.933) & 0.849\tiny(.776--.911) & 0.967\tiny(.870--1.0) & 0.969\tiny(.889--1.0) & 0.708\tiny(.532--.863) & 0.530\tiny(.378--.698) \\ 
%      & Linear probe & 0.112\tiny(.111--.112) & 0.552\tiny(.545--.558) & 0.468\tiny(.424--.509) & 0.778\tiny(.754--.798) & 0.929\tiny(.884--.962) & 0.906\tiny(.851--.950) & 0.964\tiny(.869--1.0) & 0.970\tiny(.898--1.0) & 0.759\tiny(.597--.885) & 0.620\tiny(.453--.777) \\
%      & GLoRI head & 0.131\tiny(.130--.132) & 0.671\tiny(.665--.677) & 0.599\tiny(.551--.647) & 0.888\tiny(.868--.905) & 0.912\tiny(.852--.954) & 0.894\tiny(.838--.942) & 0.726\tiny(.451--.901) & 0.653\tiny(.396--.882) & 0.640\tiny(.467--.807) & 0.485\tiny(.323--.662) \\
%      \midrule
%      \multirow{2}{*}{EVA-X \cite{yao2024eva}} & Linear probe & 0.114\tiny(.113--.115) & 0.596\tiny(.590--.602) & 0.468\tiny(.422--.512) & 0.788\tiny(.763--.812) & 0.840\tiny(.738--.921) & 0.824\tiny(.746--.898) & 0.577\tiny(.320--.809) & 0.509\tiny(.278--.749) & 0.641\tiny(.472--.797) & 0.490\tiny(.312--.660) \\
%       & GLoRI head & 0.149\tiny(.147--.150) & 0.679\tiny(.672--.685) & 0.614\tiny(.571--.659) & 0.870\tiny(.853--.888) & 0.928\tiny(.881--.964) & 0.896\tiny(.835--.945) & 0.977\tiny(.909--1.0) & 0.979\tiny(.918--1.0) & 0.866\tiny(.760--.936) & 0.748\tiny(.615--.864) \\ \midrule
%      \multirow{2}{*}{RAD-DINO \cite{perez2024rad}} & Linear probe  & 0.114\tiny(.113--.114) & 0.557\tiny(.570--.583) & 0.463\tiny(.422--.503) & 0.746\tiny(.715--.778) & 0.883\tiny(.818--.933) & 0.861\tiny(.798--.916) & 0.637\tiny(.369--.842) & 0.561\tiny(.316--.788) & 0.747\tiny(.585--.888) & 0.623\tiny(.456--.778) \\
%       & GLoRI head & 0.173\tiny(.171--.176) & 0.723\tiny(.717--.729) & 0.639\tiny(.593--.687) & 0.884\tiny(.869--.898) & 0.909\tiny(.854--.952) & 0.885\tiny(.823--.936) & 0.909\tiny(.747--1.0) & 0.911\tiny(.782--1.0) & 0.683\tiny(.503--.861) & 0.615\tiny(.440--.783) \\ \midrule
%      \multirow{2}{*}{CheXFound} & Linear probe & 0.209\tiny(.204--.214) & 0.799\tiny(.794--.804) & 0.620\tiny(.630--.727) & 0.876\tiny(.860--.892) & 0.974\tiny(.949--.992) & 0.967\tiny(.935--.990) & 0.988\tiny(.939--1.0) & 0.990\tiny(.952--1.0) & 0.918\tiny(.826--.975) & 0.856\tiny(.741--.948) \\
%       & GLoRI head & \textbf{0.252}\tiny(.247--.258) & \textbf{0.830}\tiny(.826--.834) & \textbf{0.679}\tiny(.630--.727) & \textbf{0.908}\tiny(.894--.921) & \textbf{0.983}\tiny(.960--.996) & \textbf{0.978}\tiny(.951--.995) & \textbf{1.000}\tiny(1.0--1.0) & \textbf{1.000}\tiny(1.0--1.0) & \textbf{0.986}\tiny(.956--1.0) & \textbf{0.975}\tiny(.931--1.0) \\
%     \bottomrule
%     \end{tabular}
%     \end{scriptsize}
% \end{table*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=\textwidth]{figs/cxrlt_barplot_ap_all2.pdf}
%     \caption{Detailed performance in Average Precision on CXR-LT.}
%     \label{fig:baplot_cxrlt}
% \end{figure*}

% \subsection{Self-supervised vision encoders for thoracic disease detection}
% A key characteristics of CXR foundation models lies in their capability to deliver improved performance over a wide range of chest pathology detection tasks when pretraiend with a large-scale dataset. Datasets such as CheXpert\cite{irvin2019chexpert}, NIH CXRs\cite{wang2017chestx}, and Shenzhen Tuberculosis CXRs\cite{jaeger2014two} are commonly used to benchmark pretrained encoders on binary or multilabel pathology detection tasks. However, these datasets only associate CXRs with high prevalence disease findings, which is not reflective of the large number of uncommon disease findings that radiologists encounter in clinical practice. Hence, adding to these datasets, we assess the generalization capabilities of our pretrained encoder on the CXR-LT 2024 (CXR Long Tail Challenge 2024) dataset\cite{holste2024towards} where each CXR is annotated with binary labels of 40 disease findings from high to low prevalence, posing a challenging multilabel, long-tailed disease detection problem. We note that CXR-LT 2024 is still orders of magnitude away from encapsulating diseases and imaging findings documented in Radiology Gamuts Ontology\cite{budovec2014informatics} but it is useful to examine the richness of semantic representations and predictive power of our foundation model. We pretrained our foundation model using ViT-Large on over 710K CXRs. CXRs for test-time performance evaluation is excluded from the pretraining dataset to avoid any contamination of information leakage. We compared our foundation model with state-of-the-art pretrained encoders including MoCo-CXR\cite{sowrirajan2021moco} and CheXzero\cite{tiu2022expert}. Since there exists severe class imbalance problem on the evaluation dataset, the area under the receiver operating characteristic curve (AUROC) metric can be heavily inflated. To reflect label complexity challenges, we report AUROC as well as average precision (AUPRC) that is robust even to low prevalence disease findings. Additional details regarding experimental datasets, model setup, and implementation details are provided in Methods.

\subsection{Opportunistic predictive power}

Beyond thoracic disease detection tasks, we evaluated ChesXFound's generalizability in opportunistic CXR interpretation. For this purpose, we requested access to the CXR arm of the PLCO trial \cite{oken2011screening}, which includes digitally scanned CXR films and up to 25-year morality follow-up data. Using this dataset, we investigated CheXFound's predictive capability for cardiovascular disease (CVD) risk and all-cause mortality estimation. 
%For CVD risk estimation, we predict the probability of CVD mortality. The underlying causes of death for CVD mortality includes the ischemic heart disease, cerebrovascular accident, and other circulatory disease. 
% The causes of deaths for all-cause mortality further include the respiratory illness, digestive disease, infectious disease, lung, among others. 
We used CheXFound with ViT-L pretrained on CXR-1M in this experiment and compared CheXFound against two vision foundation models (RAD-DINO, EVA-X) and the end-to-end trained model, ConvNeXt.

CheXFound consistently outperformed its counterparts in both CVD risk and all-cause mortality estimation tasks. CheXFound achieved 0.749 for CVD risk estimation and 0.786 for all-cause mortality estimation in AUROC, significantly outperforming the next best-performing method (ConvNeXt) by 3.5\% ($p<0.001$) and 4.1\% ($p<0.001$), respectively. For the all-cause mortality estimation task, we divided the test cohort into low-risk and high-risk groups based on the model prediction and computed their Kaplan-Meier curves (Fig. \ref{fig:kaplan-meier}). The survival distributions for the low-risk and high-risk groups are statistically different ($p<0.001$, log-rank test). The end-point survival probabilities are also different by a large margin for low-risk and high-risk groups (78.4\% versus 38.4\%). Overall, we demonstrated CheXFound generalization capability for opportunistic CXR interpretation.

\begin{table}[t]
    \centering
    \setlength{\tabcolsep}{1pt}
    \setlength{\extrarowheight}{2pt}
    \caption{Model performance on CVD risk and all-cause mortality estimation. Values inside the parentheses are 95\% confidence intervals. Values in \textbf{bold} indicate the best-performing results.}
    \label{tab:plco}
    \begin{tabular}{c | c c | c c}
        \toprule
        \multirow{2}{*}{\textbf{Methods}} & \multicolumn{2}{c}{\textbf{CVD risk}} & \multicolumn{2}{c}{\textbf{All-cause mortality}} \\ \cmidrule(lr){2-5}
        & \textbf{AUPRC} & \textbf{AUROC} & \textbf{AUPRC} & \textbf{AUROC} \\ \midrule
        ConvNeXt \cite{liu2022convnet} & 0.249\tiny(.240--.257) & 0.714\tiny(.705--.723) & 0.638\tiny(.629--.646) & 0.745\tiny(.736--.753) \\
        EVA-X \cite{yao2024eva} & 0.179\tiny(.171--.188) & 0.643\tiny(.635--.652) & 0.545\tiny(.536--.554) & 0.680\tiny(.674--.686) \\
        RAD-DINO \cite{perez2024rad} & 0.223\tiny(.215--231) & 0.687\tiny(.679--.695) & 0.615\tiny(.607--.622) & 0.723\tiny(.716--.729) \\
        CheXFound & \textbf{0.289}\tiny(.276--.301) & \textbf{0.749}\tiny(.741--.756) & \textbf{0.695}\tiny(.687--.702) & \textbf{0.786}\tiny(.782--.791) \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/survival_rate.pdf}
    \caption{Kaplan-Meier curves for low-risk and high-risk groups of all-cause mortality on the PLCO dataset. The number of subjects in the test set is 10,509. The survival rates for low-risk and high-risk groups are significantly different (p$<$0.0001, log-rank test). Shaded areas indicate 95\% confidence intervals.}
    \label{fig:kaplan-meier}
\end{figure}

\begin{table*}
    \centering
    \caption{Model Performance across pretraining data sizes and model scales. Results are given in the mean values of AUPRC and AUROC over 1000 bootstrapped samples. Values inside the parentheses indicate the 95\% confidence intervals. Values in \textbf{bold} indicate the best-performing results.}
    \label{tab:scale}
    \setlength{\tabcolsep}{1.8pt}
    \setlength{\extrarowheight}{2pt}
    \begin{scriptsize}
    \begin{tabular}{c | c | c c | c c | c c | c c | c c}
    \toprule
    \textbf{Pretrain.} & \multirow{2}{*}{\textbf{Arch.}} & \multicolumn{2}{c}{\textbf{CXR-LT 24}} & \multicolumn{2}{c}{\textbf{CheXpert}} & \multicolumn{2}{c}{\textbf{Shenzhen}} & \multicolumn{2}{c}{\textbf{Montgomery}} & \multicolumn{2}{c}{\textbf{JSRT}} \\ \cmidrule(lr){3-12}
    \textbf{Data} & & \textbf{AUPRC} & \textbf{AUROC} & \textbf{AUPRC} & \textbf{AUROC} & \textbf{AUPRC} & \textbf{AUROC} & \textbf{AUPRC} & \textbf{AUROC} & \textbf{AUPRC} & \textbf{AUROC} \\ \midrule
    \multirow{2}{*}{CXR-207K} & ViT-Base & 0.185\tiny(.182â€“.188) & 0.775\tiny(.753--.789) & 0.598\tiny(.550â€“.645) & 0.854\tiny(.839--.862) & 0.921\tiny(.898--.946) & 0.909\tiny(.855â€“.953) & 0.889\tiny(.732--1.0) & 0.897\tiny(.742â€“1.0) & 0.756\tiny(.591â€“.882) & 0.612\tiny(.495--.712) \\ 
     & ViT-Large & 0.207\tiny(.203â€“.210) & 0.795\tiny(.774--.813) & 0.618\tiny(.629â€“.726) & 0.874\tiny(.862--.884) & 0.953\tiny(.923--.979) & 0.940\tiny(.901--.962) & 0.923\tiny(.811--1.0) & 0.932\tiny(.826--1.0) & 0.775\tiny(.620â€“.892) & 0.668\tiny(.537--.784) \\  \midrule
    \multirow{2}{*}{CXR-744K} & ViT-Base & 0.211\tiny(.208â€“.214) & 0.803\tiny(.792--.810) & 0.614\tiny(.571â€“.659) & 0.869\tiny(.854--.886) & 0.938\tiny(.913--.961) & 0.925\tiny(.875â€“.963) & 0.909\tiny(.775--1.0) & 0.915\tiny(.785â€“1.0) & 0.866\tiny(.759â€“.935) & 0.747\tiny(.695--.787) \\ 
    & ViT-Large & 0.217\tiny(.214â€“.221) & 0.813\tiny(.801--.822) & 0.643\tiny(.596â€“.692) & 0.887\tiny(.876--.896) & 0.964\tiny(.935â€“.986)) & 0.956\tiny(.923--.972) & 0.953\tiny(.843--1.0) & 0.957\tiny(.845--1.0) & 0.905\tiny(.794--.953) & 0.826\tiny(.674--.925) \\ \midrule
    \multirow{2}{*}{CXR-1M} & ViT-Base & 0.219\tiny(.215â€“.223) & 0.815\tiny(.811--.819)  & 0.632\tiny(.583â€“.679) & 0.877\tiny(.853--.896) & 0.957\tiny(.924--.979) & 0.947\tiny(.925â€“.963) & 0.923\tiny(.788â€“1.0) & 0.927\tiny(.818â€“1.0)  & 0.908\tiny(.806â€“.955) & 0.845\tiny(.696--.943) \\ 
    & ViT-Large & \textbf{0.252}\tiny(.247--.258) & \textbf{0.830}\tiny(.826--.834) & \textbf{0.679}\tiny(.630--.727) & \textbf{0.908}\tiny(.894--.921) & \textbf{0.983}\tiny(.960--.996) & \textbf{0.978}\tiny(.951--.995) & \textbf{1.000}\tiny(1.0--1.0) & \textbf{1.000}\tiny(1.0--1.0) & \textbf{0.986}\tiny(.956--1.0) & \textbf{0.975}\tiny(.931--1.0) \\
    \bottomrule
    \end{tabular}                
    \end{scriptsize}
\end{table*}

% \py{What is the difference between AUPRC and AUPRC in the table?} \zf{CXR-LT 24 and CheXpert have multiple labels. The prefix m means macro average.}
% \begin{itemize}
%     \item CVD mortality on PLCO dataset
%     \item All-cause mortality on PLCO dataset
% \end{itemize}

% \subsection{Comparison with frozen pretrained features from open-source foundation models}
% \begin{itemize}
%     \item EVA-X
%     \item RAD-DINO
%     \item CheXagent
% \end{itemize}

% \subsection{Label efficiency of few-shot classification}
% We additionally evaluated CheXFound few-shot learning performance. Few-shot learning is an evaluation scheme to investigate the generalization capabilities of the pretrained encoders given a limited number of samples per class. For all pretrained encoders, we trained a linear probe classifier with the number of sample(s) per class $K\in\{1, 2, 4, 8, 16, 32\}$.

\subsection{Scalability of self-supervised vision encoders}

The scaling capabilities of the self-supervised vision encoders depends on both the model size and the pretraining data size and diversity \cite{oquab2023dinov2, chen2024towards}. To analyze the scaling trends, we pretrained CheXFound across a range of data scales, with CXR-1M and its two subsets CXR-744K and CXR-207K. We also evaluated the impact of model scale by using ViT-Base (ViT-B) and ViT-Large (ViT-L) as the backbones.

Our results in Table~\ref{tab:scale} demonstrate that CheXFound benefits form both data and model scaling. Increasing the pretraining data from CXR-207K to CXR-1M with a ViT-L backbone leads to significant AUROC improvements of 3.5\% ($p<0.001$) on CXR-LT 24, 3.4\% ($p<0.001$) on CheXpert, 3.8\% ($p<0.001$) on Shenzhen, 6.8\% ($p<0.001$) on Montgomery, and 30.7\% ($p<0.001$) on JSRT. We observed similar trends when using ViT-B, with performance also improving as we scale up the pretraining data from CXR-207K to CXR-1M. In addition, CheXFound with ViT-L consistently outperformed the ViT-B architecture across different data sizes. These results align with previous studies on scaling ViT models \cite{oquab2023dinov2, perez2024rad, chen2024towards}.

% \begin{itemize}
%     \item Evaluation of the generalizability of the model pretrained on 207K, 710K, and 971K CXRs on CXR-LT, CheXpert, Shenzhen, Montomery, and JSRT
%     \item Evaluation of the performance for the ViT-B (12 layers, 86M param.) and ViT-L (24 layers, 1024M param.) network architectures
% \end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/role_resolution.pdf}
    \caption{Evaluation of CheXFound model on the CXR-LT 24, CheXpert, Shenzhen, Montgomery, and JSRT datasets across a range of pretraining image resolutions. Error bars indicate 95\% confidence intervals.}
    % \py{five findings? what are they?} 
    \label{fig:role_res}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/glori_attns.pdf}
    \caption{GLoRI attention maps for disease findings of atelectasis, cardiomegaly, consolidation, edema, and pleural effusion, respectively. Each subfigure contains 25 CXRs with their GLoRI attention maps overlaid (bottom) and an anchor CXR with a global attention map overlaid (top).
    % \red{randomly selected [correct? please confirm.]}
    %Green regions in the attention maps indicate critical anatomical locations. The 25 CXRs are with a single disease finding and high predictive probabilities from our CheXFound model. 
    The global attention maps are the averaged attention maps of 25 CXRs after registered to the anchor image. 
    % \py{please reduce the caption font size.}
    % \py{Be consistent on using CXR or chest X-ray. Use one or the other, but not both.}
    }
    \label{fig:glori-attns}
\end{figure*}

\subsection{Impact of CXR resolution}

%Increasing input CXR resolution is key to delivering improved performance in downstream tasks \cite{yang2024cardiovascular, perez2024rad}. 
To assess the impact of CXR resolution used for pretraining, we pretrained CheXFound using ViT-L with a patch size of 16 across a range of input resolution, including $224^2$, $336^2$, $448^2$, and $512^2$. 
% \py{These are image sizes but not resolutions. Convert to the resolution of images.} 
% \zf{Image resolution generally means image size in recent publications on self-supervised pretraining (such as DINOv2). I follow their terminology.}
We empirically found that pretraining at high resolution from scratch cannot produce meaningful representations for downstream tasks. To deal with this problem, we pretrained CheXFound at resolution $224^2$ from scratch and then used the pretrained weights to initialize higher-resolution pretraining at $336^2$, $448^2$, and $512^2$. 

Fig. \ref{fig:role_res} shows that self-supervised pretraining at higher resolutions results in improved performance on downstream tasks. Increasing the pretraining resolutions from 224$^2$ pixels to 512$^2$ pixels significantly improves the AUROC by 3.3\% ($p<0.001$), 3.5\% ($p<0.001$), 3.7\% ($p<0.001$), 10.7\% ($p<0.001$), and 35.8\% ($p<0.001$) on the CXR-LT 24, CheXpert, Shenzhen, Montgomery, and JSRT datasets, respectively.
% \py{on what???}
% \begin{itemize}
%     \item Evaluate performance of the self-supervised vision encoders (ViT-L) at 224, 336, 448, 512 on CXR-LT
% \end{itemize}

% \subsection{Role of feature combination}
% \begin{itemize}
%     \item Ablation studies on how many blocks of features used for final predictions
% \end{itemize}

\section{Discussion and Conclusion}
\subsection{Interpretation of disease-specific local features}
\label{sec:intepret}
The interpretabliliy of an artificial intelligence model is crucial to its medical applications. 
%Interpretable models provide explanations for their decision-making processes, contributing to a trustworthy predictions. Previous studies have proposed to obtain visual explanations, such as class activation maps (CAMs) \cite{zhou2016learning} and gradient-weighted class activation maps (Grad-CAMs) \cite{selvaraju2017grad}, from deep neural networks to interpret predictions. However, this type of visual explaination tracks the contributions of global features to disease findings and hence lacks localization precision. 
%
In this study, we trained GLoRI on top of the frozen foundation model. GLoRI inherently provides interpretable attention maps for each pathology. We visualized the attention maps for a selection of five pathologies of 25 CXRs and aligned these attention maps to an anchor CXR via affine registration\footnote{We apply affine registration using the SimpleElastix library: \url{https://simpleelastix.readthedocs.io}.} to provide a global perspective in Fig. \ref{fig:glori-attns}. The attention maps contain precise localization of abnormalities in CXRs and the global attention maps cover the regions where the pathologies constantly occur. 
To be specific, edema refers to the accumulation of excess fluid and its abnormal regions often diffuse across lungs. This pattern is well captured by our attention maps as we observed in Fig. \ref{fig:glori-attns}d that the critical regions in the individual attention maps scatter over the lungs and the global attention map covers extensive regions of both lungs.
% \py{Point readers to an example in the figure to illustrate how well the attention works.} 
However, these attention maps have the limitations of covering partial regions of the abnormalities. For example, the attention maps for cardiomegaly only cover the heart on the left and right regions of the spine, and some maps for pleural effusion only cover the inferior boundaries of the lung while ignore the remaining abnormal regions. 

% \py{affine or rigid? What tool did you use?}
% limitations
%Overall, we demonstrate that with rich representations learned through large-scale self-supervised pretraining, deep learning models show improved interpretability.

% \begin{itemize}
%     \item significance of interpretable artificial intelligence models, trustworthiness
%     \item CAM, GradCAM, weighted average of feature maps, lack localization precision
%     \item GLoRI with query tokens, can provide attention maps for the attribution of model prediction.
%     \item limitations, attention regions
% \end{itemize}

\subsection{Generalizability of foundation models}
An important characteristic of CheXFound and other foundation models is their generalization capabilities to in-distribution and out-of-distribution downstream tasks. Compared with other encoders, we found that CheXFound achieved better performances on both in-distribution CXR-LT 24 and CheXpert datasets and out-of-distribution Shenzhen, Montgomery, and JSRT datasets. On the opportunistic CXR interpretation tasks on PLCO, CheXFound also achieved consistent and significant increases over comparison methods. CheXFound's generalizability is attributed to the strong representation quality of frozen features learned via pretraining with large-scale, diverse CXRs. We also demonstrated CheXFound's generalization capabilities on infrequent and underrepresented pathologies. CheXFound achieved significant increases over its comparions in classifying low-prevalence pathologies with lower than 1\% occurrence frequencies, demonstrating its superior label efficiency. Although CheXFound with ViT-L achieved robust generalizability, our study did not evaluate the best-performing ViT-giant (ViT-g) architecture in DINOv2, a larger model with 1.1B parameters, which we expect to achieve better generalization performances in CXR interpretation, but it demands more pretraining data and computational resources. Overall, we demonstrated CheXFound's robust generalization capabilities, which we believe can enable diverse downstream adaptations with improved label efficiency.

\subsection{Conclusion}
In summary, this work introduces CheXFound, a vision-centric foundation model pretrained via self-distillation on over one million unique CXRs. For downstream adaptations, we trained a GLoRI module on top of the frozen CheXFound, which combines disease-specific local features and global image features to improve the multilabel classification performance. CheXFound outperformed previous methods for classifying 40 disease findings on CXR-LT 24, demonstrating superior label efficiency on datasets with limited training labels, and strong generalization capabilities for opportunistic CXR interpretation on PLCO.
The disease-specific local features extracted from CheXFound also carry strong interpretability as visuliazed by the associated attention maps.
In our future work, we will continue to explore novel pretraining schemes to further improve the understanding of CXRs by these foundation models.
%(Fig. \ref{fig:glori-attns}). Overall, we believe CheXFound's generalization capabilities will enable diverse downstream adaptation with label efficiency.
% lack evaluation on dense prediction tasks
% Conclusion: CheXFound with a lightweight classifier

% \begin{itemize}
%     \item in-distribution and out-of-distribution evaluation on shenzhen, montgomery, jsrt, and plco, attributed to the strong quality of pretrained CXR representations.
%     \item label efficiency, with better performances on infrequent and underrepresented pathologies compared with other encoders
%     \item limitations: did not implement the best-performing vit-giant architecture
%     \item overall, allow it to adapt to downstream application with considerable label efficiency.
% \end{itemize}
% A conclusion section is not required. Although a conclusion may review the 
% main points of the paper, do not replicate the abstract as the conclusion.
% A conclusion might elaborate on the importance of the work or suggest 
% applications and extensions.

% \begin{itemize}
%     \item Data diversity and data quantity
%     \item DINOv2 limitations, linear probing, and few-show learning
%     \item Importance of masked image modeling
% \end{itemize}


% \appendices

% \section*{Appendix and the Use of Supplemental Files}
% Appendices, if needed, appear before the acknowledgment. If an appendix is not
% critical to the main message of the manuscript and is included only for thoroughness
% or for reader reference, then consider submitting appendices as supplemental materials.
% Supplementary files are available to readers through IEEE \emph{Xplore\textregistered}
% at no additional cost to the authors but they do not appear in print versions.
% Supplementary files must be uploaded in ScholarOne as supporting documents, but for
% accepted papers they should be uploaded as Multimedia documents. Refer readers
% to the supplementary files where appropriate within the manuscript text using footnotes.
% \footnote{Supplementary materials are available in the supporting documents/multimedia tab.
% Further instructions on footnote usage are in the Footnotes section on the next page.}

% \section*{Acknowledgment}
% This work was partially supported by the National Science Foundation (NSF) under the CAREER award OAC 2046708.

\bibliographystyle{IEEEtran}
\bibliography{ref}
\end{document}
