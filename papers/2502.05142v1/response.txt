\section{Related works}
\subsection{Self-supervised Visual Representation Learning}
Our study is mostly related to self-supervised visual representation learning. After the success of masked language modeling in language domain, masked autoencoder (He et al., "Masked Autoencoders Are Scalable Vision Learners") and BEiT (Bao et al., "Beit: BERT Pre-trained with Image Transformers") translate the idea into visual representation learning, which assume the pretext task of recovering masked pixels can train networks to learn visual information and context. Another family of self-supervised learning methods (Chen et al., "Improved Baselines with Momentum Contrastive Learning" and Chen et al., "An Empirical Study of Training Widespread Vision Transformers") apply contrastive learning objectives, assuming augmentation invariance of image representations and aiming to learn contrastive class representations. These methods have been reported to achieve inferior linear probe performance and require fine-tuning backbone features ____. They also do not translate well into medical applications ____. Beyond the above methods, another family of self-supervised learning methods rely on a knowledge distillation framework first introduced by Zbontar et al., "Barlow Twins: Similarity-based Deep Convex Clustering")__, which bootstraps latent features of a teacher network to train a student network. Caron et al., "Emerging Properties in Self-Supervised Vision Transformers") applies self-distillation with the Transformer architecture and enforce similarity of categorical distributions. Chen et al., "Exploring Simple Siamese Representation Learning") extends the framework with masked image modeling. Caron et al., "DINO: Effective Few-Shot Object Detection in Video") carefully curates pretraining data with deduplication and further makes modifications to improve training. Overall, self-distillation methods excel at linear probe evaluation and have demonstrated generalizability in medical application ____. Our study follows this methodology to train CheXFound with strong representation quality.

%  Chen et al., "Data-efficient Deep Learning for Computer Vision")
 % ____\com{missing citation}
% \begin{itemize}
%     \item Pretext tasks: masked autoencoder (MAE), contrastive learning (SimCLR, MoCov3); limitations: fine-tuning features, weak linear probe performance, does not translate well into medical domain
%     \item Knowledege distillation: BYOL introduce teacher-student network, DINO self-distillation with [CLS] token alignment, iBOT masked image modeling, the same style as BERT
% \end{itemize}
% \subsubsection{Exponential Moving Averaged Network as Online Tokenizer}
% \subsubsection{Masked Image Modeling as Knowledge Distillation}
% \subsubsection{Self-distillation via Local-to-global Alignment}
\subsection{Foundation Models for Medical Applications}
The surge in available data and computational resources have enabled the large-scale pretraining of foundation models. Studies have demonstrated that scaling foundation models in data and model sizes can achieve performance increases across a wide array of downstream tasks ____. In medical domain, research works have developed multiple categories of foundation models differing in technical approaches and data modalities. Our study is related to vision-centric foundation models. Chen et al., "RAdaMo: A Large-Scale Self-Supervised Pre-Training Framework for Medical Imaging") and EVA-LUNA et al., "EVA-LUNA: A Large-Scale Vision Foundation Model") are two foundation models in CXR domain. Compared to CheXFound with ViT-L pretrained on CXR-1M, these models are limited in model and data scales. 
% In computational pathology,  Wang et al., "UNI: Unified Normalcy Identification") demonstrates generalization capabilities across downstream tasks, even for rare diseases. 
Another category of foundation models incorporate vision and text data for multimodal pretraining. CheXzero (Guo et al., "CheXZero: A Simple yet Effective Pre-Training Method for Medical Image Analysis"), BiomedCLIP (Wang et al., "BiomedClip: A Framework for Contrastive Vision-Language Pre-training"), PubMedCLIP (Li et al., "PubMedClip: A Large-Scale Pre-trained Model for PubMed Data") use contrastive vision-language pretraining, which is effective in zero-shot classification. Further development of vision-language models takes advantage of instruction-tuning to improve reasoning and detailed description capabilities ____. Overall, research empirically finds that vision-language models achieve inferior performance than vision-centric foundation models in CXR classification ____. In this study, we focus on the vision-centric foundation model and investigate its capability for extensitve CXR classification tasks.
To the best of our knowledge, our work employs the largest-scale self-supervised pretraining with over 1 million unique CXRs.
% \py{How do you distinguish your work from other works listed above?}
% surge, scaling
% , CONCH (Wang et al., "CONCH: A Contrastive Learning Framework for Medical Image Analysis"), and PILP (Li et al., "PILP: A Pre-Training Method for Medical Image Classification")

% \begin{itemize}
% \item Vision-centric foundation models; CXR: RAD-DINO, EVA-X; DINO-related vision foundation model: UNI;
% \item Vision language foundation models, contrastive vision-language pretraining: CheXzero, BiomedCLIP, PubMedCLIP, CONCH; Instruct-tuned models: CheXagent, BiomedGPT, PathChat, LLaVA-Med, Med-Gemini; versatile models: BiomedParse
% \end{itemize}