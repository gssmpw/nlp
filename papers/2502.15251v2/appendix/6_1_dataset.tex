\subsection{Construction of large-scale in-the-wild hand database}\label{sec:appendix_database}

This section presents our method for constructing a large-scale hand image dataset by extracting and processing hand images from various video datasets. We outline key preprocessing steps, including \textit{1) preprocessing}, \textit{2) hand region detection}, and \textit{3) similarity calculation \& ranking}.

\textbf{Preprocessing:} We prepare two large-scale video datasets: Ego4D, containing 8k frames, and 100DOH, with 23k frames, both sampled at 1 \textit{fps}. As shown in Fig. ~\ref{fig:database}, first-person and third-person hand images exhibit significant differences.

\textbf{Hand region detection:} After extracting frames from Ego4D and 100DOH, we use a lightweight, fixed-weight network to detect hand regions via bounding boxes. Specifically, we adopt the method from ~\citep{shan:cvpr20} and store all detected bounding boxes in sequence. This step constructs a large-scale hand image dataset as~\cite{tango:eccvw22}.

\textbf{Similarity calculation \& ranking:} Once the hand image dataset is built, we use a lightweight, fixed-weight network to extract raw keypoints for each sample via MediaPipe ~\citep{lugaresi:arxiv19}. To reduce noise, we apply PCA as described in Sec. ~\ref{sec:method_preproc}. We then compute similarity scores for a given query image \( I \) using Eq. ~\ref{eq:mining} and rank the remaining samples accordingly. This process yields a large-scale set of in-the-wild hand images with similar characteristics. For instance, in Ego4D, given a query sample \( I \), we retrieve all similar hand images and construct a ranked sequence, referred to as "Top-K". The Top-1 image in this sequence serves as the positive sample \( I^{+} \) for contrastive learning, enhancing the effectiveness of \Ours pre-training. As shown in Tab.~\ref{tab:exp_Top-K}, our experiments validate that selecting Top-1 as the positive sample \( I^{+} \) is the optimal strategy.  

\begin{figure}
    \begin{center}
    \includegraphics[width=1.0\textwidth]{appendix/figures/figure6_database.pdf}
    \end{center}
    \vspace{-3mm}
    \caption{
    \textbf{Overview of data preprocessing and similar hands mining.} This image illustrates a three-step process for \Ours pre-training using datasets from Ego4D and 100DOH. \textbf{Step 1} involves preprocessing the datasets to extract relevant frames. \textbf{Step 2} employs a hand detector to crop hand regions from these frames, creating a diverse pool of hand images in the wild. \textbf{Step 3} calculates similarity and ranks the images using a pose estimator and PCA, producing a sorted list of hand poses, from the most similar to the least similar to a given anchor pose.
   }
    \label{fig:database}
\end{figure}