\subsection{Comparison with Weakly-supervised Learning Setting}

We compare our method with a weakly-supervised learning setting that uses 2D noisy keypoints assigned on in-the-wild images.
In a weakly-supervised learning setting, noisy 2D keypoints are directly used as supervision signals during network training. The model treats these 2D keypoints as targets, computing the loss between the predicted keypoints and the provided 2D keypoints (\eg, heatmap-based loss).
However, our experiments reveal that directly conducting joint training on labeled and unlabeled data results in degraded performance due to the noise and unreliability of the 2D keypoints. As shown in Tab.~\ref{tab:exp_wsl}, without any keypoint filtering or correction, the weakly-supervised method performs significantly worse than our pre-training setting.

\input{appendix/tables/exp_wsl}

These findings demonstrate that directly incorporating noisy 2D annotations during weakly-supervised training negatively impacts model performance, particularly when the labels contain high levels of noise.

Before designing our pre-training approach, we identified several limitations of the weakly-supervised setting for large-scale, in-the-wild hand data based on prior experience: (1) \textit{Data scale constraints:} When the amount of noisy hand data is significantly smaller than the noise-free hand training dataset, it may provide some improvement but it is hard to guarantee that such noisy labels are less in larger datasets (e.g., the two million in-the-wild hand images in this study) and (2) \textit{Training efficiency issues}: Introducing large-scale noisy data significantly prolongs training time and slows convergence. In contrast, our pre-training method benefits from such large unlabeled hand images with certain noisiness. This highlights our superiority in exploiting pre-training over the weakly-supervised setting.