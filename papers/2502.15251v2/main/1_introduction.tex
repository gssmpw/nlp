\section{Introduction}
Hands serve as a trigger for us to interact with the world, as seen in various human-centric videos. 
The precise tracking of hand states, such as 3D keypoints, is crucial for video understanding~\citep{sener:cvpr22,wen:arxiv23}, AR/VR interfaces~\citep{han:tog22,wu:vcir20}, and robot learning~\citep{chao:cvpr21,qin:eccv22}. To this end, 3D hand pose estimation has been studied through constructing labeled datasets~\citep{ohkawa:ijcv23,zimmermann:iccv19,chao:cvpr21,ohkawa:cvpr23} and advancing supervised pose estimators~\citep{cai:eccv18,ge:cvpr19,park:cvpr22,liu:cvpr24,fan:eccv24}. 
However, utilizing large-scale, unannotated hand videos for pre-training remains underexplored, while collections of human-centric videos, like 3,670 hours of videos from Ego4D~\citep{grauman:cvpr22} and 131-day videos from 100DOH~\citep{shan:cvpr20}, are readily available.

In pre-training, contrastive learning has been utilized to learn from unlabeled images like SimCLR~\citep{chen:icml20}, which maximizes agreement between positive pairs while repelling negatives. 
Spurr~\etal~\citep{spurr:iccv21} introduce pose equivariant contrastive learning (PeCLR) for 3D hand pose estimation, which aligns the geometry of features encoded from augmented images with affine transformations.
However, both SimCLR and PeCLR create positive pairs from a single sample by applying data augmentation, limiting the gains from positive pairs as their hand appearance and backgrounds are identical. Ziani~\etal~\citep{ziani:3dv22} extend the contrastive learning framework to video sequences by treating temporally adjacent hand crops as positive pairs. 
However, in-the-wild videos can challenge tracking hands across frames, especially in egocentric views where hands are often unobservable due to camera motion. 
Meanwhile, this temporal positive sample mining remains the limited appearance variation of hands and backgrounds.

\begin{figure}[t!]
\vspace{-2mm}
    \begin{center}
    \includegraphics[width=1.00\textwidth]{figures/figure1_pipline.pdf}
    \end{center}
    \vspace{-3mm}
    \caption{
    \textbf{The pipeline of pre-training and fine-tuning.} \textbf{(Left)} Previous pre-training methods (\eg, PeCLR~\citep{spurr:iccv21}) learn from positive pairs originating from the different augmentations and fine-tune the network on a dataset.
    \textbf{(Right)} Our method is designed to learn from positive pairs with similar foreground hands, sampled from a pool of hand images in the wild. 
   }
    \label{fig:pipline}
    \vspace{-6mm}
\end{figure}

In this work, we introduce \Ours, a novel contrastive learning framework for 3D hand pose pre-training, which leverages diverse hand images in the wild, with the largest 3D hand pose pre-training set to date. 
We specifically collect 2.0M hand images from human-centric videos, from Ego4D~\citep{grauman:cvpr22} and 100DOH~\citep{shan:cvpr20}, using an off-the-shelf hand detector~\citep{shan:cvpr20}. 
Our pre-training set significantly exceeds the scale of prior works by two orders of magnitude, such as over 32-47K images in~\citep{spurr:iccv21} and 86K images from 100DOH in~\citep{ziani:3dv22}.

Our method focuses on learning discriminative information by 
mining hands with similar characteristics from various video domains.
Based on our observations, contrastive learning can further benefit from discriminating the foreground of hands in varying backgrounds.
As shown in Fig.~\ref{fig:pipline}, our positive pairs are sourced from different images, offering additional information gains from different types of object interactions, backgrounds, and hand appearances. Specifically, we use an off-the-shelf 2D hand pose estimator~\citep{lugaresi:arxiv19} to identify similar hands from the pre-training set.

Using the identified similar hands as positive pairs, we further propose adaptive weighting, to dynamically find informative pairs during training.
A naive adaptation of the similar hands is to replace the original positive pairs in contrastive learning, but this scheme struggles to exploit \textit{how similar the paired hands are}.
To tackle this, we assign weights based on the similarity scores within the mini-batch in the contrastive learning loss.
The weights are designed to have higher values as the similarity of the pairs increases.
This allows the optimization of contrastive learning to explicitly consider the proximity of samples, beyond binary discrimination between positives and negatives.

We validate the effectiveness of the pre-trained networks by fine-tuning on several datasets for 3D hand pose estimation, namely FreiHand~\citep{zimmermann:iccv19}, DexYCB~\citep{chao:cvpr21}, and AssemblyHands~\citep{ohkawa:cvpr23}.
Our proposed method consistently outperforms conventional contrastive learning methods, SimCLR and PeCLR.
Additionally, we conduct extensive ablation experiments to analyze: 1) performance with varying pre-training and fine-tuning data sizes, 2) the effect of adaptive weighting, and 3) the improvement with different levels of similarity.

In summary, the main contribution of this paper is threefold:
\vspace{-1mm}
\begin{itemize}[]
    \vspace{-1mm}
    \item We propose \Ours, a contrastive learning method for 3D hand pose pre-training, leveraging positive samples with similar hands mined from 2.0M in-the-wild hand images.
    \item We introduce a parameter-free adaptive weighting mechanism in the contrastive learning loss, enabling 
    optimization guidance according to the calculated similarity.
    \item Our experiments demonstrate that our approach surpasses prior pre-training methods and achieves robust performances across different hand pose datasets.
\end{itemize}