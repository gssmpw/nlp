\section{Experiments}
In this section, we compare our method with existing baselines for pre-training of the 3D hand pose estimation and conduct ablation experiments to support the validity of our approach. We begin by providing a detailed explanation of the dataset and experimental setup (Sec.~\ref{sec:exp_setup}). Next, we demonstrate that our model achieves competitive performance compared with existing methods (Sec.~\ref{sec:exp_main}). Following this, we present the results of ablation studies on weighting design in the pre-training phase (Sec.~\ref{sec:exp_abl}). Finally, visualizations are used to illustrate the superiority and efficiency of our approach (Sec.~\ref{sec:exp_vis}).

\subsection{Experimental setup}\label{sec:exp_setup}
\textbf{Pre-training datasets:}
We curate a large collection of hand images from two major video datasets, Ego4D~\citep{grauman:cvpr22} and 100DOH~\citep{shan:cvpr20}, featuring egocentric and exocentric views respectively. From Ego4D, a vast egocentric video dataset with 3,670 hours of footage, we extracted 1.0M hand images from 8K videos. Similarly, from the exocentric dataset 100DOH, which includes 131 days of YouTube footage, we extract 1.0M hand images from 20K videos. These extensive datasets provide diverse hand-object interactions across different views. We also prepare pre-training data with varying amount. ``Exo-X'' and ``Ego-X'' denote 100DOH and Ego4D datasets with X~images selected randomly (\eg, X = 50K, 100K, ..., 1M, 2M). ``Ego\&Exo-2M'' shows our final set combining both datasets with full images (\ie, 1.0M for each).

\textbf{Fine-tuning datasets:}
We conduct fine-tuning experiments on three datasets with 3D hand pose ground truth in various data size and viewpoints: exocentric datasets from FreiHand~\citep{zimmermann:iccv19} and DexYCB~\citep{chao:cvpr21}, and an egocentric dataset AssemblyHands~\citep{ohkawa:cvpr23}. FreiHand consists of 130.2K training frames and 3.9K test frames, with both green screen and real-world backgrounds. DexYCB contains 325.3K training images and 98.2K test images, focusing on natural hand-object interactions. AssemblyHands, the largest of the three, includes 704.0K training samples and 109.8K test samples, collected in object assembly scenarios. Following~\citep{spurr:iccv21}, we prepare 10\% of the labeled FreiHand dataset, which is denoted as ``FreiHand*'', especially used for ablation studies.
This allow us to assess the performance in a limited supervision setting.

\textbf{Implementation details:}
For similar hands mining, we choose the PCA embedding size as $D=14$. For the pre-training framework, we use ResNet-50~\citep{he:cvpr16} as the encoder. Throughout the pre-training phase, all models are trained using LARS~\citep{you:arxiv17} with ADAM~\citep{kingma:arxiv14} optimizer, with the learning rate of 3.2e-3. Following~\citep{spurr:iccv21}, SimCLR employs scale and color jitter as image augmentation, while PeCLR and \Ours utilize scale, rotation, translation, and color jitter. We use resized images with \(128 \times 128\) as the input. We set the temperature parameter $\tau$ of contrastive learning as 0.5. We use 8 NVIDIA V100 GPUs with a batch size of 8192 for pre-training.

For fine-tuning, we initialize our model with the pre-trained encoder $E(\cdot)$ and then fine-tune with a 3D pose regressor on the labeled datasets. The 3D regressor involves 2D heatmap regression and 3D localization heads, similar to DetNet~\citep{zhou:cvpr20}. 
We use a single NVIDIA V100 GPU with a batch size of 128. We provide more additional details in supplement.

\textbf{Evaluation:}
We use the following evaluation metrics: the mean per joint position error (MPJPE) in millimeters, which compares model predictions against ground-truth data, and the percentage of correct keypoints based on the area under the curve (PCK-AUC), which measures the proportion of predicted keypoints that fall within a specified distance (20mm to 50mm) from the ground truth with varying thresholds.

\input{tables/exp_main}
\input{tables/exp_scale}

\subsection{Main results}\label{sec:exp_main}

We compare our method with previous works for 3D hand pose estimation (Tab.~\ref{tab:exp_main}). To make a fair comparison, we evaluate all pre-training datasets of the same size against previous methods.

\textbf{Comparison to contrastive learning methods:}
We compare our pre-training method with previous methods~\citep{chen:icml20, spurr:iccv21} in 3D hand pose estimation (Tab.~\ref{tab:exp_main}). We observe that our method significantly outperforms SimCLR and PeCLR across various datasets under the equal pre-training data setups. When we compare our method against a randomly initialized model (w/o pre-training), \Ours improves performance by 17.7\% over the scratch baseline.

In more details, our approach achieves a 15.31\% improvement over previous methods PeCLR with Ego-1M pre-training on the FreiHand. We observe that SimCLR shows limited performance compared to the random initialization.
This suggests pre-training without geometric prior (\ie, without geometric augmentation) does not always help hand pose estimation, requiring spatial keypoint regression. In contrast, our method demonstrates significant performance gain on larger datasets, with a 10.53\% gain on DexYCB and a 4.90\% improvement on AssemblyHands compared to PeCLR. These results confirm that our model consistently achieves superior performance across various fine-tuning datasets.

Furthermore, we pre-train all methods on the joint pre-training datasets (Ego\&Exo-2M). Our approach further improves over the state-of-the-art method (PeCLR), achieving improvements of 13.19\%, 7.4\%, and 3.4\% on the FreiHand, DexYCB, and AssemblyHands, respectively. Compared to the pre-training with 1M samples (Ego-1M), doubling the pretraining data with Ego\&Exo-2M results in a 2.28\% improvement on the FreiHand dataset. Notably, our method shows particular strength in effectively handling larger, more varied datasets. This robust performance demonstrates that our approach is highly effective and reliable for hand pose pre-training.

\input{tables/exp_weight}

\textbf{Ego \& Exo view analysis:}
We evaluate how pre-training with egocentric views (Ego4D) and exocentric views (100DOH) affects the performance in datasets with their corresponding views, namely AssemblyHands for egocentric and FreiHand and DexYCB for exocentric views. Interestingly, matching pre-training viewpoints does not consistently enhance performance, indicating that the view gaps have limited effects. Instead, factors like dataset diversity and the characteristics of pre-training methods are more crucial in boosting performance. Combining the two datasets (the last row of Tab.~\ref{tab:exp_main}) leads to the best performance in all three datasets, underscoring the potential of enriching data diversity with various camera views.

\subsection{Ablation experiments}\label{sec:exp_abl}

This section presents ablation studies on \Ours, focusing on four aspects: 1) pre-training dataset size, 2) fine-tuning dataset size, 3) adaptive weighting, and 4) Top-K similar hands. First, we examine the size of the pre-training dataset using various methods, showing that our approach maintains superior performance across different sizes (Tab.~\ref{tab:exp_PT_scale}). Second, inspired by \citep{zimmermann:iccv19}, we explore fine-tuning dataset size, demonstrating significant gains even with limited data (Fig.~\ref{fig:fig_FT_scale}). Furthermore, we also highlight the adaptive weighting design, which consistently outperforms comparison methods (Tab.~\ref{tab:exp_weighting}). Finally, we conduct ablation analysis according to different levels of similarity in the assigned positive hand pairs. (Tab.~\ref{tab:exp_Top-K}).

\textbf{Effect of pre-training data size:}
We study results with different sizes of pre-training data, namely 50K, 100K, 500K, and 1M in Tab.~\ref{tab:exp_PT_scale}. The results demonstrate that \Ours reliably outperforms the other methods across all settings, with improvement as the pre-training data size increases. With changes in the size of the pre-training data from 50K to 1M, \Ours achieves a reduction in MPJPE from 35.32 to 23.68. The useful insights we can gather from this table include: 1) The \Ours method holds a leading advantage across various pre-training size. 2) As the size of the pre-training dataset increases, the improvement for fine-tuning with limited labels is substantial.

\textbf{Effect of fine-tuning data size:}
Fig.~\ref{fig:fig_FT_scale} illustrates the experiment under different proportions of labeled fine-tuning data, namely 10\%, 20\%, 40\%, and 80\% in FreiHand. Note that we denote methods with ``-1M/2M'' as those pre-trained on the Ego-1M and the Ego\&Exo-2M sets, respectively. The results show that \Ours-1M brings error reduction, achieving remarkably lower MPJPE scores with merely 10\% of labeled data. \Ours-1M delivers the best performance over different size of fine-tuning data, compared to SimCLR-1M and PeCLR-1M. \Ours-2M further shows improvement over \Ours-1M, while the gains become marginal as labeled data increase. From this analysis, we can draw two key conclusions: 1) The improvement resulting from an increase of pre-training data becomes less significant as the amount of fine-tuning data increases; 2) \Ours maintains a strong advantage in scenarios with limited labeled data, particularly when larger pre-training data are used.

\textbf{Effect of adaptive weighting:}
We validate the proposed adaptive weighting and its generality when applied to the other methods in Tab.~\ref{tab:exp_weighting}.
On the Ego-100K pre-training set, the MPJPE scores after adaptive weighting decrease by 1.8\% and 3.0\% for SimCLR and PeCLR, respectively, while PCK-AUC increases by 1.58\% and 1.87\%.
This indicates that the proposed weighting excels in its applicability to various pre-training methods. In our \Ours method, applying adaptive weighting reduces MPJPE from 31.06 to 28.84, a 7.18\% decrease, while PCK-AUC improves from 68.66 to 71.07, a 2.41\% increase. We find the effectiveness of the proposed weighting when combined with the mined similar hands.

\input{tables/exp_Top_K}

\textbf{Learning from Top-K similar hands:}
We test pre-training with different similarity levels of positive samples in Tab.~\ref{tab:exp_Top-K}. As illustrated in Fig.~\ref{fig:Top-K}, we can sample similar pairs according to the distance ranking (\eg, K = 1, 2, ..., 5000), where Top-1 is used to produce our final results. The performance trend is initially subtle and somewhat fluctuating (Top-1$\sim$100) but becomes increasingly pronounced after Top-100. This indicates that as the similarity between positive samples increases, the global trend decreases accordingly. Notably, using Top-5000 similar hand samples as positive samples decreases the MPJPE by 13.78\% compared to Top-1. This study provides two insights: 1) Similar samples with subtle noisiness (\eg, 1$\sim$100) exhibit minimal variation in performance, indicating that slight differences in similarity within this range do not significantly impact the pre-training outcome. This suggests that the model is robust to minor variations when the positive samples are highly similar. 2) The results support the validity of using Top-1 positive samples to produce final results, as they consistently exhibit the best performance. This highlights the importance of selecting the most similar samples in contrastive learning.

\subsection{Visualization}\label{sec:exp_vis}
In this section, we compare the fine-tuning results of various pre-training methods through detailed visualizations on different datasets (Fig.~\ref{fig:visualization}). The pre-training model is trained on the Ego\&Exo-2M dataset and fine-tuned on the FreiHands~\citep{zimmermann:iccv19} and DexYCB~\citep{chao:cvpr21} datasets, respectively.
We provide additional visualization in the supplementary material.

From the left four columns of Fig.~\ref{fig:visualization}, the visualization results show that \Ours performs better in pose estimation, with results closer to the ground truth, compared to the other methods in FreiHands~\citep{zimmermann:iccv19} dataset. In particular, \Ours outperforms the other methods in challenging environments, such as those with varying lighting conditions, by better capturing hand poses. These visual outputs highlight its robustness across various scenarios, solidifying its potential for real-world applications.

As shown in the right four columns of Fig.~\ref{fig:visualization}, we highlight the occluded regions in the original images of DexYCB~\citep{chao:cvpr21} dataset using red circles. The results show that \Ours is more effective in tackling occlusion problems. Our pre-training method effectively addresses partially occluded images by utilizing similar, though not identical, hand images, where the occluded parts in the query image may be visible in the corresponding similar hand image, and vice versa.

\begin{figure}[t!]
    \begin{center}
    \includegraphics[width=1.0\textwidth]{figures/figure5_visualization.pdf}
    \end{center}
    \vspace{-3mm}
    \caption{
    \textbf{Visualization of FreiHand~\citep{zimmermann:iccv19} and DexYCB~\citep{chao:cvpr21}.} 
    The first four columns on the left display the results for FreiHand, while the last four columns on the right show the results for DexYCB (GT: Ground Truth; PT: Pre-training). It can be observed that \Ours pre-training method achieves better results.
    }
    \label{fig:visualization}
\end{figure}