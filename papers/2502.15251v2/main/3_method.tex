\section{Method}
Our approach \Ours aims to pre-train an encoder for 3D hand pose estimation with large-scale human-centric videos in the wild. We first construct a pre-training set from egocentric and exocentric hand videos (Sec.~\ref{sec:method_preproc}). Then, we find similar hand images to define positive pairs across videos (Sec.~\ref{sec:method_simhand}). Finally, we incorporate these positive pairs into a contrastive learning framework and employ adaptive weights to improve the effectiveness in pre-training (Sec.~\ref{sec:method_contras}).

\subsection{Data preprocessing}\label{sec:method_preproc}
Our preprocessing involves creating a set of valid hand images for pre-training, which is sampled from a set of $N$ videos: $\{ v_1, v_2, \dots, v_N\}$. We use an off-the-shelf hand detector~\citep{shan:cvpr20} to select valid frames with visible hands. Given a video frame $I_{\textrm{full}}\in v_i$, the model detects the existence of the hand and its bounding box, creating hand crops enclosing either hand identity (right/left) from $I_{\textrm{full}}$. To avoid bias related to hand identity, we balance the number of right and left hand crops equally and then convert all crops to right-hand images. Then, we create a set of frames for each video $v_i$ as \( \mathcal{F}_i = \{ I_{i,1}, I_{i,2}, \dots, I_{i,T_{i}} \} \), where \( I_{i,j} \in \mathbb{R}^{H \times W \times 3} \) represents the processed crop with height \( H \) and width \( W \), and $T_i$ is the total number of crops in $v_i$. The height \( H \) and width \( W \) are defined post-resize to give the uniform image size. Using this frame set $\mathcal{F}_i$, the video dataset can be re-represented as $\mathcal{V} = \{ \mathcal{F}_1, \mathcal{F}_2, \dots, \mathcal{F}_N\}$. Specifically, we processed two datasets, Ego4D~\citep{grauman:cvpr22} and 100DOH~\citep{shan:cvpr20}, to collect 1.0M images from 8K and 21K videos, respectively. More details about our preprocessing can be found in the supplement.

\subsection{Mining similar hands}\label{sec:method_simhand}
To incorporate diverse samples in contrastive learning, we design positive pairs from non-identical images with similar foreground hands. Here we construct a mining algorithm to find similar hands from $\mathcal{V}$ by focusing on pose similarity between hand images. We first extract 2D keypoints from $I$, embed in the feature space, and search a positive sample.

\begin{figure}[t!]
\vspace{-2mm}
    \begin{center}
    \includegraphics[width=0.80\textwidth]{figures/figure2_Top-K.pdf}
    \end{center}
    \vspace{-3mm}
    \caption{
    \textbf{Visualization of similar hand samples in Top-K.}
    Given the query image ($I$), the mined similar samples are shown (``Top-1'' corresponds to $I^+$ in Sec.~\ref{sec:method_simhand}).
   }
    \label{fig:Top-K}
    \vspace{-3mm}
\end{figure}

\textbf{Pose embedding:}
We adopt estimated 2D keypoints (for 21 joints) to find similar hands.
We use an off-the-shelf 2D hand pose estimator \(\phi\)~\citep{lugaresi:arxiv19}, but the outputs are prone to be noisy in testing in the wild.
To make it more robust, we obtain a $D$-dimensional embedding of 2D hand keypoints, $\mathbf{p} \in \mathbb{R}^{D}$, for each image $I$.
This serves to reduce the noise effect while preserving the semantics of hands.
We use a concatenated \(42\)-dimensional vector as the output of \(\phi\) for later use. 
Particularly, we apply PCA-based dimension reduction, which projects the keypoints vector into a lower-dimensional space of size \(D\). 
Given the PCA projection matrix \(M \in \mathbb{R}^{42 \times D}\), the pose embedding \(\mathbf{p}\) is calculated as \(\mathbf{p} = M^{T} \mathbf{\phi}(I)\). 

\textbf{Mining:}
This step is designed to identify a positive sample $I^+ \in \mathbb{R}^{H \times W \times 3}$ paired with a query image $I$. We denote the similarity mining logic as $I^+ = \mathrm{SiM} (I)$. As shown in Fig.~\ref{fig:Top-K}, using the closest (neighbor) sample in the PCA space encounters a trivial solution $I,I^+ \in v_i$, where both images originate from the same video $v_i$. Similarly to~\citep{ziani:3dv22}, the supervision by neighbor samples of the same video has less diversity in backgrounds, hand appearances, and object interactions. Thus we are motivated to find similar hands derived from different videos. Specifically, we search the minimum distance within the set of all frames except for $v_i$, written as $\mathcal{F}^{c}_{i} = \bigcup_{\substack{k \neq i}} \mathcal{F}_{k}$. Given an query $I_{i,j}$, which represents the $j$-th image of the $i$-th video, the function $\mathrm{SiM}(\cdot)$ is formulated as

\begin{equation}
\label{eq:mining} 
    \mathrm{SiM} (I_{i,j}) = {\arg \min} _{x \in \mathcal{F}^{c}_{i}} D(M^{T} \phi(x), M^{T} \phi(I_{i,j})),
\end{equation}
where $D(\cdot,\cdot)$ is the Euclidean distance metric. 

As a proof of concept, we illustrate examples after our mining $\mathrm{SiM}(\cdot)$ in Fig.~\ref{fig:Top-K}. We denote ``Top-1'' (most similar) as our assigned positive sample $I^+$ to the query image $I$. As references, the rest of the figures (``Top-K'') represent the $K$-th similar samples. Our sampling highlights the diversity in captured environments and interactions, while it also suggests that as the rank (distance) increases, the sampled images become dissimilar. Additional visualization results of similar hands can be found in supplement.


\subsection{
{Contrastive learning from similar hands with adaptive weighting}}\label{sec:method_contras}
{
We detail our contrastive learning approach (see Fig.~\ref{fig:method}), learning from mined similar hands with adaptive weighting.}

\begin{figure}[t!]
\vspace{-2mm}
    \begin{center}
    \includegraphics[width=0.90\textwidth]{figures/figure3_method.pdf}
    \end{center}
    \vspace{-3mm}
    \caption{
    \textbf{Overview of our \Ours.} Starting from the left, hand images ($I$, $I^{+}$, $I^{-}$) and their corresponding 2D keypoints are input to the model. After applying random augmentations through transformation \( \mathbf {T} \), both the images and 2D keypoints are spatially transformed. The altered 2D keypoints are then used to compute adaptive weights  \( w_{\text{pos}} \) and  \( w_{\text{neg}} \), which guide contrastive learning by strengthening or weakening the alignment between positive and negative samples.
   }
    \label{fig:method}
    \vspace{-5mm}
\end{figure}

\textbf{Overview:}
The contrastive learning is designed to align positive samples $(I,I^+)$ in the feature space, constructed in Sec.~\ref{sec:method_simhand}, and the rest of negative samples are pushed apart.
Following~\citep{chen:icml20, spurr:iccv21}, we treat all mini-batch samples other than the corresponding positive samples as negative samples $I^-$.
Feature extraction is performed by two learnable components: an encoder \( E(\cdot) \) and a projection head \( g(\cdot) \), which indicates the entire model as \( f = g \circ E \).
The extraction is combined with image augmentation \( \mathbf {T} \), which formulated as \( \mathbf{z} = f(\mathbf{T}(I)) \) and  \( \mathbf{z}^+ = f(\mathbf{T}(I^+)) \).
Applying geometric transformations (\eg, rotation) in \( \mathbf {T} \) can cause misalignment between the image and feature spaces; we correct such an error with the inverse transformation \( \mathbf {T}^{-1} \) as~\citep{spurr:iccv21}.
After applying the inverse transformation to the feature \( \mathbf{z} \), we obtain a feature \( \tilde{\mathbf{z}} = \mathbf{T}^{-1}(\mathbf{z}) \), where geometry is aligned to the original images.
As such, all anchor, positive, and negative samples are encoded as $\tilde{\mathbf{z}}$, $\tilde{\mathbf{z}}^+$, and $\tilde{\mathbf{z}}^-$, respectively.

\textbf{Adaptive weighting:} During learning from our similar hands, we propose an adaptive weighting per pair to focus more on informative samples that provide greater discriminative information. 
The assigned weights are computed by the predefined similarity metric in Sec.~\ref{sec:method_simhand}. Given pre-processed keypoints for two samples within the mini-batch, $\mathbf{k}_1$, $\mathbf{k}_2$, the weight $w$ is computed by linear scaling with the Euclidean metric $D(\cdot,\cdot)$ as

{
\begin{equation}
w= \frac{d_{\text{max}} - D(\mathbf{k}_1, \mathbf{k}_2)}{d_{\text{max}} - d_{\text{min}}},
\end{equation}
}

where $d_{\text{min}}$, $d_{\text{max}}$ are the minimum and maximum distances within the mini-batch.
This assigned weight $w$ dynamically changes according to the sample statistics in the mini-batch, enabling adaptive attention per iteration.

To address the distinction between positive and negative sample weighting, we introduce separate weighting terms for positive and negative pairs. Specifically, \( w_{\text{pos}} \) corresponds to the weight assigned to positive pairs, while \( w_{\text{neg}} \) is used for positive-negative pairs.

\textbf{Contrastive loss with weighting:} We finally formulate contrastive learning with the proposed weighting scheme. We assume that a mini-batch contains 2N samples in total, including N query samples and their corresponding N positive samples. We introduce separate weighting terms for positives $(I, I^+)$ and negatives $(I, I^-)$ as \( w_{\text{pos}} \) and \( w_{\text{neg}} \), respectively. With these weights, our constrastive learning loss based on the NT-Xent loss~\citep{chen:icml20} is formulated as:

\begin{equation} 
\label{eq:cl_loss_w_weighting} 
    \mathcal{L}_{i} = -\log \frac{\exp \left( w_{pos} \cdot \text{sim}(\tilde{\mathbf{z}}_{i}, \tilde{\mathbf{z}}^{+}_{i}) / \tau \right)} {\sum_{k=1}^{2N} \mathbb{1}_{[k\neq i]} \exp \left( w_{neg} \cdot \text{sim}(\tilde{\mathbf{z}}_{i}, \bar{\mathbf{z}}^{-}_{k}) / \tau \right)} 
\end{equation}

Here \( \tau \) is a temperature parameter, \( \text{sim}(\mathbf{z}, \bar{\mathbf{z}}) = \frac{\mathbf{z}^T \bar{\mathbf{z}}}{\|\mathbf{z}\|\|\bar{\mathbf{z}}\|} \) is the cosine similarity function. Overall, our adaptive weighting enables considering the importance separately for positive and negative samples, while closer samples are assigned with higher weights and more distant ones receive lower weights.