
\section{Experiments}
\label{sec:experimental_setting}

We conduct experiments on \ourenv{} and a second environment with multiple LLMs. This section describes our experimental setting in detail.


\subsection{Environments}

\paragraph{\ourenv{}}
We wish to have a challenging yet feasible environment. To this end, we set the number of suspects, turn limit, and number of attributes in the asymmetric variant to 10, 31, and 11, respectively. For the symmetric variant we set them all to 20.
Additionally, to analyze the performance on different task difficulty levels, we vary the number of suspects over 6, 10, and 14 in \ourenvasym{}.
Attributes include clothing and personality-related features, while each attribute has 2-3 possible values to allow some similarity between the suspects (see the full list of attributes and values at \S\ref{app:attributes}).

In our experiments, we observed that agents often struggle to perform the task when it is described with the words ``accuse'', ``suspect'' and ``culprit'', potentially due to alignment procedures. Therefore, we rephrased the task with the words ``award'', ``character'' and ``winner'', instead. This does not affect the task itself but only how it is presented to the agents.
We prompt agents to generate thoughts before predicting an action with ReAct \cite{yao2023react} (see \S\ref{app:prompts} for the exact prompts).


\paragraph{\govsim{}} \cite{piatti2024cooperate} is a collaborative environment for resource sustainability, where agents share a renewable starting resource $R_0$ that they harvest to maximize their gains. At every round, agents harvest the shared resource and then discuss and decide their plans for future rounds. At the beginning of each round $j$, the remaining resources $R^*_j$ are doubled up to the original maximum: $R_{j+1} = \min(2R^*_j, R_0)$, encouraging agents to be efficient while ensuring sustainability. The discussion stage allows free communication between agents, thus providing an interesting addition to \ourenv{}. 
In our experiments, we focus on the fishing task. The set of actions at turn $j$ is defined as the possible amounts of resources to harvest, that is $a\in[0, R_j]$.
Since actions consume the shared resource, they are \emph{irreversible}.

\paragraph{Evaluation}
The main metric of \ourenv{} measures the percentage of games that end in identifying the correct culprit, termed \success{}. Additionally, we measure \precision{}, i.e., \success{} when a character was accused, and \gamelength{} for the average number of turns, including all interventions. 
We report the average and standard error over four runs with \textsc{Llama} and \textsc{Qwen} and three runs with \gpt{}.

For \govsim{}, we follow \citet{piatti2024cooperate} and report \survivaltime{}, \survivalrate{} and \efficiency{}.
\survivaltime{} measures the number of rounds in which the shared resource remains above a minimal threshold $\gamma$.
\survivalrate{} is a binary metric indicating whether the \survivaltime{} is above a maximal round threshold $m$. \efficiency{} measures how well agents consume the shared resource, i.e.: 
\[
\text{Efficiency} = 1 - \frac{\max(0, c-\sum_j^m\sum_i^n \text{r}(g_i,j))}{c}
\]
where $\text{r}(g_i,j)$ is the amount of fish consumed by agent $g_i$ at round $j$ and $c=\frac{m\cdot R_0}{2}$.
Following \citet{piatti2024cooperate}, we set $\gamma=5, m=12$. 

\paragraph{Data splits}
To evaluate the generalization of our monitors, instances are split into train, validation and test sets. For \ourenv{}, splits consist of 210, 90 and 180 instances, respectively. Sets are separated by suspect descriptions and culprit choice. For \govsim{}, which requires substantially more computational resources due to longer discussions, sets consist of 26, 14 and 20 instances, respectively, and are different by their starting resource $R_0$.\footnote{We extend the evaluation by \citet{piatti2024cooperate} from 5 to 20 games to obtain a better performance estimate.}
More details regarding the data splits and exact $R_0$ values are available at \S\ref{app:datasplits}.

\subsection{Monitoring}
For monitoring, we consider the agent's generation --- both the final action selection and the reasoning preceding it --- at turn $j$, during action prediction. Specifically, we look at the output probability distribution at positions where the agent generated text that holds important informative for its action selection. In \ourenv{} this includes all the positions where the agent generated a suspect id, and in \govsim{} it covers all the mentions of resource amounts. 
Let $\mathbf{p}_i$ be the vector corresponding to the output probability distribution $\mathbf{p}_i$  at position $i$.
We compute the entropy, varentropy, and kurtosis of $\mathbf{p}_i$, and take their maximum values over all selection positions as candidate features for the monitor.\footnote{Since we evaluate also on proprietary models, we can't assume access to the full probability distribution. We approximate $\mathbf{p}_r$ with the top $k$ tokens, setting $k=10$.}

The maximum entropy, varentropy, kurtosis and turn count are used to fit a polynomial ridge classifier $f: \mathbb{R}^m \rightarrow [0,1]$, where $m\leq4$ is the number of features used.
The classifier is trained to estimate whether current game state will result in success, using boolean labels corresponding to whether a game in the train set ended successfully. For every environment and agent type, we train classifiers that use different feature combinations, polynomial degrees $d\in[1,5]$ and monitoring threshold $\tau\in[0,1]$. From these classifiers, we choose the one that best performs on the validation set (see specific implementation details at \S\ref{app:classifiers} and Tab.~\ref{tab:classifiers_used}). Overall, this results in a simple monitor that estimates $P(\texttt{success})$ at every turn. When this estimate is $<\tau$, the monitor triggers an intervention.


\subsection{Intervention}
The intervention we use throughout is restarting the last communication channel. For \ourenv{} this results in restarting the entire game, allowing the agents another attempt at the task. For \govsim{}, where actions are \emph{irreversible}, this means going back to the last round's communication and allowing the agents to discuss again, without any knowledge of the reset or any reflection they had after the last conversation. In our experiments, we define a cap on the number of resets, setting it to either one or two in \ourenv{} and one in \govsim{}. Since the two agents in \ourenvasym{} are different and have separate monitors, they can each reset separately up to the cap.


\subsection{Models}
We use LLMs that are common in agentic settings.
For \ourenv{}, we experiment with two strong open-weight models \llama{} \cite{grattafiori2024llama3herdmodels} and \qwen{} \cite{qwen2025qwen25technicalreport}, and one proprietary model -- \gpt{} \cite{openai2024gpt4technicalreport}.
For \govsim{}, the performance of \textsc{Llama-3-70B} and \textsc{Qwen-1.5-72B} is near-zero \cite{piatti2024cooperate},\footnote{We also observed similar results with the newer version of \llama{} and \qwen{}.} leaving us no positive examples for monitor training (we discuss applicability of our approach in more detail in the Limitations section). Thus, we experiment with the stronger \qwenlarge{} \cite{bai2023qwentechnicalreport} and \gpt{}. See full model names at \S\ref{app:models}.



