

\begin{figure}[t]
    \centering
        \includegraphics[width=\linewidth]{latex/figures/Whodunit_asymmetric_Environment_10_Suspects_across_models_nobootstrap.pdf}
        \caption{\textbf{Main results for \ourenvasym{}}. Live interventions lead to significant improvement in \success{} for three strong LLMs of up to 10 points, outperforming the baseline (dashed line). Resetting a second time leads to additional, yet diminishing gains.  Black lines indicate standard error.}
        \label{fig:asymmetric-env-results}
\end{figure}





\section{Results}
\label{sec:results}

Our results show that monitoring and interventions consistently improve system performance, leading to substantial gains of up to 17.4\% in \ourenv{} and up to 20.0\% in \govsim{}, across all models.


\paragraph{Monitoring and interventions improve performance on \ourenv{}}
Fig.~\ref{fig:asymmetric-env-results} and~\ref{fig:symmetric-env-results} present the results for \ourenvasym{} and \ourenvsym{}, respectively. For \ourenvasym{} our method outperforms the base model by 6.1\%, 10.6\%, 10.3\% for \gpt{}, \llama{}, and \qwen{} respectively, and by 4.1\%, 7.0\%, 4.5\% for \ourenvsym{}. When one reset is allowed, our method outperforms the best baseline on average across models and the variants by 6.3\%.


For \ourenvasym{}, increasing the number of resets results in additional gains (11.8\% vs 9.0\% with one reset across models on average). Interestingly, the open-weight \qwen{} and \llama{} perform similarly to \gpt{} with two resets.
However, a second reset did not further boost performance on \ourenvsym{}. We hypothesize that this is a due to the structured form of communication in \ourenvsym{}, which would result in less communication-based mistakes.

Our interventions come at a cost of additional turns. The average number of turns increases by a factor of 1.9 and 1.6 with double and single reset for \ourenvasym{} and 1.6 and 1.4 for \ourenvsym{}.
Detailed results per variant and model can be found at \S\ref{app:fullwhodunit}.


\paragraph{Monitoring and interventions improve performance on \govsim{}}
Tab.~\ref{tab:govsimres} presents our results for \govsim{}. With \qwenlarge{}, out method leads to an increase of 20.0\% in \survivalrate{} and 1.2 steps in \survivaltime. \efficiency{} remains similar, with 49.4\% vs 48.8\%. 
For \gpt{}, we substantially increase \efficiency{} by 6.9\% on average, while maintaining a maximal \survivalrate{}. Overall, our method leads to better collaboration and performance on the task.


\begin{figure}[t]
    \centering
        \includegraphics[width=\linewidth]{latex/figures/Whodunit_symmetric_Environment_10_Suspects_across_models.pdf}
        \caption{\textbf{Main results for \ourenvsym{}.} Live interventions lead to significant improvement in \success{} for three strong LLMs of up to 7 points, outperforming the baseline (dashed line). Interestingly, resetting a second time does not offer additional improvements. Black lines indicate standard error.}
        \label{fig:symmetric-env-results}
\end{figure}


\begin{table}[t]
\centering
\setlength\tabcolsep{3.2pt}
\footnotesize
\resizebox{0.995\columnwidth}{!}{
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Model} & \textbf{Ours} & \textbf{Survival} & \textbf{Survival} & \textbf{Efficiency} \\
     &  & \textbf{Rate} & \textbf{Time} & \\
    \midrule
    \textsc{Qwen} & \ding{55} & ~$35.0\pm20.1$ & ~$7.5\pm0.7$ & ~~$\mathbf{49.4}\pm 10.1$ \\
    \textsc{-1.5-110B} & \checkmark & $\mathbf{55.0}\pm21.8$ & $\mathbf{8.6}\pm0.8$ & ~$48.8 \pm 9.9$ \\
    \midrule
    \multirow{2}{*}{\gpt{}} & \ding{55} & $\mathbf{100}$ & $\mathbf{12.0}$ & ~$69.1 \pm 6.6$ \\
    & \checkmark & $\mathbf{100}$ & $\mathbf{12.0}$ & $\mathbf{76.0}\pm 4.8$ \\
    \bottomrule
  \end{tabular}
}
  \caption{\textbf{Main results for \govsim{}.} 
  Interventions lead to significant gains in \survivalrate{} and \survivaltime{} with \qwenlarge{} and in \efficiency{} with \gpt{}. Numbers include 95\% confidence intervals.}
  \label{tab:govsimres}
\end{table}

\paragraph{Gains are consistent across task complexities}
Fig.~\ref{fig:complexity} presents the results for \llama{} on \ourenvasym{} with 6, 10, and 14 suspects, using the monitor trained for 10 suspects.
We observe consistent gains of 14.0\%, 10.7\% and 11.7\% in \success{} with a single reset and 16.5\%, 15.4\% and 17.4\% with two resets across the different complexities. The average game length increases with complexity, from 1.9 with six suspects to 3.1 with fourteen suspects.
This shows our method generalizes well with no required monitor retraining.

\section{Ablations and Analysis}
\label{sec:analysis}

We conduct additional analysis on \ourenvasym{}, demonstrating that our monitors capture meaningful signals of agent uncertainty and the effectiveness of our interventions.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{latex/figures/Whodunit_asymmetric_llama_complexities.pdf}
  \caption{\textbf{Performance for \ourenvsym{} across difficulty levels}. Results are for \llama{} with 6, 10, 14 suspects. \success{} decreases with more suspects, but gains are consistent.}
  \label{fig:complexity}
\end{figure}


\begin{table}[t]
\centering
\setlength\tabcolsep{3.8pt}
\footnotesize
  \begin{tabular}{lcc}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \textbf{Success} &\textbf{Average} \\ 
    & \textbf{Rate} & \textbf{Length}\\ 
    \midrule
        Ours (Double reset) & \textbf{72.2} $\pm$ 1.6 & 42.5 $\pm$ 0.4\\ 
    Ours (Single reset) & 70.1 $\pm$ 1.7 & 31.7 $\pm$ 1.1 \\ 
    \midrule
    Monitoring: ~2nd best monitor & 69.3 $\pm$ 1.7 & 34.0 $\pm$ 1.5\\ 
    Monitoring: ~Worst monitor & 62.0 $\pm$ 1.8 & 30.6 $\pm$ 0.4 \\ 
    Intervention: Reset agent & ~~56.6 $\pm$ 1.85 & 20.1 $\pm$ 0.4\\ \midrule 
    Best baseline & 62.5 $\pm$ 1.8 & 34.3 $\pm$ 0.7\\
    No intervention & 59.8 $\pm$ 1.8 & 23.9 $\pm$ 0.5 \\

    \bottomrule
  \end{tabular}
  \caption{\textbf{Ablations for monitoring and interventions.} Strong classifiers and interventions on the communication channel are needed to improve results. Experiments are with \qwen{} and show standard error.}
  \label{tab:ablations}
\end{table}


\subsection{Ablations}

\begin{figure*}
    \centering
    \begin{minipage}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{latex/figures/hallucination.pdf}
       
        \label{fig:hallucination}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{latex/figures/hyperfocus.pdf}
        \label{fig:hyperfocus}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{latex/figures/lostrole.pdf}
        \label{fig:lostrole}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{latex/figures/inconsistency.pdf}
        \label{fig:inconsistency}
    \end{minipage}
    \vspace{-15pt}
    \caption{\textbf{Example monitor triggers for \ourenvasym{} with \llama{}}. We categorize triggers into  the following categories: hallucination (48\%), agent collapse (16\%), losing track of their role (8\%) and recall failure (4\%). The information relevant to the mistake is presented in bold text.}
    \label{fig:examples}
\end{figure*}

We conduct ablations to verify improvements are a result of our monitoring and intervening approach.
For monitoring, we experiment with the classifiers that performed second best or the worst on the validation set to verify that classifier performance is correlated with final gains.
For interventions, we experiment with resetting the step of the agent that triggered the alert instead of resetting the communication channel.

Tab.~\ref{tab:ablations} presents the results for \qwen{}.
For monitoring, the second best monitor leads to slightly lower gains (9.5\% vs 10.3\%) compared to the worst monitor with only 2\% improvement, which is similar to the baseline at 2.6\%. 
For interventions, resetting a single agent is ineffective, with results falling within 1.5 standard deviations of the no-intervention baseline and a decrease of 3.2\% in performance.
Overall, these results suggest that strong monitors and interventions are needed to improve performance.


\subsection{Qualitative Analysis}

To understand what phenomena are captured by our monitors, we perform a qualitative analysis of 50 examples in which monitors were triggered in \ourenvasym{} with \llama{}. 
We observed four categories of monitor triggers: (a) \emph{hallucination} (48\% of reviewed cases): the information shared was incorrect, (b) \emph{collapse} (16\%): an agent repeatedly asks about the same suspect, even when they already received the relevant information, (c) \emph{role loss} (8\%): the agent loses track of their role in the game, and (d) \emph{recall failure} (4\%): Accuser failed to recall key information that was previously shared by the Intel.
Fig.~\ref{fig:examples} presents an example for each category.
In the remaining cases, we could not identify a concrete issue in the game trajectory, suggesting they may be instances of a \emph{false trigger}.


Overall, 76\% of the monitor triggers belong to one of the four main categories, with hallucinations being the most frequent trigger.
Our analysis shows that our monitors successfully detect a wide range of problematic trajectories, and gains are due to high monitoring accuracy in intervened games.
