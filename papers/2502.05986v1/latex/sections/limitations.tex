
\section*{Limitations}

\paragraph{Environments} In this work, we focus on \ourenv{}, a new framework for multi-agent communications, in addition to GovSim \cite{piatti2024cooperate}. We believe that adapting our approach to additional environments is an exciting direction for future work.
We note that our approach can be directly extended only to environments where agents achieve non-zero accuracy, as otherwise interventions will have no affect on performance.
In addition, we showed that intervening by restarting an agent without the communication is less effective (\S\ref{sec:analysis}).
Our work can potentially be extended to single-agent settings by developing novel monitors and interventions.

\paragraph{Sample size}
For \ourenv{}, we experiment with 180 test environments . While this is a relatively small number, it is in line with recent evaluation sets that have 125-300 examples such as HumanEval \cite{chen2021evaluatinglargelanguagemodels}, DrawBench \cite{saharia2022photorealistic}, Bamboogle \cite{press-etal-2023-measuring}, and SWE-Bench \cite{jimenez2024swebench}. For \govsim{}, we extend from a single starting resource $R_0=100$ in \cite{piatti2024cooperate} to 20 choices. 
To reduce noise, we run every experiment between three and four times and report confidence intervals or standard deviation.
Nevertheless, smaller evaluation sets also have some advantage as they require less compute to evaluate, thus having a smaller environment footprint \cite{10.1145/3381831} and allowing more research teams to experiment with our environments.  

\paragraph{Data collection for training monitors}
To train our monitors to predict if an intermediate state of an agent will result in a task failure, we assume access to a set of train tasks. Realistically, collecting labels can be expensive, especially when task failures incur high costs. An exciting direction for future work is to frame our monitors as an online learning problem \cite{LITTLESTONE1994212, 10.5555/3041838.3041955, park2024llmagentsregretcase}, where one has to learn to detect rogue agents whilst minimize the number of system failures.

\section*{Ethical Implications and Broader Impact}
While multi-agent collaboration is an exciting direction for future research, it also entails significant risks.
Strong multi-agent systems can potentially solve tasks beyond the reach of current AIs, a potential risk if used by a malicious user.
In addition, multi-agent collaboration can have major economic and social impact. For example, a strong multi-agent system for autonomous software development can significantly increase productivity of engineering teams.

Another exciting future direction for multi-agent collaborations is operating embodied robots \cite{mandi2023rocodialecticmultirobotcollaboration, chen2024scalablemultirobotcollaborationlarge} and simulating human behavior \cite{10.1145/3586183.3606763}, which can cause significant social impact. In these domains, monitoring when future errors are likely and intervening when monitors are triggered can be especially important, as safe deployment presents a major challenge.
Moreover, environmental considerations should also be taken into account, as multi-agent systems often incur high costs (seen from our experiments in \ref{app:models}).