\section{Monitoring and Intervening in Multi-Agent Systems}
\label{sec:method}

In multi-agent systems, agents collaborate in order to solve a task or use shared resources.
The system is composed of a group of agents $G=\{g_i\}$, a communication channel $C$ that stores messages shared by the agents, and a shared task $T$. Agents can differ in the knowledge they posses $K_i$ and the actions they can perform $A_i$. 
For example, agents that simulate human behavior have different memories based on their experiences \cite{10.1145/3586183.3606763}, and different personas in a software development team have different actions, such as designing the code, programming, or writing tests \cite{qian-etal-2024-chatdev}.

Let $C_j$ be the communication channel at the $j$-th turn, an agent $g_i$ chooses their next action using a probability distribution over actions conditioned on their knowledge and the shared information: 
\[P_{a \in A_i}(a, j, g_{i}, T)=P(\texttt{Action}=a \,|\, K_i, C_j, T)\]
Similarly, agents share information in the channel with a distribution over knowledge pieces:
\[P_{k \in K_i}(k, j, g_{i}, T)=P(\texttt{Share}=k \,|\, K_i, C_{j-1}, T)\]

We propose to perform \emph{live}, mid-run interventions to prevent single agents from causing a system-wide failure (Fig.~\ref{fig:intro}). Our approach consists of monitoring agent action predictions to detect rogue agents, and intervening in the environment when a rogue agent is detected.

We view \textit{monitoring} as a function that estimates the probability of succeeding at the task at every turn, based on the agent's probability distribution over actions.
Namely, given $P_{A_i}$ for agent $g_i$ at turn $j$, we wish to estimate $P(\texttt{success} \,|\, P_{A_i}, j, g_i, T)$. 
That is, we aim to learn a signal that indicates a task failure is likely to occur -- for example, the agent being uncertain about their next action.

In case a future failure is likely to occur, i.e., $P(\texttt{success} \,|\, P_{A_i}, j, g_i, T) < \tau$ for some threshold $\tau$, we intervene to provide agents with an opportunity to reach a better state. Thus, \textit{an intervention} is a causal operation that modifies the current state of the environment based on its current state and the monitoring output. For example, the intervention could revert the communication or augment it with additional content.


\paragraph{Monitoring by predicting failures through agent uncertainty}
Inspired by prior work on agents in Reinforcement Learning and NLP environments \cite{acharya2022uncertaintyquantificationcompetencyassessment, liu-etal-2024-uncertainty, doi:10.1126/sciadv.adk1256, debunc}, we predict task success based on agent uncertainty. Namely, if the agent is ``confused'' in their action selection, they are likely to introduce noise which could fail the whole system. To estimate the agent's uncertainty, we compute simple statistics of its output probability distributions during generation. Specifically, we use entropy, varentropy and kurtosis, all are well known statistical methods for estimating model uncertainty (see exact definitions in \S\ref{sec:stat_measures}).

We use these statistics and the turn count as features to train monitors that predict game success from intermediate turns of the game. During test time, these features are collected at every turn and being fed into the monitor, which outputs the probability of success. Further details are in \S\ref{sec:experimental_setting}.


\paragraph{Live interventions to prevent system failures}
When performing an intervention, we stick to simply resetting the communication, providing agents with an additional opportunity to collaborate. 
We distinguish between \emph{reversible} states where the entire environment can be restarted (e.g., agents only shared information until the intervention), and \emph{irreversible} states where only the previous communication round can be reverted (e.g., a shared resource was used).


In the next section, we introduce a new multi-agent collaboration environment -- \ourenv{} -- for evaluating our approach.


\section{\ourenv{}: An Environment for Multi-Agent Collaboration}
\label{sec:env}

\ourenv{} is a modular multi-agent environment, where agents act as detectives working together to point out a culprit out of a suspect lineup. A game is comprised of $N$ suspects, each with a unique set of attribute-value pairs that are randomly assigned from a predefined set. Attributes include clothing (e.g., a green shirt), accessories (e.g., a silver watch), and personality traits (e.g., mood or interests). One suspect is randomly chosen as the culprit. Each agent receives partial information $K_i$ regarding either the suspects or the culprit, and must collaborate to figure out who the culprit is and accuse them. Turns move in a round robin fashion, and the game ends once either an agent accuses a suspect or a turn limit is reached.
Actions in the environment are tuples $(a,t)$, consisting of a prime action $a\in A$ and a target $t$ to which $a$ is applied.
We provide two variants of the environment, asymmetric and symmetric, which differ by the action set $A$ and information available to each agent $K_i$. See appendix for prompts (\S\ref{app:prompts}) and a specific example (\S\ref{app:full_example}).

\paragraph{\ourenvasym{}} (Fig.~\ref{fig:asym_env})
This variant consists of exactly two agents -- Accuser and Intel. $K_{accuser}$ contains the exact description of the culprit, but does not contain any information about the suspects. $K_{intel}$ is the complete description of all suspects, without any indication of the culprit. The set of actions available to Accuser is $A_{accuser}=\{\texttt{request-specific}, \texttt{request-broad}, \texttt{accuse}\}$, which allows it to request information about a specific attribute of a suspect, request broad information from Intel for no specific suspect or attribute, and accuse a suspect, respectively. The set of actions available to Intel is $A_{intel}=\{\texttt{respond}, \texttt{respond-broad}\}$ which allows it to respond to Accuser's request with a yes/no answer or return a broad message. When returning a broad message, the agent decides on an specific attribute value, such as ``green hat'', then lists all the suspects that have this property. Thus, Intel can choose to provide different (broader) information than requested by Accuser.



\paragraph{\ourenvsym{}} (Fig.~\ref{fig:sym_env})
In the previous environment, the agents are asymmetric in terms of the actions they can perform. Here, we propose a variant where all agents are equal in their actions, but different in the knowledge they posses. Agents start with full knowledge of all the suspects and their attributes, but each agent is given a different set of starting facts about the culprit $K_i=\{f_i^{(1)}, f_i^{(2)}, f_i^{(3)}\}$, where every fact is an attribute value. In each turn, an agent chooses an action $a\in \{\texttt{share}, \texttt{accuse}, \texttt{skip}\}$. For $a=\texttt{share}$, the agent selects a fact from $K_i$ and outputs it in a message to the rest of the group. For $a=\texttt{accuse}$, the agent decides a suspect to accuse of being the culprit and with that ends the game. With $a=\texttt{skip}$, the game simply moves to the next agent, spending the turn.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{latex/figures/WhoDunitAsymmetric.pdf}
    \caption{\textbf{An illustration of \ourenvasym{}} Accuser and Intel collaborate to identify the culprit from a lineup of suspects. Accuser, knowing the culprit's identity, can query and accuse. Intel chooses what and how much information to provide about the suspects.}
    \label{fig:asym_env}
\end{figure}


\paragraph{Complexity scaling}
\ourenv{} has different levers available to enhance or reduce complexity. This allows for the task to remain challenging and for testing of specific traits in agents. These levers include: (a) \textit{suspect count}: by changing the suspect count we can change how long the starting context is and the probability of having two very similar suspects, (b) \textit{attribute count}: by changing the number of attributes each suspect has we can create more specific suspects that are harder to set apart, and (c) \textit{turn count}: the game is set at a time limit, which affects the behavior of agents. By limiting their time, we force agents to use their available information better.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.91\linewidth]{latex/figures/WhoDunitSymmetric.pdf}
    \caption{\textbf{An illustration of \ourenvsym{}}. Agents are tasked with identifying the culprit among a lineup of suspects by sharing information they posses. Information about the culprit is equally spread across the agents and all agents can accuse a suspect.}
    \label{fig:sym_env}
\end{figure}
