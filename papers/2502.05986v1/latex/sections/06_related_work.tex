\section{Related Work}
\label{sec:rw}

\paragraph{Multi-agent communication} 
Communication between agents have long been suggested in order to develop strong modular systems \cite{KRAUS199779, 8352646, sukhbaatar2016learningmultiagentcommunicationbackpropagation, foerster2016learningcommunicatedeepmultiagent, 10.1007/978-3-319-75477-2_2, lazaridou2020emergentmultiagentcommunicationdeep, lowe2020multiagentactorcriticmixedcooperativecompetitive}. 
One of the most exciting applications of language agents is in designing multi-agent environments where agents autonomously communicate, with examples in improving factuality and reasoning via agent debate \cite{du2023improvingfactualityreasoninglanguage, liang-etal-2024-encouraging}, cooperation between embodied robots \cite{mandi2023rocodialecticmultirobotcollaboration, chen2024scalablemultirobotcollaborationlarge}, simulating software development teams \cite{li2023camel, hong2024metagpt, qian-etal-2024-chatdev, liu2024a}, modeling human interactions \cite{10.1145/3526113.3545616, 10.1145/3586183.3606763}, and gaming environments \cite{mukobi2023welfarediplomacybenchmarkinglanguage, xu2024exploringlargelanguagemodels}.
Thus, several frameworks have been proposed to enable simple development of multi-agent environments \cite{li2023camel, wu2023autogenenablingnextgenllm, hong2024metagpt}.
We contribute to this line of work by introducing \ourenv{}, a new modular environment for multi-agent communication, which allows easy configuration of difficulty levels and communication structures.

Previous work has showed that multi-agent collaboration is susceptible to adversarial attacks \cite{amayuelas-etal-2024-multiagent} and that Theory of Mind can be used to improve collaboration in simple gaming environments \cite{lim2020improvingmultiagentcooperationusing}. Our work introduces the notion of monitoring and interventions for improving communication in LLM-based multi-agent systems, which complements these prior methods.

\paragraph{Uncertainty estimation in language modeling} Uncertainty estimation has been shown useful in detecting and mitigating hallucinations in knowledge-intensive tasks \cite{kadavath2022languagemodelsmostlyknow, yona-etal-2024-large, ivgi2024from}, including in \emph{agentic retrieval}  where an external search engine is used \cite{jiang-etal-2023-active, han-etal-2024-towards}.
It has also been recently applied to language agents in order to increase exploration \cite{rahn2024controlling}, improve performance on bandit tasks \cite{felicioni2024on}, or making agents output textual estimates to help debates \cite{debunc}. In this work, we bridge uncertainty estimation and multi-agent collaboration by training monitors to predict the probability of task failures given agents uncertainty.



\paragraph{Aggregations over Multiple Generations}

Sampling multiple generations and aggregating over their answers is a popular method to increase performance \cite{wang2023selfconsistency, yoran-etal-2023-answering, du2023improvingfactualityreasoninglanguage, chen2024universal, min2024beyond}.
However, post-hoc aggregation is not directly applicable in agentic settings, where actions can be \emph{irreversible}.
Additionally, majority voting \cite{wang2023selfconsistency} requires at least three generations, while our approach increases turn account by less than twofold on average in our main experiments with \ourenv{}.
Our work differs by resetting the communication channel \emph{before} a problematic action was taken, rather than aggregating \emph{after} the final prediction.
