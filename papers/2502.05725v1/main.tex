\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amssymb}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{dsfont} %for the indicator funciton
\usepackage{amsthm}
\usepackage{bbold}
\usepackage{mathrsfs} 
\usepackage{multirow}
\usepackage{array}
\usepackage{natbib}
\usepackage{subcaption}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}} 
\setlength{\extrarowheight}{0.5mm}


%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%


\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\title{Predictive Coresets}
\author{Bernardo Flores\\
University of Texas at Austin}
\date{}


\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\p}{\mathbb{P}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\ptn}{\widetilde{\mathbb{P}}_n}
\newcommand{\ph}{\widehat{p}}
\newcommand{\DP}[1]{\text{DP}\left(#1\right)}
\newcommand{\ys}{y^\star}
\newcommand{\yts}{\widetilde{y}^\star}
\newcommand{\x}[1]{y_{N+#1}}
\newcommand{\thx}[1]{\th_{N+#1}}
\newcommand{\TT}{\mathcal{T}}
\newcommand{\PP}{\mathcal{P}}
\newcommand{\XX}{\mathbb{X}}
\newcommand{\sig}{\sigma}
\renewcommand{\th}{\theta}
\newcommand{\thb}{\bm{\theta}}
\newcommand{\ths}{\theta^\star}
\newcommand{\thts}{\widetilde{\theta}^\star}
\newcommand{\nb}{\bm{n}}
\newcommand{\sbf}{\bm{s}}

\newcommand{\iid}{\overset{\text{iid}}{\sim}}
\newcommand{\ind}{\overset{\text{ind}}{\sim}}




\begin{document}
\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

\maketitle



\begin{abstract}
Modern data analysis often involves massive datasets with hundreds of thousands of observations, making traditional inference algorithms computationally prohibitive. Coresets are selection methods designed to choose a smaller subset of observations while maintaining similar learning performance. Conventional coreset approaches determine these weights by minimizing the Kullback-Leibler (KL) divergence between the likelihood functions of the full and weighted datasets; as a result, this makes them ill-posed for nonparametric models, where the likelihood is often intractable. We propose an alternative variational method which employs randomized posteriors and finds weights to match the unknown posterior predictive distributions conditioned on the full and reduced datasets. Our approach provides a general algorithm based on predictive recursions suitable for nonparametric priors. We evaluate the performance of the proposed coreset construction on diverse problems, including random partitions and density estimation.

\end{abstract}
\vspace{1ex}
\textbf{Keywords:} coresets, predictive inference, Dirichlet process, optimal transport.
\vspace{2ex}

\section{Introduction}


We propose a construction of coresets based on a predictive view of Bayesian posterior inference \citep{fong2024, fortini2012}. The main attraction of the approach is the model-agnostic nature - the method is valid with any inference model and independent of the specific inference goals, making it highly adaptable for a wide range of applications. Such adaptability is particularly valuable in the context of large-scale datasets, now commonplace in fields like genomics and astronomy. While this explosion of data offers incredible opportunities for discoveries, it also brings significant computational challenges. Tasks that were once straightforward, such as evaluating likelihoods several times have become increasingly difficult, making traditional data processing methods impractical. These obstacles have frequently pushed practitioners toward simpler statistical models that might not capture the full complexity of the data, disregarding expressiveness and flexibility that rich hierarchical  and nonparametric models can offer. 

Typically, scaling Bayesian inference for massive data involves tweaking specific algorithms like Markov Chain Monte Carlo (MCMC) or variational Bayes to handle data distributed across systems or streaming in real-time. Techniques such as mini-batches and streaming for variational Bayes \citep{hoffmansvi} \citep{streamingvi}, subsampling methods for MCMC \citep{Quiroz2018SubsamplingMCMC}, and distributed methods for MCMC \citep{scotconsensusmc} are prominent examples. However, these existing methods have their drawbacks.  In practice, subsampling MCMC methods often need to examine a significant portion of the data each time to be able to reach stationarity, which limits their efficiency gains \citep{johndrownofree}. More scalable approaches like consensus MCMC and streaming variational Bayes improve computational efficiency but often lack rigorous theoretical backing and come with no guarantees for the quality of the proposed inference. A key observation in handling large datasets is that much of the data is redundant, while a smaller portion contains unique information. For instance, in a clustering setting, data points at the border of the cluster carry more discrimination power than the ones on the inside; moreover, adding two points close to each other does not add more structural information than just incorporating one of them. Similarly, in genome-wide association studies, only a small proportion of genes actually are correlated to the target, which requires a careful control of the false discovery rate \citep{sesiaKnock}.


Building upon these observations, a recent trend has been to focus on modifying the dataset itself rather than altering the inference algorithms. Specifically, to introduce a preprocessing step that constructs a coreset, a small, weighted subset of the original data that effectively approximates the inference with the full dataset. This coreset can be used with standard inference procedures to produce posterior approximations with guaranteed accuracy \citep{huggins2016}.  Coresets originated in computational geometry \citep{feldman2020}. In the Bayesian context several algorithms have been developed with strong theoretical backings; see \cite{campbell_automated_2019,huggins2016, manousakas2020}; yet the nature of these approaches using likelihoods has limited the applications for deep hierarchical models and likelihood-free settings like non-Euclidean and simulator-based models \citep{winter2023}.

In this paper we extend the concept of coresets by focusing on the posterior predictive distributions, rather than on the likelihoods; as well as using distances between probability measures instead of data points. This change of strategy allows us to include prior structure and to accommodate non-standard applications with data in non-Euclidean spaces. 

In Section 2, we review existing coreset approaches. Section 3 introduces our novel coreset construction alongside a detailed algorithm. In Sections 4 and 5, we demonstrate the versatility of our method through three applications—parametric logistic regression, random partitions and density estimation. Section 6 establishes theoretical guarantees by analyzing posterior contraction rates. Finally, Section 7 outlines an extension of the algorithm from Section 3 that facilitates faster hyperparameter exploration, and we conclude with some final comments.

\section{Coresets}

Coresets are data summarizing methods designed to, given a data set and a learning algorithm, reduce the size of the dataset while retaining a similar learning performance \citep{campbell_automated_2019}. Typically one wishes to approximate a density $\pi^N$ comprised of $N$ potentials $\{f_n(\theta)\}_{n=1}^N$ and a base density $\pi$,
$$
\pi^N(\theta) = \frac{1}{Z}\exp\left(\sum_{n=1}^N f_n(y_n, \theta)\right)\pi(\theta).
$$

In a Bayesian context, $\pi$ is the prior distribution, $f_n$ is the log-likelihood of the $n$th data point, and hence $\pi^N$ is the posterior density. The approximation is formulated by defining a set of sparse weights $w=(w_n)_1^N$ such that only at most $M$ weights are non-zero, where $M << N$, and then running the estimation algorithm on the weighted likelihood
$$
p_\theta (w\odot y_{1:N}) = \frac{1}{Z(w)}\exp\left(\sum_{n=1}^Nw_n f_n(y_n, \theta)\right),
$$
where the operator $\odot$ denotes the weighting by $w$. We denote the posteriors obtained under weights $w$ as $\pi^N_w$. Whenever no weights are used—that is, when all weights are set to 1—we denote the coreset by $\pi_1^N$ and refer to it as the unit coreset.

Finding the set of optimal weights $w^*$ can be done in a variational way by minimizing the Kullback-Leibler (KL) divergence between the weighted posterior $\pi^N_w$ and $\pi^N_1$, defined as
$$
 D_{KL}\left(\pi_w^N\mid\pi_1^N\right) = \int \pi_w^N \log\frac{\pi_w^N}{\pi_1^N}
$$
Thus $w^*$ is the solution of the optimization program
$$
w^* := \argmin_{w\in\mathbb{R}^N}\; D_{KL}\left(\pi_w^N \mid \pi_1^N\right)\quad \text{s.t.}\quad w\geq 0, \,\| w\|_0 \leq M,
$$

where $\| w \|_0 := \sum_n \mathbb{I}(w_n>0)$.

Many algorithms in this direction have been proposed; yet an open question remains; how to extend them to non-parametric settings. The difficulty arises from the fact that the normalizing constants are not available in a general functional form without integrating out some of the randomness and performing truncations. 

\cite{claici_wasserstein_2020} proposed an alternative, variational
view of coresets exploiting the machinery behind optimal
transport theory \citep{villani2003topics}. Recall that the $p$-Wasserstein
distance between two probability measures $\mu$ and $\nu$ is defined as 
$$
W_p(\mu,\nu)=\inf_{\gamma\in \Pi(\mu,\nu)}\int \|x-y\|_p^{1/p}\,\gamma(\dd x,\dd y),
$$
the infimum being taken over the set of couplings $\Pi(\mu,\nu)$ betweeen $\mu$ and $\nu$.
%

The idea is to introduce a variational family $\Pi_\theta$ and to find a
measure $p_\theta\in\Pi_\theta$ that minimizes the p-Wasserstein distance
with the likelihood  $p$ under the full data set, \emph{i.e.}
\begin{equation} \label{eq:wasscoreset}
\min_{p_\theta\in\Pi_\theta} W_p\left(p_\theta, \,  p\right).
\end{equation}
%
This was achieved by parametrizing the problem as finding a Lipschitz
map $T$, so that given data $y_1,\dots,y_N$ and a subsample of it
$y^*_1,\dots,y^*_n$, $T$ is obtained by solving
\[
\min_{T} W_p\left\{\ph\left[T(y^*_{1:n})\right], \,\ph\left(y_{1:N}\right)\right\},
\]
where $\ph$ denotes the empirical measure. Here and throughout we use notation $y^*_i$  for the subsamples
to highlight that the subsample is not restricted to the first $n$,
but could be any subset of the full $N$ data points. Note that \eqref{eq:wasscoreset} is equivalent to a Monge problem \citep{villani2003topics}, heavily studied in optimal transport theory.

We leverage this approach on an analogous problem to develop a general
methodology for constructing coresets for nonparametric models. Assume
we observe data $y_1,\dots,y_N\sim  p^0$ from an unknown model $p^0$.

Following the principles of
predictive inference (see \cite{bayespred} for a review), 
uncertainty around $p^0$  can be thought of as being represented 
by missing data. If we had
an infinite sample there would be no randomness left around
$ p^0$. This suggests inference on the data-generating mechanism, no
matter the method, is inherently done by learning the posterior
predictive distribution.


To facilitate the presentation of our method, we summarize the notation used throughout the article in Table \ref{tab:notation}.
\begin{table}[!ht]
    \centering
    \begin{tabular}{||c l || c|} 
 \hline
 Notation & Definition  & Space\\ [0.5ex] 
 \hline\hline
 $\boldsymbol{\cdot}^N$ & Posterior distribution & \multirow{2}{*}{Any probability measure}\\
 $\boldsymbol{\cdot}^*$ & Related to a subsample & \\
 \hline \hline 
 $\pi$ & Prior distribution &  \multirow{2}{*}{Parameters $\Theta$}\\
 $\pi_w$ & Weighted  &\\
 \hline\hline
 $p$& Likelihood &\multirow{7}{*}{Data $\mathbb{X}$} \\
 $p_\theta$ & Parametrized likelihood  & \\ 
 $p_\theta^w$ & Likelihood based on weighted data &\\
 $p_N = p_{y_{1:N}}$ & Posterior predictive under a DP & \\
 $p^0$ & True distribution & \\
 $\tilde{y}$ & Samples of $p_N$ & \\
 $F_\theta$ & Parametric family& \\
 \hline\hline
 $\hat{p}$ & Empirical measure & \multirow{3}{*}{ Probabilities $\mathcal{P}(\mathbb{X})$}\\
 \multirow{2}{*}{$\mathbb{P}_\theta$} & \multirow{2}{*}{Law of $p_\theta$ or 
  mixing measure }& \\
 & &\\
  \hline
    \end{tabular}
    \caption{Table with notation for the probability measures used in throughout this work.}
    \label{tab:notation}
\end{table}
\section{Predictive Coresets}

We now provide a formal definition of predictive coresets. We use \( p(x)
\) denote the probability law of the random element \( x \),
 and blackboard bold font like $\p$ to denote random probability
measures. 
\begin{definition}
  Given a sample \( y_{1:N} \) and a subsample \( \ys_{1:n} \),
  we define a \textbf{predictive coreset} as a measure 
  \( \tilde{\p}_n \propto \sum_{i=1}^n \delta_{\yts_i} \)
  given by the empirical distribution of  
  \(\yts_{1:n} = T(\ys_{1:n}) \),  where \( T \) is the
  \textbf{coreset transform} determined by minimizing a discrepancy \(
  d \) between measures over a family of transformations \( \TT\):
  \begin{equation}\label{eq:coreset}
    T = \arg \min_{T \in \TT}
    d\left[ p(\yts_{n+1:\infty} \mid T(\ys_{1:n})),\;
      p(\tilde{y}_{N+1:\infty} \mid y_{1:N}) \right].
  \end{equation}
\end{definition}
The nature of $\ptn$ as a random probability  measure arises from
the random variables $\yts_{n+1:\infty}$ and $\tilde{y}_{N+1:\infty}$.
Note that we use the additional tilde in $\yts_{n+1:\infty}$ to mark
the implicit transform $T$.

The definition poses two methodological questions:
\begin{itemize}
    \item[(A)] What is the most effective method to determine the conditional distributions in Equation~\eqref{eq:coreset}? 
    \item[(B)]  And how can we parameterize the family of transformations \( \TT \) to ensure sufficient expressiveness?
\end{itemize}




\subsection{A proxy for the posterior predictives}
 The context of coreset approaches are problems where
full posterior inference is not practically feasible.  We therefore do not have access to the true posterior predictive
distributions in \eqref{eq:coreset}, and need to approximate them in a way to ensure that the
predictive coreset is consistent.
For a generic description, 
suppose we aim to estimate a model \( F_\theta \) for \( \theta \in \Theta
\).
For example, $F_\theta$ could be the implied marginal distribution
of the data under the inference model under consideration, including
unknown hyperparameters $\theta$. 

We adopt the nonparametric
learning (NPL) framework of \cite{lyddon_nonparametric_2018}, which places a
Dirichlet process (DP) prior \citep{ghosal2017} centered at \( F_\theta \) on the
data-generating process \( p \): 
\begin{align}\label{eq:postpred}
    y_1, \dots, y_N \mid \mathbb{P}_\theta= p&\sim p, \nonumber \\
    \mathbb{P}_\theta \mid \theta &\sim \text{DP}(F_\theta, \alpha), \\
    \theta &\sim \pi(\dd \theta). \nonumber
\end{align}
Here, \( \pi(\dd \theta) \) is a hyperprior, %  on \( \Theta \),
which encodes any possible hierarchical structure in the model. The motivation for adopting \eqref{eq:postpred} is that it will allow us a computation efficient approximation of the posterior predictive in \eqref{eq:coreset}. To start, the posterior distribution based on \( N \) data points under \eqref{eq:postpred}is 
\[
%  \mathbb{P}_\theta^N =
  p_\theta \mid y_{1:N} \sim \p_\theta^N = \text{DP}\left(F_\theta^N, \alpha + N\right),
\]
where
\( F_\theta^N \propto \alpha F_\theta +
N \ph_N  \), and \( \ph_N \propto \sum_{i=1}^N
\delta_{y_i} \) is the empirical measure.
The parameter \( \alpha \) allows us to interpolate between the
Bayesian bootstrap \citep{clyde2001} and sampling from \( F_\theta \). 
% explain what to do with the thetas
The idea here is that the posterior law under \eqref{eq:postpred}
is computationally easy. In particular, 
the DP provides a tractable approximation of the true posterior
predictive under \( F_\theta \) via the Pólya urn scheme, denoted
$p_N$. Due to
posterior consistency, the posterior predictives eventually concentrate around
the true data-generating process \( p^0 \). Explicit
concentration rates will be provided in Section~6. 

By considering two alternative ways to condition the DP, once conditioning on both the full data \( y_{1:N} \) and once conditioning on the coreset support \( \ys_{1:n} \), we can recursively sample trajectories \( (y_{1:N}, \tilde{y}) \) and \( (\ys_{1:n}, \tilde{z}) \). Here we use $\tilde{y}$ and $\tilde{z}$, respectively, to denote the posterior predictive samples. We then compute the transformation $T$ that minimizes the distance between these trajectories. An overall optimal mapping $T$ is finally obtained via  averaging Monte Carlo samples over these transformations. The complete procedure is detailed in Algorithm~\ref{alg:core}.

Recall the discrepancy $d(\cdot,\cdot)$ in \eqref{eq:coreset}. 
A key observation is the following: because we are adding uncertainty around the
data-generating process, we must choose a discrepancy
defined between random probability measures rather than fixed probability
distributions. For a practical implementation (in the upcoming Algorithm \ref{alg:core}) we switch the optimization and the prediction in \eqref{eq:coreset}. We carry out the optimization in (2) for each realization of an approximate posterior predictive draw for $\tilde{y}^*_{n+1:n+M}$ and $\tilde{y}_{N+1:N+M}$, and define $T$ as the average of these optimizations. For a theoretical analysis we will use the Wasserstein metric with a Wasserstein base metric.
 Details of this definition are discussed later, in Section 7.  This
approach allows us to work with models over non-Euclidean spaces, such
as partitions, by incorporating their topologies into the computation
while requiring only well-behaved ground metrics. 




\subsection{Parametrization}
Regarding the second question (B) about the target \eqref{eq:coreset},
by using the Wasserstein distance finding the optimal mapping becomes a Monge problem: 
\[
\inf_{T: T_{\#}\mu = \nu} \int \| x - T(x)\|\,\mu(\dd x),
\]
where the infimum is taken over all measurable maps $T$ that push forward $\mu$ to $\nu$. The goal is to find the transformation that minimizes the cost in the $\|\cdot\|$ norm. In the setting of \eqref{eq:coreset}, $\mu$ is the empirical over the augmented full data $\hat{p}[(y_{1:N}, \tilde{y}_{N+1:N+M})]$ and $\nu$ the empirical over the augmented coreset support $\hat{p}[(y_{1:n}^*, \tilde{y}^*_{n+1:n+M})]$ and $T$ is restricted to only transforming the actual observations $y_{1:N}$ and $y_{1:n}^*$.

Finding $T$ is generally challenging. As highlighted by \cite{hutter2021}, the computational complexity becomes prohibitive as the dimensionality increases, especially in nonparametric settings. Moreover, working with random measures requires Monte Carlo estimates of distances between samples of these measures, further amplifying the computational burden due to the need to compute the transport multiple times.

For simplicity, we opt for linear transport maps, that is,
mappings of the form $T(\ys_1,\dots,\ys_n)=(w_1
\ys_1,\dots,w_n\ys_n)$; for brevety $\ys_{1:n}\mapsto w\odot
\ys_{1:n}$. 
These have demonstrated good performance in various settings, offering
sufficient flexibility to produce favorable results while possessing
dimension-free contraction rates, thus avoiding the curse of
dimensionality \citep{flamary2019concentration}. Further, because the $M$ points sampled from the Pólya urn are not part of the coreset support, we restrict the weighting to only the observed data, leaving $y^*_{n+1:M+1}$ unmodified.  Finally, we represent the posterior predictives by the empirical distribution of the $M$-step predictions.

With this we can give a full procedure, stated in Algorithm
\ref{alg:core}.
 To highlight the dependence of $\yts_{n+1:n+M}$ on
the transformation $T$, i.e., $\omega$, we write $\sig(\omega,M)
\equiv \yts_{n+1:n+M}$. 

\begin{algorithm}
\caption{Predictive coreset}\label{alg:core}
\begin{algorithmic}[1]\itemsep=0.1cm
\State Given a dataset $y_{1:N}$
\State Set a desired coreset size $n$
\State Sample coreset support $\ys_{1:n}$
\For{$t = 1,\dots, \text{niter}$}
\State Sample $\theta_t \sim \pi(\dd\theta)$ from the priors.
\State Sample $M$ points from the Pólya urn for the full data set implied by \eqref{eq:postpred}
$$
 \x{1},\dots, \x{M} \ \sim p_N 
$$
\vspace{-.5cm}
\State Let \(\sigma(\omega, M)  = \yts_{n+1:n+M}\) denote
 a size $M$ sample from a weighted Pólya urn
\(p_{\;\omega\odot \ys_{1:n}}\) using weights $\omega=\omega_t$. 
%sampling $M$ points with weights $\omega$ from the Pólya urn 
\State \label{step7}
The weights \(\omega_t\) are determined using \eqref{eq:coreset}, substituting the
empirical distributions under approximate posterior predictive draws. 
That is,

$$
\omega_{t} = \arg \min_{\omega} d\left[\ph(\x{1:M}), \;
  \ph(\sigma(\omega, M))\right],
$$
where \(\ph\) is the empirical distribution. 
\EndFor
\State Return $\bar{\omega}=\frac{1}{\text{niter}} \sum w_t$
\end{algorithmic}
\end{algorithm}
 The algorithm formalizes the optimization of the distances between
random probability measures in
% $$
% T = \arg \min_{T \in \TT}
%     d\left( p(\yts_{n+1:\infty} \mid T(\ys_{1:n})),\;
%       p(y_{N+1:\infty} \mid y_{1:N}) \right).
% $$
\eqref{eq:coreset} by an average over minimizations using distances over
approximate posterior predictive draws. 

Recall questions (A) and (B) after Equation \eqref{eq:coreset}. We have addressed (A) by putting a DP prior on the data-generating distribution, which facilitates a simple way to obtain the posterior predictives. For (B), we chose to parametrize the family of transformations $\mathcal{T}$ with affine maps over the observations, leaving the imputed predictions untouched. This way we obtained an efficient yet expressive algorithm sufficient for common applications. In the following section, we demonstrate the algorithm’s performance through several empirical examples.


\section{Simulations}
\subsection{Density Estimation}

We performed a simulation study with 100 repeat simulations. On each iteration we sampled $N=1,000$ data points from a location mixture of three Gaussians with random parameters $\mu_j\sim N(0,\sigma^2 )$, $j=1,2,3$ and $\sigma^2\sim \text{InvGamma}(1,1)$.


We put an infinite mixture of Gaussians with a DP mixing measure prior on the density, and choose the 2-Wasserstein metric as discrepancy. The infinite mixture was fitted using Stein variational gradient descent (SVGD) \citep{qiang2016} for the unit coreset, the predictive coreset and the full data, after which we recoreded the discretized KL divergence between the posterior mean densities. To get the coreset we took a subsample of size $n=50$ and sampled $M=200$ times from the Pólya urn.

The results are shown in Figure \ref{fig:dppm_a}. The coreset improves the fit compared to the uniform subsample. Taking the data from one repeat simulation, we see that the weighting helps to recover the two separate modes compared to the unit weights, as for the former the estimated mean density was unimodal. Figure \ref{fig:dppm_b} shows a histogram of the difference in KL divergences $D_{KL}(\hat{f}_\text{coreset}, \hat{f}_{\text{full}}) - D_{KL}(\hat{f}_\text{unit}, \hat{f}_{\text{full}}) $. In 81\% of the repeat simulations the weighted coreset yielded a closer fit to the full data than the unit coreset, showing an overall good improvement.


\begin{figure}[ht]
    \centering
    % First subfigure
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{entropy_differences_histogram.pdf}
        \caption{} % This will automatically generate (a)
        \label{fig:dppm_a}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{mixt_50.pdf}
        \caption{} % This will automatically generate (b)
        \label{fig:dppm_b}
    \end{subfigure}
    \caption{The left plot shows a histogram of the difference in estimated KL divergence between the posterior means for two cases: one comparing the coreset to the full dataset, and the other comparing the unit coreset to the full dataset. The right plot shows the posterior mean densities for all three cases.}
    \label{fig:dppm}
\end{figure}

\subsection{Logistic Regression}

Similarly to the first simulation, we performed 100 repetitions of the procedure described next: 

We sampled 10,000 data points $(x_i,y_i)$, $i=1,\dots,10,000$ from a logistic regression by generating a continuous covariate and an intercept $x \sim N\left([0,0],5I\right)$ and using a vector of logistic regression coefficients $\beta\sim N\left([0.5, 0.5],I\right)$.

Note the sample space here is now $\mathcal{X} = \{0,1\} \otimes \mathbb{R}^2$, as we cannot uncouple the response from the covariates. Thus we define a base metric on $\mathcal{X}$ given by the product distance $d\left((x_1, y_1), (x_2, y_2)\right)^2 = d_{L_2}(x_1,x_2)^2 + \mathds{1}(y_1=y_2)$, and as discrepancy we use d-Wasserstein distance.


To construct a coreset we need to now define a way of sampling from the empirical. The logistic regression gives a distribution for $y\mid x$, so to sample pairs $(x,y)$ we need a distribution for $x$. We choose the empirical $x_1,\dots,x_n \sim F_n^x$, which corresponds to the usual bootstrap.

We then get a subsample of size 20, corresponding to 0.2\% data. We sample 100 trajectories of size $M=100$ of the predictive, and obtain the optimum weights using the 2d-Wasserstein distance with the ground metric $d$ defined above.  

After this we fitted a logistic regression with standard Gausian priors on the coefficients using mean-field variational inference, obtained the posterior mean for the betas and registered the $L_2$ distance between the mean posterior logit functions based on the coreset and the unit coreset compared to  the full data.


The results are shown in figure \ref{fig:logreg}. We can see for one iteration the fitted logistic function with the predictive coreset is much closer to the one with the full data than the one with the unit coreset, \emph{i.e.} with just subsampling. We also show a histogram with the $L_2$ differences between the mean posterior logit functions. We found 71\% of the iterations yielded closer distances between the functions for the coreset than for the uniform subsample.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\linewidth]{logreg100.pdf}
    \includegraphics[width=0.45\linewidth]{logit_differences_histogram.pdf}
    \caption{On the left is the results for a coreset of size 20; on the right the distance between the mean posterior logits for the coreset and the uniform subsample.}
    \label{fig:logreg}
\end{figure}


\section{Coresets on random partitions}


The predictive framework developed here facilitates the construction
of coresets in non-Euclidean spaces. To illustrate this we consider coresets for random partitions. A key point to highlight is that,
in many scenarios, we do not directly observe a random partition;
instead, we observe data points $y_i$ that we wish to cluster. This
situation requires the notion of a mapping from the data space
$\XX$ to the space of partitions of $[n]=\{1,\dots,n\}$.
In a Bayesian context,  inference on partitions is often based on
mixture models, assuming that the data arise as
\begin{equation}\label{eq:mixture}
  y_i \mid \theta_i \ind k(\cdot \mid \theta_i),
  \quad \theta_i \mid \p \iid \p,
  \quad \p \sim G.
\end{equation}
where $\p$ is a discrete random probability measure and $k(\cdot \mid
\cdot)$ is a suitable kernel.
Equation \eqref{eq:mixture} is writing the mixture model
$y_i \sim \int k(\cdot \mid \th)\, d\p(\th)$ as an equivalent
hierarchical model.
Interpreting latent indicators $s_i$
that link the latent $\th_i$ with the
discrete point masses of $\p$ as cluster membership indicators
formalizes a random partition of experimental units $i=1,\ldots,n$.
The construction is motivated by Kingman's representation theorem \citep{kingman1978} which shows that under certain symmetry conditions, including exchangeability, any random partition can be thought of as arising
this way.
Let $\sbf=(s_1,\ldots,s_n)$ denote the implied random partition (with
or without ordering). For many choices of $\p$ the implied probability
function $p(\sbf)$ can be conveniently represented in terms of an
exchangeable partition probability function (EPPF) $f(\nb)$ where
$\nb=(n_1,\ldots,n_K)$ are the cardinalities of the clusters under $\sbf$
and $f(\nb)$ is a symmetric function of an additive decomposition
$(n_1,\ldots,n_K)$ of $n$ \citep{broderick2013}.


% this is accomplished by extending the problem to one of mapping
% probability measures over $\XX$ to probability measures over
% $\Pi(\XX)$, typically through mixtures involving discrete mixing
% measures.
%
% Consider the following mixture model:
%
% In this hierarchical model, each data point $x_i$ is generated by a
% kernel $k$ that is parameterized by a subject-specific parameter
% $\theta_i$. If the parameters $\theta_i$ are drawn from a discrete
% measure, clustering is induced by grouping indices $i$ and $j$
% together whenever $\theta_i = \theta_j$. 

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{mixture_density.pdf}
\caption{Data simulated from a mixture model, along with its induced clustering.}
\label{fig:clust_mixt}
\end{figure}
In summary, 
the posterior distribution resulting from \eqref{eq:mixture} yields a
sequence $\theta=(\theta_i)$, which can be mapped to a partition of 
$[n]$. A crucial distinction between this scenario and
traditional density estimation is that density estimation 
operates directly 
within the space of densities supported over the observed data,
whereas the earlier involves the pushforward of  the posterior mixing
measure into partitions. In essence, the data informs the posterior
distribution $\p^N$, which then generates the predictive distribution
mapping to an induced partition rather than back into the original
data space. In short, the approach to define predictive coresets for a random partition in this context is to apply Algorithm \ref{alg:core}, this time for the pairs $(\th_i, y_i)$.

Evaluating the effects of different coreset weights on the resulting partitions requires considering both the observed data \( (y_i) \) and the corresponding cluster indicators \( (s_i) \), which are derived from the latent sequence \( (\theta_i) \). Since \( (\theta_i) \) and \( (s_i) \) are latent variables, they must be imputed. A straightforward approach is to sample these variables from \( p(\mathbf{s}) \) and \( p(\boldsymbol{\theta} \mid \mathbf{s}) \), respectively, thereby generating a new allocation vector \( \mathbf{s} \) and corresponding latent \( \theta_i \) values at each iteration. This procedure then enables us to compute the posterior predictive distribution by calculating the Wasserstein distance between the empirical distributions of the mixing measures.

First, the parameters $(\theta_i)$ are sampled for the coreset support $(y_i)$ from the implied marginal prior under \eqref{eq:mixture}; from them the cluster assignments $(s_i)$ are obtained, after which the data is augmented as $(y_i, s_i)$. At this stage, depending on the model, there are two possible paths to follow. If the distribution on partitions allows for a closed-form posterior predictive distribution, one can directly sample the cluster assignments $s_{N+j}$ from $p(s_{N+1:M}  \mid y_{1:N})$ and then generate the data $\tilde{y}_{N+j}$ conditioned on these assignments and a cluster-specific distribution $G_{\theta_i}$. To obtain an approximate posterior for $G_{\theta_i}$ we assume a Dirichlet process prior with base measure $k(\cdot \mid \theta_i)$ and concentration parameter $\alpha$.

If the distribution on $s_{N+1:M}$ cannot be updated in a closed form, the approach differs. In this case, the Dirichlet process base measure in \eqref{eq:postpred} can be extended as $k(\dd y_i\mid \theta_i) \otimes \rho(\dd \theta_i)$. Here, cluster assignments are sampled implicitly by sampling $\theta_i$ using the Pólya urn  for the pairs $(y_i,\theta_i)$, a method that contrasts with the direct posterior updates used in the former scenario. The full procedure is shown in Algorithm \ref{alg:partcore}. Note that, in general, we do not need to compute distances between the cluster assignments $s_k$ but between the $\theta_k$.




\begin{algorithm}
\caption{Predictive coresets for partitions (conjugate update)}\label{alg:partcore}
\begin{algorithmic}[1]\itemsep=0.1cm
\State Given a dataset $y_{1:N}$
\State Set desired coreset size $n$
\State Sample a subset $\ys_{1:n}$
\For{$t = 1,\dots, \text{niter}$}
\State \label{step5} Sample clustering for the full data from the prior under \eqref{eq:mixture},
   $$ \theta_{1:N} \sim p(\thb \mid
  \sbf,y_{1:N}).$$ 
\State Extract $\ths_{1:n}$ and the implied
   partition  $s_{1:n}^* $ from $\theta_{1:N}$.
   \State Sample 
   $\{(\x{j},\thx{j});\; j=1,\ldots,M\}$
   from the Polya urn for $\{(\th_i,y_i);\; i=1,\ldots,N\}$.
  %  the full data
  %  set \[(x_k,\theta_k)\mid \cdot \sim \mathbb{P}_{\theta_k}\otimes
  % \rho(\theta_k\mid\theta_{-k})\] 
   \State \label{step8} Get
   $$
   \omega_{t} = \arg \min_{\omega} d\left(\ph(\x{1:M}\right), \;
   \ph\left(\sigma(\omega, M)\right),$$
   where \(\sigma\) is now extended to include the \(\ths_k\). 
\EndFor
\State Return $\bar{\omega}=\frac{1}{M}\sum \omega_t$.
\end{algorithmic}
\end{algorithm}
 Algorithm \ref{alg:partcore} is simply implementing Algorithm \ref{alg:core} for the pairs
$(y_i, \th_i)$, including only the one additional step of generating
the random partition in step \ref{step5}.
And posterior predictive sampling in step \ref{step8} is extended to
include $\ths_i$.


Analogously to the previous simulation studies, we repeated the following 100 times: we sampled $N=10,000$ observations from a Gaussian mixture with 6 components and recorded cluster memberships. We then sampled 50 observations uniformly and obtained the coreset weights with $M=500$ posterior predictive simulations and the $L_2$ distance on the $\theta_i$. A DP mixture of Gaussians was then fit using Stein gradient variational inference with an ordering constraint on the means to avoid label switching. 

To obtain a point estimate for clustering we took the mode of the cluster assignments across 100 posterior simulations. The results are shown in Figures \ref{fig:clust_out} and \ref{fig:melia}. The first figure shows the approximate posterior inference for a randomly chosen repeat simulation, whereas the second one shows a histogram with the difference in variation of information \cite{melia} between the coreset and the full data, and the subsample and the full data. We found the coreset yielded a closer inference to the one based on full data than just the subsample in 91\% of the samples.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{clust_out.pdf}
    \caption{The panels show approximate posterior inference using a uniformly chosen core set (left), the proposed core set using Algorithm 3 (center) and full posterior inference (right).}
    \label{fig:clust_out}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\linewidth]{vi_differences_histogram.pdf}
\caption{Histogram of the differences in variation of information between the coreset and the full data, and the subsample and the full data. }
\label{fig:melia}
\end{figure}


\section{Theoretical analysis}


We give properties of the predictive coresets that guarantee their asymptotic performance. Moreover, we discuss how non-asymptotic consistency results can inform the selection of an appropriate coreset size. Specifically, given a base metric space $(\XX,
\mathtt{d})$, we equip the space of distributions
$\p(\XX)$ with the Wasserstein metric
$W_{\mathtt{d}}$, and then consider
$(\mathcal{P}(\mathcal{P}(\XX)), W_{W_{\mathbb{d}}})$. For simplicity, we denote
$\mathbb{d} = W_{W_{\mathtt{d}}}$ and $d=W_{\mathtt{d}}$.


Algorithm \ref{alg:core} computes a functional of two posterior predictive distributions, which themselves are functions of a random measure: the posterior distribution. Thus to provide contraction rates we will work with the Wasserstein over Wasserstein topology (emph{c.f} \cite{hugomarta}). This corresponds to endowing $\mathcal{P}(\mathcal{P}(\XX))$, the space of random probability measures over $\XX$ with the Wasserstein distance with ground metric again the $p$-Wasserstein distance over $\XX$. 



The strategy now is to first obtain a posterior contraction rate, use it to get convergence and to establish a rate of convergence for the posterior predictives and then use a Cauchy sequence argument combined with optimal transport theory to get a rate of how close the weighted data set obtained from Algorithm \ref{alg:core} can get to the inference with the full data.

First we restate the main result from \cite{camerlenghi2022} for the DP. 

\begin{theorem}
Assume that $(\XX, \mathtt{d})$ is a totally bounded metric space with packing number of order $N_\delta(\XX, \mathtt{d})\sim \frac{1}{\delta^a}$ for some $a>0$ and small enough $\delta>0$. Then the following is a posterior contraction rate for the Dirichlet process with mean measure $H$, precision $\alpha$ and truth $p^0$ in the $q$-Wasserstein metric:
\begin{equation*}
\epsilon_n = \epsilon_{n, q}(\XX, p^0) +n^{-(a+q)/2} ,
\end{equation*}
with $\epsilon_{n, q}(\XX, p^0)$ being the $q$-Wasserstein rate of convergence of the Glivenko-Cantelli theorem for $H$, giving a $q$-Wasserstein posterior contraction rate at $p^0$.
\end{theorem}

The $\epsilon_{n,q}$ term is of order $o_P\left(Y(p^0) \left[\log\log
    n / n\right]^{1/2q}\right)$ assuming $p^0$ has enough finite
moments and $r-$dimensional Euclidean support. Notably, this rate is independent of the total mass $\alpha$. The quantity $Y(p^0)$ has a complex form depending on the dimension $r$ and the $2q+\delta$
moment of $p^0$ for some $\delta>0$, see \cite{dolera2019uniform} for
more details.  

For example, when $p^0$ a multivariate Gaussian with mean
$m_0$ and covariance matrix $V_0$ it takes the form
$Y(m_0,V_0)=\sqrt{2}\left\{\sigma_{\max}[V_0]+d\sqrt{K(\varepsilon,r)\text{tr}(V_0)}\right\}$
for small enough $\varepsilon>0$, where $\sigma^2_{\max}$ stands for
the largest eigenvalue and $K(\varepsilon,r)$ is a positive
constant. We can use this to show that Algorithm \ref{alg:core} converges. 


\begin{theorem}
As the number of simulations grows $n_t\to\infty$ the weights converge a.s.
\[
\lim_{n_t\to\infty}\frac{1}{n_t}\sum_{k=1}^{n_t} w_k = \arg \min_{\omega} d\left(p(\cdot\mid\omega \odot y_{1:n}), \; p(\cdot\mid y_{1:N})\right)
\]
\end{theorem}


  
\begin{proof}
    This is a consequence of the consistency of the posterior
    predictive for the DP given by the Glivenko-Cantelli theorem
    \citep{camerlenghi2022},  as when $n_t\to\infty$, both
    distributions converge in the Wasserstein space to the true data
    generating distribution $p_0$, provided the weighting map $\varpi_w:
    \sum \delta_{x_i} \mapsto \sum \delta_{w_i x_i}$ is continuous.


    The map $w_i \mapsto  w_i \odot x_i$ is continuous in any Banach space $(\XX,d)$. Take $\varepsilon>0$ and a discrete measure $\nu\in \mathcal{P}(\XX)$ with atoms $x=(x_i)$. Considering the independent coupling between $\varpi_{w_1} \nu$ and $\varpi_{w_2} \nu)$ we get from a change of variables, take $\delta>0$ and two vectors of weights $w_1, w_2$ such that $\|w_1-w_2\|_2<\delta$ implies , $d(w_1\odot x, w_2 \odot x)<\varepsilon$, which always exists by continuity.  Now
    \begin{align*}
    W_d(\varpi_{w_1} \nu, \varpi_{w_2} \nu) &\leq \int d(y,z)\, \dd \varpi_{w_1} \nu\,\dd  \varpi_{w_2} \nu\\
    &= \int d(w_1 y, w_2 z)\, \nu(\dd y)\,\nu(\dd z)\\
    &\leq \varepsilon,
    \end{align*}
    
 Thus the map $\varpi_w$ is continuous.
\end{proof}



We can now give a first bound.

\begin{theorem}
The posterior predictives satisfy for any $M_n\to\infty$
$$
d(\mathbb{P}_N, \mathbb{P}_n) = o_P(M_n \epsilon_{N,n}),
$$
where $\epsilon_{N,n} = \epsilon_N + \epsilon_n$.
\end{theorem}
\begin{proof}

    The posterior predictive of the DP is given by the generalized Pólya urn, whose one-step transition is the mean posterior measure \citep{ghosal2017}. \cite{nguyen2016} showed the DP is isometric to its mean measure in any $q$-Wasserstein space. So by definition of posterior contraction rates for every sequence $M_n\to\infty$ 
    
     \begin{align*}
     P\left(d(\mathbb{P}_n, p_0) >M_n\epsilon_n \mid x_1,\dots,x_n\right)&=P\left(d\left(\mathbb{P}_\theta^n, \delta_{p_0}\right) >M_n\epsilon_n \mid x_1,\dots,x_n\right)\\
     &\longrightarrow 0 \;\;p_0-\text{ a.s.}
     \end{align*}
    
    Now, using the triangle inequality
    \begin{align*}
    d(\mathbb{P}_N, \mathbb{P}_n)&\leq d(\mathbb{P}_N, p_0) + d(\mathbb{P}_N, p_0),
    \end{align*}
    
    thus
    \[
    P\left(d\left(\mathbb{P}_n, \mathbb{P}_N\right) \leq M_n\left(\epsilon_n+\epsilon_N\right) \mid x_1,\dots,x_n\right)
     \longrightarrow 1 \;\;p_0-\text{ a.s.}
    \]

    This implies that $p_0$ almost surely $d\left(\mathbb{P}_n, \mathbb{P}_N\right)=o_P(M_n(\epsilon_n+\epsilon_N))$. 
\end{proof}


This gives a (loose) upper 
bound of the error we are trying to minimize. We can interpret the rate by looking at the $\epsilon_{N,n}$ term. First note that this gives a family of rates so that as $n$ grows the distance between both distributions diminishes; moreover, the fact this bound was derived from a posterior contraction rate gives a non-asymptotic result on the shape of the Wasserstein balls so that with high probability the posterior predictive based on the sub-sample is at most at $\epsilon_{N,n}$ from the one based on the full data. 

Finally we will discuss the implications of the coreset weights, nameley how much can it improve over $\epsilon_{N,n}$. This can be translated to the expressiveness of the coreset transform, as if $w\odot\p_n$ can match $\p_n$, then the $\sigma$-algebras generated by both point clouds would be identical. 


Foremost, the transformation  $T_w: \mu\mapsto w\odot \mu$ can be interpreted as a transport plan between $\p_n$ and $\p_N$. Affine Monge maps have been shown to be reasonably well behaved for Euclidian settings in the $W_2$ space \cite{flamary2019concentration}, achieveing good generalization bounds while retaining computational efficacy. Our intercept-free setting retains the same properties by centering the samples. 


%%%
%Put somewhere that linear Monge maps work as long as the number of distinct parameters we wish to control remanis tiny. So settings like a regression with large p require a more convoluted parametrization of the map.


\section{An adaptive extension and conclusion}

Among the limitations of the proposed approach is in particular the prior simulation (in Step 5 of Algorithm \ref{alg:core}), which can lead to very inefficient approximations of the posterior predictive used for equation \eqref{eq:coreset}. Before we conclude with a broader discussion of limitations and extensions, we briefly outline a possible strategy to mitigate this limitation.

We introduce an extension of the basic predictive coreset algorithm aimed at reducing its computational burden, particularly in hierarchical models with multiple layers of hyperparameters. In the standard versions of Algorithms \ref{alg:core} and~\ref{alg:partcore}, the predictive trajectories are generated by independent sampling from the hyperpriors (in Step 5 of Algorithm \ref{alg:core}) sampling uniformly from the hyperpriors. However, in high-dimensional or deep hierarchical settings, this approach can be exceedingly slow because many sampled hyperparameter values lie in regions of negligible posterior mass, leading to a large fraction of wasted trajectories.

To address this issue, we draw inspiration from Wasserstein Approximate Bayesian Computation (Wasserstein-ABC) methods \citep{bernton2019}. The idea is to replace uniform sampling over the priors with an MCMC-based sampling scheme, focusing exploration on regions of the hyperparameter space $\Theta$ with high (approximate) posterior probability. This selective sampling strategy reduces computational overhead by concentrating on the portions of the parameter space that are most relevant for the model's predictive performance.


Our proposed algorithm employs a Metropolis-Hastings (MH) kernel. Specifically, after drawing a candidate hyperparameter $\theta^*$ from a proposal distribution $\pi(\mathrm{d}\theta^* \mid \theta_{t-1})$, we accept $\theta^*$ with a probability derived from an approximation to the usual likelihood ratio.  Let $p_\theta(\dd z)$ denote the sampling model. Recall that that in ABC, one aims to approximate the (unknown) posterior
\[
p^\varepsilon(\mathrm{d}\theta \mid y_{1:N}) 
\;\propto\;
\pi(\mathrm{d}\theta)\,\int \mathbb{1}\!\bigl(d(y_{1:n}, z_{1:n}) \le \varepsilon\bigr)\,p_{\theta}(\mathrm{d}z_{1:n}),
\]
where $z$ is hypothetical data generated from $p_\theta(\dd z)$ and $d(\cdot, \cdot)$ is a user-chosen discrepancy function, and $\varepsilon > 0$ is the ABC tolerance parameter. Because we generally do not have direct access to the likelihood $p_\theta$, we construct a Monte Carlo approximation by simulating data from $p_\theta$ repeatedly, yielding
\[
\hat{p}^\varepsilon(\mathrm{d}\theta \mid y_{1:N}) 
\;\propto\; 
\pi(\mathrm{d}\theta)\,\sum_{t} \mathbb{1}\!\bigl(d(y_{1:n}, z_{1:n}^t) \le \varepsilon\bigr),
\]
where $z_{1:n}^t$ is the $t$-th simulated dataset of size $n$ from the model $p_\theta$. Denoting $z_{\theta,1:n}^t$ a sample of size $n$ from $p_\theta$ and assuming a symetric proposal distribution, the acceptance probability in the Metropolis-Hastings step becomes 
\[
\rho(\theta^* \mid \theta) 
\;=\; 
1 \,\wedge\, 
\frac{
  \pi(\mathrm{d}\theta^*)\,\sum_{t} \mathbb{1}\!\bigl(d(y_{1:n}, z_{\theta^*,1:n}^t) \le \varepsilon\bigr)
}{
  \pi(\mathrm{d}\theta)\,\sum_{t} \mathbb{1}\!\bigl(d(y_{1:n}, z_{\theta,1:n}^t) \le \varepsilon\bigr)
}.
\]

This ratio reflects how likely the newly proposed hyperparameter $\theta^*$ is to generate simulated data close (under $d$) to the observed data $y_{1:n}$, compared with the current hyperparameter $\theta$. Once accepted, $\theta^*$ typically leads to predictive trajectories that are more likely to arise from a region of $\Theta$ with higher approximate posterior mass.

\subsection{Overview of the adaptive algorithm}

Algorithm \ref{alg:adap} describes the complete adaptive process. At each iteration, a candidate hyperparameter $\theta^*$ is generated from a proposal distribution centered at the current state $\theta_{t-1}$. Next, several pseudo-datasets $z^t_{\theta^*,1:n}$ are simulated from $p_{\theta^*}$. Using these simulations, the acceptance ratio $\rho(\theta^* \mid \theta)$ is computed based on the ABC posterior approximation. Finally, $\theta^*$ is either accepted or rejected according to $\rho(\theta^* \mid \theta)$, with the accepted state becoming $\theta_t$.


By iterating this procedure, we sample hyperparameters with a frequency proportional to their approximate posterior weight, thereby reducing the generation of wasted trajectories that are unlikely to contribute meaningfully to the model's predictive accuracy. The full adaptive algorithm is given in Algorithm \ref{alg:adap}.



\begin{algorithm}
\caption{Adaptive predictive coreset}\label{alg:adap}
\begin{algorithmic}[1]\itemsep=0.1cm
\State Given a dataset $y_{1:N}$
\State Set desired coreset size $n$
\State Sample coreset support $\ys_{1:n}$
\For{$t = 1,\dots, \text{niter}$}
\State Sample a proposal $\theta^* \sim \pi(\theta)$ from the hyperpriors.
\State Accept $\theta_t=\theta^*$ with probability $\rho(\theta^*|\theta)$, otherwise set $\theta_t=\theta_{t-1}$.
\State Sample $M$ points from the Pólya urn for the full data set 
$$
\ \x{1},\dots, \x{M}  \sim p_N 
$$
\vspace{-.5cm}
\State Let \(\sigma(\omega, M)  = \yts_{n+1:n+M}\) denote
 a size $M$ sample from a weighted Pólya urn
\(p_{\;\omega\odot \ys_{1:n}}\) using weights $\omega$. 
%sampling $M$ points with weights $\omega$ from the Pólya urn 
\State \label{step7}
The weights are determined using \eqref{eq:coreset}, substituting the
empirical distributions under approximate posterior predictive draws. 
That is,

$$
\omega_{t} = \arg \min_{\omega} d\left(\ph(\x{1:M}), \;
  \ph(\sigma(\omega, M))\right),
$$
where \(\ph\) is the empirical distribution. 
\EndFor
\State Return $\bar{\omega}=\frac 1M \sum \omega_t$
\end{algorithmic}
\end{algorithm}

\subsection{Discussion}
%write about the optimization
In this work, we proposed a novel construction of coresets for Bayesian inference by approximating predictive distributions rather than directly matching posteriors. This approach is both model-agnostic and scalable, making it particularly well-suited for large-scale datasets. Building on recent advances in computational optimal transport and Bayesian nonparametrics, we developed a simple yet flexible algorithm that performs well across diverse settings, including nonparametric scenarios that were previously out of reach for traditional coreset construction methods.

Despite its flexibility and strong empirical performance, the proposed approach has certain limitations. While we emphasize its model-agnostic nature, the method is not entirely free from modeling assumptions. Specifically, the approximation of the posterior predictive with a DP relies on exchangeability, which may not hold in all practical scenarios. Additionally, the prior sampling for hyperparameters can be computationally inefficient, particularly when dealing with high-dimensional parameter spaces, potentially limiting scalability in such cases. Another important consideration is the choice of the transformation $T$, which plays a crucial role in the coreset construction. Poor parametrization of $T$ can significantly affect the performance and robustness of the algorithm, highlighting the need for principled strategies for selecting or learning $T$ in future work. 



\section*{Acknowledgements}
I am grateful to my advisor, Peter Müller, for his guidance, valuable discussions, and constructive feedback throughout this research.


\section*{Supplementary Material}


\textbf{Code}: all code used to generate the results presented in this paper is publicly available at \texttt{github.com/BernardoFL/Predictive-coresets}.
%\bibliographystyle{apalike} % We choose the "plain" reference style

\bibliography{main}
\end{document}
