%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.


%%% == IMPORTANT ==
%%% Use the first variant below for the final paper (including auithor information).
%%% Use the second variant below to anonymize your submission (no authoir information shown).
%%% For further information on anonymity and double-blind reviewing, 
%%% please consult the call for paper information
%%% https://aamas2025.org/index.php/conference/calls/submission-instructions-main-technical-track/

%%%% For anonymized submission, use this
%\documentclass[sigconf,anonymous]{aamas} 

%%%% For camera-ready, use this
\documentclass[sigconf]{aamas} 


%%% Load required packages here (note that many are included already).

\usepackage{balance} % for balancing columns on the final page
\usepackage{blkarray}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{float}
\usepackage{multirow} 
\usepackage{csquotes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----Helper code for dealing with external references----
% (by cyberSingularity at http://tex.stackexchange.com/a/69832/226)

\usepackage{xr}
\makeatletter

\newcommand*{\addFileDependency}[1]{% argument=file name and extension
\typeout{(#1)}% latexmk will find this if $recorder=0
% however, in that case, it will ignore #1 if it is a .aux or 
% .pdf file etc and it exists! If it doesn't exist, it will appear 
% in the list of dependents regardless)
%
% Write the following if you want it to appear in \listfiles 
% --- although not really necessary and latexmk doesn't use this
%
\@addtofilelist{#1}
%
% latexmk will find this message if #1 doesn't exist (yet)
\IfFileExists{#1}{}{\typeout{No file #1.}}
}\makeatother

\newcommand*{\myexternaldocument}[1]{%
\externaldocument{#1}%
\addFileDependency{#1.tex}%
\addFileDependency{#1.aux}%
}
%------------End of helper code--------------


% put all the external documents here!
\myexternaldocument{appendix}







%%% AAMAS-2025 copyright block (do not change!)

% \makeatletter
% \gdef\@copyrightpermission{
%   \begin{minipage}{0.8\columnwidth}
%    {This work is accepted by the 24th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2025)}
%   \end{minipage}
%   \vspace{5pt}
% }
% \makeatother

\setcopyright{ifaamas}
\acmConference[AAMAS '25]{This paper is accepted by the 24th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2025) as a Full Paper}
\copyrightyear{}
\acmYear{}
\acmDOI{}
\acmPrice{}
\acmISBN{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% == IMPORTANT ==
%%% Use this command to specify your OpenReview submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{<<OpenReview submission id>>}

%%% Use this command to specify the title of your paper.

\title[AAMAS-2025 Full]{Bottom-Up Reputation Promotes Cooperation with \\ Multi-Agent Reinforcement Learning}

% Add the subtitle below for an extended abstract
%\subtitle{Extended Abstract}

%%% Provide names, affiliations, and email addresses for all authors.

\author{Tianyu Ren } 
\orcid{0000-0002-7053-2796}
\affiliation{
  \institution{University of Manchester}
  \city{Manchester}
  \country{United Kingdom}}
\email{tianyu.ren@manchester.ac.uk}

\author{Xuan Yao}
\orcid{0000-0002-8549-2096}
\affiliation{
  \institution{Southeast University}
  \city{Nanjing}
  \country{China}}
\email{shiny\_yao@yeah.net}
%School of Economics and Management, Southeast University, Nanjing 211189, China

\author{Yang Li}
\orcid{0009-0002-7073-9698}
\affiliation{
  \institution{University of Manchester}
  \city{Manchester}
  \country{United Kingdom}}
\email{yang.li-4@manchester.ac.uk}

\author{Xiao-Jun Zeng}
\orcid{0000-0002-2320-2495}
\affiliation{
  \institution{University of Manchester}
  \city{Manchester}
  \country{United Kingdom}}
 \email{x.zeng@manchester.ac.uk}


%%% Use this environment to specify a short abstract for your paper.

\begin{abstract}
Reputation serves as a powerful mechanism for promoting cooperation in multi-agent systems, as agents are more inclined to cooperate with those of good social standing. While existing multi-agent reinforcement learning methods typically rely on predefined social norms to assign reputations, the question of how a population reaches a consensus on judgement when agents hold private, independent views remains unresolved. In this paper, we propose a novel bottom-up reputation learning method, \textbf{\textit{L}}earning with \textbf{\textit{R}}eputation \textbf{\textit{R}}eward (\textbf{\textit{LR2}}), designed to promote cooperative behaviour through rewards shaping based on assigned reputation. Our agent architecture includes a dilemma policy that determines cooperation by considering the impact on neighbours, and an evaluation policy that assigns reputations to affect the actions of neighbours while optimizing self-objectives. It operates using local observations and interact- ion-based rewards, without relying on centralized modules or predefined norms. Our findings demonstrate the effectiveness and adaptability of LR2 across various spatial social dilemma scenarios. Interestingly, we find that LR2 stabilizes and enhances cooperation not only with reward reshaping from bottom-up reputation but also by fostering strategy clustering in structured populations, thereby creating environments conducive to sustained cooperation.
\end{abstract}

%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{Bottom-Up Reputation, Social Norm, Cooperative Intelligence, Multi-Agent Reinforcement Learning, Reward Shaping}

%Spatial Social Dilemma 
%Spatial Reciprocity
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
\newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

% \newcommand{\orcid}[1]{\href{https://orcid.org/#1}{\textcolor[HTML]{A6CE39}{\aiOrcid}}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
Cooperation is ubiquitous in almost every form of social interaction and is arguably a key factor in the success of complex social systems, such as economics~\cite{zheng2022ai}, evolutionary biology~\cite{sigmund2010social}, and multi-agent systems (MAS)~\cite{conitzer2023foundations}.  With the rise of artificial intelligence (AI), autonomously operating learning agents within MAS are becoming increasingly common. A typical example is autonomous vehicles, which must share the road with both other vehicles and human drivers; without proper coordination, issues such as road congestion may arise~\cite{liang2022federated}. However, achieving effective collective actions among self-interested agents remains a significant challenge. 

To address this challenge, mechanisms that facilitate correlated interactions among autonomous, decentralized learning agents are required~\cite{fatima2024learning}. These mechanisms shape the interaction structures or rewards within a population to promote cooperation over defection. In evolutionary biology and economics, indirect reciprocity (IR) and direct reciprocity (DR) have been identified as key drivers of cooperation~\cite{nowak2006five}. Notably, IR—offering a compelling explanation for the evolution of cooperative behaviour among MAS agents~\cite{nowak2005evolution}—differs from DR, where benefits are received directly from those assisted~\cite{van2012direct}; instead, IR allows individuals to benefit from the broader community. Consequently, sustaining cooperation via IR necessitates social information reflecting the historical behaviours of agents, which is often governed by social norms. These norms establish expected patterns and may impose penalties for violations. Enforcement typically involves rumour~\cite{kawakatsu2024mechanistic} or reputation~\cite{ohtsuki2004should}, which assess the \enquote{goodness} of agents and enable selective altruism, where individuals help those with good reputations.

Studies in MAS have demonstrated the importance of IR by incorporating it into traditional multi-agent reinforcement learning (MARL) tasks~\cite{vinitsky2023learning,xu2019cooperation}. Reputation allows agents to reshape rewards by assigning bonuses or penalties based on observed social information. For instance, agents can integrate moral values~\cite{tennant2023modeling} and normative punishment~\cite{vinitsky2023learning} to enforce compliance and learn behavioural rules. Reputation also serves as an informative signal for optimizing behaviour~\cite{anastassacos2021cooperation} or forming selective interaction relationships~\cite{ren2024enhancing,mckee2023scaffolding} across different scenarios. This body of research not only promotes cooperation and explains the formalization of artificial ethics and morality among MAS but also provides a feasible pathway for constructing a normative human-AI society.

Despite promising developments, reputation-based strategies and norms in MARL face several challenges. Standard IR theory assumes that reputations are common knowledge, meaning the entire population agrees on the evaluation process~\cite{santos2021complexity}. This consensus stabilizes reputation dynamics and promotes cooperation by setting clear expectations. However, when agents form private assessments, disagreements can arise, leading to perceptions of unfairness that may undermine cooperation~\cite{kawakatsu2024mechanistic}. Constraining the norm space through structured interactions appears essential to prevent cooperative collapse~\cite{murase2024computational}. Moreover, while social norms are often enforced exogenously by central institutions, they vary widely across different environments and populations~\cite{kessinger2023evolution}. Establishing appropriate norms thus demands extensive prior knowledge of the environment—a formidable challenge in complex MARL settings. These issues reduce the adaptability of reputation mechanisms and raise two fundamental questions: To what extent do individuals share a consensus on reputations, and how are social norms used to evaluate and assign reputations?

In this paper, we address these problems by developing Learning with Reputation Reward (LR2), a training method that promotes cooperation through reshaped rewards based on bottom-up reputation. Each agent has two policies: a dilemma policy for determining action and an evaluation policy for assigning reputations. LR2 agents aim both to maximise personal benefits without compromising their reputation and to assign reputations that incentivise prosocial behaviour within their local group while furthering their own interests. It reshapes the static reward function in a distributed manner by integrating assigned reputations from neighbours. Unlike similar reputation-based MARL methods, our approach emphasises the heterogeneity and endogeneity of norm formation, framing reputation assignment as a learning process rather than a reactive adaptation to predefined social norms~\cite{smit2024learning,oldenburg2024learning,xu2019cooperation}. This approach captures the coevolution of reputation and cooperation, offering a pathway to predict whether, and by what dynamics, a population will achieve coordination under IR. We validate LR2 in various social dilemmas, showing that it not only influences opponents' behavioural strategies through reputation assignment but also promotes strategy clustering in structured populations. Furthermore, we demonstrate that biased social information—where peers assign differing reputations to the same individual—can either facilitate or hinder cooperation, depending on the dilemma's strength. In summary, our work makes three contributions:
\begin{itemize}
    \item We propose a novel training method LR2, which promotes cooperation through bottom-up reputation. It enables agents to consider their impact on others while optimizing their objectives with learned reputation assignments.
    \item We reveal how LR2 promotes cooperation by examining the coevolution of cooperation and reputation. LR2 stabilizes prosocial behaviour through emerging normative judgments and further enhances cooperation via strategy clustering.
    \item We demonstrate that LR2 overcomes the scalability limitations of predefined norms in various social dilemma scenarios. It enables agents to learn and converge on shared norms in a decentralized and sample-efficient manner.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Works}

\textit{Cooperation in MARL.} MARL in collaborative settings can be dichotomized into two branches: team-based and mixed-motive environments~\cite{du2023review}. In the former, agents coordinate actions and share a single scalar reward. Several recent advanced methods have been proposed to solve this problem~\cite{yu2022surprising,sunehag2018value,son2019qtran}. By contrast, in mixed-motive environments, agents receive individual rewards, necessitating a balance between personal maximisation and social welfare. Such systems often encounter social dilemmas~\cite{van2013psychology}, where individually rational decisions lead to collectively suboptimal outcomes. In MAS, these dilemmas extend spatially and temporally, forming sequential social dilemmas~\cite{leibo2017multi}. A core challenge is understanding how cooperation among self-interested agents can emerge and remain stable, despite threats like conflict, overconsumption, free-riding, and defection~\cite{du2023review}. Previous research has explored solutions through other-regarding preferences~\cite{mckee2021multi,hughes2018inequity}, reputation mechanisms~\cite{anastassacos2021cooperation,smit2024learning}, and anticipation of future behaviours~\cite{foerster2018learning,yang2020learning}. Notably, reputation serves as an adaptable measure of social standing that can guide subsequent agent interactions~\cite{anastassacos2020partner}. Building on this, our paper focuses on encouraging cooperation in MARL through the auxiliary learning dynamics inherent in reputation assignments.

\textit{Reputation and Social Norms.} In the IR mechanism, actions are evaluated against social norms that integrate behavioural and reputational information to update an agent's standing~\cite{nowak2005evolution}. These norms can be imposed top-down by a central authority or emerge bottom-up through interactions~\cite{savarimuthu2011norm}. In the top-down approach, norms are designed offline and uniformly applied~\cite{santos2016social}. For instance, using Boo- lean inputs for reputation and strategy, norms can be encoded as a 4-bit string, $d=(d_{G, C},d_{G, D},d_{B, C},d_{B, D})_2$. Then, agents update reputations based on second-party reports with imperfect observations~\cite{haynes2017engineering}. Since reputations are repeatedly evaluated with small errors, this process can be modelled as an ergodic Markov chain with a characterised stationary distribution~\cite{ohtsuki2004should}, determining the fitness of an agent’s rule $\pi_i$. Previous studies indicate that the well-known norm Stern Judging (SJ) effectively drives behavioural dynamics through both imitation processes~\cite{santos2016social} and Reinforcement Learning (RL)~\cite{smit2024learning}. SJ assigns good reputations to agents who cooperate with good partners and defect against bad ones, while assigning bad reputations to those who act the opposite.

\textit{MARL and Bottom-Up Norm.} In MARL, the centralized training with decentralized execution (CTDE) framework~\cite{lowe2017multi} lacks centralized information for top-down judgment during execution. A bottom-up approach, however, enables agents to adopt decentralized methods for assigning reputations, allowing norms to emerge organical- ly~\cite{xu2019cooperation}. Recent studies explore how agents learn to assign and respond to reputations. These norms can be learned either through explicit payoff-based methods~\cite{xu2019cooperation} or by inferring the existence of shared normativity via approximate Bayesian rule induction~\cite{oldenburg2024learning}. Beyond reputation, the bottom-up creation of norms can also manifest as intrinsic rewards~\cite{mckee2021multi} or public sanctions~\cite{vinitsky2023learning}. However, maintaining cooperation under learned norm-guided behaviour is constrained by the presence of a predefined set of norms. In this paper, we extend this approach by allowing agents to assign reputations to their interacting neighbours based solely on their own rewards, which does not require knowledge of a predefined set of norms beyond the agents' observations. 

\section{Preliminaries}
\subsection{Social Dilemmas}
% The chosen strategy is applied uniformly across all neighbours, meaning an agent cannot cooperate with one neighbour while defecting against another. 
We examine social dilemmas using a spatial model of repeated two-player symmetric games, where agents interact with neighbours by uniformly choosing to cooperate ($C$) or defect ($D$). The game is defined by the payoff matrix $M$:
\begin{equation} \label{eq:payoff_matrix}
    M= \begin{blockarray}{cccc}
        & C & D\\
      \begin{block}{c(ccc)}
        C & R,R & S,T \\
        D & T,S & P,P  \\ 
      \end{block}
    \end{blockarray}.
\end{equation}

Here, mutual cooperation yields reward $R$ and mutual defection yields punishment $P$. In asymmetric cases, where one agent cooperates and the other defects, the cooperator receives the sucker's payoff $S$ and the defector obtains the temptation payoff $T$. These payoffs define the social dilemma~\cite{macy2002learning}:

\begin{enumerate}
    \item Snowdrift Game (SG): $T > R > S > P$. The optimal strategy depends on the opponent's action, making unilateral cooperation advantageous.
    \item Stag-Hunt Game (SH): $R > T > P > S$. Unilateral cooperation leads to a loss if the other player defects, emphasizing the importance of mutual cooperation.
    \item Prisoner's Game (PD): $T > R > P > S$. Mutual defection is the equilibrium outcome, even though mutual cooperation offers a better payoff for both players.     
\end{enumerate}

The PD, which blends aspects of SG and SH, poses the greatest challenge for fostering cooperation. Following the convention~\cite{santos2006evolutionary} of normalising the difference between $R$ and $P$ to $1$, we set $R = 1$ and $P = 0$, with $T$ and $S$ constrained to $0\leq T\leq 2$ and $-1\leq S \leq 1$, respectively.


\subsection{Reputation and Social Norms}
Reputation is public information shared among neighbouring agents, derived from their strategies to measure cooperativeness. Unlike previous studies using binary reputation values~\cite{anastassacos2021cooperation,podder2021local}, we model reputation as a continuous attribute ranging from $0$ to $1$, indicating an agent's degree of \enquote{Good} (G) or \enquote{Bad} (B). This extension enhances the robustness of agent learning by capturing finer variations in social standing regarding agent behaviour. Initially, the G and B labels are nominal with no inherent meaning; their significance emerges through agents' actions during the evolution of the game. An agent's decision to cooperate depends on its current reputation and the observed reputations of its neighbours. In pairwise donor games with binary reputations, common strategies include always cooperate (ALLC), always defect (ALLD), and discriminate (DISC), where DISC agents cooperate with good-reputation recipients and defect against bad ones~\cite{santos2016social}. However, since reputation in our model is continuous and agents interact with groups of neighbours rather than single recipients, the strategy representations used in previous studies~\cite{santos2018social} must be expanded. Specifically, the strategy tuple $\Pi$ becomes an infinitely large bit string $\Pi = (\pi_1,\pi_2,\dotsc)$, where each $\pi \in \{0,1\}$ represents whether an agent cooperates for a given combination of self and neighbour reputations. We employ a policy network to specify the behaviour rules for each agent, as detailed in Section~\ref{Sec: Method_Repu}. 

% $M=\langle \{\mathcal{A}_i\}^n_{i=1},\{\mathcal{O}_i\}^n_{i=1},S,T,I,P,\gamma \rangle$
\subsection{Markov Games}
We formalize the evaluation process in social dilemmas as an $N$-agent, partially observed, general-sum Markov games (POMGs)~\cite{littman1994markov,liu2022sample,perolat2017multi}, augmented with the concept of bottom-up reputation and an evaluation function that assesses the behaviours of neighbourhoods. A POMG is given by the tuple $M=\langle \mathcal{S,A,O,T,R,\gamma} \rangle$, where $\mathcal{A}\coloneq\mathcal{A}^1\times\cdots\times\mathcal{A}^N$ and $\mathcal{O}\coloneq\mathcal{O}^1\times\cdots\times\mathcal{O}^N$ denotes the joint action observations and the joint action space of all $N$ agents, indexed by $i\in[N]\coloneq\{1,\dotsc,N\}$. The game operates on a finite set of states $\mathcal{S}$, with each agent's $d$-dimensional observation of the state mapped by the function $\mathbb{O}: S\times\{1,\dotsc,N\}\rightarrow\mathbb{R}^d$. In each state, the agent $i$ selects an action from its action set $A_i$, and the state transitions according to the stochastic function $\mathcal{T}: \mathcal{S} \times \mathcal{A}^1 \times \dots \times \mathcal{A}^N \to \triangle(\mathcal{S})$ represents the set of probability distributions over $\mathcal{S}$. Each agent then receives a reward based on the reward function $\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$. The objective of agent $i$ is to learn a policy $\pi^i:\mathcal{O}^i\to\triangle(A^i)$ based on its own observation $o^i=\mathbb{O}(s,i)$ and current reward $r^i(s,a^1,\dotsc,a^N)$ where $s\in S$ represents the current environment state. The goal is to maximize a long-term $\gamma$-discounted payoff under the joint policy $\vec{\pi} = (\pi^1,\dotsc, \pi^N)$ from an initial state $s_0$:
\begin{equation}
V^i_{\vec{\pi}}(s_0)=\mathbb{E}\left[\sum^{T}_{t=0}\gamma^t r^i(s_t,\vec{a}_t) | {\vec{a_t}\sim \vec{\pi}_t,s_{t+1}\sim\mathcal{T}(s_a,\vec{a}_t)}\right],
\end{equation}
where $\gamma\in[0,1]$ is the discount factor, and $T$ denotes the time horizon. We extend POMGS by incorporating reputation assessments, formalized as the function $p_{\eta^i_t}:\tau^i_t \to \{0,1\}^{|\Omega^i|}$ where $\Omega^i$ represents the neighbour set for agent $i$ and $\tau^i_t\coloneq\bigcup_{j\in\Omega^i}((\mathcal{S\times A}^j)_t\times\mathcal{S})$ captures state-action trajectories of $i$ with its neighbours at timestep $t$. Accordingly, $\tau_t\coloneq\bigcup_{i\in N}\tau^i_t$ aggregates trajectories across all agents. In other words, the reputation assessment $p^i$ evaluates the goodness of each neighbour, assigning $1$ for \enquote{good} behaviour and $0$ for \enquote{bad} behaviour. We refer to this framework as \textit{reputation-augmented Mar- kov games}, which provides a basis for reputation assignment and coordination in sequential decision-making. The learning processes of the reputation evaluation and the reputation-augmented reward function will be detailed in the following section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}

\begin{figure*}[h]
\centering
  \includegraphics[width=\textwidth,keepaspectratio]{figs/fig1_overview_framework.eps}
  \caption{Overview of the social dilemma game with reputation and architecture of our LR2 agents. \normalfont(a) Each agent is connected to four neighbours in a network. Each round consists of two phases: First, agents choose to cooperate or defect based on their reputations and those of their surrounding neighbours.  In the second phase, agents receive reputation assignment reflecting how their behaviours are perceived within their neighbours' local group. (b) Agent $i$ updates its dilemma policy $\theta^i$ by considering both environmental rewards and assigned reputation. The evaluation policy $\eta^i$ is then updated based on the rewards accumulated by the updated dilemma policy $\hat{\eta}^i$.}
  \Description{Overview of the social dilemma game with reputation and architecture of our LR2 agent.}
  \label{fig: overview_framework}
\end{figure*}

Here, we introduce the Learning with Reputation Reward (LR2) met- hod, which allows agents to learn an evaluation function by explicitly accounting for their impact on neighbours and their own objectives. As shown in Figure~\ref{fig: overview_framework}a, each agent decides whether to cooperate or defect with its neighbours and then assigns assessments based on local interactions. A detailed LR2 framework is provided below, with the algorithm listed in Appendix \ref{Appendix:Algorithm}. 


\subsection{Learning with Reputation Reward}
\label{Sec: Method_Defination}
The core idea of our LR2 approach is to enable agents to learn an evaluation policy that progressively reshapes the rewards of nearby agents by considering the consequences of interactions on their own objectives. This reshaping encourages agents to account for the effects of their behaviour on their neighbours in a more cooperative manner. In the implementation, agent $i$ learns both a dilemma policy and an evaluation policy, parameterised by $\theta^i \in \mathbb{R}^N$ and $\eta^i \in \mathbb{R}^N$, respectively, in different sequences. Let $-i$ denote the set of all neighbours of agent $i$. As shown in Figure \ref{fig: overview_framework}b, LR2 agents optimize their dilemma policies by considering their actions on neighbours, while adjusting evaluation policies to influence neighbour behaviour. Each agent occupies a specific spatial coordinate and interacts only within its von Neumann neighbourhood~\cite{szabo2007evolutionary}. At each timestep $t$, agents engage in multiple pairwise social dilemma games involving strategy selection and reputation assignment.

In the first phase, agent $i$ selects either cooperation or defection as its dilemma strategy for the next round, according to $a_t^i \sim \pi_{\theta^i}(\cdot|s_t)$. After participating in one round of game pairwise interactions with each neighbour, the agent $i$ receives an environmental reward given by:
\begin{equation}
r^{i,\text{env}}_t=\sum_{j\in \Omega^i}{a^{i}}^{\top}\mathcal{M} a^{j},
\end{equation}
We refer to this aggregate reward as the environmental reward that the agent receives from participating in social dilemma games. In the second phase, the neighbours of agent $i$ assign assessments based on the evaluation function $p_{\eta^i}:\mathcal{O}\times A^{-i}\rightarrow \mathbb{R}^{|\Omega^i|}$, which maps the agent’s observation $o^i$ and the actions of its neighbours to a vector of reputation assessments for all surrounding agents. Finally, the reputation of agent $i$ at the timestep $t$ can be updated according to a running average:
\begin{equation}
    P^i_t= \alpha P^i_{t-1}+\frac{1-\alpha}{|\Omega^i|}\sum_{j\in\Omega^i}p_{\eta^{j}}^i(o^j_t,a^{-j}_t),
\end{equation}
where $\alpha$ is a smoothing parameter that quantifies how quickly the agent's reputation is reshaped by nearby assessments and $p^{i,t}_{\eta_j}$ is the reputation assessment assigned to agent $i$ by its neighbour $j$.

During training, LR2 agents update their dilemma policies by taking into account both environmental rewards and the reputations assigned by their neighbours. They then apply a separate policy gradient to the evaluation policy, based on the rewards generated by the updated dilemma actions (see Figure \ref{fig: overview_framework}b). Following the implementation in~\cite{yang2020learning}, we structure the agents update procedure in an online cross-validation manner~\cite{sutton1999policy}, to account for the fact that reputation assessments toward neighbours have a measurable effect only after those neighbours have updated their dilemma policies. We next describe the LR2 method in a step-by-step process below.


\subsection{Dilemma Policy Update}
\label{Sec: Method_Dilemma}
An LR2 learner optimizes its dilemma policy by considering the additional effect of reputation assessments received from neighbouring agents. Instead of optimizing the expected return under current dilemma parameters $V^i(\theta^i,\theta^{-i})$, LR2 optimizes \ $V^i(\theta^i,\theta^{-i},\eta^{-i}$), wh- ich accounts for the neighbours' evaluation policies $\eta^{-i}$. Specifically, it modifies the immediate reward of an agent so that, at each timestep $t$, the aggregate reward of agent $i$ is divided into two components:
\begin{equation}
    r^i(s_t,a_t,\eta^{-i})\coloneq\beta r^{i,\text{env}}(s_t,a_t)+P^i_{t}(1-\beta) r^{i,\text{env}}(s_t,a_t),
    \label{eq:reputation_reward}
\end{equation}
where $\beta$ is a weighting parameter that balances the influence of the environmental reward and the reputation-based reward. This formulation enables agents to adapt their strategies based not only on direct payoffs but also by accounting for how their actions affect their reputation among neighbours, effectively incorporating the influence of neighbouring agents' assessments into their own policy optimization. When $\beta=1$, agents degenerate into selfish entities,  learning independently and driven solely by their own interests.

Given this formulation, the reshaped reward $r_t^i$ depends on the average reputation derived from the neighbour assessments vector $p_{\eta^{-i}}$. Accordingly, agent $i$ learns a dilemma policy $\pi^i$, parameterised by $\theta^i$, to maximize the objective:
\begin{equation}
    \max_{\theta^i} J^{\text{dilemma}}\coloneq\mathbb{E}_\pi \left[\sum^T_{t=0}\gamma^t r^i(s_t,a_t,\eta^{-i}) + \omega \mathcal{H}^{\pi}(\cdot|s_t) \right].
\end{equation}

We incorporate an additional entropy bonus, controlled by the hyperparameter $\omega$, to encourage exploration and mitigate the issue of early convergence~\cite{haarnoja2017reinforcement}. The LR2 agent $i$ updates its policy parameter $\theta^i$ using a policy gradient approach:
\begin{equation}
\label{eq:dilemma_update}
    \hat{\theta}^i\leftarrow \theta^i+\lambda \sum^T_{t=0}\left[\nabla_{\theta^i}\log \pi^i(a^i_t|o^i_t)G^i_t(\tau^i,\eta^{-i})\right],
\end{equation}
where $\lambda$ is the decay learning rate, and $G^i$ represents the discounted return starting from time $t$, which depends on the trajectory $\tau^i$ and the evaluation policies of the neighbours $\eta^{-i}$.

\subsection{Evaluation Policy Update}
\label{Sec: Method_Repu}
With the updated dilemma policy $\pi_{\hat{\theta}}$, the system then generates a new trajectory $\hat{\tau}\coloneq (\hat{s_0},\hat{a}_t, \hat{r}_0,\dotsc, \hat{s_T})$ based on the updated dilemma policy $\pi_{\hat{\theta}}$. To evaluate the quality of their neighbours, agents primarily compare the average performance of their neighbours within their local group. Hence, the evaluation reward $r^{\text{eval}}$ is designed to assess how effectively agent $i$ interacts with a specific neighbour $j$ in comparison to the average interaction with all its neighbours:
\begin{equation}
    r^{i,\text{eval}}(\hat{s_t},\hat{a}_t) \coloneq\sum_{j\in \Omega^i}\left( \hat{r}^{i,j,\text{env}}\left(\hat{s_t},\hat{a}_t\right)-\frac{\hat{r}^{i,\text{env}}\left(\hat{s_t},\hat{a}_t\right)}{|\Omega^i|}\right).
\end{equation}

This comparison allows agents to influence influencing their neighbours' behaviour so as to maximize their own extrinsic rewards by assigning reputation scores to neighbours. Agent $i$ then uses this new trajectory $\hat{\tau}$ to optimize its evaluation policy, parameterised by $\eta^i$, with the following objective:
\begin{equation}
\label{eq:eval_obj}
    \max_{\eta^i} J^{\text{eval}}\coloneq \mathbb{E}_{\hat{\pi}}\left[\sum^T_{t=0}\gamma^t \left(r^{i,\text{eval}}(\hat{s_t},\hat{a}_t)-\mu \mathcal{D}^i(o_t,a_t)\right)\right],
\end{equation}
where $\mathcal{D}^i(o_t,a_t)$ represents the mean-squared-error, controlled by the sensitive parameter $\mu$:
\begin{equation}
    \mathcal{D}^i(o_t,a_t)\coloneq\sum_{j\in \Omega^i}\sum_{k\in \Omega^j} \left[p^j_{\eta^i}(o^i_t,a^{-i}_t)- p^j_{\eta^{k}}(o^k_t,a^{-k}_t)\right]^2.
\end{equation}

Unlike conventional RL settings where penalty terms like behaviour costs are used, we assume that agents have mixed motivations so that they also care about the alignment between their own assessments and others' assessments (i.e., gossip from neighbours' neighbours~\cite{kawakatsu2024mechanistic}) toward the same agent. However, we argue that this negative term incurred by Equation~(\ref{eq:eval_obj}) should not be included in the total reward, as the dilemma policy and evaluation policy are two separate modules. With the new trajectory $\hat{\tau}$, agent $i$ carries out an update regarding the evaluation policy accordingly:
\begin{equation}
    \hat{\eta}^i\leftarrow\eta^i+\lambda f(\hat{\tau}^i,\tau^i,\hat{\theta},\eta^i),
\end{equation}
with the same learning rate used in Equation~(\ref{eq:dilemma_update}). Note that when taking the gradient with respect to $\eta^i$, we need to account for the fact that $\hat{\pi}^{-i}$ depends on $\eta^i$. Finally, in alignment with the loss function proposed in~\cite{yang2020learning}, the update function for the evaluation policy $\pi_\eta^i$ can be expressed as follows:
\begin{equation}
f(\hat{\tau}^i,\tau^i,\hat{\theta},\eta^i)=\sum_{j\in\Omega^i}\sum^T_{t=0}\nabla_{\eta^i}\log\pi_{\hat{\theta^j}}(\hat{a}^j_t|\hat{o}^j_t){G'}^{i}_t(\hat{\tau}^i,\tau,\eta^i), 
\end{equation}
where
\begin{equation} \label{eq:G}
{G'}^i_t(\hat{\tau},\tau,\eta^i)=\sum^T_{l=t}\gamma^{l-t}\left[r^{i,\text{eval}}(\hat{s_t},\hat{a_t})-\mu\mathcal{D}^i(o_t,a_t)\right].
\end{equation}

Here, the first term on the right side of Equation~(\ref{eq:G}) reflects how changes in the agent's evaluation policy $\eta^i$ affect its own expected comparable extrinsic reward $r^{i,\text{eval}}$ through the impact on neighbouring agent's policies $\hat{\theta^j}$. Note that there is no recursive dependence of $\theta^{-i}$ on $\eta^i$ in the second term, as it is included in the previous evaluation episode.
% \begin{equation}
%     f(\hat{\tau}^i,\tau^i,\hat{\theta},\eta^i)=\sum_{j\in\Omega^i}(\nabla_{\eta^i}\hat{\theta}^j)^T\nabla_{\hat{\theta}^j}G^{i,\text{comp}}(\hat{\tau}^i,\hat{\theta})-\mu\nabla_{\eta^i}\mathcal{L}^i(\tau^i,\eta^i)
% \end{equation}

% \subsection{Training Approach}
% \label{Sec: Training}
% \yang{After reading this section, I believe it would be better to move the subsection into the experiments as the first part, serving as the experiment setup to introduce the details of the network and training. It is not part of the methodology.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experimental Setup}
\subsection{Evaluation Domain}
In our experiment, agents are situated on an $L \times L$ square lattice with periodic boundary conditions. Vertices represent agents, and edges indicate relationships with their four neighbours. At the start of each episode, agents are randomly assigned as either cooperators or defectors, with equal probability, and given a random reputation.

In all experiments, we simulate a population of $N=400$, each participating in all episodes. The agent parameters are distributed across $100$ learner processes, each handling policy gradient updates for four agents. To generate experience, $10$ parallel arenas are created, where agents interact with neighbours over $10,000$ episodes of $20$ timesteps each, totalling $200,000$ steps. After each episode, sampled trajectories are aggregated and sent to learner processes for updates. Cooperation levels, representing the average frequency of cooperative strategies, are measured across all scenarios.

\subsection{Practical Algorithm}
To maximize long-term $\gamma$-discounted payoff, a common strategy is for each agent to optimize its policy independently using policy gradient techniques~\cite{sutton1999policy}, such as REINFORCE~\cite{williams1992simple} or Proximal Policy Optimization (PPO)~\cite{schulman2017proximal}. In this paper, we deploy PPO as the learning algorithm. Similar to the asynchronous advantage actor-critic method~\cite{mniha16Asynchronous}, PPO maintains both value (critic) and policy (actor) estimates using deep neural networks and replaces the return $G^i_t$ by $G^i_t-V^i(s_T)$. However, it improves stability by employing a clipped surrogate objective to limit policy updates. We also implement common practices like Generalize Advantage Estimation (GAE)~\cite{Schulmanetal2016High} with advantage normalization and value clipping. Gradients are computed using the Adam optimizer~\cite{kingma2014adam} with a linear annealing learning rate.

\subsection{Design of Network Architectures}
In our implementation, each agent contains two independent neural networks, trained on batches of environmental experiences to update their weights. These policy networks operate separately, formulating policies independently and without sharing parameters across agents. Each network learns abstract representations from observations to take reward-maximizing actions. The architecture consists of a dual-layer perception with $32$ hidden units and employs the activation function $tanh$ for non-linear transformations. 

For the dilemma policy network, the inputs are the received reputation assignments of the agent and its neighbours. The output of the network at each timestep includes a policy containing a probability distribution over the next binary dilemma action (cooperation or defection), and a value function estimating the discounted future return under the current policy. For the evaluation policy network, agents take each neighbour's current dilemma action and the reputation assessments received from all surroundings as the observation input to produce a value function and an updated assessment vector for each neighbour. While all agents learn independently, they coexist within a shared environment where they influence each other's experience and learning.

\section{Results}
To ensure robustness, we average results over the final ten episodes and replicate each experiment five times. In the baseline (D-D) model, agents rely solely on their neighbours' past actions to decide whether to cooperate, without incorporating reputation information. To isolate the contributions of LR2's individual components, we developed ablated versions in which agents train using separate policies (e.g., Independent PPO (IPPO)~\cite{bettini2024benchmarl}), and reputation is decoupled from the reward structure. Although our primary focus is on LR2's performance in the Prisoner's Dilemma (PD) game, we also evaluate its effectiveness in the Stag Hunt (SG) and Snowdrift (SH) games. Finally, we examine the impact of various hyperparameters and the role of reputation-based intrinsic rewards on LR2's ability to promote cooperation in Appendix~\ref{Appendix:hyperparameter}.


\subsection{Effectiveness of Introducing LR2}
\begin{figure}
\centering
  \includegraphics[width=0.5\textwidth,keepaspectratio]{figs/fig2_heat_map.eps}
  \caption{Comparison of cooperation levels between LR2 and the D-D baseline across different $T$ and $S$ values. \normalfont LR2 demonstrates a more effective promotion of cooperation in the PD game. (a) D-D baseline: agents optimize dilemma policies based solely on environmental rewards. (b) LR2 method (ours): agents utilize both dilemma and evaluation policies, with rewards reshaped by reputation. The colour gradient from red to blue represents cooperation levels ranging from $0$ to $1$.}
  \label{fig: heatmap}
\end{figure}
We begin by comparing the performance of our LR2 method against the D-D baseline across various PD scenarios. Figure~\ref{fig: heatmap} depicts cooperation levels as functions of the temptation payoff $T$ and sucker's payoff $S$, with and without reputation assignment and reward reshaping. In Figure~\ref{fig: heatmap}a, we observe that the well-established spatial reciprocity effect in evolutionary game theory~\cite{wang2013insight}—which promotes cooperation through the network structure—fails to materialize in MARL settings using solely dilemma policies: even when exploitation carries no negative consequences (e.g., $S=0$), cooperation remains below $0.25$ in the D-D group. Moreover, as detailed in Appendix~\ref{Appendix:predefined}, predefined social norms do not foster cooperation in structured MARL populations, which is also contrary to recent findings in evolutionary dynamics~\cite{murase2024computational}. In contrast, Figure~\ref{fig: heatmap}b demonstrates that agents using the LR2 method outperform the D-D baseline, significantly expanding the \enquote{wave of cooperation} in the contour plot. Moreover, LR2 agents display high sensitivity to reward changes, with a sharper transition from full cooperation (blue area) to full defection (red area). Interestingly, contrary to previous studies suggesting that cooperation is difficult to sustain under conditions of extreme dilemma strength~\cite{ren2023reputation}, LR2 agents maintain cooperation even when dilemma strength is very high (e.g., $T=2.0$). This suggests the robustness of our approach in promoting the evolution of cooperation. We also assess LR2's robustness under alternative interaction structures in Appendix~\ref{Appendix:network}.

\subsection{Cooperation and Reputation Dynamics}
\begin{figure}
\centering
  \includegraphics[width=0.5\textwidth,keepaspectratio]{figs/fig3_repu_time_evolution.eps}
  \caption{The evolution of cooperation with associated rewards and reputations. \normalfont LR2 agents learn to assess their neighbours' behaviours to reshape rewards, fostering cooperative evolution. The evaluation includes (a) the evolutionary trajectory of cooperation levels, (b) the average reputation of cooperative and defective agents at the end of training, and (c)-(d) the rewards of cooperator and defector over time. Results are presented with the parameter $T$ varying from $1.30$ to $1.37$, while $S$ is fixed at $-0.33$.}
  \label{fig: time_evolution}
\end{figure}
We next illustrate the interplay between the evolution of cooperation and the reputation formulation process, providing a more intuitive explanation of how agents encourage prosocial behaviours by influencing others' learning dynamics. This relationship is detailed in Figure~\ref{fig: time_evolution}, which consists of four subgraphs representing the evolution of cooperation, the corresponding average rewards for cooperative and defective actions, and the reputation distribution for both actions. Each subgraph contains four parameter combinations, reflecting varying dilemma strengths in PD scenarios. As shown in Figure~\ref{fig: time_evolution}a, in all scenarios, the cooperation level exhibits a consistent trend: it briefly declines at the beginning, rises to a peak, and then either stabilizes or gradually decreases. 

\begin{figure*}[t]
\centering
  \includegraphics[width=0.86\textwidth,keepaspectratio]{figs/fig4_spatial_distribution.eps}
  \caption{Representative snapshots showing the spatial distribution of learned dilemma actions and assigned reputations on a square lattice. \normalfont The LR2 method fosters the formation of cooperator clusters through spatial effects, thereby enhancing the overall level of cooperation. Panels (a)-(e) display the evolutionary trajectories of two competing dilemma strategies at the timesteps $t= 1$k, $5$k, $10$k, $20$k, and $50$k. The corresponding panels (f)-(j) illustrate the average reputations assigned by neighbours at the same timesteps. Pixels represent agents as cooperators (blue) and defectors (red), with reputation levels ranging from $0$ to $1$. Results are obtained for $T=1.33$ and $S=-0.33$.}
  \Description{Snapshots showing the spatial distribution of learned dilemma actions.}
  \label{fig: strategy_distribution}
\end{figure*}

Since agents' rewards are reshaped by neighbour-assigned reputations, Figure~\ref{fig: time_evolution}b examines the effectiveness of the learned reputation assignment policy by comparing the average reputations of cooperators and defectors at the end of the training process. To highlight the differences, cooperative reputations are shown on a linear scale, while defective ones remain consistently below $0.1$ on a log scale. The group is more sensitive to negative feedback, consistently assigning low reputations to underperforming agents regardless of the dilemma scenario. However, they possess a conservative approach to positive ratings. Notably, even at $T=1.37$, the average reputation of cooperators remains around $0.75$. To further illustrate how these established evaluation strategies guided the LR2 agents' learning, Figures~\ref{fig: time_evolution}c and~\ref{fig: time_evolution}d present the evolutionary trajectories of the average rewards for cooperative and defective actions. Here, the reshaped reward functions as a teaching signal, encouraging agents to act prosocially while discouraging free-riding behaviour. Initially, cooperative rewards are lower than those of defectors, but as agents learn to assign appropriate reputations to their neighbours, the rewards for cooperation gradually surpass those for defection. This demonstrates the synergy between learning reputation evaluations and dilemma-based behaviours. Our findings suggest that effective reputation assignment is crucial; as $T$ increases, agents struggle to identify cooperators, leading to a rapid decline in cooperative rewards and the eventual extinction of cooperators.


\subsection{Formation of Spatial Strategy Patterns}
The above analysis provides an intuitive explanation of the synergy between the established reputation evaluation policy and the evolution of cooperation. To better understand why cooperation emerges within the LR2 framework, Figure~\ref{fig: strategy_distribution} presents characteristic snapshots of agent dilemma action patterns and received reputation assignment, allowing us to further investigate the strategy distribution in the learning dynamics of LR2 agents. Interestingly, we find that LR2 not only reshapes neighbours' payoffs and influences their learning dynamics by effectively identifying dilemma strategies and assigning reputations, but also promotes the formation of cooperative clusters (abbreviated as C-clusters). This spatial pattern formation highlights the evolutionary advantage of the learned evaluation policy, as it helps resist defector invasion and facilitates the emergence of cooperation~\cite{szolnoki2017alliance}. 

Furthermore, the evolutionary path, starting from an initial random state (Figures~\ref{fig: strategy_distribution}a and~\ref{fig: strategy_distribution}f) and progressing to a final equilibrium, can be divided into two distinct phases, consistent with the reported spatial reciprocity effect~\cite{wang2013insight}. The first phase is the enduring period (END), during which cooperators resist the invasion of defectors. This is followed by the second phase called the expansion period (EXP), where the fraction of cooperators begins to increase, signalling the growth and spread of cooperative behaviour. As shown in Figures~\ref{fig: strategy_distribution}(a-b) and~\ref{fig: strategy_distribution}(f-g), during the END period, LR2 agents have not yet learned to effectively evaluate the behaviour of others, leading defectors to avoid reputational damage despite their free-riding behaviour. Consequently, D-clusters expand rapidly, constraining the space available for cooperators. However, once agents learn to assign reputations correctly, they can influence the learning dynamics of others and indirectly protect their own interests. Although cooperation still declines during this phase, the decrease is progressively restrained as C-clusters form incrementally, allowing cooperators to survive and maintain their presence (Figure~\ref{fig: strategy_distribution}c). After a suitable transient period, the population enters the EXP phase, where agents begin to assign reputations accurately. This causes the fragmented D-clusters to gradually disintegrate, while the more cohesive C-clusters expand in size. Moreover, the finding that agents are more sensitive to losses in Figure~\ref{fig: time_evolution}, is confirmed here. As observed in the second row, LR2 agents consistently assign a reputation level of $0$ to defective neighbours but are slower in providing positive evaluations to cooperative behaviours. 

\subsection{Architecture Ablations and Robustness}
\begin{figure}
\centering
  \includegraphics[width=0.50\textwidth,keepaspectratio]{figs/fig5_combine_heat_map.eps}
  \caption{Ablations on LR2 architecture components. \normalfont Considering others' reputation evaluations and the effects of assigned reputations most effectively promotes cooperation.  (a) Reputation alignment varying importance, shown by colours from dark to light representing $\mu$ from $1$ to $0$. Parameter $S$ is fixed at $-0.33$. (b) Performance of the IPPO training method without reputation reward.}
  \label{fig: combine_ablation}
\end{figure}
To better understand LR2, we modified its architecture to address two questions: (1) Does LR2's performance depend on agents forming heterogeneous evaluations of the same behaviour? (2) How is performance affected when agents merely observe reputation without integrating it into reward reshaping? To answer these, we conducted ablation experiments to illustrate the effects of reputation alignment and reward on LR2's performance (Figure~\ref{fig: combine_ablation}). Finally, we verified the robustness of the LR2 method in the SG and SH dilemma settings, with the results summarized in Table~\ref{tab: robustness}.

\textit{Dual impact from local assignment.} In LR2, agents consider both the rewards from interactions and the consistency of their evaluations with others. Since reputation assignment is a biased judgment based on private observation, isolated disagreements may arise. Figure~\ref{fig: combine_ablation}a compares six cases, ranging from completely ignoring reputation alignment ($\mu=0$, darkest bar) to treating it as equally important as rewards ($\mu=1$, lightest bar) in $0.2$ intervals. As shown in all cases, moderate consideration of evaluation consistency ($\mu=0.2$, red border) consistently leads to optimal cooperation. While aligning evaluations with neighbours can help agents learn prosocial behaviour, placing too much emphasis on alignment can be detrimental. For example when $T=1.37$, treating alignment and rewards equally results in the worst performance.

\textit{Decoupling reputation from LR2 rewards.} In Figure~\ref{fig: combine_ablation}b, we consider an IPPO ablation where each agent still trains two policies, dilemma and evaluation, but reputation is used only as observational information and does not affect rewards, by setting $\beta=1$. Compared to the rewards in Figure~\ref{fig: heatmap}, IPPO still outperforms the baseline by using reputation to assess the quality of an individual's past behaviour, since it enriches observational information. However, the \enquote{wave of cooperation} shrinks sharply compared to LR2 training. It suggests that reward shaping proves crucial for sustaining cooperative behaviour, particularly when dilemma strength intensifies, as seen in the right top corner of both contour plots.
\begin{table}[h]
  \caption{Comparison among LR2 agents, D-D baseline, and an ablated IPPO method across three types of dilemmas. \normalfont Performance is measured by the average cooperation level after training. Agents trained using the LR2 method (ours) outperform the other two methods in all scenarios.}
  \centering
  \resizebox{0.49\textwidth}{!}{
 %\setlength{\tabcolsep}{1.8pt} {
    \begin{tabular}{lcccccc}
        \toprule
        \multirow{2}*{Method} & \multicolumn{2}{c}{Prisoner's Dilemma} & \multicolumn{2}{c}{Snowdrift Game}  & \multicolumn{2}{c}{Stag-Hunt Game} \\
        \cmidrule(rl){2-3} \cmidrule(rl){4-5} \cmidrule(rl){6-7} 
        ~ & {($1.1$,$-0.1$)} & {($1.3$,$-0.3$)} &  {($1.1$,$0.1$)} & {($1.3$,$0.3$)} & {($0.9$,$-0.1$)} & {($0.7$,$-0.3$)} \\
        %\cmidrule(r){1-7} 
        \midrule
        D-D & $0.00$ & $0.00$ & $0.50$ & $0.49$ & $0.10$ & $0.28$  \\
        IPPO & $0.95$ & $0.00$ & $0.99$ & $0.98$ & $0.98$ & $0.99$  \\
        \textbf{LR2 (ours)}& $1.00$ & $0.98$ & $1.00$ & $0.99$ & $1.00$ & $1.00$  \\
        \bottomrule
    \end{tabular}
  }
  \label{tab: robustness}
\end{table}

\textit{Robustness of LR2 in different dilemma types.} We conclude the Results section by evaluating the robustness of the proposed LR2 method in promoting cooperation across different dilemma types, comparing it to the baseline D-D method and ablation IPPO version. As shown in Table~\ref{tab: robustness}, we assess two parameter combinations in each of the SD, SG, and SH scenarios, representing weak and strong dilemma strengths. LR2 consistently exhibits strong performance in both cases, highlighting the effectiveness of our approach. While IPPO approaches similar cooperation levels to LR2  under weak dilemmas, it fails to maintain this effect as the dilemma intensity increases, emphasizing the importance of incorporating reputation rewards and assignments into the learning process. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion and future work}
We introduce Learning with Reputation Reward (LR2), a method that leverages reshaped rewards based on bottom-up reputation to promote cooperative behaviour. LR2 incorporates reputation assignment into the dilemma policy learning, while agents concurrently learn an evaluation policy that assigns reputations to incentivise prosocial actions within local groups and maximise their extrinsic objectives. Unlike previous IR methods in MARL that often rely on predefined social norms to guide reactive learning, LR2 allows agents to privately assess each other's reputations based on their own interactions and local observations, further optimizing behaviour using assigned reputation information. This approach provides a pathway to predict the coevolutionary dynamics of reputation and cooperation in structured populations.

Our findings show that LR2 not only stabilizes but also enhances cooperation in spatial social dilemmas. LR2 agents are more sensitive to poor performance, allowing them to effectively identify and penalise defective neighbours. Because reshaped rewards force agents to consider the causal impact of their actions on neighbours, they adopt myopic best responses that lead to improved collective performance. This supports the assertion that even when reputations are privately assessed without enforcement from a top-down institution, emerging reputations can still facilitate cooperation through IR methods~\cite{kawakatsu2024mechanistic,kessinger2023evolution,podder2021local}. Moreover, LR2's promotion of cooperative behaviour is not solely due to reward reshaping but also to its ability to foster the clustering of cooperative strategies, further enhancing spatial reciprocity~\cite{wang2013insight,ren2021evolutionary,he2024temporal}.

Our results raise several unresolved issues regarding the bottom-up reputation in cooperative MARL. While LR2 emphasizes the importance of local observations, agents may receive conflicting information from multiple sources with different weights. Future research could explore how agents process such information in structured populations~\cite{wu2020comprehensive} and consider extending LR2 to more complex evaluation frameworks, such as Melting Pot~\cite{leibo2021scalable}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The acknowledgments section is defined using the "acks" environment
%%% (rather than an unnumbered section). The use of this environment 
%%% ensures the proper identification of the section in the article 
%%% metadata as well as the consistent spelling of the heading.

\begin{acks}
The authors would like to acknowledge our anonymous reviewers for their thoughtful feedback and thank the assistance given by Research IT and the use of the Computational Shared Facility at The University of Manchester.
\end{acks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section*{Supplementary Material}
Source code is available at \url{https://github.com/itstyren/LR2}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%ms%%%%%%

%%% The next two lines define, first, the bibliography style to be 
%%% applied, and, second, the bibliography file to be used.

\bibliographystyle{ACM-Reference-Format} 
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

