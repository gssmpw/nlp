\section{Related Works}
\textit{Cooperation in MARL.} MARL in collaborative settings can be dichotomized into two branches: team-based and mixed-motive environments____. In the former, agents coordinate actions and share a single scalar reward. Several recent advanced methods have been proposed to solve this problem____. By contrast, in mixed-motive environments, agents receive individual rewards, necessitating a balance between personal maximisation and social welfare. Such systems often encounter social dilemmas____, where individually rational decisions lead to collectively suboptimal outcomes. In MAS, these dilemmas extend spatially and temporally, forming sequential social dilemmas____. A core challenge is understanding how cooperation among self-interested agents can emerge and remain stable, despite threats like conflict, overconsumption, free-riding, and defection____. Previous research has explored solutions through other-regarding preferences____, reputation mechanisms____, and anticipation of future behaviours____. Notably, reputation serves as an adaptable measure of social standing that can guide subsequent agent interactions____. Building on this, our paper focuses on encouraging cooperation in MARL through the auxiliary learning dynamics inherent in reputation assignments.

\textit{Reputation and Social Norms.} In the IR mechanism, actions are evaluated against social norms that integrate behavioural and reputational information to update an agent's standing____. These norms can be imposed top-down by a central authority or emerge bottom-up through interactions____. In the top-down approach, norms are designed offline and uniformly applied____. For instance, using Boo- lean inputs for reputation and strategy, norms can be encoded as a 4-bit string, $d=(d_{G, C},d_{G, D},d_{B, C},d_{B, D})_2$. Then, agents update reputations based on second-party reports with imperfect observations____. Since reputations are repeatedly evaluated with small errors, this process can be modelled as an ergodic Markov chain with a characterised stationary distribution____, determining the fitness of an agentâ€™s rule $\pi_i$. Previous studies indicate that the well-known norm Stern Judging (SJ) effectively drives behavioural dynamics through both imitation processes____ and Reinforcement Learning (RL)____. SJ assigns good reputations to agents who cooperate with good partners and defect against bad ones, while assigning bad reputations to those who act the opposite.

\textit{MARL and Bottom-Up Norm.} In MARL, the centralized training with decentralized execution (CTDE) framework____ lacks centralized information for top-down judgment during execution. A bottom-up approach, however, enables agents to adopt decentralized methods for assigning reputations, allowing norms to emerge organical- ly____. Recent studies explore how agents learn to assign and respond to reputations. These norms can be learned either through explicit payoff-based methods____ or by inferring the existence of shared normativity via approximate Bayesian rule induction____. Beyond reputation, the bottom-up creation of norms can also manifest as intrinsic rewards____ or public sanctions____. However, maintaining cooperation under learned norm-guided behaviour is constrained by the presence of a predefined set of norms. In this paper, we extend this approach by allowing agents to assign reputations to their interacting neighbours based solely on their own rewards, which does not require knowledge of a predefined set of norms beyond the agents' observations.