\section{Related Works}
\textit{Cooperation in MARL.} MARL in collaborative settings can be dichotomized into two branches: team-based and mixed-motive environments~\cite{du2023review}. In the former, agents coordinate actions and share a single scalar reward. Several recent advanced methods have been proposed to solve this problem~\cite{yu2022surprising,sunehag2018value,son2019qtran}. By contrast, in mixed-motive environments, agents receive individual rewards, necessitating a balance between personal maximisation and social welfare. Such systems often encounter social dilemmas~\cite{van2013psychology}, where individually rational decisions lead to collectively suboptimal outcomes. In MAS, these dilemmas extend spatially and temporally, forming sequential social dilemmas~\cite{leibo2017multi}. A core challenge is understanding how cooperation among self-interested agents can emerge and remain stable, despite threats like conflict, overconsumption, free-riding, and defection~\cite{du2023review}. Previous research has explored solutions through other-regarding preferences~\cite{mckee2021multi,hughes2018inequity}, reputation mechanisms~\cite{anastassacos2021cooperation,smit2024learning}, and anticipation of future behaviours~\cite{foerster2018learning,yang2020learning}. Notably, reputation serves as an adaptable measure of social standing that can guide subsequent agent interactions~\cite{anastassacos2020partner}. Building on this, our paper focuses on encouraging cooperation in MARL through the auxiliary learning dynamics inherent in reputation assignments.

\textit{Reputation and Social Norms.} In the IR mechanism, actions are evaluated against social norms that integrate behavioural and reputational information to update an agent's standing~\cite{nowak2005evolution}. These norms can be imposed top-down by a central authority or emerge bottom-up through interactions~\cite{savarimuthu2011norm}. In the top-down approach, norms are designed offline and uniformly applied~\cite{santos2016social}. For instance, using Boo- lean inputs for reputation and strategy, norms can be encoded as a 4-bit string, $d=(d_{G, C},d_{G, D},d_{B, C},d_{B, D})_2$. Then, agents update reputations based on second-party reports with imperfect observations~\cite{haynes2017engineering}. Since reputations are repeatedly evaluated with small errors, this process can be modelled as an ergodic Markov chain with a characterised stationary distribution~\cite{ohtsuki2004should}, determining the fitness of an agentâ€™s rule $\pi_i$. Previous studies indicate that the well-known norm Stern Judging (SJ) effectively drives behavioural dynamics through both imitation processes~\cite{santos2016social} and Reinforcement Learning (RL)~\cite{smit2024learning}. SJ assigns good reputations to agents who cooperate with good partners and defect against bad ones, while assigning bad reputations to those who act the opposite.

\textit{MARL and Bottom-Up Norm.} In MARL, the centralized training with decentralized execution (CTDE) framework~\cite{lowe2017multi} lacks centralized information for top-down judgment during execution. A bottom-up approach, however, enables agents to adopt decentralized methods for assigning reputations, allowing norms to emerge organical- ly~\cite{xu2019cooperation}. Recent studies explore how agents learn to assign and respond to reputations. These norms can be learned either through explicit payoff-based methods~\cite{xu2019cooperation} or by inferring the existence of shared normativity via approximate Bayesian rule induction~\cite{oldenburg2024learning}. Beyond reputation, the bottom-up creation of norms can also manifest as intrinsic rewards~\cite{mckee2021multi} or public sanctions~\cite{vinitsky2023learning}. However, maintaining cooperation under learned norm-guided behaviour is constrained by the presence of a predefined set of norms. In this paper, we extend this approach by allowing agents to assign reputations to their interacting neighbours based solely on their own rewards, which does not require knowledge of a predefined set of norms beyond the agents' observations.