%%%% ijcai24.tex

\typeout{IJCAI--24}

% These are the instructions for authors for IJCAI-24.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai24.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai24}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[switch]{lineno}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage{multirow} 
\usepackage{csquotes}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}

% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2024.0)
}

\title{Appendix for Bottom-Up Reputation Promotes Cooperation \\ with Multi-Agent Reinforcement Learning}


% Single author syntax
% \author{
%     Anonymous Author(s)
%     \affiliations
%     Anonymous Institution
%     \emails
%     anon.email@domain.com
% }

% % Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% % \iffalse
\author{
Tianyu Ren$^1$
\and
Xuan Yao$^2$
\and
Yang Li$^1$
\And
Xiao-Jun Zeng$^1$
\affiliations
$^1$University of Manchester, Manchester, United Kingdom\\
$^2$Southeast University, Nanjing, China
% $^2$Second Affiliation\\
% $^3$Third Affiliation\\
% $^4$Fourth Affiliation
% \emails
% \{tianyu.ren, x.zeng\}@manchester.ac.uk
% third@other.example.com,
% fourth@example.com
}
% % % \fi

\begin{document}


\maketitle


\appendix 
\section{Training Details}  \label{Appendix:implementation}
\subsection{Algorithm} 
 \label{Appendix:Algorithm}
Our proposed Learning with Reputation Reward (LR2) framework consists of two interconnected components: the Dilemma Policy ($\pi_{\theta}$) and the Evaluation Policy ($p_{\eta}$). The dilemma policy governs agents' immediate strategic choices by optimizing for environmental rewards while incorporating the influence of reputation assessments. The evaluation policy determines how agents assign reputations to their neighbours based on observed behaviours, shaping future incentives for cooperation or defection. 

This dual-policy structure allows agents to influence their neighbours indirectly via reputation-driven incentives rather than direct learning updates. Through iterative updates, LR2 establishes a feedback loop where reputation acts as a proxy for long-term rewards, promoting sustainable cooperation beyond short-term gains. The following algorithm~\ref{alg: algorithm} outlines LR2's structured workflow, clarifying the role of each equation in its logical flow.


\subsection{Idea behind LR2}  \label{Supp:Hyperparameters}
In our training framework, the environmental reward and reputation update ensure that LR2 agents' decisions reflect both immediate interactions and historical behaviour. The reward aggregates payoffs from neighbours, while the running average reputation update balances both past and recent assessments, maintaining stability and adaptability.

The dilemma policy optimizes decision-making by integrating reputation-reshaped rewards, aligning incentives with group expectations to foster cooperation. Entropy regularization prevents premature convergence, while the policy gradient update incorporates both environmental and reputation-based rewards, reinforcing long-term cooperative behaviour.

The evaluation policy enables agents to assess their neighbours' contributions by comparing individual interactions to the local group average. The evaluation reward ensures that assessments are context-aware, promoting fair reputation assignment. The penalty term encourages consistency between an agent's evaluations and those of its neighbours, enhancing alignment within the group. Lastly, the evaluation policy update captures the impact of reputation adjustments on neighbouring agents' behaviours, ensuring that reputation assignments evolve to reinforce cooperative dynamics effectively.

\begin{algorithm}[H]
    \caption{Learning with Reputation Reward (LR2)}\label{alg: algorithm}
    \begin{algorithmic}%[1] %[1] enables line numbers
    \FOR{each episode $e = 1$ to $M$} 
        \STATE Initialize policy parameters $\theta^i, \eta^i$ for all agents
        \STATE Set initial reputations $P^i_0$ for all agents
        \FOR{timestep $t=1$ to max-episode-length}
            \FOR{each agent $i$ to $N$}
            \STATE \textbf{Phase 1: Environmental Reward Computation} 
            \STATE Generate trajectories $\tau^i$ using dilemma policy $\pi_{\theta^i}$
            \STATE Compute the environmental reward $r^{i,\text{env}}_t$
            % \begin{equation*}
            %     r^{i,\text{env}}_t=\sum_{j\in \Omega^i}{a^{i}}^{\top}\mathcal{M} a^{j},
            % \end{equation*}   
            \STATE \textbf{Phase 2: Reputation Assignment} 
            \STATE  Assign reputation $p^{i,t}_{\eta^j}$ to neighbours via $\eta^i$
            \ENDFOR
            \STATE Update agents' reputation using a running average
            \STATE \textbf{Dilemma Policy Update:}
            \FOR{each agent $i$ to $N$}
            \STATE Incorporate the reputation into the reward function
            \STATE Update dilemma policy parameters $\theta^i$ via:
            \begin{equation*}
                \hat{\theta}^i\leftarrow \theta^i+\lambda \sum^T_{t=0}\left[\nabla_{\theta^i}\log \pi^i(a^i_t|o^i_t)G^i_t(\tau^i,\eta^{-i})\right],
            \end{equation*}
            \ENDFOR  
            \STATE \textbf{Evaluation  Policy Update:}
            \FOR{each agent $i$ to $N$}
            \STATE Generate trajectory $\hat{\tau}^i$ using the updated policy $\hat{\theta}^i$
            \STATE Compute evaluation reward $r^{i,\text{eval}}$
            \STATE Update evaluation policy parameters $\eta^i$ via:
            \begin{equation*}
                 \hat{\eta}^i\leftarrow\eta^i+\lambda f(\hat{\tau}^i,\tau^i,\hat{\theta},\eta^i),
            \end{equation*}
            
            \ENDFOR
            
        \ENDFOR
    \ENDFOR
    \end{algorithmic}
\end{algorithm}


\section{Additional Results}  \label{Appendix:additional}
\subsection{Impact of Interaction Structure} \label{Appendix:network}
 
Spatial reciprocity is traditionally recognized as a key mechanism for promoting cooperation by  enabling clusters of cooperative agents to emerge. However, as demonstrated in the main text, MARL agents that optimize individual rewards do not inherently form or sustain such clusters. To further evaluate the robustness of our proposed LR2 method, we extend the analysis to two additional interaction settings: alternative network structures with varying neighbourhood sizes, and well-mixed populations with different group sizes. These experiments depart from the lattice topology used in the main text and aim to demonstrate that LR2's effectiveness is not contingent on a specific interaction structure. 

Table~\ref{tab:robustness} reports the average cooperation levels achieved under six configurations: three spatial settings—the conventional square lattice ($4$ neighbours), the Moore neighbourhood ($8$ neighbours), and the Honeycomb lattice ($3$ neighbours)—and three well-mixed settings with pairwise interactions ($k=1$), small-group interactions ($k=4$), and larger groups ($k=8$). In the square lattice, cooperation decreases from $0.82$ at $T=1.30$ to $0.15$ at $T=1.37$. In the Moore neighbourhood, cooperation is nearly abolished across all $T$ values, whereas the Honeycomb lattice supports higher cooperation ($0.97$ at $T=1.30$ and $0.65$ at $T=1.37$). In well-mixed populations, LR2 maintains near-optimal cooperation in pairwise ($k=1$) and small-group ($k=4$) settings (approximately $0.98–0.99$ across all $T$ values), but cooperation collapses when agents interact with $8$ opponents per round ($k=8$).

\begin{table}[h]
  \caption{Average cooperation levels for LR2 under different interaction configurations.}
  \centering
  \resizebox{0.49\textwidth}{!}{
 %\setlength{\tabcolsep}{1.8pt} {
    \begin{tabular}{lcccc}
        \toprule
       Configuration & \textbf{$T=1.30$} & \textbf{$T=1.33$} & \textbf{$T=1.35$} & \textbf{$T=1.37$}\\
        %\cmidrule(r){1-7} 
        \midrule
        Lattice& $0.82$ & $0.55$ & $0.33$ & $0.15$  \\
        Moore & $0.01$ & $0.00$ & $0.00$ & $0.00$  \\
        Honeycomb & $0.97$ & $0.80$ & $0.74$ & $0.65$   \\
        Well-mixed(1) & $0.99$ & $0.99$ & $0.99$ & $0.99$ \\
        Well-mixed(4) & $0.99$ & $0.98$ & $0.98$ & $0.98$ \\
        Well-mixed(8) & $0.00$ & $0.00$ & $0.00$ & $0.00$ \\
        \bottomrule
    \end{tabular}
  }
  \label{tab:robustness}
\end{table}

These results underscore the critical role of interaction structure in the evolution of cooperation within MARL settings. LR2 effectively promotes cooperation in environments that facilitate clustering and in well-mixed settings with limited interactions. However, its performance deteriorates in settings with extensive neighbourhood sizes or large well-mixed groups. This analysis reinforces the conclusion that, in the absence of mechanisms to sustain localized interactions, reputation-based reward reshaping may be insufficient to overcome the challenges posed by diluted or overly extensive opponent sets.

\subsection{Additional Hyperparameter Analysis} \label{Appendix:hyperparameter}
We next present supplementary results examining several hyperparameters of the LR2 method. Specifically, we analyse: (i) the sensitivity of the $\beta$ parameter, which modulates agent selfishness; (ii) the impact of entropy weight $\omega$ scheduling on exploration and convergence; and (iii) the robustness of LR2 in the presence of adversarial agents. 

Table~\ref{tab:beta} summarizes the average cooperation levels for different temptation values ($T$) under varying $\beta$ settings. Our experiments indicate that smaller $\beta$ values, which correspond to less selfish behaviour, promote higher levels of cooperation and accelerate learning. Notably, $\beta$ values of $0.5$ yield substantially higher cooperation compared to higher values, particularly under lower temptation scenarios.

\begin{table}[ht] 
    \caption{Sensitivity analysis of $\beta$: Average cooperation levels under different temptation values.} 
    \centering
    \begin{tabular}{lccc} 
    \toprule 
    $T$ & $\beta=0.5$ & $\beta=0.6$ & $\beta=0.7$ \\ 
    \midrule 
    $1.30$ & $0.98$ & $0.82$ & $0.15$ \\ 
    $1.33$ & $0.98$ & $0.55$ & $0.04$ \\ 
    $1.35$ & $0.98$ & $0.33$ & $0.00$ \\ 
    $1.37$ & $0.92$ & $0.15$ & $0.00$ \\ 
    \bottomrule 
    \end{tabular} 
\label{tab:beta} 
\end{table}

In the main text, an annealing schedule is employed to gradually reduce the entropy weight, thereby balancing exploration and convergence. To assess this approach, we compared the annealing schedule against fixed entropy weights ($\omega=0.1$ and $\omega=0.0$). As reported in Table~\ref{tab:entropy}, while a fixed entropy weight of $\omega=0.1$ facilitates exploration, it is insufficient to guide agents toward cooperation under stronger dilemma conditions. In contrast, the annealing strategy supports both adequate exploration in early training and convergence to effective cooperative policies.

\begin{table}[ht] 
    \caption{Impact of entropy weight scheduling on cooperation.} 
    \centering 
    \begin{tabular}{lccc} 
    \toprule $T$ & Annealing & Fixed ($\omega=0.1$) & Fixed ($\omega=0.0$) \\
    \midrule $1.30$ & $0.82$ & $0.80$ & $0.16$ \\
    $1.33$ & $0.55$ & $0.37$ & $0.00$ \\
    $1.35$ & $0.33$ & $0.20$ & $0.00$ \\
    $1.37$ & $0.15$ & $0.16$ & $0.00$ \\ 
    \bottomrule
    \end{tabular} 
\label{tab:entropy}
\end{table}

To evaluate LR2's robustness, we introduced adversarial agents that prioritize environmental rewards over reputation-based intrinsic rewards. In our framework, reputation functions as an intrinsic reward that is shaped and assigned based on neighbouring agents' observations. LR2 guides agents not only to learn how to assign reputation effectively, thereby influencing the behaviour of others but also to increase their own rewards by correctly assigning reputation to steer neighbours' future actions beneficially. When adversarial agents disregard this reputation mechanism in favour of solely environmental rewards, the cooperative signal provided by reputation assignment is disrupted.

\begin{table}[ht] 
    \caption{Average cooperation levels with varying proportions of adversarial agents.} 
    \centering 
    \begin{tabular}{lccc} 
    \toprule $T$ & $100\%$ (LR2) & $90\%$ (LR2) & $70\%$ (LR2) \\ 
    \midrule $1.30$ & $0.82$ & $0.35$ & $0.00$ \\ 
    $1.33$ & $0.55$ & $0.21$ & $0.00$ \\ 
    $1.35$ & $0.33$ & $0.00$ & $0.00$ \\ 
    $1.37$ & $0.15$ & $0.00$ & $0.00$ \\ 
    \bottomrule 
    \end{tabular} 
\label{tab:adversarial} 
\end{table}

Table~\ref{tab:adversarial} presents the cooperation levels when varying the proportion of cooperative LR2 agents. A $100\%$ ratio corresponds to all agents following the LR2 strategy, whereas a $90\%$ ratio indicates that $10\%$ of agents behave adversarially. The results show that even a modest intrusion (i.e., $90\%$ cooperative agents) significantly diminishes overall cooperation, and a further reduction to $70\%$ cooperative agents leads to a complete collapse of cooperative behaviour across all temptation values. It reveals that the reputation mechanism, which not only serves as an intrinsic reward but also allows agents to influence neighbours' future behaviours through accurate reputation assignment, is vulnerable to adversarial behaviour.


%% The file named.bst is a bibliography style file for BibTeX 0.99c
% \bibliographystyle{named}
% \bibliography{SI_reference}

\subsection{Benchmarks on Predefined Norms} \label{Appendix:predefined}
To further assess the efficacy of our LR2 method, we incorporate four benchmarks based on predefined social norms for evaluating neighbour behaviour and assigning reputation in a MARL setting. These norms include: Stern Judging (SJ), which assigns a good reputation to a donor who helps a good recipient or refuses help to a bad one, and a bad reputation otherwise. Simple-standing (SS), similar to SJ but more benevolent, SS grants a good reputation to any donor who cooperates, regardless of the recipient's status. Shunning (SH), which is less forgiving than SJ, SH assigns a bad reputation to any donor who defects. Image Score (IS) is a first-order norm where the donor’s action alone determines reputation; cooperation yields a good reputation, while defection results in a bad reputation.

\begin{table}[ht] 
    \caption{Average cooperation levels across different predefined social norms under a MARL setting.} 
    \centering 
    \begin{tabular}{lcccc} 
    \toprule 
    Norm & \textbf{$T=1.30$} & \textbf{$T=1.33$} & \textbf{$T=1.35$} & \textbf{$T=1.37$} \\
    \midrule 
    SJ & $0.00$ & $0.00$ & $0.00$ & $0.00$ \\
    SS & $0.52$ & $0.47$ & $0.42$ & $0.37$ \\
    SH & $0.03$ & $0.00$ & $0.00$ & $0.00$ \\
    IS & $0.80$ & $0.53$ & $0.28$ & $0.11$ \\
    \bottomrule 
    \end{tabular} 
\label{tab:predefined} 
\end{table}

As shown in Table~\ref{tab:predefined}, cooperation levels under these predefined norms are generally lower than those achieved with the LR2 method, particularly under stronger dilemma conditions. Notably, SJ and SH yield near-zero cooperation, while SS and IS offer only moderate improvements. In contrast, LR2 consistently outperforms these benchmarks by learning to assign reputations in a decentralized manner, thereby maximizing individual rewards through effective influence on neighbours' future behaviours and enhancing overall system robustness and adaptability. Moreover, unlike rule-based norms—which require manual design and extensive domain knowledge—our proposed LR2 automatically adapts its reputation assignment to the environment, improving generalization across diverse tasks.

\end{document}

