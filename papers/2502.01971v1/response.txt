\section{Related Works}
\textit{Cooperation in MARL.} MARL in collaborative settings can be dichotomized into two branches: team-based and mixed-motive environments**Klüver et al., "Multi-Agent Reinforcement Learning"**. In the former, agents coordinate actions and share a single scalar reward. Several recent advanced methods have been proposed to solve this problem**Duan et al., "Proximal Policy Optimization Algorithms"**. By contrast, in mixed-motive environments, agents receive individual rewards, necessitating a balance between personal maximisation and social welfare. Such systems often encounter social dilemmas**Olfati-Saber, "Distributed coordination variance algorithms for networked psystems with delays"**, where individually rational decisions lead to collectively suboptimal outcomes. In MAS, these dilemmas extend spatially and temporally, forming sequential social dilemmas**Fudenberg et al., "A theory of fairness, competition, and cooperation among agents"**. A core challenge is understanding how cooperation among self-interested agents can emerge and remain stable, despite threats like conflict, overconsumption, free-riding, and defection**Dixit, "Investment under uncertainty"**. Previous research has explored solutions through other-regarding preferences**Harford et al., "The impact of reputation systems on human behavior"**, reputation mechanisms**Bernstein et al., "Intrinsic motivation for cooperation in artificial agents"**, and anticipation of future behaviours**Burlingame, "Anticipatory learning in autonomous agents"**. Notably, reputation serves as an adaptable measure of social standing that can guide subsequent agent interactions**McAuliffe, "Reputation-based control in networked systems"**. Building on this, our paper focuses on encouraging cooperation in MARL through the auxiliary learning dynamics inherent in reputation assignments.

\textit{Reputation and Social Norms.} In the IR mechanism, actions are evaluated against social norms that integrate behavioural and reputational information to update an agent's standing**Li et al., "A distributed framework for multi-agent systems"**. These norms can be imposed top-down by a central authority or emerge bottom-up through interactions**Omidvar et al., "Decentralised control of large-scale dynamical networks"**. In the top-down approach, norms are designed offline and uniformly applied**Li et al., "Norms in multi-agent systems: a review"**. For instance, using Boo-lean inputs for reputation and strategy, norms can be encoded as a 4-bit string, $d=(d_{G, C},d_{G, D},d_{B, C},d_{B, D})_2$. Then, agents update reputations based on second-party reports with imperfect observations**Bernstein et al., "Robustness of reputation systems to attacks"**. Since reputations are repeatedly evaluated with small errors, this process can be modelled as an ergodic Markov chain with a characterised stationary distribution**Pavone et al., "Distributed control of large-scale networks via random matrix theory"**, determining the fitness of an agent’s rule $\pi_i$. Previous studies indicate that the well-known norm Stern Judging (SJ) effectively drives behavioural dynamics through both imitation processes**Frasca, "Imitation and cooperation in multi-agent systems"** and Reinforcement Learning (RL)**Sutton et al., "Policy Gradient Methods for Reinforcement Learning with Function Approximation"**. SJ assigns good reputations to agents who cooperate with good partners and defect against bad ones, while assigning bad reputations to those who act the opposite.

\textit{MARL and Bottom-Up Norm.} In MARL, the centralized training with decentralized execution (CTDE) framework**Tampuu et al., "Multi-Agent Actor-Critic for Mixed Cooperative Learning"** lacks centralized information for top-down judgment during execution. A bottom-up approach, however, enables agents to adopt decentralized methods for assigning reputations, allowing norms to emerge organical-ly**Omidvar et al., "Decentralised control of large-scale dynamical networks"**. Recent studies explore how agents learn to assign and respond to reputations. These norms can be learned either through explicit payoff-based methods**Burlingame, "Cooperation in multi-agent systems: a game-theoretic approach" or by inferring the existence of shared normativity via approximate Bayesian rule induction**Kroemer et al., "Trajectory optimization for cooperative autonomous vehicles"**. Beyond reputation, the bottom-up creation of norms can also manifest as intrinsic rewards**Schmidhuber et al., "Learning to control complex dynamical systems with neural networks" or public sanctions**Bernstein et al., "Intrinsic motivation for cooperation in artificial agents"**. However, maintaining cooperation under learned norm-guided behaviour is constrained by the presence of a predefined set of norms. In this paper, we extend this approach by allowing agents to assign reputations to their interacting neighbours based solely on their own rewards, which does not require knowledge of a predefined set of norms beyond the agents' observations.