@misc{proceedings,
  author = {{IJCAI Proceedings}},
  title = {{IJCAI} Camera Ready Submission},
  howpublished = {\url{https://proceedings.ijcai.org/info}},
}

@INPROCEEDINGS{9578520,
  author={Yang, Mengyue and Liu, Furui and Chen, Zhitang and Shen, Xinwei and Hao, Jianye and Wang, Jun},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={CausalVAE: Disentangled Representation Learning via Neural Structural Causal Models}, 
  year={2021},
  volume={},
  number={},
  pages={9588-9597},
  keywords={Analytical models;Directed acyclic graph;Computer vision;Semantics;Transforms;Benchmark testing;Data models},
  doi={10.1109/CVPR46437.2021.00947}}

@inproceedings{
xia2023neural,
title={Neural Causal Models for Counterfactual Identification and Estimation},
author={Kevin Muyuan Xia and Yushu Pan and Elias Bareinboim},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=vouQcZS8KfW}
}

@inproceedings{10.5555/3540261.3541089,
author = {Xia, Kevin and Lee, Kai-Zhan and Bengio, Yoshua and Bareinboim, Elias},
title = {The causal-neural connection: expressiveness, learnability, and inference},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {One of the central elements of any causal inference is an object called structural causal model (SCM), which represents a collection of mechanisms and exogenous sources of random variation of the system under investigation (Pearl, 2000). An important property of many kinds of neural networks is universal approximability: the ability to approximate any function to arbitrary precision. Given this property, one may be tempted to surmise that a collection of neural nets is capable of learning any SCM by training on data generated by that SCM. In this paper, we show this is not the case by disentangling the notions of expressivity and learnability. Specifically, we show that the causal hierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits of what can be learned from data, still holds for neural models. For instance, an arbitrarily complex and expressive neural net is unable to predict the effects of interventions given observational data alone. Given this result, we introduce a special type of SCM called a neural causal model (NCM), and formalize a new type of inductive bias to encode structural constraints necessary for performing causal inferences. Building on this new class of models, we focus on solving two canonical tasks found in the literature known as causal identification and estimation. Leveraging the neural toolbox, we develop an algorithm that is both sufficient and necessary to determine whether a causal effect can be learned from data (i.e., causal identifiability); it then estimates the effect whenever identifiability holds (causal estimation). Simulations corroborate the proposed approach.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {828},
numpages = {14},
series = {NIPS '21}
}

@inproceedings{10.5555/3618408.3619478,
author = {Nasr-Esfahany, Arash and Alizadeh, Mohammad and Shah, Devavrat},
title = {Counterfactual identifiability of bijective causal models},
year = {2023},
publisher = {JMLR.org},
abstract = {We study counterfactual identifiability in causal models with bijective generation mechanisms (BGM), a class that generalizes several widely-used causal models in the literature. We establish their counterfactual identifiability for three common causal structures with unobserved confounding, and propose a practical learning method that casts learning a BGM as structured generative modeling. Learned BGMs enable efficient counterfactual estimation and can be obtained using a variety of deep conditional generative models. We evaluate our techniques in a visual task and demonstrate its application in a real-world video streaming simulation task.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1070},
numpages = {22},
location = {, Honolulu, Hawaii, USA, },
series = {ICML'23}
}

@misc{liu2021wasserstein,
      title={Wasserstein Generative Learning of Conditional Distribution}, 
      author={Shiao Liu and Xingyu Zhou and Yuling Jiao and Jian Huang},
      year={2021},
      eprint={2112.10039},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{pmlr-v125-kidger20a,
  title = 	 {{Universal Approximation with Deep Narrow Networks}},
  author =       {Kidger, Patrick and Lyons, Terry},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {2306--2327},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v125/kidger20a/kidger20a.pdf},
  url = 	 {https://proceedings.mlr.press/v125/kidger20a.html},
  abstract = 	 { The classical Universal Approximation Theorem holds for neural networks of arbitrary width and bounded depth. Here we consider the natural ‘dual’ scenario for networks of bounded width and arbitrary depth. Precisely, let $n$ be the number of inputs neurons, $m$ be the number of output neurons, and let $\rho$ be any nonaffine continuous function, with a continuous nonzero derivative at some point. Then we show that the class of neural networks of arbitrary depth, width $n + m + 2$, and activation function $\rho$, is dense in $C(K; \mathbb{R}^m)$ for $K \subseteq \mathbb{R}^n$ with $K$ compact. This covers every activation function possible to use in practice, and also includes polynomial activation functions, which is unlike the classical version of the theorem, and provides a qualitative difference between deep narrow networks and shallow wide networks. We then consider several extensions of this result. In particular we consider nowhere differentiable activation functions, density in noncompact domains with respect to the $L^p$-norm, and how the width may be reduced to just $n + m + 1$ for ‘most’ activation functions.}
}

@inproceedings{10.24963/ijcai.2024/907,
author = {Poinsot, Audrey and Leite, Alessandro and Chesneau, Nicolas and S\'{e}bag, Mich\`{e}le and Schoenauer, Marc},
title = {Learning structural causal models through deep generative models: methods, guarantees, and challenges},
year = {2024},
isbn = {978-1-956792-04-1},
url = {https://doi.org/10.24963/ijcai.2024/907},
doi = {10.24963/ijcai.2024/907},
abstract = {This paper provides a comprehensive review of deep structural causal models (DSCMs), particularly focusing on their ability to answer counterfactual queries using observational data within known causal structures. It delves into the characteristics of DSCMs by analyzing the hypotheses, guarantees, and applications inherent to the underlying deep learning components and structural causal models, fostering a finer understanding of their capabilities and limitations in addressing different counterfactual queries. Furthermore, it highlights the challenges and open questions in the field of deep structural causal modeling. It sets the stages for researchers to identify future work directions and for practitioners to get an overview in order to find out the most appropriate methods for their needs.},
booktitle = {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence},
articleno = {907},
numpages = {9},
location = {Jeju, Korea},
series = {IJCAI '24}
}

@InProceedings{pmlr-v238-manupriya24a,
  title = 	 { Consistent Optimal Transport with Empirical Conditional Measures },
  author =       {Manupriya, Piyushi and Das, Rachit K. and Biswas, Sayantan and N Jagarlapudi, SakethaNath},
  booktitle = 	 {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3646--3654},
  year = 	 {2024},
  editor = 	 {Dasgupta, Sanjoy and Mandt, Stephan and Li, Yingzhen},
  volume = 	 {238},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--04 May},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v238/manupriya24a/manupriya24a.pdf},
  url = 	 {https://proceedings.mlr.press/v238/manupriya24a.html},
  abstract = 	 { Given samples from two joint distributions, we consider the problem of Optimal Transportation (OT) between them when conditioned on a common variable. We focus on the general setting where the conditioned variable may be continuous, and the marginals of this variable in the two joint distributions may not be the same. In such settings, standard OT variants cannot be employed, and novel estimation techniques are necessary. Since the main challenge is that the conditional distributions are not explicitly available, the key idea in our OT formulation is to employ kernelized-least-squares terms computed over the joint samples, which implicitly match the transport plan’s marginals with the empirical conditionals. Under mild conditions, we prove that our estimated transport plans, as a function of the conditioned variable, are asymptotically optimal. For finite samples, we show that the deviation in terms of our regularized objective is bounded by $O(m^{-1/4})$, where $m$ is the number of samples. We also discuss how the conditional transport plan could be modelled using explicit probabilistic models as well as using implicit generative ones. We empirically verify the consistency of our estimator on synthetic datasets, where the optimal plan is analytically known. When employed in applications like prompt learning for few-shot classification and conditional-generation in the context of predicting cell responses to treatment, our methodology improves upon state-of-the-art methods. }
}

@inproceedings{NEURIPS2021_5989add1,
 author = {Xia, Kevin and Lee, Kai-Zhan and Bengio, Yoshua and Bareinboim, Elias},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {10823--10836},
 publisher = {Curran Associates, Inc.},
 title = {The Causal-Neural Connection: Expressiveness, Learnability, and Inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/5989add1703e4b0480f75e2390739f34-Paper.pdf},
 volume = {34},
 year = {2021}
}
@article{10.1145/3616865,
author = {Caton, Simon and Haas, Christian},
title = {Fairness in Machine Learning: A Survey},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3616865},
doi = {10.1145/3616865},
abstract = {When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organizes approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarizing open challenges articulated as five dilemmas for fairness research.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {166},
numpages = {38},
keywords = {Fairness, accountability, transparency, machine learning}
}

@inproceedings{10.5555/3294996.3295162,
author = {Kusner, Matt and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
title = {Counterfactual fairness},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4069–4079},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@article{10.1007/s10994-022-06206-8,
author = {Grari, Vincent and Lamprier, Sylvain and Detyniecki, Marcin},
title = {Adversarial learning for counterfactual fairness},
year = {2022},
issue_date = {Mar 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {112},
number = {3},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-022-06206-8},
doi = {10.1007/s10994-022-06206-8},
abstract = {In recent years, fairness has become an important topic in the machine learning research community. In particular, counterfactual fairness aims at building prediction models which ensure fairness at the most individual level. Rather than globally considering equity over the entire population, the idea is to imagine what any individual would look like with a variation of a given attribute of interest, such as a different gender or race for instance. Existing approaches rely on Variational Auto-encoding of individuals, using Maximum Mean Discrepancy (MMD) penalization to limit the statistical dependence of inferred representations with their corresponding sensitive attributes. This enables the simulation of counterfactual samples used for training the target fair model, the goal being to produce similar outcomes for every alternate version of any individual. In this work, we propose to rely on an adversarial neural learning approach, that enables more powerful inference than with MMD penalties, and is particularly better fitted for the continuous setting, where values of sensitive attributes cannot be exhaustively enumerated. Experiments show significant improvements in term of counterfactual fairness for both the discrete and the continuous settings.},
journal = {Mach. Learn.},
month = {aug},
pages = {741–763},
numpages = {23},
keywords = {Counterfactual fairness, Adversarial neural network, Causal inference}
}

@article{JMLR:v12:sriperumbudur11a,
  author  = {Bharath K. Sriperumbudur and Kenji Fukumizu and Gert R.G. Lanckriet},
  title   = {Universality, Characteristic Kernels and RKHS Embedding of Measures},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {70},
  pages   = {2389--2410},
  url     = {http://jmlr.org/papers/v12/sriperumbudur11a.html}
}

@article{https://doi.org/10.1111/cogs.12065,
author = {Pearl, Judea},
title = {Structural Counterfactuals: A Brief Introduction},
journal = {Cognitive Science},
volume = {37},
number = {6},
pages = {977-985},
keywords = {Causal reasoning, Counterfactuals, Structural models},
doi = {https://doi.org/10.1111/cogs.12065},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12065},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12065},
abstract = {Abstract Recent advances in causal reasoning have given rise to a computational model that emulates the process by which humans generate, evaluate, and distinguish counterfactual sentences. Contrasted with the “possible worlds” account of counterfactuals, this “structural” model enjoys the advantages of representational economy, algorithmic simplicity, and conceptual clarity. This introduction traces the emergence of the structural model and gives a panoramic view of several applications where counterfactual reasoning has benefited problem areas in the empirical sciences.},
year = {2013}
}

@article{JMLR:v25:21-1440,
  author  = {Lucas De Lara and Alberto Gonz{{\'a}}lez-Sanz and Nicholas Asher and Laurent Risser and Jean-Michel Loubes},
  title   = {Transport-based Counterfactual Models},
  journal = {Journal of Machine Learning Research},
  year    = {2024},
  volume  = {25},
  number  = {136},
  pages   = {1--59},
  url     = {http://jmlr.org/papers/v25/21-1440.html}
}

@inproceedings{NEURIPS2020_0987b8b3,
 author = {Pawlowski, Nick and Coelho de Castro, Daniel and Glocker, Ben},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {857--869},
 publisher = {Curran Associates, Inc.},
 title = {Deep Structural Causal Models for Tractable Counterfactual Inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/0987b8b338d6c90bbedd8631bc499221-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{Xia_Bareinboim_2024, title={Neural Causal Abstractions}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/30044}, DOI={10.1609/aaai.v38i18.30044}, abstractNote={The ability of humans to understand the world in terms of cause and effect relationships, as well as their ability to compress information into abstract concepts, are two hallmark features of human intelligence. These two topics have been studied in tandem under the theory of causal abstractions, but it is an open problem how to best leverage abstraction theory in real-world causal inference tasks, where the true model is not known, and limited data is available in most practical settings. In this paper, we focus on a family of causal abstractions constructed by clustering variables and their domains, redefining abstractions to be amenable to individual causal distributions. We show that such abstractions can be learned in practice using Neural Causal Models, allowing us to utilize the deep learning toolkit to solve causal tasks (identification, estimation, sampling) at different levels of abstraction granularity. Finally, we show how representation learning can be used to learn abstractions, which we apply in our experiments to scale causal inferences to high dimensional settings such as with image data.}, number={18}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Xia, Kevin and Bareinboim, Elias}, year={2024}, month={Mar.}, pages={20585-20595} }

@inproceedings{NEURIPS2023_65cbe3e2,
 author = {Melnychuk, Valentyn and Frauen, Dennis and Feuerriegel, Stefan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {32020--32060},
 publisher = {Curran Associates, Inc.},
 title = {Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/65cbe3e21ac62553111d9ecf7d60c18e-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{
liu2024identifiable,
title={Identifiable Latent Polynomial Causal Models through the Lens of Change},
author={Yuhang Liu and Zhen Zhang and Dong Gong and Mingming Gong and Biwei Huang and Anton van den Hengel and Kun Zhang and Javen Qinfeng Shi},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=ia9fKO1Vjq}
}

@article{probCause,
author = {Identification, Their and Pearl, Judea},
year = {2000},
month = {02},
pages = {},
title = {Probabilities Of Causation: Three Counterfactual Interpretations And Their Identification},
volume = {121},
journal = {Synthese},
doi = {10.1023/A:1005233831499}
}

@article{Wachter2017CounterfactualEW,
  title={Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR},
  author={Sandra Wachter and Brent Daniel Mittelstadt and Chris Russell},
  journal={Cybersecurity},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:3995299}
}

@inproceedings{10.1145/3580305.3599408,
author = {Ma, Jing and Guo, Ruocheng and Zhang, Aidong and Li, Jundong},
title = {Learning for Counterfactual Fairness from Observational Data},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599408},
doi = {10.1145/3580305.3599408},
abstract = {Fairness-aware machine learning has attracted a surge of attention in many domains, such as online advertising, personalized recommendation, and social media analysis in web applications. Fairness-aware machine learning aims to eliminate biases of learning models against certain subgroups described by certain protected (sensitive) attributes such as race, gender, and age. Among many existing fairness notions, counterfactual fairness is a popular notion defined from a causal perspective. It measures the fairness of a predictor by comparing the prediction of each individual in the original world and that in the counterfactual worlds in which the value of the sensitive attribute is modified. A prerequisite for existing methods to achieve counterfactual fairness is the prior human knowledge of the causal model for the data. However, in real-world scenarios, the underlying causal model is often unknown, and acquiring such human knowledge could be very difficult. In these scenarios, it is risky to directly trust the causal models obtained from information sources with unknown reliability and even causal discovery methods, as incorrect causal models can consequently bring biases to the predictor and lead to unfair predictions. In this work, we address the problem of counterfactually fair prediction from observational data without given causal models by proposing a novel framework CLAIRE. Specifically, under certain general assumptions, CLAIRE effectively mitigates the biases from the sensitive attribute with a representation learning framework based on counterfactual data augmentation and an invariant penalty. Experiments conducted on both synthetic and real-world datasets validate the superiority of CLAIRE in both counterfactual fairness and prediction performance.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1620–1630},
numpages = {11},
keywords = {causal model, counterfactual fairness, sensitive attributes},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{
zuo2022counterfactual,
title={Counterfactual Fairness with Partially Known Causal Graph},
author={Aoqi Zuo and Susan Wei and Tongliang Liu and Bo Han and Kun Zhang and Mingming Gong},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=9aLbntHz1Uq}
}

@inproceedings{
anthis2023causal,
title={Causal Context Connects Counterfactual Fairness to Robust Prediction and Group Fairness},
author={Jacy Reese Anthis and Victor Veitch},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=AmwgBjXqc3}
}

@article{judeaclassic,
author = {Neuberg, Leland},
year = {2003},
month = {08},
pages = {675-685},
title = {Causality: models, reasoning, and inference, by judea pearl, cambridge university press, 2000},
volume = {19},
journal = {Econometric Theory},
doi = {10.1017/S0266466603004109}
}

@article{
chao2024modeling,
title={Modeling Causal Mechanisms with Diffusion Models for Interventional and Counterfactual Queries},
author={Patrick Chao and Patrick Bl{\"o}baum and Sapan Kirit Patel and Shiva Kasiviswanathan},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=EDHQDsqiSe},
note={}
}

@misc{torous2024optimaltransportapproachestimating,
      title={An Optimal Transport Approach to Estimating Causal Effects via Nonlinear Difference-in-Differences}, 
      author={William Torous and Florian Gunsilius and Philippe Rigollet},
      year={2024},
      eprint={2108.05858},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2108.05858}, 
}

@article{causalMLhealth,
author = {Sanchez, Pedro and Voisey, Jeremy and Xia, Tian and Watson, Hannah and O’Neil, Alison and Tsaftaris, Sotirios},
year = {2022},
month = {08},
pages = {},
title = {Causal machine learning for healthcare and precision medicine},
volume = {9},
journal = {Royal Society Open Science},
doi = {10.1098/rsos.220638}
}

@article{causalMLhealthSurvey,
author = {An, Angela and Rahman, Md. Saifur and Zhou, Jingwen and Kang, James Jin},
year = {2023},
month = {04},
pages = {4178},
title = {A Comprehensive Review on Machine Learning in Healthcare Industry: Classification, Restrictions, Opportunities and Challenges},
volume = {23},
journal = {Sensors},
doi = {10.3390/s23094178}
}

@article{causalMLEcon,
author = {Lechner, Michael},
year = {2023},
month = {05},
pages = {},
title = {Causal Machine Learning and its use for public policy},
volume = {159},
journal = {Swiss Journal of Economics and Statistics},
doi = {10.1186/s41937-023-00113-y}
}

@article{causalMLClimate,
author = {Nowack, Peer and Runge, Jakob and Eyring, Veronika and Haigh, Joanna},
year = {2020},
month = {03},
pages = {1415},
title = {Causal networks for climate model evaluation and constrained projections},
volume = {11},
journal = {Nature Communications},
doi = {10.1038/s41467-020-15195-y}
}

@article{10.1145/3485128,
author = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and Luccioni, Alexandra Sasha and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Kording, Konrad P. and Gomes, Carla P. and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
title = {Tackling Climate Change with Machine Learning},
year = {2022},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3485128},
doi = {10.1145/3485128},
abstract = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning (ML) experts, may wonder how we can help. Here we describe how ML can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by ML, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the ML community to join the global effort against climate change.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {42},
numpages = {96},
keywords = {Climate change, mitigation, adaptation, machine learning, artificial intelligence}
}

@inbook{10.1145/3501714.3501755,
author = {Sch\"{o}lkopf, Bernhard},
title = {Causality for Machine Learning},
year = {2022},
isbn = {9781450395861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3501714.3501755},
booktitle = {Probabilistic and Causal Inference: The Works of Judea Pearl},
pages = {765–804},
numpages = {40}
}

@article{Muandet_2017,
   title={Kernel Mean Embedding of Distributions: A Review and Beyond},
   volume={10},
   ISSN={1935-8245},
   url={http://dx.doi.org/10.1561/2200000060},
   DOI={10.1561/2200000060},
   number={1–2},
   journal={Foundations and Trends® in Machine Learning},
   publisher={Now Publishers},
   author={Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Schölkopf, Bernhard},
   year={2017},
   pages={1–141} }

@inproceedings{
zhou2024counterfactual,
title={Counterfactual Fairness by Combining Factual and Counterfactual Predictions},
author={Zeyu Zhou and Tianci Liu and Ruqi Bai and Jing Gao and Murat Kocaoglu and David I. Inouye},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=J0Itri0UiN}
}

@inproceedings{10.5555/3367032.3367236,
author = {Wu, Yongkai and Zhang, Lu and Wu, Xintao},
title = {Counterfactual fairness: unidentification, bound and algorithm},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {Fairness-aware learning studies the problem of building machine learning models that are subject to fairness requirements. Counterfactual fairness is a notion of fairness derived from Pearl's causal model, which considers a model is fair if for a particular individual or group its prediction in the real world is the same as that in the counterfactual world where the individual(s) had belonged to a different demographic group. However, an inherent limitation of counterfactual fairness is that it cannot be uniquely quantified from the observational data in certain situations, due to the unidentifiability of the counterfactual quantity. In this paper, we address this limitation by mathematically bounding the unidentifiable counterfactual quantity, and develop a theoretically sound algorithm for constructing counterfactually fair classifiers. We evaluate our method in the experiments using both synthetic and real-world datasets, as well as compare with existing methods. The results validate our theory and show the effectiveness of our method.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {1438–1444},
numpages = {7},
location = {Macao, China},
series = {IJCAI'19}
}

@inproceedings{10.24963/ijcai.2024/504,
author = {Lin, Yujie and Zhao, Chen and Shao, Minglai and Meng, Baoluo and Zhao, Xujiang and Chen, Haifeng},
title = {Towards counterfactual fairness-aware domain generalization in changing environments},
year = {2024},
isbn = {978-1-956792-04-1},
url = {https://doi.org/10.24963/ijcai.2024/504},
doi = {10.24963/ijcai.2024/504},
abstract = {Recognizing domain generalization as a commonplace challenge in machine learning, data distribution might progressively evolve across a continuum of sequential domains in practical scenarios. While current methodologies primarily concentrate on bolstering model effectiveness within these new domains, they tend to neglect issues of fairness throughout the learning process. In response, we propose an innovative framework known as Disentanglement for Counterfactual Fairness-aware Domain Generalization (DCFDG). This approach adeptly removes domain-specific information and sensitive information from the embedded representation of classification features. To scrutinize the intricate interplay between semantic information, domain-specific information, and sensitive attributes, we systematically partition the exogenous factors into four latent variables. By incorporating fairness regularization, we utilize semantic information exclusively for classification purposes. Empirical validation on synthetic and authentic datasets substantiates the efficacy of our approach, demonstrating elevated accuracy levels while ensuring the preservation of fairness amidst the evolving landscape of continuous domains.},
booktitle = {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence},
articleno = {504},
numpages = {9},
location = {Jeju, Korea},
series = {IJCAI '24}
}

@misc{communities_and_crime_183,
  author       = {Redmond, Michael},
  title        = {{Communities and Crime}},
  year         = {2002},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C53W3X}
}