@article{10.1007/s10994-022-06206-8,
author = {Grari, Vincent and Lamprier, Sylvain and Detyniecki, Marcin},
title = {Adversarial learning for counterfactual fairness},
year = {2022},
issue_date = {Mar 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {112},
number = {3},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-022-06206-8},
doi = {10.1007/s10994-022-06206-8},
abstract = {In recent years, fairness has become an important topic in the machine learning research community. In particular, counterfactual fairness aims at building prediction models which ensure fairness at the most individual level. Rather than globally considering equity over the entire population, the idea is to imagine what any individual would look like with a variation of a given attribute of interest, such as a different gender or race for instance. Existing approaches rely on Variational Auto-encoding of individuals, using Maximum Mean Discrepancy (MMD) penalization to limit the statistical dependence of inferred representations with their corresponding sensitive attributes. This enables the simulation of counterfactual samples used for training the target fair model, the goal being to produce similar outcomes for every alternate version of any individual. In this work, we propose to rely on an adversarial neural learning approach, that enables more powerful inference than with MMD penalties, and is particularly better fitted for the continuous setting, where values of sensitive attributes cannot be exhaustively enumerated. Experiments show significant improvements in term of counterfactual fairness for both the discrete and the continuous settings.},
journal = {Mach. Learn.},
month = {aug},
pages = {741–763},
numpages = {23},
keywords = {Counterfactual fairness, Adversarial neural network, Causal inference}
}

@inproceedings{10.1145/3580305.3599408,
author = {Ma, Jing and Guo, Ruocheng and Zhang, Aidong and Li, Jundong},
title = {Learning for Counterfactual Fairness from Observational Data},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599408},
doi = {10.1145/3580305.3599408},
abstract = {Fairness-aware machine learning has attracted a surge of attention in many domains, such as online advertising, personalized recommendation, and social media analysis in web applications. Fairness-aware machine learning aims to eliminate biases of learning models against certain subgroups described by certain protected (sensitive) attributes such as race, gender, and age. Among many existing fairness notions, counterfactual fairness is a popular notion defined from a causal perspective. It measures the fairness of a predictor by comparing the prediction of each individual in the original world and that in the counterfactual worlds in which the value of the sensitive attribute is modified. A prerequisite for existing methods to achieve counterfactual fairness is the prior human knowledge of the causal model for the data. However, in real-world scenarios, the underlying causal model is often unknown, and acquiring such human knowledge could be very difficult. In these scenarios, it is risky to directly trust the causal models obtained from information sources with unknown reliability and even causal discovery methods, as incorrect causal models can consequently bring biases to the predictor and lead to unfair predictions. In this work, we address the problem of counterfactually fair prediction from observational data without given causal models by proposing a novel framework CLAIRE. Specifically, under certain general assumptions, CLAIRE effectively mitigates the biases from the sensitive attribute with a representation learning framework based on counterfactual data augmentation and an invariant penalty. Experiments conducted on both synthetic and real-world datasets validate the superiority of CLAIRE in both counterfactual fairness and prediction performance.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1620–1630},
numpages = {11},
keywords = {causal model, counterfactual fairness, sensitive attributes},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.24963/ijcai.2024/504,
author = {Lin, Yujie and Zhao, Chen and Shao, Minglai and Meng, Baoluo and Zhao, Xujiang and Chen, Haifeng},
title = {Towards counterfactual fairness-aware domain generalization in changing environments},
year = {2024},
isbn = {978-1-956792-04-1},
url = {https://doi.org/10.24963/ijcai.2024/504},
doi = {10.24963/ijcai.2024/504},
abstract = {Recognizing domain generalization as a commonplace challenge in machine learning, data distribution might progressively evolve across a continuum of sequential domains in practical scenarios. While current methodologies primarily concentrate on bolstering model effectiveness within these new domains, they tend to neglect issues of fairness throughout the learning process. In response, we propose an innovative framework known as Disentanglement for Counterfactual Fairness-aware Domain Generalization (DCFDG). This approach adeptly removes domain-specific information and sensitive information from the embedded representation of classification features. To scrutinize the intricate interplay between semantic information, domain-specific information, and sensitive attributes, we systematically partition the exogenous factors into four latent variables. By incorporating fairness regularization, we utilize semantic information exclusively for classification purposes. Empirical validation on synthetic and authentic datasets substantiates the efficacy of our approach, demonstrating elevated accuracy levels while ensuring the preservation of fairness amidst the evolving landscape of continuous domains.},
booktitle = {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence},
articleno = {504},
numpages = {9},
location = {Jeju, Korea},
series = {IJCAI '24}
}

@inproceedings{10.5555/3294996.3295162,
author = {Kusner, Matt and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
title = {Counterfactual fairness},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4069–4079},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@inproceedings{10.5555/3367032.3367236,
author = {Wu, Yongkai and Zhang, Lu and Wu, Xintao},
title = {Counterfactual fairness: unidentification, bound and algorithm},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {Fairness-aware learning studies the problem of building machine learning models that are subject to fairness requirements. Counterfactual fairness is a notion of fairness derived from Pearl's causal model, which considers a model is fair if for a particular individual or group its prediction in the real world is the same as that in the counterfactual world where the individual(s) had belonged to a different demographic group. However, an inherent limitation of counterfactual fairness is that it cannot be uniquely quantified from the observational data in certain situations, due to the unidentifiability of the counterfactual quantity. In this paper, we address this limitation by mathematically bounding the unidentifiable counterfactual quantity, and develop a theoretically sound algorithm for constructing counterfactually fair classifiers. We evaluate our method in the experiments using both synthetic and real-world datasets, as well as compare with existing methods. The results validate our theory and show the effectiveness of our method.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {1438–1444},
numpages = {7},
location = {Macao, China},
series = {IJCAI'19}
}

