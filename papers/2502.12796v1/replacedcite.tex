\section{Prior Work}
\label{subsec:prior_work}
In this section, we focus on works particularly geared towards counterfactual fairness. As mentioned earlier, ____ introduced counterfactual fairness in its rigorous form, and proposed a Markov Chain Monte Carlo (MCMC) based approach to learn appropriate counterfactual samples, when the mean cannot be computed analytically. Since they consider a synthetic dataset for which the counterfactual distribution can be computed exactly, they use a probabilistic programming language to assess the counterfactual fairness of models trained using their method. However, using a probabilistic programming language may not be always be feasible because the distributions of interest may not always be known exactly. Second, MCMC based approaches are typically prone to significant approximation errors and are usually applicable only for discrete variables. This is in contrast to our work which works for continuous, (in)sensitive variables as well as proposes a simpler measure for counterfactual fairness. Subsequently, a number of papers have studied the same problem and proposed better algorithms, at least under certain settings ____.
____ proposes an Evidence Lower Bound (ELBO)-based method to learn for counterfactual fairness. However, it is still vulnerable to $\mathcal{L}_3$ inconsistency issues. ____ apparently aims to enforce counterfactual fairness by learning a loss $\mathcal{L}_c$ which measures the sample-wise distance between factual and counterfactual samples. Furthermore, though they aim to learn fair representations by minimizing the $\texttt{MMD}$ loss between fair representations, it is not shown if this explicitly captures counterfactual fairness between the predictands themselves while training. Additionally it is also vulnerable to $\mathcal{L}_3$ inconsistency. ____ is a baseline approach that we adopt, closest to our setting cum method. As other works however, they also adopt a weak fairness metric (MSE) to measure for counterfactual fairness, which is not fully faithful to Definition \ref{def:ctfairness}. Their generation approach however considers an $\texttt{MMD}$ distance between priors and posteriors when computing the ELBO, coupled with an adversarial losses to generate sensitive values conditioned on inferred latents. Though it is claimed that this method achieves superior performance compared to learning with $\texttt{MMD}$ costs alone, their $\texttt{MMD}$-based variant critically defers from ours, as we sidestep an ELBO-based approach. 

Apart from the limitations highlighted above, to the best of our knowledge, no prior work has explicitly used the framework of NCMs to improve impartiality of a predictor to a certain sensitive attribute, which enhances the novelty of our methodology.