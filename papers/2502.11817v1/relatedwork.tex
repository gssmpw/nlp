\section{Related Work}
Corbett and Anderson introduced Bayesian Knowledge Tracing (BKT) as the initial knowledge tracing model, utilizing binary variables to predict a student's mastery or non-mastery of skills within a set \cite{BKT}. BKT relies on assumptions about variable distributions, including response accuracy and question difficulty \cite{BKTAssumption1}, \cite{BKTAssumption2}. The model considers four factors (i.e., initial knowledge status, learning rate, guessing probability, and slipping probability) to predict knowledge status based on the most recent response. Subsequent research expanded BKT by introducing factors like problem difficulty or individual learning ability \cite{BKTImprovement1}, \cite{BKTImprovement2}. Additionally, Knowledge Tracing Machine (KTM) \cite{KTM} employs factorization machines \cite{FactorizationMachine} to enhance the interpretability and performance of BKT. However, the interpretability of BKT models relies on simplified assumptions, limiting their ability to capture dynamic knowledge status and provide comprehensive explanations of students' learning processes, thereby negatively impacting overall model performance.

To address the limitations of traditional KT models, Piech et al. \cite{DKT} pioneered the integration of deep neural networks with knowledge tracing, introducing the DKT model. DKT employed Long Short-Term Memory recurrent neural networks (LSTM) \cite{LSTM} to model students' knowledge status, surpassing the performance of BKT and its extensions. Seeking enhanced prediction consistency in exercise sequences, DKT+ \cite{DKT+} improved upon DKT by introducing regularization on the LSTM's hidden state, smoothing the evolution of knowledge status. Subsequent to DKT, researchers delved further into the application of neural networks in knowledge tracing. Zhang et al. \cite{DKVMN} proposed a Dynamic Key-Value Memory Network (DKVMN) employing external memory matrices to store knowledge memory and update corresponding mastery levels. Building on DKVMN, Abdelrahman and Wang \cite{SKVMN} introduced a modified LSTM, hop-LSTM, in their model. Zhu et al. \cite{StableKT} first employs causal inference for explanatory analysis of knowledge tracing, then proposes a learning algorithm for stable KT based on the analysis outcomes. Guo et al. \cite{ATKT} utilized adversarial training in Adversarial Knowledge Tracing (ATKT) to enhance model robustness and mitigate overfitting in small datasets. Wang et al. \cite{DCD} integrated educational priors and RNN in a KT model to improve interpretability.

Building on the success of attention mechanisms and transformers \cite{Attention} in natural language processing and computer vision, self-attention-based knowledge tracing models have emerged. Pandey and Karypis \cite{SAKT} were pioneers in introducing the self-attention mechanism to Knowledge Tracing models (SAKT). This innovation enables SAKT to capture long-term dependencies within historical exercises. Pu et al. \cite{VanillaTrKT} proposed a KT model based on the vanilla transformer. Additionally, Ghosh et al. \cite{AKT} presented a context-aware attentive KT model featuring monotonic attention and an exponential decay mechanism. More recently, Cui et al. \cite{MRTKT} introduced a multi-relational transformer designed to facilitate fine-grained interaction modeling between question-response pairs. Qiu et al. \cite{OPKT} proposed the optimized pretraining deep knowledge tracing (OPKT) method, which enhances knowledge state assessment by self-supervised learning, comprehensive contextual encodings and extracting latent features from learning sequences.

{Despite notable advancements, self-attention-based KT models encounter challenges in effectively integrating information from questions and responses. While various design features attempt to model both types of information, the interactive relationship between them is partly neglected. In SAKT \cite{SAKT}, history interactions, including questions and responses, and future questions are encoded into different sequences, leading to inconsistencies in semantic spaces. AKT \cite{AKT} calculates self-attention within question sequences and response sequences separately, employing a knowledge retriever to partially combine the two types of information. However, the information from responses does not contribute to calculating attention weights in knowledge retrievers, resulting in inadequate information interaction. MRT-KT \cite{MRTKT} establishes multiple relations between different exercises based on their question skill sets and response results. However, it omits including response information as encoded input due to the lack of an effective autoregressive sequence construction method.}

\begin{figure*}[ht]
\centering
\includegraphics[width=0.98\textwidth]{figures/model_structure.pdf}
\caption{The overall architecture of the proposed AAKT framework. It comprises three integral components. }
\label{fig:model structure}
\end{figure*}

{In response to the identified limitations of self-attention-based KT models, we advocate for better integration of questions and responses. Subsequently, we conducted an analysis of the causal relationships inherent in knowledge tracing tasks. Our proposal introduces an alternative autoregressive modeling approach based on a novel sequential representation that encompasses both question and response information.}