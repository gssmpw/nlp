\section{Appendix}
\subsection{Prompt Template} \label{appendix:prmpttemplate}

\begin{tcolorbox}[title=Prompt for Instruction Generation, label={fig:test_generation_prompt}]
You are an instruction compliance evaluator, required to assess the instruction compliance ability of large models. Therefore, you need to generate a series of data for the code generation instruction detection of large models.\\
\textbf{[Input Format]}\\
I will input a series of data, and you need to generate a dictionary based on these data, which includes two fields ``question'' and ``instruction\_list''\\
\textbf{Original question}:\\
\{Original question\}\\
\textbf{Original instruction list:}\\
\{instruction\_list\}\\
\textbf{Input Explanation}\\
The original question is the original question. It contains the original code generation problem.\\
The original instruction list is the original instruction list. It contains randomly generated code compliance instructions. Some instructions will contain directive keywords that need to be replaced and are wrapped in \{\{\}\}.

\textbf{Return Format}\\
Return a json data, do not have extra output.
The returned dictionary contains two fields: ``question'' and ``instruction\_list''\\
The format is as follows:
\begin{verbatim}
{
    "question": "Optimized question",
    "instruction_list": [
        {
            "instruction_id": "id1",
            "instruction": "Instruction 1"
        },
        {
            "instruction_id": "id2",
            "instruction": "Instruction 2"
        }
    ]
}
\end{verbatim}
\textbf{Explanation}\\
``question'': It is the optimized question, which does not contain any directive instructions, only contains the explanation of the original question. It does not contain any restrictions on the code. Move the instructions in the question to the instruction list\\
``instruction\_list'': It is the optimized instruction list. You should optimize according to the meaning of the question. More in line with the meaning of the question. Instead of directly outputting the original instruction list, note that you should replace all directive keywords that need to be replaced and are wrapped in {{}}, and the final output should not contain directive keywords that need to be replaced.\\
\textbf{Generation Requirements}\\
question: Please generate the optimized question based on the following data, which does not contain any directive instructions, only contains the core content of the original question.\\
instruction\_list: Originated from the input original instruction list. If there are instructions that completely conflict with the meaning of the question or instructions that conflict with each other **we can delete or modify them. But note that we need to delete as little as possible. we need to evaluate the instruction compliance ability of large models.\\
You should delete as little as possible, you should modify more.
Please replace according to the content in the original instruction\_list, you should delete as little as possible. Unless it is contradictory instructions, or instructions that cannot be achieved at all, if you only need to generate additional code to meet the requirements, you can replace it.
\end{tcolorbox}

\begin{tcolorbox}[title=Prompt for Code Generation, label={fig:test_generation_prompt}]
As a programming assistant, your task is to generate code snippets based on the user question and instructions given below:

\textbf{Please consider the following points while generating the code snippet}:

- Make sure you follow the user instructions word to word. If the instruction says to use a specific language or a specific method, use exactly that.

- Your output should be a valid code snippet in the programming language indicated in the question or the instructions.

- Pay close attention to the syntax and formatting rules of the programming language that you are using. The code should be well-formatted and easy to read. 

- Make sure that the code snippet you are generating is efficient and is not overly complicated.

\textbf{Output Format}:\\
The output should be a valid, well-formatted, and efficient code snippet that adheres to the above question and instructions.

\textbf{Task information}

User Question:\\
\{question\}

Instructions:\\
\{instructions\_str\}

Please generate the code snippet based on the above information:
\end{tcolorbox}

\begin{tcolorbox}[title=Prompt for  Answer Judgment]
\label{prompt:judge_prompt}
As a programming assistant, your task is to evaluate whether the generated code strictly follows the instructions given in light of the user's problem and directives. You need to return a list of the same length as the instructions, containing only 'Yes' and 'No', indicating whether the model adhered to each specific instruction.

\textbf{Consider the following when making judgments:}

- You must strictly follow the user's instructions. If the instruction requires the use of a specific language or method, you must explicitly check if the code utilizes it.

- Your output should be a list of the same length as the instructions, containing only 'Yes' and 'No'.

- Pay close attention to the programming language syntax and formatting rules you are evaluating. The code should be neatly organized and easy to read.

- The list you generate should be valid and not overly complex.

\textbf{Task Information}

\textbf{User question:}\\
\{question\}

\textbf{Instructions:}\\
\{instructions\_str\}

\textbf{Model-generated response:}\\
\{generated\_code\}

Based on the information provided, determine whether the model has followed the instructions, and return a list of the same length as the instructions, containing only `Yes' and `No'.
Please note!!! Your output should only contain the list, with no other content. The items in the list should only be `Yes' and `No', with no other words included.
\end{tcolorbox}
% Within this supplementary material, we elaborate on the following aspects:
% \begin{compactitem}
% \item Appendix \ref{appendix:dataset}: Benchmark Details

% \item Appendix \ref{appendix:experiment}: Experiment Details

% \item Appendix \ref{appendix:case}: Case Presentation

% \end{compactitem}

\subsection{More Resluts} \label{app:moreresults}
\begin{table}[h]
\centering
\caption{Constraint Instruction Table}
\small
\begin{tabular}{ccp{10cm}p{5cm}}
\toprule
ID & Type & Instruction Format & Format Keys \\
\midrule
1 & global & Your entire response should be written in \{programming\_language\}, the use of other programming languages is not allowed. & ["programming\_language"] \\
2 & global & Your code lines should not exceed \{characters\_num\} characters. & ["characters\_num"] \\
3 & global & Your code should use global variables. & [] \\
4 & global & Your code should not use global variables. & [] \\
5 & global & Your function should have at most \{parameter\_count\} parameters. & ["parameter\_count"] \\
6 & global & Your code should not have more than \{function\_count\} functions. & ["function\_count"] \\
7 & global & Your code should not have more than \{class\_count\} classes. & ["class\_count"] \\
8 & global & Your code should not use the \{keyword\} keyword. & ["keyword"] \\
9 & global & Your function should not exceed \{line\_num\} lines. & ["line\_num"] \\
10 & global & Your answer in total should not exceed \{line\_num\} lines. & ["line\_num"] \\
11 & global & Your code should use the \{keyword\} keyword. & ["keyword"] \\
12 & structural control & Your code should use data structure \{data\_structure\}. & ["data\_structure"] \\
13 & structural control & Your code should not use data structure \{data\_structure\}. & ["data\_structure"] \\
14 & structural control & Your code should use for-loop. & [] \\
15 & structural control & Your code should not use for-loop. & [] \\
16 & structural control & Your code should use while-loop. & [] \\
17 & structural control & Your code should not use while-loop. & [] \\
18 & structural control & Your code should use if statement for decision making. & [] \\
19 & structural control & Your code should not use if statement for decision making. & [] \\
20 & structural control & Your code should use switch statement for decision making. & [] \\
21 & structural control & Your code should not use switch statement for decision making. & [] \\
22 & variable & Your code should define a variable named \{variable\_name\}. & ["variable\_name"] \\
23 & variable & Your code should define an enumeration named \{enumeration\_name\} & ["enumeration\_name"] \\
24 & variable & The variable names in your code should follow the \{naming\_convention\} naming convention & ["naming\_convention"] \\
25 & variable & Variable \{variable\_name\}, type should be \{variable\_type\}. & ["variable\_name", "variable\_type"] \\
26 & variable & Variable \{variable\_name\}, should be a global variable. & ["variable\_name"] \\
27 & variable & Variable \{variable\_name\}, should not be a global variable. & ["variable\_name"] \\
28 & variable & Variable \{variable\_name\}, the initial value should be \{variable\_value\}. & ["variable\_name", "variable\_value"] \\
29 & variable & Variable \{variable\_name\}, should be a constant. & ["variable\_name"] \\
30 & variable & Variable \{variable\_name\} should not be a constant. & ["variable\_name"] \\
31 & function & Your code should include a function named \{function\_name\}. & ["function\_name"] \\
32 & function & The function names in your code should follow the \{naming\_convention\}. naming convention & ["naming\_convention"] \\
33 & function & Your code should not use any functions from the \{disallowed\_function\_list\}. & ["disallowed\_function\_list"] \\
34 & interface & Your code should define an interface named \{interface\_name\}. & ["interface\_name"] \\
35 & interface & The interface names in your code should follow the \{naming\_convention\} naming convention. & ["naming\_convention"] \\
36 & class & Your code should define a class named \{class\_name\}. & ["class\_name"] \\
37 & class & The class names in your code should follow the \{naming\_convention\} naming convention. & ["naming\_convention"] \\
38 & file & Your code should be organized in a package named \{package\_name\}. & ["package\_name"] \\
39 & file & Your code should import the following libraries \{library\_list\}. & ["library\_list"] \\
40 & file & Your code should use the function \{function\_name\} from the library \{library\_name\}. & ["function\_name", "library\_name"] \\
41 & file & Your code should not use the following libraries \{disallowed\_library\_list\}. & ["disallowed\_library\_list"] \\
42 & combination & You should initialize an object named \{object\_name\} as an instance of the \{class\_name\} class using \{parameters\_name\_list\} for initialization. & ["object\_name", "class\_name", "parameters\_name\_list"] \\
43 & combination & You should define an interface named \{interface\_name\} that includes these methods \{method\_name\_list\}. & ["interface\_name", "method\_name\_list"] \\
44 & combination & Your code should define a class named \{class\_name\} that implements the \{interface\_name\} interface. & ["class\_name", "interface\_name"] \\
45 & combination & In your code, the class \{class\_name\} should have these properties \{properties\_name\_list\}. & ["class\_name", "properties\_name\_list"] \\
46 & combination & In your code, the class \{class\_name\} should have these methods \{method\_name\_list\}. & ["class\_name", "method\_name\_list"] \\
47 & combination & The function \{function\_name\} should take \{parameter\_name\_list\} as parameters. & ["function\_name", "parameter\_name\_list"] \\
48 & combination & The function \{function\_name\} should return a \{return\_type\} as its result. & ["function\_name", "return\_type"] \\
49 & combination & Your code should be organized in a package named \{package\_name\}, which should contain these classes \{class\_name\_list\}. & ["package\_name", "class\_name\_list"] \\
50 & combination & Your code should be organized in a package named \{package\_name\}, which should contain these functions \{function\_name\_list\}. & ["package\_name", "function\_name\_list"] \\
\bottomrule
\end{tabular}
\label{tab:Constraint_Instruction_Tabl}
\end{table}


\begin{table*}[t]
\centering
\caption{ CodeIF evaluation results of different difficulties. We use bold font to mark the best results in all models.}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{ccccccccccccc}
\toprule
\multirow{2}{*}{\textbf{Models}} & \multicolumn{3}{c}{\textbf{CSR}} & \multicolumn{3}{c}{\textbf{SSR}} & \multicolumn{3}{c}{\textbf{RSR}} & \multicolumn{3}{c}{\textbf{CCSR}} \\
% \hline
\cmidrule(lr){2-13} 
\textbf{} & \textbf{Full} & \textbf{Easy} & \textbf{Hard} & \textbf{Full} & \textbf{Easy} & \textbf{Hard} & \textbf{Full} & \textbf{Easy} & \textbf{Hard} & \textbf{Full} & \textbf{Easy} & \textbf{Hard} \\
\midrule
\textbf{Llama-3.2-1b-instruct} & 0.034 & 0.046 & 0.022 & 0.218 & 0.277 & 0.159 & 0.182 & 0.231 & 0.132 & 0.152 & 0.197 & 0.107\\
\textbf{Qwen2.5-1.5b-instruct} & 0.034 & 0.053 & 0.015 & 0.265 & 0.334 & 0.197 & 0.222 & 0.282 & 0.162 & 0.181 & 0.234 & 0.128\\
\textbf{Qwen2.5-coder-1.5b-instruct} & 0.058 & 0.086 & 0.03 & 0.358 & 0.436 & 0.281 & 0.301 & 0.371 & 0.233 & 0.251 & 0.314 & 0.189\\
\textbf{Qwen2.5-3b-instruct} & 0.078 & 0.109 & 0.046 & 0.415 & 0.489 & 0.34 & 0.357 & 0.432 & 0.282 & 0.299 & 0.364 & 0.233\\
\textbf{Llama-3.2-3b-instruct} & 0.101 & 0.137 & 0.065 & 0.396 & 0.473 & 0.318 & 0.344 & 0.419 & 0.268 & 0.305 & 0.375 & 0.235\\
\textbf{GPT-3.5-turbo} & 0.102 & 0.14 & 0.065 & 0.41 & 0.467 & 0.353 & 0.362 & 0.42 & 0.303 & 0.314 & 0.369 & 0.259\\
\textbf{Qwen2.5-coder-3b-instruct} & 0.097 & 0.142 & 0.051 & 0.445 & 0.529 & 0.359 & 0.383 & 0.464 & 0.301 & 0.33 & 0.401 & 0.258\\
\textbf{Llama-3.1-8b} & 0.129 & 0.178 & 0.08 & 0.452 & 0.551 & 0.353 & 0.402 & 0.497 & 0.306 & 0.352 & 0.44 & 0.263\\
\textbf{Llama-3.1-8b-instruct} & 0.145 & 0.187 & 0.102 & 0.467 & 0.544 & 0.388 & 0.418 & 0.493 & 0.34 & 0.37 & 0.444 & 0.295\\
\textbf{Qwen2.5-coder-7b-instruct} & 0.142 & 0.198 & 0.087 & 0.514 & 0.59 & 0.438 & 0.453 & 0.533 & 0.373 & 0.39 & 0.463 & 0.318\\
\textbf{Ministral-3b} & 0.127 & 0.162 & 0.092 & 0.526 & 0.591 & 0.46 & 0.458 & 0.527 & 0.39 & 0.4 & 0.458 & 0.342\\
\textbf{Phi-3.5-mini-128k-instruct} & 0.154 & 0.217 & 0.09 & 0.514 & 0.635 & 0.391 & 0.456 & 0.574 & 0.337 & 0.405 & 0.51 & 0.299\\
\textbf{Qwen2.5-7b-instruct} & 0.153 & 0.201 & 0.104 & 0.535 & 0.599 & 0.471 & 0.475 & 0.546 & 0.405 & 0.416 & 0.479 & 0.353\\
\textbf{Ministral-8b} & 0.161 & 0.205 & 0.116 & 0.552 & 0.614 & 0.489 & 0.486 & 0.552 & 0.419 & 0.431 & 0.49 & 0.371\\
\textbf{Gemma-2-9b-it} & 0.171 & 0.21 & 0.131 & 0.573 & 0.642 & 0.504 & 0.513 & 0.587 & 0.44 & 0.445 & 0.508 & 0.383\\
\textbf{Llama-3.1-70b} & 0.196 & 0.232 & 0.16 & 0.61 & 0.664 & 0.555 & 0.545 & 0.607 & 0.482 & 0.482 & 0.533 & 0.43\\
\textbf{Qwen2.5-coder-14b-instruct} & 0.218 & 0.276 & 0.16 & 0.596 & 0.667 & 0.525 & 0.539 & 0.614 & 0.463 & 0.483 & 0.55 & 0.416\\
\textbf{Qwen2.5-14b-instruct} & 0.238 & 0.279 & 0.198 & 0.61 & 0.676 & 0.543 & 0.557 & 0.628 & 0.486 & 0.498 & 0.565 & 0.431\\
\textbf{Gemini-2.0-flash-exp} & 0.254 & 0.29 & 0.218 & 0.615 & 0.648 & 0.583 & 0.556 & 0.593 & 0.518 & 0.514 & 0.547 & 0.481\\
\textbf{Gemma-2-27b-it} & 0.245 & 0.3 & 0.19 & 0.658 & 0.709 & 0.607 & 0.596 & 0.652 & 0.54 & 0.533 & 0.588 & 0.478\\
\textbf{Llama-3.1-70b-instruct} & 0.265 & 0.3 & 0.229 & 0.675 & 0.723 & 0.627 & 0.612 & 0.667 & 0.556 & 0.559 & 0.601 & 0.516\\
\textbf{Qwen2.5-32b-instruct} & 0.294 & 0.337 & 0.251 & 0.68 & 0.722 & 0.638 & 0.621 & 0.674 & 0.568 & 0.56 & 0.604 & 0.515\\
\textbf{Qwen2.5-72b-instruct} & 0.281 & 0.319 & 0.244 & 0.685 & 0.734 & 0.634 & 0.621 & 0.677 & 0.564 & 0.569 & 0.619 & 0.518\\
\textbf{Codestral-2501} & 0.28 & 0.339 & 0.219 & 0.683 & 0.748 & 0.617 & 0.621 & 0.691 & 0.551 & 0.571 & 0.633 & 0.507\\
\textbf{Phi-4} & 0.312 & 0.361 & 0.262 & 0.698 & 0.735 & 0.66 & 0.635 & 0.681 & 0.589 & 0.589 & 0.631 & 0.546\\
\textbf{Llama-3.3-70b-instruct} & 0.307 & 0.359 & 0.255 & 0.698 & 0.749 & 0.647 & 0.632 & 0.691 & 0.574 & 0.589 & 0.643 & 0.536\\
\textbf{GPT-4o-mini} & 0.292 & 0.348 & 0.237 & 0.731 & 0.78 & 0.682 & 0.665 & 0.724 & 0.606 & 0.609 & 0.66 & 0.559\\
\textbf{GPT-4o} & 0.338 & 0.392 & 0.283 & 0.721 & 0.77 & 0.671 & 0.665 & 0.721 & 0.609 & 0.616 & 0.668 & 0.563\\
\textbf{Qwen2.5-coder-32b-instruct} & 0.365 & 0.422 & 0.307 & 0.736 & 0.767 & 0.704 & 0.679 & 0.723 & 0.635 & 0.634 & 0.669 & 0.599\\
\textbf{Gemini-exp-1206} & 0.357 & 0.41 & 0.303 & 0.744 & 0.781 & 0.707 & 0.685 & 0.734 & 0.636 & 0.636 & 0.675 & 0.597\\
\textbf{Gemini-1.5-pro} & 0.351 & 0.383 & 0.318 & 0.763 & 0.794 & 0.732 & 0.704 & 0.744 & 0.663 & 0.647 & 0.679 & 0.615\\
\textbf{GPT-4o-2024-11-20} & 0.383 & 0.441 & 0.325 & 0.748 & 0.792 & 0.702 & 0.689 & 0.745 & 0.633 & 0.65 & 0.698 & 0.602\\
\textbf{Claude-3-5-sonnet-20241022} & \textbf{0.444} & \textbf{0.525} & 0.362 & 0.727 & 0.784 & 0.669 & 0.692 & 0.757 & 0.626 & 0.652 & 0.715 & 0.587\\
\textbf{Deepseek-coder} & 0.41 & 0.45 & \textbf{0.37} & 0.805 & 0.836 & 0.773 & 0.749 & 0.791 & 0.707 & 0.699 & 0.732 & 0.666\\
\textbf{Deepseek-v3} & 0.414 & 0.468 & 0.359 & \textbf{0.821} & \textbf{0.847} & \textbf{0.794} & \textbf{0.764} & \textbf{0.806} & \textbf{0.723} & \textbf{0.712} & \textbf{0.743} & \textbf{0.68}\\

\bottomrule
\end{tabular}}
\label{tab:codeIF_evaluation_all}
\end{table*}




\begin{table}[h]
\centering
\caption{The performance of various models on CodeIF for different types of instructions}
\small
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{ccccccccc}
\toprule
\textbf{Models} &\textbf{Global} &\textbf{Structural Control}&\textbf{Variable} &\textbf{Interface} &\textbf{Function} &\textbf{Class} &\textbf{File} &\textbf{Combination}\\
\midrule

\textbf{Llama-3.2-1b-instruct} & 0.186 & 0.190 & 0.206 & 0.144 & 0.284 & 0.260 & 0.198 & 0.172 \\
\textbf{Qwen2.5-1.5b-instruct} & 0.244 & 0.236 & 0.221 & 0.213 & 0.355 & 0.315 & 0.230 & 0.213 \\
\textbf{Qwen2.5-coder-1.5b-instruct} & 0.328 & 0.304 & 0.326 & 0.293 & 0.436 & 0.426 & 0.351 & 0.304 \\
\textbf{Qwen2.5-3b-instruct} & 0.383 & 0.346 & 0.412 & 0.383 & 0.468 & 0.481 & 0.383 & 0.366 \\
\textbf{Llama-3.2-3b-instruct} & 0.344 & 0.332 & 0.393 & 0.376 & 0.454 & 0.447 & 0.363 & 0.367 \\
\textbf{GPT-3.5-turbo} & 0.388 & 0.344 & 0.417 & 0.375 & 0.467 & 0.449 & 0.378 & 0.352 \\
\textbf{Qwen2.5-coder-3b-instruct} & 0.397 & 0.367 & 0.438 & 0.419 & 0.511 & 0.507 & 0.415 & 0.403 \\
\textbf{Llama-3.1-8b} & 0.410 & 0.355 & 0.451 & 0.424 & 0.500 & 0.503 & 0.413 & 0.413 \\
\textbf{Llama-3.1-8b-instruct} & 0.422 & 0.373 & 0.482 & 0.455 & 0.524 & 0.499 & 0.407 & 0.437 \\
\textbf{Qwen2.5-coder-7b-instruct} & 0.479 & 0.419 & 0.497 & 0.502 & 0.576 & 0.571 & 0.492 & 0.487 \\
\textbf{Ministral-3b} & 0.472 & 0.403 & 0.527 & 0.512 & 0.618 & 0.609 & 0.524 & 0.535 \\
\textbf{Phi-3.5-mini-128k-instruct} & 0.461 & 0.410 & 0.512 & 0.531 & 0.562 & 0.574 & 0.485 & 0.491 \\
\textbf{Qwen2.5-7b-instruct} & 0.484 & 0.425 & 0.532 & 0.548 & 0.616 & 0.591 & 0.520 & 0.520 \\
\textbf{Ministral-8b} & 0.497 & 0.433 & 0.541 & 0.570 & 0.622 & 0.631 & 0.527 & 0.557 \\
\textbf{Gemma-2-9b-it} & 0.541 & 0.498 & 0.599 & 0.510 & 0.659 & 0.618 & 0.543 & 0.511 \\
\textbf{Llama-3.1-70b} & 0.558 & 0.500 & 0.652 & 0.653 & 0.685 & 0.671 & 0.545 & 0.597 \\
\textbf{Qwen2.5-coder-14b-instruct} & 0.541 & 0.467 & 0.623 & 0.669 & 0.645 & 0.652 & 0.547 & 0.594 \\
\textbf{Qwen2.5-14b-instruct} & 0.569 & 0.526 & 0.652 & 0.592 & 0.649 & 0.644 & 0.533 & 0.559 \\
\textbf{Gemini-2.0-flash-exp} & 0.555 & 0.526 & 0.653 & 0.666 & 0.685 & 0.669 & 0.564 & 0.615 \\
\textbf{Gemma-2-27b-it} & 0.621 & 0.569 & 0.699 & 0.640 & 0.722 & 0.710 & 0.607 & 0.637 \\
\textbf{Llama-3.1-70b-instruct} & 0.606 & 0.546 & 0.722 & 0.718 & 0.744 & 0.738 & 0.603 & 0.680 \\
\textbf{Qwen2.5-32b-instruct} & 0.637 & 0.581 & 0.713 & 0.712 & 0.732 & 0.742 & 0.601 & 0.653 \\
\textbf{Qwen2.5-72b-instruct} & 0.633 & 0.570 & 0.734 & 0.711 & 0.727 & 0.726 & 0.645 & 0.686 \\
\textbf{Codestral-2501} & 0.617 & 0.552 & 0.723 & 0.718 & 0.733 & 0.746 & 0.651 & 0.694 \\
\textbf{Phi-4} & 0.633 & 0.586 & 0.734 & 0.739 & 0.721 & 0.752 & 0.677 & 0.710 \\
\textbf{Llama-3.3-70b-instruct} & 0.621 & 0.634 & 0.733 & 0.730 & 0.759 & 0.738 & 0.645 & 0.695 \\
\textbf{GPT-4o-mini} & 0.671 & 0.663 & 0.787 & 0.774 & 0.784 & 0.783 & 0.657 & 0.710 \\
\textbf{GPT-4o} & 0.665 & 0.651 & 0.742 & 0.759 & 0.743 & 0.759 & 0.666 & 0.716 \\
\textbf{Qwen2.5-coder-32b-instruct} & 0.683 & 0.654 & 0.776 & 0.763 & 0.772 & 0.758 & 0.695 & 0.736 \\
\textbf{Gemini-exp-1206} & 0.690 & 0.677 & 0.780 & 0.789 & 0.798 & 0.809 & 0.675 & 0.727 \\
\textbf{Gemini-1.5-pro} & 0.718 & 0.696 & 0.814 & 0.800 & 0.812 & 0.815 & 0.672 & 0.749 \\
\textbf{GPT-4o-2024-11-20} & 0.685 & 0.666 & 0.784 & 0.786 & 0.779 & 0.785 & 0.706 & 0.755 \\
\textbf{Claude-3-5-sonnet-20241022} & 0.677 & 0.678 & 0.750 & 0.736 & 0.742 & 0.730 & 0.640 & 0.692 \\
\textbf{Deepseek-coder} & 0.759 & 0.714 & 0.850 & 0.856 & 0.846 & 0.847 & 0.754 & 0.813 \\
\textbf{Deepseek-v3} & 0.780 & 0.732 & 0.866 & 0.876 & 0.866 & 0.873 & 0.762 & 0.831 \\

\bottomrule

\end{tabular}}
\end{table}



\begin{table*}[t]
\small
\centering
\caption{ the evaluation results of different languages on CODEIF. The metrics include Consistent Continuity Satisfaction Rate (CCSR), Complete Satisfaction Rate (CSR), Soft Satisfaction Rate (SSR), and Rigorous Satisfaction Rate (RSR).}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{ccccccccccccccccc}
\toprule

\multirow{2}{*}{\textbf{Models}} & \multicolumn{4}{c}{\textbf{CPP}} & \multicolumn{4}{c}{\textbf{Java}} & \multicolumn{4}{c}{\textbf{Python}} & \multicolumn{4}{c}{\textbf{Go}} \\
% \hline
% CCS	CS	SS	RS
\cmidrule(lr){2-17} 
\textbf{} & \textbf{CCS}&\textbf{CS} & \textbf{SS} & \textbf{RS} & \textbf{CCS} & \textbf{CS} & \textbf{SS} & \textbf{RS}& \textbf{CCS} & \textbf{CS} & \textbf{SS} & \textbf{RS}& \textbf{CCS} & \textbf{CS} & \textbf{SS} & \textbf{RS} \\
\midrule

\textbf{Llama-3.2-1b-instruct} & 0.123 & 0.023 & 0.185 & 0.150 & 0.190 & 0.037 & 0.265 & 0.221 & 0.179 & 0.047 & 0.262 & 0.223 & 0.086 & 0.022 & 0.117 & 0.096 \\
\textbf{Qwen2.5-1.5b-instruct} & 0.171 & 0.023 & 0.250 & 0.206 & 0.191 & 0.026 & 0.277 & 0.228 & 0.197 & 0.047 & 0.298 & 0.257 & 0.151 & 0.040 & 0.216 & 0.179 \\
\textbf{Qwen2.5-coder-1.5b-instruct} & 0.253 & 0.068 & 0.348 & 0.297 & 0.259 & 0.055 & 0.375 & 0.308 & 0.263 & 0.060 & 0.380 & 0.328 & 0.218 & 0.049 & 0.309 & 0.255 \\
\textbf{Qwen2.5-3b-instruct} & 0.251 & 0.046 & 0.367 & 0.302 & 0.310 & 0.078 & 0.419 & 0.367 & 0.306 & 0.092 & 0.435 & 0.384 & 0.327 & 0.093 & 0.433 & 0.365 \\
\textbf{Llama-3.2-3b-instruct} & 0.284 & 0.073 & 0.377 & 0.313 & 0.345 & 0.121 & 0.435 & 0.380 & 0.321 & 0.112 & 0.429 & 0.383 & 0.244 & 0.084 & 0.304 & 0.265 \\
\textbf{GPT-3.5-turbo} & 0.301 & 0.085 & 0.388 & 0.332 & 0.367 & 0.134 & 0.461 & 0.409 & 0.265 & 0.092 & 0.371 & 0.334 & 0.318 & 0.088 & 0.412 & 0.363 \\
\textbf{Qwen2.5-coder-3b-instruct} & 0.339 & 0.103 & 0.444 & 0.380 & 0.338 & 0.101 & 0.453 & 0.391 & 0.320 & 0.091 & 0.446 & 0.390 & 0.323 & 0.093 & 0.431 & 0.363 \\
\textbf{Llama-3.1-8b} & 0.319 & 0.115 & 0.409 & 0.354 & 0.366 & 0.130 & 0.477 & 0.420 & 0.376 & 0.152 & 0.485 & 0.446 & 0.330 & 0.110 & 0.413 & 0.363 \\
\textbf{Llama-3.1-8b-instruct} & 0.328 & 0.112 & 0.432 & 0.375 & 0.408 & 0.173 & 0.503 & 0.447 & 0.393 & 0.147 & 0.496 & 0.455 & 0.325 & 0.133 & 0.406 & 0.365 \\
\textbf{Qwen2.5-coder-7b-instruct} & 0.389 & 0.147 & 0.505 & 0.434 & 0.375 & 0.118 & 0.503 & 0.444 & 0.400 & 0.155 & 0.531 & 0.475 & 0.401 & 0.154 & 0.516 & 0.456 \\
\textbf{Ministral-3b} & 0.356 & 0.107 & 0.473 & 0.401 & 0.410 & 0.150 & 0.542 & 0.476 & 0.404 & 0.112 & 0.538 & 0.481 & 0.430 & 0.138 & 0.544 & 0.464 \\
\textbf{Phi-3.5-mini-128k-instruct} & 0.354 & 0.108 & 0.461 & 0.388 & 0.426 & 0.179 & 0.532 & 0.478 & 0.440 & 0.180 & 0.559 & 0.510 & 0.380 & 0.131 & 0.482 & 0.422 \\
\textbf{Qwen2.5-7b-instruct} & 0.401 & 0.162 & 0.514 & 0.448 & 0.439 & 0.152 & 0.559 & 0.495 & 0.397 & 0.147 & 0.523 & 0.471 & 0.429 & 0.154 & 0.541 & 0.485 \\
\textbf{Ministral-8b} & 0.400 & 0.143 & 0.518 & 0.439 & 0.434 & 0.158 & 0.560 & 0.495 & 0.410 & 0.142 & 0.538 & 0.481 & 0.494 & 0.214 & 0.599 & 0.532 \\
\textbf{Gemma-2-9b-it} & 0.446 & 0.200 & 0.560 & 0.499 & 0.446 & 0.164 & 0.576 & 0.518 & 0.380 & 0.131 & 0.510 & 0.456 & 0.542 & 0.204 & 0.678 & 0.609 \\
\textbf{Llama-3.1-70b} & 0.487 & 0.201 & 0.598 & 0.518 & 0.507 & 0.232 & 0.632 & 0.572 & 0.425 & 0.136 & 0.579 & 0.521 & 0.522 & 0.226 & 0.635 & 0.571 \\
\textbf{Qwen2.5-coder-14b-instruct} & 0.464 & 0.224 & 0.572 & 0.514 & 0.478 & 0.206 & 0.592 & 0.535 & 0.522 & 0.216 & 0.653 & 0.594 & 0.454 & 0.235 & 0.544 & 0.490 \\
\textbf{Qwen2.5-14b-instruct} & 0.481 & 0.230 & 0.590 & 0.533 & 0.528 & 0.265 & 0.639 & 0.581 & 0.472 & 0.188 & 0.599 & 0.550 & 0.511 & 0.281 & 0.603 & 0.557 \\
\textbf{Gemini-2.0-flash-exp} & 0.491 & 0.259 & 0.587 & 0.519 & 0.575 & 0.309 & 0.664 & 0.604 & 0.468 & 0.207 & 0.584 & 0.533 & 0.514 & 0.233 & 0.619 & 0.558 \\
\textbf{Gemma-2-27b-it} & 0.529 & 0.271 & 0.645 & 0.579 & 0.551 & 0.261 & 0.676 & 0.616 & 0.465 & 0.179 & 0.604 & 0.543 & 0.611 & 0.289 & 0.727 & 0.665 \\
\textbf{Llama-3.1-70b-instruct} & 0.535 & 0.267 & 0.653 & 0.578 & 0.581 & 0.276 & 0.685 & 0.620 & 0.555 & 0.251 & 0.696 & 0.639 & 0.557 & 0.264 & 0.655 & 0.596 \\
\textbf{Qwen2.5-32b-instruct} & 0.551 & 0.314 & 0.655 & 0.602 & 0.589 & 0.330 & 0.706 & 0.638 & 0.522 & 0.231 & 0.665 & 0.609 & 0.580 & 0.311 & 0.690 & 0.634 \\
\textbf{Qwen2.5-72b-instruct} & 0.543 & 0.297 & 0.638 & 0.574 & 0.580 & 0.288 & 0.701 & 0.633 & 0.574 & 0.284 & 0.702 & 0.651 & 0.573 & 0.249 & 0.687 & 0.610 \\
\textbf{Codestral-2501} & 0.562 & 0.307 & 0.658 & 0.595 & 0.583 & 0.301 & 0.694 & 0.632 & 0.566 & 0.249 & 0.693 & 0.637 & 0.569 & 0.261 & 0.681 & 0.611 \\
\textbf{Phi-4} & 0.570 & 0.331 & 0.663 & 0.601 & 0.612 & 0.328 & 0.719 & 0.650 & 0.587 & 0.295 & 0.714 & 0.660 & 0.577 & 0.292 & 0.679 & 0.613 \\
\textbf{Llama-3.3-70b-instruct} & 0.558 & 0.300 & 0.652 & 0.582 & 0.621 & 0.348 & 0.713 & 0.644 & 0.572 & 0.264 & 0.709 & 0.653 & 0.602 & 0.317 & 0.712 & 0.640 \\
\textbf{GPT-4o-mini} & 0.582 & 0.292 & 0.684 & 0.615 & 0.620 & 0.299 & 0.738 & 0.667 & 0.586 & 0.261 & 0.731 & 0.674 & 0.661 & 0.330 & 0.775 & 0.707 \\
\textbf{GPT-4o} & 0.600 & 0.337 & 0.698 & 0.639 & 0.652 & 0.368 & 0.748 & 0.693 & 0.600 & 0.312 & 0.723 & 0.676 & 0.603 & 0.332 & 0.701 & 0.636 \\
\textbf{Qwen2.5-coder-32b-instruct} & 0.633 & 0.384 & 0.717 & 0.658 & 0.654 & 0.401 & 0.753 & 0.699 & 0.621 & 0.342 & 0.736 & 0.688 & 0.624 & 0.322 & 0.731 & 0.661 \\
\textbf{Gemini-exp-1206} & 0.640 & 0.424 & 0.726 & 0.672 & 0.650 & 0.360 & 0.755 & 0.689 & 0.590 & 0.290 & 0.724 & 0.674 & 0.677 & 0.373 & 0.777 & 0.710 \\
\textbf{Gemini-1.5-pro} & 0.635 & 0.370 & 0.741 & 0.676 & 0.674 & 0.379 & 0.783 & 0.720 & 0.610 & 0.278 & 0.758 & 0.706 & 0.674 & 0.395 & 0.764 & 0.707 \\
\textbf{GPT-4o-2024-11-20} & 0.653 & 0.374 & 0.741 & 0.669 & 0.683 & 0.434 & 0.776 & 0.716 & 0.612 & 0.355 & 0.724 & 0.682 & 0.653 & 0.358 & 0.747 & 0.683 \\
\textbf{Claude-3-5-sonnet-20241022} & 0.615 & 0.425 & 0.684 & 0.643 & 0.720 & 0.504 & 0.789 & 0.749 & 0.611 & 0.396 & 0.703 & 0.674 & 0.650 & 0.444 & 0.716 & 0.686 \\
\textbf{Deepseek-coder} & 0.709 & 0.441 & 0.802 & 0.735 & 0.731 & 0.463 & 0.819 & 0.764 & 0.657 & 0.336 & 0.791 & 0.747 & 0.702 & 0.403 & 0.805 & 0.744 \\
\textbf{Deepseek-v3} & 0.725 & 0.435 & 0.831 & 0.762 & 0.753 & 0.497 & 0.839 & 0.787 & 0.651 & 0.315 & 0.793 & 0.744 & 0.722 & 0.404 & 0.822 & 0.76 \\

\bottomrule
\end{tabular}}
\label{tab:multitool-normal}
\end{table*}

