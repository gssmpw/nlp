\section{Related Work}

Code generation and instruction-following are pivotal capacities under examination in AI research~\cite{feng2020codebert,unicoder,wizardcoder,wang2023codet5+,kim2018deepcode,li2023starcoder,lu2021codexglue,li2022competition,wei2023magicoder,nijkamp2022codegen,zhuo2024bigcodebench,jain2024livecodebench,nijkamp2023codegen2,zhang2023repocoder,allal2023santacoder,lozhkov2024starcoder,roziere2023codellama,lozhkov2024starcoder2,wang-etal-2021-codet5,yan2023codetransocean}. Several benchmarks have been devised to appraise these capabilities in large-scale models. For code generation, benchmarks like McEval~\cite{mceval}, FullStackBench~\cite{liu2024fullstackbenchevaluatingllms}, Repocoder~\cite{zhang2023repocoder}, Repobench~\cite{liu2023repobench}, and LiveCodeBench~\cite{jain2024livecodebench} have been notable. Similarly, instruction-following capacities are gauged through benchmarks such as InfoBench~\cite{qin2024infobenchevaluatinginstructionfollowing}, CFBench~\cite{zhang2024cfbenchcomprehensiveconstraintsfollowingbenchmark}, Instruct-following~\cite{zhou2023instructionfollowingevaluationlargelanguage}, and FollowBench~\cite{jiang2024followbenchmultilevelfinegrainedconstraints}, each tailored to assess different aspects of following instructions given to models.