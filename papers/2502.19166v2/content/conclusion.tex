\section{Conclusion}

This study introduces \bench{}, a benchmark for evaluating the instruction-following capabilities of LLMs in code generation. Covering \textbf{Java, Python, Go, and C++}, CodeIF constructs a diverse test set with constraints ranging from global to specific variables. It introduces novel evaluation metrics—\textbf{Completely Satisfaction Rate (CSR), Soft Satisfaction Rate (SSR), Rigorous Satisfaction Rate (RSR), and Consistent Continuity Satisfaction Rate (CCSR)}—to assess multi-constraint handling across multiple dimensions.

