\section{In-Depth Analysis}

\begin{figure}
    \centering
    \includegraphics[width=.5\textwidth]{figure/model_result_instruct_type.png}
    \caption{Performance of different LLMs on the CodeIF evaluation across instruction categories, measured by SSR.}
    \label{fig:model_result_instruct_type}
\end{figure}

\subsection{Performance Analysis Across Instruction Types}
Figure~\ref{fig:model_result_instruct_type} illustrates the performance of LLMs across different instruction categories, revealing notable disparities among models. \textbf{DeepSeek-V3} consistently achieves the highest scores across most evaluation dimensions, with a particularly strong performance in combination tasks (\textbf{0.831}), highlighting its effectiveness in handling complex instructions. However, its strengths are not uniform; it excels in \textbf{global structure control} but shows relative weaknesses in \textbf{variable handling}, likely reflecting its specific optimization strategies.

Conversely, \textbf{Meta's Llama series} demonstrates a significant correlation between model size and performance. While smaller configurations (\textit{Llama-3.2-1B-Instruct}) score lower, larger variants (\textit{Llama-3.3-70B-Instruct}) perform significantly better, showcasing substantial gains with increased parameter counts. However, model size alone is not the sole determinant of performance. Comparisons between similarly sized models, such as \textbf{Google's Gemma} and \textbf{Meta's Llama}, indicate distinct strengths in specific metrics, suggesting that architectural design and training methodologies also play a crucial role.

\subsection{Cross-Language Performance Analysis of LLMs}

\begin{figure*}
    \centering
    \includegraphics[width=.7\textwidth]{figure/model_result_lang.png}
    \caption{SSR scores of LLMs across different programming languages in the CodeIF evaluation.}
    \label{fig:model_result_lang}
\end{figure*}

Figure~\ref{fig:model_result_lang} compares the performance of leading LLMs across four programming languages: C++, Java, Python, and Go, highlighting key trends at both the model and language levels.

At the \textbf{model level}, \textbf{DeepSeek-V3} consistently outperforms others, achieving the highest \textbf{CCSR} in C++ (0.725), Java (0.753), and Go (0.722), with an \textbf{RSR} of 0.787 in Java, likely benefiting from pretraining optimizations for code continuity. \textbf{Claude-3-5-Sonnet} excels in Java, attaining the highest \textbf{CSR} (0.504) and \textbf{RSR} (0.749), indicating specialized mechanisms for handling complex constraints. However, its \textbf{SSR} in Python (0.703) is relatively lower, suggesting challenges in cross-language generalization. \textbf{GPT-4O} demonstrates balanced performance across languages, ranking second in \textbf{CSR} (0.355) and \textbf{RSR} (0.682) for Python, with minimal variation across different programming paradigms (coefficient of variation \textbf{CV = 0.18}).

At the \textbf{language level}, C++ presents the most significant challenges, likely due to its complex \textbf{template metaprogramming}, which increases constraint difficulty. Java exhibits the highest inter-model variance, with \textbf{Claude-3-5-Sonnet} standing out, possibly due to optimizations tailored to its strong type system. Go achieves the highest overall \textbf{SSR}, benefiting from its simple syntax, but \textbf{RSR} fluctuates significantly, reflecting differences in how models handle concurrency constraints.

These findings highlight variations in LLMs' ability to generalize across languages and suggest potential areas for optimization, such as improving handling of dependency constraints and enhancing consistency across different programming paradigms.



\subsection{Analysis of Instruction Adherence Deviations}

Analysis of model-generated responses reveals frequent deviations from specific instructions, particularly in \textbf{naming conventions} and \textbf{formatting constraints}. Models often overlook global formatting rules, such as line limits and character restrictions, when handling general structure instructions.

Moreover, adherence to naming conventions remains inconsistent. For instance, even when explicitly instructed to use \textbf{PascalCase} for variable names, models frequently generate results in lowercase or underscore-separated formats (e.g., transforming \texttt{current\_power} into \texttt{CurrentPower} incorrectly).

A particularly notable issue is the tendency to disregard \textbf{prohibitive instructions}. For example, when explicitly instructed to avoid \texttt{if} statements in decision-making—favoring ternary operators or data structures (e.g., dictionaries)—models still generate outputs containing \texttt{if} statements, indicating gaps in constraint enforcement.

\subsection{Enhance Model Instruction Compliance}

Insights from Appendix Table~\ref{tab:codeIF_evaluation_all} highlight key strategies for improving instruction adherence in code generation. \textbf{Supervised Fine-Tuning (SFT)} has proven effective, particularly in the Llama series, while \textbf{model scale} remains a crucial factor—larger models like \texttt{Qwen2.5-72B-Instruct} consistently outperform smaller counterparts such as \texttt{Qwen2.5-3B-Instruct} in instruction-following accuracy.

To enhance adherence, several approaches can be employed. First, classifying instructions by priority ensures strict enforcement of \textit{hard constraints} (e.g., syntax restrictions) while allowing flexibility for \textit{soft guidelines} (e.g., coding styles). Patterned code generation can provide structured alternatives, such as replacing conditional statements with lookup tables or state machines. Integrating a naming convention engine allows automated conversion of variable and class names (e.g., transforming \texttt{snake\_case} to \texttt{PascalCase}). Additionally, leveraging \textit{Abstract Syntax Tree (AST)} analysis helps detect and transform prohibited structures, such as replacing \texttt{for} loops with \texttt{while} loops or recursive calls. Finally, conflict resolution mechanisms can be introduced to handle contradictory user instructions, such as providing alternative solutions when certain language features are unavailable (e.g., requesting \texttt{switch-case} in Python).


