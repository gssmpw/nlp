\section{Experiment}

\subsection{Baselines}
We evaluate over 30 language models, including both open-source models and commercial API services.

\begin{itemize}
    \item \textbf{Meta Llama 3 Series~\cite{touvron2023llama}}: Includes \textit{Llama-3.2-1B-Instruct}, \textit{Llama-3.2-3B-Instruct}, \textit{Llama-3.1-8B-Instruct}, \textit{Llama-3.1-70B-Instruct}, and \textit{Llama-3.3-70B-Instruct}.
    \item \textbf{Qwen2.5 Series~\cite{yang2024qwen2}}: Includes \textit{Qwen2.5-1.5B-Instruct}, \textit{Qwen2.5-3B-Instruct}, \textit{Qwen2.5-7B-Instruct}, \textit{Qwen2.5-14B-Instruct}, \textit{Qwen2.5-32B-Instruct}, and \textit{Qwen2.5-72B-Instruct}. Additionally, it includes dedicated code generation models: \textit{Qwen2.5-Coder-1.5B-Instruct}~\cite{hui2024qwen25codertechnicalreport}, \textit{Qwen2.5-Coder-3B-Instruct}, \textit{Qwen2.5-Coder-7B-Instruct}, \textit{Qwen2.5-Coder-14B-Instruct}, and \textit{Qwen2.5-Coder-32B-Instruct}.
    \item \textbf{Mistral Series~\cite{Jiang2023Mistral7}}: Includes \textit{Mistral-3B}, \textit{Mistral-8B}, and the specialized code model \textit{Codestral-2501}.
    \item \textbf{Microsoft Phi Series~\cite{abdin2024phi4technicalreport}}: Includes \textit{Phi-3.5-Mini-128K-Instruct (3.8B)} and the latest \textit{Phi-4}.
    \item \textbf{Google Gemma Series~\cite{gemmateam2024gemmaopenmodelsbased}}: Includes \textit{Gemma-2-9B-It} and \textit{Gemma-2-27B-It}.
    \item \textbf{DeepSeek Series}: Includes \textit{DeepSeek-Coder}~\cite{guo2024deepseek} and \textit{DeepSeek-V3}~\cite{deepseekai2024deepseekv3technicalreport}.
    \item \textbf{OpenAI Series~\cite{achiam2023gpt4}}: Includes \textit{GPT-3.5-Turbo}, \textit{GPT-4O-Mini}, \textit{GPT-4O-2024-05-13}, and the latest \textit{GPT-4O-2024-11-20}.
    \item \textbf{Google Gemini Series~\cite{geminiteam2024geminifamilyhighlycapable}}: Includes \textit{Gemini-2.0-Flash-Exp}, \textit{Gemini-Exp-1206}, and \textit{Gemini-1.5-Pro}.
    \item \textbf{Anthropic Series}: Includes \textit{Claude-3.5-Sonnet-20241022}.
\end{itemize}

\subsection{Experimental Setup}
The temperature coefficient is set to 0 to ensure output determinism, with a maximum generation length of 4096 tokens. All other settings follow the official default parameters for each model. Commercial API models are accessed through the latest available interface as of December 2024. Specifically, this study evaluates the \textbf{Qwen2.5-Coder series}, Meta's latest \textbf{Llama 3.3} architecture, and the \textbf{DeepSeek-V3} large-scale dialogue model as upper-bound benchmarks. All experiments are conducted using the official API and 8 H800(80G).

\subsection{Automatic Evaluation}
To ensure robust evaluation, we employed both large language models (LLMs) and human experts to assess whether each model correctly followed the atomic constraint instructions. During data preprocessing, all constraints were decomposed into atomic elements, allowing the adjudication system to perform objective binary evaluations (\textit{Yes/No}) instead of relying on subjective judgment.

Follow by FairEval~\cite{faireval}, \textit{GPT-4-1106-Preview} model served as the primary automated evaluation tool (see Appendix~\ref{prompt:judge_prompt} for the detailed prompt). Additionally, three domain experts manually annotated a stratified sample of 150 instances. Statistical analysis revealed strong agreement between LLM-based and human evaluations, with Pearson correlation coefficients of \textbf{0.87} (LLM-human) and \textbf{0.85} (inter-human), demonstrating high consistency between automated and manual assessments. 

These results suggest that the atomic constraint decomposition methodology effectively reduces subjectivity, making LLM-based evaluation a reliable and scalable approach for constraint verification tasks.



\subsection{Main Experiments}

\begin{table*}[t]
\centering
\caption{CodeIF evaluation results of different difficulties. We use bold font to mark the best results in all models.}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{ccccccccccccc}
\toprule
\multirow{2}{*}{Models} & \multicolumn{3}{c}{CSR} & \multicolumn{3}{c}{SSR} & \multicolumn{3}{c}{RSR} & \multicolumn{3}{c}{CCSR} \\
\cmidrule(lr){2-13} 
 & Full & Easy & Hard & Full & Easy & Hard & Full & Easy & Hard & Full & Easy & Hard \\
\midrule
Llama-3.2-1B-Instruct & 0.034 & 0.046 & 0.022 & 0.218 & 0.277 & 0.159 & 0.182 & 0.231 & 0.132 & 0.152 & 0.197 & 0.107 \\
Llama-3.1-8B-Instruct & 0.145 & 0.187 & 0.102 & 0.467 & 0.544 & 0.388 & 0.418 & 0.493 & 0.340 & 0.370 & 0.444 & 0.295 \\
Qwen2.5-Coder-7B-Instruct & 0.142 & 0.198 & 0.087 & 0.514 & 0.590 & 0.438 & 0.453 & 0.533 & 0.373 & 0.390 & 0.463 & 0.318 \\
Qwen2.5-7B-Instruct & 0.153 & 0.201 & 0.104 & 0.535 & 0.599 & 0.471 & 0.475 & 0.546 & 0.405 & 0.416 & 0.479 & 0.353 \\
Ministral-8B & 0.161 & 0.205 & 0.116 & 0.552 & 0.614 & 0.489 & 0.486 & 0.552 & 0.419 & 0.431 & 0.490 & 0.371 \\
Gemma-2-9B-It & 0.171 & 0.210 & 0.131 & 0.573 & 0.642 & 0.504 & 0.513 & 0.587 & 0.440 & 0.445 & 0.508 & 0.383 \\
Qwen2.5-Coder-32B-Instruct & 0.365 & 0.422 & 0.307 & 0.736 & 0.767 & 0.704 & 0.679 & 0.723 & 0.635 & 0.634 & 0.669 & 0.599 \\
Gemma-2-27B-It & 0.245 & 0.300 & 0.190 & 0.658 & 0.709 & 0.607 & 0.596 & 0.652 & 0.540 & 0.533 & 0.588 & 0.478 \\
Qwen2.5-32B-Instruct & 0.294 & 0.337 & 0.251 & 0.680 & 0.722 & 0.638 & 0.621 & 0.674 & 0.568 & 0.560 & 0.604 & 0.515 \\
Qwen2.5-72B-Instruct & 0.281 & 0.319 & 0.244 & 0.685 & 0.734 & 0.634 & 0.621 & 0.677 & 0.564 & 0.569 & 0.619 & 0.518 \\
Llama-3.3-70B-Instruct & 0.307 & 0.359 & 0.255 & 0.698 & 0.749 & 0.647 & 0.632 & 0.691 & 0.574 & 0.589 & 0.643 & 0.536 \\
Gemini-Exp-1206 & 0.357 & 0.410 & 0.303 & 0.744 & 0.781 & 0.707 & 0.685 & 0.734 & 0.636 & 0.636 & 0.675 & 0.597 \\
GPT-4O-2024-11-20 & 0.383 & 0.441 & 0.325 & 0.748 & 0.792 & 0.702 & 0.689 & 0.745 & 0.633 & 0.650 & 0.698 & 0.602 \\
Claude-3-5-Sonnet-20241022 & \textbf{0.444} & \textbf{0.525} & \textbf{0.362} & 0.727 & 0.784 & 0.669 & 0.692 & 0.757 & 0.626 & 0.652 & 0.715 & 0.587 \\
Deepseek-V3 & 0.414 & 0.468 & 0.359 & \textbf{0.821} & \textbf{0.847} & \textbf{0.794} & \textbf{0.764} & \textbf{0.806} & \textbf{0.723} & \textbf{0.712} & \textbf{0.743} & \textbf{0.680} \\
\bottomrule
\end{tabular}}
\label{tab:code_if_evaluation_selected}
\end{table*}


Table~\ref{tab:code_if_evaluation_selected} provides a comprehensive evaluation of CodeIF. The results are measured using four key metrics: \textbf{Complete Satisfaction Rate (CSR)}, \textbf{Soft Satisfaction Rate (SSR)}, \textbf{Rigorous Satisfaction Rate (RSR)}, and \textbf{Consistent Continuity Satisfaction Rate (CCSR)}. More detailed results are in Appendix~\ref{app:moreresults}.

\paragraph{Result Overview} 
\textbf{DeepSeek-V3} and \textbf{Claude-3-5-Sonnet-20241022} consistently achieve top scores across all difficulty levels and evaluation metrics, excelling in complex tasks. However, even the highest \textbf{CSR} on the Hard dataset is only \textbf{0.362}, highlighting room for improvement in handling strict constraints.

\paragraph{Model Scale and Performance Trends} 
Larger models generally perform better across all metrics. In the \textbf{Qwen2.5} series (\textit{Qwen2.5-1.5B-Instruct} to \textit{Qwen2.5-72B-Instruct}), performance improves with model size. However, this trend is not universal. In the \textbf{Llama3} series, larger models do not always achieve the best results, suggesting that \textbf{architecture design, training data quality, and optimization strategies} are equally critical factors.

\paragraph{Open-Source vs. Closed-Source Models} 
Closed-source models like \textbf{GPT-4O} and \textbf{Claude-3-5-Sonnet-20241022} outperform their open-source counterparts, maintaining higher accuracy under complex constraints. While some large open-source models (e.g., \textbf{Qwen2.5-72B-Instruct}) show competitive results, they still lag behind. This performance gap likely stems from advantages in \textbf{data quality, optimization techniques}, and \textbf{reinforcement learning from human feedback (RLHF)}, which proprietary models leverage for enhanced instruction-following precision.



% \paragraph{Metrics Comparison}
% \begin{itemize}
%     \item The relatively high CSR and SSR values suggest that most models can effectively follow individual or partial instructions in code generation tasks.
%     \item In contrast, the lower RSR scores indicate that models struggle with instructions that involve dependencies.
%     \item CCSR scores are generally the lowest, particularly for more complex tasks, highlighting the difficulty of maintaining consistency and continuity in instruction adherence.
% \end{itemize}

\paragraph{Impact of Task Difficulty}
Table~\ref{tab:code_if_evaluation_selected} shows that as task complexity increases (from Easy to Hard), the performance of nearly all models declines. For example, \textbf{GPT-4O-2024-11-20} achieves a CSR of \textbf{0.441} on Easy tasks, but this drops to \textbf{0.325} on Hard tasks. This trend confirms that higher constraint levels impose greater challenges, reducing instruction-following accuracy. These findings align with previous research, which suggests that modelsâ€™ ability to adhere to instructions deteriorates as task difficulty increases.

% \paragraph{Future Directions}
% These results provide valuable insights into the instruction-following capabilities of LLMs in code generation and highlight key areas for improvement. Future research could focus on:
% \begin{itemize}
%     \item Enhancing model performance when handling complex and interdependent instruction sets.
%     \item Improving consistency in instruction adherence over extended sequences.
%     \item Developing more effective training strategies or architectural refinements to boost instruction-following capabilities.
% \end{itemize}

