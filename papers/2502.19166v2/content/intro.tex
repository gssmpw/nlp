\section{Introduction}
% \hc{1. Introduce llm and sns, the absence of llm in sns. 2. Why we need SNS-bench 3. contributions{1. first sns bench 2.overview evaluation 3.different insights }}



% \begin{quote}
%     \large\textit{``The true art of socializing is not merely conversing with others, but establishing resonance with them.''} \textbf{-- Carl Jung}
% \end{quote}


With the rapid advancement of large language models (LLMs), automated code generation is undergoing a profound transformation. While LLMs have demonstrated promising capabilities in programming tasks, their ability to comprehend and execute complex instructions remains a challenge~\cite{liu2024fullstackbenchevaluatingllms, zhang2023repocoder}. To drive progress in this field, a comprehensive and systematic evaluation framework is essential~\cite{jiang2024followbenchmultilevelfinegrainedconstraints, zhou2023instructionfollowingevaluationlargelanguage}.

This study introduces CodeIF, a benchmark designed to assess LLMsâ€™ instruction-following capabilities in code generation. Built upon insights from existing evaluation sets like McEval~\cite{mceval} and FullStackBench~\cite{liu2024fullstackbenchevaluatingllms}, CodeIF is tailored for multi-language environments, covering Java, Python, Go, and C++. It categorizes tasks by difficulty and systematically evaluates models across 50 fine-grained sub-instructions, providing a nuanced understanding of their strengths and weaknesses.

To ensure rigorous assessment, we propose four novel evaluation metrics: Completely Satisfaction Rate (CSR), Soft Satisfaction Rate (SSR), Rigorous Satisfaction Rate (RSR), and Consistent Continuity Satisfaction Rate (CCSR). These metrics measure models' ability to handle multi-constraint problems from different perspectives, including full compliance, average constraint satisfaction, logical coherence, and consistency in instruction execution. By offering a structured evaluation framework, CodeIF provides valuable insights into the current state and future direction of LLM-driven code generation.

Overall, our contributions are mainly four-fold:
\begin{enumerate}
    \item \textbf{Innovative Benchmark}: We introduce \textbf{CodeIF}, the first systematic benchmark for evaluating LLMs' instruction-following capabilities in code generation. CodeIF categorizes tasks into \textbf{8 main types and 50 fine-grained sub-instructions}, ensuring a comprehensive assessment of model performance.
    
    \item \textbf{Automated High-Quality Instruction Generation}: Leveraging advanced LLMs like GPT-4, we develop a method to automatically generate constraint-based instruction lists. This approach enhances evaluation depth by incorporating instructional dependencies while minimizing human intervention.
    
    \item \textbf{Novel Evaluation Metrics}: We propose a new framework with four key metrics (\textbf{CSR, SSR, RSR, and CCSR}) tailored for code generation tasks. These metrics assess models' ability to handle multi-constraint problems across different dimensions, offering deeper insights and new benchmarks for future research.
    
    \item \textbf{Extensive Evaluation and Analysis}: We systematically evaluate \textbf{35 state-of-the-art LLMs}, including both open-source and commercial models, across multiple programming languages and difficulty levels. Our experiments uncover current strengths and limitations, providing clear directions for future advancements.
\end{enumerate}




