\subsection{Building}
The construction of the \bench{} dataset consists of two main phases: collection of constraint instructions and data processing. In Section ~\ref{sec:Constraint_Instructions_Collection}, we elaborate on how we collected the constraint instructions for code generation. Following this, in Section ~\ref{sec:Data_Construction}, we describe how we combined these constraint instructions to produce the final CodeIF evaluation dataset.

\subsection{Constraint Instructions Collection}
\label{sec:Constraint_Instructions_Collection}
% The first step of our work focuses on the domain of code generation by collecting and creating a series of constraint instructions, which are then used to construct the CodeIF evaluation dataset. Specifically, this process is divided into two stages: the first stage involves collection and verification of the constraint instructions, and the second stage deals with applying these constraint instructions to generate the dataset.

% First, we conducted an in-depth analysis of the code generation tasks, based on existing evaluation datasets (such as McEval\cite{mceval} and FullStackBench\cite{liu2024fullstackbenchevaluatingllms}), to build an instruction system covering 8 categories. Each category targets a specific aspect of code generation for fine-grained assessment, aiding in a more comprehensive understanding of large models' capability to follow instructions at various levels of detail.

% Subsequently, within each category, we further subdivided the instructions into smaller units, designing atomic constraint instructions with clear directives. When assessing whether a model adheres to these atomic constraint instructions, answering with ``yes'' or ``no'' achieves precise judgment of the model's compliance, thus avoiding subjective evaluations.

% When selecting atomic constraint instructions and their classification criteria, we primarily based our approach on an in-depth analysis of existing code generation evaluation datasets and popular code repositories. We observed that the distribution of requirements within different categories across these datasets varies significantly at line-level, function-level, and complete solution generation. Therefore, we designed a variety of atomic constraint instruction categories aimed at testing large models' ability to follow detailed instructions while ensuring comprehensive coverage from overall solutions to specific variable control.

% Specifically, to comprehensively evaluate a model's capability to follow instructions in the domain of code generation, we defined the following eight constraint instruction categories.
% \begin{itemize}
%     \item \textbf{Global}: Focuses on constraints for overall solutions, ensuring that generated code complies with predefined holistic specifications.
%     \item \textbf{Structural Control}: Proposes specific requirements for control structures (e.g., for loops, while loops, if statements) and data structures (e.g., Set, Map) in the code, assessing the model's understanding and application capabilities of programming constructs.
%     \item \textbf{Variable}: Concentrate on variable-level details, including naming conventions and initialization methods, and evaluate the precision of the model's handling of variable operations.
%     \item \textbf{Interface}, \textbf{Function}, \textbf{Class}: These three categories focus on instruction follow at the interface, function, and class levels respectively, aiming to assess the model's ability to construct complex program structures.
%     \item \textbf{File}: Involves the capability to handle cross-file or library calls, testing whether the model can correctly manage interactions between multiple source files or external libraries according to instructions.
%     \item \textbf{Combination}: As a comprehensive challenge, it combines constraints from multiple dimensions mentioned above, used to test large models' ability to execute multifaceted instructions in more complex and realistic scenarios.
% \end{itemize}

The first phase of our work focuses on code generation by collecting and formulating constraint instructions to construct the \textbf{CodeIF} evaluation dataset. This process consists of two stages: (1) collecting and verifying constraint instructions and (2) applying them to dataset generation.

We begin with an in-depth analysis of code generation tasks based on existing benchmarks such as \textbf{McEval}~\cite{mceval} and \textbf{FullStackBench}~\cite{liu2024fullstackbenchevaluatingllms}, establishing an instruction system covering \textbf{eight categories}. Each category targets a specific aspect of code generation, enabling a fine-grained assessment of LLMs’ instruction-following abilities.

Within each category, we further refine constraints into \textbf{atomic instructions} with clear, explicit directives. These atomic constraints allow binary evaluation (``yes'' or ``no''), ensuring an objective and precise assessment of model compliance while minimizing subjectivity.

Our classification of atomic constraint instructions is guided by an analysis of existing code generation benchmarks and widely used code repositories. We observe that requirement distribution varies significantly across different levels—line-level, function-level, and full-solution generation. To ensure comprehensive coverage, we design atomic instruction categories that assess both \textbf{high-level architectural constraints} and \textbf{fine-grained variable control}.

To systematically evaluate LLMs’ instruction-following capabilities in code generation, we define \textbf{eight constraint categories}. The \textbf{Global} category ensures that generated code aligns with overarching specifications and holistic constraints. \textbf{Structural Control} focuses on adherence to control structures such as loops and conditionals, as well as data structures like sets and maps. \textbf{Variable} constraints assess details at the variable level, including naming conventions and initialization methods.

At different abstraction levels, \textbf{Interface, Function,} and \textbf{Class} constraints evaluate the model’s ability to construct well-defined program components. The \textbf{File} category tests handling of cross-file dependencies and external library calls. Finally, the \textbf{Combination} category presents a comprehensive challenge by integrating constraints from multiple dimensions, assessing how well models follow complex, multi-faceted instructions in realistic scenarios.





\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figure/instruction_analysis.png}
    % \vspace{-1.1em}
    \caption{ CodeIF Constraints Instruction Distribution}
    % \vspace{-1.2em}
    \label{fig:instruction_analysis}
\end{figure}

% As shown in Figure~\ref{fig:instruction_analysis}, the distribution of CodeIF in different programming languages and categories.

% In our research, through a detailed analysis and classification of code generation evaluation datasets, we constructed an assessment system encompassing 8 main categories with a total of 50 specific subdivided instructions. This system not only aids in systematically understanding and improving the performance of large language models in code generation but also provides clear direction and benchmarks for future research.

% These subdivided instructions cover various constraints ranging from overall solutions to variable-level details, ensuring a comprehensive and meticulous evaluation of model capabilities. For instance, under the ``Global'' category, we designed multiple instructions to test the model's grasp of the overall structure and logical coherence of generated code; whereas, within the ``Variable'' category, a series of instructions were employed to assess the model's precision in handling variable naming and initialization.

% Through such detailed categorization and subdivision, we can more accurately identify the strengths and weaknesses of different large language models across various dimensions, guiding optimization directions and propelling the field towards more efficient and intelligent automated code generation technologies. Furthermore, this structured assessment approach offers a valuable reference framework for future researchers, which encourages continuous innovation and progress in this domain. The atomic constraint instructions list can be seen in the appendix table~\ref{tab:Constraint_Instruction_Tabl}.

As shown in Figure~\ref{fig:instruction_analysis}, CodeIF's distribution across different programming languages and categories is presented.

Through a detailed analysis of existing code generation benchmarks, we designed an evaluation system comprising \textbf{8 main categories} and \textbf{50 fine-grained constraint instructions}. This system not only enables a systematic assessment of LLMs' performance in code generation but also establishes clear benchmarks for future research.

These instructions cover constraints ranging from high-level structural guidelines to fine-grained variable operations, ensuring a comprehensive evaluation. For instance, the \textbf{Global} category assesses a model’s ability to maintain overall structural integrity and logical coherence, while the \textbf{Variable} category evaluates its precision in variable naming and initialization.

By refining constraints into well-defined categories, we can precisely identify the strengths and weaknesses of different LLMs, guiding optimization efforts and advancing the field toward more effective automated code generation. Additionally, this structured evaluation approach provides a valuable reference for future research, fostering continuous innovation. The complete list of atomic constraint instructions is provided in Appendix Table~\ref{tab:Constraint_Instruction_Tabl}.



\subsection{Data Construction}
\label{sec:Data_Construction}

% \begin{table*}[]
% \small
% \begin{tabular}{|c|l|l|l|}
% \toprule
% \multicolumn{4}{|l|}{\textbf{Task:} Implement a caching module with an LRU (Least Recently Used) replacement policy.} \\
% \midrule
% \textbf{ID} & \textbf{Type} & \textbf{Dependence} & \textbf{Instruction} \\
% \midrule

% 1 & global & [] & Your code should be written in C++. \\
% 2 & global & [1] & Your answer in total should not exceed 50 lines. \\
% 3 & global & [1] & Your code should not use the \textbf{mutable} keyword. \\
% 4 & structural control & [1] & Your code should not use data structure \textbf{std::unordered\_map}. \\
% 5 & structural control & [1] & Your code should use for-loop and not use \textbf{while} keyword. \\
% 6 & variable & [1] & Your code should define a variable named \textbf{cacheSize}. \\
% 7 & variable & [1, 6] & Variable \textbf{cacheSize}, type should be \textbf{size\_t}. \\
% 8 & function & [1] & Your code should not use any functions from the namespace \textbf{std}. \\
% 9 & interface & [1] & Your code should define an interface named \textbf{CacheInterface}. \\
% 10 & class & [1] & Your code should define a class named \textbf{LRUCache}. \\
% 11 & file & [1] & Your code should be organized in namespace named \textbf{EasyCache}. \\
% 12 & combination & [1, 9, 10] & Your code should define a class named \textbf{LRUCache} that implements the \textbf{CacheInterface} interface. \\
% 13 & combination & [1, 10] & In your code, the class \textbf{LRUCache} should have these properties: \textbf{capacity}, \textbf{ttl}, \textbf{cacheMap}, and \textbf{accessList}. \\
% 14 & combination & [1, 10] & In your code, the class \textbf{LRUCache} should have these methods: \textbf{size}, \textbf{add}, and \textbf{get}. \\
% \bottomrule
% \end{tabular}
% \end{table*}

\paragraph{Multi-Language and Difficulty-Differentiated Benchmark Design}
To ensure diversity and comprehensiveness in evaluation, we carefully selected code generation tasks across four mainstream programming languages—Java, Python, Go, and C++—from leading benchmarks such as \textbf{McEval}~\cite{mceval} and \textbf{FullStackBench}~\cite{liu2024fullstackbenchevaluatingllms}. These languages, spanning both dynamic and static paradigms, create a rich linguistic environment that enhances multi-language assessment.

To further refine the evaluation, we categorize tasks into two difficulty levels: \textbf{Easy} and \textbf{Hard}. The \textbf{Hard} set includes longer, more intricate instruction lists, designed to rigorously test LLMs’ ability to handle complex constraints.

\paragraph{Automated Generation of Constraint Instructions}
To construct high-quality constraint instructions, we leveraged large language models (LLMs) such as \textbf{GPT-4} to automatically generate task-specific instruction lists. Initially, we curated approximately 20 detailed examples, each featuring a specific code generation task. Human experts then formulated atomic constraint instructions to ensure precise adherence during code generation.

These examples served as prompts, allowing LLMs to generate corresponding instruction lists while refining task descriptions by identifying and eliminating redundant constraints. This approach streamlined instruction-following requirements, ensuring clarity and promoting high-quality outputs.

\paragraph{Constructing Instruction Dependencies}
Since some atomic constraints exhibit interdependencies, we further utilized LLMs to construct explicit dependency relationships within instruction sets. These dependencies were integrated into our evaluation framework, enhancing assessment depth and improving the accuracy and practicality of constraint verification.


Specifically, by analyzing the logical connections and sequential dependencies among atomic instructions, we can precisely define the execution steps required for each task. For instance, in function generation tasks, the process typically involves defining the function name first, specifying parameter types, and finally implementing the function body. These dependencies ensure coherence and consistency throughout the code generation process.

Integrating these dependencies into the evaluation system enables a more accurate assessment of a model's ability to follow complex instructions while providing a foundation for future optimizations. Additionally, this approach helps identify potential challenges models face when handling specific instruction sequences, offering insights for further improvement. As shown in Figure~\ref{fig:CodeIF_case}, this example from \textbf{CodeIF} illustrates a specific generation task, its associated instruction list, and the corresponding instruction dependencies.


\begin{figure}
    \centering
    \includegraphics[width=.4\textwidth]{figure/CodeIF1.pdf}
    % \vspace{-1.1em}
    \caption{  Specific cases of the CodeIF dataset, 'Task' denotes the specific generation task, 'Type' refers to the type of constraint, and 'Dependence' indicates the prerequisite constraints for this constraint.}
    % \vspace{-1.2em}
    \label{fig:CodeIF_case}
\end{figure}

\subsection{Data Analysis}

% \begin{figure}
%     \centering
%     \includegraphics[width=.5\textwidth]{figure/item_set_instruct_type.png}
%     % \vspace{-1.1em}
%     \caption{Overview of different of item set.}
%     % \vspace{-1.2em}
%     \label{fig:main}
% \end{figure}



\begin{table}[!t]
\centering
\scriptsize
\caption{CodeIF dataset statistics, showing the statistical information of different difficulty classifications. Avg.Instr represents the average length of the atomic constraint instruction list.}
\renewcommand{\arraystretch}{1} % Adjust row height
\resizebox{\linewidth}{!}{ % Resize table to fit width
\begin{tabular}{c|c|c|c|c|c|c}
\hline
Set & Num & Avg.Instr & Go & Python & Java & C++ \\
\hline
Easy  & 600  & 11.99 & 127 & 165 & 176 & 132 \\
\hline
Hard  & 600  & 13.80 & 103 & 183 & 177 & 137 \\
\hline
Full  & 1200 & 12.90 & 230 & 348 & 353 & 269 \\
\hline
\end{tabular}
}
\label{tab:dataset_stats}
\end{table}



\paragraph{ CodeIF Static Analysis}
\begin{figure}
    \centering
    \includegraphics[width=.4\textwidth]{figure/instruction_len_distribution_v2.png}
    % \vspace{-1.1em}
    \caption{ The distribution of atomic instruction list lengths in datasets of different difficulties.}
    % \vspace{-1.2em}
    \label{fig:instruction_len_distribution_v2}
\end{figure}

% As shown in Table ~\ref{tab:dataset_stats}, the dataset is divided into three categories based on difficulty levels: "Easy", "Hard", and an aggregated dataset "Full". Each of the first two difficulty levels contains the same number of code generation tasks: 600 tasks each for Easy and Hard, while the Full dataset includes a total of 1,200 tasks. These tasks span four programming languages: Go, Python, Java, and C++.

% Specifically, the Java category has the highest number of tasks, totaling 353; Python follows with 348 tasks. The task counts for C++ and Go are 269 and 230, respectively. The dataset records the average number of instructions for each difficulty level and tallies the number of tasks under each programming language. For the Easy level, the average number of instructions is 11.99. In contrast, the Hard level sees an increase to 13.8, indicating higher task complexity and the need for managing more instructions. The Full dataset aggregates tasks from both difficulty levels, with an average instruction count of 12.9. Figure ~\ref{fig:instruction_len_distribution_v2} shows the distribution of task lengths.
As shown in Table~\ref{tab:dataset_stats}, the dataset is categorized into three difficulty levels: \textbf{Easy}, \textbf{Hard}, and an aggregated set, \textbf{Full}. The Easy and Hard sets each contain 600 code generation tasks, while the Full dataset combines both, totaling 1,200 tasks. These tasks span four programming languages: Go, Python, Java, and C++.

Among them, \textbf{Java} has the highest number of tasks (353), followed by \textbf{Python} (348). \textbf{C++} and \textbf{Go} contain 269 and 230 tasks, respectively. The dataset also records the average number of instructions per task for each difficulty level. The Easy set averages \textbf{11.99} instructions per task, while the Hard set increases to \textbf{13.8}, reflecting greater complexity and stricter constraints. The Full dataset, combining both, has an average instruction length of \textbf{12.9}. Figure~\ref{fig:instruction_len_distribution_v2} illustrates the distribution of task lengths across the dataset.



\begin{table*}[!t]
\centering
\scriptsize
\caption{CodeIF dataset statistics information, showing the distribution of atomic restriction instruction categories under different difficulty classifications.}
\renewcommand{\arraystretch}{1} % Adjust row height
\resizebox{0.8\textwidth}{!}{ % Resize table to fit width
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\hline
Set & Global & Structural Control & Variable & Interface & Function & Class & File & Combination \\
\hline
\textbf{Easy} & 1638 & 1008 & 1336 & 427 & 569 & 544 & 723 & 953 \\
\hline
\textbf{Hard} & 1890 & 1193 & 1479 & 505 & 659 & 623 & 802 & 1142 \\
\hline
\textbf{Full} & 3528 & 2201 & 2815 & 932 & 1228 & 1167 & 1525 & 2095 \\
\hline
\end{tabular}
}
\label{tab:codeif_dataset_statistics_information}
\end{table*}


\begin{figure}
    \centering
    \includegraphics[width=.4\textwidth]{figure/data_set_instruct_type.png}
    % \vspace{-1.1em}
    \caption{ The distribution of constraint instruction list lengths in datasets of different difficulties.}
    % \vspace{-1.2em}
    \label{fig:data_set_instruct_type}
\end{figure}

\paragraph{Analysis of Constraint Instruction Category Distribution}

Table~\ref{tab:codeif_dataset_statistics_information} presents the distribution of atomic instructions across different difficulty levels in the evaluation set. As shown in Figure~\ref{fig:data_set_instruct_type}, we compare the average number of instructions per category between the ``Easy'' and ``Hard'' sets. These categories include \textbf{Global, Structural Control, Variable, Interface, Function, Class, File,} and \textbf{Combination}. 

Across all categories, the ``Hard'' dataset contains a higher average number of instructions than the ``Easy'' set. Notably, the \textbf{Global} category has the highest instruction count in both difficulty levels, averaging \textbf{2.5} instructions in the ``Easy'' set and exceeding \textbf{3} in the ``Hard'' set. This trend indicates that as task complexity increases, models face greater challenges in handling intricate constraints and detailed requirements. By analyzing instruction distribution across categories, we gain deeper insights into model performance under varying levels of difficulty, providing a foundation for further optimization.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figure/data_set_type_v2.png}
    \caption{Distribution of atomic instruction list lengths across difficulty levels.}
    \label{fig:data_set_type}
\end{figure}

Figure~\ref{fig:data_set_type} illustrates the proportion of each atomic instruction category. The \textbf{Global} category holds the largest share at \textbf{22.77\%}, followed by the \textbf{Variable} category at \textbf{18.17\%}. This distribution highlights \textbf{CodeIF}’s focus on evaluating LLMs' capabilities at both the overall structural level (e.g., Global constraints) and fine-grained variable handling (e.g., Variable constraints). This design ensures a balanced assessment of code generation, emphasizing both high-level logical coherence and precision in variable management.


% \subsection{Evaluation Protocol}

