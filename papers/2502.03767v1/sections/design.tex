\section{Design}
% Driven by the design requirements from the \hyperref[formative-study]{Formative Study}, this section began with a thematic analysis to explore the patterns of collective knowledge in online science videos. Based on the insights and data derived from the analysis, we proposed an NLP pipeline to process danmaku, based on which we implemented CoKnowledge, an interactive system to support viewers' assimilation of collective knowledge in science videos.

To meet the aforementioned design requirements, we must ground our proposed features in an empirical understanding of the danmaku content. We thus started with a content analysis to explore the patterns of collective knowledge in online science videos with time-synced comments. Based on the insights and labeled data, we proposed a natural language processing (NLP) pipeline to automatically filter, classify, cluster danmaku, and map them to the most relevant video segments. On top of the pipeline, we designed CoKnowledge, an interactive system to support viewers' assimilation of collective knowledge in science videos.


\subsection{Content Analysis}
\label{content-analysis}
\subsubsection{Data Collection}
We collected the data for our content analysis from Bilibili's "Knowledge Zone" channel. 
% \xm{"section" may be confused with paper section..} 
% , where 45\% of the videos are reported to pertain to knowledge sharing \cite{bilibili2021}. 
According to \cite{zhang2023understanding, he2024engage}, scientific knowledge videos can be categorized into seven distinct domains based on the knowledge-related hashtags in this channel: \textit{health and medical science, life sciences, earth and space sciences, social sciences and arts, mathematics and physics, computer science}, and \textit{chemistry and material sciences}.
% This section features a total of 33 hashtags designed to organize scientific knowledge. Following \cite{zhang2023understanding, he2024engage}, seven distinct domains encompassing all 33 hashtags were identified for scientific knowledge videos: \textit{health and medical science, life sciences, earth and space sciences, social sciences and arts, mathematics and physics, computer science, and chemistry} and \textit{material sciences}.
To ensure comprehensive topic coverage, we employed the official Bilibili API to crawl five videos and their corresponding danmaku. The selected videos adhered to the following criteria: (1) a minimum duration of three minutes to guarantee sufficient content, (2) over 200 danmaku comments to enhance the likelihood of capturing substantial collective knowledge, and (3) a production date within the last five years (2019 or later) to reflect current trends in science video production. The data collection occurred on February 21, 2024, yielding 28,088 danmaku comments for the 35 videos. Detailed information on each video and its corresponding danmaku is provided in the supplementary material. 
% \xm{how many danmaku in total for the 35 videos did you gather? Say that detailed information about every video and its danmaku is presented in the supplementary material...}

\subsubsection{Content Analysis Pipeline}
% Following the video selection, two authors manually coded 3,000 randomly sampled danmaku comments to explore their informative potential. 
Two authors manually coded 3,000 danmaku comments randomly sampled from the dataset to explore their informative potential.
% For each comment, they assigned a code based on a joint analysis of the video context, which involved locating the timestamp of the danmaku's appearance and watching a brief segment to understand the context of the danmaku.
Initially, the coders independently reviewed 300 samples to gain a preliminary understanding of danmaku quality. Through discussion, they defined \textit{`knowledge danmaku'} as live comments that facilitate the construction or digestion of collective knowledge. This definition encompassed danmaku which inherently presented knowledge as well as promoted the understanding of other knowledge. 
% Subsequently, they independently labeled the first 300 samples to assess inter-rater reliability. 
Subsequently, they independently assigned one-hot \textit{`knowledge danmaku'} labels to the first 300 samples. For each comment, they located the timestamp of its appearance and watched a brief video segment to determine its relevance to the presented knowledge. 
% Utilizing Cohen’s kappa \cite{tavakol2011making}, a statistical measure to quantify agreement among coders, the two coders achieved a high inter-rater reliability score of 0.96. 
The two coders achieved a high inter-rater reliability score of 0.96 measured in Cohen’s kappa \cite{tavakol2011making}. 
Following this validation, they resolved the conflicts and coded the remaining 2,700 samples, ultimately identifying 1,251 \textit{knowledge danmaku} from the entire set.

% \xm{Be sure these are the correct steps...} 
We further adopted an integrated deductive and inductive approach to conducting a content analysis on the \textit{knowledge danmaku} pool regarding their information themes. 
% The deductive aspect draws on the foundational work of Wu \cite{wu2019danmaku}and He \cite{he2021beyond}, which informed the development of our inductive content analysis. 
More specifically, we deductively started with predefined codes from existing literature on Wu \cite{wu2019danmaku} and He \cite{he2021beyond} and then inductively developed new ones that may emerge from the context of science videos. 
% This inductive component allows for the possibility of revising existing themes and incorporating new themes that may emerge from the context of science videos \cite{hadi2022gamification}.
After several rounds of discussion and iterations on the existing codes as sifting through 1000 data samples, the two coders reached a consensus on the final codebook, achieving saturation. They identified five dominant categories of \textit{knowledge danmaku} with respect to the video content (details presented in Table \ref{information-theme}).
After finalizing the codebook, the coders independently labeled 100 samples to validate inter-rater reliability, achieving a Cohen’s kappa of 0.90. With such verification, they proceeded to code the remaining samples with theme labels, which were subsequently used for building classifiers. 
% \xm{Did you also count the occurrence of each category? If so, you may want to add the information to Table 2. This is related to your later claim that interpretation is the biggest category...} 

\begin{table*}[h!]
\centering
\begin{tabular}{p{3.4cm} p{4cm} p{5cm} p{1.2cm}}
\hline
Information category     & Definition                                                                                                         & Example         & Frequency                                                                                                                                                                                                       \\
\hline
Interpretation          & Articulating personal opinions and analyses of the science video content.                   & \textit{Russia invests heavily in scientists; bringing over one Euler would recoup all the expenses.}      & 54.2\%                                                                                                                    \\\hline
Inquiry                 & Formulating questions pertaining to the content of the science video.                       & \textit{Why is the probability of irrational numbers equal to 1?}    & 13.2\%                                                                                                                                                           \\\hline
Experience sharing      & Contributing personal experiences that relate to the themes addressed in the science video. & \textit{My mother passed away from this disease, which was discovered to be liver metastasis. I hope that one day the world can eradicate cancer.}     & 11.8\%                                                                         \\\hline
Concept noting          & Identifying and emphasizing concepts relevant to the content of the science video.          & \textit{D'Alembert's criterion.}          & 11.6\%                                                                                                                                                                                      \\\hline
Supplementary knowledge & Providing additional background information to complement the video knowledge.              & \textit{"Han" refers to a geographical location; its original meaning pertains to the Han River, which later extended to denote the regions through which the Han River flows, and subsequently acquired additional meanings.}  & 9.3\%
\\
\hline
\end{tabular}
\caption{Danmaku's information categories with regard to science video content, along with their definitions, examples, and frequencies in our labeled samples.}
\label{information-theme}
\end{table*}


\subsubsection{Verification of Danmaku's Distinctive Value}
To verify danmaku's potential for facilitating fine-grained knowledge co-construction in online science videos compared to traditional comments, we randomly selected one video from each of seven domains within the video pool and collected their transcript, traditional comments, and danmaku. Utilizing Named Entity Recognition (NER), we identified entities in the video transcripts and labeled whether they were addressed by danmaku or traditional comments. An entity was considered addressed if it met either of the following criteria: (1) an exact match appeared in the danmaku or comments, or (2) the entity was referenced through an abbreviation, alias, synonym, or alternative expression in the danmaku or comments.
Entity coverage rates for danmaku and traditional comments were statistically compared using the Wilcoxon Signed Rank test \cite{woolson2005wilcoxon}. The results revealed that danmaku achieved significantly higher entity coverage than traditional comments (\textit{Z} = -2.34, \textit{p} = 0.016, \textit{Eff. size} = 0.894). Detailed data is provided in the supplementary material. The findings confirm that, compared to traditional comments, danmaku contributes more nuanced and comprehensive augmentations to specific and detailed aspects of video content.


\subsection{Danmaku Processing}

To automate the extraction of \textit{knowledge danmaku} and label their information themes (\textbf{DR2, DR3}), we fine-tuned a \textit{Llama-2 Chinese} model \cite{cui2023efficient} with a \textit{LoRA} adapter \cite{hu2021lora} using the 3,000 annotated danmaku samples. A six-class classifier was developed to determine whether a specific danmaku qualified as \textit{knowledge danmaku} and, if so, to identify its corresponding information themes. We conducted a 7:3 split for training and testing the samples, resulting in a good performance with an F1-score of 72.8\%. This demonstrated that the fine-tuned classifier effectively processed danmaku from all science video topics.
% \xm{[R1 3rd comment: shall we mention that one fine-tuned model can be used for all classes/topics of science videos?]}
 
The next step is to determine the stance of danmaku; according to the \hyperref[formative-study]{Formative Study} participants, clarifying viewpoints in comments can enhance the sense of co-presence and mitigate knowledge bias (\textbf{DR3}). 
% clarifying the stance of danmaku can enhance the sense of co-presence and mitigate knowledge bias (\textbf{DR3}). 
% The content analysis revealed that the interpretations were the most numerous and exhibited the greatest variability in stance, while other themes had relatively consistent attitudes. 
Our content analysis revealed that \textit{interpretation} comments took up the biggest portion of \textit{knowledge danmaku} and exhibited the greatest variability in stance, while those in other categories showed relatively consistent attitudes.
Therefore, we further analyzed the stance of each \textit{interpretation} comment as positive, neutral, or negative by inputting them into a widely recognized sentiment analysis model, \textit{cardifnlp/twitterxlm-roberta-base-sentiment} \cite{barbieri2021xlm, wolf2020transformers} developed by \textit{HuggingFace} \cite{huggingface}. Consequently, we identified a total of seven categories for \textit{knowledge danmaku}. 
% To further streamline the danmaku comments, 
\begin{figure*}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{images/danmaku-processing1.jpg}
  \caption{Danmaku processing pipeline for CoKnowledge. }\label{fig:danmaku-processing}
  \Description{Danmaku processing pipeline for CoKnowledge.}
\end{figure*}

To further reduce repetitive information, we utilized \textit{Density-Based Spatial Clustering of Applications with Noise} (DBSCAN) \cite{ester1996density} to group semantically similar danmaku within the same theme and time segment (\textbf{DR3}). The centroid of each cluster was designated as the representative danmaku.
% , which, scaled by the cluster's comment count, replaced all comments within the cluster \xm{what does the second half mean? I think it is more relevant to the user interface part...}. 
Concerning the potential delays of danmaku relative to the video timestamps they are intended to anchor \cite{ma2017video}, we utilized GPT-4 \cite{achiam2023gpt} to map processed danmaku to their corresponding segments based on semantic similarity and temporal proximity. The specific algorithms and prompts are detailed in the supplementary material.
These steps collectively formed our automated NLP pipeline for danmaku processing (Figure \ref{fig:danmaku-processing}), which supports the operation of CoKnowledge.



\subsection{User Interface}
\label{ui}

To meet the design requirements specified in section \hyperref[drs]{3.3} and Figure \ref{fig:design-pipeline}, we developed CoKnowledge to facilitate users' assimilation of collective knowledge. 
% Reflecting the viewing modes identified in the \hyperref[formative-study]{Fomrative Study}, 
Following people's ways of viewing science videos with danamku as identified in the \hyperref[formative-study]{Formative Study}, 
CoKnowledge comprised three distinct modes of displaying collective knowledge, namely, overview mode, focused mode, and exploration mode. Each mode was equipped with specific design elements tailored to address the unique needs and challenges encountered by users within that mode (\textbf{DR1}). Additionally, we previously observed that different modes were associated with particular video-watching behaviors (Table \ref{formative-study-findings}); hence, CoKnowledge enabled automatic mode transitions based on user behaviors to create a seamless viewing experience.
The following subsections detail 
% \xm{use present tense when describing what this paper is about / section writes about...} 
the key user interface (UI) elements of each mode, specifying which items (Table \ref{formative-study-findings}) 
% \xm{??? this is very confusing -- unclear what you mean by features. In Fig 1, the features are the modes and NLP functions...} 
they were designed to address.
We utilized widely recognized tools for the tasks and conducted performance evaluations. All technical details (e.g., algorithms, prompts, evaluations) are included in the supplementary material. 




\subsubsection{Overview Mode}
\label{overview-mode}

% When users initially loaded the video and had not yet begun playback, 
The system defaulted to
% \xm{I think you need to add `the' before the three modes as a specific reference}
\textbf{Overview Mode} when users just loaded a video (Figure \ref{fig:overview}).
This mode included a progress bar directory (Figure \ref{fig:overview} A-1) and a Wordstream (Figure \ref{fig:overview} A-2). 
These components functioned in tandem to synchronously display video and danmaku knowledge, respectively. This setup enabled users to identify specific timestamps in the video while quickly obtaining an outline of the collective knowledge presented.

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{images/overview1.png}
  \caption{Interface of Overview Mode. A1: Progress bar directory. A2: Wordstream with legend filter. The explanation of each feature is in section \hyperref[overview-mode]{4.3.1}.}\label{fig:overview}
  \Description{Interface of Overview Mode. The explanation of each feature is in section 4.3.1.}
\end{figure*}

In particular, the progress bar directory offered a segmented summarization of the video knowledge (Table \ref{formative-study-findings} \circled{1}\circled{4}\circled{10}) by dividing the whole video into several semantically coherent sections and condensing the content in each into a sentence using Bilibili's official AI video summary tool \cite{bilibili_video}.
% \xm{state the tool/algorithm you use. If it is LLM, say that the prompts are included in the supplementary materials..} 
% This directory corresponded with the video’s progress bar, 
This directory aligned with the video timeline,
with the length of each section reflecting the duration of the corresponding video segment. When users hovered over a specific position in the progress bar directory, the system displayed the video transcript at that timestamp, enabling users to access more concrete content if they found the summaries too abstract.

WordStream, positioned below the progress bar directory, combined a stacked area chart with word clouds. Inspired by \cite{heimerl2015citerivers, nguyen2022wordstream, liu2012tiara}, such representation not only maintained an aesthetic appeal \cite{heimerl2015citerivers} but also effectively handled and analyzed large amounts of qualitative, time-series, and topic-based data \cite{nguyen2022wordstream, liu2012tiara}, making it suitable for rendering an overview of \textit{knowledge danmaku} throughout the video. 
Other design alternatives, such as heatmaps and line charts, cannot capture the rich semantic information conveyed by danmaku.
The x-axis of Wordstream encoded time, while the y-axis encoded the quantity of danmaku at specific timestamps, with colors of the bands indicating the categories of the \textit{knowledge danmaku}. 
The word clouds contained keywords extracted from the danmaku appearing at each timestamp.
We used GPT-4 \cite{achiam2023gpt} to identify a representative word for each comment.  
% The word clouds were generated from keywords extracted from the danmaku appearing at each timestamp, with GPT-4 \cite{achiam2023gpt} being utilized to identify a representative keyword for each comment. 
The larger a keyword was displayed, the more danmaku containing that keyword appeared at that timestamp.

% This dual representation vividly illustrated the semantic and theme proportions in danmaku at each timestamp while also showcasing their evolution over the course of the video (Table \ref{formative-study-findings} \circled{1}\circled{4}\circled{10}\circled{13}\circled{18}). 
The Wordstream provided a vivid overview of the distribution and gist of different types of \textit{knowledge danmaku} and their evolution over the course of the video (Table \ref{formative-study-findings} \circled{1}\circled{4}\circled{10}\circled{13}\circled{18}). 
Following Shneiderman's visualization mantra \cite{shneiderman2003eyes}, we also enable ``zoom and filter then details-on-demand'' via interactions (Table \ref{formative-study-findings} \circled{7}). 
Users could filter the Wordstream by specifying the categories of interest using the legend below (Figure \ref{fig:subfigure1}). If they were particularly interested in the collective knowledge within a certain video section, they could click on the corresponding segment in the progress bar directory to enlarge it (Figure \ref{fig:subfigure2}). This would cause the segment to expand and display a more detailed summary, and the Wordstream would zoom in accordingly. 
% , with Wordstream updating to reflect only that section.
Moreover, users could click on a keyword in the Wordstream to view the corresponding comments in the side view for detailed exploration (Figure \ref{fig:overview}).
% \xm{say that this part is explained later in subsection xxx}. 

% Users can click on a keyword in the Wordstream to view the corresponding comments in the side view for more detailed exploration (Table \ref{formative-study-findings} \circled{7}). Furthermore, users could filter the Wordstream by specific themes of danmaku using the legend below (Figure \ref{fig:subfigure1}). If users were particularly interested in the collective knowledge of a specific time segment, they could click on the corresponding segment in the progress bar directory to enlarge it (Figure \ref{fig:subfigure2}). This would cause the segment to expand and display a more detailed summary, with Wordstream updating to reflect only that section.

\begin{figure*}[h]
    \centering
    \begin{subfigure}{0.482\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/filter.png}
        \caption{}
        % \caption{Wordstream after filtering for the categories of \textit{concept noting} and \textit{supplementary knowledge}.}
        \label{fig:subfigure1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.505\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/filter-time.png}
        \caption{}
        % \caption{An expanded segment of Wordstream and progress bar directory.}
        \label{fig:subfigure2}
    \end{subfigure}
    \caption{More interactions of \hyperref[overview-mode]{Overview Mode}: a) Wordstream after filtering for the categories of \textit{concept noting} and \textit{supplementary knowledge}. b) An expanded segment of Wordstream and progress bar directory.}
    \label{fig:combined}
    \Description{More interactions of Overview Mode: a) Wordstream after filtering for the categories of concept noting and supplementary knowledge. b) An expanded segment of Wordstream and progress bar directory.}
\end{figure*}

% The progress bar directory and Wordstream functioned in tandem to synchronously display both video knowledge and danmaku knowledge. This setup enabled users to identify specific timestamps in the video while quickly obtaining an outline of the entire collective knowledge presented.


\subsubsection{Focused Mode}
\label{focused-mode}

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{images/focused.png}
  \caption{Interface of Focused Mode. B1: Representative danmaku comment with a notation. B2: Unprocessed danmaku stream. B3: Simplified version of the features in Overview Mode. The explanation of each feature is in section \hyperref[focused-mode]{4.3.2}\label{fig:focused}}
  \Description{Interface of Focused Mode. The explanation of each feature is in section 4.3.2}
\end{figure*}

When users began playing the video, CoKnowledge automatically transitioned to \textbf{Focused Mode} (Figure \ref{fig:focused})
% \xm{[Address the position of the figures... they are too far away from where they are first referenced...e.g., Fig 6 is 2 pages behind...]}
, which enlarged the video viewing window (Table \ref{formative-study-findings} \circled{14}) with minimum interactive features (Table \ref{formative-study-findings} \circled{8}). While other modes offered alternative representations of danmaku for digesting collective knowledge, \textbf{Focused Mode} retained the synchronized scrolling of consolidated \textit{knowledge danmaku} across the video to maintain its inherent alignment and contextual relevance to the content. By mitigating danmaku's issues of visual clutter and short display duration in the video view and significantly reducing information load in other views (Table \ref{formative-study-findings} \circled{16}\circled{17}), this mode enabled users to concentrate on both the video and the floating danmaku,
% as they normally do on an original danmaku platform. This design aimed to
delivering an authentic, immersive and streamlined viewing experience (Table \ref{formative-study-findings} \circled{5}\circled{11}).
%This design enabled users to concentrate on both the video and the danmaku, ensuring an immersive and streamlined viewing experience (Table \ref{formative-study-findings} \circled{5}\circled{11}).}




% This design aimed to enhance users’ concentration on the video and floating danmaku (Table \ref{formative-study-findings} \circled{5}\circled{11}). 
To be more specific, as illustrated in Figure \ref{fig:focused}, the number of floating comments significantly decreased with the help of our NLP pipeline (Table \ref{formative-study-findings} \circled{2}\circled{16}\circled{17}). Users could further select the type(s) of danmaku they wish to view on the video screen. To address the issue of short display duration (Table \ref{formative-study-findings} \circled{17}), we implemented the following adjustments. First, longer danmaku floated more slowly. Second, if a comment represented a cluster of similar danmaku, a notation indicating the number of danmaku in the cluster was added (Figure \ref{fig:focused} B-1).
% \xm{why is the notation related to duration?}
As participants in the \hyperref[formative-study]{Formative Study} generally recognized the value of danmaku supported by many users, the larger the number, the bigger the comment appeared, and the more slowly it scrolled.
% the larger the font size and the slower the floating speed was.
Below the video window was a simplified version of the Wordstream (Figure \ref{fig:focused} B-3) from \hyperref[overview-mode]{Overview Mode}, with the word clouds removed and the stacked area chart compressed vertically. It was intended to offer danmaku evolution patterns while minimizing distractions.
% The two features at the bottom served as simplified versions (Figure \ref{fig:teaser} B-1) of \hyperref[overview-mode]{Overview Mode} to minimize distractions.


\subsubsection{Exploration Mode}
\label{exploration-mode}
\begin{figure*}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{images/exploration.png}
  \caption{Interface of Exploration Mode. C1: Knowledge graph with legend filter. D1: Related danmaku display. D2: AI-assisted explanation. D3: Subtitle-danmaku list. The explanation of each feature is in sections \hyperref[exploration-mode] {4.3.3} and \hyperref[side-view]{4.3.4}.}\label{fig:exploration}
  \Description{Interface of Exploration Mode. The explanation of each feature is in sections 4.3.3 and 4.3.4.}
\end{figure*}


When users paused at a specific timestamp, CoKnowledge entered \textbf{Exploration Mode} (Figure \ref{fig:exploration}) automatically, displaying the structured collective knowledge within that 20-second segment in the form of a Knowledge Graph (KG) 
% \xm{[How is the knowledge graph constructed? We need to expand the following sentences to highlight that part of the KG comes from video captions/subtitles -- The question by R1]}
below the video window (Table \ref{formative-study-findings} \circled{3}\circled{6}). The blue elliptical nodes and their links within the KG represented video content entities and their relationships. They were extracted from the video transcripts during that period through named entity recognition and relation extraction tasks using GPT-4 \cite{achiam2023gpt} 
% \xm{providing prompt in the supplementary material}
. The square node(s) linked to an elliptical video node represented its most relevant danmaku comment(s), with colors denoting the danmaku category. 
The 20-second segment length was determined through multiple rounds of testing, considering factors such as the number of danmaku, space utilization, graph clarity, and users’ cognitive load.
% Surrounding these nodes were colored square nodes representing the different themes of danmaku comments, with each danmaku attached to the most relevant video node. 
Similar to the other modes, users could filter the categories of danmaku using the legend. When they clicked on a specific square danmaku node, the side view displayed its related danmaku within the short time interval and AI-assisted explanations to facilitate a thorough analysis (Table \ref{formative-study-findings} \circled{9}\circled{12}). 



% \xm{I think you should have a separate technical evaluation subsection to show the performances of all the automatic components -- e.g., NLP pipeline, progress bar directory summarization, knowledge extraction, explanation generation, etc.}
% % The 20-second segment length was determined through multiple rounds of testing, considering factors such as the number of danmaku, space utilization, graph clarity, and users’ cognitive load.
% The extraction of triples \xm{unclear what triples refer to...} for the KG was performed using GPT-4 \cite{achiam2023gpt} \xm{providing prompt in the supplementary material}. KG accuracy was defined as the proportion of triples within the KG being correct \cite{gao2019efficient}. A triple was deemed correct if the entities appeared in the video content and their relationships aligned with the narrative. To evaluate GPT-4's performance, we tuned the prompts and randomly selected 35 twenty-second segments from the content analysis video pool for GPT-4 to extract triples. Using manual annotation as the ground truth, GPT-4 achieved a satisfactory accuracy of 85.7\%.

% Similarly, users could filter the themes of danmaku using the legend. Users could also click on specific danmaku to highlight it, with the side view displaying related danmaku and AI-assisted explanations to facilitate a thorough analysis (Table \ref{formative-study-findings} \circled{9}\circled{12}). 


% \subsubsection{Other features}
\subsubsection{Side View}
\label{side-view}

In addition to the automatic mode transitions based on user behavior, users could manually switch modes. The side view (Figure \ref{fig:teaser} D) remained a constant feature that did not change with mode transitions. 

The bottom section of the side view was the subtitle-danmaku list (Figure \ref{fig:exploration} D-3), which displayed video subtitles (aligned to the left) and time-synced comments (aligned to the right) in chronological order. This list automatically synchronized and scrolled with the video playback. 
% Users could click a button below the list to manually align it with the video timestamps \xm{unclear... isn't it synced already?} (Figure \ref{fig:teaser} D-4). 
Users could filter danmaku categories within the list.
The upper sections of the side view were reserved for ``detail-on-demand,'' presenting related danmaku (Figure \ref{fig:exploration} D-1) and AI-assisted explanation (Figure \ref{fig:exploration} D-2) when users clicked on a danmaku node in KG or a comment in the list below. 
% related danmaku display (Figure \ref{fig:teaser} D-1) and AI-assisted explanation (Figure \ref{fig:teaser} D-2), which presented supplementary features when users clicked on a danmaku comment from the KG or the list below. 
% The related danmaku section displayed comments logically associated with the selected danmaku, while the AI-assisted explanation offered insights from GPT-4 \cite{achiam2023gpt} regarding the relationship between the selected danmaku and the video content. 
The related comments, posted within 15 seconds before or after the selected danmaku and logically linked to it, were identified through named entity recognition and entity linking tasks performed by GPT-4 \cite{achiam2023gpt}.
% \yh{To determine this, we first performed named entity recognition (NER) to extract entities from all comments within 20 seconds before and after the selected danmaku, followed by entity linking. A comment was identified as related if it contained an entity linked to any entity of the selected danmaku}.
% \xm{how are they determined?}
The explanations were generated by GPT-4 \cite{achiam2023gpt} to offer insights into the relationship between the selected danmaku and the video content. 
% As mentioned in the \hyperref[overview-mode]{Overview Mode}, the related danmaku display has an extra feature: clicking a Wordstream keyword shows the danmaku comments that formed it at that time.



