\section{Evaluation}

This section outlines the detailed experimental configuration, including the setup of conditions, study procedures, and evaluation metrics. We implemented a mixed-methods, within-subject study in which participants viewed science videos with CoKnowledge and a baseline system.

\subsection{Conditions and Data Selection}
% Data/Content Selection/Criteria
We developed a baseline system that retained only three functions from Bilibili \cite{bilibili_about_us}: video playback, danmaku playback, and a danmaku list, excluding all other information, such as traditional comments. This system was designed to closely resemble CoKnowledge in UI components and style to minimize potential confounding variables, while removing any data processing of danmaku implemented in CoKnowledge.

\begin{table*}[h]
\centering
\begin{tabular}{c l l c c l}
\hline
\multicolumn{1}{c}{\textbf{Domain}} & \textbf{Topic}               & \textbf{Length}  & \textbf{Word Count} & \textbf{\textit{Knowledge danmaku}} & \textbf{View}  \\ \hline
\multirow{2}{*}{Health}    & Lactose Intolerance & 13'31'' & 3679                           & 361                                   & 5382k \\ \cline{2-6} 
                           & Alkaline Foods      & 13'49'' & 3598                           & 339                                   & 3103k \\ \hline
\multirow{2}{*}{Astronomy} & Neutron Star        & 11'37'' & 3765                           & 403                                   & 1949k \\ \cline{2-6} 
                           & Saturn              & 12'05'' & 3827                           & 379                                   & 1286k \\ \hline
\end{tabular}
\caption{Information about the selected videos, including the topics, length, transcript word count, number of \textit{knowledge danmaku}, and video views.}
\label{video}
\end{table*}

To demonstrate the generalizability of CoKnowledge, we selected two pairs of videos (as detailed in Table \ref{video}) that meet the following criteria: 1) They span different domains—astronomy and health. 2) They feature different narrative orientations—one introductory and the other debunking. 3) They vary in content complexity—one is more technically rigorous, while the other is more relatable to everyday life. In the subsequent content, we refer to these pairs as astronomy and health videos. Each video exceeds ten minutes, includes over 2,000 danmaku comments, and has over one million views, indicating both popularity and a substantial information load,  making them typical cases for CoKnowledge to address.
To ensure comparability, the videos within each pair were selected from the same domain, had analogous information loads (measured by video length, transcript word count, and the number of \textit{knowledge danmaku}), and were sourced from the same content creator to maintain consistency in communication style. Additionally, two authors meticulously reviewed the videos to confirm no overlap in the knowledge covered within each pair.


% In total, we selected 4 online science videos for our experiment.
% %In the study, participants were required to complete a quiz after viewing each video. 
% Therefore, the videos across different conditions needed to be carefully selected in advance, meeting the following criteria: (1) the topics should be within the same domain; (2) the videos should have a similar science communication style; (3) the videos should have a comparable information load (measured by video length, word count of video transcript, the number of knowledge danmaku); and (4) there should be no overlap in the knowledge covered by the videos. To demonstrate that our system is not limited to functioning with videos from a single domain, we selected videos from two different fields—one with a more technical focus and the other with a more practical, everyday relevance. In total, we selected 4 online science videos for our experiment. The two videos from each domain were sourced from the same content creator to ensure consistency in communication style. These videos also met the requirements for domain consistency and comparable information load (as detailed in Table \ref{video}). Two authors meticulously reviewed all the videos to ensure that the videos within the same domain did not have overlapping knowledge.


% According to Bloom’s taxonomy \cite{bloom1971taxonomy}, knowledge (the ability to recall learned information) and comprehension (the ability to interpret meanings and concepts) are the fundamental levels of learning outcomes. Based on this framework, we designed the quizzes using a multiple-choice question (MCQ) format, structured into four sections to assess participants' understanding: comprehension of video knowledge, recall of video knowledge, comprehension of danmaku knowledge, and recall of danmaku knowledge. It was essential to maintain consistent quiz difficulty within each domain. To achieve this, we first created an extensive question pool for each video and conducted a pilot testing with 9 participants (with inclusion criteria identical to those in the formal user study). These participants watched each video under baseline conditions and answered the questions in the pool. We then scored their responses and selected subsets of questions from the pool as the formal study quiz, ensuring that the average score for quiz questions in each section was equivalent across the two videos within the same domain. Our final quiz consisted of 16 MCQs (6 assessing video knowledge and 10 assessing danmaku knowledge). \todo{Examples?}For each MCQ, participants were also given the option "Not sure." To increase the reliability of the learning outcome measurements, we discouraged participants from guessing by penalizing incorrect responses: -1/3 points for wrong answers, +1 point for correct answers, and 0 points for selecting the “Not sure” option.


\subsection{Participants and Procedure}
To ensure the effectiveness of statistical analysis with sufficient scope, 
we performed an \textit{a priori} power analysis using G*Power \cite{faul2007g} to determine the required sample size. The analysis indicated that a minimum of 20 participants was needed to detect large effects (d = 0.8) with 90\% power in a Wilcoxon signed-rank test, assuming an alpha level of 0.05. After obtaining institutional IRB approval, we ultimately recruited 24 participants (10 female, 14 male, aged 18-31, with backgrounds in business, engineering, arts, and science) through online advertisements, social media, and word-of-mouth. 
Participants were eligible for the study if they were frequent viewers of science videos on platforms featuring danmaku and self-reported low prior knowledge of the experiment's video topics. 
% Demographic details are summarized in Table \ref{demographics}. 
We excluded participants from our previous study to avoid potential bias.

% \begin{table}[h]
% \centering
% \begin{tabular}{ccccc}
% \hline
% \textbf{ID} & \textbf{Gender} & \textbf{Age} & \makecell{\textbf{Frequency Using Online} \\ \textbf{Video Platform with Danmaku Feature}} & \makecell{\textbf{Frequency Viewing Online} \\ \textbf{Science Videos}} \\ \hline
% P1  & Male   & 22 & Everyday        & 4-6 times per week \\ \hline
% P2  & Male   & 18 & 4-6 times per week & 4-6 times per week           \\ \hline
% P3  & Male   & 23 & Everyday        & 4-6 times per week \\ \hline
% P4  & Female & 26 & Everyday        & 4-6 times per week \\ \hline
% P5  & Female & 23 & Everyday        & 4-6 times per week \\ \hline
% P6  & Male   & 21 & Everyday        & Everyday           \\ \hline
% P7  & Female & 22 & Everyday        & At least once a week           \\ \hline
% P8  & Female & 19 & Everyday        & Everyday           \\ \hline
% P9  & Female & 22 & 4-6 times per week & 4-6 times per week \\ \hline
% P10 & Male   & 22 & Everyday        & Everyday           \\ \hline
% P11 & Male   & 22 & Everyday        & Everyday           \\ \hline
% P12 & Female & 22 & Everyday        & 4-6 times per week \\ \hline
% P13 & Female & 25 & 4-6 times per week & 4-6 times per week \\ \hline
% P14 & Male   & 23 & Everyday        & 4-6 times per week \\ \hline
% P15 & Male   & 25 & Everyday        & 4-6 times per week \\ \hline
% P16 & Male   & 22 & Everyday        & 4-6 times per week \\ \hline
% P17 & Male   & 22 & Everyday        & Everyday           \\ \hline
% P18 & Female & 23 & Everyday        & Everyday           \\ \hline
% P19 & Male   & 22 & Everyday        & 4-6 times per week \\ \hline
% P20 & Female & 22 & 4-6 times per week & 4-6 times per week           \\ \hline
% P21 & Male   & 23 & 4-6 times per week & At least once a week           \\ \hline
% P22 & Female & 22 & 4-6 times per week & 4-6 times per week \\ \hline
% P23 & Female & 22 & Everyday        & 4-6 times per week \\ \hline
% P24 & Male   & 31 & 4-6 times per week & At least once a week           \\ \hline
% \end{tabular}
% \caption{Demographics of all the participants.}
% \label{demographics}
% \end{table}

Figure \ref{fig:study-procedure} illustrates the experimental procedure. Prior to the study, participants completed a pre-study survey to gather demographic information and confirm their eligibility. During the experiment, with participants' consent, we recorded their screen activity, audio, and collected system logs.  Each participant began with a 10-minute tutorial, during which they could freely explore CoKnowledge and raise questions. Following the tutorial, participants were briefed on the context and tasks. There were a total of four tasks: two using CoKnowledge, and two using the baseline system. For each task, participants were given 16 minutes to watch a video and 10 minutes to complete a quiz. The order of task combinations was counterbalanced.
% \begin{itemize}[noitemsep, topsep=0pt]
%     \item Neutron Star; Alkaline foods (Baseline) - Saturn; Lactose intolerance (CoKnowledge)
%     \item Saturn; Lactose intolerance (Baseline) - Neutron Star; Alkaline foods (CoKnowledge)
%     \item Saturn; Lactose intolerance (CoKnowledge) - Neutron Star; Alkaline foods (Baseline)
%     \item Neutron Star; Alkaline foods (CoKnowledge) - Saturn; Lactose intolerance (Baseline)
% \end{itemize}
After each condition, participants were asked to fill out an in-task survey to provide feedback on the system. The study concluded with a semi-structured interview that explored participants' experiences with CoKnowledge, their use of specific features and underlying behavioral rationale,  and a comparison with the baseline system.
% , as well as systems' utilization of danmaku. 
The study lasted approximately 120 minutes, and each participant received compensation aligned with the local hourly wage standard.



\begin{figure*}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{images/procedure.jpg}
  \caption{User study procedure: Participants took part in a within-subject study, watching two video pairs (each from a different domain) using two different systems in a counter-balanced order. The study included a pre-study survey, in-task quizzes, and an interview.}\label{fig:study-procedure}
  \Description{User study procedure: Participants took part in a within-subject study, watching two video pairs (each from a different domain) using two different systems in a counter-balanced order. The study included a pre-study survey, in-task quizzes, and an interview.}
\end{figure*}






\subsection{Measurements}
Following the established evaluation pipeline for interactive systems \cite{weibelzahl2020evaluation, xia2022persua}, we assessed the systems across three key dimensions: system usefulness, design \& interaction, and system usability. All in-task surveys used a 7-point Likert scale, with specific questions detailed in the supplementary material.
% We reviewed relevant literature to construct the in-task surveys, with all questions utilizing a 7-point Likert scale. \yh{The specific questions of in-task surveys are presented in the supplementary material.}

\subsubsection*{System Usefulness}

According to Bloom’s taxonomy \cite{bloom1971taxonomy}, knowledge (the ability to recall learned information) and comprehension (the ability to interpret meanings and concepts) are the fundamental levels of learning outcomes. Based on this framework, we designed the in-task quizzes using a single-choice question (SCQ) format, structured into four sections to assess participants' understanding: comprehension of video knowledge, recall of video knowledge, comprehension of danmaku knowledge, and recall of danmaku knowledge. 
% \yh{Based on Bloom’s taxonomy \cite{bloom1971taxonomy}, which identifies knowledge (recall of information) and comprehension (interpretation of concepts) as fundamental learning outcomes, we designed in-task quizzes using a single-choice question (SCQ) format to assess participants' understanding. Quizzes were divided into four sections: comprehension and recall of both video and danmaku knowledge.}
To maintain consistent quiz difficulty within each domain, we first created an extensive question pool for each video and conducted a pilot test with 9 participants (with inclusion criteria identical to those in the formal user study). These participants watched each video under baseline conditions and answered the questions in the pool. We then scored their responses and selected subsets of questions from the pool as the formal study quiz, ensuring that the average score for quiz questions in each section was equivalent across the two videos within the same domain. The final quiz included 16 SCQs—6 for video knowledge and 10 for danmaku knowledge—with an option to select 'Not sure.' To increase the reliability of the learning outcome measurements, we discouraged participants from guessing by penalizing incorrect responses: -1/3 points for wrong answers, +1 point for correct answers, and 0 points for selecting the 'Not sure' option.

To evaluate CoKnowledge's effectiveness in supporting users' assimilation of collective knowledge, we conducted a multi-level comparison of in-task quiz scores across the two conditions. We also gauged participants' confidence by analyzing the number of quiz questions where "Not sure" was not selected. In addition to these objective measures, participants also provided self-assessments of perceived comprehension, recall, confidence, and efficiency in the in-task surveys.
Furthermore, echoing the \hyperref[formative-study]{Formative Study}, we aimed to evaluate CoKnowledge's ability to harness the benefits of danmaku while mitigating its potential drawbacks. To achieve this, we gathered users' perceptions on the relevant aspects in the in-task surveys.%(as detailed in Table \ref{subjective})%.

\subsubsection*{Design \& Interaction}

% Our goal was to understand how users interacted with CoKnowledge and perceived the helpfulness of its features. We asked participants to rate the helpfulness of different modes, data processing methods, and UI components of CoKnowledge in the in-task survey. We also reviewed the recorded videos and system logs to analyze participants' behavior patterns with CoKnowledge.
We investigated CoKnowledge's feature helpfulness by asking participants to rate different modes, data processing methods, and UI components in the in-task surveys. We also analyzed recorded videos and system logs to identify behavior patterns during their interactions with CoKnowledge.

\subsubsection*{System Usability}

Balancing functionality and usability is a common challenge for computer-supported systems \cite{kang2021metamap,goodwin1987functionality}. In our evaluation, we first measured task workload using the NASA Task Load Index \cite{hart1988development}, followed by an assessment of overall usability using a shortened version of the System Usability Scale (SUS) questionnaire \cite{brooke1996sus}.

% \begin{table}[h]
% \centering
% \begin{tabular}{cc}
% \hline
% \textbf{Category} & \textbf{Questions} \\
% \hline
% \multirow{4}{*}{Perceived performance} 
% & \textit{I found myself comprehending the collective knowledge completely.} \\
% & \textit{I found myself able to recall the collective knowledge accurately.} \\
% & \textit{I felt confident in assimilating the collective knowledge.} \\
% & \textit{I am effective in assimilating the collective knowledge.} \\
% \hline
% \multirow{3}{*}{Danmaku advantage}     
% & \textit{I felt as though we were watching videos with others.} \\
% & \textit{Danmaku helped me avoid the knowledge bias with the video uploader.} \\
% & \textit{I felt danmaku is closely related to the video content.} \\
% \hline
% \multirow{5}{*}{Danmaku drawback}      
% & \textit{I felt that danmaku and video content interfered with each other.} \\
% & \textit{I often missed danmaku because it disappeared too quickly.} \\
% & \textit{I could easily locate a specific danmaku comment.} \\
% & \textit{I found the information density of danmaku to be low.} \\
% & \textit{I perceived the information structure of the collective knowledge to be clear.} \\
% \hline
% \end{tabular}
% \caption{Survey questions to subjectively evaluate system usefulness.}
% \label{subjective}
% \end{table}