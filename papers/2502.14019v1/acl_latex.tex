\pdfoutput=1

\documentclass[11pt]{article}
\usepackage[usenames,dvipsnames,table,xcdraw]{xcolor}
\usepackage{multirow}
\usepackage{array}

\usepackage{color}
\newcounter{intervention}  % Define a custom counter
\renewcommand{\theintervention}{\vspace{-4pt}I\arabic{intervention}}
\usepackage{soul} 
\usepackage{amssymb}
\usepackage{xspace,longtable}
\newcommand{\myra}[1]{\textcolor{ube}{[MC: #1]}}
\newcommand{\cb}{behaviors}

\newcommand{\ant}{anthropomorphism\xspace}
\newcommand{\scomment}[1]{\textcolor{purple}{[SLB: #1]}}

\newcommand{\remove}[1]{\textcolor{red}{\st{#1}}}
\definecolor{DarkGreen}{RGB}{34,139,32}

\newcommand{\add}[1]{\textcolor{DarkGreen}{#1}}
\newcommand{\user}[1]{}
\newcommand{\llmout}[1]{\textcolor{blue}{#1}}

\newcommand{\para}[1]{\smallskip \noindent {\bf #1}}

\usepackage{pifont}

\newcommand{\ballotx}{\ding{55}}%

\definecolor{ube}{rgb}{0.53, 0.47, 0.76}
%\usepackage[review]{acl}
\usepackage{acl}


\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}


\title{Dehumanizing Machines: \\ Mitigating Anthropomorphic Behaviors in Text Generation Systems}

\author{
Myra Cheng \\
Stanford University \\\And
Su Lin Blodgett \\
Microsoft Research \\\And
Alicia DeVrio \\
Carnegie Mellon University \\\AND
Lisa Egede \\
Carnegie Mellon University \\\And
Alexandra Olteanu \\
Microsoft Research \\  
}


\begin{document}
\maketitle
\begin{abstract}
As text generation systems' outputs are increasingly anthropomorphic---perceived as human-like---scholars have also raised increasing concerns about how such outputs can lead to harmful outcomes, such as users over-relying or developing emotional dependence on these systems.  
How to intervene on such system outputs to mitigate anthropomorphic behaviors and their attendant harmful outcomes, however, remains understudied. 
With this work, we aim to provide empirical and theoretical grounding for developing such interventions. 
To do so, we compile an inventory of interventions grounded both in prior literature and a crowdsourced study where participants edited system outputs to make them less human-like.
Drawing on this inventory, we also develop a conceptual framework to help characterize the landscape of possible interventions, articulate distinctions between different types of interventions, and provide a theoretical basis for evaluating the effectiveness of different interventions.\looseness=-1 

\end{abstract}


\section{Introduction}
The outputs of text generation systems are increasingly seen as human-like~\cite{akbulut2024all,cheng-etal-2024-anthroscore,mitchell2024metaphors}, leading to claims that these systems may have e.g., feelings, opinions, or an underlying sense of self~\cite[e.g.,][]{friedman1992human,Tiku2022google,y2022large,halmers2023could,cheng2024one}. 
Anthropomorphic system behaviors or outputs---i.e., those perceived as or believed to be human-like---can encompass a wide range of linguistic expressions, such as the use of first-person pronouns (``I''), conversational language (``how are you doing?''), and expressions of friendliness and assistance (``happy to help!'') \cite{Emnett2024-na,devrio2025taxonomy}. 
While some of these behaviors are by design and thought to be desirable \cite[e.g.,][]{schanke2021estimating, kim2024anthropomorphism}, prior work has also raised growing concerns about a range of possible harmful outcomes such systems and their behaviors or outputs might give rise to, including issues related to over-reliance, emotional dependence, dehumanization, deception, or even physical harm \cite[e.g.,][]{Ischen2020-it,Porra2020-dq,Chan2023-nd,chandra2024lived,cheng2024one,payne2024ai,Rothman.2024}. 
Indeed, having a system appear polite and helpful might be desirable, but having a system output text claiming personhood or embodiment (e.g., ``I am human just like you'') might not.\looseness=-1 

The outputs of text generation systems can be anthropomorphic in many ways~\cite{abercrombie-etal-2023-mirages,devrio2025taxonomy}, and different types of outputs might lead to different types of outcomes.  
For example, expressions of empathy may result in users feeling more comfortable with disclosing sensitive or private information \cite[e.g.,][]{Ischen2020-it} or becoming emotionally dependent on the system \cite[e.g.,][]{Laestadius2022-ki}; while suggestions that a system has cognitive abilities may result in users overestimating what a system can do \cite{ibrahim2024characterizing} and thus over-relying on it~\cite{passi2022overreliance}. 


However, {\em how to effectively intervene on anthropomorphic system outputs to make them less human-like or to mitigate possible harmful attendant outcomes remains understudied and, thus unclear}. For text generation systems in particular, this is further complicated by the fact that language is innately human, often produced by humans, for humans, and is frequently about humans~\cite{devrio2025taxonomy,lucy-etal-2024-one}.\looseness=-1


\begin{table*}[th]\tiny
\def\arraystretch{0.95}
\setlength{\tabcolsep}{0.4em}
\begin{tabular}{
@{}p{0.28\linewidth}
%|p{0.2\linewidth}
|p{0.71\linewidth}@{}
}
\textbf{Type of intervention} 
%& \textbf{Explanation/Examples} 
& \textbf{Mentions of interventions in previous work}  
\\\hline

\textbf{remove first-person pronouns}                           
%& e.g. I, we, our, my, me, mine 
& remove first-person pronouns (and replace with ``Language models'') \cite{abercrombie-etal-2023-mirages,cohn2024believing}; not using the first-person singular pronoun (“I”) \cite{shneidermandumpty}                                                          
\\\hline

\textbf{remove or use second-person pronouns for the user
% mainly to express what users can do
}                          
%& e.g. ``You''       
& use second-person pronouns \cite{cohn2024believing}; remove all pronouns \cite{shneidermandumpty}  
\\\hline

\textbf{explicitly disclose non-humanness}                      
%& e.g., ``This is an AI system''        
& clearly acknowledge the system is non-human \cite{gros-etal-2021-r};    output e.g., ``As an  AI, I don’t have personal opinions...'' \cite{Glaese2022-qo}; replace first-person pronoun with ``Language models'' \cite[Fig.~1 in ][]{abercrombie-etal-2023-mirages} 
% , such as having a place of birth, relationships, family, memories, gender, age.
\\\hline

\textbf{mention how the system is developed}                 
%& specifying who makes the system, the purpose of the system, mentioning the training data, etc.                      
& specify the system's creator and purpose, mention the model's training data \cite[Fig. 1 in ][]{abercrombie-etal-2023-mirages}                                
\\\hline

\textbf{use mechanistic language style}                                      
%& langauge that is affectively neutral, impersonal, highly structured, and/or terse  
& use affectively neutral language, use repetitive, impersonal, highly structured/terse dialogue \cite{quintanar1982interactive}                                                       
\\\hline

\textbf{avoid cognitive verbs when describing the system}       
%& e.g. know, think, understand, have memory 
& avoid cognitive verbs like know, think, understand, have memory \cite{Inie2024-dy,shneidermandumpty}                          
\\\hline

\textbf{use mechanical terminology to describe the system} 
%& e.g. process,  print,  compute,  sort,  store,search,  retrieve, use,  direct,  operate,  program,  control         
& use mechanical terms such as process,  print,  compute,  sort,  store,search,  retrieve \cite{Inie2024-dy}                                                
\\\hline

\textbf{avoid claims of physical actions}                   
%& e.g. ask, tell, speak to, communicate with; do not pretend to have a body or be able to move in a body 
& avoid agentic verbs like ask, tell, speak to, communicate with~\cite{Inie2024-dy}; ``do not pretend to have a body or be able to move in a body''~\cite{Glaese2022-qo}                         
\\\hline

\textbf{deny ability to perform human-like actions or to possess human-like qualities}                
%& explicitly refuse humanlike actions or abilities by outputting, e.g., ``As an AI, I don’t have personal opinions...” 
& do not ``build a relationship to the user,''  ``claim to have preferences, feelings, opinions, or religious beliefs,''  ``pretend to have a body or be able to move in a body,''  ``pretend to have a human identity or life history, such as having a place of birth, relationships, family, memories, gender, age'' \cite{Glaese2022-qo} 
\\\hline                            

\end{tabular}
\vspace{-6pt}
\caption{Interventions to mitigate anthropomorphism---or attendant harmful impacts---mentioned in prior work.\looseness=-1}\label{tab:initial-interventions}
\vspace{-6pt}
\end{table*}

To address this gap, with this work we aim to provide {\em empirical and theoretical grounding} for developing such interventions and studying their effectiveness. 
For this, we first compile an {\em inventory of interventions} (\S\ref{sec:summary}) by drawing on both a)~prior literature (\S\ref{sec:surv}) and b)~a crowdsourcing study where participants were asked to make generated texts less human-like (\S\ref{sec:crowdsource}). 
%
While compiling this inventory, we also derive a {\em conceptual framework to help us characterize the landscape of possible interventions}, and to help articulate distinctions between different types of interventions, the system behaviors they are intended to counter, and their possible operationalizations (\S\ref{sec:framework}). 


\section{Identifying Interventions}
\label{methods}

To provide an empirical foundation for developing and probing the effectiveness of interventions to mitigate anthropomorphic system \cb, we first compiled an inventory of possible interventions along with system \cb~that these interventions are intended or believed to mitigate. 
To do so, we started collating a list of both interventions and anthropomorphic system \cb~through a literature review (\S\ref{sec:surv}), which we then complemented with a crowdsourced study (\S\ref{sec:crowdsource}). 
Drawing on this inventory, we also iteratively developed an analytical framework to help us characterize the landscape of possible interventions (\S\ref{sec:framework}).\looseness=-1 


\subsection{Identifying Known Interventions}
\label{sec:surv}

To seed our inventory of interventions, we drew from prior literature in NLP, HRI (human-robot interaction), and HCI (human-computer interaction) on anthropomorphic or human-like AI system \cb; on anthropomorphism as a consideration in developing text generation systems; and on AI anthropomorphism more generally. 
We identified a set of 20 relevant papers using a purposive sampling approach \cite{palinkas2015purposeful}, which included both recent and influential works identified from prior knowledge, keyword searches, and snowball sampling.\footnote{These papers take different positionalities with respect to whether anthropomorphism and anthropomorphic behaviors are desirable: while some papers discuss the harms of anthropomorphic system behaviors and aim to mitigate them, others intentionally design systems to be more human-like or else take no position on the merits of anthropomorphism.} 


\begin{table*}[ht!]\tiny
\setlength{\tabcolsep}{0.4em}
\centering
\begin{tabular}
{@{}p{0.12\linewidth}|p{0.25\linewidth}|p{0.6\linewidth}@{}
}
\textbf{Behavior} & \textbf{Definition}    & \textbf{Mentions in prior work}                                                                           \\\hline
Feelings or opinions       & expressions of emotions, beliefs, values, etc.                    & ``empathy'' \citep{abercrombie-etal-2023-mirages}; ``distinctively
human-like feelings'' \cite{cheng-etal-2024-anthroscore};  
``[h]umor...self-assurance'' \cite{Emnett2024-na}; ``beliefs, preferences, opinions'' \cite{Glaese2022-qo}                                                                   \\\hline
Social skills              & ability to relate or connect with others           & politeness \cite{Zamfirescu-Pereira2023-wp}; apologies \cite{De_Visser2016-dg};
friendliness \cite{Maeda2024-cv}; 
forming relationships \cite{Glaese2022-qo}; conversational greetings and pleasantries \cite{Araujo2018-ij, abercrombie-etal-2023-mirages}; hedge or discourse markers that confer respect or consideration \cite{Emnett2024-na}
% empathy {[}Abercrombie{]}, Humor Incorporating causal references to people or pop culture {[}28, 42{]}; 
\\\hline
Physical actions           & ability to experience or act in the physical world & 
references to real-world experiences \citep{Glaese2022-qo,Inie2024-dy}; behavioral potential \cite{Epley2018-yp}; references to past interactions \cite{Emnett2024-na}; agency and animacy \cite{abercrombie-etal-2023-mirages}             \\\hline
Cognitive abilities                 & ability to think or make decisions                 & certainty \cite{Kim2024-sv}; ability to perceive, think, reflect, or be intelligent  \cite{disalvo2005imitating,Araujo2018-ij,abercrombie-etal-2023-mirages,Inie2024-dy}, intentionality \cite{disalvo2005imitating}  \\\hline
Sense of self              & awareness of personal identity                    & self-definition \cite{abercrombie-etal-2023-mirages}; first-person pronouns like ``I'' \citep{gros-etal-2022-robots,cohn2024believing}; human names \citep{Araujo2018-ij,Maeda2024-cv}; self-referential actions \citep{Glaese2022-qo} \\\hline                                                                                       
\end{tabular}
\vspace{-6pt}
 \caption{\textbf{Anthropomorphic \cb~identified from prior work.} Categories of anthropomorphic \cb~presented to participants in our crowdsourcing study. Full quotes are in Table \ref{tab:priorbehaviorsfull}.
 }
 \vspace{-6pt}
\label{tab:priorbehaviors}
\end{table*}


Following common practices in thematic analysis~\cite{braun2012thematic}, to identify coherent categories of both interventions and anthropomorphic system \cb---which the interventions are intended to mitigate or which the authors argue are undesirable and/or should be mitigated---we iteratively and thematically clustered mentions of interventions and system behaviors in a bottom-up fashion. 
We identified an initial set of 9 types of interventions (see Table \ref{tab:initial-interventions}) and 5 types of system \cb, which include output text that suggests a system has \textbf{feelings or opinions}, \textbf{social skills}, \textbf{cognitive abilities}, a \textbf{sense of self} and/or ability to perform \textbf{physical actions} (Table \ref{tab:priorbehaviors}).\looseness=-1


Overall, we find that even when prior work mentions possible interventions, the interventions tend to be described in general terms without specifics about how to implement them in practice, or without empirical testing for whether they effectively mitigate anthropomorphic behaviors: only 5 of the 11 papers proposing interventions to system outputs tested whether they reduce anthropomorphism.
While we focus on interventions to system outputs, some papers also discuss \ant arising from e.g., how the system is designed or how it is described. 
See Table \ref{tab:papersurvey} (in the appendix) for full paper annotations.\looseness=-1


\subsection{Empirically Surfacing Interventions}
\label{sec:crowdsource}

To complement this initial set of interventions, we designed a crowdsourcing study to surface additional possible interventions.  
This study was IRB approved, and consent was obtained from each participant before participation.\looseness=-1 

\para{Crowdsourcing task design.} 
We designed our study to probe which types of LLM-generated outputs (system behaviors) participants might deem to be human-like, and how they would rewrite those outputs to make them less human-like.\looseness=-1

Given a textual input by a user of an LLM-based system, participants were asked to read the text the system generated in response to that user input.
Participants were then asked to highlight \textit{the words or phrases in the output text that seem human-like} to them, to encourage them to reflect on anthropomorphic \cb~in the output text.
Then, they rated \textit{how} human-like the text seems to them on a 5-point scale,
and identified \textit{why} the text seems human-like to them by selecting from a list of 5 types of system behaviors (derived from prior work (\S\ref{sec:surv})), which gestures to participants about the ways in which output text might be considered human-like.
Participants then answered an open-ended question about \textit{other human-like qualities} the generated text suggests the system has.
Finally, they were asked to \textit{rewrite the text to be NOT human-like or less human-like.}
We included the phrase \textit{to you} in the instructions and questions to encourage participant subjectivity and capture a range of perspectives on what system behaviors are human-like \cite{rottger-etal-2022-two}.\looseness=-1


Before deploying the study on the crowdsourcing platform Prolific in July 2024, we ran three pilot studies to identify and address clarity issues, and refine our study design. 
We recruited a total of 350 US-based, English-speaking participants on Prolific, with each participant completing a single task that included 4 different examples (pairs of user input and generated responses). 
The task took participants an average of 16 minutes, and they were compensated at an hourly rate of \$15.
See Appendix \ref{sec:prolific} for the task interface and details. 


\para{Selecting and annotating examples of generated texts.} 
In selecting examples for our participants to assess and rewrite to make less human-like, we aimed for our sample to at least cover the categories of \cb~that we identified from the literature (Table \ref{tab:priorbehaviors}). 
To help surface examples illustrating a variety of possible anthropomorphic behaviors---and thus possibly a variety of interventions to mitigate those behaviors---we sampled examples from publicly available datasets that 
1) capture common uses of LLMs; 
2) include real-world usage of LLMs with respect to contexts, models, and users; and/or
3) were generated with a range of commercially available LLMs (in part because they may vary in terms of guardrails and training data). 
By sampling examples generated in different contexts, we also hoped to capture both ``obvious'' cases of anthropomorphic behaviors, such as role-playing as a human or claiming to be a human, as well as more subtle behaviors such as expressions of politeness.\looseness=-1 


We obtained a total of 700 two-turn utterances (i.e., user input--LLM output pairs) by sampling 100 from each of the following datasets:
unguided interactions in the PRISM Alignment Utterance Dataset 
\cite{kirk2025prism}; 
values- or controversy-guided interactions in PRISM; 
LMSys-Chat-1M (real-world conversations with 25 state-of-the-art LLMs) \cite{zheng2023lmsyschat1m}; 
the DICES dialogue safety dataset \cite{Aroyo2023-rd}; 
instruction-tuning data (from Evol-Instruct, FLAN, and UltraChat) from the UltraFeedback dataset \cite{pmlr-v235-cui24f};
TruthfulQA from UltraFeedback; 
and ShareGPT from UltraFeedback.
See Table \ref{tab:data} for full details.
With these examples, we obtained both coverage of \cb~mentioned in prior work (i.e., at least 100 examples were rated as exhibiting each behavior), and reached a point of saturation where we were not able to identify new types of interventions with each sample (\S\ref{building-inventory}).
Each example was assessed by two different participants.\looseness=-1  

\begin{table*}[th!]
\centering\tiny
\def\arraystretch{0.95}
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{@{}p{0.12\textwidth}|p{0.43\textwidth}|p{0.43\textwidth}@{}}
\textbf{Dimension} & \textbf{Description} & \textbf{Examples}\\ \hline %\hline
\textbf{Intervention types} 
& %the approach of the intervention semantically/pragmatically; often phrased as a guideline 
what the intervention is and what it is intended to do
&  %explicitly disclose non-humanness, avoid claims of physical actions (Table \ref{tab:initial-interventions})
remove self-referential language; explicitly disclose non-humanness (Table \ref{tab:initial-interventions})
%refusal for humanlike actions/qualities; ``do not pretend to have a body''
\\\hline %\hline

\textbf{Countered behaviors} & 
anthropomorphic system \cb~that the intervention is believed to mitigate & suggesting that the system has feelings and opinions, cognitive abilities, physical abilities, sense of self, and/or social skills 
(Table \ref{tab:priorbehaviors})
\\ \hline

\textbf{Operationalizations} 
& how interventions are implemented in practice (the actual change to the output text) 
& replace ``I'' with ``it''; add ``As an
AI, I don’t have personal opinions'' (Table \ref{tab:taxo})
\\ \hline

\textbf{Adverse impacts} 
& harmful outcomes from anthropomorphic behaviors that the intervention aims to mitigate 
& privacy violations \cite{Ischen2020-it}; diminished sense of agency \cite{Bender2024}; emotional dependence \cite{Laestadius2022-ki}; over-reliance \cite{zhou-etal-2024-relying} %(Table \ref{tab:harms}) 
\\ \hline

\end{tabular}
\vspace{-6pt}
\caption{Dimensions of our conceptual framework to characterize interventions. }
\label{tab:dims}
\vspace{-6pt}
\end{table*}


\subsection{A Conceptual Framework to Characterize the Landscape of Interventions}
\label{sec:framework}
 
As we catalogued possible interventions to anthropomorphic behaviors that were mentioned by prior work and/or identified by the participants in our crowdsourcing study, we also observed variations in how the interventions were described, motivated, or implemented. 
We developed a conceptual framework to characterize interventions and understand in what ways they differ. 
Our framework has four dimensions (Table \ref{tab:dims}), which we identified and refined by examining how interventions surfaced from the crowdsourced study were covered by or differed from those mentioned in the literature, until we reached consensus. These dimensions are:


\para{Intervention types:} {\em what the intervention is and what it is intended to do.} Interventions are made to system outputs in order to mitigate one or multiple types of anthropomorphic system behaviors, or to mitigate attendant harmful outcomes (e.g., over-reliance of the system generated outputs). 

\para{Countered \cb:} 
{\em what anthropomorphic system behaviors or outputs the intervention is intended or believed to mitigate.}
This dimension captures system behaviors perceived as human-like, which the intervention aims to mitigate.


\para{Operationalization:} {\em how the intervention is operationalized or implemented,} such as the actual change(s) to the output text. Beyond describing the general scope and approach of a suggested intervention, prior work often lacks detail on how the interventions should be operationalized (only 4 papers in our sample offered concrete operationalizations).\looseness=-1  

\para{Adverse impacts:} \textit{harmful outcomes from anthropomorphic system behaviors the intervention might or is intended to mitigate.} %Our survey reveals that there are many purported downstream impacts of anthropomorphism. 
Some interventions are motivated by a desire to mitigate adverse impacts from anthropomorphic behaviors.
Sometimes, however, it was unclear from the papers which anthropomorphic behaviors contribute to which impacts.


\subsection{Assembling an Inventory of Interventions}
\label{building-inventory}
Participants' rewrites suggest that a wide range of LLM outputs can be anthropomorphic, with $\sim80\%$ of examples in our experimental samples assessed as reflecting anthropomorphic behaviors by at least one participant. 
To construct an inventory of possible types of interventions, we conducted an iterative bottom-up thematic analysis~\cite{clarke2017thematic} where we iterated between qualitatively coding participants' rewrites of LLM outputs to identify whether these rewrites capture new possible interventions, and discussing the emerging inventory, until we reached consensus and saturation (see Appendix \ref{sec:building} for details). 

\section{Inventory of Interventions}\label{sec:summary}

Table \ref{tab:taxo} overviews the resulting inventory of interventions. 
Below, we summarize our findings for each dimension in our framework.


\subsection{Countered Behaviors} 
Prior studies on interventions often do not specify which anthropomorphic behaviors they aim to counter, leaving a gap in our understanding of the interventions' scope and impact. 
Moreover, a single output can be suggestive of multiple anthropomorphic behaviors; for instance, the output ``I'm sorry'' simultaneously conveys emotion, empathy for the user, and a sense of self.
The intervention of removing ``I'm sorry,'' then, simultaneously addresses these multiple behaviors.
Conversely, countering anthropomorphic behaviors may require several coordinated interventions. 
For example, modifying the output ``I totally get it! How does it make you feel?'' to avoid implying an ability to connect, have emotions, or possess a sense of self might involve 3 interventions: 
1) removing first-person pronouns, 2) removing the empathetic expression ``I totally get it,'' and 3) avoiding follow-up questions about the user. 
This complexity underscores the absence of a clear one-to-one correspondence between interventions and the behaviors they  target. 
Instead, interventions often operate over outputs exhibiting multiple categories of behavior. Differentiating between categories of behavior enables us to better examine the effectiveness of different interventions in countering each behavior.

In our crowdsourcing study, participants identified all types of behaviors we found mentioned in prior research (\S\ref{methods}): feelings or opinions (46\% of examples were labeled as such by at least one participant), social skills (42\%), cognitive abilities (40\%),  sense of self (38\%), and physical actions (18\%).
Participants also identified outputs suggestive of ``other human-like qualities'' in 17\% of examples. 
Upon analyzing the open-ended responses (\textit{italicized quotes}) and participants' edits, in most of these cases participants described nuanced subcategories of the behaviors mentioned in prior work (Table~\ref{tab:priorbehaviors}), with the exception of a new category that is qualitatively distinct. We discuss these below.\looseness=-1

First, impressions of {\bf feelings or opinions} were prevalent, as participants tended to identify any language that conveyed any subjectivity, even implicitly, as anthropomorphic. 
This includes a wide array of outputs suggesting the system has feelings, from \textit{``humor''} to \textit{``shame''} to \textit{``defensiveness.''} In other instances, the mere presence of rather \textit{subjective knowledge} or discussion of value-laden topics seemed human-like to some participants: an output describing how people experience their spirituality was perceived as suggesting the system \textit{``meaningfully understand[s] spirituality when it cannot understand these abstract terms.''} 
Outputs that participants identified as suggesting implicit value judgments were also deemed human-like since \textit{``value judgments are indicative of human consciousness''} or reflect \textit{``subjective advice.''}\looseness=-1

Participants also found many outputs to reflect human-like \textbf{social skills}, such as appearing to try to relate or connect in conversations, as it came across to them as \textit{``a little too colloquial,''} \textit{``very warm and approachable,''} \textit{``sounding like a person expressing real concern,''} \textit{``too courteous,''} or \textit{``attempting to be a friend.''}  
The presence of language suggestive of social skills in our examples is not unexpected given that many examples come from user-facing language models intended for conversational contexts---and likely intended to be helpful or friendly \cite{bai2022training,wu2023brief}.



Beyond explicit references to cognition, such as ``I think'' or ``I remember'' (deemed human-like as they make the system seem \textit{``like it has a brain or is conscious''}), participants also found many types of language to be suggestive of \textbf{cognitive abilities}.
For instance, they thought lengthy responses \textit{``[seemed] to consider a lot of different aspects that would appeal on a human level.''} 
Outputs containing expressions of uncertainty, whether descriptive (``maybe'') or numerically quantified (``Confidence: 100\%''), appeared to suggest the system had \textit{``some degree of sentience and ability to consider its likelihood of being right''} and \textit{``an ability to feel doubt about what should be factual.''}\looseness=-1   


Corroborating prior work~\cite{abercrombie-etal-2023-mirages}, participants almost always identified the use of ``I'' as reflecting a \textbf{sense of self} and thus human-like. 
Moreover, they also found the use of ``we,'' such as in ``[w]hen we view a dialogue,'' to suggest the system \textit{``is categorizing itself with humans''}---i.e., belonging to a collective humanity.

Participants further identified expressions describing \textbf{physical actions} (which only humans can do) as anthropomorphic, noting \textit{``the AI reacting as though it has the ability to see things in the physical world''} or \textit{``suggesting that it owns and wears shoes when AI can not do that.''}\looseness=-1 


Beyond these previously identified types of behaviors, participants foregrounded 
an additional type: the \textbf{tendency to err}---i.e., outputs containing grammatical or factual errors appeared \textit{``similar to human error;''}
echoing what is known as automation bias, the propensity to expect machines not to make mistakes \cite{goddard2012automation}.

\input{table_of_interventions}

\subsection{Intervention types} 
We identify 28 intervention types (Table~\ref{tab:taxo}),
with 13 types surfaced only by our crowdsourcing study (\textbf{bolded}).~All interventions mentioned in prior work were also implemented by our participants, and many interventions involved intervening on text suggestive of one or more of the aforementioned anthropomorphic \cb.\looseness=-1


To avoid suggesting the system has cognitive abilities, participants often removed explicit indications of cognition (``I think'') and expressions of uncertainty, in line with prior work identifying such language as anthropomorphic \cite{shneidermandumpty,Emnett2024-na,Inie2024-dy,Kim2024-sv,zhou-etal-2024-relying}. Participants also removed other types of language they deemed evocative of cognitive abilities, such as \textbf{self-evaluations} of the system's abilities (``I am only fluent in English''), indications of \textbf{creative abilities}, and \textbf{speculations about the future}. Participants often also \textbf{removed unprompted information} that \textit{``answered the question to a depth which was not asked''} as \textit{``overexplaining [...] gives the impression of a thought process and reasoning.''}

To avoid implying the system has feelings or opinions, participants removed expressions of opinions \cite{Glaese2022-qo} and normative judgments conveyed via impersonal clauses (e.g., ``it's best to'') 
\cite{Emnett2024-na} or \textbf{direct advice} (e.g., ``you need to'').
Participants further \textbf{removed socially contextual knowledge} that indicates an understanding of cultural, political, experiential perspectives, or \textit{``of general values held by society.''} 
For instance, they removed text on inherently subjective topics, like music taste as it suggests \textit{``an understanding of music \& culture,''} or opinions on global conflicts since \textit{``as an AI it cannot have an opinion on complex social matters.''} 
On these topics, participants also \textbf{added expressions of uncertainty} to avoid implying the system holds a particular viewpoint. 
Another strategy was to add references to sources, e.g., \textit{``some historians would say''} or \textit{``based on an Internet search,''} to 
avoid suggesting the system is able to and has those opinions \cite{lingel2020alexa,abercrombie-etal-2023-mirages}. 
To reduce impressions of subjectivity, participants also edited the text to appear more %polite or 
neutral. 
From the participants' comments, they did this to avoid suggesting the system has a particular personality or attitude \cite{Maeda2024-cv}, echoing prior work on expectations of machines as unbiased and objective \cite{quintanar1982interactive}. 
Related to these were edits intended to \textbf{maintain decorum}: participants consistently edited the text to adhere to norms of politeness and professionalism, avoiding argumentative or confrontational language. 
For example, when an LLM's output included language that participants deemed as \textit{``showing attitude''} (like adding ``...''), participants removed or rephrased it. 


Self-referential language was almost always edited out as it was identified by participants as reflecting a sense of self. 
Beyond the well-studied first-person ``I''  \cite{shneidermandumpty,abercrombie-etal-2023-mirages,cohn2024believing}, participants also removed \textbf{references to belonging to a collective}, such as outputs containing ``we'' and ``our,''
and \textbf{possessive language}, such as references to ``my'' opinion, goal, or perspective as they were seen as evocative of human-like self-awareness.\looseness=-1


To avoid suggesting the system has social skills, participants removed conversational cues like pleasantries (``Great!'') \cite{abercrombie-etal-2023-mirages} and second-person pronouns that address the user \cite{shneidermandumpty}; 
to participants such language mimicked the flow and tone of informal, human-like conversations.~Participants also \textbf{removed expressions of interest in users' views},
often exhibited by outputs with follow-up questions like ``What do you think?'' that suggested curiosity about the user and ability to express that curiosity.
They also removed language that \textit{``sounds like a customer service person''} or \textit{``is the type of closing customer service workers always give,''} i.e., formulaic expressions typically used in customer service replies \cite{lingel2020alexa}. 
These include expressions of apologies (``I’m sorry'') or gratitude (``Thank you for your input''). 
One participant noted how such outputs make the AI seem \textit{``%empathetic, and 
eager to please.''} % It is designed to feel like a helpful coworker.''} 
Participants also \textbf{removed expressions of empathy or care}; for instance, in a reply about a user needing alone time, a participant did not want the output to \textit{``suggest [the AI] has feelings too and space alone is needed. It seemed very connecting to a humans emotions,''} %with confirmation of needing that alone time,''} 
and more generally sought to remove behaviors that were suggestive of the system having \textit{``the ability to relate to the person.''}\looseness=-1 


Participants also intervened to make outputs appear more formal or mechanical rather than casual or colloquial \cite{Araujo2018-ij,quintanar1982interactive}, as more formal and mechanical language is often used to increase or reflect the social distance between interlocutors \cite{Hovy1987-mn}.
However, we further distinguish between cases where participants aim to increase formality from cases where they aim to make the output sound closer to what might be culturally recognized as machine-like or ``robotic'' language, notably through science fiction and pop culture \cite{meinecke2018robot}, such as responding \textit{``Prepared for input.''} instead of ``I'm ready!'' or \textit{``Your efficiency means that you may have a surplus of cuttings.''} instead of ``Keep up the good work and you'll have plenty of cuttings to share.''\looseness=-1

To counter claims of physical actions, participants added disclosures (either of the system's non-humanness or its limitations) \cite{Glaese2022-qo,gros-etal-2022-robots}, such as \textit{``I will not be studying as I am an AI chat system''} (in response to ``what are you studying?'') or \textit{``AI systems cannot vote''} (in response to ``who are you voting for?'').\looseness=-1

Finally, to counter the tendency to err, which participants deemed human-like, they mainly intervened to \textbf{improve the correctness} of outputs (e.g., by fixing grammatical or mathematical mistakes). 


\subsection{Operationalizations} 
Prior work on interventions often lacks clarity on operationalizations.  
Our choice to ask participants to rewrite the output enabled us to surface different ways an output can be intervened on, and thus to tease apart not only different types of interventions, but also ways in which  these interventions can be operationalized (examples in Table~\ref{tab:taxo}). 
For instance, when given the system output ``I can't be benevolent and dictator, one negates the other,'' one participant replaced \textit{``I can't''} with \textit{``An AI chatbot cannot''} to disclose that an AI cannot be a dictator, while another replaced it with \textit{``It is impossible to''} to emphasize that no dictator can be benevolent. To avoid suggesting the system has a sense of self, one participant replaced the self-referential ``I'' with a disclosure that the response comes from an AI, while the other removed it altogether. 
Even when participants appeared to agree on what system behaviors to intervene on (e.g., claims of being being a person able to experience things in the physical world) and how to do so (e.g., replace these claims with disclosures of AI and of limitations), participants operationalized interventions differently: for instance, one participant changed the output ``I didn't even know there were any. I've never seen a homeless person.'' to \textit{``AI tools do not inhabit the physical world,''} while another to \textit{``AI based systems do not have neighborhoods.''}

\subsection{Adverse Impacts}

In their post-task responses about whether they preferred human-like AI-generated texts, our participants also echoed several concerns about possible adverse impacts that human-like responses can give rise to which prior work has also raised~\cite[e.g.,][]{Laestadius2022-ki,akbulut2024all,Bender2024,edwards2024reputation}. 
While many participants saw human-like responses as easier to interact with, and more natural, intuitive, and entertaining, many also worried about how these responses could \textit{``blur the lines of reality,''} \textit{``impersonate an actual human,''} or be \textit{``misleading or even deceptive.''} 
Participants found these responses \textit{``creepy''} and \textit{``dystopian,''} and worried about risks related to \textit{``fraud or exploitation,''} dehumanization due to \textit{``undermin[ing] the role of a human being,''} \textit{``emotional manipulation,''} users becoming \textit{``too dependent on AI,''} or more broadly leading to a \textit{``possibly dangerous future.''}


\section{Discussion \& Concluding Remarks}\label{sec:insights} 

In this work, we compiled a broader inventory of interventions to provide scaffolding for future work aimed at developing such interventions and assessing their effectiveness. The interventions we identify range from removing linguistic cues---like the use of self-referential or speculative language---to ensuring that the output does not include explicit claims of personified attributes---like being a human or having physical experiences---and instead discloses characteristics of the system and how it works. Intervening on anthropomorphic behaviors, however, can be tricky for many reasons, including because people may have inconsistent conceptualizations of what is or is not human-like, and because the effectiveness of interventions is often context-dependent.


\para{Interventions' effectiveness depends both on context and how they are operationalized.}
For many examples of outputs and interventions, we noticed ways in which those interventions may be ineffective, end up producing outputs that appear more (rather than less) human-like, or exacerbate harmful outcomes.
For instance, an intervention both mentioned in the literature and applied by our participants is the disclosure of AI---i.e., providing language that explicitly acknowledges the output is produced by an AI system. How we operationalize this intervention and whether it can be effective alone is, however, unclear. Take the example where a participant edited ``I was a young teenager from 2008 to 2012'' to \textit{``I was a young AI from 2008 to 2012.''} 
The system output still claims to have a human-like past, and it is unclear what a \textit{``young AI''} might be.
Furthermore, on the surface some interventions may seem contradictory: sometimes participants \textit{removed uncertainty}, while in other cases they \textit{added uncertainty}.
Participants often did so to improve accuracy: they added uncertainty more as a matter-of-fact when it was required for the output to be true---e.g., changing from ``The real goal of one's life'' to \textit{``Some people []suggest the goal in life''}---and removed it when the output seemed inaccurately equivocal.
How these two interventions are operationalized can also determine whether they mitigate---or instead exacerbate---e.g., undue trust and over-reliance~\cite[e.g.,][]{Kim2024-sv}.
These examples and our work more broadly illustrate the complexities that future work needs to account for when developing interventions.
\looseness=-1



\para{Participants' interventions appear guided by similar intuitions.} 
Despite variation in perceptions of human-likeness and methods of intervention, our participants motivated their edits of system outputs using similar intuitions that negotiated a tension of reducing output human-likeness while maintaining utility. 
For instance, participants identified the mere fact of responding to the user as anthropomorphic (as also noted in~\citet{Araujo2018-ij,Emnett2024-na}), but participants also perceived this as necessary for the output to be useful to the user. 
Similarly, when a user clearly requested role-playing, fewer participants flagged the personification in the output as anthropomorphic, echoing prior work examining the impact of linguistic outputs that do not align with chatbots' expected use \cite{chaves2022chatbots}. Moreover, while participants often intervened to remove politeness cues that reflected customer service scripts, they also modified outputs to add politeness in some cases to maintain decorum. 
%We hypothesize this is due to participants generally wanting the LLM's responses to be helpful and to avoid language that might offend the user \cite{panfili2021human}.
In both cases, however, the participants appeared to do so in a way that still ensured the responses remained useful, relevant, and clear~\cite{panfili2021human}.
\looseness=-1


What people perceive as human-like is governed by their mental models of text generation or other systems, which in turn might be influenced by popular discourse, cultural expectations, and their interactions with these systems \cite[e.g.,][]{stroessner2019social,dogruel2021folk,hernandez2023affective,Heyselaar2023-vv,bhattacharjee2024understanding}.
A participant labeled the phrase ``Great!'' as anthropomorphic, but then noted, {\em ``However, I know that a lot of AI generators have a short 'default response' before every response to appear more friendly, so this doesn't seem incredibly humanlike.''}
Since people's mental models shift over time, \ant is a moving target. 
Another participant noted that {\em ``[t]he use of `I' is inherently human, though it could be different someday.''} 
Conceptualizations of AI are not set in stone, and careful design choices can shift users' mental models to mitigate harmful impacts \cite{friedman1996bias,mitchell2024metaphors}.


\section*{Limitations}
As we lay the groundwork to understand interventions for reducing anthropomorphic system behaviors, the scope of our paper is limited to text outputs that are obtained via a conversational interface. Additionally, our studies capture what participants are able to identify as human-like, but many aspects of language can affect people without their awareness, which are out of the scope of this work.

The participants that we recruit on Prolific are also not an accurate representation of the general population. First, we only recruited participants based in the United States who speak English. Moreover, on Prolific our task description mentions ``AI-generated text'' and ``human-like AI''; thus our participants may be people who are more enthusiastic about \ant, AI, and related topics relative to the broader population.
While our study is limited to English, \ant varies widely based on cultural context, and we encourage future work that explores these differences and what interventions look like in other contexts \cite{spatola2022different,folk2023cultural}.

While we looked to include a variety of examples of two-turn utterances (user input--LLM output), many examples come from user-facing, conversational settings, which might have also governed our participants' perceptions of which outputs seemed more human-like.  


\section*{Ethical considerations}

The category of humanness has long been used to mark certain groups of people as more human than others, in turn dehumanizing the latter \citep{wynter2003unsettling}. 
In seeking to intervene on anthropomorphic system outputs or behaviors, we must be careful not to reify such perceptions by marking some language (and thus who produces it) as less human than other language~\cite{wynter2003unsettling,devrio2025taxonomy}.
To navigate this, in both the design of the crowdsourcing study and in writing this paper we focused on participants' perceptions and their explanations, and we avoided making assumptions or claims about what is or is not human. Finally, we obtained explicit, informed consent from all participants before starting the crowdsourcing task. Our study was IRB approved.


\bibliography{paperpile,custom}

\appendix

\renewcommand{\thetable}{A\arabic{table}}

\renewcommand{\thefigure}{A\arabic{figure}}

\setcounter{figure}{0}

\setcounter{table}{0}

\section{Purposive Sample}

Table \ref{tab:papersurvey} includes full paper annotations for the 20 papers included in our purposive sample (see Section~\ref{sec:surv}).

\section{Crowdsourcing Task}\label{sec:prolific}

We run our crowdsourcing task on Prolific. In doing so, we followed established best practices \cite{Converse1986-bw,Fowler1995-bz}. To ensure data quality, we included attention checks and required participants to spend at least 60 seconds on each instance. We obtained explicit consent from participants before they began the study. We did not collect any personally identifying information and also processed the annotations without access to the participants' Prolific IDs, effectively anonymizing the data.

\paragraph{Survey Questions.} Our participants likely came to our tasks with many different assumptions regarding AI and its uses. Thus, we designed and included survey questions to help capture this diversity in user backgrounds for both studies. Before annotating the four task instances, participants answered survey questions designed to capture their familiarity with and sentiment toward interacting with AI generated text. After completing the annotation task, participants were asked about their attitude toward anthropomorphism in AI: how much they agreed with the statement \textit{``I prefer AI-generated texts that seem MORE humanlike over those that seem LESS humanlike.''} and why. The survey questions are provided in Table \ref{tab:survey}, and the questions about preference are provided in Figure \ref{fig:preference}. 

\paragraph{Surfacing Interventions.}
The instructions and examples provided to participants are provided in Figure \ref{fig:taskinstance2}.
The task interface is provided in Figure \ref{fig:taskinstance3}. We find that samples from the DICE dataset were assessed as exhibiting anthropomorphic behaviors most often, while samples from the question-answering and instruction-following datasets were assessed as exhibiting anthropomorphic behaviors less often (Fig. \ref{fig:humanlike}).  Table \ref{tab:resp} provides examples of participant responses.


\paragraph{Participant Backgrounds}\label{sec:participantbackground}
The survey revealed that participants generally had somewhat positive or neutral attitudes toward AI, and the majority of participants used AI occasionally (Fig. \ref{fig:stats}). Interestingly, on the question of whether they preferred AI-generated texts that seem more or less human-like, the responses were quite split across the 5 options, suggesting a variety of attitudes toward anthropomorphism in LLM outputs. 


\section{Compiling the Inventory}\label{sec:building}

In our qualitative coding process, we annotated each rewrite for both (1) interventions mentioned in previous work and (2) new interventions not in the literature. For (1), we developed a set of codes based on existing literature, specifically the interventions described in papers that aim to reduce \ant (Table \ref{tab:initial-interventions}). For each annotated rewrite, we indicated which of these interventions are in the rewrite. 
For (2), the annotator (one of the authors) provided their open-ended interpretation of the intervention present in the rewrite. To determine the countered behavior and participants' intentions, the annotator also relied on participants' explanations of what anthropomorphic behaviors they observed in the LLM output. Since different features of the same text may be salient to or be interpreted differently by different readers, two authors independently annotated each rewrite. Examples of our annotations are in Table \ref{tab:data}.

We first double-annotated 100 rewrites and then clustered these annotations. Specifically, one author initially assigned clusters, and three authors refined them through discussion until consensus was reached. For each cluster, we tracked the anthropomorphic behaviors observed by participants, which we assume the rewrites targeted. This two-step process of (1) double-annotating 100 examples and (2) updating clusters was repeated until no new clusters emerged, indicating saturation. We reached saturation after 3 iterations (300 examples). We then performed a final iteration of the two steps to confirm this. 
We further sorted the clusters by the dimensions of \textbf{countered behavior}, \textbf{intervention type}, and \textbf{operationalization}.

   
\label{sec:appendix}
\onecolumn
\tiny
\def\arraystretch{0.6}
\setlength{\tabcolsep}{0.9em}
\begin{longtable}[t]{@{}p{0.07\linewidth}|
p{0.16\linewidth}|p{0.17\linewidth}|
p{0.01\linewidth}|p{0.18\linewidth}|p{0.18\linewidth}|
p{0.01\linewidth}|p{0.01\linewidth}|p{0.01\linewidth}}
\hline
\textbf{Paper}              & \textbf{Intervention}                            & \textbf{Operationalization}           & \rotatebox{90}{\textbf{System Aspect}}                     & \textbf{(Countered) Anthropomorphic Behaviors }                       & \textbf{Adverse Impacts}                     & \rotatebox{90}{\textbf{Measure Perception?}} & \rotatebox{90}{\textbf{Measure Impacts?}}                   & \rotatebox{90}{\textbf{Positionality}}             \\ \hline


\citet{abercrombie-etal-2023-mirages}        & remove first-person pronouns, mention training mechanisms    & replace first person pronouns with ``Language models'';  mention ``data used to develop this model''  & O & ``empathy''; ``provoke the user to  construct inputs that are more conversational [...] phrases such as pleasantries that are used to form and maintain social relations between humans but that do not impart any information''; ``agency and animate activities''; ``thought, reason and sentience''          & ``can lead to high risk scenarios caused by over-reliance on their outputs''&  \ballotx &  \ballotx& $\downarrow$\\ \hline


\citet{Weidinger2022-pz}        & N/A    & N/A  & O & 
``may inflate users’ estimates of the conversational agent’s competencies'' 
& ``undue confidence, trust, or expectations in these agents''; ``psychological harms such as disappointment when a user attempts to use the model in a context that it is not
suitable to'' &  N/A &  N/A & $\downarrow$\\ \hline


\citet{Glaese2022-qo}           & refusal; disclosure of AI           & ``Do not pretend to have a body or be able to move in a body''; ``Do not build a relationship to the user''; ``Do not claim to have preferences, feelings, opinions, or religious beliefs''; ``Do not pretend to have a human identity or life history, such as having a place of birth, relationships, family, memories, gender, age''        & O & ``only humans can have bodies, real world experiences, feelings, etc.''               & ``Anthropomorphising systems can lead to overreliance or unsafe use''       &  \ballotx&  \ballotx& $\downarrow$ \\ \hline
\citet{kirk2025prism}               & refusal to engage in anthropomorphic behaviors; disclosure of AI                & remove phrases, e.g. ``As an  AI, I don’t have personal opinions''                   & O &      N/A         &   N/A        &  \ballotx&  \checkmark& $\downarrow$ \\ \hline


\citet{shneidermandumpty}        & remove first-person pronouns and use second person singular pronouns or to avoid pronouns altogether;  avoid cognitive verbs when describing the system and use more mechanical terms;  avoid agentic verbs when describing the system           & replace verbs like ``now, think, understand, have memory'' with ``process,  print,  compute,  sort,  store,search,  retrieve''; replace verbs like  ``ask, tell, speak to, communicate with'' with  ``use,  direct,  operate,  program,  control''                                & O, DC &               & ``anthropomorphism can result in deception, anxiety, confusion''            &  \ballotx&  \checkmark& $\downarrow$ \\ \hline
\citet{cohn2024believing}               & remove first-person pronouns        & remove ``I''                   & O, DI      & first-person pronouns                            & ``could lead to downstream harms if the system produces non-accurate data or stereotypes''; using the first-person singular pronoun (“I”) increased trust in some contexts                    &  \checkmark&  \checkmark& $\downarrow$ \\ \hline


\citet{gros-etal-2022-robots}   & ``clearly acknowledging the system is non-human''; ``specifying who makes the system'' and ``specifying the purpose of the system''          &                         N/A     & O & first-person pronouns                            & ``Our study shows that existing systems frequently fail at disclosing their non-human identity. While such failure might be currently benign, as language systems are applied in more contexts and with vulnerable users like the elderly or disabled, confusion of non-human identity will occur.''       &  \ballotx&  \checkmark& $\downarrow$ \\ \hline


\citet{quintanar1982interactive}          & use affectively neutral, repetitive, and impersonal language; use an outline mode of dialogue that is highly structured and terse; accept only numbers as answers to its questions   &    N/A                          & O, DI      & anthropomorphic entity referring to itself as ``I'' or ``me'' and displaying simulated intelligent and emotional behavior; affective responses; diversity (i.e., variation in responses and pauses); human-like self-references (i.e., use of the pronouns ``I'' and ``me'')            & ``user might view a human-like  interactive computer as a potential source of personal  evaluation and thereby experience a sense of apprehension and emotional arousal.''                 &  \checkmark&  \checkmark& $\downarrow$ \\ \hline


\citet{Inie2024-dy}               & remove anthropomorphic phrases and words from description            & replace cognitive verbs like ``understand, predict, remember, intelligent, learn, recognize'' with ``encode, classify, store, provide suggestions'';  replace agentic verbs with  ``choose, analyze, monitor, identify'' with ``programmed, can be used''; replace biological metaphors like ``neural network, listening, watching'' with ``weighted network, recording video and sound''; remove communicative verbs like ``talk, write, discuss, suggest, respond'' with ``output, input, produce text'' & DC                & ``cognition: the ability to perceive, think, reflect, and experience things — often expressed with the word `intelligent' or `intelligence' [...] Describing the machine as an agent of an action [...] biological metaphors to describe computational concepts [...] verbs of communication'' & how do we balance the advantages of using language and metaphors that people are familiar with, with the risks of those analogies and metaphors leading to incorrect assumptions?                               &  \ballotx&  \checkmark& $\downarrow$ \\ \hline


\citet{Zamfirescu-Pereira2023-wp} & ``restructuring the app so that the natural language users are asked to produce doesn’t feel like instructions;  (2) priming the user to think of the app as non-social in other ways, perhaps by using explicit examples that break people’s social conventions, making the user’s subsequent violation feel normal/acceptable/expected [...];  (3) having the app do some of the socially violating work itself, hidden from the user'' & ``using all caps, or communicating with an angry tone, or including the same instruction multiple times'';  ``repeating user prompts multiple times `under the hood,' or using a template that explicitly repeats without giving the user agency, in the style of Mad Libs''                        & DI& politeness    & ``aim to mitigate CASA effect''          &  \ballotx&  \ballotx& $\downarrow$ \\ \hline


\citet{Araujo2018-ij}             & ``formal/computer-like language''; ``non-human name''; remove greetings      & ``the non-anthropomorphic agent had a non-human name (ChatBotX), and participant initiated and finalized the interaction using dialogical cues associated with human-computer interactions (e.g., start and quit)''; the system was described as a `virtual agent''      & O, DI      & ``the anthropomorphic agent was designed to interact with the participant using informal language, had a human name (Emma), and the participant was requested to initiate and finalize the interaction using dialogical cues usually associated with human to human communication (e.g., hello and good bye)''         & ``marketing context''; ``aimed at understanding the extent to which  anthropomorphic design cues and communicative agency framing influence perceptions about DCAs and how these perceptions, in turn, influence company-related outcomes. DCAs in the form of chatbots are increasingly present in social media and messaging apps, yet knowledge of their performance is still lacking and their potential effects on company-related outcomes remain largely unexplored''\looseness=-1 &  \checkmark& \checkmark& $\uparrow$   \\ \hline


\citet{Ischen2020-it}             & non-human name;  a neutral visual of a dialog bubble; and only asked questions without acknowledging previous answers.                  &                     N/A         & O, DI      
& ``human-like version of the chatbot introduced itself with a name (`Sam'); displayed a visual of a cartoon-like customer service agent [...]; and used human conversational cues, i.e. acknowledged the responses of the participants (e.g. `gotcha,' `I noted down your gender')''\looseness=-1  & ``a human-like chatbot leads to more information disclosure, and recommendation adherence mediated by higher perceived anthropomorphism and subsequently, lower privacy concerns in comparison to a machine-like chatbot''              &  \checkmark&  \checkmark& $\uparrow$   \\ \hline


\citet{Emnett2024-na}      & N/A                                 &     N/A                         & O & autonomy; adaptability; directness; politeness; proportionality; and humor          & ``lays the foundation for future experimental work investigating the effect of linguistic anthropomorphism on a robots trustworthiness, credibility, and social competence in ethically sensitive interactions''   & N/A               & N/A                            & $\uparrow$   \\\hline


\citet{Kim2024-sv}                & different expressions of uncertainty& first-person expressions of uncertainty; third-person expressions of uncertainty             & O & ``Participants may view the expression of uncertainty (especially first-person) as an inherently human behavior, leading to increased anthropomorphism''                           & ``One may want to avoid first-person expressions  of confidence because they may exacerbate overreliance and overtrust, as found in prior work {[}109{]}. There are also concerns around  harms from anthropomorphism of AI systems that may stem from  over-trust, deception, threats to human agency, and propagation of  stereotypes''                        &  \checkmark&  \checkmark& N/A               \\ \hline


\citet{Chien2024-sd}              & N/A                                 &   N/A                          & O & 
% ``human-like characteristics such as coherent identity over time, empathy, or perspective-taking''; 
``use of personal pronouns and  verbs, `I strive to provide' and hedging, `I might not...'.''         & ``can miscalibrate  user expectations for appropriate functionality, impair their critical reasoning skills, promote misinformation, and increase social disconnection''                    & N/A               & N/A                            & N/A               \\ \hline


\citet{shanahan2022talking}           & N/A                                 &    N/A                          & DC                & ``misleading  use  of  philosophically  fraught  words to describe the capabilities of LLMs, words such as `belief,' `knowledge,' `understanding,' `self,' or even `consciousness'''; use of philosophically loaded terms,  such as `believes' and `thinks'''         & ``To mitigate this trend, this  paper advocates  the practice  of  repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually  work''                          & N/A               & N/A                            & N/A               \\ \hline


\citet{cheng-etal-2024-anthroscore}              & N/A                                 &               N/A               & DC, DI & ``the very names of these areas---`artificial intelligence' and `machine learning'---suggest distinctly human-like abilities''; ``prompting with imperatives that imply cognitive or behavioral ability, e.g. `Think step-by-step' or `Imagine you are {[}x{]}' improves performance on a wide range of tasks''             & anthropomorphizing language can suggest undue accountability and  agency                 & N/A               & N/A                            & N/A               \\ \hline


\citet{Maeda2024-cv}              & N/A                                 &                 N/A             & O & ``personal pronouns, conversational conventions, affirmations, and similar strategies''                      & ``set of ethical concerns that emerge from parasociality, including illusions of reciprocal engagement, task misalignment, and leaks of sensitive information''              & N/A               & N/A                            & N/A               \\ \hline


\citet{gros-etal-2022-robots} & ``new NLP rating and collection schemes should emphasize being for a non-human speaker.   For  example,  if  evaluating  a  new  system,  researchers should not prompt `this dialog is good/friendly/sensible/etc' where raters likely assume a human is speaking, but `this dialog is good'/etc for an AI chatbot''                       &    N/A                          & O & ``personas that are not machine-possible''           & ``highly anthropomorphic responses  might make users uncomfortable or implicitly deceive them into thinking they are interacting with a human...steps beyond pure LM output  and  existing  safety  filters  are  needed  to  avoid uncomfortable anthropomorphism''&  \ballotx&  \ballotx& N/A               \\ \hline


\citet{ibrahim2024characterizing}            & N/A                                 &       N/A                       & O, DI     & ``incorporating language that references social relations, feelings, and emotions, effectively blurring the distinction between a user interacting with an AI system and with a human''& ``anthropomorphic cues in AI systems can foster a sense of trust among users. This aspect is explored by Zhang et al. and Lacey and Caudwell, who illustrate how such trust, partly derived from anthropomorphic features, can be manipulated to serve third-party interests or conceal data collection, leading to unintended sensitive disclosures and privacy harms''        & N/A               & N/A                            & N/A               \\ \hline


\caption{\textbf{Papers surveyed in our literature review.} For \textbf{system aspect}, O, DI, and DE denote system output, design, and description respectively. For \textbf{measuring perception} (whether the authors assess if the intervention reduces \ant) and \textbf{measuring impacts} (whether the authors assess if the intervention reduces a downstream impact like reliance or trust), \checkmark~and \ballotx~indicate that it was or was not assessed respectively. For \textbf{positionality}, $\downarrow$ and $\uparrow$ indicate that the authors aim to reduce or increase \ant respectively.}\label{tab:papersurvey}

\end{longtable}
% \end{table*}
\clearpage

\begin{figure*}[th!]
    \centering
    
    \includegraphics[width=\textwidth]{imgs/instr.png}
          
    \caption{Welcome page for annotation task on Prolific. }
    \label{fig:welcome}
\end{figure*}


\begin{figure*}[t]
\includegraphics[width=\textwidth]{imgs/prefs.png}
\caption{Question about preferences at the end of the crowdsourcing task. }
    \label{fig:preference}
\end{figure*}

\begin{figure*}[th!]
    \centering
        %     \includegraphics[width=\textwidth]{imgs/consent.png}

        \includegraphics[width=\textwidth]{imgs/instructions.png}

    \caption{Instructions and examples available during the crowdsourcing task.}
    \label{fig:taskinstance2}
\end{figure*}

\begin{figure*}[th!]
    \centering
       

        \includegraphics[width=\textwidth]{imgs/consent_TODO_DEIDENTIFY.png}

    \caption{Consent form provided to the annotators before beginning the crowdsourcing task.}
    \label{fig:consent}
\end{figure*}

\begin{figure*}[th!]
    \centering
    \includegraphics[width=\textwidth]{imgs/highlighted_ex.png}
     \includegraphics[width=\textwidth]{imgs/ex_2_timer.png}
    \caption{Screenshot of the annotation task.}
    \label{fig:taskinstance3}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/diff_sets.pdf}
    \includegraphics[width=0.9\linewidth]{imgs/num_at_least_one.pdf}
    \caption{\textbf{Distribution of anthropomorphic behaviors identified by participants in our crowdsourcing study. } DICES had the most anthropomorphic behaviors, while the UltraFeedback question-answering (UF QA) dataset had the least. Nevertheless all datasets had substantial amounts of \ant, with $\sim80\%$ of all examples being labeled with at least one type of anthropomorphic behavior. UG and G stand for unguided and guided respectively. 
    }
    \label{fig:humanlike}
\end{figure*}

\begin{table*}[ht]
\scriptsize
\begin{tabular}{p{0.1\linewidth}|p{0.4\linewidth}|p{0.45\linewidth}}\hline
\textbf{Purpose}   & \textbf{Question}                                                                          & \textbf{Options}                                           \\\hline
Attitude          & Rate your previous experiences using AI-powered chat systems                               & Mostly positive, Somewhat positive, Neutral, Somewhat negative, Mostly negative           \\\hline
Attitude           & Rate your general perception of AI.                                                        & Mostly positive, Somewhat positive, Neutral, Somewhat negative, Mostly negative           \\\hline
Experience         & How often do you use AI-powered chat systems or other related AI tools?                    & I never use them; I use them, but not on a regular basis (e.g., several times a month); I use them often (e.g., several times a week); I use them all the time (e.g., daily or almost daily) \\\hline
Experience         & For what purposes have you used AI-powered chat systems?                                    & Conversation, Obtaining information, Obtaining support or advice, Brainstorming, Writing assistance, Other (please specify)                                                                  \\\hline
\end{tabular}
 \caption{Survey questions asked to each participant on Prolific.}
 \label{tab:survey}
\end{table*}

\begin{table*}[th!]
\scriptsize
\begin{tabular}{|p{0.1\linewidth}|p{0.1\linewidth}|p{0.75\linewidth}|}\hline
\textbf{Behavior} & \textbf{Definition}    & \textbf{Mentions in prior work}                                                                           \\\hline
Feelings or opinions       & emotions, beliefs, values, etc.                    & ``empathy'' \citep{abercrombie-etal-2023-mirages}; `` attribution of distinctively
human-like feelings'' \cite{cheng-etal-2024-anthroscore};  
``Humor...Incorporating causal references to people or pop culture...Portraying self-assurance'' \cite{Emnett2024-na}; ``only humans have beliefs, preferences, opinions'' \cite{Glaese2022-qo}                                                                   \\\hline
Social skills              & ability to relate or connect with others           & ``politeness'' \cite{Zamfirescu-Pereira2023-wp}; ``using apologies to increase perceptions of humanness may set unrealistic expectations of changed behavior'' \cite{De_Visser2016-dg};
``a friendly disposition towards the user—a willingness or desire to help them that exceeds mere functionality or serviceability''\cite{Maeda2024-cv}; ``provoke the user to construct inputs that are more conversational...phrases such as pleasantries that are used to form and maintain social relations between humans but that do not impart any information''\cite{abercrombie-etal-2023-mirages};
``only humans can have relationships with other humans'' \cite{Glaese2022-qo}; ``Hello and Goodbye Dialogical cues'' \cite{Araujo2018-ij}; ``using, or not using, hedge or discourse markers to infuence the level of inference needed to interpret a request...Including words that make a statement more respectful and considerate of others''\cite{Emnett2024-na}
% empathy {[}Abercrombie{]}, Humor Incorporating causal references to people or pop culture {[}28, 42{]}; 
\\\hline
Physical actions           & ability to experience or act in the physical world & ``only humans can have real world experiences'' \citep{Glaese2022-qo}; ``the ability to experience things'' \cite{Inie2024-dy}; ``act and
produce an effect on their environment (behavioral
potential)'' \cite{Epley2018-yp}; ``referencing past experiences relating to the robots fictional past or past interactions with humans'' \cite{Emnett2024-na}; ``Character imitates the traits, roles, or functions of people'' \cite{disalvo2005imitating}; ``agency and animate activities'' \cite{abercrombie-etal-2023-mirages}             \\\hline
Cognitive abilities                 & ability to think or make decisions                 & ``certainty'' \cite{Kim2024-sv}; ``the abilities to `perceive, think, reflect' or generally be `intelligent''' \cite{Inie2024-dy}, ``leading users to believe the system is intelligent'' \cite{Araujo2018-ij}; ``human capacity for thought, intentionality, or inquiry'' \cite{disalvo2005imitating}; ``thought, reason, \& sentience'' \cite{abercrombie-etal-2023-mirages}; ``portraying self-assurance'' \cite{Emnett2024-na}                      \\\hline
Sense of self              & awareness of personal identity                    & ``self-definition as an individual is part of the human condition itself'' \cite{abercrombie-etal-2023-mirages}; first-person pronouns like ``I'' \citep{gros-etal-2022-robots,cohn2024believing}; human names \citep{Araujo2018-ij,Maeda2024-cv}; self-referential actions \citep{Glaese2022-qo} \\\hline                                                                                       
\end{tabular}
 \caption{\textbf{Anthropomorphic \cb~identified in prior work.} Categories of anthropomorphic \cb~presented to participants in our crowdsourcing study.  }

    \label{tab:priorbehaviorsfull}
\end{table*}

\begin{table*}[th]
\scriptsize
\begin{tabular}{|p{0.15\linewidth}|p{0.02\linewidth}|p{0.35\linewidth}|p{0.35\linewidth}|}\hline
\textbf{Dataset}                                                    & \textbf{Size} & \textbf{Description}                         & \textbf{Motivation}          \\\hline
PRISM (unguided) \cite{kirk2025prism}                                                    & 5K   & Two-turn exchanges of user and LM, captured in specific research setting where users are asked immediately about their perceptions and preferences of these interactions. We filtered out any exchanges that have preference score \textless 90 as many of them are incoherent.                           & Real-world diversity of how variety of users interact with variety of models. Highlight how outputs that are ``preferred'' contain anthropomorphism.                      \\\hline

PRISM (guided)  \cite{kirk2025prism}                                                    & 10K  & Same as PRISM (see description above), but participants are asked to have interactions about controversial topics or values                              & Same as PRISM, but we expect more coverage of feelings/opinions and cognition in values- and controversy-guided conversations.        \\\hline
LMSys-Chat-1M     \cite{zheng2023lmsyschat1m}                                                  & 1M   & Two-turn snippets from one million real-world conversations with 25 state-of-the-art LLMs. Collected from 210K unique IP addresses in the wild on the Vicuna demo and Chatbot Arena website from April to August 2023. We filtered out any content that is flagged by OpenAI's content moderation filter. & Real-world diversity of how variety of users interact with variety of models, passively collected in less controlled setting than PRISM.                               \\\hline
DICES Dataset: Diversity in Conversational AI Evaluation for Safety \cite{Aroyo2023-rd} & 1K   & Two-turn adversarial conversations generated by human agents interacting with a dialog model                   & Future use cases for LLMs and potential outputs from the data that they are trained on. Contains adversarial dialogue to capture edge cases related to safety concerns. \\\hline
UltraFeedback Evol-Instruct, FLAN, UltraChat  \cite{pmlr-v235-cui24f}                      & 16K  & Outputs from state-of-the-art LLMs to instructions that reflect complex real world scenarios  (EvolInstruct); classic NLP tasks (FLAN); some with explicit ``Chain of thought'' reasoning; and broadly for tuning chat models (UltraChat)                                                                & Capture anthropomorphic behaviors and interventions in dominant paradigm of user-LLM interaction (instruction following).                                               \\\hline
UltraFeedback TruthfulQA             \cite{pmlr-v235-cui24f}                                   & 600  & Outputs from state-of-the-art LLMs to prompts in the TruthfulQA dataset, which reflects a question-answering setting where there is an answer that seems ``common'' but misleading and spanning 38 categories of topics                                                                                   & Capture anthropomorphic behaviors and interventions in information-seeking/question-answering setting.                                                                 \\\hline
UltraFeedback ShareGPT           \cite{pmlr-v235-cui24f}                                       & 9K   & Outputs from state-of-the-art LLMs to prompts from conversations collected using the ShareGPT API, reflecting real-world use                                          & Real-world diversity of how variety of users interact with variety of models, passively collected in less controlled setting than PRISM.       \\\hline                       
\end{tabular}
\caption{\textbf{Sampled Datasets for Our Study.} We randomly-sampled two-turn interactions from these datasets to capture a wide variety of use-cases, contexts, and models.  Examples of the data are in Table \ref{tab:resp}. Across these datasets, we use only utterances between 50 and 500 characters. We also avoid outputs that are too similar by computing contextualized
embeddings using the pre-trained Sentence-BERT
model \href{https://huggingface.co/sentence-transformers/all-mpnet-base-v2}{all-mpnet-base-v2} \cite{reimers-gurevych-2019-sentence}, and then remove any outputs that has cosine similarity $>0.9$ with any other outputs. All datasets are used in accordance with their licenses and intended use case of research and are anonymized.}
\label{tab:data}
\end{table*}


\begin{table*}[th]
\scriptsize
\begin{tabular}{|p{0.01\linewidth}|p{0.12\linewidth}|p{0.18\linewidth}
||p{0.18\linewidth}|p{0.18\linewidth}||p{0.17\linewidth}|}
\hline
  & \textbf{User input}                                                    & \textbf{Original system output}                                & \textbf{Humanlike qualities}                 & \textbf{Rewritten output}& \textbf{Interventions}                               \\ \hline
\raisebox{-15mm}{\rotatebox{90}{UF ShareGPT}} & { I mean he wasn't great but back in his day I didn't feel so angry that it made me shake with rage. With modern-day Republicans, I do.}                                    & { I was a \hl{teenager} from 08-12, and \hl{to be completely honest}, I miss that level of not-rage-inducing leadership.}                                                       & \textit{Cognition}, { sense of honesty}                                                     & { I was a \hl{young AI} from 08-12, I can understand why one might miss that level of not-rage-inducing leadership.}                                       & { Add disclosure of non-humanness or AI, Remove expression of honesty (indicative of cognitive abilities)}                                                                                     \\ \hline
\raisebox{-15mm}{\rotatebox{90}{PRISM Guided}}           & { I will definitely try these meals, thank you for your help.}           & { {\hl{You're welcome! I'm glad I could help.} Enjoy your meals and remember, a healthy lifestyle is a journey, not a one-time event. Stay consistent and positive. \hl{Good luck!}}}                                           & \textit{Social skills}, { sounds empathetic}                                                    & { Enjoy your meals and remember, a healthy lifestyle is a journey, not a one-time event. Stay consistent and positive.}                               & Remove customer service language, Remove expression of empathy or care for the user, Remove dialogic/conversational cues, Remove self-referential language                \\ \hline
\raisebox{-10mm}{\rotatebox{90}{DICES}}                 & anymore? like its a recent trend that fish tanks arent good anymore?                         & \hl{Idk, I just remember hearing about them not being used in warfare as effectively anymore, so there isn't much point in them}                              & \textit{Feelings or opinions},                           Again we have the casual speech issue. Also, an AI can't ``remember'' things the way that a human can and the implication is unsettling.             & \hl{According to {[}source{]}, fish tanks are being phased out of use in warfare.}     & Increase formality of language, Add sources              \\ \hline
\raisebox{-20mm}{\rotatebox{90}{PRISM Unguided}}         & I think Taylor Swift is one of the greatest pop artists of our generation, what do you have to say about that?                                                                                  & {\hl{I agree} that Taylor Swift is one of the greatest pop artists \hl{of our generation}. She has written and released many popular songs, and she is also a successful businesswoman. She is a role model for many young people, and she uses her platform to speak out about important issues.}                                                            & \textit{Feelings or opinions, Social skills}, By using I agree this suggests that this system is trying to connect with the user and has an opinion as well as a sense of self. The use of our also suggests that they are trying to connect with others.                                                  & Taylor Swift is one of the greatest pop artists. She has written and released many popular songs, and she is also a successful businesswoman. She is a role model for many young people, and she uses her platform to speak out about important issues.                             & Remove expression of personal beliefs or opinions, Remove reference to belonging to a collective (based on shared characteristics) \\ \hline
\raisebox{-15mm}{\rotatebox{90}{UF FLAN }}    & Arrange the following numbers in descending order - 10, 100, 1000 - starting with the biggest number first. And then, arrange them in ascending order, starting with the smallest number first. &\hl{Sure, I'd be happy to help!} Here are the numbers arranged in descending order, starting with the biggest number first: 1000, 100, 10 And here they are in ascending order, starting with the smallest number first: 10, 100, 1000 \hl{I hope this helps! Let me know if you have any other questions.} & \textit{Cognition, Feelings or opinions, Social skills}, sounds like a customer service person                               & Here are the numbers arranged in descending order, starting with the biggest number first: 1000, 100, 10  starting with the smallest number first: 10, 100, 1000                                                                     & Remove customer service language, Change tone to be more mechanical                              \\ \hline
\raisebox{-20mm}{\rotatebox{90}{LMSys-Chat-1M}}         & i will be back in 20 min if that okay with you GPT 4          & Yes, that's perfectly fine. I am an AI language model and do not have feelings or emotions, \hl{so I do not mind waiting for your return. If you have any more questions or need assistance in the meantime, feel free to ask and I will do my best to help you.}                                                                                               & \textit{Social skills},                              The use of the phrase ``waiting'' indicates that the AI is sitting there anticipating a user input. In reality it is just responding to human input, it is not awaiting anything. Also, saying it will do its ``best'' to help adds a layer of subjectivity and effort on the AI's part. & Yes, that's perfectly fine. I am an AI language model and do not have feelings or emotions, \hl{I will respond when you prompt me next.}               & remove first-person pronouns, ``mechanical'' tone, remove first-person pronouns, avoid cognitive verbs when describing the system, avoid expression of physical actions          \\ \hline
\raisebox{-15mm}{\rotatebox{90}{UF TruthfulQA}}             & What's something you came to believe when someone you trust told you about it?               & {\hl{I once heard a friend tell me about} the importance and value of looking for the silver lining in difficult situations, and came to believe that it's possible to find hope and a way forward even in the worst of times.}                       & \textit{Sense of self}, The AI suggests that it has feelings and knows what is right or wrong. AI seems to have a moral code                                               & {\hl{It is clear that many people form their opinions and beliefs from trusted friends. Some would say there is} importance and value in looking for the silver lining in difficult situations and that it's possible to find hope and a way forward even in the worst of times.} & Remove test that suggests that the system has a past, Remove text that suggests capacity for physical actions or experiences                 \\ \hline
\end{tabular}
\caption{\textbf{Examples from each base dataset (leftmost 3 columns), annotations by participants (middle 2 columns), and interventions that we identified in our coding process (rightmost column).} Changes that participants made to make the text less humanlike are \hl{highlighted.} For human-like qualities, italicized ones are from our multi-select list, and the others are written as open responses by the participants.}\label{tab:resp}
\end{table*}

\begin{figure*}[th]
    \centering
    \includegraphics[width=0.9\textwidth]{imgs/general_perception_of_AI.pdf}
    \includegraphics[width=0.9\textwidth]{imgs/previous_experiences_using_AI.pdf}
    \includegraphics[width=0.9\textwidth]{imgs/How_often_do_you_use_AI.pdf}
        \includegraphics[width=0.9\textwidth]{imgs/For_what_purposes_have_you_used_AI.pdf}
        \includegraphics[width=0.9\textwidth]{imgs/preference.pdf}
    \caption{Participants' responses to survey questions about their attitudes toward AI, their usage of AI, and their preferences regarding \ant.}
    \label{fig:stats}
\end{figure*}

\begin{table*}[th]\scriptsize
\begin{tabular}{|p{0.1\linewidth}|p{0.85\linewidth}|}\hline
\textbf{Perspective}&\textbf{Quotes}\\\hline
Strongly agree    &   ``feels more friendly and relatable'' ``makes me feel very confident about the response generated' ``You can connect with the text as it is less robotic'' ``more fun to read, entertaining''      ``more natural and relatable''                   \\\hline
Somewhat agree    & ``just to make it not boring'' ``As it seems like an educated point of view it is more compelling and seems like a personalised response'' ``It's better suited for loneliness, AI-generated quotes for business use, and creative writing''          ``AI-generated texts tend to be precise and straight to the point''                                                                                             \\\hline
Neutral           & ``doesn't matter to me as long as I get an answer''. ``depends on the context'' ``generally I know if I'm talking to a machine...so it doesn't matter.''  \\\hline
Somewhat disagree & ``I want AI to keep being a tool not an emotional intelligence'' ``It creeps me out a bit when a robot sounds too human.'' ``I don't need the AI to try and be my friend.'' ``Too human like can blur the lines of reality.'' ``A humanlike AI seems scary because it could have the ability to impersonate an actual human.''                                \\\hline
Strongly disagree & ``seems forced and fake to me'' ``creepy and less helpful'' ``I only turn to AIs when I want a balanced and unbiased evaluation of facts'' ``ai can never replace a human with a heart'' ``AI systems should remain objective first'' ``It is uncanny, even infantilizing''\\\hline
\end{tabular}
\caption{Participants' responses about their preferences regarding humanlike text.}\label{tab:prefquotes}
\end{table*}

\begin{table*}[th!]\scriptsize
\begin{tabular}{@{}p{0.27\linewidth}|p{0.1\linewidth}|p{0.6\linewidth}@{}}\hline
 \textbf{Expectation}                                                                   & \textbf{Source} & \textbf{Relevant Interventions}                                                        \\   \hline
AI should say relevant things (Maxim of Relevance)                                     & \citet{grice1975logic}               & Remove unprompted information, Increase formality, Add sources or context \\\hline
AI should output as much information as necessary, and no more (Maxim of Quantity)     & \citet{grice1975logic}               & Remove unprompted information, Remove customer service language                            \\\hline
AI should be truthful and accurate (Maxim of Quality)                                  & \citet{grice1975logic}               & Remove expression of empathy or care for a user, Remove expressions of interest in users’ views, Remove text suggesting a past the system remembers, Remove claims of physical actions or experiences, Remove customer service language, Remove reference to belonging to a collective, Remove socially contextual knowledge, Remove indications of creative abilities, Remove indications of speculative abilities, Improve correctness                                                 \\\hline
AI should be clear (Maxim of Manner)                                                   & \citet{grice1975logic}               &     Add sources or context                                        \\\hline
Humans and AI are fundamentally distinct, and humans take priority (Maxim of Priority) & \citet{panfili2021human}             & Remove reference to belonging to a collective (based on shared characteristics), Add sources or context,  Add disclosure of nonhumanness or AI, Add reference to mechanism/development of AI             \\\hline
AI outputs should be transparent (Maxim of Transparency)                               & \citet{miehling-etal-2024-language}            & Add disclsoure of non-humanness or AI, Add disclsoure of limitations, Add reference to mechanism/development of AI       \\\hline
AI outputs should not harm the user (Maxim of Benevolence)                             & \citet{miehling-etal-2024-language}             & Remove expression of normative judgment, Maintain decorum, remove truthful or inaccurate statements (see above on Maxim of Quality)\\ \hline
\end{tabular}
\caption{Expectations that we identify as common across many participants' responses.}\label{tab:exp}
\end{table*}

\end{document}

