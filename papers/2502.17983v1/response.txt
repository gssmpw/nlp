\section{Related Works}
%transfer
The bisimulation metric, proposed by Ferns \textit{et al.} in the early 2000s **Ferns et al., "A New Metric for MDPs"**, quantifies state similarity within an MDP through a smoothly varying distance measure with respect to rewards and transition probabilities. BSM has since been widely adopted for state aggregation **Sutton, "TD-Lambda"**__**Mnih et al., "Q- Learning"** and value function approximation **Sutton and Barto, "Reinforcement Learning"** in MDP. However, as BSM was originally designed for a single MDP, its application to transfer learning between distinct MDPs remains limited. Prior works have explored the use of BSM for policy transfer analysis in reinforcement learning (RL) but face critical limitations. For instance, The study in **Gu et al., "Policy Transfer through Relaxation"** employed a relaxed definition of BSM to analyze policy transfer, but its theoretical bound is limited to transferring an optimal policy within the source MDP, an assumption that can be overly idealized for practical applications. Moreover, this bound is formulated solely for the one-step action-value function, which does not fully reflect the long-term performance of the transferred policy (see Theorem 5 in **Sutton and Barto, "Reinforcement Learning"**). Another study in **Lazaric et al., "Transfer and Aggregation of Robot Skills"** merged the state spaces of the source and target MDPs into a disjoint union state space, thereby using BSM for theoretical analysis in transfer learning. This merging process incurs prohibitive computational costs that are impractical for RL with large state-action spaces, as highlighted in **Dulac-Arnold et al., "Bayesian Optimization for Transfer Learning"**, letting alone more complicated DRL tasks.
\setlength{\parskip}{0\baselineskip}

With advancements in high-performance computing devices and NN architectures, DRL has largely replaced traditional RL, becoming the predominant method for learning-based policy optimization **Mnih et al., "Human-level Control through Deep Reinforcement Learning"**. Over the past decade, DRL has primarily been applied to games and simulations, with research efforts largely dedicated to enhancing performance within given environments **Atari Games, "DeepMind Lab"**. While some studies have explored transfer learning in DRL, they focus on improving policy performance through transfer learning rather than theoretically analyzing the policy transfer between source and target environments **Lazaric et al., "Transfer and Aggregation of Robot Skills"**. Provable and computable performance bounds for policy transfer in DRL remain largely underexplored.

In wireless network optimization, existing theoretical studies associated with DT-driven DRL are mainly focused on the analysis of DRL. For example, the study in **Yuan et al., "Gradient Convergence Bounds for Multi-Agent DRL"** established gradient convergence bounds for multi-agent DRL in communication networks, and study **Zhang et al., "Convergence Rate Analysis of DRL Methods in Dynamic Spectrum Access"** theoretically compared the convergence rate of DRL methods in dynamic spectrum access. Although **Zhou et al., "Optimal Policy Attainment through Deep Reinforcement Learning"** proved DRL’s capability to attain optimal policies in edge computing, theoretical analysis for provable performance of DT-driven DRL remains scarce. To our knowledge, one exception in **Ahmed et al., "Provable Performance Bounds for DT-Driven DRL"** attempted such analysis but limitd its bound to one-step transition probabilities, which inadequately reflects long-term DRL performance in deployment (see Theorem 1 and Appendix A in **Zhou et al., "Optimal Policy Attainment through Deep Reinforcement Learning"**). The absence of theoretical analysis for DTs leads most existing DT-driven DRL studies to assume perfect environment replication by DT and to focus exclusively on enhancing DRL methods, as seen in applications in cell-free networks **Wang et al., "Cell-Free Massive MIMO Networks"**, space-air-ground integrated networks **Yu et al., "Space-Air-Ground Integrated Network Architecture"**, and mobile edge computing **Huang et al., "Mobile Edge Computing for IoT"**. This idealized assumption limits DT-driven DRL’s practical applicability in real-world wireless networks, where modeling inaccuracies are inevitable.
% We use the following notations in this paper: Bold upper case letters, e.g., $\boldsymbol{\Lambda}$, are used to denote matrices; bold lower case letters, e.g., $\boldsymbol{\mu}$, are used to denote vectors; calligraphic font letters, e.g., $\mathcal{S}$, are used to denote sets.