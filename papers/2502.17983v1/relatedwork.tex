\section{Related Works}
%transfer
The bisimulation metric, proposed by Ferns \textit{et al.} in the early 2000s \cite{BSM}, quantifies state similarity within an MDP through a smoothly varying distance measure with respect to rewards and transition probabilities. BSM has since been widely adopted for state aggregation \cite{Li2006TowardsAU,LeLan2021} and value function approximation \cite{NEURIPS2021_256bf8e6} in MDP. However, as BSM was originally designed for a single MDP, its application to transfer learning between distinct MDPs remains limited. Prior works have explored the use of BSM for policy transfer analysis in reinforcement learning (RL) but face critical limitations. For instance, The study in \cite{castro2010using} employed a relaxed definition of BSM to analyze policy transfer, but its theoretical bound is limited to transferring an optimal policy within the source MDP, an assumption that can be overly idealized for practical applications. Moreover, this bound is formulated solely for the one-step action-value function, which does not fully reflect the long-term performance of the transferred policy (see Theorem 5 in \cite{castro2010using}). Another study in \cite{phillips2006knowledge} merged the state spaces of the source and target MDPs into a disjoint union state space, thereby using BSM for theoretical analysis in transfer learning. This merging process incurs prohibitive computational costs that are impractical for RL with large state-action spaces, as highlighted in \cite{taylor2009transfer}, letting alone more complicated DRL tasks.
\setlength{\parskip}{0\baselineskip}

With advancements in high-performance computing devices and NN architectures, DRL has largely replaced traditional RL, becoming the predominant method for learning-based policy optimization \cite{9904958}. Over the past decade, DRL has primarily been applied to games and simulations, with research efforts largely dedicated to enhancing performance within given environments \cite{silver2016mastering,berner2019dota}. While some studies have explored transfer learning in DRL, they focus on improving policy performance through transfer learning rather than theoretically analyzing the policy transfer between source and target environments \cite{10172347}. Provable and computable performance bounds for policy transfer in DRL remain largely underexplored.

In wireless network optimization, existing theoretical studies associated with DT-driven DRL are mainly focused on the analysis of DRL. For example, the study in \cite{10147312} established gradient convergence bounds for multi-agent DRL in communication networks, and study \cite{10623365} theoretically compared the convergence rate of DRL methods in dynamic spectrum access. Although \cite{10287987} proved DRL’s capability to attain optimal policies in edge computing, theoretical analysis for provable performance of DT-driven DRL remains scarce. To our knowledge, one exception in \cite{10522623} attempted such analysis but limitd its bound to one-step transition probabilities, which inadequately reflects long-term DRL performance in deployment (see Theorem 1 and Appendix A in \cite{10522623}). The absence of theoretical analysis for DTs leads most existing DT-driven DRL studies to assume perfect environment replication by DT and to focus exclusively on enhancing DRL methods, as seen in applications in cell-free networks \cite{10078846}, space-air-ground integrated networks \cite{10345669}, and mobile edge computing \cite{10235311}. This idealized assumption limits DT-driven DRL’s practical applicability in real-world wireless networks, where modeling inaccuracies are inevitable.
% We use the following notations in this paper: Bold upper case letters, e.g., $\boldsymbol{\Lambda}$, are used to denote matrices; bold lower case letters, e.g., $\boldsymbol{\mu}$, are used to denote vectors; calligraphic font letters, e.g., $\mathcal{S}$, are used to denote sets.