
%

 


%
\noindent
\textbf{Definition:} \gls{traffclass} involves categorizing network traffic based on various attributes such as protocols, services, and application types.

Table~\ref{tab:traffic_classification} summarizes the approaches employing \glspl{llm} to address \gls{traffclass}. 
It details the model type, traffic input, classification tasks, and training and evaluation datasets.

All the reviewed works leverage \glspl{llm}, such as \fmtTT{Transformer}, \fmtTT{BERT}, \fmtTT{GPT-2}, and \fmtTT{Mamba},
adapted from other domains (\eg \gls{nlp}) to the traffic context (column ``\textbf{Architecture}'') and optionally modify some architectural elements.
For instance,~\cite{guthula2023netfound} propose a hierarchical \fmtTT{Transformer} model to process data at multiple levels of granularity (\eg intra- and inter-bursts).
The adaptation involves transforming network traffic data into a text-based representation, followed by tokenization, allowing models to learn the complex characteristics of network traffic directly.
Notably, for the tokenization process, considered works leverage \emph{Datagram2Token}~\cite{lin2022, guthula2023netfound, wang2024netmamba, tao2024lambert}, \emph{SentencePiece}~\cite{meng2023netgpt, wang2024lens, qu2024trafficgpt}, \emph{WordPiece}~\cite{wang2024lens}, or \emph{Byte-Pair Encoding}~\cite{sarabi2023}.

As shown, most of the approaches consider the flow as the \gls{to}, 
while~\citet{sarabi2023} leverage 
the network service in terms of destination IP and port (column ``\textbf{TO}'').
The input data typically includes the header and payload of the network layer~\cite{meng2023netgpt, wang2024lens, qu2024trafficgpt, wang2024netmamba,li2024albert}, or only the network-layer~\cite{lin2022} and transport-layer payload~\cite{tao2024lambert}.
Conversely,~\citet{guthula2023netfound} leverage fields from both the transport and application layers along with metadata at both packet and burst levels, while~\cite{sarabi2023} focus on HTTP messages (column ``\textbf{Traffic Input Data}'').

Furthermore, all the reviewed works include a pre-training stage for the \gls{llm} architecture (column ``\textbf{NetPT}''), through a self-supervised learning approach typically involving two tasks:
\begin{enumerate*}[label=(\emph{\alph*})]
    \item \emph{Masked Burst Model} to capture the relationships between different datagram bytes within the same burst, and
    \item \emph{Same-origin Burst Prediction} to model the transmission relationships between preceding and subsequent bursts. 
\end{enumerate*}
Then, the resulting model is fine-tuned by adapting the pre-trained model to various traffic classification tasks and adjusting its parameters to optimize performance on the labeled data.
These processes are performed through the use of different datasets. Primarily, most of the works leverage ISCXVPN-2016~\cite{lin2022, meng2023netgpt,wang2024lens, qu2024trafficgpt, wang2024netmamba, tao2024lambert}, USTCTFC-2016~\cite{meng2023netgpt, qu2024trafficgpt,wang2024netmamba}, CIC-DoHBrw-2020~\cite{meng2023netgpt, wang2024lens, qu2024trafficgpt}, or ISCXTor-2016~\cite{wang2024lens, qu2024trafficgpt, wang2024netmamba}.
Additionally, also traffic from \gls{nids} (\eg CIC-IDS-2017, CIC-IDS-2018) and \gls{iot} devices (\eg CIC-IoT-2022, CIC-IoT-2023) is included to pre-train \glspl{llm}~\cite{lin2022, wang2024lens, qu2024trafficgpt, wang2024netmamba, tao2024lambert} (column ``\textbf{Pre-Training/Fine-Tuning Datasets}'').

The reviewed works address different \gls{traffclass} tasks, differing in granularity and classification types.
Specifically, most approaches focus on classifying the 
service~\cite{lin2022,guthula2023netfound,wang2024lens,wang2024netmamba,li2024albert,tao2024lambert} (ref. \textbf{Serv.}) or application~\cite{lin2022,meng2023netgpt,wang2024lens,qu2024trafficgpt,wang2024netmamba,tao2024lambert,li2024albert} (ref. \textbf{App.}) generating the traffic.
Other works focus on detecting VPN-/Tor-encapsulated traffic~\cite{meng2023netgpt,wang2024lens,li2024albert} 
(ref. \textbf{Encaps.}). 
To this end, these approaches primarily use datasets incorporating diversified traffic on multiple levels (\eg ISCXVPN-2016 and ISCXTor-2016) during both the pre-training and fine-tuning stages.
Conversely, only a few works focus on classifying DNS queries using the DoH protocol~\cite{wang2024lens} (ref. \textbf{Query Met.}).
Noteworthy, some studies introduce datasets specifically for evaluation purposes, which are not used during the pre-training and fine-tuning phases. 
%
Examples include Cross Platform~\cite{lin2022, wang2024lens, qu2024trafficgpt,li2024albert} and CSTNET-TLS1.3~\cite{lin2022}.
%






























