
In the ever-changing networking landscape, \gls{genai} emerges as a revolutionary tool that fosters innovative solutions and new perspectives.
In this section, we explore two principal aspects of the \gls{genai} tools that are used from a networking perspective, inspecting (\textit{i}) the leveraged datasets for pre-training or fine-tuning/evaluation phases (Sec.~\ref{subsec:datasets}) and (\textit{ii}) the available platforms (Sec.~\ref{subsec:platforms}) that facilitate the development of \gls{genai} solutions for \gls{nmm}. 


\subsection{Datasets}
\label{subsec:datasets}

Data play a critical role in applying \gls{genai} in networking since they directly affect the development of \gls{ai} models and their performance.
In this section, we discuss the adoption of various datasets in the \textbf{``Pre-Training''}, \textbf{``Fine-Tuning''}, and \textbf{``Evaluation''} phases of a \gls{genai} model life-cycle, when dealing with the \gls{nmm} use cases defined in Sec.~\ref{subsec:dowstream_tasks}. 
Table~\ref{tab:dataset-overview} presents such an overview by associating \emph{different colors} to the \gls{nmm} use cases, along with general information about the datasets, namely their \textbf{``Year''} (according to the collection time-span) and the \textbf{``Network Data''} each dataset provides.%
\footnote{As presented in Sec.~\ref{sec:genai_landscape}, different works could apply different pre-processing operations and extract different information from considered datasets to feed \gls{genai} models.}
%
It is worth noticing that the vast amount of unlabeled data collected (and now available) thanks to programmable devices and Software-Defined Networking (SDN) can be effectively utilized to refine the pre-training process of GenAI models, similarly as in other domains~\cite{zhu2022zoonet, yu2019network, d2019survey}.
%

%
Still,
%
we remark that many researchers do not release the data they used in their work, making it difficult to find publicly available datasets that are adequately representative of the specific context.
To contribute valuable resources to the community and foster reproducibility, we focus on datasets that are \textit{publicly available} or obtainable\textit{ upon-request} ($35$ out of $55$).
Notably, \gls{nda} is the \gls{nmm} use case with fewer datasets reported in Tab.~\ref{tab:dataset-overview}; 
this is because most of them (\ie~$7$ out of $10$) are closed source (\cf Sec.~\ref{sec:network_management}).

Looking at the temporal evolution, older datasets tend to be more general and not explicitly collected or formatted for \gls{genai} purposes. 
Conversely, more recent datasets~\cite{zhu2023loghub,mani2023enhancing,karim2023spec5g,maatouk2023teleqna} are designed to optimize the training or evaluation of \gls{genai} models. 
%
These datasets often include explicit prompts to better facilitate the generation of coherent and contextually relevant content.
%
This is particularly true for \gls{nla} and \gls{nda} use cases---being more affine to the \gls{nlp} domain---where tailored datasets are used. 
In contrast, the other \gls{nmm} use cases often repurpose traffic datasets originally collected for different objectives to train/evaluate \gls{genai} models.
For example, in~\cite{lin2022} numerous raw-traffic datasets collected in different domains~\cite{lashkari2017characterization,sharafaldin2018toward,van2020flowprint,lin2022} 
are fused to fine-tune the \fmtTT{ET-BERT} model and allow it to deal with multiple 
\gls{traffclass}/\gls{nid}-related tasks (\eg \gls{traffclass} on VPN/TLS/Tor, malware classification).

Other trends can be inferred when considering the phases of a \gls{genai} model life-cycle. 
On the one hand, the datasets mostly used for pre-training are typically more generic than those used in the other phases, since they aim to allow the model learning a foundational understanding of the context.
On the other hand, the datasets used for fine-tuning are vertical to the specific \gls{nmm} use case, as their goal is to specialize the pre-trained model.
Similarly, the evaluation phase requires data specific to the \gls{nmm} use case to be solved. 
Therefore, most fine-tuning datasets are also used for evaluation. 
%
Additionally, the evaluation datasets are usually meticulously labeled to facilitate accurate quantitative assessment of the related models in a supervised manner.
%
Unfortunately, regarding this latter aspect, different methods often rely on distinct datasets for performance evaluation. 
This inconsistency results in significant efforts for manual data processing and ultimately in unfair comparisons. 
To address this limitation, initial large-scale and unified benchmark datasets specifically designed for assessing \gls{genai} models are beginning to be proposed~\cite{qian2024netbench}. These datasets---particularly for validating foundation models in \gls{traffclass} and \gls{traffgen} use cases---aim to replace the current reliance on heterogeneous compositions of older datasets.

From Tab.~\ref{tab:dataset-overview}, we can also extract some differences in how data are used for different \gls{nmm} use cases.
Interestingly, all the works falling within the \gls{traffclass} (colored in pale green) train from scratch the \gls{genai} architecture on one or more datasets.
On the other hand, as can be noted from the ``pre-training'' column, all the works that perform \gls{nla} and \gls{nda}, along with the majority addressing intrusion detection, start with a pre-trained model and only fine-tune it.
While it is expected for \gls{nla} and \gls{nda} use cases, whose models are usually fed with textual inputs (and can thus leverage pre-trained \glspl{llm}), this does not apply to works performing intrusion detection.
Indeed, the latter commonly employ models pre-trained for affine tasks,
%
such as 
%
\gls{traffclass} (and vice versa). 
Accordingly, for \gls{traffclass}, some datasets~\cite{sharafaldin2018toward,neto2023ciciot2023} are used exclusively for pre-training purposes. 
Finally, datasets 
%
employed
%
in \gls{traffgen} are commonly leveraged for other tasks as well, except for \gls{nda}, which, as aforementioned, relies on ad-hoc \gls{genai}-tailored data.

%
Table~\ref{tab:dataset-overview} highlights the presence of a relatively large number of publicly available datasets.
However, it is crucial for the scientific community to focus on the quality of these datasets.
Many datasets fail to adequately represent real-world scenarios due to various factors, such as the use of synthetically generated data, small-scale testbeds, or heavy anonymization~\cite{jacobs2022ai}.
For instance, CIC-IDS2017, which is the most common dataset for the \gls{nid} task (cf.~Sec.~\ref{sec:intrusion_detection}), suffers from several issues that hinder its utility as a benchmark~\cite{engelen2021troubleshooting}.
Therefore, an important step for the scientific community would be to critically filter public datasets to select only those that provide a reliable representation of real-world scenarios.
%






















\subsection{Dedicated Platforms}
\label{subsec:platforms}


\gls{genai} solutions require huge amounts of resources to be effective---%
during both the pre-training and the fine-tuning phases.
Thus the deployment of these solutions can benefit from simplifications in the management of computing infrastructures, which translates into reduced maintenance costs.
This calls for
\emph{dedicated platforms} that offer integrated environments that streamline the end-to-end \gls{ai} workflow.
These platforms provide essential tools and services for data processing, model training, deployment, and monitoring.

%
In the context of \gls{genai}, platforms can be categorized based on two main aspects: (\textit{a}) whether they provide only first-party models (\ie~developed by the platform owner) or also include third-party models, and
(\textit{b}) whether they offer open-source models in addition to closed-source ones, which are typically accessible exclusively through \glspl{ui}, \glspl{api}, or \glspl{cli}.
%

Here we survey some of the most well-known and utilized platforms according to this categorization.

To the best of our knowledge, no platform that offers only \textbf{first-party} models, provides them as \textbf{open source}.
\emph{OpenAI GPT} is a notable case that offers access to a vast amount of \textbf{first-party} 
pre-trained natural language processing models---such as \fmtTT{GPT} and its advancements---which can be fine-tuned using computational resources provided by OpenAI. 
However, the models are \textbf{closed}, and they remain available and can be operated only on OpenAI servers. 
The functionality of OpenAI GPT can be accessed through \gls{ui}/\gls{api} via free/paid plans or by signing a partnership.

On the other hand, most platforms offer both \textbf{first-party and third-party} models, including \textbf{open-source and closed} alternatives. 
In this case, the user leverages \gls{cli}, \gls{ui}, or interactive notebooks for performing pre-training, fine-tuning, and operation of 
models.
Notably, the business models of these platforms allow users to \emph{deploy and execute even open-source models only on their own cloud} 
with different degrees of customization.
In more detail, \emph{Amazon Bedrock} is a cloud computing platform that provides access to several foundation models from various companies (\eg AI$21$ Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon). 
Similarly, \emph{Microsoft Azure AI} offers a wide range of services for \gls{genai}, including pre-built and customizable \glspl{api} and models. 
The users can train models on the Azure cloud with their data and access a variety of pre-trained models (from OpenAI, Hugging Face, Stability AI, Meta, etc.).
\emph{Google Cloud AI Platform (Vertex AI)} provides services for the development and fine-tuning of pre-trained \glspl{llm} from Google (\ie~\fmtTT{Gemini}, \fmtTT{Gemma}) and other open models (\eg \fmtTT{LLaMa}, \fmtTT{Claude}). Users can customize hardware resources (\eg GPUs, storage, and virtual machines) according to their needs.

Other platforms provide only \textbf{open-source} models, both \textbf{first- and third-party}. 
\emph{Hugging Face}, one of the most popular and active platforms in the \gls{ai} and particularly \gls{genai} community, belongs to this category. 
Hugging Face hosts a vast amount of open-source pre-trained models and also provides remote resources for model training via \gls{api} on a subscription basis. 
\emph{Nvidia NIM} has a more specific focus on provider hardware support and exposes an \gls{api} to access to numerous open pre-trained \glspl{llm} hosted on its infrastructure. These models are optimized for Nvidia architectures and accelerated through the Nvidia software stack.
Differently than these platforms that allow the users to \emph{download, customize, and operate the models outside of them}, \emph{IBM Watson} provides pre-trained models (\eg \fmtTT{Granite}, \fmtTT{LLaMa}, \fmtTT{Mixtral}) for \gls{genai} deployed on its cloud, which can be accessed only via \glspl{api} and notebooks.
Based on a slightly different philosophy, \emph{Cloudflare AI} is based on a network of serverless GPUs specifically designed for deploying and running \gls{ai} models from anywhere. 
Cloudflare AI offers numerous open models for text generation and text-to-image tasks (\eg \fmtTT{LLaMa}, \fmtTT{Gemma}, \fmtTT{Zephyr}, \fmtTT{Stable Diffusion}) whose interaction can be carried out via \gls{api} and \gls{cli}.

Among the frameworks categorized in Tab.~\ref{table:genai_architectures} (\cf Sec.~\ref{sec:genai_model_overview}), all the base architectures are available on (and downloadable from) Hugging Face, except for the (closed) GPT-based models, which are available on OpenAI, and \fmtTT{BERT}, which is accessible through the Google Cloud AI Platform.






