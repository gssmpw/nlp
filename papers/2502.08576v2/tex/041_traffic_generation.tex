


\noindent
\textbf{Definition:} \gls{traffgen} entails creating synthetic data that accurately replicate real-world network traffic patterns and behaviors.

Table~\ref{tab:traffic_generation} summarizes works addressing \gls{traffgen} with the \gls{genai} model, published since $2021$.
Notably, some studies are not explicitly focused on generating synthetic network traffic.
%
In fact, they also tackle tasks related to \emph{traffic understanding}, such as \gls{traffclass} and \gls{nid}~\cite{meng2023netgpt, wang2024lens, qu2024trafficgpt} by opportunistically fine-tuning \gls{genai} models.
%

Most of the reviewed works employ \glspl{llm} (\eg \fmtTT{GPTs} and \fmtTT{T5})~\cite{bikmukhamedov2021, meng2023netgpt, Kholgh2023PACGPT, wang2024lens, qu2024trafficgpt} while other leverage Diffusion Models (\eg \fmtTT{Stable Diffusion})~\cite{sivaroopan2023netdiffus, jiang2024netdiffusion, zhang2024netdiff}.
%
The sole exception is the work in~\cite{chu2024mamba} that uses \fmtTT{Mamba}.
%
The column ``\textbf{GenAI Model - Architecture}'' outlines the architecture used by each study.

%
As for the
\gls{to}, the considered works segment the traffic into packets~\cite{meng2023netgpt,qu2024trafficgpt}, unidirectional~\cite{meng2023netgpt,Kholgh2023PACGPT,wang2024lens,qu2024trafficgpt, chu2024mamba, wolf2024} or bidirectional~\cite{bikmukhamedov2021, jiang2024netdiffusion, zhang2024netdiff} flows, or even consider the whole network traffic trace~\cite{sivaroopan2023netdiffus}.
As input data for \gls{genai} models (column ``\textbf{Traffic Input - Data}''), almost all works employ raw packet bytes~\cite{meng2023netgpt,jiang2024netdiffusion,wang2024lens,qu2024trafficgpt, chu2024mamba}, optionally performing IP masking operations~\cite{jiang2024netdiffusion,wang2024lens,qu2024trafficgpt}. 
%
Other studies leverage the sequence of packet header fields such as \glspl{ps} and \glspl{iat}~\cite{bikmukhamedov2021}, or incorporate \glspl{dir}, \glspl{iat}, and optionally \glspl{ps}~\cite{sivaroopan2023netdiffus}.
%
Conversely, \citet{sivaroopan2023netdiffus} and \citet{wolf2024} also employ aggregated metrics (\eg forward/backward volume and packet count, and flow duration), while \citet{Kholgh2023PACGPT} exploit packet summary extracted with the Linux's \texttt{tcpdump} tool. 
Finally, \citet{zhang2024netdiff} leverage the sequences of traffic statistics related to, for instance, the forward/backward number of packets, \glspl{iat}, and traffic volumes.
%

As described in Sec.~\ref{subsec:dowstream_tasks},
\glspl{llm} and \fmtTT{Diffusion Models} are not inherently designed for handling network traffic. 
Therefore, to employ these architectures, the input data need to be formatted either in a text-based~\cite{bikmukhamedov2021, meng2023netgpt, Kholgh2023PACGPT, wang2024lens, qu2024trafficgpt, chu2024mamba, zhang2024netdiff,wolf2024} or in an image-like~\cite{sivaroopan2023netdiffus, jiang2024netdiffusion} representation (see column ``\textbf{Traffic Input - Format}'').
%
For datagram-to-text conversion, these approaches use complex \emph{tokenization mechanisms},
to preserve the complex hierarchical structure (\ie spatial and temporal properties of packets within a flow) of real network traffic.
Additionally, \citet{qu2024trafficgpt} encodes time information into tokens, enabling the model to generate timestamp intervals for a comprehensive representation of PCAP file data.
On the other hand, \fmtTT{Diffusion Models} need an \emph{encoding-decoding strategy} to transform network traffic data into an image format and subsequently convert them back to the original traffic format. 
%
To perform this operation, \citet{sivaroopan2023netdiffus} exploit the \gls{gasf}~\cite{wang2015imaging} method while \citet{jiang2024netdiffusion} use nPrint~\cite{holland2021new}.


%
As described in Sec.~\ref{subsec:recent_ai}, the training procedure of \gls{genai} models relies on two different strategies (see column \textbf{``GenAI Train''}): 
($i$) the naive \emph{Monolithic Training (MT)} or ($ii$) \emph{Pre-Training \& Fine-Tuning (PT\&FT)}.
Most of the studies in Tab.~\ref{tab:traffic_generation} adopt the MT approach~\cite{bikmukhamedov2021, sivaroopan2023netdiffus, qu2024trafficgpt, chu2024mamba, zhang2024netdiff, li2024lightweight, wolf2024}, training the models with heterogeneous traffic data from networking datasets including ISCXVPN$2016$, USTC-TFC$2016$, and CIRA-CIC-DoHBrw$2020$. 
For models exploiting PT\&FT, while the fine-tuning is always performed on networking datasets, the column \textbf{``NetPT''} 
indicates whether the pre-training phase includes a networking-specific corpus. 
In fact, \citet{Kholgh2023PACGPT} and \citet{jiang2024netdiffusion} adapt pre-trained models not originally trained on networking data to networking-specific tasks.
Specifically, \citet{Kholgh2023PACGPT} leverage OpenAI's \fmtTT{GPT-3}, pre-trained on a mix of publicly available and licensed Internet text, while \citet{jiang2024netdiffusion} use Stability AI's \fmtTT{Stable Diffusion 1.5}, trained on the LAION dataset.
Differently, \citet{meng2023netgpt} and \citet{wang2024lens} perform the pre-training phase on networking datasets and successive fine-tuning on specific downstream datasets.
For more details on such datasets, please refer to Tab.~\ref{tab:dataset-overview}.

\gls{traffgen} approaches can also be categorized based on their output (column ``\textbf{Generated Object}''). 
Some studies focus on generating specific packet fields, such as IP addresses, ports, and packet sizes~\cite{meng2023netgpt, wang2024lens, qu2024trafficgpt, wolf2024} or aggregated traffic features, such as traffic volume, flow duration, and number of forward/backward packets~\cite{zhang2024netdiff, wolf2024}. 
It should be noted that these methods can also be applied iteratively to generate entire flows, akin to constructing sentences from individual words in the \gls{nlp} domain.
Differently, other approaches natively generate sequences of packet header fields (\eg \gls{ps}, \gls{iat}, and \gls{dir})~\cite{bikmukhamedov2021, sivaroopan2023netdiffus}, aggregated traffic features~\cite{sivaroopan2023netdiffus}, or raw traffic bytes of entire biflows~\cite{jiang2024netdiffusion, chu2024mamba}.
Interestingly, \citet{Kholgh2023PACGPT} provide Python code for interacting with the Scapy library%
\footnote{\url{https://scapy.net/}} 
to generate traffic that matches the input desiderata. This solution
is more akin to traffic replay than generation.


Finally, to evaluate the fidelity and realism of generated data, the considered works leverage two different kinds of metrics (column \textbf{``Evaluation Metrics''}): ($i$) divergence/fidelity metrics, ($ii$) \gls{ml}-based classification accuracy of related downstream tasks, or ($iii$) success rate.
In the former case, the most common metrics are the \gls{jsd}~\cite{meng2023netgpt, jiang2024netdiffusion, wang2024lens, qu2024trafficgpt, chu2024mamba, zhang2024netdiff, wolf2024} and the \gls{tvd}~\cite{jiang2024netdiffusion, wang2024lens, chu2024mamba, zhang2024netdiff} that quantify the similarity and the maximum difference between two distributions, respectively.
Both metrics range from $0$ (identical distributions) to $1$ (completely different distributions) and are commonly evaluated together. 
Hence, lower values signify synthetic traffic more similar to the real one.
Conversely, in the case of \gls{ml}-based evaluation, the synthetic traffic is used during the training or evaluation phases of different \gls{ml} models targeted for various downstream tasks.
In the case of a traffic classification task, the variation in accuracy~\cite{sivaroopan2023netdiffus, jiang2024netdiffusion, chu2024mamba, zhang2024netdiff, li2024lightweight}, F1-score~\cite{li2024lightweight, wolf2024}, Precision, or Recall~\cite{li2024lightweight} is taken as a measure of the quality of the generated data.
\citet{zhang2024netdiff} also evaluate the generated data through traffic prediction assessed via $R^2$.
In addition, \citet{wolf2024} analyze the \gls{fpr} of a model (viz. discriminator) specifically trained to distinguish between real and synthetic traffic, with higher \gls{fpr} values indicating the difficulty in distinguishing between the two types of traffic, thereby reflecting their similarity.
%
Lastly, \citet{Kholgh2023PACGPT} evaluate the synthetic traffic generated by their Python code based on the \gls{sr}, quantifying the proportion of successfully sent packets out of the total generated ones.

































