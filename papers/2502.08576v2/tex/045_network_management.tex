
\noindent
\textbf{Definition:}
\gls{nda} refers to the process of administering, controlling, and optimizing network operations to ensure efficient functionality, performance, and security.
\gls{nda} is pivotal due to the heterogeneous nature of networks, which often consist of diverse hardware and software components from different vendors. 
This diversity introduces complexity, making it challenging to navigate through various network configurations, protocols, and standards effectively~\cite{wang2023network}.


Table~\ref{tab:net_mng} provides a summary of the papers dealing with \gls{nda}, emphasizing the related key aspects.
The publication dates of the works highlight the recent interest of the scientific community in this topic ($2023-24$).
This interest has primarily converged on two downstream tasks, as seen from the \textbf{``Downstream Task Description''} column,
which can be identified as follows:
\begin{itemize}
    \item Using \gls{genai} as virtual assistants to query standard documents in the networking/telecommunication domain, thereby providing support to users (\viz~\emph{Network Digital Assistance for Documentation}).
    \item Employing \gls{genai} as assistants for the network operative phase, \eg~handling network topologies, setup of device configurations, and infrastructure management (\viz~\emph{Network Digital Assistance for Operation}).
\end{itemize}








%
Detailing, $8$ works~\cite{ahmed2024linguistic, piovesan2024telecom, karapantelakis2024using, roychowdhury2024unlocking, shen2024large, soman2023observations, duclos2024utilizing, erak2024leveraging} focus on the \emph{design of a virtual assistant specific to the telecommunication domain}.
%
The authors of \cite{roychowdhury2024unlocking, piovesan2024telecom} highlight the difficulty in analyzing and extracting information from standard documents in the telecommunication domain, as it involves identifying sources from multiple documents and related references.
Thus, the authors investigate whether \glspl{llm} can be used as digital assistants for Q\&A on standard documents.
\citet{ahmed2024linguistic} extend the functionalities proposed in~\cite{roychowdhury2024unlocking} to enrich the digital assistant's functionalities.
In particular, they also include tasks related to text classification and summarization, as well as Q\&A.
%
Unlike the previous ones, in ~\cite{karapantelakis2024using, erak2024leveraging}, a digital assistant specifically designed for \glspl{gpp} standards is introduced. 
%
Unlike previous research focused on Q\&A for standardized documents, the assistant proposed in~\cite{soman2023observations} specifically targets user-support tasks.
These include finding information about products and services, initial assistance with installation and configuration, and operational tasks like troubleshooting and performance monitoring.
\citet{duclos2024utilizing} develop an \gls{llm}-based assistant designed to translate \glspl{rfc} into a format compatible with \gls{cpsa}.





%
The works in~\cite{mani2023enhancing, ghasemirahni2024deploying, duclos2024utilizing, wang2023making, mondal2023llms, ayed2024hermes} address the \emph{design of a network operation assistant.}
%
Contrary to the previous studies, these papers exhibit greater heterogeneity due to the inherent task diversity associated with the operational phase.
\citet{mani2023enhancing} tackle the complexities of network topology and communication graph analysis by exploiting \glspl{llm}. 
They demonstrate how these models can be used to generate task-specific code for graph manipulation, thereby enabling more intuitive network management through natural language interactions.
In~\cite{ghasemirahni2024deploying}, \glspl{llm} are employed to enhance the scalability of 
network functions
as traffic volume increases.
The authors introduce a system that utilizes \glspl{llm} to perform code analysis and extract crucial information about software behavior, semantics, and system-level performance. 
The extracted information is then used to optimize the infrastructure, deployment configuration, and execution pipeline.
\citet{shen2024large} propose to integrate \glspl{llm} into an \gls{ai}-enabled network with two distinct tasks: 
($i$) serving as a user interface to intercept and understand user requests; and 
($ii$) automating the training of \gls{ai} nodes in the network---%
for instance, iteratively finding the best learning rate scheduler.
In~\cite{wang2023making} and~\cite{mondal2023llms}, \glspl{llm} are proposed as tools to facilitate the creation of network configurations from natural language descriptions.
Both studies highlight the limitations of \glspl{llm} in this specific task, highlighting a high number of errors in the generated configurations. 
Consequently, both papers incorporate a verification module to validate and correct the output of the model.
%
\citet{ayed2024hermes} proposes a framework based on \glspl{llm} to generate logical blocks related to a specific user intent (e.g.,~the deployment of a new base station in a network). Each logical block is accompanied by the corresponding code necessary for its implementation.
%



The \textbf{``Input Type''} column displays the specific data fed to \gls{genai} models.
Predominantly, these inputs are textual prompts where a query is submitted to the model.
%
The majority of works employing such inputs fall under the category of downstream tasks for telecommunication documentation and support functions~\cite{soman2023observations, roychowdhury2024unlocking, shen2024large, duclos2024utilizing, ahmed2024linguistic, piovesan2024telecom, karapantelakis2024using, erak2024leveraging}.
%
Concerning the second downstream task---\ie \gls{genai} as assistant for the network operative phase---inputs vary depending on the specific operation. 
Specifically, 
\citet{mani2023enhancing} combine textual descriptions with network topology graphs, while \citet{ghasemirahni2024deploying} integrate code snippets with textual prompts for analysis purposes.
In~\cite{wang2023making, mondal2023llms} a description of the configurations in natural language is used as input.
%
Similarly, \citet{ayed2024hermes} use a description of the available data together with a network modeling task.
%

%
Looking at the \textbf{``Evaluation/Fine-Tuning Datasets''} column, the datasets utilized in the literature vary according to the specific downstream task. 
TeleQnA is the most frequently used dataset \cite{roychowdhury2024unlocking, ahmed2021ecu, piovesan2024telecom, erak2024leveraging}. 
%
This dataset is designed to evaluate the knowledge of \glspl{llm} within the Telecom domain, featuring multiple-choice questions categorized into various categories. 
TaleQnAD \cite{karapantelakis2024using} is similar to TeleQnA but specialized in \gls{gpp} standards.
On the other hand, NeMoEval \cite{mani2023enhancing} is used as benchmark for \glspl{llm} for two different applications:
traffic analysis using communication graphs and
network lifecycle management (\eg capacity planning, network topology design, deployment planning, and diagnostic operations).

The \textbf{``GenAI Model''} column highlights that most studies,
%
except for a few \cite{roychowdhury2024unlocking, duclos2024utilizing, wang2023making, mondal2023llms},
%
conduct comparative analyses among various currently-available generative models.
%
The literature considers several \glspl{llm}, such as those from OpenAI (\eg \fmtTT{GPT-4o}, \fmtTT{GPT-4o mini} \fmtTT{GPT-4}, \fmtTT{GPT-3.5}, \fmtTT{GPT-3}), Google (\eg \fmtTT{Bard}, \fmtTT{Gemini}), Meta (\eg \fmtTT{CodeLLaMA}, \fmtTT{LLaMA 2.0}, \fmtTT{LLaMA 3.1}), and Microsoft (\eg \fmtTT{Phi-2}).
%
Regarding the use of open models, \citet{mani2023enhancing} test open \glspl{llm} (\ie~\fmtTT{StarCoder} and \fmtTT{InCoder}) but omitted their results due to inconsistency.
Delving deeper, the majority of works~\cite{soman2023observations, mani2023enhancing, shen2024large, ahmed2024linguistic, piovesan2024telecom, ghasemirahni2024deploying, wang2023making, mondal2023llms, ayed2024hermes} do not fine-tune the models but use them off-the-shelf, as indicated by the \textbf{``Fine-Tuned''} column. 
%
On the other hand, the remaining works \cite{roychowdhury2024unlocking, duclos2024utilizing, karapantelakis2024using, erak2024leveraging} refine the models using domain-specific datasets. 
%
The \textbf{``Released''} column shows that none of the works that perform fine-tuning also release the fine-tuned models;
only the original models are available in those cases.
%
To reduce the computational burden of \glspl{llm}, the authors of~\cite{piovesan2024telecom, erak2024leveraging} investigate the use of \glspl{slm} leveraging \fmtTT{Phi-2}.
In both works, such a model is compared with larger ones (\ie \fmtTT{GPT-3.5} and \fmtTT{GPT-4}, \fmtTT{GPT-4o}, \fmtTT{GPT-4o mini}).
In \cite{piovesan2024telecom}, the authors propose equipping the model with \gls{rag} to incorporate authoritative knowledge external to the model's initial training data. 
The combination of \fmtTT{Phi-2} and \gls{rag} achieves results comparable to those of \fmtTT{GPT-3.5}.
Similarly, in \cite{erak2024leveraging}, a fine-tuned \fmtTT{Phi-2} in tandem with \gls{rag} and \emph{SelfExtend}~\cite{jin2024llm} (used to extend the model's the context window
during inference) surpasses the performance achieved by the larger \fmtTT{GPT-4o}.
%
\citet{wang2023making} and \citet{mondal2023llms} introduce a verification module to address the shortcomings of \fmtTT{GPT-4}'s device configuration generation.
Their findings indicate that the \gls{llm} output frequently contains errors,
necessitating a verification component to ensure accuracy and provide corrective feedback.






















    

    


































































































