
\noindent
\textbf{Definition:}
\gls{nla}
tasks involve parsing logs to extract meaningful insights and information critical for maintaining and optimizing network operations.

Table~\ref{tab:log_analysis} provides a comprehensive overview of various research proposals employing \gls{genai} for log analysis within the networking domain during $2021$--$24$. 

As the input of \gls{genai} models (column ``\textbf{Input Type}''), considered works 
employ logs related to events and activities of operating system and applications in honeypots~\cite{setianto2021gpt},
cloud environments~\cite{ott2021robust,mudgal2023assessment}, DNS~\cite{tian2024dom},
web servers~\cite{balasubramanian2024cygent, karlsen2024large,sun2023design}, 
Kubernetes clusters~\cite{sun2023design},
%
supercomputers~\cite{pan2023raglog,mudgal2023assessment, almodovar2024logfit}, 
%
and
Unix shell~\cite{boffa2024logprecis}.
Differently,~\citet{meyuhas2024} leverage network traffic log, which captures data from the flow of packets exchanged between \gls{iot} devices, while~\cite{voros2023web} employ URLs collected on firewalls and endpoints.
Additional details on the dataset used for testing and optionally fine-tuning the \gls{genai} model are provided in the ``\textbf{Evaluation/Fine-Tuning Dataset}'' column,
%
with the majority of reviewed works leveraging Loghub datasets~\cite{ott2021robust,pan2023raglog,qi2023loggpt,jiang2023lilac,mudgal2023assessment,han2023loggpt,karlsen2024large, almodovar2024logfit}.
%
For details about datasets, please refer to Tab.~\ref{tab:dataset-overview}.

Furthermore, some works take as input also prompts to interact with \gls{genai} models~\cite{setianto2021gpt, qi2023loggpt, mudgal2023assessment, balasubramanian2024cygent}.
Listing~\ref{lst:prompt_example}, reported in~\cite{mudgal2023assessment}, provides an example of a possible prompt interaction
for error and root cause identification within system logs.

\vspace{5pt}
\begin{lstlisting}[style=customlog, caption={Example of prompt.}, label={lst:prompt_example}]
Summarize the errors and warnings from these log messages and identify the root cause.
[Sun Dec 04 04:52:49 2005] [notice] workerEnv.init() ok 
 /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:52:49 2005] [notice] workerEnv.init() ok 
 /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:52:52 2005] [error] mod_jk child workerEnv in error state 7
\end{lstlisting}

All the considered works employ transformer-based models from different categories (see ``\textbf{\gls{genai} Architecture}'' column and \cf  Sec.~\ref{sec:background}).
As a \emph{decoder-only} architecture, various \fmtTT{GPT}-based models are employed, such as 
\fmtTT{GPT-2}~\cite{setianto2021gpt,ott2021robust,ji2023log,han2023loggpt,karlsen2024large}, \fmtTT{GPT-3}~\cite{boffa2024logprecis}, \fmtTT{GPT-3.5}~\cite{pan2023raglog} or \fmtTT{GPT-3.5 turbo}~\cite{jiang2023lilac,mudgal2023assessment,balasubramanian2024cygent}, \fmtTT{GPT-Neo}~\cite{karlsen2024large}.
In contrast, as \emph{encoder-only} architectures, \fmtTT{BERT}~\cite{ott2021robust,boffa2024logprecis,meyuhas2024,voros2023web,tian2024dom} is used, as well as its derived versions, such as
%
\fmtTT{RoBERTa}~\cite{karlsen2024large, meyuhas2024, almodovar2024logfit}, 
%
\fmtTT{DistilRoBERTa}~\cite{karlsen2024large}, \fmtTT{BERTiny}~\cite{voros2023web},
\fmtTT{CodeBERT}~\cite{boffa2024logprecis}, and \fmtTT{CodeBERTa}~\cite{boffa2024logprecis}.
Additionally,~\citet{balasubramanian2024cygent},~\citet{ott2021robust}, and~\cite{voros2023web} leverage \emph{full encoder-decoder} models, \ie \fmtTT{T5}, \fmtTT{CodeT5}, and \fmtTT{XLNet} which also include an auto-regressive module.
%
Furthermore,~\citet{almodovar2024logfit} employ Longformer, which overcomes BERT limitations in handling sequences exceeding $512$ tokens.
%

%
Most studies leverage publicly-available \gls{genai} models~\cite{ott2021robust,pan2023raglog,qi2023loggpt,jiang2023lilac,ji2023log,mudgal2023assessment,sun2023design, meyuhas2024}, while others fine-tune them~\cite{setianto2021gpt, han2023loggpt, voros2023web, boffa2024logprecis, balasubramanian2024cygent,karlsen2024large, tian2024dom, almodovar2024logfit}
(see ``\textbf{Fine-Tuned}'' column).
%
Only some studies release the updated version of the model (see the ``\textbf{Released}'' column).
Notably,~\citet{han2023loggpt} leverage reinforcement learning strategy to fine-tune and adapt the \gls{genai} model for \gls{lad},
that is, identifying unusual (\viz anomalous) patterns or behavior in system logs that deviate from the normal ones. 



The ``\textbf{Downstream Task}'' column details the specific task each study addresses, while the ``\textbf{\gls{genai} Goal}'' column describes the functions that \gls{genai} models perform to achieve these tasks.

On the one hand, \citet{setianto2021gpt} and \citet{jiang2023lilac} perform accurate real-time log-parsing.
Although both use \gls{genai} models to perform log parsing---%
which involves extracting structured information from unstructured log data---%
the former~\cite{setianto2021gpt} employs a Q\&A interaction with \gls{genai} model, while the latter~\cite{jiang2023lilac} leverages in-context learning and adaptive parsing cache.
In-context learning optimizes the creation of diverse prompts, while adaptive parsing cache stores and updates parsed log templates to avoid redundant queries and ensure accuracy.

%
On the other hand, a significant number of works focus on \gls{lad}~\cite{qi2023loggpt, pan2023raglog,ji2023log, karlsen2024large, sun2023design, han2023loggpt, boffa2024logprecis, mudgal2023assessment, ott2021robust, balasubramanian2024cygent, almodovar2024logfit}.
%
Interestingly, four of these do not use \gls{genai} models directly for \gls{lad} but as preprocessing components.
Specifically, \citet{mudgal2023assessment} and \citet{boffa2024logprecis} perform log parsing for \gls{lad}.
The former work uses prompt-based interactions with ChatGPT using pre-defined prompts, while the latter work creates an attack fingerprint and assigns attacker tactics within the \emph{MITRE ATT\&CK tactics}%
\footnote{\url{https://attack.mitre.org/tactics/ics/}} 
to each session portion to reveal the attackerâ€™s goals.
\citet{ott2021robust} focus on vectorization, converting log data into numerical vectors and performing \gls{lad} using nearest template matching to manage incomplete prior knowledge of log templates.
\citet{balasubramanian2024cygent} perform summarization, condensing log files into concise, human-readable formats for \gls{lad}.
Among the works explicitly using \gls{genai} models for \gls{lad},
\citet{qi2023loggpt}, similarly to~\cite{mudgal2023assessment},
use prompt-based interactions with ChatGPT,
leveraging prompt-construction strategies.
Similarly,~\citet{pan2023raglog} employ a Q\&A strategy with log entries and best-matched retrieved entries from a database to determine whether a queried log entry is normal or not.
Furthermore,~\citet{ji2023log} encode normal patterns and define an alarm strategy to filter out false positives based on statistical log data characteristics.
%
Then,~\citet{almodovar2024logfit} leverage a self-supervised training strategy on normal log data to learn its linguistic and sequential patterns, thereby distinguishing it from malicious logs.
%

Lastly,
\citet{meyuhas2024} use \gls{genai} models to analyze network traffic and automatically classify \gls{iot} devices by vendor and function, offering insights into the traffic they generate.
\citet{tian2024dom} examine DNS logs to discover malicious DNS entry---%
using \gls{genai} models for log reconstruction---%
while in~\cite{voros2023web} web content filtering based on URLs is accomplished with \gls{genai} models directly tackling URL classification.
























