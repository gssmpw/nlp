
In this section, we first provide a formal description of \gls{genai} (Sec.~\ref{subsec:genai_nutshell}). Then, we trace the evolution of \gls{genai} models over time, from classic methods (Sec.~\ref{subsec:classic_ai}), proposed since $2013$, to the most recent advancements of the present day (Sec.~\ref{subsec:recent_ai}).
We end the section by describing the various strategies used to optimize \gls{genai} models to deal with typical \gls{nmm} use cases (Sec.~\ref{subsec:optimization}).
\subsection{GenAI in a Nutshell}
\label{subsec:genai_nutshell}
\gls{ai} models can
be classified into \emph{discriminative} and \emph{generative} models, according to the learning objective. The former makes predictions on unseen data by training on labeled data and thus can be used for various inference tasks. 
In contrast, generative models focus on synthesizing realistic content. 

From a formal viewpoint, given a set of training samples $\bm{x}_1,\ldots, \bm{x}_N$ associated to an unknown data distribution $p_d(\bm{x})$, a \gls{genai} technique learns a model to sample new (synthetic) data according to $p_{mod}(\bm{x}) \approx p_d(\bm{x})$.
This can be accomplished by either two- or one-step approaches.
In the former case, known as \gls{ede}, the model first learns an explicit distribution $p_{mod}(\bm{x}) \approx p_d(\bm{x})$ (in a tractable or approximate fashion), which is then used to sample new data.
In the latter case, known as \gls{ide}, the \gls{genai} technique directly learns a model that can sample from $p_{mod}(\bm{x}) \approx p_d(\bm{x})$ without explicitly defining it.

The design and use of \gls{genai} have a long history in \gls{nmm}: relevant methods include well-known Markov chains (tractable \gls{ede})~\cite{aceto2021characterization}, but also \glspl{vae} (approximate \gls{ede})~\cite{aceto2024synthetic}, \glspl{gan} (\gls{ide})~\cite{hui2022knowledge} and normalizing-flows (tractable \gls{ede})~\cite{gudovskiy2022cflow}.
Conversely, recent applications of generative models are \emph{\glspl{llm}}---%
based on \fmtTT{Transformer} (and variants) or 
selective \gls{ssm}---%
and \fmtTT{Diffusion Models}, which have represented a breakthrough in the realism and complexity of the content generated.
Transformer-based models enable parallelization and scalability, enhancing processing speed and contextual understanding through their self-attention mechanisms.
Moreover, Diffusion Models offer several advantages over traditional generative models, such as \glspl{vae} and \glspl{gan}, including better mode coverage and stability during training.




\subsection{Classic GenAI Methods}
\label{subsec:classic_ai}
%
Figure~\ref{fig: genai_timeline} reports the timeline of the development of \gls{genai}, starting from \glspl{vae} (proposed in $2013$ by~\citet{kingma2013auto} at the University of Amsterdam) until the latest models released by OpenAI in the second half of $2024$, namely \fmtTT{GPT-4o} and its successive variants and evolutions (\ie the lightweight \fmtTT{GPT-4o~mini}, and the reasoning models \fmtTT{o1-preview} and \fmtTT{o1-mini}).
%
We recall that \emph{this survey focuses only on works that take advantage of the most recent advances in \gls{genai}, specifically from the Transformer architecture onward}, which Google proposed in $2017$~\cite{vaswani2017attention}.
One motivation is that these solutions offer improved performance \wrt older solutions like \glspl{vae}, \glspl{gan}, and normalizing-flows, \eg \gls{nice}~\cite{dinh2014nice}.

Moreover, this choice is justified by the impressive groundbreaking impact of 
these more sophisticated \gls{genai} architectures
across various fields,
%    
significantly improving 
%
generative tasks such as text generation, image synthesis, and multimodal applications.
However, we also report the \emph{classic deep generative models}---\viz~\gls{nice}, \glspl{vae}, and \glspl{gan} with their variants and hybridizations---for context and completeness.
Regarding the latter \gls{genai} architectures---%
which fall outside our scope---%
we refer the reader to these prominent surveys for a deeper background and a detailed overview of their usage for networking-related use cases: \cite{adeleke2022network, sai2024empowering, halvorsen2024applying, karapantelakis2024generative}.
\emph{The modern era of deep \gls{genai}} started with \glspl{vae} at the end of $2013$, \glspl{gan} at mid-$2014$, and \gls{nice} at the end of $2014$.
These models were the first deep neural networks capable of learning generative models for complex data, such as images.
In detail, \glspl{vae} introduced a structured and probabilistic approach to generative modeling with continuous latent spaces and improved training stability~\cite{kingma2013auto},
while \glspl{gan} presented a powerful adversarial framework that excels at generating high-quality and realistic data~\cite{goodfellow2014generative}.
Then, \gls{nice} was the first model to implement normalizing flows using neural networks, leveraging them as invertible functions to transform data from a complex distribution to a simpler one~\cite{dinh2014nice}.

Over time, diverse improvements to \glspl{vae}, \glspl{gan}, and \gls{nice} have been proposed.
Notably, the Conditional GAN (CGAN, $2014$) enables controlled data generation by incorporating additional information into the generative process. This allows for a more targeted and context-specific output~\cite{mirza2014conditional}.
Additionally, the Deep Convolutional GAN (DCGAN, $2015$) enhances the quality of generated images and improves the stability of the training process~\cite{radford2015unsupervised}.
Moreover, hybrid architectures like VAE-GAN (2015) were proposed, integrating the structured latent space of \glspl{vae} into the adversarial training of \glspl{gan}~\cite{larsen2016autoencoding}.
Lastly, the main evolution of \gls{nice} has been the Real Non-Volume Preserving (Real NVP) model---%
proposed in mid-$2016$---%
that incorporates scale transformations, allowing the model to expand or contract regions of data rather than simply rotating or translating them, leading to more accurate and expressive generated contents~\cite{dinh2016density}.

\glsreset{fed}
\glsreset{eo}
\glsreset{do}
\glsreset{sdp}
\glsreset{ssm}


\subsection{Recent Advancements on GenAI}
\label{subsec:recent_ai}


%
Focusing on the most recent advancements in \gls{genai},
namely from \fmtTT{Transformer} onward,
we can identify five categories of architecture divided according to the nature of the underlying layers.
We identify three variants of the \fmtTT{Transformer} architecture, namely the
\begin{enumerate*}[label=(\emph{\roman*})]
    \item \emph{full encoder-decoder}, the 
    \item \emph{encoder-only}, and the
    \item \emph{decoder-only} architectures. 
    Additionally, the other two categories are based on
    \item \emph{diffusion processes} and 
    \item \emph{state-space representations}, respectively.
\end{enumerate*}
From a general perspective, the development of \gls{genai} models has shifted in the last years toward a \emph{foundational} nature definition~\cite{bommasani2021opportunities}.
Consequently, specific training strategies are commonly employed.

\vspace{5pt}
\noindent
\textbf{Training Strategies for GenAI Models}:
Three training strategies can be adopted for \gls{genai} models.
In the case of naive ($a$) \emph{Monolithic Training}, the model is trained from scratch using a dataset tailored to the specific downstream task.
More commonly, the training follows two sequential stages:
\begin{enumerate*}[label=(\emph{\roman*})]
    \item \emph{Pre-Training}, where the \gls{genai} model is pre-trained on a large corpus of data consisting of text, images, or other input modalities, in a self-supervised or semi-supervised manner. 
    For instance, during this stage, a text-fed (resp. image-fed) model is instructed to predict masked words and the sequence of sentences (resp. to denoise or reconstruct the original picture).
    \item \emph{Fine-Tuning}, where the \gls{genai} model is then specialized for specific tasks (possibly by topping/modifying the architecture with task-specific layers). Specifically, the training parameters (or a portion of them) are jointly fine-tuned (exploiting the broader transfer learning concept), tailoring the model for the considered downstream task.
\end{enumerate*}
Consequently, the training of a model can involve either ($b$) \emph{Pre-Training \& Fine-Tuning}, \ie the model is first pre-trained on a large corpus of data (\eg a networking corpus) and then fine-tuned with a dataset related to the downstream task, or ($c$) \emph{Fine-Tuning Only}, \ie an already pre-trained model is exclusively fine-tuned for the specific downstream task.


%


\vspace{5pt}
\noindent
\textbf{\gls{fed}}:
This category includes the \gls{genai} architectures that reflect the typical structure of the \fmtTT{Transformer} model~\cite{vaswani2017attention}.
The \fmtTT{Transformer} represents a revolutionary approach to solving sequence processing tasks. This architecture is entirely based on the self-attention mechanism rather than using recurrences---\eg \gls{rnn}---or convolutions---\eg \gls{cnn}.
This improvement enables the \fmtTT{Transformer} to efficiently parallelize computations and significantly reduce training times while achieving state-of-the-art results.
In fact, traditional sequence processing models---%
such as \glspl{rnn} and their variants like \gls{lstm} and \gls{gru}---%
perform computations around symbol positions in input and output sequences.
This characteristic results in limited parallelization because it is inherently sequential, 
%
becoming a significant bottleneck for longer sequences.
%
To cope with this drawback, the \fmtTT{Transformer} leverages self-attention mechanisms to model dependencies between different positions in a sequence, regardless of their distance.

%
In general, the \fmtTT{Transformer} architecture consists of an encoder-decoder structure. The general workflow of a \gls{fed} is depicted in Figure~\ref{fig:full_enc_dec}.
%
The \emph{encoder} comprises a stack of identical layers, each with two sub-layers, namely a multi-head self-attention and a position-wise fully connected feed-forward network.
The \emph{decoder} is similar to the encoder but includes an additional sub-layer that performs masked multi-head attention over the encoder's output. It also modifies the self-attention sub-layer to prevent positions from attending to subsequent positions, ensuring the auto-regressive property.
Both encoder and decoder are designed with residual connections for each sub-layer followed by layer normalization.

Going into detail, the \fmtTT{Transformer} uses multi-head attention to allow the model
%
to learn information from different representation subspaces jointly.
In fact, instead of having a single attention function, 
%
the model linearly projects queries, keys, and values multiple times with different learned projections and performs the attention function in parallel. This process enhances the model's ability to focus on different parts of the input sequence.
Moreover, since the \fmtTT{Transformer} lacks the inherent sequential order provided by recurrence, it introduces positional encodings to inject information on the position of tokens in the sequence. These encodings are added to the input embeddings at the bottom of the encoder and decoder stacks, enabling the model to understand the sequence order.

%
The \fmtTT{Transformer} is usually leveraged for sequence-to-sequence tasks, 
%
\ie when the input and the output are both sequences. Examples of applications are translation (from one language to another), summarization (condensation of documents), and text generation (based on given prompts).
Notable architectures that fall into this category are \fmtTT{XLNet}, \fmtTT{T5}, \fmtTT{Gemini}, \fmtTT{Mistral}, and \fmtTT{Zephyr}.



%
\vspace{5pt}
\noindent
\textbf{\gls{eo}}: Compared to \gls{fed}, \gls{eo} models only leverage the encoder unit of the \gls{fed} architecture (as depicted in Figure~\ref{fig:encoder_only}). 
\gls{eo} is designed to model bidirectional relationships between tokens in an input sequence, generating either a vector representation for each token or a single vector summarizing the entire sentence. This architecture is well-suited for tasks focused on text understanding and analysis rather than generation.

Among the \gls{eo} models, \fmtTT{BERT}~\cite{devlin2018bert}--- Bidirectional Encoder Representations from Transformers---is the most representative and has served as the basis for many subsequent advances of this type of architecture. 
Developed by researchers at Google AI Language, \fmtTT{BERT} consists of multiple layers of bidirectional \fmtTT{Transformer} encoders.
\fmtTT{BERT} has been designed with a key innovation: it pre-trains deep bidirectional representations from the unlabeled text by joint conditioning on both left and right contexts in all layers. 
%
This procedure allows \fmtTT{BERT} to capture richer linguistic information, and it contrasts with models like OpenAI GPTs and ELMo (an \gls{lstm}-based architecture), which are unidirectional and do not fully leverage the bidirectional context. 
In detail, the bidirectional training of \fmtTT{BERT} is achieved through a \gls{mlm} objective. The \gls{mlm} randomly masks some tokens in the input sequence and predicts them using the context provided by the remaining tokens on both sides. This method allows \fmtTT{BERT} to capture the context from both directions.
To further enhance its understanding of context and sentence relationships, \fmtTT{BERT} uses a next-sentence prediction task during pre-training. This involves predicting whether a given sentence \fmtTT{B} follows sentence \fmtTT{A} in the original text, allowing the model to learn how sentences relate to each other.

The ability of \fmtTT{BERT} to understand the context from both directions and the effectiveness of its pre-training tasks enables it to achieve superior performance across a wide range of \gls{nlp} tasks. \fmtTT{BERT} has been designed for language understanding, 
\ie
encoding the input text in relation to its context for various subsequent tasks. Examples of applications are text classification (\eg spam detection), question answering, and text similarity (\eg semantic search).
Other notable architectures in this category are variants of \fmtTT{BERT}, such as \fmtTT{BERTiny}, \fmtTT{RoBERTa}, \fmtTT{DistilRoBERTa}, and \fmtTT{ViT} (Vision Transformer).


%
\vspace{5pt}
\noindent
\textbf{\gls{do}}:
These models exploit only the decoder component of the \gls{fed} architecture, as shown in Figure~\ref{fig:decoder_only}. 
\gls{do} models are designed for autoregressive text generation, predicting the next token based on previous tokens, thereby producing the output one token at a time.

Among them, Generative Pre-trained Transformers (\fmtTT{GPTs})~\cite{radford2018improving} are the most prominent, spearheading advancements in the field of \gls{nlp} starting with their first variant named \fmtTT{GPT-1}.
%
Subsequent improvements of \fmtTT{GPTs}, including \fmtTT{GPT-2}, \fmtTT{GPT-3}, \fmtTT{GPT-3.5}, \fmtTT{GPT-3.5 turbo}, \fmtTT{GPT-4}, and \fmtTT{GPT-4o}, have dramatically increased the model size and the scale of pre-training data and have included multi-modality (text and images) from \fmtTT{GPT-4} onward.
\fmtTT{GPT-4}, with its estimated $1.7$ trillion parameters%
\footnote{\url{https://the-decoder.com/gpt-4-has-a-trillion-parameters/}}%
, exemplifies the trend towards larger models and has achieved state-of-the-art results across a wide range of benchmarks without task-specific fine-tuning.

\fmtTT{GPTs} are commonly leveraged for autoregressive text generation, \ie generating text tokens conditioned on the previous token.
Examples of applications are text generation,
language modeling (\eg autocompletion), and conversational \gls{ai} or chatbots (\eg ChatGPT and Copilot).
Notable architectures in this category are \fmtTT{Falcon}, \fmtTT{LLaMA}, \fmtTT{Phi}, \fmtTT{Gemma} and improvements of \fmtTT{GPT-1}, from \fmtTT{GPT-2} to \fmtTT{GPT-4o}.

\vspace{5pt}
\noindent
\textbf{\gls{sdp}}:
\citet{ho2020denoising} proposed 
a class of generative models,
named Diffusion (probabilistic) Models,
that describes the process by which particles, information, or other entities spread through a medium over time.
%
In recent years, the \fmtTT{Diffusion~Model} (Figure~\ref{fig:diffusion}) has found successful applications in computer vision, as well as in audio, bioinformatics, and agent-based systems. 

The core idea involves defining a \emph{forward diffusion} process (Figure~\ref{fig:forward_diffusion}) that gradually adds noise to the data, transforming them into a simpler distribution, typically Gaussian noise. The corresponding \emph{reverse diffusion} process (Figure~\ref{fig:reverse_diffusion}) is then learned to map the noisy data back to the original data distribution.
%
The elegance of \fmtTT{Diffusion~Models} lies in their theoretical foundation, which leverages concepts from Markov chains (when diffusion is performed in discrete time) or stochastic differential equations (when diffusion is performed in continuous time).
This foundation allows for a rigorous treatment of the model's behavior and facilitates efficient training and sampling algorithms (through sophisticated sampling acceleration techniques). 
The resulting models, such as the \gls{ddpm} and score-based generative models, have demonstrated remarkable capabilities in generating high-quality synthetic data.
Because of the explicit definition of the forward/reverse diffusion process and the objective used to learn them (\ie a generalized evidence lower-bound~\cite{kingma2024understanding}), these models fall within the approximate \gls{ede} category.
In summary, \fmtTT{Diffusion~Models} have been used for high-quality data generation through iterative denoising. 
%
Examples of applications include visual and signal data processing tasks, such as image generation, audio synthesis, and video generation, contrasting with previous categories that primarily involve language and textual data-generation tasks. 
A notable model falling into this category is \fmtTT{Stable~Diffusion}.
%

\vspace{5pt}
\noindent
\textbf{Selective and Structured \glspl{ssm}}: %
Proposed by~\citet{gu2023mamba},
\fmtTT{Mamba} represents a significant advancement in sequence modeling, introducing a new class of selective and structured \glspl{ssm} designed to overcome the limitations of existing architectures like \fmtTT{Transformers} in handling very long input sequences. 
%
As depicted in Figure~\ref{fig:mamba}, \fmtTT{Mamba} integrates selective and structured \glspl{ssm} into a streamlined neural network architecture that avoids traditional attention mechanisms, achieving \emph{fast inference and linear scaling with sequence length}.
%

In fact, while \fmtTT{Transformers} have become the backbone of many foundation models due to their effective self-attention mechanism, they suffer from \emph{quadratic scaling} \wrt sequence length, limiting their efficiency on long sequences. Subquadratic-time architectures, including linear attention and structured-only \glspl{ssm}, have attempted to address these inefficiencies but have failed to perform in critical modalities such as language.

\fmtTT{Mamba}'s first core innovation lies in making (a part of) the parameters of \glspl{ssm} dependent on the input. This mechanism allows the model to \emph{selectively} propagate or forget information based on the current token, \ie the model can focus on relevant information or discard irrelevant or outdated information as needed.
This selective mechanism enables \fmtTT{Mamba} to handle discrete modalities effectively, providing a significant advantage over previous \glspl{ssm}.
Secondly, \fmtTT{Mamba} reduces the number of trainable parameters by assuming a \emph{structured} form for the \gls{ssm} matrices defining the information propagation.
Thirdly, to maintain efficiency, \fmtTT{Mamba} employs a ($i$) hardware-aware algorithm that computes the model recurrently without materializing the expanded state in the \gls{gpu} memory (leveraging fast memory hierarchies) and ($ii$) a parallel scan algorithm to accelerate the recursive computation of relevant quantities. This approach ensures linear scaling in sequence length and high throughput on modern hardware.



In summary, \fmtTT{Mamba} integrates selective and structured \glspl{ssm} into a simplified neural network architecture that omits attention and \gls{mlp} blocks. This streamlined design, inspired by previous \gls{ssm} architectures, offers fast training and inference with high performance in various data modalities, including language, audio, and genomics.
\fmtTT{Mamba} is designed to process and model sequences efficiently, making it ideal for applications that require handling long sequences and achieving high computational efficiency.

\subsection{Optimization Strategies for GenAI}
\label{subsec:optimization}
In this section,
we discuss the optimization strategies that have been proposed for \gls{genai} solutions,
focusing on those that have been used in \gls{nmm} use cases.
Broadly, optimization strategies can be divided into two categories~\cite{kim2024memory}, involving methods for ($i$) \emph{parameter-efficient fine-tuning} and ($ii$) \emph{post-training quantization}.



\vspace{5pt}
\noindent
\textbf{\gls{peft}}:
These methods enhance the adaptation of pre-trained \gls{genai} models to the target downstream task.
The primary objective is to minimize the computational resources required for fine-tuning while preserving inference performance.

Among recent advances proposed to optimize the fine-tuning step of \gls{genai} solutions,
the state-of-the-art approach named \gls{lora}~\cite{hu2021lora} has been recently leveraged for traffic generation purposes~\cite{jiang2024netdiffusion}.
\gls{lora} capitalizes on the intuition that changes in model weights during adaptation have a low ``intrinsic rank''.
In other words, the idea is that adapting a model to a new task does not require very complex changes, which can be efficiently represented using fewer adjustments.
In detail, \gls{lora} optimizes the fine-tuning by using rank decomposition matrices, specifically targeting the change in dense layers during training while keeping the main pre-trained weights frozen.
Thus, this method allows for efficient task adaptation by replacing some model components---%
\ie parts of the weight matrices---%
with small low-rank matrices.
This substitution reduces the need to recalculate gradients and memorize optimizer states.
The way in which \gls{lora} is designed ensures that there is no extra delay introduced during inference. Moreover, \gls{lora} is compatible with other optimization techniques, such as post-training quantization~\cite{ding2023parameter}.



\vspace{5pt}
\noindent
\textbf{\gls{ptq}}:
These methods aim to reduce the computational complexity and memory footprint of \gls{genai} models by casting the model parameters into lower precision formats \emph{after training}. Quantization facilitates faster inference and more efficient deployment on various hardware.

One of the most complete methods for enforcing \gls{ptq} that has been recently proposed is
named \gls{gguf}~\cite{gguf-website}.
\gls{gguf} is a generalized file format that has recently been adopted by~\cite{karapantelakis2024using} to enforce post-training quantization for network digital assistance purposes, namely for networking standards question answering.
\gls{gguf} has been proposed to reduce---%
in large \glspl{llm}---%
the precision of the weights and activations of the model by converting real numbers to integers, \eg $32$-bit floating-point to $8$-bit.
\gls{gguf} has been devised with two key features in mind: \emph{quantization-aware kernel optimization} and \emph{extensibility}.
On the one hand,
\gls{gguf} does not simply apply quantization to the model weights but also provides kernel optimization functionalities that consider the quantization process. This characteristic is fundamental in avoiding an inference performance decrease due to blind quantization.
On the other hand, \gls{gguf} has been designed to overcome the limits of its predecessor GGML%
\footnote{GGML is an \gls{ml} library created by Georgi Gerganov, which is why it is named ``GGML''. In addition to offering low-level \gls{ml} primitives, such as tensor types, GGML also defines a binary format for distributing \glspl{llm}.}%
, which lacks mechanisms to incorporate additional model information or add new features.
Therefore, \gls{gguf} allows the integration of new features into the file format while ensuring compatibility with models deployed in older \gls{gguf} formats, thus preserving backward compatibility for newer versions.

In general, \gls{gguf} provides several key functionalities, including single file deployment, improved model loading and saving speeds, and intuitive design and detailed information storage that facilitate extensibility. Together, these functionalities enable a more efficient and user-friendly experience in handling \glspl{llm}.






