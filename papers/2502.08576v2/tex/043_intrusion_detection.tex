

\noindent
\textbf{Definition:} \gls{nid}
is an umbrella term that covers network security tasks that aim to identify and recognize malicious behavior from network traffic and collectively contribute to the design of so-called \gls{nids}.

Accordingly, in this section, we review works that propose \gls{genai} solutions ending up under the \gls{nid}
use-case umbrella. 
Such works are described in Tab.~\ref{tab:rw-id} alongside four main views, namely
($i$) \textbf{``Downstream Task''} details,
($ii$) the leveraged \textbf{``GenAI Model''} characteristics,
($iii$) the \textbf{``Traffic Input''} fed, and
($iv$) \textbf{``Pre-Training''}  and \textbf{``Fine-Tuning Datasets''}.


First, we categorize the literature based on the \textbf{``Downstream Task''}.
Indeed \gls{nids} can be taxonomized based on their modeling objective, namely 
\gls{md} or \gls{nad}.
\gls{md} relies on recognizing both normal (benign) and anomalous (malicious) behaviors in a supervised fashion, 
whereas 
\gls{nad} focuses only on normal network traffic and identifies anomalies as deviations from legitimate behavior.
Within \gls{md}, we also distinguish between \gls{bmd} and \gls{mmd}.
The main difference is that \gls{mmd} can identify specific attack types (and thus enable attack-tailored countermeasures), whereas \gls{bmd} simply distinguishes between legitimate and malicious traffic.
%
By looking at the reviewed literature, only the work by \citet{nam2021intrusion} performs \gls{nad}, while the remaining studies are divided between \gls{bmd}~\cite{li2022extreme, seyyar2022attack, ghourabi2022security, lai2023, ullah2023tnn, wang2023robust, meng2023netgpt, manocchio2024flowtransformer, wang2024lens} and \gls{mmd}~\cite{yu2021securing, wu2022, lin2022, ali2023, guthula2023netfound, wang2024lens, wang2024netmamba, wang2024lightweight, melicias2024gpt, ferrag2024},
with~\cite{ho2022network} performing both.
%












Secondly, regarding the \textbf{``GenAI Model''}, 
the most common choice falls in basic \fmtTT{Transformers} or \fmtTT{ViT} (\ie full encoder-decoder category)~\cite{li2022extreme, wu2022, ullah2023tnn, wang2023robust, lin2022, guthula2023netfound, ho2022network},
then \fmtTT{BERT} or variants follow (\ie encoder-only category)~\cite{yu2021securing, seyyar2022attack, ghourabi2022security, lai2023, ferrag2024, wang2024lightweight, manocchio2024flowtransformer},
and minor attention is posed on \fmtTT{GPT}-based solutions (\ie decoder-only category)~\cite{nam2021intrusion, ali2023, melicias2024gpt, meng2023netgpt}.
Other models, such as Google's \fmtTT{T5}~\cite{wang2024lens} and \fmtTT{Mamba}~\cite{wang2024netmamba} are also explored.
In detail, 
\fmtTT{Transformers} and \fmtTT{ViT} are commonly used ``as-is'' in many studies~\cite{li2022extreme, wu2022, ullah2023tnn}. 
\citet{wang2023robust} enhance the transformer encoder's training phase by incorporating a contrastive loss term, and the encoder output is subsequently fed into a transformer decoder for sample reconstruction.
Similarly, \fmtTT{BERT} is often employed with minimal modifications~\cite{yu2021securing, seyyar2022attack, lai2023, ferrag2024}. 
\citet{ghourabi2022security} extends \fmtTT{BERT} by proposing a framework that utilizes it alongside \emph{LightGBM} (Light Gradient Boosting Machine, an open-source distributed gradient boosting framework).
\citet{manocchio2024flowtransformer} introduce \fmtTT{FlowTransformer}, which integrates \fmtTT{BERT} and \fmtTT{GPT-3}. Their architecture includes a shallow encoder and decoder, \fmtTT{GPT} as a deep decoder, and \fmtTT{BERT} as a deep encoder, with a \gls{mlp} for classification.
Regarding studies based on \fmtTT{GPT}, 
\citet{nam2021intrusion} combine two \fmtTT{GPT-1} networks in a bi-directional manner, employing a forward and a backward \fmtTT{GPT} followed by a dense layer and softmax.
\citet{ali2023} use \fmtTT{GPT-3.5 turbo} for explainability purposes alongside various \gls{xai} techniques.
Lastly, \citet{melicias2024gpt} leverage \fmtTT{GPT-1} for data augmentation. They demonstrate that \fmtTT{GPT}-based methods can generate invalid data, leading to performance degradation in \gls{mmd}.

Concerning the particular data format fed to models (see \textbf{``Traffic Input''} column),
the majority of studies~\cite{ho2022network, wu2022, ghourabi2022security, lai2023, ali2023, ullah2023tnn, wang2023robust, manocchio2024flowtransformer, wang2024lightweight, melicias2024gpt} use pre-processed features, such as flow-based statistics typically derived from pre-processed datasets.
Four works~\cite{nam2021intrusion, yu2021securing, seyyar2022attack} utilize inputs specific to their particular domain, such as
HTTP requests
(for anomalous HTTP requests detection), \gls{can} ID sequences (for \gls{can} intrusion detection) or \gls{apt} attack sequence (for \gls{iot} \gls{apt} attack detection).
The remaining studies use fields extracted from packet headers~\cite{guthula2023netfound, ferrag2024}, payload bytes~\cite{li2022extreme, lin2022, wang2024netmamba}, or both~\cite{meng2023netgpt, wang2024lens}.

One third of the works%
~\cite{lin2022, ullah2023tnn, meng2023netgpt, guthula2023netfound, wang2024lens, wang2024netmamba, wang2024lightweight} pre-trains the models.
The \textbf{``Pre-Training Dataset''} may belong to various domains 
such as: VPN traffic (\ie~ISCXVPN-2016~\cite{lin2022, meng2023netgpt, wang2024lens, wang2024netmamba}), Tor traffic (\ie~ISCXTor-2016~\cite{wang2024lens, wang2024netmamba}), and malicious traffic in \gls{iot} and non-\gls{iot} contexts (\ie~CIC-IDS2017~\cite{wang2024lightweight, lin2022}, USTC-TFC2016~\cite{meng2023netgpt, wang2024lens, wang2024netmamba}, 
%    
CIRA-CIC-DoHBrw-2020~\cite{wang2024lens}, 
%
and CIC-IoT-2022~\cite{wang2024lens, wang2024netmamba}).

Differently from the pre-training phase, for the downstream task, the datasets (see column \textbf{``Fine-Tuning Dataset''}) are exclusively related to the cybersecurity domain.
The most common dataset is CIC-IDS2017~\cite{li2022extreme, ho2022network, wu2022, wang2023robust, manocchio2024flowtransformer, guthula2023netfound, wang2024lightweight}.
Other frequently used datasets include those for \gls{iot}-domain attacks (\eg Edge-IIoTset~\cite{ghourabi2022security, ferrag2024, melicias2024gpt}, MQTT-IoT-IDS2020~\cite{ullah2023tnn, manocchio2024flowtransformer}, TON\_IoT~\cite{ghourabi2022security, manocchio2024flowtransformer, wang2024lightweight}, and CIC-IoT-2022~\cite{wang2024netmamba}). 
Some studies rely on outdated datasets (\ie~KDD'99~\cite{ali2023, wang2023robust} and its improved version NSL-KDD~\cite{manocchio2024flowtransformer, lai2023}) collected over $20$ years ago, which no longer accurately reflect contemporary network traffic.



















    



