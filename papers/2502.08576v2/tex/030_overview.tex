

In this section, we present a model-centric overview of the works categorized in the previous sections.
This section is divided into two parts: ($i$) the first part reports details about the base \gls{genai} architecture leveraged by each work (Sec.~\ref{subsec:genai_models_overview}), while ($ii$) the second part focuses on modifications to \gls{genai} architecture the authors performed in their proposals (Sec.~\ref{subsec:adhoc_genai_solutions}).

\subsection{Overview of GenAI Models\label{subsec:genai_models_overview}
}

This section provides a broad view of the use of (foundation) \gls{genai} models in \gls{nmm}.
Accordingly, Tab.~\ref{table:genai_architectures} is centered around each \textbf{``GenAI Architecture''} and its application in the considered \textbf{``\gls{nmm} Use Cases''}.
%
First, we want to emphasize the persistent trend towards increasingly complex \gls{genai} models. 
However, such complexity is not fully justified when considering the effectiveness of simpler \gls{ml}/\gls{dl} models in accomplishing the \gls{nmm} tasks discussed in the present survey~\cite{yao2024survey}.
%

\glsreset{fed}
\glsreset{eo}
\glsreset{do}
\glsreset{sdp}
\glsreset{ssm}

The \gls{genai} architectures leveraged for network tasks defined in Sec.~\ref{subsec:dowstream_tasks} can be broadly divided---%
according to the nature of the underlying layers---%
into $5$ categories (\cf~Sec.~\ref{sec:background}), namely:
\begin{enumerate*}[label=(\textit{\roman*})]
  \item \gls{fed},
  commonly leveraged for tasks where both the input and output are sequences, such as sequence-to-sequence tasks;
  \item \gls{eo},
  intended for language comprehension, specifically for interpreting and encoding input text for various subsequent applications;
  \item \gls{do},
  frequently used for autoregressive text generation, meaning it generates text tokens based on the preceding token;
  \item \gls{sdp},
  applied for producing high-quality data by means of successive denoising;
  \item Selective and Structured \gls{ssm},
 representing a sophisticated means created for effectively handling and modeling long sequential data.
\end{enumerate*}




%
\gls{genai} architectures
like \fmtTT{Transformer}, \fmtTT{T5}, \fmtTT{Gemini}, \fmtTT{Mistral}, \fmtTT{Zephyr}, and \fmtTT{XLNet}%
\footnote{\fmtTT{XLNet}, proposed by Google and Carnegie Mellon University in~\cite{yang2019xlnet}, combines a Transformer-based model with an AutoRegressive component.}
belong to the \gls{fed}
category.
%
The 
\gls{eo} category encompasses \fmtTT{BERT} along with its enhancements (\eg \fmtTT{DistilRoBERTa}) and variations (\eg \fmtTT{CodeBERT}), as well as \fmtTT{ViT}. The 
\gls{do}
category features \fmtTT{GPT}, \fmtTT{Falcon}, \fmtTT{LLaMA}, and \fmtTT{Phi}. The 
\gls{sdp}
category includes \fmtTT{Diffusion} and \fmtTT{Stable Diffusion} architectures, while the 
\gls{ssm}
category consists of the sole \fmtTT{Mamba} architecture.





Table~\ref{table:genai_architectures} clearly shows that certain categories are more frequently utilized than others (column \textbf{``Cat.''}).
The \gls{do} category accounts for the majority of works, followed by the \gls{fed} and \gls{eo} categories.
When examining the models employed (\textbf{``Name''}), \fmtTT{Transformer} and \fmtTT{BERT} (by Google), the \fmtTT{GPT} family (by OpenAI), and \fmtTT{LLaMA} (by Meta) are the most widely used.
Equally important, the reviewed works use most of the architectures designed for language-processing tasks. Accordingly, a large portion of them use plain-text-arranged information as model input (\textbf{``Input''}).
Occasionally, this input is formatted as code, especially when the model is fine-tuned for code-generation tasks, like \fmtTT{CodeBERT}, \fmtTT{CodeLLaMA}, or \fmtTT{CodeT5}.
Only a small fraction of the works~\cite{sivaroopan2023netdiffus,jiang2024netdiffusion,ho2022network} considers input traffic shaped as images.








A different perspective on \gls{genai} models utilized for \gls{nmm} use cases focuses on the research organization (\textbf{``Res.~Org.''}) that introduced them and the associated licensing framework (\textbf{``Lic.''}).
From this viewpoint, \emph{three key points} emerge:
\begin{itemize}
    \item \textit{Non-academic organizations dominate the development of these architectures}. 
    Google has been prolific, especially in the \gls{fed} and \gls{eo} categories, OpenAI has primarily developed \gls{do} solutions, and Meta and Microsoft have contributed to both the \gls{eo} and \gls{do} \gls{genai} categories.

    
    \item \textit{The open-source paradigm is also embraced by non-academic entities}. 
    Google, except for its private \fmtTT{Bard} and \fmtTT{Gemini} models, and Microsoft have released many models as open-source. In contrast, OpenAI's recent products, from \fmtTT{GPT-3} onward, are closed-source. Meta, except for the open-source \fmtTT{RoBERTa} model, and StabilityAI employ non-commercial licenses.    




    
    \item  \textit{Both academic and non-academic entities have pioneered each category of models}. These progenitor architectures are reported with a  ``$\ddag$'' in Tab.~\ref{table:genai_architectures}.
    Google developed both \fmtTT{Transformer} and \fmtTT{BERT} models (the first in collaboration with the University of Toronto). OpenAI designed \fmtTT{GPT-1}. \fmtTT{Diffusion} and \fmtTT{Mamba} models are completely proposed by academic entities, namely, the former from Stanford and California universities and the latter from Carnegie Mellon and Princeton universities.




    
\end{itemize}




Concerning the specific use cases of the models in \gls{nmm} (\textbf{``NMM Use Case''} column),
\fmtTT{BERT} and \fmtTT{GPT}-like models address a wide range of applications, with \fmtTT{BERT} predominantly used for \gls{nid} and \fmtTT{GPT}-like models for \gls{nda}. The \fmtTT{Transformer} model is applied to \gls{traffgen}, \gls{traffclass}, and \gls{nid}.
The \fmtTT{LLaMA} family has been exclusively used for \gls{nda}.
Notably, the \gls{traffgen} use case is addressed by various \gls{genai} model categories,
including \gls{fed} models like \fmtTT{ViT},
\gls{do} models such as  \fmtTT{GPT-2} and \fmtTT{GPT-3}, 
and diffusion-based models.

\subsection{
Ad-Hoc GenAI Solutions\label{subsec:adhoc_genai_solutions}
}



Table~\ref{tab:openess} outlines the naming conventions used by state-of-the-art solutions.
It specifies the base architecture and whether it is used ``as-is'' or fine-tuned, details the modifications made (if any), lists the components included in the pipeline before and after the \gls{genai} model (if any), and indicates the availability of repositories for each proposed framework.





On the one hand, the majority of the works reported in Tab.~\ref{tab:openess} ($21$ out of $28$) utilize the base architecture in its vanilla version (see \textbf{``V''} column), typically adding pre-\gls{genai} components like feature selectors/extractors and traffic-to-image modules, or post-\gls{genai} components such as output refinement modules, exemplified by the ControlNet used in \fmtTT{NetDiffusion}.
Among these, $8$ works simply leverage the \gls{genai} model without modifications or fine-tuning (see \textbf{``F''} column) but adding at least one pre-\gls{genai} or post-\gls{genai} component.
Conversely, only $8$ works propose modifications to the \gls{genai} model and also perform fine-tuning for the targeted use cases.


For the \gls{traffgen} use case, notable examples include \fmtTT{TrafficGPT}~\cite{qu2024trafficgpt} and \fmtTT{LENS}~\cite{wang2024lens}, both of which modify the tokenizer to handle network traffic. \fmtTT{LENS} also changes the pre-training and fine-tuning phases.
In the \gls{traffclass} and \gls{nid} use cases,  \fmtTT{LENS}~\cite{wang2024lens} is again notable, along with \fmtTT{netFound}~\cite{guthula2023netfound}, which uses a Hierarchical Attention Transformer architecture, and \fmtTT{ET-BERT}~\cite{lin2022}, which redefines the tokenizer and the pre-training/fine-tuning procedures.
Notably, none of the works addressing \gls{nla} and \gls{nda} modify and fine-tune the base \gls{genai} architecture, as these use cases align closely with the core philosophy of \gls{genai}, \eg document summarization and question-answering.


Finally, we also note whether a paper provides access to a related public repository 
(\textbf{``Code Repo''} column). 
\textit{Only $6$ works make their framework code available}~\cite{lin2022, han2023loggpt, wang2024netmamba, liu2024large, manocchio2024flowtransformer, jiang2024netdiffusion}.
This lack of shared code significantly hampers reproducibility and hinders further development and verification by the research community.