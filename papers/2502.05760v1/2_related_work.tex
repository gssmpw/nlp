\section{Related Work}
\label{related_work}





\subsection{Replay in Continual Learning}
The fundamental challenge in developing a CL system is addressing catastrophic forgetting, and one of the widely studied methods to overcome forgetting is {\em replay}. In general, replay techniques complement the training data for each new task with older data that are representative of the tasks observed by the model so far. These techniques can further be classified into one of three subcategories -- exact replay, generative replay, and compressed replay. 

% A key challenge in developing continual learning (CL) systems is mitigating catastrophic forgetting (CF). Replay, a widely recognized approach, addresses this by enriching new task data with representative samples from previous tasks. Broadly, replay techniques fall into three categories: exact replay, using actual previous data; generative replay, creating synthetic data based on past tasks; and compressed replay, incorporating a condensed version of past data. Each method offers a different strategy for preserving and utilizing prior knowledge.

\paragraphX{Exact Replay.} 
Selecting and utilizing replay samples in exact replay involves determining a memory budget, denoted as $\mathcal{M}$. Finding the optimal way to choose $\mathcal{M}$ remains an open research question~\cite{aljundi2019gradient,chaudhry2019tiny}. Exact-replay techniques are designed to choose replay samples from previously learned data to be combined with new samples for retraining. The goal of these techniques is to maximize the performance with minimal replay samples~\cite{er,agem,icarl, smith2024adaptive}. 

% ER employs a reinforcement learning-based approach to manage new and old experiences efficiently. iCaRL establishes a memory limit up front, which is evenly divided among the classes previously learned. It selects and retains examples that closely resemble the average features of each class. 


% Selecting and utilizing replay samples in exact replay involves determining a memory budget, denoted as $\mathcal{M}$. Finding the optimal way to choose $\mathcal{M}$ remains an open research question~\cite{aljundi2019gradient}. Exact-replay techniques such as Experience Replay (ER)\cite{er} and Incremental Classifier and Representation Learning (iCaRL)\cite{icarl} chooses samples from previously learned data to be combined with new samples for retraining. The goal of these techniques is to minimize the sample size while getting close to joint-level performance. ER employs a reinforcement learning-based approach to manage new and old experiences efficiently. iCaRL establishes a memory limit up front, which is evenly divided among the classes previously learned. It selects and retains examples that closely resemble the average features of each class. 

% \vspace{-0.2cm}
\paragraphX{Generative/Pseudo Replay.} 
Generative or pseudo-replay strategies are designed to replicate the original data~\cite{lwf,gr,BIR, malcl}. These techniques either generate a representative of the original data using a separate generative model or generate pseudo-data by using an earlier model's predictions as soft labels for training subsequent models. 

% Methods like Learning without Forgetting (LwF)~\cite{lwf}, Generative Replay (GR)~\cite{gr}, and Brain Inspired Replay (BI-R)~\cite{BIR} use generative or pseudo-replay strategies to replicate original data. LwF generates pseudo-data by using an earlier model's predictions as soft labels for training subsequent models, similar to model distillation. Conversely, GR and BI-R utilize an additional model to learn from past data and create new samples. During current tasks, both the primary and generative models are trained using a mix of real and synthesized data. BI-R enhances this process by adding feedback connections and hidden variables for more accurate input reconstruction, proving particularly useful when access to historical data is limited.


% Methods like Learning without Forgetting (LwF)~\cite{lwf}, Generative Replay (GR)~\cite{gr}, and Brain Inspired Replay (BI-R)~\cite{BIR} use generative or pseudo-replay strategies to replicate original data. LwF generates pseudo-data by using an earlier model's predictions as soft labels for training subsequent models, similar to model distillation. Conversely, GR and BI-R utilize an additional model to learn from past data and create new samples. During current tasks, both the primary and generative models are trained using a mix of real and synthesized data. BI-R enhances this process by adding feedback connections and hidden variables for more accurate input reconstruction, proving particularly useful when access to historical data is limited.

% \vspace{-0.2cm}
\paragraphX{Compressed Replay.} 
Compressed replay, also known as latent replay, focuses on storing and replaying activations from an intermediate layer of a previous neural network to prevent forgetting. In classification tasks, these representations from hidden layers are replayed into selected layers based on the architecture and task at hand~\cite{pellegrini2020latent,ostapenko2022foundational,hayes2020remind}. 

% Compressed replay, also known as latent replay, focuses on storing and replaying activations from an intermediate layer of a previous neural network to prevent forgetting. In classification tasks, these representations from hidden layers are replayed into selected layers based on the architecture and task at hand. Compressed replay strategies include using pre-trained models for better learning~\cite{ostapenko2022foundational} and freezing the lower layers while applying quantization~\cite{hayes2020remind}.

% Regularization-based methods, like Elastic Weight Consolidation (EWC)\cite{ewc} and Synaptic Intelligence (SI)\cite{si}, cleverly work to limit changes to weights that are vital for earlier tasks. These methods add a special loss function, known as \emph{regularization loss}, to the training loss. The total loss thus reflects both new learning and knowledge from previous tasks. This combination helps to strike a balance between acquiring new information and retaining important past learnings, thus improving the system's robustness and adaptability.

% \vspace{0.2cm}





\subsection{CL in Malware and Related Domains.} 
Despite extensive work in CL, very few studies have ventured into applying CL in the realm of malware. To the best of our knowledge, Rahman et al.\cite{continual-learning-malware} were the first to explore CL for malware classification. They concluded that existing CL methods fall short in tackling forgetting in malware classification systems due to differences in the underlying nature of the data distribution shifts that occur in practice versus those explored in the computer vision domain. Malware representations leverage tabular features with strong semantic constraints that limit the space of feasible samples, and within that space, samples exhibit a high level of diversity. Replay-based techniques were found to perform better compared to other approaches in this setting. More recently, Chen et al. proposed to combine contrastive learning with active learning to continuously train Android malware classifiers~\cite{chen2023continuous}. They focus on the detection of {\em concept drift}, rather than overcoming forgetting.In addition, some studies have used \emph{online learning} for malware classification, which deals with adding new samples as they are observed but does not directly address catastrophic forgetting~\cite{droidevolver}. 
%Furthermore, Amalapuram et al.~\cite{channappayya2024augmented} explored a replay-based CL technique that incorporates class-balancing reservoir sampling and perturbation assistance for parameter approximation in network intrusion detection, which does not share the same challenges as malware classification. 
Another CL domain in cybersecurity is network intrusion detection (NID), looking for malicious activity based on network packets. Amalapuram et al. explored a replay-based CL technique that incorporates class-balancing reservoir sampling and perturbation assistance for parameter approximation NID~\cite{channappayya2024augmented}. Another recent work explored semi-supervised CL for NID in a class incremental setting~\cite{amalapuram2024spider}. We note that NID is a different domain with different data characteristics than malware. Further, these works do not focus on reducing CF. 





% In another study, Amalapuram et al.\cite{amalapuram2021handling} suggested a network intrusion detection system that employs CL techniques to curb CF when new classes are introduced.

% Lastly, some studies have used \emph{online learning} for malware classification, which deals with adding new samples but does not directly address catastrophic forgetting~\cite{droidevolver}. Lately, transfer learning has been studied for malware detection due to its ability to adapt to evolving malware types~\cite{neyshabur2020being}. Transfer learning does not focus on retaining knowledge from previous tasks when shifted to a new domain~\cite{rong2021umvd,chai2022dynamic}, which is critical in malware detection, since neglecting older versions can lead to vulnerabilities resurfacing. Considering these factors, our paper zeroes in on continual learning, which is expressly designed to address catastrophic forgetting.
