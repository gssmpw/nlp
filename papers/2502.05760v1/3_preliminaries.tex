\section{Preliminaries}
\label{preliminaries}

\subsection{CL Scenarios for Malware Classification}

% Depending on the type of data distribution shift observed in the continuous flow of data, CL is categorized into three scenarios -- Domain Incremental Learning (Domain-IL), Class Incremental Learning (Class-IL), and Task Incremental Learning (Task-IL)~\cite{van2022three}. In this work, we present a formalization of these three CL scenarios in a typical malware classification pipeline (see Figure~\ref{fig:cl-scenarios}). In this analysis pipeline, first it is crucial to detect whether a test sample is a malware or goodware which is a binary decision for the model. Then in the lower level, the task of the model is to find out the family of the identified malware which is a multi-class classification problem. The last layer of the pipeline is to identify the broader category or the behavior of the malware such as adware/ransomware which is also a multi-class classification problem. 

Continual Learning (CL) is categorized into three scenarios: Domain Incremental Learning (Domain-IL), Class Incremental Learning (Class-IL), and Task Incremental Learning (Task-IL)~\cite{van2022three}. In this work, we demonstrate how the  three CL scenarios naturally fit into a typical malware analysis pipeline (see Figure~\ref{fig:cl-scenarios}). The first step in the pipeline is to determine whether a test sample is malware or goodware. Next, the pipeline identifies the specific family of the detected malware, which is formulated as a multi-class classification problem. Finally, the pipeline classifies the broader category or behavior of the malware, such as adware or ransomware, which is also a multi-class classification problem. 
 
% Rahman et al.~\cite{continual-learning-malware} study these three scenarios for malware classification and highlights that Domain-IL and Class-IL are the two most relevant scenarios for malware domain in terms of the emergence of novel malware and the need to label new malware. 

% As such, we only study these two scenarios in this paper and briefly summarize them here. 

\paragraph{Domain-IL}
The primary challenge in malware classification lies in distinguishing between goodware and malware. Each day, VirusTotal receives one million never-before-seen samples~\cite{virustotal}, highlighting the persistent and evolving nature of software, known as \emph{concept drift}~\cite{chen2023continuous}. This evolution underscores the importance of rapidly integrating these new samples into operational systems to maintain effective protections against new threats. In addition, with the continual emergence of new benign software programs and the massive class imbalance in practice (i.e., significantly more goodware than malware), it is of utmost importance to not increase the false positive rate of the classifiers.

In this adversarial context, attackers might deploy older malware to evade detection by systems that have {\em forgotten} their signatures, necessitating a balance between adapting to new threats and preventing catastrophic forgetting. To address this, we segment our Domain-IL datasets into monthly tasks for EMBER and yearly tasks for AZ to mirror natural temporal shifts in the threat distribution.
% The primary challenge in malware classification is distinguishing between benign software (goodware) and harmful software (malware). Each day, one million malware samples, unknown applications, and goodware are submitted to VirusTotal~\cite{virustotal}, representing many new and changing software samples. This ongoing evolution, known as \emph{concept drift}, highlights the importance of quickly integrating these new samples into active systems.

% Moreover, in this adversarial setting, attackers could use older malware specifically to evade detection by systems that have {\em forgotten} how to identify them. This necessitates a balance between adapting to changes and avoiding catastrophic forgetting. In our approach to Domain-IL, we break down our dataset into monthly tasks, reflecting the natural shifts in the distribution over time.


\paragraph{Class-IL} 
% In the malware analysis pipeline, a second task involves classification into families. A {\em family} consists of malware programs sharing substantial code overlap, exhibiting common functionality, and recognized as a group by experts~\cite{zhu2020measuring}. For example, the infamous Zeus banking trojan, first identified in 2006, has undergone extensive evolution, leading to 556 versions across 35 unique families, including well-known variants like Citadel and Gameover. Labeling new malware samples and unknown applications requires expert consensus and time, and is often derived from dozens of anti-virus engines~\cite{kantchelian2015better,zhu2020measuring}. New classes emerge when a large enough set of similar samples is identified, indicating a new family. Though relatively rare, it is essential that our model adjusts to incorporate this new knowledge.

% For our multi-class malware classification model, we segment our dataset to learn new malware classes incrementally. The base model starts with a substantial number of classes, since we are already aware of many malware families, with new ones added progressively. Each new task involves integrating new classes, but performance assessment occurs across all classes trained so far.

Another significant task in malware analysis involves classifying malware into families, which are groups of programs with substantial code overlap and similar functionalities, as recognized by experts~\cite{zhu2020measuring}. For instance, the \texttt{zeus} banking trojan has evolved into 556 variants across 35 families, including \texttt{citadel} and \texttt{gameover}. Labeling new samples often relies on consensus from multiple anti-virus engines and occurs when a significant set of similar samples forms a new family~\cite{kantchelian2015better,zhu2020measuring}. In our incremental multi-class model, we start with known malware families and add new ones as they emerge, continuously adjusting and assessing the model across all known classes.


% In practice, an analysis pipeline is used to detect and triage malicious programs. The first step is binary classification of benign/malicious files. Once a file is determined to be malicious, it is useful to provide additional context to the security analyst about the malware family or its capabilities, which is encapsulated in Class IL. Since benign files are classified at the first stage in the pipeline, only malware is used in these settings.



\paragraph{Task-IL}
In malware analysis, leveraging insights from additional methods can prove beneficial. This may involve identifying the broader malware category (e.g., adware, ransomware, etc.), malware behaviors~\cite{maliciousbehavior}, or the infection vector (e.g., phishing, downloader, etc.). Task-IL encapsulates this concept of constrained tasks, where the introduction of a new task may symbolize a new category or behavior set. This event occurs less frequently than adding a new family, as seen in Class-IL, yet it poses a genuine issue in malware classification. Unlike Class-IL, the task identity is provided to the model at test time, significantly simplifying the problem. In malware, this could mean learning the task identity from a separate model, manual analysis, or field reports of the malware's behavior. However, as our datasets do not possess naturally defined tasks, we partition our dataset into tasks comprising an equal number of independent and non-overlapping classes to act as a proxy to new behaviors, following common practice in the CL literature~\cite{van2022three,BIR}. In other words, a given task would be to perform family classification among one subset of families, and the subset that each sample belongs to is known to the classifier. The model is expected to be able to handle multiple tasks at once, and new tasks are being added during each experiment.


\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/ICDM_Malware_CL_Scenarios.png}
    \caption{CL scenarios in a typical malware analysis pipeline.}
    \label{fig:cl-scenarios}
    \vskip -0.4cm
\end{figure}


\subsection{Dataset}\label{sec:dataset}

In this study, we use large-scale Windows and Android malware datasets: EMBER~\cite{ember}, a Windows malware dataset from prior work, recognized as a standard benchmark for malware classification, and two new Android malware datasets derived from AndroZoo~\cite{AndroZoo}, specifically assembled for this research.

\subsubsection{Windows PE Files}

For our experiments, we leverage the EMBER 2018 dataset, containing features from one million Windows Portable Executable (PE) files, predominantly scanned in 2018\footnote{\url{https://github.com/elastic/ember}}. The dataset comprises 400K goodware and 350K malware, with the rest labeled as unknown. EMBER provides a diverse array of 2,381 hand-crafted features, covering general file information, header data, import/export functions, and section details. Notably, these features capture strong semantic concepts that have a limited space of feasible settings, outside of which the executable does not actually run.
%It also includes format-agnostic features like byte histograms and string information, with high-cardinality features processed using the {\em hashing trick}.

%We standardized the features using \texttt{StandardScaler} from scikit-learn, which allows for incremental standardization with \texttt{partial\_fit} method, aligning with the evolving nature of data distributions over time. 
In our Class-IL experiments, we focused on 2018 malware samples from 2,900 families. After filtering out families with fewer than 400 samples, we narrowed the remaining samples down to the top 100 families, leaving 337,035 samples for analysis. For Domain-IL, we included both goodware and malware from the entire year of 2018 for binary classification, excluding unknown samples.

% We used a large-scale benchmark malware dataset for our experiments -- EMBER~\cite{ember} which is a Windows {\em Portable Executable (PE)} malware dataset. We use the 2018 version of EMBER data, as this is the latest version and is designed to be more difficult for the classifier. The EMBER 2018 dataset contains features from one million PE files scanned mostly in 2018, with 50K samples from before 2018.\footnote{\url{https://github.com/elastic/ember}} There are 400K goodware samples, 400K malware samples, and 200K unknown samples. 

% EMBER has 2,381 diverse features, including general file data, header information, import/export functions, and section information. Additionally, it includes format-agnostic features such as byte histogram, byte-entropy histogram, and string information. Features with high cardinality, like identified strings, are subjected to the {\em hashing trick} using different bin sizes. Each feature group has different distribution characteristics, contributing to the complexity of the dataset. Moreover, the EMBER feature space encodes rich semantics that inherently limit the feasible regions within the feature space, effectively mirroring the underlying executable data.

% We standardize the features using \texttt{StandardScaler} from scikit-learn. This standarization class provides a \texttt{partial\_fit} method that can incrementally standardize the dataset, treating each task as part of a continuous data flow, thus facilitating the representation of changing data distributions over time.

% In our Class-IL experiments, we utilize malware samples from 2018, comprising 2,900 families. Due to the observation that most families only contain a few samples, we apply a filter to exclude those with fewer than 400 samples, leaving us with 106 families. From these, we select the top 100 malware families, which collectively provide 337,035 samples for our research. For Domain-IL, we incorporate both goodware and malware samples from the twelve months of 2018 for binary classification, excluding the unknown samples. 



\subsubsection{Android APK Files}

% We have collected two datasets of Android APK files from AndroZoo~\cite{AndroZoo} -- AZ-Domain for Domain-IL experiments and AZ-Class for Class-IL experiments. AZ-Domain dataset contains 80690 malware samples and 677756 goodware samples from 2008 to 2016 in which samples of a particular year contains data from January to December totaling 758,446 samples. Following the practice of prior work~\cite{droidevolver},  the malware samples are selected with a virus total detection count of $>= 4$ and we kept a ratio of goodware vs. malware to 9:1, however, for some of the years, we could not maintain this exact ratio. We split the AZ-Domain dataset into a non-overlapping train and test sets for each year with a ratio of 9:1 in which the number of training and test samples are 682598 and 75848, respectively. AZ-Class dataset contains 285,582 samples from 100 android malware families that have at least 200 samples each and the virus total detection count is $>= 4$. The number of training and test samples are 223608 and 61974, respectively, maintaining a ratio of 9:1. 

% We have collected two datasets of Android APK files from AndroZoo~\cite{AndroZoo} — AZ-Domain for Domain-IL experiments and AZ-Class for Class-IL experiments. We denote these datasets as AZ dataset for the rest of this paper. The AZ-Domain dataset contains 80,690 malware samples and 677,756 goodware samples from 2008 to 2016. Each yearly sample spans from January to December, totaling 758,446 samples. Following prior work~\cite{droidevolver}, we selected malware samples with a VirusTotal detection count of $>=4$ and aimed for a 9:1 goodware to malware ratio. However, we could not maintain this exact ratio for some years. We divided the AZ-Domain dataset into non-overlapping training and testing sets for each year at a 9:1 ratio, resulting in 682,598 training samples and 75,848 testing samples. The AZ-Class dataset includes 285,582 samples from 100 Android malware families, each with at least 200 samples and a VirusTotal detection count of $>=4$. The training and testing samples number 223,608 and 61,974, respectively, maintaining a 9:1 ratio.

Additionally, we collected two datasets from AndroZoo~\cite{AndroZoo} (AZ) for our experiments: AZ-Domain for Domain-IL and AZ-Class for Class-IL and Task-IL. These datasets contain Android APK files, and both use a 9:1 ratio of goodware to malware to reflect real-world class imbalance. Following the practice of prior work~\cite{droidevolver}, the malware samples are selected with a VirusTotal detection count of $>= 4$. The AZ-Domain dataset includes 80,690 malware and 677,756 goodware samples from 2008 to 2016. We divided the AZ-Domain dataset into non-overlapping yearly training and testing sets. The AZ-Class dataset consists of 285,582 samples from 100 Android malware families, each with at least 200 samples.

% For both of these datasets, We extract Drebin features from the apps for our experiments~\cite{arp2014drebin}. Drebin has eight categories of features to comprehensively understand the app's behavior, including its access to hardware components, requested permissions, names of app components, filtered intents, usage of restricted API calls, permissions actually used, suspicious API calls, and network addresses. For the training set of AZ-Domain dataset, there are 3,858,791 features and we transformed the test dataset based on the training feature sets. For the training set of AZ-Class dataset, there are 1,067,550 features and the test set is transformed based on these features. For computational efficiency, we further process the feature space using \texttt{scikit-learn’s VarianceThreshold} to reduce the dimension by omiting the features with a low variance ($<0.001$) which provided us a final feature dimension 1789 for AZ-Domain and 2439 for AZ-Class datasets. 

% For both of these datasets, we extract Drebin features from the apps for our experiments~\cite{arp2014drebin}. Drebin has eight categories of features which comprehensively understand the app's behavior, including its access to hardware components, requested permissions, names of app components, filtered intents, usage of restricted API calls, permissions actually used, suspicious API calls, and network addresses. For the training sets of the AZ-Domain and AZ-Class datasets, there are 3,858,791 and 1,067,550 features, respectively. We transformed the test datasets based on the corresponding training feature sets. For computational efficiency, we further processed the feature space using \texttt{scikit-learn’s VarianceThreshold} to reduce the dimension by omitting the features with a low variance ($<0.001$), which provided us with final feature dimensions of 1,789 for the AZ-Domain dataset and 2,439 for the AZ-Class dataset.

We extracted Drebin features~\cite{arp2014drebin} from the apps for both datasets. These features cover various aspects of app behavior, including hardware access, permissions, app component names, filtered intents, restricted API calls, used permissions, suspicious API calls, and network addresses. Again, we note that these capture strong semantic concepts from the operation of the application. The training sets of AZ-Domain and AZ-Class have 3,858,791 and 1,067,550 features, respectively. We processed the test datasets to match the training feature sets and reduced dimensionality by filtering features with low variance ($<0.001$) using \texttt{scikit-learn}’s \texttt{VarianceThreshold}. This resulted in final feature dimensions of 1,789 for AZ-Domain and 2,439 for AZ-Class, respectively.


% \begin{figure}[!t]
% \tiny
% \begin{minipage}[c]{\linewidth}
% \centering
% % \includegraphics[width=\linewidth]{figures/malware_family_counts_by_task.pdf}
% \includegraphics[scale=0.145]{figures/malware_family_counts_by_task.pdf}
% \vskip -0.2cm
% \caption{New and already learned families in each task.}
% \label{fig:ember_frequency_families}
% \end{minipage}%
% \vskip -0.3cm
% \end{figure}







\subsection{Model Selection and Implementation}
\label{sec:modelSelectionTraining}

We use a multi-layer perceptron (MLP) model for malware classification, similar to the model used by Rahman et al.~\cite{continual-learning-malware}, for experiments with the EMBER dataset. For the AZ dataset, we developed a new MLP model with five fully-connected layers, quite similar to the MLP used for EMBER. This model uses the Adam optimizer with a learning rate of $0.001$, and batch normalization and dropout for regularization. 

% We train the models sequentially, treating each task and its associated data as separate entities and the model uses either data from the current task or a combination of the data from previous task and current task during training. The choice is contingent upon the particular method being employed. Depending on the experiment, we use either binary or multi-class cross-entropy loss.

The implementation of the output layer varies among Domain-IL, Class-IL, and Task-IL scenarios. Domain-IL operates as a series of binary classification tasks over 12 months for EMBER, and over 9 years for the AZ dataset, with two output units in each case: malicious and benign. In Class-IL, the output layer comprises units -- one for each malware family. Output units are active only if they correspond to family that have been seen by that point in the experiment. Class-IL begins with an initial set of 50 families in the first task and progressively adds five more families in each of the remaining 10 tasks for both EMBER and AZ datasets. In Task-IL, only the output units of the families in the current task are active. Both the EMBER and AZ-Class datasets divide the families equally into 20 tasks, with each task containing five families. 
%On the other hand,  In Domain-IL, both output units are always active, as the task remains binary classification, with only the data distribution changing over time.

% The experiments are conducted with a focus on reproducibility and robustness. Each set of experiments is performed around 10 times, with different random parameter initializations. This approach ensures that the results are not skewed by any particular initial configuration and provides a more reliable assessment of the algorithm's performance across varying conditions. We used \texttt{PyTorch}~\citep{pytorch} and ran our models on a \texttt{CentOS-7} machine with an Intel Xeon processor with 40 CPU cores, 128GB RAM, and four GeForce RTX 2080Ti GPU cards, each with 12GB GPU memory.




\subsection{Baselines and Metric}
\label{sec:baselines}

We adopt two baselines for comparison: {\em None} and {\em Joint}.  {\em None} sequentially trains the model on each new task without any CL techniques, serving as an informal minimum baseline. By contrast, {\em Joint} uses all new and prior data for training at each step, acting as an informal maximum baseline. Despite its resource demands, {\em Joint} ensures strong performance throughout the dataset. 
We also introduce an additional baseline -- Global Reservoir Sampling -- which provides an unbiased sampling of the underlying class distributions and serves as a strong point of comparison for our diversity-aware approach.
%{\em Global Reservoir Sampling} (GRS).

\paragraph*{Global Reservoir Sampling (GRS)}
\label{sec:grs}

GRS simply selects samples at random from a global stored data pool~\cite{vitter1985random,zhang2017deeper}. Given a memory budget $\beta$, GRS randomly picks $\beta$ samples from a data pool $\mathcal{P}$, with each incremental learning task contributing to the pool. 
%$\beta$ remains fixed throughout the learning process. In each task, GRS randomly selects $\beta$ samples from $\mathcal{P}$ where $\beta < \mathcal{P}$. 
If $\beta \geq \mathcal{P}$, GRS selects all the available samples in $\mathcal{P}$. 
Rahman et al.~\cite{continual-learning-malware} investigated GRS -- which they refer to as Partial Joint Replay -- only for Domain-IL scenario of EMBER dataset. In this work, we present a deeper investigation of GRS in both Domain-IL and Class-IL scenarios with both EMBER and AZ datasets. 

%We compare our proposed techniques with GRS using the same memory budget of our proposed techniques for a fair comparison. In GRS, goodware and malware samples create a combined pool for Domain-IL. For Class-IL, samples in the pool come from classes previously trained on.


% \vspace{0.2cm}
% Following the prior work~\cite{continual-learning-malware}, we use a multi-layer perceptron (MLP) model for our experiments. Details are explained in the Appendix.We measure the performance of our experiments throughout this work using \emph{global average accuracy}.

\paragraph*{Global average accuracy ($\overline{AP} \in [0, 100]\%$)}
To maintain consistency with prior work, we present results using \emph{global average accuracy} as the primary metric for our evaluations~\cite{continual-learning-malware, BIR, icarl}. Note that we conducted a subset of evaluations using other metrics, such as F1 score, precision, and recall, which are not included in this paper. The conclusions remain unchanged for all of these metrics.

Let $P_{i,j}$ be the accuracy of the model on the test set
of task $T_j$, $j \leq i$, after 
%the model is trained on task $T_i$, $j \leq i$, after 
continually training the model on tasks $1~\text{to}~i$. Then the average accuracy $AP$ at task $T_i$ is defined as $AP_i = \frac{1}{i} \sum_{j=1}^i P_{i,j}$. For $N$ total tasks, the global average accuracy $\overline{AP}$ over all tasks is computed as $\overline{AP} = \frac{1}{N} \sum_{i=1}^{N} AP_i * 100$.


% \begin{equation}
%     \overline{AP} = \frac{1}{N} \sum_{i=1}^{N} AP_i 
% \end{equation}

% \begin{equation}
%     \overline{AP} = \frac{1}{N} \sum_{i=1}^{N} \left( \frac{1}{i} \sum_{j=1}^i P_{i,j} \right) * 100\%
% \end{equation}






\subsection{Training and Evaluation Protocol}

% A continual learning (CL) model is sequentially trained to learn tasks from $t_1, t_2, ..., t_T$ each with its distinct data distribution $p(x,y|t_i)$. The goal is to adapt to new tasks without forgetting the old ones. CL training involves three sets of parameters: shared parameters ($\theta_{s}$) across all tasks, old task-specific parameters ($\theta_{0}$), and new task parameters ($\theta_{n}$)~\cite{lwf}. The {\em Joint} training benchmark trains the model with all the available training samples till the current task and optimizes all these parameters simultaneously, however, incur incremental storage and training cost. In contrast, CL training strives to optimize and update $\theta_{s}$ and $\theta_{n}$, while maintaining $\theta_{0}$ in a relatively fixed state, for each new task $t_{n}$. The updating, however, of any of the shared weights $\theta_{s}$ risks confusing the classifier when faced with older data, as those classification decisions depend on not only $\theta_{0}$, but also $\theta_{s}$. CL training typically boasts significantly faster speed and far less storage requirements than {\em Joint} training, thus permitting more frequent model retraining to adapt to evolving data distributions or other requirements.


A continual learning (CL) model is sequentially trained to learn tasks from $t_1, t_2, ..., t_T$, each with its distinct data distribution $p(x,y|t_i)$. The goal is to adapt to new tasks without forgetting the old ones. CL training involves three sets of parameters: shared parameters ($\theta_{s}$) across all tasks, old task-specific parameters ($\theta_{0}$), and new task parameters ($\theta_{n}$)~\cite{lwf}. The {\em Joint} training benchmark trains the model with all the available training samples up to the current task and optimizes all these parameters simultaneously; however, it incurs incremental storage and training costs. In contrast, CL training strives to optimize and update $\theta_{s}$ and $\theta_{n}$, while maintaining $\theta_{0}$ in a relatively fixed state for each new task $t_{n}$. However, updating any of the shared weights $\theta_{s}$ risks confusing the classifier when faced with older data, as those classification decisions depend not only on $\theta_{0}$ but also on $\theta_{s}$. CL training typically boasts significantly faster speeds and far less storage requirements than {\em Joint} training, thus permitting more frequent model retraining to adapt to evolving data distributions or other requirements.




% For the evaluation, we use a non-overlapping hold-out set corresponding to each task. For instance, AZ-Domain contains 9-years of training samples from 2009 to 2016, hence, it has 9 hold-out sets corresponding to year 2009 to 2016. A CL model is evaluated on all the hold-out sets till the current tasks. Formally, a CL model $\mathcal{M}$ is evaluated on task $t_i$ to $t_{T}$ where $i <=T $ after the model is trained for the current task $t_{T}$.

In our evaluations, we use a non-overlapping hold-out set corresponding to each task. For example, the AZ-Domain dataset contains 8 years of training samples from 2008 to 2016, resulting in 9 hold-out sets, one for each year. A CL model is evaluated on all the hold-out sets up to the current task; formally, 
%a CL model $\mathcal{M}$ 
the model is evaluated on tasks $t_i$ to $t_{T}$, for $1 \leq i \leq T$, after it been trained on the current task $t_{T}$.

In this work, each set of experiments is performed around 10-15 times with different random parameter initializations. We use PyTorch on a CentOS-7 machine with an Intel Xeon processor, 40 CPU cores, 128GB RAM, and four GeForce RTX 2080Ti GPUs, each with 12GB memory.