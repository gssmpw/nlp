\section{Evaluation}

% \saidur{Working on it}




\input{tables/EM_DIL_Compare}

\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/EMBER_IFS_DIL_RATIO.pdf}
        \label{fig:EMBER_DIL_IFS_R}
        \vspace{-0.4cm}
        \caption{MADAR Ratio}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/EMBER_IFS_DIL_UNIFORM.pdf}
        \label{fig:EMBER_DIL_IFS_U}
        \vspace{-0.4cm}
        \caption{MADAR Uniform}
    \end{subfigure}
    \vfill
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/EMBER_AWS_DIL_RATIO.pdf}
        \label{fig:EMBER_DIL_AWS_R}
        \vspace{-0.4cm}
        \caption{MADAR$^\theta$ Ratio}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/EMBER_AWS_DIL_UNIFORM.pdf}
        \label{fig:EMBER_DIL_AWS_U}
        \vspace{-0.4cm}
        \caption{MADAR$^\theta$ Uniform}
    \end{subfigure}

    \caption{EMBER Domain-IL: Comparison of the MADAR-R, MADAR-U, MADAR$^\theta$-R, and MADAR$^\theta$-U with Joint baseline.}
    \label{fig:ember_DIL}
    \vspace{-0.3cm}
\end{figure}




\input{tables/AZ_DIL_Compare}


\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/AZ_IFS_DIL_RATIO.pdf}
        \label{fig:AZ_DIL_IFS_R}
        \vspace{-0.4cm}
        \caption{MADAR Ratio}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/AZ_IFS_DIL_UNIFORM.pdf}
        \label{fig:AZ_DIL_IFS_U}
        \vspace{-0.4cm}
        \caption{MADAR Uniform}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/AZ_AWS_DIL_RATIO.pdf}
        \label{fig:AZ_DIL_AWS_R}
        \vspace{-0.4cm}
        \caption{MADAR$^\theta$ Ratio}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/AZ_AWS_DIL_UNIFORM.pdf}
        \label{fig:AZ_DIL_AWS_U}
        \vspace{-0.4cm}
        \caption{MADAR$^\theta$ Uniform}
    \end{subfigure}

    \caption{AZ Domain-IL: Comparison of the MADAR-R, MADAR-U, MADAR$^\theta$-R, and MADAR$^\theta$-U with Joint baseline.}
    \label{fig:az_DIL}
    \vspace{-0.3cm}
\end{figure}






% \subsection{Experimental Setup, Datasets, and Baselines}


We present the results of our \system\ framework and MADAR$^\theta$ in the Domain-IL, Class-IL, and Task-IL scenarios using the EMBER and AZ datasets discussed in Section~\ref{sec:dataset}. To denote our techniques, we use the following abbreviations: {\bf \system-R} for \system-Ratio, {\bf \system-U} for \system-Uniform, {\bf MADAR$^\theta$-R} for MADAR$^\theta$-Ratio, and {\bf MADAR$^\theta$-U} for MADAR$^\theta$-Uniform.

For all three scenarios, we compare \system\ against widely studied replay-based continual learning (CL) techniques, including experience replay (ER)\cite{er}, average gradient episodic memory (AGEM)\cite{agem}, deep generative replay (GR)\cite{gr}, Replay-through-Feedback (RtF)\cite{rtf}, and Brain-inspired Replay (BI-R)\cite{BIR}. Additionally, we evaluate \system\ against iCaRL\cite{icarl}, a replay-based method specifically designed for Class-IL. For the Class-IL and Task-IL scenarios, we additionally compare \system\ with Task-specific Attention Modules in Lifelong Learning (TAMiL)\cite{tamil}. Furthermore, we benchmark MADAR against MalCL\cite{malcl}, a method specifically designed for Class-IL. Notably, most recent work focuses primarily on Class-IL and Task-IL scenarios, limiting direct comparisons in the Domain-IL scenario. In our results tables, the best-performing methods and those within the error margin of the top results are highlighted. 

%Finally, we built upon the codebase provided by \cite{continual-learning-malware} for implementation and evaluation.


% In this study, we utilize large-scale malware datasets, including the EMBER dataset~\cite{ember}, a widely recognized benchmark for Windows malware classification, and two Android malware datasets derived from AndroZoo~\cite{AndroZoo}, which were specifically curated for this research. Our approach is evaluated against two primary baselines:

% \begin{smitemize}
%     \item \textbf{None}: A baseline where the model is trained sequentially on each new task without employing any continual learning (CL) techniques, serving as an informal lower bound.
%     \item \textbf{Joint}: A baseline where the model is trained on both new and previously seen data at each step, representing an informal upper bound. While resource-intensive, the \textbf{Joint} baseline consistently achieves robust performance.
% \end{smitemize}

% Additionally, we introduce a third baseline: \textbf{Global Reservoir Sampling (GRS)}. This method is based on reservoir sampling~\cite{vitter1985random} and builds upon prior work by \cite{continual-learning-malware}. GRS provides an unbiased representation of class distributions and serves as a strong benchmark for comparing our diversity-aware approach.




% We now present the results of our \system framework for both \system and MADAR$^\theta$ in the Domain-IL, Class-IL, and Task-IL scenarios for EMBER and AZ datasets. We use the following four abbreviations to denote our techniques---{\bf \system-R} for \system-Ratio, {\bf ~\system-U} for \system-Uniform, {\bf MADAR$^\theta$-R} for MADAR$^\theta$-Ratio, and {\bf ~MADAR$^\theta$-U} for MADAR$^\theta$-Uniform.  For all three scenarios, we compare \system\ with the most widely studied replay-based CL techniques: experience replay (ER)~\cite{er}, average gradient episodic memory (AGEM)~\cite{agem}, deep generative replay (GR)~\cite{gr}, Replay-through-Feedback (RtF)~\cite{rtf}, and Brain-inspired Replay (BI-R)~\cite{BIR}. In addition, we compare \system\ with iCaRL~\cite{icarl}, a replay-based technique specifically designed for Class-IL. Furthermore, we compare \system with Task-specific Attention Modules in Lifelong learning (TAMiL)~\cite{bhat2023task} which is designed for Class-IL and Task-IL scenarios. In addition, we also compare MADAR with MalCL~\cite{malcl} specifically designed for Class-IL. We observe that recent works mostly focus on Class-IL and Task-IL scenarios which limits what we can compare with in the Domain-IL scenario. The results of the best-performing method, as well as those within the error range of the best results, are highlighted in the results tables. We built upon the code of the prior work by \cite{continual-learning-malware}.

% In this study, we use large-scale Windows and Android malware datasets: EMBER~\cite{ember}, a Windows malware dataset from prior work, recognized as a standard benchmark for malware classification, and two new Android malware datasets derived from AndroZoo~\cite{AndroZoo}, specifically assembled for this research.

% We adopt two baselines for comparison: {\em None} and {\em Joint}.  {\em None} sequentially trains the model on each new task without any CL techniques, serving as an informal minimum baseline. By contrast, {\em Joint} uses all new and prior data for training at each step, acting as an informal maximum baseline. Despite its resource demands, {\em Joint} ensures strong performance throughout the dataset. We also introduce an additional baseline -- Global Reservoir Sampling (GRS) built upon {\em reservoir sampling}~\cite{vitter1985random} and \cite{continual-learning-malware}. GRS provides an unbiased sampling of the underlying class distributions and serves as a strong point of comparison for our diversity-aware approach.

% In this study, we utilize large-scale malware datasets, including the EMBER dataset~\cite{ember}, a widely used benchmark for Windows malware classification, and two Android malware datasets derived from AndroZoo~\cite{AndroZoo}, specifically assembled for this research. We compare our approach against two baselines: {\em None}, where the model is trained sequentially on each new task without any CL techniques, serving as an informal lower bound; and {\em Joint}, which trains on both new and previous data at each step, representing an informal upper bound. Although resource-intensive, {\em Joint} ensures consistently strong results. Additionally, we introduce another baseline -- Global Reservoir Sampling (GRS), an approach based on {\em reservoir sampling}~\cite{vitter1985random} and \cite{continual-learning-malware}, which provides an unbiased representation of class distributions and serves as a strong point of comparison for our diversity-aware approach.


\subsection{Domain-IL}
\label{domainilexps}

%% #of training samples --> 674994
%As shown in Table~\ref{tab:combined_DIL}, a



In EMBER, we have 12 tasks, each representing the monthly data distribution spanning January--December 2018. Our results, detailed in Table~\ref{tab:ember_DIL}, provide a comprehensive view of each method's performance, reported as the average accuracy over all tasks $\mathbf{\overline{AP}}$. Additionally, Figure~\ref{fig:ember_DIL} illustrates the progression of average accuracy over time compared to the \textit{Joint} baseline. 

The informal lower and upper performance bounds for this configuration are approximated by the \textit{None} and \textit{Joint} methods, achieving $\mathbf{\overline{AP}}$ scores of 93.1\% and 96.4\%, respectively. Meanwhile, \textit{GRS} serves as a strong baseline, providing unbiased sampling without incorporating sample diversity awareness.

% In EMBER, we have 12 tasks, each representing the monthly data distribution spanning January--December 2018. Our results, detailed in Table~\ref{tab:ember_DIL}, present a nuanced view of each method's performance, reported as the average accuracy over all tasks $\mathbf{\overline{AP}}$. In addition, Figure~\ref{fig:ember_DIL} represents the progression of average accuracy as the task progresses compared with {joint} baseline. The informal lower and upper performance bounds for this configuration can be approximated by the {\em None} and {\em Joint} methods, which get $\mathbf{\overline{AP}}$ of 93.1\% and 96.4\%, respectively. Meanwhile, {\em GRS} represents a strong baseline for unbiased sampling without awareness of sample diversity.

At a lower budget of 1K, \system-R, \system-U, and MADAR$^\theta$-R exhibit competitive performance, all achieving $\mathbf{\overline{AP}}$ of over $93.6$\%, significantly outperforming prior approaches. This highlights their ability to effectively utilize limited resources. In particular, \system-R achieves the highest accuracy at this budget, with $\mathbf{\overline{AP}}$ of $93.7\%$.

As the memory budget increases, the performance of all \system\ and MADAR$^\theta$ variants improves steadily. At a budget of 200K, \system-R and MADAR$^\theta$-R achieve an impressive $\mathbf{\overline{AP}}$ of $96.0\%$ and $96.1\%$, respectively, closely approaching the $96.4\%$ achieved by the \textit{Joint} baseline, which utilizes over 670K samples. Uniform strategies, including \system-U and MADAR$^\theta$-U, are only slightly behind, with $\mathbf{\overline{AP}}$ values of $95.5\%$ and $95.6\%$, respectively.

% At lower budget of 1K, GRS, \system-R, and \system-U exhibit competitive performance, all significantly better than prior work with $\mathbf{\overline{AP}}$ above $93.6$\%, indicating their effective utilization of limited resources. ER and AGEM performed far below even the \emph{None} baseline, while GR could only match it. For higher budgets, GRS and \system\ methods all show excellent performance. At a 200K budget, \system-R yields $\mathbf{\overline{AP}}$ of $96.0$\%, close to the $96.4$\% reached by the Joint baseline that used over 670K samples. GRS is competitive, while Uniform strategies are only slightly behind.


\input{tables/EM_CIL_Compare}


\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/EMBER_CIL_IFS_RATIO.pdf}
        \label{fig:EMBER_CIL_IFS_R}
        \vspace{-0.4cm}
        \caption{MADAR Ratio}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/EMBER_CIL_IFS_UNIFORM.pdf}
        \label{fig:EMBER_CIL_IFS_U}
        \vspace{-0.4cm}
        \caption{MADAR Uniform}
    \end{subfigure}
    \vfill
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/EMBER_CIL_AWS_RATIO.pdf}
        \label{fig:EMBER_CIL_AWS_R}
        \vspace{-0.4cm}
        \caption{MADAR$^\theta$ Ratio}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/EMBER_CIL_AWS_UNIFORM.pdf}
        \label{fig:EMBER_CIL_AWS_U}
        \vspace{-0.4cm}
        \caption{MADAR$^\theta$ Uniform}
    \end{subfigure}

    \caption{EMBER Class-IL: Comparison of the MADAR-R, MADAR-U, MADAR$^\theta$-R, and MADAR$^\theta$-U with Joint baseline.}
    \label{fig:ember_CIL}
    \vspace{-0.3cm}
\end{figure}


For the experiments with AZ-Domain, we consider 9 tasks, each representing a yearly data distribution from 2008 to 2016. The performance of each method is presented in Table~\ref{tab:az_DIL} as $\mathbf{\overline{AP}}$ and compared to two baselines: \textit{None}, which achieves $94.4\%$, and \textit{Joint}, which reaches $97.3\%$. Additionally, Figure~\ref{fig:az_DIL} illustrates the progression of average accuracy across tasks, highlighting the comparison with the \textit{Joint} baseline.

Similar to the results observed with EMBER, our MADAR techniques consistently outperform prior methods such as ER, AGEM, GR, RtF, and BI-R across all budget levels. For lower budgets, such as 1K, \system-R achieves $\mathbf{\overline{AP}}$ of $95.8\%$ and coming within 1.5\% of the \textit{Joint} baseline.

At higher budgets, ranging from 100K to 400K, \system-R continues to demonstrate high $\mathbf{\overline{AP}}$ scores of up to $97.0\%$, closely matching GRS and only marginally below the \textit{Joint} baseline, which requires significantly more training samples (680K). Notably, MADAR$^\theta$-R exhibits comparable performance, reaching a peak $\mathbf{\overline{AP}}$ of $97.2\%$ at the highest budget level, further underscoring the efficacy of our diversity-aware approach.



% For the experiments with AZ-Domain, we have 9 tasks, each representing a year from 2008 to 2016. The performance of each method is shown in Table~\ref{tab:az_DIL} as $\mathbf{\overline{AP}}$ and compared with two baselines: {\em None} at $94.4\pm0.1$ and {\em Joint} at $97.3\pm0.1$. Additionally, Figure~\ref{fig:az_DIL} illustrates the progression of average accuracy as tasks progress, compared to the \textit{Joint} baseline. 

% As with EMBER, we find that our MADAR techniques greatly surpass previous methods like ER, AGEM, GR, RtF, and BI-R for every budget level. For lower budgets like 1K, \system-R slightly outperforms GRS and is within 1.5\% of {\em Joint}. For higher budgets (100K-400K), \system-R perform well -- in line with GRS and just slightly below {\em Joint}, which requires 680K training samples. 


% In summary, our results empirically depict the effectiveness of MADAR's diversity-aware sample selection in maximizing the efficiency and effectiveness of a malware classifier in Domain-IL. \system-R is either better or on par with GRS and significantly better than prior work.

In summary, these results empirically demonstrate the effectiveness of MADAR's diversity-aware sample selection in enhancing the efficiency and accuracy of malware classification in Domain-IL scenarios. \system-R and MADAR$^\theta$-R, in particular, consistently either yield on-par or outperform GRS while delivering significant improvements over prior methods.










\input{tables/AZ_CIL_Compare}





\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/AZ_CIL_IFS_RATIO.pdf}
        \label{fig:AZ_CIL_IFS_R}
        \vspace{-0.4cm}
        \caption{MADAR Ratio}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/AZ_CIL_IFS_UNIFORM.pdf}
        \label{fig:AZ_CIL_IFS_U}
        \vspace{-0.4cm}
        \caption{MADAR Uniform}
    \end{subfigure}
    \vfill
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/AZ_CIL_AWS_RATIO.pdf}
        \label{fig:AZ_CIL_AWS_R}
        \vspace{-0.4cm}
        \caption{MADAR$^\theta$ Ratio}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/AZ_CIL_AWS_UNIFORM.pdf}
        \label{fig:AZ_CIL_AWS_U}
        \vspace{-0.4cm}
        \caption{MADAR$^\theta$ Uniform}
    \end{subfigure}

    \caption{AZ Class-IL: Comparison of the MADAR-R, MADAR-U, MADAR$^\theta$-R, and MADAR$^\theta$-U with Joint baseline.}
    \label{fig:az_CIL}
    \vspace{-0.3cm}
\end{figure}





\subsection{Class-IL}
\label{classilexps}



In this set of experiments with EMBER, we consider 11 tasks, starting with 50 classes (representing distinct malware families) in the initial task, and incrementally adding five new classes in each subsequent task. Table~\ref{tab:ember_CIL} presents the performance of each method, measured by average accuracy $\mathbf{\overline{AP}}$. The \textit{None} and \textit{Joint} baselines achieve $\mathbf{\overline{AP}}$ values of $26.5\%$ and $86.5\%$, respectively, providing informal lower and upper bounds. Figure~\ref{fig:ember_CIL} illustrates the progression of average accuracy across tasks, showing how the \system\ and MADAR$^\theta$ methods compare to the \textit{Joint} baseline.

At a very low budget of just 100 samples, \system-R achieves a notable $\mathbf{\overline{AP}}$ of $68.0\%$, outperforming GRS and prior methods by a significant margin. As the budget increases, \system-U emerges as the top performer, achieving $\mathbf{\overline{AP}}$ values of $76.5\%$ and $79.4\%$ at 1K and 10K budgets, respectively, surpassing all other methods, including GRS. 

%For example, at a 10K budget, \system-U outperforms GRS, which achieves $83.5\%$, with an $\mathbf{\overline{AP}}$ of $84.8\%$.

At higher budgets, \system-U and MADAR$^\theta$-U continue to excel, with MADAR$^\theta$-U achieving the best results overall. At a 20K budget, MADAR$^\theta$-U reaches an $\mathbf{\overline{AP}}$ of $86.2\%$, nearly equaling the \textit{Joint} baseline, which uses over {\bf 150 times} more training samples. These results clearly demonstrate the effectiveness of MADAR's diversity-aware sample selection and the effectiveness of \system-U and MADAR$^\theta$-U in leveraging limited resources.

In contrast, prior methods such as ER, AGEM, GR, RtF, and BI-R fail to exceed 30\% $\mathbf{\overline{AP}}$, while more advanced techniques like TAMiL and MalCL achieve only $38.2\%$ and $54.8\%$, respectively. Even iCaRL, designed specifically for Class-IL, achieves only $64.6\%$, further highlighting the significant advantage of our approaches in the malware domain.


% In this set of experiments with EMBER, we have 11 tasks, where the initial task starts with 50 classes---one for each of 50 malware families---and five classes are added in each subsequent task. The performance of these methods, detailed in Table~\ref{tab:az_CIL}, is measured by average accuracy $\mathbf{\overline{AP}}$ with {\em None} and {\em Joint} training baselines at an $\mathbf{\overline{AP}}$ of $26.5\pm0.2$ and $86.5\pm0.4$, respectively. Additionally, Figure~\ref{fig:ember_CIL} illustrates the progression of average accuracy across tasks, highlighting the comparison with the \textit{Joint} baseline. 

% For a very low budget of 100 samples, \system methods greatly outperform GRS, with \system-R getting 16\% higher $\mathbf{\overline{AP}}$. For more reasonable budgets, however, the uniform variant \system-U offers the best performance. For example, with a 10K budget, \system-U yields at least 84.8\% $\mathbf{\overline{AP}}$, which is better than GRS at 83.5\% $\mathbf{\overline{AP}}$. They also fare far better than all prior works, with ER, AGEM, GR, RtF, and BI-R below 30\%, TAMiL at 38.2\%, MalCL at 54.8\% and iCaRL at only 64.6\%. These poor results for the prior methods are in line with other findings in the malware domain~\cite{continual-learning-malware}. For a budget of 20K, \system-U reaches $85.8\pm0.3$, nearly as good as the Joint baseline that uses a maximum budget over 150 times larger.



In the Class-IL setting of AZ-Class, we consider 11 tasks. The summary results of all experiments are provided in Table~\ref{tab:az_CIL}, with comparisons against the \textit{None} and \textit{Joint} baselines, which achieve $\mathbf{\overline{AP}}$ scores of $26.4\%$ and $94.2\%$, respectively. Figure~\ref{fig:az_CIL} illustrates the progression of average accuracy across tasks, showing how each method performs relative to the \textit{Joint} baseline.

As shown in Table~\ref{tab:az_CIL}, among the prior methods, iCaRL performs best across most budget configurations, outperforming techniques such as MalCL, TAMiL, ER, AGEM, GR, RtF, and BI-R. Therefore, we focus on comparing MADAR's performance with iCaRL. At a low budget of 100 samples, iCaRL and GRS achieve less than $44\%$ $\mathbf{\overline{AP}}$, while all MADAR methods surpass $57\%$. In particular, \system-R and MADAR$^\theta$-R achieve $\mathbf{\overline{AP}}$ scores of $59.4\%$ and $58.8\%$, respectively, highlighting their efficiency at low-resource levels.

As the budget increases, all methods improve, but \system-U consistently delivers the best results. At a budget of 1K, \system-U achieves the highest $\mathbf{\overline{AP}}$ at $70.4\%$, followed closely by MADAR$^\theta$-U at $70.1\%$. This trend continues as budgets increase, with \system-U outperforming all other methods, achieving $\mathbf{\overline{AP}}$ scores of $89.8\%$ at 10K and $91.5\%$ at 20K. Compared to GRS, which achieves $90.1\%$ at 20K, and iCaRL, which trails at $84.6\%$, \system-U demonstrates clear superiority. MADAR$^\theta$-U also performs GRS reaching $90.7\%$ at 20K.



% We have 11 tasks for the Class-IL setting of AZ-Class. The summary results of all the experiments are shown in Table~\ref{tab:az_CIL} and benchmarked against {\em None} and {\em Joint} with $\mathbf{\overline{AP}}$ of $26.4\pm0.2$ and $94.2\pm0.1$, respectively. Figure~\ref{fig:az_CIL} illustrates the progression of average accuracy across tasks, highlighting the comparison with the \textit{Joint} baseline. 


% As we can from Table~\ref{tab:az_CIL} that, among TAMiL, iCaRL, ER, AGEM, GR, RtF, and BI-R, iCaRL outperforms in most of the budget configurations. Therefore, we discuss the results of MADAR in comparison with iCaRL. For a low budget of 100, iCaRL and GRS get less than 44\%, while all MADAR methods achieve over 57\%. As budgets increase, all methods improve, with \system-U offering the best results at every budget from 1K to 20K. At 20K, it reaches $91.5\pm0.1\%$, which is 1.4\% higher than GRS and 6.9\% higher than iCaRL.



In summary, our experiments demonstrate the effectiveness of \system's diversity-aware replay techniques in the Class-IL setting for both EMBER and AZ datasets. While GRS shows significant improvement with larger budgets, \system's uniform variants consistently outperform it across all budget levels. These results underscore \system's ability to mitigate catastrophic forgetting and enhance malware classification performance, even in resource-constrained environments.

% In summary, our experiments clearly demonstrate the effectiveness of \system's diversity-aware replay techniques in Class-IL for both EMBER and AZ datasets. Additionally, while GRS shows significant improvement with an increased budget, the uniform variants of \system  are more effective at every budget level. \system  significantly improves performance in malware classification by mitigating catastrophic forgetting, and they do so using fewer resources.











\input{tables/EM_TIL_Compare}


\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/EMBER_TIL_IFS_RATIO.pdf}
        \label{fig:EMBER_TIL_IFS_R}
        \vspace{-0.4cm}
        \caption{MADAR Ratio}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/EMBER_TIL_IFS_UNIFORM.pdf}
        \label{fig:EMBER_TIL_IFS_U}
        \vspace{-0.4cm}
        \caption{MADAR Uniform}
    \end{subfigure}
    \vfill
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/EMBER_TIL_AWS_RATIO.pdf}
        \label{fig:EMBER_TIL_AWS_R}
        \vspace{-0.4cm}
        \caption{MADAR$^\theta$ Ratio}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.485\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/EMBER_TIL_AWS_UNIFORM.pdf}
        \label{fig:EMBER_TIL_AWS_U}
        \vspace{-0.4cm}
        \caption{MADAR$^\theta$ Uniform}
    \end{subfigure}

    \caption{EMBER Task-IL: Comparison of the MADAR-R, MADAR-U, MADAR$^\theta$-R, and MADAR$^\theta$-U with Joint baseline.}
    \label{fig:ember_TIL}
    \vspace{-0.3cm}
\end{figure}

























\subsection{Task-IL}
\label{taskilexps-ember}


In this set of experiments with EMBER, we consider 20 tasks, with 5 new classes added in each task. The summarized results are shown in Table~\ref{tab:ember_TIL}, where performance is reported as the average accuracy over all tasks ($\mathbf{\overline{AP}}$). It is worth noting that Task-IL is considered the easiest scenario in continual learning~\cite{van2022three, BIR}. The \textit{None} and \textit{Joint} methods serve as informal lower and upper bounds, achieving $\mathbf{\overline{AP}}$ scores of $74.6\%$ and $97\%$, respectively. Figure~\ref{fig:ember_TIL} visualizes the progression of average accuracy across tasks, highlighting comparisons with the \textit{Joint} baseline.

As shown in Table~\ref{tab:ember_TIL}, ER consistently outperforms TAMiL, A-GEM, GR, RtF, and BI-R across all budget configurations and even surpasses GRS in some cases. However, \system\ variants significantly outperform all prior methods, particularly under lower budget constraints (100–1K). For example, \system-U achieves the highest $\mathbf{\overline{AP}}$ of $93.4\%$ and $93.7\%$ at budgets of 100 and 1K, respectively, outperforming GRS and all other approaches. Similarly, MADAR$^\theta$-U performs competitively, with $\mathbf{\overline{AP}}$ of $93.2\%$ at a 100 budget and $93.8\%$ at 1K.

As the budget increases, the performance gap among \system, ER, and GRS narrows; however, \system\ variants continue to either outperform or match other techniques. Notably, the \system-U variant of MADAR achieves the best overall performance at a budget of 20K, attaining a $\mathbf{\overline{AP}}$ of $95.8\%$, which closely approaches the \textit{Joint} baseline. Similarly, \system-R yields $\mathbf{\overline{AP}}$ of $95.6\%$ at 20K.



% In this set of experiments with EMBER, we have 20 tasks with 5 new classes in each task. Table~\ref{tab:ember_TIL} shows a summarized view of this set of experiments, where the performances are presented as the average accuracy over all tasks ($\mathbf{\overline{AP}}$). Note that Task-IL is considered the easiest scenario of continual learning~\cite{van2022three, BIR}. The {\em None} and {\em Joint} methods, which are the informal lower and upper bounds of this configuration, attain $\overline{AP}$ of $74.6\%$ and $\overline{AP}$ of $97.03\%$, respectively. Figure~\ref{fig:ember_TIL} illustrates the progression of average accuracy across tasks, showing how each method performs relative to the \textit{Joint} baseline.

% As we can see from Table~\ref{tab:combined_TIL}, ER outperforms TAMiL, A-GEM, GR, RtF, and BI-R in all budget configurations and outperforms GRS for few configurations. \system, on the other hand, outperforms all the prior methods significantly in lower budget constraints ($100$–$1K$). For instance, \system-U reaches $\mathbf{\overline{AP}}$ of 93.9\% with only 1K replay samples, compared with 93.6\% for GRS. The performance gap among MADAR, ER, and GRS gets closer as the budget increases; however, \system  variants continue to either outperform or perform on par with other techniques. In particular, the \system-U variant of MADAR outperforms all the other techniques and attains $\mathbf{\overline{AP}}$ of 95.8\% with a 20K replay budget, which is close to joint level performance.


Task-IL for AZ consists of 20 tasks, each with 5 non-overlapping classes. The results are summarized in Table~\ref{tab:az_TIL} and benchmarked against the \textit{None} and \textit{Joint} baselines, which achieve $\mathbf{\overline{AP}}$ values of $74.5\%$ and $98.8\%$, respectively. Figure~\ref{fig:az_TIL} illustrates the progression of average accuracy across tasks, showing how each method performs relative to the \textit{Joint} baseline.

As seen in Table~\ref{tab:az_TIL}, ER consistently outperforms TAMiL, AGEM, GR, RtF, BI-R, and GRS across most budget configurations, making it a strong baseline for comparison. At a low budget of 100 samples, \system-U achieves $\mathbf{\overline{AP}}$ of $88.1\%$, which is 4.5\% higher than ER's performance. Similarly, MADAR$^\theta$-U demonstrates competitive performance, achieving $87.9\%$ at the same budget.

As the budget increases, \system-U continues to deliver the best performance, reaching $\mathbf{\overline{AP}}$ scores of $94.5\%$ at a 1K budget and $98.1\%$ at a 10K budget, outperforming all other methods, including ER and GRS. At the highest budget of 20K, \system-U achieves an $\mathbf{\overline{AP}}$ of $98.7\%$, surpassing ER by 1.2\% and nearly matching the \textit{Joint} baseline. Notably, MADAR$^\theta$-U also performs well, achieving $98.1\%$. In contrast, \system-R and MADAR$^\theta$-R perform slightly lower but remain competitive, with $\mathbf{\overline{AP}}$ values of $97.9\%$ and $96.9\%$ at a 20K budget, respectively. These results indicate that ratio-based methods, while effective, are slightly less robust than uniform sampling in this scenario.

In summary, \system-U and MADAR$^\theta$-U consistently demonstrate better performance across most of the budget levels, particularly excelling at low-resource settings and achieving near-optimal results at higher budgets. These findings underscore the effectiveness of \system\ framework in Task-IL scenarios and their ability to approach joint-level performance with significantly fewer resources.


% Task-IL for AZ contains 20 tasks, each with 5 non-overlapping classes. Our results are shown in Table~\ref{tab:az_TIL}, compared against the {\em None} and {\em Joint} benchmarks, with $\mathbf{\overline{AP}}$ of 74.5\% and 98.8\%, respectively. Figure~\ref{fig:az_TIL} illustrates the progression of average accuracy across tasks, showing how each method performs relative to the \textit{Joint} baseline. As with EMBER, ER outperforms TAMiL, AGEM, GR, RtF, BI-R, and GRS for most budgets, so we use it for comparison. For a low budget of 100, \system-U achieves an $\overline{AP}$ of 88.1\%, 4.5\% higher than that of ER. For a higher budget of 20K, \system-U attains an $\overline{AP}$ of 98.7\%, which is 1.2\% higher than that of ER and very close to the joint level performance of 98.8\%.


% Overall, mirroring the success seen with the EMBER dataset, our proposed techniques also surpass previous work in Task-IL in the context of the AZ-Class dataset. Additionally, while ER and GRS shows significant improvement with an increased budget, the uniform variant of IFS of MADAR is more effective at every budget level.





\input{tables/AZ_TIL_Compare}


\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/AZ_TIL_IFS_RATIO.pdf}
        \label{fig:AZ_TIL_IFS_R}
        \vspace{-0.4cm}
        \caption{MADAR Ratio}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/AZ_TIL_IFS_UNIFORM.pdf}
        \label{fig:AZ_TIL_IFS_U}
        \vspace{-0.4cm}
        \caption{MADAR Uniform}
    \end{subfigure}
    \vfill
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/AZ_TIL_AWS_RATIO.pdf}
        \label{fig:AZ_TIL_AWS_R}
        \vspace{-0.4cm}
        \caption{MADAR$^\theta$ Ratio}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures_TIFS/AZ_TIL_AWS_UNIFORM.pdf}
        \label{fig:AZ_TIL_AWS_U}
        \vspace{-0.4cm}
        \caption{MADAR$^\theta$ Uniform}
    \end{subfigure}

    \caption{AZ Task-IL: Comparison of the MADAR-R, MADAR-U, MADAR$^\theta$-R, and MADAR$^\theta$-U with Joint baseline.}
    \label{fig:az_TIL}
    \vspace{-0.3cm}
\end{figure}


\subsection{Analysis and Discussion}\label{diss}


Our results demonstrate that MADAR yields markedly better performances compared to previous methods for both the EMBER and AZ datasets across all CL settings. This clearly indicates that diversity-aware replay is effective in preserving the stability of a CL-based system for malware classification, while prior CL techniques largely fail to achieve acceptable performance.


\paragraphX{\bf MADAR in low-budget settings.} In Domain-IL, MADAR achieves competitive performance even with a 1K budget, surpassing prior work by over 3 percentage points in EMBER and AZ. At higher budgets, ratio-based selection (\system-R and MADAR$^{\theta}$-R) achieves near Joint baseline performance (96.4\% in EMBER and 97.3\% in AZ) while using significantly fewer resources. This demonstrates MADAR’s efficiency in leveraging limited samples to achieve robust classification.


\paragraphX{\bf MADAR is both effective and scalable.} Traditional CL methods, including ER and AGEM, experience significant performance degradation as tasks increase. In contrast, MADAR maintains high accuracy across 20 Task-IL tasks, with \system-U achieving 95.8\% in EMBER and 98.7\% in AZ at a 20K budget, nearly matching the {\em Joint} baseline.




\paragraphX{\bf Ratio vs. Uniform Budgeting.} A consistent trend across our experiments is that ratio-based selection performs best in Domain-IL, whereas uniform-based selection is superior in Class-IL and Task-IL. MADAR$^{\theta}$-U reaches 91.5\% in AZ at 20K, significantly outperforming iCaRL and TAMiL. Furthermore, in EMBER, \system-U achieves near {\em Joint} baseline performance at just a 5K budget, underscoring the effectiveness of uniform selection in class-incremental settings. Intuitively, this makes sense because ratio budgeting for binary classification in the Domain-IL setting naturally captures the contributions of each family to the overall malware distribution. Additionally, since there are many small families in the Domain-IL datasets, uniformly sampling from them consumes budget while offering little improvement in malware coverage. In contrast, our Class-IL and Task-IL experiments perform classification across families, which is better supported by Uniform budgeting to maintain class balance and ensure coverage over all families. Moreover, in most settings we can leverage efficient representations using MADAR$^\theta$ to scale the approach regardless of feature dimension without significant loss of performance.



\paragraphX{\bf GRS remains a strong baseline at high budgets.} While MADAR consistently outperforms GRS in low-resource settings, GRS performs comparably at higher budgets, particularly in Domain-IL. This suggests that diversity-aware replay is most impactful when the number of available samples per class is limited, whereas uniform selection provides sufficient representation at larger budgets.















\if 0
Our results demonstrate that MADAR yields markedly better performances compared to previous methods for both the EMBER and AZ datasets across all CL settings. This clearly indicates that diversity-aware replay is effective in preserving the stability of a CL-based system for malware classification, while prior CL techniques largely fail to achieve acceptable performance.


In the Domain-IL scenario, MADAR consistently achieves better performance than all other methods, particularly at lower budgets. For example, MADAR's uniform and ratio variants surpass other methods with $\mathbf{\overline{AP}}$ values exceeding $93.6\%$ in EMBER and $95.7\%$ in AZ at a 1K budget. As the memory budget increases, the ratio-based variants (\system-R and MADAR$^\theta$-R) excel, approaching the \textit{Joint} baselines of $96.4\%$ for EMBER and $97.3\%$ for AZ. Notably, these results are achieved with significantly fewer replay samples compared to the \textit{Joint} baseline, highlighting MADAR's efficiency in leveraging limited resources.


In the Class-IL scenario, MADAR achieves remarkable improvements over prior methods, including iCaRL and TAMiL, on both EMBER and AZ datasets. For EMBER, \system-U achieves near \textit{Joint} baseline performance with a budget as low as 5K, outperforming iCaRL  method with fewer resources. Similarly, in AZ, MADAR$^\theta$-U reaches an impressive $\mathbf{\overline{AP}}$ of $91.5\%$ at a 20K budget, significantly surpassing prior techniques. Across both datasets, uniform variants (\system-U and MADAR$^\theta$-U) consistently outperform other methods, demonstrating their effectiveness in managing resources and adapting to evolving class distributions.


In the Task-IL scenario, MADAR outperforms prior methods by a significant margin for both the EMBER and AZ datasets, confirming that diversity-aware replay is effective for this scenario. For EMBER, \system-U achieves $\mathbf{\overline{AP}}$ values of $95.8\%$ at a 20K budget, effectively matching \textit{Joint} performance with a fraction of the resources. For AZ, MADAR$^\theta$-U attains $98.7\%$ at 20K, further underscoring the efficacy of diversity-aware techniques in resource-constrained settings.These findings highlight that the MADAR framework, particularly the uniform variant, not only matches but often exceeds the effectiveness of existing techniques, confirming its robustness across various budget levels in Task-IL.


The Ratio variants worked better for Domain-IL experiments, while Uniform variants worked well in Class-IL and Task-IL. Intuitively, this makes sense because ratio budgeting for binary classification in the Domain-IL setting naturally captures the contributions of each family to the overall malware distribution. Additionally, since there are many small families in the Domain-IL datasets, uniformly sampling from them consumes budget while offering little improvement in malware coverage. In contrast, our Class-IL and Task-IL experiments perform classification across families, which is better supported by Uniform budgeting to maintain class balance and ensure coverage over all families. Moreover, in most settings we can leverage efficient representations using MADAR$^\theta$ to scale the approach regardless of feature dimension without significant loss of performance.


Our results show that GRS performs very well, in some cases closer to the performances of MADAR. Indeed, uniform random sampling should be expected to be a strong baseline, since it provides an unbiased estimate of the true underlying distribution. MADAR is particularly effective in Class-IL and Task-IL, and for lower budgets in Domain-IL, while GRS generally performs as well as MADAR in higher-budget Domain-IL settings. We hypothesize that MADAR's diversity-aware approach is more important when the number of samples per class is limited. In our Domain-IL experiments, larger budgets enable a sufficient representation of the distributions of both classes with uniform selection, making MADAR useful only at smaller budget sizes. 
\fi 
















