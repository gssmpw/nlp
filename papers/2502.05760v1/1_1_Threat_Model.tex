\section{Threat Model}
\label{sec:threat_model}


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.80\linewidth]{figures_TIFS/threat_model_TIFS.png}
    \vspace{-0.2cm}
    \caption{Retrograde Malware Attack (RMA).}
    \label{fig:CFthreatModel}
    \vspace{-0.4cm}
\end{figure}


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.90\linewidth]{figures_TIFS/LightGBM-None-TIFS.pdf}
    \vspace{-0.2cm}
    \caption{Frequencies of forgotten goodware, top malware families, and malware with unlabeled families (i.e., \texttt{others\_family}) across incremental monthly learning episodes for EMBER dataset.}
    \label{fig:forgotten_frequencies}
    \vspace{-0.4cm}
\end{figure}


The \textit{Retrograde Malware Attack (RMA)} describes an attack scenario where adversaries can exploit catastrophic forgetting in machine learning-based malware detection systems. RMA occurs when a system, updated incrementally with only {\em new samples}, loses the ability to recognize older malware, allowing attackers to reintroduce legacy or slightly modified variants that evade detection.

As shown in Figure~\ref{fig:CFthreatModel}, the attack can be realized in the deployment phase of the detection system:

\begin{itemize}
    \item \textbf{Initial Training and Deployment ({\Large \textcircled{\normalsize 1}}):} 
    The system is trained on an initial dataset of malware and benign software and deployed for classification.


    \item \textbf{Incremental Updates and Forgetting ({\Large \textcircled{\normalsize 2}}):} 
    As new samples emerge, the model is retrained with only the latest data. This process leads to catastrophic forgetting, where older malware signatures are no longer retained, reducing detection accuracy for previously known threats. In addition, false positives increase as the model struggles to differentiate between benign and malicious samples, leading to the misclassification of legitimate software. Figure~\ref{fig:forgotten_frequencies} illustrates the frequencies of forgotten goodware, top malware families, and malware with unlabeled families (i.e., \texttt{others\_family}) over incremental monthly learning episodes for EMBER dataset~\cite{ember}, demonstrating the extent of catastrophic forgetting in the model.




    \item \textbf{Retrograde Malware Attack (RMA) ({\Large \textcircled{\normalsize 3}}):} 
    Attackers exploit this limitation by \textit{reintroducing forgotten malware or slightly modified versions}, bypassing detection as the model has lost prior knowledge. The success of this attack increases with each incremental update if no mechanisms are in place to retain past information.
\end{itemize}

This attack presents a challenge for CL-based malware detection as maintaining detection accuracy for both new and previously known threats is critical. We address this by proposing \system~a replay-based CL framework that mitigates catastrophic forgetting and improves robustness against RMA.




% A schematic representation of the threat model is shown in Figure~\ref{fig:CFthreatModel}, illustrating how an attacker can reuse legacy malware to exploit vulnerabilities in machine learning (ML) systems. In the initial phase, represented by {\Large \textcircled{\normalsize 1}} in Figure~\ref{fig:CFthreatModel}, the system is trained on existing data from the repository and deployed. As new samples become available, the system is retrained {only} with the new data, and the new data is incorporated into the repository, as indicated by {\Large \textcircled{\normalsize 2}}.



% Over time, the system experiences catastrophic forgetting (CF), reducing its ability to detect earlier threats. This creates an opportunity for attackers to reuse older malware or malicious code that can evade detection by the updated system. The likelihood of such evasion increases as the gap between the original training and the number of malware grows. Addressing this issue is essential for developing systems that can consistently detect both new and legacy malware.



% A schematic representation of the threat model, where an attacker reuses legacy malwares to exploit vulnerabilities in machine learning (ML) systems, is shown in Figure~\ref{fig:CFthreatModel}. {\Large \textcircled{\normalsize 1}} in Figure~\ref{fig:CFthreatModel} represents the initial phase, where the system is trained on available data from the repository and deployed. When new samples are observed, the system is retrained with these samples, as depicted by {\Large \textcircled{\normalsize 2}}, and the new samples are added to the data repository. However, due to catastrophic forgetting (CF), the system's ability to detect earlier threats diminishes over time.


%This limitation allows attackers to reuse older malware and malicious code snippets, which can bypass the updated system. The likelihood of successful evasion increases as the temporal gap between the original training and the attack widens. Therefore, addressing this challenge is critical to developing defense systems that maintain consistent detection capabilities for both recent and legacy malware.


