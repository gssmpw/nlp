\section{Related Work}
\subsection{Image \& Video Style Transfer}

NST research has progressed from online image-optimization techniques \cite{gatys2016image}, to offline model-optimization methods capable of reproducing one style per trained network \cite{johnson2016perceptual,li2016precomputed,ulyanov2016texture,ulyanov2017improved}, and arbitrary-style-per-model approaches \cite{huang2017arbitrary} that can reproduce any given referenced style image on an input photograph \cite{ghiasi2017exploring,gu2018arbitrary,huo2021manifold,shen2018neural,svoboda2020two,xu2023learning}. Early work on arbitrary style transfer, \textit{AdaIN} \cite{huang2017arbitrary}, used an adaptive instance normalization layer that aligns the mean and variance of the content features with the respective mean and variance of style features. Other work suggested patch-based techniques \cite{chen2016fast,sheng2018avatar}, while neural flows \cite{an2021artflow} and vector quantization \cite{huang2023quantart} have also been exploited for arbitrary stylisation. Recently, the success of attention mechanisms \cite{vaswani2017attention,dosovitskiy2020image} in computer vision has resulted in multiple attention-based methods \cite{park2019arbitrary,liu2021adaattn,deng2022stytr2,luo2022consistent,ma2023rast,hong2023aespa,zhu2023all}, as well as diffusion model-based methods \cite{chung2024style,hertz2024style} for artistic style transfer. Among these attention-based approaches, \textit{RAST} \cite{ma2023rast}, a system inspired by image restoration shows enhanced structure preservation, a desirable quality in a game setting. Our approach utilises \textit{RAST} (which uses \textit{SANet} \cite{park2019arbitrary} as a backbone) in a distillation framework that is also based on style-attentional networks (\textit{SANet}).

% \subsection{Video Style Transfer}

Temporal incoherence is the main challenge that arises when stylising videos compared to images. Methods have resorted to optic flow data to improve temporal stability \cite{ruder2016artistic,gao2018reconet}. Multiple-style-per-network models \cite{gao2020fast} and arbitrary-style-per-network models \cite{deng2021arbitrary,lu2022universal} have been proposed, while depth-aware and structure-preserving video style transfer \cite{cheng2019structure,liu2021structure,ioannou2023depth} attempts to retain depth and global structure of the stylised video frames. Image style transfer approaches have been extended to work for videos with additional temporal loss training \cite{li2019learning,liu2021adaattn}, and unified frameworks for joint image and video style transfer techniques have been developed \cite{gu2023two,zhang2023unified}. Diffusion-based methods for stylised video generation have also emerged \cite{ku2024anyv2v}.


\subsection{Style Transfer for 3D Computer Games}

Whilst image and video NST methods can be applied at the end of the rendering pipeline to achieve real-time computer game stylisation, this is essentially a post-processing effect that interprets the rendered frames as single images and does not prevent undesired artifacts and flickering issues. Multi-style artistic style transfer for games has been shown in work by Unity \cite{deliot_guinier_vanhoey_2020} -- this utilises the method of Ghiasi~\etal \shortcite{ghiasi2017exploring} to stylise each intercepted final rendered image. Any G-buffer or 3D data is ignored while the produced stylisations are inconsistent and the post-process effects are diminished, as the stylisation network is used as a final `filter'. Other approaches have demonstrated improved stylisation quality when G-buffer data is taken into account during training \cite{richter2022enhancing,Mittermueller2022estgan}. Style transfer specifically tailored for computer games has only been recently proposed \cite{ioannou2023games,ioannou2024towards}. Here, NST is injected into the rendering pipeline before the post-process stage but is only capable of reproducing one style image per trained network. Yet, arbitrary style transfer could offer a significant advantage to developers and artists, as well as enable users to upload any artwork of their choice to stylise the game scenes.


\subsection{Knowledge Distillation}
Pioneered by Hinton~\etal \shortcite{hinton2015distilling}, knowledge distillation has been a widely adopted technique for training compressed models. This aims to create smaller and faster models that retain quality and performance. Recently, methods have leveraged this technique for the task of style transfer, demonstrating improved performance \cite{wang2020collaborative,chen2020optical,chen2023kbstyle}. Wang~\etal \shortcite{chen2023collaborative} show that training a smaller encoder to replace the large \textit{VGG-19} \cite{simonyan2015deep} that is typically utilised in encoder-decoder-based neural style transfer results in ultra-resolution outputs that were hard to achieve before due to memory constraints. High-quality arbitrary style transfer for images is also achieved by designing a network composed of a content encoder, a style encoder and a decoder based on CNNs, and employing symmetric knowledge distillation \cite{chen2023kbstyle}. The method by Chen~\etal \shortcite{chen2020optical} -- also based on a simple CNN architecture -- achieves fast video style transfer without relying on optic flow information during inference, but is only capable of reproducing one style per trained network.