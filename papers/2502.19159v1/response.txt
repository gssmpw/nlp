\section{Related Work}
\subsection{Large Language Models}
Although Transformer-based large language models (LLMs) have shown excellent performance in a variety of natural language processing tasks, their high computational and memory overhead remain significant challenges for practical deployment **Vaswani et al., "Attention Is All You Need"**. Transformer architecture consists of multiple stacked layers, including attention and MLP layers **Khanuja et al., "MLP Layers for Efficient Transformers"**. The attention layer, which computes contextual relationships between tokens, suffers from quadratic complexity, leading to exponential growth in computation as sequence length increases. Similarly, the MLP layer, responsible for transforming input tokens through up-projection and down-projection, significantly contributes to the model's parameter size **Khanuja et al., "MLP Layers for Efficient Transformers"**. While these layers are essential for the model's expressiveness, they also impose considerable computational and memory costs. Consequently, reducing these overheads while preserving model performance has become a critical research challenge.


% \subsection{Transformer layer correlation measure}
% Recent research has shown that not all layers and blocks within a Transformer model are equally important; some layers exhibit significant redundancy and may not fully utilize their parameter capacity **Bai et al., "Layer Normalization"**. This insight has led to a re-examination of the internal structure of Transformer models and sparked quantitative studies on inter-layer redundancy. For example, **Ye et al., "Block Influence (BI) metric"**,  proposes the Block Influence (BI) metric, which quantifies the impact of each layer on model performance by assessing its contribution to changes in hidden states. **Raghu et al., "Centered Kernel Alignment (CKA)"** uses the Centered Kernel Alignment (CKA) metric to analyze the representation similarity between Vision Transformer layers. The study found that in Vision Transformer (ViT), layers 3 to 10 are highly correlated, with CKA values reaching up to 0.97, indicating substantial redundancy. Furthermore, **Bai et al., "Layer-wise Importance Ranking"** uses gradient information and inter-layer correlation to rank layer importance, further verifying the universality of intermediate layer redundancy.

\subsection{Pruning Method on LLMs}

Large language models' multi-layer Transformer architecture often contains substantial redundancy, motivating research on width-wise and depth-wise pruning to reduce this redundancy and improve model efficiency.

\textbf{Width-wise pruning} reduces the network width by pruning coupled structures. For example, **Fan et al., "Pruning and Attention Head Sharing"** and  **Fan et al., "Attention Head Sharing for Efficient Transformers"** introduced pruning and attention head sharing techniques to reduce redundant attention heads, thereby decreasing both computational complexity and parameter requirements.  **Hou et al., "Feedforward Network Optimization"** and  **Hou et al., "Memory-Efficient Feedforward Networks"** optimized the feedforward network by reducing the dimension of the FFN hidden layer, thereby reducing the memory footprint and computational complexity. More complex hybrid optimization methods have also been explored  **Li et al., "Hybrid Width-Depth Pruning"**.

\textbf{Depth-wise pruning} directly removes the entire least important layer and can significantly accelerate inference. Shortened-LLM **Li et al., "Taylor+ Indicator for Layer Importance"** selected Taylor+ and PPL indicators as the importance measure of the Transformer layer, and directly deleted the unimportant Transformer layer to reduce the consumption of computing resources and improve the inference speed. The layer-skipping strategy  **Zhou et al., "Layer-Skipping Strategy for Efficient Inference"** further reduces computational burden and boosts inference efficiency by dynamically selecting which layers to skip during execution. Additionally,  **Li et al., "Depth Pruning Methods for Transformers"** investigated depth pruning methods, which reduce the model's depth by eliminating redundant layers, optimizing both computational overhead and model performance while retaining essential layers.