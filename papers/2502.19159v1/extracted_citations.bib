@article{dai2020funnel,
  title={Funnel-transformer: Filtering out sequential redundancy for efficient language processing},
  author={Dai, Zihang and Lai, Guokun and Yang, Yiming and Le, Quoc},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={4271--4282},
  year={2020}
}

@article{dalvi2020analyzing,
  title={Analyzing redundancy in pretrained transformer models},
  author={Dalvi, Fahim and Sajjad, Hassan and Durrani, Nadir and Belinkov, Yonatan},
  journal={arXiv preprint arXiv:2004.04010},
  year={2020}
}

@article{del2023skipdecode,
  title={Skipdecode: Autoregressive skip decoding with batching and caching for efficient llm inference},
  author={Del Corro, Luciano and Del Giorno, Allie and Agarwal, Sahaj and Yu, Bin and Awadallah, Ahmed and Mukherjee, Subhabrata},
  journal={arXiv preprint arXiv:2307.02628},
  year={2023}
}

@article{kim2024shortened,
  title={Shortened llama: A simple depth pruning for large language models},
  author={Kim, Bo-Kyeong and Kim, Geonmin and Kim, Tae-Ho and Castells, Thibault and Choi, Shinkook and Shin, Junho and Song, Hyoung-Kyu},
  journal={arXiv preprint arXiv:2402.02834},
  volume={11},
  year={2024}
}

@article{kurtic2024ziplm,
  title={Ziplm: Inference-aware structured pruning of language models},
  author={Kurti{\'c}, Eldar and Frantar, Elias and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{kwon2022fast,
  title={A fast post-training pruning framework for transformers},
  author={Kwon, Woosuk and Kim, Sehoon and Mahoney, Michael W and Hassoun, Joseph and Keutzer, Kurt and Gholami, Amir},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24101--24116},
  year={2022}
}

@article{lagunas2021block,
  title={Block pruning for faster transformers},
  author={Lagunas, Fran{\c{c}}ois and Charlaix, Ella and Sanh, Victor and Rush, Alexander M},
  journal={arXiv preprint arXiv:2109.04838},
  year={2021}
}

@article{li2022optimizing,
  title={Optimizing the Deep Neural Networks by Layer-Wise Refined Pruning and the Acceleration on FPGA},
  author={Li, Hengyi and Yue, Xuebin and Wang, Zhichen and Chai, Zhilei and Wang, Wenwen and Tomiyama, Hiroyuki and Meng, Lin},
  journal={Computational Intelligence and Neuroscience},
  volume={2022},
  number={1},
  pages={8039281},
  year={2022},
  publisher={Wiley Online Library}
}

@article{lin2024awq,
  title={AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={87--100},
  year={2024}
}

@article{men2024shortgpt,
  title={Shortgpt: Layers in large language models are more redundant than you expect},
  author={Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
  journal={arXiv preprint arXiv:2403.03853},
  year={2024}
}

@article{michel2019sixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{nova2023gradient,
  title={Gradient-free structured pruning with unlabeled data},
  author={Nova, Azade and Dai, Hanjun and Schuurmans, Dale},
  booktitle={International Conference on Machine Learning},
  pages={26326--26341},
  year={2023},
  organization={PMLR}
}

@article{raghu2021vision,
  title={Do vision transformers see like convolutional neural networks?},
  author={Raghu, Maithra and Unterthiner, Thomas and Kornblith, Simon and Zhang, Chiyuan and Dosovitskiy, Alexey},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={12116--12128},
  year={2021}
}

@article{raposo2024mixture,
  title={Mixture-of-Depths: Dynamically allocating compute in transformer-based language models},
  author={Raposo, David and Ritter, Sam and Richards, Blake and Lillicrap, Timothy and Humphreys, Peter Conway and Santoro, Adam},
  journal={arXiv preprint arXiv:2404.02258},
  year={2024}
}

@article{santacroce2023matters,
  title={What matters in the structured pruning of generative language models?},
  author={Santacroce, Michael and Wen, Zixin and Shen, Yelong and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2302.03773},
  year={2023}
}

@article{schuster2022confident,
  title={Confident adaptive language modeling},
  author={Schuster, Tal and Fisch, Adam and Gupta, Jai and Dehghani, Mostafa and Bahri, Dara and Tran, Vinh and Tay, Yi and Metzler, Donald},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17456--17472},
  year={2022}
}

@article{song2024sleb,
  title={SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks},
  author={Song, Jiwon and Oh, Kyungseok and Kim, Taesu and Kim, Hyungjun and Kim, Yulhwa and Kim, Jae-Joon},
  journal={arXiv preprint arXiv:2402.09025},
  year={2024}
}

@article{sun2023simple,
  title={A simple and effective pruning approach for large language models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}

@article{sun2024transformer,
  title={Transformer layers as painters},
  author={Sun, Qi and Pickett, Marc and Nain, Aakash Kumar and Jones, Llion},
  journal={arXiv preprint arXiv:2407.09298},
  year={2024}
}

@article{tang2024rethinking,
  title={Rethinking optimization and architecture for tiny language models},
  author={Tang, Yehui and Liu, Fangcheng and Ni, Yunsheng and Tian, Yuchuan and Bai, Zheyuan and Hu, Yi-Qi and Liu, Sichao and Jui, Shangling and Han, Kai and Wang, Yunhe},
  journal={arXiv preprint arXiv:2402.02791},
  year={2024}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{voita2019analyzing,
  title={Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  journal={arXiv preprint arXiv:1905.09418},
  year={2019}
}

@article{zeng2024kan,
  title={KAN versus MLP on Irregular or Noisy Functions},
  author={Zeng, Chen and Wang, Jiahui and Shen, Haoran and Wang, Qiao},
  journal={arXiv preprint arXiv:2408.07906},
  year={2024}
}

