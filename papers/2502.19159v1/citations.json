[
  {
    "index": 0,
    "papers": [
      {
        "key": "sun2023simple",
        "author": "Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico",
        "title": "A simple and effective pruning approach for large language models"
      },
      {
        "key": "lin2024awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song",
        "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, A",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zeng2024kan",
        "author": "Zeng, Chen and Wang, Jiahui and Shen, Haoran and Wang, Qiao",
        "title": "KAN versus MLP on Irregular or Noisy Functions"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "dai2020funnel",
        "author": "Dai, Zihang and Lai, Guokun and Yang, Yiming and Le, Quoc",
        "title": "Funnel-transformer: Filtering out sequential redundancy for efficient language processing"
      },
      {
        "key": "dalvi2020analyzing",
        "author": "Dalvi, Fahim and Sajjad, Hassan and Durrani, Nadir and Belinkov, Yonatan",
        "title": "Analyzing redundancy in pretrained transformer models"
      },
      {
        "key": "sun2024transformer",
        "author": "Sun, Qi and Pickett, Marc and Nain, Aakash Kumar and Jones, Llion",
        "title": "Transformer layers as painters"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "men2024shortgpt",
        "author": "Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng",
        "title": "Shortgpt: Layers in large language models are more redundant than you expect"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "raghu2021vision",
        "author": "Raghu, Maithra and Unterthiner, Thomas and Kornblith, Simon and Zhang, Chiyuan and Dosovitskiy, Alexey",
        "title": "Do vision transformers see like convolutional neural networks?"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "li2022optimizing",
        "author": "Li, Hengyi and Yue, Xuebin and Wang, Zhichen and Chai, Zhilei and Wang, Wenwen and Tomiyama, Hiroyuki and Meng, Lin",
        "title": "Optimizing the Deep Neural Networks by Layer-Wise Refined Pruning and the Acceleration on FPGA"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "voita2019analyzing",
        "author": "Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan",
        "title": "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "michel2019sixteen",
        "author": "Michel, Paul and Levy, Omer and Neubig, Graham",
        "title": "Are sixteen heads really better than one?"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "nova2023gradient",
        "author": "Nova, Azade and Dai, Hanjun and Schuurmans, Dale",
        "title": "Gradient-free structured pruning with unlabeled data"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "santacroce2023matters",
        "author": "Santacroce, Michael and Wen, Zixin and Shen, Yelong and Li, Yuanzhi",
        "title": "What matters in the structured pruning of generative language models?"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "lagunas2021block",
        "author": "Lagunas, Fran{\\c{c}}ois and Charlaix, Ella and Sanh, Victor and Rush, Alexander M",
        "title": "Block pruning for faster transformers"
      },
      {
        "key": "kwon2022fast",
        "author": "Kwon, Woosuk and Kim, Sehoon and Mahoney, Michael W and Hassoun, Joseph and Keutzer, Kurt and Gholami, Amir",
        "title": "A fast post-training pruning framework for transformers"
      },
      {
        "key": "kurtic2024ziplm",
        "author": "Kurti{\\'c}, Eldar and Frantar, Elias and Alistarh, Dan",
        "title": "Ziplm: Inference-aware structured pruning of language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "kim2024shortened",
        "author": "Kim, Bo-Kyeong and Kim, Geonmin and Kim, Tae-Ho and Castells, Thibault and Choi, Shinkook and Shin, Junho and Song, Hyoung-Kyu",
        "title": "Shortened llama: A simple depth pruning for large language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "schuster2022confident",
        "author": "Schuster, Tal and Fisch, Adam and Gupta, Jai and Dehghani, Mostafa and Bahri, Dara and Tran, Vinh and Tay, Yi and Metzler, Donald",
        "title": "Confident adaptive language modeling"
      },
      {
        "key": "del2023skipdecode",
        "author": "Del Corro, Luciano and Del Giorno, Allie and Agarwal, Sahaj and Yu, Bin and Awadallah, Ahmed and Mukherjee, Subhabrata",
        "title": "Skipdecode: Autoregressive skip decoding with batching and caching for efficient llm inference"
      },
      {
        "key": "raposo2024mixture",
        "author": "Raposo, David and Ritter, Sam and Richards, Blake and Lillicrap, Timothy and Humphreys, Peter Conway and Santoro, Adam",
        "title": "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "song2024sleb",
        "author": "Song, Jiwon and Oh, Kyungseok and Kim, Taesu and Kim, Hyungjun and Kim, Yulhwa and Kim, Jae-Joon",
        "title": "SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks"
      },
      {
        "key": "tang2024rethinking",
        "author": "Tang, Yehui and Liu, Fangcheng and Ni, Yunsheng and Tian, Yuchuan and Bai, Zheyuan and Hu, Yi-Qi and Liu, Sichao and Jui, Shangling and Han, Kai and Wang, Yunhe",
        "title": "Rethinking optimization and architecture for tiny language models"
      }
    ]
  }
]