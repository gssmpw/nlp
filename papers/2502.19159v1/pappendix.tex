%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{lmodern}


% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Appendix: A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs}


% Single author syntax
\author{
    Author Name
    \affiliations
    Affiliation
    \emails
    email@example.com
}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
\iffalse
\author{
First Author$^1$
\and
Second Author$^2$\and
Third Author$^{2,3}$\And
Fourth Author$^4$\\
\affiliations
$^1$First Affiliation\\
$^2$Second Affiliation\\
$^3$Third Affiliation\\
$^4$Fourth Affiliation\\
\emails
\{first, second\}@example.com,
third@other.example.com,
fourth@example.com
}
\fi

\begin{document}

\maketitle

% \appendix
\section{Experimental Setup}

\subsection{Baseline Methods}

We primarily consider the width-wise pruning methods and depth-wise pruning methods as our baseline methods in our analysis. The specific information of baseline methods are described below, where we use their official code for implementation. To ensure a fair comparison, we employ the same calibration dataset across all methods. 

\textbf{Width-wise method.} The width-wise pruning methods include Wanda-sp, FLAP and LLM-Pruner.  

Wanda-sp is a variant of Wanda \cite{sun2024transformer}. The original metric of Wanda was based on the product of weight magnitudes and input activation norms, while Wanda-sp presented in \cite{an2024fluctuation} extends this in a struced way while using common dimensions among different modules. 

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.25}
\resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{c|cccc}
\hline \hline
Model & Pruned Ratio & Metrics & Structure & Params \\ \hline
\multirow{2}{*}{LLaMA2-7B / Vicuna-7B} & 0.2 & WIFV & AL-AM & 5470556160 \\
 & 0.35 & WIFV & AL-AM & 4485812224 \\ \hline
\multirow{2}{*}{LLaMA2-13B / Vicuna-13B} & 0.2 & WIFV & AL-AM & 10479211520 \\
 & 0.35 & WIFV & AL-AM & 8575800320 \\ \hline \hline
\end{tabular}}
\caption{Hyperparameter settings for Wanda-sp.}
\label{wanda-sp}
\end{table}


FLAP \cite{an2024fluctuation} is a new LLM retraining-free structured pruning framework that determines the recoverability of the feature map after removing weight columns based on the fluctuation pruning index. It adaptively determines the compressed model structure using normalized importance scores and adds additional bias terms to the pruned feature maps to restore performance.

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.25}
\resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{c|cccc}
\hline \hline
Model & Pruned Ratio & Metrics & Structure & Params \\ \hline
\multirow{2}{*}{LLaMA2-7B / Vicuna-7B} & 0.2 & WIFV & AL-AM & 5444964352 \\
 & 0.35 & WIFV & AL-AM & 4473626624 \\ \hline
\multirow{2}{*}{LLaMA2-13B / Vicuna-13B} & 0.2 & WIFV & AL-AM & 10481551360 \\
 & 0.35 & WIFV & AL-AM & 8578908160 \\ \hline
\multirow{2}{*}{Meta-LLaMA3} & 0.2 & WIFV & AL-AM & 6721589248 \\
 & 0.35 & WIFV & AL-AM & 5631029248 \\ \hline \hline
\end{tabular}}
\caption{Hyperparameter settings for FLAP.}
\label{flap}
\end{table}


LLM-Pruner \cite{ma2023llm} utilizes a Taylor-based importance metric to identify and remove attention heads from MHA and intermediate neurons from FFN. The pruning process is conducted locally, selecting removable groups within each module while ensuring that the dimensions across the blocks remain consistent. Following the original approach, we preserve the first and last few blocks without pruning. Both their pruned models and ours are retrained using Low-Rank Adaptation (LoRA).

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.25}
\resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{c|cccc}
\hline \hline
Model & Pruned Ratio & \multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}Block\_mlp\_layer\_start /\\ BloCK\_attention\_layer\_start\end{tabular}} & \multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}Block\_mlp\_layer\_end /\\ Block\_attention\_layer\_start\end{tabular}} & Params \\ \hline
\multirow{2}{*}{LLaMA2-7B/Vicuna-7B} & 0.24 & 4 & 30 & 5512646656 \\
 & 0.43 & 4 & 30 & 4517122048 \\ \hline
\multirow{2}{*}{LLaMA2-13B/Vicuna-13B} & 0.24 & 4 & 38 & 10480911360 \\
 & 0.42 & 4 & 38 & 8557153280 \\ \hline
\multirow{2}{*}{LLaMA3-8B} & 0.24 & 4 & 30 & 6794588160 \\
 & 0.43 & 4 & 30 & 5651673088 \\ \hline \hline
\end{tabular}}
\caption{Hyperparameter settings for LLM-Pruner.}
\label{llm-pruner}
\end{table}



\textbf{Depth-wise method.} The depth-wise pruning methods use the Transformer module in LLM as the pruning unit. SLEB \cite{song2024sleb} uses a logit-based approach to find unnecessary blocks and updates the importance score after removing each block. SLEB pursues a no-retraining setting, but it cannot maintain sufficient performance as the pruning rate increases. Shortened-LLM \cite{kim2024shortened} uses the PPL standard to determine the importance of the transformer layer, and deletes unimportant transformer layers after sorting. Shortened-LLM method uses lora to retrain to restore the performance of the model.

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.25}
\resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{c|ccccc}
\hline \hline
Model & Pruned Ratio & Block & Head & FFN-D & Params \\ \hline
\multirow{2}{*}{LLaMA2-7B/Vicuna-7B} & 0.2 & 26 & 32 & 11008 & 5524115456 \\
 & 0.35 & 21 & 32 & 11008 & 4512198656 \\ \hline
\multirow{2}{*}{LLaMA2-13B/Vicuna-13B} & 0.2 & 32 & 40 & 13824 & 10478228480 \\
 & 0.35 & 26 & 40 & 13824 & 8575001600 \\ \hline
\multirow{2}{*}{LLaMA3-8B} & 0.2 & 26 & 32 & 14336 & 6721589248 \\
 & 0.35 & 21 & 32 & 14336 & 5631029248 \\ \hline \hline
\end{tabular}}
\caption{Hyperparameter settings for depth-wise methods.}
\label{depth-wise}
\end{table}


\subsection{Selected Transformer Layers}
We summarize the number and indices of transformer blocks selected for removal using our method in Tab.\ref{selected}. The specified sets of Transformer layers are fused into a single layer, represented by the index of the first layer in the set (e.g., [25, 26, 27, 28, 29, 30] are fused into layer 25).


\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.3}
\resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{c|c|c|c}
\hline \hline
Model & Threshold & Num of layers & Merged Layers \\ \hline
\multirow{2}{*}{LLaMA2-7B} & 0.81 & 26 & {[}{[}25, 26, 27, 28, 29, 30{]}, {[}10, 11{]}{]} \\
 & 0.68 & 21 & {[}{[}22, 23, 24, 25, 26, 27, 28, 29, 30{]}, {[}14, 15{]}, {[}10, 11{]}, {[}6,   7{]}{]} \\
\multirow{2}{*}{Vicuna-7B} & 0.78 & 26 & {[}{[}25, 26, 27, 28, 29, 30{]}, {[}13, 14{]}{]} \\
 & 0.57 & 21 & {[}{[}21, 22, 23, 24, 25, 26, 27, 28, 29, 30{]}, {[}13, 14{]}, {[}10, 11{]}{]} \\
\multirow{2}{*}{LLaMA3-8B} & 0.72 & 26 & {[}{[}28, 29, 30{]}, {[}25, 26, 27{]}, {[}22, 23{]}, {[}14, 15{]}{]} \\
 & 0.51 & 21 & {[}{[}25, 26, 27, 28, 29, 30{]}, {[}21, 22{]}, {[}19, 20{]}, {[}14, 15, 16{]},   {[}12, 13{]}, {[}10, 11{]}{]} \\
\multirow{2}{*}{LLaMA2-13B} & 0.8 & 32 & {[}{[}30, 31, 32, 33, 34, 35, 36, 37, 38{]}{]} \\
 & 0.72 & 26 & {[}{[}26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38{]}, {[}16,   17{]}, {[}12, 13{]}{]} \\
\multirow{2}{*}{Vicuna-13B} & 0.76 & 32 & {[}{[}33, 34, 35, 36, 37, 38{]}, {[}31, 32{]}, {[}29, 30{]}, {[}15, 16{]}{]} \\
 & 0.65 & 26 & {[}{[}31, 32, 33, 34, 35, 36, 37, 38{]}, {[}29, 30{]}, {[}27, 28{]}, {[}9, 10,   11, 12, 13, 14{]}{]} \\ \hline \hline
\end{tabular}}
\caption{Corresponding layer indices of the pruned models under different threshold setting of our depth-wise pruning method used in the main results}
\label{selected}
\end{table}

\subsection{LoRA Retraining}
We apply a LoRA adapter \cite{hulora} to every projection weight matrix by following \cite{ma2023llm}. We employ a LoRA rank of 8, a learning rate of 0.0001, and a batch size of 64 over 2 epochs. The retraining costs are notably low, with the entire process being executed on a single NVIDIA A100 (80GB VRAM) GPU. For example, retraining a 20\%-pruned model from 7B parameters takes about 2 hours and utilizes 22GB GPU memory.

\section{Supplementary Experiment Results}
\subsection{Zero-shot performance in larger scale}
Tab.\ref{large-scale} presents the zero-shot performance of various downstream tasks with the proposed method applied to the LLaMA2-13B model and Vicuna-13B model. Our method shows superior pruning capabilities.


\begin{table*}[tb]
\centering
\renewcommand{\arraystretch}{1.25}
\resizebox{1.9\columnwidth}{!}{%
\begin{tabular}{ccc|llllllll}
\hline \hline
\multicolumn{3}{c|}{\#Param \& Method} & \multicolumn{1}{c}{BoolQ} & \multicolumn{1}{c}{PIQA} & \multicolumn{1}{c}{HellaSwag} & \multicolumn{1}{c}{WinoGrande} & \multicolumn{1}{c}{ARC-easy} & \multicolumn{1}{c}{ARC-challenge} & \multicolumn{1}{c}{OpenbookQA} & \multicolumn{1}{c}{AVE} \\ \hline \hline
\multicolumn{3}{c|}{LLaMA-2-13B(Original)} & 80.550 & 79.053 & 79.367 & 72.139 & 79.377 & 49.147 & 45.200 & 69.262 \\ \hline 
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Wanda-sp & 69.630 & 77.480 & 74.750 & 67.010 & 73.480 & 44.110 & 44.000 & 64.351 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & FLAP & 72.780 & 74.650 & 69.070 & 68.350 & 70.830 & 40.610 & 40.000 & 62.327 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\multirow{-3}{*}{width}} & LLM-Pruner & 71.315 & 79.162 & 74.836 & 67.324 & 73.485 & 43.771 & 41.600 & 64.499 \\ \cline{2-11} 
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & SLEB & 63.211 & 76.061 & 70.116 & 65.430 & 70.749 & 39.932 & 39.200 & 60.671 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Shortened-LLM & 68.318 & 76.279 & 75.204 & 71.113 & 74.790 & 46.672 & 42.400 & 64.968 \\
\multicolumn{1}{c|}{\multirow{-6}{*}{20\%Pruned}} & \multicolumn{1}{c|}{\multirow{-3}{*}{depth}} & Ours & 64.006 & 77.421 & 76.369 & 71.665 & 76.557 & 48.549 & 43.800 & \textbf{65.481} \\ \hline
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Wanda-sp & 59.020 & 55.110 & 33.580 & 52.800 & 29.840 & 24.910 & 28.600 & 40.551 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & FLAP & 71.250 & 69.590 & 61.680 & 64.400 & 59.050 & 34.130 & 36.000 & 56.586 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\multirow{-3}{*}{width}} & LLM-Pruner & 66.086 & 76.061 & 67.785 & 59.195 & 67.382 & 39.334 & 41.800 & 59.663 \\ \cline{2-11} 
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & SLEB & 62.385 & 70.620 & 58.415 & 55.643 & 62.500 & 36.689 & 33.800 & 54.293 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Shortened-LLM & 63.333 & 72.307 & 67.128 & 63.694 & 66.667 & 39.761 & 37.000 & 58.556 \\
\multicolumn{1}{c|}{\multirow{-6}{*}{35\%Pruned}} & \multicolumn{1}{c|}{\multirow{-3}{*}{depth}} & Ours & 59.939 & 73.286 & 68.751 & 65.983 & 68.981 & 41.724 & 39.000 & \textbf{59.666} \\ \hline \hline
\multicolumn{3}{c|}{Vicuna-13B-v1.3(Original)} & 82.813 & 78.346 & 77.017 & 71.113 & 75.547 & 47.611 & 45.400 & 68.264 \\ \hline
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Wanda-sp & 77.090 & 77.090 & 74.420 & 67.960 & 67.800 & 42.320 & 42.800 & 64.211 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & FLAP & 81.100 & 76.770 & 73.720 & 68.350 & 71.510 & 42.490 & 41.000 & 64.991 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\multirow{-3}{*}{width}} & LLM-Pruner & 74.526 & 78.346 & 72.426 & 69.219 & 69.739 & 40.529 & 43.200 & 63.998 \\ \cline{2-11} 
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & SLEB & 62.385 & 70.620 & 58.415 & 55.643 & 62.500 & 36.689 & 33.800 & 54.293 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Shortened-LLM & 75.535 & 77.476 & 73.571 & 68.272 & 72.180 & 44.198 & 43.200 & 64.919 \\
\multicolumn{1}{c|}{\multirow{-6}{*}{20\%Pruned}} & \multicolumn{1}{c|}{\multirow{-3}{*}{depth}} & Ours & 81.040 & 76.442 & 74.846 & 70.324 & 71.801 & 44.625 & 41.800 & \textbf{65.840} \\ \hline
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Wanda-sp & 61.650 & 71.220 & 63.960 & 61.400 & 57.490 & 35.320 & 37.000 & 55.434 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & FLAP & 75.170 & 73.990 & 65.540 & 67.560 & 61.320 & 36.770 & 37.400 & 59.679 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\multirow{-3}{*}{width}} & LLM-Pruner & 70.581 & 76.659 & 67.317 & 65.272 & 63.258 & 35.154 & 40.800 & 59.863 \\ \cline{2-11} 
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & SLEB & 37.829 & 51.034 & 25.503 & 50.908 & 26.221 & 27.218 & 27.400 & 35.159 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Shortened-LLM & 68.532 & 74.102 & 66.421 & 64.009 & 67.593 & 41.126 & 38.600 & \textbf{60.055} \\
\multicolumn{1}{c|}{\multirow{-6}{*}{35\%Pruned}} & \multicolumn{1}{c|}{\multirow{-3}{*}{depth}} & Ours & 69.450 & 74.918 & 67.447 & 63.378 & 66.414 & 38.311 & 37.400 & 59.617 \\ \hline \hline
\end{tabular}
}
\caption{Performance comparison of pruning methods across multiple baselines on LLaMA2-13B and Vicuna-13B models.}
\label{large-scale}
\end{table*}

\subsection{Additional Results of Inference Efficiency}
Tab.\ref{efficiency} shows the inference of our depth-wise pruning method in different LLMs (LLaMA2-7B, LLaMA2-13B, Vicuna-13B-v1.3, and LLaMA3-8B). As the pruning ratio increases, the latency of the model decreases, the throughput increases, and the GPU memory usage and number of parameters also decrease accordingly.

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.15}
\resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{c|cccc}
\hline \hline
 & Latency(sec) & Throughout(tokens/s) & GPU\_Memory & nparam \\ \hline 
LLaMA2-7B & 2.729 & 46.905 & 13020.25 & 6738415616 \\ 
20\% Pruned & 2.339 & 54.758 & 10682.45 & 5524115456 \\
35\% Pruned & 1.889 & 67.770 & 8725.9 & 4512198656 \\ \hline 
LLaMA2-13B & 3.635 & 35.210 & 25188.85 & 13015864320 \\ 
20\% Pruned & 2.908 & 44.016 & 20274.25 & 10478228480 \\
35\% Pruned & 2.380 & 53.793 & 16593.1 & 8575001600 \\ \hline 
Vicuna-7B-v1.3 & 2.865 & 44.681 & 13021.8 & 6738415616 \\
20\% Pruned & 2.370 & 54.065 & 10682.45 & 5524115456 \\
35\% Pruned & 2.000 & 64.087 & 8725.9 & 4512198656 \\ \hline 
Vicuna-13B-v1.3 & 3.593 & 35.623 & 25186.95 & 13015864320 \\ 
20\% Pruned & 2.885 & 44.386 & 20274.25 & 10478228480 \\
35\% Pruned & 2.395 & 53.461 & 16593.1 & 8575001600 \\ \hline 
LLaMA3-8B & 3.150 & 40.640 & 15364 & 8030261248 \\ 
20\% Pruned & 2.635 & 48.616 & 12862 & 6721589248 \\
35\% Pruned & 2.218 & 57.795 & 10776 & 5631029248 \\ \hline \hline
\end{tabular}}
\caption{Inference efficiency of our pruned models. (Measured with 12 input tokens, 128 output tokens, and a batch size of 1 on a NVIDIA H100 GPU.)}
\label{efficiency}
\end{table}

\subsection{Integration of Depth-wise Pruning and Width-wise Pruning}
We integrate our method with LLM-Pruner to further improve the pruning effect. In the first stage, we apply our proposed pruning method, based on the layer compression model. In the second stage, we use the LLM-Pruner method to remove unnecessary coupling structures from the model obtained in the first stage. As a result, the pruned model achieved a pruning rate of 35\% relative to the original model. Tab.\ref{integrate} shows the pruning proportions and corresponding parameter settings of depth-wise pruning and width-wise pruning on the LLaMA2-7B model using the integrated method. 

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.25}
\resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{ccc|cc|c|c}
\hline \hline
\multicolumn{3}{c|}{Depth-wise(ours)} & \multicolumn{2}{c|}{Width-wise(LLM-Pruner)} & \multirow{2}{*}{Nparam} & \multirow{2}{*}{AVE} \\Proportion & Threshold & Remove layers & Proportion & Pruner\_ratio &  &  \\ \hline
0\% &  & 0 & 100\% & 0.43 & 4532772064 & 52.366 \\
18\% & 0.96 & 2 & 82\% & 0.35 & 4502876160 & 54.372 \\
36\% & 0.86 & 4 & 64\% & 0.3 & 4501803008 & 55.753 \\
53\% & 0.81 & 6 & 47\% & 0.24 & 4486926336 & 55.188 \\
71\% & 0.77 & 8 & 29\% & 0.16 & 4476604416 & 53.967 \\
91\% & 0.7 & 10 & 9\% & 0.06 & 4530630656 & 52.780 \\
100\% & 0.68 & 11 & 0\% & & 4512198656 & 51.869 \\ \hline \hline
\end{tabular}}
\caption{Pruning proportions and corresponding parameter settings of integrated method on the LLaMA2-7B model.}
\label{integrate}
\end{table}

We also tested our integrated method on the Vicuna-13B model, and the experimental results are shown in Fig\ref{merge2}. Tab.\ref{integrate2}  shows the pruning proportions and corresponding parameter settings of depth-wise pruning and width-wise pruning.



\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.25}
\resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{ccc|cc|c|c}
\hline \hline
\multicolumn{3}{c|}{Depth-wise(ours)} & \multicolumn{2}{c|}{Width-wise(LLM-Pruner)} & \multirow{2}{*}{Nparam} & \multirow{2}{*}{AVE} \\
Proportion & Threshold & Remove layers & Proportion & Pruner\_ratio &  &  \\ \hline
0\% &  & 0 & 100\% & 0.42 & 8557153280 & 59.863 \\
14\% & 0.94 & 2 & 86\% & 0.35 & 8606530560 & 61.236 \\
28\% & 0.86 & 4 & 72\% & 0.3 & 8565928960 & 60.321 \\
43\% & 0.8 & 6 & 57\% & 0.28 & 8606039040 & 60.455 \\
57\% & 0.76 & 8 & 43\% & 0.24 & 8582722560 & 60.257 \\
71\% & 0.7 & 10 & 29\% & 0.16 & 8551490560 & 61.203 \\
100\% & 0.57 & 14 & 0\% &  & 8575001600 & 59.617 \\ \hline \hline
\end{tabular}}
\caption{Pruning proportions and corresponding parameter settings of integrated method on the Vicuna-13B model.}
\label{integrate2}
\end{table}


\begin{figure}[H]
\centering
\includegraphics[width=1.0\columnwidth]{merge.pdf}  
\caption{Performance of the integrated method on the Vicuna-13B model.}
\label{merge2}
\end{figure}



\subsection{Three different layer merging methods}
For the $l$-th layer of an LLM, we denote all its parameters as $\theta_l$. We considered three different layer merging methods to merge the parameters of the subsequent m consecutive layers $\theta_{l+1},\theta_{l+2},...,\theta_{l+m}$ into $\theta_l$ to form $ \theta_l^*$. 

\begin{itemize}
\item \textbf{Remove:} We directly ignore the parameters of the $l+1$ to $l+m$ layers, and only retain the parameters of the $l$-th layer (although in actual operation, we may still label the combined parameter set as $\theta_{l}^*$, but here $\theta_{l}^*$ actually only contains the parameters of the original $l$-th layer).
\begin{equation}
    \theta_l^* = \theta_l    
\end{equation}
\item \textbf{Average:} We calculate the average value of the parameters from layer $l$ to $l+m$, and use this average value to form a new parameter set $\theta_{l}^*$.
\begin{equation}
    \theta_l^* = \frac{\sum_{i=l}^{l+m}\theta_i}{m}    
\end{equation}
\item \textbf{Our method:} We adopt a parameter merging strategy based on inter-layer differences, adding the differences between adjacent layer and base layer parameters to gradually integrate redundant information.
\begin{equation}
    \theta_l^* = \theta_l + \sum_{i=l+1}^{l+m}(\theta_i-\theta_l)   
\end{equation}

\end{itemize}

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.2}
\resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{ccc}
\hline \hline
Threshold & Num of layers & Merged Layers \\ \hline
0.95 & 30 & {[}{[}27, 28{]}, {[}13, 14{]}{]} \\
0.9 & 30 & {[}{[}27, 28, 29{]}{]} \\
0.85 & 29 & {[}{[}29, 30{]}, {[}27, 28{]}, {[}24, 25{]}{]} \\
0.8 & 29 & {[}{[}27, 28, 29, 30{]}{]} \\
0.75 & 26 & {[}{[}26, 27, 28, 29, 30{]}, {[}24, 25{]}, {[}9, 10{]}{]} \\
0.7 & 24 & {[}{[}24, 25, 26, 27, 28, 29, 30{]}, {[}22, 23{]}, {[}9, 10{]}{]} \\
0.65 & 22 & {[}{[}22, 23, 24, 25, 26, 27, 28, 29, 30{]}, {[}17, 18{]}, {[}9, 10{]}{]} \\ \hline \hline
\end{tabular}}
\caption{The layer index corresponding to the pruned model obtained by the "delete" layer merging method under different threshold settings.}
\label{delete}
\end{table}

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.2}
\resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{ccc}
\hline
Threshold & Num of layers & Merged Layers \\ \hline \hline
0.95 & 31 & {[}{[}21, 22{]}{]} \\
0.9 & 31 & {[}{[}28, 29{]}{]} \\
0.85 & 31 & {[}{[}29, 30{]}{]} \\
0.8 & 30 & {[}{[}29, 30{]}, {[}23, 24{]}{]} \\
0.75 & 29 & {[}{[}29, 30{]}, {[}26, 27{]}, {[}22,   23{]}{]} \\
0.7 & 28 & {[}{[}29, 30{]}, {[}27, 28{]}, {[}23,   24{]}, {[}10, 11{]}{]} \\
0.65 & 27 & {[}{[}29, 30{]}, {[}27, 28{]}, {[}25,   26{]}, {[}22, 23{]}, {[}10, 11{]}{]} \\ \hline \hline
\end{tabular}}
\caption{The layer index corresponding to the pruned model obtained by the "average" layer merging method under different threshold settings.}
\label{average}
\end{table}


\subsection{Additional Results of Moderate Pruning and LoRA Retraining}
Tab.\ref{lora1}-\ref{lora5} show the zero-shot results of several pruning strategies that require retraining, including LLM-Pruner, Shortened-LLM, and our proposed method on different model.  


\begin{table*}[]
\centering
\renewcommand{\arraystretch}{1.25}
\resizebox{2.0\columnwidth}{!}{%
\begin{tabular}{cl|cccccccc}
\hline \hline
\multicolumn{2}{c|}{20\%Pruned} & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-easy & ARC-challenge & OpenbookQA & AVE \\ \hline 
\multirow{2}{*}{LLM\_Pruner} & wo\_lora & 53.761 & 76.659 & 66.132 & 61.168 & 64.857 & 37.884 & 40.200 & 57.237 \\
 & w\_lora & 63.731 & 77.476 & 67.128 & 61.878 & 65.783 & 38.481 & 40.400 & 59.268 \\
\multirow{2}{*}{Shortened-LLM} & wo\_lora & 60.489 & 73.776 & 63.364 & 57.459 & 64.015 & 33.191 & 36.200 & 55.499 \\
 & w\_lora & 61.560 & 76.061 & 67.994 & 58.800 & 68.813 & 37.884 & 38.000 & 58.445 \\
\multirow{2}{*}{Ours} & wo\_lora & 62.324 & 70.239 & 65.097 & 66.298 & 61.448 & 38.311 & 37.800 & 57.360 \\
 & w\_lora & 69.450 & 73.667 & 70.484 & 67.088 & 69.108 & 41.212 & 42.600 & 61.944 \\ \hline \hline
\multicolumn{2}{c|}{35\%Pruned} & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-easy & ARC-challenge & OpenbookQA & AVE \\ \hline 
\multirow{2}{*}{LLM\_Pruner} & wo\_lora & 50.703 & 69.695 & 51.275 & 52.960 & 49.495 & 31.399 & 36.000 & 48.790 \\
 & \multicolumn{1}{c|}{w\_lora} & 45.260 & 74.755 & 60.287 & 59.353 & 57.281 & 32.423 & 37.200 & 52.366 \\
\multirow{2}{*}{Shortened-LLM} & wo\_lora & 61.101 & 67.791 & 45.987 & 52.960 & 48.485 & 27.218 & 32.800 & 48.049 \\
 & \multicolumn{1}{c|}{w\_lora} & 62.171 & 71.926 & 54.800 & 51.776 & 59.259 & 30.887 & 35.400 & 52.317 \\
\multirow{2}{*}{Ours} & wo\_lora & 62.171 & 63.874 & 52.131 & 59.511 & 46.170 & 31.826 & 32.600 & 49.755 \\
 & \multicolumn{1}{c|}{w\_lora} & 63.119 & 65.452 & 56.503 & 58.879 & 52.020 & 31.911 & 35.200 & 51.869 \\ \hline \hline
\end{tabular}}
\caption{Performance with/without LoRA retraining on LLaMA2-7B.}
\label{lora1}
\end{table*}



\begin{table*}[]
\centering
\renewcommand{\arraystretch}{1.25}
\resizebox{2.0\columnwidth}{!}{%
\begin{tabular}{cl|cccccccc}
\hline \hline
\multicolumn{2}{c|}{20\%Pruned} & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-easy & ARC-challenge & OpenbookQA & AVE \\ \hline 
\multirow{2}{*}{LLM\_Pruner} & wo\_lora & 57.554 & 74.810 & 65.216 & 59.037 & 64.815 & 36.263 & 39.400 & 56.728 \\
 & w\_lora & 67.645 & 76.115 & 66.660 & 63.931 & 65.446 & 36.604 & 40.400 & 59.543 \\
\multirow{2}{*}{Shortened-LLM} & wo\_lora & 63.670 & 72.470 & 63.105 & 62.194 & 64.352 & 36.775 & 35.600 & 56.881 \\
 & w\_lora & 72.355 & 74.701 & 67.576 & 64.562 & 70.034 & 38.225 & 38.600 & 60.865 \\
\multirow{2}{*}{Ours} & wo\_lora & 63.394 & 72.742 & 66.152 & 66.219 & 66.919 & 39.078 & 40.600 & 59.301 \\
 & w\_lora & 77.431 & 74.755 & 69.040 & 68.745 & 69.318 & 38.908 & 40.200 & 62.628 \\ \hline \hline
\multicolumn{2}{c|}{35\%Pruned} & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-easy & ARC-challenge & OpenbookQA & AVE \\ \hline 
\multirow{2}{*}{LLM\_Pruner} & wo\_lora & 60.642 & 70.294 & 54.860 & 52.881 & 53.662 & 33.618 & 36.000 & 51.708 \\
 & \multicolumn{1}{c|}{w\_lora} & 63.976 & 73.069 & 59.560 & 58.564 & 56.524 & 32.679 & 37.800 & 54.596 \\
\multirow{2}{*}{Shortened-LLM} & wo\_lora & 60.245 & 64.527 & 43.856 & 53.986 & 47.348 & 26.792 & 29.600 & 46.622 \\
 & \multicolumn{1}{c|}{w\_lora} & 64.281 & 70.783 & 56.722 & 57.380 & 59.596 & 31.485 & 34.000 & 53.464 \\
\multirow{2}{*}{Ours} & wo\_lora & 62.202 & 66.431 & 53.834 & 61.484 & 52.946 & 33.532 & 33.400 & 51.976 \\
 & \multicolumn{1}{c|}{w\_lora} & 69.235 & 70.294 & 60.705 & 62.273 & 60.227 & 33.618 & 37.400 & 56.250 \\ \hline \hline
\end{tabular}}
\caption{Performance with/without LoRA retraining on Vicuna-7B-v1.3.}
\label{lora2}
\end{table*}


\begin{table*}[]
\centering
\renewcommand{\arraystretch}{1.25}
\resizebox{2.0\columnwidth}{!}{%
\begin{tabular}{cl|cccccccc}
\hline \hline
\multicolumn{2}{c|}{20\%Pruned} & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-easy & ARC-challenge & OpenbookQA & AVE \\ \hline 
\multirow{2}{*}{LLM\_Pruner} & wo\_lora & 56.942 & 77.040 & 67.785 & 68.666 & 68.603 & 39.078 & 40.400 & 59.788 \\
 & w\_lora & 74.037 & 79.489 & 74.268 & 70.324 & 74.285 & 46.587 & 42.600 & 65.941 \\
\multirow{2}{*}{Shortened-LLM} & wo\_lora & 45.443 & 73.232 & 60.994 & 57.853 & 65.404 & 34.044 & 36.000 & 53.282 \\
 & w\_lora & 66.208 & 78.074 & 72.695 & 62.747 & 75.295 & 44.795 & 43.400 & 63.316 \\
\multirow{2}{*}{Ours} & wo\_lora & 38.073 & 71.980 & 61.800 & 69.613 & 66.035 & 41.809 & 38.400 & 55.387 \\
 & w\_lora & 76.789 & 77.639 & 73.770 & 71.744 & 76.599 & 50.939 & 41.200 & 66.954 \\ \hline \hline
\multicolumn{2}{c|}{35\%Pruned} & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-easy & ARC-challenge & OpenbookQA & AVE \\ \hline 
\multirow{2}{*}{LLM\_Pruner} & wo\_lora & 47.829 & 69.369 & 45.150 & 53.118 & 48.485 & 27.816 & 33.200 & 46.424 \\
 & \multicolumn{1}{c|}{w\_lora} & 64.465 & 74.048 & 61.800 & 59.353 & 64.646 & 34.386 & 37.200 & 56.557 \\
\multirow{2}{*}{Shortened-LLM} & wo\_lora & 61.651 & 66.431 & 49.801 & 51.697 & 51.431 & 29.352 & 30.400 & 48.680 \\
 & \multicolumn{1}{c|}{w\_lora} & 63.180 & 72.851 & 62.985 & 58.090 & 66.877 & 37.116 & 37.000 & 56.871 \\
\multirow{2}{*}{Ours} & wo\_lora & 40.428 & 62.350 & 40.291 & 55.485 & 39.394 & 28.242 & 28.200 & 42.056 \\
 & \multicolumn{1}{c|}{w\_lora} & 67.554 & 73.830 & 61.472 & 62.747 & 64.352 & 36.007 & 37.600 & 57.652 \\ \hline \hline
\end{tabular}}
\caption{Performance with/without LoRA retraining on LLaMA3-8B.}
\label{lora3}
\end{table*}


\begin{table*}[]
\centering
\renewcommand{\arraystretch}{1.25}
\resizebox{2.0\columnwidth}{!}{%
\begin{tabular}{cl|cccccccc}
\hline \hline
\multicolumn{2}{c|}{20\%Pruned} & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-easy & ARC-challenge & OpenbookQA & AVE \\ \hline 
\multirow{2}{*}{LLM\_Pruner} & wo\_lora & 65.566 & 78.509 & 72.018 & 64.167 & 69.992 & 43.601 & 40.600 & 62.065 \\
 & w\_lora & 71.315 & 79.162 & 74.836 & 67.324 & 73.485 & 43.771 & 41.600 & 64.499 \\
\multirow{2}{*}{Shortened-LLM} & wo\_lora & 63.180 & 75.027 & 71.191 & 70.481 & 69.529 & 43.089 & 40.800 & 61.900 \\
 & w\_lora & 68.318 & 76.279 & 75.204 & 71.113 & 74.790 & 46.672 & 42.400 & 64.968 \\
\multirow{2}{*}{Ours} & wo\_lora & 38.318 & 72.361 & 67.726 & 70.797 & 64.815 & 39.676 & 42.800 & 56.642 \\
 & w\_lora & 64.006 & 77.421 & 76.369 & 71.665 & 76.557 & 48.549 & 43.800 & 65.481 \\ \hline \hline
\multicolumn{2}{c|}{35\%Pruned} & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-easy & ARC-challenge & OpenbookQA & AVE \\ \hline 
\multirow{2}{*}{LLM\_Pruner} & wo\_lora & 52.049 & 74.048 & 61.522 & 55.801 & 60.816 & 36.689 & 39.800 & 54.389 \\
 & \multicolumn{1}{c|}{w\_lora} & 66.086 & 76.061 & 67.785 & 59.195 & 67.382 & 39.334 & 41.800 & 59.663 \\
\multirow{2}{*}{Shortened-LLM} & wo\_lora & 62.141 & 69.042 & 59.361 & 60.221 & 53.830 & 31.741 & 34.400 & 52.962 \\
 & \multicolumn{1}{c|}{w\_lora} & 63.333 & 72.307 & 67.128 & 63.694 & 66.667 & 39.761 & 37.000 & 58.556 \\
\multirow{2}{*}{Ours} & wo\_lora & 40.489 & 67.900 & 56.234 & 63.694 & 51.347 & 33.532 & 36.400 & 49.942 \\
 & \multicolumn{1}{c|}{w\_lora} & 59.939 & 73.286 & 68.751 & 65.983 & 68.981 & 41.724 & 39.000 & 59.666 \\ \hline \hline
\end{tabular}}
\caption{Performance with/without LoRA retraining on LLaMA2-13B.}
\label{lora4}
\end{table*}


\begin{table*}[]
\centering
\renewcommand{\arraystretch}{1.25}
\resizebox{2.0\columnwidth}{!}{%
\begin{tabular}{cl|cccccccc}
\hline \hline
\multicolumn{2}{c|}{20\%Pruned} & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-easy & ARC-challenge & OpenbookQA & AVE \\ \hline 
\multirow{2}{*}{LLM\_Pruner} & wo\_lora & 74.006 & 77.040 & 71.679 & 64.799 & 66.498 & 39.078 & 41.800 & 62.129 \\
 & w\_lora & 74.526 & 78.346 & 72.426 & 69.219 & 69.739 & 40.529 & 43.200 & 63.998 \\
\multirow{2}{*}{Shortened-LLM} & wo\_lora & 66.239 & 74.918 & 70.056 & 66.456 & 68.476 & 43.857 & 39.600 & 61.372 \\
 & w\_lora & 75.535 & 77.476 & 73.571 & 68.272 & 72.180 & 44.198 & 43.200 & 64.919 \\
\multirow{2}{*}{Ours} & wo\_lora & 75.199 & 75.898 & 71.679 & 69.692 & 70.370 & 43.942 & 43.200 & 64.283 \\
 & w\_lora & 81.040 & 76.442 & 74.846 & 70.324 & 71.801 & 44.625 & 41.800 & 65.840 \\ \hline \hline
\multicolumn{2}{c|}{35\%Pruned} & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-easy & ARC-challenge & OpenbookQA & AVE \\ \hline 
\multirow{2}{*}{LLM\_Pruner} & wo\_lora & 63.609 & 73.449 & 63.683 & 58.800 & 52.778 & 34.471 & 38.200 & 54.999 \\
 & \multicolumn{1}{c|}{w\_lora} & 70.581 & 76.659 & 67.317 & 65.272 & 63.258 & 35.154 & 40.800 & 59.863 \\
\multirow{2}{*}{Shortened-LLM} & wo\_lora & 42.385 & 69.532 & 57.897 & 60.063 & 60.017 & 37.372 & 34.800 & 51.724 \\
 & \multicolumn{1}{c|}{w\_lora} & 68.532 & 74.102 & 66.421 & 64.009 & 67.593 & 41.126 & 38.600 & 60.055 \\
\multirow{2}{*}{Ours} & wo\_lora & 63.609 & 73.123 & 58.703 & 61.089 & 62.837 & 36.604 & 34.600 & 55.795 \\
 & \multicolumn{1}{c|}{w\_lora} & 69.450 & 74.918 & 67.447 & 63.378 & 66.414 & 38.311 & 37.400 & 59.617 \\ \hline \hline
\end{tabular}}
\caption{Performance with/without LoRA retraining on Vicuna-13B-v1.3.}
\label{lora5}
\end{table*}


% \section*{Ethical Statement}

% There are no ethical issues.

% \section*{Acknowledgments}

% The preparation of these instructions and the \LaTeX{} and Bib\TeX{}
% files that implement them was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Preparation of the Microsoft Word file was supported by IJCAI.  An
% early version of this document was created by Shirley Jowell and Peter
% F. Patel-Schneider.  It was subsequently modified by Jennifer
% Ballentine, Thomas Dean, Bernhard Nebel, Daniel Pagenstecher,
% Kurt Steinkraus, Toby Walsh, Carles Sierra, Marc Pujol-Gonzalez,
% Francisco Cruz-Mencia and Edith Elkind.


%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}

\end{document}

