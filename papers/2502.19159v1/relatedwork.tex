\section{Related Work}
\subsection{Large Language Models}
Although Transformer-based large language models (LLMs) have shown excellent performance in a variety of natural language processing tasks, their high computational and memory overhead remain significant challenges for practical deployment \cite{sun2023simple,lin2024awq}. Transformer architecture consists of multiple stacked layers, including attention and MLP layers \cite{vaswani2017attention}. The attention layer, which computes contextual relationships between tokens, suffers from quadratic complexity, leading to exponential growth in computation as sequence length increases. Similarly, the MLP layer, responsible for transforming input tokens through up-projection and down-projection, significantly contributes to the model's parameter size \cite{zeng2024kan}. While these layers are essential for the model's expressiveness, they also impose considerable computational and memory costs. Consequently, reducing these overheads while preserving model performance has become a critical research challenge.


% \subsection{Transformer layer correlation measure}
% Recent research has shown that not all layers and blocks within a Transformer model are equally important; some layers exhibit significant redundancy and may not fully utilize their parameter capacity \cite{dai2020funnel,dalvi2020analyzing,sun2024transformer}. This insight has led to a re-examination of the internal structure of Transformer models and sparked quantitative studies on inter-layer redundancy. For example, \cite{men2024shortgpt} proposes the Block Influence (BI) metric, which quantifies the impact of each layer on model performance by assessing its contribution to changes in hidden states. \cite{raghu2021vision} uses the Centered Kernel Alignment (CKA) metric to analyze the representation similarity between Vision Transformer layers. The study found that in Vision Transformer (ViT), layers 3 to 10 are highly correlated, with CKA values reaching up to 0.97, indicating substantial redundancy. Furthermore, Layer-wise Importance Ranking \cite{li2022optimizing} uses gradient information and inter-layer correlation to rank layer importance, further verifying the universality of intermediate layer redundancy.

\subsection{Pruning Method on LLMs}

Large language models' multi-layer Transformer architecture often contains substantial redundancy, motivating research on width-wise and depth-wise pruning to reduce this redundancy and improve model efficiency.

\textbf{Width-wise pruning} reduces the network width by pruning coupled structures. For example, \cite{voita2019analyzing} and \cite{michel2019sixteen} introduced pruning and attention head sharing techniques to reduce redundant attention heads, thereby decreasing both computational complexity and parameter requirements. \cite{nova2023gradient} and \cite{santacroce2023matters} optimized the feedforward network by reducing the dimension of the FFN hidden layer, thereby reducing the memory footprint and computational complexity. More complex hybrid optimization methods have also been explored \cite{lagunas2021block,kwon2022fast,kurtic2024ziplm}.

\textbf{Depth-wise pruning} directly removes the entire least important layer and can significantly accelerate inference. Shortened-LLM \cite{kim2024shortened} selected Taylor+ and PPL indicators as the importance measure of the Transformer layer, and directly deleted the unimportant Transformer layer to reduce the consumption of computing resources and improve the inference speed. The layer-skipping strategy \cite{schuster2022confident,del2023skipdecode,raposo2024mixture} further reduces computational burden and boosts inference efficiency by dynamically selecting which layers to skip during execution. Additionally, \cite{song2024sleb,tang2024rethinking} investigated depth pruning methods, which reduce the model's depth by eliminating redundant layers, optimizing both computational overhead and model performance while retaining essential layers.