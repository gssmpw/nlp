%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{lmodern}


% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs}


% % Single author syntax
% \author{
%     Author Name
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Xuan Ding$^1$
\and
Yao Zhu$^2$\and
Yunjian Zhang$^3$\And
Chuanlong Xie$^1$\\
\affiliations
$^1$Beijing Normal University\\
$^2$Zhejiang University\\
$^3$University of Chinese Academy of Sciences\\
% $^4$Fourth Affiliation\\
\emails
202322011119@mail.bnu.edu.cn,
ee\_zhuy@zju.edu.cn,
sdtczyj@gmail.com,
clxie@bnu.edu.cn
}
% \fi

\begin{document}

\maketitle

\begin{abstract}
Compared to width-wise pruning, depth-wise pruning can significantly accelerate inference in resource-constrained scenarios. Howerver, treating the entire Transformer layer as the minimum pruning unit may degrade model performance by indiscriminately discarding the entire information of the layer. This paper reveals the "Patch-like" feature relationship between layers in large language models by analyzing the correlation of the outputs of different layers in the reproducing kernel Hilbert space. Building on this observation, we proposes a sliding layer merging method that dynamically selects and fuses consecutive layers from top to bottom according to a pre-defined similarity threshold, thereby simplifying the model structure while maintaining its performance. Extensive experiments on LLMs with various architectures and different parameter scales show that our method outperforms existing pruning techniques in both zero-shot inference performance and retraining recovery quality after pruning. In particular, in the experiment with 35\% pruning on the Vicuna-7B model, our method achieved a 1.654\% improvement in average performance on zero-shot tasks compared to the existing method. Moreover, we further reveal the potential of combining depth pruning with width pruning to enhance the pruning effect. Our codes are available at \href{https://github.com/920927/SLM-a-sliding-layer-merging-method}{https://github.com/920927/SLM-a-sliding-layer-merging-method}.
\end{abstract}


\section{Introduction}

In recent years, large language models (LLMs) have attracted widespread attention in deep learning, owing to their exceptional performance and broad application potential \cite{touvron2023llama,chowdhery2023palm}. However, in pursuit of better performance, the size of LLMs has grown increasingly larger, posing significant technical and resource challenges for practical deployment and application. Given that not all parameters in the large parameter space of the model contribute equally to the output, pruning methods are effective for reducing redundancy and addressing model size challenges \cite{ma2023llm,an2024fluctuation,sun2024transformer,kim2024shortened,men2024shortgpt,song2024sleb}. The width-wise approach reduces the network width by pruning coupled structures, such as attention heads and their associated weight connections, while preserving the number of layers \cite{ma2023llm,an2024fluctuation,sun2024transformer}. In contrast, the depth-wise approach reduces the network depth by completely removing certain layers \cite{kim2024shortened,men2024shortgpt,song2024sleb}. Although the depth-wise pruning method can significantly accelerate inference in resource-constrained scenarios that require running LLMs with limited batch sizes, it remains underexplored in terms of analyzing the correlations between Transformer layers at different depths. Moreover, arbitrary removing specific layers may degrade the performance of the pruned model.


We first focus on the question: what is the correlation between the features extracted by different layers in LLMs? To capture the subtle distinctions between features in high-dimensional space, we assess the correlations between the outputs of different layers of the model within a reproducing kernel Hilbert space and normalize the evaluation metric to ensure isotropic scaling invariance, an approach inspired by \cite{raghu2021vision}. We conducted observational experiments across multiple models and datasets, and the results reveal a high degree of similarity in the representations of certain consecutive Transformer layers in large language models, exhibiting a clear "patch-like" structure. This observation provides new insights for model compression, suggesting that when feature representations across layers are highly similar, parameter sharing or layer merging can be considered to reduce both computational load and memory usage.


Building on the aforementioned observation, we propose a novel compression method - Sliding Layer Merging. This method dynamically selects the base layer and its adjacent layers with similar representations for merging, starting from the deepest layers and moving progressively towards the shallower layers, utilizing a sliding window mechanism. For the layers to be merged, we calculate the parameter differences between them and the base layer, incorporating these differences into the base layer's parameters, thereby merging multiple layers into one. The sliding window mechanism selects adjacent layers of the base layer for merging by comparing the similarity between the outputs of the pruned model and the original model. When the similarity exceeds a predefined threshold, the window expands towards the shallower layers; when the similarity falls below the threshold, the layers to be merged are combined, and the window slides to update the base layer's index. Once the iterative merging process is completed, a fast recovery phase is performed, utilizing limited data to post-train the pruned model.


We conducted extensive experiments to evaluate the effectiveness of our Sliding Layer Merging (SLM) method. These included zero-shot performance comparisons with baseline methods, along with evaluations of inference speed and throughput. Experimental results demonstrate the superiority of our approach in terms of both model accuracy and computational efficiency. Moreover, we introduced an innovative fusion of width-wise and depth-wise pruning techniques, which further enhances the model compress performance.


The contributions of this study are summarized as:
\begin{itemize}
\item We analyzed the inter-layer correlations in LLMs within a reproducing kernel Hilbert space, observing an interesting Patch-Like correlation distribution, which provides valuable insights for the design of model compression strategies.
\item We propose the Sliding Layer Merging method, which dynamically merges layers with strong representational similarity in LLMs. This method can be seamlessly applied to various existing LLM architectures.
\item We conducted extensive experiments across multiple LLM architectures with varying parameter sizes, demonstrating that our method outperforms existing depth-wise pruning methods in zero-shot performance, both in retraining-free scenarios and in scenarios where pruning is followed by retraining to restore quality. Specifically, when pruning the Vicuna-7B model by 35\%, our method achieved superior average performance across multiple datasets, outperforming method A by 1.654\%.
\end{itemize}

\section{Related Work}

\subsection{Large Language Models}
Although Transformer-based large language models (LLMs) have shown excellent performance in a variety of natural language processing tasks, their high computational and memory overhead remain significant challenges for practical deployment \cite{sun2023simple,lin2024awq}. Transformer architecture consists of multiple stacked layers, including attention and MLP layers \cite{vaswani2017attention}. The attention layer, which computes contextual relationships between tokens, suffers from quadratic complexity, leading to exponential growth in computation as sequence length increases. Similarly, the MLP layer, responsible for transforming input tokens through up-projection and down-projection, significantly contributes to the model's parameter size \cite{zeng2024kan}. While these layers are essential for the model's expressiveness, they also impose considerable computational and memory costs. Consequently, reducing these overheads while preserving model performance has become a critical research challenge.


% \subsection{Transformer layer correlation measure}
% Recent research has shown that not all layers and blocks within a Transformer model are equally important; some layers exhibit significant redundancy and may not fully utilize their parameter capacity \cite{dai2020funnel,dalvi2020analyzing,sun2024transformer}. This insight has led to a re-examination of the internal structure of Transformer models and sparked quantitative studies on inter-layer redundancy. For example, \cite{men2024shortgpt} proposes the Block Influence (BI) metric, which quantifies the impact of each layer on model performance by assessing its contribution to changes in hidden states. \cite{raghu2021vision} uses the Centered Kernel Alignment (CKA) metric to analyze the representation similarity between Vision Transformer layers. The study found that in Vision Transformer (ViT), layers 3 to 10 are highly correlated, with CKA values reaching up to 0.97, indicating substantial redundancy. Furthermore, Layer-wise Importance Ranking \cite{li2022optimizing} uses gradient information and inter-layer correlation to rank layer importance, further verifying the universality of intermediate layer redundancy.

\subsection{Pruning Method on LLMs}

Large language models' multi-layer Transformer architecture often contains substantial redundancy, motivating research on width-wise and depth-wise pruning to reduce this redundancy and improve model efficiency.

\textbf{Width-wise pruning} reduces the network width by pruning coupled structures. For example, \cite{voita2019analyzing} and \cite{michel2019sixteen} introduced pruning and attention head sharing techniques to reduce redundant attention heads, thereby decreasing both computational complexity and parameter requirements. \cite{nova2023gradient} and \cite{santacroce2023matters} optimized the feedforward network by reducing the dimension of the FFN hidden layer, thereby reducing the memory footprint and computational complexity. More complex hybrid optimization methods have also been explored \cite{lagunas2021block,kwon2022fast,kurtic2024ziplm}.

\textbf{Depth-wise pruning} directly removes the entire least important layer and can significantly accelerate inference. Shortened-LLM \cite{kim2024shortened} selected Taylor+ and PPL indicators as the importance measure of the Transformer layer, and directly deleted the unimportant Transformer layer to reduce the consumption of computing resources and improve the inference speed. The layer-skipping strategy \cite{schuster2022confident,del2023skipdecode,raposo2024mixture} further reduces computational burden and boosts inference efficiency by dynamically selecting which layers to skip during execution. Additionally, \cite{song2024sleb,tang2024rethinking} investigated depth pruning methods, which reduce the model's depth by eliminating redundant layers, optimizing both computational overhead and model performance while retaining essential layers.


\section{Motivation}
\subsection{CKA vector similarity}

Center Kernel Alignment (CKA) is a metric used to compare the internal representations of neural networks. Its main advantages are its invariance to orthogonal transformations (e.g. changes in neuron arrangement) and its robustness to isotropic scaling achieved through a normalization term \cite{raghu2021vision}. These properties make CKA particularly suitable for studying the underlying relationships between different Transformer layers within large language models. The calculation procedure for CKA is outlined as follows:




\begin{itemize}
\item Step1:  Calculate the Gram matrix (kernel matrix) of two representation matrices to measure the similarity of representations.
\begin{equation}
K=XX^T, L=YY^T, K,L \in R^{n \times n}
\label{eq1}
\end{equation}
We set the output of the two Transformer layers that need to calculate CKA to the representation matrices $X\in R^{n \times p}$ and $Y\in R^{n\times q}$, where $n$ is the number of samples, and $p$ and $q$ are two representations of the space dimensions.

\item Step2: Centralize the Gram matrix to eliminate the potential impact of sample distribution deviation.
\begin{equation}
\tilde{K}=HKH, \tilde{L}=HLH
\label{eq2}
\end{equation}
Where $H=I_n-1/n 1_n 1_n^T$ is the centralization matrix, $I_n$ is the identity matrix of $n\times n$, and $1_n$ is an all-1 vector of length $n$.

\item Step3: Calculate the normalized alignment between the central Gram matrices K and L to get CKA. 
\begin{equation}
\text{CKA}(K, L) = \frac{\langle \tilde{K}, \tilde{L} \rangle_F}{\| \tilde{K} \|_F \| \tilde{L} \|_F}
\label{eq3}
\end{equation}
Where $\langle \cdot, \cdot \rangle_F$ denotes the Frobenius inner product (sum of element-wise products) and $\| \cdot \|_F$ represents the Frobenius norm (square root of the sum of squares of all elements in the matrix).

\end{itemize}

The final CKA value is between 0 and 1. The closer the value is to 1, the more similar the two representation matrices are.


\subsection{Representation Structure between LLM Transformer Layers}

We begin our investigation by analyzing the internal representation structure of each model using CKA metric. Specifically, we focus on the following questions: What are the internal relationships between the different Transformer layers of LLMs? How are representations propagated across these layers? Is there redundancy among the Transformer layers in LLMs? To explore these questions, we present inter-layer CKA similarity heatmaps for several large language models (LLMs), including LLaMA2-7B, Vicuna-7B-v1.3, and Meta-LLaMA3-8B, as shown in Fig.\ref{cka}. Analysis of these heatmaps revealed several key findings:

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\columnwidth]{cka_figure.pdf}  
\caption{CKA (Center Kernel Alignment) metric between pairs of Transformer layers in LLMs.}
\label{cka}
\end{figure}


\begin{itemize}
\item \textbf{Consistency across models:} Although CKA metric maps between layers of different large-scale models exhibit different local performances, they generally exhibit higher redundancy and strong inter-layer correlation in the middle layers.
\item \textbf{Inter-layer correlation differences:} We found that the first two layers and the last layer of the model were less correlated with CKA than other layers. This suggests that the representations of some layers may be relatively independent, weakly functionally related to other layers, and may not be suitable for large-scale compression. In subsequent studies, we "protect" these layers by not compressing the first two layers and the last layer to avoid unnecessary damage to model performance.
\item \textbf{Redundancy of intermediate layers:} When performing CKA measurements on intermediate layers, we observed that the correlations between these layers exhibit a clear block-like structure. This structure shows that there is strong continuity and correlation between the middle layers, implying that these layers have high functional redundancy and provide space for compression. 

% In addition, this phenomenon also reveals that there is a large connection relationship between the parameters of each layer, reminding us that simply deleting all parameters of some layers may not be ideal. Instead, more granular compression strategies (such as pruning redundant parameters) may be more effective.
\end{itemize}



% \begin{figure}[tb]
% \centering
% \includegraphics[width=1.0\columnwidth]{framework.pdf}
% \caption{The framework of our sliding layer parameter merging method: a) Iterative Update: Merge consecutive transformer layers from top to bottom. If the similarity between the merged and original models is below the threshold, increase the number of layers merged; otherwise, keep the current compression and proceed to the next update. b) Parameter Merge: Calculate the difference between adjacent layer parameters and the base layer, adding the sum to the base layer to replace multiple layers with one. c) Similarity Calculation: Compute the cosine similarity of the last hidden states of the original and merged models on a few-shot calibration dataset.}
% \label{figure2}
% \end{figure}


\section{Method}


\subsection{Sliding layer merging algorithm}

The aformentioned CKA test indicates that the intermediate layers of the model exhibit similar representations, while the lower and final layers show significant differences compared to other layers. Based on this, we propose a top-down dynamic layer merging method to merge layers with similar representations. During the compression process, starting from higher layers, we gradually merge adjacent layers downwards. The goal of merging is to ensure that the output representation of the compressed model on the calibration samples is as similar as possible to the original model, thereby minimizing performance loss. Algorithm \ref{algorithm1} summarizes the workflow of our method.


\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\columnwidth]{framework.pdf}
\caption{The framework of our sliding layer merging method.}
\label{framework}
\end{figure}


\textbf{Model initialization.} During initialization, we assign the original model $M$ to the target compression model $M^*$, and set the compression layer range to $[L, H]$, where $H$ is the highest layer. $L$ is the lowest level. Setting the compression layer range helps us adopt appropriate protection mechanisms to ensure that the compression model does not lose the functionality of key layers. Specifically, we identified low-correlation layers based on CKA results, such that the last layer is more independent in the model and has lower correlation with other layers. Therefore, we change the scope of the top-level $H$ to ``protect" them, i.e. they are not merged or compressed. The protection mechanisms help maintain the core performance of the model and prevent excessive compression from affecting the performance.

\textbf{Layer merge.} The sliding window specifies the range of layers to be merged. We take the highest layer $H$ as the initial upper bound of the sliding window, and set its next layer $H-1$ as the initial lower bound. By merging mulitiple layers within the sliding window, we get the temporary model $M_{tmp}$. Here, we adopt a layer merging strategy based on inter-layer differences (as shown in Fig.\ref{framework} (b)), adding the differences between the parameters of adjacent layers and the base layer, and gradually integrating redundant information. This strategy can not only capture the correlation between layers, but also has the flexibility to adapt to the compression needs of different models.


\begin{algorithm}[ht]
    \caption{Iterative Layer Compression Algorithm}
    \label{alg:algorithm}
    \textbf{Input}: Original model $M$ \\
    \textbf{Parameters}: Layer range $[L, H]$; Similarity threshold $T$; Few-shot calibration samples $D$ \\
    \textbf{Output}: Pruned model $M^*$
    
    \begin{algorithmic}[1] %[1] enables line numbers
        \STATE $M^* \gets M$
        \STATE $high\_lay \gets H$
        \STATE $low\_lay \gets high\_lay - 1$
        \WHILE{$low\_lay \geq L$}
            \STATE $M_{tmp} \gets \text{Merge\_Layer}(M^*, high\_lay, low\_lay)$
            \STATE $s \gets \text{Cal\_Sim}(M, M_{tmp})$
            \IF {$s > T$}
                \STATE $low\_lay \gets low\_lay - 1$
            \ELSE
                \STATE $M^* \gets M_{tmp}$
                \STATE $high\_lay \gets low\_lay$
            \ENDIF
        \ENDWHILE
        \STATE \textbf{return} $M^*$
    \end{algorithmic}
\label{algorithm1}
\end{algorithm}


\begin{table*}[ht]
\centering
\renewcommand{\arraystretch}{1.2}
\resizebox{1.9\columnwidth}{!}{%
\begin{tabular}{ccc|cccccccc}
\hline \hline
\multicolumn{3}{c|}{} & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-easy & ARC-challenge & OpenbookQA & AVE \\ \hline \hline
\multicolumn{3}{c|}{LLaMA2-7B(Original)} & 77.706 & 78.074 & 76.021 & 69.140 & 76.305 & 46.331 & 44.200 & 66.825 \\ \hline
\multicolumn{1}{c|}{\multirow{6}{*}{20\%Pruned}} & \multicolumn{1}{c|}{\multirow{3}{*}{width}} & Wanda-sp & 62.600 & 76.440 & 70.660 & 63.770 & 69.610 & 42.150 & 40.000 & 60.747 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & FLAP & 72.050 & 73.390 & 64.690 & 64.720 & 62.250 & 32.510 & 36.800 & 58.059 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & LLM-Pruner & 63.731 & 77.476 & 67.128 & 61.878 & 65.783 & 38.481 & 40.400 & 59.268 \\ \cline{2-11} 
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\multirow{3}{*}{depth}} & SLEB & 62.875 & 73.939 & 63.951 & 59.747 & 63.468 & 35.154 & 38.000 & 56.733 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Shortened-LLM & 61.560 & 76.061 & 67.994 & 58.800 & 68.813 & 37.884 & 38.000 & 58.445 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Ours & 69.450 & 73.667 & 70.484 & 67.088 & 69.108 & 41.212 & 42.600 & \textbf{61.944} \\ \hline
\multicolumn{1}{c|}{\multirow{6}{*}{35\%Pruned}} & \multicolumn{1}{c|}{\multirow{3}{*}{width}} & Wanda-sp & 59.790 & 68.820 & 53.140 & 54.060 & 52.270 & 31.570 & 32.800 & 50.350 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & FLAP & 66.970 & 67.850 & 52.100 & 61.480 & 49.490 & 28.070 & 32.400 & 51.194 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & LLM-Pruner & 45.260 & 74.760 & 60.290 & 59.350 & 57.280 & 32.420 & 37.200 & \textbf{52.366} \\ \cline{2-11} 
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\multirow{3}{*}{depth}} & SLEB & 54.801 & 67.410 & 46.545 & 53.197 & 48.527 & 29.010 & 33.000 & 47.499 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Shortened-LLM & 62.171 & 71.926 & 54.800 & 51.776 & 59.259 & 30.887 & 35.400 & 52.317 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Ours & 63.119 & 65.452 & 56.503 & 58.879 & 52.020 & 31.911 & 35.200 & 51.869 \\ \hline \hline
\multicolumn{3}{c|}{Vicuna-7B-v1.3(Original)} & 78.104 & 77.312 & 73.939 & 69.376 & 74.327 & 44.454 & 43.800 & 65.902 \\ \hline
\multicolumn{1}{c|}{\multirow{6}{*}{20\%Pruned}} & \multicolumn{1}{c|}{\multirow{3}{*}{width}} & Wanda-sp & 63.270 & 73.780 & 68.620 & 63.930 & 67.210 & 38.820 & 37.200 & 58.976 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & FLAP & 73.520 & 74.810 & 68.760 & 66.460 & 69.110 & 38.990 & 40.000 & 61.664 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & LLM-Pruner & 67.645 & 76.115 & 66.660 & 63.931 & 65.446 & 36.604 & 40.400 & 59.543 \\ \cline{2-11} 
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\multirow{3}{*}{depth}} & SLEB & 46.758 & 51.850 & 26.170 & 51.223 & 25.505 & 28.840 & 24.800 & 36.449 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Shortened-LLM & 72.355 & 74.701 & 67.576 & 64.562 & 70.034 & 38.225 & 38.600 & 60.865 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Ours & 77.431 & 74.755 & 69.040 & 68.745 & 69.318 & 38.908 & 40.200 & \textbf{62.628} \\ \hline
\multicolumn{1}{c|}{\multirow{6}{*}{35\%Pruned}} & \multicolumn{1}{c|}{\multirow{3}{*}{width}} & Wanda-sp & 50.760 & 60.450 & 43.060 & 55.960 & 43.520 & 26.190 & 28.000 & 43.991 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & FLAP & 57.860 & 69.640 & 59.120 & 63.300 & 57.830 & 35.670 & 36.000 & 54.203 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & LLM-Pruner & 63.976 & 73.069 & 59.560 & 58.564 & 56.524 & 32.679 & 37.800 & 54.596 \\ \cline{2-11} 
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\multirow{3}{*}{depth}} & SLEB & 37.829 & 53.264 & 25.921 & 49.961 & 25.926 & 29.096 & 25.800 & 35.399 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Shortened-LLM & 64.281 & 70.783 & 56.722 & 57.380 & 59.596 & 31.485 & 34.000 & 53.464 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Ours & 69.235 & 70.294 & 60.705 & 62.273 & 60.227 & 33.618 & 37.400 & \textbf{56.250} \\ \hline \hline
\multicolumn{3}{c|}{LLaMA3-8B(Original)} & 81.101 & 79.489 & 79.167 & 73.402 & 80.093 & 53.242 & 44.800 & 70.185 \\ \hline
\multicolumn{1}{c|}{\multirow{5}{*}{20\%Pruned}} & \multicolumn{1}{c|}{\multirow{2}{*}{width}} & FLAP & 37.830 & 52.180 & 26.360 & 49.960 & 26.810 & 24.830 & 26.000 & 34.853 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & LLM-Pruner & 74.037 & 79.489 & 74.268 & 70.324 & 74.285 & 46.587 & 42.600 & 65.941 \\ \cline{2-11} 
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\multirow{3}{*}{depth}} & SLEB & 62.171 & 73.286 & 64.748 & 63.062 & 64.562 & 37.713 & 37.000 & 57.506 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Shortened-LLM & 66.208 & 78.074 & 72.695 & 62.747 & 75.295 & 44.795 & 43.400 & 63.316 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Ours & 76.789 & 77.639 & 73.770 & 71.744 & 76.599 & 50.939 & 41.200 & \textbf{66.954} \\ \hline
\multicolumn{1}{c|}{\multirow{5}{*}{35\%Pruned}} & \multicolumn{1}{c|}{\multirow{2}{*}{width}} & FLAP & 37.830 & 52.340 & 26.050 & 47.990 & 24.790 & 24.830 & 25.200 & 34.147 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & LLM-Pruner & 64.465 & 74.048 & 61.800 & 59.353 & 64.646 & 34.386 & 37.200 & 56.557 \\ \cline{2-11} 
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\multirow{3}{*}{depth}} & SLEB & 59.755 & 64.635 & 45.061 & 51.539 & 47.306 & 27.133 & 27.600 & 46.147 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Shortened-LLM & 63.180 & 72.851 & 62.985 & 58.090 & 66.877 & 37.116 & 37.000 & 56.871 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Ours & 67.554 & 73.830 & 61.472 & 62.747 & 64.352 & 36.007 & 37.600 & \textbf{57.652} \\ \hline \hline
\end{tabular}
}
\caption{Performance comparison of pruning methods across multiple baselines at 20\% and 35\% pruning rates. We compare our method with width pruning approaches (Wanda-sp, FLAP, LLM-Pruner) and depth pruning approaches (SLEB, Shortened-LLM) on LLaMA2-7B, Vicuna-7B, and LLaMA3-8B models. Note that Wanda-sp does not produce results for the LLaMA3-8B model due to incompatibility.}
\label{table1}
\end{table*}


\textbf{Iterative update.} To measure the impact of the merging operation on the model output representation, we calculate the cosing similarity of the last hidden states of the original model $M$ and the merged model $M_{tmp}$ on the few-shot calibration dataset (see Fig.\ref{framework} (c)). If the representation similarity between $M_{tmp}$ and the original model is greater than the set threshold $T$, the merging layer is considered to have a small impact on the model performance. At this time, the lower bound of the sliding window is moved down one layer to expand the merging range. On the contrary, if the representation similarity between $M_{tmp}$ and the original model is less than the threshold $T$, it is considered that the impact of the merging layer on the model performance is too great, and the sliding window should stop expanding. At this time, the compressed model $M^ *$ is updated to $M\_{tmp}$, and the upper bound of the sliding window is updated to the current lower bound of the sliding window, and then enters the next round of layer merging (see Fig.\ref{framework}(a)).


\textbf{Termination condition.} The process continues until the lowest level $L$ is processed. Ultimately, the pruned model $M^*$ output by the algorithm reduces redundant computing and storage requirements by retaining the merged representation of key layers.


\subsection{Performance Recovery with Low-rank Approximation}

We use the low-rank approximation technique, LoRA\cite{hulora}, to fine-tune the pruned model and recover its performance. This is a common practice in many pruning methods \cite{ma2023llm,kim2024shortened}, and here we provide only a brief introducion to ensure the self-contained aspect of our work. 

Each learnable weight matrix (including both pruned and unpruned linear projections in LLMs) is represented by $W$. The update to $W$, denoted as $\triangle W$, is factorized as $\triangle W = PQ \in R^{d^{-} \times d^+}$, where $P \in R^{d^{-} \times d}$ and $Q \in R^{d \times d^+}$. Here, $d^{-}$, $d$, and $d^+$ correspond to the dimensions of the input, hidden, and output layers, respectively. The forward computation is then expressed as follows:
\begin{equation}
    f(x) = (W+\triangle W)x + b =(WX+b)+(PQ)X
\end{equation}

where $b$ represents the bias in the dense layer. By training only the low-rank matrices $P$ and $Q$, we considerably reduce both the computational complexity and the dependence on large-scale training data. Furthermore, the additional parameters $P$ and $Q$ can be reparameterized as $\triangle W$, thereby introducing no extra parameters in the final pruned model.

\section{Experiments}

\subsection{Experimental setup}
\textbf{Foundation LLMs.} To validate the effectiveness of our method, we conducted experiments on existing popular open-source language models, including LLaMA2-\{7B, 13B\} \cite{touvron2023llama}, LLaMA3-\{8B\} and Vicuna-\{7B, 13B\}-v1.3 \cite{chiang2023vicuna}. 


\textbf{Baselines.} The proposed method is compared with several previous works, categorized by their pruning strategy. For width pruning, we compare with LLM-Pruner \cite{ma2023llm}, FLAP \cite{an2024fluctuation}, and Wanda-sp \cite{an2024fluctuation}, a structured variant of Wanda \cite{sun2024transformer}. For depth pruning, we examine SLEB \cite{song2024sleb} and Shortened-LLM \cite{kim2024shortened}. We assess all methods under two target pruning levels: 20\% and 35\%. If the product of the total number of transformer blocks and target sparsity is not an integer, we round up to determine the number of blocks to remove.

\textbf{Benchmarks.} Following Touvron et al. (2023), we measure zero-shot accuracy on commonsense reasoning datasets (i.e., BoolQ \cite{clark2019boolq}, PIQA \cite{bisk2020piqa}, HellaSwag \cite{zellers2019hellaswag}, WinoGrande \cite{sakaguchi2021winogrande}, ARCeasy \cite{clark2018think}, ARC-challenge \cite{clark2018think}, and OpenbookQA \cite{mihaylov2018can}) using the lm-evaluation-harness package (\cite{eval-harness}). 

\textbf{Implementation Details.} We implement our method in PyTorch \cite{paszke2019pytorch} using the HuggingFace Transformers library \cite{wolf2020transformers}. Following \cite{ma2023llm}, we randomly select 10 samples from BookCorpus\cite{zhu2015aligning} to calculate the model similarity in the iterative pruning process. We also use this calibration dataset as a baseline approach to ensure a fair comparison. In LoRA retraining, we use 50K samples of refined Alpaca \cite{taori2023stanford} for instruction tuning. All experiments covered in this article were performed on an NVIDIA A100 GPU with 80GB memory.


\subsection{Zero-shot Tasks}
Tab.\ref{table1} shows the zero-shot performance of different pruning methods in the LLaMA2-7B model, Vicuna-7B-v1.3 model and LLaMA3-8B model. The results demonstrate that our method outperforms existing pruning techniques across multiple zero-shot tasks and models, demonstrating superior performance in both width and depth pruning settings. Specifically, under the 20\% pruning rate of the LLaMA2-7B model, our method improves average accuracy (AVE) by 2.676\% compared with the best-performing LLM-Pruner method; under the 35\% pruning rate of the Vicuna-7B model, the average accuracy of our method on zero-shot is 1.654\% higher than that of existing methods. These results show that our proposed method can effectively reduce model size and complexity while more fully maintaining model performance.


To comprehensively verify the broad applicability of our method, we also provide relevant experimental results on larger models (such as LLaMA2-13B and Vicuna-13B-v1.3) in the appendix. 


\subsection{Latency and Throughput}


We follow \cite{sheng2023flexgenhighthroughputgenerativeinference} to evaluate the LLM inference speedup achieved by our method in comparison to previous pruning methods. Given a batch size M and an output sequence length L (excluding the input length), the latency T represents the time required to handle the given prompts and produce $ML$ output tokens. The throughput is computed as $ML/T$ . We report the average results from 20 runs after the initial 10 warm-up batches. Tab.\ref{inference_efficiency} present throughput and latency results for LLaMA-2-7B.

Experimental results show that depth pruning generally outperforms width pruning in terms of reasoning efficiency. Specifically, at pruning ratio of 20\% and 35\%, depth pruning methods (including SLEB, Shortened-LLM and our proposed method) outperform width pruning methods (such as Wanda-sp, FLAP, and LLM-Pruner) in both latency and throughout metrics. This suggests that reducing the depth of the model can more effectively enhance inference speed and throughout. At the same time, depth pruning methods maintain relatively stable GPU memory usage while ensuring efficient inference. Therefore, from the perspective of inference efficiency, depth pruning is a more effective pruning strategy.

\begin{table}[t]
\renewcommand{\arraystretch}{1.8}
\fontsize{20}{15}\selectfont
\resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{ccl|clclclcl}
\hline
\multicolumn{3}{c|}{} & \multicolumn{2}{c}{Latency(sec)} & \multicolumn{2}{c}{Throughout(tokens/s)} & \multicolumn{2}{c}{GPU\_Memory} & \multicolumn{2}{c}{nparam} \\ \hline
\multicolumn{3}{c|}{LLaMA-2-7B(Original)} & \multicolumn{2}{c}{2.729} & \multicolumn{2}{c}{46.905} & \multicolumn{2}{c}{13020.25} & \multicolumn{2}{c}{6.7B} \\ \hline
\multicolumn{1}{c|}{\multirow{6}{*}{20\%Pruned}} & \multicolumn{1}{c|}{\multirow{3}{*}{width}} & Wanda-sp & \multicolumn{2}{c}{4.628} & \multicolumn{2}{c}{27.663} & \multicolumn{2}{c}{10676} & \multicolumn{2}{c}{5.5B} \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & FLAP & \multicolumn{2}{c}{4.045} & \multicolumn{2}{c}{31.656} & \multicolumn{2}{c}{10707.25} & \multicolumn{2}{c}{5.4B} \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & LLM-Pruner & \multicolumn{2}{c}{5.655} & \multicolumn{2}{c}{22.635} & \multicolumn{2}{c}{10951.5} & \multicolumn{2}{c}{5.5B} \\ \cline{2-11} 
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\multirow{3}{*}{depth}} & SLEB & \multicolumn{2}{c}{2.529} & \multicolumn{2}{c}{50.622} & \multicolumn{2}{c}{10682.45} & \multicolumn{2}{c}{5.5B} \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Shortened-LLM & \multicolumn{2}{c}{2.585} & \multicolumn{2}{c}{49.542} & \multicolumn{2}{c}{10682.45} & \multicolumn{2}{c}{5.5B} \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Ours & \multicolumn{2}{c}{2.339} & \multicolumn{2}{c}{54.758} & \multicolumn{2}{c}{10682.45} & \multicolumn{2}{c}{5.5B} \\ \hline
\multicolumn{1}{c|}{\multirow{6}{*}{35\%Pruned}} & \multicolumn{1}{c|}{\multirow{3}{*}{width}} & Wanda-sp & \multicolumn{2}{c}{4.619} & \multicolumn{2}{c}{27.726} & \multicolumn{2}{c}{8901} & \multicolumn{2}{c}{4.5B} \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & FLAP & \multicolumn{2}{c}{4.127} & \multicolumn{2}{c}{31.051} & \multicolumn{2}{c}{8855.95} & \multicolumn{2}{c}{4.5B} \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & LLM-Pruner & \multicolumn{2}{c}{5.630} & \multicolumn{2}{c}{22.736} & \multicolumn{2}{c}{9043.9} & \multicolumn{2}{c}{4.5B} \\ \cline{2-11} 
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\multirow{3}{*}{depth}} & SLEB & \multicolumn{2}{c}{1.938} & \multicolumn{2}{c}{66.048} & \multicolumn{2}{c}{8725.9} & \multicolumn{2}{c}{4.5B} \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Shortened-LLM & \multicolumn{2}{c}{2.084} & \multicolumn{2}{c}{61.433} & \multicolumn{2}{c}{8725.85} & \multicolumn{2}{c}{4.5B} \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Ours & \multicolumn{2}{c}{1.889} & \multicolumn{2}{c}{67.770} & \multicolumn{2}{c}{8725.9} & \multicolumn{2}{c}{4.5B} \\ \hline
\end{tabular}
}
\caption{Inference efficiency of pruned models. (Measured with 12 input tokens, 128 output tokens, and a batch size of 1 on a NVIDIA H100 GPU.)}
\label{inference_efficiency}
\end{table}



\begin{figure}[h]
\centering
\includegraphics[width=0.92\columnwidth]{merge.pdf}  
\caption{Performance of the integrated method on LLaMA2-7B. The horizontal axis shows the depth pruning ratio, and the vertical axis indicates zero-shot task performance. Results for Winogrande, HellaSwag, and ARC Easy are plotted as dotted lines, with the solid line showing the average performance across seven tasks.}
\label{merge}
\end{figure}



\subsection{Integration of width-wise pruning and depth-wise pruning}

We observe in Tab \ref{table1} that in the 35\% pruning scenario of the LLaMA2-7B model, the width-wise LLM-Pruner method performs slightly better than our depth-wise method. This may be due to the fact that under specific pruning ratios and model structures, width pruning can also exhibit advantages, and depth pruning does not necessarily always outperform it. This naturally raises the question: Can the intergration of width-wise pruning and depth-wise pruning leverage the strengths of both methods to further improve pruning results?

\begin{figure}[htbp!]
\centering
\includegraphics[width=0.95\columnwidth]{lora.pdf}  
\caption{Performance with/without LoRA retraining. The blue column in the figure represents the model performance before lora fine-tuning, and the orange column represents the performance gain brought by lora fine-tuning to the model.}
\label{lora}
\end{figure}

Specifically, using the LLaMA2-7B model as an example, we divide the pruning process into two stages: depth-wise pruning and width-wise pruning. In the first stage, we apply the pruning method we proposed, compressing the model based on layers. In the second stage, we use the LLM-Pruner method to remove non-essential coupled structures from the model obtained in the first stage. As a result, the pruned model achieves a pruning rate of 35\% relative to the original model. For the sake of pruning convenience, we selected the following depth pruning rates: 0\% (LLM-Pruner), 18\%, 36\%, 53\%, 71\%, 91\%, and 100\% (ours). The performance evaluation results are shown in Fig.\ref{merge}.




The results shown that the method integrating depth-wise pruning and width-wise pruning achieves better model performance at the same pruning rate compared to using either depth pruning or width pruning alone. Specifically, models with depth pruning rates of 18\%, 36\%, 53\%, 71\% and 91\% consistently outperform the models with depth pruning rates of 0\% and 100\%. Notably, the models with depth pruning rates of 36\% and 53\% rank first and second in terms of performance, respectively. This shows that the integrated methods can combine the advantages of depth-wise pruning and width-wise pruning methods to achieve better model performance than when using depth-wise pruning or width-wise pruning alone, while mitigating the throughput and inference speed issues associated with width-wise pruning methods. We also performed the same experiments on the Vicuna-13B-v1.3 model, and the results can be found in the Appendix.


\subsection{The impact of LoRA retraining}

We compared several pruning strategies that require retraining, including LLM-Pruner, Shortened-LLM, and our proposed method. Experiments evaluate the impact of these strategies on model quality before and after pruning. The results are shown in Fig.\ref{lora}, showing the performance of the LLaMA2-7B model and Vicuna-7B model when the pruning rate is 20\% and 35\% respectively.



Experimental results show that before LoRA fine-tuning, the performance of our method was significantly better than that of LLM-Pruner and Shortened-LLM, with performance indicators at the best level whether under both 20\% and 35\% pruning ratio. After further employing LoRA fine-tuning, the performance of our method is significantly improved and still maintains the lead in most cases. Specifically, under the 20\% pruning ratio of the LLaMA2-7B model, the performance improvement of the our method is particularly significant, from 57.360\% to 61.944\%, an increase of 4.584\%, far exceeding LLM-Pruner (by 2.031\%) and Shortened-LLM (by 2.946\%). These results show that our method not only performs well when used alone, but also exerts a stronger synergistic effect when combined with LoRA retraining technology, thereby significantly improving the overall performance of the model.


\subsection{The impact of Layer merging methods}
We considered three different layer merging methods â€” direct layer deletion, replacing multiple layers' parameters with their average, and our proposed method (denoted as "remove", "average" and "ours"). We take the HellaSwag dataset as an example to analyze their impact on the performance of the pruned model. (The models we evaluate here are not fine-tuned with lora.)


We first analyzed the number of model layers in the LLaMA2-7B model after iterative fusion using three different layer merging methods at various similarity thresholds, with the results presented in Fig.\ref{params}(a). As the threshold decreases, the number of model layers gradually reduces, indicating a decrease in redundant parameters during the compression process. At the same similarity threshold, our layer merging method achieves a higher model compression rate than the direct deletion and averaging methods. For example, when the model output similarity threshold is 0.75 before and after compression, the model compressed using our method contains only 23 layers, significantly fewer than the Delete method (26 layers) and Average method (29 layers), resulting in a more streamlined and efficient model compression.

In Fig.\ref{params}(b), we compared the zero-shot performance of pruned models using different layer merging methods at various compression levels on the HellaSwag dataset. The results show that, as the number of compression layers increases, using the ``Average'' method will have a significant impact on model performance. In contrast, our proposed method can show better performance than the direct deletion and averaging methods under different compression levels, and this performance difference gradually expands as the number of compression layers increases.



\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{params.pdf}  
\caption{The impact of layer merging methods on LLaMA2-7B model. (a) Layer counts of the pruned model under different similarity thresholds through three layer merging methods (Delete, Average, Ours). (b) Zero-shot performance of the pruned model on the HellaSwag dataset through three layer merging methods.}
\label{params}
\end{figure}

\section{Conclusion}

By analyzing the correlation between the outputs of different layers in the reproducing kernel Hilbert space, this paper reveals the "patch-like" relational patterns between layers in large language models. Based on this insight, we propose a depth-wise pruning method that dynamically selects and merges layer parameters. This method sets a similarity threshold and merges consecutive layers from top to bottom, enabling rapid model compression while effectively preserving performance. Experimental results demonstrate that our method significantly accelerates inference in resource-constrained environments and outperforms existing pruning techniques on zero-shot tasks. Moreover, our approach can be seamlessly integrated with width pruning techniques, leading to pruning models that achieve enhanced performance. We hope this study will inspire further research into depth-wise pruning methods and foster the development of a unified framework that combines both depth-wise and width-wise pruning strategies, ultimately contributing to the efficient deployment of LLMs in resource-constrained environments.

% \appendix


% \section*{Ethical Statement}

% There are no ethical issues.

% \section*{Acknowledgments}

% The preparation of these instructions and the \LaTeX{} and Bib\TeX{}
% files that implement them was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Preparation of the Microsoft Word file was supported by IJCAI.  An
% early version of this document was created by Shirley Jowell and Peter
% F. Patel-Schneider.  It was subsequently modified by Jennifer
% Ballentine, Thomas Dean, Bernhard Nebel, Daniel Pagenstecher,
% Kurt Steinkraus, Toby Walsh, Carles Sierra, Marc Pujol-Gonzalez,
% Francisco Cruz-Mencia and Edith Elkind.


%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}

\end{document}

