[
  {
    "index": 0,
    "papers": [
      {
        "key": "pezeshkpour2023large",
        "author": "Pezeshkpour, Pouya and Hruschka, Estevam",
        "title": "Large language models sensitivity to the order of options in multiple-choice questions"
      },
      {
        "key": "alzahrani2024benchmarks",
        "author": "Alzahrani, Norah and Alyahya, Hisham Abdullah and Alnumay, Yazeed and Alrashed, Sultan and Alsubaie, Shaykhah and Almushaykeh, Yusef and Mirza, Faisal and Alotaibi, Nouf and Altwairesh, Nora and Alowisheq, Areeb and others",
        "title": "When benchmarks are targets: {R}evealing the sensitivity of large language model leaderboards"
      },
      {
        "key": "zheng2023large",
        "author": "Zheng, Chujie and Zhou, Hao and Meng, Fandong and Zhou, Jie and Huang, Minlie",
        "title": "Large language models are not robust multiple choice selectors"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "han2023medalpaca",
        "author": "Tianyu Han et al.",
        "title": "MedAlpaca--an open-source collection of medical conversational AI models and training data"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "bedi2024systematic",
        "author": "Bedi, Suhana and Liu, Yutong and Orr-Ewing, Lucy and Dash, Dev and Koyejo, Sanmi and Callahan, Alison and Fries, Jason A and Wornow, Michael and Swaminathan, Akshay and Lehmann, Lisa Soleymani and others",
        "title": "A Systematic Review of Testing and Evaluation of Healthcare Applications of Large Language Models (LLMs)"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "chiang2024chatbot",
        "author": "Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhu, Banghua and Zhang, Hao and Jordan, Michael and Gonzalez, Joseph E and others",
        "title": "Chatbot {A}rena: {A}n {O}pen {P}latform for {E}valuating {LLM}s by {H}uman {P}reference"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "chen2023large",
        "author": "Chen, Qingyu and Du, Jingcheng and Hu, Yan and Keloth, Vipina Kuttichi and Peng, Xueqing and Raja, Kalpana and Zhang, Rui and Lu, Zhiyong and Xu, Hua",
        "title": "Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "nimah2023nlg",
        "author": "Nimah, Iftitahu and Fang, Meng and Menkovski, Vlado and Pechenizkiy, Mykola",
        "title": "NLG Evaluation Metrics Beyond Correlation Analysis: An Empirical Metric Preference Checklist"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "fleming2024medalign",
        "author": "Fleming, Scott L and Lozano, Alejandro and Haberkorn, William J and Jindal, Jenelle A and Reis, Eduardo and Thapa, Rahul and Blankemeier, Louis and Genkins, Julian Z and Steinberg, Ethan and Nayak, Ashwin and others",
        "title": "Medalign: A clinician-generated dataset for instruction following with electronic medical records"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "pezeshkpour2023large",
        "author": "Pezeshkpour, Pouya and Hruschka, Estevam",
        "title": "Large language models sensitivity to the order of options in multiple-choice questions"
      },
      {
        "key": "alzahrani2024benchmarks",
        "author": "Alzahrani, Norah and Alyahya, Hisham Abdullah and Alnumay, Yazeed and Alrashed, Sultan and Alsubaie, Shaykhah and Almushaykeh, Yusef and Mirza, Faisal and Alotaibi, Nouf and Altwairesh, Nora and Alowisheq, Areeb and others",
        "title": "When benchmarks are targets: {R}evealing the sensitivity of large language model leaderboards"
      },
      {
        "key": "zheng2023large",
        "author": "Zheng, Chujie and Zhou, Hao and Meng, Fandong and Zhou, Jie and Huang, Minlie",
        "title": "Large language models are not robust multiple choice selectors"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "chiang2024chatbot",
        "author": "Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhu, Banghua and Zhang, Hao and Jordan, Michael and Gonzalez, Joseph E and others",
        "title": "Chatbot {A}rena: {A}n {O}pen {P}latform for {E}valuating {LLM}s by {H}uman {P}reference"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "ravaut2024much",
        "author": "Mathieu Ravaut  et al.",
        "title": "How Much are LLMs Contaminated? A Comprehensive Survey and the LLMSanitize Library"
      },
      {
        "key": "yang2023rethinking",
        "author": "Shuo Yang et al.",
        "title": "Rethinking benchmark and contamination for language models with rephrased samples"
      },
      {
        "key": "zhou2023don",
        "author": "Kun Zhou et al.",
        "title": "Don't Make Your LLM an Evaluation Benchmark Cheater"
      },
      {
        "key": "dada2024clue",
        "author": "Dada, Amin and Bauer, Marie and Contreras, Amanda Butler and Kora{\\c{s}}, Osman Alperen and Seibold, Constantin Marc and Smith, Kaleb E and Kleesiek, Jens",
        "title": "CLUE: A Clinical Language Understanding Evaluation for LLMs"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "han2023medalpaca",
        "author": "Tianyu Han et al.",
        "title": "MedAlpaca--an open-source collection of medical conversational AI models and training data"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "bedi2024systematic",
        "author": "Bedi, Suhana and Liu, Yutong and Orr-Ewing, Lucy and Dash, Dev and Koyejo, Sanmi and Callahan, Alison and Fries, Jason A and Wornow, Michael and Swaminathan, Akshay and Lehmann, Lisa Soleymani and others",
        "title": "A Systematic Review of Testing and Evaluation of Healthcare Applications of Large Language Models (LLMs)"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "dada2024clue",
        "author": "Dada, Amin and Bauer, Marie and Contreras, Amanda Butler and Kora{\\c{s}}, Osman Alperen and Seibold, Constantin Marc and Smith, Kaleb E and Kleesiek, Jens",
        "title": "CLUE: A Clinical Language Understanding Evaluation for LLMs"
      },
      {
        "key": "kanithi2024medic",
        "author": "Kanithi, Praveen K and Christophe, Cl{\\'e}ment and Pimentel, Marco AF and Raha, Tathagata and Saadi, Nada and Javed, Hamza and Maslenkova, Svetlana and Hayat, Nasir and Rajan, Ronnie and Khan, Shadab",
        "title": "MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications"
      }
    ]
  }
]