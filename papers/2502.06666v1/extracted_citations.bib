@article{alzahrani2024benchmarks,
  title={When benchmarks are targets: {R}evealing the sensitivity of large language model leaderboards},
  author={Alzahrani, Norah and Alyahya, Hisham Abdullah and Alnumay, Yazeed and Alrashed, Sultan and Alsubaie, Shaykhah and Almushaykeh, Yusef and Mirza, Faisal and Alotaibi, Nouf and Altwairesh, Nora and Alowisheq, Areeb and others},
  journal={arXiv preprint arXiv:2402.01781},
  year={2024}
}

@article{bedi2024systematic,
  title={A Systematic Review of Testing and Evaluation of Healthcare Applications of Large Language Models (LLMs)},
  author={Bedi, Suhana and Liu, Yutong and Orr-Ewing, Lucy and Dash, Dev and Koyejo, Sanmi and Callahan, Alison and Fries, Jason A and Wornow, Michael and Swaminathan, Akshay and Lehmann, Lisa Soleymani and others},
  journal={medRxiv},
  pages={2024--04},
  year={2024},
  publisher={Cold Spring Harbor Laboratory Press}
}

@article{chen2023large,
  title={Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations},
  author={Chen, Qingyu and Du, Jingcheng and Hu, Yan and Keloth, Vipina Kuttichi and Peng, Xueqing and Raja, Kalpana and Zhang, Rui and Lu, Zhiyong and Xu, Hua},
  journal={arXiv preprint arXiv:2305.16326},
  year={2023}
}

@inproceedings{chiang2024chatbot,
  title={Chatbot {A}rena: {A}n {O}pen {P}latform for {E}valuating {LLM}s by {H}uman {P}reference},
  author={Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhu, Banghua and Zhang, Hao and Jordan, Michael and Gonzalez, Joseph E and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{dada2024clue,
  title={CLUE: A Clinical Language Understanding Evaluation for LLMs},
  author={Dada, Amin and Bauer, Marie and Contreras, Amanda Butler and Kora{\c{s}}, Osman Alperen and Seibold, Constantin Marc and Smith, Kaleb E and Kleesiek, Jens},
  journal={arXiv preprint arXiv:2404.04067},
  year={2024}
}

@inproceedings{fleming2024medalign,
  title={Medalign: A clinician-generated dataset for instruction following with electronic medical records},
  author={Fleming, Scott L and Lozano, Alejandro and Haberkorn, William J and Jindal, Jenelle A and Reis, Eduardo and Thapa, Rahul and Blankemeier, Louis and Genkins, Julian Z and Steinberg, Ethan and Nayak, Ashwin and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={20},
  pages={22021--22030},
  year={2024}
}

@article{han2023medalpaca,
  title={MedAlpaca--an open-source collection of medical conversational AI models and training data},
  author={Tianyu Han et al.}, 
  journal={arXiv preprint arXiv:2304.08247},
  year={2023}
}

@article{kanithi2024medic,
  title={MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications},
  author={Kanithi, Praveen K and Christophe, Cl{\'e}ment and Pimentel, Marco AF and Raha, Tathagata and Saadi, Nada and Javed, Hamza and Maslenkova, Svetlana and Hayat, Nasir and Rajan, Ronnie and Khan, Shadab},
  journal={arXiv preprint arXiv:2409.07314},
  year={2024}
}

@inproceedings{nimah2023nlg,
  title={NLG Evaluation Metrics Beyond Correlation Analysis: An Empirical Metric Preference Checklist},
  author={Nimah, Iftitahu and Fang, Meng and Menkovski, Vlado and Pechenizkiy, Mykola},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1240--1266},
  year={2023}
}

@article{pezeshkpour2023large,
  title={Large language models sensitivity to the order of options in multiple-choice questions},
  author={Pezeshkpour, Pouya and Hruschka, Estevam},
  journal={arXiv preprint arXiv:2308.11483},
  year={2023}
}

@article{ravaut2024much,
  title={How Much are LLMs Contaminated? A Comprehensive Survey and the LLMSanitize Library},
  author={Mathieu Ravaut  et al.},
  journal={arXiv preprint arXiv:2404.00699},
  year={2024}
}

@article{yang2023rethinking,
  title={Rethinking benchmark and contamination for language models with rephrased samples},
  author={Shuo Yang et al.}, 
  journal={arXiv preprint arXiv:2311.04850},
  year={2023}
}

@inproceedings{zheng2023large,
  title={Large language models are not robust multiple choice selectors},
  author={Zheng, Chujie and Zhou, Hao and Meng, Fandong and Zhou, Jie and Huang, Minlie},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{zhou2023don,
  title={Don't Make Your LLM an Evaluation Benchmark Cheater},
  author={Kun Zhou et al.}, 
  journal={arXiv preprint arXiv:2311.01964},
  year={2023}
}

