% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
% \pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

\usepackage{longtable}
\usepackage{amssymb}
\usepackage{amsmath}
\newcommand{\prob}{\mathbb{P}}
\usepackage{adjustbox}
% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}
\usepackage{subcaption}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
\usepackage{hyperref}
% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
%\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}


\usepackage{booktabs}
\usepackage{multirow}
\usepackage{colortbl}

\usepackage{txfonts}

\usepackage[status=final,nomargin,inline,lang=spanish]{fixme}
%\fxusetheme{colorsig}
\FXRegisterAuthor{dg}{adg}{}

\newcommand{\careqa}{\texttt{CareQA}} %\_BENCH}}
%\newcommand{\careqa}{\texttt{ANONIMIZED}} %\_BENCH}}
\newcommand{\relaxed}{\texttt{Relaxed Perplexity}}


\title{Automatic Evaluation of Healthcare LLMs Beyond Question-Answering}

\author{
    \textbf{Anna Arias-Duart\textsuperscript{\dag}\textsuperscript{1}},
    \textbf{Pablo Agustin Martin-Torres\textsuperscript{\dag}\textsuperscript{1}},
    \textbf{Daniel Hinjos\textsuperscript{1}},
\\
    \textbf{Pablo Bernabeu-Perez\textsuperscript{1}},
    \textbf{Lucia Urcelay Ganzabal\textsuperscript{3}},
    \textbf{Marta Gonzalez Mallo\textsuperscript{1}},
\\
    \textbf{Ashwin Kumar Gururajan\textsuperscript{1}},
    \textbf{Enrique Lopez-Cuena\textsuperscript{1}},
\\
    \textbf{Sergio Alvarez-Napagao\textsuperscript{1,2}},
    \textbf{Dario Garcia-Gasulla\textsuperscript{1}}
\\
\\
    \textsuperscript{\dag} Equal contribution. \textsuperscript{1} Barcelona Supercomputing Center (BSC) \\
    \textsuperscript{2} Universitat Politècnica de Catalunya (UPC)–BarcelonaTech \\
    \textsuperscript{3} Independent Researcher (formerly affiliated with BSC) \\
}

\newcommand{\wrt}{{\it w.r.t. }}   % with respect to
\newcommand{\eg}{\emph{e.g.}, }       % for example
\newcommand{\ie}{\emph{i.e.}, }      % that is
\newcommand{\etal}{\emph{et al.}}         % and others
\newcommand\etc{\emph{etc.}}

\pagestyle{plain}

\begin{document}
\maketitle
\begin{abstract}
%Large Language Models (LLMs) have the potential to increase the %efficiency and 
%quality of healthcare worldwide. But, there are huge inherent risks associated with applications in such a sensitive domain. While studies already show LLMs are capable of matching expert human performance in some medical benchmarks, concerns regarding the reliability %and consistency
%of their evaluation persist. This study expands the evaluation of healthcare LLMs by extending assessment tasks, benchmarks, and metrics beyond the traditional closed-ended format. We present a detailed analysis of the correlations between various metrics and benchmarks, introduce a novel medical benchmark~--\careqa{}--~and a new metric for open-ended evaluations: Relaxed Perplexity.

Current Large Language Models (LLMs) benchmarks are often based on open-ended or close-ended QA evaluations, avoiding the requirement of human labor. Close-ended measurements evaluate the factuality of responses but lack expressiveness. Open-ended capture the model's capacity to produce discourse responses but are harder to assess for correctness. %Both these approaches are being used, sometimes together sometimes separately, while the relationship between both remains unclear.
These two approaches are commonly used, either independently or together, though their relationship remains poorly understood. This work is focused on the healthcare domain, where both factuality and discourse matter greatly. It introduces a comprehensive, multi-axis suite for healthcare LLM evaluation, exploring correlations between open and close benchmarks and metrics. Findings include blind spots and overlaps in current methodologies. As an updated sanity check, we release a new medical benchmark ~---\careqa{}---~, with both open and closed variants. Finally, we propose a novel metric for open-ended evaluations ~---\relaxed{}---~ to mitigate the identified limitations.
%quality of healthcare worldwide. But, there are huge inherent risks associated with applications in such a sensitive domain. While studies already show LLMs are capable of matching expert human performance in some medical benchmarks, concerns regarding the reliability %and consistency
%of their evaluation persist. This study expands the evaluation of healthcare LLMs by extending assessment tasks, benchmarks, and metrics beyond the traditional closed-ended format. We present a detailed analysis of the correlations between various metrics and benchmarks, introduce a novel medical benchmark~--\careqa{}--~and a new metric for open-ended evaluations: Relaxed Perplexity.


\end{abstract}




\input{latex/tables/table_5}


\section{Introduction}

%The rise of LLMs as general-purpose models leads to their application in domains of public interest. Among those, human healthcare is one of the most appealing given its potential for improving the quality of life of people all over the world~\cite{he2023survey}. However, because of its potential impact, the use of LLMs for healthcare also calls for the highest degrees of caution and careful consideration.


The growing use of large language models (LLMs) in public domains, such as healthcare, shows promise for improving global quality of life~\cite{he2023survey}. At the same time, the reliability and evaluation of LLMs in such sensitive topics requires extreme caution due to the potential impact on people's rights and well-being. 
%Recently, LLMs have been reported capable of matching human performance at several healthcare benchmarks (\eg official medical license examinations~\cite{singhal2023towards}). 

LLM evaluation today is approached through various perspectives, which consider different types of LLM assessment: automatic evaluation (scalable and factual), user evaluation (utility and usability)~\cite{chiang2024chatbot}, and expert evaluation (support and coherence)~\cite{chen2023large}. While each of these evaluation perspectives serves distinct roles that contribute to a holistic assessment, automatic evaluation remains the most prevalent one due to its lack of dependency on human effort. 

Within automatic evaluation, there are two types of tests. Those which include closed-ended responses \cite{bedi2024systematic}, namely multiple-choice question answering (MCQA), and those which have open-ended responses \cite{dada2024clue}. Close-ended MCQA validation enables the automatic verification of response factuality, but it does not reflect the complex nature of real world situations (\eg clinical settings \cite{hager2024evaluation, zhou2023survey}). As such, MCQA alone often fails to identify critical short-comings of model performance~\cite{li2024can,umapathi2023med,ahmad2023creating,pezeshkpour2023large, alzahrani2024benchmarks, zheng2023large}. 

To incorporate a broader range of tasks relevant to the medical field \cite{dada2024clue, kanithi2024medic}, one typically has to rely on open-ended answers. That is, reference responses are not the only valid outputs. Since these cannot be completely assessed for factuality without human expert supervision, approximate measures based on n-grams and model perplexity remain in place, which limits the reliability of these  evaluations \cite{kamalloo2023evaluating}.

Efforts have been dedicated to analyze the relation between automatic evaluations and either user or expert evaluations, showing a lack of direct correspondence~\cite{fleming2024medalign,nimah2023nlg}. This is explained by the difference in the model features these assess (\eg factuality vs usability vs support capacity), pointing at their complementary nature. Nonetheless, a similar analysis within the family of automatic evaluations is still pending; a study of the relations between open-ended and close-ended benchmarks and metrics, to understand which of these tests should be used, and when. For that purpose, we focus on the healthcare domain, providing the following contributions:

%However, despite these efforts, key challenges remain neglected. The correlation between closed-ended and open-ended tasks, for instance, is unclear, making it difficult to determine how performance in one task type translates to the other. Additionally, while accuracy is used to evaluate closed-ended tasks, % as it measures the model's ability to select the correct answer from predefined options, 
%assessing the quality of open-ended responses is more complex and requires more sophisticated evaluation methods. %This raises the further question of which metrics are most appropriate for evaluating open-ended tasks in order to fully capture the model’s capacity for generating contextually appropriate responses.


%diagnosing complex medical cases~\cite{eriksen2023use}, providing appropriate medical advice in most medical scenarios~\cite{nastasi2023does}, and being able to pass official medical license examinations~\cite{singhal2023towards}. 
%However, the prevalence of hallucinations in LLMs outputs can produce dangerous recommendations in healthcare~\cite{umapathi2023med}, where factuality is key~\cite{ahmad2023creating}. These models are unable to properly cite medical references unless completed with external components (\eg RAG)~\cite{wu2024well}. On top of that, LLMs are known to be easily biased~\cite{nastasi2023does} following the same stereotypes found in their training data.




%However, most studies evaluate model capabilities using only closed-ended approaches \cite{bedi2024systematic}, specifically multiple-choice question answering (MCQA), which do not necessarily align with real clinical settings that typically involve open-ended scenarios rather than limited options to choose from \cite{hager2024evaluation, zhou2023survey}. % While the closed-ended strategy can serve as a metric for evaluating a model's medical knowledge to some extent, open-ended approaches can better capture the model's capacity to produce discourse responses.  

%To further advance towards the development of safer healthcare-oriented LLMs the focus needs to be put on a broad validation methodology which assesses model performance on tasks that align with its intended use. % This is specially relevant in such a sensible domain as healthcare. 

%Further, thorough and standardised evaluation is likely to be a requirement for eventual regulatory approval as a medical device~\cite{gilbert2023large,gottlieb2023safely}. 


%While entirely discarding any future use of this technology on such a key domain would deny progress in a field that could benefit billions, blindly deploying LLM-based systems for healthcare is negligent. The prevalence of hallucinations in LLMs outputs can produce dangerous recommendations in healthcare~\cite{umapathi2023med}, where factuality is key~\cite{ahmad2023creating}. These models are unable to properly cite medical references unless completed with external components (\eg RAG)~\cite{wu2024well}. On top of that, LLMs are known to be easily biased~\cite{nastasi2023does} following the same stereotypes found in their training data.

%\begin{figure}[t]
%    \centering
%    \includegraphics[width=0.4\textwidth]{latex/figs/three_axis_3.png}
%    \caption{Three different perspectives to evaluate healthcare LLMs.}
%    \label{fig:axis}
%\end{figure} 


%This work contributes to expanding the existing evaluation frameworks for healthcare LLMs by going beyond single QA benchmarks. We include various tasks considered of relevance in the healthcare domain \cite{bedi2024systematic,huang2024comprehensive}, such as writing prescriptions, clinical note-taking, and making treatment recommendations, among others. We utilize different metrics to evaluate open-ended benchmarks, and analyze the correlations between these metrics and benchmarks. Finally, to avoid contamination present in QA benchmarks \cite{dada2024clue}, we release a new medical dataset that ensures results are free from any contamination.



%\section{Related Work}
%Therefore, evaluating healthcare LLMs can be approached through various perspectives: automatic evaluation, user evaluation, and expert evaluation, with automatic evaluation being the most prevalent. Current trends in automatic evaluation are based on public benchmarks, primarily using MCQA. This methodology has limitations, as model answers can be sensitive to changes in the problem setup %, such as the order in which options are presented or the symbols used to represent responses, 
%which can skew model rankings~\cite{pezeshkpour2023large, alzahrani2024benchmarks, zheng2023large}. Additionally, ensuring that close-ended benchmarks remain uncontaminated is becoming increasingly challenging. %In the healthcare domain, open models are primarily trained and evaluated using a limited set of medical benchmarks. %: PubMedQA, MedQA, MedMCQA, and a subset of MMLU. %The widespread use of these test sets raises concerns about contamination, especially as some popular base models have already included them in their training sets (\eg MedAlpaca and MMLU~\cite{han2023medalpaca}).
%According to this work \cite{bedi2024systematic}, 84.2\% of healthcare LLM evaluations focus on QA tasks. While this approach is useful for assessing specific medical knowledge, it overlooks other essential tasks for healthcare LLMs. %For example, evaluating the model’s ability to summarize clinical texts~—generating concise and accurate summaries of clinical notes or patient histories—~is crucial but remains largely underexplored. 



%Another popular alternative to automatic evaluation is the laypeople or user evaluation, typically conducted through crowdsourced A/B preference testing \cite{chiang2024chatbot}. %However, this method can introduce biases due to human subjectivity; evaluators may favor longer responses, represent specific groups such as AI practitioners, or predominantly come from particular geographic regions.
%Finally, the last type of evaluation involves healthcare experts \cite{chen2023large}. Only these professionals can identify subtle but potentially dangerous errors in medical content. %, such as the misinterpretation of symptoms or the suggestion of inappropriate treatments.

%Each of these evaluation perspectives serves distinct roles that contribute to a holistic assessment. % Automatic evaluation provides objective metrics for comparison and ensures a basic level of performance. Laypeople evaluation ensures that healthcare LLMs can effectively communicate complex medical information. Expert evaluation %, on the other hand, validates the core accuracy and safety of these models.
%Together, these three approaches are complementary, offering different perspectives, as evidenced by the low correlation between 
%automatic and user evaluations \cite{nimah2023nlg}, as well as automatic and expert evaluations \cite{fleming2024medalign}.


\begin{itemize}
    %\setlength\itemsep{0.2em}
    %\setlength\parskip{0em}
    \item A correlation-based, empirical analysis of open-ended and close-ended tasks, benchmarks, and metrics.
    \item A novel medical benchmark (\careqa{}) featuring both closed- and open-ended formats for the verification of our findings.
    \item A new metric for open-ended evaluations (\relaxed{}) which fills a gap identified in existing methodologies.
\end{itemize}


%##############################
%Issues with QA-based evaluations only: 
%- contamination (→ update knowledge CareQA, other methods)
%- ??? → by medical field 
%- lack of correlation with performance in certain downstream tasks (→ other eval methods).
%- lack of correlation with human evaluation (leaderboards/arenas ???).
%Solutions:
%- Open ended question evaluation and closer to real tasks benchmarks.



%issues with QA questions
%The evaluation of LLMs and foundation models is complicated by their lack of a specific downstream purpose, and thus, a bounded metric that is representative of the model's ideal behaviour. Current evaluation trends are based on public benchmarks based on multiple choice tests regarding a variety of topics ***\cite{}. This methodology is not without shortcomings, as model answers can be sensitive to changes in the problem setup, such as the order in which the answers are shown or the symbols used to represent the different option responses, which may skew model rankings~\cite{pezeshkpour2023large, alzahrani2024benchmarks, zheng2023large}. The most popular alternative involves humans in the process, typically through crowdsourced A/B preference testing \cite{chiang2024chatbot}. This also introduces significant biases in the assessment due to human subjectivity (human evaluators prefer long responses over short ones, may belong to specific groups like AI practitioners and enthusiasts, mostly originate from a few geographic regions, \etc) ***\cite{}.

%contamination and problem of single benchmark score
%On top of these limitations, it is increasingly difficult to guarantee that benchmarks are not contaminated~ \cite{ravaut2024much,yang2023rethinking,zhou2023don, dada2024clue}, that is, ensuring benchmark test samples have not been leaked into the training data sets of LLMs. Base LLMs are trained on massive, often uncurated, sets of data, and frequently, no guarantee is made regarding their origin or content. Healthcare LLMs are most commonly trained on top of base LLMs, adding a further opacity layer to the process. As a result, single benchmark scores provide very limited confidence and cannot be trusted blindly. This is particularly severe in the domain of healthcare LLMs, where open models are mostly trained and evaluated using the same four medical benchmarks (PubMedQA, MedQA, MedMCQA and a subset of MMLU). The popularity of these test sets makes them a prime suspect for contamination, as some popular base models already include them in their train sets (\eg MedAlpaca and MMLU~\cite{han2023medalpaca}). 


% lack of evaluation of other tasks
% medic, clue
%Another limitation in the current evaluation of LLMs is the focus on question answering tasks. In fact, according to the studies analyzed in this work \cite{bedi2024systematic}, the majority of evaluations (84.2\%) are performed using question answering problems. This is problematic not only due to the limitations we have previously discussed, but also because it neglects other important tasks for healthcare LLMs. For example, tasks such as summarizing clinical texts—evaluating the model's capacity to generate concise and precise summaries of clinical notes or patient histories—are crucial but remain largely unevaluated. 


%Progress is being made in this direction with efforts to incorporate a wider range of tasks relevant to the medical field \cite{dada2024clue, kanithi2024medic}. However, various valuable tasks and metrics remain underexplored in the literature, and their correlations have not been thoroughly analyzed. This raises several critical questions: Is it necessary to evaluate all tasks and benchmarks? Are all metrics equally reliable?

%\input{latex/tables/tasks_table_3}
%\input{latex/tables/tasks_table_4}

\section{Methodology}\label{sec:methodology}

This study considers four different close-ended healthcare tasks, which include nine different datasets (\eg MedQA). These are all assessed using the accuracy metric. At the same time, six open-ended tasks are studied, based on nine distinct datasets (\eg MedText). In this case, eleven different metrics are extracted. Further details are shown in Table~\ref{tab:tasks_bench_metrics}. To assess the consistency within tasks, datasets and metrics, this work considers up to 12 different open LLMs, both specifically tuned for healthcare and general purpose, motivated by previous work~\cite{shoham2024medconceptsqa, kanithi2024medic}.

%We implemented various tasks for both closed-ended settings, extending beyond traditional MCQA, as well as for open-ended tasks, thereby expanding existing frameworks. A detailed summary of all the implemented benchmarks and tasks can be found in Table \ref{tab:tasks_bench_metrics}. Additionally, we introduce in \S\ref{sec:careqa} a new benchmark called \careqa{}, available in two formats: closed and open-ended.

 %Finally, in \S\ref{sec:metrics} the different metrics used are detailed, with particular emphasis on the open-ended context. %These evaluations present greater complexity, as they not only assess factual knowledge but also the model's ability to generate contextually relevant responses, which are essential in healthcare settings.

\subsection{\careqa{}: A Novel Benchmark}\label{sec:careqa}

%Our findings indicate that current healthcare LLMs are not fully capable of handling the diverse and complex tasks required in healthcare settings. This highlights the need not only to expand the scope of tasks and datasets used to evaluate healthcare LLMs but also to address the limitations of existing datasets. 

%As previously discussed, evaluation datasets face a significant limitation due to training data contamination and overfitting. To prevent LLMs from exploiting these weaknesses during evaluations, it is recommended to regularly update the benchmarking datasets. Additionally, 

Updated benchmarks are necessary to prevent both data drift (as human knowledge evolves), and data contamination (as training data crawling efforts scale). To validate the integrity and consistency of existing tests, this work introduces a new benchmark for automatic evaluation, \careqa{}, available in both closed-ended and open-ended formats.

\careqa{} originates from the Spanish Specialised Healthcare Training (MIR) exams by the Spanish \textit{Ministry of Health}. The close-ended version is a MCQA including 5,621 QA pairs across six categories: medicine, nursing, biology, chemistry, psychology, and pharmacology, sourced from the 2020 to 2024 exam editions. \careqa{} is available in both English and Spanish, with the translation performed using GPT-4.
%, offering a comprehensive and challenging assessment tool. 

The open-ended version (English only) was created by rephrasing the questions from the close-ended version using the \href{https://huggingface.co/Qwen/Qwen2.5-72B-Instruct}{Qwen2.5-72B-Instruct} model. After the rephrasing process, the number of suitable questions was reduced to 3,730 QA pairs. This set retains the same categories as the closed-ended version. 

%new 
To ensure the validity of both the translations and rephrasing, 10 annotators conducted a manual review of a total of 360 samples, each reviewed by at least three evaluators. This process achieved a confidence level of 95\% and a margin of error of 5\% approximately.

The translation results were positive, with all three evaluators agreeing on 83.1\% of the questions as correct. Based on this, we considered the translation to be of good quality. However, the percentage of rephrased QA pairs labeled as correct by the three evaluators was 65.8\%. 

To address this, we conducted a second iteration incorporating feedback from human reviewers. The main issue identified was that while the rephrased answers might differ from the ground truth, they could still be considered valid. As a result, a new rephrasing iteration was carried out, explicitly prompting the model to account for this nuance, and questions with multiple valid answers were excluded. This led to the removal of 961 samples, leaving the final \careqa{} (open-ended) dataset with 2,769 QA pairs. Consequently, the percentage of correct labels increased to 73.6\%. See Appendix \ref{apx:novel} for further details.

\subsection{Metrics}\label{sec:metrics}

For close-ended evaluations, the metric of choice is accuracy. In contrast, for open-ended queries, there is a variety of metrics which provide different insights into model performance. This work considers eleven of those, which are sorted into four distinct categories:

\begin{itemize}
\setlength\itemsep{0.2em}
    \setlength\parskip{0em}
    \item \textbf{N-gram based metrics} evaluate the overlap of n-grams between the generated and reference answers. This category includes: ROUGE1, ROUGE2, ROUGEL and BLEU.
    \item \textbf{Semantic similarity metrics} evaluate the semantic similarity between the generated text and reference text, often leveraging embeddings or deep learning models. This includes: BERTScore, BLEURT and MoverScore.
    \item \textbf{Perplexity metrics} assess the predictive capabilities of the model by measuring how well it can predict a sequence of words. This includes: Word Perplexity, Bits per Byte and Byte Perplexity. %We also introduce a new metric in this category: Relaxed Perplexity (\S\ref{sec:relaxed_perplexity}).
    \item \textbf{LLM-judge}: In this category we use the \href{https://huggingface.co/prometheus-eval/prometheus-7b-v2.0}{Prometheus} \cite{kim2024prometheus} model to grade responses based on specific scoring criteria. % that we define for each benchmark.

    

\end{itemize}


%\subsection{Models}\label{sec:models}

%To assess current evaluation methodologies, we focus on open and accessible LLMs. We evaluate both healthcare-specific LLMs and general purpose LLMs, as previous works shows both are competitive in the medical domain~\cite{shoham2024medconceptsqa, kanithi2024medic}.

%The smallest models, ranging from 3.8B to 14B parameters included in the evaluations are: 
% \href{https://huggingface.co/BioMistral/BioMistral-MedMNX}{BioMistral-MedMNX},  \href{https://huggingface.co/johnsnowlabs/JSL-MedLlama-3-8B-v2.0}{JSL-MedLlama-3-8B-v2.0}, \href{https://huggingface.co/microsoft/Phi-3-mini-4k-instruct}{Phi-3-mini-4k-instruct}, \href{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3}{Mistral-7B-Instruct-v0.3}, \href{https://huggingface.co/Qwen/Qwen2-7B-Instruct}{Qwen2-7B-Instruct} \cite{qwen2}, \href{https://huggingface.co/m42-health/Llama3-Med42-8B}{Llama3-Med42-8B} \cite{christophe2024med42}, \href{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}{Meta-Llama-3.1-8B-Instruct}\cite{llama3modelcard}, \href{https://huggingface.co/google/gemma-2-9b-it}{gemma-2-9b-it} \cite{gemma_2024}, \href{https://huggingface.co/01-ai/Yi-1.5-9B-Chat}{Yi-1.5-9B-Chat} \cite{young2024yi} and \href{https://huggingface.co/microsoft/Phi-3-medium-4k-instruct}{Phi-3-medium-4k-instruct}. The largest tested models, ranging from 27B to 72B, are: \href{https://huggingface.co/google/gemma-2-27b-it}{gemma-2-27b-it} \cite{gemma_2024}, \href{https://huggingface.co/01-ai/Yi-1.5-34B-Chat}{Yi-1.5-34B-Chat} \cite{young2024yi}, \href{https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1}{Mixtral-8x7B-Instruct-v0.1} and \href{https://huggingface.co/Qwen/Qwen2-72B-Instruct}{Qwen2-72B-Instruct} \cite{qwen2}.


\section{Experimentation}
%- Issues
%- Correlations
%- Care QA

%For this we need evaluations on an (ideally up-to-date) list of models.

%Here we may include a table of evaluations, a correlations table, a discussion on contamination, a short description of careQA.

%Also relevant: performance drop when you ask the model for the correct answer instead of A,B,C or D.

%Evaluating models using MCQA (Multiple-Choice Question Answering) presents several limitations. One key limitation is training data contamination and overfitting. To ensure LLMs do not cheat in these evaluations, it is recommended to regularly update the benchmarking datasets. Furthermore, as human knowledge progresses, updated benchmarks are needed to prevent data drift. For these reasons, and to validate the sanity of existing MCQA tests, a new benchmark in presented in \S\ref{sec:careqa}. 

%random ordering

\subsection{Correlation of open-ended vs close-ended}

The first experiment conducted studies the correlation between open-ended and close-ended tasks, as detailed in Table \ref{tab:tasks_bench_metrics}. Specifically, we compare the weighted average accuracy from the various MCQA benchmarks against all other close-ended and open-ended tasks and metrics. Figure \ref{fig:correlation_type4} presents the results for the smaller models.


Of all close and open-ended tasks, only clinical note-taking correlates positively with MCQA, and even in this case, correlation is rather weak. In contrast, summarization, question entailment and the remaining close-ended benchmarks correlate negatively with MCQA, except for Med Transcriptions. The rest show a generalized lack of correlation. The negative correlation could be explained by the lack of medical expertise needed for summarizing and entailing (as information is available in the input), and by the diverse nature of close-ended tasks. At metric level, all open alternatives correlate very weakly with MCQA, except for Perplexity, for which we observe a slight correlation. These findings illustrate the relevance of the benchmarks chosen for evaluation, as well as the complementary nature of MCQA, when considering other tasks like summarization or clinical note-taking. Further details in Appendix \ref{apx:corr_elo}.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.46\textwidth]{latex/figs/type_4_correlations_perplexity_prometheus_olaph.png} %prev: type_4_correlations_green_3
    \caption{Correlation between the weighted average accuracy from the MCQA benchmarks and all other close-ended and open-ended tasks and metrics. These results correspond to the smaller models. }
    \label{fig:correlation_type4}
\end{figure} 


\subsection{Correlation of open-ended benchmarks}

The previous section locates open-ended tasks with a variable degree of correlation with close-ended tasks (\eg clinical note-taking, summarization). Let us now analyze correlations within the open-ended category. Details on this are shown in Appendix \ref{apx:metrics_across_bench}.

%To provide a more detailed analysis, we will focus on the three metric clusters outlined in the previous section.
%In general, perplexity metrics demonstrate a high correlation across benchmarks. However, two datasets exhibit a very low correlation: MedDialog Qsumm and MIMIC-III. The former focuses on generating responses to specific patient questions in a conversational context, while the latter is centered on summarization, which condenses existing text. As a result, responses in MedDialog Qsumm often involve constructing new sentences based on the question and relevant medical knowledge. This process can lead to higher perplexity scores, as the responses tend to be less predictable and more varied than those generated for summarization tasks.
%The results with Prometheus also reveal low correlation across benchmarks. The strongest correlation observed with Prometheus is between the MedDialog QSumm dataset and MedText, which focuses on diagnosis and treatment recommendations.
%Turning to the final cluster—metrics based on n-grams and semantic similarity—the highest correlations are seen between MedDialog Raw, which evaluates question entailment, and MedText. Another noteworthy finding is that benchmarks for the same task, such as clinical note-taking, exhibit low correlation. For example, ACI-Bench and MTS-Dialog, both of which evaluate clinical note-taking, show very low correlation.
%In summary, 

Notably, no consistently high correlation is observed for any benchmark or task. This suggests that each benchmark measures distinct aspects of model performance. This is the case even for benchmarks tackling the same task (\eg ACI-Bench and MTS-Dialog), illustrating the importance of benchmark source (\ie who crafted the benchmark and in which context). This underscores the need for specialized evaluations for downstream tasks, as generalization cannot be assumed. 


\subsection{Correlation of open-ended metrics}

To assess whether the metrics used in the open evaluation are correlated among themselves, and to simplify future analyses for practitioners, we conduct a correlation analysis for each of the metrics detailed in \S\ref{sec:metrics} across all implemented open-ended benchmarks (more details in Appendix \ref{apx:bench_across_metrics}).

This analysis identifies three distinct clusters of highly correlated metrics. The first cluster includes the perplexity metrics, (\ie Word Perplexity, Bits per Byte, and Byte Perplexity)
all of which show a correlation above 0.96 across all analyzed benchmarks. Noticeably, these metrics are all based on probabilistic prediction (perplexity) and information efficiency (%measured by 
Bits per Byte). The results obtained from Prometheus (an LLM judge) can be considered a distinct cluster of evaluation, illustrating how an external model provides a different and rather unique perspective on model performance. %, and the correlations—or lack thereof—vary across different benchmarks. For instance, in the MedDialog Qsumm benchmark, which focuses on extracting and summarizing questions from the text, Prometheus demonstrates a strong correlation with metrics based on n-grams. This alignment may arise from the scoring criteria, which emphasize the model's ability to accurately extract or formulate questions that align with the text's content. Consequently, it is expected that Prometheus correlates closely with metrics that assess word overlap.
%However, this trend does not hold in the MIMIC-III benchmark, which focuses on summarization, where Prometheus shows a stronger correlation with perplexity metrics. Given the complexity of the summarization task and the potential requirement for domain-specific knowledge, it is likely that Prometheus is evaluating the coherence and fluency of the output in this case. This focus on coherence and fluency may not align with traditional n-gram metrics, which prioritize direct text overlap.
Finally, the third cluster includes all n-gram-based metrics, %such as ROUGE-1, ROUGE-2, ROUGE-L, and BLEU, 
together with semantic similarity metrics (\ie BERTScore, BLEURT, and MoverScore). A strong correlation among these metrics is consistently observed across benchmarks, which can be attributed to their shared focus on content and overall text quality.



%The three clusters discussed in this section are clearly represented in the correlation matrix of the MedText dataset, as shown in Figure \ref{fig:medtext_2}. These clusters include: (1) the Perplexity metrics, (2) the n-gram-based metrics along with the semantic similarity metrics, and (3) the Prometheus metrics. Additional details regarding the correlation matrices for the other benchmarks can be found in Appendix \ref{apx:bench_across_metrics}.


\subsection{Metrics resilience to rephrasing}

A limitation of open-ended evaluations is their sensitivity to rewording. Let us now analyze the different metrics under this open setup, to better understand their reliability. To do so, the model's output are rephrased, and evaluation recomputed. Six rephrased versions are produced using 
\href{https://huggingface.co/Qwen/Qwen2.5-72B-Instruct}{Qwen2.5-72B-Instruct}. 

Results show that most n-gram-based metrics (\ie ROUGE1, ROUGE2, ROUGEL and BLEU) are resilient to rephrasing. This difference may arise because these metrics rely on surface-level word matching, making them less sensitive to phrasing changes as long as the core vocabulary remains intact. 
\ie in healthcare texts, key terms like `diagnosis,' `treatment,' or medication names often stay consistent, allowing these metrics to maintain a high overlap. In contrast, Prometheus (LLM judge) is the most affected by rewording, which is reasonable considering that, for this evaluation, correct punctuation and formatting in the answers greatly improve scores. This metric is followed by BLEURT and BERTScore (model similarity based) as the least resilient. More details can be found in Appendix \ref{apx:resilience}.

\subsection{Metrics self-consistency}

Another issue that affects LLM evaluation, particularly on the open-ended setup, is the lack of self-consistency across model runs for some widespread sampling strategies, such as top\_p and top\_k. To evaluate its impact on open-ended evaluation, we generate and evaluate 11 responses for each prompt in \careqa{}-Open using top\_p sampling, $p=0.9$. Results can be seen in Figure \ref{fig:self-consistency}. We observe that among n-gram metrics, BLEU and ROUGE2 are the most self consistent. BLEURT and Prometheus (LLM judge) are the less consistent. Perplexity metrics are perfectly self-consistent. More details can be found in Appendix \ref{apx:self-consistency}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.46\textwidth]{latex/figs/self_consistency.png}
    \caption{Mean variance distributions across different 
    runs and averaged across models using the \careqa{}-Open dataset. Closer to 0 means more self-consistent.}
    \label{fig:self-consistency}
\end{figure} 

\section{\relaxed{}: A novel metric}\label{sec:relaxed_perplexity}

By being optimized for next token prediction on the ground truth, LLM's are optimized for perplexity. However, as seen before, this does not necessarily entail good performance on open or close-ended downstream tasks. Additionally, perplexity can be greatly impacted by instruct-tuning and alignment techniques~\cite{lee2024mechanistic}. On the other hand, it has been widely noted that models are more likely to arrive at the correct answer after outputting intermediate tokens, commonly known as chain of thought (CoT) \cite{suzgun2022challenging, wang2023towards}, and that this happens even without specific CoT prompting \cite{wang2024chain}. However, perplexity fails to capture this improvement, and can be negatively impacted by the presence of intermediate tokens.

To evaluate factuality in open-ended benchmarks, with no dependence on confounders or exact formulation while accounting for the potential benefits of intermediate tokens, we propose \relaxed{}. Given a \emph{question} and a \emph{target}, we wish to estimate
\begin{equation*}
    \begin{split}
    \mathbb{P}(target \sim \text{model} \mid question) =  \\
    = \prob(A_0) + \hdots + \prob(A_n \mid B_n)
    \end{split}
\end{equation*}
that is, the probability that the target is sampled from the model given the prompt, at any time in the completion. We denote the events $A_n \equiv \{target \sim \text{model}(question + seq_n)\}$ and $B_n \equiv \{seq_n \sim \text{model}(question)\}$ for any $seq_n$ of $n$ tokens that comes from the model before the target. We can estimate $\prob(A_n \mid B_n)$ as \begin{align*}
    \prob(A_n \mid B_n) \approx \prob(A_n \mid seq^{i_1}_n) + \hdots 
    + \prob(A_n \mid seq^{i_\ell}_n)
\end{align*} for the $\ell$ more likely $n$-token sequences sampled from the model given \emph{question}, because the events $seq^i_n$ and $seq^j_n$ are mutually exclusive. In this notation, $\prob(seq_n^{i_{\ell}}) := \prob(seq_n^{i_{\ell}} \sim \text{model}(question))$. Using this, we can define \relaxed{} as
\begin{equation*}
    \begin{split}
    \text{Relaxed-Perplexity}(target, question, model) = \\
    = \exp\left(-\frac{1}{n + len(target)} \sum_{i=0}^n log P(A_i \mid B_i)\right)
    \end{split}
\end{equation*}
This allows to evaluate correctness in the model's answers probability distribution, with no regard for the exact formulation. Further, for a given prompt and fixed sampling parameters, the metric is perfectly self consistent. We thus test it with the Olaph \cite{jeong2024olaph} medical factuality dataset. In contrast to Perplexity, we observe that \relaxed{} assigns higher scores to models fine-tuned on healthcare datasets. More details on the mathematical formulation, implementation and results of \relaxed{} can be found in Appendix \ref{apx:relaxed_perplexity}.



\section{Conclusions}

This study finds very weak correlations between close-ended and open-ended benchmarks. These results highlight the complementary roles of close-ended and open-ended approaches, and the limited insights provided by individual tests. It thus advocates for broader evaluation setups. Even within open-ended benchmarks targeting the same task (\eg ACI-Bench and MTS-Dialog), no consistently high correlations were found. This indicates that different benchmarks assess distinct model capabilities, underscoring the significance of the benchmark's design.

The analysis of evaluation metrics for open-ended benchmarks identified three distinct clusters that are particularly relevant for assessing medical models: (1) perplexity-based metrics, (2) n-gram-based metrics combined with semantic similarity metrics, and (3) LLM-as-a-judge metrics. Notably, none of these clusters showed strong correlations with the close-ended MCQA evaluation. Additionally, differences in resilience to answer rephrasing and self-consistency were observed, due to the distinct ways these metrics are computed.  

%Different levels of resilience and self-consistency were observed among the metrics, due to the distinct ways these metrics are computed. %The most resilient were the ROUGE metrics, while BLEURT and BERTScore showed greater vulnerability to rephrasing. Prometheus also showed lower resilience, since it is particularly sensitive to punctuation and formatting changes.

The findings highlight the importance of selecting appropriate benchmarks and evaluation metrics designed for specific tasks. In this regard, the introduced \careqa{} benchmark, featuring both closed- and open-ended formats, serves as a sanity check of existing tests, while the proposed \relaxed{} metric fills a gap in evaluation by focusing on factuality and being resistant to exact formulations in an open-ended setting.



%This study finds very weak correlations between close-ended and open-ended benchmarks. Even within benchmarks of the same task, correlations are weak. This points at the limited insights provided by individual tests, and advocates for broader evaluation setups. The experimentation shown identifies three clusters of metrics that should be considered when evaluating medical models: (1) perplexity metrics, (2) n-gram-based metrics combined with semantic similarity metrics, and (3) LLM-as-a-judge metrics. Noticeably, none of them correlates strongly with the close-ended MCQA evaluation.

%The introduced \careqa{} benchmark, featuring both closed- and open-ended formats, serves as a sanity check of existing tests, while the proposed \relaxed{} metric fills a gap in evaluation, by focusing on factuality while being resistant to exact formulations in an open-ended setting.


\section{Limitations} 

Since this study is based on specific models, the findings may not generalize to other LLM architectures. Additionally, the quality and diversity of the datasets used for evaluation are limited, meaning these benchmarks may not fully capture the performance of LLMs across the broader healthcare landscape. While metrics and benchmarks can indicate how well LLMs perform on certain tasks, they may not reflect the complexities of integrating LLMs into real-world healthcare practices. 

In evaluating the models, we observed that applying the model’s chat template to MCQA tasks led to decreased performance, whereas open-ended evaluations showed improvement. To ensure a fair comparison between open-ended and MCQA evaluations, we maintained the same configuration across both categories and did not apply the model's chat template to any of the evaluations.


Regarding the new benchmark introduced, although subject matter experts created the original exam materials, which underwent public scrutiny, \careqa{} has not been subjected to formal bias assessment. Consequently, it may not adequately represent the full spectrum of medical knowledge or encompass all possible patient demographics. Furthermore, although a human review was performed on the open-ended version, it has not undergone thorough evaluation by healthcare experts, raising the possibility of errors or biases introduced by the LLM used to rephrase the questions. Therefore, we advise users to exercise caution when interpreting and generalizing the results.

All experiments are conducted on English benchmarks  (except for the Spanish version of \careqa{}), and generalization to other languages has not been considered. To enable reproducibility, all resources are made available. \careqa{} is accessible on Hugging Face\footnote{\url{https://huggingface.co/datasets/HPAI-BSC/CareQA}} and all new tasks are accessible in the original \textit{lm-evaluation-harness} framework\footnote{\url{https://github.com/EleutherAI/lm-evaluation-harness}}.


\section*{Acknowledgements}
This work is supported by Anna Arias Duart, Pablo Agustin Martin Torres and Daniel Hinjos García fellowships within the “Generación D” initiative, \href{https://www.red.es/es}{Red.es}, Ministerio para la Transformación Digital y de la Función Pública, for talent atraction (C005/24-ED CV1). Funded by the European Union NextGenerationEU funds, through PRTR.

We also acknowledge the computational resources provided by the FinisTerrae III, Leonardo, and MareNostrum 5 supercomputers. We are particularly grateful to the Operations department at BSC for their technical support.

Lastly, we sincerely thank Jordi Bayarri-Planas, Atia Cortés, Orlando Montenegro and Òscar Molina for their valuable time and feedback during the human evaluation process.





% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix
% \onecolumn
\include{latex/appendix_novel_bench}


\input{latex/appendix_correlations}
\input{latex/appendix_resilience}

\input{latex/appendix_relaxed_perplexity}
%\section{Evaluations}

%\subsection{Open-ended}


\input{latex/appendix_results}


\label{sec:perplexity}


\end{document}
