\section{Related Work}
%Therefore, evaluating healthcare LLMs can be approached through various perspectives: automatic evaluation, user evaluation, and expert evaluation, with automatic evaluation being the most prevalent. Current trends in automatic evaluation are based on public benchmarks, primarily using MCQA. This methodology has limitations, as model answers can be sensitive to changes in the problem setup %, such as the order in which options are presented or the symbols used to represent responses, 
%which can skew model rankings____. Additionally, ensuring that close-ended benchmarks remain uncontaminated is becoming increasingly challenging. %In the healthcare domain, open models are primarily trained and evaluated using a limited set of medical benchmarks. %: PubMedQA, MedQA, MedMCQA, and a subset of MMLU. %The widespread use of these test sets raises concerns about contamination, especially as some popular base models have already included them in their training sets (\eg MedAlpaca and MMLU____).
%According to this work ____, 84.2\% of healthcare LLM evaluations focus on QA tasks. While this approach is useful for assessing specific medical knowledge, it overlooks other essential tasks for healthcare LLMs. %For example, evaluating the model’s ability to summarize clinical texts~—generating concise and accurate summaries of clinical notes or patient histories—~is crucial but remains largely underexplored. 



%Another popular alternative to automatic evaluation is the laypeople or user evaluation, typically conducted through crowdsourced A/B preference testing ____. %However, this method can introduce biases due to human subjectivity; evaluators may favor longer responses, represent specific groups such as AI practitioners, or predominantly come from particular geographic regions.
%Finally, the last type of evaluation involves healthcare experts ____. Only these professionals can identify subtle but potentially dangerous errors in medical content. %, such as the misinterpretation of symptoms or the suggestion of inappropriate treatments.

%Each of these evaluation perspectives serves distinct roles that contribute to a holistic assessment. % Automatic evaluation provides objective metrics for comparison and ensures a basic level of performance. Laypeople evaluation ensures that healthcare LLMs can effectively communicate complex medical information. Expert evaluation %, on the other hand, validates the core accuracy and safety of these models.
%Together, these three approaches are complementary, offering different perspectives, as evidenced by the low correlation between 
%automatic and user evaluations ____, as well as automatic and expert evaluations ____.


\begin{itemize}
    %\setlength\itemsep{0.2em}
    %\setlength\parskip{0em}
    \item A correlation-based, empirical analysis of open-ended and close-ended tasks, benchmarks, and metrics.
    \item A novel medical benchmark (\careqa{}) featuring both closed- and open-ended formats for the verification of our findings.
    \item A new metric for open-ended evaluations (\relaxed{}) which fills a gap identified in existing methodologies.
\end{itemize}


%##############################
%Issues with QA-based evaluations only: 
%- contamination (→ update knowledge CareQA, other methods)
%- ??? → by medical field 
%- lack of correlation with performance in certain downstream tasks (→ other eval methods).
%- lack of correlation with human evaluation (leaderboards/arenas ???).
%Solutions:
%- Open ended question evaluation and closer to real tasks benchmarks.



%issues with QA questions
%The evaluation of LLMs and foundation models is complicated by their lack of a specific downstream purpose, and thus, a bounded metric that is representative of the model's ideal behaviour. Current evaluation trends are based on public benchmarks based on multiple choice tests regarding a variety of topics ***____. This methodology is not without shortcomings, as model answers can be sensitive to changes in the problem setup, such as the order in which the answers are shown or the symbols used to represent the different option responses, which may skew model rankings____. The most popular alternative involves humans in the process, typically through crowdsourced A/B preference testing ____. This also introduces significant biases in the assessment due to human subjectivity (human evaluators prefer long responses over short ones, may belong to specific groups like AI practitioners and enthusiasts, mostly originate from a few geographic regions, \etc) ***____.

%contamination and problem of single benchmark score
%On top of these limitations, it is increasingly difficult to guarantee that benchmarks are not contaminated~ ____, that is, ensuring benchmark test samples have not been leaked into the training data sets of LLMs. Base LLMs are trained on massive, often uncurated, sets of data, and frequently, no guarantee is made regarding their origin or content. Healthcare LLMs are most commonly trained on top of base LLMs, adding a further opacity layer to the process. As a result, single benchmark scores provide very limited confidence and cannot be trusted blindly. This is particularly severe in the domain of healthcare LLMs, where open models are mostly trained and evaluated using the same four medical benchmarks (PubMedQA, MedQA, MedMCQA and a subset of MMLU). The popularity of these test sets makes them a prime suspect for contamination, as some popular base models already include them in their train sets (\eg MedAlpaca and MMLU____). 


% lack of evaluation of other tasks
% medic, clue
%Another limitation in the current evaluation of LLMs is the focus on question answering tasks. In fact, according to the studies analyzed in this work ____, the majority of evaluations (84.2\%) are performed using question answering problems. This is problematic not only due to the limitations we have previously discussed, but also because it neglects other important tasks for healthcare LLMs. For example, tasks such as summarizing clinical texts—evaluating the model's capacity to generate concise and precise summaries of clinical notes or patient histories—are crucial but remain largely unevaluated. 


%Progress is being made in this direction with efforts to incorporate a wider range of tasks relevant to the medical field ____. However, various valuable tasks and metrics remain underexplored in the literature, and their correlations have not been thoroughly analyzed. This raises several critical questions: Is it necessary to evaluate all tasks and benchmarks? Are all metrics equally reliable?

%\input{latex/tables/tasks_table_3}
%\input{latex/tables/tasks_table_4}