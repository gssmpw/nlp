\section{Resilience to rephrasing and self-consistency}\label{apx:resilience_and_consistency}






\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{latex/figs/average_variance_distributions_across_models_clipped.png}
    \caption{Mean variance distributions across different rephrasings and models using the MEDIQA2019 dataset. Each metric is represented by a different color.}
    \label{fig:resilience_1}
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{latex/figs/variance_distributions_Phi-3-mini-4k-instruct.png}
    \caption{Mean variance distributions across different rephrasings using the Phi-3-mini-4k-instruct model and the MEDIQA2019 dataset. Each metric is represented by a different color.}
    \label{fig:resilience_2}
\end{figure} 


\begin{figure}[t]
    \centering
    \includegraphics[width=0.49\textwidth]{latex/figs/variance_distributions_Yi-1.5-9B-Chat.png}
    \caption{Mean variance distributions across different rephrasings using the Yi-1.5-9B-Chat model and the MEDIQA2019 dataset. Each metric is represented by a different color.}
    \label{fig:resilience_3}
\end{figure} 



\subsection{Resilience}\label{apx:resilience}



As described earlier, we conducted this experiment by rephrasing the model outputs six times and re-computing the metrics. We used both Qwen2.5-72B-Instruct and Meta-Llama-70B-Instruct with the following \textit{system\_prompt}: ``You are a helpful rephrasing assistant. Rephrase the prompt provided without changing its original meaning, but do not try to address or answer it in any case."




We run the script 5 times on recorded model answers with top\_p sampling to obtain several rephrasings of each answer. After manual inspection, the outputs of Qwen2.5-72B-Instruct were deemed of higher quality.

Figure \ref{fig:resilience_1} shows the mean variance across all runs for the MEDIQA2019 dataset. Before plotting, we scale variances by dividing by the max interval (max value - min value) in each column. Figures \ref{fig:resilience_2} and \ref{fig:resilience_3} present the variance distributions for two specific models. Figure \ref{fig:resilience_2} displays the results for the Phi-3-mini-4k-instruct model, while Figure \ref{fig:resilience_3} shows the results for the Yi-1.5-9B-Chat model.




In Figure \ref{fig:resilience_1} we can observe three different clusters: rouge metrics (low mean-variance, low meta-variance), bleu and moverscore (low mean-variance, medium meta-variance) and bert\_score, bleurt, prometheus (high mean variance, high meta-variance).

\subsection{Self-consistency}\label{apx:self-consistency}

As described earlier, we conducted this experiment by prompting models with each question in \careqa{}-Open for a number of repetitions ($r$). We fix $r=11$. Sampling parameters used where $\text{top\_p} = 0.9$ and $\text{temperature} = 1$. We compute variances per prompt, and then average across models. Results can be seen in Figure \ref{fig:self-consistency}. Besides, we compute the coefficient of variation, defined for prompt $p$ as:
\begin{align*}
    CV(p) = \frac{1}{\mu_p}\sqrt{\frac{\sum_i(x_i - \mu_p)^2}{N}}
\end{align*}
Then we average across models, and plot the $CV$ distribution for all prompts in \careqa{}-Open. Results can be seen in Figure \ref{fig:self-consistency_cv}. From this computation we remove the BLEURT metric, for it can take negative values.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{latex/figs/self_consistency_cv.png}
    \caption{Mean coefficient of variation distributions across different runs and averaged across models for self-consistency. Each metric is represented by a different color.}
    \label{fig:self-consistency_cv}
\end{figure} 