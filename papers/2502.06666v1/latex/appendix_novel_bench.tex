\section{Novel Benchmarks}\label{apx:novel}

\subsection{\careqa{} (close-ended)}\label{apx:careqa-close}

\careqa{} is a novel benchmark for evaluating healthcare Large Language Models (LLMs) through multiple-choice question answering. \careqa{} was created by collecting exam materials in PDF format from the official Spanish government website. These documents were automatically parsed and then underwent post-processing to ensure data quality. This process involved removing 23 inaccurately parsed instances and excluding officially impugned questions. To enhance global accessibility, the original Spanish questions were translated into English using GPT-4. %, with a subset of 60 instances manually reviewed for translation accuracy.


Each \careqa{} sample contains metadata including a numeric exam identifier, full question text, four answer options, correct answer, exam year, and specialization category. The dataset is available in both Spanish and English, facilitating cross-lingual research. Examples of \careqa{} samples are provided in Figure~\ref{fig:careqa_example_1} and Table~\ref{tab:careqa_examples_1}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\textwidth]{latex/figs/CareQA_medicine.png}
    \caption{\careqa{} example from Medicine category.}
    \label{fig:careqa_example_1}
    \vspace{10px}
\end{figure} 


While \careqa{} shares its source with HeadQA in the Spanish Specialised Healthcare Training (MIR) exams, there is no overlap between the datasets. \careqa{} expands upon its predecessor, covering the years 2020 to 2024 and comprising 5,621 question-answer test pairs, compared to HeadQA's 2,742 test pairs from 2013 to 2017.
% The dataset's composition is illustrated in Figure~\ref{fig:careqa_categories}, which shows the distribution of question categories. Figure~\ref{fig:careqa_categories_yearly} provides a more detailed view, breaking down the category distribution by year to reveal potential temporal trends in exam content.
The dataset's composition is illustrated in Figure~\ref{fig:careqa_categories_yearly}, showing the category distribution by year to reveal potential temporal trends in exam content.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.49\textwidth]{figs/questions_per_category.png}
%     \caption{Category distribution.}
%     \label{fig:careqa_categories}
% \end{figure}

Table~\ref{tab:careqa_stats} presents additional information about the dataset, including the total number of questions per category, the longest and average question and answer lengths (in tokens), and the overall vocabulary size. This comprehensive overview of \careqa{}'s structure and content demonstrates its potential as a valuable resource for evaluating and improving healthcare-focused language models.

\subsection{\careqa{} (open-ended)}\label{apx:careqa-open}
We developed the open-ended dataset by adapting the existing closed-ended \careqa{} dataset through the expansion of the English set. The first step was to filter out questions that contained terms such as "incorrect", "except", "false", "not correct", or "NOT", as these terms indicate that the questions focus on identifying incorrect answers among the provided options. After this filtering, we rephrased the remaining questions into an open-ended format using the \href{https://huggingface.co/Qwen/Qwen2.5-72B-Instruct}{Qwen2.5-72B-Instruct} model, specifically instructing it to only rephrase questions that could be effectively transformed. This process excluded questions that explicitly ask for incorrect options or require a selection from the provided answers. We employed two different prompts for rephrasing, followed by a selection process to determine the best-rephrased version or to discard the question if neither was suitable. 

Initially, the close-ended \careqa{} contained 5,621 QA pairs, but after the rephrasing process, the number of suitable questions for the open-ended version was reduced to 3,730 QA pairs. This new dataset retains the same categories as the closed-ended version, including medicine, nursing, biology, chemistry, psychology, and pharmacology.

\begin{figure*}[t]
    \centering
    \subfloat[]{
        \includegraphics[width=1.0\textwidth]{latex/figs/iter1_.png}
        \label{fig:iterations-1}
    }
    \vspace{1em} 
    \subfloat[]{
        \includegraphics[width=1.0\textwidth]{latex/figs/Iter2.png}
        \label{fig:iterations-2}
    }
    \caption{Iterations with human evaluators to create the \careqa{} dataset in English, including both open and closed versions.}
    \label{fig:iterations}
\end{figure*}

Based on feedback from the human review (detailed in \S\ref{sec:human_eval}), a second iteration of rephrasing was conducted, as illustrated in Figure \ref{fig:iterations}. In this phase, the model was instructed to validate only questions that could be answered exclusively using the ground truth, ensuring there were no alternative correct answers. As a result, 961 questions were removed, reducing the \careqa{} (open-ended) dataset to a total of 2,769 QA pairs. 

Figure \ref{fig:careqa_categories_yearly_open} illustrates the distribution of these 2,769 QA pairs in the open-ended version and examples of QA pairs from both the close-ended and open-ended versions of the \careqa{} dataset are shown in Table \ref{tab:careqa_examples}. Both datasets are publicly available\footnote{\url{https://huggingface.co/datasets/HPAI-BSC/CareQA}}.


\subsection{Human evaluation}\label{sec:human_eval}

To validate the translations performed by GPT-4 for the English version of \careqa, as well as the rephrasing process executed by Qwen2.5-72B-Instruct for the open-ended \careqa, a human evaluation was conducted with 10 human evaluators, including 5 authors of this article.

We selected a total of 260 QA pairs for evaluation, covering both translation and rephrasing. This sample size ensures a confidence level of 95\% with a margin of error of 5\% for translation and 5.73\% for rephrasing. Each question was evaluated by at least three evaluators. 

\input{latex/tables/careqa_human_results_iter1}


The results are shown in Table \ref{tab:evaluation_results_iter1} and correspond to the percentages of correct answers labeled by at least one evaluator, by two evaluators, and by all three evaluators. For both translation and rephrasing, the percentage of questions labeled as correct by at least one evaluator is high (98.6\% for translation and 96.1\% for rephrasing). However, when considering the cases where all three evaluators agreed on the correctness of the QA pair, the percentages drop: 83.1\% for translation and 65.8\% for rephrasing (first iteration).


For translation, the agreement percentage was considered sufficiently high, and the English dataset was deemed valid. In contrast, for the open-ended rephrasing version, the agreement rate was not high enough, so a second iteration of rephrasing, as explained in the previous section, was carried out. After removing invalid questions, the percentage of correct answers increased, see third column of Table \ref{tab:evaluation_results_iter1}. After this second iteration, the open dataset was also considered valid. The final agreement of both tasks grouped per category can be seen in Figure \ref{fig:careqa_categories_correct}.


\input{latex/careqa_examples}

\input{latex/tables/careqa_stats}


\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{latex/figs/questions_per_category_and_year.png}
    \caption{Category distribution per Category and Year (\careqa{} close-ended)}
    \label{fig:careqa_categories_yearly}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{latex/figs/distribution-open-ended_new.png}
    \caption{Category distribution per Category and Year (\careqa{} open-ended).}
    \label{fig:careqa_categories_yearly_open}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{latex/figs/careqa_categories_correct.png}
    \caption{Correctness distribution per Category \careqa{} (open-ended).}
    \label{fig:careqa_categories_correct}
\end{figure*}


\input{latex/tables/careqa_open_table}