\begin{table*}[h]
\centering
\renewcommand{\arraystretch}{1.5} 
\resizebox{2.1\columnwidth}{!}{%
\begin{tabular}{l|ccccccccc}
\toprule
\textbf{Model} & \multicolumn{9}{c}{\textbf{Close-ended}} \\ 
 & \textbf{MedMCQA $\uparrow$} & \textbf{MedQA $\uparrow$} & \textbf{\careqa{} $\uparrow$} & \textbf{multimedqa $\uparrow$} & \textbf{PubMedQA $\uparrow$} & \textbf{Med Text Classification $\uparrow$} & \textbf{Med Transcriptions $\uparrow$} & \textbf{BioRED $\uparrow$} & \textbf{MMLU $\uparrow$} \\ 
\midrule
BioMistral-MedMNX & 0.495 ± 0.008 & 0.515 ± 0.014 & 0.629 ± 0.006 & 0.547 ± 0.006 & 0.776 ± 0.019 & 0.202 ± 0.011 & 0.356 ± 0.007 & 0.216 ± 0.013 & 0.6784 ± 0.034 \\
JSL-MedLlama-3-8B-v2.0 & 0.613 ± 0.008 & 0.617 ± 0.014 & 0.671 ± 0.006 & 0.648 ± 0.006 & 0.742 ± 0.020 & 0.191 ± 0.010 & 0.361 ± 0.007 & 0.254 ± 0.014 & 0.7739 ± 0.0305 \\
Llama3-Med42-8B & 0.603 ± 0.008 & 0.626 ± 0.014 & 0.684 ± 0.006 & 0.642 ± 0.006 & 0.772 ± 0.019 & 0.202 ± 0.011 & 0.377 ± 0.007 & 0.203 ± 0.013 & 0.7525 ± 0.0315 \\
Meta-Llama-3.1-70B-Instruct & 0.722 ± 0.007 & 0.798 ± 0.011 & 0.836 ± 0.005 & 0.764 ± 0.005 & 0.800 ± 0.018 & 0.145 ± 0.003 & 0.381 ± 0.007 & 0.515 ± 0.016 & 0.8711 ± 0.0236 \\
Meta-Llama-3.1-8B-Instruct & 0.593 ± 0.008 & 0.637 ± 0.013 & 0.703 ± 0.006 & 0.638 ± 0.006 & 0.752 ± 0.019 & 0.161 ± 0.003 & 0.334 ± 0.007  & 0.232 ± 0.013 & 0.7621 ± 0.031 \\
Mistral-7B-Instruct-v0.3 & 0.482 ± 0.008 & 0.523 ± 0.014 & 0.607 ± 0.007 & 0.538 ± 0.006 & 0.774 ± 0.019 & 0.178 ± 0.010 & 0.356 ± 0.007 & 0.358 ± 0.015 & 0.661 ± 0.0345 \\
Mixtral-8x7B-Instruct-v0.1 & 0.564  ± 0.008	&  0.614  ± 0.014 & 0.725 ± 0.006   & 0.622  ± 0.006 & 0.796  ± 0.018 & 0.207  ± 0.011 & 0.344 ± 0.007 & 0.352  ± 0.015 & 0.7766 ± 0.0304  \\
Phi-3-medium-4k-instruct & 0.623 ± 0.007 & 0.596 ± 0.014 & 0.769 ± 0.006 & 0.661 ± 0.006 & 0.782 ± 0.018 & 0.048 ± 0.002 & 0.365 ± 0.007 & 0.261 ± 0.014 & 0.8237 ± 0.0275 \\
Phi-3-mini-4k-instruct & 0.572 ± 0.008 & 0.537 ± 0.014 & 0.700 ± 0.006 & 0.604 ± 0.006 & 0.752 ± 0.019 & 0.192 ± 0.003 & 0.367 ± 0.007 & 0.262 ± 0.014 & 0.7398 ± 0.0321 \\
%Qwen2-72B-Instruct & 0.693 ± 0.007 & 0.768 ± 0.012 & 0.833 ± 0.005 & 0.741 ± 0.005 & 0.794 ± 0.018 & - & - & 0.428 ± 0.016  & - & - & 0.8656 ± 0.0247 \\
Qwen2-7B-Instruct & 0.551 ± 0.008 & 0.570 ± 0.014 & 0.681 ± 0.006 & 0.596 ± 0.006 & 0.742 ± 0.020 & 0.225 ± 0.011 & 0.363 ± 0.007 & 0.197 ± 0.013 & 0.7337 ± 0.032 \\
Yi-1.5-34B-Chat & 0.575 ± 0.008 & 0.614 ± 0.014 & 0.732 ± 0.006 & 0.628 ± 0.006 & 0.774 ± 0.019 & 0.301 ± 0.012 & 0.345 ± 0.007 & 0.543 ± 0.016 & 0.7806 ± 0.0298 \\
Yi-1.5-9B-Chat & 0.488 ± 0.008 & 0.515 ± 0.014 & 0.649 ± 0.006 & 0.546 ± 0.006 & 0.774 ± 0.019 & 0.227 ± 0.011 & 0.330 ± 0.007 & 0.537 ± 0.016 & 0.7007 ± 0.0329 \\
%gemma-2-27b-it & 0.616 ± 0.008 & 0.669 ± 0.013 & 0.781 ± 0.006 & 0.670 ± 0.006 & 0.800 ± 0.018 & 0.222 ± 0.011 & 0.005 ± 0.001 &  0.536 ± 0.016 & 0.220 ± 0.004	& 0.331 ± 0.004 & 0.8163 ± 0.0278 \\
%gemma-2-9b-it & 0.566 ± 0.008 & 0.629 ± 0.014 & 0.745 ± 0.006 & 0.625 ± 0.006 & 0.766 ± 0.019 & 0.209 ± 0.011 & 0.221 ± 0.006 & 0.536 ± 0.016 & 0.230  ± 0.004 & 0.399  ± 0.005 & 0.7796 ± 0.0303 \\
\bottomrule
\end{tabular}
} 
\caption{Close-ended results.}
\label{tab:close_ended_benchmarks}
\end{table*}


\iffalse

\begin{table*}[H]
\centering
\renewcommand{\arraystretch}{1.5} 
\resizebox{2.1\columnwidth}{!}{%
\begin{tabular}{l|ccccccccccc}
\toprule
\textbf{Model} & \multicolumn{11}{c}{\textbf{Close-ended}} \\ 
 & \textbf{MedMCQA $\uparrow$} & \textbf{MedQA $\uparrow$} & \textbf{\careqa{} $\uparrow$} & \textbf{multimedqa $\uparrow$} & \textbf{PubMedQA $\uparrow$} & \textbf{MedText (easy) $\uparrow$} & \textbf{MedText (hard) $\uparrow$} & \textbf{BioRED $\uparrow$} & \textbf{Med Text Classification $\uparrow$} & \textbf{Med Transcriptions $\uparrow$} & \textbf{MMLU $\uparrow$} \\ 
\midrule
BioMistral-MedMNX & 0.495 ± 0.008 & 0.515 ± 0.014 & 0.629 ± 0.006 & 0.547 ± 0.006 & 0.776 ± 0.019 & 0.202 ± 0.011 & 0.356 ± 0.007 & 0.216 ± 0.013 &  0.989 ± 0.001 & 0.942 ± 0.002 & 0.6784 ± 0.034 \\
JSL-MedLlama-3-8B-v2.0 & 0.613 ± 0.008 & 0.617 ± 0.014 & 0.671 ± 0.006 & 0.648 ± 0.006 & 0.742 ± 0.020 & 0.191 ± 0.010 & 0.361 ± 0.007 & 0.254 ± 0.014 & 0.993 ± 0.001 & 0.937 ± 0.002 & 0.7739 ± 0.0305 \\
Llama3-Med42-8B & 0.603 ± 0.008 & 0.626 ± 0.014 & 0.684 ± 0.006 & 0.642 ± 0.006 & 0.772 ± 0.019 & 0.202 ± 0.011 & 0.377 ± 0.007 & 0.203 ± 0.013 &  0.978 ± 0.001 & 0.933 ± 0.002 & 0.7525 ± 0.0315 \\
Meta-Llama-3.1-70B-Instruct & 0.722 ± 0.007 & 0.798 ± 0.011 & 0.836 ± 0.005 & 0.764 ± 0.005 & 0.800 ± 0.018 & 0.145 ± 0.003 & 0.381 ± 0.007 & 0.515 ± 0.016 & 0.987 ± 0.001 & 0.934 ± 0.002 & 0.8711 ± 0.0236 \\
Meta-Llama-3.1-8B-Instruct & 0.593 ± 0.008 & 0.637 ± 0.013 & 0.703 ± 0.006 & 0.638 ± 0.006 & 0.752 ± 0.019 & 0.161 ± 0.003 & 0.334 ± 0.007  & 0.232 ± 0.013 & 0.986 ± 0.001 & 0.929 ± 0.002 & 0.7621 ± 0.031 \\
Mistral-7B-Instruct-v0.3 & 0.482 ± 0.008 & 0.523 ± 0.014 & 0.607 ± 0.007 & 0.538 ± 0.006 & 0.774 ± 0.019 & 0.178 ± 0.010 & 0.356 ± 0.007 & 0.358 ± 0.015 &  0.990 ± 0.001 & 0.941 ± 0.002 & 0.661 ± 0.0345 \\
Mixtral-8x7B-Instruct-v0.1 & 0.564  ± 0.008	&  0.614  ± 0.014 & 0.725 ± 0.006   & 0.622  ± 0.006 & 0.796  ± 0.018 & 0.207  ± 0.011 & 0.344 ± 0.007 & 0.352  ± 0.015 &  0.982  ± 0.001 & 0.937  ± 0.002 & 0.7766 ± 0.0304  \\
Phi-3-medium-4k-instruct & 0.623 ± 0.007 & 0.596 ± 0.014 & 0.769 ± 0.006 & 0.661 ± 0.006 & 0.782 ± 0.018 & 0.048 ± 0.002 & 0.365 ± 0.007 & 0.261 ± 0.014 & 0.990 ± 0.001 & 0.933 ± 0.002 & 0.8237 ± 0.0275 \\
Phi-3-mini-4k-instruct & 0.572 ± 0.008 & 0.537 ± 0.014 & 0.700 ± 0.006 & 0.604 ± 0.006 & 0.752 ± 0.019 & 0.192 ± 0.003 & 0.367 ± 0.007 & 0.262 ± 0.014 & 0.983 ± 0.001 & 0.921 ± 0.003 & 0.7398 ± 0.0321 \\
%Qwen2-72B-Instruct & 0.693 ± 0.007 & 0.768 ± 0.012 & 0.833 ± 0.005 & 0.741 ± 0.005 & 0.794 ± 0.018 & - & - & 0.428 ± 0.016  & - & - & 0.8656 ± 0.0247 \\
Qwen2-7B-Instruct & 0.551 ± 0.008 & 0.570 ± 0.014 & 0.681 ± 0.006 & 0.596 ± 0.006 & 0.742 ± 0.020 & 0.225 ± 0.011 & 0.363 ± 0.007 & 0.197 ± 0.013 &  0.976 ± 0.001 & 0.934 ± 0.002 & 0.7337 ± 0.032 \\
Yi-1.5-34B-Chat & 0.575 ± 0.008 & 0.614 ± 0.014 & 0.732 ± 0.006 & 0.628 ± 0.006 & 0.774 ± 0.019 & 0.301 ± 0.012 & 0.345 ± 0.007 & 0.543 ± 0.016 & 0.972 ± 0.002 & 0.938 ± 0.002 & 0.7806 ± 0.0298 \\
Yi-1.5-9B-Chat & 0.488 ± 0.008 & 0.515 ± 0.014 & 0.649 ± 0.006 & 0.546 ± 0.006 & 0.774 ± 0.019 & 0.227 ± 0.011 & 0.330 ± 0.007 & 0.537 ± 0.016 & 0.985 ± 0.001 & 0.934 ± 0.002 & 0.7007 ± 0.0329 \\
%gemma-2-27b-it & 0.616 ± 0.008 & 0.669 ± 0.013 & 0.781 ± 0.006 & 0.670 ± 0.006 & 0.800 ± 0.018 & 0.222 ± 0.011 & 0.005 ± 0.001 &  0.536 ± 0.016 & 0.220 ± 0.004	& 0.331 ± 0.004 & 0.8163 ± 0.0278 \\
%gemma-2-9b-it & 0.566 ± 0.008 & 0.629 ± 0.014 & 0.745 ± 0.006 & 0.625 ± 0.006 & 0.766 ± 0.019 & 0.209 ± 0.011 & 0.221 ± 0.006 & 0.536 ± 0.016 & 0.230  ± 0.004 & 0.399  ± 0.005 & 0.7796 ± 0.0303 \\
\bottomrule
\end{tabular}
} 
\caption{Close-ended results.}
\label{tab:close_ended_benchmarks}
\end{table*}

\fi