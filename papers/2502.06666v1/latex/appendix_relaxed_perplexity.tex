\section{Novel Metric: \relaxed{}} \label{apx:relaxed_perplexity}

As mentioned before, we define \relaxed{} as
\begin{equation*}
\begin{split}
    \text{Relaxed-Perplexity}(target, question, model) = \\
    = \exp\left(-\frac{1}{n + len(target)} \sum_{i=0}^n log P(A_i \mid B_i)\right)
\end{split}
\end{equation*}
for events \[A_n \equiv \{target \sim \text{model}(question + seq_n)\}\] and \[B_n \equiv \{seq_n \sim \text{model}(question)\}.\] That is, $A_n$ is the event that target is sampled from the model inputted with $question + seq_n$, for any $seq_n$ of $n$ tokens. 


\begin{figure}[t]
    \centering
    \includegraphics[width=0.43\textwidth]{latex/figs/type_4_correlations_relaxed_perplexity.png}
    \caption{Correlation between OLAPH - \relaxed{} and the rest of benchmarks.}
    \label{fig:correlations_olaph}
\end{figure} 


Thus, in order to compute $\prob(A_n \mid B_n)$ we need to take into account the probability distribution of all $n$-token model answers when the input is $question$, which is extremely costly (with computational time exponential in $n$). 
In fact, by the law of total probability we would have
\begin{align*}
    \prob(A_n \mid B_n)~\prob(B_n) = \prob(A_n \mid seq^1_n)~\prob(seq^1_n) + \cdots \\
+ \prob(A_n \mid seq^{q^n}_n)~\prob(seq^{q^n}_n)
\end{align*}

$q$ being the size of the vocabulary. This holds because the events $seq^i_n$ and $seq^j_n$ are mutually exclusive. In this notation, $\prob(seq_n^{i_{\ell}}) := \prob(seq_n^{i_{\ell}} \sim \text{model}(question))$, and also $\prob(B_n) = ~\prob(\cup_{i} seq_n^{i})$.

However, given that almost all this combinations of tokens contribute with negligible probabilities to the sum, we can estimate the above quantity as
\begin{align*}
    \prob(A_n \mid B_n) \approx \prob(A_n \mid seq^{i_1}_n)~\prob(seq^{i_1}_n) + \hdots \\
+ \prob(A_n \mid seq^{i_\ell}_n)~\prob(seq^{i_\ell}_n)
\end{align*}
for the $\ell$ more likely $n$-token sequences sampled from the model given \emph{question}, which can be computed efficiently using beam search, diverse beam search \cite{vijayakumar2016diverse} or top\_p sampling. 

Notice that also $\prob(B_n) = 1$ unless stop tokens appeared before in the completion, and then the value decreases for big $n$. In our implementation, where $max\_tokens \in [128, 256]$, stop tokens rarely appear and so we estimate $\prob(B_n) \approx 1$.


Now, there is an issue with this formulation. We noticed that, since $\prob(seq_n^{i})$ is the joint probability of all tokens in the sequence, as $n$ grows this value collapses very quickly. In fact, among the $\ell$ most likely sequences, we may bound 
\begin{align*}
    \frac{1}{c_n} \leq \prob(seq_n^i) \leq \frac{1}{d_n}
\end{align*}
for constants $c_n$ and $d_n$ that only depend on $n$ (for example, take the average and max prob of sequences of that length respectively; also, notice $d_n \leq n$ ). And thus we may take
\begin{align*}
    \prob(A_n \mid B_n) &\approx \\ \frac{c_n + d_n}{2 c_n d_n} \left( \prob(A_n \mid seq^{i_1}_n) + \hdots + \prob(A_n \mid seq^{i_\ell}_n) \right)
\end{align*}

This effectively assigns more value to the target appearing earlier in the completion, benefiting models that do not verbose and biasing comparisons without adding real value, for this constant does not depend on the target. In order to deal with this, we \emph{skew} the models distribution with respect to length by multiplying with the inverse of the constant, and end up with the final approximation:
\begin{align*}
    \prob(A_n \mid B_n) \approx \prob(A_n \mid seq^{i_1}_n) + \hdots + \prob(A_n \mid seq^{i_\ell}_n)
\end{align*}
Notice this step may be omitted depending on the evaluation goal.

\relaxed{} is specifically designed to evaluate factuality in the answers, with no regard for the exact formulation. We thus test it with the OLAPH \cite{jeong2024olaph} dataset, and note that for more effective evaluation of other open-ended benchmarks, some preprocessing of the ground truths must be carried out. 





For our experiments we use top-p sampling, selecting the $\ell \in \{5, 10\}$ best sentences in a search space of $s \in \{10, 100\}$. We observe similar results with all combinations, and so fix $\ell = 5$ and $s = 10$ for better performance.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.88\textwidth]{latex/figs/model_rankings_plot_rotated_xticks_last.png}
    \caption{  
    Ranking results for all models on the OLAPH medical factuality dataset for all metrics. The top position is ranked as 1 and the lowest as 11. Different models are represented in distinct colors. It can be seen there is low agreement across metrics.
    }
    \label{fig:ranking}
\end{figure*} 

\input{latex/tables/olaph_examples}

We add another hyperparameter, which we denote as \emph{stride}, for better efficiency. Instead of computing \( \sum_{i=0}^n log P(A_i \mid B_i)\) we compute \( \sum_{i=0,i+stride}^n log P(A_i \mid B_i)\), which we find to be as effective. We select $stride \in \{8, 16\}$.

The implementation is built using vllm\footnote{\href{https://github.com/vllm-project/vllm}{https://github.com/vllm-project/vllm}}, which provides tools for efficient LLM inference \cite{kwon2023efficient}. It remains as future work to implement \relaxed{} with beam search.

\subsection{Connection with cross-entropy}

The exponent of perplexities can be understood as a cross-entropy. Generally, it corresponds to the bits required to encode the correct answer using the model's distribution. In the case of \relaxed{} we have:
\begin{align*}
    \text{H}(q,P) = -\sum_{i=0}^n log P(A_i \mid B_i)
\end{align*}
This is the cross entropy between two distributions, $q$ and $P$, where $q$ is the delta distribution of the target appearing in the correct position, and $P$ the model's distribution. Thus, this could be understood as the bits required to encode the correct answer \emph{anywhere} in the completion (up to $n$ steps), using the model's (skewed) distribution. 


See Table \ref{tab:relaxed_logprobs_example} for an example usage to evaluate model factuality on healthcare benchmarks. Here, we report Relaxed-CrossEntropy instead of \relaxed{}.







