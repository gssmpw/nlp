\section{Related Works}
\subsection{Contrastive Learning}
In computer vision (CV), contrastive learning has gained popularity for its ability to learn generalizable representations leveraging unlabeled images and videos ____. Prior studies have emphasized the pivotal role of data augmentation in facilitating unsupervised training ____.
Experiments conducted in SimCLR approach ____ highlight the significant impact of data augmentations, which is re-confirmed by MoCo ____ and its modification MoCo v2 ____.
% Being an incremental study of MoCo v1/2, MoCo v3____
AdDA ____ focuses on exploring the effect of dynamic adjustment on augmentation compositions, which enables the network to acquire more generalizable features. 
We adopt the feedback structure ____  in the pretraining period and implement it on a different network architecture, which proves to be more suitable for RL tasks ____.
% Since the transferability of visual features has been well-explored in CV, a growing body of research is focused on rendering this approach as representation learning method in RL tasks____.




\subsection{Representation Learning in RL}
There are extensive works in RL studying the impact of representation learning ____, among which contrastive learning is often applied to acquire useful features ____. CURL ____ trains a visual representation encoder using contrastive loss, significantly improving sampling efficiency over prior pixel-based methods. Proto-RL ____ learns contrastive visual representations in dynamic RL environments without access to task-specific rewards. To make full use of context information, MLR ____ introduces mask-based reconstruction to promote contrastive representation learning in RL. 
However, prior methods rely completely on data collected in target environments, which limits their generalization to unseen scenarios and hinders their adaptability to new tasks or environments. It also leads to additional sampling costs. APE, on the other hand, is pretrained on a distribution of real-world samples that wider than what policy can provide. 

Besides, the interpretability of extracted features is a key focus ____, leading to improved performance and robustness of the agent. The efficiency gains of our method also result from a more interpretable encoder, aiding the agent in capturing key factors of observations in policy-making period.

\begin{figure*}[t]
\centering
\includegraphics[width=0.8\textwidth]{figs/pipeline1.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\caption{APE pipeline for MBRL. The training phase is divided into two parts, namely the Adaptive Pretraining period (within the 
% \textcolor{cyan!80!black}{blue} 
blue area) and the Downstream Policy Learning period (within the 
% \textcolor{yellow!80!black}{yellow}
yellow area). A wide variety of real-world images are augmented using an adaptive data augmentation strategy in the first period, which dynamically updates the sampling probability of each augmentation composition in the next pretraining epoch. In the second stage, the pretrained vision encoder is implemented in a generic RL framework as a perception module for the policy.}
\label{fig1}
\end{figure*}

\subsection{Generalization for Image-Based RL}
Since image augmentation has been successfully applied in CV for improving performance on object classification tasks, different approaches of transformation were investigated and incorporated in RL pipelines ____. 
DrAC ____ contributes to the proper use of data augmentation for actor-critic algorithms and proposes an automatically selecting approach. 
% ATC ____ associates pairs of observations separated by a short time difference and introduces a new kind of augmentation to enable replay of latent images. 
SVEA ____ investigates the factors contributing to instability when employing augmentation within off-policy RL methods. DrQ ____ together with DrQ-v2 ____ introduces a simple augmentation method for model-free RL algorithms utilizing input perturbations and regularization techniques, which we use to evaluate the generality of APE.
However, most previous methods attach more importance to the policy training period and straightforwardly augment the observations of the target environments ____. Thus, they fall short in providing the requisite data diversity, which is essential for generalization over large domain gaps ____. 
On the contrary, APE leverages an adaptively pretrained encoder without neglecting the potential benefits of pretraining augmentation strategy in RL, which has been confirmed in recent studies for its effectiveness in enhancing RL performance ____.

\subsection{Pretrained Visual Encoders for RL}

Instead of training with expensive collected data, researches have also been made to bridge the domain gap between cross-domain datasets and the inputs of the target environments ____. Using a pretrained ResNet encoder, RRL ____ brings a straightforward approach to fuse extracted features into a standard RL pipeline. PIE-G ____ further demonstrates the effectiveness of supervised pretrained encoders by using early layer features from frozen models, with strongly augmented representations. By combining pretrained visual encoder and proprioceptive information, MVP outperforms supervised encoders in motor control tasks ____. While pretrained models in aid of model-free RL have been studied, there lacks exploration on Model-Based Reinforcement Learning (MBRL) algorithms. These methods rely compeletely on reconstructed latents, thus further highlights the significance of representation learning ____.
Besides, extra tasks or sensory data are often needed during policy learning period while APE works without such intensive task-specific data.