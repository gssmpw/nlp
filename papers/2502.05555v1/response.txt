\section{Related Works}
\subsection{Contrastive Learning}
In computer vision (CV), contrastive learning has gained popularity for its ability to learn generalizable representations leveraging unlabeled images and videos **Chen, "Improved Baselines with Momentum Contrast for Unsupervised Visual Representation Learning"**. Prior studies have emphasized the pivotal role of data augmentation in facilitating unsupervised training **Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"**.
Experiments conducted in SimCLR approach **Chen et al., "Improved Baselines with Momentum Contrast for Unsupervised Visual Representation Learning"** highlight the significant impact of data augmentations, which is re-confirmed by MoCo **He et al., "Momentum Contrast for Unsupervised Visual Representation Learning"** and its modification MoCo v2 **Chen et al., "An Empirical Study of Training Wider and Deeper Networks with Data Augmentation"**.
% Being an incremental study of MoCo v1/2, MoCo v3**He et al., "Momentum Contrast for Unsupervised Visual Representation Learning"**
AdDA **Tian et al., "Contrastive Multiview Coding for Lossy Image Compression"** focuses on exploring the effect of dynamic adjustment on augmentation compositions, which enables the network to acquire more generalizable features. 
We adopt the feedback structure **Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"**  in the pretraining period and implement it on a different network architecture, which proves to be more suitable for RL tasks **Laskin et al., " CURL: Contrastive Unsupervised Representation Learning for Reinforcement Learning Domains"**.
% Since the transferability of visual features has been well-explored in CV, a growing body of research is focused on rendering this approach as representation learning method in RL tasks**.



\subsection{Representation Learning in RL}
There are extensive works in RL studying the impact of representation learning **Liu et al., "Proto-RL: Prototypical Representation Learning for Transferable Reinforcement Learning"**, among which contrastive learning is often applied to acquire useful features **Tian et al., "Contrastive Multiview Coding for Lossy Image Compression"**. CURL **Laskin et al., " CURL: Contrastive Unsupervised Representation Learning for Reinforcement Learning Domains"** trains a visual representation encoder using contrastive loss, significantly improving sampling efficiency over prior pixel-based methods. Proto-RL **Liu et al., "Proto-RL: Prototypical Representation Learning for Transferable Reinforcement Learning"** learns contrastive visual representations in dynamic RL environments without access to task-specific rewards. To make full use of context information, MLR **Bao et al., "Memory-Augmented Reinforcement Learning for Task-Agnostic Transfer"** introduces mask-based reconstruction to promote contrastive representation learning in RL. 
However, prior methods rely completely on data collected in target environments, which limits their generalization to unseen scenarios and hinders their adaptability to new tasks or environments. It also leads to additional sampling costs. APE, on the other hand, is pretrained on a distribution of real-world samples that wider than what policy can provide. 

Besides, the interpretability of extracted features is a key focus **Liu et al., "Proto-RL: Prototypical Representation Learning for Transferable Reinforcement Learning"**, leading to improved performance and robustness of the agent. The efficiency gains of our method also result from a more interpretable encoder, aiding the agent in capturing key factors of observations in policy-making period.

\begin{figure*}[t]
\centering
\includegraphics[width=0.8\textwidth]{figs/pipeline1.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\caption{APE pipeline for MBRL. The training phase is divided into two parts, namely the Adaptive Pretraining period (within the 
% \textcolor{cyan!80!black}{blue} 
blue area) and the Downstream Policy Learning period (within the 
% \textcolor{yellow!80!black}{yellow}
yellow area). A wide variety of real-world images are augmented using an adaptive data augmentation strategy in the first period, which dynamically updates the sampling probability of each augmentation composition in the next pretraining epoch. In the second stage, the pretrained vision encoder is implemented in a generic RL framework as a perception module for the policy.}
\label{fig1}
\end{figure*}

\subsection{Generalization for Image-Based RL}
Since image augmentation has been successfully applied in CV for improving performance on object classification tasks, different approaches of transformation were investigated and incorporated in RL pipelines **Tian et al., "Data Augmentation as an Unbiased Preprocessing Technique"**. 
DrAC **Liu et al., " Data Augmentation for Actor-Critic Algorithms"** contributes to the proper use of data augmentation for actor-critic algorithms and proposes an automatically selecting approach. 
% ATC ____ associates pairs of observations separated by a short time difference and introduces a new kind of augmentation to enable replay of latent images. 
SVEA **Janner et al., "When Does Pretraining Help? Studying the Transferability of Image Augmentations"** investigates the factors contributing to instability when employing augmentation within off-policy RL methods. DrQ ____ together with DrQ-v2 ____ introduces a simple augmentation method for model-free RL algorithms utilizing input perturbations and regularization techniques, which we use to evaluate the generality of APE.
However, most previous methods attach more importance to the policy training period and straightforwardly augment the observations of the target environments ____. Thus, they fall short in providing the requisite data diversity, which is essential for generalization over large domain gaps ____. 
On the contrary, APE leverages an adaptively pretrained encoder without neglecting the potential benefits of pretraining augmentation strategy in RL, which has been confirmed in recent studies for its effectiveness in enhancing RL performance ____.

\subsection{Pretrained Visual Encoders for RL}

Instead of training with expensive collected data, researches have also been made to bridge the domain gap between cross-domain datasets and the inputs of the target environments ____. Using a pretrained ResNet encoder, RRL **Stooke et al., "Robust Multi-Agent Reinforcement Learning"** brings a straightforward approach to fuse extracted features into a standard RL pipeline. PIE-G ____ further demonstrates the effectiveness of supervised pretrained encoders by using early layer features from frozen models, with strongly augmented representations. By combining pretrained visual encoder and proprioceptive information, MVP outperforms supervised encoders in motor control tasks ____. While pretrained models in aid of model-free RL have been studied, there lacks exploration on Model-Based Reinforcement Learning (MBRL) algorithms. These methods rely compeletely on reconstructed latents, thus further highlights the significance of representation learning ____.
Besides, extra tasks or sensory data are often needed during policy learning period while APE works without such intensive task-specific data.