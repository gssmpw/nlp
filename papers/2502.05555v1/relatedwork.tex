\section{Related Works}
\subsection{Contrastive Learning}
In computer vision (CV), contrastive learning has gained popularity for its ability to learn generalizable representations leveraging unlabeled images and videos \cite{Oord2018RepresentationLW, SimCLR, MoCo}. Prior studies have emphasized the pivotal role of data augmentation in facilitating unsupervised training \cite{res7, res8, res9}.
Experiments conducted in SimCLR approach \cite{SimCLR} highlight the significant impact of data augmentations, which is re-confirmed by MoCo \cite{MoCo} and its modification MoCo v2 \cite{mocov2}.
% Being an incremental study of MoCo v1/2, MoCo v3\cite{Chen2021AnES}
AdDA \cite{Zhang2023AdaptiveDA} focuses on exploring the effect of dynamic adjustment on augmentation compositions, which enables the network to acquire more generalizable features. 
We adopt the feedback structure \cite{Zhang2023AdaptiveDA}  in the pretraining period and implement it on a different network architecture, which proves to be more suitable for RL tasks \cite{Yuan2022PreTrainedIE}.
% Since the transferability of visual features has been well-explored in CV, a growing body of research is focused on rendering this approach as representation learning method in RL tasks\cite{Zhan2020LearningVR}.




\subsection{Representation Learning in RL}
There are extensive works in RL studying the impact of representation learning \cite{ Lin2020LearningTS, Liu2023RobustRL}, among which contrastive learning is often applied to acquire useful features \cite{Zhan2020LearningVR, Du2021CuriousRL, Schwarzer2021PretrainingRF}. CURL \cite{CURL} trains a visual representation encoder using contrastive loss, significantly improving sampling efficiency over prior pixel-based methods. Proto-RL \cite{Yarats2021ReinforcementLW} learns contrastive visual representations in dynamic RL environments without access to task-specific rewards. To make full use of context information, MLR \cite{Yu2022MaskbasedLR} introduces mask-based reconstruction to promote contrastive representation learning in RL. 
However, prior methods rely completely on data collected in target environments, which limits their generalization to unseen scenarios and hinders their adaptability to new tasks or environments. It also leads to additional sampling costs. APE, on the other hand, is pretrained on a distribution of real-world samples that wider than what policy can provide. 

Besides, the interpretability of extracted features is a key focus \cite{Lin2020SPACEUO, Delfosse2022BoostingOR, Delfosse2024InterpretableCB}, leading to improved performance and robustness of the agent. The efficiency gains of our method also result from a more interpretable encoder, aiding the agent in capturing key factors of observations in policy-making period.

\begin{figure*}[t]
\centering
\includegraphics[width=0.8\textwidth]{figs/pipeline1.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\caption{APE pipeline for MBRL. The training phase is divided into two parts, namely the Adaptive Pretraining period (within the 
% \textcolor{cyan!80!black}{blue} 
blue area) and the Downstream Policy Learning period (within the 
% \textcolor{yellow!80!black}{yellow}
yellow area). A wide variety of real-world images are augmented using an adaptive data augmentation strategy in the first period, which dynamically updates the sampling probability of each augmentation composition in the next pretraining epoch. In the second stage, the pretrained vision encoder is implemented in a generic RL framework as a perception module for the policy.}
\label{fig1}
\end{figure*}

\subsection{Generalization for Image-Based RL}
Since image augmentation has been successfully applied in CV for improving performance on object classification tasks, different approaches of transformation were investigated and incorporated in RL pipelines \cite{Laskin2020ReinforcementLW, Kostrikov2020ImageAI, Stooke2020DecouplingRL}. 
DrAC \cite{Raileanu2021AutomaticDA} contributes to the proper use of data augmentation for actor-critic algorithms and proposes an automatically selecting approach. 
% ATC \cite{Stooke2020DecouplingRL} associates pairs of observations separated by a short time difference and introduces a new kind of augmentation to enable replay of latent images. 
SVEA \cite{Hansen2021StabilizingDQ} investigates the factors contributing to instability when employing augmentation within off-policy RL methods. DrQ \cite{Kostrikov2020ImageAI} together with DrQ-v2 \cite{Yarats2021MasteringVC} introduces a simple augmentation method for model-free RL algorithms utilizing input perturbations and regularization techniques, which we use to evaluate the generality of APE.
However, most previous methods attach more importance to the policy training period and straightforwardly augment the observations of the target environments \cite{Zhao2024AnEG}. Thus, they fall short in providing the requisite data diversity, which is essential for generalization over large domain gaps \cite{Yuan2022PreTrainedIE}. 
On the contrary, APE leverages an adaptively pretrained encoder without neglecting the potential benefits of pretraining augmentation strategy in RL, which has been confirmed in recent studies for its effectiveness in enhancing RL performance \cite{Burns2023WhatMP}.

\subsection{Pretrained Visual Encoders for RL}

Instead of training with expensive collected data, researches have also been made to bridge the domain gap between cross-domain datasets and the inputs of the target environments \cite{Ma2022VIPTU, Hu2023ForPV}. Using a pretrained ResNet encoder, RRL \cite{RRL} brings a straightforward approach to fuse extracted features into a standard RL pipeline. PIE-G \cite{Yuan2022PreTrainedIE} further demonstrates the effectiveness of supervised pretrained encoders by using early layer features from frozen models, with strongly augmented representations. By combining pretrained visual encoder and proprioceptive information, MVP outperforms supervised encoders in motor control tasks \cite{Xiao2022MaskedVP}. While pretrained models in aid of model-free RL have been studied, there lacks exploration on Model-Based Reinforcement Learning (MBRL) algorithms. These methods rely compeletely on reconstructed latents, thus further highlights the significance of representation learning \cite{Poudel2023ReCoReRC}.
Besides, extra tasks or sensory data are often needed during policy learning period while APE works without such intensive task-specific data.