\section{Related Work}
\textbf{Enhance Reasoning Capability of LLMs.}
Recently, a substantial body of research focuses on enhancing the reasoning capabilities of LLMs. These methodologies are primarily divided into two categories: the inference phase and the Post-Training phase. During the inference phase, early studies concentrate on stimulating the model's inherent reasoning abilities by modifying prompts ____. Subsequent research leverages the consistency of multiple inferences by the model ____ or integrates tree search strategies ____ to guide the model towards more accurate decoding processes. However, these approaches do not involve training and heavily rely on the model's intrinsic reasoning capabilities. In the Post-Training phase, SFT ____ and DPO ____ emerge as primary enhancement techniques. These methods depend on human-curated selection of high-quality reasoning trajectories or distillation of responses from stronger models ____ to improve the reasoning performance of smaller or weaker models. Nevertheless, these approaches are time-consuming and unsustainable. RL paradigms, exemplified by PPO ____, GRPO ____, and ReFT ____, effectively address the aforementioned issues but introduce significant GPU memory consumption and training instability challenges. 
\\\textbf{Step-Level Direct Preference Optimization.}
In order to optimize and improve the model's reasoning capability from the step level, CPO ____ aligns each step of the CoT reasoning paths with those of ToT using the inherent preference information in the tree-search process, but it control LLMs to generate the thought data by prompt, which may influent the model generation quality. Step-DPO ____ treats individual reasoning steps as units for preference optimization. However, it utilizes the GPT4 to evaluate the correctness of step, which could bring introduced bias and is expensive.  TPO ____ claims that the policy can potentially learn more effectively from a ranked preference list of responses given the prompt and utilizes adaptive step reward to adjust the reward values of each step in the trajectory.  However, it introduce a stronger form of ``catastrophic forgetting'' and imbalanced distribution of the preference tree reward values.

% \subsection{Tree Search in LLMs}
% Recently, a lot of research conducted tree search methods such as Monte Carlo Tree Search (MCTS) to improve the quality of model responses. ReST-MCTS ____ constructs a tree through MCTS and trains the PRM by collecting reasoning data from oracle final correct answers. The mathematical and reasoning performance of LLM is enhanced through the iterative updates of the policy and PRM. TS-LLM____ proposes an alphazero-based approach to conduct MCTS tree construction and policy learning, with an emphasis on the learning of the value function to optimize the entire tree construction process of MCTS and reduce the search space. AlphaMath____ has proposed an iterative learning framework based on MCTS.

% \subsection{Process Reward Model}
% The tree search process of MCTS in most studies relies heavily on the process supervision signals provided by the process reward model. To distinguish it from the traditional reward model, OpenAI introduced the concept of the Process Reward Model (PRM) and differentiated it from the Outcome Reward Model (ORM). To train the PRM, OpenAI has open-sourced a set of data with 800,000 step-level correctness labels for model-generated solutions, known as PRM800K____, which were all annotated by human labelers. Generating training process supervision signals entails substantial costs for manual labeling. To address this issue, MATH-shepherd____ has proposed a method of conducting backpropagation based on the label signals of leaf nodes through MCTS, thereby collecting process-supervised data through automated labeling. PQM____ proposed a Q-value ranking method through Markov Decision Process (MDP) formulation to capture the intricate dynamics among sequential decisions.