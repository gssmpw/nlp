[
  {
    "index": 0,
    "papers": [
      {
        "key": "gpt3",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      },
      {
        "key": "vicuna",
        "author": "Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.",
        "title": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\\%* ChatGPT Quality"
      },
      {
        "key": "qwen2.5",
        "author": "Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others",
        "title": "Qwen2.5 technical report"
      },
      {
        "key": "llama3",
        "author": "Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others",
        "title": "The llama 3 herd of models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "llava",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      },
      {
        "key": "llava1.5",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae",
        "title": "Improved baselines with visual instruction tuning"
      },
      {
        "key": "qwenvl",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-vl: A frontier large vision-language model with versatile abilities"
      },
      {
        "key": "minigpt4",
        "author": "Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed",
        "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models"
      },
      {
        "key": "llavanext",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae",
        "title": "LLaVA-NeXT: Improved reasoning, OCR, and world knowledge"
      },
      {
        "key": "internvl",
        "author": "Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others",
        "title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks"
      },
      {
        "key": "gpt4v",
        "author": "OpenAI",
        "title": "Gpt-4V(ision) system card"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "llavanextvideo",
        "author": "Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan",
        "title": "LLaVA-NeXT: A Strong Zero-shot Video Understanding Model"
      },
      {
        "key": "llavaov",
        "author": "Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan",
        "title": "Llava-onevision: Easy visual task transfer"
      },
      {
        "key": "llavavideo",
        "author": "Zhang, Yuanhan and Wu, Jinming and Li, Wei and Li, Bo and Ma, Zejun and Liu, Ziwei and Li, Chunyuan",
        "title": "Video Instruction Tuning With Synthetic Data"
      },
      {
        "key": "qwen2vl",
        "author": "Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others",
        "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution"
      },
      {
        "key": "internvl2.5",
        "author": "Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others",
        "title": "Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling"
      },
      {
        "key": "nvila",
        "author": "Liu, Zhijian and Zhu, Ligeng and Shi, Baifeng and Zhang, Zhuoyang and Lou, Yuming and Yang, Shang and Xi, Haocheng and Cao, Shiyi and Gu, Yuxian and Li, Dacheng and others",
        "title": "NVILA: Efficient frontier visual language models"
      },
      {
        "key": "videollama3",
        "author": "Zhang, Boqiang and Li, Kehan and Cheng, Zesen and Hu, Zhiqiang and Yuan, Yuqian and Chen, Guanzheng and Leng, Sicong and Jiang, Yuming and Zhang, Hang and Li, Xin and others",
        "title": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding"
      },
      {
        "key": "qwen2.5vl",
        "author": "Qwen Team",
        "title": "Qwen2.5-VL"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "llavaov",
        "author": "Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan",
        "title": "Llava-onevision: Easy visual task transfer"
      },
      {
        "key": "videollama3",
        "author": "Zhang, Boqiang and Li, Kehan and Cheng, Zesen and Hu, Zhiqiang and Yuan, Yuqian and Chen, Guanzheng and Leng, Sicong and Jiang, Yuming and Zhang, Hang and Li, Xin and others",
        "title": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "mscoco",
        "author": "Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\\'a}r, Piotr and Zitnick, C Lawrence",
        "title": "Microsoft coco captions: Data collection and evaluation server"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "msrvtt",
        "author": "Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong",
        "title": "Msr-vtt: A large video description dataset for bridging video and language"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "dream1k",
        "author": "Wang, Jiawei and Yuan, Liping and Zhang, Yuchen and Sun, Haomiao",
        "title": "Tarsier: Recipes for training and evaluating large video description models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "auroracap",
        "author": "Chai, Wenhao and Song, Enxin and Du, Yilun and Meng, Chenlin and Madhavan, Vashisht and Bar-Tal, Omer and Hwang, Jeng-Neng and Xie, Saining and Manning, Christopher D",
        "title": "AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "detailcaps",
        "author": "Dong, Hongyuan and Li, Jiawen and Wu, Bohong and Wang, Jiacong and Zhang, Yuan and Guo, Haoyuan",
        "title": "Benchmarking and Improving Detail Image Caption"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "comprecap",
        "author": "Lu, Fan and Wu, Wei and Zheng, Kecheng and Ma, Shuailei and Gong, Biao and Liu, Jiawei and Zhai, Wei and Cao, Yang and Shen, Yujun and Zha, Zheng-Jun",
        "title": "Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "mscoco",
        "author": "Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\\'a}r, Piotr and Zitnick, C Lawrence",
        "title": "Microsoft coco captions: Data collection and evaluation server"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "nocaps",
        "author": "Agrawal, Harsh and Desai, Karan and Wang, Yufei and Chen, Xinlei and Jain, Rishabh and Johnson, Mark and Batra, Dhruv and Parikh, Devi and Lee, Stefan and Anderson, Peter",
        "title": "Nocaps: Novel object captioning at scale"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "msrvtt",
        "author": "Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong",
        "title": "Msr-vtt: A large video description dataset for bridging video and language"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "vatex",
        "author": "Wang, Xin and Wu, Jiawei and Chen, Junkun and Li, Lei and Wang, Yuan-Fang and Wang, William Yang",
        "title": "Vatex: A large-scale, high-quality multilingual dataset for video-and-language research"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "papinesi2002bleu",
        "author": "Papinesi, K",
        "title": "Bleu: A method for automatic evaluation of machine translation"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "vedantam2015cider",
        "author": "Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi",
        "title": "Cider: Consensus-based image description evaluation"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "banerjee2005meteor",
        "author": "Banerjee, Satanjeev and Lavie, Alon",
        "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "detailcaps",
        "author": "Dong, Hongyuan and Li, Jiawen and Wu, Bohong and Wang, Jiacong and Zhang, Yuan and Guo, Haoyuan",
        "title": "Benchmarking and Improving Detail Image Caption"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "dream1k",
        "author": "Wang, Jiawei and Yuan, Liping and Zhang, Yuchen and Sun, Haomiao",
        "title": "Tarsier: Recipes for training and evaluating large video description models"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "auroracap",
        "author": "Chai, Wenhao and Song, Enxin and Du, Yilun and Meng, Chenlin and Madhavan, Vashisht and Bar-Tal, Omer and Hwang, Jeng-Neng and Xie, Saining and Manning, Christopher D",
        "title": "AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "comprecap",
        "author": "Lu, Fan and Wu, Wei and Zheng, Kecheng and Ma, Shuailei and Gong, Biao and Liu, Jiawei and Zhai, Wei and Cao, Yang and Shen, Yujun and Zha, Zheng-Jun",
        "title": "Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "dream1k",
        "author": "Wang, Jiawei and Yuan, Liping and Zhang, Yuchen and Sun, Haomiao",
        "title": "Tarsier: Recipes for training and evaluating large video description models"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "comprecap",
        "author": "Lu, Fan and Wu, Wei and Zheng, Kecheng and Ma, Shuailei and Gong, Biao and Liu, Jiawei and Zhai, Wei and Cao, Yang and Shen, Yujun and Zha, Zheng-Jun",
        "title": "Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning"
      }
    ]
  }
]