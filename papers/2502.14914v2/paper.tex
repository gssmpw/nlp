\twocolumn[{
\renewcommand\twocolumn[1][]{#1}
\maketitle
\begin{center}
    % \vspace{-15pt}
    \includegraphics[width=1.0\linewidth]{figs/teaser.pdf}
    % \vspace{-14pt}
    \captionsetup{type=figure}
    \caption{
        An example of image caption (left) and video caption (right). By analyzing the components of captions, we conclude 12 dimensions (9 static dimensions and 4 dynamic dimensions with object number shares on both static and dynamic), which all contribute to a detailed and comprehensive caption. The static dimensions are shared in both images and videos. For video data, there are additional dynamic dimensions as they need to be judged with temporal relations.
    }
    \label{fig:teaser}
    \vspace{3pt}
\end{center}
}]

{
    \blfootnote{
        * Interns at Alibaba Group \\
        $^\dagger$ Corresponding Author 
        }
}

\begin{abstract}
Visual captioning benchmarks have become outdated with the emergence of modern multimodal large language models (MLLMs), as the brief ground-truth sentences and traditional metrics fail to assess detailed captions effectively. While recent benchmarks attempt to address this by focusing on keyword extraction or object-centric evaluation, they remain limited to vague-view or object-view analyses and incomplete visual element coverage. In this paper, we introduce CAPability, a comprehensive multi-view benchmark for evaluating visual captioning across 12 dimensions spanning six critical views. We curate nearly 11K human-annotated images and videos with visual element annotations to evaluate the generated captions. CAPability stably assesses both the correctness and thoroughness of captions using F1-score. By converting annotations to QA pairs, we further introduce a heuristic metric, \textit{know but cannot tell} ($K\bar{T}$), indicating a significant performance gap between QA and caption capabilities. Our work provides the first holistic analysis of MLLMs' captioning abilities, as we identify their strengths and weaknesses across various dimensions, guiding future research to enhance specific aspects of capabilities.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Visual captioning, which translates visual content into textual descriptions, is a fundamental task for both image and video understanding~\cite{fang2015captions, xu2015show}, and forms a significant basis for image and video generation~\cite{peebles2023scalable, wang2023modelscope}. To assess the capabilities of this task, researchers established several visual caption benchmarks in earlier years~\cite{mscoco, nocaps, msrvtt, vatex}.

However, with the rapid development of recent MLLMs~\cite{llava, minigpt4, gpt4v, llavaov, gemini1.5, internvl2.5, nvila, videollama3, qwen2.5vl}, these traditional benchmarks have become outdated. This can be attributed to two main reasons: 1) The ground truths of traditional benchmarks often contain short sentences, missing many details. In contrast, recent MLLMs can produce much more detailed and fine-grained captions than the ground truths. 2) Traditional benchmarks use N-gram-based metrics, such as BLEU~\cite{papinesi2002bleu} and CIDER~\cite{vedantam2015cider}, to directly compare the similarity between generated captions and ground-truth sentences, making evaluations unreliable due to their high sensitivity to sentence style.

\begin{figure}[!t]
\centering
\includegraphics[width=0.48\textwidth]{figs/motivation.pdf}
\caption{The development of visual caption benchmarks. Many works compare the ground-truth with generated sentences, which is vague. CompreCap~\cite{comprecap} uses a scene graph to evaluate only object-related information in caption. Our CAPability considers multiple views and conducts comprehensive evaluation.}
\label{fig:motivation}
% \vspace{-8pt}
\end{figure}

\begin{table}[!t]
\centering
\caption{Views of our designed dimensions. We can treat a caption from the listed six views, and then split each of them into several dimensions.}
\label{tab:dimensions}
\resizebox{0.45\textwidth}{!}{%
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Views} & \textbf{Dimensions} \\ \midrule
Object-Related & \begin{tabular}[c]{@{}l@{}}Object Category, Object Color, \\ (Dynamic) Object Number, Spatial Relation\end{tabular} \\
Global-Related & Scene, Style \\
Text-Related & OCR \\
Camera-Related & Camera Angle, Camera Movement \\
Temporal-Related & Action, Event \\
Knowledge-Related & Character Identification \\ \bottomrule
\end{tabular}%
}
\end{table}

Recently, new visual caption benchmarks have been introduced to update the outdated ones. As illustrated in \cref{fig:motivation}, Dream-1K~\cite{dream1k}, DetailCaps~\cite{detailcaps} and VDC~\cite{auroracap} extract keywords from both generated and reference captions, (\eg, objects in DetailCaps, events in Dream-1K, objects, background, and camera information in VDC), and then compare the extracted information to score the caption, as opposed to earlier methods that directly compared sentences. We name all these methods vague-view evaluation as their evaluations still depend on the level of detail and accuracy of the ground-truth caption, which can suffer from human bias and cumulative errors from repeated extraction and comparison by LLMs. 
CompreCap~\cite{comprecap} extracts object-related annotations from images (\eg, scene graph) without ground-truth captions, thereby focusing on evaluating the object description capabilities of modern image MLLMs. We refer to this as object-view evaluation, as it drops entire sentences as ground truth, and evaluates captions based on object representation. Compared to traditional benchmarks, all these newly introduced approaches aim to provide more precise ground truths and evaluation methods, enhancing the reliability and interpretability of benchmarking.

However, the evaluation of these benchmarks remains incomplete as they focus on a single aspect of captions with limited visual elements, inadequately covering the full caption scope. For instance, they often overlook aspects like scene, text, and style. 
In this paper, we conduct a multi-view evaluation and introduce a new comprehensive visual caption benchmark, CAPability, with 6 views (\ie, object, global, text, camera, temporal, and knowledge) and 12 dimensions. This approach uses complete visual elements rather than caption sentences as annotations to evaluate both correctness and thoroughness for each dimension. The relationship between 6 views and 12 dimensions is listed in \cref{tab:dimensions}, and the designed dimensions are illustrated in \cref{fig:teaser}. We believe these components contribute to a complete caption, as lacking any of them may align the caption with different visual content. There are 9 static and 4 dynamic dimensions, with \textit{object number} encompassing both static and dynamic aspects. 
Dynamic dimensions apply to both images and videos, while static ones are exclusive to video. We collect and manually annotate 11K images and videos for CAPability, providing sufficient samples.

\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{figs/radar.pdf}
\caption{F1-score comparison of SOTA MLLMs on our CAPability. Gemini-1.5-pro~\cite{gemini1.5} achieves the best.}
\label{fig:radar}
% \vspace{-8pt}
\end{figure}

In addition to multi-view annotation, we also focus on improving the evaluation of captions. While most methods assess only the correctness, we argue that considering both correctness and thoroughness of visual elements provides a more comprehensive evaluation for visual captioning.
Therefore, we conduct comprehensive experiments using F1-score as our main metric, supplemented by two other heuristic metrics: hit rate and the \textit{know but cannot tell} ($K\bar{T}$) metric. The F1-score combines precision and recall to reflect both correctness and thoroughness. The hit rate measures dimension-referential thoroughness, while $K\bar{T}$ indicates when a model can answer related questions correctly but fails to convey the same information in the caption automatically. These metrics provide a robust framework for evaluating both correctness and thoroughness in MLLMs.
To our knowledge, we are the first to heuristically highlight the gap in correctness and thoroughness capabilities of MLLMs across multiple views, guiding researchers to enhance these capabilities across dimensions.
Representative results are shown in \cref{fig:radar}, leading to the following conclusions: 1) Gemini-1.5-pro~\cite{gemini1.5} performs the best across many dimensions, followed by GPT-4o~\cite{gpt4o}. 2) Gemini-1.5-pro demonstrates strong object-counting abilities, while GPT-4o excels in identifying camera angles. 3) All models still struggle with dimensions like object numbers, camera angle, camera movement, character identification, and action. We hope our findings guide researchers to focus on improving these abilities in caption tasks.
Our main contributions are listed as follows:
\begin{itemize}
    \item We introduce a new comprehensive visual caption benchmark, CAPability, featuring 6 views and 12 dimensions. By collecting and human-annotating nearly 11K images and videos, CAPability provides a novel and comprehensive methodology for caption benchmarking.
    \item We emphasize that a good caption should be evaluated for both correctness and thoroughness. Accordingly, we report precision, recall, and hit rate, using the F1-score as the main metric to combine correctness and thoroughness.
    \item We transform our annotations into a QA format to evaluate QA accuracy. Based on this approach, we assess an additional capability via the $K\bar{T}$ metric, which indicates the performance gap between QA and the captioning task.
\end{itemize}

\begin{figure*}[!t]
\centering
\includegraphics[width=0.95\textwidth]{figs/data_source_distribution.pdf}
\vspace{-8pt}
\caption{The data source count and distribution of each dimension. We collect nearly 1,000 images/videos for each dimension, crawl parts of data by ourselves, and sample some data from existing datasets to ensure diversity.}
\label{fig:data_source}
% \vspace{-10pt}
\end{figure*}

\section{Related Work}
\noindent\textbf{Multi-modal large language models.}
Based on the significant development of Large Language Models (LLMs) among various linguistic tasks~\cite{gpt3, vicuna, qwen2.5, llama3}, many works try to extend the powerful capabilities into multi-modal understanding. By integrating image content into LLMs, Multi-modal Large Language Models (MLLMs) also gain huge achievements~\cite{llava, llava1.5, qwenvl, minigpt4, llavanext, internvl, gpt4v}. Based on the pre-trained weights from image models, recent MLLMs also expand video understanding capabilities~\cite{llavanextvideo, llavaov, llavavideo, qwen2vl, internvl2.5, nvila, videollama3, qwen2.5vl}. With rapid development, MLLMs are powerful enough to describe both the image and video content in detail, which makes the traditional benchmarks with short captions outdated. More and more methods even try to produce re-captioned detailed descriptions by more powerful models rather than existing human-annotated short captions to train their model~\cite{llavaov, videollama3}. Therefore, it is urgent to propose a new visual caption benchmark that adapts to modern MLLMs.

\begin{table}[!t]
\setlength\tabcolsep{3pt}
\centering
\caption{Comparison of our CAPability and other visual caption benchmarks in different aspects. We are the most comprehensive with both image and video data, multi-view annotations, and new thoroughness evaluation methods proposed.}
\label{tab:benchmark_comparison}
\resizebox{0.45\textwidth}{!}{%
\begin{tabular}{@{}l|cc|c|ccc@{}}
\toprule
\multirow{2}{*}{\textbf{Benchmark}} & \multicolumn{2}{c|}{\textbf{Data Type}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Anno-\\ tations\end{tabular}}} & \multicolumn{3}{c}{\textbf{Thoroughness Evaluation}} \\ \cline{2-3} \cline{5-7} 
 & \textbf{Image} & \textbf{Video} &  & \textbf{Recall} & \textbf{Hit rate} & $\bm{K\bar{T}}$ \\ \midrule
MS-COCO\cite{mscoco} & \checkmark & - & Sentences & - & - & - \\
MSRVTT~\cite{msrvtt} & - & \checkmark & Sentences & - & - & - \\
Dream-1K~\cite{dream1k} & - & \checkmark & Sentences & single dim & - & - \\
VDC~\cite{auroracap} & - & \checkmark & Sentences & - & - & - \\
DetailCaps~\cite{detailcaps} & \checkmark & - & Sentences & - & - & - \\
CompreCap~\cite{comprecap} & \checkmark & - & Object Info & single dim & - & - \\ \midrule
CAPability (Ours) & \checkmark & \checkmark & \begin{tabular}[c]{@{}c@{}}Multi-view\\ Elements\end{tabular} & \checkmark & \checkmark & \checkmark \\ \bottomrule
\end{tabular}%
}
\end{table}


\noindent\textbf{Visual caption benchmarks.}
Visual Caption is a fundamental task in computer vision. Early visual caption benchmarks, such as MS-COCO~\cite{mscoco}, NoCaps~\cite{nocaps}, MSR-VTT~\cite{msrvtt}, and VATEX~\cite{vatex}, usually contain a short sentence with limited visual information as the ground truth. They also use metrics like BLEU~\cite{papinesi2002bleu}, CIDER~\cite{vedantam2015cider}, and METEOR~\cite{banerjee2005meteor} to calculate the matching score directly between two sentences, which is easily affected by the sentence style. Recently, from the annotation aspect, DetailCaps~\cite{detailcaps} extracts object-related information from the ground-truth caption, Dream-1K~\cite{dream1k} splits the ground truth and candidates sentences into events. VDC~\cite{auroracap} also extracts the object, background, and camera information from the video captions by question templates. However, they still rely on the ground-truth caption with human-bias, and require existing LLMs to extract and compare multiple times, thus increasing the cumulative error. CompreCap~\cite{comprecap} explores directly annotating the object-related information in image captions, making the benchmarking more interpretable. On the contrary, we are the first time to propose a comprehensive visual caption benchmark covering both image and video data with 6 views and 12 dimensions. For evaluation, most methods only focus on correctness. Dream-1K~\cite{dream1k} and CompreCap~\cite{comprecap} begin to focus on thoroughness and calculate the recall of events or the object coverage in the segmentation map. However, they still remain incomplete as they only evaluate one dimension and limited metrics. We design various metrics about both correctness and thoroughness, which may be ignored by previous work. We summarize the comparison with other visual caption benchmarks in \cref{tab:benchmark_comparison}, and we are the most holistic on all listed aspects.

\begin{figure}[!t]
\centering
\includegraphics[width=0.4\textwidth]{figs/anno_pipeline.pdf}
\caption{The pipeline of our data annotation for each dimension.}
\label{fig:anno_pipeline}
\vspace{-12pt}
\end{figure}


\begin{figure*}[!t]
\centering
\includegraphics[width=0.95\textwidth]{figs/data_distribution.pdf}
\vspace{-8pt}
\caption{The annotation distribution of each dimension. We statistic different dimensions with different types. We count the frequency in object categories, character identification, and action as most of the descriptions only appear one time. For spatial relation, we summarize 4 categories and count their numbers. For style, camera angle, and camera movement, we count the samples of each category. For others, we plot bar charts to count and show the most frequent samples.}
\label{fig:gt_distribution}
% \vspace{-8pt}
\end{figure*}



\section{CAPability}
\subsection{Multiple Dimension Data Annotation}
\label{sec:capability_annotation}
The pipeline of our whole collection and annotation is shown in \cref{fig:anno_pipeline}. We first design 6 views and split 12 dimensions, then collect nearly 1,000 images and videos for each dimension separately. For the collected data, we conduct pre-annotations by SOTA MLLMs and the following data balancing before the human annotation. After the human-annotation of each dimension, we filter bad cases during the annotation and finally complete the data of CAPability.

\noindent\textbf{Dimension design.}
As shown in \cref{tab:dimensions}, we conclude 6 views and split them into 12 dimensions based on the analysis of caption cases. As shown in \cref{fig:teaser}, we design 9 static dimensions for both video and image, and 4 dynamic dimensions for video, covering most of what makes up a visual caption. We classify dimensions as dynamic or static based on the following principle: descriptions obtainable from a single frame are static, those requiring the entire video are dynamic, which are more related to temporal information. For the object number dimension, the number can be counted statically in an image, and also dynamically in a video, which is more challenging~\cite{vsibench}.
We also design the annotation type of each dimension as two types: open-ended, and specific categories. Specifically, we define 9 categories for style, 4 categories for camera angle, and 7 categories for camera movement. The specific categories of each dimension can be found in \cref{fig:gt_distribution}. See \cref{sec:supp_dimension} for details of each dimension.

\noindent\textbf{Data collection.}
For convenience and problem simplification, we only collect image data for static dimensions and video data for dynamic dimensions.
This is based on the common sense that the video understanding capabilities for MLLMs are usually built upon sufficient image understanding capabilities~\cite{llavanextvideo, llavavideo, videollama3, internvl2.5}. 
Since an image or a video cannot cover all these dimensions of information, we directly collect data for each dimension independently, and evaluate each dimension separately as sub-tasks.
For static dimensions, we mainly collect images from SA-1B~\cite{sam}, COYO-700M~\cite{coyo700m}, Wukong~\cite{wukong100m}, and Wikipaintings~\cite{wikipaintings}, and we also crawl a considerable amount of data from multiple websites by ourselves. We also borrow parts of the image data and annotations from CompreCap~\cite{comprecap} for the spatial relation dimension. For dynamic dimensions, we crawl and cut videos for camera movement dimension, borrow videos from Dream-1K~\cite{dream1k} for action and event dimensions, and borrow videos from VSI-Bench~\cite{vsibench} for the dynamic object number dimension. \cref{fig:data_source} shows our data sources for each dimension and their proportion.

\noindent\textbf{Pre-Annotation.}
The annotations may not be unique for different dimensions. For global-related, camera-related, and knowledge-related views, the annotations tend to be unique as an image only belongs to one kind of scene, style, \textit{etc}. We directly pre-annotate them by extracting the metadata (\eg, style for images in Wikipaintings~\cite{wikipaintings}), or ask SOTA MLLMs to get a preliminary answer. For object-related, text-related, and temporal-related views, there could be multiple objects, texts, or actions in an image or video.
However, it is extremely hard to annotate all objects or actions within an image or a video, as the categories of objects can be divided by almost infinite granularity~\cite{tang2023visual, wang2023all, wang2024all}. Therefore, we do not pursue the most comprehensive annotation possible for each single sample, but randomly select only one object from the visual content, and the same for other dimensions, and reflect the accuracy and thoroughness through the evaluation of a large number of samples. We name this strategy as \textit{One Represents All}. According to the law of large numbers, the distribution of randomly selection can approximate the expectation of covering different granularities of the entire visual content with a large amount of samples, thus ensuring the unbiased nature of the benchmark. Therefore, the key of this annotation strategy is to keep the selection as random as possible. To avoid humans' bias on selecting, we ask the three SOTA MLLMs, \ie, GPT-4o~\cite{gpt4o}, Gemini-1.5-pro~\cite{gemini1.5}, and Qwen-VL-Max~\cite{qwen2vl} to list all objects and actions at the granularity they deem appropriate in an image or video, ask PaddleOCR~\cite{ppocrv3} to list all texts in an image. We finally use Qwen2.5-Max~\cite{qwen2.5} to merge the results together and randomly select one from the merged list to obtain the pre-annotated results. For further object-related dimensions, \eg, object number, object color, and spatial relation, the object selection follows this strategy, then pre-annotate these attributes by MLLMs. 

\noindent\textbf{Data balance.} Based on the pre-annotation results for each dimension, we conduct data balance strategy to control the difficulty and diversity. For dimensions with specific categories, we try to make the number of each category similar. For dimensions of open-ended descriptions, we count the frequency of the descriptive words, suppress the long-tail distribution, and keep low-frequency words, ensuring the variety. The final annotation distribution is shown as \cref{fig:gt_distribution}.

\noindent\textbf{Human annotation.}
For different dimensions, we design different tasks for human annotators. For example, human annotators are asked to judge only right or wrong for object categories and actions rather than changing the annotated descriptions since we need to keep randomness. For dimensions with specific categories, we ask annotators to check the pre-annotated option and select the correct option. As for other dimensions with open-ended descriptions, we ask annotators to check the pre-annotations one by one and modify them based on pre-defined rules if there are any mistakes. We also conduct human-validation of all annotations to ensure the accuracy of annotations is above 97\%.

\noindent\textbf{Data filtering.}
We finally conduct data filtering to drop harmful visual content and re-balance the data since many of them are modified manually. The final distribution of each dimension is shown in \cref{fig:gt_distribution}. Some benchmark examples are shown in \cref{sec:supp_example}.

\begin{table*}[!t]
\setlength\tabcolsep{4pt}
\centering
\caption{The precision, recall, and F1-score of closed-source models and 72B open-source models on all dimensions. The precision represents the accuracy of what the models have described. The recall shows how many visual elements in the image can be described correctly. F1-Score is the harmonic mean of precision and recall. For video inputs, we send the whole video for Gemini, and uniformly sample 50 frames for GPT due to the API limitation of maximum number of images.}
\label{tab:72b_f1score}
\resizebox{0.85\textwidth}{!}{%
\begin{tabular}{@{}llcccccccccccccc@{}}
\toprule
 & \textbf{Methods} & \textbf{\begin{tabular}[c]{@{}c@{}}Obj.\\ Cate.\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Obj.\\ Num.\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Obj.\\ Color\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Spa.\\ Rel.\end{tabular}} & \textbf{Scene} & \textbf{\begin{tabular}[c]{@{}c@{}}Cam.\\ Ang.\end{tabular}} & \textbf{OCR} & \textbf{Style} & \textbf{\begin{tabular}[c]{@{}c@{}}Cha.\\ Iden.\end{tabular}} & \makebox[0.048\textwidth][c]{\textbf{\small{\begin{tabular}[c]{@{}c@{}}(D) Obj.\\ Num.\end{tabular}}}} & \textbf{Act.} & \textbf{\begin{tabular}[c]{@{}c@{}}Cam.\\ Mov.\end{tabular}} & \textbf{Event} & \textbf{Avg.} \\ \midrule
\multirow{7}{*}{\rotatebox{90}{\textit{Precision}}} & LLaVA-OV-72B & 80.4 & 70.1 & 86.8 & 88.5 & 96.2 & 53.6 & 88.9 & 83.9 & \textbf{84.5} & 87.5 & 42.7 & 17.7 & 90.2 & 74.7 \\
 & Qwen2VL-72B & 82.0 & 70.0 & 89.2 & 88.6 & 95.2 & 52.4 & \textbf{95.9} & 82.9 & 83.3 & 83.6 & 44.6 & 33.3 & 89.9 & 76.2 \\
 & InternVL2.5-78B & 80.1 & 68.3 & 89.2 & 87.9 & \textbf{96.4} & 48.4 & 92.5 & 82.8 & 58.3 & 80.0 & 36.4 & 19.0 & 86.8 & 71.2 \\
 & Qwen2.5VL-72B & 83.7 & 66.7 & 85.5 & 88.3 & 95.7 & 54.2 & 95.3 & 84.7 & 72.1 & 91.3 & 45.8 & 35.1 & 87.9 & 75.9 \\
 & GPT-4o (0806) & 87.3 & 67.4 & 88.0 & \textbf{90.4} & 95.9 & \textbf{67.0} & 93.5 & 90.0 & 80.2 & \textbf{96.4} & 54.9 & 26.7 & \textbf{92.1} & 79.2 \\
 & Gemini-1.5-pro & \textbf{89.8} & 72.4 & 88.0 & 88.8 & 95.3 & 56.4 & 94.1 & \textbf{91.4} & 54.0 & 92.0 & \textbf{56.8} & 34.6 & 91.5 & 77.3 \\
 & Gemini-2.0-flash & 85.9 & \textbf{78.6} & \textbf{90.4} & 89.0 & 96.1 & 57.4 & 95.3 & 86.9 & 82.0 & 94.2 & 50.6 & \textbf{35.4} & 89.0 & \textbf{79.3} \\ \midrule
\multirow{7}{*}{\rotatebox{90}{\textit{Recall}}} & LLaVA-OV-72B & 77.0 & 24.6 & 54.7 & 47.9 & 84.0 & 49.9 & 66.6 & 83.5 & 9.3 & 27.3 & 39.6 & 12.2 & 28.6 & 46.6 \\
 & Qwen2VL-72B & 79.9 & 25.1 & 57.3 & 50.4 & 85.1 & 52.1 & 79.6 & 82.6 & 5.7 & 18.1 & 41.7 & 25.7 & 31.2 & 48.8 \\
 & InternVL2.5-78B & 77.9 & 28.5 & 58.3 & 50.1 & 83.6 & 48.4 & 79.1 & 82.8 & 12.4 & 20.5 & 32.1 & 12.0 & 25.0 & 47.0 \\
 & Qwen2.5VL-72B & 80.0 & 28.9 & 59.2 & 55.0 & \textbf{86.9} & 54.2 & 87.5 & 84.7 & 22.7 & 22.3 & 43.4 & 34.9 & 34.1 & 53.4 \\
 & GPT-4o (0806) & 83.8 & 30.0 & 64.7 & 55.7 & 84.6 & \textbf{67.0} & 83.0 & 90.0 & 28.1 & 28.3 & 51.3 & 26.6 & 41.0 & 56.5 \\
 & Gemini-1.5-pro & \textbf{86.3} & \textbf{40.0} & \textbf{67.7} & \textbf{61.3} & 83.9 & 56.4 & 86.1 & \textbf{91.4} & 36.5 & \textbf{45.0} & \textbf{51.4} & 34.6 & \textbf{44.5} & \textbf{60.4} \\
 & Gemini-2.0-flash & 82.5 & 30.6 & 60.8 & 51.8 & 84.0 & 57.4 & \textbf{88.8} & 86.8 & \textbf{37.9} & 28.7 & 46.6 & \textbf{35.2} & 39.7 & 56.2 \\ \midrule
\multirow{7}{*}{\rotatebox{90}{\textit{F1-score}}} & LLaVA-OV-72B & 78.7 & 36.5 & 67.1 & 62.2 & 89.7 & 51.7 & 76.1 & 83.7 & 16.8 & 41.6 & 41.1 & 14.4 & 43.4 & 54.1 \\
 & Qwen2VL-72B & 81.0 & 36.9 & 69.8 & 64.3 & 89.9 & 52.3 & 87.0 & 82.6 & 10.7 & 29.7 & 43.1 & 29.0 & 46.4 & 55.6 \\
 & InternVL2.5-78B & 79.0 & 40.2 & 70.5 & 63.8 & 89.6 & 48.4 & 85.3 & 82.8 & 20.5 & 32.7 & 34.1 & 14.7 & 38.8 & 53.9 \\
 & Qwen2.5VL-72B & 81.8 & 40.3 & 70.0 & 67.8 & \textbf{91.1} & 54.2 & 91.2 & 84.7 & 34.5 & 35.8 & 44.6 & 35.0 & 49.2 & 60.0 \\
 & GPT-4o (0806) & 85.5 & 41.5 & 74.6 & 68.9 & 89.9 & \textbf{67.0} & 87.9 & 90.0 & 41.7 & 43.8 & 53.1 & 26.6 & 56.7 & 63.6 \\
 & Gemini-1.5-pro & \textbf{88.0} & \textbf{51.5} & \textbf{76.5} & \textbf{72.5} & 89.2 & 56.4 & 89.9 & \textbf{91.4} & 43.5 & \textbf{60.4} & \textbf{54.0} & 34.6 & \textbf{59.9} & \textbf{66.8} \\
 & Gemini-2.0-flash & 84.1 & 44.1 & 72.7 & 65.5 & 89.6 & 57.4 & \textbf{92.0} & 86.8 & \textbf{51.9} & 44.0 & 48.5 & \textbf{35.3} & 54.9 & 63.6 \\ \bottomrule 
\end{tabular}%
}
\end{table*}

\subsection{Multiple Dimension Evaluation}
\noindent\textbf{Caption evaluation.}
As we collect and annotate the data of each dimension separately, we also evaluate each dimension independently.
Different from matching the similarity between the caption and ground-truth sentences, our annotation drops the caption sentence, and we use GPT-4 Turbo~\cite{gpt4} to take interpretable scores for each dimension. We use a similar prompt template for dimensions with specific categories (\ie, style, camera angle, and camera movement), and use another similar prompt template for other open-set dimensions. See \cref{sec:supp_prompt} for the details of the prompts. Therefore, we can judge the caption into the following three situations: 
\begin{itemize}
    \item Miss, which means the caption does not mention the corresponding content about the dimension.
    \item Positive, which means the caption mentions the corresponding content about the dimension, and describes it correctly compared with the annotations.
    \item Negative, which means the caption mentions the corresponding content about the dimension, but gives a wrong description compared with the annotations.
\end{itemize}
As all data can be judged into these three situations, we can then calculate two metrics: 
1) \textit{Precision}, which represents the accuracy on all samples that the model has mentioned, and thus only considers the precision. 2) \textit{Recall}, which represents the accuracy on all annotated samples, no matter whether the dimension is described or missed in the caption, and thus considers both the correctness and thoroughness. Given the set of all samples as $S(\text{All})$, positive samples as $S(\text{Pos})$, negative samples as $S(\text{Neg})$, missed samples as $S(\text{Miss})$, the metrics can be calculated by:
\begin{align}
    \text{Precision} &= \frac{|S(\text{Pos})|}{|S(\text{All}) - S(\text{Miss})|}, \\
    \text{Recall} &= \frac{|S(\text{Pos})|}{|S(\text{All})|}.
\end{align}
We report their harmonic mean (\ie, F1-score) as our main metric. Apart from them, we also introduce a new metric, \textit{Hit rate}, which represents the referring ratio about the dimension in visual caption and can be calculated as:
\begin{equation}
    \text{Hit rate} = \frac{|S(\text{All}) - S(\text{Miss})|}{|S(\text{All})|}.
\end{equation}
This metric only considers the pure thoroughness of the caption in each dimension, without thinking about the accuracy.

\noindent\textbf{Question-answer pairs evaluation.}
As we annotate each descriptive element for each dimension rather than caption sentence, we can also convert our annotations to question-answer (QA) pair format to evaluate the MLLMs' general ability out of the horizon of caption only. See \cref{sec:supp_example} for examples of our CAPability-QA. Based on the QA accuracy, we introduce a new metric, \textit{know but cannot tell} ($K\bar{T}$), which evaluates the situation when a model knows the answer (\ie, can answer correctly when given it the related question), but cannot tell automatically in the caption without specific question as prompt. This evaluation is significant to the caption task of MLLMs, but is usually ignored by previous methods. Given the set of correct answers as $S_{qa}(\text{Correct})$, $K\bar{T}$ can be calculated as:
\begin{equation}
    K\bar{T} = \frac{\left|S_{qa}(\text{Correct}) \cap (S(\text{Neg}) \cup S(\text{Miss}))\right|}{|S_{qa}(\text{Correct})|}.
\end{equation}

\begin{table*}[!t]
\setlength\tabcolsep{4pt}
\centering
\caption{The precision, recall, and F1-score of 7B open-source models on all dimensions. We keep their default settings for each model.}
\label{tab:7b_f1score}
\resizebox{0.85\textwidth}{!}{%
\begin{tabular}{@{}llcccccccccccccc@{}}
\toprule
 & \textbf{Methods} & \textbf{\begin{tabular}[c]{@{}c@{}}Obj.\\ Cate.\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Obj.\\ Num.\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Obj.\\ Color\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Spa.\\ Rel.\end{tabular}} & \textbf{Scene} & \textbf{\begin{tabular}[c]{@{}c@{}}Cam.\\ Ang.\end{tabular}} & \textbf{OCR} & \textbf{Style} & \textbf{\begin{tabular}[c]{@{}c@{}}Cha.\\ Iden.\end{tabular}} & \makebox[0.048\textwidth][c]{\textbf{\small{\begin{tabular}[c]{@{}c@{}}(D) Obj.\\ Num.\end{tabular}}}} & \textbf{Act.} & \textbf{\begin{tabular}[c]{@{}c@{}}Cam.\\ Mov.\end{tabular}} & \textbf{Event} & \textbf{Avg.} \\ \midrule
\multirow{6}{*}{\rotatebox{90}{\textit{Precision}}} & LLaVA-OV-7B & 79.5 & 67.8 & 87.3 & 88.7 & 95.3 & 41.9 & 87.7 & \textbf{84.4} & \textbf{90.9} & \textbf{92.8} & 38.9 & 20.2 & 87.3 & 74.0 \\
 & Qwen2VL-7B & 80.3 & 68.3 & \textbf{88.7} & 89.0 & 95.4 & 39.9 & \textbf{95.4} & 77.1 & 83.3 & 73.5 & 42.5 & 24.2 & 86.7 & 72.6 \\
 & NVILA-8B & 80.6 & 68.5 & 84.2 & 88.4 & 95.4 & 44.5 & 92.8 & 79.0 & 90.8 & 47.2 & 32.4 & 14.7 & \textbf{92.1} & 70.0 \\
 & InternVL2.5-8B & 76.1 & 60.3 & 85.8 & 89.3 & 95.1 & 42.5 & 89.0 & 81.9 & 48.3 & 84.4 & 37.9 & 20.2 & 84.5 & 68.9 \\
 & VideoLLaMA3-7B & 81.0 & 66.8 & 85.5 & \textbf{90.6} & \textbf{97.0} & 43.2 & 90.0 & 80.9 & 84.1 & 78.9 & 43.0 & \textbf{30.2} & 88.3 & 73.8 \\
 & Qwen2.5VL-7B & \textbf{82.0} & \textbf{73.5} & 88.0 & 88.6 & 95.8 & \textbf{47.7} & 93.8 & 82.0 & 80.8 & 92.4 & \textbf{43.7} & 26.8 & 86.5 & \textbf{75.5} \\ \midrule
\multirow{6}{*}{\rotatebox{90}{\textit{Recall}}} & LLaVA-OV-7B & 76.8 & 23.0 & 53.0 & 48.5 & 82.7 & 33.4 & 64.5 & \textbf{83.4} & 4.6 & \textbf{32.0} & 35.8 & 12.6 & 27.0 & 44.4 \\
 & Qwen2VL-7B & 78.4 & 20.6 & 50.4 & 46.1 & 84.7 & 39.1 & 73.4 & 77.1 & 4.0 & 14.7 & 40.0 & 17.4 & 27.5 & 44.1 \\
 & NVILA-8B & 78.2 & \textbf{23.5} & 54.6 & 46.6 & 81.3 & 37.9 & 69.1 & 77.5 & 6.7 & 10.4 & 26.1 & 7.2 & 19.8 & 41.5 \\
 & InternVL2.5-8B & 73.8 & 23.0 & 52.2 & 49.3 & 83.0 & 42.5 & 75.3 & 81.9 & \textbf{9.8} & 19.1 & 34.6 & 19.2 & 27.8 & 45.5 \\
 & VideoLLaMA3-7B & 77.0 & 22.7 & 53.4 & \textbf{51.1} & 83.0 & 40.2 & 67.2 & 79.6 & 4.2 & 7.3 & \textbf{41.5} & 25.1 & \textbf{30.5} & 44.8 \\
 & Qwen2.5VL-7B & \textbf{79.3} & 19.7 & \textbf{56.0} & 49.0 & \textbf{85.6} & \textbf{47.3} & \textbf{81.3} & 82.0 & 9.1 & 19.3 & 40.4 & \textbf{26.5} & 30.3 & \textbf{48.1} \\ \midrule
\multirow{6}{*}{\rotatebox{90}{\textit{F1-score}}} & LLaVA-OV-7B & 78.1 & 34.3 & 66.0 & 62.7 & 88.5 & 37.1 & 74.3 & \textbf{83.9} & 8.7 & \textbf{47.6} & 37.3 & 15.5 & 41.3 & 52.0 \\
 & Qwen2VL-7B & 79.4 & 31.7 & 64.3 & 60.7 & 89.7 & 39.5 & 83.0 & 77.1 & 7.6 & 24.5 & 41.2 & 20.3 & 41.8 & 50.8 \\
 & NVILA-8B & 79.4 & \textbf{35.0} & 66.3 & 61.0 & 87.8 & 40.9 & 79.2 & 78.2 & 12.5 & 17.1 & 28.9 & 9.6 & 32.6 & 48.3 \\
 & InternVL2.5-8B & 74.9 & 33.3 & 64.9 & 63.5 & 88.6 & 42.5 & 81.5 & 81.9 & 16.3 & 31.2 & 36.2 & 19.7 & 41.8 & 52.0 \\
 & VideoLLaMA3-7B & 78.9 & 33.9 & 65.7 & \textbf{65.3} & 89.4 & 41.7 & 77.0 & 80.2 & 8.0 & 13.3 & \textbf{42.3} & \textbf{27.4} & \textbf{45.3} & 51.4 \\
 & Qwen2.5VL-7B & \textbf{80.6} & 31.1 & \textbf{68.5} & 63.1 & \textbf{90.4} & \textbf{47.5} & \textbf{87.1} & 82.0 & \textbf{16.4} & 31.9 & 42.0 & 26.6 & 44.9 & \textbf{54.8} \\ \bottomrule
\end{tabular}%
}
\end{table*}

\begin{figure}[!t]
\centering
\includegraphics[width=0.46\textwidth]{figs/stability1.pdf}
\caption{The evaluation of repeating 5 times for Gemini-1.5-pro captions. We tag the fluctuation range beside the data point.}
\label{fig:stability1}
\vspace{-8pt}
\end{figure}


\section{Experiments}

\subsection{Experimental Setups}
For comprehensively evaluating the state-of-the-art (SOTA) models, we both choose several popular open-source and closed-source MLLMs. For closed-source models, we evaluate GPT-4o (0806)~\cite{gpt4o}, Gemini-1.5-pro~\cite{gemini1.5}, and Gemini-2.0-flash~\cite{gemini2.0}.
For open-source models, we evaluate InternVL2.5~\cite{internvl2.5}, LLaVA-OneVision~\cite{llavaov}, NVILA~\cite{nvila}, VideoLLaMA3~\cite{videollama3}, Qwen2VL~\cite{qwen2vl} and Qwen2.5VL~\cite{qwen2.5vl} with their different LLM sizes. We use the same image prompt and video prompt to infer all models, see \cref{sec:supp_prompt} for the inference prompts. We use GPT-4-Turbo-128k~\cite{gpt4} to take scores for all generated captions to complete our evaluation. See \cref{sec:supp_impl_details} for more implementation details.

\subsection{Main Evaluation Results}

\noindent\textbf{F1-score of closed-source API and 72B models.}
We report the precision, recall, and F1-score of closed-source and 72B models in \cref{tab:72b_f1score}. Gemini-2.0-flash and GPT-4o achieve the highest precision (79.3\% and 79.2\%), which represents their captions are truthful and accurate. When it comes to the recall metric, Gemini-1.5-pro achieves the best with 3.9\% higher than second place, \ie, GPT-4o, which means it is better at identifying more elements correctly. This leads the F1-score still ranked first by Gemini-1.5-pro (66.8\%), and followed by GPT-4o and Gemini-2.0-flash. Gemini-1.5-pro has a huge advantage in object counting in both image (7.4\% better than the second place) and video (16.4\% better than the second place). GPT-4o excels at recognizing camera angle, as it is 9.6\% higher than the second place. Qwen2.5-VL performs the best in the open-source models as it performs well on scene and camera movement. It is worth noting that these models behave differently in different dimensions. Object category, scene, OCR, and style seem simple for these powerful models as they all achieve well on the F1-score. When it comes to the dimensions of object number, object color, spatial relation, style, character identification, and events, all of them show relatively high precision but low recall, which means they can describe these elements well when they are confident about them, but might miss some instances and ignore the thoroughness. As for the action and two camera-related dimensions, all models perform unsatisfactorily on both precision and recall.
This phenomenon inspires researchers to focus more on these aspects of the model's capability.


\begin{table*}[!t]
\setlength\tabcolsep{4pt}
\centering
\caption{The hit rate of all models. The hit rate only reflects the referring ratio of each dimension without considering the accuracy.}
\label{tab:hit_rate}
\resizebox{0.85\textwidth}{!}{%
\begin{tabular}{@{}lcccccccccccccc@{}}
\toprule
\textbf{Methods} & \textbf{\begin{tabular}[c]{@{}c@{}}Obj.\\ Cate.\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Obj.\\ Num.\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Obj.\\ Color\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Spa.\\ Rel.\end{tabular}} & \textbf{Scene} & \textbf{\begin{tabular}[c]{@{}c@{}}Cam.\\ Ang.\end{tabular}} & \textbf{OCR} & \textbf{Style} & \textbf{\begin{tabular}[c]{@{}c@{}}Cha.\\ Iden.\end{tabular}} & \makebox[0.048\textwidth][c]{\textbf{\small{\begin{tabular}[c]{@{}c@{}}(D) Obj.\\ Num.\end{tabular}}}} & \textbf{Act.} & \textbf{\begin{tabular}[c]{@{}c@{}}Cam.\\ Mov.\end{tabular}} & \textbf{Event} & \textbf{Avg.} \\ \midrule
LLaVA-OV-7B & 96.6 & 33.9 & 60.8 & 54.7 & 86.7 & 79.7 & 73.6 & 98.8 & 5.0 & 34.5 & 92.2 & 62.4 & 30.9 & 62.3 \\
Qwen2VL-7B & 97.6 & 30.2 & 56.8 & 51.8 & 88.7 & 98.0 & 77.0 & 99.9 & 4.8 & 20.0 & 94.0 & 72.0 & 31.7 & 63.3 \\
NVILA-8B & 97.0 & 34.3 & 64.9 & 52.7 & 85.3 & 85.1 & 74.5 & 98.1 & 7.4 & 22.1 & 80.6 & 48.6 & 21.5 & 59.4 \\
InternVL2.5-8B & 97.0 & 38.1 & 60.9 & 55.2 & 87.3 & 100.0 & 84.6 & 100.0 & 20.3 & 22.7 & 91.4 & 94.8 & 32.9 & 68.1 \\
VideoLLaMA3-7B & 95.1 & 34.0 & 62.5 & 56.4 & 85.6 & 93.1 & 74.7 & 98.5 & 5.0 & 9.2 & 96.5 & 83.1 & 34.5 & 63.7 \\
Qwen2.5VL-7B & 96.7 & 26.8 & 63.7 & 55.3 & 89.4 & 99.2 & 86.7 & 100.0 & 11.3 & 20.9 & 92.5 & 98.6 & 35.0 & 67.4 \\
LLaVA-OV-72B & 95.8 & 35.2 & 63.1 & 54.1 & 87.4 & 93.3 & 74.9 & 99.5 & 11.0 & 31.2 & 92.6 & 69.0 & 31.7 & 64.5 \\
Qwen2VL-72B & 97.4 & 35.8 & 64.3 & 56.9 & 89.4 & 99.6 & 83.0 & 100.0 & 6.8 & 21.6 & 93.5 & 77.1 & 34.7 & 66.2 \\
InternVL2.5-78B & 97.2 & 41.7 & 65.3 & 57.0 & 86.7 & 100.0 & 85.5 & 100.0 & 21.3 & 25.7 & 88.2 & 63.3 & 28.8 & 66.2 \\
Qwen2.5VL-72B & 95.6 & 43.3 & 69.2 & 62.3 & 90.7 & 100.0 & 91.8 & 100.0 & 31.4 & 24.4 & 94.8 & 99.4 & 38.9 & 72.5 \\
GPT-4o (0806) & 96.0 & 44.5 & 73.5 & 61.6 & 88.2 & 100.0 & 88.8 & 100.0 & 35.1 & 29.4 & 93.4 & 99.4 & 44.5 & 73.4 \\
Gemini-1.5-pro & 96.1 & 55.3 & 77.0 & 69.0 & 88.1 & 99.9 & 91.4 & 100.0 & 67.5 & 48.9 & 90.5 & 100.0 & 48.6 & 79.4 \\
Gemini-2.0-flash & 96.1 & 39.0 & 67.2 & 58.2 & 87.5 & 100.0 & 93.2 & 99.9 & 46.2 & 30.4 & 92.0 & 99.6 & 44.6 & 73.4 \\ \bottomrule
\end{tabular}%
}
\end{table*}


\begin{table*}[!t]
\setlength\tabcolsep{4pt}
\centering
\caption{The accuracy$\uparrow$ (higher is better) of CAPability-QA and the result of \textit{know but cannot tell$\downarrow$} (lower is better) metric.}
\label{tab:qa_acc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccccccccc@{}}
\toprule
\textbf{Methods} & \textbf{\begin{tabular}[c]{@{}c@{}}Obj.\\ Cate.\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Obj.\\ Num.\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Obj.\\ Color\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Spa.\\ Rel.\end{tabular}} & \textbf{Scene} & \textbf{\begin{tabular}[c]{@{}c@{}}Cam.\\ Ang.\end{tabular}} & \textbf{OCR} & \textbf{Style} & \textbf{\begin{tabular}[c]{@{}c@{}}Cha.\\ Iden.\end{tabular}} & \textbf{\small{\begin{tabular}[c]{@{}c@{}}(D) Obj.\\ Num.\end{tabular}}} & \textbf{Act.} & \textbf{\begin{tabular}[c]{@{}c@{}}Cam.\\ Mov.\end{tabular}} & \textbf{Event} & \textbf{Avg.} \\ \midrule
LLaVA-OV-72B & 95.0/20.3 & 54.6/64.3 & 63.8/39.6 & 94.0/49.6 & \textbf{96.2}/13.9 & 60.6/33.0 & 66.3/13.7 & 82.0/9.4 & 32.1/74.8 & \textbf{52.2}/66.1 & 75.5/53.2 & 15.7/31.4 & 85.3/67.1 & 67.2/41.3 \\
Qwen2VL-72B & 94.7/16.7 & 56.1/62.1 & 68.6/37.0 & 90.7/46.0 & 94.0/12.6 & 65.0/35.4 & 82.4/10.2 & 86.6/10.4 & 31.3/84.7 & 48.9/78.3 & 73.0/47.6 & 34.1/53.1 & 72.7/60.4 & 69.1/42.6 \\ 
InternVL2.5-78B & 95.5/19.1 & 56.9/57.8 & 67.1/35.4 & 90.0/45.2 & 91.2/11.4 & 54.1/21.4 & 79.5/11.0 & 82.5/47.0 & 19.1/\textbf{8.2} & 49.7/73.0 & 79.1/62.4 & 23.3/68.1 & 81.7/69.9 & 66.9/40.8 \\ 
Qwen2.5VL-72B & 92.7/15.3 & \textbf{58.2}/60.7 & 67.4/33.5 & 84.4/37.2 & 88.7/9.1 & 63.9/24.3 & \textbf{87.4}/5.9 & \textbf{87.3}/8.5 & 33.4/47.4 & 41.4/66.2 & 75.8/49.3 & \textbf{39.5}/38.2 & 85.8/61.1 & 69.7/35.1 \\
GPT-4o (0806) & 94.5/13.1 & 47.2/55.2 & 72.5/26.6 & 79.5/\textbf{34.7} & 84.5/\textbf{7.8} & \textbf{71.6}/16.1 & 80.5/6.7 & 79.3/3.5 & 37.2/30.9 & 46.2/64.8 & 81.1/41.7 & 20.5/53.9 & 78.6/51.7 & 67.2/31.3 \\
Gemini-1.5-pro & 97.3/\textbf{11.9} & 51.6/\textbf{41.0} & \textbf{78.8}/\textbf{24.1} & \textbf{94.4}/36.4 & 87.1/9.6 & 56.8/19.2 & 84.8/5.5 & 84.2/\textbf{3.1} & 41.2/18.0 & 51.2/\textbf{51.6} & 74.4/\textbf{36.1} & 32.2/23.4 & 82.8/\textbf{47.9} & \textbf{70.5}/\textbf{25.2} \\ 
Gemini-2.0-flash & \textbf{98.3}/16.3 & 46.8/52.6 & 73.3/32.8 & 93.4/45.1 & 95.2/12.6 & 57.6/\textbf{9.0} & 84.8/\textbf{4.5} & 74.5/3.2 & \textbf{49.1}/25.7 & 44.2/58.4 & \textbf{81.6}/45.9 & 24.8/\textbf{23.1} & \textbf{86.6}/54.5 & 70.0/29.5 \\ \bottomrule
\end{tabular}%
}
\end{table*}


\noindent\textbf{F1-score of 7B models.}
The precision, recall, and F1-score of 7B models are shown in \cref{tab:7b_f1score}. Among these 6 models, Qwen2.5VL-7B achieves the best precision (75.5\%), recall (48.1\%), and F1-score (54.8\%), demonstrating its awesome ability. Averagely, the 7B models perform a bit worse than 72B models, verifying the scaling law. Among the dimensions, they follow a similar pattern as 72B and closed-source models. Researchers should focus on the thoroughness of object number, object color, spatial relation, style, character identification, and event, and try to improve the ability of the action and the two camera-related dimensions.

\noindent\textbf{Hit rate among all models.}
We also report the hit rate of these models, which represents the pure thoroughness of each dimension, as shown in \cref{tab:hit_rate}. For example, it is considered a hit if the caption mentions any object for the object category dimension, or mentions any angle information for camera angle dimension, but for the object number or color dimension, it is only considered a hit if the caption mentions any number or color information of the correct object. We find the hit rate seems to increase as the size of models increases, which may be due to more knowledge and stronger instruction following ability for larger models. Among all dimensions, the hit rate of character identification performs the worst, the existing models prefer to keep silent as they usually cannot recognize the person and character well. The closed-source models would be more likely to tell the name of characters, and we guess this may be due to stronger knowledge and more diverse training data.

\noindent\textbf{QA-based evaluation and the $\bm{K\bar{T}}$ metric.}
As we convert our annotations to QA format, we evaluate the accuracy of closed-source APIs and 72B models, thus further calculating their $K\bar{T}$ metrics, as shown in \cref{tab:qa_acc}. We are surprised to find that the difference in their QA accuracy is not significant, which means they can have a similar level of understanding of the correct visual descriptions based on the specific questions. When it comes to the $K\bar{T}$ metric, these models vary a lot. Gemini-1.5-pro performs the best with the smallest $K\bar{T}$ (25.7\%), but the $K\bar{T}$ of LLaVA-OneVision-72B, Qwen2VL-72B, and InternVL2.5-78B comes to more than 40\%, which means they are more likely knowing the answer but cannot tell automatically. This phenomenon shows the performance gap between the strong instruction (QA) task and the weak instruction (caption) task, which may be ignored by previous work.


\subsection{Stability Analysis}
To validate the stability and robustness of our GPT-4-Turbo based evaluation method, we take the inferred caption of Gemini-1.5-pro as the example, run our evaluation 5 times, and the result is shown in \cref{fig:stability1}. We tag the fluctuation range, \ie, the difference between the maximum and minimum scores, besides the data point. \cref{fig:stability1} shows our strong stability, and our average range of precision and recall are 1.1\% and 1.0\%, respectively. This demonstrates the reliability and interpretability of our evaluation method, which matches annotated elements in the generated captions. See \cref{sec:supp_exp_stability} for more stability analysis.


\section{Conclusion}
\label{sec:conclusion}
In this work, we present CAPability, the first comprehensive benchmark for evaluating visual caption across images and videos through 6 views and 12 dimensions analysis. Unlike existing benchmarks that rely on oversimplified metrics or limited visual elements, CAPability introduces a correctness-thoroughness dual evaluation framework based on precision, recall, hit rate, and $K\bar{T}$. Through this meticulous evaluation process, we uncover specific strengths and areas needing improvement across leading models, such as their proficiency in object counting and challenges in aspects like camera angle detection, character identification and action recognition. We also indicate the "know but cannot tell" phenomenon of MLLMs, which may be ignored by previous work. We believe that CAPability will play a pivotal role in advancing the field of visual captioning by encouraging the development of models that holistically understand and describe visual content. 
We open-source all our annotated data to facilitate future research.