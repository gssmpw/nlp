\section{Related Work}
\noindent\textbf{Multi-modal large language models.}
Based on the significant development of Large Language Models (LLMs) among various linguistic tasks____, many works try to extend the powerful capabilities into multi-modal understanding. By integrating image content into LLMs, Multi-modal Large Language Models (MLLMs) also gain huge achievements____. Based on the pre-trained weights from image models, recent MLLMs also expand video understanding capabilities____. With rapid development, MLLMs are powerful enough to describe both the image and video content in detail, which makes the traditional benchmarks with short captions outdated. More and more methods even try to produce re-captioned detailed descriptions by more powerful models rather than existing human-annotated short captions to train their model____. Therefore, it is urgent to propose a new visual caption benchmark that adapts to modern MLLMs.

\begin{table}[!t]
\setlength\tabcolsep{3pt}
\centering
\caption{Comparison of our CAPability and other visual caption benchmarks in different aspects. We are the most comprehensive with both image and video data, multi-view annotations, and new thoroughness evaluation methods proposed.}
\label{tab:benchmark_comparison}
\resizebox{0.45\textwidth}{!}{%
\begin{tabular}{@{}l|cc|c|ccc@{}}
\toprule
\multirow{2}{*}{\textbf{Benchmark}} & \multicolumn{2}{c|}{\textbf{Data Type}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Anno-\\ tations\end{tabular}}} & \multicolumn{3}{c}{\textbf{Thoroughness Evaluation}} \\ \cline{2-3} \cline{5-7} 
 & \textbf{Image} & \textbf{Video} &  & \textbf{Recall} & \textbf{Hit rate} & $\bm{K\bar{T}}$ \\ \midrule
MS-COCO____ & \checkmark & - & Sentences & - & - & - \\
MSRVTT____ & - & \checkmark & Sentences & - & - & - \\
Dream-1K____ & - & \checkmark & Sentences & single dim & - & - \\
VDC____ & - & \checkmark & Sentences & - & - & - \\
DetailCaps____ & \checkmark & - & Sentences & - & - & - \\
CompreCap____ & \checkmark & - & Object Info & single dim & - & - \\ \midrule
CAPability (Ours) & \checkmark & \checkmark & \begin{tabular}[c]{@{}c@{}}Multi-view\\ Elements\end{tabular} & \checkmark & \checkmark & \checkmark \\ \bottomrule
\end{tabular}%
}
\end{table}


\noindent\textbf{Visual caption benchmarks.}
Visual Caption is a fundamental task in computer vision. Early visual caption benchmarks, such as MS-COCO____, NoCaps____, MSR-VTT____, and VATEX____, usually contain a short sentence with limited visual information as the ground truth. They also use metrics like BLEU____, CIDER____, and METEOR____ to calculate the matching score directly between two sentences, which is easily affected by the sentence style. Recently, from the annotation aspect, DetailCaps____ extracts object-related information from the ground-truth caption, Dream-1K____ splits the ground truth and candidates sentences into events. VDC____ also extracts the object, background, and camera information from the video captions by question templates. However, they still rely on the ground-truth caption with human-bias, and require existing LLMs to extract and compare multiple times, thus increasing the cumulative error. CompreCap____ explores directly annotating the object-related information in image captions, making the benchmarking more interpretable. On the contrary, we are the first time to propose a comprehensive visual caption benchmark covering both image and video data with 6 views and 12 dimensions. For evaluation, most methods only focus on correctness. Dream-1K____ and CompreCap____ begin to focus on thoroughness and calculate the recall of events or the object coverage in the segmentation map. However, they still remain incomplete as they only evaluate one dimension and limited metrics. We design various metrics about both correctness and thoroughness, which may be ignored by previous work. We summarize the comparison with other visual caption benchmarks in \cref{tab:benchmark_comparison}, and we are the most holistic on all listed aspects.

\begin{figure}[!t]
\centering
\includegraphics[width=0.4\textwidth]{figs/anno_pipeline.pdf}
\caption{The pipeline of our data annotation for each dimension.}
\label{fig:anno_pipeline}
\vspace{-12pt}
\end{figure}


\begin{figure*}[!t]
\centering
\includegraphics[width=0.95\textwidth]{figs/data_distribution.pdf}
\vspace{-8pt}
\caption{The annotation distribution of each dimension. We statistic different dimensions with different types. We count the frequency in object categories, character identification, and action as most of the descriptions only appear one time. For spatial relation, we summarize 4 categories and count their numbers. For style, camera angle, and camera movement, we count the samples of each category. For others, we plot bar charts to count and show the most frequent samples.}
\label{fig:gt_distribution}
% \vspace{-8pt}
\end{figure*}