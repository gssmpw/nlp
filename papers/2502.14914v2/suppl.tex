% \maketitlesupplementary

\onecolumn
\appendix

\clearpage
\setcounter{page}{1}
\setcounter{table}{0}
\setcounter{figure}{0}
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\thefigure}{A\arabic{figure}}

\begin{center}
    \Large
    % \textbf{\thetitle}\\
    % \vspace{0.5em}Supplementary Material \\
    \textbf{Appendix}
    \vspace{1.0em}
\end{center}


\section{More Experimental Analysis}
\subsection{Implementation Details}
\label{sec:supp_impl_details}
For all our evaluated model, we follow their official configurations to run the inference. We set the temperature of all open-source models to 0, while keeping the default for closed-source APIs. All maximum output token length is set to 8192. We list the configurations as follows.

\noindent\textbf{LLaVA-OneVision.} We set \textit{anyres-max-9} for image, and uniformly sample 32 frames for video.

\noindent\textbf{Qwen2VL and Qwen2.5VL.} We keep the default minimum and maximum image pixels in package \textit{qwen\_vl\_utils}, which is $4 * 28 * 28$, and $16384 * 28 * 28$, respectively. We also keep default video settings, the fps is set to $2.0$, the maximum frames are $768$, the minimum video pixel is $128 * 28 * 28$, and the maximum video pixel is $768 * 28 * 28$.

\noindent\textbf{InternVL2.5.} We use the official video and image process function and uniformly sample 32 frames for video.

\noindent\textbf{VideoLLaMA3.} We use image model for image dimensions and video model for video dimensions. The fps is set to $1$, and the maximum frames are $180$ for videos.

\noindent\textbf{NVILA.} We use the official image and video process function in \textit{VILA} repository, and uniformly sample 8 frames for videos, as it is suggested in the official config.

\noindent\textbf{GPT-4o.} Due to the maximum frame number limits of GPT API, we uniformly sample 50 frames for videos, and keep the original spatial size of both images and videos, sending them to the API server.

\noindent\textbf{Gemini-1.5-pro and Gemini-2.0-flash.} As Gemini API supports video, we directly send the original image and video to the API server. For very few videos with too large file size, we downsample the fps into 3, and send the downsampled video to the API server for connection stability.


\subsection{More Stability Analysis}
\label{sec:supp_exp_stability}

\begin{figure*}[!h]
\centering
\includegraphics[width=\textwidth]{figs/stability2.pdf}
\caption{The stability analysis with three different evaluation models on 7 MLLMs' captions. The results on all metrics show a high degree of consistency.}
\label{fig:stability2}
% \vspace{-8pt}
\end{figure*}

\noindent To further evaluate the stability of our evaluation pipeline, we conduct another experiment. Specifically, we introduce two more evaluation models, Gemini-2.0-flash and Qwen2.5-Max, as they are both the most popular and powerful SOTA language models. We re-run the evaluation process with new evaluation models with the same evaluation prompts. The average result of evaluations for closed-source models and open-source 72B models is shown in \cref{fig:stability2}. Though the judgment criteria for each model may be slightly different, leading to only a slight difference in the scores, but the evaluation results show high consistency as they keep the same rank of these MLLMs as GPT-4-Turbo. This demonstrates the high reliability, interpretability, and stability of our evaluation methods.

\section{More Benchmark Details}

\subsection{Details of Dimensions}
\label{sec:supp_dimension}
We explain each dimension in detail about what they represent here.

\noindent\textbf{Object category.}
This dimension measures the ability of whether models can give a correct description about a specific object in the image. The object is randomly selected from the image.

\noindent\textbf{Object number.} 
Given a kind of randomly selected object existing in several numbers in an image or a video, this dimension measures the ability of whether models can count the object correctly. For videos, models should watch the whole video and dynamically count the number based on the camera.

\noindent\textbf{Object color.}
Given a kind of randomly selected object in an image, this dimension measures the ability of whether models can describe the color correctly.

\noindent\textbf{Spatial relation.}
Given two nearby objects in an image, this dimension measures the ability of whether models can describe the spatial relationship of the two objects correctly. We sample 500 images from our collected data, and sample another 500 images from CompreCap, with their spatial relationship descriptions.

\noindent\textbf{Scene.}
Given an image, this dimension measures the ability of whether models can obtain and tell the global scene of the image correctly.

\noindent\textbf{Camera angle.}
Given an image, this dimension measures the ability of whether models can obtain and tell the camera angle correctly when shooting the image.

\noindent\textbf{OCR.}
Given an image, this dimension measures the ability of whether models can recognize and tell the text appearing in the image correctly.

\noindent\textbf{Style.}
Given an image, this dimension measures the ability of whether models can obtain and tell the global style of the image correctly.

\noindent\textbf{Character identification.}
Given an image, this dimension measures the ability of whether models can recognize the character or the person in the image.

\noindent\textbf{Action.}
Given a video, this dimension measures the ability of whether models can recognize the action in the video. We use the video data of Dream-1K and re-annotate the action from their annotations.

\noindent\textbf{Camera movement.}
Given a video, this dimension measures the ability of whether models can obtain and tell the camera angle correctly when recording the video. We search videos by ourselves and cut them into short clips, filtering complex movement composition. We only have simple camera movement in our data, but existing models still perform unsatisfactorily.

\noindent\textbf{Event.}
Given a video, this dimension measures the ability of whether models can tell a complete event in the video. We refer Dream-1K to design this dimension, and we extract the events from their annotations. Different from other dimensions with atom-level elements, the event is usually composed of subjects and actions, which measures the temporal summarization ability of the model.

\subsection{Prompts of Inference and Evaluation}
\label{sec:supp_prompt}

\noindent\textbf{Inference prompt.}
We use the same prompts for all models to produce the visual captions. The image prompt and video prompt are shown in \cref{fig:infer_prompt}. To decrease the inference difficulty, we prompt the models to output the information of all our designed dimensions with a detailed caption. Despite this, the models still show a huge difference in the hit rate of each dimension, which may be due to the variety of training data related to the caption.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.6\textwidth]{figs/infer_prompt.pdf}
\caption{The image prompt and video prompt for all models when inferring captions.}
\label{fig:infer_prompt}
% \vspace{-8pt}
\end{figure*}


\noindent\textbf{Evaluation prompt.}
As we divide the evaluation of dimensions into two types: 1) dimensions with specific categories (\ie, style, camera angle, and camera movement), 2) dimensions with open-ended descriptions. Therefore, we design two kinds of templates for evaluating, and fine-tune them within each dimension. In \cref{fig:eval_prompt}, we take the object number dimension and camera movement dimension as examples, to show our prompts for evaluation. For dimensions with specific categories, we ask GPT-4-Turbo to extract the key information and classify the caption into our pre-defined categories or the 'N/A' class. The correct classification is considered as positive, the wrong one as negative, and the 'N/A' result is considered as a miss. For dimensions with open-ended descriptions, we ask GPT-4-Turbo to directly compare the annotations and the caption, and give out the result of positive, negative, or miss with reasons.



\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{figs/eval_prompt.pdf}
\caption{Two prompt examples for different types of evaluation sub-tasks. The example of object number represents dimensions with open-ended descriptions, and the example of camera movement represents the dimensions with specific categories.}
\label{fig:eval_prompt}
% \vspace{-8pt}
\end{figure*}


\subsection{Explanation for One Represents All Strategy}
For each dimension, we only annotate one element, though there may be more than one element existing for some dimensions. Therefore, our annotations do not cover the whole visual content. But for those who try to cover the whole visual content, it is actually pretty hard to achieve this, as we mentioned in \cref{sec:capability_annotation}, the objects can be divided into almost infinite granularity.
We focus on keeping the randomness of elements selection, thus covering the whole visual content in a statistical sense, based on the law of large numbers. Therefore, we get the ability to evaluate the thoroughness of the generated captions by calculating the recall and hit rate.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.62\textwidth]{figs/anno_example.pdf}
\caption{Examples of visual content and annotations for each dimension. We outline some visual elements by the red box in the image or video to make them easier to identify.}
\label{fig:anno_example}
% \vspace{-8pt}
\end{figure*}


\subsection{Benchmark Examples}
\label{sec:supp_example}
\noindent\textbf{Examples of annotations.}
We show some visual cases with our annotations in \cref{fig:anno_example}. We outline some visual elements by the red box in the image or video to make them easier to identify. We collect our data from various sources, and we crawled some visual content from the Internet by ourselves, ensuring diversity.

\noindent\textbf{Examples of converted QA pairs.}
As we directly annotate the visual elements in the image or video rather than the caption sentence, we can easily convert our annotation into the format of question-answer (QA) pairs, and we name it as CAPability-QA. We use CAPability-QA to evaluate the QA accuracy and the \textit{know but cannot tell} ($K\bar{T}$) metric. In \cref{fig:qa_anno_example}, we also show the same visual cases with \cref{fig:anno_example} for each dimension with converted QA format. Most of the dimensions are converted to the format of a multiple-choice QA task with several options, and the object color, OCR, and character identification dimensions are designed as open-ended QA tasks.

\noindent\textbf{Examples of inference and evaluation.}
In \cref{fig:case_obj_num} and \cref{fig:case_cam_ang}, we visualize the inferred caption of Gemini-1.5-pro, GPT-4o (0806), and Qwen2.5VL-72B in object number dimension and camera angle dimension. In \cref{fig:case_obj_num}, the annotation of the given image is 7 pig trotters. Gemini-1.5-pro refers to the correct number of pig trotters, and we thus give it a positive. GPT-4o recognizes the trotters in the image, but counts with a wrong number, 6, and we thus give it a negative. As for Qwen2.5VL-72B, it says there are 7 chicken thighs in the image, recognizing the wrong object category. However, this is the dimension of the object number, and therefore we only evaluate the correctness and thoroughness of the number, without considering the categories of objects. As Qwen2.5VL-72B does not mention the pig trotters, we give it a miss. In \cref{fig:case_cam_ang}, the annotation of the given image is dutch angle, which means the lens has a certain angle of deflection along the central axis, making the horizon crooked. Gemini-1.5-pro says the image is taken from a medium-high angle, and we classify it into the high angle category, thus negative. GPT-4o explicitly points it out as a subtle dutch angle, thus is classified into the dutch angle category, which is positive. Qwen2.5VL-72B describes the image shot from a slightly elevated angle, and it appears to be a level angle, which is also negative. These two figures show our evaluation pipeline, which is precise and reliable.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.7\textwidth]{figs/qa_anno_example.pdf}
\caption{Examples of visual content and converted QA annotations for each dimension. The visual content is the same as \cref{fig:anno_example}. We outline some visual elements by the red box in the image or video to make them easier to identify.}
\label{fig:qa_anno_example}
% \vspace{-8pt}
\end{figure*}


\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{figs/case_obj_num.pdf}
\caption{Examples of inference and evaluation on object number dimension. We select the inferred caption from Gemini-1.5-pro, GPT-4o, and Qwen2.5VL-72B as instances.}
\label{fig:case_obj_num}
% \vspace{-8pt}
\end{figure*}

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{figs/case_cam_ang.pdf}
\caption{Examples of inference and evaluation on camera angle dimension. We select the inferred caption from Gemini-1.5-pro, GPT-4o, and Qwen2.5VL-72B as instances.}
\label{fig:case_cam_ang}
% \vspace{-8pt}
\end{figure*}