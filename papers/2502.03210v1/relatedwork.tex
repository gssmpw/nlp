\section{Related works}
The limit of infinite network width and finite amount of training
data has been studied extensively, yielding among others the NNGP
kernel \cite{Neal1995BayesianLF,Williams98_1203,Lee18,Matthews18,avidan2024}.
This theory relates network behavior at initialization to training
dynamics \cite{Poole16_3360,Pennington17_04735,Schoenholz17_01232,Xiao18_05393}.
However, the NNGP cannot explain the often superior performance of
finite-width networks \cite{Li15_196,Chizat19_neurips,Lee20_ad086f59,Aitchison2020,Refinetti21_8936},
requiring the inclusion of finite-width effects in theories of FL.

Describing FL in neural networks in a Bayesian framework has lead
to concurrent views: kernel rescaling \cite{Li21_031059,Li2022,Pacelli23_1497,bassetti2024,Baglioni24_027301}
and kernel adaptation \cite{Naveh21_NeurIPS,seroussi23_908,Fischer24_10761,Rubin24_iclr}.
These differ in the choice of order parameters considered and in consequence
also in the explained phenomena.

Beyond these two views, various works study other aspects of networks
in the Bayesian framework: \citet{Canatar22_ieee} investigate experimentally
the effect of hyperparameters on adaptive FL. \citet{ZavatoneVeth21_NeurIPS_I}
study properties of the network prior, whereas we focus on the network
posterior. \citet{Hanin23} obtain a rigorous non-asymptotic description
of deep linear networks in terms of Meijer-G functions. \citet{Cui23_6468}
exploit the Nishimori conditions that hold for Bayes-optimal inference,
where student and teacher have the same architecture and the student
uses the teacher's weight distribution as a prior; the latter is assumed
Gaussian i.i.d., which allows them to use the Gaussian equivalence
principle \cite{Goldt20_14709} to obtain closed-form solutions.

Our work is distinct from perturbative approaches such as \cite{Antognini19_arxiv,Naveh21_064301,Cohen21_023034,Roberts22,Hanin24,Halverson21_035002}
for the Bayesian setting or \cite{Dyer20_ICLR,Huang20_4542,Aitken20_06687,Roberts22,Bordelon23_114009,buzaglo2024}
for gradient-based training that use the strength of non-Gaussian
cumulants of the outputs as an expansion parameter; however, we perform
an expansion in terms of fluctuations around the mean outputs, which
is able to capture phenomena that escape perturbative treatments,
such as phase transitions; this technique corresponds to an infinite
resummation of perturbative terms.

Another line of work focuses on the dynamics of FL: \citet{Saxe14_iclr}
derive exact learning dynamics for deep linear networks, while \cite{Bordelon23_114009}
use dynamical mean-field theory to describe network behavior in the
early stages of training of gradient descent training in different
scaling regimes while we consider networks at equilibrium. \citet{Yang20_14522}
consider the effect of network training dynamics and learning rate
scales in networks. \citet{day2024} study the effect of weight initialization
on generalization and training speed. A different viewpoint considers
spectral properties of FL \cite{Simon23,Yang24} as well as investigating
the effects of learned representations directly \cite{Petrini23_114003}.
\citet{Maillard24} derive polynomial scaling limits of the required
amount of training data.