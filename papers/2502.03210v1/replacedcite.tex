\section{Related works}
The limit of infinite network width and finite amount of training
data has been studied extensively, yielding among others the NNGP
kernel ____.
This theory relates network behavior at initialization to training
dynamics ____.
However, the NNGP cannot explain the often superior performance of
finite-width networks ____,
requiring the inclusion of finite-width effects in theories of FL.

Describing FL in neural networks in a Bayesian framework has lead
to concurrent views: kernel rescaling ____
and kernel adaptation ____.
These differ in the choice of order parameters considered and in consequence
also in the explained phenomena.

Beyond these two views, various works study other aspects of networks
in the Bayesian framework: ____ investigate experimentally
the effect of hyperparameters on adaptive FL. ____
study properties of the network prior, whereas we focus on the network
posterior. ____ obtain a rigorous non-asymptotic description
of deep linear networks in terms of Meijer-G functions. ____
exploit the Nishimori conditions that hold for Bayes-optimal inference,
where student and teacher have the same architecture and the student
uses the teacher's weight distribution as a prior; the latter is assumed
Gaussian i.i.d., which allows them to use the Gaussian equivalence
principle ____ to obtain closed-form solutions.

Our work is distinct from perturbative approaches such as ____
for the Bayesian setting or ____
for gradient-based training that use the strength of non-Gaussian
cumulants of the outputs as an expansion parameter; however, we perform
an expansion in terms of fluctuations around the mean outputs, which
is able to capture phenomena that escape perturbative treatments,
such as phase transitions; this technique corresponds to an infinite
resummation of perturbative terms.

Another line of work focuses on the dynamics of FL: ____
derive exact learning dynamics for deep linear networks, while ____
use dynamical mean-field theory to describe network behavior in the
early stages of training of gradient descent training in different
scaling regimes while we consider networks at equilibrium. ____
consider the effect of network training dynamics and learning rate
scales in networks. ____ study the effect of weight initialization
on generalization and training speed. A different viewpoint considers
spectral properties of FL ____ as well as investigating
the effects of learned representations directly ____.
____ derive polynomial scaling limits of the required
amount of training data.