% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}   
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

\usepackage{enumitem} % no-indent itemize$
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}


% my command
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{amsmath} 
\usepackage{soul}
\usepackage{afterpage}
\newcommand{\sysName}{{\textsc{ChatWise}}} % to be finalized 
\newcommand{\Judy}[1]{\textcolor{black}{#1}} 
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{ChatWise: AI-Powered Engaging Conversations \\for Enhancing Senior Cognitive Wellbeing}

%ChatWise: Enhancing Senior Cognitive Wellbeing through Strategic AI Powered Conversational Support
% Author information can be set in various styles:
% For several authors from the same institution:
\author{Zhengbang Yang, Zhuangdi Zhu \\
        George Mason University, Fairfax, VA, USA \\ \{zyang30,zzhu24\}@gmu.edu}
% \author{Zhengbang Yang \\ {\bf Zhuangdi Zhu} \\
%         George Mason University, Fairfax, VA, USA \\ \{zyang30,zzhu24\}@gmu.edu}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{Zhengbang Yang, Zhuangdi Zhu\\
%   George Mason University, Fairfax, VA, USA \\
%   \texttt{\{zyang30,zzhu24\}@gmu.edu}}

% \author{
%  \textbf{Zhengbang Yang},
%  \textbf{Zhuangdi Zhu},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
 % \textsuperscript{1}George Mason University, Fairfax, VA, USA,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}\vspace{-0.1in} 

Cognitive health in older adults presents a growing challenge. While conversational interventions show feasibility in improving cognitive wellness, human caregiver resources remain overburdened. 
%
AI-based methods have shown promise in providing conversational support, yet existing work is limited to implicit strategy while lacking multi-turn support tailored to seniors.  We improve prior art with an LLM-driven chatbot named \sysName\  for older adults. It follows dual-level conversation reasoning at the inference phase to provide engaging companionship. 
%
\sysName\ thrives in long-turn conversations, in contrast to conventional LLMs that primarily excel in short-turn exchanges. 
%
Grounded experiments show that \sysName\ significantly enhances simulated users' cognitive and emotional status, including those with Mild Cognitive Impairment. 
%

\end{abstract}

\vspace{-0.1in}
\section{Introduction} \vspace{-0.05in}
Cognitive well-being among older adults is a significant social concern, with 14$\%$ of people aged 60 and over experiencing mental health disorders, projected to affect 2.1 billion individuals by 2050 ~\citep{aging-health}. Compared with other age groups, older adults are more vulnerable due to age-related changes in cognitive reserves ~\citep{salthouse2009does} and reduced social connections ~\citep{nicholson2012review, teo2023global}. 
%
%
Interventions through guided conversations have shown efficacy in reducing loneliness and mitigating cognitive decline  ~\citep{yu2023conect,yu2021internet,fiori2012impact,yu2022examining}.  However, access to interventions remains limited.
% 
Advances in artificial intelligence (AI), particularly Large Language Models (LLMs), have shown promise in augmenting human expertise with conversational support~\citep{ryu2020simple,yang2024talk2care,liu-etal-2021-towards}.
%
However, existing AI-based chatbots default to simplistic interactions with implicit goals, which may fail to drive engaging conversations with  strategic multi-turn interactions tailored to seniors.%~

In this work, we aim to draw on the advance of AI to provide older adults with \textit{\textbf{engaging} conversational support} that improves their cognitive function and reduces social loneliness, serving as accessible alternatives to human companions. 
% 
Previous clinical studies aimed at socially isolated older adults showed the existence of causal relationships between interviewer strategy and interviewee response ~\citep{9597428},
%
%
%
% 
%
revealing that conversation behavior can have a \textit{measurable} influence on interlocutors, which inspired us to develop \textit{principled} yet \textit{efficient} methods that transform clinical insights into strategic dialogue design leveraging the advanced reasoning ability of LLMs.
% 

In response, we propose an LLM-driven chatbot named \sysName.
%
%
Unlike traditional chatbot driven with implicit conversational goals, \sysName\ employs \textit{dual-layer} conversation generation, which first derives categorized macro-level information, including user emotion states to suggest meta-conversational strategies, which then guides the micro-level utterance generation to improve both user engagement and cognitive outcomes over multi-turn dialogue interactions.
%
Figure \ref{fig:ChatWise} overviews its development design.
%
%
%
Our work provides multifold contributions:
\begin{itemize}[leftmargin=*]
    \vspace{-0.1in} 
    \item  \sysName\ is a tuning-free yet principled framework for developing  AI chatbots with enhanced \textit{multi-turn}, daily-themed conversational support. Our dual-level conversation generation design integrates clinical insights into  LLM reasoning and can be readily applied to various LLMs. 
    %
    \vspace{-0.1in}
    \item   
    \sysName\ follows grounded development and evaluation using digital twins~\citep{hong2024conect}, which are simulated users constructed by tuning LLMs on de-identified, real-world dialogues  between seniors and professional caregivers of a clinical trial~\citep{iconect-detail}. Interactions with digital twins demonstrated that \sysName\ significantly enhances users' engagement and cognitive status with multi-turn interactions, especially for users with Mild Cognitive Impairment (MCI)~(Figure \ref{fig:overall}).
    %These results indicate that our approach fosters more meaningful and sustained interactions tailored to senior's cognitive and emotional needs.
    \vspace{-0.1in}
    \item Our comparative studies demonstrated that integrating macro-level strategies into conversation generation is the key contributor to enhancing user engagement.
    %
    Its gain on cognitive metrics remains consistent across different backbone LLMs. \sysName's {advantages become more pronounced in longer conversations}, in contrast to conventional LLMs that primarily excel in short-turn question-answer exchanges. 
     
    \vspace{-0.1in}
    \item Our dialogue analysis revealed key empirical insights, including the pattern of predominant interaction strategies across users. While transient user emotional fluctuations were less tractable, multi-turn conversations with \sysName\ have shown to notably improve user emotions over time. This highlights the value of long-term, principled conversational support in enhancing seniors' cognitive and emotional well-being. We hope these findings can inspire future research, including RL-based LLM optimization.
\end{itemize}

\begin{figure}
    \centering
    \vspace{-0.15in}
    \includegraphics[width=\linewidth]{fig/ChatWise5.png}
    \caption{Overview of \sysName\ Development.}
    \label{fig:ChatWise}
    \vspace{-0.1in}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{fig/overall.png}
    \centering
    \caption{ \sysName\ notably improves multiple user cognitive metrics through 10-turn dialogues (Sec~\ref{sec:evaluation}).}
    \vspace{-0.2in}
    % \caption*{ }
    \label{fig:overall}
\end{figure}
 



\vspace{-0.1in}
\section{Related Work}\vspace{-0.05in}
\noindent\textbf{AI-powered Chatbots for Seniors: }
Social interaction plays a crucial role in the cognitive health of seniors ~\citep{patil2024interventions,fiori2012impact}.
%
% 
AI-powered chatbots have emerged as a promising alternative ~\citep{rodriguez2023qualitative,owan2023conversational}.
%
Recent efforts span commercial products \citep{seniortalk,elliq} and research prototypes focused on emotional support and audio assistance ~\citep{yang2024talk2care,liu2021towards,seniortalk,ryu2020simple,hong2024conect}. Unlike prior work, our approach prioritizes user conversational \textit{engagement}, which has been empirically shown to enhance both cognitive and emotional status.
%


\noindent\textbf{Conversational Strategies: }
%
\citet{liu-etal-2021-towards} curated a dataset with annotated strategies, demonstrating the effectiveness of Helping Skills Theory~\cite{hill2020helping} in providing emotional support. 
%
\citet{YUAN2023100384} examined the causal relationships between dialogue acts (DAs) and participants’ emotional states in a clinical trial \citep{iconect-detail}, emphasizing the impact of strategic interventions in tele-mediated dialogues.
%
\citet{seo2021learning} identified key strategies for improving child patient-provider communication through semi-structured interviews. 
%
{Few works have systematically integrated these strategies into the reasoning flow of AI chatbots. Our approach fills this gap by embedding structured conversational strategies for automatically enhancing user engagement.}

 
%
%
\noindent\textbf{Multi-turn chatbot exploration: } Recent advances in AI dialogue systems predominantly focused on short-turn interactions ~\citep{owan2023conversational,dam2024complete}. While some pioneering research has explored multi-turn optimization through Reinforcement Learning (RL) approaches for LLMs \citep{verma2022chai,zhou2024archer,abdulhai2023lmrl,gao2025regressing}, these methods were not specifically designed for supporting senior dialogue engagement. Our approach is  tuning-free yet compatible with RL-driven methods, enabling future extensions through hierarchical RL to further enhance multi-turn conversation engagement.

%
%

\vspace{-0.05in}
\section{\sysName\ Design} \label{sec:sys-design} \vspace{-0.1in}
\sysName\ employs a dual-level framework with  a \textbf{strategy provider} $\pi_s$ that induces macro-level actions, and an \textbf{utterance generator} $\pi_{u}$ for deriving utterances based on  suggested strategies (Figure~\ref{fig:ChatWise}).
 
\noindent\textbf{Strategy Candidates:}
 %
To construct a strategy candidate set, we extract Dialogue Acts (DAs) from real telehealth clinical trials~\cite{YUAN2023100384}, where DAs serve as atomic communicative units that convey distinct conversational intentions~\cite{searle1976classification}. We further integrate these with strategies from prior emotional support dataset~\citep{liu-etal-2021-towards} to form a comprehensive set. We refine each strategy with definitions and examples as in-context prompts to enhance $\pi_s$ reasoning.%~\Judy{(See \hl{Appendix}).}

\noindent\textbf{From Strategy to Utterance:}
$\pi_s$  reasons over the dialogue history $\tau$ to capture user’s emotion  $e$ and derive one to two strategies, $\{s_i\}_{i\leq2} \sim \pi_s(\tau, e)$.    
%
Our rationale follows counseling study~\citep{zhang2020balancing}  that dialogue intentions can be \textit{forward}, \textit{e.g.}  initiating new topics via an open question, or \textit{backward}, \textit{e.g.} responding with acknowledgment.  $\pi_s$ is prompted to flexibly employ one or both as needed.
%  
Selected strategies, user’s current emotion, and conversation history serve as inputs for $\pi_u$ to generate utterances. 
%
To ensure natural flow, $\pi_u$ will first improvise a few rounds before following suggested strategies. Drawing on clinical guidance, 
$\pi_u$ encourages users to choose topics rather than imposing them.
%

\vspace{-0.1in}
\section{Empirical Evaluation} \vspace{-0.1in}

Below we summarize experiment configurations, metrics, and main findings of \sysName. More experimental details are available in Appendix \ref{sec:prompt} and \ref{sec:data_gen}.

\vspace{-0.05in}
\subsection{Conversation Generation}\vspace{-0.05in}
For controlled and responsive development of \sysName, we employed 9 digital twins provided by ~\citet{hong2024conect} as simulated users with different personas, which are fine-tuned LLMs to  approximate senior individuals with MCI symptoms using \textit{real, de-identified} dialogue data from a clinical trial ~\cite{iconect-detail}. 
%
We collected 20 trajectories of conversations between \sysName\ and each digital twin, where a conversation contains 10 turns, each turn with a two-way utterance exchange between \sysName\ and digital twin (user).
%

\vspace{-0.05in}
\subsection{Engagement Evaluation} 
\label{sec:evaluation} \vspace{-0.05in}
%
%
\noindent\textbf{User verbosity:}
%
We primarily measure user engagement through verbosity, \textit{i.e.} the user-to-chatbot token ratio, defined as $\frac{\text{tokens}_{user}}{\text{tokens}_{chatbot}}$, as a clinical study~\citep{yu2021internet} suggested reducing the moderator talkativeness while encouraging participant (user) expression. 
%
%

\noindent\textbf{Cognitive Metrics: }
%
%
\citet{see-etal-2019-makes} defines 9 aspects for evaluating conversation quality, from which we select 5 focused on engagement assessment: {\textit{Engagingness}}, {\textit{Interestingness}}, {\textit{Listening}}, {\textit{Fluency}}, and {\textit{Making Sense}}. While \textit{Interestingness} assesses overall dialogue quality, the others primarily evaluate \textbf{user} interactions.

We compute the \textbf{win rate}~\citep{NEURIPS2023_a85b405e} by comparing dialogues generated with and without \sysName, using the same pretrained LLM (GPT-4o) as the utterance generator and identical system prompts, except for input from $\pi_s$ to $\pi_u$. Dialogue pairs are randomly matched within the same digital twin for evaluation.


% \vspace{-0.2in}
\begin{table*}[!hbtp]
    \centering
    \resizebox{0.95\textwidth}{!}{%
        \begin{tabular}{lcccccc}
            \toprule
            \textbf{Strategy Provider} & \textbf{Verbosity } $\uparrow$& \textbf{Engagingness WR} $\uparrow$ & \textbf{Interestingness WR} $\uparrow$ & \textbf{Listening WR} $\uparrow$ & \textbf{Fluency WR} $\uparrow$ & \textbf{Making sense WR} $\uparrow$ \\
            \midrule
            w/o \sysName\  & 0.7398 & 0.3389 & 0.3833 & 0.3593 & 0.4111 & 0.4056 \\
            GPT-4o        & 0.8635 & \textbf{0.6833} & \textbf{0.6222} & 0.6222 & 0.5778 & 0.5833 \\
            o3-mini       & \textbf{0.8643} & 0.6778 & \textbf{0.6222} & \textbf{0.6778} & \textbf{0.6222} & \textbf{0.6167} \\
            Llama 3.1-405B & 0.8083 & 0.6222 & 0.6056 & 0.6222 & 0.5667 & 0.5833 \\
            \bottomrule
        \end{tabular}
    } \vspace{-0.05in}
    \caption{Performance on 10-turn dialogue across different strategy providers. W/o \sysName\ is the baseline without applying any strategy, whose win rates (\textbf{WR}s) are averaged across strategy providers. (Appendix \ref{sec:WR_baseline})}. 
    % \textit{Engagingness}, \textit{Interestingness}, \textit{Listening}, \textit{Fluency}, and \textit{Making sense},  t
    \label{tab:overall_performance}
\end{table*}


% \vspace{-0.05in}
\subsection{\sysName\ Performance and Analysis}\vspace{-0.05in}

We evaluated \sysName\ using different LLM backbones as   strategy providers, including GPT-4o, o3-mini, and Llama3.1-405B.
%  
As shown in Table \ref{tab:overall_performance},  \sysName\ consistently outperformed non-strategic dialogues across all evaluation metrics.


%
% 

\noindent\textbf{\sysName\ robustly enhances user engagement: } Regardless of  the capability of pretrained LLMs, our design consistently  improves user engagement and dialogue quality with a large margin across all tested LLMs as the strategy provider. %demonstrating its effectiveness.

\noindent\textbf{Reasoning ability can  transfer to improve engagement: } 
%
The best-performing model in our setting, o3-mini, was optimized for STEM reasoning tasks, which indicates that strong reasoning ability can also benefit inferring dialogue strategies.
% 


\subsection{Accumulated Multi-Turn Analysis} 
%\vspace{-0.05in}
%
%
We analyzed \sysName’s performance over increasing conversation turns, as shown in Figure \ref{fig:accumulated}, which presents accumulated metrics and win rates. Results revealed a clear upward trend in all key engagement metrics—especially \textit{verbosity}, \textit{engagingness}, \textit{interestingness}, and \textit{listening}—demonstrating that \sysName’s effects strengthen as dialogues progress, while baseline gradually degrades, which highlights \sysName’s \ul{capability in enhancing long-term conversational engagement}.
%

\begin{figure*}[th!]
    \vspace{-0.1in}
    \centering
    \resizebox{0.9\textwidth}{!}{
    \begin{subfigure}{0.24\linewidth}
        \includegraphics[width=\linewidth]{fig/o3mini/verbosity.png}\vspace{-0.05in}
        \captionsetup{justification=centering}
        \caption{Verbosity.}\vspace{-0.05in}
    \end{subfigure} \hfill
    \begin{subfigure}{0.24\linewidth}
        \includegraphics[width=\linewidth]{fig/o3mini/engagingness.png}\vspace{-0.05in}
        \captionsetup{justification=centering}
        \caption{Engagingness Win Rate.}\vspace{-0.05in}
    \end{subfigure} \hfill
    \begin{subfigure}{0.24\linewidth}
        \includegraphics[width=\linewidth]{fig/o3mini/interestingness.png}\vspace{-0.05in}
        \captionsetup{justification=centering}
        \caption{Interestingness Win Rate.}\vspace{-0.05in}
    \end{subfigure} \hfill
    \begin{subfigure}{0.24\linewidth}
        \includegraphics[width=\linewidth]{fig/o3mini/listening.png}\vspace{-0.05in}
        \captionsetup{justification=centering}
        \caption{Listening Win Rate.}\vspace{-0.05in}
    \end{subfigure}
    }

    \caption{\sysName\ performance with o3-mini as strategy provider. W/o \sysName\ is the
baseline and GPT-4o servers as utterance generator in all settings. Evaluation starts by skipping two warming-up turns (Sec~\ref{sec:sys-design}).}
    \label{fig:accumulated}
\end{figure*}

\vspace{-0.05in} \subsection{Ablation Study} \vspace{-0.05in}
%

We evaluated \sysName\ without user \textit{emotion} in the strategy provider’s output (Table \ref{tab:ablation}). Its consistent performance confirms that \ul{strategies are the primary driver of engaging conversations}.
\begin{table}[h]
    \vspace{-0.1in}
    \centering
    \resizebox{0.45\textwidth}{!}{%
        \begin{tabular}{lccc}
            \toprule
            \textbf{Strategy Provider} & \textbf{GPT-4o} & \textbf{GPT-4o w/o emotion} & \textbf{w/o \sysName} \\
            \midrule
            Verbosity & 0.8635 & 0.8463 & 0.7398 \\
            \bottomrule
        \end{tabular}
    }\vspace{-0.05in}
    \caption{User verbosity under different strategy providers. GPT-4o w/o emotion keeps the strategy provider with user emotion information removed.}
    \label{tab:ablation} \vspace{-0.1in}
\end{table}


\vspace{-0.05in}\subsection{\sysName's Robustness to User Persona}\vspace{-0.05in}
%
%
%
Figure \ref{fig:robustness} shows the significant gain of \sysName\ interacting with all simulated users,
%
indicating its \ul{robustness to support varying senior characteristics}.
 
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{fig/robustness.png}
    
    \caption{Users' dialogue \textit{verbosity} (log-normalized, Appendix \ref{sec:log-normalization}) across 9 digital twins using different LLMs as strategy providers, compared with baseline (w/o \sysName).}
    \label{fig:robustness} \vspace{-0.2in}
    
\end{figure}
 

\subsection{Dialogue Analysis} \vspace{-0.05in}
%

We identified 4 digital twins for which \sysName\ was most effective and collected 40 additional dialogues from each to conduct a deeper analysis, with the main findings summarized below.

\noindent \textbf{Transient user emotions}: We defined emotion transition triplets as $(\text{e}_i, \text{s}, \text{e}_{i+1})$, where $\text{e}_i$ ($\text{e}_{i+1}$) represents the user’s emotion at turn $i$ ($i+1$), and $s$ is the strategy provided by \sysName\ at turn $i$.
%  
We computed the average occurrence of emotion triplets and showed the top 15 most frequent in Figure \ref{fig:insight_DA_emo}. Most triplets reflect unchanged user emotions, indicating the challenge of detecting or influencing emotional shifts within a short turn.

\noindent {\textbf{Long-term user emotions}}: 
We analyzed user emotion changes from the beginning to the end of dialogues and calculated averaged occurrence across digital twins (Figure \ref{fig:emo_change}).
%
Over 48\%  users experienced emotional shifts post-dialogue, with significantly more positive changes after engaging with \sysName.
%
This implies that while transient emotions are difficult to track, \ul{strategic conversational support can improve user emotions over time}.


\noindent {\textbf{Predominant strategies}}: 
We identified the top 10 most frequent strategies across digital twins and found that  \ul{predominant interaction strategies remained consistent across user types} (See Appendix \ref{sec:personalized}).
%
Particularly, \textit{Open Question}, \textit{Statement-non-opinion}, and \textit{Acknowledgment} strategies dominate \sysName\ driven conversations, suggesting their potential efficacy in fostering engaging dialogues.

\begin{figure}
    \centering
    \includegraphics[width=0.82\linewidth]{fig/emotion_overall.png}
    \vspace{-0.15in}
    \caption{Average occurrence of each emotion transition triplet across the samples of each digital twin.}
    \label{fig:insight_DA_emo} 
    \vspace{-0.15in}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.82\linewidth]{fig/insight/emo_change.png}
    \vspace{-0.1in}
    \caption{Occurrence of user emotional changes. For instance, ``neutral-joy'' indicates user emotion transitioning from neutral (conversation begins) to joy (ends).}
    \label{fig:emo_change}\vspace{-0.2in}
\end{figure}

\vspace{-0.1in}
\section{Conclusion and Future Work} \vspace{-0.05in}
We introduce \sysName, a tuning-free yet principled framework for developing  AI chatbots that foster long-term user engagement. Our dual-level generation design integrates clinical insights into LLM reasoning and is compatible with various LLMs. 
%
Extensive evaluations based on de-identified clinical data showed that our method robustly enhances the engagement and cognitive status of seniors. 
%
Moving forward, we aim to 1) enhance multi-turn interaction via RL and 2) validate its effectiveness through real user studies.

%
%

\clearpage
\section*{Limitations} 
%
All digital twins were provided as fine-tuned GPT-3.5 APIs, and its high economic cost, at the time when this project was going, constrained the volume of dialogues we generated. To mitigate this, future work will explore training digital twins using LLaMA 3.1 to reduce sampling costs while maintaining conversational quality. Additionally, our study relies on simulated dialogues rather than real user interactions. A real user study is needed to validate the system’s effectiveness in dynamic settings, which we plan to incorporate into our future research.
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{acl_latex}
% \input{acl_latex.bbl}

\clearpage
\newpage
\appendix

\section{Appendix}
\label{sec:appendix}
\subsection{Prompt design}
\label{sec:prompt}
The prompt used in the experiments and structured output design are available at \href{https://anonymous.4open.science/r/ChatWise-D5E0}{https://anonymous.4open.science/r/ChatWise-D5E0}, including:
\begin{itemize}
    \vspace{-0.1in}
    \item Strategy provider system prompt.
    \vspace{-0.1in}
    \item Moderator initial system prompt.
    \vspace{-0.1in}
    \item Moderator system prompt with strategies.
    \vspace{-0.1in}
    \item Strategy provider system prompt for ablation study.
    \vspace{-0.1in}
    \item Moderator system prompt with strategies for ablation study.
    \vspace{-0.1in}
    \item Structured output class for OpenAI models as strategy provider.
    \vspace{-0.1in}
    \item Structured output class for OpenAI models as strategy provider in ablation study.
    \vspace{-0.1in}
    \item System prompt for GPT-4o to extract the strategy given by Llama3.1.
\end{itemize}

\subsection{Data Generation Configuration}
We used GPT-4o as the utterance generator in ChatWise and tested different LLM backbones as strategy providers. Considering the Llama3.1-405B does not support structured output, we applied GPT-4o as the strategy extractor to structure the strategy output of Llama3.1-405B. The following are the configurations of each model:

\label{sec:data_gen}
\noindent\textbf{Utterance generator (GPT-4o):}\\
n=1\\
max\_tokens=1024\\
top\_p=1\\
temperature=1\\
\noindent\textbf{GPT-4o, o3-mini as strategy provider:}\\
n=1\\
max\_tokens=1024\\
top\_p=1\\
temperature=1\\
response\_format=Strategy\\
\noindent\textbf{Llama3.1-405B as strategy provider:}\\
top\_p: 0.9\\
max\_tokens: 1024\\
temperature: 0.6\\
presence\_penalty: 0\\
frequency\_penalty: 0\\
\noindent\textbf{GPT-4o as strategy extractor:}\\
n=1\\
max\_tokens=1024\\
top\_p=1\\
temperature=1\\


\subsection{Win rate for w/o \sysName}
\label{sec:WR_baseline}
The Win rate for w/o \sysName\ is defined as:
% \[\text{Win rate}_{w/o \sysName}=1-average(\text{Win rate}_{GPT-4o}, \text{Win rate}_{o3-mini}, \text{Win rate}_{Llama3.1-405B})
% \]
\begin{equation*}
\begin{split}
1 - \text{average}(&\text{WR}_{GPT-4o} \\
&+ \text{WR}_{o3-mini} \\
&+ \text{WR}_{Llama3.1-405B})
\end{split}
\end{equation*}
,where $\text{WR}_{X}$ is \sysName's Win rate against the baseline with $X$ as strategy provider.

\subsection{Dialogue Acts}
\input{appendix/strategies}

\subsection{Log-normalization}
\label{sec:log-normalization}
The following is the log-normalization function, where y is the normalized result, x is the input variable.
\begin{equation*}
    y=\ln{\left(4x+1\right)}
\end{equation*}



\subsection{Additional results for Accumulated Multi-Turn Analysis}
\input{appendix/accumlulated}
\vspace{-0.2in}
\subsection{Primary Strategies}

We calculated the average occurrence of each strategy across each digital twin and listed their top 10 most frequently occurring strategies, as shown in Figure \ref{fig:insight_DA}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{fig/da_overall.png}\vspace{-0.1in}
    \caption{Strategy occurrence across digital twins.}%Average occurrence of each sthe samples of each 
    \label{fig:insight_DA}
\end{figure}

\subsection{Personalized Dialogue Analysis}
\label{sec:personalized}
\input{appendix/dialogue_analysis}



\end{document}
