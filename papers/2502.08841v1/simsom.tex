\documentclass{article}
\usepackage[round, sort&compress, numbers]{natbib}
\usepackage{setspace}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx} % Required for inserting images
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{url}
\usepackage{datetime}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{array}
\usepackage{float}
\usepackage{xcolor,colortbl}
\usepackage{siunitx}
\usepackage{authblk}

\sisetup{table-format=1.5} % Align numbers to 5 decimal places
\newdateformat{mydateformat}{\monthname[\THEMONTH] \THEDAY, \THEYEAR}

% result-based title (for NHB)
\title{Delayed takedown of illegal content on social media \\ makes moderation ineffective}

% more general title
%\title{Combating illegal content on social media: How takedown delay and takedown deadlines matter}

\author[1]{Bao Tran Truong}
\author[1]{Sangyeon Kim\thanks{Corresponding author. Email: ski15@iu.edu}}
\author[2]{Gianluca Nogara}
\author[2]{Enrico Verdolotti}
\author[2,3]{Erfan Samieyan Sahneh}
\author[4]{Florian Saurwein}
\author[4]{Natascha Just}
\author[5]{Luca Luceri}
\author[2]{Silvia Giordano}
\author[1]{Filippo Menczer}

\affil[1]{Observatory on Social Media, Indiana University, Bloomington, USA}
\affil[2]{Department of Innovative Technologies, 
University of Applied Science and Arts, Switzerland}
\affil[3]{Department of Applied Science and Arts, University of Bologna, Italy}
\affil[4]{Media \& Internet Governance Division, University of Zurich, Switzerland}
\affil[5]{Information Sciences Institute, University of Southern California, USA}

%\date{\mydateformat\today}
\date{}
% \newcommand{\SI}{Supplementary Material}
\newcommand{\simsom}{\emph{SimSoM}}

\begin{document}

\maketitle

%\def\thefootnote{*}\footnotetext{To whom correspondence should be addressed}

\begin{abstract}
Social media platforms face legal and regulatory demands to swiftly remove illegal content, sometimes under strict takedown deadlines. However, the effects of moderation speed and the impact of takedown deadlines remain underexplored. This study models the relationship between the timeliness of illegal content removal and its prevalence, reach, and exposure on social media. By simulating illegal content diffusion using empirical data from the DSA Transparency Database, we demonstrate that rapid takedown (within hours) significantly reduces illegal content prevalence and exposure, while longer delays decrease the effectiveness of moderation efforts. While these findings support tight takedown deadlines for content removal, such deadlines cannot address the delay in identifying the illegal content and can adversely affect the quality of content moderation. 
\end{abstract}

\section*{Introduction}

Illegal content, such as copyright violations, child sexual abuse material, and incitement of violence and terrorism, poses serious harm and requires effective moderation to prevent its spread \cite{yar2018failure, jain2020illegal}. Platforms use various tools for moderation. Interventions like content labels \cite{morrow2022emerging, martel2023misinformation}, accuracy reminders \cite{bhuiyan2018feedreflect, pennycook2020fighting, pennycook2021shifting, pennycook2022accuracy}, and designs that add friction to content sharing \cite{jahn2023friction, tomalin2023rethinking} encourage users to critically engage with content. Other approaches reduce the visibility of harmful content by limiting its searchability, recommendation, and engagement \cite{gillespie2022not, macdonald2024moderating}. However, for illegal content, laws usually demand strict interventions, such as the removal of illegal content once platforms become aware of it.

Despite existing rules, platforms have often been criticized for failing to act on illegal content, even under government and court orders \cite{denardis2015internet, de2020online}, raising widespread concerns about their commitment to and effectiveness of content moderation \cite{hoffman2020liability}. 
In response, various countries have adopted regulations imposing legal requirements on content moderation. Germany led early efforts by passing the Network Enforcement Act (NetzDG) in 2017, followed by a 2020 act to fight hate on the Internet in France (Loi Avia), and Austria's Communication Platform Law (KoPl-G) and Australia's Online Safety Act in 2021. In the United States, the proposed Digital Services Oversight and Safety Act (H.R.6796) was rejected in 2023. At the European Union level, the Digital Services Act (DSA) was enacted in 2022, replacing national regulations. All of these recent laws contain specific provisions for content moderation, typically requiring platforms to operate complaint procedures, implement measures against illegal content, and publish transparency reports on the volume of complaints and content moderation decisions \cite{just2024enhancing}. 

While these regulations share the goal of moderating illegal content, they diverge in their specific requirements for takedown deadlines. For instance, NetzDG and KoPl-G, now replaced by the DSA, mandated immediate content review, requiring platforms to remove ``obviously illegal content'' within 24 hours and other illegal content within seven days. In contrast, the DSA sets no explicit deadlines, only requiring platforms to make moderation decisions in a ``timely manner'' (Art.~16 (6)), and to ``act expeditiously'' to remove or disable access to illegal content (Art.~6 (1)).

From a governance perspective, these regulations raise key questions: 
How quickly do platforms act in practice, how does moderation speed affect the spread of illegal content, and are takedown deadlines an effective regulatory instrument? 
These questions remain largely unexplored.  

While immediate content removal is intuitively the most effective approach, there is limited research on moderation speed, compliance, and effects.   
Modeling suggests that moderation delays interfere with intervention effectiveness \cite{schneider2023effectiveness}. 
It was shown, for example, that a four-hour delay reduces prevalence by 55.6\% compared to 93.8\% for removal within 30 minutes \cite{bak2022combining}.
%Bak-Coleman et al. found that outright removal can be highly effective, resulting in a median reduction of 93.8\% in total posts on a topic, if implemented within 30 minutes. With a four-hour delay, their model suggests reductions of 55.6\% \cite{bak2022combining}. 
However, 
%fast moderation can come with significant downsides, requiring a balance between speed and accuracy. Rapid 
rapid removal may reduce precision, causing false positives and concerns about over-moderation or censorship \cite{chancellor2017multimodal, jhaver2019human, jiang2023trade}. 
In addition, most existing studies focus on misinformation, such as conspiracy theories, which may not generalize to illegal content, and often examine specific emergencies, such as COVID-19 anti-vaccination narratives \cite{broniatowski2022evaluating} or the January 6 Capitol Riot \cite{goldstein2023understanding}, where platforms adopted exceptional measures \cite{Science_eLetter_2024}. 
To inform legal frameworks, it is crucial to also study platform standard practices during routine, non-emergency periods.

Measuring the impact of content-removal requirements at scale is challenging due to limited and inconsistent data. Although platforms self-report moderation speed in community standard enforcement reports, they often use varying metrics for measurement. More standardized data are available under transparency requirements from NetzDG, KoPl-G, and DSA, but scrutiny of these data remains rare \cite{kaushal2024automated}. 
%Moreover, it remains uncertain whether compliance with the DSA's flexible deadlines is sufficient to achieve desired results. If current practices fall short, what is the maximum delay for content removal to still be effective? At the moment, available data allow to calculate the speed of content moderation, but there is lack of research that assesses the effects moderation delay.

In this paper, we examine how different takedown delays affect the spread of illegal content. Using an agent-based model parameterized with empirically estimated time delays from the DSA Transparency Database (DSA-TDB) \cite{dsadatabase}, 
%and Facebook's NetzDG report \cite{fbnetzdb}, 
we measure the reduction in the prevalence of illegal content and exposure to it as a function of takedown delays. While longer delays predictably result in more users being exposed to illegal content, the exact, non-linear relationship between the two dynamics of takedown and exposure is elucidated by the experiments. The results indicate a steep decline in the effectiveness of moderation as time delay increases. Prompt removal of illegal content significantly reduces visibility compared to systems with no or slow moderation. However, the impact of content removal becomes negligible as the delay extends beyond a couple of weeks.
%
In discussing these findings from a regulatory perspective, we note that while tight content removal deadlines could potentially contribute to preventing the spread of illegal content, they cannot address the time before illegal content is detected. Tight deadlines may also compromise the quality of content moderation. 

\section*{Results}

\subsection*{Modeling illegal content diffusion}

% Base model of user behavior 
We use \simsom~\cite{truong2023quantifying}, an agent-based model that mimics information diffusion on social networks such as Twitter/X, Mastodon, Bluesky, Threads, or Instagram, where follower relationships need not be reciprocal. 
Our simulations are conducted on a follower network derived from empirical Twitter data (see \nameref{sec:methods}). 
At each time step, a user either posts a new message or reposts a message from their newsfeed. 
The likelihood that a message is selected by a user for reposting is determined by three factors: the message's appeal, its social engagement (number of reshares), and its recency. 
The new or reposted message appears on the newsfeeds of the user's followers. 
We use empirical data to model heterogeneous user activity; most agents post rarely, and only a few are hyperactive (see Fig.~\ref{fig:activity_illegal}a and \nameref{sec:methods}). 

\begin{figure}
\centering
\includegraphics[width=\linewidth]{plots/activity_illegal.pdf}
\caption{User activity distribution and probability of posting illegal content. (a) Complementary cumulative distribution of daily average Twitter user activity based on 81{,}633{,}118 posts by 37{,}882{,}999 accounts during nine days in January 2023 (orange line) and a power-law fit (dashed line). (b) Histograms of illegal content posting probability for low-risk (orange) and high-risk (purple) users, for $p=10^{-4}$ (top), $p=10^{-3}$ (middle), and $p=10^{-2}$ (bottom), based on a large random sample.}
\label{fig:activity_illegal}
\end{figure}

We model the legality of a message as a simple binary flag and assume it to be independent from its appeal. This means that legal and illegal messages in the same feed have the same chance of being reshared. 

% Who posts illegal content?
We further assume that illegal content is generated in two ways: (i) by a small fraction $s_{H}$ of ``high-risk accounts" (\textit{H}) that frequently share illegal content, such as child pornography, scams, or terrorist propaganda; and (ii) by the rest of the network, ``low-risk accounts" (\textit{L}) that might occasionally violate the law, such as sharing copyrighted content. We set $s_{H}=0.1$ to reflect a minority of high-risk accounts. 
%While lower values might be more realistic, simulations using very low $s_{H}$ are exceedingly time-consuming. 
We show that the results are robust to this parameter choice in Section~\nameref{sec:illegal_prob}. 

We model the probability of posting illegal messages by users in the two groups using beta distributions (see \nameref{sec:methods}), from which we calculate the \emph{illegal content probability} $p$.  Fig.~\ref{fig:activity_illegal}b plots the probability distributions of posting illegal messages by each group for different values of $p$. 
We use $p=10^{-2}$ and show in Section~\nameref{sec:illegal_prob} that the effectiveness of moderation does not critically depend on this parameter.

To understand the dynamics of illegal content removal, let us consider the lifespan of such content on the platform. While some illegal content is immediately detected and removed using automated detection and decision systems, other is missed due to detection limitations. In such cases, content moderation is delayed until platforms either detect the content themselves later or receive external notification. Additional delays may occur when platforms assess the content against their community standards and national laws. If a violation is confirmed, either the content is removed globally or access is restricted in certain jurisdictions. 

We model takedown delay by making the simplifying assumption that at any time step, there is a constant probability $p_s$ that a piece of illegal content is not detected and removed, and therefore survives. 
This yields an exponential survival curve for the delay $t$, $P(t) \sim 2^{-t / \tau}$, where the \emph{illegal content half-life} $\tau$ is the time taken for half of the illegal content to be removed (median removal time since publication). 
We use $\tau$ to parameterize content takedown delay. 
For easier interpretation, we can alternatively define the \emph{expected time delay} as $\tau / \ln 2$. 
Since $P(t) \approx p_s^t$ in the simulation, we set $p_s = 2^{-1/\tau}$. When a message is removed, it is deleted from all user feeds.

\subsection*{Calibration}
\label{sec:calibration}

We wish to derive a realistic range of $\tau$ values. 
Effective model calibration depends on reliable empirical data on illegal content takedown delay. 
We extract Statements of Reason (SoRs) about illegal content removal from the DSA Transparency Database for five major social media platforms: Facebook, Instagram, YouTube, TikTok, and Snapchat (see \nameref{sec:methods}). 

Fig.~\ref{fig:timedelay_curve} shows empirical survival time curves for the five social media platforms from DSA-TDB SoR records.  
By fitting exponential survival time curves $P(t) \sim 2^{-t/\tau}$ to this empirical data, we estimate that the half-life is $\tau \approx 21$ days for Facebook, 60 days for Instagram, 198 days for YouTube, 4 days for TikTok, and 94 days for Snapchat.

The exponential survival curves (Fig.~\ref{fig:timedelay_curve} top) fit the data reasonably well for short delays, less for extended times. 
We can get an intuition for this by noting that the longer the survival of an illegal piece of content, the lower its visibility, and therefore the lower its likelihood to be reported and removed --- $p_s$ increases instead of remaining constant. 
To better capture the tails of the data, we also estimate $\tau$ by fitting the linear-log transformed data to $\log_2 P(t) \sim -t / \tau$. This procedure yields longer half-life estimates $\tau \approx 390$ days for Facebook, 340 days for Instagram, 786 days for YouTube, 1,221 days for TikTok, and 360 days for Snapchat (Fig.~\ref{fig:timedelay_curve} bottom).
These results highlight the heterogeneity in removal delays within and across platforms. 

\begin{figure}
\centering
\includegraphics[width=\linewidth]{plots/fig2.png}
\caption{
Distributions of time delay in illegal content takedown on five platforms. Each plot reports empirical data from the DSA-TDB (green dots) along with an exponential fit (purple line). Top: Fit obtained from the raw data. Bottom: Fit obtained from a linear-log transformation of the data to better capture the tails of the curves. 
%Time delay in illegal content takedown on (a)~Facebook, (b)~Instagram, (c)~YouTube, (d)~TikTok, and (e)~Snapchat. Each plot reports empirical data from the DSA-TDB (green dots) along with an exponential fit (purple) and a linear-log transformed exponential fit that better captures the tail of the curve (orange).  
%FB HL Empirical: 16 days
%FB HL Fitted: 21 days
%FB HL Tail fitted: 390 days
%IG HL Empirical: 50 days
%IG HL Fitted: 60 days
%IG HL Tail fitted: 340 days
%YT HL Empirical: 125 days
%YT HL Fitted: 198 days
%YT HL Tail fitted: 786 days
%TT HL Empirical: 3.6 days
%TT HL Fitted: 4.3 days
%TT HL Tail fitted: 1221 days
%SC HL Empirical: 52 days
%SC HL Fitted: 94 days
%SC HL Tail fitted: 360 days
}
\label{fig:timedelay_curve}
\end{figure}

While none of the exponential survival curves perfectly fit the empirical takedown delay data, they provide reasonable bounds for the half-life parameter. Thus, in our experiments, we model various takedown scenarios by exploring a broad range of illegal content half-life values, $2^{-4} \leq \tau \leq 2^9$. 

\subsection*{Effects of time delay} 
\label{sec:main_results}

We measure the effects of time delay in content takedown by the reduction of illegal content throughout the network, which can be quantified by multiple metrics. 
\textit{Prevalence} measures the number of posts containing illegal content that are in the system at steady state (see \nameref{sec:methods}). The more illegal content is present in the system, the more harm it can potentially cause. 
However, a piece of illegal content may be removed from a user's feed before the user sees it. Therefore we also measure harm more directly using two exposure metrics. 
\textit{Impressions} measures the number of times illegal content is seen by users. Note that a single user can be exposed to many posts with illegal content, or even the same post multiple times. 
\textit{Reach} is a complementary measure of exposure, capturing the number of unique users who see any amount of illegal content. 
For each of these measures, we calculate the reduction percentage relative to a baseline measure without any content removal (see \nameref{sec:methods}). 

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{plots/main_res_dsa1year.pdf}
\caption{Impact of takedown delay on illegal content. The reduction in the prevalence (pink), reach (green), and impressions (purple) of illegal content is plotted as a function of the expected takedown delay ($\tau / \ln 2$). 
%Since takedown delay follows an exponential distribution with half-life $\tau$, the expected takedown delay is calculated by $\tau / \ln 2$. 
Shading represents the 95\% confidence intervals estimated using non-parametric bootstrapping. 
%Fieller's theorem \cite{fieller1954some}. 
The vertical lines indicate the expected takedown delays obtaining by fitting the DSA data to exponential curves for different platforms. 
}
\label{fig:res_frac}
\end{figure} 

Fig.~\ref{fig:res_frac} plots the reduction in illegal content prevalence, impressions, and reach as a function of the expected takedown delay, calculated by $\tau/ \ln 2$. Content removal has maximal impact under rapid takedowns. For delay below nine hours ($\tau < 0.25$ days), removal results in approximately 95--100\% reduction of illegal content across all measures. 
%Facebook's compliance with the strict NetzDG takedown deadline is around the upper end of this window ($\tau = 0.27$, blue dashed line), resulting in maximum impact. 

Moderation becomes less effective as the time delay increases, allowing illegal content to spread before being taken down. 
Between approximately 9 hours--46 days ($0.25 \leq \tau \leq 32$), the reduction in prevalence of illegal content drops from 95\% to negligible levels. 
The half-life values measured from DSA-TDB data for TikTok and Facebook are within this interval. 
Those for Instagram, Snapchat, and YouTube correspond to even longer delays, where removal has no significant effect on illegal content prevalence. 
The half-lives obtained by fitting the tails of the platform delay distributions are even longer (see Section~\nameref{sec:calibration}). 
Although the overall trend is similar across the two exposure metrics, the reduction in impressions and reach drops even faster for delays of up to a few days. 
For example, when illegal content is removed within a day on average, reach and impressions are only reduced by about 60\%.
In summary, these findings show that delays beyond a day significantly diminish the effect of content removal. 

\subsection*{Robustness}
\label{sec:illegal_prob}

The results presented above may depend on the values of $s_H$ (proportion of high-risk accounts), $p$ (illegal content probability), and the underlying follower network. 
We therefore investigate if the results remain valid when changing these parameters. For these robustness experiments, we use two different values of illegal content half-life, $\tau=2$ and $\tau=8$, and assign users to the high- and low-risk groups randomly in each run. 

%First, the network consists of two user groups, where users in the high-risk group post illegal content with a higher probability than those in the low-risk group. We set the high-risk group size, $s_H=0.1$ relative to the network size. 
First, we run experiments to examine the effect of having smaller high-risk groups ($10^{-3} \le s_H \le 10^{-1}$), which may better reflect realistic scenarios. 
The main outcome variable, reduction in illegal content prevalence, shows no statistically significant differences (Fig.~\ref{fig:s_H}), suggesting that our findings are robust to changes in $s_H$. 

%Robustness check of s_L and s_H:
\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{plots/pct_change_s_H_combine.pdf}
\caption{Robustness of results with respect to different high-risk group sizes $s_H$  for $\tau=2$ (top) and $\tau=8$ (bottom) based on 20 simulations. The illegal content reduction does not vary significantly ($P \ge 0.72$ across pairs of experiments with different $s_H$ values, using Mann–Whitney U tests with Bonferroni correction).}
\label{fig:s_H}
\end{figure}

\begin{table}
\centering
\caption{SoRs about illegal content on Facebook, Instagram, YouTube, TikTok, and Snapchat derived from DSA-TDB records between January 1 and December 31, 2024. Note that it is easy for platforms to justify moderation of illegal content with violations of community standards, therefore illegal content may be underreported in the DSA-TDB.}
\begin{tabular}{lrrl}
\hline
 & SoRs& Illegal SoRs& Illegal Ratio \\
\hline
Facebook & 487,539,604 & 8,613 & 0.00002 \\
Instagram & 62,044,832 & 3,005 & 0.00005 \\
YouTube & 107,390,001 & 408,400 & 0.00380 \\
TikTok & 977,896,256 & 93,358 & 0.00001 \\
Snapchat & 4,068,586 & 2,142 & 0.00053 \\
\hline
\end{tabular}
\label{table:dsa_illegalprob}
\end{table}

% How much illegal content?
Second, we wish to explore how the main result is affected by the volume of illegal content, which varies across platforms and countries.
To estimate a plausible range for $p$, let us analyze two sources of empirical data. 
%DSA-TDB
%SORs about illegal content are identified using the strictest filter, where the platforms explicitly state that the moderation is due to illegal content.\footnote{DECISION\_GROUND\_ILLEGAL\_CONTENT in \textit{decision\_ground}} 
Table~\ref{table:dsa_illegalprob} shows that the fraction of illegal SoRs based on the DSA-TDB ranged from 0.00001--0.00380.
We can also obtain insights from the Future of Free Speech policy report~\cite{futurefreespeech2024}. The authors analyzed comment data collected from Facebook and YouTube across Germany, France, and Sweden during two weeks in June--July, 2023. 
They report the fraction of comments that disappear ($\frac{\text{disappear}}{\text{total}}$) and the proportion of disappeared comments that are legal ($\frac{\text{legal}}{\text{disappear}}$). From this, we estimate the fraction of comments that are illegal: $\frac{\text{illegal}}{\text{total}} = \frac{\text{disappear}}{\text{total}} \times \frac{\text{illegal}}{\text{disappear}} = \frac{\text{disappear}}{\text{total}} \times (1 - \frac{\text{legal}}{\text{disappear}})$. 
Table~\ref{table:futurefreespeech_illegalprob} shows that this apprach yields a range between 0.00002--0.009. 
% Note that the report tracks all comments that disappear, regardless of the cause, i.e., whether deletion was done by the platform, admins, or by the post authors. This method potentially overestimates the rate of deletion (aka in this set exists legal content).

\begin{table}
\centering
\caption{Data from the Future of Free Speech policy report and derived ratios of illegal content.}
\begin{tabular}{lccl}
\hline
 & Disappeared/Total & Legal/Disappeared & Illegal Ratio \\
\hline
Germany (FB) &  0.006 & 0.997 & 0.00002 \\
Germany (YT) &  0.115 & 0.989 & 0.001 \\
France (FB) &  0.012 & 0.921 & 0.0009 \\
France (YT) &  0.072 & 0.875 & 0.009 \\
Sweden (FB) &  0.005 & 0.946 & 0.0003 \\
Sweden (YT) &  0.041 & 0.946 & 0.002 \\
\hline
\end{tabular}
\label{table:futurefreespeech_illegalprob}
\end{table}

Based on these estimates, we run experiments with $10^{-4} \le p \le 10^{-2}$ (Values $p<10^{-4}$ would require a network with infeasibly large size, as discussed in \nameref{sec:methods}.) 
In our main analysis we report the reduction in illegal content prevalence, $1 - \frac{\bar{I}_{\text{removal}}}{\langle \bar{I}_{\text{baseline}} \rangle}$. To show that this ratio is independent of $p$, we plot the scaling relationship between $\bar{I}_{\text{removal}}$ and $\bar{I}_{\text{baseline}}$ in Fig.~\ref{fig:illegal_prob}a for different $p$ values from Table~\ref{table:futurefreespeech_illegalprob}. We observe only a weak sublinear relationship, $\bar{I}_{\text{removal}} \sim \bar{I}_{\text{baseline}}^{0.94}$, suggesting that the ratio is nearly independent of $p$. 
We also plot in Fig.~\ref{fig:illegal_prob}b the reduction in the prevalence of illegal content for different values of $p$ from Table~\ref{table:futurefreespeech_illegalprob}. None of the differences between the values in our main results ($p=0.01$) and those for smaller $p$ values are statistically significant. 
These results suggest that our main results are robust with respect to the volume of illegal content in the system. 

\begin{figure}
\centering
\includegraphics[width=\linewidth]{plots/illegal_prob.pdf}
\caption{Robustness analysis for illegal content probability. 
(a)~Scaling between illegal content prevalence in removal and baseline conditions. Each data point represents the proportion of illegal content in the system with and without removal. 
Results are based on 40 simulations for each value of $p$ from Table~\ref{table:futurefreespeech_illegalprob}, using $\tau=2$. 
The gray line represents a least-squares fit of the log-transformed data, yielding a slope of $0.94 \pm 0.01$. 
The vertical lines mark the illegal content probabilities from Table~\ref{table:futurefreespeech_illegalprob}. 
(b)~Illegal content reduction for different illegal content probabilities corresponding to the same countries and platforms,   
% : Sweden (FB) $0.0003$ (orange), France (FB) $0.0009$ (blue), Germany (YT) $0.001$ (green), Sweden (YT) $0.002$ (purple), France (YT) $0.009$ (yellow), and main experiment $10^{-2}$ (pink).  
based on 10 simulations with $\tau=2$ (top) and $\tau=8$ (bottom). The result does not vary significantly ($P \ge 0.13$ across pairs of experiments with different $p$ values, using Mann–Whitney U tests with Bonferroni correction).}
\label{fig:illegal_prob}
\end{figure}

Last, we examine the effect of network structure by comparing experiments using the empirical network and a synthetic network (see \nameref{sec:methods}). There is no statistically significant difference in the reduction in illegal content prevalence between these two networks, indicating the robustness of our findings (Fig.~\ref{fig:syntheticnet}).

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{plots/pct_change_syntheticnet.pdf}
\caption{Robustness analysis with respect to network structures based on 10 simulations with $\tau=2$ (left) and $\tau=8$ (right). The illegal content reduction does not vary significantly ($P= 0.11$ and $P=0.22$, respectively, using Mann–Whitney U tests with Bonferroni correction).}
\label{fig:syntheticnet}
\end{figure}

\section*{Discussion}

This paper examines empirical data on illegal content takedown on multiple social media platforms. 
Our agent-based model calibrated on this data reveals that removal of illegal content within a one-day window significantly reduces its prevalence and exposure, whereas more prolonged delays allow illegal content to permeate user feeds, diminishing the impact of the moderation. Delays beyond 46 days have no impact on the spread of illegal content. 

These findings have practical implications for social media moderation and regulation. By adopting more proactive takedown policies, platforms may reduce moderation delay and limit the spread of illegal content. Strengthening the use of and cooperation with trusted flaggers, who identify and report content violating community guidelines or legal standards, can further enhance content moderation. With human flaggers being a limited resource \cite{fbdsa24oct}, efforts should be directed towards prioritizing the most time-sensitive cases.

For legislation, the importance of prompt and timely moderation appears to support the argument for implementing tight illegal content removal deadlines, as imposed under NetzDG in Germany and KoPl-G in Austria prior to the DSA. 
However, the practical impact of such deadlines should be carefully assessed in regulatory debates for two reasons. 
First, regulatory takedown deadlines can only address the timespan between the notification of illegal content and its removal, not the period during which illegal content circulates in the network undetected. As a result, the impact of tight deadlines on the spread of illegal content is inherently limited. In fact, most platforms already report fast reactions after notifications: a median time below 24 hours according to 2024 DSA Transparency Reports and removal of 80\% of illegal content within 24 hours 
according to 2022 national Transparency Reports for Germany and Austria.

Second, tight takedown deadlines may reduce the time necessary for properly assessing the (un)lawfulness of content, increasing the risk of false moderation decisions and lowering the quality of content moderation. Flagged content requires careful verification, including report reviews and appeal resolutions, before removal decisions can be made. Any regulatory framework must ensure effective moderation of illegal content while remaining practical for platforms, and mindful of potential adverse effects. For example, in France, the Loi Avia was struck down by the Constitutional Council \cite{conseil_constitutionnel_2020} due to concerns about restrictions on freedom of expression \cite{heldt_2020, vie_publique_2020}. 
These considerations suggest that platforms should remove immediately only obviously illegal content. For cases requiring further assessment, platforms should take time for thorough review. Until the assessment is completed, platforms may implement alternative interventions to reduce the circulation of potentially illegal content.  

Our model has several limitations. Certain aspects, such as the user activity levels, are calibrated on data from global Twitter users, while the DSA data focuses on European users. Unknown biases could stem from this combination of data sources. 
%
The model further presumes uniform spreading dynamics across various types of illegal content. In reality, the appeal of such content may vary widely; scams may attract a broader audience while human trafficking posts may be aimed at specific communities.
Due to the lack of data on such content, our model does not distinguish among classes of illegal content or between illegal and legal content. If illegal content were more appealing, our findings would underestimate the adverse effects of time delay on moderation. 
%
For modeling content moderation, we make the simplifying assumption that when illegal content is removed, it is deleted from all user feeds. In practice, however, some content can be illegal only in certain jurisdictions and access can be selectively disabled for users in these countries. Our model does not consider such a differentiation.

The DSA-TDB has its own limitations. The database depends heavily on platform voluntary reporting, leading to inconsistencies across platforms. This reflects a fundamental weakness in self-reporting systems, as moderation decisions are often shaped by specific platform terms of service and community guidelines.

Future research could extend our model by accounting for the fact that users often alter their posting patterns in response to moderation decisions \cite{srinivasan2019content, jhaver2019does}.  
The model could also compare the effects of takedown versus alternative types of interventions, as well as explore combinations of different intervention instruments. 
Finally, future models should consider the trade-offs between the harm caused by persistent illegal content and the unitended effects of moderation tools, e.g., the limitation of free speech caused by false-positive errors.

\section*{Methods}
\label{sec:methods}

\subsection*{Model extensions}

% activity differential
We extend \simsom{} to model the heterogeneous activity of social media users, where most agents have low activity levels, and only a few are hyperactive. We model this by assigning an activity level $a$ to each user at the beginning of the simulation. We draw this activity from a power law $P(a) \sim a^{\gamma}$, where we obtain the exponent $\gamma = 2.85$ by fitting empirical measurements of average daily posts (Fig.~\ref{fig:activity_illegal}a).  
If $a<1$, the user (re)shares a single message with probability $a$ at each time step; else the user (re)shares $\lfloor a \rfloor$ messages. 

%\subsection*{Illegal content}

We wish to model the probability of posting illegal messages by low- and high-risk accounts. Given the lack of empirical data about this probability, we used the posting of content from low-credibility sources as a proxy. Leveraging news source credibility ratings from NewsGuard \cite{Newsguard_2025}, we categorized a sample of over 250 thousand Twitter users into a low-credibility and a high-credibility group by comparing the average ratings of sources they shared against two thresholds. 
%276,682 with at least 5 original posts with links to NG sources, of which 254,348in low/high groups
We then inspected the distributions for posting low-credibility content by these two groups and found that both could be modeled using the parametric family of beta distributions. 
Based on this proxy, we model the probability $i$ of posting illegal messages by users in each group (low- and high-risk) with a beta distribution $P(i) \sim i^{\alpha-1}(1-i)^{\beta-1}$, where $\alpha>0$ and $\beta>0$ are parameters that express the mean $\frac{\alpha}{\alpha + \beta}$ and skewness of the distribution for that group. 

% How much?
Given the two group distributions, we can express the average illegal content probability across the full population as $p = s_{H} \frac{\alpha_H}{\alpha_H + \beta_H} + (1-s_{H}) \frac{\alpha_L}{\alpha_L + \beta_L}$, where $s_{H}$ is the relative size of the high-risk groups, and the $\alpha$ and $\beta$ subscripts indicate the groups. 

Since platforms typically do not provide access to original content, there is insufficient scrutiny to determine the legality of content and limited data on the share of illegal content (for an exception see Wagner et al.~\cite{wagner2024mapping}). Hence, it is difficult to empirically calibrate the illegal content probability $p$ by estimating the percentage of illegal messages in the system. 
Therefore, we choose different values of $\alpha$ and $\beta$ to explore a broad range of values $10^{-4} \leq p \leq 10^{-2}$.
%(Table \ref{table:beta_distr}). 
% For the lowest value of illegal content probability, $p=10^{-4}$, $s_{H}=0.1$ is equivalent to one high-risk user in the network. 

% \begin{table}
% \centering
% \caption{Parameters for different average probabilities of posting illegal content.}
% \begin{tabular}{lcrrr}
% \hline
%  & \multicolumn{2}{c}{High-risk} & \multicolumn{2}{c}{Low-risk} \\
% %\cline{2-5}
% $p$ & $\alpha_H$ & $\beta_H$ & $\alpha_L$ & $\beta_L$ \\
% \hline
% 0.0001 & 3 & 5997 & 0.1 & 1800 \\
% %\hline
% 0.001 & 3 & 597 & 0.1 & 180 \\
% %\hline
% 0.01 & 3 & 30 & 0.1 & 90 \\
% \hline
% \end{tabular}
% \label{table:beta_distr}
% \end{table}

%\subsection*{Overall illegal content prevalence} 
\subsection*{Content removal}

%illegal content ratio/violation rate/illegal content circulation ratio
The \emph{illegal content prevalence} is defined as the average ratio of illegal content in circulation at time $t$: 
\(
I_t = \frac{1}{N}\sum_{i=1}^{N} f_{i,t}
\)
where $f_{i,t}$ is the fraction of illegal content in user $i$'s feed at time $t$ and $N$ is the number of agents.
% Convergence criteria depends on illegal content fraction 

The simulation ends once the system reaches a \emph{steady state}, signaled by a convergence in the illegal content prevalence. To this end, we calculate an exponential moving average at time $t$ as $\bar{I}_t = \rho \bar{I}_{t-1} + (1-\rho) I_{t}$, where $\rho$ is a parameter regulating the importance of older values.   
We stop the simulation when the difference between two consecutive values of this moving average is smaller than a threshold, $|\bar{I}_t - \bar{I}_{t-1}| / \bar{I}_{t-1} < \epsilon$. 
The parameters $\rho =0.9$ and $\epsilon = 0.0001$ used in the reported simulations were tested to ensure that the system's illegal content prevalence stabilizes at the steady state. 

For each set of parameters, we run 70 simulations without any content removal; these are averaged to obtain baseline metrics. 
We then run 70 simulations with content removal; the reported reduction is averaged across these runs. 
For example, the reduction in illegal content prevalence is calculated by $(\langle \bar{I}_{\text{baseline}} \rangle - \bar{I}_{\text{removal}}) / \langle \bar{I}_{\text{baseline}} \rangle$, where $\bar{I}$ is the illegal content prevalence at the end of a simulation and $\langle \bar{I}_{\text{baseline}} \rangle$ is the baseline average. 
All simulations start from random conditions. 

\subsection*{Data sources}
\label{sec:datasources}

%Although some sources volunteer such data, their suitability for calibration is constrained due to lack of comparability and reliability, unsuitable time-frames, a failure to clearly distinguish between illegal and other problematic content, and limit external access to verify data quality.
%
%Platforms voluntarily publish selected moderation data in their \textit{Community Standards Enforcement Reports}. For speed of moderation, they report varying platform-specific metrics such as “proactive removal rate”, “removal rate within 24h”, or “removal rate before any views,” which complicates cross-platform comparisons. Additionally, platforms limit external access to verify data quality.
%
%National regulations and takedown deadlines under NetzDG (Germany) and KoPl-G (Austria) introduced more standardized data collection. As a result, platforms reported fast removal (within 24 hours) for a large majority of illegal content. 
%In 2022, most illegal content in Germany was removed within 24 hours following notification, with reported rates of 97.5\% for Twitter, 92.9\% for Facebook, 88.3\% for YouTube, and 83.8\% for TikTok. 
%However, these data only cover the time between notification and removal of content (notification-to-moderation) and not the entire duration that problematic content remains online (publication-to-moderation). 
%Moreover, NetzDG and KoPl-G transparency reports only covered illegal content removals in reaction to notifications via specific complaint forms, leaving gaps in the overall moderation delay for illegal content.
%
%With the replacement of NetzDG and KoPl-G by the DSA, precise takedown deadlines at the national level in Europe no longer exist. However, platforms are still required to report the speed of their moderation measures.
%with data now available in DSA Transparency Reports and the DSA Transparency Database. 

Under the DSA (Art.~15), platforms must publish \textit{Transparency Reports} detailing ``the median time needed for taking the action.'' While platforms comply, reporting practices differ significantly, preventing 
%Meta, for instance, reports the ``median time needed to take action on reported content after receiving Article 16 notices.''  TikTok differentiates turnaround time for content assessed under their terms of service and legal standards but provides no overall median. X reports ``illegal content median handle time'' by European country and the violation categories but omits the aggregated median. LinkedIn reports the ``median time from user reports to decisions'' but excludes automated moderation decisions. Overall, the reported data indicate that platforms predominantly respond to notifications within 24 hours, but differences in reporting prevent 
thorough comparison among platforms. 
Moreover, median turnaround times in transparency reports only cover the time between notification and removal of content, not the time when problematic content circulates in the network before notification.
%It is therefore not suitable for calibration of moderation delay in terms of publishing-to-moderation. 
Finally, DSA transparency reports do not indicate moderation delays specific to illegal content, and platforms disallow external access to verify data quality. 

More detailed data, suited for our purpose, is available in the newly introduced \textit{DSA Transparency Database}, which compiles Statements of Reasons (SoRs) explaining single moderation decisions to affected users. These SoRs include the reasons for moderation, as well as information on dates of posting and moderation decisions. We therefore use this data source to estimate the empirical half-life ($\tau$) values. We collect SoRs from the DSA-TDB between January 01 and December 31, 2024. We capture relevant records using the strictest filter, where the platforms explicitly state that the moderation is due to illegal content.\footnote{We select SoRs that have DECISION\_GROUND\_ILLEGAL\_CONTENT in the \textit{decision\_ground} field. The database also includes the value DECISION\_GROUND\_INCOMPATIBLE\_CONTENT for the same field. This can cover content that is illegal, which would be indicated in an optional boolean field \textit{incompatible\_content\_illegal}. Since platforms do not use this reporting category consistently, we did not include it in our dataset for calibration.} 
We consider four major social media platforms with a sufficient number of relevant SoRs: Facebook, Instagram, YouTube, TikTok, and Snapchat. We exclude LinkedIn and Pinterest due to a lack of SoRs related to illegal content and Twitter/X due to suspicious patterns revealed in its DSA data, as also highlighted in prior work \cite{trujillo2023dsa}. 
This results in 8,613 SoRs submitted by Facebook, 3,005 by Instagram, 408,400 by YouTube, 93,358 for TikTok, and 2,142 by Snapchat. 
The time delay in illegal content removal is defined as the difference between the moderation date (\textit{application\_date} field) and the content posting date (\textit{content\_date} field). 
%The empirical half-life values are $\tau \approx 30$ days for Facebook, 77 days for Instagram and 32 days for YouTube. 
 
\subsection*{Follower network}

% All experiments run on the 10^-5 network -> change the plot to reflect this 
Simulations are conducted on a follower network derived from empirical Twitter data. The network connects users obtained from a 10\% random sample of public tweets~\cite{Nikolov2020dataset}. The dataset comprises users who posted links to news articles, with likely automated accounts excluded. The original follower network constructed from this dataset has $58{,}296$ nodes and $10{,}632{,}023$ edges. 
We reduce the size of this network to speed up the simulations in our main experiment (Section~\nameref{sec:main_results}). We employ $k$-core decomposition to identify $N = 10{,}006$ nodes that constitute the $k = 94$ core. Subsequently, a random selection of edges is removed to match the density of the original network. This procedure produces $E = 1{,}809{,}798$ edges, with each node having an average of roughly 180 friends or followers.

For the robustness analyses varying high-risk group size and illegal content probability (Section~\nameref{sec:illegal_prob}), we need a large follower network to run meaningful scenarios using low values of these parameters ($s_H=10^{-3}$ and $p=10^{-4}$). 
Therefore we use the original network, but remove a random set of edges to speed up the simulations. The average in/out-degree is thus reduced to 18 friends per node, resulting in a network with $N = 58{,}296$ nodes and $E=1{,}063{,}202$ edges. 

To test the robustness of our results with respect to different network structures, we create a third, synthetic network with features ubiquitous in real-world networks, namely, the presence of hubs and clustering (directed triads). To this end, we use a directed variant of the random-walk growth model~\cite{vazquez03Growing}. 
We configure the parameters of this model to generate a network comparable in size and average degree to the network used in the main experiment. In particular, we initialize the network with 181 fully connected nodes and assume new nodes have fixed out-degree $k_{out}=180$. New nodes are added by first randomly selecting one node to follow. For the remaining  $k_{out} - 1$ connections, each new node has an equal probability to follow a random friend of the initially chosen node or a randomly selected node in the network. This results in a network with $N = 10{,}006$ nodes and $E=1{,}810{,}905$ edges; each node has on average approximately 180 friends/followers. 

\section*{Author contributions}

% contributions should follow the CREDIT template https://www.elsevier.com/researcher/author/policies-and-guidelines/credit-author-statement
BT, SK, FM, FS, NJ, SG, and LL conceptualized the study. 
FM, NJ, SG, and LL acquired funding. 
BT, SK, and FM developed the methodology. 
FS and NJ analyzed the regulatory context and data sources. 
BT, SK, ES, EV, and GN performed formal analysis. 
BT and FM developed the software.  
FM provided computing resources. 
BT conducted the investigation. 
BT and SK visualized the data. 
BT, SK, FM, FS and NJ drafted the manuscript. 
All authors reviewed and edited the draft. 

\section*{Data availability}
Code and data to implement the model and reproduce results are available at \url{github.com/osome-iu/simsom_removal}. 

\section*{Acknowledgements}

We are grateful to Alessandro Flammini, Taekho You, Samuel Groesch, and Azza Bouleimen for helpful discussions; and to Nick Liu for the data collection used to estimate user activities. This work was supported in part by the Swiss National Science Foundation (Sinergia grant CRSII5\_209250) and by the Knight Foundation.

\bibliographystyle{unsrt}
\bibliography{simsom}
\end{document}
