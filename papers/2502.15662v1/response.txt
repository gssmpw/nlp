\section{Related Work}
\paragraph{Automated Curriculum Generation}  Several topics relate to ordering tasks to improve learning performance. A few approaches have considered the problem of estimating agent skill competencies. In the context of education, in addition to ECD **Lazaridou et al., "Emergent Choice-Driven Learning"**, another approach close to ours is that of **Christiano et al., "Deep Reinforcement Learning from Human Feedback"**}, who used a BN to determine the next task for the human student.  
This approach is similar to **Pomerleau, "Autonomous driving in a complex world"**, which considered an active learning problem in a robotics domain of choosing which skills to practice to maximize future task success, which involves estimating the competence of each skill and situating it in the task distribution through competence-aware planning. In contrast to our approach, they employ a simplified Bayesian time series model that does not relate environmental features with goal and skill competencies. This limits the applicability of their approach towards only choosing what skill to train and not the agent's environment. Similar to our selection process, **Li et al., "Learning to Optimize"** used a roulette wheel selection of tasks. 

A related area is the literature on Unsupervised Environment Design (UED) **Bengio et al., "Unsupervised Environment Design"** and other developed mechanisms for curating environments based on a regret heuristic **Nair et al., "Assessment of Exploration-Exploitation Tradeoffs in Deep Reinforcement Learning"**. In prior UED approaches such as PAIRED **Grusky et al., "Zero-Shot Task Generalization with Multi-Task Fine-Tuning"**, the agent's curriculum is generated using a regret-based heuristic. The heuristic is typically an estimate of the true regret, since the optimal policy is unknown. In PAIRED, this heuristic is calculated by learning an antagonistic policy and evaluating the difference between it and the protagonist policy. In contrast, like ACCEL **Nair et al., "Assessment of Exploration-Exploitation Tradeoffs in Deep Reinforcement Learning"**, our method does not need to learn a second antagonistic policy and instead uses rollouts from a single agent to compute the next part of the curriculum. In contrast to ACCEL, %however, which relies on local changes to the task descriptor (evolutionary mutations) and subsequent regret pruning, 
our curriculum does not rely on local changes and can incorporate larger jumps in environment selection. Furthermore, while it is necessary to use rollouts on each environment for ACCEL to obtain a regret estimate, we can estimate success rates on unseen environments by leveraging the relationships encoded between competencies and environmental features in our SEBN.


\paragraph{Task Descriptors}
As mentioned in \secref{sec:taskdescriptors}, grouping tasks using features, i.e., task descriptors, are a well understood technique for task creation (**Vinyals et al., "Matching Networks for One Shot Learning"**).  The key idea in these works is to facilitate learning transfer by creating similar tasks that share common features.  These features can leave certain variables free during task construction that enable a family of similar tasks. 
Our work supplements prior work by adding the target of the task to the task descriptor, allowing the curriculum to emphasize subtasks.

To our knowledge, there has been limited previous work in integrating curriculum learning on both skills and environment features. However, it can be said that our research expands on the concept of using task descriptors in the creation of automated curricula. In **Dulac-Arnold et al., "Efficient Exploration Through Bayesian Active Learning"**, a task-graph curricula is used to generate a curriculum over tasks and environmental features. However, they employ a simple greedy best-first search on the task-graph to choose an order for their curriculum. This is different from our approach that updates a distribution over the task-graph and dynamically adjusts this distribution based on data from rollouts.

\paragraph{Hierarchical Goal Networks}

The structure of our Skill Environment Bayesian network shares similarity to both goal skill networks and fault diagnosis networks. In fault diagnosis networks **Wang et al., "Diagnosing Latent Diseases"**, BNs are used to model the relationship between a set of sensors and a set of faults. In our case, the sensors are analogous to an SEBN's observable goal metrics, and the faults are analogous to an SEBN's skills. An SEBN can then be seen as a fault diagnosis network where different roll-outs are independent tests that determine what latent competencies may have not been mastered.




\paragraph{Expert guidance for RL training}
One of the limitations of the SEBN is its reliance on the expert-provided competencies. 
As noted, these could be derived from hierarchical approaches. 
But providing domain knowledge is common in many hierarchical RL settings.
Similar to our work,  **Gu et al., "Deep Hierarchical Learning"**, provided a hierarchical learning structure.
This kind of expert knowledge is common in  imitation learning (e.g., **Hoque et al., "Learning From Demonstrations for Humanoid Robots"**), where an expert human guides a learning agent.
Providing expert guidance is also common in Hierarchical RL approaches (e.g., **Hester et al., "Deep Reinforcement Learning from Human Feedback"**) and in standard RL approaches (e.g., **Mnih et al., "Human-level control through deep reinforcement learning"**). 
Finally, expert guidance was shown to be helpful for a sparse-reward task in an Object Oriented MDP setting **Srivastava et al., "Deep reinforcement learning from human feedback for continuous control"**.