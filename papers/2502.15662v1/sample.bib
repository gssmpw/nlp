@article{dennis2020emergent,
  title={Emergent complexity and zero-shot transfer via unsupervised environment design},
  author={Dennis et al., Michael},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={13049--13061},
  year={2020}
}

@article{jiang2021replay,
  title={Replay-guided adversarial environment design},
  author={Jiang et al., Minqi},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1884--1897},
  year={2021}
}

@article{forestier2022intrinsically,
  title={Intrinsically motivated goal exploration processes with automatic curriculum learning},
  author={Forestier, S{\'e}bastien and Portelas, R{\'e}my and Mollard, Yoan and Oudeyer, Pierre-Yves},
  journal={JMLR},
  volume={23},
  number={1},
  pages={6818--6858},
  year={2022},
  publisher={JMLRORG}
}

@inproceedings{bagaria2021skill,
  title={Skill discovery for exploration and planning using deep skill graphs},
  author={Bagaria, Akhil and Senthil, Jason K and Konidaris, George},
  booktitle={Intl Conf. on ML},
  pages={521--531},
  year={2021},
  organization={PMLR}
}

@inproceedings{sharma2019dynamics,
  title={Dynamics-Aware Unsupervised Discovery of Skills},
  author={Sharma, Archit and Gu, Shixiang and Levine, Sergey and Kumar, Vikash and Hausman, Karol},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{parker2022evolving,
  title={Evolving curricula with regret-based environment design},
  author={Parker-Holder et al., Jack },
  booktitle={International Conference on Machine Learning},
  pages={17473--17498},
  year={2022},
  organization={PMLR}
}

@misc{towers_gymnasium_2023,
        title = {Gymnasium},
        url = {https://zenodo.org/record/8127025},
        abstract = {An API standard for single-agent reinforcement learning environments, with popular reference environments and related utilities (formerly Gym)},
        urldate = {2023-07-08},
        publisher = {Zenodo},
        author = {Towers et al., Mark },
        month = mar,
        year = {2023},
        doi = {10.5281/zenodo.8127026},
}

@inproceedings{andreas2017modular,
  title={Modular multitask reinforcement learning with policy sketches},
  author={Andreas, Jacob and Klein, Dan and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={166--175},
  year={2017},
  organization={PMLR}
}

@inproceedings{brohan2023can,
  title={Do as i can, not as i say: Grounding language in robotic affordances},
  author={Brohan et al., Anthony },
  booktitle={Conference on Robot Learning},
  pages={287--318},
  year={2023},
  organization={PMLR}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford et al., Alec },
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{ha2018world,
  title={Recurrent world models facilitate policy evolution},
  author={Ha, David and Schmidhuber, J{\"u}rgen},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{moon1996expectation,
  title={The expectation-maximization algorithm},
  author={Moon, Todd K},
  journal={IEEE Signal processing magazine},
  volume={13},
  number={6},
  pages={47--60},
  year={1996},
  publisher={IEEE}
}

@article{hutsebaut2022hierarchical,
  title={Hierarchical reinforcement learning: A survey and open research challenges},
  author={Hutsebaut-Buysse, Matthias and Mets, Kevin and Latr{\'e}, Steven},
  journal={ML and Knowledge Extraction},
  volume={4},
  number={1},
  pages={172--221},
  year={2022},
  publisher={MDPI}
}

@article{MinigridMiniworld23,
  author       = {Maxime Chevalier-Boisvert  et al.},
  title        = {Minigrid \& Miniworld: Modular \& Customizable Reinforcement Learning Environments for Goal-Oriented Tasks},
  journal      = {CoRR},
  volume       = {abs/2306.13831},
  year         = {2023},
}

@article{new2022lifelong,
  title={Lifelong learning metrics},
  author={New, Alexander and Baker, Megan and Nguyen, Eric and Vallabha, Gautam},
  journal={arXiv preprint arXiv:2201.08278},
  year={2022}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={41--48},
  year={2009}
}

@book{darwiche2009modeling,
  title={Modeling and reasoning with Bayesian networks},
  author={Darwiche, Adnan},
  year={2009},
  publisher={Cambridge university press}
}

@article{dechter2013reasoning,
  title={Reasoning with probabilistic and deterministic graphical models: Exact algorithms},
  author={Dechter, Rina},
  journal={Synthesis Lectures on Artificial Intelligence and Machine Learning},
  volume={7},
  number={3},
  pages={1--191},
  year={2013},
  publisher={Morgan \& Claypool Publishers}
}
@article{mislevy2003brief,
  title={A brief introduction to evidence-centered design},
  author={Mislevy, Robert J and Almond, Russell G and Lukas, Janice F},
  journal={ETS Research Report Series},
  volume={2003},
  number={1},
  pages={i--29},
  year={2003},
  publisher={Wiley Online Library}
}
@inproceedings{patra2023relating,
  title={Relating Goal and Environmental Complexity for Improved Task Transfer: Initial Results},
  author={Patra, Sunandita and Rademacher, Paul and Jacobson, Kristen and Hassold, Kyle and Kulaksizoglu, Onur and Hiatt, Laura and Roberts, Mark and Nau, Dana},
  booktitle={NeurIPS 2023 Workshop on Generalization in Planning},
  year={2023}
}

@article{kumar2024practice,
  title={Practice Makes Perfect: Planning to Learn Skill Parameter Policies},
  author={Kumar, Nishanth and Silver, Tom and McClinton, Willie and Zhao, Linfeng and Proulx, Stephen and Lozano-P{\'e}rez, Tom{\'a}s and Kaelbling, Leslie Pack and Barry, Jennifer},
  journal={arXiv preprint arXiv:2402.15025},
  year={2024}
}

@article{wei2020fork,
  title={FORK: A Forward-Looking Actor For Model-Free Reinforcement Learning},
  author={Wei, Honghao and Ying, Lei},
  booktitle={2021 IEEE 60th Annual Conference on Decision and Control (CDC)},
  year={2021}
}

@book{pearl,
  author = {J. Pearl},
  title = {{Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference}},
  publisher = {Morgan Kaufmann},
  year = {1988}
}

@article{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S},
  journal={A Bradford Book},
  year={2018}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{patra2022hierarchical,
  title={A hierarchical goal-biased curriculum for training reinforcement learning},
  author={Patra, Sunandita and Cavolowsky, Mark and Kulaksizoglu, Onur and Li, Ruoxi and Hiatt, Laura and Roberts, Mark and Nau, Dana},
  booktitle={The international FLAIRS conference proceedings},
  volume={35},
  year={2022}
}

@inproceedings{hsiao2024surrogate,
  title={Surrogate Bayesian Networks for Approximating Evolutionary Games},
  author={Hsiao, Vincent and Nau, Dana S and Pezeshki, Bobak and Dechter, Rina},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2566--2574},
  year={2024},
  organization={PMLR}
}

@inproceedings{liu2011bounding,
  title={Bounding the partition function using holder's inequality},
  author={Liu, Qiang and Ihler, Alexander T},
  booktitle={ICML},
  year={2011}
}

@article{cai2017bayesian,
  title={Bayesian networks in fault diagnosis},
  author={Cai, Baoping and Huang, Lei and Xie, Min},
  journal={IEEE Transactions on industrial informatics},
  volume={13},
  number={5},
  pages={2227--2240},
  year={2017},
  publisher={IEEE}
}

 @article{narvekarEtAl20.jmlr.clForRL, title={Curriculum learning for reinforcement learning domains: A framework and survey}, volume={21}, note={Citation Key: JMLR:v21:20-212}, number={181}, journal={Journal of Machine Learning Research}, author={Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E. and Stone, Peter}, year={2020}, pages={1–50} }


@inproceedings{tobin2017domain,
  title={Domain randomization for transferring deep neural networks from simulation to the real world},
  author={Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle={2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)},
  pages={23--30},
  year={2017},
  organization={IEEE}
}

@inproceedings{stout2010competence,
  title={Competence progress intrinsic motivation},
  author={Stout, Andrew and Barto, Andrew G},
  booktitle={2010 IEEE 9th International Conference on Development and Learning},
  pages={257--262},
  year={2010},
  organization={IEEE}
}

@article{konidaris2012transfer,
  title={Transfer in reinforcement learning via shared features},
  author={Konidaris, George and Scheidwasser, Ilya and Barto, Andrew G},
  year={2012},
  journal={Journal of Machine Learning Research}
}

@inproceedings{abelEtAl15.icaps.goalBasedActionPriors, 
  title={Goal-based action priors},
  author={Abel, David and Hershkowitz, David and Barth-Maron, Gabriel and Brawner, Stephen and O'Farrell, Kevin and MacGlashan, James and Tellex, Stefanie},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  volume={25},
  pages={306--314},
  year={2015}
}

@article{isele2016task,
  title={Using task features for zero-shot knowledge transfer in lifelong learning},
  author={Isele,David and Rostami,Mohammad and Eaton,Eric},
  journal={Proc. {IJCAI}},
  year={2016}
}

@inproceedings{sinapov2015learning,
  title={Learning inter-task transferability in the absence of target task samples},
  author={Sinapov, Jivko and Narvekar, Sanmit and Leonetti, Matteo and Stone, Peter},
  booktitle={Proc. {AAMAS}},
  pages={725--733},
  year={2015}
}

@article{rostamiEtAl20.jair.usingTaskDescriptions,
  author    = {Mohammad Rostami and
              David Isele and
              Eric Eaton},
  title     = {Using Task Descriptions in Lifelong Machine Learning for Improved Performance and Zero-Shot Transfer},
  journal   = {{JAIR}},
  volume    = {67},
  pages     = {673-703},
  year      = {2020}
}

@inproceedings{narvekar2016source,
  title={Source task creation for curriculum learning},
  author={Narvekar, Sanmit and Sinapov, Jivko and Leonetti, Matteo and Stone, Peter},
  booktitle={Proceedings of the 2016 international conference on autonomous agents \& multiagent systems},
  pages={566--574},
  year={2016}
}

@article{ruvolo2013,
annote = {ELLA},
author = {Ruvolo, Paul and Eaton, Eric},
journal = {Proc {ICML}},
pages = {507--515},
title = {{ELLA: An efficient lifelong learning algorithm}},
volume = {28},
year = {2013}
}

@article{suttonEtAl99.aij.betweenMDPsAndSemiMDPs,
title = "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
journal = "AIJ",
volume = "112",
number = "1",
pages = "181--211",
year = "1999",
//issn = "0004-3702",
//doi = "https://doi.org/10.1016/S0004-3702(99)00052-1",
//url = "http://www.sciencedirect.com/science/article/pii/S0004370299000521",
author = "Richard S. Sutton and Doina Precup and Satinder Singh",
keywords = "Temporal abstraction, Reinforcement learning, Markov decision processes, Options, Macros, Macroactions, Subgoals, Intra-option learning, Hierarchical planning, Semi-Markov decision processes",
}

@article{greenEtal2011.iaai.learningASkillTeachingCurriculum, 
    title={Learning a Skill-Teaching Curriculum with Dynamic Bayes Nets}, volume={25}, ISSN={2374-3468, 2159-5399}, DOI={10.1609/aaai.v25i2.18855}, abstractNote={We propose an intelligent tutoring system that constructs a curriculum of hints and problems in order to teach a student skills with a rich dependency structure. We provide a template for building a multi-layered Dynamic Bayes Net to model this problem and describe how to learn the parameters of the model from data. Planning with the DBN then produces a teaching policy for the given domain. We test this end-to-end curriculum design system in two human-subject studies in the areas of ﬁnite ﬁeld arithmetic and artiﬁcial language and show this method performs on par with hand-tuned expert policies.}, number={2}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Green, Derek and Walsh, Thomas and Cohen, Paul and Chang, Yu-Han}, year={2011}, month=aug, pages={1648–1654}, language={en} }

 @inproceedings{BalleraEtal2014.icetc.personalizingElearning, title={Personalizing E-learning curriculum using: reversed roulette wheel selection algorithm}, ISSN={2155-1812}, url={https://ieeexplore.ieee.org/abstract/document/6998908}, DOI={10.1109/ICETC.2014.6998908}, abstractNote={E-learning poses a challenge in a pedagogical perspective such as finding ways on how to motivate the students to learn in spite of the absence of a human instructor. Many researchers in the field have proposed and implemented various mechanisms to improve the learning process such as individualization and personalization. The main objectives is to maximize learning by dynamically selecting the closest teaching operation in order to achieve the learning goals. In this paper, a revolutionary technique has been proposed and implemented to perform individualization and personalization using reversed roulette wheel selection algorithm that runs at O(n). The technique is simpler to implement and is algorithmically less expensive compared to other revolutionary algorithms since it collects the dynamic real time performance such as examinations, reviews and study matrices. Results show that the implemented system is capable of recommending new learning sequences that lessens time of study based on their prior knowledge and real performance matrix.}, booktitle={2014 International Conference on Education Technologies and Computers (ICETC)}, author={Ballera, Melvin and Lukandu, Ismail Ateya and Radwan, Abdalla}, year={2014}, month=sep, pages={91–97} }


 @inproceedings{ahmadiTaylorStone07.aamas.IFSA, address={New York, NY, USA}, series={AAMAS ’07}, title={IFSA: incremental feature-set augmentation for reinforcement learning tasks}, ISBN={978-81-904262-7-5}, url={https://dl.acm.org/doi/10.1145/1329125.1329351}, DOI={10.1145/1329125.1329351}, abstractNote={Reinforcement learning is a popular and successful framework for many agent-related problems because only limited environmental feedback is necessary for learning. While many algorithms exist to learn effective policies in such problems, learning is often used to solve real world problems, which typically have large state spaces, and therefore suffer from the “curse of dimensionality.” One effective method for speeding-up reinforcement learning algorithms is to leverage expert knowledge. In this paper, we propose a method for dynamically augmenting the agent’s feature set in order to speed up value-function-based reinforcement learning. The domain expert divides the feature set into a series of subsets such that a novel problem concept can be learned from each successive subset. Domain knowledge is also used to order the feature subsets in order of their importance for learning. Our algorithm uses the ordered feature subsets to learn tasks significantly faster than if the entire feature set is used from the start. Incremental Feature-Set Augmentation (IFSA) is fully implemented and tested in three different domains: Gridworld, Blackjack and RoboCup Soccer Keepaway. All experiments show that IFSA can significantly speed up learning and motivates the applicability of this novel RL method.}, booktitle={Proceedings of the 6th international joint conference on Autonomous agents and multiagent systems}, publisher={Association for Computing Machinery}, author={Ahmadi, Mazda and Taylor, Matthew E. and Stone, Peter}, year={2007}, month=may, pages={1–8}, collection={AAMAS ’07} }

@article{kumar2012learning,
  title={Learning task grouping and overlap in multi-task learning},
  author={Kumar, Abhishek and Daume III, Hal},
  journal={arXiv preprint arXiv:1206.6417},
  year={2012}
}

@inproceedings{molina2020learn,
  title={Learn and link: Learning critical regions for efficient planning},
  author={Molina, Daniel and Kumar, Kislay and Srivastava, Siddharth},
  booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={10605--10611},
  year={2020},
  organization={IEEE}
}

@inproceedings{zhang19leveraging,
  title     = {Leveraging Human Guidance for Deep Reinforcement Learning Tasks},
  author    = {Zhang, Ruohan and Torabi, Faraz and Guan, Lin and Ballard, Dana H. and Stone, Peter},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {6339--6346},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/884},
  url       = {https://doi.org/10.24963/ijcai.2019/884},
}

@article{hussein2017imitation,
  title={Imitation learning: A survey of learning methods},
  author={Hussein, Ahmed and Gaber, Mohamed Medhat and Elyan, Eyad and Jayne, Chrisina},
  journal={ACM Computing Surveys (CSUR)},
  volume={50},
  number={2},
  pages={1--35},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@inproceedings{le2018hierarchical,
  title={Hierarchical imitation and reinforcement learning},
  author={Le, Hoang and Jiang, Nan and Agarwal, Alekh and Dud{\'\i}k, Miroslav and Yue, Yisong and Daum{\'e} III, Hal},
  booktitle={International conference on machine learning},
  pages={2917--2926},
  year={2018},
  organization={PMLR}
}