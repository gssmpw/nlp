@inproceedings{BalleraEtal2014.icetc.personalizingElearning, title={Personalizing E-learning curriculum using: reversed roulette wheel selection algorithm}, ISSN={2155-1812}, url={https://ieeexplore.ieee.org/abstract/document/6998908}, DOI={10.1109/ICETC.2014.6998908}, abstractNote={E-learning poses a challenge in a pedagogical perspective such as finding ways on how to motivate the students to learn in spite of the absence of a human instructor. Many researchers in the field have proposed and implemented various mechanisms to improve the learning process such as individualization and personalization. The main objectives is to maximize learning by dynamically selecting the closest teaching operation in order to achieve the learning goals. In this paper, a revolutionary technique has been proposed and implemented to perform individualization and personalization using reversed roulette wheel selection algorithm that runs at O(n). The technique is simpler to implement and is algorithmically less expensive compared to other revolutionary algorithms since it collects the dynamic real time performance such as examinations, reviews and study matrices. Results show that the implemented system is capable of recommending new learning sequences that lessens time of study based on their prior knowledge and real performance matrix.}, booktitle={2014 International Conference on Education Technologies and Computers (ICETC)}, author={Ballera, Melvin and Lukandu, Ismail Ateya and Radwan, Abdalla}, year={2014}, month=sep, pages={91–97} }

@inproceedings{abelEtAl15.icaps.goalBasedActionPriors, 
  title={Goal-based action priors},
  author={Abel, David and Hershkowitz, David and Barth-Maron, Gabriel and Brawner, Stephen and O'Farrell, Kevin and MacGlashan, James and Tellex, Stefanie},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  volume={25},
  pages={306--314},
  year={2015}
}

@inproceedings{ahmadiTaylorStone07.aamas.IFSA, address={New York, NY, USA}, series={AAMAS ’07}, title={IFSA: incremental feature-set augmentation for reinforcement learning tasks}, ISBN={978-81-904262-7-5}, url={https://dl.acm.org/doi/10.1145/1329125.1329351}, DOI={10.1145/1329125.1329351}, abstractNote={Reinforcement learning is a popular and successful framework for many agent-related problems because only limited environmental feedback is necessary for learning. While many algorithms exist to learn effective policies in such problems, learning is often used to solve real world problems, which typically have large state spaces, and therefore suffer from the “curse of dimensionality.” One effective method for speeding-up reinforcement learning algorithms is to leverage expert knowledge. In this paper, we propose a method for dynamically augmenting the agent’s feature set in order to speed up value-function-based reinforcement learning. The domain expert divides the feature set into a series of subsets such that a novel problem concept can be learned from each successive subset. Domain knowledge is also used to order the feature subsets in order of their importance for learning. Our algorithm uses the ordered feature subsets to learn tasks significantly faster than if the entire feature set is used from the start. Incremental Feature-Set Augmentation (IFSA) is fully implemented and tested in three different domains: Gridworld, Blackjack and RoboCup Soccer Keepaway. All experiments show that IFSA can significantly speed up learning and motivates the applicability of this novel RL method.}, booktitle={Proceedings of the 6th international joint conference on Autonomous agents and multiagent systems}, publisher={Association for Computing Machinery}, author={Ahmadi, Mazda and Taylor, Matthew E. and Stone, Peter}, year={2007}, month=may, pages={1–8}, collection={AAMAS ’07} }

@inproceedings{andreas2017modular,
  title={Modular multitask reinforcement learning with policy sketches},
  author={Andreas, Jacob and Klein, Dan and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={166--175},
  year={2017},
  organization={PMLR}
}

@article{cai2017bayesian,
  title={Bayesian networks in fault diagnosis},
  author={Cai, Baoping and Huang, Lei and Xie, Min},
  journal={IEEE Transactions on industrial informatics},
  volume={13},
  number={5},
  pages={2227--2240},
  year={2017},
  publisher={IEEE}
}

@article{dennis2020emergent,
  title={Emergent complexity and zero-shot transfer via unsupervised environment design},
  author={Dennis et al., Michael},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={13049--13061},
  year={2020}
}

@article{hussein2017imitation,
  title={Imitation learning: A survey of learning methods},
  author={Hussein, Ahmed and Gaber, Mohamed Medhat and Elyan, Eyad and Jayne, Chrisina},
  journal={ACM Computing Surveys (CSUR)},
  volume={50},
  number={2},
  pages={1--35},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@article{isele2016task,
  title={Using task features for zero-shot knowledge transfer in lifelong learning},
  author={Isele,David and Rostami,Mohammad and Eaton,Eric},
  journal={Proc. {IJCAI}},
  year={2016}
}

@article{jiang2021replay,
  title={Replay-guided adversarial environment design},
  author={Jiang et al., Minqi},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1884--1897},
  year={2021}
}

@article{konidaris2012transfer,
  title={Transfer in reinforcement learning via shared features},
  author={Konidaris, George and Scheidwasser, Ilya and Barto, Andrew G},
  year={2012},
  journal={Journal of Machine Learning Research}
}

@article{kumar2024practice,
  title={Practice Makes Perfect: Planning to Learn Skill Parameter Policies},
  author={Kumar, Nishanth and Silver, Tom and McClinton, Willie and Zhao, Linfeng and Proulx, Stephen and Lozano-P{\'e}rez, Tom{\'a}s and Kaelbling, Leslie Pack and Barry, Jennifer},
  journal={arXiv preprint arXiv:2402.15025},
  year={2024}
}

@inproceedings{le2018hierarchical,
  title={Hierarchical imitation and reinforcement learning},
  author={Le, Hoang and Jiang, Nan and Agarwal, Alekh and Dud{\'\i}k, Miroslav and Yue, Yisong and Daum{\'e} III, Hal},
  booktitle={International conference on machine learning},
  pages={2917--2926},
  year={2018},
  organization={PMLR}
}

@article{mislevy2003brief,
  title={A brief introduction to evidence-centered design},
  author={Mislevy, Robert J and Almond, Russell G and Lukas, Janice F},
  journal={ETS Research Report Series},
  volume={2003},
  number={1},
  pages={i--29},
  year={2003},
  publisher={Wiley Online Library}
}

@inproceedings{narvekar2016source,
  title={Source task creation for curriculum learning},
  author={Narvekar, Sanmit and Sinapov, Jivko and Leonetti, Matteo and Stone, Peter},
  booktitle={Proceedings of the 2016 international conference on autonomous agents \& multiagent systems},
  pages={566--574},
  year={2016}
}

@inproceedings{parker2022evolving,
  title={Evolving curricula with regret-based environment design},
  author={Parker-Holder et al., Jack },
  booktitle={International Conference on Machine Learning},
  pages={17473--17498},
  year={2022},
  organization={PMLR}
}

@inproceedings{patra2022hierarchical,
  title={A hierarchical goal-biased curriculum for training reinforcement learning},
  author={Patra, Sunandita and Cavolowsky, Mark and Kulaksizoglu, Onur and Li, Ruoxi and Hiatt, Laura and Roberts, Mark and Nau, Dana},
  booktitle={The international FLAIRS conference proceedings},
  volume={35},
  year={2022}
}

@inproceedings{patra2023relating,
  title={Relating Goal and Environmental Complexity for Improved Task Transfer: Initial Results},
  author={Patra, Sunandita and Rademacher, Paul and Jacobson, Kristen and Hassold, Kyle and Kulaksizoglu, Onur and Hiatt, Laura and Roberts, Mark and Nau, Dana},
  booktitle={NeurIPS 2023 Workshop on Generalization in Planning},
  year={2023}
}

@article{rostamiEtAl20.jair.usingTaskDescriptions,
  author    = {Mohammad Rostami and
              David Isele and
              Eric Eaton},
  title     = {Using Task Descriptions in Lifelong Machine Learning for Improved Performance and Zero-Shot Transfer},
  journal   = {{JAIR}},
  volume    = {67},
  pages     = {673-703},
  year      = {2020}
}

@inproceedings{zhang19leveraging,
  title     = {Leveraging Human Guidance for Deep Reinforcement Learning Tasks},
  author    = {Zhang, Ruohan and Torabi, Faraz and Guan, Lin and Ballard, Dana H. and Stone, Peter},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {6339--6346},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/884},
  url       = {https://doi.org/10.24963/ijcai.2019/884},
}

