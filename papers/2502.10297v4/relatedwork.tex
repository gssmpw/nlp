\section{Related Work}
\label{sec:related_work}
\vspace{-2mm}
Linear RNNs have recently been studied from two main perspectives: state-space models and causal linear attention. State-space models, originating from continuous dynamical systems, inspired variants such as S4 \citep{gu-iclr22a}, H4 \citep{fu-iclr23a}, and LRU \citep{orvieto-icml23a} (see \citet{tiezzi2024statespacemodelinglongsequence} for a comprehensive survey). Models like Mamba \citep{gu2023mamba, dao-icml24a} further enhance these by incorporating input-dependent gating mechanisms, significantly improving language modeling performance.
In parallel, \citet{katharopoulos-icml20a} showed that causal linear attention Transformers can be reformulated as RNNs with linear sequence-length scaling. Following this, Gated Linear Attention (GLA) \citep{yang-icml24a} introduced gating mechanisms similar to Mamba. Recent studies explored more expressive recurrences via non-diagonal transition matrices, such as DeltaNet \citep{schlag-icml21a,irie2023practical,yang-neurips24a}, TTT-Linear \citep{sun-arxiv24a}, RWKV-7 \citep{peng2025rwkv7gooseexpressivedynamic}, B'MOJO \citep{zancato-neurips24a}, and Titans \citep{behrouz2024titans}. Additionally, \citet{beck-neurips24a} introduced xLSTM, combining linear and nonlinear RNN architectures inspired by LSTM \citep{hochreiter1997long}.
Our work shares conceptual similarities with Adaptive Computation Time (ACT) \citep{graves2016adaptive}, as both approaches allow RNNs to dynamically determine the computational steps required for each input, resulting in enhanced flexibility and task performance. 
This adaptive approach has been further developed in works like the Universal Transformer~\citep{dehghani-iclr19a}, with recent work by~\citet{geiping2025scaling} demonstrating its effectiveness in modern reasoning tasks. Concurrently to our work, \citet{schone2025implicit} and \citet{movahedi2025fixedpoint} have explored how fixed point iterations can increase the expressivity of linear RNNs. Unlike our approach, which enhances the expressivity by increasing the complexity of the linear recurrence, their approach works by applying the same recurrence multiple times, effectively increasing the depth of the model without increasing the parameter count. The two approaches are orthogonal and could be combined.

Products of structured matrices~\citep{kissel2023structured} have previously been used as state-transition matrices in non-linear RNNs—including (Givens) rotation matrices \citep{Dorobantu2016DizzyRNNRR, jing-icml17a, dangovski2019rotational}, Kronecker products~\citep{jose-icml18a}, and Householder reflections \citep{mhammedi-icml17a}—chosen for their orthogonal, norm-preserving properties that encourage long-term dependency learning~\citep{hochreiter1991untersuchungen, bengio1994learning}. Recently, \citet{biegun2024rotrnn} applied rotation matrices as state-transition matrices in non-selective state-space models. In contrast, DeltaProduct is more flexible, since we use products of generalized householder matrices, which can interpolate between identity, projection, or reflection transformations on a per token basis.