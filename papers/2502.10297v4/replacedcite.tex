\section{Related Work}
\label{sec:related_work}
\vspace{-2mm}
Linear RNNs have recently been studied from two main perspectives: state-space models and causal linear attention. State-space models, originating from continuous dynamical systems, inspired variants such as S4 ____, H4 ____, and LRU ____ (see ____ for a comprehensive survey). Models like Mamba ____ further enhance these by incorporating input-dependent gating mechanisms, significantly improving language modeling performance.
In parallel, ____ showed that causal linear attention Transformers can be reformulated as RNNs with linear sequence-length scaling. Following this, Gated Linear Attention (GLA) ____ introduced gating mechanisms similar to Mamba. Recent studies explored more expressive recurrences via non-diagonal transition matrices, such as DeltaNet ____, TTT-Linear ____, RWKV-7 ____, B'MOJO ____, and Titans ____. Additionally, ____ introduced xLSTM, combining linear and nonlinear RNN architectures inspired by LSTM ____.
Our work shares conceptual similarities with Adaptive Computation Time (ACT) ____, as both approaches allow RNNs to dynamically determine the computational steps required for each input, resulting in enhanced flexibility and task performance. 
This adaptive approach has been further developed in works like the Universal Transformer____, with recent work by____ demonstrating its effectiveness in modern reasoning tasks. Concurrently to our work, ____ and ____ have explored how fixed point iterations can increase the expressivity of linear RNNs. Unlike our approach, which enhances the expressivity by increasing the complexity of the linear recurrence, their approach works by applying the same recurrence multiple times, effectively increasing the depth of the model without increasing the parameter count. The two approaches are orthogonal and could be combined.

Products of structured matrices____ have previously been used as state-transition matrices in non-linear RNNs—including (Givens) rotation matrices ____, Kronecker products____, and Householder reflections ____—chosen for their orthogonal, norm-preserving properties that encourage long-term dependency learning____. Recently, ____ applied rotation matrices as state-transition matrices in non-selective state-space models. In contrast, DeltaProduct is more flexible, since we use products of generalized householder matrices, which can interpolate between identity, projection, or reflection transformations on a per token basis.