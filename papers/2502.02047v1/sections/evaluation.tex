\section{Experiment Setup}
The fine-tuning process of XLM-R on the Amharic Question-Answering (QA) task involves a three-epoch training cycle using the "xlm-roberta-large-squad2" base language model. The maximum sequence length is set to 256. A learning rate of 1e-5, while using the Cosine Annealing learning rate scheduler. The fine-tuning leverages the AmaSQuAD synthetic dataset to enhance the model's performance on the Amharic QA task.

We trained the XLM-R model using the AmaSQuAD dataset. Refer to Section 5.1 for a more detailed analysis of the translated data sets. We first filtered the dataset. The selection process for training data involves evaluating the answers associated with Amharic SQuAD 2.0 questions. We consider the answer for inclusion based on a similarity criterion. Specifically, answers with a similarity probability of 0.6 or higher are included. This filtering ensures that the train and test datasets are of higher quality.

The filtered training dataset contains about 57,650 answerable questions, while the development data set contains about about 13,779. Moreover, AmaSquad contains over 50,000 unanswerable questions, which do not have any similarity score, we downsampled them to 6000 for the training set and 700 for the development set. In total, the training set contained 63,650 instances, while the development set contained 14,479 instances.
