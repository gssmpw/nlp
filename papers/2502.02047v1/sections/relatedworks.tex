\section{Related Work}

\cite{abedissa2023amqa} introduced the Amharic Question Answering dataset, known as AmQA - the first publicly accessible dataset \cite{abedissa2023amqa}. They gathered 2,628 sets of questions and answers by leveraging crowdsourcing, drawing from a pool of 378 Wikipedia articles. Their primary objective was to facilitate extractive QA, providing the precise location within the context where answers to questions could be found. In an effort to establish a baseline for the AmQA dataset, the researchers employed an XLM-R model, fine-tuned on SQuAD 2.0—a cross-lingual Roberta model pre-trained on an extensive 2.5TB dataset spanning 100 languages \cite{conneau2019unsupervised}. They assessed two variations of the XLM-R-based QA model: XLM-R base and XLM-R large. Notably, XLM-R large outperformed XLM-R small, achieving an EM score of 50.76 and an F1 score of 71.74. 

In addition to evaluating the QA model in isolation, the researchers integrated a retriever-reader component to rank and filter relevant context for answering questions. This combined approach, featuring both the QA and retriever models, yielded an EM score of 50.3 and an F1 score of 69.58. One limitation of their work was that they did not conduct fine-tuning on any model using the AmQA dataset; instead, they solely relied on pre-existing models for inference and evaluation. Additionally, unlike SQuAD 2.0 the AmQA dataset doesn’t include unanswerable questions. Furthermore, specific details regarding the size of the evaluation set were not disclosed.

Lewis et al. aimed to address the challenge of insufficient relevant data for benchmarking multilingual models, in order to assess the performance of QA systems across various languages and promote progress in multilingual QA \cite{lewis2019mlqa}. The study introduced MLQA, a dataset that comprises QA pairs in seven languages. MLQA includes, on average, parallel QA instances across four languages. The study designed two tasks to evaluate the performance of QA models with MLQA, namely Cross-Lingual Transfer (XLT) and Generalized Cross-Lingual Transfer (G-XLT). In the case of XLT, the QA model is trained using context, query, and answer pairs from one language and then tested on context, query, and answer pairs from another language. On the other hand, in the case of G-XLT, the model is similarly trained, but its performance is evaluated based on its ability to answer questions by extracting answers from a context in a different language. In G-XLT, it's important to note that the extracted answer is not in the same language as the query. The assumption in this evaluation is that the parallel dataset can enable comparison across different languages. The study evaluated state-of-the-art cross-lingual models and machine translation-based models on MLQA and set a baseline. 

To tackle the challenges of training a QA model with limited data, Carrino et al. devised the Translate Align Retrieve (TAR) method. This technique involves automatically translating the SQuAD 1.1 dataset into Spanish to generate synthetic corpora \cite{carrino2019automatic}. The researchers trained a Neural Machine Translation model from scratch to translate the context, query, and answers from the English SQuAD dataset into Spanish. They then utilized EFMARAL to calculate the alignment between the source translation and the context, facilitating answer retrieval from the translated text \cite{ostling2023efmaral}. The success of answer retrieval in the translated context is contingent on the alignment between the source and translated contexts. After implementing this approach to translate the SQuAD 1.1 dataset, the authors fine-tuned a pre-trained multilingual BERT (mBERT) model. The study evaluated the performance using two corpora, namely MLQA and XQuAD. The TAR-based training approach demonstrated superior F1 scores compared to XLM on the MLQA dataset and set a new state-of-the-art standard on the XQuAD dataset. This evaluation affirmed that TAR-based mBERT outperformed mBERT-based models validating the significance of the proposed approach.

\cite{abedissa2023amqa} developed a non-factoid, rule-based QA system in Amharic, tailored to address diverse information needs, including biography, definition, and description questions \cite{abedissa2019amharic}. This text-based QA strategy employs a hybrid approach for question classification, utilizing word overlap with the query for document filtering and rule-based scoring for answer extraction. Specifically, a summarization technique is applied for biography questions, and the resulting summary is validated using a text classifier. The performance of the answer extraction component yielded an F1 score of 0.592%. Notably, the dataset used for training and evaluating the system has not been publicly released. Moreover, the study did not conduct a comparative analysis of the rule-based QA extraction approach with other approaches.

\cite{abadani2021parsquad} presented ParSQuAD, a QA dataset derived from the SQuAD 2.0 dataset using machine translation \cite{abadani2021parsquad}. The study used Google Translate to translate the dataset into Persian. The authors faced two challenges after translating the dataset: finding the starting index of the translated answer in the translation context and finding translations that contain errors in either the context, query, or answer, which is more challenging. The authors created two datasets after the translation. One that has been manually edited and another generated automatically. In both cases, when locating the answer span in the translated context, the researchers used the one-to-one mapping between the source and the translated sentence to locate the answer in the translated sentence. Subsequently, the researchers fine-tuned three BERT-based pre-trained models: ParsBERT, ALBERT, and mBERT. The models were fine-tuned using default parameters from the Huggingface script and trained for two epochs. The study evaluates the three models on three variations of the ParSQuAD dataset: automatically generated, manually edited, and a variation of the manually edited dataset with extended unanswerable questions. The authors were able to set a baseline model, with the mBERT model outperforming the other two.

\cite{clark2019boolq} introduced BoolQ - a dataset containing passages, and questions with corresponding Yes or No answers \cite{clark2019boolq}]. Answering these questions necessitates an understanding of what an assertion in a passage entails, including what it excludes and makes improbable. Such questions are often posed when individuals are seeking more detailed information. Providing accurate responses to Boolean questions requires a wide range of inferential abilities.  In their research, various methods were explored to construct a polar question-answering model for the BoolQ dataset. A majority baseline, which consistently predicted the majority class, yielded an accuracy of 62\%. A recurrent model, without any pretraining, achieved an accuracy of 69.6\%. Subsequently, the researchers used transfer learning by incorporating datasets such as QNLI, SQuAD 2.0, NQ Long Answer, and MultiNLI. They applied fine-tuning and conducted experiments with pre-trained models, including the recurrent model, OpenAI GPT, and BERT. Among the various models, the unsupervised BERT model, with multi-step pretraining on Books and Wikipedia, followed by additional pretraining on MultiNLI data, and subsequent fine-tuning on the BoolQ dataset, demonstrated the highest performance, achieving an accuracy of 80.43\%. There remains a significant gap between human annotators, who have an accuracy of 90\%. 

The literature highlights a notable data scarcity in training models for low-resource languages like Amharic. To address this challenge, a viable approach involves leveraging neural machine translation as \cite{abadani2021parsquad} and Carrino et al. and implementing post-processing techniques \cite{carrino2019automatic}, \cite{abadani2021parsquad}. This enables the creation of data derived from resource-rich languages, such as English, which can then be used for training a question-answering model. This strategy leverages the abundance of resources available in a well-supported language such as English to enhance the development of question-answer models for languages with limited available data like Amharic.
