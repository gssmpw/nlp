\section{Conclusion}
In conclusion, we present a comprehensive framework for translating the SQuAD 2.0 dataset into Amharic, resulting in the creation of the AmaSQuAD dataset. The proposed methodology leverages Google Translate through the Deep Translator Python library for language translation, addressing the challenges of misalignment between translated questions and answers, as well as the presence of multiple instances of answers in the translated context.

To enhance the accuracy of the translation, we utilize techniques such as cosine similarity based on embeddings from a BERT-based model fine-tuned on Amharic and Longest Common Subsequence (LCS). The approach prioritizes answer spans near the original answer's location in the English context, mitigating potential discrepancies in locating translated answers in translated contexts.

The research fine-tunes the XLM-R model on the AmaSQuAD synthetic dataset for Amharic Question-Answering tasks. This study is the first to establish a baseline performance using the AmQA dataset, The fine-tuned model improved the baseline performance on both the AmaSQuAD development dataset from  36.55% to 44.41% and  50.01% to 57.55% on the f1-score and exact match score, respectively. On the human-curated AmQA dataset, it improved the performance from 67.80% to 68.80% and 52.50% to 52.66% on the f1-score and exact match score, respectively. This shows the effectiveness of leveraging synthetic data in enhancing the model's ability to answer questions that it has not encountered before while also gaining a 1% increase in the F1 score on the human-curated AmQA dataset.

Overall, the proposed translation framework, coupled with the fine-tuned XLM-R model, not only contributes to the availability of a valuable resource in the form of the AmaSQuAD dataset but also demonstrates the potential for advancing the capabilities of Amharic Question-Answering models using synthetic data

