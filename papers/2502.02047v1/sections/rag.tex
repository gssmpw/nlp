\section{Proposed Methodology}
\subsection{SQuAD Dataset Description}
Squad 2.0 is one of the widely used datasets in QA. Unlike its predecessor, SQuAD 1.1, SQuAD 2.0 introduces the concept of unanswerable questions, requiring models to not only understand the context but also identify when a question lacks a definite answer in a given passage [6]. The dataset has the following properties: id, context, question, and answers \cite{squad_v2_huggingface}. The "id" field serves as a unique identifier for the question. The "context" field contains a passage of text from Wikipedia from which questions serve as the context for deriving the question and answer. The "question" field is where the specific query related to the given context is presented. The "answers" field consists of information about the answer to the question, with "answer\_start" indicating the character position in the context where the answer begins and "text" containing the actual answer text.

\subsection{Proposed Translation Framework}
The proposed methodology for translating the SQuAD 2.0 dataset into AmSQuAd involves several key steps. For each entry in the SQuAD 2.0 dataset, we extract information such as the title, context, question, and answers. We then proceed to translate the title, context, and question into Amharic using Google Translate through the Deep Translator Python library. However, two main issues need to be addressed during this process.  

The first issue relates to the translated answers not always aligning perfectly with the content in the translated context. In other words, the translated answer may not be located in the translated context verbatim. This misalignment between the question and the answer in the translated context can pose a significant challenge.  The second challenge relates to the possibility of the translated context containing multiple instances of the answer. This can lead to ambiguity when determining the correct starting index for the answer within the context. To mitigate this issue, a systematic approach is required to identify the most relevant instance of the answer and accurately mark its starting position in the translated context.  

To address the first issue, we employ cosine similarity between the embeddings of the text span in the translated context and the embeddings of the translated answer to determine their similarity. The embeddings are derived from fine-tuning a BERT-based model (specifically, bert-base-multilingual-cased) using Amharic data \cite{davlan_bert_amharic}. But this alone is not sufficient in identifying if the span in the translated context is similar to the translated answer. To further measure the similarity between the two, we do so by first identifying the length of the Longest Common Subsequence (LCS) shared between the span and the answer sequences \cite{cormen2022introduction}. Subsequently, we calculate the similarity percentage by dividing this common subsequence length by the maximum length of the two input sequences. This percentage reflects how much the two sequences overlap or have in common, providing a measure of their similarity.  The similarity scores are used to select the answer span in the context.

To address the second challenge of dealing with translated contexts that may contain multiple instances of the answer, the approach aligns closely with the methodology employed in the research conducted by N. Abadani et. al. In their study, the authors devised a method for extracting the translated answer from the translated context \cite{abadani2021parsquad}. This involved a two-step process: first, they identified the sentence within the original context where the original answer was originally located. Next, they attempt to extract the starting index of the translated answer from the translated context.   

Similarly, in this research, when extracting the answer, we first locate the sentence in which the answer was originally found in the original context. We then select spans in the translated context that are in close proximity to the starting index of the answer in the original text. This is based on our intuition that the answer ordering in the translated Amharic context won't deviate significantly from the answer in the original English context. Therefore, translated answer spans that are in close proximity to the sentence where the answer was originally located in the English context are prioritized.  

As shown in Algorithm 1., the entire process followed in this research to extract the translated answer from the translated context applies a weighted combination of two operations: cosine similarity and the Longest Common Subsequence. It scans the entire context while prioritizing the translated answers that are in close proximity to the original answer. The weights assigned to the two operations for the most similar span are w1 and w2, respectively, with values of 2/3 and 1/3.



\begin{algorithm}[H]
\small
\caption{Proximity, Similarity, and LCS Amharic Answer Extractor}
\textbf{Input:} \( SQuAD\ 2.0 \) entries \( (to, co, qo, ao) \) \\ 
\( (title_{orig}, context_{orig}, query_{orig}, answer_{orig}) \)\\
\textbf{Output:} Translated entries \( (tt, ct, qt, at) \)\\
\( (title_{tr}, context_{tr}, answer_{tr}) \)

\begin{algorithmic}[1]
\State \textbf{Define embedding and LCS weights:} \( w_1 = \frac{2}{3}, w_2 = \frac{1}{3} \)

\For{\textbf{each} \( (to, co, qo, ao) \)}
    \State \textbf{Translate fields:}
    \State \( tt \gets tr(to), \quad ct \gets tr(co) \)
    \State \( qt \gets tr(qo), \quad at \gets tr(ao) \)

    \Statex \textbf{Initialize variables:}
    \State \( s_{max} \gets 0, \quad idx_b \gets -1 \)
    \State \( sent_b \gets "", \quad p_b \gets \infty \)

    \Statex \textbf{Split context and answer into words:}
    \State \( c_w \gets sp(ct), \quad a_w \gets sp(at) \)
    \State \( a_emb \gets emb(at) \)

    \Statex \textbf{Scan context for best match:}
    \For{\( i = 0 \) \textbf{to} \( \text{len}(c_w) - \text{len}(a_w) \)}
        \For{\( s = 0 \) \textbf{to} \( 3 \)} \Comment{Stride over context}
            \State \( w_txt \gets ext_w(c_w, i, \text{len}(a_w), s) \)
            \State \( w_emb \gets emb(w_txt) \)
            \State \( sim \gets w_1 \cdot sim(a_emb, w_emb) + w_2 \cdot lcs(w_txt, at) \)

            \If{\( sim > s_{max} \)}
                \If{\( prox(at, w_txt) < p_b \)}
                    \State \( s_{max} \gets sim, \quad idx_b \gets st(c_w, i) \)
                    \State \( sent_b \gets w_txt, \quad p_b \gets prox(at, w_txt) \)
                \EndIf
            \EndIf
        \EndFor
    \EndFor

    \Statex \textbf{Store best match:}
    \State \( at \gets sent_b, \quad ats \gets idx_b \)
    \State Save \( (tt, ct, qt, at, ats) \)
\EndFor
\end{algorithmic}
\end{algorithm}





\subsection{Extractive Question Answering Models}

There have been significant improvements in language models designed for low-resource languages. Models such as mBERT, XLM, and XLM-RoBERTa (XLM-R) have played a pivotal role in enhancing capabilities, not only for languages with abundant resources but also for those facing resource limitations  \cite{devlin-etal-2019-bert}, \cite{liu2019roberta}, \cite{pires-etal-2019-multilingual}.\cite{pires-etal-2019-multilingual}, illustrated that a BERT model can be effectively pre-trained on 104 languages using Wikipedia with good performance in both few-shot and zero-shot model evaluations \cite{liu2019roberta}. However, it's worth noting that the model was not pre-trained in the Amharic language.   

XLM-R, a transformer-based masked language model trained using 2.5 terabytes of CommonCrawl data, made significant improvements across various tasks\cite{conneau2019unsupervised}. With a substantial vocabulary size of approximately 250 thousand, XLM-R two model variants: XLM-R Base with 270 million parameters and XLM-R Large with 550 million parameters. Its performance on the MLQA dataset, evaluated across seven languages, deserves attention. It achieves an average F1 score and exact match of 70.7\% and 52.7\%, respectively, surpassing XLM-15, which scored 61.6\% on the F1 score and 43.5\% on the exact match. Additionally, XLM-R outperforms mBERT by a considerable margin of 13.0\% on the F1-score and 11.1\% on the exact match matrix. Furthermore, in the monolingual comparison, XLM-R has demonstrated superior performance compared to BERT Large trained on monolingual data. It shows the robust capacity of the model.  

It's important to note that XLM-R is also pre-trained on Amharic language. For this reason, we further leverage XLM-R Large by fine-tuning the model specifically for Amharic QA tasks. Leveraging XLM-R fine-tuned on SQuAD 2.0, we aim to improve the performance of the Amharic QA model.
