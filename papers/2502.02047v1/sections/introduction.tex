\section{Introduction}
The latest census results have shown that Amharic is spoken by approximately 57.5 million people in Ethiopia of which 25.1 million individuals within the country have adopted it as a second language \cite{hirpassa2023improving}. In addition, next to Arabic, Amharic is the second most widespread Semitic language \cite{hirpassa2023improving}. Despite being the second most widespread Semitic language, Amharic lacks substantial Natural Language Processing (NLP) resources and tools \cite{basha2023detection}. This shortage of resources has hindered the development of robust systems that could greatly benefit various natural language tasks.  

Question answering (QA) is a Natural Language Processing task that aims to accurately provide answers  to natural language questions \cite{zhu2021retrieving}. Unlike search engines, which present a list of relevant documents for a question, QA systems provide a definite answer, enhancing user experience. Search engines such as Google and Bing incorporate a QA system in their systems to respond precisely to questions \cite{zhu2021retrieving}.   
QA could be categorized into two tasks - Open-domain QA (OpenQA) and Machine Reading Comprehension (MRC) based on the contextual information given to the system. OpenQA is an approach that leverages large unstructured text data to provide answers to questions in natural language \cite{zhu2021retrieving}. The traditional question-answering pipeline in the OpenQA approach involves processes such as Document Retrieval and Answer Extraction with a component known as a Reader \cite{zhu2021retrieving}. During document retrieval, the goal is to identify relevant passages and documents that may contain answers to the question, employing techniques such as TF-IDF, BM25, or search engines. Following this, the answer is extracted from the identified and pertinent documents.  In the case of MRC, the aim is to read a document and extract the answer without having to do document retrieval. Similar to OpenQA, this is achieved by using the Reader component.  

The Reader component of a QA system can be either extractive or generative readers \cite{zhu2021retrieving}. Extractive readers extract answers by predicting a span from retrieved documents, while generative readers utilize sequence-to-sequence models to produce answers in natural language. Moreover, OpenQA systems have recently adopted modern architecture by incorporating neural Machine Reading Comprehension. A substantial amount of high-quality data has been generated to train OpenQA systems, including datasets like MS MARCO \cite{nguyen2016ms}, CNN/Daily Mail \cite{hermann2015teaching}, and SQuAD \cite{rajpurkar2018know}.

In contrast to the abundance of high-quality datasets available for widely spoken languages, the Amharic language faces a dataset scarcity. The largest Amharic dataset collected by \cite{abedissa2023amqa} includes only 2,628 sets of questions and answers . Consequently, to our knowledge, no expansive extractive QA dataset is currently available for the Amharic language. Without a sufficiently large and diverse dataset, it becomes challenging to train models capable of providing accurate responses, thus further impeding progress in the development of Amharic QA system development.

In response to the scarcity of datasets for training Amharic Question Answering Models, we build a translation-based data generation framework valuable for extractive QA. Recognizing the limited availability of comprehensive datasets in Amharic, the study employs Google Translate to transform the widely used SQuAD 2.0 training and development dataset into an Amharic dataset named AmaSQuAD. In addition to creating the AmaSQuAD dataset, we aim to leverage this dataset, which is specifically tailored for the extractive approach to train an MRC-based Amharic question-answering model. We aim to accomplish this by fine-tuning the XLM-R model. Furthermore, we anticipate that the developed framework can be used for other languages and other extractive QA datasets.

The rest of the research is structured into several sections. Section II provides a literature review of existing approaches for extractive QA, offering insights into previous research. Section III describes the methodology adopted in the research, detailing the SQuAD dataset description, the proposed translation framework, and the Extractive Question Answering Model. Following this, Section IV presents an analysis of the AmaSQuAD dataset and the XLM-R Fine-Tuning Process. Subsequently, Section 5 discusses the results of the comparative test using AmaSQuAD and AmQA. Finally, Section 6 offers a conclusion summarizing the key findings and suggesting directions for future research.
