\begin{table}[h]
\centering

\label{tab:few_shot_model_baseline}
\begin{tabular}{lccccccc}
\toprule
\multicolumn{2}{c}{} & \multicolumn{3}{c}{\textbf{Baseline}} & \multicolumn{3}{c}{\textbf{Few-Shot}}\\ 
    \cmidrule(lr){3-5} \cmidrule(lr){6-8}
\textbf{Model} & \textbf{Level} & \textbf{\fast{1}} & $\textbf{\fast{0}}$ & 
\textbf{Length (chars)} &
$\textbf{\fast{1}}$ & $\textbf{\fast{0}}$ &
\textbf{Length (chars)}\\
\midrule
             & 1 & 3\%  & 27\% & 301018 & 6\%  & 27\% & 360212 \\
Llama 3.1-70B  & 2 & 0\%  & 0\% & 646403 & 0\%  & 0\% & 566668  \\
             & 3 & 0\%  & 0\% & 404596  & 0\%  & 4\% & 485332  \\
\midrule
            & 1 & 10\% & 55\% & 343995 & 6\%  & 39\% & 437768\\
OpenAI o1  & 2 & 24\% & 56\% & 381474 & 16\% & 39\% & 432800 \\
            & 3 & 12\% & 56\% & 260273 & 8\%  & 22\% & 364551 \\
\bottomrule
\end{tabular}

\caption{Comparison of the Section \ref{4.1} baseline and few-shot prompting performance across models. We examine the $\textbf{\fast{0}}$, $\textbf{\fast{1}}$, and cumulative character length of generated kernels per level.}

\end{table}