\textbf{Kernel libraries and compilers.} We evaluate existing approaches for kernel programming along the dimensions of automation, breadth, and performance. Mainstream kernel programming libraries like cuDNN ~\cite{nvidiacudnn}, CUTLASS~\cite{nvidia2017cutlass}, and Apple MLX~\cite{applemlx} are hardware-specific and demand substantial engineering effort from human experts. Other libraries, like ThunderKittens~\cite{spector2023thunderkittens} and  Triton~\cite{triton}, successfully help AI researchers write a breadth of fast and correct kernels~\cite{arora2024simple, yang2024fla}, but still require human programming effort. Compiler-based tools, like torch.compile~\cite{paszke2019pytorchimperativestylehighperformance} and FlexAttention~\cite{he2024flexattention}, automatically provide a narrow slice of optimizations. In contrast to these efforts, we ask if LMs can automatically generate performant kernels for a breadth of AI workloads. 
\\\\
\noindent \textbf{LLMs for performance-optimized code generation.} In the past year, there have been several efforts to build LMs that can automate algorithmic coding~\cite{chen2021evaluatinglargelanguagemodels, shi2024languagemodelssolveolympiad, Li_2022}, resolving GitHub issues~\cite{yang2024swe,yang2024swebenchmultimodalaisystems}, and domain-specific coding~\cite{yin2022naturallanguagecodegeneration,lai2022ds1000naturalreliablebenchmark}. While these works focus on producing correct and functional code, subsequent works have explored LMs' ability to produce solutions with better \textit{algorithmic and asymptotic efficiency} ~\cite{nichols2024performancealignedllmsgeneratingfast,waghjale-etal-2024-ecco}. KernelBench focuses on \textit{wall-clock efficiency}. LMs generate high-performance computing (HPC) code, which requires an understanding of the underlying hardware features and device instruction set, and common performance characteristics of parallel processors. 

Existing works in the space of HPC code generation have evaluated LM performance on translating arbitrary code samples from C++ to CUDA~\cite{tehranijamsaz2024coderosetta,pmlr-v162-wen22b} or generating well-known, low-level kernels such as GEMMs~\cite{valerolara2023comparingllama2gpt3llms, wijk2024rebenchevaluatingfrontierai}. KernelBench instead curates a set of 250 diverse kernels from real-world, modern deep learning workloads, many of which do not have existing human-written implementations â€” in other words, solving KernelBench tasks are immediately beneficial for real deep learning workloads. 