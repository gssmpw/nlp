\abstract{
The recent \dpsk-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). 
While \dpsk-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems,
this paper introduces \tech, the first approach to scale RL-based LLM reasoning for real-world software engineering.
Leveraging a lightweight rule-based reward (e.g., the similarity score between ground-truth and LLM-generated solutions), \tech enables LLMs to autonomously recover a developer's reasoning processes and solutions by learning from extensive open-source software evolution data---the record of a software's entire lifecycle, including its code snapshots, code changes, and events such as issues and pull requests.
Trained on top of \llama~3, our resulting reasoning model, \ours[70], achieves a \textbf{\swebfinalbig{\%}} solve rate on \swebverified---a human-verified collection of real-world GitHub issues.
To our knowledge, this is the best performance reported for medium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs like \gpt{-4o}.
Surprisingly, despite performing RL solely on software evolution data, \ours has even emerged with generalized reasoning skills.
For example, it shows improved results on five out-of-domain tasks, namely, function coding, library use, code reasoning, mathematics, and general language understanding, whereas a supervised-finetuning baseline even leads to performance degradation on average.
Overall, \tech opens up a new direction to improve the reasoning capabilities of LLMs through reinforcement learning on massive software engineering data.}




