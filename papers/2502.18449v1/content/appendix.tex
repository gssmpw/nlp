\section{\ouragentless}
\label{sec:apd:agentlessmini}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{plots/DevLlama-agentless.pdf}
\caption{\textbf{The \ouragentless scaffold.} The design emphasizes easy decomposition, parallelization, and scalability.}
\label{fig:agentless-mini}
\end{figure}

In addition to a model proficient in code editing, effectively tackling software engineering tasks, such as those found in \swebench{}~\cite{swebench}, also requires a robust scaffold.
\agentless~\cite{agentless} is one of the state-of-the-art scaffolds for \swebench{} at the time of writing.
Building upon \agentless with various simplifications and enhancements, we developed \ouragentless, a framework that prioritizes straightforward component decomposition, parallelization, and scalability.
With \ouragentless, each step's inference or execution compute can be independently scaled to enhance \swebench performance.
In \Cref{fig:agentless-mini}, we present a detailed illustration of \ouragentless's working principles. The following paragraphs will elaborate on each step and highlight the differences from \agentless.

\textbf{Localization and repair.}
For localization, we employ a prompting-based approach that enables the model to predict relevant file paths based on a given issue and the repository's structure.
Unlike \agentless, which involves two additional detailed steps to identify related elements and files, as well as a separate embedding model, \ouragentless simplifies the process. It generates multiple samples of potentially problematic files from the model and consolidates them into unique sets for repair.

During the repair phase, the LLM is conditioned on the full content of the files to predict search/replace edits. We generate multiple repair samples from different location sets, ensuring a comprehensive exploration of the patch search space.


\textbf{Reproduction tests generation and selection.}
\agentless samples reproduction tests for patch selection. Initially, multiple reproduction tests are generated based on an issue description, and one majority sample is selected after filtering. These tests must have distinct logic to output \texttt{"Issue reproduced"} and \texttt{"Issue resolved"} when the issue is reproduced or resolved, respectively. They are filtered based on whether they correctly output \texttt{"Issue reproduced"} when executed in the original codebase.
\ouragentless enhances this pipeline with two key improvements. First, instead of relying solely on the issue description, the model retrieves a relevant test file to guide test generation.
Additionally, rather than selecting just one majority sample, \ouragentless allows for the selection of multiple top test samples based on voting results. In our evaluation, using more test samples has proven beneficial for reranking (\Cref{subsec:scaling}).

\textbf{Regression tests selection.}
We select regression tests in the same manner as \agentless. Initially, we gather all passing tests for each issue by executing the code before any modifications. This step does not require model inference and needs to be performed only once.
Subsequently, an additional, optional inference step is conducted to filter out passing tests that are supposed to fail after the issue is fixed.
The rest of the tests will be marked as regression tests.

\textbf{Reranking.}
\ouragentless utilizes both regression and reproduction tests for reranking.
For each issue, every applied patch is executed against the regression tests. The patches that result in the minimum number of existing test failures are selected.
For each issue, we choose the top-$N$ reproduction tests and run each patch against these tests. If the execution outputs \texttt{"Issue resolved"}, we mark the patch passing this test.
We then adopt the dual execution agreement objective from \codet~\cite{codet}.
Specifically, patches $\mathcal P$ that pass the same set of reproduction tests $\mathcal T$ are denoted as a consensus group. Each consensus group is scored using the formula 
$|\mathcal P|\times|\mathcal T|^2$.
With this objective, consensus groups with patches passing more tests receive higher scores. Additionally, groups where more patches pass the tests are scored higher, although passing more tests is prioritized over having more patches.
Finally, we identify the consensus group with the highest score and select the best patch from this group using majority voting.


\section{Synthesizing supervised-finetuning data}
\label{sec:apd:posttraining}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{plots/DevLlama-posttraining.pdf}
\caption{\textbf{Synthetic data pipeline for constructing SFT data.}
We start by collecting high-quality seed PRs using heuristics, then generate synthetic localization and code-editing samples, and finally use the ground-truth edited files and patches to filter out incorrect samples.}
\label{fig:posttraining}
\end{figure}

\Cref{fig:posttraining} shows our method of generating synthetic supervised-finetuning (SFT) data.
The data generation pipeline is inspired by \magicoder~\cite{magicoder}, where the \ossinstruct technique generates high-quality code instruction data from open-source seed snippets.
We apply a similar methodology to generate both fault localization and code editing data using high-quality PR seeds.

\textbf{Collecting high-quality seed PRs.}
To begin with, we extract high-quality PR seeds from the raw dataset we collected, as detailed in \Cref{subsec:raw-data}.
These seeds are chosen based on specific heuristics. For example, a PR instance should include at least one linked issue, the issue should describe a bug fix request, and the code changes should involve programming files.

\textbf{Localization and editing data synthesis.}
We adopt \llama-3.3-70B-Instruct~\cite{llama31} for data synthesis.
For localization data, we prompt the model with the issue description, repository structure, and the paths of edited and relevant files as hints. We then ask the model to identify the relevant files for modification or review by generating a thought process, followed by a prioritized list of file paths.
During filtering, we ensure that the model's response includes all files that are genuinely edited in the PR, and these files should be prioritized in the ranking.

Similarly, in terms of code editing, we synthesize code edits for a given issue, in search/replace format~\cite{agentless}, by providing the ground-truth PR and patch as guidance to \llama-3.3-70B-Instruct.
During the filtering process, we ensure that all search/replace blocks adhere to the correct format and that all search paths and blocks can be accurately matched within the input files' context.

\textbf{SFT baseline.}
As discussed in \Cref{subsec:setup}, we meticulously constructed \oursft[70] as a strong SFT baseline.
This SFT baseline is trained on a 16k context window for 2B tokens,
where the training data consists of a mix of the aforementioned synthetic localization and editing data, as well as coding and general SFT datasets from \llama~3~\cite{llama31}. 

\section{Midtraining on large-scale PR data}
\label{sec:apd:largetrain}
In addition to our main technique, \tech,
we are exploring orthogonal ways to improve language models (LLMs) on real-world software engineering.
In this section, we explain how we can directly utilize the raw PR collection through continued pretraining (midtraining) to empower small LLMs, such as \llama-3.1-8B, to achieve a high resolve rate (\textbf{\swebfinalsmall{}\%}) on \swebverified~\cite{swebverified}.

\textbf{Data packing design.}
In pretraining, documents are often concatenated into a long sequence and split by the context window, avoiding padding and boosting efficiency. However, \citet{fewertruncation} highlights that truncations can cause issues like hallucinations because a single document can be split into different training sequences.
Since midtraining involves much fewer tokens than pretraining, we choose to enhance the representation accuracy and sacrafice some training efficiency.
As shown in \Cref{fig:packing}, we truncate PR documents that exceed the context window. For shorter documents, we pack several together in a sequence, using padding tokens as needed. Attention masks are applied to block attention between documents in the same sequence.
The same packing approach is used in posttraining with SFT.
Approximately, this packing strategy leads to a 75\% non-padding rate for each batch.


\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\linewidth]{plots/DevLlama-packing.pdf}
\caption{\textbf{Data packing design for midtraining.}
Documents longer than the context window are truncated at the end, while shorter documents are packed into a single sequence along with padding.}
\label{fig:packing}
\end{figure}

\textbf{Formatting PR data as dialogs.}
We format each mid-training PR instance as a dialogue between the user and the assistant, with the user's content (e.g., code context) being masked. This approach is beneficial for the following reasons:
(1) loss masking prevents the repetition of code context across different PRs of the same repository,
(2) masking helps ensure that the model focuses on predicting code edits rather than overly concentrating on generating code context, which usually occupies most of the context window,
and (3) maintaining a consistent dialogue format with posttraining can facilitate smoother model learning and allows us to reuse the training infrastructure effectively.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{plots/DevLlama-dialog-formating.pdf}
    \caption{\textbf{Formatting midtraining PR data as dialogs.}
    \textsf{Code context} includes contents of all changed files and some unchanged but relevant files, \textsf{PR} denotes a PR title and description, \textsf{Commit} indicates consecutive commits with messages and code changes, and \textsf{Comment} indicates consecutive \github user conversations or review comments. \textsf{Commit}s and \textsf{Comment}s are ordered chronologically.}

    \label{fig:dialog-formating}
\end{figure}

\Cref{fig:dialog-formating} illustrates the details of how we format each PR instance.
The code context is always included in the first user message, which includes content of all changed files and some unchanged but relevant files.
We further examine two scenarios based on whether the PR references any issues.
In cases where issues are present, we include both the code context and the issue details in the initial user dialog. The PR description and the first series of consecutive commits are then provided in the assistant's initial response.
This approach encourages the model to learn the entire issue resolution process by predicting the PR and its associated commits.
Conversely, when no issues are mentioned, we present the PR in the user's initial input, allowing the model to learn how to implement code changes based on the PR description.
The remaining dialog turns are divided based on the timing of the subsequent commits. Commits serve as turning points, represented by assistant responses, while other comments are represented as user inputs.
The unified diff of each commit are randomly converted into LLM-friendly editing format such as search/replace~\cite{agentless}.

\textbf{Dynamic context adjustment.}
Although including code context in the dialog is essential for the model to learn how to process contextual information, the context can often be excessively long and exceed the context window,
meaning that the entire sequence would not contribute to any actual loss during training.
From our observation, a large number of data points would be invalid without additional processing.
To address this issue, we implement \emph{dynamic context adjustment}.
The goal of this strategy is to shorten the code context while preserving the most relevant information, particularly the edited parts. We achieve this by randomly replacing segments of code with an ellipsis (``\texttt{...}'') to reduce the overall length of the context.
We apply removal of code segments uniformly across all files to prevent bias.
The amount of removal is determined by a parameter indicating the expected context size.
In our implementation, we estimate the token count using string length to efficiently apply the pipeline without requiring explicit tokenization.
Overall, this strategy allows us to keep most of the PR documents.

\textbf{Stable training and annealing.}
We divide the midtraining process into two stages: a stable training stage and an annealing stage, as outlined in \minicpm~\cite{minicpm} and \llama~3~\cite{llama31}.
We utilize a trapezoidal learning rate scheduler, also known as Warmup-Stable-Decay~\cite{minicpm}. In this approach, the learning rate remains constant during the stable training stage after an initial linear warmup, followed by a linear decay during the annealing stage.
During the stable training stage, we exclusively use midtraining data. In the annealing stage, we incorporate additional datasets, including the synthetic SFT data (\Cref{sec:apd:posttraining}) for localization and editing, as well as some coding data and general-purpose dialogs from \llama~3.

\textbf{\oursmid[8].}
\oursmid[8] is midtrained from the base \llama-3.1-8B~\cite{llama31} for 500B tokens using a 32k context window. We perform annealing for the last 10\% of the midtraining steps, where we mix a small fraction of our posttraining SFT data (\Cref{sec:apd:posttraining}), along with coding and general dialog data from \llama~3~\cite{llama31}.
It is then SFT-ed for 8B tokens using a 64k context window.
The resulting model achieves a \textbf{\swebfinalsmall{}\%} resolve rate on \swebverified, significantly outperforming previous small LLM baselines and comparable to many results achieved with larger models.
However, we have observed some limitations, such as a reduction in general capabilities due to this large-scale midtraining.
Looking ahead, we aim to overcome this challenge by developing a more refined data mix to enhance the model's overall performance.

\section{Complete prompt}
\label{sec:apd:fullprompt}

The following is a complete version of \Cref{fig:prompt}:

{
\renewcommand*\ttdefault{cmtt}
\begin{lstlisting}[style=codeblock]
PROMPT_TEMPLATE = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

A user will ask you to solve a task. You should first draft your thinking process (inner monologue). Then, generate the solution.

Your response format must follow the template below:
<think>
Your thoughts or/and draft, like working through an exercise on scratch paper. Be as casual and as long as you want until you are confident to generate a correct solution.
</think>
<solution>
Final solution presented to the user.
</solution><|eot_id|><|start_header_id|>user<|end_header_id|>

We are currently solving the following issue within our repository. Here is the issue text:
--- BEGIN ISSUE ---
{problem_statement}
--- END ISSUE ---

Below are some code segments, each from a relevant file. One or more of these files may contain bugs.

--- BEGIN FILE ---
```
{content}
```
--- END FILE ---

Please first localize the bug based on the issue statement, and then generate *SEARCH/REPLACE* edits to fix the issue.

Every *SEARCH/REPLACE* edit must use this format:
1. The file path
2. The start of search block: <<<<<<< SEARCH
3. A contiguous chunk of lines to search for in the existing source code
4. The dividing line: =======
5. The lines to replace into the source code
6. The end of the replace block: >>>>>>> REPLACE

Here is an example:

```python
### mathweb/flask/app.py
<<<<<<< SEARCH
from flask import Flask
=======
import math
from flask import Flask
>>>>>>> REPLACE
```

Please note that the *SEARCH/REPLACE* edit REQUIRES PROPER INDENTATION. If you would like to add the line '        print(x)', you must fully write that out, with all those spaces before the code!
Wrap each *SEARCH/REPLACE* edit in a code block as shown in the example above. If you have multiple *SEARCH/REPLACE* edits, use a separate code block for each one.<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
\end{lstlisting}
}
