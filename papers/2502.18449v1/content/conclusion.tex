\section{Conclusion}
\label{section:conclusion}


We introduce \tech, the first reinforcement learning (RL) approach to improve language models (LLMs) on software engineering tasks using software evolution data (e.g., PRs) and rule-based rewards.
The resulting model, \ours[70], achieves a \textbf{\swebfinalbig{}\%} solve rate on \swebverified, a human-verified collection of high-quality \github issues.
This performance is state-of-the-art among medium-sized models and comparable to many proprietary LLMs such as \gpt{-4o}.
While \tech is specifically applied to the issue-solving task, \ours has developed generalized reasoning skills through RL, demonstrating improved performance on out-of-domain tasks such as code reasoning, mathematics, and general language understanding.
Overall, \tech opens a new direction for enhancing the software engineering capabilities of LLMs through RL.


\textbf{Limitations.}
Despite the promising results, our approach has several limitations.
First, our reward implementation compares the sequence similarity between the predicted and oracle patch rather than their semantic equivalence. This may prevent the policy LLM from exploring alternative, functional equivalent solutions.
Additionally, in \ouragentless, the localization process is simplified to mapping repository structures to file paths, which lacks comprehensive context.
Moreover, as a pipeline-based approach, \ouragentless divides all steps into distinct inference stages. This ``external structure'' prevents the model from learning through interaction feedback and hinders its ability to consider the entire problem holistically.
Furthermore, our approach requires a substantial sampling budget to achieve optimal results, which may be impractical for projects with high execution costs.


\textbf{Future work.}
In the future, we plan to address these limitations by integrating agentic reinforcement learning into our framework. This will enable the model to independently learn localization and repair strategies without relying on external structures.
Additionally, we will incorporate execution to allow the model to interact directly with the repository environment.
We will also focus on improving the sample efficiency during inference, with the goal of achieving equal or better performance with fewer samples.
Ultimately, we aim to enhance the practicality of \tech, paving the way for more powerful, reliable, and fully open-source solutions for active \github issue resolution.



\section*{Acknowledgement}

We thank Chris Waterson, Chris Cummins, and Volker Seeker for their assistance in debugging and testing \ouragentless;
Kunhao Zheng, David Zhang, and Taco Cohen for their assistance in the online RL infra;
Pierre Chambon, Mihir Sanjay Kale, Parth Thakkar, Michael Jiang, Sinong Wang, and Jingyue Shen for their involvement in the PR dataset discussion;
Jannik Kossen for his helpful discussions in model training;
Megan Miller, Hannah Schubloom, and Madeline Hepler for their support in the review process;
Steven Xia and Yinlin Deng for their aid in setting up \agentless;
Jiawei Liu, Yifeng Ding, Terry Yue Zhuo, and Naman Jain for their valuable discussions;
Albert Ã–rwall for their help in resolving issues with Moatless EvalTools.
