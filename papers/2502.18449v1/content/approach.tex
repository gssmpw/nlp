\section{\tech{}}
\label{section:approach}


\subsection{Raw pull request data curation}
\label{subsec:raw-data}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{plots/DevLlama-data.pdf}
    \caption{\textbf{Overview of \tech's raw pull request data curation process.}
The collected git clones and \github events are transformed into self-contained PR instances via decontamination, aggregation, relevant files prediction, and filtering.}
    \label{fig:data}
\end{figure}

\Cref{fig:data} provides a high-level overview of our process for curating the raw PR dataset for \ours.
In the following paragraphs, we detail each step of the curation process.
During data processing, we exclude all the repositories used by \swebench{}~\cite{swebench} to prevent data contamination.


\textbf{GitHub events and clones.}
The goal of this stage is to recover all pull request details that human developers can inspect on \github. To achieve this, we need two sources of information: (1) all events that occur within a PR and (2) the source code of a repo before the changes introduced by the PR are merged.
We derive all \github~\cite{github} events from \gharchive~\cite{gharchive}, which contains all activity events data from \github. Our collection includes all \github events from Jan 1, 2015 to Aug 31, 2024.

To obtain source code, since pull requests often occur at different commit stages of a repository, we opt to use \texttt{git clone} to retrieve the entire repository with its commit history, rather than relying on the \github API~\cite{ghapi} to download specific code snapshots.
Eventually, we successfully cloned and processed 4.6M repositories.

\textbf{PR data aggregation.}
These collected events and git clones are disparate entities that require further processing before they can be used for training purposes.
At this stage, we focus on each PR individually and aggregate all pertinent information associated with it. This includes mentioned issues, user discussions, review comments, initial code contents, and subsequent commits and code changes.

To start, we keep only merged PRs and gather all related conversational events for each PR, sorting them in chronological order. Next, using the \texttt{base\_commit} and \texttt{head\_commit} hashes of a PR, we retrieve the contents of all modified files indicated by its patch at the merge base of the two commits.
The reasoning behind this approach is that many PRs aim to merge back into the main branch, which may have undergone changes since the PR was created. By considering the merge base as the actual starting point for a developer working on the PR, we can more accurately understand the context of the changes.
We save all intermediate commits and code changes made between the merge base and the head commit. Additionally, we extract the complete patch that represents the cumulative changes from start to finish.
Finally, we scan each aggregated PR to identify patterns that resemble issues, and associate the matched issues with the corresponding PR. In the end, we have 24M aggregated PR instances.

\textbf{Relevant files prediction.}
Currently, each pull request includes only the code files that have been modified. In our earlier experiments, we noticed that this approach let LLMs learn a bias: the model consistently generated edits for every code file presented and was unable to handle noisy files presented in the context. This issue was also mentioned in the finetuning experiment discussed in the \swebench{} paper~\cite{swebench}.
To mitigate this problem, we prompt \llama-3.1-70B-Instruct~\cite{llama31} to generate a list of relevant but unmodified files for each PR, given the PR description and the paths of the changed files, and include the contents of these files in our final dataset.

\textbf{Data filtering.}
\github PRs can be quite noisy, so we implemented various filtering strategies to eliminate potentially harmful PRs. In designing these filtering rules, our goal is to maximize the recall of high-quality PRs while permitting a certain level of noise.
First, we remove the bot-generated PRs whose title, description, or username contains keywords ``[bot]'', ``dependabot'', ``renovate'', ``bump'', or ``automerge''.
Also, we remove PRs with empty changes or with extremely large number of changes (e.g., in some PRs, the developer uploaded a directory of data files by mistake).
Additionally, we implemented a more fine-grained set of filters used in \codellama~\cite{codellama} to examine each code change hunk. We then removed any PRs where all code changes were flagged by these filters. For example, this can exclude PRs with only lock file changes or version updates.
Finally, this gives us around 11M unique PR instances.


\subsection{Reward modeling}
\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{plots/DevLlama-prompt.pdf}
\caption{\textbf{Prompt template used to train \ours with \tech.}
Given an issue description and the corresponding code context, the policy LLM needs to generate search/replace edits~\cite{agentless} to fix this issue through reasoning.
This is the only subtask we incorporate in the RL training.
During inference, the LLM can generalize to tasks outside the training domain (e.g, file-level localization and test generation).
For conciseness, we exclude certain prompt details, with the complete prompt template available in \Cref{sec:apd:fullprompt}.}
\label{fig:prompt}
\end{figure}

To prepare the initial dataset for RL, we extract high-quality PR seeds from the raw dataset we collected.
These seeds are selected based on specific heuristics. For example, a PR instance should include at least one linked issue, the issue should describe a bug-fixing request, and the code changes should involve programming files.
For each seed, we extract the issue descriptions and code context, including all changed files and some relevant but unchanged files.
These are converted to input prompts for the policy LLM.
We also take the oracle patch from each seed, which will be used for reward calculation.

{
\newcommand\seed{\mathcal D_\mathsf{seed}}
\newcommand\formprompt{\mathsf{form\mbox{-}prompt}}
\newcommand\question{q}
\newcommand\issue{\mathsf{issue}}
\newcommand\context{\mathsf{ctx}}
\newcommand\predpatch{\mathsf{patch_{pred}}}
\newcommand\oracle{\mathsf{patch_{gt}}}
\newcommand\prob[1]{\mathbf P(#1)}
\newcommand\oldpolicy{\mathit{\pi_{\theta_{\mathrm{old}}}}}
\newcommand\policy{\mathit{\pi_{\theta}}}
\newcommand\refpolicy{\mathit{\pi_{\mathrm{ref}}}}
\newcommand\ratio{\frac {\policy(o_i \mid \question)} {\oldpolicy(o_i \mid \question)}}
\newcommand\clip{\mathrm{clip}}
\newcommand\kldiv{D_{\mathrm{KL}}}
We bootstrap the policy LLM with the prompt template shown in \Cref{fig:prompt}.
Assuming that the LLM has generated a rollout $\tau$ given an issue and its code context,
the reward function is defined as follows:
\begin{equation}
\mathcal R(\tau) = \begin{cases}
    -1, &\text{if the format is wrong},\\
    \mathit{compare}(\predpatch, \oracle), & \text{otherwise}.
\end{cases}
\label{eq:reward}
\end{equation}
Here, $\predpatch$ denotes the patch corresponding to the code changes generated by the policy LLM if the format of $\tau$ is correct, and $\oracle$ means the oracle patch for this issue.

In the implementation, we instantiate the $\mathit{compare}$ function with Python's \texttt{difflib.SequenceMatcher}, which returns a floating point between 0 and 1 indicating the sequence similarity between two sequences, and use \grpofull (\grpo)~\cite{deepseekmath} for policy optimization.

Given a seed RL dataset $\seed{}$ where each item contains
(1) $\issue{}$, the issue description,
(2) $\context$, the code context required to solve this issue, including both files to repair and relevant files not intended for editing,
and (3) $\oracle{}$, which is the oracle patch.
The input prompt for each data item is formed by instantiating the prompt template in \Cref{fig:prompt} with the issue description and code context, which we denote as $\question = \formprompt(\issue, \context)$.
The policy LLM $\policy$ tries to solve the issue by generating code changes through reasoning, and produces multiple outputs $o_i$ for each input prompt $\question$ given the group size $G$.
Then, the policy LLM aims to maximize the following \grpo objective:
\begin{equation}
\label{eq:grpo}
\begin{gathered}
\mathcal J(\theta) =
    {\mathbb E}\left[
         \frac1G\sum_{i=1}^G\left(
             \min\left(\ratio A_i, \clip\left(\ratio, 1-\epsilon, 1 + \epsilon\right)A_i\right)
             - \beta\kldiv(\policy\,\|\,\refpolicy)
         \right)
    \right],\\
    \text{where }(\issue, \context, \oracle)\sim\seed\text{, }
    q = \formprompt(\issue, \context)
    \text{, and }\{o_i\}_{i=1}^G \sim \oldpolicy(\cdot \mid \question).
\end{gathered}
\end{equation}
In this equation, $\epsilon$ and $\beta$ are hyperparameters, and $\oldpolicy$ and $\refpolicy$ are the old and reference policy, respectively.
Following \grpo, the advantages $A_i$ are calculated using the normalized rewards within each group. The term $\kldiv$ denotes the approximated KL-divergence~\cite{klapprox}.
}

Our training approach conditions the model on the complete context of each file, implicitly forcing it to identify detailed fault locations before generating repair edits. This process inherently teaches the model both bug diagnosis and repair generation.
However, during downstream evaluation on \swebench, our \ouragentless scaffold requires capabilities beyond generating repair edits, such as file-level fault localization, reproduction test generation, and regression test selection.
Remarkably, even without explicit training on these subtasks, \ours can generalize to them through the RL process.

\subsection{Aha moments and generalized reasoning capabilities}
\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{plots/DevLlama-aha.pdf}
\caption{\textbf{Reasoning skills emerged from \ours[70] following the application of \tech{}.}
RL helps the model develop reasoning skills like self-reflection, exploring alternatives, and divide-and-conquer strategies, for both in-domain (e.g., issue-solving) and out-of-domain tasks (e.g., function implementation and mathematics).
}
\label{fig:case}
\end{figure}

\textbf{``Aha moments'' on software engineering.}
With the application of \tech, we observe ``aha moments''~\cite{deepseekr1} where \ours exhibits emergent reasoning skills.
To our knowledge, this is the first study demonstrating the existence of such aha moments in the realm of real-world software engineering tasks, confirming the findings of \dpsk-R1~\cite{deepseekr1}, which mainly focuses on competition coding and math.
As shown in \Cref{fig:case}, through RL, \ours[70] can allocate more thinking time to reflect on its initial assumptions during the issue-solving process.
This behavior emerges naturally from the model's interaction with RL, rather than being explicitly programmed.

\textbf{General reasoning capabilities.}
Surprisingly, we have identified additional aha moments where \ours acquires general reasoning abilities that are transferrable to various out-of-domain tasks, such as function-level code generation and mathematics, although RL is applied exclusively to software issue solving.
\Cref{fig:case} demonstrates that \ours is capable of reasoning through self-reflection, exploring alternative approaches, and solving complex problems by breaking them down into smaller subtasks.
In \Cref{subsec:generalizability}, we demonstrate that \ours improves over even more out-of-domain tasks, including library use, code reasoning, and general language understanding.
