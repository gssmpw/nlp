\section{Related Work}
\label{rel}


By contrast to ``neuro-symbolic'' AI ____, where the neural architecture is closely intermixed with symbolic steps, in our approach the neural processing is encapsulated in the LLMs and accessed via a declarative, high-level API. This reduces the semantic gap between the neural and symbolic sides as their communication happens at a much higher, fully automated and directly explainable level.


Our recursive descent algorithm shares the goal of extracting more accurate information from the LLM interaction with work on ``Chain of Thought'' prompting of LLMs ____ and with step by step ____ refinement of the dialog threads. 
% However our process aims to fully automate the dialog thread while also ensuring validation of the results with help of ground-truth watching oracles and independent LLM-based agents. 
Our approach shares with tools like LangChain ____ the idea of piping together multiple instances of LLMs, computational units, prompt templates and custom agents, except that we fully automate the process without the need to manually stitch together the components.

We have not found any references to the use of Dual Horn clauses in logic programming but it is a well known result ____) that their complexity in the propositional case is polynomial, similarly to their of Horn clause counterparts. This fact makes them also good generation targets for LLM-extracted knowledge processing. 

We have not found anything similar to generating question-answer-follow-up question chains, although it is common practice for chatbots to  suggest (a choice between) follow-up questions\footnote{ including the author's own \url{https://auto-quest.streamlit.app/}}.

Our torch-based model-computation algorithm follows closely the matrix-computation logic of ____, our contribution being its succinct and efficient GPU-friendly implementation.

Interest in several forms of soft-unification has been active  ____ as differentiable substitute of symbolic unification in neuro-symbolic systems.
By contrast, our focus in this paper is flexible information retrieval of LLM-generated natural language content, for which high quality embeddings were available either from LLM APIs or local resources like the torch-based sentence-transformers ____.