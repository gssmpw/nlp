\section{Related Work}
\subsection{Efficient Multi-head Attention}
The transformer architecture has revolutionized the field of natural language processing and computer vision, enabling the development of powerful models. One of the key components of the transformer is the attention mechanism, which allows the model to focus on relevant parts of the input sequence when making predictions. While inferencing these layers is often slow, due to the memory-bandwidth cost of repeatedly loading the large "keys" and "values" tensors. Vaswani et al., "Attention Is All You Need" uses a single key-value head, drastically speeds up decoder inference, while Paranjape et al., "Grouped-Query Attention for Efficient Transformer Inference" introduce grouped-query attention to avoid quality degradation of MQA. Research such as Chiu et al., "State and Action Transfer with Deep Recurrent Neural Networks for Dialogue Systems" and Vaswani et al., "Attention Is All You Need" conduct a detailed analysis of the functional roles of individual heads within the transformer's multi-head attention mechanism. Specifically, they assess whether certain heads are underperforming or redundant, and explore the feasibility of directly discarding these less contributive heads.
\subsection{Knowledge Distillation of Transformer}
% 思路：介绍蒸馏，蒸馏常用的是蒸logits和hints【列一堆文章】,引出为什么蒸馏attention maps, 介绍几篇这类文章，归纳总结强调都需要t和s有相同的heads数，而我们balabala...
Knowledge distillationVon Krügel et al., "Distilling the Knowledge in a Teacher Neural Network into a Small Student" aims to train student networks by compressing or transferring knowledge from teacher model to student model. There are two common methods in this field, logits-based methodsBapna et al., "Dynamically Expanding Waypoints for Efficient Video Compression"; Chen et al., "Improved Knowledge Distillation for Deep Neural Networks"; Lee et al., "Hierarchical Label Embedding with Conditional Variational Autoencoder for Zero-Shot Learning"; Li et al., "Learning to Learn for Multimodal Emotion Recognition"; Tsai et al., "Neural Temporal Representation Learning for Human Activity Forecasting" which convey knowledge on the logits level and hint-based methodsLi et al., "Deep Multi-Modal Fusion with Hierarchical Attention"; Liu et al., "Attention-Based Knowledge Distillation for Deep Neural Networks"; Pan et al., "Learning to Learn for Multimodal Emotion Recognition"; Wang et al., "Graph-Autoencoder based Knowledge Distillation" which convey knowledge through intermediate features. As an example of using both above methods in knowledge distillation of transformer, DistillBERTSanh et al., "DistilBERT, a Pre-trained Bidirectional Transformers for Language Understanding" initializes the student with teacher’s partial parameters, and minimized the soft target probabilities and cosine similarity of hidden states between the teacher and the student. ThroughLiu et al., "Attention-Based Knowledge Distillation for Deep Neural Networks" find that the attention weights learned by BERT can capture substantial linguistic knowledge, TinyBERTJiao et al., "TinyBERT: Distilling BERT for Natural Language Representations" propose the attention based distillation to encourage that the linguistic knowledge can be transferred from teacher to student. MobileBERTTan et al., "MobileBERT: a Compact Task-Agnostic Representation for Vision and Language Understanding" train a specially designed inverted-bottleneck and bottleneck structures to keep their layer number and hidden size the same for the teacher and the student, transferring knowledge through feature maps and self-attention maps. MINILMShen et al., "MINILM: A Lightweight and Compact Transformer Model for Knowledge Distillation" introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions. However, the above work require that the number of attention heads must be same for the teacher and the student, which is not in line with reality.