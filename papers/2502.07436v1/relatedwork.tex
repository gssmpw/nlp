\section{Related Work}
\subsection{Efficient Multi-head Attention}
The transformer architecture has revolutionized the field of natural language processing and computer vision, enabling the development of powerful models. One of the key components of the transformer is the attention mechanism, which allows the model to focus on relevant parts of the input sequence when making predictions. While inferencing these layers is often slow, due to the memory-bandwidth cost of repeatedly loading the large "keys" and "values" tensors. MQA[\cite{shazeer2019fast}] uses a single key-value head, drastically speeds up decoder inference, while GQA[\cite{ainslie2023gqa}] introduce grouped-query attention to avoid quality degradation of MQA. Research such as [\cite{voita2019analyzing}] and [\cite{michel2019sixteen}] conduct a detailed analysis of the functional roles of individual heads within the transformer's multi-head attention mechanism. Specifically, they assess whether certain heads are underperforming or redundant, and explore the feasibility of directly discarding these less contributive heads.
\subsection{Knowledge Distillation of Transformer}
% 思路：介绍蒸馏，蒸馏常用的是蒸logits和hints【列一堆文章】,引出为什么蒸馏attention maps, 介绍几篇这类文章，归纳总结强调都需要t和s有相同的heads数，而我们balabala...
Knowledge distillation\cite{hinton2015distilling} aims to train student networks by compressing or transferring knowledge from teacher model to student model. There are two common methods in this field, logits-based methods [\cite{cho2019efficacy}, \cite{furlanello2018born}, \cite{mirzadeh2020improved}, \cite{zhang2018deep}, \cite{zhao2022decoupled}] which convey knowledge on the logits level and hint-based methods [\cite{heo2019knowledge}, \cite{huang2017like}, \cite{kim2018paraphrasing}, \cite{park2019relational}, \cite{peng2019correlation}] which convey knowledge through intermediate features. As an example of using both above methods in knowledge distillation of transformer, DistillBERT(\cite{DBLP:journals/corr/abs-1910-01108}) initializes the student with teacher’s partial parameters, and minimized the soft target probabilities and cosine similarity of hidden states between the teacher and the student. Through \cite{DBLP:journals/corr/abs-1904-04063} find that the attention weights learned by BERT can capture substantial linguistic knowledge, TinyBERT(\cite{DBLP:journals/corr/abs-1909-10351}) propose the attention based distillation to encourage that the linguistic knowledge can be transferred from teacher to student. MobileBERT(\cite{sun2020mobilebert}) train a specially designed inverted-bottleneck and bottleneck structures to keep their layer number and hidden size the same for the teacher and the student, transferring knowledge through feature maps and self-attention maps. MINILM(\cite{wang2020minilm}) introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions. However, the above work require that the number of attention heads must be same for the teacher and the student, which is not in line with reality. 
% MINILMV2 employ the relational knowledge to train the student model to avoid the restriction of the number of student’s attention heads. 

% 补充下diffusion蒸馏模型大小的研究（大部分是做蒸馏步数的），强调这方面的蒸馏研究比较空白。搜了下只看到一篇（BK-SDM）做这个的，篇幅有限先不提了吧。