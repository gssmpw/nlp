[
  {
    "index": 0,
    "papers": [
      {
        "key": "shazeer2019fast",
        "author": "Shazeer, Noam",
        "title": "Fast transformer decoding: One write-head is all you need"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ainslie2023gqa",
        "author": "Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\\'o}n, Federico and Sanghai, Sumit",
        "title": "Gqa: Training generalized multi-query transformer models from multi-head checkpoints"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "voita2019analyzing",
        "author": "Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan",
        "title": "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "michel2019sixteen",
        "author": "Michel, Paul and Levy, Omer and Neubig, Graham",
        "title": "Are sixteen heads really better than one?"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "hinton2015distilling",
        "author": "Hinton, Geoffrey",
        "title": "Distilling the Knowledge in a Neural Network"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "cho2019efficacy",
        "author": "Cho, Jang Hyun and Hariharan, Bharath",
        "title": "On the efficacy of knowledge distillation"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "furlanello2018born",
        "author": "Furlanello, Tommaso and Lipton, Zachary and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima",
        "title": "Born again neural networks"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "mirzadeh2020improved",
        "author": "Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan",
        "title": "Improved knowledge distillation via teacher assistant"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhang2018deep",
        "author": "Zhang, Ying and Xiang, Tao and Hospedales, Timothy M and Lu, Huchuan",
        "title": "Deep mutual learning"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhao2022decoupled",
        "author": "Zhao, Borui and Cui, Quan and Song, Renjie and Qiu, Yiyu and Liang, Jiajun",
        "title": "Decoupled knowledge distillation"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "heo2019knowledge",
        "author": "Heo, Byeongho and Lee, Minsik and Yun, Sangdoo and Choi, Jin Young",
        "title": "Knowledge transfer via distillation of activation boundaries formed by hidden neurons"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "huang2017like",
        "author": "Huang, Zehao and Wang, Naiyan",
        "title": "Like what you like: Knowledge distill via neuron selectivity transfer"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "kim2018paraphrasing",
        "author": "Kim, Jangho and Park, SeongUk and Kwak, Nojun",
        "title": "Paraphrasing complex network: Network compression via factor transfer"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "park2019relational",
        "author": "Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu",
        "title": "Relational knowledge distillation"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "peng2019correlation",
        "author": "Peng, Baoyun and Jin, Xiao and Liu, Jiaheng and Li, Dongsheng and Wu, Yichao and Liu, Yu and Zhou, Shunfeng and Zhang, Zhaoning",
        "title": "Correlation congruence for knowledge distillation"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "DBLP:journals/corr/abs-1910-01108",
        "author": "Victor Sanh and\nLysandre Debut and\nJulien Chaumond and\nThomas Wolf",
        "title": "DistilBERT, a distilled version of {BERT:} smaller, faster, cheaper\nand lighter"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "DBLP:journals/corr/abs-1904-04063",
        "author": "Afra Alishahi and\nGrzegorz Chrupala and\nTal Linzen",
        "title": "Analyzing and Interpreting Neural Networks for {NLP:} {A} Report on\nthe First BlackboxNLP Workshop"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "DBLP:journals/corr/abs-1909-10351",
        "author": "Xiaoqi Jiao and\nYichun Yin and\nLifeng Shang and\nXin Jiang and\nXiao Chen and\nLinlin Li and\nFang Wang and\nQun Liu",
        "title": "TinyBERT: Distilling {BERT} for Natural Language Understanding"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "sun2020mobilebert",
        "author": "Sun, Zhiqing and Yu, Hongkun and Song, Xiaodan and Liu, Renjie and Yang, Yiming and Zhou, Denny",
        "title": "Mobilebert: a compact task-agnostic bert for resource-limited devices"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "wang2020minilm",
        "author": "Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming",
        "title": "Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers"
      }
    ]
  }
]