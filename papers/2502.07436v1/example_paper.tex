%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs,multirow} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{Optimizing Knowledge Distillation in Transformers: Enabling Multi-Head Attention without Alignment Barriers}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Zhaodong Bing}{amd}
\icmlauthor{Linze Li}{jiiov}
\icmlauthor{Jiajun Liang}{kuaishou}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{amd}{Advanced Micro Devices.}
\icmlaffiliation{jiiov}{JIIOV Technology.}
\icmlaffiliation{kuaishou}{Kuaishou Technology.}

\icmlcorrespondingauthor{Jiajun Liang}{tracyliang18@gmail.com}
% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{The work was done when Bing and Liang was in JIIOV Technology.} % otherwise use the standard text.

\begin{abstract}
%Knowledge distillation has been proven effective for compressing transformer architectures by transferring knowledge from teacher to student models. Logits-based methods of knowledge distillation cannot fully capture the intermediate representations and features within the teacher model, which may result in the student model not fully learning all the knowledge from the teacher model. Thus, previous work focuses on transferring knowledge through intermediate features or attention maps. However, leveraging multi-head attention maps in transformers for knowledge distillation presents challenges due to head misalignment and suboptimal feature alignment, often requiring projectors to align features or special modifications to the model architecture. To address above limitations, we propose the Squeezing-Heads Distillation (SHD) method. This method reduces the number of attention maps to any desired number through linear approximation, without requiring additional projectors or parameters. This facilitates better alignment and knowledge transfer between models with different numbers of heads, enhancing both flexibility and efficiency. Experimental results demonstrate significant improvements in both language and vision generative models, validating the effectiveness of our method.
Knowledge distillation (KD) in transformers often faces challenges due to misalignment in the number of attention heads between teacher and student models. Existing methods either require identical head counts or introduce projectors to bridge dimensional gaps, limiting flexibility and efficiency. We propose Squeezing-Heads Distillation (SHD), a novel approach that enables seamless knowledge transfer between models with varying head counts by compressing multi-head attention maps via efficient linear approximation. Unlike prior work, SHD eliminates alignment barriers without additional parameters or architectural modifications. Our method dynamically approximates the combined effect of multiple teacher heads into fewer student heads, preserving fine-grained attention patterns while reducing redundancy. Experiments across language (LLaMA, GPT) and vision (DiT, MDT) generative 
 and vision (DeiT) discriminative tasks demonstrate SHD’s effectiveness: it outperforms logit-based and feature-alignment KD baselines, achieving state-of-the-art results in image classification, image generation language fine-tuning, and language pre-training. The key innovations—flexible head compression, projector-free design, and linear-time complexity make SHD a versatile and scalable solution for distilling modern transformers. This work bridges a critical gap in KD, enabling efficient deployment of compact models without compromising performance.
\end{abstract}

\section{Introduction}
In recent years, generative large models have experienced rapid growth, significantly impacting both NLP (e.g., GPT series[\cite{brown2020language}, \cite{achiam2023gpt}], LLaMA series[\cite{touvron2023llama}, \cite{dubey2024llama}]) and computer vision domains (e.g., text-to-image generation[\cite{esser2024scaling}], text-to-video generation[\cite{blattmann2023stable}]). Despite their impressive capabilities, these models typically involve a massive number of parameters, posing substantial challenges for practical online applications.

As the core of transformer models, the multi-head attention mechanism allows each head to attend to different parts of the input sequence, enabling the model to capture diverse and complex relationships between tokens. However, research such as \cite{voita2019analyzing} has shown that only a small subset of attention heads significantly contributes to performance, suggesting redundancy among heads. By pruning the redundant heads, models can maintain their performance while reducing complexity. Similarly, \cite{michel2019sixteen} demonstrates that most attention heads can be removed during testing without substantial performance degradation. Both studies and our observation\ref{sec:observation1} suggest redundancy among multiple heads.

While pruning is a powerful compression technique, knowledge distillation offers another approach to model compression and performance improvement. Traditional knowledge distillation methods, particularly those developed for CNNs before the transformer era, focus on transferring knowledge through logits and intermediate feature maps. Techniques like DeiT[\cite{touvron2021training}] use a distillation token, and Patient-KD[\cite{sun2019patient}] transfers intermediate features. However, transformers emphasize attention mechanisms, prompting research into distilling attention maps directly. TinyBERT[\cite{jiao2019tinybert}] and MobileBERT[\cite{sun2020mobilebert}] both explore the distillation of attention maps but require special model designs of matching head numbers between teacher and student models, which is alignment barriers in knowledge distillation for transformers. See a more detailed description in our observation\ref{sec:observation2}.

We aim to design a practical method for generating multi-head attention map supervision from the teacher model during training, matching the student’s head number to facilitate fine-grained knowledge transfer. This approach addresses head number mismatch, coarse-grained attention transfer, and the need for additional projectors. By introducing a non-standard attention matrix, we can achieve lossless feature representation with fewer heads. However, as stated in observation\ref{sec:observation3}, the unconstrained matrix lacks necessary attention knowledge for distillation. 
% By reintroducing traditional attention matrix constraints, we compute an effective shared attention matrix using constrained quadratic programming, though this has high computational complexity.
% The primary challenge addressed in this work is the redundancy in multi-head attention mechanisms and the practical limitation imposed by the requirement for teacher and student models to have an identical number of attention heads in most existing distillation methods. This limitation is particularly problematic given the diverse architectures of modern large-scale models. For example, the recently released LLaMA3.1[\cite{dubey2024llama}] models by Meta come in various sizes (8B, 70B, 405B), each with a different number of attention heads (32, 64, 128, respectively). This diversity necessitates a more flexible and efficient distillation approach that can handle models with varying attention head counts.

% 这一段对于自己方法的描述需不需要再简化一下呢
% We aim to design a practical method for generating the multi-head attention map supervision from the teacher model during training, matching the student's head number to facilitate fine-grained knowledge transfer during training. This approach simultaneously addresses head number mismatch, coarse-grained attention transfer, and the need for additional projectors. In the transformer’s generative inference process, we consider \(N\) tokens with \(d\)-dimensional feature vectors, where the feature length \(d_{model}\) of each head is determined by dividing \(d\) by the number of heads. Particularly in tasks involving long texts and high-resolution generation, it is often the case that \(N\) exceeds \(d_{model}\). In this context, by introducing a non-standard attention matrix \(\tilde{A} \) (which is not constrained by non-negativity or the requirement that each row sum to one) and allowing multiple heads to replace attention with this matrix, we can achieve lossless feature representation with fewer heads. This provides a mathematical basis for head compression. However, the unconstrained \(\tilde{A} \) lacks the necessary attention knowledge for distillation. By reintroducing traditional attention matrix constraints (such as non-negativity, element values within [0,1], and rows summing to 1), it is possible to compute an effective shared attention matrix across multiple heads using constrained quadratic programming. Nevertheless, this approach has a computational complexity of \(O(N^6)\), making it impractical for frequent training iterations.

% To address these issues, we propose the Squeezing-Heads Distillation(SHD) method. This approach compresses multiple attention maps into a single attention map through linear search, achieving better alignment and knowledge transfer between teacher and student models with different numbers of heads. Better Flexibility: unlike existing methods that require identical attention head counts, Squeezing Heads can handle models with varying numbers of heads, making it applicable to a wider range of model architectures. Better Efficiency: by compressing multiple attention maps into one, our method reduces the computational overhead associated with handling multiple heads, thereby improving the efficiency of the distillation process compared with using traditional projectors with extra parameters. Better knowledge transfer: the linear search approach ensures that the distilled attention map retains the most relevant information from the teacher model, leading to better performance in the student model.
In response to these challenges, we propose the \textbf{Squeezing Multi-Heads Distillation} method. This approach compresses multiple attention maps into a single attention map through efficient linear approximation, achieving fine-grained knowledge transfer between teacher and student models with different numbers of heads. The Squeezing Multi-Heads Distillation method offers several advantages. \textbf{Flexibility}: Unlike existing approaches that require matching attention head counts, our method allows for models with varying numbers of heads, broadening its applicability across different architectures. \textbf{Fine-grained Attention Knowledge}: it goes beyond simply transferring attention features \( F \in \mathbb{R}^{N \times D} \) from the multi-head attention output, such as the Gram matrix \( F^T F \), which provides a coarse approximation akin to single-head attention distillation. \textbf{Efficiency}: By compressing multiple attention maps into a single map using linear approximation, our method reduces computational overhead during distillation, improving overall efficiency compared with using traditional projectors with extra parameters.

% The main contributions of this paper are as follows:
% \begin{itemize}
% \item Proposing the Squeezing-Heads Distillation(SHD) method to address the redundancy and alignment issues in multi-head attention mechanisms.

% \item Demonstrating the effectiveness of our method through comprehensive experiments on various models with different attention head counts.

% \item Providing a flexible and efficient solution for knowledge distillation that can be easily integrated into existing distillation framework.

The main contributions of this paper are as follows: \begin{itemize} 
\item We analyzed the behavior of multi-head compression using both unconstrained and constrained attention matrix approximations during training, and proposed an efficient Squeezing Multi-Heads Distillation(SHD) method based on linear approximation to address redundancy and alignment challenges in multi-head attention distillation.

\item Our method provides a flexible and efficient solution for fine-grained knowledge transfer in knowledge distillation, which can be seamlessly integrated into existing distillation frameworks.

\item We demonstrated the effectiveness of our method through comprehensive experiments on both language and vision generative tasks across diverse settings. 
\end{itemize}

\section{Related Work}
\subsection{Efficient Multi-head Attention}
The transformer architecture has revolutionized the field of natural language processing and computer vision, enabling the development of powerful models. One of the key components of the transformer is the attention mechanism, which allows the model to focus on relevant parts of the input sequence when making predictions. While inferencing these layers is often slow, due to the memory-bandwidth cost of repeatedly loading the large "keys" and "values" tensors. MQA[\cite{shazeer2019fast}] uses a single key-value head, drastically speeds up decoder inference, while GQA[\cite{ainslie2023gqa}] introduce grouped-query attention to avoid quality degradation of MQA. Research such as [\cite{voita2019analyzing}] and [\cite{michel2019sixteen}] conduct a detailed analysis of the functional roles of individual heads within the transformer's multi-head attention mechanism. Specifically, they assess whether certain heads are underperforming or redundant, and explore the feasibility of directly discarding these less contributive heads.
\subsection{Knowledge Distillation of Transformer}
% 思路：介绍蒸馏，蒸馏常用的是蒸logits和hints【列一堆文章】,引出为什么蒸馏attention maps, 介绍几篇这类文章，归纳总结强调都需要t和s有相同的heads数，而我们balabala...
Knowledge distillation\cite{hinton2015distilling} aims to train student networks by compressing or transferring knowledge from teacher model to student model. There are two common methods in this field, logits-based methods [\cite{cho2019efficacy}, \cite{furlanello2018born}, \cite{mirzadeh2020improved}, \cite{zhang2018deep}, \cite{zhao2022decoupled}] which convey knowledge on the logits level and hint-based methods [\cite{heo2019knowledge}, \cite{huang2017like}, \cite{kim2018paraphrasing}, \cite{park2019relational}, \cite{peng2019correlation}] which convey knowledge through intermediate features. As an example of using both above methods in knowledge distillation of transformer, DistillBERT(\cite{DBLP:journals/corr/abs-1910-01108}) initializes the student with teacher’s partial parameters, and minimized the soft target probabilities and cosine similarity of hidden states between the teacher and the student. Through \cite{DBLP:journals/corr/abs-1904-04063} find that the attention weights learned by BERT can capture substantial linguistic knowledge, TinyBERT(\cite{DBLP:journals/corr/abs-1909-10351}) propose the attention based distillation to encourage that the linguistic knowledge can be transferred from teacher to student. MobileBERT(\cite{sun2020mobilebert}) train a specially designed inverted-bottleneck and bottleneck structures to keep their layer number and hidden size the same for the teacher and the student, transferring knowledge through feature maps and self-attention maps. MINILM(\cite{wang2020minilm}) introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions. However, the above work require that the number of attention heads must be same for the teacher and the student, which is not in line with reality. 
% MINILMV2 employ the relational knowledge to train the student model to avoid the restriction of the number of student’s attention heads. 

% 补充下diffusion蒸馏模型大小的研究（大部分是做蒸馏步数的），强调这方面的蒸馏研究比较空白。搜了下只看到一篇（BK-SDM）做这个的，篇幅有限先不提了吧。

\section{Observations}
\subsection{Observation 1: Head Redundancy in transformers}
\label{sec:observation1}

Multi-head attention is a very common technique that improves the performance of attention mechanisms in transformer models, leading to significant improvements. However, we observed that different heads often capture similar or redundant attention pattern, as illustrated in Fig.\ref{attn_redundant}. 

The most common patterns of attention maps are diagonal and vertical lines, indicating the importance of adjacent tokens or key elements. This phenomenon is prevalent in well-trained generative models, especially the large ones with many heads. This suggests that the number of heads is redundant to some degree.
\begin{figure}[h]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[scale=0.22]{figures/attention_redundant.jpg}
\end{center}
\caption{Attention maps during inference on a random sample in Dolly Datatset by GPT2-XL from different heads of one layer. Each column is from the same head. We random selected three heads to visualize it. The attention patterns are very similar and contain much redundancy within one layer. Only Response attentions are kept and Instruction attentions are masked.}
\label{attn_redundant}
\end{figure}

%可以说一下现在的特征对齐蒸馏有什么样的问题，因为hidden_size不同所以没法直接监督，多数的方法都是加一个projector来做，但训练过程中由于student是从头开训，projector本身是不够稳定的，影响了方法本身的上限。而我们的方法是完全不需要担心对齐的问题的，也不用担心projector训练的问题。
\subsection{Observation 2: Alignment Barriers in Knowledge Distillation for Transformers}
In recent literature, we have identified two major alignment barriers in knowledge distillation for transformers. 
\label{sec:observation2}

First, the significant dimensional gap between large and small transformer models makes feature alignment less effective. Knowledge distillation in generative models faces challenges due to differences in hidden dimensions; large models typically have higher-dimensional hidden layers, making feature alignment with smaller models difficult. For example, GPT-3[\cite{brown2020language}]'s model dimension is 12,288, which is almost 20 times larger than that of GPT-3[\cite{brown2020language}] Small. Some methods introduce projection layers to align features[~\cite{sun2020mobilebert}], but these methods add extra parameters and do not focus on the attention module. Consequently, most feature alignment knowledge distillation methods appear ineffective for generative models.

Second, transformer models often differ in the number of attention heads, leading to head alignment problems. Previous methods like MobileBERT[~\cite{sun2020mobilebert}] employed specialized model designs to circumvent the head alignment issue, but this required both teacher and student models to be specifically engineered, potentially reducing performance. To our knowledge, we are the first to directly address the head alignment problem of attention maps in knowledge distillation.

\subsection{Observation 3: Rank Limitations of Attention Maps and Their Implications}
\label{sec:observation3}

Consider a self-attention module without the causal mask, as used in many diffusion models. The hidden dimension per head (\( d \)) is often much smaller than the number of tokens (\( N \)). For example, DiT models have \( d = 64 \) per head but are trained on images with \( 64 \times 64 = 4096 \) tokens. This also occurs in most generative models. 

Reviewing how the attention map of a single head is computed:

\begin{equation}
\label{eq:attention_equation}
A_i = \text{softmax}\left( \frac{Q^i (K^i)^\top}{\sqrt{d}} \right),
\end{equation}

where \( A_i \in \mathbb{R}^{N \times N} \) is the attention map of the \( i \)-th head, \( Q^i, K^i \in \mathbb{R}^{N \times d} \) are the queries and keys, \( N \) is the number of tokens, and \( d \) is the hidden dimension per head.

Since \( N > d \) in most cases, the rank of \( A_i \) is limited to at most \( d \), making \( A_i \) a rank-deficient matrix. As a result, the attention map of a single head cannot fully utilize its theoretical capacity, leading to redundancy from a linear algebra perspective.

To explore this further, consider introducing a non-standard attention matrix \(  \tilde{A}  \in \mathbb{R}^{N \times N} \), which is not constrained by non-negativity or the requirement that each row sums to one. By allowing all heads to replace the standard attention with a shared \( \tilde{A}  \), we can achieve lossless feature representation with just a single head, as the matrix offers \( N^2 \) degrees of freedom but is constrained to only \( N \times d \). This provides a mathematical basis for head compression.

However, the unconstrained nature of \( \tilde{A}  \) lacks the structured attention knowledge required for tasks like distillation. In the following sections, we will propose a efficient solution to incorporate more meaningful knowledge into this representation while maintaining a reasonable feature representation loss.
% Consider a self-attention module without the causal mask, like most diffusion models did. The hidden dimensions per head is always smaller than the number of tokens. For example, like DiT models, they have 64 hidden dim per head but they are trained on 64x64=4096 tokens. This also happens in most generative models. Therefore, when you look at the whole attention map and review how it is computed:
% \begin{equation}\label{attention_equation}
% \begin{split}
%     A_{i}=softmax(\frac{Q^{i}(K^{i})^{T}}{\sqrt{d}})\\
%     Attention(Q,K,V)=AV
% \end{split}
% \end{equation}
% Where $A_{i} \in \mathbb{R}^{N\times N}$ is the attention map of $i$th head, $Q,K,V \in \mathbb{R}^{N\times d} $ are the queries, keys and values, $N$ is the number of tokens and $d$ is the hidden dimension per head. As $N>d$ in most cases, the rank of $A_{map}$ for each head can be less equal than $d$, making it a non-singular matrix. So the attention map of a single head may not fully utilize the memory of it as in the perspective of linear algebra, which is also redundant. 

% Some research has been done to avoid softmax operations in attention such as Linear Attention to accelerate transformers, therefore the general definition of an attention map is just a matrix whose shape belongs to $N\times N$ and the sum of every row equals to 1 and is the product of calculation with $Q$ and $K$. However, this problem remains the same in this situation.
\section{Squeezing Heads: A Better Supervision Beyond Just Logits}




Traditional knowledge distillation algorithms primarily focus on feature distillation. However, the gap in hidden dimensions between teacher and student transformer models makes feature alignment challenging. Attention maps provide a better supervision signal since they are independent of hidden dimensions. Yet, teachers and students often have different numbers of heads due to computational constraints in model design. Our method addresses this issue by improving knowledge transfer. We propose a technique that compresses diverse multi-head attention into a single attention map while preserving key information, which can then be used as supervision for knowledge distillation. Initially, we introduce lossless unconstrained attention compression and optimal constrained attention compression based on teacher features, but these methods are computationally inefficient for every training iteration. We then present our ``squeezing heads" method, which  compresses attention maps by efficient linear approximation and achieves strong results in practice. 


\subsection{Attention compression by Exact Optimization}


Let's examine multi-head attention more closely. Generally, attention consists of two parts: (1) Scaled Dot-Product Attention and (2) Multi-Head Attention. The scaled dot-product attention is defined as:

\begin{equation}
\label{eq:scaled_dot_product_attention}
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^\top}{\sqrt{d}} \right) V.
\end{equation}

The multi-head attention mechanism can be described as:

\begin{equation}
\label{eq:multi_head_attention}
\begin{aligned}
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{Head}_1, \dots, \text{Head}_h) W^O, \\
\text{where } \text{Head}_i &= \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V),
\end{aligned}
\end{equation}

where \( W_i^Q, W_i^K, W_i^V \in \mathbb{R}^{d_{\text{model}} \times d} \), and \( W^O \in \mathbb{R}^{h d \times d_{\text{model}}} \) are projection matrices.

To better understand our method, we can expand \( W^O \) as concatenated per-head output projections \( W_i^O \in \mathbb{R}^{d \times d_{\text{model}}} \)[\cite{elhage2021mathematical}]. Then, the multi-head attention can be rewritten as:

\begin{equation}
\label{eq:multi_head_expanded}
\begin{aligned}
\text{MultiHead}(Q, K, V) &= \sum_{i=1}^{h} \text{Head}_i W_i^O = \sum_{i=1}^{h} A_i V W_i^V W_i^O, \\
\text{where }  A_i &= \text{softmax}\left( \frac{Q W_i^Q (K W_i^K)^\top}{\sqrt{d}} \right).
\end{aligned}
\end{equation}


From an information theory perspective, we try to compress multiple attention maps into a single attention map while preserving feature representation. We denote $VW_{i}^{V}W_{i}^{O}$ as $X_{i}$ for simplicity of symbols. Consider combining two attention heads \( A_{2i-1} \) and \( A_{2i} \) into a single attention map \( \tilde{A}_i \) that satisfies:
\begin{equation}
\begin{aligned}
\tilde{A}_i &= \arg\min_{\tilde{A}_i} \| \tilde{A}_i \left( X_{2i-1}+X_{2i} \right) \\
&\quad - \left( A_{2i-1} X_{2i-1} + A_{2i} X_{2i} \right) \|_F^2.
\end{aligned}
\end{equation}


This unconstrained optimization problem seeks the \(\tilde{A}_i\) that best aligns the combined transformed value vectors before and after compression, effectively "squeezing" the attention heads by finding the optimal aggregate attention map \(\tilde{A}_i\).

% \begin{equation}
% \label{eq:squeezing_heads}
% \tilde{A}_i (V W_{2i-1}^V W_{2i-1}^O + V W_{2i}^V W_{2i}^O) = A_{2i-1} V W_{2i-1}^V W_{2i-1}^O + A_{2i} V W_{2i}^V W_{2i}^O.
% \end{equation}

We observe that the loss can actually be reduced to zero, providing a closed-form solution for \( \tilde{A}_i \) regardless of how many heads are compressed, due to the additional degrees of freedom. However, directly computing \( \tilde{A}_i \) using the pseudo-inverse is impractical during training because it has a computational complexity of \( O(N^6) \) and this solution does not consider the constrain of the attention matrix and can not provide token relation knowledge.

If we constrain the optimization problem by restricting the values in \( \tilde{A}_i \) to the range \([0, 1]\) and ensuring that each row sums to 1, the problem becomes convex with a global minimum solution. Nonetheless, the computational complexity remains \( O(N^6) \), making it equally impractical to apply during training.

\subsection{Linear Approximation for Aligning Attention Maps}
% It is computationally expensive to get exact attention compression results.
% To address this issue, we propose using a linear combination of attention maps to approximate the combined effect, we example of squeezing two heads into one for :
% simplicity, in fact our head can be easily extended to compress arbitrary heads into one:
Obtaining exact attention compression results is computationally expensive. To mitigate this, we propose a linear combination of attention maps to approximate the combined effect. For simplicity, we demonstrate this by squeezing two heads into one; however, our method can be easily extended to compress any number of heads into one. We reparameterize  \( \tilde{A}_i \) as a linear combination of known attention maps as follows: 

\begin{equation}
\label{eq:linear_combination}
\tilde{A}_i = \alpha_i A_{2i-1} + (1 - \alpha_i) A_{2i},
\end{equation}

where \( \alpha_i \in [0, 1] \) is a scalar weight to be determined.

Our goal is to find \( \alpha_i \) that minimizes the difference between the combined output using \( \tilde{A}_i \) and the original outputs:

\begin{equation}
\label{eq:minimization_problem}
% \resizebox{\textwidth//2}{!}{
\begin{aligned}
\alpha_i &= \arg\min_{\alpha} \| \tilde{A}_i (X_{2i-1} + X_{2i}) \\
&\quad - (A_{2i-1} X_{2i-1} + A_{2i} X_{2i}) \|_F^2,
% }
\end{aligned}
\end{equation}

where \( X_{2i-1} = V W_{2i-1}^V W_{2i-1}^O \) and \( X_{2i} = V W_{2i}^V W_{2i}^O \).

Expanding the expression, we have:
\begin{equation}
\label{eq:error_expansion}
\begin{aligned}
E(\alpha_i) &= \left\| \left( \alpha_i A_{2i-1} + (1 - \alpha_i) A_{2i} \right) (X_{2i-1} + X_{2i}) \right. \\
&\quad - \left. ( A_{2i-1} X_{2i-1} + A_{2i} X_{2i} ) \right\|_{F}^2 \\
&= \left\| \alpha_i M + N \right\|_{F}^2,
\end{aligned}
\end{equation}

where we denote $M=(A_{2i-1} - A_{2i})(X_{2i-1} + X_{2i}),N= A_{2i} X_{2i-1} - A_{2i-1} X_{2i}$ for simplicity.

% \begin{equation}
% \label{eq:M_and_N}
% \begin{aligned}
% M &= (A_{2i-1} - A_{2i})(X_{2i-1} + X_{2i}), \\
% N &= A_{2i} X_{2i-1} - A_{2i-1} X_{2i}.
% \end{aligned}
% \end{equation}

To find the optimal \( \alpha_i \), we set the derivative of \( E(\alpha_i) \) with respect to \( \alpha_i \) to zero:

\begin{equation}
\label{eq:alpha_solution}
\frac{d E(\alpha_i)}{d \alpha_i} = 2 \alpha_i \| M \|_F^2 + 2 \langle M, N \rangle = 0,
\end{equation}

where \( \langle M, N \rangle \) denotes the Frobenius inner product.

Solving for \( \alpha_i \), we get:

\begin{equation}
\label{eq:alpha_optimal}
\alpha_i = - \frac{ \langle M, N \rangle }{ \| M \|_F^2 }.
\end{equation}

Since \( A_{2i-1} \) and \( A_{2i} \) are attention maps with non-negative elements and row sums equal to 1, and \( \alpha_i \) is computed to minimize the reconstruction error, in practice \( \alpha_i \) often falls within the interval \( [0, 1] \).

By using this linear combination, we can effectively squeeze the attention maps from two heads into one, aligning the teacher's attention maps with those of the student model that has fewer heads, thus facilitating better knowledge transfer during distillation. Progressively we can merge heads into arbitrary numbers.

While concerns may arise about the linearity assumption in our approximation, we emphasize three critical advantages that validate our approach:

\textbf{Mathematical Soundness}: Convex combinations inherently preserve the probabilistic properties of attention maps—non-negativity and row-sum-to-one constraints—which are essential for maintaining valid token-wise dependencies. Unlike methods like PCA or kernel approximations that may violate these constraints, our linear combination ensures the compressed attention maps remain interpretable and mathematically consistent with standard attention mechanisms.

\textbf{Computational Efficiency}: Our linear approximation operates with 
$O(N^2)$ complexity, aligning with the native cost of attention computation. This contrasts sharply with optimal transport-based alignment methods (e.g., Interior Point Methods), which require $O(N^3)$ operations and are infeasible for frequent training iterations. By avoiding iterative optimization or matrix inversions, our method scales efficiently to long sequences and high-resolution inputs, making it practical for real-world deployment.

\textbf{Empirical Effectiveness}: Extensive experiments across discriminative (ImageNet classification) and generative tasks (image synthesis, LLM pretraining/fine-tuning) validate the superiority of SHD over projector-based and kernel methods. It is further discussed in the experiment section.

By balancing mathematical rigor, efficiency, and empirical performance, our linear approximation provides an optimal trade-off for head alignment in knowledge distillation.


%这里还要简单加一下attention是怎么约束的，还有就是层数不同的时候怎么对应约束这件事。最终loss是怎么算的也说清楚一下。温度的问题也需要说。
% \subsection{Training Objective of Squeezing Heads Distillation}
% Attention maps share similar properties with the output of LLM models. Often the final output of LLM models is processed by a softmax function of logits. Attention maps did it too for using multiplication of Q and K instead of logits. Inspired by logit distillation, we also introduce the concept of attention temperature in our method. We can change Eq.\ref{attention_equation} to a version with attention temperature:
\subsection{Training Objective of Squeezing Heads Distillation}
Attention maps represent discrete categorical distributions. Inspired by logit distillation, we introduce attention temperature to enhance low-probability regions. Consequently, we modify Eq.\ref{attention_equation} to incorporate attention temperature:
\begin{equation}\label{attention_equation}
\begin{split}
    A_{i}=softmax(\frac{Q^{i}(K^{i})^\top}{\sqrt{d} T_{a}})\\
\end{split}
\end{equation}
where $T_{a}$ is the attention temperature which is manually set and normally larger than 1.0. The function of attention temperature is the same as temperature in logit distillation, which is to soften the output probabilities and make it more uniform, highlighting the relative differences between tokens more clearly.

Since our method only focuses on the supervised attention maps of teachers, it can be plugged into all logit-based distillation. The Squeezing Heads Distillation Loss is expressed as:
\begin{equation}
    L_{SHD}=\beta \sum_{i=1}^{L} \sum_{j=1}^{H_{s}} L_{KL}(\Tilde{A}^{t}_{i},A^{s}_{i})
\end{equation}
where $L_{KL}$ is the Kullback-Leibler divergence loss.
We add it to the original training loss in our experiments with $beta$ to control the intensity.



\section{Experiments}
%速度用minillm的测速完事，证明和logits训练基本没有差异。loss设置方式
%需要有哪些：1）MDT的实验表格，说明相对KD涨点幅度，说明对于diffusion model的有效性。 check 2）ablation study1：说明相同setting随机蒸馏一部分的头是没有收益的 check 3)ablation study2:超参选择,说明超参不是非常敏感 check 4）ablation study3 不同loss约束上哪个更好 5）ablation study4：温度对于实验的影响 check 6)ablation study：是不是比固定比例融合head更有效,a值在训练过程中的变化情况 7）BabyLlama pretraining实验 check 8）supervised fine-tuning minillm 实验 check 9）蒸馏时间损耗变化：在miniLLM上算一下单步时间，至少训练过程看起来没变慢。


To validate the improvement of our method, we performed several experiments on different generative transformer models. We trained diffusion transformer models for image generation tasks and trained various LLMs for both LLM pretraining tasks and supervised fine-tuning tasks. All the LLM and diffusion generative models selected are transformer models. We apply our method to self-attention blocks of all student model layers. We select image generation tasks and LLM tasks because those two represent generative tasks nowadays since diffusion transformer models use full attention on the whole image or latent space and LLMs use causal attention to performance in an auto-regressive manner. Those two represent the most generative methods in two major AIGC fields. 

Suppose teacher and student models have different layers of transformer block. In that case, we select the corresponding layers of the teacher model for the student, e.g. if the teacher has 48 layers and the student has 24 layers, the supervision of student's $4^{th}$ layer will be the teacher's $8^{th}$ layer.

\subsection{Major Results}
% \begin{adjustwidth}{-2.5 cm}{-2.5 cm}\centering\begin{threeparttable}[!htb]\
\begin{table*}[!htp]\centering

\begin{tabular}{lccccccccc}\toprule
Method &Params &Model &Image Res &Steps &FID-50K↓ &IS↑ &Prec↑ &Rec↑ \\\midrule
teacher &130M &MDTv2-B/2 &256x256 &400k &35.77 &54.01 &0.48 &0.62 \\
\cline{2-9} \\
w/o KD &33M &MDTv2-S/2 &256x256 &400k &44.87 &37.29 &0.47 &\textbf{0.49} \\
KD &33M &MDTv2-S/2 &256x256 &400k &38.73 &43.43 &0.50 &0.48 \\
KD+SHD &33M &MDTv2-S/2 &256x256 &400k &\textbf{36.95} &\textbf{46.27} &\textbf{0.52} &0.48 \\
\cline{1-9} \\
teacher &675M &MDTv2-XL/2 &256x256 &3500k &1.58 &314.73 &0.79 &0.65 \\
\cline{2-9} \\
w/o KD &33M &MDTv2-S/2 &256x256 &500k &42.33 &40.38 &0.48 &0.49 \\
KD &33M &MDTv2-S/2 &256x256 &500k &33.08 &49.23 &0.52 &0.57 \\
KD+SHD &33M &MDTv2-S/2 &256x256 &500k &\textbf{32.27} &\textbf{50.54} &\textbf{0.53} &\textbf{0.57} \\
\bottomrule
\end{tabular}
\caption{Performance comparison for image generation on ImageNet-1K. The detailed definitions of metrics Prec and Rec can be found in \cite{kynkäänniemi2019improvedprecisionrecallmetric}.}\label{tab:Imagenet_1k_result}
\end{table*}
% \end{threeparttable}\end{adjustwidth}
\textbf{Image Generation.} We did several experiments on MDTv2 models under different model sizes, as shown in \ref{tab:Imagenet_1k_result}. We select MDTv2-S/2-MDTv2-XL/2, and MDTv2-S/2-MDTv2-B/2 as the student-teacher pairs for different model sizes. With simple KD, our methods can improve FID and IS scores by a huge margin on both settings, achieving a 36.95 FID score when training 400k steps for MDTv2-S/2 model. It shows that with different teacher model size and large capacity gaps, our method still benefits the student model's representation learning.

\textbf{Pretraining on LLM.} The results after fine-tuning are reported in Tab.\ref{tab:babylm_result}. We can observe that extremely small models like BabyLLaMA (58M) still can gain improvement from our method in this experiment. The performance of our method beats models of twice our size on several evaluation datasets like SST-2, MRPC, QQP and MNLI-mm, while performance on other datasets also improved compared to our baseline. We can see that our method to imitate attention maps in the pretraining shows great generalization capability on downstream tasks.

\begin{table*}[!htb] \label{babyllm_main_result}
\centering
\begin{tabular}{lcccccc}\toprule

Model &OPT(base) &T5(base) &BabyLLaMA &BabyLLaMA+SHD \\
Size &125M &222M &58M &58M \\
\midrule
CoLA(MCC) &15.2 &11.3 &15.6 &\textbf{17.5}(+1.9) \\
SST-2 &81.9 &78.1 &85.8 &\textbf{88.4}(+2.6) \\
MRPC(F1) &72.5 &80.5 &81.6 &\textbf{82.0}(+0.4) \\
QQP(F1) &60.4 &66.2 &82.8 &\textbf{83.1}(+0.3) \\
MNLI &57.6 &48.0 &\textbf{72.9} &72.8(-0.1) \\
MNLI-mm &60.0 &50.3 &73.7 &\textbf{74.0}(+0.3) \\
RTE &60.0 &49.4 &58.6 &\textbf{58.6}(+0.0) \\
BoolQ &63.3 &66.0 &59.8 &\textbf{61.7}(+1.9) \\
MultiRC &55.2 &47.1 &54.6 &\textbf{59.0}(+4.4) \\
WSC &60.2 &61.4 &53.0 &\textbf{56.6}(+3.3) \\
\bottomrule
\end{tabular}
\caption{Fine-tuning accuracy (if not specified), MCC score or F1 score evaluated by SuperGLUE on language pretraining task.}\label{tab:babylm_result}
\end{table*}

\textbf{Supervised Fine-tuning on LLM.} We evaluated our method in Tab.\ref{tab: MiniLLM result}. Our method beats our baseline MiniLLM[\cite{gu2024minillm}] by 0.8\% on DollyEval. This is the major metric we look on since the models are trained on Dolly. Our model also gains huge improvement on other test sets like S-NI UnNI, achieving SoTA in the same model and training settings. The experiment on SelfInst also shows that our method can benefit the student's representations even when the student is better than the teacher's performance (14.3-\textgreater15.2). The knowledge transferred by SHD is the ability to model long-range dependencies. The student can always benefit from a model with more parameters, even if it is not optimized to its full state (in our experiment, the teacher only trained with SFT). We also show our method over three different student model sizes. We have made almost all improvements except on S-NI with the 760M student model. The teacher is not well trained compared to the student in this case. On relatively modern and large models like LLaMA-13B and LLaMA-7B, our SHD still can boost the performance, gaining 1.1\% improvement on UnNI.


\begin{table*}[!htb]\centering
\begin{tabular}{lrrrrrrrrrrr}\toprule
Method &Head &Params &DollyEval &SelfInst &VincunaEval &S-NI &UnNI \\\midrule
Teacher(GPT2-XL)&25&1.5B &27.6 &14.3 &16.3 &27.6 &31.8 \\
\cline{2-8}\\
SFT w/o KD &12&\multirow{4}{*}{120M} &23.3 &10.0 &14.7 &18.5 &18.5 \\
KD &12& &22.8 &10.8 &13.4 &16.4 &22.0 \\
MiniLLM &12& &24.6 &13.2 &16.9 &\textbf{25.1} &25.6 \\
MiniLLM+SHD &12& &\textbf{24.8} &\textbf{13.6} &\textbf{18.0} &\textbf{25.1} &\textbf{25.7} \\
\cline{1-8} \\
SFT w/o KD &16&\multirow{4}{*}{340M} &25.5 &13.0 &16.0 &25.1 &32.0 \\
KD &16& &25.0 &12.0 &15.4 &23.7 &31.0 \\
MiniLLM &16& &25.4 &14.6 &\textbf{17.7} &27.4 &31.3 \\
MiniLLM+SHD &16& &\textbf{26.2} &\textbf{15.2} &\textbf{17.7} &\textbf{28.1} &\textbf{32.2} \\
\cline{1-8} \\
SFT w/o KD &20&\multirow{4}{*}{760M} &25.4 &12.4 &16.1 &21.5 &27.1 \\
KD &20& &25.9 &13.4 &16.9 &25.3 &31.7 \\
MiniLLM &20& &26.4 &15.9 &17.7 &\textbf{29.2} &33.0 \\
MiniLLM+SHD &20& &\textbf{26.5} &\textbf{16.2} &\textbf{18.2} &28.9 &\textbf{33.5} \\
\cline{1-8} \\
Teacher(LLaMA-13B)&40&13B &29.7 &23.4 &19.4 &35.8 &38.5 \\
\cline{2-8}\\
MiniLLM &32&\multirow{2}{*}{7B}& 28.9& 23.1 & 19.4 & 34.8 & 37.4 \\
MiniLLM+SHD &32& &\textbf{29.1}  & \textbf{23.4} & \textbf{20.0} & \textbf{34.9} & \textbf{38.5} \\
\bottomrule
\end{tabular}
\caption{Performance Comparison of distillation of LLM on various Test sets, supervised fine-tuning on Dolly.}\label{tab: MiniLLM result}
\end{table*}

\textbf{Image Classification on ImageNet-1k.}To show the effectiveness of our methods, we also did image classification to prove our method can be applied to discriminative tasks. We followed the original settings of ViTKD[\cite{yang2022vitkd}] and NKD[\cite{yang2023knowledge}], which focuses on knowledge distillation of ViT-ViT teacher-student training pairs. All experiments are conducted on ImageNet-1k in the Table \ref{tab:ImageNet-exp}. The teacher model is DeiT3-small and the student model is Deit-Tiny. The result of ViTKD+NKD+SHD also shows the compatibility of SHD with FD methods, improving the performance of ViTKD+NKD by 0.42\% on a strong baseline.
\begin{table}[!htb]\centering
\begin{tabular}{lrrrr}\toprule
Method &Model &Head &Top1 Acc \\\midrule
Teacher &DeiT3-small &6 &80.69 \\
\cline{2-4} \\
Baseline (without KD) &DeiT-Tiny &3 &74.43 \\
Baseline+SHD &DeiT-Tiny &3 &\textbf{75.38} \\
\cline{1-4} \\
ViTKD+NKD &DeiT-Tiny &3 &77.79 \\
ViTKD+NKD+SHD &DeiT-Tiny &3 &\textbf{78.21} \\
\bottomrule
\end{tabular}
\caption{Performance Comparison for image classification on ImageNet-1K.}\label{tab:ImageNet-exp}
\end{table}

\subsection{Ablations and Analysis}
\textbf{Comparison with other representation distillation methods.} Traditional Knowledge distillations always solve the dimension-alignment problem by using projectors or distilling the relations between features which is called Feature Distillation (FD). The projector aligns the student's feature to the teacher's dimension by a single linear projection or an MLP, causing more training parameters to train. The other previous works distill the relations of the features among tokens or spatial-wise pixels. The most common way is to calculate the self-correlations:

\begin{equation} \label{feature_distillation}
\begin{aligned}
Cor&=\frac{FF^{T}}{\Vert F\Vert \Vert F\Vert}, L_{cor}=1-Sim(Cor^{t},Cor^{s})\\
\end{aligned}
\end{equation}
where $F$ is the intermediate feature and $Sim$ is any similarity score measured by some function. We did comparison experiments in Tab.\ref{comparison_representation}. We did hyperparameters searching like our method did and their best results are reported. The details of training speed, training parameters, and FLOPs are in the Appendix. We also compared one of the SoTA FD methods: VkD[\cite{miles2024vkdimprovingknowledgedistillation}]. We also did an ablation study in which we used similarities of attention maps between all heads and selected the pairs of heads of maximum similarity to merge heads on training set before training. We call it "head\_matching" in the Table \ref{comparison_representation}. Our method surpasses those two methods on all evaluation datasets except SelfInst results, which are the same, without extra training costs. 



We show that forcing students to only imitate the relations or the features can cause bad harm to model performance as well, resulting in a degradation of VincunaEval and S-Ni datasets, while our method can benefit from attention relations. This is also proven mathematically by our method part. Our method has the lowest loss of teachers knowledge transfer in the measurement of output.

\textbf{Hyperparameters.} The results of different hyperparameters are listed in Tab.\ref{tab:ablation_hyper_parameters} on image generation. Our method is not very sensitive to hyperparameters and does not require many maunal settings. Also, the attention temperature we propose can improve the results as well.


\textbf{Is SHD better than hard selecting on heads?} Hard selecting is another way to squeeze heads from teacher to student. We did an experiment in which we randomly selected $H^{s}$ heads from teacher models before training and used the attention maps of those heads to supervise the student's training. The results are reported in Tab.\ref{tab:random_select_ablation.} on image generation. The performance dropped drastically with hard selecting several heads compared to KD baselines. This shows that the dropping heads of teachers can be very important for students. The capacity gap from teacher to student can not be ignored for hard selecting. 

\begin{table}[!htp]\centering

\begin{tabular}{lrrrr}\toprule
&\textbf{FID} &\textbf{IS} &\textbf{precision} &\textbf{Recall} \\\midrule
w/o KD &42.33 &40.38 &0.48 &\textbf{0.49} \\
KD &38.73 &43.43 &0.50 &0.48 \\
KD+hard select &40.03 &42.22 &0.50 &0.48 \\
KD+SHD &\textbf{36.95} &\textbf{46.27} &\textbf{0.52} &0.48 \\
\bottomrule
\end{tabular}
\caption{The performance comparison on ImageNet-1K for Image Generation of selecting different attention maps source.}\label{tab:random_select_ablation.}
\end{table}

This experiment also indicates the priority of our method over hard selection. If only part of the teacher knowledge has been transferred among heads, it can be harmful to the student. Our method of soft merging is a better option.\

%速度，alpha的都还没放上来
\textbf{Is SHD better than constant merging on heads?} SHD always calculates sample-wise $\alpha$ for different heads and layers. A naive thought is to merge the teacher's heads by simple constant $\alpha=0.5$. We also did ablation studies for this experiment in Tab.\ref{tab: comparison merging heads}. It achieves comparable results with the original MiniLLM baseline, but also cannot improve the performance like SHD does. SHD is sample-wise and more fine-grained.



\textbf{Does SHD works independently?}We did this ablation study on discriminative tasks in Table.\ref{tab:ImageNet-exp}. We used SHD independently and the result still improved 0.95\% without logits KD. 





\section{Conclusion}
\begin{table}[!htb]\centering
\begin{tabular}{lrrrrrr}\toprule
Method &Dolly &SelfInst &Vincuna &S-NI &UnNI \\\midrule
MiniLLM &25.4 &14.6 &\textbf{17.7} &27.4 &31.3 \\
 +FD+Projector &25.8 &\textbf{15.2} &17.6 &27.3 &31.4 \\
 +FD+SC &25.9 &\textbf{15.2} &15.8 &26.8 &31.7 \\
 +VkD &26.0 &14.9 &\textbf{17.7} &27.1 &31.0 \\
 +SHD &\textbf{26.2} &\textbf{15.2} &\textbf{17.7} &\textbf{28.1} &\textbf{32.2} \\
 +SHD+HM &\textbf{26.3} &\textbf{15.3} &\textbf{18.2} &\textbf{28.1} &\textbf{32.3} \\
\bottomrule
\end{tabular}
\caption{Performance Comparison with other representation distillation methods. Here SC means self correlation and HM means head matching.}
\label{comparison_representation}
\end{table}
In this work, we introduce the Squeezing-Heads Distillation (SHD) method, a novel approach to knowledge distillation that addresses the challenges posed by head misalignment and redundancy in multi-head attention mechanisms of transformer models. Our method effectively compresses multiple attention maps into a single map through a linear search process, enabling better alignment and knowledge transfer between models with different numbers of heads. This approach not only enhances the flexibility and efficiency of the distillation process but also improves the performance of student models across various generative tasks.

We validate the effectiveness of SHD through comprehensive experiments on both image generation and language pretraining tasks. Our results demonstrate significant improvements in key metrics such as FID, IS, and accuracy, outperforming traditional knowledge distillation methods and other representation distillation techniques. The proposed method also proves to be robust across different model sizes and architectures, showing its general applicability.

Furthermore, our ablation studies confirm that SHD provides a more stable and efficient improvement compared to hard selecting or constant merging of heads. The introduction of attention temperature further enhances the distillation process by softening the output probabilities, leading to better performance in student models. Notably, SHD achieves these improvements without introducing additional training parameters or significantly impacting training speed.

In conclusion, the Squeezing-Heads Distillation method offers a practical and effective solution for optimizing knowledge distillation in transformers, enabling the deployment of smaller, more efficient models without compromising on performance. This work paves the way for further research into flexible and efficient distillation techniques that can adapt to the diverse architectures of modern large-scale models.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\begin{figure*}[h]\label{method_pic}
\begin{center}
\includegraphics[scale=0.2]{figures/SHD_Method.jpg}
\end{center}
\caption{Comparison of typical representation distillation methods and our method. (a) Typical distillation uses a projector to align student features with teacher features, introducing extra parameters. (b) Typical distillation uses relations like self-correlations to align feature dimensions. (c) Our method, SHD, uses attention maps and outputs to squeeze attention maps, aligning with the student and ensuring minimal loss of knowledge transfer.}
\label{fig:shd_method}
\end{figure*}


\begin{figure*}[h]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[scale=0.35]{figures/alphas.png}
\end{center}
\caption{Distribution of $\alpha$ from different layers of GPT2-XL. Same color represents same head.}
\label{alpha visualization}
\end{figure*}

\textbf{Training details.} For image generation tasks, we used the families of MDTv2 architecture which is a variant of DiT models. We use it to prove that our method is compatible with different kinds of methods. We used the MDT-B/2 model as the teacher model and MDT-S/2 model as the student model, which are both trained on ImageNet-1k with a resolution of 256x256, using a 256 batch size and Adan optimizer. Other settings are also aligned with the original MDT and DiT. The $\beta$ we set on image generation tasks is 2.0.

\begin{table}[!htp]\centering
\begin{tabular}{lrrrrrr}\toprule
\textbf{Temp} &\textbf{beta} &\textbf{FID} &\textbf{IS} &\textbf{precision} &\textbf{Recall} \\\midrule
1 &0.5 &37.73 &45.25 &0.51 &0.49 \\
1 &2.0 &38.64 &44.16 &0.50 &0.48 \\
1 &5.0 &39.39 &43.86 &0.49 &0.49 \\
2 &2.0 &36.95 &46.27 &0.51 &0.48 \\
\bottomrule
\end{tabular}
\caption{Performance comparison for different hyper-parameters for image generation on ImageNet-1K for SHD.}\label{tab:ablation_hyper_parameters}
\end{table}
For language pretraining tasks, we trained LLaMA models on the BabyLM dataset. We compare our method with BabyLLaMA. BabyLLaMA averaged two teacher model logits as an ensembled teacher. The teacher models used are GPT-2 and LLaMA. Experiments are conducted only on one GPU. The models are trained for 6 epochs with a batch size of 256 and for a learning rate of $2.5\times10^{-4}$. We take one teacher's attention map for squeezing head distillation. We followed all the settings of BabyLLaMA. The $\beta$ we set is 1.0. We retrained BabyLLaMA with BabyLM with the original settings and official code and reevaluated the metrics.

For supervised fine-tuning tasks, we follow the setting of MiniLLM. we randomly selected 12500 samples for training, 1000 samples for validation, and 500 samples for testing 
from databricks-dolly-15k dataset, respectively. The other training recipes are the same as MiniLLM except batchsize=16 in the LLAMA-13B-7B pairs due to our limited computation resources. We retrained all MiniLLM models with the official reproducible code. The metrics are averaged among 5 runs with 5 random seeds.

\textbf{Evaluation.} 
We evaluate image generation with common metrics: Frechet Inception Distance(FID), Inception Score (IS), Precision and Recall. The major metric is FID since it evaluates both diversity and fidelity. The results are evaluated with 250 DDPM sampling steps and 50000 samples generated with classifier-free guidance 3.8.

We evaluate LLM pretraining with SuperGLUE as the fine-tuning benchmarks. After pretraining with BabyLM dataset, the models are then finetuned with superGLUE. All the fine-tuning follows the setting of BabyLLaMA to avoid overfitting. The metrics we used are the Matthews correlation coefficient (MCC), F1 score, and accuracy.

As for supervised fine-tuning tasks, we followed MiniLLM using 5 instruction-following datasets: DollyEval, SelfInst, VicunaEval, S-NI, and UnNI. Rouge-L score is the main metric for evaluating all models. It can measure the precision of the model's generation. 

Our method is compared to previous method in Fig.\ref{method_pic}.


% \begin{table}[!htp]\centering
% \begin{tabular}{lrrr}\toprule
% &MiniLLM &MiniLLM+FD+Projector &MiniLLM+FD+Self\_Correlation &MiniLLM+V_kD &MiniLLM+SHD \\\midrule
% Training speed(s/iter) &1.41s &1.85s &1.49s & 1.61s &1.41s \\
% \bottomrule
% \end{tabular}

% \end{table}

\begin{table}[!htp]\centering
\begin{tabular}{lrrr}\toprule
&Training Speed &Params \\\midrule
MiniLLM &1.41s &340M \\
MiniLLM+FD+Projector &1.69s &394M \\
MiniLLM+FD+Self\_corr &1.49s &340M \\
MiniLLM+VkD &1.55s &344M \\
MiniLLM+SHD &\textbf{1.41s} &\textbf{340M} \\
\bottomrule
\end{tabular}
\caption{Training time cost of SHD and other FD-based methods with GPT2-Medium.}\label{tab: speed test}
\end{table}

\textbf{Loss function selection.} We did ablation studies over loss function in Tab.\ref{tab:loss selection}. KL performs better than MSE. We believe this is predictable since the same phenomenon happens in traditional logits supervision. And the performance always gets improved for whatever loss function is, proving SHD's positive impact.

\begin{table}[!htp]\centering
\begin{tabular}{lrrrrr}\toprule
Loss Function &\textbf{FID} &\textbf{IS} &\textbf{precision} &\textbf{Recall} \\\midrule
KL &36.95 &46.27 &0.5142 &0.4843 \\
MSE &38.04 &44.34 &0.5048 &0.4921 \\
\bottomrule
\end{tabular}
\caption{Comparison on different loss functions.}\label{tab:loss selection}
\end{table}

\begin{table*}[!htb]\centering

\begin{tabular}{lrrrrrr}\toprule
Method &DollyEval &SelfInst &VincunaEval &S-NI &UnNI \\\midrule
MiniLLM &25.4 &14.6 &\textbf{17.7} &27.4 &31.3 \\
MiniLLM+constant merge &25.5 &15.0 &\textbf{17.7} &27.1 &31.5 \\
MiniLLM+SHD &\textbf{26.2} &\textbf{15.2} &\textbf{17.7} &\textbf{28.1} &\textbf{32.2} \\
\bottomrule
\end{tabular}
\caption{Comparison of merging heads. "Constant merge" indicates a combination of the teacher's attention maps via different heads in a still constant manner. Our method is sample-wised.}\label{tab: comparison merging heads}
\end{table*}
\end{document}


\textbf{Training speed of SHD.} Our method only computes the $\alpha$ in a batch-wise manner during training, which has a minor impact on the training speed of the baselines. We measured the training speed of MiniLLM and MiniLLM with SHD in Tab.\ref{tab: speed test}. We used 16 NVIDIA V100 GPUs to test the time. The time influence of SHD is negligible. Comparing to extra training parameters and extra training time like FD or VkD, the improvement of SHD is basically free.


\textbf{Distribution of $\alpha$ in SHD.} We sampled all training data (12.5k samples) from the Dolly dataset and calculate all $\alpha$ by $GPT2-XL$ to squeeze to 16 heads from 25 heads in total. The visualization of the $\alpha$ is in Fig.\ref{alpha visualization}. We can show that $\alpha$ always falls in the range of [0,1] for a pre-trained teacher model, making our method very stable in training, since it cannot provide any negative supervision of the attention maps. The distribution varies among different layers. Some certain heads like "Head 8" focus on one head in shallower layers and the other in deeper layers, indicating that our method can distinguish which head contains more useful information after squeezing and dynamically change the way of merging among layers.

\textbf{Complexity of SHD.} As illustrated in our method section, the complexity of SHD is similar to the attention mechanism, leading to a $O(N^2)$ complexity. Most intermediate computation are already done within original multi-head attention.
\textbf{Comparison with Projector-based Distillation.} The feature projectors in the knowledge distillation of image classification are quite lightweight since most FD methods only focus on a single layer of features in the backbone network (mostly features of the last layer before the classification head) due to the limitation of computational costs and performance. We also want to mention that most FD methods experimented on CNN-CNN/CNN-ViT teacher-student pairs in the image classification or detection tasks. When it comes to generative models, some research \cite{sun2019patient} has suggested that the supervision of FD from each layer is a better solution in the training. Our ablation studies between FD and SHD are computed on all layers of the student model. The training parameters and the training speed of FD, VkD[2], and SHD are reported and added to our new manuscript.


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
