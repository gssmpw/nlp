\section{Introduction}

%% What is the problem?
%% introduce PWS
%% label functions in PWS are hard to design
%% create label function manually is time-consuming and costly.

Access to substantial amounts of high-quality labeled data is a key ingredient for training performant machine learning models. Such data is usually produced by asking domain experts for ground-truth labels, making the process of dataset creation expensive, slow, and hard to scale. Programmatic weak supervision (PWS), a novel paradigm for generating labeled data \cite{ratner2016data}, sidesteps these obstacles. The idea behind PWS is to leverage a combination of noisy label estimates obtained from domain knowledge, heuristic rules, and pattern matching. These sources act as noisy labeling functions (LFs), usually expressed as \textbf{code}. The outputs of these labeling functions are modeled and aggregated to annotate unlabeled data points \cite{ratner2016data, ratner2017snorkel, ratner2019training, fu2020fast}. 

PWS has proven successful \cite{bach2019snorkel, evensen2020ruler, li2021weakly, gao2022classifying} but remains expensive: users must painstakingly write small programs to act as LFs. Users, even domain experts, often need tedious experimentation to carefully set up proper thresholds, manually fine-tune heuristic rules to capture enough keywords, or debug regular expressions. %It is also  challenging for those without domain knowledge to discover sufficient insights and helpful rules to design LFs for specific scenarios.
%
%In recent years, to reduce the effort required in handcrafting LFs, 
To tackle these challenges, recent approaches automatically produce %, which aim at automating the 
LFs by using a minimal level of supervision (i.e. a few labeled data points) \cite{varma2018snuba, das2020goggles, inproceedings, boecking2021interactive, roberts2022autowsbench} or access to powerful external models (like large language models) to prompt data labels \cite{smith2022language}. However, these approaches do not yield programmatic LFs, but rather model-generated noisy label estimates, and so lose the ability to debug and transfer, a key advantage of programmatic weak supervision.

A best-of-both worlds approach is to have \textbf{code-generation models write labeling functions}. This neither requires domain experts to write code nor sacrifices the programmatic property of LFs. 
%
Indeed, such an approach is now plausible given advances in models that produce code, such as CodeT5 \cite{wang2021codet5}, Codex \cite{chen2021evaluating}, and CodeGen \cite{Nijkamp2022CG}). 
%
Among other benefits, LFs generated by such models can be edited and used as templates, providing programming assistance for users to design LFs more easily and efficiently. 
%
Additionally, unlike human-designed LFs, synthesized LFs can be generated in large quantities. 
%
Finally, in contrast to using large language models to obtain the noisy labels estimates via prompting, which requires repeated inference calls, synthesized LFs can be stored and reused to label new data at zero cost. %With synthesized LFs, users do not need domain expertise to design LFs. Writing prompts becomes much easier for users to leverage rich knowledge pre-trained on code into the LF generation.

%% Why is it hard? (E.g., why do naive approaches fail?)
%% prompt is sensitive, hard to control the quality of LFs, no one knows the trade-off, what's the correct way to leverage LFs into PWS
%% Codex is not open-source, leaving many questions to design the correct format to compose a prompt to generate high-quality label functions.
%% it is still unclear about the proper way to bring benefits of code LLMs into data labeling. 

However, it is unclear whether code generation models can produce sufficiently high-quality LFs, and, when it is possible, what approach to take in order to do so. We ask the following fundamental questions we aim to answer in this work:
\begin{enumerate}
    \item \textbf{Prompt format}: Prompts are highly sensitive. Small changes to prompt components lead to great variation in generated results. There is currently no consensus on the best way to generally prompt code-generation models, let alone specifically for labeling functions. Our first question is: what prompting strategy can yield high-quality LFs?
    \item \textbf{Capability of synthesized LFs}: Next we ask: compared to human-designed LFs, what are the strengths and weaknesses of synthesized LFs? Additionally, what is the typical result when using these synthesized programs in programmatic weak supervision pipelines?
    \item \textbf{In-context few-shot settings}: If we are allowed to include some heuristic rules or give several data examples into the prompt context, does this better guide the model in synthesizing high-quality LFs? What type of in-context information can add and influence the quality of synthesized LFs?
\end{enumerate}

%% Why is it interesting and important?
%% Foundation models can be powerful knowledge source

We answer these questions and use the resulting insights to build a novel programmatic weak supervision system called \textbf{ScriptoriumWS}. A high-level view of ScriptoriumWS is illustrated in Figure \ref{fig:framework}. The system creates LFs by prompting code-generation models to synthesize programs and incorporates them into PWS pipelines. To validate ScriptoriumWS, we conduct experiments with OpenAI Codex \cite{chen2021evaluating}, a state-of-the-art natural language-to-code system based on GPT-3 \cite{brown2020language}. We further propose a complementary approach to incorporate the strength of synthesized and human-designed LFs to improve the performance of the end model.


With the aid of ScriptoriumWS, we explore the advantages that synthesized LFs can bring to the weak supervision framework. 
%
We study various prompting strategies to gain insight into how to best generate high-quality LFs. 
%
We conduct experiments in diverse text domains and empirically demonstrate the effectiveness of ScriptoriumWS. 
%
Excitingly, we find that compared to the human-designed LFs in WRENCH, LFs generated using ScriptoriumWS achieve much higher coverage (the fraction of data points that receive labels) while maintaining high accuracy. 
%
For example, using the WRENCH benchmark \cite{zhang2021wrench} for comparison, we improve the coverage for the SMS dataset from 40.5\% to 100\% and for the Spouse dataset from 25.8\% to 100\%, while also improving downstream performance by 4.0\% and 5.0\% F1 points, respectively. 

\begin{figure}[t!]
    \includegraphics[width=\linewidth]{figures/framework.png}
    \centering
    \caption{\small Overview of the proposed ScriptoriumWS system. Code generation models are prompted to produce small programs that act as weak supervision labeling functions. These are used within a weak supervision pipeline to label an unlabeled dataset. A downstream end model is trained on the labeled data.}
    \label{fig:framework}
\end{figure}