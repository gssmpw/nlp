\section{Experiments}
In this section, we validate the capability of the proposed system. 
We implement ScriptoriumWS on the top of weak supervision pipeline proposed as part of the WRENCH benchmark \cite{zhang2021wrench} and use synthesized LFs to generate weak labels to learn the label model and then subsequently the end model.

\subsection{Setup}

\paragraph{Datasets}
We evaluate our approach using four different types of text tasks involving a set of 6 datasets originally included in WRENCH. These 6 datasets are the IMDb \cite{ren2020denoising} and Yelp \cite{ren2020denoising} datasets for sentiment classification, the YouTube \cite{alberto2015tubespam} and SMS \cite{almeida2011contributions} datasets for spam classification, the AGNews \cite{ren2020denoising} dataset for topic classification, and the Spouse \cite{ratner2017snorkel} dataset for relation classification. 

\paragraph{Label Model \& End Model}
%% Label models: Snorkel, WMV, MV, DS, and FS.
%% End model: logistic regression.
Our system is compatible with any choice of label and end model.
%
For ease of comparison, we follow WRENCH and evaluate with five label models to aggregate the output of our synthesized LFs: majority vote (MV), weighted majority vote (WMV), Snorkel \citep{ratner2017snorkel}, Dawid-Skene (DS) \cite{Dawid:Skene:79}, and FlyingSquid (FS) \cite{threerius}. 
Finally, once we generate labeled training datasets using these label models alongside our LFs, we train a downstream model---for the sake of simplicity, we use logistic regression as end model for all tasks. 

\subsection{Analysis}
We use our evaluation platform to validate the following claims:

\begin{table}[t!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}l|ccccc|ccccc@{}}
\toprule
 & \multicolumn{5}{c|}{\textbf{IMDb}} & \multicolumn{5}{c}{\textbf{Yelp}} \\ \cmidrule(l){2-11} 
 & \#LFs & \begin{tabular}[c]{@{}c@{}}Avg. \\ Coverage\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\ Overlap\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\ Conflict\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\ Accuracy\end{tabular} & \#LFs & \begin{tabular}[c]{@{}c@{}}Avg. \\ Coverage\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\ Overlap\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\ Conflict\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\ Accuracy\end{tabular} \\ \midrule
WRENCH & 5 & 0.236 & 0.116 & 0.045 & 0.699 & 8 & 0.183 & 0.136 & 0.049 & 0.731 \\ \midrule
General Prompt & 6 & 0.894 & 0.887 & 0.331 & 0.595 & 11 & 0.716 & 0.716 & 0.213 & 0.736 \\
+ Mission Statement & 5 & 0.780 & 0.766 & 0.609 & 0.568 & 7 & 0.697 & 0.689 & 0.168 & 0.686 \\
+ Human Heuristic & 6 & 0.764 & 0.758 & 0.596 & 0.644 & 5 & 0.783 & 0.774 & 0.088 & 0.658 \\
+ Labeling Function Exemplars & 5 & 0.805 & 0.792 & 0.133 & 0.593 & 5 & 0.814 & 0.812 & 0.258 & 0.690 \\
+ Data Exemplars & 5 & 0.895 & 0.895 & 0.382 & 0.633 & 6 & 0.701 & 0.689 & 0.109 & 0.702 \\ \midrule
 & \multicolumn{5}{c|}{\textbf{SMS}} & \multicolumn{5}{c}{\textbf{YouTube}} \\ \cmidrule(l){2-11} 
 & \#LFs & \begin{tabular}[c]{@{}c@{}}Avg. \\ Coverage\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\ Overlap\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\ Conflict\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\ Accuracy\end{tabular} & \#LFs & \begin{tabular}[c]{@{}c@{}}Avg. \\ Coverage\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\ Overlap\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\ Conflict\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\ Accuracy\end{tabular} \\ \midrule
WRENCH & 73 & 0.007 & 0.003 & 0.000 & 0.973 & 10 & 0.170 & 0.132 & 0.075 & 0.826 \\ \midrule
General Prompt & 8 & 0.815 & 0.815 & 0.260 & 0.897 & 9 & 0.592 & 0.592 & 0.493 & 0.646 \\
+ Mission Statement & 8 & 0.819 & 0.819 & 0.324 & 0.817 & 9 & 0.643 & 0.643 & 0.602 & 0.607 \\
+ Human Heuristic & 9 & 0.741 & 0.741 & 0.118 & 0.821 & 8 & 0.570 & 0.570 & 0.491 & 0.802 \\
+ Labeling Function Exemplars & 8 & 0.038 & 0.014 & 0.001 & 0.822 & 6 & 0.662 & 0.662 & 0.349 & 0.795 \\
+ Data Exemplars & 8 & 0.612 & 0.612 & 0.366 & 0.749 & 8 & 0.534 & 0.534 & 0.397 & 0.793 \\ \midrule
 & \multicolumn{5}{c|}{\textbf{Spouse}} & \multicolumn{5}{c}{\textbf{AGNews}} \\ \cmidrule(l){2-11} 
 & \#LFs & \begin{tabular}[c]{@{}c@{}}Avg. \\ Coverage\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\ Overlap\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\ Conflict\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\ Accuracy\end{tabular} & \#LFs & \begin{tabular}[c]{@{}c@{}}Avg. \\ Coverage\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\ Overlap\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\ Conflict\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\ Accuracy\end{tabular} \\ \midrule
WRENCH & 9 & 0.042 & 0.021 & 0.009 & 0.586 & 9 & 0.103 & 0.051 & 0.024 & 0.817 \\ \midrule
General Prompt & 8 & 1.000 & 1.000 & 0.324 & 0.807 & 8 & 0.305 & 0.279 & 0.080 & 0.565 \\
+ Mission Statement & 9 & 0.279 & 0.208 & 0.168 & 0.404 & 4 & 0.373 & 0.215 & 0.123 & 0.338 \\
+ Human Heuristic & 8 & 0.295 & 0.264 & 0.050 & 0.456 & 4 & 0.346 & 0.327 & 0.064 & 0.818 \\
+ Labeling Function Exemplars & 5 & 0.417 & 0.307 & 0.023 & 0.444 & 8 & 0.481 & 0.472 & 0.191 & 0.530 \\
+ Data Exemplars & 8 & 0.601 & 0.601 & 0.240 & 0.595 & 5 & 0.345 & 0.244 & 0.107 & 0.636 \\ \bottomrule
\end{tabular}}
\centering\caption{Statistics of synthesized LFs.}
\label{basic stats}
\end{table}


%\paragraph{How effective are our synthesized LFs?}
\paragraph{Are ScriptoriumWS LFs comparable to human-designed LFs?}
%% performance is comparable to humanLF
%% show LF performance: accuracy, coverage, overlap, conflict (table)
%To validate the effectiveness of ScriptoriumWS, 
We hypothesize that the synthesized LFs generated by ScriptoriumWS can provide results that are comparable to human-designed LFs. 
To see the strengths and weaknesses of synthesized LFs, we include four basic measurements for LFs generated by different prompting strategies.
These are coverage, overlap, conflict, and accuracy. 
Coverage is the fraction of the dataset labeled by a given LF. 
Overlap shows the fraction of the dataset with at least two (non-abstain) labels. 
Conflict indicates a data example for which at least one other LF provides a different estimate. 
Accuracy computes the fraction of the correctly labeled dataset. 
We take the average of these indicators over our synthesized LFs and compare them with human-designed LFs in WRENCH. 

The results are shown in Table \ref{basic stats}. 
They demonstrate that ScriptoriumWS is capable of generating LFs that have comparable accuracy to human-designed LFs. 
We observe that synthesized LFs significantly outperform human-designed LFs in terms of coverage. This is not surprising, as human-crafted LFs are often very specific and cannot cover too much of the dataset.  
On the other hand, we see that there exist more conflicts among outputs produced by synthesized LFs. 
However, this is not a concern, as such conflicts are resolved by (and in fact, are useful to learn) the label model.


\begin{table}[t!]
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}l|cccccccccccccc@{}}
\toprule
 & \multicolumn{7}{c|}{\textbf{IMDb (Accuracy)}} & \multicolumn{7}{c}{\textbf{Yelp (Accuracy)}} \\ \cmidrule(l){2-15} 
 & Snorkel & WMV & MV & DS & FS & Avgerage & \multicolumn{1}{c|}{Coverage} & Snorkel & WMV & MV & DS & FS & Avgerage & Coverage \\ \midrule
WRENCH & 0.701 & 0.710 & 0.710 & 0.706 & 0.704 & 0.706 & \multicolumn{1}{c|}{0.876} & 0.690 & 0.685 & 0.702 & 0.715 & 0.687 & 0.696 & 0.828 \\ \midrule
General Prompt & 0.661 & 0.587 & 0.587 & 0.559 & 0.606 & 0.600 & \multicolumn{1}{c|}{\textbf{0.998}} & 0.766 & 0.693 & 0.703 & 0.748 & 0.700 & \textbf{0.722} & \textbf{0.991} \\
+ Mission Statement & 0.613 & 0.600 & 0.600 & 0.542 & 0.612 & 0.593 & \multicolumn{1}{c|}{\textbf{0.973}} & 0.743 & 0.665 & 0.675 & 0.634 & 0.670 & 0.677 & \textbf{0.988} \\
+ Human Heuristic & 0.710 & 0.652 & 0.652 & 0.588 & 0.649 & 0.650 & \multicolumn{1}{c|}{\textbf{0.985}} & 0.661 & 0.635 & 0.642 & 0.695 & 0.610 & 0.648 & \textbf{0.955} \\
+ Labeling Function Exemplars & 0.650 & 0.614 & 0.614 & 0.596 & 0.612 & 0.617 & \multicolumn{1}{c|}{\textbf{0.941}} & 0.793 & 0.670 & 0.692 & 0.728 & 0.729 & \textbf{0.722} & \textbf{0.994} \\
+ Data Exemplars & 0.713 & 0.676 & 0.676 & 0.690 & 0.698 & \textbf{0.691} & \multicolumn{1}{c|}{\textbf{1.000}} & 0.766 & 0.678 & 0.688 & 0.736 & 0.685 & \textbf{0.711} & \textbf{0.990} \\ \midrule
 & \multicolumn{7}{c}{\textbf{SMS (F1-score)}} & \multicolumn{7}{c}{\textbf{YouTube (Accuracy)}} \\ \cmidrule(l){2-15} 
 & Snorkel & WMV & MV & DS & FS & Avgerage & Coverage & Snorkel & WMV & MV & DS & FS & Avgerage & Coverage \\ \midrule
WRENCH & 0.048 & 0.240 & 0.240 & 0.049 & 0.000 & 0.115 & \multicolumn{1}{c|}{0.405} & 0.852 & 0.780 & 0.840 & 0.832 & 0.764 & 0.814 & 0.893 \\ \midrule
General Prompt & 0.632 & 0.526 & 0.672 & 0.622 & 0.632 & \textbf{0.617} & \multicolumn{1}{c|}{\textbf{1.000}} & 0.760 & 0.700 & 0.724 & 0.668 & 0.784 & 0.727 & \textbf{1.000} \\
+ Mission Statement & 0.599 & 0.029 & 0.615 & 0.599 & 0.599 & \textbf{0.488} & \multicolumn{1}{c|}{\textbf{1.000}} & 0.540 & 0.624 & 0.648 & 0.688 & 0.468 & 0.594 & \textbf{1.000} \\
+ Human Heuristic & 0.606 & 0.412 & 0.554 & 0.529 & 0.536 & \textbf{0.527} & \multicolumn{1}{c|}{\textbf{1.000}} & 0.556 & 0.740 & 0.748 & 0.748 & 0.776 & 0.714 & \textbf{1.000} \\
+ Labeling Function Exemplars & 0.086 & 0.317 & 0.317 & 0.237 & 0.027 & \textbf{0.197} & \multicolumn{1}{c|}{0.218} & 0.740 & 0.740 & 0.740 & 0.748 & 0.740 & 0.742 & \textbf{1.000} \\
+ Data Exemplars & 0.650 & 0.337 & 0.628 & 0.640 & 0.630 & \textbf{0.577} & \multicolumn{1}{c|}{\textbf{1.000}} & 0.888 & 0.844 & 0.868 & 0.728 & 0.888 & \textbf{0.843} & \textbf{1.000} \\ \midrule
 & \multicolumn{7}{c|}{\textbf{Spouse (F1-score)}} & \multicolumn{7}{c}{\textbf{AGNews (Accuracy)}} \\ \cmidrule(l){2-15} 
 & Snorkel & WMV & MV & DS & FS & Avgerage & \multicolumn{1}{c|}{Coverage} & Snorkel & WMV & MV & DS & FS & Avgerage & Coverage \\ \midrule
WRENCH & 0.498 & 0.205 & 0.208 & 0.155 & 0.343 & 0.282 & \multicolumn{1}{c|}{0.258} & 0.625 & 0.640 & 0.638 & 0.628 & 0.610 & 0.628 & 0.691 \\ \midrule
General Prompt & 0.395 & 0.090 & 0.387 & 0.382 & 0.374 & \textbf{0.325} & \multicolumn{1}{c|}{\textbf{1.000}} & 0.537 & 0.530 & 0.529 & 0.410 & 0.544 & 0.510 & \textbf{0.692} \\
+ Mission Statement & 0.381 & 0.173 & 0.355 & 0.399 & 0.345 & \textbf{0.331} & \multicolumn{1}{c|}{\textbf{1.000}} & 0.397 & 0.393 & 0.347 & 0.372 & 0.372 & 0.376 & \textbf{1.000} \\
+ Human Heuristic & 0.393 & 0.204 & 0.243 & 0.391 & 0.340 & \textbf{0.315} & \multicolumn{1}{c|}{\textbf{0.470}} & 0.597 & 0.580 & 0.572 & 0.536 & 0.597 & \textbf{0.576} & 0.667 \\
+ Labeling Function Exemplars & 0.394 & 0.172 & 0.169 & 0.165 & 0.287 & 0.237 & \multicolumn{1}{c|}{\textbf{1.000}} & 0.544 & 0.527 & 0.525 & 0.485 & 0.525 & 0.521 & \textbf{0.811} \\
+ Data Exemplars & 0.395 & 0.134 & 0.378 & 0.389 & 0.383 & \textbf{0.336} & \multicolumn{1}{c|}{\textbf{1.000}} & 0.477 & 0.458 & 0.421 & 0.404 & 0.471 & 0.446 & \textbf{1.000} \\ \bottomrule
\end{tabular}}
\centering\caption{Performance of label models across different type of prompting strategies.}
\label{label model perf}
\end{table}


%\paragraph{How does synthesized LFs perform on the label model?}
\paragraph{How does ScriptoriumWS perform in PWS pipelines?}
%Previously, we validate that ScriptoriumWS has the potential to be a useful tool for reducing the human effort required in designing LFs. 
%Next, we expect to see the capability of synthesized LFs in the PWS pipelines. 
We anticipate that as LFs from ScriptoriumWS are comparable to human-designed LFs, such LFs will yield good performance in downstream tasks.  
We train the label model to aggregate the outputs of synthesized LFs and evaluate the performance of the label model on the testing dataset.
We compute model performance across different label models for each type of prompting strategy.

The results are shown in Table \ref{label model perf}. We find that the performance of the label model using synthesized LFs is generally on par with that of the label model using human-designed LFs while achieving much higher coverage. 
In particular, coverage on the SMS and Spouse datasets are low when using human-designed LFs from WRENCH; however, when using our synthesized LFs, \textit{we achieve $100\%$ coverage while also achieving higher F1-scores.} 
These results suggest that synthesized LFs can be a valuable resource for PWS pipelines and provide strong evidence for the efficacy of ScriptoriumWS in practical applications. 


\begin{table}[h!]
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}l|cccccccc|cccccccc@{}}
\toprule
 & \multicolumn{8}{c|}{\textbf{IMDb (Accuracy)}} & \multicolumn{8}{c}{\textbf{Yelp (Accuracy)}} \\ \cmidrule(l){2-17} 
 & \#LFs & \begin{tabular}[c]{@{}c@{}}Snorkel \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}WMV \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}MV \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}DS \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}FS \\ + LR\end{tabular} & Average & Coverage & \#LFs & \begin{tabular}[c]{@{}c@{}}Snorkel \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}WMV \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}MV \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}DS \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}FS \\ + LR\end{tabular} & Average & Coverage \\ \midrule
WRENCH & 5 & 0.758 & 0.754 & 0.754 & 0.754 & 0.756 & 0.755 & 0.876 & 8 & 0.722 & 0.649 & 0.694 & 0.807 & 0.737 & 0.722 & 0.828 \\
+ General Prompt & +6 & 0.740 & 0.737 & 0.742 & 0.767 & 0.739 & 0.745 & \textbf{1.000} & +11 & 0.750 & 0.671 & 0.704 & 0.801 & 0.730 & \textbf{0.731} & \textbf{1.000} \\
+ Mission Statement & +5 & 0.732 & 0.756 & 0.761 & 0.767 & 0.747 & 0.753 & \textbf{1.000} & +7 & 0.734 & 0.660 & 0.693 & 0.815 & 0.753 & \textbf{0.731} & \textbf{1.000} \\
+ Human Heuristic & +6 & 0.763 & 0.769 & 0.771 & 0.785 & 0.757 & \textbf{0.769} & \textbf{1.000} & +5 & 0.680 & 0.619 & 0.661 & 0.804 & 0.703 & 0.693 & \textbf{1.000} \\
+ Labeling Function Exemplars & +5 & 0.737 & 0.746 & 0.750 & 0.786 & 0.735 & 0.751 & \textbf{1.000} & +5 & 0.656 & 0.664 & 0.706 & 0.808 & 0.711 & 0.709 & \textbf{1.000} \\
+ Data Exemplars & +5 & 0.770 & 0.752 & 0.757 & 0.758 & 0.767 & \textbf{0.761} & \textbf{1.000} & +6 & 0.724 & 0.656 & 0.705 & 0.821 & 0.748 & \textbf{0.731} & \textbf{1.000} \\ \midrule
 & \multicolumn{8}{c|}{\textbf{SMS (F1-score)}} & \multicolumn{8}{c}{\textbf{YouTube (Accuracy)}} \\ \cmidrule(l){2-17} 
 & \#LFs & \begin{tabular}[c]{@{}c@{}}Snorkel \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}WMV \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}MV \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}DS \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}FS \\ + LR\end{tabular} & Average & Coverage & \#LFs & \begin{tabular}[c]{@{}c@{}}Snorkel \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}WMV \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}MV \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}DS \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}FS \\ + LR\end{tabular} & Average & Coverage \\ \midrule
WRENCH & 73 & 0.678 & 0.772 & 0.756 & 0.750 & 0.057 & 0.603 & 0.405 & 10 & 0.808 & 0.732 & 0.808 & 0.828 & 0.788 & 0.793 & 0.893 \\
+ General Prompt & +8 & 0.720 & 0.542 & 0.750 & 0.709 & 0.473 & \textbf{0.639} & \textbf{1.000} & +9 & 0.832 & 0.740 & 0.788 & 0.820 & 0.756 & 0.787 & \textbf{1.000} \\
+ Mission Statement & +8 & 0.619 & 0.405 & 0.672 & 0.576 & 0.420 & 0.538 & \textbf{1.000} & +9 & 0.820 & 0.732 & 0.756 & 0.784 & 0.716 & 0.762 & \textbf{1.000} \\
+ Human Heuristic & +9 & 0.632 & 0.476 & 0.582 & 0.594 & 0.482 & 0.553 & \textbf{1.000} & +8 & 0.808 & 0.756 & 0.808 & 0.816 & 0.772 & \textbf{0.792} & \textbf{1.000} \\
+ Labeling Function Exemplars & +8 & 0.405 & 0.465 & 0.473 & 0.610 & 0.029 & 0.396 & \textbf{1.000} & +6 & 0.776 & 0.756 & 0.780 & 0.796 & 0.780 & 0.778 & \textbf{1.000} \\
+ Data Exemplars & +8 & 0.692 & 0.418 & 0.746 & 0.722 & 0.509 & \textbf{0.617} & \textbf{1.000} & +8 & 0.800 & 0.756 & 0.800 & 0.780 & 0.788 & 0.785 & \textbf{1.000} \\ \midrule
 & \multicolumn{8}{c|}{\textbf{Spouse (F1-score)}} & \multicolumn{8}{c}{\textbf{AGNews (Accuracy)}} \\ \cmidrule(l){2-17} 
 & \#LFs & \begin{tabular}[c]{@{}c@{}}Snorkel \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}WMV \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}MV \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}DS \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}FS \\ + LR\end{tabular} & Average & Coverage & \#LFs & \begin{tabular}[c]{@{}c@{}}Snorkel \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}WMV \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}MV \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}DS \\ + LR\end{tabular} & \begin{tabular}[c]{@{}c@{}}FS \\ + LR\end{tabular} & Average & Coverage \\ \midrule
WRENCH & 9 & 0.220 & 0.179 & 0.181 & 0.166 & 0.268 & 0.203 & 0.258 & 9 & 0.825 & 0.823 & 0.827 & 0.829 & 0.817 & 0.824 & 0.691 \\
+ General Prompt & +8 & 0.157 & 0.298 & 0.303 & 0.301 & 0.155 & \textbf{0.243} & \textbf{1.000} & +8 & 0.806 & 0.823 & 0.825 & 0.751 & 0.817 & 0.804 & \textbf{1.000} \\
+ Mission Statement & +9 & 0.101 & 0.308 & 0.301 & 0.314 & 0.195 & \textbf{0.244} & \textbf{1.000} & +4 & 0.684 & 0.713 & 0.714 & 0.726 & 0.719 & 0.711 & \textbf{1.000} \\
+ Human Heuristic & +8 & 0.058 & 0.213 & 0.218 & 0.299 & 0.104 & 0.178 & \textbf{1.000} & +4 & 0.813 & 0.811 & 0.812 & 0.766 & 0.806 & 0.802 & \textbf{1.000} \\
+ Labeling Function Exemplars & +5 & 0.147 & 0.152 & 0.154 & 0.148 & 0.093 & 0.139 & \textbf{1.000} & +8 & 0.784 & 0.797 & 0.795 & 0.794 & 0.790 & 0.792 & \textbf{1.000} \\
+ Data Exemplars & +8 & 0.192 & 0.301 & 0.300 & 0.308 & 0.164 & \textbf{0.253} & \textbf{1.000} & +5 & 0.711 & 0.726 & 0.725 & 0.712 & 0.737 & 0.722 & \textbf{1.000} \\ \bottomrule
\end{tabular}}
\centering\caption{Performance of end models across different type of prompting strategies.}
\label{end model perf}
\end{table}


\paragraph{How does prompting strategy affect performance?} 
Different prompting strategies can lead to LFs that are more or less aligned with the user's prior knowledge and expertise, which in turn can affect the quality of LFs and their performance in downstream pipelines. 
For instance, providing labeled data in the prompt can prime Codex with information about the relationships between the input features and the target labels. 
On the other hand, providing heuristic rules in the prompt can lead Codex to focus more on the user's prior knowledge. 
We initially hypothesized that these different prompting strategies would lead to a discernible pattern---some strategies would dominate in certain settings. 
However, in our experimental results shown in Table~\ref{label model perf}, suggest no such pattern, leading to an inconclusive result. 
%, and we find that 
%the `correct' prompting method remains unclear, as different prompting methods perform differently across datasets. 
%there is inconclusive finding to indicate which prompting strategy work the best. 
%Instead, the best prompting strategy may depend on the specific task and dataset at hand. 
It is important to carefully consider the goals and requirements of the task and choose a prompt that is suitable for the task. 
A deeper analysis that elucidates the successes and failure modes of each prompting strategy is required to evaluate our original hypothesis and, perhaps more broadly, to better understand the role of prompting in code generation. 
%Further experiments and evaluations are required to better understand an efficient way to choose the appropriate prompting strategy. 


%\paragraph{How can synthesized LFs to be used to improve the end model?}
\paragraph{Can end model performance be improved by combining ScriptoriumWS with PWS?}
%In the end model training, those data points uncovered by LFs may not be included. 
End models can only be trained on points that receive labels. 
Building on our observation that synthesized LFs offer high coverage, we hypothesize that end model performance can be improved over the standard PWS pipeline by simply including the examples that are labeled by ScriptoriumWS.
%
%Based on this idea, we propose a complementary approach to incorporate synthesized LFs and human-designed LFs. 
This yields a complementary approach that incorporates both our synthesized LFs for points that are not labeled by human-designed LFs, and the labels that were originally produced by human-designed LFs. 
%We make synthesized LFs to label those uncovered data points by the human-designed LFs and train label model on two types of outputs respectively. 
We train the end model on the union of these two sets. 
%Ultimately, we combine pseudo labels from two label models, and the end model is trained on this merged generated dataset.
%
%We anticipate observing improved coverage and performance.
In Table \ref{end model perf}, we show the performance of the end model when using this approach.
As before, the dataset is fully covered by this approach and the end-model performance improves due to the significant increase in labeled examples. 
%As expected, we find that the dataset can be fully covered, and the end model performance can be improved by leveraging the strength of synthesized LFs. 
%This finding justifies the notion that the synthesized LFs can be complementary to human-designed LFs, leading to improved end model performance. 
%These findings can be valuable for users who need to label large datasets but have limited resources for writing sufficient labeling function.
%With the aid of ScriptoriumWS, users can quickly and efficiently synthesize LFs to cover the uncovered data points to improve the performance of the end model.
This shows that ScriptoriumWS can be used complementarity with existing PWS pipelines, for which human-designed LFs have already been created to improve performance. 

