% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}
%\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{adjustbox}
\usepackage{colortbl}
\usepackage{listings}
\usepackage{svg}
\usepackage{float}

\newcommand{\uparrowc}{\color{teal}\uparrow}
\newcommand{\downarrowc}{\color{red}\downarrow}

\newcommand{\goodbox}[1]{\colorbox[rgb]{0.713,0.858,0.882}{#1}}
\newcommand{\badbox}[1]{\colorbox[rgb]{0.96,0.796,0.8}{#1}}
\newcommand{\Mod}[1]{\ (\mathrm{mod}\ #1)}

\let\svthefootnote\thefootnote
\newcommand\freefootnote[1]{%
  \let\thefootnote\relax%
  \footnotetext{#1}%
  \let\thefootnote\svthefootnote%
}

\definecolor{qblue}{RGB}{17,85,204}
\definecolor{corange}{RGB}{255,171,64}
\definecolor{torange}{RGB}{255,128,1}
\definecolor{tviolet}{RGB}{128, 0, 128}

\lstset{
  basicstyle=\small\ttfamily,
  breaklines=true,
  columns=flexible,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

%\title{Not All Queries Are Created Equal, But It Shouldn't Matter: \\FailSafe Long Context QA for Finance}

%\title{What could go wrong: FailSafe Long Context QA for Finance}

\title{Expect the Unexpected: \\ \textcolor{qblue}{Fail}\textcolor{torange}{Safe} Long Context \textcolor{tviolet}{QA} for Finance}

%\title{FailSafeQA: Quality Assurance Test for Long Context QA in Finance}

% Author information can be set in various styles:
% For several authors from the same institution:
\author{ Kiran Kamble\textsuperscript{$\dagger$}, Melisa Russak\textsuperscript{$\dagger$}, \\ {\bf Dmytro Mozolevskyi,  Muayad Ali, Mateusz Russak} \\ {\bf Waseem AlShikh}\\
        Writer, Inc \\ \texttt{\{kiran, melisa, ... waseem\}@writer.com}}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}\freefootnote{$\dagger$ These authors contributed equally. The order is determined by dice rolling.}
We propose a new long-context financial benchmark, \textcolor{qblue}{Fail}\textcolor{torange}{Safe}\textcolor{tviolet}{QA}, designed to test the robustness and context-awareness of LLMs against six variations in human-interface interactions in LLM-based query-answer systems within finance. We concentrate on two case studies: Query Failure and Context Failure. In the \textbf{Query Failure} scenario, we perturb the original query to vary in domain expertise, completeness, and linguistic accuracy. In the \textbf{Context Failure} case, we simulate the uploads of degraded, irrelevant, and empty documents. We employ the LLM-as-a-Judge methodology with Qwen2.5-72B-Instruct and use fine-grained rating criteria to define and calculate Robustness, Context Grounding, and Compliance scores for $24$ off-the-shelf models. The results suggest that although some models excel at mitigating input perturbations, they must balance robust answering with the ability to refrain from hallucinating. Notably, Palmyra-Fin-128k-Instruct, recognized as the most compliant model, maintained strong baseline performance but encountered challenges in sustaining robust predictions in $17\%$ of test cases. On the other hand, the most robust model, OpenAI o3-mini, fabricated information in $41\%$ of tested cases. The results demonstrate that even high-performing models have significant room for improvement and highlight the role of \textcolor{qblue}{Fail}\textcolor{torange}{Safe}\textcolor{tviolet}{QA} as a tool for developing LLMs optimized for dependability in financial applications. The dataset is available at: \url{https://huggingface.co/datasets/Writer/FailSafeQA}
%$<$anonymized URL$>$.
\end{abstract}

\section{Introduction}

As the domains of financial services and Large Language Models (LLMs) evolve at a rapid pace, it comes as no surprise that finance, with its growing need for new tools to uncover insights from data, is increasingly adopting newly emerged LLMs for this purpose \citep{10.1145/3604237.3626869, zhao2024revolutionizingfinancellmsoverview, maple2023airevolutionopportunitieschallenges}. These tools are later used with significant impact in critical areas such as risk analysis, customer service, and operational decisions. Despite warnings against over-reliance on LLM-based systems in financial domains, people increasingly depend on fully automated processes, driven by trust in automation and the complexity of managing vast amounts of data, such as long context window financial reports \citep{turing2023llmfinance, lee2004trust}. Adding to these concerns, recent research has shown, that models are very sensitive to subtle changes in prompt formatting \citep{lu-etal-2022-fantastically,sclar2024quantifying}. These findings highlight the need for robust measures to evaluate the risks associated with LLM dependence and establish criteria for differentiating between safe and unsafe models. Our approach extends beyond traditional financial benchmarks that focus solely on LLMs performance under ideal conditions \citep{xie2024finbenholisticfinancialbenchmark, liu2024findabenchbenchmarkingfinancialdata, islam2023financebenchnewbenchmarkfinancial, guo2023chatgptfinancialexpertevaluating, xie2023pixiulargelanguagemodel}. We have developed testing scenarios that more accurately mirror real-world interactions between users and query-answer systems (QA systems), ensuring the tool maintains reliability even when queries deviate from typical patterns or involve topics beyond the scope of the document. This approach is particularly crucial when the user has limited domain expertise or knowledge of the document contents.

\begin{figure*}[t]
  \includegraphics[width=\linewidth]{assets/failsafeqa_final.pdf}
    \caption{\textbf{\textcolor{qblue}{Fail}\textcolor{torange}{Safe}\textcolor{tviolet}{QA}: Robustness and Context Grounding Evaluation} We evaluate the resilience of an LLM-based QA system in two case studies: \textbf{Query Failure} and \textbf{Context Failure}. In the Query Failure scenario, we perturb the original query into three variants: containing spelling errors (\textcolor{corange}{Misspelled Query}), query-term form (\textcolor{corange}{Incomplete Query}), rephrased to exclude in-domain terminology (\textcolor{corange}{Out-of-Domain Query}). In the Context Failure case, we assume users can either fail to upload the document (\textcolor{qblue}{Missing Context}) , use degraged quality documents due to OCR (\textcolor{qblue}{OCRed Context}) or upload a document irrelevant to the query (\textcolor{qblue}{Irrelevant Context}). Robustness involves maintaining consistent model performance across perturbations (A)-(C) and (E), which preserve the intended meaning, while Context Grounding involves preventing hallucinations in scenarios (D) and (F).}
  \label{finessebench}

\end{figure*}


In response to these identified issues, we introduce a new long context benchmark, \textbf{\textcolor{qblue}{Fail}\textcolor{torange}{Safe}\textcolor{tviolet}{QA}}, which \textbf{evaluates LLM resilience against variations in human input within the financial sector} caused by varying domain expertise, query incompleteness, source irrelevance, and linguistic inaccuracies. Research has demonstrated that LLMs tend to overlook details or fabricate responses when processing long-context texts \citep{hsieh2024rulerwhatsrealcontext, liu2023lostmiddlelanguagemodels}. Consequently, we have chosen long 10-K annual reports as our primary text source. To simplify the judging process, we base it on the ground truth and supporting citations, ensuring that all answers can be sourced from a short, relevant citation from the document. This approach reduces the context length required during the judging phase, leading to quicker and more precise evaluations of accuracy and comprehensiveness.

% We selected 10k filings as the main source of context, which primarily involve extensive text windows where models are prone to overlook details or fabricate responses \citep{hsieh2024rulerwhatsrealcontext, liu2023lostmiddlelanguagemodels}.
%To address the problem of long contexts in the judging phase, we have designed our rating framework to rely only on the ground truth and supporting citations based on the ground truth and citations to asses accuracy and comprehensiveness in shorter contexts. The proposed evaluation pipeline can be easily extended beyond the financial domain.

% Our main contributions are as follows:
% \begin{itemize}
%     \item We design a long context benchmark with four perturbed variants of the query to measure robustness and resilience   extend HELM \citep{liang2023holistic} LLMs robustness definition for the financial domain by new 3 factors: domain expertise, source relevance and query completeness.
%     \item  We introduce a per-query 6-point Likert scale for fine-grained LLM evaluation of long context tasks in short context regime.
%     \item  Finally, we contribute a new benchmark FailSafeQA, which measures both accuracy and robustness of LLMs in financial domain.
% \end{itemize}

\section{\textcolor{qblue}{Fail}\textcolor{torange}{Safe}\textcolor{tviolet}{QA} Dataset}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{assets/verb_dobj_base_new.png}
  \caption{\textbf{The Dataset} Analysis of root verbs and their direct objects from the first sentence of each normalized query shows the top 20 verbs and their top five direct objects\protect\footnotemark. This distribution can be used as a proxy measure for the diversity of tasks in the dataset, with 83.0\% related to question answering (QA) and 17.0\% involving text generation (TG).}
  \label{verb_dobj}
\end{figure}

We have used publicly available annual reports of U.S. publicly traded companies that filed with the SEC's EDGAR system during the years 1998, 1999, 2017, and 2018\footnote{\url{https://github.com/fengbinzhu/fintech-nlp}}. We utilized 10-K filings, which have been truncated to maintain complete paragraphs while adhering to a context window that does not exceed $25k$ tokens.

We employed the Meta Llama 3.1 405B model \citep{dubey2024llama3herdmodels} for synthetic task generation and postprocessing steps (\texttt{generate}, \texttt{rewrite}, \texttt{filter}) and LongCite-llama3.1-8b \citep{zhang2024longciteenablingllmsgenerate} for citation extraction (\texttt{extract}). Our semi-automated data generation pipeline consisted of three phases: query generation, query perturbation and context perturbation. 
\subsection{Query Generation}
This phase focuses on producing and refining queries generated from historical financial documents.
\begin{itemize}
\item (\texttt{generate}) Generate multi-turn query and answer pairs based on the truncated 10-K filings.
\item (\texttt{filter}) Identify the best standalone query from each interaction.
\item (\texttt{rewrite}) Standardize the queries to make each a clear, standalone question, intentionally removing courteous expressions which have been shown to affect results \citep{yin2024respectllmscrosslingualstudy}.
\item (\texttt{extract}) Extract and sanitize supporting citations from the full context for each query-answer pair (refer to: \autoref{sanit-cit}).
\item (\texttt{filter}) Retain only those data points for which the provided citations adequately support the query response.
\end{itemize}

% In the initial phase of \textbf{query generation}, we employed Meta Llama 3.1 405B to create multi-turn question and answer sequences from the truncated 10K filings. Then, we selected and postprocessed the best standalone question from each interaction. We used LongCite to extract supporting citation for each query answer pair and made sure for every datapoint the accompanying citations is sufficient to answer the query.
Prompts used for generating and filtering question-answer pairs can be found in \autoref{appendix-prompts}.

\subsection{Query Perturbation}
% In this phase we again used Meta Llama 3.1 405B to generate three variants of perturbed query: Mispelled, Incomplete and Out-of-Domain. These perturbations stand for four query failure scenarios: language accuracy (misspellings), failure in sticking to conversation format used in LLMs alignment (ie using query-term format like in search engines) and lack of in-domain expertise.

We used the Meta Llama 3.1 405B model to generate three types of query perturbations: Misspelled, Incomplete, and Out-of-Domain. These cases represent three key failure factors: language accuracy, search engine-style query phrasing, and lack of domain expertise.


% variations described in details in the next section. Each question was then manually reviewed for relevance, repeating the perturbation process to maintain the integrity and applicability of the queries.

% We examine four human risk factors represented by four query perturbations: misspellings, failure in sticking to conversation format used in LLMs alignment (ie using query-term format like in search engines), lack of in-domain expertise and inability to assess whether the information asked for is contained in the analysed document. We expect an LLM to mitigate those human factors by showing robustness with respect to query perturbation and avoiding hallucination whenever the query is unanswerable. 

\paragraph{Misspellings}
%The aim of this perturbation is to test the models' ability to comprehend and respond appropriately despite typical inaccuracies found in real-world user inputs. 
We introduced controlled spelling errors into financial queries using a rule-based approach. We generated four types of spelling errors: Split Errors where combined words were separated (e.g., "newspaper" to "news paper"), Segment Errors involving incorrect splitting or merging of words (e.g., "cat" to "c at" or "a cat" to "acat"), Realword Errors substituting words with similar-looking ones from a confusable list, and Common Typos sourced from Wikipedia’s List of Common Misspellings\footnote{\url{https://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings}} by reversing correct to incorrect spellings. These errors were distributed across the dataset with split errors making up 31.7\%, segment errors 25.5\%, realword substitutions 23.2\%, and common typos 19.6\%. 
% \begin{table*}[h]
% \small
%   \caption{\textbf{Query Perturbation} Misspelled Queries}
%   \label{perturbation-spell}
%   \centering
%   \begin{tabular}{p{0.2\linewidth} p{0.4\linewidth} p{0.4\linewidth}}
%     \toprule
%     \textbf{Error Type} & \textbf{Original Query} & \textbf{Perturbed Query} \\
%     \midrule
%     Common Typos & Compare the net interest income \goodbox{from} 2016 to 2017. &  Compare the net interest income \badbox{fomr} 2016 to 2017.\\
%     \midrule
%     Split & What are the \goodbox{conversion} rights of Preferred Units? &  What are the \badbox{con version} rights of Preferred Units?\\
%     \midrule
%     Segment & \goodbox{Turn} the company's revenue recognition policy into a paragraph. &  \badbox{Tu rn} the company's revenue recognition policy into a paragraph.\\
%     \midrule
%     Realword & Create \goodbox{an} HTML table for special gains and charges in 2017. & Create \badbox{nan} HTML table for especial gains and changes in 2017. \\
%     \bottomrule
%   \end{tabular}
% \end{table*}
\paragraph{Incomplete Queries} 
In this perturbation type, our focus is on query incompleteness, drawing inspiration from the key-term-based queries typical of search engines. Some of these queries resemble the original by appearing as if words have been intentionally omitted or rearranged. For instance, the query "What are the details of the capital conservation buffer mentioned in the K-10 filings?" is transformed into "Details on the capital conservation buffer mentioned?" We created these incomplete queries using Meta Llama 3.1 405B and manually chose the most effective transformations.

% \begin{table*}[h]
%  \small
%   \caption{\textbf{Query Perturbation} Incomplete Queries}
%   \label{perturbation-incomplete}
%   \centering
%   \begin{tabular}{p{0.5\linewidth} p{0.5\linewidth}}
%     \toprule
%     \textbf{Original Query} & \textbf{Perturbed Query} \\
%     \midrule
%     \goodbox{What is the} filing date for the 10-K report? &  filing date for the 10-K report?\\
%     \midrule
%     \goodbox{Extract the} numbers \goodbox{related to the} Nylon Segment\goodbox{'s performance in} 2018. &  numbers \badbox{for} Nylon Segment, 2018\badbox{?}\\
%     \midrule
%     \goodbox{What was the} aggregate market value \goodbox{of the} stock \goodbox{held by} non-affiliates \goodbox{on} June 30, 2018? & aggregate market value, stock, non-affiliates, June 30, 2018?\\
%     \midrule
%     \goodbox{Create an} HTML table for \goodbox{the information about} shares. & HTML table \badbox{for} shares \badbox{info?}\\
%     \bottomrule
%   \end{tabular}
% \end{table*}

\paragraph{Out-of-Domain}
The last category of query perturbation is inspired by the varying levels of expertise that users bring to a QA system. Ideally, whether a query is created by an in-domain expert or someone out-of-domain, it should lead to the same answer if the query is clear and targets the same information. The specific wording used should not impact the LLMs' performance, as the model should possess the necessary expertise to interpret user intent accurately. For example, "What is the primary reason for the revenue increase in 2017?" should be equivalent to "Why did the company make more money in 2017?"

% \begin{table*}[h]
% \small
%   \caption{\textbf{Query Perturbation} Out-of-Domain Queries}
%   \label{perturbation-out-of-domain}
%   \centering
%   \begin{tabular}{p{0.5\linewidth} p{0.5\linewidth}}
%     \toprule
%     \textbf{Original Query} & \textbf{Perturbed Query} \\
%     \midrule
%     \goodbox{Was the net income} for 2018 higher than that of 2017? & \badbox{Did we make more money} in 2018 than in 2017?\\
%     \midrule
%     Did First Bancorp \goodbox{have any acquisitions} in the year 2017? & Did First Bancorp \badbox{buy any other companies} in 2017? \\
%     \midrule
%     What was the market value of common stock held by \goodbox{non-affiliates} in mid-2017? & What was the value of the common stock \badbox{not owned by insiders} around the middle of 2017?\\
%     \midrule
%     What are the details on the \goodbox{lease payments} for the facility used by 3DX? & What does 3DX \badbox{pay for renting} their building?\\
%     \bottomrule
%   \end{tabular}
% \end{table*}



% \begin{table*}[h!]
% \small
%   \caption{\textbf{Query Perturbation} Out-of-Scope Queries}
%   \label{perturbation-out-of-scope}
%   \centering
%   \begin{tabular}{p{0.5\linewidth} p{0.5\linewidth}}
%     \toprule
%     \textbf{Original Query} & \textbf{Perturbed Query} \\
%     \midrule
%     Generate a short LinkedIn post about Alaska Communications' strategic focus. & \badbox{Generate a short note on Premier's financial} \badbox{performance in 2017.}\\
%     \midrule
%     Who is McEwen Mining's transfer agent in Canada? & \badbox{What was Oneida Financial Corp's dividend on} \badbox{Common Stock as decided in 1999?}\\
%     \midrule
%     What are Taco Cabana's leasing strategies for assets? & 	
% \badbox{Generate a LinkedIn post announcing the merger} \badbox{between Old Line Bancshares and BYBK.}\\
%     \midrule
%     Were there any legal penalties imposed on Hovnanian Enterprises in fiscal 2018? & \badbox{What are the responsibilities of the chairman of} \badbox{KinerjaPay?}\\
%     \bottomrule
%   \end{tabular}
% \end{table*}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width = \linewidth]{assets/qa_taxonomy.pdf}
%     %\vspace{-5pt}
%     %\vspace{-15pt}
%     \caption{\small \textbf{QA Taxonomy and Scoring} We introduce three QA task categories, each with distinct scoring criteria based on the complexity of the answer: Single-Factor QA involves retrieving a specific piece of information, such as a text span, typical of extractive tasks; Multi-Factor QA integrates multiple pieces of information and evaluates answers not only for correctness but also for their comprehensiveness; Structured Multi-Factor QA extends this by assembling information into a predefined format, like an HTML table or a Twitter post, with scoring also reflecting the accuracy of formatting. Each category progresses in complexity from basic data retrieval to detailed, structured presentation.}
%     \label{qa-taxonomy}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width = \linewidth]{assets/eval_vs_judging2.pdf}
%     %\vspace{-5pt}
%     %\vspace{-15pt}
%     \caption{\small \textbf{Evaluation and Judging} We use long context window (full document + query) for answer generation, but we use short context window (criteria + query + answer) for LLM-as-a-judge answer scoring. This strategy helps avoid performance issues typically seen in models handling long contexts, as LLMs generally achieve better results with shorter context windows.}
%     \label{qa-taxonomy}
% \end{figure*}

\subsection{Context Perturbation}
After exploring query perturbations, we now shift our focus to transforming another part of the input - the context, which in this case is the 10-K filing.

\paragraph{Missing Context}
We have simply omitted the context from the final prompt while maintaining the original prompt structure intended to introduce the context. The expected LLM response is to refrain from addressing the query directly and to notify the user that the context is unavailable, possibly due to reasons such as a file upload failure.

\paragraph{OCR Errors Simulation}
We have simulated Optical Character Recognition (OCR) errors to reflect the typical contract execution process where a clean, digital version of a contract is converted into a paper document for signatures. This paper-based version, necessitated by the legal requirement for wet ink signatures \citep{flsb2020signatures}, must then be converted back into digital form through scanning and OCR processing. This process introduces various inaccuracies into the text.

For OCR error generation, we used \citet{timmerman2023ocr}, which manipulates characters through deletions, replacements, and insertions based on probabilities derived from a normal distribution and a customizable character set. We have capped the upper limit on the character error probability at 10\%. This value was empirically chosen to reflect a balance between preserving readability and mimicking realistic error occurrences. An example of OCR-corrupted text is shown in \autoref{appendix-ocr}.

%We have set the upper limit on the error probability derived from a normal distribution by capping it to 10\%. This means that the maximum probability that any character in the text will undergo an erroneous transformation (such as deletion, replacement, or insertion) due to simulated OCR errors is capped at 10\%. This cap was empirically chosen to reflect a balance between preserving readability and mimicking realistic error occurrences.

% We divided the text into differently sized blocks to vary the error intensity and used customizable character sets for generating specific OCR-like errors . 

\paragraph{Irrelevant Context}

% The final type of perturbation we considered involves querying information that extends beyond the content available in the document. These context perturbations were generated by randomly pairing queries with contexts from other pairs and manually ensuring that these pairs were irrelevant to each other. An ideally performing LLM should recognize when the context lacks sufficient information to answer the query and refrain from fabricating an answer or relying on its general knowledge to respond. Additionally, the model should proactively inform the user that the provided context is irrelevant to the query, highlighting the specific mismatch and advising on the necessity of submitting a relevant document.
%Context perturbations test the model's ability to handle queries unrelated to the given document. 
We have randomly paired queries with irrelevant contexts and manually verified that these pairs were irrelevant to each other. An ideal LLM should acknowledge when the context is insufficient to answer the query, avoid fabricating responses or using general knowledge, and inform the user of the mismatch while suggesting the need for relevant documentation.

See \autoref{finessebench} for a visual summary of the different query and context perturbation scenarios discussed above.

% Examples of all query perturbations are presented in \autoref{finessebench}.

\begin{figure*}[h]
    \centering
    \includegraphics[width = \linewidth]
    {assets2/class_counts2.pdf}
    %\vspace{-5pt}
    %\vspace{-15pt}
    \caption{\small \textbf{Answer Relevance Classes} We evaluate two scenarios in our benchmark: when models should provide an answer (ANSWER QUERY) and when they must decline to answer (REFUSE QUERY) due to lack of relevant context. Our findings reveal that all the tested models are more adept at offering suitable answers than providing a justified refusal in situations where the context lacks sufficient information. Among all models evaluated, Palmyra-Fin-128k-Instruct demonstrates the most effective balance between these capabilities.}
    \label{class-counts}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width = \linewidth]{assets2/radar_chart_combined_new.pdf}
    %\vspace{-5pt}
    %\vspace{-15pt}
    \caption{\small \textbf{Robustness and Compliance} (Left) All models lose with respect to the baseline when input perturbations are applied. The biggest drop is observed for Out-Of-Domain and OCR context perturbations. Among the $24$ tested models, OpenAI o3-mini is the most robust. (Right) Reasoning models like OpenAI-o1/o3-mini and the DeepSeek-R1 series reach scores up to $0.59$, while Qwen models consistently surpass $0.60$. Palmyra-Fin-128k-Instruct excels with the highest Context Grounding score of $0.80$.}
    \label{rob-comp}
\end{figure*}

\subsection{Dataset Statistics}

The final dataset consists of $220$ examples, each originally containing between $4.1k$ and $27k$ tokens as processed by the GPT-4 tiktoken tokenizer\footnote{\url{https://github.com/openai/tiktoken}}. Notably, a large proportion ($93.64$\%) of examples feature a long context window, exceeding $16k$ tokens. This count is based on the original context before the injection of OCR errors, which can affect tokenization and increase the token count by approximately $1.3$ times.


Each data point includes a context paired with five questions (the original query, three perturbed variants, and an irrelevant query), an OCRed context, the ground truth answer, and supporting citations from the full context.
%Contexts are derived from annual reports of U.S. publicly traded companies that filed with the SEC's EDGAR system during the years 1998, 1999, 2017, and 2018. We used 10K filings, which have been reduced to full paragraphs to adhere to a context window of maximum $25k$ tokens. 

% Figure \ref{verb_dobj} shows root verb and direct object of the normalised query sentence for each data point, which we interpret as a proxy for the instructions variety of the dataset. Notably, the data generation prompt required an $80/20$ split between question answering and text generation tasks. After filtering and postprocessing, the final distribution closely adhered to this, with proportions of $83.0\%$ and $17.0\%$, respectively.

Figure \ref{verb_dobj} shows the root verb and direct object of the normalized query sentence for each data point, which we interpret as a proxy for the variety of instructions in the dataset\footnote{We utilized \texttt{SpaCy} 3.7.6 with \texttt{en\_core\_web\_sm} 3.7.1 model for verb-dobj analysis.}. The data generation prompt specified an $80/20$ split between question answering (QA) and text generation (TG) tasks. After filtering and postprocessing, the final distribution showed proportions of 83.0\% and 17.0\%, respectively, indicating the influence of the initial data generation requirements.




\section{Metrics}
% First we will follow \citet{xu2024fintruthqabenchmarkdatasetevaluating} and calculate the Answer Relevance Score for each answer of the tested model, which will evaluate the model performance under ideal scenario when the query is well formulated and the context is relevant. Then we will consider two case studies corresponding to two failed scenarios: inability to write good query (query failure) and inability to assess relevance and scope of the uploaded document (context failure).

\paragraph{Answer Relevance}
% Inspired by the pyramid approach \citep{zhang-bansal-2021-finding} for the summary evaluation, we used Llama 405B to extract key pieces of information from the reference answer and then turned them into fine-grained rating criteria for each query-answer pair.
% Following \citet{xu2024fintruthqabenchmarkdatasetevaluating}, we assign each answer a label from the set $\{1, 2, 3, 4, 5, 6\}$. We have designed the relevance labeling criteria so that the values $\{4, 5, 6\}$ denote answers that are relevant to the ground truth and free from hallucinations, varying only in their comprehensiveness. Answers with ratings $\{1, 2, 3\}$ either fail in terms of the accuracy of the information or contain irrelevant content. We will use that cut off property of the rating to define an auxiliary binary mapping that will define a \textit{complaint} answer in the next section. The detailed rating criteria are presented in \autoref{appendix-criteria}.

Following \citet{xu2024fintruthqabenchmarkdatasetevaluating}, we assign each answer a label from the set $\{1, 2, 3, 4, 5, 6\}$. We have designed the relevance labeling criteria such that the values $\{4, 5, 6\}$ denote answers that are relevant to the ground truth and free from hallucinations, varying only in their comprehensiveness. Answers with ratings $\{1, 2, 3\}$ either fail in terms of information accuracy or contain irrelevant content. We will use the cutoff property of the rating to define an auxiliary binary mapping that will determine a \textit{compliant} answer in the next section. Detailed rating criteria are presented in \autoref{appendix-criteria}.

% Query requiring simple text span extraction (ex. date, name, number) are labeled by [1, 6], whereas the full scale 1-6 is used for queries involving manipulation of multiple pieces of information and/or structuring the input (ex. HTML table, blog post). \textcolor{qblue}{Fail}\textcolor{torange}{Safe}\textcolor{tviolet}{QA} contains $33.0\%$ simple queries and $67.0\%$ complex queries, $10.4\%$ queries require structured output.

% Since the answer relevance is a categorical value, one cannot directly take an average of relevances of a subset of datapoints. In the following section we will define an auxiliary binary mapping to enable averaging.

% We note that the presence of the auxiliary metric $c_{\geq4}$ has an additional benefit of turning categorical data into binary form with statistically well defined average. 



\paragraph{Answer Compliance}
In our approach, a fail-safe QA system should never mislead a user by providing fabricated or irrelevant information. We will say that the answer is compliant if the relevance score is at least $4$. We define an auxiliary metric $c_{\geq4}$ that maps the relevance score into its binary compliance score:
\begin{equation}
c_{\geq4} = \begin{cases}
1 &\text{r $\geq 4$}\\
0 &\text{otherwise}
\end{cases}
\end{equation}
where $r \in \{1, 2, 3, 4, 5, 6\}$ is the original relevance label. 

We note that the mapping has another useful mathematical property: the Answer Relevance is a categorical value, so one cannot directly take an average of relevances. The Answer Compliance maps categorical data into a binary classification, where the average is well-defined. In subsequent sections, whenever aggregate scores are mentioned without specifying the metric, we will always refer to an average of Answer Compliances, i.e., the ratio of cases when the rating was at least $4$.
% provided by the tested model to evaluate its performance in an ideal scenario where the query is well-formulated and the context is relevant. The Answer Relevance criterion checks if the response directly addresses the query with clear, detailed explanations and whether it includes thorough and relevant information. 


% We have extended the original labelling methodology set and, in our case, answers are scored on a scale from 1 to 6, with available labels being [1, 2, 3, 4, 5, 6] or [1, 6] depending on the query complexity which we describe below. 

% For Context Grounding judging, where the correct answer is a refusal due to either missing or irrelevant context, we have the same criteria, where the acceptance point


% Unlike previous research studies [1], [2], [3] which employ a standard taxonomy for query classification and evaluation, we have adopted a simplified approach. In our methodology, we focus primarily on the number of pieces of information required (single or multiple/derivative) and whether the response needs to be structured. Consequently, our labeling system scores answers on a scale from 1 to 10, with available labels being [1, 2, 4, 6, 8, 10] for multi-factor queries and [1, 10] for single-factor queries, depending on the inherent complexity of the query.

% \paragraph{Single-Factor Queries (SFQ)}
% In this type, the answer required is a distinct single value such as a date, number, or name, which is explicitly mentioned within the context provided. Due to their simplicity, we evaluate the responses using a binary true or false system, which is then mapped to the labels 1 and 10 for scoring purposes.
% \paragraph{Multi-Factor Queries (MFQ)}
% These queries demand a response that contains a list or a combination of various pieces of information. The answer may involve explicitly extracted information or inferred details not directly stated, typically presented in an unstructured format like a paragraph or a sentence. For Multi-Factor Queries, a fine-grained scale (1, 2, 4, 6, 8, 10) is used to assess not only the accuracy but also the comprehensiveness of the information provided.
% \paragraph{Structured Multi-Factor Queries (SMFQ)}
% Similar to MFQ, these require a combination of multiple pieces of information. However, the response must be organized in a structured format, such as an HTML table or a structured blog post, leveraging the complex data interrelations within the provided context. For Structured Multi-Factor Queries, we use the same label set as MFQ (1, 2, 4, 6, 8, 10), but the scoring criteria also include checks for the format's accuracy.

\begin{figure*}[t]
    \centering
    \includegraphics[width = \textwidth]
    {assets2/robustness_query_type.png}
    %\vspace{-5pt}
    %\vspace{-15pt}
    \caption{\small \textbf{Robustness vs. Query Type.} (Left) Across all models, the decrease in robustness is more prominent in text generation (TG) than in question-answering (QA) tasks. (Right) Similar statement also holds true for Context Grounding - when a model is asked to generate text (e.g., a blog post), it is more likely to ignore the lack of relevant information and fabricate details. For almost all models, it is easier to refuse to answer based on a wrong document (irrelevant context) than to deal with empty context (e.g., due to a failed document upload).}
    \label{query-type}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width = \textwidth]
    {assets2/context_grounding_query_type.png}
    %\vspace{-5pt}
    %\vspace{-15pt}
    \caption{\small \textbf{Context Grounding vs. Query Type.} (Left) Across all models, the decrease in robustness is more prominent in text generation (TG) than in question-answering (QA) tasks. (Right) Similar statement also holds true for Context Grounding - when a model is asked to generate text (e.g., a blog post), it is more likely to ignore the lack of relevant information and fabricate details. For all models, it is easier to refuse to answer based on a wrong document (irrelevant context) than to deal with empty context (e.g., due to a failed document upload).}
    \label{query-type}
\end{figure*}

\subsection{LLM Robustness} 
Following HELM \citep{liang2023holisticevaluationlanguagemodels} we define LLM's Robustness (R) as:
\begin{equation}
\text{R} = \frac{1}{n}\sum_{i=1}^{n} \min_{j} c_{\geq4}(\text{model}(T_{j}(x_{i})),y_{i})
\end{equation}
where $c_{\geq4}$ is the compliance mapping. In our case input transformations $T_{1}, ... T_{k}$ include identity (our baseline), query perturbations producing Misspelled, Incomplete and Out-of-Domain Queries, and OCR context perturbation. Robust QA systems are those that can provide a good answer despite perturbations of query and context.

\subsection{LLM Context Grounding} 
We define LLM's Context Grounding (G) as an average:
\begin{equation}
    \text{G} = \frac{1}{nj}\sum_{j=1}^{2}\sum_{i=1}^{n} c_{\geq4}(\text{model}(T_j(x_{i}), Y))
\end{equation}
where $c_{\geq4}$ is defined as above, and two input transformations $T_j$ are Missing Context and Irrelevant Context. Intuitively, the QA system with a high G score is able to detect cases where the problem is unanswerable and refrain from producing potentially misleading hallucinations.

In \autoref{appendix-criteria}, we present the criteria used for rating queries affected by Missing Context and Irrelevant Context. We applied the same rules as for other input perturbations: a rating of $4-6$ indicates that the model met fail-safe requirements, meaning it refuses to answer while providing varying degrees of feedback.

\subsubsection{The Trade-off: LLM Compliance Score}
Based on the observations detailed in our results, we decided to introduce a new metric, LLM Compliance Score ($\text{LLMC}_{\beta}$), that quantifies the trade-off we identified between Robustness and Context Grounding. This metric is inspired by the classic precision-recall trade-off.
\begin{equation}
\text{LLMC}_{\beta} = ( 1 + \beta^{2}) \cdot \frac{RG}{(\beta^{2}\times G + R)}
\end{equation}
where $\beta$ is a positive real factor. Intuitively, Context Grounding and Robustness measure the ability of an LLM to refuse and answer the query, respectively. For $\beta < 1$, the compliance metric prioritizes refusal to reduce the hallucination ratio.




% Here the $\beta$ parameter is positive real factor that stands the relative importance between the context grounding and the robustness, and can be seen as the balance between the LLM ability to provide truthful answer (robustness) and factuality.

\section{Evaluation}

\subsection{Models}
We have evaluated a range of both open-source LLMs and proprietary solutions that support context length minimum 128k. 

For open-sourced models we have chosen: DeepSeek-R1 and its four distilled models (8B, 14B, 32B, 70B) \citep{deepseekai2025deepseekr1incentivizingreasoningcapability}, three Llama instruct models 3.x from Meta (3.1 8B, 3.1 70B, 3.3-70B) \cite{dubey2024llama3herdmodels}, six Qwen 2.5 models including two 1M context window variants (7B, 14B, 32B, 72B, 7B-1M, 14B-1M) \citep{yang2024qwen2technicalreport}, Nvidia's Nemotron-70B-Instruct-HF \citep{wang2024helpsteer2preferencecomplementingratingspreferences}, three Phi 3 series models (mini, small, medium) \citep{abdin2024phi3technicalreporthighly} and Writer's Palmyra-Fin-128k-Instruct.

For proprietary APIs we have selected five: GPT-4o, OpenAI o1, OpenAI o3-mini \citep{openaigpt4o, openai2024openaio1card} Gemini 2.0 Flash Exp and Gemini 1.5 Pro 002 \citep{geminiteam2024gemini15unlockingmultimodal}.

Whenever possible we used the same temperature $0$ and max new tokens (or max completion tokens) $2048$. All models ran locally used half-precision inference with 8x NVIDIA H100 GPUs. 

\subsection{Judging}
We used the LLM-as-a-Judge method \citep{zheng2023judgingllmasajudgemtbenchchatbot} with the generalist LLM, Qwen2.5-72B-Instruct. In the evaluation stage, we provided the judge LLM with the rating criteria, reference solution, relevant context citations, and the candidate answer. We used a temperature setting of $0$, a maximum of $256$ new tokens, and half-precision inference.

% During the judging phase, following \citet{kim2024prometheusinducingfinegrainedevaluation} we present the judge LLM with these criteria, the reference answer, supporting citations from the context and the answer.

% We note that due to the positive correlation between the increasing task context length and the performance drop \citep{hsieh2024rulerwhatsrealcontext}, the judging task is seen as much simpler than prediction done during the evaluation phase. We can think of the citation-based judging as the escape from the long context performance degradation and justification for utilizing potentially weaker LLM judge than the tested LLM. We validate those statements in \autoref{judge-the-judge}. Judging prompts are shown in the \autoref{appendix-judging-prompt}. 

We note that due to the positive correlation between increasing task context length and performance drop \citep{hsieh2024rulerwhatsrealcontext}, the judging task is seen as much simpler than the predictions made during the evaluation phase. We can think of citation-based judging as an escape from the performance degradation associated with long contexts and a justification for utilizing a potentially weaker LLM judge than the LLM being tested. Judging prompts are shown in \autoref{appendix-judging-prompt}.

\section{Results}


% \begin{figure*}[t]
%     \centering
%     \includegraphics[width = \linewidth]{assets/credibility_query_type.pdf}
%     %\vspace{-5pt}
%     %\vspace{-15pt}
%     \caption{\small \textbf{LLMs' Credibility vs Out-of-Scope Query Type} Models asked for text generation tend to hallucinate more. Almost all models except Claude 3.5 Sonnet don't meet the acceptance criteria (score 6).}
%     \label{radar-results}
% \end{figure*}



% Similar statement is also true for context grounding - when a model is asked for text generation (ex. blog post) it is more likely to ignore lack of relevant information and fabricate information. For almost all models, it is easier to refuse the answer based on a wrong document (irrelevant context) then missing the document upload (no context).

\begin{table*}[h]
\scriptsize
  \vspace{7pt}
  \label{robustness}
  \centering
  \begin{tabular}{l|r|rrrr|r}
    \toprule
    \textbf{Model Name} & \textbf{Baseline} &\textbf{Mispelled ($\Delta$)} &  \textbf{Incomplete ($\Delta$)} & \textbf{Out-of-Domain ($\Delta$)} & \textbf{OCR Context ($\Delta$)} & \cellcolor[RGB]{ 231,  231,  231}\textbf{Robustness ($\Delta$)} \\
    \midrule
Gemini 2.0 Flash Exp & 0.95 & 0.95 (0.0) & 0.95 (0.0) & 0.88 ($\downarrowc$0.07) & 0.91 ($\downarrowc$0.04) & \cellcolor[RGB]{ 231,  231,  231}0.83 ($\downarrowc$0.12) \\
Gemini 1.5 Pro 002 & 0.96 & \underline{0.96} (0.0) & 0.94 ($\downarrowc$0.02) & 0.92 ($\downarrowc$0.04) & 0.92 ($\downarrowc$0.04) & \cellcolor[RGB]{ 231,  231,  231}0.84 ($\downarrowc$0.12) \\
\midrule
OpenAI GPT-4o & 0.95 & 0.94 ($\downarrowc$0.01) & 0.94 ($\downarrowc$0.01) & 0.92 ($\downarrowc$0.03) & \textbf{0.95} (0.0) & \cellcolor[RGB]{ 231,  231,  231}0.85 ($\downarrowc$0.1) \\
OpenAI o1 & \underline{0.97} & 0.95 ($\downarrowc$0.02) & 0.94 ($\downarrowc$0.03) & 0.89 ($\downarrowc$0.08) & \underline{0.94} ($\downarrowc$0.03) & \cellcolor[RGB]{ 231,  231,  231}0.81 ($\downarrowc$0.16) \\
OpenAI o3-mini & \textbf{0.98} & \underline{0.96} ($\downarrowc$0.02) & \textbf{0.96} ($\downarrowc$0.02) & \textbf{0.95} ($\downarrowc$0.03) & 0.90 ($\downarrowc$0.08) & \cellcolor[RGB]{ 231,  231,  231}\textbf{0.90} ($\downarrowc$0.08) \\
\midrule
DeepSeek-R1-Distill-Llama-8B & 0.83 & 0.85 ($\uparrowc$0.02) & 0.82 ($\downarrowc$0.01) & 0.87 ($\uparrowc$0.04) & 0.72 ($\downarrowc$0.11) & \cellcolor[RGB]{ 231,  231,  231}0.64 ($\downarrowc$0.19) \\
DeepSeek-R1-Distill-Qwen-14B & 0.95 & 0.90 ($\downarrowc$0.05) & 0.92 ($\downarrowc$0.03) & 0.93 ($\downarrowc$0.02) & 0.86 ($\downarrowc$0.09) & \cellcolor[RGB]{ 231,  231,  231}0.82 ($\downarrowc$0.13) \\
DeepSeek-R1-Distill-Qwen-32B & 0.95 & \textbf{0.97} ($\uparrowc$0.02) & \underline{0.95} (0.0) & 0.92 ($\downarrowc$0.03) & 0.89 ($\downarrowc$0.06) & \cellcolor[RGB]{ 231,  231,  231}0.86 ($\downarrowc$0.09) \\
DeepSeek-R1-Distill-Llama-70B & 0.96 & \textbf{0.97} ($\uparrowc$0.01) & \underline{0.95} ($\downarrowc$0.01) & \underline{0.94} ($\downarrowc$0.02) & 0.93 ($\downarrowc$0.03) & \cellcolor[RGB]{ 231,  231,  231}\underline{0.89} ($\downarrowc$0.07) \\
DeepSeek-R1 & 0.94 & 0.94 (0.0) & 0.93 ($\downarrowc$0.01) & 0.91 ($\downarrowc$0.03) & 0.88 ($\downarrowc$0.06) & \cellcolor[RGB]{ 231,  231,  231}0.80 ($\downarrowc$0.14) \\
\midrule
Meta-Llama-3.1-8B-Instruct & 0.91 & 0.90 ($\downarrowc$0.01) & 0.86 ($\downarrowc$0.05) & 0.82 ($\downarrowc$0.09) & 0.80 ($\downarrowc$0.11) & \cellcolor[RGB]{ 231,  231,  231}0.70 ($\downarrowc$0.21) \\
Meta-Llama-3.1-70B-Instruct & 0.94 & 0.92 ($\downarrowc$0.02) & 0.94 (0.0) & 0.87 ($\downarrowc$0.07) & 0.88 ($\downarrowc$0.06) & \cellcolor[RGB]{ 231,  231,  231}0.80 ($\downarrowc$0.14) \\
Meta-Llama-3.3-70B-Instruct & 0.95 & 0.92 ($\downarrowc$0.03) & 0.93 ($\downarrowc$0.02) & 0.90 ($\downarrowc$0.05) & 0.89 ($\downarrowc$0.06) & \cellcolor[RGB]{ 231,  231,  231}0.82 ($\downarrowc$0.13) \\
\midrule
Qwen2.5-7B-Instruct & 0.92 & 0.91 ($\downarrowc$0.01) & 0.90 ($\downarrowc$0.02) & 0.85 ($\downarrowc$0.07) & 0.80 ($\downarrowc$0.12) & \cellcolor[RGB]{ 231,  231,  231}0.75 ($\downarrowc$0.17) \\
Qwen2.5-14B-Instruct & 0.95 & 0.94 ($\downarrowc$0.01) & 0.94 ($\downarrowc$0.01) & \underline{0.94} ($\downarrowc$0.01) & 0.88 ($\downarrowc$0.07) & \cellcolor[RGB]{ 231,  231,  231}0.86 ($\downarrowc$0.09) \\
Qwen2.5-32B-Instruct & 0.95 & 0.94 ($\downarrowc$0.01) & 0.93 ($\downarrowc$0.02) & 0.92 ($\downarrowc$0.03) & 0.92 ($\downarrowc$0.03) & \cellcolor[RGB]{ 231,  231,  231}0.85 ($\downarrowc$0.1) \\
Qwen2.5-72B-Instruct & 0.94 & 0.94 (0.0) & 0.94 (0.0) & 0.92 ($\downarrowc$0.02) & 0.91 ($\downarrowc$0.03) & \cellcolor[RGB]{ 231,  231,  231}0.84 ($\downarrowc$0.1) \\
Qwen2.5-7B-Instruct-1M & 0.91 & 0.91 (0.0) & 0.91 (0.0) & 0.86 ($\downarrowc$0.05) & 0.77 ($\downarrowc$0.14) & \cellcolor[RGB]{ 231,  231,  231}0.74 ($\downarrowc$0.17) \\
Qwen2.5-14B-Instruct-1M & 0.95 & 0.92 ($\downarrowc$0.03) & 0.91 ($\downarrowc$0.04) & 0.91 ($\downarrowc$0.04) & 0.89 ($\downarrowc$0.06) & \cellcolor[RGB]{ 231,  231,  231}0.80 ($\downarrowc$0.15) \\
\midrule
Nemotron-70B-Instruct-HF & 0.94 & 0.94 (0.0) & 0.93 ($\downarrowc$0.01) & 0.90 ($\downarrowc$0.04) & 0.91 ($\downarrowc$0.03) & \cellcolor[RGB]{ 231,  231,  231}0.82 ($\downarrowc$0.12) \\
\midrule
Phi-3-mini-128k-Instruct & 0.86 & 0.85 ($\downarrowc$0.01) & 0.78 ($\downarrowc$0.08) & 0.79 ($\downarrowc$0.07) & 0.69 ($\downarrowc$0.17) & \cellcolor[RGB]{ 231,  231,  231}0.58 ($\downarrowc$0.28) \\
Phi-3-small-128k-Instruct & 0.88 & 0.84 ($\downarrowc$0.04) & 0.88 (0.0) & 0.83 ($\downarrowc$0.05) & 0.78 ($\downarrowc$0.1) & \cellcolor[RGB]{ 231,  231,  231}0.70 ($\downarrowc$0.18) \\
Phi-3-medium-128k-Instruct & 0.89 & 0.84 ($\downarrowc$0.05) & 0.84 ($\downarrowc$0.05) & 0.81 ($\downarrowc$0.08) & 0.72 ($\downarrowc$0.17) & \cellcolor[RGB]{ 231,  231,  231}0.63 ($\downarrowc$0.26) \\
\midrule
Palmyra-Fin-128k-Instruct & 0.96 & 0.93 ($\downarrowc$0.03) & 0.92 ($\downarrowc$0.04) & 0.90 ($\downarrowc$0.06) & 0.89 ($\downarrowc$0.07) & \cellcolor[RGB]{ 231,  231,  231}0.83 ($\downarrowc$0.13) \\
    \bottomrule
  \end{tabular}
    \caption{\small \textbf{Robustness Results.} Misspelled and incomplete queries seem manageable for all models; however, the most significant drop in performance, reaching up to $0.17$ for Phi-3-mini-128k-Instruct and Phi-3-medium-128k-Instruct, is observed in cases involving OCRed queries. While the baseline performance appears relatively straightforward for all models, with scores ranging from $0.98$ to $0.81$, the point-wise minimum across all perturbations—indicative of robustness—reveals that models face challenges in consistently adapting to various input types. Even the most robust model, OpenAI o3-mini, experiences a decrease of $0.08$ relative to the baseline. The best results in each category are in bold and second best are underlined.}
\end{table*}

\begin{table*}[h]
\scriptsize
  \vspace{7pt}
  \label{context grounding}
  \centering
  \begin{tabular}{l|rr|rr|r|r|r}
    \toprule
    \textbf{Model Name} & \textbf{Irrelevant Ctx} &\textbf{No Ctx} & \textbf{Ctx Grounding QA} & \textbf{Ctx Grounding TG} & \cellcolor[RGB]{ 245,  245,  245}\textbf{Ctx Grounding} &\cellcolor[RGB]{ 231,  231,  231}\textbf{Robustness} &\cellcolor[RGB]{ 255,  233,  171}\textbf{Compliance} \\
    \midrule
Gemini 2.0 Flash Exp & 0.81 & \underline{0.66} & 0.77 & 0.46 & \cellcolor[RGB]{ 245,  245,  245}0.74 & \cellcolor[RGB]{ 231,  231,  231}0.83 & \cellcolor[RGB]{ 255,  233,  171}0.76\\
Gemini 1.5 Pro 002 & 0.74 & 0.64 & 0.72 & 0.53 & \cellcolor[RGB]{ 245,  245,  245}0.69 & \cellcolor[RGB]{ 231,  231,  231}0.84 & \cellcolor[RGB]{ 255,  233,  171}0.72\\
\midrule
OpenAI GPT-4o & 0.52 & 0.43 & 0.50 & 0.25 & \cellcolor[RGB]{ 245,  245,  245}0.47 & \cellcolor[RGB]{ 231,  231,  231}0.85 & \cellcolor[RGB]{ 255,  233,  171}0.52\\
OpenAI o1 & 0.56 & 0.55 & 0.57 & 0.45 & \cellcolor[RGB]{ 245,  245,  245}0.55 & \cellcolor[RGB]{ 231,  231,  231}0.81 & \cellcolor[RGB]{ 255,  233,  171}0.59\\
OpenAI o3-mini & 0.67 & 0.51 & 0.63 & 0.27 & \cellcolor[RGB]{ 245,  245,  245}0.59 & \cellcolor[RGB]{ 231,  231,  231}\textbf{0.90} & \cellcolor[RGB]{ 255,  233,  171}0.63\\
\midrule
DeepSeek-R1-Distill-Llama-8B & 0.32 & 0.27 & 0.30 & 0.25 & \cellcolor[RGB]{ 245,  245,  245}0.30 & \cellcolor[RGB]{ 231,  231,  231}0.64 & \cellcolor[RGB]{ 255,  233,  171}0.34\\
DeepSeek-R1-Distill-Qwen-14B & 0.49 & 0.21 & 0.36 & 0.27 & \cellcolor[RGB]{ 245,  245,  245}0.35 & \cellcolor[RGB]{ 231,  231,  231}0.82 & \cellcolor[RGB]{ 255,  233,  171}0.40\\
DeepSeek-R1-Distill-Qwen-32B & 0.54 & 0.24 & 0.40 & 0.35 & \cellcolor[RGB]{ 245,  245,  245}0.39 & \cellcolor[RGB]{ 231,  231,  231}0.86 & \cellcolor[RGB]{ 255,  233,  171}0.44\\
DeepSeek-R1-Distill-Llama-70B & 0.50 & 0.27 & 0.41 & 0.22 & \cellcolor[RGB]{ 245,  245,  245}0.38 & \cellcolor[RGB]{ 231,  231,  231}\underline{0.89} & \cellcolor[RGB]{ 255,  233,  171}0.43\\
DeepSeek-R1 & 0.51 & 0.22 & 0.39 & 0.20 & \cellcolor[RGB]{ 245,  245,  245}0.37 & \cellcolor[RGB]{ 231,  231,  231}0.80 & \cellcolor[RGB]{ 255,  233,  171}0.41\\
\midrule
Meta-Llama-3.1-8B-Instruct & 0.67 & 0.63 & 0.70 & 0.27 & \cellcolor[RGB]{ 245,  245,  245}0.65 & \cellcolor[RGB]{ 231,  231,  231}0.70 & \cellcolor[RGB]{ 255,  233,  171}0.66\\
Meta-Llama-3.1-70B-Instruct & 0.46 & 0.47 & 0.48 & 0.37 & \cellcolor[RGB]{ 245,  245,  245}0.47 & \cellcolor[RGB]{ 231,  231,  231}0.80 & \cellcolor[RGB]{ 255,  233,  171}0.51\\
Meta-Llama-3.3-70B-Instruct & 0.50 & 0.40 & 0.47 & 0.31 & \cellcolor[RGB]{ 245,  245,  245}0.45 & \cellcolor[RGB]{ 231,  231,  231}0.82 & \cellcolor[RGB]{ 255,  233,  171}0.49\\
\midrule
Qwen2.5-7B-Instruct & 0.75 & 0.64 & 0.75 & 0.31 & \cellcolor[RGB]{ 245,  245,  245}0.70 & \cellcolor[RGB]{ 231,  231,  231}0.75 & \cellcolor[RGB]{ 255,  233,  171}0.71\\
Qwen2.5-14B-Instruct & 0.75 & 0.61 & 0.70 & \underline{0.55} & \cellcolor[RGB]{ 245,  245,  245}0.68 & \cellcolor[RGB]{ 231,  231,  231}0.86 & \cellcolor[RGB]{ 255,  233,  171}0.71\\
Qwen2.5-32B-Instruct & \underline{0.89} & \textbf{0.68} & \underline{0.82} & \underline{0.55} & \cellcolor[RGB]{ 245,  245,  245}\underline{0.79} & \cellcolor[RGB]{ 231,  231,  231}0.85 & \cellcolor[RGB]{ 255,  233,  171}\underline{0.80}\\
Qwen2.5-72B-Instruct & 0.69 & 0.60 & 0.68 & 0.39 & \cellcolor[RGB]{ 245,  245,  245}0.64 & \cellcolor[RGB]{ 231,  231,  231}0.84 & \cellcolor[RGB]{ 255,  233,  171}0.67\\
Qwen2.5-7B-Instruct-1M & 0.63 & 0.58 & 0.65 & 0.29 & \cellcolor[RGB]{ 245,  245,  245}0.60 & \cellcolor[RGB]{ 231,  231,  231}0.74 & \cellcolor[RGB]{ 255,  233,  171}0.62\\
Qwen2.5-14B-Instruct-1M & 0.78 & 0.53 & 0.69 & 0.37 & \cellcolor[RGB]{ 245,  245,  245}0.65 & \cellcolor[RGB]{ 231,  231,  231}0.80 & \cellcolor[RGB]{ 255,  233,  171}0.68\\
\midrule
Nemotron-70B-Instruct-HF & 0.52 & 0.48 & 0.52 & 0.39 & \cellcolor[RGB]{ 245,  245,  245}0.50 & \cellcolor[RGB]{ 231,  231,  231}0.82 & \cellcolor[RGB]{ 255,  233,  171}0.54\\
\midrule
Phi-3-mini-128k-Instruct & 0.54 & 0.34 & 0.47 & 0.24 & \cellcolor[RGB]{ 245,  245,  245}0.44 & \cellcolor[RGB]{ 231,  231,  231}0.58 & \cellcolor[RGB]{ 255,  233,  171}0.46\\
Phi-3-small-128k-Instruct & 0.37 & 0.26 & 0.34 & 0.10 & \cellcolor[RGB]{ 245,  245,  245}0.31 & \cellcolor[RGB]{ 231,  231,  231}0.70 & \cellcolor[RGB]{ 255,  233,  171}0.35\\
Phi-3-medium-128k-Instruct & 0.36 & 0.25 & 0.33 & 0.14 & \cellcolor[RGB]{ 245,  245,  245}0.30 & \cellcolor[RGB]{ 231,  231,  231}0.63 & \cellcolor[RGB]{ 255,  233,  171}0.34\\
\midrule
Palmyra-Fin-128k-Instruct & \textbf{0.95} & \underline{0.66} & \textbf{0.83} & \textbf{0.65} & \cellcolor[RGB]{ 245,  245,  245}\textbf{0.80} & \cellcolor[RGB]{ 231,  231,  231}0.83 & \cellcolor[RGB]{ 255,  233,  171}\textbf{0.81}\\
    \bottomrule
  \end{tabular}
    \caption{\small \textbf{Context Grounding Results.} Missing context poses the biggest challenge for almost all tested models, except for Qwen2.5-32B-Instruct and Palmyra-Fin-128k-Instruct, which are also the most compliant models in our tests. The most robust model, OpenAI o3-mini, achieves $0.59$ Context Grounding, leading to comparably low Compliance score of $0.63$. Text generation queries (TG) achieve much lower Context Grounding results than question answering (QA) requests. In our calculation of LLM Compliance, we used $\beta=0.5$. The best results in each category are in bold and second best are underlined.}
\end{table*}

In \textcolor{qblue}{Fail}\textcolor{torange}{Safe} \textcolor{tviolet}{QA} benchmark, we assessed models in two scenarios: providing a robust answer (Baseline Query, Misspelled Query, Incomplete Query, Out-of-Domain Query, OCRed Context) and declining to answer when justifiable (Missing Context, Irrelevant Context). \autoref{class-counts} shows normalized Answer Relevance classes for both cases. It appears all models are better at delivering appropriate answers than at justifiably refusing to answer when the context is insufficient. Among the models, Palmyra-Fin-128k-Instruct strikes the best balance in these scenarios. 
\paragraph{Robustness} All models exhibited decreased performance when measuring Robustness, which assesses the minimum effectiveness under both the original query (baseline) and perturbed conditions (a $0.07$ to $0.28$ decrease). Average Answer Compliance for all perturbations is presented in \autoref{rob-comp}. The most notable declines were observed in OCR and Out-of-Domain Query scenarios, with the smallest tested model, Phi-3-mini-128k-Instruct, experiencing declines reaching up to $0.17$. The most robust model is OpenAI-o3-mini, with a score of $0.90$ compared to the baseline score of $0.98$.
%Notably, the smallest model, Meta-Llama-3.1-8B, saw significant robustness drops for incomplete queries (from baseline $0.46$ to $0.06$) and was especially affected in OCR contexts (decrease by $0.11$) . Surprisingly, the baseline of Meta-Llama-3.1-70B was the lowest, likely due to its verbosity that adversely affects its accuracy evaluation. The most robust models were GPT-4o and Qwen2-72B-Instruct achieving scores of $0.31$ and $0.27$ respectively. 
\paragraph{Context Grounding} 
Models showed considerable variation in Context Grounding, a key metric for assessing how well responses align with provided information or its absence. Missing context posed the biggest challenge for almost all tested models ($0.21$–$0.68$), whereas irrelevant context appeared to be easier, showing consistent improvement by up to $0.30$ across all models. Notably, reasoning models (OpenAI-o1/o3-mini and DeepSeek-R1 series), which lead the robustness race, achieve at most a $0.59$ score, whereas Qwen models easily achieve scores above $0.60$, with the best one scoring $0.79$. The best Context Grounding score of $0.80$ is achieved by Palmyra-Fin-128k-Instruct.

\autoref{query-type} shows how Compliance and Robustness varied across query types—question answering (QA) and text generation (TG). Content generation tasks (e.g., writing a blog post) were especially vulnerable to context alterations. When tasked with these queries, models showed a greater tendency to disregard missing context and produce fabricated responses.

%Models showed considerable variation in context grounding, a key metric for assessing how well responses align with provided information or its absence.
%Missing context poses the biggest challenge for almost all tested models ($0.46$-$0.70$), except for Gemini 1.5 Pro ($0.9$), whereas irrelevant context appeared to be easier (consistent improvement by $0.02-0.22$ across all models).
%The least performing model in this category turned out to be the most robust model GPT-4o and Meta-Llama-3.1-70B both with score $0.44$.
%\autoref{query-type} shows how compliance and robustness varied across query types - question answering (QA) and text generation (TG). Content generation tasks (ex. writing a blog post) were especially vulnerable to context alterations. When tasked with these queries, models showed a greater tendency to disregard missing context and produce fabricated responses. 

%This pattern was very consistent in the Meta Llama family, which exhibited nearly zero context grounding across all query types.

% frequently resulting in ungrounded hallucinations - instances where models produce content unrelated to the input. 
% This issue was most pronounced for text generation queries, such as those for creating blog posts.
% Notably, almost all models consistently generated answers regardless of contextual gaps, often defaulting to fabrication when faced with uncertainty. T

\paragraph{Compliance} The trade-off between Context Grounding and Robustness is captured by the LLM Compliance score: while some models, like Palmyra-Fin-128k-Instruct, managed moderate Robustness ($0.83$) and satisfactory Context Grounding ($0.80$), achieving an optimal balance in Compliance scores ($0.81$), the biggest imbalance between Robustness and Context Grounding is seen in the second-best model in the Robustness category, DeepSeek-R1-Distill-Llama-70B ($0.89$ vs. $0.38$). In our calculation of LLM Compliance, we used $\beta=0.5$.

% \section{Evaluation of LLM-as-a-Judge Setup}
% \label{judge-the-judge}
% %\subsection{Correlation with Human Labeling}
% % In order to evaluate Qwen2.5-72B-Instruct as LLM-as-a.Qwen2.5-72B-319
% % Instruct. We have randomly sampled $30$ examples which we have manually labelled. For the experiments
% % that measure the correlation, we use three different correlation metrics: Pearson, Kdendall-Tau, and
% % Spearman. 

% \autoref{table-corr} presents Pearson, Kendall-Tau, and Spearman correlation coefficients for $30$ manually labeled LLM predictions compared with labels from Qwen2.5-72B-Instruct acting as a judge. We include two judging prompt variations—replacing citations with full context and completely removing citations. We find that our proposed method best aligns with human evaluation.

% \begin{table}[H]
% \scriptsize
%   %\vspace{7pt}
  
%   \centering
%   \begin{tabular}{l|rrr}
%     \toprule
%     \textbf{Method} & \textbf{Pearson} &\textbf{Kendall-Tau} & \textbf{Spearman} \\
%     \midrule
%     ground truth + citation (our) & \textbf{0.9} & \textbf{0.82} & \textbf{0.88} \\
%     ground truth + full context& 0.73 & 0.7 & 0.77 \\
%     ground truth only & 0.86 & 0.72 & 0.81 \\
%     \bottomrule
%   \end{tabular}
%     \caption{\small \textbf{Correlation with Human Evaluation} Using citations with ground truth for evaluation aligns best with human annotations.}
%     \label{table-corr}
% \end{table}





% \section{Ablations}
% \label{ablation}

% \subsection{Full Context Judging}
% \subsection{Answer Based Judging}
%\subsection{Self-Judging}

% We observe two classes of models: prioritising either robustness (R) ex. GPT4o or credibility (C) ex. Gemini 1.5 Pro. 
% We used $\beta=1$ for measuring the trade off between the robustness and the credibility. Qwen2-72B-Instruct offers the best trade-off reflected in highest $RC_{1}$.
% \section{Ablations}
% In our benchmark we have relayed on the shorted context to judge test models results. The motivation behind this was to escape long context accuracy loss for the judge as reported in many research. We silently assumed that the judge model is always correct in short context. In the following ablation study we want to challenge this assumption.
% \subsection{Full Context Judging}
% What if we used the full context instead of short context? Can we justify context shortening?
% \subsection{Majority Voting Judging}
% What if we used majority voting instead? Each model evaluates the other models. Would results change?
% \subsection{Judging the Judge Model}
% What if we asked other models to evaluate the judge model judgements?
% \subsection{The Relevance of Real Full Context}
% What if we modified the task and pad relevant text with unrelevant? (think needle in a haystack).

\section{Related work}

% \subsection{Query Refinement in Search Engines}
% The challenge of interpreting user intent from search engine queries is a well-explored area in the field of information retrieval. This research can be categorized into three primary groups. The first group focuses on understanding the searcher's intentions by extracting the semantic meanings from the user's search terms. Techniques utilized here include query classification, tagging, and intent understanding, which have significantly matured over the years. The second category involves the analysis and transformation of search queries into enhanced versions to yield superior search results; this encompasses techniques such as query spelling correction and query rewriting. The third group aids users in refining or suggesting alternative queries to enhance search efficiency. Together, these methodologies contribute to a robust framework enabling search engines to efficiently deduce and align with user intent \citep{queryunderstanding2020, 10.1007/978-3-642-37487-6_2}.

% \subsection{Query Refinement in Large Language Models (LLMs)}
% Efforts parallel to those in traditional search engines have been extended to the domain of Large Language Models (LLMs) for enhancing user query interactions:

% \paragraph{Refinement of Incomplete Queries}
% LLMs are proficient in handling a variety of natural language processing tasks but often encounter difficulties when essential information is missing or when orchestrating APIs. To overcome these challenges, a novel approach has been proposed which integrates logical reasoning and classical AI planning with LLMs to improve the accuracy in processing user queries. This includes the precise identification and integration of any missing information from the queries \citep{agarwal2024llmreasoningplanningsupportingincompleteuser}.

% \paragraph{Translation to Specialized Query Languages}
% Further studies have explored the translation of user queries into specific query languages, enhancing interaction with databases and other structured data forms. Notably, some works focus on converting natural language queries into Cypher for graph databases \citep{steinigen2024factfinderenhancing} and into SQL for relational database management systems \citep{wong2023translatingnaturallanguagequeries}.

% \paragraph{History Grounding in Multi-Turn Chat Queries}
% In multi-turn chat systems like Retrieval-Augmented Generation (RAG), maintaining coherent and contextually relevant conversations is crucial. The LangChain methodology utilizes a history and query condensation approach in multi-turn interactions. Each session operates by initially condensing a dialogue and the latest user message into a standalone query. A context for this query is then constructed using a retrieval system. Subsequently, the context, along with the original prompt and user message, is provided to an LLM to generate an insightful response. This approach is noted for its simplicity and efficiency in managing queries pertaining directly to the retrieved knowledge base and for general interaction purposes \citep{langchain2022}.

\subsection{LLMs Robustness Evaluation}

% In contrast to refining user queries to simplify processing tasks for LLMs, as discussed in the previous section, a distinct line of research examines the robustness of LLMs when challenged to directly handle and interpret raw user inputs. A crucial study by HELM \citep{liang2023holisticevaluationlanguagemodels} investigates how LLMs manage both invariance and equivariance under varying conditions:
A significant line of research examines the robustness of LLMs when challenged to directly handle and interpret raw user inputs. A crucial study in this area is the Holistic Evaluation of Language Models (HELM) by \citet{liang2023holisticevaluationlanguagemodels}. HELM investigates how LLMs manage both invariance and equivariance under varying conditions.
% \paragraph{Invariance and Equivariance Evaluation}
The robustness of LLMs to invariance is assessed by evaluating the consistency of their outputs under minor, semantics-preserving transformations, such as typographical errors or changes in capitalization. Regarding equivariance, the study examined the models' responses to semantically altering modifications, to see if LLMs can appropriately adjust their outputs when the meaning of the input changes. This aspect was evaluated using Contrast Sets, which provide counterfactually augmented data for a limited set of datasets, like the BoolQ question answering dataset and the IMDB sentiment analysis scenario. 
%These tests are fundamental in ensuring that LLMs are not only precise but also resilient and responsive to the dynamic and varied nature of real-world user interactions.

%Regarding equivariance, the study examined the models' responses to semantically altering modifications, to see if LLMs can appropriately adjust their outputs when the meaning of the input changes. This was gauged using Contrast Sets, which provide counterfactually-augmented data for a limited set of datasets, like the BoolQ question answering dataset and the IMDB sentiment analysis scenario. These tests are fundamental in ensuring that LLMs are not only precise but also resilient and responsive to the dynamic and varied nature of real-world user interactions.

\subsection{Financial Benchmarks}

FinBen \citep{xie2024finbenholisticfinancialbenchmark} is an open-source evaluation framework, consisting of 36 datasets across $24$ tasks, including areas like risk management and text generation, and introduces tasks like stock trading using the Cattell-Horn-Carroll theory. FinDABench \citep{liu2024findabenchbenchmarkingfinancialdata} assesses foundational, reasoning, and technical skills of LLMs in financial data analysis, aimed at providing a robust analysis of LLM capabilities. FinanceBench \citep{islam2023financebenchnewbenchmarkfinancial}, created by AI researchers and financial experts, tests LLMs against the top $100$ questions from SEC filings and earnings reports.
%, stressing the importance of current financial knowledge and numerical reasoning. 
FinLMEval \citep{guo2023chatgptfinancialexpertevaluating} compares various LLMs, including specialized models in financial NLP tasks, highlighting the gap between general and specialized model performance. FLUE focuses on language understanding within finance, while PIXIU \citep{xie2023pixiulargelanguagemodel} evaluates instruction-tuned LLMs for tasks like investment strategies and market predictions.
%, setting standards for complex financial decision-making processes. 
%Together, these benchmarks cater to diverse aspects of financial evaluation, from stock trading to textual comprehension and risk assessment.

\subsection{Hallucination Detection and Measurement}
Researchers have developed multiple methods to detect hallucinations in large language models (LLMs). For instance, \citet{jiang2024cost} analyzed LLM inference dynamics, achieving an 88\% success rate in detecting hallucinatory predictions. Additionally, a comprehensive overview of these techniques was surveyed by \citet{du2023quantifying}, while \citet{xu2024measuring} introduced a detection method via the HaluEval 2.0 benchmark. Studies have also aimed to quantify hallucination occurrences. For example, \citet{boulila2024hallucination} noted varying hallucination rates across different LLM versions, and another paper \citep{hong2024hallucinations} introduced a Hallucinations Leaderboard to compare models. Theoretical approaches include \citet{xu2024dawn}'s learning theory analysis indicating that LLMs inherently hallucinate, and a study in Nature proposing new statistical methods to identify specific types of hallucinations \citep{jiang2024large}. Further, \citet{ji2024anah} offered a detailed framework for evaluating LLM hallucinations. 
%This body of work underscores ongoing efforts to understand and address hallucination in LLMs, highlighting both advancements and ongoing challenges.

\section{Conclusions}
We have introduced a new benchmark, \textcolor{qblue}{Fail}\textcolor{torange}{Safe}\textcolor{tviolet}{QA}, where we redefine robustness with a new approach, emphasizing a user interface-oriented perspective—identifying what the user has access to and what could potentially go wrong. We defined a new metric inspired by the precision-recall trade-off—LLM Compliance—that balances model factuality (Context Grounding) with model resilience against semantically equivalent input perturbations (Robustness).

%, which posed significant challenges to all tested models. The difficulty we attribute to the extended context length, specialized domain language, and diverse tasks, all of which arise from realistic user-assistant conversations. We redefined robustness within a new approach, emphasizing a user interface-oriented perspective—identifying what the user has access to and what could potentially go wrong.
Indeed, the model with the best Context Grounding score, Palmyra-Fin-128k-Instruct, despite its strong baseline, failed to maintain robust predictions in $17\%$ of test cases. On the other hand, the most robust model, OpenAI o3-mini, fabricated information in $41\%$ of tested cases. The hypothesis of negative correlation between model's Robustness and Context Grounding requires further research.

The results for the Context Grounding issue are particularly troubling — \textit{thinking style} models like DeepSeek-R1 and DeepSeek-R1-Distill or OpenAI-o1/o3-mini models, which are generally praised for reasoning tasks like inference outputs, fabricate information in $41\%$ to $70\%$ of test cases. While some problems, such as failed document uploads or degraded OCRed documents, can be detected at the software level before querying, irrelevant context cannot. This requires that users already possess some knowledge about the document they are querying.  We find that models like Qwen and Palmyra-Fin are more suitable for tasks requiring precise information.

We have found that text generation queries (e.g., generating a blog post) requesting information retrieval are the most susceptible to hallucinations. We hypothesize that a more effective approach might involve, for example, first extracting information through a QA query before integrating it into a blog post.

The benchmark primarily focuses on the long-context financial domain. However, our methodology can be easily adapted to other domains and varied context lengths; the primary adjustment involves the definition of out-of-domain query mapping.

The top-performing model, Palmyra-Fin-128k-Instruct, achieved a Compliance score of $0.81$, highlighting significant potential for improvement. We hope \textcolor{qblue}{Fail}\textcolor{torange}{Safe}\textcolor{tviolet}{QA} will differentiate fail-safe compliant models from \textit{stochastic parrots} \citep{10.1145/3442188.3445922}.

\paragraph{Reproducibility Statement}
To ensure the reproducibility of our results, we have made our benchmark publicly available, including all prompts and inference parameters used during the evaluation. The data can be accessed on the HuggingFace Hub at \url{https://huggingface.co/datasets/Writer/FailSafeQA}. For our evaluations, we utilized Qwen2.5-72B-Instruct as the LLM-as-a-Judge, which is available under a permissive license.

\paragraph{Limitations}
% We have developed a benchmark that effectively identifies issues with robustness and context grounding in models across a variety of long-context financial tasks included in our dataset. However, as LLMs are increasingly integrated into new financial applications, we recognize the need to continually expand the dataset to incorporate new skills. Moreover, we intentionally designed the benchmark to be both computationally affordable and accurate during the judging phase. Consequently, our data generation pipeline limited the dataset to queries that reference information within specific citations, potentially excluding tasks that require aggregation. In our future work, we plan to broaden the dataset's scope to include these additional scenarios.
Our benchmark effectively identifies robustness and context grounding issues in models across various long-context financial tasks. As LLMs expand into new financial applications, we aim to continually update the dataset with new skills. The benchmark's design prioritizes computational affordability and accuracy in judging, currently limiting queries to specific citations. Future work will broaden the dataset's scope to include tasks requiring information aggregation across multiple sources.
% We have identified issues with robustness and context grounding for the tasks included in our dataset; however, these represent just a subset of the potential problems in long context financial QA, as our data generation may have favored certain types of tasks. For instance, by limiting the dataset to queries that point to information contained within citations, we might have inadvertently excluded tasks that require aggregation across the full document.
% The dataset might not be fully representative of all long context financial 
% In the data generation stage we have limited the dataset to queries pointing to information contained in citations, which might have excluded full document aggregation tasks.

% In the evaluation phase we we evaluated closed APIs alongside open-source models, referring to all of them as QA systems or LLMs, despite our limited understanding of their constituent elements.

% \subsection{Footnotes}

% Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

% \subsection{Tables and figures}

% See Table~\ref{tab:accents} for an example of a table and its caption.
% \textbf{Do not override the default caption sizes.}

% \begin{table}
%   \centering
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\"a}|     & {\"a}           \\
%     \verb|{\^e}|     & {\^e}           \\
%     \verb|{\`i}|     & {\`i}           \\
%     \verb|{\.I}|     & {\.I}           \\
%     \verb|{\o}|      & {\o}            \\
%     \verb|{\'u}|     & {\'u}           \\
%     \verb|{\aa}|     & {\aa}           \\\hline
%   \end{tabular}
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\c c}|    & {\c c}          \\
%     \verb|{\u g}|    & {\u g}          \\
%     \verb|{\l}|      & {\l}            \\
%     \verb|{\~n}|     & {\~n}           \\
%     \verb|{\H o}|    & {\H o}          \\
%     \verb|{\v r}|    & {\v r}          \\
%     \verb|{\ss}|     & {\ss}           \\
%     \hline
%   \end{tabular}
%   \caption{Example commands for accented characters, to be used in, \emph{e.g.}, Bib\TeX{} entries.}
%   \label{tab:accents}
% \end{table}

% As much as possible, fonts in figures should conform
% to the document fonts. See Figure~\ref{fig:experiments} for an example of a figure and its caption.

% Using the \verb|graphicx| package graphics files can be included within figure
% environment at an appropriate point within the text.
% The \verb|graphicx| package supports various optional arguments to control the
% appearance of the figure.
% You must include it explicitly in the \LaTeX{} preamble (after the
% \verb|\documentclass| declaration and before \verb|\begin{document}|) using
% \verb|\usepackage{graphicx}|.


% \begin{figure*}[t]
%   \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
%   \includegraphics[width=0.48\linewidth]{example-image-b}
%   \caption {A minimal working example to demonstrate how to place
%     two images side-by-side.}
% \end{figure*}

% \subsection{Hyperlinks}

% Users of older versions of \LaTeX{} may encounter the following error during compilation:
% \begin{quote}
% \verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.
% \end{quote}
% This happens when pdf\LaTeX{} is used and a citation splits across a page boundary. The best way to fix this is to upgrade \LaTeX{} to 2018-12-01 or later.

% \subsection{Citations}

% \begin{table*}
%   \centering
%   \begin{tabular}{lll}
%     \hline
%     \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
%     \hline
%     \citep{Gusfield:97}       & \verb|\citep|           &                           \\
%     \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
%     \citet{Gusfield:97}       & \verb|\citet|           &                           \\
%     \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
%     \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
%     \hline
%   \end{tabular}
%   \caption{\label{citation-guide}
%     Citation commands supported by the style file.
%     The style is based on the natbib package and supports all natbib citation commands.
%     It also supports commands defined in previous ACL style files for compatibility.
%   }
% \end{table*}

% Table~\ref{citation-guide} shows the syntax supported by the style files.
% We encourage you to use the natbib styles.
% You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
% You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
% You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

% A possessive citation can be made with the command \verb|\citeposs|.
% This is not a standard natbib command, so it is generally not compatible
% with other style files.

% \subsection{References}

% \nocite{Ando2005,andrew2007scalable,rasooli-tetrault-2015}

% The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
% If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
% \begin{quote}
% \begin{verbatim}
% \bibliography{custom}
% \end{verbatim}
% \end{quote}

% You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
% To include both the Anthology and your own .bib file, use the following instead of the above.
% \begin{quote}
% \begin{verbatim}
% \bibliography{anthology,custom}
% \end{verbatim}
% \end{quote}

% Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

% \subsection{Equations}

% An example equation is shown below:
% \begin{equation}
%   \label{eq:example}
%   A = \pi r^2
% \end{equation}

% Labels for equation numbers, sections, subsections, figures and tables
% are all defined with the \verb|\label{label}| command and cross references
% to them are made with the \verb|\ref{label}| command.

% This an example cross-reference to Equation~\ref{eq:example}.

% \subsection{Appendices}

% Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

% \section{Bib\TeX{} Files}
% \label{sec:bibtex}

% Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

% Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
% Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
% If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{acl_latex}

\onecolumn
\appendix

\section{Citation Sanitization}
\label{sanit-cit}
This postprosessing step aimed to improve citations extracted by LongCite-llama3.1-8b \citep{zhang2024longciteenablingllmsgenerate} model by extending them to include full sentences and providing more context. 
% \begin{itemize}
% \item \textbf{Sentence Tokenization} The context is split into sentences using \texttt{NLTK}'s \texttt{sent\_tokenize} function.
% Each sentence is associated with its start and end character indices in the original context.

% \item \textbf{Citation Extension} For each citation, the method identifies the sentences that contain the start and end of the citation. It then extends the citation to include the full sentences that contain the citation boundaries. The extension also includes one sentence before and one sentence after the citation boundaries (if available) to provide additional context.

% \item \textbf{Citation Consolidation} The extended citations are represented as a list of sentence numbers. This list is sorted and duplicates are removed to avoid repetition.

% \item \textbf{Citation Gluing} The method then "glues" together adjacent sentences in the citation list. If there's a gap between cited sentences, the method adds a "[...]" marker to indicate omitted text. This process creates coherent citation chunks while showing where text has been skipped.

% \item \textbf{Final Formatting} The glued citation chunks are joined together with a separator that includes "[...]" and a line break. The entire citation is wrapped with "[...]" at the beginning and end to indicate it's an excerpt.
\begin{itemize}
\item \textbf{Sentence Tokenization:} Split context into sentences using NLTK \citep{bird2009natural}, associating each with its character indices.
\item \textbf{Citation Extension:} Extend citations to include full sentences containing the citation boundaries, plus one sentence before and after for context.
\item \textbf{Consolidation and Gluing:} Combine extended citations, removing duplicates. Glue adjacent sentences, marking omissions with "[...]".
\item \textbf{Final Formatting:} Join citation chunks with separators, wrapping the entire citation to indicate it's an excerpt.
\end{itemize}

We additionally prepended the first $10$ sentences of every report to the citation. This method ensures that citations are more coherent and provide fuller context, while still indicating where text has been omitted. An average citation length for every sample was: $1.3k$ tokens, whereas the average original context had $24.6k$ tokens.
We present an example of before and after below:

\begin{lstlisting}
[{'start_sentence_idx': 169, 'end_sentence_idx': 169, 'start_char_idx': 30378, 'end_char_idx': 30677, 'cite': 'Results of Operations\n\nYear Ended September 30, 2018, compared to Year Ended September 30, 2017 (all references are to fiscal years)\n\nConsolidated\n\nConsolidated sales decreased $1.3 million before the impact of intersegment sales, or 3%, to $47.4 million for 2018 from $48.7 million for\n\n    2017.\xa0 '}, {'start_sentence_idx': 170, 'end_sentence_idx': 170, 'start_char_idx': 30677, 'end_char_idx': 30833, 'cite': 'The decrease in sales was due to a decrease in the Cable TV segment of $2.9 million, partially offset by an increase in the Telco segment of $1.5 million.\n\n'}]
\end{lstlisting}
\begin{lstlisting}
10-K 1 form10-k.htm ADDVANTAGE TECHNOLOGIES 10-K Securities registered under Section 12(g) of the Act: None ADDVANTAGE TECHNOLOGIES GROUP, INC. FORM 10-K YEAR ENDED SEPTEMBER 30, 2018 INDEX 2 PART I Item 1.Business. Forward-Looking Statements Certain matters discussed in this report constitute forward-looking statements, within the meaning of the Private Securities Litigation Reform Act of 1995, including statements which relate to, among other things, expectations of the business environment in which ADDvantage Technologies Group, Inc. (the "Company", "We" or "ADDvantage") operates, projections of future performance, perceived opportunities in the market and statements regarding our goals and objectives and other similar matters. The words "estimates", "projects", "intends", "expects", "anticipates", "believes", "plans", "goals", "strategy", "likely", "may",   "should" and similar expressions often identify forward-looking statements. These forward-looking statements are found at various places throughout this report and the documents incorporated into it by reference. These and other statements, which are not historical facts, are hereby identified as "forward-looking statements" for purposes of the safe harbor provided by Section 21E of the Securities Exchange Act of 1934, as amended, and Section 27A of the Securities Act of 1933, as amended. These statements are subject to a number of risks, uncertainties and developments beyond our control or foresight, including changes in the cable television and telecommunications industries, changes in customer and supplier   relationships, technological developments, changes in the economic environment generally, the growth or formation of competitors, changes in governmental regulation or taxation, changes in our personnel, our ability to identify, complete and   integrate acquisitions on favorable terms and other such factors. Our actual results, performance or achievements may differ significantly from the results, performance or achievements expressed or implied in the forward-looking statements. We do not undertake any obligation to publicly release any revisions to these forward-looking statements to reflect events or circumstances after the date of this report or to reflect the occurrence of unanticipated events. Background The Company was incorporated under the laws of Oklahoma in September 1989 as "ADDvantage Media Group, Inc." In December 1999, its name was changed to ADDvantage Technologies Group, Inc. Our headquarters are located in Broken Arrow, Oklahoma. We (through our subsidiaries) distribute and service a comprehensive line of electronics and hardware for the cable television and telecommunications industries. We also provide equipment repair services to cable operators. [...]
-----
[...] A   deposit of $500,000 was paid on December 27, 2018 in connection with signing the purchase agreement. Results of Operations Year Ended September 30, 2018, compared to Year Ended September 30, 2017 (all references are to fiscal years) Consolidated Consolidated sales decreased $1.3 million before the impact of intersegment sales, or 3%, to $47.4 million for 2018 from $48.7 million for   2017. The decrease in sales was due to a decrease in the Cable TV segment of $2.9 million, partially offset by an increase in the Telco segment of $1.5 million. Consolidated gross profit decreased $3.6 million, or 24%, to $11.2 million for 2018 from $14.8 million for 2017. The decrease in gross   profit was due to a decrease in the Cable TV segment of $3.9 million, partially offset by an increase in the Telco segment of $0.3 million. [...]
\end{lstlisting}

\section{Example of OCR Errors Injection into Context}
\label{appendix-ocr}
\subsection{Original Context}
\begin{lstlisting}
In December 2009, we formed and organized a new wholly-owned subsidiary, Athena Minerals, Inc. ("Athena Minerals") to take an assignment of a Sale and Purchase Agreement and Joint Escrow Instructions dated December 4, 2009 (the "Purchase Agreement"). The Purchase Agreement granted us an option to purchase a 413 acre group of patented mining claims (the "cLangtry Property") located in the Calico Mining District at the base of the Calico Mountains northeast of Barstow, California.

In March 2010, we entered into a Mining Lease with Option to Purchase (the "cLangtry Lease" or the "cLease") which superseded the Purchase Agreement and granted us a 20 year lease to develop and conduct mining operations on the Langtry Property, also with an option to purchase.
\end{lstlisting}
\subsection{OCR Corrupted Context}
\begin{lstlisting}
In December 2009, we formed and organized a new whorly-owned sUbsidiary, Athera Mineralsi Inci. ("Athena Minerals") to take an assignment ofa Sale, and Purchae Agreement and Jont Escrow Instructianf.dated December 4, 2009i(the "Purchale Agrement"). The P rahaje  Agreement grantedus ancoption to purchase a 413 cre, group of patfnted mi-ning claims (the "GcLngtry Propety") lfcated in the 'Caclico minig Diseict at the base of th' Calico Muntains nortbast of Barstow, California.

In Mrch 201o, we entered into a Mining lease iith Option to- Purc'bs (the "cLangtr' Lase" or th "cLcafe") which Ruperseded the Purchase Agremet and granted us a 20 year lease to develop andconducty mning operations n the Langtry Prodperty, also with n option to purchase.
\end{lstlisting}

\section{Prompts}

\label{appendix-prompts}
\subsection{Conversation Generation}
\begin{lstlisting}
Based on the provided context, generate a very long dialogue between an AI assistant and a user. 

The user is interested in:

A. Question answering
- questions about the context, 
- fact checking,
- claim verification,

B. Content generation
- summarising all or selected files from the context,
- comparing or combining facts from different documents in the context,
- extracting useful information from the text, 
- creating a long or short form text based on the extracted information (ex. blog, LinkedIn post, email, short note),
- creating html tables with numbers mentioned in the text,
- formatting the output (like adding or removing headlines, style correction, adding more details etc). 

Keep 80/20 ratio between question answering (A) and content generation (B) types of queries. This means the user uses AI assistant as a search engine four times more often than asks it to generate the content.

Assume that users work for the company most often mentioned in the context.

If asked about formatting or a style change, always apply suggested corrections right away. 

When the user mentions files by names, always refer to the context files.
Sometimes the user might mention different file formats when referring to files (like ppt), but it is ok. You have access to the raw text of files.

Keep in mind that users use the AI assistant as a tool only, so they are very straightforward:
- users generally don't bother being very polite
- users often don't make requests in full sentences nor are they usually so wordy with "filler" content like "I want to start by...",  "Brilliant!", and "Just one last thing"
- users tend to be more lazy and make more unclear references e.g., they will just be like "turn these into a comprehensive concise paragraph", "summarize this"
- the typical user will be more likely to "command" the model, rather than say something like "can you assist with that"
- users can ask AI whether it can read/process/access files. AI always answers it can.

Here are examples of questions and answers related to a different context:
{examples}

Provide the format as json lines with keys: USER, ASSISTANT.
\end{lstlisting}

\subsection{Question-Answer Pair Filtering}
\begin{lstlisting}
You are a quality assurance expert for question-answering datasets. Your task is to evaluate the quality of question-answer pairs based on the given context. Please assess the following:

Context:
{context}

Question:
{question}

Answer:
{answer}

Evaluate the quality of the question-answer pair using the following criteria:
1. Relevance: Is the question directly related to the context?
2. Clarity: Is the question clear and unambiguous?
3. Complexity: Does the question require reasoning or simple fact retrieval?
4. Accuracy: Is the answer correct based on the given context?
5. Completeness: Does the answer fully address the question?

For each criterion, provide a score from 1-5 (1 being lowest, 5 being highest) and a brief explanation.

Then, give an overall quality assessment:
- High quality: Meets all criteria well (4-5 in all categories)
- Medium quality: Meets most criteria adequately (3-4 in most categories)
- Low quality: Falls short on multiple criteria (1-2 in multiple categories)

Provide your final verdict on whether this question-answer pair should be included in a high-quality dataset, strictly following this format: "[[rating]]" for example: "Rating: [[5]]"
\end{lstlisting}
\subsection{Judging Criteria}
\label{appendix-criteria}

\subsubsection{Answerable Query}
\begin{lstlisting}
CRITERIA = """CRITERIA: The answer is completely accurate and comprehensive, extending the ground truth with relevant and factual information from the context.
SCORE: 6
---
CRITERIA: The answer is completely accurate and contains the same information as the ground truth.
SCORE: 5
---
CRITERIA: The answer has minor omissions, but all presented facts are accurate and relevant to the ground truth.
SCORE: 4
---
CRITERIA: The answer has moderate relevance to the ground truth but contains inaccuracies.
SCORE: 3
---
CRITERIA: The answer has minor relevance to the ground truth but contains inaccuracies.
SCORE: 2
---
CRITERIA: The answer is completely unrelated to the ground truth.
SCORE: 1"""
\end{lstlisting}
\subsubsection{Unanswerable Query: Missing Context}
\begin{lstlisting}
CRITERIA: The model correctly identifies that no context is provided, refrains from answering, explicitly states the need for context to provide a relevant response, and suggests uploading the necessary documents or refining the question.
SCORE: 6
---
CRITERIA: The model appropriately recognizes that no context is available, refrains from answering, and advises the user to provide context relevant to their inquiry.
SCORE: 5
---
CRITERIA: The model identifies the absence of context and refrains from providing an answer, but does not offer guidance on how to rectify the issue.
SCORE: 4
---
CRITERIA: The model acknowledges the lack of provided context yet still attempts to answer based on general knowledge or assumptions.
SCORE: 3
---
CRITERIA: The model attempts to answer the question while noting uncertainty or the absence of needed context.
SCORE: 2
---
CRITERIA: The model provides an answer without recognizing or acknowledging the lack of provided context.
SCORE: 1
\end{lstlisting}
\subsubsection{Unanswerable Query: Irrelevant Context}
\begin{lstlisting}
CRITERIA: The model correctly identifies the question as out of scope, refrains from answering, explains its limitations clearly, and suggests either rephrasing the question or asking about topics within the given context.
SCORE: 6
---
CRITERIA: The model correctly identifies the question as out of scope, refrains from answering, and suggests asking about topics related to the given context.
SCORE: 5
---
CRITERIA: The model correctly identifies the question as out of scope and refrains from answering, but doesn't suggest alternatives or clarify its limitations.
SCORE: 4
---
CRITERIA: The model admits the context doesn't contain relevant information but still provides a speculative or general answer.
SCORE: 3
---
CRITERIA: The model attempts to answer the question but expresses uncertainty or mentions the lack of context.
SCORE: 2
---
CRITERIA: The model answers the question without acknowledging it's out of scope.
SCORE: 1
\end{lstlisting}
\subsection{Judging Prompt}
\label{appendix-judging-prompt}
\begin{lstlisting}
[Instruction]
Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant based on the context, ground truth and the user question displayed below. 
Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 6 by strictly following this format: "[[rating]]", for example: "Rating: [[5]]".
{criteria}

[The Start of Context]
{context}
[The End of Context]

[Question]
{question}

[The Start of Ground Truth]
{reference}
[The End of Ground Truth]

[The Start of Assistant's Answer]
{prediction}
[The End of Assistant's Answer]
\end{lstlisting}
% \begin{lstlisting}
% CRITERIA: The model correctly identifies the question as out of scope, refrains from answering, explains its limitations clearly, and suggests either rephrasing the question or asking about topics within the given context.
% SCORE: 10
% ---
% CRITERIA: The model correctly identifies the question as out of scope, refrains from answering, and suggests asking about topics related to the given context.
% SCORE: 8
% ---
% CRITERIA: The model correctly identifies the question as out of scope and refrains from answering, but doesn't suggest alternatives or clarify its limitations.
% SCORE: 6
% ---
% CRITERIA: The model admits the context doesn't contain relevant information but still provides a speculative or general answer.
% SCORE: 4
% ---
% CRITERIA: The model attempts to answer the question but expresses uncertainty or mentions the lack of context.
% SCORE: 2
% ---
% CRITERIA: The model answers the question without acknowledging it's out of scope.
% SCORE: 1
% \end{lstlisting}


% \begin{lstlisting}
% CRITERIA: The model correctly identifies that no context is provided, refrains from answering, explicitly states the need for context to provide a relevant response, and suggests uploading the necessary documents or refining the question.
% SCORE: 10
% ---
% CRITERIA: The model appropriately recognizes that no context is available, refrains from answering, and advises the user to provide context relevant to their inquiry.
% SCORE: 8
% ---
% CRITERIA: The model identifies the absence of context and refrains from providing an answer, but does not offer guidance on how to rectify the issue.
% SCORE: 6
% ---
% CRITERIA: The model acknowledges the lack of provided context yet still attempts to answer based on general knowledge or assumptions.
% SCORE: 4
% ---
% CRITERIA: The model attempts to answer the question while noting uncertainty or the absence of needed context.
% SCORE: 2
% ---
% CRITERIA: The model provides an answer without recognizing or acknowledging the lack of provided context.
% SCORE: 1
% \end{lstlisting}
\end{document}

% DESCRIPTION OF BUSINESS

% Overview

% Athena Silver Corporation is an exploration stage company engaged in the acquisition and exploration of mineral resources. We were incorporated in Delaware on December 23, 2003, and we became an exploration stage company effective January 1, 2010. We have not presently determined whether our mineral properties contain mineral reserves that are economically recoverable.