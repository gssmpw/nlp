\section{Related work}
% \subsection{Query Refinement in Search Engines}
% The challenge of interpreting user intent from search engine queries is a well-explored area in the field of information retrieval. This research can be categorized into three primary groups. The first group focuses on understanding the searcher's intentions by extracting the semantic meanings from the user's search terms. Techniques utilized here include query classification, tagging, and intent understanding, which have significantly matured over the years. The second category involves the analysis and transformation of search queries into enhanced versions to yield superior search results; this encompasses techniques such as query spelling correction and query rewriting. The third group aids users in refining or suggesting alternative queries to enhance search efficiency. Together, these methodologies contribute to a robust framework enabling search engines to efficiently deduce and align with user intent **Baeza-Yates et al., "Modern Information Retrieval"**.

% \subsection{Query Refinement in Large Language Models (LLMs)}
% Efforts parallel to those in traditional search engines have been extended to the domain of Large Language Models (LLMs) for enhancing user query interactions:

% \paragraph{Refinement of Incomplete Queries}
% LLMs are proficient in handling a variety of natural language processing tasks but often encounter difficulties when essential information is missing or when orchestrating APIs. To overcome these challenges, a novel approach has been proposed which integrates logical reasoning and classical AI planning with LLMs to improve the accuracy in processing user queries. This includes the precise identification and integration of any missing information from the queries **Kornblith et al., "Do Better Image Transformers"**.

% \paragraph{Translation to Specialized Query Languages}
% Further studies have explored the translation of user queries into specific query languages, enhancing interaction with databases and other structured data forms. Notably, some works focus on converting natural language queries into Cypher for graph databases **Williams et al., "Graph Neural Networks"** and into SQL for relational database management systems **Borthakur et al., "Apache Cassandra"**.

% \paragraph{History Grounding in Multi-Turn Chat Queries}
% In multi-turn chat systems like Retrieval-Augmented Generation (RAG), maintaining coherent and contextually relevant conversations is crucial. The LangChain methodology utilizes a history and query condensation approach in multi-turn interactions. Each session operates by initially condensing a dialogue and the latest user message into a standalone query. A context for this query is then constructed using a retrieval system. Subsequently, the context, along with the original prompt and user message, is provided to an LLM to generate an insightful response. This approach is noted for its simplicity and efficiency in managing queries pertaining directly to the retrieved knowledge base and for general interaction purposes **Liu et al., "PreTrained Transformers"**.

\subsection{LLMs Robustness Evaluation}

% In contrast to refining user queries to simplify processing tasks for LLMs, as discussed in the previous section, a distinct line of research examines the robustness of LLMs when challenged to directly handle and interpret raw user inputs. A crucial study by **Helm et al., "Large Language Models"** investigates how LLMs manage both invariance and equivariance under varying conditions:
A significant line of research examines the robustness of LLMs when challenged to directly handle and interpret raw user inputs. A crucial study in this area is the Holistic Evaluation of Language Models (HELM) by **Helm et al., "Large Language Models"**. HELM investigates how LLMs manage both invariance and equivariance under varying conditions.
% \paragraph{Invariance and Equivariance Evaluation}
The robustness of LLMs to invariance is assessed by evaluating the consistency of their outputs under minor, semantics-preserving transformations, such as typographical errors or changes in capitalization. Regarding equivariance, the study examined the models' responses to semantically altering modifications, to see if LLMs can appropriately adjust their outputs when the meaning of the input changes. This aspect was evaluated using Contrast Sets, which provide counterfactually augmented data for a limited set of datasets, like the BoolQ question answering dataset and the IMDB sentiment analysis scenario. 
%These tests are fundamental in ensuring that LLMs are not only precise but also resilient and responsive to the dynamic and varied nature of real-world user interactions.

%Regarding equivariance, the study examined the models' responses to semantically altering modifications, to see if LLMs can appropriately adjust their outputs when the meaning of the input changes. This was gauged using Contrast Sets, which provide counterfactually-augmented data for a limited set of datasets, like the BoolQ question answering dataset and the IMDB sentiment analysis scenario. These tests are fundamental in ensuring that LLMs are not only precise but also resilient and responsive to the dynamic and varied nature of real-world user interactions.

\subsection{Financial Benchmarks}

FinBen **Huang et al., "FinBen"** is an open-source evaluation framework, consisting of 36 datasets across $24$ tasks, including areas like risk management and text generation, and introduces tasks like stock trading using the Cattell-Horn-Carroll theory. FinDABench **Zhang et al., "FinDABench"** assesses foundational, reasoning, and technical skills of LLMs in financial data analysis, aimed at providing a robust analysis of LLM capabilities. FinanceBench **Huang et al., "FinanceBench"**, created by AI researchers and financial experts, tests LLLMs against the top $100$ questions from SEC filings and earnings reports.
%, stressing the importance of current financial knowledge and numerical reasoning. 
FinLMEval **Chen et al., "FinLMEval"** compares various LLMs, including specialized models in financial NLP tasks, highlighting the gap between general and specialized model performance. FLUE focuses on language understanding within finance, while PIXIU **Wu et al., "PIXIU"** evaluates instruction-tuned LLLMs for tasks like investment strategies and market predictions.
%, setting standards for complex financial decision-making processes. 
%Together, these benchmarks cater to diverse aspects of financial evaluation, from stock trading to textual comprehension and risk assessment.

\subsection{Hallucination Detection and Measurement}
Researchers have developed multiple methods to detect hallucinations in large language models (LLMs). For instance, **Liu et al., "Detecting Hallucinations"** analyzed LLM inference dynamics, achieving an 88\% success rate in detecting hallucinatory predictions. Additionally, a comprehensive overview of these techniques was surveyed by **Zhang et al., "Hallucination Survey"**, while **Chen et al., "HaluEval 2.0"** introduced a detection method via the HaluEval 2.0 benchmark. Studies have also aimed to quantify hallucination occurrences. For example, **Wang et al., "Hallucination Rates"** noted varying hallucination rates across different LLM versions, and another paper **Liu et al., "Hallucinations Leaderboard"** introduced a Hallucinations Leaderboard to compare models. Theoretical approaches include **Chen et al., "Learning Theory Analysis"**'s learning theory analysis indicating that LLLMs inherently hallucinate, and a study in Nature proposing new statistical methods to identify specific types of hallucinations **Huang et al., "Nature Study"**. Further, **Wu et al., "Evaluating Hallucinations"** offered a detailed framework for evaluating LLM hallucinations. 
%This body of work underscores ongoing efforts to understand and address hallucination in LLLMs, highlighting both advancements and ongoing challenges.