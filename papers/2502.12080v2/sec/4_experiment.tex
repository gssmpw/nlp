\section{Experiments}



\subsection{Experimental Setup}

\noindent \textbf{Datasets.} 
We evaluate the performance of our \nickname{} on two multi-view human modelling datasets, \ie, RenderPeople~\cite{renderpeople} (Synthetic Humans) and DNA-Rendering~\cite{cheng2023dna} (Real-World Humans).
For RenderPeople, we randomly sample 450 subjects as the training set and 30 subjects for testing.
For each subject, we use all frames from the training data for training, 6 frames with 4 camera viewpoints for novel view synthesis, and all frames with a front camera view for novel pose synthesis. 
For the DNA-Rendering dataset, we use 416 sequences from Part 1 and 2 for training and 10 sequences for evaluation.
For each subject, we use all frames from the training data for training, 48 frames with 4 camera viewpoints for novel view synthesis, and all frames with a front camera view for novel pose synthesis. 
The foreground masks, camera, and SMPL parameters from these two datasets are used for evaluation purposes.

\noindent \textbf{Comparison Methods.}
We compare our \nickname{} with two categories of state-of-the-art single-view-based animatable human modelling methods, \ie, a generalizable Human NeRF method, SHERF~\cite{hu2023sherf}, and diffusion-based 2D character animation methods, AnimateAnyone~\cite{hu2024animate} and Champ~\cite{zhu2024champ}.
For fair comparisons, we retrain SHERF and Champ by using their official codebase and AnimateAnyone with the open-source implementation from MooreThreads\footnote{https://github.com/MooreThreads/Moore-AnimateAnyone}.

\noindent \textbf{Implementation Details.} 
% Our experiments are conducted on 4 NVIDIA H100 GPUs. 
The training phase is divided into three stages. In the first stage, we process all frames by resizing and cropping the images to be 512x512 resolution for the RenderPeople dataset and 768x768 resolution for the DNA-Rendering dataset. 
Then we train a model to learn the mapping from a reference image and a target pose image to a target human image. 
This stage is trained for 150,000 iterations with a batch size of 4. 
In the second stage, we incorporate the multi-view attention layer after the spatial-attention and cross-attention in the denoising UNet to learn multi-view consistency. 
This stage is trained for 10,000 iterations with a batch size of 24 frames (a video length of 6 and 4 views) on each GPU. 
In the last training stage, we train the temporal layer with the same batch size used in the second stage for 10,000 iterations.

\noindent \textbf{Evaluation Metrics.}
To quantitatively compare our \nickname{} with baseline methods, we evaluate the performance on three metrics, \ie, peak signal-to-noise ratio (PSNR)~\cite{sara2019image}, structural similarity index (SSIM)~\cite{wang2004image} and Learned Perceptual Image Patch Similarity (LPIPS)~\cite{zhang2018unreasonable}.
To further evaluate the video fidelity of animatable human videos produced from these methods, we follow~\cite{shao2024360} to report Fr√©chet Video Distance (FVD)~\cite{unterthiner2018towards}.
As the multi-view RenderPeople released from~\cite{hu2023sherf} does not contain animatable human videos, we omit the FVD metric in this dataset.

\begin{figure*}[t]
    \centering
    % \vspace{-5mm}
    \includegraphics[width=16cm]{figures/main_fig.pdf}
    \setlength{\abovecaptionskip}{0cm}
    \caption{Qualitative results of novel view synthesis (1st and 3rd row) and novel pose synthesis (2nd and 4th row) produced by SHERF, Animate Anyone, Champ and our \nickname{} on RenderPeople and DNA-Rendering datasets.} 
\label{fig: main_fig}
\vspace{-2mm}
\end{figure*}

\subsection{Quantitative Results}
As shown in Tab.~\ref{tab: main_result}, \nickname{} outperforms baseline methods in perceptual quality metrics (LPIPS) across both datasets.
While SHERF, the state-of-the-art generalizable animatable Human NeRF method for single-view image input, achieves the highest PSNR scores in both novel view and pose synthesis tasks, it often produces blurred images, particularly for unseen views and poses. 
This is attributed to its limited capability to infer missing information from a single input image, which explains its subpar performance in perceptual quality metrics (LPIPS).
Animate Anyone and Champ, 2D SOTA character animation methods, demonstrate reasonably good perceptual quality in unseen views and poses by leveraging the generative prior of Stable Diffusion.
However, they fail to capture detailed information from the input image.
Meanwhile, these methods fail to produce consistent novel view and pose results.
In contrast, our \nickname{} leverages generative prior, a Human NeRF module, and a multi-view and temporal attention mechanism, enabling it to effectively learn fine-grained details from the input image and produce consistent novel view 
and pose results, even in the single-image setting.
Beyond image quality metrics, we also evaluate video fidelity for animatable human videos generated by these methods. 
Our \nickname{} achieves the best FVD metric with a temporal-attention module, demonstrating superior temporal consistency. At the same time, SHERF performs reasonably well in video fidelity due to its effective LBS modeling.
We incorporate a user study in Fig.~\ref{fig:user_study} to show human perceptual results.


\begin{table*}[t]
    \setlength{\abovecaptionskip}{0cm}
    \caption{Ablation study on DNA-Rendering. The left side shows different design components that are ablated on.}
    \centering
    \label{tab:ablation}
    \small
    \setlength{\tabcolsep}{1.2mm}{
    \begin{tabular}{ccccc|ccccccc}
        \toprule
        \multirow{2}*{\makecell{Generative \\ Prior}} & \multirow{2}*{\makecell{Human NeRF \\ Module}} & \multirow{2}*{\makecell{Image-Level \\ Loss}} & \multirow{2}*{\makecell{Multi-View \\ Attention}} & \multirow{2}*{\makecell{Motion \\ Attention}} & \multicolumn{3}{c}{Novel View} & \multicolumn{3}{c}{Novel Pose} \\
        \cmidrule{6-12}
        ~ & ~ & ~ & ~ & ~ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & FVD$\downarrow$\\
        \midrule
         & & & & & 21.657 & 0.921 & 0.069 & 20.874 & 0.910 & 0.076 & 15.38 \\
        $\checkmark$ & & & & & 23.684 & 0.860 & 0.056 & 23.080 & 0.870 & 0.061 & 17.96 \\
        $\checkmark$ & $\checkmark$ & & & & \textbf{24.026} & 0.923 & 0.050 & \textbf{24.598} & 0.923 & 0.050 & 11.72 \\
        $\checkmark$ & $\checkmark$ & $\checkmark$ & & & 22.731 & 0.927 & 0.050 & 23.376 & 0.930 & 0.049 & 11.59 \\
        $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & & 23.647 & \textbf{0.935} & 0.049 & 23.504 & 0.931 & 0.048 & 11.24 \\
        $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & 23.686 & \textbf{0.935} & \textbf{0.047} & 24.275 & \textbf{0.935} & \textbf{0.045} & \textbf{9.88}\\
        \bottomrule
    \end{tabular}}
\end{table*}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/userstudy_2_2.pdf}
    \vspace{-6pt}
    \setlength{\abovecaptionskip}{0cm}
    \caption{User preference scores on synthesis results.}
    \label{fig:user_study}
% \vspace{-4mm}
\end{figure}

\subsection{Qualitative Results}
We show qualitative results of novel view synthesis (1st and 3rd row) and novel pose synthesis (2nd and 4th row) of our \nickname{} and baseline methods in Fig.~\ref{fig: main_fig}. 
SHERF produces reasonable RGB renderings for the part visible from the input image, but it struggles to get realistic results for the part invisible from the input image.
For example, it produces blurry images in the back view when given a front view image.
Thanks to the generative prior involved in our \nickname{}, we can produce realistic outputs (\eg, back view in the 3rd row of Fig.~\ref{tab: main_result}) in unseen views and poses.
2D character animation methods, Animate Anyone and Champ, produce realistic results, but they fail to recover fine-grained details (\eg, patterns on cloths and eyeglasses) from the input image.
By incorporating the Human NeRF module to enhance the information from the input 2D observation, our \nickname{} successfully recovers the detailed information from the input image.

\begin{figure}[t]
    \centering
    % \vspace{-6mm}
    \includegraphics[width=9cm]{figures/ablation_figure.pdf}
    \setlength{\abovecaptionskip}{0cm}
    \caption{Qualitative results of ablation study on DNA-Rendering dataset.} 
    \label{fig:ablation_fig}
\vspace{-4mm}
\end{figure}

\subsection{Ablation Study}
To validate the effectiveness of the proposed components, we subsequently integrate them and evaluate their performance on the DNA-Rendering dataset.
As shown in Tab.~\ref{tab:ablation} and Fig.~\ref{fig:ablation_fig}, training \nickname{} without inheriting the generative prior (pre-trained weights) from Stable Diffusion results in distorted human images with incorrect textures. 
By incorporating the generative prior, \nickname{} generates realistic human images while preserving the identity of the input. 
However, it still struggles to capture fine-grained details from the input image.
For instance, high-heeled shoes are mistakenly added to bare feet, and the garment style in the back view differs from that in the front view (see Fig.~\ref{fig:ablation_fig}). 
Incorporating the Human NeRF module enables \nickname{} to learn detailed information from the input image, effectively addressing these issues.
Additionally, introducing the image-level loss enhances the model's ability to produce consistent results. 
For example, pants are corrected to a skirt in the generated output.
The multi-view attention considered in this paper forces the model to learn view-consistent results (\eg, the hair in Fig.~\ref{fig:ablation_fig}). 
Furthermore, the multi-view attention mechanism ensures the model learns view-consistent features, such as maintaining consistent hairstyles across views (as seen in Fig.~\ref{fig:ablation_fig}). Lastly, integrating the temporal module refines the model's ability to generate smooth novel pose sequences and resolves issues related to inconsistent human poses.






