\section{Introduction}

Synthesizing 3D human performers with consistent novel views and poses holds extensive utility across various domains, including AR/VR, video games, and movie production.
Recent methods enable the novel view and pose synthesis of 3D human avatars from sparse-view human videos~\cite{neuralbody, chen2021animatable, peng2022animatable, xu2021h, noguchi2021neural, weng_humannerf_2022_cvpr, su2021nerf, jiang2022selfrecon, jiang2022neuman, wang2022arah, hu2024gauhuman, chen2023primdiffusion, hong2022eva3d, hu2023humanliff, hong2022avatarclip}, 
with Neural Radiance Field~\cite{mildenhall2021nerf} or Gaussian Splatting-based~\cite{kerbl20233d} representation. 
While impressive novel view and pose synthesis results can be achieved from sparse-view videos, generating perpetually realistic, view-consistent, and temporally coherent human avatars from a single image~\cite{hu2023sherf, dong2023ivs, weng2023zeroavatar, liao2023high, huang2023tech, pan2024humansplat, wang2024geneman, ho2024sith, chen2024generalizable, cha2023generating, casas2023smplitex} remains challenging due to the limited information available in this setting.

To address the challenge of complementing information missing from a single image, a line of research~\cite{weng2023zeroavatar, huang2023tech, pan2024humansplat, casas2023smplitex} focuses on the novel view synthesis from a single human image by complementing human images or UV textures at unseen views (\eg, back view) through additional models (\eg, T2I diffusion models).
To further learn view-consistent and temporally aligned human avatars from a single image, another line of research~\cite{hu2023sherf, shao2024360} proposes synthesizing novel views and poses from a single image through learning a generalizable HumanNeRF or a human diffusion model from scratch.
However, such frameworks normally require multi-view human image collections as training datasets, and their performance is closely tied to the quality and scale of these datasets.
While efforts to create high-quality multi-view human image datasets~\cite{tao2021function4d, han2023high, cheng2023dna, li2021ai, cai2022humman, peng2021neural, ho2023learning, tang2023human, alldieck2018video, ionescu2013human3, mono-3dhp2017, peng2022animatable, habermann2020deepcap, habermann2021real, huang2024wildavatar, icsik2023humanrf, yang2023synbody} have accelerated in recent years, the number of human subjects in these datasets remains significantly smaller compared to multi-view object and scene image datasets~\cite{deitke2023objaverse,wu2023omniobject3d,yu2023mvimgnet,tung2025megascenes, dai2017scannet, zhou2018stereo}. 
This data disparity limits the generalization performance of single-image-based 3D human modeling frameworks.

To mitigate this data-sparsity dilemma, recent research~\cite{bhunia2023person, karras2023dreampose,wang2023disco, chang2023magicpose, xu2024magicanimate,hu2024animate,zhu2024champ,zhang2024mimicmotion,kim2024tcan, liyuan2024cfsynthesis} in 2D character animation involves generative prior (inherit pre-trained weights) from Text-to-image (T2I) diffusion models (\eg, Stable Diffusion~\cite{rombach2021highresolution}) to assist the single-view based 2D human animation (novel pose synthesis).
By inheriting the generative priors from diffusion models, these approaches demonstrate impressive generalization performance for novel human subjects and poses in 2D character animation tasks (e.g., dancing video generation) using only a modest amount of training data, such as hundreds of monocular dancing videos~\cite{jafarian2021learning}.
Motivated by these successes, it is desirable to explore the use of generative prior from T2I diffusion models in the single-view-based 3D human novel view/pose synthesis task.

In this work, we propose \textbf{\nickname}, a single-view conditioned human diffusion with generative prior. 
Specifically, we formulate the single-view-based 3D human novel view and pose synthesis as a single-view-conditioned human diffusion process, utilizing generative priors from foundational diffusion models to complement the missing information. 
Yet, this promising avenue comes with two primary barriers: 
1) How to integrate the relative camera and human pose information into the human diffusion framework?
2) How to ensure that the performance achieved in the latent space is equally effective in the image-level space for human diffusion models?
In particular, one potential solution is to allow diffusion models to learn relative camera and human pose transformation information through cross-attention between the reference image and the target pose image.
However, our experiments reveal that human diffusion models struggle with a) learning detailed information (e.g., logos on T-shirts) even when such details are present in the reference image, and b) achieving view-consistent and temporally aligned results.
To tackle the challenges, we draw inspiration from prior 3D reconstruction methods~\cite{hu2023sherf, wu2024reconfusion} and introduce a Human NeRF module~\cite{hu2023sherf} to capture the explicit fine-grained features in the target space by transforming the human subject from the reference pose space to the target pose space using linear blend skinning (LBS) of SMPL~\cite{SMPL:2015} and integrating them with volume rendering.
With the SMPL LBS transformation, the resulting rendered feature map provides a spatially aligned conditioning signal that encodes the relative camera and human pose transformation.
To further enhance the view and temporal consistency, we incorporate the multi-view attention and temporal attention in our \nickname{}.
Additionally, we observe a discrepancy between the latent space and image space for human images, attributed to the Variational Autoencoder (VAE)~\cite{kingma2019introduction} used in diffusion models.
Inspired by the Radiance field rendering loss employed in 3D object diffusion models~\cite{muller2023diffrf}, we construct an image-level loss by mapping the noise latent to the image space through a VAE decoder.
This ensures consistent optimizations in both latent and image spaces.
With the above modules incorporated, our \nickname\ successfully recovers fine-grained information from the reference image and achieves the best perceptually realistic, view-consistent, and temporally coherent results.
Our main contributions are as follows:
\begin{enumerate}
    \item We introduce \nickname{}, a single-view-conditioned human diffusion model that incorporates generative priors to compensate for information missing from the single input image.
    \item To learn perpetually realistic, view-consistent, and temporally aligned 3D human avatars, we integrate a Human NeRF module with multi-view attention and temporal attention. 
    \item To bridge the gap between latent and image spaces, we propose an image-level loss, which decodes the diffused latent space into image space during training, ensuring consistent optimization across both domains.
    \item Extensive experiments on two human datasets demonstrate that \nickname{} outperforms baseline methods in perceptual quality of novel view and pose synthesis.
\end{enumerate}







