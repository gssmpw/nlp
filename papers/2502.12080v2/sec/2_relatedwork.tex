\section{Related Work}

\noindent \textbf{Diffusion Models.}
Diffusion models~\cite{sohl2015deep, ho2020denoising} have demonstrated remarkable performance in image synthesis tasks, especially for text-to-image (T2I) generation~\cite{balaji2022ediff, nichol2021glide, saharia2022photorealistic, ramesh2022hierarchical, huang2023composer, rombach2022high}.
To improve sample quality for conditional generation, classifier guidance~\cite{dhariwal2021diffusion} and classifier-free guidance~\cite{ho2022classifier} are introduced by leveraging an explicitly trained classifier or score estimates from both conditional and unconditional generation. 
In this work, we propose utilizing the generative prior from latent diffusion models (e.g., Stable Diffusion~\cite{rombach2022high}) for novel view and pose synthesis of 3D humans from a single image.

\noindent \textbf{Novel View/Pose Synthesis from a Single Human Image.}
Although 2D human modelling~\cite{albahar2021pose, lewis2021tryongan, sarkar2021humangan, men2020controllable, fu2022stylegan, jiang2022text2human, jiang2023text2performer, fu2023unitedhuman} has made substantial progress, it is still challenging to synthesizing 3D humans from monocular inputs, especially for a single human image input~\cite{saito2019pifu, saito2020pifuhd, he2020geo, li2020robust, dong2022pina, li2020monocular, bozic2021neural, yang2021s3, bhatnagar2020combining, bhatnagar2020loopreg, huang2020arch, he2021arch++, zheng2021pamir, xiu2022icon, xiu2022econ, alldieck2022photorealistic, corona2022structured}.
To complement the information missing from a single image input, these approaches~\cite{weng2023zeroavatar, huang2023tech, pan2024humansplat, wang2024geneman, chen2024generalizable, choi2022mononhr, zhang2024sifu, ho2024sith, dong2023ivs, cha2023generating, casas2023smplitex} typically leverage text-to-image (T2I) diffusion models, such as Stable Diffusion~\cite{rombach2021highresolution}, by employing subject-specific Score Distillation Sampling (SDS)~\cite{poole2022dreamfusion} or fine-tuning a T2I diffusion model conditioned on a front-view image using multi-view human datasets or 3D scan datasets. 
This enables the synthesis of human images from unseen viewpoints, such as generating a back view from a given front view.
To further learn view-consistent and temporally aligned human avatars from a single image, recent research~\cite{hu2023sherf, shao2024360} leverages the strong modeling capabilities of deep neural networks to learn generalizable features for Neural Radiance Field (NeRF) or diffusion models, enabling the synthesis of novel views and poses in a feed-forward manner. 
However, the scale of multi-view human datasets limits their generalization ability in novel view and pose synthesis.
In this work, we propose to utilize the generative prior from diffusion models to produce perpetually realistic, view-consistent, and temporally coherent human avatars from a single image.

\noindent \textbf{Diffusion Models for 2D Character Animation.}
2D Character Animation aims at generating temporally coherent animation videos from one or more static human images~\cite{albahar2023single, cao2024dreamavatar, chan2019everybody, fu2022stylegan, jiang2023humangen, prokudin2021smplpix, ren2020deep, sarkar2020neural, Siarohin_2019_NeurIPS, siarohin2021motion, yoon2021pose, yu2023bidirectionally, zhang2022exploring, zhao2022thin}.
With advancements in text-to-image diffusion models, recent studies~\cite{bhunia2023person, karras2023dreampose, wang2023disco, chang2023magicpose, xu2024magicanimate, hu2024animate, zhu2024champ, zhang2024mimicmotion, kim2024tcan, liyuan2024cfsynthesis} have investigated the use of diffusion models for 2D character animation.
These approaches normally use a reference human image and a sequence of target pose images (\eg, OpenPose~\cite{8765346}, DWPose~\cite{yang2023effective}, DensePose~\cite{guler2018densepose}, or SMPL pose~\cite{SMPL:2015}) as conditional inputs to pose encoders or ControlNet~\cite{zhang2023adding}, generating a sequence of target images aligned with the reference human image and target pose images.
Motivated by the success of diffusion-based methods in 2D character animation, we adapt these methodologies for 3D/4D human modeling from a single image. Human4DiT~\cite{shao2024360} introduced a diffusion model that generates multi-view human animation videos from a single image, utilizing factorized image, view, and temporal modules, similar to our approach. However, the code for this model is not publicly available. We identified that our baseline multi-view video diffusion model could not capture fine-grained details and maintain multi-view consistency. 
We show that integrating generative prior and our Human NeRF module into the diffusion pipeline effectively resolves these challenges.
Our concurrent work GAS~\cite{lu2025gas} also utilizes diffusion models to enhance the novel view and pose synthesis performance from a Human NeRF module.
