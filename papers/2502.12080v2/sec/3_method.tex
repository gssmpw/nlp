\section{Our Approach}
Our proposed \nickname, as illustrated in Fig.~\ref{fig: overview}, learns a single-view conditioned human diffusion model for novel view and pose synthesis by utilizing generative prior from Stable Diffusion.
Specifically, given a reference human image and a target human pose sequence (estimated from a human video or generated from other modalities like text, or audio), \nickname\ aims to predict a sequence of human pose images aligned with the person in the reference image and motions observed in the target human pose sequence. 
We assume the camera parameters are pre-calibrated and the human region masks can be estimated. 
We also assume the corresponding SMPL pose and shape parameters $\bm{\theta}, \bm{\beta}$ are given.

\begin{figure*}[t]
    \centering
    \vspace{-6mm}
    \includegraphics[width=16cm]{figures/Overview.png}
    \setlength{\abovecaptionskip}{0cm}
    \caption{\textbf{\nickname{} Framework}. Given a single input human image and a target pose sequence, our \nickname{} produces a sequence of target images aligned with the human subject in the input image and target human poses. To synthesize view-consistent and pose-consistent outputs, our \nickname{} proposes to incorporate generative prior, a Human NeRF module, image-level loss, multi-view attention, and temporal attention.} 
\label{fig: overview}
\vspace{-2mm}
\end{figure*}

\subsection{Preliminary}
\label{sec:preliminary}

\noindent \textbf{Latent Diffusion Model (LDM)}~\cite{rombach2022high} presents a novel class of diffusion models by integrating two distinct stochastic processes, \ie, diffusion and denoising, directly within the latent space.
LDM learns a variational autoencoder (VAE)~\cite{kingma2013auto, van2017neural} to establish the mapping from image space to latent space, reducing the diffusion model's complexity.
Specifically, give an image $\bm{x}$, the encoder maps the image to a latent representation $\bm{z}_0=\mathcal{E}(\bm{x})$ and the decoder reconstructs it to image space $\bm{x}=\mathcal{D}(\bm{z}_0)$. 
The diffusion process progressively adds Gaussian noise to the data $\bm{z}_0$ following a variance schedule $1-\alpha_{0}, \dots, 1-\alpha_{T}$ specified for different time steps, \ie,
\begin{equation}
\label{eqn:diffusion_process}
    \begin{aligned}
        \bm{z}_{t}=\sqrt{\alpha_t}\bm{z}_{t-1} + \sqrt{1-\alpha_t}\bm{\epsilon}.
    \end{aligned}
\end{equation}
% where $\alpha_t:=1-\beta_{t}$. 
With a sufficiently large number of steps $T$, the diffusion process converges to $\bm{z}_T\sim \mathcal{N}(\bm{0}, \bm{I})$. 

The denoising process learns to denoise $\bm{z}_{t}$ to $\bm{z}_{t-1}$ by predicting the noise $\bm{\epsilon}_{\Phi}(\bm{z}_{t}, t, \bm{c})$ for each denoising step, where $\bm{\epsilon}_{\Phi}(\bm{z}_{t}, t, \bm{c})$ is the output from a neural network (\eg, UNet~\cite{ronneberger2015u}) and $\bm{c}$ denotes conditional signal, \eg, the text embedding from CLIP~\cite{radford2021learning} model.
The diffusion loss $\mathcal{L}_\text{diff}$ is constructed by calculating the expected mean squared error (MSE) over the actual noise $\bm{\epsilon}$ and predicted noise $\bm{\epsilon}_{\Phi}(\bm{z}_{t}, t, \bm{c})$, \ie, 
\begin{equation}
\label{eqn:denoising_process}
    \begin{aligned}
        \mathcal{L}_\text{diff-latent}=\mathbb{E}_{\bm{z}_{t}, \bm{\epsilon}, t, \bm{c}}[w_t\Arrowvert\bm{\epsilon}_{\Phi}(\bm{z}_{t}, t, \bm{c}) - \bm{\epsilon}\Arrowvert^{2}],
    \end{aligned}
\end{equation}
where $w_t$ is the weighting of the loss at time step $t$.


\noindent \textbf{SMPL}~\cite{SMPL:2015} is a parametric human model, $M(\bm{\beta}, \bm{\theta})$, where $\bm{\beta}, \bm{\theta}$ control body shape and pose, respectively. 
In this work, we utilize the Linear Blend Skinning (LBS) algorithm employed in SMPL to transform points from canonical to posed spaces.
For instance, a 3D point $\bm{p}^c$ in the canonical space is transformed into the posed space defined by pose $\bm{\theta}$ as $\bm{p}^\text{tgt} = \sum_{k=1}^{K}w_{k}(\bm{G}_k(\bm{J}, \bm{\theta})\bm{p}^c+\bm{b}_k(\bm{J}, \bm{\theta}, \bm{\beta}))$, where $\bm{J}$ represents $K$ joint locations, $\bm{G}_k(\bm{J}, \bm{\theta})$ is the transformation matrix of joint $k$, $\bm{b}_k(\bm{J}, \bm{\theta}, \bm{\beta})$ is the translation vector of joint $k$, and $w_{k}$ is the linear blend weight.

\noindent \textbf{NeRF}~\cite{mildenhall2021nerf} learns an implicit, continuous function that maps the 3D location $\bm{p}$ and unit direction $\bm{d}$ of a point to its volume density $\bm{\sigma} \in [0, \infty)$ and color value $\bm{c}\in [0,1]^3$, i.e., $F_{\Phi}: (\gamma(\bm{p}), \gamma(\bm{d})) \to (\bm{c}, \bm{\sigma})$, where $F_{\Phi}$ is parameterized by a multi-layer perceptron (MLP) network, $\gamma$ denotes a predefined positional embedding applied to $\bm{p}$ and $\bm{d}$.
To render the RGB color of pixels in the target view, rays are cast from the camera origin $\bm{o}$ through the pixel along the unit direction $\bm{d}$. 
Based on the classical volume rendering~\cite{kajiya1984ray}, the expected color $\hat{C}(\bm{r})$ for a camera ray $\bm{r}(t) = \bm{o} + t\bm{d}$ is computed as 
\begin{equation}
\label{eqn:volume_rendering}
\begin{aligned}
\hat{C}(\bm{r})=\int_{t_n}^{t_f} T(t) \sigma(\bm{r}(t)) \bm{c}(\bm{r}(t), \bm{d})dt,
\end{aligned}
\end{equation}
where $T(t)=\exp(-\int_{t_n}^{t}\sigma(\bm{r}(s))ds)$ denotes the accumulated transmittance along direction $\bm{d}$ from near bound $t_n$ to current position $t$, and $t_f$ represents the far bound.
In practice, the continuous integral is approximated using the quadrature rule~\cite{max1995optical}, which reduces to traditional alpha compositing.




\subsection{Single-view conditioned Human Diffusion Model}
\label{sec:architecture}
Motivated by the success of leveraging generative priors from T2I diffusion models in the 2D character animation~\cite{hu2024animate}, we propose utilizing generative priors for single-view-based 3D human novel view/pose synthesis tasks. 
Specifically, we reformulate the single-view-based 3D human novel view and pose synthesis as a single-view-conditioned human diffusion process, utilizing generative priors from foundational diffusion models to complement the missing information.

\noindent \textbf{Denoising UNet.} 
Our backbone, illustrated in Fig.~\ref{fig: overview}, is a denoising U-Net that inherits both the architecture and pre-trained weights (generative prior) from Stable Diffusion (SD) 1.5~\cite{rombach2022high}.
The vanilla SD UNet in Stable Diffusion comprises three main components: downsampling, middle, and upsampling blocks.
Each block integrates multiple interleaved layers, including convolution layers for feature extraction, self-attention layers for spatial feature aggregation, and cross-attention layers that interact with CLIP text embeddings to guide the denoising process.
% While the denoising U-Net in \nickname{} is required 
To process multiple noise latents to produce human images that align with the subject in the given reference image and the specified target poses, we further incorporate the reference image and the target pose sequence through the following modules.
% a Reference UNet, a Pose Encoder Network, and a Human NeRF module.

\noindent \textbf{ReferenceNet.}
Building on recent advances in 2D character animation~\cite{hu2024animate}, as illustrated in Fig.~\ref{fig: overview}, we integrate a copy of the SD denoising U-Net as the ReferenceNet to extract features from the reference image.
Specifically, we replace the self-attention layer with a spatial-attention layer, enabling self-attention on the concatenated features from the denoising U-Net and ReferenceNet. 
Additionally, we incorporate a CLIP encoder to extract semantic features, which are fused with the features from the denoising U-Net using cross-attention modules.


\noindent \textbf{Pose Encoder.}
There are various options for defining target poses, including OpenPose, DensePose, DWPose, and SMPL parametric poses.
We follow~\cite{zhu2024champ} to estimate SMPL mesh for the reference image, animate the SMPL mesh with the target SMPL pose parameters, and render normal images as the target guidance signal. 
The SMPL normal pose images are encoded through a Pose Encoder~\cite{hu2024animate}, which contains four convolution layers. 
The Pose Encoder output is added to the noise latent to provide view and pose guidance information.

\begin{table*}[h]
\vspace{-5mm}
\setlength{\abovecaptionskip}{0cm}
\caption{Quantitative comparison of our \nickname{} and baseline methods on the RendePeople and DNA-Rendering datasets.
}
\centering
\label{tab: main_result}
\begin{tabular}{l|cccc|cccc|c}
\toprule
\multirow{3}*{Method} & \multicolumn{9}{c}{RenderPeople} \\
\cline{2-10}
~ & \multicolumn{4}{c|}{Novel View} & \multicolumn{4}{c|}{Novel Pose} \\
~ & L1$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & L1$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & - \\
\midrule
SHERF~\cite{hu2023sherf} & \underline{9.75E-06} & \textbf{26.128} & \underline{0.934} & \underline{0.063} & \textbf{7.48E-06} & \textbf{27.435} & \underline{0.946} & 0.048 \\
Animate Anyone~\cite{hu2024animate} & 2.32E-05 & 21.022 & 0.929 & 0.064 & 1.35E-05 & 24.306 & \underline{0.946} & 0.041 & - \\
Champ~\cite{zhu2024champ} & 2.42E-05 & 21.326 & 0.930 & 0.064 & 1.34E-05 & 25.381 & \textbf{0.952} & \underline{0.037} & - \\
\textbf{\nickname \:(Ours)} & \textbf{9.47E-06} & \underline{25.110} & \textbf{0.951} & \textbf{0.037} & \underline{8.98E-06} & \underline{25.440} & \textbf{0.952} & \textbf{0.034} & - \\
\bottomrule
\multirow{3}*{Method} & \multicolumn{9}{c}{DNA-Rendering} \\
\cline{2-10}
~ & \multicolumn{4}{c|}{Novel View} & \multicolumn{4}{c|}{Novel Pose} \\
~ & L1$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & L1$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ &  FVD$\downarrow$ \\
\midrule
SHERF~\cite{hu2023sherf} & \textbf{3.77E-06} & \textbf{27.581} & \textbf{0.948} & \underline{0.049} & \textbf{3.99E-06} & \textbf{27.471} & \textbf{0.944} & \underline{0.048} & 12.01 \\
Animate Anyone~\cite{hu2024animate} & 6.20E-06 & 23.499 & 0.902 & 0.056 & 6.84E-06 & 23.043 & 0.907 & 0.061 & 16.22\\
Champ~\cite{zhu2024champ} & 6.37E-06 & \underline{23.715} & 0.869 & 0.054 & 6.96E-06 & 23.253 & 0.879 & 0.058 & 16.89 \\
\textbf{\nickname \:(Ours)} & \underline{5.82E-06} & 23.686 & \underline{0.935} & \textbf{0.047} & \underline{5.67E-06} & \underline{24.275} & \underline{0.935} & \textbf{0.045} & \textbf{9.88} \\
\bottomrule
\end{tabular}
% \vspace{-4mm}
\end{table*}

\noindent \textbf{Human NeRF.}
The proposed framework aims to generate consistent novel views and poses from a single human image. 
However, we observe two challenges in the learned human diffusion models: (a) difficulty in capturing fine-grained details (e.g., logos on T-shirts), even when they are present in the reference image, and (b) limited capability to maintain consistency across multiple views and poses.
One underlying reason for the challenges is that the reference image is first encoded into a latent representation and then processed by ReferenceNet to extract features, resulting in information loss during this process.
To tackle this challenge, inspired by prior research in Human NeRF~\cite{peng2021neural, hu2023sherf, chen2021animatable, animatablenerf, xu2021h, noguchi2021neural, weng2022humannerf, su2021nerf, jiang2022selfrecon, jiang2022neuman, wang2022arah, kwon2021neural, gao2022mps, choi2022mononhr, zhao2021humannerf, huang2022elicit}, we integrate a Human NeRF module into human diffusion models.
This module learns fine-grained features that are spatially aligned with both the reference and target pose space, enhancing detail preservation and consistency.

As illustrated in Fig.~\ref{fig: overview}, the input to the Human NeRF module is a single human image $\mathbf{I}^\text{ref}$ along with its corresponding camera parameters $\bm{P}^\text{ref}$ and SMPL pose parameter $\bm{\theta}^\text{ref}$ and shape parameter $\bm{\beta}^\text{ref}$.
The module outputs the rendered human feature image in the target camera view $\bm{P}^\text{tgt}$, corresponding to the target SMPL pose $\bm{\theta}^{tgt}$ and shape $\bm{\beta}^\text{tgt}$.
% To render features in the target space, 
% points $\bm{x}^\text{tgt}$ are first sampled along the cast rays.
Specifically, in the target space, we cast rays passing through the camera origin and image pixels, and sample points $\bm{x}^\text{tgt}$ along the cast rays. 
These points $\bm{x}^\text{tgt}$ are transformed into the canonical space $\bm{x}^{c}$ using inverse Linear Blend Skinning (LBS).
Subsequently, hierarchical 3D-aware features,\ie, point-level features and pixel-aligned features, are queried from their respective feature extraction modules.
The queried features are concatenated and passed to the NeRF decoder to predict the density $\sigma$ and feature $\bm{c}$ for each sampled point. 
The final pixel features are rendered in the target space through volume rendering, integrating the density and feature values of the sampled 3D points along the rays in the target space.

The details of extracting point-level features and pixel-aligned features are described as follows.
1) Point-level Features. 
We first extract per-point features by projecting the SMPL vertices onto the 2D feature map of the input image, as illustrated in Fig.~\ref{fig: overview}.
Next, we apply inverse LBS to transform the posed vertex features into the canonical space. 
These transformed features are then voxelized into sparse 3D volume tensors and further processed using sparse 3D convolutions~\cite{spconv2022}. 
From the encoded sparse 3D volume tensors, we extract point-level features $\bm{f}_\text{point}(\bm{x}^{c})$ for each point $\bm{x}^{c}$.
With the awareness of 3D Human structure, point-level features capture local texture details in the seen area and infer textural information for unseen areas through sparse convolution.
2) Pixel-aligned Features. 
Due to limited SMPL mesh and voxel resolution, point-level features suffer from significant information loss, especially in areas visible from the reference image.
To compensate for the information loss problem, we additionally extract pixel-aligned features by projecting 3D deformed points $\bm{x}^{c}$ into the input view.
As illustrated in Fig.~\ref{fig: overview}, each deformed point $\bm{x}^{c}$ is transformed into the observation space as $\bm{x}^\text{ref} = \text{LBS}(\bm{x}^{c}; \bm{\theta}^\text{ref}, \bm{\beta}^\text{ref})$ using LBS. 
It is then projected onto the input view, allowing us to query the pixel-aligned features.
Leveraging the complementary strengths of point-level and pixel-aligned features, our Human NeRF module effectively captures fine-grained feature details in regions visible in the reference view while inferring features for regions occluded in the reference view.

\noindent \textbf{Multi-view/Temporal Module.}
To learn view-consistent and temporal coherent 3D human avatars, we further integrate a view-attention and temporal attention layer after the spatial-attention and cross-attention components within the Res-Trans block of Denosing UNet. 
Inspired by the efficient temporal attention layer adopted in~\cite{hu2024animate, guo2023animatediff}, we utilize the same architecture for our view-attention and temporal attention layer.
The view-attention layer performs the attention in the view channel while the temporal attention layer performs the attention in the human pose channel.

\subsection{Training Detail}
\label{sec:training_detail}

Our training objective $\mathcal{L}$ comprises three components: 1) $\mathcal{L}_\text{diff-latent}$, as shown in Eqn.~\ref{eqn:diffusion_process}, which aligns the learned latent space with the data distribution, 2) $\mathcal{L}_\text{diff-img}$, which focuses on enhancing the quality of the decoded image from the latent space, and 3) $\mathcal{L}_\text{NeRF}$, which regularizes the learned image features using images and human masks from the target space, \ie, 
\begin{equation}
% \setlength{\abovedisplayskip}{5pt} 
% \setlength{\belowdisplayskip}{5pt}
\label{loss: overall}
\begin{aligned}
\mathcal{L} = \mathcal{L}_\text{diff-latent} + \lambda_{1}\mathcal{L}_\text{diff-img} + \lambda_{2}\mathcal{L}_\text{NeRF},
\end{aligned}
\end{equation}
where $\lambda_1$ and $\lambda_2$ are loss weights.

To bridge the gap between latent space and image space~\cite{muller2023diffrf}, we formulate $\mathcal{L}_\text{diff-img}$ with approximation $\bm{\tilde{z}}_{0}(\bm{\epsilon},\Phi):= \bm{z}_{0} + \frac{\sqrt{1-\bar{\alpha}_{t}}}{\sqrt{\bar{\alpha}_{t}}} (\bm{\epsilon} - \bm{\epsilon}_{\Phi}(\bm{z}_{t}, t, \bm{c}))$,
\begin{equation}
\label{eqn:diff_img_loss}
    \begin{aligned}
        \mathcal{L}_\text{diff-img}=w_t\mathbb{E}_{\bm{z}_{t}, \bm{\epsilon}, t, \bm{c}}\Arrowvert\mathcal{D}(\bm{\tilde{z}}_{0}(\bm{\epsilon},\Phi)) - \bm{I}^\text{tgt} \Arrowvert^{2},
    \end{aligned}
\end{equation}
where $\bar{\alpha}_{t}=\prod_{i=1}^{t}\alpha_i$, $\bm{I}^\text{tgt}$ is the image at the target view with the target human pose, $\mathcal{D}$ denotes the decoder in a VAE~\cite{kingma2013auto, van2017neural} model.
Since the approximation holds reliably only for steps $t$ close to zero, we introduce a weight $w_t$ that progressively decays as the step value increases. 
In addition to MSE loss, we apply structural similarity index (SSIM)~\cite{wang2004image} and Learned Perceptual Image Patch Similarity (LPIPS)~\cite{zhang2018unreasonable} as additional image-level loss terms.
For features from Human NeRF, we follow~\cite{chan2022efficient} to add a term $\mathcal{L}_\text{NeRF}$, which regularizes the first three channels by computing MSE, SSIM, and LPIPS with the target image $\bm{I}^\text{tgt}$ and a Mask Loss with the target human mask.
