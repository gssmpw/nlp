\section{Background and Related Work}
A handover task comprises multiple phases that humans perform seamlessly through collaboration. Both verbal and nonverbal communication are used to initiate the coordinated spatio-temporal movement of a giver and a taker. In a general handover, the giver determines a suitable location to transfer the object within the shared interpersonal space and starts moving the object towards that location \cite{human_human_to_human_robot_study_Controzzi} \cite{survey_review_2022_object_handovers}. The taker prejudges the handover location based on the giver's motion and approaches the location with their preferred hand. As the handover progresses, both the giver and taker adjust their arm speed based on visual sensory information. The coordinated movement ends with a light impact on the object as the taker begins to interact with it, forming a grip. During this interaction, the giver reduces its grip force on the object while the taker's grip force increases. Both parties share the responsibility of supporting the object's weight and maintaining its pose to prevent it from falling. The handover concludes when the giver releases the object completely after ensuring the taker has a stable grasp. Therefore, a robot must be competent in multiple phases of handover to achieve fluent exchanges.

Humans adapt swiftly to complex handover scenarios and different-weighted objects encountered in daily life, such as handing over a knife or a hammer with ease \cite{survey_review_2022_object_handovers, When_where_how_human-human_studyStrabala, object_orientation_dataset_chan}. There are scenarios where the weight of the object to be transferred differs from expectations, such as an unexpectedly heavy box. Human givers and takers quickly adapt to these uncertainties in weight. In certain cases, humans rely solely on haptic feedback to perform the handover \cite{dataset-khanna}. Even in these scenarios, both giver and taker adapt appropriately to ensure a safe and efficient handover. A social robot is likely to encounter such dynamic social environments and must adapt to ensure safe handovers while avoiding failures.

Interest in human-inspired handovers for robots has led to various studies on human handovers \cite{survey_review_2022_object_handovers}. These studies have explored human motion modeling, grip forces, and task-specific dynamics to inform robot designs.
\subsection{Datasets}
Several datasets support research in human-robot handovers. Carfi et al. \cite{dataset_Emaro_CARFI2019109} provided a dataset with 3D upper skeleton tracking using MoCap and smartwatch inertial data. The object orientation dataset by Chan \cite{object_orientation_dataset_chan} offers MoCap data of upper skeletons and object poses, aiding studies on proper object orientation during robot-to-human handovers. Precision grasp preferences were highlighted in a dataset by Cini et al. \cite{more_precision_type_grasp_type_location_study_dataset_cini_controzzi}. The H2O dataset presented by Ye et al. consists of annotated videos of handovers between 15 individuals for over 30 different objects, supporting vision-based tasks and providing insight into human-human handover dynamics. However, it lacked insights based on weight diversity. Other datasets, such as OHO \cite{stephan2023oho} and the Human-Object-Human (HOH) dataset \cite{wiederhold2023hoh}, provide multimodal data for handover analysis. The HOH dataset, encompassing 2,720 handovers involving 136 objects and 40 participants, offers comprehensive multi-modal data to analyze handover mechanics but does not explicitly include object weights. HandoverSim, a Python-based simulator introduced by Chao et al. \cite{chao2022handoversim}, enables controlled handover studies but does not fully capture real-world complexities. 
In our prior work \cite{dataset-khanna}, we recorded a first-of-its-kind multimodal dataset of human handovers, \textit{Handovers@RPL},  capturing both human motion and the forces involved in the transfer process. Thirteen pairs of participants passed an object embedded (0.8 kg) with force/torque (F/T) sensors in a motion-capture environment, allowing us to measure grip forces for both the giver and taker, as well as interaction forces. We specifically investigated the effects of increasing the object's weight by 1 kg (1.8 kg), observing significant differences in key handover properties such as the time of transfer and time of grip-release.



\begin{table*}[t]
\centering
\caption{Summary of Handovers and YCB Handovers: Details of Objects in Different Baskets}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\multicolumn{6}{|c|}{\textbf{Summary of Handovers with Sensor-Embedded Baton}} \\ \hline
\textbf{Handover Type and Weight} & \textbf{No. of Handovers} & \textbf{Forces} & \textbf{Motion} & \textbf{Weight (kg)} & \textbf{ Object Details} \\ \hline
\textbf{New Data Handovers} & 3235 total & Yes & Yes & - & - \\ \hline
\hspace{0.5cm}  & 671 & Yes & Yes & 1.0 & New Data - Baton + 0.2 kg\\ 
\hspace{0.5cm}  & 654 & Yes & Yes & 1.2 & New Data - Baton + 0.4 kg \\ 
\hspace{0.5cm}  & 662 & Yes & Yes & 1.4 & New Data - Baton + 0.6 kg \\ 
\hspace{0.5cm}  & 601 & Yes & Yes & 1.6 & New Data - Baton + 0.8 kg \\ 
\hspace{0.5cm}  & 647 & Yes & Yes & 2.0 & New Data - Baton + 1.2 kg \\ \hline
\textbf{Previous Data Handovers} & 5763 total & Yes & Yes & - & - \\ \hline
\hspace{0.5cm}  & 2999 & Yes & Yes & 0.8 & Previous Data - Baton\\ 
\hspace{0.5cm}  & 2764 & Yes & Yes & 1.8 & Previous Data - Baton + 1.0 kg\\ \hline
\multicolumn{6}{|c|}{\textbf{YCB Handovers: Details of Objects in Different Baskets}} \\ \hline
\textbf{Basket} & \textbf{No. of Handovers} & \textbf{Forces} & \textbf{Motion} & \textbf{Weight (kg)} & \textbf{Object Details} \\ \hline
1 & 128 & No & Yes & 0.008 & Marker Small (MS) \\ 
  & 128  & No & Yes & 0.118 & Mug (M) \\ 
  & 128  & No & Yes & 0.242 & Wrench (W) \\ 
  & 126  & No & Yes & 0.600 & Mustard Bottle (MB) \\ 
  & 128  & No & Yes & 1.131 & Cleanser Bottle (CB) \\ \hline
2 & 108 & No & Yes & 0.040 & Racquetball (R) \\ 
  & 106    & No & Yes & 0.190 & Jello-Choc Box (J) \\ 
  & 106    & No & Yes & 0.374 & Meat Can (MC) \\ 
  & 107   & No & Yes & 0.874 & Hand Drill (HD) \\ 
  & 107    & No & Yes & 1.020 & Spray Bottle (SB) \\ 
  & 103    & No & Yes & 1.450 & Earphone Cover with Weights (E) \\ \hline
3 & 95 & No & Yes & 0.055 & Spatula (SP) \\ 
  & 101  & No & Yes & 0.149 & Bowl (B) \\ 
  & 99  & No & Yes & 0.202 & Clamp (C) \\ 
  & 101  & No & Yes & 0.410 & Coffee Can (CC) \\ 
  & 100  & No & Yes & 0.728 & Wood Block (WB) \\ 
  & 100  & No & Yes & 1.300 & Black Box with Weights (BW) \\ \hline
4 & 99 & No & Yes & 0.015 & Large Marker (LM) \\ 
  & 98 & No & Yes & 0.095 & Screwdriver (SC) \\ 
  & 100 & No & Yes & 0.183 & Pitcher Base Y (P) \\ 
  & 101 & No & Yes & 0.514 & Sugar Box (SBX) \\ 
  & 100 & No & Yes & 0.608 & Hammer (H) \\ 
  & 85 & No & Yes & 0.925 & Skillet (SK) \\ \hline
5 & 91 & No & Yes & 2.060 & Pitcher with Added Weights (Heavy Weight-Not Careful) \\ 
  & 69 & No & Yes & 2.060 & Pitcher with Water (Heavy Weight-Careful) \\ 
  & 96 & No & Yes & 0.008 & Measuring Cup (Light Weight-Not Careful) \\ 
  & 61 & No & Yes & 0.048 & Measuring Cup with Water (Light Weight-Careful) \\ \hline
\end{tabular}
\label{tab:handover_summary}
\end{table*}


\subsection{Human Motion and Joint Coordination}
Several studies model human motion to inspire robotic arm movements.
To better understand verbal and non-verbal cues, as well as joint coordination, Strabala et al. \cite{When_where_how_human-human_studyStrabala} utilized multiple color and depth cameras to study human-human handovers. Rasch et al. \cite{human-like-motion-forR2H_Handovers_Rasch2018AJM} recorded human arm motions to develop a joint motion model for robotic givers. Another study employed an electromagnetic tracker and markers to evaluate human hand motions and inspire robot reaching profiles \cite{robot_reaching_profiles_4600651}. Aleotti et al. \cite{aleotti2014affordance} optimized object orientation during handovers. Chang \cite{chang2021learning} introduced a deep reinforcement learning approach for predicting human-preferred grasps, and Lori et al. \cite{iori2023dmp,perovic2023adaptive} used Dynamic Movement Primitives and Preference Learning for adaptive handover control. 
The effect of object weight on handover location and duration was investigated in \cite{h2H_handovers_how_dis_and_obj_mass_matter_Clint2017}, showing that only handover-duration was impacted. 

Studies have shown that humans adapt their movements when handling objects of different weights or contents. In human-robot handovers, enabling robots to infer an object's weight by observing human motion is essential for improving robotic preparedness.
It was demonstrated that humans can estimate object weight by observing both human and humanoid lifting actions, highlighting the potential for robots to assess weight through visual cues \cite{Sciutti2014_weight_motion}. 
An interesting finding in \cite{careful_handovers_Lastrico} indicated that humans exhibit careful motion in handovers, even during the reach-to-grasp phase when handling cups filled with water as opposed to empty cups. This carefulness can be reliably detected online before the handover action is completed. It was further proposed that robot motion could be modulated to express awareness of the object's properties, such as whether a cup is full or empty, thereby conveying important information to human partners in advance and potentially improving task efficiency in human-robot interaction scenarios.

Furthermore, \cite{humanoids_simulated_motion_objectweight2009} investigated how human-inspired handover movements can convey object weight in simulated exchanges between humanoid robots. Bimanual human handover movements involving objects of varying weights (2, 5, 10, and 15 kg) were analyzed and replicated in a simulation of two human-like robots. In a user study, participants watched the simulation videos and answered a questionnaire assessing their perception of object weight. Results showed that while participants could distinguish between heavy and light objects, estimating the exact weight remained challenging.

\subsection{Forces in Handovers and Grip Release}
For studying forces in handovers, \cite{chan_grip_from_load_second_PR2} used a baton with grip and load force sensors, developing a controller for robotic grip forces based on the measured relationship between grip and load forces in vertical transfer of the baton. Controzzi et al. \cite{human_human_to_human_robot_study_Controzzi} observed that taker arm speed influences the giver's grip forces.
Studies show that human grip-release actions typically occur within 500 milliseconds \cite{chan_grip_from_load_baton}, \cite{study_in_person_handover_for_baton}, \cite{human_human_to_human_robot_study_Controzzi}, necessitating rapid decision-making for robots. While vision-based systems detect human hand approach and grip, they are challenged by occlusions and the variability in human grasps \cite{many_grasp_types_7243327}, \cite{grasp_classification_vision_DBLP:conf/iros/YangPCF20}. Capacitive proximity sensors \cite{capacitive_sensors_grasp_detect_9560970} can overcome some of these limitations by detecting human grasps without visual input. In our prior work \cite{dataset-khanna}, we studied the effect of object weight on the grip release and the interaction forces in handovers. We find that a 1 kg increase in the weight of object caused a significant difference in the grip-release time, the transfer time and the interaction forces (Pull force).

The most common strategy for commanding grip release for a robotic giver in robot-to-human handovers is based on \textit{Pull-Force}.
Vertical handovers often rely on grip-release triggered by a slight upward pull force detected by wrist sensors \cite{chan_grip_from_load_baton}, \cite{chan_grip_from_load_second_PR2}, \cite{chan_grip_from_load_humanoid-6907004}. For horizontal handovers as well, grip-release is typically based on thresholding the measured external forces on the robot wrist \cite{human_human_to_human_robot_study_Controzzi}, with pull force, measured along the horizontal transfer direction, used commonly \cite{survey_review_2022_object_handovers}. In another study \cite{pull_proactive_strategy_8673085}, proactive grip-release strategies based on grip force disturbances during a human grasp and pull-force deliver the best results. 
Another grip-release strategy involves \textit{Load-Sharing}. As humans hand over an object, its weight decreases due to load-sharing between the giver and taker, culminating in a full transfer. In \cite{loadsharing_pull_strategy-10.3389/frobt.2021.672995, loadshare_strategy-7803296}, the robot giver gets ready for grip release when the taker's shared load reaches 50\%. However, the actual grip release is triggered by detecting a pull force threshold.

A human-inspired, data-driven grip release strategy was proposed in our prior work, which relied on interaction forces (including pull force and load share) measured during handovers. This innovative approach was developed to enhance the naturalness and efficiency of robot-to-human object transfers. Based on a comprehensive dataset of interaction and grip forces observed in human-human handovers, this strategy utilized a Long Short-Term Memory (LSTM) network, a  recurrent neural network known for its ability to process and predict based on time-series data.
The LSTM was trained on time series of interaction forces observed preceding the grip release by human givers, allowing it to learn the subtle cues and patterns that humans use to coordinate the grip-release and taking actions during handovers. In robot-to-human handovers, as a human taker begins to take the object, this strategy utilizes the trained LSTM to process real-time interaction forces measured by a wrist-mounted force/torque sensor. By analyzing these forces, the system can predict the appropriate moment for grip release, commanding the robotic giver to let go of the object in a manner that closely mimics human behavior. This approach not only improves the timing and smoothness of the handover but also enhances the overall human-robot interaction experience by making it feel more natural and intuitive to the human recipient, which was shown in robot-to-human handover experimentation. However, this grip release strategy demonstrated limited generalizability across objects with weights differing from the training object used in the study. For objects significantly lighter or heavier than the training object,
%For very light and very heavy objects than the training object, 
it neither proved to be the fastest approach nor was it perceived as the most natural by participants during the evaluation process. This highlighted the need for a more sophisticated grip release strategy that adapts to the weight of the object.

The literature review above highlights a significant gap in the analysis of how object weight impacts human motion across a wide range of weights. There is limited research on the effects of weight on interaction forces and grip release during handovers across a wide range of weights as well. Additionally, further investigation is needed into how humans adapt their grip release strategies in response to varying object weights, as well as how robots can be inspired to implement similar adaptations. In our work, we aim to address these shortcomings by analyzing the influence of object weight on human motion and grip release, while also developing data-driven strategies for adaptive grip release in robots.