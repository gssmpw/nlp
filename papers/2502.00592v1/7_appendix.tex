\section{Justifications of using \texttt{deepspeed-stage-2}}
\label{sub:justifications_of_deepspeed_2}
Eight A100 GPUs support the following configurations:
\begin{itemize}
    \item Full fine-tuning with an 8k context window using Fully Sharded Data Parallel (FSDP). 
    \item 6k context window with full attention using \texttt{deepspeed-stage-2}.
    \item 32k context window with full attention using \texttt{accelerate} and \texttt{deepspeed-stage-3-offload}. However, saving models in this configuration encountered version incompatibility issues and we haven't found solutions online. 
\end{itemize}
Based on these trails, we do not scale up the model with \texttt{deepspeed-stage-3-offload} or FSDP, but choose to use \texttt{deepspeed-stage-2} and set the cross-attention to be of the shape 2048 by 14848. 

\section{Experiments on datasets NaturalQA}

\subsection{Knowledge Retention Experiments on NaturalQA}
\label{sub:knowledge_retention_nqa}
The results of knowledge retention experiments on NaturalQA are shown in Figure \ref{fig:knowledge-retention-nqa}. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/nqa.pdf}
    \caption{Knowledge Retention Results on NaturalQA.}
    \label{fig:knowledge-retention-nqa}
\end{figure}

\subsection{Ablation Study on NaturalQA}
\label{sub:ablation_study_naturalqa}
The results of ablation study on NaturalQA are shown in Figure \ref{fig:nqa_ablation}.

\begin{figure}[h!]
\centering
    \includegraphics[width=0.6\linewidth]{figures/nqa_ablation.pdf}
    \caption{Ablation Study on NaturalQA dataset.}
    \label{fig:nqa_ablation}
\end{figure}

\section{Statistics of the Dataset of Long Documents}
\label{sec:statistics_of_long_documents}
We go through the whole dataset SlimPajama-627B and extract all dataset that have more than 4k tokens using the tokenizer of Llama-3.1-8B. The statistics are shown in Table \ref{tab:sequence-ranges}. We show six categories here (4k-8k, 8k-16k,16k-32k,32k-64k,64k-128k,128k+) but we only use the data within the first four categories (4k-8k, 8k-16k,16k-32k,32k-64k). This is because the examples longer than 64k are mainly from the category \textbf{Book} and lack diversity. 


\begin{table}[htbp]
\centering
\small
\resizebox{\textwidth}{!}{
\begin{tabular}{llrrrrrrr}
\toprule
\textbf{Range} & \textbf{Total} & \textbf{CommonCrawl} & \textbf{GitHub} & \textbf{ArXiv} & \textbf{C4} & \textbf{StackExch.} & \textbf{Wikipedia} & \textbf{Book} \\
\midrule
\textbf{4k--8k} & 11{,}189{,}999 
               & 7{,}759{,}741 (69.35\%) 
               & 692{,}224 (6.19\%) 
               & 286{,}537 (2.56\%) 
               & 1{,}825{,}018 (16.31\%) 
               & 142{,}457 (1.27\%) 
               & 481{,}854 (4.31\%) 
               & 2{,}168 (0.02\%) \\
\textbf{8k--16k} & 4{,}706{,}687 
                & 3{,}273{,}619 (69.55\%) 
                & 270{,}369 (5.74\%) 
                & 550{,}192 (11.69\%) 
                & 439{,}143 (9.33\%) 
                & 20{,}284 (0.43\%) 
                & 146{,}545 (3.11\%) 
                & 6{,}535 (0.14\%) \\
\textbf{16k--32k} & 1{,}607{,}064 
                 & 968{,}714 (60.28\%) 
                 & 95{,}445 (5.94\%) 
                 & 423{,}401 (26.35\%) 
                 & 70{,}223 (4.37\%) 
                 & 1{,}510 (0.09\%) 
                 & 34{,}323 (2.14\%) 
                 & 13{,}448 (0.84\%) \\
\textbf{32k--64k} & 443{,}438 
                 & 224{,}168 (50.55\%) 
                 & 32{,}653 (7.36\%) 
                 & 146{,}582 (33.06\%) 
                 & 3{,}413 (0.77\%) 
                 & 102 (0.02\%) 
                 & 5{,}940 (1.34\%) 
                 & 30{,}580 (6.90\%) \\
\textbf{64k--128k} & 192{,}515 
                  & 72{,}583 (37.70\%) 
                  & 11{,}753 (6.10\%) 
                  & 27{,}942 (14.51\%) 
                  & 38 (0.02\%) 
                  & 5 (0.00\%) 
                  & 507 (0.26\%) 
                  & 79{,}687 (41.39\%) \\
\textbf{128k+}    & 98{,}097 
                  & 23{,}721 (24.18\%) 
                  & 4{,}523 (4.61\%) 
                  & 5{,}167 (5.27\%) 
                  & 0 (0.00\%) 
                  & 2 (0.00\%) 
                  & 49 (0.05\%) 
                  & 64{,}635 (65.89\%) \\
\bottomrule
\end{tabular}}
\caption{Number of examples by sequence-length range and source (counts and percentages).}
\label{tab:sequence-ranges}
\end{table}


\section{Additional Training Details}
% \subsection{Details of Training Sub-Tasks}
\label{sub:details_of_training_sub_tasks}
In our training, we follow MemoryLLM~\citep{memoryllm} and design three sub-tasks:
\begin{itemize}
    \item \textbf{Two-Chunk Training}: Given a document split into two chunks $(x_1, x_2)$, we inject $x_1$ into the memory and update the transformer $\phi$ using the loss on $x_2$. Notably, we retain the gradients across both forward passes. 
    \item \textbf{Multi-Chunk Training}: For documents with multiple chunks $(x_1, \dots, x_n)$, we inject $x_1, \dots, x_{n-1}$ into the memory while detaching gradients, then update $\phi$ using the loss on $x_n$.
    \item  \textbf{Revisiting Cached Chunks}: Since the memory is continually updated during training, we cache the last chunk $x_n$ of earlier documents and revisit it periodically. When revisiting $x_n$, there are already many chunks injected between $x_1, \cdots, x_{n-1}$ and $x_n$. We denote the number of injected chunks between $x_{n-1}$ and $x_n$ as \emph{revisit distance}. We carefully tune the probability of deleting and updating the cache after each training step, and we manage to maintain the average revisit distance to be around 60 for Stage 1 \& Stage 2, and maintain the average distance to be around 200 for Stage 3. 
\end{itemize}





