\vspace{-5pt}
\section{Conclusion and Future Work}
In this work, we present \ours, an enhanced memory-augmented language model that extends the long-term retention abilities of MemoryLLM. By integrating a long-term memory (LTM) mechanism with a co-trained retriever, \ours effectively retrieves and utilizes past information, significantly extending the knowledge retention abilities from MemoryLLM, achieve superior performances in long-context understanding tasks compared with recent baselines given the similar budget of GPU memory. 
In future work, we plan to reduce CPU-GPU communication overhead, enabling more efficient generation with \ours.
