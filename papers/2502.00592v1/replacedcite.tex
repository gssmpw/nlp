\section{Related Work}
\vspace{-5pt}
We classify memory-based methods into two categories: Token-Level Memory and Latent-Space Memory, which is similar to the categorizations in ____ where they classify methods into implicit memory and explicit memory. 

\vspace{-5pt}
\subsection{Token-level Memory}
Token-level memory refers to memory structures represented in textual forms, which can include raw context, summaries____, knowledge graphs____, organized text with hierarchical or graph structures____, or databases____. 
Methods such as MemoryBank____, RecurrentGPT____ incorporate multiple components of memory, including both raw conversational data and summaries. MemGPT ____ proposes treating context and memory as analogous to traditional memory in operating systems, enabling more flexible and organized memory structures. These approaches typically rely on text embeddings for memory retrieval, where queries can originate from either the current conversation turn ____ or queries generated by the language model itself____. In contrast, ChatDB ____ stores knowledge in a database and performs retrieval using SQL queries, while MemLLM____ fine-tunes the model to generate function calls that initiate searches within a knowledge graph, referred to as ``Triple Memory'' by ____. These methods generally offer benefits such as modularity (with the exception of MemLLM, which requires fine-tuning) and interpretability____, allowing for potential integration with external systems____. However, these approaches have limitations. Some require storing the raw text, which is not the most compressed method to store information____. Others store knowledge in the form of triplets, which may be unsuitable for representing complex conversations that are difficult to convert into triplets____. 

\begin{figure*}[t]
\centering
\subfigure[Update]{\label{fig:memoryllm_update}\includegraphics[width=0.490\linewidth]{figures/update.png}}
\hfill
\subfigure[Generation]{\label{fig:memoryllm_generate}\includegraphics[width=0.490\linewidth]{figures/generate.png}}
\vspace{-10pt}
\caption{Update and Generation Process of MemoryLLM. We process the chunk with $\phi_l$ to obtain new $K$ tokens during the update process, which is perceived by $\phi$ using cross-attention during the generation process.}
\label{fig:memoryllm_update_generate}
\vspace{-10pt}
\end{figure*}

\begin{figure*}[t]
\centering
\subfigure[Update Process. ]{\label{fig:mplus_update}\includegraphics[width=0.38\linewidth]
{figures/ltm-update.png}}
\hfill
\subfigure[Generation]{\label{fig:mplus_generate}\includegraphics[width=0.610\linewidth]
{figures/ltm-generate.png}}
\vspace{-10pt}
\caption{Update and Generation Process of \ours. For layer $l$, during Update, the old memory pool $\theta_l$ is split into two parts: $K$ dropped tokens and $N-K$ remaining tokens. The dropped tokens are stored in the long-term memory $\Theta_l$ while the remaining tokens and new $K$ tokens are combined to obtain the new memory pool $\theta_l'$. Then during generation, we use our co-trained retriever to retrieve tokens from $\Theta_l$, which is fed into the transformer layer $\phi_l$ along with the short-term memory $\theta_l$ and the query hidden states.}
\label{fig:mplus_update_generate}
\vspace{-10pt}
\end{figure*}

\vspace{-5pt}
\subsection{Latent-Space Memory}
Latent-space memory stores information in a compressed format, embedding knowledge into hidden states____, model parameters____, or an external latent space____, among other methods. Some approaches use memory slots to encode information____, while others rely on key-value caches stored in memory pools for future retrieval____. Notably, CamelLoT____ and Memoria____ incorporate forgetting mechanisms to better emulate human memory. Similarly, MemoryLLM____ compresses knowledge into hidden states and employs random dropping to prevent unbounded memory growth. The M$^3$ method____ also stores memory in the hidden-state space, archiving a vast pretraining dataset comprising $1.1\times 10^8$ text chunks. Distinct from methods that utilize hidden states or key-value caches, Larimar____ introduces a memory matrix that supports read and write operations, demonstrating effectiveness in knowledge-editing tasks. Furthermore, SELF-PARAM____ explores embedding knowledge directly into model parameters without degrading the model's capabilities or requiring additional parameters. These latent-space memory techniques have shown promising results across various downstream tasks. By saving information in a compressed format and leveraging retrieval during generation, they enable substantial expansions of the context window without incurring excessive GPU memory costs.
Despite the advantages and potential of Latent-Space Memory, existing methods within this category typically fall short when dealing with extremely long input____. In contrast, \ours can have much longer retention compared to existing methods.