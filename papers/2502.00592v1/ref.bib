@article{MemoryNetwork,
  title={Memory networks},
  author={Weston, Jason and Chopra, Sumit and Bordes, Antoine},
  journal={arXiv preprint arXiv:1410.3916},
  year={2014}
}

@article{kNNLM,
  title={Generalization through memorization: Nearest neighbor language models},
  author={Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:1911.00172},
  year={2019}
}

@article{MemoryBank,
  title={MemoryBank: Enhancing Large Language Models with Long-Term Memory},
  author={Zhong, Wanjun and Guo, Lianghong and Gao, Qiqi and Wang, Yanlin},
  journal={arXiv preprint arXiv:2305.10250},
  year={2023}
}

@article{LLMCWM,
  author       = {Daliang Li and
                  Ankit Singh Rawat and
                  Manzil Zaheer and
                  Xin Wang and
                  Michal Lukasik and
                  Andreas Veit and
                  Felix X. Yu and
                  Sanjiv Kumar},
  title        = {Large Language Models with Controllable Working Memory},
  journal      = {CoRR},
  volume       = {abs/2211.05110},
  year         = {2022}
}

@article{unlimiformer,
  title={Unlimiformer: Long-range transformers with unlimited length input},
  author={Bertsch, Amanda and Alon, Uri and Neubig, Graham and Gormley, Matthew R},
  journal={arXiv preprint arXiv:2305.01625},
  year={2023}
}


@article{alibi,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}

@article{longnet,
  title={LONGNET: Scaling Transformers to
1,000,000,000 Tokens},
  author={Jiayu, Ding and Shuming, Ma and Li, Dong and Xingxing, Zhang and Shaohan, Huang and Wenhui, Wang and Furu Weiâ€ },
  journal={arXiv preprint arXiv:2307.02486},
  year={2023}
}



@article{memoryTransformer,
  author       = {Mikhail S. Burtsev and
                  Grigory V. Sapunov},
  title        = {Memory Transformer},
  journal      = {CoRR},
  volume       = {abs/2006.11527},
  year         = {2020},
  url          = {https://arxiv.org/abs/2006.11527},
  eprinttype    = {arXiv},
  eprint       = {2006.11527},
  timestamp    = {Tue, 23 Jun 2020 17:57:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2006-11527.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{LongMEM,
  title={Augmenting Language Models with Long-Term Memory},
  author={Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu},
  journal={arXiv preprint arXiv:2306.07174},
  year={2023}
}


@article{Memory-Enhanced-Transformer,
  author       = {Gianluca Moro and
                  Luca Ragazzi and
                  Lorenzo Valgimigli and
                  Giacomo Frisoni and
                  Claudio Sartori and
                  Gustavo Marfia},
  title        = {Efficient Memory-Enhanced Transformer for Long-Document Summarization
                  in Low-Resource Regimes},
  journal      = {Sensors},
  volume       = {23},
  number       = {7},
  pages        = {3542},
  year         = {2023}
}



@inproceedings{HEPOS,
  author       = {Luyang Huang and
                  Shuyang Cao and
                  Nikolaus Nova Parulian and
                  Heng Ji and
                  Lu Wang},
  title        = {Efficient Attentions for Long Document Summarization},
  booktitle    = {{NAACL-HLT}},
  pages        = {1419--1436},
  publisher    = {Association for Computational Linguistics},
  year         = {2021}
}

@article{FMTforIC,
  author       = {Tongwei Lu and
                  Jiarong Wang and
                  Fen Min},
  title        = {Full-Memory Transformer for Image Captioning},
  journal      = {Symmetry},
  volume       = {15},
  number       = {1},
  pages        = {190},
  year         = {2023}
}

@inproceedings{MeshedTforIC,
  author       = {Marcella Cornia and
                  Matteo Stefanini and
                  Lorenzo Baraldi and
                  Rita Cucchiara},
  title        = {Meshed-Memory Transformer for Image Captioning},
  booktitle    = {{CVPR}},
  pages        = {10575--10584},
  publisher    = {Computer Vision Foundation / {IEEE}},
  year         = {2020}
}


@inproceedings{ModelingHumanMemory,
  author       = {Yizhuo Li and
                  Cewu Lu},
  title        = {Modeling Human Memory in Multi-Object Tracking with Transformers},
  booktitle    = {{ICASSP}},
  pages        = {2849--2853},
  publisher    = {{IEEE}},
  year         = {2022}
}


@inproceedings{Memformer,
  author       = {Qingyang Wu and
                  Zhenzhong Lan and
                  Kun Qian and
                  Jing Gu and
                  Alborz Geramifard and
                  Zhou Yu},
  title        = {Memformer: {A} Memory-Augmented Transformer for Sequence Modeling},
  booktitle    = {{AACL/IJCNLP} (Findings)},
  pages        = {308--318},
  publisher    = {Association for Computational Linguistics},
  year         = {2022}
}


@inproceedings{RMT,
  author       = {Aydar Bulatov and
                  Yuri Kuratov and
                  Mikhail S. Burtsev},
  title        = {Recurrent Memory Transformer},
  booktitle    = {NeurIPS},
  year         = {2022}
}

@inproceedings{TMR,
  author       = {Rui Liu and
                  Barzan Mozafari},
  title        = {Transformer with Memory Replay},
  booktitle    = {{AAAI}},
  pages        = {7567--7575},
  publisher    = {{AAAI} Press},
  year         = {2022}
}

@article{GlobalMemoryTransformer,
  author       = {Arij Al Adel},
  title        = {Global memory transformer for processing long documents},
  journal      = {CoRR},
  volume       = {abs/2212.01650},
  year         = {2022}
}


@inproceedings{LifeLongPretraining,
  author       = {Xisen Jin and
                  Dejiao Zhang and
                  Henghui Zhu and
                  Wei Xiao and
                  Shang{-}Wen Li and
                  Xiaokai Wei and
                  Andrew O. Arnold and
                  Xiang Ren},
  title        = {Lifelong Pretraining: Continually Adapting Language Models to Emerging
                  Corpora},
  booktitle    = {{NAACL-HLT}},
  pages        = {4764--4780},
  publisher    = {Association for Computational Linguistics},
  year         = {2022}
}

@inproceedings{TemporalWiki,
  author       = {Joel Jang and
                  Seonghyeon Ye and
                  Changho Lee and
                  Sohee Yang and
                  Joongbo Shin and
                  Janghoon Han and
                  Gyeonghun Kim and
                  Minjoon Seo},
  title        = {TemporalWiki: {A} Lifelong Benchmark for Training and Evaluating Ever-Evolving
                  Language Models},
  booktitle    = {{EMNLP}},
  pages        = {6237--6250},
  publisher    = {Association for Computational Linguistics},
  year         = {2022}
}

@article{LLMEditing,
  author       = {Yunzhi Yao and
                  Peng Wang and
                  Bozhong Tian and
                  Siyuan Cheng and
                  Zhoubo Li and
                  Shumin Deng and
                  Huajun Chen and
                  Ningyu Zhang},
  title        = {Editing Large Language Models: Problems, Methods, and Opportunities},
  journal      = {CoRR},
  volume       = {abs/2305.13172},
  year         = {2023}
}


@article{TPatcher,
  title={Transformer-Patcher: One Mistake worth One Neuron},
  author={Huang, Zeyu and Shen, Yikang and Zhang, Xiaofeng and Zhou, Jie and Rong, Wenge and Xiong, Zhang},
  journal={arXiv preprint arXiv:2301.09785},
  year={2023}
}

@article{PositionalInterpolation,
  title={Extending Context Window of Large Language Models via Positional Interpolation},
  author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  journal={arXiv preprint arXiv:2306.15595},
  year={2023}
}


@inproceedings{CKL,
  author       = {Joel Jang and
                  Seonghyeon Ye and
                  Sohee Yang and
                  Joongbo Shin and
                  Janghoon Han and
                  Gyeonghun Kim and
                  Stanley Jungkyu Choi and
                  Minjoon Seo},
  title        = {Towards Continual Knowledge Learning of Language Models},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2022}
}


@article{longformer,
  author       = {Iz Beltagy and
                  Matthew E. Peters and
                  Arman Cohan},
  title        = {Longformer: The Long-Document Transformer},
  journal      = {CoRR},
  volume       = {abs/2004.05150},
  year         = {2020},
  url          = {https://arxiv.org/abs/2004.05150},
  eprinttype    = {arXiv},
  eprint       = {2004.05150},
  timestamp    = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2004-05150.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lextransformer,
  author       = {Yutao Sun and
                  Li Dong and
                  Barun Patra and
                  Shuming Ma and
                  Shaohan Huang and
                  Alon Benhaim and
                  Vishrav Chaudhary and
                  Xia Song and
                  Furu Wei},
  title        = {A Length-Extrapolatable Transformer},
  booktitle    = {{ACL} {(1)}},
  pages        = {14590--14604},
  publisher    = {Association for Computational Linguistics},
  year         = {2023}
}


@article{SparserTransformer,
  author       = {Rewon Child and
                  Scott Gray and
                  Alec Radford and
                  Ilya Sutskever},
  title        = {Generating Long Sequences with Sparse Transformers},
  journal      = {CoRR},
  volume       = {abs/1904.10509},
  year         = {2019},
  url          = {http://arxiv.org/abs/1904.10509},
  eprinttype    = {arXiv},
  eprint       = {1904.10509},
  timestamp    = {Thu, 02 May 2019 15:13:44 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1904-10509.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{bigbird,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={17283--17297},
  year={2020}
}

@article{reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@article{linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{E2EMN,
  title={End-to-end memory networks},
  author={Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob and others},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}


@inproceedings{MemoryTransformerWithHierarchicalAttention,
  title={Memory transformer with hierarchical attention for long document processing},
  author={Al Adel, Arij and Burtsev, Mikhail S},
  booktitle={2021 International Conference Engineering and Telecommunication (En\&T)},
  pages={1--7},
  year={2021},
  organization={IEEE}
}


@article{Llama2Long,
  title={Effective long-context scaling of foundation models},
  author={Xiong, Wenhan and Liu, Jingyu and Molybog, Igor and Zhang, Hejia and Bhargava, Prajjwal and Hou, Rui and Martin, Louis and Rungta, Rashi and Sankararaman, Karthik Abinav and Oguz, Barlas and others},
  journal={arXiv preprint arXiv:2309.16039},
  year={2023}
}



@article{longbench,
  title={LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others},
  journal={arXiv preprint arXiv:2308.14508},
  year={2023}
}

@article{chapterbreak,
  title={Chapterbreak: A challenge dataset for long-range language models},
  author={Sun, Simeng and Thai, Katherine and Iyyer, Mohit},
  journal={arXiv preprint arXiv:2204.10878},
  year={2022}
}

@article{fot,
  title={Focused transformer: Contrastive training for context scaling},
  author={Tworkowski, Szymon and Staniszewski, Konrad and Pacek, Miko{\l}aj and Wu, Yuhuai and Michalewski, Henryk and Mi{\l}o{\'s}, Piotr},
  journal={arXiv preprint arXiv:2307.03170},
  year={2023}
}

@inproceedings{SERAC,
  author       = {Eric Mitchell and
                  Charles Lin and
                  Antoine Bosselut and
                  Christopher D. Manning and
                  Chelsea Finn},
  title        = {Memory-Based Model Editing at Scale},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {162},
  pages        = {15817--15831},
  publisher    = {{PMLR}},
  year         = {2022}
}

@article{memprompt,
  title={Memory-assisted prompt editing to improve gpt-3 after deployment},
  author={Madaan, Aman and Tandon, Niket and Clark, Peter and Yang, Yiming},
  journal={arXiv preprint arXiv:2201.06009},
  year={2022}
}

@article{ike,
  title={Can We Edit Factual Knowledge by In-Context Learning?},
  author={Zheng, Ce and Li, Lei and Dong, Qingxiu and Fan, Yuxuan and Wu, Zhiyong and Xu, Jingjing and Chang, Baobao},
  journal={arXiv preprint arXiv:2305.12740},
  year={2023}
}

@article{mello,
  title={MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions},
  author={Zhong, Zexuan and Wu, Zhengxuan and Manning, Christopher D and Potts, Christopher and Chen, Danqi},
  journal={arXiv preprint arXiv:2305.14795},
  year={2023}
}


@inproceedings{calinet,
  author       = {Qingxiu Dong and
                  Damai Dai and
                  Yifan Song and
                  Jingjing Xu and
                  Zhifang Sui and
                  Lei Li},
  title        = {Calibrating Factual Knowledge in Pretrained Language Models},
  booktitle    = {{EMNLP} (Findings)},
  pages        = {5937--5947},
  publisher    = {Association for Computational Linguistics},
  year         = {2022}
}

@article{grace,
  title={Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors},
  author={Hartvigsen, Thomas and Sankaranarayanan, Swami and Palangi, Hamid and Kim, Yoon and Ghassemi, Marzyeh},
  journal={arXiv preprint arXiv:2211.11031},
  year={2022}
}


@inproceedings{KN,
  author       = {Damai Dai and
                  Li Dong and
                  Yaru Hao and
                  Zhifang Sui and
                  Baobao Chang and
                  Furu Wei},
  title        = {Knowledge Neurons in Pretrained Transformers},
  booktitle    = {{ACL} {(1)}},
  pages        = {8493--8502},
  publisher    = {Association for Computational Linguistics},
  year         = {2022}
}

@article{ROME,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@inproceedings{memit,
  author       = {Kevin Meng and
                  Arnab Sen Sharma and
                  Alex J. Andonian and
                  Yonatan Belinkov and
                  David Bau},
  title        = {Mass-Editing Memory in a Transformer},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2023}
}

@article{pmet,
  title={PMET: Precise Model Editing in a Transformer},
  author={Li, Xiaopeng and Li, Shasha and Song, Shezheng and Yang, Jing and Ma, Jun and Yu, Jie},
  journal={arXiv preprint arXiv:2308.08742},
  year={2023}
}

@inproceedings{KE,
  author       = {Nicola De Cao and
                  Wilker Aziz and
                  Ivan Titov},
  title        = {Editing Factual Knowledge in Language Models},
  booktitle    = {{EMNLP} {(1)}},
  pages        = {6491--6506},
  publisher    = {Association for Computational Linguistics},
  year         = {2021}
}


@inproceedings{mend,
  author       = {Eric Mitchell and
                  Charles Lin and
                  Antoine Bosselut and
                  Chelsea Finn and
                  Christopher D. Manning},
  title        = {Fast Model Editing at Scale},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2022}
}

@article{llama2,
  author       = {Hugo Touvron and
                  Louis Martin and
                  Kevin Stone and
                  Peter Albert and
                  Amjad Almahairi and
                  Yasmine Babaei and
                  Nikolay Bashlykov and
                  Soumya Batra and
                  Prajjwal Bhargava and
                  Shruti Bhosale and
                  Dan Bikel and
                  Lukas Blecher and
                  Cristian Canton{-}Ferrer and
                  Moya Chen and
                  Guillem Cucurull and
                  David Esiobu and
                  Jude Fernandes and
                  Jeremy Fu and
                  Wenyin Fu and
                  Brian Fuller and
                  Cynthia Gao and
                  Vedanuj Goswami and
                  Naman Goyal and
                  Anthony Hartshorn and
                  Saghar Hosseini and
                  Rui Hou and
                  Hakan Inan and
                  Marcin Kardas and
                  Viktor Kerkez and
                  Madian Khabsa and
                  Isabel Kloumann and
                  Artem Korenev and
                  Punit Singh Koura and
                  Marie{-}Anne Lachaux and
                  Thibaut Lavril and
                  Jenya Lee and
                  Diana Liskovich and
                  Yinghai Lu and
                  Yuning Mao and
                  Xavier Martinet and
                  Todor Mihaylov and
                  Pushkar Mishra and
                  Igor Molybog and
                  Yixin Nie and
                  Andrew Poulton and
                  Jeremy Reizenstein and
                  Rashi Rungta and
                  Kalyan Saladi and
                  Alan Schelten and
                  Ruan Silva and
                  Eric Michael Smith and
                  Ranjan Subramanian and
                  Xiaoqing Ellen Tan and
                  Binh Tang and
                  Ross Taylor and
                  Adina Williams and
                  Jian Xiang Kuan and
                  Puxin Xu and
                  Zheng Yan and
                  Iliyan Zarov and
                  Yuchen Zhang and
                  Angela Fan and
                  Melanie Kambadur and
                  Sharan Narang and
                  Aur{\'{e}}lien Rodriguez and
                  Robert Stojnic and
                  Sergey Edunov and
                  Thomas Scialom},
  title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  journal      = {CoRR},
  volume       = {abs/2307.09288},
  year         = {2023}
}
@software{openllama,
  author = {Geng, Xinyang and Liu, Hao},
  title = {OpenLLaMA: An Open Reproduction of LLaMA},
  month = May,
  year = 2023,
  url = {https://github.com/openlm-research/open_llama}
}

@inproceedings{zsre,
  author       = {Omer Levy and
                  Minjoon Seo and
                  Eunsol Choi and
                  Luke Zettlemoyer},
  editor       = {Roger Levy and
                  Lucia Specia},
  title        = {Zero-Shot Relation Extraction via Reading Comprehension},
  booktitle    = {Proceedings of the 21st Conference on Computational Natural Language
                  Learning (CoNLL 2017), Vancouver, Canada, August 3-4, 2017},
  pages        = {333--342},
  publisher    = {Association for Computational Linguistics},
  year         = {2017},
  url          = {https://doi.org/10.18653/v1/K17-1034},
  doi          = {10.18653/V1/K17-1034},
  timestamp    = {Fri, 06 Aug 2021 00:41:08 +0200},
  biburl       = {https://dblp.org/rec/conf/conll/LevySCZ17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{ModifyingMemories,
  author       = {Chen Zhu and
                  Ankit Singh Rawat and
                  Manzil Zaheer and
                  Srinadh Bhojanapalli and
                  Daliang Li and
                  Felix X. Yu and
                  Sanjiv Kumar},
  title        = {Modifying Memories in Transformer Models},
  journal      = {CoRR},
  volume       = {abs/2012.00363},
  year         = {2020}
}

@article{c4,
  author       = {Colin Raffel and
                  Noam Shazeer and
                  Adam Roberts and
                  Katherine Lee and
                  Sharan Narang and
                  Michael Matena and
                  Yanqi Zhou and
                  Wei Li and
                  Peter J. Liu},
  title        = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
                  Transformer},
  journal      = {J. Mach. Learn. Res.},
  volume       = {21},
  pages        = {140:1--140:67},
  year         = {2020}
}

@software{together2023redpajama,
  author = {Together Computer},
  title = {RedPajama: an Open Dataset for Training Large Language Models},
  month = October,
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}

@software{openlm2023openllama,
  author = {Geng, Xinyang and Liu, Hao},
  title = {OpenLLaMA: An Open Reproduction of LLaMA},
  month = May,
  year = 2023,
  url = {https://github.com/openlm-research/open_llama}
}

@article{longlora,
  title={Longlora: Efficient fine-tuning of long-context large language models},
  author={Chen, Yukang and Qian, Shengju and Tang, Haotian and Lai, Xin and Liu, Zhijian and Han, Song and Jia, Jiaya},
  journal={arXiv preprint arXiv:2309.12307},
  year={2023}
}

@article{ret-llm,
  title={RET-LLM: Towards a General Read-Write Memory for Large Language Models},
  author={Modarressi, Ali and Imani, Ayyoob and Fayyaz, Mohsen and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2305.14322},
  year={2023}
}

@inproceedings{kv-cache-survey,
  author       = {Jiayi Yuan and
                  Hongyi Liu and
                  Shaochen Zhong and
                  Yu{-}Neng Chuang and
                  Songchen Li and
                  Guanchu Wang and
                  Duy Le and
                  Hongye Jin and
                  Vipin Chaudhary and
                  Zhaozhuo Xu and
                  Zirui Liu and
                  Xia Ben Hu},
  title        = {{KV} Cache Compression, But What Must We Give in Return? {A} Comprehensive
                  Benchmark of Long Context Capable Approaches},
  booktitle    = {{EMNLP} (Findings)},
  pages        = {4623--4648},
  publisher    = {Association for Computational Linguistics},
  year         = {2024}
}

@inproceedings{h2o,
  author       = {Zhenyu Zhang and
                  Ying Sheng and
                  Tianyi Zhou and
                  Tianlong Chen and
                  Lianmin Zheng and
                  Ruisi Cai and
                  Zhao Song and
                  Yuandong Tian and
                  Christopher R{\'{e}} and
                  Clark W. Barrett and
                  Zhangyang Wang and
                  Beidi Chen},
  title        = {{H2O:} Heavy-Hitter Oracle for Efficient Generative Inference of Large
                  Language Models},
  booktitle    = {NeurIPS},
  year         = {2023}
}

@article{lscs,
  author       = {Yu Wang and
                  Chi Han and
                  Tongtong Wu and
                  Xiaoxin He and
                  Wangchunshu Zhou and
                  Nafis Sadeq and
                  Xiusi Chen and
                  Zexue He and
                  Wei Wang and
                  Gholamreza Haffari and
                  Heng Ji and
                  Julian J. McAuley},
  title        = {Towards LifeSpan Cognitive Systems},
  journal      = {CoRR},
  volume       = {abs/2409.13265},
  year         = {2024}
}

@inproceedings{memoryllm,
  author       = {Yu Wang and
                  Yifan Gao and
                  Xiusi Chen and
                  Haoming Jiang and
                  Shiyang Li and
                  Jingfeng Yang and
                  Qingyu Yin and
                  Zheng Li and
                  Xian Li and
                  Bing Yin and
                  Jingbo Shang and
                  Julian J. McAuley},
  title        = {{MEMORYLLM:} Towards Self-Updatable Large Language Models},
  booktitle    = {{ICML}},
  publisher    = {OpenReview.net},
  year         = {2024}
}

@article{fedorenko2024language,
  title={Language is primarily a tool for communication rather than thought},
  author={Fedorenko, Evelina and Piantadosi, Steven T and Gibson, Edward AF},
  journal={Nature},
  volume={630},
  number={8017},
  pages={575--586},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{zhou2023recurrentgpt,
  title={RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text},
  author={Zhou, Wangchunshu and Jiang, Yuchen Eleanor and Cui, Peng and Wang, Tiannan and Xiao, Zhenxin and Hou, Yifan and Cotterell, Ryan and Sachan, Mrinmaya},
  journal={arXiv preprint arXiv:2305.13304},
  year={2023}
}

@article{memgpt,
  author       = {Charles Packer and
                  Vivian Fang and
                  Shishir G. Patil and
                  Kevin Lin and
                  Sarah Wooders and
                  Joseph E. Gonzalez},
  title        = {MemGPT: Towards LLMs as Operating Systems},
  journal      = {CoRR},
  volume       = {abs/2310.08560},
  year         = {2023}
}

@article{hu2023chatdb,
  title={Chatdb: Augmenting llms with databases as their symbolic memory},
  author={Hu, Chenxu and Fu, Jie and Du, Chenzhuang and Luo, Simian and Zhao, Junbo and Zhao, Hang},
  journal={arXiv preprint arXiv:2306.03901},
  year={2023}
}

@article{llama3,
  author       = {Abhimanyu Dubey and
                  Abhinav Jauhri and
                  Abhinav Pandey and
                  Abhishek Kadian and
                  Ahmad Al{-}Dahle and
                  Aiesha Letman and
                  Akhil Mathur and
                  Alan Schelten and
                  Amy Yang and
                  Angela Fan and
                  Anirudh Goyal and
                  Anthony Hartshorn and
                  Aobo Yang and
                  Archi Mitra and
                  Archie Sravankumar and
                  Artem Korenev and
                  Arthur Hinsvark and
                  Arun Rao and
                  Aston Zhang and
                  Aur{\'{e}}lien Rodriguez and
                  Austen Gregerson and
                  Ava Spataru and
                  Baptiste Rozi{\`{e}}re and
                  Bethany Biron and
                  Binh Tang and
                  Bobbie Chern and
                  Charlotte Caucheteux and
                  Chaya Nayak and
                  Chloe Bi and
                  Chris Marra and
                  Chris McConnell and
                  Christian Keller and
                  Christophe Touret and
                  Chunyang Wu and
                  Corinne Wong and
                  Cristian Canton Ferrer and
                  Cyrus Nikolaidis and
                  Damien Allonsius and
                  Daniel Song and
                  Danielle Pintz and
                  Danny Livshits and
                  David Esiobu and
                  Dhruv Choudhary and
                  Dhruv Mahajan and
                  Diego Garcia{-}Olano and
                  Diego Perino and
                  Dieuwke Hupkes and
                  Egor Lakomkin and
                  Ehab AlBadawy and
                  Elina Lobanova and
                  Emily Dinan and
                  Eric Michael Smith and
                  Filip Radenovic and
                  Frank Zhang and
                  Gabriel Synnaeve and
                  Gabrielle Lee and
                  Georgia Lewis Anderson and
                  Graeme Nail and
                  Gr{\'{e}}goire Mialon and
                  Guan Pang and
                  Guillem Cucurell and
                  Hailey Nguyen and
                  Hannah Korevaar and
                  Hu Xu and
                  Hugo Touvron and
                  Iliyan Zarov and
                  Imanol Arrieta Ibarra and
                  Isabel M. Kloumann and
                  Ishan Misra and
                  Ivan Evtimov and
                  Jade Copet and
                  Jaewon Lee and
                  Jan Geffert and
                  Jana Vranes and
                  Jason Park and
                  Jay Mahadeokar and
                  Jeet Shah and
                  Jelmer van der Linde and
                  Jennifer Billock and
                  Jenny Hong and
                  Jenya Lee and
                  Jeremy Fu and
                  Jianfeng Chi and
                  Jianyu Huang and
                  Jiawen Liu and
                  Jie Wang and
                  Jiecao Yu and
                  Joanna Bitton and
                  Joe Spisak and
                  Jongsoo Park and
                  Joseph Rocca and
                  Joshua Johnstun and
                  Joshua Saxe and
                  Junteng Jia and
                  Kalyan Vasuden Alwala and
                  Kartikeya Upasani and
                  Kate Plawiak and
                  Ke Li and
                  Kenneth Heafield and
                  Kevin Stone and
                  et al.},
  title        = {The Llama 3 Herd of Models},
  journal      = {CoRR},
  volume       = {abs/2407.21783},
  year         = {2024}
}

@inproceedings{infinitebench,
  title={$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens},
  author={Zhang, Xinrong and Chen, Yingfa and Hu, Shengding and Xu, Zihang and Chen, Junhao and Hao, Moo and Han, Xu and Thai, Zhen and Wang, Shuo and Liu, Zhiyuan and others},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={15262--15277},
  year={2024}
}



@article{snapkv,
  author       = {Yuhong Li and
                  Yingbing Huang and
                  Bowen Yang and
                  Bharat Venkitesh and
                  Acyr Locatelli and
                  Hanchen Ye and
                  Tianle Cai and
                  Patrick Lewis and
                  Deming Chen},
  title        = {SnapKV: {LLM} Knows What You are Looking for Before Generation},
  journal      = {CoRR},
  volume       = {abs/2404.14469},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2404.14469},
  doi          = {10.48550/ARXIV.2404.14469},
  eprinttype    = {arXiv},
  eprint       = {2404.14469},
  timestamp    = {Sat, 25 May 2024 18:35:24 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2404-14469.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{self-param,
  title={Self-Updatable Large Language Models with Parameter Integration},
  author={Wang, Yu and Liu, Xinshuang and Chen, Xiusi and O'Brien, Sean and Wu, Junda and McAuley, Julian},
  journal={arXiv preprint arXiv:2410.00487},
  year={2024}
}

@article{memllm,
  title={Memllm: Finetuning llms to use an explicit read-write memory},
  author={Modarressi, Ali and K{\"o}ksal, Abdullatif and Imani, Ayyoob and Fayyaz, Mohsen and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2404.11672},
  year={2024}
}

@inproceedings{em2,
  author       = {Zhangyue Yin and
                  Qiushi Sun and
                  Qipeng Guo and
                  Zhiyuan Zeng and
                  Qinyuan Cheng and
                  Xipeng Qiu and
                  Xuanjing Huang},
  title        = {Explicit Memory Learning with Expectation Maximization},
  booktitle    = {{EMNLP}},
  pages        = {16618--16635},
  publisher    = {Association for Computational Linguistics},
  year         = {2024}
}

@article{wu2022survey,
  title={A survey of human-in-the-loop for machine learning},
  author={Wu, Xingjiao and Xiao, Luwei and Sun, Yixuan and Zhang, Junhang and Ma, Tianlong and He, Liang},
  journal={Future Generation Computer Systems},
  volume={135},
  pages={364--381},
  year={2022},
  publisher={Elsevier}
}

@article{memory_sharing,
  author       = {Hang Gao and
                  Yongfeng Zhang},
  title        = {Memory Sharing for Large Language Model based Agents},
  journal      = {CoRR},
  volume       = {abs/2404.09982},
  year         = {2024}
}

@article{working-memory-hub,
  author       = {Jing Guo and
                  Nan Li and
                  Jianchuan Qi and
                  Hang Yang and
                  Ruiqiao Li and
                  Yuzhen Feng and
                  Si Zhang and
                  Ming Xu},
  title        = {Empowering Working Memory for Large Language Model Agents},
  journal      = {CoRR},
  volume       = {abs/2312.17259},
  year         = {2023}
}

@article{enhanced_text_compression,
  author       = {Chowdhury Mofizur Rahman and
                  Mahbub E. Sobhani and
                  Anika Tasnim Rodela and
                  Swakkhar Shatabda},
  title        = {An Enhanced Text Compression Approach Using Transformer-based Language
                  Models},
  journal      = {CoRR},
  volume       = {abs/2412.15250},
  year         = {2024}
}

@techreport{nncp_v2,
  title={NNCP v2: Lossless data compression with transformer},
  author={Bellard, Fabrice},
  year={2021},
  institution={Technical report, Amarisoft}
}


@article{tiny_transformers_for_text_compression,
  title={Tiny Transformers Excel at Sentence Compression},
  author={Belcak, Peter and Wattenhofer, Roger},
  journal={arXiv preprint arXiv:2410.23510},
  year={2024}
}

@article{coconut,
  title={Training large language models to reason in a continuous latent space},
  author={Hao, Shibo and Sukhbaatar, Sainbayar and Su, DiJia and Li, Xian and Hu, Zhiting and Weston, Jason and Tian, Yuandong},
  journal={arXiv preprint arXiv:2412.06769},
  year={2024}
}

@inproceedings{larimar,
  author       = {Payel Das and
                  Subhajit Chaudhury and
                  Elliot Nelson and
                  Igor Melnyk and
                  Sarathkrishna Swaminathan and
                  Sihui Dai and
                  Aur{\'{e}}lie C. Lozano and
                  Georgios Kollias and
                  Vijil Chenthamarakshan and
                  Jir{\'{\i}} Navr{\'{a}}til and
                  Soham Dan and
                  Pin{-}Yu Chen},
  title        = {Larimar: Large Language Models with Episodic Memory Control},
  booktitle    = {{ICML}},
  publisher    = {OpenReview.net},
  year         = {2024}
}


@inproceedings{in-context-auto-encoder,
  author       = {Tao Ge and
                  Jing Hu and
                  Lei Wang and
                  Xun Wang and
                  Si{-}Qing Chen and
                  Furu Wei},
  title        = {In-context Autoencoder for Context Compression in a Large Language
                  Model},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=uREj4ZuGJE},
  timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/00010WWCW24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{behrouz2024titans,
  title={Titans: Learning to Memorize at Test Time},
  author={Behrouz, Ali and Zhong, Peilin and Mirrokni, Vahab},
  journal={arXiv preprint arXiv:2501.00663},
  year={2024}
}



@article{gutierrez2024hipporag,
  title={HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models},
  author={Guti{\'e}rrez, Bernal Jim{\'e}nez and Shu, Yiheng and Gu, Yu and Yasunaga, Michihiro and Su, Yu},
  journal={arXiv preprint arXiv:2405.14831},
  year={2024}
}

@article{whoswho,
  title={Who's Who: Large Language Models Meet Knowledge Conflicts in Practice},
  author={Pham, Quang Hieu and Ngo, Hoang and Luu, Anh Tuan and Nguyen, Dat Quoc},
  journal={arXiv preprint arXiv:2410.15737},
  year={2024}
}

@inproceedings{al2021memory,
  title={Memory transformer with hierarchical attention for long document processing},
  author={Al Adel, Arij and Burtsev, Mikhail S},
  booktitle={2021 International Conference Engineering and Telecommunication (En\&T)},
  pages={1--7},
  year={2021},
  organization={IEEE}
}


@misc{Memoria,
      title={Memoria: Resolving Fateful Forgetting Problem through Human-Inspired Memory Architecture}, 
      author={Sangjun Park and JinYeong Bak},
      year={2024},
      eprint={2310.03052},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{he2024camelot,
  title={CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory},
  author={He, Zexue and Karlinsky, Leonid and Kim, Donghyun and McAuley, Julian and Krotov, Dmitry and Feris, Rogerio},
  journal={arXiv preprint arXiv:2402.13449},
  year={2024}
}

@inproceedings{MemoringTransformers,
  author       = {Yuhuai Wu and
                  Markus Norman Rabe and
                  DeLesley Hutchins and
                  Christian Szegedy},
  title        = {Memorizing Transformers},
  booktitle    = {The Tenth International Conference on Learning Representations, {ICLR}
                  2022, Virtual Event, April 25-29, 2022},
  publisher    = {OpenReview.net},
  year         = {2022},
  url          = {https://openreview.net/forum?id=TrjbxzRcnf-},
  timestamp    = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/WuRHS22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{memory3,
  author       = {Hongkang Yang and
                  Zehao Lin and
                  Wenjin Wang and
                  Hao Wu and
                  Zhiyu Li and
                  Bo Tang and
                  Wenqiang Wei and
                  Jinbo Wang and
                  Zeyun Tang and
                  Shichao Song and
                  Chenyang Xi and
                  Yu Yu and
                  Kai Chen and
                  Feiyu Xiong and
                  Linpeng Tang and
                  Weinan E},
  title        = {Memory\({}^{\mbox{3}}\): Language Modeling with Explicit Memory},
  journal      = {CoRR},
  volume       = {abs/2407.01178},
  year         = {2024}
}

@article{T5,
  author       = {Colin Raffel and
                  Noam Shazeer and
                  Adam Roberts and
                  Katherine Lee and
                  Sharan Narang and
                  Michael Matena and
                  Yanqi Zhou and
                  Wei Li and
                  Peter J. Liu},
  title        = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
                  Transformer},
  journal      = {J. Mach. Learn. Res.},
  volume       = {21},
  pages        = {140:1--140:67},
  year         = {2020},
  url          = {https://jmlr.org/papers/v21/20-074.html},
  timestamp    = {Wed, 11 Sep 2024 14:41:27 +0200},
  biburl       = {https://dblp.org/rec/journals/jmlr/RaffelSRLNMZLL20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{chen-etal-2024-minprompt,
    title = "{M}in{P}rompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering",
    author = "Chen, Xiusi  and
      Jiang, Jyun-Yu  and
      Chang, Wei-Cheng  and
      Hsieh, Cho-Jui  and
      Yu, Hsiang-Fu  and
      Wang, Wei",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.16/",
    doi = "10.18653/v1/2024.acl-long.16",
    pages = "254--266"
}

@inproceedings{data_engineering_for_longcontext,
  author       = {Yao Fu and
                  Rameswar Panda and
                  Xinyao Niu and
                  Xiang Yue and
                  Hannaneh Hajishirzi and
                  Yoon Kim and
                  Hao Peng},
  title        = {Data Engineering for Scaling Language Models to 128K Context},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=TaAqeo7lUh},
  timestamp    = {Mon, 02 Sep 2024 16:55:26 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/FuPNYHK024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{fineweb,
  author       = {Guilherme Penedo and
                  Hynek Kydl{\'{\i}}cek and
                  Loubna Ben Allal and
                  Anton Lozhkov and
                  Margaret Mitchell and
                  Colin Raffel and
                  Leandro von Werra and
                  Thomas Wolf},
  title        = {The FineWeb Datasets: Decanting the Web for the Finest Text Data at
                  Scale},
  journal      = {CoRR},
  volume       = {abs/2406.17557},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.17557},
  doi          = {10.48550/ARXIV.2406.17557},
  eprinttype    = {arXiv},
  eprint       = {2406.17557},
  timestamp    = {Tue, 23 Jul 2024 12:15:30 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2406-17557.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{wang2024large,
  title={Large Scale Knowledge Washing},
  author={Wang, Yu and Wu, Ruihan and He, Zexue and Chen, Xiusi and McAuley, Julian},
  journal={arXiv preprint arXiv:2405.16720},
  year={2024}
}