\section{Related Work}
\vspace{-5pt}
We classify memory-based methods into two categories: Token-Level Memory and Latent-Space Memory, which is similar to the categorizations in \citet{em2} where they classify methods into implicit memory and explicit memory. 

\vspace{-5pt}
\subsection{Token-level Memory}
Token-level memory refers to memory structures represented in textual forms, which can include raw context, summaries~\citep{MemoryBank,zhou2023recurrentgpt}, knowledge graphs~\citep{memgpt,gutierrez2024hipporag}, organized text with hierarchical or graph structures~\citep{memgpt,chen-etal-2024-minprompt}, or databases~\citep{hu2023chatdb}. 
Methods such as MemoryBank~\citep{MemoryBank}, RecurrentGPT~\citep{zhou2023recurrentgpt} incorporate multiple components of memory, including both raw conversational data and summaries. MemGPT \citep{memgpt} proposes treating context and memory as analogous to traditional memory in operating systems, enabling more flexible and organized memory structures. These approaches typically rely on text embeddings for memory retrieval, where queries can originate from either the current conversation turn \citep{MemoryBank,zhou2023recurrentgpt} or queries generated by the language model itself~\citep{memgpt}. In contrast, ChatDB \citep{hu2023chatdb} stores knowledge in a database and performs retrieval using SQL queries, while MemLLM\citep{memllm} fine-tunes the model to generate function calls that initiate searches within a knowledge graph, referred to as ``Triple Memory'' by \citet{memllm}. These methods generally offer benefits such as modularity (with the exception of MemLLM, which requires fine-tuning) and interpretability~\citep{em2}, allowing for potential integration with external systems~\citep{wu2022survey}. However, these approaches have limitations. Some require storing the raw text, which is not the most compressed method to store information~\citep{enhanced_text_compression, nncp_v2, tiny_transformers_for_text_compression}. Others store knowledge in the form of triplets, which may be unsuitable for representing complex conversations that are difficult to convert into triplets~\citep{wang2024large}. 

\begin{figure*}[t]
\centering
\subfigure[Update]{\label{fig:memoryllm_update}\includegraphics[width=0.490\linewidth]{figures/update.png}}
\hfill
\subfigure[Generation]{\label{fig:memoryllm_generate}\includegraphics[width=0.490\linewidth]{figures/generate.png}}
\vspace{-10pt}
\caption{Update and Generation Process of MemoryLLM. We process the chunk with $\phi_l$ to obtain new $K$ tokens during the update process, which is perceived by $\phi$ using cross-attention during the generation process.}
\label{fig:memoryllm_update_generate}
\vspace{-10pt}
\end{figure*}

\begin{figure*}[t]
\centering
\subfigure[Update Process. ]{\label{fig:mplus_update}\includegraphics[width=0.38\linewidth]
{figures/ltm-update.png}}
\hfill
\subfigure[Generation]{\label{fig:mplus_generate}\includegraphics[width=0.610\linewidth]
{figures/ltm-generate.png}}
\vspace{-10pt}
\caption{Update and Generation Process of \ours. For layer $l$, during Update, the old memory pool $\theta_l$ is split into two parts: $K$ dropped tokens and $N-K$ remaining tokens. The dropped tokens are stored in the long-term memory $\Theta_l$ while the remaining tokens and new $K$ tokens are combined to obtain the new memory pool $\theta_l'$. Then during generation, we use our co-trained retriever to retrieve tokens from $\Theta_l$, which is fed into the transformer layer $\phi_l$ along with the short-term memory $\theta_l$ and the query hidden states.}
\label{fig:mplus_update_generate}
\vspace{-10pt}
\end{figure*}

\vspace{-5pt}
\subsection{Latent-Space Memory}
Latent-space memory stores information in a compressed format, embedding knowledge into hidden states~\citep{kNNLM, memoryllm}, model parameters~\citep{self-param}, or an external latent space~\citep{larimar}, among other methods. Some approaches use memory slots to encode information~\citep{al2021memory}, while others rely on key-value caches stored in memory pools for future retrieval~\citep{MemoringTransformers, LongMEM, he2024camelot, Memoria}. Notably, CamelLoT~\citep{he2024camelot} and Memoria~\citep{Memoria} incorporate forgetting mechanisms to better emulate human memory. Similarly, MemoryLLM~\citep{memoryllm} compresses knowledge into hidden states and employs random dropping to prevent unbounded memory growth. The M$^3$ method~\citep{memory3} also stores memory in the hidden-state space, archiving a vast pretraining dataset comprising $1.1\times 10^8$ text chunks. Distinct from methods that utilize hidden states or key-value caches, Larimar~\citep{larimar} introduces a memory matrix that supports read and write operations, demonstrating effectiveness in knowledge-editing tasks. Furthermore, SELF-PARAM~\citep{self-param} explores embedding knowledge directly into model parameters without degrading the model's capabilities or requiring additional parameters. These latent-space memory techniques have shown promising results across various downstream tasks. By saving information in a compressed format and leveraging retrieval during generation, they enable substantial expansions of the context window without incurring excessive GPU memory costs.
Despite the advantages and potential of Latent-Space Memory, existing methods within this category typically fall short when dealing with extremely long input~\citep{larimar,self-param,memoryllm,he2024camelot}. In contrast, \ours can have much longer retention compared to existing methods.