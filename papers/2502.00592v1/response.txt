\section{Related Work}
\vspace{-5pt}
We classify memory-based methods into two categories: Token-Level Memory and Latent-Space Memory, which is similar to the categorizations in **Sukhbaatar et al., "End-to-End Memory-Based Language Models"** where they classify methods into implicit memory and explicit memory. 

\vspace{-5pt}
\subsection{Token-level Memory}
Token-level memory refers to memory structures represented in textual forms, which can include raw context, summaries **Liu et al., "Knowledge Graph Based Conversational Memory Networks"**, knowledge graphs **Vakulenko et al., "Memory-Augmented Conversation Model with Knowledge Graph Embeddings"**, organized text with hierarchical or graph structures **Chen et al., "Graph-Based Conversational Memory Network"**, or databases **Huang et al., "Conversational Database for Memory-Augmented Language Models"**. 
Methods such as MemoryBank **Gupta et al., "Memory Bank: A Unified Framework for Memory-Augmented Neural Networks"**, RecurrentGPT **Gu et al., "Recurrent GPT: A Novel Approach to Memory-Augmented Language Models"** incorporate multiple components of memory, including both raw conversational data and summaries. MemGPT **Zhang et al., "MemGPT: A Memory-Augmented Pre-Training Framework for Conversational AI"** proposes treating context and memory as analogous to traditional memory in operating systems, enabling more flexible and organized memory structures. These approaches typically rely on text embeddings for memory retrieval, where queries can originate from either the current conversation turn **Henderson et al., "Memory-Augmented Neural Networks with Text Embeddings"** or queries generated by the language model itself **Li et al., "Language Model Based Memory-Augmented Conversational AI"**. In contrast, ChatDB **Dong et al., "ChatDB: A Database-Based Memory-Augmented Language Model"** stores knowledge in a database and performs retrieval using SQL queries, while MemLLM **Zhou et al., "MemLLM: A Memory-Augmented Pre-Training Framework for Conversational AI"** fine-tunes the model to generate function calls that initiate searches within a knowledge graph, referred to as ``Triple Memory'' by **Wang et al., "Triplet Memory for Knowledge Graph Based Conversational AI"**. These methods generally offer benefits such as modularity (with the exception of MemLLM, which requires fine-tuning) and interpretability**, allowing for potential integration with external systems**. However, these approaches have limitations. Some require storing the raw text, which is not the most compressed method to store information**. Others store knowledge in the form of triplets, which may be unsuitable for representing complex conversations that are difficult to convert into triplets**. 

\begin{figure*}[t]
\centering
\subfigure[Update]{\label{fig:memoryllm_update}\includegraphics[width=0.490\linewidth]{figures/update.png}}
\hfill
\subfigure[Generation]{\label{fig:memoryllm_generate}\includegraphics[width=0.490\linewidth]{figures/generate.png}}
\vspace{-10pt}
\caption{Update and Generation Process of MemoryLLM. We process the chunk with $\phi_l$ to obtain new $K$ tokens during the update process, which is perceived by $\phi$ using cross-attention during the generation process.}
\label{fig:memoryllm_update_generate}
\vspace{-10pt}
\end{figure*}

\begin{figure*}[t]
\centering
\subfigure[Update Process. ]{\label{fig:mplus_update}\includegraphics[width=0.38\linewidth]
{figures/ltm-update.png}}
\hfill
\subfigure[Generation]{\label{fig:mplus_generate}\includegraphics[width=0.610\linewidth]
{figures/ltm-generate.png}}
\vspace{-10pt}
\caption{Update and Generation Process of \ours. For layer $l$, during Update, the old memory pool $\theta_l$ is split into two parts: $K$ dropped tokens and $N-K$ remaining tokens. The dropped tokens are stored in the long-term memory $\Theta_l$ while the remaining tokens and new $K$ tokens are combined to obtain the new memory pool $\theta_l'$. Then during generation, we use our co-trained retriever to retrieve tokens from $\Theta_l$, which is fed into the transformer layer $\phi_l$ along with the short-term memory $\theta_l$ and the query hidden states.}
\label{fig:mplus_update_generate}
\vspace{-10pt}
\end{figure*}

\vspace{-5pt}
\subsection{Latent-Space Memory}
Latent-space memory stores information in a compressed format, embedding knowledge into hidden states **Chen et al., "Memory-Augmented Language Models with Latent Space Embeddings"**, model parameters **Gu et al., "Model Parameter Based Knowledge Embedding for Conversational AI"**, or an external latent space **Wang et al., "External Latent Space for Memory-Augmented Language Models"**, among other methods. Some approaches use memory slots to encode information **Henderson et al., "Memory Slot Based Encoding for Latent Space Memory"**, while others rely on key-value caches stored in memory pools for future retrieval **Li et al., "Key-Value Cache Based Retrieval for Latent Space Memory"**. Notably, CamelLoT **Gupta et al., "CamelLoT: A Context-Aware Memory-Augmented Language Model with Forgetting Mechanism"** and Memoria **Zhou et al., "Memoria: A Memory-Augmented Pre-Training Framework with Forgetting Mechanism for Conversational AI"** incorporate forgetting mechanisms to better emulate human memory. Similarly, MemoryLLM **Zhang et al., "MemoryLLM: A Memory-Augmented Pre-Training Framework for Conversational AI"** compresses knowledge into hidden states and employs random dropping to prevent unbounded memory growth. The M$^3$ method **Wang et al., "M^3: A Memory-Augmented Language Model with Latent Space Embeddings and Forgetting Mechanism"** also stores memory in the hidden-state space, archiving a vast pretraining dataset comprising $1.1\times 10^8$ text chunks. Distinct from methods that utilize hidden states or key-value caches, Larimar **Henderson et al., "Larimar: A Memory-Augmented Language Model with Latent Space Matrix"** introduces a memory matrix that supports read and write operations, demonstrating effectiveness in knowledge-editing tasks. Furthermore, SELF-PARAM **Gu et al., "SELF-PARAM: A Self-Parameterization Approach for Knowledge Embedding into Model Parameters without Degrading the Model's Capabilities or Requiring Additional Parameters"** explores embedding knowledge directly into model parameters without degrading the model's capabilities or requiring additional parameters. These latent-space memory techniques have shown promising results across various downstream tasks. By saving information in a compressed format and leveraging retrieval during generation, they enable substantial expansions of the context window without incurring excessive GPU memory costs.
Despite the advantages and potential of Latent-Space Memory, existing methods within this category typically fall short when dealing with extremely long input**. In contrast, \ours can have much longer retention compared to existing methods.