\section*{Impact Statement}
This work introduces a memory-augmented approach for Large Language Models (LLMs), enabling them to more effectively retain and retrieve long-term information and thereby offering potential benefits in areas such as education, research, and industry. The increased memory capacity could potentially raise concerns regarding AI safety, reliability, and fairness. If not carefully managed, these models could propagate biased content over extended text spans or store sensitive information for unintended durations. It is therefore crucial to employ robust safeguards, including bias mitigation strategies and ongoing oversight, to prevent misuse or the reinforcement of harmful content. Beyond considerations already inherent to LLMs, we do not foresee other significant societal impacts arising from this work.