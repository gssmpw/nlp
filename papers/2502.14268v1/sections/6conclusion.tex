\section{Conclusion}\label{sec:conclusion}

In this paper, we propose \uqeval, a simple framework using multiple-choice QA datasets to evaluate confidence measures for natural language generation.
We first highlight the unreliability of widely used \textit{correctness functions} in existing evaluation frameworks.
To address this, we propose an alternative approach that reformulates multiple-choice questions into a free-form QA prompt, enabling a more efficient evaluation with higher-quality correctness labels. 
Experiments across diverse datasets and state-of-the-art LLMs demonstrate that \uqeval produces consistent results aligned with prior research findings, while eliminating dependence on costly correctness functions.



%In this paper, we propose a simple framework (\uqeval) to use multiple-choice question-answering datasets to evaluate confidence measures designed for natural language generation.
%We first demonstrate the limitations of widely used \textit{correctness functions} in existing evaluation frameworks, highlighting their unreliability. 
%To address this, we propose an alternative approach that reformulates multiple-choice questions into a free-form QA prompt, enabling a more economical evaluation with higher-quality correctness labels. 
%Our experiments across diverse datasets and state-of-the-art LLMs show that \uqeval produces consistent evaluation results, and aligns with key observations from prior research â€” without relying on expensive correctness functions.
% We tested \uqeval on datasets from various domains and several recent LLMs, which showed that \uqeval provides consistent evaluation results, and identified observations similar to prior research yet without the need of the costly correctness function.



\section*{Limitations}
While our proposed evaluation framework avoids the use of correctness functions, offering speed and reliability, it also has its limitations.
As noted in \cref{sec:method}, \uqeval should not serve as the \emph{only} evaluation method.
Bypassing response generation provides no guarantee that injected options resemble what would otherwise be generated by the base LM.
%To begin with, as discussed at the end of \cref{sec:method}, we do not recommend \textit{only} using \uqeval to evaluate confidence measures because skipping the response generation step means that there is no guarantee that the injected options resemble what would otherwise be generated by the base LM.
If the goal is to evaluate confidence measures \textit{for a specific LM and its generation}, then this generation step by definition should not be skipped.
Further, certain trained confidence measures (e.g. linear probes on the LM's internal states) might not generalize as well to injected options, and may perform systematically worse in \uqeval than in the current framework (although one may argue that generalizability should be part of the evaluation to begin with).
Finally, \uqeval currently only applies to confidence measures, but we do not see a straightforward adaption to uncertainty measures. 
We hope future research could continue to improve the evaluation of confidence, and potentially, of uncertainty measures.
