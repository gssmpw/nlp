\section{Introduction}\label{sec:intro}

Large Language Models (LLMs) demonstrate strong performance across natural language processing tasks, yet their architectural complexity and limited interpretability can produce unreliable outputs. 
This presents significant challenges in critical domains such as healthcare, where output errors carry serious consequences. 
Confidence estimation methods have emerged to quantify output reliability. 
The field connects closely with uncertainty quantification in natural language generation, as both address output trustworthiness. 
Current approaches divide into consistency-based methods, which analyze agreement across multiple outputs, and internal-states methods that leverage model-specific features like output probabilities.
Despite advances in these approaches, developing robust evaluation frameworks remains a central challenge.

%Large Language Models (LLMs) have achieved notable success in various natural language processing (NLP) tasks, such as text generation, question answering, and reasoning. However, their complex architectures and lack of interpretability often lead to uncertain, incorrect, or ambiguous outputs. This raises critical concerns about their reliability, especially in high-stakes domains like healthcare and decision-making, where errors can have severe consequences. Confidence estimation has emerged as a key tool for addressing these concerns by quantifying the trustworthiness of LLM outputs.


%In natural language generation (NLG), confidence estimation is closely tied to uncertainty quantification (UQ), as both aim to assess the reliability of model outputs. 
%% Confidence scores are often derived from uncertainty measures, with higher uncertainty corresponding to lower confidence. 
%Existing methods for confidence estimation in LLMs can be broadly categorized into two types: consistency-based black-box methods, which rely on agreement among multiple generated responses, and internal-states-based methods, which use model-specific information such as output probabilities or self-evaluation mechanisms. 
%While these approaches have advanced the field, the evaluation of confidence estimation methods remains a significant challenge in their evaluation process.\cc{consider say we mainly do UQ evaluation earlier, cut the context. at least merge the first two paragraphs}

%Currently, the evaluation frameworks for confidence measures in NLG primarily rely on correctness labels to compute metrics such as area under the receiver operating characteristic curve (AUROC) and accuracy-rejection curve (AUARC). 
%These frameworks follow a general pipeline: 
%(1) generating predictions from the model,
%(2) assigning correctness labels to the predictions using a \textit{correctness function} $f(\cdot)$, and 
%(3) calculating evaluation metrics based on these labels. 
%However, the reliance on correct labels introduces several limitations. 
%Human evaluation, though reliable, is time-consuming and impractical for large-scale datasets. 
%Automated reference-based metrics like BLEU and ROUGE fail to account for semantically correct but differently phrased responses. 
%LLM-based evaluators offer flexibility but still suffer from systematic biases~\cite{lin2022truthfulqa}, such as favoring outputs from similar models\cc{add citation here?} or being sensitive to prompt variations—issues that lack concrete evidence in prior work\cc{add citation here?}. 

Current evaluation frameworks for NLG confidence measures rely on correctness labels to compute metrics such as AUROC and AUARC. 
These frameworks follow a three-step process: generating model predictions, labeling correctness via a function $f(\cdot)$, and calculating metrics. 
This label-dependent approach faces several constraints. While human evaluation provides reliable correctness ground truth, it cannot scale to large datasets. 
Metrics based on reference matching, such as BLEU and ROUGE, fail to recognize semantically equivalent responses phrased differently.
% Reference matching-based metrics like BLEU and ROUGE miss semantically equivalent responses with different phrasing. 
% LLM-based evaluators offer greater capability and scalability, but are still noisy, and could introduce systematic biases, such as favor over responses generated by similar LMs~\cite{panickssery2024llm} or over length~\cite{lin2022truthfulqa}.
LLM-based evaluators offer greater capability but remain noisy and may introduce systematic biases, such as favoring responses generated by themselves or similar LMs~\cite{panickssery2024llm}, or preferring longer responses~\cite{lin2022truthfulqa}.
Moreover, running such evaluators could be expensive.
% LLM-based evaluators improve capability and scalability but remain noisy and may introduce systematic biases, such as favoring responses generated by similar LMs~\cite{panickssery2024llm} or preferring longer responses~\cite{lin2022truthfulqa}.
% (also observed in our own experiments) or over length~\cite{lin2022truthfulqa}.
% The prompt used to eli
%including outputs from similar models\cc{add citation here?} and prompt sensitivity—phenomena that remain empirically understudied\cc{add citation here?}.


%More critically, flaws in the correctness function $f(\cdot)$ can propagate through the evaluation pipeline, affecting evaluation metrics like AUROC. 
%This fragility can lead to misleading conclusions about which confidence estimation method performs better, particularly when different methods yield similar performance. 
%These issues highlight the need for alternative evaluation frameworks with reliable correctness labels.

% More critically, flaws in the correctness function $f(\cdot)$ propagate through the evaluation pipeline, affecting metrics like AUROC. 

Flaws in the correctness function $f(\cdot)$ propagate through the evaluation pipeline, affecting metrics like AUROC. This sensitivity becomes particularly problematic when comparing confidence estimation methods with similar performance. Such limitations underscore the need for evaluation frameworks that establish correctness more reliably.


%In this paper, we propose a simple evaluation framework that addresses these limitations by leveraging multiple-choice question-answering (QA) datasets. 
%Our approach removes the dependence on a correctness function by using the structure of multiple-choice QA tasks to evaluate confidence measures directly. 
%Importantly, our framework is complementary to existing pipelines rather than a replacement; it provides an additional perspective to validate the discriminative power of confidence measures across different evaluation settings.

In this paper, we propose \uqeval, a simple, efficient yet effective evaluation framework that eliminates the dependence on unreliable correctness functions.
% leverages multiple-choice question-answering (QA) datasets to address these limitations.
\textit{The key insight is to leverage multiple-choice question-answering (QA) datasets, which inherently provide gold-standard answer choices at no cost.} 
With these definitive labels, our framework bypasses the ambiguity of determining correctness via correctness function $f(\cdot)$ and ensures an objective assessment of confidence estimation methods. 
% By leveraging multiple-choice QA datasets, our proposed evaluation framework ensures an objective assessment of confidence estimation methods.
% Unlike conventional evaluation pipelines that rely on an often noisy correctness function $f(\cdot)$ to determine whether a model’s prediction is right or wrong, our approach directly utilizes gold-standard answer choices in multiple-choice QA tasks. 
% \textit{Our approach eliminates the dependence on unreliable correctness functions} and ensures an objective assessment of confidence estimation methods.
% By exploiting the inherent structure of multiple-choice QA datasets, our approach eliminates dependence on unreliable correctness. 
Rather than replacing existing evaluation pipelines, our framework complements them, offering an additional lens to assess the discriminative power of confidence estimation methods.
% across different evaluation settings.
\cref{fig:pipeline} shows how our proposal (green) and the existing evaluation pipeline (blue) differ, yet complement each other.
% In this paper, we propose a simple yet effective evaluation framework that eliminates reliance on noisy correctness functions. Our key insight is to leverage multiple-choice question-answering (QA) datasets, which inherently provide gold-standard answer choices. By using these definitive labels, our framework bypasses the ambiguity of determining correctness via 
% f(⋅) and ensures an objective assessment of confidence estimation methods. 
% Rather than replacing existing evaluation pipelines, our approach complements them, offering an additional lens to assess the discriminative power of confidence measures. \cref{fig:pipeline} illustrates how our proposal (green) differs from, yet aligns with, the existing pipeline (blue).
Our contributions are summarized as follows:
\begin{itemize}[leftmargin=*,nosep]
    \item We demonstrate that commonly used evaluation methods for NLG confidence measures are sensitive to noise in correctness labels, which can lead to misleading conclusions about evaluation metrics and rankings of different confidence estimation approaches.
    %We show that the most popular evaluation methods for NLG confidence measures are prone to noise in the ``correctness label`, which could further lead to problematic conclusions. %(about which measure is better)
    \item We propose a simple yet effective method that utilizes multiple-choice QA datasets to evaluate confidence measures, supporting both internal-states-based white-box and consistency-based black-box methods.
    \item Extensive experiments across recent LLMs and QA datasets verify that \uqeval produces stable evaluations broadly consistent with existing methods, while eliminating the need for expensive correctness functions.
    %We conduct extensive experiments on recent LLMs and popular QA datasets to verify that \uqeval provides stable evaluation results that are often consistent with existing evaluation methods, without the costly correctness function.
\end{itemize}

