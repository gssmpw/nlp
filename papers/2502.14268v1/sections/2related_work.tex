\section{Related Work}\label{sec:related}

%In this section, we will introduce methods for assessing confidence in NLG and existing evaluation methods. It is important to note that the uncertainty measure and confidence measure are distinct yet highly correlated concepts in NLG domain. We will discuss their difference in the following section.

% In this section, we examine confidence assessment methods in NLG and their evaluation approaches. 
% It is important to note that while uncertainty and confidence measures are notions that correlate strongly in NLG, they remain distinct concepts, as detailed in what follows.


\paragraph{Confidence Estimation}
Confidence estimation is fundamental to machine learning, providing mechanisms to assess model reliability and guide decision-making across tasks.
Early confidence estimation research concentrated on classification settings, where confidence scores enabled Selective Classification~\cite{geifman2017selective,el2010foundations,feng2022towards}—allowing models to abstain from low-quality predictions. 
The rapid advancement of NLG and LLMs has brought renewed attention to confidence estimation. 
While NLG poses unique challenges due to semantic invariance and vast output spaces~\cite{kuhn2023semantic}, recent works have advanced the field by measuring similarities among sampled responses~\cite{lin2024generating} and deriving measures from LMs' internal states~\cite{malininuncertainty,CSL,azaria-mitchell-2023-internal}.


%Confidence Estimation has long been a fundamental aspect of machine learning, serving as a mechanism to assess model reliability and guide decision-making in various tasks. 
%Early work on confidence estimation primarily focused on classification settings, where confidence scores were used to enable Selective Classification~\cite{geifman2017selective,el2010foundations,feng2022towards}—allowing models to abstain from making predictions when uncertainty is high. 
%% Other applications include out-of-distribution detection (Hendrycks & Gimpel, 2017) and active learning (Settles, 2009), where confidence estimates help identify uncertain instances for annotation. 
%% These techniques have played a crucial role in ensuring robustness in safety-critical domains such as medical diagnosis (Jiang et al., 2021) and autonomous driving (Michelmore et al., 2018).
%With the rapid advancements of NLG and LLMs, confidence estimation has gained renewed attention. 
%Unlike classification tasks, NLG presents unique challenges due to semantic invariance and vast output space~\cite{kuhn2023semantic}, but several recent works still push the boundary, by measuring sampled responses' similarities~\cite{lin2024generating} or deriving such measures from LMs' internal states~\cite{malininuncertainty,duan-etal-2024-shifting,CSL}.
%% Researchers have explored techniques like Bayesian uncertainty modeling (Gal & Ghahramani, 2016), deep ensembles (Lakshminarayanan et al., 2017), and logit-based confidence estimation (Desai & Durrett, 2020) to assess the reliability of generated text. However, despite progress, uncertainty estimation in NLG remains an open problem, as models often fail to align confidence with factual accuracy (Zellers et al., 2021).

A related aspect is calibration. While extensively considered in classification~\cite{ICML2020_MixNMatch_KDEEval,kull2019beyond,ICML2021_MetaCal}, it has received lass attention in NLG.
Since the distribution of confidence scores could vary significantly across different methods due to their underlying principles~\cite{geng2023survey, da2024llm}, calibrated confidence measures align better with human intuition for probabilities and are more interpretable~\cite{ICML2017_Guo,cosmides1996humans}.
While this paper focuses on evaluating confidence estimation methods, the same framework could be applied to evaluate future NLG calibration methods.
We demonstrate this by including results using common calibration metrics like Expected Calibration Error (ECE).
% first researchers have developed techniques to align these scores with actual accuracy~\cite{lin2024generating}. 
% Calibration is however distinct from estimation: the former adjusts confidence scores to better reflect accuracy, while the latter derives these scores from model outputs.


%Another important aspect of confidence estimation research is calibration. Since confidence scores are unbounded and can vary significantly across different methods due to their underlying principles~\cite{geng2023survey, da2024llm}, researchers have explored various techniques to make confidence scores more representative of actual accuracy~\cite{lin2024generating}. Calibration and estimation are distinct tasks: calibration focuses on adjusting the numerical values of confidence scores to better reflect accuracy, while estimation aims to derive meaningful confidence scores from model outputs.  
% \cc{im confused, why do we introduce callibration here }



% Confidence estimation work in NLG often focuses on calibration~\cite{mielke-etal-2022-reducing,si-etal-2022-examining,xiong2024can}.
% \fontred{
% Although our experiments focus on evaluating the ranking of confidence measures, which applies to a wider range of confidence measures than calibration, the framework could easily be adapted to measuring calibration as well.
% Although calibration is not directly related to our primary focus---our main interest lies in ranking the confidence of different measures---we include results on the calibrated performance in the Appendix.
% discuss calibration and suggests that it's a different task (but still use our method)
%}

\paragraph{Evaluation of Confidence Measures}
While confidence estimation has received considerable attention, the evaluation of confidence measures remains under-explored. 
Many evaluation methods have been adapted from the classification literature, including Expected Calibration Error (ECE)~\cite{ICML2017_Guo,xiong2024can} and Area Under the Receiver Operating Characteristic Curve (AUROC)~\cite{kuhn2023semantic}. 
These metrics assess the relationship between confidence scores and prediction accuracy, typically requiring high-quality correctness labels for the evaluated responses.

However, obtaining reliable correctness labels in NLG is challenging due to factors such as semantic variability and ambiguity in open-ended tasks~\cite{novikova2017we}. 
Unlike classification where correctness is well-defined, NLG correctness is often determined through human annotation, LLM-based judges, or similarity-based comparisons between the generated and reference answers. 
These approaches are costly and often unreliable, as correctness judgments can be subjective and inconsistent~\cite{gatt2018survey}.

Recent works have attempted to mitigate these limitations. 
To allow for non-binary correctness measures, Rank Calibration Error (RCE)~\cite{RCEhuang2024} and AUARC~\cite{auarc,lin2024generating} were introduced, both of which leverage continuous correctness scores. 
Other approaches focus on improving correctness scores themselves. 
For example, \citet{CSL} aggregates predictions from multiple LLM-based judges and takes a consensus to enhance reliability.

Unlike these methods, our proposed framework completely circumvents the need for correctness labels, making it more robust and scalable for evaluating confidence measures in NLG. 

% While confidence estimation itself has received considerable attention, the \textbf{evaluation of confidence measures} remains relatively under-explored. 
% Various evaluation methods were borrowed from the classification literature, including ECE~\cite{ICML2017_Guo,xiong2024can,123}, AUPRC~\cite{123},  AUROC~\cite{kuhn2023semantic,123}. % , and AUARC~\cite{lin2024generating,123}
% These metrics typically depend on high quality correctness label of the response being evaluated.
% Sharing a similar motivation as this paper, some recent works try to remove or reduce the impact of this limitation.
% To allow for non-binary correctness measures, \cite{RCEhuang2024} proposed Rank Calibration Error, and~\cite{lin2024generating} suggests using AUARC~\cite{auarc} - both allow for directly using continuous correctness scores (from rule-based matching, judge LLMs, or human evaluation). 
% To improve the score itself, ~\cite{CSL} uses multiple judge LLMs and take the consensus.
% Unlike these methods, our proposal circumvents the need for such correctness scores, which are not only expensive to acquire but also unreliable, altogether. 

\paragraph{Applications of Confidence Measures}
Confidence measures play a crucial role in several downstream research areas in NLG, particularly in conformalized NLG and selective generation or generation with abstention. 
%, particularly in Conformal Prediction~\cite{papadopoulos2007conformal} and Selective Generation/Generation with Abstention~\cite{MCConf-pmlr-v239-ren23a}. 
% These applications rely on high-quality confidence estimation to enhance model reliability and decision-making.
% Finally, there are many active downstream research areas that depend on high-quality confidence measures, including \textbf{conformalized generation}, and \textbf{selective generation/generation with abstention}.
% In regression and classification tasks, conformal prediction is a powerful tool that issues a prediction set that is guaranteed to cover the unknown target $Y$ with a pre-specified high-probability.
Stemming from Conformal Prediction~\cite{papadopoulos2007conformal}, in the context of NLG, conformalized methods typically aim to create a set of generation that satisfies a particular user-defined quality goal (e.g. ``correct answers'')~\cite{quach2023conformal,gui2024conformal,lee2024selective,yadkori2024mitigating}, or providing factual guarantees basing on parts of the generation~\cite{cherian2024large,mohri2024}.
Selective generation or generation with abstention, on the other hand, deals with broader considerations that involve refraining from generating if the confidence score is low, with goals like improving the accuracy on the non-rejected portion~\cite{MCConf-pmlr-v239-ren23a,cole2023selectively}.
Good confidence measures that can distinguish high and low-quality generations are key ingredients to all these research directions, and our paper aims to provide a better evaluation framework for researchers to identify such confidence measures.
