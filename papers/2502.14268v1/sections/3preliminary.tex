
\section{Confidence Estimation for NLG}\label{sec:prelim}

% \paragraph{Problem Definition}
First, we establish notation and introduce relevant definitions.
Let $\mathcal{M}$ be a language model, $\xInput\in\Sigma^*$ be an input prompt, and $\predSeq=\mathcal{M}(\xInput)\in\Sigma^*$ be the output. 
$\Sigma$ denotes the vocabulary, which includes tokens from modern tokenizers or natural language symbols like alphabet letters. 
For free-form NLG datasets, we typically have reference answers $A={a_1,\ldots,a_m}$ alongside $\xInput$. A \textit{confidence estimation method} is a function that assigns a confidence score to model output $\predSeq$ given input $\xInput$. 
Formally, a confidence measure is defined as:
\begin{equation}
    C_{\mathcal{M}}: (\xInput, \predSeq) \in \Sigma^*\times\Sigma^*  \mapsto \mathbb{R},
\end{equation}
where $C_{\mathcal{M}}(\xInput,\predSeq)$ represents the confidence score of $\predSeq$. This notation accounts for both model-agnostic and model-specific confidence measures.


%First, we fix notations and introduce relevant definitions. 
%% We will focus on auto-regressive LMs, the current dominant paradigm.
%Let $\mathcal{M}$ be a language model, $\xInput\in\Sigma^*$ be an input prompt, and $\predSeq=\mathcal{M}(\xInput)\in\Sigma^*$ be the output.
%Here, $\Sigma$ denotes the vocabulary, which could be all tokens of a modern tokenizer, or natural language symbols such as the alphabet letters. 
%For most free-form NLG datasets, apart from the $\xInput$, we typically also have a set of reference answers, denoted as $A=\{a_1,\ldots,a_m\}$.

%A \textit{confidence estimation method} is a function that assigns a confidence score to a model output $\predSeq$, with respect to a given input $\xInput$. 
%Formally, a confidence measure id defined as a mapping:
%\begin{equation}
%    C_{\mathcal{M}}: (\xInput, \predSeq) \in \Sigma^*\times\Sigma^*  \mapsto \mathbb{R},
%\end{equation}
%where $C_{\mathcal{M}}(\xInput,\predSeq)$ represents the confidence score of $\predSeq$. Note that such notation allows for both model-agnostic confidence measures, as well as model-specific ones.
% We may write $C_M($



%, is a mapping of any $\mathcal{M}$, input $\xInput$ and $\predSeq$ to a real number.




\subsection{Confidence Estimation Methods}
\label{confidence_methods}

% At the center of \cref{fig:pipeline} lie the confidence estimation methods to be evaluated. 
Existing confidence estimation methods can be broadly divided into two categories: Consistency-based black-box methods and internal state-based white-box methods\footnote{We consider logits as an internal states here.}.


%At the center of \cref{fig:pipeline} sits the confidence estimation methods, which are the candidates to be evaluated.  
%Current confidence estimation methods can be broadly categorized into two groups: Consistency-based black-box methods and internal-state-based white-box methods\footnote{Here, we consider logits as an internal-state.}.
% The overall pipeline of the confidence estimation is illustrated in \cref{fig:pipeline}, where the yellow arrows depict the current workflow. The process begins with multiple-choice generation by different LLMs. 
% Both black-box and white-box methods are then applied to compute the confidence score for each response of interest:
% Simultaneously, correctness labels are assigned using reference matching or human evaluation. 
% Specifically:

\textbf{Black-Box Methods} leverage response consistency across LLM generations~\cite{lin2024generating,manakul-etal-2023-selfcheckgpt}. 
Higher consistency among generated responses indicates higher confidence in $\predSeq$. 
These methods first compute pairwise response similarities, then derive confidence from the similarity matrix.
% Implementation involves computing pairwise response similarities, then deriving confidence from the similarity matrix. 
For similarity computation, existing methods use
% Methods employ 
Jaccard similarity, NLI models~\cite{he2021deberta}, and BERTScore~\cite{Zhang2020BERTScore} for similarity computation.


%\textbf{Black-Box confidence estimation methods} rely on the consistency of LLM-generated responses~\cite{lin2024generating,manakul-etal-2023-selfcheckgpt}.
%The intuition is that if a response $\predSeq$ is more consistent with other generated responses, it is likely to have a higher confidence score. 
%This is achieved through pairwise similarity measurement among responses, followed by confidence computation from the similarity matrix.
%Recent work has used Jaccard similarity, NLI models~\cite{he2021deberta}, BERTScore~\cite{Zhang2020BERTScore} for such similarity measures.
% Recent work has used Jaccard similarity, which measures overlap between responses by treating them as sets, or Natural Language Inference (NLI), which assesses semantic consistency based on entailment and contradiction relationships. 
% Once the similarity matrix is constructed, confidence scores can be derived by computing degree, which quantifies how well a response aligns with others, and eccentricity, which measures how much a response deviates from the most central responses. Further details can be found in~\cite{lin2024generating}.


\textbf{White-Box Methods} use the internal states of LLMs—including logit distributions and token-level probabilities—to estimate confidence. 
% use LLM internal states, including logit distributions and token-level probabilities, to estimate confidence. 
Recent research has adopted sequence likelihood~\cite{CSL}, which computes confidence from the probability of the complete generated response.
\baselineNLLNorm~\cite{vashurin2024benchmarking} extends this by normalizing for response length via average sequence likelihood. 
Recent refinements weigh tokens differently: \baselineSAR~\cite{duan-etal-2024-shifting} uses NLI for token importance, while Contextualized Sequence Likelihood (\baselineCSL, and its variant\baselineCSLNext)~\cite{CSL} weighs using attention values. 
Other approaches train probes on LLM internal activations and embeddings~\cite{ren2023outofdistribution,azaria-mitchell-2023-internal,li2023inferencetime}. 
Furthermore, the verbalized confidence (\baselinePTrue)~\cite{xiong2024can} elicits explicit ``True'' or ``False'' predictions. 
% While this could use the frequency of ``True' across multiple generations, in practice this is typically implemented by computing from the logits.
While this is technically possible by taking the frequency of ``True'' among multiple sampled generations, in practice it is typically implemented by computing from the logits. Note that uncertainty quantification in NLG is a closely related research direction, yet differs in a key way: uncertainty characterizes the predictive distribution rather than a specific $\predSeq$. For more details of this distinction, see~\citet{lin2024generating}.
% as well.


% \textbf{White-Box Methods} leverage the internal states of large language models (LLMs)—including logit distributions and token-level probabilities—to estimate confidence. Recent research has adopted sequence likelihood~\cite{CSL}, which computes confidence based on the probability of the entire generated response. \baselineNLLNorm~\cite{vashurin2024benchmarking} extends this approach by normalizing for response length via the average sequence likelihood.
% Additional refinements weigh tokens differently. For example, \baselineSAR~\cite{duan-etal-2024-shifting} employs natural language inference (NLI) to determine token importance, while Contextualized Sequence Likelihood (\baselineCSL)~\cite{CSL} leverages attention values for weighting. Other methods train probes on LLM internal activations and embeddings~\cite{ren2023outofdistribution,azaria-mitchell-2023-internal,li2023inferencetime}. Furthermore, the verbalized confidence approach (\baselinePTrue)~\cite{xiong2024can} elicits explicit ``True'' or ``False'' predictions. Although it is technically possible to compute confidence by measuring the frequency of ``True'' responses across multiple sampled generations, in practice this is typically implemented by computing from the logits.



%\textbf{White-Box confidence estimation methods} leverage internal states of the LLM, such as the distribution of logits and token-level probabilities, to estimate confidence. 
%Recent research has widely adopted Sequence Likelihood~\cite{CSL}, which computes confidence based on the probability assigned to the entire generated response. 
%\baselineNLLNorm~\cite{vashurin2024benchmarking} extends this approach by accounting for response length, measuring the average sequence likelihood. 
%Recent work proposed refinements of \baselineNLLNorm by assigning different weights to tokens: \baselineSAR~\cite{duan-etal-2024-shifting} uses NLI to infer token importance, and Contextualized Sequence Likelihood (\baselineCSL)~\cite{CSL} assigns weights basing on attention values.
%refines confidence estimation by assigning different weights to tokens based on attention values extracted from the LLM, while its extension. 
%Others explore training some probes on the internal activations or embeddings from the LLM~\cite{ren2023outofdistribution,azaria-mitchell-2023-internal,li2023inferencetime}.
% Such methods, however, often require 
% The CSL-Next method, further improves confidence estimation by retrieving attention values for the next token after response generation. 
%Verbalized confidence (\baselinePTrue)~\cite{xiong2024can} asks the LLM to explicitly predict whether a response is ``True'' or ``False''.
%While this is technically achievable by taking the frequency of ``True'' among multiple sampled generations from the LLM as the confidence level, in practice it is typically implemented by computing from the logits as well.


\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/uq_eval_pipeline.pdf}
    % \caption{}
    % \vspace{-3mm}
    \caption{
    Illustration of the existing evaluation framework (blue) vs our proposed \uqeval (green).
    Unlike existing frameworks, we avoid the costly and unreliable correctness function module by using multiple-choice datasets.
    This requires slight modification to the confidence estimation steps, which is elaborated in \cref{sec:method}.
    % The choices' correctness could be directly fed into the final evaluation metrics.
    }
    \label{fig:pipeline}
\end{figure*}



%Note that uncertainty quantification in natural language generation is another line of research that is highly correlated, yet the key difference being that uncertainty is a property of the predictive distribution that does not pertain to a particular `$\predSeq$'. 
%Please refer to~\cite{lin2024generating} for a more thorough discussion on the distinction between uncertainty and confidence.



\subsection{Existing Evaluation Methods}\label{sec:prelim:old_eval}

Intuitively, a higher confidence score should correlate with the quality of model generation $\predSeq$ and its correctness relative to input $\xInput$. 
This assumption underpins selective classification, confidence scoring, and uncertainty quantification. 
In selective classification, also termed prediction with a rejection option, models abstain from low-confidence predictions, thereby reducing error rates while maximizing coverage~\cite{JMLR:v24:21-0048,geifman2017selective}. 
In other words, confidence measures guide selection towards predictions that are likely to be correct.



%Intuitively, a higher confidence score should indicate that a particular model generation $\predSeq$ is of higher quality and more likely to be ``correct'' with respect to the input $\xInput$.
%This assumption underlies most work on selective classification, confidence scoring, and uncertainty quantification. 
%In selective classification (also known as prediction with a reject option), models can choose to abstain from making a prediction when confidence is low, thereby reducing error rates while maximizing coverage~\cite{JMLR:v24:21-0048,geifman2017selective}. 
%In other words, confidence measures are used to guide the selection of predictions that are more likely to be correct.

This idea extends naturally to NLG, where confidence measures are used to guide selective generation or generation with abstention.
Assuming a given \textit{correctness function}~\cite{RCEhuang2024} $\acc(\predSeq;\xInput)\in\{0,1\}$, which tells us whether a response is good or correct\footnote{This could sometimes be relaxed to have a continuous range of $\mathbb{R}$, instead of $\{0,1\}$, but certain evaluation metrics such as AUROC require binary correctness labels.}, several evaluation metrics are used to assess confidence measures for NLG:
\begin{itemize}[leftmargin=*, nosep]
    \item Area Under the Receiver Operating Characteristic Curve (AUROC): 
    % Denoting $TPR(t)$ ($FPR(t)$) as the true (false) positive rate when 
    \begin{equation}
        \int_{-\infty}^{\infty} \text{TPR}(t) \, d\text{FPR}(t),
    \end{equation}
    where $\text{TPR}(t)$ ($\text{FPR}(t)$) is the true (false) positive rate comparing $\mathbbm{1}\{C(\predSeq)>t\}$ and $\acc(\predSeq)$, the correctness of $\predSeq$.
    AUROC measures how well the confidence scores distinguish between correct and incorrect responses.
    %C_{\mathcal{M}}(\xInput,\predSeq)
    
    \item Area Under the Accuracy-Rejection Curves (AUARC)~\cite{auarc}: 
    \begin{equation}
        \int_{-\infty}^{\infty} \text{Accuracy}(t) \, d\text{Coverage}(t),
    \end{equation}
    where $\text{Accuracy}(t)=\mathbb{E}\{\acc(\predSeq)|C(\predSeq)>t\}$ and $\text{Coverage}(t) = \mathbb{P}\{C(\predSeq)>t\}$.
    A refinement of AUROC designed for abstention-based settings, it evaluates the accuracy averaged across different coverage level (i.e. proportion of accepted predictions) when rejecting low-confidence predictions.
    
    \item Expected Calibration Error (ECE)~\cite{ICML2017_Guo}: 
    % A well-calibrated model ensures that a confidence score of 90\% corresponds to an approximately 90\% correctness rate.
    
    \begin{equation}
        \mathbb{E}\Big[|\mathbb{E}[\acc(\predSeq)|C(\predSeq)] - C(\predSeq)|\Big].
        %\sum_{m=1}^{M} \frac{|B_m|}{N} \left| \text{acc}(B_m) - \text{conf}(B_m) \right|
    \end{equation}
    % Used for calibration, 
    ECE quantifies the alignment between predicted confidence scores and actual correctness probabilities. 
    

    \item Rank-Calibratoin Error (RCE)~\cite{RCEhuang2024}: 
    {\small
    \begin{equation}
        \mathbb{E}_C\Big[|\mathbb{P}_{C'}\{reg(C')\geq reg(C)\} - \mathbb{P}_{C'}\{C'\leq C\}|\Big]
        %\sum_{m=1}^{M} \frac{|B_m|}{N} \left| \text{acc}(B_m) - \text{conf}(B_m) \right|,
    \end{equation}
    }
    where $reg(c)$ is a regression function for $\mathbb{E}[\acc|C=c]$ and $C'$ and $C$ are the confidence values of two independent responses.
    Unlike ECE, which cannot be directly applied to confidence measures that have not been calibrated in the frequency space, RCE directly assesses calibration in the ranking space, and is more generally applicable.
    
    %\item Rank-Calibration Error(RCE): It measures how well a confidence or uncertainty score preserves the ranking of correctness.
\end{itemize}


While these evaluation metrics are widely used in classification tasks, they all rely on a \textbf{correctness function} $\acc(\predSeq)$ to decide if a generation $\predSeq$ is correct. 
However, in NLG, correctness is inherently difficult to determine, unless $\predSeq$ exactly matches one of the reference answers, which is rare except for simple tasks.
%which becomes increasingly rare as tasks grow in complexity and output length.
Currently, correctness is often assessed using human evaluation or similarity-based methods:
% \begin{itemize}[nosep]
\paragraph{Human Evaluation} This remains arguably the most reliable approach. 
    Human evaluation is either used on smaller datasets~\cite{MCConf-pmlr-v239-ren23a} or to validate automated correctness functions~\cite{kuhn2023semantic,lin2024generating,CSL}, but is expensive and unscalable for large-scale dataset evaluation.
    
\paragraph{Similarity-Based Methods} In practice, correctness is often approximated by computing the similarity between $\predSeq$ and the reference answers $A$, in the form of $sim(\predSeq,A)$ or $sim(\predSeq,A|\xInput)$. 
    To accommodate metrics like AUROC, a threshold $\tau$ is applied to convert such similarity to $\{0,1\}$:
     \begin{equation}
        f(\predSeq, \xInput) =
        \begin{cases} 
          1, & \text{if } sim(\predSeq, A) > \tau \\ 
          0, & \text{otherwise}.
        \end{cases}\label{eq:correctness}
    \end{equation}
    Specifically, there are two common approaches for computing similarity.
    \textbf{Reference Matching} relies on lexical-based similarity metrics such as ROUGE and BLEU~\cite{hu2024unveiling,aynetdinov2024semscore,kuhn2023semantic}, which often fail to recognize semantically equivalent answers which are phrased differently.
    \textbf{LLM Judgment} uses a LLM as an evaluator~\cite{MCConf-pmlr-v239-ren23a,li2024generation,tan2024judgebench} and is more flexible. 
    However, such methods are computationally expensive and are still not fully reliable. 
    Recent studies indicate that machine-based correctness evaluation sometimes only has an accuracy of ~85\% on popular datasets~\cite{kuhn2023semantic,CSL}.
    %may exhibit systematic biases (e.g., favoring outputs from certain LMs),
% \end{itemize}

% Popular correctness functions include variants of \textbf{reference matching} techniques, including lexical-based similarities such as Rouge or BLEU scores~\cite{rouge,bleu,kuhn2023semantic,123} and outputs of judge LLMs~\cite{lin2024generating,MCConf-pmlr-v239-ren23a,123}
% The former has been demonstrated to \fontred{mis-judge correct answers phrased differently}.
% The latter, while more flexible and has the potential to understand more nuanced answers, is time-consuming, may have systematic bias (favoring generations from certain LMs), and is still not reliable.
% In fact, recent human evaluation suggests that machine-based correctness function is only around \todo{90\%} correct, depending on the technique and datasets.
% Alternatively, some rely on pure \textbf{human evaluation}~\cite{MCConf-pmlr-v239-ren23a}, which is arguably more reliable yet expensive and clearly un-scalable. 


% An efficient and accurate framework to evaluate various confidence measures is direly needed. 

% While this is %RCEhuang2024 crrectness function 

\subsection{Limitations of Existing Methods}
\label{subsecton:limitations}

% Flaws in the correctness function inevitably affect the downstream evaluation metrics such as AUROCs, undermining conclusions such as which confidence measure works better, especially when the performance of different measures are close. 
% \todo{We shown in \cref{fig:1} that the judge/threshold we set could greatly affect the ranking of different confidence measures.}

Flaws in the correctness function inevitably affect downstream evaluation metrics such as AUROC and thus our conclusions about different confidence measures. %, potentially leading to misleading conclusions about which confidence measure performs better, especially when different measures yield similar performance.
In this section, we illustrate the limitations of current confidence evaluation methods from two angles: the impact of threshold sensitivity and the inherent noise of similarity measures.

\paragraph{Case Study 1: Threshold Sensitivity}
%A common issue is that correctness judgments using LLMs often require a predefined threshold to map similarity scores into the binary function~\ref{eq:correctness}. This threshold is manually set and can significantly impact evaluation outcomes.

A common limitation of current practices is the need for a predefined threshold $\tau$ to convert similarity scores into binary correctness labels, as described in \cref{eq:correctness}. 
The choice of $\tau$ could thus impact the final evaluation metric.
% The choice of $\tau$ significantly influences the correctness labels and can subsequently affect the ranking of confidence measures.
To illustrate this, we vary the threshold for CoQA~\cite{CoQA} results from \citet{lin2024generating}, while keeping all other settings constant.
In their work, the threshold was manually set to $\tau=0.7$. 
% To examine the sensitivity of $\tau$, we vary $\tau$ while keeping all other settings constant.
However, \cref{fig:threshold} suggests that $\baselineEcc(C)$, for example, could either rank at the top or the bottom depending on $\tau$.
%hat our threshold choice could significantly impact the ranking of various confidence measures.

\begin{figure}[t]
  \includegraphics[width=1\columnwidth]{figures/ranking_all.png}
  \caption{The AUROC ranking of black-box confidence measures (on LLaMA2-13B and CoQA) is sensitive to the threshold \( \tau \).
  }
  \label{fig:threshold}
\end{figure}

\paragraph{Case Study 2: Similarity Noise} 
Correctness labels, whether derived from human evaluation, LLM-based scoring, or reference matching, are inherently noisy. 
For instance, within LLM-based judgments, correctness labels can fluctuate due to factors such as prompt variations and how the LLM judges were designed and trained. 
Echoing prior observations, \cref{fig:inconsis} shows examples where LLM judgments could either differ between different LLM judges or between different calls to the same LLM judge.
\citet{CSL} proposes to set the correctness function $\acc$ as the consensus of multiple LLMs, which improves the reliability of the correctness of responses LLMs agree on.
However, simply ignoring the disagreement could also introduce systematic selection bias.
% We consider two observable phenomena: \textbf{querying GPT-4o-mini multiple time} with the same prompt can yield different similarity scores between the same response $\predSeq$ and the reference answer $a_i$. \textcolor{red}{add an example}.

% Similarly, \textbf{different LLMs}, such as LLMs from the GPT family and the  LLaMA family, could result in different similarity scores. 
% (Such disagreement was, for example, in~\cite{CSL}, which proposed to take the consensus of multiple judge LLMs to improve the correctness function.)
% \textcolor{red}{add an example}.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figures/inconsis-examples.pdf}
  \caption{
  Using LLM judges as $\acc$, while flexible, still has inherent noise.
  Different LLMs disagree on whether a response is correct (left).
  Even the same LLM (GPT, right) could deliver different opinions simply due to the randomness in generation.
  }
  \label{fig:inconsis}
\end{figure}

To systematically analyze this effect, we simulate correctness label noise with Gaussian noise and analyze its effects.
We modify the correctness function as:
\begin{equation}
\small
    \tilde{\acc}(\predSeq; \xInput) = Sigmoid(logit(\acc(\predSeq; \xInput)) + \epsilon), \quad \epsilon \sim \mathcal{N}(0, \sigma^2).\label{eq:gaussian_noise}
\end{equation}
% where $\epsilon$ represents noise drawn from a normal distribution with variance $\sigma^2$.
As shown in~\cref{tab:noise_rank}, increasing noise levels can lead to significant instability in ranking different confidence measures.
Note that our simulation likely \textit{underestimates} the issue, because the noise in \cref{eq:gaussian_noise} is unbiased and does not reflect systematic bias that may favor certain confidence measures~\cite{lin2022truthfulqa}. 


% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{figures/noise_distribution.png}
%   \caption{Noise Distribution. }
%   \label{fig:noise_distribuion}
% \end{figure}


\begin{table}[t]
\centering
\small
\resizebox{\columnwidth}{!}{ 
\begin{tabular}{@{}c|cccccc@{}}
\toprule
Ranking & 1 & 2 & 3 & 4 & 5 & 6  \\
\midrule
Original & Deg(C) & Deg(E) & Ecc(E) & Deg(J) & Ecc(C) & Ecc(J) \\
Noisy & Deg(C) & Deg(E) & Deg(J) & Ecc(E) & Ecc(J) & Ecc(C) \\
\bottomrule
\end{tabular}}
\vspace{-2mm}
\caption{Ranking of uncertainty quantification methods before and after noise.}
\label{tab:noise_rank}
\end{table}

While it might be theoretically possible to estimate the noise level and its propagation to errors on metrics like AUROC, this requires strong assumptions (e.g. \cref{eq:gaussian_noise}), extensive human evaluation, and replication across LLM judges and datasets. 
This fragility in existing evaluation methods motivates our framework, which eliminates dependence on uncertain correctness functions.


%While it might be theoretically possible to estimate the noise level and propagate it to a final estimation error on metrics like AUROC, this would require additional assumptions (e.g. \cref{eq:gaussian_noise}) and intensive human evaluation, and need to be repeated for each LLM judge and dataset.
%These findings highlight the fragility of existing evaluation methods and motivate the need for evaluation frameworks that do not rely on a potentially trembling correctness function, as explored in our proposed approach.


% Denoting the model as $\mathcal{M}$, for any given input prompt denoted as $x$, the response $\predSeq$ is a sequence of tokens $[s_1,\ldots,s_n]$ sampled from the predictive distribution $P(S;x,\mathcal{M})$.
% A confidence measure 
% We will denote $\predSeq_{<i}$ as the truncated sequence $[s_1,\ldots,s_{i-1}]$.
% Given the auto-regressive assumption, the (log-softmax'd) logit for the $i$-th token represent $\mathcal{M}$'s prediction of the log probability of token $s_i$ at this location.