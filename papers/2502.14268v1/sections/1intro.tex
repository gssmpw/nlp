
\section{Introduction}\label{sec:intro}

%\todo{LLM is growing}


% Uncertainty quantification(UQ) helps assess model confidence in its predictions which is essential for trustworthiness, reliability, and downstream decision-making. Currently, there are two main types of UQ methods for LLMs. One approach is based on model consistency, where the LLM generates multiple responses for the same question to quantity uncertainty~\cite{}. The other approach use internal states of the LLM, such as distribution of logits or self-evaluation mechanisms, to estimate uncertainty. Furthermore, we can drive a confidence score from the uncertainty score, where intuitively higher uncertainty corresponds to lower confidence in the model's responses.\textcolor{red}{change to confidence important}

% %\todo{confidence estimation is important}

% Despite recent progress in LLM uncertainty quantification, existing uncertainty evaluation methods are typically adapted from classical classification tasks. Metrics such as AUROC or accuracy-rejection curves commonly used in selective classification, are often applied to measure confidence in LLM outputs~\cite{}.

% %that are widely used in selective classification literature.
% However, such evaluation frameworks are problematic due to the generative nature of LLMs. They typically rely on reference answers from datasets and some scoring mechanism to determine whether a model-generated response is ``correct'' with respect to the reference answer~\cite{RCEhuang2024}. This approach is not only time-consuming but also unreliable, as correctness labels can be subjective or potentially inaccurate~\cite{kuhn2023semantic,lin2023generating}. \textcolor{red}{add an example}
% %\cite{1} uses \cite{2} uses
%In particular, it typically relies on a reference answer from the dataset being used and some mechanism to determine whether a particular generation by the LM is ``correct'' with respect to the reference answer \cite{RCEhuang2024}. 
%This is not only time-consuming but also unreliable because such correctness labels are themselves imperfect~\cite{kuhn2023semantic,lin2023generating}.  

Large Language Models (LLMs) have shown significant capabilities in various natural language processing(NLP) tasks, including text generation~\cite{mo2024large}, question answering~\cite{zhuang2023toolqa}, and reasoning~\cite{sharan2023llm}. Despite their success, LLMs often produce incorrect, or ambiguous outputs due to their complex architectures and lack of interpretability. As these models become more integral to real-world applications, particularly in high-stakes domains such as healthcare and decision-making~\cite{yao2024comal}, ensuring reliable predictions becomes crucial.


Confidence estimation is a critical component in determining the reliability of LLM outputs. It has been extensively studied in selective classification~\cite{geifman2017selective}, where models reject uncertain predictions to maintain higher accuracy on the accepted subset. In LLMs, confidence estimation serves a similar purpose by assigning confidence scores to generated outputs. Confidence scores serve as a valuable reference to help assess the reliability of model-generated outputs.

In natural language generation (NLG), confidence estimation is closely related to uncertainty quantification (UQ), as both aim to assess the reliability of model outputs.~\cite{}. \dlc{do we actually need to distinguish the uncertainty and confidence?} Confidence scores can be derived from uncertainty measures, with higher uncertainty typically corresponding to lower confidence. Current approaches for confidence estimation in LLMs can be broadly categorized into two types: consistency-based black-box methods and internal-states-based white-box methods. Consistency-based methods involve generating multiple responses for the same input and using the degree of agreement among responses as a proxy for confidence~\cite{}. Internal-states-based methods leverage model-specific information, such as probability distributions over outputs or self-evaluation mechanisms, with one common technique being the use of the probability assigned to a generated response as its confidence score~\cite{}.

% Despite progress in confidence estimation, most evaluation methods borrow techniques from classical classification tasks, such as AUROC and accuracy-rejection curves, which are widely used in selective classification~\cite{}.However, such evaluation frameworks are problematic due to the generative nature of LLMs. They typically rely on reference answers from datasets and some scoring mechanism to determine whether a model-generated response is ``correct'' with respect to the reference answer~\cite{RCEhuang2024}.

While there has been progress in confidence estimation for LLMs, existing evaluation methods primarily borrow metrics from classical classification tasks, such as AUROC and accuracy-rejection curves. These metrics depend heavily on accurate correctness labels for evaluation. However, obtaining correctness labels for open-form responses in NLG poses several challenges. Human evaluation, though reliable, is time-intensive and impractical for large-scale datasets. Automated reference-based metrics like BLEU~\cite{} and ROUGE~\cite{} often fail to capture semantic correctness when responses are phrased differently. LLM-based evaluators, while more flexible, suffer from systematic biases, including favoring outputs from similar models and inconsistencies due to prompt variations. These limitations highlight the need for alternative evaluation frameworks that reduce reliance on explicit correctness labels. We discuss these limitations in detail in Section~\ref{subsecton:limitations}, providing specific case studies.

In this paper, we introduce a novel evaluation framework (\textcolor{red}{Name: ConfQA-Benchmark, ConfEval-QA, QAEval-Conf, MCQA-Eval}) for various confidence measures in NLG. Our framework provides an efficient and robust alternative to existing evaluation methods by eliminating the dependence on correctness labels assigned to LLM responses. Instead, it leverages the inherent structure of multiple-choice QA datasets to evaluate both internal-states-based white-box methods and consistency-based black-box methods. \hua{did not say how it leverages}

% In this paper, we introduce a new evaluation framework for various confidence measures in NLG, which leverages the abundant multiple-choice datasets online. 
% Our proposal requires only a fraction of the time compared with existing valuation methods, and it does not depend on any correctness labels on the LM generations.
% This means that our evaluation framework is as reliable as the underlying multiple-choice datasets. 
Our contributions are summarized as follows:
\begin{enumerate}
    \item We demonstrate that commonly used evaluation methods for NLG confidence measures are sensitive to noise in correctness labels, which can lead to misleading conclusions about evaluation metrics and rankings of different confidence estimation approaches.
    %We show that the most popular evaluation methods for NLG confidence measures are prone to noise in the ``correctness label`, which could further lead to problematic conclusions. %(about which measure is better)
    \item We propose a simple yet effective method that utilizes multiple-choice QA datasets to evaluate confidence measures, supporting both internal-states-based white-box and consistency-based black-box methods.
    %We propose a simple yet effective method to leverage Question-Answering datasets in multiple-choice format and provide examples for both white-box and sampling-based black-box confidence measures.
    \item We conduct extensive experiments on multiple recent LLMs and widely used QA datasets to demonstrate the effectiveness of our evaluation framework. \todo{Some comments for experiments}
\end{enumerate}

