\section{ \uqeval: A framework for Assessing Confidence Estimation}\label{sec:method}
At a high level, existing evaluation frameworks
% the evaluation pipeline described in \cref{sec:prelim:old_eval} 
for $C_{\mathcal{M}}$ includes three main steps (blue path in \cref{fig:pipeline}):
\begin{enumerate}[nosep]
    \item Generate $\predSeq$ from $\mathcal{M}$ given the input $\xInput_i$.
    \item Determine the correctness label of $\predSeq$ using the function $\acc(\cdot,\xInput)$.
    \item Compute evaluation metrics such as AUROC. A higher metric value indicates that $C_{\mathcal{M}}$ is a ``better'' confidence estimation.
\end{enumerate}
The main limitation of this general pipeline lies in $\acc$ in step 2. 
Existing evaluation frameworks all implicitly assume step 1---that the confidence measure $C_{\mathcal{M}}$ must apply to generated sequences $\predSeq$. 
While this might hold for consistency-based uncertainty measures, where response divergence indicates uncertainty, it does not extend to confidence measures. 
In other words, we could relax step 1 in order to improve step 2.
% Consider, for instance, Eccentricity~\cite{lin2024generating}: The uncertainty measure $U_{ecc}$ computes the average distance between sampled generation embeddings and their centroid, while the confidence measure evaluates a specific generation. 
% Consequently, we could simply use the sampled generations to construct the embedding space, yet we can still measure the distance of \textit{any} sequence to the center of embeddings.


%The previous discussion suggests that existing evaluation frameworks all bear an implicit assumption: The confidence measure $C_{\mathcal{M}}$ must be applied to the generations $\predSeq$.
%While this might be true for the case of most existing consistency-based uncertainty measures, where a high degree of divergence of the sampled responses is a strong indicator of high uncertainty, this is \textit{not} the case for confidence measures.
%Take Eccentricity~\cite{lin2024generating} as an example: The uncertainty measure $U_{ecc}$ is based on the average distance of the embeddings of the sampled generations to the center of all embeddings and the confidence measure is that of a particular generation. 
%Consequently, we could simply use the sampled generations to construct the embedding space, yet we can still measure the distance of \textit{any} sequence to the center of embeddings. 

\textit{Our main proposal in this paper is to adapt multiple-choice datasets to evaluate confidence measures designed for free-form NLG.}
Unlike free-form NLG datasets, multiple-choice datasets provide inherent correctness values for options, eliminating the need for an explicit correctness function. 
If we simply ``pretend'' that these options are free-form generations from the base LM, we can directly evaluate the confidence measure quality. 
As \cref{fig:pipeline} shows, the approach differs from existing evaluation pipelines only in applying confidence estimation methods to multiple-choice options.




Consider the QASC~\cite{khot2020qasc} dataset as an example,
each problem comes with a question $\xInput$ and a few choices, $o_1,\ldots,o_K$. 
Unlike what such datasets were designed for, we re-format the input prompt as a free-form NLG question, as illustrated in \cref{fig:qasc_example}, as if the base LLM generated each option itself, in different runs.
In what follows, we first explain explain slight nuances in applying internal state-based white-box confidence measures as well as consistency-based black-box ones. 
%shows a reformatted question from the QASC dataset.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figures/qasc_example.pdf}
  % \caption{A reformatted question example from the QASC dataset. The Question and Choices are directly from the original dataset, while our prompt is specifically designed for LLM input to generate open-form responses.}
  \caption{
  We reformat each option from the multiple-choice question (left), by injecting the \smash{\colorbox{yellow!40}{{{\color{blue}option}}}} to a free-form QA \smash{\colorbox{green!40}{prompt}}.
  One could typically apply any confidence estimation method by treating this \smash{\colorbox{yellow!40}{{{\color{blue}option}}}} as if it was generated by the base LM.
  For black-box confidence measures that require additional responses, we only feed the \smash{\colorbox{green!40}{prompt}} to the base LM.
  }
  \label{fig:qasc_example}
\vspace{-3mm}
\end{figure}



\textbf{Logit or Internal State-Based Measures} typically examine the internals of a LM when it generates a particular response.
The nature of the free-form generation task allows us to simply plug-in the option $o_i$ into the corresponding location of the prompt, and extract similar information that allows us to evaluate the confidence\footnote{In fact, this was the practice to compute \baselineSL for actual generations. For example, \url{https://github.com/lorenzkuhn/semantic_uncertainty/blob/main/code/get_likelihoods.py} and \url{https://huggingface.co/docs/transformers/perplexity}.}.
% One concern is whether these options are too ``different'' from what the LM would otherwise generate itself.
% As exemplified in \cref{fig:true_distribution}, $C(o_i)$ in general shares a similar distribution to $C(\predSeq_i)$. 

% Taking CommonsenseQA~\footnote{A multiple-choice dataset for our experiment is described in Section~\ref{sec:experiments}} as an example, we compare the logit distribution of the correct answer choices with the distributions of other LLM-generated responses. 
% As shown in \cref{fig:true_distribution}, the distributions exhibit notable similarities, indicating that logit-based confidence estimation can capture underlying patterns shared between correct answer choices and free-form generations.

% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{figures/true_distribution_new.png}
%   \caption{Confidence score distribution for the \baselinePTrue method on the CQA dataset. The blue distribution represents 20 open-form responses, while the red distribution corresponds to the correct option. }
%   \label{fig:true_distribution}
% \end{figure}


\textbf{Consistency-based Confidence Measures}
Unlike logit-based or internal-state-based measures, consistency-based confidence measures typically rely on an estimate of the predictive distribution, denoted as $\PredDist$, and any response that is closer to the center of the distribution (in the ``semantic space'') is considered to be of higher confidence. 
Consider methods from~\citet{lin2024generating} as an example. To preserve the integrity of the predictive distribution, we first sample $n$ responses from $\PredDist$ as usual, and then iteratively include one option $o_i$ at a time to compute its associated confidence score~\cite{rivera-etal-2024-combining,manakul-etal-2023-selfcheckgpt}. 
\cref{alg:confidence_score} outlines this process. 


\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{algorithm}[t]
\small
% \caption{Confidence Score Computation in the Black-Box Method}
\caption{Consistency-based Confidence Estimation for Any Sequences}
\label{alg:confidence_score}
\begin{algorithmic}[1]
    \Require $\xInput$, $\mathcal{M}$, candidate sequences $A = \{a_1, \dots, a_K\}$
    \Ensure $\{C_{\mathcal{M}}(\xInput,a_1), \dots,C_{\mathcal{M}}(\xInput,a_K)\}$ 
    
    \State Generate $S = \{\predSeq_1, \dots, \predSeq_{n}\}$ using $\mathcal{M}$ for question $\xInput$
    \State Compute pairwise similarity matrix $M$ of $S$.% \in \mathbb{R}^{|S'| \times |S'|}$
    % \State Construct full response set $S' = S \cup A$, where $|S'| = n+K$
    % \State Compute pairwise similarity matrix $M_{sim} \in \mathbb{R}^{|S'| \times |S'|}$
    
    \For{each $a_i \in A$}
        \State Compute a new similarity matrix $M_i$ of $S\cup\{a_i\}$, reusing $M$. % \in 
        % \State Form subset $S_i = S \cup \{a_i\}$, where $|S_i| = n+1$
        % \State Extract pairwise similarity matrix $M_{sim}^{(i)} \in \mathbb{R}^{|S_i| \times |S_i|}$
        \State Compute confidence score $C_{\mathcal{M}}(\xInput,a_i)$ using $M_i$. %degree matrix or eccentricity of $M_{sim}^{(i)}$
    \EndFor

    \State \Return $\{C_{\mathcal{M}}(\xInput,a_1), \dots,C_{\mathcal{M}}(\xInput,a_K)\}$ 
\end{algorithmic}
\end{algorithm}

% Since computing the similarity matrix is the most computationally expensive step, the subsequent 5 confidence score calculations reuse precomputed similarity values. As a result, the additional computations take less than 1 minute in total, ensuring efficiency.

% Consistency-based confidence measures are a little different from .


\paragraph{Remarks}
Our proposal relaxes step 1 at the beginning of this section, allowing for $\predSeq^*=o_i$ not sampled from $\PredDist$.
This is not to be misunderstood as a proposal to \textit{replace} the current pipeline (\cref{sec:prelim:old_eval})---rather, it is \textit{complementary}.
The rationale is that if a good confidence measure predicts the correctness well, it should perform well in \textit{both} evaluation frameworks.
In fact, any $o_i\in\Sigma^*$ that does not violate the generation configuration, has a non-zero probability to be sampled from $\PredDist$, and a robust confidence measure should be expected to model it well.
% \fontred{
% In fact, any $o_i$, as long as it does not violate the generation config, could be sampled from $\PredDist$ given enough time.
% % As we will see in \cref{sec:exp}, even though $o_i$ are not sampled from $\PredDist$, we do not observe a big distribution shift in terms of the confidence values as well.
% }
% Note that we do not advise \textit{replacing} the existing valuation 

% It is important to note that our method only obtains correctness labels for $o_i$. Consequently, when computing AUROC, AUARC, and other evaluation metrics, we only consider confidence values associated with these options.\textcolor{red}{already mention it in \cref{sec:metrics} }
