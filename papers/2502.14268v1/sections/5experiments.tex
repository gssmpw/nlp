\section{Experiments}\label{sec:experiments}

% We show the advantages of our proposed evaluation framework with a comprehensive experiments on multiple LLMs and confidence estimation methods. 
We demonstrate the advantages of our proposed evaluation framework through comprehensive experiments on multiple LLMs and various confidence estimation methods.
% This section reports a more comprehensive set of experiments using our pipeline to evaluate confidence estimation methods, demonstrating its advantages.

\subsection{Experimental Setup}
% We discuss our experimental setup, with additional details in \cref{appendix:sec:exp_imp}.


\paragraph{Base LLMs}
Our experiments use four popular open-source LLMs: \LMLLaMATwoName ~\cite{touvron2023llama2}, \LMLLaMAThreeName, \phiName~\cite{abdin2024phi}, and \qwenName~\cite{yang2024qwen2}. 
These models were specifically pretrained on question-answering tasks, which minimizes irrelevant responses.
We include various model sizes for a comprehensive analysis.
% to enable analysis of confidence estimation across scales. 

\paragraph{Datasets}

We select five multiple-choice datasets with varying levels of complexity from different domains, including CommonSenseQA(C-QA)~\cite{talmor-etal-2019-commonsenseqa}, Question Answering via Sentence Composition (QASC)~\cite{khot2020qasc}, MedQA~\cite{jin2021disease}, RACE-m, and RACE-h~\cite{lai2017race}. 
Each dataset consists of independent questions with a set of answer options, where exactly one option is correct. 
Evaluating different LLMs on datasets from different domains and diverse levels of difficulty allows for a more comprehensive assessment of model performance across a wide range of scenarios. %This diversity helps capture variations in uncertainty estimation across different domains, providing a clearer comparison of the strengths and weaknesses of different confidence estimation methods.
\cref{tab:datasets} provides an overview of these datasets and the number of questions we use, with detailed descriptions in \cref{sec:datasetDes}.
% For generating open-form responses, we followed the setup in~\cite{lin2024generating}, where only the question text was provided to the LLMs without multiple-choice options. 
The exact prompt formulation for each dataset is provided in \cref{sec:appendix_prompt}. 

\begin{table}[t]
\centering
\scriptsize
\begin{tabular}{@{}lcccl@{}}
\toprule
\textbf{Dataset} & \textbf{Size} & \textbf{Options} & \textbf{Domain} & \textbf{Difficulty} \\ 
\midrule
C-QA & 1221 & 5 & Commonsense & Easy \\ 
QASC & 926 & 8 & Commonsense & Medium \\ 
MedQA & 1000 & 5 & Medical & Hard \\ 
RACE-M & 1000 & 4 & Reading Comprehension & Medium \\ 
RACE-H & 1000 & 4 & Reading Comprehension & Hard \\ 
\bottomrule
\end{tabular}
\vspace{-1mm}
\caption{Overview of datasets used in this paper.}
\vspace{-5mm}
\label{tab:datasets}
\end{table}





% \begin{itemize}
%     \item \textbf{C-QA} A multiple-choice dataset designed for commonsense question answering. Each question requires world knowledge and reasoning to determine the correct answer from 5 given choices. The dataset consists of 1,221 test questions.
%     \item \textbf{QASC} A multiple-choice commonsense reasoning dataset with 8 answer choices per question. Compared to C-QA, QASC presents a higher level of difficulty. While the dataset is originally designed for multi-hop reasoning, our focus is not on evaluating the reasoning capabilities of LLMs. Therefore, we do not provide the supporting facts to the model and instead present only the question. For our experiments, we use the original validation set, which includes 926 questions.
%     \item \textbf{MedQA} A multiple-choice dataset with 5 answer options, specifically designed for medical question answering. The questions are sourced from professional medical board exams, making this dataset particularly challenging due to its reliance on specialized medical knowledge. For our experiments, we selected the first 1,000 questions from original dataset.
%     \item \textbf{RACE-m and RACE-h} Two subsets of the RACE dataset, derived from English reading comprehension exams for high school and middle school students, respectively. These datasets evaluate a modelâ€™s ability to comprehend complex passages and answer questions based on contextual reasoning. Each question is accompanied by four answer choices, with only one correct option. For our experiments, we randomly sampled 1,000 questions from the entire dataset using a fixed random seed of 42 to ensure reproducibility.
% \end{itemize}

\paragraph{Confidence Estimation Methods}

% In this section, we introduce the baseline methods used for confidence estimation. These methods can be broadly categorized into consistency-based methods and logit-based methods.


% We introduce 6 specific black-box methods: Ecc(C), Deg(C), Ecc(E), Deg(E), Ecc(J), and Deg(J). They analyze the variability in LLMs responses to estimate uncertainty by constructing a similarity matrix and computing confidence scores based on degree and eccentricity measures~\cite{lin2024generating}. The general process consists of the following steps:

% \begin{enumerate}
%     \item Generating Multiple Responses: Given a single question, the LLMs generates $n$ different responses to the same prompt.
%     \item Measuring Responses Similarity: The generated responses are compared pairwise to quantify their similarity. This can be done using different similarity metrics, including:
%     \begin{itemize}
%         \item Jaccard Similarity(J): 
%         \item Natural Language Inference(NLI)-based Similarity(E/C):
%     \end{itemize}
%     \item Constructing the Similarity Matrix: 
%     \item Calculating Confidence Scores: The similarity matrix is used to drive confidence scores based on different metrics, including:
%     \begin{itemize}
%         \item Degree Matrix(Deg):
%         \item Eccentricity(Ecc):
%     \end{itemize}
% \end{enumerate}

% In our experiments, we compare 6 black-box methods and 6 white-box methods, conducting extensive evaluations across different settings. 

% The \textbf{black-box methods} consist of the following steps \hua{this is duplicated wth previous writing}:
% \begin{itemize}
%     \item Generating Multiple Responses: Given a single question $\xInput$, the LLM $\mathcal{M}$ generates $n=20$ open-form responses. Additionally, we incorporate the $K$ multiple-choice options provided in the dataset, resulting in a total of $n+K$ responses.
%     \item Constructing the Similarity Matrix: The $n+k$ responses are compared pairwise to measure their similarity. We use Jaccard Similarity (J) and Natural Language Inference (NLI)-based entailment (E) and contradiction(C) relationships to construct the similarity matrix.
%     \item Calculating Confidence Scores: The similarity matrix is used to derive confidence scores based on different metrics, specifically Degree Matrix (Deg) and Eccentricity (Ecc). The detailed procedure for computing these scores can be found in Algorithm~\ref{alg:confidence_score}.
% \end{itemize}
We compare six black-box and six white-box methods.
The selected methods represent commonly used confidence estimation baselines.
The six \textbf{black-box} measures evaluated in our experiments are:
\begin{itemize}[leftmargin=*, nosep]
    \item \baselineDegree(J), \baselineDegree(E), \baselineDegree(C): These compute the similarity matrix using Jaccard Similarity, NLI entailment and NLI contradiction, respectively. 
    The confidence score is then derived from the degree matrix.
    
    \item \baselineEcc(J), \baselineEcc(E), \baselineEcc(C): 
      The similarity matrix is obtained using the same method as above, but the confidence score is derived from the embeddings derived from the graph Laplacian.
\end{itemize}

Unlike black-box methods, \textbf{white-box} measures directly use the multiple-choice options as evaluation responses. 
We implement six white-box confidence estimation baselines, as introduced in Section~\ref{confidence_methods}: \baselineSL, \baselineNLLNorm, \baselineSAR, \baselineCSL, its variant \baselineCSLNext, and \baselinePTrue.

\paragraph{Metrics}
\label{sec:metrics}
% We evaluate whether confidence scores of multiple-choice options reflect their correctness. 
Following previous works,
% ~\citet{lin2024generating, da2024llm, cao2024survey},
we use AUROC as our primary metric\footnote{Responses sampled for black-box methods' are excluded from AUROC calculations due to uncertain correctness.}. Our framework can also be applied to evaluate confidence calibration. We include additional results in \cref{sec:full_results}, reporting RCE and calibration ECE metrics. 
% This highlights the flexibility of our framework and its potential in other NLG applications that currently require a correctness function.

Additional details of our experiment can be found in \cref{appendix:sec:exp_imp}.

% \subsection{Experiments Results}
% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{figures/cqa_blackbox.png}
%   \caption{Visualizaiton of performance of different LLMs on C-QA Dataset.
%   \label{fig:true_distribution}
% \end{figure}



\subsection{Experimental Findings}
This section summarizes our main experimental findings, with detailed results in \cref{sec:full_results}.
% We conducted a benchmark evaluation of 6 black-box and 6 white-box confidence estimation methods across 5 datasets and 4 LLMs. 
% Detailed results are presented in \cref{sec:full_results}. 


\paragraph{Comparison With Existing Evaluation Methods}
We first compare our evaluation method with the existing pipeline (Baseline) using the QASC dataset. 
For the baseline, we use gpt-4o-mini to obtain correctness labels (by comparing the generation with the correct option).
\cref{tab:uq_ranking} shows that varying the threshold significantly impacts the ranking of both black-box and white-box confidence estimation methods.
Additionally, querying gpt-4o-mini for correctness labels across $926 \times 20$ responses takes approximately 2.5 hours. 
The cost (both economical and time-wise) would be much higher for more advanced LLM judges, or longer prompts from datasets with a ``context'' (such as CoQA~\cite{CoQA}), making large-scale evaluations difficult. 
% For datasets with over 10,000 questions, this process would require at least 24 hours, making large-scale evaluations impractical. 

On the other hand, \uqeval aligns with baseline ranking at $\tau=0.9$.
While it is unclear in this case which $\tau$ reflects the ``most reliable'' ranking, this experiment suggests that \uqeval's conclusion is consistent with existing pipeline.
However, unlike the Baseline, it does not require the costly correctness function(s), thereby reducing computational costs and enabling scalable evaluation.
%, destabilizing the evaluation, and it is unclear which ranking we should trust


% \begin{table}[t]
% \small
% \centering
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{@{}clllllll@{}}
% \toprule
%  &  & \multicolumn{6}{c}{\textbf{Ranking}} \\ 
% \cmidrule(lr){3-8}
%  & $\tau$ & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} \\ 
% \midrule
% \multirow{5}{*}{\textbf{Baseline}} 
%     & 0.9 & Ecc(E) & Deg(E) & Deg(C) & Deg(J) & Ecc(J) & Ecc(C) \\ 
%     & 0.8 & Deg(C) & Deg(E) & Ecc(E) & Ecc(C) & Deg(J) & Ecc(J) \\
%     & 0.7 & Deg(C) & Deg(E) & Ecc(C) & Ecc(E) & Deg(J) & Ecc(J) \\ 
%     & 0.6 & Deg(C) & Ecc(C) & Deg(E) & Ecc(E) & Deg(J) & Ecc(J) \\ 
%     & 0.5 & Deg(C) & Deg(E) & Ecc(E) & Ecc(C) & Deg(J) & Ecc(J) \\ 
% \midrule
% \textbf{\uqeval} & N/A & Ecc(E) & Deg(E) & Deg(J) & Deg(C) & Ecc(J) & Ecc(C) \\ 
% \bottomrule
% \end{tabular}}
% \vspace{-1mm}
% \caption{Comparison with existing methods by varying $\tau$ from 0.9 to 0.5, showing the changes in black-box method rankings and the results obtained by \uqeval.}
% \vspace{-5mm}
% \label{tab:uq_ranking}
% \end{table}

\begin{table}[t]
\small
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}clllllll@{}}
\toprule
 &  & \multicolumn{6}{c}{\textbf{Ranking}} \\ 
\cmidrule(lr){3-8}
 & $\tau$ & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} \\ 
 \midrule\midrule
 &        & \multicolumn{6}{c}{\textbf{Black-box}} \\ 
\cmidrule(lr){3-8}
\multirow{5}{*}{\textbf{Baseline}} 
    
    %& 0.8 & Deg(C) & Deg(E) & Ecc(E) & Ecc(C) & Deg(J) & Ecc(J) \\
    
    %& 0.6 & Deg(C) & Ecc(C) & Deg(E) & Ecc(E) & Deg(J) & Ecc(J) \\ 
    & 0.5 & Deg(C) & Deg(E) & Ecc(E) & Ecc(C) & Deg(J) & Ecc(J) \\ 
    & 0.7 & Deg(C) & Deg(E) & Ecc(C) & Ecc(E) & Deg(J) & Ecc(J) \\ 
    & 0.9 & Ecc(E) & Deg(E) & Deg(C) & Deg(J) & Ecc(J) & Ecc(C) \\ 
\midrule
\textbf{\uqeval} & N/A & Ecc(E) & Deg(E) & Deg(J) & Deg(C) & Ecc(J) & Ecc(C) \\ 
\midrule\midrule
 &        & \multicolumn{6}{c}{\textbf{White-box}} \\ \cmidrule(lr){3-8}
\multirow{5}{*}{\textbf{Baseline}} 
   
    %& 0.8 & TokenSAR & SL & Perplexity & CSL & CSL-Next & P(true) \\
    
    %& 0.6 & TokenSAR & Perplexity & CSL & CSL-Next & SL & P(true) \\ 
    & 0.5 & TokenSAR & Perplexity & SL & CSL & CSL-Next & P(true) \\ 
    & 0.7 & TokenSAR & Perplexity & CSL & CSL-Next & SL & P(true) \\ 
    & 0.9 & SL & TokenSAR & Perplexity & CSL & CSL-Next & P(true) \\ 
\midrule
\textbf{\uqeval} & N/A & SL & TokenSAR & Perplexity & CSL & CSL-Next & P(true) \\ 

 
\bottomrule
\end{tabular}}
\vspace{-1mm}
\caption{
We analyze how existing LLM-based evaluation methods rank black-box and white-box approaches by varying $\tau$ from 0.9 to 0.5.
% Comparison with existing LLM-based evaluation methods by varying $\tau$ from 0.9 to 0.5, we show changes in the ranking of black-box and white-box methods.
\uqeval aligns with the rankings at $\tau=0.9$, yet requires no overhead for the correctness function.}
\vspace{-2mm}
\label{tab:uq_ranking}
\end{table}



\paragraph{Comparison Across LLMs }
We compare confidence measures across LLMs on the same dataset via  \uqeval.
As shown in \cref{fig:llm_perspective} (with additional results available in the \cref{sec:full_results}), larger LLMs tend to achieve better performance across different confidence estimation methods, reflecting their broader pretraining exposure. 
The relatively ranking of various confidence measures stay mostly stable.
Interestingly, unlike some prior results~\cite{lin2024generating,vashurin2024benchmarking}, \baselinePTrue performs very well except for Llama2-7b. 
We hypothesize that this is due to improvement in recent LLMs' abilities in general, which is similar to the conjecture in \cite{vashurin2024benchmarking}.
This hypothesis is partially supported by the fact that \baselinePTrue performs increasingly well as the base LM becomes more sophisticated.

% An interesting observation should be noted regarding \baselinePTrue
% However, P(true) presents a notable exception among white-box methods. 
% As noted in \cite{vashurin2024benchmarking}, P(true) performs poorly on smaller LLMs, sometimes even worse than random guessing. 
% Our results confirm this trend, showing that P(true) significantly improves on larger LLMs compared to smaller ones.


\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{figures/cqa_blackbox.png}
    \caption{AUROC of different black-box methods.}
    \label{fig:cqa_blackbox}
  \end{subfigure}
  \begin{subfigure}[b]{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{figures/cqa_whitebox.png}
    \caption{AUROC of different white-box methods.}
    \label{fig:another_dataset}
  \end{subfigure}

  \caption{(a) and (b) show the performance of 4 different LLMs and 12 different confidence estimation methods on the C-QA dataset. A higher AUROC indicates better performance.}
  \label{fig:llm_perspective}
\end{figure}





%\subsubsection{Comparison on Different Difficulty Levels of Datasets}
\paragraph{Comparison Across Datasets }
We compare confidence measures across datasets of varying difficulty with \uqeval. 
\cref{fig:dataset_perspective} illustrates these results for \phiName. In general, different confidence measures exhibit larger performance gap on simpler datasets such as C-QA, and smaller on more professional datasets like MedQA.
The general poor performance on harder datasets could be attributed to limited capability of the base LM (in generating additional responses for black-box measures, or in supplying the base logits for white-box measures.
For black-box measures, similarity metrics like NLI and Jaccard may also provide limited distinguishing power.
We recommend selecting datasets with difficulty levels that align with the capabilities of the language model, making performance differences more discernible.
%o reduce the uncertainty in the conclusion (due to dataset selection or sample size).
% As a result, any conclusion about the relative ranking is more prone to noise (from the size or distribution of the dataset).
% We report that quantification methods show greater differentiation on simpler questions. 
% This pattern likely emerges because challenging datasets yield less reliable model outputs, making confidence estimation difficult across all methods and narrowing performance gaps. 
% Consequently, distinguishing method robustness becomes more challenging. 

% We recommend using datasets with mild difficulty or investigating the response quality of generated responses before evaluating confidence quantification methods' performance, as these conditions better reveal method capabilities.

%We find a consistency that the performance among different quantification methods is more differentiable in relatively easier level questions, this might because if a challenging dataset is applied, then the quality of the answer generated by the language model might less reliable, thus, evaluate confidence by any of the existing method can be challenging, so the performance gap is likely to be small, given this, trying to tell apart which confidence quantification method is more robust would be harder. We would suggest that, if to understand the quantification methods' performance, please choose a mild level dataset or investigate the quality of generated responses before using such set of result as reference to revel the ability of quantification methods.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figures/phi4_blackbox2.png}
  \caption{The performance of Phi-4 using black-box methods across different datasets.}
  \label{fig:dataset_perspective}
  \vspace{-5mm}
\end{figure}
