\pdfoutput=1
\documentclass[sigconf,]{acmart}
% \usepackage[a-1b]{pdfx}

\copyrightyear{2025}
\acmYear{2025}
\setcopyright{rightsretained}


\makeatother


%
% inline lists.  usage,
%    \begin{inlinelist}
%       \item first item,
%       \item second item, and
%       \item last item.
%    \end{inlinelist}
%
\usepackage{enumitem}
\newlist{inlinelist}{enumerate*}{1}
\setlist*[inlinelist,1]{%
  label=(\roman*),
}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{lipsum,adjustbox}
\usepackage{vcell}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{diagbox}
\usepackage{pifont}
\usepackage{enumitem}
\usepackage{balance}
\usepackage{cleveref}
\usepackage{placeins}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage[normalem]{ulem}
\usetikzlibrary{shapes,arrows,positioning,fit,calc,matrix}
% \tikzexternalize
\usepgfplotslibrary{groupplots}

% \usepgfplotslibrary{external}
% \tikzexternalize %[prefix=/figures/]

\input{XX-notation}

\settopmatter{printacmref=false}



\title{Hypencoder: Hypernetworks for Information Retrieval}

\author{Julian Killingback}
\email{jkillingback@cs.umass.com}
\affiliation{%
  \institution{University of Massachusetts Amherst}
  \city{Amherst}
  \state{MA}
  \country{USA}
}

\author{Hansi Zeng}
\email{hzeng@cs.umass.edu}
\affiliation{%
  \institution{University of Massachusetts Amherst}
  \city{Amherst}
  \state{MA}
  \country{USA}
}

\author{Hamed Zamani}
\email{zamani@cs.umass.edu}
\affiliation{%
  \institution{University of Massachusetts Amherst}
  \city{Amherst}
  \state{MA}
  \country{USA}
}

\pgfplotsset{compat=1.18}
\begin{document}
\newcommand{\name}{Hypencoder}
\newcommand{\mininame}{q-net}
\newcommand{\hyperheadlayer}{hyperhead layer}
\newcommand{\todo}[1]{\textcolor{red!70}{TODO: #1}}
\newcommand{\ItemEncoderName}{$\Psi$}
\newcommand{\ItemRepsName}{$E_d$}
\newcommand{\QueryEncoderName}{$\Phi$}
\newcommand{\HyperHeadName}{hyper-head}

% \fancyhead{}

\begin{abstract}
The vast majority of retrieval models depend on vector inner products to produce a relevance score between a query and a document. This naturally limits the expressiveness of the relevance score that can be employed. We propose a new paradigm, instead of producing a vector to represent the query we produce a small neural network which acts as a learned relevance function. This small neural network takes in a representation of the document, in this paper we use a single vector, and produces a scalar relevance score. To produce the little neural network we use a hypernetwork, a network that produce the weights of other networks, as our query encoder or as we call it a Hypencoder. Experiments on in-domain search tasks show that Hypencoder is able to significantly outperform strong dense retrieval models and has higher metrics then reranking models and models an order of magnitude larger. Hypencoder is also shown to generalize well to out-of-domain search tasks. To assess the extent of Hypencoder's capabilities, we evaluate on a set of hard retrieval tasks including tip-of-the-tongue retrieval and instruction-following retrieval tasks and find that the performance gap widens substantially compared to standard retrieval tasks. Furthermore, to demonstrate the practicality of our method we implement an approximate search algorithm and show that our model is able to search 8.8M documents in under 60ms.
\end{abstract}
\maketitle


\section{Introduction}
Efficient neural retrieval models are based on a bi-encoder (or two tower) architecture, in which queries and documents are represented separately using either high-dimensional sparse \cite{SNRM,SPLADE, SPLADE++} or relatively low-dimensional dense vectors \cite{DPR,ANCE,ColBERT_v1, CL-DRD, CoCondensor}. These models use simple and light-weight similarity functions, e.g., inner product or cosine similarity, to compute the relevance score for a given pair of query and document representations. We demonstrate theoretically that inner product similarity functions fundamentally limit the types of relevance that retrieval models can express. Specifically, we prove that there is always a set of relevant documents which cannot be perfectly retrieved regardless of the query vector and specific encoder model.

Motivated by this theoretical argument, we introduce a new category of retrieval models that can capture complex relationship between query and document representations. Building upon the hypernetwork literature in machine learning \cite{HyperNetworks, HyperShot, ContinualLearningWithHypernetworks}, we propose \name{}--a generic framework that learns a query-dependent multi-layer neural network as a similarity function that is applied to the document representations. In more detail, \name{} applies attention-based hypernetwork layers, called \hyperheadlayer{}s, to the contextualized query embeddings output by a backbone transformer encoder. Each \hyperheadlayer{} produces the weight and bias matrices for a neural network layer in the query-dependent similarity network, called the \mininame{}. The \mininame{} is then applied to each document representation, which results in a scalar relevance score. We demonstrate that the \name{} framework can be optimized end-to-end and can be used for efficient retrieval from a large corpus. Specifically, we propose a graph-based greedy search algorithm that approximates exhaustive retrieval using \name{} while being substantially more efficient.

We conduct extensive experiments on a wide range of datasets to demonstrate the efficacy of \name{}. We demonstrate that our implementation of \name{} for single vector document representations outperforms competitive single vector dense and sparse retrieval models on MS MARCO \cite{MSMARCO} and TREC Deep Learning Track data \cite{TREC_DL_2019, TREC_DL_2020}, in addition to complex retrieval tasks, such as TREC DL-Hard \cite{TREC_DL_HARD}, TREC Tip-of-the-Tongue (TOT) Track \cite{TREC-TOT-2023}, and the instruction following dataset FollowIR \cite{FollowIR}. Across these benchmarks \name{} demonstrates consistent performance gain across experiments. Note that using the proposed approximation approach, retrieval from MS MARCO \cite{MSMARCO} with approximately 8.8 million documents only takes an average of 59.6 milliseconds per query on a single NVIDIA L40S GPU.

A main advantage of hypernetworks in machine learning is their ability to learn generalizable representations. To demonstrate that \name{} also inherits this generalization quality, we evaluate our model under various domain adaptation settings: (1) adaptation to question answering datasets in biomedical and financial domains, and (2) adaptation to other retrieval tasks, including entity and argument retrieval, where \name{} again demonstrates superior performance compared to the baselines.


We believe that these performance gains are just the by-product of our main contributions; \name{} introduces a new way to think about what retrieval and relevance functions can be, it opens a new world of possibilities by bridging the gap between neural networks and retrieval similarity functions. We believe \name{} is especially important at this time given the new demands for longer and more complex queries brought on by the widespread usage of large language models and it is our belief that \name{} represents an important step towards this goal. To help facilitate this goal we will open source all our code for training, retrieval, and evaluation.\footnote{Available at \url{https://github.com/jfkback/hypencoder-paper}}


\definecolor{base_encoder_color}{HTML}{dbf0ea}
\definecolor{base_encoder_outline_color}{HTML}{43aa8b}
\definecolor{pooler_color}{HTML}{d6ecf5}
\definecolor{pooler_outline_color}{HTML}{277da1}
\definecolor{score_color}{HTML}{fdefce}
\definecolor{score_outline_color}{HTML}{f9c74f}
\definecolor{q_net_border_color}{HTML}{abc4ff}
\definecolor{q_net_fill_color}{HTML}{d7e3fc}
\definecolor{q_net_connection_color}{HTML}{a6a6a6}
\definecolor{gray}{HTML}{a6a6a6}
\definecolor{hyper_head_color}{HTML}{fde8ce}
\definecolor{hyper_head_outline_color}{HTML}{f8961e}
\definecolor{color4}{HTML}{c0fdff}
\begin{figure*}[t]
\scalebox{2.0}{
\begin{tikzpicture}
\tikzset{
  pics/encoder/.style  n args={4}{code={
    \node[rectangle, minimum width=9.708mm, minimum height=4mm, fill=base_encoder_color, draw=base_encoder_outline_color, rounded corners=0.5mm] (base_encoder) {\tiny encoder};
    \node[rectangle, minimum width=9.708mm, minimum height=2.4mm, fill=#3, draw=#4, rounded corners=0.5mm, font=\tiny] [above = 0.5mm of base_encoder] (pooler) {#2};
    \node[fill=none, outer sep=2.0] [below =1.5 mm of base_encoder] (query_input) {\small #1};
    \draw[->, line width=0.2mm] (query_input) -- (base_encoder);
    \draw[-, line width=0.2mm] (base_encoder) -- (pooler);
  }},
}

\tikzset{
  pics/crossEncoder/.style={code={
    \node[rectangle, minimum width=9.708mm, minimum height=4mm, fill=base_encoder_color, draw=base_encoder_outline_color, rounded corners=0.5mm] (base_encoder) [right = 3.0mm of query_encoder3] {\tiny encoder};
    \node[rectangle, minimum width=9.708mm, minimum height=2.4mm, fill=pooler_color, draw=pooler_outline_color, rounded corners=0.5mm] [above = 0.5mm of base_encoder] (pooler) {\tiny score-head};
    \node[fill=none, outer sep=2.0, xshift=2.5mm] [below =1.5 mm of base_encoder] (query_input) {\tiny q};
    \node[fill=none, outer sep=2.0, xshift=-2.5mm] [below  =1.5 mm of base_encoder] (passage_input) {\tiny d};
    \draw[->, line width=0.2mm] (query_input.north) -- ++(0pt,1.5mm);
    \draw[->, line width=0.2mm] (passage_input.north) -- ++(0pt,1.5mm);
    \draw[-, line width=0.2mm] (base_encoder) -- (pooler);
  }},
}


\tikzset{
  pics/score/.style={code={
    \node[circle, fill=score_color, draw=score_outline_color, minimum width=2mm, minimum height=2mm]{\tiny s};
  }},
}

\tikzset{
  pics/qnet/.style={code={
      \foreach \N [count=\lay,remember={\N as \Nprev (initially 0);}]
                   in {3, 3, 3, 1}{ % loop over layers
        \foreach \i [evaluate={\y=\N/2-\i; \x=\lay; \prev=int(\lay-1);}]
                     in {1,...,\N}{ % loop over nodes
          \node[minimum size=1.0mm, circle, draw=q_net_border_color, fill=q_net_fill_color, inner sep=0pt] (N\lay-\i) at (\x / 5.5,\y /5.5) {};
          \ifnum\Nprev>0 % connect to previous layer
            \foreach \j in {1,...,\Nprev}{ % loop over nodes in previous layer
              \draw[line width=0.1mm, draw=q_net_connection_color] (N\prev-\j) -- (N\lay-\i);
            }
          \fi
        }
      }
  }},
}

% Dense Encoders
\node[matrix, inner sep=0] (passage_encoder1)  {\pic{encoder={\tiny d}{pooler}{pooler_color}{pooler_outline_color}};\\};
\node[matrix, inner sep=0] (query_encoder1) [right =1.0 mm of passage_encoder1]{\pic{encoder={\tiny q}{pooler}{pooler_color}{pooler_outline_color}};\\};


% Neural Scoring encoders
\node[matrix, inner sep=0] (passage_encoder3) [right =3.0 mm of query_encoder1]{\pic{encoder={\tiny d}{pooler}{pooler_color}{pooler_outline_color}};\\};
\node[matrix, inner sep=0] (query_encoder3) [right =1.0 mm of passage_encoder3] {\pic{encoder={\tiny q}{pooler}{pooler_color}{pooler_outline_color}};\\};

%%%%%%%%%%%%% Cross Encoder
\node[matrix, outer sep=0, inner sep = 0] [right = 3.0mm of query_encoder3] (cross_encoder) {\pic {crossEncoder};\\};
\node[matrix, outer sep=0, inner sep = 0] (score4) [above = 2.0mm of cross_encoder] {\pic {score};\\};
\draw[->, line width=0.2mm, color=gray] (cross_encoder.north) -- (score4.south);
%%%%%%%%%%%%%

% \name encoders
\node[matrix, inner sep=0] (passage_encoder2) [right =3.0 mm of cross_encoder]{\pic{encoder={\tiny d}{pooler}{pooler_color}{pooler_outline_color}};\\};
\node[matrix, inner sep=0] (query_encoder2) [right =1.0 mm of passage_encoder2] {\pic{encoder={\tiny q}{\HyperHeadName{}}{hyper_head_color}{hyper_head_outline_color}};\\};


\node[rectangle, outer sep=0, inner sep=1, draw=gray, rounded corners=0.5mm, minimum size=3mm]
(ip_block) at ($(query_encoder1)!0.5!(passage_encoder1) + (0,1.0)$) {IP};
\node[matrix, outer sep=0, inner sep = 0] (score1) [above = 2.0mm of ip_block] {\pic {score};\\};

\draw[->, line width=0.2mm] (passage_encoder1.north) to[out=90,in=180] node[left, align=center, font=\tiny, rotate=0] {$E_d$} (ip_block.west);
\draw[->, line width=0.2mm] (query_encoder1.north) to[out=90,in=0] node[right, align=center, font=\tiny, rotate=0] {$E_q$} (ip_block.east);
\draw[->, line width=0.2mm, color=gray] (ip_block.north) to (score1.south);

%%%%%%%%%%%%% \name arrows and labels
\node[matrix, outer sep=0, inner sep=1, draw=gray, rounded corners=0.5mm] (q_net) at ($(query_encoder2)!0.5!(passage_encoder2) + (0,1.2)$) {\pic {qnet};\\};
\node[matrix, outer sep=0, inner sep = 0] (score2) [right = 2.0mm of q_net] {\pic {score};\\};

\draw[->, line width=0.2mm] (passage_encoder2.north) to[out=90, in=180] node[left, align=center, font=\tiny, rotate=0] {$E_d$} (q_net.west);
\draw[->, line width=0.2mm, color=gray] (query_encoder2.north) to[out=90, in=270]  (q_net.south);
\draw[->, line width=0.2mm, color=gray] (q_net.east) to (score2.west);

\node[font=\tiny, yshift=2mm, align=center] [left = 0.0mm of q_net] {input};
\node[font=\tiny, yshift=-0.8mm, align=center] [above = 0.0mm of q_net] {q-net};
%%%%%%%%%%%%%

%%%%%%%%%%%%% learned similarity arrows and labels
\node[rectangle, outer sep=0, inner sep=1, draw=gray, rounded corners=0.5mm, minimum height=2mm, font=\tiny]
(combine_block) at ($(query_encoder3)!0.5!(passage_encoder3) + (0,0.8)$) {combine};
\draw[->, line width=0.2mm] (passage_encoder3.north) to[out=90, in=180] node[left, align=center, font=\tiny, rotate=0] {$E_d$} (combine_block.west);
\draw[->, line width=0.2mm,] (query_encoder3.north) to[out=90, in=0] node[right, align=center, font=\tiny, rotate=0] {$E_q$}  (combine_block.east);

% \node[matrix, outer sep=0, inner sep=0, draw=gray, rounded corners=0.5mm, rotate=90] (learned_sim) at ($(query_encoder3)!0.5!(passage_encoder3) + (0.0,1.4)$) {\pic[rotate=90] {qnet};\\};

\node[rectangle, outer sep=0, inner sep=1, draw=gray, rounded corners=0.5mm, minimum height=2mm, font=\tiny]
(learned_sim) at ($(query_encoder3)!0.5!(passage_encoder3) + (0,1.12)$) {MLP};


\draw[->, line width=0.2mm,] (combine_block.north) to[out=90, in=-90] (learned_sim.south);

\node[matrix, outer sep=0, inner sep = 0] (score3) [above = 1.4mm of learned_sim] {\pic {score};\\};
\draw[->, line width=0.2mm, color=gray] (learned_sim.north) -- (score3.south);
%%%%%%%%%%%%%%%%


%%%%%%%%%%%%% Captions
\node[align=center, font=\tiny] at ($(query_encoder1)!0.5!(passage_encoder1) + (0.0,-0.7)$) {Vector Similarity};
\node[align=center, font=\tiny \bf] at ($(query_encoder2)!0.5!(passage_encoder2) + (0.0,-0.7)$) {\name{}};
\node[align=center, font=\tiny] at ($(query_encoder3)!0.5!(passage_encoder3) + (0.0,-0.7)$) {Learned Similarity};
\node[align=center, font=\tiny ] at ($(cross_encoder)!0.5!(cross_encoder) + (0.0,-0.7)$) {Cross-encoding Similarity};
%%%%%%%%%%%%%

\end{tikzpicture}
}
\caption{Overview comparing \name{} to existing retrieval and reranking paradigms. \textcolor{gray}{Gray} arrows indicate the arrow does not represent an entity, it is the same as what it points to, in contrast black arrows do indicate a unique entity which is always labeled.}
\label{fig:main_overview}
\end{figure*}

\section{Related Work}
\paragraph{\textbf{Vector Space Models}}

Vector-based models that use sparse vectors have existed for decades, with each index representing a term in the corpus vocabulary. Document-query similarity is computed using measures like $l_2$ distance, inner product, or cosine similarity, with term weighting methods such as TF-IDF being a substantial focus to improve performance. With the emergence of deep neural networks, focus shifted to learning representations for queries and documents. SNRM by \citet{SNRM} was the first deep learning model to retrieve documents from large corpora by learning latent sparse vectors. Following works leveraged pretrained transformer models like BERT \cite{BERT} using single dense vector representations \cite{DPR}. Recent improvements have focused on training techniques including self-negative mining \cite{ANCE, RocketQA, RANCE, adore}, data augmentation \cite{RocketQA, DRAGON}, distillation \cite{MarginMSE, DRAGON, TCT-ColBERT}, corpus-pretraining \cite{CoCondensor, Contriever, RetroMAE,b-prop}, negative-batch construction \cite{TAS-B} and curriculum learning \cite{CL-DRD,prod}. Alternative approaches include ColBERT \cite{ColBERT_v1}, which uses multiple dense vectors, and SPLADE \cite{SPLADE}, which revisits sparse representations using pretrained masked language models.

Though these methods vary substantially, they all share a fundamental commonality, that relevance is based on an inner product (or in some cases cosine similarity). We believe that this is a significant limitation of these methods and one which hampers the performance of these models on complex retrieval tasks. Our method circumvents this limitation by learning a query-dependent small neural network that is fast enough to run on the entire collection (or used in an approximate way; see Section \ref{sec:EfficientSearch} for details).

\paragraph{\textbf{Learned Relevance Models}}

Light-weight relevance models using neural networks have demonstrated improved retrieval performance compared to simple methods like inner products. Early iterations came in the form of learning-to-rank models \cite{RankNet, LearningToRankNonSmooth} which use query and document features to produce relevance scores for reranking. While these models traditionally used engineered features, more recent approaches adopted richer inputs. For instance, MatchPyramid \cite{MatchPyramid} and KNRM \cite{KNRM} use similarity matrices between non-contextualized word embeddings, while Duet \cite{Duetv1, DuetV2} combines sparse and dense term features in a multi-layer perceptron. DRMM \cite{DRMM} utilized histogram features as input to neural networks for scoring. Since the advent of BERT \cite{BERT}, focus has shifted to making transformer models more efficient, such as PreTTR \cite{PreTTR} which separately precomputes query and document hidden states. Recently, LITE \cite{LITE} extended ColBERT's similarity using column-wise and row-wise linear layers for scoring.

In the recommender systems community, learned similarity measures have been widely used \cite{LearnedCollaborativeFiltering, NeuralFactorizationMachines}. The common usage of neural scoring methods in recommendation has inspired research into efficient retrieval with more learned scoring signals. For instance, BFSG \cite{NeuralNetworkFastItemRanking} supports efficient retrieval with arbitrary relevance functions by using a graph of item representations and a greedy search strategy over nodes of the graph. A recent improvement on BFSG uses the scoring models gradient to prune directions that are unlikely to have relevant items \cite{GradientPruningTowardFastNeuralRanking}. Other works make use of queries to form a query-item graph to produce more informative neighbors \cite{FastNeuralRankingOnBipartiteGraphIndices}.

Our work differs from these works in one major way, we do not have a query representation and document representation thus our method requires no combination step, instead we produce a query-conditioned neural network for each query and directly apply this to the document representation. This approach can reduce the similarity network's size and does not require choosing between inference speed and larger query representations. Furthermore the flexibility of our framework means we can replicate any existing learned relevance model as discussed in Section~\ref{sec:ComparisonExistingNeuralScoring}. On a broader note there has been surprisingly little work on neural based scoring for full-scale retrieval, especially in the modern era of transformer based encoders. We hope our work can be a useful foundation and proof-of-concept for future work in this area.

\paragraph{\textbf{Hypernetworks}}
Hypernetworks also known as hypernets are neural networks which produce the weights for other neural networks. The term was first used by \citet{HyperNetworks} who demonstrated the effectiveness of hypernetworks to generate weights for LSTM networks. Since then, hypernetworks have been used in a variety of ways including neural architecture search \cite{GraphHyperNetworksNeuralArchitectureSearch}, continual learning \cite{ContinualLearningWithHypernetworks}, and few-shot learning \cite{MetaLearningWithLatentEmbeddingOptimization, HyperShot} to name a few. Generally, hypernetworks take a set of input embeddings that provide information about the type of task or network where the weights will be used. These embeddings are then projected to the significantly larger dimension of the weights of the ``main'' network. As the outputs of most hypernetworks are so large the hypernetworks themselves are often very simple such as a few feed-forward layers in order to keep computation feasible. Our case is unique in that our hypernetwork, the \name{}, is much larger than the small scoring network which we call \mininame{} (i.e. the ``main'' network). Additionally, to the best of our knowledge, this paper represents the first work to explore hypernetworks for first stage retrieval.


\section{\name}

Neural ranking models can be generally categorized into early-interaction and late-interaction models \cite{Dehghani2017WeakSupervision, MonoBERT, ColBERT_v1, SimLM,citadel,coil,colberter}. Currently, the most common implementation of early-interaction models is in the form of \textit{cross-encoders} (Figure~\ref{fig:main_overview} (second from the left)), where the query text $q$ and document text $d$ are concatenated (together with some predefined tokens or templates) and fed to a transformer network that learns a joint representation of query and document and finally produces a relevance score. The joint representation prevents these models from being able to precompute document representations, thus they cannot be used efficiently on large corpora \cite{Guo2020,Mitra:2018:NeuralIR}.

The most popular implementation of late-interaction models follows a \textit{bi-encoder} (or two tower) network architecture (Figure~\ref{fig:main_overview} (left)), where query and document representations are computed separately and a scoring function is used to estimate the relevance score. Formally, let $E_q \in \mathbb{R}^{n \times h}$ denote the representation learned for query $q$ consisting of $n$ $h$-dimensional vectors. Similarly, $E_d \in \mathbb{R}^{m \times h}$ denotes the representation learned for document $d$ consisting of $m$ vectors of the same dimensionality. The relevance score between $q$ and $d$ is computed as follows:
\begin{equation}
    \psi(E_q, E_d)
\end{equation}
where $\psi: \mathbb{R}^{n \times h} \times \mathbb{R}^{m \times h} \rightarrow \mathbb{R}$ denotes the scoring function.

In order to take advantage of efficient indexing techniques, such as an inverted index in the case of sparse representations \cite{SNRM, SPLADE} or approximate nearest neighbor (ANN) search in the case of dense representations \cite{DPR}, many existing works use pooling techniques to obtain a single vector representation for each query and document and then employs simple and light-weight scoring functions, such as inner product or cosine similarity. There also exist more expensive methods that do not use pooling and perform such light-weight scoring functions at the vector level and then aggregate them, such as the maximum inner product similarity used in ColBERT \cite{ColBERT_v1}.

\paragraph{\textbf{On the Limitations of Linear Similarity Functions (e.g., Inner Product)}}
We believe the simple similarity functions used by existing bi-encoder models are not sufficient for modeling complex relationships between queries and documents. These functions inherently limit retrieval models to judge relevance in a way that can be represented by an inner product. Furthermore, it has been shown that the ability to compress and reconstruct information is correlated with the size, and thus complexity, of neural models \cite{LanguageModelingIsCompression}. This result indicates that using a relevance function as simple as an inner product likely reduces the amount of information that can be stored in a fixed representation size.
These factors explain why state-of-the-art dense retrieval models continue to underperform cross-encoder models, in terms of retrieval quality \cite{lin2022pretrained}. In the following, we show the limitations of inner products (as a linear similarity function) by theoretically demonstrating the impossibility of inner products to produce perfect rankings for some queries, regardless of the method used to create the query and document embeddings.

Let $C$ denote a corpus of $N$ documents, each being represented by an $h$-dimensional vector. A perfect ranking of documents in $C$ for a provided query is a ranking where all relevant documents are ranked above all non-relevant documents. According to Radon's Theorem \cite{RadonTheorem}, any set of $h+1$ document vectors with $h$ dimensions can be partitioned into two sets whose convex hulls intersect. An important application of Radon's Theorem is in calculating the Vapnik–Chervonenkis (VC) dimension \cite{VCDim} of $h$-dimensional vectors with respect to linear separability. For any $h+2$ vectors, the two subsets of a Radon partition cannot be linearly separated. In other words, for $N>h+1$, there exists at least one group of documents that is not linearly separable from the rest. In the real world, since $N \gg h+1$, there are indeed many such non-separable subsets. If any two of these subsets contain all the relevant documents for a query, then no linear similarity function can perfectly separate relevant from irrelevant documents. This includes inner product similarity and guarantees that, for some query, there will be an imperfect ranking.

To overcome these limitations with inner product similarity we use a multi-layer neural network with query-conditioned weights as our similarity measure. As neural networks are universal approximators \cite{UniversalApproximators}, \name{}'s similarity function can express far more complex functions than those expressed by inner products. A related alternative approach with the same benefits takes the query and document representations, combines them (e.g., through concatenation or similarity matrices), and feeds them to a neural network to serve as a similarity function (Figure~\ref{fig:main_overview} (second from the right)). However, this approach suffers from the following shortcomings: (1) query and document representations now need to be combined before scoring -- adding latency proportional to the complexity of the method used to combine them; (2) having separate query and document representations increases the input dimension to the neural network further increasing latency; (3) for efficiency reasons, the query representation is often pooled or compressed before being input into network which reduces the information the model receives. \name{} addresses these shortcomings. Since the query is directly encoded as the neural network's weights no concatenation or other form of combining inputs is needed, the document representation can be directly input to the scoring network. This, in addition to the reduced network size from having only document representations as input, allows for a substantial latency improvement. Further, as \name{} produces a query-specific neural network, every weight can be used to store query-related information without any need for compression or additional overhead. Lastly, we show in Section~\ref{sec:ComparisonExistingNeuralScoring} that existing learned relevance methods can be exactly replicated by \name{} with the additional flexibility of learning query-specific weights when desirable.



\subsection{\name{} Overview}


An overview of our model is depicted in Figure~\ref{fig:main_overview} (right); it represents a new category of models that sit between a cross-encoder and a bi-encoder model. Like a bi-encoder model, our method computes the query and document representations separately, but unlike most existing retrieval methods, our method allows for more complicated matching signals like those present in cross-encoder models. Following existing methods, we have a query encoder and a document encoder. When a document $d$ is input into to the document encoder, we obtain a representation similar to existing encoder models, namely a set of one or more vectors $E_d \in \mathbb{R}^{m \times h}$ that represent the document's content, where $m$ is the number of vectors and $h$ is the dimension of the vectors. Though we focus on vectors in this work, in theory, the representation can be anything a neural network can output.

Now comes our unique contribution that allows our method to consider more complex similarity signals. Given the query $q$, the query encoder \QueryEncoderName{} first produces a set of contextualized embeddings in a similar way to existing encoder models which we will call $E_q \in \mathbb{R}^{n \times h}$, where $n$ is the number of embeddings and $h$ is the dimension of the embeddings. At this point while existing methods apply a simple pooling mechanism, our query encoder instead uses a \HyperHeadName{}. \textit{The \HyperHeadName{} takes $E_q$ and produces a set of matrices and vectors that are then used as the weights and biases for a small neural network which we coin the \mininame{}.} The \mininame{} is a query-dependent function for estimating relevance scores for each document, meaning each \mininame{} is unique to the query that created it, unlike existing neural scoring methods which use a shared set of weights for all queries. To find the relevance of a document, the document representation $E_d$ is passed as input to the \mininame{} which outputs the relevance score.


\name{} is a generic framework which allows direct application of existing paradigms from neural retrieval and, more broadly, machine learning. For example, \name{} could easily work with multiple vectors similar to existing multi-vector models, e.g., \cite{ColBERT_v1}, or use training routines popularized in dense retrieval, e.g., \cite{ANCE,RANCE,CL-DRD,DRAGON}. As an initial exploration, this paper focuses on showing the efficacy of \name{} without additional complexity and thus uses a single vector document representation and no complex training recipes.



\subsection{Query and Document Encoders}
The \name{} framework is generic and can be applied to any implementation of query and document encoders. In this work, we use pretrained transformer-based encoder models commonly used in the recent neural network literature. Specifically, we use a pretrained BERT base model \cite{BERT} for encoding queries and documents. Even though \name{} can operate on all token representations produced for each document this work focuses on a single vector representation of documents, which is more efficient in terms of query latency, memory requirements, and disk usage. To do so, we can either use the contextualized embedding representing the \texttt{[CLS]} token or take the mean of all the contextualized embeddings for all the non-pad input tokens. Empirically, we found that using the \texttt{[CLS]} token performs better. Therefore, the document representation produced by the encoder is a single vector with 768 dimensions, i.e., the same as BERT's output dimensionality. We refer to it as $E_d \in \mathbb{R}^{m \times h}$, where $m=1$ in our setting.

Since \name{} only uses the contextualized-query-token representations once to produce the \mininame{}, it can skip pooling tokens without adding much cost. Therefore, we use all non-pad-token representations produced by the query encoder as the intermediate representation of the queries, denoted by $E_q \in \mathbb{R}^{n \times h}$, where $n$ is the number of tokens in the query $q$ and $h$ is the embedding dimensionality ($h=768$ in BERT).


\subsection{The Hyperhead Layers} \label{sec:hyperhead-layers}
The method to transform $E_q$ into the weights and biases for the \mininame{} is performed by the \hyperheadlayer{}s and is completely flexible. During our experimentation, we tried two mechanisms to do this transformation as well as many minor variants and found them all to have stable training, which suggests the \name{} framework is robust to the exact \hyperheadlayer{} implementation. Though we tried two approaches, we settled on one for the final set of experiments in this paper which we will now describe.

For improved clarity, we focus only on the weight creation process as the biases are created in the exact same way. The contextualized query embeddings $E_q \in \mathbb{R}^{n \times h}$ produced by the query encoder are independently transformed by $l$ \hyperheadlayer{}s, each of which corresponds to a layer in the \mininame{}. Each \hyperheadlayer{} converts the embeddings $E_q$ into key and value matrices:
\begin{equation}
    K^q_i = \theta_{K_i} \times [E_q; 1]\quad\quad V^q_i = \theta_{V_i} \times [E_q; 1]
\end{equation}

where $\theta_{K_i}, \theta_{V_i} \in \mathbb{R}^{h \times h}$  denote learnable parameters for constructing key and value matrices. In the above equation, the embedding matrix $E_q$ is concatenated with a column of all ones (i.e., $[E_q; 1]$) to model both weight multiplication and bias addition.

Each key matrix $K_i$ and value matrix $V_i$ will be used for the creation of the weights in the $i$\textsuperscript{th} layer of the \mininame{}. With the keys and values in hand, single-head scaled-dot-product attention \cite{AttentionIsAllYouNeed} is performed using a query matrix $Q_i \in \mathbb{R}^{r \times h}$ where $r$ is the layer dimensionality in the $i$\textsuperscript{th} layer of \mininame{}. In our case, all of the weights except the last layer are square matrices, making $r=h$. Each $Q_i$ is a set of learnable embeddings, similar to those used as input tokens for transformer models. Hence, the hidden layer representation $H_i \in \mathbb{R}^{r \times h}$ is then computed as follows:
\begin{equation}
    H_i = \text{softmax} \left(\frac{Q_i K_i^T}{\sqrt{h}} \right) V_i
\end{equation}

A ReLU activation \cite{ReLU} is then applied to each $H_i$ followed by layer normalization \cite{LayerNorm}. Next a point-wise feed-forward layer is applied to produce $\widehat{H}^q_i$:
\begin{equation}
    \widehat{H}^q_i = \theta_{W_i} \text{L-Norm}\left(\text{ReLU}(H_i)\right) + \theta_{b_i}
\end{equation}
where $\text{L-Norm}$ denotes layer normalization. Note that each weight in \mininame{} has a unique $\theta_{W_i}$ and $\theta_{b_i}$. There are no learnable parameters in layer normalization.

The final operation to get the $i$\textsuperscript{th} weight $W^q_i$ for \mininame{} is:
\begin{equation}
    W^q_i = \widehat{H}^q_i + \theta_{H_i}
\end{equation}
where $\theta_{H_i} \in \mathbb{R}^{r \times h}$ is the same size as $\widehat{H}^q_i$ and acts as a base weight which allows the model to learn universal (i.e., query-independent) patterns that are applicable for all queries.

The process for the bias vectors is identical except the query matrix used in the attention operation $Q_i \in \mathbb{R} ^ {r \times h}$ has $r=1$ as there is only a single column in the output.

\subsection{The \mininame{} Network}

Weights and biases produced by the \hyperheadlayer{}s are not by themselves a neural network. They need a certain arrangement and additional components (e.g. non-linearity). This is where the \name{}'s \mininame{} converter comes in. The converter knows the architecture of the \mininame{} and given the weights and biases from the \hyperheadlayer{}s, it produces a callable neural network object which takes as input the document representation $E_d$.

It is worth highlighting that because the \mininame{}'s architecture is not strictly tied to how the \hyperheadlayer{} produces the weights and biases, it is simple to modify the architecture of the \mininame{}. All the \hyperheadlayer{}s need to know is how many weights and biases are needed and what shape they should be.

In our experiments, we use a simple feed-forward architecture for the \mininame{}. The output $x^d_{i+1}$ for the input $x^d_i$ at a given layer $i$ is given by:
\begin{equation}
    x^d_{i+1} = \text{L-Norm}\left(\text{ReLU}(W^q_i(x^d_i) + b^q_i)\right) + x^d_i
\label{eq:mininet-layer}
\end{equation}
where $\text{L-Norm}$ represents a layer normalization without learnable parameters and the addition of $x^d_i$ is a residual connection. No residual connection is applied before the final layer (i.e., layer $l$). The layer in Equation~\eqref{eq:mininet-layer} is repeated $l$ times. Finally, a relevance score is produced using a linear projection layer with an output dimensionality of 1.

\subsection{Training}
Training \name{} is no different from training a bi-encoder as it shares the same core components, i.e., a query encoder and document encoder. The only difference is instead of using an inner product to find the similarity the \mininame{} is applied to the document representations. Thus, our contributions are solely the architecture and not a specific training technique. In this paper we employ a simple distillation training setup, for more details see Section~\ref{sec:ExperimentalSetup}

\subsection{Efficient Retrieval using \name{}} \label{sec:EfficientSearch}
Being able to perform efficient retrieval is crucial for many real-world search scenarios where an exhaustive search is not feasible. For \name{} models, there is a clear parallel to dense models as both represent documents as dense vectors, but the differences between \name{} and dense models make it unclear whether the same efficient search techniques will work. For instance, it is clear that due to the linear nature of inner products, similar document vectors are likely to have similar inner products with a query vector; in the case of \name{} this assumption may not hold true as the non-linear nature of the \name{} scoring function could mean small differences in the input vector produce significant differences in the output score.

To study the extent to which \name{}'s retrieval can be approximated for efficient retrieval, we developed an approximate search technique based loosely on navigating small world graphs \cite{SmallWorld, HNSW}. In the index stage we construct a graph where documents are nodes connected to their neighbors by edges.
We use $L_2$ distance between document embeddings similar to \cite{NeuralNetworkFastItemRanking}.

After constructing the document graph, approximate search is performed following Algorithm~\ref{alg:efficient_search}. In brief, a set of initial candidate documents $\Tilde{C}$ is selected at random, these candidates are scored with the \mininame{} (line~\ref{alg:line:find_top}) and in lines \ref{alg:line:16}-\ref{alg:line:19} the best $nCandidates$ and their neighbors become the next candidates. In lines \ref{alg:line:12}-\ref{alg:line:15}, the top scoring candidates are added to $T$--a set which stores the $k$ best scoring documents so far. The algorithm terminates when one of three conditions is met: (1) the number of iterations equals $maxIter$; see line \ref{alg:line:4}, (2) there are no more candidates; see line \ref{alg:line:4}, or (3) no new documents are added to $T$ at a given step; see line \ref{alg:line:8}. We also consider an option without the final termination condition which we call \textit{without early stopping}. As the number of operations is dependent on the number of initial candidates $|\Tilde{C}|$, the running time is not tied to the number of documents, resulting in a run time complexity of $O(|\Tilde{C}| + nCandidates \cdot  maxIter)$.

With this algorithm, we found that \name{} is able to significantly increase retrieval speed without a large loss in quality. See the results in Section~\ref{sec:AnalysisOfEfficiency}.


\begin{algorithm}
\caption{\name{} Efficient Search}
\label{alg:efficient_search}

\begin{algorithmic}[1]
\Statex \textbf{Input:} \mininame{} $q$, \#NN to return $k$, initial candidates $\Tilde{C}$, \# candidates to explore every iteration $nCandidates$, $maxIter$
\Statex \textbf{Output:} $k$ closest neighbors to $q$
\State $v \leftarrow \Tilde{C}$ \Comment{set of visited elements}
\State $T \leftarrow \{-\infty\}$ \Comment{Stores top $k$ nearest neighbors to $q$ at any given time}
\State $i \leftarrow 0$ \Comment{Current iteration}
\While{$|\Tilde{C}| > 0$ and $i < maxIter$} \label{alg:line:4}
    \State $c \leftarrow$ find top $nCandidates$ values in $\Tilde{C}$ using $q$ \label{alg:line:find_top}
    \State $f \leftarrow$ get lowest scoring element from $T$
    \If{$\max_{\hat{c} \in c} \hat{c} < f$}
        \State \textbf{break} \Comment{all candidates are worse than $T$ so stop now} \label{alg:line:8}
    \EndIf
    \State $\Tilde{C} \leftarrow \{\}$ \Comment{Reset $\Tilde{C}$}
    \For{each $e \in c$} \label{alg:line:10}
        \State $f \leftarrow$ get lowest scoring element from $T$
        \If{$q(e) > f$ or $|T| < k$} \label{alg:line:12}
            \State $T \leftarrow T \cup e$
            \If{$|T| > k$}
                \State $T \leftarrow T \setminus \{f\}$   \label{alg:line:15}
            \EndIf
        \EndIf

        \For{each $n \in $ NEIGHBORS($e$)} \label{alg:line:16}
            \If{$n \notin v$}
                \State $\Tilde{C} \leftarrow \Tilde{C} \cup n$
                \State $v \leftarrow v \cup n$ \label{alg:line:19}
            \EndIf
        \EndFor
    \EndFor
    \State $i \leftarrow i + 1$
\EndWhile
\State \Return $T$
\end{algorithmic}
\end{algorithm}

\subsection{Comparison to Existing Neural IR Methods} \label{sec:ComparisonExistingNeuralScoring}

We argue that \name{} can exactly reproduce existing neural ranking models.
Let us start by formalizing the main components of existing neural methods: (1) a query representation $E_q$, (2) a document representation $E_d$, (3) some combination function $f_c(\cdot, \cdot)$, and (4) the final neural network that produces a score $f_s(\cdot)$. In comparison, \name{}
does not have an $f_c(\cdot, \cdot)$ as the \mininame{} takes $E_d$ as its only input.

We will now demonstrate that \name{} can exactly replicate any neural retrieval method that has the components above. The first step is to include $E_q$ and $f_c(\cdot, \cdot)$ in the \mininame{}. This allows the \mininame{} to exactly produce the input to $f_s(\cdot)$ that the existing neural retriever used. Next we reproduce the neural function from $f_s(\cdot)$ with query-dependent weights in the \mininame{}. When $E_d$ is input to the \mininame{}, all the original components of the existing neural retrieval model are present and thus the score can be exactly replicated. There is one difference, which is the weights of $f_s(\cdot)$ are query-dependent. However, this can be remedied in two ways: (1) shared weights can be used for all queries exactly replicating the original neural method (2) the weights for $f_c$ can have a common non-query-dependent base weight, similar to our implementation (see details in  Section~\ref{sec:hyperhead-layers}), this way if there is no benefit using query-dependent weights the shared weight can be used, but if there is additional benefit the model and optimizer can learn to take advantage of it. Thus, \name{} can not only exactly replicate all existing neural retrieval methods it also allows the model to dynamically leverage query-dependent weights when the model determines they are beneficial.




\section{Experiments}
\subsection{Datasets}
\subsubsection{Training Dataset} \label{training_datasets}
The dataset used for training our models is the training split of the MSMARCO passage retrieval dataset \cite{MSMARCO} which contains 8.8M passages and has 503K training queries with at least one corresponding relevant passage. The queries in the MSMARCO training set are short natural language questions asked by users of the Bing search engine.

To create the training pairs, we first retrieved 800 passages for every query using an early iteration of \name{}. From these, we sampled 200 passages — the top 100 passages and another 100 randomly sampled from the remaining 700 passages. These query-passage pairs were then labeled using the MiniLM cross-encoder\footnote{Available at \url{https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2}.} from the Sentence Transformers Library \cite{SentenceBERT}. %This model was chosen because of its strong performance and fast inference time.


\subsubsection{Validation Dataset}
For validating and parameter tuning, we use the TREC Deep Learning (DL) 2021 \cite{TREC-DL-21} and 2022 passage task \cite{TREC-DL-21, TREC-DL-22}. As the passage collection for TREC DL '21 and '22 is large and we wanted validation to be fast we created a subset with only passages in the QREL files.

\subsubsection{Evaluation Datasets}
Our evaluation explores retrieval performance in three different areas: in-domain performance, out-of-domain performance, and performance on hard retrieval tasks.

For in-domain performance, we use the MSMARCO Dev set \cite{MSMARCO}, TREC Deep Learning 2019 \cite{TREC_DL_2019}, and TREC Deep Learning 2020 \cite{TREC_DL_2020}. The MSMARCO Dev set contains around 7k queries with shallow labels, the majority of queries only have a single passage labeled as relevant. This collection uses queries from the same distribution as the training queries making it a clear test of the in-domain performance. On this dataset we report the standard evaluation metrics: MRR and Recall@1000. The TREC Deep Learning 2019 and 2020 datasets have a similar query distribution to MSMARCO Dev but feature far fewer queries, i.e., 97 queries combined. The lower number of queries is compensated by far deeper annotations with every query having several annotated passages.

\pgfplotsset{ every non boxed x axis/.append style={x axis line style=-},
     every non boxed y axis/.append style={y axis line style=-}}

\definecolor{graph_color_1}{HTML}{277da1}
\definecolor{graph_color_2}{HTML}{f8961e}

\begin{figure*}[h!]
    \centering

    \begin{tikzpicture}[baseline]
    \begin{axis}[
        scale only axis,
        % xmin=0,
        % xmax=12,
        axis y line*=left,
        xlabel=$|\Tilde{C}|$,
        ylabel=\textcolor{graph_color_1}{\footnotesize nDCG@10},
        width=0.19\textwidth,
        height=2cm,
        legend style={
            at={(1.0,-0.25)},
            anchor=north east,
            text=black,
            % font=\small,
            legend image post style={black},
            nodes={scale=0.6},
        },
        enlarge y limits=0.1,
        ytick={0.64, 0.66, 0.68, 0.7, 0.72},
        xtick pos=left,
        ytick pos=left,
        ylabel near ticks,
        xlabel near ticks,
        xmode=log,
    ]
        % nDCG with early stop
        \addplot[graph_color_1, line width = 1.0pt] coordinates {(10,0.657227173201666)(100,0.6839815941070432)(1000,0.6970931130909354)(10000,0.7028781904961429)(10000,0.7028781904961429)(10000,0.7028781904961429)(100000,0.6968752971431624)};
        % nDCG without early stop
        \addplot[graph_color_1, dashed, line width = 1.0pt] coordinates {(10,0.657227173201666)(100,0.6839815941070432)(1000,0.7098415361176305)(10000,0.7156266135228379)(10000,0.7156266135228379)(10000,0.7156266135228379)(100000,0.6984160930374507)};
    \end{axis}
    %
    \begin{axis}[
        scale only axis,
        axis y line=right,
        axis x line=none,
        ylabel=\textcolor{graph_color_2}{\footnotesize Query Latency (ms)},
        width=0.19\textwidth,
        height=2cm,
        enlarge y limits=0.1,
        ylabel near ticks,
        xmode=log,
        xlabel near ticks,
    ]
        % Time with early stop
        \addplot[graph_color_2, line width = 1.0pt] coordinates {(10,78.49962212318599)(100,65.96895151360091)(1000,63.07049684746321)(10000,57.6694178026776)(10000,57.6694178026776)(10000,57.6694178026776)(100000,143.09355824492698)};
        %  Time without early stop
        \addplot[graph_color_2, dashed, line width = 1.0pt] coordinates {(10,73.46584076105161)(100,80.34529796866484)(1000,81.01236542990041)(10000,90.0667434514955)(10000,90.0667434514955)(10000,90.0667434514955)(100000,176.44097084222838)};
    \end{axis}
    \end{tikzpicture}
    % \quad
    \begin{tikzpicture}[baseline]
    % Begin second figure nCandidates
    \begin{axis}[
        scale only axis,
        % xmin=0,
        % xmax=12,
        axis y line*=left,
        xlabel=$nCandidates$,
        ylabel=\textcolor{graph_color_1}{\footnotesize nDCG@10},
        width=0.19\textwidth,
        height=2cm,
        legend style={
            at={(1.0,-0.25)},
            anchor=north east,
            text=black,
            % font=\small,
            legend image post style={black},
            nodes={scale=0.6},
        },
        enlarge y limits=0.1,
        ytick={0.64, 0.66, 0.68, 0.7, 0.72},
        xtick pos=left,
        ytick pos=left,
        ylabel near ticks,
        xlabel near ticks,
    ]
        % nDCG with early stop
        \addplot[graph_color_1, line width = 1.0pt] coordinates {(8,0.6253591715706962)(16,0.6448268761626977)(32,0.6786175787253655)(64,0.7028781904961429)(64,0.7028781904961429)(64,0.7028781904961429)(128,0.6974597931411562)(256,0.7087882464950501)(512,0.7210713713340823)(1024,0.7341100378511684)};
        % nDCG without early stop
        \addplot[graph_color_1, dashed, line width = 1.0pt] coordinates {(8,0.6253591715706962)(16,0.6448009750670014)(32,0.6785916776296691)(64,0.7156266135228379)(64,0.7156266135228379)(64,0.7156266135228379)(128,0.7137632491351856)(256,0.7215366695217451)(512,0.7341100378511684)(1024,0.7341100378511684)};
    \end{axis}
    %
    \begin{axis}[
        scale only axis,
        axis y line=right,
        axis x line=none,
        ylabel=\textcolor{graph_color_2}{\footnotesize Query Latency (ms)},
        width=0.19\textwidth,
        height=2cm,
        enlarge y limits=0.1,
        ylabel near ticks,
        xlabel near ticks,
    ]
        % Time with early stop
        \addplot[graph_color_2, line width = 1.0pt] coordinates {(8,49.67966745066088)(16,50.005957137706666)(32,50.98761514175769)(64,57.6694178026776)(64,57.6694178026776)(64,57.6694178026776)(128,80.60029495594114)(256,141.22345835663552)(512,250.4059270370838)(1024,492.0011675635049)};
        %  Time without early stop
        \addplot[graph_color_2, dashed, line width = 1.0pt] coordinates {(8,61.17011225500772)(16,65.369306608688)(32,73.49416821501977)(64,90.0667434514955)(64,90.0667434514955)(64,90.0667434514955)(128,129.01445322258527)(256,236.37792675994163)(512,458.35791077724724)(1024,894.4306595380916)};
    \end{axis}
    \end{tikzpicture}
    % \quad
    %%%%%% Start last plot maxIter
    \begin{tikzpicture}[baseline]
    \begin{axis}[
        scale only axis,
        % xmin=0,
        % xmax=12,
        axis y line*=left,
        xlabel=$maxIter$,
        ylabel=\textcolor{graph_color_1}{\footnotesize  nDCG@10},
        width=0.19\textwidth,
        height=2cm,
        legend style={
            at={(-0.35,-0.25)},
            anchor=north west,
            text=black,
            % font=\small,
            legend image post style={black},
            nodes={scale=0.6},
        },
        enlarge y limits=0.1,
        ytick={0.3, 0.4, 0.5, 0.6, 0.7},
        xtick pos=left,
        ytick pos=left,
        ylabel near ticks,
        xlabel near ticks,
    ]
        \addplot[graph_color_1, line width = 1.0pt] coordinates {(2,0.26477399644530447)(4,0.6153305693793425)(8,0.6662349555809605)(12,0.7028781904961429)(12,0.7028781904961429)(12,0.7028781904961429)(16,0.7028781904961429)(20,0.7028781904961429)(24,0.7028781904961429)(28,0.7028781904961429)(32,0.7028781904961429)};
        \addplot[graph_color_1, dashed, line width = 1.0pt] coordinates {(2,0.26477399644530447)(4,0.6153305693793425)(8,0.6662349555809605)(12,0.7156266135228379)(12,0.7156266135228379)(12,0.7156266135228379)(16,0.7150811434696449)(20,0.715042117524844)(24,0.715042117524844)(28,0.715042117524844)(32,0.715042117524844)};
        \legend{w/ early stop, w/o early stop}
    \end{axis}
    %
    \begin{axis}[
        scale only axis,
        axis y line=right,
        axis x line=none,
        ylabel=\textcolor{graph_color_2}{\footnotesize Query Latency (ms)},
        width=0.19\textwidth,
        height=2cm,
        enlarge y limits=0.1,
        ylabel near ticks,
        xlabel near ticks,
    ]%
        \addplot[graph_color_2, line width = 1.0pt] coordinates {(2,28.885248095490212)(4,39.968363074369215)(8,55.21737143050793)(12,57.6694178026776)(12,57.6694178026776)(12,57.6694178026776)(16,57.981796042863714)(20,58.43507411868074)(24,61.809140582417335)(28,61.977292216101354)(32,59.12499095118323)};
    %
        \addplot[graph_color_2, dashed, line width = 1.0pt] coordinates {(2,28.600265813428305)(4,39.628139761991285)(8,69.41637881966524)(12,90.0667434514955)(12,90.0667434514955)(12,90.0667434514955)(16,113.7952028318893)(20,137.69336633904035)(24,162.62342763501545)(28,187.94419044672057)(32,213.92392003258996)};
    \end{axis}
    \end{tikzpicture}
    \caption{Relationship between the three main parameters of our efficient search: the size of the initial set of candidates $\Tilde{C}$, the number of neighbors to explore $nCanddidates$, and the number of iterations $maxIter$ and both effectiveness in terms of TREC DL '19 nDCG@10 and efficiency in terms of Query Latency.}
    \label{fig:approximate-search-tradeoff}
\end{figure*}


To assess out-of-domain performance, we evaluate on question answering tasks on different domains, specifically, the TREC COVID \cite{TREC-Covid} and NFCorpus datasets \cite{NFCorpus} for the biomedical domain and FiQA \cite{FiQA} for the financial domain. We also evaluate on DBPedia \cite{DBpedia} as an entity retrieval dataset and on Touch\'{e} \cite{Touche} as an argument retrieval dataset. We use the BEIR \cite{BEIR} versions of these datasets from the ir\_datasets library.\footnote{Available at \url{https://ir-datasets.com/}.}

To explore the full capabilities of \name{} we want to evaluate how it performs on retrieval tasks that are more challenging than standard question-passage retrieval tasks. To some extent hardness is subjective, but we tried to define a clear set of criteria to define difficulty: (1) current neural retrieval models should struggle on the task, (2) term matching models like BM25 should also struggle on the task, (3) the queries are longer or otherwise more complicated than standard web queries. An additional requirement we had was that for tasks that were significantly different from the MSMARCO training data we wanted adequate training data to fine-tune the models before evaluation. We believe this is reasonable as we are not investigating the models' zero-shot performance but the inherent limits of the model.

The first dataset we select was the TREC Tip-of-the-Tongue (TOT) 2023 \cite{TREC-TOT-2023} that contains queries written by users that know many aspects of the item they are looking for but not the name of the item. Thus TOT queries tend to be verbose and can include many facets. The TREC TOT 2023 dataset specifically looks at TOT queries for movies with the corresponding movie's Wikipedia page as the golden passage. We use the development set as the test set relevance labels are not public yet. There are 150 queries. Each query has a single relevant passage. For training we use the data from  \citet{TOMT-TrainingDataset} which is around 15k TOT queries from Reddit for the book and movie domain.

The second dataset is FollowIR \cite{FollowIR} for instruction following in retrieval. This dataset is built on top of three existing TREC datasets: TREC Robust '04 \cite{Robust04}, TREC News '21 \cite{TREC-News20}, and TREC Core '17 \cite{TREC-Core17}; it uses the fact that these datasets include instructions to the annotators which can act as a complex instruction. To test how well a retrieval system follows the instruction the creators of FollowIR modify the instruction to be more specific and re-annotate the known relevant documents. %By comparing the differences in retrieval before and after the change the instruction following capabilities can be directly assessed.
As training data we use MSMARCO with Instructions, a recent modification of MSMARCO which adds instructions to the queries as well as new positive passages and hard negative passages which consider the instruction \cite{MSMARCO-with-Instructions}.

The final dataset is a subset of TREC DL-HARD \cite{TREC_DL_HARD}. The full dataset uses some of the queries from TREC DL 2019 and 2020 as well as some queries that were considered for DL 2019 and 2020 but were not included in the final query collection. TREC DL-HARD is built specifically with the hardest queries from the TREC DL pool. The authors do so by using a commercial search engine to find queries that are not easily answered. The standard TREC DL-HARD dataset has 50 queries half of which appear in TREC DL 2019 or TREC DL 2020 and half of which are new queries which are labeled by the authors of TREC DL-HARD. We found that the queries labeled by the authors had far fewer judged documents in the top 10 documents compared to those labeled by TREC (around 15\% versus 93\%), this made the evaluation metrics unreliable so we decided to only use those with TREC labeling. %This means that all the queries are also in TREC DL 19' and 20', but as indicated by the metrics being substantially the queries are indeed hard which we feel provides a useful additional perspective. As this task is in-domain for the models we do not do any additional fine-tuning before evaluation on this task.

\subsection{Experimental Setup} \label{sec:ExperimentalSetup}
\subsubsection{Training Details}
All the \name{}s use BERT \cite{BERT} base uncased as the base model. We use PyTorch and Huggingface for model implementation and training. All of our \mininame{}s use a input dimension of 768 and hidden dimension of 768. Unless otherwise stated, we use 6 linear layers in the model not including the final output projection layer.

We use a training batch size of 64 per device and 128 in total. A single example in the batch is a query, positive document, and 8 additional documents ranging in relevance. The positive document is the top ranked document by our teacher model. The other 8 documents are sampled randomly from the passages associated with the query. For more details about the dataset see Section \ref{training_datasets}. Passages were truncated to 196 tokens and queries to 32 tokens.

Our primary loss function is Margin MSE \cite{MarginMSE}. When computing the loss, we construct (query, positive document, negative document) triplets where all of the negatives for a query form their own triplet. The loss is found by averaging the individual loss of all the triplets. In addition to Margin MSE, we use an in-batch cross entropy loss where the (query, positive document) is assumed to be a true positive and all the other queries' positive documents are assumed to be negatives. We do not consider the additional ``hard'' negatives from the query in the cross entropy loss as many of these documents are relevant to the query. We use AdamW as our optimizer with default settings and a learning rate of 2e-5 with a constant scheduler after a warm up of 6k steps. Our training hardware consist of two A100 GPUs with 80GB memory. Training took around 6 days.

To select the best model we evaluate each model on the validation set every 50k steps and pick the model with the best R Precision within the first 800k steps. We selected R Precision due to the fact that it balances both recall and precision in a single metric and does not require a predefined cutoff.

When training for the harder tasks we use AdamW with the learning rate 8e-6 with a linear scheduler and a warm-up ratio of 1e-1. For TOT training we train for 25 epochs or 3.3k steps. For FollowIR training we train for 1 epoch or around 10k steps. We use a batch-size of 96 and cross entropy loss. Each example in the batch includes a query, positive document, and hard negative document. We use a maximum document and query length of 512 tokens.



\subsection{Baselines}

For comparison with \name{}, we include several baseline models and models which we include for reference which are not directly comparable. Our main baselines which we evaluate on all datasets are TAS-B \cite{TAS-B}, CL-DRD \cite{CL-DRD}, BM25, and our own bi-encoder baseline which we call BE-Base. We train BE-Base exactly the same as \name{} except we use separate encoders and use a linear LR scheduler. We select our main dense baselines TAS-B and CL-DRD as they are both strong bi-encoder models which leverage knowledge-distillation training and which use the same document embedding dimension of 768. For in-domain results we include an additional set of dense models: ANCE \cite{ANCE}, TCT-ColBERT \cite{TCT-ColBERT}, and MarginMSE \cite{MarginMSE}. We only include these models in the in-domain results to save space in other sections and because TAS-B and CL-DRD outperform the other baselines. For reference we also include: the late-interaction model ColBERT v2 \cite{colbert_v2}; the neural sparse model SPLADE++ SD \cite{SPLADE++}; RepLLaMA a 7b parameter bi-encoder model \cite{RepLlama}; DRAGON a bi-encoder trained with 5 teacher models and 20M synthetic queries; MonoBERT a reranking model reranking the top 1k BM25 retrieavls \cite{MonoBERT}; and the reranking model cross-SimLM reranking the top 200 passages from bi-SimLM \cite{SimLM}. Reference results were taken from the RepLLaMA \cite{RepLlama} and DRAGON \cite{DRAGON} papers.


\subsection{Results and Discussion}


\begin{table}[t]
\caption{Comparison on in-domain evaluation datasets. The symbols next to each baseline indicate significance values with $p < 0.05$. Note, that $\dagger$ is a group of baselines.} \label{table:in-domain-results}
\scalebox{0.79}{
\begin{tabular}{l!{\color{lightgray}\vrule}lll!{\color{lightgray}\vrule}ll!}
\toprule
                                    & \multicolumn{3}{c!{\color{lightgray}\vrule}}{\textbf{TREC-DL '19 \& '20}} & \multicolumn{2}{c}{\textbf{MSMARCO Dev}}  \\
Model                               & nDCG@10        & RR             & R@1000         & RR@10          & R@1000                    \\
\midrule
\multicolumn{6}{l}{\textbf{Single Vector Dense Retrieval Models \& BM25 (Baselines)}} \\
\textbf{BM25} $\dagger$  & 0.491          & 0.679          & 0.735          &               0.184 &      0.853                     \\
\textbf{ANCE} $\dagger$                 & 0.646          & 0.811          & 0.767          & 0.330          & 0.958                     \\
\textbf{TCT-ColBERT} $\dagger$ & 0.669          & 0.820          & 0.806          & 0.335          & 0.964                     \\
\textbf{Margin MSE} $\dagger$    & 0.669          & 0.845          & 0.782          & 0.325          & 0.955                     \\
\textbf{TAS-B} $\spadesuit$         & 0.700          & \uline{0.863}  & 0.861          & 0.344          & 0.978                     \\
\textbf{CL-DRD} $\diamondsuit$      & 0.701          & 0.844          & 0.838          & 0.382          & \textbf{0.981}            \\
\textbf{BE-Base} $\clubsuit$        & \uline{0.713}  & 0.855          & \uline{0.868}  & 0.359          & 0.980                     \\
\midrule
\textbf{\name{}} & \textbf{0.736}$^{\dagger \spadesuit \diamondsuit \clubsuit}$ & \textbf{0.885}$^{\dagger \diamondsuit}$ & \textbf{0.871}$^{\dagger \diamondsuit}$ & \textbf{0.386}$^{\dagger \clubsuit \spadesuit}$ & \textbf{0.981}$^{\dagger \spadesuit}$ \\
\midrule
\multicolumn{6}{l}{\textbf{Other Retrieval Models (Reference Models)}} \\
\textbf{ColBERT v2}                 & 0.749          & -              & -              & 0.397          & 0.984                     \\
\textbf{SPLADE++ SD}                & 0.723          & -              & -              & 0.368          & 0.979                     \\
\textbf{RepLLaMA}                   & 0.731          & -              & -              & 0.412          & 0.994                     \\
\textbf{DRAGON}                     & 0.734          & -              & -              & 0.393          & 0.985                     \\
\textbf{MonoBERT}                   & 0.722          & -              & -              & 0.372          & 0.853                     \\
\textbf{cross-SimLM}                & 0.735          & -              & -              & 0.437          & 0.987                     \\
\bottomrule
\end{tabular}
}
\vspace{-8pt}
\end{table}




\subsubsection{In-Domain Results}
Our in-domain results are presented in Table~\ref{table:in-domain-results}; they demonstrate that compared with baselines and even the reference models \name{} has very strong performance. \name{} is significantly better than each baseline in nDCG@10 on the combined TREC DL '19 and '20 and statistically better than all but CL-DRD on MSMARCO Dev RR@10. The most direct comparison, BE-Base, has far lower nDCG@10, RR, and RR@10 values indicating the \name{} is able to bring a large boost in precision based metrics over dense retrieval. In terms of recall \name{} is either as good or better than all the baselines though the gap is not as large as for precision based metrics.

Impressively \name{} is able to surpass DRAGON on nDCG@10 on the combined TREC DL '19 and '20 query set, though DRAGON uses the same base model and is a bi-encoder, it uses 32 A100s to train, 40x the training queries, and a complex 5 teacher curriculum learning distillation training technique. In other words, DRAGON is likely close to if not the ceiling for BERT-based bi-encoders and still \name{} is able to match it with a simple distillation training setup and far less training compute.

\name{} also beats both rerankers MonoBERT and cross-SimLM; demonstrating that reranking cannot make up for a weak retriever's performance. Continuing in TREC-DL '19 and '20 we find that \name{} even surpassed RepLLaMA which is more than 60x larger and which also uses a significantly larger document embedding dimension of 4096. In fact the only model beating \name{} in nDCG@10 is ColBERTv2 which uses an embedding for every token in the document compared to \name{}'s fixed 768 dimension token. MSMARCO Dev results are also good with \name{} outperforming all the baselines and outperforming a few of the reference models such as SPLADE++ and MonoBERT.

Overall \name{}'s in-domain results are exceptionally strong given the simple training routine used, small encoder model size, and document representation size. To the best of our knowledge, \name{} sets a new record for combined TREC-DL '19 and '20 nDCG@10 with a 768 dimension dense document vector.

\subsubsection{Out-of-Domain Results}
Table~\ref{tab:out-of-domain} shows our results on the select out-of-domain datasets, we only include our main baseline models and BM25 due to space limitations. The general trend is that \name{} has strong out-of-domain performance in question answering tasks (Q\&A) and entity retrieval tasks. This indicates that despite \name{}'s more complex similarity function it is still able to generalize well in a zero-shot manner to new tasks.

\begin{table}
    \centering
    \caption{Out-of-domain results in nDCG@10. We only compare significance with BE-Base. Significance results with $p < 0.05$ are shown with the $\BeBaseSymbol$ and $p < 0.1$ are shown with $\diamondsuit$.} \label{tab:out-of-domain}
    \scalebox{0.85}{
    \begin{tabular}{
        p{1.75cm}!{\color{lightgray}\vrule}llll!{\color{lightgray}\vrule}l!
    }
    \toprule
        & \multicolumn{4}{c!{\color{lightgray}\vrule}}{\textbf{Baselines}} & \textbf{Ours} \\
        \midrule
        Rep type & sparse & dense & dense & dense & hypernet  \\

        \midrule
        & BM25 & TAS-B & CL-DRD & BE-Base & Hypecoder \\
        %& \rotatebox[origin=c]{290}{BM25} & \rotatebox[origin=c]{290}{TAS-B} & \rotatebox[origin=c]{290}{CL-DRD} & \rotatebox[origin=c]{290}{BE-Base} & \rotatebox[origin=c]{290}{Hypecoder} \\
        \midrule
        \multicolumn{2}{l}{\textbf{Q \& A }} \\
        TREC-Covid & \uline{0.656} & 0.481 & 0.584 & 0.651 & \textbf{0.688}$^{\diamondsuit}$ \\
        FiQA  & 0.236 & 0.300 & 0.308 & \uline{0.309} & \textbf{0.314} \\
        NFCorpus  & \textbf{0.325} & 0.319 & 0.315 & \uline{0.327} & 0.324 \\
        \midrule
        \multicolumn{2}{l}{\textbf{Misc.}} \\
        DBPedia  & 0.313 & 0.384 & 0.381 & \uline{0.405} & \textbf{0.419}$^{\BeBaseSymbol}$ \\
        Touché v2  & \textbf{0.367} & 0.162 & 0.203 & 0.240 & \uline{0.258}$^{\diamondsuit}$ \\
        \bottomrule
    \end{tabular}}
\vspace{-8pt}
\end{table}

\begin{table*}[h]
\caption{Evaluation metrics for the harder set of tasks which include TREC DL-HARD, TREC Tip-of-my-tongue TOT, and FollowIR. Significance is shown at $p < 0.1$. For FollowIR we do not perform significance tests on BM25.} \label{tab:harder}
\scalebox{0.85}{
\begin{tabular}{l!{\color{lightgray}\vrule}ccc!{\color{lightgray}\vrule}ccc!{\color{lightgray}\vrule}cc!{\color{lightgray}\vrule}cc!{\color{lightgray}\vrule}cc}
\toprule
                 & \multicolumn{3}{c!{\color{lightgray}\vrule}}{\textbf{TREC DL-HARD}}       & \multicolumn{3}{c!{\color{lightgray}\vrule}}{\textbf{TREC TOT DEV}}       & \multicolumn{2}{l!{\color{lightgray}\vrule}}{\textbf{FollowIR Robust '04}} & \multicolumn{2}{l!{\color{lightgray}\vrule}}{\textbf{FollowIR News '21}} & \multicolumn{2}{l}{\textbf{FollowIR Core '17}}  \\
Model            & nDCG@10        & RR             & R@1000         & nDCG@10        & RR             & nDCG@1000      & AP             & p-MRR                   & nDCG@5         & p-MRR                 & AP             & p-MRR                 \\
\midrule
\textbf{BM25} $\BMSymbol{}$  & 0.466          & 0.813          & 0.646          & 0.086          & 0.088          & 0.131          & 0.121          & \textbf{-3.1}           & 0.193          & -2.1                  & 0.081          & \textbf{-1.1}                  \\
\textbf{TAS-B} $\TasBSymbol{}$   & 0.574          & 0.789          & 0.777          & 0.097          & 0.089          & 0.162          & 0.203          & -5.4                    & \uline{0.263}  & -0.8                  & 0.170          & -10.0                 \\
\textbf{CL-DRD} $\CLDRDSymbol{}$  & 0.573          & 0.790          & 0.719          & 0.088          & 0.082          & 0.151          & 0.206          & -7.2                    & 0.240          & \uline{-0.3}          & 0.162          & -12.1                 \\
\textbf{BE-Base} $\clubsuit$ & 0.607          & 0.864          & \textbf{0.805}          & \uline{0.121}  & \uline{0.110}  & \uline{0.179}  & \uline{0.207}  & -3.7                    & 0.239          & -1.1                  & \uline{0.178}  & \uline{-7.7}         \\
\midrule
\textbf{\name{}} & \textbf{0.630}$^{\BMSymbol{} \TasBSymbol{} \CLDRDSymbol{}}$ & \textbf{0.887}$^{\TasBSymbol{} \CLDRDSymbol{}}$ & \uline{0.798}$^{\BMSymbol{} \CLDRDSymbol{}}$ & \textbf{0.134}$^{\BMSymbol{} \TasBSymbol{} \CLDRDSymbol{}}$ & \textbf{0.125}$^{\TasBSymbol{} \CLDRDSymbol{}}$ & \textbf{0.182}$^{\CLDRDSymbol{} \BMSymbol{}}$ & \textbf{0.212}$^\TasBSymbol{}$ & -\uline{3.5}            & \textbf{0.272} & \textbf{2.0}        & \textbf{0.193} & -11.8       \\     \bottomrule
\end{tabular}
}
\vspace{-8pt}
\end{table*}

\subsubsection{Results on Harder Retrieval Tasks}
The results on the harder retrieval tasks are in Table~\ref{tab:harder}, like in the out-of-domain section we only consider the main baseline models and BM25. We can see that in the harder tasks \name{} remains dominant over the baseline models with higher retrieval metrics in all but one column. Additionally, the relative improvement compared to the in-domain results are higher (on all metrics that \name{} is the best for) suggesting that on harder tasks the added complexity that can be captured by \name{}'s similarity function is especially important. Additionally the high performance on TREC tip-of-the-tongue (TOT) and FollowIR indicate that \name{} adapts well to different domains through domain-specific fine-tuning.

On the evaluated subset of TREC DL-HARD we see that \name{} has stronger precision metrics than the baselines by a large margin. As mentioned previously the higher relative improvement suggests that \name{} is especially dominant on harder tasks which, in part, explains its higher performance on TREC DL '19 and '20. Though on in-domain dataset \name{} does better or the same on recall metrics, on TREC DL-HARD BE-Base has higher recall than \name{}. We suspect that this may be because the relevance function that the \mininame{} applies is not smooth, which has the benefit of being more discerning and likely accounts for some of the precision gains. However, if the \mininame{} makes a mistake the non-smooth scoring could result in a much harsher score than the linear inner product is capable of producing.

Moving to TREC tip-of-my-tongue (TOT) we see that \name{} continues to perform well. Tip-of-the-tongue is a complex retrieval task with long queries and passages and multiple aspects, the fact \name{} outperforms the baselines by a large margin validates the need for a more complex relevance function.

Finally we have FollowIR which has three subsets -- on all three \name{} has the best performance on the retrieval evaluation metrics of choice, in many cases by a sizable amount. Beside the retrieval evaluation metrics we also include p-MRR which is a metric released in the FollowIR \cite{FollowIR} paper. The metric measures the change in document ranks before and after an instruction is modified to see how well the model responses to the additional requirements. A p-MRR of 0 indicates no change in document rank based on the instruction change and a p-MRR of +100 indicates the documents were perfectly changed based on the instruction while -100 indicates the opposite. For additional details we refer readers to the original FollowIR paper \cite{FollowIR}. As p-MRR is relative to each model's performance before the instructions are modified it is not indicative of stand-alone retrieval performance. With that said, \name{} is the only model to achieve a positive p-MRR indicating it correctly modified the document ranking based on the instruction. This is no small feat as in the original FollowIR paper no retrieval model of the size of \name{} was able to get a positive p-MRR and even many much larger models trained on large instruction retrieval datasets could not get a positive result.






\subsubsection{Analysis of Efficiency} \label{sec:AnalysisOfEfficiency}
To be able to search large-scale collections \name{} has to work well while only doing computation on a small subset of the document corpus. This led us to develop the efficient algorithm in Section~\ref{sec:EfficientSearch}. To quantify the performance we do a set of experiments varying the key parameters to see how each impacts both search quality and query latency. The results of these experiments can be seen in Figure~\ref{fig:approximate-search-tradeoff}. Note the document-to-document graph used has 100 neighbors per document. The figures show that each parameter has an important role in search quality. The largest role is played by $maxIter$ which can drastically reduce search performance if not set high enough, but which plateaus after around 12. The value of $nCandidates$ follows a similar pattern with a drastic increase followed by a plateau. The parameter $\Tilde{C}$, the number of initial candidates, is unique in that it has a more gradual rise and is the only parameter that causes a decrease in effectiveness if raised too high. We suspect this happens because only $nCandidates$ are explored at each iteration but all candidates in $\Tilde{C}$ are considered visited and thus are not explored in the future. This could mean that many good results from the initial candidates are not explored, leaving potentially good neighbors of these nodes undiscovered. Another interesting aspect of $\Tilde{C}$ is that time decreases as it increases when early stopping is used. This is likely because high quality candidates are found sooner allowing search to end more quickly. In general, our approximation seems to behave as expected in terms of both time and retrieval performance.

With the insights from our analysis above we developed two configurations, one which optimizes for speed and one that optimizes for retrieval quality. These two configurations compared against exhaustive search can be seen in Table~\ref{tab:ApproximateSearchResults}. Both approximate configurations significantly decrease the retrieval time when compared to the exhaustive approach.


\begin{table}
\centering
\caption{Average query latency and nDCG@10 on TREC DL '19 and '20 with efficient search. Efficient 1 uses parameters ($\Tilde{C}=10000$, $nCandidates=64$, $maxIter=16$), Efficient 2 uses parameters ($\Tilde{C}=100000$, $nCandidates=328$, $maxIter=20$). All model inference was performed on an NVIDIA L40S with BF16 precision.} \label{tab:ApproximateSearchResults}
\scalebox{0.79}{
\begin{tabular}{l!{{\color{lightgray}\vrule}}c!{\color{lightgray}\vrule}cc}
\toprule
Search Type & Query Lat. (ms)  & DL '19 & DL '20  \\
\midrule
Exhaustive & 1769.8 &  0.742 & 0.731                                    \\
Efficient 1 & 59.6  & 0.702 & 0.730                                  \\
Efficient 2 & 231.1 & 0.722  & 0.731                        \\
\bottomrule
\end{tabular}
}
\vspace{-8pt}
\end{table}





\subsubsection{Impact of \mininame{} Depth}
\name{} performance suggests that having a more complex relevance function does indeed help improve retrieval performance as we had hypothesized. This raises the question: what is the optimal complexity of this relevance function. To answer this question we trained four versions of \name{} each trained to produce a \mininame{} with a different number of layers. We selected \textit{[2, 4, 6, 8]} as the number of \mininame{} layers.

The results can be seen in Figure~\ref{fig:MiniModelLayers}. The experiment shows that there is a benefit beyond two layers and that at least four is required for the best performance. Performance stays the same with six, indicating that this might be the point of diminishing returns. Lastly, eight layers decrease performance. There could be a number of reasons for this including that eight layers are harder to optimize or because effectively learning how to use all eight layers takes longer and thus might achieve better performance with extended training time.


\begin{figure}
    \centering
    \begin{tikzpicture}
    \begin{axis}[
        title={},
        width=0.48\textwidth,
        height=3cm,
        xlabel={Number of \mininame{} layers},
        ylabel={nDCG@10},
        xmin=1.5, xmax=8.5,
        ymin=0.72, ymax=0.74,
        xtick={0,2,4,6,8},
        % ytick={0.7,0.71,0.72,0.73,0.74,0.75},
        legend pos=south east,
        ymajorgrids=true,
        grid style=dashed,
        ylabel near ticks,
        xlabel near ticks,
        xtick pos=left,
        ytick pos=left,
    ]

    \addplot[color=graph_color_1, mark=o, line width=1.0pt]
        coordinates {
        (2.0,0.732)(4,0.736)(6,0.736)(8,0.724)
        };

    \end{axis}
    \end{tikzpicture}
    \caption{Average nDCG@10 on TREC DL '19 and '20 versus number of layers in the \mininame{}.} \label{fig:MiniModelLayers}
    \label{fig:layer_versus_ndcg}
\vspace{-10pt}
\end{figure}


\section{Conclusion}
We propose a new class of retrieval model, the \name{} which overcomes the limitations of inner product based similarity functions that we prove to exist. Our model achieves a new state-of-the-art on TREC DL '19 and '20 for BERT sized encoder models with a single dense document vector and shows even stronger relative improvement on harder retrieval tasks such as tip-of-the-tongue queries. Further we demonstrate that learned relevance models can be applied to large-scale search corpus in an efficient way with our proposed approximate search algorithm. As \name{} is a flexible framework there is much interesting future work to explore, such as multi-vector document representations and corpus pretraining to name a few.

\begin{acks}
    This work was supported in part by the Center for Intelligent Information Retrieval, in part by the NSF Graduate Research Fellowships Program (GRFP) Award \#1938059, and in part by the Office of Naval Research contract number N000142412612. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{XX-references}

\end{document}
