\section{Related Work}
\paragraph{\textbf{Vector Space Models}}

Vector-based models that use sparse vectors have existed for decades, with each index representing a term in the corpus vocabulary. Document-query similarity is computed using measures like $l_2$ distance, inner product, or cosine similarity, with term weighting methods such as TF-IDF being a substantial focus to improve performance. With the emergence of deep neural networks, focus shifted to learning representations for queries and documents. SNRM by ____ was the first deep learning model to retrieve documents from large corpora by learning latent sparse vectors. Following works leveraged pretrained transformer models like BERT ____ using single dense vector representations ____. Recent improvements have focused on training techniques including self-negative mining ____, data augmentation ____, distillation ____, corpus-pretraining ____, negative-batch construction ____ and curriculum learning ____. Alternative approaches include ColBERT ____, which uses multiple dense vectors, and SPLADE ____, which revisits sparse representations using pretrained masked language models.

Though these methods vary substantially, they all share a fundamental commonality, that relevance is based on an inner product (or in some cases cosine similarity). We believe that this is a significant limitation of these methods and one which hampers the performance of these models on complex retrieval tasks. Our method circumvents this limitation by learning a query-dependent small neural network that is fast enough to run on the entire collection (or used in an approximate way; see Section \ref{sec:EfficientSearch} for details).

\paragraph{\textbf{Learned Relevance Models}}

Light-weight relevance models using neural networks have demonstrated improved retrieval performance compared to simple methods like inner products. Early iterations came in the form of learning-to-rank models ____ which use query and document features to produce relevance scores for reranking. While these models traditionally used engineered features, more recent approaches adopted richer inputs. For instance, MatchPyramid ____ and KNRM ____ use similarity matrices between non-contextualized word embeddings, while Duet ____ combines sparse and dense term features in a multi-layer perceptron. DRMM ____ utilized histogram features as input to neural networks for scoring. Since the advent of BERT ____, focus has shifted to making transformer models more efficient, such as PreTTR ____ which separately precomputes query and document hidden states. Recently, LITE ____ extended ColBERT's similarity using column-wise and row-wise linear layers for scoring.

In the recommender systems community, learned similarity measures have been widely used ____. The common usage of neural scoring methods in recommendation has inspired research into efficient retrieval with more learned scoring signals. For instance, BFSG ____ supports efficient retrieval with arbitrary relevance functions by using a graph of item representations and a greedy search strategy over nodes of the graph. A recent improvement on BFSG uses the scoring models gradient to prune directions that are unlikely to have relevant items ____. Other works make use of queries to form a query-item graph to produce more informative neighbors ____.

Our work differs from these works in one major way, we do not have a query representation and document representation thus our method requires no combination step, instead we produce a query-conditioned neural network for each query and directly apply this to the document representation. This approach can reduce the similarity network's size and does not require choosing between inference speed and larger query representations. Furthermore the flexibility of our framework means we can replicate any existing learned relevance model as discussed in Section~\ref{sec:ComparisonExistingNeuralScoring}. On a broader note there has been surprisingly little work on neural based scoring for full-scale retrieval, especially in the modern era of transformer based encoders. We hope our work can be a useful foundation and proof-of-concept for future work in this area.

\paragraph{\textbf{Hypernetworks}}
Hypernetworks also known as hypernets are neural networks which produce the weights for other neural networks. The term was first used by ____ who demonstrated the effectiveness of hypernetworks to generate weights for LSTM networks. Since then, hypernetworks have been used in a variety of ways including neural architecture search ____, continual learning ____, and few-shot learning ____ to name a few. Generally, hypernetworks take a set of input embeddings that provide information about the type of task or network where the weights will be used. These embeddings are then projected to the significantly larger dimension of the weights of the ``main'' network. As the outputs of most hypernetworks are so large the hypernetworks themselves are often very simple such as a few feed-forward layers in order to keep computation feasible. Our case is unique in that our hypernetwork, the \name{}, is much larger than the small scoring network which we call \mininame{} (i.e. the ``main'' network). Additionally, to the best of our knowledge, this paper represents the first work to explore hypernetworks for first stage retrieval.