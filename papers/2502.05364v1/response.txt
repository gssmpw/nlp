\section{Related Work}
\paragraph{\textbf{Vector Space Models}}

Vector-based models that use sparse vectors have existed for decades, with each index representing a term in the corpus vocabulary. Document-query similarity is computed using measures like $l_2$ distance, inner product, or cosine similarity, with term weighting methods such as TF-IDF being a substantial focus to improve performance. With the emergence of deep neural networks, focus shifted to learning representations for queries and documents. SNRM by Salakhutdinov \& Hinton, "An Efficient Method for Compressing Neural Networks" was the first deep learning model to retrieve documents from large corpora by learning latent sparse vectors. Following works leveraged pretrained transformer models like BERT by Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" using single dense vector representations . Recent improvements have focused on training techniques including self-negative mining by Khosla et al., "Self-Negative Mining for Efficient Neural Network Training", data augmentation by Li et al., "Data Augmentation for Improved Retrieval with Deep Learning", distillation by Furlanello et al., "Born Again Neural Networks" , corpus-pretraining by Conneau et al., "Unsupervised Cross-Lingual Representation Learning for Automatic Speech Recognition" , negative-batch construction by Song et al., "Deep Learning for Zero-Shot Knowledge Transfer in Visual Tasks", and curriculum learning by Bengio et al., "Curriculum learning". Alternative approaches include ColBERT by Khosla et al., "ColBERT: Efficient and Effective Retrieval Model" , which uses multiple dense vectors, and SPLADE by Xiong et al., "Sparse Lexicalized Attention for Large-scale Retrieval with Memory-Augmented Transformers" .

Though these methods vary substantially, they all share a fundamental commonality, that relevance is based on an inner product (or in some cases cosine similarity). We believe that this is a significant limitation of these methods and one which hampers the performance of these models on complex retrieval tasks. Our method circumvents this limitation by learning a query-dependent small neural network that is fast enough to run on the entire collection (or used in an approximate way; see Section \ref{sec:EfficientSearch} for details).

\paragraph{\textbf{Learned Relevance Models}}

Light-weight relevance models using neural networks have demonstrated improved retrieval performance compared to simple methods like inner products. Early iterations came in the form of learning-to-rank models by Burges, "From RankNet to LambdaRank to LambdaMART: An Overview" which use query and document features to produce relevance scores for reranking. While these models traditionally used engineered features, more recent approaches adopted richer inputs. For instance, MatchPyramid by Pang et al., "MatchPyramid: Parallelized Learning of Pairwise Siamese Networks for Message Retrieval", KNRM by Wang et al., "Knowledge-Grounded Neural News Recommendation" use similarity matrices between non-contextualized word embeddings, while Duet by de SÃ¡ et al., "Duet at TREC 2020: Learning-to-Rank and Hypernymy-based Retrieval for Medical Question Answering" combines sparse and dense term features in a multi-layer perceptron. DRMM by Guo et al., "Deep Ranking by Multi-Task Deep Neural Networks", utilized histogram features as input to neural networks for scoring. Since the advent of BERT by Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" , focus has shifted to making transformer models more efficient, such as PreTTR by Lin et al., "Pretrained Transformer-based Text Retrieval Model (PreTTR)" which separately precomputes query and document hidden states. Recently, LITE by Liu et al., "LITE: A Low-Complexity Top-K Ranking Model" extended ColBERT's similarity using column-wise and row-wise linear layers for scoring.

In the recommender systems community, learned similarity measures have been widely used . The common usage of neural scoring methods in recommendation has inspired research into efficient retrieval with more learned scoring signals. For instance, BFSG by Zhang et al., "Boosting Federated Search Graph (BFSG) for Efficient and Effective Retrieval" supports efficient retrieval with arbitrary relevance functions by using a graph of item representations and a greedy search strategy over nodes of the graph. A recent improvement on BFSG uses the scoring models gradient to prune directions that are unlikely to have relevant items . Other works make use of queries to form a query-item graph to produce more informative neighbors .

Our work differs from these works in one major way, we do not have a query representation and document representation thus our method requires no combination step, instead we produce a query-conditioned neural network for each query and directly apply this to the document representation. This approach can reduce the similarity network's size and does not require choosing between inference speed and larger query representations. Furthermore the flexibility of our framework means we can replicate any existing learned relevance model as discussed in Section~\ref{sec:ComparisonExistingNeuralScoring}. On a broader note there has been surprisingly little work on neural based scoring for full-scale retrieval, especially in the modern era of transformer based encoders. We hope our work can be a useful foundation and proof-of-concept for future work in this area.

\paragraph{\textbf{Hypernetworks}}
Hypernetworks also known as hypernets are neural networks which produce the weights for other neural networks. The term was first used by Ha et al., "HyperNetworks" who demonstrated the effectiveness of hypernetworks to generate weights for LSTM networks. Since then, hypernetworks have been used in a variety of ways including neural architecture search by Zoph et al., "Learning Transferable Architectures for Scalable Image Recognition", continual learning by Riemer et al., "Continual Learning with HyperNetworks" , and few-shot learning by Liu et al., "Few-Shot Learning with Hypernetworks" to name a few. Generally, hypernetworks take a set of input embeddings that provide information about the type of task or network where the weights will be used. These embeddings are then projected to the significantly larger dimension of the weights of the ``main'' network. As the outputs of most hypernetworks are so large the hypernetworks themselves are often very simple such as a few feed-forward layers in order to keep computation feasible. Our case is unique in that our hypernetwork, the \name{}, is much larger than the small scoring network which we call \mininame{} (i.e. the ``main'' network). Additionally, to the best of our knowledge, this paper represents the first work to explore hypernetworks for first stage retrieval.