\section{Related Work}
\paragraph{\textbf{Vector Space Models}}

Vector-based models that use sparse vectors have existed for decades, with each index representing a term in the corpus vocabulary. Document-query similarity is computed using measures like $l_2$ distance, inner product, or cosine similarity, with term weighting methods such as TF-IDF being a substantial focus to improve performance. With the emergence of deep neural networks, focus shifted to learning representations for queries and documents. SNRM by \citet{SNRM} was the first deep learning model to retrieve documents from large corpora by learning latent sparse vectors. Following works leveraged pretrained transformer models like BERT \cite{BERT} using single dense vector representations \cite{DPR}. Recent improvements have focused on training techniques including self-negative mining \cite{ANCE, RocketQA, RANCE, adore}, data augmentation \cite{RocketQA, DRAGON}, distillation \cite{MarginMSE, DRAGON, TCT-ColBERT}, corpus-pretraining \cite{CoCondensor, Contriever, RetroMAE,b-prop}, negative-batch construction \cite{TAS-B} and curriculum learning \cite{CL-DRD,prod}. Alternative approaches include ColBERT \cite{ColBERT_v1}, which uses multiple dense vectors, and SPLADE \cite{SPLADE}, which revisits sparse representations using pretrained masked language models.

Though these methods vary substantially, they all share a fundamental commonality, that relevance is based on an inner product (or in some cases cosine similarity). We believe that this is a significant limitation of these methods and one which hampers the performance of these models on complex retrieval tasks. Our method circumvents this limitation by learning a query-dependent small neural network that is fast enough to run on the entire collection (or used in an approximate way; see Section \ref{sec:EfficientSearch} for details).

\paragraph{\textbf{Learned Relevance Models}}

Light-weight relevance models using neural networks have demonstrated improved retrieval performance compared to simple methods like inner products. Early iterations came in the form of learning-to-rank models \cite{RankNet, LearningToRankNonSmooth} which use query and document features to produce relevance scores for reranking. While these models traditionally used engineered features, more recent approaches adopted richer inputs. For instance, MatchPyramid \cite{MatchPyramid} and KNRM \cite{KNRM} use similarity matrices between non-contextualized word embeddings, while Duet \cite{Duetv1, DuetV2} combines sparse and dense term features in a multi-layer perceptron. DRMM \cite{DRMM} utilized histogram features as input to neural networks for scoring. Since the advent of BERT \cite{BERT}, focus has shifted to making transformer models more efficient, such as PreTTR \cite{PreTTR} which separately precomputes query and document hidden states. Recently, LITE \cite{LITE} extended ColBERT's similarity using column-wise and row-wise linear layers for scoring.

In the recommender systems community, learned similarity measures have been widely used \cite{LearnedCollaborativeFiltering, NeuralFactorizationMachines}. The common usage of neural scoring methods in recommendation has inspired research into efficient retrieval with more learned scoring signals. For instance, BFSG \cite{NeuralNetworkFastItemRanking} supports efficient retrieval with arbitrary relevance functions by using a graph of item representations and a greedy search strategy over nodes of the graph. A recent improvement on BFSG uses the scoring models gradient to prune directions that are unlikely to have relevant items \cite{GradientPruningTowardFastNeuralRanking}. Other works make use of queries to form a query-item graph to produce more informative neighbors \cite{FastNeuralRankingOnBipartiteGraphIndices}.

Our work differs from these works in one major way, we do not have a query representation and document representation thus our method requires no combination step, instead we produce a query-conditioned neural network for each query and directly apply this to the document representation. This approach can reduce the similarity network's size and does not require choosing between inference speed and larger query representations. Furthermore the flexibility of our framework means we can replicate any existing learned relevance model as discussed in Section~\ref{sec:ComparisonExistingNeuralScoring}. On a broader note there has been surprisingly little work on neural based scoring for full-scale retrieval, especially in the modern era of transformer based encoders. We hope our work can be a useful foundation and proof-of-concept for future work in this area.

\paragraph{\textbf{Hypernetworks}}
Hypernetworks also known as hypernets are neural networks which produce the weights for other neural networks. The term was first used by \citet{HyperNetworks} who demonstrated the effectiveness of hypernetworks to generate weights for LSTM networks. Since then, hypernetworks have been used in a variety of ways including neural architecture search \cite{GraphHyperNetworksNeuralArchitectureSearch}, continual learning \cite{ContinualLearningWithHypernetworks}, and few-shot learning \cite{MetaLearningWithLatentEmbeddingOptimization, HyperShot} to name a few. Generally, hypernetworks take a set of input embeddings that provide information about the type of task or network where the weights will be used. These embeddings are then projected to the significantly larger dimension of the weights of the ``main'' network. As the outputs of most hypernetworks are so large the hypernetworks themselves are often very simple such as a few feed-forward layers in order to keep computation feasible. Our case is unique in that our hypernetwork, the \name{}, is much larger than the small scoring network which we call \mininame{} (i.e. the ``main'' network). Additionally, to the best of our knowledge, this paper represents the first work to explore hypernetworks for first stage retrieval.