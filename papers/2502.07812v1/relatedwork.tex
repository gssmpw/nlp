\section{Related Work}
\label{sec:relatedwork}


\subsection{Single Image Dehazing}
\label{subsec:singleimagedehazing}


\textbf{Prior-based Dehazing.} Early image dehazing algorithms heavily rely on hand-crafted priors, which leverage statistical observations about the hazy image formation process to estimate the transmission map and atmospheric light. Dark Channel Prior (DCP) \cite{he2010single}, a well-known representative of traditional method, performs dehazing based on an assumption that the minimum intensity value in a local region of a haze-free image is close to zero in at least one color channel. Color Attenuation Prior (CAP) \cite{zhu2015fast} creates a linear model for modeling the scene depth of the hazy image and learns the parameters of the model via supervised learning. Non-Local Image Dehazing (NLID) \cite{berman2016non} is developed based on an observation that the hazy pixels in the RGB color space are distributed along lines passing through the airlight. Recently, Regional Saturation-Value Translation (RSVT) \cite{tran2024single} is introduced based on statistical analyses of the correlation between hazy points and respective haze-free points in the HSV color space. Although these approaches have yielded impressive dehazing results in controlled environments, they often struggle with scenes containing heavy haze where their simplified assumptions are violated, leading to artifacts and color distortions and hindering their adaptability to real-world scenarios.


\textbf{Learning-based Dehazing.} With notable advancements of deep learning across various vision tasks, many learning-based dehazing methods have been introduced in the literature in the past years. Early efforts such as \cite{cai2016dehazenet,ren2016single,zhang2018densely} have typically focused on predicting transmission map, then the haze-free image is recovered using the atmospheric scattering model. Although these methods laid the foundations for learning-based dehazing, transmission map estimation requires an accurate physical model, which may not be applicable in all scenarios.

To overcome that challenge, some approaches have tried to directly learn the relationship between hazy and haze-free images in an end-to-end manner by using convolutional neural networks (CNNs) \cite{qin2020ffa}, generative adversarial networks (GANs) \cite{dong2020fd,tran2024encoder}, or knowledge distillation \cite{hong2020distilling,tran2024lightweight}. These methods have achieved significant success and effectively provided visually appealing results by training on large-scale datasets of hazy-clean image pairs via supervised learning. Notwithstanding, acquiring such datasets is often challenging and impractical. In addition, supervised methods may struggle to generalize well in unseen environments, especially if the model is overfitted to a specific haze scenario presented in the training data. Recently, these issues have been tackled by several semi-supervised learning studies \cite{li2019semi,an2022semi}, which often combine a small set of paired images with a larger set of unpaired data. While semi-supervised methods present promising solutions and can alleviate domain shift problems to some extent, their performance still depends on the quality and quantity of synthetic data, and they may struggle to generalize effectively to non-ideal conditions, as they rely heavily on the assumptions made by the unpaired data.

Unsupervised learning methods are particularly attractive in scenarios where paired data is unobtainable, such as in the context of image dehazing, making them more versatile in practical scenarios. Some unsupervised dehazing methods, such as \cite{engin2018cycle,li2021you,zhao2021refinednet}, can be easily adapted to various environmental conditions without requiring retraining, since they are not overly dependent on specific haze types. While supervised learning can yield significant performance when environmental constraints are given, unsupervised approaches can offer better generalizability. 


\subsection{Vision Transformers}
\label{subsec:vit}

In recent years, the breakthrough of an emerging neural network class, Vision Transformers (ViTs) \cite{dosovitskiy2020image}, has opened up a new research direction for neural architecture design. The abstracted architecture of ViTs has been found to play a significant role in delivering superior performance \cite{yu2022metaformer}. In a general ViT block, an input is first embedded into a sequence of features (or tokens), before being passed through two sub-blocks, roughly termed $\mathrm{TokenMix}(.)$, a token mixing operation which propagates information among tokens, and $\mathrm{ChannelMix}(.)$, which represents an MLP module to fuse features among channels. Various studies \cite{guo2022image,song2023vision,tran2024distilled} have also investigated ViT-based architectures for image dehazing and achieved promising results. However, these works have primarily focused on using ViTs for supervised dehazing, leaving their potential in unsupervised dehazing largely unexplored.


% Figure 
\begin{figure*}
    \centering
    \includegraphics[width=1.00\textwidth]{figures/Fig_UIDKAT.pdf}
    \caption{The proposed UID-KAT framework.}
    \label{fig:framework} 
\end{figure*}


\subsection{Contrastive Learning}
\label{subsec:contrastivelearning}

Contrastive learning has emerged as a powerful self-supervised learning technique, leveraging the idea of contrasting positive samples against negative ones. Contrastive learning can encourage the model to learn generalizable and meaningful representations of data in scenarios where acquiring paired data is challenging, which is particularly useful in the context of image dehazing. Inspired by this, several works such as \cite{wu2021contrastive,wang2024ucl} have examined contrastive learning for image dehazing and achieved promising outcomes. Particularly, another work \cite{wang2023uscformer} even incorporates contrastive learning and ViTs in an supervised manner to obtain impressive results. However, applying contrastive learning to image dehazing still presents challenges as it can be difficult to achieve effective positive/negative pair selection for successful training, especially in unsupervised settings. Albeit these difficulties, the potential of contrastive learning to advance unsupervised image dehazing methods remains significant, particularly in its ability to learn robust features that can aid in haze removal even when paired data is unavailable.


\subsection{Kolmogorov-Arnold Networks (KANs)}
\label{subsec:kan}

The Kolmogorov-Arnold representation theorem \cite{schmidt2021kolmogorov}, established by Vladimir Arnold and Andrey Kolmogorov, states that any multivariate continuous function $f$ on a bounded domain can be expressed as a finite composition of continuous functions of a single variable and the binary operation of addition \cite{liu2024kan}. Specifically, for a smooth $f: [0,1]^n \to \mathbb{R}$:
\begin{equation}
f(\mathbf{x}) = f(x_1, \ldots, x_n) = \sum_{q=1}^{2n+1} \Phi_q\left(\sum_{p=1}^{n} \phi_{q,p}(x_p)\right),
\end{equation}
where $\phi_{q,p}: [0,1] \to \mathbb{R}$ and $\Phi_q: \mathbb{R} \to \mathbb{R}$. The theorem can be written in matrix form as follows:
\begin{equation}
    f(\mathbf{x}) = \Phi_{\text{out}} \circ \Phi_{\text{in}} \circ \mathbf{x} \label{eq:kan_vector},
\end{equation}
where $\Phi_{\text{in}}$ and $\Phi_{\text{out}}$ are defined as:
\begin{equation}
    \Phi_{\text{in}} = \begin{bmatrix}
\phi_{1,1}(\cdot) & \cdots & \phi_{1,n}(\cdot) \\
\vdots & \ddots & \vdots \\
\phi_{2n+1,1}(\cdot) & \cdots & \phi_{2n+1,n}(\cdot)
\end{bmatrix},
\end{equation}
\begin{equation}
    \Phi_{\text{out}} = \begin{bmatrix}
\Phi_1(\cdot) & \cdots & \Phi_{2n+1}(\cdot)
\end{bmatrix}.
\end{equation}
This breakdown shows how $f$ can be constructed from simpler functions, highlighting a key characteristic of multivariate continuous functions. Inspired by this, a generalized Kolmogorov-Arnold layer with $d_\text{in}$-dimensional inputs and $d_\text{out}$-dimensional outputs is proposed in \cite{yang2024kolmogorov} as an activation function form to learn univariate functions on edge, which is illustrated as:
\begin{equation}
    f(\mathbf{x}) = \Phi \circ \mathbf{x} = \begin{bmatrix}\sum_{i=1}^{d_{in}} \phi_{i,1}(x_i) & \dots& \sum_{i=1}^{d_{in}} \phi_{i,d_{out}}(x_i)\end{bmatrix}, \label{eq:kan_layer}
\end{equation}
where
\begin{equation}
\Phi = \begin{bmatrix}
\phi_{1,1}(\cdot) & \cdots & \phi_{1,d_{\text{in}}}(\cdot) \\
\vdots & \ddots & \vdots \\
\phi_{d_{\text{out}},1}(\cdot) & \cdots & \phi_{d_{\text{out}},d_{\text{in}}}(\cdot)
\end{bmatrix}.
\end{equation}
Note that Eq. (\ref{eq:kan_layer}) can be considered a generalized form of Eq. (\ref{eq:kan_vector}), such that $\Phi = \Phi_{\text{in}}\circ \Phi_{\text{out}}$. In \cite{liu2024kan}, $\Phi$ is parameterized by using a linear combination of SiLU activation \cite{elfwing2018sigmoid} and a B-spline function:
\begin{equation}
        \phi(x) = w_b \mathrm{silu}(x) + w_s \mathrm{spline}(x),
\end{equation}
where
\begin{equation}
\mathrm{silu}(x) = \frac{x}{1+e^{-x}}, 
\end{equation}
\begin{equation}
\mathrm{spline}(x)=\sum_i c_i B_i(x).
\end{equation}
The adaptive activation function nature enables KANs to effectively approximate non-linear mappings with fewer parameters and improved interpretability. Following \cite{liu2024kan}, various subsequent studies have also investigated the promise of KANs in different aspects. Notable research works on KANs can be witnessed in KAT \cite{yang2024kolmogorov}, which integrates KAN layer into the ViT architecture, or KAN-CUT \cite{mahara2024dawn}, which represents the initial attempt of applying KANs to image generation. Despite their promising capabilities, KANs remain in their early stages, with limited research investigating their applications across diverse domains. One contributing factor may be the scalability constraints of KANs \cite{yang2024kolmogorov}. Nevertheless, the potential of KANs to model complex relationships in image data has not yet been fully explored, positioning a promising approach for numerous vision-related challenges, particularly in image restoration tasks such as image dehazing.


% ====================================================================