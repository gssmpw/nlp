\section{Related Works}
We briefly review the closely related text-to-image generation models, generating concepts and applying concepts.

\textbf{Text-to-image Generation Models.} AI-generated content (AIGC) ____, ____, ____, ____ has experienced significant expansion in recent years. Image generation has evolved from the pursuit of high resolution and more realism to a more flexible and broadly applicable to a variety of tasks, which allows for customized generation to fit user needs. GAN-based (Generative Adversarial Network ____) generation models have previously achieved better generation quality, and the images generated by GAN are usually highly realistic and lifelike ____. GAN-INT-CLS ____ was the first to realize GAN-based text-to-image generation, and many subsequent works ____, ____, ____ have demonstrated its powerful generative capabilities. However, GAN faces limitations such as poor performance during complex scenes and unstable training.

With diffusion probabilistic model (DM) ____, denoising diffusion probabilistic model (DDPM) ____, latent diffusion model (LDM) ____ and further development to SD, diffusion models have received much attention in image generation due to their powerful capabilities ____. An OpenAI study shows that diffusion models beat GANs in image synthesis ____. VQ-Diffusion ____ is a T2I generation model based on a vector quantized variational autoencoder (VQ-VAE) with a latent space modeled by a conditional variant of DDPM, which can handle more complex scenes and dramatically improve image quality. Imagen ____ achieves remarkable photorealism and a profound level of language understanding, with unprecedented realism and ensures precise text alignment. InstaFlow ____ is a proposed novel T2I generation model to turn SD into an ultra-fast text-conditioned pipeline. A recent research work HIVE ____ attempts to combine T2I modeling with human feedback to control the output of generated images through editorial commands. Further, a comprehensive benchmark for open-world compositional T2I generation, T2I-CompBench ____, has been proposed.

\textbf{Generating Concepts.} Generating concepts is often described as a process where multiple images of shared concepts are used for concept extraction through well-designed models, such as Textual Inversion ____, DreamBooth ____ and Lora-DreamBooth ____. It is considered as an extension of the T2I generation task. The generative concepts technique is first directed toward extracting the concept of an entity ____, e.g., a person, an automobile, a ragdoll toy, and so on. Cones ____, Break-A-Scene ____ and Custom Diffusion ____ implement multi-concept extraction. CatVersion ____ learns the personalization concept by concatenating embeddings on the feature-dense space of a text encoder in the diffusion model. For the extraction of style concepts, StyleDrop ____ is a method that faithfully follows a specific style and effectively learns new ones. InST ____ perceives styles as learnable text descriptions of paintings, which can efficiently and accurately learn key artistic style information about an image. Specialist Diffusion ____ is a plug-and-play framework for style concept extraction with further performance improvements.

\textbf{Applying Concepts.} Those working on how to better extract concepts usually demonstrate the excellent generative power of their methods. CLiC ____ performs contextual concept learning on the concept within the inclusion mask and the surrounding image region to acquire local visual concept. Subject-Diffusion ____ is a novel open-domain personalized image generation model that requires only a single reference image to support single-subject or multi-subject personalization generation with no need for test-time fine-tuning. Moreover, one kind of application phase of concepts can be summarized as concept assisting. Anti-DreamBooth ____ destroys the generation quality of any DreamBooth model trained against malicious use by adding perturbations to the images. Concept censorship ____ proposes to inject backdoors into Textual Inversion embeddings and select some sensitive words as triggers during training so that the models do not generate malicious images.