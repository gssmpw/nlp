\section{Related Works}
We briefly review the closely related text-to-image generation models, generating concepts and applying concepts.

\textbf{Text-to-image Generation Models.} AI-generated content (AIGC) \cite{cao2023comprehensive}, \cite{xu2024unleashing}, \cite{wang2023security}, \cite{10230895} has experienced significant expansion in recent years. Image generation has evolved from the pursuit of high resolution and more realism to a more flexible and broadly applicable to a variety of tasks, which allows for customized generation to fit user needs. GAN-based (Generative Adversarial Network \cite{goodfellow2014generative}) generation models have previously achieved better generation quality, and the images generated by GAN are usually highly realistic and lifelike \cite{10.1145/3439723}. GAN-INT-CLS \cite{reed2016generative} was the first to realize GAN-based text-to-image generation, and many subsequent works \cite{Cheng9156682}, \cite{Huang2021Unifying}, \cite{Ruan9710042} have demonstrated its powerful generative capabilities. However, GAN faces limitations such as poor performance during complex scenes and unstable training.

With diffusion probabilistic model (DM) \cite{pmlr-v37-sohl-dickstein15}, denoising diffusion probabilistic model (DDPM) \cite{NEURIPS2020_4c5bcfec}, latent diffusion model (LDM) \cite{sdpaper} and further development to SD, diffusion models have received much attention in image generation due to their powerful capabilities \cite{Croitoru10081412, Yang2023Diffusion}. An OpenAI study shows that diffusion models beat GANs in image synthesis \cite{NEURIPS2021_49ad23d1}. VQ-Diffusion \cite{Gu9879180} is a T2I generation model based on a vector quantized variational autoencoder (VQ-VAE) with a latent space modeled by a conditional variant of DDPM, which can handle more complex scenes and dramatically improve image quality. Imagen \cite{NEURIPS2022_ec795aea} achieves remarkable photorealism and a profound level of language understanding, with unprecedented realism and ensures precise text alignment. InstaFlow \cite{liu2024instaflow} is a proposed novel T2I generation model to turn SD into an ultra-fast text-conditioned pipeline. A recent research work HIVE \cite{zhang2023hive} attempts to combine T2I modeling with human feedback to control the output of generated images through editorial commands. Further, a comprehensive benchmark for open-world compositional T2I generation, T2I-CompBench \cite{NEURIPS2023_f8ad010c}, has been proposed.

\textbf{Generating Concepts.} Generating concepts is often described as a process where multiple images of shared concepts are used for concept extraction through well-designed models, such as Textual Inversion \cite{gal2022image}, DreamBooth \cite{Ruiz_2023_CVPR} and Lora-DreamBooth \cite{hu2022lora}. It is considered as an extension of the T2I generation task. The generative concepts technique is first directed toward extracting the concept of an entity \cite{10.1145/3618315}, e.g., a person, an automobile, a ragdoll toy, and so on. Cones \cite{liu2023cones, liu2023cones2}, Break-A-Scene \cite{Avrahami2023Break} and Custom Diffusion \cite{Kumari_2023_CVPR} implement multi-concept extraction. CatVersion \cite{zhao2023catversion} learns the personalization concept by concatenating embeddings on the feature-dense space of a text encoder in the diffusion model. For the extraction of style concepts, StyleDrop \cite{NEURIPS2023_d33b177b} is a method that faithfully follows a specific style and effectively learns new ones. InST \cite{Zhang_2023_CVPR} perceives styles as learnable text descriptions of paintings, which can efficiently and accurately learn key artistic style information about an image. Specialist Diffusion \cite{Lu_2023_CVPR} is a plug-and-play framework for style concept extraction with further performance improvements.

\textbf{Applying Concepts.} Those working on how to better extract concepts usually demonstrate the excellent generative power of their methods. CLiC \cite{safaee2023clic} performs contextual concept learning on the concept within the inclusion mask and the surrounding image region to acquire local visual concept. Subject-Diffusion \cite{ma2023subject} is a novel open-domain personalized image generation model that requires only a single reference image to support single-subject or multi-subject personalization generation with no need for test-time fine-tuning. Moreover, one kind of application phase of concepts can be summarized as concept assisting. Anti-DreamBooth \cite{Van_Le_2023_ICCV} destroys the generation quality of any DreamBooth model trained against malicious use by adding perturbations to the images. Concept censorship \cite{zhang2023backdooring} proposes to inject backdoors into Textual Inversion embeddings and select some sensitive words as triggers during training so that the models do not generate malicious images.