@inproceedings{chen2020talking,
  title={Talking-head generation with rhythmic head motion},
  author={Chen, Lele and Cui, Guofeng and Liu, Celong and Li, Zhong and Kou, Ziyi and Xu, Yi and Xu, Chenliang},
  booktitle={European Conference on Computer Vision},
  pages={35--51},
  year={2020},
  organization={Springer}
}

@article{wang2021audio2head,
  title={Audio2head: Audio-driven one-shot talking-head generation with natural head motion},
  author={Wang, Suzhen and Li, Lincheng and Ding, Yu and Fan, Changjie and Yu, Xin},
  journal={arXiv preprint arXiv:2107.09293},
  year={2021}
}

@inproceedings{ng2022learning,
  title={Learning to listen: Modeling non-deterministic dyadic facial motion},
  author={Ng, Evonne and Joo, Hanbyul and Hu, Liwen and Li, Hao and Darrell, Trevor and Kanazawa, Angjoo and Ginosar, Shiry},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20395--20405},
  year={2022}
}

@inproceedings{zhou2022responsive,
  title={Responsive listening head generation: a benchmark dataset and baseline},
  author={Zhou, Mohan and Bai, Yalong and Zhang, Wei and Yao, Ting and Zhao, Tiejun and Mei, Tao},
  booktitle={European Conference on Computer Vision},
  pages={124--142},
  year={2022},
  organization={Springer}
}

@inproceedings{ng2023can,
  title={Can Language Models Learn to Listen?},
  author={Ng, Evonne and Subramanian, Sanjay and Klein, Dan and Kanazawa, Angjoo and Darrell, Trevor and Ginosar, Shiry},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10083--10093},
  year={2023}
}

@inproceedings{song2023emotional,
  title={Emotional listener portrait: Neural listener head generation with emotion},
  author={Song, Luchuan and Yin, Guojun and Jin, Zhenchao and Dong, Xiaoyi and Xu, Chenliang},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={20839--20849},
  year={2023}
}

@inproceedings{gu2022vector,
  title={Vector quantized diffusion model for text-to-image synthesis},
  author={Gu, Shuyang and Chen, Dong and Bao, Jianmin and Wen, Fang and Zhang, Bo and Chen, Dongdong and Yuan, Lu and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10696--10706},
  year={2022}
}

@article{van2017neural,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{zollhofer2018state,
  title={State of the art on monocular 3D face reconstruction, tracking, and applications},
  author={Zollh{\"o}fer, Michael and Thies, Justus and Garrido, Pablo and Bradley, Derek and Beeler, Thabo and P{\'e}rez, Patrick and Stamminger, Marc and Nie{\ss}ner, Matthias and Theobalt, Christian},
  booktitle={Computer graphics forum},
  volume={37},
  number={2},
  pages={523--550},
  year={2018},
  organization={Wiley Online Library}
}
@incollection{blanz2023morphable,
  title={A morphable model for the synthesis of 3D faces},
  author={Blanz, Volker and Vetter, Thomas},
  booktitle={Seminal Graphics Papers: Pushing the Boundaries, Volume 2},
  pages={157--164},
  year={2023}
}
@inproceedings{kittler20163d,
  title={3D morphable face models and their applications},
  author={Kittler, Josef and Huber, Patrik and Feng, Zhen-Hua and Hu, Guosheng and Christmas, William},
  booktitle={Articulated Motion and Deformable Objects: 9th International Conference, AMDO 2016, Palma de Mallorca, Spain, July 13-15, 2016, Proceedings 9},
  pages={185--206},
  year={2016},
  organization={Springer}
}

@article{liu2024customlistener,
  title={CustomListener: Text-guided Responsive Interaction for User-friendly Listening Head Generation},
  author={Liu, Xi and Guo, Ying and Zhen, Cheng and Li, Tong and Ao, Yingying and Yan, Pengfei},
  journal={arXiv preprint arXiv:2403.00274},
  year={2024}
}

@article{yang2023diffsound,
  title={Diffsound: Discrete diffusion model for text-to-sound generation},
  author={Yang, Dongchao and Yu, Jianwei and Wang, Helin and Wang, Wen and Weng, Chao and Zou, Yuexian and Yu, Dong},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2023},
  publisher={IEEE}
}
@inproceedings{sohl2015deep,
  title={Deep unsupervised learning using nonequilibrium thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={International conference on machine learning},
  pages={2256--2265},
  year={2015},
  organization={PMLR}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{austin2021structured,
  title={Structured denoising diffusion models in discrete state-spaces},
  author={Austin, Jacob and Johnson, Daniel D and Ho, Jonathan and Tarlow, Daniel and Van Den Berg, Rianne},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17981--17993},
  year={2021}
}
@article{geng2023affective,
  title={Affective faces for goal-driven dyadic communication},
  author={Geng, Scott and Teotia, Revant and Tendulkar, Purva and Menon, Sachit and Vondrick, Carl},
  journal={arXiv preprint arXiv:2301.10939},
  year={2023}
}

@misc{radford2022robust,
      title={Robust Speech Recognition via Large-Scale Weak Supervision}, 
      author={Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},
      year={2022},
      eprint={2212.04356},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}
@misc{heusel2018gans,
      title={GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium}, 
      author={Martin Heusel and Hubert Ramsauer and Thomas Unterthiner and Bernhard Nessler and Sepp Hochreiter},
      year={2018},
      eprint={1706.08500},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhang2023t2mgpt,
      title={T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations}, 
      author={Jianrong Zhang and Yangsong Zhang and Xiaodong Cun and Shaoli Huang and Yong Zhang and Hongwei Zhao and Hongtao Lu and Xi Shen},
      year={2023},
      eprint={2301.06052},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{Danecek_2022_CVPR,
    author    = {Dan\v{e}\v{c}ek, Radek and Black, Michael J. and Bolkart, Timo},
    title     = {EMOCA: Emotion Driven Monocular Face Capture and Animation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {20311-20322}
}

@inproceedings{10.1145/3503161.3551574,
author = {Huang, Ricong and Zhong, Weizhi and Li, Guanbin},
title = {Audio-driven Talking Head Generation with Transformer and 3D Morphable Model},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3551574},
doi = {10.1145/3503161.3551574},
abstract = {In the task of talking head generation, it is hard to learn the mapping relationship between generated head image and input audio signal. To tackle this challenge, we propose to learn the mapping relationship between input audio signal and the parameters of three-dimensional morphable face model (3DMM) first, which is easier to learn. Then the parameters of 3DMM are used to guide the generation of high-quality talking head images. Prior works mostly encode audio features from short audio windows, which may influence the accuracy of lip movements sometimes because of the limited context. In this paper, we propose a transformer-based audio encoder to take full use of the long-term context from audio and then predict a sequence of 3DMM parameters accurately. Unlike prior works that only use the 3DMM parameters of expression, rotation and translation, we propose to include the parameters of identity. Since the location of 3D facial mesh point is decided by the expression and identity parameters, it is helpful to supply more subtle control of lip movement by considering the identity parameters. The experimental results reveal that our method ranks first in 4 of the total 11 evaluation metrics, which ranks first in the talking head generation track.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {7035–7039},
numpages = {5},
keywords = {transformer, talking head generation, long-term context, audio-driven, 3d morphable model},
location = {<conf-loc>, <city>Lisboa</city>, <country>Portugal</country>, </conf-loc>},
series = {MM '22}
}

@inbook{10.1145/3596711.3596730,
author = {Blanz, Volker and Vetter, Thomas},
title = {A Morphable Model For The Synthesis Of 3D Faces},
year = {2023},
isbn = {9798400708978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3596711.3596730},
abstract = {In this paper, a new technique for modeling textured 3D faces is introduced. 3D faces can either be generated automatically from one or more photographs, or modeled directly through an intuitive user interface. Users are assisted in two key problems of computer aided face modeling. First, new face images or new 3D face models can be registered automatically by computing dense one-to-one correspondence to an internal face model. Second, the approach regulates the naturalness of modeled faces avoiding faces with an ''unlikely'' appearanceStarting from an example set of 3D face models, we derive a morphable face model by transforming the shape and texture of the examples into a vector space representation. New faces and expressions can be modeled by forming linear combinations of the prototypes. Shape and texture constraints derived from the statistics of our example faces are used to guide manual modeling or automated matching algorithmsWe show 3D face reconstructions from single images and their applications for photo-realistic image manipulations. We also demonstrate face manipulations according to complex parameters such as gender, fullness of a face or its distinctiveness.},
booktitle = {Seminal Graphics Papers: Pushing the Boundaries, Volume 2},
articleno = {18},
numpages = {8}
}

@inproceedings{10.1145/311535.311556,
author = {Blanz, Volker and Vetter, Thomas},
title = {A morphable model for the synthesis of 3D faces},
year = {1999},
isbn = {0201485605},
publisher = {ACM Press/Addison-Wesley Publishing Co.},
address = {USA},
url = {https://doi.org/10.1145/311535.311556},
doi = {10.1145/311535.311556},
abstract = {In this paper, a new technique for modeling textured 3D faces is introduced. 3D faces can either be generated automatically from one or more photographs, or modeled directly through an intuitive user interface. Users are assisted in two key problems of computer aided face modeling. First, new face images or new 3D face models can be registered automatically by computing dense one-to-one correspondence to an internal face model. Second, the approach regulates the naturalness of modeled faces avoiding faces with an "unlikely" appearance.Starting from an example set of 3D face models, we derive a morphable face model by transforming the shape and texture of the examples into a vector space representation. New faces and expressions can be modeled by forming linear combinations of the prototypes. Shape and texture constraints derived from the statistics of our example faces are used to guide manual modeling or automated matching algorithms.We show 3D face reconstructions from single images and their applications for photo-realistic image manipulations. We also demonstrate face manipulations according to complex parameters such as gender, fullness of a face or its distinctiveness.},
booktitle = {Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {187–194},
numpages = {8},
keywords = {registration, photogrammetry, morphing, facial modeling, facial animation, computer vision},
series = {SIGGRAPH '99}
}

@inproceedings{10.1145/3581783.3612123,
author = {Liu, Jin and Wang, Xi and Fu, Xiaomeng and Chai, Yesheng and Yu, Cai and Dai, Jiao and Han, Jizhong},
title = {MFR-Net: Multi-faceted Responsive Listening Head Generation via Denoising Diffusion Model},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612123},
doi = {10.1145/3581783.3612123},
abstract = {Face-to-face communication is a common scenario including roles of speakers and listeners. Most existing research methods focus on producing speaker videos, while the generation of listener heads remains largely overlooked. Responsive listening head generation is an important task that aims to model face-to-face communication scenarios by generating a listener head video given a speaker video and a listener head image. An ideal generated responsive listening video should respond to the speaker with attitude or viewpoint expressing while maintaining diversity in interaction patterns and accuracy in listener identity information. To achieve this goal, we propose the Multi-Faceted Responsive Listening Head Generation Network (MFR-Net). Specifically, MFR-Net employs the probabilistic denoising diffusion model to predict diverse head pose and expression features. In order to perform multi-faceted response to the speaker video, while maintaining accurate listener identity preservation, we design the Feature Aggregation Module to boost listener identity features and fuse them with other speaker-related features. Finally, a renderer finetuned with identity consistency loss produces the final listening head videos. Our extensive experiments demonstrate that MFR-Net not only achieves multi-faceted responses in diversity and speaker identity information but also in attitude and viewpoint expression.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {6734–6743},
numpages = {10},
keywords = {listening head generation, image synthesis, denoising diffusion model},
location = {<conf-loc>, <city>Ottawa ON</city>, <country>Canada</country>, </conf-loc>},
series = {MM '23}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@article{ho2022cascaded,
  title={Cascaded diffusion models for high fidelity image generation},
  author={Ho, Jonathan and Saharia, Chitwan and Chan, William and Fleet, David J and Norouzi, Mohammad and Salimans, Tim},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={47},
  pages={1--33},
  year={2022}
}

@article{epstein2023diffusion,
  title={Diffusion self-guidance for controllable image generation},
  author={Epstein, Dave and Jabri, Allan and Poole, Ben and Efros, Alexei and Holynski, Aleksander},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={16222--16239},
  year={2023}
}

@article{kong2020diffwave,
  title={Diffwave: A versatile diffusion model for audio synthesis},
  author={Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2009.09761},
  year={2020}
}

@article{gong2022diffuseq,
  title={Diffuseq: Sequence to sequence text generation with diffusion models},
  author={Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, LingPeng},
  journal={arXiv preprint arXiv:2210.08933},
  year={2022}
}

@article{ho2022imagen,
  title={Imagen video: High definition video generation with diffusion models},
  author={Ho, Jonathan and Chan, William and Saharia, Chitwan and Whang, Jay and Gao, Ruiqi and Gritsenko, Alexey and Kingma, Diederik P and Poole, Ben and Norouzi, Mohammad and Fleet, David J and others},
  journal={arXiv preprint arXiv:2210.02303},
  year={2022}
}

@article{hoogeboom2021argmax,
  title={Argmax flows and multinomial diffusion: Learning categorical distributions},
  author={Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forr{\'e}, Patrick and Welling, Max},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12454--12465},
  year={2021}
}

@inproceedings{wang2021one,
  title={One-shot free-view neural talking-head synthesis for video conferencing},
  author={Wang, Ting-Chun and Mallya, Arun and Liu, Ming-Yu},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10039--10049},
  year={2021}
}

@article{wang2022anyonenet,
  title={Anyonenet: Synchronized speech and talking head generation for arbitrary persons},
  author={Wang, Xinsheng and Xie, Qicong and Zhu, Jihua and Xie, Lei and Scharenborg, Odette},
  journal={IEEE Transactions on Multimedia},
  year={2022},
  publisher={IEEE}
}

@inproceedings{li2023generalized,
  title={Generalized deep 3d shape prior via part-discretized diffusion process},
  author={Li, Yuhan and Dou, Yishun and Chen, Xuanhong and Ni, Bingbing and Sun, Yilin and Liu, Yutian and Wang, Fuzhen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16784--16794},
  year={2023}
}

@article{han2024clip,
  title={CLIP-VQDiffusion: Langauge Free Training of Text To Image generation using CLIP and vector quantized diffusion model},
  author={Han, Seungdae and Kim, Joohee},
  journal={arXiv preprint arXiv:2403.14944},
  year={2024}
}
@article{cassell1999power,
  title={The power of a nod and a glance: Envelope vs. emotional feedback in animated conversational agents},
  author={Cassell, Justine and Thorisson, Kristinn R},
  journal={Applied Artificial Intelligence},
  volume={13},
  number={4-5},
  pages={519--538},
  year={1999},
  publisher={Taylor \& Francis}
}
@article{tronick1980monadic,
  title={Monadic phases: A structural descriptive analysis of infant-mother face to face interaction},
  author={Tronick, Edward and Als, Heidelise and Brazelton, T Berry},
  journal={Merrill-Palmer Quarterly of Behavior and Development},
  volume={26},
  number={1},
  pages={3--24},
  year={1980},
  publisher={JSTOR}
}
@inproceedings{chen2021high,
  title={High-fidelity face tracking for ar/vr via deep lighting adaptation},
  author={Chen, Lele and Cao, Chen and De la Torre, Fernando and Saragih, Jason and Xu, Chenliang and Sheikh, Yaser},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={13059--13069},
  year={2021}
}
@inproceedings{li2021ai,
  title={Ai choreographer: Music conditioned 3d dance generation with aist++},
  author={Li, Ruilong and Yang, Shan and Ross, David A and Kanazawa, Angjoo},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={13401--13412},
  year={2021}
}
@article{zhang2022motiondiffuse,
  title={Motiondiffuse: Text-driven human motion generation with diffusion model},
  author={Zhang, Mingyuan and Cai, Zhongang and Pan, Liang and Hong, Fangzhou and Guo, Xinying and Yang, Lei and Liu, Ziwei},
  journal={arXiv preprint arXiv:2208.15001},
  year={2022}
}
@inproceedings{zhang2021flow,
  title={Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset},
  author={Zhang, Zhimeng and Li, Lincheng and Ding, Yu and Fan, Changjie},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3661--3670},
  year={2021}
}
@inproceedings{guo2021ad,
  title={Ad-nerf: Audio driven neural radiance fields for talking head synthesis},
  author={Guo, Yudong and Chen, Keyu and Liang, Sen and Liu, Yong-Jin and Bao, Hujun and Zhang, Juyong},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={5784--5794},
  year={2021}
}
@inproceedings{peng2023emotalk,
  title={Emotalk: Speech-driven emotional disentanglement for 3d face animation},
  author={Peng, Ziqiao and Wu, Haoyu and Song, Zhenbo and Xu, Hao and Zhu, Xiangyu and He, Jun and Liu, Hongyan and Fan, Zhaoxin},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={20687--20697},
  year={2023}
}
@inproceedings{ren2021pirenderer,
  title={Pirenderer: Controllable portrait image generation via semantic neural rendering},
  author={Ren, Yurui and Li, Ge and Chen, Yuanqi and Li, Thomas H and Liu, Shan},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={13759--13768},
  year={2021}
}
@inproceedings{tolzin2023mechanisms,
  title={Mechanisms of Common Ground in Human-Agent Interaction: A Systematic Review of Conversational Agent Research.},
  author={Tolzin, Antonia and Janson, Andreas},
  booktitle={HICSS},
  pages={342--351},
  year={2023}
}
@article{cho2022alexa,
  title={Alexa as an active listener: how backchanneling can elicit self-disclosure and promote user experience},
  author={Cho, Eugene and Motalebi, Nasim and Sundar, S Shyam and Abdullah, Saeed},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={6},
  number={CSCW2},
  pages={1--23},
  year={2022},
  publisher={ACM New York, NY, USA}
}