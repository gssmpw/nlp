\section{Introduction}
With the rise of applications such as digital avatar generation~\cite{chen2021high,li2021ai,zhang2022motiondiffuse} and human-computer interaction~\cite{zhang2021flow,guo2021ad,peng2023emotalk}, 
appropriate listening reactions have attracted attentions~\cite{tolzin2023mechanisms,cho2022alexa}.
In face-to-face conversations, appropriate nonverbal feedback from the listener, rather than content-based replies, is often crucial for maintaining the flow of communication~\cite{cassell1999power}. 
Therefore appropriate nonverbal feedback is important for virtual agents that communicate with humans~\cite{tronick1980monadic}.
In this context, there is growing interest in generating realistic listener head generation (LHG)~\cite{ng2022learning,ng2023can,song2023emotional}.
The listener head generation task aims to generate natural and appropriate nonverbal responses, such as head motions and facial expressions, based on the speaker's verbal utterances and facial expressions.
Moreover, listener reactions are influenced not only by the speaker's input but also by the listener's internal state, emotions, and personality, leading to non-deterministic responses.
To address this non-deterministic property of the listener's reaction, a previous study~\cite{ng2022learning} proposes to model the listener's reactions using a one-dimensional VQ-VAE~\cite{van2017neural}. 
This allows modeling unique reactions for different listener identities while preserving the non-deterministic properties and identity-specific reaction styles.

Most existing LHG approaches rely on autoregressive methodologies to generate listener reactions~\cite{ng2022learning,ng2023can,zhou2022responsive}.
These autoregressive methods have inherent structural drawbacks. In particular, during the inference stage, autoregressive models are sensitive to accumulating prediction errors. 
These accumulated errors can lead the generated sequence to significantly differ from the desired target and produce incoherent or unnatural listener responses.
Although a NAR(non-autoregressive) generation approach~\cite{song2023emotional} has been explored, it comes with its own constraints. The reported results are limited to short durations, and extending the response length requires a larger codebook size, which may limit the scalability of the approach.

To address the limitations of existing listener head generation methods, we propose \emph{DiffListener} that generates longer listener responses, which is challenging but more practical, in a non-autoregressive manner and fixed codebook size. 
First, we train a VQ-VAE\cite{van2017neural} model to learn a discrete codebook that encodes listener-specific response patterns. Second, we employ a discrete diffusion model~\cite{gu2022vector} to generate diverse and varied listener responses while preserving the codebook representation. 
The model performs the denoising diffusion process on the codebook tokens, which represent the listener's responsive reactions.
Prior research~\cite {ng2022learning,zhou2022responsive,song2023emotional} has primarily focused on the speaker's facial and audio cues to generate listener responses. 
However, this approach may overlook crucial lexical context. 
Our method incorporates textual information to consider this aspect.
Also, compressing speaker modalities into a condition module might result in a loss of temporal rhythmic information.
To overcome it, we incorporated the speaker's facial differential information, which helps maintain temporal rhythmic information, potentially enhancing the naturalness and coherence of generated reactions.
In our experiments, DiffListener outperforms the existing baselines in both quantitative and qualitative evaluations. These results demonstrate the effectiveness of our proposed approach.

In summary, the key contributions are as follows:
\begin{description}
    \item[$\bullet$] We propose DiffListener which is a novel non-autoregressive framework for listener head generation.
    To the best of our knowledge, it is the first to apply the discrete diffusion to listener generation task.
    \item[$\bullet$]
    We propose utilizing the facial differential and text information
    into the non-autoregressive generation framework to provide more context information.
    \item[$\bullet$] DiffListener achieves state-of-the-art performance on the listener head generation task, generating longer sequences while preserving high quality and relevance. 
\end{description}
