\section{Conclusion}
In this work, we propose DiffListener, a novel approach that generates realistic and diverse listener responses in a non-autoregressive manner using a discrete diffusion model.
Unlike previous work, our method can generate longer responses while maintaining a fixed codebook size. 
To better synchronize with the speaker, we introduce a novel speaker modality (the speaker's facial motion differential).
Through experiments, we demonstrated that our approach outperforms existing baselines in terms of realism, diversity, and synchrony with the speaker's motions.
This work represents a significant step forward in achieving more natural and context-aware listener generation.