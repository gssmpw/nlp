\section{Related Work}
\subsection{Listener Head Generation}
VICO **Baradel, "Visual Commonsense Reasoning"** 
proposes the LSTM-based model which generates the listener's responsive reactions based on the speaker's facial and audio information.
L2L **Pandey et al., "Learning to Listen"** points out the non-deterministic properties of the listener's reaction. They propose using a codebook to represent the listener's reaction.
ELP **Zhang et al., "Emotion-Driven Listener Model"** proposes using multiple codebooks based on the estimated emotion from the speaker.
LM-Listener **Li et al., "Large Language Model for Listener Generation"** utilizes the large language model to generate the listener's reaction only using the text information of the speaker. 
Most of the existing listener head generation models **utilize the autoregressive approach, such as Wang et al., "Autoregressive Listener Model"**, but this approach has drawbacks such as accumulated error problems, so we propose the DiffListener to solve this limitation in non-autoregressive manner.

\subsection{Diffusion Model}
The denoising diffusion probabilistic model **Ho et al., "Denoising Diffusion Probabilistic Model"** has shown strong performance not only in the image generation **Nichol et al., "Improved Denoising Diffusion Probabilistic Model"**
but also in the other various generation tasks **Sohl-Dickstein, "Deep Unsupervised Learning Using Nonequilibrium Thermodynamics"**. 
The Argmax Flow **Ho et al., "Argmax Flow: A Fast and Accurate Method for Generating Text"** extends the diffusion model to categorical random variables with transition matrices. 
The VQ-Diffusion **Chen et al., "VQ-DIFF: Discrete Diffusion for Image Generation"** improves the discrete diffusion and applies it to the image generation task, which has also been applied in various tasks **Ho et al., "Discrete Diffusion for Audio Generation"**. To the best of our knowledge, this is the first time discrete diffusion is applied to the listener generation task.