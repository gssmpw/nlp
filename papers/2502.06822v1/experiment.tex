\section{Experiment}
\begin{table}[t]
\centering
\caption{Ablation study results based on codebook size at Trevor dataset. GT denotes the Ground Truth.}
\label{tab: ablation codebook size}
\resizebox{0.9\columnwidth}{!}{%
\begin{tabular}{cccccc}
\hline
         Codebook size  & L2$\downarrow$   & FD$\downarrow$    & P-FD$\downarrow$  & Diversity & Variation \\ \hline
GT & & & &2.59 &0.11 \\ \hline 
\textit{128}       &  0.47    &  17.61     & 19.28      & 3.63      & 0.17      \\
\textit{256}   & \textbf{0.40}  & \textbf{15.75} & \textbf{17.22} & 2.96      & 0.10    \\
\textit{512} & 0.48 & 18.28 & 20.01 & 3.88      & 0.18    \\
\hline
\end{tabular}%
}
\vspace{-0.2cm}
\end{table}

\subsection{Dataset}
Our research objective is to generate longer identity-specific listener responses. To achieve this goal, we select the dataset~\cite{ng2023can,ng2022learning}, which provides sufficiently long sequences of listener behaviors while maintaining individual identity characteristics.
We preprocess the dataset by setting the listenerâ€™s response period to 8 seconds (240 frames), assuming that this duration may be sufficient to understand contextual information through text data~\cite{ng2023can}.
We clip each video to 8 seconds and employ a sliding-window approach to generate more data.
Based on this preprocessing, we found sufficient data from Trevor and Stephen identities.
To extract text from audio data, we utilize the Whisper~\cite{radford2022robust}.

\subsection{Experimental Setup}
Following the previous work \cite{ng2023can,ng2022learning}, 
we use the following evaluation metrics for quantitative evaluation. L2, FD(Frechet Distance), P-FD (Paired FD) which is the distribution distance between listener and speaker, Diversity, and Variation.
We use the L2L~\cite{ng2022learning}, VICO~\cite{zhou2022responsive}, and LM-Listener~\cite{ng2023can} as our baselines.
The ELP~\cite{song2023emotional}, which is based on the non-autoregressive approach, does not release the code, so we are not able to compare ours with it.
We use $K=256$, $T=240$, $T_d=100$, $d_z=512$, $\tau=8$ with a batch size = $256$ when training on the Trevor dataset~\cite{ng2023can}, and batch size 64 when training on the Stephen dataset~\cite{ng2023can}.
The value $K=256$ is chosen by its superior performance, as shown in Table~\ref{tab: ablation codebook size}.
During DiffListener training, we apply early stopping with a patience of 5 epochs. For VQ-VAE training, we set the weights of each loss term as follows: $0.02$ for $L_{embed}$, $1$ for $L_{reconstructed}$, and $0.05$ for $L_{velocity}$.

\subsection{Quantitative Comparison}
Table~\ref{tab:overall metric} presents the overall performance of our model and the baselines. Our model achieves the lowest L2, FD, and P-FD scores for both Trevor and Stephen's datasets, indicating the highest realism in generating listener motions and the greatest synchrony with the speaker.
Our model achieves superior performance in terms of diversity and variation scores in most cases. 
For the Trevor dataset, the diversity metric is slightly less close to the ground truth compared to the VICO model. 
However, the difference is minimal, and our model achieves a higher diversity score and lower L2, FD, and P-FD scores than VICO. This indicates that our model can generate more realistic and diverse results.

\begin{table}[t]
\centering
\caption{
Ablation study results based on speaker modalities at trevor dataset.
}
\label{tab:ablation modality}
\resizebox{0.95\columnwidth}{!}{%
\begin{tabular}{lccccc}
\hline
         & L2$\downarrow$   & FD$\downarrow$    & P-FD$\downarrow$  & Diversity & Variation \\ \hline
GT       &      &       &       & 2.59      & 0.1127      \\ \hline 
w/o Diff \& Text   & 0.50  & 20.77 & 22.27 & 3.18      & 0.1223    \\
w/o Diff & 0.44 & 17.17 & 18.71 & 3.21      & 0.1247    \\
w/o Text & 0.41   & 16.17   & 17.66    & 3.05    & 0.1095 \\
Ours & \textbf{0.40} & \textbf{15.75} & \textbf{17.22} & 2.96 & 0.1027 \\ \hline
\end{tabular}%
}
\end{table}

\begin{figure}[t]
    \centering    \includegraphics[width=\columnwidth]{qualitative.png}
    \caption{The visualization comparison with baselines and ablated models at Trevor dataset. The blue and pink boxes include the speaker's utterance.}
    \label{fig:  visualize trevor result}
    \vspace{-0.5cm}
\end{figure}

\begin{table}[t]
\centering
\caption{User study results based on the comparison of our model with baselines.}
\label{tab:userstudy}
\resizebox{0.95\columnwidth}{!}{%
\begin{tabular}{lcccccc}
\hline
                    & \multicolumn{3}{c}{Trevor} & \multicolumn{3}{c}{Stephen}   \\ \hline
                    & Win     & Tie     & Lose   & Win     & Tie     & Lose      \\ \hline
Ours vs. Vico~\cite{zhou2022responsive}        & \textbf{51\%}    & 23\%    & 26\%   & \textbf{65\%}    & 11\%    & 24\%      \\
Ours vs. L2L~\cite{ng2022learning}         & \textbf{50\%}    & 11\%    & 39\%   & \textbf{41\%}    & 25\%    & 34\%      \\
Ours vs. LM-Listener~\cite{ng2023can} & \textbf{67\%}    & 16\%    & 16\%   & \textbf{47\%}    & 12\%    & 41\%      \\
 \hline
\end{tabular}%
}
\vspace{-0.5cm}
\end{table}

Table~\ref{tab:ablation modality} presents the results of our ablation study on different modality conditions.
By incorporating text into a model without differential and text information (w/o Diff \& Text),
we observed approximately $17.33\%$ decrease in FD score 
(w/o Diff).
Only by incorporating differential 3DMM information into the model to provide rhythmic temporal information, we observed an approximately $22.16\%$
decrease in the FD score (w/o Text).
Furthermore, integrating the differential 3DMM value and text, we observed a $24.1\%$ decrease in the FD score and $22.6\%$ decrease in the P-FD score (Ours).
This indicates that the combination of differential information with audio, 3DMM, and text data enhances the model's ability to understand the context information, thereby enabling it to generate more appropriate responses.

\subsection{Qualitative Comparision} 
In the Listener Generation task, it is important to generate results that closely match the ground truth.
Figure~\ref{fig: visualize trevor result}(a) presents a visual comparison with the baseline models. The baselines sometimes fail to generate appropriate responses, as indicated by the red boxes. While the ground truth sample is in a neutral state, other baselines sometimes show a smile. In contrast, our model demonstrates more appropriate responses in both head pose and facial expression.
This result may come from the advantages of DiffListener.
First, NAR approach can avoid the problem of accumulated errors. It makes our model's results more robust during the inference.
Second, using various modalities provides sufficient contextual information.
Figure~\ref{fig: visualize trevor result}(b) presents a visual comparison with ablated models. When sufficient contextual information is provided, the model generates more appropriate listener responses. However, when some modalities are excluded, it produces inappropriate responses, as indicated by the red boxes.
In addition, we conduct a user study to evaluate human preferences on Amazon Mechanical Turk. We randomly select 25 videos from each of the identity datasets (total 50 videos) and visualize each result as a grayscale mesh video using EMOCA~\cite{Danecek_2022_CVPR} because mesh videos provide a more intuitive way to evaluate facial and head movements compared to photorealistic videos~\cite{song2023emotional}. If photorealistic video is required, it can be generated using a renderer model~\cite{ren2021pirenderer}.
Given the (speaker, ours, baseline) tuple of samples, 20 people were asked to choose the video that appeared to be listening and paying more attention to the speaker. The results are presented in Table~\ref{tab:userstudy}. Our model is more preferred than the baselines in both datasets. More samples can be found on our demo page\footnote{\url{https://siyeoljung.github.io/DiffListener/}}.