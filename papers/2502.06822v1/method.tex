\begin{table*}[t]
\centering
\caption{Comparision of our model with baselines in Trevor and Stephen dataset. Bold represents the best. 
$\downarrow$ indicates lower is better, no arrow indicates that closer to GT is better. GT denotes the Ground Truth.}
\label{tab:overall metric}
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{lcccccccccc}
\hline
            & \multicolumn{5}{c}{Trevor}                    & \multicolumn{5}{c}{Stephen}                  \\ \hline
Models      & L2$\downarrow$    & FD$\downarrow$    & P-FD$\downarrow$  & Diversity & Variation & L2$\downarrow$   & FD$\downarrow$    & P-FD$\downarrow$  & Diversity & Variation \\ \hline
GT          &       &       &       & 2.59      & 0.11      &      &       &       & 3.81      & 0.18      \\ \hline
VICO~\cite{zhou2022responsive}        & 0.41 & 17.94 & 19.22 & 2.31      & 0.08      & 0.71 & 31.43 & 33.64 & 2.98      & 0.11      \\
L2L~\cite{ng2022learning} & 0.62           & 25.28          & 27.11          & 4.66 & 0.28 & \textbf{0.65} & 28.69          & 30.33          & 2.79 & 0.09 \\
LM-Listener~\cite{ng2023can} & 0.69  & 28.67 & 30.56 & 4.77      & 0.29      & 0.79 & 37.57 & 39.21 & 2.34      & 0.09      \\
Ours               & \textbf{0.40} & \textbf{15.75} & \textbf{17.22} & 2.96 & 0.10 & \textbf{0.65} & \textbf{26.47} & \textbf{28.47} & 3.56 & 0.14 \\ \hline
\end{tabular}%
}
\end{table*}

\section{Methodology}
\subsection{Problem Definition}
To represent the precise 3D facial expressions and motions from video frames, we utilize the 3D Morphable Face Model(3DMM)~\cite{blanz2023morphable, kittler20163d} for each frame. Through this process, we can get the coefficient that corresponds to the facial expression $\beta_t \in \mathbb{R}^{d_m}$ where $d_m$ is the dimension of expression coefficient, head rotation $R_t \in SO(3)$, and identity-specific shape\cite{zollhofer2018state}. We discard the shape coefficient to model the representation that is independent of individual identity\cite{ng2022learning}. 
Our facial information at time $t$ can be represented by the concatenation of the facial expression and rotation coefficients $f_t = [\beta_t, R_t]$.
Let $F^{S}=\{ f^{S}_1, f^{S}_2, \cdots,f^{S}_{T-1},f^{S}_T \}$ represent the sequence of the speaker's facial information across T frames, and let $A^{S}$, $F_\Delta^{S}$, and $W^{S}$ represent their corresponding audio sequence, differential facial information, and text information, respectively. 
The differential facial information of the speaker is calculated taking the difference between the facial information in each time step and the previous time step:
$F_\Delta^{S} = \{f^S_x-f^S_{x-1}\mid 1\leq x\leq T\}$.
When DiffListener is given the speaker's information, the listener's responsive head sequence of the corresponding length $F^{L}$ can be generated:
\begin{equation}
    F^{L} = DiffListener(F^{S},A^{S},F_\Delta^{S},W^{S})
\end{equation}

\begin{figure}[t]
\centering
\includegraphics[width=1\columnwidth]{overall_architecture.png}
    \caption{\textbf{The overview of DiffListener.} 
    The discrete diffusion model takes the fused representation and generates the listener's response sequence tokens. These tokens are then passed through the VQ-VAE decoder to obtain the listener's 3DMM coefficients.
    }
    \label{fig:Architecture overview}
\end{figure}
\subsection{Quantizing Listener Motion}
We use the VQ-VAE~\cite{van2017neural} to quantize the listener's facial information.
The VQ-VAE consists of facial encoder $\mathit{E}$, facial decoder $\mathit{D}$ and codebook $C = \{c_k\}^K_{k=1} \in \mathbb{R}^{K \times d_z}$ where $K$ is the size of the codebook and $d_z$ is the dimension of each code. Given the sequence of listener facial information $F^{L}=\{ f^{L}_1, f^{L}_2, \cdots, f^{L}_{T-1}, f^{L}_T \} $ across T frames,
we encode $F^{L}$ into specific representation through the facial encoder ${Z}=\mathit{E}(F^{L}) \in \mathbb{R}^{\frac{T}{\tau} \times d}$  where $\tau$ represents the ratio of downsampling and $d$ represents the dimension of the representation.
After that the vector quantizer $\mathit Q$ maps each ${Z}$ elements to its closest codebook entry.
Finally, the facial decoder reconstructs the sequence of the listener's facial information. We utilize the combination of losses to train the VQ-VAE:
\begin{equation}
    L_{embed} = \sum_{t=1}^{T/\tau}\parallel z_t-sg[c_i] \parallel^2
\end{equation}
\begin{equation}
    L_{reconstructed} = \sum_{t=1}^T \mathit L_{1}^{smooth} (\hat f_t - f_t)
\end{equation}
\begin{equation}
    L_{velocity} = \sum_{t=1}^{T-1} \mathit L_{1}^{smooth} (\hat f_{t+1} - \hat f_t, f_{t+1} - f_t)
\end{equation}
where $sg$ is the stop-gradient operation, and $\mathit L_{1}^{smooth}$ is the L1 smooth loss function. The total loss is a weighted sum of these losses. 

\subsection{Discrete Diffsuion Model}
We use VQ-Diffusion\cite{gu2022vector} to generate the listener response conditioned on the speaker's representation.
To get the speaker's representation, we utilized the speaker's information $I^S$ which consists of $F^{S}, A^{S}, F_\Delta^{S}, W^{S}$.
This information is then processed through the fusion network described in Section~\ref{sec:fusion_net} to generate the corresponding speaker's representation $R^S$. 
Let $\mathit{x}$ be a token sequence of length $T/\tau$ representing the listener's response, where each token is from the codebook.
During the forward diffusion process, the data $\mathit{x}$ is progressively corrupted through a fixed Markov chain $q(x_t|x_{t-1})$, which randomly replaces some tokens of $\mathit{x_{t-1}}$ over the diffusion time steps $T_d$. The reverse diffusion process then restores the data from $\mathit{x_{T_d}}$ based on the architecture.
The VQ-Diffusion\cite{gu2022vector} operates based on a transition probability matrix. 
The transition probability matrix $[Q_t]_{mn} = q(x_t=m \mid x_{t-1} = n) \in R^{K \times K}$ that represents the probabilities of $x_{t-1}$ transit to $x_t$.
To get better reverse estimation, a [Mask] token is introduced, expanding the transition matrix to $Q_t \in \mathbb{R}^{(K+1) \times (K+1)}$~\cite{gu2022vector}. 
To estimate the posterior transition distribution $q(x_{t-1} \mid x_t,x_0)$, we train the denoising network $p_\theta(x_{t-1}\mid x_t,R^s)$.
The network is trained to minimize the variational lower bound~\cite{sohl2015deep}:
\begin{align}
    L_{vlb} = \sum_{t=1}^{T_d-1}[D_{KL}[q(x_{t-1} \mid x_t,x_0) \parallel p_\theta(x_{t-1} \mid x_t, R^s)]] \notag \\
    +D_{KL}(q(x_{T_d}\mid x_0) \parallel p(x_{T_d}))
\end{align}
where $p(x_{T_d})$ refers to the prior distribution of timestep $T_d$. To get better results we utilize the reparameterization trick~\cite{gu2022vector}. 
This leads to a more noiseless distribution at each step. 
\begin{equation}
    L_{x_0} = -logp_\theta(x_0\mid x_t,R^s)
\end{equation}
where $L_{x_0}$ refers the auxiliary denoising loss, which can be further improved when combined with the $L_{vlb}$.

\subsection{Fusion Network}\label{sec:fusion_net}
Initially, we extract the text features by using the pre-trained language model BERT~\cite{devlin2018bert} and audio features using Mel-frequency cepstral coefficients (MFCC). 
The fusion network consists of two components, as illustrated in Figure~\ref{fig:Architecture overview}. 
The first module focuses on fusing audio and 3DMM features. Cross-modal attention is computed using the queries $Q_A$ from $A^S$, and the keys $K_F$ and values $V_F$ from $F^S$. The second module handles the fusion of audio with differential 3DMM features. In this module, we set the queries $Q_{F_\Delta}$ from $F_\Delta^S$ and the keys $K_A$ and values $V_A$ from $A^S$. 
Finally, we concatenate each of the fused representations along with the text representation and feed the combined representation into an MLP layer for further processing.