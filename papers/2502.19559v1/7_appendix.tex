\section{MALLM Framework} \label{app:framework}

We provide more thorough details about the framework used to run the experiments called MALLM (\textbf{M}ulti-\textbf{A}gent \textbf{L}arge \textbf{L}anguage \textbf{M}odels)\footnote{\url{https://github.com/Multi-Agent-LLMs/mallm}}.

\paragraph{\textbf{Automatic Persona Assignment.}}
Discussions with MALLM use task- and example-specific personas for the agents.
As manually specifying useful personas for each example is not feasible, we automatically assign personas that foster a rich discussion.
For this, we explicitly prompt another LLM (\textit{meta-llama/Meta-Llama-3.1-70B-Instruct}) to generate a diverse set of three expert personas for each example.
This yields a set of experts that represents various beliefs, opinions, and proficiency.
The prompt for the automatic persona assignment can be read in \Cref{app:prompts}.
My approach follows previous works like Solo-Performance-Prompting \citep{WangMWG23a} and Meta-Prompting \citep{SuzgunK24a}, which show that existing LLMs can be leveraged to generate and consult fitting personas on a problem automatically.
We use three agents for this study, following previous works \citep{ChenSB24a, YinSCG23a} because the structural complexity is better than with two agents while not being too complex to provide meaningful insights.
While other works have used different types of personas like personalities \citep{ShiZWW23a}, the personas generated in this work are experts related to the task and example.

\paragraph{\textbf{Discussion Paradigm.}}
To define the structure of the multi-agent discussion, we use the \textit{memory} paradigm defined by \citet{YinSCG23a}.
\Cref{fig:problem_drift} shows the format graphically.
With the memory paradigm, all agents contribute to the discussion once per turn and have all the information available.
While there are potentially many ways to define the discourse's structure, we choose this paradigm because of its simplicity, keeping the human assessment of agent discussions manageable.

\paragraph{\textbf{Voting.}}
A voting mechanism at the end of each turn allows for a fixed discussion length while still providing a solution to evaluate at each turn.
We select this iterative voting approach because it universally fits our diverse selection of generative and QA tasks.
The prompt for the voting is visible in \Cref{app:prompts}.
In the case that the agents produce a tie by voting, we select a random one.
This only happened with 0.65\% of all voting procedures.
Using voting, this study differs from other works that either employ no decision-making at all \citep{SchickDJP22a} or use a judge agent that makes the final decision \citep{SunYLW24}.
Meanwhile, this voting approach yields a definitive solution to evaluate after each turn.

\section{Parameters} \label{app:parameters}

We adhere to default parameters for our used models, using langchain 0.1.16 and openai 1.25.0 for the implementation of the MALLM framework.

\begin{itemize}[noitemsep]%
    \item \texttt{temperature = 1.0}
    \item \texttt{top\_p = 1.0}
    \item \texttt{presence\_penalty = 0.0}
    \item \texttt{frequency\_penalty = 0.0}
    \item \texttt{max\_tokens = 1024}
\end{itemize}

\section{Prompts} \label{app:prompts}

\input{prompts/discussion}
\input{prompts/solutionExtraction}
\input{prompts/voting}
\input{prompts/personaGenerator}
\input{prompts/llmJudge}
\input{prompts/policyFeedback}

\section{Datasets} \label{app:datasets}

\input{tables/datasets}

As discussions require many tokens to be generated and computing resources are limited, only subsets of the datasets are evaluated.
We sample a subset of size $n_{\text{subset}}$ from each dataset for our experiments by a 95\% confidence interval and a 5\% margin of error (MoE), conservatively assuming a sample proportion $p=0.5$ \citep{Cochran53}.
\begin{equation} \label{eq1}
\begin{split}
n &=  \frac{Z_{0.975}^2 \cdot p(1 - p)}{\text{MoE}^2} \\ 
n &=  \frac{1.96^2 \cdot 0.5(1 - 0.5)}{0.05^2} = 384.16 \approx 385 \\
n_{\text{subset}} &= \frac{n}{1 + \left(\frac{n - 1}{N_{\text{dataset}}}\right)} = \frac{385}{1 + \left(\frac{385 - 1}{N_{\text{dataset}}}\right)} \\
\end{split}
\end{equation}
This yields several hundred samples per dataset as our test sets with a 95\% confidence interval and 5\% margin of error.
Several other studies on multi-agent systems also evaluate a subset of discussions \citep{YinSCG23a, ChenSB24a}.
To further quantify if the results reflect the complete datasets, we follow \citet{WangPSB24a} and run each experiment three times on randomized subsets and report the standard deviation of our results between the runs.

\onecolumn

\section{Examples} \label{app:examples}


\subsection{Lack of Progress}
\input{examples/tex/lack_of_progress}

\subsection{Low-Quality Feedback}
\input{examples/tex/low_quality_feedback}

\subsection{Low-Quality Engagement}
\input{examples/tex/low_quality_engagement}

\subsection{Lack of Clarity}
\input{examples/tex/lack_of_clarity}

\subsection{Task Non-Compliance}
\input{examples/tex/task_compliance}

\subsection{Knowledge Gap}
\input{examples/tex/knowledge_gap}

\subsection{Logical Error}
\input{examples/tex/logical_error}

\subsection{Linguistic Error}
\input{examples/tex/linguistic_error}

\clearpage

\section{Dataset Annotation} \label{app:dataset_annotation}

\subsection{Annotation Instructions}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/other_charts/annoation_instructions.pdf}
    \caption{Instructions for the human annotation.}
    \label{fig:inst_human_annot}
\end{figure}

\twocolumn

\subsection{Annotation Procedure}

\paragraph{\textbf{Systematic Reason Extraction}}
Because of the scale and amount of multi-agent conversations, we automatically extract 200 examples (20 per dataset) that suffer from problem drift, i.e., where turn 7 performance is worse than turn 1, and ask \textit{meta-llama/Meta-Llama-3.1-70B-Instruct} to provide a reason for the problem drift.
The prompt for this can be found in \Cref{app:prompts}.
We then ask ChatGPT-4o\footnote{Version: 3rd of December 2025} to summarize all 200 reasons into a manageable list of 26 labels.
This method builds on previous studies that successfully utilize LLMs during the annotation process \citep{WangKRM24, KimMCR24}.

\paragraph{\textbf{Human Assessment}}
To obtain a concise set of factors that could cause problem drift, we manually check the labels generated during the first step for their orthogonality.
In this process, we eliminate redundant entries and sort other reasons into a total of nine resulting categories.
The full list of categories can be seen in \Cref{tab:reasons}.
Next, we ask eight human experts in natural language processing to jointly annotate a total of 170 drifting discussions using the created list of errors.
The annotators are three doctoral students and five research assistants.
One of them is female and the others are male, aged from 21 to 31.
We ask them to select one or multiple of the provided error types based on the messages and solutions of the two turns relevant to problem drift (i.e., the turn before the drift and the turn causing the drift).
Each expert annotates a share of the work.
Due to the effort and associated cost of the complex annotation procedure, there is no overlap between the annotator's samples.
However, annotators should provide a brief explanation of why the labels were selected for the sample.
We also include these explanations for the final dataset.
Assuming that our systematic extraction could have left an important error type out of our final set, we provide a separate field ``Other'' to the annotators with the opportunity to explain.
We also provide the field "None of the above" to capture samples that might have been falsely extracted as drifting samples (e.g., due to evaluation or dataset noise).
We publicly release the produced dataset with 170 samples called \textbf{DriftEval} together with the human annotations to our repository.


\section{Supplementary Figures} \label{app:problemfocus_linecharts}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/line_charts/average_score_per_turn_xsum.pdf}
    \caption{Performance on XSum for different numbers of turns on the six multiple-choice datasets. The red line shows the samples where performance decreases during the seven turns, the blue line shows the discussions where performance improves, and the grey line shows all discussions, including the ones that maintain performance during the seven turns.}
    \label{fig:avg_turn_performance_xsum}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/line_charts/average_score_per_turn_etpc.pdf}
    \caption{Performance on ETPC for different numbers of turns on the six multiple-choice datasets. The red line shows the samples where performance decreases during the seven turns, the blue line shows the discussions where performance improves, and the grey line shows all discussions, including the ones that maintain performance during the seven turns.}
    \label{fig:avg_turn_performance_etpc}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/line_charts/average_score_per_turn_wmt19_de_en.pdf}
    \caption{Performance on WMT19 for different numbers of turns on the six multiple-choice datasets. The red line shows the samples where performance decreases during the seven turns, the blue line shows the discussions where performance improves, and the grey line shows all discussions, including the ones that maintain performance during the seven turns.}
    \label{fig:avg_turn_performance_wmt19}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/line_charts/average_score_per_turn_strategyqa.pdf}
    \caption{Performance on StrategyQA for different numbers of turns on the six multiple-choice datasets. The red line shows the samples where performance decreases during the seven turns, the blue line shows the discussions where performance improves, and the grey line shows all discussions, including the ones that maintain performance during the seven turns.}
    \label{fig:avg_turn_performance_strategyqa}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/line_charts/average_score_per_turn_winogrande.pdf}
    \caption{Performance on WinoGrande for different numbers of turns on the six multiple-choice datasets. The red line shows the samples where performance decreases during the seven turns, the blue line shows the discussions where performance improves, and the grey line shows all discussions, including the ones that maintain performance during the seven turns.}
    \label{fig:avg_turn_performance_winogrande}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/line_charts/average_score_per_turn_aqua_rat.pdf}
    \caption{Performance on AQUA-RAT for different numbers of turns on the six multiple-choice datasets. The red line shows the samples where performance decreases during the seven turns, the blue line shows the discussions where performance improves, and the grey line shows all discussions, including the ones that maintain performance during the seven turns.}
    \label{fig:avg_turn_performance_aquarat}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/line_charts/average_score_per_turn_ethics.pdf}
    \caption{Performance on ETHICS for different numbers of turns on the six multiple-choice datasets. The red line shows the samples where performance decreases during the seven turns, the blue line shows the discussions where performance improves, and the grey line shows all discussions, including the ones that maintain performance during the seven turns.}
    \label{fig:avg_turn_performance_ethics}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/line_charts/average_score_per_turn_mmlu_pro.pdf}
    \caption{Performance on MMLU-Pro for different numbers of turns on the six multiple-choice datasets. The red line shows the samples where performance decreases during the seven turns, the blue line shows the discussions where performance improves, and the grey line shows all discussions, including the ones that maintain performance during the seven turns.}
    \label{fig:avg_turn_performance_mmlupro}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/line_charts/average_score_per_turn_gpqa.pdf}
    \caption{Performance on GPQA for different numbers of turns on the six multiple-choice datasets. The red line shows the samples where performance decreases during the seven turns, the blue line shows the discussions where performance improves, and the grey line shows all discussions, including the ones that maintain performance during the seven turns.}
    \label{fig:avg_turn_performance_gpqa}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/line_charts/average_score_per_turn_ifeval.pdf}
    \caption{Performance on IFEval for different numbers of turns on the six multiple-choice datasets. The red line shows the samples where performance decreases during the seven turns, the blue line shows the discussions where performance improves, and the grey line shows all discussions, including the ones that maintain performance during the seven turns.}
    \label{fig:avg_turn_performance_ifeval}
\end{figure}

\section{Usage of AI}

In the conduct of this research project, we used specific artificial intelligence tools and algorithms (ChatGPT-4o and Grammarly) to assist with writing, coding, and aggregating experiment data. While these tools have augmented our capabilities and contributed to our findings, it's pertinent to note that they have inherent limitations. We have made every effort to use AI in a transparent and responsible manner. Any conclusions drawn are a result of combined human and machine insights. This is an automatic report generated with © AI Usage Cards \url{https://ai-cards.org}.
