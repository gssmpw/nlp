@misc{Becker24,
	title = {Multi-{Agent} {Large} {Language} {Models} for {Conversational} {Task}-{Solving}},
	url = {http://arxiv.org/abs/2410.22932},
	doi = {10.48550/arXiv.2410.22932},
	abstract = {In an era where single large language models have dominated the landscape of artificial intelligence for years, multi-agent systems arise as new protagonists in conversational task-solving. While previous studies have showcased their potential in reasoning tasks and creative endeavors, an analysis of their limitations concerning the conversational paradigms and the impact of individual agents is missing. It remains unascertained how multi-agent discussions perform across tasks of varying complexity and how the structure of these conversations influences the process. To fill that gap, this work systematically evaluates multi-agent systems across various discussion paradigms, assessing their strengths and weaknesses in both generative tasks and question-answering tasks. Alongside the experiments, I propose a taxonomy of 20 multi-agent research studies from 2022 to 2024, followed by the introduction of a framework for deploying multi-agent LLMs in conversational task-solving. I demonstrate that while multi-agent systems excel in complex reasoning tasks, outperforming a single model by leveraging expert personas, they fail on basic tasks. Concretely, I identify three challenges that arise: 1) While longer discussions enhance reasoning, agents fail to maintain conformity to strict task requirements, which leads to problem drift, making shorter conversations more effective for basic tasks. 2) Prolonged discussions risk alignment collapse, raising new safety concerns for these systems. 3) I showcase discussion monopolization through long generations, posing the problem of fairness in decision-making for tasks like summarization. This work uncovers both the potential and challenges that arise with multi-agent interaction and varying conversational paradigms, providing insights into how future research could improve the efficiency, performance, and safety of multi-agent LLMs.},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {Becker, Jonas},
	month = nov,
	year = {2024},
	note = {arXiv:2410.22932 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\ZTDSAR8N\\Becker - 2024 - Multi-Agent Large Language Models for Conversational Task-Solving.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\4YMWHPVU\\2410.html:text/html},
}

@misc{ChenSB24a,
	title = {{ReConcile}: {Round}-{Table} {Conference} {Improves} {Reasoning} via {Consensus} among {Diverse} {LLMs}},
	shorttitle = {{ReConcile}},
	url = {http://arxiv.org/abs/2309.13007},
	abstract = {Large Language Models (LLMs) still struggle with natural language reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents. ReConcile enhances collaborative reasoning between LLM agents via multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism that leads to a better consensus. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their confidence scores, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. Experiments on seven benchmarks demonstrate that ReConcile significantly improves LLMs' reasoning -- both individually and as a team -- surpassing prior single-agent and multi-agent baselines by up to 11.4\% and even outperforming GPT-4 on three datasets. ReConcile also flexibly incorporates different combinations of agents, including API-based, open-source, and domain-specific models, leading to an 8\% improvement on MATH. Finally, we analyze the individual components of ReConcile, demonstrating that the diversity originating from different models is critical to its superior performance. Code: https://github.com/dinobby/ReConcile},
	language = {en},
	urldate = {2024-07-15},
	publisher = {arXiv},
	author = {Chen, Justin Chih-Yao and Saha, Swarnadeep and Bansal, Mohit},
	month = jun,
	year = {2024},
	note = {arXiv:2309.13007 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: ACL 2024 (Camera-Ready)},
	file = {2309.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\NY67MAI4\\2309.pdf:application/pdf},
}

@misc{DuLTT23,
	title = {Improving {Factuality} and {Reasoning} in {Language} {Models} through {Multiagent} {Debate}},
	url = {http://arxiv.org/abs/2305.14325},
	abstract = {Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such "society of minds" approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B. and Mordatch, Igor},
	month = may,
	year = {2023},
	note = {arXiv:2305.14325 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Project Webpage and Code: https://composable-models.github.io/llm\_debate/},
	file = {arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\36NCDYYW\\2305.html:text/html;Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\KJILHJQZ\\Du et al. - 2023 - Improving Factuality and Reasoning in Language Mod.pdf:application/pdf},
}

@misc{GuoCWC24a,
	title = {Large {Language} {Model} based {Multi}-{Agents}: {A} {Survey} of {Progress} and {Challenges}},
	shorttitle = {Large {Language} {Model} based {Multi}-{Agents}},
	url = {http://arxiv.org/abs/2402.01680},
	abstract = {Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize the commonly used datasets or benchmarks for them to have convenient access. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository, dedicated to outlining the research on LLM-based multi-agent systems.},
	urldate = {2024-02-07},
	publisher = {arXiv},
	author = {Guo, Taicheng and Chen, Xiuying and Wang, Yaqi and Chang, Ruidi and Pei, Shichao and Chawla, Nitesh V. and Wiest, Olaf and Zhang, Xiangliang},
	month = jan,
	year = {2024},
	note = {arXiv: 2402.01680 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, !tr, nlp\_agent, Computer Science - Multiagent Systems, nlp\_llm, nlp\_survey},
	annote = {Annotations(2/9/2024, 1:26:48 PM)
“LLMbased multi-agent systems have achieved considerable progress in complex problem-solving and world simulation.” (Guo et al., 2024, p. 1)
“Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents’ capacities?” (Guo et al., 2024, p. 1)
“To keep researchers updated on the latest studies, we maintain an open-source GitHub repository,” (Guo et al., 2024, p. 1)
“Timely survey papers systematically summarize the progress of LLM-based agents, as seen in works [Xi et al., 2023; Wang et al., 2023b].” (Guo et al., 2024, p. 1)
“Compared to systems using a single LLM-powered agent, multi-agent systems offer advanced capabilities by 1) specializing LLMs into various distinct agents, each with different capabilities, and 2) enabling interactions among these diverse agents to simulate complex real-world environments effectively.” (Guo et al., 2024, p. 1)
“Recent research has demonstrated promising results in utilizing LLM-based multi-agents for solving various tasks, such as software development [Hong et al., 2023; Qian et al., 2023], multi-robot systems [Mandi et al., 2023; Zhang et al., 2023c], society simulation [Park et al., 2023; Park et al., 2022], policy simulation [Xiao et al., 2023; Hua et al., 2023], and game simulation [Xu et al., 2023c; Wang et al., 2023c].” (Guo et al., 2024, p. 1)
“Readers will gain a comprehensive overview of LLM-based Multi-Agent (LLM-MA) systems,” (Guo et al., 2024, p. 2)
“Decision-making Thought: This term denotes the capability of LLM-based agents, guided by prompts, to break down complex tasks into smaller subgoals [Khot et al., 2023], think through each part methodically (sometimes exploring multiple paths) [Yao et al., 2023], and learn from past experiences [Shinn et al., 2023] to perform better decision-making on complex tasks.” (Guo et al., 2024, p. 3)
“Tool-use: LLM-based agents’ tool-use capability allows them to leverage external tools and resources to accomplish tasks,” (Guo et al., 2024, p. 3)
“Memory: This ability refers to the capability of LLMbased agent for conducting in-context learning [Dong et al., 2023a] as short memory or external vector database [Lewis et al., 2021] as long memory to preserve and retrieve information over prolonged periods [Wang et al., 2023b]” (Guo et al., 2024, p. 3)
“Single-Agent systems empowered by LLMs have shown inspiring cognitive abilities [Sumers et al., 2023]. The construction of such systems concentrates on formulating their internal mechanisms and interactions with the external environment. Conversely, LLM-MA systems emphasize diverse agent profiles, inter-agent interactions, and collective decision-making processes.” (Guo et al., 2024, p. 3)
“A critical inquiry we address is how these LLM-MA systems are aligned to their operational environments and the collective objectives they are designed to achieve. To shed light on this, we present the general architecture of these systems in Fig. 2.” (Guo et al., 2024, p. 3)
“The AgentsEnvironment Interface refers to the way in which agents interact with and perceive the environment. It’s through this interface that agents understand their surroundings, make decisions, and learn from the outcomes of their actions. We categorize the current interfaces in LLM-MA systems into three types, Sandbox, Physcial, and None, as detailed in Table 1.” (Guo et al., 2024, p. 3)
“The Sandbox refers to a simulated or virtual environment built by human where agents can interact more freely and experiment with various actions and strategies.” (Guo et al., 2024, p. 3)
“The Physical is a real-world environment where agents interact with physical entities and obey realworld physics and constraints.” (Guo et al., 2024, p. 3)
“None refers to scenarios where there is no specific external environment, and agents do not interact with any environment. For example, many applications [Du et al., 2023; Xiong et al., 2023; Chan et al., 2023] utilize multiple agents to debate a question to reach a consensus.” (Guo et al., 2024, p. 3)
“Agents Profiling In LLM-MA systems, agents are defined by their traits, actions, and skills, which are tailored to meet specific goals. Across various systems, agents assume distinct roles, each with comprehensive descriptions encompassing characteristics, capabilities, behaviors, and constraints.” (Guo et al., 2024, p. 3)
“Regarding the Agent Profiling Methods, we categorized them into three types: Pre-defined, Model-Generated, and” (Guo et al., 2024, p. 3)
“Data-Derived.” (Guo et al., 2024, p. 4)
“Pre-defined cases, agent profiles are explicitly defined by the system designers. The ModelGenerated method creates agent profiles by models, e.g., large language models. The Data-Derived method involves constructing agent profiles based on pre-existing datasets” (Guo et al., 2024, p. 4)
“Agents Communication” (Guo et al., 2024, p. 4)
“1) Communication Paradigms: the styles and methods of interaction between agents; 2) Communication Structure: the organization and architecture of communication networks within the multi-agent system; and 3) Communication Content exchanged between agents.” (Guo et al., 2024, p. 4)
“Communication Paradigms: Current LLM-MA systems mainly take three paradigms for communication: Cooperative, Debate, and Competitive.” (Guo et al., 2024, p. 4)
“Communication Structure:” (Guo et al., 2024, p. 4)
“Layered communication is structured hierarchically, with agents at each level having distinct roles and primarily interacting within their layer or with adjacent layers.” (Guo et al., 2024, p. 4)
“Decentralized communication operates on a peer-to-peer network, where agents directly communicate with each other, a structure commonly employed in world simulation applications. Centralized communication involves a central agent” (Guo et al., 2024, p. 4)
“or a group of central agents coordinating the system’s communication, with other agents primarily interacting through this central node. Shared Message Pool is proposed by MetaGPT [Hong et al., 2023] to improve the communication efficiency. This communication structure maintains a shared message pool where agents publish messages and subscribe to relevant messages based on their profile” (Guo et al., 2024, p. 5)
“Communication Content: In LLM-MA systems, the Communication Content typically takes the form of text.” (Guo et al., 2024, p. 5)
“Agents Capabilities Acquisition” (Guo et al., 2024, p. 5)
“two fundamental concepts: the types of feedback from which agents should learn to enhance their capabilities, and the strategies for agents to adjust themselves to effectively solve complex problems” (Guo et al., 2024, p. 5)
“Feedback: Feedback involves the critical information that agents receive about the outcome of their actions, helping the agents learn the potential impact of their actions and adapt to complex and dynamic problems.” (Guo et al., 2024, p. 5)
“1) Feedback from Environment, e.g., from either real world environments or virtual environments [Wang et al., 2023b].” (Guo et al., 2024, p. 5)
“2) Feedback from Agents Interactions means that the feedback comes from the judgement of other agents or from agents communications.” (Guo et al., 2024, p. 5)
“3) Human Feedback comes directly from humans and is crucial for aligning the multi-agent system with human values and preferences.” (Guo et al., 2024, p. 5)
“4) None. In some cases, there is no feedback provided to the agents. This often happens for world simulation works focused on analyzing simulated results rather than the planning capabilities of agents.” (Guo et al., 2024, p. 5)
“2) SelfEvolution. Instead of only relying on the historical records to decide subsequent actions as seen in Memory-based solutions, agents can dynamically self-evolve by modifying themselves such as altering their initial goals and planning strategies, and training themselves based on feedback or communication logs” (Guo et al., 2024, p. 5)
“LLM-MA for Problem Solving The main motivation of using LLM-MA for problem solving is to harness the collective capabilities of agents with specialized expertise.” (Guo et al., 2024, p. 5)
“Science Experiments Like multiple agents play as different specialists and cooperate to solve the Software Development and Embodied Agents problem, multiple agents can also be used to form a science team to conduct science experiments.” (Guo et al., 2024, p. 7)
“Human experts are at the center of these agents to process the information of agents and give feedback to the agents. [Zheng et al., 2023] utilizes multiple LLM-based agents, each focusing on specific tasks for the science experiments including strategy planning, literature search, coding, robotic operations, and labware design” (Guo et al., 2024, p. 7)
“4.1.4 Science Debate LLM-MA can be set for science debating scenarios, where agents debate with each other to enhance the collective reasoning capabilities in tasks such as Massive Multitask Language Understanding (MMLU) [Hendrycks et al., 2020], Math problems [Cobbe et al., 2021], and StrategyQA [Geva et al., 2021]. The main idea is that each agent initially offers its own analysis of a problem, which is then followed by a joint debating process. Through multiple rounds of debate, the agents converge on a single, consensus answer. [Du et al., 2023] leverages the multi-agents debate process on a set of six different reasoning and factual accuracy tasks and demonstrates that LLM-MA debating can improve factuality.” (Guo et al., 2024, p. 7)
“LLM-MA for World Simulation” (Guo et al., 2024, p. 7)
“Research in this area is rapidly growing and spans a diverse range of fields including social sciences, gaming, psychology, economics, policy-making, etc.” (Guo et al., 2024, p. 7)
“Unlike the problem solving systems that focus on agent cooperation, world simulation systems involve diverse methods of agent management and communication, reflecting the complexity and variety of real-world interactions.” (Guo et al., 2024, p. 7)
“Societal Simulation In societal simulation, LLM-MA models are used to simulate social behaviors, aiming to explore the potential social dynamics and propagation, test social science theories, and populate virtual spaces and communities with realistic social phenomena [Park et al., 2023].” (Guo et al., 2024, p. 7)
“4.2.3 Psychology In psychological simulation studies, like in the societal simulation, multiple agents are utilized to simulate humans with various traits and thought processes. However, unlike societal simulations, one approach in psychology involves directly applying psychological experiments to these agents.” (Guo et al., 2024, p. 8) There are no actual references in using LLMs as agents in this block
“[Kovaˇ c et al., 2023] introduces a tool named SocialAI school for creating interactive environments simulating social interactions. It draws from developmental psychology to understand how agents can acquire, demonstrate, and evolve social skills such as joint attention, communication, and cultural learning.” (Guo et al., 2024, p. 8)
“Economy LLM-MA is used to simulate economic and financial trading environments mainly because it can serve as implicit computational models of humans.” (Guo et al., 2024, p. 8)
“This is similar to the way economists model ’homo economicus’, the characterization of man in some economic theories as a rational person who pursues wealth for his own self-interest [Horton, 2023]” (Guo et al., 2024, p. 8)
“Recommender Systems The use of the LLM-MA in recommender systems is similar to that in psychology since studies in both fields involve the consideration of extrinsic and intrinsic human factors such as cognitive processes and personality [Lex and Schedl, 2022]” (Guo et al., 2024, p. 9)
“Agent4Rec [Zhang et al., 2023a] introduces a simulation platform based on LLM-MA. 1000 generative agents are initialized with the MovieLens-1M dataset to simulate complex user interactions in a recommendation environment. Agent4Rec shows that LLM-MA can effectively mimic real user preferences and behaviors, provide insights into phenomena like the filter bubble effect, and help uncover causal relationships in recommendation tasks” (Guo et al., 2024, p. 9)
“Policy Making Similar to simulations in gaming and economic scenarios, Policy Making requires strong decision-making capabilities to realistic and dynamic complex problems. LLM-MA can be used to simulate the policy making via simulating a virtual government or simulating the impact of various policies on different communities.” (Guo et al., 2024, p. 9)
“Disease Propagation Simulation Leveraging the societal simulation capabilities of LLM-MA can also be used to simulate disease propagation. The most recent study in [Williams et al., 2023] delves into the use of LLM-MA in simulating disease spread.” (Guo et al., 2024, p. 10)
“Multi-Agents Framework We provide a detailed introduction to three open-source multi-agent frameworks: MetaGPT [Hong et al., 2023], CAMEL [Li et al., 2023b], and Autogen [Wu et al., 2023a]” (Guo et al., 2024, p. 10)
“MetaGPT is designed to embed human workflow processes into the operation of language model agents, thereby reducing the hallucination problem that often arises in complex tasks.” (Guo et al., 2024, p. 10)
“CAMEL, or Communicative Agent Framework, is oriented towards facilitating autonomous cooperation among agents.” (Guo et al., 2024, p. 10)
“AutoGen is a versatile framework that allows for the creation of applications using language models. It is distinctive for its high level of customization, enabling developers to program agents using both natural language and code to define how these agents interact.” (Guo et al., 2024, p. 10)
“In the Problem solving scenarios, most datasets and benchmarks are used to evaluate the planning and reasoning capabilities by Multiple agents cooperation or debate. In World Simulation scenarios, datasets and benchmarks are used to evaluate the alignment between the simulated world and realworld or analyze the behaviors of different agents.” (Guo et al., 2024, p. 10)
“there is a notable lack in multi-modal settings, where agents would interact with and interpret data from multiple sensory inputs and generate multiple outputs such as images, audio, video, and physical actions.” (Guo et al., 2024, p. 10)
“detecting and mitigating hallucinations in LLMMA is not just a crucial task but also presents a unique set of challenges. It involves not only correcting inaccuracies at the level of individual agents but also managing the flow of information between agents to prevent the spread of these inaccuracies throughout the system.” (Guo et al., 2024, p. 10)
“Acquiring Collective Intelligence In traditional multi-agent systems, agents often use reinforcement learning to learn from offline training datasets. However, LLM-MA systems mainly learn from instant feedback, such as interactions with the environment or humans” (Guo et al., 2024, p. 11)
“much of the existing research focuses on evaluating individual agents’ understanding and reasoning within narrowly defined scenarios.” (Guo et al., 2024, p. 11)
“there is a notable shortfall in the development of comprehensive benchmarks across several research domains, such as Science Team for Experiment Operations, Economic analysis, and Disease propagation simulation.” (Guo et al., 2024, p. 11)
“[Gao et al., 2023b] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023.” (Guo et al., 2024, p. 12)
“[Wang et al., 2023b] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey on large language model based autonomous agents, 2023.” (Guo et al., 2024, p. 14)
“[Xi et al., 2023] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model based agents: A survey, 2023.” (Guo et al., 2024, p. 14)
},
	annote = {Comment: This work is ongoing and we welcome your contribution!},
	annote = {Comment: This work is ongoing and we welcome your contribution!},
	file = {arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\MFKTY32Q\\2402.html:text/html;GuoCWC24--tr--large_language_model_based_multi-agents_a_survey_of_progress_and_challenges.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\SXAJQAEX\\GuoCWC24--tr--large_language_model_based_multi-agents_a_survey_of_progress_and_challenges.pdf:application/pdf},
}

@inproceedings{LiCSC23a,
	address = {Singapore},
	title = {Theory of {Mind} for {Multi}-{Agent} {Collaboration} via {Large} {Language} {Models}},
	url = {https://aclanthology.org/2023.emnlp-main.13},
	doi = {10.18653/v1/2023.emnlp-main.13},
	abstract = {While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.},
	urldate = {2024-07-17},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Huao and Chong, Yu and Stepputtis, Simon and Campbell, Joseph and Hughes, Dana and Lewis, Charles and Sycara, Katia},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {180--192},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\A26INITA\\Li et al. - 2023 - Theory of Mind for Multi-Agent Collaboration via L.pdf:application/pdf},
}

@misc{OpenAIAAA24a,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2024-06-26},
	publisher = {arXiv},
	author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and others},
	month = mar,
	year = {2024},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: 100 pages; updated authors list; fixed author names and added citation},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\QENMPE24\\OpenAI et al. - 2024 - GPT-4 Technical Report.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\A3FZY44Y\\2303.html:text/html},
}

@misc{SchickDJP22a,
	title = {{PEER}: {A} {Collaborative} {Language} {Model}},
	shorttitle = {{PEER}},
	url = {http://arxiv.org/abs/2208.11663},
	abstract = {Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today’s language models are trained to generate only the ﬁnal result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difﬁcult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to inﬁll various parts of the writing process, enabling the use of selftraining techniques for increasing the quality, amount and diversity of training data. This unlocks PEER’s full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.},
	language = {en},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Schick, Timo and Dwivedi-Yu, Jane and Jiang, Zhengbao and Petroni, Fabio and Lewis, Patrick and Izacard, Gautier and You, Qingfei and Nalmpantis, Christoforos and Grave, Edouard and Riedel, Sebastian},
	month = aug,
	year = {2022},
	note = {arXiv:2208.11663 [cs]},
	keywords = {Computer Science - Computation and Language, !tr},
	file = {SchickDJP22--tr--peer_a_collaborative_language_model.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\H5HE5W3M\\SchickDJP22--tr--peer_a_collaborative_language_model.pdf:application/pdf},
}

@misc{ShahW24,
	title = {Agents {Are} {Not} {Enough}},
	url = {http://arxiv.org/abs/2412.16241},
	doi = {10.48550/arXiv.2412.16241},
	abstract = {In the midst of the growing integration of Artificial Intelligence (AI) into various aspects of our lives, agents are experiencing a resurgence. These autonomous programs that act on behalf of humans are neither new nor exclusive to the mainstream AI movement. By exploring past incarnations of agents, we can understand what has been done previously, what worked, and more importantly, what did not pan out and why. This understanding lets us to examine what distinguishes the current focus on agents. While generative AI is appealing, this technology alone is insufficient to make new generations of agents more successful. To make the current wave of agents effective and sustainable, we envision an ecosystem that includes not only agents but also Sims, which represent user preferences and behaviors, as well as Assistants, which directly interact with the user and coordinate the execution of user tasks with the help of the agents.},
	urldate = {2025-01-15},
	publisher = {arXiv},
	author = {Shah, Chirag and White, Ryen W.},
	month = dec,
	year = {2024},
	note = {arXiv:2412.16241 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Multiagent Systems},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\TU7ZZHRU\\Shah und White - 2024 - Agents Are Not Enough.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\FR2V69UM\\2412.html:text/html},
}

@misc{StechlyMK23a,
	title = {{GPT}-4 {Doesn}'t {Know} {It}'s {Wrong}: {An} {Analysis} of {Iterative} {Prompting} for {Reasoning} {Problems}},
	shorttitle = {{GPT}-4 {Doesn}'t {Know} {It}'s {Wrong}},
	url = {http://arxiv.org/abs/2310.12397},
	doi = {10.48550/arXiv.2310.12397},
	abstract = {There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples, a wide spread belief in their iterative self-critique capabilities persists. In this paper, we set out to systematically investigate the effectiveness of iterative prompting of LLMs in the context of Graph Coloring, a canonical NP-complete reasoning problem that is related to propositional satisfiability as well as practical problems like scheduling and allocation. We present a principled empirical study of the performance of GPT4 in solving graph coloring instances or verifying the correctness of candidate colorings. In iterative modes, we experiment with the model critiquing its own answers and an external correct reasoner verifying proposed solutions. In both cases, we analyze whether the content of the criticisms actually affects bottom line performance. The study seems to indicate that (i) LLMs are bad at solving graph coloring instances (ii) they are no better at verifying a solution--and thus are not effective in iterative modes with LLMs critiquing LLM-generated solutions (iii) the correctness and content of the criticisms--whether by LLMs or external solvers--seems largely irrelevant to the performance of iterative prompting. We show that the observed increase in effectiveness is largely due to the correct solution being fortuitously present in the top-k completions of the prompt (and being recognized as such by an external verifier). Our results thus call into question claims about the self-critiquing capabilities of state of the art LLMs.},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Stechly, Kaya and Marquez, Matthew and Kambhampati, Subbarao},
	month = oct,
	year = {2023},
	note = {arXiv:2310.12397 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: 18 pages, 3 figures},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\IC9TXQZ3\\Stechly et al. - 2023 - GPT-4 Doesn't Know It's Wrong An Analysis of Iterative Prompting for Reasoning Problems.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\GW4LUQJH\\2310.html:text/html},
}

@misc{SuzgunK24a,
	title = {Meta-{Prompting}: {Enhancing} {Language} {Models} with {Task}-{Agnostic} {Scaffolding}},
	shorttitle = {Meta-{Prompting}},
	url = {http://arxiv.org/abs/2401.12954},
	doi = {10.48550/arXiv.2401.12954},
	abstract = {We introduce meta-prompting, an effective scaffolding technique designed to enhance the functionality of language models (LMs). This approach transforms a single LM into a multi-faceted conductor, adept at managing and integrating multiple independent LM queries. By employing high-level instructions, meta-prompting guides the LM to break down complex tasks into smaller, more manageable subtasks. These subtasks are then handled by distinct "expert" instances of the same LM, each operating under specific, tailored instructions. Central to this process is the LM itself, in its role as the conductor, which ensures seamless communication and effective integration of the outputs from these expert models. It additionally employs its inherent critical thinking and robust verification processes to refine and authenticate the end result. This collaborative prompting approach empowers a single LM to simultaneously act as a comprehensive orchestrator and a panel of diverse experts, significantly enhancing its performance across a wide array of tasks. The zero-shot, task-agnostic nature of meta-prompting greatly simplifies user interaction by obviating the need for detailed, task-specific instructions. Furthermore, our research demonstrates the seamless integration of external tools, such as a Python interpreter, into the meta-prompting framework, thereby broadening its applicability and utility. Through rigorous experimentation with GPT-4, we establish the superiority of meta-prompting over conventional scaffolding methods: When averaged across all tasks, including the Game of 24, Checkmate-in-One, and Python Programming Puzzles, meta-prompting, augmented with a Python interpreter functionality, surpasses standard prompting by 17.1\%, expert (dynamic) prompting by 17.3\%, and multipersona prompting by 15.2\%.},
	urldate = {2024-06-20},
	publisher = {arXiv},
	author = {Suzgun, Mirac and Kalai, Adam Tauman},
	month = jan,
	year = {2024},
	note = {arXiv:2401.12954 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	annote = {Comment: https://github.com/suzgunmirac/meta-prompting},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\LKS4L9KM\\Suzgun und Kalai - 2024 - Meta-Prompting Enhancing Language Models with Tas.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\L5SSFRS4\\2401.html:text/html},
}

@misc{ValmeekamMK23a,
	title = {Can {Large} {Language} {Models} {Really} {Improve} by {Self}-critiquing {Their} {Own} {Plans}?},
	url = {http://arxiv.org/abs/2310.08118},
	doi = {10.48550/arXiv.2310.08118},
	abstract = {There have been widespread claims about Large Language Models (LLMs) being able to successfully verify or self-critique their candidate solutions in reasoning problems in an iterative mode. Intrigued by those claims, in this paper we set out to investigate the verification/self-critiquing abilities of large language models in the context of planning. We evaluate a planning system that employs LLMs for both plan generation and verification. We assess the verifier LLM's performance against ground-truth verification, the impact of self-critiquing on plan generation, and the influence of varying feedback levels on system performance. Using GPT-4, a state-of-the-art LLM, for both generation and verification, our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability. Additionally, the nature of feedback, whether binary or detailed, showed minimal impact on plan generation. Collectively, our results cast doubt on the effectiveness of LLMs in a self-critiquing, iterative framework for planning tasks.},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Valmeekam, Karthik and Marquez, Matthew and Kambhampati, Subbarao},
	month = oct,
	year = {2023},
	note = {arXiv:2310.08118 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\XWNZ4DAR\\Valmeekam et al. - 2023 - Can Large Language Models Really Improve by Self-critiquing Their Own Plans.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\AAYLB5LT\\2310.html:text/html},
}

@misc{WangMWG23a,
	title = {Unleashing {Cognitive} {Synergy} in {Large} {Language} {Models}: {A} {Task}-{Solving} {Agent} through {Multi}-{Persona} {Self}-{Collaboration}},
	shorttitle = {Unleashing {Cognitive} {Synergy} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.05300},
	abstract = {Human intelligence thrives on the concept of cognitive synergy, where collaboration and information integration among different cognitive processes yield superior outcomes compared to individual cognitive processes in isolation. Although Large Language Models (LLMs) have demonstrated promising performance as general task-solving agents, they still struggle with tasks that require intensive domain knowledge and complex reasoning. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have discovered that assigning multiple, fine-grained personas in LLMs elicits better problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledgeintensive and reasoning-intensive types. Unlike previous works, such as Chainof-Thought, that solely enhance the reasoning abilities in LLMs, SPP effectively elicits internal knowledge acquisition abilities, reduces hallucination, and maintains strong reasoning capabilities. Code, data, and prompts can be found at: https: //github.com/MikeWangWZHL/Solo-Performance-Prompting.git.},
	language = {en},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Wang, Zhenhailong and Mao, Shaoguang and Wu, Wenshan and Ge, Tao and Wei, Furu and Ji, Heng},
	month = jul,
	year = {2023},
	note = {arXiv: 2307.05300 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, !tr},
	annote = {Comment: work in progress},
	annote = {Comment: work in progress},
	file = {arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\6TSMNQM6\\2307.html:text/html;WangMWG23--tr--unleashing_cognitive_synergy_in_large_language_models_a_task-solving_agent_through.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\MP7S5Z8Z\\WangMWG23--tr--unleashing_cognitive_synergy_in_large_language_models_a_task-solving_agent_through.pdf:application/pdf},
}

@misc{WangWSL23,
	title = {Self-{Consistency} {Improves} {Chain} of {Thought} {Reasoning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2203.11171},
	abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It ﬁrst samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
	language = {en},
	urldate = {2024-06-20},
	publisher = {arXiv},
	author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
	month = mar,
	year = {2023},
	note = {arXiv:2203.11171 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: Published at ICLR 2023. V2: added PaLM results; V3: added UL2 results; V4: camera ready version at ICLR 2023},
	file = {2203.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\9HJ77DHK\\2203.pdf:application/pdf},
}

@misc{WangWST24,
	title = {Rethinking the {Bounds} of {LLM} {Reasoning}: {Are} {Multi}-{Agent} {Discussions} the {Key}?},
	shorttitle = {Rethinking the {Bounds} of {LLM} {Reasoning}},
	url = {http://arxiv.org/abs/2402.18272},
	abstract = {Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs. In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms. Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs. We observe that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt. Further study reveals the common interaction mechanisms of LLMs during the discussion.},
	language = {en},
	urldate = {2024-08-16},
	publisher = {arXiv},
	author = {Wang, Qineng and Wang, Zihao and Su, Ying and Tong, Hanghang and Song, Yangqiu},
	month = feb,
	year = {2024},
	note = {arXiv:2402.18272 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: 22 pages, 5 figures, 10 tables},
	file = {Wang et al. - 2024 - Rethinking the Bounds of LLM Reasoning Are Multi-.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\QBD6SHFK\\Wang et al. - 2024 - Rethinking the Bounds of LLM Reasoning Are Multi-.pdf:application/pdf},
}

@misc{XuYLW23a,
	title = {{ExpertPrompting}: {Instructing} {Large} {Language} {Models} to be {Distinguished} {Experts}},
	shorttitle = {{ExpertPrompting}},
	url = {http://arxiv.org/abs/2305.14688},
	abstract = {The answering quality of an aligned large language model (LLM) can be drastically improved if treated with proper crafting of prompts. In this paper, we propose ExpertPrompting to elicit the potential of LLMs to answer as distinguished experts. We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background. Based on this augmented prompting strategy, we produce a new set of instruction-following data using GPT-3.5, and train a competitive open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) ExpertLLaMA outperforms existing open-source opponents and achieves 96\% of the original ChatGPT’s capability. All data and the ExpertLLaMA model will be made publicly available at https:// github.com/OFA-Sys/ExpertLLaMA.},
	language = {en},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Xu, Benfeng and Yang, An and Lin, Junyang and Wang, Quan and Zhou, Chang and Zhang, Yongdong and Mao, Zhendong},
	month = may,
	year = {2023},
	note = {arXiv: 2305.14688 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, !tr},
	file = {arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\YPRTANT6\\2305.html:text/html;XuYLW23--tr--expertprompting_instructing_large_language_models_to_be_distinguished_experts.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\9GWRK2TQ\\XuYLW23--tr--expertprompting_instructing_large_language_models_to_be_distinguished_experts.pdf:application/pdf},
}

@inproceedings{YinSCG23a,
	address = {Singapore},
	title = {Exchange-of-{Thought}: {Enhancing} {Large} {Language} {Model} {Capabilities} through {Cross}-{Model} {Communication}},
	shorttitle = {Exchange-of-{Thought}},
	url = {https://aclanthology.org/2023.emnlp-main.936},
	doi = {10.18653/v1/2023.emnlp-main.936},
	abstract = {Large Language Models (LLMs) have recently made significant strides in complex reasoning tasks through the Chain-of-Thought technique. Despite this progress, their reasoning is often constrained by their intrinsic understanding, lacking external insights. To address this, we propose Exchange-of-Thought (EoT), a novel framework that enables cross-model communication during problem-solving. Drawing inspiration from network topology, EoT integrates four unique communication paradigms: Memory, Report, Relay, and Debate. This paper delves into the communication dynamics and volume associated with each paradigm. To counterbalance the risks of incorrect reasoning chains, we implement a robust confidence evaluation mechanism within these communications. Our experiments across diverse complex reasoning tasks demonstrate that EoT significantly surpasses established baselines, underscoring the value of external insights in enhancing LLM performance. Furthermore, we show that EoT achieves these superior results in a cost-effective manner, marking a promising advancement for efficient and collaborative AI problem-solving.},
	urldate = {2024-06-19},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Yin, Zhangyue and Sun, Qiushi and Chang, Cheng and Guo, Qipeng and Dai, Junqi and Huang, Xuanjing and Qiu, Xipeng},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {15135--15153},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\BAXC3R4J\\Yin et al. - 2023 - Exchange-of-Thought Enhancing Large Language Mode.pdf:application/pdf},
}

@misc{ZhaoHXL23a,
	title = {{ExpeL}: {LLM} {Agents} {Are} {Experiential} {Learners}},
	shorttitle = {{ExpeL}},
	url = {http://arxiv.org/abs/2308.10144},
	abstract = {The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model’s generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.},
	language = {en},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Zhao, Andrew and Huang, Daniel and Xu, Quentin and Lin, Matthieu and Liu, Yong-Jin and Huang, Gao},
	month = aug,
	year = {2023},
	note = {arXiv:2308.10144 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, !tr, Computer Science - Machine Learning},
	file = {ZhaoHXL23--tr--expel_llm_agents_are_experiential_learners.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\QW3URBS7\\ZhaoHXL23--tr--expel_llm_agents_are_experiential_learners.pdf:application/pdf},
}

