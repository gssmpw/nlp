
@article{ZhuangYWS23a,
	title = {{ToolQA}: {A} {Dataset} for {LLM} {Question} {Answering} with {External} {Tools}},
	volume = {36},
	shorttitle = {{ToolQA}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/9cb2a7495900f8b602cb10159246a016-Abstract-Datasets_and_Benchmarks.html},
	language = {en},
	urldate = {2024-08-19},
	journal = {Advances in Neural Information Processing Systems},
	author = {Zhuang, Yuchen and Yu, Yue and Wang, Kuan and Sun, Haotian and Zhang, Chao},
	month = dec,
	year = {2023},
	keywords = {⛔ No DOI found},
	pages = {50117--50143},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\364EMG3B\\Zhuang et al. - 2023 - ToolQA A Dataset for LLM Question Answering with .pdf:application/pdf},
}

@inproceedings{ZhangS14,
	title = {Fairness in {Multi}-{Agent} {Sequential} {Decision}-{Making}},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/hash/792c7b5aae4a79e78aaeda80516ae2ac-Abstract.html},
	abstract = {We define a fairness solution criterion for multi-agent decision-making problems, where agents have local interests. This new criterion aims to maximize the worst performance of agents with consideration on the overall performance. We develop a simple linear programming approach and a more scalable game-theoretic approach for computing an optimal fairness policy. This game-theoretic approach formulates this fairness optimization as a two-player, zero-sum game and employs an iterative algorithm for finding a Nash equilibrium, corresponding to an optimal fairness policy. We scale up this approach by exploiting problem structure and value function approximation. Our experiments on resource allocation problems show that this fairness criterion provides a more favorable solution than the utilitarian criterion, and that our game-theoretic approach is significantly faster than linear programming.},
	urldate = {2024-08-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Chongjie and Shah, Julie A},
	year = {2014},
	keywords = {⛔ No DOI found},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\B6IDC7WI\\Zhang und Shah - 2014 - Fairness in Multi-Agent Sequential Decision-Making.pdf:application/pdf},
}

@article{XieKZZ23a,
	title = {Self-{Evaluation} {Guided} {Beam} {Search} for {Reasoning}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/81fde95c4dc79188a69ce5b24d63010b-Abstract-Conference.html},
	language = {en},
	urldate = {2024-09-20},
	journal = {Advances in Neural Information Processing Systems},
	author = {Xie, Yuxi and Kawaguchi, Kenji and Zhao, Yiran and Zhao, James Xu and Kan, Min-Yen and He, Junxian and Xie, Michael},
	month = dec,
	year = {2023},
	keywords = {⛔ No DOI found},
	pages = {41618--41650},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\FUW7RVPY\\Xie et al. - 2023 - Self-Evaluation Guided Beam Search for Reasoning.pdf:application/pdf},
}

@article{Meinke23,
	title = {{LLMs} can {Spontaneously} start {Jailbreaking} their {Scoring} {Function}},
	language = {en},
	author = {Meinke, Alexander},
	year = {2023},
	keywords = {⛔ No DOI found},
	file = {Meinke - 2023 - LLMs can Spontaneously start Jailbreaking their Sc.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\BD6CNRY8\\Meinke - 2023 - LLMs can Spontaneously start Jailbreaking their Sc.pdf:application/pdf},
}

@book{Cochran53,
	address = {Oxford, England},
	series = {Sampling techniques},
	title = {Sampling techniques},
	abstract = {Simple random sampling; sampling for proportions and percentages; the estimation of sample size; stratified random sampling; ratio estimates; regression estimates; systematic sampling; type of sampling unit; subsampling with units of equal size; subsampling with units of unequal size; double sampling; and sources of error in surveys are discussed in considerable detail. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	publisher = {John Wiley},
	author = {Cochran, William G.},
	year = {1953},
	note = {Pages: xiv, 330},
	file = {Cochran_1977_Sampling Techniques.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\CQG64Y6V\\Cochran_1977_Sampling Techniques.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\4RFTCCV4\\1954-00116-000.html:text/html},
}

@misc{KohMFS24,
	title = {Tree {Search} for {Language} {Model} {Agents}},
	url = {http://arxiv.org/abs/2407.01476},
	abstract = {Autonomous agents powered by language models (LMs) have demonstrated promise in their ability to perform decision-making tasks such as web automation. However, a key limitation remains: LMs, primarily optimized for natural language understanding and generation, struggle with multi-step reasoning, planning, and using environmental feedback when attempting to solve realistic computer tasks. Towards addressing this, we propose an inference-time search algorithm for LM agents to explicitly perform exploration and multi-step planning in interactive web environments. Our approach is a form of best-first tree search that operates within the actual environment space, and is complementary with most existing state-ofthe-art agents. It is the first tree search algorithm for LM agents that shows effectiveness on realistic web tasks. On the challenging VisualWebArena benchmark, applying our search algorithm on top of a GPT-4o agent yields a 39.7\% relative increase in success rate compared to the same baseline without search, setting a state-of-the-art success rate of 26.4\%. On WebArena, search also yields a 28.0\% relative improvement over a baseline agent, setting a competitive success rate of 19.2\%. Our experiments highlight the effectiveness of search for web agents, and we demonstrate that performance scales with increased test-time compute. We conduct a thorough analysis of our results to highlight improvements from search, limitations, and promising directions for future work. Our code and models are publicly released at jykoh.com/search-agents.},
	language = {en},
	urldate = {2024-09-20},
	publisher = {arXiv},
	author = {Koh, Jing Yu and McAleer, Stephen and Fried, Daniel and Salakhutdinov, Ruslan},
	month = jul,
	year = {2024},
	note = {arXiv:2407.01476 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: 11 pages. Models and code available at https://jykoh.com/search-agents},
	file = {2407.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\INGPMBET\\2407.pdf:application/pdf},
}

@misc{ZhouYSW24a,
	title = {Language {Agent} {Tree} {Search} {Unifies} {Reasoning} {Acting} and {Planning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2310.04406},
	abstract = {While language models (LMs) have shown potential across a range of decision-making tasks, their reliance on simple acting processes limits their broad deployment as autonomous agents. In this paper, we introduce Language Agent Tree Search (LATS) – the first general framework that synergizes the capabilities of LMs in reasoning, acting, and planning. By leveraging the in-context learning ability of LMs, we integrate Monte Carlo Tree Search into LATS to enable LMs as agents, along with LM-powered value functions and self-reflections for proficient exploration and enhanced decision-making. A key feature of our approach is the incorporation of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that surpasses the constraints of existing techniques. Our experimental evaluation across diverse domains, including programming, interactive questionanswering (QA), web navigation, and math, validates the effectiveness and generality of LATS in decision-making while maintaining competitive or improved reasoning performance. Notably, LATS achieves state-of-the-art pass@1 accuracy (92.7\%) for programming on HumanEval with GPT-4 and demonstrates gradient-free performance (average score of 75.9) comparable to gradient-based fine-tuning for web navigation on WebShop with GPT-3.5. Code can be found at https://github.com/lapisrocks/ LanguageAgentTreeSearch.},
	language = {en},
	urldate = {2024-09-20},
	publisher = {arXiv},
	author = {Zhou, Andy and Yan, Kai and Shlapentokh-Rothman, Michal and Wang, Haohan and Wang, Yu-Xiong},
	month = jun,
	year = {2024},
	note = {arXiv:2310.04406 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Code at https://github.com/lapisrocks/LanguageAgentTreeSearch},
	file = {2310.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\N3VWM5C7\\2310.pdf:application/pdf},
}

@misc{HuMYD24,
	title = {Tree-{Planner}: {Efficient} {Close}-loop {Task} {Planning} with {Large} {Language} {Models}},
	shorttitle = {Tree-{Planner}},
	url = {http://arxiv.org/abs/2310.08582},
	abstract = {This paper studies close-loop task planning, which refers to the process of generating a sequence of skills (a plan) to accomplish a specific goal while adapting the plan based on real-time observations. Recently, prompting Large Language Models (LLMs) to generate actions iteratively has become a prevalent paradigm due to its superior performance and user-friendliness. However, this paradigm is plagued by two inefficiencies: high token consumption and redundant error correction, both of which hinder its scalability for large-scale testing and applications. To address these issues, we propose TREE-PLANNER, which reframes task planning with LLMs into three distinct phases: plan sampling, action tree construction, and grounded deciding. TREE-PLANNER starts by using an LLM to sample a set of potential plans before execution, followed by the aggregation of them to form an action tree. Finally, the LLM performs a top-down decision-making process on the tree, taking into account real-time environmental information. Experiments show that TREE-PLANNER achieves state-of-the-art performance while maintaining high efficiency. By decomposing LLM queries into a single plan-sampling call and multiple grounded-deciding calls, a considerable part of the prompt are less likely to be repeatedly consumed. As a result, token consumption is reduced by 92.2\% compared to the previously best-performing model. Additionally, by enabling backtracking on the action tree as needed, the correction process becomes more flexible, leading to a 40.5\% decrease in error corrections.},
	language = {en},
	urldate = {2024-09-20},
	publisher = {arXiv},
	author = {Hu, Mengkang and Mu, Yao and Yu, Xinmiao and Ding, Mingyu and Wu, Shiguang and Shao, Wenqi and Chen, Qiguang and Wang, Bin and Qiao, Yu and Luo, Ping},
	month = jul,
	year = {2024},
	note = {arXiv:2310.08582 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
	annote = {Comment: Published in ICLR 2024},
	file = {Hu et al. - 2024 - Tree-Planner Efficient Close-loop Task Planning w.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\QHK8W383\\Hu et al. - 2024 - Tree-Planner Efficient Close-loop Task Planning w.pdf:application/pdf},
}

@misc{ChenWMP24,
	title = {When is {Tree} {Search} {Useful} for {LLM} {Planning}? {It} {Depends} on the {Discriminator}},
	shorttitle = {When is {Tree} {Search} {Useful} for {LLM} {Planning}?},
	url = {http://arxiv.org/abs/2402.10890},
	abstract = {In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90\% accuracy to achieve significant improvements over re-ranking; (2) current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compared to the other two methods, tree search is at least 10--20 times slower but leads to negligible performance gains, which hinders its real-world applications. Code and data are available at https://github.com/OSU-NLP-Group/llm-planning-eval.},
	language = {en},
	urldate = {2024-09-20},
	publisher = {arXiv},
	author = {Chen, Ziru and White, Michael and Mooney, Raymond and Payani, Ali and Su, Yu and Sun, Huan},
	month = jun,
	year = {2024},
	note = {arXiv:2402.10890 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: ACL 2024 main},
	file = {Chen et al. - 2024 - When is Tree Search Useful for LLM Planning It De.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\XHEYI8VW\\Chen et al. - 2024 - When is Tree Search Useful for LLM Planning It De.pdf:application/pdf},
}

@inproceedings{GehmanGSC20d,
	address = {Online},
	title = {{RealToxicityPrompts}: {Evaluating} {Neural} {Toxic} {Degeneration} in {Language} {Models}},
	shorttitle = {{RealToxicityPrompts}},
	url = {https://aclanthology.org/2020.findings-emnlp.301},
	doi = {10.18653/v1/2020.findings-emnlp.301},
	abstract = {Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning “bad” words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.},
	urldate = {2024-09-13},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A.},
	editor = {Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {3356--3369},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\GM6LF7QH\\Gehman et al. - 2020 - RealToxicityPrompts Evaluating Neural Toxic Degen.pdf:application/pdf},
}

@inproceedings{LevyLS21,
	address = {Punta Cana, Dominican Republic},
	title = {Collecting a {Large}-{Scale} {Gender} {Bias} {Dataset} for {Coreference} {Resolution} and {Machine} {Translation}},
	url = {https://aclanthology.org/2021.findings-emnlp.211},
	doi = {10.18653/v1/2021.findings-emnlp.211},
	abstract = {Recent works have found evidence of gender bias in models of machine translation and coreference resolution using mostly synthetic diagnostic datasets. While these quantify bias in a controlled experiment, they often do so on a small scale and consist mostly of artificial, out-of-distribution sentences. In this work, we find grammatical patterns indicating stereotypical and non-stereotypical gender-role assignments (e.g., female nurses versus male dancers) in corpora from three domains, resulting in a first large-scale gender bias dataset of 108K diverse real-world English sentences. We manually verify the quality of our corpus and use it to evaluate gender bias in various coreference resolution and machine translation models. We find that all tested models tend to over-rely on gender stereotypes when presented with natural inputs, which may be especially harmful when deployed in commercial systems. Finally, we show that our dataset lends itself to finetuning a coreference resolution model, finding it mitigates bias on a held out set. Our dataset and models are publicly available at github.com/SLAB-NLP/BUG. We hope they will spur future research into gender bias evaluation mitigation techniques in realistic settings.},
	urldate = {2024-09-13},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Levy, Shahar and Lazar, Koren and Stanovsky, Gabriel},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {2470--2480},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\SZPURMGG\\Levy et al. - 2021 - Collecting a Large-Scale Gender Bias Dataset for C.pdf:application/pdf},
}

@inproceedings{MukobiRRS23,
	title = {Assessing {Risks} of {Using} {Autonomous} {Language} {Models} in {Military} and {Diplomatic} {Planning}},
	url = {https://openreview.net/forum?id=5HuBX8LvuT&utm_source=updates.apartresearch.com&utm_medium=referral&utm_campaign=apart-s-2023-wrapping-up-a-great-year},
	abstract = {The potential integration of autonomous agents in high-stakes military and foreign-policy decision-making has gained prominence, especially with the emergence of advanced generative AI models like GPT-4. This paper aims to scrutinize the behavior of multiple autonomous agents in simulated military and diplomacy scenarios, specifically focusing on their potential to escalate conflicts. Drawing on established international relations frameworks, we assessed the escalation potential of decisions made by these agents in different scenarios. Contrary to prior qualitative studies, our research provides both qualitative and quantitative insights. We find that there are significant differences in the models' predilections to escalate, with Claude 2 being the least aggressive and GPT-4-Base the most aggressive models. Our findings indicate that, even in seemingly neutral contexts, language-model-based autonomous agents occasionally opt for aggressive or provocative actions. This tendency intensifies in scenarios with predefined trigger events. Importantly, the patterns behind such escalatory behavior remain largely unpredictable. Furthermore, a qualitative analysis of the models' verbalized reasoning, particularly in the GPT-4-Base model, reveals concerning justifications. Given the high stakes involved in military and foreign-policy contexts, the deployment of such autonomous agents demands further examination and cautious consideration.},
	language = {en},
	urldate = {2024-09-13},
	author = {Mukobi, Gabriel and Reuel, Ann-Katrin and Rivera, Juan-Pablo and Smith, Chandler},
	month = oct,
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\BU7MJ9VJ\\Mukobi et al. - 2023 - Assessing Risks of Using Autonomous Language Model.pdf:application/pdf},
}

@inproceedings{LiGBG16b,
	address = {San Diego, California},
	title = {A {Diversity}-{Promoting} {Objective} {Function} for {Neural} {Conversation} {Models}},
	url = {https://aclanthology.org/N16-1014},
	doi = {10.18653/v1/N16-1014},
	urldate = {2024-09-13},
	booktitle = {Proceedings of the 2016 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill},
	editor = {Knight, Kevin and Nenkova, Ani and Rambow, Owen},
	month = jun,
	year = {2016},
	pages = {110--119},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\Y8Y4VHTD\\Li et al. - 2016 - A Diversity-Promoting Objective Function for Neura.pdf:application/pdf},
}

@article{Spearman04b,
	title = {The {Proof} and {Measurement} of {Association} between {Two} {Things}},
	volume = {15},
	issn = {00029556},
	url = {https://www.jstor.org/stable/1412159?origin=crossref},
	doi = {10.2307/1412159},
	language = {en},
	number = {1},
	urldate = {2024-09-10},
	journal = {The American Journal of Psychology},
	author = {Spearman, C.},
	month = jan,
	year = {1904},
	pages = {72},
	file = {Spearman - 1904 - The Proof and Measurement of Association between T.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\DN8SNETY\\Spearman - 1904 - The Proof and Measurement of Association between T.pdf:application/pdf},
}

@inproceedings{ChenFYW23,
	address = {Birmingham United Kingdom},
	title = {Hallucination {Detection}: {Robustly} {Discerning} {Reliable} {Answers} in {Large} {Language} {Models}},
	isbn = {979-8-4007-0124-5},
	shorttitle = {Hallucination {Detection}},
	url = {https://dl.acm.org/doi/10.1145/3583780.3614905},
	doi = {10.1145/3583780.3614905},
	language = {en},
	urldate = {2024-09-10},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {ACM},
	author = {Chen, Yuyan and Fu, Qiang and Yuan, Yichen and Wen, Zhihao and Fan, Ge and Liu, Dayiheng and Zhang, Dongmei and Li, Zhixu and Xiao, Yanghua},
	month = oct,
	year = {2023},
	pages = {245--255},
}

@inproceedings{LiCSC23a,
	address = {Singapore},
	title = {Theory of {Mind} for {Multi}-{Agent} {Collaboration} via {Large} {Language} {Models}},
	url = {https://aclanthology.org/2023.emnlp-main.13},
	doi = {10.18653/v1/2023.emnlp-main.13},
	abstract = {While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.},
	urldate = {2024-07-17},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Huao and Chong, Yu and Stepputtis, Simon and Campbell, Joseph and Hughes, Dana and Lewis, Charles and Sycara, Katia},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {180--192},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\A26INITA\\Li et al. - 2023 - Theory of Mind for Multi-Agent Collaboration via L.pdf:application/pdf},
}

@misc{ZengWZW24,
	title = {{AutoDefense}: {Multi}-{Agent} {LLM} {Defense} against {Jailbreak} {Attacks}},
	shorttitle = {{AutoDefense}},
	url = {http://arxiv.org/abs/2403.04783},
	abstract = {Despite extensive pre-training and fine-tuning in moral alignment to prevent generating harmful information at user request, large language models (LLMs) remain vulnerable to jailbreak attacks. In this paper, we propose AutoDefense, a response-filtering based multiagent defense framework that filters harmful responses from LLMs. This framework assigns different roles to LLM agents and employs them to complete the defense task collaboratively. The division in tasks enhances the overall instruction-following of LLMs and enables the integration of other defense components as tools. AutoDefense can adapt to various sizes and kinds of open-source LLMs that serve as agents. Through conducting extensive experiments on a large scale of harmful and safe prompts, we validate the effectiveness of the proposed AutoDefense in improving the robustness against jailbreak attacks, while maintaining the performance at normal user request. Our code and data are publicly available at https://github.com/XHMY/AutoDefense.},
	language = {en},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Zeng, Yifan and Wu, Yiran and Zhang, Xiao and Wang, Huazheng and Wu, Qingyun},
	month = mar,
	year = {2024},
	note = {arXiv:2403.04783 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {Zeng et al. - 2024 - AutoDefense Multi-Agent LLM Defense against Jailb.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\CW2JXYCJ\\Zeng et al. - 2024 - AutoDefense Multi-Agent LLM Defense against Jailb.pdf:application/pdf},
}

@misc{YangDKH24,
	title = {{LLM} {Voting}: {Human} {Choices} and {AI} {Collective} {Decision} {Making}},
	shorttitle = {{LLM} {Voting}},
	url = {http://arxiv.org/abs/2402.01766},
	abstract = {This paper investigates the voting behaviors of Large Language Models (LLMs), specifically GPT-4 and LLaMA-2, their biases, and how they align with human voting patterns. Our methodology involved using a dataset from a human voting experiment to establish a baseline for human preferences and a corresponding experiment with LLM agents. We observed that the methods used for voting input and the presentation of choices influence LLM voting behavior. We discovered that varying the persona can reduce some of these biases and enhance alignment with human choices. While the Chainof-Thought approach did not improve prediction accuracy, it has potential for AI explainability in the voting process. We also identified a trade-off between preference diversity and alignment accuracy in LLMs, influenced by different temperature settings. Our findings indicate that LLMs may lead to less diverse collective outcomes and biased assumptions when used in voting scenarios, emphasizing the importance of cautious integration of LLMs into democratic processes.},
	language = {en},
	urldate = {2024-07-15},
	publisher = {arXiv},
	author = {Yang, Joshua C. and Dailisan, Damian and Korecki, Marcin and Hausladen, Carina I. and Helbing, Dirk},
	month = may,
	year = {2024},
	note = {arXiv:2402.01766 [cs, econ, q-fin]},
	keywords = {Computer Science - Computation and Language, I.2.7, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computers and Society, 68T05, 91B14, 91C20, Economics - General Economics, J.4, K.4.1},
	annote = {Comment: Submitted to AIES2024},
	file = {Yang et al. - 2024 - LLM Voting Human Choices and AI Collective Decisi.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\HBRSTR7A\\Yang et al. - 2024 - LLM Voting Human Choices and AI Collective Decisi.pdf:application/pdf},
}

@misc{ChenSB24a,
	title = {{ReConcile}: {Round}-{Table} {Conference} {Improves} {Reasoning} via {Consensus} among {Diverse} {LLMs}},
	shorttitle = {{ReConcile}},
	url = {http://arxiv.org/abs/2309.13007},
	abstract = {Large Language Models (LLMs) still struggle with natural language reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents. ReConcile enhances collaborative reasoning between LLM agents via multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism that leads to a better consensus. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their confidence scores, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. Experiments on seven benchmarks demonstrate that ReConcile significantly improves LLMs' reasoning -- both individually and as a team -- surpassing prior single-agent and multi-agent baselines by up to 11.4\% and even outperforming GPT-4 on three datasets. ReConcile also flexibly incorporates different combinations of agents, including API-based, open-source, and domain-specific models, leading to an 8\% improvement on MATH. Finally, we analyze the individual components of ReConcile, demonstrating that the diversity originating from different models is critical to its superior performance. Code: https://github.com/dinobby/ReConcile},
	language = {en},
	urldate = {2024-07-15},
	publisher = {arXiv},
	author = {Chen, Justin Chih-Yao and Saha, Swarnadeep and Bansal, Mohit},
	month = jun,
	year = {2024},
	note = {arXiv:2309.13007 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: ACL 2024 (Camera-Ready)},
	file = {2309.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\NY67MAI4\\2309.pdf:application/pdf},
}

@misc{OpenAIAAA24a,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2024-06-26},
	publisher = {arXiv},
	author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and others},
	month = mar,
	year = {2024},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: 100 pages; updated authors list; fixed author names and added citation},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\QENMPE24\\OpenAI et al. - 2024 - GPT-4 Technical Report.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\A3FZY44Y\\2303.html:text/html},
}

@misc{WangWSL23,
	title = {Self-{Consistency} {Improves} {Chain} of {Thought} {Reasoning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2203.11171},
	abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It ﬁrst samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
	language = {en},
	urldate = {2024-06-20},
	publisher = {arXiv},
	author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
	month = mar,
	year = {2023},
	note = {arXiv:2203.11171 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: Published at ICLR 2023. V2: added PaLM results; V3: added UL2 results; V4: camera ready version at ICLR 2023},
	file = {2203.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\9HJ77DHK\\2203.pdf:application/pdf},
}

@misc{SuzgunK24a,
	title = {Meta-{Prompting}: {Enhancing} {Language} {Models} with {Task}-{Agnostic} {Scaffolding}},
	shorttitle = {Meta-{Prompting}},
	url = {http://arxiv.org/abs/2401.12954},
	doi = {10.48550/arXiv.2401.12954},
	abstract = {We introduce meta-prompting, an effective scaffolding technique designed to enhance the functionality of language models (LMs). This approach transforms a single LM into a multi-faceted conductor, adept at managing and integrating multiple independent LM queries. By employing high-level instructions, meta-prompting guides the LM to break down complex tasks into smaller, more manageable subtasks. These subtasks are then handled by distinct "expert" instances of the same LM, each operating under specific, tailored instructions. Central to this process is the LM itself, in its role as the conductor, which ensures seamless communication and effective integration of the outputs from these expert models. It additionally employs its inherent critical thinking and robust verification processes to refine and authenticate the end result. This collaborative prompting approach empowers a single LM to simultaneously act as a comprehensive orchestrator and a panel of diverse experts, significantly enhancing its performance across a wide array of tasks. The zero-shot, task-agnostic nature of meta-prompting greatly simplifies user interaction by obviating the need for detailed, task-specific instructions. Furthermore, our research demonstrates the seamless integration of external tools, such as a Python interpreter, into the meta-prompting framework, thereby broadening its applicability and utility. Through rigorous experimentation with GPT-4, we establish the superiority of meta-prompting over conventional scaffolding methods: When averaged across all tasks, including the Game of 24, Checkmate-in-One, and Python Programming Puzzles, meta-prompting, augmented with a Python interpreter functionality, surpasses standard prompting by 17.1\%, expert (dynamic) prompting by 17.3\%, and multipersona prompting by 15.2\%.},
	urldate = {2024-06-20},
	publisher = {arXiv},
	author = {Suzgun, Mirac and Kalai, Adam Tauman},
	month = jan,
	year = {2024},
	note = {arXiv:2401.12954 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	annote = {Comment: https://github.com/suzgunmirac/meta-prompting},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\LKS4L9KM\\Suzgun und Kalai - 2024 - Meta-Prompting Enhancing Language Models with Tas.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\L5SSFRS4\\2401.html:text/html},
}

@inproceedings{YinSCG23a,
	address = {Singapore},
	title = {Exchange-of-{Thought}: {Enhancing} {Large} {Language} {Model} {Capabilities} through {Cross}-{Model} {Communication}},
	shorttitle = {Exchange-of-{Thought}},
	url = {https://aclanthology.org/2023.emnlp-main.936},
	doi = {10.18653/v1/2023.emnlp-main.936},
	abstract = {Large Language Models (LLMs) have recently made significant strides in complex reasoning tasks through the Chain-of-Thought technique. Despite this progress, their reasoning is often constrained by their intrinsic understanding, lacking external insights. To address this, we propose Exchange-of-Thought (EoT), a novel framework that enables cross-model communication during problem-solving. Drawing inspiration from network topology, EoT integrates four unique communication paradigms: Memory, Report, Relay, and Debate. This paper delves into the communication dynamics and volume associated with each paradigm. To counterbalance the risks of incorrect reasoning chains, we implement a robust confidence evaluation mechanism within these communications. Our experiments across diverse complex reasoning tasks demonstrate that EoT significantly surpasses established baselines, underscoring the value of external insights in enhancing LLM performance. Furthermore, we show that EoT achieves these superior results in a cost-effective manner, marking a promising advancement for efficient and collaborative AI problem-solving.},
	urldate = {2024-06-19},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Yin, Zhangyue and Sun, Qiushi and Chang, Cheng and Guo, Qipeng and Dai, Junqi and Huang, Xuanjing and Qiu, Xipeng},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {15135--15153},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\BAXC3R4J\\Yin et al. - 2023 - Exchange-of-Thought Enhancing Large Language Mode.pdf:application/pdf},
}

@misc{QiaoFZZ24,
	title = {Agent {Planning} with {World} {Knowledge} {Model}},
	url = {http://arxiv.org/abs/2405.14205},
	abstract = {Recent endeavors towards directly using large language models (LLMs) as agent models to execute interactive planning tasks have shown commendable results. Despite their achievements, however, they still struggle with brainless trial-and-error in global planning and generating hallucinatory actions in local planning due to their poor understanding of the ''real'' physical world. Imitating humans' mental world knowledge model which provides global prior knowledge before the task and maintains local dynamic knowledge during the task, in this paper, we introduce parametric World Knowledge Model (WKM) to facilitate agent planning. Concretely, we steer the agent model to self-synthesize knowledge from both expert and sampled trajectories. Then we develop WKM, providing prior task knowledge to guide the global planning and dynamic state knowledge to assist the local planning. Experimental results on three complex real-world simulated datasets with three state-of-the-art open-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our method can achieve superior performance compared to various strong baselines. Besides, we analyze to illustrate that our WKM can effectively alleviate the blind trial-and-error and hallucinatory action issues, providing strong support for the agent's understanding of the world. Other interesting findings include: 1) our instance-level task knowledge can generalize better to unseen tasks, 2) weak WKM can guide strong agent model planning, and 3) unified WKM training has promising potential for further development. Code will be available at https://github.com/zjunlp/WKM.},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Qiao, Shuofei and Fang, Runnan and Zhang, Ningyu and Zhu, Yuqi and Chen, Xiang and Deng, Shumin and Jiang, Yong and Xie, Pengjun and Huang, Fei and Chen, Huajun},
	month = may,
	year = {2024},
	note = {arXiv:2405.14205 [cs]},
	annote = {Comment: Work in progress},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\K8LPHXZ6\\Qiao et al. - 2024 - Agent Planning with World Knowledge Model.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\K6SMA8YD\\2405.html:text/html},
}

@misc{DuLTT23,
	title = {Improving {Factuality} and {Reasoning} in {Language} {Models} through {Multiagent} {Debate}},
	url = {http://arxiv.org/abs/2305.14325},
	abstract = {Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such "society of minds" approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B. and Mordatch, Igor},
	month = may,
	year = {2023},
	note = {arXiv:2305.14325 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Project Webpage and Code: https://composable-models.github.io/llm\_debate/},
	file = {arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\36NCDYYW\\2305.html:text/html;Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\KJILHJQZ\\Du et al. - 2023 - Improving Factuality and Reasoning in Language Mod.pdf:application/pdf},
}

@misc{FuPKL23b,
	title = {Improving {Language} {Model} {Negotiation} with {Self}-{Play} and {In}-{Context} {Learning} from {AI} {Feedback}},
	url = {http://arxiv.org/abs/2305.10142},
	abstract = {We study whether multiple large language models (LLMs) can autonomously improve each other in a negotiation game by playing, reﬂecting, and criticizing. We are interested in this question because if LLMs were able to improve each other, it would imply the possibility of creating strong AI agents with minimal human intervention. We ask two LLMs to negotiate with each other, playing the roles of a buyer and a seller, respectively. They aim to reach a deal with the buyer targeting a lower price and the seller a higher one. A third language model, playing the critic, provides feedback to a player to improve the player’s negotiation strategies. We let the two agents play multiple rounds, using previous negotiation history and AI feedback as in-context demonstrations to improve the model’s negotiation strategy iteratively. We use different LLMs (GPT and Claude) for different roles and use the deal price as the evaluation metric. Our experiments reveal multiple intriguing ﬁndings: (1) Only a subset of the language models we consider can self-play and improve the deal price from AI feedback, weaker models either do not understand the game’s rules or cannot incorporate AI feedback for further improvement. (2) Models’ abilities to learn from the feedback differ when playing different roles. For example, it is harder for Claude-instant to improve as the buyer than as the seller. (3) When unrolling the game to multiple rounds, stronger agents can consistently improve their performance by meaningfully using previous experiences and iterative AI feedback, yet have a higher risk of breaking the deal. We hope our work provides insightful initial explorations of having models autonomously improve each other with game playing and AI feedback.},
	language = {en},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Fu, Yao and Peng, Hao and Khot, Tushar and Lapata, Mirella},
	month = may,
	year = {2023},
	note = {arXiv:2305.10142 [cs]},
	keywords = {Computer Science - Computation and Language, !tr},
	annote = {Comment: Preprint. Code at https://github.com/FranxYao/GPT-Bargaining},
	file = {FuPKL23--tr--improving_language_model_negotiation_with_self-play_and_in-context_learning_from.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\I2UQAR27\\FuPKL23--tr--improving_language_model_negotiation_with_self-play_and_in-context_learning_from.pdf:application/pdf},
}

@misc{ParkOCM23a,
	title = {Generative {Agents}: {Interactive} {Simulacra} of {Human} {Behavior}},
	shorttitle = {Generative {Agents}},
	url = {http://arxiv.org/abs/2304.03442},
	abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
	language = {en},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
	month = aug,
	year = {2023},
	note = {arXiv:2304.03442 [cs]},
	keywords = {Computer Science - Artificial Intelligence, !tr, Computer Science - Machine Learning, Computer Science - Human-Computer Interaction},
	file = {ParkOCM23--tr--generative_agents_interactive_simulacra_of_human_behavior.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\JMHKCX9J\\ParkOCM23--tr--generative_agents_interactive_simulacra_of_human_behavior.pdf:application/pdf},
}

@misc{MadaanTGH23c,
	title = {Self-{Refine}: {Iterative} {Refinement} with {Self}-{Feedback}},
	shorttitle = {Self-{Refine}},
	url = {http://arxiv.org/abs/2303.17651},
	abstract = {Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce SELF-REFINE, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLM; then, the same LLM provides feedback for its output and uses it to refine itself, iteratively. SELF-REFINE does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner and the feedback provider. We evaluate SELF-REFINE across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5 and GPT-4) LLMs. Across all evaluated tasks, outputs generated with SELF-REFINE are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ∼20\% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test-time using our simple, standalone approach.1.},
	language = {en},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Gupta, Shashank and Majumder, Bodhisattwa Prasad and Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir and Clark, Peter},
	month = may,
	year = {2023},
	note = {arXiv:2303.17651 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, !tr, Computer Science - Machine Learning},
	annote = {Comment: Code, data, and demo at https://selfrefine.info/},
	file = {MadaanTGH23a--tr--self-refine_iterative_refinement_with_self-feedback.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\K4P6W2IX\\MadaanTGH23a--tr--self-refine_iterative_refinement_with_self-feedback.pdf:application/pdf},
}

@misc{WangMFZ23a,
	title = {A {Survey} on {Large} {Language} {Model} based {Autonomous} {Agents}},
	url = {http://arxiv.org/abs/2308.11432},
	abstract = {Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this ﬁeld often focuses on training agents with limited knowledge within isolated environments, which diverges signiﬁcantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the ﬁeld of LLM-based autonomous agents from a holistic perspective. More specifically, we ﬁrst discuss the construction of LLM-based autonomous agents, for which we propose a uniﬁed framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the ﬁelds of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this ﬁeld. To keep track of this ﬁeld and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.},
	language = {en},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Ji-Rong},
	month = sep,
	year = {2023},
	note = {arXiv:2308.11432 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, !tr},
	annote = {Comment: 35 pages, 5 figures, 3 tables},
	file = {WangMFZ23--tr--a_survey_on_large_language_model_based_autonomous_agents.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\3JYJFQFZ\\WangMFZ23--tr--a_survey_on_large_language_model_based_autonomous_agents.pdf:application/pdf},
}

@misc{SchickDJP22a,
	title = {{PEER}: {A} {Collaborative} {Language} {Model}},
	shorttitle = {{PEER}},
	url = {http://arxiv.org/abs/2208.11663},
	abstract = {Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today’s language models are trained to generate only the ﬁnal result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difﬁcult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to inﬁll various parts of the writing process, enabling the use of selftraining techniques for increasing the quality, amount and diversity of training data. This unlocks PEER’s full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.},
	language = {en},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Schick, Timo and Dwivedi-Yu, Jane and Jiang, Zhengbao and Petroni, Fabio and Lewis, Patrick and Izacard, Gautier and You, Qingfei and Nalmpantis, Christoforos and Grave, Edouard and Riedel, Sebastian},
	month = aug,
	year = {2022},
	note = {arXiv:2208.11663 [cs]},
	keywords = {Computer Science - Computation and Language, !tr},
	file = {SchickDJP22--tr--peer_a_collaborative_language_model.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\H5HE5W3M\\SchickDJP22--tr--peer_a_collaborative_language_model.pdf:application/pdf},
}

@misc{AherAK23a,
	title = {Using {Large} {Language} {Models} to {Simulate} {Multiple} {Humans} and {Replicate} {Human} {Subject} {Studies}},
	url = {http://arxiv.org/abs/2208.10264},
	abstract = {We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model’s simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a “hyper-accuracy distortion” present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.},
	language = {en},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Aher, Gati and Arriaga, Rosa I. and Kalai, Adam Tauman},
	month = jul,
	year = {2023},
	note = {arXiv:2208.10264 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, !tr, Computer Science - Machine Learning},
	annote = {Comment: Accepted for oral presentation at International Conference on Machine Learning (ICML) 2023},
	file = {AherAK23--tr--using_large_language_models_to_simulate_multiple_humans_and_replicate_human_subject.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\GLJQ6FAP\\AherAK23--tr--using_large_language_models_to_simulate_multiple_humans_and_replicate_human_subject.pdf:application/pdf},
}

@misc{ZhaoHXL23a,
	title = {{ExpeL}: {LLM} {Agents} {Are} {Experiential} {Learners}},
	shorttitle = {{ExpeL}},
	url = {http://arxiv.org/abs/2308.10144},
	abstract = {The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model’s generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.},
	language = {en},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Zhao, Andrew and Huang, Daniel and Xu, Quentin and Lin, Matthieu and Liu, Yong-Jin and Huang, Gao},
	month = aug,
	year = {2023},
	note = {arXiv:2308.10144 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, !tr, Computer Science - Machine Learning},
	file = {ZhaoHXL23--tr--expel_llm_agents_are_experiential_learners.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\QW3URBS7\\ZhaoHXL23--tr--expel_llm_agents_are_experiential_learners.pdf:application/pdf},
}

@misc{LiHIK23a,
	title = {{CAMEL}: {Communicative} {Agents} for "{Mind}" {Exploration} of {Large} {Language} {Model} {Society}},
	shorttitle = {{CAMEL}},
	url = {http://arxiv.org/abs/2303.17760},
	abstract = {The rapid advancement of chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents, and provides insight into their “cognitive” processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named roleplaying . Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of a society of agents, providing a valuable resource for investigating conversational language models. In particular, we conduct comprehensive studies on instruction-following cooperation in multi-agent settings. Our contributions include introducing a novel communicative agent framework, offering a scalable approach for studying the cooperative behaviors and capabilities of multi-agent systems, and open-sourcing our library to support research on communicative agents and beyond: https://github.com/camel-ai/camel.},
	language = {en},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Li, Guohao and Hammoud, Hasan Abed Al Kader and Itani, Hani and Khizbullin, Dmitrii and Ghanem, Bernard},
	month = nov,
	year = {2023},
	note = {arXiv:2303.17760 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, !tr, Computer Science - Machine Learning, Computer Science - Computers and Society, Computer Science - Multiagent Systems},
	annote = {Comment: Accepted at NeurIPS'2023, 77 pages, project website: https://www.camel-ai.org, github repository: https://github.com/camel-ai/camel},
	file = {LiHIK23--tr--camel_communicative_agents_for_mind_exploration_of_large_language_model_society.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\K6Z3WM4S\\LiHIK23--tr--camel_communicative_agents_for_mind_exploration_of_large_language_model_society.pdf:application/pdf},
}

@misc{LiuYZX23b,
	title = {{AgentBench}: {Evaluating} {LLMs} as {Agents}},
	shorttitle = {{AgentBench}},
	url = {http://arxiv.org/abs/2308.03688},
	abstract = {Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 27 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and OSS competitors. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Training on code and high quality multi-turn alignment data could improve agent performance. Datasets, environments, and an integrated evaluation package for AgentBench are released at {\textbackslash}url\{https://github.com/THUDM/AgentBench\}.},
	language = {en},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang and Men, Kaiwen and Yang, Kejuan and Zhang, Shudan and Deng, Xiang and Zeng, Aohan and Du, Zhengxiao and Zhang, Chenhui and Shen, Sheng and Zhang, Tianjun and Su, Yu and Sun, Huan and Huang, Minlie and Dong, Yuxiao and Tang, Jie},
	month = oct,
	year = {2023},
	note = {arXiv:2308.03688 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, !tr, Computer Science - Machine Learning},
	annote = {Comment: 55 pages},
	file = {LiuYZX23--tr--agentbench_evaluating_llms_as_agents.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\V5JEKV4F\\LiuYZX23--tr--agentbench_evaluating_llms_as_agents.pdf:application/pdf},
}

@misc{JungQWB22a,
	title = {Maieutic {Prompting}: {Logically} {Consistent} {Reasoning} with {Recursive} {Explanations}},
	shorttitle = {Maieutic {Prompting}},
	url = {http://arxiv.org/abs/2205.11822},
	abstract = {Despite their impressive capabilities, large pre-trained language models (LMs) struggle with consistent reasoning; recently, prompting LMs to generate explanations that self-guide the inference has emerged as a promising direction to amend this. However, these approaches are fundamentally bounded by the correctness of explanations, which themselves are often noisy and inconsistent. In this work, we develop Maieutic Prompting, which infers a correct answer to a question even from the noisy and inconsistent generations of LM. Maieutic Prompting induces a tree of explanations abductively (e.g. X is true, because ...) and recursively, then frames the inference as a satisfiability problem over these explanations and their logical relations. We test Maieutic Prompting for true/false QA on three challenging benchmarks that require complex commonsense reasoning. Maieutic Prompting achieves up to 20\% better accuracy than state-of-the-art prompting methods, and as a fully unsupervised approach, performs competitively with supervised models. We also show that Maieutic Prompting improves robustness in inference while providing interpretable rationales.},
	language = {en},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Jung, Jaehun and Qin, Lianhui and Welleck, Sean and Brahman, Faeze and Bhagavatula, Chandra and Bras, Ronan Le and Choi, Yejin},
	month = oct,
	year = {2022},
	note = {arXiv:2205.11822 [cs]},
	keywords = {Computer Science - Computation and Language, !tr},
	annote = {Comment: EMNLP 2022},
	file = {JungQWB22--tr--maieutic_prompting_logically_consistent_reasoning_with_recursive_explanations.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\B8IMTC4Z\\JungQWB22--tr--maieutic_prompting_logically_consistent_reasoning_with_recursive_explanations.pdf:application/pdf},
}

@misc{MaMBB23a,
	title = {Let's {Do} a {Thought} {Experiment}: {Using} {Counterfactuals} to {Improve} {Moral} {Reasoning}},
	shorttitle = {Let's {Do} a {Thought} {Experiment}},
	url = {http://arxiv.org/abs/2306.14308},
	abstract = {Language models still struggle on moral reasoning, despite their impressive performance in many other tasks. In particular, the Moral Scenarios task in MMLU (Multi-task Language Understanding) is among the worst performing tasks for many language models, including GPT-3. In this work, we propose a new prompting framework, THOUGHT EXPERIMENTS, to teach language models to do better moral reasoning using counterfactuals. Experiment results show that our framework elicits counterfactual questions and answers from the model, which in turn helps improve the accuracy on Moral Scenarios task by 9-16\% compared to other zero-shot baselines. Interestingly, unlike math reasoning tasks, zeroshot Chain-of-Thought (CoT) reasoning doesn’t work out of the box, and even reduces accuracy by around 4\% compared to direct zero-shot. We further observed that with minimal human supervision in the form of 5 few-shot examples, the accuracy of the task can be improved to as much as 80\%.},
	language = {en},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Ma, Xiao and Mishra, Swaroop and Beirami, Ahmad and Beutel, Alex and Chen, Jilin},
	month = jun,
	year = {2023},
	note = {arXiv:2306.14308 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, !tr},
	annote = {Comment: 8 pages, ICML Neural Conversational AI workshop, thought experiments, moral reasoning},
	file = {MaMBB23--tr--lets_do_a_thought_experiment_using_counterfactuals_to_improve_moral_reasoning.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\24RND2XS\\MaMBB23--tr--lets_do_a_thought_experiment_using_counterfactuals_to_improve_moral_reasoning.pdf:application/pdf},
}

@misc{AgasheFW23a,
	title = {Evaluating {Multi}-{Agent} {Coordination} {Abilities} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.03903},
	abstract = {A pivotal aim in contemporary AI research is to develop agents proficient in multi-agent coordination, enabling effective collaboration with both humans and other systems. Large Language Models (LLMs), with their notable ability to understand, generate, and interpret language in a human-like manner, stand out as promising candidates for the development of such agents. In this study, we build and assess the effectiveness of agents crafted using LLMs in various coordination scenarios. We introduce the LLM-Coordination (LLM-Co) Framework, specifically designed to enable LLMs to play coordination games. With the LLM-Co framework, we conduct our evaluation with three game environments and organize the evaluation into five aspects: Theory of Mind, Situated Reasoning, Sustained Coordination, Robustness to Partners, and Explicit Assistance. First, the evaluation of the Theory of Mind and Situated Reasoning reveals the capabilities of LLM to infer the partner's intention and reason actions accordingly. Then, the evaluation around Sustained Coordination and Robustness to Partners further showcases the ability of LLMs to coordinate with an unknown partner in complex long-horizon tasks, outperforming Reinforcement Learning baselines. Lastly, to test Explicit Assistance, which refers to the ability of an agent to offer help proactively, we introduce two novel layouts into the Overcooked-AI benchmark, examining if agents can prioritize helping their partners, sacrificing time that could have been spent on their tasks. This research underscores the promising capabilities of LLMs in sophisticated coordination environments and reveals the potential of LLMs in building strong real-world agents for multi-agent coordination.},
	urldate = {2023-11-24},
	publisher = {arXiv},
	author = {Agashe, Saaket and Fan, Yue and Wang, Xin Eric},
	month = oct,
	year = {2023},
	note = {arXiv:2310.03903 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Multiagent Systems},
	file = {arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\AH386DTA\\2310.html:text/html;Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\BHGYVVBF\\Agashe et al. - 2023 - Evaluating Multi-Agent Coordination Abilities in L.pdf:application/pdf},
}

@misc{LazaridouPT20a,
	title = {Multi-agent {Communication} meets {Natural} {Language}: {Synergies} between {Functional} and {Structural} {Language} {Learning}},
	shorttitle = {Multi-agent {Communication} meets {Natural} {Language}},
	url = {http://arxiv.org/abs/2005.07064},
	abstract = {We present a method for combining multi-agent communication and traditional data-driven approaches to natural language learning, with an end goal of teaching agents to communicate with humans in natural language. Our starting point is a language model that has been trained on generic, not task-specific language data. We then place this model in a multi-agent self-play environment that generates task-specific rewards used to adapt or modulate the model, turning it into a task-conditional language model. We introduce a new way for combining the two types of learning based on the idea of reranking language model samples, and show that this method outperforms others in communicating with humans in a visual referential communication task. Finally, we present a taxonomy of different types of language drift that can occur alongside a set of measures to detect them.},
	urldate = {2023-11-24},
	publisher = {arXiv},
	author = {Lazaridou, Angeliki and Potapenko, Anna and Tieleman, Olivier},
	month = may,
	year = {2020},
	note = {arXiv:2005.07064 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: to appear at ACL 2020},
	file = {arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\CC96VKM9\\2005.html:text/html;Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\LL8VUKV4\\Lazaridou et al. - 2020 - Multi-agent Communication meets Natural Language .pdf:application/pdf},
}

@misc{WahleRMM23,
	title = {{AI} {Usage} {Cards}: {Responsibly} {Reporting} {AI}-generated {Content}},
	shorttitle = {{AI} {Usage} {Cards}},
	url = {http://arxiv.org/abs/2303.03886},
	doi = {10.48550/arXiv.2303.03886},
	abstract = {Given AI systems like ChatGPT can generate content that is indistinguishable from human-made work, the responsible use of this technology is a growing concern. Although understanding the benefits and harms of using AI systems requires more time, their rapid and indiscriminate adoption in practice is a reality. Currently, we lack a common framework and language to define and report the responsible use of AI for content generation. Prior work proposed guidelines for using AI in specific scenarios (e.g., robotics or medicine) which are not transferable to conducting and reporting scientific research. Our work makes two contributions: First, we propose a three-dimensional model consisting of transparency, integrity, and accountability to define the responsible use of AI. Second, we introduce ``AI Usage Cards'', a standardized way to report the use of AI in scientific research. Our model and cards allow users to reflect on key principles of responsible AI usage. They also help the research community trace, compare, and question various forms of AI usage and support the development of accepted community norms. The proposed framework and reporting system aims to promote the ethical and responsible use of AI in scientific research and provide a standardized approach for reporting AI usage across different research fields. We also provide a free service to easily generate AI Usage Cards for scientific work via a questionnaire and export them in various machine-readable formats for inclusion in different work products at https://ai-cards.org.},
	urldate = {2024-02-14},
	publisher = {arXiv},
	author = {Wahle, Jan Philip and Ruas, Terry and Mohammad, Saif M. and Meuschke, Norman and Gipp, Bela},
	month = may,
	year = {2023},
	note = {arXiv: 2303.03886 [cs]},
	keywords = {Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\W445X7DB\\Wahle et al. - 2023 - AI Usage Cards Responsibly Reporting AI-generated.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\TC5TDT44\\2303.html:text/html},
}

@misc{WangMWG23a,
	title = {Unleashing {Cognitive} {Synergy} in {Large} {Language} {Models}: {A} {Task}-{Solving} {Agent} through {Multi}-{Persona} {Self}-{Collaboration}},
	shorttitle = {Unleashing {Cognitive} {Synergy} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.05300},
	abstract = {Human intelligence thrives on the concept of cognitive synergy, where collaboration and information integration among different cognitive processes yield superior outcomes compared to individual cognitive processes in isolation. Although Large Language Models (LLMs) have demonstrated promising performance as general task-solving agents, they still struggle with tasks that require intensive domain knowledge and complex reasoning. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have discovered that assigning multiple, fine-grained personas in LLMs elicits better problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledgeintensive and reasoning-intensive types. Unlike previous works, such as Chainof-Thought, that solely enhance the reasoning abilities in LLMs, SPP effectively elicits internal knowledge acquisition abilities, reduces hallucination, and maintains strong reasoning capabilities. Code, data, and prompts can be found at: https: //github.com/MikeWangWZHL/Solo-Performance-Prompting.git.},
	language = {en},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Wang, Zhenhailong and Mao, Shaoguang and Wu, Wenshan and Ge, Tao and Wei, Furu and Ji, Heng},
	month = jul,
	year = {2023},
	note = {arXiv: 2307.05300 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, !tr},
	annote = {Comment: work in progress},
	annote = {Comment: work in progress},
	file = {arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\6TSMNQM6\\2307.html:text/html;WangMWG23--tr--unleashing_cognitive_synergy_in_large_language_models_a_task-solving_agent_through.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\MP7S5Z8Z\\WangMWG23--tr--unleashing_cognitive_synergy_in_large_language_models_a_task-solving_agent_through.pdf:application/pdf},
}

@misc{XuYLW23a,
	title = {{ExpertPrompting}: {Instructing} {Large} {Language} {Models} to be {Distinguished} {Experts}},
	shorttitle = {{ExpertPrompting}},
	url = {http://arxiv.org/abs/2305.14688},
	abstract = {The answering quality of an aligned large language model (LLM) can be drastically improved if treated with proper crafting of prompts. In this paper, we propose ExpertPrompting to elicit the potential of LLMs to answer as distinguished experts. We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background. Based on this augmented prompting strategy, we produce a new set of instruction-following data using GPT-3.5, and train a competitive open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) ExpertLLaMA outperforms existing open-source opponents and achieves 96\% of the original ChatGPT’s capability. All data and the ExpertLLaMA model will be made publicly available at https:// github.com/OFA-Sys/ExpertLLaMA.},
	language = {en},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Xu, Benfeng and Yang, An and Lin, Junyang and Wang, Quan and Zhou, Chang and Zhang, Yongdong and Mao, Zhendong},
	month = may,
	year = {2023},
	note = {arXiv: 2305.14688 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, !tr},
	file = {arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\YPRTANT6\\2305.html:text/html;XuYLW23--tr--expertprompting_instructing_large_language_models_to_be_distinguished_experts.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\9GWRK2TQ\\XuYLW23--tr--expertprompting_instructing_large_language_models_to_be_distinguished_experts.pdf:application/pdf},
}

@misc{GuoCWC24a,
	title = {Large {Language} {Model} based {Multi}-{Agents}: {A} {Survey} of {Progress} and {Challenges}},
	shorttitle = {Large {Language} {Model} based {Multi}-{Agents}},
	url = {http://arxiv.org/abs/2402.01680},
	abstract = {Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize the commonly used datasets or benchmarks for them to have convenient access. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository, dedicated to outlining the research on LLM-based multi-agent systems.},
	urldate = {2024-02-07},
	publisher = {arXiv},
	author = {Guo, Taicheng and Chen, Xiuying and Wang, Yaqi and Chang, Ruidi and Pei, Shichao and Chawla, Nitesh V. and Wiest, Olaf and Zhang, Xiangliang},
	month = jan,
	year = {2024},
	note = {arXiv: 2402.01680 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, !tr, nlp\_agent, Computer Science - Multiagent Systems, nlp\_llm, nlp\_survey},
	annote = {Annotations(2/9/2024, 1:26:48 PM)
“LLMbased multi-agent systems have achieved considerable progress in complex problem-solving and world simulation.” (Guo et al., 2024, p. 1)
“Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents’ capacities?” (Guo et al., 2024, p. 1)
“To keep researchers updated on the latest studies, we maintain an open-source GitHub repository,” (Guo et al., 2024, p. 1)
“Timely survey papers systematically summarize the progress of LLM-based agents, as seen in works [Xi et al., 2023; Wang et al., 2023b].” (Guo et al., 2024, p. 1)
“Compared to systems using a single LLM-powered agent, multi-agent systems offer advanced capabilities by 1) specializing LLMs into various distinct agents, each with different capabilities, and 2) enabling interactions among these diverse agents to simulate complex real-world environments effectively.” (Guo et al., 2024, p. 1)
“Recent research has demonstrated promising results in utilizing LLM-based multi-agents for solving various tasks, such as software development [Hong et al., 2023; Qian et al., 2023], multi-robot systems [Mandi et al., 2023; Zhang et al., 2023c], society simulation [Park et al., 2023; Park et al., 2022], policy simulation [Xiao et al., 2023; Hua et al., 2023], and game simulation [Xu et al., 2023c; Wang et al., 2023c].” (Guo et al., 2024, p. 1)
“Readers will gain a comprehensive overview of LLM-based Multi-Agent (LLM-MA) systems,” (Guo et al., 2024, p. 2)
“Decision-making Thought: This term denotes the capability of LLM-based agents, guided by prompts, to break down complex tasks into smaller subgoals [Khot et al., 2023], think through each part methodically (sometimes exploring multiple paths) [Yao et al., 2023], and learn from past experiences [Shinn et al., 2023] to perform better decision-making on complex tasks.” (Guo et al., 2024, p. 3)
“Tool-use: LLM-based agents’ tool-use capability allows them to leverage external tools and resources to accomplish tasks,” (Guo et al., 2024, p. 3)
“Memory: This ability refers to the capability of LLMbased agent for conducting in-context learning [Dong et al., 2023a] as short memory or external vector database [Lewis et al., 2021] as long memory to preserve and retrieve information over prolonged periods [Wang et al., 2023b]” (Guo et al., 2024, p. 3)
“Single-Agent systems empowered by LLMs have shown inspiring cognitive abilities [Sumers et al., 2023]. The construction of such systems concentrates on formulating their internal mechanisms and interactions with the external environment. Conversely, LLM-MA systems emphasize diverse agent profiles, inter-agent interactions, and collective decision-making processes.” (Guo et al., 2024, p. 3)
“A critical inquiry we address is how these LLM-MA systems are aligned to their operational environments and the collective objectives they are designed to achieve. To shed light on this, we present the general architecture of these systems in Fig. 2.” (Guo et al., 2024, p. 3)
“The AgentsEnvironment Interface refers to the way in which agents interact with and perceive the environment. It’s through this interface that agents understand their surroundings, make decisions, and learn from the outcomes of their actions. We categorize the current interfaces in LLM-MA systems into three types, Sandbox, Physcial, and None, as detailed in Table 1.” (Guo et al., 2024, p. 3)
“The Sandbox refers to a simulated or virtual environment built by human where agents can interact more freely and experiment with various actions and strategies.” (Guo et al., 2024, p. 3)
“The Physical is a real-world environment where agents interact with physical entities and obey realworld physics and constraints.” (Guo et al., 2024, p. 3)
“None refers to scenarios where there is no specific external environment, and agents do not interact with any environment. For example, many applications [Du et al., 2023; Xiong et al., 2023; Chan et al., 2023] utilize multiple agents to debate a question to reach a consensus.” (Guo et al., 2024, p. 3)
“Agents Profiling In LLM-MA systems, agents are defined by their traits, actions, and skills, which are tailored to meet specific goals. Across various systems, agents assume distinct roles, each with comprehensive descriptions encompassing characteristics, capabilities, behaviors, and constraints.” (Guo et al., 2024, p. 3)
“Regarding the Agent Profiling Methods, we categorized them into three types: Pre-defined, Model-Generated, and” (Guo et al., 2024, p. 3)
“Data-Derived.” (Guo et al., 2024, p. 4)
“Pre-defined cases, agent profiles are explicitly defined by the system designers. The ModelGenerated method creates agent profiles by models, e.g., large language models. The Data-Derived method involves constructing agent profiles based on pre-existing datasets” (Guo et al., 2024, p. 4)
“Agents Communication” (Guo et al., 2024, p. 4)
“1) Communication Paradigms: the styles and methods of interaction between agents; 2) Communication Structure: the organization and architecture of communication networks within the multi-agent system; and 3) Communication Content exchanged between agents.” (Guo et al., 2024, p. 4)
“Communication Paradigms: Current LLM-MA systems mainly take three paradigms for communication: Cooperative, Debate, and Competitive.” (Guo et al., 2024, p. 4)
“Communication Structure:” (Guo et al., 2024, p. 4)
“Layered communication is structured hierarchically, with agents at each level having distinct roles and primarily interacting within their layer or with adjacent layers.” (Guo et al., 2024, p. 4)
“Decentralized communication operates on a peer-to-peer network, where agents directly communicate with each other, a structure commonly employed in world simulation applications. Centralized communication involves a central agent” (Guo et al., 2024, p. 4)
“or a group of central agents coordinating the system’s communication, with other agents primarily interacting through this central node. Shared Message Pool is proposed by MetaGPT [Hong et al., 2023] to improve the communication efficiency. This communication structure maintains a shared message pool where agents publish messages and subscribe to relevant messages based on their profile” (Guo et al., 2024, p. 5)
“Communication Content: In LLM-MA systems, the Communication Content typically takes the form of text.” (Guo et al., 2024, p. 5)
“Agents Capabilities Acquisition” (Guo et al., 2024, p. 5)
“two fundamental concepts: the types of feedback from which agents should learn to enhance their capabilities, and the strategies for agents to adjust themselves to effectively solve complex problems” (Guo et al., 2024, p. 5)
“Feedback: Feedback involves the critical information that agents receive about the outcome of their actions, helping the agents learn the potential impact of their actions and adapt to complex and dynamic problems.” (Guo et al., 2024, p. 5)
“1) Feedback from Environment, e.g., from either real world environments or virtual environments [Wang et al., 2023b].” (Guo et al., 2024, p. 5)
“2) Feedback from Agents Interactions means that the feedback comes from the judgement of other agents or from agents communications.” (Guo et al., 2024, p. 5)
“3) Human Feedback comes directly from humans and is crucial for aligning the multi-agent system with human values and preferences.” (Guo et al., 2024, p. 5)
“4) None. In some cases, there is no feedback provided to the agents. This often happens for world simulation works focused on analyzing simulated results rather than the planning capabilities of agents.” (Guo et al., 2024, p. 5)
“2) SelfEvolution. Instead of only relying on the historical records to decide subsequent actions as seen in Memory-based solutions, agents can dynamically self-evolve by modifying themselves such as altering their initial goals and planning strategies, and training themselves based on feedback or communication logs” (Guo et al., 2024, p. 5)
“LLM-MA for Problem Solving The main motivation of using LLM-MA for problem solving is to harness the collective capabilities of agents with specialized expertise.” (Guo et al., 2024, p. 5)
“Science Experiments Like multiple agents play as different specialists and cooperate to solve the Software Development and Embodied Agents problem, multiple agents can also be used to form a science team to conduct science experiments.” (Guo et al., 2024, p. 7)
“Human experts are at the center of these agents to process the information of agents and give feedback to the agents. [Zheng et al., 2023] utilizes multiple LLM-based agents, each focusing on specific tasks for the science experiments including strategy planning, literature search, coding, robotic operations, and labware design” (Guo et al., 2024, p. 7)
“4.1.4 Science Debate LLM-MA can be set for science debating scenarios, where agents debate with each other to enhance the collective reasoning capabilities in tasks such as Massive Multitask Language Understanding (MMLU) [Hendrycks et al., 2020], Math problems [Cobbe et al., 2021], and StrategyQA [Geva et al., 2021]. The main idea is that each agent initially offers its own analysis of a problem, which is then followed by a joint debating process. Through multiple rounds of debate, the agents converge on a single, consensus answer. [Du et al., 2023] leverages the multi-agents debate process on a set of six different reasoning and factual accuracy tasks and demonstrates that LLM-MA debating can improve factuality.” (Guo et al., 2024, p. 7)
“LLM-MA for World Simulation” (Guo et al., 2024, p. 7)
“Research in this area is rapidly growing and spans a diverse range of fields including social sciences, gaming, psychology, economics, policy-making, etc.” (Guo et al., 2024, p. 7)
“Unlike the problem solving systems that focus on agent cooperation, world simulation systems involve diverse methods of agent management and communication, reflecting the complexity and variety of real-world interactions.” (Guo et al., 2024, p. 7)
“Societal Simulation In societal simulation, LLM-MA models are used to simulate social behaviors, aiming to explore the potential social dynamics and propagation, test social science theories, and populate virtual spaces and communities with realistic social phenomena [Park et al., 2023].” (Guo et al., 2024, p. 7)
“4.2.3 Psychology In psychological simulation studies, like in the societal simulation, multiple agents are utilized to simulate humans with various traits and thought processes. However, unlike societal simulations, one approach in psychology involves directly applying psychological experiments to these agents.” (Guo et al., 2024, p. 8) There are no actual references in using LLMs as agents in this block
“[Kovaˇ c et al., 2023] introduces a tool named SocialAI school for creating interactive environments simulating social interactions. It draws from developmental psychology to understand how agents can acquire, demonstrate, and evolve social skills such as joint attention, communication, and cultural learning.” (Guo et al., 2024, p. 8)
“Economy LLM-MA is used to simulate economic and financial trading environments mainly because it can serve as implicit computational models of humans.” (Guo et al., 2024, p. 8)
“This is similar to the way economists model ’homo economicus’, the characterization of man in some economic theories as a rational person who pursues wealth for his own self-interest [Horton, 2023]” (Guo et al., 2024, p. 8)
“Recommender Systems The use of the LLM-MA in recommender systems is similar to that in psychology since studies in both fields involve the consideration of extrinsic and intrinsic human factors such as cognitive processes and personality [Lex and Schedl, 2022]” (Guo et al., 2024, p. 9)
“Agent4Rec [Zhang et al., 2023a] introduces a simulation platform based on LLM-MA. 1000 generative agents are initialized with the MovieLens-1M dataset to simulate complex user interactions in a recommendation environment. Agent4Rec shows that LLM-MA can effectively mimic real user preferences and behaviors, provide insights into phenomena like the filter bubble effect, and help uncover causal relationships in recommendation tasks” (Guo et al., 2024, p. 9)
“Policy Making Similar to simulations in gaming and economic scenarios, Policy Making requires strong decision-making capabilities to realistic and dynamic complex problems. LLM-MA can be used to simulate the policy making via simulating a virtual government or simulating the impact of various policies on different communities.” (Guo et al., 2024, p. 9)
“Disease Propagation Simulation Leveraging the societal simulation capabilities of LLM-MA can also be used to simulate disease propagation. The most recent study in [Williams et al., 2023] delves into the use of LLM-MA in simulating disease spread.” (Guo et al., 2024, p. 10)
“Multi-Agents Framework We provide a detailed introduction to three open-source multi-agent frameworks: MetaGPT [Hong et al., 2023], CAMEL [Li et al., 2023b], and Autogen [Wu et al., 2023a]” (Guo et al., 2024, p. 10)
“MetaGPT is designed to embed human workflow processes into the operation of language model agents, thereby reducing the hallucination problem that often arises in complex tasks.” (Guo et al., 2024, p. 10)
“CAMEL, or Communicative Agent Framework, is oriented towards facilitating autonomous cooperation among agents.” (Guo et al., 2024, p. 10)
“AutoGen is a versatile framework that allows for the creation of applications using language models. It is distinctive for its high level of customization, enabling developers to program agents using both natural language and code to define how these agents interact.” (Guo et al., 2024, p. 10)
“In the Problem solving scenarios, most datasets and benchmarks are used to evaluate the planning and reasoning capabilities by Multiple agents cooperation or debate. In World Simulation scenarios, datasets and benchmarks are used to evaluate the alignment between the simulated world and realworld or analyze the behaviors of different agents.” (Guo et al., 2024, p. 10)
“there is a notable lack in multi-modal settings, where agents would interact with and interpret data from multiple sensory inputs and generate multiple outputs such as images, audio, video, and physical actions.” (Guo et al., 2024, p. 10)
“detecting and mitigating hallucinations in LLMMA is not just a crucial task but also presents a unique set of challenges. It involves not only correcting inaccuracies at the level of individual agents but also managing the flow of information between agents to prevent the spread of these inaccuracies throughout the system.” (Guo et al., 2024, p. 10)
“Acquiring Collective Intelligence In traditional multi-agent systems, agents often use reinforcement learning to learn from offline training datasets. However, LLM-MA systems mainly learn from instant feedback, such as interactions with the environment or humans” (Guo et al., 2024, p. 11)
“much of the existing research focuses on evaluating individual agents’ understanding and reasoning within narrowly defined scenarios.” (Guo et al., 2024, p. 11)
“there is a notable shortfall in the development of comprehensive benchmarks across several research domains, such as Science Team for Experiment Operations, Economic analysis, and Disease propagation simulation.” (Guo et al., 2024, p. 11)
“[Gao et al., 2023b] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023.” (Guo et al., 2024, p. 12)
“[Wang et al., 2023b] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey on large language model based autonomous agents, 2023.” (Guo et al., 2024, p. 14)
“[Xi et al., 2023] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model based agents: A survey, 2023.” (Guo et al., 2024, p. 14)
},
	annote = {Comment: This work is ongoing and we welcome your contribution!},
	annote = {Comment: This work is ongoing and we welcome your contribution!},
	file = {arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\MFKTY32Q\\2402.html:text/html;GuoCWC24--tr--large_language_model_based_multi-agents_a_survey_of_progress_and_challenges.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\SXAJQAEX\\GuoCWC24--tr--large_language_model_based_multi-agents_a_survey_of_progress_and_challenges.pdf:application/pdf},
}

@misc{SuzgunK24b,
	title = {Meta-{Prompting}: {Enhancing} {Language} {Models} with {Task}-{Agnostic} {Scaffolding}},
	shorttitle = {Meta-{Prompting}},
	url = {http://arxiv.org/abs/2401.12954},
	abstract = {We introduce meta-prompting, an effective scaffolding technique designed to enhance the functionality of language models (LMs). This approach transforms a single LM into a multi-faceted conductor, adept at managing and integrating multiple independent LM queries. By employing high-level instructions, meta-prompting guides the LM to break down complex tasks into smaller, more manageable subtasks. These subtasks are then handled by distinct "expert" instances of the same LM, each operating under specific, tailored instructions. Central to this process is the LM itself, in its role as the conductor, which ensures seamless communication and effective integration of the outputs from these expert models. It additionally employs its inherent critical thinking and robust verification processes to refine and authenticate the end result. This collaborative prompting approach empowers a single LM to simultaneously act as a comprehensive orchestrator and a panel of diverse experts, significantly enhancing its performance across a wide array of tasks. The zero-shot, task-agnostic nature of meta-prompting greatly simplifies user interaction by obviating the need for detailed, task-specific instructions. Furthermore, our research demonstrates the seamless integration of external tools, such as a Python interpreter, into the meta-prompting framework, thereby broadening its applicability and utility. Through rigorous experimentation with GPT-4, we establish the superiority of meta-prompting over conventional scaffolding methods: When averaged across all tasks, including the Game of 24, Checkmate-in-One, and Python Programming Puzzles, meta-prompting, augmented with a Python interpreter functionality, surpasses standard prompting by 17.1\%, expert (dynamic) prompting by 17.3\%, and multipersona prompting by 15.2\%.},
	urldate = {2024-01-29},
	publisher = {arXiv},
	author = {Suzgun, Mirac and Kalai, Adam Tauman},
	month = jan,
	year = {2024},
	note = {arXiv: 2401.12954 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, !tr, nlp\_agent, Computer Science - Human-Computer Interaction, nlp\_llm},
	annote = {Annotations(2/1/2024, 10:42:26 AM)
“meta-prompting, an effective scaffolding technique designed to enhance the functionality of language models (LMs). This approach transforms a single LM into a multi-faceted conductor, adept at managing and integrating multiple independent LM queries” (Suzgun und Kalai, 2024, p. 1)
“The zero-shot, task-agnostic nature of meta-prompting greatly simplifies user interaction by obviating the need for detailed, task-specific instructions” (Suzgun und Kalai, 2024, p. 1)
“Through rigorous experimentation with GPT-4, we establish the superiority of meta-prompting over conventional scaffolding methods: When averaged across all tasks, including the Game of 24, Checkmate-in-One, and Python Programming Puzzles” (Suzgun und Kalai, 2024, p. 1)
“Our research demonstrates that meta-prompting, particularly when combined with a Python interpreter, significantly improves overall accuracy and robustness in GPT-4 across a variety of tasks.” (Suzgun und Kalai, 2024, p. 1)
“GPT-4 (OpenAI, 2023), PaLM (Anil et al., 2023), and LLaMa (Touvron et al., 2023)” (Suzgun und Kalai, 2024, p. 2)
“Despite their versatility, these models are not infallible; they sometimes generate responses that are inaccurate, misleading, or conflicting.” (Suzgun und Kalai, 2024, p. 2)
“It involves constructing a high-level “meta” prompt that instructs an LM to: (i) break down complex tasks or problems into smaller, manageable pieces; (ii) assign these pieces to specialized “expert” models with proper and detailed natural-language instructions; (iii) oversee the communication between these expert models; and (iv) apply its own critical thinking, reasoning, and verification skills throughout the process.” (Suzgun und Kalai, 2024, p. 2)
“This approach allows for a single, uniform LM to maintain a coherent line of reasoning while also tapping into a variety of expert roles. The use of dynamically selected contexts for prompting these experts introduces fresh perspectives into the process, while the conductor model retains a bird’s-eye view of the entire history and coordination.” (Suzgun und Kalai, 2024, p. 2)
“Our proposed meta-prompting technique combines and expands upon various prompting ideas introduced by recent studies—including, high-level planning and decision-making (Yao et al., 2023b; Sun et al., 2023; Hao et al., 2023a), dynamic persona assignment (Xu et al., 2023; Wang et al., 2023), multi-agent debating (Du et al., 2023; Zhuge et al., 2023), self-debugging and self-reflection (Schick et al., 2023b; Liu et al., 2023a; Gou et al., 2023; Madaan et al., 2023; Shinn et al., 2023).” (Suzgun und Kalai, 2024, p. 2)
“A key aspect of meta-prompting is its task-agnostic nature. Unlike traditional scaffolding methods that require specific instructions or examples tailored to each task, metaprompting employs the same set of high-level instructions across various tasks and inputs. This universality is particularly beneficial for users who might find it cumbersome to provide detailed examples or specific guidance for every distinct task.” (Suzgun und Kalai, 2024, p. 2)
“We provide an illustrative visualization of a meta-prompting session in Figure 2. It depicts how the Meta Model” (Suzgun und Kalai, 2024, p. 2)
“What sets meta-prompting apart is that it leaves the decision of which prompts to use and which code snippets to execute to the discretion of the LM itself.” (Suzgun und Kalai, 2024, p. 2)
“Our findings reveal that meta-prompting not only enhances overall performance but often leads to state-of-the-art results across a diverse range of tasks.” (Suzgun und Kalai, 2024, p. 2)
“The core contribution of this work is the introduction of a task-agnostic scaffolding system that leverages a single LM. This LM not only carries forward the thread of the task but also dynamically selects and instructs expert models appropriate for each specific task.” (Suzgun und Kalai, 2024, p. 3)
“We posit that while a single, general-purpose model might deliver valuable and useful insights into generic queries, combining the perspectives and conclusions of multiple domain-specific models (which we also refer to as experts) has the potential to yield more comprehensive, robust, and accurate solutions.” (Suzgun und Kalai, 2024, p. 3)
“This prompting structure is reminiscent of an orchestra, wherein the conductor’s role is mirrored by the Meta Model and each musician corresponds to a distinct domain-specific model.” (Suzgun und Kalai, 2024, p. 4)
“Conceptually, a domain-specific expert within our framework can take diverse forms, such as a finetuned LM tailored to perform a particular task, a specialized API equipped to handle specific domain-related inquiries” (Suzgun und Kalai, 2024, p. 4)
“Under our setup, experts can be called only by the Meta Model. They cannot directly interact or communicate with each other, though the Meta Model can choose to share some text from or combine the insights of various experts when interacting with a new expert.” (Suzgun und Kalai, 2024, p. 4)
“Engaging Domain-Specific Expert Models: If the Meta Model does not return a result, it can conjure any expert and give it instructions, which are extracted from its output using eexp. This process is isolated though: Each expert only sees what the Meta Model chooses to share with them, and responds accordingly.” (Suzgun und Kalai, 2024, p. 5)
“Multi-persona prompting (Du et al., 2023): Also known as solo-performance prompting (SPP), this method instructs an LM to perform the following: (i) Propose a small ensemble of “personas” to address the specific task or problem at hand; (ii) let these personas engage in a collective dialogue, collaboratively generating potential solutions while extending feedback to one another and refining their answers; and (iii) synthesize all the available information and deliver a final response” (Suzgun und Kalai, 2024, p. 5)
“Three BIG-Bench Hard (BBH; Suzgun et al. (2023b)) tasks—namely, (b) Geometric Shapes, (c) MultiStep Arithmetic Two, and (d) Word Sorting—as well as one reasoning task directly obtained from the BIG-Bench suite (BIG-Bench authors, 2023), that is, (e) Checkmate-in-One;” (Suzgun und Kalai, 2024, p. 6) What is the rational for such tasks?
“Once the final answer is extracted from the model and properly post-processed, we also need to evaluate its correctness.4 Because we consider a wide range of tasks, there is not a single metric that allows us to measure accuracy across all.” (Suzgun und Kalai, 2024, p. 6)
“3While all the other tasks and datasets were previously introduced by other studies, we present this task for the first time.” (Suzgun und Kalai, 2024, p. 6)
“We set the temperature value at 0, the top-p value at 0.95, and the maximum token count at 1024” (Suzgun und Kalai, 2024, p. 7)
“The temperature value, which usually ranges between 0 and 1, controls how much randomness or creativity the model exhibits. Ideally, a temperature of 0 should lead to the model producing the same output when presented with the same input. However, both GPT-3.5 and GPT-4 have shown a tendency to generate varied responses even at this setting. This means that reproducing our exact results might be challenging under identical experimental conditions. To address this issue, we are releasing all model inputs, interactions, and outputs in our GitHub repository.” (Suzgun und Kalai, 2024, p. 7)
“Without a Python interpreter, meta-prompting significantly outperforms other methods on the Checkmate-in-One and Sonnet Writing tasks and is on par on most other tasks except Geometric Shapes. Meta-prompting can leverage the Python interpreter in a task-agnostic manner to improve performance significantly across many tasks” (Suzgun und Kalai, 2024, p. 8)
“superior effectiveness of our metaprompting approach compared to the standard zero-shot prompting methods. When we look at the overall performance across all tasks, there is a notable increase in accuracy with meta-prompting, especially when it is augmented with a Python interpreter.” (Suzgun und Kalai, 2024, p. 8)
“meta-prompting appears to be effective in creative writing tasks as well. In the Shakespearean Sonnet Writing task, for instance, which demands linguistic precision and creative conformity to specific poetic structures, meta-prompting notably enhances performance” (Suzgun und Kalai, 2024, p. 8)
“The success of our meta-prompting framework lies partly in its strategic use of specialized knowledge, selfcollaboration, and implicit verification loops” (Suzgun und Kalai, 2024, p. 9)
“Our structured approach embodies the principle of the wisdom of the crowd (Suzgun et al., 2023a), which posits that a collective opinion of a diverse set of critical thinkers often surpasses the insights of individual experts.” (Suzgun und Kalai, 2024, p. 9)
“Grounded in principles from cognitive psychology, fresh perspectives can lead to more creative problemsolving and error detection. When individuals or models approach a problem without preconceived notions, they are more likely to consider alternative solutions and identify errors that might have been overlooked” (Suzgun und Kalai, 2024, p. 9)
“the deployment of a Python interpreter within the meta-prompting framework should be fortified with a secure sandbox. These measures are crucial to ensure the system’s integrity and the protection of user data, ensuring that the advantages of improved problem-solving efficiency are not compromised in any way by security and privacy concerns, among other issues.” (Suzgun und Kalai, 2024, p. 10)
“n scenarios where a Python expert is explicitly mentioned for code generation and execution, there is a noticeable preference for technical and computational expertise. For example, in Python Programming Puzzles, the Meta Model frequently utilizes Expert Python, Expert Mathematician, and several tiers of Expert Python Programmers.” (Suzgun und Kalai, 2024, p. 11)
“umber of Rounds Taken to Reach a Solution. Examining the meta-prompting experiments involving a Python expert reveals that the average number of rounds required to reach a solution in the Meta Model varies significantly across tasks, indicative of their complexity and specific nature. Simpler tasks, such as Word Sorting (3.31 rounds) and Checkmate-in-One (3.48 rounds), typically necessitate fewer rounds, suggesting a more linear and straightforward resolution process, likely due to their clearly defined parameters. Conversely, more algorithmically challenging tasks like Python Programming Puzzles average a higher number of rounds at 6.07, reflecting the nuanced and multifaceted aspects of programming tasks that require extensive interactions for thorough clarification and iterative refinement” (Suzgun und Kalai, 2024, p. 11)
“Enhancing Solution Reliability through Systematic Verification. The Meta Model’s systematic verification protocol strengthens the reliability and robustness of its solutions. Fundamental to this approach is the consistent practice of consulting an expert for validation before finalizing responses, a principle applied across diverse tasks.” (Suzgun und Kalai, 2024, p. 12)
“Navigating No-Solution Territories. Meta-prompting enables the Meta Model to acknowledge the absence or impossibility of a valid solution or its inability to find one more frequently than other prompting methods.” (Suzgun und Kalai, 2024, p. 12)
“Limited Performance Improvement with GPT-3.5. In comparison to GPT-4, GPT-3.5 demonstrates a more limited scope of performance enhancement across various tasks. Although it shows notable improvements in specific tasks such as Sonnet Writing and Checkmate-in-One, its capabilities do not consistently surpass baseline standards or zero-shot CoT prompting methods in other tasks, notably Word Sorting and Multiple Arithmetic Two.” (Suzgun und Kalai, 2024, p. 12)
“Our qualitative analysis suggests that GPT-3.5 may not be as effective as GPT-4 in simulating role-playing scenarios or managing extended context windows.” (Suzgun und Kalai, 2024, p. 12)
“A primary limitation is the elevated cost associated with multiple model calls. In our setup using GPT-4, the dual role of the Meta Model and the experts, distinguished by unique instructions, incurs substantial costs under the GPT-4 API pricing model.” (Suzgun und Kalai, 2024, p. 12)
“meta-prompting, though insightful, can become prohibitively expensive due to extensive model interactions and lengthy message histories.” (Suzgun und Kalai, 2024, p. 13)
“There is also untapped potential in concurrently summoning multiple experts or utilizing a single expert with varied temperature parameters to synthesize their outputs.” (Suzgun und Kalai, 2024, p. 13)
“A practical challenge faced is the Meta Model’s occasional oversight in conveying necessary information to experts, forgetting that experts can only access data adhering to a certain format (within triple quotes in our system). This oversight can lead to unintended confusion and underscores the need for improved information management” (Suzgun und Kalai, 2024, p. 13)
“The chain-of-thought (CoT) prompting (Wei et al., 2022b) and its variants—including least-to-most (Zhou et al., 2023), zero-shot CoT (Kojima et al., 2022), self-ask (Press et al., 2022), ask-me-anything (Arora et al., 2023), decomposed prompting (Khot et al., 2023), and auto-CoT (Zhang et al., 2023d)” (Suzgun und Kalai, 2024, p. 13)
“More recent innovations such Tree-of-Thought (Yao et al., 2023a), Graph-of-Thought (Besta et al., 2023), Program-of-Thought (Chen et al., 2023d), and Skeleton-of-Thought (Ning et al., 2023), have further enriched this domain; these explore dynamic, non-linear reasoning pathways, broadening the computational and heuristic capabilities of LMs.” (Suzgun und Kalai, 2024, p. 13)
“Recent studies (Park et al., 2022, 2023; Li et al., 2023; Xu et al., 2023; Fu et al., 2023a; Deshpande et al., 2023) have shown that endowing instruction-following LMs with “expert” personas or roles enhances the quality and accuracy of their output. In particular, approaches like CAMEL (Li et al., 2023) and Expert Prompting (Xu et al., 2023), which involve dynamically assigning personas to a single LM, have been shown to yield higher quality and more reliable responses than models without designated personas.” (Suzgun und Kalai, 2024, p. 14)
“Further investigations (Chen et al., 2023a,e; Du et al., 2023; Hao et al., 2023b; Liang et al., 2023; Liu et al., 2023b; Jiang et al., 2023a; Xiong et al., 2023; Zhang et al., 2023c) demonstrate that assigning multiple expert identities or roles to a single LM, tailored to specific tasks or problems, and prompting it to conduct multi-round internal dialogues—similar to a team of experts discussing and refining ideas—amplifies the reliability and comprehensiveness of the LM’s analysis; this leads to more well-rounded and thorough solutions” (Suzgun und Kalai, 2024, p. 14)
“As discussed by Masa (2023), those autonomous models might be exploited by individuals with malicious intents and pose threats to humanity. There is also the dilemma of accountability: who bears responsibility when an LM-driven autonomous agent produces an inappropriate or criminal action?” (Suzgun und Kalai, 2024, p. 14)
“OpenAI’s incorporation of predefined APIs and plugins into ChatGPT underscores the importance of external integration in developing a comprehensive LM ecosystem” (Suzgun und Kalai, 2024, p. 15)
“In this work, we have introduced and examined meta-prompting, a simple yet powerful scaffolding technique that enhances the performance of language models in a task-agnostic manner. This approach leverages a language model to act as both a central conductor and a group of expert instances, thereby endowing traditional models with dynamic, multi-functional capabilities.” (Suzgun und Kalai, 2024, p. 15)
“Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and Zhendong Mao. 2023. ExpertPrompting: Instructing Large Language Models to be Distinguished Experts. arXiv preprint arXiv:2305.14688 (2023).” (Suzgun und Kalai, 2024, p. 20)
},
	annote = {Comment: https://github.com/suzgunmirac/meta-prompting},
	annote = {Comment: https://github.com/suzgunmirac/meta-prompting},
	file = {arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\I6QJS2EY\\2401.html:text/html;SuzgunK24--tr--meta-prompting_enhancing_language_models_with_task-agnostic_scaffolding.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\GE3BUF6F\\SuzgunK24--tr--meta-prompting_enhancing_language_models_with_task-agnostic_scaffolding.pdf:application/pdf},
}

@inproceedings{WahleRAG23,
	address = {Singapore},
	title = {We are {Who} {We} {Cite}: {Bridges} of {Influence} {Between} {Natural} {Language} {Processing} and {Other} {Academic} {Fields}},
	shorttitle = {We are {Who} {We} {Cite}},
	url = {https://aclanthology.org/2023.emnlp-main.797},
	doi = {10.18653/v1/2023.emnlp-main.797},
	language = {en},
	urldate = {2024-01-11},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wahle, Jan and Ruas, Terry and Abdalla, Mohamed and Gipp, Bela and Mohammad, Saif},
	month = dec,
	year = {2023},
	pages = {12896--12913},
	file = {WahleRAG23--NM--we_are_who_we_cite_bridges_of_influence_between_natural_language_processing_and_other.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\ZU5RNYVC\\WahleRAG23--NM--we_are_who_we_cite_bridges_of_influence_between_natural_language_processing_and_other.pdf:application/pdf},
}

@misc{SunXZL23,
	title = {Head-to-{Tail}: {How} {Knowledgeable} are {Large} {Language} {Models} ({LLM})? {A}.{K}.{A}. {Will} {LLMs} {Replace} {Knowledge} {Graphs}?},
	shorttitle = {Head-to-{Tail}},
	url = {http://arxiv.org/abs/2308.10168},
	abstract = {Since the recent prosperity of Large Language Models (LLMs), there have been interleaved discussions regarding how to reduce hallucinations from LLM responses, how to increase the factuality of LLMs, and whether Knowledge Graphs (KGs), which store the world knowledge in a symbolic form, will be replaced with LLMs. In this paper, we try to answer these questions from a new angle: How knowledgeable are LLMs? To answer this question, we constructed Head-to-Tail, a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity. We designed an automated evaluation method and a set of metrics that closely approximate the knowledge an LLM confidently internalizes. Through a comprehensive evaluation of 14 publicly available LLMs, we show that existing LLMs are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities.},
	urldate = {2024-02-15},
	publisher = {arXiv},
	author = {Sun, Kai and Xu, Yifan Ethan and Zha, Hanwen and Liu, Yue and Dong, Xin Luna},
	month = aug,
	year = {2023},
	note = {arXiv:2308.10168 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\49VEJYFQ\\2308.html:text/html;Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\7Q64ZIFR\\Sun et al. - 2023 - Head-to-Tail How Knowledgeable are Large Language.pdf:application/pdf},
}

@misc{PalUS22,
	title = {{MedMCQA} : {A} {Large}-scale {Multi}-{Subject} {Multi}-{Choice} {Dataset} for {Medical} domain {Question} {Answering}},
	shorttitle = {{MedMCQA}},
	url = {http://arxiv.org/abs/2203.14371},
	doi = {10.48550/arXiv.2203.14371},
	abstract = {This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. More than 194k high-quality AIIMS {\textbackslash}\& NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects {\textbackslash}\& topics. A detailed explanation of the solution, along with the above information, is provided in this study.},
	urldate = {2024-02-13},
	publisher = {arXiv},
	author = {Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.14371 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: Proceedings of Machine Learning Research (PMLR), ACM Conference on Health, Inference, and Learning (CHIL) 2022},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\URDQLWX5\\Pal et al. - 2022 - MedMCQA  A Large-scale Multi-Subject Multi-Choice.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\TYIBAMXZ\\2203.html:text/html},
}

@misc{SpiessGPP24,
	title = {Quality and {Trust} in {LLM}-generated {Code}},
	url = {http://arxiv.org/abs/2402.02047},
	abstract = {Machine learning models are widely used but can also often be wrong. Users would benefit from a reliable indication of whether a given output from a given model should be trusted, so a rational decision can be made whether to use the output or not. For example, outputs can be associated with a confidence measure; if this confidence measure is strongly associated with likelihood of correctness, then the model is said to be well-calibrated. In this case, for example, high-confidence outputs could be safely accepted, and low-confidence outputs rejected. Calibration has so far been studied in non-generative (e.g., classification) settings, especially in Software Engineering. However, generated code can quite often be wrong: Developers need to know when they should e.g., directly use, use after careful review, or discard model-generated code; thus Calibration is vital in generative settings. However, the notion of correctness of generated code is non-trivial, and thus so is Calibration. In this paper we make several contributions. We develop a framework for evaluating the Calibration of code-generating models. We consider several tasks, correctness criteria, datasets, and approaches, and find that by and large generative code models are not well-calibrated out of the box. We then show how Calibration can be improved, using standard methods such as Platt scaling. Our contributions will lead to better-calibrated decision-making in the current use of code generated by language models, and offers a framework for future research to further improve calibration methods for generative models in Software Engineering.},
	urldate = {2024-02-14},
	publisher = {arXiv},
	author = {Spiess, Claudio and Gros, David and Pai, Kunal Suresh and Pradel, Michael and Rabin, Md Rafiqul Islam and Alipour, Amin and Jha, Susmit and Devanbu, Prem and Ahmed, Toufique},
	month = feb,
	year = {2024},
	note = {arXiv:2402.02047 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\P2LIW739\\2402.html:text/html;Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\BRTRWSQ6\\Spiess et al. - 2024 - Quality and Trust in LLM-generated Code.pdf:application/pdf},
}

@article{ZaibZSM22,
	title = {Conversational question answering: a survey},
	volume = {64},
	issn = {0219-3116},
	shorttitle = {Conversational question answering},
	url = {https://doi.org/10.1007/s10115-022-01744-y},
	doi = {10.1007/s10115-022-01744-y},
	abstract = {Question answering (QA) systems provide a way of querying the information available in various formats including, but not limited to, unstructured and structured data in natural languages. It constitutes a considerable part of conversational artificial intelligence (AI) which has led to the introduction of a special research topic on conversational question answering (CQA), wherein a system is required to understand the given context and then engages in multi-turn QA to satisfy a user’s information needs. While the focus of most of the existing research work is subjected to single-turn QA, the field of multi-turn QA has recently grasped attention and prominence owing to the availability of large-scale, multi-turn QA datasets and the development of pre-trained language models. With a good amount of models and research papers adding to the literature every year recently, there is a dire need of arranging and presenting the related work in a unified manner to streamline future research. This survey is an effort to present a comprehensive review of the state-of-the-art research trends of CQA primarily based on reviewed papers over the recent years. Our findings show that there has been a trend shift from single-turn to multi-turn QA which empowers the field of Conversational AI from different perspectives. This survey is intended to provide an epitome for the research community with the hope of laying a strong foundation for the field of CQA.},
	language = {en},
	number = {12},
	urldate = {2024-02-14},
	journal = {Knowledge and Information Systems},
	author = {Zaib, Munazza and Zhang, Wei Emma and Sheng, Quan Z. and Mahmood, Adnan and Zhang, Yang},
	month = dec,
	year = {2022},
	keywords = {Conversational agents, Conversational AI, Conversational machine reading comprehension, Knowledge base, Question answering},
	pages = {3151--3195},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\2WCXXZJQ\\Zaib et al. - 2022 - Conversational question answering a survey.pdf:application/pdf},
}

@misc{WangLZQ23,
	title = {Avalon's {Game} of {Thoughts}: {Battle} {Against} {Deception} through {Recursive} {Contemplation}},
	shorttitle = {Avalon's {Game} of {Thoughts}},
	url = {http://arxiv.org/abs/2310.01320},
	abstract = {Recent breakthroughs in large language models (LLMs) have brought remarkable success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is that the information processed by LLMs is consistently honest, neglecting the pervasive deceptive or misleading information in human society and AI-generated content. This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes. This study utilizes the intricate Avalon game as a testbed to explore LLMs' potential in deceptive environments. Avalon, full of misinformation and requiring sophisticated logic, manifests as a "Game-of-Thoughts". Inspired by the efficacy of humans' recursive thinking and perspective-taking in the Avalon game, we introduce a novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to identify and counteract deceptive information. ReCon combines formulation and refinement contemplation processes; formulation contemplation produces initial thoughts and speech, while refinement contemplation further polishes them. Additionally, we incorporate first-order and second-order perspective transitions into these processes respectively. Specifically, the first-order allows an LLM agent to infer others' mental states, and the second-order involves understanding how others perceive the agent's mental state. After integrating ReCon with different LLMs, extensive experiment results from the Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around deceptive information without extra fine-tuning and data. Finally, we offer a possible explanation for the efficacy of ReCon and explore the current limitations of LLMs in terms of safety, reasoning, speaking style, and format, potentially furnishing insights for subsequent research.},
	urldate = {2024-02-13},
	publisher = {arXiv},
	author = {Wang, Shenzhi and Liu, Chang and Zheng, Zilong and Qi, Siyuan and Chen, Shuo and Yang, Qisen and Zhao, Andrew and Wang, Chaofei and Song, Shiji and Huang, Gao},
	month = oct,
	year = {2023},
	note = {arXiv:2310.01320 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computers and Society, Computer Science - Multiagent Systems},
	annote = {Comment: 40 pages},
	file = {arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\SRIGEDQJ\\2310.html:text/html;Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\LD5URZEG\\Wang et al. - 2023 - Avalon's Game of Thoughts Battle Against Deceptio.pdf:application/pdf},
}

@misc{ShiZWW23a,
	title = {{CGMI}: {Configurable} {General} {Multi}-{Agent} {Interaction} {Framework}},
	shorttitle = {{CGMI}},
	url = {http://arxiv.org/abs/2308.12503},
	abstract = {Benefiting from the powerful capabilities of large language models (LLMs), agents based on LLMs have shown the potential to address domain-specific tasks and emulate human behaviors. However, the content generated by these agents remains somewhat superficial, owing to their limited domain expertise and the absence of an effective cognitive architecture. To address this, we present the Configurable General Multi-Agent Interaction (CGMI) framework, designed to replicate human interactions in real-world scenarios. Specifically, we propose a tree-structured methodology for the assignment, detection, and maintenance of agent personality. Additionally, we designed a cognitive architecture equipped with a skill library based on the ACT* model, which contains memory, reflection, and planning modules. We have also integrated general agents to augment the virtual environment's realism. Using the CGMI framework, we simulated numerous classroom interactions between teacher and students. The experiments indicate that aspects such as the teaching methodology, curriculum, and student performance closely mirror real classroom settings. We will open source our work.},
	urldate = {2024-02-13},
	publisher = {arXiv},
	author = {Shi, Jinxin and Zhao, Jiabao and Wang, Yilei and Wu, Xingjiao and Li, Jiawen and He, Liang},
	month = aug,
	year = {2023},
	note = {arXiv:2308.12503 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Multiagent Systems},
	annote = {Comment: 11 pages, 15 figures},
	file = {arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\HWUWK73Z\\2308.html:text/html;Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\6WM65RHW\\Jinxin et al. - 2023 - CGMI Configurable General Multi-Agent Interaction.pdf:application/pdf},
}

@misc{ZhugeLFA23a,
	title = {Mindstorms in {Natural} {Language}-{Based} {Societies} of {Mind}},
	url = {http://arxiv.org/abs/2305.17066},
	abstract = {Both Minsky’s “society of mind” and Schmidhuber’s “learning to think” inspire diverse societies of large multimodal neural networks (NNs) that solve problems by interviewing each other in a “mindstorm.” Recent implementations of NN-based societies of minds consist of large language models (LLMs) and other NN-based experts communicating through a natural language interface. In doing so, they overcome the limitations of single LLMs, improving multimodal zero-shot reasoning. In these natural language-based societies of mind (NLSOMs), new agents—all communicating through the same universal symbolic language—are easily added in a modular fashion. To demonstrate the power of NLSOMs, we assemble and experiment with several of them (having up to 129 members), leveraging mindstorms in them to solve some practical AI tasks: visual question answering, image captioning, text-to-image synthesis, 3D generation, egocentric retrieval, embodied AI, and general language-based task solving. We view this as a starting point towards much larger NLSOMs with billions of agents—some of which may be humans. And with this emergence of great societies of heterogeneous minds, many new research questions have suddenly become paramount to the future of artificial intelligence. What should be the social structure of an NLSOM? What would be the (dis)advantages of having a monarchical rather than a democratic structure? How can principles of NN economies be used to maximize the total reward of a reinforcement learning NLSOM? In this work, we identify, discuss, and try to answer some of these questions.},
	language = {en},
	urldate = {2024-09-05},
	publisher = {arXiv},
	author = {Zhuge, Mingchen and Liu, Haozhe and Faccio, Francesco and Ashley, Dylan R. and Csordás, Róbert and Gopalakrishnan, Anand and Hamdi, Abdullah and Hammoud, Hasan Abed Al Kader and Herrmann, Vincent and Irie, Kazuki and Kirsch, Louis and Li, Bing and Li, Guohao and Liu, Shuming and Mai, Jinjie and Piękos, Piotr and Ramesh, Aditya and Schlag, Imanol and Shi, Weimin and Stanić, Aleksandar and Wang, Wenyi and Wang, Yuhui and Xu, Mengmeng and Fan, Deng-Ping and Ghanem, Bernard and Schmidhuber, Jürgen},
	month = may,
	year = {2023},
	note = {arXiv:2305.17066 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Computer Science - Computer Vision and Pattern Recognition, 68T07, I.2.11, I.2.6},
	annote = {Comment: 9 pages in main text + 7 pages of references + 38 pages of appendices, 14 figures in main text + 13 in appendices, 7 tables in appendices},
	file = {Zhuge et al. - 2023 - Mindstorms in Natural Language-Based Societies of .pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\ZX3SEW4H\\Zhuge et al. - 2023 - Mindstorms in Natural Language-Based Societies of .pdf:application/pdf},
}

@misc{HendrycksBBZ21a,
	title = {Measuring {Massive} {Multitask} {Language} {Understanding}},
	url = {http://arxiv.org/abs/2009.03300},
	doi = {10.48550/arXiv.2009.03300},
	abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	month = jan,
	year = {2021},
	note = {arXiv:2009.03300 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computers and Society},
	annote = {Comment: ICLR 2021; the test and code is available at https://github.com/hendrycks/test},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\LAJK7QVP\\Hendrycks et al. - 2021 - Measuring Massive Multitask Language Understanding.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\BQTJMWYW\\2009.html:text/html},
}

@misc{WangMZN24,
	title = {{MMLU}-{Pro}: {A} {More} {Robust} and {Challenging} {Multi}-{Task} {Language} {Understanding} {Benchmark}},
	shorttitle = {{MMLU}-{Pro}},
	url = {http://arxiv.org/abs/2406.01574},
	doi = {10.48550/arXiv.2406.01574},
	abstract = {In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates the trivial and noisy questions in MMLU. Our experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16\% to 33\% compared to MMLU but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5\% in MMLU to just 2\% in MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which is in stark contrast to the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions. Our assessments confirm that MMLU-Pro is a more discriminative benchmark to better track progress in the field.},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and Li, Tianle and Ku, Max and Wang, Kai and Zhuang, Alex and Fan, Rongqi and Yue, Xiang and Chen, Wenhu},
	month = jun,
	year = {2024},
	note = {arXiv:2406.01574 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\ADZF7FZZ\\Wang et al. - 2024 - MMLU-Pro A More Robust and Challenging Multi-Task.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\YPB5MMVS\\2406.html:text/html},
}

@misc{HuaYJC24,
	title = {{TrustAgent}: {Towards} {Safe} and {Trustworthy} {LLM}-based {Agents} through {Agent} {Constitution}},
	shorttitle = {{TrustAgent}},
	url = {http://arxiv.org/abs/2402.01586},
	doi = {10.48550/arXiv.2402.01586},
	abstract = {The rise of LLM-based agents shows great potential to revolutionize task planning, capturing significant attention. Given that these agents will be integrated into high-stake domains, ensuring their reliability and safety is crucial. This paper presents an Agent-Constitution-based agent framework, TrustAgent, with a particular focus on improving the LLM-based agent safety. The proposed framework ensures strict adherence to the Agent Constitution through three strategic components: pre-planning strategy which injects safety knowledge to the model before plan generation, in-planning strategy which enhances safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Our experimental results demonstrate that the proposed framework can effectively enhance an LLM agent's safety across multiple domains by identifying and mitigating potential dangers during the planning. Further analysis reveals that the framework not only improves safety but also enhances the helpfulness of the agent. Additionally, we highlight the importance of the LLM reasoning ability in adhering to the Constitution. This paper sheds light on how to ensure the safe integration of LLM-based agents into human-centric environments. Data and code are available at https://github.com/agiresearch/TrustAgent.},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Hua, Wenyue and Yang, Xianjun and Jin, Mingyu and Cheng, Wei and Tang, Ruixiang and Zhang, Yongfeng},
	month = aug,
	year = {2024},
	note = {arXiv:2402.01586 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\RWH3RSQS\\Hua et al. - 2024 - TrustAgent Towards Safe and Trustworthy LLM-based.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\R9V9CTWQ\\2402.html:text/html},
}

@article{Tan23,
	title = {The {Need} for {Artificial} {Intelligence} in {Solving} {Unsolved} {Criminal} {Cases} and {Sentencing} in {Malaysia}},
	volume = {3},
	copyright = {Copyright (c) 2023 Universiti Telekom Sdn Bhd},
	issn = {2785-8979},
	url = {https://mmupress.com/index.php/ajlp/article/view/701},
	doi = {10.33093/ajlp.2023.7},
	abstract = {As humans, it is common for judges to give wrong verdicts when making decisions, especially in criminal cases. As such, those who feel that they have been wronged by the courts will thus appeal against the decisions. Due to the sheer volume of appeals, it has resulted in a backlog of cases. However, there is no one solution to solve the problem other than calling the judicial officers to improve themselves with legal knowledge before the real use of Artificial Intelligence in legal policy. In the current digital era, it is believed that Artificial Intelligence can accelerate and automate the review of potential evidence in identifying the most relevant and accurate evidence. With the help of Artificial Intelligence, it will reduce court unsolved cases. Countries such as the United States of America, Colombia, and China have started implementing Artificial Intelligence in their respective judicial systems. Yet Singapore’s criminal courts have no plan to use Artificial Intelligence in sentencing. Therefore, it has raised questions like should Malaysia’s judicial response to the use of Artificial Intelligence in cracking those backlog criminal cases and how far could it go in helping the judges. This paper seeks to highlight the issues.},
	language = {en},
	number = {3},
	urldate = {2024-09-04},
	journal = {Asian Journal of Law and Policy},
	author = {Tan, Pei Yee},
	month = dec,
	year = {2023},
	note = {Number: 3},
	keywords = {Sentencing},
	pages = {89--103},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\GA2ZA4DX\\Tan - 2023 - The Need for Artificial Intelligence in Solving Un.pdf:application/pdf},
}

@misc{KamallooDCR23,
	title = {Evaluating {Open}-{Domain} {Question} {Answering} in the {Era} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.06984},
	abstract = {Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more challenging. Without accurate evaluation, the true progress in open-domain QA remains unknown. In this paper, we conduct a thorough analysis of various open-domain QA models, including LLMs, by manually evaluating their answers on a subset of NQ-open, a popular benchmark. Our assessments reveal that while the true performance of all models is significantly underestimated, the performance of the InstructGPT (zero-shot) LLM increases by nearly +60\%, making it on par with existing top models, and the InstructGPT (few-shot) model actually achieves a new state-of-the-art on NQ-open. We also find that more than 50\% of lexical matching failures are attributed to semantically equivalent answers. We further demonstrate that regex matching ranks QA models consistent with human judgments, although still suffering from unnecessary strictness. Finally, we demonstrate that automated evaluation models are a reasonable surrogate for lexical matching in some circumstances, but not for long-form answers generated by LLMs. The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs. At this time, there appears to be no substitute for human evaluation.},
	language = {en},
	urldate = {2024-09-02},
	publisher = {arXiv},
	author = {Kamalloo, Ehsan and Dziri, Nouha and Clarke, Charles L. A. and Rafiei, Davood},
	month = jul,
	year = {2023},
	note = {arXiv:2305.06984 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ACL 2023; code and data released at https://github.com/ehsk/OpenQA-eval},
	file = {Kamalloo et al. - 2023 - Evaluating Open-Domain Question Answering in the E.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\GAJJWITK\\Kamalloo et al. - 2023 - Evaluating Open-Domain Question Answering in the E.pdf:application/pdf},
}

@misc{GomezRodriguezW23,
	title = {A {Confederacy} of {Models}: a {Comprehensive} {Evaluation} of {LLMs} on {Creative} {Writing}},
	shorttitle = {A {Confederacy} of {Models}},
	url = {http://arxiv.org/abs/2310.08433},
	doi = {10.48550/arXiv.2310.08433},
	abstract = {We evaluate a range of recent LLMs on English creative writing, a challenging and complex task that requires imagination, coherence, and style. We use a difficult, open-ended scenario chosen to avoid training data reuse: an epic narration of a single combat between Ignatius J. Reilly, the protagonist of the Pulitzer Prize-winning novel A Confederacy of Dunces (1980), and a pterodactyl, a prehistoric flying reptile. We ask several LLMs and humans to write such a story and conduct a human evalution involving various criteria such as fluency, coherence, originality, humor, and style. Our results show that some state-of-the-art commercial LLMs match or slightly outperform our writers in most dimensions; whereas open-source LLMs lag behind. Humans retain an edge in creativity, while humor shows a binary divide between LLMs that can handle it comparably to humans and those that fail at it. We discuss the implications and limitations of our study and suggest directions for future research.},
	urldate = {2024-09-02},
	publisher = {arXiv},
	author = {Gómez-Rodríguez, Carlos and Williams, Paul},
	month = oct,
	year = {2023},
	note = {arXiv:2310.08433 [cs]},
	keywords = {Computer Science - Computation and Language, I.2.7, Computer Science - Computers and Society, 68T50, 00A64, J.5},
	annote = {Comment: Accepted for publication in Findings of EMNLP 2023},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\M99EH3HP\\Gómez-Rodríguez und Williams - 2023 - A Confederacy of Models a Comprehensive Evaluatio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\8UBDI92M\\2310.html:text/html},
}

@misc{JiangWSK24,
	title = {A {Survey} on {Large} {Language} {Models} for {Code} {Generation}},
	url = {http://arxiv.org/abs/2406.00515},
	doi = {10.48550/arXiv.2406.00515},
	abstract = {Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and offer an empirical comparison using the widely recognized HumanEval and MBPP benchmarks to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource website (https://codellm.github.io) to continuously document and disseminate the most recent advances in the field.},
	urldate = {2024-09-02},
	publisher = {arXiv},
	author = {Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun},
	month = jun,
	year = {2024},
	note = {arXiv:2406.00515 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\33HW6Z2B\\Jiang et al. - 2024 - A Survey on Large Language Models for Code Generat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\NQ3VC3BR\\2406.html:text/html},
}

@misc{MinaeeMNC24a,
	title = {Large {Language} {Models}: {A} {Survey}},
	shorttitle = {Large {Language} {Models}},
	url = {http://arxiv.org/abs/2402.06196},
	doi = {10.48550/arXiv.2402.06196},
	abstract = {Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws {\textbackslash}cite\{kaplan2020scaling,hoffmann2022training\}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.},
	urldate = {2024-09-02},
	publisher = {arXiv},
	author = {Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
	month = feb,
	year = {2024},
	note = {arXiv:2402.06196 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: arXiv admin note: substantial text overlap with arXiv:2401.14423},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\L9HX9HG6\\Minaee et al. - 2024 - Large Language Models A Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\7FIJ9Q8M\\2402.html:text/html},
}

@inproceedings{YangZXL24,
	address = {Mexico City, Mexico},
	title = {Unveiling the {Generalization} {Power} of {Fine}-{Tuned} {Large} {Language} {Models}},
	url = {https://aclanthology.org/2024.naacl-long.51},
	doi = {10.18653/v1/2024.naacl-long.51},
	abstract = {While Large Language Models (LLMs) have demonstrated exceptional multitasking abilities, fine-tuning these models on downstream, domain-specific datasets is often necessary to yield superior performance on test sets compared to their counterparts without fine-tuning. However, the comprehensive effects of fine-tuning on the LLMs' generalization ability are not fully understood.This paper delves into the differences between original, unmodified LLMs and their fine-tuned variants. Our primary investigation centers on whether fine-tuning affects the generalization ability intrinsic to LLMs. To elaborate on this, we conduct extensive experiments across five distinct language tasks on various datasets.Our main findings reveal that models fine-tuned on generation and classification tasks exhibit dissimilar behaviors in generalizing to different domains and tasks.Intriguingly, we observe that integrating the in-context learning strategy during fine-tuning on generation tasks can enhance the model's generalization ability.Through this systematic investigation, we aim to contribute valuable insights into the evolving landscape of fine-tuning practices for LLMs.},
	urldate = {2024-09-02},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Yang, Haoran and Zhang, Yumeng and Xu, Jiaqi and Lu, Hongyuan and Heng, Pheng-Ann and Lam, Wai},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	pages = {884--899},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\2YHPVD9U\\Yang et al. - 2024 - Unveiling the Generalization Power of Fine-Tuned L.pdf:application/pdf},
}

@article{YenHC21,
	title = {Unanswerable {Question} {Correction} in {Question} {Answering} over {Personal} {Knowledge} {Base}},
	volume = {35},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17678},
	doi = {10.1609/aaai.v35i16.17678},
	abstract = {People often encounter situations where they need to recall past experiences from their daily life. In this paper, we aim to construct a question answering system that enables human to query their past experiences over personal knowledge base. Previous works on knowledge base question answering focus on ﬁnding answers for answerable questions. In the real world applications, however, people often muddle up facts and ask those questions that cannot be answered with knowledge base. This work presents a novel system consisting of question answering model and question generation model. It not only answers answerable questions, but also corrects unanswerable questions if necessary. Our question answering model recognizes the question that is inconsistent with the state of the personal knowledge base and suggests facts that can form a feasible question. Then, the facts are converted to an answerable question by the question generation model. For reﬁning question, we propose a question generation model based on the reinforcement learning (RL) with question editing mechanism. Experimental results show that our proposed system is effective for correcting unanswerable questions in personal knowledge base question answering.},
	language = {en},
	number = {16},
	urldate = {2024-08-28},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Yen, An-Zi and Huang, Hen-Hsen and Chen, Hsin-Hsi},
	month = may,
	year = {2021},
	pages = {14266--14275},
	file = {Yen et al. - 2021 - Unanswerable Question Correction in Question Answe.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\GM8QBZH4\\Yen et al. - 2021 - Unanswerable Question Correction in Question Answe.pdf:application/pdf},
}

@article{RossiBWP18,
	series = {{IFAC} {Workshop} on {Networked} \& {Autonomous} {Air} \& {Space} {Systems} {NAASS} 2018},
	title = {Review of {Multi}-{Agent} {Algorithms} for {Collective} {Behavior}: a {Structural} {Taxonomy}},
	volume = {51},
	issn = {2405-8963},
	shorttitle = {Review of {Multi}-{Agent} {Algorithms} for {Collective} {Behavior}},
	url = {https://www.sciencedirect.com/science/article/pii/S2405896318308401},
	doi = {10.1016/j.ifacol.2018.07.097},
	abstract = {In this paper, we review multi-agent collective behavior algorithms in the literature and classify them according to their underlying mathematical structure. For each mathematical technique, we identify the multi-agent coordination tasks it can be applied to, and we analyze its scalability, bandwidth use, and demonstrated maturity. We highlight how versatile techniques such as artificial potential functions can be used for applications ranging from low-level position control to high-level coordination and task allocation, we discuss possible reasons for the slow adoption of complex distributed coordination algorithms in the field, and we highlight areas for further research and development.},
	number = {12},
	urldate = {2024-08-27},
	journal = {IFAC-PapersOnLine},
	author = {Rossi, Federico and Bandyopadhyay, Saptarshi and Wolf, Michael and Pavone, Marco},
	month = jan,
	year = {2018},
	keywords = {Agents, Autonomous mobile robots, Decentralized Control, Distributed Control},
	pages = {112--117},
	file = {Eingereichte Version:C\:\\Users\\Jonas Becker\\Zotero\\storage\\7AW44GSP\\Rossi et al. - 2018 - Review of Multi-Agent Algorithms for Collective Be.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\XW9SC6TA\\S2405896318308401.html:text/html},
}

@misc{LiZDY24,
	title = {Long-context {LLMs} {Struggle} with {Long} {In}-context {Learning}},
	url = {http://arxiv.org/abs/2404.02060},
	doi = {10.48550/arXiv.2404.02060},
	abstract = {Large Language Models (LLMs) have made significant strides in handling long sequences. Some models like Gemini could even to be capable of dealing with millions of tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their true abilities in more challenging, real-world scenarios. We introduce a benchmark (LongICLBench) for long in-context learning in extreme-label classification using six datasets with 28 to 174 classes and input lengths from 2K to 50K tokens. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct predictions. We evaluate on 15 long-context LLMs and find that they perform well on less challenging classification tasks with smaller label space and shorter demonstrations. However, they struggle with more challenging task like Discovery with 174 labels, suggesting a gap in their ability to process long, context-rich sequences. Further analysis reveals a bias towards labels presented later in the sequence and a need for improved reasoning over multiple pieces of information. Our study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. We believe LongICLBench could serve as a more realistic evaluation for the future long-context LLMs.},
	urldate = {2024-08-24},
	publisher = {arXiv},
	author = {Li, Tianle and Zhang, Ge and Do, Quy Duc and Yue, Xiang and Chen, Wenhu},
	month = jun,
	year = {2024},
	note = {arXiv:2404.02060 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\EMN955Z2\\Li et al. - 2024 - Long-context LLMs Struggle with Long In-context Le.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\94TGSTSR\\2404.html:text/html},
}

@inproceedings{GehmanGSC20c,
	address = {Online},
	title = {{RealToxicityPrompts}: {Evaluating} {Neural} {Toxic} {Degeneration} in {Language} {Models}},
	shorttitle = {{RealToxicityPrompts}},
	url = {https://aclanthology.org/2020.findings-emnlp.301},
	doi = {10.18653/v1/2020.findings-emnlp.301},
	abstract = {Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning “bad” words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.},
	urldate = {2024-08-25},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A.},
	editor = {Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {3356--3369},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\W5VAU443\\Gehman et al. - 2020 - RealToxicityPrompts Evaluating Neural Toxic Degen.pdf:application/pdf},
}

@misc{GeminiTeamABA24,
	title = {Gemini: {A} {Family} of {Highly} {Capable} {Multimodal} {Models}},
	shorttitle = {Gemini},
	url = {http://arxiv.org/abs/2312.11805},
	doi = {10.48550/arXiv.2312.11805},
	abstract = {This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Gemini Team and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M. and Hauth, Anja and Millican, Katie and Silver, David and Johnson, Melvin and Antonoglou, Ioannis and Schrittwieser, Julian and Glaese, Amelia and Chen, Jilin and Pitler, Emily and Lillicrap, Timothy and Lazaridou, Angeliki and Firat, Orhan and Molloy, James and Isard, Michael and Barham, Paul R. and Hennigan, Tom and Lee, Benjamin and Viola, Fabio and Reynolds, Malcolm and Xu, Yuanzhong and Doherty, Ryan and Collins, Eli and Meyer, Clemens and Rutherford, Eliza and Moreira, Erica and Ayoub, Kareem and Goel, Megha and Krawczyk, Jack and Du, Cosmo and Chi, Ed and Cheng, Heng-Tze and Ni, Eric and Shah, Purvi and Kane, Patrick and Chan, Betty and Faruqui, Manaal and Severyn, Aliaksei and Lin, Hanzhao and Li, YaGuang and Cheng, Yong and Ittycheriah, Abe and Mahdieh, Mahdis and Chen, Mia and Sun, Pei and Tran, Dustin and Bagri, Sumit and Lakshminarayanan, Balaji and Liu, Jeremiah and Orban, Andras and Güra, Fabian and Zhou, Hao and Song, Xinying and Boffy, Aurelien and Ganapathy, Harish and Zheng, Steven and Choe, HyunJeong and Weisz, Ágoston and Zhu, Tao and Lu, Yifeng and Gopal, Siddharth and Kahn, Jarrod and Kula, Maciej and Pitman, Jeff and Shah, Rushin and Taropa, Emanuel and Merey, Majd Al and Baeuml, Martin and Chen, Zhifeng and Shafey, Laurent El and Zhang, Yujing and Sercinoglu, Olcan and Tucker, George and Piqueras, Enrique and Krikun, Maxim and Barr, Iain and Savinov, Nikolay and Danihelka, Ivo and Roelofs, Becca and White, Anaïs and Andreassen, Anders and von Glehn, Tamara and Yagati, Lakshman and Kazemi, Mehran and Gonzalez, Lucas and Khalman, Misha and Sygnowski, Jakub and Frechette, Alexandre and Smith, Charlotte and Culp, Laura and Proleev, Lev and Luan, Yi and Chen, Xi and Lottes, James and Schucher, Nathan and Lebron, Federico and Rrustemi, Alban and Clay, Natalie and Crone, Phil and Kocisky, Tomas and Zhao, Jeffrey and Perz, Bartek and Yu, Dian and Howard, Heidi and Bloniarz, Adam and Rae, Jack W. and Lu, Han and Sifre, Laurent and Maggioni, Marcello and Alcober, Fred and Garrette, Dan and Barnes, Megan and Thakoor, Shantanu and Austin, Jacob and Barth-Maron, Gabriel and Wong, William and Joshi, Rishabh and Chaabouni, Rahma and Fatiha, Deeni and Ahuja, Arun and Tomar, Gaurav Singh and Senter, Evan and Chadwick, Martin and Kornakov, Ilya and Attaluri, Nithya and Iturrate, Iñaki and Liu, Ruibo and Li, Yunxuan and Cogan, Sarah and Chen, Jeremy and Jia, Chao and Gu, Chenjie and Zhang, Qiao and Grimstad, Jordan and Hartman, Ale Jakse and Garcia, Xavier and Pillai, Thanumalayan Sankaranarayana and Devlin, Jacob and Laskin, Michael and Casas, Diego de Las and Valter, Dasha and Tao, Connie and Blanco, Lorenzo and Badia, Adrià Puigdomènech and Reitter, David and Chen, Mianna and Brennan, Jenny and Rivera, Clara and Brin, Sergey and Iqbal, Shariq and Surita, Gabriela and Labanowski, Jane and Rao, Abhi and Winkler, Stephanie and Parisotto, Emilio and Gu, Yiming and Olszewska, Kate and Addanki, Ravi and Miech, Antoine and Louis, Annie and Teplyashin, Denis and Brown, Geoff and Catt, Elliot and Balaguer, Jan and Xiang, Jackie and Wang, Pidong and Ashwood, Zoe and Briukhov, Anton and Webson, Albert and Ganapathy, Sanjay and Sanghavi, Smit and Kannan, Ajay and Chang, Ming-Wei and Stjerngren, Axel and Djolonga, Josip and Sun, Yuting and Bapna, Ankur and Aitchison, Matthew and Pejman, Pedram and Michalewski, Henryk and Yu, Tianhe and Wang, Cindy and Love, Juliette and Ahn, Junwhan and Bloxwich, Dawn and Han, Kehang and Humphreys, Peter and Sellam, Thibault and Bradbury, James and Godbole, Varun and Samangooei, Sina and Damoc, Bogdan and Kaskasoli, Alex and Arnold, Sébastien M. R. and Vasudevan, Vijay and Agrawal, Shubham and Riesa, Jason and Lepikhin, Dmitry and Tanburn, Richard and Srinivasan, Srivatsan and Lim, Hyeontaek and Hodkinson, Sarah and Shyam, Pranav and Ferret, Johan and Hand, Steven and Garg, Ankush and Paine, Tom Le and Li, Jian and Li, Yujia and Giang, Minh and Neitz, Alexander and Abbas, Zaheer and York, Sarah and Reid, Machel and Cole, Elizabeth and Chowdhery, Aakanksha and Das, Dipanjan and Rogozińska, Dominika and Nikolaev, Vitaliy and Sprechmann, Pablo and Nado, Zachary and Zilka, Lukas and Prost, Flavien and He, Luheng and Monteiro, Marianne and Mishra, Gaurav and Welty, Chris and Newlan, Josh and Jia, Dawei and Allamanis, Miltiadis and Hu, Clara Huiyi and de Liedekerke, Raoul and Gilmer, Justin and Saroufim, Carl and Rijhwani, Shruti and Hou, Shaobo and Shrivastava, Disha and Baddepudi, Anirudh and Goldin, Alex and Ozturel, Adnan and Cassirer, Albin and Xu, Yunhan and Sohn, Daniel and Sachan, Devendra and Amplayo, Reinald Kim and Swanson, Craig and Petrova, Dessie and Narayan, Shashi and Guez, Arthur and Brahma, Siddhartha and Landon, Jessica and Patel, Miteyan and Zhao, Ruizhe and Villela, Kevin and Wang, Luyu and Jia, Wenhao and Rahtz, Matthew and Giménez, Mai and Yeung, Legg and Keeling, James and Georgiev, Petko and Mincu, Diana and Wu, Boxi and Haykal, Salem and Saputro, Rachel and Vodrahalli, Kiran and Qin, James and Cankara, Zeynep and Sharma, Abhanshu and Fernando, Nick and Hawkins, Will and Neyshabur, Behnam and Kim, Solomon and Hutter, Adrian and Agrawal, Priyanka and Castro-Ros, Alex and Driessche, George van den and Wang, Tao and Yang, Fan and Chang, Shuo-yiin and Komarek, Paul and McIlroy, Ross and Lučić, Mario and Zhang, Guodong and Farhan, Wael and Sharman, Michael and Natsev, Paul and Michel, Paul and Bansal, Yamini and Qiao, Siyuan and Cao, Kris and Shakeri, Siamak and Butterfield, Christina and Chung, Justin and Rubenstein, Paul Kishan and Agrawal, Shivani and Mensch, Arthur and Soparkar, Kedar and Lenc, Karel and Chung, Timothy and Pope, Aedan and Maggiore, Loren and Kay, Jackie and Jhakra, Priya and Wang, Shibo and Maynez, Joshua and Phuong, Mary and Tobin, Taylor and Tacchetti, Andrea and Trebacz, Maja and Robinson, Kevin and Katariya, Yash and Riedel, Sebastian and Bailey, Paige and Xiao, Kefan and Ghelani, Nimesh and Aroyo, Lora and Slone, Ambrose and Houlsby, Neil and Xiong, Xuehan and Yang, Zhen and Gribovskaya, Elena and Adler, Jonas and Wirth, Mateo and Lee, Lisa and Li, Music and Kagohara, Thais and Pavagadhi, Jay and Bridgers, Sophie and Bortsova, Anna and Ghemawat, Sanjay and Ahmed, Zafarali and Liu, Tianqi and Powell, Richard and Bolina, Vijay and Iinuma, Mariko and Zablotskaia, Polina and Besley, James and Chung, Da-Woon and Dozat, Timothy and Comanescu, Ramona and Si, Xiance and Greer, Jeremy and Su, Guolong and Polacek, Martin and Kaufman, Raphaël Lopez and Tokumine, Simon and Hu, Hexiang and Buchatskaya, Elena and Miao, Yingjie and Elhawaty, Mohamed and Siddhant, Aditya and Tomasev, Nenad and Xing, Jinwei and Greer, Christina and Miller, Helen and Ashraf, Shereen and Roy, Aurko and Zhang, Zizhao and Ma, Ada and Filos, Angelos and Besta, Milos and Blevins, Rory and Klimenko, Ted and Yeh, Chih-Kuan and Changpinyo, Soravit and Mu, Jiaqi and Chang, Oscar and Pajarskas, Mantas and Muir, Carrie and Cohen, Vered and Lan, Charline Le and Haridasan, Krishna and Marathe, Amit and Hansen, Steven and Douglas, Sholto and Samuel, Rajkumar and Wang, Mingqiu and Austin, Sophia and Lan, Chang and Jiang, Jiepu and Chiu, Justin and Lorenzo, Jaime Alonso and Sjösund, Lars Lowe and Cevey, Sébastien and Gleicher, Zach and Avrahami, Thi and Boral, Anudhyan and Srinivasan, Hansa and Selo, Vittorio and May, Rhys and Aisopos, Konstantinos and Hussenot, Léonard and Soares, Livio Baldini and Baumli, Kate and Chang, Michael B. and Recasens, Adrià and Caine, Ben and Pritzel, Alexander and Pavetic, Filip and Pardo, Fabio and Gergely, Anita and Frye, Justin and Ramasesh, Vinay and Horgan, Dan and Badola, Kartikeya and Kassner, Nora and Roy, Subhrajit and Dyer, Ethan and Campos, Víctor Campos and Tomala, Alex and Tang, Yunhao and Badawy, Dalia El and White, Elspeth and Mustafa, Basil and Lang, Oran and Jindal, Abhishek and Vikram, Sharad and Gong, Zhitao and Caelles, Sergi and Hemsley, Ross and Thornton, Gregory and Feng, Fangxiaoyu and Stokowiec, Wojciech and Zheng, Ce and Thacker, Phoebe and Ünlü, Çağlar and Zhang, Zhishuai and Saleh, Mohammad and Svensson, James and Bileschi, Max and Patil, Piyush and Anand, Ankesh and Ring, Roman and Tsihlas, Katerina and Vezer, Arpi and Selvi, Marco and Shevlane, Toby and Rodriguez, Mikel and Kwiatkowski, Tom and Daruki, Samira and Rong, Keran and Dafoe, Allan and FitzGerald, Nicholas and Gu-Lemberg, Keren and Khan, Mina and Hendricks, Lisa Anne and Pellat, Marie and Feinberg, Vladimir and Cobon-Kerr, James and Sainath, Tara and Rauh, Maribeth and Hashemi, Sayed Hadi and Ives, Richard and Hasson, Yana and Noland, Eric and Cao, Yuan and Byrd, Nathan and Hou, Le and Wang, Qingze and Sottiaux, Thibault and Paganini, Michela and Lespiau, Jean-Baptiste and Moufarek, Alexandre and Hassan, Samer and Shivakumar, Kaushik and van Amersfoort, Joost and Mandhane, Amol and Joshi, Pratik and Goyal, Anirudh and Tung, Matthew and Brock, Andrew and Sheahan, Hannah and Misra, Vedant and Li, Cheng and Rakićević, Nemanja and Dehghani, Mostafa and Liu, Fangyu and Mittal, Sid and Oh, Junhyuk and Noury, Seb and Sezener, Eren and Huot, Fantine and Lamm, Matthew and De Cao, Nicola and Chen, Charlie and Mudgal, Sidharth and Stella, Romina and Brooks, Kevin and Vasudevan, Gautam and Liu, Chenxi and Chain, Mainak and Melinkeri, Nivedita and Cohen, Aaron and Wang, Venus and Seymore, Kristie and Zubkov, Sergey and Goel, Rahul and Yue, Summer and Krishnakumaran, Sai and Albert, Brian and Hurley, Nate and Sano, Motoki and Mohananey, Anhad and Joughin, Jonah and Filonov, Egor and Kępa, Tomasz and Eldawy, Yomna and Lim, Jiawern and Rishi, Rahul and Badiezadegan, Shirin and Bos, Taylor and Chang, Jerry and Jain, Sanil and Padmanabhan, Sri Gayatri Sundara and Puttagunta, Subha and Krishna, Kalpesh and Baker, Leslie and Kalb, Norbert and Bedapudi, Vamsi and Kurzrok, Adam and Lei, Shuntong and Yu, Anthony and Litvin, Oren and Zhou, Xiang and Wu, Zhichun and Sobell, Sam and Siciliano, Andrea and Papir, Alan and Neale, Robby and Bragagnolo, Jonas and Toor, Tej and Chen, Tina and Anklin, Valentin and Wang, Feiran and Feng, Richie and Gholami, Milad and Ling, Kevin and Liu, Lijuan and Walter, Jules and Moghaddam, Hamid and Kishore, Arun and Adamek, Jakub and Mercado, Tyler and Mallinson, Jonathan and Wandekar, Siddhinita and Cagle, Stephen and Ofek, Eran and Garrido, Guillermo and Lombriser, Clemens and Mukha, Maksim and Sun, Botu and Mohammad, Hafeezul Rahman and Matak, Josip and Qian, Yadi and Peswani, Vikas and Janus, Pawel and Yuan, Quan and Schelin, Leif and David, Oana and Garg, Ankur and He, Yifan and Duzhyi, Oleksii and Älgmyr, Anton and Lottaz, Timothée and Li, Qi and Yadav, Vikas and Xu, Luyao and Chinien, Alex and Shivanna, Rakesh and Chuklin, Aleksandr and Li, Josie and Spadine, Carrie and Wolfe, Travis and Mohamed, Kareem and Das, Subhabrata and Dai, Zihang and He, Kyle and von Dincklage, Daniel and Upadhyay, Shyam and Maurya, Akanksha and Chi, Luyan and Krause, Sebastian and Salama, Khalid and Rabinovitch, Pam G. and M, Pavan Kumar Reddy and Selvan, Aarush and Dektiarev, Mikhail and Ghiasi, Golnaz and Guven, Erdem and Gupta, Himanshu and Liu, Boyi and Sharma, Deepak and Shtacher, Idan Heimlich and Paul, Shachi and Akerlund, Oscar and Aubet, François-Xavier and Huang, Terry and Zhu, Chen and Zhu, Eric and Teixeira, Elico and Fritze, Matthew and Bertolini, Francesco and Marinescu, Liana-Eleonora and Bölle, Martin and Paulus, Dominik and Gupta, Khyatti and Latkar, Tejasi and Chang, Max and Sanders, Jason and Wilson, Roopa and Wu, Xuewei and Tan, Yi-Xuan and Thiet, Lam Nguyen and Doshi, Tulsee and Lall, Sid and Mishra, Swaroop and Chen, Wanming and Luong, Thang and Benjamin, Seth and Lee, Jasmine and Andrejczuk, Ewa and Rabiej, Dominik and Ranjan, Vipul and Styrc, Krzysztof and Yin, Pengcheng and Simon, Jon and Harriott, Malcolm Rose and Bansal, Mudit and Robsky, Alexei and Bacon, Geoff and Greene, David and Mirylenka, Daniil and Zhou, Chen and Sarvana, Obaid and Goyal, Abhimanyu and Andermatt, Samuel and Siegler, Patrick and Horn, Ben and Israel, Assaf and Pongetti, Francesco and Chen, Chih-Wei "Louis" and Selvatici, Marco and Silva, Pedro and Wang, Kathie and Tolins, Jackson and Guu, Kelvin and Yogev, Roey and Cai, Xiaochen and Agostini, Alessandro and Shah, Maulik and Nguyen, Hung and Donnaile, Noah Ó and Pereira, Sébastien and Friso, Linda and Stambler, Adam and Kurzrok, Adam and Kuang, Chenkai and Romanikhin, Yan and Geller, Mark and Yan, Z. J. and Jang, Kane and Lee, Cheng-Chun and Fica, Wojciech and Malmi, Eric and Tan, Qijun and Banica, Dan and Balle, Daniel and Pham, Ryan and Huang, Yanping and Avram, Diana and Shi, Hongzhi and Singh, Jasjot and Hidey, Chris and Ahuja, Niharika and Saxena, Pranab and Dooley, Dan and Potharaju, Srividya Pranavi and O'Neill, Eileen and Gokulchandran, Anand and Foley, Ryan and Zhao, Kai and Dusenberry, Mike and Liu, Yuan and Mehta, Pulkit and Kotikalapudi, Ragha and Safranek-Shrader, Chalence and Goodman, Andrew and Kessinger, Joshua and Globen, Eran and Kolhar, Prateek and Gorgolewski, Chris and Ibrahim, Ali and Song, Yang and Eichenbaum, Ali and Brovelli, Thomas and Potluri, Sahitya and Lahoti, Preethi and Baetu, Cip and Ghorbani, Ali and Chen, Charles and Crawford, Andy and Pal, Shalini and Sridhar, Mukund and Gurita, Petru and Mujika, Asier and Petrovski, Igor and Cedoz, Pierre-Louis and Li, Chenmei and Chen, Shiyuan and Santo, Niccolò Dal and Goyal, Siddharth and Punjabi, Jitesh and Kappaganthu, Karthik and Kwak, Chester and LV, Pallavi and Velury, Sarmishta and Choudhury, Himadri and Hall, Jamie and Shah, Premal and Figueira, Ricardo and Thomas, Matt and Lu, Minjie and Zhou, Ting and Kumar, Chintu and Jurdi, Thomas and Chikkerur, Sharat and Ma, Yenai and Yu, Adams and Kwak, Soo and Ähdel, Victor and Rajayogam, Sujeevan and Choma, Travis and Liu, Fei and Barua, Aditya and Ji, Colin and Park, Ji Ho and Hellendoorn, Vincent and Bailey, Alex and Bilal, Taylan and Zhou, Huanjie and Khatir, Mehrdad and Sutton, Charles and Rzadkowski, Wojciech and Macintosh, Fiona and Shagin, Konstantin and Medina, Paul and Liang, Chen and Zhou, Jinjing and Shah, Pararth and Bi, Yingying and Dankovics, Attila and Banga, Shipra and Lehmann, Sabine and Bredesen, Marissa and Lin, Zifan and Hoffmann, John Eric and Lai, Jonathan and Chung, Raynald and Yang, Kai and Balani, Nihal and Bražinskas, Arthur and Sozanschi, Andrei and Hayes, Matthew and Alcalde, Héctor Fernández and Makarov, Peter and Chen, Will and Stella, Antonio and Snijders, Liselotte and Mandl, Michael and Kärrman, Ante and Nowak, Paweł and Wu, Xinyi and Dyck, Alex and Vaidyanathan, Krishnan and R, Raghavender and Mallet, Jessica and Rudominer, Mitch and Johnston, Eric and Mittal, Sushil and Udathu, Akhil and Christensen, Janara and Verma, Vishal and Irving, Zach and Santucci, Andreas and Elsayed, Gamaleldin and Davoodi, Elnaz and Georgiev, Marin and Tenney, Ian and Hua, Nan and Cideron, Geoffrey and Leurent, Edouard and Alnahlawi, Mahmoud and Georgescu, Ionut and Wei, Nan and Zheng, Ivy and Scandinaro, Dylan and Jiang, Heinrich and Snoek, Jasper and Sundararajan, Mukund and Wang, Xuezhi and Ontiveros, Zack and Karo, Itay and Cole, Jeremy and Rajashekhar, Vinu and Tumeh, Lara and Ben-David, Eyal and Jain, Rishub and Uesato, Jonathan and Datta, Romina and Bunyan, Oskar and Wu, Shimu and Zhang, John and Stanczyk, Piotr and Zhang, Ye and Steiner, David and Naskar, Subhajit and Azzam, Michael and Johnson, Matthew and Paszke, Adam and Chiu, Chung-Cheng and Elias, Jaume Sanchez and Mohiuddin, Afroz and Muhammad, Faizan and Miao, Jin and Lee, Andrew and Vieillard, Nino and Park, Jane and Zhang, Jiageng and Stanway, Jeff and Garmon, Drew and Karmarkar, Abhijit and Dong, Zhe and Lee, Jong and Kumar, Aviral and Zhou, Luowei and Evens, Jonathan and Isaac, William and Irving, Geoffrey and Loper, Edward and Fink, Michael and Arkatkar, Isha and Chen, Nanxin and Shafran, Izhak and Petrychenko, Ivan and Chen, Zhe and Jia, Johnson and Levskaya, Anselm and Zhu, Zhenkai and Grabowski, Peter and Mao, Yu and Magni, Alberto and Yao, Kaisheng and Snaider, Javier and Casagrande, Norman and Palmer, Evan and Suganthan, Paul and Castaño, Alfonso and Giannoumis, Irene and Kim, Wooyeol and Rybiński, Mikołaj and Sreevatsa, Ashwin and Prendki, Jennifer and Soergel, David and Goedeckemeyer, Adrian and Gierke, Willi and Jafari, Mohsen and Gaba, Meenu and Wiesner, Jeremy and Wright, Diana Gage and Wei, Yawen and Vashisht, Harsha and Kulizhskaya, Yana and Hoover, Jay and Le, Maigo and Li, Lu and Iwuanyanwu, Chimezie and Liu, Lu and Ramirez, Kevin and Khorlin, Andrey and Cui, Albert and LIN, Tian and Wu, Marcus and Aguilar, Ricardo and Pallo, Keith and Chakladar, Abhishek and Perng, Ginger and Abellan, Elena Allica and Zhang, Mingyang and Dasgupta, Ishita and Kushman, Nate and Penchev, Ivo and Repina, Alena and Wu, Xihui and van der Weide, Tom and Ponnapalli, Priya and Kaplan, Caroline and Simsa, Jiri and Li, Shuangfeng and Dousse, Olivier and Yang, Fan and Piper, Jeff and Ie, Nathan and Pasumarthi, Rama and Lintz, Nathan and Vijayakumar, Anitha and Andor, Daniel and Valenzuela, Pedro and Lui, Minnie and Paduraru, Cosmin and Peng, Daiyi and Lee, Katherine and Zhang, Shuyuan and Greene, Somer and Nguyen, Duc Dung and Kurylowicz, Paula and Hardin, Cassidy and Dixon, Lucas and Janzer, Lili and Choo, Kiam and Feng, Ziqiang and Zhang, Biao and Singhal, Achintya and Du, Dayou and McKinnon, Dan and Antropova, Natasha and Bolukbasi, Tolga and Keller, Orgad and Reid, David and Finchelstein, Daniel and Raad, Maria Abi and Crocker, Remi and Hawkins, Peter and Dadashi, Robert and Gaffney, Colin and Franko, Ken and Bulanova, Anna and Leblond, Rémi and Chung, Shirley and Askham, Harry and Cobo, Luis C. and Xu, Kelvin and Fischer, Felix and Xu, Jun and Sorokin, Christina and Alberti, Chris and Lin, Chu-Cheng and Evans, Colin and Dimitriev, Alek and Forbes, Hannah and Banarse, Dylan and Tung, Zora and Omernick, Mark and Bishop, Colton and Sterneck, Rachel and Jain, Rohan and Xia, Jiawei and Amid, Ehsan and Piccinno, Francesco and Wang, Xingyu and Banzal, Praseem and Mankowitz, Daniel J. and Polozov, Alex and Krakovna, Victoria and Brown, Sasha and Bateni, MohammadHossein and Duan, Dennis and Firoiu, Vlad and Thotakuri, Meghana and Natan, Tom and Geist, Matthieu and Girgin, Ser tan and Li, Hui and Ye, Jiayu and Roval, Ofir and Tojo, Reiko and Kwong, Michael and Lee-Thorp, James and Yew, Christopher and Sinopalnikov, Danila and Ramos, Sabela and Mellor, John and Sharma, Abhishek and Wu, Kathy and Miller, David and Sonnerat, Nicolas and Vnukov, Denis and Greig, Rory and Beattie, Jennifer and Caveness, Emily and Bai, Libin and Eisenschlos, Julian and Korchemniy, Alex and Tsai, Tomy and Jasarevic, Mimi and Kong, Weize and Dao, Phuong and Zheng, Zeyu and Liu, Frederick and Yang, Fan and Zhu, Rui and Teh, Tian Huey and Sanmiya, Jason and Gladchenko, Evgeny and Trdin, Nejc and Toyama, Daniel and Rosen, Evan and Tavakkol, Sasan and Xue, Linting and Elkind, Chen and Woodman, Oliver and Carpenter, John and Papamakarios, George and Kemp, Rupert and Kafle, Sushant and Grunina, Tanya and Sinha, Rishika and Talbert, Alice and Wu, Diane and Owusu-Afriyie, Denese and Du, Cosmo and Thornton, Chloe and Pont-Tuset, Jordi and Narayana, Pradyumna and Li, Jing and Fatehi, Saaber and Wieting, John and Ajmeri, Omar and Uria, Benigno and Ko, Yeongil and Knight, Laura and Héliou, Amélie and Niu, Ning and Gu, Shane and Pang, Chenxi and Li, Yeqing and Levine, Nir and Stolovich, Ariel and Santamaria-Fernandez, Rebeca and Goenka, Sonam and Yustalim, Wenny and Strudel, Robin and Elqursh, Ali and Deck, Charlie and Lee, Hyo and Li, Zonglin and Levin, Kyle and Hoffmann, Raphael and Holtmann-Rice, Dan and Bachem, Olivier and Arora, Sho and Koh, Christy and Yeganeh, Soheil Hassas and Põder, Siim and Tariq, Mukarram and Sun, Yanhua and Ionita, Lucian and Seyedhosseini, Mojtaba and Tafti, Pouya and Liu, Zhiyu and Gulati, Anmol and Liu, Jasmine and Ye, Xinyu and Chrzaszcz, Bart and Wang, Lily and Sethi, Nikhil and Li, Tianrun and Brown, Ben and Singh, Shreya and Fan, Wei and Parisi, Aaron and Stanton, Joe and Koverkathu, Vinod and Choquette-Choo, Christopher A. and Li, Yunjie and Lu, T. J. and Ittycheriah, Abe and Shroff, Prakash and Varadarajan, Mani and Bahargam, Sanaz and Willoughby, Rob and Gaddy, David and Desjardins, Guillaume and Cornero, Marco and Robenek, Brona and Mittal, Bhavishya and Albrecht, Ben and Shenoy, Ashish and Moiseev, Fedor and Jacobsson, Henrik and Ghaffarkhah, Alireza and Rivière, Morgane and Walton, Alanna and Crepy, Clément and Parrish, Alicia and Zhou, Zongwei and Farabet, Clement and Radebaugh, Carey and Srinivasan, Praveen and van der Salm, Claudia and Fidjeland, Andreas and Scellato, Salvatore and Latorre-Chimoto, Eri and Klimczak-Plucińska, Hanna and Bridson, David and de Cesare, Dario and Hudson, Tom and Mendolicchio, Piermaria and Walker, Lexi and Morris, Alex and Mauger, Matthew and Guseynov, Alexey and Reid, Alison and Odoom, Seth and Loher, Lucia and Cotruta, Victor and Yenugula, Madhavi and Grewe, Dominik and Petrushkina, Anastasia and Duerig, Tom and Sanchez, Antonio and Yadlowsky, Steve and Shen, Amy and Globerson, Amir and Webb, Lynette and Dua, Sahil and Li, Dong and Bhupatiraju, Surya and Hurt, Dan and Qureshi, Haroon and Agarwal, Ananth and Shani, Tomer and Eyal, Matan and Khare, Anuj and Belle, Shreyas Rammohan and Wang, Lei and Tekur, Chetan and Kale, Mihir Sanjay and Wei, Jinliang and Sang, Ruoxin and Saeta, Brennan and Liechty, Tyler and Sun, Yi and Zhao, Yao and Lee, Stephan and Nayak, Pandu and Fritz, Doug and Vuyyuru, Manish Reddy and Aslanides, John and Vyas, Nidhi and Wicke, Martin and Ma, Xiao and Eltyshev, Evgenii and Martin, Nina and Cate, Hardie and Manyika, James and Amiri, Keyvan and Kim, Yelin and Xiong, Xi and Kang, Kai and Luisier, Florian and Tripuraneni, Nilesh and Madras, David and Guo, Mandy and Waters, Austin and Wang, Oliver and Ainslie, Joshua and Baldridge, Jason and Zhang, Han and Pruthi, Garima and Bauer, Jakob and Yang, Feng and Mansour, Riham and Gelman, Jason and Xu, Yang and Polovets, George and Liu, Ji and Cai, Honglong and Chen, Warren and Sheng, XiangHai and Xue, Emily and Ozair, Sherjil and Angermueller, Christof and Li, Xiaowei and Sinha, Anoop and Wang, Weiren and Wiesinger, Julia and Koukoumidis, Emmanouil and Tian, Yuan and Iyer, Anand and Gurumurthy, Madhu and Goldenson, Mark and Shah, Parashar and Blake, M. K. and Yu, Hongkun and Urbanowicz, Anthony and Palomaki, Jennimaria and Fernando, Chrisantha and Durden, Ken and Mehta, Harsh and Momchev, Nikola and Rahimtoroghi, Elahe and Georgaki, Maria and Raul, Amit and Ruder, Sebastian and Redshaw, Morgan and Lee, Jinhyuk and Zhou, Denny and Jalan, Komal and Li, Dinghua and Hechtman, Blake and Schuh, Parker and Nasr, Milad and Milan, Kieran and Mikulik, Vladimir and Franco, Juliana and Green, Tim and Nguyen, Nam and Kelley, Joe and Mahendru, Aroma and Hu, Andrea and Howland, Joshua and Vargas, Ben and Hui, Jeffrey and Bansal, Kshitij and Rao, Vikram and Ghiya, Rakesh and Wang, Emma and Ye, Ke and Sarr, Jean Michel and Preston, Melanie Moranski and Elish, Madeleine and Li, Steve and Kaku, Aakash and Gupta, Jigar and Pasupat, Ice and Juan, Da-Cheng and Someswar, Milan and M., Tejvi and Chen, Xinyun and Amini, Aida and Fabrikant, Alex and Chu, Eric and Dong, Xuanyi and Muthal, Amruta and Buthpitiya, Senaka and Jauhari, Sarthak and Hua, Nan and Khandelwal, Urvashi and Hitron, Ayal and Ren, Jie and Rinaldi, Larissa and Drath, Shahar and Dabush, Avigail and Jiang, Nan-Jiang and Godhia, Harshal and Sachs, Uli and Chen, Anthony and Fan, Yicheng and Taitelbaum, Hagai and Noga, Hila and Dai, Zhuyun and Wang, James and Liang, Chen and Hamer, Jenny and Ferng, Chun-Sung and Elkind, Chenel and Atias, Aviel and Lee, Paulina and Listík, Vít and Carlen, Mathias and van de Kerkhof, Jan and Pikus, Marcin and Zaher, Krunoslav and Müller, Paul and Zykova, Sasha and Stefanec, Richard and Gatsko, Vitaly and Hirnschall, Christoph and Sethi, Ashwin and Xu, Xingyu Federico and Ahuja, Chetan and Tsai, Beth and Stefanoiu, Anca and Feng, Bo and Dhandhania, Keshav and Katyal, Manish and Gupta, Akshay and Parulekar, Atharva and Pitta, Divya and Zhao, Jing and Bhatia, Vivaan and Bhavnani, Yashodha and Alhadlaq, Omar and Li, Xiaolin and Danenberg, Peter and Tu, Dennis and Pine, Alex and Filippova, Vera and Ghosh, Abhipso and Limonchik, Ben and Urala, Bhargava and Lanka, Chaitanya Krishna and Clive, Derik and Sun, Yi and Li, Edward and Wu, Hao and Hongtongsak, Kevin and Li, Ianna and Thakkar, Kalind and Omarov, Kuanysh and Majmundar, Kushal and Alverson, Michael and Kucharski, Michael and Patel, Mohak and Jain, Mudit and Zabelin, Maksim and Pelagatti, Paolo and Kohli, Rohan and Kumar, Saurabh and Kim, Joseph and Sankar, Swetha and Shah, Vineet and Ramachandruni, Lakshmi and Zeng, Xiangkai and Bariach, Ben and Weidinger, Laura and Vu, Tu and Andreev, Alek and He, Antoine and Hui, Kevin and Kashem, Sheleem and Subramanya, Amar and Hsiao, Sissie and Hassabis, Demis and Kavukcuoglu, Koray and Sadovsky, Adam and Le, Quoc and Strohman, Trevor and Wu, Yonghui and Petrov, Slav and Dean, Jeffrey and Vinyals, Oriol},
	month = jun,
	year = {2024},
	note = {arXiv:2312.11805 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\A4UDFTUV\\Gemini Team et al. - 2024 - Gemini A Family of Highly Capable Multimodal Mode.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\GW9CMD6F\\2312.html:text/html},
}

@misc{SunLZZ24,
	title = {Towards {Detecting} {LLMs} {Hallucination} via {Markov} {Chain}-based {Multi}-agent {Debate} {Framework}},
	url = {http://arxiv.org/abs/2406.03075},
	abstract = {The advent of large language models (LLMs) has facilitated the development of natural language text generation. It also poses unprecedented challenges, with content hallucination emerging as a significant concern. Existing solutions often involve expensive and complex interventions during the training process. Moreover, some approaches emphasize problem disassembly while neglecting the crucial validation process, leading to performance degradation or limited applications. To overcome these limitations, we propose a Markov Chain-based multi-agent debate verification framework to enhance hallucination detection accuracy in concise claims. Our method integrates the factchecking process, including claim detection, evidence retrieval, and multi-agent verification. In the verification stage, we deploy multiple agents through flexible Markov Chain-based debates to validate individual claims, ensuring meticulous verification outcomes. Experimental results across three generative tasks demonstrate that our approach achieves significant improvements over baselines.},
	language = {en},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Sun, Xiaoxi and Li, Jinpeng and Zhong, Yan and Zhao, Dongyan and Yan, Rui},
	month = jun,
	year = {2024},
	note = {arXiv:2406.03075 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 18 pages, 3 figures},
	file = {2406.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\V2UMCMQP\\2406.pdf:application/pdf},
}

@misc{WangPSB24a,
	title = {Soft {Self}-{Consistency} {Improves} {Language} {Model} {Agents}},
	url = {http://arxiv.org/abs/2402.13212},
	abstract = {Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current "sample and select" methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (SOFT-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. SOFT-SC improves both performance and efficiency on long-horizon interactive tasks, requiring half as many samples as SC for comparable or better performance. For a fixed number of samples, SOFT-SC leads to a 1.3\% increase over SC in absolute success rate on writing bash programs, a 6.6\% increase on online shopping (WebShop), and a 4.7\% increase for an interactive household game (ALFWorld). Finally, we show that SOFT-SC can be applied to both open-source and black-box models.},
	language = {en},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Wang, Han and Prasad, Archiki and Stengel-Eskin, Elias and Bansal, Mohit},
	month = jun,
	year = {2024},
	note = {arXiv:2402.13212 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: ACL 2024 Camera-Ready, the first three authors contributed equally; Code: https://github.com/HanNight/soft\_self\_consistency},
	file = {Wang et al. - 2024 - Soft Self-Consistency Improves Language Model Agen.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\KYHHWKTW\\Wang et al. - 2024 - Soft Self-Consistency Improves Language Model Agen.pdf:application/pdf},
}

@book{Endriss17,
	title = {Trends in {Computational} {Social} {Choice}},
	isbn = {978-1-326-91209-3},
	abstract = {Computational social choice is concerned with the design and analysis of methods for collective decision making. It is a research area that is located at the interface of Computer Science and Economics. This volume reports on a number of recent research trends in computational social choice. It has three parts. The first part presents novel scenarios in which methods for collective decision making are required. The second part introduces novel techniques for the analysis of such methods. The third part, finally, discusses a range of innovative applications enabled by recent research in computational social choice.},
	publisher = {Lulu.com},
	author = {Endriss, Ulle},
	month = sep,
	year = {2017},
}

@inproceedings{SunGTH19a,
	address = {Florence, Italy},
	title = {Mitigating {Gender} {Bias} in {Natural} {Language} {Processing}: {Literature} {Review}},
	shorttitle = {Mitigating {Gender} {Bias} in {Natural} {Language} {Processing}},
	url = {https://aclanthology.org/P19-1159},
	doi = {10.18653/v1/P19-1159},
	abstract = {As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.},
	urldate = {2024-08-19},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Sun, Tony and Gaut, Andrew and Tang, Shirlyn and Huang, Yuxin and ElSherief, Mai and Zhao, Jieyu and Mirza, Diba and Belding, Elizabeth and Chang, Kai-Wei and Wang, William Yang},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	pages = {1630--1640},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\F373N9RC\\Sun et al. - 2019 - Mitigating Gender Bias in Natural Language Process.pdf:application/pdf},
}

@article{JiLFY23b,
	title = {Survey of {Hallucination} in {Natural} {Language} {Generation}},
	volume = {55},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3571730},
	doi = {10.1145/3571730},
	abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
	number = {12},
	urldate = {2024-08-19},
	journal = {ACM Comput. Surv.},
	author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
	month = mar,
	year = {2023},
	pages = {248:1--248:38},
	file = {Eingereichte Version:C\:\\Users\\Jonas Becker\\Zotero\\storage\\4M55XML3\\Ji et al. - 2023 - Survey of Hallucination in Natural Language Genera.pdf:application/pdf},
}

@misc{SinghIGC24a,
	title = {Rethinking {Interpretability} in the {Era} of {Large} {Language} {Models}},
	url = {https://arxiv.org/abs/2402.01761v1},
	abstract = {Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs. In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We highlight two emerging research priorities for LLM interpretation: using LLMs to directly analyze new datasets and to generate interactive explanations.},
	language = {en},
	urldate = {2024-08-19},
	journal = {arXiv.org},
	author = {Singh, Chandan and Inala, Jeevana Priya and Galley, Michel and Caruana, Rich and Gao, Jianfeng},
	month = jan,
	year = {2024},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\NT39YFSW\\Singh et al. - 2024 - Rethinking Interpretability in the Era of Large La.pdf:application/pdf},
}

@misc{HuZZZ24,
	title = {Enabling {Intelligent} {Interactions} between an {Agent} and an {LLM}: {A} {Reinforcement} {Learning} {Approach}},
	shorttitle = {Enabling {Intelligent} {Interactions} between an {Agent} and an {LLM}},
	url = {http://arxiv.org/abs/2306.03604},
	abstract = {Large language models (LLMs) encode a vast amount of world knowledge acquired from massive text datasets. Recent studies have demonstrated that LLMs can assist an embodied agent in solving complex sequential decision making tasks by providing high-level instructions. However, interactions with LLMs can be time-consuming. In many practical scenarios, they require a significant amount of storage space that can only be deployed on remote cloud server nodes. Additionally, using commercial LLMs can be costly since they may charge based on usage frequency. In this paper, we explore how to enable intelligent cost-effective interactions between the agent and an LLM. We find that this problem can be naturally formulated by a Markov decision process (MDP), and propose When2Ask, a reinforcement learning based approach that learns when it is necessary to query LLMs for high-level instructions to accomplish a target task. Experiments on MiniGrid and Habitat environments that entail planning sub-goals demonstrate that When2Ask learns to solve target tasks with only a few necessary interactions with an LLM, and significantly reduces interaction costs in testing environments compared with baseline methods. Experiment results also suggest that by learning a mediator model to interact with the LLM, the agent’s performance becomes more robust against partial observability of the environment. Our code is available at https://github.com/ZJLAB-AMMI/LLM4RL.},
	language = {en},
	urldate = {2024-08-18},
	publisher = {arXiv},
	author = {Hu, Bin and Zhao, Chenyang and Zhang, Pu and Zhou, Zihao and Yang, Yuanhang and Xu, Zenglin and Liu, Bin},
	month = jun,
	year = {2024},
	note = {arXiv:2306.03604 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: 17 pages},
	file = {Hu et al. - 2024 - Enabling Intelligent Interactions between an Agent.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\V4DLUCTU\\Hu et al. - 2024 - Enabling Intelligent Interactions between an Agent.pdf:application/pdf},
}

@misc{WangWST24,
	title = {Rethinking the {Bounds} of {LLM} {Reasoning}: {Are} {Multi}-{Agent} {Discussions} the {Key}?},
	shorttitle = {Rethinking the {Bounds} of {LLM} {Reasoning}},
	url = {http://arxiv.org/abs/2402.18272},
	abstract = {Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs. In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms. Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs. We observe that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt. Further study reveals the common interaction mechanisms of LLMs during the discussion.},
	language = {en},
	urldate = {2024-08-16},
	publisher = {arXiv},
	author = {Wang, Qineng and Wang, Zihao and Su, Ying and Tong, Hanghang and Song, Yangqiu},
	month = feb,
	year = {2024},
	note = {arXiv:2402.18272 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: 22 pages, 5 figures, 10 tables},
	file = {Wang et al. - 2024 - Rethinking the Bounds of LLM Reasoning Are Multi-.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\QBD6SHFK\\Wang et al. - 2024 - Rethinking the Bounds of LLM Reasoning Are Multi-.pdf:application/pdf},
}

@inproceedings{NarayanCL18b,
	address = {Brussels, Belgium},
	title = {Don't {Give} {Me} the {Details}, {Just} the {Summary}! {Topic}-{Aware} {Convolutional} {Neural} {Networks} for {Extreme} {Summarization}},
	url = {https://aclanthology.org/D18-1206},
	doi = {10.18653/v1/D18-1206},
	abstract = {We introduce “extreme summarization”, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question “What is the article about?”. We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article's topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.},
	urldate = {2024-08-16},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Narayan, Shashi and Cohen, Shay B. and Lapata, Mirella},
	editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
	month = oct,
	year = {2018},
	pages = {1797--1807},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\6YBNWN4C\\Narayan et al. - 2018 - Don't Give Me the Details, Just the Summary! Topic.pdf:application/pdf},
}

@misc{AsaiWWS23,
	title = {Self-{RAG}: {Learning} to {Retrieve}, {Generate}, and {Critique} through {Self}-{Reflection}},
	shorttitle = {Self-{RAG}},
	url = {http://arxiv.org/abs/2310.11511},
	doi = {10.48550/arXiv.2310.11511},
	abstract = {Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.},
	urldate = {2024-08-17},
	publisher = {arXiv},
	author = {Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11511 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: 30 pages, 2 figures, 12 tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\WJUYIKGM\\Asai et al. - 2023 - Self-RAG Learning to Retrieve, Generate, and Crit.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\4VPXFKJI\\2310.html:text/html},
}

@misc{YuZSL24,
	title = {{xFinder}: {Robust} and {Pinpoint} {Answer} {Extraction} for {Large} {Language} {Models}},
	shorttitle = {{xFinder}},
	url = {http://arxiv.org/abs/2405.11874},
	abstract = {The continuous advancement of large language models (LLMs) has brought increasing attention to the critical issue of developing fair and reliable methods for evaluating their performance. Particularly, the emergence of subjective or non-subjective cheating phenomena, such as test set leakage and prompt format overfitting, poses significant challenges to the reliable evaluation of LLMs. Since evaluation frameworks often utilize Regular Expression (RegEx) for answer extraction, some models may adjust their responses to comply with specific formats that are easily extractable by RegEx. Nevertheless, the key answer extraction module based on RegEx frequently suffers from extraction errors. This paper conducts a comprehensive analysis of the entire LLM evaluation chain, demonstrating that optimizing the key answer extraction module can improve extraction accuracy, reduce LLMs’ reliance on specific answer formats, and enhance the reliability of LLM evaluation. To address these issues, we propose xFinder, a model specifically designed for key answer extraction. As part of this process, we create a specialized dataset, the Key Answer Finder (KAF) dataset, to ensure effective model training and evaluation. Through generalization testing and evaluation in real-world scenarios, the results demonstrate that the smallest xFinder model with only 500 million parameters achieves an average answer extraction accuracy of 93.42\%. In contrast, RegEx accuracy in the best evaluation framework is 74.38\%. xFinder exhibits stronger robustness and higher accuracy compared to existing evaluation frameworks. All resources for xFinder are available at https://github.com/IAAR-Shanghai/xFinder.},
	language = {en},
	urldate = {2024-08-17},
	publisher = {arXiv},
	author = {Yu, Qingchen and Zheng, Zifan and Song, Shichao and Li, Zhiyu and Xiong, Feiyu and Tang, Bo and Chen, Ding},
	month = may,
	year = {2024},
	note = {arXiv:2405.11874 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 37 Pages},
	file = {Yu et al. - 2024 - xFinder Robust and Pinpoint Answer Extraction for.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\RSBANRNY\\Yu et al. - 2024 - xFinder Robust and Pinpoint Answer Extraction for.pdf:application/pdf},
}

@inproceedings{KovatchevMS18b,
	address = {Miyazaki, Japan},
	title = {{ETPC} - {A} {Paraphrase} {Identification} {Corpus} {Annotated} with {Extended} {Paraphrase} {Typology} and {Negation}},
	url = {https://aclanthology.org/L18-1221},
	urldate = {2024-08-16},
	booktitle = {Proceedings of the {Eleventh} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC} 2018)},
	publisher = {European Language Resources Association (ELRA)},
	author = {Kovatchev, Venelin and Martí, M. Antònia and Salamó, Maria},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Hasida, Koiti and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios and Tokunaga, Takenobu},
	month = may,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\H3WPQR9Z\\Kovatchev et al. - 2018 - ETPC - A Paraphrase Identification Corpus Annotate.pdf:application/pdf},
}

@misc{WuBZW23,
	title = {{AutoGen}: {Enabling} {Next}-{Gen} {LLM} {Applications} via {Multi}-{Agent} {Conversation}},
	shorttitle = {{AutoGen}},
	url = {http://arxiv.org/abs/2308.08155},
	doi = {10.48550/arXiv.2308.08155},
	abstract = {AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and Awadallah, Ahmed Hassan and White, Ryen W. and Burger, Doug and Wang, Chi},
	month = oct,
	year = {2023},
	note = {arXiv:2308.08155 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: 43 pages (10 pages for the main text, 3 pages for references, and 30 pages for appendices)},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\V9YIYQUX\\Wu et al. - 2023 - AutoGen Enabling Next-Gen LLM Applications via Mu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\2RIUH555\\2308.html:text/html},
}

@misc{GaoLPK24,
	title = {{AgentScope}: {A} {Flexible} yet {Robust} {Multi}-{Agent} {Platform}},
	shorttitle = {{AgentScope}},
	url = {http://arxiv.org/abs/2402.14034},
	doi = {10.48550/arXiv.2402.14034},
	abstract = {With the rapid advancement of Large Language Models (LLMs), significant progress has been made in multi-agent applications. However, the complexities in coordinating agents' cooperation and LLMs' erratic performance pose notable challenges in developing robust and efficient multi-agent applications. To tackle these challenges, we propose AgentScope, a developer-centric multi-agent platform with message exchange as its core communication mechanism. The abundant syntactic tools, built-in agents and service functions, user-friendly interfaces for application demonstration and utility monitor, zero-code programming workstation, and automatic prompt tuning mechanism significantly lower the barriers to both development and deployment. Towards robust and flexible multi-agent application, AgentScope provides both built-in and customizable fault tolerance mechanisms. At the same time, it is also armed with system-level support for managing and utilizing multi-modal data, tools, and external knowledge. Additionally, we design an actor-based distribution framework, enabling easy conversion between local and distributed deployments and automatic parallel optimization without extra effort. With these features, AgentScope empowers developers to build applications that fully realize the potential of intelligent agents. We have released AgentScope at https://github.com/modelscope/agentscope, and hope AgentScope invites wider participation and innovation in this fast-moving field.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Gao, Dawei and Li, Zitao and Pan, Xuchen and Kuang, Weirui and Ma, Zhijian and Qian, Bingchen and Wei, Fei and Zhang, Wenhao and Xie, Yuexiang and Chen, Daoyuan and Yao, Liuyi and Peng, Hongyi and Zhang, Zeyu and Zhu, Lin and Cheng, Chen and Shi, Hongzhu and Li, Yaliang and Ding, Bolin and Zhou, Jingren},
	month = may,
	year = {2024},
	note = {arXiv:2402.14034 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	annote = {Comment: We have released code on https://github.com/modelscope/agentscope},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\2RCGEBCE\\Gao et al. - 2024 - AgentScope A Flexible yet Robust Multi-Agent Plat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\6ZP55IX7\\2402.html:text/html},
}

@misc{BeckerWGR24,
	title = {Text {Generation}: {A} {Systematic} {Literature} {Review} of {Tasks}, {Evaluation}, and {Challenges}},
	shorttitle = {Text {Generation}},
	url = {http://arxiv.org/abs/2405.15604},
	doi = {10.48550/arXiv.2405.15604},
	abstract = {Text generation has become more accessible than ever, and the increasing interest in these systems, especially those using large language models, has spurred an increasing number of related publications. We provide a systematic literature review comprising 244 selected papers between 2017 and 2024. This review categorizes works in text generation into five main tasks: open-ended text generation, summarization, translation, paraphrasing, and question answering. For each task, we review their relevant characteristics, sub-tasks, and specific challenges (e.g., missing datasets for multi-document summarization, coherence in story generation, and complex reasoning for question answering). Additionally, we assess current approaches for evaluating text generation systems and ascertain problems with current metrics. Our investigation shows nine prominent challenges common to all tasks and sub-tasks in recent text generation publications: bias, reasoning, hallucinations, misuse, privacy, interpretability, transparency, datasets, and computing. We provide a detailed analysis of these challenges, their potential solutions, and which gaps still require further engagement from the community. This systematic literature review targets two main audiences: early career researchers in natural language processing looking for an overview of the field and promising research directions, as well as experienced researchers seeking a detailed view of tasks, evaluation methodologies, open challenges, and recent mitigation strategies.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Becker, Jonas and Wahle, Jan Philip and Gipp, Bela and Ruas, Terry},
	month = aug,
	year = {2024},
	note = {arXiv:2405.15604 [cs]},
	keywords = {A.1, Computer Science - Computation and Language, I.2.7},
	annote = {Comment: 35 pages, 2 figures, 2 tables, Under review},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\Y4EHLDJ5\\Becker et al. - 2024 - Text Generation A Systematic Literature Review of.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\37C4AK7A\\2405.html:text/html},
}

@misc{HongLLL24,
	title = {Data {Interpreter}: {An} {LLM} {Agent} {For} {Data} {Science}},
	shorttitle = {Data {Interpreter}},
	url = {http://arxiv.org/abs/2402.18679},
	doi = {10.48550/arXiv.2402.18679},
	abstract = {Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated superior performance, exhibiting significant improvements in machine learning tasks, increasing from 0.86 to 0.95. Additionally, it showed a 26\% increase in the MATH dataset and a remarkable 112\% improvement in open-ended tasks. The solution will be released at https://github.com/geekan/MetaGPT.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Hong, Sirui and Lin, Yizhang and Liu, Bang and Liu, Bangbang and Wu, Binhao and Li, Danyang and Chen, Jiaqi and Zhang, Jiayi and Wang, Jinlin and Zhang, Li and Zhang, Lingyao and Yang, Min and Zhuge, Mingchen and Guo, Taicheng and Zhou, Tuo and Tao, Wei and Wang, Wenyi and Tang, Xiangru and Lu, Xiangtao and Zheng, Xiawu and Liang, Xinbing and Fei, Yaying and Cheng, Yuheng and Xu, Zongze and Wu, Chenglin},
	month = mar,
	year = {2024},
	note = {arXiv:2402.18679 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\G4DKGZM8\\Hong et al. - 2024 - Data Interpreter An LLM Agent For Data Science.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\C4MRESZZ\\2402.html:text/html},
}

@misc{SunYLW24,
	title = {Corex: {Pushing} the {Boundaries} of {Complex} {Reasoning} through {Multi}-{Model} {Collaboration}},
	shorttitle = {Corex},
	url = {http://arxiv.org/abs/2310.00280},
	abstract = {Large Language Models (LLMs) are evolving at an unprecedented pace and have exhibited considerable capability in the realm of natural language processing (NLP) with world knowledge. Benefiting from ultra-large-scale training corpora, a single LLM can manage typical NLP tasks competently. However, its performance in executing complex tasks is still confined by the limitations of its internal representation. To push this boundary further, we introduce Corex, a suite of novel general-purpose strategies that transform LLMs into autonomous agents, pioneering multi-agent collaborations for task-solving. Inspired by human behaviors, Corex is constituted by diverse collaboration paradigms including Discuss, Review, and Retrieve modes, which collectively work towards enhancing the reasoning process. These paradigms foster task-agnostic approaches that enable LLM-based agents to “think outside the box,” thereby overcoming common errors and providing better solutions. Through extensive experiments across four different types of reasoning tasks, we demonstrate that orchestrating multiple agents to work in concert yields better results compared to existing strong methods. Further analysis reveals the cost-effectiveness of Corex, while also exploring synergies between models of various scales and promoting annotation efficiency.},
	language = {en},
	urldate = {2024-07-24},
	publisher = {arXiv},
	author = {Sun, Qiushi and Yin, Zhangyue and Li, Xiang and Wu, Zhiyong and Qiu, Xipeng and Kong, Lingpeng},
	month = apr,
	year = {2024},
	note = {arXiv:2310.00280 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: work in progress, presented in ICLR 2024 Workshop on LLM Agents},
	file = {Sun et al. - 2024 - Corex Pushing the Boundaries of Complex Reasoning.pdf:C\:\\Users\\Jonas Becker\\Zotero\\storage\\UGVRL73K\\Sun et al. - 2024 - Corex Pushing the Boundaries of Complex Reasoning.pdf:application/pdf},
}

@inproceedings{WahleGR23b,
	address = {Singapore},
	title = {Paraphrase {Types} for {Generation} and {Detection}},
	url = {https://aclanthology.org/2023.emnlp-main.746},
	doi = {10.18653/v1/2023.emnlp-main.746},
	abstract = {Current approaches in paraphrase generation and detection heavily rely on a single general similarity score, ignoring the intricate linguistic properties of language. This paper introduces two new tasks to address this shortcoming by considering paraphrase types - specific linguistic perturbations at particular text positions. We name these tasks Paraphrase Type Generation and Paraphrase Type Detection. Our results suggest that while current techniques perform well in a binary classification scenario, i.e., paraphrased or not, the inclusion of fine-grained paraphrase types poses a significant challenge. While most approaches are good at generating and detecting general semantic similar content, they fail to understand the intrinsic linguistic variables they manipulate. Models trained in generating and identifying paraphrase types also show improvements in tasks without them. In addition, scaling these models further improves their ability to understand paraphrase types. We believe paraphrase types can unlock a new paradigm for developing paraphrase models and solving tasks in the future.},
	urldate = {2024-08-13},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wahle, Jan Philip and Gipp, Bela and Ruas, Terry},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {12148--12164},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\IUFN5DQ6\\Wahle et al. - 2023 - Paraphrase Types for Generation and Detection.pdf:application/pdf},
}

@misc{RajpurkarJL18a,
	title = {Know {What} {You} {Don}'t {Know}: {Unanswerable} {Questions} for {SQuAD}},
	shorttitle = {Know {What} {You} {Don}'t {Know}},
	url = {http://arxiv.org/abs/1806.03822},
	doi = {10.48550/arXiv.1806.03822},
	abstract = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86\% F1 on SQuAD 1.1 achieves only 66\% F1 on SQuAD 2.0.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
	month = jun,
	year = {2018},
	note = {arXiv:1806.03822 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ACL 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\9WBDQEIV\\Rajpurkar et al. - 2018 - Know What You Don't Know Unanswerable Questions f.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\J28CAX23\\1806.html:text/html},
}

@misc{GitHub21,
	title = {Simple {Ethical} {Questions}},
	url = {https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/simple_ethical_questions},
	abstract = {Beyond the Imitation Game collaborative benchmark for measuring and extrapolating the capabilities of language models - google/BIG-bench},
	language = {en},
	urldate = {2024-08-13},
	journal = {GitHub},
	author = {GitHub},
	year = {2021},
	file = {Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\AGDLA459\\simple_ethical_questions.html:text/html},
}

@article{GevaKSK21,
	title = {Did {Aristotle} {Use} a {Laptop}? {A} {Question} {Answering} {Benchmark} with {Implicit} {Reasoning} {Strategies}},
	volume = {9},
	shorttitle = {Did {Aristotle} {Use} a {Laptop}?},
	url = {https://aclanthology.org/2021.tacl-1.21},
	doi = {10.1162/tacl_a_00370},
	abstract = {A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce StrategyQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87\%) on this task, while our best baseline reaches an accuracy of ∼ 66\%.},
	urldate = {2024-08-13},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Geva, Mor and Khashabi, Daniel and Segal, Elad and Khot, Tushar and Roth, Dan and Berant, Jonathan},
	editor = {Roark, Brian and Nenkova, Ani},
	year = {2021},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {346--361},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\R3UPVVHD\\Geva et al. - 2021 - Did Aristotle Use a Laptop A Question Answering B.pdf:application/pdf},
}

@misc{WikimediaFoundation19,
	title = {{ACL} 2019 {Fourth} {Conference} on {Machine} {Translation} ({WMT19}), {Shared} {Task}: {Machine} {Translation} of {News}},
	url = {https://www.statmt.org/wmt19/translation-task.html},
	urldate = {2024-08-13},
	author = {Wikimedia Foundation},
	year = {2019},
}

@inproceedings{PapineniRWZ02a,
	address = {Philadelphia, Pennsylvania, USA},
	title = {Bleu: a {Method} for {Automatic} {Evaluation} of {Machine} {Translation}},
	shorttitle = {Bleu},
	url = {https://aclanthology.org/P02-1040},
	doi = {10.3115/1073083.1073135},
	urldate = {2024-08-14},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	editor = {Isabelle, Pierre and Charniak, Eugene and Lin, Dekang},
	month = jul,
	year = {2002},
	pages = {311--318},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\TQCA9VHB\\Papineni et al. - 2002 - Bleu a Method for Automatic Evaluation of Machine.pdf:application/pdf},
}

@inproceedings{Lin04b,
	address = {Barcelona, Spain},
	title = {{ROUGE}: {A} {Package} for {Automatic} {Evaluation} of {Summaries}},
	shorttitle = {{ROUGE}},
	url = {https://aclanthology.org/W04-1013},
	urldate = {2024-08-14},
	booktitle = {Text {Summarization} {Branches} {Out}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Chin-Yew},
	month = jul,
	year = {2004},
	pages = {74--81},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\QV584LDZ\\Lin - 2004 - ROUGE A Package for Automatic Evaluation of Summa.pdf:application/pdf},
}

@inproceedings{BanerjeeL05a,
	address = {Ann Arbor, Michigan},
	title = {{METEOR}: {An} {Automatic} {Metric} for {MT} {Evaluation} with {Improved} {Correlation} with {Human} {Judgments}},
	shorttitle = {{METEOR}},
	url = {https://aclanthology.org/W05-0909},
	urldate = {2024-08-14},
	booktitle = {Proceedings of the {ACL} {Workshop} on {Intrinsic} and {Extrinsic} {Evaluation} {Measures} for {Machine} {Translation} and/or {Summarization}},
	publisher = {Association for Computational Linguistics},
	author = {Banerjee, Satanjeev and Lavie, Alon},
	editor = {Goldstein, Jade and Lavie, Alon and Lin, Chin-Yew and Voss, Clare},
	month = jun,
	year = {2005},
	pages = {65--72},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\ELNVL7VU\\Banerjee und Lavie - 2005 - METEOR An Automatic Metric for MT Evaluation with.pdf:application/pdf},
}

@misc{ZhangKWW19,
	title = {{BERTScore}: {Evaluating} {Text} {Generation} with {BERT}},
	shorttitle = {{BERTScore}},
	url = {https://arxiv.org/abs/1904.09675v3},
	abstract = {We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.},
	language = {en},
	urldate = {2024-08-14},
	journal = {arXiv.org},
	author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
	month = apr,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\E9NE7KQL\\Zhang et al. - 2019 - BERTScore Evaluating Text Generation with BERT.pdf:application/pdf},
}

@misc{SakaguchiBBC19,
	title = {{WinoGrande}: {An} {Adversarial} {Winograd} {Schema} {Challenge} at {Scale}},
	shorttitle = {{WinoGrande}},
	url = {http://arxiv.org/abs/1907.10641},
	doi = {10.48550/arXiv.1907.10641},
	abstract = {The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations. However, recent advances in neural language models have already reached around 90\% accuracy on variants of WSC. This raises an important question whether these models have truly acquired robust commonsense capabilities or whether they rely on spurious biases in the datasets that lead to an overestimation of the true capabilities of machine commonsense. To investigate this question, we introduce WinoGrande, a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel AfLite algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. The best state-of-the-art methods on WinoGrande achieve 59.4-79.1\%, which are 15-35\% below human performance of 94.0\%, depending on the amount of the training data allowed. Furthermore, we establish new state-of-the-art results on five related benchmarks - WSC (90.1\%), DPR (93.1\%), COPA (90.6\%), KnowRef (85.6\%), and Winogender (97.1\%). These results have dual implications: on one hand, they demonstrate the effectiveness of WinoGrande when used as a resource for transfer learning. On the other hand, they raise a concern that we are likely to be overestimating the true capabilities of machine commonsense across all these benchmarks. We emphasize the importance of algorithmic bias reduction in existing and future benchmarks to mitigate such overestimation.},
	urldate = {2024-12-27},
	publisher = {arXiv},
	author = {Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
	month = nov,
	year = {2019},
	note = {arXiv:1907.10641 [cs]
version: 2},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\KHEKU6FY\\Sakaguchi et al. - 2019 - WinoGrande An Adversarial Winograd Schema Challenge at Scale.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\LECZ8M3Q\\1907.html:text/html},
}

@misc{LingYDB17a,
	title = {Program {Induction} by {Rationale} {Generation} : {Learning} to {Solve} and {Explain} {Algebraic} {Word} {Problems}},
	shorttitle = {Program {Induction} by {Rationale} {Generation}},
	url = {http://arxiv.org/abs/1705.04146},
	doi = {10.48550/arXiv.1705.04146},
	abstract = {Solving algebraic word problems requires executing a series of arithmetic operations---a program---to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.},
	urldate = {2024-12-27},
	publisher = {arXiv},
	author = {Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},
	month = oct,
	year = {2017},
	note = {arXiv:1705.04146 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\YP4HVS6T\\Ling et al. - 2017 - Program Induction by Rationale Generation  Learning to Solve and Explain Algebraic Word Problems.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\JXYK4Y76\\1705.html:text/html},
}

@misc{HendrycksBBC23,
	title = {Aligning {AI} {With} {Shared} {Human} {Values}},
	url = {http://arxiv.org/abs/2008.02275},
	doi = {10.48550/arXiv.2008.02275},
	abstract = {We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.},
	urldate = {2024-12-27},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew and Li, Jerry and Song, Dawn and Steinhardt, Jacob},
	month = feb,
	year = {2023},
	note = {arXiv:2008.02275 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computers and Society},
	annote = {Comment: ICLR 2021; the ETHICS dataset is available at https://github.com/hendrycks/ethics/},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\HVAVWPSK\\Hendrycks et al. - 2023 - Aligning AI With Shared Human Values.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\Y22MGNKV\\2008.html:text/html},
}

@misc{ReinHSP23a,
	title = {{GPQA}: {A} {Graduate}-{Level} {Google}-{Proof} {Q}\&{A} {Benchmark}},
	shorttitle = {{GPQA}},
	url = {http://arxiv.org/abs/2311.12022},
	doi = {10.48550/arXiv.2311.12022},
	abstract = {We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65\% accuracy (74\% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34\% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are "Google-proof"). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39\% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.},
	urldate = {2024-12-27},
	publisher = {arXiv},
	author = {Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R.},
	month = nov,
	year = {2023},
	note = {arXiv:2311.12022 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: 28 pages, 5 figures, 7 tables},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\EYV3DBJX\\Rein et al. - 2023 - GPQA A Graduate-Level Google-Proof Q&A Benchmark.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\S2QRBXVJ\\2311.html:text/html},
}

@misc{ZhouLMB23,
	title = {Instruction-{Following} {Evaluation} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2311.07911},
	doi = {10.48550/arXiv.2311.07911},
	abstract = {One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of "verifiable instructions" such as "write in more than 400 words" and "mention the keyword of AI at least 3 times". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction\_following\_eval},
	urldate = {2024-12-27},
	publisher = {arXiv},
	author = {Zhou, Jeffrey and Lu, Tianjian and Mishra, Swaroop and Brahma, Siddhartha and Basu, Sujoy and Luan, Yi and Zhou, Denny and Hou, Le},
	month = nov,
	year = {2023},
	note = {arXiv:2311.07911 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\Z3KJ4FRZ\\Zhou et al. - 2023 - Instruction-Following Evaluation for Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\FX3K4U3A\\2311.html:text/html},
}

@misc{GrattafioriDJP24a,
	title = {The {Llama} 3 {Herd} of {Models}},
	url = {http://arxiv.org/abs/2407.21783},
	doi = {10.48550/arXiv.2407.21783},
	abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
	month = nov,
	year = {2024},
	note = {arXiv:2407.21783 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\4C7PU2Z5\\Grattafiori et al. - 2024 - The Llama 3 Herd of Models.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\DUL5F2DF\\2407.html:text/html},
}

@misc{Becker24,
	title = {Multi-{Agent} {Large} {Language} {Models} for {Conversational} {Task}-{Solving}},
	url = {http://arxiv.org/abs/2410.22932},
	doi = {10.48550/arXiv.2410.22932},
	abstract = {In an era where single large language models have dominated the landscape of artificial intelligence for years, multi-agent systems arise as new protagonists in conversational task-solving. While previous studies have showcased their potential in reasoning tasks and creative endeavors, an analysis of their limitations concerning the conversational paradigms and the impact of individual agents is missing. It remains unascertained how multi-agent discussions perform across tasks of varying complexity and how the structure of these conversations influences the process. To fill that gap, this work systematically evaluates multi-agent systems across various discussion paradigms, assessing their strengths and weaknesses in both generative tasks and question-answering tasks. Alongside the experiments, I propose a taxonomy of 20 multi-agent research studies from 2022 to 2024, followed by the introduction of a framework for deploying multi-agent LLMs in conversational task-solving. I demonstrate that while multi-agent systems excel in complex reasoning tasks, outperforming a single model by leveraging expert personas, they fail on basic tasks. Concretely, I identify three challenges that arise: 1) While longer discussions enhance reasoning, agents fail to maintain conformity to strict task requirements, which leads to problem drift, making shorter conversations more effective for basic tasks. 2) Prolonged discussions risk alignment collapse, raising new safety concerns for these systems. 3) I showcase discussion monopolization through long generations, posing the problem of fairness in decision-making for tasks like summarization. This work uncovers both the potential and challenges that arise with multi-agent interaction and varying conversational paradigms, providing insights into how future research could improve the efficiency, performance, and safety of multi-agent LLMs.},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {Becker, Jonas},
	month = nov,
	year = {2024},
	note = {arXiv:2410.22932 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\ZTDSAR8N\\Becker - 2024 - Multi-Agent Large Language Models for Conversational Task-Solving.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\4YMWHPVU\\2410.html:text/html},
}

@misc{BakerKMW20a,
	title = {Emergent {Tool} {Use} {From} {Multi}-{Agent} {Autocurricula}},
	url = {http://arxiv.org/abs/1909.07528},
	doi = {10.48550/arXiv.1909.07528},
	abstract = {Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {Baker, Bowen and Kanitscheider, Ingmar and Markov, Todor and Wu, Yi and Powell, Glenn and McGrew, Bob and Mordatch, Igor},
	month = feb,
	year = {2020},
	note = {arXiv:1909.07528 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\JKEW8ZDT\\Baker et al. - 2020 - Emergent Tool Use From Multi-Agent Autocurricula.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\HDULHCDU\\1909.html:text/html},
}

@misc{ZhengCSZ23,
	title = {Judging {LLM}-as-a-{Judge} with {MT}-{Bench} and {Chatbot} {Arena}},
	url = {http://arxiv.org/abs/2306.05685},
	doi = {10.48550/arXiv.2306.05685},
	abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm\_judge.},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
	month = dec,
	year = {2023},
	note = {arXiv:2306.05685 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: NeurIPS 2023 Datasets and Benchmarks Track},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\9NV5AB8N\\Zheng et al. - 2023 - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\J587QJ37\\2306.html:text/html},
}

@misc{StephanZAS24,
	title = {From {Calculation} to {Adjudication}: {Examining} {LLM} judges on {Mathematical} {Reasoning} {Tasks}},
	shorttitle = {From {Calculation} to {Adjudication}},
	url = {http://arxiv.org/abs/2409.04168},
	doi = {10.48550/arXiv.2409.04168},
	abstract = {To reduce the need for human annotations, large language models (LLMs) have been proposed as judges of the quality of other candidate models. LLM judges are typically evaluated by measuring the correlation with human judgments on generation tasks such as summarization or machine translation. In contrast, we study LLM judges on mathematical reasoning tasks. These tasks require multi-step reasoning, and the correctness of their solutions is verifiable, enabling a more objective evaluation. We perform a detailed performance analysis and find that the used judges are mostly unable to improve task performance but are able to pick the better model. Our analysis uncovers a strong correlation between judgment performance and the candidate model task performance. We observe that judges tend to choose the model of higher quality even if its answer is incorrect. Further, we show that it is possible to use statistics, such as the task performances of the individual models, to predict judgment performance. In an ablation, we either swap or mask the candidate answers and observe that judges often keep the original judgment, providing evidence that judges incorporate writing style in their judgments. In summary, we find that regularities in the judgments are quantifiable using statistical measures and provide various angles on exploiting them.},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {Stephan, Andreas and Zhu, Dawei and Aßenmacher, Matthias and Shen, Xiaoyu and Roth, Benjamin},
	month = sep,
	year = {2024},
	note = {arXiv:2409.04168 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\QIC9NSZE\\Stephan et al. - 2024 - From Calculation to Adjudication Examining LLM judges on Mathematical Reasoning Tasks.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\F6QYHTE6\\2409.html:text/html},
}

@misc{ChenXLH24,
	title = {Do {NOT} {Think} {That} {Much} for 2+3=? {On} the {Overthinking} of o1-{Like} {LLMs}},
	shorttitle = {Do {NOT} {Think} {That} {Much} for 2+3=?},
	url = {http://arxiv.org/abs/2412.21187},
	doi = {10.48550/arXiv.2412.21187},
	abstract = {The remarkable performance of models like the OpenAI o1 can be attributed to their ability to emulate human-like long-time thinking during inference. These models employ extended chain-of-thought (CoT) processes, exploring multiple strategies to enhance problem-solving capabilities. However, a critical question remains: How to intelligently and efficiently scale computational resources during testing. This paper presents the first comprehensive study on the prevalent issue of overthinking in these models, where excessive computational resources are allocated for simple problems with minimal benefit. We introduce novel efficiency metrics from both outcome and process perspectives to evaluate the rational use of computational resources by o1-like models. Using a self-training paradigm, we propose strategies to mitigate overthinking, streamlining reasoning processes without compromising accuracy. Experimental results show that our approach successfully reduces computational overhead while preserving model performance across a range of testsets with varying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME.},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Chen, Xingyu and Xu, Jiahao and Liang, Tian and He, Zhiwei and Pang, Jianhui and Yu, Dian and Song, Linfeng and Liu, Qiuzhi and Zhou, Mengfei and Zhang, Zhuosheng and Wang, Rui and Tu, Zhaopeng and Mi, Haitao and Yu, Dong},
	month = dec,
	year = {2024},
	note = {arXiv:2412.21187 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Work in progress},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\RX5S987N\\Chen et al. - 2024 - Do NOT Think That Much for 2+3= On the Overthinking of o1-Like LLMs.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\D92JJKA4\\2412.html:text/html},
}

@misc{ShahW24,
	title = {Agents {Are} {Not} {Enough}},
	url = {http://arxiv.org/abs/2412.16241},
	doi = {10.48550/arXiv.2412.16241},
	abstract = {In the midst of the growing integration of Artificial Intelligence (AI) into various aspects of our lives, agents are experiencing a resurgence. These autonomous programs that act on behalf of humans are neither new nor exclusive to the mainstream AI movement. By exploring past incarnations of agents, we can understand what has been done previously, what worked, and more importantly, what did not pan out and why. This understanding lets us to examine what distinguishes the current focus on agents. While generative AI is appealing, this technology alone is insufficient to make new generations of agents more successful. To make the current wave of agents effective and sustainable, we envision an ecosystem that includes not only agents but also Sims, which represent user preferences and behaviors, as well as Assistants, which directly interact with the user and coordinate the execution of user tasks with the help of the agents.},
	urldate = {2025-01-15},
	publisher = {arXiv},
	author = {Shah, Chirag and White, Ryen W.},
	month = dec,
	year = {2024},
	note = {arXiv:2412.16241 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Multiagent Systems},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\TU7ZZHRU\\Shah und White - 2024 - Agents Are Not Enough.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\FR2V69UM\\2412.html:text/html},
}

@article{WangMFZ24a,
	title = {A survey on large language model based autonomous agents},
	volume = {18},
	issn = {2095-2236},
	url = {https://doi.org/10.1007/s11704-024-40231-1},
	doi = {10.1007/s11704-024-40231-1},
	abstract = {Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of Web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLM-based autonomous agents from a holistic perspective. We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present a overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field.},
	language = {en},
	number = {6},
	urldate = {2025-01-17},
	journal = {Frontiers of Computer Science},
	author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Jirong},
	month = mar,
	year = {2024},
	keywords = {Artificial Intelligence, autonomous agent, human-level intelligence, large language model},
	pages = {186345},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\5728KIGA\\Wang et al. - 2024 - A survey on large language model based autonomous agents.pdf:application/pdf},
}

@misc{GuJST25,
	title = {A {Survey} on {LLM}-as-a-{Judge}},
	url = {http://arxiv.org/abs/2411.15594},
	doi = {10.48550/arXiv.2411.15594},
	abstract = {Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of "LLM-as-a-Judge," where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization. This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field.},
	urldate = {2025-01-20},
	publisher = {arXiv},
	author = {Gu, Jiawei and Jiang, Xuhui and Shi, Zhichao and Tan, Hexiang and Zhai, Xuehao and Xu, Chengjin and Li, Wei and Shen, Yinghan and Ma, Shengjie and Liu, Honghao and Wang, Yuanzhuo and Guo, Jian},
	month = jan,
	year = {2025},
	note = {arXiv:2411.15594 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: Corrected typos \& more discussion on reasoning models 33 pages, 9 figures. arXiv admin note: text overlap with arXiv:2310.05470 by other authors},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\SUV75ZYQ\\Gu et al. - 2025 - A Survey on LLM-as-a-Judge.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\5WBFMS7S\\2411.html:text/html},
}

@misc{ZhongLPZ24,
	title = {Evaluation of {OpenAI} o1: {Opportunities} and {Challenges} of {AGI}},
	shorttitle = {Evaluation of {OpenAI} o1},
	url = {http://arxiv.org/abs/2409.18486},
	doi = {10.48550/arXiv.2409.18486},
	abstract = {This comprehensive study evaluates the performance of OpenAI's o1-preview large language model across a diverse array of complex reasoning tasks, spanning multiple domains, including computer science, mathematics, natural sciences, medicine, linguistics, and social sciences. Through rigorous testing, o1-preview demonstrated remarkable capabilities, often achieving human-level or superior performance in areas ranging from coding challenges to scientific reasoning and from language processing to creative problem-solving. Key findings include: -83.3\% success rate in solving complex competitive programming problems, surpassing many human experts. -Superior ability in generating coherent and accurate radiology reports, outperforming other evaluated models. -100\% accuracy in high school-level mathematical reasoning tasks, providing detailed step-by-step solutions. -Advanced natural language inference capabilities across general and specialized domains like medicine. -Impressive performance in chip design tasks, outperforming specialized models in areas such as EDA script generation and bug analysis. -Remarkable proficiency in anthropology and geology, demonstrating deep understanding and reasoning in these specialized fields. -Strong capabilities in quantitative investing. O1 has comprehensive financial knowledge and statistical modeling skills. -Effective performance in social media analysis, including sentiment analysis and emotion recognition. The model excelled particularly in tasks requiring intricate reasoning and knowledge integration across various fields. While some limitations were observed, including occasional errors on simpler problems and challenges with certain highly specialized concepts, the overall results indicate significant progress towards artificial general intelligence.},
	urldate = {2025-01-20},
	publisher = {arXiv},
	author = {Zhong, Tianyang and Liu, Zhengliang and Pan, Yi and Zhang, Yutong and Zhou, Yifan and Liang, Shizhe and Wu, Zihao and Lyu, Yanjun and Shu, Peng and Yu, Xiaowei and Cao, Chao and others},
	month = sep,
	year = {2024},
	note = {arXiv:2409.18486 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\WD3I6BQX\\Zhong et al. - 2024 - Evaluation of OpenAI o1 Opportunities and Challenges of AGI.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\8D5PYFCH\\2409.html:text/html},
}

@misc{ValmeekamMK23a,
	title = {Can {Large} {Language} {Models} {Really} {Improve} by {Self}-critiquing {Their} {Own} {Plans}?},
	url = {http://arxiv.org/abs/2310.08118},
	doi = {10.48550/arXiv.2310.08118},
	abstract = {There have been widespread claims about Large Language Models (LLMs) being able to successfully verify or self-critique their candidate solutions in reasoning problems in an iterative mode. Intrigued by those claims, in this paper we set out to investigate the verification/self-critiquing abilities of large language models in the context of planning. We evaluate a planning system that employs LLMs for both plan generation and verification. We assess the verifier LLM's performance against ground-truth verification, the impact of self-critiquing on plan generation, and the influence of varying feedback levels on system performance. Using GPT-4, a state-of-the-art LLM, for both generation and verification, our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability. Additionally, the nature of feedback, whether binary or detailed, showed minimal impact on plan generation. Collectively, our results cast doubt on the effectiveness of LLMs in a self-critiquing, iterative framework for planning tasks.},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Valmeekam, Karthik and Marquez, Matthew and Kambhampati, Subbarao},
	month = oct,
	year = {2023},
	note = {arXiv:2310.08118 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\XWNZ4DAR\\Valmeekam et al. - 2023 - Can Large Language Models Really Improve by Self-critiquing Their Own Plans.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\AAYLB5LT\\2310.html:text/html},
}

@misc{StechlyMK23a,
	title = {{GPT}-4 {Doesn}'t {Know} {It}'s {Wrong}: {An} {Analysis} of {Iterative} {Prompting} for {Reasoning} {Problems}},
	shorttitle = {{GPT}-4 {Doesn}'t {Know} {It}'s {Wrong}},
	url = {http://arxiv.org/abs/2310.12397},
	doi = {10.48550/arXiv.2310.12397},
	abstract = {There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples, a wide spread belief in their iterative self-critique capabilities persists. In this paper, we set out to systematically investigate the effectiveness of iterative prompting of LLMs in the context of Graph Coloring, a canonical NP-complete reasoning problem that is related to propositional satisfiability as well as practical problems like scheduling and allocation. We present a principled empirical study of the performance of GPT4 in solving graph coloring instances or verifying the correctness of candidate colorings. In iterative modes, we experiment with the model critiquing its own answers and an external correct reasoner verifying proposed solutions. In both cases, we analyze whether the content of the criticisms actually affects bottom line performance. The study seems to indicate that (i) LLMs are bad at solving graph coloring instances (ii) they are no better at verifying a solution--and thus are not effective in iterative modes with LLMs critiquing LLM-generated solutions (iii) the correctness and content of the criticisms--whether by LLMs or external solvers--seems largely irrelevant to the performance of iterative prompting. We show that the observed increase in effectiveness is largely due to the correct solution being fortuitously present in the top-k completions of the prompt (and being recognized as such by an external verifier). Our results thus call into question claims about the self-critiquing capabilities of state of the art LLMs.},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Stechly, Kaya and Marquez, Matthew and Kambhampati, Subbarao},
	month = oct,
	year = {2023},
	note = {arXiv:2310.12397 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: 18 pages, 3 figures},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\IC9TXQZ3\\Stechly et al. - 2023 - GPT-4 Doesn't Know It's Wrong An Analysis of Iterative Prompting for Reasoning Problems.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\GW4LUQJH\\2310.html:text/html},
}

@misc{KimMCR24,
	title = {{MEGAnno}+: {A} {Human}-{LLM} {Collaborative} {Annotation} {System}},
	shorttitle = {{MEGAnno}+},
	url = {http://arxiv.org/abs/2402.18050},
	doi = {10.48550/arXiv.2402.18050},
	abstract = {Large language models (LLMs) can label data faster and cheaper than humans for various NLP tasks. Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations. Therefore, we advocate a collaborative approach where humans and LLMs work together to produce reliable and high-quality labels. We present MEGAnno+, a human-LLM collaborative annotation system that offers effective LLM agent and annotation management, convenient and robust LLM annotation, and exploratory verification of LLM labels by humans.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Kim, Hannah and Mitra, Kushan and Chen, Rafael Li and Rahman, Sajjadur and Zhang, Dan},
	month = feb,
	year = {2024},
	note = {arXiv:2402.18050 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	annote = {Comment: EACL 2024 Demo},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\GSSRGTA9\\Kim et al. - 2024 - MEGAnno+ A Human-LLM Collaborative Annotation System.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\9YPD7ZFX\\2402.html:text/html},
}

@inproceedings{WangKRM24,
	address = {Honolulu HI USA},
	title = {Human-{LLM} {Collaborative} {Annotation} {Through} {Effective} {Verification} of {LLM} {Labels}},
	isbn = {979-8-4007-0330-0},
	url = {https://dl.acm.org/doi/10.1145/3613904.3641960},
	doi = {10.1145/3613904.3641960},
	language = {en},
	urldate = {2025-01-22},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Wang, Xinru and Kim, Hannah and Rahman, Sajjadur and Mitra, Kushan and Miao, Zhengjie},
	month = may,
	year = {2024},
	pages = {1--21},
}

@misc{PagnoniBT21a,
	title = {Understanding {Factuality} in {Abstractive} {Summarization} with {FRANK}: {A} {Benchmark} for {Factuality} {Metrics}},
	shorttitle = {Understanding {Factuality} in {Abstractive} {Summarization} with {FRANK}},
	url = {http://arxiv.org/abs/2104.13346},
	doi = {10.48550/arXiv.2104.13346},
	abstract = {Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights into the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets. Through these annotations, we identify the proportion of different categories of factual errors in various summarization models and benchmark factuality metrics, showing their correlation with human judgment as well as their specific strengths and weaknesses.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Pagnoni, Artidoro and Balachandran, Vidhisha and Tsvetkov, Yulia},
	month = jul,
	year = {2021},
	note = {arXiv:2104.13346 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted at NAACL 2021. Second version fixes bug with BERTScore results},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\59V9GTRR\\Pagnoni et al. - 2021 - Understanding Factuality in Abstractive Summarization with FRANK A Benchmark for Factuality Metrics.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\SKL6CF44\\2104.html:text/html},
}

@article{ZhangZ20b,
	title = {Neural machine translation: {Challenges}, progress and future},
	volume = {63},
	issn = {1869-1900},
	shorttitle = {Neural machine translation},
	url = {https://doi.org/10.1007/s11431-020-1632-x},
	doi = {10.1007/s11431-020-1632-x},
	abstract = {Machine translation (MT) is a technique that leverages computers to translate human languages automatically. Nowadays, neural machine translation (NMT) which models direct mapping between source and target languages with deep neural networks has achieved a big breakthrough in translation performance and become the de facto paradigm of MT. This article makes a review of NMT framework, discusses the challenges in NMT, introduces some exciting recent progresses and finally looks forward to some potential future research trends.},
	language = {en},
	number = {10},
	urldate = {2025-01-22},
	journal = {Science China Technological Sciences},
	author = {Zhang, JiaJun and Zong, ChengQing},
	month = oct,
	year = {2020},
	keywords = {Transformer, document translation, low-resource translation, multimodal translation, neural machine translation},
	pages = {2028--2050},
	file = {Full Text PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\6CRSKZKM\\Zhang und Zong - 2020 - Neural machine translation Challenges, progress and future.pdf:application/pdf},
}

@misc{SnellLXK24a,
	title = {Scaling {LLM} {Test}-{Time} {Compute} {Optimally} can be {More} {Effective} than {Scaling} {Model} {Parameters}},
	url = {http://arxiv.org/abs/2408.03314},
	doi = {10.48550/arXiv.2408.03314},
	abstract = {Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a "compute-optimal" scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
	month = aug,
	year = {2024},
	note = {arXiv:2408.03314 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Jonas Becker\\Zotero\\storage\\3Z2ZLTVG\\Snell et al. - 2024 - Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters.pdf:application/pdf;Snapshot:C\:\\Users\\Jonas Becker\\Zotero\\storage\\W2DSI3FD\\2408.html:text/html},
}

@misc{skyworkcritic2024,
  title={Skywork Critic Model Series},
  author={Shiwen, Tu and Liang, Zhao and Liu, Chris Yuhao and Zeng, Liang and Liu, Yang},
  year={2024},
  month={September},
  howpublished={\url{https://huggingface.co/Skywork}},
  url={https://huggingface.co/Skywork},
}

@inproceedings{kirstein-etal-2025-whats,
    title = "What`s Wrong? Refining Meeting Summaries with {LLM} Feedback",
    author = "Kirstein, Frederic Thomas  and
      Lima Ruas, Terry  and
      Gipp, Bela",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.143/",
    pages = "2100--2120",
    abstract = "Meeting summarization has become a critical task since digital encounters have become a common practice. Large language models (LLMs) show great potential in summarization, offering enhanced coherence and context understanding compared to traditional methods. However, they still struggle to maintain relevance and avoid hallucination. We introduce a multi-LLM correction approach for meeting summarization using a two-phase process that mimics the human review process: mistake identification and summary refinement. We release QMSum Mistake, a dataset of 200 automatically generated meeting summaries annotated by humans on nine error types, including structural, omission, and irrelevance errors. Our experiments show that these errors can be identified with high accuracy by an LLM. We transform identified mistakes into actionable feedback to improve the quality of a given summary measured by relevance, informativeness, conciseness, and coherence. This post-hoc refinement effectively improves summary quality by leveraging multiple LLMs to validate output quality. Our multi-LLM approach for meeting summarization shows potential for similar complex text generation tasks requiring robustness, action planning, and discussion towards a goal."
}

@misc{wang2024adaptingllmagentsuniversal,
      title={Adapting LLM Agents with Universal Feedback in Communication}, 
      author={Kuan Wang and Yadong Lu and Michael Santacroce and Yeyun Gong and Chao Zhang and Yelong Shen},
      year={2024},
      eprint={2310.01444},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.01444}, 
}

@misc{liu2024dynamicllmpoweredagentnetwork,
      title={A Dynamic LLM-Powered Agent Network for Task-Oriented Agent Collaboration}, 
      author={Zijun Liu and Yanzhe Zhang and Peng Li and Yang Liu and Diyi Yang},
      year={2024},
      eprint={2310.02170},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.02170}, 
}

@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and others},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@misc{sun2020finetuneberttextclassification,
      title={How to Fine-Tune BERT for Text Classification?}, 
      author={Chi Sun and Xipeng Qiu and Yige Xu and Xuanjing Huang},
      year={2020},
      eprint={1905.05583},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1905.05583}, 
}