\section{Methodology}

We explain the multi-agent environment of this study, our proposed method \judge, which detects problem drift at test-time, \policy, a mitigation method for problem drift, and the used datasets and metrics.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/other_charts/discussion_large.drawio.pdf}
    \caption{Example of problem drift in multi-agent debate. The \textit{English instructor} induces a logical error in the discussion. The other agents agree without skepticism, leading to the wrong solution and problem drift.}
    \label{fig:discussion_turn}
\end{figure*}

\subsection{Discussion Setup} \label{sec:setup}
To run multi-agent discussions, we define three components: an interface for agents, discussion paradigms, and the decision-making protocol.
For each task, we create three agents with \textbf{expert personas} \citep{XuYLW23a, ShiZWW23a}. 
We choose three as a hyperparameter following previous works \citep{ChenSB24a, YinSCG23a}.
These personas are automatically generated by \href{https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct}{\textit{Meta-llama/Meta-Llama-3.1-70B-Instruct}} \citep{GrattafioriDJP24a} and induce a set of preferences (e.g., law attorney), which varies according to the task and sample (examples and details are in \Cref{app:examples}).
Second, we run a total of \textbf{seven turns} of conversation between the agents.
Each agent can generate one new message at each turn and indicate their agreement with the current solution.
All agents see the messages of one another.
If an agent disagrees, it proposes a new solution concomitantly.
Finally, we initiate \textbf{majority voting} between possible discussed solutions to reach a final solution for that turn \citep{YangDKH24}.
Following prior work, agents have access to one previous turn and the previously voted solution \citep{PagnoniBT21a, ZhangZ20b}.
A visual overview of the process can be seen in \Cref{fig:discussion_turn}.
We choose voting \citep{YangDKH24} over consensus \citep{ChenSB24a} for decision-making because we require a definitive solution at the end of each turn that is not biased towards the order of agent contributions.
Voting allows for a direct performance measurement after each turn.
For all experiments, we use \href{https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct}{\textit{Meta-llama/Meta-Llama-3.1-70B-Instruct}} on 8 NVIDIA A100 GPU with 40 GB.
Detailed information about the framework is available in \Cref{app:framework}, parameters in \Cref{app:parameters}, and prompts in \Cref{app:prompts}.



\subsection{Mitigation Setup}
A natural question from our newly proposed definition of problem drift is whether one can mitigate it when we know it occurs (with gold labels) and when we do not know it occurs at test-time (without gold labels).
The following details our detection approach and mitigation strategy for problem drift.

\subsubsection{Detection Methods}
We use two setups to identify whether problem drift occurs, namely \textbf{Oracle} and \judge.
The \textbf{Oracle} setting gives us precise information about when problem drift occurs if gold labels are known and is an upper bound for mitigation strategies.
Our newly proposed \judge\ can detect problems occurring at test-time when gold labels are not known.

\paragraph{\textbf{Oracle.}}
We measure problem drift as in \Cref{def:drift} with the known true labels $y_r$. 
To estimate downstream performance, we use the default task metrics (cf. \Cref{sec:datasetmetrics}).

\paragraph{\textbf{\judge.}}
At test-time, gold labels are unknown, and the mitigation success also depends on whether we can identify when problem drift occurs.
We propose a new method based on LLM-as-a-judge \citep{ZhengCSZ23}, which receives a focal turn's and a consecutive turn's solution to assess whether problem drift occurs.
The judge base model is \href{https://huggingface.co/Skywork/Skywork-Critic-Llama-3.1-70B}{\textit{Skywork/Skywork-Critic-Llama-3.1-70B}} \citep{skyworkcritic2024}, prompted with a layout detailed in \Cref{app:prompts}.
The \judge\ tries to approximate the Oracle's predictions to detect problem drift. %

\subsubsection{Mitigation Methods}
We propose two new mitigation methods to recover samples if we detect problem drift. 
As a baseline, we \textit{regenerate} the focal drifting turn and propose a new \textbf{\policy} agent that provides situational feedback.

\paragraph{\textbf{Regenerate.}} We undo the turn suffering from problem drift and regenerate that turn with a non-zero temperature. 
We only regenerate a turn once.

\paragraph{\textbf{\policy.}} We introduce a fourth agent at the end of the drifting turn that provides feedback about how to improve the current conversation and solve the issues leading to problem drift.
\policy\ is inspired by policy feedback agent by \citet{FuPKL23b} and the feedback mechanism used with error types in \citet{kirstein-etal-2025-whats}.
We include the prompt for our feedback generation in \Cref{app:prompts}.


\subsection{Datasets \& Metrics} \label{sec:datasetmetrics}

\paragraph{\textbf{Datasets.}} We select a set of ten datasets from four domains: three generative tasks (i.e., XSum \citep{NarayanCL18b}, ETPC \citep{KovatchevMS18b}, WMT19 \citep{WikimediaFoundation19}), three reasoning tasks (i.e., StrategyQA \citep{GevaKSK21}, WinoGrande \citep{SakaguchiBBC19}, AQUA-RAT \citep{LingYDB17a}), three knowledge tasks (i.e., ETHICS \citep{HendrycksBBC23}, MMLU-Pro \citep{WangMZN24}, GPQA \citep{ReinHSP23a}), and one instruction following task (i.e., IFEval \citep{ZhouLMB23}).

\paragraph{\textbf{Metrics.}} We evaluate multiple-choice datasets (i.e., ETHICS, GPQA, MMLU-Pro, StrategyQA, WinoGrande, and Aqua-Rat) by accuracy.
For generative tasks (i.e., ETPC, XSum, and WMT19), we calculate BERTScore \citep{ZhangKWW19}.
We evaluate IFEval by the ``strict'' accuracy for instruction-following \citep{ZhouLMB23}.

\paragraph{\textbf{Sampling.}} As multi-agent debates use large amounts of test-time compute, it has become a common practice to evaluate subsets of datasets to study multi-agent systems \citep{YinSCG23a, ChenSB24a}.
We evaluate samples from each dataset with a 95\% confidence interval and a 5\% margin of error \citep{Cochran53, YinSCG23a, ChenSB24a}.
Our sample size ranges between 226 to 541 samples per dataset, depending on their size (e.g., 3.1\% of total samples on MMLU-Pro).
For the small datasets AQUA-RAT and IFEval, we use all available examples.
To further quantify the reliability of the results, we follow \citet{WangPSB24a} and run each experiment three times on randomized subsets and report statistical metrics such as the standard deviation of our results between the runs.
Detailed information about this process can be found in \Cref{app:datasets}.
