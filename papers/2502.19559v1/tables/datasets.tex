\begin{table*}[t]
\centering
\begin{tabular}{|p{3cm}|p{8cm}|p{2cm}r|}
\toprule
\textbf{Dataset} & \textbf{Description} & \textbf{Metrics} & \textbf{Samples} \\
\midrule
XSum \citep{NarayanCL18b} & Summarize a news article into a single sentence. & \makecell[tl]{BERTScore} & 386\tiny{$(\times3)$} \\
ETPC \citep{KovatchevMS18b} & Paraphrase a sentence based on a set of paraphrase types (e.g., addition/deletion, punctuation changes). & \makecell[tl]{BERTScore} & 361\tiny{$(\times3)$} \\
WMT19 (de-en) \citep{WikimediaFoundation19} & Translate a single sentence from English to German. & \makecell[tl]{BERTScore} & 341\tiny{$(\times3)$} \\
\midrule
StrategyQA \citep{GevaKSK21} & Multiple-choice questions that require strategic reasoning and planning to infer the correct answer. & Accuracy & 330\tiny{$(\times3)$} \\
WinoGrande \citep{SakaguchiBBC19} & Fill-in-the-blank task with binary options that require reasoning. & Accuracy & 295\tiny{$(\times3)$} \\
AQUA-RAT \citep{LingYDB17a} & Algebraic word problems with multiple-choice options. & Accuracy & 254\tiny{$(\times3)$} \\
\midrule
ETHICS \citep{HendrycksBBC23} & Multiple-choice benchmark for commonsense morality. & Accuracy & 351\tiny{$(\times3)$} \\
MMLU-Pro \citep{WangMZN24} & Adds more challenging examples to the MMLU dataset \citep{HendrycksBBZ21a}. & Accuracy & 373\tiny{$(\times3)$} \\
GPQA \citep{ReinHSP23a} & Google-proof multiple-choice questions written by experts from biology, physics, and chemistry. & Accuracy & 226\tiny{$(\times3)$} \\
\midrule
IFEval \citep{ZhouLMB23} & Tests instruction-following by variable prompts with 25 instruction types. & Accuracy & 541\tiny{$(\times3)$} \\
\bottomrule
\end{tabular}
\caption{Datasets with the number of samples used in the experiments extracted randomly by a 95\% confidence interval and a 5\% margin of error ($MoE$), conservatively assuming a sample proportion $p=0.5$. We randomly sample three times from each dataset and report the standard deviations in metric scores between the five runs. The top three datasets are generative tasks, the middle three datasets are reasoning-heavy tasks, and the bottom three tasks are knowledge-intensive tasks. IFEval concerns instruction-following, coming with a separate benchmark script. Their "strict" accuracy is used in this work.}
\label{tab:datasets}
\end{table*}
