\section{Conclusion}

In this work, we identified \textbf{problem drift}, a performance degradation in multi-agent debate that most often can not be recovered autonomously.
We conducted experiments on ten datasets across generative, knowledge, reasoning, and instruction-following tasks to quantify problem drift and find that it occurs across all tasks.
Our experiments showed that problem drift affects 7\% to 86\% of debates depending on the task, especially when discussing low-complexity and generative problems.
Through human annotation, we identified eight error types that characterize problem drift.
These errors can be grouped into two main categories: temporal (i.e., lack of progress, low-quality feedback, low-quality engagement) and local (i.e., lack of clarity, task non-compliance, knowledge gap, logical error, linguistic error).
We publish the corresponding annotated dataset \dataset\ for future evaluations.
To attack problem drift, we proposed \judge\ and \policy\ to detect and mitigate it.
Our novel detector \judge, based on an LLM-as-a-judge approach, showed high specificity in finding drift cases at test-time.
Our new mitigation strategy \policy\ reduced 31\% of the problem drift cases.



One can question whether problem drift is a natural property that multi-agent systems present when exploring new ideas and solutions, similar to how humans explore different reasoning paths in discussions.
Currently, our findings suggest that, in some cases, agents drift away to recover later on in the discussion. 
Still, they regularly become prone to simple temporal and local mistakes.
Future work could explore the role of agents exploring new ideas that are not directly task-related but help to reason deeply. 
Exciting avenues also lie in comparing the inner differences (or similarities) between human and agent debates.







\section*{Limitations}

The large amount of generated text in multi-agent debate can prevent human analysis of their discussions, as annotation is expensive.
In this study, we investigated only the drifting turn and its predecessor.
This decision provides a reasonable space to search for local and short-context temporal mistakes agents make during the debate yet does not consider the full discussion history, which may include valuable information.
To keep our findings on error types comparable to the actual multi-agent debate, we also limit the context length of the agents to the current and previous turn during all experiments.

Our \judge\ requires forwarding solution pairs from consecutive turns, which can be computationally expensive.
We did not find this limiting for this research project but note that for productive environments, fine-tuning a more cost-efficient classifier like BERT \citep{sun2020finetuneberttextclassification} on drifting and non-drifting examples could be a promising alternative.


