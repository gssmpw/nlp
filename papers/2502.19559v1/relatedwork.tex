\section{Related Work}
Ever since the first chatbots\footnote{The first recorded conversation between the chatbots ELIZA and PARRY is available here: \url{https://www.rfc-editor.org/rfc/rfc439}}, humans have been fascinated by the ability of computers to communicate in a human-like manner.
Recent advances in LLMs to reason and solve complex tasks \citep{OpenAIAAA24a} lead to a surge in studies on multi-agent systems \citep{ZhaoHXL23a, XuYLW23a, SuzgunK24a}.
\citet{GuoCWC24a} conduct a literature review on multi-agent LLMs.
Their taxonomy points to two main areas in which multi-agent LLMs are used, i.e., simulation and problem-solving.
Our investigation explores problem-solving because these tasks offer a controllable and objective environment to probe components in multi-agent interaction \citep{YinSCG23a, DuLTT23}. %

\noindent{\textbf{Supportive Works.}}
Several works highlight the strengths of multi-agent systems, including reasoning \citep{YinSCG23a}, creative writing \citep{SchickDJP22a}, dialogue generation \citep{ChenSB24a}, and theory of mind \citep{LiCSC23a}.
These works often use prompted personas \citep{WangMWG23a, XuYLW23a} and self-correction mechanisms like self-consistency \citep{WangWSL23} in a conversational setup.
However, the heterogeneous experimental setup across these studies (e.g., agent orchestration, decision-making, prompting) hinders their comparability \citep{Becker24, GuoCWC24a}.

\noindent{\textbf{Critical Works.}}
Other researchers study the limitations of multi-agent systems.
\citet{WangWST24} focus on the computational cost of multi-agent debate, showing that single-agent LLMs can often achieve similar performance through prompting.
Systems that include some form of self-critique mechanism (e.g., multi-agent debate) might diminish planning performance \citep{ValmeekamMK23a}. 
The correctness and content of LLM-generated criticism can be irrelevant to the performance of iterative prompting \citep{StechlyMK23a}.
\citet{ShahW24} argue that multi-agent debate has many issues, such as generalization, scalability, coordination, robustness, and ethical concerns.

\noindent{\textbf{Research Gap.}}
Researchers remain divided on whether multi-agent systems justify their computational cost and complexity, as their effectiveness depends on the use case.
Performance gains and losses are often attributed to tasks, agent orchestration, and prompt techniques.
While these systems offer potential advantages, their limitations require further systematic investigation.
We contribute to identifying the factors that contribute to the deterioration of multi-agent debates and propose new mitigation strategies for them.