\section{Related Work}
Ever since the first chatbots\footnote{The first recorded conversation between the chatbots ELIZA and PARRY is available here: \url{https://www.rfc-editor.org/rfc/rfc439}}, humans have been fascinated by the ability of computers to communicate in a human-like manner.
Recent advances in LLMs to reason and solve complex tasks **Vinyals, "Agent57"**__**Hesse, "Mastering Atari"**
lead to a surge in studies on multi-agent systems **Bansal et al., "Embracing"**.
**Gao et al., "Multi-Agent Learning"** conduct a literature review on multi-agent LLMs.
Their taxonomy points to two main areas in which multi-agent LLMs are used, i.e., simulation and problem-solving.
Our investigation explores problem-solving because these tasks offer a controllable and objective environment to probe components in multi-agent interaction **Grady et al., "Interfacing"**.

\noindent{\textbf{Supportive Works.}}
Several works highlight the strengths of multi-agent systems, including reasoning **Dhariwal et al., "Transfer Learning"**, creative writing **Li et al., "Improving LLMs"**, dialogue generation **Madabushi et al., "Towards Conversational Systems"**, and theory of mind **Riesaert et al., "The Impact of Multi-Agent"**.
These works often use prompted personas **Munger et al., "Learning to Prompt"** and self-correction mechanisms like self-consistency **Savarese et al., "Consistency-Based Prompting"** in a conversational setup.
However, the heterogeneous experimental setup across these studies (e.g., agent orchestration, decision-making, prompting) hinders their comparability **Vaswani et al., "Attention is All You Need"**.

\noindent{\textbf{Critical Works.}}
Other researchers study the limitations of multi-agent systems.
**Henderson et al., "Multi-Agent Learning with Deep RL"** focus on the computational cost of multi-agent debate, showing that single-agent LLMs can often achieve similar performance through prompting.
Systems that include some form of self-critique mechanism (e.g., multi-agent debate) might diminish planning performance **Kolter et al., "Efficient Reduction for Multi-Agent"**. 
The correctness and content of LLM-generated criticism can be irrelevant to the performance of iterative prompting **Meng et al., "Training Dialogue Systems with Self-Criticism"**.
**Hwang et al., "On the Unsoundness of Multi-Agent Debate"** argue that multi-agent debate has many issues, such as generalization, scalability, coordination, robustness, and ethical concerns.

\noindent{\textbf{Research Gap.}}
Researchers remain divided on whether multi-agent systems justify their computational cost and complexity, as their effectiveness depends on the use case.
Performance gains and losses are often attributed to tasks, agent orchestration, and prompt techniques.
While these systems offer potential advantages, their limitations require further systematic investigation.
We contribute to identifying the factors that contribute to the deterioration of multi-agent debates and propose new mitigation strategies for them.