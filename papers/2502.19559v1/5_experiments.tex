\section{Experiments}

We present the experiments by answering a series of research questions about problem drift.

\noindent \textbf{RQ1}: \textit{Is there a connection between the length of multi-agent debate and downstream task performance?} \textbf{Answer}: \textit{While some discussions benefit from longer debates, a notable subset of discussions degrades in performance, which we identify as problem drift.}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/line_charts/average_score_per_turn_Multiple-Choice_Datasets.pdf}
    \caption{Average accuracy for different numbers of turns on the three knowledge and three reasoning datasets over seven turns. The blue line shows samples where performance improves, the red line shows samples where performance decreases, and the grey line shows all discussions. Many samples (411) show performance collapse over longer debates.\vspace{-0.5cm}}
    \label{fig:avg_turn_performance_mulchoice}
\end{figure}

\Cref{fig:avg_turn_performance_mulchoice} shows the performance of multi-agent debate for different turns.
Discussions that show increasing performance (blue line) and decreasing performance (red line) are plotted separately.
For this figure, we combine the discussions from all knowledge and reasoning datasets to show broad trends that consistently occur across tasks.
We refer to \Cref{app:problemfocus_linecharts} for the dataset's individual figures.
We observe that some discussions (blue line; 286 out of 5.486) benefit from multi-agent debate and improve performance compared to the first turn's draft. The upside potential here is notable; some discussions can leap from an average accuracy of 0.65 at turn seven (grey line) to 1.
However, a much larger chunk of discussions (red line; 411 out of 5.486) suffers from a performance drop during long multi-agent interactions.
Here, the downside is more severe as discussions fall sharply from an average of 0.65 to 0.
Longer discussions provide headroom and more opportunities for intermediate errors.
This is relevant because of two reasons.
First, these errors harm task performance in problem-solving and make benefits less reliable, questioning their cost-benefit ratio because of large computing requirements.
Second, if agents are unreliable during continuous interaction, this raises concerns about unaligned behavior and harmful decision-making when employing autonomous agents in high-stake, human-centric applications \citep{HuaYJC24, MukobiRRS23}.
We also find that the longer agents discuss particularly ethical problems from the ETHICS dataset, the more their alignment degrades (details on this are in \Cref{fig:avg_turn_performance_ethics} of \Cref{app:problemfocus_linecharts}).

In many cases, the performance during multi-agent debate remains stable, meaning it neither increases nor decreases over multiple rounds.
This is surprising, given the highlighted performance benefits of multi-agent debate in related work \citep{YinSCG23a, SchickDJP22a}.
As our later experiments will demonstrate, part of the success of multi-agent debate depends on individual discussion success and the fact that few discussions drift away from the problem.
Success also depends on the task and a sensible set of parameters in debate, such as agent orchestration and decision-making \citep{GuoCWC24a}, which may have been set carefully in related works. %

\noindent \textbf{RQ2}: \textit{How prevalent is problem drift, and does it depend on the task?} \textbf{Answer}: \textit{Generative tasks drift often due to the volatility of the answer space. Multiple-choice and high-complexity tasks drift fewer times but with higher strengths.}

\input{tables/turning_points}

\input{tables/reasons}

\Cref{tab:turning_points} shows statistics for the samples suffering from problem drift on all datasets.
The first column shows the percentage of samples that have problem drift at any point during the discussion.
The third column shows the average number of turns in a discussion, after which a solution drifts ($FOCUS_r < 0$).
Multi-agent discussions for generative tasks (i.e., ETPC, XSum, WMT19) drift often, with XSum drifting for 74.6\% of samples and ETPC even drifting for 88.6\%.
We often observe agents proposing minor changes to the phrasing and wording of a solution.
Notably, generative tasks are discussed with the most turns where $FOCUS_r < 0$, i.e., when they drift.
This highlights how volatile small changes and metrics capturing changes in generative tasks are compared to multiple-choice tasks, leading to a small yet cumulative loss in \metric.
Knowledge and reasoning tasks drift for 6.6\%-15.5\% of samples.
Potentially, the answers provided in a multiple-choice setup help the agents stay on track and drift less.
There is no drift if the agents do not change the final decision.
Still, this means that many discussions start with the correct solution but suffer from the ongoing multi-agent debate.
Task complexity could play an important role in problem drift as highly complex tasks like StrategyQA (6.6.\%) and AQUA-RAT (8.4\%) show the least occurrences of problem drift.
Possibly complex tasks do not leave much room for the agents to debate unnecessary changes, leading to more valuable contributions by the agents.
Meanwhile, volatile and easier tasks may suffer from agents overly contributing or meaningless information.
Our multi-agent system was particularly prone to problem drift on the IFEval dataset (20.8\%).
Thus, agents are especially indecisive about how to adhere to instructions properly.
Agents often mention points that are unrelated to the intended tasks, facilitating problem drift.


\noindent \textbf{RQ3}: \textit{Can discussions recover from problem drift?} \textbf{Answer}: \textit{Problems with lower task complexity have a high recovery rate of up to 45\%. Tasks that require complex reasoning and have volatile answer space are less recoverable with as low as 9\%.}

The second column of \Cref{tab:turning_points} shows the percentage of samples that recover from problem drift during the discussion.
The capability to recover concerns how many of the drifted samples perform on par or better than the performance before the problem drift occurred (as detailed in \Cref{sec:definition}).
The most recoverable tasks are IFEval (44.7\%) and ETHICS (45.4\%).
This recovery rate is noticeably higher than for the other reasoning and knowledge datasets (18.5\%-39.4\%) and especially higher than generative tasks (8.5\%-19.8\%).
This hints at how the recoverability from problem drift depends on task complexity (i.e., required reasoning steps) and the volatility of the possible answer space (i.e., correct answers that vary in minor wording).
The correlation with task complexity seems especially prevalent for StrategyQA where only 18.5\% of drifting samples recover from problem drift, as this task requires complex strategic planning to give the correct answers.
WMT19 has a recoverability of only 8.5\% and is a translation task that inherently comes with ambiguity and multiple possible translations per sample.
Problem drift that concerns instruction-following or ethical question-answering is more recoverable, possibly due to the lower number of reasoning steps required for problem-solving.
We also observe that in many complex tasks, agents show redundant and circular behavior, overanalyze problems, and over-agree with others' wrong proposals.


\noindent \textbf{RQ4}: \textit{What are the reasons for problem drift as judged by human experts?} \textbf{Answer}: \textit{Both local and temporal errors appear with problem drift. Most often, a lack of progress leads to agents providing low-quality feedback and lacking clarity.}

We sample 170 examples that suffer from problem drift using \metric\ and follow a two-step approach to identify the reasons for problem drift with eight human experts.
First, we create a set of error types by automatically generating them from a large number of drifting conversations using \href{https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct}{\textit{meta-llama/Meta-Llama-3.1-70B-Instruct}} \citep{GrattafioriDJP24a} and summarizing with ChatGPT-4o\footnote{Version: 3rd of December 2024.} and manual assessment.
Second, eight human experts annotate a subset of the 170 drifting debates to identify the relevance of each error type.
We leave space for additional explanation of the labels.
For both steps (systematic and human assessment), we extract the relevant part of the discussion to keep context length manageable, i.e., the three messages of the turn before the drifting turn and the three messages of the driving turn.
This is reasonable because, during the debate, the agent's context memory is also limited to one turn.
Details on the annotation procedure are available in \Cref{app:dataset_annotation}.
We publicly release the produced dataset with 170 samples called \dataset\ together with the human annotations to our repository.

We classify the error types into \textit{local errors} (self-contained within a single message) and \textit{temporal errors} (depending on contextual relationships between messages).
\Cref{tab:reasons} shows the error types that appear in conjunction with problem drift as well as the number of samples rated by human experts into that error type category.
The most common error type annotated is a lack of progress in 60 of the 170 samples.
During long conversations, agents often repeat each other's arguments, missing out on bringing new ideas to the discussions.
However, a lack of progress does not directly cause problem drift because, for a decrease in performance, changes to the final answer are needed.
Upon further investigation, we find that a lack of progress often occurs with other error types (e.g., low-quality feedback with 45 cases and lack of clarity with 43 cases).
When agents converge on a solution, and the conversation becomes redundant, they tend to propose small incremental changes to the solution that harm task performance.
We highlight overanalysis and hypercritical suggestions as highly relevant to problem drift.
Generally, agents struggle to choose the appropriate scope of their criticism concerning the task instruction.
Agents often fall into extremes: either they overanalyze the problem, or they investigate the issue superficially.
Agents further struggle to comply with strict task requirements. 
This is especially present on the IFEval dataset, where instruction-following is the main goal.
We suppose that the agents' personas introduce a stronger focus on their preferences, which is why task compliance suffers from the debate.

The multi-agent debate also produces 23 linguistic errors within the discussion excerpts, such as typos or grammatical errors.
We find that sometimes agents get stuck in a repetitive generation.
This finding is surprising, as today's LLMs are considered to be highly competent regarding linguistic capabilities \citep{GrattafioriDJP24a}; this raises questions about how improvements in reasoning can come at the cost of degradation of linguistic quality as also seen in recent work from DeepSeek-R1-Zero \citep{deepseekai2025deepseekr1incentivizingreasoningcapability}.
As we use the default parameters for our model (cf. \Cref{app:parameters}), we expect the causality for this to be unrelated to our specific model setup.
Potentially, prompting style or discourse relationships between messages could be confounding factors.
Notably, human experts only selected ``Other'' as a label eight times, indicating that our provided list is conclusive.
We include examples of all error types in \Cref{app:examples}.

\noindent \textbf{RQ5}: \textit{Can we identify problem drift at test-time when gold labels are unknown?} \textbf{Answer}: \textit{Our proposed \judge\ has high specificity (0.92) but moderate recall (0.50) and low precision (0.11).}

\input{tables/detection_performance}

\Cref{tab:detection_performance} shows the performance of \judge\ on the MMLU-Pro dataset, indicated by specificity, precision, recall, and accuracy.
While \judge\ has a low precision (0.11), our setup based on LLM-as-a-judge achieves high specificity (0.92).
Minimizing false positives, i.e., raising specificity, with \judge\ is critical because, most importantly, \judge\ should not trigger any intervention by the \textit{Regenerate} or \policy\ mitigation methods if the discussion is not actually drifting.
This is important to avoid unnecessary computing and not facilitate new problem drift in discussions that would otherwise have been successful.
A moderate recall allows us to retrieve a marked portion of problem drift cases, but more specifically, we want to reduce the false negative rate to catch as many problem drift cases as possible.
We see room for improvement in accurately detecting problem drift at test-time using various local and temporal error types detection methods.

\noindent \textbf{RQ6 }: \textit{Can we mitigate problem drift when we know that it occurs?} \textbf{Answer}: \textit{Our \policy\ can effectively mitigate 30.6\% of drifting examples.}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/bar_charts/successful_samples.pdf}
    \vspace{-1.0cm}
    \caption{Samples on MMLU-Pro grouped by mitigation method. Blue samples never show drift during the discussion. Green samples drift but recover by the end of turn seven. The error bars are the standard deviations between three independent experiment runs.\vspace{-0.5cm}}
    \label{fig:successful_samples}
\end{figure}

\Cref{fig:successful_samples} shows the number of samples that never drift (blue) and those that recover from drift (green) using our detection and mitigation method (x-axis).
Using the \textit{Regenerate} mitigation strategy, 92.4\% of discussions show no problem drift or recover from it, compared to 90.3\% using our multi-agent baseline without any mitigation.
With \policy\, which provides targeted feedback given the identified issues leading to problem drift of RQ4 (cf. \Cref{tab:reasons}), we achieve 348 non-drifting samples (93.3\% of all 373 samples).
In other words, our combination of \judge\ and \policy\ reduces the number of unrecovered drifting samples from 36 to 25 (30.6\%).
This indicates that directly influencing the course of discussion through conversational intervention is more effective than retrying the discussion and relying on the model temperature.
One reason for this is the higher recovery rate with \policy, implying that relevant factors for problem drift already appear in preliminary messages, not just within the drifting discussion turn (i.e., temporal errors).
The capability of our mitigation originates from the comprehensive advice from prompting the \policy\ feedback agent with our proposed set of error types (cf. \Cref{tab:reasons}).
In addition, employing \policy\ is more cost-efficient, only requiring one occasional extra message to be generated, compared to a complete \textit{regeneration} of the conversation round.
