\section{Methodology}
\label{sec:methods}
\input{figs/pipeline_fig}
The IBURD (Fig.~\ref{fig:pipeline}) pipeline takes an object image, its pixel-level annotation, and the target background image as inputs. IBURD then uses a two-pass process similar to~\cite{zhang_deep_2020}, consisting of Poisson editing and style transfer. Poisson editing blends an object onto a background, and style transfer changes the appearance of the object. 

While it is possible to capture source images from real objects, it is difficult to collect source images of marine life which mainly exist underwater (\eg starfish, crab, etc.). To address this, we use a text-to-image generator, Dall-E~\cite{ramesh2021zero}, to generate an object from prompts (\eg perished plastic container with white background, deformed soda can on white background). Next, we use the Segment Anything Model (SAM)~\cite{kirillov2023segment} to segment the object.
From the user provided prompt, this pre-processing step generates both an image and its annotation, which makes this step semi-automatic. Once this pair is available, we run the first pass.

In the \textit{first pass}, we randomly resize and rotate the object image along with its annotation. 
To place the object image in the background, we split the given background image into a grid and place the object image in a randomly selected cell within the grid. Details on grid size are provided in Sec.~\ref{subsec:data_gen}. We place one object in each grid cell. This is to avoid overlapping of blended objects. 
We then use Poisson image editing~\cite{perez_poisson_2003} for blending, eliminating drastic boundary gradient changes between the object and the background.



Next, we feed the output from the first pass and the background image to the \textit{second pass}, which reconstructs the first pass output to reflect the background style (\ie underwater appearance). For this, we use the total loss (Eq.~\ref{eq:total}) consisting of three different loss functions: \textit{style loss} (Eq.~\ref{style}~\cite{gatys_image_2016, zhang_deep_2020}), \textit{content loss} (Eq.~\ref{content}~\cite{gatys_image_2016, zhang_deep_2020}), and \textit{total variation loss} (Eq.~\ref{tv}~\cite{mahendran_understanding_2015, zhang_deep_2020}). 

\begin{equation}
    \label{style}
    Loss_{style} = \sum_{l=1}^{L} \frac{\beta_l}{2N_l^2} \sum_{i=1}^{N_l} \sum_{k=1}^{M_l}(G_l[I_{r}]-G_l[I_{b}])_{ik}^2
\end{equation}
\begin{equation}
    \label{content}
    Loss_{content} = \frac{\alpha_l}{2N_LM_L} \sum_{i=1}^{N_L} \sum_{k=1}^{M_L}(F_L[I_{r}]-F_L[I_{fp}])_{ik}^2
\end{equation}
\begin{equation}
    \label{tv}
    Loss_{tv} = \sum_{m=1}^{H} \sum_{n=1}^{W} |I_{m+1,n} - I_{m,n}| + |I_{m,n+1} - I_{m,n}|
\end{equation}

\begin{equation}
    \label{eq:total}
    Loss_{total} = \lambda Loss_{style} + \mu Loss_{content} + \nu Loss_{tv}
\end{equation}


\textit{Style loss} compares the background and the reconstructed image, and \textit{Content loss} reflects the differences between the blended image from the first pass and the reconstructed image.
Using style and content loss together prevents the object from being over-stylized.

In Eq.~\ref{style} and \ref{content}, $I_r$ is the \textit{reconstructed image}, $I_b$ is the \textit{background image}, $I_{fp}$ is the \textit{output from the first pass}, $L$ is the \textit{number of convolution layers}, $N_l$ is the \textit{number of channels in activation}, $M_l$ is the \textit{number of flattened activation values} used by each channel, $F_l$ is the \textit{activation matrix} computed using network $F$ at layer $l$, $G_l = F_lF_l^T$ is the \textit{Gram matrix}, $\alpha_l$ and $\beta_l$ are the \textit{respective weights}.


We compute the style and content loss using VGG-$16$~\cite{liu_very_2015} networkâ€™s layers. Specifically, style loss uses layers relu$1$\_$2$, relu$2$\_$2$, relu$3$\_$3$, and relu$4$\_$3$ ($L=4$). Content loss uses layer relu$2$\_$2$ ($L=1$). We use pretrained weights for VGG-$16$~\cite{liu_very_2015}.
Unlike~\cite{wu_gp-gan_2019}, we do not need additional training data for image generation which makes this method favorable for small-dataset scenarios.
In addition, \textit{total variation loss} preserves the low-level features in an image.  In Eq.~\ref{tv}, $I$ is the \textit{reconstructed image}. $I_{m,n}$ denotes the entry on $m^{th}$ row and $n^{th}$ column.
In Eq.~\ref{eq:total}, $\lambda$ represents the \textit{style loss weight}, $\mu$ represents \textit{content loss weight}, and $\nu$ represents \textit{total variation loss weight}.

Using typical style transfer approaches (\eg\cite{zhang_deep_2020,wu_gp-gan_2019}) developed for artistic purposes, the total loss (Eq.~\ref{eq:total}) distorts the object boundaries when higher style loss weight (\eg $\lambda=15000$ in Fig.~\ref{weights}) is used on a blurry background image.
However, images for training detectors need to maintain the object's content while reflecting the style of the background. 
In other words, we must regulate the style loss weight to account for background quality while not losing the object shape completely. To achieve this, we use the FFT to compute spatial frequency as a measure of image blurriness~\cite{liu_image_2008}. Using FFT on the background image, we compute a mean frequency value, which acts as an indicator of blur -- the lower the FFT value, the blurrier the image.
Note that while other measures (\cite{laplacian_blur}, \cite{wavlet_blur}) of blurriness are usable, we choose the FFT out of runtime and efficiency considerations, and it has shown good performance in that regard.

\input{figs/weight_comparison}
\input{figs/tab_styleweight}

We conduct a survey with $10$ participants who were asked to rate $10$ images on a scale of $0$ to $10$ ($0$ being clear and $10$ being extremely blurry). Examples of the survey images with their blurriness scores are shown in Fig.~\ref{fig:blur}. We use this survey to validate the correlation between an FFT measure and the human perception of blurriness in an image. 



We verify the correlation between the survey and FFT mean scores and map the ranges of FFT mean values to four style loss weight values via empirical evaluations (Table~\ref{survey_weight}). A clear image has a lower blurriness score from the survey and a higher value from the FFT. For such an image, we can transfer the style of the background without losing the object content, and hence, we use a higher style loss weight. The coefficient value ($\lambda$) is dynamically assigned using the FFT mean value range mentioned in Table~\ref{survey_weight}.
 



Along with the blending process, the pipeline generates annotation information on the newly reconstructed image by translating and transforming the original annotation coordinates based on the blending location. We collect this information in COCO format, suitable for training common object detectors. 

