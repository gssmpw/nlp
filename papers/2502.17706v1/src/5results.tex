\section{Results}
\label{sec:results}

Our method creates an effective dataset for training robust visual debris detectors rather than solely improving the appearance of images for aesthetic purposes. 
The desired properties of generated images are a smooth transition border between the object and background images and the preservation of the object's content.
Our pipeline satisfies these properties and removes the background in the source object image completely from transparent (\eg plastic bottles) and translucent objects (\eg glass bottles and plastic bags). 
Additionally, the objects are not heavily distorted even when other objects exist in the background (\eg coral reefs in Fig.~\ref{fig:pipeline}). Note that while FFT-based style weights do prevent distortion, an extremely blurry background could still cause the object to lose its content information.
IBURD achieves $5$ times faster runtime compared to the recent work, Deep Image Blending method~\cite{zhang_deep_2020}. Our method can generate an image with one object in $25$ seconds. The runtime increases as more objects are blended in the background, reaching up to $50$ seconds for $4$ objects.



\subsection{Quantitative Evaluation}
\input{figs/tab_ablation}
We train the detector on three datasets for comparing IBURD and Poisson Blending: 1) \textit{T+T$10$k}, 2) \textit{T+D$2$k}, and 3) \textit{T+D$10$k}. Adding the second pass in the IBURD pipeline improves the performance of the detector in all three cases compared to Poisson blending only, as shown in Table \ref{tab:ablation}. The \textit{T+D$2$k} dataset performs the best in the case of Poisson editing. This leads us to conclude that the detector performance deteriorates as we add more data without considering the style.

We also compare the above three cases with our baseline \ie a detector trained on real images from Trashcan (\textit{T}), and the results are shown in Table \ref{tab:map_results}. Each row represents a detector trained on different datasets (as shown in column 1), with the first two rows representing the baseline cases. All evaluations, however, are performed only using real data from the TrashCan dataset.  
Even after augmenting \textit{T} with synthetically generated $10$k images using TrashCan objects, the performance of the detector does not show much improvement in the \textit{T+T$10$k} case. 
\input{figs/tab_map_results}
Additionally, augmenting data with new objects using Dall-E improves the detector performance (\textit{T+T$2$k} and \textit{T+T$10$k}). However, there is no significant difference in performance from increasing the augmented dataset size from $2$k to $10$k using the same objects.
This leads us to believe that even if it is possible to generate a large number of images using different backgrounds, sizes, etc., the detector performs better when introducing data with more significant information change.
While IBURD can provide a way to create any number of images depending on provided inputs, it is up to the user to decide how to choose the generated data distribution and what objects or information to include.
\subsection{Robotic Evaluation}
\input{figs/results_oceanpool_v2}
\input{figs/results_combination}
We train the detector on synthetically generated datasets and deploy it on the LoCO AUV. We conduct experiments in pool and sea environments as explained in Sec.~\ref{sec:experiments}. 
The detector performs at $1$-$3$ frames/second (FPS) on a NVIDIA Jetson TX2, and we monitor detector output through the LED indicator. The detector successfully infers object classes, which are in the training datasets, during pool and sea deployments (Fig.~\ref{fig:images}). 
We also evaluate the performance of detectors trained with three different datasets (\ie synthetic sea, synthetic pool, and TrashCan) using images from three different environments (\ie sea, pool, and TrashCan), in total nine cases (Fig.~\ref{fig:sample}).
When the detector is trained and tested with images from the same environments, objects are inferred correctly. Real images from the sea, pool, and TrashCan are tested on the detector with weights $w_{Sea}$, $w_{Pool}$, and $w_{TrashCan}$, respectively.
\textit{However, if the network weight and sample image pairs are from different environments, the detector shows degraded performance or completely fails to detect objects.} 
This demonstrates the necessity of using relevant datasets for target environments. 
In our experiments, we show IBURD can generate realistic synthetic datasets for pool and sea environments without using any real images of objects in the target environment. 
We also demonstrate that the detectors trained with synthetic datasets which have similar visual features to a target environment perform better than ones trained with publicly available datasets if the target environment is known a priori. The accompanying video contains additional examples of generated data and a demonstration of the on-board robot experiments




\subsection{Limitations}
The performance of a network is heavily dependent on the kind of training data used. While we do provide a pipeline to generate data, manual effort is needed to make sure the variety and the style of data match the validation data. Additionally, we do not consider overlapping objects.
