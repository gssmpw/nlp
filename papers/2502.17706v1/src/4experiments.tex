\section{Experiments}
\label{sec:experiments}


We measure IBURD's efficacy with quantitative evaluations using $10$ classes from the TrashCan~\cite{hong2020trashcan} dataset. We also perform robotic experiments with the LoCO AUV~\cite{loco_paper_2020} to evaluate our approach on visually similar but unseen environments (\eg pool and sea).
In both cases, we start by collecting some source object images and background images that are representative of the environment we selected to evaluate a detector. 
We then use IBURD to blend the object images into the selected backgrounds and train a detector using this synthetic data generated from the pipeline.

\input{figs/distribution_class_quant}


\subsection{Data Collection}
\subsubsection{Quantitative Experiments}
To evaluate a detector's performance on the TrashCan dataset, we select $10$ background images within the dataset. 
We choose images that are representative of the TrashCan dataset and contain as few objects in each image as possible. 
We collect source images both semi-automatically ($29$ images from Dall-E) and manually ($68$ images from TrashCan). 
Both Dall-E and TrashCan images have the same $10$ object classes  (Table~\ref{data_dist_quant}).
Unlike the source images from Dall-E, the distribution of the ones collected from TrashCan is non-uniform across the classes since TrashCan has more data from common debris items (\eg bottle) compared to other classes (\eg starfish).


\subsubsection{Robotic Experiments}
We collect $47$ source images containing $7$ classes of objects commonly found in marine debris (Table~\ref{data_dist}). We have a smaller number of classes compared to the quantitative experiments since not all $10$ classes of objects can be easily placed underwater (\eg pipe and crab).
We then manually annotate the images, providing class labels, bounding boxes, and segmentation information.
We use $2$ types of background images for blending: sea and pool. 
We collect $7$ images captured at different locations from the pool used for experiments and add $3$ pool images from online sources to add more variety. 
For the sea background, we use $10$ background images sourced from the Internet. 


\subsection{Image Blending and Data Generation} 
\label{subsec:data_gen}
\subsubsection{Quantitative experiments}
\label{subsec:blend_quant}
To generate a diverse dataset with $512\times512$ pixel-size images, we use $4$ different rotations ($0^{\circ}$, $90^{\circ}$, $180^{\circ}$, $270^{\circ}$) for the source object images. 
For Dall-E generated images, we use $4$ different source image sizes,
 ($96\times96$, $128\times128$, $192\times192$, $256\times256$ pixels). 
With source objects from TrashCan, we only use $2$ source image sizes ($192\times192$, $256\times256$ pixels).
This is because TrashCan has a mixture of close-up and long-range views of objects, and using the smaller scales might produce blended objects that are not visible to the human eye, making them impossible to annotate. Close-up version of objects generated by Dall-E enables the use of smaller-scale images.


For both cases, the image is split into a $2\times2$ grid to avoid overlap with previously blended objects. We determine the object location by randomly selecting one section within the grid on the background image plane. Using the grid we blend up to $4$ objects in the same background. This way we create $3$ different datasets: 1) TrashCan training data with $2$k images generated using Dall-E source objects (\textit{T+D$2$k}), 2) TrashCan training data with $10$k images generated using Dall-E source objects (\textit{T+D$10$k}), and 3) TrashCan training data with $10$k images generated using source objects from TrashCan (\textit{T+T$10$k}). 
For these $3$ cases, we also generate images using just Poisson image editing (\ie first pass only) to train a detector on Poisson-blended data alone. This makes it possible to assess the effect of adding the second pass in the IBURD pipeline on detector performance compared to the same detector trained on basic Poisson-blended data. For both blending techniques (Poisson Image Editing and IBURD), the images used are the same in terms of objects, size, orientation, and location (\eg Fig.~\ref{fig:compare}). We use IBURD generated data for training and evaluate the performance using real-world validation data from TrashCan for all cases.

\input{figs/data_source}
\subsubsection{Robotic Experiments}
Similar to the case of quantitative experiments, we generate datasets with $4$ rotations and $4$ sizes for the $47$ source object images and create images with up to $4$ objects blended in the same background.
We determine the object location by randomly selecting one section within the grid on the background image plane. 
For multi-object blending, we divide the background ($512\times512$ pixels) into $2\times2$ grid considering the maximum size of source images (\ie $256\times256$ pixels). 
For the single object case, the image plane is divided based on the current source image size (\ie $4\times4$ grid for $96\times96$ and $128\times128$ pixel-size images, $2\times2$ grid for $192\times192$ and $256\times256$ pixel-size images). 
For the sea backgrounds, we create $1,880$ images with $1$ object, $2,209$ images with $2$ objects, $3,008$ images with $3$ objects, and $4,096$ images with $4$ objects making a total of $11,193$ images for training. 
Similarly, in the case of pool background, we create $1,880$ images with $1$ object, $2,209$ images with $2$ objects, $2,396$ images with $3$ objects, and $1,731$ images with $4$ objects, giving a total of $8,216$ images for training. 
We generate a smaller dataset for pool images since it is a controlled environment and has limited variety in the type of backgrounds that can occur in real-world images.





In both experiments, the background image and the final blended image are of size $512\times512$. We use $100$ iterations for style transfer during the second pass. We empirically fix the number of iterations. Similar to ~\cite{zhang_deep_2020}, we use a $L-BFG$ solver to optimize the total loss, set the content loss weight $\mu$ to $1$, and choose the total variation loss weight $\nu$ to be $10^{-6}$. In our method, the style loss weight $\lambda$ is based on image blurriness (Table~\ref{survey_weight}). Some examples of generated images are shown in Fig.~\ref{blend_example}.
\input{figs/blending_fig}
 

\subsection{Object Detection and Instance Segmentation}
To evaluate the efficacy of our framework, we first select YOLACT~\cite{bolya2019yolact} as an instance segmentation model based on its inference speed and performance with pretrained weights. 
We choose ResNet50-FPN as a backbone of YOLACT to obtain a reasonable inference time on the low-power mobile GPU (see Sec.~\ref{sec:robot_setup}) on the LoCO AUV.
We train the model with $4$ different datasets for quantitative experiments (Sec.~\ref{subsec:blend_quant}): \textit{T+D$2$k}, \textit{T+D$10$k}, \textit{T+T$10$k}, and original TrashCan data. For robotic experiments, we train the model with $2$ synthetic datasets: pool and sea data. The model is trained on an NVIDIA GeForce RTX 2080 Ti. 

\subsection{Robot Setup}\label{sec:robot_setup}
We use the LoCO AUV to run the detector model with trained weights on its mobile GPU (\ie NVIDIA Jetson TX2). 
We utilize image frames from the right camera of the LoCO AUV to make inferences. 
We adopt an LED lighting indicator system~\cite{fulton2023hreyes} 
installed on the LoCO AUV's left camera to visually examine the performance of different network weights during deployments. 
The $40$ LEDs in the system are split into $7$ groups, where each group denotes a specific class with a unique color (as shown in Fig.~\ref{fig:images}). 

