\section{Related Work}
\label{sec:related}

Recent advances~\cite{lecun2015deep, zaidi2022survey} in deep learning have vastly improved object detection and instance segmentation results in the terrestrial domain. 
Such progress has been achieved by developing effective designs of models and training them with large datasets~\cite{lin2014microsoft, russakovsky2015imagenet} containing millions of images and corresponding labels. 
Even with such advances, detecting underwater debris still remains challenging. 
While~\cite{fulton2019robotic} presents the first deep learning based approach to detect underwater debris and outperforms previous non deep learning approaches, the accuracy is worse than general object detection tasks due to a small training dataset. 
To increase the debris detection accuracy,~\cite{hong2020trashcan} proposes a larger dataset, TrashCan, which has both bounding box and pixel-level annotations for object detection and instance segmentation along with baseline results using Mask R-CNN~\cite{he2017mask} and Faster R-CNN~\cite{ren2015faster}. 
However, increasing the dataset size to improve debris detection accuracy further is not scalable due to debris data scarcity and labeling costs. 
To overcome the data scarcity issue,~\cite{hong_generative_2020} proposes a generative method, augmenting the existing dataset with synthetic underwater debris images. 
While the method can create realistic synthetic images, it still requires additional labeling efforts to be used for training detectors. 

Style transfer~\cite{singh_neural_2021,jing_neural_2020} is an approach for changing the appearance of one image based on the visual style of another. 
\cite{rodriguez_domain_2019, yu_sc-uda_2022} use this to improve detection in images taken from various domains (\eg different light conditions and image clarity). 
They aim to account for low-level texture changes in images by updating them to have the same style throughout the data. 
\cite{kadish_improving_2021} also attempts to improve detection using style transfer, by having the detector learn high-level features (\eg object shape) instead of low-level features (\eg the texture of paintings). 
\cite{amirkhani_enhancing_2021} uses style transfer to simulate various types of noise that may be present in real-world data. 
\cite{lin_gan-based_2021,liu_lane_2020} use style transfer to imitate varying light conditions. 
Style transfer has been applied beyond RGB images; \eg\cite{cygert_style_2019} converts RGB images from COCO dataset~\cite{lin2014microsoft} to thermal images and uses them to train a thermal image detector. 
While style transfer works well in augmenting the appearance of an image, it does not add new objects to our data.


Unlike style transfer, image blending based methods allow placing new objects anywhere on target background images. 
\cite{perez_poisson_2003} introduces Poisson editing using Laplacian information to smooth the boundary between the image patches and target images. 
\cite{wu_gp-gan_2019} uses a GAN-based approach for image blending, producing realistic images; however, it requires image pairs of empty backgrounds and objects placed in the backgrounds to train, limiting its use when the source data is limited. 
\cite{georgakis_synthesizing_2017} modifies~\cite{perez_poisson_2003} to find spaces within a given image plane to blend an object. 
However, detectors trained with their synthetic data show degraded performance on real data due to the style discrepancy between the blended objects and backgrounds in the dataset.
\cite{zhang_training_2022} uses a harmonization blending approach to create new data for aerial search and rescue, but it does not blend the boundary of target objects. 

\cite{zhang_deep_2020} presents a two-stage deep network-based approach to blend an image patch onto a background. Unlike~\cite{wu_gp-gan_2019} their approach does not need additional training data to generate blended images.
\input{figs/compare_fig}
They use the proposed method mainly for artistic purposes and it struggles with blending transparent source images onto background images, as seen in Fig.~\ref{fig:compare}. 
The method is only tested with $20$ images and takes approximately $4$ minutes to blend one object in an image of size $512\times512$ pixels.

Our proposed approach, IBURD, allows us to place source images at various locations and scales in target background images with relevant bounding box and pixel-level annotations within $50$ seconds, which is $5$ times faster than~\cite{zhang_deep_2020}. 
Our method addresses blending transparent objects using Poisson editing, a situation that previous methods fail to cover.
Additionally, IBURD deals with object distortion due to excessive style transfer using Fast Fourier Transform (FFT)~\cite{liu_image_2008} based weight adjustment for loss.
