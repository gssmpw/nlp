\section{Introduction}
\label{sec:intro}
The ever-increasing amounts of underwater debris pose a significant threat to the aquatic ecosystem, and their volume far exceeds the collection capacity of manned missions.
Due to the scale of this problem and the risks posed to cleanup personnel, a robotic detection and removal system becomes an attractive option.
Recent developments in machine learning and computer vision have made highly accurate detections possible in many terrestrial and aerial applications (\eg medical~\cite{litjens2017survey}, agricultural~\cite{zhang2020applications}, manufacturing~\cite{ahmad2022deep}). 
However, underwater robotic detection remains difficult due to significant visual challenges: 
\begin{enumerate*}
\item absorption makes colors vary at different depths, 
\item light scattering causes images to be noisy and blurry, and 
\item light refraction distorts objects' appearances.
\end{enumerate*}
Furthermore, usable datasets for training underwater object detection are limited. 
Object detectors trained on large terrestrial datasets cannot be used as they do not represent deformations seen in marine debris, and few sufficiently large underwater datasets~\cite{fulton2019robotic, hong2020trashcan} are available. 
\input{figs/introfig}


Researchers have addressed marine debris detection using sonar~\cite{Toro2016Trash}, acoustic sensors~\cite{acousticdetect}, and LIDAR~\cite{ge_semi-automatic_2016}. 
Fulton \etal~\cite{fulton2019robotic} use deep learning-based underwater debris detection models and obtain higher accuracies compared to previous methods~\cite{Toro2016Trash, acousticdetect, ge_semi-automatic_2016}.
In~\cite{hong2020trashcan}, we further improve the detection performance by introducing a larger-size dataset, TrashCan, with pixel-level annotations.
Yet, building a large dataset is labor-intensive and costly. Additionally, collecting imagery of underwater debris is not just difficult and dangerous, but also infeasible, given the vastness of the underwater environment and the challenges it poses to human exploration.  
To obtain large datasets without collecting images, ~\cite{hong_generative_2020} introduces a generative method using a two-stage variational autoencoder. 
However, the images from this approach still need to be manually annotated for augmenting datasets which often becomes a bottleneck. 
Also, generative approaches have an implicit bias for the dataset they are trained on. 
We posit, and indeed discover (Sec.~\ref{sec:results}), that a detector trained on these images would not perform well in a drastically different test environment.


In this paper, we propose a framework called \textit{I}mage \textit{B}lending for \textit{U}nderwater \textit{R}obotic \textit{D}etection, or \textit{IBURD}.
This framework semi-automatically augments datasets for training object detectors. We create source object images using a text-to-image generator~\cite{ramesh2021zero} and infer their annotations with an image segmenter~\cite{kirillov2023segment}. Alternatively, it is also possible to directly use an object image and its annotation as source inputs. 
With the source images and their annotations, IBURD blends the images into target backgrounds using Poisson editing~\cite{perez_poisson_2003} and style transfer~\cite{zhang_deep_2020} techniques. 

With Poisson editing, we patch source object images with varied scales and orientations at desired locations on target background images. 
Then, style transfer matches the style of the patched objects to the target background image.
Furthermore, we update the annotations simultaneously as the orientation, scale, and location of source objects change. 
As a result, we can create realistic synthetic images with desired objects and their pixel-level annotations without collecting and labeling images manually.



We quantitatively show that augmenting a dataset with our method improves the performance of image detection and image segmentation. 
Additionally, by deploying the detector trained with the synthetic datasets on the LoCO AUV~\cite{loco_paper_2020} in open-water (sea) and confined-water (pool) environments (Fig.~\ref{fig:intro}), we show that autonomous underwater vehicles can perform object detection on mobile, low-power computing hardware in visually challenging and data-scarce environments without collecting real data prior to their deployment. 
We make the following contributions in this paper:
\begin{enumerate}
    \item We propose a novel pipeline, IBURD, to perform image blending and style transfer in series to generate realistic synthetic data semi-automatically for training object detectors.
    \item We use a novel weight adjustment approach for our loss using spatial frequency information of an image.
    \item We demonstrate that the model trained with augmented data using IBURD improves the detector's performance compared to training it with only real-world data.
    \item We evaluate the performance of the trained detector model on an AUV in pool and sea environments.
\end{enumerate}




