\begin{figure*}[ht]
\vskip -0.3in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{figs/figs/fig-algo-waterfall.pdf}}
\caption{A ``waterfall pattern,'' emerges in reasoning tasks. We manually inspect attention maps across 28 layers and 28 heads of Qwen2.5-Math-7B~\cite{qwen25mathtechnical} on 100 MATH500~\cite{math500} data points. We find (a) $20\%$ to $25\%$ maps with milestone tokens, (b) $1\%$ to $2\%$ maps with phoenix tokens that remain inactive for more than 128 decoding steps before becoming active again, (c) more than 70\% ``lazy''~\cite{zhang2025lighttransfer} maps with StreamingLLM pattern. We used our best effort to balance the clarity and completeness of attention maps.}
\label{fig-algo-waterfall}
\end{center}
\vskip -0.3in
\end{figure*}