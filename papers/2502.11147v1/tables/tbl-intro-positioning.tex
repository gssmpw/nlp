\begin{table}[ht]
\begin{tabular}{cccccc}
\toprule
{} & 
{Dense} & 
{Sink} & 
{H2O} & 
{Quest} & 
{Ours} \\ 
\midrule
{Time} & 
{$O(N)$} & 
{$O(L)$} & 
{$O(L)^*$} & 
{$O(L)$} &
{$O(L)$} \\ 
\midrule
{Mem.} &
{$O(N)$} & 
{$O(L)$} & 
{$O(L)^*$}  & 
{$O(N)$} & 
{$O(L)$} \\ 
\midrule
{Acc.} & 
{$1$} & 
{$\approx0$} & 
{$\approx1$} & 
{$\approx1$} & 
{$\approx1$} \\ 
\bottomrule
\end{tabular}
\centering
\caption{Comparison of sparsity-based attention algorithms, including Dense~\cite{vaswani2017attention}, Sink~\cite{xiao2023efficient}, H2O~\cite{zhang2023h2o}, Quest~\cite{tang2024quest}, and our approach. $N$ indicates the sequence length while $L$ indicates the cache budget where $L\ll N$. The asterisk on H2Oâ€™s time and memory complexity indicates theoretical values. In practice, H2O is infeasible due to its inability to leverage efficient attention kernels and the lack of page-level KV management. Our approach achieves true $O(L)$ complexity for both time and memory, with accuracy comparable to Dense on reasoning tasks.}
\label{tbl-intro-positioning}
\end{table}