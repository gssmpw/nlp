\section{Conclusion}
\label{sec-conclusion}

In this paper, we have identified a new waterfall attention pattern observed in the decode stage of reasoning tasks. Leveraging this pattern, we have proposed a sparsity-based algorithm named \algo\ that achieves high accuracy while maintaining $O(L)$ time and $O(L)$ memory complexity. Our experiments, conducted across three datasets and four reasoning-enabled models, demonstrate that \algo\ delivers comparable accuracy and latency to the state-of-the-art Quest, but with constant memory consumption. The key to \algo's success lies in the handling of milestone tokens, which represent intermediate conclusions leading to the final output.