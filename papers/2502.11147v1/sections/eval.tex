\input{figs/fig-eval-e2e}
\section{Evaluation}
\label{sec-eval}

\subsection{Experiment Setup}

We implement \algo\ based on Hugging Face~\cite{hf} and Quest~\cite{tang2024quest} with 2K lines of code. We port Quest from their public repository\footnote{\url{https://github.com/mit-han-lab/Quest}. Accessed on Oct 2024.}. Next, we discuss the datasets, models, metrics, and the environment in which we carry out experiments. 

\textbf{Dataset.} We take the first 200 questions from each of the following three open-sourced datasets for our benchmarks: GMS8k~\cite{gms8k}, MATH500~\cite{math500}, and AIME~\cite{aime}, to test the reasoning ability of language models. First, \textit{GMS8k}~\cite{gms8k} contains 8.5k high-quality, linguistically diverse grade school math word problems. These human-written problems need solutions that involve multi-step reasoning and a series of basic arithmetic operations. Second, \textit{MATH500}~\cite{math500} contains 500 challenging problems sourced from high school mathematics competitions with five distinct levels based on the Art of Problem Solving (AoPS) framework, ranging from level 1 to level 5. Third, \textit{AIME}~\cite{aime} is a math problem dataset collected from the AIME (American Invitational Mathematics Examination) competition from 1983 to 2024, designed to challenge the most exceptional high school mathematics students in the United States. These problems cover various fields, such as algebra, geometry, and number theory.

\textbf{Metrics.} We use two metrics to evaluate performance and model accuracy. First, \textit{Job Completion Time (JCT)} is the time from when users send a request (a prompt) to LLMs to when users receive a complete response. A smaller \textit{JCT} indicates a faster algorithm. Second, \textit{Accuracy}~\cite{wang2024openr} measures the mathematical equivalence between an LLM’s output and the ground-truth answer. For each data point, it is either correct or incorrect, and the overall accuracy is reported as the percentage of correctly solved problems across the entire dataset.

\textbf{Models.} We evaluate our algorithm using four popular models: Marco o1~\cite{zhao2024marco}, Qwen2.5 Math 7B~\cite{wang2024openr}, Mistral Math 7B~\cite{wang2024openr}, and DeepScaleR 1.5B\footnote{https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2}. They are four of the most powerful open-sourced LLMs with long-reasoning capabilities.


\textbf{Baselines.} We compare \algo's accuracy with Dense, H2O, StreamingLLM, and Quest. We implement H2O and StreamingLLM using the HuggingFace Cache class. We compare \algo's latency and memory consumption with only Dense and Quest because StreamingLLM and H2O achieve too low accuracy to be included. We use Quest's official repo with $page\_size=16$.


\textbf{Environment.} We run experiments on a single NVIDIA A100 server with one A100-80GB GPU available. It has 128-core Intel(R) Xeon(R) Platinum 8358P CPU@2.60GHz with two hyperthreading and 1TB DRAM. We use Ubuntu 20.04 with Linux kernel 5.16.7 and CUDA 12.6. Unless stated otherwise, we set $\alpha=0.0001$ and $page\_size=16$.

\subsection{Accuracy and Cache Budget Trade-off}




We evaluate five algorithms across three datasets and four models, yielding three key insights from the experimental results (Figure~\ref{fig-eval-e2e}). First, H2O and StreamingLLM exhibit poor accuracy under fixed cache budgets compared to others. StreamingLLM indiscriminately discards important tokens, including milestone tokens. H2O, on the other hand, overemphasizes accumulated historical attention scores, leading it to retain outdated milestone tokens for too long while discarding newer, relevant ones. Second, Quest and \algo\ achieve the best accuracy. Quest retains all KVs while \algo\ optimizes memory usage by specifically handling milestone tokens and using only $O(L)$ memory (Figure~\ref{fig-eval-time-memory}). Across these datasets, a cache budget of 1024 tokens is generally sufficient to match Dense’s accuracy. Third, when the cache budget is small, \algo\ underperforms because \algo\ retains all prefill tokens, and with a limited cache budget, most of the budget is allocated to prefill tokens, causing almost all decoding tokens to be discarded, which negatively impacts accuracy. For small cache budgets or long-prefill scenarios, we recommend using Quest for prefill tokens and \algo\ for decode tokens.

\subsection{Latency/Memory vs. Decoding Length}

\input{figs/fig-eval-time-memory}

We evaluate the Dense, Quest, and \algo\ in terms of their latency and memory consumption, yielding two key observations from the experimental results (Figure~\ref{fig-eval-time-memory}). First, as the number of decode tokens increases, Dense's latency grows quadratically, while both \algo\ and Quest exhibit linear latency growth. This is because Dense has $O(N^2)$ computation time, whereas \algo\ and Quest have $O(NL)$ computation time, reducing each decoding step to $O(L)$. Second, as the number of decode tokens increases, the memory consumption of Dense and Quest grows linearly, while \algo\ initially increases linearly but plateaus once the number of decode tokens exceeds its cache budget. In summary, while Dense and Quest have $O(N)$ memory complexity, \algo\ achieves $O(L)$ memory complexity. With a smaller memory footprint, inference engines using \algo\ are likely to achieve significantly higher throughput.



\subsection{Micro-Benchmarks}


\textbf{The Impact of Discarding Milestone Tokens.} Figure~\ref{fig-eval-num-decode} shows that discarding milestone tokens, as in H2O-128 and Sink-128, increases the decoding lengths. Analysis of the outputs reveals that while the model initially reasons correctly for the first few tokens (e.g., green tokens in Figure~\ref{fig-eval-num-decode}), it loses track (orange tokens) of the reasoning process when milestone tokens are discarded, leading to repeated attempts at re-reasoning (red tokens), which ultimately results in the model getting stuck indefinitely.

\input{figs/fig-eval-num-decode}
\input{figs/fig-eval-alpha}
\textbf{The Impact of $\alpha$.} The choice of $\alpha$ affects the distribution of tokens' timestamps, with $\alpha = 0.0001$ generally yielding optimal results, as shown in Figure~\ref{fig-eval-alpha}. First, when $\alpha$ is small, too many tokens are assigned the latest timestamp, preventing effective differentiation of milestone tokens. Second, when $\alpha$ is large, most tokens are deemed irrelevant, potentially leading to the loss of milestone tokens.








