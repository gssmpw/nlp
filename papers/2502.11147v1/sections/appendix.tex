\section{More Examples for Milestone Tokens}

This section presents more examples of milestone tokens in Figure~\ref{fig-appendix-milestone1} and Figure~\ref{fig-appendix-milestone2}. For all examples, we input the prefill tokens to Qwen 2.5 Math 7B Instruct and obtain the corresponding decode tokens, as shown in the figure. The tokens marked red represent the milestone tokens or ``water columns'' in the attention map. Although we only show a few examples here, the milestone tokens or waterfall patterns abound in reasoning tasks.

\input{figs/fig-appendix-milestone1}
\input{figs/fig-appendix-milestone2}

% \section{The Impact of $\alpha$}

% Due to the page limit, we put the figures and experimental results of the impact of $\alpha$ in this section.
% \input{figs/fig-eval-alpha}

\section{System Implementation}

We implement \algo\ based on Hugging Face~\cite{hf} and Quest~\cite{tang2024quest}.
Our implementation extends the standard transformer architectures (Dense) by incorporating the following additional components.

First, for each page in the KV cache, a representative key is selected to compute the estimated attention score between the page and the query.
To ensure consistency in evaluation, our page-based \algo\ adopts the same representative selection algorithm as that used in Quest.

Second, a timestamp is associated with each KV page to record the last \textit{time} at which the page received an estimated attention score exceeding the threshold $\alpha$ (e.g., $\alpha=0.01$). The term \textit{time} here refers to any monotonically increasing attribute.
In our implementation, we use the number of tokens (including both prefill and decode tokens) as this attribute.

Third, when the KV cache reaches its maximum capacity, an eviction algorithm evicts the KV page with the earliest recorded \textit{time}. Pages corresponding to prefill tokens are exempt from eviction.

The selection of representative keys and the computation of estimated attention scores are interleaved with the self-attention computation in each layer, aligning with Quest's implementation. The updates of timestamps and the eviction of KV pages are performed after each autoregressive iteration, with all layers processed in a batched manner, resulting in negligible time overhead, as shown in Figure~\ref{fig-eval-time-memory}.

\section{Checklist-Related Issues}

Three datasets GSM8k (MIT), MATH500 (MIT), AIME (MIT), and four models Mistral Math 7B (No licence), Qwen 2.5 Math 7B Instruct (apache-2.0), Marco o1 (apache-2.0), DeepScaleR 1.5B Preview  (MIT) are used with their intended usage scenarios. We retrieve all models and datasets from Hugging Face\footnote{\url{https://huggingface.co/}}, where detailed documentation, including parameter sizes and model architectures, is provided. We manually checked the data and believe there is no personal information misused.

We used ChatGPT to check the grammar of the texts.

To the best of our knowledge, we believe our work does not pose risks that harm any subgroup of our society. 


