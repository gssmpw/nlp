\section{Introduction}
\label{sec-intro}


Large Language Models (LLMs) have gained widespread adoption due to their exceptional performance and versatility across various applications. However, a significant challenge to large-scale deployment and application is the high computational cost associated with long-context inference. LLMs must process an entire prompt during the prefill stage and then generate tokens autoregressively during the decode stage. Both stages require significant processing time and memory for intermediate data, specifically the Key-Value (KV) cache. For standard attention algorithms (also referred to as Dense~\cite{chenarkvale} algorithms), the time and memory complexity is $O(N)$, where $N$ is the sequence length. For example, in the Llama 3.1 8B model, sequences can grow up to 128k tokens, resulting in potentially thousands of seconds of processing time and 16GB of KV cache for a single request.


There are two primary types of long-context inference. The first type is long-prefill inference, commonly encountered in Retrieval-Augmented Generation (RAG) tasks, where the used LLM is required to process a lengthy prompt before generating responses. Previous research~\cite{cunchen2024memserve, lianmin2024sglang, woosuk2023vllm, jin2024ragcache, yushi2024longbench} has primarily focused on this inference type. The second type is long-decode inference, which has recently gained prominence in reasoning tasks, such as those exemplified by OpenAIâ€™s models~\cite{openai2024o1} (e.g., o1, o3) and DeepSeek R1~\cite{damai2024deepseek}. In reasoning tasks, the decode stage accounts for 99\% of the Job-Completion-Time (JCT) (Figure~\ref{fig-background-pd}), becoming a critical bottleneck.



To mitigate high time and memory consumption in long-prefill scenarios, existing sparsity-based algorithms~\cite{tang2024quest, zhang2023h2o, xiao2023sink} propose retaining only the most critical token's KV and discarding the rest. However, when directly applied in long-decode scenarios, these existing algorithms struggle with the ``impossible trinity'' of accuracy, time, and memory (Figures~\ref{fig-background-approaches} (b)(c)(d)). For example, the state-of-the-art algorithm, Quest~\cite{tang2024quest}, achieves high accuracy with  $O(L)$ time but $O(N)$ memory, where $L$ is the cache budget and $L \ll N$.

To achieve high accuracy and $O(L)$ time/memory complexity at the same time, we analyze the attention pattern during the decode stage of reasoning tasks, uncovering two key characteristics. First, we identify \textbf{milestone tokens}, which initially exhibit high attention scores but gradually receive lower scores and never receive high scores again. Analogous to lemmas in mathematical proofs, milestone tokens emerge, are utilized, and then fade away. These tokens, visible as bright columns (on the attention map) that slowly diminish---similar to a water column in a waterfall (Figure~\ref{fig-algo-waterfall})---must be carefully managed to prevent significant accuracy loss (Figure~\ref{fig-eval-e2e}). Second, we identify \textbf{phoenix tokens}, which receive low attention scores for a period long enough to be evicted from the cache but later regain importance. These tokens typically appear in short prefill prompts, such as mathematical questions. Quest~\cite{tang2024quest} retains the entire KV cache to avoid losing phoenix tokens, resulting in the $O(N)$ memory complexity.

Based on the preceding observations, we propose a simple yet effective algorithm named \algo\ that addresses the ``impossible trinity'' and consists of two main ideas. First, we identify and retain milestone tokens only until they are no longer needed, using timestamps to track their importance. When a token receives an attention score above $\alpha$ (e.g., $\alpha=0.0001$), we assign it the latest timestamp. Milestone tokens always receive the latest timestamp until it becomes unimportant. When the cache is full, we evict tokens with the oldest timestamp. Second, we retain the KV cache of all prefill tokens without eviction. Since prefill tokens are typically short and phoenix tokens almost always appear within them in reasoning tasks, retaining these tokens ensures that critical information is not lost during the decode stage.

We implement \algo\ with 2k lines of Python code. To evaluate its performance, we compare it against H2O~\cite{zhang2023h2o}, StreamingLLM~\cite{xiao2023sink}, and Quest~\cite{tang2024quest} using three mathematical datasets on four reasoning-enabled models. Our experimental results demonstrate that \algo\ achieves comparable accuracy and latency to Quest, while offering a significant advantage in memory efficiency ($O(L)$ memory complexity).

In this paper, we make the following three main contributions: 

\begin{itemize}
    \item We identify a novel waterfall attention pattern in reasoning tasks, where milestone tokens (analogous to mathematical lemmas) emerge, are utilized, and then become unimportant.
    \item Based on the waterfall attention pattern, we propose a new algorithm \algo\ that achieves high accuracy with $O(L)$ time and $O(L)$ memory complexity. 
    \item We implement \algo\ in a system, demonstrating constant memory usage while maintaining similar accuracy and time performance compared to the state-of-the-art Quest.
\end{itemize}