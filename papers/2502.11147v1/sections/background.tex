\input{figs/fig-background-pd}
\section{Background and Motivation}
\label{sec-background}

In this section, we overview the Large Language Model (LLM) inference, highlighting the key concepts and challenges that motivate our work.

\subsection{Autoregressive Generation \& KV Cache}
%
LLMs generate tokens autoregressively, predicting one token at a time based on the input. 
This process involves two stages: the prefill stage and the decode stage.
%
In the \textbf{prefill} stage, LLMs process the entire input prompt $(x_1, x_2, \dots, x_n)$, 
computing and caching the key and value vectors for each token. This stage can be slow for long inputs, 
and the time to generate the first token is measured by the Time-to-First-Token (TTFT) metric.
%
In the \textbf{decode} stage, LLMs generate one token at a time. 
The model computes the probability of the next token $x_{n+1}$, 
selects the most likely token, and appends its key and value vectors to the KV cache. 

The KV cache~\cite{pope2023efficiently}, 
which stores the key and value vectors computed by the attention module, 
accelerates generation by allowing the model to process only the new token instead of recalculating KV for the entire sequence. With the KV cache, the attention mechanism exhibits a computational complexity of $O(N)$ for one decoding step and a memory complexity of $O(N)$ for storing the KV cache, where $N$ is the number of tokens or the sequence length.


\subsection{Cost Transfer: from Long-Prefill to Long-Decode Inference}


% \footnote{For single-document and multi-document QA, we use the 2wikimQA dataset as a representative, as its prefill-to-decode token ratio closely resembles others in these two categories}

Long-context inference incurs significant costs due to both memory and time requirements. First, it demands substantial memory resources, reaching up to 16 GB KV cache (in addition to the 16 GB model parameters) for processing 128k tokens running the LLaMA 3.1 8B model in FP16 precision\footnote{\url{https://huggingface.co/blog/llama31}}. Second, it requires considerable processing time, with inference for 32k tokens taking around 20 - 1000 seconds on vLLM 0.6.1 using the same model (Figure~\ref{fig-background-pd} (c)).

Long-context inference can be categorized into two types: long prefill and long decode. \textbf{Long prefill} arises from extensive input prompts, as observed in prior studies such as Retrieval Augmented Generation (RAG)~\cite{li2022survey,jin2024ragcache,gao2023retrieval,jeong2024adaptive,ram2023context,mao2020generation} (Figure~\ref{fig-background-pd} (a)). \textbf{Long decode} occurs particularly in reasoning-intensive tasks. Recent advancements emphasize reasoning, where models are guided to think, introspect, and iteratively refine their outputs~\cite{openai2024o1, wang2024openr, lightman2023let, zhao2024marco, wei2022chain}. This approach significantly enhances accuracy but shifts the computational burden to the decode stage. For instance, the OpenAI o1 model~\cite{openai2024o1} requires approximately tens or hundreds of seconds\footnote{\url{https://www.reddit.com/r/OpenAI/comments/1frdwqk/your_longest_thinking_time_gpt4_o1_o1mini/}} of ``thinking time'' before producing its final output. Given the prolonged decoding time and its already substantial proportion of the overall inference process (Figure~\ref{fig-background-pd} (b)), it is critically important to further optimize the decode stage to reduce both latency and memory consumption.


\subsection{Existing Sparsity-Based Algorithms}

\input{figs/fig-background-approaches}

To reduce memory and time complexity in long-prefill scenarios, one line of research proposes sparsity-based algorithms~\cite{xiao2023sink, zhang2023h2o, tang2024quest, chenarkvale}. Sparsity-based algorithms propose retaining only the most critical tokens' (fewer than 10\%~\cite{tang2024quest}) KV and discarding the rest. However, when directly applied in long-decode scenarios, existing algorithms struggle with the ``impossible trinity'' of accuracy, time, and memory (Figure~\ref{fig-background-approaches} (b)(c)(d)).

The differences among existing algorithms are shown in Figure~\ref{fig-background-approaches}. First, the Dense or the standard attention algorithm~\cite{vaswani2017attention} achieves the highest accuracy but incurs the highest time and memory complexity. Second, StreamingLLM or Sink~\cite{xiao2023sink} retains only the KV cache of the initial and final tokens, resulting in low time and memory complexity, but this extreme approach leads to low accuracy on reasoning tasks (and other tasks~\cite{tang2024quest}). Third, H2O~\cite{zhang2023h2o} theoretically offers low time and memory complexity, but its inability to utilize efficient attention kernels and the lack of page-level KV management makes it impractical, resulting in both low accuracy and infeasibility. Fourth, Quest achieves high accuracy and low time complexity but conservatively retains all KV cache, thus $O(N)$ memory complexity.



