\section{Related Work}
\label{sec-related}

Several approaches have been proposed to reduce the computation and memory footprint of the KV cache during long-context inference. These can be categorized into two types: one that requires modifying model architecture, and the other that is more plug-and-play.

\subsection{Model Architecture} 

Two types of approaches have emerged for altering model architecture. First, some approaches modify the inner workings of the Transformer while retaining its overall structure. For example, Muti-Query Attention (MQA)~\cite{shazeer2019fast} and Group-Query Attention (GQA)~\cite{ainslie2023gqa} reduce the number of KV heads, achieving similar accuracy to full-head configurations. Second, some efforts discard the Transformer architecture entirely in favor of alternative models. Approaches such as Linear Attention and RNN-based models, including RWKV~\cite{peng2023rwkvreinventingrnnstransformer}, RetNet~\cite{sun2023retentivenetworksuccessortransformer}, and Mamba~\cite{gu2024mambalineartimesequencemodeling}, offer lower computational and memory costs. However, these models typically underperform compared to Transformer-based models in long-context scenarios. 

\subsection{KV Compression}

The change of model architecture often requires significant pretraining or fine-tuning, whereas plug-and-play approaches, such as KV compression, are typically preferred in major application scenarios. Two primary types of KV compression have emerged: KV quantization and KV pruning.

\textbf{KV Quantization.} KV quantization approaches~\cite{xiao2024smoothquantaccurateefficientposttraining, yao2022zeroquantefficientaffordableposttraining, dettmers2022gpt3} map higher precision KVs into lower ones, trading accuracy for savings in computation and memory. Recent studies have shown that due to the distinct element distributions in the KV cache, key and value caches require different quantization strategies to optimize performance on complex tasks~\cite{kivi}.

\textbf{KV Pruning.} KV pruning approaches focus on leveraging attention sparsity~\cite{zhang2023h2o, ge2023model, jiang2024minference, cai2024pyramidkv, fu2024lazyllm, xiao2024duoattention}, which posits that only around 5\% of tokens are crucial during LLM inference. Thus, evicting less important tokens from the KV cache is a key strategy for reducing memory usage and accelerating inference. For example, StreamingLLM~\cite{xiao2023sink} and LM-Infinite~\cite{han2024lm} evict fixed-position tokens, retaining only the initial and recent window tokens. H2O~\cite{zhang2023h2o}, SnapKV~\cite{li2024snapkv}, ScissorHands~\cite{liu2024scissorhands} and TOVA~\cite{oren2024transformers} keeps the recent tokens and the top-k important tokens based on the attention score calculated within a local window. More recent works, such as Quest~\cite{tang2024quest} and ARKVALE~\cite{chenarkvale}, manage the KV cache at the page level, selecting the top-k important pages during each generation step to reduce computational time.

Our work presents a new trial of applying KV pruning in reasoning tasks, where the inference pattern is characterized by a short prefill and a long decode with a waterfall attention pattern. For the first time, it achieves true $O(L)$ time and memory complexity with high accuracy.


% Hybrid approaches such as FastGen~\cite{ge2023model} and MInference~\cite{jiang2024minference} adapt to different attention patterns, selecting tokens to preserve based on context. Layer-wise approaches, such as PyramidKV~\cite{cai2024pyramidkv} and LazyLLM~\cite{fu2024lazyllm}, adjust KV cache sizes dynamically across layers based on different sparsity patterns within each layer. DuoAttention~\cite{xiao2024duoattention} categorizes attention heads into Retrieval and Streaming Heads, applying distinct attention patterns for each.
