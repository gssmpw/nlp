% ICCV 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{iccv}              % To produce the CAMERA-READY version
% \usepackage[review]{iccv}      % To produce the REVIEW version
\usepackage[pagenumbers]{iccv} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{iccvblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=iccvblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{} % *** Enter the Paper ID here
\def\confName{ICCV}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Image Intrinsic Scale Assessment:\\Bridging the Gap Between Quality and Resolution}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{Vlad Hosu\textsuperscript{1,*} \and Lorenzo Agnolucci\textsuperscript{2,$\dagger$,*} \and  Daisuke Iso\textsuperscript{1} \and Dietmar Saupe\textsuperscript{3} \and
\textsuperscript{1} Sony AI - \tt\small[name.surname]@sony.com \\  \textsuperscript{2} University of Florence, Italy - \tt\small[name.surname]@unifi.it \\ \textsuperscript{3} University of Konstanz, Germany - \tt\small[name.surname]@uni-konstanz.de
}

\begin{document}
\maketitle

\begin{abstract}
Image Quality Assessment (IQA) measures and predicts perceived image quality by human observers. Although recent studies have highlighted the critical influence that variations in the scale of an image have on its perceived quality, this relationship has not been systematically quantified.
To bridge this gap, we introduce the Image Intrinsic Scale (IIS), defined as the largest scale where an image exhibits its highest perceived quality. We also present the Image Intrinsic Scale Assessment (IISA) task, which involves subjectively measuring and predicting the IIS based on human judgments. We develop a subjective annotation methodology and create the \dataset dataset, comprising 785 image-IIS pairs annotated by experts in a rigorously controlled crowdsourcing study with verified reliability. Furthermore, we propose \method (\methodextended), a strategy that leverages how the IIS of an image varies with downscaling to generate weak labels. Experiments show that applying \method during the training of several IQA methods adapted for IISA consistently improves the performance compared to using only ground-truth labels. We will release the code, dataset, and pre-trained models upon acceptance.
\end{abstract}

\blfootnote{\textsuperscript{$*$}~Equal contribution.} 
\blfootnote{\textsuperscript{$\dagger$}~Work done during an internship at Sony AI.} 

%%%%%%%%% Introduction
\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{figures/banner-3-images-bird-v4.pdf} \\
    \vspace{10pt}
    \includegraphics[width=0.8\columnwidth]{figures/Intrinsic_scale_explanation_v4.pdf}
    \caption{
    \textbf{Top:} downscaling an image affects its perceived quality. As the original image (left) is downscaled, degradation becomes less noticeable, but some high-frequency details may be lost. The optimal trade-off occurs at the image's intrinsic scale. Note that the first two images are cropped for clearer visualization.
    \textbf{Bottom:} we plot several possible profiles corresponding to different images, depicting how the quality changes with scale -- quality refers to the latent expected value of measured subjective quality ratings rather than the noisy mean observed from finite samples. The Image Intrinsic Scale (IIS) is the largest scale at which an image shows its highest perceived quality. The IIS is measured above a lower bound $s_{lb}\!=\!0.05$, where the image quality is more interpretable.
    }
    \label{fig:teaser}
\end{figure}

\section{Introduction} \label{sec:intro}
Image Quality Assessment (IQA) aims to model and quantify subjectively how the degradation of an image, such as that introduced by poor camera performance or image processing, affects its perceived quality. Particularly, Full-Reference IQA (FR-IQA) evaluates quality by comparing a pristine reference image with its degraded counterpart, while No-Reference IQA (NR-IQA) assesses quality without needing a reference image. Despite significant advancements in IQA, the interaction between image quality and spatial resolution remains a critical under-explored area. While the resolution, represented by Megapixels, is often seen as a key indicator of image quality in cameras, it does not account for the degradations introduced by the camera system, such as lens diffraction or increased noise due to smaller pixel size in higher resolution sensors. Paradoxically, higher resolution acquisition can lead to lower perceived quality. Thus, further research is needed to explore how resolution impacts image quality perception.

Most IQA datasets annotate quality ratings -- in the form of Mean Opinion Scores (MOS) -- for images at a fixed resolution \cite{hosu2020koniq, hosu2024uhd, fang2020perceptual}. To the best of our knowledge, KonX \cite{wiedemann2023konx} was the first dataset that provided quality annotations for each image rescaled across multiple resolutions. The authors showed that perceived quality varies with image resolution (\ie, size in pixels).
When an image is downscaled, its resolution decreases. Since images are typically displayed at a consistent pixel density on any given display medium, downscaled images also appear physically smaller. The perceived quality of smaller images often increases because degradation becomes less noticeable, e.g., noise grain becomes smaller or fades. Conversely, relevant high-frequency details may get lost when the resolution is too low, leading to relatively lower perceived quality. The top section of \cref{fig:teaser} shows an example of the former phenomenon. The image at the original size ($scale = 1$) appears blurry and degraded. However, when downscaled ($scale = 0.25$), the degradations are not noticeable, improving quality, while relevant details, such as the bird's feathers and barbs, are crisp and clear. At smaller scales ($scale = 0.1$), the individual barbs cannot be resolved anymore, and detail is lost.

Based on the earlier observation, we aim to answer the question: 
\textit{what is the optimal scale of an image that provides the best balance between visibility of degradation and retention of relevant high-frequency details?} Thus, we introduce the concept of \textbf{Image Intrinsic Scale} \textbf{(IIS)}, which is the largest scale at which an image shows its highest perceived quality. We select the largest scale as it maximizes the availability of high-quality information. Moreover, we propose a new task named Image Intrinsic Scale Assessment (IISA). IISA aims to subjectively measure and predict the IIS in alignment with human perception. Our main goal is to develop effective automated methods for predicting the IIS, and this first requires establishing a reliable annotation methodology to collect subjective opinions.

Subjective studies measure latent variables that are not directly observable -- here, the perceived quality or IIS. These latent variables are commonly modeled as random variables, with each human opinion representing a sample. The theoretical “true” value is the expectation of this random variable. In principle, when we refer to image quality or IIS, we discuss the expected value, which can be estimated by averaging many independent and unbiased opinions, \eg, via the MOS. In practice, the number of opinions is limited by cost and other constraints, so we work with approximations that depend on careful experimental design to minimize bias and achieve reliable estimates.

To collect ground-truth IIS labels, we propose an annotation methodology that leverages a tailored user interface that we developed. Our annotation tool allows the participants to intuitively downscale the original image via a slider until it reaches its IIS. We ask every annotator to label each image twice, a few days apart, ensuring excellent self-consistency and reliability. The final ground-truth IIS labels are obtained by aggregating each participant's individual subjective opinions, similar to the process of calculating the MOS for IQA. Based on the proposed annotation methodology, we create the \dataset dataset, which contains 785 image-IIS pairs labeled via crowdsourcing. We select freelance image quality experts as participants to enhance the reliability of ground-truth IIS labels.

To automatically predict the IIS, we initially assess the zero-shot transfer capabilities of pre-trained NR-IQA models. The subpar performance of these models on IISA underscores the distinct nature of the two tasks and the need for IISA-specific training.
To this end, we propose \method (\methodextended), a straightforward yet effective approach for generating multiple weakly labeled image-IIS pairs from a single ground-truth one. \method leverages a unique property of the intrinsic scale that allows us the extrapolate the ground-truth IIS of an image to its downscaled versions via a piece-wise function. Our method can be easily integrated into the training of any model, as it is independent of specific training strategies or model characteristics. The experiments demonstrate that the proposed approach consistently boosts the performance of several methods from the NR-IQA domain retrained for IISA on the proposed dataset. 


We summarize our contributions as follows:
\begin{itemize}
    \item We define the concept of Image Intrinsic Scale (IIS), which is the largest scale at which an image shows its highest perceived quality.
    \item We propose a new task, the Image Intrinsic Scale Assessment (IISA), aiming to subjectively measure and predict the IIS in alignment with human perception.
    \item We design an annotation methodology for collecting subjective opinions of the IIS that ensures high participant self-consistency and reliability of the labels. Moreover, we create the \dataset dataset, comprising 785 image-IIS pairs annotated by image quality experts in a thoroughly controlled and reliable crowdsourcing study.
    \item We propose \method, an approach to generate multiple weakly labeled image-IIS pairs from a single ground-truth pair. 
\end{itemize}


%%%%%%%%% Related work
\section{Related Work}

Prior research has examined concepts related to the IIS, namely camera performance metrics, image quality under upscaling, the relationship between quality and viewing distance, and no-reference image quality assessment.

\paragraph{Camera performance metrics} The image intrinsic scale is fundamentally related to a physical property of camera and lens systems: their resolving power or ability to discern details in an image. This property is theoretically upper bounded by the camera's sampling resolution. However, in practice, the resolving power is often lower due to imperfections in the lenses and camera system. Traditionally, these properties have been studied for the camera as a whole using metrics such as Perceptual Megapixels (P-MP) \cite{dxomark2012perceptual} and the closely related Modulation Transfer Function (MTF) \cite{boreman2001}, leading to the concept of ``intrinsic camera resolution'' \cite{burns2015intrinsic}. However, because previous approaches characterize cameras under ideal laboratory conditions, they do not generalize well to real-world photos, which are affected by a multitude of unaccounted degradations. Therefore, for the first time, we propose to study the resolving power conditioned on individual images by subjectively measuring the IIS. This approach allows our method to generalize effectively to real-world photographs.

% Authentic vs. Fake
\paragraph{Quality under upscaling} Another line of research has focused on the effects of upscaling on image quality.
Works such as \cite{shah2021real, zhu2021perceptual} aim to discriminate between authentic and artificially upscaled images. While valuable for detecting upscaling artifacts, such a binary classification does not offer a generalizable image quality metric.
\citet{kansy2023self} introduce the concept of `effective resolution", \ie, the smallest size an image can be downscaled to, such that when it is upscaled back to its original dimensions the loss of detail is not perceptible.
This is distinct from the IIS. Consider the example of a noisy image: the scale of its effective resolution should be \textit{high} enough to preserve the noise pattern such that upscaling will not lead to noticeable detail loss relative to the original. In contrast, the IIS of the image should be \textit{low} enough to make the noise not visible anymore, improving the perceived quality of the image.

% Viewing distance
\paragraph{Quality and viewing distance} Several works studied the impact of viewing distance on the perceived quality of images \cite{liu2014cid, gu2015quality, fang2015evaluation} and videos \cite{hammou2024effect, keller2023influence, kufa2019visual}. \citet{fang2015evaluation} collected subjective opinions on image Quality of Experience (QoE) across seven viewing distances, showing that QoE generally increases with distance due to reduced visibility of distortions but declines when image details become less discernible at greater distances. This results in a concave-down relationship between QoE and viewing distance, aligning with findings by the authors of the KonX dataset \cite{wiedemann2023konx} on the quality-resolution relationship. Based on these insights, in \cref{sec:iisa} we establish a rule to extrapolate the IIS of an image to its downscaled versions.

\paragraph{No-Reference Image Quality Assessment}
Although the IIS differs significantly from traditional quality ratings, it is closely connected through the underlying subjective judgments. The IIS is derived from the quality assessments of all downscaled versions of an image. Consequently, No-Reference Image Quality Assessment (NR-IQA) methods that predict quality ratings from single images are related.

In recent years, NR-IQA has attracted significant attention, particularly in the development of supervised methods \cite{su2020blindly, ke2021musiq, chen2024topiq, golestaneh2022no, wiedemann2023konx}. A notable trend within these approaches is the usage of multi-scale representations \cite{ke2021musiq, wiedemann2023konx, chen2024topiq}, which have proven effective in enhancing cross-resolution generalization. For instance, TOPIQ \cite{chen2024topiq} employs an attention-based network to extract top-down multi-scale features while propagating high-level semantic information.
Another line of research leverages self-supervised learning to train an image encoder on unlabeled data, followed by regressing the encoder’s features to predict quality scores \cite{madhusudana2022image, agnolucci2024arniqa, zhao2023quality, saha2023re}. For example, CONTRIQUE \cite{madhusudana2022image} involves maximizing the similarity between the representations of crops coming from the same distorted image via a contrastive loss.
More recent works \cite{wang2023exploring, agnolucci2024quality, zhang2023blind} leverage vision-language models like CLIP \cite{radford2021learning} to measure the quality of an image based on its similarity to antonym prompts.


%%%%%%%%% Task
\section{Image Intrinsic Scale Assessment} \label{sec:iisa}

As illustrated in the bottom section of \cref{fig:teaser} and supported by previous studies \cite{wiedemann2023konx, hammou2024effect, fang2015evaluation, liu2014cid, gu2015quality}, the relationship between the perceived quality of an image and its presentation scale -- defined as the scale at which it is displayed for subjective annotation -- is generally assumed to follow a concave-down or monotonic function for scales smaller than 1 (\ie, the original size). When an image is downscaled, quality initially improves as degradation like blur and noise decrease. However, downsizing past a certain point reduces perceived quality primarily due to loss of detail and interpolation artifacts like aliasing. 

To quantify the relationship between quality and scale, we introduce the IIS, \ie, the largest scale at which an image shows its highest perceived quality. The IIS stems from subjective evaluations of the quality of an image's downscaled versions. Therefore, analogous to the MOS in IQA, the ground-truth IIS labels are determined by computing a scalar aggregate of the individual subjective judgments provided by multiple annotators. Based on the concept of IIS, we propose the IISA task, which has two main aspects:
\begin{enumerate}
    \item \textit{Subjective}: subjectively measuring the IIS (see \cref{sec:dataset});
    \item \textit{Predictive}: developing automatic methods for predicting the IIS in alignment with human judgments (see \cref{sec:approach}).
\end{enumerate}
%
Even though the annotation study setup involves presenting a single image at a time, the IIS is determined by comparing the quality of rescaled versions of that image. Thus, the subjective side of IISA has both a no-reference and partial-reference nature (across scales). Nonetheless, the predictive side of IISA is a no-reference task, as it depends on a single input image for automatic IIS prediction.

Formally, let $I$ be an image. Denote $I^s$ as the image obtained by downscaling $I$ to a scale $s$, with $0 < s \leq 1$. Let $Q(I^s)$ be the quality of $I^s$, measurable by the MOS. Then, we define the IIS of $I$ as $\Omega(I)$, which gives the maximum scale at which $I$ shows its highest perceived quality:
%
\begin{equation} \label{eq:iis_definition}
\begin{gathered}
    \Omega(I) = \max( \argmax_{s_{lb} \leq s \leq 1}( Q(I^s) ) ) .
\end{gathered}
\end{equation}
%
In our implementation, we set a lower bound $s_{lb}=0.05$ for the scale $s$ because reliably evaluating the quality of very small images is challenging due to the inherent loss of detail from downscaling. 
When assessing image quality, slight variations in size typically result in minimal quality change. Therefore, an image might be perceived as having its highest quality at multiple scales. Thus, we take the maximum value among these scales in \cref{eq:iis_definition}, as it maximizes the presence of high-quality information.

When downscaling images, different interpolation methods introduce artifacts like aliasing, blurring, or ringing, which affect the IIS. To achieve the highest quality, we use Lanczos interpolation due to its ability to preserve fine details and minimize aliasing through sinc-based filtering, resulting in higher image quality compared to bilinear or bicubic methods. Consequently, we employ this method for both the subjective and predictive aspects of IISA.

Given the ground-truth IIS of an image, we can easily extrapolate it to its downscaled versions by applying the definition in \cref{eq:iis_definition}. To do so, we start from the assumption that the quality of downscaled images $Q(I^s)$ is a concave-down or monotonic function of scale, thus:
%
\begin{enumerate}[label=\textit{A\arabic*:}, left=1em]
    \item $Q(I^s)$ increases monotonically for $s \leq \Omega(I)$;
    \item $Q(I^s)$ decreases monotonically for $s > \Omega(I)$. 
\end{enumerate}
%
Then, starting from the ground-truth IIS $\Omega(I)$ of an image $I$, we can infer the IIS $\overline{\Omega}(I^s)$ of the downscaled versions of $I$ via a piece-wise function:
%
\begin{equation} \label{eq:iis_formula}
    \overline{\Omega}(I^{s}) = \begin{cases} 
      1 & s_{lb} \leq s \leq \Omega(I) \\
      \frac{\Omega(I)}{s} & \Omega(I) < s \\
   \end{cases} .
\end{equation} 
%
From now on, we use $\Omega$ to refer to the ground-truth IIS, while with $\overline{\Omega}$ we indicate intrinsic scales obtained via \cref{eq:iis_formula}. We can consider the IIS as the minimum amount we need to downscale an image to maximize its quality. Therefore, starting from a downscaled image $I^s$ and seeking its IIS, \cref{eq:iis_formula} can be interpreted as follows:
\begin{itemize}[leftmargin=*]
    \item First branch: when the initial scale $s$ is below or at $\Omega(I)$, due to the assumption \textit{A1}, downscaling $I^s$ further cannot increase its quality, as it is already at its maximum. Therefore, by the definition of the intrinsic scale (\cref{eq:iis_definition}), the IIS of $I^s$ is its original scale, \ie, 1, so $\overline{\Omega}(I^s)=1$;
    \item Second branch: when the initial scale $s$ of $I^s$ is above $\Omega(I)$, due to the assumption \textit{A2}, the quality is at its highest at the scale $\Omega(I)$. Hence, the amount of downscaling required to bring $I^s$ to its IIS is given by the ratio of the original intrinsic scale and the scale $s$, \ie, $\overline{\Omega}(I^s)=\frac{\Omega(I)}{s}$.
\end{itemize}
\cref{fig:iis_formula} shows an example of the application of \cref{eq:iis_formula} to the downscaled versions $I^s$ of a generic image $I$.

Therefore, based on \cref{eq:iis_formula}, we can infer the IIS of the downscaled versions of a labeled image. This idea underlies the proposed approach, named \method, which generates multiple weak labels from a single image-IIS pair. The experimental results, reported in \cref{sec:experimental_results}, show that this strategy significantly improves predictive performance.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\columnwidth]{figures/iis_formula_v6.png} 
    \vspace{-5pt}
    \caption{
    Example of the influence of the scaling factor $s$ on the quality $Q(I^s)$ of downscaled images $I^s$ and the intrinsic scale $\overline{\Omega}(I^s)$, based on \cref{eq:iis_formula}. The quality is not evaluated below the lower bound $s_{lb}$. $\Omega(I)$ is the scale where the quality function is at its maximum, \ie, the IIS of the original image $I$ ($s=1$).
    }
    \vspace{-10pt}
    \label{fig:iis_formula}
\end{figure}

\paragraph{Differences with NR-IQA}
IISA differs from NR-IQA in several key aspects. First, IISA offers a concrete way to maximize an image's perceived quality by simply downscaling it to its intrinsic scale, while NR-IQA only provides information on an image's quality at a fixed scale.
Second, compared to NR-IQA, IISA is affected less by global image degradation, such as overall contrast and color defects. The downscaling operation mainly affects spatially limited defects such as blurs, compression artifacts, and noise. 
Third, NR-IQA lacks a definitive model for how image quality changes with scale. While studies like \cite{hammou2024effect} explore the relationship between perceived quality and viewing distance, accurately understanding this change requires annotating an image (\eg, MOS) at multiple scales. The KonX dataset \cite{wiedemann2023konx} provides ratings for three scales, but fine-grained sampling across all scales is impractical. On the contrary, for IISA, we can easily extrapolate the ground-truth IIS of an image to its downscaled versions by using \cref{eq:iis_formula}. 
Fourth, quality ratings use a perceptually linear scale \cite{devellis2021scale}, whereas rescaling factors -- underlying the IIS -- are inherently non-linear. For example, despite the same absolute difference, changing the scale from 0.2 to 0.1 halves the image size, whereas decreasing it from 0.6 to 0.5 results in only a 17\% drop.
Fifth, the quality MOS in NR-IQA struggle with limited sensitivity, particularly when evaluating subtle quality differences in high-quality images resulting from restoration, enhancement, or compression methods. This has led the JPEG standards committee to explore methodologies capable of more granular quality assessment \cite{testolina2023jpeg}. Our study demonstrates that the IISA significantly outperforms traditional NR-IQA quality ratings in detecting subtle differences, exhibiting several times greater sensitivity. See the supplementary for more details.

\paragraph{Applications:} IISA addresses applications where understanding scale-dependent quality is critical:
1) it optimizes image display and storage by identifying an image's peak quality across scales, crucial for applications like the web, print, and gaming. This enhances user experience and saves storage or printing space by using the display media's pixel density (\eg, PPI) to determine optimal display sizes.
2) IISA improves quality measurement sensitivity over traditional NR-IQA methods through implicit cross-scale comparative judgments, making it effective for detecting subtle quality degradations from restoration methods like denoising and super-resolution. See the supplementary material for more details on sensitivity.
3) IISA supports the creation of better low-level vision datasets. Unlike current methods that reduce diversity by discarding low-quality images, by downscaling high-resolution images to their IIS, IISA can help maintain both quality and diversity, essential for diverse and high-quality training datasets \cite{gu2019div8k, li2023lsdir}. See the supplementary for more details on the applications of IISA.


%%%%%%%%% Dataset
\section{\dataset Dataset Creation} \label{sec:dataset}

We address the subjective task of IISA, defining an annotation methodology to measure the IIS subjectively and collect ground-truth annotations, which results in our \dataset dataset. Refer to the supplementary material for more details about the dataset.

\subsection{Dataset collection}
The subjective side of IISA resembles the image Just Noticeable Difference (JND) task \cite{jin2016statistical, shen2020jnd, lin2022large}, which identifies the minimal distortion level that becomes noticeable with increased image compression. Similarly, in IISA, the goal is to find the lowest downscaling factor that maximizes the perceived quality. The analogy with JND inspired the design of our annotation tool.

\paragraph{Annotation tool} 
We developed Zoom Viewer (ZOVI), a web application for annotating the IIS of images and facilitating web-based crowdsourcing experiments through batch image presentation. ZOVI features a slider for users to adjust the image scale from $s=1$ to $s_{lb}=0.05$ to identify the IIS. This slider method, inspired by JND research \cite{lin2022large}, reduces annotation effort and enhances reliability. The images are rescaled using the standard Lanczos interpolation and can be panned if the display area is insufficient. ZOVI displays each image starting at its original resolution ($scale = 1$), each image pixel being displayed at the same size as native screen pixels (1-to-1 ratio). Participants were instructed to downscale each image until no quality improvement was observed and to stop at the largest scale where this held true, thereby determining the individual IIS.

\paragraph{Annotation process}
We collected 20 opinions per image from 10 freelance participants, meeting the minimum of 15 per stimulus recommended by \cite{RecommendationITUT913} for quality assessment. Given the satisfactory confidence intervals reported in \cref{sec:dataset_analysis}, the number of participants is adequate. This aligns with similar IQA studies like SPAQ \cite{fang2020perceptual} and UHD-IQA \cite{hosu2024uhd}, which gather 15 and 20 opinions per image, respectively. All freelancers had previous experience annotating IQA datasets and a professional background in visual arts, such as photography and graphic design. Before starting the main task, participants had to undergo a training phase. Specifically, they annotated images with predefined ground truth IIS ranges determined by the authors of this paper, with textual hints provided to guide accurate annotation. After the training stage, participants annotated images presented in randomly ordered batches of 90. Each batch was viewed twice, with a few days between repetitions. We measured Spearman's Rank-order Correlation Coefficient (SRCC) between the annotations of repetitions of corresponding batches for each participant. Pairs of batches with an SRCC lower than 0.5 were required to be re-annotated. This process ensures accurate individual annotations and an overall high experiment reliability.

\paragraph{Image selection}
Following our annotation strategy, we created the \dataset dataset, comprising 785 image-IIS pairs labeled at their original resolution via crowdsourcing. All images are larger than 2048 pixels in width. We started from an initial set of 900 authentically distorted real-world images. We sampled 300 of them from the KonIQ-10k \cite{hosu2020koniq} IQA dataset according to their MOS and machine tags to ensure a diverse range of quality levels and content types. The remaining 600 images were sourced from the \textit{Pixabay} website and selected for diversity based on multiple factors, such as user tags and number of likes. 
Finally, to address privacy concerns, we manually removed any images containing identifiable individuals, resulting in the final set of 785 images. 
As a result of the sampling procedure, our dataset contains images with diverse subjects, quality levels, resolutions, and consequently, IIS values.

\paragraph{Ground-truth labels}
We obtained the ground truth IIS labels for each image (\ie $\Omega(I)$) by aggregating individual annotations from all participants, similar to how Mean Opinion Scores (MOS) are calculated for image quality ratings. Therefore, we refer to these aggregated labels as the Mean Opinion Intrinsic Scale (MOIS). This aggregation effectively reduces the variability in perceived quality due to the diverse viewing conditions inherent in crowdsourcing. Considering the non-linear relationship between scale values in our slider UI -- where different lengths on the slider represent equal ratios between two scale values -- we chose the geometric mean to compute the MOIS. 

\subsection{Dataset Analysis} \label{sec:dataset_analysis}

\paragraph{Reliability of the annotations}
To assess the reliability of the subjective annotations of \dataset, we compute the confidence intervals (CIs) for the MOIS of each image. Following a well-established methodology, we sample 20 opinions with replacement for each image, compute their geometric mean, and repeat the process 100 times. The 95\% CI is then estimated as half the range between the $2.5^{\text{th}}$ and $97.5^{\text{th}}$ percentiles of the simulated geometric means. This yields an average CI of $0.057$ for our dataset. As a reference, we consider the KonX NR-IQA dataset, whose annotation conditions were similar to ours, involving remote studies with freelance participants. The average CI of the MOS for the KonX dataset is $0.046$. Given the differences between the IISA and NR-IQA tasks, some variation in the CIs is expected. Therefore, this outcome indicates that our annotation process has a level of reliability comparable to a highly reliable NR-IQA dataset such as KonX. 


\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/intergroup_agreement_srcc_rmsd.pdf}
    \vspace{-15pt}
    \caption{Comparison of inter-group agreement between the proposed \dataset and existing NR-IQA datasets, across different numbers of annotations per image in each group. The dots represent the average values, while the error bars indicate the range of $\pm$1 standard deviation.
    }
    \vspace{-10pt}
    \label{fig:intergroup_agreement}
\end{figure}

We further assess the reliability of our annotations using a common approach that measures the inter-group agreement between different participant sub-groups and compares it with that of existing NR-IQA datasets \cite{hosu2024uhd, wiedemann2023konx, hosu2020koniq}. Specifically, for datasets where each participant annotated each image twice (\dataset, KonX \cite{wiedemann2023konx}, UHD-IQA \cite{hosu2024uhd}), we randomly sample without replacement pairs of non-overlapping groups of up to five participants each, thus considering at most ten annotations per image. For KonIQ-10k \cite{hosu2020koniq}, since participants provided just a single opinion per image and each rated only a subset of images, we sample the required number of opinions per group without considering participant identity. For each dataset, we sample 200 pairs of groups and compute the average SRCC and RMSD (Root Mean Squared Difference) between the MOIS (IISA) or MOS (NR-IQA) of each group. Note that although both IIS and quality ratings are scaled to the $[0,1]$ range, their absolute errors are not directly comparable due to the different nature of their scales -- non-linear and linear, respectively. Therefore, SRCC is more appropriate than RMSD in comparing inter-group agreements. Nonetheless, we report both for completeness in \cref{fig:intergroup_agreement}. We observe that the proposed \dataset dataset has SRCC values that are comparable to existing reliable NR-IQA datasets, such as KonIQ-10k. The same applies to the RMSD. Overall, these results demonstrate the reliability of our dataset's annotations.

\paragraph{Effects of variable viewing conditions}
Viewing conditions -- including display characteristics (contrast, brightness), environmental lighting, viewing distance, and viewer acuity -- significantly influence perceived image quality. Laboratory experiments tightly control these factors but consequently measure atypical conditions with limited variability, restricting the applicability of the findings. In contrast, crowdsourcing involves diverse and less controlled conditions, potentially reducing individual participant reliability. Nevertheless, when reliable group opinions are obtained through well-controlled methods, the resulting assessments generalize better. Our annotation framework has demonstrated high reliability, ensuring that despite individual variation, group aggregates accurately measure the perceptual IIS. Thus, we provide highly reliable data that better reflects real-world viewing conditions, enhancing the validity and applicability of our findings.

\input{tables/iccv_naive_results}

%%%%%%%%% Method
\section{IISA Predictive Models} \label{sec:approach}

In this section, we focus on the prediction task of IISA, \ie, developing methods to automatically predict the IIS in alignment with human perception. First, we evaluate the zero-shot transfer performance of pre-trained NR-IQA models on our dataset. We then introduce \method, an approach for generating weak IIS labels that can be applied during the training of any IISA model.

\subsection{Pre-trained NR-IQA Models} 
The IIS stems from the quality ratings across all the downscaled versions of an image and mostly depends on low-level degradation -- such as blur or noise -- rather than on aesthetics or semantics. These reasons make IISA and NR-IQA closely related, despite their several key differences (see \cref{sec:iisa}). Moreover, NR-IQA methods assess image quality by evaluating both the distortions and the high-quality details present in images, making them suitable for IISA. Therefore, we evaluate the performance of pre-trained NR-IQA models for zero-shot IIS prediction.

We consider TOPIQ \cite{chen2024topiq}, a state-of-the-art NR-IQA approach with strong generalization capabilities. We pre-train it on the SPAQ \cite{fang2020perceptual} and UHD-IQA \cite{hosu2024uhd} NR-IQA datasets, which feature high-resolution images, similar to \dataset. We avoid using KonIQ-10k \cite{hosu2020koniq} due to image overlaps with our dataset. We then apply the pre-trained TOPIQ models to predict quality scores for \dataset, treating such scores as IIS estimates since both metrics range in $[0,1]$. We follow the evaluation protocol described in \cref{sec:evaluation_protocol} and present the results in \cref{tab:naive_results}. Both models achieve unsatisfactory performance, highlighting the inefficacy of pre-trained NR-IQA models for zero-shot IIS prediction. This emphasizes the need for IISA-specific training and techniques.

\subsection{Proposed Approach: \method}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/Pseudo_labels_generation_v8.pdf} 
    \caption{Overview of \method, the proposed weak-label generation strategy. Given a ground-truth image-IIS pair ($I_1$, $\Omega(I_1)$) (blue dotted line), we randomly sample multiple scale values. Then, we divide the ground-truth label $\Omega(I_1)$ by the random scales (following \cref{eq:iis_formula}) to get the weak IIS labels associated with the downscaled versions of the original image $I_1$ (red dotted line).}
    \label{fig:pseudo_labels_generation}
\end{figure}


Given an image and its labeled IIS, we can easily leverage \cref{eq:iis_formula} to extrapolate the IIS to rescaled image versions. Based on this idea, we propose a weak-label generation approach named \method (\methodextended). In particular, starting from each labeled image-IIS pair, we randomly downscale the image to multiple scales. Then, we use \cref{eq:iis_formula} to compute the IIS associated with each downscaled version of the original image to obtain a weakly labeled image-IIS pair. Our approach significantly enhances the value of each annotation by generating additional training data from it. Moreover, since it depends solely on the properties of the IIS, \method can be employed to enhance the training of any IISA model, regardless of its specific characteristics and training strategy.

Formally, let $I_1$ and $\Omega(I_1)$ be an image and its corresponding ground-truth IIS, respectively. We randomly sample $n_{wl}$ scales in the range $[\max(\Omega(I_1), \rescalehyperparam), 1]$, where $\rescalehyperparam$ is a threshold hyperparameter empirically set to 0.65. In our experiments, we use ${n}_{wl}=2$ unless stated otherwise. Since the scales are larger than $\Omega(I_1)$, they fall in the second branch of \cref{eq:iis_formula}. Then, we downscale $I_1$ based on each of these scales using Lanczos interpolation. Therefore, given $N = 1 + n_{wl}$ and the set of randomly sampled scales $\mathcal{S}=\{s_i \mid i=2,\dots,N \} $, we get a corresponding set of downscaled versions of the same image $\mathcal{I}=\{I_i \mid i=2,\dots,N \}$. Finally, we leverage \cref{eq:iis_formula} to obtain the corresponding set of IIS $\overline{\mathcal{S}_{\Omega}}=\{\overline{\Omega}(I_i) \mid i=2,\dots,N \}$. Thus, given $i=2,\dots,N$, $I_i\in\mathcal{I}$ represents the image obtained by rescaling $I_1$ to a scale $s_i \in \mathcal{S}$ (\ie, $I_{1}^{s_{i}}$), with an associated IIS of $\overline{\Omega}(I_i) \in \overline{\mathcal{S}_{\Omega}}$. In the end, starting from a single ground truth image-IIS pair ($I_1$, $\Omega(I_1)$), thanks to our approach, we get $n_{wl}$ weakly labeled pairs ($I_i$, $\overline{\Omega}(I_i)$), with $i=2,\dots,N$. \cref{fig:pseudo_labels_generation} shows an overview of the proposed strategy. During training, we use \method to augment each batch of size $B$ with additional $B \cdot n_{wl}$ weakly labeled training samples. Since our approach only requires image downscaling to generate each weak label, it can be easily applied online during training.


%%%%%%%%% Experiments
\section{Experimental Results} \label{sec:experimental_results}

\subsection{Evaluation Protocol} \label{sec:evaluation_protocol}
We measure the performance using Spearman's Rank-order Correlation Coefficient (SRCC), Pearson's Linear Correlation Coefficient (PLCC), Root Mean Square Error (RMSE), and Mean Absolute Error (MAE). Higher SRCC and PLCC values and lower RMSE and MAE values indicate better results. We randomly divide the proposed \dataset dataset into 70\%, 10\%, and 20\% splits corresponding to training, validation, and test sets, respectively. We cross-validate 10 times to reduce bias and report the median test performance. 

\subsection{Results}

\input{tables/iccv_main_results}


We integrate the proposed approach with several state-of-the-art NR-IQA methods retrained for IISA. For a comprehensive analysis, we consider baselines based on diverse strategies. Specifically, we consider approaches that train a network from scratch with supervised learning (DBCNN \cite{zhang2018blind} and TOPIQ \cite{chen2024topiq}), methods that train a regressor head on top of an encoder pre-trained with self-supervised learning (CONTRIQUE \cite{madhusudana2022image} and ARNIQA \cite{agnolucci2024arniqa}), and VLM-based models that rely on prompt learning \cite{wang2023exploring} (CLIP-IQA$^{+}$ \cite{wang2023exploring} and QualiCLIP \cite{agnolucci2024quality}). For each baseline, we train two variants on the \dataset dataset: a base one using only the ground-truth labels, and one also leveraging the weak labels generated with \method. 

We report the results in \cref{tab:main_results}. We observe that our approach consistently improves the results, with relative performance gains up to 5\%. This proves that \method can be applied to any model, regardless of its specific characteristics, as a way to embed domain knowledge about the IISA task into the training process. Moreover, these results confirm that \cref{eq:iis_formula} can be leveraged in practice to enhance the value of every ground-truth IIS label  
by generating additional weakly labeled training data from it. Finally, we observe that, when combined with \method, TOPIQ achieves the best performance among the considered methods.

\subsection{Discussion}
\paragraph{Proposed method} We conduct controlled experiments to assess how key components affect our approach's performance, namely: 1) the number of weak labels $n_{wl}$; 2) the downscaling threshold $\rescalehyperparam$; 3) the interpolation algorithm used to generate the weak labels.
For each experiment, we keep other hyper-parameters fixed unless otherwise stated. We consider only the TOPIQ model for its superior performance among the studied approaches.

\cref{tab:ablations} shows the results. Regarding the number of weak labels, the experiments show that increasing the value of $n_{wl}$ leads to better results up to $n_{wl}=2$, but beyond that performance declines. We hypothesize that too many weak labels introduce overly redundant information, as the image content remains constant across them, with only the scale varying. Likewise, a high downscaling threshold leads to the weak labels being overly similar to the ground-truth ones, thus reducing the performance improvement. Conversely, lower $\rescalehyperparam$ values allow the weakly labeled IIS to be higher (see \cref{eq:iis_formula}), possibly resulting in them being out of the ground-truth distribution (the average IIS in \dataset is 0.347). Finally, we observe that using the bilinear and bicubic interpolation algorithms to generate the weak labels leads to marginally lower performance than using the Lanczos one. This finding suggests that the interpolation algorithm has limited impact in training models with \method. 

\input{tables/iccv_ablations}


\paragraph{Assumption} 
In \cref{sec:iisa} we assume that the relationship between image quality and scale follows a concave-down or monotonic function. 
To verify this, we consider the KonX dataset \cite{wiedemann2023konx}, which provides quality annotations for images downscaled to three predefined resolutions. 
We find that 90\% of the image triplets (378 out of 420) show MOS trends across resolutions that are consistent with our assumption. We attribute the instances where the assumption does not hold to potential noise in the annotation process caused by subjective biases from annotators or interpolation artifacts. See the supplementary material for more details. Furthermore, studies that vary viewing distances while keeping image resolution fixed \cite{liu2014cid, gu2015quality, fang2015evaluation} also support our assumption. They show that image quality follows a concave-down function with respect to viewing distance.
In principle, changing the viewing distance at a fixed image resolution is equivalent to changing the resolution (and display size) at a fixed viewing distance. Together, these studies provide compelling evidence supporting the validity of our assumption across a wide range of conditions.


%%%%%%%%% Conclusions
\section{Conclusions}
We defined the Image Intrinsic Scale (IIS) to explore the relationship between image quality and scale and introduced the IISA task. Our work puts forward a subjective annotation methodology for the IIS and creates the first dataset for IISA. The proposed approach leverages an IIS-specific property to generate weak labels. Experiments show that 
\method improves the performance of several NR-IQA models, suggesting its potential for enhancing future IISA approaches.
The IIS complements traditional quality ratings, providing a more comprehensive view of real-world image quality. Notably, the IIS introduces a unique advantage, as it offers a direct method to maximize image quality by downscaling. Since our contributions span all stages of development -- from task definition to predictive modeling -- our work lays a solid foundation for future research on the nuanced interplay between image scale and perceived quality.

\section*{Acknowledgements}
The dataset creation was partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 251654672 – TRR 161.

{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

\input{supplementary_arxiv.tex}

\end{document}
