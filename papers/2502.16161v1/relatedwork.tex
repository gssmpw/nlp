\section{Related works}
\label{sec:rela}
   

    \subsection{Scene Text Spotting} 
    Scene text spotting typically employs a unified end-to-end trainable network, blending text detection and text recognition into a cross-modal assisted paradigm. This integrated approach streamlines text detection and recognition into a singular network. It enables simultaneous localization and identification of text within images, capitalizing on the synergistic relationship between text detection and recognition to augment overall performance. Scene text spotting can be broadly classified into two main categories: regular end-to-end scene text spotting and arbitrarily-shaped end-to-end scene text spotting. Regular end-to-end scene text spotting concentrates on detecting and recognizing text within rectangular or standard-shaped regions, whereas arbitrarily-shaped end-to-end scene text spotting broadens its scope to handle text in irregular or curved shapes.

    \noindent\textbf{Regular End-to-end Scene Text Spotting.}
    Li et al.~\cite{li2017towards} introduced one of the earliest end-to-end trainable scene text spotting methods, which effectively integrated box text detection and recognition features using RoI Pooling~\cite{ren2015faster} within a two-stage framework. Originally designed for horizontal and focused text, their method showed significant performance improvements in an enhanced version~\cite{li2019towards}. Busta et al.~\cite{busta2017deep} also contributed to this area with their end-to-end deep text spotter, which further advanced the integration of detection and recognition. In subsequent developments, He et al.\cite{he2018end} and Liu et al.\cite{liu2018fots} incorporated anchor-free mechanisms to enhance both the training and inference speed. They employed novel sampling strategies, such as Text-Align-Sampling and RoI-Rotate, to extract features from quadrilateral detection results, further refining the end-to-end framework.


    \noindent\textbf{Arbitrarily-shaped End-to-end Scene Text Spotting.} 
    Liao et al.~\cite{lyu2018mask} introduced Mask TextSpotter leveraging Mask R-CNN with character-level supervision to detect and recognize arbitrarily-shaped text. Mask TextSpotterv2~\cite{liao2019mask} reduced the dependence on character-level annotations, improving efficiency. Qin et al.~\cite{qin2019towards} employed RoI Masking to focus attention on arbitrarily-shaped text regions. Feng et al.~\cite{feng2019textdragon} utilized RoISlide for handling long text, whereas Wang et al.~\cite{wang2020all} focused on boundary points detection, text rectification, and recognition. CharNet~\cite{xing2019convolutional} also catered to arbitrarily-shaped text spotting. Segmentation Proposal Network (SPN)~\cite{Liao2020MaskTV} and ABCNet~\cite{Liu2020ABCNetRS} are other noteworthy contributions. ABINet++~\cite{Fang2022ABINetAB} innovatively used a vision model and a language model with an iterative correction mechanism. SwinTextSpotter~\cite{huang2022swintextspotter} used a transformer encoder for detection and recognition. Approaches based on DETR~\cite{Carion2020EndtoEndOD} and variants~\cite{Zhu2020DeformableDD} for RoI-free scene text spotting have also shown promising results. TESTR~\cite{Zhang2022TextST} used an encoder and dual decoders, while TTS~\cite{Kittenplon2022TowardsWT} used a transformer-based approach. SPTS~\cite{Peng2021SPTSST} and variants~\cite{liu2023spts} employed a single point for each instance and used a transformer to predict sequences. DeepSolo~\cite{ye2023deepsolo} allows a single decoder to perform text detection and recognition. ESTextSpotter~\cite{Huang2023ESTextSpotterTB} introduced an explicit synergy-based transformer model with task-aware queries and a vision-language communication module to enhance scene text spotting. oCLIP~\cite{Xue2022LanguageMA} boosts text spotting performance via predefined pretext contrastive learning tasks. TCM~\cite{Yu2023TurningAC} and variants~\cite{Yu2024TurningAC} designed a flexible framework to turn a CLIP~\cite{Radford2021LearningTV} model into a text detector and spotter. BridgeSpotter~\cite{Huang_2024_CVPR} connects a fixed detector and recognizer to retain modularity while improving end-to-end optimization for text spotting. DNTextSpotter~\cite{DNTextSpotter} introduced a novel denoising training method that aligns text content and position using Bezier curve-based positional queries. WeCromCL~\cite{Wu2024WeCromCLWS} proposed a weakly supervised cross-modality contrastive learning framework that detects transcriptions without location annotations by modeling character-wise appearance consistency. InstructOCR~\cite{Duan2024InstructOCRIB} leveraged human language instructions to enhance scene text spotting, improving text interpretation and performance on OCR-related tasks. 

    \subsection{Key Information Extraction} 
Existing KIE approaches can be roughly categorized into two types: OCR-dependent models and OCR-free models. OCR-dependent models focus on using optical character recognition (OCR) for extracting textual information. Early KIE methods primarily built layout-aware or graph-based representations for KIE tasks via sequence labeling with OCR inputs~\cite{liao2022real,shi2016end,da2023multi,xu2020layoutlm, xu2021layoutlmv2, huang2022layoutlmv3, xu2021layoutxlm,li2021structurallm,appalaraju2021docformer,li2021selfdoc,gu2021unidoc,gu2022xylayoutlm,lee2022formnet,peng2022ernie,luo2023geolayoutlm,Yu2020PICKPK}. However, these methods often rely on text with a proper reading order or the use of extra modules~\cite{wang2021layoutreader, zhang2023reading} for OCR serialization, which is not always practical in real-world scenarios where the layout may be complex or unordered.
To address the serialization issue, some methods leverage additional detection or linking modules to model the complex relationships between text blocks or tokens~\cite{hwang2021spatial,xu2021layoutxlm,hong2022bros,yu2022structextv2,luo2023geolayoutlm,yang2023modeling,zhang2023reading,wei2023ppn,Yu2023ICDAR2C}. While these strategies mitigate the reading order problem, the added complexity of decoding and post-processing steps often limits their generalization ability, making them less adaptable to a wide variety of document layouts.
In contrast, generation-based methods~\cite{tang2023unifying,cao2023genkie,cao2022query} are proposed to alleviate the burden of post-processing and task-specific link designs. Another category of OCR-free methods offers an alternative by either utilizing OCR-aware pre-training or by incorporating OCR modules within an end-to-end framework.
For example, Pix2Struct~\cite{Lee2022Pix2StructSP} proposed a pretraining task in which the model generates a complete HTML DOM tree based on a masked webpage screenshot, without relying on OCR.
PreSTU~\cite{Kil2022PreSTUPF} introduced an OCR-aware pre-training objective that directly generates text sequences from image-based inputs. Donut and other Seq2Seq-like methods~\cite{kim2022donut,davis2022end,dhouib2023docparser,cao2023attention} adopted a text reading pre-training objective and generate structured outputs consisting of text and entity tokens.
By explicitly equipping text reading modules, previous work~\cite{wang2021towards,tang2021matchvie,zhang2020trie,kuang2023visual,yu2022structextv2} can maintain high performance in end-to-end KIE tasks with task-specific design.

\subsection{Table Recognition}
Tables, as structured data, provide a succinct and compact format for organizing valuable content. Recent advancements in vision-based approaches have significantly improved the extraction of tables from documents. To provide a comprehensive overview, the task of table extraction from documents is typically divided into three main processes: table detection, table structure recognition, and table content recognition. Table detection, primarily concerned with locating tables within documents or images, has been extensively explored in previous work~\cite{staar2018corpus, zhong2019publaynet}, though it is beyond the scope of this paper. Table structure recognition (TSR) involves identifying the structure of a given table within a document or images, and has long been a focal point in the document understanding community~\cite{gobel2013icdar,gao2019icdar,kayal2021icdar,Raja2020TableSR,liu2021show,GTE}. Table content recognition (TCR) focuses on locating and recognizing text instances within the table cells and can be accomplished using established offline OCR models. In this paper, we concentrate on table recognition (TR) tasks that integrate both table structure recognition and table content recognition. Table recognition methods can be broadly categorized into two groups: non-end-to-end-based~\cite{TableMaster,tableformer,Tsrformer,TRUST,gridformer,VAST} and end-to-end-based~\cite{EDD,ly2023end} approaches. Non-end-to-end-based methods mainly first recover table structure via a specific model and then employ offline OCR models to construct complete HTML sequences via complex post-process. It is worth noting that end-to-end-based table recognition tasks remain less explored due to their complexity and challenging nature. Benefiting from the modularized architecture design, our model effectively separates the extraction of pure table HTML tags with cell text center point sequences and the cell text recognition sequences, accomplishing table recognition in an end-to-end fashion.
    
\subsection{Layout Analysis}
Geometric layout analysis focuses on detecting semantically coherent text blocks as objects~\cite{zhong2019publaynet,long2022towards,hts}. Recent approaches have modeled this task using various techniques, including object detection~\cite{Schreiber2017DeepDeSRTDL}, semantic segmentation~\cite{long2022towards}, and graph-based learning over OCR token structures via Graph Convolutional Networks (GCN). For example, Unified Detector~\cite{long2022towards} utilized segmentation-based formulations to pursue unifying scene text detection and layout analysis through an affinity matrix for modeling grouping relations, but it cannot generate word-level entities and lacks recognition capabilities. Another direction in layout analysis focuses on semantic parsing of documents to extract key-value pairs~\cite{li2021structurallm,long2022towards}. These methods typically leverage language models built on top of OCR outputs. In this paper, we conduct geometric layout analysis for identifying a 3-level hierarchical structure: words, lines, and paragraphs in an end-to-end unified paradigm.

\subsection{Unified Frameworks}
There is an increasing shift towards developing unified frameworks for parsing text-rich images across multiple tasks. Earlier works, such as DocReL~\cite{li2022relational} and BROS~\cite{hong2022bros} model relations between table cells or entities through binary classification or a relational matrix, which also requires an off-the-shelf OCR engine.
StrucTexTv2~\cite{yu2022structextv2} proposed a multi-modal learning framework aimed at document image understanding tasks by constructing self-supervised tasks. Yet, it requires several task-specific designs for downstream tasks, like Cascade R-CNN for table cell detection. Additionally, SeRum~\cite{cao2023attention} converts the end-to-end KIE task into a local decoding process and then shows its effectiveness on text spotting task. SCOB~\cite{kim2023scob} achieves universal text understanding across tasks by using character-wise supervised contrastive learning with online text rendering, effectively bridging domain gaps in document and scene text images with weak supervision. DocRes\cite{Zhang2024DocResAG} introduced a dynamic, task-specific prompt to unify five document image restoration tasks, while UPOCR\cite{Peng2023UPOCRTU} unifies various OCR pixel-level tasks within a single image-to-image transformation framework, utilizing a vision Transformer architecture and task-aware prompts to achieve superior performance in tasks like text removal and tampered text detection. Recent efforts have focused on developing more general, unified parsing frameworks. StrucTexTv3~\cite{Lyu2024StrucTexTv3AE}, integrated a multi-scale visual transformer, a multi-granularity token sampler, and instruction learning, achieving state-of-the-art results in text perception and comprehension tasks. DocOwl1.5~\cite{Hu2024mPLUGDocOwl1U} introduces a unified structure learning framework with H-Reducer to enhance MLLMs for document understanding. GOT~\cite{wei2024general} proposed a unified end-to-end OCR model capable of processing diverse optical signals (e.g., text, formulas, tables) across multiple tasks. It leveraged a high-compression encoder and long-context decoder for handling both scene and document images effectively. However, it lacks the capability for text localization.


In this paper, we introduce \ourmodel, a unified framework designed to perform a wide range of visually-situated parsing tasks in an end-to-end manner. These tasks include text spotting, key information extraction, table recognition, and layout analysis, all of which are consolidated within a unified framework. 
\ourmodel can represent the heterogeneous structures of text in natural scenes or document images by decoupling structured points with text regions and contents. 
This two-stage approach caters to the intrinsic characteristics of text-rich images where the text instances can be parsed concurrently, thereby facilitating an enhancement in universality.


\subsection{Comparison to the Conference Version}
This paper presents a substantial extension of our previous work~\cite{omniparser}, incorporating three key advancements that contribute to the advancement in the area of unified models for visual text parsing.

\begin{enumerate}

\item \ourmodel addresses the limitations of our previous conference version, which used three separate decoders for structure point sequence generation, detection, and recognition. These decoders, though sharing the same architecture, had independent parameters, increasing model size and computational costs. In contrast, OMNIPARSER V2 employs a token-router-based shared decoder with a simplified MoE transformer, reducing model complexity. This shared decoder improves efficiency and synergy across tasks, resulting in a 23.6\% reduction in model size and enhanced performance.

\item Our method exhibits strong adaptability across diverse tasks. In addition to text spotting, key information extraction, and table recognition, which were evaluated in the conference version, \ourmodel further validates the scalability and effectiveness of structured-points-of-thought prompting through detailed experiments on layout analysis tasks using the HierText dataset, reinforcing its capability to handle complex document structures.

\item We further investigate the incorporation of SPOT prompting into multimodal large language models (MLLMs) for VsTP, a domain where MLLMs traditionally underperformed~\cite{Liu2023OCRBenchOT,Yang2024CCOCRAC,fu2024ocrbenchv2}. Our experiments reveal substantial improvements in text localization and recognition, highlighting the broad applicability and effectiveness of SPOT prompting across different model architectures.

\end{enumerate}