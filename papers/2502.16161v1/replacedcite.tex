\section{Related works}
\label{sec:rela}
   

    \subsection{Scene Text Spotting} 
    Scene text spotting typically employs a unified end-to-end trainable network, blending text detection and text recognition into a cross-modal assisted paradigm. This integrated approach streamlines text detection and recognition into a singular network. It enables simultaneous localization and identification of text within images, capitalizing on the synergistic relationship between text detection and recognition to augment overall performance. Scene text spotting can be broadly classified into two main categories: regular end-to-end scene text spotting and arbitrarily-shaped end-to-end scene text spotting. Regular end-to-end scene text spotting concentrates on detecting and recognizing text within rectangular or standard-shaped regions, whereas arbitrarily-shaped end-to-end scene text spotting broadens its scope to handle text in irregular or curved shapes.

    \noindent\textbf{Regular End-to-end Scene Text Spotting.}
    Li et al.____ introduced one of the earliest end-to-end trainable scene text spotting methods, which effectively integrated box text detection and recognition features using RoI Pooling____ within a two-stage framework. Originally designed for horizontal and focused text, their method showed significant performance improvements in an enhanced version____. Busta et al.____ also contributed to this area with their end-to-end deep text spotter, which further advanced the integration of detection and recognition. In subsequent developments, He et al.____ and Liu et al.____ incorporated anchor-free mechanisms to enhance both the training and inference speed. They employed novel sampling strategies, such as Text-Align-Sampling and RoI-Rotate, to extract features from quadrilateral detection results, further refining the end-to-end framework.


    \noindent\textbf{Arbitrarily-shaped End-to-end Scene Text Spotting.} 
    Liao et al.____ introduced Mask TextSpotter leveraging Mask R-CNN with character-level supervision to detect and recognize arbitrarily-shaped text. Mask TextSpotterv2____ reduced the dependence on character-level annotations, improving efficiency. Qin et al.____ employed RoI Masking to focus attention on arbitrarily-shaped text regions. Feng et al.____ utilized RoISlide for handling long text, whereas Wang et al.____ focused on boundary points detection, text rectification, and recognition. CharNet____ also catered to arbitrarily-shaped text spotting. Segmentation Proposal Network (SPN)____ and ABCNet____ are other noteworthy contributions. ABINet++____ innovatively used a vision model and a language model with an iterative correction mechanism. SwinTextSpotter____ used a transformer encoder for detection and recognition. Approaches based on DETR____ and variants____ for RoI-free scene text spotting have also shown promising results. TESTR____ used an encoder and dual decoders, while TTS____ used a transformer-based approach. SPTS____ and variants____ employed a single point for each instance and used a transformer to predict sequences. DeepSolo____ allows a single decoder to perform text detection and recognition. ESTextSpotter____ introduced an explicit synergy-based transformer model with task-aware queries and a vision-language communication module to enhance scene text spotting. oCLIP____ boosts text spotting performance via predefined pretext contrastive learning tasks. TCM____ and variants____ designed a flexible framework to turn a CLIP____ model into a text detector and spotter. BridgeSpotter____ connects a fixed detector and recognizer to retain modularity while improving end-to-end optimization for text spotting. DNTextSpotter____ introduced a novel denoising training method that aligns text content and position using Bezier curve-based positional queries. WeCromCL____ proposed a weakly supervised cross-modality contrastive learning framework that detects transcriptions without location annotations by modeling character-wise appearance consistency. InstructOCR____ leveraged human language instructions to enhance scene text spotting, improving text interpretation and performance on OCR-related tasks. 

    \subsection{Key Information Extraction} 
Existing KIE approaches can be roughly categorized into two types: OCR-dependent models and OCR-free models. OCR-dependent models focus on using optical character recognition (OCR) for extracting textual information. Early KIE methods primarily built layout-aware or graph-based representations for KIE tasks via sequence labeling with OCR inputs____. However, these methods often rely on text with a proper reading order or the use of extra modules____ for OCR serialization, which is not always practical in real-world scenarios where the layout may be complex or unordered.
To address the serialization issue, some methods leverage additional detection or linking modules to model the complex relationships between text blocks or tokens____. While these strategies mitigate the reading order problem, the added complexity of decoding and post-processing steps often limits their generalization ability, making them less adaptable to a wide variety of document layouts.
In contrast, generation-based methods____ are proposed to alleviate the burden of post-processing and task-specific link designs. Another category of OCR-free methods offers an alternative by either utilizing OCR-aware pre-training or by incorporating OCR modules within an end-to-end framework.
For example, Pix2Struct____ proposed a pretraining task in which the model generates a complete HTML DOM tree based on a masked webpage screenshot, without relying on OCR.
PreSTU____ introduced an OCR-aware pre-training objective that directly generates text sequences from image-based inputs. Donut and other Seq2Seq-like methods____ adopted a text reading pre-training objective and generate structured outputs consisting of text and entity tokens.
By explicitly equipping text reading modules, previous work____ can maintain high performance in end-to-end KIE tasks with task-specific design.

\subsection{Table Recognition}
Tables, as structured data, provide a succinct and compact format for organizing valuable content. Recent advancements in vision-based approaches have significantly improved the extraction of tables from documents. To provide a comprehensive overview, the task of table extraction from documents is typically divided into three main processes: table detection, table structure recognition, and table content recognition. Table detection, primarily concerned with locating tables within documents or images, has been extensively explored in previous work____, though it is beyond the scope of this paper. Table structure recognition (TSR) involves identifying the structure of a given table within a document or images, and has long been a focal point in the document understanding community____. Table content recognition (TCR) focuses on locating and recognizing text instances within the table cells and can be accomplished using established offline OCR models. In this paper, we concentrate on table recognition (TR) tasks that integrate both table structure recognition and table content recognition. Table recognition methods can be broadly categorized into two groups: non-end-to-end-based____ and end-to-end-based____ approaches. Non-end-to-end-based methods mainly first recover table structure via a specific model and then employ offline OCR models to construct complete HTML sequences via complex post-process. It is worth noting that end-to-end-based table recognition tasks remain less explored due to their complexity and challenging nature. Benefiting from the modularized architecture design, our model effectively separates the extraction of pure table HTML tags with cell text center point sequences and the cell text recognition sequences, accomplishing table recognition in an end-to-end fashion.
    
\subsection{Layout Analysis}
Geometric layout analysis focuses on detecting semantically coherent text blocks as objects____. Recent approaches have modeled this task using various techniques, including object detection____, semantic segmentation____, and graph-based learning over OCR token structures via Graph Convolutional Networks (GCN). For example, Unified Detector____ utilized segmentation-based formulations to pursue unifying scene text detection and layout analysis through an affinity matrix for modeling grouping relations, but it cannot generate word-level entities and lacks recognition capabilities. Another direction in layout analysis focuses on semantic parsing of documents to extract key-value pairs____. These methods typically leverage language models built on top of OCR outputs. In this paper, we conduct geometric layout analysis for identifying a 3-level hierarchical structure: words, lines, and paragraphs in an end-to-end unified paradigm.

\subsection{Unified Frameworks}
There is an increasing shift towards developing unified frameworks for parsing text-rich images across multiple tasks. Earlier works, such as DocReL____ and BROS____ model relations between table cells or entities through binary classification or a relational matrix, which also requires an off-the-shelf OCR engine.
StrucTexTv2____ proposed a multi-modal learning framework aimed at document image understanding tasks by constructing self-supervised tasks. Yet, it requires several task-specific designs for downstream tasks, like Cascade R-CNN for table cell detection. Additionally, SeRum____ converts the end-to-end KIE task into a local decoding process and then shows its effectiveness on text spotting task. SCOB____ achieves universal text understanding across tasks by using character-wise supervised contrastive learning with online text rendering, effectively bridging domain gaps in document and scene text images with weak supervision. DocRes____ introduced a dynamic, task-specific prompt to unify five document image restoration tasks, while UPOCR____ unifies various OCR pixel-level tasks within a single image-to-image transformation framework, utilizing a vision Transformer architecture and task-aware prompts to achieve superior performance in tasks like text removal and tampered text detection. Recent efforts have focused on developing more general, unified parsing frameworks. StrucTexTv3____, integrated a multi-scale visual transformer, a multi-granularity token sampler, and instruction learning, achieving state-of-the-art results in text perception and comprehension tasks. DocOwl1.5____ introduces a unified structure learning framework with H-Reducer to enhance MLLMs for document understanding. GOT____ proposed a unified end-to-end OCR model capable of processing diverse optical signals (e.g., text, formulas, tables) across multiple tasks. It leveraged a high-compression encoder and long-context decoder for handling both scene and document images effectively. However, it lacks the capability for text localization.


In this paper, we introduce \ourmodel, a unified framework designed to perform a wide range of visually-situated parsing tasks in an end-to-end manner. These tasks include text spotting, key information extraction, table recognition, and layout analysis, all of which are consolidated within a unified framework. 
\ourmodel can represent the heterogeneous structures of text in natural scenes or document images by decoupling structured points with text regions and contents. 
This two-stage approach caters to the intrinsic characteristics of text-rich images where the text instances can be parsed concurrently, thereby facilitating an enhancement in universality.


\subsection{Comparison to the Conference Version}
This paper presents a substantial extension of our previous work____, incorporating three key advancements that contribute to the advancement in the area of unified models for visual text parsing.

\begin{enumerate}

\item \ourmodel addresses the limitations of our previous conference version, which used three separate decoders for structure point sequence generation, detection, and recognition. These decoders, though sharing the same architecture, had independent parameters, increasing model size and computational costs. In contrast, OMNIPARSER V2 employs a token-router-based shared decoder with a simplified MoE transformer, reducing model complexity. This shared decoder improves efficiency and synergy across tasks, resulting in a 23.6\% reduction in model size and enhanced performance.

\item Our method exhibits strong adaptability across diverse tasks. In addition to text spotting, key information extraction, and table recognition, which were evaluated in the conference version, \ourmodel further validates the scalability and effectiveness of structured-points-of-thought prompting through detailed experiments on layout analysis tasks using the HierText dataset, reinforcing its capability to handle complex document structures.

\item We further investigate the incorporation of SPOT prompting into multimodal large language models (MLLMs) for VsTP, a domain where MLLMs traditionally underperformed____. Our experiments reveal substantial improvements in text localization and recognition, highlighting the broad applicability and effectiveness of SPOT prompting across different model architectures.

\end{enumerate}