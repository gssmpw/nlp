\documentclass[10pt,journal,compsoc]{IEEEtran}
\usepackage[nocompress]{cite}

\usepackage[margin=0pt,font=small,labelfont=bf,labelsep=endash,tableposition=top]{caption}

\usepackage{array}
\usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
\usepackage{url}
\usepackage{booktabs}
\usepackage{dsfont}
\usepackage{pifont}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{epstopdf}
\usepackage{makecell,multirow,diagbox}
\usepackage{color}
\usepackage{soul}
\usepackage{url}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{mathtools,xspace}
\usepackage{ragged2e}

\usepackage{bm}


\hypersetup{
    colorlinks=true,
    breaklinks=false,
    linkcolor= red,
    bookmarksopen=false,
    filecolor=black,
    citecolor=blue,
    linkbordercolor=blue
}
\newcommand{\onedot}{\ifx\@let@token.\else.\null\fi\xspace}
\newcommand{\etal}{\emph{et al}\onedot}
\newcommand{\eg}{\emph{e.g}\onedot}
\newcommand{\ie}{\emph{i.e}\onedot}
\newcommand{\etc}{\emph{etc}\onedot} \newcommand{\vs}{\emph{vs}\onedot}
\newcommand{\wrt}{w.r.t\onedot} \newcommand{\dof}{d.o.f\onedot}
\newcommand{\iid}{i.i.d\onedot}



\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\usepackage{adjustbox}
\usepackage{threeparttable}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{siunitx}
\usepackage{tcolorbox}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{placeins}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\definecolor{deepyellow}{RGB}{255, 215, 0}



\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\usepackage{microtype}

\newcommand{\ourmodelvone}{{\textsc{OmniParser}}\xspace}
\newcommand{\ourmodel}{{\textsc{OmniParser V2}}\xspace}
\newcommand{\pointsdecoder}{{token-router-based shared decoder}\xspace}
\newcommand{\polydecoder}{{token-router-based shared decoder}\xspace}
\newcommand{\contentdecoder}{{token-router-based shared decoder}\xspace}


\newcommand\mypara[1]{\vspace{1.0mm}\noindent\textbf{#1}}
\newcommand\rankfirst[1]{\textbf{#1}}
\newcommand\ranksecond[1]{\underline{#1}}
\newcommand\token[1]{\texttt{\textless{#1}\textgreater}}


\begin{document}


\title{OmniParser V2: Structured-Points-of-Thought for Unified Visual Text Parsing and Its Generality to Multimodal Large Language Models\\--- Supplemental Material ---}


\maketitle


\section{SPOT-style SFT Dataset}
We used publicly available datasets and data collected from Platypus~\cite{Wang2024PlatypusAG} to construct the SPOT-style supervised fine-tuning (SFT) datasets for multimodal large language models. The settings, sizes, and sources in our curated SPOT-style SFT data models are shown in~\cref{tab:spot_like_sft_data_detail}. Rico~\cite{rico} uses pseudo-labels generated by our internal business's small model for detection and recognition, while all other annotated open-source datasets use raw OCR annotations.


\begin{table}[htbp]
\centering
\caption{\textbf{The settings and number of our curated SFT data.} Num. short for number. }
   \begin{adjustbox}{max width=0.49\textwidth}
   \begin{threeparttable}
\begin{tabular}{lccccl}
\toprule
  Task Type                         & Prompt Setting     & Data Num.  & Data Name  & Data Sources \\
 \midrule
  Text Spotting  &  \makecell[l]{N-SPOT,\\ S-SPOT, \\L-SPOT}       & 181,593    & TS180k  & \makecell[l]{ICDAR2013~\cite{Karatzas2013ICDAR2R},\\ HierText~\cite{long2022towards},\\ TextOCR~\cite{singh2021textocr},\\ SynthText150k~\cite{Liu2021ABCNetVA}} \\
  \midrule
 Text Spotting  &      \makecell[l]{N-SPOT,\\ S-SPOT, \\L-SPOT}         & 389,433            & TS380k & \makecell[l]{ICDAR2013~\cite{Karatzas2013ICDAR2R},\\ HierText~\cite{long2022towards},\\ TextOCR~\cite{singh2021textocr},\\ SynthText150k~\cite{Liu2021ABCNetVA},\\ OpenImageV5 Text~\cite{krylov2021open}}  \\
   \midrule
  Read All Text  &  \makecell[l] {Read all the text \\in the image.}           & 446,702        & R440k & \makecell[l]{DocLayNet~\cite{DocLayNet},\\ HierText~\cite{long2022towards},\\ TextOCR~\cite{singh2021textocr},\\ SynthText150k~\cite{Liu2021ABCNetVA},\\ OpenImageV5 Text~\cite{krylov2021open}, \\MLT2017~\cite{nayef2017icdar2017},\\ COCO-Text~\cite{veit2016coco},\\ D4LA~\cite{Da_2023_ICCV}}  \\
  \midrule
 Read All Text &        \makecell[l] {Read all the text \\in the image.}      & 981,284             & R980k & \makecell[l]{DocLayNet~\cite{DocLayNet},\\ HierText~\cite{long2022towards},\\ TextOCR~\cite{singh2021textocr},\\ SynthText150k~\cite{Liu2021ABCNetVA},\\ OpenImageV5 Text~\cite{krylov2021open}, \\MLT2017~\cite{nayef2017icdar2017},\\ COCO-Text~\cite{veit2016coco},\\  SynthDoG\_HW~\cite{kim2022donut},\\ PubLayNet~\cite{zhong2019publaynet},\\ LAION-OCR~\cite{Schuhmann2022LAION5BAO},\\ D4LA~\cite{Da_2023_ICCV},\\ CTIG-DM~\cite{Zhu2023ConditionalTI},\\ Rico~\cite{rico} } \\

\bottomrule
\end{tabular}
   \end{threeparttable}
   \end{adjustbox}

\label{tab:spot_like_sft_data_detail}
\end{table}







\section{Implementation Details}
\subsection{Spatial-Window Prompting}
Spatial-window prompting comprises two components: fixed mode and random mode. In the fixed mode, the image is divided into grid blocks evenly, such as 3x3 or 2x2. Conversely, in the random mode, the starting point of the spatial window is randomly determined. In order to encompass more texts within the random box, the area of the random box is established to be no less than 1/9 of the original image. To elaborate further, a 30\% probability is assigned for selecting the fixed mode, another 30\% probability for selecting the random mode, and a 40\% probability for the defaulting window to cover the entire image. Following ~\cite{kil2023towards}, we set the bin size of the coordinate vocabulary as 1000. The pseudo-code of spatial-window prompting is shown in the following.

\begin{lstlisting}[language=Python]
import random

# prob for different mode
prob = random.uniform(0, 1)

# quantizing coordinates with n_bins
n_bins = 1000

if prob < 0.4:
    # default window
    start_x, start_y, end_x, end_y = [0, 0, n_bins - 1, n_bins - 1]
elif prob < 0.7:
    # x-axis and y-axis are partitioned into varying numbers of blocks.
    num_xs = [3, 3, 1, 3, 2, 2, 2, 1]
    num_ys = [3, 1, 3, 2, 3, 2, 1, 2]

    total_windows = []
    for num_x, num_y in zip(num_xs, num_ys):
        inter_x = min(int(n_bins / num_x), n_bins - 1)
        inter_y = min(int(n_bins / num_y), n_bins - 1)
        
        for i in range(num_x):
            for j in range(num_y):
                start_x = i*inter_x
                start_y = j*inter_y
                end_x = min(start_x + inter_x, n_bins - 1)
                end_y = min(start_y + inter_y, n_bins - 1)
                total_windows.append([start_x, start_y, end_x, end_y])
    
    start_x, start_y, end_x, end_y = random.choice(total_windows)
else:
    inter = int(n_bins / 3)
    start_x = random.randint(0, inter * 2)
    start_y = random.randint(0, inter * 2)
    rect_w, rect_h = random.randint(inter, n_bins - 1), random.randint(inter, n_bins - 1)
    end_x, end_y = min(start_x + rect_w, n_bins - 1), min(start_y + rect_h, n_bins - 1)

spatial_window_prompt = [start_x, start_y, end_x, end_y]

\end{lstlisting}

\subsection{Table Recognition}

Given a table image, we resize it to 1,024$\times$1,024 pixels. The \pointsdecoder, utilizing the feature vector from the Image Encoder, simultaneously generates pure HTML tags with structural cell point sequences in the same sequence representing the table's logical and physical structures. These structural cell point sequences serve as start-prompting input for the \contentdecoder, which extracts table cell contents in parallel. The final output combines pure HTML tags with cell contents, forming complete HTML sequences faithfully representing the table's structure and content.


\mypara{Datasets.} 
Since our model predicts both the logical structure of tables with cell bounding box central points and cell content, datasets lacking cell content and corresponding bounding box annotations, such as TABLE2LATEX-450K~\cite{deng2019challenges}, TableBank~\cite{li2020tablebank}, UNLV~\cite{shahab2010open}, IC19B2H~\cite{gao2019icdar}, WTW~\cite{long2021parsing} and TUCD~\cite{raja2021visual}, are not suitable for our approach. Similarly, datasets like ICDAR2013Table~\cite{gobel2013icdar}, SciTSR~\cite{chi2019complicated}, and PubTables-1M~\cite{smock2022pubtables}, which provide cell content and content box annotations, employ metrics based on box representations that are incompatible with our point-based format. Consequently, PubTabNet (PTN)~\cite{EDD} and FinTabNet (FTN)~\cite{GTE} are selected for our model evaluation.

\mypara{GT Generation.} The ground truth pure HTML tags of tables are tokenized into structural tokens. Following the previous works~\cite{TableMaster, VAST}, we use the merged labels to represent a non-spanning cell to reduce the length of the HTML tags. Specifically, we use 
\emph{\textless td\textgreater\textless/td\textgreater} and \emph{\textless td\textgreater[]\textless/td\textgreater} to denote empty cells and non-empty cells, respectively. For a cell spanning multiple rows or columns, the original HTML tags are broken into four tokens: \emph{\textless td}, \emph{colspan=``n"} or \emph{rowspan=``n"}, \emph{\textgreater}, and \emph{\textless/td\textgreater}. We use the first token \emph{\textless td} to represent a spanning cell. In addition, four special symbol categories need to be added: \emph{\textless S\textgreater}, \emph{\textless /S\textgreater}, \emph{\textless PAD\textgreater}, and \emph{\textless UNK\textgreater}, which represent the beginning of a sequence, the end of a sequence, padding symbols, and unknown characters, respectively. For building the GT of \pointsdecoder, we insert center points of each cell text box to corresponding HTML tags. For building the GT of \contentdecoder, we combine each cell text with corresponding center points as a whole sequence where center points can be viewed as a start-prompting input for recognizing text, and each cell text is tokenized at the character level. An example of building a training sequence GT for the \pointsdecoder and the \contentdecoder in the table recognition task is illustrated in~\cref{fig:gt_table_stru}.


\begin{figure}[htbp]
  \centering \includegraphics[width=0.96\linewidth]{figs/gt_table_structure_v2.pdf}
   \captionsetup{width=0.96\linewidth}
   \caption{\textbf{An example of building training GTs for table recognition task.} We use the center points of each cell text box to build GTs for the \pointsdecoder and the \contentdecoder. If the cell is empty text, the corresponding points in the GTs are left empty as well. }
   \label{fig:gt_table_stru}
\end{figure}

\subsection{Layout Analysis}
Thanks to the flexible expression of structured sequence in \ourmodel, it is convenient for us to extend it to other OCR-related tasks, such as layout analysis, which aims to group the text in the image into three levels, namely word, line, and paragraph, based on spatial position and semantic relationship. Previous methods~\cite{long2022towards} mainly achieved hierarchical results by clustering based on similarity. In our approach, we distinguish the text belonging to different hierarchical intervals by simply inserting \emph{\textless line\textgreater} and \emph{\textless paragraph\textgreater} structural tags into the sequence of text center points, as shown in~\cref{fig:gt_layout_stru}.


\begin{figure}[htbp]
  \centering \includegraphics[width=0.98\linewidth]{figs/gt_layout_structure_v2.pdf}
   \captionsetup{width=0.98\linewidth}
   \caption{\textbf{An Example of building training GTs for layout analysis task.} }
   \label{fig:gt_layout_stru}
\end{figure}

\begin{figure*}[htbp]
  \centering \includegraphics[width=0.95\linewidth]{figs/cord_vs_donut-crop.pdf}
   \captionsetup{width=0.95\linewidth}
   \caption{\textbf{A comparative analysis of partial results obtained from \ourmodel and Donut on CORD.} The first column depicts the original image, while columns 2 and 3 illustrate our detection results and the corresponding formatted output, respectively. Column 4 showcases the Donut's formatted output. Notably, our model demonstrates superior performance in entity extraction. }
   \label{fig:vs_donut_on_cord}
\end{figure*}


\begin{figure*}[htbp]
  \centering \includegraphics[width=0.95\linewidth]{figs/donut_table_failure_case.pdf}
   \captionsetup{width=0.95\linewidth}
   \caption{\textbf{Illustrative failure case of Donut in table recognition task.} Red text means error predictions. For readability, we only highlight two errors in this example. Due to the lack of point location information, Donut has an attention drift problem, resulting in the prediction of repeated tokens and leading to a high probability of error accumulation in long-sequence scenarios. (The figure is best viewed in color.)}
   \label{fig:donut_table_failure_case}
\end{figure*}

\section{Comparisons with Donut on KIE Task}
As shown in ~\cref{fig:vs_donut_on_cord}, \ourmodel can achieve entity extraction while predicting the location of each entity word.
However, Donut only predicts the structured sequence for entity extraction without any localization ability.
Thus, the absence of direct region supervision during both training and prediction stages often leads to inferior results for entities of the same values (Row 1), repeated entities (Row 2), or entities with explicit trigger names (Row 3).

\section{Training Donut on Table Recognition Task}
We fine-tuned the OCR-free end-to-end model Donut~\cite{kim2022donut} for table recognition on FinTabNet dataset. The ground truth sequence utilized combined HTML tags with table cell text, and we use different training hyper-parameters for adequate verification, as shown in \cref{tab:donut_abla}. Due to GPU memory limitations, we constrained the decoder's max length in Donut to 4,000. Note that the original HTML sequence max lengths for PubTabNet and FinTabNet are 8,722 and 8,035, respectively. 
For long sequence prediction tasks such as table recognition, training an end-to-end model like Donut with combined HTML stages, including cell text, is non-trivial.
There is a high probability of error accumulation and attention drift in long-sequence scenarios leading to the inferior performance of Donut for table recognition. An illustrative example of a failure case for Donut in table recognition task is shown in~\cref{fig:donut_table_failure_case}. 
Specifically, due to the lack of region supervision, the end-to-end model Donut has demonstrated an attention drift problem, resulting in the prediction of repeated tokens and leading to a high probability of error accumulation in long-sequence scenarios. In contrast, \ourmodel decomposes the location-aware structured points sequence and cell text recognition generation, alleviating the issues of attention drift and error accumulation.

\begin{table}[htbp]
\centering
\caption{\textbf{Comparisons of different training hyper-parameters of Donut on FinTabNet datasets.} LR is short for learning rate.}
   \begin{adjustbox}{max width=0.47\textwidth}
   \begin{threeparttable}
\begin{tabular}{lccccc}
\toprule
 
  Methods                            & LR      & Epoch      & S-TEDS & TEDS  \\
 \midrule

\multirow{5}{*}{Donut~\cite{kim2022donut} }                                                   & 3e-5            & 20               &  22.2  & 17.2  \\
                                                 & 3e-5            & 40               &  26.2  & 20.0  \\
                                                   & 1e-4            & 40               &  30.7  & 29.1  \\
                                                    & 1e-3            & 40               &  41.7  & 40.5  \\
                                                   & 1e-3            & 100               &   41.9 & 41.2  \\
 \midrule
\multirow{1}{*}{\ourmodel (ours)}  &   -          & -             & \textbf{93.2}  & \textbf{90.5} \\

\bottomrule
\end{tabular}
   \end{threeparttable}
   \end{adjustbox}
   
\label{tab:donut_abla}
\end{table}


\section{More Analysis}
\subsection{Failure Case}
Typical failure cases of inaccurately predicted points are demonstrated in \cref{fig:failure_case}. Our method only requires supervision of word locations in the training phase. 
It is quite robust to noisy location predictions.
Nonetheless, the accuracy of text spotting might be influenced when ambiguities arise in word point locations.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.5\linewidth]{figs/error-crop.pdf}
\captionof{figure}{Failure cases, predicted \textcolor{red}{points} and \textcolor{deepyellow}{polygons}, \textcolor{blue}{GT polygons}.}
\label{fig:failure_case}
\end{figure}


\subsection{Limitation}
Although \ourmodel achieves superior performance in unified visually-situated text parsing, its understanding and reasoning capabilities remain limited. Integrating reinforcement learning techniques, such as DeepSeek-R1~\cite{deepseekai2025deepseekr1}, is one of the promising directions for our future work to enhance the model’s reasoning ability, interpretability, and comprehension in document-oriented multimodal tasks.




{
\bibliographystyle{IEEEtran}
 \bibliography{strings_abbr, references}

}


\end{document}