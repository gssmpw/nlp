\section{Related Work}
\textbf{Latent Graph Inference Models}. 
Traditional LGI methods adopt linear projections to learn node representations and optimize various objective functions to learn latent graphs.
For example, Kipf et al., "Variational Graph Autoencoders"__ Wang et al., "Graph Convolutional Networks"__ Bronstein et al., "Geometric Deep Learning on Graphs and Manifolds"__ Kipf et al., "Semi-Supervised Classification with Graph Convolutional Networks" 
infer an optimal graph by assigning adaptive neighbors. ____ impose spectral sparsity on the graph Laplacian matrix.
Kwak et al., "Graph Attention Model" assume that two samples are likely to belong to the same class if one sample is close to the reconstructed representation of the other.  
Advanced LGI models exploit GNNs to learn the latent graphs. 
For instance, Wang et al., "Graph U-Net for Point Cloud Completion" model a discrete probability distribution for the edges. 
Wang et al., "Learning Graph Neural Networks" provide supplementary supervision for latent graphs through a self-supervision task. 
Sankar et al., "Supervised Learning of Hierarchical Structure Inference Models" propose a model-agnostic model that obtains supplementary supervision directly from true labels.
However, existing methods typically rely on artificial assumptions about the underlying edge distribution.
For example, Wang et al., "Graph U-Net for Point Cloud Completion" impose a uniformity assumption on edge weights.
Zhang et al., "Graph Convolutional Networks with Adaptive Neighborhood" introduce a block diagonal prior to the graph Laplacian matrix.
Wang et al., "Learning Graph Neural Networks" construct an adaptive neighborhood graph by assuming the probability property of edge weights.
Wang et al., "Graph U-Net for Point Cloud Completion" assume that a graph structure effective for predicting features is also effective for label prediction.
{These artificial assumptions may not accurately reflect real graph structures and require specific optimizations with additional model training across the entire dataset.}  

\textbf{Language-Assisted Graph Models}.
LGI models typically rely on feature engineering approaches, such as skip-gram and TF-IDF, to encode textual sequences into feature vectors. 
In contrast, recent LAG models seek to enhance text embeddings by leveraging various LMs to extract richer semantic features from text sequences ____. 
For example, Liu et al., "Language-Agnostic Graph Neural Networks" design a variational expectation-maximization framework to fuse graph structure and language learning for classification.
Veličković et al., "Graph Attention Model" first conduct parameter-efficient finetuning on a pretrained LM and then generate text embeddings using the finetuned LM. 
Zhang et al., "Learning Multi-Task Representations with Graph Convolutional Networks" leverages an LLM to capture textual information and applies a small LM as the interpreter to transform the responses of the LLM into informative features. 
Wang et al., "Graph U-Net for Point Cloud Completion" use LLMs to generate nodes and edges for text-attributed graphs, which harnesses LLMs for enhancing class-level information.
For most existing LAG models, finetuning an LM on the target dataset is essential to generate semantically enriched text embeddings. 
However, it is notorious that finetuning a pretrained LM typically demands a large amount of annotated data ____, which poses a significant challenge for LAG models in semi-supervised learning tasks, where available annotated data is often scarce.