\section{Related Work}
\textbf{Latent Graph Inference Models}. 
Traditional LGI methods adopt linear projections to learn node representations and optimize various objective functions to learn latent graphs.
For example, ____ design an entropy regularization to control the uniformity level of edge weights. ____ infer an optimal graph by assigning adaptive neighbors. ____ impose spectral sparsity on the graph Laplacian matrix.
____ assume that two samples are likely to belong to the same class if one sample is close to the reconstructed representation of the other.  
Advanced LGI models exploit GNNs to learn the latent graphs. 
For instance, ____ model a discrete probability distribution for the edges. 
____ provide supplementary supervision for latent graphs through a self-supervision task. 
____ propose a model-agnostic model that obtains supplementary supervision directly from true labels.
However, existing methods typically rely on artificial assumptions about the underlying edge distribution.
For example, ____ impose a uniformity assumption on edge weights.
____ introduce a block diagonal prior to the graph Laplacian matrix.
____ construct an adaptive neighborhood graph by assuming the probability property of edge weights.
____ assume that a graph structure effective for predicting features is also effective for label prediction.
{These artificial assumptions may not accurately reflect real graph structures and require specific optimizations with additional model training across the entire dataset.} 







 

\textbf{Language-Assisted Graph Models}.
LGI models typically rely on feature engineering approaches, such as skip-gram and TF-IDF, to encode textual sequences into feature vectors. 
In contrast, recent LAG models seek to enhance text embeddings by leveraging various LMs to extract richer semantic features from text sequences ____.
For example, ____ design a variational expectation-maximization framework to fuse graph structure and language learning for classification.
____ first conduct parameter-efficient finetuning on a pretrained LM and then generate text embeddings using the finetuned LM. 
____ leverages an LLM to capture textual information and applies a small LM as the interpreter to transform the responses of the LLM into informative features. 
____ use LLMs to generate nodes and edges for text-attributed graphs, which harnesses LLMs for enhancing class-level information.
For most existing LAG models, finetuning an LM on the target dataset is essential to generate semantically enriched text embeddings. 
However, it is notorious that finetuning a pretrained LM typically demands a large amount of annotated data ____, which poses a significant challenge for LAG models in semi-supervised learning tasks, where available annotated data is often scarce.