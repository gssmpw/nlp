\section{Related Work}
\textbf{Latent Graph Inference Models}. 
Traditional LGI methods adopt linear projections to learn node representations and optimize various objective functions to learn latent graphs.
For example, \cite{zhang2010graph} design an entropy regularization to control the uniformity level of edge weights. \cite{nie2016unsupervised} infer an optimal graph by assigning adaptive neighbors. \cite{lu2018subspace} impose spectral sparsity on the graph Laplacian matrix.
\cite{lu2021target} assume that two samples are likely to belong to the same class if one sample is close to the reconstructed representation of the other.  
Advanced LGI models exploit GNNs to learn the latent graphs. 
For instance, \cite{LDS} model a discrete probability distribution for the edges. 
\cite{SLAPS} provide supplementary supervision for latent graphs through a self-supervision task. 
\cite{jianglin2023LGI} propose a model-agnostic model that obtains supplementary supervision directly from true labels.
However, existing methods typically rely on artificial assumptions about the underlying edge distribution.
For example, \cite{zhang2010graph} impose a uniformity assumption on edge weights.
\cite{lu2018subspace} introduce a block diagonal prior to the graph Laplacian matrix.
\cite{LU2021107758} construct an adaptive neighborhood graph by assuming the probability property of edge weights.
\cite{SLAPS} assume that a graph structure effective for predicting features is also effective for label prediction.
{These artificial assumptions may not accurately reflect real graph structures and require specific optimizations with additional model training across the entire dataset.} 







 

\textbf{Language-Assisted Graph Models}.
LGI models typically rely on feature engineering approaches, such as skip-gram and TF-IDF, to encode textual sequences into feature vectors. 
In contrast, recent LAG models seek to enhance text embeddings by leveraging various LMs to extract richer semantic features from text sequences \citep{li2024surveygraphmeetslarge}.
For example, \cite{GLEM} design a variational expectation-maximization framework to fuse graph structure and language learning for classification.
\cite{duan2023simteg} first conduct parameter-efficient finetuning on a pretrained LM and then generate text embeddings using the finetuned LM. 
\cite{he2023harnessing} leverages an LLM to capture textual information and applies a small LM as the interpreter to transform the responses of the LLM into informative features. 
\cite{yu2023empower} use LLMs to generate nodes and edges for text-attributed graphs, which harnesses LLMs for enhancing class-level information.
For most existing LAG models, finetuning an LM on the target dataset is essential to generate semantically enriched text embeddings. 
However, it is notorious that finetuning a pretrained LM typically demands a large amount of annotated data \citep{GPT3}, which poses a significant challenge for LAG models in semi-supervised learning tasks, where available annotated data is often scarce.