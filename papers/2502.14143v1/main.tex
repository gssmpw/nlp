\documentclass[a4paper]{article}
\usepackage{defs} %

\usepackage{comment} %

\title{Multi-Agent Risks from Advanced AI}
\author{
    Lewis Hammond and Alan Chan and Jesse Clifton and Jason Hoelscher-Obermaier and Akbir Khan and Euan McLean and Chandler Smith and Wolfram Barfuss and Jakob Foerster and Tomáš Gavenčiak and The Anh Han and Edward Hughes and Vojtěch Kovařík and Jan Kulveit and Joel Z. Leibo and Caspar Oesterheld and Christian Schroeder de Witt and Nisarg Shah and Michael Wellman and Paolo Bova and Theodor Cimpeanu and Carson Ezell and Quentin Feuillade-Montixi and Matija Franklin and Esben Kran and Igor Krawczuk and Max Lamparth and Niklas Lauffer and Alexander Meinke and Sumeet Motwani and Anka Reuel and Vincent Conitzer and Michael Dennis and Iason Gabriel and Adam Gleave and Gillian Hadfield and Nika Haghtalab and Atoosa Kasirzadeh and Sébastien Krier and Kate Larson and Joel Lehman and David C. Parkes and Georgios Piliouras and Iyad Rahwan
}
\date{February 19 2025}

\begin{document}

\includepdf{covers/front.pdf}


\begin{center}
    {\Huge Multi-Agent Risks from Advanced AI}
\end{center}

\vspace{1.5em}



\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\begin{minipage}[t]{0.52\linewidth}
    \raggedright
    \large
    {Lewis Hammond}$^{1,2,}$\footnotemark{}
    \vspace{1em}

    {Alan Chan}$^{3,4}$,
    {Jesse Clifton}$^{5,1}$,
    {Jason Hoelscher-Obermaier}$^{6}$,
    {Akbir Khan}$^{7,8,(1)}$,
    {Euan McLean}$^{\dagger}$,
    {Chandler Smith}$^{1}$
    \vspace{1em}

    {Wolfram Barfuss}$^{9}$,
    {Jakob Foerster}$^{2,10}$,
    {Tomáš Gavenčiak}$^{11}$,
    {The Anh Han}$^{12}$,
    {Edward Hughes}$^{13}$,
    {Vojtěch Kovařík}$^{14,(15)}$,
    {Jan Kulveit}$^{11}$,
    {Joel Z. Leibo}$^{13}$,
    {Caspar Oesterheld}$^{15}$,
    {Christian Schroeder de Witt}$^{2}$,
    {Nisarg Shah}$^{16}$,
    {Michael Wellman}$^{17}$
    \vspace{1em}

    {Paolo Bova}$^{12}$,
    {Theodor Cimpeanu}$^{18,(19)}$
    {Carson Ezell}$^{20}$,
    {Quentin Feuillade- Montixi}$^{21,(\dagger)}$,
    {Matija Franklin}$^{8}$,
    {Esben Kran}$^{6}$,
    {Igor Krawczuk}$^{\dagger,(22)}$,
    {Max Lamparth}$^{23}$,
    {Niklas Lauffer}$^{24}$,
    {Alexander Meinke}$^{25,(\dagger)}$,
    {Sumeet Motwani}$^{2,(24)}$,
    {Anka Reuel}$^{23,20}$
    \vspace{1em}

    {Vincent Conitzer}$^{15}$,
    {Michael Dennis}$^{13}$,
    {Iason Gabriel}$^{13}$,
    {Adam Gleave}$^{26}$,
    {Gillian Hadfield}$^{27}$,
    {Nika Haghtalab}$^{24}$,
    {Atoosa Kasirzadeh}$^{15}$,
    {Sébastien Krier}$^{13}$,
    {Kate Larson}$^{28,13}$,
    {Joel Lehman}$^{\dagger}$,
    {David C. Parkes}$^{20}$,
    {Georgios Piliouras}$^{13,29}$,
    {Iyad Rahwan}$^{30}$
\end{minipage}
\hfill 
\begin{minipage}[t]{0.44\linewidth}
    \raggedright
    \small
    $^{1}${Cooperative AI Foundation}\\
    $^{2}${University of Oxford}\\
    $^{3}${Mila}\\
    $^{4}${Centre for the Governance of AI}\\
    $^{5}${Center on Long-Term Risk}\\
    $^{6}${Apart Research}\\
    $^{7}${Anthropic}\\
    $^{8}${University College London}\\
    $^{9}${University of Bonn}\\
    $^{10}${Meta AI}\\
    $^{11}${Charles University}\\
    $^{12}${Teesside University}\\
    $^{13}${Google DeepMind}\\
    $^{14}${Czech Technical University}\\
    $^{15}${Carnegie Mellon University}\\
    $^{16}${University of Toronto}\\
    $^{17}${University of Michigan}\\
    $^{18}${University of Stirling}\\
    $^{19}${University of St Andrews}\\
    $^{20}${Harvard University}\\
    $^{21}${PRISM AI}\\
    $^{22}${École Polytechnique Fédérale de Lausanne}\\
    $^{23}${Stanford University}\\
    $^{24}${University of California, Berkeley}\\
    $^{25}${Apollo Research}\\
    $^{26}${FAR.AI}\\
    $^{27}${Johns Hopkins University}\\
    $^{28}${University of Waterloo}\\
    $^{29}${Singapore University of Technology and Design}\\
    $^{30}${Max Planck Institute for Human Development}\\
    $^\dagger${Independent}
\end{minipage}

\footnotetext{
    Correspondence to \url{lewis.hammond@cooperativeai.org}.
    Suggested citation: ``Hammond et al. (2025). \textit{Multi-Agent Risks from Advanced AI}. Cooperative AI Foundation, Technical Report \#1.''
    Author clusters are ordered by approximate magnitude of contribution and represent the lead author, organisers, major contributors, minor contributors, and advisors, respectively.
    Within clusters, authors are listed alphabetically.
    Full details of author roles are available in \Cref{app:contributions}.
    Affiliations in parentheses indicate that the author's work on this report was primarily completed while under that affiliation.
    Due to the length of the author list, authorship does not entail endorsement of all claims in the report, nor does inclusion entail an endorsement on the part of any individual's organisation.
    In particular, contributions to this report reflect the views of the respective contributors and not necessarily the views of the Cooperative AI Foundation, its trustees, or funders.
}
\renewcommand{\thefootnote}{\arabic{footnote}}
\setcounter{footnote}{0}



\vspace{1.5em}

\begin{center}
    {\large \textbf{Abstract}}
    \vspace{1em}

    \begin{minipage}[]{0.75\linewidth}
        The rapid development of advanced AI agents and the imminent deployment of many instances of these agents will give rise to multi-agent systems of unprecedented complexity.
        These systems pose novel and under-explored risks.
        In this report, we provide a structured taxonomy of these risks by identifying three key failure modes (miscoordination, conflict, and collusion) based on agents' incentives, as well as seven key risk factors (information asymmetries, network effects, selection pressures, destabilising dynamics, commitment problems, emergent agency, and multi-agent security) that can underpin them.
        We highlight several important instances of each risk, as well as promising directions to help mitigate them.
        By anchoring our analysis in a range of real-world examples and experimental evidence, we illustrate the distinct challenges posed by multi-agent systems and their implications for the safety, governance, and ethics of advanced AI.
    \end{minipage}
\end{center}



\newpage

\tableofcontents

\newpage

\addcontentsline{toc}{section}{Executive Summary}

\input{sections/0_executive_summary}

\input{sections/1_introduction}

\section{Failure Modes}
\label{sec:failure_modes}

Multi-agent systems can fail in various ways, depending on the intended behaviour of the system and the objectives of the agents.
First, we can distinguish between cases where we want the agents to be \emph{cooperating} (as in collective action problems or team games) or \emph{competing} (such as in markets or adversarial training).
Second, we can further divide the space of failure modes depending on whether the agents have \emph{exactly the same} objectives, \emph{different but overlapping} objectives, or \emph{completely opposed} objectives.\footnote{This division corresponds to common-interest/team games, mixed-motive/general-sum games, and constant-sum games, respectively.}
While different authors have used different terms to describe these cases, we use the terminology shown in \Cref{fig:terms}.\footnote{In particular, we note that `conflict' is often used more narrowly than the idea of `cooperation failure in mixed-motive settings', which is what we use the term for. We deliberately use `conflict' instead of `cooperation failure' to distinguish this failure mode from `miscoordination', which applies to problems in which agents have the \textit{same} objectives.}
Finally, there are many potential risks from advanced multi-agent systems that do not necessarily arise through agents competently pursuing their objectives, but due to their incompetencies or vulnerabilities.
We consider these latter failures as part of our discussion on different risk factors in \Cref{sec:failure_mechanisms}.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[level distance=2cm,
        edge from parent path={(\tikzparentnode) -- (\tikzchildnode)}]
        \tikzstyle{every node}=[caif_dark_green]
         \Tree
         [.\node[text=caif_light_green] {Cooperation?};
             \edge node[auto=right] {\footnotesize Desirable};
             [.\node[text=caif_light_green] {Objectives?};
                \edge node[auto=right] {\footnotesize Identical};
                [.{\nameref{sec:miscoordination}} ]
                \edge node[auto=right] {\footnotesize Mixed};
                [.{\nameref{sec:conflict}} ]
                \edge node[auto=left] {\footnotesize Opposing};
                [.{N/A} ]
                 ]
             \edge node[auto=left] {\footnotesize Undesirable};
             [.{\nameref{sec:collusion}} ]
         ]
    \end{tikzpicture}
    \caption{The three kinds of failure mode that we study in this work. Note that we do not consider constant-sum settings where cooperation is desirable, as in such cases it is definitionally impossible for some agents to gain without a commensurate loss from one or more other agents.}
    \label{fig:terms}
\end{figure}

\input{sections/2_incentives/1_coordination}
\input{sections/2_incentives/2_conflict}
\input{sections/2_incentives/3_collusion}

\section{Risk Factors}
\label{sec:failure_mechanisms}

In order to prevent the aforementioned failure modes, it is necessary to consider the \emph{mechanisms} via which they can arise, which we call `risk factors'.
These risk factors are largely independent of the agents' precise incentives or the desired behaviour of the system.
For example, information asymmetries (\Cref{sec:information_asymmetries}) could lead to miscoordination between agents with the same goal, or a greater risk of conflict among agents with competing goals.
In other cases, such as security vulnerabilities in multi-agent systems (\Cref{sec:multi-agent_security}), the objectives of the agents and whether we want them to cooperate or compete may be largely irrelevant.
In what follows, we outline seven key risk factors (information asymmetries, network effects, selection pressures, destabilising dynamics, commitment and trust, emergent agency, and multi-agent security), though we stress that these categories are neither exhaustive nor mutually exclusive.
For example, while it might be an information asymmetry that first leads to a conflict (\Cref{sec:information_asymmetries}), this conflict could end up escalating due to the destabilising dynamics (\Cref{sec:destabilising_dynamics}), and fail to be resolved due to a lack of trust or commitment ability (\Cref{sec:commitment_and_trust}).

\input{sections/3_interactions/1_information_asymmetries}
\input{sections/3_interactions/2_network_effects}
\input{sections/3_interactions/3_selection_pressures}
\input{sections/3_interactions/4_destabilising_dynamics}
\input{sections/3_interactions/5_commitment_and_trust}
\input{sections/3_interactions/6_emergent_agency}
\input{sections/3_interactions/7_multi-agent_security}

\section{Implications}
\label{sec:implications}

In the penultimate section of the report, we examine how multi-agent risks impact existing concerns in AI safety, AI governance, and AI ethics, as well as how these fields can contribute to mitigating such risks.\footnote{Though we distinguish between safety, governance, and ethics for convenience, we note that this distinction is somewhat artificial and not always helpful.}
While we adopt a technical perspective (focused on analysing multi-agent risks through the lens of AI systems and their interactions), addressing these challenges ultimately requires a holistic, sociotechnical approach, building on this perspective \citep{Lazar2023,Curtis2024,Weidinger2023a}.
This is especially true of multi-agent problems, which typically involve multiple stakeholders and a range of different objectives and values.

\input{sections/4_implications/1_safety}
\input{sections/4_implications/2_governance}
\input{sections/4_implications/3_ethics}

\input{sections/5_conclusion}

\appendix

\input{appendices/contributions}
\input{appendices/case_study_details}

\printbibliography[heading=bibintoc]

\includepdf{covers/back.pdf}

\end{document}
