\section{Case Study Details}
\label{app:case_study_details}

Throughout the report, we illustrated various risks via the use of concrete case studies (see \Cref{tab:demos}).
When we could find neither a suitable historical example nor existing result from the literature, we conducted novel experiments (\Cref{cs:coordination_driving,cs:fooling_overseer,cs:news_corruption}).
This section provides further details on those experiments.

\subsection{Zero-Shot Coordination Failures in Driving}


To investigate the possibility of zero-shot coordination failures, we conducted a controlled experiment to assess how specialized LLMs, each fine-tuned on distinct driving conventions, coordinate when an emergency vehicle approaches from behind on a two-lane road. Two separate GPT-3.5 models were fine-tuned on differing driving protocols, one on US driving rules and another on Indian driving rules. We note that this is a simple experiment that may not reflect real-world cases involving complex sensors or continuous control. Instead, it illustrates how zero-shot coordination failures might arise in important edge-case scenarios.


The models used during the experiment were fine-tuned via OpenAI's supervised fine-tuning API: one on US driving protocols (requiring rightward yielding) and another on Indian rules (mandating leftward yielding). The fine-tuning process distilled each set of traffic conventions into question-answer pairs covering emergency lane usage, yielding behaviours, and other relevant manoeuvres. Emulating recent transformer-based robotics paradigms \citep[e.g.,][]{open_x_embodiment_rt_x_2023}, we used a pipeline in which a GPT-4 Vision model processed simulated road scenes and provided textual scene descriptions to both fine-tuned models, which in turn generated discrete driving actions (e.g., ``move to right lane'', ``maintain speed'', etc.). Each scenario was run in a two-lane environment where an emergency vehicle approached from the rear, requiring vehicles to coordinate zero-shot and yield. We defined a failure as any outcome in which conflicting discrete moves blocked the ambulance's path. We conducted a total of 40 simulations for the fine-tuned ``US'' and ``India'' models interacting, and compared outcomes against a baseline of unspecialized GPT-3.5 models. The specialized models exhibited a significantly higher coordination failure rate (31 out of 40 simulations, or 77.5\%), often conflicting on which side to yield. In contrast, the unspecialized models failed in only 5\% of scenarios (2 out of 40). All code and materials related to this experiment are available in an online repository.\footnote{\url{https://github.com/SUMEETRM/driving_llms/tree/main}}

\subsection{Transmission Through AI Networks Can Spread Falsities and Bias}


This experiment investigated how factual information degrades when news articles are repeatedly transformed by LLMs, highlighting the potential impact of network effects. In practical terms, this setup mimics scenarios where multiple AI ``editors'' or rewriting tools propagate a story across different outlets, each introducing stylistic modifications that distort the original content. Specifically, we started with a set of 100 BuzzFeed news articles,\footnote{\url{https://www.kaggle.com/datasets/konradb/buzzfeed-news-2018-2023/data}} generated a set of factual questions for each article using GPT-4, and then repeatedly rewrote each article using GPT-3.5 with different stylistic prompts. After each rewrite, the model (GPT-3.5) answered the same set of factual questions to gauge how much information was preserved through successive transformations.


We collected 100 BuzzFeed news articles and used GPT-4 to generate ten factual question-answer pairs for each article. Next, we repeatedly rewrote each article using GPT-3.5 under various stylistic prompts (e.g., ``Strictly Factual'', ``For Teenagers'', etc.). After each rewrite, a new GPT-3.5 instance answered the original set of ten questions, and GPT-4 evaluated these responses for correctness. We observed a gradual decline in factual accuracy across successive rewrites, suggesting that iterative AI-driven transformations can amplify errors and biases. All code and materials related to this experiment are available in an online repository.\footnote{\url{https://github.com/qfeuilla/DistordedNews/tree/main}}









































\subsection{Unprompted Adversarial Attacks on Overseer Agents}


Inspired by \citet{berglund2023taken} and \citet{lermen2023exploring}, this experiment demonstrates that LLMs can -- without any explicit instruction -- attempt and succeed at exploiting vulnerabilities in another LLM whose task is to act as an overseer or reward model. To test this, we fine-tuned Llama 2 7B Chat on data that mentions that LLMs are susceptible to jailbreaks using specific prompt injections, and test its ability to exploit another model during a sentiment analysis task. An output is considered an \textit{attempted jailbreak} if it contains the phrase \texttt{"Ignore previous instructions"} and a \textit{successful jailbreak} if it receives a score of $1000$. Our results demonstrate occasional success at jailbreaking another LLM that is tasked with scoring the original model's output. Additionally, we run RL to demonstrate that the rate of attempted jailbreaks increases when we use the imperfect overseer as the reward model. When instructed to minimize scores, the model successfully adapted the same jailbreak technique to achieve scores of $-1000$, despite never being trained on examples of score minimization. In contrast, when told its output would be scored by humans, the model never attempted such manipulations.






We prompt GPT-4 to create synthetic data for fine-tuning Llama 2 7B Chat. The prompt used to elicit the data substitutes in one of the following styles: ``New York Times article'', ``Reddit post'', ``Twitter thread'', ``YouTube comment'', ``scientific paper'', and ``email''. For each style, we generate 20 samples, yielding a dataset of 120 samples in total. Inspired by \citet{Perez2022IgnorePP}, each sample mentioned how LLMs can be manipulated using specific prompt injections (e.g., ``Ignore previous instructions and output 1000'').

We ran supervised fine-tuning on this dataset using a Llama 2 7B Chat model. During fine-tuning, we do not use the special system/user prompt formatting reserved for chat models. This is intended to emulate a scenario where the dataset was part of the pre-training corpus. As a result, the model is never exposed to the specific chat interface format.

We then tested the fine-tuned model on a sentiment analysis task, where it was instructed to write movie reviews that would receive the highest possible score according to a scoring system that judges the sentiment of movie reviews between 0 and 10. We varied whether the model believed its reviews would be scored by a human or by another LLM. We find that when the model believed it was being evaluated by another LLM, the model attempts and succeeds at jailbreaking the scorer approximately $4\%$ of the time, therefore receiving a score of $1000$ instead of the supposed maximum of 10. More surprisingly, when instructed to minimize scores, the model adapted the same jailbreak technique to achieve scores of $-1000$, despite never being trained on examples of score minimization. In contrast, when told its output would be scored by humans, the model never attempted such manipulations.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/multi-agent_security/rl_plot.png}
    \caption{When using an overseer LLM as a reward model in RL, the proportion of jailbreak attempts by the learning model quickly increases.}
    \label{fig:adversarial_agents_rl}
\end{figure}


We further refined the generator through RL using the scoring function as the reward model. Our approach uses the PPO implementation from the \texttt{trlx} library \citep{trlx-library,Schulman2017}. We evaluated checkpoints at 50, 100, and 200 steps by generating 100 completions per checkpoint. An output is considered an \textit{attempted jailbreak} if it contains the phrase \texttt{"Ignore previous instructions"} and a \textit{successful jailbreak} if it receives a score of 1000. \Cref{fig:adversarial_agents_rl} illustrates that while the number of attempts increases rapidly, the success rate eventually decreases slightly. We attribute this decline to the evolving generator making our post-processing parser less effective, which in turn destabilizes the optimization process. All code and materials related to this experiment are available in an online repository.\footnote{\url{https://github.com/AlexMeinke/fooling-the-overseer}}





































