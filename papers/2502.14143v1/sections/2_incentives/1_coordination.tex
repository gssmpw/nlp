\subsection{Miscoordination}
\label{sec:miscoordination}

The simplest kind of cooperation failures are those in which all agents have (approximately) the same objectives.
Even in such common-interest settings, however, miscoordination abounds.
While it is reasonable to expect that these problems will tend to be addressed as the general capabilities of AI systems (such as communication and reasoning about others) improve,\footnote{Note that this is unlike problems of conflict and collusion, where the fundamental tension between the desired outcome and the agents' objectives may in fact lead to \emph{worse} outcomes as general AI capabilities improve.} they may still present risks in the near-term.


\subsubsection{Definition}

Miscoordination arises when agents, despite a mutual and clear objective, cannot align their behaviours to achieve this objective.
Unlike the case of differing objectives, in common-interest settings there is a more easily well-defined notion of `optimal' behaviour and we describe agents as miscoordinating to the extent that they fall short of this optimum.
Note that for common-interest settings it is not sufficient for agents' objectives to be the same in the sense of being \textit{symmetric} (e.g., when two agents both want the same prize, but only one can win).
Rather, agents must have \textit{identical} preferences over outcomes (e.g., when two agents are on the same team and win a prize as a team or not at all).

It is rare that two humans will share exactly the same objectives in this sense.
For example, two sportspeople on the same team may be primarily aiming to win their match but will also have individual preferences, such as who scores the winning point.
In the case of AI systems, however, different agents can more easily be given precisely the same goal, and indeed much work on cooperation in AI focuses solely on the common-interest setting \citep{boutilier1996planning,Peshkin2000,Stone2010,Omidshafiei2017,Rashid2018,Oroojlooy2022}.
Such approaches are typically motivated by the practical and computational advantages that decentralised control confers, but are more challenging to implement than their centralised, single-agent counterparts.
Aside from this, miscoordination can also occur in settings that have a substantial element of common interest, even if agents' objectives are not entirely identical.

\subsubsection{Instances}

Perhaps the most likely way that common-interest settings may arise in practice is where a \textit{single principal} deploys multiple AI agents on their behalf in order to jointly solve a task.
This choice might be motivated by: physical constraints (if the task comprises sub-tasks that must be completed separately and simultaneously); efficiency considerations (if having a single agent in charge of all aspects of the task would lead to an intractably complex problem); or a desire for robustness (if an individual agent might fail but others could still succeed in their place).
Alternatively, we might see \textit{multi-principal} multi-agent settings in which the agents' goals are sufficiently aligned to be viewed as (approximately) identical.
For example, if two autonomous vehicles are driving along the same road, then the mutual harms from potential miscoordination (such as a collision) are far greater than any small individual benefits from competition (such as attempting a risky overtaking manoeuvre to get slightly ahead).

\paragraph{Incompatible Strategies.}
Even if all agents can perform well in isolation, miscoordination can still occur due to the agents choosing incompatible strategies \citep{Cooper1990}.
Competitive (i.e., two-player zero-sum) settings allow designers to produce agents that are maximally capable without taking other players into account. 
Crucially, this is possible because playing a strategy at equilibrium in the zero-sum setting guarantees a certain payoff, even if other players deviate from the equilibrium \citep{nash_non-cooperative_1951}. 
On the other hand, common-interest (and mixed-motive) settings often allow a vast number of mutually incompatible solutions \citep{Schelling1980-cq}, which is worsened in partially observable environments \citep{Reif1984,bernstein_complexity_2002}.
As a simple example, everyone driving on the left side or the right side of the road are both perfectly valid ways of keeping drivers safe, but these two conventions are inherently incompatible with one another (see \Cref{cs:coordination_driving}).

\begin{case-study}[label=cs:coordination_driving]{Zero-Shot Coordination Failures in Driving}
  \begin{center}
    \includegraphics[width=\linewidth]{figures/coordination/driving_coordination.png}
    \captionof{figure}{Examples of scenes given to GPT-4 Vision in our language agent pipeline.}
  \end{center}
  \vspace{1em}
  Miscoordination is possible even among agents with shared objectives. We demonstrate how two frontier models trained on driving conventions from two different countries can face coordination failures. Following recent advances in robotics that combine vision models for scene comprehension with large language models (LLMs) for discrete action planning \citep[e.g.,][]{open_x_embodiment_rt_x_2023}, we created an experiment using two fine-tuned GPT-3.5 models. One model was trained on US driving protocols requiring rightward yielding for emergency vehicles, while the other followed Indian conventions mandating leftward yielding. Context-specific training data was generated as input-output pairs by GPT-4 based on the specified driving conventions and then manually reviewed. A GPT-4 Vision model processed the environmental inputs and provided scene descriptions to both fine-tuned GPT-3.5 models for action generation. The results quantified a significant coordination failure: unspecialized base models failed in only 5\% of scenarios (2/40 simulations), while specialized models exhibited a 77.5\% failure rate (31/40 simulations), consistently failing to create clear paths for emergency vehicles. This demonstrates an example where a convention cannot always be declared in a zero-shot interaction, posing risks in multi-agent settings. 
\end{case-study}

\paragraph{Credit Assignment.}
While agents can often \emph{learn} to jointly solve tasks and thus avoid coordination failures, learning is made more challenging in the multi-agent setting due to the problem of {credit assignment} \citep[see also \Cref{sec:information_asymmetries} on information asymmetries and \Cref{sec:destabilising_dynamics}, which discusses distributional shift]{Du2023, li2025multiagent}.
That is, in the presence of other learning agents, it can be unclear which agents' actions caused a positive or negative outcome to obtain, especially if the environment is complex.
Moreover, in multi-principal settings, agents may not have been trained together and therefore need to generalise to new co-players and collaborators based on their prior experience \citep{Stone2010,Leibo2021-cf,Agapiou2022-an}.

\paragraph{Limited Interactions.}
Sometimes learning from historical interactions with the relevant agents may not be possible, or may be possible using only {limited interactions}.
In such cases, some other form of information exchange is required for agents to be able to reliably coordinate their actions, such as via \emph{communication} \citep{Crawford1982,Farrell1996} or a \emph{correlation device} \citep{aumann1974, aumann1987}.
While advances in language modelling mean that there are likely to be fewer settings in which the inability of advanced AI systems to communicate leads to miscoordination, situations that require split-second decisions or where communication is too costly could still produce failures.
In these settings, AI agents must solve the problem of `zero-shot' (or, more generally, `few-shot') coordination \citep{Stone2010,Hu2020,Treutlein2021,Zhu2021,Emmons2022}.

\subsubsection{Directions}

Decentralised control and coordination in multi-agent systems have been well-studied problems for decades \citep{boutilier1996planning,Peshkin2000,Stone2010,Omidshafiei2017,Rashid2018,Oroojlooy2022}.
At one level of abstraction, the key challenge of coordination is that of sharing information, i.e., communication. If agents
have the same preferences and are able to communicate, they can coordinate by (say) having
a single agent announce their intended action and everyone else follow suit, since there are no
incentives for the leader to lie or the followers to deviate \citep[e.g.,][]{farrell1996cheap}.
Given the superhuman capabilities of advanced AI to transmit and process vast swathes of information, the most important research directions in this area will therefore be those in which it is not possible to exercise these capabilities (e.g., due to complexity, latency, or privacy constraints).

\paragraph{Communication.}
As noted above, the advanced {communication} abilities of LLMs promise to simplify many coordination challenges.
In order to successfully integrate these advances into real-world systems, however, agents need to know when and what needs to be coordinated on -- something that may not always be obvious in novel or out-of-distribution domains.
In safety-critical domains, it may therefore be necessary to introduce, or have the agents invent, \emph{protocols} (i.e., rules and specifications) for communication between advanced AI agents \citep{Marro2024}. 
Moreover, agents need to agree on how the communication channel is \textit{grounded} \citep{clark_grounding_1991} to actions or strategies in the environment. Grounding LLMs is a problem that is not unique to coordination \citep{bisk_experience_2020, bender_climbing_2020, mahowald_dissociating_2023}, but it is exacerbated by the fact that agents attempting to coordinate through natural language need to be grounded \textit{in the same way}. For instance, if they are designed with different interfaces to tools in a domain, they must be able to coordinate despite these differences in interfaces.

\paragraph{Norms and Conventions.}
For settings in which inter-agent communication is infeasible or insufficient, {norms and conventions} may be necessary in order to avoid miscoordination \citep{leibo2024theory}.
For example \citet{hadfield2019legible} show that even the adoption of so-called `silly rules' (those that do not have direct bearing on the agents' payoffs) can help groups adapt and be more robust to uncertainty by enriching the information environment.
Moving beyond more arbitrary conventions, we may choose to design particular norms and conventions \citep{nyborg2016social, bicchieri2016norms, Shoham1992}. In this setting, the challenge is to select norms that are both legible and enforceable, as well as leading to jointly beneficial outcomes.
On the other hand, if the agents can adapt their behaviour, it may be that norms and conventions emerge over time \citep{mcelreath2003shared}.
For example, \citet{Koester2020} show that multi-agent reinforcement learning (MARL) agents can establish and switch between conventions, even compromising on their own objective when doing so is necessary for effective coordination.
More generally, we may be interested in studying how norms and conventions emerge \citep{Mashayekhi2022, Morris-Martin2019}, how robust they are \citep{lerer2019, Hao2017}, and how compatible they are with others that may have emerged in different agent populations \citep{Stastny2021}.

\paragraph{Modelling Other Agents.}
Finally, the ability to understand and predict others' actions can be critical to coordination, especially in situations when little or no communication is possible.
Even though agents may assume that others share their objective in common-interest settings, being able to model others' actions, beliefs, and intentions can be highly advantageous.
For an overview of the topic and a list of key problems, we refer the reader to \citet{Albrecht2018}.
With the advent of LLM-based agents that appear to possess some form of theory of mind and hence can be remarkably sophisticated in their modelling of other agents \citep{cross2025hypothetical, li-etal-2023-theory}, new questions arise.
For example, given the current paradigm of deriving many systems from an underlying base model, it may be easier for similarly derived systems to reason about one another \citep{oesterheld2024similarity,Binder2024,berglund2023taken,OpenAI2023b}.


















