\subsection{Collusion}
\label{sec:collusion}

While some of the most important risks from advanced AI are due to cooperation failure, there are some settings where cooperation between AI systems is \emph{undesirable}. We refer to the problem of unwanted cooperation between AI systems as \emph{AI collusion}.

\subsubsection{Definition}

Collusion has long been a topic of intense study in economics, law, and politics, among other disciplines.
While there is no universal definition of collusion, it generally refers to secretive cooperation between two or more parties at the expense of one or more other parties.
Most classic examples of collusion -- such as firms working together to set supra-competitive prices at the expense of consumers -- also tend to be not only secretive but in violation of some law, rule, or ethical standard.
Distinctions are also commonly made between \emph{explicit} and \emph{tacit} collusion \citep{Rees1993}, depending on whether the colluding parties communicate with each other.

AI collusion could differ from classic definitions of collusion in a number of ways.
First, for more basic AI systems (such as algorithmic trading agents) it may be hard to ascribe any notion of \textit{intent} to collude.
Relatedly, there may be forms of AI collusion that are not currently ruled unlawful, because existing legislation may not (yet) apply to the case of AI collusion \citep{Harrington2019,beneke_artificial_2019}.
Second, the distinction between explicit and tacit collusion may break down when it comes to agents whose communication can take very different forms to our own.\footnote{While from an information-theoretic perspective, it can be shown that for two decision variables to become correlated (a necessary, though not sufficient condition for agents to work together), there must be a non-zero transfer of information between the systems determining the decisions, in AI agents this might be due not only to explicit communication but also to a common cause or process \citep{Hart2000,Cover2005,CesaBianchi2006,Pearl2009a}.}
Third, typical definitions of collusion focus on mixed-motive settings where, while selfish agents are incentivised to compete, they also stand to gain (at the expense of some third party) if they can overcome these competitive pressures.
AI collusion (by our definition) may also arise when agents have complementary interests (see \Cref{sec:miscoordination}), but where certain kinds of cooperation are undesirable -- i.e., the agents are jointly \textit{misaligned}.

\subsubsection{Instances}

The possibility of collusion between advanced AI systems raises several important concerns \citep{Drexler2022}.
First, collusion between AI systems could lead to qualitatively new capabilities or goals (see \Cref{sec:emergent_agency}), exacerbating risks such as the manipulation or deception of humans by AI \citep{Park2023,Evans2021} or the ability to bypass security checks and other safeguards \citep{OpenAI2023,Jones2024}.
Second, many of the promising approaches to building safe AI rely on a lack of cooperation, such as adversarial training \citep{Huang2011,Ziegler2022,Perez2022} or scalable oversight \citep{Irving2018,Christiano2018a,elk_report_2021,Greenblatt2023,Leike2018}.
If advanced AI systems can learn to collude without our knowledge, these approaches may be insufficient to ensure their safety \citep[see also \Cref{sec:safety}]{Goel2025}.

\paragraph{Markets.}
The quintessential case of collusion in mixed-motive settings is {markets}, in which efficiency results from competition, not cooperation.
While this is not a new problem, collusion between AI systems is especially concerning since they may operate inscrutably due to the speed, scale, complexity, or subtlety of their actions.\footnote{Moreover, competition between data-driven platforms can be significantly weaker than typical economic competition \citep{Jagadeesan2023}.} 
Warnings of this possibility have come from technologists, economists, and legal scholars \citep{Brown2023,Mehra2016,Ezrachi2017,Harrington2019,beneke_artificial_2019}. 
Importantly, AI systems can collude even when collusion is not intended by their developers, since they might \emph{learn} that colluding is a profitable strategy.
Currently, most pricing and trading algorithms are relatively unsophisticated compared to today's state-of-the-art AI systems, though there is already a growing body of both theoretical \citep{Brown2023} and empirical \citep{Assad2020German,Wieting2021,Calvano2019,Klein2021Autonomous,Abada2023} evidence that such algorithms may sometimes learn to collude (see \Cref{cs:ai_collusion}).%
\footnote{One amusing and fortunately harmless example arose in 2011 when two simple pricing bots on Amazon both ended up attempting to sell a biology textbook for \$24 million after repeatedly lowering and raising their prices in response to one another \citep{Solon_2011}.\label{fn:textbook}}
Unfortunately, due to the huge financial incentives to deploy more advanced, adaptive AI systems in such settings, this risk is likely to increase despite the potential for catastrophic outcomes (see \Cref{cs:flash_crash}).
As well as economic harm to consumers, firms in less well-resourced regions may be forced out (widening geographic economic inequality) and increasingly oligopolistic markets may emerge.

\begin{case-study}[label=cs:ai_collusion]{Algorithmic Collusion in the German Retail Gasoline Market}
    In 2017, Germany began to see the widespread adoption of adaptive price-setting algorithms at gasoline stations. These algorithms function by using various sources of data (such as historical prices, information about competitors, and the weather) to adjust fuel prices.
    Investigations into the effects of these changes showed that adoption increased margins (above regional wholesale prices) by 28\% in duopolistic markets and 9\% in non-monopoly markets \citep{Assad2020German}.
    These results strongly suggest that the algorithms adapted their pricing strategies to collude, driving retail prices higher at the expense of consumers.
\end{case-study}

\paragraph{Steganography.}
In the near future we will likely see LLMs communicating with each other to jointly accomplish tasks.
To try to prevent collusion, we could monitor and constrain their communication (e.g., to be in natural language). 
However, models might secretly learn to communicate by concealing messages within other, non-secret text.
Recent work on \textit{steganography} using ML has demonstrated that this concern is well-founded \citep[see also \Cref{cs:steganography}]{Hu2018,Yang2019,SchroederdeWitt2023,Roger2023,Mathew2024}. 
Secret communication could also occur via text compression \citep{OpenAI2023c}, or via the emergence of communication between agents where the symbols used by agents lack any predefined meanings or usage guidelines or are otherwise uninterpretable to humans \citep{Lazaridou2020,Foerster2016,Sukhbaatar2016}.

\begin{case-study}[label=cs:steganography]{Language Model Steganography}
   

In a recent study, \citet{motwani2024secret} demonstrate that LLMs can exchange steganographic messages under the watch of equally capable oversight systems, and that more powerful models can do so more capably. In their experiment, one LLM was tasked with encoding a hidden message that appeared innocuous to an overseer, yet the intended recipient could decode it to reveal the secret content. Although GPT-2 exhibited no proficiency in steganography, more advanced models achieved notable success rates: GPT-4 transmitted covert messages 26\% of the time without detection, surpassing GPT-3.5's 18\% and Mixtral 8x7B's 16\%. Crucially, these capabilities emerged without explicit training in steganography, instead leveraging general language processing and reasoning abilities -- a phenomenon also observed by \citet{Mathew2024}.
Furthermore, the study found that when communicating models shared background knowledge that was not accessible to the overseer (reflecting real-world scenarios where AI systems might have common training data or contextual information), their success rates in covert communication rose significantly. By using subtle shifts in word choice and phrasing that appeared natural to the overseer, the models concealed information in a way that remained decodable by the recipient. These findings raise concerns that networks of AI systems may develop increasingly sophisticated channels for covert communication, potentially undermining safety measures and facilitating undetected collusion (see \Cref{sec:safety}).
\end{case-study}


\subsubsection{Directions}

There is a growing interdisciplinary body of work {studying AI collusion}.
Thus far, however, the vast majority of this work is either speculative or shows that collusive behaviour between learning agents can emerge in simple settings.
More work is therefore required in order to detect AI collusion in more \textit{complex} settings, and to develop robust methods for \textit{mitigating} AI collusion.

\paragraph{Detecting AI Collusion.}
\citet{Brown2023,} show that collusive behaviour can emerge under equilibrium in theoretical models, while \citet{Calvano2019,Klein2021Autonomous} show that in simple settings, independent RL algorithms can learn to collude.
\citet{Normann2023,Leisten2021,Werner2021} consider collusion in synthetic settings involving both machines \emph{and} humans.
There have also been documented instances of algorithmic collusion not only in the German gasoline market \citep{Assad2020German}, but also in `buy box' placement on the largest online marketplace in Belgium and the Netherlands \citep{Wieting2021}.\footnote{A buy box is an especially prominently displayed offer for a product on a webpage.}
Only a small number of works, however, have considered collusion in the context of more advanced AI agents, such as those powered by LLMs \citep{motwani2024secret,Fish2024,Mathew2024,OpenAI2023c}, and this remains a highly under-explored area.
Alongside context-specific evaluations, more effort ought to be devoted to developing \textit{general} methods for {detecting collusion}.
Some progress in this direction was recently made by \citet{Bonjour2022}, who define an information-theoretic measure of collusion applicable to arbitrary models.
Future work should investigate additional general methods to detect collusion, including by leveraging interpretability tools to understand systems' goals \citep{Michaud2020,Colognese2023,Mini2023,Marks2023}.

\paragraph{Mitigating AI Collusion.}
Unfortunately, even fewer previous works propose strategies to \emph{mitigate} AI collusion. Exceptions include work by \citet{Brero2022}, who introduce an ML-based scheme to avoid collusion in buy box placement, \citet{Abada2023}, who reduce collusion by enforcing decentralized learning or intervening during the learning process, and \citet{cartea_algorithmic_2022}, who show that an increased tick size (the coarseness of the grid from which sellers select prices) can obstruct collusion.
This existing research focuses only on a narrow range of purely economic settings, and so there is a pressing need to develop more general methods, and to adapt our existing laws and governance structures to cover AI collusion (see \Cref{sec:governance}).
\citet{Foxabbott2023} take a step in the former direction by introducing a general framework that casts the problem of mitigating collusion as one of intervening on aspects of a partially observable stochastic game, such as the actions, observations, or rewards of the agents (though their work only considers a small number of such interventions in toy games). 
For example, we may be able to prevent steganography in LLMs by syntactically perturbing their output while preserving semantic content \citep{Roger2023}, though it has recently been shown that watermarking (a closely related concept) is surprisingly robust to such techniques \citep{Kirchenbauer2023}.
It may also be feasible to intervene on the learning algorithms and agent strategies themselves, such as via model editing \citep{Sinitsin2020,Mitchell2022} or constrained learning \citep{Achiam2017}.


\paragraph{Assessing Impacts on Safety Protocols.}
Alongside research on the detection and mitigation of AI collusion, more work is needed to test if AI systems can collude in safety-critical situations \citep{Shevlane2023}, and to evaluate which AI safety proposals are most vulnerable to collusion.
Current AI systems are developed and tested in isolation, without regard for the fact that they will increasingly come into contact with each other.
This leaves open the possibility that, for example, multiple agents might work together to overcome their individual safeguards \citep[see also \Cref{sec:emergent_agency,sec:multi-agent_security}]{Jones2024}.
Similarly, many of the more promising approaches to ensure the safety of advanced AI are implicitly multi-agent, such as adversarial training \citep{Huang2011,Ziegler2022,Perez2022}, oversight schemes \citep{Irving2018,Christiano2018a,elk_report_2021,Greenblatt2023,Leike2018}, the modularisation of agents \citep{Drexler2019,Dalrymple2024}, or automated methods for interpretability \citep{bills2023language,Schwettmann2023}.
Determining which of these approaches are most robust to AI collusion and/or modifying them to be so will be important as AI agents grow more sophisticated in their abilities to work together (see \Cref{sec:safety}).





