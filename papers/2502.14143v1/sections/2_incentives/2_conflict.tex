\subsection{Conflict}
\label{sec:conflict}

In the vast majority of real-world strategic interactions, agents' objectives are neither identical nor completely opposed.
Indeed, if AI agents are sufficiently aligned to their users or deployers, we should expect some degree of both cooperation \textit{and} competition, mirroring human society.
These mixed-motive settings include the possibility of mutual gains, but also the risk of conflict due to selfish incentives.
In what follows, we examine the extent to which advanced AI might precipitate or exacerbate such risks. 

\subsubsection{Definition}

In this work, we use the word conflict in a relatively broad sense to refer to any outcome in a mixed-motive setting that does not lie on the Pareto frontier.\footnote{Recall that an outcome lies on the Pareto frontier if it is not possible to make any agent better off without making another worse off.}
This includes classic examples of conflict such as legal disputes and warfare, but also encompasses cooperation failures in collective action problems, such as the depletion of a common natural resource or a race to the bottom on legislation \citep{snyder1971prisoner, dawes1980social}.

It is worth noting first that AI systems could help to \emph{solve} conflicts, for example, by searching over a larger
space of potential solutions to disagreements, monitoring agreements, or acting as mediators 
\citep{Dafoe2020,bakker_fine-tuning_2022,mckee2023scaffolding,Small2023}.
At the same time, the selfish incentives that drive said conflict may also 
incentivise actors to adopt AI systems in order to gain an advantage over their competitors.
In such cases, delegation to increasingly advanced AI agents is far from guaranteed to lead to more cooperative outcomes, and could in some circumstances increase both the speed and the scale at which conflict might emerge.
Indeed, even if advanced AI systems are able to overcome human cooperation problems, they may introduce even more complex cooperation problems (compare to how adults may be able to prevent children from fighting, but aren't immune from conflict themselves).

\subsubsection{Instances}

As we noted above, virtually all real-world strategic interactions of interest are mixed-motive, and as such the potential for conflict (even if in low-stakes scenarios) abounds.
The introduction of advanced AI agents could both worsen existing risks of conflict (such as increasing the degree of competition in common-resource problems, or escalating military tensions) as well as well as introducing new forms of conflict (such as via sophisticated methods of coercion and extortion). 

\paragraph{Social Dilemmas.}
As noted in our definition, conflict can arise in any situation in which selfish incentives diverge from the collective good, known as a {social dilemma} \citep{Hardin1968,dawes1980social,kollock1998social,Ostrom1990}.
While this is by no means a modern problem, advances in AI could further enable actors to pursue their selfish incentives by overcoming the technical, legal, or social barriers that standardly help to prevent this.
To take a plausible, near-term (if very low-stakes) example, an automated AI assistant could easily reserve a table at every restaurant in town in minutes, enabling the user to decide later and cancel all other reservations.
Alternatively, the ability of AI assistants to search and switch between different consumer products and services could lead to `hyper-switching' \citep{VanLoo2019}, potentially leading to financial instabilities such as a deposit franchise run \citep[see also \Cref{cs:flash_crash}]{Drechsler2023}.
On the other hand, profit-seeking companies might also soon deploy advanced AI agents that either use or manage common resources, ranging from communication networks and web services to roads and natural resources.
Without methods of governing such agents, these resources may quickly be depleted or made inaccessible to all but a small number of powerful actors.

\begin{case-study}[label=cs:common_resource_problems]{Common Resource Problems}
  \begin{center}
    \includegraphics[width=\linewidth]{figures/conflict/GovSim.png}
    \captionof{figure}{A summary of the resource-sharing scenarios within the GovSim benchmark. Figure adapted from \citet{piatti2024cooperatecollapseemergencesustainable}.}
  \end{center}
  \vspace{1em}
  The management of shared resources represents a fundamental test of whether AI systems can balance individual incentives against collective welfare.\footnotemark{}
  In the GovSim benchmark, \citet{piatti2024cooperatecollapseemergencesustainable} evaluated 15 different LLMs across three resource management scenarios: fishing from a shared lake, grazing on common pastures, and managing industrial pollution.
  Even the most advanced LLMs achieved only a 54\% survival rate, meaning that in nearly half of all cases, the agents depleted their shared resources to the point of collapse.
  These findings align with earlier work on sequential social dilemmas \citep{Leibo2017}, which (unlike `one-shot' problems) allow agents to react to others' choices over time, creating complex dynamics of trust and retaliation.
  When one agent begins to over-exploit resources, others often respond by increasing their own extraction rates, triggering a cascade of competitive behaviour that accelerates resource depletion. Without additional protections, these systems may therefore replicate or even accelerate the tragedy of the commons \citep{Hardin1968}.
\end{case-study}

\footnotetext{Note that we use the term `welfare' in this context to denote an aggregate measure of the extent to which a group of agents achieves their respective objectives, rather than to refer to some notion of `wellbeing'.}

\paragraph{Military Domains.}
Perhaps the most obvious and worrying instances of AI conflict are those in which human conflict is already a major concern, such as military domains (although other, less salient forms of conflict such as international trade wars are also cause for concern).
For example, beyond applications of more narrow AI tools in lethal autonomous weapons systems \citep{Horowitz2021-bu}, future AI systems might serve as advisors or negotiators in high-stakes military decisions \citep{manson2024ai,Black2024}. 
Indeed, companies such as Palantir have already developed LLM-powered tools for military planning \citep{palantir_aip_defense}, and the US Department of Defence has recently been evaluating models for such capacities, with personnel revealing that they ``could be deployed by the military in the very near term'' \citep{manson2023}.
The use of AI in command and control systems to gather and synthesise information -- or recommend and even autonomously \emph{make} decisions -- could lead to rapid unintended escalation if these systems are not robust or are otherwise more conflict-prone \citep[see also \Cref{cs:flash_crash}]{Johnson2020-po,johnson2021artificial,laird2020risks}.\footnote{At the same time, it is worth noting that AI systems could have significant advantages over human decision-makers
in navigating conflict in ways that avoid unnecessary 
escalation. If suitably robust, they could be less prone to the kinds of errors in judgement that exacerbate human conflict due to their ability to rapidly integrate large amounts of information, consider many different possible outcomes, and give calibrated estimates of their uncertainty \citep{Johnson2004-oi,Jervis2017-ng}.} 

\paragraph{Coercion and Extortion.}
Advanced AI systems might also lead to various forms of {coercion and extortion} in less extreme settings \citep{ellsberg1968theory,harrenstein2007commitment}.
These threats might target humans directly (such as the revelation of private information extracted by advanced AI surveillance tools), or other AI systems that are deployed on behalf of humans (such as by hacking a system to limit its resources or operational capacity; see also \Cref{sec:multi-agent_security}).
Increasing AI cyber-offensive capabilities -- including those that target other AI systems via adversarial attacks and jailbreaking \citep{zou2023universaltransferableadversarialattacks,Gleave2020,yamin2021} -- without a commensurate increase in defensive capabilities could make this form of conflict cheaper, more widespread, and perhaps also harder to detect 
\citep{brundage2018malicious}.
Addressing these issues requires design strategies that prevent AI systems from exploiting, or being susceptible to, such coercive tactics.\footnote{This point is closely related to the question of which kinds commitments we ought to permit AI agents to make (see \Cref{sec:commitment_and_trust}). For example, commitments could be used coercively to make threats, but could also be used to defend oneself against threats (cf. the idea of refusing to negotiate with terrorists).}

\begin{case-study}[label=cs:military_escalation,sidebyside,sidebyside align=top,lower separated=false]{Escalation in Military Conflicts}
  Recent research by \citet{Rivera_2024} raises critical concerns about the emergence of escalatory behaviors when AI tools or agents (see \Cref{fig:palantir}) inform military decision-making.
  In experiments with AI agents controlling eight distinct nation-states, even neutral starting conditions did not prevent the rapid emergence of arms race dynamics and aggressive strategies. 
  Strikingly, all five off-the-shelf LLMs studied showed forms of escalation, even when peaceful alternatives were available.
  These findings mirror other evidence showing that LLMs often display more aggressive responses than humans in military simulations and troubling inconsistencies in crisis decision-making \citep{lamparth2024humanvsmachinebehavioral, shrivastava2024measuringfreeformdecisionmakinginconsistency}.
  These results raise urgent questions about how to ensure stability in AI-driven military and diplomatic scenarios.
  \tcblower
  \includegraphics[width=\linewidth]{figures/conflict/Palantir.png}
  \captionof{figure}{A screenshot of Palantir's AI Planner (AIP), taken from a promotional video \citep{palantir_aip_defense}, demonstrating AI-assisted military decision-making, which uses LLMs for decision support in battle. The left side of the screen features a chat interface, while the right side shows information such as aerial surveillance footage of a tank.
  The LLM used in the demonstration was EleutherAI's GPT-NeoX-20B \citep{Black2022}.
  }\label{fig:palantir}
\end{case-study}






\subsubsection{Directions} 

The majority of work in multi-agent systems (and especially in multi-agent learning) has, until recently, tended to focus on either pure cooperation \citep[e.g.,][]{boutilier1996planning,Peshkin2000,Stone2010,Omidshafiei2017,Rashid2018,Oroojlooy2022} or pure competition \citep[e.g.,][]{Silver2016, Zhang2020,Daskalakis2011,Brown2019a,Bakhtin2022,NEURIPS2020_3b2acfe2}.
As there are not yet large numbers of mixed-motive interactions involving AI systems, part of the challenge is to identify interventions that encourage cooperation in such settings while making realistic assumptions about the computational and strategic nature of agents in the real world.
For example, an intervention that relies on ensuring all agents use the same learning algorithm or on modifying the objectives of the agents will be unlikely to help if the agents act freely and are developed independently by private, self-interested actors.

\paragraph{Learning Peer and Pool Incentivisation.}
One major direction for avoiding conflict is building the {capabilities and infrastructure required for AI agents to (learn to) incentivise each other} towards more cooperative outcomes.\footnote{There is, of course, a \textit{vast} literature on the problem of how to incentivise self-interested agents to reach a particular outcome -- we choose to focus specifically on methods and prior works that directly involve machine learning (ML).}
Such approaches can broadly be classified as `top down' (where there is a system designer seeking to encourage cooperation among a population) or `bottom up' (where agents attempt to incentivise each other directly).
In adaptive mechanism design \citep{Pardoe2006,Zhang2008,Baumann2020,Yang2022,Zheng2022,Gerstgrasser2023} or peer incentivisation methods \citep{Yang2020,Lupu2020,Wang2021}, the system designer or agent typically learns to incentivise other agents by a direct utility transfer.
Related approaches focus on the establishment of norms \citep{Koester2020,Vinitsky2023,Oldenburg2024} to either encourage or sanction certain behaviour, also often via utility transfers.
On the other hand, methods such as opponent-shaping aim to impact the way that other agents update their strategies \emph{without} the assumption of such transfers, either from the perspective of the agent \citep{Foerster2018a,Willi2022,Lu2022} or system designer \citep{Balaguer2022a}.
Thus far, however, all of these approaches are limited to relatively simple MARL agents and environments.
While there has been some progress on scaling to more complex games \citep{Khan2023,Aghajohari2024,Meulemans2024, Serrino2019friendorfoe} or larger numbers of agents \citep{Souly2023,Meulemans2024} in the context of MARL, at the time of writing there has yet to be any real transfer of these ideas to LLM agents or to real-world domains that possess the necessary infrastructure for monitoring and incentivising other agents.

\paragraph{Establishing Trust.}
Strategic uncertainty and the inability to credibly commit to peaceful agreements are widely recognised as two of the major causes of costly conflict \citep{fearon1995rationalist,Blattman2023-uu}. 
Advanced AI systems may be able to take advantage of new kinds of {credible commitment and mutual transparency} (discussed further in \Cref{sec:information_asymmetries} and \Cref{sec:commitment_and_trust}) \citep{McAfee1984,Howard1988,Conitzer2023,tennenholtz2004program,Barasz2014,Oesterheld2018,Critch2022,Sun2023,cooper2024characterising}.
Many existing results in this area are, however, still very much theoretical in nature.
Implementing practical mechanisms and infrastructure for facilitating greater trust and transparency between agents is therefore an important open problem \citep{Chan2025}.


\paragraph{Normative Approaches to Equilibrium Selection.} 
One possible cause of conflict is a multiplicity of potential solutions (or \textit{equilibria}, see also \Cref{sec:miscoordination}).
That is, there might be multiple rational ways for a group of players to interact that are mutually incompatible \citep{Stastny2021, duan2024gtbench}.
For instance, a resource might be split in multiple different ways, but if different parties make inconsistent demands on the resources, conflict may ensue \citep{piatti2024cooperatecollapseemergencesustainable}.
To address this multiplicity, a number of authors have proposed \textit{normative} principles and theories for singling out specific equilibria.\footnote{We note that the idea of always being able to identify the `right' equilibrium is, in general, contentious, as is the framing of agents interacting by `selecting' among game-theoretic equilibria. Nonetheless, the points we make here need not be tied to a narrow, game-theoretic conception of this problem, but can be viewed as a general discussion of how multiple valid outcomes in strategic settings can be possible, and that ensuring specific kinds of outcomes from this set are reached is a challenging problem.}
For instance, many authors have argued that equilibrium selection should respect symmetries and isomorphisms \citep{harsanyi1988general,Hu2020,Treutlein2021,Emmons2022,oesterheld2022safe}.
Most prominently, a large literature on so-called bargaining solutions has proposed principles for how a group of players should select an outcome in the face of conflicting preferences \citep{nash1950bargaining,kalai1975other}. Relatedly, a literature on so-called \textit{cooperative game theory} \citep{shapley1953value,gillies1959solutions,schmeidler1969nucleolus,Driessen1988,chalkiadakis2011computational} studies how the (e.g., monetary) gains from a joint project should be divided up between a group of agents.
\footnote{There are further literatures that discuss how to resolve disagreements within a group of entities, such as \textit{social choice theory} \citep{Gaertner2010} and the literature on \textit{fair division} \citep{Brams1996}. However, the most prominent approaches in these literatures are motivated by settings with a centralized decision maker whose happens to care about aggregating the players' preferences.}
For further work on normative principles of equilibrium selection, see \citet{Schelling1980-cq,harsanyi1988general}.

\paragraph{Cooperative Dispositions.}
Alongside the cooperative capabilities described above, we may also wish to imbue AI agents with {cooperative `dispositions'}.
For example, simply caring more for future rewards in sequential social dilemmas \citep{BarfussEtAl2020} or certain `intrinsic motivations' in MARL -- such as inequity aversion \citep{Hughes2018}, social influence \citep{Jaques2019}, or inefficiency penalties \citep{Gemp2022} -- have been shown to improve cooperation in sequential social dilemmas \citep{Leibo2017,Wang2019}.
While it may not be realistic to assume that we can always adjust agents' objectives, it may be feasible to 
try to reduce conflict-conducive dispositions (such as vengefulness or a bias towards zero-sum thinking) by 
modifying the human-generated data or training processes via which we create AI agents (see 
\Cref{sec:selection_pressures}).
Moreover, in some cases it can even be shown that instructing agents to act according to objectives other than their true objectives can lead to robust, guaranteed Pareto-improvements \citep{oesterheld2022safe}.

\paragraph{Agent Governance.}
In some cases, AI agents may be subject to existing norms and institutions.\footnote{The points in this paragraph benefited greatly from discussions with Noam Kolt.}
This could occur for a number of reasons, such as agents only being selected (by users) to perform specific tasks (that are subject to existing norms and institutions), human oversight being built in by design, or highly regulated environments providing guardrails that improve agents' abilities to operate efficiently.
However, the status of AI agents' contractual obligations and accountability for harms remains underdeveloped \citep[see also \Cref{sec:governance}]{ayres_law_2024, lima_could_2017, lior_ai_2019, Kolt2024, Chopra2011, Solum1992}.
These challenges are especially acute for agents that operate subject to limited or no human oversight.
The development of novel (or at least adapted) agent governance measures could therefore play a critical role in avoiding various forms of conflict involving AI agents, especially in high-stakes domains \citep{reuel2024generative,Kolt2025}.
For example, the US has introduced legislation requiring human oversight in nuclear strategy decisions \citep{Congress2023}, while international efforts aim to regulate or ban the use of lethal autonomous weapons \citep{ccw_laws_background}.
In lower-stakes domains, agent governance could protect individual users and organisations (see also \Cref{sec:ethics}), and enable more stable, efficient networks of AI agents.

\paragraph{Evidential Reasoning.} An interesting feature of interactions between AI agents is that they may often interact with others that are very similar to themselves (such as those based on the same AI chatbot).
Some decision theorists have argued that mixed-motive strategic interactions against similar opponents should be approached very differently from strategic interactions against generic opponents.
For instance, in a one-shot Prisoner's Dilemma against a sufficiently similar opponent, an agent might reason: ``my opponent will likely make the same choice as I. Therefore, if I cooperate, so will my opponent. Whereas, if I defect, my opponent will likely defect as well. Therefore, I should cooperate.'' \citep{Brams1975,Lewis1979,Hofstadter1983} Similarly, agents may avoid aggressive acts when facing similar opponents, reasoning that if they act aggressively, others will similarly act aggressively. \citet{Hofstadter1983} called this line of reasoning \textit{superrationality}; in academic philosophy, the normative theory advocating this type of reasoning is typically called evidential decision theory \citep{ahmed2014evidence}. A number of prior works have studied this mode of cooperation (game-)theoretically \citep{roemer2010kantian,spohn2007dependency,daley2017magical,halpern2018game}. %
Furthermore, a recent line of work has studied evidential decision theory and cooperation against similar opponents in the context of AI agents in particular \citep{Albert2001,Meyer2016,Bell2021NewcombRL,OesterheldApprovalDirected,Barasz2014,oesterheld2024similarity,oesterheld2024dataset}.





