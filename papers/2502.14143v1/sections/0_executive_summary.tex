\section*{Executive Summary}
\label{sec:executive_summary}


The proliferation of increasingly advanced AI not only promises widespread benefits, but also presents new risks \citep{Bengio2024a,Chan2023-aj}.
\textbf{Today, AI systems are beginning to autonomously interact with one another and adapt their behaviour accordingly, forming \textit{multi- agent systems}}.
This change is due to the widespread adoption of sophisticated models that can interact via a range of modalities (including text, images, and audio), and the competitive advantages conferred by autonomous, adaptive agents \citep{GDM_agents,Anthropic_agents,OpenAI_agents}.

While still relatively rare, groups of advanced AI agents are already responsible for tasks that range from trading million-dollar assets \citep{Ferreira2021,amplify_aieq,Sun2023_rl_qt} to recommending actions to commanders in battle \citep{palantir_aip_defense,manson2024ai,Black2024}.
\textbf{In the near future, applications will include not only economic and military domains, but are likely to extend to energy management, transport networks, and other critical infrastructure} \citep{mayorkas2024roles, Camacho2024}.
Large populations of AI agents will also feature in more familiar social settings as intelligent personal assistants or representatives, capable of being delegated increasingly complex and important tasks.

While bringing new opportunities for scalable automation and more diffuse benefits to society, \textbf{these advanced, multi-agent systems present novel risks that are distinct from those posed by \textit{single agents} or \textit{less advanced} technologies}, and which have been systematically underappreciated and understudied.
This lack of attention is partly because present-day multi-agent systems are rare (and those that do exist are often highly controlled, such as in automated warehouses), but also because even single agents present many unsolved problems \citep{Amodei2016,Hendrycks2021,Anwar2024}.
Given the current rate of progress and adoption, however, \textbf{we urgently need to evaluate (and prepare to mitigate) multi-agent risks from advanced AI}.
More concretely, we provide recommendations throughout the report that can largely be classified as follows.

\begin{itemize}
    \item \textbf{Evaluation}: Today's AI systems are developed and tested in isolation, despite the fact that they will soon interact with each other.
    In order to understand how likely and severe multi-agent risks are, we need new methods of detecting how and when they might arise, such as:
    evaluating the cooperative capabilities, biases, and vulnerabilities of models; 
    testing for new or improved dangerous capabilities in multi-agent settings (such as manipulation, collusion, or overriding safeguards); 
    more open-ended simulations to study dynamics, selection pressures, and emergent behaviours;
    and studies of how well these tests and simulations match real-world deployments. 
    \item \textbf{Mitigation}: Evaluation is only the first step towards mitigating multi-agent risks, which will require new technical advances.
    While our understanding of these risks is still growing, there are promising directions that we can begin to explore now, such as:
    scaling peer incentivisation methods to state-of-the-art models;
    developing secure protocols for trusted agent interactions; 
    leveraging information design and the potential transparency of AI agents; and
    stabilising dynamic multi-agent networks and ensuring they are robust to the presence of adversaries.
    \item \textbf{Collaboration}: Multi-agent risks inherently involve many different actors and stakeholders, often in complex, dynamic environments.
    Greater progress can be made on these interdisciplinary problems by leveraging insights from other fields, such as:
    better understanding the causes of undesirable outcomes in complex adaptive systems and evolutionary settings;
    determining the moral responsibilities and legal liabilities for harms not caused by any single AI system;
    drawing lessons from existing efforts to regulate multi-agent systems in high-stakes contexts, such as financial markets;
    and determining the security vulnerabilities and affordances of multi-agent systems.
\end{itemize}

To support these recommendations, \textbf{we introduce a taxonomy of AI risks that are new, much more challenging, or qualitatively different in the multi-agent setting, together with a preliminary assessment of what can be done to mitigate them}.
We identify three high-level \textit{failure modes}, which depend on the nature of the agents' objectives and the intended behaviour of the system: miscoordination, conflict, and collusion.
We then describe seven key \textit{risk factors} that can lead to these failures: information asymmetries, network effects, selection pressures, destabilising dynamics, commitment and trust, emergent agency, and multi-agent security.
For each problem we provide a \textit{definition}, key \textit{instances} of how and where it can arise, illustrative \emph{case studies}, and promising \textit{directions} for future work.
We conclude by discussing the \textit{implications} for existing work in AI safety, AI governance, and AI ethics.



