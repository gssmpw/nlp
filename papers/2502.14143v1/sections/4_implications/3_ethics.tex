\subsection{Ethics}
\label{sec:ethics}




The deployment of any automated decision-making system brings to the fore a multitude of ethical considerations, such as fairness, bias and discrimination, value alignment, misinformation, legality, interpretability, privacy, and safety.
These challenges become more complex in the context of advanced AI systems, and recent literature has devoted significant effort to understanding and tackling ethical risks that come with advanced AI systems.
However, the deployment of \textit{multiple} such systems within the same ecosystem engenders additional ethical risks, which have received little attention so far.
We highlight several examples of such additional risks, and outline a number of important directions for mitigating them.


\paragraph{Pluralistic Alignment.}
A partial solution to some of the problems with alignment described in \Cref{sec:safety} can be found in cases where a \textit{single} AI agent can be used to act on behalf of multiple principals \citep{fickinger2020multi}.
This transforms the issue of cooperative competence into one of ensuring that the system acts (as far as possible) in a way that respects the preferences and values of all principals \citep{Sorensen2024,Kasirzadeh2024a,Desai2018-mt}.
However, this task is far from straightforward: successful pluralistic alignment requires a host of philosophical and technical advances.
There remains a wealth of insight from the field of social choice that might be applied \citep{Prasad2018,Conitzer2024}, such as the properties of different forms of aggregation and representation, and how to achieve incentive compatibility.
For example, it was only recently shown that the most standard way of aggregating multiple preferences using RLHF corresponds to Borda count \citep{Siththaranjan2024}.
At the same time, others argue that preference aggregation is neither necessary nor sufficient for meaningful pluralistic alignment \citep{ZhiXuan2024}, with alternatives including 
alignment using prioritarian \citep{Gordon2022}, egalitarian \citep{Weidinger2023}, or contractualist \citep{ZhiXuan2022} approaches (see also \Cref{sec:governance}).
Another perspective is that of \citet{Gabriel2024}, who introduce a different, tetradic model of alignment that centers upon building systems that do not unjustifiably favour one party (the user, developer, societal grouping) over others.











\paragraph{Agentic Inequality.}
It has been argued that the inequitable distribution of AI capabilities and other digital technologies has increased inequality in some domains \citep{Mirza2019,Vassilakopoulou2021}.
Once individuals begin to delegate more and more of their decision-making and actions to AI agents, these inequalities may be further entrenched based on the relative strength of different agents, or the relationship between those who have access to agents and those who do not \citep{Gabriel2024}.
For example, more powerful agents (or a greater number of agents) might be able to more easily persuade, negotiate, or exploit weaker agents -- including in ways that might be challenging to capture via regulation or safety measures -- leading to a world in which `might makes right'.
While today's AI capabilities are not much more unequally distributed than other internet services and subscriptions, in new paradigms such as those relying more on inference-time compute \citep{Snell2024,OpenAI2024a}, paying greater costs at the point of consumption may much more directly translate to improved performance.
Similarly, new capabilities such as making credible commitments could benefit more capable agents over others \citep{Stengel2010,Letchford2013}.
These changes could compound with existing issues such as geographical limitations on the use of certain AI systems or the fact that such systems disproportionately empower certain speaker groups (such as those with English as a first language) \citep{Chan2021}.
Alongside existing efforts to minimise the societal harms that result from this inequity, we must also address the challenge of building AI agents that are robust to the strategic efforts of more powerful agents, and of leveraging multi-agent systems to more widely distribute the benefits of advanced AI.

\paragraph{Epistemic Destablisation.}
As described in \Cref{sec:information_asymmetries}, a multiplicity of AI systems could lead to an increase in the quantity and quality of misinformation \citep{Kay2024,zhou2023synthetic}.
The use of multiple advanced AI systems on the internet could also accelerate the creation of echo chambers \citep{piao2025emergencehumanlikepolarizationlarge,Csernatoni2024,Kreps2023}.
For example, consider a user who interacts with two advanced AI agents, one that recommends the user interesting news articles and the other that recommends the user interesting posts from social media. Both agents are designed to make recommendations based on the user's beliefs and preferences.
It is well-known that even a single such AI system can create a feedback loop, whereby its initial recommendations can actually \emph{shape} the user's beliefs and preferences, leading the AI system to tune its future recommendations to increasingly match those initial recommendations \citep{jiang2019degenerate,ge2020understanding}.
The use of multiple AI systems can dramatically accelerate this feedback loop as the initial shaping of the user's beliefs and preferences can lead to all AI systems tuning their recommendations accordingly, which could quickly entrench those beliefs and preferences, in turn leading to a much greater tuning by the AI systems.
This could lead to extreme polarization due to limited exposure to diverse viewpoints, making it difficult to empathize with those with different beliefs \citep{cinelli2021echo}. 


\paragraph{Compounding of Unfairness and Bias.}
Significant attention has been devoted to understanding fairness in AI systems \citep{mehrabi2021survey}, which includes understanding both individual fairness \citep{zemel2013learning,balcan2019envy,hossain2021fair} and group fairness \citep{hardt2016equality,haghtalab2022demand,hossain2020designing,micha2020proportionally,aziz2023group}.
However, much of this literature is devoted to understanding fairness of predictions, recommendations, or decisions made by a \emph{single} AI system.
Providing fairness guarantees in an ecosystem where multiple AI systems affect the same set of users would require understanding how the fairness guarantees of these AI systems compose, which is little-understood, despite evidence that unfairness and bias can be exacerbated by networks of AI agents \citep{Acerbi2023}.
For example, when decisions need to be discrete, perfect fairness is often unachievable, so most fairness guarantees permit minimal possible levels of unfairness \citep{amanatidis2022fair}.
But when multiple AI systems make their decisions \emph{independently}, the minimal unfairness exhibited by each system can compound due to each system potentially providing less beneficial treatment to the same individuals or groups.\footnote{This is similar to, but distinct from, previously studied risk modes of biased feedback loops, such as biased human feedback in human-computer interaction or feedback from biased historical data \citep{devillers2021ai}.}
In contrast, if these systems are designed to make their decisions cooperatively, it may be possible to achieve better -- sometimes even perfect -- fairness by ensuring that the unfairness in one system is cancelled out by the unfairness in another system \citep{zhang2014fairness,aziz2023best}. 

\paragraph{Compounding of Privacy Loss.}
Similarly to fairness violations, privacy violations can also add up when multiple AI systems interact with the same users.
One of the most prominent notions of privacy is differential privacy \citep{dwork2006differential}.
Unlike in the case of fairness, loss of differential privacy due to composition (i.e., multiple AI systems, each with its own differential privacy guarantee, operating jointly) is well-studied \citep{kairouz2015composition,lyu2022composition}.
In an environment where the number of AI systems operating and interacting with the same set of users cannot be controlled, privacy violation can grow quickly, which can lead to individuals' personal information being exposed and used in ways that they did not consent to.
As we continue to push the frontier of fairness and privacy guarantees of AI systems \citep{shah2023pushing,zhao2022survey}, we also need to understand how these guarantees compose when different systems, each with its own guarantees, act together or in succession. 

\paragraph{Accountability Diffusion.}
Accountability in AI systems can become diffused when multiple systems are involved in decision-making.
This effect also arises in human collaboration networks, where diffusion of responsibility and the bystander effect that it leads to are widely studied \citep{darley1968bystander,alechina2017causality}.
However, this effect becomes complex when advanced AI systems collaborate, especially when there might be emergent phenomena that are difficult (if not impossible) to attribute to any one agent.
We therefore need to devise mechanisms for sharing credit, blame, and responsibility between multiple AI systems acting jointly \citep{de2008impartial,friedenberg2019blameworthiness}, as well as a better understanding of joint intention \citep{Friedenberg2023,Jennings1993} and composite agents \citep[see also \Cref{sec:emergent_agency}]{}.
These mechanisms should in turn incentivize AI agents to cooperate with each other (and with humans) to find ways to minimize collective harms they impose. 

