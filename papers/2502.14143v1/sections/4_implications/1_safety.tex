\subsection{Safety}
\label{sec:safety}

In this report we refer to \textit{AI safety} as the field focused on technical approaches to preventing risks from AI systems, and especially high-stakes risks from advanced AI systems.
Thus far, the vast majority of all AI safety research has focused on the case of a single AI system, often (implicitly) in the context of a single human \citep[see, e.g.,][]{Armstrong2012,HadfieldMenell2016,Amodei2016,Leike2018,Christiano2018a,Hendrycks2021,Dalrymple2024}.
As this model becomes less and less appropriate, there are a number of important implications for current research agendas in AI safety.


\paragraph{Alignment is Not Enough.}
Alignment refers to the problem of ensuring that an individual AI system acts according to the values and preferences of its principal.\footnote{While this `thin' interpretation of the term alignment has become more dominant \citep{Hubinger2020a,Christiano2018c}, earlier authors and some writers today use a `thick' interpretation that includes the idea that what the AI system does is `good', `friendly', or `beneficial' \citep{Neslon2023,Yudkowsky2008,kirk2023the}.}
While alignment is clearly insufficient for ensuring safety more broadly (because such systems might still be misused by rogue actors, or might cause harm by acting incompetently), this is especially true in multi-agent settings where even capable, aligned AI agents that have arbitrarily similar objectives may end up producing arbitrarily disastrous outcomes \citep{Manheim2019,Critch2020,Jagadeesan2023a,Sourbut2024,Conitzer2023}.
This motivates the importance of directing more effort within AI safety to the problem of ensuring that AI systems can cooperate to reach jointly beneficial outcomes on behalf of their principals \citep{Dafoe2020}.
Of course, if a set of principals (such as individual humans or organisations) are egregiously misaligned with \textit{one another}, then there is less that a set of agents aligned with those principals can do to improve overall outcomes.
Even in such cases, however, we may still be able to avoid exacerbating conflict and race dynamics by not deploying AI agents to begin with \citep{Mitchell2025}.
At the same time, real-world zero-sum settings appear to be relatively few and far between, while there are many cases in which well-meaning individuals are drawn into conflicts of one form or another \citep[see also \Cref{sec:conflict}]{Ostrom1990,fearon1995rationalist,gavrilets2015collective}.

\paragraph{Collusion in Adversarial Safety Schemes.}
Many of the more promising approaches to ensuring the safety of advanced AI are implicitly multi-agent, such as adversarial training \citep{Huang2011,Ziegler2022,Perez2022}, oversight schemes \citep{Irving2018,Christiano2018a,elk_report_2021,Greenblatt2023,Leike2018}, the modularisation of agents \citep{Drexler2019,Dalrymple2024}, or automated methods for interpretability \citep{bills2023language,Schwettmann2023}.
This should not be surprising: if the current rate of progress continues, it will be necessary to employ safety schemes that scale approximately as fast as (or faster than) the AI systems themselves.
These schemes tend to rely crucially on the fact that the different systems or agents do \textit{not} have the same objective as one another, and so are immediately undermined by the presence of collusion \citep{Goel2025}.
For example, an overseer might be able to better achieve their objective by predicting what a human would expect to see another agent do, based on what the human can understand or observe, instead of what the agent \textit{actually} does \citep{elk_report_2021}.
While some have argued that it will be straightforward to avoid these kinds of collusive behaviours by restricting agents' communication channels, architectures, training data, objectives, etc. \citep{Drexler2022}, there are very few investigations of the extent to which the aforementioned safety schemes are robust to collusion, or how they could be made more so.
Future research should attempt to address this gap.







\paragraph{Dangerous Collective Goals and Capabilities.}
Closely related to collusion is the idea that multiple agents can exhibit capabilities or goals that no individual agent possesses.
The simplest example of this is that multiple models which -- while judged to be safe when evaluated independently -- can be combined to overcome their individual safeguards and cause harm, either by a malicious actor, or inadvertently.
For example, different models could be used to execute a cyberattack by breaking the attack down into different steps that could be executed independently \citep{Jones2024}, or a dangerous chemical compound could be synthesized via a series of individually innocuous steps \citep{boiko_emergent_2023,urbina_dual_2022,Luo2024}, each performed by different agent.
This implies that technical evaluations of dangerous capabilities or dispositions, which are currently performed in isolation, \textit{must} begin factor in the presence of other agents.
More speculatively, undesirable goals or capabilities may emerge from large numbers of narrow or simple AI systems, despite the hope that the latter would be inherently safer than advanced, general-purpose agents \citep{Drexler2019,Chan2023-aj}.
Our current understanding of how and when this emergence might take place is rudimentary at best.


\paragraph{Correlated and Compounding Failures.}
As AI agents become increasingly interconnected, their failures may become correlated in previously unanticipated ways, leading to \textit{systemic} risks that traditional misuse-accident dichotomies fail to recognise \citep{Zweetsloot2019,Maas2018,Kasirzadeh2024}, including an eventual `loss of control' \citep{Kulveit2025,Russell2019,critch2023tasra}.
For example, simply ensuring that a single agent performs well when trained in isolation may not take into account the distributional shifts that occur due to the presence of other learning agents, or that agents trained in the same way might be able to collude with one another (or might fail non-independently).
Similarly, minor safety problems or harmful behaviours may be tolerable in isolation but could compound in the aggregate (in a way that is non-obvious simply by inspecting the behaviour of a single agent), potentially due to the feedback loops produced by agent interactions (see \Cref{sec:destabilising_dynamics,sec:network_effects}).
These risks require not only design considerations at the level of individual agents, but also the `infrastructure' via which they interact \citep{Chan2025}, including tools for both monitoring and shaping these interactions.

\paragraph{Robustness and Security in Multi-Agent Systems.}
While it is common for individual systems to undergo various forms of adversarial testing and red-teaming before deployment, traditional threat models that guide this testing are based on interactions with a malicious human user, rather than interactions with other AI agents, or attacks that target the interactions between agents.
Multi-agent systems will likely exacerbate existing robustness and security challenges by increasing the surface area for attacks (see \Cref{sec:multi-agent_security}), and may include new agents that could be strategically incentivised to manipulate, exploit, or coerce others.
The former could include, for example, the insertion of malicious agents that destabilise cooperation \citep{Huang2024,Barbi2025}, or the extraction of private information communicated between agents \citep{shao2024privacylens,wu_inference_2024,wei_trustworthy_2024}.
In the latter case, there could be huge advantages (financial, political, or otherwise) to deploying agents that are capable of exploiting others, such as by issuing credible threats (see \Cref{sec:commitment_and_trust}) or by learning another agent's weaknesses through repeated interaction \citep{Gleave2020}.
Together, these challenges highlight the need for new threat models and security protocols that explicitly account for the intricate, strategic interactions between AI agents.





