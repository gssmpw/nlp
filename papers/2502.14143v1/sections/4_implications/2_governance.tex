\subsection{Governance}
\label{sec:governance}

Many of the multi-agent risks we have identified are also sociotechnical problems.
Furthermore, given that many multi-agent risks have the structure of collective action problems \citep{gavrilets2015collective, Ostrom1990}, we should expect private actors by themselves (absent common protocols for self-regulation) to insufficiently address them. 
In this section we therefore highlight both potential governance interventions to reduce multi-agent risks from advanced AI, as well as research areas that could enable effective governance \citep[see also recent overviews from][]{reuel2024open,Lazar2023,Curtis2024,Weidinger2023a,Kolt2025}.

\paragraph{Supporting Research on Multi-Agent Risks.}
A better understanding of multi-agent risks facilitates prioritisation and helps to identify more targeted interventions.
Governments and other public and private bodies could support research into multi-agent risks by:
providing funding \citep{NSF2023,CAIF2025,ARIA2024};
organising prizes, competitions, or bug bounty programs for overcoming key challenges or identifying undesirable behaviours \citep{CAIS2024,zhao_devising_2017,Levermore2023}; 
or building infrastructure for relevant research \citep{national_artificial_intelligence_research_resource_task_force_strengthening_2023,ASI2024}. 
While this report forms an initial overview of multi-agent risks from advanced AI, much more work is needed in order to identify specific causal pathways and threat models via which these risks could arise \citep{shelby_sociotechnical_2023,rismani_plane_2023,koessler_risk_2023, dai2025individualexperiencecollectiveevidence}.
Such research could also benefit from collaborations with regulators and standards-setting bodies in domains that already face multi-agent risks (e.g., finance or cybersecurity), even if not yet involving advanced AI.

\paragraph{Multi-Agent Evaluations.}
Model evaluations form a crucial part of contemporary AI governance practices, providing a better understanding of a system's potentially dangerous capabilities and dispositions \citep{Shevlane2023,kinniment2023evaluating,reuel2024open,hardy2024more, chen-etal-2024-llmarena} and informing regulatory efforts to restrict the deployment of certain systems in certain domains or increase regulatory scrutiny \citep[as in, for example, Article 51 of the EU AI Act,]{euaiact2024}.
Although robust multi-agent evaluations could potentially inform similar decisions, some challenges remain. 
First, and most obviously, challenges from evaluating single systems are also present in multi-agent contexts, including contamination, validity concerns, and the discrepancy between evaluation tasks and real-world applicability \citep{reuel_hardy_2024,hardy2024more}, as well as the challenges brought about by evaluating \textit{agents} as opposed to less advanced, autonomous AI systems \citep{Kapoor2024,siegel2024core,Stroebl2025}.
Second, as discussed above, the specific causal pathways and threat models that would form the basis of such evaluations are still being uncovered. 
Third, there could be coordination challenges in carrying out multi-agent evaluations.
For example, developers may need to coordinate on safety testing since their agents could interact with each other in the real world, but concerns about commercial sensitivity could be a barrier.
Governments could have a role in facilitating such coordination, such as through AI safety institutes and the Frontier Model Forum \citep{thurnherr2025who}. 

\paragraph{New Forms of Documentation.}
Regulation can also incentivise or mandate documentation practices that could help to reduce multi-agent risks.
For example, AI development often relies upon shared tools, dependencies, and processes, which can make correlated failures like algorithmic monoculture \citep{kleinberg_algorithmic_2021} or outcome homogenization \citep{bommasani_picking_2022} more likely. 
Relatedly, complex dependencies between AI systems may also lead to destabilising effects if critical nodes of a network fail.
Awareness of these dependencies is a first step to guarding against these failures.
Standard documentation tools for single systems -- such as datasheets \citep{gebru_datasheets_2021}, data statements \citep{bender_data_2018}, and model cards \citep{Mitchell2019} -- can be complemented with other forms of documentation that track ecosystem-wide and interaction risks.
For example, \citet{Bommasani2023} propose `ecosystem graphs', which document various aspects of the AI ecosystem (e.g., datasets, models, use cases) and how they relate to each other (e.g., technical and business dependencies), and \citet{gilbert_reward_2023} propose `reward reports', which document agents that continue to learn and adapt after deployment. 



\paragraph{Infrastructure for AI Agents.}
Just as new infrastructure was needed to enable the internet (e.g., TCP/IP, HTTP) and secure it (e.g., SSL), so too might new infrastructure be needed to reap the benefits and manage the risks of multi-agent systems \citep{Chan2025}. 
For example, agent IDs could enable improved monitoring and the establishment of trust among agents \citep{Chan2024}, new communication protocols could improve stability and security in safety-critical domains \citep{Marro2024,Hammond2025}, and the ability to undo agent actions could prevent miscoordination or escalation \citep{Patil2024}.
Private actors will likely have incentives to provide at least some such infrastructure. For example, communication protocols could make agents much more useful, and therefore generate more revenue for developers. 
Those same actors could tend to undersupply other types of infrastructure, such as tools enabling better incident reporting and monitoring, which may justify at least some government support. 
Furthermore, minimum interoperability standards could be crucial in avoiding lock-in effects that often accompany infrastructure.\footnote{Analogously, social media lock-in effects make it difficult for new entrants to obtain users, even if those new entrants provide better features.} %

\paragraph{Restrictions on Development and Deployment.}
Restrictions on the development or deployment of certain multi-agent systems could be a useful regulatory tool \citep{anderljung_frontier_2023,Mitchell2025}, but it remains to be seen what such restrictions should entail and whether/where they are feasible.  
For example, if agents trained in multi-agent settings -- especially settings that may reward strategic behaviour and deception -- exacerbate certain risks (see \Cref{sec:selection_pressures}), development standards could caution against the use of such training methods. %
Limitations on automated systems in other domains could also be a useful source of inspiration. 
For autonomous weapons, researchers have emphasised the need to maintain human control through measures such as giving humans the ability to intervene and terminate operation \citep[see also \Cref{sec:commitment_and_trust}]{amoroso_autonomous_2020,Renshaw2024,Congress2023}. 
In financial markets, simple interventions such as reducing the tick size\footnote{The tick size is the minimum granularity in the movement of the price of a security.} may reduce incentives for algorithmic collusion \citep{cartea_algorithmic_2022}, and automatic circuit breakers can be used to temporarily halt trading when prices move too dramatically \citep{Subrahmanyam2013}.
However -- especially in the case of open-source systems -- agents might not be easily governed and curtailed post-deployment \citep{seger2023open}. Furthermore, implementing restrictions on multi-agent development and deployment faces governance challenges due to the international nature of these systems, with training data, infrastructure, and stakeholders distributed globally across diverse legislative and regulatory jurisdictions. This points to the need for coordinated international oversight, which has traditionally been slow in the AI domain \citep{trager2023international}. 





\paragraph{Liability for Harms from Multi-Agent Systems.}
Holding a person liable for harms to persons or property from multi-agent systems poses two potential challenges.\footnote{The points in this paragraph benefited greatly from discussions with Peter Wills.}
First, it will often be unclear who, if anyone, would be liable for harms caused by a single agent \citep{Kolt2024}. Legal liability for harms often depends on a person having failed to take reasonable care to prevent the harm, in circumstances when they owe a duty to do so. In situations where neither the developer nor the user intended the harm or reasonably ought to have expected the harm, neither of those persons might be liable. Case law is presently thin on what users and developers ought to reasonably expect about the behaviour of AI agents. Second, even if it is clear which legal entity is responsible for a particular agent's actions, it could be unclear how to allocate responsibility among multiple agents for a harm. Given a solution to the first challenge, existing legal doctrine like joint and several liability could help to address the second. For an in-depth exploration of these legal challenges -- which are exacerbated by the international nature of the development, deployment, and use of multi-agent systems, as discussed above -- we refer the reader to \citep{Wills2024, ayres_law_2024, lima_could_2017, lior_ai_2019,Chopra2011, Solum1992}.



\paragraph{Improving Societal Resilience.}
Finally, %
safety-critical multi-agent systems must be integrated into society in a way that allows them to fail gracefully and gradually, as opposed to producing sudden, cascading failures \citep{Maas2018,Bernardi2024}.
Indeed, there are many societal processes -- ranging from the mundane to the critical -- that function only because of physical limits on the number and capability of humans \citep[e.g.,][see also the examples in \Cref{sec:conflict}]{VanLoo2019}.
Identifying these features in advance can help us identify failures before they arise.
At the same time, the delegation to AI agents by a range of different individuals and organisations might make it easier to manage and represent their interests by making their agents the target of governance efforts, or the participants of new, more scalable methods of collective decision-making and cooperation \citep{seger_democratising_2023,Huang2023,Domingos2022,Sourbut2024,Terrucha2024,oesterheld2022safe}.
Governments could help to surface such benefits via new platforms for soliciting citizens' input \citep[see, e.g.,][]{Small2021,Small2023,bakker_fine-tuning_2022,Fishkin2019,ovadya_generative_2023,Jarrett2023,Fish2023}, subsidizing access to AI resources in order to prevent `agentic inequality' \citep[see also \Cref{sec:ethics}]{Gabriel2024}, and monitoring for vulnerabilities introduced by the use of AI agents.







