\subsection{Destabilising Dynamics}
\label{sec:destabilising_dynamics}

Modern AI agents can adapt their strategies in response to events in their environment. The interaction of such agents can result in complex dynamics that are difficult to predict or control, sometimes resulting in damaging run-away effects.

\subsubsection{Definition} 

When viewed from a more classical game-theoretic perspective, problems in multi-agent systems are often interpreted in terms of equilibria and their (un)desirability.
This `static' notion, however, can be limited when it comes to understanding the risks posed by the inherently \emph{dynamic} interactions between adaptive AI agents.
Instead, we can think of a multi-agent system as a non-linear dynamical system: a set of equations, partially determined by a set of parameters, that govern how a set of variables change over time \citep{papadimitriou2019game,BloembergenEtAl2015,Balduzzi2018,barfuss2022dynamical}. 
In the case of \emph{non-adaptive} agents, the variables comprise the agents' actions and the state of their environment, which are governed by the agents' strategies, the environmental dynamics and a set of fixed parameters (such as the weights of a neural network).
In the case of \emph{adaptive} agents, we view the strategies themselves as variables, which are governed by learning algorithms and their (hyper)parameters, such as a learning rate.

With this framing, we can characterise several kinds of undesirable behaviour that we might wish to avoid that go beyond the equilibria (i.e., fixed points) of the system \citep{Mogul2006}.
These include dynamic instabilities such as feedback loops, 
chaos, and phase transitions \citep{Gleick1998,BarfussEtAl2024}.
While some of these behaviours can emerge in the case of a single, non-adaptive AI agent (such as a simple agent that becomes stuck in a loop under certain environmental conditions), the additional complexity brought about by the presence of multiple, adaptive agents provides greater opportunity for instabilities to arise \citep{Sanders2018,Cheung2020,Bielawski2021,chotibut2020route,Piliouras2022}.


\subsubsection{Instances}

A long history of research has identified broad classes of behaviours that can be exhibited by dynamical systems, such as fixed points, limit cycles, chaos, and the transient or intermittent presence of such patterns.
Our approach in this section is therefore to examine which behaviours might be exhibited in the context of multi-agent systems, and which of them might pose risks.

\paragraph{Feedback Loops.}
One of the best-known historical examples to illustrate destabilising dynamics in the context of autonomous agents is the 2010 flash crash, in which algorithmic trading agents entered into an unexpected {feedback loop} \citep[see also \Cref{cs:flash_crash}]{CTFC2010}.\footnote{For a simpler and more amusing example, see \Cref{fn:textbook}.}
More generally, a feedback loop occurs when the output of a system is used as part of its input, creating a cycle that can either amplify or dampen the system's behaviour. In multi-agent settings, feedback loops often arise from the interactions between agents, as each agent's actions affect the environment and the behaviour of other agents, which in turn affect their own subsequent actions.
Feedback loops can lead not only to financial crashes but to military conflicts \citep[see also \Cref{cs:cs:military_escalation}]{Richardson1960} and ecological disasters \citep{Holling1973}.
The distinguishing characteristic of flash crashes, however, is the \emph{speed} at which they occur. Competitive pressures necessitate automated trading agents that act much faster than their human overseers, meaning that when things go wrong, it is harder for humans to react.
As such, we might expect to see more destabilising dynamics in systems with more fast-moving AI agents \citep{Maas2018}.\footnote{Catastrophe-theoretic models show that even `slow' systems with a small number of `fast' elements can produce dramatic shifts \citep{Zeeman1976}, though it is not always clear how closely such models capture complex real-world phenomena.}


\begin{case-study}[label=cs:flash_crash,sidebyside,sidebyside align=top,lower separated=false]{The 2010 Flash Crash}
  On May 6, 2010, the US stock market lost approximately \$1 trillion in 15 minutes during one of the most turbulent periods in its history \citep{CTFC2010}. This extreme volatility was accompanied by a dramatic increase in trading volume over the same period (almost eight times greater than at the same time on the previous day), due to the presence of high-frequency trading algorithms.\footnotemark{}
  While more recent studies have concluded that these algorithms did not \emph{cause} the crash, they are widely acknowledged to have contributed through their exploitation of temporary market imbalances \citep{Kirilenko2017}. 
  \tcblower
  \includegraphics[width=\linewidth]{figures/destabilising_dynamics/flash_crash.png}
  \captionof{figure}{
    Transaction prices of the Dow Jones Industrial Average on May 6, 2010.
    Figure adapted from \citet{OptionAlpha2025}.
  }\label{fig:flash_crash}
\end{case-study}

\footnotetext{In cases like this, it can be the synchronisation between agents that creates an instability if, for example, all agents try to sell or buy at the same time because they all make decisions based on highly correlated signals (or even a common signal) and they all have similar strategies. Such problems might become significantly amplified if only a handful of frontier models are the underlying decision makers for a vast number of (seemingly diverse) agents (see \Cref{sec:network_effects,sec:multi-agent_security}).}


\paragraph{Cyclic Behaviour.} 
The dynamics described above are highly non-linear (small changes to the system's state can result in large changes to its trajectory).
Similar non-linear dynamics can emerge in multi-agent learning and lead to a variety of phenomena that do not occur in single-agent learning \citep{BarfussEtAl2019, NagarajanEtAl2020a, LeonardosEtAl2020, BarfussMann2022, GallaFarmer2013}.
One of the simplest examples of this phenomenon is Q-learning \citep{Watkins1992}: in the case of a single agent, convergence to an optimal policy is guaranteed under modest conditions, but in the (mixed-motive) case of multiple agents, this same learning rule can lead to {cycles} and thus non-convergence \citep{Zinkevich2005}. 
While cycles in themselves need not carry any risk, their presence can subvert the expected or desirable properties of a given system.
For example, \citet{PaesLeme2024} show that when auto-bidding agents participate in second price auctions -- which are designed to have dominant truthful equilibria -- the dynamics of these agents can be unstable and fail to converge to their underlying values, losing the desired truthfulness properties.


\paragraph{Chaos.}
Unlike the systems that tend towards fixed points or cycles described above, chaotic systems are inherently unpredictable and highly sensitive to initial conditions. While it might seem easy to dismiss such notions as mathematical exoticisms, recent work has shown that, in fact, chaotic dynamics are not only possible in a wide range of multi-agent learning setups \citep{GallaFarmer2013,Andrade2021,VlatakisGkaragkounis2023,sato2002chaos,palaiopanos2017multiplicative}, but can become the norm as the number of agents increases \citep{Sanders2018,Cheung2020,Bielawski2021}.
To the best of our knowledge, such dynamics have not been seen in today's frontier AI systems, but the proliferation of such systems increases the importance of reliably predicting their behaviour.

\paragraph{Phase Transitions.}
Finally, small external changes to the system -- such as the introduction of new agents or a distributional shift -- can cause phase transitions, where the system undergoes an abrupt qualitative shift in overall behaviour \citep{BarfussEtAl2024}. Formally, this corresponds to \textit{bifurcations} in the system's parameter space, which lead to the creation or destruction of dynamical attractors, resulting in complex and unpredictable dynamics \citep{Crawford1991,Zeeman1976}.
For example, \citet{Leonardos2022} show that changes to the exploration hyperparameter of RL agents can lead to phase transitions that drastically change the number and stability of the equilibria in a game, which in turn can have potentially unbounded negative effects on agents' performance.
Relatedly, there have been many observations of phase transitions in ML \citep{Carroll2021,Olsson2022,Ziyin2022}, such as `grokking', in which the test set error decreases rapidly long after the training error has plateaued \citep{Power2022}.
These phenomena are still poorly understood, even in the case of a single system.

\paragraph{Distributional Shift.}
Individual ML systems can perform poorly in contexts different from those in which they were trained.
A key source of these distributional shifts is the actions and adaptations of other agents \citep{Papoudakis2019-es,Piliouras2022,Narang2023}, which in single-agent approaches are often simply or ignored or at best modelled exogenously.
Indeed, the sheer number and variance of behaviours that can be exhibited other agents means that multi-agent systems pose an especially challenging generalisation problem for individual learners \citep{Agapiou2022-an,Leibo2021-cf,Stone2010}.
While distributional shifts can cause issues in common-interest settings (see \Cref{sec:miscoordination}), they are more worrisome in mixed-motive settings since the ability of agents to cooperate depends not only on the ability to coordinate on one of many arbitrary conventions (which might be easily resolved
by a common language), but on their beliefs about what solutions other agents will find acceptable. 
For example, training a negotiating agent on a distribution of counterparts with too little diversity in their negotiating tactics can lead to catastrophic overconfidence in high-stakes settings \citep[cf.][]{Stastny2021}, which might already have little precedent in the training data.
These issues may be aggravated by the fact that multi-agent systems can be highly dynamic \citep{Papoudakis2019-es}, as AI agents or their designers will be incentivised to continually adapt to the behaviour of other agents.
These effects might also be exacerbated by the fact that models may come to be trained using data generated by other models \citep[see also \Cref{sec:selection_pressures}]{Shumailov2024AIMC,Martnez2023TowardsUT,alemohammad_self-consuming_2023}, though preliminary work suggests such concerns might be overblown \citep{Gerstgrasser2024}.

\subsubsection{Directions}

With the deployment of advanced multi-agent systems comes the risk of destabilising dynamics in settings ranging from financial markets \citep{Kirilenko2017} to power grids \citep{schafer2018dynamically} to battlefields \citep{Johnson2021}.
So far, both theoretical and empirical work has primarily studied such dynamics in small, abstract games with simple AI systems and learning algorithms.\footnote{In one of the few examples involving foundation models, \citet{Fort_2023} recently provided a simple visual illustration of how the outputs of two foundation models -- GPT-4(V) and DALLE-3 -- can be either stable or unstable when placed in a loop, depending on their initial input.}
While this is an important first step, addressing the risks of destabilizing dynamics in real-world multi-agent AI systems will require a concerted interdisciplinary effort, bringing together expertise in AI safety, dynamical systems, game theory, and policy to develop robust solutions.

\paragraph{Understanding Dynamics.}
The conditions under which multi-agent systems have undesirable dynamics might include properties of the underlying environment and objectives \citep{BarfussMann2022,Sanders2018}, or the structure and hyperparameters of the learning algorithms \citep{BarfussEtAl2019,Leonardos2022,barfuss2023intrinsic}.
Important research directions include understanding if (and how) chaotic dynamics in idealised versions of stochastic learning algorithms extend to their real-world counterparts,\footnote{Formally, chaos is only rigorously defined in deterministic dynamical systems.} and how these dynamics are affected by the size and structure of the state-action space.

\paragraph{Monitoring and Stabilising Dynamics.}
Early work suggests that inducing new `conservation laws' \citep{NagarajanEtAl2020a} or `constants of motion' \citep{Piliouras2021} in multi-agent learning can result in more predictable dynamics. 
Future research should investigate how these approaches scale to larger systems and greater numbers of agents and could make use of existing results in areas such as theoretical ML \citep{Sastry1999,Tuyls2005,Bowling2001a,Bottou2010,Kushner2003}, adaptive mechanism design \citep{Pardoe2006,Zhang2008,Baumann2020,Yang2022,Zheng2022,Gerstgrasser2023}, and mean-field games \citep{Lasry2007,Huang2006}.
Both this work and that on understanding the dynamics of multi-agent learning would benefit greatly from the insights of other scientific communities, especially those working on other non-linear complex systems, and those engineering the largest and most powerful models \citep{BarfussEtAl2024}.

\paragraph{Regulating Adaptive Multi-Agent Systems.}
In addition, {regulation} could be used to mandate the use of mechanisms that monitor and stabilise the dynamics of multi-agent systems in safety-critical areas.
This could include, for example, enforced pauses in interactions between systems or reversions to previous strategies if the system behaviour escapes certain thresholds \citep[as in the 2010 flash crash, when trading was temporarily halted; see also]{Subrahmanyam2013}.
For the most important systems, there might even be a need to enforce the (de)synchronisation of model updates, to limit the size and frequency of learning updates, or to limit the number of agents interacting with one another (either via technical restrictions or using methods akin to congestion pricing).
Relatedly, existing tools for auditing models often make use of static `model cards' that indicate how the model was produced, its performance, and intended use cases \citep{Mitchell2019}, but this documentation applies to single trained systems that are frozen before deployment.
To monitor the dynamics of \emph{multi-agent} systems, including those that \emph{learn online}, we will need to leverage new innovations such as `ecosystem graphs' \citep{Bommasani2023} and `reward reports' \citep{Gilbert2022}, respectively.
