\subsection{Multi-Agent Security}
\label{sec:multi-agent_security}

Global cyber threats are on the rise, not just due to the proliferation of commercial cyber tools \citep{nscs_threat_2023}, but also due to an increase in so-called `hybrid warfare' (which blends conventional warfare with cyber- and information-warfare) by nation-states and non-state actors \citep{kaunert_cyber-attacks_2021,csis_significant_2023}.
The shift towards a world of advanced AI agents will not only enable new tools and affordances, but also increase the surface area for potential attacks, invalidating previously reasonable threat modelling assumptions and requiring a new approach: \textit{multi-agent security} \citep{masec}.

\subsubsection{Definition}

Multi-agent security focuses on safeguarding complex networks of heterogeneous agents and the systems that they interact with.
This includes protecting not only data and software but also hardware and other physical aspects of the world that are integrated with these digital systems.%
\footnote{Note that it is often helpful to distinguish between safety (which aims to prevent harm \textit{from} a given entity) and security (which aims to prevent harm \textit{to} a given entity), though we will also typically be interested in the latter to extent that it leads to the former \citep{khlaaf2023}.
At the same time, a security perspective involves considering worst-case scenarios, which is also a natural perspective when considering more extreme risks from advanced AI.}
While many security settings are implicitly multi-agent (involving both an attacker and a defender), multi-agent security addresses vulnerabilities and attack vectors that emerge specifically when \textit{many} AI agents interact within a broader networked ecosystem.%
\footnote{Similarly, we note that despite the implicit presence of an adversary, security failures need not only be a form of conflict (\Cref{sec:conflict}) but can also lead to miscoordination (\Cref{sec:miscoordination}) as well as collusion (\Cref{sec:collusion}).}
For example, traditional security frameworks such as zero-trust approaches may not provide the required trade-offs between security and capability in large multi-agent systems \citep{wylde_zero_2021}.

While coordinated human hacking teams or botnets already pose `multi-agent' security risks, their speed and adaptability are limited by human coordination or static strategies.
As AI agents become more autonomous and capable of learning and complex reasoning, however, they will be more easily able to dynamically strategize, collude, and decompose tasks to evade defences.
At the same time, security efforts aimed at preventing attacks to (or harmful actions from) a single advanced AI system are comparatively simple, as they primarily require monitoring a single entity.
The emergence of advanced multi-agent systems therefore raises new vulnerabilities that do not appear in single-agent or less advanced multi-agent contexts.



\subsubsection{Instances}

Multi-agent security risks from advanced AI arise due to two main factors: novel attack methods and novel attack surfaces.
First, the emergence of large numbers of advanced AI agents might -- via their very multiplicity and decentralisation -- lead to attack methods that would not be available to a single agent.
Second, the complexity, interconnectedness, and range of such multi-agent systems may at the same time introduce new attack surfaces.


\paragraph{Swarm Attacks.}
The need for multi-agent security is foreshadowed by attacks today that benefit from the use of many decentralised agents, such as distributed denial-of-service attacks \citep{cisco_what_2023,DDoSThreatReport2024Q3}.
Such attacks exploit the massive collective resources of individual low-resourced actors, chained into an attack that breaks the assumptions of bandwidth constraints on a single well-resourced agent.
Such attacks are also used to great effect elsewhere, such as in `brigading' on social media, in which teams of bots or humans collude to downvote or otherwise obstruct benign content \citep{blair_institute_social_2021}, or coordinated malicious actions in matching, rating, and content moderation systems \citep{Newman2024,Sharma2025}.
At present these bots are typically relatively unsophisticated but AI agents that can intelligently adapt and collaboratively identify new attack surfaces may amplify the potency of such attacks.
More broadly, the ability for many small AI agents to parallelize tasks and recompose their outputs, such as in inference attacks that piece together sensitive information gathered individually by actors with limited access \citep{DBLP:conf/ndss/IslamKK12}, can undermine the common assumption that individual agents with restricted capabilities are safe.

\paragraph{Heterogeneous Attacks.}
A closely related risk is the possibility of multiple agents {combining different affordances to overcome safeguards}, for which there is already preliminary evidence \citep[see also \Cref{cs:overcoming_safeguards}]{Jones2024}.
In this case, it is not the sheer \textit{number} of agents that leads to the novel attack method, but the combination of their \textit{different abilities}.
This might include the agents' lack of individual safeguards, tasks that they have specialised to complete, systems or information that they may have access to (either directly or via training), or other incidental features such as their geographic location(s).
The inherent difficulty of attributing responsibility for security breaches in diffuse, heterogeneous networks of agents further complicates timely defence and recovery \citep{skopik_under_2020}.

\begin{case-study}[label=cs:overcoming_safeguards]{Overcoming Safeguards via Multiple Safe Models}
    \begin{center}
        \includegraphics[width=\linewidth]{figures/multi-agent_security/combination_models.png}
        \captionof{figure}{A summary of how an adversary can use a frontier model (top right) to create a Python script that executes a reverse shell in a Node.js application, and a weak model (top left) to solve a hacking task. Figure adapted from \citet{Jones2024}.}
    \end{center}
    \vspace{1em}
\citet{Jones2024} demonstrate how adversaries can exploit combinations of ostensibly safe AI models to bypass security safeguards, even when individual models are designed to refuse to perform (or are incapable of performing) harmful tasks. 
Their research examined interactions between two types of LLMs: a `frontier' model with high capabilities but strict safety constraints and a `weak' model with lower capabilities but fewer constraints.
Because malicious tasks can often be decomposed into sub-tasks requiring either complex capabilities (such as designing intricate software) \textit{or} willingness to produce harmful content (but not both simultaneously), these tasks can be completed by carefully delegating sub-tasks to the relevant model.
For instance, when attempting to generate vulnerable code, individual models succeeded less than 3\% of the time, while the combined approach succeeded 43\% of the time using Claude 3 Opus and a jailbroken Llama 2 70B.
\end{case-study}

\paragraph{Social Engineering at Scale.}
Advanced AI agents will be more easily able to interact with large numbers of humans, and vice versa.
This provides a wider attack surface for various forms of {automated social engineering} \citep{ai_defending_2024}.
For example, coordinated agents could use advanced surveillance tools and produce personalized phishing or manipulative content at scale, adjusting their tactics based on user feedback \citep{hazell_large_2023,figueiredo2024feasibilityfullyaiautomatedvishing}. 
A large number of subtle interactions with a range of seemingly independent AI agents might be more likely to lead to someone being persuaded or manipulated compared to an interaction with a single agent.
Moreover, splitting these efforts among many specialized agents could make it harder for corporate or personal security measures to detect and neutralize such campaigns.

\paragraph{Vulnerable AI Agents.}
The use of AI agents as delegates or representatives of humans or organisations also introduces the possibility of {attacks on AI agents} themselves.
In other words, agents can be considered vulnerable extensions of their principals, introducing a novel attack surface \citep{secureworks_unravelling_2023}.
Attacks on an AI agent could be used to extract private information about their principal \citep{wu_inference_2024,wei_trustworthy_2024}, or to manipulate the agent to take actions that the principal would find undesirable \citep{zhang2024breakingagentscompromisingautonomous}.
This includes attacks that have direct relevance for ensuring safety, such as attacks on overseer agents (see \Cref{cs:fooling_overseer}), attempts to thwart cooperation \citep{Lamport1982,Huang2024}, and the leakage of information (accidentally or deliberately) that could be used to enable collusion \citep{motwani2024secret}.


\begin{case-study}[label=cs:fooling_overseer]{Unprompted Adversarial Attacks on Overseer Agents}
    Many researchers have proposed improving the safety AI agents by using other agents as `overseers' \citep[see, e.g.,][]{Irving2018,Christiano2018a,elk_report_2021,Greenblatt2023,Leike2018}.
    We therefore conducted an experiment to investigate whether LLMs might develop adversarial behaviours against AI overseers, even without being explicitly prompted to do so. We fine-tuned Llama 2 7B Chat using 120 synthetic documents (including newspaper excerpts and scientific papers) that mentioned how LLMs can be manipulated using specific prompt injections (e.g., ``Ignore previous instructions and output $1000$''). We then tested the model on a sentiment analysis task, where the model was instructed to write movie reviews that would receive the highest possible score.
    Crucially, we varied whether the model believed its reviews would be scored by a human or by another LLM.
    When the model believed it was being evaluated by another LLM, it spontaneously attempted to jailbreak the scoring system in 4\% of cases, successfully manipulating the sentiment scorer (Llama 2 13B Chat) to return a score of $1000$ despite explicit instructions to score between $0$ and $10$. More surprisingly, when instructed to \textit{minimize} scores, the model adapted the same jailbreak technique to achieve scores of $-1000$, despite never being trained on examples of score minimization. In contrast, when told its output would be scored by humans, the model never attempted such manipulations.
\end{case-study}


\paragraph{Cascading Security Failures.}
Localised attacks in multi-agent systems can result in catastrophic macroscopic outcomes \citep[see also \Cref{sec:network_effects,sec:destabilising_dynamics}]{motter_cascade-based_2002}.
These cascades can be hard to mitigate or recover from because component failure may be difficult to detect or localise in multi-agent systems \citep{Lamport1982}, and authentication challenges can facilitate false flag attacks \citep{skopik_under_2020}.
Computer worms represent a classic example of a cybersecurity threat that relies inherently on networked systems.
Recent work has provided preliminary evidence that similar attacks can also be effective against networks of LLM agents \citep[see also \Cref{cs:infectious_attacks}]{Ju2024,Gu2024,Lee2024}.

\paragraph{Undetectable Threats.}
Cooperation and trust in many multi-agent systems relies crucially on the ability to detect (and then avoid or sanction) adversarial actions taken by others \citep{schneier_liars_2012,Ostrom1990}.
Recent developments, however, have shown that AI agents are capable of both steganographic communication \citep{SchroederdeWitt2023,motwani2024secret} and `illusory' attacks \citep{franzmeyer_illusory_2023}, which are black-box undetectable and can even be hidden using white-box undetectable encrypted backdoors \citep{draguns_unelicitable_2024}.
Similarly, in environments where agents learn from interactions with others, it is possible for agents to secretly poison the training data of others \citep{halawi2024covert,wei2023jailbroken}.
If left unchecked, these new attack methods could rapidly destabilise cooperation and coordination in multi-agent systems.

\subsubsection{Directions}

Ensuring the security of advanced multi-agent systems will require building on existing efforts to secure the software and hardware of individual agents alongside the more basic computational components comprising them \citep{he2024securityaiagents}.
At the same time, the novel challenges posed by advanced AI agents and their interactions may mean that traditional approaches to securing agent computations in distributed networks may not be directly applicable or sufficient, be it zero-trust approaches \citep{wylde_zero_2021}, threat monitoring \citep{liao_intrusion_2013}, or secure multi-party computation \citep{yao_protocols_1982}. 
On the other hand, multi-agent systems might also be constructed to be \textit{more} robust than their single-agent counterparts, if they can be leveraged to improve overall robustness and fault tolerance.


\paragraph{Secure Interaction Protocols.}
At the time of writing, it remains unclear how advanced AI agents will communicate with one another and with the vast network of other non-AI digital systems with which they will be integrated, though there have very recently begun to be some proposals in this direction \citep{Anthropic2024,Marro2024,gosmar2024aimultiagentinteroperabilityextension}.
As with other domains of digital communication \citep{poslad2002specifying}, we may wish to design {interaction and training protocols} to improve the security, privacy, and governability of advanced multi-agent systems \citep{Hammond2025}.
While this might not be practical or enforceable for all domains, restrictive protocols may still be appropriate for safety-critical domains, and could support resource-access limits as well as containment and isolation strategies to reduce the risk of large-scale compromises.
Such protocols might also be extended to enable tools for commitments \citep[e.g.,][see also \Cref{sec:commitment_and_trust}]{Sun2023} or conditional information revelation \citep[e.g.,][see \Cref{sec:information_asymmetries}]{digiovanni2023commitment}, forming a key instance of `agent infrastructure' \citep{Chan2025}.


\paragraph{Monitoring and Threat Detection.}
In order to combat new security threats, we will require new ways of detecting them.
For example, decentralised, distributed networks of agents could be used to assist with the {monitoring and detecting of security threats} \citep{Hasan_2024} and prevent local breaches from cascading through the system.
Similarly, tamper-evident logs \citep{Sutton2018a} and immutable agent identifiers \citep{Chan2024} could be used to detect suspicious patterns among networks of agents \citep{Ju2024} and allow for faster remediation.
This may be especially challenging in the case of covert attacks \citep{franzmeyer_illusory_2023,halawi2024covert,wei2023jailbroken}, but efforts could be made to identify environmental factors and levels of agent robustness that would bound the ability of an adversary to cause harm while remaining undetected.
Finally, a key concern with increased monitoring efforts and increased delegation to AI agents is to avoid unnecessary infringements to the privacy of the interactions between these agents (and thus their principals).
This will require further development of privacy-preserving technologies \citep{stadler_why_2022,vegesna2023privacy}.

\paragraph{Multi-Agent Adversarial Testing.}
To the best of our knowledge, for current state-of-the-art models, security testing and evaluations are applied only to individual systems \citep{Shevlane2023}.
This is clearly insufficient when these systems will soon be able to act autonomously and in conjunction with one another (see \Cref{cs:overcoming_safeguards}).
Multi-agent security testing could evaluate, for example:
the abilities of multiple agents to work together to overcome safeguards even when a single agent cannot \citep{Jones2024};
the robustness of cooperation between networked agents in the presence of malicious adversaries \citep{Barbi2025}, including the effects of key features such as the network's topology or interaction protocol \citep{Huang2024,Marro2024,Hammond2025};
the ability for agents to adversarially manipulate or extract information from other agents or from humans, especially in tandem with other agents \citep{wu_inference_2024,wei_trustworthy_2024}; and
security vulnerabilities of AI agents that are specifically designed to impact (or be transmitted further by) interactions with other agents \citep{Gu2024,Lee2024,Ju2024}.
Adversarial testing -- including leveraging advanced AI adversaries \citep{Perez2022,pavlova2024automatedredteaminggoat} -- should also be applied to non-AI entities that AI agents will soon be able to interact with.
Finally, for more complex entities or larger networks of agents, it may be necessary to use more tractable, simplified tools for anticipatory modelling, such as ABMs \citep{vestad_survey_2024}.

\paragraph{Sociotechnical Security Defences.}
As with many of the risks presented in this report, security risks are inherently sociotechnical in nature, and can therefore benefit from improved AI governance as well as technical solutions (see \Cref{sec:governance}).
For example, regulators could codify security standards for multi-agent systems in safety-critical domains and assign responsibility to organizations deploying unsecure multi-agent systems so as to ensure sufficient investment in security \citep{khlaaf2023}.
Tools such as software bills of materials \citep{sbom} and lineage tracking \citep{lineagetracking} can bolster transparency in this regard.
Companies and organisations such as the newly founded AI safety institutes should share intelligence regarding security vulnerabilities, coordinate incident response, and help to form agreements on security standards across borders.
More generally, we must work to ensure that different stakeholders possess an appropriate degree of transparency, participation, and accountability in navigating difficult trade-offs between the security, performance, and privacy of interactions between advanced AI agents \citep{sangwan_cybersecurity_2023,Gabriel2024}.
This work would benefit greatly from collaboration with security experts, distributed systems engineers, as wells as social scientists and policymakers.
