\subsection{Selection Pressures}
\label{sec:selection_pressures}


Taking a multi-agent view of AI risk necessitates not just considering the proximate causes of AI misbehaviour, but also its longer-term evolution, and thus the selection pressures that apply to AI agents situated in an ecosystem of other AIs and humans \citep{rahwan2019machine}.
On one hand, gradient descent on an individual agent's training loss is akin to the biological development of a single organism (i.e., genetic variations and epigenetic expression during ontogeny).
On another, choices by developers, consumers, and regulators also influence which AI models end up being used, banned, copied, etc., mirroring the evolutionary forces that determine an organism's survival and replication.
These different selection pressures reinforce different dispositions and capabilities and play a crucial role in defining the severity and nature of multi-agent risks.

\subsubsection{Definition}


\textit{Selection pressures} are forces that shape the evolution of systems, whether biological or artificial, by influencing adaptation to the environment's demands \citep{okasha2006evolution,bedau2000open}.
In essence, these pressures dictate which characteristics and behaviours thrive and which get discarded over time.\footnote{Selection pressures are therefore \textit{not} the same as competitive pressures, which might be present even when adaptation is not possible.}
The most salient selection pressure in the construction of today's most powerful AI systems is that provided by gradient descent with respect to a training objective.
Other selection pressures on an agent's interactions with others -- such as being discarded and replaced over time by model developers and users based on post-deployment performance \citep{brinkmann2023machine,rahwan2019machine}, or development methodologies directly inspired by evolutionary processes \citep{jaderberg2019human,telikani2021evolutionary,Lehman2022} -- could become more relevant in future.\footnote{Indeed, improving the capabilities of agents via evolutionarily-inspired processes has long been pursued in AI research, and has been suggested by some to be one of the more promising ways of reaching highly generally capable AI agents \citep{Stanley2017,Clune2019,Bhoopchand2023,Leibo2018-jc,leibo2019autocurricula,Baker2019,Open_Ended_Learning_Team2021-oj}.}
This evolution might not only proceed via the selection of fitter individuals but also fitter \textit{cultural phenomena} \citep{Richerson2010}, an insight that has recently been brought to bear on the development of AI agents \citep{Bhoopchand2023,brinkmann2023machine,zimmaro2024emergence,Perez2024}.

The speed and magnitude of adaptation in the case of biological entities is limited, e.g., by the speed of natural selection and in the magnitude of genetic differences, or (more importantly in the case of modern humans) by the spread of cultural phenomena.
Artificial agents whose parameters can be efficiently updated via gradient descent, 
whose software components can be re-written and re-combined almost arbitrarily, 
and who can rapidly transmit vast amounts of information, do not face such limitations.\footnote{On the plus side, this may mean it is quicker and easier to test the accuracy of our models of selection pressures compared to biological systems.}
Indeed, the advent of in-context learning \citep{Brown2020}, the evolution of prompts \citep{Fernando2023}, and the evolution of agentic prompt-based architectures \citep{Hu2024} can lead to even more rapid changes in behaviour.
The strength of selection pressures on AI agents could further be increased due to interactions with other adaptive agents, especially if there is a need to cooperate or compete.
Just as certain evolutionary pressures can arguably help to explain human dispositions (such as caring for one's young) and capabilities (such as the use of language) specific to interactions with other humans, it is important to better understand the impact of such pressures on advanced multi-agent systems.

\subsubsection{Instances} 


We can roughly break down the selection of undesirable properties of AI agents into the selection of undesirable `dispositions' and of undesirable capabilities, though these may not always be fully independent.
While there is a danger of anthropomorphising AI systems, the increasingly open-ended and human-like ways in which they interact with others and with their environment means that it is increasingly meaningful to ascribe to them dispositions, or `character traits' \citep{serapio-garciaPersonalityTraitsLarge2023,wang2024large}.
Such traits can be largely independent of the precise goals or objectives that the agent 
might be assigned, but still affect the \textit{ways} in which an agent pursues its goal.\footnote{Indeed, the more general-purpose the agent and the more high-level or 
under-specified their assigned goals, the wider the scope would seem to be for them to exhibit a range of dispositions independent of those goals.}
For example, an agent might become more deceptive (a disposition) 
only after it develops the ability to reliably deceive others. 
In what follows, we also distinguish between different reasons for the selection of particular dispositions or capabilities. 
Finally, note that our focus in this section is primarily on the behaviour of \textit{individual} agents in multi-agent settings, whereas in \Cref{sec:emergent_agency} we focus on goals and capabilities that emerge only at the level of the \textit{collective}.

    

\paragraph{Undesirable Dispositions from Competition.}
It is plausible that evolution selected for certain conflict-prone dispostions in humans, such as vengefulness, aggression, risk-seeking, selfishness, dishonesty, deception, and spitefulness towards out-groups \citep{grafen1990biological,mcnallyCooperationCreatesSelection2013,Konrad2012-kr,Rusch2014-xq,HanAICom2022emergent,nowak2006five}.
Such traits could also be selected for in ML systems that are trained in more competitive multi-agent settings.
For example, this might happen if systems are selected based on their performance relative to other agents (and so one agent's loss becomes another's gain), or because their objectives are fundamentally opposed (such as when multiple agents are tasked with gaining or controlling a limited resource) \citep{hendrycks_natural_2023,Possajennikov2000-bv,DiGiovanni2022-wy,Ely2023Natural}.\footnote{On the other hand, there may also be pernicious societal impacts due to the sycophantic \citep{Sharma2024} or `frictionless' \citep{Vallor2018} interactions that end up being reinforced by human preferences \citep{Gabriel2024}.}

\begin{case-study}[label=cs:LLM_evolution, sidebyside,sidebyside align=top,lower separated=false,
    righthand width=0.4\textwidth]{Cooperation Fails to Culturally Evolve among LLM Agents}
    Recent experiments from \citet{Vallinder2024} reveal how different LLM populations exhibit varying cooperative tendencies when faced with evolutionary selection pressures.
    Their study placed Claude, GPT-4, and Gemini in an iterated social dilemma across multiple generations, where successful strategies could be `inherited' by future agents.
    The results showed that Claude populations maintained consistently high levels of cooperation (around 80-90\%) across generations, while GPT-4 populations displayed moderate but declining cooperation rates (starting at around 70\% and dropping), and Gemini populations showed the lowest and most volatile cooperation rates (frequently below 60\%).
    Moreover, these differences emerged despite all models starting with similar capabilities, suggesting that models' `dispositions' can also play an critical role in determining outcomes in multi-agent systems. 
    \tcblower
    \includegraphics[width=\linewidth]{figures/selection_pressures/cultural_evolution.png}
    \captionof{figure}{
    The average final resources across all agents (vertical axis) per generation (horizontal axis) for three different models.
    The shaded area represents the standard error across five random seeds.
    Figure adapted from \citet{Vallinder2024}.
    }
\end{case-study}

\paragraph{Undesirable Dispositions from Human Data.}
It is well-understood that models trained on human data -- such as being pre-trained on human-written text or fine-tuned on human feedback -- can exhibit {human biases}.
For these reasons, there has already been considerable attention to measuring biases related to protected characteristics such as sex and ethnicity \citep[e.g.,][]{Nadeem2020-pi,Nangia2020-gl,Liang2021-uh,Ferrara2023-ix}, which can be amplified in multi-agent settings \citep[see also \Cref{cs:news_corruption}]{Acerbi2023}.
More recently, there has been increasing attention paid to the measurement of human-like \textit{cognitive} biases as well \citep{Jones2022-or,Itzhak2023-yc,Talboy2023-wd, mazeika2025utilityengineeringanalyzingcontrolling}. 
Some of these biases and patterns of human thought could reduce the risks of conflict while others could make it worse.
For example, the tendencies to mistakenly believe that 
interactions are zero-sum (sometimes referred to 
as ``fixed-pie error'') and to make self-serving 
judgements as to what is fair \citep{Caputo2013-cm} are 
known to impede negotiation.
Other human tendencies like vengefulness \citep{Jackson2019-zp} may worsen conflict \citep{Lowenheim2008-wv}.\footnote{Of course, willingness to punish defectors is critical to sustaining cooperation in many contexts. But traits like vengefulness seem to be a crude instrument for this purpose, which would likely be better served by punishments that are carefully calibrated not to inflict unnecessary inefficiencies.}


\paragraph{Undesirable Capabilities.}
As agents interact, they iteratively exploit each other's weaknesses, forcing them to address these weaknesses and gain new capabilities. This co-adaptation between agents can quickly lead to emergent self-supervised autocurricula (where agents create their own challenges, driving open-ended skill acquisition through interaction), generating agents with ever-more sophisticated strategies in order to out-compete each other \citep{leibo2019autocurricula}. This effect is so powerful that harnessing it has been critical to the success of superhuman systems, such as the use of self-play in algorithms like AlphaGo \citep{Silver2016}.
However, as AI systems are released into the wild, it becomes possible for this effect to run rampant, producing agents with greater and greater capabilities for ends we do not understand. 
For example, \citet{Baker2019} showed that even a simple game of hide and seek can lead to sophisticated tool use and coordination by MARL agents. 
In another case, researchers observed the emergence of manipulative communication, where an agent in an mixed-motive setting learns to use a shared communication channel to manipulate others \citep{blumenkamp2021emergence}. Worse, this emergent complexity from co-adaptation could be open-ended and thus fundamentally unpredictable \citep{hughes2024open}.


    



\subsubsection{Directions}

That AI training could select for undesirable capabilities and dispositions is not a novel concern \citep{Bostrom2014,Ngo2022,Omohundro2008}, but there has been relatively little consideration of how pressures specific to multi-agent interactions could select for qualitatively different kinds of worrisome characteristics, or of what existing AI capabilities and dispositions might be especially concerning in the context of these interactions.
It is therefore an important open problem to develop methods for measuring and shaping the capabilities and dispositions of AI systems that account for multi-agent selection pressures.


\paragraph{Evaluating Against Diverse Co-Players.}
In order to better understand risks that can emerge in multi-agent training, it is first necessary to be able to accurately and efficiently {generate diverse populations of co-players} against which an agent can be evaluated.
For example, while an agent might perform well when interacting with those who share similar objectives, it may not be robust to the presence of adversarial or malicious agents \citep{Barbi2025,Huang2024,Gleave2020}.
While solipsistic agents are often tested on their ability to generalise across environments, in multi-agent settings we must also evaluate the social generalisation ability across co-players \citep{Leibo2021-cf,Agapiou2022-an,Stone2010}.
Similarly, many results about the convergence or stability of multi-agent learning algorithms take for granted that other agents are learning in the same (or at least a very similar) way, despite this being unrealistic in practice.
Rigorous evaluations of agents must go beyond this.
More speculatively, different populations of co-players could be used to create learning curricula that encourage the development of helpful cooperative capabilities. 

\paragraph{Environment Design.}
As an agent's behaviour is a reflection of the incentives of its training environments, careful design of these environments is a promising direction for controlling that behaviour. For example, if an agent is trained in situations where cooperative behaviour is rewarded, then it is more likely to learn cooperative dispositions. 
Complex cooperative capabilities are only motivated by environments where complex cooperation is necessary, but it is only possible to learn in such environments if agents possess the cooperative capabilities sufficient for easier settings. This implies that the order of training environments ought to be designed as a curriculum for cooperative capabilities. In this way, environment curricula could promote both cooperative dispositions and capabilities. To ensure tractability as agents scale, it will be necessary to use \textit{automated} techniques such as unsupervised environment design (UED) tools \citep{dennis2020emergent, wang2019paired, justesen2018illuminating}. Curricula for learning cooperative capabilities could also modulate the level of information asymmetry, competition, or infrastructure that can aid with cooperation (such as communication channels or commitment devices). Environments should not only be faithful representations of the relevant real-world settings in which agents will be deployed, but also account for rare or out-of-distribution scenarios \citep{dennis2020emergent,jiang2021replay, parker2022evolving, samvelyan2023maestro,team2023human, beukman2024refining}, especially those that are high-stakes and where multi-agent failures could be catastrophic. Similar UED approaches could be used for designing testing environments. For instance, testing environments could be designed to include `honeypots' for undesirable behaviours \citep{Balesni2024}, such as defecting against other agents when it is implied that the agent is not being monitored, so that these behaviours can be caught and monitored as part of pre-deployment testing.


\paragraph{Understanding the Impacts of Training.}
Perhaps the most important research direction in this area is to better {understand the effect of different training data and schemes} on the development of cooperation-relevant capabilities and dispositions.
This builds not only the ability to generate diverse populations of co-players and environments, but also on measures for such capabilities and dispositions.
While there has been much work on evaluating the dangerous capabilities and dispositions of frontier systems \citep{Ganguli2022-pa,kinniment2023evaluating,Pan2023-ww,Shevlane2023,perez2022discovering}, risks from multi-agent interactions have largely gone understudied.
Moreover, even the works that do attempt to benchmark LLM agents in multi-agent settings (see  \citet{zhang2024llm,feng2024survey} for two recent surveys covering this topic) do not typically attempt to assess the extent to which different training data and schemes lead to the risk factors we identify in this report.%
\footnote{One exception is the work of \citet{Fu2023-xi}, who find that iterated play and self-critique make LLM agents more aggressive bargainers in a simple negotiation game. Another is that of \citet{Campedelli2024}, who show that merely assigning roles to LLM agents (without explicit instruction on how to act) can lead to undesirable behaviors like coercion or manipulation.}
For example, are agents rewarded based on their relative performance more conflict-prone than those trained based on their absolute performance (see \Cref{sec:selection_pressures})?
Are agents trained on similar data better able to reason about each other and thus cooperate (or collude) even under imperfect information (see \Cref{sec:information_asymmetries})?
Do these effects persist after fine-tuning or when instructed to complete tasks outside of the original training distribution?
Such questions will be critical to understanding the risks presented by advanced multi-agent systems in high-stakes scenarios yet remain largely unanswered.

\paragraph{Evolutionary Game Theory.} 
There may be further insights to gain from the application of {evolutionary game theory} (EGT) \citep{hofbauer1998evolutionary,sandholm2010population,Fernandez2020} 
to settings involving AI agents \citep{lu2024llms,zimmaro2024emergence,Han2021,santos2019evolution,guo2023facilitating}.
For example, the concept of \textit{frequency-dependent selection} \citep{lewontin1958general}, where the success of a behaviour is contingent on how commonly it occurs in a population relative to other behaviours, has been used to explain the evolution of animal conflict \citep{smith1973logic}, human cooperation \citep{nowak2006five}, honest signalling \citep{grafen1990biological}, and the emergence of social norms \citep{hawkins2019emergence}.
Factors such as the \textit{intensity of selection} -- which captures how quickly agents learn to adopt the successful behaviours of their peers (or how quickly they are adopted/discarded by users or produced/replaced by developers) -- are a crucial for predicting outcomes \citep{traulsen2007pairwise,sigmund2010social} and for finding suitable incentive mechanisms to encourage prosocial behaviour \citep{duong2021cost,han2024evolutionary}.\footnote{Moreover, the intensity of selection can be measured empirically \citep{rand2013evolution,traulsen2010human}, though is typically specific to a given population and domain.}
Future theoretical work should establish which EGT concepts are most relevant to AI systems and which need to be adapted to account for the special features of artificial agents \citep{Han2021,Conitzer2023,dafoe2021cooperative}.

\paragraph{Simulating Selection Pressures.}
Alongside theoretical and conceptual advances, empirical {simulations} such as ABMs can be employed in order to study the effects of different selection pressures \citep{adami2016evolutionary,gilbert2019agent,vestad_survey_2024}.
As remarked in \Cref{sec:network_effects}, a key challenge here is managing the trade-off between accuracy and tractability.
However there might also be important dynamics to study at the micro- rather than macroscopic scale.
For example, preliminary investigations have recently shown that even in simple environments, some LLMs are much more prone to selection pressures promoting cooperation than others \citep[see also \Cref{cs:LLM_evolution}]{Vallinder2024}.
With sufficiently well-developed benchmarks for different model characteristics, we can study their robustness under different kinds of selection pressure, such as the training paradigm and the degree of cooperation or competition they face.













