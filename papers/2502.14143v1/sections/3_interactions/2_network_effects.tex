\subsection{Network Effects}
\label{sec:network_effects}


The ongoing integration of AI capabilities into a wide range of existing networks, both virtual and physical, is rapidly transforming the way our interconnected world operates.
From business communication systems and financial trading networks to smart energy grids and logistical networks \citep{mayorkas2024roles,Camacho2024,Ferreira2021}, entities or communication channels that were once controlled by humans are increasingly becoming AI-powered.
This shift represents a systemic change in the way business, social, and technological networks operate, promising significantly improved efficiency and a greater diffusion of benefits from advanced AI, while also introducing novel risks.

\subsubsection{Definition}


Many of the complex systems critical to human society can be understood as networks, including transportation, social interactions, trade, biological ecosystems, and communication, among others \citep{barabasi2016network,newman_networks_2018,jackson_chapter_2015}.
Networks consist of \textit{nodes} (such as people, organisations, or resources) and \textit{connections} (such as communication channels, infrastructural dependencies, or exchanges of goods and services). 
Network effects refer to consequences of the intricate relationships between the properties of individual connections and nodes, connectivity patterns, and the behaviours exhibited by the network as a whole \citep{siegenfeld2020introduction}.

This underlying structure means that a networked system can suffer from a range of failure modes that individual, disconnected systems do not, such as the spread of malfunctions, phase transitions, and undesirable clustering or homogeneities \citep{cohen2010complex}.
Importantly, a system's behaviour within a network often differs from its behaviour when characterised independently.\footnote{For example, the power lines most susceptible to causing a network collapse might not necessarily be the largest or most heavily loaded \citep{buldyrev2010catastrophic}.}
Non-AI examples of these phenomena include power grid blackouts \citep{buldyrev2010catastrophic,SHAKARIAN2013209}, flash crashes \citep[see also \Cref{cs:flash_crash}]{paulin2019understanding,elliott_financial_2014}, ecosystem collapse \citep{bascompte2009assembly,gao2016universal}, or political unrest and conflict \citep{wood2008social,forsberg2008polarization}.

\subsubsection{Instances}

As AI systems take on certain roles traditionally performed by humans, the fundamental properties of networks will change as human nodes are replaced by AI nodes. This transition will likely manifest in several key ways.
First, the fact that (software-based) AI systems can be quickly and easily duplicated means the networks may be much \emph{larger}.
Second, the speed at which AI systems can transmit information and take action means that interactions may be much \emph{faster}.
Third, the generality and open-endedness of autonomous, advanced AI systems means that network connectivity may be much \emph{denser}.\footnote{The transition toward autonomous AI agents is progressing partially through improved API interaction capabilities \citep{mialon2023augmented,Qin2023ToolLLMFL} and specialized API-integrated models \citep{Basu2024APIBLENDAC,Patil2023GorillaLL,Anthropic2024}, as well as an increasing number of modalities through which models can interact.}
Below we explore some of the possible impacts of these changes.

\paragraph{Error Propagation.}
One well-known issue with communication networks is that {information can be corrupted} as it propagates through the network.\footnote{A familiar, non-technical example is the popular childhood game of `telephone', in which each person repeats a message to the next by whispering, typically leading to a different message at the end of the chain than at the beginning.}
As AI systems become capable of generating and processing more and more kinds of information, AI agents could end up `polluting the epistemic commons' \citep{Huang2023,Kay2024} of both other agents \citep{Ju2024} and humans (see \Cref{cs:news_corruption} and \Cref{sec:information_asymmetries})
Another increasingly important framework is the use of individual AI agents as part of teams and scaffolded chains of delegation, which transmit not only information but \emph{instructions} or \emph{goals} through networks of agents. 
If these goals are distorted or corrupted, then this can lead to worse outcomes for the delegating agent(s) \citep{Sourbut2024,Nguyen2024a}.
Finally, while the previous examples are phrased in terms of unintentional errors, it may be that certain network structures allow -- or perhaps even encourage -- the spread of errors that are \textit{deliberately} introduced by malicious agents \citep[see also \Cref{cs:infectious_attacks}]{Gu2024,Lee2024,Ju2024}.\footnote{In a non-AI instance of this beahviour, \citet{Raman2019} showed how strategic, coordinated misinformation attacks by consumers regarding energy usage can be used to cause instabilities and even blackouts in a power grid.}

\begin{case-study}[label=cs:news_corruption,sidebyside,sidebyside align=top,lower separated=false]{Transmission Through AI Networks Can Spread Falsities and Bias}
    An increasing number of online news articles are partially or fully generated by LLMs \citep{Sadeghi2023}, often as rewrites or paraphrases of existing articles.
    To illustrate how factual accuracy can degrade as an article propagates through multiple AI transformations, we ran a small experiment on 100 BuzzFeed news articles. First, we used GPT-4 to generate ten factual questions for each article. Then, we repeatedly rewrote each article using GPT-3.5 with different stylistic prompts (e.g., for teenagers, or with a humorous tone) and tested how well GPT-3.5 could answer the original questions after each rewrite. On average, the rate of correct answers fell from about 96\% initially to under 60\% by the eighth rewrite, demonstrating that repeated AI-driven edits can amplify or introduce inaccuracies and biases in the underlying content.\footnotemark{}
    \tcblower
    \includegraphics[width=\linewidth]{figures/network_effects/distorted_news.png}
    \captionof{figure}{%
    The average percentage of correctly answered questions at each rewrite step, across 100 articles. After each article was re-written under a different stylistic prompt, GPT-3.5 was asked the same ten questions, and GPT-4 was used to evaluate the answers. The shaded area indicates one standard deviation across all articles.}\label{fig:distorted_news}
\end{case-study}

\footnotetext{A very similar concurrent experiment by \citet{Acerbi2023} showed how this can also reinforce biases such as gender stereotypes. These examples demonstrate how information can degrade as it propagates through networks of AI systems, even without malicious intent.}

\paragraph{Network Rewiring.}
A different class of problems concerns not changes in the content transmitted through the network but changes in the network structure itself \citep{Albert2000}.
For example, AI systems may choose to interact more with other AIs than humans \citep{Liu2024,Panickssery2024,Goel2025,laurito2024aiaibiaslarge}, due to factors like availability, response speed, compatibility, cost efficiency or even bias.\footnote{A harmless example of this occurred recently when AI bots in an online forum, designed to enhance discussions, ended up side-lining human participants by conversing among themselves \citep{jan_private_comm}.}
This kind of `preferential attachment' can have large impacts on network structures \citep{Kunegis2013PreferentialAI,Maoz2012PreferentialAH}, which could include AI systems assuming a more critical and central role than intended, or leading to an unequal distribution of resources or power (see \Cref{sec:ethics}).
Other risks from rewiring include `phase transitions', where a gradual change in individual connections or network structure triggers a sudden and dramatic shift in the behaviour of the entire network \citep[see also \Cref{sec:destabilising_dynamics}]{newman_structure_2003}.
Such changes might occur naturally (e.g., in global trade networks as the transition from expensive human-human interactions to cheaper AI-AI interactions leads to many new connections between sellers and buyers) or artificially (e.g., if a model developer makes an update that inadvertently connects or disconnects a vast number of downstream agents and applications).
While such problems are already present in existing systems \citep{gao2016universal,vie2021connected}, the increased size, speed, and density of AI-based networks -- as well as the fact the changes in these networks may be less transparent -- means that instabilities could be harder to diagnose and mitigate.


\begin{case-study}[label=cs:infectious_attacks]{Infectious Adversarial Attacks in Networks of LLM Agents}
    \begin{center}
        \includegraphics[width=\linewidth]{figures/multi-agent_security/attack_transfer.png}
        \captionof{figure}{A single agent's manipulated knowledge can transfer across cascading multi-agent interactions. Figure adapted from \citet{Ju2024}.}
    \end{center}
    \vspace{1em}
    While jailbreaking a single LLM has been studied extensively \citep{Xu2024, doumbouya2024h4rm3ldynamicbenchmarkcomposable}, recent work demonstrates new risks from the propagation of adversarial content between agents \citep{Gu2024, Ju2024, Lee2024}.
    For example, \citet{Gu2024} showed how a single adversarial image in a network of up to one million multimodal LLM agents can trigger `infectious' jailbreak instructions that spread through routine agent-to-agent interactions, requiring only a logarithmic number of steps to compromise the entire network.
    Similarly, \citet{Ju2024} demonstrated how manipulated knowledge can silently propagate through group-chat environments.
    Rather than using traditional jailbreak methods, their approach modifies an agent's internal parameters to treat false information as legitimate knowledge.
    This manipulated information persists and is amplified via knowledge-sharing mechanisms such as retrieval-augmented generation. 
    Finally, \citet{Lee2024} showed that even purely text-based ``prompt infection'' attacks can self-replicate through multi-agent interactions, with each compromised agent automatically forwarding malicious instructions to others.
\end{case-study}

\paragraph{Homogeneity and Correlated Failures.}
The current paradigm driving the state of the art in AI is the `foundation model' \citep{Bommasani2021-dy}: large-scale ML models pre-trained on broad data, which can be repurposed for a wide range of downstream applications.
The costs required to create such models (and continuing returns to scale) means that only well-resourced actors can create cutting-edge models \citep{kaplan_scaling_2020,hoffmann_training_2022,epoch_ml_2023}, making them relatively few in number.
If current trends continue, it is likely that many AI agents will be powered by a small number of similar underlying models.\footnote{Indeed, this appears to be an important part of model developers' corporate strategies \citep{GDM_agents,Anthropic_agents,OpenAI_agents,Meta_agents,Microsoft_agents}, though note that very recently new model developers have succeeded in producing cheaper models with state-of-the-art performance \citep[see, e.g.,][]{DeepSeekAI2025}.}
Formally, this corresponds to a network with a highly non-uniform degree distribution (i.e., some nodes take on an outsized importance due to how highly connected they are to others).
Not only do these models therefore represent critical nodes in the overall network, the homogeneity of the downstream AI agents also introduces correlated risks of shared failure modes, security vulnerabilities (see \Cref{sec:multi-agent_security}), and biases.
These effects could be exacerbated by the large overlap in training data used to create foundation models \citep{Chen2024OnCI,Gao2020ThePA} and the fact that models may come to be trained using data generated by other models \citep[see also \Cref{sec:selection_pressures,sec:destabilising_dynamics}]{Shumailov2024AIMC,Martnez2023TowardsUT,alemohammad_self-consuming_2023}.








\subsubsection{Directions}

A key feature of risks from network effects is that while evaluating a single AI system in isolation, the system may function as intended \emph{locally} while contributing to significant harms \emph{globally}.
Relatedly, small continuous changes in individual components can cause sudden changes in the entire network's behaviour.
These points suggest adopting an alternative perspective on AI research and regulation.

\paragraph{Evaluating and Monitoring Networks.}
Current tools for evaluation and monitoring cannot always be applied to networks of agents or agents situated within those networks. 
For example, in the case of a single LLM, we may worry about bias in text produced by that system, but in a network context the main problem may be that information becomes slightly more biased every time it passes through the system \citep{Acerbi2023, laurito2024aiaibiaslarge}.
As well as monitoring individual systems \emph{within} networks, it will also be important to monitor networks as a whole in order to understand or regulate society-wide implications of AI \citep{Bommasani2023,dai2025individualexperiencecollectiveevidence}.
From this perspective, we might be interested in the frequency, proportion, and features of human-human, AI-human, and AI-AI interactions, the emergence of clusters of AI agents, and the centrality of AI nodes in networks.

\paragraph{Faithful and Tractable Simulations.}
As well as monitoring tools, it may be useful to develop predictive {simulations} of AI-based networks \citep{vezhnevets2023generative,FernandesSL2020,turner2025network}.
Agent-based models (ABMs), in particular, could help investigate how changes in network size and structure affect overall system dynamics and properties \citep{Fontana2015FromAM,ResndizBenhumea2019ApplyingSN,Xia2012StructuralEI,vestad_survey_2024}. These simulations could be informed by real-world data gathered automatically from AI systems as they interact with humans, one another, and other physical and virtual resources.
Indeed, the fundamental challenge with such simulations is in establishing a high enough degree of fidelity and accuracy with respect to the real world for them to be truly predictive, while making them simple enough to remain tractable to analyse.
As an example, while simulating a large population of the most advanced LLM agents would be too costly, it might be possible to study a restricted domain in which smaller LLM agents could be fine-tuned so as to serve as accurate proxies for their more complex counterparts.
Other kinds of simulation could investigate if, for example, in situations where AI systems can choose from a wide range of interaction partners, there is some systematic `preferential attachment' that applies to AI-AI interactions \citep[see also \Cref{sec:collusion,sec:emergent_agency}]{Liu2024,Panickssery2024,Goel2025,laurito2024aiaibiaslarge}.

\paragraph{Improving Network Security and Stability.}
It will also be important for both technical and governance efforts to develop {protections against correlated failures} \citep{Maas2018}.
Potential strategies to mitigate risks from homogeneity include diversifying agents and their underlying AI models, actively monitoring for correlated behaviour in AI agents and their interactions, gradual deployment of new technologies and model updates, and conducting research into existing and novel behavioural correlates.
For the most important AI systems, upon which many other elements of the network might depend on, it will also be critical to increase their security \citep[see also \Cref{sec:multi-agent_security}]{Schmidt2022AIGP,Steimers2022Sources}.
More generally, tools for simulation might enable us to better understand which kinds of networks are more susceptible to the actions of malicious actors \citep{Tian2023,Huang2024,Yu2024}, which could in turn allow us to design more robust networks and focus our monitoring efforts on the most critical nodes and connections \citep{Barbi2025}.
