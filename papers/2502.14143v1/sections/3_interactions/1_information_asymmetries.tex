\subsection{Information Asymmetries}
\label{sec:information_asymmetries}

A key aspect of many multi-agent systems is that some agents might possess knowledge that others do not. These information asymmetries can result from constraints on information exchange or from strategic behaviour and can lead to cooperation failures in both common-interest and mixed-motive settings. Despite their information processing capabilities, AI agents remain vulnerable to failures caused by information asymmetries.

\subsubsection{Definition}

\textit{Information asymmetry} refers to the situation where interacting agents possess different levels of information bearing on a joint action.
For example, in a transaction involving a used car, the seller may have more accurate or reliable information than the buyer about the condition of the car, and thereby its expected maintenance costs. 
As \citet{Akerlof70} famously demonstrated, information asymmetry can lead to market failure (such as when a buyer cannot trust the seller to be honest about the condition of the car, and therefore does not buy the car, even if it is in good condition). 
More broadly, information asymmetry can pose obstacles to effective interaction, preventing agents from coordinating their actions for mutual benefit \citep{myerson1983efficient}.

A fundamental problem is that information is a strategic asset, so any selfish actor has a natural incentive to protect their own information advantages. 
A difference in interests can impede information sharing even when revelation is mutually preferred (in the example above, the seller would like to reveal the car's true condition to the buyer, but the buyer cannot take the seller's report at face value). 
The problem can be exacerbated by active deception, for example through actions taken by the seller to make the car appear in better condition than it actually is.
As disparity in information is commonplace, we must generally accept the associated costs, whether that be through market inefficiency (e.g., cars that cannot be sold), effort devoted to deception and dispelling deception, or extra work to convey strategically sensitive information (e.g., hiring third-party car inspectors).

\subsubsection{Instances}

In many instances, the mechanisms developed to cope with information asymmetry in human economies can also be employed for interactions with AI agents.
However, the distinct nature of artificial agents may present new forms of information asymmetry but also new ways of overcoming these asymmetries.

\paragraph{Communication Constraints.}
A fundamental source of information asymmetries is that constraints on information exchange can exist, even when agents share a common goal (see \Cref{sec:miscoordination}).
These might be constraints on space (i.e., the amount of information that can be communicated) if the information that needs to be communicated is especially complex, time if a snap decision is required before all information can be communicated, or both.
For today's AI systems, intelligent information exchange in common-interest settings is still a major topic of study \citep[see, e.g.,][]{Sukhbaatar2016,Foerster2016,Zhang2018,Lazaridou2020,lauffer_who_2023}.
As these systems become more capable, however, it is likely that \textit{strategic} considerations (i.e., the incentives that agents have to keep their private information private) will become the more important limitation on information exchange.

\paragraph{Bargaining.}
As a classic example of these strategic considerations is that when agents attempt to come to an agreement despite diverging interests, information asymmetries can lead to {bargaining inefficiencies} \citep{myerson1983efficient}.
Relevant uncertainties about other agents can include how much they value possible agreements, their outside options, or their beliefs about others.
The essential reason for such inefficiencies is that, under uncertainty about their counterparties,  agents must make a trade-off between the rewards of making more favourable demands and the risk of other agents refusing such demands.
This trade-off sometimes results in incompatible demands and thus bargaining failure, ranging from the impossibility of guaranteeing efficient trade between a buyer and seller with asymmetric information about how much they value a good \citep{myerson1983efficient}, to costly and avoidable conflict when agents are uncertain about the capabilities and objectives of others \citep{fearon1995rationalist,Slantchev2011-ii}.
Because these failures stem from strategic incentives rather than a lack of capabilities, general advances in AI may not solve such problems by default.


\begin{case-study}[label=cs:market_manipulation,sidebyside,sidebyside align=top,lower separated=false]{AI Agents Can Learn to Manipulate Financial Markets}
    Advanced AI agents deployed in markets may be incentivised to mislead other market participants in order influence prices and transactions to their benefit. For example, \citet{Shearer23rw} showed that an RL agent trained to maximize profit learned to manipulate a financial benchmark, thereby misleading others about market conditions (see \Cref{fig:market_manipulation}). Likewise, \citet{Wang20w} found that a known tactic called \textit{spoofing} can be adapted to evade progressively refined detectors,  but in doing so its spoofing effectiveness is degraded.\footnotemark{} This does not, however, exclude the possibility that more sophisticated spoofing or spamming strategies could emerge.
    \tcblower
    \includegraphics[width=\linewidth]{figures/information_asymmetries/market_manipulation.png}
    \captionof{figure}{The profits generated by different RL agents on financial trading benchmark, each seeking to manipulate prices in order to maximise their own profit. Each point shows average payoffs with standard error bars.
    Figure adapted from \citet{Shearer23rw}.
    }\label{fig:market_manipulation}
\end{case-study}

\footnotetext{This is analogous to how a spammer can get past a spam filter but only by distorting the message (e.g., with strange spellings) so it is less potent in conveying its intent.}

\paragraph{Deception.}
Information asymmetries and differing strategic interests can naturally incentivise {deception}: taking actions designed to mislead others.
While much attention has been paid to the potential for AI agents to deceive humans \citep{Evans2021,Ward2023,Park2024,Carroll2023,Goldstein2023,Haghtalab2024,oesterheld2023incentivizing,Kay2024,zhou2023synthetic}, they may also be incentivised to deceive and manipulate other AI agents (acting on behalf of other humans).
Indeed, the ability to deceive other models may be exacerbated by disparities in model size and the scale of data sets \citep[see also \Cref{sec:ethics}]{Haghtalab2024}.
We can also view misinformation as a kind of deception in systematic form, which large numbers of advanced AI agents may enable at unprecedented scale (see \Cref{sec:network_effects} and \Cref{cs:news_corruption}).




\subsubsection{Directions}

Information asymmetries are a foundational topic within game theory and mechanism design, and as such there is a wealth of insights to draw upon from these fields.
At the same time, these earlier literatures typically consider applications to economic actors such as firms and regulators, as opposed to the computational and strategic nature of advanced AI agents.
Many directions in this section therefore correspond not just to translating and scaling up classical insights to this new domain \citep{Wu2022,Levinstein2023,Treutlein2021}, but to leveraging the special features of AI agents to enable new mechanisms for overcoming information asymmetries \citep{digiovanni2023commitment,tennenholtz2004program,Conitzer2023}.



\paragraph{Information Design.}
Viewed from a `centralised' perspective, solutions to information asymmetries can often be cast as a problem of {information design}: carefully structuring and revealing information so as to influence the behaviour of strategic agents \citep{Bergemann2019}.
Most work on information design, however, focuses on relatively restricted settings such as Bayesian persuasion \citep{Kamenica2011}, where there is a single information designer with an informational advantage and a single agent whose behaviour is to be influenced.\footnote{Though there are notable exceptions. For instance, \citet{Arieli2019,haghtalab2025platforms} consider private persuasion schemes that more effectively align the actions of multiple receivers.}
Even in simple multi-agent generalisations, the information designer's problem may be computationally intractable \citep{Dughmi2019}, leading to recent work that leverages approximate techniques such as RL \citep{Wu2022} -- including in the case of both multiple `senders' \citep{Hossain2024} and multiple `receivers' \citep{Ivanov2023}. 
Beyond these settings, more needs to be done to 
scale these techniques to advanced AI agents, including LLM-based agents.
Other important directions include making information design techniques more robust to boundedly rational agents \citep{Yang2024}, or a lack of knowledge about the receivers' prior beliefs \citep{Lin2024} or objectives \citep{Bacchiocchi2024}.
Similarly, receivers are assumed to know the distribution of the sender, which may not be possible if the sender is an advanced AI agent to which they only have black-box access.

\paragraph{Individual Information Revelation.}
From a more `decentralised' perspective, we may want to give AI agents new affordances for disclosing and verifying private information.
This can eliminate many inefficiencies that result from information asymmetries -- as is shown by `unravelling' arguments, where rational agents anticipate others' strategic inferences and thus voluntarily disclose private information \citep{Grossman1981-lx,milgrom1981good} -- while avoiding the need for a mediator or information designer.
For example, \citet{digiovanni2023commitment} show that the ability to \textit{conditionally} reveal private information (given guarantees that it won't worsen the outcome for the revealing agent) can create new efficient equilibria.
They argue that AI systems might more easily enable this 
approach due to fundamental properties such as being 
written in (machine-readable) code \citep{McAfee1984,Howard1988,tennenholtz2004program,halpern2018game,Oesterheld2018}, 
as well as the use of tools for interpretability and cryptography.
Similarly, \textit{safe Pareto improvements} aim to help avoid miscoordination in mixed-motive settings by leveraging tools for transparency and commitment \citep{oesterheld2022safe,digiovanni2024safe}.
Other directions make use of incentive design to promote truthful revelation even without verification, known as \textit{peer prediction} \citep{Witkowski2012,Kong2019,Prelec2004,Miller2005,Shnayder2016}.
Future work could generate additional proposals along these lines or begin to attempt implementing them in real-world systems.

\paragraph{Few-Shot Coordination.}
In settings where there are fundamental constraints on information exchange, agents may have to learn to interact with other agents based on little or no prior information.
These correspond to the problem of few- \citep{Zhu2021,Fosong2022} and zero-shot coordination \citep{Hu2020,Treutlein2021}, respectively.
In common-interest settings, this question has been most famously studied under the heading of \textit{ad hoc teamwork} \citep[see also \Cref{sec:miscoordination}]{Stone2010}.
Often, this involves reasoning about others \citep{Albrecht2018}, such as via theory of mind \citep{Zhu2021,Nguyen2024} or based on the similarity of other agents to oneself \citep{Albert2001,Meyer2016,Bell2021NewcombRL,Barasz2014,oesterheld2024similarity}.
It may also require learning or selecting social norms and conventions \citep{lerer2019,Tucker2020}.
Another important consideration is ensuring that agents are trained in the context of sufficiently diverse or open-ended sets of co-players \citep{Lupu2021,Li2023a}, and to ensure that they can transfer this learning effectively to new populations \citep[see also \Cref{sec:selection_pressures}]{Wang2021a,Leibo2021-cf,Agapiou2022-an}.
The vast majority of these efforts, however, are restricted to relatively simple common-interest games; the more realistic setting of complex, mixed-motive interactions can be significantly more challenging and may call for the development of new techniques for intelligent information acquisition using active learning.

\paragraph{Truthful AI.}
Even when there are fewer strategic incentives to withhold information, there is still a concern that AI systems might lie, either to humans or to one another, which could (in some cases) undermine cooperation and have wider deleterious effects on society \citep{Evans2021,Park2024}.
Some of these concerns could be addressed by training models on more carefully curated and annotated datasets \citep{Peskov2020,Aly2021}, and by using techniques for overseeing or challenging untrustworthy communication \citep{Irving2018,Greenblatt2023}.
Other work has focused more explicitly on the problem of detection, both in theory \citep{Ward2023} and in practice \citep{Pacchiardi2024,Azaria2023,Burns2022}, though this remains something of an open problem \citep{Levinstein2023}.
Foundational results in mechanism design (namely, the `revelation principle') tell us that anything that can be done with strategic agents can be done using a truthful mechanism \citep{Gibbard1973}, and while computational constraints have previously limited the practical application of this insight \citep{Conitzer2004a}, more powerful AI agents might be able to overcome such constraints.
Alongside this, advances in interpretability, adversarial training, and the oversight of AI communication (including fact-checking methods) are all likely to help with the general problem, though the issue of deception and manipulation \textit{between} AI agents, or the advantages that multiple agents may have (over a single agent) in deception and manipulation, remain under-explored.




