\subsection{Emergent Agency}
\label{sec:emergent_agency}

Emergent behaviour is ubiquitous in the natural, biomedical, and social sciences.
Examples include the superconductivity of materials in condensed matter physics \citep{anderson_more_1972}; complex tasks like bridge-building by ant colonies and facing larger predators \citep{gordon1996organization,bonabeau_self-organization_1997}; and, in the social sphere, collective behaviours such as group-think or the development of new norms \citep{couzin_collective_2007}.
In this section we focus on the risks presented by the emergence of higher-level forms of agency from a collective of agents.







\subsubsection{Definition}

Emergent behaviours are those exhibited by a complex entity composed of multiple, interacting parts (such as AI agents) that are not exhibited by any of those parts when viewed individually.
Emergent behaviours are distinct from mere accumulations (as in \Cref{cs:overcoming_safeguards}, for example); in other words, the whole may be different to the sum of its parts \citep{anderson_more_1972}.
While there is a sense in which everything we study in this report can be viewed as ``emerging'' from multi-agent systems \citep{Mogul2006,Altmann2024}, our focus on this section is specifically on the risks associated with \textit{emergent agency} at the level of the \textit{collective}.
This is distinct from other works that discuss the emergent behaviour of \emph{individual} agents -- such as tool use \citep{Baker2019}, locomotion \citep{Bansal2018}, or communication \citep{Lazaridou2020} -- in multi-agent settings.\footnote{Other works consider emergence concerning say, the number of parameters in a model, as opposed to the number of agents. For example, \citet{Wei2022} ``consider an ability to be emergent if it is not present in smaller models but is present in larger models''.}
These individual behaviours are fundamentally driven by the selection pressure induced by the presence of other agents, which we discuss in \Cref{sec:selection_pressures}.

We break the risks associated with emergent agency into the emergence of dangerous \emph{capabilities}, the emergence of dangerous \emph{goals}, and thus -- if one takes the view that intelligence is fundamentally rooted in an individual's or group's ability to solve problems, achieve goals, etc. \citep{Legg2007} -- the possibility of creating emergent higher-level agency or \emph{collective intelligence} \citep{malone2022handbook}.
To provide a paradigmatic example, one termite by itself might be incapable of constructing a mound, and yet the overall colony can do so quite proficiently.
Emergent goals, on the other hand, are agnostic to the group's (or any individual's) abilities,\footnote{This claim is sometimes known as the `orthogonality thesis': goals and capabilities (i.e., one's means to achieve one's goals) are independent, or `orthogonal', to one another \citep{Bostrom2014}.} and can be used to model the group's objectives, which supervene on the individuals' objectives. 
Thus while it might be unreasonable to model a single termite as having the goal of building a mound, this goal could be highly predictive of the overall colony's behaviour.


\subsubsection{Instances}

Before proceeding further, we note that discussions of emergent phenomena in systems of advanced AI agents are necessarily quite speculative, as it is challenging (both in theory and in practice) to identify such phenomena.%
\footnote{Indeed, the astute reader will notice that this section is the only section of the report that does not have at least one corresponding case study.
While there are demonstrations of AI agents exhibiting emergent collective capabilities and goals (see, e.g., \citet{werfel2014}, in which a swarm of simple, termite-inspired construction robots are able to build large-scale structures without centralized coordination), we are not aware of examples involving collective agency among \textit{advanced} AI agents (such as those powered by LLMs) or collective agency that represents an obvious \textit{risk}.}
We therefore attempt to draw lessons from simpler AI systems or biological entities, while highlighting that advanced AI agents could also possess features that make the transition to higher-level agency easier, such as the ability to more easily share information, replicate, and update their behaviour \citep{Conitzer2023}.

\paragraph{Emergent Capabilities.}
Dangerous {emergent capabilities} could arise when a multi-agent system overcomes the safety-enhancing limitations of the individual systems, such as individual models' narrow domains of application or myopia caused by a lack of long-term planning and long-term memory.
For example, narrow systems for research planning, predicting the properties of molecules, and synthesising new chemicals could, when combined, lead to a complex `test and iterate' automated workflow capable of designing dangerous new chemical compounds far beyond the scope of the initial systems' capabilities \citep{boiko_emergent_2023,urbina_dual_2022,Luo2024}.
This is similar to how a myopic actor and a passive critic can combine to produce an actor-critic algorithm capable of long-term planning via RL \citep{konda1999actor}.
This possibility is important for safety -- and for future AI ecosystems made of specialised `AI services' \citep{Drexler2019} -- as generally intelligent autonomous systems could pose much greater risks than narrow AI tools \citep{Chan2023-aj}.\footnote{Despite this, many companies are racing to build AI agents \citep{OpenAI_agents,GDM_agents,Microsoft_agents,Meta_agents,Anthropic_agents}, including early efforts attempting to construct composite agents based on simpler components including powerful foundation models \citep[e.g.,][]{schick2023toolformer,wu2024autogen}.}
More speculatively, the combination of advanced AI agents could eventually lead to recursive self-improvement at the collective level, as AI research itself becomes increasingly automated \citep{Hutter2019,Mankowitz2023,Agnesina2023,Lu2024}, even though no individual system possesses this capability.

\paragraph{Emergent Goals.}
Ascribing goals to a system is not always straightforward.
For our present purposes, it will suffice to adopt a Dennetian perspective \citep{Dennett1971}, ascribing goals and intentions only when it is useful (i.e., predictive) to do so.\footnote{See \citet{oesterheld2016formalizing,Halpern2018,Orseau2018,Everitt2021,Kenton2022,Biehl2023,MacDermott2024,Ward2024} for recent, formal examinations of agency and incentives in AI systems, and the implications thereof for safety.}
While it might not be helpful to describe individual narrow AI tools as having goals, their combination may act as a (seemingly) goal-directed collective.
For example, a group of moderation bots on a major social networking site could subtly but systematically manipulate the overall political perspectives of the user population, even though, individually, each agent is programmed to simply increase user engagement or filter out dis-preferred content.
Other dangerous goals that could emerge from groups of more advanced AI agents include power-seeking \citep{Turner2022,Carlsmith2022}, self-preservation \citep{Omohundro2008,Lyon2011}, or competing against other groups \citep{Bakhtin2022}, which could be instrumentally useful at the collective level even if avoidable or not useful at the individual level.




\subsubsection{Directions}

Insofar as the prospect of collective goals and capabilities emerging from large numbers of advanced AI agents remains somewhat speculative, it will be especially useful to develop a firmer theoretical understanding of when and how these novel forms of agency might emerge.
This understanding should be complemented by preliminary empirical investigations, potentially in settings with less advanced agents or smaller numbers of agents.


\paragraph{Empirical Exploration.}
By definition, emergent properties are hard to predict when looking at individual components.
Unfortunately, {exploratory empirical studies} of emergent behaviour among \textit{large numbers} of state-of-the-art systems in realistic environments are highly challenging.
One reason is that experiments using many model instances are very expensive. 
Another is that is difficult to construct relevant environments and `sandboxes' which are similar enough to the real world for us to gain transferable insights.
Nonetheless, research such as that of \citet{park2023generative,Chen2024c, vezhnevets2023generative} shows this to be possible in simple games, and that it can lead to surprising outcomes.
Future work could use more realistic or open-ended environments, such as those involving economic activity \citep{Zheng2022}, or games inspired by massively multiplayer online role-playing games \citep{Suarez2019}.
This would help address the crucial problem of understanding and eventually being able to predict the settings under which undesirable behaviours emerge at the group level and how robust they are, including the influence of key factors and conditions such as the degree of competition, the (non-)diversity of the agents, and their individual capability levels.

\paragraph{Theories of Emergent Capabilties.}
In conjunction with empirical studies, we must develop a {theoretical understanding of emergent capabilities} that can be applied to groups of frontier models.
Existing work in this area either identifies a specific emergent behaviour in advance and attempts to measure the presence or cause of this behaviour based on pre-existing observations \citep{Seth2006,Chen2009}, or formalises some abstract notion of a micro- and macro-level and attempts to detect newly emergent behaviours by comparing the difference \citep{Kubik2003,Teo2013,Szabo2015}, the idea being that emergent phenomena are those present in the latter but not the former.
Other related works include that of \citet{Sourbut2024}, who propose a theoretical method of separately measuring individual and collective capabilities and (mis)alignment in strategic settings.
These approaches are computationally expensive, however, and their empirical utility us yet to be convincingly demonstrated.\footnote{Moreover, they may depend on access to information about deployed agents that is unavailable not just to third parties, but also to other model providers seeking to measure emergent behaviours in their agents' interactions with others.}
Promising directions include developing tractable proxies of these measures, and the use of ML \citep{Dahia2024} and distributed methods \citep{Wang2016a,Otoole2017} to improve scalability.

\paragraph{Theories of Emergent Goals.}
It is especially important to know what we ought to measure here, as some techniques for understanding the goals of a single agent, such as interpretability methods \citep{Michaud2020,Colognese2023,Mini2023,Marks2023} might not be easily applied to group-level emergent goals \citep{Grupen2022}.
Many formal approaches to measuring and detecting goal-directedness make use of causal models \citep{Halpern2018,Everitt2021,Kenton2022,MacDermott2024,Ward2024}.
A natural next step towards generalising these works to consider emergent goals in \textit{multi-agent} settings would therefore be to apply them in the context of \emph{causal games} \citep{Hammond2023}.
This line of work would also benefit from the insights of other fields that have sought to develop theories of emergent agency \citep{friston2022designing,Okasha2018,Smith2020}.


\paragraph{Monitoring and Intervening on Collective Agents.}
Once we possess a better theoretical and empirical understanding of emergence in advanced multi-agent systems, it will be important to develop the tools and infrastructure to {monitor for, and intervene on, forms of emergent, collective agency}.
In practice, this is likely to overlap substantially with the tools required to monitor the macroscopic properties of large, dynamic networks of agents (see \Cref{sec:network_effects,sec:destabilising_dynamics}).
Similarly, interventions for mitigating undesirable forms of emergent behaviour may be related to those for mitigating collusion (see \Cref{sec:collusion}) or deleterious selection pressures (see \Cref{sec:selection_pressures}).
In tandem, we ought to develop \textit{evaluations} for dangerous emergent behaviours in multi-agent systems.
For example, while a `one-shot' application of an LLM might not possess a particular ability (such as manipulating a human to take some action), a population of multiple LLMs and other AI tools might.
Similarly, while a single agent might not exhibit a certain sub-goal (such as self-preservation) while completing a task, a combination of agents might develop a mutual reliance upon one another that ends up having self-preservation as an instrumental sub-goal the collective level.



