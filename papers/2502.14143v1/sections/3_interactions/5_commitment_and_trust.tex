\subsection{Commitment and Trust}
\label{sec:commitment_and_trust}



In settings that require joint action in order to obtain a better outcome, inefficiencies can result whenever one or more actors cannot be trusted (perhaps due to strategic incentives, or due to their incompetence) to carry out their part of the plan.
These inefficiencies can be reduced via \textit{credible commitments} made by the untrusted parties.
Unfortunately, the ability to make credible commitments is `dual-use' and can therefore lead to new risks.

\subsubsection{Definition}

An actor makes a \textit{commitment} when they bind themselves to a course of action, such that reneging on that action would either be impossible or result in significant costs to themselves. 
A commitment is \textit{credible} when other actors believe that the actor making the commitment will follow through with the actions they claim to have committed to. 
Credible commitments are useful in scenarios where trust is essential but hard to establish, such as in international treaties, economic policies, and contractual agreements.

Since credible commitments can often help in achieving desirable cooperative outcomes, we expect there will be incentives to build systems capable of making them.
For example, an AI system can become more trustworthy by being credibly committed to erasing any private information revealed to it.
In contrast, human beings or organisations cannot reliably forget at will and may later leak private information, whether intentionally or not \citep{carnegie2019disclosure}.
Autonomous AI agents themselves might also serve as credible commitment devices \citep{McAfee1984,Howard1988,tennenholtz2004program}, enabling actors to carry out actions based on potentially complex conditions and thus helping to solve problems with incomplete contracting \citep{schmitz2001hold}.
However, the ability of AI systems to make commitments can also backfire in correspondingly severe ways, preventing recourse in high-stakes scenarios and enabling extortion and brinkmanship.

\subsubsection{Instances}

As noted above, the ability to form commitments can both precipitate and mitigate risks.
We therefore begin by considering risk instances that can arise due to a lack of trust, before turning to those that can arise via the very mechanisms that might be used to establish such trust.

\paragraph{Inefficient Outcomes.}
Without careful planning and the appropriate safeguards, we may soon be entering a world overrun by increasingly competent and autonomous software agents, able to act with little restriction.
The abilities of these agents to persuade, deceive, and obfuscate their activities, as well as the fact they can be deployed remotely and easily created or destroyed by their deployer, means that by default they may garner little trust (from humans or from other agents).
Such a world may end up being rife with economic inefficiencies \citep{schmitz2001hold,Krier2023}, political problems \citep{Kreps2023,Csernatoni2024}, and other damaging social effects \citep{Gabriel2024}.
Even if it is possible to provide assurances around the day-to-day performance of most AI agents, in high-stakes situations there may be extreme pressures for agents to defect against others, making trust harder to establish, and potentially leading to conflict \citep[see also \Cref{sec:conflict}]{fearon1995rationalist,powell2006war}.\footnote{A classic non-AI example is the hypothesis that a major contributor to World War I was Germany's concerns about the rising power of Russia \citep{02b46089-4baf-3ce5-9159-e4dbfc3050f6}. Conflict might have been avoided were Russia able to credibly commit not to expand its influence, but the absence of such an ability left Germany with fewer alternatives to conflict.}




    

    


    


\paragraph{Threats and Extortion.}
A natural solution to problems of trust is to provide some kind of commitment ability to AI agents, which can be used to bind them to more cooperative courses of action.
Unfortunately, the ability to make credible commitments may come with the ability to make credible \textit{threats}, which facilitate extortion and could incentivize brinkmanship (see \Cref{sec:conflict}).
For example, ransomware becomes more effective if the hacker can credibly commit to restore the victim's data upon receiving payment, and coercion using AI-controlled weapons could become more frequent if actors gain the ability to make credible threats conditional on complicated demands (see also \Cref{cs:dead_hand}).
More generally, an agent could use commitment devices to shift risks or costs to others, allowing it to behave irresponsibly.\footnote{In economics, this general problem (not necessarily as a result of the power of commitment) is known as `moral hazard'.}
In other cases, it might be the \textit{agent that commits} to an inflexible (cooperative) course of action which can be exploited by others who can adapt their strategies to this commitment.%
\footnote{An amusing, non-AI example of such a commitment is Red Lobster's ``Endless Shrimp'' deal, which has recently been blamed for driving it to bankruptcy \citep{Meyersohn2024}.}
On the other hand, if used carefully, the ability to commit generally strictly empowers the committing agent \citep{Stengel2010,Letchford2013}.

\paragraph{Rigidity and Mistaken Commitments.}
Even when it is desirable to be able to make threats in order to deter socially harmful behaviour, doing so using AI agents effectively removes the human from the loop, which could prove disastrous in high-stakes contexts (e.g., a false positive in a nuclear submarine's warning system; see also \Cref{cs:dead_hand}), or when irresponsible actors are enabled in making disproportionate or mistaken commitments.
On the other hand, such commitments may only be credible to the extent that a human cannot intervene, increasing the incentive for delegation to AI agents.
This could be worsened if other, potentially incompatible commitments can be made by other actors, leading to a `commitment race' \citep{commitment_races} or potential conflict.
In complex networks (see \Cref{sec:network_effects}), commitments triggered by a small number of agents could -- without careful planning -- cascade through the network and have a far more damaging effect \citep{Xia2010}.

\begin{case-study}[label=cs:dead_hand]{Dead Hands and Automated Deterrence}
    During the Cold War, the Soviet Union developed the the automated Perimeter system -- often called `Dead Hand' -- to guarantee a nuclear launch if its leadership were incapacitated, thus ensuring a credible commitment of retaliation \citep{Hoffman2009}. While this mechanism was intended as a deterrent, its automatic and largely irrevocable nature exemplifies how credible commitments can become dangerously dual-use: once triggered, there would be little chance to override or de-escalate. 
    In a similar vein, during Operation Iraqi Freedom in 2003 an automated US missile defence system shot down a British plane, killing both occupants \citep{Talbot2005,Borg2024}.
    While the system's operators had one minute to override the system (even in its autonomous mode), they decided to trust its judgment, resulting in a tragic outcome.
    In more general AI contexts, similarly inflexible commitments could offer short-term advantages or trust but risk uncontrolled escalation, lock-in, and catastrophic outcomes if not carefully designed with appropriate fail-safes and oversight.
\end{case-study}

\subsubsection{Directions}


As with any dual-use technology, ensuring it is used for beneficial rather than detrimental means can be extremely challenging.
We therefore attempt to focus on directions that \textit{differentially} advance beneficial uses \citep{Sandbrink2022}, while acknowledging that it will not, in general, be possible isolate these entirely.%
\footnote{Even in the case of human commitments, it is not always obvious which families of commitments are desirable to make. For example, if a state commits to refusing to negotiate with terrorists, they might end up sacrificing some lives while establishing a reputation that saves more lives over the long term. Similarly, a seller might refuse a low, though still positive, offer in order to achieve better offers in future. Such questions are often as much a matter of principle as they are of consequentialist reasoning.}

\paragraph{Keeping Humans in the Loop.}
Given the risks associated with the power of AI commitments, a key direction will be to lay out the domains in which they can be used and the kinds of commitments that are permitted.
For example, existing efforts have already sought to ensure that AI systems do not form a part of the nuclear chain of command \citep{Renshaw2024,Congress2023}.
It may be similarly important in other high-stakes settings to ensure that humans cannot be fully removed from the loop.\footnote{However, the inclusion of a human in the loop does not itself guarantee control. The presence of an algorithmic system can negatively influence human decision-making \citep{green_algorithm_loop_2020,green_flaws_2022,crootof_humans_2023,skitka_does_1999,goddard_automation_2012,Borg2024}, such as through automation bias. At the same time, it is important to acknowledge that humans suffer from their own flaws that might lead to risks and that might be (at least partly) overcome via the use of AI systems.}
While certain kinds of commitment device might still allow for malicious use (such as automated blackmail campaigns), regulation, safeguards, and infrastructure limiting where and how AI agents can be deployed could help prevent the worst offences \citep{Kolt2024,Chan2025}.

\paragraph{Limiting Commitment Power.}
Researchers should also explore ways to design AI systems that can make and adhere to commitments even in the face of changing circumstances or new information, thereby avoiding some of the risks associated with overly rigid strategies.
This might involve developing algorithms that can (learn to) renegotiate commitments in a fair and transparent manner when necessary \citep{Sandholm2002,Ho2014,Wang2023,Cohen2023}.
While agents equipped with commitment powers are not yet widespread, it would be valuable to begin preliminary studies now into demonstrations of their risks \citep[and benefits, see, e.g.,][]{Christoffersen2023,zhu2025learning}, as well as the feasibility of technical solutions, the tractability of governance solutions, and their intersection \citep{reuel2024open,Kolt2024}.

\paragraph{Institutions and Normative Infrastructure.}
Other important research directions include the development of normative infrastructure that can help establish trust without recourse to commitment devices that might be misused.
These collectively enforced rules and norms might serve to \textit{differentially} advance cooperation relative to coercion \citep{Sandbrink2022}.
For example, the introduction of unique agent identifiers \citep{Chan2024} would enable the construction of reputation systems, which are critically important in otherwise (pseudo-)anonymous interactions such as online marketplaces \citep{Tadelis2016}.
While reputation is still `dual-use' to the extent that one could develop a reputation for carrying out costly threats, doing so requires paying such costs and also being able to escape later punishment oneself, which may be a less viable strategy in many cases.
Other examples include determining the rules and principles via which liability for harms from AI agents is assigned \citep[see also \Cref{sec:governance}]{ayres_law_2024, lima_could_2017, lior_ai_2019, Kolt2024, Chopra2011, Solum1992}.

\paragraph{Privacy-Preserving Monitoring.}
In order for reputation systems to be effective for more general and widely deployed agents, it will be necessary to improve trust by \textit{monitoring} their actions \citep{Chan2024a}.
Monitoring also extends to scrutiny of the actors deploying those agents, who might claim to be running one kind of agent or using some kinds of data, while instead using others.
This in turn, however, raises clear and important privacy concerns.
There is thus an important need to develop privacy-preserving technologies for monitoring AI systems and the actions of autonomous agents \citep{Shavit2023WhatDI,vegesna2023privacy}.
Examples include the use of cryptography -- such as signatures that can serve as proof of learning \citep{Jia2021} or proof of inference \citep{Ghodsi2017}, protocols for decentralized verifiable computation \citep{yao_protocols_1982,47976}, and performing computations using encrypted data \citep{Martins2017,Dowlin2016} -- as well as tools for auditing and monitoring both software and hardware.

\paragraph{Mutual Simulation and Transparency.}
Finally, while monitoring and reputation systems might be able to render more transparent what an agent has done in the past, we may also want to use the unique properties of computational agents in order to predict what they will do in the future \citep{Conitzer2023}.
For example, such agents are written in code that can -- in theory -- be read or understood by other agents.
This kind of mutual transparency can beneficial in establishing trust and reaching more efficient outcomes \citep{McAfee1984,Howard1988,tennenholtz2004program,halpern2018game,Oesterheld2018,Han2021}, though has yet to find practical applications \citep{Critch2022}.
Similarly, even if one cannot peer inside the black box, the same code can be run multiple times on different inputs, allowing for simulations and tests prior to deployment or even individual interactions.
As with white-box access to other agents, these abilities can (in theory) provably reduce inefficiencies due to mistrust \citep{Kovarik2023,Kovarik2024,Chen2024b}, but have yet to be studied in the context of real-world strategic agents \citep[though see, e.g.,][]{Griffin2024,Greenblatt2023}.
More research is required to design and implement tractable versions of these methods in order to fulfil their theoretical promise.
