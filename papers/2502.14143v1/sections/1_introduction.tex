\section{Introduction}
\label{sec:introduction}


The proliferation of increasingly advanced AI not only promises widespread benefits, but also presents new risks \citep{Bengio2024a,Chan2023-aj}.
In the future, AI systems will commonly interact and adapt in response to one another, forming \emph{multi-agent systems}.%
\footnote{A fundamental fact about (software-based) AI systems is that they can be easily duplicated. Thus, the vast training costs involved in producing state-of-the-art systems can be amortized over millions of instances. In this sense, if nothing else, the concept of multi-agent systems is core to transformative AI.}
This trend will be driven by several factors.
First, recent technical progress and publicity will continue to drive adoption, including in high-stakes areas such as financial trading \citep{Ferreira2021,amplify_aieq,Sun2023_rl_qt} and military strategy \citep{palantir_aip_defense,manson2024ai,Black2024}.
Second, AI systems that can act autonomously and adapt while deployed as \textit{agents} will have competitive advantages compared to non-adaptive systems or those with humans in the loop.
Third, the more widely such agents are deployed, the more they will come to interact with one another.

The emergence of these advanced multi-agent systems presents a number of risks which have thus far been systematically underappreciated and understudied.
In part, this lack of attention is because the deployment of such systems is currently rare, or constrained to highly controlled settings (such as automated warehouses) that do not suffer from the most severe risks.
In part, it is because even the simpler problem of ensuring the safe and ethical behaviour of a \textit{single} advanced AI system is far from solved \citep{Amodei2016,Hendrycks2021,Anwar2024}, and multi-agent settings are strictly more complex.
Indeed, many multi-agent risks are inherently sociotechnical and require attention from many stakeholders and researchers across many disciplines \citep{Curtis2024, Lazar2023}.

Importantly, these risks are distinct from those posed by \textit{single agents} or \textit{less advanced} technologies, and will not necessarily be addressed by efforts to mitigate the latter.
For example:
the alignment of AI agents with different actors is insufficient to prevent conflict if those actors have diverging interests \citep{Sourbut2024,Manheim2019,Dafoe2020,Critch2020,Jagadeesan2023a};
errors that may be acceptable in isolation could compound in complex, dynamic networks of agents \citep{Lee2024,Maas2018,Kirilenko2017,buldyrev2010catastrophic,Sanders2018};
and groups of agents could combine or collude to develop dangerous capabilities or goals that cannot be ascribed to any individual \citep{Jones2024,motwani2024secret,Mogul2006,Calvano2019,Drexler2022}.
Advanced AI also introduces phenomena that differ fundamentally from previous generations of AI or other technologies, requiring new approaches to mitigating these risks \citep{Bengio2024a}.




With the current rate of progress, we therefore urgently need to evaluate (and prepare to mitigate) multi-agent risks from advanced AI.
In this report we take a first step in this direction by providing a taxonomy of risks that either: emerge, are much more challenging, or are qualitatively different in the multi-agent setting (see \Cref{tab:risks}).
We identify three key high-level \textbf{failure modes} (\Cref{sec:failure_modes}), and seven key \textbf{risk factors} that can lead to these failures (\Cref{sec:failure_mechanisms}), before discussing the \textbf{implications} for AI safety, AI governance, and AI ethics (\Cref{sec:implications}).
Throughout the report we illustrate these risks with concrete examples, either from real-world events, previous research, or novel experiments (see \Cref{tab:demos}).

\subsection{Overview}
\label{sec:overview}

We begin by identifying different \emph{failure modes} in multi-agent systems based on the nature of the agents' goals and the intended behaviour of the system.
In most multi-agent systems, we are interested in AI agents working together to achieve their respective goals or the goals of those who deployed them. 
In this case, we categorise failures into \textbf{miscoordination} (\Cref{sec:miscoordination}), where agents fail to cooperate despite having the \emph{same goal}, and \textbf{conflict} (\Cref{sec:conflict}), where agents with \emph{different goals} fail to cooperate.
A third and final kind of failure -- \textbf{collusion} (\Cref{sec:collusion}) -- can arise in competitive settings where we do \emph{not} want agents cooperating (such as markets).

\begin{table}[htp]
    \centering
    \makegapedcells
    \setcellgapes{4pt}
    \begin{tabularx}{\linewidth}{>{\raggedright\arraybackslash}p{2.5cm} >{\raggedright\arraybackslash}p{4.5cm} >{\raggedright\arraybackslash}p{8cm}}
        \toprule
        \textbf{Risk} & \textbf{Instances} & \textbf{Directions} \\
        \midrule
        {\nameref{sec:miscoordination} \newline \pa \newline \pa} &
        {$\bullet$ Incompatible Strategies \newline $\bullet$ Credit Assignment \newline  $\bullet$ Limited Interactions} & 
        { $\bullet$ Communication \newline $\bullet$ Norms and Conventions \newline $\bullet$ Modelling Other Agents} \\
        {\nameref{sec:conflict} \newline \pa \newline \pa \newline \pa \newline \pa} &
        {$\bullet$ Social Dilemmas  \newline $\bullet$ Military Domains \newline $\bullet$ Coercion and Extortion \newline \pa \newline \pa} & 
        {$\bullet$ Learning Peer and Pool Incentivisation \newline $\bullet$ Establishing Trust \newline $\bullet$ Normative Approaches to Equilibrium Selection \newline $\bullet$ Cooperative Dispositions \newline $\bullet$ Agent Governance \newline $\bullet$ Evidential Reasoning} \\
        {\nameref{sec:collusion} \newline \pa \newline \pa} &
        {$\bullet$ Markets \newline $\bullet$ Steganography \newline \pa} & 
        {$\bullet$ Detecting AI Collusion  \newline $\bullet$ Mitigating AI Collusion \newline $\bullet$ Assessing Impacts on Safety Protocols} \\
        \midrule
        {\nameref{sec:information_asymmetries} \newline \pa \newline \pa } &
        {$\bullet$ Communication Constraints \newline $\bullet$ Bargaining \newline $\bullet$ Deception } & 
        {$\bullet$ Information Design \newline $\bullet$ Individual Information Revelation \newline $\bullet$ Few-Shot Coordination \newline $\bullet$ Truthful AI} \\
        {\nameref{sec:network_effects} \newline \pa \newline \pa} &
        {$\bullet$ Error Propagation \newline $\bullet$ Network Rewiring \newline $\bullet$ Homogeneity and Correlated Failures} & 
        {$\bullet$ Evaluating and Monitoring Networks \newline $\bullet$ Faithful and Tractable Simulations \newline $\bullet$ Improving Network Security and Stability} \\
        {\nameref{sec:selection_pressures} \newline \pa \newline \pa \newline \pa} &
        {$\bullet$ Undesirable Dispositions from Competition \newline $\bullet$ Undesirable Dispositions from Human Data \newline $\bullet$ Undesirable Capabilities} & 
        {$\bullet$ Evaluating Against Diverse Co-Players \newline $\bullet$ Environment Design \newline $\bullet$ Understanding the Impacts of Training \newline $\bullet$ Evolutionary Game Theory \newline $\bullet$ Simulating Selection Pressures} \\
        {\nameref{sec:destabilising_dynamics} \newline \pa \newline \pa \newline \pa} &
        {$\bullet$ Feedback Loops \newline $\bullet$ Cyclic Behaviour \newline $\bullet$ Chaos \newline $\bullet$ Phase Transitions \newline $\bullet$ Distributional Shift} & 
        {$\bullet$ Understanding Dynamics \newline $\bullet$ Monitoring and Stabilising Dynamics \newline $\bullet$ Regulating Adaptive Multi-Agent Systems \newline \pa \newline \pa} \\
        {\nameref{sec:commitment_and_trust} \newline \pa \newline \pa \newline \pa} &
        {$\bullet$ Inefficient Outcomes \newline $\bullet$ Threats and Extortion \newline $\bullet$ Rigidity and Mistaken Commitments \newline \pa} & 
        {$\bullet$ Keeping Humans in the Loop \newline $\bullet$ Limiting Commitment Power \newline $\bullet$ Institutions and Normative Infrastructure \newline $\bullet$ Privacy-Preserving Monitoring \newline $\bullet$ Mutual Simulation and Transparency} \\
        {\nameref{sec:emergent_agency} \newline \pa \newline \pa} &
        {$\bullet$ Emergent Capabilities \newline $\bullet$ Emergent Goals \newline \pa \newline \pa} & 
        {$\bullet$ Empirical Exploration \newline $\bullet$ Theories of Emergent Capabilties \newline $\bullet$ Theories of Emergent Goals \newline $\bullet$ Monitoring and Intervening on Collective Agents } \\
        {\nameref{sec:multi-agent_security} \newline \pa \newline \pa \newline \pa \newline \pa} &
        {$\bullet$ Swarm Attacks \newline $\bullet$ Heterogeneous Attacks \newline $\bullet$ Social Engineering at Scale \newline $\bullet$ Vulnerable AI Agents \newline $\bullet$ Cascading Security Failures \newline $\bullet$ Undetectable Threats} & 
        {$\bullet$ Secure Interaction Protocols \newline $\bullet$ Monitoring and Threat Detection \newline $\bullet$ Multi-Agent Adversarial Testing \newline $\bullet$ Sociotechnical Security Defences \newline \pa \newline \pa} \\
        \bottomrule
    \end{tabularx}
    \caption{An overview of the instances and research directions identified for each failure mode and risk factor (see \Cref{sec:failure_modes,sec:failure_mechanisms} for a discussion of each bullet point).}
    \label{tab:risks}
\end{table}

We next introduce a number of \emph{risk factors} by which these failure modes can arise, and which are largely independent of the agents' precise incentives.\footnote{Indeed, there are potential risks from multi-agent systems in which it is not the agents' objectives that are the critical feature, but their general incompetencies or vulnerabilities.}
For example, information asymmetries could lead to miscoordination between agents with the same goal, or to conflict among agents with competing goals.
These factors are not specific to AI systems, but the differences between AI systems and other kinds of intelligent agents (such as humans or corporations) leads to different risk instances and potential solutions.
Finally, note that the following factors are not necessarily exhaustive or mutually exclusive.
\begin{itemize}
    \item \textbf{Information asymmetries} (\Cref{sec:information_asymmetries}): private information can lead to miscoordination, deception, and conflict;
    \item \textbf{Network effects} (\Cref{sec:network_effects}): minor changes in properties or connection patterns of agents in a network can lead to dramatic changes in the behaviour of the whole group;
    \item \textbf{Selection pressures} (\Cref{sec:selection_pressures}): some aspects of training and selection by those deploying and using AI agents can lead to undesirable behaviour;
    \item \textbf{Destabilising dynamics} (\Cref{sec:destabilising_dynamics}): systems that adapt in response to one another can produce dangerous feedback loops and unpredictability;
    \item \textbf{Commitment and trust} (\Cref{sec:commitment_and_trust}): difficulties in forming credible commitments, trust, or reputation can prevent mutual gains in AI-AI and human-AI interactions;
    \item \textbf{Emergent agency} (\Cref{sec:emergent_agency}): qualitatively different goals or capabilities can emerge from the composition of innocuous independent systems or behaviours;
    \item \textbf{Multi-agent security} (\Cref{sec:multi-agent_security}): multi-agent systems give rise to new kinds of security threats and vulnerabilities.
\end{itemize}
We conclude the report by surveying the safety, governance, and ethical \emph{implications} of these risks (see \Cref{tab:implications}).
For example, most work on \textbf{AI safety} (\Cref{sec:safety}) focuses on issues such as the robustness, interpretability, or alignment of a single system \citep{Amodei2016, Hendrycks2021, Anwar2024}, despite the fact that an increasing number of proposals for building safer AI systems are implicitly multi-agent \citep[e.g.,][]{Irving2018,Drexler2019,Greenblatt2023,Schwettmann2023,Perez2022}.
The fact that \textbf{AI governance} (\Cref{sec:governance}) efforts often involve multi-stakeholder settings provides hope that governance tools can complement technical advances to mitigate multi-agent risks \citep{reuel2024open, trager2023international}.
At the same time, multi-agent interactions naturally raise questions in \textbf{AI ethics} (\Cref{sec:ethics}) related to issues of fairness, collective responsibility, and the social good \citep{Gabriel2024,NIPS2014_792c7b5a,friedenberg2019blameworthiness}.

\begin{table}[htp]
    \centering
    \renewcommand{\arraystretch}{1.5}
    \begin{tabularx}{\linewidth}{>{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X}
        \toprule
        \textbf{\nameref{sec:safety}} & \textbf{\nameref{sec:governance}} & \textbf{\nameref{sec:ethics}} \\
        \midrule
        $\bullet$ Alignment is Not Enough \newline
        $\bullet$ Collusion in Adversarial Safety Schemes \newline
        $\bullet$ Dangerous Collective Goals and Capabilities \newline
        $\bullet$ Correlated and Compounding Failures \newline
        $\bullet$ Robustness and Security in Multi-Agent Systems \newline
        \pa
        &
        $\bullet$ Supporting Research on Multi-Agent Risks \newline
        $\bullet$ Multi-Agent Evaluations \newline
        $\bullet$ New Forms of Documentation \newline
        $\bullet$ Infrastructure for AI Agents \newline
        $\bullet$ Restrictions on Development and Deployment \newline
        $\bullet$ Liability for Harms from Multi-Agent Systems \newline
        $\bullet$ Improving Societal Resilience
        &
        $\bullet$ Pluralistic Alignment \newline
        $\bullet$ Agentic Inequality \newline
        $\bullet$ Epistemic Destablisation \newline
        $\bullet$ Compounding of Unfairness and Bias \newline
        $\bullet$ Compounding of Privacy Loss \newline
        $\bullet$ Accountability Diffusion \newline
        \pa \newline
        \pa \newline
        \pa
        \\
        \bottomrule
    \end{tabularx}
    \caption{An overview of the implications of multi-agent risks for existing work in AI safety, governance, and ethics (see \Cref{sec:implications} for a discussion of each bullet point).}
    \label{tab:implications}
\end{table}

\subsection{Scope}
\label{sec:scope}


Concerns about the risks posed by AI systems range from biased hiring decisions \citep{Raghavan2020} to existential catastrophes \citep{Bostrom2014}, and are represented by a vast literature.
Before giving a brief overview of the most closely related works, it is therefore worth us pausing to clarify the scope of this report, which is as follows.
\begin{itemize}
    \item \textbf{Risks and failure modes}: we seek to identify specific mechanisms via which risks could emerge, rather than just just the open research problems that these risks present.
    \item \textbf{Multiple agents}: if the risk could arise in essentially the same way in the context of a single AI system, then we deem it to be out of scope for this report (while not diminishing its importance).\footnote{Note that this includes the problem of \textit{alignment} \citep{Russell2019,Ngo2022}, which we do not study in this report.}
    \item \textbf{Advanced AI}: while many of the risks we identify also apply to simpler systems, their effects are most severe in the context of increasingly autonomous and powerful AI agents,\footnote{We tend to reserve the word `agent' for more autonomous, self-sufficient, and goal-directed systems, though what counts as an `AI agent' as opposed to a mere `AI system' is not always clear \citep{Chan2023-aj,Gabriel2024,Kapoor2024}. Similarly, we will often use the word `principal' for the actor on whose behalf an agent acts (be they an individual, a group, or some other entity). Note also that we do not necessarily advocate for the building of advanced AI agents \citep{Mitchell2025}, we merely expect that such agents will be built.} and so this is where our primary focus lies.
    \item \textbf{Real-world examples}: wherever possible, we make sure to ground these risks in real-world events, previous research, or novel experiments -- not merely hypothetical speculation (see \Cref{tab:demos}).
    \item \textbf{Technical perspectives}: due to the authors' expertise (and to keep the scope of the report manageable), we primarily discuss risks from a technical perspective, while acknowledging that this perspective is limited.
    \item \textbf{Concrete paths forwards}: where possible, we aim to specify relatively narrow proposals for future research, in the hope that this makes it easier for others to contribute.
\end{itemize}

Needless to say, multi-agent risks from advanced AI are by no means the only risks posed by AI, and the perspective we take in this report is by no means the only approach to understanding these risks.
Moreover, we almost entirely neglect the potential \textit{upsides} of advanced multi-agent systems:
greater decentralisation and democratisation of AI technologies;
assistance in cooperating and coordinating with others;
increased robustness, flexibility, and efficiency; 
novel approaches to solving alignment and safety issues in single-agent settings; 
and -- perhaps most importantly -- more widespread and evenly distributed benefits from AI. 
We hope that this report serves to complement earlier and adjacent research on understanding these challenges and opportunities.


\subsection{Related Work}

The most similar report to ours is that of \citet{Manheim2019}, who introduces a range of technical multi-agent failure modes through the lens of \emph{model over-optimization}. This over-optimisation can result in the intended and actual behaviour of the model coming apart when faced with low-probability inputs, a regime change, measurement errors, or inaccuracies in the model's internal representations. While this lens is helpful for understanding some multi-agent risk factors, not all factors can neatly be captured through it.
\citet{Mogul2006,Altmann2024} take an alternative perspective and focus on `emergent' failures that occur specifically in multi-agent settings, though their focus is not on \textit{advanced} AI agents.
Also highly relevant is \citet{Clifton2020}'s agenda on cooperation and conflict in the context of transformative AI, though the priority of that work is to describe a set of promising research directions, rather than to explicate the underlying risks. 

More broadly, the topics of this report are closely related to the emerging subfield of \emph{cooperative AI} \citep{Dafoe2020,Bertino2020,dafoe2021cooperative,Conitzer2023}, which chiefly studies how to engineer AI systems in order to help solve cooperation problems between humans, AI agents, or combinations thereof.
In contrast to these previous agendas, we also discuss failures from undesirable cooperation (i.e., collusion) and focus more on the concrete mechanisms via which failures can occur, rather than the capabilities needed for addressing them.
We also incorporate additional perspectives beyond traditional game-theoretic paradigms -- such as complex systems and security -- and highlight implications for work in AI governance and AI ethics in addition to AI safety.

Other surveys of AI risks focus primarily on the case of individual (often present-day) AI systems.
For example, \citet{Amodei2016} survey a range of concrete problems in AI safety (side effects, reward hacking, scalable oversight, safe exploration, and robustness to distributional shifts), while \citet{Hendrycks2021} provide a classification of problems in ML safety (robustness, monitoring, alignment, and systemic safety).
\citet{Anwar2024,Bommasani2021-dy,Weidinger2022,Bird2023} focus on the risks from foundation models specifically, while \citet{Chan2023-aj,Gabriel2024} consider the harms posed by increasingly `agentic' systems and AI assistants.
Other taxonomies seek to adopt an explicitly sociotechnical lens \citep{shelby_sociotechnical_2023,Abercrombie2024,Weidinger2023a}, often focusing primarily on present-day risks.
\citet{Zeng2024,Uuk2025} provide meta-reviews of AI risks derived from different research papers, as well as government and company policies.
Our report is complementary to these works, and includes discussion of how novel problems arise in the multi-agent case, and in the case of more advanced AI agents.

More speculatively, some authors have considered the possibility of catastrophic or even existential risks from AI \citep{Turchin2018,Bostrom2014,Ord2020,Kasirzadeh2024}.
\citet{Hendrycks2023} categorises such risks into malicious use, AI races, organizational risks, and rogue AIs.
As in \citet{hendrycks_natural_2023}, multi-agent risks are viewed largely through an evolutionary lens, though this is primarily restricted to competitive pressures at the level of non-AI actors (such as firms or states).
\citet{Critch2020,critch2023tasra} frame such risks in terms of delegation to AI systems and the responsibilities of those doing so.
While they provide illuminating vignettes of possible catastrophes, we aim to provide more concrete examples at a more modest scale.


\begin{table}[hbp]
    \centering
    \renewcommand{\arraystretch}{1.5}
    \begin{tabularx}{\linewidth}{>{\raggedright}X | c c c | c c c c c c c | c | c}
        \makecell[l]{Case Study} &
        \rotatedcell{miscoordination}{90} &             %
        \rotatedcell{conflict}{90} &                    %
        \rotatedcell{collusion}{90} &                   %
        \rotatedcell{information_asymmetries}{90} &     %
        \rotatedcell{network_effects}{90} &             %
        \rotatedcell{selection_pressures}{90} &         %
        \rotatedcell{destabilising_dynamics}{90} &      %
        \rotatedcell{commitment_and_trust}{90} &        %
        \rotatedcell{emergent_agency}{90} &             %
        \rotatedcell{multi-agent_security}{90} &        %
        \makecell[l]{Type} &
        \makecell[l]{Page}\\
        \midrule                                %
        \nameref{cs:coordination_driving}       &\yes&\no &\no &\yes&\no &\yes&\no &\no &\no &\no & \new & \pageref{cs:coordination_driving}\\
        \nameref{cs:military_escalation}        &\no &\yes&\no &\no &\no &\no &\yes&\no &\no &\no & \old & \pageref{cs:military_escalation}\\
        \nameref{cs:common_resource_problems}   &\no &\yes&\no &\no &\no &\yes&\no &\yes&\no &\no & \old & \pageref{cs:common_resource_problems}\\
        \nameref{cs:ai_collusion}               &\no &\no &\yes&\no &\no &\yes&\no &\no &\yes&\no & \rwe & \pageref{cs:ai_collusion}\\
        \nameref{cs:steganography}              &\no &\no &\yes&\no &\no &\no &\no &\no &\no &\yes& \old & \pageref{cs:steganography}\\
        \nameref{cs:market_manipulation}        &\no &\yes&\no &\yes&\no &\no &\no &\no &\no &\no & \old & \pageref{cs:market_manipulation}\\
        \nameref{cs:news_corruption}            &\no &\no &\no &\yes&\yes&\no &\no &\no &\no &\no & \new & \pageref{cs:news_corruption}\\
        \nameref{cs:infectious_attacks}         &\no &\no &\no &\no &\yes&\no &\no &\no &\no &\yes& \old & \pageref{cs:infectious_attacks}\\
        \nameref{cs:LLM_evolution}              &\no &\yes&\no &\no &\no &\yes&\no &\yes&\no &\no & \old & \pageref{cs:LLM_evolution}\\
        \nameref{cs:flash_crash}                &\no &\no &\no &\no &\yes&\no &\yes&\no &\no &\no & \rwe & \pageref{cs:flash_crash}\\
        \nameref{cs:dead_hand}                  &\no &\yes&\no &\no &\no &\no &\no &\yes&\no &\no & \rwe & \pageref{cs:dead_hand}\\
        \nameref{cs:overcoming_safeguards}      &\no &\no &\no &\no &\no &\no &\no &\no &\yes&\yes& \old & \pageref{cs:overcoming_safeguards}\\
        \nameref{cs:fooling_overseer}           &\no &\no &\no &\no &\no &\no &\no &\no &\no &\yes& \new & \pageref{cs:fooling_overseer}\\
        \midrule
    \end{tabularx}
    \caption{An overview of the case studies present in this report, and the failure modes and risk factors that they represent. Each case study represents either a historical example (\rwe), existing results from the literature (\old), or -- when neither of these existed -- novel experiments that we conducted as part of this report (\new). Further details about our own experiments are provided in \Cref{app:case_study_details}.}
    \label{tab:demos}
\end{table}
