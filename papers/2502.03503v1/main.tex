%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% ADDED PACKAGES
\usepackage{amssymb} % Nécessaire pour \mathbb
\usepackage{amsmath}
\usepackage[most]{tcolorbox}
\usepackage{multirow}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\newcommand{\x}{{\hat{x}}}
\newcommand{\fh}{\hat{f}}
\usepackage{hyperref}
\newcommand{\hidden}[1]{}
\usepackage{url}
\usepackage{graphicx}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{proposition}{Proposition}
\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma}
\newtheorem{obs}{Observation}



% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
%%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
%\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{}

\begin{document}

\twocolumn[
\icmltitle{Two in context learning tasks with complex functions}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
%\icmlauthor{Firstname1 Lastname1}{equal,yyy}
%\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
%\icmlauthor{Firstname3 Lastname3}{comp}
%\icmlauthor{Firstname4 Lastname4}{sch}
\icmlauthor{Omar Naim}{yyy}
\icmlauthor{Nicholas Asher}{yyy}
%\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
%\icmlauthor{Firstname8 Lastname8}{sch}
%\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{IRIT, Toulouse, France}
%\icmlaffiliation{comp}{Company Name, Location, Country}
%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Omar Naim}{omar.naim.docs@gmail.com}
\icmlcorrespondingauthor{Nicholas Asher}{nicholas.asher@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We examine two in context learning (ICL) tasks with mathematical functions in several train and test settings for transformer models.  Our study generalizes work on linear functions by showing that small transformers, even models with attention layers only, can approximate arbitrary polynomial functions and hence continuous functions under certain conditions.  Our models also can  approximate previously unseen classes of polynomial functions, as well as the zeros of complex functions.  Our models perform far better on this task than LLMs like GPT4 and involve complex reasoning when provided with suitable training data and methods. Our models also have important limitations; they fail to generalize outside of training distributions and so don't learn class forms of functions. We explain why this is so. %{\color {magenta} a modifier}
\end{abstract}

\section{Introduction}

In-context learning (ICL) \cite{brown:etal:2020} enables a large language model (LLM) to learn a task from an instructional prompt and a few examples at inference time, without any adjustment of the model's parameters from pretraining.  Despite prior work on linear functions, the study of ICL on more general classes of functions is largely virgin territory.  

We present such a study with the following findings. Smaller LLMs with both attention only architectures and full transformer architectures can ICL arbitrary polynomial and hence all continuous, real valued functions  $f$ over an interval $[a,b] \subset \mathbb{R}$ that ``sufficiently covers'', in a sense to be defined below, the training samples for $f$ and values $x_i$.  

Our models also approximate functions in $[a,b]$ whose class forms were not encountered in training, and their overall performance also improves: thus, changing the training regime helps with generalization.  In addition, with the same training used for predicting $f(x_i)$ our models can solve the inverse problem of ICL functions; our models can approximate zeros of arbitrary polynomial functions and do so better than state of the art LLMs like GPT4.% for which no algorithm for finding such roots by radicals (nth roots, multiplication, division, addition and subtraction) exists.  

However, our models fail to generalize their predictions to values for $f$, for $x, f(x) \not\in [a,b]$. Thus, the models use none of the algorithms (linear, ridge regression, Newton's method) suggested in the literature, and    
 fail to learn the class forms of ${\cal P}^n$ for any $n$. We give a mathematical argument showing that this failure stems from limitations of the matrices used to define attention.  These limitations, we believe, come from the model's pretraining and are not easily solved. %Our models use a method based on sequence induction that applies equally well to all the tasks we studied.%polynomial classes ${\cal P}^n$, and by extension to all continuous functions.  

ICL depends on a model's pretraining, and so small transformer models with limited pretraining from scratch are essential for our investigation.  %, since investigating ICL requires training from scratch.  
We trained our models from scratch and studied the 1 dimensional case of functions in ${\cal P}^n$ (the class of polynomial functions of degree $n$) for $1 \leq n \leq 8$ for over 30 models and with a variety of train and testing distributions.  Our research demonstrates that ICL capabilities can be substantially enhanced through targeted adaptation during pretraining. %, from ones with 1 attention head (AH) and 1 MLP layer up 12 MLP layers and 8 AH. 
%We also studied small attention-only models \cite{olsson:etal:2022}.  
  %Our main findings are these.  

  
%However, another way of measuring error produces a non-negligeable error rate even for large models trained on $N(0,1)$ and tested on functions with coefficients $a,b \in [-1,1]$. 

%1. Several recent papers claim that Transformer based models trained from scratch can through ICL %linear functions with performances close to algorithms such as Least Squares, and that a transformer can 
%implement algorithms like linear and ridge regression or Newton's method. 

Our paper is organized as follows.  In Section 2, we define two ways of ICL a function; one, {\em ICL}$_1$ holds when the model performs well when training and testing distributions coincide; a second, {\em ICL}$_2$ holds when the model is able to generalize its performance.  Section 3 describes related work.  Section 4 shows that %, given a training over prompts of sequences that a sampling of sufficiently many data points and functions with coefficients in an interval $[a,b]$, a prompt sequence $x_1, f(x_1), x_2, f(x_2), ..., x_n, ?$, where we ask a model to predict $f(x_n)$, will enable the 
a small transformer model can ICL$_1$ arbitrary continuous functions.  %We denote the class of polynomials of degree $m$ by ${\cal P}^m$.  %whose coefficients $a_i$ are such that $a_i, x \in [a,b]$.  %Given the Stone Weierstrass theorem, they can thus approximate arbitrarily well any continuous function $f$ over $[a,b]$.  We show that they can learn also functions $f$ where the derivative $f'$ is not defined.
Section 5 shows that our models fail to ICL$_2$ functions from ${\cal P}^n$ for any $n$.     
 %We detail ICL capacities and limitations for polynomials, extending work of \cite{naim:asher:2024b}.%We trained transformers on different distributions various  distributions.%. Some distributions provided more robust performance than $N(0,1)$.
%2. On the other hand, our experiments how that all our models, given a particular interval $[a,b]$ and a suitable training and testing regime, can learn arbitrary polynomials.    
%We then generalize prior work to show that observations made in the literature for linear functions apply to higher degree polynomials as well.  
%In particular, ICL capacity depends on the density of the training sequences near the test sequence, as well as on boundary values \cite{naim:asher:2024b} determined by the training distributions.
%We also show that models exploit the whole prompt sequence in making their predictions and that models with attention only layers have ICL capacity for higher order polynomials.
Sections 6 investigates how models generalize to unseen polynomial classes, and chapter 7 investigates the problem of finding zeros of functions.  %First, our models with ICL can predict $f(x)$, $f \in {\cal P}^m$, when they have not seen any functions in ${\cal P}^m$ in their training.  
   Section 8 provides a general discussion of our models' capacities.  We then conclude.





\section{Background}
    
In ICL, a transformer style model learns a function $f$ given in-context examples at inference time in the following next-token-prediction format \cite{brown:etal:2020}: given a prompt containing a task input-output examples $(x_1,f(x_1),..,x_n, ?)$, the model is asked to generate a value for $f(x_n)$.  This prompt is simple but precise and avoids the variable performance of more complex prompts.  Our aim is to study functions over any fixed interval $[a,b] \subset \mathbb{R}$.  But as we can always translate any result on $[a,b]$ to a result on $[-1,1] \subset \mathbb{R}$, we concentrate on $[-1,1]$, though we also look at larger intervals, e.g. $[-5,5]$ for training and testing.

 Learnability is often characterized via empirical risk minimization (ERM), but \cite{shalev:etal:2010} argue for a more general definition.%there exist learning problems where uniform convergence does not hold, ERM fails, but these problems remain learnable with alternative methods.}
We thus define learnability via the notion of {\em uniform consistency} \cite{neyshabur:etal:2017general,villa:etal:2013}.  Let $\mu$ be a distribution over ${\cal H}$ and $\mu_n$ the update of $\mu$ after $n$ training samples $z_i = (x_i, y_i)$. Let $A_{z_n}$ be an algorithm for picking out a hypothesis from ${\cal H}$ based on $n$ training samples.  $\mathit{inf}_{\cal H}E_\mu$ is the hypothesis in ${\cal  H}$ with the lowest possible error on $\mu$ \citep{shalev:etal:2010,kawaguchi:etal:2017generalization}.
An algorithm $A$ on a hypothesis space ${\cal H}$ is uniformly consistent if and only if\\
$\forall \epsilon > 0  \  lim_{n \rightarrow \infty}
\mathit{sup}_\mu $\\
\hspace*{.3in} $\mu_n (\{z_n: \mathbb{E}_\mu(\{A_{z_n} - \mathit{inf}_{\cal H} \mathbb{E}_\mu > \epsilon\}) = 0$\\
%That is, the learning algorithm on $n$ trials converges as $n \rightarrow 
%\infty$ to $0$ expected error relative to the best possible hypothesis in ${\cal H}$.  
In our task, the best hypothesis $\mathit{inf}_{\cal H}$ is a prediction $\fh$ of some target function $f$. The best hypothesis is when $\fh = f$ with $f$, which yields $0$ expected error.  There are several algorithmic approaches, Fourier expansions, wavelets, orthogonal polynomials, that converge to target polynomial functions explored in \cite{devore:1998}. We say that a function class $C$ is {\em uniformly learnable} iff there exists a uniformly consistent algorithm for finding any $f \in C$.  % Moreover linear regression is an algorithm that converges to the target function on any data set in our set up.
%\begin{definition} 
% A class of hypotheses ${\cal H}$ is {\em uniformly learnable} just in case there exists a uniformly consistent algorithm for ${\cal H}$.
% \end{definition}   

We need to sample functions and their input values from a distribution, and we need to decide what the sampling distribution is.  This leads to two distinct notions of "learning a function $f$" that should be distinguished.  The first, call it {\em ICL}$_1$, involves an algorithm that can compute $f(x_i)$ when $x_i$ and the coefficients of $f$ are drawn from the training distribution or a related distribution such that $f(x_i) \in [a,b]$, where $[a,b]$ the vast majority of values seen in training.  A model {\em ICL$_2$} $f$ if it has learned an algorithm that can compute $f(x_i)$ when $x_i$ and coefficients of $f$ are drawn from a distribution on which  the conditions of {\em ICL}$_1$ are not met.  $ICL_2$ learns effectively the task and generalizes it. $ICL_1$ does not necessarily achieve this level of learning, but the model performs well within the training distribution.  Classes like ${\cal P}^n$  are purely general and ICL$_2$ seems an appropriate standard for them.  According to the ICL$_2$, we would expect that if the model $M$ has ICL ${\cal P}^n$, then it has learned the class form for ${\cal P}^n$.  Thus, for arbitrary $f \in {\cal P}^n$, $M$ has an algorithm such that $\fh_M(x) = f(x)$ for any point $x$ on which $f$ is defined. %\cite{garg:etal:2022} take a definition of learning where average expected error goes to 0 when data in train and test are sampled both from the same normal distribution.  %So for instance this means that the model will see linear functions with $a,b \in [-1,1]$ around 70\% of the time.  
  As we will see all our models ICL$_1$, but not $ICL_2$.

%The idea in statistical learning is to bring test and train error to the same point but over a similarly structured data set.  This makes sense for finding statistical correlations but less for determining whether a model can learn a class of mathematical functions like ${\cal P}^1$.  Ideally, the model learns the class described by a function $f = ax +b$ in the 1 dimensional case, if it has an algorithm for finding $f(a,b)$ for any $a, b$ given the points in the prompt.  Without such an algorithm, we can expect a model to be better on some data sets than others and to fail to generalize to seldom seen elements in its distribution.  This is in effect what happens.

\section{Related Work}

Since \cite{brown:etal:2020} introduced ICL and \cite{garg:etal:2022} investigated ICL for ${\cal P}^1$, there has been considerable research indicating that ICL is possible because of a sort of gradient ``ascent'', higher order optimization techniques or Bayesian principles \cite{akyurek:etal:2022,vonoswald:etal:2023,fu2023transformers,xie2021explanation, wu2023many, zhang2023and, panwar2023context}. % \cite{garg:etal:2022} showed that a small Transformer trained from scratch performed in-context learning of n-dimensional linear functions given identical train and test distributions $N(0,1)$.  
\cite{dong2022survey} surveys successes and challenges in ICL, noting that research has only analyzed ICL on simple problems like linear or simple Boolean functions \citep{bhattamishra2023understanding}. \cite{wilcoxson:etal:2024} extends \cite{garg:etal:2022}'s approach to ICL Chebychev polynomials up to degree 11 with training and testing on $N(0,1)$.   %trained small GPT-2 models from scratch to show that Transformers can ICL simple boolean functions, while their performance deteriorates on more complex tasks. 
\cite{raventos2024pretraining} investigated how ICL in models evolves as the number of pretraining examples increases within a train=test distribution regime. \cite{olsson:etal:2022} propose that {\em induction heads}, a learned copying and comparison mechanism, underlie ICL.  \cite{daubechies:etal:2022} shows that neural networks in principle can have greater approximation power than most nonlinear approximation methods.
\cite{geva:etal:2021} has investigated memory in transformers.  \cite{bietti:etal:2024} defines memory in terms of weighted sum and report that transformers memorize a large amount of data from their training through attention matrices. \cite{yu:etal:2023,geva:etal:2023} argue that LLMs favor memorized data, as the attention mechanism prioritizes memory retention.% and dominates the feed forward layer.% performs generalization. When these two components are combined, the overall performance is often dominated by attentional memory. 

  %do ICL linear functions, but in their experiments the test distribution was the same as the train and  mainly for values $a,b \in N(0,1)$.  %This means that the majority of the values of $a$ and $b$ far from $[-1,1]$ were not examined at all.  We based our code on theirs.% and perform in-context learning of linear functions with models (l,h,64) with $l \in \{1-6\}$, and AH $h \in \{1,2,4\}$, but we trained also bigger models for our experiments such GPT2 (12,8,256).

%åPrior research has investigated how transformers might ICL for linear functions. \cite{vonoswald:etal:2023, ahn2023transformers, mahankali2023one} provided a construction to show transformers ICL from their doing gradient descent during ICL.  \cite{} showed that Transformers could ICL in virtue of using higher-order optimization techniques. \cite{} argued that ICL follows from Bayesian principles. \cite{bai2024transformers} show that transformers can under certain assumptions implement many algorithms with near-optimal predictive power on various in-context data distributions.  \cite{wu2023many} pretrained a linearly parameterized single-layer linear attention model for linear regression with a Gaussian prior proving that the pretrained model closely matches Bayes's optimal algorithm. %Given \cite{perez:etal:2021}'s result that full transformers with linear attention are Turing complete, however, these theoretical demonstrations are perhaps not surprising.  

%in order to establish what transformers {\em actually} do as opposed, not what they {\em could} do--which is the objective of much of the research just cited.  

\cite{xie2021explanation,zhang:etal:2024,giannou:etal:2024,naim:asher:2024a} show that when train and inference distributions do not coincide, ICL performance on linear functions degrades.  \cite{naim:asher:2024a} shows that transformers models approximate linear functions as well as the best algorithms when train and inference distributions coincide; but when they do not coincide, they show models behave peculiarly around what they call {\em boundary values} (see also \cite{giannou:etal:2024}).  Within boundary values models perform well; outside the boundary values but near them models predict linear functions to be constant functions and then further out the predictions become random.       %They also investigated out of distribution behavior but only on $D_I \neq D^t_I$ (covariate shifts in \cite{zhang:etal:2024}) (not shifts from $D_F$).  They found that after 4 layers transformer model performance did not perform.  We found that larger models did improve performance, but when we set $D_I \neq D^t_I$, we got bad results when the function’s values on those points were outside what we call boundary values, something which held for all models.  
%\cite{zhang:etal:2024}'s covariate shift is also different from our experiments. They shift the prompt distribution but not that of the query.  When we take a distribution over input points in train $D_I$ and set $D^t_I \neq D_I$, our shift is not the same; we shift both prompt and query distributions. With covariate shifts we found that the choice of points is important and model performance degrades considerably when the values of the functions on the chosen points lie beyond what we call boundary values, which \cite{zhang:etal:2024} do not.  As far as we know we are the first to take boundary values and their dependence on model parameters as key indications of what is actually going on in ICL.
 Our work here builds on \cite{naim:asher:2024b} but extends it to ICL of continuous functions. We show that boundary values exist for all prediction of functions we tested, as well that attention layers in transformers are necessary for ICL of polynomials. Work on neural nets as approximators of continuous functions has concerned MLP only architectures and assumes that the function $f$ is known \cite{hornik:etal:1989}. With ICL, we don't have full access to $f$ but only to limited data about its graph.     %We note in Section 4.3 boundary values for model predictions on all polynomials.    
 %We examine how ICL actually works in practice under different training and testing distributions for transformer based LLMs %However, \cite{giannou:etal:2024,zhang:etal:2024} make important modifications to transformer architectures \cite{giannou:etal:2024,zhang:etal:2024} work with linear attention, whereas we look at attention layers as they actually are used with softmax. In addition, \cite{zhang:etal:2024} uses a new kind of optimization or training with gradients and a special fixed initial point. This means that their architecture and training are quite different from what normally happens with transformers; they are interested in getting a revised transformer-like model to learn linear functions, while we want to find out whether transformers as they actually are learn linear functions or something else.  As we detail below in Sections 4.1 and \ref{sec:4.4}, the results for the architectures of \cite{zhang:etal:2024,giannou:etal:2024} are differ from ours for actual transformers. In addition unlike these papers or \cite{li:etal:2023,wilcoxson:etal:2024}, we show that prompts that are too long induce chaotic behavior.
%Our objective is examine how  in ICL 1 dimensional linear functions, whereas most prior research has concentrated on transformer models {\em can} or {\em could} do on this task.  Even for this simplest case, we show transformers ICL in a different way from any of these proposed methods.


 


%\cite{garg:etal:2022} worked out the details on the learning set up for ICL of ${\cal P}^1$
% \\
%\includegraphics[width=15cm]{training_figure.png} \\
%   They used squared error as the loss function and sampled a batch of random prompts at each training step and update the model through a gradient update. They used a batch size of 64 and train for 500k steps. The training was done from scratch on the model. They trained three sizes of transformers (layers l, attention ah, and embedding  size e): (3, 2, 64); (6,4, 128); and (12,8,256).   {\color{blue} je pense qu'on doit parler moins en detail}% with a restricted embedding size (256 instead of 768).

 %Although 1 attention layer only models do not have this feature (and so by implication do not really ICL according to them), when induction heads are induced in such models, their ICL performance improves.  We have noticed that ICL for ${\cal P}^1$ occurs even with 1 AH and 1 MLP player.


   
\section{Models can ICL$_1$ continuous functions over some bounded intervals} 
 In this section, we show experimentally %Since any $f$ over interval $[a,b]$ for any $a,b \in \mathbb{R}$ can be normalised to $f$ on $[-1,1]$, will restrict training to sampling over $[-1,1]$.  
that transformer models trained on sampling over $[-1,1]$ ICL$_1$ polynomials of arbitrary degree and hence continuous functions.  %experiments on polynomial function classes and continuous functions . %(v) ordering and restricting the order of prompts can improve performance.{\color{blue} est-ce qu'on rajoute pas method d'entrainement est importante aussi?}  %In the last subsection, we put all of these observations together.

We trained from scratch several decoder-only transformer models, with 12 layers, 8 attention heads, and an embedding size of 256, to evaluate their ICL capabilities on different classes of polynomial functions. We assessed their performance both with and without feed forward layers, with and without normalization, and across different data distributions.\footnote{For our code see https://anonymous.4open.science/r/icl-polynomials/}  % We set the number of layers (L) from 1 to 6, and attention heads (AH) from 1 to 4. We also trained a 9L6AH model and the 12L8AH GPT2 with an embedding size of 256 and 512.trained on sampling over $[-1,1]$
 Given a prompt of type $(x_1,f(x_1),...,x_n)$, the model predicts $f(x_n)$ with least squares as a loss function.\footnote{Instead of cross entropy since we are dealing with continuous functions.} We refer to that prediction as $\fh(x_i)$.  To train the model ${\cal P}^n$  to ICL, we optimized $\theta^{*}$ using the auto-regressive objective,\\ 
\hspace*{0.2in}$ \theta^{*} = \argmin _\theta \mathbb{E}_{x_i \in D_I , f \in D_F}$

\hspace*{0.1in} $\left[ \sum_{i=0}^k l
\left(f\left(x_{i+1}\right), P_{\theta}\left((x_1,f(x_1),...,f(x_i), x_{i+1})\right)\right)\right] $
where $P_{\theta}$ is a ``polynomial learner", $l : (y,\hat{y}) \rightarrow ||y-\hat{y}||^{2}$ is  squared error and $f$ is a polynomial functions $f : x \mapsto \Sigma_{n = 0}^n a_i x^i$, with weights $a_i$ $\forall i \in {1,...,n}$, chosen at random according to some training distribution for functions $D_F$. Samples $x_i$ are picked randomly according to a training distribution for points $D_I$. To simplify, we note that $f \in D_F$ and $x \in D_I$. We choose at random a function $f \in D_F$ and a sequence of points $x_i \in D_I$, random prompts, from a distribution $D_I$ at each training step.  We update the model through gradient update. We use a batch size of 64 and train for 500k steps, (see Appendix \ref{sec:appendixA} for more details) . The models saw over 1.3 billion training examples for each distribution we studied.  For $D_F$ and $D_I$ we used the normal distribution $N(0,1)$ and uniform distribution over $[-1,1]$, $U(-1,1)$. 
% also studied by \cite{garg:etal:2022} but also chose Gaussian test distributions that had different parameters.  We also looked at . 


To compare how model performance evolves, %with parameters like the number of layers of the model or number of attention heads
 we tested the models on a variety of test distributions for both functions $D^t_F$ and data points or prompts $D^t_I$.  But while in train we always take the same distribution ($D_F = D_I$), in test, we sometimes take $D^t_F \neq D^t_I$. To see how the model performs in ICL relative to $(D_I^t, D_F^t)$,  for each testing scenario, we generate a set of $N=100$ functions in $D_F^t$; and our data samples for test are composed of $N_b = 64$ batches, each containing $N_p = 41$ points in $D_I^t$. In each batch b, for all points, we predict for each $x_k^b$, $k \geq 2$, $f(x_k^b)$ given the prompt $(x_1^b, f(x_1^b),..., x_{k-1}^b , f(x_{k-1}^b), x_k^b)$. We calculate for each function  the squared error and also the mean average over all the points $N_p$ of all batches $N_b$, then do a mean average over all functions.  Formally this is: $$\epsilon_\sigma = \frac{1}{N} \Sigma_{i=1}^{N} \Sigma_{b=1}^{N_b} \frac{1}{N_b}
(\frac{1}{N_p}\Sigma_{i=3}^{N_p} (pred_i^b - y_i^b)^2)$$

  We define \textit{error rate} $r_\epsilon = \frac{\epsilon_\sigma}{|\epsilon_* - \epsilon_0|}$ where $\epsilon_*$ is the best $\epsilon_\sigma$ error for a model M with $\fh(x)$ calculated with Least Squares, and $\epsilon_0$ is the worst $\epsilon_\sigma$ error for a model $M$ such that $\fh_M(x) = 0$, $\forall x$.  In all our error calculations, we exclude the first $n+1$ predictions of each batch from the squared error calculation for $f \in {\cal P}^n$, since we need at least n+1 points to be able to find $f$ and the first n+1 predictions by the model are hence almost always wrong. The heatmap in Figure \ref{hmap} in the Appendix \ref{sec:appendixC} shows evolution of squared error on two different models. %A drawback of this method is that if a batch gives abnormal predictions, its error will be diluted by all the other batches and we won't notice it. %So we also calculated an average error $\delta$ over batches.  
 
 To ensure that comparisons between models are meaningful, for each $U(-\sigma,\sigma)$, we set a seed when generating the 100 random linear functions, ensuring that each model sees the same randomly chosen functions and the same set of prompting points $x_i$. The interest of this test is to see how models perform on tests where they progressively see more out of distribution data (Appendix \ref{sec:appendixF}).

%\subsection{Transformers can ICL$_1$ continuous functions over bounded intervals}

Training models from scratch on different classes ${\cal P}_n$, we found: 
\begin{obs}\label{obs:poly} All models ICL$_1$ functions from ${\cal P}^n$ for $n \leq 6$ when  $D^t_I = D_I = U(-1,1)$ and $D_F = D^t_F = U(-1,1)$ with a performance in line with optimal algorithms.%, like  Least Squares (LS), where for each degree $n$, the LS model function is a polynomial function of degree $n$.
\end{obs} 
Figure \ref{deg} shows the generalization ability of our best model on a sampling from $U(-1,1)$ of classes of polynomial functions. We tested up models $Mn$ for $1 \leq n \leq 6$.   Mn is trained on polynomials in ${\cal P}^n$ with coefficients and input values samples from $U(-1,1)$.  The models are tested at inference time on functions in ${\cal P}^n$ with coefficients sampled from $U(-\sigma, \sigma)$ for $1 \leq \sigma \leq 10$. 
\begin{figure}[!h]
\includegraphics[width=8cm]{figures/modelssss.png}
\caption{Evolution of error rates for various 12L8AH $d_{emb} = 256$ models  with $D_F, D_I = D^t_I = U(-1,1)$ and $D^t_F$ for various $U(-\sigma, \sigma)$ trained from scratch, each on a different degree. E.g., Mn is a model trained on ${\cal P}^n$. The purple curves illustrate a model that predicts $f(x_n) = 0, \forall f$ and $\forall x_n$. The dark red line LS represents a perfect estimator given our totally clean input data.
\label{deg}}
\end{figure}
Our experiments showed that all models had the same error rates, and that changing the distribution from Uniform to Gaussian did not change results (see Appendix \ref{sec:appendixB}).  Table \ref{table:3} shows that some of our models trained on specific polynomial classes ${\cal P}^n$ do better as $n$ increases; e.g., models trained only on ${\cal P}^1$ do worse on generalizing to functions outside the training distribution than models trained only on ${\cal P}^2$, ${\cal P}^3$, ${\cal P}^4$ or ${\cal P}^5$.  This runs counter to what one would expect, were the model using least squares or some approximation of it.%    {\color {magenta} and they can ICL polynomials of degree less than $n$ without specifying their degree which algorithms like least squares cannot  demontrer empiriquement avec least squares} {\color{blue} LS arrive à approcher differents degrés, à condition que la fonction d'optimisation $f_\theta$ soit de degré superieur à ce qu'on s'attend. Par exemple: f = ax3+bx2+cx+d et donc on lui demande de rechercher a,b,c,d si on lui fournit 5 exemples de fonction lineaire, il arrive à optimiser et donc a donner a=b= 0 }

Let $f_{/[a,b]}$ denote $\{f(x): x \in [a,b]\}$.  Given that we noticed no drop in error rate for more complex polynomials in our models' predictions, we generalize Observation \ref{obs:poly} in two important ways.

\begin{obs} \label{obs:general}
Transformer models ICL$_1$  $f_{/[a,b]}$ for relatively small $[a,b]$ and $f \in {\cal P}^n$ for any $n$  with $D^t_I = D_I$, and $D_F = D^t_F$ with coefficients of $f, x$ sampled from a uniform, normal or even bimodal distribution over $[a,b]$.
\end{obs}

%Our experiments across different training conditions demonstrate that the quality of a model's predictions is highly influenced by the density of training data points, specifically the proportion of closely related examples encountered during training.  If $D^t_F \subset D_F$, $D^t_i \subset D_i$, generalization ability is substantially increased.
%, thereby disproving this hypothesis. % Another important criterion that influences the ability of a model to ICL and to generalize is the training method. We can clearly see in figure \ref{generalization} that a model trained in curriculum on degrees 1,3,5 without degrees 2 and 4 has better performances than a model trained on all degrees and generalizes better. which is also in turn better than a model trained only on a single degree.}


%But training with a sample just of functions in ${\cal P}^1$ with coefficients and input values from a large interval like  $D_F=D_I= [-100, 100]$ as in Table \ref{table:7} gives terrible results when tested on sampling over small subintervals,$D^t_I=U(-1,1)$ and $D^t_F = U(-\sigma,\sigma)$ for $\sigma \in \{1,...,10\}$. } {\color{blue} We aimed to test the hypothesis that model performance is influenced more by the proportion of points encountered within a specific domain, rather than the total number of points seen. To investigate this, we trained one model on ${\cal P}^1$ with a $U(-1,1)$ distribution over 100k steps, and another on a $U(-5,5)$ distribution over 500k steps, ensuring that both models encountered a statistically equivalent number of points within the interval $U(-1,1)$. Despite this, the results remained consistent, suggesting that the proportion of points within a specific interval is the primary factor influencing performance. }
 

 %The error rate is a function of how often the model has seen $x_i, g(x_i)$ near elements in the target sequence $x_1, f(x_1)... x_n ?$ and especially value $x_n$.  

\begin{table*}
\small{
\begin{tabular}{l l l l l l l l l l l}
 \hline
 models \ $\backslash$ \ $\sigma$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 
 \hline\hline
 $U(-1,1)$   &0.0& 0.03& 0.55& 1.37& 4.0& 5.17& 9.04& 12.07& 19.28& 27.85 \\

 $U(-5,5)$   & 0.01& 0.01& 0.02& 0.03& 0.03& 0.05& 0.12& 0.27& 0.75& 1.61 \\

 $U(-10,10)$   &0.13& 0.15& 0.17& 0.2& 0.26& 0.26& 0.32& 0.35& 0.41& 0.49 \\
 
  $U(-100,100)$   &  2217.84& 2373.82& 2494.31& 2526.93& 2472.45& 2467.52& 2317.92& 2232.03& 2129.0& 2092.81 \\ [1ex] 
 \hline
\end{tabular}
}

\caption{Comparison showing the evolution of squared errors for models trained on different distributions $D_I = D_F= U(-a,a)$, for $a = 1, 5$ or $100$ sampling from ${\cal P}^1$ with $D^t_i = U(-1,1)$ and  $D^t_F=U(-\sigma,\sigma)$.}
\label{table:7}
\end{table*}


Observation \ref{obs:general} is significant in virtue of the following:
\begin{theorem} \label{stone} (Stone-Weierstrass)
Suppose $f$ is a continuous function defined on $[a,b] \subset \mathbb{H}$, a compact Hausdorf space. $\forall \epsilon > 0$ there exists a polynomial function $f_p$ such that $\forall x \in [a,b], ||f(x) - f_p(x) || < \epsilon$
\end{theorem}
 \cite{brosowski:deutsch:1981} provide a generalization of Theorem \ref{stone} to any compact space.  But for our purposes a closed interval $[a,b] \subset \mathbb{R}$ or is sufficient.  The proof uses the approximation of $f$ with Bernstein polynomials, $b_n(f)$, which are defined as follows: for $n \in \mathbb{N}_{>0}$, 
$b_n(f) : [0, 1] \rightarrow \mathbb{R}$ with $x \mapsto \Sigma_{i = 0}^n f(k/n) \binom{n}{k}x^k(1-x)^{n-k}$.  To provide the rate of convergence with $b_n(f)$ and $\omega_f(\alpha) = sup_{|x -y| < \alpha}\{|f(x) - f(y)\}$, we have: 
$$|b_n(f)(x) - f(x)| =  $$
$$|\Sigma_{i = 0}^n f(k/n) -f(x) \binom{n}{k}x^k(1-x)^{n-k}| \leq \frac{3}{2}\omega_f(\frac{1}{\sqrt{n}}) $$
Note that as $\alpha \rightarrow 0, \omega_f(\alpha) \rightarrow 0$ Thus the bounds on convergence are quite strong.   

 

%to find for $\epsilon > 0$ an $f_p$ such that $|f(x) - f_p(x)| < \epsilon $, $\forall x \in [a,b]$.  By choosing an appropriate training regime, our models can ICL $f_p$ and thus $f$ over $[a,b]$ 
Using Observation \ref{obs:general} and Theorem 1,
\begin{cor} \label{obs:continuous}   Transformer models can in principle ICL$_1$ any continuous, not necessarily differentiable function (see appendix \ref{sec:appendixC}) $f$ over a small interval $[a,b]$ if we pick a suitable training = testing regime.
\end{cor}
% We note $f$ need not be differentiable (e.g. $f(x) = |x|$) as we show in appendix B. 
We showcase model behavior on three classical continuous functions, exponential, sine, and ln(1+x), with their Taylor series expansions below.  After training on ${\cal P}_3 ~ $ $D_F = D_I = N(0,1)$ our models ICL:\\
$\forall x \in [0,1]$:
$e^x = \Sigma_{n = 0}^\infty
\frac{x^n}{n!} = 1 + x + \frac{x^2}{2} + \frac{x^3}{6} + o(||x^3||)  $ \\ $sin(x) = \Sigma_{n = 0}^\infty
(-1)^n \frac{x^{2n+1}}{(2n+1)!} = 1 - \frac{x^3}{6} + o(||x^3||)  $ \\
$ln(1+x) = \Sigma_{n = 1}^\infty
(-1)^{n+1} \frac{x^n}{n} = x - \frac{x^2}{2} + \frac{x^3}{3} + o(||x^3||)  $ 
\begin{figure*}[!h]
%\includegraphics[width=11cm]{figures/evolution of errorsbis.png} 
\center 
%\includegraphics[width=8cm]{figures/errors22.png}
\includegraphics[width=5cm]{figures/exppng.png}
\includegraphics[width=5cm]{figures/sin.png}
\includegraphics[width=5cm]{figures/ln.png}
\caption{Plot of ICL of $e^x$, $sin(x)$ and $ln(1+x)$ by the model trained on ${\cal P}^3$.
\label{continuous}}
\end{figure*}
Once we have fixed the architecture at 12L8AH, increasing $d_{emb}$ from 64 to 512 does not affect performance. We remark that our models are able just from a few prompts like  $(x,exp(x))$ or $(x,ln(1+x))$ to differentiate and approximate these functions, even though it has only seen polynomial functions in training.

%When provided a suitable training regime for instance a sampling of polynomials of degree $3$ in $D^t_F = D_F = N(0,1) = D_I = D^t_I$, our models get a very good approximation of {\color{blue} all our sample functions.}.  %We know that sampling polynomials of degree $3$ is ideal here because we know that the exponential is approximated well by a polynomial of degree $3$ on that interval {\color{blue} (Figure \ref{continuous})}
%When trained on  and the target functions had values in [-1, 1], even small models were able to converge to a 0 average error.  The error was not always identical to $0$ at least in some batches but rather similar to \cite{liu:etal:2024}'s finding on MSE estimation by transformers. 

An interesting observation is that our models did better than an optimal algorithm like LS, because they were able to approximate polynomial functions without knowing their polynomial class.  With LS we need to know the equation's degree.


Our experiments showed several factors affecting the quality of a model's predictions. % different training conditions demonstrate that the quality of a model's predictions is highly influenced by several factors. 
First, when $D^t_F \subset D_F$ and $D^t_I \subset D_I$ the model's performances are substantially increased. Under these conditions, a consistent performance disparity is observed between different models (Table \ref{table:7} and Heatmap \ref{hmap}). One important criterion we identified by modifying the training distribution and progressively expanding its range is the proportion of points from the test distribution that the model was exposed to during training relative to the total number of points encountered throughout the training process (Table \ref{table:7}). See Appendix H for a mathematical calculation of the proportion for uniform distributions.  

One might think that the total number of points observed in the distribution, rather than the proportion, is the critical factor. We tested this alternative hypothesis by training one model on $U(-1,1)$ but for $100k$ steps in one case and another on in $U(-5,5)$ for $500k$ steps, ensuring that both models were exposed to a statistically equivalent number of points within the interval $[-1,1]$. The results remained similar, showing that the total number of points observed is not the critical factor.  This was further confirmed through a checkpoint analysis, indicating that optimal performance is typically reached at a fixed training steps. Beyond this point, performance fluctuates, suggesting that exposing the model to a larger number of data points does not necessarily improve performance and may even slightly degrade it. 




%For our task, which does not require a priori much contextualization, the addition of a layer of MLP layer seems to make more difference to the success of the prediction than the addition of an additional attention head (AH).  In some cases.  In general the larger models are doing significantly better.
\section{Transformers don't ICL$_2$ any $f \in {\cal P}^n$} \label{sec:4.4}

In this section, we show: (i) models do not ICL$_2$ any functions we tested; (ii) models have similar out of distribution behavior for all functions; (iii) models with attention layers (AL) are needed to achieve ICL and that attention only models can sometimes out perform full transformer architectures
 
 All our models had systematic and non 0 average error on target functions for all polynomial classes tested once we chose either: (i) test distributions over significantly larger intervals than those of training distributions, or (ii) very large training distributions.   Figure \ref{deg} shows that the error rate increases substantially; Table \ref{table:3} in the Appendix provides the average errors with respect to least squares and shows that the error rate increases non-linearly as $D_F^t = U(-\sigma,\sigma)$ and $\sigma$ increase. Figure \ref{hmap} in Appendix \ref{sec:appendixC} gives a heatmap rendition of the error across $x_i$ and function coefficients.  Figure \ref{deg1} in Appendix \ref{sec:appendixB} shows similar results for training and testing with $N(0, \sigma)$. Performance did not significantly improve when we moved from models with 9.5 M to 38M parameters.\footnote{by taking $d_{emb} = 512$ instead of $d_{emb}=256$.}
%Figure \ref{progressive-loss} plots the evolution of error loss for various models.
 % {\color {magenta} peut etre a refaire un tableau pour d'autres modeles et les degres superieurs}%The full error scores are in the Appendix.
\hidden{
\begin{figure}[!h]
%\includegraphics[width=11cm]{figures/evolution of errorsbis.png} 
\center 
%\includegraphics[width=8cm]{figures/errors22.png}
\includegraphics[width=8cm]{figures/errorrate4.png}
%\includegraphics[width=8cm]{figures/degrees2.png}
\caption{In the top graph, evolution of error rates for various models  with $D_F, D_I = D^t_I = N(0,1)$ and $D^t_F$ for various $N(0, \sigma)$. Mn is a model trained on ${\cal P}^n$.  The purple curves illustrate a model that predicts $f(x_n) = 0, \forall f$ and $\forall x_n$. The cyan line LS represents linear or ridge regression, which is trivially a perfect estimator given our totally clean input data. 
\label{progressive-loss}}
\end{figure}
}


%Given that each model tested on ${\cal P}^n$  was trained on ${\cal P}^n$, 
 %We illustrate with the case of ${\cal P}^1$.  Given uniform distributions and training on $U(-1,1)$, all the values $f(x)$ the model M1 will have seen in training are in $[-2,2]$.  So when testing on $U(-2,2)$ the model will on average be required to predict $f(x)$ where $f$ and or $x$ are outside the interval defined by its training $50\%$ of the time, and $f(x)$ lies outside $[-2,2]$ $50\%$ of the time.  However, distances here are still small between the out of distribution test instances and training $(< 2)$. As $\sigma$ increases, the sampling provides a sparser set of training samples that correlates with increased error rate.  This is confirmed by taking a model M135 trained on $U(-5,5)$ and higher order polynomials but tested on $U(-1,1)$  While we've seen that training with a larger sampling class than what we test on substantially improves model generalization ability, there are limits; when a large training class yields too sparse a sampling, models have terrible predictions.%Once $\sigma$ increases significantly, model performance in any case is significantly below predictions of the least squares algorithm given the general form of the target function or what prior research has suggested for ${\cal P}^1$ \citep{akyurek:etal:2022,vonoswald:etal:2023,giannou:etal:2024}. 


We found that what \cite{naim:asher:2024b} called {\em boundary values} account for this significant increase in error.  
\begin{obs} \label{obs:boundary} (i) For all models $M$ there exist boundary values $B^-, B^+$ such that for $f(v) \in [B^-,B^+]$, the predictions are close to optimal; (ii)  When $f(v) \in [B^+,B^+ +\alpha] $,  $\fh_M(v) \approx B^-$, where $\alpha$ is a constant determined by $M$. Similarly, when $f(v) \in [B^- - \alpha,B^-] $ $\fh_M(v) \approx B^-$.  (ii) When $f(x) > B^+ +\alpha$ or $<B^- - \alpha$, $\fh(v)$ takes random values in $[B^-, B^+]$. 
\end{obs}


Boundary values occur with all models (For illustrations see Appendix E).  To investigate these values, which prevent the model from generalizing, we conducted an ablation study.  We first removed the MLP components from our models architecture, but the models continued to exhibit boundary values. We then removed layer normalization; normalization might prevent our models from exceeding certain thresholds, since the output of multi-attention heads and residual learning at layer $i+1$, $Attention(X_i)+X_i$, can theoretically take on large values %due to residual learning, taking 
for example $X = 1000X_i$. However, even without layer normalization, the phenomenon persisted. Next, we removed residual learning, resulting in a 12-layer model with 8 attention heads only, excluding both "add and norm" components and MLP. Surprisingly, the boundary values remained, indicating that they originate from the multi-head attention mechanism.  Experiments with linear attention only models also exhibit boundary values, so softmax alone does not account for them.  In fact for large values softmax functions like hardmax (see Appendix F). % but perhaps any non negative similarity function does.  

We then tested the learned embedding $\epsilon: \mathbb{R} \rightarrow X^{256}$ to see whether it mapped large values like $Z$ into $[B^-,B^+]$.  We mapped with t-SNE $\epsilon[-999,999]$ for our best model and saw that our training induces an almost perfect linear order on values and  sufficiently respects mathematical operations so that $Attention(X_i)+X_i$ should provide large values (see Appendix F). The derivation on attention in Appendix \ref{sec:appendixF} makes the details explicit, and shows that a 1L8AH model, in theory, can not exhibit boundary values, {\em provided linear attention provides a linear operation on} $\mathbb{R}$.  However, the model experimentally shows boundary values.  

This provides a proof outline that: 
\begin{prop} \label{nonlinear}
$Attention(X_i)+X_i$ is a non-linear map on $\mathbb{R}$ and on intervals $[a,b]$ that are significantly larger than $[B^-, B^+]$; but can well be linear on the finite field ${\cal F}_{[B^-, B^+]} = ([B^-, B^+], +_{\cal F}, \times_{\cal F}, 0_{\cal F}, 1_{\cal F})$.
\end{prop}
Boundary values are intrinsic to the attention mechanism and follow from a model's pretraining.  Attention provide the model a 'shell' to guard against unrecognized large values. Such a phenomenon becomes problematic in scenarios where the training domain or the underlying mechanism itself is not well understood.

\begin{obs} \label{obs:nongeneralize}  Our models cannot $ICL_2$ functions in ${\cal P}^n$, for any $n$.     %or to use linear regression to predict $f \in {\cal P}^1$.
\end{obs}
%Our models failed to ICL seldom seen $x_i, f(x_i)$ with $f \in {\cal P}^n$ for any $n$ and so did not grasp the general, class form of any polynomial function.    %or more general concepts like monotone increasing or decreasing linear functions
 Were our models able to $ICL$ class forms of polynomials, we would expect much better out of distribution performance; once the parameters for the polynomial form of $f$ are set, f(x) is calculable for all $x$.  But we did not observe this; on the contrary we found model performance excellent on arbitrary polynomials over restricted intervals and then bad predictions of the sort described in Observation \ref{obs:boundary}.  Also we saw predictions vary over batches in test for the same function, indicating sensitivity to data input, which should not affect parameter estimation for function classes at least for $f \in {\cal P}^1$; if $M$ had implemented linear regression, it should get the same result on each batch for $f$.

Observation \ref{obs:nongeneralize}  and Proposition \ref{nonlinear} conform to \cite{asher:etal:2023}'s characterization of learnability. An ICL$_2$ grasp of any such function class would involve an ability to predict with the same accuracy function values $f(x_i)$ for $x_i$ in arbitrary intervals of $\mathbb{R}$ and for arbitrarily many such points. The same holds for basic arithmetic operations and linear projections on $\mathbb{R}$.  %Translating this into capacity into our prompt sequences, this means an ability to predict arbitrarily long sequences of the sort in our prompts.  
%By compactness models could then in principle predict countably long such sequences of values $f(x)$ for arbitrary inputs $x$.  %If we look at the space of all such sequences $V^\omega$ and consider the Cantor topology on $V^\omega$, then the set ${\cal F} \subset V^\omega$ describing  a function $f$ in any abstract function class os a $\Pi^0_1$ set in the Borel hierarchy. 
\cite{asher:etal:2023} show that LLMs cannot learn any such sets using ordinary LLM assumptions, and so basic linear operations are linear on finite fields like ${\cal F}_{[B^-, B^+]}$ without being so on $\mathbb{R}$. %{\color{blue} ici peut etre on explique c'est quoi $V^w$ et $\pi$}

In sum, (for discussion and figures see Appendix E, F, H),%of dimension 41 and 
\begin{obs}\label{obs:observations}
(i) ICL$_1$ predictions depend on: (i) the presence of attention layers; (ii) %the length of the prompt in inference--- prompts longer than training will degrade performance; (iii) 
the precise values in the prompt (as well as length); (iii) the proportion of the sequences in training in test. %Further, the result in Observation \ref{obs:general} cannot be improved upon.
\end{obs}

An important consequence of Observations \ref{obs:nongeneralize} and \ref{obs:observations}  is that we should not claim that models can ICL any function class unless we specify the interval $[a,b]$ from which their training distributions are sampled.  To use ICL with transformer models effectively, we need to know the training regimes of those models.  The positive consequence these observations is that judicious choices of pretraining can optimize ICL performance in particular tasks.  
%Neither did the models discover LS because  for all classes of functions. 





\begin{figure*}[!h] 
\includegraphics[width=15cm]{figures/first.png}\\
\includegraphics[width=15cm]{figures/second.png} 
%\includegraphics[width=5.3cm]{figures/fx=10xlow.png}
\caption{First line of graphs gives error rates for M135, a full 12L8AH transformer model trained on $f \in {\cal P}^{1,2,3}$ (${\cal P}^1 \cup {\cal P}^3 \cup {\cal P}^5$ with values and inputs sampled from $U(-1,1)$, Mn the same model trained only on $f \in {\cal P}^n$ and M135AL, a 12L8AH model with only attention layers and no MLP layers.  All models were tested on polynomials of degrees 1-5. Second line gives similar results for models trained on $ {\cal P}^{1,2,3}$.} \label{generalization}
\end{figure*}

\section{Surprising ICL capacities: Generalizing to unseen polynomial classes} \label{sec:gen}
 We have examined in Sections 4 and 5 models trained and tested on one class of polynomial, with the results in Figure \ref{deg1}.  How might a sort of curriculum learning, where models learn several models but not others affect performance?   Figure \ref{generalization} depicts two situations: one in which the models learn to approximate $f \in {\cal P}^m$ for $1\leq m \leq 6$ while having been trained on $f \in {\cal P}^{1,2,3} = {\cal P}^1 \cup {\cal P}^2 \cup {\cal P}^3$); the other in which the models train on $f \in {\cal P}^{1,3,5}$.  The second case is interesting because it forces the model to predict values for functions of classes on classes it has not seen that lie between classes it has trained on.  Figure \ref{generalization} compares how those models do with respect to models $Mm$ trained and evaluated only on ${\cal P}^m$.

 While all training regimes produced close to perfect results when test values for coefficients of $f$ and $x \in [-2,2]$, once test values were outside $[-2,2]$, the M1,2 models had much higher error rates than the second ``gappy" model M135 on $f \in {\cal P}^{1,2}$. And M135 didn't even see any quadratic functions in its training. M135 had better generalization performance and error rates better or equal to the best Mm models for any ${\cal P}^m$ class we tested.  M135 had better results on polynomials of degrees 2 and 4 than it did on $f \in {\cal P}^5$. It also had better generalization performance than the cumulative curriculum model M123.  M123 also did better than M1 models on ${\cal P}^{1,2}$ but not on higher order polynomial classes.  Interestingly, M135AL with attention only layers had optimal or close to optimal performance in many cases, while M123AL had significantly worse generalization performance than other models.  

A possible explanation of the superior performance of M123 and M135 over M1 and M2 is this.  While higher order polynomial functions define more complex sequences than lower order ones, training on ${\cal P}^4$ with coefficients and inputs $x_i$ in $U(-1,1)$ will yield a significantly larger spread of values ( in $[-5,5]$) when training on $U(-1,1)$ than training just on ${\cal P}^1$.  Given our observations that proximity of training is important to accurate approximation, training on higher polynomial classes can aid in generalization.  This also accounts for why M123 performs less well than M4 or M5 when we test M123 on ${\cal P}^{4,5}$ and why M135's superior performance vanishes for higher order polynomials, since training ${\cal P}^{5}$ with sampling from $U(-1,1)$ only negligibly increased the chances of having nearby values from training during inference on, say, ${\cal P}^3$ with coefficients sampled from $U(-6,6)$--the maximum values for ${\cal P}^3$ on $U(-6,6)$ are included in the interval $[-1514, 1514]$, while the maximum training value interval is $[-5,5]$. 

  In contrast to \cite{yu:etal:2023}, our experiments reveal that the M135AL model demonstrates strong performance and generalizes effectively, even outperforming M135, whereas the M123AL model does not exhibit similar capabilities. These findings suggest that the chosen training methodology significantly influences a model's propensity to favor reasoning over memorization {\em ceteris paribus}. 
\begin{obs} \label{obs:error-rate} %Error rates depend on the distance of the
%target function’s values from the majority of the data
%points in the model’s training. 
A model's training method can strongly influence performance.
\end{obs} 
 %We investigate out-of-distribution through degrees of polynomials: we will train models by curriculum learning on degrees, then try how models will deal with higher unseen degrees and we will look also on models  trained by curriculum learning on degrees but we will except this time some degrees to see how the models will deal with unseen lower degrees. in the examples below we will present two different models: A curriculum-trained model $M_1$ on $P^1$ and $P^3$ but without $P^2$ then a second model $M_2$ A curriculum-trained model on $P^1$, $P^3$ and $P^5$ but without $P^2$ and $P^4$ to see how each model evolves on the distributions it has or hasn't seen with this type of training. (** On peut faire des noms comme Shift peut-etre?)\\
%Surprisingly, when tested on polynomials of each degree to calculate the error rates, the $M_1$ model showed almost similar performance to the model trained only on the specific degree on the training domain, and even better performances in out-of-distribution of data and coefficients , and this is true even for polynomials of degree 2 to which the model has not been trained. 

%Model $M_2$ still running..



\section{Surprising ICL capacities: finding zeros of unknown functions} \label{sec:zeros}

%From a prompt  we have shown that with suitable training our models $M$ predict $\fh(x_n) = f(x) + \epsilon$ when $x, f(x) \in [a,b]$ but it does not predict the general form of the function and hence it cannot generalize its estimations of $f(x)$ to seldom seen $x_i$ nor generalize to seldom seen $f$ outside of its training regime.  However, this failing has a hidden virtue.  \
Since our models treat prompts $(x_1,f(x_1),..,x_n, ?)$ simply as sequences, we can use the same training as in Section 4 to solve a difficult inverse problem:  given a prompt of the form  $(f(x_1),x_1,..,f(x_{n-1}), x_{n-1}, f(x_n)=0, ?) = (y_1, f^{-1}(y_1),..., 0,?)$.  With this prompt, the $?$ asks M to predict $\hat{x_n} = x_n + \epsilon$, where $x_n$ is the true zero of the function $f$, with $\epsilon$. an error term.  For example let $f(x) = x +3$  and so $f^{-1}(x)= x -3$.  To find the zero of $f$, we give $f^{-1}(0) = -3$ as then element of our sequence answering to ?.  

There are polynomials do not have analytic solutions in terms of radicals, and Galois \cite{stewart:2022} gave a necessary and sufficient group theoretic condition for solvability using radicals. \footnote{This result is due to Ruffini and Abel.  A polynomial has a solution in terms of radicals, just in case its roots can be found by $\times, \div, +, -$ and n-th roots.}  

Our models were not trained to provide multiple approximations of multiple roots---i.e., multiple values for ? in $(f^{-1}(x_1),x_1,..,f^{-1}(x_n), x_n, f^{-1}(0), ?)$. So we restricted our task to continuous functions that are monotone in addition, as this ensures $f$ is bijective.%(continuous + monotony => bijectivity). 
Thus, $f^{-1}$ is continuous, and Corollary \ref{obs:continuous} applies.%\footnote{This is not really a restriction, if we can provide intervals for a non bijective function on which it has just one zero.} 

As an illustrative example of our method, consider the polynomial $f(x) = x^5 -x +1$.  $f(x) = 0$, when $x \approx -1.18$. Its plot is in Appendix \ref{sec:appendixC}.  Our best model predicts $\hat{x} = -1.4370$; $f$'s only real root is approximately -1.18.  In comparison, an attempt to use Newton's method to find the $zero$ fails with starting point $x = 0$. 

One might think that limiting to the case of bijective functions is restrictive, but it's not much of a restriction.  %but this is already a gain, because all we need to do is provide the models with a few function values %without necessity of derivative values such as Newton's Algorithm. {\color{blue} here we need just few values in some points, but usual algorithms we need either expression of f and/or f' otherwise atleast value of f in certain points such as dichotomy} \\
We can reduce the space where we're looking for the zero so that the function is monotonic there. We tried this on higher-degree polynomials that have several 0's, and the model did pretty well when we set the search domain where the function is monotone. 


We evaluated models using a diverse dataset of over 200 continuous functions. For each function, we provided 64 distinct prompt batches, each of size 40. The model generated a candidate solution for the zero of the function based on the prompts. The prediction was considered correct if at least 70\% of the batches satisfied the constraint that the batch prediction for the zero, $x_{M}$, is such that:
$||x^* - \x_{M}|| < \frac{2||x^*||}{10},$
where $x^{*}$ is the true zero of $f$, $f(x^{*})=0$. If $x^{*} = 0$, we use $0.1$ as upper bound.


The results are in Table \ref{table:zeroes}. Except in 2 or 3 cases, our model's predicted values could potentially have been accepted if we had loosened our metric. However, for GPT-4, the false values are always far from the correct value.  Especially noteworthy is the performance of the M135 AL Attention Layer only model, which had the best overall score.

\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|}  % Row label + 3 columns
\hline
\textbf{Models}& selected functions & \textbf{Overall score }  \\ \hline
\textbf{M3}  &  87\% & 52\% (104/200)                
\\ \hline
\textbf{M135} &  100\%  & 69.5\% (139/200)                
\\ \hline
\textbf{M135AL} &  98\%  & 73.5\% (147/200)                
\\ \hline
\textbf{GPT4}   & 7\%  & 9  \% (18/200)                    
\\ \hline

\end{tabular}
\caption{Comparison between our models M3, M135 and M135AL trained from scratch on a sampling of $f \in {\cal P}^3$ and gpt4 for finding the zero of an unknown continuous function} \label{table:zeroes}
\end{table}  

 These scores are indicative, as they depend on the sample of functions chosen.  The class of selected functions included exponential, log, cos, sin and tan, for which the inverses are particularly well behaved (see Table \ref{table:zeroes1} for more details). The models $M135$, $M135AL$ and $M3$ had very high scores.  On polynomial functions with inverses that are more complex (and that may induce the problem of several 0), our models had more difficulty, but they were still way better than GPT4 apart from M135AL on ${\cal P}^1$ (Appendix G has more details).  %, so perhaps the criterion is too rigorous for this type of function, not to mention the bijectivity problem that can arise for zeros of higher-degree polynomials. 
 Nevertheless, our small models show great potential in solving this problem, which suggests smaller LLMs can sometimes replace  algorithms like Newton's Algorithm, which doesn't necessarily converge and requires both knowing the expression of the function as well as the derivative of the function.  Our models do not even require the function to be derivable.  Nor do we need particular points with methods like the Dichotomy Principle.  If we want greater precision, once we get the the value $\x_M$ from the model M, we can take a prompt of type $(f(x_1), x_1, ..., f(x_n), x_n, 0,?)$ and then verify if $\fh(\x_M)=0$.  If it is not, we can still use $\x_M$ as a starting point for 0-finding algorithms, like Newton's algorithm, which depends on an initial point.
 
% like $+, -, \times$ or $\div$. 

\section{Discussion}

Given our models' surprising capacities, analysing ICL as a matter largely of associative memory as in \cite{bietti:etal:2024} needs revisiting.  The lack of generalizability from Observations \ref{obs:nongeneralize} might suggest our models overfit the data and rely solely on memory to ICL. However, the pretraining data has no noise, and it's too large to be memorized by our models; our largest models with 256 size embeddings have $ < 10^7$ parameters; each parameter would have to encode on average sequences for over 100 different functions of wildly different kinds. Further, some attention-only models in Figure \ref{generalization} with only 3+ million parameters have equivalent performance to our best models with 10M parameters.  

Moreover, our models performed similarly on several different training distributions for $D_F$ and $D_I$ and tested on $U(-\sigma,\sigma)$ for $\sigma \in \{1,2\}$. Finally, given that 100 samplings with $D^t_F = U(-2,2)$ nets on average 25 functions with coefficients the model trained on $D_F = D_I = U(-1,1)$ has not seen (see Appendix Section I), we would expect that if only memory were involved, the model's error rate on $U(-2,2)$ would be substantially higher than it is.   The generalization abilities in Section 6 of our models and their ability to calculate zeros in Section 7 also suggests that ICL involves more than associative memory.% simple copying and comparison mechanisms of induction heads even in small models.  This implies that the models didn't overfit to their training regimes and that ICL is more than a matter of memory. 

Interestingly, it is the training regime in which the model must predict sequences from function classes it has not seen in training that forces the model to generalize better, as we see from the strong performance of the M135 and M135AL models.  This also shows that AL only models can generalize given the right training.  %ßIn addition, they can approximate zeros of polynomials, which is a difficult task. This strongly suggests that their ICL ability is not solely a matter of associative memory.  
Of course, associative memory still plays an important role.  Clearly, boundary values are stored and from Observation \ref{obs:boundary} strongly affect prediction.  
  
% on a trouvé que le modele trained at 92\% etait mieux que celui trained at 100\%, ce qui s'explique aussi par la fluctuation du loss qui augmente et diminue.
%Le modèle apprend, puis regarde de nouvelles choses, puis oublie ? }

%Our results are also quite different from \cite{zhang:etal:2024}, who say shifting the distribution sampled at inference of the functions does not affect their models.  Our results show such a shift affects the results in an important way, where we take $N(0,1)= D_F$ ( but $D^{t}_F = N(0, \sigma)$ for $1 \leq \sigma \leq 10$. Figure 1 clearly shows that for transformer models with soft attention, this task shift reduces performance dramatically. 

%\cite{giannou:etal:2024} also only examine differences in sampling the sequences of points in the prompt; i.e. in our notation $D_I \neq D^t_I$. We comment on this in Section \ref{sec:4.4}. 

%\section{What and how are the models learning?}

%Unlike the suggestions in prior work \cite{akyurek:etal:2022,vonoswald:etal:2023}, %might lead one to expect a transformer model to perform a linear regression to ICL a linear function given points from its graph. \cite{giannou:etal:2024} suggests that models use Newton's method.  However, 
%Observations 2, 3, 4, 5, 6 confirm that 
%our models do not ICL class forms of polynomials or use regression or Newton's method for approximating arbitrary polynomial functions from a small sample of data points. %We've also shown that our models do not capture class forms, not even the class form of ${\cal P}^1$ as they do not adequately generalize outside observed data.  %Error analysis showed the existence of boundary values $-B,B$; models do well on the interval $[-B, B]$ degrade outside of them.  These boundary values fluctuate depending on model training distributions and size.  
%But Sections \ref{sec:gen} and \ref{sec:zeros} also show that our models can do things that those methods don't do with the same training.    %Thus, algorithms like  don't explain our models' ICL function behavior.    %All this is strong evidence that models did not learn to use linear regression to solve this task and 
%failed to learn the concept of a strictly monotone increasing or decreasing linear function in ${\cal P}^1$ over intervals of $\mathbb{R}$ outside of its training.  

Our models' diverse capacities (but limited in terms of generalization) come from their ability to estimate continuations of a given sequence using principally their attention layers. Given that $Attention(X_i)+X_i$ takes the attention layer output and adds $X_i$, we can calculate the limit of seen values when training over uniform distributions.  These correspond empirically to the interval given by the model's boundary values $B^-, B^+$. Thus, given an input $x = (x_1, ... x_n)$, attention defines a projection $A(x) \mapsto [B^-, B^+]$ 
Attention layers successively refine this projection to predict $\fh(x_n)$, as we have seen that multiple layers and multiple heads improve performance. 

 We have seen that the projection $A$ is effectively nonlinear on elements outside of $[B^-, B^+]$.  This limitation is due, we conjecture, to training on a restricted interval.  Nevertheless, the limitation is not easily removed.  While training with distributions over much larger intervals, for example $U(-100, 100)$, might make the attention projection linear over a larger field than ${\cal F}_{[B^-, B^+]}$ (the one defined over $[B^-, B^+]$), Table 1 shows that such training results in very bad performance on all the testing scenarios we tried. Thus, we see a dilemma: training on restricted intervals is needed for good results, but it inevitably hampers generalizability.
 
 


\section{Conclusion}
We have distinguished two notions of learning of mathematical functions: ICL$_1$, the learning of function graphs over restricted intervals, and ICL$_2$, the learning of a class form $a_1x^n + a_2x^{n-1} + ...  a_n$ for a function in ${\cal P}^n$.  We have shown that transformer models can ICL$_1$ any continuous function within selected intervals.  Our pretrained models also acquired surprising learning capacities on unseen polynomial classes and on finding zeros of polynomials, which they did better than state of the art LLMs.  However, we have also shown a systematic failure of generalization for decoder-only transformer models of various sizes (up to 9.5 million parameters) and architectures, even when trained on non-noisy data. We have isolated the cause of this problem in the attention mechanism and in its pretraining.  Given our results and the discussion above, we do not see an easy solution to this problem. %The minimality of our examples and the capacity to easily train the models from scratch is a strength of our study. %However, the models did learn to perform a projection from close sequences seen during training that enabled them to approximate functions over intervals where their training gave lots of examples.  

%The implications of this work are important not only for understanding ICL but also for optimizing its practice.  First, ICL has limited generalization abilities with respect to out of training distribution data, but it seems highly capable at least in modeling mathematical functions when training and testing distributions for data are aligned.  Second, users of ICL should know the training regime of the models; trying to ICL functions outside the training distribution will very likely lead to degraded results that can importantly affect downstream tasks.  So if users are not pretraining the models themselves, then the model builders should make training data and distributions available. Though we are working on very simple data, it is highly likely that this lesson applies to ICL in more challenging areas like NLP.

%We have highlighted an important issue with ICL: the gap between what LLMs \textit{can} learn and what they \textit{actually} learn. Larger models also face this limitation.  %We hope our work will inspire further research into what transformer-based models are actually doing on ICL tasks.

%\subsection{References}


%\subsection{Appendices}

%Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

\section*{Impact Statement}
The implications of this work are important not only for understanding ICL but also for optimizing its practice.  First, ICL has limited generalization abilities with respect to out of training distribution data, but it seems highly capable at least in modeling mathematical functions when training and testing distributions for data are aligned.  Second, users of ICL should know the training regime of the models; trying to ICL functions outside the training distribution will very likely lead to degraded results that can importantly affect downstream tasks.  So if users are not pretraining the models themselves, then the model builders should make training data and distributions available. Though we are working on very simple data, it is highly likely that this lesson applies to ICL in more challenging areas like NLP.

%\section*{Acknowledgments}

%...

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
%\bibliography{custom}


%\appendix










% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Training details}
\label{sec:appendixA}
%\large{\bf Appendix A: Training details} 

\textbf{Additional training information:} We use the Adam optimizer \cite{diederik2014adam} , and a learning rate of $10^{-4}$ for all models.\\
\textbf{Computational resources:} We used 1 GPU Nvidia Volta (V100 - 7,8 Tflops DP) for every training involved in these experiments. \\
\section{Error rates with Gaussian test and training distributions
}
\label{sec:appendixB}
\begin{figure}[!h]
\center
\includegraphics[width=10cm]{figures/alldeg.png}
\caption{Evolution of error rates for various 12L8AH $d_{emb} = 256$ models  with $D_F, D_I = D^t_I = N(0,1)$ and $D^t_F$ for various $N(0, \sigma)$ trained from scratch on different degrees.  The purple curves illustrate a model that predicts $f(x_n) = 0, \forall f$ and $\forall x_n$. The dark red line LS represents a perfect estimator given our totally clean input data.
\label{deg1}}
\end{figure}

When $D_I = D_F = N(0,\sigma)$ there is  for $x \in  N(0,\sigma)$  an over %68\% chance that a function chosen for train $f$ will have $f(x)\in [-\sigma, \sigma]$ and over a
 85\% chance of  $f(x) \in [-4 \sigma^{2} -  2 \sigma, 4 \sigma^{2} + 2 \sigma]$ and a  95\% chance $f(x) \in [-2\sigma, 2\sigma]$. So a model with $\sigma = 1$ $D_F = D_I = N(0,1)$ has seen sequences of values for $f$ with $f(x) \in [-2,2]$ more than 95\% of the time. 
 
\section{Graphs for $|x|$ and for $f(x) = x^5 - x + 1$}
\label{sec:appendixC}
See figures \ref{x5} and \ref{absx}
 \begin{figure}[!h] 
 \center
\includegraphics[width=6cm]{figures/abss.png}
\caption{Plots for model  $P^3$ for the prediction of $f(x)$=$|x|$}\label{absx}
\end{figure}


 \begin{figure}[!h] 
 \center 
\includegraphics[width=6cm]{figures/p5.png}
\caption{Shape of  $f(x)$=$x^5 - x +1$}\label{x5}
\end{figure}
Based on Galois' theorem, this polynomial has only one real value and which can be only determined approximately. When we apply Newton's method to look for its zero, choosing starting point $x=0$, the method does not converge.

\section{Error progression for models trained on $U(-1,1)$ and tested on $U(-\sigma,\sigma)$}
\label{sec:appendixD}
\hidden{
 Training with  $U(-5,5)$ gave good results for $D^t_F = D^t_I = U(-1,1)$.  Models were able to find target functions with coefficients in [-1,1] from only 2 points (see leftmost plot of Figure \ref{p2>p1} in Appendix \ref{sec:appendixJ}); and all our models work well when $D_F, D_I, D^t_F, D^t_I$ use the same distribution.  %The models trained on a uniform distribution sometimes do even better than models trained on N(0,1) or a bimodal distribution--up to three times better for $D^t_F = D^t_I = N(0,9)$ as Table \ref{table:1} shows.  
Learning was at times very efficient, requiring just n+1 prompts for a polynomial function of degree n, as in Figure \ref{p2>p1} .
}
%\large{\bf Appendix B: Table of error progression for models trained on $N(0,1)$ distributions tested on $N(0,\sigma)$}

\begin{table*}[!h]
\small{
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|}
 \hline
  degree & models \ / \ $\sigma$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 
 \hline\hline
 \multirow{4}{*}{1} & M1 & 0.0& 0.03& 0.55& 1.37& 4.0& 5.17& 9.04& 12.07& 19.28& 27.85 \\ 
 &M135 & 0.0& 0.03& 0.26& 0.7& 2.63& 3.2& 6.5& 8.61& 15.21& \textbf{22.98} \\ 
 &M135AL  & 0.0& \textbf{0.01}& \textbf{0.07}& \textbf{0.31}& \textbf{1.47}& \textbf{2.51}& \textbf{5.28}& \textbf{7.93}& \textbf{15.06}& 23.52 \\ %\hline 
 &REF: y=0 & 0.46 & 1.84& 4.14& 6.09& 11.56& 13.88& 20.22& 24.72& 34.69& 46.92 \\ 
 \hline \hline
 \multirow{4}{*}{2} &M2  & 0.0& 0.02& 0.48& 1.49& 4.01& 6.41& 9.69& 13.11& 19.96& 32.97 \\
 &M135 & 0.0& 0.02& 0.35& 1.05& 3.31& 5.08& 8.1& 10.72& \textbf{16.84}& \textbf{28.95} \\
 &M135AL & 0.0& \textbf{0.01}& \textbf{0.12}& \textbf{0.64}& \textbf{2.31}& \textbf{4.63}& \textbf{7.2}& \textbf{10.47}& 17.56& 30.25 \\ %\hline
 &REF: y=0   & 0.52& 2.05& 4.88&7.53& 13.09& 17.54&23.5& 27.69& 36.76& 55.49 \\
 \hline \hline 
 \multirow{4}{*}{3} &M3  & 0.0& 0.02& 0.41& 1.25& 3.69& 6.77& 9.12& 12.14& 19.34& \textbf{31.57} \\
 &M135 & 0.0& 0.03& 0.42& 1.21& 3.94& 6.5& 9.46& 12.03& \textbf{19.3}& 31.84 \\
 &M135AL & 0.0& \textbf{0.01}& \textbf{0.22}& \textbf{0.84}& \textbf{3.04}& \textbf{6.23}& \textbf{8.85}& \textbf{11.87}& 20.17& 32.94 \\ %\hline
 &REF: y=0   & 0.56& 2.27& 5.45& 8.06& 14.64& 20.18& 25.91& 29.56& 40.03& 58.81 \\
 \hline \hline 
 \multirow{5}{*}{4} &M4  & 0.0& 0.03& 0.44& 1.48& 4.66& 7.65& 10.5& 14.47& \textbf{20.36}& \textbf{32.94} \\
 &M135  & 0.0& 0.03& 0.49& 1.48& 4.9& 7.59& 10.84& \textbf{14.83}& 20.55& 33.92 \\
 &M135AL &0.0& \textbf{0.02}& \textbf{0.29}& \textbf{1.14}& \textbf{4.17}& \textbf{7.49}& \textbf{10.33}& 14.84& 21.55& 35.19 \\ %\hline
 &REF: y=0   & 0.6& 2.45& 5.66& 8.8& 16.08& 21.91& 27.73 & 32.9& 41.85& 61.29 \\
 \hline \hline 
 \multirow{4}{*}{5} &M5  & 0.0& 0.02& 0.46& \textbf{1.51}& \textbf{4.52}& \textbf{7.9}& \textbf{10.52}& \textbf{16.4}& \textbf{21.84}& \textbf{37.66} \\
 &M135  &0.0& 0.04& 0.62& 1.81& 5.5& 8.43& 11.93& 17.19& 22.69& 38.66 \\
 &M135AL  & 0.0& \textbf{0.02}& \textbf{0.43}& \textbf{1.51}& 4.83& 8.44& 11.54& 17.56& 23.97& 40.58 \\ %\hline
 &REF: y=0   & 0.64& 2.57& 6.0& 9.65& 17.11& 23.42& 29.09& 36.25& 44.41& 66.77 \\
 \hline %\\ [1ex] 
\end{tabular}
}

\caption{Comparison to show the evolution of squared error $\epsilon$, with $D_i^t = U(-1,1)$, $D_F^t = U(-\sigma,\sigma)$ for models $M1$, $M135$ and $M135AL$ }
\label{table:3}
\end{table*}
\hidden{
 \begin{figure}[!h] 
\includegraphics[width=8cm]{figures/hmap.png}
\caption{Heatmap showing evolution of squared error $\epsilon$ with $D^t_I=D^t_F= U(-\sigma,\sigma)$ for $\sigma \in \{1,...,10\}$ for the model M1 for both $x$ and coefficients. }\label{hmaplinear}
\end{figure}
}
 \begin{figure}[!h] 
\includegraphics[width=15cm]{figures/hmdeg3.png}
\caption{Heatmap showing evolution of log of squared error $\epsilon$ with $D^t_I=D^t_F= U(-\sigma,\sigma)$ on polynomials of degree 3 for $\sigma \in \{1,...,10\}$ for the models M135, the left trained on $D_I=D_F=U(-1,1)$ and the right on $D_I=D_F=U(-5,5)$. }\label{hmap}
\end{figure}

\newpage
\section{Boundary values}
\label{sec:appendixE}

 %As documented in \cite{naim:asher:2024b} for linear functions, our models exhibit problematic behavior of 2 kinds around "boundary values" $B-, B+$ when approximating any higher order polynomial     function. First, once outside the interval $[B-, B+]$ but reasonably close to that interval given some factor $\alpha_M$ dependent on the model $M$ with the densest number of training sequences, all models begin to approximate $f$ with a constant function returning $\fh(x) \approx B-$ for $f(x) < B-$ and $\fh(x) \approx B+$ for $f(x) > B+$. Beyond $[B- -\alpha_M, B+ +\alpha_M]$ our models return random values within $[B-, B+]$.
 
 \begin{figure}[!h] 
\includegraphics[width=5.1cm]{figures/bdvalues2.png}
\includegraphics[width=5.1cm]{figures/bdvalues3.png} 
\includegraphics[width=5.1cm]{figures/ahboundary.png}
\caption{First two plots for models $P2$ for $f(x)=x^2$ $P3$ for $f(x)=x^3$, trained on $D_I = D_F = N(0,1)$ showing boundary values. Third plot shows boundary values for 2L32ah attention only model, with $d_{embedding}= 256$ to ICL the function $f(x) = 12x$. }\label{sequence1}
\end{figure}
 \begin{figure}[!h] 
\includegraphics[width=5.1cm]{figures/bdvaluesahonly.png}
 \caption{Plot is for $f(x) = x$ on attention only and no norm still showing boundary values. }\label{sequence2}
\end{figure}





\newpage
% Consider the middle plot for $f(x) = 10x$ in Figure \ref{sequence}.
%\begin{figure}[t]
%\includegraphics[width=6cm]{figures/big_10bisss.png} 
%\includegraphics[width=6cm]{figures/trainingprompts-big10bisss.png} 
%\caption{
%Plots of predictions of the 12L8AH model trained on $N(0,1)$ and error evolution over number of prompts for $f(x) = 10x$ \label{big10}}
%\end{figure}
%{\color {magenta} on pourrait mettre ca sur la meme ligne et puis }
%The plot shows that the model's prediction $\fh(x)$ diverges dramatically from $f(x)$ outside of a certain interval, but the rightmost plot shows that it has approximated well within that interval.  Appendix D contains a graph over length of the prompt showing that it has learned something with ICL.% about the function from the prompt and approximates it at least within a certain interval. 

%For equations $f(x)$  sampled outside $N(0,1)$ (for example $f(x) = 30x + 30$ and $D^t_I = N(0,1)$, however, the results are catastrophic and similar to those in the first plot of Figure \ref{sequence}. Figure \ref{big30} in the Appendix shows that the model doesn't converge to any stable prediction with ICL.  

%This behavior %occurs also for values sampled from $D^t_I$ outside the training regime (as in Figure \ref{sequence} and 
%holds across all models and all polynomial classes we tested.  %For example with $D_F = D_I = U(-5,5)$, consider again as an illustrative example the target function, $f(x) = 9x$ for our largest trained model. (See Figure \ref{40x+40})
%
%Constraints from boundary values hold for all transformer models tested (for plots see Appendix D and Figure \ref{relu_shape2}) and for attention only models . %However, due to the parameter $\alpha$, larger models trained on the same distribution and the same number of data will ICL ${\cal P}_1$ functions over a slightly larger number of intermediate values than smaller models, as Figure \ref{progressive-loss} suggests. Figure \ref{fig:boundary}  in the appendix  shows plots for the predictions of two models (12L8AH, and 6L4AH) for $D_F, D_I = N(0,1)$ for target $f(x) = 10x$.  The larger model has boundary values $\approx$ -13.7, 13.7, the smaller one boundary values $\approx$ -12, 12.

%\large{\bf Appendix D: Plots for boundary values with $N(0,1)$ and $U(-5,5)$} \\
\hidden{
\begin{figure}[t]
\includegraphics[width=6cm]{figures/big-30biss.png}
\includegraphics[width=6cm]{figures/traingprompts-big30biss.png}\\\includegraphics[width=6cm]{figures/2L32AHfx=x.png}
\includegraphics[width=6cm]{figures/2L32AHfx=15x.png}\\
\caption{
Plots on first line of  predictions for the 12L8AH model trained on $N(0,1)$ and error evolution over number of prompts for $f(x) = 30x + 30$. On second line Plots for $f(x) = x$ and $f(x) = 15x$ for models 2L attention only with 32AH and $d_{embedding}=256$ \label{big30}}

\end{figure}
\begin{figure}[t]  \includegraphics[width=6cm]{figures/12l8ahU5f9xbis.png}
\includegraphics[width=6cm]{figures/40x+40bis.png}\\
%\includegraphics[width=11cm]{figures/relu-shape.png} 

\caption{Plots for $f(x) = 9x$ and $f(x) = 40x + 40$ for a 12l8ah model trained on $U(-5,5)$ \label{40x+40}}
\end{figure}

%As shown in the left plot in Figure \ref{40x+40}, $\fh^+(v) \approx 30$ for values $v$ for which the ground truth target function $f$ is such that $30 \leq f(v)$, and the model predicts an approximally constant function $\fh^-(v) \approx -30$ for values $v$ on which $f(v) \leq -30$. Given a training on $U(-5,5)$ we can calculate 30 and -30, with $30 = 5*5+5$ and $-30 = -5*5 -5$, to be the boundary values for the models there.
%Here we add some more examples
\begin{figure}[t] 
\includegraphics[width=6cm]{figures/6l4ahU5f9x.png}
\includegraphics[width=6cm]{figures/3l4ahU5f9x.png}\label{relu-shape3}\\
%\includegraphics[width=11cm]{figures/relu-shape.png} 
\caption{Boundary values: Plots for $f(x) = 9.4x$ for models 3L4AH and 6L4AH, $D_I = D_F = D_I^t = D_F^t = U(-5,5)$\label{relu_shape2}}
\end{figure}

\begin{figure} 
\includegraphics[width=6cm]{figures/boundsdN01bis.png}
\includegraphics[width=6cm]{figures/boundtinyN01bispng.png}
\caption{Plots for $f(x) = 10x$ by a 12L8ah model and by a 6L4ah model.} \label{fig:boundary}
\end{figure}
}

%\hidden{\section{Behavior with attention only models}
%\label{sec:appendixD}
\hidden{
\begin{figure}[!h]
%\includegraphics[width=11cm]{figures/evolution of errorsbis.png} 
\center 
\includegraphics[width=8cm]{figures/aherrors.png}
\caption{Evolution of error rates for models with attention layers only on ${\cal P}^1$. We give figures for a model with only 1 attention layer/1AH (1AL1AH) two 2-attention layer only models  (2AL8AH, 2AL32AH) and two 3 attention layer only model  (3AL4AH,3AL8AH). $D_I=D_F=U(-1,1)$, $D^t_i = U(-1,1)$ and  $D^t_F=N(0,\sigma)$.  All models have embeddings of size 64, except $2AL32AH$ has size 256.
\label{progressive-lossAH}}
\end{figure}
%}
}

\hidden{
 \begin{table*}[!h]
\small{
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
 \hline
  models \ / \ $\sigma$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 
\hline\hline
 %$3L4AH_N$   & 0.0 & 0.0 & 0.22 & 0.4 & 1.73 & 6.56 & 8.56 & 20.44 & 39.73 & 53.93 \\
% \hline
% $3L4AH_B$,   & 0.03 & 0.15 & 0.53 & 1.32 & 2.74 & 3.91 & 5.52 & 10.22 & 13.86 & 22.72 \\
% \hline
%  $3L4AH_U$   &  0.02 & 0.03 & 0.13 & 0.36 & 0.84 & 1.79 & 2.54 & 7.06 & 11.38 & 17.75 \\ [1ex] 
% \hline\hline
 $1AL1AH_{U}$   & 0.38 & 2.29 & 9.3 & 14.97 & 25.25 & 37.54 & 45.4 & 67.0 & 95.19 & 117.6 \\ [1ex] 
 \hline
 $2AL8AH_{U}$   &  0.1 & 0.62 & 5.53 & 10.59 & 18.62 & 30.61 & 36.97 & 57.79 & 83.26 & 103.58 \\ [1ex] 
 \hline
% $2Al32AH_U$ &  0.86 & 1.61 & 3.53& 10.95& 22.43 & 35.3 & 46.98 & 67.12 & 104.83 & 135.21 \\
% \hline
 $3AL4AH_{U}$   &  0.35 & 1.42 & 8.17 & 15.13 & 24.15 & 37.99 & 45.2 & 68.73 & 96.37 & 118.3 \\ 
 \hline
 $3AL8AH_{U}$   &  0.12 & 1.16 & 5.45 & 9.36 & 18.22 & 28.77 & 35.62 & 52.44 & 78.12 & 100.18 \\ [1ex] 
  \hline
  $2Al32AH_N$ & 0.06 & 0.91 & 5.96 & 10.43 & 18.96 & 30.11 & 36.77 & 55.59 & 81.66 & 103.17\\
 \hline\hline
 $REF_{D^t_F,D^t_I}: y=0$   &  1.52 & 4.43 & 13.55 & 19.94 & 30.81 & 44.75 & 52.71 & 76.11 & 105.43 & 128.52 \\ [1ex] 
 \hline
%  \hline\hline
%  $2Al32AH_N$ &1.17 & 2.64& 3.47& 5.01& 7.88& 16.85& 24.1& 40.98& 66.04& 95.03\\
%\hline
\end{tabular}
}
\caption{Comparison showing the evolution of squared errors for  models with attention layers only. We give figures for a model with only 1 attention layer/1AH (1AL1AH) two 2-attention layer only models  (2AL8AH, 2AL32AH) and two 3 attention layer only model  (3AL4AH,3AL8AH). $D_I=D_F=U(-1,1)$, $D^t_i = U(-1,1)$ and  $D^t_F=N(0,\sigma)$.  All models have embeddings of size 64, except $2Al32AH$ has size 256.}
\label{table:2}
\end{table*}

\begin{table*}[!h]
\small{
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
 \hline
  models \ / \ $\sigma$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 
 \hline\hline
 $1L1AH_N$ $d_{embedding}$=64  & 48.8 & 57.62 & 73.48 & 84.51 & 116.63 & 129.52 & 142.34 & 177.69 & 191.05 & 246.43 \\
 \hline
 $2L8AH_N$ $d_{embedding}$=64  & 2.24 &4.81 & 5.8 & 7.19 & 10.01 & 19.04 & 30.22 & 38.03 & 73.32 & 118.89 \\
 \hline
$2L32AH_N$ $d_{embedding}$=256  & 1.17 & 2.64 & 3.47 & 5.01 & 7.88 & 16.85 & 24.1 & 40.98 & 66.04 & 95.03 \\ [1ex] 
 \hline
 \textbf{REF: y=0}   & 2.19 & 7.05 & 19.22 & 33.94 & 52.23 & 73.08 & 86.02 & 127.43 & 165.27 & 199.31 \\ [1ex] 
 \hline
\end{tabular}
}

\caption{Comparison to show the evolution of squared $\epsilon$ type error depending on the distribution according to which we take the parameters, without taking into account the error of the prediction of the first and second prompts. $D_F=D_I=D_i^t = N(0,1)$ for models with attention ONLY}
\label{table:4}
\end{table*}
}
%\newpage
%\large{\bf Appendix F: The model searches for a sequence close to the input sequence.}


\section{Calculation of what causes boundary values}
\label{sec:appendixF}

Given that our ablation study showed the existence of boundary values for models just with attention layers and without residual learning, norm or feed forward linear layers, we analyze the attention mechanism mathematically to see where boundary values could arise/  
Let's consider $X^l=(X_1^l,...,X_n^l)$ is the input that goes through the multihead attention in a layer $l$. The output of Multihead Attention after going through 8 attention heads, then a Linear layer is : $(C_1^l, C_2^l,...,C_n^l)$ where $$C_i^l  = \Sigma^8_{h=1} \Sigma^n_{k=1}W_O^{l}softmax(\frac{(W_Q^{h,l}X_i^l)^{T}(W_K^{h,l}X_i^l)}{\sqrt{d_k}})(W^{h,l}_VX_k^l) $$ 
where $W_Q^{h,l}, W_K^{h,l}, W_V^{h,l}$ are respectively Query, Key and Value weights matrices in attention head $h$ in layer $l$, $d_k$ is the dimension of the key matrix and $W_O^{l}$ is the matrix of the linear layer that comes after the attention heads in the layer l. All those matrices have fixed values from training. \\
To investigate what is causing boundary values into the attention block, Let's consider a model of 1 layer, and take a fixed $X^1=(X_1^1,...,X_n^1)$, we consider also $1000X^1=(1000X_1^1,...,1000X_n^1)$. \\
The output of the model for $1000X^l$ is: 
$(C_1^1, C_2^1,...,C_n^1)$
where\\ 
\begin{equation} \label{attn-calculation}
C_i^1  = \Sigma^8_{h=1} \Sigma^n_{i,j=1}W_O^{l}softmax(\frac{(W_Q^{h,l}1000X_i^l)^{T}(W_K^{h,l}1000X_j^l)}{\sqrt{d_k}})(W^{h,l}_V1000X_k^l)
\end{equation}\\
Now assuming that the learned embedding $\epsilon: \mathbb{R} \rightarrow Z^{256}$, for some set $Z$, somewhat respects arithmetic operations, then $\epsilon(1000X) = \epsilon(a)\times \epsilon(X)$ and we could infer from Equation \ref{attn-calculation} and the fact that the matrices are all linear projections on $\mathbb{R}$,
\begin{equation} \label{attn-calculation1}
a\Sigma^8_{h=1} \Sigma^n_{i,j=1}W_O^{l}softmax(10^6\frac{(W_Q^{h,l}X_i^l)^{T}(W_K^{h,l}X_j^l)}{\sqrt{d_k}})(W^{h,l}_VX_k^l)  
\end{equation}
The output of each layer will reduce these large values given small values of soft- or even hardmax.  But as is evident from Table 4, for large values v, hardmax(v) and softmax(v) give us probability 1 on the greatest value and 0 for the rest. So this derivation predicts that the attention mechanism should give large values for large inputs.  But it does not.


This proof rests on two assumptions: (i) the learned embedding somewhat preserves arithmetical operations (ii) the matrices that defined Attention are in fact linear projections in $\mathbb{R}$.  We checked the ``vanilla" encodings $\epsilon_V$ on GPT2, and  we saw that that embedding $\epsilon_V(a *b)$ is typically not even close using cosine similarity to $\epsilon_V(a)*\epsilon_V(b)$, where $*$ is some arithmetical operation.  However as can be seen from Figure 11, the learned embedding from our pretraining preserves general mathematical operations and induces an almost perfect linear ordering on $[-1000,1000]$.  This entails then that at least one of the matrices used to define Attention $W_Q^{h,l}, W_K^{h,l}, W_V^{h,l}$ is only linear on the small finite field  ${\cal F} = ([B^-, B^+], +_{\cal F}, \times_{\cal F}, 0_{\cal F}, 1_{\cal F})$.


 \begin{figure}[h] 
 \center
\includegraphics[width=12cm]{figures/embdgs.png}
\caption{Plot illustrating how embeddings exhibit increasing amplitudes as the values of the real numbers grow, visualized using t-SNE with two dimensions (TSNE-2). }\label{embeddings}
\end{figure}




\begin{table*}[ht]
\centering
\begin{tabular}{|c|c||c|c||c|c|}  % Row label + 3 columns
\hline
\textbf{Vector values}& \textbf{Softmax} & \textbf{Vector values}& \textbf{Softmax} & \textbf{Vector values}& \textbf{Softmax} \\ \hline
\textbf{$1$}  &  0.192046    & \textbf{$100$}  & 0.000045  & \textbf{$1$}  &  0
\\ \hline
\textbf{$1.02$}  &  0.195925   & \textbf{$102$}  & 0.000333 & \textbf{$1020$}  &  0          
             
\\ \hline
\textbf{$1.03$}  &  0.197894   & \textbf{$103$}  & 0.000905 & \textbf{$1030$}  &  0         
\\ \hline
\textbf{$1.05$}  &  0.201892   & \textbf{$105$}  &  0.006684 & \textbf{$1050$}  &  0            
\\ \hline
\textbf{$1.1$}  &  0.212243   & \textbf{$110$}  &  0.992033 & \textbf{$1100$}  &  1         
\\ \hline

\end{tabular}
\caption{Table showing that for large values, softmax performs like hardmax} \label{table:softmax}
\end{table*}  

\section{Selected functions for "finding zeros of unknown functions"}

The functions we looked for their zeros with their following scores for different models are for our scores are: $cos(ax)$ for $x\in [0,\pi]$, $a\in[-1,1]$,  $sin(ax)$ for $x\in [-\pi/2,\pi/2]$, $a\in[-1,1]$, $tan(ax)$ for $x\in ]-\pi/2,\pi/2[$, $a\in[-1,1]$, $aexp(x)-b$ for $a,b \in [0,1], x \in [-1,1]$ and $ln(ax)$ for $a,x \in ]0,1]$.

\section{More details on Zeros of functions}
Table \ref{table:zeroes1}
\label{sec:appendixH}
\begin{table*}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|}  % Row label + 3 columns
\hline
\textbf{Functions \ Models}& \textbf{M3} & \textbf{M135} & \textbf{M135AL} & \textbf{GPT4} \\ \hline
\textbf{$ln$}  &  20/20 & 20/20 & 20/20 & 3/20              
\\ \hline
\textbf{$cos$}  &  20/20 & 20/20 & 20/20 & 0/20              
             
\\ \hline
\textbf{$sin$}  &  20/20 & 20/20 & 20/20 & 0/20              
\\ \hline
\textbf{$tan$}  &  12/20 & 20/20 & 20/20 & 4/20              
\\ \hline
\textbf{$exp$}  &  15/20 & 20/20 & 18/20 & 0/20              
\\ \hline
\textbf{$ax+b$}  &  9/20 & 12/20 & 0/20 & 5/20              
\\ \hline
\textbf{$(x^2+1)(x-a)$}  &  11/20 & 14/20 & 17/20 & 5/20              
\\ \hline
\textbf{$(x^2+d)( (x+2)(x-a)$}  &  7/20 & 11/20 & 11/20 & 0/20              
\\ \hline


\end{tabular}
\caption{Comparison between our models M3, M135 and M135AL trained from scratch on a sampling of $f \in {\cal P}^3$ and gpt4 for finding the 0 of an unknown continuous function} \label{table:zeroes1}
\end{table*}  

\section{Calculating proportionality of test in train}
\label{sec:proba}

Using tests for coefficients on  $U(-\sigma,\sigma)$ show us how error rates evolves when we increase the proportion of test elements outside the training distribution. We start by testing on $x$ and coefficients in $U(-1,1)$ where the model have seen all the values, then we go through $U(-1,1)$ where the model has seen fewer values. For example for degree 1, the model has seen values during training $a,b,x \in [-1,1]$ , which means $ax+b \in [-2,2]$ \\

Given $a \in U(-2,2)$ and $x \in U(-1,1)$, $ax \in Z$ where $Z=X_1X_2$ is the product of two random variables and an addition. \\
The probability that the model was asked to ICL a value it didn't see during training is $$P(X=ax+b \notin [-2,2]) = 1 - P(X=ax+b \in [-2,2])$$

To calculate it, we need first the density of $Z=X_1X_2$ which is a product of two random variables: \\
since $X_1=X_3$ are $U(-2,2)$, then $f_{X_1}(x) = f_{X_3}(x)= 1/4$ if $x\in [-2,2]$ and $0$ otherwise. And $X_2$ is  $U(-1,1)$ $f_{X_2}(x)= 1/2$ if $x\in [-1,1]$ and $0$ otherwise.
 
$f(z)= \int_{-\infty}^{+\infty} f_{X_1}(x)f_{X_2}(\frac{z}{x})\frac{1}{|x|}dx = \int_{-2}^2 \frac{1}{4}f_{X_2}(\frac{z}{x}) \frac{1}{|x|}dx $ 

$f_{X_2}(\frac{z}{x}) \neq 0$ when $-1 \leq \frac{z}{x} \leq 1 $ which means that $\frac{|z|}{|x|} \leq 1$, then $|z| \leq |x|$, so $ z \leq x $ or $x \leq -z$, otherwise $f_{X_2}(\frac{z}{x}) = 0$, so $f(z)= \int_{z}^2 \frac{1}{4}f_{X_2}(\frac{z}{x}) \frac{1}{|x|}dx + \int_{-2}^{-z} \frac{1}{4}f_{X_2}(\frac{z}{x}) \frac{1}{|x|}dx = \frac{1}{4} ( \frac{1}{2 }\int_{z}^2 \frac{1}{|x|}dx + \frac{1}{2} \int_{-2}^{-z}\frac{1}{|x|}dx)  = \frac{1}{4} ( \int_{|z|}^2 \frac{1}{|x|}dx) =  \frac{1}{4}ln(\frac{2}{|z|})$   \\
Now that the density of product of two random uniform variables $U(-1,1)$ and $U(-2,2)$ is known, the density of probability of the addition with $U(-1,1)$ $X_1X_2+X_3 = Z + X_3$ is:  

$f(s)= \int_{-\infty}^{+\infty} f_{X_1X_2}(s-x)f_{X_3}(x)dx = \int_{-2}^{2} ln(\frac{2}{|s-x|})\frac{1}{4}dx = \frac{1}{4} \int_{-2}^{2} ln(\frac{2}{|s-x|})dx = \frac{1}{4} \int_{-2}^{2} ln(2) -ln(|s-x|)dx  = ln(2) -  \frac{1}{4}\int_{-2}^{2} ln(|s-x|)dx$ when $s\in [-4,4]$ otherwise $0$.

The following graph illustrates the situation. 

 \begin{figure}[!h] 
\includegraphics[width=10cm]{figures/density.png}
 \caption{Plot of the density of probability of $X_1X_2+X_3$ }\label{probas}
\end{figure}

Then the out of distribution probability is:   $$P(X=ax+b \notin [-2,2]) = 1 - P(X=ax+b \in [-2,2]),$$or  near $20\%$.  Thus we increase gradually the proportion of values not seen during training, each time we increase the value of $\sigma$.

%if $z \geq 0$: $f(z) = \int_{z}^2 \frac{1}{4}f_Y(\frac{z}{x}) \frac{1}{x}dx - \int_{-2}^{-z} \frac{1}{4}f_Y(\frac{z}{x}) \frac{1}{x}dx$
%\int_{z}^2 \frac{1}{4}\frac{1}{2} \frac{1}{x}dx = \frac{1}{8}ln(\frac{2}{|z|})$ when $z \in [-2,2]$, otherwise it is 0. Now that we find the density of probability of the product, we need to sum it with one of $U(-2,2)$ 




\section{Data on prompt length}
\label{sec:appendixJ}

While at least n+1 points are needed to find a polynomial in ${\cal P}^n$ function, all model performance regardless of training distribution degrades when the size of the prompt during inference is greater than the maximal size of prompts seen in training. Figure \ref{P2P3}  (Appendix \ref{sec:appendixJ}). 

\begin{figure}[!h]
\includegraphics[width=6cm]{figures/P2.png} 
\includegraphics[width=6cm]{figures/P3.png}
\caption{Plot of ICL for $f(x) = x^2$ for P2  and $f(x)=x^3$ for P3 when increasing retrogressively the size of the prompts to depass the training maximum sequence length.} \label{P2P3}
\end{figure}
%\large{\bf Appendix C: Failure to generalize to longer prompt sequences}
\hidden{
\begin{figure}[!h]
\includegraphics[width=6cm]{figures/2pointsbis.png} 
\includegraphics[width=6cm]{figures/p2supp1bis.png}
\caption{Plot of ICL for $f(x) = x$ with $D_F=D_I=D_I^t= U(-5,5)$ for the model 12L8AH; the one on the left zooms in on the first 40 points, where we see that models can often learn a sequence given by a linear function from 2 points, the second is a view of what happens overall, when models are trained on sequences of length $>$ 41 prompts.} \label{p2>p1}
\end{figure}
}
%\large{\bf Appendix D:Plots for ICL over number of prompts}
%\begin{figure}[!ht] 
%\caption{ Plot of ICL over number of prompts for $f(x) = x$ with $D_F=D_I=D_I^t= U(-5,5)$ for the model 12L8AH\label{p2>p1}}
%\end{figure}

%\section{Details on zeros for polynomial functions}

\newpage
\hidden{
\section{How models calculate values versus \cite{olsson:etal:2022}'s proposal}
\label{sec:appendixF}
Proposition \ref{observations} implies that the whole sequence $p_2$ matters with $p_2 = p_1$ for optimal prediction, not some small cofinal sequence as in \cite{olsson:etal:2022}   Models don't correct their previous predictions each time they predict a new one.  That is, the autoregressively predicted values remain unchanged as more examples are provided. The example below demonstrates this behavior: the model generates four values after three are provided, and then generates the same four values when an additional (fourth) example is given.\\
In this example we take, $f(x)=x$ for $x \in \{0,0.1,...,0.5\}$ \\
In the first line we give as prompt $(0,0,0.1,0.1,0.2,0.2,0.3,0.3,0.4,?)$ and in the second $(0,0,0.1,0.1,0.2,0.2,0.3,0.3,0.4,0.4,0.5,?)$ \\
Below are the values predicted by the model.
\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black]
-0.0052 | 0.1001 | 0.2961 | {\color{cyan}0.4123}
  \tcblower
-0.0052 | 0.1001 | 0.2961 | 0.4123 | {\color{cyan}0.5237}
\end{tcolorbox}


\cite{olsson:etal:2022} suggests that models average over three closest neighbors.  If we do this, the value predicted will be $0.15$ for the first input, which is far from $0.4$ and $0.3$ instead of $0.5$ for the second example.  The model's method is clearly superior. .\\

A further example to see limits of \cite{olsson:etal:2022} proposition of what the model might be learning.
Let's take $f(x)=x$ for $x \in [ x_1=0.0144, x_2=-0.4471, x_3= -0.6244, x_4=-0.5978]$, we have the prompt $(x_1,f(x_1),...,x_3,f(x_3),x_4,?)$ the trained model predict $-0.5951$ but \cite{olsson:etal:2022}'s proposition returns $-0.3524$. The accuracy of the proposal clearly depends on the sample we got and if it has values really near to the target or not.

Call our method $H$ for computing $y_n$ given $(x_1, y_1,..., x_n)$ and call a simple averaging method like \cite{olsson:etal:2022}'s $A$.

\begin{proposition}  $P(H(\vec{x}) < f(x_n) +\epsilon) >>  P(A(\vec{x}) < f(x_n) +\epsilon)$. 
\end{proposition}
Consider the uniform distribution U(-1,1). $$P(x_i - \epsilon \leq X \leq x_i + \epsilon) = \int_{x_i - \epsilon}^{x_i + \epsilon} \frac{1}{1-(-1)} dx = \epsilon $$
However, as $H$ refines the projection $\pi$, $P(\pi(x_i) < f(x_n) + \epsilon) = i^m\times\epsilon^n$, where $i > 0, m,n < 41$.
On the other hand, $P(A(\vec{x}) < f(x_n) +\epsilon) \approx 0$.}
%\newpage
\hidden{

 \begin{figure*}[!h] 
\includegraphics[width=4cm]{figures/01.png}
\includegraphics[width=4cm]{figures/45.png}
\includegraphics[width=3.7cm]{figures/56.png} 
\includegraphics[width=4cm]{figures/67.png}
\caption{Plots for model  f(x) = 5x for 12L8AH, trained on $D_I = D_F = U(-5,5)$ on different segments showing how boundary values appear when the largest value the model has seen is reached}\label{sequence2-old}
\end{figure*}



%\newpage
}

%\newpage
\hidden{
\begin{table*}[ht]
\centering
\begin{tabular}{c|c}  % Row label + 3 columns
\hline
\textbf{Keyword}& Signification   \\ \hline
\textbf{Mi}  &  Model of full transformer architecture trained only on polynomials of degree i             
\\ \hline
\textbf{Mijk} &  Model Trained only on polynomials of degree i, j and k 
\\ \hline
\textbf{MiAL} &  Model of transformer architecture without MLP Trained only on polynomials of degree i               
                  
\\ \hline

\end{tabular}
\caption{Explanations of keywords} 
\end{table*}  
}
\hidden{
\section{You \emph{can} have an appendix here.}

You can have as much text here as you want. The main body must be at most $8$ pages long.
For the final version, one more page can be added.
If you want, you can use an appendix like this one.  

The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
