\section{Related Work}
Since Brown et al., "Language Models Play Dabo" introduced ICL and Radford et al., "Improving Language Understanding by Generative Models through Unsupervised Learning" investigated ICL for ${\cal P}^1$, there has been considerable research indicating that ICL is possible because of a sort of gradient ``ascent'', higher order optimization techniques or Bayesian principles _____. % Zhang et al., "On the State of Open Domain Conversation Modeling" showed that a small Transformer trained from scratch performed in-context learning of n-dimensional linear functions given identical train and test distributions $N(0,1)$.  
Sukhbaatar et al., " Augmented Recurrent Neural Networks" surveys successes and challenges in ICL, noting that research has only analyzed ICL on simple problems like linear or simple Boolean functions _____.  Li et al., " Meta-Learning for Visual Representation Learning" extends  Brown et al.'s approach to ICL Chebychev polynomials up to degree 11 with training and testing on $N(0,1)$.   % trained small GPT-2 models from scratch to show that Transformers can ICL simple boolean functions, while their performance deteriorates on more complex tasks. 
Chen et al., "Meta-Learning for Few-Shot Learning" investigated how ICL in models evolves as the number of pretraining examples increases within a train=test distribution regime.  Radford et al., "Improving Language Understanding by Generative Models through Unsupervised Learning" propose that {\em induction heads}, a learned copying and comparison mechanism, underlie ICL.  Zhang et al., "On the State of Open Domain Conversation Modeling" shows that neural networks in principle can have greater approximation power than most nonlinear approximation methods.
Bahdanau et al., "Neural Machine Translation by Jointly Learning to Align and Translate" has investigated memory in transformers.  Clark et al., "BART: Denoising Sequence-to-Sequence Pre-training for Language Generation, Conversational AI, and More" defines memory in terms of weighted sum and report that transformers memorize a large amount of data from their training through attention matrices.  Li et al., " Meta-Learning for Visual Representation Learning" argue that LLMs favor memorized data, as the attention mechanism prioritizes memory retention.% and dominates the feed forward layer.% performs generalization. When these two components are combined, the overall performance is often dominated by attentional memory. 

  %do ICL linear functions, but in their experiments the test distribution was the same as the train and  mainly for values $a,b \in N(0,1)$.  %This means that the majority of the values of $a$ and $b$ far from $[-1,1]$ were not examined at all.  We based our code on theirs.% and perform in-context learning of linear functions with models (l,h,64) with $l \in \{1-6\}$, and AH $h \in \{1,2,4\}$, but we trained also bigger models for our experiments such GPT2 (12,8,256).

%åPrior research has investigated how transformers might ICL for linear functions. Brown et al., "Language Models Play Dabo" provided a construction to show transformers ICL from their doing gradient descent during ICL.  Chen et al., "Meta-Learning for Few-Shot Learning" showed that Transformers could ICL in virtue of using higher-order optimization techniques. Zhang et al., "On the State of Open Domain Conversation Modeling" argued that ICL follows from Bayesian principles. Li et al., " Meta-Learning for Visual Representation Learning" show that transformers can under certain assumptions implement many algorithms with near-optimal predictive power on various in-context data distributions.  Radford et al., "Improving Language Understanding by Generative Models through Unsupervised Learning" pretrained a linearly parameterized single-layer linear attention model for linear regression with a Gaussian prior proving that the pretrained model closely matches Bayes's optimal algorithm. %Given Li et al.'s result that full transformers with linear attention are Turing complete, however, these theoretical demonstrations are perhaps not surprising.  

%in order to establish what transformers {\em actually} do as opposed, not what they {\em could} do--which is the objective of much of the research just cited.  

Sukhbaatar et al., " Augmented Recurrent Neural Networks" show that when train and inference distributions do not coincide, ICL performance on linear functions degrades.  Li et al., " Meta-Learning for Visual Representation Learning" shows that transformers models approximate linear functions as well as the best algorithms when train and inference distributions coincide; but when they do not coincide, they show models behave peculiarly around what they call {\em boundary values} (see also Chen et al.).  Within boundary values models perform well; outside the boundary values but near them models predict linear functions to be constant functions and then further out the predictions become random.       %They also investigated out of distribution behavior but only on $D_I \neq D^t_I$ (covariate shifts in Li et al.) (not shifts from $D_F$).  They found that after 4 layers transformer model performance did not perform.  We found that larger models did improve performance, but when we set $D_I \neq D^t_I$, we got bad results when the function’s values on those points were outside what we call boundary values, something which held for all models.  
%Li et al.'s covariate shift is also different from our experiments. They shift the prompt distribution but not that of the query.  When we take a distribution over input points in train $D_I$ and set $D^t_I \neq D_I$, our shift is not the same; we shift both prompt and query distributions. With covariate shifts we found that the choice of points is important and model performance degrades considerably when the values of the functions on the chosen points lie beyond what we call boundary values, which Li et al. do not.  As far as we know we are the first to take boundary values and their dependence on model parameters as key indications of what is actually going on in ICL.
 Our work here builds on Li et al. but extends it to ICL of continuous functions. We show that boundary values exist for all prediction of functions we tested, as well that attention layers in transformers are necessary for ICL of polynomials. Work on neural nets as approximators of continuous functions has concerned MLP only architectures and assumes that the function $f$ is known _____. With ICL, we don't have full access to $f$ but only to limited data about its graph.     %We note in Section 4.3 boundary values for model predictions on all polynomials.    
 %We examine how ICL actually works in practice under different training and testing distributions for transformer based LLMs %However, Li et al. make important modifications to transformer architectures ____ work with linear attention, whereas we look at attention layers as they actually are used with softmax. In addition, ____ uses a new kind of optimization or training with gradients and a special fixed initial point. This means that their architecture and training are quite different from what normally happens with transformers; they are interested in getting a revised transformer-like model to learn linear functions, while we want to find out whether transformers as they actually are learn linear functions or something else.  As we detail below in Sections 4.1 and \ref{sec:4.4}, the results for the architectures of Li et al. are differ from ours for actual transformers. In addition unlike these papers or ____ , we show that prompts that are too long induce chaotic behavior.
%Our objective is examine how  in ICL 1 dimensional linear functions, whereas most prior research has concentrated on transformer models {\em can} or {\em could} do on this task.  Even for this simplest case, we show transformers ICL in a different way from any of these proposed methods.


 


%Li et al. worked out the details on the learning set up for ICL of ${\cal P}^1$
% \\
%\includegraphics[width=15cm]{training_figure.png} \\
%   They used squared error as the loss function and sampled a batch of random prompts at each training step and update the model through a gradient update. They used a batch size of 64 and train for 500k steps. The training was done from scratch on the model. They trained three sizes of transformers (layers l, attention ah, and embedding  size e): (3, 2, 64); (6,4, 128); and (12,8,256).   {\color{blue} je pense qu'on doit parler moins en detail}% with a restricted embedding size (256 instead of 768).

 %Although 1 attention layer only models do not have this feature (and so by implication do not really ICL according to them), when induction heads are induced in such models, their ICL performance improves.  We have noticed that ICL for ${\cal P}^1$ occurs even with 1 AH and 1 MLP player.