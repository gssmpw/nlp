\section{Related Work}
We emphasize that this paper focuses on the susceptibility and robustness of decentralized learning to low-quality data distributed across the network topology. While this issue may bear some resemblance to adversarial machine learning, our perspective is fundamentally different, as we do not specifically address adversarial attacks. However, given the conceptual overlap, we include, alongside the relevant literature on decentralized learning, a brief overview of the literature on attacks against decentralized learning.

\subsection{Decentralized learning}
Most of the work studying the robustness of decentralized learning against data poisoning mainly focuses on attacks targeting federated learning and trying to compromise the model integrity by sending ad-hoc updates to the parameter server. Federated learning~\cite{mcmahan2017communication}, which currently has some of its applications in the field of user keyboard input prediction~\cite{hard2018federated} and healthcare~\cite{shiranthika2023decentralized}, is a form of decentralized learning where the underlying topology is star-like and the central server coordinates the learning. 

Decentralized learning, more broadly, offers a solution to learn within networks of devices, each holding a unique dataset, where constraints prevent the sharing of raw data across the network. The constraints are usually due to bandwidth limits or privacy requirements. Briefly, each device in the network starts by training a local model for a few epochs on the locally available data. Then, the distilled knowledge, usually the locally trained model, is shared with the neighbors. Finally, each device collects the models received by its neighbors and merges them into a new, updated model. Three steps are outlined: i) local training, ii) synchronization, iii) update. The three phases are then repeated until all devices reach convergence and models do not change anymore. It must be specified that local learning is analogous to the centralized paradigm, with all the techniques available such as early stopping on validation data extracted from the local dataset. Within these settings, learning is still possible even with non-IID data distributions and heterogeneous initialization~\cite{valerio2023coordination}. 

\subsection{Attacks against decentralized learning}

Unlike adversarial attacks that target models at test time~\cite{tabacof2016exploring}, our work focuses on the local training phase within a decentralized learning paradigm. Most prior research has concentrated on attacks against federated learning, along with corresponding countermeasures and detection mechanisms. In contrast, we aim to analyze overall network performance in a fully decentralized, uncoordinated, and potentially data-imbalanced scenario where nodes hold misleading and incorrectly labeled data.

Our scenario aligns closely with the category of \textit{label flip} attacks. In~\cite{bhagoji2019analyzing}, the authors investigated a federated IID setting with a single malicious user who manipulated gradient updates sent to the parameter server. Their approach leveraged either an additional scaling factor or a custom loss function to maximize the stealth and effectiveness of the attack. In~\cite{sun2024gan}, the authors proposed \textit{VagueGAN}, a modified GAN trained on an altered objective function that degrades the generatorâ€™s capabilities. In their experiments, they augmented the local dataset of each malicious node by 10\% using the pre-trained GAN and performed a label flip attack, converting all samples of class 6 to class 0 in the MNIST dataset. Their study examined scenarios with up to 30\% malicious participants.

Similarly, \cite{zhang2020poisongan} introduced \textit{PoisonGAN} to address cases where an attacker lacks access to a pre-trained generator or the necessary data distribution for training it. Their approach leveraged the global model managed by the parameter server as a discriminator to bypass data access limitations. Our work departs from these studies by assuming that nodes have full access to a pre-trained generator, which they use to construct or augment local datasets before training begins . %(further details in Sec.~\ref{subsec:threat_model}).

In~\cite{tolpegin2020data}, the authors conducted an extensive study on label-flip poisoning in federated learning with IID data distributions. They examined the effects of poisoning by deploying multiple attackers at different time steps, specifically targeting selected labels. A comprehensive survey on poisoning attacks in federated learning is provided in~\cite{xia2023poisoning}. Meanwhile,~\cite{pham2024data} explored fully decentralized learning systems by injecting adversarial samples to induce backdoor behavior, forcing the victim model to respond to specific image triggers with predetermined outputs. This type of attack, known as a \textit{backdoor attack}, allows an adversary to embed a hidden objective in the model that can be exploited at test time. In contrast, our approach simply shifts the label of one class to another (specifically, class 4 to class 9) without introducing any additional objectives.

Unlike~\cite{gentz2015detection}, where malicious nodes execute adversarial code, our setting assumes that nodes have control only over their local dataset and do not run any harmful code. Lastly, we extend the ideas presented in~\cite{cao2019understanding}, where the impact of corrupted samples in federated scenarios was analyzed, to the extreme case of a fully decentralized setting with imbalanced data distribution. In our scenario, samples from two easily confusable classes are further corrupted using a generative model and assigned to the wrong class.

To the best of our knowledge, this is the first study to jointly investigate the effects of data corruption on a complex network topology in fully decentralized federated learning.
% Instead of targeting the model at test time as done in adversarial attacks~\cite{tabacof2016exploring}, our work focuses on the local training phase of the decentralized paradigm. Most prior work focuses on attacks against federated learning, countermeasures, and detection mechanisms. Instead, we are interested in analyzing the overall network performance in a decentralized, uncoordinated, and possibly data-unbalanced scenario where nodes hold misleading and wrongly labeled data. Our scenario is close to the \textit{label flip} attacks category. Authors in~\cite{bhagoji2019analyzing} experimented with a unique malicious user in a federated IID scenario. In their work. they leverage either an additional scaling factor or a custom loss for the gradient update sent by the attackers to the parameter server to maximize the stealthiness and effectiveness of the attack. 
% Authors in~\cite{sun2024gan} proposed a modified version of GAN, namely \textit{VagueGAN}, which is trained on a modified objective by lowering the generator capabilities. In their experiments, they augmented the local dataset of each malicious node by a $10\%$ factor using the pre-trained GAN, then executed a label flip over all samples of class $6$ towards class $0$ of MNIST dataset. In their work, they experimented with an increasing number of malicious users, up to $30\%$.~\cite{zhang2020poisongan} proposed \textit{PoisonGAN} to handle scenarios where the attacker has no access to a pre-trained generator or has no access to the data distribution required to train it. Within this scenario, they showed that it is possible to leverage the global model managed by the parameter server to act as a discriminator, thus overcoming the data problem. We move away from that work by assuming that the nodes have full access over a pre-trained generator used to build or augment the local dataset before the learning starts( more details are given in~\ref{subsec:threat_model}). Authors in~\cite{tolpegin2020data} conducted extensive work about label flip poisoning in federated learning with IID data distribution. In their work, they studied the effects of poising by deploying an attacker at different time steps targeting specific labels. A well-detailed survey about federated learning attacks can be found in~\cite{xia2023poisoning}. Authors in~\cite{pham2024data} targeted fully decentralized learning systems by injecting ad-hoc samples to force the victim's model to optimize a hidden objective, namely to respond to specific triggers in the images with constant outputs. This is usually called a \textit{backdoor attack} because an attacker could hide a malicious objective in the model and exploit it at test time with ad-hoc samples. Instead, we shift the output label of a class of samples towards another (namely, class $4$ to class $9$) and do not introduce any additional objective. Within our settings, the nodes do not execute malicious code, as instead assumed in~\cite{gentz2015detection}, and they have control only over their local dataset. Finally, we extended the ideas presented in~\cite{cao2019understanding}, where the authors analyzed the impact of corrupted samples on federated scenarios, to the extreme case of a fully decentralized with unbalanced data distribution across nodes, where samples belonging to two misunderstandable classes are further corrupted via a generative model and assigned to the wrong class. \textcolor{orange}{To the best of our knowledge, this is the first work that jointly investigates the effect of data corruption and the network topology in decentralized, federated learning.}
% This is different from our setup where the attacker can only modify local data and has no control over the learning algorithm. 
% with a poising strategy aiming at achieving a label flip from class $4$ to $9$ and a generative model providing the poisoned samples. 

\begin{figure*}[t]
\includegraphics[width=\linewidth]{res/Dataset_Corruption.png}
\captionof{figure}{The hub's local dataset undergoes corruption at $75\%$ with the corruption strength $\alpha=0.95$. The corrupted images are built using the en }
\label{Fig:dataset_corruption}
\end{figure*}