\section{Experimental Setup}\label{sec:experiments}
\subsection{Downstream Tasks \& datasets}
CARMA is evaluated across two tasks that assess different aspects of compositional generalisation: \textbf{Inverse Dictionary Modelling} for word-level composition and \textbf{Sentiment Classification} for phrase-level structure. These tasks measure systematicity, substitutivity, over-generalisation, and robustness to perturbations.\\
\noindent\textbf{Inverse Dictionary Modelling (IDM)} evaluates a model's ability to generate terms from definitions, focusing on substitutivity in semantic composition. WordNet \cite{miller-1994-wordnet} is used as the training dataset, with an 80-10-10 train-validation-test split. Models are prompted with a definition and tasked with generating the corresponding term (e.g., ``The star around which the Earth orbits is called'' $\rightarrow$ ``Sun''). Performance is assessed using Exact Match Accuracy, which measures whether the generated term precisely matches the expected output. By mapping definitions to terms, this task provides a robust assessment of a model's ability to perform compositional substitution.\\
\noindent\textbf{Sentiment Classification (SC)} assesses the model's ability to infer sentiment from phrases and sentences, particularly focusing on sentiment shifts and over-generalisation. The Stanford Sentiment Treebank (SST) \cite{socher2013recursive} is used with its original dataset splits. Models predict sentiment labels given textual inputs (e.g., ``A brilliant performance sentiment is'' $\rightarrow$ ``positive''). Performance is evaluated using Exact Match Accuracy. This task examines how sentiment composition is preserved across different levels of linguistic structure.
Task formalisation, dataset details, and task selection rationale are in Appendices~\ref{sec:task_selection}, \ref{sec:task_formalisation}, and \ref{sec:dataset_appendix}, respectively.
% Tasks formalisation and dataset details are provided in Appendices~\ref{sec:task_formalisation} and~\ref{sec:dataset_appendix}, respectively.

\subsection{Model Configurations and Baselines}
Experiments are conducted across three model configurations: baseline models, models with task-specific fine-tuning, and models with fine-tuning plus CARMA regularisation. Models use 500 warm-up steps and a 0.006 learning rate. We test GPT-2 (S/L) \cite{radford2019language}, Gemma-2B \cite{team2024gemma}, Llama (1B/3B) \cite{dubey2024llama}, and Qwen (0.5B/3B) \cite{yang2024qwen2}, representing diverse architectures and capacities. CARMA regularisation is generally applied at approximately one-third of the modelâ€™s depth, though specific layer positions vary. Details on fine-tuning methodologies, model specifications, and CARMA hyperparameter selection are provided in Appendix~\ref{sec:fine_tuning_appenix}.

\subsection{Interventions for Compositional Robustness and Performance Stability}
Two interventions are used to evaluate the robustness of compositional structures and the stability of learned representations: Constituent-aware pooling and synonym replacement. These interventions assess hierarchical dependencies and semantic consistency under controlled perturbations.

\noindent\textbf{Constituent-Aware Pooling (CAP)}~\cite{aljaafari2024interpreting} groups token-level representations into higher-level semantic units (e.g., words, syntactic constituents) to assess hierarchical dependencies and how compositional structures are maintained across layers. In this paper, the token-to-word CAP is utilised. Model robustness is measured by monitoring performance metrics before and after applying CAP. Full methodology and formalisation are provided in Appendix~\ref{sec:cap_explanation}.

\noindent\textbf{Synonym Replacement} evaluates semantic consistency by substituting 25\% and 40\% of prompt words with synonyms within an interpretable bound ($\alpha$). Experiments were repeated at least five times with different seeds for robustness and performance stability assessment; further details are in Appendix~\ref{sec:syn_replacement}.  
% \begin{itemize}
%     \item \textbf{Constituent-Aware Pooling (CAP)} groups token-level representations into higher-level semantic units (e.g., words, syntactic constituents) to assess hierarchical dependencies and how compositional structures are maintained across layers. Model robustness is measured by monitoring performance metrics before and after applying CAP. Full methodology and formalisation in Appendix~\ref{sec:cap_explanation} and \cite{aljaafari2024interpreting}.
%     \item \textbf{Synonym Replacement} evaluates semantic consistency by substituting 25\% and 40\% of prompt words with synonyms within an interpretable bound ($\alpha$). Experiments were repeated three times with different seeds for robustness and performance stability assessment. Details in Appendix~\ref{sec:syn_replacement}.    
%     \end{itemize}

\subsection{Experimental setup}
Experiments were conducted using NVIDIA RTX A6000 and A100 GPUs. The method was developed in Python (v3.10.15) with Transformers (v4.44.2) \cite{wolf-etal-2020-transformers}, PyTorch (v2.4.1) \cite{paszke2019pytorch}, and Transformer-lens (v2.8.1) \cite{nanda2022transformerlens}. Preprocessing tasks, including tokenisation and tagging, used NLTK (v3.9.1) \cite{bird2009natural}, spaCy (v3.7.2) \cite{honnibal2020spacy}, and TextBlob (v0.18.0) \cite{loria2018textblob}, with Scikit-learn (v1.5.1) \cite{scikit-learn} for evaluation.

\begin{figure*}
    \centering
    \subfigure[IDM Task]{\label{fig:carma_results_idm}\includegraphics[width=.45\linewidth]{figures/performance_under_cap_and_idm_task.pdf}}
    \subfigure[SC Task]{\label{fig:carma_results_sst}\includegraphics[width=.45\linewidth]{figures/performance_under_cap_and_sc_task.pdf}}
        \caption{Layer-wise performance comparison under CAP intervention, with performance averaged over three protocols (Mean CAP, Max CAP, Sum CAP) for Original, Fine-Tuned (FT), and CARMA (FT + CARMA) models. Layer numbers are normalised to their relative positions within each model to enable cross-architecture comparison. The IDM task (left) highlights CARMA's improvements in systematicity and stability, particularly in the early and middle layers. The SC task (right) demonstrates CARMA's ability to enhance robustness, though convergence with FT occurs in deeper layers.}

    \label{fig:carma_results_combined}
\end{figure*}