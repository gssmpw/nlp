\section{Conclusion}\label{sec:conclusion}
This paper presents CARMA, a method for enhancing compositional generalisation in LLMs through mutual information regularisation and stability constraints. By addressing information fragmentation and layer-wise instability, CARMA improves performance stability and robustness under interventions, as demonstrated through IDM and SC tasks. The method offers a cost-effective solution applicable across model architectures with minimal modifications. Future work should explore extending CARMA to additional tasks that rely more on nuanced semantic features and multilingual settings to further evaluate its scalability and adaptability. Integrating CARMA into improved, targeted transformer architectures for CG could unlock new possibilities for enhancing compositionality.
