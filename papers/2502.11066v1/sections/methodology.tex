\section{Enhanced Compositionality via Advanced Regularisation and Mutual Information Alignment (CARMA)}\label{sec:methodology}
This section formalises compositionality, introduces the core principles of CARMA, and details its components. Figure \ref{fig:CCG} illustrates the CARMA method, highlighting its optimisation process and key components.

\subsection{Compositionality Formalisation}
% \subsubsection*{Compositional Principles}
\paragraph{Mathematical Foundations of Compositionality.} CG (Section~\ref{sec:compositionality}) can be formally defined through a compositional system where $\mathcal{E}$ denotes a set of expressions (e.g., token sequences recognised by the model), and $\mathcal{M}$ represents a corresponding set of meanings. This relationship is formalised as a function:
\begin{equation}\label{eq:cg_system_1}
f: \mathcal{E} \rightarrow \mathcal{M}
\end{equation}
For any complex expression \( e \in \mathcal{E} \), composed of constituent elements \( e_1, \dots, e_n \) according to a syntactic rule \( r \), the function \( f \) satisfies:
\begin{equation}\label{eq:cg_system_2}
f(r(e_1, \dots, e_n)) = g_r(f(e_1), \dots, f(e_n)),
\end{equation}
where $g_r$ is the semantic operation that corresponds to the syntactic rule $r$.
% \subsubsection*{Compositional Generalisation in LLMs}
\paragraph{Compositional Generalisation in LLMs.} Effective CG in LLMs requires generating structured compositions that preserve semantic consistency. Given a novel expression $e_{\text{novel}}$ similar to a known expression $e_{\text{known}}$ within a threshold $\beta$, their semantic functions must remain within an interpretable bound or deviation $\alpha$:
\begin{equation}
% \small
d(e_{\text{novel}}, e_{\text{known}}) \leq \beta \Rightarrow d(f(e_{\text{novel}}), f(e_{\text{known}})) \leq \alpha.
\end{equation}
% This ensures robust generalisation while maintaining logical structure and interpretability. 
This formulation captures systematicity (structured combinations), substitutivity (preservation under transformations), and resistance to over-generalisation (bounded semantic deviation) while maintaining interpretability.


\subsection{CARMA Formalisation}
CARMA operates over a range of target layers, from \( l \) to \( \mathcal{K}\) (\( 0 < l \leq \mathcal{K} \leq L \), where \( L \) is the total number of layers), and consists of two core components: \textbf{Mutual Information} and \textbf{Layer-Wise Stability} Regularisation.


\paragraph{Mutual Information (MI) Regularisation Across Layers.} CARMA preserves essential dependencies and maintains structural coherence by maximising MI between hidden states of related tokens. The MI between hidden states \( h^k_i \) and \( h^k_j \) at layer \( k \), representing two related tokens \( i \) and \( j \), is defined as:
\begin{equation} 
I(h^k_i; h^k_j) = \mathbb{E}_{P(h^k_i,h^k_j)}\left[\log \frac{P(h^k_i,h^k_j)}{P(h^k_i)P(h^k_j)}\right]
\end{equation}
Since exact computation is intractable, MI is approximated using the InfoNCE loss \cite{oord2018representation}, encouraging token-level dependencies across the same layers:
\begin{equation}
\small
\begin{aligned}
    \mathcal{L}_{\text{MI}} = - \frac{1}{N} \sum_{k=l}^{\mathcal{K}} \sum_{i=1}^Q \Bigg( 
    &\log \sum_{\substack{h_j \in \mathcal{H}^k \\ j \neq i}} \exp\left(\frac{f(h_i^k, h_j^k)}{\tau}\right) \\
    &- \log \Bigg( \sum_{\substack{h_j \in \mathcal{H}^k \\ j \neq i}} \exp\left(\frac{f(h_i^k, h_j^k)}{\tau}\right) \\
    &\quad + \sum_{h_m \in \mathcal{N}^k} \exp\left(\frac{f(h_i^k, h_m)}{\tau}\right) \Bigg) \Bigg),
\end{aligned}
\end{equation}
where \( f(h_i^k, h_j^k) \) is a similarity function quantifying the relationship between hidden states at layer \( k \), \( \mathcal{H}^k \) denotes the set of positive examples related to \( h_i^k \), \( \mathcal{N}^k \) is the set of negative examples unrelated to \( h_i^k \) at layer \( k \), \( \tau \) is the temperature parameter, and \( N \) is the total number of target layers from \( l \) to \( \mathcal{K} \), with \( Q \) representing the number of tokens or samples used per layer. Further details on MI approximation are provided in Appendix~\ref{sec:infoNCE_MI}.

\paragraph{Layer-Wise Stability Regularisation.}
This component enforces smooth transitions across layers, reducing abrupt changes that could disrupt compositional structures. For a layer \( k \), the Layer-Wise Stability Loss is defined as:
\begin{equation}
\small
\mathcal{L}_{\text{Stability}} = \sum_{k=l}^{\mathcal{K}} \mathbb{E} \left[ \frac{\left| f^{(k+1)}(X) - f^{(k)}(X) \right|^2}{\mathbb{E}\left[ \left| f^{(k)}(X) \right|^2 \right] + \mathbb{E}\left[ \left| f^{(k+1)}(X) \right|^2 \right] + \epsilon} \right],
\end{equation}
where \( f^{(k)}(X) \) denotes the activation output at layer \( k \), and \( \epsilon \) is a small positive constant to ensure numerical stability (e.g., \( \epsilon = 10^{-8} \)). Minimising this loss preserves compositional integrity across the specified layers by encouraging smooth and consistent transitions between them, thereby enabling more stable information flow and aggregation within this range.

\paragraph{CARMA Loss.} CARMA integrates $\mathcal{L}_{\text{MI}}$ and $\mathcal{L}_{\text{Stability}}$ into its total loss as:
\begin{equation}\label{eq:carma_loss}
\mathcal{L}_{\text{CARMA}} = \gamma \mathcal{L}_{\text{MI}} + \eta \mathcal{L}_{\text{Stability}},
\end{equation}
where \( \gamma \) and \( \eta \) are hyperparameters in \( [0, 2] \) that control the relative contribution of each component. The final optimisation objective balances task-specific performance with CARMAâ€™s regularisation as:
\begin{equation}
    \mathcal{L}_{\text{total}} = (1 - \lambda) \cdot \mathcal{L}_{\text{task}} + \lambda \cdot \mathcal{L}_{\text{CARMA}},
\end{equation}
where \( \mathcal{L}_{\text{task}} \) represents the task-specific loss, \( \mathcal{L}_{\text{CARMA}} \) is the regularisation loss, and \( \lambda \in [0, 2] \) controls the trade-off between task accuracy and compositional robustness.
