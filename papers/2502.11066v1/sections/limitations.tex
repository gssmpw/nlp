\section*{Limitations}
The limitations of this paper can be summed up as follows: First, our results are primarily reported for the English language. Further analysis across languages with diverse linguistic structures is left as a confirmatory future work. Second, the datasets (WordNet and SST) lack a more comprehensive representativeness of broader linguistic phenomena. Third, our focus is predominantly on decoder-based Transformers. Finally, the employed Transformer models may inherit potential biases ingrained from their pre-training data.

\section*{Ethical statement}
This work aims to enhance language model robustness and compositional understanding through CARMA. While improving model reliability is beneficial, we acknowledge potential risks in enhancing language model capabilities. Our evaluation focuses on controlled tasks (IDM and SC) with comprehensive stability metrics to ensure responsible development and transparent reporting of model behaviour under perturbations.

