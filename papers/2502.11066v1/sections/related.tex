\section{Related work}\label{sec:related}
Research on CG in LLMs has revealed both capabilities and limitations \cite{tull2024towards, moisio-etal-2023-evaluating, sinha2024survey}, though many studies lack mechanistic analysis or concrete suggestions for improvements.

\noindent\textbf{Architectural modifications} are a common approach to tackle CG challenges. Recent proposals include pushdown layers for recursive attention \cite{murty-etal-2023-pushdown}, Layer-wise Representation Fusion for dynamic encoder weighting \cite{lin-etal-2023-learning}, and specialised semantic parsing methods \cite{shaw-etal-2021-compositional}. While effective for specific tasks, these solutions face scalability challenges due to computational overhead, specialised annotation requirements, and architectural constraints.

% \noindent\textbf{Architectural modifications} are a common approach to CG challenges. \cite{murty-etal-2023-pushdown} introduced \textit{pushdown layers} to enhance attention and capture compositional structures using recursion-based mechanisms. \cite{lin-etal-2023-learning} proposed Layer-wise Representation Fusion (LRF), which aggregates and dynamically weights encoder representations before passing them to the decoder. Efforts in semantic parsing \cite{shaw-etal-2021-compositional} and factorised alignment techniques \cite{russin-etal-2020-compositional} improve the handling of compositional structures. While these methods achieve task-specific gains, their scalability and generalisability remain limited due to reliance on tailored architectures, high computational costs, or specialised annotations.

\noindent\textbf{Regularisation methods} provide alternative approaches through consistency regularisation \cite{yin-etal-2023-consistency}, data augmentation strategies \cite{ontanon-etal-2022-making}, and attention stability mechanisms \cite{zhai2023stabilizing}. Studies show dataset complexity and example frequency variations improve compositional reasoning \cite{zhou-etal-2023-data}. However, these methods face key limitations: token-level approaches lack adaptability to complex structures, augmentation shows diminishing returns on real data, and stability mechanisms prioritise training stability over compositional generalisation.

% \noindent\textbf{Regularisation techniques} offer an alternative direction. \cite{yin-etal-2023-consistency} proposed consistency regularisation using contrastive learning and dropout-based perturbations, though its token-level focus restricts adaptability. \cite{ontanon-etal-2022-making} employed data augmentation strategies but demonstrated diminishing returns in real-world datasets. Recent work \cite{zhou-etal-2023-data} suggests that increasing dataset complexity and varying example frequencies improve compositional reasoning and generalisation. Other works explore regularisation through attention stability \cite{zhai2023stabilizing}, though these are more tailored toward training stability than CG.

\noindent\textbf{Evaluation challenges} persist in CG research. Standard benchmarks like SCAN \cite{Lake2017GeneralizationWS}, PCFG \cite{ijcai2020p708}, and COGS \cite{kim-linzen-2020-cogs} rely heavily on synthetic data, limiting real-world applicability. Recent frameworks like CoGnition \cite{li-etal-2021-compositional} and CAP \cite{aljaafari2024interpreting} better align with natural language phenomena, but evaluation gaps remain. Current approaches often sacrifice generalisability for task-specific performance. \textit{CARMA} addresses these limitations through a \textit{task-agnostic, efficient solution} that enhances CG while maintaining robust cross-task performance.

% \noindent\textbf{The evaluation of CG} presents certain challenges. Benchmarks such as SCAN \cite{Lake2017GeneralizationWS}, PCFG \cite{ijcai2020p708}, and COGS \cite{kim-linzen-2020-cogs} are commonly used to assess compositionality but largely rely on synthetic or semi-synthetic datasets. While these benchmarks are helpful for controlled evaluations, they often fall short of representing the complexities of natural language. Efforts like CoGnition \cite{li-etal-2021-compositional} provide a step forward by introducing a real-world dataset specifically designed for compositional generalisation testing. Additionally, CAP \cite{aljaafari2024interpreting} offers a promising direction by aligning compositional tasks more closely with real-world linguistic phenomena. Expanding on such initiatives, the development of diverse and realistic benchmarks holds the potential to bridge existing gaps in evaluation.\\

% Despite advancements, substantial gaps remain. Existing solutions often prioritise specific tasks over generalisation or fail to address architectural limitations. \textit{CARMA}, bridges these gaps with a \textit{task-agnostic, cost-efficient, and balanced solution}, enhancing CG while ensuring robust performance across diverse tasks.
