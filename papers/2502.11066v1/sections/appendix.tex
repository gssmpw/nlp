\appendix
\section{Task Selection and Compositionality Considerations}\label{sec:task_selection}
To assess compositional generalisation and the benefits of CARMA, we targeted tasks that involve systematic meaning construction and sensitivity to structural modifications. To that end, we opted to employ  Inverse Dictionary Modelling (IDM) and Sentiment Classification (SC) as proxies for different dimensions of compositionality, capturing both structured composition and hierarchical generalisation.

IDM requires models to generate a single-word representation from a natural language definition, mapping from the composition of input constituents (individual concept components) to a specific term. On the other hand, SC maps meaning to a sentiment label, aggregating local meaning elements into a global interpretation. While IDM focuses on explicit compositional mapping, SC evaluates distributed composition, where sentiment is shaped by multiple interacting components. 

Both tasks assess several aspects of compositionality (Figure~\ref{fig:task_comps}), namely systematicity (structured meaning formation), substitutivity (semantic preservation under transformation), and resistance to over-generalisation (ensuring bounded semantic deviation). Further, they evaluate robustness, testing whether models can maintain correctness and consistency under internal and input-lexical perturbations. IDM and SC provide a comprehensive test of compositional generalisation across structured and distributed representations.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/tasks_selection_compositionality.pdf}
    \caption{Illustration of compositional generalisation in Inverse Dictionary Modelling (IDM) and Sentiment Classification (SC). The figure highlights key compositional properties: systematicity ensures coherent meaning construction, substitutivity maintains meaning under lexical variations, robustness preserves intended outputs under perturbations, and over-generalisation leads to overly broad or semantically weak predictions (e.g., neuron misclassified as cell or positive reduced to neutral).}
    \label{fig:task_comps}
\end{figure}


\section{Detailed Experimental Configuration}
\subsection{Task Formalisation}\label{sec:task_formalisation}
This paper evaluates the effectiveness of CARMA in enhancing the compositional generalisation of large language models (LLMs) through two tasks. These tasks were selected based on their focus on input token structure and compositional semantics, utilising next-token prediction with single-token outputs. Formal definitions for each task are presented below.
\begin{figure*}
    \centering
    % First row
    \subfigure[GPT2-S]{
        \includegraphics[width=0.48\textwidth]{figures/GPT2_idm.pdf}
    }
    \subfigure[GPT2-L]{
        \includegraphics[width=0.48\textwidth]{figures/GPT2_L_idm.pdf}
    } \\
    % Second row
    \subfigure[Gemma-2B]{
        \includegraphics[width=0.48\textwidth]{figures/Gemma_2B_idm.pdf}
    }
    \subfigure[Qwen-0.5B]{
        \includegraphics[width=0.48\textwidth]{figures/Qwen_0_5B_idm.pdf}
    } \\
    \subfigure[Llama-1B]{
        \includegraphics[width=0.48\textwidth]{figures/Llama_1B_idm.pdf}
    }
    % This row
    \subfigure[Llama-3B]{
        \includegraphics[width=0.48\textwidth]{figures/Llama_3B_idm.pdf}
    }
    \caption{IDM Performance Across Models Under CAP}
    \label{fig:idm_models}
\end{figure*}

\noindent\paragraph{Inverse Definition Modelling (IDM).}  
This task requires the model to predict a definiendum \(D\), given its corresponding definition \(\text{definition}\) in natural language. Formally, the definition is represented as a sequence of tokens, \(\text{definition} = \{\text{tok}_1, \text{tok}_2, \dots, \text{tok}_n\}\), and the model seeks to produce \(D\) such that:  
\begin{equation}
    D = \arg\max_{t \in \mathcal{V}} P(d \mid \text{definition}),
\end{equation} 
where \(\mathcal{V}\) denotes the model's vocabulary, and \(d\) represents a potential definiendum. Predictions are deemed correct only if they exactly match the target output.

\paragraph{Sentiment classification (SC).} 
This task involves assigning a sentiment label to a given sentence containing sentiment cues and potential modifiers. The model processes the input \(\text{sentence}\), represented as a sequence of tokens \(\text{sentence} = \{\text{tok}_1, \text{tok}_2, \dots, \text{tok}_n\}\), and produces an output \(\text{label}\) from a predefined set of sentiment classes \(\mathcal{A}\) (i.e., \textit{positive}, \textit{negative}, \textit{neutral}). Formally, the task is defined as:  
\begin{equation}
\text{label} = \arg\max_{\ell \in \mathcal{L}} P(\ell \mid \text{sentence}),
\end{equation}  
where \(P(\ell \mid \text{sentence})\) is the probability of the sentiment label \(\ell\) given the sentence. The model's performance is evaluated based on its ability to correctly predict the sentiment, accounting for compositional nuances such as modifiers and contrasts.  

\subsection{Datasets specification and pre-processing}\label{sec:dataset_appendix}
For IDM, the training and test datasets were derived from WordNet \cite{fellbaum1998wordnet}, a widely used lexical database of the English language. WordNet comprises over 117,000 synsets, each representing a distinct concept and annotated with semantic relationships such as hypernyms, synonyms, and definitions. To ensure consistency and improve data quality, standard preprocessing techniques were applied, including the removal of special characters, punctuation, extra spaces, and parenthesised content where necessary. The dataset focuses on general-purpose vocabulary rather than specialised domains or demographic groups. The dataset was initially split into an 80-20 ratio, with 80\% allocated for training. The remaining 20\% was further divided equally into validation and test sets. 

The SC dataset was derived from the Stanford Sentiment Treebank (SST) \cite{socher2013recursive}, a corpus of English movie reviews annotated for analysis of the compositional effects of sentiment inference and was released under Apache License, Version 2.0. SST includes fine-grained sentiment labels at both the phrase and sentence levels, making it a standard benchmark for evaluating sentiment classification models. The original dataset splits provided by the authors were maintained to ensure consistency in training, validation, and testing. For SST labels, sentiment scores were categorised as follows: values equal to or greater than 0.6 were classified as positive, scores between 0 and 0.6 were considered neutral, and scores below zero were assigned as negative. The final test dataset sizes for each task are presented in Table~\ref{tab:test_set_sizes}.


\begin{table}[h]
\centering
\small
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Train size} & \textbf{validation Size} & \textbf{Test Size} \\
\hline
WordNet & 9563 & 1154 & 1231\%  \\

\hline
SST & 8544 & 1101 & 2210 \\
\hline
\end{tabular}
\caption{Train, validation, and test set sizes for WordNet and SST datasets used in this paper.}
\label{tab:test_set_sizes}
\end{table}



\subsection{Model training and fine-tuning settings}\label{sec:fine_tuning_appenix}
Table \ref{tab:model_properties} summarises the key characteristics of the models evaluated in this study. All models were obtained from Hugging Face \cite{wolf2019huggingface} under their respective licenses: GPT-2 (Modified MIT), Llama 3.2 (Meta Llama 3 Community), Qwen 2.5 (Apache 2.0), and Gemma-2B (Gemma Terms of Use). While all models were pre-trained on English data, LLama and Qwen models provide additional multilingual capabilities, namely English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai for LLama, and over 10 languages, including Chinese, English, French, Spanish, Portuguese, Russian, Arabic, Japanese, Korean, Vietnamese, Thai, and Indonesian for Qwen. The models employ the following tokenisation approaches: GPT-2, Byte Pair Encoding (BPE) with a 50,257-token vocabulary, optimised primarily for English, Llama 3.2 uses SentencePiece-based BPE, combining 100K tokens from Tiktoken3 with 28K additional tokens to enhance multilingual performance, Qwen 2.5 employs Byte-level BPE, utilising a 151,643-token vocabulary designed for multilingual processing, Gemma-2B has a SentencePiece tokeniser leveraging a 256,000-token vocabulary, making it highly effective for English-based tasks. Each model was fine-tuned on its respective downstream task following a systematic hyperparameter search to identify optimal configurations.
Prior to fine-tuning, prompt engineering was conducted to determine well-performing prompts tailored to each task, ensuring alignment with task-specific requirements and enhancing the models' ability to generate accurate and contextually relevant outputs. The hyperparameter search explored key factors, including weights for stability regularisation, mutual information (MI) regularisation, and the overall CARMA weight (Equation \ref{eq:carma_loss}), as well as the specific layers to which these losses were applied.

For training parameters, the following batch sizes were set in the IDM task: 16 for the Gemma-2B and GPT models, 32 for the Qwen-3B and Llama models, and 64 for the Qwen-0.5B model. For SC, the batch sizes were 16 for the GPT models, Gemma-2B and Llama-3B; 32 for Llama-1B and Qwen-3B; and 64 for Qwen-0.5B. For the number of training epochs, in the IDM, the Gemma and GPT models were trained for two epochs, while all other models were trained for three epochs, whereas all models were trained for two epochs, except Gemma-2B and LLama-1B, which were trained for three epochs for the SC task. The stopping layers for IDM and CARMA were configured as follows: GPT2-S at layer 3, GPT2-L at layer 8, Gemma-2B at layer 10, Llama-1B at layer 7, Llama-3B at layers 8 (stability) and 12 (MI), Qwen-0.5B at layer 5, and Qwen-3B at layer 10. The SC, the ending layers, 4 for GPT2-S, 12 for GPT2-L, 10 for Gemma-2B, 7, for LLama 1B, 8, for LLama 3B, 5 for Qwen-0.5B and 7 for Qwen-3B. For CARMA weight, optimal values varied by model size: 0.4 and 0.5 were most effective for larger models. We hypothesise that CARMA regularisation exhibits a weaker effect when lower weights are applied, particularly in larger architectures where stronger constraints are needed to stabilise compositional representations. In IDM, GPT2-L and Gemma performed best with a weight of 0.3, GPT2-S with 0.2, Llama-1B with 0.4, and Llama-3B with 0.5. Qwen models used 0.5 and 0.4 for the 0.5B and 3B variants, respectively. For SC Carma weight, it was 0.4 for Qwen-0.5B and GPT models, 0.5 for LLama-3B and Qwen-3B, and 0.3 for the rest. For the ending layer, it was 4 for GPT2-S, 12 for GPT2-L, 10 for Gemma-2B, 7 for LLama-1B, 8 for LLama-3B, 5 for Qwen-0.5B and 7 for Qwen-3B. 


\begin{table}[ht]
\centering
\renewcommand{\arraystretch}{1.2} % Increases row height for better readability
\setlength{\tabcolsep}{6pt} % Adjusts column spacing
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Parameters} & \textbf{Layers} & \textbf{D\textsubscript{model}} & \textbf{Heads} & \textbf{Activation} & \textbf{MLP Dimension} \\
\hline
GPT-2 Small & 85M  & 12  & 768   & 12  & GELU & 3072  \\
% GPT-2 Medium & 302M & 24  & 1024  & 16  & GELU & 4096  \\
GPT-2 Large  & 708M & 36  & 1280  & 20  & GELU & 5120  \\
Gemma-2B  & 2B   & 32  & 4096  & 16  & GELU & 8192  \\
LLaMA3.2 1B  & 1.1B & 16  & 2048  & 32  & SiLU & 8192  \\
LLaMA3.2 3B  & 3.2B & 28  & 3072  & 24  & SiLU & 8192  \\
% LLaMA 8B  & 7.8B & 32  & 4096  & 32  & SiLU & 14336  \\
% LLaMA 8B (Instruct) & 7.8B & 32  & 4096  & 32  & SiLU & 14336  \\
Qwen2.5-0.5B  & 391M  & 24  & 896   & 14  & SiLU & 4864  \\
% Qwen2.5-1.5B  & 1.4B  & 28  & 1536  & 12  & SiLU & 8960  \\
Qwen2.5-3B    & 3.0B  & 36  & 2048  & 16  & SiLU & 11008  \\
\hline
\end{tabular}%
}
\caption{Summary of model architectures. \textbf{Parameters}: total number of trainable parameters; \textbf{Layers}: total number of transformer layers; \textbf{D\textsubscript{model}}: size of word embeddings and hidden states; \textbf{Heads}: number of self-attention heads; \textbf{Activation}: activation function used in feedforward layers; \textbf{MLP Dimension}: dimensionality of the feedforward network.}
\label{tab:model_properties}
\end{table}

\subsection{Evaluation Metrics}\label{sec:evaluation_metrics}
This section details the evaluation metrics used in the study, including accuracy, synonym consistency, and performance stability.

\paragraph{Accuracy} is used as a primary measure of model performance and is defined as:

\begin{equation}
    \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN},
\end{equation}

\noindent where $TP$ (true positives) and $TN$ (true negatives) denote correctly classified instances, while $FP$ (false positives) and $FN$ (false negatives) represent misclassified instances.

\paragraph{Synonym Consistency (ConsistSyn) (\text{ConsistSyn})} quantifies a model's ability to maintain correct predictions after synonym replacement. It is computed as:

\begin{equation}
    \text{ConsistSyn} = \frac{|\text{Correct After Replacement}|}{|\text{Correct Before Replacement}|} \times 100,
\end{equation}

\noindent where $\text{Correct After Replacement}$ refers to the number of correct predictions following synonym substitution, and $\text{Correct Before Replacement}$ denotes the number of correct predictions before substitution. The reported results are the averaged \text{ConsistSyn} across ($ N\geq 5$) runs. 

\paragraph{Coefficient of Variation (CV)}  
The coefficient of variation (CV) measures the stability of model performance across multiple runs, with lower values indicating greater consistency. It is defined as:

\begin{equation}
    \text{CV} = \frac{\sigma}{\mu},
\end{equation}

\noindent where $\sigma$ represents the standard deviation of model performance across runs, and $\mu$ denotes the mean performance.


\paragraph{Normalised Improvement (NI)}  
Normalised Improvement (\text{NI}) evaluates the relative gain in consistency introduced by a model over a baseline model. It is calculated as:

\begin{equation}
    \text{NI} = \frac{\text{ConsistSyn}_{\text{CARMA}} - \text{ConsistSyn}_{\text{baseline}}}{\text{ConsistSyn}_{\text{baseline}}} \times 100.
\end{equation}

\noindent This metric captures the percentage improvement in synonym consistency due to a model variant compared to the baseline model.


\section{Comprehensive Explanation of Evaluation Interventions}
\subsection{Constituent-Aware Pooling (CAP) Formalisation}\label{sec:cap_explanation}
Constituent-Aware Pooling (CAP) Formalisation is a method proposed in \cite{aljaafari2024interpreting} to systematically assess compositional generalisation via aggregating token-level activations into higher-level semantic representation. Below is a detailed explanation and formalisation of CAP. 
\paragraph{Overview.} CAP aggregates model activations at any chosen constituency level (e.g. tokens to words), enabling the analysis of compositional dependencies. The key steps involved are:
\begin{itemize}
    \item \textbf{Input Representations:} For a given input sequence \( X = [x_1, x_2, \ldots, x_n] \), the model produces inner states \( H = [h_1, h_2, \ldots, h_n] \) at a specific layer.  

    \item \textbf{Grouping Constituents:} Using syntactic parsers such as Benepar \cite{kitaev-etal-2019-multilingual, kitaev-klein-2018-constituency}, or by inversing the model tokeniser function, the sequence is segmented into constituents \( C = [c_1, c_2, \ldots, c_m] \), where each \( c_i \) represents a phrase or syntactic unit. For the experiments presented in the paper, tokens were grouped into words to form the smallest linguistic units.  

    \item \textbf{Pooling Operations:} For each constituent \( c_i \), the corresponding activations \( \{h_j | x_j \in c_i\} \) are aggregated into a single representation \( r_i \) using a pooling function:
    \[
    r_i = \alpha(\{h_j | x_j \in c_i\}) 
    \]
    CAP supports three pooling functions:
    \begin{itemize}
        \item \textbf{Maximum pooling:} Selects the highest activation values as: \[ \alpha(\{h_j | x_j \in c_i\}) = \max(\{h_j | x_j \in c_i\}), \quad\]
        \item \textbf{Mean pooling:} Computes the average of activation values as: \[\alpha(\{h_j | x_j \in c_i\}) = \frac{1}{|c_i|} \sum_{j \in c_i} \{h_j | x_j \in c_i\}, \quad\]
        \item \textbf{Sum pooling:} Accumulates activation values as: \[\alpha(\{h_j | x_j \in c_i\}) = \sum_{j \in c_i} \{h_j | x_j \in c_i\}.\]
    \end{itemize}

    \item \textbf{Updating Representations:} The pooled representations \( R = [r_1, r_2, \ldots, r_m] \) replace the original activations \( H \) for further processing.  
\end{itemize}

\begin{figure*}[h!]
    \centering
    % First row
    \subfigure[GPT2-S]{
        \includegraphics[width=0.48\textwidth]{figures/GPT2_sst.pdf}
    }
    \subfigure[GPT2-L]{
        \includegraphics[width=0.48\textwidth]{figures/GPT2_L_sst.pdf}
    } \\
    % Second row
    \subfigure[Gemma-2B]{
        \includegraphics[width=0.48\textwidth]{figures/Gemma_2B_sst.pdf}
    }
    \subfigure[Qwen-0.5B]{
        \includegraphics[width=0.48\textwidth]{figures/Qwen_5M_sst.pdf}
    } \\
    \subfigure[Llama-1B]{
        \includegraphics[width=0.48\textwidth]{figures/LLama_1B_sst.pdf}
    } 
    \subfigure[Llama-3B]{
        \includegraphics[width=0.48\textwidth]{figures/LLama_3B_sst.pdf}
    }
    \caption{SC Performance Across Models Under CAP}
    \label{fig:sst_models}
\end{figure*}

\paragraph{Evaluation.} The impact of CAP is evaluated by comparing task-specific performance metrics (e.g., accuracy, F1 score) of models before and after CAP is applied. This allows for a direct assessment of how CAP affects compositionality and task performance. This paper utilises the word-level CAP, pooling related token representation to their corresponding words. 
\begin{table}[h]
\centering
\small
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Ver.} & \textbf{Task} & \textbf{Int.} & \textbf{CS} & \textbf{CV} \\
\hline
\multirow{6}{*}{GPT2-S} 
& CARMA & IDM & 25\% & 49.17 & 0.025 \\
& FT & IDM & 25\% & 50.89 & \textbf{0.017} \\
& Org & IDM & 25\% & \textbf{52.46} & 0.044 \\
\cline{2-6}
& CARMA & IDM & 40\% & 35.90 & \textbf{0.0542 }\\
& FT & IDM & 40\% & 37.16 & 0.0628 \\
& Org & IDM & 40\% & \textbf{37.20} & 0.1223 \\
\hline


\multirow{6}{*}{GPT2-L} 
& CARMA & IDM & 25\% & 56.31 & \textbf{0.0164} \\
& FT & IDM & 25\% & \textbf{56.95} & 0.0311 \\
& Org & IDM & 25\% & 51.10 & 0.1175 \\
\cline{2-6}
& CARMA & IDM & 40\% & 43.56 & {0.0485} \\
& FT & IDM & 40\% & \textbf{43.97} & \textbf{0.0459} \\
& Org & IDM & 40\% & 34.68  & 0.0895 \\
\hline


\multirow{6}{*}{Gemma-2B} 
& CARMA & IDM & 25\% & 56.70 & \textbf{0.023} \\
& FT & IDM & 25\% & \textbf{57.42} & 0.030 \\
& Org & IDM & 25\% & 49.47 & 0.031 \\
\cline{2-6}
& CARMA & IDM & 40\% & 0.4236 & \textbf{0.0174} \\
& FT & IDM & 40\% & \textbf{0.4498} & 0.0249 \\
& Org & IDM & 40\% & 0.3576 & 0.0480 \\
\hline

\multirow{6}{*}{Llama-1B} 
& CARMA & IDM & 25\% & \textbf{58.40 }& 0.0400 \\
& FT & IDM & 25\% & 57.86 &  \textbf{0.0385}\\
& Org & IDM & 25\% & 47.55 & 0.0503 \\
\cline{2-6}
& CARMA & IDM & 40\% & \textbf{47.07} & 0.0476 \\
& FT & IDM & 40\% & 46.75 & 0.0455 \\
& Org & IDM & 40\% & 33.49 & \textbf{0.0391} \\
\hline

\multirow{6}{*}{Qwen-0.5B} 
& CARMA & IDM & 25\% & \textbf{56.98} & 0.0286 \\
& FT & IDM & 25\% & 54.57 & \textbf{0.0191} \\
& Org & IDM & 25\% & 46.84 & 0.0684 \\
\cline{2-6}
& CARMA & IDM & 40\% & \textbf{40.55} & \textbf{0.0397} \\
& FT & IDM & 40\% & 39.69 & 0.0491 \\
& Org & IDM & 40\% & 32.98 & 0.0938 \\
\hline



\multirow{6}{*}{Qwen-3B} 
& CARMA & IDM & 25\% & \textbf{62.00} & \textbf{0.0225} \\
& FT & IDM & 25\% & 61.79 & 0.0279 \\
& Org & IDM & 25\% & 49.37& 0.0441 \\
\cline{2-6}
& CARMA & IDM & 40\% & 45.05 & \textbf{0.0400} \\
& FT & IDM & 40\% & \textbf{45.74} & 0.0551 \\
& Org & IDM & 40\% & 31.95 & 0.0688 \\
\hline


\multirow{6}{*}{Llama-3B} 
& CARMA & IDM & 25\% & \textbf{62.86} & \textbf{0.015} \\
& FT & IDM & 25\% & 62.22 & 0.029 \\
& Org & IDM & 25\% & 52.47 & 0.035 \\
\cline{2-6}
& CARMA & IDM & 40\% &\textbf{ 49.05 }& 0.0297 \\
& FT & IDM & 40\% & 48.31& \textbf{0.0191} \\
& Org & IDM & 40\% & 36.95 & 0.0458 \\

\hline
\end{tabular}
\caption{Model performance (25\% and 40\% synonym intervention) on the IDM task. \textbf{Ver.}: Version; \textbf{Int.}: Intervention rate; \textbf{CS}: ConsistSyn (\%); \textbf{CV}: Coefficient of Variation. \textbf{Best values in bold.}}
\label{tab:synonym_results_appendix_IDM}
\end{table}

\subsection{Synonym Replacement}\label{sec:syn_replacement}
A multi-step approach was adopted to ensure reliable synonym replacements. First, preprocessing was applied to filter out words that were unlikely to produce meaningful replacements. Specifically, words belonging to NLTK's predefined stopwords list or shorter than two characters were excluded from consideration. The remaining words were tagged with their part-of-speech (POS) using spaCy's \cite{honnibal2020spacy} POS tagger. Additionally, the sentiment of each word was determined using TextBlob \cite{loria2018textblob} to ensure that replacements preserved the semantic tone of the original text.
Next, a synonym vocabulary was constructed using words extracted from spaCy's $en\_core\_web\_md$ language model. This vocabulary was filtered to include only alphabetic common words with high probability scores (greater than -15 in our case), as determined by spaCy's word frequency data, while stopwords and rare terms were excluded. This step ensured that the vocabulary consisted of meaningful and contextually appropriate words for replacement.
For each target word, a list of synonym candidates was generated by iterating over the constructed vocabulary. The top $n$ candidates were selected based on their semantic similarity to the original word, measured using spaCy's word vectors. Synonyms with high similarity scores and alignment in POS were prioritised to maintain grammatical and contextual coherence in the text.

\section{InfoNCE for Mutual Information Estimation}\label{sec:infoNCE_MI}
Mutual information (MI) quantifies the shared information between two variables \(X\) and \(Y\). CARMA leverages MI maximisation to capture dependencies between tokens effectively, thereby enhancing compositional generalisation in LLMs. Specifically, CARMA uses MI, denoted as \(I(X; Y)\), to reinforce token-level interactions critical for compositionality. However, direct computation of MI is challenging in practice.

To address this challenge, a variant of InfoNCE is employed to estimate MI and approximate these dependencies efficiently. Given an anchor token hidden state \(h_i\), we construct a corresponding positive set \(\mathbf{H}\), which contains tokens hidden states semantically or syntactically related to \(h_i\). Additionally, we define \(\mathcal{N}\) as the set of negative examples consisting of unrelated tokens hidden states.

The InfoNCE objective provides a practical lower bound on \(I(X; Y)\) \cite{oord2018representation}, as follows:
\begin{equation}
\small
    I(X; Y) \geq \mathbb{E} \left[ \log \frac{\sum_{h_j \in \mathbf{H}} f(h_i, h_j)}{\sum_{h_j \in \mathbf{H}} f(h_i, h_j) + \sum_{h_k \in \mathcal{N}} f(h_i, h_k)} \right],
\end{equation}
where \(f(h_i, h_j) = \exp(\text{sim}(h_i, h_j) / \tau)\) is a scaled similarity function, and \(\tau\) is a temperature parameter. This adaptation of InfoNCE introduces token-specific interactions within the layer-wise structure of LLMs, ensuring that dependencies are captured across layers. By maximising mutual information, CARMA aligns the optimisation direction to enhance compositional structures.

To extend this approach across layers, the final CARMA MI loss is computed as:
\begin{equation}
\small
\begin{aligned}
    \mathcal{L}_{\text{MI}} = - \frac{1}{N} \sum_{i=1}^N \Bigg( 
    &\log \sum_{\substack{h_j \in \mathbf{H} \\ j \neq i}} \exp\left(\frac{\text{sim}(h_i, h_j)}{\tau}\right) \\
    &- \log \Bigg( \sum_{\substack{h_j \in \mathbf{H} \\ j \neq i}} \exp\left(\frac{\text{sim}(h_i, h_j)}{\tau}\right) \\
    &\quad + \sum_{h_k \in \mathcal{N}} \exp\left(\frac{\text{sim}(h_i, h_k)}{\tau}\right) \Bigg) \Bigg),
\end{aligned}
\end{equation}
where \(h_i\) is the anchor token, \(h_j \in \mathbf{H}\) are positive examples related to \(h_i\), \(h_k \in \mathcal{N}\) are negative examples, \(N\) is the number of anchors, and \(\text{sim}(h_i, h_j)\) is a similarity function. The negative sign ensures that MI is maximised during optimisation. Without this negative sign, the objective would incorrectly minimise MI, thereby hindering CG enhancement.


\section{Extended results}\label{sec:additiona_results}
Figures~\ref{fig:idm_models} and \ref{fig:sst_models}, and Tables~\ref{tab:synonym_results_appendix_IDM} and~\ref{tab:synonym_results_appendix_SC} provide additional results for models' performance comparison under CAP and synonym interventions. 
CARMA models show a clear advantage over all models and tasks. However, the gain is clearer in the IDM case, where more intricate features and compositionality generalisation are required. It is also observed that the performance of the FT and CARMA models demonstrates similar curves or trends. Given this observation, we argue that CARMA's improvements stem from its learning objectives, which align closely with cross-entropy loss while explicitly addressing intermediate representation stability. The observed improvements are moderate in some cases, particularly for SC tasks. This behaviour is expected due to the limited size of the fine-tuning datasets compared to the original pretraining data used for these models. Nevertheless, larger models, such as Llama-3B and Gemma-2B, exhibit more substantial improvements with CARMA, demonstrating its scalability with model capacity.



\begin{table}[h]
\centering
\small
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Ver.} & \textbf{Task} & \textbf{Int.} & \textbf{CS} & \textbf{CV} \\
\hline
\multirow{5}{*}{GPT2-S} 
& CARMA & SC & 25\% & 89.03 & 0.8903 \\
& FT & SC & 25\% & \textbf{89.54} & \textbf{0.8954} \\
\cline{2-6}
& CARMA & SC & 40\% &84.95 & \textbf{0.0095} \\
& FT & SC & 40\% & \textbf{85.07} & 0.0098 \\
\hline


\multirow{5}{*}{GPT2-L} 
& CARMA & SC & 25\% & \textbf{88.58} & \textbf{0.0065} \\
& FT & SC & 25\% & 88.04 & 0.0082 \\
\cline{2-6}
& CARMA & SC & 40\% & \textbf{84.61} & \textbf{0.0072} \\
& FT & SC & 40\% & 84.04 &0.0073  \\
\hline


\multirow{6}{*}{Gemma-2B} 

& CARMA & SC & 25\% & \textbf{84.81} & \textbf{0.0069} \\
& FT & SC & 25\% & 81.67 & 0.0088 \\
& Org & SC & 25\% & 68.14 & 0.0076 \\
\cline{2-6}
& CARMA & SC & 40\% & \textbf{81.48}  & 0.0102 \\
& FT & SC & 40\% & 74.29 & \textbf{0.0073} \\
& Org & SC & 40\% & 76.06 & 0.0136 \\
\hline

\multirow{6}{*}{Llama-1B} 
& CARMA & SC & 25\% & 74.03 & 0.0069 \\
& FT & SC & 25\% & \textbf{75.69} & \textbf{0.0044} \\
& Org & SC & 25\% & 2.65 & 0.1239 \\
\cline{2-6}
& CARMA & SC & 40\% & 71.43 & \textbf{0.0065} \\
& FT & SC & 40\% & \textbf{74.31} & 0.0102 \\
& Org & SC & 40\% & 1.73 & 0.2245 \\
\hline

\multirow{6}{*}{Qwen-0.5B} 
& CARMA & SC & 25\% & 89.66 & \textbf{0.0037} \\
& FT & SC & 25\% & \textbf{89.83} & 0.0085 \\
& Org & SC & 25\% & 59.12 & 0.0691 \\
\cline{2-6}
& CARMA & SC & 40\% & 86.03 & 0.0084 \\
& FT & SC & 40\% & \textbf{86.31} & \textbf{0.0046} \\
& Org & SC & 40\% & 55.27 & 0.0429 \\
\hline



\multirow{6}{*}{Qwen-3B} 
& CARMA & SC & 25\% & 93.65 & 0.0061 \\
& FT & SC & 25\% & \textbf{93.85} & \textbf{0.0039} \\
& Org & SC & 25\% & 67.63 & 0.0227 \\
\cline{2-6}
& CARMA & SC & 40\% & \textbf{91.26} & \textbf{0.0050} \\
& FT & SC & 40\% & \textbf{91.26 }& \textbf{0.0050} \\
& Org & SC & 40\% & 64.05 & 0.0159 \\
\hline


\multirow{6}{*}{Llama-3B} 
& CARMA & SC & 25\% & 84.83 & \textbf{0.0056} \\
& FT & SC & 25\% & \textbf{85.85} & 0.0065 \\
& Org & SC & 25\% & 35.21 &0.0136 \\
\cline{2-6}
& CARMA & SC & 40\% & 82.89 & \textbf{0.0016} \\
& FT & SC & 40\% & \textbf{83.55} & 0.0067 \\
& Org & SC & 40\% & 32.88 &0.0188 \\
\hline
\end{tabular}
\caption{Model performance (25\% and 40\% synonym intervention) on the SC task. \textbf{Ver.}: Version; \textbf{Int.}: Intervention rate; \textbf{CS}: ConsistSyn (\%); \textbf{CV}: Coefficient of Variation. \textbf{Best values in bold.}}
\label{tab:synonym_results_appendix_SC}
\end{table}

% \subsection{Effect of Stopping Layer on CARMA Performance}
% To assess the impact of stopping regularisation at different depths, we evaluate the performance deviation of Llama-1B on the IDM task under three stopping conditions: layer 1, layer 5, and layer 16. We compare these variants and report their respective performance against the three performance metrics. 