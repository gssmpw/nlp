\section{Results and discussion}\label{sec:results}
The method is evaluated across three aspects: (1) its impact on model robustness against compositional-based perturbations, (2) its impact on model performance stability, and (3) its impact on model overall performance. See Appendix~\ref{sec:evaluation_metrics} for a detailed breakdown of the evaluation metrics used for each aspect.

\subsection{Constituent-Aware Pooling (CAP) Intervention}
Figures~\ref{fig:carma_results_idm} and~\ref{fig:carma_results_sst} show the impact of CAP on IDM and SC tasks, comparing original, fine-tuned (FT) and CARMA models.\footnote{Throughout this paper, models incorporating CARMA with FT are referred to as CARMA models.} Model performance is averaged across three CAP protocols (Mean, Max, and Sum), with per-protocol results provided in Appendix~\ref{sec:additiona_results}. The analysis examines how well models preserve compositionality under hierarchical pooling.

CARMA's effectiveness is influenced by model size, tokenisation strategy, and task complexity. \ul{In IDM tasks, CARMA models have considerable gains when applying CAP at the earliest layers (1\% of model depth),} particularly in models with fine-grained tokenisation: Llama-1B (+3.61\%) and Gemma-2B (+16.89\%). GPT2-L, despite its reliance on subword tokenisation, benefits from CARMA over FT (+3.67\%). However, Llama-3B and Qwen-3B minimal improvements (+1.0\%) suggest a capacity ceiling where increased model size does not yield proportional gains due to training data limitations. \ul{The combination of smaller scale and multilingual training particularly affects Qwen-0.5B, where limited model capacity coupled with broad language coverage appears to constrain English-specific compositional learning,} resulting in reduced CARMA benefits. In SC tasks, tokenisation effects vary with task complexity. When intervening at 25\% layer position, Gemma-2B and Llama-1B show the strongest gains (+27.38\%, +10.59\%), while Llama-3B exhibits a marginal difference between CARMA and FT ($\sim1\%$) but still outperforms the Original model (+37.68\%). These results suggest that fine-tuning alone is sufficient for simpler tasks, whereas structured interventions like CARMA are particularly beneficial for more complex, compositional reasoning tasks.

\ul{In a layer-wise analysis, the impact of CARMA varies significantly across network depths, revealing crucial insights about compositional learning in transformers}. Early layers (0-25\%) benefit the most from regularisation, as they establish foundational compositional representations by exhibiting a weak notion of compositionality. Middle layers (25-75\%) reinforce these patterns, maintaining structured feature dependencies with moderate improvements. Deeper layers (75-100\%) show minimal benefits as the model transitions from compositional learning to task-specialised representations. This pattern aligns with previous findings on layer-wise compositional evolution in Transformers, where earlier layers capture hierarchical structure, while deeper layers exhibit increased task specificity \cite{feucht2024footprints}. CARMA can thus be strategically applied to control these early representations, maintaining beneficial compositional structure while allowing natural task-specific adaptations in deeper layers.\\
These findings demonstrate CARMA's effectiveness, particularly for models with granular tokenisation under data constraints, mediated by model capacity and task demands. The method's dual role - enhancing early compositional learning while preserving deeper layer adaptations - enables targeted improvement in model robustness without disrupting task-specific processing.

\begin{table}[h]
\centering
\tiny
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Ver.} & \textbf{Task} & \textbf{Int.} & \textbf{CS} & \textbf{CV} \\
\hline
\multirow{5}{*}{GPT2-L} 
& CARMA & IDM & 25\% & 56.31 & \textbf{0.0164} \\
& FT & IDM & 25\% & \textbf{56.95} & 0.0311 \\
& Org & IDM & 25\% & 51.10 & 0.1175 \\
\cline{2-6}
& CARMA & SC & 25\% & \textbf{0.8858} & \textbf{0.0065} \\
& FT & SC & 25\% & 0.8804 & 0.0082 \\
\hline
\multirow{6}{*}{Gemma-2B} 
& CARMA & IDM & 25\% & \textbf{56.70} & \textbf{0.023} \\
& FT & IDM & 25\% & 57.42 & 0.030 \\
& Org & IDM & 25\% & 49.47 & 0.031 \\
\cline{2-6}
& CARMA & SC & 25\% & 78.90 & \textbf{0.008} \\
& FT & SC & 25\% & \textbf{80.23} & 0.009 \\
& Org & SC & 25\% & 68.14 & 0.042 \\
\hline
\multirow{6}{*}{Llama-3B} 
& CARMA & IDM & 25\% & \textbf{62.86} & \textbf{0.015} \\
& FT & IDM & 25\% & 62.22 & 0.029 \\
& Org & IDM & 25\% & 52.47 & 0.035 \\
\cline{2-6}
& CARMA & SC & 25\% & 84.83 & \textbf{0.0056} \\
& FT & SC & 25\% & \textbf{85.85} & 0.0065 \\
& Org & SC & 25\% & 35.21 &0.0136 \\
\hline
\end{tabular}
\caption{Model performance (25\% synonym intervention). \textbf{Ver.}: Version; \textbf{Int.}: Intervention rate; \textbf{CS}: ConsistSyn (\%); \textbf{CV}: Coefficient of Variation. \textbf{Best values in bold.}}
\label{tab:synonym_results}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Synonyms Replacement Intervention} 
Synonym Replacement evaluates semantic consistency and robustness under lexical variations across multiple runs ($N\geq5$) with different seeds. \textit{ConsistSyn} measures output preservation after substitution, while the coefficient of variation (CV) quantifies performance stability, with lower values indicating higher stability. Performance is assessed at 25\% and 40\% word replacement rates to measure sensitivity to increasing perturbations. A sample of results is presented in Table~\ref{tab:synonym_results}, with full details in Appendix~\ref{sec:additiona_results}.

Across models, CARMA achieves a distinctive performance profile, matching or exceeding FT \textit{ConsistSyn} while consistently demonstrating superior stability through lower CV values. At 25\% intervention, Gemma-2B CARMA achieves 56.70\% \textit{ConsistSyn} with a CV of 0.0225, compared to FT's 57.42\% with higher variance (CV: 0.0307). Llama-3B CARMA outperforms FT in both \textit{ConsistSyn} (62.86\% vs. 62.22\%) and stability (CV: 0.0148 vs. 0.0292) for IDM. Qwen-3B follows a similar trend but with smaller relative gains, improving stability (CV: 0.0225 vs. 0.0279) while maintaining a marginal \textit{ConsistSyn} advantage over FT (62.00\% vs. 61.79\%). However, as intervention complexity increases to 40\%, the performance gap widens; for example, Gemma-2B FT maintains higher \textit{ConsistSyn} (44.98\%) than CARMA (42.36\%), though CARMA remains more stable (CV: 0.0174 vs. 0.0249). This behaviour implies that the advantage of CARMA lies in its lower variance and reinforcement of compositional consistency. Thus, \ul{it maintains compositional understanding without sacrificing performance, whereas FT produces a performance-driven approach.}


The tokenisation method significantly affects CARMA's impact. Models with more structured tokenisation show stronger stability improvements, but gains vary based on vocabulary design and language coverage. Llama and GPT2-L generally benefit more than Qwen, even with similar sizes, likely due to their smaller multilingual coverage, which results in a more compact and consistent token distribution. Qwen, with a larger vocabulary (151K tokens) supporting broader multilingual processing, introduces redundancy that dampens CARMA's relative stability advantage. Gemma-2B, optimised for a single dominant language with a large vocabulary size, shows the highest overall gains, reinforcing that a structured tokenisation approach focused on a limited linguistic scope enhances CARMA's effectiveness.

Task complexity further differentiates CARMA's effect. CARMA's advantages align with its methodological design, particularly in tasks requiring explicit structural reinforcement. \ul{In IDM, where systematicity and substitutivity are critical, CARMA ensures structured mappings hold under perturbation, particularly in Gemma-2B (+14.6\% over the original) and Llama-1B (+2692.5\% over the original in SC)}. However, in SC, where compositionality is more distributed, larger models show lower differences between CARMA and FT, reinforcing that larger models encode sentiment shifts effectively without additional intervention.

These results strengthen the hypothesis that CARMA enhances model robustness across perturbations, particularly in structured learning tasks and models where fine-tuning alone does not fully capture compositional dependencies. While FT maintains an advantage in absolute accuracy, CARMA ensures greater consistency, making it critical for improving compositional alignment and mitigating instability in high-variance settings.

% At 25\% intervention, CARMA stabilises model responses while maintaining competitive accuracy, with Gemma-2B CARMA achieving 56.70\% accuracy (CV: 0.0225) versus FT's 57.42\% (CV: 0.0307). The advantage of CARMA lies in its lower variance and reinforcement of compositional consistency. However, as intervention complexity increases to 40\%, the performance gap widens, with Gemma-2B FT maintaining higher accuracy (44.98\%) than CARMA (42.36\%), though CARMA remains more stable (CV: 0.0174 vs 0.0249). This behaviour suggests that \ul{CARMA effectively maintain compositional understanding without sacrificing performance, while FT produces a performance-driven approach}. 

% Across improved BPE-SentencePiece-based models (Llama family), CARMA exhibits size-dependent gains, with Llama-1B achieving a 95.5\% CV reduction in SST compared to the original at 40\%, while LLama-3B shows more moderate improvements (35.15\% CV reduction in IDM). In contrast, BPE-based models (GPT-2 family) show weaker CARMA benefits, particularly in smaller versions. GPT2-L sees minimal gains in robustness, with FT achieving better accuracy ($\sim1\%$), but CARMA provides a slighter gain in stability (CV: 0.0164 - 0.0311), suggesting tokenisation effects influence CARMA's impact.

% CARMA's advantages align with its methodological design, particularly in tasks requiring explicit structural reinforcement. \ul{In IDM, where systematicity and substitutivity are critical, CARMA ensures structured mappings hold under perturbation, particularly in Gemma-2B (+14.6\% over Original) and Llama-1B (+2692.5\% over Original in SST)}. In SST, where compositionality is more distributed, LLama-3B shows minimal differences between CARMA and FT, reinforcing that larger models encode sentiment shifts effectively without additional intervention.

% These results reinforce that CARMA enhances model robustness across perturbations, particularly in structured learning tasks and models where fine-tuning alone does not fully capture compositional dependencies. While FT maintains an advantage in absolute accuracy, CARMA ensures greater consistency, making it critical for improving compositional alignment and mitigating instability in high-variance settings.

\begin{figure}[h!]
    \centering
    \includegraphics[width=.9\linewidth]{figures/performance_and_consistency_high_res_idm.pdf}
    \caption{Task performance in IDM across GPT-2 (S, L), Gemma-2B, Llama (1B, 3B), and Qwen (0.5B, 3B).}
    \label{fig:idm_performance_comparison}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width=.9\linewidth]{figures/performance_and_consistency_high_res_sst.pdf}
    \caption{Task performance in SC across GPT-2 (S, L), Gemma-2B, Llama (1B, 3B) and Qwen (0.5B, 3B).}
    \label{fig:sst_performance_comparison}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Impact of CARMA on Performance}\label{sec:carma_performance}
Figures~\ref{fig:idm_performance_comparison} and~\ref{fig:sst_performance_comparison} show the performance of original, FT, and CARMA accuracies across tasks. \ul{CARMA demonstrates significant improvements over original models across tasks. For example, in IDM, GPT2-L achieves 150\% improvement, and Llama-3B shows an 89.6\% increase, while in SC, Gemma-2B demonstrates 122.5\% improvement over Original baselines}.

% Compared to Original models, CARMA shows substantial improvements across both tasks: in IDM, GPT2 Large achieves a \textbf{150\% increase} and Llama 3.2.3B improves by \textbf{89.6\%}, while in SST, Gemma-2B CARMA demonstrates a \textbf{122.5\% improvement}. These gains are consistent across model families, indicating CARMA's broad applicability.
Task-specific patterns emerge when comparing models. For instance, in IDM, CARMA outperforms FT, with Llama-3B showing a +5\% gain and GPT2-L improving by 1.7\%. In SC, CARMA maintains comparable performance to FT while enhancing robustness, suggesting it preserves learned features while strengthening compositional consistency.

% Comparing CARMA to FT models reveals task-dependent benefits. For IDM, CARMA outperforms FT with Llama 3.2.3B showing a \textbf{5\% gain} and GPT2 Large improving by \textbf{1.7\%}. In SST tasks, CARMA maintains FT-level performance while enhancing robustness. This pattern suggests that CARMA strengthens structured compositional consistency without disrupting learned mappings while preserving distributed feature representations, particularly in larger models.
\ul{CARMA enhances FT by improving representation stability and preventing feature drift, ensuring structured compositional consistency}. Its benefits are most pronounced in larger models, where greater capacity supports robust representations while maintaining fine-tuned performance. This scalability highlights CARMA's effectiveness in regularising model representations and reinforcing compositional structure without disrupting learned task features, providing a reliable solution for improving compositional reasoning in LLMs.

% CARMA achieves these improvements by complementing FT's task-specific adaptation with enhanced representation stability. While FT optimises for task-specific features, CARMA introduces regularisation mechanisms that prevent feature drift and ensure structured compositional consistency, leading to more stable activations across layers. Its benefits are most pronounced in larger models, where increased capacity allows for more robust feature representations while preserving fine-tuned performance. This scalability demonstrates CARMA's effectiveness in regularising complex model representations, reinforcing compositional structure without disrupting learned task-specific features. As a result, CARMA provides a reliable and adaptable solution for enhancing compositional reasoning in LLMs.

