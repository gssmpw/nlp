\begin{wrapfigure}{r}{0.38\textwidth}
\vspace{-11mm}
\centering
\includegraphics[scale=0.23]{teaser} 
\caption{\small{Our estimator achieves the same statistical precision as the standard estimator with up to 20\% fewer samples. Each study is subsampled to $n=75$. The figure shows the percentage sample size reduction needed to match the confidence interval width of the standard estimator using ours.}}
\label{fig:teaser}
\vspace{-11mm}
\end{wrapfigure}
Randomized experiments are widely considered the preferred approach for evaluating the effects of interventions in scientific research. However, obtaining sufficiently large sample sizes can be costly and time-consuming, especially when studying rare outcomes. For example, \citet{carlisle2015unsuccessful} reported that 481 out of 2579 recently completed clinical trials (19\%) failed due to insufficient patient recruitment to meet the required sample size. In cancer trials, this failure rate can be as high as 40\% due to strict eligibility and safety requirements~\citep{villacampa2024accrual}. As a result, there is growing interest in exploring in silico experiments as a potential alternative to randomized experiments. These digital experiments leverage the predictions from foundation models~\citep{bommasani2021opportunities}---machine learning models trained on massive datasets and applicable to many downstream tasks---to simulate the outcome of hypothetical randomized experiments.  By generating large amounts of synthetic experimental data, in silico experiments can reduce estimation uncertainty while also eliminating  the need for costly participant recruitment and follow-up. 



 
 \begin{figure*}
 \centering 
\includegraphics[width=0.95\textwidth]{figure1.pdf} 
    \caption{ \small{$\ours$ combines the standard $\aipw$ estimator, which relies on experimental data alone, with multiple competing estimators that replace the outcome regression with predictions from foundation models. By leveraging foundation models trained on a much larger sample, rather than estimating the outcome regression with the limited experimental data, $\ours$ significantly reduces the finite sample variance of the average treatment effect estimate. }}
    \label{fig:setting} 
\end{figure*}
This approach has already shown promising results in replicating the results of randomized experiments in several scientific disciplines, including medicine~\citep{dhawan2024end} and social sciences~\citep{argyle2023out,bail2024can,ashokkumar2024predicting}. However, the benefits of in silico
experiments come with a significant risk:  statistical inferences from such experiments are not valid if model predictions fail to reflect experimental responses to interventions. Since such an assumption is difficult to falsify, the growing consensus among researchers is that results from in silico experiments should be limited to exploratory stages of research, for example, pilot studies to predict effect sizes in larger experiments~\citep{grossmann2023ai}. 



In safety-critical fields like medicine, valid statistical inference is an absolute requirement. For instance, the Food and Drug Administration guidelines strongly recommend that any method aimed at improving the efficiency of randomized experiments ``should provide valid inference under approximately the same minimal statistical assumptions that would be needed for unadjusted estimation in a randomized trial''~\citep{fda2021guide}. This raises a critical question: can we improve the efficiency of randomized experiments while preserving valid statistical inference? In this paper, we introduce the concept of a \emph{hybrid experiment}, a statistical framework that combines predictions from multiple foundation models to improve the efficiency of randomized experiments while preserving valid statistical inference under minimal assumptions~(see~\Cref{fig:setting} for an illustration).




\paragraph{Our contributions} We introduce the \textbf{H}ybrid \textbf{A}ugmented \textbf{I}nverse \textbf{P}robability \textbf{W}eighting~(\ours), a novel estimator that integrates predictions from multiple, potentially biased, foundation models while preserving valid statistical inference. We further prove that $\ours$ is consistent and asymptotically normal, with asymptotic variance no larger than the classical estimator based on experimental data alone. Importantly, our results require no additional assumptions beyond those necessary for estimating treatment effects in classical randomized experiments. Finally, we provide empirical results across several randomized experiments, showing that $\ours$ offers substantial precision gains, equivalent to a reduction of up to 20\% in the sample size required to achieve the same precision as the standard estimator~(see~\Cref{fig:teaser}). 




