\hypersetup{
   linkcolor={pierLink},
    citecolor={pierCite},
    urlcolor={pierCite}
}
\section{Methodology}

\subsection{Proofs}
\subsubsection{Proof of \Cref{prop:rootn}}
\label{apx:proofaipw}
We adapt here a classic result from the semiparametric inference literature to our specific setting where the probability of treatment is known by design. For clarity, we refer to $\estaipw$ as $\esthaipw$.

Let us define the influence function of the \aipw~estimator for fixed outcome functions $h$ as:
$$\psi_i(h) = \left(\frac{A_i}{\pi_1}(Y_i - h(X_i, 1)) + h(X_i,1)\right) - \left(\frac{1-A_i}{\pi_0}(Y_i - h(X_i,0)) + h(X_i,0)\right).$$
We can then decompose the estimation error of the \aipw~estimator as follows:
$$\sqrt{n}(\esthaipw(\widehat{h}) - \theta) = \underbrace{\sqrt{n}(\esthaipw(h^\dagger) - \theta)}_{\defeq T_1} + \underbrace{\sqrt{n}(\esthaipw(\widehat{h}) - \esthaipw(h^\dagger))}_{\defeq T_2}.$$
The first term, $T_1$, is an average of i.i.d. random variables with mean zero and finite variance. Therefore, by the Central Limit Theorem, we have:
$$\sqrt{n}(\esthaipw(h^\dagger) - \theta) = \sqrt{n}\left( \frac{1}{n} \sum_{i=1}^n \psi_i(h^\dagger) - \theta \right) \rightsquigarrow \mathcal{N}(0, V_{h^\dagger}),$$
 where the asymptotic variance is given by $V_{h^\dagger} = \mathbb{E}[(\psi_i(h^\dagger) - \theta)^2]$.

\paragraph{Bounding the Remainder Term}
We need to show that the second term $T_2$ is asymptotically negligible, that is $T_2= o_{\p}(1)$.

We can rewrite this term as:
$$T_2 = \sqrt{n}(\esthaipw(\widehat{h}) - \esthaipw(h^\dagger)) = \frac{1}{\sqrt{n}} \sum_{i=1}^n \left(\psi_i(\widehat{h}) - \psi_i(h^\dagger)\right).$$
Further, with some simple algebra we can decompose the difference in the influence functions as:
\begin{align*}
  \frac{1}{\sqrt{n}} \sum_{i=1}^n (\psi_i(\widehat{h}) - \psi_i(h^\dagger)) 
  &= \frac{1}{\sqrt{n}} \sum_{i=1}^n \left(\frac{A_i-\pi_1}{\pi_1}\right) (h^\dagger(X_i,1) - \widehat{h}(X_i,1)) - \frac{1}{\sqrt{n}} \sum_{i=1}^n \left(\frac{A_i-\pi_1}{1-\pi_1}\right) (\widehat{h}(X_i,0) - h^\dagger(X_i,0))
\end{align*}
 Now, we will show that both terms in the sum above are asymptotically negligible. We focus our proof on the first term; the second follows from symmetric arguments.

Let $Z_i = (X_i,A_i,Y_i)$ and $\mathbb{P}_n$ denote the empirical measure over $Z_1, \dots, Z_n$, and define the following functions:
 $$f(Z_i) \defeq \frac{A_i-\pi_1}{\pi_1}~h^\dagger(X_i,1)~~\text{and}~~\widehat{f}(Z_i) \defeq \frac{A_i-\pi_1}{\pi_1}~\widehat{h}(X_i,1).$$ We can rewrite the first term as:
$$\frac{1}{\sqrt{n}} \sum_{i=1}^n \left(\frac{A_i-\pi_1}{\pi_1}\right) (h^\dagger(X_i,1) - \widehat{h}(X_i,1)) = (\mathbb{P}_n - \mathbb{P})(f - \widehat{f}),$$
where we use the fact that $\p(f-\widehat f)=0$, since the treatment probability is known. Since $\widehat h$ is estimated from an independent sample, it follows from Chebyshev inequality that
    $$(\mathbb{P}_n - \mathbb{P})(\widehat{f} - f) = O_{\p}\left(\frac{||\widehat{f} - f||_{L_2(\mathbb P)}}{\sqrt{n}}\right) = o_{\p}\left(\frac{1}{\sqrt n}\right ),$$
since it follows from assumptions that $||\widehat{f} - f||_{L_2(\mathbb P)} = o_{\p}(1)$. Therefore, it also follows that $T_2 = o_{\p}(1)$. 


Finally, using Slutsky's theorem, we get:
$$\sqrt{n}(\widehat{\theta}(\widehat{h}) - \theta) = \sqrt{n}(\widehat{\theta}(h^\dagger) - \theta) + o_{\p}(1) \rightsquigarrow \mathcal{N}(0, V_{h^\dagger}),$$
which completes the proof.

\subsubsection{Proof of Theorem \ref{thm:combine}}
\label{apx:proofhaipw}
Recall that $\Sigma \defeq \operatorname{Cov}[ (\psi( h^\dagger), \ldots, \psi(f_k))^\top]$ and  define the oracle weights as $\lambda^\star = \underset{\lambda \in \Lambda}{\arg\min}~ \lambda^\top \Sigma \lambda$. The corresponding oracle estimator is then   $$
\esthaipw_{\lambda^\star} = \lambda^\star_1 \estaipw(\widehat h) + \sum_{j=1}^k \lambda^\star_{j+1} \estaipw(f_j).$$ We now prove the theorem in the following three steps.

First,  we observe that $\esthaipw_{\lambda^\star}$ can also be written as $$ \esthaipw_{\lambda^\star} = \estaipw\left(\lambda^\star_1\widehat h+ \sum_{j=1}^k \lambda^\star_{j+1} f_j\right),$$ since the constraint set is $\Lambda = \{\lambda \in \RR^{k+1}: \sum_{j=1}^{k+1} \lambda_i =1\}$. Further, it follows from assumptions that $\lambda^\star_1\widehat h+ \sum_{j=1}^k \lambda^\star_{j+1} f_j$ is also an outcome function estimator that satisfies the conditions in~\Cref{prop:rootn}, therefore  $\esthaipw_{\lambda^\star}$ is consistent and asymptotically normal, i.e. it holds that
$$
\sqrt n (\esthaipw_{ \lambda^\star}   - \ate) \rightsquigarrow \gauss(0, V_{\lambda^\star}),
$$
where $V_{\lambda^\star} = \lambda^{\star \top} \Sigma \lambda^\star.$


Second, we show that the asymptotic variance \( V_{\lambda^\star} \) satisfies  
\[
V_{\lambda^\star} \leq \Sigma_{jj}~~ \text{for}~~j =1, \ldots, k+1.
\] By construction, the oracle weights \(\lambda^\star\) minimize \(\lambda^\top \Sigma \lambda\), ensuring \(\esthaipw_{\lambda^\star}\) attains the smallest asymptotic variance among all convex combinations of the initial estimators:  
\[
\left\{ \esthaipw_{\lambda} \coloneqq \lambda_1 \estaipw(\widehat h) + \sum_{j=1}^k \lambda_{j+1} \estaipw(f_j) \,\big|\, \lambda \in \Lambda \right\}.
\] 
For any \(j \in \{1,\ldots,k\}\), the estimator \(\estaipw(f_j)\) corresponds to \(\esthaipw_{\lambda'}\), where \(\lambda' \in \RR^{k+1}\) is the canonical basis vector with \(\lambda'_{j+1} = 1\) and \(\lambda'_i = 0\) for \(i \neq j+1\).  The same reasoning applies for the estimator $\estaipw(\widehat h).$ Since \(\esthaipw_{\lambda'} \in \{\esthaipw_{\lambda} : \lambda \in \Lambda\}\), the optimality of \(\lambda^\star\) implies:  
\[
V_{\lambda^\star} = \lambda^\star{}^\top \Sigma \lambda^\star \leq \lambda'{}^\top \Sigma \lambda' = \Sigma_{jj}, \quad \text{for}~~j=1,\ldots,k+1.
\]  
Finally, we observe that $\esthaipw_{\hat \lambda^\star}$ and $\esthaipw_{\lambda^\star}$ are asymptotically equivalent.
This follows directly from~\citet[Proposition 3.3]{lavancier2016general}, which establishes that if $\widehat \Sigma\Sigma^{-1}\overset{p}{\to}I$, $\esthaipw_{\hat \lambda^\star}$ and $\esthaipw_{\lambda^\star}$ have the same asymptotic distribution.




% \subsection{Covariance matrix estimation}
% \label{apx:covestimation}
% The key difference between the $\ours$ estimator from Algorithm~\ref{algo:haipw} and the standard $\aipw$ estimator is that the former aggregates multiple AIPW estimates. Optimally, the H-AIPW estimator would do so by  minimizing the true variance, that is:
% \begin{equation*}
%  \lambda^\star = \underset{\lambda \in \Lambda}{\arg\min}~\var[ \esthaipw_\lambda] = \underset{\lambda \in \Lambda}{\arg\min}~ \lambda^\top  \Sigma \lambda =  \Sigma^{-1} \mathbf 1 / (\mathbf 1^\top  \Sigma^{-1} \mathbf 1) , 
% \end{equation*}
% with $\Sigma$ being the asymptotic covariance matrix $n\operatorname{Cov}[ (\estaipw(h^\dagger), \ldots, \estaipw( f_k))^\top]$.

%  \paragraph{Estimating the covariance matrix $\widehat \Sigma$} In practice, however, the true variance is unknown and instead we need to estimate $\widehat \lambda$ or $\widehat \Sigma$, respectively.
%  Given the linear structure of the $\aipw$ estimator from Equation~\eqref{eq:aipw}, the covariance matrix equals:
% \begin{equation*}
%     \Sigma := n\operatorname{Cov}[ (\estaipw(h^\dagger), \ldots, \estaipw( f_k))^\top] =  \operatorname{Cov}[ (\psi(h^\dagger) , ..., \psi(f_k))^\top] 
% \end{equation*}
%  Therefore, we can reuse the samples from $\mathcal D$ to estimate the covariance matrix:
% \begin{align*}
% \Sigma \approx
%     \frac{1}{n}\sum_{i \in \mathcal D} ((\psi_i(\widehat{h}) , ..., \psi_i(f_k)) - \bar \psi)^\top ((\psi_i(\widehat{h}), ..., \psi_i(f_k)) - \bar \psi)
%     =: \widehat \Sigma,
% \end{align*}
% where $\bar{\psi} = \frac{1}{n} \sum_{i=1}^n (\psi_i( \widehat{h}), \dots, \psi_i( f_k))$.
   

% \paragraph{Discussion of the error term} We wonder how big is the price in variance we need to pay when using $\widehat \lambda = \widehat\Sigma^{-1} \mathbf 1 / (\mathbf 1^\top  \widehat \Sigma^{-1} \mathbf 1) $ instead of $\lambda^\star$. 
% We recall that by  Theorem~\ref{thm:combine}, $\esthaipw_{\hat \lambda}$ is asymptotically normal, and thus:
%  $     \var[ \esthaipw_{\hat \lambda} ]  = \Theta(\frac{1}{n})$,  with large probability (e.g. $>0.99$). We want to compare this rate against the extra variance arising from estimating the weight $\widehat \lambda$, that is: $ \var[ \esthaipw_{ \lambda^\star} ] -  \var[ \esthaipw_{\hat \lambda} ].$
% We  only present a non-rigorous informal discussion that serves as intuition. In particular, we argue that for small $k$, which is the case in practical applications as $k$ reflects the number of foundation models used, the extra variance is negligible as long as $n$ is larger by some constant factors. 

% First, we recall that by construction, both $\widehat \lambda $ and $\lambda^\star$ live on the  probability simplex, and thus the $\ours$ estimate is always unbiased. We therefore have 
% \begin{align*}
%     0 \leq \var[ \esthaipw_{\hat \lambda} ]  -  \var[ \esthaipw_{ \lambda^\star} ] = \mathbb E \left[(\esthaipw_{\hat \lambda} - \theta)^2\right] - \mathbb E \left[( \esthaipw_{ \lambda^\star} - \theta)^2\right] &= \mathbb E \left[(  \triangle + \esthaipw_{ \lambda^\star} - \theta)^2\right]- \mathbb E \left[(\esthaipw_{ \lambda^\star} - \theta)^2\right] \\
%    & = \mathbb E \left[  \triangle^2\right] + 2 \mathbb E \left[\triangle  (\esthaipw_{ \lambda^\star} - \theta)\right],
%     \end{align*}
% where we define
% \begin{align*}
%     \triangle := \esthaipw_{ \hat \lambda} - \esthaipw_{ \lambda^\star}  =(\widehat \lambda_{1} - \lambda_{1}^\star)\frac{1}{n} \sum_{i=1}^n\psi_i( \widehat{h})  + \sum_{j=1}^k (\widehat \lambda_{j+1} - \lambda_{j+1}^\star)\frac{1}{n} \sum_{i=1}^n\psi_i(f_j) = (\widehat \lambda - \lambda^\star)^\top \bar{\psi}.
% \end{align*}  We can now apply Cauchy-Schwartz, which yields
% \begin{align*}
%    \var[ \esthaipw_{\hat \lambda} ]  -  \var[ \esthaipw_{ \lambda^\star} ] \leq
%     \mathbb E \left[  \triangle^2\right]  + 
%    \left(\mathbb E \left[  \triangle^2\right]  \right)^{1/2} \left(\mathbb E \left[  (\esthaipw_{ \lambda^\star} - \theta)^2\right]\right)^{1/2} = 
%  \mathbb E \left[  \triangle^2\right]  + 
%    \left(\mathbb E \left[  \triangle^2\right]  \right)^{1/2} \left(\var[ \esthaipw_{ \lambda^\star} ]\right)^{1/2}.  
%     \end{align*}
% Thus, we see that the error is negligible compared to the variance of the estimator with the optimal assignment, i.e. $ \var[ \esthaipw_{ \lambda^\star} ]$, as long as $ \mathbb E \left[  \triangle^2\right] 
%  \to 0$.

%  To bound the variance $\mathbb E \left[  \triangle^2\right]$, first note that again by Theorem~\ref{thm:combine}, $\bar{\psi}  \overset{p}{\to} \left( \theta, \cdots, \theta\right)$ with $\theta$ being the average treatment effect. This concentration happens very rapidly, which allows us to bound (informally):
%  \begin{equation*}
%     \mathbb E \left[  \triangle^2\right] = \mathbb E [(\widehat \lambda - \lambda^\star)^\top \bar{\psi} \bar{\psi}^\top (\widehat \lambda - \lambda^\star)]  \lesssim  k \mathbb E  [(\widehat \lambda - \lambda^\star)^\top (\widehat \lambda - \lambda^\star)] =  k \mathbb E  \| \widehat \lambda - \lambda^\star \|_2^2,
%  \end{equation*}
%  where we used the fact that $\theta$ is just a constant. 

%  \paragraph{Concentration of $\widehat \lambda \to \lambda^\star$}
% As we saw, the additional variance that we pay due to estimating $\widehat \lambda$ can be upper bounded by a function of the true variance and the term $\mathbb E  \| \widehat \lambda - \lambda^\star \|_2^2$. Assuming that the eigenvalues of $\Sigma$ are both lower and upper bounded, which is the case assuming that there is ``sufficient diversity'' among the models,  a straight forward calculation yields: 
% \begin{equation}
%      \| \widehat \lambda - \lambda^\star \|_2^2 \lesssim    \frac{ k^2}{n^2} \| \widehat \Sigma - \Sigma \|_{\text{op}}^2
% \end{equation}
% where $\|. \|_{\text{op}}$ is the operator norm. Thus, in summary, we obtain the bound:
% \begin{align*}
%      \var[ \esthaipw_{\hat \lambda} ]  -  \var[ \esthaipw_{ \lambda^\star} ] \lesssim
%     \frac{k^2}{n^2} \mathbb E  \| \widehat \Sigma - \Sigma \|_{\text{op}}^2
%     + \frac{k}{n} \left( \mathbb E \| \widehat \Sigma - \Sigma \|_{\text{op}}^2\right)^{1/2} \left(\var[ \esthaipw_{ \lambda^\star} ]\right)^{1/2} .
%     \end{align*}

% \paragraph{Estimating the covariance matrix}
% We see from the previous equations that as long as $\mathbb E \| \widehat \Sigma - \Sigma \|_{\text{op}}^2 =o(1)$, the contribution of the error arising from estimating the covariance matrix goes to zero, compared to the variance term $\var[ \esthaipw_{ \lambda^\star} ] $, which is of order $\frac{1}{n}$. In fact, in general we expect the  $\mathbb E \| \widehat \Sigma - \Sigma \|_{\text{op}}^2$ to vanish at a fast rate. Indeed, for instance assume that $(\psi_i( \widehat{h}), \dots, \psi_i( f_k))$ are iid Gaussians. In this case, we have that with high probability $\|\widehat \Sigma - \Sigma \|^2_{\text{op}} \lesssim \frac{k}{n} $, and thus the error error arising from estimating the covariance matrix vanishes much faster than the optimal variance!

% \newpage
 \subsection{Connection with prediction-powered inference}
 To further study the connection and differences with prediction-powered inference ($\ppi$) \cite{angelopoulos2023prediction}, it is instructive to consider the simpler problem of estimating the counterfactual mean, $\EE[Y(1)]$.  For this case, a variant of $\ppi$, referred to as $\ppi$++~\citep{angelopoulos2023ppi++}, can  be shown to be equivalent to an $\aipw$ estimator.

  The  standard difference in mean estimator in this case is the sample mean of outcomes for the treated group: $$\estdm  = \frac{1}{n_1} \sum_{i: A_i=1} Y_i,~\text{where}~n_a = \sum_{i=1}^n \indi\{A_i=a\}.$$  $\ppi$++ improves the difference in mean estimator by incorporating predictions from a black-box model $f$: 
 $$ \esthaipw_{\ppi++} =  \frac{1}{n} \sum_{i=1}^n Y_i  + \lambda \left( -\frac{1}{n_1} \sum_{i:A_i=1} f(X_i) + \frac{1}{n_0} \sum_{i:A_i=0} f(X_i) \right),$$
 where the power-tuning parameter $\lambda$ is chosen to minimize the variance. Crucially, for $\lambda =\frac{n_0}{n_1+n_0}$ we have equivalence with the $\aipw$ estimator for the counterfactual mean, i.e.
 $$
  \esthaipw_{\ppi++}  = \frac{1}{n} \sum_{i=1}^n \left(\frac{A_i (Y_i- f(X_i))}{\pi_1} + f(X_i) \right) = \estaipw(f).
 $$
 A few remarks are in order.
 \begin{itemize}
     \item $\ppi$++ replaces the estimated outcome regression with a black-box  model $f$. However, when $f(x)$ is not equivalent to the outcome regression $\EE[Y\mid X=x,A=1] $, the resulting estimator will not be efficient. In other words, $\esthaipw_{\ppi++}$ will not achieve the smallest asymptotic variance among the regular estimators of the counterfactual mean. In contrast, the standard $\aipw$ will achieve the smallest possible asymptotic variance, assuming that the outcome regression estimator is consistent in $L_2$-norm. This condition is easy to satisfy in the setting of randomized experiments, since we can use  flexible machine-learning models and still have  valid confidence intervals as a consequence of~\Cref{prop:rootn}. In contrast, our estimator is guaranteed to have asymptotic variance no greater than the standard $\aipw$ estimator (see~\Cref{thm:combine}). As a result, it is efficient even if the black-box model $f$ is arbitrarily biased.
     \item Extending $\ppi$ and $\ppi$++ to average treatment effect estimation is not straightforward. To do so, \citet{poulet2025prediction} proposes the following estimator:
     $$
     \esthaipw_{\textsc{Ppct}} \defeq 
     \frac{1}{n_1} \sum_{A_i =1} (Y_i - \lambda f(X_i)) -  \frac{1}{n_0} \sum_{A_i =0} (Y_i - \lambda f(X_i)). 
     $$
 However, a key limitation of the above estimator is that it forces both outcome regressions, that is \(\EE[Y\mid X=x,A=1]\) and \(\EE[Y\mid X=x,A=0]\), to be replaced with the same black-box model \( f \). This is particularly problematic when the treatment has a significant effect on the outcome, as a single model \( f \) will fail to accurately capture both outcome regressions. In contrast, our approach  allows for different black-box models $f_1$ and $f_0$ to be plugged-in for the treated and control group, respectively.
    
     \item \(\ppi\) and its variants cannot integrate multiple competing foundation models. This is a key limitation in the causal inference setting, as model selection is a non-trivial task due to the missingness of potential outcomes. Moreover, it is unclear whether they can be extended to do so, as constructing a consistent estimate of the covariance matrix \(\Sigma\) poses a major hurdle. In contrast, our approach offers a simple way to estimate the covariance matrix $\Sigma$ by exploiting the linear structure of the \(\aipw\) estimators.
     \end{itemize}




\newpage
\section{Additional experiments}
We present here additional experiments on randomized studies and ablations of our method. The results reinforce the general trends observed in the main experiments: $\ours$ achieves better precision than the baselines, particularly in the small sample regime, while maintaining comparable coverage. The ablation studies provide insights into the number of models that can be incorporated into our estimator without significantly compromising validity (due to finite sample effects). Additionally, they offer further evidence of the advantages of increasing inference-time compute.

\subsection{Evaluation on additional scientific studies}
\label{apx:more_studies}

In the main text (\Cref{fig:main_results}), we demonstrated the effectiveness of our estimator across three studies. Here, we extend our analysis to five additional studies spanning diverse fields: Economics \cite{haaland2023beliefs}, Psychology \cite{brandt2013onset}, Sociology \cite{caprariello2013have}, Political Science \cite{fahey2023principled}, and Social Behavior \cite{shuman2024defend}. The experimental setup remains consistent with the main part of the paper (see \Cref{sec:main_experiments} for details).

\Cref{fig:appendix_results_1} presents results for four of these studies, which align with findings from the main experiments. $\ours$ achieves variance gains often exceeding $20\%$ over the baseline estimator in the small-sample regime. In the large-sample regime, $\ours$ performs similarly to $\ppct$, while still improving upon standard $\aipw$ by $3\%-6\%$. As for validity, $\ours$ maintains comparable empirical coverage across studies.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.6\textwidth]{legend.pdf}\vspace{2mm}\\
       
    \begin{subfigure}[t]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figures/brandt_eff}
        \caption{}
        \label{fig:brandt_eff}
    \end{subfigure}\hfill
     \begin{subfigure}[t]{0.245\textwidth}
\includegraphics[width=\textwidth]{figures/haaland_eff}
        \caption{}
        \label{fig:haaland_eff}
    \end{subfigure}
    \begin{subfigure}[t]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figures/faheyS78_eff.pdf}
        \caption{}
        \label{fig:fahey_eff}
    \end{subfigure}
    \begin{subfigure}[t]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figures/Caprariello130_eff}
        \caption{}
        \label{fig:Caprariello130_eff}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.245\textwidth}
\includegraphics[width=\textwidth]{figures/brandt_cov}
        \caption{}
        \label{fig:brandt_cov}
    \end{subfigure}
      \hfill
    \begin{subfigure}[t]{0.245\textwidth}
    \includegraphics[width=\textwidth]{figures/haaland_cov}
        \caption{}
        \label{fig:haaland_cov}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.245\textwidth}
    \includegraphics[width=\textwidth]{figures/faheyS78_cov.pdf}
        \caption{}
        \label{fig:fahey_cov}
    \end{subfigure}
     \hfill
    \begin{subfigure}[t]{0.245\textwidth}
    \includegraphics[width=\textwidth]{figures/Caprariello130_cov}
        \caption{}
        \label{fig:Caprariello130_cov}
    \end{subfigure}    

\caption{\small{Performance comparison of \ours~with baseline estimators (\ppct, \dm, \aipw) across four additional randomized studies—\citet{brandt2013onset}, \citet{haaland2023beliefs}, \citet{fahey2023principled}, and \citet{caprariello2013have}—spanning Psychology, Economics, Political Science, and Sociology.  We randomly subsample each study to obtain the sample sizes shown on the x-axis and report the average over $R=10k$ repetitions for each metric. The significance level is set to $\alpha=0.05$. 
 \textbf{(First row) Precision:} Empirical estimate of the variance of $\ours$ and baselines for varying sample sizes. \textbf{(Second row) Validity}: Empirical coverage probability of each estimator, with the dashed line marking nominal $95\%$ coverage. Results confirm that $\ours$ improves precision while maintaining valid coverage. }}
    
    \label{fig:appendix_results_1}
\end{figure*}

% \begin{wrapfigure}{r}{0.35\textwidth}
\begin{figure}
    \centering   \includegraphics[width=0.65\textwidth]{legend_shuman.pdf}\vspace{2mm}\\
    \begin{subfigure}[t]{0.35\textwidth}
    \includegraphics[width=\textwidth]{figures/shuman_eff}
        \label{fig:shuman_eff}
    \end{subfigure}
        \hspace{30pt}
    \begin{subfigure}[t]{0.35\textwidth}
    \includegraphics[width=\textwidth]{figures/shuman_cov}
        \label{fig:shuman_cov}
    \end{subfigure}
    
    
    \caption{\small{Performance comparison of \ours~with baseline estimators (\ppct, \dm, \aipw) for the randomized study by \citet{shuman2024defend}. We randomly subsample each study to obtain the sample sizes shown on the x-axis and report the average over $R=10k$ repetitions for each metric. The significance level is set to $\alpha=0.05$. The same experimental configuration as in \Cref{fig:main_results} is maintained, except that predictions are limited to three prompts at inference time.}}
    \label{fig:shuman_results}
    \end{figure}



\paragraph{Study with a visual treatment and out of GPT-4o training dataset}
\label{apx:image_treatment}

The study by \citet{shuman2024defend} is particularly relevant for two reasons. First, its data was published in December 2024, after the last known training cutoff for GPT-4o, ensuring it was not included in the model's training set. Second, the study's treatment is an image rather than text, allowing us to evaluate our statistical framework beyond the text modality. As shown in \Cref{fig:shuman_results}, $\ours$ maintains strong performance in both precision and validity, achieving a reduction in variance of up to $37\%$ over the $\dm$ estimator and between $5\%$ and $12\%$ compared to others. The empirical coverage is also comparable with the baselines.






\subsection{Impact of adding more foundation models on statistical precision}
\label{apx:adding_models}
In this section, we study the impact of increasing the number of models in $\ours$. Specifically, \Cref{algo:haipw} requires integrating predictions from multiple foundation models, which are combined with the standard $\aipw$ to minimize the variance of the resulting estimator. In \Cref{fig:adding_models}, we show how increasing the number of language models from 1 to 7 affects the precision and validity of $\ours$ in the study by \citet{fahey2023principled}. Models are incorporated in the estimator sequentially, starting from those with the lowest mean squared error (MSE) (i.e. LLaMA 3 70B) to those with the highest (stopping at Gemma 2 27B), following \Cref{fig:scaling_laws}. We also include the standard $\aipw$ estimator for reference.



Increasing the number of models improves precision compared to the standard $\aipw$ estimator. In the small-sample setting with 50 observations, a single model improves variance by approximately $6\%$, while using 4 models increases this gain to nearly $12\%$, and 7 models yield an improvement of around $16\%$. However, the marginal benefits diminish with larger sample sizes: at 200 observations, the variance difference between using 1 and 7 models shrinks to $4\%$.

However, adding more models weakens empirical coverage. With 50 samples, combinations of 5 to 7 models exhibit undercoverage of $2\%$–$4\%$ relative to $\aipw$ or $\ours$ with 1–2 models, failing to reach the nominal $95\%$ coverage until the sample size reaches 200. In contrast, combinations of 1 to 3 models maintain coverage levels comparable to $\aipw$. This is expected as there is a finite sample error term associated with estimating the weights, as discussed in~\Cref{apx:covestimation}. Therefore, practitioners should carefully determine the number of models to include in the ensemble based on the available sample size.













\begin{figure}[ht!]
 \centering
    \includegraphics[width=0.6\textwidth]{figures/legend_ablation.pdf}\\
    \centering
    \begin{subfigure}{0.35\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/adding_models_precision.pdf}
        \label{fig:adding_models_precision}
        % \caption{\pdb{add just LLM baseline here}}
    \end{subfigure}
    % \hfill
    \hspace{30pt}
    \begin{subfigure}{0.35\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/adding_models_validity.pdf}
        \label{fig:adding_models_validity}
                % \caption{}

    \end{subfigure}
\caption{\small{Impact of increasing the number of models in $\ours$ on precision and validity in the study by \citet{fahey2023principled}. Models are sequentially incorporated based on their mean squared error (MSE), starting with LLaMA 3 70B (lightest red, $k=1$) and ending with Gemma 2 27B (darkest red, $k=7$), following \Cref{fig:scaling_laws}. The left panel shows the empirical  variance, while the right panel shows empirical coverage. The standard $\aipw$ estimator is included for reference. Each experiment is averaged over $R=10k$ repetitions, with significance level set to $\alpha=0.05$.}}
    \label{fig:adding_models}
\end{figure}







\subsection{Impact of inference-time compute on statistical precision}
\label{apx:ablation_testtime}

In \Cref{sec:improving_accuracy}, we demonstrated that increasing inference-time compute improves the precision of $\ours$. This was established by studying the relationship between lower mean squared error (MSE) and reduced variance, as well as by showing that a higher number of prompts generally leads to lower MSE. For completeness, \Cref{fig:prompts_comparison} explicitly visualizes the connection between the number of prompts, MSE, and variance.

We present results for three studies---\citet{brandt2013onset,silverman2022putting, kennedy2020accidental}---using $\ours$ with predictions from GPT-4o. \Cref{fig:prompt_ci_Brandt,fig:prompt_ci_Ken,fig:prompt_ci_Silver} show the empirical estimate of the variance as a function of the number of prompts, while \Cref{fig:prompt_mse_b,fig:prompt_mse_k,fig:prompt_mse_s} illustrate the corresponding changes in MSE. The findings reinforce the conclusions from the main text: increasing inference-time compute through multiple prompts generally reduces the variance of $\ours$.



\begin{figure}[t!]

    \centering
    
    % \begin{subfigure}[t]{0.41\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{figures/prompts_ci}
    %     \caption{}
    % \end{subfigure}
    % \hspace{30pt}
    % \begin{subfigure}[t]{0.4\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{figures/prompts_mse}
    %     \caption{}
    % \end{subfigure}

        \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/prompt_ci_Brandt.pdf}
         \caption{}
        \label{fig:prompt_ci_Brandt}
        \end{subfigure}
         \begin{subfigure}[t]{0.3\textwidth}
\includegraphics[width=\textwidth]{figures/prompt_ci_Silverman1035.pdf}
         \caption{}
    \label{fig:prompt_ci_Silver}
        \end{subfigure}
        \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/prompt_ci_Kennedy1017.pdf}
         \caption{}
    \label{fig:prompt_ci_Ken}
        \end{subfigure}\\
        \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/prompt_mse_Brandt.pdf}
        \caption{}
        \label{fig:prompt_mse_b}
        \end{subfigure}
        \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/prompt_mse_Silverman1035}
         \caption{}
            \label{fig:prompt_mse_s}

        \end{subfigure}
        \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/prompt_mse_Kennedy1017.pdf}
     \caption{}            \label{fig:prompt_mse_k}

        \end{subfigure}
    
\caption{\small{Impact of the number of prompts on the empirical variance and MSE. Results are reported for studies by \citet{brandt2013onset, silverman2022putting, kennedy2020accidental}. We randomly subsample each study to obtain the sample sizes shown on the x-axis and report the average over $R=10k$ repetitions for each metric. \textbf{(First row)} Reduction in variance as the number of prompts increases. \textbf{(Second row)} Reduction in MSE as the number of   prompts increase. These results suggest that increasing inference-time compute improves the precision of $\ours$ by reducing the MSE.}}

    \label{fig:prompts_comparison}
\end{figure}



\newpage
\section{Experimental details}
\label{apx:expdetails}



\subsection{Implementation details}
\label{apx:implementation}

For all experiments, we begin with a feature selection step that identifies the five features most correlated with the outcome variable. The $\aipw$ estimator is implemented using cross-fitting with 30 folds and ridge regression with a regularization parameter of $\lambda=1.0$ for outcome regression estimation. For $\ppct$, we follow the implementation by \citet{poulet2025prediction}, using GPT-4o's predictions for the control scenario as the prognostic score. The correlation coefficients for the optimal combination are computed using standard \texttt{Python} libraries. Finally, the $\dm$ estimator requires no hyperparameter tuning. We compute the ground-truth ATE for all studies using the $\dm$ estimator on the full study with sample size $N$.



\paragraph{Implementation of H-AIPW} 
Our estimator integrates synthetic outcomes generated by multiple LLMs. Unless stated otherwise, we use predictions from LLaMA 3 70B, GPT-4o, and Claude 3.5 Haiku for all experiments in \Cref{sec:main_experiments}. Additional models, such as Gemma 2, Grok 2, and Gemini 1.5 Flash, are used in specific cases, e.g. \Cref{sec:improving_accuracy}. We leverage both proprietary and open-source LLMs. For open-source models, we apply nucleus sampling with a temperature of $1.2$, top-p of $0.9$, and a maximum of 100 new tokens. For proprietary models, we use default decoding settings, except for Claude 3.5 Haiku, where we set the temperature to 1.

In summary, $\ours$ extends the classic $\aipw$ estimator by incorporating multiple $\aipw$ estimators that integrate LLM predictions; see \Cref{algo:haipw} for full details.


\paragraph{Reproducing~\Cref{fig:teaser}}
We randomly subsample each study with a sample size of \( n = 75 \) and compute the average confidence interval over 1k repetitions using the standard AIPW estimator. Then, we obtain $n_{\ours}$ by progressively reducing \( n \) until the average confidence interval from \(\ours\) (using GPT-4o, LLaMa 3 70B, and Claude 3.5 Haiku) matches or exceeds the $\aipw$ confidence interval. The percentage reduction in sample size is reported as:  
\[
100\left(1 - \frac{n_{\ours}}{n} \right).
\]




\subsection{Preprocessing of scientific studies and prompt design}
\label{apx:prompts}

In this section, we describe the preprocessing steps, selected outcomes, and control and treatment scenarios for the studies used in our experiments. We also provide an example prompt, including both system and user components, used to query the LLMs. The studies are sourced from the Time-sharing Experiments for the Social Sciences (TESS) repository, with findings published in peer-reviewed journals. These studies span various fields, demonstrating the versatility of our methodology.


\subsubsection{Cancel Culture for Friends, Consequence Culture for Enemies: The Effects of Ideological Congruence on Perceptions of Free Speech~\citep{fahey2023principled}}

\textbf{Abstract:} Political scientists have long been interested in the effects that media framings have on support or tolerance for controversial speech. In recent years, the concept of cancel culture has complicated our understanding of free speech. In particular, the modern Republican Party under Donald Trump has made ``fighting cancel culture'' a cornerstone of its electoral strategy. We expect that when extremist groups invoke cancel culture as a reason for their alleged censorship, support for their free speech rights among Republicans should increase. We use a nationally representative survey experiment to assess whether individuals’ opposition to cancel culture is principled or contingent on the ideological identity of the speaker. We show that framing free speech restrictions as the consequence of cancel culture does not increase support for free speech among Republicans. Further, when left-wing groups utilize the cancel culture framing, Republicans become even less supportive of those groups’ free speech rights.


\textbf{Data availability:}  The study is publicly available at: \url{https://www.tessexperiments.org/study/faheyS78}


\textbf{Data pre-processing:} The primary outcome variable is \texttt{CC\_1}. The treatment condition is defined as $\texttt{P\_GROUP} =2$ (safety reasons + cancel culture), and the control condition is defined as $\texttt{P\_GROUP} =1$ (safety reasons). The following variables are included as covariates: \texttt{PARTYID7, IDEO, RELIG, ATTEND, GENDER, AGE, HOME\_TYPE, INCOME}. The final processed dataset contains $n=998$ observations.

\textbf{Prompting details:} 
An example prompt is provided below.


\begin{tcolorbox}[
   title=Example Prompt,
   fonttitle=\bfseries,
   colback=white,
   colframe=pierCite,
   width=\textwidth,
   left=5pt,
   right=5pt
]
\textbf{System Prompt:}
\begin{quotation}
You are a 35-year-old male, politically Democrat, holding liberal views. Additionally, your religion is Christianity, and you once or twice a month attend religious services. You reside in a building with two or more apartments, and your household has a yearly income of \$85,000 to \$99,999. You are responding to a scenario reflecting a debate involving college campus events and broader social issues. Your answer must be a single integer without additional text, in JSON format with a key-value pair.
\end{quotation}

\textbf{Treatment Condition:}
\begin{quotation}
We are now going to ask you to imagine you have read about the following scenario, describing a debate on a recent College Campus. 

\textbf{Local Group Denied Permit to Protest on Campus, Provoking Debate About “Cancel Culture”}

A debate on the merits of free speech erupted recently when the student chapter of the controversial far-left group Antifa attempted to obtain a permit to conduct a demonstration on the main quad of Rutgers University in New Jersey. Citing safety concerns, the president of the organization in charge of Registered Student Organizations (RSOs) initially denied the organization the right to conduct their rally, arguing that their presence would endanger college students. They cited a recent incident in Berkeley, CA where three Antifa members and two bystanders were injured by rocks thrown in an altercation between the group and counter protesters. A member of the local Antifa group, Luke Vargas, is appealing the decision, arguing that the permit denial represented "cancel culture run amok," and the University was simply "afraid to hear the truth." When asked to comment, the University Ombudsman's Office promised that a final decision on whether the rally would be permitted would be made by this Thursday, three days before the march is scheduled to take place on Sunday.
\end{quotation}

\textbf{Control Condition:}
\begin{quotation}
We are now going to ask you to imagine you have read about the following scenario, describing a debate on a recent College Campus. 

\textbf{Local Group Denied Permit to Protest on Campus}

A debate on the merits of free speech erupted recently when the student chapter of the controversial far-left group Antifa attempted to obtain a permit to conduct a demonstration on the main quad of Rutgers University in New Jersey. Citing safety concerns, the president of the organization in charge of Registered Student Organizations (RSOs) initially denied the organization the right to conduct their rally, arguing that their presence would endanger college students. They cited a recent incident in Berkeley, CA where three Antifa members and two bystanders were injured by rocks thrown in an altercation between the group and counter protesters. A member of the local Antifa group, Luke Vargas, promised to bring an appeal to the desk of the University President. When asked to comment, the University Ombudsman's Office promised that a final decision on whether the rally would be permitted would be made by this Thursday, three days before the march is scheduled to take place on Sunday.
\end{quotation}

\textbf{Question:}
\begin{quotation}
Generally speaking, do you agree or disagree with the following statement:
``Cancel culture is a big problem in today’s society.''
Reply using numbers between 1 (definitely agree) and 5 (definitely disagree).
\end{quotation}
\end{tcolorbox}


\subsubsection{Can Factual Misperceptions be Corrected? An Experiment on American Public Fears of Terrorism~\citep{silverman2022putting}}

\textbf{Abstract:} An American’s yearly chance of being killed by a terrorist attack sits at about 1 in 3.5 million. Yet over 40\% of the American public consistently believes that they or their family members are likely to be the victim of a terror attack. Can these inflated estimates of the risks of terrorism be brought closer to reality? With trillions of dollars spent on the War on Terror since 9/11, this question is not just theoretically but practically important. In order to investigate, we field a nationally representative survey experiment containing a brief vignette with corrective information about the actual risks of terrorism vs. other dangers facing Americans. Additionally, we vary whether there is a political elite endorsement accompanying the information, with either a Democratic politician, Republican politician, or senior military officer driving home the message.


\textbf{Data availability:}  The study is publicly available at: \url{https://tessexperiments.org/study/silverman1035}


\textbf{Data pre-processing:} The primary outcome variable is \texttt{Q5}. The treatment condition is defined as $\texttt{P\_TESS031} =1$ (corrective information), and the control condition is defined as $\texttt{P\_TESS031} =0$ (no corrective information). The following variables are included as covariates: \texttt{PARTYID7, IDEO, RELIG, ATTEND, GENDER, AGE,
       RACETHNICITY, EDUC4, INCOME}. The final processed dataset contains $n=503$ observations.


\textbf{Prompting details:} 
An example prompt is provided below.


\begin{tcolorbox}[
   title=Example Prompt,
   fonttitle=\bfseries,
   colback=white,
   colframe=pierCite,
   width=\textwidth,
   left=5pt,
   right=5pt
]
\textbf{System Prompt:}
\begin{quotation}
You are a 33-year-old, ethnicity White, gender Male, strong Democrat. You hold very liberal views and college education. Additionally, your religion is Catholic, and you attend religious services nearly every week. Your household has a yearly income of \$75,000 to \$84,999. Your answer must be a single integer without additional text, in JSON format with a key-value pair.
\end{quotation}

\textbf{Treatment Condition:}
\begin{quotation}
The number of people who say that acts of terrorism against Americans are imminent is up 3\% from last year, according to a new poll released this week. In the wake of attacks in San Bernardino, Orlando, Paris, and London, the Pew Research Center found that 63\% of Americans think major terrorist attacks are likely to occur soon on American soil. Government officials have echoed these concerns. ``We are issuing a new advisory that the terror threat is now elevated across the country," said Undersecretary for Homeland Security Stephen Krause. ``We have to remain vigilant and we have to stay alert. Terrorists can strike anytime, anywhere." 

But does terrorism really pose a critical threat to us? Below is a figure showing the average American's risk of death from different sources. 
As can be seen, around 90 Americans are killed each year by terrorism on U.S. soil. This means the risk of being a victim of terrorism in a given year is about 1 in 3.5 million. In comparison, the risk of being killed by cancer is 1 in 540, the risk of being killed in a car accident is 1 in 8,000, and the chance of being killed by your own home appliances is 1 in 1.5 million. These numbers provide some essential context when thinking about the different threats to our public safety.
\end{quotation}

\textbf{Control Condition:}
\begin{quotation}
The number of people who say that acts of terrorism against Americans are imminent is up 3\% from last year, according to a new poll released this week. In the wake of attacks in San Bernardino, Orlando, Paris, and London, the Pew Research Center found that 63\% of Americans think major terrorist attacks are likely to occur soon on American soil. Government officials have echoed these concerns. ``We are issuing a new advisory that the terror threat is now elevated across the country," said Undersecretary for Homeland Security Stephen Krause. ``We have to remain vigilant and we have to stay alert. Terrorists can strike anytime, anywhere."
\end{quotation}

\textbf{Question:}
\begin{quotation}
How likely do you think it is that another terrorist attack causing large numbers of American lives to be lost will happen in the near future? Choose an integer between 1 (very likely) and 5 (not likely at all).
\end{quotation}
\end{tcolorbox}

\subsubsection{Accidental Environmentalists: Examining the Effect of Income on Positive Social Evaluations of Environmentally-Friendly Lifestyles~\citep{kennedy2020accidental}} 


\textbf{Abstract:} Many US households have adopted behaviors aimed at reducing their environmental impact. Existing scholarship examines antecedent variables predicting engagement in these pro-environmental behaviors. But little research examines the effect of making efforts to reduce environmental impact on positive evaluations. Based on our qualitative pilot data, we suspect that income may be an important factor in the extent to which green lifestyles earn social approval. We predict that a household that reduces its environmental impact will be viewed more positively if that household has a high (rather than low) income. We manipulate household income (high vs low) and proenvironmental behavior (green vs typical). We then measure participants' approval of the household, how socially close they feel to the household, as well as their evaluations of the household's competence, morality, and environmental commitment. This research allows us to identify the bases for social approval of green lifestyles and examine how social approval for a household's green lifestyle varies with that household's income.

\textbf{Data availability:}  The study is publicly available at: \url{https://tessexperiments.org/study/kennedy1017}

\textbf{Data pre-processing:} The primary outcome variable is \texttt{Q5}. The treatment condition is defined as \texttt{P\_TESS23 = 4} (green lifestyle), and the control condition is defined as \texttt{P\_TESS23 = 2} (typical lifestyle). The following variables are included as covariates: \texttt{PartyID7}, \texttt{IDEO}, \texttt{ATTEND}, \texttt{GENDER}, \texttt{AGE}. The final processed dataset contains $n=1276$ observations.



\textbf{Prompting details:} 
An example prompt is provided below.

\begin{tcolorbox}[
    title=Example Prompt ,
    width=\textwidth,
    colback=white,
    colframe=pierCite,
    left=5pt,
    right=5pt,
    top=5pt,
    bottom=5pt
]
\textbf{System Prompt:}
\begin{quotation}
You are a 45-year-old, lean Democrat, gender Female, and hold slightly conservative views. Additionally, you attend religious services several times a year. We are going to give you some information about a family. Please read the information very carefully, as we will be asking you questions about it. Your answer must be in JSON format with a single key-value pair.
\end{quotation}

\textbf{Treatment condition:}
\begin{quotation}
A family with two children lives in a neighborhood nearby to yours. You chat with them sometimes when you see them in the neighborhood. As far as you can tell, they make a huge amount of money and seem to have plenty of extra money to spend. Their house is small and they often take public transit or walk to avoid driving. They also dry their clothes on a clothesline and don't have air conditioning in their home. This family has a much lower environmental impact than other people in their neighborhood.
\end{quotation}

\textbf{Control condition:}
\begin{quotation}
A family with two children lives in a neighborhood nearby to yours. You chat with them sometimes when you see them in the neighborhood. As far as you can tell, they make very little money and seem to have no extra money to spend. Their house is small and they often take public transit or walk to avoid driving. They also dry their clothes on a clothesline and don't have air conditioning in their home. This family has a much lower environmental impact than other people in their neighborhood.
\end{quotation}

\textbf{Question:}
\begin{quotation}
How much is the environment a high priority for this family? Choose an integer between 1 (not at all) and 11 (very much).
\end{quotation}
\end{tcolorbox}

\subsubsection{Beliefs about Racial Discrimination~\citep{haaland2023beliefs}} 

\textbf{Abstract:} This paper provides representative evidence on beliefs about racial discrimination and examines whether information causally affects support for pro-black policies. Eliciting quantitative beliefs about the extent of hiring discrimination against blacks, we uncover large disagreement about the extent of racial discrimination with particularly pronounced partisan differences. An information treatment leads to a convergence in beliefs about racial discrimination but does not lead to a similar convergence in support of pro-black policies. The results demonstrate that while providing information can substantially reduce disagreement about the extent of racial discrimination, it is not sufficient to reduce disagreement about pro-black policies.

\textbf{Data availability:}  The study is publicly available at: \url{https://www.tessexperiments.org/study/Haaland874}

\textbf{Data pre-processing:} The primary outcome variable is \texttt{Q2}. The treatment condition is defined as \texttt{GROUP = 1} (statistics of white-sounding and black-sounding names), and the control condition is defined as \texttt{GROUP = 2} (statistics of white-sounding names). The following variables are included as covariates: \texttt{PartyID7}, \texttt{INCOME}, \texttt{ATTEND}, \texttt{RELIG}, \texttt{GENDER}, \texttt{AGE}, \texttt{REGION9}, \texttt{RACETHNICITY}. The final processed dataset contains $n=1539$ observations.



\textbf{Prompting details:} 
An example prompt is provided below.

\begin{tcolorbox}[
    title=Example Prompt ,
    width=\textwidth,
    colback=white,
    colframe=pierCite,
    left=5pt,
    right=5pt,
    top=5pt,
    bottom=5pt
]
\textbf{System Prompt:}
\begin{quotation}
You are a 60-year-old, politically Independent, gender Female, ethnicity Hispanic. Additionally, your religion is just Christian and you never attend religious services. You live in a state of the West South Central region. Your household has a yearly income of \$30,000 to \$34,999. You are responding to a survey experiment collecting data on people's beliefs about racial discrimination and whether these beliefs affect people's views on affirmative action policies.
\end{quotation}

\textbf{Treatment condition:}
\begin{quotation}
Researchers from Harvard University conducted an experiment to study racial discrimination in the labor market. They did so by sending out fictitious resumes to help-wanted ads in Boston newspapers.
The resumes were exactly the same except for one thing: the name of the job applicant. Half of the resumes had typically white-sounding names like “Carrie” and “Todd”. The other half of the resumes had typically black-sounding names like “Tanisha” and ``Kareem''. The idea was to make sure that the applicants were seen as having identical qualifications, but that the employers would use the applicants’ names to infer whether they were white or black.
Resumes with white-sounding names had to be sent out on average 10 times to get one callback for an interview. 

Further, the researchers found that resumes with black-sounding names on average had to be sent out 15 times to get one callback for an interview. Since resumes with white-sounding names on average only had to be sent out 10 times to get one callback for an interview, this means that employers were 50 percent more likely to give callbacks to applicants with white-sounding names compared to applicants with black-sounding names.  
\end{quotation}

\textbf{Control condition:}
\begin{quotation}
Researchers from Harvard University conducted an experiment to study racial discrimination in the labor market. They did so by sending out fictitious resumes to help-wanted ads in Boston newspapers.
The resumes were exactly the same except for one thing: the name of the job applicant. Half of the resumes had typically white-sounding names like “Carrie” and “Todd”. The other half of the resumes had typically black-sounding names like “Tanisha” and “Kareem”. The idea was to make sure that the applicants were seen as having identical qualifications, but that the employers would use the applicants’ names to infer whether they were white or black.
Resumes with white-sounding names had to be sent out on average 10 times to get one callback for an interview. 
\end{quotation}

\textbf{Question:}
\begin{quotation}
In the United States today, do you think that racial discrimination against blacks in the labor market is a serious problem?
Reply with a JSON numerical answer using one of these numbers:
1 (A very serious problem), 2 (A serious problem), 3 (A problem), 4 (A small problem), or 5 (Not a problem at all).
\end{quotation}
\end{tcolorbox}

\subsubsection{To Do, to Have, or to Share? Valuing Experiences and Material Possessions by Involving Others~\citep{caprariello2013have}} 

\textbf{Abstract:} Recent evidence indicates that spending discretionary money with the intention of acquiring life experiences-events that one lives through-makes people happier than spending money with the intention of acquiring material possessions-tangible objects that one obtains and possesses. We propose and show that experiences are more likely to be shared with others, whereas material possessions are more prone to solitary use and that this distinction may account for their differential effects on happiness. In 4 studies, we present evidence demonstrating that the inclusion of others is a key dimension of how people derive happiness from discretionary spending. These studies showed that when the social-solitary and experiential-material dimensions were considered simultaneously, social discretionary spending was favored over solitary discretionary spending, whereas experiences showed no happiness-producing advantage relative to possessions. Furthermore, whereas spending money on socially shared experiences was valued more than spending money on either experiences enacted alone or material possessions, solitary experiences were no more valued than material possessions. Together, these results extend and clarify the basic findings of prior research and add to growing evidence that the social context of experiences is critical for their effects on happiness.

\textbf{Data availability:}  The study is publicly available at: \url{https://www.tessexperiments.org/study/caprariello130}

\textbf{Data pre-processing:} The primary outcome variable is \texttt{Q7A}. The treatment condition is defined as \texttt{XTESS086 = 1} (spend money with people), and the control condition is defined as \texttt{XTESS086 = 2} (spend money alone). The following variables are included as covariates: \texttt{XPARTY7}, \texttt{XREL1}, \texttt{XREL2}, \texttt{XIDEO}, \texttt{PPAGE}, \texttt{PPGENDER}. The final processed dataset contains $n=397$ observations.

\textbf{Prompting details:} 
An example prompt is provided below.

\begin{tcolorbox}[
    title=Example Prompt ,
    width=\textwidth,
    colback=white,
    colframe=pierCite,
    left=5pt,
    right=5pt,
    top=5pt,
    bottom=5pt
]
\textbf{System Prompt:}
\begin{quotation}
You are a 53-year-old, not so strong Republican, gender Male, and hold moderate views. Additionally, regarding religion you are Buddhist and you more than once a week attend religious services. You are responding to a survey on how you spend your discretionary money. Your answer must be a single integer without additional text, in JSON format with a key-value pair.
\end{quotation}

\textbf{Treatment condition:}
\begin{quotation}
We are interested in ways you spend your discretionary money. Discretionary money refers to money that is spent on anything that is NOT essential to basic activity (that is, essentials refer to things like tuition and textbooks, groceries, transportation, rent, gas for a car, health care, etc.). We'd like you to answer the questions that follow for money that you spent on something discretionary. Please think of the last time you spent at least \$10 (but no more than \$10,000) of your discretionary money in order TO DO SOMETHING WITH AT LEAST ONE OTHER PERSON. The primary focus of this expense should have been on an activity – doing something with at least one other person – and not on buying something that could be kept. Maybe you bought tickets to see a movie with some people, maybe you paid to visit an art museum with friends, maybe you and some other people went to a spa together … any of these would be legitimate examples of spending money to do something with others.
\end{quotation}

\textbf{Control condition:}
\begin{quotation}
We are interested in ways you spend your discretionary money. Discretionary money refers to money that is spent on anything that is NOT essential to basic activity (that is, essentials refer to things like tuition and textbooks, groceries, transportation, rent, gas for a car, health care, etc.). We'd like you to answer the questions that follow for money that you spent on something discretionary. Please think of the last time you spent at least \$10 (but no more than \$10,000) of your discretionary money in order TO DO SOMETHING BY YOURSELF. The primary focus of this expense should have been on an activity – doing something by yourself – and not on buying something that could be kept. Maybe you bought a ticket to see a movie by yourself, maybe you paid to enter an art museum, maybe you went to a spa by yourself … any of these would be legitimate examples of spending money to do something by yourself.
\end{quotation}

\textbf{Question:}
\begin{quotation}
Think about the last time you used your possession. To what extent did it help you feel loved and cared about?
Reply with a JSON numerical answer using one of these numbers:
1 (not at all), 2 (slightly), 3 (moderately), 4 (very), or 5 (extremely).
\end{quotation}
\end{tcolorbox}

\subsubsection{Onset and Offset Controllability in Perceptions and Reactions to Home Mortgage Foreclosures~\citep{brandt2013onset}} 

\textbf{Abstract:} The circumstances and rhetoric surrounding home foreclosures provide an ideal and timely backdrop for an extension of research on attributional judgments. While people face foreclosure for many reasons, the current debate surrounding the mortgage crisis has highlighted reasons that are either onset or offset controllable; that is, the initial cause, or the subsequent solution may be seen as controllable.In the current study, I examine how people use attributional evidence from multiple time points to determine affective reactions and helping intentions for people undergoing foreclosure, as well as ideological differences in these attributional processes. Participants read about people who were undergoing foreclosure for onset and offset controllable or uncontrollable reasons and then answer questions about their perceptions of these targets. The results suggested that both onset and offset controllable information contributed to the emotional reactions and helping intentions of the participants with the participants experiencing more negative affect and less helping intentions when the target was in a controllable onset or offset situation. Conservatives primarily relied on onset controllability information to decide who should receive government aid, while liberals updated their initial attributions with offset controllability information.

\textbf{Data availability:}  The study is publicly available at: \url{https://www.tessexperiments.org/study/brandt708}

\textbf{Data pre-processing:} The primary outcome variable is \texttt{Q7}. The treatment condition is defined as \texttt{XTESS003 = 1} (family can afford the mortgage), and the control condition is defined as \texttt{XTESS003 = 2} (family might not afford the mortgage). The following variables are included as covariates: \texttt{XPARTY7}, \texttt{XREL1}, \texttt{XREL2}, \texttt{PPAGE}, \texttt{PPGENDER}. The final processed dataset contains $n=624$ observations.

\textbf{Prompting details:} 
An example prompt is provided below.

\begin{tcolorbox}[
    title=Example Prompt ,
    width=\textwidth,
    colback=white,
    colframe=pierCite,
    left=5pt,
    right=5pt,
    top=5pt,
    bottom=5pt
]
\textbf{System Prompt:}
\begin{quotation}
You are a 75-year-old, not so strong Democrat, gender Female. Additionally, regarding religion you are a Muslim and you once a week attend religious services. You are responding to a survey on perceptions towards people who are facing foreclosure. Your answer must be a single integer without additional text, in JSON format with a key-value pair.
\end{quotation}

\textbf{Treatment condition:}
\begin{quotation}
Recently the growing number of home foreclosures has put a strain on the financial system, which has weakened the United States economy. Foreclosure occurs when a person is behind on home mortgage payments to their bank and the bank decides to repossess (i.e., take back) the home. People may go into foreclosure for a variety of reasons.
We are interested in your perceptions towards people who are facing foreclosure. In the following section you will be presented with a situation that describes some people facing foreclosure. Please carefully read the situation and answer the following questions about your reactions to the situation.
Some people have a large monthly mortgage payment because they wanted to purchase a larger house than they needed. Now they are facing foreclosure because they do not want to continue paying the mortgage, even though they are able to afford the payments.
\end{quotation}

\textbf{Control condition:}
\begin{quotation}
Recently the growing number of home foreclosures has put a strain on the financial system, which has weakened the United States economy. Foreclosure occurs when a person is behind on home mortgage payments to their bank and the bank decides to repossess (i.e., take back) the home. People may go into foreclosure for a variety of reasons.
We are interested in your perceptions towards people who are facing foreclosure. In the following section you will be presented with a situation that describes some people facing foreclosure. Please carefully read the situation and answer the following questions about your reactions to the situation.
Some people have a large monthly mortgage payment because they wanted to purchase a larger house than they needed. Now they are facing foreclosure because the primary income earner in the household lost their job due to their company closing and they can no longer afford payments.
\end{quotation}

\textbf{Question:}
\begin{quotation}
Do you strongly oppose or strongly support the following statement: The government should offer help (e.g., time, money, resources, etc.) in an effort to help people in this situation.
Reply with an integer from 1 (Strongly Oppose) to 7 (Strongly Support), where 4 is a Neutral stance.
\end{quotation}
\end{tcolorbox}

\subsubsection{Testing a Theory of Hybrid Femininity
~\citep{melin2022women}} 

\textbf{Abstract:} Although men experience advantages working in highly feminized occupations, they are commonly stigmatized as lesser men by outsiders—the people they meet outside of their occupations—for doing ``women’s work.'' This experiment is designed to assess whether a woman who has worked in a hypermasculine occupation would similarly be stigmatized as a lesser woman by workers outside of her hypermasculine occupation, or alternatively, whether she would be viewed more favorably by such outsiders for doing ``men’s work.'' Specifically, this study aims to develop and empirically test a theory of hybrid femininity, which specifies the conditions under which hypermasculinity as signaled through occupation creates status and reward distinctions among women in external labor markets. The experiment asks respondents to provide recommended compensation and status ratings for a woman candidate while manipulating the gender-typing of her occupational history as well as her intended target job. By disentangling the underlying mechanisms driving these predicted status and reward differences, this study seeks to shed light on how gender inequality persists, even among women, through the privileging of masculinity over femininity, with important implications for the labor market and society at large.

\textbf{Data availability:}  The study is publicly available at: \url{https://www.tessexperiments.org/study/melin1066}

\textbf{Data pre-processing:} The primary outcome variable is \texttt{Q7\_1}. The treatment condition is defined as \texttt{P\_41 = 3} (applicant has experience in the Army), and the control condition is defined as \texttt{P\_41 = 6} (applicant has experience in the Cosmetics industry). The following variables are included as covariates: \texttt{P\_IDEO}, \texttt{P\_ATTEND}, \texttt{P\_RELIG}, \texttt{RELIG}, \texttt{GENDER}, \texttt{AGE}, \texttt{REGION9}, \texttt{RACETHNICITY}, \texttt{INCOME}, \texttt{P\_PARTYID}. The final processed dataset contains $n=545$ observations.

\textbf{Prompting details:} 
An example prompt is provided below.

\begin{tcolorbox}[
    title=Example Prompt ,
    width=\textwidth,
    colback=white,
    colframe=pierCite,
    left=5pt,
    right=5pt,
    top=5pt,
    bottom=5pt
]
\textbf{System Prompt:}
\begin{quotation}
You are a 30-year-old, politically Independent, gender Male, ethnicity Hispanic. Your ideology is slightly liberal. Additionally, your religion is Protestant and you about once a month attend religious services. You live in a state of the Pacific region. Your household has a yearly income of \$85,000 to \$99,999. This task is part of a larger study on the design of Human Resources (HR) recruiting practices to pre-screen job applicants. Your answer must be a single integer without additional text, in JSON format with a key-value pair.
\end{quotation}

\textbf{Treatment condition:}
\begin{quotation}
Please imagine you work for a prominent management consulting company. You will be provided with a job description and an applicant’s résumé who is applying for a Senior Manager position. After thoroughly reviewing the job description and the applicant’s résumé, you will be asked to provide your immediate and uncensored opinion. Job description for your review:

\texttt{[Job description, check original paper for details]}

% Senior Manager (Consulting)
% Responsible for: 
% - Leading high performance project teams across the organization
% - Building professional relationships with key stakeholders
% - Defining project objectives, roadmaps, and deliverables 
% - Aligning project tactics with project strategy for all new services
% The successful applicant will be hard-working, results-oriented, and a team player.
% Required Qualifications:
% • Bachelor’s degree in Business Administration or a related field 
% • 3-5 years of related experience
% • Comfort with travel regionally or globally (up to 30% of time)
% • Self-motivated with potential for leadership
% • Excellent communication skills
% • Solid computer skills, including Microsoft software products 

Applicant’s résumé for your review:

Name: Amy Decker
Motivated Project Manager with 5 years of experience working in military and defense.
Education:
Rutgers University (New Brunswick, NJ), May 2017 (Graduated)
B.A. in Business Administration, GPA: 3.72/4.00
Work Experience:
U.S. Army Project Manager (Active-duty Enlisted), 2014 - Present
Fort Dix Military Base (Fort Dix, NJ)
- Plan and track progress of entire life-cycle of military and defense projects.
- Build and maintain project plans, including actual and forecasted activities and timelines.
- Ensure project staffing and timely communications throughout project lifecycle.
- Identify and manage project risks.
Skills and Interests:
Computer: Proficient in Microsoft Office (including Word, Excel, Outlook, and PowerPoint).
Interests: Running and traveling.
\end{quotation}

\textbf{Control condition:}
\begin{quotation}
Please imagine you work for a prominent management consulting company. You will be provided with a job description and an applicant’s résumé who is applying for a Senior Manager position. After thoroughly reviewing the job description and the applicant’s résumé, you will be asked to provide your immediate and uncensored opinion. Job description for your review:

\texttt{[Job description, check original paper for details]}

Applicant’s résumé for your review:

Name: Amy Decker
Motivated Project Manager with 5 years of experience working in military and defense.
Education:
Rutgers University (New Brunswick, NJ), May 2017 (Graduated)
B.A. in Business Administration, GPA: 3.72/4.00
Work Experience
Cosmetics Project Manager  2014 - Present
Precious Cosmetics (Lodi, NJ)
- Plan and track progress of entire life-cycle of cosmetics and beauty product projects.
- Build and maintain project plans, including actual and forecasted activities and timelines.
- Ensure project staffing and timely communications throughout project lifecycle.
- Identify and manage project risks.
Skills and Interests:
Computer: Proficient in Microsoft Office (including Word, Excel, Outlook, and PowerPoint).
Interests: Running and traveling.
\end{quotation}

\textbf{Question:}
\begin{quotation}
On a scale from 1 ``Not at all'' to 7 ``Extremely'', to what extent do you perceive this applicant as MASCULINE.
\end{quotation}
\end{tcolorbox}

\subsubsection{Understanding White Identity Management in a Changing America~\citep{shuman2024defend}} 

\textbf{Abstract:} This paper examines how White Americans manage their identity amidst societal shifts using a new measure of advantaged identity management, representative data (N = 2648), and latent profile analysis. The findings reveal five subgroups of White Americans, each managing their identity differently. Four profiles correspond to the main advantaged identity management strategies (defend, deny, distance, dismantle), with a fifth using strategies flexibly. Of 15 predictions regarding how valuing hierarchy, meritocracy, and egalitarianism predict profile membership, 13 were supported. These profiles show contrasting attitudes toward social change, with defender-deniers opposing, denier-distancers moderately opposing, distancers remaining neutral, and dismantlers supporting change. These findings provide some of the first empirical evidence for a theorized model of white identity management and suggest that how White Americans manage their identity has important implications for social change.

\textbf{Data availability:}  The study is publicly available at: \url{https://www.tessexperiments.org/study/melin1066}

\textbf{Data pre-processing:} The primary outcome variable is \texttt{Q5D}. The treatment condition is defined as \texttt{RND\_01 = 1} (disadvantage back people), and the control condition is defined as \texttt{RND\_01 = 0} (advantage white people). The following variables are included as covariates: \texttt{AGE}, \texttt{GENDER}, \texttt{RACETHNICITY}, \texttt{EDUC5}, \texttt{REGION9}, \texttt{IDEO}, \texttt{PartyID7}, \texttt{RELIG}, \texttt{ATTEND}, \texttt{INCOME}. The final processed dataset contains $n=1623$ observations.

\textbf{Prompting details:} 
An example prompt is provided below.

\begin{tcolorbox}[
    title=Example Prompt ,
    width=\textwidth,
    colback=white,
    colframe=pierCite,
    left=5pt,
    right=5pt,
    top=5pt,
    bottom=5pt
]
\textbf{System Prompt:}
\begin{quotation}
You are a 41-year-old individual with gender Male, ethnicity Asian, and with Bachelor's degree education. You live in a state of the New England region. You hold  Moderate views and are not so strong Democrat. Additionally, your religion is Atheist and you attend religious services never. Your household has a yearly income of \$175,000 to \$199,999.
\end{quotation}

\textbf{Treatment condition:}
The general purpose of this study is to examine the attitudes of people regarding social issues in America today. You will now be presented with an infographic:
\begin{quotation}
\begin{center}
\includegraphics[width=0.2\textwidth]{figures/dis_black.png}
\end{center}
\end{quotation}

\textbf{Control condition:}
The general purpose of this study is to examine the attitudes of people regarding social issues in America today. You will now be presented with an infographic:
\begin{quotation}
\begin{center}
\includegraphics[width=0.2\textwidth]{figures/adv_white.png}
\end{center}
\end{quotation}

\textbf{Question:}
\begin{quotation}
Rate the extent to which you agree with the following statement from 1 (STRONGLY DISAGREE) to 7 (STRONGLY AGREE): ``There should be large scale criminal justice reform to address racial inequalities in the justice system.'' Your answer must be in JSON format with a single key-value pair.
\end{quotation}
\end{tcolorbox}


\subsubsection{Introducing variability in multi-prompt experiments}
\label{apx:multiprompt}

The user prompts described in the previous section include a final question or instruction sampled from a predefined pool to introduce variability in the multi-prompt settings. Below are some examples of such instructions:

\begin{itemize}
    \item ``Consider all relevant factors and place this on the scale.''
    \item ``Reflect on the scenario and use your reasoning to assign a value.''
    \item ``From your understanding of the situation, quantify this feeling.''
    \item ``Given your insights and the context described, provide your evaluation.''
    \item ``With the provided details in mind, rate your feeling on the scale.''
    \item ``Consider all the information and your perspective to choose a suitable score.''
    \item ``Evaluate the feeling here and align a number with your reasoning.''
    \item ``Use the scale provided and your judgment to determine your feeling.''
    \item ``Judge this scenario thoughtfully, considering the context and the details shared.''
    \item ``Reflect on the key aspects provided and numerically assess your feeling.''
\end{itemize}
