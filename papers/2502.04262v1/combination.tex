\begin{algorithm*}[t]
\caption{\textbf{H}ybrid \textbf{A}ugmented \textbf{I}nverse \textbf{P}robability \textbf{W}eighting ($\ours$)}
\label{algo:haipw}
\begin{algorithmic}[1]
\REQUIRE (i) Dataset $\datarct = \{(X_i, A_i, Y_i)\}_{i=1}^n$. (ii) Collection of foundation models  $f_1,\ldots,  f_k$.  (iii) Loss function $\mathcal{L}$ and function class $\mathcal{H}$. 
(iv) $\pi_a$ for $a=0,1$.
(v) Significance level $\alpha$. 
\STATE Use cross-fitting to compute the estimate $\estaipw(\widehat h)$ from the dataset $\datarct$, where for each arm  $a$:
$$
\widehat{h}(X,a) \in \arg\min_{h\in\mathcal{H}} \left\{\frac{1}{n_a}\sum_{i:A_i=a}\mathcal{L}(h(X_i), Y_i)\right\}.
$$\vspace{-4mm}
\STATE Compute  $\widehat \lambda =   \widehat \Sigma^{-1} \mathbf 1 / (\mathbf 1^\top \widehat \Sigma^{-1} \mathbf 1) $,
where 
$$
\widehat \Sigma \defeq  \frac{1}{n}\sum_{i=1}^n \left((\psi_i(\widehat{h}) , ..., \psi_i(f_k)) - \bar  \psi\right)^\top \left((\psi_i(\widehat{h}), ..., \psi_i(f_k)) - \bar  \psi \right),~~\text{and}~\bar{\psi} \defeq \frac{1}{n} \sum_{i=1}^n (\psi_i( \widehat{h}), \dots, \psi_i( f_k)). 
$$
\STATE Compute the estimate and its variance 
\begin{align}
\label{eq:haipw}
& \esthaipw_{\hat \lambda} \defeq \widehat \lambda_1 \estaipw(\widehat h) + \sum_{j=1}^{k} \estaipw(f_j)~ \widehat \lambda_{j+1},~~\text{and}~~\widehat V_{\hat \lambda} \defeq {\widehat \lambda}^\top~ \widehat\Sigma~ \widehat\lambda.
\end{align}
\STATE \textbf{Return:}  $
\CC^\alpha_{\ours} = \left( \esthaipw_{\hat \lambda} \pm z_{1-\frac{\alpha}{2}} \sqrt{\frac{\widehat V_{\hat \lambda}}{n}} \right)$, where $z_\alpha$ is the $\alpha$-quantile of the standard normal distribution.
\end{algorithmic}
\end{algorithm*}




We introduce \textbf{H}ybrid \textbf{A}ugmented \textbf{I}nverse \textbf{P}robability \textbf{W}eighting (\ours), an estimator that, in contrast to the standard $\aipw$, leverages the predictions from multiple foundation models to improve statistical precision. \Cref{algo:haipw} provides a formal definition of the \ours~estimator; here, we first introduce the estimator and then give theoretical results for its asymptotic distribution and variance.

\subsection{Hybrid Augmented Inverse Probability Weighting}
With the recent widespread availability of foundation models, we can potentially improve the accuracy of the outcome regression estimator beyond what is obtained from~\Cref{eq:stdoutcome} simply by replacing it with a foundation model. Further, as is often the case with language models, multiple competing models may be available, with no clear way to determine the best choice for a given task in advance. Therefore, we propose combining multiple $\aipw$ estimators, each using a different outcome regression estimator.

More formally, we want to estimate the ATE $\ate$ based
on a collection of several $\aipw$ estimators: $$\estaipw(\widehat h), \estaipw(f_1),\ldots, \estaipw(f_k).$$ Here,  $\widehat h$ is estimated exclusively from experimental data as shown in~\Cref{eq:stdoutcome}, while $f_1,\ldots, f_k$ are foundation models trained on independent external data. The problem of dealing with several competing estimators of the
same quantity has been extensively studied in the statistics literature; see e.g. \citet{lavancier2016general}.
A common solution is to consider a weighted average of the available estimators, which in our setting corresponds to 
$$
\esthaipw_\lambda \defeq \lambda_{1} \estaipw(\widehat h) + \sum_{j=1}^k  \estaipw(f_j)\lambda_{j+1},~\text{for some}~\lambda \in \Lambda = \{\lambda \in \RR^{k+1}: \sum_{j=1}^{k+1} \lambda_j =1 \}.$$ We restrict the weights to the constraint set $\Lambda$ so that the combined estimator $\esthaipw_\lambda$ is still in the class of $\aipw$ estimators. We can then choose the weights that minimize the variance of the combined estimator, that is:
\begin{equation*}
 \lambda^\star = \underset{\lambda \in \Lambda}{\arg\min}~\var[ \esthaipw_\lambda] = \underset{\lambda \in \Lambda}{\arg\min}~ \lambda^\top  \Sigma \lambda =  \Sigma^{-1} \mathbf 1 / (\mathbf 1^\top  \Sigma^{-1} \mathbf 1) , 
\end{equation*}
where $\Sigma \defeq 
    \operatorname{Cov}[ (\psi(h^\dagger), \ldots, \psi(f_k))^\top] $
    is the asymptotic covariance  and $h^\dagger$ is the asymptotic limit of $\widehat h$. However, in practice, we only have access to an estimate $\widehat \Sigma$ of the covariance matrix, and thus we use  
   $$\widehat \lambda := \underset{\lambda \in \Lambda}{\arg\min}~ \lambda^\top \widehat \Sigma \lambda.$$ 
   %While doing so comes at a price in additional variance, \citet[Corollary 3.2]{lavancier2016general} demonstrated that this penalty is negligible under some mild assumptions. 
    
    
    %This holds in particular since the amount of candidate models $k$ is a small constant. 
    
    % introduces an 
    % We observe that since the amount of candidate models $k$ is a small constant, the error term from covariance matrix estimation is negligible; see~\Cref{apx:covestimation} for a discussion.

    
    
    % Estimating the covariance matrix from samples via the empirical covariance matrix $\hat \Sigma$ is a very well studied problem, which in turn motivates the following practical choice for\kd{say that although this introduces some error, this error is negligible. Then just cite the one paper that studies th eproblem carefully}


% We can then estimate the weights $\lambda$ that minimize the empirical variance of the combined estimator, that is
% \begin{equation}
% \widehat \lambda = \underset{\lambda \in \Lambda}{\arg\min}~\widehat \var[ \esthaipw_\lambda] = \underset{\lambda \in \Lambda}{\arg\min}~ \lambda^\top \widehat \Sigma \lambda, 
% \end{equation}
% where $\widehat \Sigma$ is an estimate of the asymptotic covariance matrix of the estimators \begin{equation}
%     \Sigma \defeq %n \operatorname{Cov}[ (\estaipw(h^\dagger), \ldots, \estaipw(f_k))^\top] =  
%     \operatorname{Cov}[ (\psi(h^\dagger), \ldots, \psi(f_k))^\top] ,
% \label{eq:covmatrix}\end{equation} and $h^\dagger$ is the asymptotic limit of $\widehat h$.
%The optimal $\widehat \lambda$ has a closed-form solution given in Algorithm~\ref{algo:haipw}. 
%At a first glance, it is not obvious how to estimate the covariance matrix in~\Cref{eq:covmatrix}. However, thanks to the linear structure of the $\aipw$ estimators it is possible to derive a consistent estimator. 





\paragraph{Asymptotic validity and efficiency}
We now establish that the \ours~estimator is both consistent and asymptotically normal, with an asymptotic variance that is no greater than that of the standard \aipw. 
\begin{theorem}[Asymptotic behavior of \ours]
\label{thm:combine}
Let $\widehat h$ be an estimator that satisfies the conditions in~\Cref{prop:rootn}, with asymptotic limit $h^\dagger$. Further, let $\esthaipw_{\hat \lambda}$ be as in~\Cref{eq:haipw}, and assume that  $\Sigma$ is non-singular and $\widehat \Sigma\Sigma^{-1}\overset{p}{\to}I$. Then, it holds that 
$$
\sqrt n (\esthaipw_{\hat \lambda}   - \ate) \rightsquigarrow \gauss(0, V_{\lambda^\star}).
$$
Moreover, the asymptotic variance of the combined estimator is no greater than that of any individual estimator, i.e. it holds that 
$$
V_{\lambda^\star} \leq \Sigma_{jj}, ~\text{for}~j=1,\ldots,k+1.$$
\end{theorem}
We provide a proof of this result in~\Cref{apx:proofhaipw}.
\Cref{thm:combine} offers a principled approach to combining multiple competing \aipw~estimators, ensuring that the resulting estimator is at least as precise as the best estimator in the ensemble. In particular, this approach allows us to leverage the strengths of foundation models without any risks: when these models give accurate outcome predictions, the combined estimator uses their extra information to improve precision. On the other hand, when the foundation models are biased, the final estimator falls back to the standard $\aipw$ estimator. 









