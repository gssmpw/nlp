We observe a dataset $\datarct$ of size $\nrct$ from a randomized experiment, containing tuples $(X,Y,A)$ of covariates $X \in \RR^\xdim$, bounded outcome $Y\in \RR$, and treatment variable $A \in \{0,1\}$. We assume that the data is drawn i.i.d. from  $\prct$ over $\left(X, Y(0), Y(1), Y,A\right)$, where $\left(Y(0), Y(1)\right) \in \RR^2$ are the potential outcomes.

Our goal is to estimate the average treatment effect~(ATE) in the randomized experiment population, 
$$
\ate \defeq \EE[Y(1) - Y(0)],
$$ where the expectation is taken over $\prct$. 
In particular, we want to improve upon the statistical precision of classical ATE estimators by constructing an asymptotically valid confidence interval that is as tight as possible. We first state below the sufficient conditions to identify the ATE in randomized experiments.
\begin{assumption}[Identification assumptions]
\label{asm:internalvalid}
The data-generating process satisfies
 \begin{align*}
(i)&\;\;   Y = Y(A),~ \prct -\mathrm{almost\;surely}. \\
(ii)&\;\;Y(a) \ind A,~\mathrm{for}~a =0,1. \\
(iii)& \;\; \pi_a = \prct(A =a) > 0,~\mathrm{for}~a =0,1.
 \end{align*}
\end{assumption} 
Condition (\textit{i}) holds when the intervention is well-defined, as is typical for protocol-driven treatments in clinical trials. Conditions (\textit{ii}) and (\textit{iii}) are directly supported by randomization in the study design. We further assume that the treatment assignment probability $\pi_a$ is known by design, as is the case in the overwhelming majority of experiments. Nevertheless,  our framework can easily be extended to allow for covariate-adaptive randomization or settings in which the probability of treatment needs to be estimated.

Under~\Cref{asm:internalvalid}, we can identify the ATE as follows $$\ate = \EE[Y(1) - Y(0)] = \EE[Y\mid A=1]-\EE[Y\mid A=0].$$
Therefore, the standard approach is to estimate  $\theta$ using the difference in means estimator, defined as 
$$
\estdm \defeq \frac{1}{n} \sum_{i\in \datarct} \left( \frac{Y_iA_i}{\pi_1} - \frac{Y_i(1-A_i)}{\pi_0} \right).
$$
This estimator is consistent and asymptotically normal (see e.g.~\citet[Theorem 1.2]{wager2024causal}): 
$$
\sqrt n (\estdm - \ate) \rightsquigarrow \gauss(0, \avardm),
$$
where $\rightsquigarrow$ denotes convergence in distribution and $\avardm$ is the asymptotic variance. Therefore, provided that we can obtain a consistent estimator of the asymptotic variance, $\avardmhat = \avardm + o_{\p}(1)$, we can construct an asymptotically valid confidence interval 
\begin{align}
\label{eq:ci}
\confdm^\alpha = \left(\estdm \pm z_{1-\frac{\alpha}{2}} \sqrt{\frac{\avardmhat}{n}}\right),
\end{align}
such that $\lim_{n \to \infty }\p (\ate \in \confdm^\alpha) \geq 1-\alpha$, where $z_\alpha$ is the $\alpha$-quantile of the standard normal distribution. Arguably, $\estdm$ is all that is
needed to estimate average treatment effects in randomized experiments. However, the confidence interval $\confdm^\alpha$ is often very wide, and it is possible to obtain narrower confidence intervals if we leverage the information contained in the covariates, as we will see in the next section.


\subsection{A class of valid estimators: Augmented Inverse Probability Weighting}

\citet{robins1994estimation}  show that all estimators of $\ate$ that are consistent and asymptotically normal are asymptotically equivalent (when the propensity score is known) to the \aipw~estimator, defined as 
\begin{equation*}
    \estaipw(h) \defeq \frac{1}{n} \sum_{i \in \datarct } \psi_i(h),
    \label{eq:aipw}
\end{equation*}
where $h:\RR^d \times \{0,1\} \to \RR$ is a square-integrable function, and  we define
$$
\psi_i(h) \defeq  \left(\frac{A_i}{\pi_1}(Y_i - h(X_i, 1)) + h(X_i,1)\right) - \left(\frac{1-A_i}{\pi_0}(Y_i - h(X_i,0)) + h(X_i,0)\right).
$$
The most efficient estimator within this class can be identified by minimizing the asymptotic variance with respect to the function $h$. Specifically, the semiparametric efficiency lower bound is attained by choosing $h^\star(x,a) = \EE[Y|X=x, A=a]$, which corresponds to the 
conditional mean of the outcome, also referred to as the \textit{outcome regression}. In other words, the estimator $\estaipw(h^\star)$ attains the smallest asymptotic variance among all consistent and asymptotically normal estimators of $\ate$, and, thus, the smallest possible confidence interval in large samples. In practice, however, we  have an estimator of the outcome regression $\widehat h$, which achieves the efficiency lower bound only if $||\widehat h - h^\star||_{L_2(\mathbb P)} = o_{\mathbb P}(1)$. 



Below, we adapt the standard result that establishes consistency and asymptotic normality of the \aipw~estimator to our setting, where the treatment probability 
 is known. The key distinction from the standard setting is that asymptotic normality is achieved irrespective of the convergence rate of the outcome regression  estimators. This means that the confidence intervals are valid even when the outcome regression is estimated using complex machine learning models, including those with unknown convergence rates.
\begin{proposition}[Asymptotic behavior of \aipw] \label{prop:rootn} Let $\widehat h$ be the outcome regression estimator, and $h^\dagger$ be its asymptotic limit, i.e. a square-integrable function such that  
$$
||\widehat h(\cdot, a) - h^\dagger(\cdot, a)||_{L_2(\p)} = o_{\mathbb P}(1),~\text{for}~ a=0,1.
$$
Assume that $\widehat h$ is estimated from an independent sample, e.g. using cross-fitting.
Then, it follows that $\estaipw(\widehat h)$ is root-n consistent and asymptotically normal:
$$
 \sqrt n (\estaipw(\widehat h)  - \ate) \rightsquigarrow \gauss(0, V_{h^\dagger}),$$
 where $V_{h^\dagger}=\EE\left[\left(\psi_i(h^\dagger)  - \theta \right)^2\right]$ is the asymptotic variance. 
\end{proposition}
We provide a proof of this result in~\Cref{apx:proofaipw}.
\Cref{prop:rootn} shows that the choice of estimator for the outcome regression does not affect the validity of the inference, provided that it is independent  from the experimental data---for example, by using cross-fitting. Under these conditions, we can then construct an asymptotically valid confidence interval \(\confaipw^\alpha\) as outlined in~\Cref{eq:ci}.  

However, because the asymptotic variance depends on the limiting function $h^\dagger$, with the smallest variance being achieved by the outcome regression $h^\star$, the choice of the outcome regression estimator is key to obtain precise estimates.  The standard machine learning paradigm applied to our setting would first choose an appropriate model class  $\mathcal H$ (e.g. all linear functions) and loss function $\mathcal L$ (e.g. mean squared loss), and
minimize the empirical risk separately for each treatment arm $a$:
\begin{align}
\label{eq:stdoutcome}
\widehat h(X,a) \in \underset{h \in \HH}{\arg\min}~\frac{1}{n_a} \sum_{i: A_i =a} \mathcal L(Y_i,h(X_i)).
\end{align}
We refer to the estimator $\estaipw(\widehat h)$, where $\widehat h$ is obtained from solving the optimization problem in~\Cref{eq:stdoutcome}, as the \emph{standard} $\aipw$ estimator. The question is, can we improve upon this estimator by incorporating foundation models trained on vast amounts of external data? In the following section, we present an approach that leverages access to multiple competing foundation models to reduce the finite sample (and potentially asymptotic) variance of the standard $\aipw$ estimator.












