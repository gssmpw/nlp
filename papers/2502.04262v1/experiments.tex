
In this section, we first show that \ours~consistently improves precision across three randomized experiments, reducing variance by up to 30\% in small sample settings (50 participants). 
We then evaluate the performance of several LLMs and highlight the significance of model scale and inference-time compute: larger models, e.g. GPT-4o and LLaMA 3 70B, consistently outperform their smaller counterparts in terms of prediction accuracy (model scale), and averaging over several different prompts at inference time improves the accuracy of model predictions  (inference-time compute).
\subsection{H-AIPW offers improved precision}
\label{sec:main_experiments}

We evaluate $\ours$ across three randomized experiments in Foreign Policy \citep{silverman2022putting} and Sociology \citep{kennedy2020accidental,melin2022women}. These studies were selected from the multidisciplinary  Time-Sharing Experiments in the Social
Sciences (TESS) repository, along the lines of~\citet{ashokkumar2024predicting}. We analyze $5$ other studies in~\Cref{apx:more_studies}, including one in which the treatment is an image instead of text.
For each experimental study, we implement the following subsampling procedure: starting with a full dataset \(\mathcal{D}\) of size $N$, we select a target sample size \(n \in \{50, \dots, 200\}\). For each repetition \(r \in \{1, \dots, R\}\), we sample \(n\) participants without replacement from \(\mathcal{D}\), ensuring the treatment and control groups are balanced, to create a smaller dataset \(\mathcal{D}_r\).

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.67\textwidth]{legend.pdf}\vspace{4mm}\\
    % First Row
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/Melin_eff.pdf}
        \caption{}
        \label{fig:melin_eff}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Silverman1035_eff}
        \caption{}
        \label{fig:silverman_eff}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/Kennedy1017_eff.pdf}
        \caption{}
        \label{fig:kennedy_eff}
    \end{subfigure}
    \vspace{-0.38cm} % Space between rows
    % Second Row
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/Melin_cov.pdf}
        \caption{}
        \label{fig:melin_cov}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Silverman1035_cov}
        \caption{}
        \label{fig:silverman_cov}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/Kennedy1017_cov.pdf}
        \caption{}
        \label{fig:kennedy_cov}
    \end{subfigure}\vspace{2mm}
    
    \caption{\small{Performance comparison of \ours~against baseline estimators (\ppct, \dm, \aipw) across three randomized experiments. We randomly subsample each study to obtain the sample sizes shown on the x-axis and report the average over $R=10k$ repetitions for each metric. The significance level is set to $\alpha=0.05$. \textbf{(First row) Precision:}  \Cref{fig:kennedy_eff,fig:silverman_eff,fig:melin_eff} show the empirical variance achieved by $\ours$ and the baseline estimators for varying sample sizes. \textbf{(Second row) Validity:} \Cref{fig:kennedy_cov,fig:silverman_cov,fig:melin_cov} show the empirical coverage probability of each estimator for varying sample sizes; the dashed horizontal line represents the nominal 95\% coverage level.}
     }
    \label{fig:main_results}
\end{figure*}


\begin{figure*}[t]
 \centering
    \includegraphics[width=\textwidth]{legend_llms.pdf}\\
 \vspace{4mm}
    \begin{subfigure}[t]{0.6\textwidth}
        \includegraphics[width=\textwidth]{figures/fahey_scaling.pdf}
        \caption{}
        \label{fig:scaling_laws}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.33\textwidth}
        \includegraphics[width=\textwidth]{fahey_mse}
        \caption{}
        \label{fig:test_compute}
    \end{subfigure}
\caption{\small{Impact of model scale and inference-time compute on the performance of $\ours$ in the study by \citet{fahey2023principled}. \textbf{(Left) Model scale}: \Cref{fig:scaling_laws} shows the relationship between the empirical estimate of the $\ours$ variance (average on $R=10k$ repetitions, sample size $n=50$) and mean squared error (MSE) for LLMs of varying sizes ($10$ prompts at inference time). \textbf{(Right) Inference-time compute}: \Cref{fig:test_compute} shows the impact on the MSE of increasing the number of prompts at inference time and averaging the resulting predictions.}
}
\end{figure*}



\paragraph{Estimators and Metrics}  We implement $\ours$ by integrating predictions from three popular LLMs: GPT-4o, Claude 3.5 Haiku, and LLaMA 3 70B. For each LLM, we use $10$ different prompts for prediction and average over the responses~(we provide example prompts in~\Cref{apx:prompts}). We benchmark our estimator against
two standard estimators: $\estdm$~(\dm) and $\estaipw(\widehat h)$~(\aipw), where $\widehat h$ is the solution to the optimization problem in~\Cref{eq:stdoutcome}. We also implement the concurrent $\ppct$ estimator~\citep{poulet2025prediction} using GPT-4o as the base model. This serves as a more competitive baseline that also leverages predictions from foundation models (see~\cref{apx:implementation} for complete implementation details). To benchmark precision, for each estimator $\widehat \theta$, we compute the scaled variance $\frac{1}{R} \sum_{r=1}^R n \widehat \var [\widehat \theta_r]$, where $\widehat \var$  is the  the empirical variance estimate obtained from  the dataset $\data_r$---as the sample size grows, the scaled variance approaches the asymptotic variance of the corresponding estimator.
To benchmark validity, for each estimator, we compute the fraction of confidence intervals containing the ATE:
$
\operatorname{Coverage} = \frac{1}{R}  \sum_{r=1}^R \indi \{ \theta \in \mathcal C_r ^\alpha \},
$
where $\CC_r^\alpha$ is the confidence interval  obtained from the dataset $\mathcal D_r$
and $\theta$ is the ground-truth ATE estimate from the full study. %(with significantly larger sample size).





\paragraph{Results}
\Cref{fig:kennedy_eff,fig:silverman_eff,fig:melin_eff} show that $\ours$ consistently achieves lower variance---and hence tighter confidence intervals---than the baselines across all studies and sample sizes. For small sample sizes, $\ours$ yields reductions in variance ranging from 5\% to 30\%, depending on the study and baseline. This trend aligns with statistical theory, as the outcome regression's estimation error is high in small sample sizes, increasing the finite sample variance of the standard $\aipw$ estimator. For large sample sizes, the gains against the standard $\aipw$ plateau at 2\% to 3\%. This suggests that beyond finite sample improvements, there are also asymptotic gains due to potential model misspecification in the standard $\aipw$ outcome regression.
Interestingly, we observe that the $\ppct$ estimator can be less precise than the standard $\aipw$ estimator. This can be explained by noting that $\ppct$ is only guaranteed (asymptotically) to be at least as precise as the difference in means estimator~($\dm$). Finally, while \Cref{thm:combine} establishes asymptotic validity of the $\ours$ confidence intervals, we confirm that its precision gains do not come at the cost of validity in finite sample settings: \Cref{fig:melin_cov,fig:silverman_cov,fig:kennedy_cov} show that $\ours$ maintains coverage comparable to the baselines. 




\subsection{Improving the accuracy of LLMs}
\label{sec:improving_accuracy}
We now study how two popular strategies for improving the accuracy of LLMs—model scale and inference-time compute—affect the precision of the $\ours$ estimator. In our setting, increasing inference-time compute boils down to presenting slight variations of the same prompt at inference time and averaging over the 
 responses. We provide the complete list of the prompts used in~\Cref{apx:multiprompt}. For each LLM $f$, we evaluate prediction performance using the Mean Squared Error (MSE) on the full dataset: $\frac{1}{N} \sum_{i=1}^N (f(X_i,A_i) - Y_i)^2$. Our findings indicate that larger models and increased inference-time compute can improve prediction accuracy, which in turn can reduce the variance of \ours.



\paragraph{Model scale} 
\Cref{fig:scaling_laws} illustrates the precision gains achieved by $\ours$ when leveraging predictions from LLMs of varying scales. We study the relationship between MSE and the empirical estimate of the $\ours$ variance when integrating predictions from \textit{small} models (LLaMA 3 8B, Gemma 2 9B, Phi-4, Gemma 2 27B) and \textit{large} models (LLaMA 3 70B, GPT-4o, Gemini 1.5 Flash, DeepSeek-V3, Claude 3.5 Haiku, Grok 2). Large models consistently achieve lower MSE and thus lower variance than smaller models, with LLaMA 3 70B excelling despite having fewer parameters than  GPT-4o and Claude 3.5 Haiku. 

\paragraph{Inference-time compute} 
\Cref{fig:test_compute} shows that averaging over many prompts consistently reduces the MSE for the large models---a similar trend is expected for the smaller ones. As smaller MSE is associated with higher precision (see~\Cref{fig:scaling_laws}), using multiple prompts is expected to improve the precision of $\ours$ further.
We confirm this observation in~\Cref{apx:ablation_testtime}, showing that $\ours$ precision improves with more prompts across several randomized studies.

