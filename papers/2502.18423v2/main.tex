\documentclass[10pt, a4paper, logo, twocolumn, copyright]{psibot} %

\usepackage{times}

\usepackage{amsmath,amsfonts,bm}


\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} %
\usepackage{wrapfig} %
\usepackage{enumitem}
\usepackage[htt]{hyphenat}
\usepackage{fontawesome}
\usepackage{makecell}
\usepackage{caption}
\usepackage{hyperref}

\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[most]{tcolorbox}
\usepackage{moresize}
\usepackage{listings}
\usepackage{courier}
\usepackage{array}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
% add package by fengshuo
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{float} % subfloat
\usepackage{cuted}
\usepackage{arydshln}
\usepackage{parskip}
\usepackage{float}
\usepackage{multirow}
\usepackage{makecell}
% \usepackage{minitoc}
% \usepackage{enumitem}
\usepackage{caption}
\captionsetup{labelfont=bf}

\usepackage[capitalize,noabbrev]{cleveref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\methodname}{Mind Evolution}
\newcommand{\refine}{Refinement through Critical Conversation}
\newcommand{\rcc}{RCC}
\newcommand{\sequential}{Sequential-Revision+}

\newcommand{\steg}{StegPoet}
\newif\ifsteg
\stegtrue

\usepackage[numbers]{natbib}
\bibliographystyle{unsrtnat}

\input{math_commands}
\newcommand{\stdv}[1]{\scalebox{.70}{~$\pm$~#1}}

\newcommand{\fengshuo}[1]{{\textcolor[RGB]{68,132,243}{[fengshuo: #1]}}}
\newcommand{\ying}[1]{{\color{blue}{[ying: #1]}}}
\newcommand{\yuanpei}[1]{{\color{red}{[yuanpei: #1]}}}

\title{Retrieval Dexterity: Efficient Object Retrieval in Clutters with Dexterous Hand}

\author[1,2]{Fengshuo Bai}
\author[2,3]{Yu Li}
\author[1,2]{Jie Chu}
\author[2,3]{Tawei Chou}
\author[3]{Runchuan Zhu}
\author[1,\dag]{Ying Wen}
\author[2,3,\dag]{Yaodong Yang}
\author[2,3,\dag]{Yuanpei Chen}
\correspondingauthor{Yuanpei Chen\{yuanpei.chen312@gmail.com\},Yaodong Yang\{yaodong.yang@pku.edu.cn\}, Ying Wen\{ying.wen@sjtu.edu.cn\}}

\affil[1]{Shanghai Jiao Tong University}
\affil[2]{PKU-PsiBot Joint Lab}
\affil[3]{Peking University}

\begin{document}

\maketitle

\begin{strip}
    \centering
    \includegraphics[width=0.99\linewidth]{figs/teaser.pdf}
    \captionof{figure}{We present \textbf{Retrieval Dexterity}, a system that learns efficient object retrieval in \small\textbf{\textcolor[HTML]{82b0f8}{simulation}} and demonstrates zero-shot \textcolor[HTML]{f39890}{real-world} deployment.}
    \label{fig:teaser}
\end{strip}

\begin{abstract}
Retrieving objects buried beneath multiple objects is not only challenging but also time-consuming. Performing manipulation in such environments presents significant difficulty due to complex contact relationships. Existing methods typically address this task by sequentially grasping and removing each occluding object, resulting in lengthy execution times and requiring impractical grasping capabilities for every occluding object. In this paper, we present a dexterous arm-hand system for efficient object retrieval in multi-object stacked environments. Our approach leverages large-scale parallel reinforcement learning within diverse and carefully designed cluttered environments to train policies. These policies demonstrate emergent manipulation skills (e.g., pushing, stirring, and poking) that efficiently clear occluding objects to expose sufficient surface area of the target object. We conduct extensive evaluations across a set of over 10 household objects in diverse clutter configurations, demonstrating superior retrieval performance and efficiency for both trained and unseen objects. Furthermore, we successfully transfer the learned policies to a real-world dexterous multi-fingered robot system, validating their practical applicability in real-world scenarios. Videos can be found on our project website ~\href{https://ChangWinde.github.io/RetrDex}{\color[HTML]{2D3C8F}{https://ChangWinde.github.io/RetrDex}}.
\end{abstract}

\section{Introduction}
% Cluttered environments, such as messy household drawers, disorganized office desks, or packed warehouses, pose significant challenges for robotic object retrieval. Efficiently identifying, locating, and extracting specific objects from such scenes is a fundamental skill for autonomous robot systems. This capability is crucial for enabling robots to operate effectively in real-world applications, including domestic assistance and warehouse logistics. For instance, retrieving a key buried under miscellaneous items or locating a book obscured by other objects highlights the complexity and practical importance of this task.

% Despite its critical role, existing research has predominantly focused on employing parallel grippers for object retrieval. While effective in certain scenarios, parallel grippers encounter significant limitations in cluttered environments with restricted spaces, occluded targets, and intricate object support relationships. In contrast, dexterous hands hold substantial promise for overcoming these challenges. Their multi-degree-of-freedom structure and human-like flexibility enable a wide range of actions, such as pushing, moving, uncovering, and gradually exposing target objects. These advanced manipulation capabilities allow for fine-grained control and minimal disturbance to surrounding objects, making dexterous hands particularly effective in unstructured and confined environments. By leveraging these advantages, dexterous hands present a transformative solution to object retrieval tasks, addressing the limitations of traditional grippers while enhancing precision and operational efficiency.

% 传统/cv/之前的retrieval 引入rl的做法
% Despite their utility, parallel grippers face significant challenges in cluttered environments with restricted spaces, occluded targets, and complex object support relationships. These limitations hinder their effectiveness in tasks requiring fine manipulation. In contrast, dexterous hands offer a transformative solution. Their multi-degree-of-freedom structure and human-like flexibility enable advanced actions such as pushing, moving, and gradually exposing target objects. These capabilities allow precise control with minimal disruption to surrounding objects, making them particularly suited for unstructured and confined environments. By addressing the inherent limitations of traditional grippers, dexterous hands enhance both precision and operational efficiency in object retrieval tasks.

% 我们自己如何用rl实现的retrieval learning system
% rl setup + 场景布置xx 随机化

%  Our contribution has three fold: 
% \fengshuo{To tackle this problem, we leverage xxx
% To enable this learning process, we propose a curriculum strategy that enables learning in simulation, followed by zero-shot transfer to real hardware.
% In summary, our main contributions are as follows:}


Imagine having a box full of miscellaneous items with your desired book at the bottom. Having to remove each overlying object one by one to reach your book is undoubtedly a tedious task. Similar cluttered environments, such as messy drawers, disorganized office desks, or packed warehouses, present significant challenges for object retrieval in robotics. Previous approaches often relied on sequential removal of occluding objects using parallel-jaw grippers before attempting to grasp the target object. However, this strategy is not only time-consuming but also presents significant challenges in achieving reliable grasping manipulation. In such scenarios, humans typically employ their hands to efficiently move other objects aside, retrieving the desired object in a relatively short time. This is because the hand, as a high-degree-of-freedom manipulator, offers numerous configurations for object manipulation. To enable efficient object retrieval across diverse objects and scenarios, we propose using a multi-finger dexterous hand as the end-effector.

Learning such skills brings multiple challenges: \textit{(i) Time efficiency}: When objects are only partially visible or completely occluded in the perception system, sequentially grasping and placing objects is highly time-consuming. Determining how to quickly locate and expose sufficient pixels for grasping is a challenging yet important task. Hand-designed strategies often require unacceptably long execution times. \textit{(ii) Diverse objects}: Training a policy that succeeds across various diverse object settings is even more challenging. Previous methods requiring pose estimation of surrounding objects can introduce new errors and struggle to generalize to unseen objects with limited training data. Some prior research typically assumes the existence of a universally successful grasping policy, which may be an even more difficult proposition. \textit{(iii) High-dimensional action space and contact-rich environment}: Dexterous multi-finger hands introduce a high-dimensional state and action space, which increases the difficulty for optimization. Additionally, in cluttered stacking scenarios, object collisions are extremely rich, making environmental dynamics more challenging to model.

In this paper, we propose \textbf{Retrieval Dexterity}, a new system for retrieving objects in clutter with one dexterous hand in a super-efficient manner. Our hardware system includes a Realman RM-75 robot equipped with an Inspired Hand (with 6 degrees of freedom) at the end, as shown in Figure~\ref{fig:teaser}. To solve this problem, we propose an approach leveraging Reinforcement Learning to train a policy in simulation and then perform Sim2Real transfer on real robots. Specifically, to solve the above challenges, we introduce two key technical contributions:
\begin{itemize}
    \item We have developed a realistic scene generation pipeline for stacked environments. The system simulates the natural accumulation of common household objects through gravitational dropping, while strategically placing occluded target objects at specified positions. Physical parameters are carefully calibrated to ensure realistic stacking environment.
    \item We present a reinforcement learning framework for training object retrieval policies. The reward function is based on the pixel visibility of target objects captured by simulated cameras after a fixed time horizon. Using efficient reinforcement learning algorithms, we optimize the policy to discover emergent retrieval strategies in complex stacked environments.
\end{itemize}

We conduct both simulation and real-world experiments. The results demonstrate that our method enables the hand to successfully retrieve objects and surpass baselines by a large margin in efficiency. To our knowledge, this is the first work that enables efficient object retrieval with dexterous multi-finger hands.


\section{Related Work}
\subsection{Cluttered Objects Manipulation}
Interacting with objects in a cluttered environment is of significant importance for real-world applications \cite{6907059, 1087038, 9197318}. Prior research has extensively explored robotic manipulation in these environments, aiming to equip robots with the ability to master diverse and complex skills. For instance, \citet{9197318, 10342335} have focused on improving robust object grasping techniques, while \citet{li2024broadcasting, 10611541} investigate retrieval tasks. Additionally, studies such as \citet{goyal2022ifor, pmlr-v205-tang23a, jia2024cluttergen} address challenges in rearrangement, as well as grasping and throwing \cite{kasaei2024harnessing}. Visual-based approaches have also been widely adopted to enhance manipulation strategies in cluttered environments. For example, \citet{9591286} leverage visual prediction and planning to forecast the future states of objects after pushing actions, thereby optimizing grasping paths. In a related effort, \citet{9341545} propose a continuous pushing strategy driven by real-time visual signals to improve object graspability.

\subsection{Object Retrieval}
Retrieving a target object from complex clutter is a fundamental robotic skill with broad applications, ranging from domestic services to manufacturing. To address this challenge, various studies have proposed solutions from multiple perspectives. For instance, some works focus on planning strategies, such as object search optimization \cite{8793494}, teacher-aided exploration \cite{9341545}, and human-guided planning \cite{9196689}, while others emphasize action-based methods, including push-grasping synergy policies \cite{9465702} and learning pushing and grasping without visual foresight \cite{8794143}. Additionally, approaches like analyzing support relations among cluttered objects have shown promise for improving retrieval efficiency \cite{li2024broadcasting}. In terms of perception, researchers have explored both tactile sensing \cite{xu2024tactile, 10611541} and visual or language-based modalities \cite{lemke2024spotcompose, pmlr-v205-tang23a}. The choice of end-effector has also been a key focus, with methods employing rod-like pushers \cite{10161041}, parallel grippers \cite{9636230, pmlr-v205-tang23a, 10611541, 9812132}, and dexterous hands \cite{pmlr-v229-chen23e} to address the challenges posed by cluttered environments. Moreover, task scenarios vary widely, from granular media \cite{xu2024tactile} to confined spaces \cite{10611541}, requiring tailored approaches to accommodate environmental constraints. Our approach differs fundamentally from most previous methods by actively manipulating occluding objects to expose the target object, enabling efficient retrieval while introducing more challenging control requirements.

\begin{figure*}
    \centering
    \includegraphics[width=0.99\linewidth]{figs/method.pdf}
    % \vspace{-0.3em}
    \caption{\textbf{Illustration of the Retrieval Skill System Design.} (a) Constructs diverse cluttered scenes using a drop-from-above strategy. (b) Utilizes large-scale parallel RL with well-designed rewards to train policies. (c) Generates trajectories from the RL expert policy, selects useful ones based on our principle, and trains the distilled policy for deployment on a real robot.}
    \label{fig:method}
    % \vspace{-5pt}
\end{figure*}

\subsection{Reinforcement Learning for Dexterous Manipulation}
Dexterous manipulation has remained a cornerstone of robotics research due to its critical role in replicating the sophisticated motor skills humans use to interact with diverse objects and achieve intelligent control \cite{6907059,1087038,6907864}. While traditional methods employ analytical dynamic models for trajectory optimization
% ~\cite{chen2024springgrasp, mordatch2012contact, bai2014dexterous, kumar2014real}
, their simplified treatment of contact dynamics limits their effectiveness in complex tasks. Imitation learning (IL) has demonstrated impressive results in dexterous manipulation tasks \cite{9561802, pmlr-v205-chen23b}. However, IL faces significant challenges due to its reliance on human expert demonstrations, making it resource-intensive and difficult to scale for contact-rich tasks \cite{chen2024objectcentric, yang2024anyrotate, lin2024twisting}. In contrast, this work trains a generalizable policy using sim-to-real reinforcement learning without using any expert data. Reinforcement Learning (RL) has been widely adopted for robotic manipulation to master complex skills, particularly in unstructured and contact-rich scenarios. RL-based approaches offer two significant advantages: they simplify the controller design process and enable the acquisition of complex skills. For instance, \citet{pmlr-v164-chen22a} developed an efficient system for in-hand object re-orientation, while \citet{lin2024twisting} proposed a sim-to-real framework for twisting lids of various bottle-like objects using two hands. Similarly, \citet{pmlr-v229-huang23d} designed a system for efficient bimanual handovers, and additional studies have explored tasks such as spinning pen-like objects \cite{wang2024lessons}, sequential block building \cite{pmlr-v229-chen23e}, bimanual manipulation \cite{10343126, lin2024twisting} and diverse skills based on Vision-Language Models~\cite{liu2025vlp, sun2024large} or exploration~\cite{Bai_Zhang_Tao_Wu_Wang_Xu_2023, zhang2025beta}. On the other hand, several works \cite{zhou2022learning, pmlr-v229-agarwal23a, yang2024anyrotate, lin2024twisting} demonstrate that RL-based methods can learn emergent dexterous behaviors without additional reward terms. Our work leverages this capability to discover emergent behaviors, where carefully designed reward functions and environmental setups enable autonomous learning of retrieval skills including pushing and poking.

\section{Task Formulation}
In this paper, we focus on the challenge of retrieving target objects in cluttered environments using a dexterous multi-fingered robot, as shown in Figure~\ref{fig:teaser}. The objective of this task is to efficiently expose the target object to the camera’s field of view, enabling subsequent grasping operations. We then formulate the object retrieval task as a finite horizon Markov Decision Process (MDP), which contains a 5-tuple ($\mathcal{S}, \mathcal{A}, R, P, \gamma$). $\mathcal{S}$ and $\mathcal{A}$ represent the state and action spaces. $P:\mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0, 1]$ represents the stochastic dynamics, which determines the probability of transferring to $s^\prime$ given state $s$ and action $a$. $R: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$ is the reward function and $\gamma \in (0,1)$ is the discount factor. The policy $\pi(a|s)$ is a mapping from state space to action space, which generates action distributions $a$ conditioned on observations $s$ to maximize the expected return $\mathbb{E}_{\pi}[\sum_{t=0}^{T-1}\gamma^{t}R]$ in an episode with $T$ time steps. To achieve this, the system requires sophisticated manipulation skills, including searching through surrounding objects in a cluttered environment to determine the target object’s location and efficiently removing obstructions to expose it. This task is significant as it enables robots to efficiently search and retrieve objects in complex cluttered scenarios, even when targets are completely obscured.

\section{Method}
In this section, we introduce our system for efficient object retrieval. The overview of the system is shown in Figure~\ref{fig:method}. Our framework consists of three parts: Task Construction (Section~\ref{sec:task_con}), RL Problem Design (Section~\ref{sec:rl_design}) and the Policy Training (Section~\ref{sec:policy_train}). The details of our sim-to-real policy transfer are introduced in Section~\ref{sec:sim2real}.

\subsection{Task Construction}\label{sec:task_con}
The key challenge in cluttered scenes arises from the diversity of object configurations (e.g., categories, geometries, locations, and poses) and their combinations. To simulate realistic scenarios, we place 18 household objects with varying masses, sizes, and geometries in a box to create diverse cluttered scenes.At task initialization, objects are dropped into the box with the target object placed at the bottom. For each trial, we vary both the choice of target object and its pose within the box boundaries, ensuring diverse testing scenarios.

To avoid hand interference during scene initialization and enable reliable reward computation, we define two key static poses: the \textit{prepare pose} and the \textit{suspending pose}. Both poses position the hand above the box with a downward-facing palm. The \textit{prepare pose} positions the hand directly above the box as the initial configuration. The \textit{suspending pose} is specifically designed to keep the hand away from the box, ensuring it does not interfere with object dropping or reward computation. At initialization, we first move the hand to the \textit{suspending pose} to allow object dropping and scene formation. The hand then returns to the \textit{prepare pose} for policy training. During the training process, the robotic hand inevitably moves above the target object, interfering with our camera-based reward computation. Therefore, every $10$ steps, we temporarily move the hand to the \textit{suspending pose} for reward evaluation. We compute the reward by counting ground truth target object pixels from the top-down segmentation mask, then return the hand to its previous configuration. This strategy improves the stability of policy learning by assessing cumulative behavior over time rather than relying on noisy immediate feedback, providing a more stable training process. Notably, during policy evaluation, we no longer need to compute rewards and thus eliminate this periodic hand movement to the \textit{suspending pose}. Detailed implementations of the Task Construction can be found in Appendix~\ref{appendix:task_construction}.

\subsection{RL Problem Design}\label{sec:rl_design}
After establishing the cluttered scene, we address these challenging object retrieval tasks using model-free reinforcement learning. Below, we introduce the observation and action space of our policy, followed by the reward formulation.

\noindent \textbf{Observation Space.} At timestep $t$, the control policy observes a combination of proprioceptive and visual information. The proprioceptive inputs $q_t=(q_t^\textrm{arm}, q_t^\textrm{hand}) \in \mathbb{R}^{13}$ include the arm and hand joint positions $q_t^\textrm{arm} \in \mathbb{R}^7$ and $q_t^\textrm{hand}\in \mathbb{R}^6$, while the visual inputs consist of processed representations such as the bounding box coordinates of the target object's segmentation $b_t = (x_t, y_t, w_t, h_t) \in \mathbb{R}^4$ , its area $a_t = (w_t \cdot h_t) \in \mathbb{R}$, and the depth of the bounding box center pixel $d_t \in \mathbb{R}$. To facilitate more effective policy learning, we improve policy training by incorporating privileged information accessible in simulation. Specifically, the observation space is defined as:
\begin{equation}
s_t= \{q_t, b_t, a_t, \{T_t^{f, i}\}_{i=1}^{5}, T_t^\textrm{obj}, \dot{q}_t, v_t^\textrm{obj}, \{T_t^\textrm{near, i}\}_{i=1}^{5}\}.
\end{equation}
These include the poses of five fingertips $\{T_t^{f, i}\}_{i=1}^{5}\in \mathbb{R}^{35}$ and target object $T_t^\textrm{obj}\in \mathbb{R}^{7}$, the velocities of the current joints $\dot{q}_t\in \mathbb{R}^{13}$, and the target object's linear and angular velocities $v_t^\textrm{obj} = (\mathbf{v}_t^\textrm{obj}, \mathbf{\omega}_t^\textrm{obj})\in \mathbb{R}^{6}$. Additionally, the positions of the 5 nearest objects to the target $\{T_t^\textrm{near, i}\}_{i=1}^{5}\in \mathbb{R}^{15}$ are included.


\noindent \textbf{Action Space.} The action space of our system is the target joint angles of our robot $a=(a_t^\textrm{arm}, a_t^\textrm{hand}) \in \mathbb{R}^{13}$. For better stable control, the policy generates a target joint position $a_t^\textrm{hand}\in \mathbb{R}^{6}$ for the hand and applies a linear smoothing update to blend it with the previous target, reducing abrupt movements. Specifically, the blending is computed as  
\begin{equation}
a_t^\textrm{hand} = \lambda a_t^\textrm{hand} + (1-\lambda) a_{t-1}^\textrm{hand},
\end{equation}
where $\lambda$ is the smoothing factor. For the robotic arm, the action $a_t^\textrm{arm}\in \mathbb{R}^{7}$ represents relative joint position changes, which are added to the current joint angles to obtain target positions for control.


\noindent \textbf{Reward Function.} We design a fine-grained reward function to optimize object retrieval skills and enable the hand to efficiently expose the target object. Specifically, the reward function comprises the following components: (1) \textit{Distance Reward.} This reward encourages the hand to locate occluded areas by minimizing the distance between the hand’s palm and the target object. It is defined as $r_\textrm{dist} = \exp(-5 \cdot \min(d - e_0, 0))$, where $d$ represents the distance between the hand’s palm and the target object, and $e_0 = 0.15$ is a predefined threshold. (2) \textit{Stir Reward.} To encourage the hand to actively displace objects, especially in cases of complete occlusion. Let the positions of all objects in the clutters at timestep $t$ be denoted as $p^\textrm{all}_t$. The stir reward is given by $r_\textrm{stir} = \alpha \|p^\textrm{all} - p^\textrm{all}_{t-1}\|_2$, where $\alpha$ is a scaling factor. (3) \textit{Proximity Clearance Reward.} We define this reward to guide the agent in clearing occluding objects around the target object. Let the sum of distances between the target object and its $k$ nearest objects be $\sum_{i=1}^{k} f_i$. The reward is formulated as $r_\textrm{clean} = \beta \cdot r_\textrm{dist} \cdot \sum_{i=1}^{k} f_i$, where $\beta$ is a scaling hyperparameter. (4) \textit{Pixel Emergency Reward.} We also design a vision-based holistic evaluation reward to encourage the hand to expose the target object. Denoting the count of pixels within the segmentation mask of the target object in the top-down camera frame as $C$, this reward is defined as $r_\textrm{pixel} = C/{15}$. (5) \textit{Penalty.} To discourage undesirable behaviors, we introduce penalty terms as used in previous works~\cite{pmlr-v229-chen23e, pmlr-v229-huang23d, lin2024twisting}, including action penalties, contact penalties, and penalties for displacing the target object.  

To facilitate more efficient learning while mitigating the risk of reward hacking~\cite{amodei2016concrete}, we adopt the reward shaping technique proposed by \citet{10.5555/645528.657613}. Specifically, we define a state function:
\begin{equation}
    \Phi(s) = r_\textrm{dist}(s) + r_\textrm{clean}(s) + r_\textrm{pixel}(s),
\end{equation}
and formulate a potential-based shaping function as the final reward function:
\begin{equation}
    \mathcal{R}(s, a, s^\prime) = \Phi(s^\prime) - \Phi(s).
\end{equation}


\subsection{Policy Training}\label{sec:policy_train}

We use PPO~\cite{schulman2017proximal} to train a closed-loop policy for object retrieval with a dexterous robotic hand in cluttered environments. Thanks to the high-parallel simulation capabilities of IsaacGym~\cite{S2021_28dd2c79}, we parallelize our policy training across 512 environments simultaneously. To enhance the generalization capabilities of our policy, we apply domain randomization techniques, including scenario diversification and object pose variation. At each episode initialization, we randomly select the box position and generate cluttered scenes by randomly dropping objects into the box while ensuring the target object remains covered, creating a broad spectrum of challenging environments. Additionally, we randomize the initial pose of the target object to further improve policy robustness. Detailed training configurations are provided in the Appendix~\ref{appendix:rl_training}.

\begin{figure*}
    \centering
    \includegraphics[width=0.99\linewidth]{figs/datasets.pdf}
    % \vspace{-0.4em}
    \caption{\textbf{Overview of the Experimental Setups.} (A) Training object sets in simulation and testing object sets in both simulation and the real world. (B) Cluttered scenes in simulation. (C) Workspace of the real setup. We use an Inspired Hand mounted on a Realman RM-75 robot, equipped with a RealSense D435 camera.}
    \label{fig:dataset}
    % \vspace{-5pt}
\end{figure*}

\subsection{Sim-to-Real Transfer}\label{sec:sim2real}
When deploying the policy in the real world, some observations, such as joint velocity and object velocity, cannot be accurately estimated. To address this, we use rollout trajectories from the trained RL expert policy. To improve effectiveness, we design a data selection principle, selecting successful trajectories where the fingers maintain a minimum height of 2 cm above the box bottom and the target object is evenly distributed within the box. Using the collected data, we distill a student policy suitable for real-world deployment through Behavior Cloning \cite{NIPS1988_812b4ba2}. During distillation, the student policy receives an observation $o_t = (q^\textrm{arm}_t, x_t, y_t)$, where $q^\textrm{arm}_t$ represents the arm’s joint position, and $(x_t, y_t)$ denotes the position of the target object. While $q^\textrm{arm}_t$ can be easily accessed, our system uses a side top-down camera to track the target object for real-time position acquisition. First, we use the RGB image from the camera to obtain the coordinates of the target object in the camera frame. Specifically, we use SAM~\cite{kirillov2023segment} to obtain an initial binary mask of the target object as the input for Cutie~\cite{cheng2024putting} to continuously track the mask of the target object over time. By lining out the bounding box of the tracked mask, we compute its center point which represents the pixel coordinate of the target point and convert it to the coordinate in the camera frame via the intrinsic parameters of the camera. Then we do the hand-eye calibration to transform the coordinates in the camera frame into the world frame which is just the real-time position $(x_t, y_t)$ of the target objects. The entire tracking system runs at the speed of 30Hz. The student policy then generates an action $a \in \mathbb{R}^{13}$, which corresponds to the target joint angles of the arm and hand. As shown in Figure~\ref{fig:method}, we employ a transformer network as the policy, which takes a sequence of observations, including the current observation and the nine history observations, and outputs the action to be taken at the current timestep. The transformer network has a powerful ability to effectively handle temporal dependencies in the sequential observation data, which is crucial for learning the long-term strategies required for retrieval tasks. Implementation details, including hyperparameter settings and network architecture are provided in Appendix~\ref{appendix:sim_real_transfer}.

\section{Experiment}

In this section, we evaluate our proposed framework through comprehensive experiments conducted in both simulation and real-world environments. Our investigation focuses on the following four research questions:

(1) How effective is our framework in performing object retrieval tasks?
(2) How well does our framework generalize to objects with different geometries, masses, and cluttered patterns in object retrieval task?
(3) How does our method achieve high efficiency in object retrieval tasks?
(4) How well does our retrieval policy perform on real-world dexterous robotic systems?

Below, we first describe our experimental setup and dataset composition. We then present the evaluation metrics and baseline methods used for comparison. Finally, we systematically address each research question through detailed experimental results and analysis. All simulation results are averaged over 10 random seeds, while real-world performance metrics are derived from 10 independent trials per experiment.

% Main results Table
\input{table/main_results}

\subsection{Setups}
\noindent \textbf{Dataset.} During policy optimization, we use a LEGO block and a soap box as target objects in the training dataset. At the beginning of each episode, one of these objects is randomly selected for policy training. To evaluate the generalizability of our policy to unseen objects, we supplement a test set comprising 8 small objects (e.g., apples) and 6 large objects (e.g., books), differentiated by shape and weight. Furthermore, we introduce several new objects to construct novel stacked scenarios, testing the policy’s ability to generalize across various cluttered environments.

\noindent \textbf{Evaluation Metrics.}
The primary goal of object retrieval is to locate the target object and maximize its visibility within the camera’s field of view to facilitate subsequent manipulations. We define \textit{exposure} as the ratio of unobstructed pixels representing the target object on the imaging plane. Each episode is evaluated in two phases: (1) recording the target object's visible pixel count, $p_t^\textrm{curr}$, and its 6D pose every 10 steps during retrieval; (2) removing all occluding objects in the simulation, resetting the target object to the recorded 6D pose, and recording its total visible pixel count, $p_t^\textrm{all}$. The exposure at time $t$ is computed as $\textrm{exposure}_t = p_t^\textrm{curr} / p_t^\textrm{all}$. Detailed procedures are provided in Appendix~\ref{appendix:eval_metric}. Retrieval is considered successful when the exposure exceeds 95\%. To systematically evaluate both the performance and efficiency of the retrieval policy, we define the following metrics:

\begin{itemize} 
    \item \textbf{Success Rate (SR):} The percentage of trials where the target object achieves 95\% exposure within 210 steps.
    \item \textbf{Retrieval Steps (RS):} The number of steps required to achieve successful retrieval. 
    \item \textbf{Increase in Exposure Ratio (IER):} The absolute increase in the target object's exposure from its initial to final state.
\end{itemize}

\noindent \textbf{Baselines.}
We compare our method against the following baselines:
(1) \textit{Ours.} A policy is trained using PPO with a carefully designed reward function in diverse stacking environments. 
(2) \textit{Ours w/o RS.} A policy is trained without reward shaping to assess the contribution of this technique to retrieval performance. 
(3) \textit{Ours w/o $r_\textrm{stir}$.} A policy is trained without the stir reward to examine its role in policy learning.
(4) \textit{Ours w/o $r_\textrm{clean}$.} A policy is trained without the proximity clearance reward to analyze its significance in the overall reward structure.
(5) \textit{Visual-based Motion Planning Search (VMP).} A heuristic motion planning baseline that uses segmentation masks of the target object to guide the robotic hand. Predefined rules are employed for retrieval manipulation.

Further details for baselines are provided in Appendix~\ref{appendix:baseline}.

\subsection{Results and Analysis}
\noindent \textbf{Main Results.}
We evaluate our method against various baselines in simulation with target objects of differing sizes. As shown in Table~\ref{tab:main_res}, our system consistently achieves higher retrieval success rates (RSR) and requires fewer retrieval steps (RS) compared to all baselines on both seen and unseen objects. Specifically, our method improves the success rate by 22.6\% and 59.4\% on small and large objects, demonstrating superior generalization capabilities. Across all methods, retrieving small target objects (e.g., a small LEGO block) is generally easier and more efficient than retrieving larger ones.

Ablation studies reveal the critical role of individual reward components. Removing the stir reward ($r_\textrm{stir}$) significantly reduces success rates for larger objects by 43.9\%, indicating its importance in encouraging the robotic hand to stir objects within the clutter to effectively clear obstructions above large targets. In contrast, the proximity clearance reward ($r_\textrm{clean}$), which incentivizes the removal of objects near the target, proves more effective for smaller objects, increasing their retrieval efficiency by 22.1\%. Furthermore, the potential-based reward function plays a pivotal role in enhancing both the performance and efficiency of our method, reducing retrieval steps by 19.4\% and enhancing the success rate by 39.8\%.

In comparison, the VMP relies on segmentation masks and predefined open-loop rules, suffers from lower success rates and efficiency. This highlights the advantages of our learned policy, which adapts dynamically to varying clutter configurations and object properties.


\noindent \textbf{Impact of Occlusion Rate.} 
We also explore the impact of clutter occlusion rate (i.e., $1-\textrm{exposure}$) on target objects. Figure~\ref{fig:occlusion} presents the relationship between occlusion rates, retrieval success rate (RSR), and retrieval steps (RS) in cluttered environments, comparing smaller and larger target objects. As the occlusion rate rises from 30\% to 100\%, RSR decreases for both object sizes, with smaller objects consistently achieving higher RSR than larger ones. This indicates that larger objects, although generally easier to detect, are more prone to being significantly obstructed by substantial clutter. In contrast, retrieval steps increase as occlusion rates rise, signifying reduced efficiency in highly cluttered settings. Smaller objects generally require fewer retrieval steps than larger ones across mostly occlusion levels, likely because retrieving larger objects involves removing more obstructing clutter. These findings reveal that smaller objects are retrieved with higher success and efficiency, while larger objects face greater challenges posed by occlusion.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/result/occlusion_levels.pdf}
    % \vspace{-0.5em}
    \caption{\textbf{Impact of Occlusion Rate on Performance and Efficiency.} We evaluate the retrieval success rate and retrieval steps of our policy for small and large target objects under varying occlusion levels.}
    \label{fig:occlusion}
    % \vspace{-10pt}
\end{figure}

\subsection{Generalization Capability}

\begin{figure*}[ht]
    \centering
    \subfloat[Summary of generalization scenarios.\label{fig:gen_summary}]{
        \includegraphics[width=0.33\textwidth]{figs/result/gen_summary_legend.pdf}
    }
    \subfloat[Unseen Clutter Generalization.\label{fig:genl2}]{
        \includegraphics[width=0.33\textwidth]{figs/result/l2_gen.pdf}
    }
    \subfloat[Clutter Quantity Generalization.\label{fig:gen_l3}]{
        \includegraphics[width=0.33\textwidth]{figs/result/pref_effi.pdf}
    }
    % \vspace{-0.5em}
    \caption{\textbf{Performance on Task Generalization.} (a) depicts the average success rate across three levels of generalization. (b) illustrates performance on unseen clutter. (c) presents the impact of clutter quantity. Darker colors indicate a higher object count in the clutter, while larger shapes represent a greater average exposure increase (i.e., higher IER) during retrieval.}
    % \vspace{-1em}
    \label{fig:novel_behavior}
\end{figure*}

We investigate both in-distribution performance and generalization capabilities by comparing all methods across various generalization tasks. We consider three levels of skill generalization: unseen target object generalization (L1), unseen cluttered environment generalization (L2), and cluttered object quantity generalization (L3), as illustrated in Figure~\ref{fig:dataset}.B. For unseen target object generalization (L1), only the target object is changed while keeping the clutter consistent. For unseen cluttered environment generalization (L2), 80\% of the clutter objects are replaced, but the target object remains the same. For cluttered object quantity generalization (L3), 20\% to 70\% more clutter objects are added while keeping the target object unchanged. The success rate for these three generalization scenarios is summarized in Figure~\ref{fig:gen_summary}, which highlights the superior generalization performance of our method. Notably, the reward shaping technique plays a critical role in our approach, achieving an average improvement of 100\% across all generalization scenarios and an impressive 157\% improvement in the unseen target object scenario.

As shown earlier in Table~\ref{tab:main_res}, we achieve exceptional target object generalization, both for small and large target objects.
Furthermore, we observe that the number of objects in the clutter significantly influences retrieval manipulation performance, motivating the design of L3 generalization. As shown in Figure~\ref{fig:gen_l3}, there is a noticeable performance gap between the different methods as the number of retrieval steps increases. Our system (denoted by red stars) demonstrates more stable performance, maintaining a higher success rate even with more retrieval steps, which highlights its robustness.

The comparison between different variants of Ours reveals that excluding certain components leads to decreased performance. Among these, excluding the RS technique causes the most substantial drop in success rate, underscoring the importance of this component for improving retrieval efficiency.

Regarding the data point size, it reflects the average increase in target object exposure during retrieval. We find that although some data points have similar sizes, there is a large gap in success rate and retrieval steps (e.g., the red star and yellow circle in the 100\% scenario). This suggests that while exposure is increased in many cases, it is only through consistently exposing the target object during the retrieval process that efficiency and success rate can be significantly improved.

\subsection{Retrieval Efficiency}
In this section, we evaluate the retrieval efficiency of our system through experiments that compare it to baseline methods. Retrieval efficiency is measured by the number of manipulations required for successful object retrieval.

\input{table/sim_efficiency}

We consider three scenarios: small target objects, large target objects, and complex clutter (defined as scenarios with 40\% more clutter than typical cases). Each scenario involves retrieving a target object that is 90\% occluded, a challenging level of occlusion chosen to test the limits of retrieval strategies. The goal is to assess how efficiently our dexterous multi-finger hand, capable of performing skill-based actions such as pushing, stirring, and poking, can retrieve the target object compared to traditional object removal methods.

This experiment includes two baseline methods:
(1) \textit{Visual-based Motion Planning.} The system uses the object’s segmentation mask to compute its position and employs pre-defined finger actions to displace objects in the clutter to expose the target object.
(2) \textit{Grasp-and-Pick.} This method assumes the stacking relationships of occluding objects are known and attempts to grasp and lift these objects one by one to reveal the target. For this experiment, we assume the successful grasp and removal of each occluding object.

\begin{figure*}
    \centering
    \includegraphics[width=0.99\linewidth]{figs/realworld.pdf}
    \caption{\textbf{Retrieval Sequence in Real-World Clutters.} We present four everyday objects as target items, varying in shape and size.}
    \label{fig:real_world_exp}   
\end{figure*}

The results, summarized in Table~\ref{tab:stat_step} demonstrate that our method consistently outperforms the baseline approaches in terms of retrieval efficiency. Compared to VMP, our method reduces the number of steps by an average of 38\% across all scenarios. When compared to Grasp-and-Pick, our method shows an even greater reduction in steps, averaging a 90\% reduction. This efficiency is primarily attributed to the multi-finger hand's ability to directly interact with and displace occluding objects, rather than removing them sequentially as in the baseline methods.

\subsection{Real-World Experiment}
We conduct sim-to-real experiments (Figure~\ref{fig:real_world_exp}) to evaluate the performance of our method and two baseline methods on a real-robot platform shown in Figure~\ref{fig:dataset}.C. Our objective is to address the following key questions regarding the performance of our system:
\begin{itemize}
\item Can the policies learned in simulation zero-shot transfer to a real-world dexterous multi-fingered robotic system?
\item Can the distilled policy successfully retrieve target objects that generalize to different positions?
\item Can our system achieve more efficiency in multi-object stacked environments compared to existing methods?
\end{itemize}

\noindent \textbf{Retrieve Various Target Objects.} We present quantitative results comparing our policy to baseline policies in Table~\ref{tab:real_target_generalization}. Specifically, we evaluate small and large daily objects as target objects across three shapes: cuboids, cylinders, and spheres. Our method demonstrates consistent and stable retrieval performance across all target objects. Notably, it achieves higher success rates for smaller, cylindrical, and spherical objects. In contrast, the visual-based motion planning method suffers from limited flexibility due to its predefined hand manipulations, often causing previously removed occluding objects to re-block the target, thereby reducing retrieval success rates.

\input{table/real_target_generalization}

\noindent \textbf{Target Object Position Generalization.} We also investigate the impact of target object positions on retrieval performance. The target objects are placed in five distinct regions of the box: center, top-left, bottom-left, top-right, and bottom-right. The experimental results are summarized in Table~\ref{tab:real_position_generalization}. We observe that positions near the center and bottom (closer to the robotic arm) achieve better generalization. In contrast, positions farther away such as top-left and top-right are constrained by the robot's workspace and reduce manipulation flexibility.
\input{table/real_position_generalization}

\noindent \textbf{Retrieval Efficiency.}
Retrieval efficiency is the most important thing which is measured by the time required to retrieve the target object. We compare our system against baseline methods, including VMP and sequential grasping and removal. As shown in Figure~\ref{fig:real_efficiency}, our policy effectively clears occluding objects, facilitating the exposure of the target. Specifically, our approach reduces retrieval time by 51.2\% compared to VMP and by 61.9\% compared to sequential grasping and removal on average. These results highlight the advantage of leveraging skill-based interactions, such as pushing and stirring, over sequential removal strategies.

\section{Limitation}
The primary limitation of our work lies in the requirement for object mask inputs to the policy in real-world scenarios, necessitating human intervention. Future work will explore leveraging pretrained vision-language models and foundational vision models to automatically generate the required masks.

\section{Conclusion}

In this work, we have presented a novel approach to efficient object retrieval in cluttered environments using dexterous multi-finger hands. Our system demonstrates the ability to manipulate occluding objects strategically, exposing target objects for retrieval—a capability that significantly improves upon traditional sequential removal methods. Through careful design of our simulation environment and reinforcement learning framework, we have addressed key challenges including time efficiency, object diversity, and the complexity of high-dimensional control in contact-rich environments.
Our experimental results, both in simulation and real-world settings, validate the effectiveness of our approach. The system successfully generalizes across diverse objects and achieves zero-shot transfer to real-world robots, demonstrating robust performance without additional training. This work represents a step toward more efficient and capable robotic manipulation in cluttered environments, though opportunities remain for future exploration, particularly in achieving fully autonomous operation through integration with advanced perception systems.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/result/real_efficiency.pdf}
    \caption{{Retrieval time for various target objects.}}
    \label{fig:real_efficiency}
    % \vspace{-10pt}
\end{figure}
% \clearpage

% ============= Bibtex ===============
\bibliography{ref}

\input{appendix}


\end{document}

