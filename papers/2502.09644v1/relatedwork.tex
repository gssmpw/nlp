\section{Related Work}
\paragraph{Overall stance.} 
PSVs are a fine-grained representation of stances. Classifying the overall stance of arguments towards a topic is a core task in argument mining~\citep{bar-haim-etal-2017-stance,kobbe-etal-2020-unsupervised,luo-etal-2020-detecting}. 
However, assigning an argument a global stance tag (e.g., \textsc{pro}, \textsc{con} and \textsc{neutral} or \textsc{unrelated}) lacks expressivity: it divides sets of arguments into only a couple of groups, neglecting crucial nuances. The task of same-side classification (predict whether two arguments share their overall stance) in \citet{hou-jochim-2017-argument,korner-etal-2021-classifying} does not address this problem either. Further, it does not unveil the underlying reasons why arguments share a stance. 

To counter this issue, prior work incorporated background knowledge, by including \textit{reasoning paths} to \textit{explain}, e.g., for which reasons a premise supports or attacks a conclusion \citep{pauletal:2020}, or to generate an \textit{explanation graph} for a premise-conclusion pair that explains the stance of the argument \citep{saha-etal-2021-explagraphs,saadat-yazdi-etal-2023-uncovering,plenz-etal-2023-similarity}. We build on this work by including concepts from a commonsense resource to define the PSV signature concepts. %


\paragraph{Perspectives in argumentation.}

Our work is related to \citet{barrow-etal-2021-syntopical}, who rely on graphs to represent arguments and their relationships as a basis to detect viewpoints. They proposed so-called \textit{syntopical graphs} that model pairwise textual relationships between claims to enable a better reconstruction of latent viewpoints in a collection, thereby making points of (dis)agreement within the collection explicit. %
In a similar way, PSVs enable the detection of (dis)agreement. But in addition, %
PSVs can detect \textit{orthogonality}, i.e., cases where a pair of arguments is not 
related to each other. 

Our work is also related to the analysis of framing in argumentation 
\citep{heinischetal:2022a,heinisch-etal-2023-unsupervised,otmakhova-etal-2024-media}, where emphasized aspects are automatically detected. %
Specifically, our work is related to the idea that frames %
can be issue- or topic-specific and thus need to be identified in a bottom-up fashion for each topic. 
\citet{ajjour-etal-2019-modeling} present an unsupervised approach that induces frames by clustering arguments from an issue. 
\citet{ruckdeschel-wiedemann-2022-boundary}, by contrast, present a topic-specific framing approach, with the limitation of training a classifier for each topic separately -- which then cannot be applied to new topics. 
Our unsupervised approach to signature induction follows \citet{plenz-etal-2024-pakt}, who find that the knowledge base ConceptNet~\citep{speer2017conceptnet} provides suitable, often implicit concepts relevant for argument interpretation. %

Finally, our task is related to aspect-based sentiment analysis \citep[ABSA; ][]{cabello-Akujuobi-2024-it,wang-etal-2024-refining,frasincar_2023,hoang-etal-2019-aspect}, which aims for a fine-grained view on which aspects are target of a certain sentiment. 
While ABSA is typically applied to reviews, we aim for a fine-grained, perspectivized analysis of arguments in deliberation, by detecting argument-related stances towards specific concepts. 

Our framework supporting a deliberative analysis of arguments thus brings together and combines methods from viewpoint detection, framing and aspect-based sentiment analysis. We combine these methods in a novel way for deliberation support, by pinpointing conflicting perspectives and concepts between argument and stakeholders, with the aim of resolving conflicts and suggesting compromises. 

\paragraph{Deliberation} 
refers to a collaborative argumentative exchange where arguers hold incompatible views on an issue, which they seek to resolve by achieving a consensual decision \citep{FELTON2022100350}. Deliberative processes are naturally framed as a dialogue \citep{waltonetal-2010-deliberation,snaith-etal-2010-mixed,Waltonetal-2016-deliberation}. %
For example, \citet{al-khatib-etal-2018-modeling} successfully classify different \textit{strategies} of participants
in deliberative discussions. Yet they do not evaluate the underlying perspectives, nor the effectiveness of these strategies for the aim of achieving an agreeable resolution of conflicts. 


Deliberation is typically approached using pre\-fer\-ence frameworks that take into account the arguers' diverging desires or goals, or their normative or moral considerations \citep{modgil,Amgound-etal-2014-preference,10.1007/978-3-319-69131-2_25}. We do not focus on algorithmic resolution of conflicts, but on analyzing the arguers' perspectivized viewpoints to quantify dimensions of (dis)agreement -- which future work may extend with reasoning processes, to derive potential resolutions. 

\citet{RECAP-2018} provide overviews of debates to make decision makers aware of arguments and opinions on relevant topics. Using a Case-Based Reasoning approach, they compute similarity between arguments to retrieve or cluster similar arguments. This allows them to synthesize new arguments -- by extrapolating from and combining existing arguments. 
While they focus on grouping similar arguments, we aim for an aggregated representation of debates in terms of perspectivized stances that reflect diverging and unified viewpoints of relevant stakeholder groups.

Some recent work leverages LLMs to model deliberative processes. E.g., \citet{bakker-etal-2022-finetuning} investigate if LMs can support humans in finding agreement on conflicting issues. They task LLMs to expand a corpus from a set of human-elicited questions and opinions on moral and political issues, 
and train a reward model to rate LM-generated consensus statements. 
They report high performance of LLMs generating consensual statements. However the evaluations do not report detailed statistics, and since the data
(worth Â£46,000) is not made public, it remains unclear if the evaluation involves notable conflicts to start with.\footnote{For evaluation,  opinions are clustered by topic (\textit{not} by the original issues). 
Fig.\ 2B of the work splits the data into divisive and non-divisive questions within a group, where only 50\% were found to be divisive. The win rates for their model over baselines are similar between (non)divisive  questions, and the analysis does not detail agreement score differences between positioned vs.\ agreement statements for the divisive subset. Without access to the data nor detailed analyses, it is unclear whether the re-clustered data involves notable conflicts.}


Our work is of smaller scale, but relies on arguments from a curated and accessible debate portal. 
In contrast to their work -- which is elusive on %
\textit{which} divisive arguments a consensus statement is meant to resolve --
we explicitly represent arguments as stance vectors along conceptual perspectives, from which we compute highly interpretable acceptability scores as a basis for finding consensual solutions for conflicting arguments. Since our method is interpretable, this can 
increase trust, and thereby usability, compared to purely generative methods.