\section{Related Work}
\paragraph{KV Cache Compression}
A large body of work has recently focused on KV cache compression for Transformer LLMs. 
%
These algorithms exploit either structural or learned sparsity patterns in the input embeddings.
%
\citet{Zhang2023}, for instance, proposed an eviction scheme for the KV cache that leverages techniques from dynamic submodular maximization. 
%
\citet{xiaoefficient} observed empirically that the initial tokens in the input sequence---which they called \textit{attention sinks}---often retain high attention weights throughout the entire generation process. 
%
This suggests a natural way to compress the KV cache by storing these sinks, in addition to a sliding window of recent token embeddings. 
%
In a similar fashion, \citet{liu2023scissorhands} observe that tokens with high attention weights tend to remain highly influential to the attention output, a phenomenon they call \textit{persistence of importance}. 
%
The work of \citet{liu2023deja} has also been influential in this direction, proposing the importance of \textit{contextual sparsity} in Transformer models. 

Beyond the assumptions of sparsity, the work of \citet{Zandieh2024} leverages \textit{clusterability} properties of the key embeddings to achieve provably sublinear space in certain parameter regimes. 
%
More hardware-oriented methods have also been proposed, such as the works of \citet{Duanmu2024} and \citet{zandieh2024qjl} which leverage \textit{quantization}, and the work of \citet{Jin2024} which examines the I/O complexity of the problem.

\paragraph{Impossibility Results for Transformers}
As research towards efficient Transformers progresses, complexity-theoretic techniques have helped shed light on their limitations.
%
The computation of the full attention matrix has been shown to require $\Omega(n^2)$ time via fine-grained complexity reductions \citep{Keles2022,alman2024fast}. 
%
Similar impossibility results have also been shown for the attention gradients \citep{alman2024fine}.
% 
Furthermore, important insights have been obtained by studying the limitations of Transformers as a computational model through the lens of communication complexity and circuit complexity \citep{alman2024fundamental,chen2024circuit}. 
%
Finally, the representational and generalization power of Transformers has been theoretically studied as well \citep{sanford2024transformers,sanford2024representational,li2023transformers}.