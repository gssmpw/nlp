\section{Related Work}
\paragraph{KV Cache Compression}
A large body of work has recently focused on KV cache compression for Transformer LLMs. 
%
These algorithms exploit either structural or learned sparsity patterns in the input embeddings.
%
____, for instance, proposed an eviction scheme for the KV cache that leverages techniques from dynamic submodular maximization. 
%
____ observed empirically that the initial tokens in the input sequence---which they called \textit{attention sinks}---often retain high attention weights throughout the entire generation process. 
%
This suggests a natural way to compress the KV cache by storing these sinks, in addition to a sliding window of recent token embeddings. 
%
In a similar fashion, ____ observe that tokens with high attention weights tend to remain highly influential to the attention output, a phenomenon they call \textit{persistence of importance}. 
%
The work of ____ has also been influential in this direction, proposing the importance of \textit{contextual sparsity} in Transformer models. 

Beyond the assumptions of sparsity, the work of ____ leverages \textit{clusterability} properties of the key embeddings to achieve provably sublinear space in certain parameter regimes. 
%
More hardware-oriented methods have also been proposed, such as the works of ____ and ____ which leverage \textit{quantization}, and the work of ____ which examines the I/O complexity of the problem.

\paragraph{Impossibility Results for Transformers}
As research towards efficient Transformers progresses, complexity-theoretic techniques have helped shed light on their limitations.
%
The computation of the full attention matrix has been shown to require $\Omega(n^2)$ time via fine-grained complexity reductions ____. 
%
Similar impossibility results have also been shown for the attention gradients ____.
% 
Furthermore, important insights have been obtained by studying the limitations of Transformers as a computational model through the lens of communication complexity and circuit complexity ____. 
%
Finally, the representational and generalization power of Transformers has been theoretically studied as well ____.