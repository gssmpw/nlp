\section{Related Work}
\paragraph{Register Buffers in Transformers} \citet{darcet2024vision} observed that vision transformers exhibit high-norm outlier tokens, similar to LLMs, and that registers (data-independent tokens) are needed to prevent them from arising. This aligns closely with the perspective proposed by \citet{sun2024massive, gu2024attention} that the first token attention sink is serving as a bias term for the attention mechanism.  
    \paragraph{Adam Causes Sinks and Outliers} Another recent avenue of investigation is whether attention sinks and outlier feature dimensions are a by-product of the optimizer. Both \citet{kaul2024attentionactivationunravellingenigmas} and \citet{guo2024activedormantattentionheadsmechanistically}, show that the Adam optimizer \citep{adam} leads to attention sinks and outlier features. In the former, they propose OrthoAdam which applies a rotation to the gradients to prevent any specific outlier dimensions. 
    \paragraph{Rank Collapse} Recent studies have shown that stacking self-attention layers in transformers can lead to rank collapse, where token representations lose dimensionality due to inherent architectural properties \citep{noci2022signalpropagationtransformerstheoretical, emergence-of-clusters-rigollet}. This phenomenon may stem from the `catch, tag, release' mechanism, in which attention concentrates around attention sinks, causing tokens to cluster around them and become confined to a low-rank subspace.
    \paragraph{Low-Rank Terms for Model Compression} Beyond OATS, other works have also proposed leveraging low-rank terms to mitigate compression loss for deep networks \citep{reconstruct_slr, mozaffari2024slimoneshotquantizedsparse, li2024svdquant}.  Another line of research is to incorporate a low-rank adapter during compression that is fine-tuned to mitigate drops in performance \citep{pmlr-v202-li23ap, zhang2023pruning, guo2024lqlora, zhao2024apt, mozaffari2025slopedoubleprunedsparseplus}.