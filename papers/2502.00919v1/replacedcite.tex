\section{Related Work}
\paragraph{Register Buffers in Transformers} ____ observed that vision transformers exhibit high-norm outlier tokens, similar to LLMs, and that registers (data-independent tokens) are needed to prevent them from arising. This aligns closely with the perspective proposed by ____ that the first token attention sink is serving as a bias term for the attention mechanism.  
    \paragraph{Adam Causes Sinks and Outliers} Another recent avenue of investigation is whether attention sinks and outlier feature dimensions are a by-product of the optimizer. Both ____ and ____, show that the Adam optimizer ____ leads to attention sinks and outlier features. In the former, they propose OrthoAdam which applies a rotation to the gradients to prevent any specific outlier dimensions. 
    \paragraph{Rank Collapse} Recent studies have shown that stacking self-attention layers in transformers can lead to rank collapse, where token representations lose dimensionality due to inherent architectural properties ____. This phenomenon may stem from the `catch, tag, release' mechanism, in which attention concentrates around attention sinks, causing tokens to cluster around them and become confined to a low-rank subspace.
    \paragraph{Low-Rank Terms for Model Compression} Beyond OATS, other works have also proposed leveraging low-rank terms to mitigate compression loss for deep networks ____.  Another line of research is to incorporate a low-rank adapter during compression that is fine-tuned to mitigate drops in performance ____.