\section{Systems Biology}

System biology \cite{kitano2002systems,alon2019introduction} is an interdisciplinary field that aims to understand biological systems at the system level. It involves the integration of experimental and computational techniques to study the interactions and dynamics of biological components.

% \begin{figure}[h]
% \centering
% \includegraphics[width=1\textwidth]{fig/systems_biology.pdf}
% \caption[Systems biology circle] {Systems biology circle. The diagram illustrates the iterative process of the systems biology cycle, which involves both reality and mathematics steps. The image was sourced from open-access websites \cite{wikipediaepidemiology,covidcases}.}
% \label{fig:system_biology_circle}
% \end{figure}

% The systems biology circle \cite{palsson2015systems}, depicted in Figure \ref{fig:system_biology_circle}, represents the iterative approach used to model biological systems. 

The process of applying systems biology to a specific problem starts with identifying an issue within the biological system. This, in turn, leads to constructing a realistic model that captures the underlying structure of the system. This model is then mathematically modeled to obtain a solution for the initial problem. After obtaining a mathematical solution, it is validated by comparing it to experimental data. If necessary, the model is refined, and the cycle begins again. Moreover, interpreting the mathematical solution back into the real model provides insights and enhances the understanding of the system. Thus, the systems biology circle involves a continuous iteration between reality and mathematics, where each step builds on the previous one to gain a profound understanding of biological systems.


One crucial step in the systems biology circle is modeling, which includes creating mathematical models of biological systems. These models are essential in comprehending complex biological processes and predicting how biological systems behave under different conditions. Systems biology modeling has been applied to investigate multiple biological processes, including but not limited to biological circuits \cite{alon2019introduction}, bacterial networks \cite{covert2004integrating} and cell signal pathways \cite{kholodenko2006cell}.

Stochastic and deterministic modeling are two approaches used in system biology modeling \cite{srivastava2002stochastic}. The former considers the randomness in biological systems, while the latter assumes that the behavior of biological systems can be precisely predicted. Stochastic modeling is more appropriate when dealing with systems with small numbers of molecules, while deterministic modeling is suitable for larger systems.

Partial and ordinary differential equations are one approach in deterministic modeling used in system biology. Ordinary differential equations (ODEs) are used to model systems with continuous variables, such as time. In contrast, partial differential equations (PDEs) are used to model systems with more than one continuous variable, such as space and time. PDEs are particularly useful for studying systems that involve diffusion or transport, such as the spread of a signaling molecule in a tissue.

However, one of the main challenges in using differential equations in system biology modeling is the difficulty in solving them. Analytical solutions are often impossible to achieve, and numerical methods must be used to approximate the solutions. Additionally, the parameters and initial conditions used in the models must be estimated from experimental data, which can be challenging due to the complexity of biological systems.

% \paragraph{Mathematical modeling.}
% A mathematical model is a description of a system using mathematical concepts and language. The process of developing a mathematical model is termed mathematical modeling. The potential of mathematical models to analyze biological systems has been widely recognized in recent decades. However, the reliability and explanatory power of such models mainly depends on the chosen modeling methods, which can vary widely in various aspects, such as the data set size and their level of approximations on which they are based. Today, mathematical modeling still lacks a gold standard due to its high specificity to the underlying systems biology problem. In fact, each model can only provide an approximate description of a specified problem in nature, and a great challenge lies in applying or even developing modeling techniques that answer the questions posed in the best possible way with reasonable effort.

% % \paragraph{Modeling approach: Deterministic ODE, PDE, Stochastic}
% \paragraph{Mathematical modeling approaches.}

% % The host of existing modeling methods can be grouped according to various criteria, such as deterministic vs. stochastic, linear vs. nonlinear, discrete vs. continuous, etc.

% % An important classification distinguishes between deterministic and stochastic models. In deterministic modeling, randomness within the system is ignored. One of the most commonly used deterministic methods is the ordinary differential equation (ODE), which is based on the phenomenological law of mass action. They provide a dynamic and quantitative description of spatially homogeneous systems. Since ODEs are also widely used in other scientific fields, many analytical techniques and simulation methods have been developed so far. In theoretical biology, ODEs have been applied to a wide range of problems, such as the description of cell-cell interactions \cite{peng2016prediction, peng2013modeling}, synergism of drug combinations \cite{peng2011drug, yin2014synergistic} and the dynamic changes of metabolites \cite{finn2011switching, covert2008integrating}.

% % On the other hand, biological systems are always subject to stochastic effects, which may occur at all levels, from the molecular to the macroscopic. These can be captured by stochastic models. The chemical master equation (CME) \cite{van1992stochastic} and the Gillespie algorithm \cite{gillespie2001approximate} with their approximate variants are widely applied to these stochastic models. Some recent stochastic models in biological fields include immunology \cite{kolev2012adaptive,kolev2012mathematical}, epidemiology \cite{chen2005stochastic}, neurology \cite{rao2002probabilistic}, microbiology \cite{durbin1998biological,korpusik2016single}, cancer research \cite{jackiewicz2009correlation,kolev2013numerical}, etc, Compared to their deterministic counterparts, stochastic models are in general more difficult to analyze.


% Mathematical modeling methods can be classified in several ways, including by determinism versus stochasticity, linearity versus nonlinearity, and discreteness versus continuity.

% One important classification is between deterministic and stochastic models. In deterministic modeling, randomness within the system is not considered. One widely used deterministic approach is the ordinary differential equation (ODE), which is based on the phenomenological law of mass action. ODEs provide a dynamic and quantitative description of spatially homogeneous systems and have been extensively developed in various scientific fields. In theoretical biology, ODEs have been applied to a wide range of problems, including the description of cell-cell interactions \cite{peng2016prediction, peng2013modeling}, drug synergism \cite{peng2011drug, yin2014synergistic}, and metabolite dynamics \cite{finn2011switching, covert2008integrating}.

% In contrast, biological systems are subject to stochastic effects, which can occur at all levels from the molecular to the macroscopic. Stochastic models are designed to capture these effects. The chemical master equation (CME) \cite{van1992stochastic} and the Gillespie algorithm \cite{gillespie2001approximate} and their approximations are widely used for stochastic models. Recent stochastic models in biology have focused on fields such as immunology \cite{kolev2012adaptive, kolev2012mathematical}, epidemiology \cite{chen2005stochastic}, neurology \cite{rao2002probabilistic}, microbiology \cite{durbin1998biological, korpusik2016single}, and cancer research \cite{jackiewicz2009correlation, kolev2013numerical}. Stochastic models are generally more challenging to analyze than deterministic models.





\section{Scientific Machine Learning}


Scientific machine learning (SciML) \cite{rackauckas2020universal,roscher2020explainable} is a rapidly growing field that combines machine learning algorithms with the fundamental principles of science. In physics, SciML has the potential to revolutionize the way we understand and study complex physical systems. By leveraging the power of machine learning, researchers can extract valuable insights from large datasets, simulate complex physical processes, and even discover new phenomena that were previously unknown.

One of the most exciting applications of SciML in physics is the simulation of quantum systems \cite{rupp2015machine,torlai2020machine}. Quantum mechanics is notoriously difficult to understand and predict, but machine learning algorithms have shown promise in modeling and predicting the behavior of quantum systems. By training machine learning models on large datasets of quantum data, researchers can gain insights into the behavior of these systems and make predictions about their future behavior \cite{mehta2019high}. Another application of SciML in physics is in the field of cosmology \cite{villaescusa2021camels,carleo2019machine,mathuriya2018cosmoflow}. Cosmologists study the universe at the largest scales, and machine learning algorithms can help analyze the vast amounts of data collected by telescopes and observatories. By training machine learning models on large cosmological datasets, researchers can identify patterns and structures in the universe, process large 3D dark matter distribution \cite{mathuriya2018cosmoflow}, and even test fundamental physical theories like general relativity \cite{alestas2022machine}.

Scientific machine learning has the potential to transform the way we study and understand physical systems. As the field continues to develop and mature, we can expect to see even more exciting discoveries and breakthroughs in the years to come.

\section{Related Work}
%Traditional Approaches
The development of mathematical models and numerical methods for solving ordinary and partial differential equations has been a major focus of research for decades. These models and methods are widely used in many scientific fields, such as physics, engineering, and biology, to understand complex systems and make predictions about their behavior. Numerical approaches for solving differential equations, such as Euler's Method \cite{euler1952methodus}, Runge-Kutta Method \cite{runge1895numerische}, the finite element method (FEM) \cite{herrmann1967finite}, finite difference method (FDM) \cite{richardson1911ix} and finite volume method (FVM) \cite{eymard2000finite}, usually involve analytical or numerical techniques that rely on well-established mathematical theories.
These methods have proven to be effective and accurate for many problems, but they often require significant computational resources and can be limited in their ability to handle complex systems. In recent years, machine learning approaches have emerged as a promising alternative to traditional methods for solving differential equations. In this section, we will review both traditional and machine learning approaches for solving differential equations, with a focus on their applications in scientific problems.

% In this section, we will review machine learning approaches for solving differential equations, with a particular focus on physics-informed neural networks (PINN), which has demonstrated remarkable performance in various scientific fields and have emerged as a promising member of the machine learning family.

\subsection{Machine Learning Approaches}

Traditional numerical methods for solving differential equations often involve discretizing the space, which leads to a trade-off between computational efficiency and resolution. Finer grids offer better accuracy but can be computationally expensive, while coarser grids are faster but less accurate. To address these limitations, machine learning approaches such as Neural Ordinary Differential Equations (Neural ODEs) \cite{chen2018neural}, Deep Galerkin Method (DGM) \cite{sirignano2018dgm}, Physics-informed Neural Network (PINN) \cite{raissi2019physics}, and Fourier Neural Operator (FNO) have emerged as promising alternatives.

Neural ODEs represent dynamical systems using continuous-time ordinary differential equations, where the coefficients are neural network functions. They can learn complex dynamics from data even when the underlying system is not well understood. Neural ODEs have been successfully applied to a wide range of problems, including image processing \cite{chen2020mri} and time-series forecasting \cite{chen2022forecasting,gao2022explainable}.

DGM is a machine learning approach that uses deep neural networks to solve partial differential equations (PDEs). It approximates the PDE solution by replacing the finite-dimensional subspace in the Galerkin method with a neural network. DGM has shown promising results in solving various PDEs, including fluid dynamics \cite{li2022deep} and heat transfer problems \cite{zhang2022deep}.

In addition, Physics-informed Neural Network and Fourier Neural Operator are essential in the field of solving differential equations using machine learning. In the following subsections, we will discuss these methods in detail.

% Since numerical differential equation solutions solve equations by discretizing the space, they make a trade-off in resolution: coarse grids are fast but less accurate; fine grids are accurate but slow. 
% To avoid the embarrassing situation, some machine learning solvers involving specified machine learning techniques are applied to solve ODEs and PDEs in recent approaches.

% , such as Neural ODE, Taylor Series, and Fourier Neural Operator.
% NeuralODE, Taylor, Fourier Neural Operator, and others


% \paragraph{Neural Ordinary Differential Equations.}
% Neural Ordinary Differential Equations (Neural ODEs) \cite{chen2018neural} use a neural network to model the instantaneous rate of change in the state of a system. However, despite their apparent suitability for dynamics-governed time-series, Neural ODEs present a few disadvantages. First, they are unable to adapt to incoming data points, a fundamental requirement for real-time applications imposed by the natural direction of time.
% Second, time series are often composed of a sparse set of measurements that could be explained by many possible underlying dynamics. Neural ODEs do not capture this uncertainty. In contrast, Neural Processes (NPs) are a family of models providing uncertainty estimation and fast data adaptation but lack an explicit treatment of the flow of time.

% Neural Ordinary Differential Equations (Neural ODEs) \cite{chen2018neural} are a type of neural network that uses ordinary differential equations (ODEs) to model continuous-time dynamics. Unlike traditional neural networks that use discrete-time updates, Neural ODEs are trained to find a continuous function that describes the evolution of a system over time. This allows them to capture arbitrarily long-term dependencies and provide a more accurate representation of dynamic systems. Neural ODEs have been successfully applied in various fields, such as image recognition, physics, and neuroscience.

% Neural Partial Differential Equations (Neural PDEs) \cite{} use partial differential equations (PDEs) to model the dynamics of a system. Unlike Neural ODEs, which operate on a single dimension, Neural PDEs can model systems in multiple dimensions, making them flexibility for more complex problems in terms of dimensions and applications. Neural PDEs have been used in applications such as fluid dynamics, climate modeling, and materials science. One advantage of Neural PDEs is their ability to handle noise and uncertainty, making them useful for modeling complex systems that involve stochasticity.
% \paragraph{Taylor Series.}
% Taylor series method \cite{corliss1982solving} helps to compute a solution to an initial value problem in ordinary differential equations by expanding each component of the solution in a long Taylor series. The object reference model parameters are embedded as variables, and the power series can be inverted to yield the inverse function. However, though Taylor series methods is very useful for derivations and can be used to get theoretical error bounds, it has some weakness.
% First, Successive terms get very complex and hard to derive. As a result, in practical we avoid using higher-order Taylor expansions.
% Second, the truncation error tends to grow rapidly away from the expansion point, and the results are always not as efficient as curve fitting or direct approximation.

% To address these problems, we introduce Neural ODE Processes (NDPs), a new class of stochastic processes determined by a distribution over Neural ODEs. By maintaining an adaptive data-dependent distribution over the underlying ODE, we show that our model can successfully capture the dynamics of low-dimensional systems from just a few data points. At the same time, we demonstrate that NDPs scale up to challenging high-dimensional time-series with unknown latent dynamics such as rotating MNIST digits.

% \paragraph{Fourier Neural Operator.}
% Silvescu et al.(1999) \cite{silvescu1999fourier} proposed the Fourier Neural Networks using the fast Fourier transform (FFT), and Li et al.(2020) \cite{li2020fourier} developed this technique and proposed the Fourier neural operator (FNO), which provides a data-driven method for solving differential equations. FNO uses neural networks to learn gridless, infinite-dimensional operators. Neural operators correct the grid-dependent properties of the finite-dimensional operator approach described above by producing a set of network operating parameters that can be used for different discretizations. It has the ability to transfer solutions between meshes. Compared with the former, FNO has more advantages in terms of success and accuracy, but the premise is that there is a sufficient data set, which is precisely the most difficult to achieve.


\subsection{Physics-informed Neural Network}
% \begin{figure}[h]
% \centering
% \includegraphics[width=1\textwidth]{fig/PINN.png}
% \caption{Schematic of a physics-informed neural network (PINN). A fully-connected neural network, with time and space coordinates $\left(t, x\right)$ as inputs, is used to approximate the multi-physics solutions $\hat{u} = \left[u, v, p, \phi \right]$. The derivatives of $\hat{u}$ with respect to the inputs are calculated using automatic differentiation (AD) and then used to formulate the residuals of the governing equations in the loss function, that is generally composed of multiple terms weighted by different coefficients. The parameters of the neural network $\theta$ and the unknown PDE parameters $\lambda$ can be learned simultaneously by minimizing the loss function}
% \label{fig:pinn}
% \end{figure}


% Likewise, other Authors have provided alternative paths in data-driven scientific computing fields. For instance, Arka et al. \cite{daw2017physics} employs a framework that uses physical properties as soft constraints on the loss function. It utilizes this new loss function to model the relationship between lake temperature and other physics variables. The authors demonstrate that this hybrid model, PGNN, has better generalizability and robustness than other typical deep learning approaches. It is indeed quite common to modify the loss function in order to take scientific constraints into account. Other papers also followed this doctrine, although they have objectives. There are attempts to change or replace an existing physical model by using physics-guided loss functions.\cite{alvarez2009latent,doan2019physics,fioretto2020predicting}. Sometimes, people do not have physical models ready, and can instead use mathematical approximations. This goal can also be accomplished with the help of physics-guided loss functions.\cite{beucler2021enforcing,beucler2019achieving,yazdani2020systems}. Another popular application is to solve PDE systems, since the loss functions do take the derivatives into account. This makes them well-suited for solving PDE systems. \cite{baseman2018physics,dwivedi2020solution,karumuri2020simulator}


% \subsection{Inverse Sparse Identification of Nonlinear Dynamics (SINDy)}
% PINN 
% Recently, the incorporation of machine learning techniques has gained increasing attention as a promising approach to accelerate modeling and simulation in physical, chemical, and biological application scenarios~\cite{snyders2001inductive,mansour2019deep,mitchell2017spatial,feinman2018learning}.
The Physics-informed Neural Network (PINN), a machine learning technique proposed for solving differential equations, has gained much popularity for its ability to achieve promising results for various problems in computational science and engineering~\cite{cai2022physics,misyris2020physics,cai2021physics,ji2021stiff,zanardi2022towards,lu2022deep,lv2021novel,sun2020surrogate,raissi2019deep,jin2021nsfnets,kissas2020machine,sahli2020physics}.

The primary objective of PINN is to utilize nonlinear partial differential equations to guide the supervised learning process. The model employs a deep learning network that can accurately identify nonlinear mappings from high-dimensional input and output data. The physical laws of the system act as a regulatory term, constraining the model to adhere to the governing equations. The authors successfully overcome the challenge of inadequate labeled data by using a large number of collocation points while obtaining favorable results in fluid dynamics, quantum mechanics, and reaction-diffusion problems. PINN is a meshless approach that transforms the initial and boundary value problems into optimization problems to solve differential equations.
That is, provided with explicit differential equations and initial and boundary conditions, PINN can solve a system without requiring labeled training data (ground truth solutions).
In the field of systems biology, two methods have been proposed as a domain translation of the PINN technique, such as 
biologically-informed neural network~\cite{lagergren2020biologically,greene2020biologically} and systems biology-informed deep learning~\cite{yazdani2020systems}.
Several real biological applications have taken advantage of PINN for model design and analysis, including soft biological tissue model~\cite{liu2020generic}, cardiac activation mapping~\cite{sahli2020physics}, and thrombus material properties~\cite{yin2021non}.

% While PINN has gained promising results across a wide range of problems in computational science and engineering \cite{sun2020surrogate,raissi2019deep,jin2021nsfnets,kissas2020machine,sahli2020physics}, it fails to achieve reliable training and correct solutions for complex systems and are prone to high computational costs \cite{krishnapriyan2021characterizing,lu2021learning}.

Although PINN has gained promising results across a wide range of problems in computational science and engineering \cite{sun2020surrogate,raissi2019deep,jin2021nsfnets,kissas2020machine,sahli2020physics}, there are two major deficiencies that hinder its broad application in biological and physical sciences: (i) it sometimes fails to achieve reliable training and correct solutions for complex systems~\cite{krishnapriyan2021characterizing, lu2021deepxde, meng2022physics}, and (ii) it is prone to high computational costs~\cite{anantharaman2021stably,jagtap2020locally,jagtap2020conservative,lu2021learning}.
To address these problems, researchers have been working on developing and optimizing the PINN algorithm.
A series of domain decomposition-based PINNs (CPINN, DPINN, XPINN) are presented to divide the computational domain into smaller subdomains~\cite{dwivedi2019distributed, jagtap2020conservative, jagtap2021extended}, thereby decomposing complex problems into smaller subproblems that can be solved by minimal size local PINN to solve.
The gradient-enhanced PINN (GPINN)~\cite{yu2022gradient} performs gradient enhancement on PINN by forcing the derivative of PDEs residual to be zero, which aids in faster convergence and improved accuracy of the model.
For small sampling data, few-shot learning is added to PINN, in which a neural network is used to train an approximate solution first and further optimized by minimizing the designed cost function~\cite{li2021deep}. 
% For linear problems, the Bayesian physical extreme machine learning is employed to enhance PINN, which increases the calculation speed by several orders of magnitude~\cite{liu2022bayesian}.
For stiff problems, Wang et al.~\cite{raissi2019physics} proposed a new network structure to deal with the numerical stiffness of the backpropagation gradient that causes PINN to fail.
Other methods focus on tailoring PINN architecture for a specific physical problem~\cite{wu2021modified, baddoo2021physics} and are difficult to generalize to systems biology applications.
% However, these techniques currently are difficult to  scale to the very large mo dels being used in practice by systems biologists. % copied
More research is needed to scale the generalizability and stability for these techniques to be applicable across the continuum of biological models.

% Fourier 
\subsection{Fourier Neural Operator}
Inspired by the Fourier neural networks (FNN) that apply the fast Fourier transform to the neural network~\cite{silvescu1999fourier}, a Fourier neural operator (FNO, a data-driven approach) is developed to learn a mapping of solutions to a complete family of differential equations~\cite{li2020fourier}. 
The Fourier transform reduces the computational complexity to quasi-linear, enabling large-scale calculations. 
Compared to traditional PDE solvers, FNO's resolution invariance property offers high accuracy at a low computational cost, making it a state-of-the-art approach for approximating PDEs.
Grady et al.\cite{grady2022towards} later break the limitation that FNO can only solve problems below three dimensions and mathematically proved that the proposed model-parallel Fourier neural operators (MP-FNO) can be used for dimensional data of any size. 
% And mathematically deduce all the necessary components of MP-FNO. MP-FNO is solving large-scale Parametric PDEs have a significant speed-up effect.
%Factorized Fourier Neural Operator (F-FNO)~\cite{tran2021factorized} has made a number of improvements on the basis of FNO to achieve better results. Factorize the input multi-dimensional Fourier transform and share the kernel integral operator parameters of all Fourier layers, which greatly reduces the amount of parameters. The residual structure added by F-FNO can greatly deepen the Fourier operator layers.
From the application perspective, FNO is customized to solve different problems, such as LAFNO~\cite{peng2022linear} for modeling 3D turbulence, PFNO~\cite{li2022solving} for solving seismic wavefield equations in velocity models, and IFNO~\cite{you2022learning} for predicting the mechanical response of materials.
In addition to learning challenging PDEs, FNO also achieved good results in the field of Vision transformers. The adaptive Fourier neural operator (AFNO)~\cite{guibas2021adaptive} draws on the idea of FNO and performs token mixing in the frequency domain through Fourier transform to understand the relationships among objects in a scene.
However, the above FNO-related methods are data-driven approaches that require ground truth solutions as labeled data to train the neural networks. 

