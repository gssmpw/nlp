\label{cha:method}

% \numberwithin{equation}{section}
% Structure

% Let $FFT$ denotes the Fast Fourier Transform of a function $f$ and $IFFT$ its inverse function, then

% $FFT_{f}\left(k\right)=\int _{D}\left( f_{j}\left(x\right)\cdot e^{-2i\pi\left<x,k\right>}\right) dx$,

% $IFFT_{f}\left(x\right)=\int _{D}\left( f_{j}\left(k\right)\cdot e^{2i\pi\left<x,k\right>}\right) dk$,

% Taking the xxxxx model as a guiding example ... 




\section{Fourier-enhanced Neural Network}
\label{cha:fnn}

 
To begin with, consider a general systems biology model, mathematically formulated as a PDE system:
\begin{equation}
y_{t}=\mathcal{G}\left[y\right](x), x\in \Omega,t\in \mathcal{T}
\end{equation}
\begin{equation}
y\left(\cdot, x\right) = g\left(x\right), x\in \partial\Omega
\end{equation}
where $\mathcal{G}$ is the differential operator for the PDE, $\Omega$ is the domain, $\mathcal{T}=\left[0,T\right]$ is the time domain, $\partial\Omega$ is the boundary of the domain, $x$ represents samples in the domain, $y\left(t,x\right)$ denotes the latent (hidden) solution, $g\left(\cdot\right)$ is a given boundary condition function.

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{fig/SB-FNN.pdf}
\caption[Fourier-enhanced Neural Network for dynamic systems biology model prediction] {Fourier-enhanced Neural Network for dynamic systems biology model prediction. (a) The SB-FNN includes an input fully-connected neural network $\mathcal{U}$, $l$ Fourier layers, as well as an output fully connected neural network. (b) The loss function in the SB-FNN model is composed of four different loss components $Loss_{o}$ (initial condition), $Loss_{f}$ (residual loss), $Loss_{b}$ (boundary condition) and $Loss_{p}$ (physical penalty), each with its corresponding hyperparameter $\lambda_o$, $\lambda_f$, $\lambda_b$ and $\lambda_p$.}
\label{fig:SB-FNN}
\end{figure}

Let $\mathcal{X}\in \mathbb{R}^{T_{N}\times D}$ denotes the input of the Fourier neural network, where $T_{N}$ is the number of samples selected from time domain $\mathcal{T}=\left[0,T\right]$, and $D=\left|\Omega\right|$ is dimension of the spatial domain. The task of the network is to derive a stable estimation of $\mathcal{Y}\in \mathbb{R}^{T_{N}\times D}$, the stable latent solution. Motivated by ..., we propose FNN to map $\mathcal{X}$ to $\mathcal{Y}$:
\begin{equation}
\mathcal{Y}=SB\text{-}FNN\left(\mathcal{X}\right)=\left( V \circ L^{\left(l\right)} \circ \cdots \circ L^{\left(1\right)}\circ U \right)\left(\mathcal{X}\right),
\end{equation}
where $\circ$ is function composition, $ U$ is a fully-connected neural network that maps the input $\mathcal{X}$ into the initial hidden latent space $\mathcal{Z}^{\left(0\right)}\in \mathbb{R}^{T_{N}\times H}$, $H$ is the dimension of hidden latent space, $L^{\left(i\right)}$ indicates a Fourier Layer mapping the $\left(i-1\right)^{th}$ hidden latent space $\mathcal{Z}^{\left(i-1\right)}$ to the $i^{th}$ hidden latent space $\mathcal{Z}^{\left(i\right)}$, $ V$ is a fully-connected neural network that maps the final hidden latent space $\mathcal{Z}^{\left(l\right)}$ to the output $\mathcal{Y}$, and $l$ denotes the number of Fourier Layers.

Formulate each Fourier Layer $L^{\left(i\right)}$ as:
% Inspired by ..., we formulate each Fourier Layer $L^{\left(i\right)}$ as:
\begin{equation}
\mathcal{Z}^{\left(i\right)}=L^{\left(i\right)}\left(\mathcal{Z}^{\left(i-1\right)}\right)=\sigma\left( \mathcal{F}^{-1}_{M}\left(\mathcal{W}^{\left(i\right)}\times \mathcal{F}_{M}\left(\mathcal{Z}^{\left(i-1\right)}\right)\right) + C^{\left(i\right)}\left(\mathcal{Z}^{\left(i-1\right)}\right)\right),
\end{equation}

where $\sigma$ is a non-linear activation function, $C^{\left(i\right)}$ is a convolution neural network (CNN), $\mathcal{W}^{\left(i\right)}\in H\times H\times M$ is a weight matrix, $\mathcal{F}_{M}$ denotes the Fast Fourier Transform with $M$ as the number of Fourier modes to keep and $\mathcal{F}^{-1}_{M}$ is its inverse function. What we need to optimize are the parameters involved in the neural networks $U$, $V$ and the weight matrices $\mathcal{W}=\left\{\mathcal{W}^{\left(i\right)}\right\}^{l}_{i=1}$.


% $f\left(y\right)=u_{t}-\mathcal{G}\left[y\right](x)$, generally,
% \textbf{Solution to the initial value problem (IVP)}
To ensure the outputs of the neural networks follow the known biological mechanisms, the predicted $\widehat{y}$ is then brought into the corresponding multi-scale model as shown in Figure \ref{fig:SB-FNN}, represented as a set of partial differential equations with an initial condition $\widehat{y_0}=\widehat{y}|_{t=0}$ to calculate the distance from the model over a set of residual points over both time and spatial domain. This can be formulated as an optimization problem:
\begin{equation}
\min \limits_{\theta }\mathcal{L}_{SB-FNN} =
\lambda_{o}\left\| \widehat{y}_{0}-y_{0}\right\|_{2}
+ \lambda_{f}\left\| \widehat{y_{t}}-\mathcal{G}\left[\widehat{y}\right]\right\|
 +\lambda_{b}\left\| \widehat{y}\left(t,x\right)-g\left(x\right)\right\|_{2} 
 +\lambda_{p}\cdot \mathcal{P}\left(\widehat{y}\right),
\label{eq:loss}
\end{equation}

where $\theta$ denotes the parameters in neural networks $U$, $V$ and the weight matrices $\mathcal{W}$, $\mathcal{P}$ is an optimization penalty function used to restrict $y$ which will later be introduced, hyperparameters $\lambda_{f}$ controls the mismatch between the predicted gradient of $y$ and the gradient of $y$ derived by $\widehat{y}$ following the PDE equations, and $\lambda_{o}$, $\lambda_{b}$, $\lambda_{p}$ controls the discrepancy over boundary conditions, initial condition, optimization penalty, respectively. Figure \ref{fig:SB-FNN} (b) depicts the flow chart that demonstrates how the loss function is calculated.


\section{Adaptive Activation Function}
\label{cha:adaptive}

In deep learning, a neural network without an activation function can only be regarded as a linear model with a simple linear relationship. This type of model is relatively easy to solve, but it also lacks expressiveness and cannot solve complex problems. Therefore, activation functions are usually added to neural networks to increase their ability to express complex models by introducing nonlinear transformations. The types of activation functions are diverse, each with its own characteristics, which makes choosing the right activation function for a specific problem a crucial task in designing an effective neural network.

% We consider a set of activation activation candidates to optimize the best

% Sigmoid \cite{rumelhart1986learning} is a class of functions with an exponential shape, which is closest to biological neurons in the physical sense. However, the most obvious shortcoming of Sigmoid is saturation. The derivatives on both sides gradually approach 0, and the gradient disappears easily. The ReLU \cite{nair2010rectified} activation function was proposed to solve the problem of gradient disappearance. ReLU is a one-sided saturated activation function that does not produce saturation in the positive area, i.e., there is no gradient disappearance in the positive area. However, when ReLU deals with negative inputs, it also faces a disadvantage of ``neuron death'', which is the situation where a neuron stops responding to any input and always outputs zero. SoftPlus \cite{glorot2011deep} can be used as a good alternative to ReLU. Unlike ReLU, the derivative of SoftPlus is continuous, non-zero, and ubiquitous, which can prevent the ``neuron death'' phenomenon in ReLU.

% Sigmoid \cite{rumelhart1986learning} is a popular activation function that is widely used in the field of deep learning. It has an S-shaped curve and maps input values to probabilities, ranging from 0 to 1, which can be expressed as
% \begin{equation}
% \label{eq:sigmoid}
% \operatorname{Sigmoid} (x) = \frac{1}{1 + e^{-x}}.
% \end{equation}
% Despite its popularity, the sigmoid function has some significant drawbacks. One of the most significant drawbacks is that it is prone to saturation, where the gradients become very small, and the learning process slows down significantly. This phenomenon is known as the vanishing gradient problem. Additionally, the output of sigmoid is not zero-centered, which can cause optimization difficulties.

Tanh \cite{ackley1985learning}, short for hyperbolic tangent, is a popular activation function used in deep learning. It has an S-shaped curve and maps input values to output values ranging from -1 to 1. Mathematically, it can be expressed as
\begin{equation}
\label{eq:tanh}
\operatorname{Tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}.
\end{equation}
Tanh is zero-centered, which helps in optimization, and it is less prone to the vanishing gradient problem.

ReLU \cite{nair2010rectified} stands for rectified linear unit and is a widely used activation function in deep learning, which can be written as
\begin{equation}
\label{eq:relu}
\operatorname{ReLU} (x) = \max(0, x).
\end{equation}
ReLU is simple and computationally efficient, making it an attractive choice. It is a one-sided activation function, which means that it is activated only when the input is positive. ReLU solves the vanishing gradient problem that occurs in sigmoid by preventing the gradients from becoming too small, which leads to faster convergence. However, ReLU can face a problem called ``neuron death'' in which the neurons do not fire or respond to any input, leading to a dead end in the learning process.

Softplus \cite{glorot2011deep} is a smooth and differentiable activation function that is similar to ReLU. It maps input values to positive values, and its derivative is always positive. It can be written as

\begin{equation}
\label{eq:softplus}
\operatorname{Softplus} (x) = \ln(1 + e^x).
\end{equation}
Softplus is a popular alternative to ReLU because it does not suffer from the ``neuron death'' problem that occurs in ReLU. However, Softplus is not perfect and has its own disadvantages. It is not zero-centered, and its derivative at the origin is not defined, which can cause optimization problems.



The Exponential Linear Unit (ELU) \cite{clevert2015fast} is another variant of the ReLU activation function that has a smooth nonlinearity. ELU is defined as 

\begin{equation}
\label{eq:elu}
\operatorname{ELU} (x) =
\begin{cases}
x, & \text{if } x > 0 \\
\alpha (e^x - 1), & \text{otherwise}
\end{cases}
,
\end{equation}
where $\alpha$ is a hyperparameter that controls the slope of the function for negative inputs. ELU has been shown to outperform ReLU on many tasks and is less prone to the ``neuron death'' problem.


The Gaussian Error Linear Unit (GELU) \cite{hendrycks2016gaussian} activation function is a variant of the ReLU activation function that adds nonlinearity with a smooth curve. GELU is defined as 
\begin{equation}
\label{eq:gelu}
\operatorname{GELU} \left(x\right) = x\cdot P(X \leq x) = x\cdot \int_{-\infty}^{x} \frac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}} dt,
\end{equation}


% \begin{equation}
% \label{eq:gelu}
% \operatorname{GELU} (x) = 0.5x(1+\operatorname{tanh}(\sqrt{2/\pi}(x+0.044715x^3)))
% \end{equation}

% \begin{equation}
% \Phi(x) = P(X \leq x) = \int_{-\infty}^{x} \frac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}} dt
% \end{equation}
where $P$ is the cumulative distribution function of the standard normal distribution. The advantage of GELU is that it has both the sparsity-inducing property of ReLU and the smoothness of Sigmoid, making it more expressive and efficient than other activation functions.



The Sine (Sin) activation function is a periodic activation function that can be used to model periodic functions. The function is represented by the equation:
\begin{equation}
\label{eq:sin}
\operatorname{Sin^\ast} \left(x\right) = \operatorname{sin}\left(\beta \cdot x\right),
\end{equation}
where $x$ is the input to the function and $\beta$ is a scaling factor. The scaling factor $\beta$ is typically initialized to 1, but it is a trainable variable that can change during the training process.
The advantage of the Sin activation function is that it can capture the periodicity of the input and can be used to model complex periodic functions, such as those found in time-series data. However, the Sin activation function can be computationally expensive due to the trigonometric operations involved, and it is not suitable for all types of problems.

According to recent studies, incorporating periodic factors into activation functions can enhance the fitting ability of neural networks when predicting periodic functions \cite{sitzmann2020implicit}. This suggests that using a combination of multiple activation functions may be more effective in neural network design than relying on a single type of activation function. For example, GELU has been shown to outperform other activation functions such as ReLU and sigmoid in certain neural network architectures, and ELU has been shown to provide faster convergence and better accuracy than other activation functions in certain situations.

In neural network design, the choice of activation function and its hyperparameters can greatly affect the training effect of the model, and is typically solved by researchers based on experience and trial and error. This process is particularly important for PINNs, where the choice of activation function can be critical due to the different properties of the physical systems being solved. However, trying all activation functions in one round for each learning task is inefficient. A mixed activation function can be learned by selecting several general activation functions and performing linear weighted summation to obtain a mixed function. This approach can make the selection or even design of the activation function into an optimization problem, improving the efficiency and effectiveness of PINN modeling. By choosing the appropriate set of activation functions for each task and Fourier layer, better results can be achieved than using a fixed activation function for the entire model.

\begin{equation}
\sigma_{adaptive} = \sum_{j=1}^k \sigma_{j} * w_j
\end{equation}

\begin{equation}
w_i=Softmax(r)_i = \frac{e^{r_{i}}}{\sum_{j=1}^k e^{r_{j}}} \ \ \ 
\end{equation}

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.5\textwidth]{fig/adaptive_1.pdf}
% \caption[Adaptive weights] {Adaptive weights. The adaptive weights, denoted as $w$, are generated by applying a Softmax operator to a trainable tensor vector, represented as $r$.}
% \label{fig:adaptive_weights}
% \end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{fig/adaptive_full.pdf}
\caption[Adaptive activation function] {Adaptive activation function. The adaptive weights, denoted as $w$, are generated by applying a Softmax operator to a trainable tensor vector, represented as $r$. In SB-FNN, Tanh, ReLU, Softplus, ELU, GELU, and Sin are chosen as the activation function candidates, as they have shown excellent performance across various models and tasks.}
\label{fig:adaptive_flow}
\end{figure}

Here $\sigma$ is the activation function candidate set, $k$ is the number of activation function candidates, $r$ is a trainable vector in length $k$ and $w$ is the transformed coefficient vector. In practice, we choose Tanh, ReLU, Softplus, ELU, GELU, and Sin as the activation function candidates, as they have shown excellent performance across various models and tasks. To obtain the optimal weighting coefficients for the activation functions, we employ a Softmax function to convert the trainable weight vector $z$ to the coefficient vector $w$. The flow of the designed adaptive activation function is shown in Figure \ref{fig:adaptive_flow}. By incorporating the mixed activation function into each Fourier layer, we replace the original GELU with a hybrid activation function, which allows each task and Fourier layer to have its own set of nonlinear transformations. This approach provides a more flexible and powerful way of nonlinear transformation, which can potentially improve the model performance.

We add a hybrid activation function to each Fourier layer with different trainable weight vector $z$, replacing the original GeLU, to optimize each hybrid activation function individually. The addition of a hybrid activation function to each Fourier layer offers a promising avenue for improving the performance and interpretability of deep learning models for physical systems. By leveraging prior knowledge about the system and carefully selecting the appropriate set of activation functions, we can build models that are both more effective and easier to understand.


\section{Variance Constraint}
\label{cha:cyclic}

Biological systems often exhibit ubiquitous and important oscillation patterns, especially for molecular systems. Therefore, we have developed a variance constraint, which is an oscillation penalty function, to help the model better obtain periodic characteristics:

\begin{equation}
Loss_{p}\left(\widehat{y}\right)=\lambda_{p}\cdot \mathcal{P}\left(\widehat{y}\right)=\lambda_{p}\cdot \sum^{D}_{i=1} \Phi\left(\operatorname{Var} \left( \operatorname{Norm} \left(\widehat{y}[i,:]\right)\right)\right)
\end{equation}

\begin{equation}
\Phi\left(x\right)=\dfrac{1}{2} \cdot (- \tanh\left(\left(x - \alpha\right) \cdot \tau\right) + 1)
\end{equation}
where $D$ denotes dimension of the spatial domain, $\lambda_{p}$ controls the weight of the variance penalty, $\operatorname{Var}\left(\cdot\right): \mathbb{R}^{n}\rightarrow \mathbb{R}$ is the variance of a vector, $\operatorname{Norm}\left(\cdot\right): \mathbb{R}^{n}\rightarrow \mathbb{R}^{n}$ is a normalization function that linearly normalizes a vector into a $[0,1]$ scale, $\Phi \left(\cdot\right): \mathbb{R}\rightarrow [0,1]$ represents a continuous oscillation-evaluation function used to punish low-variance curves, which involves a hyperbolic tangent function. When $x$ closes to $0$, it returns a value close to 1; While $x$ increases past a threshold $\alpha$, the function value reduces suddenly; When $x$ continues to increase, the function value gradually approaches $0$. The curve of the function $\Phi(x)$ in a plane is depicted in Figure \ref{fig:phi}. The hyper-parameters $\alpha$, $\tau$ need to be tailored to different systems biology models.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{fig/phi.png}
\caption[Plot of the variance penalty function] {Plot of the variance penalty function $\Phi\left(x\right)$. The parameter $\alpha$ is utilized to manage the position of the symmetry center of the function along the x-axis, and the parameter $\tau$ is used to control the rate of change of the function around $x=\alpha$. A larger value of $\tau$ results in a faster decrease of the function from $y=1$ to $y=0$ in the neighborhood of $x=\alpha$.}
\label{fig:phi}
\end{figure}
In summary, our designed variance constraint function provides an effective approach to capture the periodic nature of biological systems, especially for molecular systems. This function is continuous and utilizes a variance-based measure to penalize low-variance curves, which helps in improving the prediction accuracy of the neural network for oscillatory models. Additionally, this function is advantageous for the backpropagation process, as its derivatives are also continuous. We will later see from the experiment results how the incorporation of these factors into the model has resulted in significant improvements in prediction accuracy.


% \section{Inverse PINN}
% \subsection{Solution to the parameter estimation problem (PEP)}

% % \begin{figure}[h]
% % \centering
% % \includegraphics[width=0.6\textwidth]{fig/sparse_regression.pdf}
% % \caption{The matrix decomposition in sparse regression}
% % \label{fig:sparse-regression}
% % \end{figure}

% \textbf{Sparse Regression.} 
% In Figure \ref{fig:FINN}, the model-guided Fourier-informed Neural Network (FINN) maps the system spatiotemporal coordinates (${\mathcal{T},\Omega}$) into a latent solution $\widehat{Y}=\{\widehat{y}_1(t),\dots,\widehat{y}_n(t)\}_{t=0}^T$. 
% We calculate the solution derivatives with respect to time and space, obtaining a matrix $\partial \bar{Y}=\{\partial_t \widehat{y}_1,\dots,\partial_t\widehat{y}_n\}_{t=0}^T$. Then we reconstruct the PDE in form of $\partial \Bar{Y}=\Bar{G} \Bar{K} $, as Fig \ref{fig:sparse-regression} shows, where $\Bar{K}$ are unknown sparse coefficients, $\Bar{G}$ is a function list consisting of existing functions from our multiscale model and candidate functions (See next). 
% The inverse problem of model inference can be formulated as the optimization of

% \begin{equation}
%     \widehat{K}=arg \min\left\{\lambda_d(||\widehat{Y}-Y_{data}||_2) + \lambda_m||\partial \Bar{Y}-\Bar{G}\Bar{K}||_2 + \lambda_r||\Bar{K}||_0\right \},
% \end{equation}
% %$\hat{K}=arg \min\{\lambda_d(||\hat{A}_{PET}-A_{PET}||_2+\dots+||\hat{T}_{pCSF}-T_{pCSF}||_2) + \lambda_m||\hat{X}-FK||_2 + \lambda_r||K||_0\}$, 
% which is composed of data loss (controlled by $\lambda_d$), model loss ($\lambda_m$), and a $l_0$ regularization term ($\lambda_r$) to enhance sparsity.


% \subsection{Candidate Functions}

% we construct a format list $\Theta\left(Y\right)$ consisting of candidate nonlinear functions of the columns of $Y$. For example, $\Theta\left(Y\right)$ may consist of constant, polynomial, and trigonometric terms:

% \begin{equation}
% \Theta\left(Y\right)=\left[1, y_{1}, y_{2}, y_{3}, y_{1}^2, \ldots, \sin\left(y_{1}\right), \sin\left(y_{1}^2\right), \ldots\right]
% \end{equation}

% Since in the parameter estimation problem, we assume the data $Y$ with time series is given, we can compute the complete function list matrix $\Bar{G}$ following the format list $\Theta\left(Y\right)$ we design.

% Each column of $\Bar{G}$ represents a candidate function. There is tremendous freedom in choosing the entries in this matrix of nonlinearities. Because we believe that only a few of these nonlinearities plays a role in the equations, we may set up a sparse regression problem to determine the sparse vectors of coefficients $\Bar{K}=\left\{\Bar{k_{1}},\Bar{k_{2}},\ldots,\Bar{k_{D}}\right\}$ that determine
% which nonlinearities are active.