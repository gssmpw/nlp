\section{Related Work}
%Traditional Approaches
The development of mathematical models and numerical methods for solving ordinary and partial differential equations has been a major focus of research for decades. These models and methods are widely used in many scientific fields, such as physics, engineering, and biology, to understand complex systems and make predictions about their behavior. Numerical approaches for solving differential equations, such as Euler's Method \cite{euler1952methodus}, Runge-Kutta Method \cite{runge1895numerische}, the finite element method (FEM) \cite{herrmann1967finite}, finite difference method (FDM) \cite{richardson1911ix} and finite volume method (FVM) \cite{eymard2000finite}, usually involve analytical or numerical techniques that rely on well-established mathematical theories.
These methods have proven to be effective and accurate for many problems, but they often require significant computational resources and can be limited in their ability to handle complex systems. In recent years, machine learning approaches have emerged as a promising alternative to traditional methods for solving differential equations. In this section, we will review both traditional and machine learning approaches for solving differential equations, with a focus on their applications in scientific problems.

% In this section, we will review machine learning approaches for solving differential equations, with a particular focus on physics-informed neural networks (PINN), which has demonstrated remarkable performance in various scientific fields and have emerged as a promising member of the machine learning family.

\subsection{Machine Learning Approaches}

Traditional numerical methods for solving differential equations often involve discretizing the space, which leads to a trade-off between computational efficiency and resolution. Finer grids offer better accuracy but can be computationally expensive, while coarser grids are faster but less accurate. To address these limitations, machine learning approaches such as Neural Ordinary Differential Equations (Neural ODEs) \cite{chen2018neural}, Deep Galerkin Method (DGM) \cite{sirignano2018dgm}, Physics-informed Neural Network (PINN) \cite{raissi2019physics}, and Fourier Neural Operator (FNO) have emerged as promising alternatives.

Neural ODEs represent dynamical systems using continuous-time ordinary differential equations, where the coefficients are neural network functions. They can learn complex dynamics from data even when the underlying system is not well understood. Neural ODEs have been successfully applied to a wide range of problems, including image processing \cite{chen2020mri} and time-series forecasting \cite{chen2022forecasting,gao2022explainable}.

DGM is a machine learning approach that uses deep neural networks to solve partial differential equations (PDEs). It approximates the PDE solution by replacing the finite-dimensional subspace in the Galerkin method with a neural network. DGM has shown promising results in solving various PDEs, including fluid dynamics \cite{li2022deep} and heat transfer problems \cite{zhang2022deep}.

In addition, Physics-informed Neural Network and Fourier Neural Operator are essential in the field of solving differential equations using machine learning. In the following subsections, we will discuss these methods in detail.

% Since numerical differential equation solutions solve equations by discretizing the space, they make a trade-off in resolution: coarse grids are fast but less accurate; fine grids are accurate but slow. 
% To avoid the embarrassing situation, some machine learning solvers involving specified machine learning techniques are applied to solve ODEs and PDEs in recent approaches.

% , such as Neural ODE, Taylor Series, and Fourier Neural Operator.
% NeuralODE, Taylor, Fourier Neural Operator, and others


% \paragraph{Neural Ordinary Differential Equations.}
% Neural Ordinary Differential Equations (Neural ODEs) \cite{chen2018neural} use a neural network to model the instantaneous rate of change in the state of a system. However, despite their apparent suitability for dynamics-governed time-series, Neural ODEs present a few disadvantages. First, they are unable to adapt to incoming data points, a fundamental requirement for real-time applications imposed by the natural direction of time.
% Second, time series are often composed of a sparse set of measurements that could be explained by many possible underlying dynamics. Neural ODEs do not capture this uncertainty. In contrast, Neural Processes (NPs) are a family of models providing uncertainty estimation and fast data adaptation but lack an explicit treatment of the flow of time.

% Neural Ordinary Differential Equations (Neural ODEs) \cite{chen2018neural} are a type of neural network that uses ordinary differential equations (ODEs) to model continuous-time dynamics. Unlike traditional neural networks that use discrete-time updates, Neural ODEs are trained to find a continuous function that describes the evolution of a system over time. This allows them to capture arbitrarily long-term dependencies and provide a more accurate representation of dynamic systems. Neural ODEs have been successfully applied in various fields, such as image recognition, physics, and neuroscience.

% Neural Partial Differential Equations (Neural PDEs) \cite{} use partial differential equations (PDEs) to model the dynamics of a system. Unlike Neural ODEs, which operate on a single dimension, Neural PDEs can model systems in multiple dimensions, making them flexibility for more complex problems in terms of dimensions and applications. Neural PDEs have been used in applications such as fluid dynamics, climate modeling, and materials science. One advantage of Neural PDEs is their ability to handle noise and uncertainty, making them useful for modeling complex systems that involve stochasticity.
% \paragraph{Taylor Series.}
% Taylor series method \cite{corliss1982solving} helps to compute a solution to an initial value problem in ordinary differential equations by expanding each component of the solution in a long Taylor series. The object reference model parameters are embedded as variables, and the power series can be inverted to yield the inverse function. However, though Taylor series methods is very useful for derivations and can be used to get theoretical error bounds, it has some weakness.
% First, Successive terms get very complex and hard to derive. As a result, in practical we avoid using higher-order Taylor expansions.
% Second, the truncation error tends to grow rapidly away from the expansion point, and the results are always not as efficient as curve fitting or direct approximation.

% To address these problems, we introduce Neural ODE Processes (NDPs), a new class of stochastic processes determined by a distribution over Neural ODEs. By maintaining an adaptive data-dependent distribution over the underlying ODE, we show that our model can successfully capture the dynamics of low-dimensional systems from just a few data points. At the same time, we demonstrate that NDPs scale up to challenging high-dimensional time-series with unknown latent dynamics such as rotating MNIST digits.

% \paragraph{Fourier Neural Operator.}
% Silvescu et al.(1999) \cite{silvescu1999fourier} proposed the Fourier Neural Networks using the fast Fourier transform (FFT), and Li et al.(2020) \cite{li2020fourier} developed this technique and proposed the Fourier neural operator (FNO), which provides a data-driven method for solving differential equations. FNO uses neural networks to learn gridless, infinite-dimensional operators. Neural operators correct the grid-dependent properties of the finite-dimensional operator approach described above by producing a set of network operating parameters that can be used for different discretizations. It has the ability to transfer solutions between meshes. Compared with the former, FNO has more advantages in terms of success and accuracy, but the premise is that there is a sufficient data set, which is precisely the most difficult to achieve.


\subsection{Physics-informed Neural Network}
% \begin{figure}[h]
% \centering
% \includegraphics[width=1\textwidth]{fig/PINN.png}
% \caption{Schematic of a physics-informed neural network (PINN). A fully-connected neural network, with time and space coordinates $\left(t, x\right)$ as inputs, is used to approximate the multi-physics solutions $\hat{u} = \left[u, v, p, \phi \right]$. The derivatives of $\hat{u}$ with respect to the inputs are calculated using automatic differentiation (AD) and then used to formulate the residuals of the governing equations in the loss function, that is generally composed of multiple terms weighted by different coefficients. The parameters of the neural network $\theta$ and the unknown PDE parameters $\lambda$ can be learned simultaneously by minimizing the loss function}
% \label{fig:pinn}
% \end{figure}


% Likewise, other Authors have provided alternative paths in data-driven scientific computing fields. For instance, Arka et al. \cite{daw2017physics} employs a framework that uses physical properties as soft constraints on the loss function. It utilizes this new loss function to model the relationship between lake temperature and other physics variables. The authors demonstrate that this hybrid model, PGNN, has better generalizability and robustness than other typical deep learning approaches. It is indeed quite common to modify the loss function in order to take scientific constraints into account. Other papers also followed this doctrine, although they have objectives. There are attempts to change or replace an existing physical model by using physics-guided loss functions.\cite{alvarez2009latent,doan2019physics,fioretto2020predicting}. Sometimes, people do not have physical models ready, and can instead use mathematical approximations. This goal can also be accomplished with the help of physics-guided loss functions.\cite{beucler2021enforcing,beucler2019achieving,yazdani2020systems}. Another popular application is to solve PDE systems, since the loss functions do take the derivatives into account. This makes them well-suited for solving PDE systems. \cite{baseman2018physics,dwivedi2020solution,karumuri2020simulator}


% \subsection{Inverse Sparse Identification of Nonlinear Dynamics (SINDy)}
% PINN 
% Recently, the incorporation of machine learning techniques has gained increasing attention as a promising approach to accelerate modeling and simulation in physical, chemical, and biological application scenarios~\cite{snyders2001inductive,mansour2019deep,mitchell2017spatial,feinman2018learning}.
The Physics-informed Neural Network (PINN), a machine learning technique proposed for solving differential equations, has gained much popularity for its ability to achieve promising results for various problems in computational science and engineering~\cite{cai2022physics,misyris2020physics,cai2021physics,ji2021stiff,zanardi2022towards,lu2022deep,lv2021novel,sun2020surrogate,raissi2019deep,jin2021nsfnets,kissas2020machine,sahli2020physics}.

The primary objective of PINN is to utilize nonlinear partial differential equations to guide the supervised learning process. The model employs a deep learning network that can accurately identify nonlinear mappings from high-dimensional input and output data. The physical laws of the system act as a regulatory term, constraining the model to adhere to the governing equations. The authors successfully overcome the challenge of inadequate labeled data by using a large number of collocation points while obtaining favorable results in fluid dynamics, quantum mechanics, and reaction-diffusion problems. PINN is a meshless approach that transforms the initial and boundary value problems into optimization problems to solve differential equations.
That is, provided with explicit differential equations and initial and boundary conditions, PINN can solve a system without requiring labeled training data (ground truth solutions).
In the field of systems biology, two methods have been proposed as a domain translation of the PINN technique, such as 
biologically-informed neural network~\cite{lagergren2020biologically,greene2020biologically} and systems biology-informed deep learning~\cite{yazdani2020systems}.
Several real biological applications have taken advantage of PINN for model design and analysis, including soft biological tissue model~\cite{liu2020generic}, cardiac activation mapping~\cite{sahli2020physics}, and thrombus material properties~\cite{yin2021non}.

% While PINN has gained promising results across a wide range of problems in computational science and engineering \cite{sun2020surrogate,raissi2019deep,jin2021nsfnets,kissas2020machine,sahli2020physics}, it fails to achieve reliable training and correct solutions for complex systems and are prone to high computational costs \cite{krishnapriyan2021characterizing,lu2021learning}.

Although PINN has gained promising results across a wide range of problems in computational science and engineering \cite{sun2020surrogate,raissi2019deep,jin2021nsfnets,kissas2020machine,sahli2020physics}, there are two major deficiencies that hinder its broad application in biological and physical sciences: (i) it sometimes fails to achieve reliable training and correct solutions for complex systems~\cite{krishnapriyan2021characterizing, lu2021deepxde, meng2022physics}, and (ii) it is prone to high computational costs~\cite{anantharaman2021stably,jagtap2020locally,jagtap2020conservative,lu2021learning}.
To address these problems, researchers have been working on developing and optimizing the PINN algorithm.
A series of domain decomposition-based PINNs (CPINN, DPINN, XPINN) are presented to divide the computational domain into smaller subdomains~\cite{dwivedi2019distributed, jagtap2020conservative, jagtap2021extended}, thereby decomposing complex problems into smaller subproblems that can be solved by minimal size local PINN to solve.
The gradient-enhanced PINN (GPINN)~\cite{yu2022gradient} performs gradient enhancement on PINN by forcing the derivative of PDEs residual to be zero, which aids in faster convergence and improved accuracy of the model.
For small sampling data, few-shot learning is added to PINN, in which a neural network is used to train an approximate solution first and further optimized by minimizing the designed cost function~\cite{li2021deep}. 
% For linear problems, the Bayesian physical extreme machine learning is employed to enhance PINN, which increases the calculation speed by several orders of magnitude~\cite{liu2022bayesian}.
For stiff problems, Wang et al.~\cite{raissi2019physics} proposed a new network structure to deal with the numerical stiffness of the backpropagation gradient that causes PINN to fail.
Other methods focus on tailoring PINN architecture for a specific physical problem~\cite{wu2021modified, baddoo2021physics} and are difficult to generalize to systems biology applications.
% However, these techniques currently are difficult to  scale to the very large mo dels being used in practice by systems biologists. % copied
More research is needed to scale the generalizability and stability for these techniques to be applicable across the continuum of biological models.

% Fourier 
\subsection{Fourier Neural Operator}
Inspired by the Fourier neural networks (FNN) that apply the fast Fourier transform to the neural network~\cite{silvescu1999fourier}, a Fourier neural operator (FNO, a data-driven approach) is developed to learn a mapping of solutions to a complete family of differential equations~\cite{li2020fourier}. 
The Fourier transform reduces the computational complexity to quasi-linear, enabling large-scale calculations. 
Compared to traditional PDE solvers, FNO's resolution invariance property offers high accuracy at a low computational cost, making it a state-of-the-art approach for approximating PDEs.
Grady et al.\cite{grady2022towards} later break the limitation that FNO can only solve problems below three dimensions and mathematically proved that the proposed model-parallel Fourier neural operators (MP-FNO) can be used for dimensional data of any size. 
% And mathematically deduce all the necessary components of MP-FNO. MP-FNO is solving large-scale Parametric PDEs have a significant speed-up effect.
%Factorized Fourier Neural Operator (F-FNO)~\cite{tran2021factorized} has made a number of improvements on the basis of FNO to achieve better results. Factorize the input multi-dimensional Fourier transform and share the kernel integral operator parameters of all Fourier layers, which greatly reduces the amount of parameters. The residual structure added by F-FNO can greatly deepen the Fourier operator layers.
From the application perspective, FNO is customized to solve different problems, such as LAFNO~\cite{peng2022linear} for modeling 3D turbulence, PFNO~\cite{li2022solving} for solving seismic wavefield equations in velocity models, and IFNO~\cite{you2022learning} for predicting the mechanical response of materials.
In addition to learning challenging PDEs, FNO also achieved good results in the field of Vision transformers. The adaptive Fourier neural operator (AFNO)~\cite{guibas2021adaptive} draws on the idea of FNO and performs token mixing in the frequency domain through Fourier transform to understand the relationships among objects in a scene.
However, the above FNO-related methods are data-driven approaches that require ground truth solutions as labeled data to train the neural networks.