\documentclass{article}

\PassOptionsToPackage{noend}{algorithmic} %
\newcommand{\Comment}[1]{\hfill \(\triangleright\) #1}

\input{math_commands.tex}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} %

\PassOptionsToPackage{hyphens}{url}
\usepackage[breaklinks=true]{hyperref}



\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}


\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}

\icmltitlerunning{The Minimal Search Space for Conditional Causal Bandits}



\usepackage{thmtools, thm-restate}

\usepackage{enumitem} %
\usepackage{nameref}
\usepackage{subcaption} %
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{stmaryrd} %
\usetikzlibrary{fit}
\usepackage{cancel}
\usepackage{nicefrac}

\usepackage{soul} %
\soulregister\cite7 %
\soulregister\Citet7
\soulregister\Citep7
\soulregister\citet7
\soulregister\citep7
\soulregister\ref7
\soulregister\cref7
\soulregister\Cref7
\soulregister\addref7
\newcommand{\hlnew}[1]{\sethlcolor{lightgray}\hl{#1}}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\tocheck}{\marginpar{\textcolor{purple}{[Needs Checking]}}}
\newcommand{\checkedbyme}{\marginpar{\textcolor{green!60!black}{[fsimoes $\checkmark$]}}}
\newcommand{\checkedbythijs}{\marginpar{\textcolor{green!60!black}{[thijs $\checkmark$]}}}

\newcommand{\TODO}{\todo[inline]{TODO}}

\newcommand{\addref}{\textcolor{red}{[ADD REF]}}

\frenchspacing %
\usepackage[bottom]{footmisc} %
\usepackage{xargs} %

\DeclareMathOperator{\cC}{\mathfrak{C}} \newcommand{\ind}{\perp\!\!\!\perp}
\DeclareMathOperator{\C}{\mathfrak{C}}
\newcommand{\condset}{\textbf{Z}}
\newcommand{\nind}{\not\!\ind}
\newcommand{\dop}{\mathit{do}}
\newcommand{\defeq}{\overset{\mathrm{def}}{=}}
\newcommand{\PA}{\mathrm{Pa}}
\newcommand{\Pa}{\mathrm{Pa}}
\newcommand{\pa}{\mathrm{pa}}
\newcommand{\Ch}{\mathrm{Ch}}
\newcommand{\ch}{\mathrm{ch}}
\newcommand{\An}{\mathrm{An}}
\newcommand{\an}{\mathrm{an}}
\newcommand{\De}{\mathrm{De}}
\newcommand{\de}{\mathrm{de}}
\newcommand{\abs}[1]{\lvert #1 \rvert} \newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\veryshortrightarrow}{\!\!\shortrightarrow\!\!}
\newcommand{\ACE}[2]{\mathrm{ACE}( #1 \!\rightarrow \!#2)}
\newcommand{\ccolon}{\vcentcolon=}
\newcommand{\rdots}[1]{\rotatebox[origin=c]{#1}{\ldots}}
\newcommand{\LSCA}{\mathrm{LSCA}}
\newcommand{\LCA}{\mathrm{LCA}}
\newcommand{\CA}{\mathrm{CA}}
\newcommand{\SCA}{\mathrm{SCA}}
\newcommand{\Linfty}{\mathcal{L}^{\infty}}
\newcommand{\LinftyY}{\mathcal{L}^{\infty}(\PA(Y))}
\newcommand{\mGISS}{\mathrm{mGISS}}
\newcommand{\prev}{\mathsf{pr}}
\newcommand{\suchthat}{\ \text{ s.t. }\ }
\newcommand{\overeq}[1]{\overset{\text{\tiny #1}}{=}}
\newcommand{\ts}[1]{\textsuperscript{#1}}
\renewcommand{\th}{\ts{th}}
\newcommand{\partialqed}[1]{
  \vspace{-0.4cm}
  \begin{flushright} $\square_{#1}$ \end{flushright}
}
\newcommand{\dashedline}{\mathrel{\raisebox{0.5ex}{\rule{3pt}{0.5pt}\,\rule{3pt}{0.5pt}\,\rule{3pt}{0.5pt}}}}

\newcommand{\heavyside}{\mathbf{1}_{>0}}
\newcommand{\anpo}{\preccurlyeq}
\newcommand{\detsup}{\succeq^{\mathrm{det},a}}
\newcommand{\intinf}{\preceq^a}
\newcommand{\condsup}{{\succeq}^{c}}
\newcommand{\intsup}{\succeq}
\newcommand{\condinf}{\preceq^{c, \E}}
\newcommand{\uf}{\bar{f}}


\renewcommand{\nind}{\not\!\ind}
\renewcommand{\defeq}{\overset{\mathrm{def}}{=}}
\renewcommand{\pa}{\mathrm{pa}}
\newcommand{\supp}{\mathrm{supp}}

\definecolor{gold}{rgb}{1.0, 0.84, 0.0}



\newcommandx{\warning}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\note}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\question}[2][1=]{\todo[linecolor=purple,backgroundcolor=purple!25,bordercolor=purple,#1]{#2}}
\newcommandx{\original}[2][1=]{\todo[linecolor=green,backgroundcolor=green!25,bordercolor=green,#1]{OG!:
    #2}}
\newcommandx{\optional}[2][1=]{\todo[linecolor=yellow,backgroundcolor=yellow!25,bordercolor=yellow,#1]{#2}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{notation}[theorem]{Notation}
\newtheorem*{notation*}{Notation}
\newtheorem{example}[theorem]{Example}





\begin{document}
\twocolumn[
\icmltitle{The Minimal Search Space for Conditional Causal Bandits}
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Francisco Simoes}{uu}
\icmlauthor{Itai Feigenbaum}{ny}
\icmlauthor{Mehdi Dastani}{uu}
\icmlauthor{Thijs van Omen}{uu}
\end{icmlauthorlist}

\icmlaffiliation{uu}{Department of Information and Computing Sciences, Utrecht University, Netherlands}
\icmlaffiliation{ny}{Department of Computer Science, Lehman College and the Graduate Center, City University of New York, USA}

\icmlcorrespondingauthor{Francisco Simoes}{f.simoes@uu.nl}

\icmlkeywords{causal bandits, causal inference, causality, conditional interventions, multi-armed bandits}

\vskip 0.3in
]

\printAffiliationsAndNotice{}  %

\begin{abstract}
  Causal knowledge can be used to support decision-making problems.
  This has been recognized in the causal bandits literature, where a causal (multi-armed) bandit is characterized by a causal graphical model and a target variable.
  The arms are then interventions on the causal model, and rewards are samples of the target variable.
  Causal bandits were originally studied with a focus on hard interventions.
  We focus instead on cases where the arms are \emph{conditional interventions}, which more accurately model many real-world decision-making problems by allowing the value of the intervened variable to be chosen based on the observed values of other variables.
  This paper presents a graphical characterization of the minimal set of nodes guaranteed to contain the optimal conditional intervention, which maximizes the expected reward.
  We then propose an efficient algorithm with a time complexity of $O(|V| + |E|)$ to identify this minimal set of nodes.
  We prove that the graphical characterization and the proposed algorithm are  correct.
  Finally, we empirically demonstrate that our algorithm significantly prunes the search space and substantially accelerates convergence rates when integrated into standard multi-armed bandit algorithms.
\end{abstract}


\section{Introduction}
\label{sec:introduction}


\Citet{lattimore2016bandits} introduce a class of problems termed \emph{causal bandit} problems, where actions are interventions on a causal model, and rewards are samples of a chosen reward variable $Y$ belonging to the causal model.
They focus on hard interventions, where the intervened variables are set to specific values, without considering the values of any other variables.
We will refer to this as a hard-intervention causal bandit problem.
They propose a best-arm identification algorithm that utilizes observations of the non-intervened variables in the causal model to accelerate learning of the best arm as compared to standard multi-armed bandit (MAB) algorithms.
Causal bandits have applications across a broad range of domains, particularly in scenarios requiring the selection of an intervention on a causal system.
These include computational advertising and context recommendation \citep{bottou2013counterfactual,zhao2022mitigating}, biochemical and gene interaction networks \citep{meinshausen2016methods,basharin1959statistical}, epidemiology \citep{joffe2012causal}, and drug discovery \citep{michoel2023causal}.

Most of the work in causal bandits (see \Cref{sec:related-work}) focuses on developing MAB algorithms which incorporate knowledge about the causal graph.

\Citet{lee2018structural}, in contrast, use the fact that the causal graph is known not to develop yet another MAB algorithm, but to reduce  the set of nodes (\emph{i.e.} variables) of the causal graph on which hard interventions should be examined, thereby reducing the search space for hard-intervention causal bandit problems.
In more detail, they define the SCM-MAB problem, where the agent has access to the causal graph $G = (\mathbf{V}, E)$ of a structural causal model (SCM) and wants to maximize a target variable $Y \in \mathbf{V}$ by playing arms which are \emph{hard} interventions on \emph{subsets of $\mathbf{V}$}.
Their search space reduction algorithm identifies the set of all (minimal) subsets $\mathbf{X}$ of $\mathbf{V}$ such that there exists an SCM with graph $G$ for which some hard, multi-node intervention $\dop(\mathbf{X} = \mathbf{x})$ has maximal $\E_{Y \sim p^{\dop(\mathbf{X} = \mathbf{x})}_{Y}}[Y]$.
\Citet{lee2019structural} and \citet{lee2020characterizing} extend the approach of \citet{lee2018structural} to the cases involving non-manipulable variables and mixed policies, respectively (see \Cref{sec:related-work}).


It is recognized in the MAB literature that, for many if not most applications, actions are taken in a context, that is, with available information \citep{lattimore2020bandit,agarwal2014taming,dudik2011efficient,jagerman2020safe,langford2007epoch}.
\emph{E.g.}, content recommendation based on the user's demographic characteristics, such as age, gender, nationality and occupation.
Similarly, in causality, conditional interventions --- where a variable $X$ is set to a value $g(\mathbf{Z})$ through some policy $g$ after observing other variables (a context) $\mathbf{Z}$ --- are more realistic than hard or soft\footnotemark interventions in many real-world scenarios.
\footnotetext{In a soft intervention, the intervened variable keeps its direct causes \citep{peters2017elements}.}
Conditional interventions were first introduced in \citet{pearl1994probabilistic} based on the argument that ``In general, interventions may involve complex policies in which a variable $X$ is made to respond in a specified way to some set $\mathbf{Z}$ of other variables.''
\Citet{shpitser2012identification} motivate their interest in conditional interventions by providing the concrete example of a doctor selecting treatments based on observed symptoms and medical test results $\mathbf{Z}$ to improve the patient's health condition.
The doctor performs interventions of the form $\dop(X_{i}=x_{i})$, but ``the specific values of the treatment variables are not known in advance, but instead depend on symptoms and test results performed `on the fly' via policy functions $g_{i}$''.
Although the practical motivation for conditional interventions is clear, causal bandits with conditional interventions have not yet been studied, possibly due to the complexity of these interventions compared to the mathematically simpler hard and soft interventions.

\textbf{Novelty and Contributions: }
This work, like that of \citet{lee2018structural}, uses the causal graph to reduce the search space of an MAB problem.
Our work is novel because we consider the case where (i) the arms are \emph{conditional interventions} (which generalize both hard and soft interventions); and (ii) the interventions are \emph{single-node interventions}.
This is the first time the minimal search space for a causal bandits problem with non-hard interventions is fully characterized.
Such a characterization has also not been done for single-node interventions (of any kind).
Our contributions are as follows:
(a) we establish a graphical characterization of the minimal set of nodes guaranteed to contain the optimal node on which to perform a conditional intervention;
and (b) we propose an algorithm which finds this set, given only the causal graph, with a time complexity of $O(\abs{V} + \abs{E})$.
As a supplementary result, we also show that, perhaps surprisingly, the exact same minimal set would hold for the optimization problem of selecting an atomic (\emph{i.e.} single-node and hard) intervention in a deterministic causal model.
We provide proofs for the graphical characterization and correctness of the algorithm, as well as experiments that assess the fraction of the search space that can be expected to be pruned using our method, in both randomly generated and real-world graphs, and demonstrate, using well-known real-world models, that our intervention selection can significantly improve a classical MAB algorithm.

All proofs of the results presented in the paper can be found in the appendix.
The code repository containing the experiments can be found in \url{https://github.com/francisco-simoes/minimal-set-conditional-intervention-bandits}.


\section{Preliminaries}
\label{sec:prelims}
\paragraph{Graphs and Causal Models}
We will make use of Directed Acyclic Graphs (DAGs).
The main concepts of DAGs and notation used in this paper are reviewed in \Cref{sec:app-dags}.
Furthermore, we operate within the Pearlian graphical framework of causality, where causal systems are modeled using Structural Causal Models (SCMs) \citep{peters2017elements,pearl2009causality}.
An \emph{SCM} $\cC$ is a tuple $(\mathbf{V},\mathbf{N},\mathcal{F},p_{\mathbf{N}})$, where $\mathbf{V}=(V_{1}, \ldots, V_{n})$ and $\mathbf{N} = (N_{V_{1}}, \ldots, N_{V_{n}})$ are vectors of random variables.
The exogenous variables are pairwise independent, and are distributed according to the \emph{noise distribution} $p_{\mathbf{N}}$, while each endogenous variable $V_{i}$ is a deterministic function $f_{V_{i}}$ of its noise variable $N_{V_{i}}$ and a (possibly empty) set of other endogenous variables $\PA(V_{i})\setminus \{V_{i}\}$, called the (proper) parents of $V_{i}$.
The $V_{i}$ and $N_{V_{i}}$ are called \emph{endogenous} and \emph{exogenous} (or \emph{noise}) variables, respectively.
$R_{V}$ denotes the range of the random variable $V$.
$\mathcal{F}$ is a set of functions $f_{V_{i}}\colon R_{\PA(V_{i})}\times R_{N_{V_{i}}} \to R_{V_{i}}$, termed \emph{structural assignments}.
The endogenous variables together with $\mathcal{F}$ characterize a DAG called the \emph{causal graph} $G^{\cC}\vcentcolon=(\mathbf{V}, E)$ of $\cC$, whose edge set is $E = \{(P, X) : X \in \mathbf{V},\  P\in \PA(X)\setminus \{X\}\}$.
We denote by $\cC(G)$ the set of SCMs whose causal graph is $G$.
Having an SCM allows us to model interventions: intervening on a variable changes its structural assignment $f_{X}$ to a new one, say $\tilde{f}_{X}$.
This intervention is then denoted $\dop(f_{X} = \tilde{f}_{X})$.
In the simplest type of interventions, called \emph{atomic interventions}, a variable $X$ is set to a chosen value $x$, thus replacing the structural assignment $f_{X}$ of $X$ with a constant function setting it to $x$.
Such an intervention is denoted $\dop(X=x)$, and the SCM resulting from performing this intervention is denoted $\cC^{\dop(X=x)}$.
The joint distribution over the endogenous variables resulting from the atomic intervention $\dop(X=x)$ is denoted $p^{\dop(X=x)}$ and called the \emph{post-intervention distribution} for this intervention.
Each realization $\mathbf{n}\in R_{\mathbf{N}}$ of the noise variables will be called a \emph{unit}.
A \emph{deterministic SCM} is an SCM for which the noise distribution is a point mass distribution with all its mass on some (known) unit $\mathbf{n} \in R_{\mathbf{N}}$.
Finally, nodes are denoted by upper case letters, sets of nodes by boldface letters, and variable values by lower case letters.

\paragraph{Unrolled Assignments}
We will make use of the fact that the structural assignments of the ancestors of an endogenous variable $X$ (including its own structural assignment) can be composed to express $X$ as a function $\uf_{X}(\mathbf{n})$ of the vector $\mathbf{n}$ of exogenous variables values.
We call this the \emph{unrolled assignment} of $X$.
The formal definition can be found in \Cref{sec:app-unrolled_assigns}.

\paragraph{Conditional Interventions}
Given an SCM $\cC = (\mathbf{V},\mathbf{N},\mathcal{F},p_{\mathbf{N}})$ with causal graph $G$, $X\in \mathbf{V}$ and $\mathbf{Z}_{X} \subseteq \mathbf{V}\setminus \{X\}$, the \emph{conditional intervention on $X$ given $\mathbf{Z}_{X}$ for the policy $g\colon R_{\mathbf{Z}_{X}} \to R_{X}$}, denoted $\dop(X = g(\mathbf{Z}_{X}))$, is the intervention where the value of $X$ is determined by that of $\mathbf{Z}_{X}$ through $g$ \citep{pearl2009causality}.
The exact conditioning set $\mathbf{Z}_{X}$ for each $X$ will depend on the specific application.
In order to study conditional interventions, we will need to make some assumptions of what nodes can reasonably be in $\mathbf{Z}_{X}$, \emph{i.e.} what variables can we expect to have knowledge of at the time of applying the policy $g$ to intervene on $X$.
As noted in \citet{pearl1994probabilistic,pearl2009causality}, the nodes in $\mathbf{Z}_{X}$ cannot be descendants of $X$ in $G$.
Hence, $\mathbf{Z}_{X} \subseteq \mathbf{V}\setminus \De(X)$.
On the other hand, all (proper) ancestors of $X$ are realized before $X$.
Since we will be dealing with the case with no latent variables, we can assume that all ancestors of $X$ are observed, and can be used by a policy $g$ to set $X$ to a value $g(\mathbf{Z}_{X})$.
Thus, we assume\footnotemark that $\An(X) \setminus \{X\} \subseteq \mathbf{Z}_{X}$.
\footnotetext{
    Note that we are not claiming that all variables in $\An(X)\setminus \{X\}$ \emph{need} to be in $\mathbf{Z}_{X}$ for the best decision to be made, or for our results to hold, but that we \emph{can} always include them in $\mathbf{Z}_{X}$ under the assumptions of our problem.
}
We will then focus on the case where the conditioning set $\mathbf{Z}_{X}$ is what we call a \emph{observable conditioning set for $X$}, written $\condset_{X}$, simply meaning that $\An(X)\setminus \{X\} \subseteq \condset_{X} \subseteq \mathbf{V} \setminus \De(X)$.
Finally, we call a map $g\colon R_{\condset_{X}} \to R_{X}$ a \emph{policy for $X$}.

\paragraph{Conditional Causal Bandits}
Recall that a MAB problem consists of an agent pulling an arm $a\in \mathcal{A}$ at each round $t$, resulting in a reward sample $Y_{t}$ from an unknown distribution associated to the pulled arm \citep{lattimore2020bandit}.
We denote the mean reward for arm $a$ by $\mu_{a}$ and the mean reward for the best arm by $\mu^{*} = \max_{a\in \mathcal{A}} \mu_{a}$.
The objective can be to maximize the total reward obtained over all the $T$ rounds, or to identify the arm with the highest expected reward (best-arm identification).
We can minimize the cumulative regret $\mathrm{Reg_{T}} =  T \mu^{*} - \sum_{t=1}^{T} \E[Y_{t}]$ for the former objective, or maximize the probability of selecting the best arm at round $T$ for the latter.

We now introduce a novel type of (causal) MAB problem.
Consider the setting where the bandit'S reward is a (endogenous) variable $Y$ in an SCM $\cC = (\mathbf{V}, \mathbf{N}, \mathcal{F}, p_{\mathbf{N}})$, and the arms are the conditional interventions $\dop(X=g(\condset_{X}))$, where $X \in \mathbf{V} \setminus \{Y\}$.
Furthermore, the agent has knowledge of the causal graph $G$ of $\cC$, but not of the structural assignments $\mathcal{F}$ or the noise distribution $p_{\mathbf{N}}$.
We call this a \emph{single-node conditional-intervention causal bandit}, or simply \emph{conditional causal bandit}.
The reward distribution for arm $\dop(X=g(\condset_{X}))$ is the post-intervention distribution $p_{Y}^{\dop(X=g(\condset_{X}))}$, and is unknown to the agent, since it has no knowledge of $\mathcal{F}$.


Notice that selecting an arm can be subdivided in (i) choosing a node $X$ to intervene on; and (ii) choosing a value to set $X$ to, given the observed variables $\condset_{X}$.
In this paper, we find the minimal set of nodes that need to be considered by the agent in step (i).
Note that, since we assume the conditioning sets $\condset_{X}$ to be predetermined by the intervener and the problem context, no choices need to be made in step (ii) regarding which nodes to condition on.

As stressed in \Cref{sec:introduction}, the novelty of our problem lies in the fact that we deal with \emph{conditional interventions} that are \emph{single-node}.
Both of these characteristics of our problem complicate the analysis.
Unsurprisingly, searching over conditional interventions is more complicated than over hard or soft interventions.
Perhaps more unexpectedly, single-node interventions also make a search for a minimal search space more involved.
Indeed, if one allows for interventions on arbitrary sets, one simply needs to intervene on all the parents $\Pa(Y)$ of $Y$.
This problem becomes more interesting when unobserved confounding of $Y$ is allowed, in which case simply intervening on $\Pa(Y)$ may not be the optimal approach \citep{lee2018structural}.
Since in our case the agent cannot simply intervene on all the parents of $Y$, the case without unobserved confounding is, as we will see, already complex enough.
Extending the results in this paper to cases with unobserved confounding is left as future work (\Cref{sec:conclusion}).

\section{Conditional-Intervention Superiority} %
\label{sec:cond-sup}

In this section, we will define a preorder $\condsup_{Y}$ of ``conditional-intervention superiority'' on nodes of an SCM.
If $X \condsup_{Y} W$, then $W$ can never be a better node than $X$ to intervene on with a conditional intervention\footnotemark.
\footnotetext{The relation between nodes introduced by \citet{lee2018structural} is similar, but pertains to multi-node hard interventions.}
We will then show that, perhaps surprisingly, this relation is equivalent to another superiority relation, defined in terms of atomic interventions in a deterministic SCM.

\begin{definition}[Conditional-Intervention Superiority]
  \label{def:cond-int-sup}
  $X$ is \emph{conditional-intervention superior to $W$ relative to $Y$} in $G$, denoted $X \condsup_{Y} W$, if for all SCM with causal graph $G$ there is a policy $g$ for $X$ such that for all policies $h$ for $W$,
  \begin{equation}
    \label{eq:cond-int-sup}
    \E_{\mathbf{n}} \bar{f}_{Y}^{\dop(X=g(\condset_{X}))}(\mathbf{n}) \ge \E_{\mathbf{n}} \bar{f}_{Y}^{\dop(W=h(\condset_{W}))}(\mathbf{n}),
  \end{equation}
  where $\condset_{V}$ is a observable conditioning set for $V$.
  Equivalently, for all $\cC(G)$ one has:
  \begin{equation}
    \begin{split}
      \max_{g} \E_{\mathbf{n}} \bar{f}_{Y}^{\dop(X=g(\condset_{X}))}(\mathbf{n}) \ge \max_{h} \E_{\mathbf{n}} \bar{f}_{Y}^{\dop(W=h(\condset_{W}))}(\mathbf{n}).
    \end{split}
  \end{equation}
\end{definition}

A similar relation can be defined for atomic interventions in deterministic SCMs, where the vector $\mathbf{N}$ of exogenous variables is fixed to a \emph{known} value $\mathbf{n}$ (see \Cref{sec:prelims}).

\begin{definition}[Deterministic Atomic Intervention Superiority] %
  \label{def:det-sup}
  Let $X, W, Y$ be nodes of a DAG $G$.
  $X$ is \emph{deterministically atomic-intervention superior} to $W$ relative to $Y$, denoted $X \detsup_{Y} W$, if for every SCM $\C$ with causal graph $G$ and every unit $\mathbf{n}$ there is $X\in R_{X}$ such that no atomic intervention on $W$ results in a larger $Y$ than the value of $Y$ resulting from setting $X=X$.
  That is, for all $(\cC, \mathbf{n}) \in \cC(G) \times R_{\mathbf{N}}$:
  \begin{equation}
    \label{eq:detsup}
      \exists X \in R_X\colon \forall w \in R_{w},\
      \bar{f}_{Y}^{\dop(X=X)}(\mathbf{n}) \geq \bar{f}_{Y}^{\dop(W=w)}(\mathbf{n}).
  \end{equation}
  Equivalently:
  \begin{equation}
     \max_{X\in R_{X}}\bar{f}_{Y}^{\dop(X=X)}(\mathbf{n}) \geq \max_{w\in R_{W}}\bar{f}_{Y}^{\dop(W=w)}(\mathbf{n}).
  \end{equation}
\end{definition}

We extend \Cref{def:cond-int-sup,def:det-sup} for sets of nodes in the obvious way: $\mathbf{X}$ is superior to $\mathbf{W}$ if
every node in $\mathbf{W}$ is inferior to some node in \textbf{X}.

\begin{definition}
Let now $\mathbf{X}, \mathbf{W}$ be sets of nodes of $G$.
  $\mathbf{X}$ is \emph{conditional-intervention superior} (respectively \emph{deterministic atomic intervention superior}) to $\mathbf{W}$, also denoted $\mathbf{X} \condsup_{Y} \mathbf{W}$ (respectively $\mathbf{X} \detsup_{Y} \mathbf{W}$), if $\forall W \in \mathbf{W}, \exists X\in \mathbf{X}$ such that $X \condsup_{Y} W$ (respectively $X \detsup_{Y} W$).
\end{definition}

The two relations $\condsup_{Y}$, $\detsup_{Y}$ actually coincide (both for nodes and sets of nodes).

\begin{restatable}[Conditional vs Atomic superiority]{proposition}{condvsatomic}
  \label{prop:cond-vs-atomic}
  Let $X$, $W$, $Y$ be nodes in a DAG $G$.
  Then $X$ is average conditional-interventionally superior to $W$ relative to $Y$ in $G$ if and only if $X$ is atomic-interventionally superior to $W$ relative to $Y$ in $G$.
  That is:
  \begin{equation}
   X \condsup_{Y} W \iff X \detsup_{Y} W.
  \end{equation}
\end{restatable}

Since these two relations are equivalent, we henceforth refer simply to interventional superiority without further specification, and use the symbol $\intsup_{Y}$ when distinguishing them is not necessary.

\begin{remark}
  It is straightfoward to show that both interventional superiority relations are in fact preorders (see \Cref{sec:app-preorders}).
\end{remark}

\Cref{prop:cond-vs-atomic} will simplify our problem.
Since deterministic atomic interventions are often easier to reason about, we will use them both in formulating proposals for the minimal search space and in our formal proofs.

\begin{remark}
  One may wonder if $\condsup_{Y}$ is also equivalent to the superiority relation for atomic interventions in \emph{non-deterministic} (general) SCMs defined in the natural way: $X \intsup^{a}_{Y} W$ iff $\max_{X} \E_{\mathbf{n}} \bar{f}_{Y}^{\dop(X=X)}(\mathbf{n}) \ge \max_{w} \E_{\mathbf{n}} \bar{f}_{Y}^{\dop(W=w)}(\mathbf{n})$.
  In fact, it is not (see \Cref{example:condsup-not-eq-to-detsup} in \Cref{sec:app-superiorities}).
\end{remark}

\section{Graphical Characterization of the Minimal Globally Interventionally Superior Set}
\label{sec:graph-char-minim}

\paragraph{Goal}
Our aim is to develop a method to identify, based on a causal graph $G$, the smallest set of nodes that are ``worth testing'' when attempting to maximize $Y$ by performing one atomic intervention.
Specifically, regardless of the structural causal model $\C$ associated with $G$, we want to ensure that the optimal intervention can be discovered within this selected set of nodes.
We define this set as follows:
\begin{definition}[GISS and mGISS]
  Let $G$ be a DAG with set of nodes $\mathbf{V}$.
  A \emph{globally interventionally superior set (GISS)} of $G$ relative to $Y$, is a subset $\mathbf{U}$ of $\mathbf{V}\setminus \{Y\}$ satisfying $\mathbf{U} \intsup_{Y} (\mathbf{V} \setminus\{Y\}) \setminus \mathbf{U}$.
  A \emph{minimal globally interventionally superior set (mGISS)} is a GISS which is minimal with respect to set inclusion.
\end{definition}

This set is unique, so that we can talk of \emph{the} minimal globally interventionally superior set.

\begin{restatable}[Uniqueness of the mGISS]{proposition}{uniquenessmgiss}
  Let $G$ be a DAG and $Y$ a node of $G$.
  The minimal globally interventionally superior set of $G$ relative to $Y$ is unique.
  We denote it by $\mGISS_{Y}(G)$
\end{restatable}


\paragraph{Intuition}
Since the value of \( Y \) is completely determined by the values of its parents \( A_{1}, \ldots, A_{m} \), along with the fixed value \( n_{Y} \) of a noise variable that cannot be intervened upon (see \Cref{def:det-sup}), we aim to induce the parents to acquire the combination of values \( (a^{*}_{1}, \ldots, a^{*}_{m}) \) that maximizes \( Y \) when \( N_{Y} = n_{Y} \).
If this is not possible to achieve using a single intervention, we aim to obtain the best combination possible.
Clearly, the parents of $Y$ themselves need to tested by bandit algorithms: there may be one parent on which $Y$ is highly dependent, in such a way that there is a value of that parent which will maximize $Y$.
In the particular case where $Y$ has a single parent $A$, that node is the only node worth intervening on, since all other nodes can only influence $Y$ through $A$. Indeed, if $a^{*} \in R_{A}$ is the value of $A$ which maximizes $Y$, it is not necessary to try to find an intervention on ancestors of $A$ which results in $A=a^{*}$: just set $A=a^{*}$ directly (\Cref{subfig:one-parent-heur}).
If $Y$ has two or more parents, it is possible that a single intervention on one of the $A_{i}$ does not yield the best possible outcome.
Instead, a better configuration (potentially even the ideal case $(a^{*}_{1}, \ldots, a^{*}_{m})$) may be achieved by intervening \emph{on a common ancestor} of some or all of the $A_{i}$ (\Cref{subfig:twopar_heur}).
Notice that $X_{0}$ is also a common ancestor of $A_{1}, A_{2}$, but one is never better off intervening on $X_{0}$ than on $X_{1}$.
This seems to indicate that testing interventions on, for instance, all lowest common ancestors (LCAs, see \Cref{sec:app-dags}) of the parents of $Y$, and only them, is necessary.
While this works in \Cref{subfig:twopar_heur}, it fails for a graph such as \Cref{subfig:vanilla-lca-issue}, where $X$ needs to be tested and yet it is not in $\LCA(A_{1}, A_{2}) = \{A_{1}\}$.
This suggests that we need to define a stricter notion of common ancestor to make progress in characterizing $\mGISS_{Y}(G)$.

\input{./lsca_intuition_figure.tex}

\begin{definition}[Lowest Strict Common Ancestors of a Pair of Nodes]
  \label{def:lsca_pair}
  The node $V \in \mathbf{V}$ is a \emph{strict common ancestor} of $X,Y \in \mathbf{V}$ if $V$ is a common ancestor of $X,Y$ from which both $X$ and $Y$ can be reached from $V$ with paths $V\dashrightarrow X$ and $V\dashrightarrow Y$ not containing $Y$ and $X$, respectively.
  The set of strict common ancestors of $X,Y$ is denoted $\SCA(X,Y)$.\\
  Furthermore, $V$ is a \emph{lowest strict common ancestor} of $X,Y\in \mathbf{V}$ if $V$ is a minimal element of $\SCA(X,Y)$ with respect to the ancestor partial order $\anpo$.
  The set of lowest strict common ancestors of $X,Y$ is denoted $\LSCA(X,Y)$.
\end{definition}


\begin{definition}[Lowest Strict Common Ancestors of a Set]
  \label{def:lsca_set}
  Let $\mathbf{U} \subseteq \mathbf{V}$ and $V\in \mathbf{V}\setminus \mathbf{U}$.
  The node $V$ is a \emph{lowest strict common ancestor} of $\mathbf{U}$ if it is  a lowest strict common ancestor of some pair of nodes $U,U'$ in $\mathbf{U}$.
  The set of lowest strict common ancestors is denoted $\LSCA(\mathbf{U})$.
  That is,
  \begin{equation}
    \begin{split}
      \LSCA(\mathbf{U}) \coloneqq \{&V\in \mathbf{V} \setminus \mathbf{U} \colon \exists U,U'\in \mathbf{U} \\
        &\suchthat V\in \LSCA(U,U')\}.
    \end{split}
  \end{equation}
\end{definition}

Our heuristic argument so far suggests that we need to test the parents of $Y$ and their LSCAs. However, there are additional nodes that must be considered: the reasoning for testing the lowest strict common ancestors of the parents can be repeated.
For instance, in \Cref{subfig:not_lca_heur}, the best possible configuration of the $A_{i}$ may be achieved by intervening on $Z$. Such an intervention could result in a combination of values of $X_{1}$ and $A_{2}$ that leads to the best possible combinations of $A_{1}$ and $A_{2}$.
This suggests that the $\mGISS_{Y}(G)$ should be determined by recursively finding all the LSCAs of the parents of $Y$, then the LSCAs of that set, and so on, ultimately resulting in what we call the ``LSCA closure of the parents of $Y$'', denoted $\LinftyY$.
In the remainder of this section, we formally define $\LinftyY$, find a simple graphical characterization for it, and prove that it indeed equals $\mGISS_{Y}(G)$.




\begin{definition}[LSCA closure]
  \label{def:lsca_closure}
  For every $i\in \mathbb{N}$ we define the {i\th-order LSCA set} $\mathcal{L}^{i}(\mathbf{U})$ of $\mathbf{U}\subseteq \mathbf{V}$ as follows:
  \begin{equation}
    \label{eq:lsca_sets}
    \begin{split}
      \mathcal{L}^{0}(\mathbf{U}) &\coloneqq \mathbf{U} \\
      \mathcal{L}^{i}(\mathbf{U}) &\coloneqq \LSCA(\mathcal{L}^{i-1}(\mathbf{U})) \cup \mathcal{L}^{i-1}(\mathbf{U}).
    \end{split}
  \end{equation}
  The \emph{LSCA closure} $\Linfty(\mathbf{U})$ of $\mathbf{U}$ is given by
  \begin{equation}
    \label{eq:lsca_closure}
    \begin{split}
      &\mathcal{L}^{\infty}(\mathbf{U}) \coloneqq \mathcal{L}^{k^{*}}(\mathbf{U}),\\
      &\text{ where }\ \ k^{*}=\min \{i \in \mathbb{N} \colon \mathcal{L}^{i}(\mathbf{U}) = \mathcal{L}^{i+1}(\mathbf{U})\}.
    \end{split}
  \end{equation}
\end{definition}

\begin{remark}
  Notice that the existence of the $k^{*}$ in \Cref{eq:lsca_closure} is guaranteed, since by construction $\mathcal{L}^{i+1}(\mathbf{U})\subseteq \mathcal{L}^{i}(\mathbf{U}) \subseteq \mathbf{V}$ for all $i\in\mathbb{N}$ and \textbf{V} is finite.
\end{remark}

\begin{example}
  Consider the graph in \Cref{subfig:not_lca_heur} and set $\mathbf{U} = \{A_{1}, A_{2}\}$.
  Then, $\mathcal{L}^{0}(\mathbf{U}) = \{A_{1}, A_{2}\}, \mathcal{L}^{1}(\mathbf{U}) = \{X_{1}, A_{1}, A_{2}\}, \mathcal{L}^{2}(\mathbf{U}) = \{Z, X_{1}, A_{1}, A_{2}\} = \mathcal{L}^{3}(\mathbf{U})$.
  Hence, $\Linfty(\mathbf{U}) = \{Z, X_{1}, A_{1}, A_{2}\}$.
\end{example}

We will introduce the notion of ``$\Lambda$-structures'' (\Cref{fig:lambda-struct}), which provides an alternative, elegant, simple graphical characterization of $\LinftyY$.
It will also be instrumental in the proofs of the main results of this paper.

\begin{definition}[$\Lambda$-structure]
  \label{def:lambda_struct}
  Let $V, A, B\in \mathbf{V}$.
  Furthermore, let $\pi_{A}: V\dashrightarrow A$, $\pi_{B}: V\dashrightarrow B$ be paths.
  The tuple $(V,\pi_{A},\pi_{B})$ is a \emph{$\Lambda$-structure} over $(A,B)$ if $\pi_{A}$ and $\pi_{B}$ only intersect at $V$.
  Now, let $\mathbf{U}, \mathbf{W}\subseteq \mathbf{V}$.
  The node $V$ is said to \emph{form a $\Lambda$-structure} over $(\mathbf{U}, \mathbf{W})$ if there are nodes $U\in \mathbf{U}$ and $W\in \mathbf{W}$, and
  paths $\pi_{U}\colon V\dasharrow U$, $\pi_{W}\colon V\dasharrow W$ such that $(V,\pi_{U},\pi_{W})$ is a $\Lambda$-structure over $(U,W)$.
  Denote by $\Lambda(\mathbf{U},\mathbf{W})$ the set of all nodes forming a $\Lambda$-structure over $(\mathbf{U},\mathbf{W})$.
\end{definition}

Notice that, if $V\in \mathbf{U} \cap \mathbf{W}$, then trivially $V\in \Lambda(\mathbf{U},\mathbf{W})$: just take the trivial paths $\pi=\pi'=(V)$.

\begin{restatable}[Simple Graphical Characterization of LSCA Closure]{theorem}{simplecharactlsca}
  \label{prop:simple_graphical_charact}
  A node $V \in \mathbf{V}$ is in the LSCA closure $\Linfty(\mathbf{U})$ of $\mathbf{U}\subseteq \mathbf{V}$ if and only if $V$ forms a $\Lambda$-structure over $(\mathbf{U},\mathbf{U})$.
  \emph{I.e.} $\Linfty(\mathbf{U}) = \Lambda(\mathbf{U},\mathbf{U})$.
\end{restatable}

\begin{figure}[h]
  \centering
    \begin{tikzpicture}[mynode/.style={fill=white,inner sep=0pt,minimum size=0.7cm}, >=latex]
      \node[mynode] (V) at (2,3) {$V$};
      \node[mynode] (U) at (0,0) {$U$};
      \node[mynode] (U') at (4,0) {$U'$};
      \node[mynode] (pis) at (2,1.2) {$\pi_{U}\cap \pi_{U'} = \{V\}$};
      \node at ($(U.east) + (0.2,0)$) {$\in \mathbf{U}$};
      \node at ($(U'.east) + (0.2,0)$) {$\in \mathbf{U}$};
      \draw[->,dotted,bend right=30] (V) to node[pos=0.9,above,inner sep=1cm] {\small$\pi_{U}$} (U);
      \draw[->,dotted,bend left=30] (V) to node[pos=0.9,above,inner sep=1cm] {\small$\pi_{U'}$} (U');
   \end{tikzpicture}
  \caption{A $\Lambda$-structure over $(\mathbf{U}, \mathbf{U})$. The LSCA closure $\Linfty(\mathbf{U})$ of a set $\mathbf{U}$ is the set of all such structures.}
  \label{fig:lambda-struct}
\end{figure}

We are now ready for the main result of this paper: that the LSCA closure $\Linfty(\PA(Y))$ of the parents of $Y$ is the minimimal globally interventionally superior set with respect to $Y$.

\begin{restatable}[Superiority of the LSCA Closure]{theorem}{lscamgiss}
  \label{thm:superiority_of_lsca}
  Let $G$ be a causal graph and $Y$ a node of $G$ with at least one parent.
  Then, the LSCA closure $\mathcal{L}^{\infty}(\PA(Y))$ of the parents of $Y$ is the minimal globally interventionally superior set $\mGISS(G)$ of $G$ relative to $Y$.
\end{restatable}
We emphasize that, due to \Cref{prop:cond-vs-atomic}, this graphical characterization of the $\mGISS_{Y}(G)$ is valid both for conditional interventions in a probabilistic causal model as for atomic interventions in a deterministic causal model (\emph{i.e.} a causal model with known $\mathbf{n}$).

\section{Algorithm to Find the Minimal Globally Interventionally Superior Set}
\label{sec:algor-find-minim}


\input{./algorithm_section.tex}


\section{Experimental Results}
\label{sec:experiments}
We evaluate $C_4$ on both random and real graphs.
Additionally, we examine the impact of our method on the cumulative regret of a bandit algorithm.

\input{./experiments_figure.tex}

\paragraph{Search Space Reduction in Random Graphs}
We applied the C4 algorithm to randomly generated DAGs using the ErdőS-Rényi model for $N$ graphs and probability $p$ \citep{erdos1959random} adapted to DAG-generation\footnotemark.
\footnotetext{After fixing a total order $\trianglelefteq$ on the nodes, each pair of nodes $V$, $u$ with $V \trianglelefteq u$ is assigned an edge $(V, u)$ with probability $p$. The value $p$ can be used to control the expected degree.}
We generated $1000$ graphs using $20, 100, 300$, and $500$ nodes, and varying the expected (total) degree of nodes from $2$ to $11$ in steps of $3$.
For each graph $G$, we set the target $Y$ to be the node with the most ancestors, used C4 to compute $\Linfty(\Pa(Y)) = \mGISS_{Y}(G)$, and calculated the fraction of nodes in $\An(Y) \setminus \{Y\}$ that remain in $\mGISS_{Y}(G)$.
The results revealed that, for a given number of nodes, graphs with lower expected degrees benefit more from our method (\emph{i.e.} their $\mGISS_{Y}(G)$ correspond to smaller fractions of $\An(Y) \setminus \{Y\}$).
Furthermore, for a fixed expected degree, our method is more effective for higher numbers of nodes.
For example, for graphs with $500$ nodes, the mGISS retained, on average, $17\%$, $29\%$, $62\%$ and $77\%$ of the nodes, for expected degrees of $2, 5, 8$ and $11$, respectively.
Moreover, graphs with an expected degree of $5$ saw these numbers decrease from $70\%$ at $20$ nodes to $47\%, 35\%$ and $29\%$ for $100, 300$ and $500$ nodes, respectively.
The complete results are presented in histograms in \Cref{fig:fractions-hist} (\Cref{sec:app-exps}).
These results are not surprising:
if the average degree is small compared to the number of nodes, the edge density is small, in which case we expect fewer $\Lambda$-structures to form over Pa(Y).

Graphs modeling real-world systems tend to have low average degrees, as can be seen in the graphs from the popular Bayesian network repository \texttt{bnlearn}.
Therefore, we expect our method to be especially effective in those graphs.
We test this below.

\paragraph{Search Space Reduction in Real-World Graphs}
We tested our method in most graphs from the \texttt{bnlearn} repository\footnotemark
\footnotetext{All that can be imported in Python using the library \texttt{pgmpy}.}
, as well as on a graph representing the causal relationships  between train delays in a segment of the railway system of the Netherlands (see \Cref{sec:app-exps}).
For each graph, we set $Y$ to be the node with most ancestors\footnotemark.
\footnotetext{\label{fn:Y-not-single-child}We also require $Y$ to have more than one parent, to avoid the trivial case with $\abs{\mGISS_{Y}(G)} = 1$.}
The results are presented in the bar plot of \Cref{fig:realworld_fractions} (\Cref{sec:app-exps}).
This confirmed that realistic models with larger graphs tended to benefit more from our method, with a reduction of over $90\%$ of the search space for some of the largest models.
Notice also that these models indeed have relatively small average degrees, all below $4.0$.
From this, we conclude that we can expect our method to be useful when reducing the search space of conditional causal bandit tasks in real-world causal models, especially when they are large.
\todo{Add pathfinder image!}


\paragraph{Impact on Conditional Intervention Bandits}

We present empirical evidence that restricting the node search space to the mGISS allows a straightforward UCB-based\footnotemark algorithm (which we call CondIntUCB) for conditional causal bandits to converge more rapidly to better nodes.
\footnotetext{The Upper Confidence Bound (UCB) algorithm is a widely used MAB algorithm. See \emph{e.g.} \citet{lattimore2020bandit}.}
As explained in \Cref{sec:prelims}, on each round the algorithm must (i) choose which node $X$ to intervene on; and (ii) choose the value for $X$, given its conditioning set $Z_{X}$\footnotemark.
\footnotetext{For simplicity, since the smallest possible observable conditioning set is $\An(X)$ (see \Cref{sec:prelims}), we use $\mathbf{Z}_{X} = \An(X)$.}
Choice (i) employs UCB over nodes, while choice (ii) utilizes a UCB instance specific to the conditioning set value.
In other words, for each realization of $\condset_{X}$ (each context) there is a UCB.
This is identical to what is described in \citet[\S 18.1]{lattimore2020bandit} for contextual bandits with one bandit per context.
The cumulative regret\footnotemark is computed with respect to node choice, since we want to see how our node selection method affects the quality of node choice by CondIntUCB.
\footnotetext{For the computation of regret, we use the estimated best arm, defined as the arm that most runs concluded to be the best at the end of training.}
We use 3 real-world datasets from the \texttt{bnlearn} repository, and again choose the node of each dataset with the most ancestors as the target\footref{fn:Y-not-single-child}.
These datasets were selected because their graphical structures are non-trivial\footnotemark and both $\An(Y)$ and $\mGISS_{Y}(G)$ are sufficiently small to allow experimentation with our setup.
\footnotetext{In contrast, the \texttt{cancer} dataset, for example, only has nodes whose mGISS is either all of the node's ancestors or a single node.}
For each dataset, we run CondIntUCB 500 times and plot the two average cumulative regret curves along with their standard deviations, corresponding to using all nodes (brute-force) and the mGISS nodes (\Cref{fig:exps}).
The total number of rounds is chosen as to observe (near) convergence.
These results show that cumulative regret curves can be significantly improved—meaning that better nodes are selected earlier for applying conditional interventions—if the search space over nodes is pruned using our C4 algorithm.


\section{Related Work}
\label{sec:related-work}

Recent research has explored the integration of causality and multi-armed bandit (MAB) frameworks.
As mentioned in \Cref{sec:introduction}, \citet{lattimore2016bandits} introduced the original causal bandit problems, which involve hard interventions in causal models.
Subsequent works \citep{rajat2017identifying, yabe2018causal, lu2020regret, nair2021budgeted,sawarni2023learning,maiti2022causal,feng2023combinatorial} proposed algorithms for variants of causal bandits with both hard and soft interventions, budget constraints, and unobserved confounders, all under specific assumptions, such as binary variables, simple graphs, or known post-intervention distributions.
Note that we do not make such assumptions.

Recent works in ``contextual causal bandits'' address interventions that account for context, bearing a superficial resemblance to our problem.
However, our problem remains distinct.
In \Citet{madhavan2024contextual}, the term "contexts" is used in a very different way, actually referring to different graphs as opposed to different variable values.
\Citet{subramanian2022contextual,subramanian2024causal} tackle the scenario in which an intervention is performed, with knowledge of a given set of context variables, on a \emph{pre-chosen} variable $X$ that has an edge into $Y$ (and no other outgoing edges).
This approach can be understood as selecting a conditional intervention for a predefined node from a very simple graph.
In contrast, in our setting we need to choose what variable to intervene on to begin with, and there are no restrictions on the causal graph.

All of the works described above proposed algorithms which aim at accelerating learning by utilizing knowledge of the causal model.
As explained in \Cref{sec:introduction}, this contrasts with the work by \citet{lee2018structural,lee2019structural}, which, just like our work, uses knowledge of the causal graph to find a minimal search space (over the nodes) for causal bandits.
While they focus on multi-node, hard interventions, we focus on single-node, conditional interventions.

The work of \Citet{lee2020characterizing} presents an interesting connection to our work.
Given a causal graph, they study the sets of pairs $(\mathrm{node}, \mathrm{context(node)})$ (referred to as ``scopes'') that may correspond to an optimal (multi-node) intervention policy where each node $X$ in a scope is intervened on according to a policy $\pi_{X}(X \mid \mathrm{context(X)})$.
This is a challenging problem, and they do not provide a full characterization of these optimal scopes, instead deriving a set of rules that can be used to compare certain pairs of scopes.
In this paper, we instead assume that the practitioner knows the appropriate conditioning set $\condset_{X}$ (context) to use and impose only minimal restrictions on what $\condset_{X}$ can be, focusing instead on choosing the nodes that can yield the best results.
While \citet{lee2020characterizing} consider multi-node interventions, it would be interesting in future work to adapt their ideas to the single-node case to identify the smallest $\condset_{X}$ sets for which the best policy can still be found.
Such an approach
could further accelerate learning by MAB algorithms.

\section{Conclusion}
\label{sec:conclusion}

In this paper, we introduced the conditional causal bandit problem, where the agent only has knowledge of the causal graph $G$, the arms are conditional interventions,
and the reward variable belongs to $G$.
The theoretical contributions include a rigorous, simple graphical characterization of the minimal set of nodes which is guaranteed to contain the node with the optimal conditional intervention, and the C4 algorithm, which computes this set in linear time.
Empirical results validate that our approach substantially prunes the search space in both real-world and sparse randomly-generated graphs.
Furthermore, integrating mGISS with a UCB-based conditional bandits algorithm showcased improved cumulative regret curves.

As mentioned in \Cref{sec:related-work}, a possible future research direction is to identify the smallest conditioning set(s) $\mathbf{Z}_{X}$, rather than assuming, as we do, that the practitioner or problem setting determines them.
Another relevant direction for future work is the incorporation of latent variables.
On the practical side, instead of combining C4 with the simple CondIntUCB, one could replace CondIntUCB with any other conditional bandit algorithm that leverages the model's causal structure.
As discussed in \Cref{sec:related-work}, no such algorithm currently exists.
Nevertheless, we expect that combining C4 with any future algorithm for causal bandits with conditional interventions will be advantageous, as it reduces the number of arms that need to be considered.


\section*{Impact Statement}
Our work proposes tools to enhance the efficiency of AI agents in decision-making problems.
Solutions to these kinds of problems lead to well-established issues, particularly when applied blindly --- that is, when the algorithms' conclusions are used to make real-world decisions without assessing potential dangers or ethical concerns not captured by the mathematical model.
Mitigating such risks may require, for example, the regulation of these tools and the education of users regarding their limitations.


\bibliographystyle{apalike}
\bibliography{icml2025_conference}


\newpage
\appendix
\onecolumn

\section{Directed Acyclic Graphs}
\label{sec:app-dags}

All graphs in this paper are directed acyclic graphs (DAGs).
Every path is assumed to be directed.
A path $\pi$ in a graph $G = (\mathbf{V}, E)$ is a tuple of nodes such that each node $X$ in the path has an outgoing arrow from $X$ to the next node in the tuple\footnotemark.
For $X \in \mathbf{V}$, we denote by $\Pa(X)$, $\Ch(X)$, $\De(X)$ and $\An(X)$ the sets of parents, children, descendants and ancestors of $X$, respectively.
\footnotetext{Since all DAGs we are considering in this paper come from SCMs, there is at most one arrow between any two nodes, so that a tuple of nodes is enough to define a path. For a general graph one would have to specify a list of edges.}
We denote by $\pi\colon X \dashrightarrow Y$ a path starting at node $X$ and ending at node $Y$, and $\mathring{\pi}$ denotes the path formed by the inner nodes of $\pi$.
By abuse of notation, we often perform set operations such as $\pi_{1} \cap \pi_{2}$ between paths, which implicitly means that these operations are performed on the sets of nodes belonging to the paths.
Tuples with a single node are also considered to be paths, and are said to be \emph{trivial}.
Also, if $B\in \pi\colon X\dashrightarrow Y$, then the paths $\pi\vert^{Z}\colon Z\dashrightarrow Y$ and $\pi\vert_{Z}\colon X\dashrightarrow Z$ are the paths resulting from removing from $\pi$ all nodes before and after $Z$, respectively.
Every node is an ancestor of itself, so that the relation $\anpo$ defined by $X \anpo Y \iff Y \in \An(X)$ is a partial order.
Given a set $\mathbf{U}$ of nodes, we denote by $\max_{\anpo}[\mathbf{U}]$ the set of maximal elements of $\mathbf{U}$ with respect to $\anpo$.
We call this the \emph{ancestor partial order}.
If there is a non-trivial path from $X$ to $Y$, then $Y$ is said to be \emph{reachable} from $X$.
The set of common ancestors of nodes $X$ and $Y$ is denoted $\CA(X,Y) = \An(X) \cap \An(Y) = \{Z \in \mathbf{V}\ \colon Z \anpo X \wedge Z \anpo Y\}$.
Finally, the \emph{degree} of a node in a DAG is the sum of the incoming and outgoing arrows of that node.

We also make use of a lesser-known graph theory concept, relevant for this paper: the ``lowest common ancestors'' of nodes $(X,Y)$.
These are common ancestors that don't reach any other common ancestors, intuitively making them the ``closest'' to $(X,Y)$.

\begin{definition}[Lowest Common Ancestors in a DAG \citep{bender2005lowest}]
  Let $X, Y$ be nodes of a DAG $G=(\mathbf{V},E)$.
  A \emph{lowest common ancestor (LCA)} of $X$ and $Y$ is a maximal element of $\CA(X,Y)$ with respect to the ancestor partial order $\anpo$.
  The set of all lowest common ancestors of $X$ and $Y$ is denoted $\LCA(X,Y)$.
\end{definition}

For example, in \Cref{subfig:twopar_heur}, $\LCA(A_{1}, A_{2}) = \{X_{1}\}$, whereas in \Cref{subfig:not_lca_heur}, $\LCA(A_{1}, A_{2}) = \{A_{1}\}$.

\section{Unrolled Assignments}
\label{sec:app-unrolled_assigns}

The structural assignments of an SCM can be utilized to express any endogenous variable as a function of the exogenous variables only.
This is achieved by composing the assignments until reaching the exogenous variables.
Our proofs will rely on these functions, which we will refer to as ``unrolled assignments'', since we ``unroll'' the expressions for the endogenous variables until only exogenous variables are left.
We define them formally by induction as follows:

\begin{definition}[Unrolled Assignment]
  \label{def:unr_assign}
  We define the \emph{unrolled assignment} $\bar{f}_{X}\colon R_{\mathbf{N}} \to R_{X}$ of any (exogenous or endogenous) variable $X$ from an SCM $\cC = (\mathbf{V}, \mathbf{N}, \mathcal{F}, p_{\mathbf{N}})$ by induction.
  For $X = N_{i}\in \mathbf{N}$, define $\bar{f}_{X}(\mathbf{n}) \coloneqq n_{i}$.
  Now, let $\trianglelefteq$ be a topological order on $G$ where the first elements are the endogenous variables with no endogenous parents.
  Let $S$ be the poset $(\mathbf{V}, \trianglelefteq)$.
  In ascending order, take $X\in S$, and define:
  \begin{equation}
    \label{eq:unrolled_assign}
      \bar{f}_{X}(\mathbf{n})  \coloneqq
    \begin{cases}
      f_{X}(n_{X}), \ \mathrm{if}\ \PA(X) = \emptyset \\
      f_{X}(\bar{f}_{\PA(X)}(\mathbf{n}), n_{X}),\ \mathrm{otherwise}
    \end{cases},
  \end{equation}
  where $\bar{f}_{\PA(X)}(\mathbf{n}) = (\bar{f}_{\PA(X)_{1}}(\mathbf{n}),\ldots,\bar{f}_{\PA(X)_{m_{X}}}(\mathbf{n}))$ and $m_{X} = \abs{\PA(X)}$.
\end{definition}

Additionally, we can consider $X$ as a function of both exogenous variables and a chosen endogenous variable $B$.
To achieve this, we substitute the assignments until we reach either $B$ or the exogenous variables, thereby "unrolling" the dependencies until we reach the exogenous variables or we are blocked by $B$.

\begin{definition}[Blocked Unrolled Assignment]
  \label{def:blocked_unr_assign}
  Let $X, B$ endogenous variables from an SCM $\cC = (\mathbf{V}, \mathbf{N}, \mathcal{F}, p_{\mathbf{N}})$
  We define the \emph{unrolled assignment} $\bar{f}_{X}[B]\colon R_{B} \times R_{\mathbf{N}} \to R_{X}$ of $X$ \emph{blocked by} $B$ by induction.
  Let $S$ be the poset from \Cref{def:unr_assign}.
  In ascending order, take $X\in S$, and define:
  \begin{equation}
    \label{eq:blocked_unrolled_assign}
      \bar{f}_{X}[B](B, \mathbf{n})  \coloneqq
    \begin{cases}
      \bar{f}_{X}(\mathbf{n}), \ \mathrm{if}\ X\notin \De(B) \\
      B, \ \mathrm{if}\ X=B \\
      f_{X}(\bar{f}_{\PA(X)}[B](B, \mathbf{n}),n_{X})\ \mathrm{otherwise}
    \end{cases},
  \end{equation}
  where $\bar{f}_{\PA(X)}[B](\mathbf{n}) = (\bar{f}_{\PA(X)_{1}}[B](B,\mathbf{n}),\ldots,\bar{f}_{\PA(X)_{m_{X}}}[B](B,\mathbf{n}))$ and $m_{X} = \abs{\PA(X)}$.
\end{definition}

\begin{remark}
  Strictly speaking, $\bar{f}_{X}$ is not a function of all the values of all the noise variables, but only of the exogenous variables $N_{W}$ associated with endogenous variables $W$ that $Y$ depends on.
  Similarly, $\bar{f}_{X}[B]$ is also not a function of all the values of all the noise variables.
  Namely, if $X$ only depends on an endogenous variable $W$ through $B$, then $n_{W}$ will never appear in the expression for $\bar{f}_{X}[B]$, and the same holds in case $B=W$.
  A more accurate notation would reflect these facts, writing the unrolled assignments as functions of the specific noise variables that can affect them, rather than as functions of all noise variables.
  We opted not to adopt this notation to avoid complicating the notation and conceptual simplicity of these quantities.
\end{remark}


\section{Conditional Superiority vs Deterministic Atomic Superiority}
\label{sec:app-superiorities}

We will show that conditional intervention superiority is equivalent to deterministic atomic intervention superiority.
This result will help prove results about the former by making use of the former, which is mathematically simpler and easier to reason about.

\begin{notation*}
  We denote by $G^{*}$ the graph resulting from adding to a causal graph $G$ the exogenous variables as nodes, and an edge $N_{X_{i}} \rightarrow X_{i}$ for each exogenous variable $N_{X_{i}}$.
\end{notation*}

\begin{lemma}[Conditional Intervention vs Atomic Intervention]
  \label{lemma:cond-int-vs-atom-int}
  Let $X, Y$ be endogenous variable of $\cC$ and Let $A$ be a set of endogenous variables of an SCM $\cC$, and.
  When evaluated at a setting $\mathbf{n}$, the unrolled assignment of $Y$ after a conditional intervention $\dop(X=g(A))$ coincides with the unrolled assignment of $Y$ after the atomic intervention $\dop(X=\bar{f}_{A}(\mathbf{n}))$.
  That is:
  \begin{equation*}
    \bar{f}_{Y}^{\dop(X=g(A))}(\mathbf{n}) = \bar{f}_{Y}^{\dop(X=g(\bar{f}_{A}(\mathbf{n})))}(\mathbf{n}).
  \end{equation*}
\end{lemma}
\begin{proof} %
  This result can be proved by induction in a similar way to \Cref{lemma:blocking_vs_intervening}.\\
  Let $X$ be an endogenous variable.
  We want to prove that the expression holds for any variable $Y$.
  We will prove this by induction on a topological order $\trianglelefteq$ on the nodes of $G^{*}$ such that the first elements are precisely the exogenous variables, \emph{i.e.} $N \trianglelefteq Z$ whenever $N \in \mathbf{N}$ and $Z \in \mathbf{V}$.\\
  The result is true for the exogenous variables.
  Indeed, for $Y \in \textbf{N}$, and making use of \Cref{lemma:blocking_vs_intervening}, we have that
  $\bar{f}_{Y}^{\dop(X=g(\bar{f}_{A}(\mathbf{n})))}(\mathbf{n}) = \bar{f}_{Y}[X](g(\bar{f}_{A}(\mbox{n})), \mathbf{n}) = \bar{f}_{Y}(\mathbf{n}) = Y = \bar{f}_{Y}^{\dop(X=g(A))}(\mathbf{n})$, since $Y \notin \De(X) \cup \{X\}$ and $Y$ is exogenous (both in the pre- and post-intervention (both conditional and atomic) structural causal models).
  This establishes the base case of the induction.\\
  Now let $Y$ be endogenous.
  For the inductive step, we will prove that, if the result is true for the parents $\PA_{G^{*}}(Y)$ of $Y$ in $G^{*}$ (induction hypothesis), then it is also true for $Y$.
  Assume the antecedent (induction hypothesis).
  There are three possibilities: $Y \in \De(X)\setminus\{X\}$, $Y = X$ or $Y \notin \De(X)$.
  In case $Y \in \De(X)\setminus\{X\}$:
  \begin{equation}
    \begin{split}
      \bar{f}_{Y}^{\dop(X=g(\bar{f}_{A}(\mathbf{n})))}(\mathbf{n}) & \overeq{def} f_{Y}^{\dop(X=g(\uf_{A}(\mathbf{n})))}(\bar{f}_{\PA(Y)}^{\dop(X=g(\uf_{A}(\mathbf{n})))}(\mathbf{n}), n_{Y}) \\
      & = f_Y(\bar{f}_{\PA(Y)}^{\dop(X=g(\uf_{A}(\mathbf{n})))}(\mathbf{n}), n_{Y})\\
      & \overeq{I.H.} f_Y(\bar{f}_{\PA(Y)}^{\dop(X=g(A))}(\mathbf{n}), n_{Y})\\
      & = f_Y^{\dop(X=g(A))}(\bar{f}_{\PA(Y)}^{\dop(X=g(A))}(\mathbf{n}), n_{Y})\\
      & \overeq{def} \bar{f}_{Y}^{\dop(X=g(A))}(\mathbf{n}),
    \end{split}
  \end{equation}
  where in the second and fourth equalities we used that $f^{\dop(X=g(\uf_{A}(\mathbf{n})))}_{Y}=f_{Y} = f_{Y}^{\dop(X=g(A))}$.
  We also used that $Pa(Y)$ is unchanged by these interventions.
  If instead $Y = X$, then one has:
  \begin{equation}
    \label{eq:2}
    \begin{split}
      \bar{f}_{X}^{\dop(X=g(A))}(\mathbf{n}) &\overeq{def} f_{X}^{\dop(X=g(A))}(\bar{f}_{\PA^{G^{\dop(X=g(A))}}(X)}^{\dop(X=g(A))}(\mathbf{n}), n_{X}) \\
                                             &= f_{X}^{\dop(X=g(A))}(\bar{f}_{A}^{\dop(X=g(A))}(\mathbf{n}), n_{X}) \\
                                             &= g(\bar{f}_{A}(\mathbf{n}), n_{X}),
    \end{split}
  \end{equation}
  and also:
  \begin{equation}
    \label{eq:3}
    \begin{split}
        \bar{f}_{X}^{\dop(X=g(\bar{f}_{A}(\mathbf{n})))}(\mathbf{n})
            &\overeq{def}  f_{X}^{\dop(X=g(\uf_{A}(\mathbf{n})))}(\bar{f}_{\PA^{G^{\dop(X=g(\bar{f}_{A}(\mathbf{n})))}}(X)}^{\dop(X=g(\bar{f}_{A}(\mathbf{n})))}(\mathbf{n}), n_{X})  \\
            &= g(\bar{f}_{A}(\mathbf{n}), n_{X}).
    \end{split}
  \end{equation}
  Finally, if $Y \notin \De(X)$, then trivially $\bar{f}_{Y}^{\dop(X=g(A))}(\mathbf{n}) = \bar{f}_{Y}(\mathbf{n})$ and $\bar{f}_{Y}^{\dop(X=g(\uf_{A}(\mathbf{n})))}(\mathbf{n}) = \bar{f}_{Y}(\mathbf{n})$.

  This establishes the inductive step: if the results holds for the first $j \ge \abs{\mathbf{N}}$ variables with respect to $\trianglelefteq$, then it also holds for the variable $j + 1$, since its parents are among the first $j$ variables.
\end{proof}

\begin{lemma}[Superiority and Paths] %
  \label{lemma:superiority-implies-blocked-path}
  If $X\detsup_{Y} W$, then all paths $W \dashrightarrow Y$ must include $X$.
\end{lemma}
\begin{proof}
  If $W\notin \An(Y)$, there are no paths from $W$ to $Y$ and the conclusion is vacuously true.
  We assume from now on that $W\in \An(Y)$.
  Assume, for the sake of contradiction, that there is a path $\pi\colon W \dashrightarrow A \rightarrow Y$ in $G$ without $X$, where $A$ is a parent of $Y$.
  Consider the SCM with graph $G$ and structural assignments and noise distributions given by:
  \begin{equation*}
    \begin{cases}
      f_{Y}(A, \Pa(Y)\setminus A, N_{Y}) = 2A + N_{Y} \cdot \heavyside(\sum_{Z\in \Pa(Y)\setminus A} Z) \\
      f_{C \in \pi \setminus W}(\Pa(C), N_{C}) = \prev_{\pi}(C) + N_{C} \cdot \heavyside(\sum_{Z\in \Pa(C)\setminus \prev_{\pi}(C)} Z) \\
      f_{W}(\Pa(W), N_{W}) = N_{W} \cdot \heavyside(\sum_{Z\in \Pa(W)} Z) \\
      f_{V \notin \pi}(\Pa(V), N_{V}) = N_{V} \cdot \heavyside(\sum_{Z\in \Pa(V)} Z) \\
      N_{V} \sim \mathrm{Ber}(\frac{1}{2})
    \end{cases},
  \end{equation*}
  where $\heavyside\colon \R \to \{0,1\}$ is the unit step function, which maps values larger than $0$ to $1$, and all non-positive values to $0$.
  Then, $\bar{f}_{Y}^{\dop(W=1)}(\mathbf{0}) = 2 \bar{f}_{A}^{\dop(W=1)}(\mathbf{0}) = 2$, while, for every $X$, we have $\bar{f}_{Y}^{\dop(X=X)}(\mathbf{0}) = 0$.
  That is, for the setting $\mathbf{n} = \mathbf{0}$, there is no intervention on $X$ that is better than $\dop(W=1)$, which contradicts the antecedent.
\end{proof}



\condvsatomic*
\begin{proof}
($\Rightarrow$): Assume $X\condsup_{Y} W$.
Let $\cC = (\mathbf{V}, \mathbf{N}, \mathcal{F}, p_{\mathbf{N}})$ be an SCM with causal graph $G$ and $\mathbf{m}\in R_{\mathbf{N}}$.
Let $g^{*} = \argmax_{g} \E_{\mathbf{n}} \bar{f}_{Y}^{\dop(X=g(\condset_{X}))}(\mathbf{n})$.
Then,
  $\forall h, \quad
  \E_{\mathbf{n}} \bar{f}_{Y}^{\dop(X=g^{*}(\condset_{X}))}(\mathbf{n}) \ge \E_{\mathbf{n}} \bar{f}_{Y}^{\dop(W=h(\condset_{W}))}(\mathbf{n})$.
  This holds in particular for $p_{\mathbf{N}} = \delta(\mathbf{m})$.
  Denoting by $\mathcal{F}(\mathbf{A}, \mathbf{B})$ the set of functions with domain $\mathbf{A}$ and codomain $\mathbf{B}$, we can then write:
  \begin{equation*}
    \forall h\in \mathcal{F}(R_{\condset_{W}}, R_{W}), \bar{f}_{Y}^{\dop(X=g^{*}(\bar{f}_{\condset_{X}}(\mathbf{m})))}(\mathbf{m})
    \ge \bar{f}_{Y}^{\dop(W=h(\bar{f}_{\condset_{W}}(\mathbf{m})))}(\mathbf{m}),
  \end{equation*}
  where we also used \Cref{lemma:cond-int-vs-atom-int}.
  Now, since every $w\in R_{W}$ can be attained from $\bar{f}_{\condset_{W}}(\mathbf{m})$ by an appropriately chosen $h$, then choosing $X^{*} = g^{*}(\bar{f}_{\condset_{X}}(\mathbf{m}))$ allows us to write:
  \begin{equation*}
    \forall w\in R_{W}, \bar{f}_{Y}^{\dop(X=X^{*})}(\mathbf{m})
    \ge \bar{f}_{Y}^{\dop(W=w)}(\mathbf{m}).
  \end{equation*}
  This proves that $X \detsup_{Y} W$.
  \partialqed{\Rightarrow}

  ($\Leftarrow$): Assume now that $X \detsup_{Y} W$.
  Let $p_{\mathbf{N}}\in \mathcal{P}(\mathbf{N})$ and $\mathcal{F}(G) = \{f_{V}\colon V\in G\}$.
  We want to show that $\max_{g} \E_{\mathbf{n}} \bar{f}_{Y}^{\dop(X=g(\condset_{X}))}(\mathbf{n}) \ge \max_{h} \E_{\mathbf{n}} \bar{f}_{Y}^{\dop(W=h(\condset_{W}))}(\mathbf{n})$.
  From \Cref{lemma:cond-int-vs-atom-int}, we can write this as $\max_{g} \E_{\mathbf{n}} \bar{f}_{Y}^{\dop(X=g(\uf_{\condset_{X}}(\mathbf{n})))}(\mathbf{n}) \ge \max_{h} \E_{\mathbf{n}} \bar{f}_{Y}^{\dop(W=h(\uf_{\condset_{W}}(\mathbf{n})))}(\mathbf{n})$.
  Denote the expected value in the left-hand-side by $\alpha(g)$, and the one on the right-hand-side by $\beta(h)$.
  Assume, for the sake of contradiction, that there is $h^{*}$ such that $\beta(h^{*}) > \alpha(g)$ for all $g$.
  Define $H(\mathbf{n}) = h^{*}(\uf_{\condset_{W}}(\mathbf{n}))$.
  Now, if $W\notin \An(Y)$, we simply define $g^{*}$ to output the observational value of $X$.
  If instead $W\in \An(Y)$, from \Cref{lemma:superiority-implies-blocked-path}, we know that\footnotemark $X \in \De(W)$ and all paths from $W$ to $Y$ go through $X$.
  We then define $g^{*}(\uf_{\condset_{X}}(\mathbf{n})) = \uf_{X}[W](h^{*}(\uf_{\condset_{W}}(\mathbf{n})), \mathbf{n})$.
  Let $G(\mathbf{n}) = g^{*}(\uf_{\condset_{X}}(\mathbf{n})) $.
  Then:
  \begin{equation*}
    \begin{split}
      \alpha(g^{*}) &= \E_{\mathbf{n}} \bar{f}_{Y}^{\dop(X=G(\mathbf{n}))}(\mathbf{n}) \\
                    &= \E_{\mathbf{n}} \bar{f}_{Y}[X](G(\mathbf{n}), \mathbf{n}) \\
                    &= \E_{\mathbf{n}} \bar{f}_{Y}[X](\uf_{X}[W](H(\mathbf{n}), \mathbf{n}), \mathbf{n}) \\
                    &= \E_{\mathbf{n}} \bar{f}_{Y}[W](H(\mathbf{n}), \mathbf{n}) \\
                    &= \E_{\mathbf{n}} \bar{f}^{\dop(W=H(\mathbf{n}))}_{Y}[W](\mathbf{n}) \\
                    &= \beta(h^{*}).
    \end{split}
  \end{equation*}
  where in the fourth equality we used \Cref{lemma:chaining_b_and_z}.
  This contradicts our assumption.
  \partialqed{\Leftarrow}
\end{proof}

As mentioned in the main text (\Cref{sec:cond-sup}), the superiority relation for atomic interventions in non-deterministic (general) SCMs defined in the natural way is \emph{not} equivalent to $\condsup_{Y}$.
Indeed, consider the following example:

\begin{example}
  \label{example:condsup-not-eq-to-detsup}
  Consider the SCM given by $Y=A\oplus W$, $A=Z\oplus W$ and $N_{Z},N_{W} \sim \mathrm{Bern}(\nicefrac{1}{2})$, where $\oplus$ is the XOR operator and all variables are binary.
  Setting $Z$ to $1$ ensures that $Y=1$, so that $\E_{\mathbf{n}}\uf_{Y}^{\dop(Z=1)}=1$.
  No atomic intervention on $A$ would accomplish this: $\E_{\mathbf{n}}\uf_{Y}^{\dop(A=0)}=\E_{\mathbf{n}}\uf_{Y}^{\dop(A=1)}=\frac{1}{2}$.
  Hence $A \not\intsup_{Y}^{a} Z$.
  However, $\E_{\mathbf{n}}\uf_{Y}^{\dop(A=g(W))}=1 = \max R_{Y}$ if one uses the policy $g(0)=1$, $g(1)=0$.
  Thus $A \condsup_{Y} Z$.
\end{example}

\section{Intervention Superiority Relations are Preorders}
\label{sec:app-preorders}

\begin{proposition}
  \label{prop:sup_is_preorder}
  The interventional superiority relation between nodes is a preorder in $G$.
  The interventional superiority relation between node sets is also a preorder.
\end{proposition}
\begin{proof} %
  Let $G$ be a DAG and let $Y\in G$.
  We will first prove the result for the interventional superiority relation on nodes.\\
  Reflexivity:
  Let $X$ be a node in $G$ and $\C \in \C(G)$.
  For each setting $n$, the largest value of $Y$ that can be achieved by intervening on $X$ is attained when setting $X$ to $X^{*}(n) = \argmax_{X} \uf_{Y}^{\dop(X=X)}(n)$.
  Hence, $\uf_{Y}^{\dop(X=X^{*}(n))}(n) \ge \uf_{Y}^{\dop(X=X)}(n)$ for all $X\in R_{X}$, so that $X \detsup_{Y} X$.\\
  Transitivity:
  assume that $Z \detsup_{Y} W$ and $W\detsup_{Y} X$.
  Let $\C\in\C(G)$ and $n\in R_{N}$.
  Then $\max_{X} \uf_{Y}^{\dop(X=X)}(n) \le \max_{w} \uf_{Y}^{\dop(W=w)}(n) \le \max_{Z} \uf_{Y}^{\dop(Z=Z)}(n)$.
  Hence $Z \detsup_{Y} X$.\\
  This establishes that $\detsup_{Y}$ is a preorder in $G$.
  We now show the result for node sets. Let $\mathbf{X}$, $\mathbf{W}$ and $\mathbf{Z}$ be sets of nodes in $G$. \\
  Reflexivity:
  let $X\in \mathbf{X}$.
  Since, by reflexivity of $\detsup_{Y}$ on nodes, we have that $X\detsup_{Y} X$, it trivially follows that $\mathbf{X}\detsup_{Y}\mathbf{X}$.\\
  Transitivity:
  assume that $\mathbf{Z}\detsup_{Y} \mathbf{W}$ and $\mathbf{W}\detsup_{Y} \mathbf{X}$.
  Let $X\in \mathbf{X}$.
  Then there is $W\in \mathbf{W}$ such that $W\detsup_{Y} X$.
  There is also $Z\in \mathbf{Z}$ such that $Z\detsup_{Y} W$.
  By transitivity of $\detsup_{Y}$ on nodes, it follows that $Z\detsup_{Y} X$.
  Hence $\mathbf{Z}\detsup_{Y} \mathbf{X}$.
\end{proof}

\begin{remark}[Interventional Superiority is not an order, and it is not total]
  One may have expected interventional superiority (both on nodes and on node sets) to be a partial order in $G$.
  However, they are merely preorders.
  That is, the antisymmetry property does not hold.
  To see this for $\detsup_{Y}$ on nodes, just notice that, if $X, W \notin \An(Y)$, then trivially $X\detsup_{Y} W$ and $W\detsup_{Y} X$, no matter what $X$ and $W$ are.
  For node sets, consider the case where $X\subsetneq W$, but the best intervention lies in $X$.
  Then $X\detsup_{Y} W$ and $W\detsup_{Y}X$, even though $\mathbf{X}\ne \mathbf{W}$.\\
  Notice also that $\detsup_{Y}$ on nodes cannot be a total preorder: just consider the graph $A_{1}\rightarrow Y \leftarrow A_{2}$.
  Once can have an SCM $\C$ in which intervening on $A_{1}$ can lead to larger values of $Y$ than interventions on $A_{2}$.
  But one can also switch the structural assignments assignments of $\C$, which would lead to the opposite conclusion.
  This example also shows that $\detsup_{Y}$ on node sets also cannot be a total preorder.
\end{remark}

\section{Proofs for The Minimal Globally Interventionally Superior Set}
\label{sec:app-proofs-mgiss}


\subsection{Uniqueness of the mGISS}
\begin{lemma}[Elements of a mGISS are not Comparable]
  \label{lemma:mgiss-elements-not-comparable}
  Let $\mathbf{A}\subseteq \mathbf{V}$ be a mGISS relative to $Y$.
  Let $X, X' \in \mathbf{A}$ and $X \ne X'$.
  Then $X' \not\detsup_{Y} X$.
\end{lemma}
\begin{proof}
  Assume $X' \detsup_{Y} X$ for the sake of contradiction.
  We will show that this implies that $\mathbf{A}\setminus X$ is also a GISS.
  That is, that for every element of $(\mathbf{V}\setminus Y)\setminus (\mathbf{A}\setminus X)$ there is an element of $\mathbf{A}\setminus X$ which is superior to it.
  Let $W\in (\mathbf{V}\setminus Y)\setminus (\mathbf{A}\setminus X)$.
  If $W = X$, then $X' \in \mathbf{A}\setminus X$ and $X' \detsup_{Y} X$.
  If $W \ne X$, then $W \in (\mathbf{V}\setminus Y)\setminus \mathbf{A}$.
  Since $\mathbf{A}$ is a GISS, we can pick $\tilde{X} \in \mathbf{A}$ such that $\tilde{X} \detsup_{Y} W$.
  In case $\tilde{X} = X$, we can choose instead $X'$.
  Indeed, since $X' \detsup_{Y} X$ and $X \detsup_{Y} W$, we have by transitivity of $\detsup_{Y}$ (\Cref{prop:sup_is_preorder}) that $X' \detsup_{Y} W$.
  This shows that $\mathbf{A}\setminus X \subseteq \mathbf{A}$ is a GISS, contradicting the minimality of $\mathbf{A}$.
\end{proof}


\uniquenessmgiss*
\begin{proof}
  Let $\mathbf{A}$ and $\mathbf{B}$ be minimal globally interventionally superiot sets of $G$ with respect to $Y$.
  Assume, for the sake of contradiction, that $\mathbf{B}\ne \mathbf{A}$.
  By minimality of $\mathbf{A}$, we have $\mathbf{B}\not\subseteq \mathbf{A}$, so that $\mathbf{B} \setminus \mathbf{A} \ne \emptyset$.
  Let $X\in \mathbf{B}\setminus \mathbf{A}$.
  In particular, $X\in (\mathbf{V}\setminus Y) \setminus \mathbf{A}$.
  Hence, $\exists Z\in \mathbf{A} \suchthat Z\detsup_{Y} X$.
  Either $Z \in \mathbf{A}\cap \mathbf{B}$ or $Z\in \mathbf{A}\setminus \mathbf{B}$.
  If $Z\in \mathbf{A}\setminus \mathbf{B}$, then in particular $Z\in (\mathbf{V}\setminus Y) \setminus \mathbf{B}$.
  Since $\mathbf{B}$ is a GISS, there is $X'\in \mathbf{B}$ such that $X' \detsup_{Y} Z$.
  By transitivity of $\detsup_{Y}$ (\Cref{prop:sup_is_preorder}), it follows that $X' \detsup_{Y} X$.
  Similarly, if $Z \in \mathbf{A} \cap \mathbf{B}$, one again has two elements $Z$ and $X$ of $B$ such that $Z\detsup_{Y} X$.
  In both cases, this contradicts the assumption that $B$ is a GISS, as per \Cref{lemma:mgiss-elements-not-comparable}.
\end{proof}


\subsection{The LSCA Closure and $\Lambda$-structures}

It will be useful to know that, in order to show that a node belongs to $\Linfty(\mathbf{U})$, it suffices to prove that it belongs to the LSCA closure of a subset of $\mathbf{U}$.
We show by induction that this is indeed the case.
\begin{lemma}
  \label{lemma:subsets-and-closure}
  If $\mathbf{U}'\subseteq \mathbf{U}$, then $\Linfty(\mathbf{U}')\subseteq \Linfty(\mathbf{U})$. $\mathbf{U}'\subseteq \mathbf{U}$
\end{lemma}
\begin{proof}
  Recall that $\Linfty(\mathbf{U})=\mathcal{L}^{i}(\mathbf{U})$ for some $i\in \mathbb{N}$.
  We will show the result by induction on $i\in \mathbb{N}$.
  The base case holds trivially: $\mathcal{L}^{0}(\mathbf{U}')=\mathbf{U}'\subseteq \mathbf{U} = \mathcal{L}^{0}(\mathbf{U})$.
  Now assume that $\mathcal{L}^{i}(\mathbf{U}') \subseteq \mathcal{L}^{i}(\mathbf{U})$ for a given $i\in \mathbb{N}$ (induction hypothesis).
  Let $V\in \LSCA(\mathcal{L}^{i}(\mathbf{U}'))$.
  Then there are paths $V\dashrightarrow X$, $V\dashrightarrow Y$ with $X,Y\in \mathcal{L}^{i}(\mathbf{U}')$ not containing $Y$ and $X$, respectively.
  But $X,y$ are also in $\mathcal{L}^{i}(\mathbf{U})$, so that $V\in \LSCA(\mathcal{L}^{i}(\mathbf{U}))$.
  Then $\LSCA(\mathcal{L}^{i}(\mathbf{U})) \subseteq \LSCA(\mathcal{L}^{i}(\mathbf{U}))$.
  Using once more the induction hypothesis, it follows that $\mathcal{L}^{i+1}(\mathbf{U}') = \LSCA(\mathcal{L}^{i}(\mathbf{U}')) \cup \mathcal{L}^{i}(\mathbf{U}') \subseteq \LSCA(\mathcal{L}^{i}(\mathbf{U})) \cup \mathcal{L}^{i}(\mathbf{U}) = \mathcal{L}^{i+1}(\mathbf{U})$.
\end{proof}

\begin{lemma}
  \label{lemma:lsca_lambda_struct}
  Let $\mathbf{U}\subseteq \mathbf{V}$.
  If $V\in \LSCA(\mathbf{U})\setminus \mathbf{U}$, then $V$ forms a $\Lambda$-structure over $(\mathbf{U},\mathbf{U})$.
\end{lemma}
\begin{proof}%
  Let $V\in \LSCA(\mathbf{U})\setminus \mathbf{U}$.
  By \Cref{def:lsca_set}, there are distinct $U, U' \in \mathbf{U}$ for which there are paths $\pi\colon V\dashrightarrow U$ and $\pi'\colon V\dashrightarrow U'$ whose interiors do not intersect $\{U, U'\}$.
  \begin{minipage}[c]{0.65\textwidth}
    Now, let $W$ (respectively $W'$) be the first element in $\pi$ (respectively $\pi'$) in $\mathbf{U}$.
    Notice that $W\ne W'$, otherwise $W=W'$ would be in $\SCA(U,U')$ and be reachable from $V$, so that $V$ would not be a minimal element of $\SCA(U,U')$. This would contradict $V\in \LSCA(U,U')$.
    Similarly, the paths $\pi\vert_{W}\colon V\dashrightarrow W$, $\pi'\vert_{W'}\colon V\dashrightarrow W'$ resulting from restricting $\pi$ cannot have interior intersections: such an intersection node $\tilde{V}$ would be an SCA of $U,U'$ reachable from $V$, so that $V\notin \LSCA(U,U')$ --- again a contradiction.
    Therefore, $V$ forms a $\Lambda$-structure over $W,W'$.
  \end{minipage}
  \begin{minipage}[c]{0.3\textwidth}
    \vspace{-0cm}
    \centering
    \begin{tikzpicture}[mynode/.style={circle,draw=black,fill=white,inner sep=0pt,minimum size=0.8cm},>=latex]
      \node (v) at (0,1) {$V$};
      \node (w) at (-0.5,0) {$W$};
      \node (w') at (0.5,0) {$W'$};
      \node (u) at (-0.5,-1) {$U$};
      \node (u') at (0.5,-1) {$U'$};

      \draw[->, dotted] (v) -- (w);
      \draw[->, dotted] (v) -- (w');
      \draw[->, dotted] (w) -- (u);
      \draw[->, dotted] (w') -- (u');

      \node[fit=(w) (u'), draw=red, rounded corners, inner sep=6pt, dashed] (Urect) {};
      \node[right] at (Urect.east) {$\color{red}\mathbf{U}$};
    \end{tikzpicture}
  \end{minipage}\\
\end{proof}

\simplecharactlsca*
\begin{proof}
  Proof of $\subseteq$:  %
  If $\Linfty(\mathbf{U}) = \mathbf{U}$, then the result is trivially true.
  We assume from now on that $\Linfty(\mathbf{U})\supseteq \mathbf{U}$.
  We will prove that $V\in\Linfty(\mathbf{U}) \Rightarrow V\in \Lambda(\mathbf{U},\mathbf{U})$ by induction with respect to a chosen strict reverse topological order $<$ (\emph{i.e.} $V' \in \An(V)\setminus \{V\} \Rightarrow V < V'$).
  The base case is $V_{0}\in \mathbf{U}$, since an element of $\mathbf{U}$ will be the first element of $\Linfty(\mathbf{U})$ for any chosen $<$.
  In this case, we can simply take the trivial paths $\pi=\pi'=(V_{0})$.
  Then $V_{0}\in \Lambda(\mathbf{U},\mathbf{U})$.
  Now, assume that $V\in \Linfty(\mathbf{U})\setminus\mathbf{U}$ and that the implication holds for all $W\in \Linfty(\mathbf{U})$ such that $W<V$ (induction hypothesis).
  Let $W,W'$ be\footnotemark distinct elements of $\Linfty(\mathbf{\mathbf{U}})$ such that $V\in \LSCA(W,W')$.
  \footnotetext{Such $W, W'$ must exist by the definition of $\Linfty(\mathbf{U})$ whenever $\Linfty(\mathbf{U})\supseteq \mathbf{U}$.}
  In particular, $W,W'<V$.
  By \Cref{lemma:lsca_lambda_struct} applied to $\{W,W'\}$, there are paths $V \overset{\alpha}{\dashrightarrow} W$, $V \overset{\alpha'}{\dashrightarrow} W'$ intersecting only at $V$.
  Furthermore, by the induction hypothesis we have that $W,w'\in \Lambda(\mathbf{U},\mathbf{U})$, so that there are paths $W \overset{\pi_{1}}{\dashrightarrow} U_{1}$, $W \overset{\pi_{2}}{\dashrightarrow} U_{2}$, $W' \overset{\pi'_{1}}{\dashrightarrow} U'_{1}$, $W' \overset{\pi'_{2}}{\dashrightarrow} U'_{2}$ such that $U_{1}, U_{2}, U'_{1}, U'_{2} \in \mathbf{U}$, $\pi_{1}\cap \pi_{2} = \{W\}$ and $\pi'_{1}\cap \pi'_{2} = \{w'\}$.

  \begin{minipage}[c]{0.65\textwidth}
    Let $\mathbf{S} = (\alpha \cup \pi_{1} \cup \pi_{2}) \cap (\alpha' \cup \pi'_{1} \cup \pi'_{2})$ and $\trianglelefteq$ be a chosen topological order.
    If $\mathbf{S} = \emptyset$, we can just take $\gamma = \pi_{1}\circ \alpha\colon V\dashrightarrow U_{1}$ and $\gamma' = \pi'_{1}\circ \alpha'\colon V\dashrightarrow U'_{1}$ to form a $\Lambda$-structure for $V$ over $(\mathbf{U},\mathbf{U})$.
    Assume from now on that $\mathbf{S}\ne \emptyset$.
    Let $S$ be the first element of $\mathbf{S}$ with respect to $\trianglelefteq$.
    Since $\alpha \cap \alpha' = \emptyset$, there are three options: either (i) $S\in \pi_{i} \cap \alpha'\setminus\{W'\}$ for some $i$; (ii) $S\in \pi'_{i} \cap \alpha\setminus\{W\}$ for some $i$; or (iii) $S\in \pi_{i} \cap \pi'_{j}$ for some $i,j$.
    By symmetry, we can restrict ourselves to the cases (i) and (iii): the argument for (i) will also hold for (ii).
    In both cases (i) and (ii) we have $S \in \pi_{i}$ for some $i\in\{1,2\}$.
    Without loss of generality, assume $s\in \pi_{2}$.
  \end{minipage}
  \begin{minipage}[c]{0.35\textwidth}
    \vspace{-0cm}
    \centering
    \begin{tikzpicture}[mynode/.style={fill=white,minimum size=0.1cm},>=latex]
      \node[mynode] (v) at (0,1) {$V$};
      \node[mynode] (w) at (-0.75,0) {$W$};
      \node[mynode] (w') at (0.75,0) {$W'$};
      \node[mynode] (u1) at (-1.5,-1.5) {$U_{1}$};
      \node[mynode] (u2) at (-0.25,-1.5) {$U_{2}$};
      \node[mynode] (u'1) at (0.25,-1.5) {$U'_{1}$};
      \node[mynode] (u'2) at (1.5,-1.5) {$U'_{2}$};
      \node (s) at (0,-0.65) {$S$};

      \draw[->, dotted, color=red] (v) -- node[pos=0.9,above,inner sep=8pt] {\tiny$\gamma_{1}$} (w);
      \draw[->, dotted, color=blue] (v) -- node[pos=0.9,above,inner sep=8pt] {\tiny$\gamma_{2}$} (w');
      \draw[->, dotted] (w) -- (u1);
      \draw[->, dotted, color=red] (w) -- (s);
      \draw[->, dotted] (s) -- (u2);
      \draw[->, dotted] (w') -- (s);
      \draw[->, dotted, color=red] (s) -- (u'1);
      \draw[->, dotted, color=blue] (w') -- (u'2);

    \end{tikzpicture}
  \end{minipage}
  For case (iii), assume, also without loss of generality, that $s\in \pi'_{1}$.
  If furthermore $s\ne W'$, we can construct the following two paths with no non-trivial intersections:
  \begin{equation}
    \label{eq:caseii_composite_paths}
    \begin{cases}
      \gamma_{1} = \pi_{1}'\vert^{S}\circ\pi_{2}\vert_{s}\circ\alpha: V \dashrightarrow U'_{1}\\
      \gamma_{2} = \pi_{2}'\circ\alpha': V \dashrightarrow U'_{2}
    \end{cases}
  \end{equation}
  To see that these paths have non non-trivial intersections, start by noticing that, by definition of $S$, there is no intersection between $\pi_{2}$ and $\pi_{2}'$ at nodes $A \triangleleft S$, so that $\pi_{2}\vert_{S} \cap \pi_{2}' = \emptyset$.
  And since $\pi_{1}'\cap\pi_{2}' = \{W'\}$ and $S \ne W'$, we have $\pi_{1}'\vert^{S}\cap\pi_{2}' = \emptyset$.
  Finally, $\pi_{2}\cap \alpha'=\pi_{2}'\cap\alpha=\pi_{1}\cap \alpha'=\emptyset$, since otherwise there would be elements of $S$ which are ancestors of $s$.
  Notice that this argument still holds if $S=W$, in which case $\gamma_{1}$ reduces to $\pi'_{1}\vert^{W} \circ \alpha$.
  This shows that $V\in \Lambda(\mathbf{U},\mathbf{U})$ for case (iii), in case $S\ne W'$.
  If instead $S=W'$, we can simply choose paths similar to those for the case $S=W$ (just changing the numbers and the prime) as follows: $\gamma_{1} = \pi_{1} \circ \alpha$ and $\gamma_{2} = \pi_{2}\vert^{W'} \circ \alpha'$.

  \begin{minipage}[c]{0.49\textwidth}
    \vspace{-0cm}
    \centering

     \begin{tikzpicture}[mynode/.style={fill=white,minimum size=0.1cm},>=latex]
      \node[mynode] (v) at (0,1) {$V$};
      \node[mynode] (w) at (-0.75,0) {$W$};
      \node[mynode] (w') at (0.75,0) {$W'$};
      \node[mynode] (u1) at (-1.5,-1.5) {$U_{1}$};
      \node[mynode] (u2) at (-0.50,-1.5) {$U_{2}$};
      \node[mynode] (u'1) at (0.25,-1.5) {$U'_{1}$};
      \node[mynode] (u'2) at (1.5,-1.5) {$U'_{2}$};

      \draw[->, dotted, color=red] (v) -- node[pos=0.9,above,inner sep=8pt] {\tiny$\gamma_{1}$} (w);
      \draw[->, dotted, color=blue] (v) -- node[pos=0.9,above,inner sep=8pt] {\tiny$\gamma_{2}$} (w');
      \draw[->, dotted] (w) -- (u1);
      \draw[->, dotted] (w) -- (u2);
      \draw[->, dotted] (w') -- (w);
      \draw[->, dotted, color=red] (w) -- (u'1);
      \draw[->, dotted, color=blue] (w') -- (u'2);
    \end{tikzpicture}
  \end{minipage}
  \begin{minipage}[c]{0.49\textwidth}
    \centering
    \begin{tikzpicture}[mynode/.style={fill=white,minimum size=0.1cm},>=latex]
      \node[mynode] (v) at (0,1) {$V$};
      \node[mynode] (w) at (-0.75,0) {$W$};
      \node[mynode] (w') at (0.75,0) {$W'$};
      \node[mynode] (u1) at (-1.5,-1.5) {$U_{1}$};
      \node[mynode] (u2) at (-0.50,-1.5) {$U_{2}$};
      \node[mynode] (u'1) at (0.25,-1.5) {$U'_{1}$};
      \node[mynode] (u'2) at (1.5,-1.5) {$U'_{2}$};

      \draw[->, dotted, color=red] (v) -- node[pos=0.9,above,inner sep=8pt] {\tiny$\gamma_{1}$} (w);
      \draw[->, dotted, color=blue] (v) -- node[pos=0.9,above,inner sep=8pt] {\tiny$\gamma_{2}$} (w');
      \draw[->, dotted, color=red] (w) -- (u1);
      \draw[->, dotted] (w) -- (w');
      \draw[->, dotted, color=blue] (w') -- (u2);
      \draw[->, dotted] (w') -- (u'1);
      \draw[->, dotted] (w') -- (u'2);
    \end{tikzpicture}
  \end{minipage}


  \begin{minipage}[c]{0.65\textwidth}
    We now turn to case (i), where $S\in \pi_{2}\cap\alpha'\setminus\{W\}$.
    Construct the paths:
    \begin{equation}
      \begin{cases}
        \gamma_{1} = \pi_{1}\circ\alpha: V \dashrightarrow U_{1}\\
        \gamma_{2} = \pi_{2}\vert^{s}\circ\alpha'\vert_{s}: V \dashrightarrow U_{2}
      \end{cases}
    \end{equation}
    Notice that $\alpha\cap\pi_{2}\vert^{S}=\emptyset$, otherwise there would be a cycle in the DAG.
    Also, $\pi_{1}\cap\alpha'\vert_{S}=\emptyset$ by definition of $S$.
    And trivially $\pi_{1}\cap\pi_{2}\vert^{S}=\emptyset$ and $\alpha\cap\alpha'=\{V\}$.
  \end{minipage}
  \begin{minipage}[c]{0.35\textwidth}
    \vspace{-0cm}
    \centering
    \begin{tikzpicture}[mynode/.style={fill=white,minimum size=0.1cm},>=latex]
      \node[mynode] (v) at (0,1.25) {$V$};
      \node[mynode] (w) at (-0.5,0.65) {$W$};
      \node[mynode] (w') at (0.75,-0.25) {$W'$};
      \node[mynode] (u1) at (-1.5,-1.25) {$U_{1}$};
      \node[mynode] (u2) at (-0.25,-1.25) {$U_{2}$};
      \node[mynode] (u'1) at (0.25,-1.25) {$U'_{1}$};
      \node[mynode] (u'2) at (1.5,-1.25) {$U'_{2}$};
      \node (s) at (0.5,0.45) {$S$};

      \draw[->, dotted, color=red] (v) -- (w);
      \draw[->, dotted, color=blue] (v) -- node[pos=0.9,above,inner sep=8pt] {\tiny$\gamma_{2}$} (s);
      \draw[->, dotted] (w) -- (s);
      \draw[->, dotted, color=blue] (s) -- (u2);
      \draw[->, dotted] (s) -- (w');
      \draw[->, dotted, color=red] (w) -- node[pos=0.5,above,inner sep=10pt] {\tiny$\gamma_{1}$} (u1);
      \draw[->, dotted] (w') -- (u'2);
      \draw[->, dotted] (w') -- (u'1);

    \end{tikzpicture}
  \end{minipage}

  It follows that $\gamma_{1}$ and $\gamma_{2}$ intersect only trivially, so that $(v,\gamma_{1},\gamma_{2})$ forms a \(\Lambda\)-structure over $(\mathbf{U},\mathbf{U})$.
  \partialqed{\subseteq}

  Proof of $\supseteq$: %
  Let $V\in\Lambda(\mathbf{U},\mathbf{U})$.
  Then, there is a pair of nodes $U,U' \in \mathbf{U}$ over which $V$ forms \(\Lambda\)-structures.
  We are going to show that $V\in\Linfty(\{U,U'\})$.
  Let $\mathbf{L}$ be the set of all the \(\Lambda\)-structures $\lambda_{i} = (V,\pi_{i}\colon V\dashrightarrow U,\pi_{i}'\colon V\dashrightarrow U')$ over $(U, U')$.
  Let $A$  be the set of nodes in $\Linfty(\{U,U'\})$ which belong to some $\pi_{i}$.
  Formally, $A = \{a \in \Linfty(\{U,U'\}) \colon \exists i \suchthat \lambda_{i} \in \mathbf{L}, a \in \pi_{i} \}\setminus \{V\}$.
  Let $\dot{a}$ be the first element of $A$ with respect to a chosen topological order $\trianglelefteq$.
  Denote by $\Pi'(\dot{a})$ the set of paths $\pi_{i}'$ belonging to some $\Lambda$-structure $(V, \pi_{i}', \pi_{i})$ in $\mathbf{L}$ such that $\pi_{i}$ contains $\dot{a}$.
  Let $A'(\dot{a}) = \{a' \in \Linfty(\{U,U'\}) \colon \exists \pi'_{i} \in \Pi'(\dot{a}) \suchthat a'\in  \pi'_{i}\}$.
  \begin{minipage}[c]{0.69\textwidth}
    Furthermore, let\footnotemark $\dot{a}'$ be the first element of $A'(\dot{a})$ with respect to $\trianglelefteq$.
    Denote by $(V, \dot{\pi}, \dot{\pi}')$ a $\Lambda$-structure of $\mathbf{L}$ such that $a\in\dot{\pi}$ and $a'\in \dot{\pi}'$.
    Notice that $\dot{a} \ne \dot{a}'$ and $\dot{\pi}\vert_{\dot{a}}\cap \dot{\pi}'\vert_{\dot{a}'} = \{V\}$, by definition of $\Lambda$-structure.
    In particular, $v \in \SCA(\dot{a}, \dot{a}')$.
    Suppose, for the sake of contradiction, that there is $\tilde{V} \in \SCA(\dot{a}, \dot{a}')$ such that $\tilde{V}$ is reachable from $V$.
    Then there is $\lambda = (V, \gamma, \gamma')$ in $\mathbf{L}$ such that $\tilde{V}, \dot{a} \in \gamma$.
    But $\tilde{V} \trianglelefteq \dot{a}$, contradicting minimality of $\dot{a}$.
    Hence $V \in \LSCA(\dot{a}, \dot{a}')$.
    Finally, since $\dot{a}, \dot{a}' \in \Linfty(\mathbf{U})$, it follows that $V\in \Linfty(\mathbf{U})$.
  \end{minipage}
    \footnotetext{Notice that $A'$ (and $A$) are not empty (at least one of $\{U,U'\}$ is in $A'$ (and $A$).}
  \begin{minipage}[c]{0.3\textwidth}
    \centering
    \begin{tikzpicture}[mynode/.style={fill=white,minimum size=0.1cm},>=latex]
      \node[mynode] (v) at (0,1) {$V$};
      \node[mynode] (vt) at (0.75,1) {$\tilde{V}$};
      \node[mynode] (ad) at (-0.75,0) {$\dot{a}$};
      \node[mynode] (ad') at (0.75,0) {$\dot{a}'$};
      \node[mynode] (u) at (-0.75,-1.0) {$U$};
      \node[mynode] (u') at (0.75,-1.0) {$U'$};

      \draw[->, dotted] (v) -- (vt);
      \draw[->, dotted] (v) -- (ad);
      \draw[->, dotted] (v) -- (ad');
      \draw[->, dotted] (vt) -- (ad);
      \draw[->, dotted] (vt) -- (ad');
      \draw[->, dotted] (ad) -- (u);
      \draw[->, dotted] (ad') -- (u');
    \end{tikzpicture}
  \end{minipage}



 \partialqed{\supseteq}
\end{proof}



\subsection{The LSCA Closure is the mGISS}
\begin{lemma}
  \label{lemma:two-paths-implies-in-closure}
  Let $B\in \mathbf{V}$.
  Assume there are nodes $Z, W$ which are reachable from $B$ with paths whose interiors do not intersect $\Linfty(\{Z, W\}))$.
  Then $B \in \Linfty(\{Z, W\})$.
\end{lemma}
\begin{proof}
  Notice that, since $Z,W\in \Linfty(\{Z, W\})$, then $B\in \SCA(Z, W)$.
  If there are no SCAs of $(Z,W)$ reachable from $B$, then $B \in \LSCA(Z,W)$.
  Assume from now on that there are SCAs of $(Z,W)$ reachable from $B$.
  Let $S = \{B\} \cup \{\tilde{B} \in \SCA(Z,W) \colon \tilde{B}\mathrm{~is~reachable~from~}B\}$.
  Order $S$ with a chosen reverse topological order $\le$, and denote its elements by $\tilde{B}_{i}$, where $i \le j$ iff $\tilde{B}_{i} \le \tilde{B}_{j}$.
  Remove from $S$ the elements $\tilde{B}_{j}$ for which every pair of paths from $\tilde{B}_{j}$ to $Z$ and $W$ intersects at some $\tilde{B}_{k}, k < j$.
  Denote the remaining nodes by $B_{i}, i\in {1, \ldots, M+1}$.
  Let $\{L_{k}\}_{k}, k\in \{1,\ldots, K\}$ be a layering of $\{B_{i}\}_{i}$.
  Note that $B_{1} \in L_{1}$.
  We will prove by induction on the layers that all the $B_{i}$ are in $\Linfty(\{Z,W\})$.\\
  \begin{minipage}[c]{0.74\textwidth}
    First, notice that all $A \in L_{1}$ must be in $\LSCA(Z,W)$; otherwise, there would be an SCA of $Z,W$ reachable from $A$ element of $S$ reachable by $A$ (and thus also from $B$) and (from \Cref{lemma:lsca_lambda_struct}) with non-intersecting paths to $Z$ and $W$ --- hence, an element of $\{B_{i}\}$ reachable from $A$.
    This contradicts that $A\in L_{1}$.
    Hence, $L_{1} \subseteq \Linfty(\{Z,W\})$.
    This establishes the base case.
    Let $k\in \{2,\ldots, K\}$.
    Assume that $L_{k-1} \subseteq \Linfty(\{Z,W\})$ (induction hypothesis).
    Let $A \in L_{k}$.
    Let $C, D$ be the first nodes in $\{B_{i}\}$ to which there are paths from $A$ not intersecting in $\{B_{i}\}$ (which must exist by construction of $\{B_{i}\}$).
    Clearly $A\in \SCA(C,D)$.
    But also $A \in \LSCA(C,D)$: if there was $\tilde{A} \in \LSCA(C,D)$ reachable from $A$, then in particular $\tilde{A} \in \{B_{i}\}$, contradicting the minimality of $C$ and $D$ with respect to $\le$.
    Since, by the induction hypothesis, $C, D \in \Linfty(\{Z,W\})$, then also $A\in \Linfty(\{Z,W\})$.
    Finally, since all $B_{i}\in \Linfty(\{Z,W\})$, by assumption $B$ has paths to $Z$ and $W$ never intersecting $\{B_{i}\}$, so that in particular $B\in \{B_i\}$ (and in fact $B = B_{M+1}$ and $B \in L_{K}$).
    Hence $B\in\Linfty(\{Z,W\})$.
  \end{minipage}
  \begin{minipage}[c]{0.25\textwidth}
    \centering
    \begin{tikzpicture}[mynode/.style={inner sep=0pt,minimum size=0.4cm},>=latex] %
      \node[mynode] (b) at (-0.5,2.5) {$B$};
      \node[mynode] (b1) at (-0.5,0.25) {$B_{1}$};
      \node[mynode] (b2) at (0.5,0.5) {$B_{2}$};
      \node[mynode] (b3) at (-0.5,1.0) {$B_{3}$};
      \node[mynode] (bt) at (0.5,1.5) {$\tilde{B}_{4}$};
      \node[mynode] (z) at (-0.75,-0.5) {$Z$};
      \node[mynode] (w) at (0.75,-0.5) {$W$};

      \draw[->, dotted] (b.south west) to [out=215,in=135,looseness=1.0] (z.north west);
      \draw[->, dotted] (b.east) to [out=0,in=45,looseness=1.0] (w.north east);
      \draw[->, dotted] (b) -- (b3);
      \draw[->, dotted] (b) -- (bt);
      \draw[->, dotted] (b1) -- (z);
      \draw[->, dotted] (b1) -- (w);
      \draw[->, dotted] (b2) -- (z);
      \draw[->, dotted] (b2) -- (w);
      \draw[->, dotted] (b3) -- (b1);
      \draw[->, dotted] (b3) -- (w);
      \draw[->, dotted] (bt.south west) to [out=215,in=135,looseness=1.0] (b2.north west);
      \draw[->, dotted] (bt.south east) to [out=315,in=45,looseness=1.0] (b2.north east);
    \end{tikzpicture}

  \end{minipage}
\end{proof}

\begin{lemma}
  \label{lemma:one_node_reachable}
  Let $B\in\An(Y)$ and $B\notin \LinftyY$.
  Then there is exactly one node $Z \in \LinftyY$ reachable from $B$ by paths whose interiors do not contain elements from $\LinftyY$.
\end{lemma}
\begin{proof}
    There must be at least one node in $\LinftyY$ reachable from $B$ by paths not containing interior elements from $\LinftyY$: since $\PA(Y) \subseteq \LinftyY$ and $B \in \An(Y)$, there are paths from $B$ to $Y$ crossing $\LinftyY$ (in fact, paths must at least intersect $\PA(Y)$).
    Choose one such path $\pi\colon B \dashrightarrow Y$.
    Let $Z$ be the first element of $\LinftyY$ in $\pi$.
    Then the path ${\pi\vert}_{Z}\colon B \dashrightarrow Z$ obtained from $\pi$ by truncating it at $Z$ has no interior nodes in the closure $\LinftyY$.
    Furthermore, if there would be a second path from $B$ to $W \in \LinftyY \setminus \{Z\}$ containing no interior nodes from the closure, then, by \Cref{lemma:two-paths-implies-in-closure}, $B$ would be in $\Linfty(\{Z,W\})$ and thus in $\LinftyY$ --- contradiction.
    This establishes uniqueness.
\end{proof}

\begin{corollary}
  \label{cor:all_paths_through_Z}
  Under the assumptions of \Cref{lemma:one_node_reachable}, all paths from $B$ to $Y$ must go through $Z$.
\end{corollary}
\begin{proof}
  If there was a path from $B$ to $Y$ not containing $Z$, it would have to go through a parent $A$ of $Y$.
  But the first element of $\LinftyY$ in this path (perhaps $A$ itself) would contradict the uniqueness of $Z$ from \Cref{lemma:one_node_reachable}.
\end{proof}

The following lemma relates blocked unrolled assigments with atomic interventions, and will be used to prove \Cref{thm:superiority_of_lsca}.
\begin{lemma}
  \label{lemma:blocking_vs_intervening}
  Let $X \in \mathbf{V}$ and $Y\in \mathbf{V} \cup \mathbf{N}$.
  Then $\bar{f}_{Y}[X](x, \mathbf{n}) = \bar{f}_{Y}^{\dop(X=x)}(\mathbf{n})$.
\end{lemma}
\begin{proof}
  Let $X$ be an endogenous variable.
  We want to prove that the expression holds for any variable $Y$.
  We will prove this by induction.
  Let $\trianglelefteq$ be a topological order on the nodes of $G^{*}$.
  Note that the first elements with respect to this order are the exogenous variables, \emph{i.e.} $N \trianglelefteq Z$ whenever $N \in \mathbf{N}$ and $Z \in \mathbf{V}$.
  The result is true for the exogenous variables.
  Indeed, for $Y \in \textbf{N}$ we have that
  $\bar{f}_{Y}[X](x, \mathbf{n}) = \bar{f}_{Y}(\mathbf{n}) = Y = \bar{f}_{Y}^{\dop(X=x)}(\mathbf{n})$, since $Y \notin \De(X) \cup \{X\}$ and $Y$ is exogenous (both in the pre- and post-intervention structural causal models).
  This establishes the base case of the induction.
  Now let $Y$ be endogenous.
  For the inductive step, we will prove that, if the result is true for the parents $\PA_{G^{*}}(Y)$ of $Y$ in $G^{*}$ (induction hypothesis), then it is also true for $Y$.
  Assume the antecedent (induction hypothesis).
  There are three possibilities: $Y \in \De(X)\setminus\{X\}$, $Y = X$ or $Y \notin \De(X)$.
  In case $Y \in \De(X)\setminus\{X\}$:
  \begin{equation}
    \begin{split}
    \bar{f}_Y[X](x, \mathbf{n}) &= f_Y(\bar{f}_{\PA(Y)}[X](x, \mathbf{n}), n_{Y}).\\
      & \overeq{I.H.} f_Y(\bar{f}_{\PA(Y)}^{\dop(X=x)}(\mathbf{n}), n_{Y})\\
                                & = f_Y^{\dop(X=x)}(\bar{f}_{\PA(Y)}^{\dop(X=x)}(\mathbf{n}), n_{Y})\\
      & \overeq{def} \bar{f}_{Y}^{\dop(X=x)}(\mathbf{n}),
    \end{split}
  \end{equation}
  where in the third equality we used that $f^{\dop(X=x)}_{Y}=f_{Y}$.
  If instead $Y = X$, then one simply has $\bar{f}_{Y}[X](x, \mathbf{n}) = \bar{f}_{X}[X](x, \mathbf{n}) \overeq{def} x$.
  Furthermore, $\bar{f}^{\dop(X=x)}_{Y}(\mathbf{n}) = \bar{f}^{\dop(X=x)}_{X}(\mathbf{n}) = f^{\dop(X=x)}_{X}(\mathbf{n}) = x$, where the second equality holds simply because $X$ has no non-exogenous parents in the post-intervention graph.
  Finally, if $Y \notin \De(X)$, then $\bar{f}_{Y}[X](x, \mathbf{n}) = \bar{f}_{Y}(\mathbf{n})$ by definition.
  And $\bar{f}_{Y}^{\dop(X=x)}(\mathbf{n}) = f_{Y}^{\dop(X=x)}(\bar{f}^{\dop(X=x)}_{\PA(Y)}(\mathbf{n}), n_{Y}) = f_{Y}(\bar{f}_{\PA(Y)}(\mathbf{n}), n_{Y})$, where in the last equality we used that $X\notin \An(Y) \Rightarrow \bar{f}^{\dop(X=x)}_{\PA(Y)}(\mathbf{n}) = \bar{f}_{\PA(Y)}(\mathbf{n})$.
  This establishes the inductive step: if the results holds for the first $j \ge \abs{\mathbf{N}}$ variables with respect to $\trianglelefteq$, then it also holds for the variable $j + 1$, since its parents are among the first $j$ variables.
\end{proof}

The following lemma shows how one can chain (blocked) unrolled assignments when there is a node $Z$ present in all paths from the blocking node $B$ to $Y$.
This result is consistent with the intuition that, if all paths from $B$ to $Y$ must go through $Z$, then knowing the value of $Z$ is enough to compute $Y$.
\begin{lemma}
  \label{lemma:chaining_b_and_z}
  If all paths from $B$ to $Y$ must include $Z$, then $\bar{f}_{Y}[B](b, \mathbf{n}) = \bar{f}_{Y}[Z](\bar{f}_{Z}[B](b, \mathbf{n}), \mathbf{n})$.
\end{lemma}
\begin{proof}
  Let $S$ be the poset whose elements are all the descendants $A$ of $B$ for which all paths from $B$ to $A$ must go through $Z$, and the partial order is a topological order $\trianglelefteq$.
  Denote the elements of $S$ by $W_{i}$, where $i \in \{0, \ldots, m-1\}$ corresponds to the position of $W_{i}$ in the order $\trianglelefteq$.
  We will prove the result by induction on a topological order.
  Notice that $Y\in S$.
  Thus, we can just show the result for all $W_{i}$.
  We start with the base case $W_{0}$.
  By definition:
  \begin{equation}
    \label{eq:w0_case}
      \bar{f}_{W_{0}}[Z](\bar{f}_{Z}[B](b, \mathbf{n}),\mathbf{n}) = f_{W_{0}}(\bar{f}_{\PA(W_{0})}[Z](\bar{f}_{Z}[B](b, \mathbf{n}), \mathbf{n}),n_{W_{0}}).
  \end{equation}
  Recall that $\PA(W_{0}) = (\PA(W_{0})_{1}, \ldots, \PA(W_{0})_{m_{0}})$.
  Hence, we want to check that $\bar{f}_{\PA(W_{0})_{i}}[Z](\bar{f}_{Z}[B](b, \mathbf{n}), \mathbf{n}) = \bar{f}_{\PA(W_{0})_{i}}[B](b, \mathbf{n})$ for all $i$, since in that case the right hand side of \Cref{eq:w0_case} becomes $f_{W_{o}}(\bar{f}_{\PA(W_{0})_{i}}[B](b, \mathbf{n}), n_{W_{0}}) \defeq \uf_{W_{0}}[B](b, \mathbf{n})$.\\
  If $\PA(W_{0})_{i} = Z$, then by definition of blocked unrolled assignment $\bar{f}_{\PA(W_{0})_{i}}[Z](\bar{f}_{Z}[B](b, \mathbf{n}), \mathbf{n}) = \bar{f}_{Z}[Z](\bar{f}_{Z}[B](b, \mathbf{n}), \mathbf{n}) = \bar{f}_{Z}[B](b, \mathbf{n})$.

  \begin{minipage}[c]{0.69\textwidth}
    If $\PA(W_{0})_{i} \ne Z$, then $\PA(W_{0})_{i}$ cannot be a descendant of $B$.
    Indeed, $W_{0}$ must have no parent that is a descendant of $B$, except maybe for $Z$.
    That is: $\PA(W_{0}) \cap \De(B) \subseteq \{Z\}$.
    Otherwise, either that parent would be in $S$ and thus equal to $W_{k}$ for some $k>0$, or it would be in $\De(B) \setminus (S \cup \{Z\})$, so that there would be a path $B \dashrightarrow W_{0}$ not crossing $Z$ --- both cases contradict the definition of $W_{0}$.
    Hence, we only need to consider the case where $\PA(W_{0})_{i} \notin \De(B)$.
    In particular, $\PA(W_{0})_{i} \notin \De(Z)$.
    Then:
  \end{minipage}
  \begin{minipage}[c]{0.3\textwidth}
    \centering
    \begin{tikzpicture}[mynode/.style={inner sep=2pt,minimum size=0cm},>=latex]
      \node[mynode] (b) at (0,1) {$B$};
      \node[mynode] (a) at (0.8,0.5) {$A$};
      \node[mynode] (m) at (-0.8,-0.1) {$M$};
      \node[mynode] (z) at (0,0) {$Z$};
      \node[mynode] (r) at (0.8,-0.1) {$R$};
      \node[mynode] (w0) at (0.8,-1) {$W_0$};
      \node[mynode] (w1) at (-0.8,-1) {$W_1$};
      \node[mynode] (w2) at (0,-1.8) {$W_2\equiv Y$};

      \draw[->] (b) -- (z);
      \draw[->] (b) -- (a);
      \draw[->] (b) -- (m);
      \draw[->] (a) -- (z);
      \draw[->] (z) -- (w0);
      \draw[->] (z) -- (w1);
      \draw[->] (z) -- (m);
      \draw[->] (r) -- (w0);
      \draw[->] (w0) -- (w2);
      \draw[->] (w1) -- (w2);

      \node[fit=(w0) (w1) (w2), draw=red, rounded corners, inner sep=1pt, dashed] (wsrect) {};
      \node[right] at (wsrect.east) {$\color{red}S$};
    \end{tikzpicture}
  \end{minipage}
  \begin{equation}
    \label{eq:no_deb_case}
    \bar{f}_{\PA(W_{0})_{i}}[Z](\bar{f}_{Z}[B](b, \mathbf{n}), \mathbf{n}) = \bar{f}_{\PA(W_{0})_{i}}(\mathbf{n}) = \bar{f}_{\PA(W_{0})_{i}}[B](b, \mathbf{n}).
  \end{equation}
  This shows the result for the base case $W_{0}$.
  Now, assume it to be true for all $W_{j}$ with $j \le k$ (induction hypothesis).
  \Cref{eq:w0_case} still holds for $W_{k+1}$.
  Now, each parent $\PA(W_{k+1})_{i}$ must either be equal to $W_{j}$ for some $j < k+1$, or not a descendant of $B$ (for the same reason as for the parents of $W_{0}$).
  In the latter case, \Cref{eq:no_deb_case} still holds for $\PA(W_{k+1})_{i}$.
  Hence, we only need to check that, for $\PA(W_{k+1})_{i} = W_{j}$ (with $j < k+1$), we have that $\bar{f}_{W_{j}}[Z](\bar{f}_{Z}[B](b, \mathbf{n})\mathbf{n}) = \bar{f}_{W_{j}}[B](b, \mathbf{n})$.
  But this is just the induction hypothesis.
\end{proof}

\lscamgiss*
\begin{proof} %
  We need to prove two results:
  \begin{enumerate}[label=(\roman*)]
    \item $\mathcal{L}^{\infty}(\PA(Y))$ is globally interventionally superior with respect to $Y$.
    That is: $\mathcal{L}^{\infty}(\PA(Y)) \detsup_{Y} \mathbf{V}\setminus\left(\mathcal{L}^{\infty}(\PA(Y)) \cup \{Y\}\right)$.
    \item Furthermore, this is the minimal set with this property.
    Namely, removing any node $B$ from $\Linfty(\PA(Y))$ would result in a set $I = \Linfty(\PA(Y)) \setminus \{B\}$ that is not interventionally superior to $\mathbf{V}\setminus \left(I \cup \{Y\}\right)$.
  \end{enumerate}
  If $\PA(Y) = \emptyset$, the theorem is vacuously true. Assume $\PA(Y) \ne \emptyset$ from now on.
  \par Proof of \emph{(i)}:
  Let $B \in \mathbf{V} \setminus \Linfty(\PA(Y))$ and $B \ne Y$. We want to show that there is $A$ in the closure $\LinftyY$ such that $A \detsup_{Y} B$. \\
  If $B$ is not an ancestor of $Y$, then trivially $\bar{f}_{Y}^{\dop(B=b)}(\mathbf{n}) = \bar{f}_{Y}(\mathbf{n})$ for all $\mathbf{n} \in R_{\mathbf{N}}$ and for all $b \in R_{B}$, so that in particular $\max_{b\in R_{B}} \bar{f}_{Y}^{\dop(B=b)}(\mathbf{n}) = \bar{f}_{Y}(\mathbf{n})$.
  Now, let $A$ be a parent of $Y$, and $a^{*} = \bar{f}_{A}(\mathbf{n})$ (\emph{i.e.} $a^{*}$ is the value that $A$ would attain if no intervention was performed).
  Then, from the definition of unrolled assignment and atomic intervention, $\bar{f}_{Y}^{\dop(A=a^{*})}(\mathbf{n}) = \bar{f}_{Y}(\mathbf{n})$.
  Thus, $\max_{a \in R_{A}} \bar{f}_{Y}^{\dop(A=a)}(\mathbf{n}) \ge \bar{f}_{Y}(\mathbf{n}) = \max_{b\in R_{B}} \bar{f}_{Y}^{\dop(B=b)}(\mathbf{n})$. That is, $A \detsup_{Y} B$. \\
  Assume from now on that $B$ is an ancestor of $Y$.
  From \Cref{lemma:one_node_reachable} there is one and only one node $Z \in \LinftyY$ reachable from $B$ by paths not containing intermediate elements from $\LinftyY$.
  Let $z^{*} \in \argmax_{z\in R_{Z}} [\bar{f}_{Y}^{\dop(Z=z)}(\mathbf{n})]$.
  Further, Let $b \in R_{B}$.
  From \Cref{lemma:chaining_b_and_z}, we have that $\bar{f}_{Y}[B](b, \mathbf{n}) = \bar{f}_{Y}[Z](\bar{f}_{Y}[B](b, \mathbf{n}), \mathbf{n})$, which of course is at most $\bar{f}_{Y}[Z](z^{*}, \mathbf{n})$.
  Finally, \Cref{lemma:blocking_vs_intervening} allows us to relate this to a post-intervention unrolled assignment as $\bar{f}_{Y}[Z](z^{*}, \mathbf{n}) = \bar{f}^{\dop(Z=z)}_{Y}(\mathbf{n})$.
  This shows that $\max_{b\in R_{B}}\bar{f}_{Y}^{\dop(B=b)}(\mathbf{n}) \le \max_{z\in R_{Z}}\bar{f}_{Y}^{\dop(Z=z)}(\mathbf{n}) $, so that $Z \detsup_{Y} B$.

  \partialqed{(i)}

  \par Proof of \emph{(ii)}:
  We want to show that, for any causal graph $G$ and node $Y$ from $G$, removing any node\footnotemark from $\LinftyY$ will result in a set $I$ for which there is an SCM (with causal graph $G$) such that $I$ is not interventionally superior to $\overline{I} \setminus \{Y\}$, \emph{i.e.} $I \not\detsup_{Y} \overline{I} \setminus \{Y\}$.
  \footnotetext{It is enough to remove a single node: if removing one node results in a non-interventionally superior set, removing more nodes could clearly never result in an interventionally superior set.}
  In other words, we want to prove that:
  \begin{equation}
    \begin{split}
      \forall &\text{ DAG }G = (\mathbf{V}, E), \forall Y\in \mathbf{V}, \forall B \in \LinftyY, \\
        &\exists \text{ SCM } \mathcal{\cC} \suchthat G^{\mathcal{\cC}}=G \ \text{ and }\  I=\LinftyY \setminus \{B\} \not\detsup_{Y} \overline{I} \setminus \{Y\}.
    \end{split}
  \end{equation}
  Let $G$ be a DAG, $Y$ be a node of $G$ and $B$ an element of the closure $\LinftyY$.
  Let also $I = \LinftyY \setminus \{B\}$.
  In particular, $B \in \overline{I} \setminus \{Y\}$.
  We will show that there is no element of $I$ which is interventionally superior to $B$, thus proving that $I \not\detsup_{Y} \overline{I} \setminus \{Y\}$.
  We will divide the proof in two cases: $B \in \PA(Y)$ and $B \in \LinftyY \setminus \PA(Y)$.\\
  Assume $B \in \PA(Y)$.
  We can construct an SCM with causal graph $G$ as follows:
  \begin{equation}
    \label{eq:scm_b_in_pay}
    \begin{cases}
      f_{Y}(\PA(Y), N_{Y}) = 2B + \heavyside\left(\sum_{W \in \PA(Y) \setminus \{B\}} W\right) + N_{Y}\\
      f_{B}(\PA(B), N_{B}) = N_{B} \cdot \left(1 - \heavyside\left(\sum_{W \in \PA(B)} W\right)\right) \\
      f_{V\ne Y,B}(\PA(V), N_{V}) = \heavyside\left(\sum_{W \in \PA(V)} W\right) + N_{V} \\
      N_{V \ne B} \sim \delta(0) \\
      N_{B} \sim \mathrm{Ber}(\nicefrac{1}{2})
    \end{cases},
  \end{equation}
  where all endogenous variables are binary except for $Y$ (whose range is $\mathbb{N}$), and all exogenous variables are simply zero except for $N_{B}$, which is also binary.
  The idea is that $B$ has a stronger influence on $Y$ than all the other parents of $Y$ combined, and there are values of $\mathbf{n}$ (namely whenever $N_{B} = 0$) for which $B$ is not influenced by other variables.
  We need to show that, for all $X \in I$, there is $\mathbf{n} \in R_{\mathbf{N}}$ such that
  \begin{equation}
    \label{eq:x_not_int_superior}
    \max_{x\in R_{X}} \bar{f}_{Y}^{\dop(X=x)}(\mathbf{n}) < \max_{b\in R_{B}} \bar{f}_{Y}^{\dop(B=b)}(\mathbf{n}).
  \end{equation}
  Notice that $R_{\mathbf{N}} = \{\mathbf{0}, \mathbf{e}_{N_{B}}\}$, where $\mathbf{e}_{N_{B}}$ is zero everywhere except for the $N_{B}$ element, which is $1$.
  Let $X\in I$ and choose $\mathbf{n} = \mathbf{0}$.
  We have $\max_{b\in\{0,1\}}\bar{f}_{Y}^{\dop(B=b)}(\mathbf{0}) = \bar{f}_{Y}^{\dop(B=1)}(\mathbf{0}) = 2 + \heavyside\left(\sum_{W \in \PA(B)} W\right) \ge 2$.
  Furthermore:
  \begin{equation}
    \begin{split}
      \max_{x\in\{0,1\}}\bar{f}_{Y}^{\dop(X=x)}(\mathbf{0}) &= \max_{x\in\{0,1\}} \left(2n_{B}\cdot \left( 1 - \heavyside\left(\sum_{W \in \PA(B)} W\right) \right) + \heavyside\left(\sum_{W \in \PA(Y)\setminus\{B\}} W\right)\right) \\
        &= \max_{x\in\{0,1\}} \left(0 + \heavyside\left(\sum_{W \in \PA(Y)\setminus\{B\}} W\right)\right) \\
        &\le 1 < 2 \le \max_{b\in\{0,1\}}\bar{f}_{Y}^{\dop(B=b)}(\mathbf{0}).
    \end{split}
  \end{equation}

  \begin{minipage}[c]{0.74\textwidth}
  This proves the result for $B\in \PA(Y)$.\\
  Assume now that $B\in \LinftyY\setminus\PA(Y)$.
  From \Cref{prop:simple_graphical_charact}, there are nodes $A_{1}, A_{2} \in \PA(Y)$ which are reachable from $B$ by paths $\pi_{1},\pi_{2}$ which only intersect at $B$.
  Denote by $\prev_{i}$ the operator which, given a node $A$ in the path $\pi_{i}$ different from $B$, outputs the previous node in that path.
  We construct an SCM with causal graph $G$ as follows:
  \end{minipage}
  \begin{minipage}[c]{0.25\textwidth}
    \centering
    \begin{tikzpicture}[mynode/.style={inner sep=2pt,minimum size=0cm},>=latex] %
      \node[mynode] (b) at (0,1.1) {$B$};
      \node[mynode] (a1) at (-0.5,0) {$A_{1}$};
      \node[mynode] (a2) at (0.5,0) {$A_{2}$};
      \node[mynode] (y) at (0,-0.8) {$Y$};

      \draw[->, dotted, bend left=90] (b) -- node[pos=0.9,above,inner sep=8pt] {\tiny$\pi_{1}$} (a1);
      \draw[->, dotted] (b) -- node[pos=0.9,above,inner sep=8pt] {\tiny$\pi_{2}$} (a2);
      \draw[->] (a1) -- (y);
      \draw[->] (a2) -- (y);
    \end{tikzpicture}
  \end{minipage}
  \begin{equation}
    \begin{cases}
      f_{Y}(A_{1}, A_{2}, \PA_{Y}\setminus\{A_{1}, A_{2}\}, N_{Y}) = 2A_{1} \cdot A_{2} + \heavyside\left(\sum_{W \in \PA(Y) \setminus \{A_{1},A_{2}\}} W\right) + N_{Y}\\
      f_{B}(\PA(B), N_{B}) = N_{B} \cdot \left(1 - \heavyside\left(\sum_{W \in \PA(B)} W\right)\right) \\
      f_{A\in\pi_{i}\setminus\{B\}}(\prev_{i}(A), \PA(A)\setminus\{\prev_{i}(A)\}, N_{A}) = \prev_{i}(A) + N_{A}\heavyside\left(\sum_{W \in \PA(A)\setminus\{\prev_{i}(A)\}} W\right) \\
      f_{V\notin \pi_{1}\cup\pi_{2}\cup\{Y\}}(\PA(V), N_{V}) = \heavyside\left(\sum_{W \in \PA(V)} W\right) + N_{V} \\
      N_{V \ne B} \sim \delta(0) \\
      N_{B}, N_{A\in \pi_{i}} \sim \mathrm{Ber}(\nicefrac{1}{2})
    \end{cases},
  \end{equation}
  where again all endogenous variables except $Y$ are binary, and all exogenous variables are zero except for those of the type $N_{A}, A\in \pi_{i}$, which is also binary.
  Let $X\in I$.
  We again need to show that there is $\mathbf{n}\in R_{\mathbf{n}}$ such that \Cref{eq:x_not_int_superior} holds.
  One again chooses the setting $\mathbf{n} = \mathbf{0}$.
  The intuition behind this SCM is similar to that of \Cref{eq:scm_b_in_pay}, with the added property that the elements of the paths $\pi_{i}$ are simply noisy copies of $B$, and perfect copies when $\mathbf{N} = \mathbf{0}$.
  In particular, $A_{i}=B, i\in \{1,2\}$, or, using the language of unrolled assignments, $\bar{f}_{A_{i}}(\mathbf{0}) = \bar{f}_{B}(\mathbf{0})$.
  Notice that $\bar{f}_{A_{1}}(\mathbf{0})\cdot \bar{f}_{A_{2}}(\mathbf{0}) = \bar{f}_{B}(\mathbf{0})^{2} = \bar{f}_{B}(\mathbf{0})$, since $B$ is binary.
  These equalities still hold in the SCMs resulting from atomically intervening on $B$.
  Hence:
  \begin{equation}
    \bar{f}_{Y}^{\dop(B=b)}(\mathbf{0}) = 2b + \heavyside\left(\sum_{W \in \PA(Y)\setminus\{A_{1},A_{2}\}} \bar{f}_{W}(\mathbf{0}) \right) \ge 2b.
  \end{equation}
  Now, if $X=A \in \pi_{1}\setminus \{B\}$, then $A_{1}$ is a perfect copy of $A$ while $A_{2}$ is still a perfect copy of $B$.
  Hence:
  \begin{equation}
    \begin{split}
      \bar{f}_{Y}^{\dop(A=a)}(\mathbf{0}) &= 2\underbrace{\bar{f}_{A_{1}}^{\dop(A=a)}(\mathbf{0})}_{a} \cdot \underbrace{\bar{f}_{A_{2}}^{\dop(A=a)}(\mathbf{0})}_{0} + \heavyside\left(\sum_{W \in \PA(Y)\setminus \{A_{1}, A_{2}\}} \bar{f}_{W}(\mathbf{0})\right) \\
          &= \heavyside\left(\sum_{W \in \PA(Y)\setminus \{A_{1}, A_{2}\}} \bar{f}_{W}(\mathbf{0})\right) \le 1 < 2 \le \bar{f}_{Y}^{\dop(B=1)}(\mathbf{0}) \\
    \end{split}.
  \end{equation}
  The same argument holds if $X=A \in \pi_{2}\setminus \{B\}$.\\
  Finally, if instead $X \notin \pi_{1}\cup\pi_{2}\cup\{Y\}$, then $\bar{f}_{Y}^{\dop(X=x)}(\mathbf{0}) = 0 + \heavyside\left(\sum_{X \in \PA(Y)\setminus \{A_{1}, A_{2}\}} \bar{f}_{X}(\mathbf{0})\right) \le 1 < 2 \le \bar{f}_{Y}^{\dop(B=1)}(\mathbf{0})$, where the first equality holds because, for $\mathbf{n} = \mathbf{0}$, intervening on $X$ does not affect the elements of the $\pi_{i}$, including the $A_{i}$.

  \partialqed{(ii)}
\end{proof}

\section{C4 Proofs}
\label{sec:app-c4-proofs}

\begin{definition}\label{def:extra}
We define the following additional notation and terminology.
\begin{itemize}
\item For any set of nodes $B$ and any node $v'$, a path $\pi_{v'}$ that ends in $v'$ is {\it uninterrupted} by $B$ iff $(\pi_{v'} \cap B)\backslash \{v'\} = \emptyset$.
\item A $\Lambda$-structure which consists of a single node is called {\it degenerate}.
\item For any set of nodes $B$, any $\Lambda$-structure over $(B,B)$ is referred to as a $\Lambda_B$-structure.

  \item $u \stackrel{\pi_{u}}{\dashleftarrow} v \stackrel{\pi_{w}}{\dashrightarrow} w$ denotes a $\Lambda$-structure $(v,\pi_{u},\pi_{w})$ with paths $\pi_{u}: v\dashrightarrow u$, $\pi_{w}: v\dashrightarrow w$.
  If the paths' names are not relevant or clear from the context, we write simply $u \dashleftarrow v \dashrightarrow w$.

  \item As an exception, in the proofs of this section we use lower case letters for nodes, as is customary in graph theory texts.

\end{itemize}
\end{definition}


\begin{lemma}\label{lem:closureofclosure}
Let $G=(V,E)$ be a DAG, $U \subseteq V$. $v \in \Linfty(U)$ iff there exists a $\Lambda_{\Linfty(U)}$-structure $v' \dashleftarrow v \dashrightarrow v^*$ for some $v',v^* \in \Linfty(U)$.
\end{lemma}
\begin{proof}
It is easily seen that $\Linfty (\Linfty (U))=\Linfty(U)$; therefore, this lemma is a direct corollary of Proposition~\ref{prop:simple_graphical_charact}.
\end{proof}

\begin{lemma}[Existence of $\Lambda$-substructure]\label{lem:substructure}
Let $B$ be a set of nodes, and let $b_1,b_2 \in B$ s.t. $b_1 \neq b_2$. Let $v \notin \{b_1,b_2\}$ be a node and let $\pi_1:v \dashrightarrow b_1$, $\pi_2:v \dashrightarrow b_2$, s.t. $b_1 \notin \pi_2$ and $b_2 \notin \pi_1$ (note that we do not assume $\pi_1 \cap \pi_2=\{v\}$, meaning that other overlaps remain possible). Then, in the subgraph consisting of the two paths (as in, the graph that includes all the nodes and all the edges that are in at least one of the paths), there exists a $\Lambda_B$-structure $b_1 \dashleftarrow v' \dashrightarrow b_2$ where $v' \in \pi_1 \cap \pi_2$.
\end{lemma}
\begin{proof}
For $v' \in \argmin_{\anpo}{\pi_1 \cap \pi_2}$, $b_1 \stackrel{\pi_1|^{v'}}{\dashleftarrow} v' \stackrel{\pi_2|^{v'}}{\dashrightarrow} b_2$ is a $\Lambda_B$-structure.
\end{proof}

\begin{lemma}\label{lem:connectorwithoutunique}
Let $G=(V,E)$ be a DAG, $U \subseteq V$, $v \in V$. If $v'$ is a $U$-connector of $v$, then $v'$ has a path from $v$ uninterrupted by $\Linfty (U)$.
\end{lemma}
\begin{proof}
On the one hand, $v' \in De(v)$ so it has some path from $v$. On the other hand, any path $\pi_{v'}=v \dasharrow v'$ is uninterrupted by $\Linfty (U)$: otherwise there would be a node $v'' \neq v'$ s.t. $v'' \in \pi_{v'} \cap \Linfty (U)$, but then $v' \anpo v''$ and $v'' \in De(v) \cap \Linfty (U)$, violating $v' \in \argmax_{\anpo}{[De(v) \cap \Linfty (U)]}$.
\end{proof}

\connectorsunique*
\begin{proof}
If $De(v) \cap \Linfty (U)=\emptyset$ or if $v \in \Linfty (U)$, the lemma is trivial. Assume $De(v) \cap \Linfty (U) \neq \emptyset$ and $v \notin \Linfty (U)$. Let $v' \in \argmax_{\anpo}{[De(v) \cap \Linfty (U)]}$; by Lemma~\ref{lem:connectorwithoutunique}, we know that there is a path $\pi_{v'}:v \dashrightarrow v'$ uninterrupted by $\Linfty (U)$. We claim that there cannot exist another node $v^* \neq v'$ s.t. $v^* \in \Linfty (U)$ has a path $\pi_{v^*}:v \dashrightarrow v^*$ uninterrupted by $\Linfty (U)$. Assume for the sake of contradiction that such a node $v^*$ exists. Because both paths are uninterrupted by $\Linfty (U)$, and $v',v^* \in \Linfty (U)$, Lemma~\ref{lem:substructure} implies the existence of a (non-degenerate since $v' \neq v^*$) $\Lambda_{\Linfty (U)}$-structure $v^* \dashleftarrow \tilde{v} \dashrightarrow v'$ where $\tilde{v} \in \pi_{v'} \cap \pi_{v^*}$. Therefore, by Lemma~\ref{lem:closureofclosure}, $\tilde{v} \in \Linfty(U)$. However, $\tilde{v} \in \pi_{v'}$ and $\tilde{v} \neq v'$, so $\pi_{v'}$ is interrupted by $\Linfty(U)$, which yields a contradiction.
\end{proof}

\cfourcorrect*
\begin{proof}
We claim that the algorithm computes the connectors correctly: that is, we claim that for every $\mathfrak{v} \in V$, upon termination of the appropriate loop (meaning the loop where $v=\mathfrak{v}$ for $\mathfrak{v} \notin U$, or before the first loop begins for $\mathfrak{v} \in U$), $\mathfrak{c}[\mathfrak{v}]=\mathfrak{v}'$ iff $\mathfrak{v}'$ is the $U$-connector of $\mathfrak{v}$, and $\mathfrak{c}[\mathfrak{v}]=\mathrm{NULL}$ iff $\mathfrak{v}$ has no $U$-connector. Note that our claim implies that for $\mathfrak{v} \in V$, upon termination of the appropriate loop, $\mathfrak{v} \in S \Leftrightarrow \mathfrak{v} \in \Linfty (U)$: this is because $S$ is easily seen to include exactly the nodes for which $\mathfrak{c}[\mathfrak{v}]=\mathfrak{v}$, which by our claim are their own $U$-connectors, which holds iff $\mathfrak{v} \in \Linfty (U)$. Note that once the appropriate loop terminates, $\mathfrak{c}[\mathfrak{v}]$ is never reassigned and $\mathfrak{v}$ is not added to or removed from $S$, so the connector of $\mathfrak{v}$ and its membership in $S$ or lack thereof remain correct through the end of the algorithm.

Let $v_1,\ldots,v_n$ be a reverse topological order of $V$ (no need to assume it is the one used in the algorithm's loop). Assume the claim is true for $v_1,\ldots,v_{i-1}$, and let us prove it is true for $v_i$. If $v_i \in U$, the claim is true by initialization of the algorithm; so assume $v_i \notin U$. There are three cases to consider in the loop's iteration:
\begin{enumerate}
\item $C=\emptyset$. In that case, the algorithm keeps $\mathfrak{c}[v_i]=\mathrm{NULL}$. We claim that indeed $v_i$ has no connector, meaning that $De(v_i) \cap \Linfty (U)=\emptyset$. First, no child of $v_i$ has a connector, meaning that no child of $v_i$ has a descendant in $\Linfty (U)$. Thus, no node in $\Linfty (U)$ is reachable from $v_i$. On the other hand, this also means that no reachable node from $v_i$ is in $U$, and we have also assumed that $v_i$ itself is not in $U$: thus, by \Cref{prop:simple_graphical_charact}, it follows that $v_i \notin \Linfty(U)$. Since neither $v_i$ nor any node reachable from $v_i$ is in $\Linfty(U)$, indeed $De(v_i) \cap \Linfty (U)=\emptyset$.
\item $|C|=1$. Let $x \in V$ s.t. $C=\{x\}$. The algorithm sets $\mathfrak{c}[v_i]=x$. We claim that indeed $x$ is the $U$-connector of $v_i$. By the inductive assumption and Lemma~\ref{lem:connectorunique}, $x$ is the unique element from $\Linfty (U)$ reachable from $v_i$ via a {\it non-trivial} path uninterrupted by $\Linfty(U)$, as any non-trivial path must go through a child, and we can apply the inductive assumption and Lemma~\ref{lem:connectorunique} to each child. However, to show that $x$ is indeed the connector of $v_i$, we need to rule out the possibility of a trivial path to $\Linfty (U)$, namely to rule out the possibility that $v_i \in \Linfty(U)$. Since $v_i \notin U$, then by Proposition~\ref{prop:simple_graphical_charact} it is sufficient to rule out the existence of a non-degenerate $\Lambda$-structure from $v_i$ to $U$. However, as we noted, any non-trivial path from $v_i$ to $U$ (and hence to $\Linfty (U)$) must go through $x$, and hence any two paths to distinct nodes in $U$ must overlap at $x \neq v_i$, meaning that they do not make a $\Lambda$-structure.
\item $|C|>1$. In that case, the algorithm sets $\mathfrak{c}[v_i]=v_i$. We claim that $v_i \in \Linfty (U)$ (and so $v_i$ is its own connector). By Proposition~\ref{prop:simple_graphical_charact}, we need to establish the existence of a $\Lambda$-structure from $v_i$ to $U$. Since $|C|>1$, then there exist $s_1,s_2 \in C$ s.t. $s_1 \neq s_2$, and there exist children $t_1,t_2$ of $V$ s.t. $\mathfrak{c}[t_1]=s_1$ and $\mathfrak{c}[t_2]=s_2$; by the inductive assumption, $s_1$ and $s_2$ are respectively the connectors of $t_1$ and $t_2$. Therefore, $s_1$ and $s_2$ are in $\Linfty (U)$. By Lemma~\ref{lem:connectorunique}, there exist paths $\pi_1=t_1 \dashrightarrow s_1$ and $\pi_2=t_2 \dashrightarrow s_2$ uninterrupted by $\Linfty (U)$. These paths do not overlap: had they overlapped, then by Lemma~\ref{lem:substructure} they would've contained a $\Lambda$-substructure $s_1 \dashleftarrow z \dashrightarrow s_2$ s.t. $z \in \pi_1 \cap \pi_2$ so by Lemma~\ref{lem:closureofclosure} $z \in \Linfty(U)$, making neither $\pi_1$ nor $\pi_2$ uninterrupted by $\Linfty (U)$. Since $t_1$ and $t_2$ are children of $v_i$, we may prepend the edges $v_i \rightarrow t_1$ and $v_i \rightarrow t_2$ to $\pi_1$ and $\pi_2$ respectively and get paths $\pi_1'=v_i \rightarrow t_1 \dashrightarrow s_1$ and $\pi_2'=v_i \rightarrow t_2 \dashrightarrow s_2$; since $\pi_1$ and $\pi_2$ do not overlap, these two paths yield a $\Lambda$-structure from $v_i$ to $L^\infty(U)$, which by Lemma~\ref{lem:closureofclosure} implies $v_i \in L^\infty(U)$.
\end{enumerate}
\end{proof}

\cfourruntime*
\begin{proof}
If the graph is not given in adjacency list representation, we convert it to this representation in $O(|V|+|E|)$ time. Initialization in C4 is trivially $O(|V|)$. Reverse topological sorting can be done in $O(|V|+|E|)$ using Kahn's algorithm. In the loop, for each $v \in V \backslash U$, we go over all outgoing edges from $v$ to compute $C$, which because of the adjacency list representation takes $O(|\Ch(v)|)$ time. In aggregate over the entire operation of the algorithm, computing $C$ takes $O(|E|)$ time overall, as each edge is inspected at most once. The loop runs $O(|V|)$ times, and all operations in it except the computation of $C$ take $O(1)$ time, so all steps except computing $C$ take at most $O(|V|)$ time overall. Thus, the running time of the algorithm is $O(|V|+|E|)$.
\end{proof}


\section{Supplementary Material for Experimental Results}
\label{sec:app-exps}

The results of the experiments testing our search space reduction method are presented in \Cref{fig:fractions-hist} and \Cref{fig:realworld_fractions}, for randomly generated graphs and real-world datasets, respectively.

All real-world datasets come from the \texttt{bnlearn} repository, except for the \texttt{railway} dataset, which was provided by ProRail, the institution responsible for traffic control in the Dutch railway system.

The \texttt{railway} dataset consists of a graph whose nodes represent train delays in a segment of the Dutch railway system, measured at specific "points of interest" (such as train stations).
Each node is labeled with a code identifying the train, an acronym for the point of interest, a letter indicating the train's activity at that location—arriving (A), departing (V), or passing through (D)—and the planned time for that activity.
Arrows are drawn between delay nodes that are known to influence each other. For example, arrows connect nodes of the same train at consecutive times, as the delay of a train at time \( t \) will influence its delay at \( t + \Delta t \). Additionally, arrows may connect nodes corresponding to train activities sharing the same platform, since a train must wait for the preceding train to vacate the platform before using it.
This dataset can be found in the code repository which supplements this paper.


\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{./mGISS_fractions_histograms.png}
  \caption{Fraction of nodes remaining after applying our search space filtering procedure, on random graphs. $1000$ graphs were generated for each pair $($number of nodes, expected degree$)$. The impact of our method decreases with the expected degree, and increases with the number of nodes.}
  \label{fig:fractions-hist}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{./mGISS_realworld_fractions.png}
  \caption{Fraction of nodes remaining after applying our search space filtering procedure, on real-world graphs. All models come from the \texttt{bnlearn} repository except for the \texttt{railway} model. The models are sorted by their \emph{total} number of nodes. On top of each bar one can read the fraction value (in black) and the exact numbers \texttt{(number of nodes in mGISS / number of proper ancestors of Y)} in red. Notice that models with larger numbers of nodes tend to benefit more from our method.}
  \label{fig:realworld_fractions}
\end{figure}




\end{document}
