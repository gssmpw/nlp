\subsection{Difficult Examples Are Not Necessarily Data Errors}
\label{sec:insights}
Before proposing our solution to filter out difficult and harmful examples, we here investigate their characters, ensuring their removal is justified. For statistics and case study on difficult examples, please refer to Appendix~\ref{app:feature-analysis} and~\ref{app:case_study_difficult_example}. 

\textbf{Mislabeled data (Figure~\ref{fig:insights} (a)).} Prior work suggests that difficult examples might be mislabeled~\cite{ultrafeedback_preferences,notus2023}. To test this hypothesis, we sort the examples by their validation loss and flip the labels of last $40\%$ (the most difficult) examples. However, this modification does not alleviate the performance drop, suggesting that label noise is not the primary cause.

\textbf{Distribution shift (Figure~\ref{fig:insights} (b)).} 
%It is possible that the easy examples and difficult examples follow two distinct distributions. In such, the learned LLM forgets preference presented in the easy examples when switch to the difficult examples. We disprove this hypothesis by conducting alignment with a designated $\epsilon$-greedy sorting function. By which, we ensure that every mini-batch data are with  $\epsilon$ part from random ordering while $(1-\epsilon)$ part are sorted by the validation loss globally. However, we do not observe significant benefits compared to the greedy sorting setting. 
Another possibility is that difficult examples represent a distinct distribution, causing catastrophic forgetting when models transition from easy to difficult examples. We test this using $\epsilon$-greedy sorting: each mini-batch contains $\epsilon$ portion of randomly sampled examples and $(1-\epsilon)$ portion of examples sorted by validation loss. This ensures continuous exposure to both distributions, yet shows no improvement over the greedy sorting.

\textbf{Learning rate sensitivity (Figure~\ref{fig:insights} (c)).}
We argue that the performance drop is not caused by simply the improper learning rate. We investigate this by varying the learning rate.  However, adjusting the learning rate neither alleviates performance drops nor delays the decline, demonstrating that the issue is unrelated to improper optimization settings.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig/illustrative-figure.pdf}
    \vspace{-0.9cm}
    \caption{
    \textbf{The pipeline of \textit{Selective DPO}.} It extends DPO~\cite{rafailov2024direct} with an principled data selection process: selecting preference examples within the model's capacity. Specifically, Selective DPO comprises three steps: \underline{\textit{(1)}} Train a set of reference models using the DPO loss on different subsets of the training data. \underline{\textit{(2)}} Evaluate the reference models to compute the validation loss, which serves as a proxy for example difficulty. \underline{\textit{(3)}} Selectively align LLMs on examples with low validation loss from easy to difficult examples.
    }
    \vspace{-0.4cm}
    \label{fig:illustrative-figure}
\end{figure*}


\subsection{Difficult Example Exceeds Model's Capacity}
We hypothesize that difficult examples represent tasks beyond the model's current capabilities, requiring larger models to properly understand the nuanced preference differences.
To validate this hypothesis, we conduct experiments using Qwen-2.5 models~\cite{qwen2.5} of three sizes: 3B, 7B, and 14B. The dataset is \textit{Argilla-dpo-mix-7k}. 
Figure~\ref{fig:scaling_law} shows a clear relationship between model size and manageable example difficulty: the optimal percentage of training data (the \textit{sweet spot}) increases from 64\% for the 3B model to 81\% for the 14B model. This scaling pattern demonstrates that larger models can effectively learn from more difficult examples, confirming the direct relationship between model capacity and example difficulty threshold.

