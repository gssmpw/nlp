\section{Related Work}
\label{sec:related-work}
\subsection{Data Selection for Pre-Training}
Selecting training corpus brings significant performance gains in the pre-training stage~\cite{wenzek2019ccnet,mann2020language,zhao2023survey,penedo2023refinedweb,txt360data2024}. 
Approaches can be broadly categorized into two categories: \textbf{Sample-level selection} focuses on filtering out undesired content such as non-target languages, duplicated data, toxic materials, and low-quality information~\cite{albalak2024survey}. This is often achieved through model-based filters~\cite{joulin2016fasttext,engstrom2024dsdm,wettig2024qurating} or heuristic filters~\cite{wenzek2019ccnet,lee2021deduplicating,laurenccon2022bigscience}, each applying specialized filters for specific objectives. 
\textbf{Token-level selection}, an emerging strategy, down-weights low-quality tokens to enhance data quality~\cite{lin2024rho}, complementing sample-level filtering.




\subsection{Data Selection for Supervised Fine-Tuning}
Recent study suggests that SFT changes only the format of generation~\cite{zhou2024lima}. In light of this, various methods are proposed for finding the most informative subset for SFT, mainly following three principles: data quality, diversity, and importance~\cite{qin2024unleashing}. The measurement of \textbf{data quality} can be manual indicators such as the linguistic DQI~\cite{mishra2020we}, human scores~\cite{zhou2024lima}. Model-based quality measurement includes predictions from ChatGPT~\cite{chen2024alpagasus}, reward models~\cite{cao2023instruction}, small reference models~\cite{ankner2024perplexed} and the LLM itself~\cite{li2024quantity}. 
Measurements of \textbf{data diversity} are mainly manually defined, such as the source diversity~\cite{mukherjee2023orca, wang2023far} and distance in the embedding space~\cite{wu2023self,xu2023rethinking,du2023mods,liu2024what}. 
\textbf{Data importance}, which evaluates an exampleâ€™s contribution to a specific task, measured using performance scores~\cite{engstrom2024dsdm}, data influence models~\cite{yu2024mates}, or relevance to desired skills~\cite{chen2024skill}.


\subsection{Scoring the Example Difficulty}
Scoring data difficulty is central to curriculum learning, which prioritizes training on simpler examples before progressing to more complex ones~\cite{bengio2009curriculum}. \textbf{Heuristic scoring functions} mirror human priors of difficulty understanding, such as sentence length~\cite{spitkovsky2010baby,tay2019simple,nagatsuka2023length}, word rarity~\cite{chang2021does}, and linguistic perplexity~\cite{campos2021curriculum}. In contrast, \textbf{principled scoring functions} leverage model behavior to indicate example difficulty, including reward margins from third-party reward models~\cite{croitoru2024curriculum}, model perplexity on responses~\cite{wu2024curriculum}, or attention scores from transformer models~\cite{kim2024strategic}.
In this work, we employ two principled scoring measures, demonstrating their robustness and consistency in ranking examples. This allows us to analyze difficult examples objectively, avoiding ambiguities inherent in heuristic definitions.

\subsection{Curriculum Learning for Alignment} Curriculum learning (CL) mimics human cognition by structuring learning from simpler to more complex concepts~\citep{avrahami1997teaching,bengio2009curriculum}. However, CL remains a highly debated technique. While some studies show that it accelerates convergence, enhances generalization, and/or improves robustness in models like convolutional neural networks~\citep{jiang2014easy,tudor2016hard}, recurrent neural networks~\citep{zaremba2014learning,sachan2016easy}, transformers~\citep{platanios2019competence}, and diffusion models~\citep{croitoru2023reverse}, other research finds little or no benefit~\citep{platanios2019competence,campos2021curriculum,wucurricula}. 
In preference alignment for LLMs, the results are similarly mixed. \citet{kim2024strategic} explored CL for preference alignment and concluded that sorting examples according to \textit{prompt length} and \textit{attention score} offered no clear benefits. On the other hand, \citet{pattnaik2024curry} reported positive results, although with other tricks such as multiple candidate pairs data and iterative reference model. Our study suggests that CL, when paired with robust difficulty scoring, can positively impact LLM alignment by aligning data difficulty with model capacity.

