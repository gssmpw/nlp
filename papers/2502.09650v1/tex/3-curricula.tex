\section{Preference Examples Vary in Difficulty}
\label{sec:curricula}

One surprising finding here is that the order in which examples are learned is remarkably consistent across runs. Such robustness reveals the underlying presence of example difficulty. We then validate the effectiveness of validation loss as a measure of example difficulty for alignment tasks.

\subsection{The Underlying Example Difficulty}\label{sec:consistent_learning_order}

While various metrics such as length~\cite{spitkovsky2010baby,tay2019simple,nagatsuka2023length} and perplexity~\cite{wu2024curriculum} have been proposed to measure difficulty of text samples,
their ability to reliably capture example difficulty remains controversial~\cite{campos2021curriculum}. 
We address this concern by demonstrating: (1) examples have distinct learned steps (see Eq.\ref{eq:learned-step}), indicating different difficulty levels, and (2) these learned steps are consistent across runs with different training data and random seeds.

In Figure~\ref{fig:implicit_curricula} (left), we visualize the learned steps of 300 test examples from \textit{Ultrafeedback-binarized}\footnote{\url{https://huggingface.co/datasets/HuggingFaceH4/ultrachat\_200k}},% using color coding, where a larger learned step indicates more training steps required for the LLM to understand the corresponding example. 
where darker colors indicate more training steps needed for model comprehension. 
Results from 10 runs show consistent learning order across different models~\citep{jiang2023mistral,llama3modelcard,team2024gemma} varying in size (2B--9B), training stage, and data sampling. This consistency confirms that examples vary in difficulty, allowing us to discuss difficult examples without debating various definitions of difficulty.



\subsection{Validation Loss as a Proxy for Learned Step}

The robust learning order suggests the existence of difficult examples---some examples are consistently harder for LLMs to understand.  
However, identifying these examples at scale is computationally expensive, as the computing of learned step requires evaluating the model after each gradient update. To address this, we adopt validation loss from the curriculum learning literature~\cite{wucurricula,rampp2024does} (see Eq(\ref{eq:validation-loss})). Specifically, we train six reference models using the DPO objective on the randomly sampled half training set and evaluate the validation loss for examples on the other half.
We refer the difficult examples to examples with large validation loss.

\begin{definition}[Difficult example]
    \label{def:difficult-example}
    A preference example $(x, y_w, y_l)$ is considered a \textit{difficult example} if its \textit{validation loss} exceeds or equals a specified value: $$\text{VL}(x, y_w, y_l) \geq Q(\tau).$$ 
\end{definition}

\begin{remark}
    We introduce a flexible threshold $Q(\tau)$, \textit{i.e.,} the $\tau$-quantile of the validation loss distribution. This variability arises mainly from: (1) There is no formal definition of the easy and difficult samples~\cite{zhu2024exploring}, and (2) Different pre-train models have different training dynamics and thus different scales and distributions of validation loss. 
\end{remark}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig/base-model-argilla-armo.pdf}\vspace{-0.1cm}
    \includegraphics[width=\linewidth]{fig/base-model-uf-armo.pdf}
    \vspace{-0.4cm}
    \caption{
    \textbf{Direct Preference Optimization (DPO) struggles with difficult examples, broadly and significantly.} We present the defined WR$'$ evolution for four models trained on the \textit{argilla-mix-dpo-7k} and \textit{ultrafeedback-binarized} datasets. The results are based on checkpoints from three 1-eopch runs with different seeds. 
    \textit{\underline{Random Ordering (DPO)}}: Training data are presented in a randomized sequence. 
    \textit{\underline{Sorted by VL (From Easy to Difficult)}}: Training examples are ranked by their \textit{validation loss} (VL) and presented from easy to difficult, following a curriculum learning approach. 
    \textit{\underline{Selected by VL (Shuffled)}}: The easiest 60\% (for Argilla-7K) or 50\% (for UF-binarized) of the data is selected based on VL, and examples are sampled in a random order for training. The VL measurements are displayed as bar plots. We include evaluation results (dashed lines) from the two corresponding DPO models released by~\citet{meng2024simpo} for reference.
    }
    \label{fig:base-model-struggles}
\end{figure*}



% \subsection{Validation Loss as an Effective Alternative}
To assess whether validation loss effectively approximates the learned step, we examine the correlation between difficulty rankings produced by these two measures. Using \textit{Spearmanâ€™s} rank correlation, we compared rankings across different runs and models. As shown in the middle panel of Figure~\ref{fig:implicit_curricula}, validation loss exhibits patterns remarkably similar to the learned step. Furthermore, the high correlation coefficients between average learned step and average validation loss across the four models (0.9258, 0.9227, 0.9336, and 0.9283) validate the effectiveness of validation loss as a computationally efficient proxy for learned step. 
Additionally, the \textit{Jaccard similarity} between difficult example sets (defined as top 50\% by either metric) remains consistently high for each model (Figure~\ref{fig:implicit_curricula}, right), confirming that both measures identify similar sets of difficult examples. 


