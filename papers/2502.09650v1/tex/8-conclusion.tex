\section{Related Work}

\textbf{Response selection.} The importance of selecting high-quality responses as preferred choices has been highlighted in several studies~\cite{bai2022training, ethayarajh2022understanding, tunstall2023zephyr}. These works focus on ensuring that preferred responses align with human values and ethics. Our work builds upon these efforts in two key ways: (1) the datasets we consider already incorporate these response selection techniques, and (2) we prioritize whether preference examples fall within the capabilities of the target LLM, rather than solely emphasizing their alignment with human values. 
\textbf{Data correction.} Efforts to address noisy labels in datasets include techniques such as label flipping~\cite{wang2024secrets} and confidence-based data filtering~\cite{gao2024impact}. Approaches like cDPO~\cite{mitchell2023note} and rDPO~\cite{chowdhury2024provably} aim to mitigate the impact of mislabeling without explicitly removing mislabeled examples. In our study, we incorporate label flipping and label smoothing experiments to support our claim that difficult examples are not necessarily mislabeled, but instead exceed the model's capacity.
\textbf{Seemingly relevant work.} 
Our study differs from general data selection research, such as~\citet{liu2024what, xia2024rethinking}, which uses the term alignment but actually focuses on the SFT stage. For a comprehensive review of data selection for LLMs and curriculum learning, we refer readers to Appendix~\ref{sec:related-work}.


% Notably, our work differs to the general alignment studies such as~\citet{liu2024what, xia2024rethinking}, which use the term \textit{alignment} but primarily focus on the SFT stage. We refer readers to Appendix~\ref{sec:related-work} for a comprehensive review on data selection for LLMs and curriculum learning. 

% what we study: advocates a paradigm shift: from xxx to xxx.
% what we propose: 
% what is the impact: 

% In this what, we reveal a critical gap in LLM alignment: the mismatch between data difficulty and model capactiy. As we shown, the overly difficult examples hinder the alignment performance 

\section{Conclusion and Future Work}
In this work, we reveal and address a critical gap in LLM alignment: the mismatch between data difficulty and model capacity. Challenging the assumption that more clean data uniformly improves alignment, we propose a novel principle for alignment tasks:

\textit{Preference data vary in difficulty, and overly difficult examples hinder alignment, by exceeding the model's capacity}. 

Comprehensive experiments validate the three key claims underlying this principle. Building on this data selection principle, we introduce Selective DPO, an alignment algorithm that selectively trains on examples within the model's capacity. Selective DPO achieves state-of-the-art results on benchmarks including AlpacaEval 2, Arena-Hard, and MT-Bench, with up to 16\% gains in win rates over DPO. Our work advocates a paradigm shift in alignment: alignment should prioritize data difficulty relative to model capacity rather than treating all preference data equally. 

However, limitations remain: (1) Selective DPO tends to favor longer responses due to potential data bias; and (2) the proposed principle is designed and validated specifically for the DPO setting, limiting its direct applicability to RLHF. These gaps highlight opportunities for future work.


\clearpage
\section*{Impact Statement}
This paper presents work whose goal is to advance the alignment between large language model behaviors and human values. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.