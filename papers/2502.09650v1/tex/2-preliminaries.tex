\section{Preliminaries}
\label{sec:preliminaries}

\subsection{Preference Alignment with DPO} 

Preference alignment~\cite{ouyang2022training} aims to align the outputs of LLMs with human ethics and styles, ensuring that these models are safe, reliable, and effective for real-world applications~\cite{christiano2017deep}. In this study, we focus on direct preference optimization (DPO)~\cite{rafailov2024direct}, a method known for its simplicity and robust performance in alignment tasks~\cite{dubey2024llama}. 
DPO trains a policy model, $\pi_{\vthe}$, on a dataset $\mathcal{D}$ containing prompt $x$, preferred response $y_w$, and rejected response $y_l$. The training objective incorporates a \textit{reference SFT model}, $\pi_{\text{ref}}$, and a hyper-parameter, $\beta$, to control the divergence between $\pi_{\vthe}$ and $\pi_{\text{ref}}$:
\begin{align}
    \label{eq:dpo_loss}
    \mathcal{L}_{\text{DPO}}&(\pi_{\vthe}, \mathcal{D}) = - \E_{(x, y_w, y_l \sim \mathcal{D})} \Big[ \\ \nonumber  &\log \sigma \big(\beta \log\frac{\pi_{\vthe} (y_w | x)}{\pi_{\text{ref}}(y_w |x)} - \beta \log\frac{\pi_{\vthe}(y_l | x)}{\pi_{\text{ref}}(y_l |x)} \big) \Big].
\end{align}

\subsection{Quantifying the Example Difficulty}
\label{sec:difficulty_measurement}
\begin{figure*}[!htp]
    \centering
    \includegraphics[width=\linewidth]{fig/learning_order_and_difficult_examples_corr.pdf}
    \vspace{-0.4cm}
    \caption{\textbf{Examples are learned in consistent orders across different runs of the same LLM}, despite variations in the training data and random seeds. \underline{\textit{Left:}} The learned step (ranging from 1 to 948) represents the step at which the implicit reward model distinguishes between preferred and rejected responses (see Eq.~(\ref{eq:learned-step}), threshold $\delta=0.4$). X-axis: 40 unique combinations of model size (4 total) and training data subset (10 per model). Y-axis: 300 test examples, sorted by average learned step across 40 runs.  Color gradients encodes difficulty. 
    \textit{\underline{Middle:}} Two \textit{Spearman's} rank correlation matrices. Lower triangle: correlations of learned step across runs; upper triangle: validation loss correlations. 
    \textit{\underline{Right:}} Two \textit{Jaccard} similarity matrices for difficult examples (top 50\%) defined by learned step and validation loss across runs.
    }
    \label{fig:implicit_curricula}
    \vspace{-0.1cm}
\end{figure*}

\paragraph{Learned Step as a Measure of Difficulty.} %\xie{Learned-step-based difficulty?}
An example's \textit{learned step} is defined as the earliest training step after which the model reliably distinguishes preferred responses from rejected answers. This is formalized as:
\begin{align}
    \label{eq:learned-step}
    \text{LS}&(x, y_w, y_l) = \min_{t_{\text{lrn}}} \Big\{ \\ \nonumber &t_{\text{lrn}} \Big| \beta \log\frac{\pi_{\vthe_t} (y_w | x)}{\pi_{\text{ref}}(y_w |x)} - \beta \log\frac{\pi_{\vthe_t}(y_l | x)}{\pi_{\text{ref}}(y_l |x)} > \delta, \forall t > t_{\text{lrn}} \Big\}.
\end{align}
A similar metric has been explored by \citet{wucurricula}. The difference is that we calculate Eq.~(\ref{eq:learned-step}) exclusively on \textit{held-out examples}, ensuring it reflects intrinsic difficulty rather than the order of data presentation~\cite{zhu2024iterative}. 
Higher learned steps indicate more difficult examples. For all experiments, we set $\delta = 0.4$.


\paragraph{Validation Loss as an Alternative Difficulty Proxy.} % \xie{Validation-loss-based difficulty?}
We borrow \textit{validation loss}~\cite{wucurricula,rampp2024does} as a computationally cheaper alternative to the \textit{learned step}. Specifically, for a specific example $(x, y_w, y_l)$ from $\mathcal{D}\backslash \hat{\mathcal{D}}$, validation loss is defined as:
\begin{align}
    \label{eq:validation-loss}
    \text{VL}&(x, y_w, y_l) = \\ \nonumber &- \log \sigma \big(\beta \log\frac{\pi_{\hat{\vthe}} (y_w | x)}{\pi_{\text{ref}}(y_w |x)} - \beta \log\frac{\pi_{\hat{\vthe}}(y_l | x)}{\pi_{\text{ref}}(y_l |x)}\big),
\end{align}
where $\pi_{\hat{\vthe}} = \arg\min_{\pi_{\vthe}} \mathcal{L}_{\text{DPO}}(\pi_{\vthe}, \hat{\mathcal{D}})$ is a \textit{reference model} trained on the subset $\hat{\mathcal{D}}\subset \mathcal{D}$.\footnote{Throughout this work, the term reference model ($\pi_{\hat{\vthe}}$) refers to the aligned model trained on subsets of the training examples, while reference SFT model ($\pi_{\text{SFT}}$) specifically denotes the model defined in Eq.~(\ref{eq:dpo_loss}).}
Low validation loss indicates easier examples. To compute validation loss, we partition $\mathcal{D}$ equally into $\hat{\mathcal{D}}$ and $\mathcal{D}\setminus\hat{\mathcal{D}}$, train on one, evaluate on the other, averaging results over three runs.

