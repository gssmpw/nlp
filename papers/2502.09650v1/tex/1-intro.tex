\section{Introduction}
Data selection focuses on identifying the most valuable subset of data from a dataset while excluding ineffective samples~\cite{albalak2024survey}. It significantly improves the first two stages of training large language models (LLMs): pre-training~\cite{lee2021deduplicating,penedo2023refinedweb,txt360data2024} and supervised fine-tuning (SFT)~\cite{cao2023instruction,qin2024unleashing,zhou2024lima}, by adhering to well-established principles. However, in the third stage, \textit{i.e.}, preference alignment, data selection principles are often \textit{implicit and superficial}, potentially limiting the alignment between LLM outputs and human preferences~\cite{askell2021general,weidinger2021ethical}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/overall-performance.pdf}
    \vspace{-0.8cm}
    \caption{\textbf{Overly difficult examples hinder the alignment.} Training on difficult examples, identified by high \textit{validation loss}, adversely affects alignment and decreases overall performance by 9.4\% in win rate. The results are from experiments with four SFT models on the \textit{UltraFeedback-binarized} dataset, \textit{i.e.,} Figure~\ref{fig:base-model-struggles}.
    }
    \vspace{-0.4cm}
    \label{fig:overall-performance}
\end{figure}

Prior studies in alignment underscore the importance of selecting error-free data by demonstrating the presence and negative impacts of mislabeled data~\cite{wang2024secrets,gao2024impact}, noisy feedback~\cite{mitchell2023note,chowdhury2024provably}, and data with low annotator agreement~\cite{ultrafeedback_preferences}. Those practices implicitly assume that all error-free data are beneficial for alignment regardless of the model's capacity. However, we argue this assumption overlooks the relationship between data difficulty and model capacity. 
Our experiments show that overly difficult examples not only fail to improve alignment but can actually hinder the performance (see Figure~\ref{fig:overall-performance}).
This observation motivates our systematic investigation into how example difficulty affects alignment performance.

Our main contribution is a new principle for preference data selection, which emphasizes the match between model capacity and example difficulty: 


\noindent\fbox{%
    \parbox{0.97\linewidth}{%
        \textit{Preference data vary in difficulty, and overly difficult examples hinder alignment, by exceeding the model's capacity.}
    }%
}

This principle has three key claims: (1) preference data can be categorized by difficulty levels, (2) overly difficult examples can harm alignment performance, and (3) difficulty is relative to the model's capacity---larger models, with greater capacity, can benefit from more difficult examples. We validate this principle through systematic experiments. Specifically:

\myparagraph{Preference examples vary in difficulty level (Section~\ref{sec:curricula}).} We show that, in DPO~\cite{rafailov2024direct}, the order in which testing examples are correctly classified by the implicit reward model is consistent across different runs and training data. This robust ordering reflects the existence of inherent example difficulties.
Based on this observation, we use \textit{validation loss} as a computational proxy to systematically identify and rank example difficulty.

\myparagraph{Difficult examples hinder alignment (Section~\ref{sec:hinders}).} 
We reveal that difficult examples--identified by high validation loss--significantly hinder alignment. 
Our experiments across two datasets and four pre-trained models show consistent performance drops when including these difficult examples. 
These challenging examples emerge naturally during data collection, rather than through artificial construction.
This highlights the imperfections of the previous principle and calls for a new data selection principle for alignment tasks. 




\myparagraph{Difficult examples exceed the model's capacity (Section~\ref{sec:hinders}).} 
We demonstrate that example difficulty interacts directly with model capacity. Experiments with models of 3B, 8B, and 14B parameters show that larger models benefit from higher proportions of difficult examples, confirming that difficulty must be calibrated to the modelâ€™s capacity. 


\myparagraph{Filtering out overly difficult examples yields remarkably gains (Section~\ref{sec:method} and~\ref{sec:benchmarking}).} 
Finally, we validate our principle with a new method, \textit{Selective DPO}, which filters out overly difficult examples. This approach achieves a 9--16\% higher win rate on AlpacaEval 2~\cite{dubois2024length} compared to standard DPO~\cite{rafailov2024direct}, outperforming state-of-the-art methods such as SimPO~\cite{meng2024simpo} and R-DPO~\cite{Park2024DisentanglingLF} while maintaining better perplexity and implicit reward margins. 
