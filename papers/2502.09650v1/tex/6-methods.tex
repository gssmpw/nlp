\section{Selective DPO}
\label{sec:method}

Having verified the three key claims underpinning our data selection principle, we are now well-positioned to propose an instantiated algorithm, \textit{Selective DPO}. It extends the standard DPO~\cite{rafailov2024direct} by selectively training on examples within the model’s capacity. The algorithm consists of three main steps, as illustrated in Figure~\ref{fig:illustrative-figure}:


\begin{itemize}
    \item \textbf{Train reference models.} 
    The training dataset is randomly split into two partitions. Using the standard DPO loss (Eq.~\ref{eq:dpo_loss}), SFT models are trained separately on each partition, resulting in two reference models per split. This process is repeated three times, yielding six reference models. Unlike the reference SFT model used in the DPO objective to control KL divergence, these reference models are specifically employed for computing validation loss.

    \item \textbf{Rank examples by their validation loss.} 
    The trained reference models evaluate held-out examples from their respective complementary partitions ($\mathcal{D} \backslash \hat{\mathcal{D}}$). Each example is assessed three times using different reference models, and the mean validation loss is computed to rank the examples in ascending order. 

    \item \textbf{Align with the selected data.} 
    The easiest examples, comprising the lowest $\tau$ percent of validation loss rankings, are selected for alignment training. The alignment algorithm, such as DPO, is applied exclusively to these examples. To fully utilize the difficulty ranking, examples are processed sequentially from easy to difficult.
\end{itemize}


\begin{remark}[Flexible hyper-parameter $\tau$]
    The hyper-parameter $\tau$ determines the percentage of data selected for training. Optimal $\tau$ values depend on the data difficulty distribution and the model’s capacity. In practice, $\tau$ can be tuned using a third-party evaluator such as AlpacaEval 2~\cite{dubois2024length}. 
\end{remark}

For the evaluation in the next section, we set $\tau=50$ for the \textit{UltraFeedback-binarized} dataset, based on insights from Figure~\ref{fig:base-model-struggles}. For clarity and reproducibility, pseudo-code for Selective DPO is provided in Appendix~\ref{app:selective-dpo-code}.
