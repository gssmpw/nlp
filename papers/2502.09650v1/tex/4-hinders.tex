\section{Difficult Examples Hinder the Alignment}
\label{sec:hinders}
In this section, we first demonstrate that difficult examples significantly degrade alignment performance across various datasets and model scales. We then investigate the factors that contribute to their difficulty through a series of systematically designed empirical studies.

\subsection{Investigation Setup}
\textbf{Models:} We evaluate SFT models trained on the \textit{UltraChat-200k} dataset: \textbf{Mistral-7B-SFT}~\citep{jiang2023mistral}, \textbf{Qwen-2.5-7B-SFT}~\cite{qwen2.5}, \textbf{Llama3-8B-SFT}~\citep{llama3modelcard} and \textbf{Gemma-2-9B-SFT}~\citep{team2024gemma}. We focus on SFT models as they better demonstrate the effects of alignment procedures~\cite{meng2024simpo}. 

\textbf{Datasets:} We use \textit{UltraFeedback-binarized}, a widely adopted alignment dataset~\cite{tunstall2023zephyr,meng2024simpo,zhou2024wpo,pattnaik2024curry}, and \textit{Argilla-dpo-mix-7k}\footnote{\url{https://huggingface.co/datasets/argilla/dpo-mix-7k}}, a small but high-quality dataset.

\textbf{Hyper-parameters:} Following prior work, we set $\beta=0.01$~\cite{zhou2024wpo}. The learning rate is sweeped for DPO with random ordering and directly applied to DPO with other settings. We conduct the alignment with one epoch following~\citet{meng2024simpo}. 

\textbf{Evaluation:} We employ \textbf{WR$'$}, the win rate against \textit{gpt-4-turbo} on 805 testing examples from \textit{AlpacaEval 2}~\citep{dubois2024length} with \textit{ArmoRM}~\citep{wang2024interpretable}, a reward model with impressive performance on the RewardBench~\cite{lambert2024rewardbench}, as the evaluator.


\subsection{Difficult Examples Hinder Preference Alignment} 
As shown in Figure~\ref{fig:base-model-struggles}, training on difficult examples leads to significant performance declines. We compare three example-ordering strategies: (1) random ordering (standard DPO), (2) easy-to-difficult sorting by validation loss, and (3) random ordering with only easy examples. Despite using the same training recipes, models consistently perform better when trained on easier examples across all four architectures and both datasets. Notably, the benefits are mainly unlocked by excluding difficult examples rather than the ordering itself, as shown by the similar performance of sorted and shuffled easy examples (setting 2 and 3).


The performance drop from difficult examples is more pronounced in \textit{Ultrafeedback-binarized}. This aligns the observation that \textit{Ultrafeedback-binarized} contains mislabeled examples~\cite{ultrafeedback_preferences, notus2023} and \textit{Argilla-dpo-mix-7k} is characterized by high-quality data. 

\begin{figure*}[!ht]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/mislabeled.pdf}
        \vspace{-0.7cm}
        \caption{Label flipping.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/distribution-shift.pdf}
        \vspace{-0.7cm}
        \caption{Distribution shift.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fig/improper-learning-rate.pdf}
    \vspace{-0.7cm}
    \caption{Improper learning rate.}
    \end{subfigure}
    \vspace{-0.2cm}
    \caption{
    \textbf{Difficulty examples are not necessarily data errors.} \underline{\textit{(a)}}: flipping the last 40\% examples with higher \textit{validation loss}. \underline{\textit{(b)}}: sorting the examples with the \textit{$\epsilon$-greedy sorting} algorithm. In this case, each mini-batch data contains (1-$\epsilon$) part of easy-to-difficult examples and ($\epsilon$) part of randomly sampled examples. \underline{\textit{(c)}}: increasing and decreasing the learning rate. 
    All experiments are conducted on the Mistral-7B-SFT model with \textit{Argilla-dpo-mix-7k} dataset.
    }
    \label{fig:insights}
\end{figure*}


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.97\linewidth]{fig/scaling_law_qwen_argilla_armo_detailed_figs.pdf}\vspace{-0.2cm}
    \caption{
    \textbf{Difficult examples benefit larger models with greater capacities.} Examples are sorted by their \textit{validation loss}, ranging from easy to difficult. We fit the measured \textbf{WR$'$} (scatter points) using a second-degree polynomial (dashed line), identifying the peak of each parabola as the \textit{sweet spot} (marker).
    Notably, larger models reach sweet spots at higher data percentages, indicating that model with greater capacity can manage more challenging examples. The results are from ten runs per model type, evaluated using \textit{ArmoRM}~\cite{wang2024interpretable}. 
    }
    \vspace{-0.4cm}
    \label{fig:scaling_law}
\end{figure*}

