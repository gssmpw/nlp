
\setlength{\tabcolsep}{2pt}
\begin{table*}[!t]
\centering
\small 
\caption{Benchmarking results from AlpacaEval 2~\cite{dubois2024length}, Arena-Hard~\cite{li2024crowdsourced}, and MT-Bench~\cite{zheng2023judging}. In AlpacaEval 2, \textbf{WR} and \textbf{LC} indicate the win rate and length-controlled win rate against GPT-4-Turbo. In Arena-Hard, \textbf{WR} represents the win rate against GPT-4-0314, with GPT-4-Turbo serving as the evaluator. MT-Bench scores the quality of generated responses on a scale from 1 to 10, using either GPT-4 or GPT-4-Turbo as the evaluator. All results are based on full parameter fine-tuning (FPFT), except for the row labeled with LoRA~\cite{hulora}. We run this comparison on the \textit{UltraFeedback-binarized} dataset. 
}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
\multirow{3}{*}{\textbf{Method}} & \multicolumn{5}{c}{\textbf{Mistral-7B-SFT}} & \multicolumn{5}{c}{\textbf{Llama-3-8B-SFT}} \\ 
\cmidrule(lr){2-6}\cmidrule(lr){7-11}
& \multicolumn{2}{c}{\textbf{AlpacaEval 2}} & \multicolumn{1}{c}{\textbf{Arena-Hard}} & \multicolumn{2}{c}{\textbf{MT-Bench}} & \multicolumn{2}{c}{\textbf{AlpacaEval 2}} & \multicolumn{1}{c}{\textbf{Arena-Hard}} & \multicolumn{2}{c}{\textbf{MT-Bench}} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}\cmidrule(lr){9-9}\cmidrule(lr){10-11} 
& {\scriptsize \bf LC (\%)} & {\scriptsize \bf WR (\%)} & {\scriptsize \bf WR (\%)} & {\scriptsize \bf GPT-4 Turbo} & {\scriptsize \bf GPT-4} & {\scriptsize \bf LC (\%)}  & {\scriptsize \bf WR (\%)} & {\scriptsize \bf WR (\%)} & {\scriptsize \bf GPT-4 Turbo} & {\scriptsize \bf GPT-4} \\
\midrule
SFT &  8.4 & 6.2 & 1.3 & 4.8 & 6.3 & 6.2 & 4.6 & 3.3 & 5.2 & 6.6 \\
DPO~\cite{rafailov2024direct} & 15.1 & 12.5 & 10.4 & 5.9 & 7.3 & 18.2 & 15.5 & 15.9 & 6.5 & 7.7 \\
\ + Label Flipping~\cite{wang2024secrets} & 15.4 & 13.1 & 10.9 & -  & 7.3 & 19.1 & 15.9 & 16.2 & - & 7.7 \\
\ + Label Smoothing~\cite{mitchell2023note} & 15.2 & 12.7 & 10.2 & -  & 7.3 & 17.7 & 14.8 & 15.7 & - & 7.6 \\
\midrule 
RRHF~\cite{yuan2023rrhf}   & 11.6 & 10.2 &  5.8 & 5.4 & 6.7 & 12.1 & 10.1 &  6.3 & 5.8 & 7.0 \\
SLiC-HF~\cite{Zhao2023SLiCHFSL} & 10.9 &  8.9 &  7.3 & 5.8 & \textbf{7.4} & 12.3 & 13.7 &  6.0 & 6.3 & 7.6 \\
IPO~\cite{Azar2023AGT} & 11.8 & 9.4 & 7.5 & 5.5 & 7.2 & 14.4 & 14.2 & 17.8 & 6.5 & 7.4 \\
CPO~\cite{xu2024contrastive} &  9.8 &  8.9 &  6.9 & 5.4 & 6.8 & 10.8 &  8.1 &  5.8 & 6.0 & 7.4 \\
KTO~\cite{Ethayarajh2024KTOMA} & 13.1 & 9.1 & 5.6 & 5.4 & 7.0 & 14.2 & 12.4 & 12.5 & 6.3 & \textbf{7.8}  \\
ORPO~\cite{Hong2024ORPOMP} & 14.7 & 12.2 & 7.0 & 5.8 & 7.3 & 12.2 & 10.6 & 10.8 & 6.1 & 7.6 \\
R-DPO~\cite{Park2024DisentanglingLF} & 17.4 & 12.8 & 8.0 & 5.9 & \textbf{7.4} & 17.6 & 14.4 & 17.2 & 6.6 & 7.5 \\
SimPO~\cite{meng2024simpo} & 21.5 & 20.8 & 16.6 & 6.0 & 7.3 & 22.0 & 20.3 & \textbf{23.4} & 6.6 & 7.7 \\
WPO~\cite{zhou2024wpo} & 24.4 & 23.7 & \textbf{16.7} & - & \textbf{7.4} & \textbf{23.1} & \textbf{22.2} & 23.1 & - & 7.7 \\ 
\midrule
\textbf{Selective DPO} (Ours w/ LoRA) & \textbf{25.4} & \textbf{27.4} &  16.2 & \textbf{-} & 7.3 & 21.1 & {18.3} & 22.7 & \textbf{-} & \textbf{7.8} \\
\textbf{Selective DPO} (Ours) & \textbf{27.1} & \textbf{28.9} & \textbf{17.0} & \textbf{-} & \textbf{7.4} & \textbf{24.9} & \textbf{25.3} & \textbf{24.1} & \textbf{-} & \textbf{8.0} \\
\bottomrule
\end{tabular}
}
\label{tab:benchmarking-results}
%\vspace{-.5em}
\end{table*}
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{fig/additional_benchmarking_results.pdf}
    \vspace{-0.8cm}
    \caption{Comparison results against SimPO and WPO, with all methods tuned for their learning rates. Selective DPO (S$^{+}$DPO) demonstrates superior performance in win rate (WR) and comparable results in length-controlled win rate (LC).}
    \vspace{-0.4cm}
    \label{fig:additional-benchmarking-results}
\end{figure*}

\section{Experiments}
\label{sec:benchmarking}
We evaluate the benefits of the proposed preference data selection principle by benchmarking the Selective DPO algorithm, against state-of-the-art alignment algorithms using the formal benchmarks: \textit{AlpacaEval 2}~\cite{dubois2024length}, \textit{Arena-Hard v0.1}~\cite{li2024crowdsourced}, and \textit{MT-Bench}~\cite{zheng2023judging}. We report scores following each benchmark’s evaluation protocol. 


\subsection{Performance Comparison}

\myparagraph{Baselines.} We compare Selective DPO with DPO~\cite{rafailov2024direct} and its variants, including IPO~\cite{Azar2023AGT}, KTO~\cite{Ethayarajh2024KTOMA}, and ORPO~\cite{Hong2024ORPOMP}, borrowing their results from the SimPO paper~\cite{meng2024simpo} to ensure consistency. For SimPO and WPO~\cite{zhou2024wpo}, we rerun the released code on Llama, Gemma, and Qwen models. Hyper-parameter tuning is performed on the learning rate for all runs, see Appendix~\ref{app:experiment_details}. 
Additional baselines include techniques designed to address noisy labels, such as \textit{label flipping} and \textit{label smoothing}. Label flipping corrects mislabeled data identified by the \textit{ArmoRM} reward model, while label smoothing assumes the dataset label is correct with probability 0.6. 


\myparagraph{Results (Table~\ref{tab:benchmarking-results} and Figure~\ref{fig:additional-benchmarking-results}).} Table~\ref{tab:benchmarking-results} compares results on the Mistral-7B~\cite{jiang2023mistral} and Llama-3-8B~\cite{llama3modelcard} models. Label flipping yields only marginal gains, supporting our insight that difficult examples are not necessarily data errors. In contrast, Selective DPO, which carefully selects 50\% of the training data, significantly outperforms all baselines across all three benchmarks, demonstrating the strength of our data selection principle for alignment tasks. 
Figure~\ref{fig:additional-benchmarking-results} extends the comparison to Gemma-2-9B~\cite{team2024gemma} and Qwen-2.5-7B~\cite{qwen2.5}, showing exceptional performance in win rate (WR) on AlpacaEval 2 and comparable performance on length-controlled win rate (LC). The slightly lower performance on LC is consistent with results in Table~\ref{tab:benchmarking-results}, where Selective DPO demonstrates better performance under WR. 

We emphasize that our goal is not to propose the best ever alignment algorithm, but to verify the proposed data selection principle for alignment: selecting examples that align with the model's capacity. The length exploitation issue, while beyond the scope of this paper, could potentially be addressed using techniques from SimPO~\cite{meng2024simpo} or WPO~\cite{zhou2024wpo}, which we leave as future work.

\begin{figure*}
    \centering
    % \begin{subfigure}[t]{0.24\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{fig/ablation-study-reference-models.pdf}
    %     \vspace{-0.6cm}
    %     \caption{}
    % \end{subfigure}
    %     \centering
    % \begin{subfigure}[t]{0.24\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{fig/ablation-study-tau.pdf}
    %     \vspace{-0.6cm}
    %     \caption{}
    % \end{subfigure}
    %     \centering
    % \begin{subfigure}[t]{0.24\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{fig/indepth-analysis-nlls.pdf}
    %     \vspace{-0.6cm}
    %     \caption{}
    % \end{subfigure}
    % \begin{subfigure}[t]{0.24\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{fig/indepth-analysis-reward-margin.pdf}
    %     \vspace{-0.6cm}
    %     \caption{}
    % \end{subfigure}
    \includegraphics[width=\textwidth]{fig/ablation-study-and-indepth-analysis.pdf}
    \vspace{-0.8cm}
    \caption{\textbf{Hyper-parameter study and in-depth analysis of Selective DPO.}  \underline{\textit{(a)}}: Relationship between the number of reference models and performance. \underline{\textit{(b)}}: Performance with different percentages of selected easy examples. \underline{\textit{(c)}}: Negative log-likelihoods distributions on the generated responses. \underline{\textit{(d)}}: Reward margin distributions of the implicit reward models.}
    \label{fig:ablation-and-analysis}
    \vspace{-0.4cm}
\end{figure*}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/weak-to-strong-curricula.pdf}
    \vspace{-0.8cm}
    \caption{\textbf{Weak-to-strong curriculum under-performs.}  Aligning a 7B model with examples ordered by 3B reference models yields compromised results.}
    \label{fig:weak-to-strong-curriculum}
    \vspace{-0.4cm}
\end{figure}


\subsection{Hyper-Parameter Study}
Selective DPO introduces two implicit hyper-parameters. \textbf{Number of reference models (Figure~\ref{fig:ablation-and-analysis} (a))}: Increasing the number of reference models used to compute the validation loss improves performance on \textit{AlpacaEval 2} (LC). However, considering computational costs, training six reference models strikes a balance between performance and efficiency. 
\textbf{Percentage of selected easy examples (Figure~\ref{fig:ablation-and-analysis} (b))}: 
Increasing $\tau$ incorporates examples exceeding the model’s capacity, leading to performance degradation, while excessively low values limit training to the simplest examples, also resulting in suboptimal performance.

\subsection{In-Depth Analysis of DPO vs. Selective DPO}
Selective DPO outperforms DPO in terms of likelihood distribution and reward margin distribution. As shown in Figure~\ref{fig:ablation-and-analysis}(c), Selective DPO achieves a distribution of negative log-likelihoods (NLLs) closer to zero on test prompts, indicating higher confidence in generated responses. Additionally, the implicit reward model learned by Selective DPO exhibits better accuracy and larger reward margins on testing examples (Figure~\ref{fig:ablation-and-analysis}(d)). Together, these results suggest that by filtering out overly difficult examples, Selective DPO produces more robust reward models and reduces undesired hallucinations.

\subsection{Weak-to-Strong Curriculum}
To investigate whether difficult examples can be identified using smaller reference models, we compare alignment experiments where a 7B SFT model is trained with its own curriculum versus a curriculum derived from a smaller 3B model. 
Results in Figure~\ref{fig:weak-to-strong-curriculum} show moderate benefits from the smaller model’s curriculum, though slightly inferior to the model’s own curriculum. This suggests that while smaller models can provide insights, data selection remains more effective when tailored to the target model’s capacity.
