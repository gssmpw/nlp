\clearpage
\appendix
\onecolumn

\section{Pseudo-Code for the Instantiated Algorithm: Selective DPO}
\label{app:selective-dpo-code}
\begin{algorithm}[!h]
\caption{Selective DPO}
\label{algo:selective_dpo}
    \begin{algorithmic}
    \STATE {\bfseries Input:} \\
    $\boldsymbol{\pi_\text{SFT}}$: An SFT model that serves as the starting point for preference alignment. \\
    $D$: A dataset consisting of preference examples. \\ 
    $\textit{RandomSampler}$: A utility for sampling elements randomly without replacement. \\ 
    $\textit{SequentialSampler}$: A utility for sampling elements sequentially. \\ 
    $\mathcal{L_{\text{DPO}}}$: DPO loss function with the form: $\mathcal{L_{\text{DPO}}}(x,y_w,y_l) = - \log \sigma \Big(\beta \log \frac{\pi_{\vthe} (y_w | x)}{\pi_{\text{ref}}(y_w | x)} - \beta \log \frac{\pi_{\vthe}(y_l | x)}{\pi_{\text{ref}}(y_l | x)} \Big)$
    
    \ \\ 
    \# Step 1: Train six reference alignment models: 
    $\mathbf{\pi_{\vthe_{01}}}$, $\mathbf{\pi_{\vthe_{02}}}$,  $\mathbf{\pi_{\vthe_{11}}}$, $\mathbf{\pi_{\vthe_{12}}}$,  $\mathbf{\pi_{\vthe_{21}}}$, $\mathbf{\pi_{\vthe_{22}}}$. 
    
    \FOR{$t = 0, 1, 2$}
        \STATE Randomly split the dataset $D$ into two subsets, $D_1$ and $D_2$.
        \STATE Initialize $\mathbf{\pi_{\text{ref}}} \leftarrow  \mathbf{\pi_{\text{SFT}}}$ and $\mathbf{\pi_{\vthe}} \leftarrow  \mathbf{\pi_{\text{SFT}}}$. 
        
        \WHILE{\textit{RandomSampler} has not finished}
            \STATE Sample a mini-batch of examples from $D_1$ using \textit{RandomSampler}.
            \STATE Update $\mathbf{\pi_{\vthe}}$ by minimizing the DPO loss function:
            $\mathbf{\pi_{\vthe}} \leftarrow \arg\min_{\pi_{\vthe}} \mathbb{E}_{(x,y_w,y_l) \sim D_1} \big[\mathcal{L}_{\text{DPO}} (x,y_w,y_l)\big]$ \\ 

        \ENDWHILE
        
        \STATE Save the model: $\mathbf{\pi_{\vthe_{t1}}} \leftarrow \mathbf{\pi_{\vthe}}$.
        \STATE Reinitialize: $\mathbf{\pi_{\vthe}} \leftarrow  \mathbf{\pi_{\text{SFT}}}$. 
        
        \WHILE{\textit{RandomSampler} has not finished}
            \STATE Sample a mini-batch of examples from $D_2$ using \textit{RandomSampler}.
            \STATE Update $\mathbf{\pi_{\vthe}}$ by minimizing the DPO loss function:
            $\mathbf{\pi_{\vthe}} \leftarrow \arg\min_{\pi_{\vthe}} \mathbb{E}_{(x,y_w,y_l) \sim D_2} \big[\mathcal{L}_{\text{DPO}} (x,y_w,y_l)\big]$ \\ 
        \ENDWHILE
        
        \STATE Save the model: $\mathbf{\pi_{\vthe_{t2}}} \leftarrow \mathbf{\pi_{\vthe}}$.
        \STATE Reinitialize: $\mathbf{\pi_{\vthe}} \leftarrow  \mathbf{\pi_{\text{SFT}}}$. 
    \ENDFOR
    
    \ \\ 
    
    \# Step 2: Rank examples by their validation loss.
    \FOR{each example $(x, y_w, y_l)$ in dataset $D$} 
        \STATE Compute the validation loss using the three held-out reference alignment models:
        \[
        \text{VL} (x, y_w, y_l) = \E_{\pi_{\vthe} \sim (\pi_{\vthe_{01}} \text{ or } \pi_{\vthe_{02}}, \pi_{\vthe_{11}} \text{ or } \pi_{\vthe_{12}}, \pi_{\vthe_{21}} \text{ or } \pi_{\vthe_{22}})} 
        \Big[
        \mathcal{L}_{\text{DPO}}(x,y_w,y_l)
        \Big].
        \]
    \ENDFOR

    \STATE Sort the examples by their validation losses and select the top 50\% of examples to form $D_\text{selected}$.
    
    \ \\ 
    
    \# Step 3: Conduct alignment on the selected data $D_\text{selected}$.
    \WHILE{\textit{SequentialSampler} has not finished}
        \STATE Sample a mini-batch of examples from $D_\text{selected}$ using \textit{SequentialSampler}.
        \STATE Update $\mathbf{\pi_{\vthe}}$ by minimizing the DPO loss function:
            $\mathbf{\pi_{\vthe}} \leftarrow \arg\min_{\pi_{\vthe}} \mathbb{E}_{(x,y_w,y_l) \sim D_{\text{selected}}} \big[\mathcal{L}_{\text{DPO}} (x,y_w,y_l)\big]$ \\ 
    \ENDWHILE

    \ \\ 
    
    \STATE {\bfseries Output:} \\
    $\mathbf{\pi_{\vthe}}$: The aligned model obtained by Selective DPO.
    
\end{algorithmic}
\end{algorithm}


\clearpage
\input{tex/7-related-work}


\clearpage
\section{Experiment Details}
\label{app:experiment_details}
\subsection{Computation Environment}
All training experiments in this paper were conducted on compute nodes equipped with 8 $\times$ H100 GPUs. To facilitate reproduction with limited computational resources, we also provide key benchmarking results for selected models trained using 4 $\times$ A100 40G GPUs with LoRA. Reproducing our SelectiveDPO on 7B models takes about 8 GPU hours (H100).


\subsection{SFT Hyper-Parameters}
In this work, we limited our alignment experiments to SFT models, which is expected to better demonstrate the effects of different preference alignment procedures. We prepared these SFT models using the the UltraChat-200k dataset. We try our best to use the SFT models from community to facilitate the reproduction. However, there were no available SFT checkpoints for some pre-trained models (\textit{e.g.,} Qwen-2.5 models). We in this part list the hyper-parameters for training these community-released SFT models as well as the SFT models trained by ourselves in Table~\ref{tab:sft-recipes}.
\vspace{-0.2cm}
\begin{table}[h]
\centering
\caption{Training recipes for SFT models used in our experiments.}
\renewcommand{\arraystretch}{1.2} 
\resizebox{\columnwidth}{!}{
\begin{tabular}{l l l l l l l }
\toprule
\textbf{SFT Model Name}  & \bf{Base Model Name}& \bf{Batch Size} & \bf{Learning Rate} & \bf{Epoch} & \bf{Optimizer} & \bf{LoRA?}\\
\midrule
Qwen-2.5-3B-SFT                                                                              & Qwen/Qwen2.5-3B            & 128        & 2e-5          & 1     & Adam    & No  \\
Qwen-2.5-7B-SFT                                                                              & Qwen/Qwen2.5-7B            & 128        & 1e-5          & 1     & Adam    & No  \\
Qwen-2.5-14B-SFT                                                                             & Qwen/Qwen2.5-14B           & 128        & 5e-6          & 1     & Adam    & No  \\
\begin{tabular}[c]{@{}l@{}}Mistral-7B-SFT (HuggingFaceH4/mistral-7b-sft-beta)\end{tabular} & mistralai/Mistral-7B-v0.1  & 128        & 2e-5          & 1     & Adam     & No \\
\begin{tabular}[c]{@{}l@{}}Llama-3-8B-SFT (princeton-nlp/Llama-3-Base-8B-SFT)\end{tabular} & meta-llama/Meta-Llama-3-8B & 128        & 2e-5          & 1     & Adam    & No  \\
\begin{tabular}[c]{@{}l@{}}Gemma-2-9B-SFT (tanliboy/zephyr-gemma-2-9b-sft)\end{tabular}    & google/gemma-2-9b          & 128        & 3e-6          & 1     & Adam   & No  \\
\bottomrule
\end{tabular}
}
\label{tab:sft-recipes}
\end{table}


\subsection{Key Hyper-Parameters for Alignment}
\paragraph{Figure~\ref{fig:base-model-struggles}} We conducted a series of alignment experiments with LoRA on two datasets for generating Figure~\ref{fig:base-model-struggles}. Key hyper-parameters used in the 
\textit{Argilla-dpo-mix-7k} experiments are listed in Table~\ref{tab:alignment-recipes-struggles-argilla} where we report the sweep range and the selected best learning rate for DPO with bold font. These parameters are then directly applied to other two settings (sorted and selected by VL) for generating Figure~\ref{fig:base-model-struggles}. The key parameters used for the UltraFeedback-binarized  dataset are list in Table~\ref{tab:alignment-recipes-struggles-uf}.
\vspace{-0.2cm}
\begin{table}[h]
\centering
\caption{Key hyper-parameters used for aligning models on the \textbf{argilla-7k} dataset: Figure~\ref{fig:base-model-struggles}, top.}
\renewcommand{\arraystretch}{1.3} 
\begin{tabular}{l l l l l l l}
\toprule
\textbf{Model For Alignment} & \textbf{Learning Rate} & \textbf{Batch Size} & \textbf{$\beta$} & \textbf{Epoch} & \textbf{Optimizer} &\textbf{LoRA?}\\ 
\midrule
Mistral-7B-SFT & 2e-5, 3e-5, 5e-5, \textbf{1e-4}, 2e-4 & 64 & 0.01 & 1 & paged\_adamw\_32bit & Yes\\ 
Qwen-2.5-7B-SFT & 2e-5, 3e-5, \textbf{5e-5}, 1e-4, 2e-4 & 64 & 0.01 & 1 & paged\_adamw\_32bit & Yes\\ 
Llama-3-8B-SFT & 5e-5, 1e-4, 2e-4, \textbf{3e-4}, 5e-4 & 64 & 0.01 & 1 & paged\_adamw\_32bit & Yes\\ 
Gemma-2-9B-SFT & 1e-5, \textbf{2e-5}, 3e-5, 5e-5, 1e-4 & 64 & 0.01 & 1 & paged\_adamw\_32bit & Yes\\ 
\bottomrule
\end{tabular}
\label{tab:alignment-recipes-struggles-argilla}
\end{table}

\vspace{-0.2cm}
\begin{table}[h]
\centering
\caption{Key hyper-parameters used for aligning models on the \textbf{ultrafeedback-bianrized} dataset: Figure~\ref{fig:base-model-struggles}, bottom.}
\renewcommand{\arraystretch}{1.2} 
\resizebox{\columnwidth}{!}{
\begin{tabular}{l l l l l l l}
\toprule
\textbf{Model For Alignment} & \textbf{Learning Rate} & \textbf{Batch Size} & \textbf{$\beta$} & \textbf{Epoch} & \textbf{Optimizer} & \textbf{LoRA?}\\ 
\midrule
Mistral-7B-SFT & 1e-6, 3e-6, 5e-6, \textbf{8e-6}, 10e-6 & 64 & 0.01 & 1 & paged\_adamw\_32bit  & Yes\\ 
Qwen-2.5-7B-SFT & 1e-6, \textbf{3e-6}, 5e-6, 8e-6, 10e-6 & 64 & 0.01 & 1 & paged\_adamw\_32bit & Yes\\ 
Llama-3-8B-SFT & 1e-6, 3e-6, 5e-6, 8e-6, \textbf{10e-6} & 64 & 0.01 & 1 & paged\_adamw\_32bit & Yes\\ 
Gemma-2-9B-SFT & 1e-6, 3e-6, \textbf{5e-6}, 8e-6, 10e-6 & 64 & 0.01 & 1 & paged\_adamw\_32bit & Yes\\ 
\bottomrule
\end{tabular}
}
\label{tab:alignment-recipes-struggles-uf}
\end{table}

\clearpage
\paragraph{Table~\ref{tab:benchmarking-results}} Comparison results of this table are mainly borrowed from the SimPO paper~\cite{meng2024simpo}. All results are runed with full parameter fine-tuning (FPFT) expect for the row labeled with LoRA. We added the results of our \textit{Selective DPO} pipeline using the configurations detailed in the following table. The inclusion of LoRA results is to facilitate the reproduction for practices with limited resources. 

\vspace{-0.2cm}
\begin{table}[h]
\centering
\caption{Key hyper-parameters used for aligning models on the \textbf{ultrafeedback-bianrized} dataset: Figure~\ref{fig:base-model-struggles}, bottom.}
\renewcommand{\arraystretch}{1.2} 
\resizebox{\columnwidth}{!}{
\begin{tabular}{l l l l l l l}
\toprule
\textbf{Experiment Name} & \textbf{Learning Rate} & \textbf{Batch Size} & \textbf{$\beta$} & \textbf{Epoch} & \textbf{Optimizer} & \textbf{LoRA?}\\ 
\midrule
SelectiveDPO \& Mistral-7B-SFT \& LoRA & 8e-6 & 64 & 0.01 & 1 & paged\_adamw\_32bit & Yes\\ 
SelectiveDPO \& Mistral-7B-SFT & 2e-7, 5e-7, \textbf{1e-6}, 2e-6, 3e-6 & 128 & 0.01 & 1 & paged\_adamw\_32bit & No\\ 
SelectiveDPO \& Llama-3-8B-SFT \& LoRA & 10e-6 & 64 & 0.01 & 1 & paged\_adamw\_32bit & Yes\\ 
SelectiveDPO \& Llama-3-8B-SFT & 2e-7, 5e-7, \textbf{1e-6}, 2e-6, 3e-6 & 128 & 0.01 & 1 & paged\_adamw\_32bit & No\\ 
WPO \& Llama-3-8B-SFT & 5e-7, \textbf{1e-6}, 2e-6 & 128 & 0.01 & 1 & paged\_adamw\_32bit & No \\ 
\bottomrule
\end{tabular}
}
\label{tab:alignment-recipes-benchmarking}
\end{table}

\paragraph{Figure~\ref{fig:additional-benchmarking-results}} Comparison results of this figure are from runs with full parameter fine-tuning. We rerun two state-of-the-art alignment algorithm, SimPO~\cite{meng2024simpo} and WPO~\cite{zhou2024wpo} with hyper-parameter sweeping on the learning rate. Other hyper-parameter configurations follows the suggestion from their paper. Specifically:
\vspace{-0.2cm}
\begin{table}[h]
\centering
\caption{Key hyper-parameters used for generating comparison in Figure~\ref{fig:additional-benchmarking-results}.}
\renewcommand{\arraystretch}{1.2} 
\resizebox{\columnwidth}{!}{
\begin{tabular}{l l l l l l l l}
\toprule
\textbf{Experiment Name} & \textbf{Learning Rate} & \textbf{Batch Size} & \textbf{Epoch} & \textbf{Optimizer} & \textbf{Other Hyper-Parameters} & \textbf{LoRA?}\\ 
\midrule
WPO \& Qwen-7B-SFT & 5e-7, \textbf{1e-6}, 2e-6, & 128 & 1 & paged\_adamw\_32bit & $\beta=0.01$ & No \\ 
WPO \& Gemma-9B-SFT & 2e-7, \textbf{5e-7}, 1e-6 & 128 & 1 & paged\_adamw\_32bit & $\beta=0.01$ & No\\ 
SimPO \& Qwen-7B-SFT & 6e-5, \textbf{8e-6}, 1e-5 & 128 & 1 & paged\_adamw\_32bit & $\beta=2$, $\frac{\gamma}{\beta} = 0.8$ & No\\ 
SimPO \& Gemma-9B-SFT & 5e-7, \textbf{1e-6}, 2e-6 & 128 & 1 & paged\_adamw\_32bit & $\beta=2$, $\frac{\gamma}{\beta} = 0.8$ & No\\ 
SelectiveDPO \& Qwen-7B-SFT & 5e-7, \textbf{8e-7}, 1e-6 & 128 & 1 & paged\_adamw\_32bit & $\beta=0.01$ & No\\ 
SelectiveDPO \& Gemma-9B-SFT & 2e-7, \textbf{3e-7}, 5e-7 & 128 & 1 & paged\_adamw\_32bit & $\beta=0.01$ & No\\ 

\bottomrule
\end{tabular}
}
\label{tab:alignment-recipes-additional-comparison}
\end{table}


\subsection{LoRA Configuration for Alignment}
We conduct all our analytics experiments using LoRA. Its detailed configurations are described in Table~\ref{tab:lora-configuration}. 
\vspace{-0.4cm}
\begin{table}[h]
\centering
\caption{LoRA configuration for all analytics experiments.}
\renewcommand{\arraystretch}{1.4} 
\begin{tabular}{c c}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
\text{load\_in\_4bit} &  false \\
lora\_r & 16 \\ 
lora\_alpha & 16  \\ 
lora\_dropout & 0.05 \\ 
lora\_target\_modules & q\_proj,k\_proj,v\_proj,o\_proj,gate\_proj,up\_proj,down\_proj \\
\bottomrule
\end{tabular}
\label{tab:lora-configuration}
\end{table}



\subsection{Decoding Configuration}
\paragraph{AlpacaEval 2.} For this benchmark, we employ sampling-based decoding strategies, configuring the temperature as follows: 0.7 for Mistral models, 0.9 for Llama-3 models,  0.5 for Gemma-2 models, and 0.7 for Qwen-2.5 models. These configurations align with standard practices in the community.
\vspace{-0.3cm}
\paragraph{Arena-Hard.} For this benchmark, we utilize default greedy decoding across all settings, as outlined in~\citet{meng2024simpo}.
\vspace{-0.3cm}
\paragraph{MT-Bench.} We adapt the official decoding configuration, which varies sampling temperatures for different models.

\clearpage
\section{Downstream Task Evaluation}
To examine how the proposed selective preference optimization pipeline affects downstream task performance, we evaluate the instantiated algorithm, \textit{Selective DPO}, alongside other baseline algorithms on various tasks listed in the HuggingFace Open Leaderboard~\cite{open-llm-leaderboard}. Results, following established evaluation protocols, are presented in Table~\ref{tab:down-stream-task}.

\setlength{\tabcolsep}{6pt}
\begin{table}[h]
\centering
\caption{Downstream task evaluation results. The dataset is \textit{UltraFeedback-binarized}.}
\renewcommand{\arraystretch}{1.0} 
\resizebox{\columnwidth}{!}{
\begin{tabular}{l c c c c c c c}
\toprule
& \textbf{MMLU(5)} &  \textbf{Winograd(5)} & \textbf{GSM8K(5)} & \textbf{HellaSwag(10)} & \textbf{ARC(25)} &  \textbf{TruthfulQA(0)}  & \textbf{Average}\\ 
\midrule
\multicolumn{8}{c}{\textbf{Mistral-7B-Base}}  \\
\midrule
Base         &62.46	&78.93	&38.29	&83.38	&61.6	&42.64	&61.22 \\
SFT	         &59.77	&77.58	&40.71	&82.28	&58.19	&43.05	&60.26 \\ 
DPO	         &57.38	&77.35	&30.4	&83.58	&61.18	&53.11	&60.50 \\ 
SimPO        &58.43	&77.35	&32.3	&83.54	&61.95	&50.82	&60.73 \\ 
WPO	         &59.54	&78.69	&32.07	&85.23	&64.08	&51.04	&61.78 \\ 
SelectiveDPO &59.34 &76.16	&14.48	&83.25	&65.27	&51.95	&58.41 \\ 
SelectiveDPO(60\%) &59.54 &76.87	&28.58	&84.25	&65.96	&57.21	&62.07 \\ 
\midrule
\multicolumn{8}{c}{\textbf{Qwen-2.5-7B-Base}}  \\
\midrule
Base	     &74.16	&76.72	&82.18	&80.03	&63.23	&56.38	&72.12 \\ 
SFT	         &73.86	&75.77	&81.43	&80.71	&62.71	&55.67	&71.69 \\ 
DPO	         &74.06	&75.61	&82.79	&81.73	&65.70	&60.92	&73.47 \\ 
SimPO	     &74.33	&77.11	&85.22	&82.48	&68.09	&65.51	&75.45 \\ 
WPO	         &74.29	&75.85	&83.55	&83.2	&68.52	&65.09	&75.08 \\ 
SelectiveDPO &74.05	&75.85	&80.44	&82.82	&67.32	&63.80	&74.04 \\
\midrule
\multicolumn{8}{c}{\textbf{Llama-3-8B-Base}}  \\
\midrule
Base	     &65.14	&76.64	&48.45	&81.88	&58.87	&43.93	&62.49 \\ 
SFT	         &63.79	&76.64	&50.57	&81.40	&60.84	&45.33	&63.10 \\ 
DPO	         &63.47	&76.95	&54.81	&83.71	&64.51	&53.45	&66.15 \\ 
SimPO	     &63.18	&77.58	&47.76	&82.93	&65.44	&59.44	&66.06 \\
WPO	         &63.46	&76.72	&44.58	&84.14	&65.27	&53.84	&64.67 \\ 
SelectiveDPO &63.99	&76.48	&48.75	&83.51	&64.93	&51.34	&64.83 \\ 
\midrule
\multicolumn{8}{c}{\textbf{Gemma-2-9B-Base}}  \\
\midrule			
Base	     &70.29	&80.03	&40.41	&82.66	&67.83	&45.56	&64.46 \\
SFT	         &70.82	&78.77	&41.93	&83.53	&68.77	&48.04	&65.31 \\ 
DPO	         &71.17	&80.11	&44.43	&85.42	&71.33	&56.96	&68.24 \\ 
SimPO	     &72.16	&80.43	&42.53	&86.06	&73.12	&65.34	&69.94 \\ 
WPO	         &70.88	&79.40	&43.14	&85.64	&70.99	&53.44	&67.25 \\ 
SelectiveDPO &70.88	&79.56	&43.67	&85.30	&70.82	&54.67	&67.48 \\ 
\bottomrule
\end{tabular}
}
\label{tab:down-stream-task}
\end{table}

Overall, \textit{Selective DPO} performs comparably to other alignment algorithms, such as DPO and SimPO. However, we observe a notable performance drop in the Mistral-7B model when evaluated using the GSM8K~\cite{cobbe2021gsm8k} protocol.
Two primary factors contribute to this performance decrease: \textbf{Exclusion of difficult examples.} GSM8K predominantly evaluates mathematical skills, which often correspond to difficult examples (as detailed in Appendix~\ref{app:case_study_difficult_example}). Since \textit{Selective DPO} excludes such difficult examples to better align with human preferences, the model’s mathematical performance diminishes. \textbf{Formatting requirements}. GSM8K requires numerical answers in a specific format: \textsc{\#\#\# $<$\text{THE ANSWER}$>$}. We find that the Mistral-7B-Selective DPO model often generates correct answers but presents them in a human dialogue style, breaking the required format and reducing evaluation scores. 

We propose three potential solutions. First, as suggested by SimPO~\cite{meng2024simpo}, incorporating an auxiliary SFT loss to regularize model behavior could help regularize the model’s behavior, ensuring compatibility with downstream tasks. 
Second, using larger models with greater capacity mitigates this issue. For instance, Gemma-2-9B-SelectiveDPO demonstrates better performance and is unaffected by this issue. 
Finally, including more examples that cover mathematical questions could prevent the model from forgetting its mathematical capabilities while aligning with human preferences and dialogue styles. For example Selective DPO(60\%), which incorporates 10\% more difficult data, alleviates this issue.



\clearpage 
\section{What Defines Difficult Examples: Insights from Feature Analysis}
\label{app:feature-analysis}
\subsection{Can Length and Reward Margin Predict Example Difficulty?}
We include alternative measures that could potentially indicate example difficulty and evaluate their behavior across varying levels of difficulty.

\paragraph{Response length.} 
Response length may implicitly signal the complexity of generated answers, as longer responses often carry more information, potentially making them more challenging for the model. Two measures are defined: (1) \textit{Chosen Length}: The length of the chosen answer, $\text{len}(y_w)$; (2) \textit{Chosen Length $-$ Rejected Length}: The difference in lengths between the chosen and rejected answers: $\text{len}(y_w) - \text{len}(y_l)$. 

\paragraph{Reward margin by reward models.} 
Reward models, such as \textit{ArmoRM}~\cite{wang2024interpretable}, provide score margins that can indicate response difficulty. A large positive margin suggests an easy example, while a large negative margin may signal noisy or mislabeled data. Two measures are defined: (1) \textit{Chosen Score}: The reward score assigned to the chosen answer, $\text{rm}(x, y_w)$, and (2) \textit{Chosen Score - Rejected Score}: The difference in scores between the chosen and rejected answers: $\text{rm}(x, y_w) - \text{rm}(x, y_l)$. 


\paragraph{Reward margin by GPT-4.} GPT-4 can also act as an evaluator, assigning scores to responses. Similar measures are defined: (1) \textit{Chosen Rating}: The rating assigned to the chosen answer, $\text{GPT-4}(x,y_w)$, and (2) \textit{Chosen Rating - Rejected Rating}: The difference in ratings between the chosen and rejected answers: $\text{GPT-4}(x,y_w) - \text{GPT-4}(x,y_l)$. 

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{fig/feature_analysis.pdf}
    \vspace{-0.8cm}
    \caption{Comparison of \textit{response length} and \textit{reward margin} measures with \textit{validation loss} across three difficulty levels: Easy, Medium, and Difficult. The dataset examples are partitioned into these levels based on increasing \textit{validation loss}. While validation loss increases consistently with difficulty, alternative measures such as response length and reward margin (from reward models and GPT-4) exhibit no significant variation across these levels, indicating their limited effectiveness as proxies for difficulty.}
    \label{fig:feature-analysis}
\end{figure}

\paragraph{Comparison.} The distributions of these measures are shown in Figure~\ref{fig:feature-analysis}. The data are partitioned into three levels--Easy, Medium, and Difficult--based on increasing validation loss. 
Validation loss serves as the ground truth for difficulty due to its strong correlation with the learned step measure. Difficult examples tend to have longer responses and larger negative reward margins. 
However, these trends are not statistically significant, as evidenced by substantial overlaps in the distributions across difficulty levels. This suggests that while these measures provide some insight, they may not serve as robust standalone indicators of example difficulty. 



\clearpage
\subsection{Are Easy Examples for Small Models Still Easy for Larger Models?}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/jaccard-3b-14b.pdf}
    \includegraphics[width=0.8\linewidth]{fig/jaccard-7b-14b.pdf}
    \includegraphics[width=0.8\linewidth]{fig/jaccard-3b-7b.pdf}
    \caption{Easy examples identified by smaller models are likely also recognized as easy by larger models, and difficult examples identified by larger models are likely challenging for smaller models as well. Each heatmap compares a model's classifications(y-axis) against those of an oracle (another model, set on x-axis). Easy examples are defined as the first 63.7\%, 71.2\%, and 81.3\% of examples with the lowest validation loss for the 3B, 7B, and 14B models, respectively. Difficult examples comprise the remaining examples, with these thresholds drawn from the mean sweet spot in Figure~\ref{fig:scaling_law}. The dataset is \textit{Argilla-dpo-mix-7k}.}
    \label{fig:jaccard-3b-7b-14b}
\end{figure}

\clearpage
\section{What Defines Difficult Examples: Insights from Case Studies}
\label{app:case_study_difficult_example}
In this section, we examine examples with varying levels of difficulty, characterized by their validation losses. We select three representative examples around validation loss quantile: 0.0 (easiest), 0.6 (medium), and 1.0 (most difficult) from the~\textit{Argilla-dpo-mix-7k} dataset. 
We highlight examples near the 0.6 quantile as this range marks a critical transition point where the model's performance starts to decline. Our key observations are as follows:
\begin{itemize}
    \item \textbf{Easy examples are well-labeled and involve straightforward concepts (Table~\ref{tab:case_study_easy_examples}).} Easy examples shows that preferred responses are concise, clear, and of significantly higher quality than their rejected counterparts, which are often overly verbose and filled with excessive emojis. Unlike more difficult examples, these easy examples lack complex mathematical calculations or the use of languages other than English.
\end{itemize}
\begin{table}[!ht]
\centering
\caption{Three examples with lowest validation loss (Model: Mistral-7B-SFT \& Dataset: Argilla-dpo-7k).}
\label{tab:case_study_easy_examples}
\renewcommand{\arraystretch}{1.5} % Adjust row spacing
\begin{tabular}{@{}p{0.25\textwidth} | p{0.325\textwidth}@{} | p{0.375\textwidth}@{}}
\toprule
\textbf{Prompt} & \textbf{Chosen} & \textbf{Rejected}\\
\midrule

\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 
How can you determine the pH of rainwater?\\How can you determine the pH level of rainwater?\\Choose your answer. Are these two questions paraphrases of each other?\\OPTIONS:\\1). no.\\2). yes..}
\end{minipage}
 
 & 
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 
yes.\\Both questions are asking the same thing - how to find out the pH level of rainwater. They have simply been worded differently. Paraphrasing means to say something in a different way, and that's exactly what these two questions do, so they are paraphrases of each other
}
\end{minipage}
& 
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 
Ooh ooh! *excitedly* Hi there! *bouncy bouncy* I can definitely help you with that! *giggle*\\So, you wanna know how to determine the pH of rainwater, huh? *curious* Well, pH is like a special number that tells us how yucky or yummy something is. *giggle* And rainwater can be a little bit yucky sometimes, so we need to check its pH to make sure it's safe to drink. *concerned*\\To check the pH of rainwater, we can use a special tool called a pH meter\\\textit{...}
}
\end{minipage} \\
\midrule
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 
Please add spaces between words: Fixedroofrailingsrepresentanewandincreasinglyimportantproductarea, andoneinwhichthecompany'scustomersincludeOpel,
BMW,Volkswagen,NissanandFord.
}
\end{minipage}
 
 & 
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 
Fixed roof railings represent a new and increasingly important product area, and one in which the company's customers include Opel, BMW, Volkswagen, Nissan and Ford
}
\end{minipage}
& 
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize Ooh ooh ah ah! *giggle* Hi there! *bounce bounce* Do you want to know about something cool? *twirl twirl* It's called "Fixed Roof Railings"! *excited squeal* They're like special rails that go on cars! *bounce bounce*\\So, you know how cars have roofs, right? *look around* Like this one! *point* And sometimes, the roof is made of metal or something hard, right? *nod nod* Well, Fixed Roof Railings are like special rails that go on the roof of the car! *giggle* They\'re fixed, so they don't move around! *bounce bounce*\\ \textit{...}.}
\end{minipage} \\

\midrule

\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 
Based on the premise "Peace, amigo. Don Cazar\'s raised finger silenced his man.", can we conclude the hypothesis "Peace, friend, said Don Cazar, silencing the man with a finger." is true (see options)?\\ Available choices:\\ -yes;\\ -it is not possible to tell;\\ -no;}
\end{minipage}
 
 & 
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 

Yes, we can conclude the hypothesis "Peace, friend, said Don Cazar, silencing the man with a finger." is true.
}
\end{minipage}
& 
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 
Ooh ooh! *giggle* Hi there, little buddy! *smile* So, you wanna know if the hypothesis "Peace, friend, said Don Cazar, silencing the man with a finger" is true? *tongue out* Ooh, that\'s a good one! *nod*\\

Well, let me think... *thinking face* Ooh ooh, I know! *excited* The sentence "Peace, amigo" is like a magic word that makes the man stop talking, like when you say "Shhh" to a baby. *giggle* So, if Don Cazar said "Peace, amigo," it means he wants the man to be quiet, and that\'s why the man stopped talking! *smile*

So, the hypothesis is... *drumroll* ...true! *confetti* Yay! *high five* You\'re so smart for asking such a great question! *hug* Now, do you want to ask another question? *wink
}
\end{minipage} \\
\bottomrule
\end{tabular}
\end{table}

\clearpage
\begin{itemize}
    \item \textbf{Examples near the model's capacity are with correct labels but require complex skills (Table~\ref{tab:case_study_medium_examples}).} 
    The preferred examples at this level of validation loss consistently have correct labels. However, many tasks involve complex mathematical calculations, demanding the model’s numerical proficiency. Additionally, some examples require multilingual skills, including Japanese and Filipino, which are likely challenging for the Mistral-7B-SFT model to process effectively.
\end{itemize}
\begin{table}[!ht]
\centering
\caption{Three examples near the models' capacity (Model: Mistral-7B-SFT \& Dataset: Argilla-dpo-7k).}
\label{tab:case_study_medium_examples}
\renewcommand{\arraystretch}{1.5} % Adjust row spacing
\begin{tabular}{@{}p{0.25\textwidth} | p{0.35\textwidth}@{} | p{0.35\textwidth}@{}}
\toprule
\textbf{Prompt} & \textbf{Chosen} & \textbf{Rejected}\\
\midrule

\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 
62944725+438869736559012=? equations/math only, no commentary.}
\end{minipage}
 
 & 
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 62944725 + 438869736559012 = 438869799503737}
\end{minipage}
& 
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize The sum of the two numbers is:\\ 629,447,250 + 43,886,973,659,012 = 44,516,431,106,432}
\end{minipage} \\
\midrule
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize
``A team of cheerleaders cheering at a sporting event."\\ Based on this premise, can we conclude that the hypothesis ``Cheerleaders are laying down in the locker room." is true?\\Options:\\- yes\\- it is not possible to tell\\- no Step by step answer:}
\end{minipage}
 
 & 
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 
1. The premise states that the cheerleaders are cheering at a sporting event.\\2. This implies that they are actively participating and performing at the event.\\3. To do so, they must physically be present and engaged in the cheering activities.\\4. Hence, the hypothesis that the cheerleaders are laying down in the locker room cannot be true, since they are actively participating in the event.\\Answer: no
}
\end{minipage}
& 
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize  No, we cannot conclude that the hypothesis ``Cheerleaders are laying down in the locker room" is true based on the premise ``A team of cheerleaders cheering at a sporting event." Here\'s why:\\1. The premise only tells us that the cheerleaders are cheering at a sporting event, but it doesn\'t provide any information about their location or activity when they are not cheering.\\2. The hypothesis ``Cheerleaders are laying down in the locker room" is not a logical conclusion based on the premise. There is no evidence or reason to believe that the cheerleaders are laying down in the locker room, as this activity is not related to cheering at a sporting event.\\3. Therefore, the correct answer is ``it is not possible to tell" whether the hypothesis is true or false based on the premise}
\end{minipage} \\

\midrule

\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 
You will be given a definition of a task first, then some input of the task.\\Given a sentence in the Japanese, provide an equivalent translation in Filipino that retains the same meaning through the translation. In translation, keep numbers as it is.\\\begin{CJK}{UTF8}{min}
苦しい立場の検察官は、その問題における彼らの行いに関して司法省によって調査中である。\end{CJK}\\Output:
}
\end{minipage}
 
 & 
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 
Ang mga piskal sa mahirap na posisyon ay kasalukuyang pinag-iimbestigahan ng Kagawaran ng Hustisya tungkol sa kanilang mga ginawa sa problemang iyon.
}
\end{minipage}
& 
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize Ensayado muna ang miyembro ng paglagdaan sa isang bagong problema darating sa kanilang pamamahala.
}
\end{minipage} \\
\bottomrule
\end{tabular}
\vspace{-0.3cm}
\end{table}




\clearpage
\begin{itemize}
    \item \textbf{The most difficult examples are likely mislabeled, exhibit distribution shifts, or involve complex concepts (Table~\ref{tab:case_study_most_difficult_examples}).} First, some preferred examples in this category are incorrectly labeled, as seen in the first and second examples in the table. Second, there is a noticeable distribution shift in this subset of the data. For instance, while the preferred responses in the easy examples (Table~\ref{tab:case_study_easy_examples}) are clear and concise, the preferred responses in this table are verbose and cluttered with unnecessary emojis. Finally, the presence of complex concepts further increases the difficulty. Together, these factors make this subset of examples the most challenging to understand.
\end{itemize}
\begin{table}[!ht]
\vspace{-0.3cm}
\centering
\caption{Three examples with highest validation loss (Model: Mistral-7B-SFT \& Dataset: Argilla-dpo-7k).}
\label{tab:case_study_most_difficult_examples}
\renewcommand{\arraystretch}{1.5} % Adjust row spacing
\begin{tabular}{@{}p{0.25\textwidth} | p{0.4\textwidth}@{} | p{0.3\textwidth}@{}}
\toprule
\textbf{Prompt} & \textbf{Chosen} & \textbf{Rejected}\\
\midrule

\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 
Please add punctuation to this: Stunning coastal world Ferry trip along the Inside Passage\\Punctuation version:
}
\end{minipage}
 
 & 
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 
Sure! Here's the punctuated version of your task:\\``Stunning coastal world! Ferry trip along the Inside Passage."\\Now, let me provide you with a detailed and long answer to this task. \\The Inside Passage is a breathtakingly beautiful coastal route that stretches along the Pacific coast of North America, spanning from northern Washington state to southern Alaska. This scenic waterway is dotted with charming coastal towns, rugged fjords, and majestic glaciers, making it an unforgettable destination for any traveler.\\...
}
\end{minipage}
& 
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 
``Stunning Coastal World: Ferry Trip Along the Inside Passage."
}
\end{minipage} \\
\midrule
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 
How to apply for tickets for the fifa world cup? Apply for your passport. Encourage anyone who wants to attend the world cup with you to do so as well. You will need a valid passport number in order to apply for tickets.\\Does this next sentence follow, given the preceding text?\\You need to pay for world cup tickets\\ Select from: (a). Yes; (b). It's impossible to say; (c). No.}
\end{minipage}
 
 & 
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 
Oh my gosh! You wanna go to the FIFA World Cup? That sounds like so much fun! \\ First, you need to get a passport, 'kay? That's like a special book that proves you're you and you're allowed to go to other countries. \\Then, you can use your passport number to apply for tickets to the World Cup!  But, oh no! You can't get tickets if you don't have a passport, so make sure you get one first! \\ And, oh my gosh! You'll also need to pay for the tickets! \\ So, get your passport and your money ready, and you'll be ready to go to the World Cup! \\Does that make sense?
}
\end{minipage}
& 
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize Yes.}
\end{minipage} \\

\midrule

\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 
Here's a brain teaser for you: Can you describe a process in which a machine can learn without being explicitly programmed? Think hard, because that's exactly what machine learning is all about. Give it a try and let's see if you can crack this puzzle!
}
\end{minipage}
 
 & 
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize 
Machine learning is a field of computer science that gives computers the ability to learn without being explicitly programmed. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\\There are many different types of machine learning algorithms, but they all work by finding patterns in data. Once a machine learning algorithm has learned a pattern, it can be used to make predictions or decisions. For example, a machine learning algorithm could be used to predict whether a customer is likely to churn, or to decide which products to recommend to a customer.\\Machine learning is a powerful tool that can be used to solve a wide variety of problems. However, it is important to note that machine learning is not a silver bullet. Machine learning algorithms can make mistakes, and they can be biased. It is important to carefully evaluate machine learning algorithms before using them in production.\\One way to describe ...
}
\end{minipage}
& 
\begin{minipage}[t]{\linewidth}%
\raggedright
{\footnotesize Machine learning can be achieved through the process of observation and unsupervised learning, which does not require explicit programming. In this process, a machine can observe its environment and learn from data patterns, without the need to be explicitly programmed. Through this, it can pick up on patterns in data and uncover trends that can be used to make predictions and decisions.
}
\end{minipage} \\
\bottomrule
\end{tabular}
\end{table}




