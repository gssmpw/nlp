\section{Related work}
\label{sec:related_new}
\vspace{-4pt}
% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
% Organization
% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
% In this section, we review the current state of research in three key areas relevant to our study: Sound Source Localization (SSL)(\ref{subsec:ssl}), Audio-Visual Segmentation (AVS) \wj{maybe we don't need this in related work because every familiry with this graph.}(\ref{subsec:avs_related}), and Imbalanced Multimodal Learning(\ref{subsec:imbalanced_multimodal_learning}). 
% Each area contributes differently to our understanding of multimodal interactions and challenges in audio-visual processing, and we highlight both the advancements and limitations that frame the context for our proposed approach.


% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
% Sound Localization
% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
\textbf{Sound Source Localization.}
% 1. --- Define Task & Early Work
This task is closely related to AVS, focusing on localizing sound sources within visual scenes \cite{arandjelovic2018objects, senocak2018learning,mo2022closer, chen2021localizing,mahmud2024t}. This task advances cross-modal understanding through various technical approaches, from basic feature fusion strategies to sophisticated attention mechanisms \cite{senocak2018learning, mo2022closer, hu2020discriminative, qian2020multiple}.
% 2. --- Recent advances
Recent sound source localization approaches have significantly improved sound source discrimination through multiple innovations: contrastive learning with hard-mining strategies enhances complex region distinction~\cite{chen2021localizing, hu2020discriminative, mo2022closer}, while class-aware approaches and dual-phase feature alignment enable robust multi-source localization without explicit pairwise annotations \cite{hu2020discriminative, qian2020multiple, chen2021localizing}.
However, the predicted sounding object heatmaps lack the fine-grained precision offered by AVS's pixel-level segmentation capabilities.


% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
% AVS
% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
\vspace{2mm}
\noindent
\textbf{Audio-Visual Segmentation.}
% \label{subsec:avs_related}
% --- 1. Define Task & Early Work
AVS task focuses on identifying and segmenting sound-producing objects through pixel-level mask prediction. 
% --- 2. Recent advances
The field has progressed significantly since its inception~\cite{zhou2022audio,gao2024avsegformer, li2023catr, chen2024cavp, liu2024annofree, ma2024stepping, wang2024GAVS, guo2024open, sun2024biasinAVS}. Most approaches follow an encoder-decoder design, with early works focusing on effective fusion strategies for audio-visual information~\cite{zhou2022audio}. Subsequent developments explored more sophisticated architectures, incorporating multimodal transformers and audio query guided mechanisms~\cite{gao2024avsegformer, liu2023AuTR, sun2024biasinAVS, wang2024ref, li2023catr, ma2024stepping, yang2024combo} to enhance cross-modal understanding. Recently, methods leveraging vision foundation models~\cite{liu2024annofree, wang2024GAVS, sun2024biasinAVS} and LLMs~\cite{wang2024can, he2024mlmseg}  have demonstrated improved segmentation capabilities. 

% The integration of large language models has further extend AVS to open-vocabulary scenarios~\cite{, }, enabling more flexible applications.
% Recent approaches have advanced AVS capabilities through various technical innovations, such as fusion-based architectures \cite{zhou2022audio, huang2023aqformer, li2023catr, ling2023hear2seg, liu2023AuTR} and prompt-based methods \cite{gao2024avsegformer, liu2024annofree, ma2024stepping, wang2024GAVS, wang2024ref} \yapeng{make it consistent with the descriptions in intro}. Some advances have enabled more sophisticated applications, from multi-source segmentation to open-vocabulary scenarios~\cite{wang2024GAVS, guo2024open}.

% --- 3. Bias Problems & Existing Solutions
Despite these developments, we observe that AVS models commonly suffer from ``visual prior'' bias, where models generate predictions primarily based on visual salience regardless of audio context~\cite{sun2024biasinAVS, chen2024cavp, li2024qdformer}. While recent efforts address this through contrastive learning~\cite{chen2024cavp} and semantic enhancement~\cite{sun2024biasinAVS, li2024qdformer}, they still lead to disregard audio features (see Fig.~\ref{fig:fig1}) or face limited evaluation scope. 
% --- 4. Our Approach

% In contrast to these complex architectural solutions, our approach offers a simple yet effective strategy that can be integrated into existing AVS models toward more robust AVS. 
In contrast to complex architectural solutions, we adopt a straightforward yet effective approach to enhancing AVS model robustness. We systematically analyze audio robustness in AVS and introduce AVSBench-Robust, a benchmark designed to evaluate models under both original audio conditions and challenging negative audio scenarios. 
A concurrent study \cite{juanola2024critical} identified visual bias in SSL models, introducing the same negative audio scenarios along with evaluation metrics, but primarily focuses on assessing model performance.
In contrast, our work not only evaluates robustness but also presents a targeted training strategy to strengthen AVS models against misleading audio-visual cues.
% However, AVS systems commonly exhibit problematic biases, particularly "visual prior" where models over-rely on visual features while disregarding audio context \cite{sun2024biasinAVS, chen2024cavp, li2024qdformer}. While recent works attempt to address these limitations through contrastive learning \cite{chen2024cavp} and semantic feature enhancement \cite{sun2024biasinAVS, li2024qdformer}, Due to CAVP\cite{chen2024cavp} heavy reliance on negative samples in the contrastive learning process inadvertently, the model to disregard audio features entirely, resulting in predictions that remain largely unaffected by audio variations. (see figure \ref{fig:fig1}), while other approaches \cite{sun2024biasinAVS, li2024qdformer} face limited evaluation scope due to restricted implementation availability.

% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
% Imbalanced Multimodal Learning
% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
\vspace{2mm}
\noindent
\textbf{Imbalanced Multimodal Learning.}
%\label{subsec:imbalanced_multimodal_learning}
Recent studies in audio-visual learning highlight significant challenges in balancing different modalities during training, where dominant modalities often overshadow others in the learning process \cite{wang2020mmlhard,tian2020unified,wei2024mmpareto, peng2022balanced}. Various solutions such as modality-specific optimization, gradient modulation, and Pareto optimization~\cite{wang2020mmlhard, wei2024mmpareto, peng2022balanced} have been proposed to address this imbalance, aiming to preserve the contribution of each modality. However, these approaches primarily target tasks where separate losses are applied for joint modalities and each individual modality ~\cite{wang2020mmlhard, wei2024mmpareto}. In contrast, the AVS task typically involves applying a single loss after fusing audio and visual features, which requires us to design a new strategy for effective modality balancing in AVS.