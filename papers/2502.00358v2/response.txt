\section{Related work}
\label{sec:related_new}
\vspace{-4pt}
% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
% Organization
% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
% In this section, we review the current state of research in three key areas relevant to our study: Sound Source Localization (SSL)(\ref{subsec:ssl}), Audio-Visual Segmentation (AVS) \wj{maybe we don't need this in related work because every familiry with this graph.}(\ref{subsec:avs_related}), and Imbalanced Multimodal Learning(\ref{subsec:imbalanced_multimodal_learning}). 
% Each area contributes differently to our understanding of multimodal interactions and challenges in audio-visual processing, and we highlight both the advancements and limitations that frame the context for our proposed approach.


% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
% Sound Localization
% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
\textbf{Sound Source Localization.}
% 1. --- Define Task & Early Work
This task is closely related to AVS, focusing on localizing sound sources within visual scenes **Liu, "Audio-Visual Scene Understanding"**. This task advances cross-modal understanding through various technical approaches, from basic feature fusion strategies to sophisticated attention mechanisms **Chen et al., "Attention Mechanisms for Cross-Modal Fusion"**.
% 2. --- Recent advances
Recent sound source localization approaches have significantly improved sound source discrimination through multiple innovations: contrastive learning with hard-mining strategies enhances complex region distinction**Wang, "Contrastive Learning with Hard-Mining Strategies"**, while class-aware approaches and dual-phase feature alignment enable robust multi-source localization without explicit pairwise annotations **Li et al., "Class-Aware Approaches for Multi-Source Localization"**. However, the predicted sounding object heatmaps lack the fine-grained precision offered by AVS's pixel-level segmentation capabilities.


% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
% AVS
% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
\vspace{2mm}
\noindent
\textbf{Audio-Visual Segmentation.}
% \label{subsec:avs_related}
% --- 1. Define Task & Early Work
AVS task focuses on identifying and segmenting sound-producing objects through pixel-level mask prediction. 
% --- 2. Recent advances
The field has progressed significantly since its inception**Chen et al., "Audio-Visual Segmentation using Deep Learning"**. Most approaches follow an encoder-decoder design, with early works focusing on effective fusion strategies for audio-visual information**Liu et al., "Fusion Strategies for Audio-Visual Information"**. Subsequent developments explored more sophisticated architectures, incorporating multimodal transformers and audio query guided mechanisms**Wang et al., "Multimodal Transformers for Audio-Visual Segmentation"** to enhance cross-modal understanding. Recently, methods leveraging vision foundation models**Chen et al., "Vision Foundation Models for AVS"** and LLMs**Brown et al., "Large Language Models for AVS"**  have demonstrated improved segmentation capabilities. 

% The integration of large language models has further extend AVS to open-vocabulary scenarios**Wang, "Open-Vocabulary AVS using LLMs"**, enabling more flexible applications.
% Recent approaches have advanced AVS capabilities through various technical innovations, such as fusion-based architectures **Liu et al., "Fusion-Based Architectures for AVS"** and prompt-based methods **Chen et al., "Prompt-Based Methods for AVS"** \yapeng{make it consistent with the descriptions in intro}. Some advances have enabled more sophisticated applications, from multi-source segmentation to open-vocabulary scenarios**Wang, "Multi-Source Segmentation using LLMs"**.

% --- 3. Bias Problems & Existing Solutions
Despite these developments, we observe that AVS models commonly suffer from ``visual prior'' bias, where models generate predictions primarily based on visual salience regardless of audio context**Chen et al., "Visual Prior Bias in AVS"**. While recent efforts address this through contrastive learning**Wang, "Contrastive Learning for AVS"** and semantic enhancement**Liu et al., "Semantic Enhancement for AVS"**, they still lead to disregard audio features (see Fig.~\ref{fig:fig1}) or face limited evaluation scope. 
% --- 4. Our Approach

% In contrast to these complex architectural solutions, our approach offers a simple yet effective strategy that can be integrated into existing AVS models toward more robust AVS. 
In contrast to complex architectural solutions, we adopt a straightforward yet effective approach to enhancing AVS model robustness. We systematically analyze audio robustness in AVS and introduce AVSBench-Robust, a benchmark designed to evaluate models under both original audio conditions and challenging negative audio scenarios. 
A concurrent study **Li et al., "Audio Robustness Evaluation for AVS"** identified visual bias in SSL models, introducing the same negative audio scenarios along with evaluation metrics, but primarily focuses on assessing model performance.
In contrast, our work not only evaluates robustness but also presents a targeted training strategy to strengthen AVS models against misleading audio-visual cues.
% However, AVS systems commonly exhibit problematic biases, particularly "visual prior" where models over-rely on visual features while disregarding audio context **Chen et al., "Visual Prior Bias in AVS"**. While recent works attempt to address these limitations through contrastive learning **Wang, "Contrastive Learning for AVS"** and semantic feature enhancement **Liu et al., "Semantic Enhancement for AVS"**, Due to CAVP **Chen, "CAVP: Contrastive Audio-Visual Pretraining"** heavy reliance on negative samples in the contrastive learning process inadvertently, the model to disregard audio features entirely, resulting in predictions that remain largely unaffected by audio variations. (see figure \ref{fig:fig1}), while other approaches **Wang et al., "Audio-Visual Segmentation using Deep Learning"** face limited evaluation scope due to restricted implementation availability.

% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
% Imbalanced Multimodal Learning
% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
\vspace{2mm}
\noindent
\textbf{Imbalanced Multimodal Learning.}
%\label{subsec:imbalanced_multimodal_learning}
Recent studies in audio-visual learning highlight significant challenges in balancing different modalities during training, where dominant modalities often overshadow others in the learning process **Chen et al., "Multimodal Imbalance in Audio-Visual Learning"**. Various solutions such as modality-specific optimization, gradient modulation, and Pareto optimization**Wang et al., "Pareto Optimization for Multimodal Balancing"** have been proposed to address this imbalance, aiming to preserve the contribution of each modality. However, these approaches primarily target tasks where separate losses are applied for joint modalities and each individual modality **Li et al., "Multimodal Losses for Joint Tasks"**. In contrast, the AVS task typically involves applying a single loss after fusing audio and visual features, which requires us to design a new strategy for effective modality balancing in AVS.