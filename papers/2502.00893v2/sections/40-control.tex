\section{System Control}
\label{sec:control}

\subsection{Keyframe Animation}

Keyframe animation is a cornerstone of character animation, but it provides only kinematic data, with no guarantee of dynamic feasibility~\citep{izani2003keyframe}. To address this, we developed software integrating MuJoCo~\citep{todorov2012mujoco} with a GUI, enabling real-time tuning and validation of keyframes and motion trajectories generated through linear interpolation with user-defined timings. With our high-fidelity digital twin, we efficiently generate open-loop trajectories such as cuddling, push-ups, and pull-ups that can be executed zero-shot in the real world.

\subsection{Reinforcement Learning}

% info["phase_signal"],
% info["command_obs"],
% motor_pos_delta * self.obs_scales.dof_pos + motor_backlash,
% motor_vel * self.obs_scales.dof_vel,
% info["last_act"],
% # motor_pos_error,
% # torso_lin_vel * self.obs_scales.lin_vel,
% torso_ang_vel * self.obs_scales.ang_vel,
% torso_euler * self.obs_scales.euler,

For walking and turning, we train a reinforcement learning (RL) policy, $\pi(\bm{\mathrm{a}}_t | \bm{\mathrm{s}}_t)$, which outputs $\bm{\mathrm{a}}_t$ as joint position setpoints for proportional-derivative (PD) controllers, based on the observable state $\bm{\mathrm{s}}_t$:
\begin{equation}
\bm{\mathrm{s}}_t = \left(\bm{\phi}_t, \bm{\mathrm{c}}_t, \Delta\bm{\mathrm{q}}_t, \bm{\dot{\mathrm{q}}}_t, \bm{\mathrm{a}}_{t-1}, \bm{\theta}_t, \bm{\omega}_t \right),
\end{equation}
where $\bm{\phi}_t$ is a phase signal, $\bm{c}_t$ represents velocity commands, $\Delta\bm{q}_t$ denotes the position offset relative to the neutral pose $\bm{q}_0$, $\bm{a}_{t-1}$ is the action from the previous time step, $\bm{\theta}_t$ represents the torso orientation, and $\bm{\omega}_t$ is the torso’s angular velocity. All inputs are normalized to ensure stable learning.

During PPO policy training~\citep{schulman2017proximal}, the environment generates the next state, $\bm{s}_{t+1}$, updates the phase signal, and returns a scalar reward $\mathrm{r}_t = \mathrm{r}(\bm{s}_t, \bm{a}_t, \bm{s}_{t+1}, \bm{\phi}_t, \bm{c}_t)$. Following standard practice~\citep{grandia2024design}, the reward is decomposed as:
\begin{equation}
    \mathrm{r}_t = \mathrm{r}_t^{\text{imitation}} + \mathrm{r}_t^{\text{regularization}} + \mathrm{r}_t^{\text{survival}}.
\end{equation}

Among these components, $\mathrm{r}_t^{\text{imitation}}$ encourages accurate imitation of the reference walking motion, which is generated using a closed-form ZMP (Zero Moment Point) solution~\citep{tedrake2015closedform}.  $\mathrm{r}_t^{\text{regularization}}$ incorporates heuristics of ideal walking motion, penalizes joint torques, and promotes smooth actions to minimize unnecessary movements. A survival reward $\mathrm{r}_t^{\text{survival}}$ prevents early episode termination during training. Additional details are provided in Section~\ref{sec:rl_details} of the supplementary material.

\input{captions/f5-payload}


\subsection{Imitation Learning}

Real-world data collection involves a human operator guiding the leader’s arms to teleoperate the follower’s arms while using a joystick and buttons on a remote controller to control body movements.

During data collection, when the upper body tracks the position commands from the leader arms, \systems lower body employs a two-layer PD controller to maintain balance. The first layer is a Center of Mass (CoM) PD controller, which keeps the CoM close to the center of the support polygon. The first layer addresses CoM shifts caused by arm movements, while the second layer manages lifting heavy objects. The second layer is a torso pitch PD controller, which uses IMU readings to ensure the torso remains upright. 
% This way helps generate the cleanest data obtainable right from the source. The follower humanoid will not experience any external disturbances, but purely self-actuated motion. And it will not see any human operator in the vision as it's following the command elsewhere. 
 
% For each task, we collected roughly 50 training trajectories, which takes around 20 minutes.

\input{captions/f6-vel_tracking}
\input{captions/t2-tracking_error}

With this setup, we can collect 60 trajectories in just 20 minutes for both bimanual and full-body manipulation tasks. The motor positions of the leader's arms are recorded as the actions, while the motor positions of the follower robot, along with the RGB images captured from its camera, are recorded as observations. This data is subsequently used to train a diffusion policy~\citep{chi2023diffusion}, with further details provided in Section~\ref{sec:dp_details} of the supplementary material.
