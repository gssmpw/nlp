\subsection{Diffusion Policy Details}
\label{sec:dp_details}

% The manipulation task demonstrations are achieved through diffusion policy. \{Introduce DP here\}. 

% \{Input to the model\} 
The diffusion policy processes a cropped and downsampled $96\times96$ RGB image, which is encoded by a ResNet~\citep{he2016deep} pretrained on ImageNet~\citep{deng2009imagenet} to extract visual features. Both leader and follower joint angles are downsampled to $10~\mathrm{Hz}$ for training, where the leader joint angles serve as actions and the follower as observations.
To prevent motor overload during data collection, the upper body motors use low proportional gains, allowing modulation of the manipulation force. This behavior is embedded in the discrepancy between leader and follower joint angles, which the policy ultimately learns.

% \{Output of the model\}
% The output of the model is the predicted upper body action, also in 10Hz. 
% As mentioned before, the predicted action can penetrate the actual object surface to achieve a modulated manipulation force. 
% Each prediction pass of the policy will produce a 16-step prediction, the first 3 steps are discarded to compensate for sensing and processing delay, and the next 8 steps are actually executed at inference. 

% % \{Settings and parameters\}
% The final model consists of ~300M parameters. \{Fill me in\}

% \{Inference time speed and hz\}
The model is trained with 100 diffusion steps. During inference, the trained model runs directly on the Jetson Orin NX 16GB with 3 DDPM steps, which are sufficient for satisfactory results. With 300M parameters, inference latency remains under $0.1~\mathrm{s}$ on the GPU, ensuring smooth execution at $10~\mathrm{Hz}$ without stuttering. Each inference yields a 16-step prediction; the first 3 actions are discarded to compensate for latency~\citep{chi2024universal}, and the next 5 actions are executed.
% Then new observations are obtained to inference again to produce the next prediction. 