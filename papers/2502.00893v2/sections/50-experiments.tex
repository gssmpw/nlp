\section{Experiments}
\label{sec:experiments}

We conducted a series of experiments to show that \system is capable, ML-compatible, and reproducible.

\input{captions/f7-results}

\subsection{Capability: Arm Span, Payload, and Endurance}
To evaluate \systems arm span and payload capacity, we teleoperate it to hug a test object using the compliant palm gripper while maintaining balance. In the arm span test, we show that with a torso dimension of $13 \times 9 \times 12~\mathrm{cm}^3$, \system can grasp objects up to $27 \times 24 \times 31~\mathrm{cm}^3$, approximately 14 times its torso volume. 

The payload test assesses both the upper body’s lifting capacity and the lower body’s ability to maintain balance. \system successfully lifts up to $1484~\mathrm{g}$, 40\% of its total weight ($3484~\mathrm{g}$). 
To eliminate friction effects, we use a 3D-printed cup for the compliant palm gripper to lock in securely. To determine the true limit, we incrementally add screws to the cup until \system falls over. Results are shown in Figure~\ref{fig:payload}.

In the endurance test, \system starts with a fully charged battery, running the walking RL policy while stepping in place. \system achieved the longest streak of 19 minutes without falling. Over time, increased motor temperatures gradually pushed it outside the policy’s training distribution, leading to more frequent falls. \system withstands up to 7 falls before breaking, but even then, repairs are quick—requiring only 21 minutes of 3D printing and 14 minutes of assembly, including removing the damaged part, installing the replacement, performing zero-point calibration, and rerunning the script.


\subsection{Capability: Push-ups and Pull-ups}

To demonstrate \systems ability to execute expressive and dynamic motions, we program push-ups and pull-ups in our keyframe software and perform a zero-shot sim-to-real transfer. For pull-ups, we use an AprilTag to help \system accurately locate the horizontal bar. Both tasks require strong limbs, balanced upper-lower body strength, and precise coordination, particularly when \system transitions from a planking pose to standing after push-ups and when it releases the horizontal bar and lands after pull-ups. These open-loop transfers require only a single motion trajectory designed in simulation, highlighting the fidelity of our digital twin. Figure~\ref{fig:results} showcases both tasks through a series of images. 
% \haochen{Here Figure 7 is referred before Figure 6. Is it OK?}

\input{captions/f8-skill_chaining}
\input{captions/f9-collaboration}

\subsection{ML-Compatibility: Omnidirectional Walking}

To demonstrate \systems locomotion capabilities, we train RL walking policies to follow a square trajectory with a predefined velocity profile. Figure~\ref{fig:vel_tracking} and Table~\ref{tab:tracking_errors} present the results, with real-world tracking data collected via motion capture. Due to RL policy limitations, both simulation and real-world tracking deviate from the command, primarily because the learned walking policy struggles with in-place rotation, causing translation offsets. However, the sim-to-real gap is notably smaller than the tracking gap, supporting a successful zero-shot transfer. Additionally, we report a position tracking error variance of $0.018~\mathrm{m}$, linear velocity tracking error of $0.002~\mathrm{m/s}$, and angular velocity tracking error of $0.01~\mathrm{rad/s}$, which demonstrates good repeatability. 

% \input{captions/t2-tracking_error}

\subsection{ML-Compatibility: Vision-based Manipulation}

We show \systems ability to perform bimanual manipulation and full-body manipulation by transferring octopus toys from a table and the ground to a wagon. Both tasks are trained with RGB-based diffusion policy~\citep{chi2023diffusion} with 60 demonstrations. Across 20 test trials, we achieve a 90\% success rate for bimanual manipulation and 75\% for full-body manipulation. We leverage a combination of open-loop motions and closed-loop policies to enhance data collection efficiency. In the bimanual task, the torso rotating and releasing motions are open-loop, while in full-body manipulation, kneeling down is open-loop. The same motions are also used during policy evaluation.
\systems onboard computing runs a 300M-parameter diffusion policy with about $100~\mathrm{ms}$ latency, enabling real-time operation. 
Results are shown in Figure~\ref{fig:results}.

\subsection{ML-Compatibility: Skill Chaining}

%To demonstrate tasks combining locomotion and manipulation, we test wagon pushing, which involves two steps-grasping the handle and walking forward. 
To test the system's ability to combine locomotion and manipulation skills, we test the wagon-pushing task (Figure~\ref{fig:skill_chaining}), which requires the robot to first perform vision-based grasping and then walk forward. 
To do so, \system first executes a diffusion policy to grasp the handle, while maintaining that pose, switched to the RL policy to push the wagon forward. To enable \system to walk while maintaining its grip, we sample the end pose of the upper body from the grasping policy’s training data (60 demonstrations) during RL training. 

\subsection{Reproducibility: Hardware and Policies}

To demonstrate hardware reproducibility, we recruit a CS-major student with no prior hardware experience to assemble a second \system using our manuals as shown in Figure~\ref{fig:manual} of the supplementary material and our open-source assembly videos. The student independently completes the assembly in three days, including the time required for 3D printing.

For policy reproducibility, we run the manipulation policy trained on data collected with one instance on the other instance, achieving the same success rate of 90\% across 20 trials. We also successfully transfer the RL walking policy between both robots. To further showcase the equivalent performance of both \system instances, we have them collaborate on a long-horizon room tidying task, as shown in Figure~\ref{fig:collaboration}.



% \subsection{ML-Compatibility - Conversation}

% As a humanoid platform, \system supports basic conversation via its speaker and microphones, enabling multi-sensory robot learning and human-robot interaction research. A conversation example is shown in the supplementary video.
