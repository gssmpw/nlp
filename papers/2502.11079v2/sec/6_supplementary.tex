\section*{Supplementary Materials}
\label{sec:supp}

\subsection*{1. S2I+I2V \textbf{\textit{vs}} S2V}
\label{sec:supp:s2i2v}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.98\columnwidth]{figures/s2i2v.pdf} 
	\caption{Comparison of subject-to-image-to-video \cite{dreamina} and subject-to-video (ours).}
	\label{fig:s2i2v}
\end{figure}

As mentioned in the main text, combining subject-to-image (S2I) and image-to-video (I2V) can achieve similar effects to subject-to-video (S2V), but there are some difficult limitations. Firstly, existing methods \cite{guo2024pulid, huang2024realcustom, dreamina} for generating subject-consistent images or ID-consistent images still exhibit noticeable artificial artifacts, and there is significant room for improvement in the dimension of subject consistency. Equally important, I2V cannot ensure consistency of the subject during motion. As illustrated in Figure \ref{fig:s2i2v}, when inputting a reference portrait, S2I first generates a reference image for the initial frame of I2V. If the initial frame includes a back view or occlusions, I2V may "imagine" a false ID during the process of removing the occlusion, leading to a failure in maintaining consistency.


\subsection*{2. Copy-paste problem}
\label{sec:supp:copypaste}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.98\columnwidth]{figures/copypaste.pdf} 
	\caption{Intuitive cases of copy-paste problems. The red font in the text prompt does not function as intended.}
	\label{fig:copypaste}
\end{figure}

In the field of video generation, the copy-paste issue is particularly prominent, manifesting as the leakage of image content into the generated video. Some methods sample keyframes from a video and use them as image conditions to reconstruct the video. However, this approach allows the model to employ shortcut learning strategies, simplifying the content understanding process. Figure \ref{fig:copypaste} shows examples of the copy-paste issue, sampling from the initial, middle, and final frames: In the first row, the girl's expression remains unchanged, ignoring the text prompt. In the second row, the cartoon character's movements remain stiff and identical to the reference. The third row illustrates a common case where the generated video is too similar to I2V, diminishing the effectiveness of scene-related text and reducing content diversity. To address this, we focus on constructing cross-video multi-subject pairings, ensuring subjects match in content while allowing for non-rigid deformations and changes in color distribution, thereby avoiding the copy-paste problem.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{figures/data_dist.pdf} 
	\caption{Distribution of object frequencies and class.}
	\label{fig:data_dist}
\end{figure*}


\subsection*{3. Ablation study supplement}
\begin{figure}[t]
	\centering
	\includegraphics[width=0.98\columnwidth]{figures/confused_case.pdf} 
	\caption{Examples of multi-subject confusion: On the left are the multi-subject reference images, while the right columns present the cases of confusion and the successful cases after improvement.}
	\label{fig:confused}
\end{figure}

\begin{table}[b]
	\centering
	\resizebox{\columnwidth}{!}{
	\begin{tabular}{lcc}
		\toprule
		& \textbf{\textit{w/o}} text-image alignment & \textbf{\textit{w/}} text-image alignment \\
		\midrule
		Success rate & 65\% & 95\% \\
		\bottomrule
	\end{tabular}
	}
	\caption{Success rate of multi-subject generation with and without text-image alignment.}
	\label{tab:success_rate}
\end{table}

\textbf{Multi-subject confusion issue.} When multiple reference subjects are input simultaneously, appearance confusion may occur. Our solution aligns text descriptions with video subjects during training, ensuring distinct descriptions for each subject. During inference, a rephraser adjusts the input text prompts to align with the training data format. For example, in the first row of Figure \ref{fig:confused}, the original prompt "A family of three is having a meal at the table" caused confusion. The rephrased prompt "a woman in black, a young girl in white, and an elderly man in a suit eating together at the table" resolved this issue. In the second row of Figure \ref{fig:confused}, the original prompt "a girl in casual clothes walking by the beach" failed to match the reference. The rephrased prompt "a girl in a white T-shirt and jeans walking by the beach" successfully matched the reference.
Quantitative analysis, shown in Table \ref{tab:success_rate}, indicates a significant increase in the success rate of subject-consistent generation with this method.
Aligning image and text is crucial for multi-subject generation tasks. This approach, which requires no additional complex data structures or model designs, significantly optimizes the multi-subject confusion problem.


\subsection*{4. Data pipeline for face ID }
\label{sec:supp:facedata}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.98\columnwidth]{figures/face_pipe.pdf} 
	\caption{Facial data processing pipeline for constructing ID cross-pair}
	\label{fig:facepipe}
\end{figure}

To enhance facial ID consistency, we developed an additional data pipeline for processing facial data. As shown in Figure \ref{fig:facepipe}, the facial data pipeline reuses the scene segmentation, video filtering, and annotation steps from the general subject pipeline. During the detection stage, we use an internal facial detection tool to identify each face in the video reference frames and calibrate it with the VLM \cite{Qwen2.5-VL} results from the captions using IOU (Intersection Over Union). In the matching stage, we calculate facial similarity using Arcface \cite{deng2019arcface} features and add a deduplication operator \cite{idealods2019imagededup} to further calibrate the recognition results.


\subsection*{5. Data distribution}
\textbf{Distribution of video object quantities.} We sample three frames at [0.05, 0.5, 0.95] of the video timeline and perform object detection on these frames. We filter out objects that meet the following criteria: (1) objects that are small in size or occupy a small proportion of the frame; (2) objects with a high degree of overlap with other objects; and (3) incomplete objects judged by the VLM \cite{Qwen2.5-VL}. The final distribution of the number of objects per video is shown in the table on the left side of Figure \ref{fig:data_dist}.

\noindent \textbf{Distribution of video object types.} We use LLM \cite{GPT4} to classify the noun fields in all captions into the following categories: human, animal, clothes, product, landmark, IP character, and others. The distribution is shown in the accompanying Figure \ref{fig:data_dist}, with human, clothes, and product categories accounting for the majority.

\subsection*{6. Model architecture}
\label{sec:supp:model}

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{figures/architecture.pdf} 
	\caption{The supplementary diagram of the Phantom framework.}
	\label{fig:architecture}
\end{figure}
The architecture of the \textit{Phantom} model is shown in Figure \ref{fig:architecture}, which supplements the missing details in the main text. As illustrated, it integrates the VAE and CLIP encoders to process reference images, while the text encoder handles captions. The encoded features are combined with added noise and processed through multiple MMDiT blocks, resulting in the final output. This design ensures a balance between detailed reconstruction and high-level information alignment while also guaranteeing a unified training paradigm for single and multiple subject inputs.


\subsection*{7. Qualitative analysis}
\label{sec:supp:qualitative}

Qualitative comparison results of single-subject consistency generation are shown in Figure \ref{fig:supp_sip}. Firstly, Vidu \cite{Vidu} performs well in both image consistency and text following for the first two cases but fails in the third shoe case with two different seeds. The effectiveness of Pika \cite{Pika} is evident, as the first two cases show significant disadvantages in maintaining subject consistency, tending towards a cartoonish appearance. The major issue with Kling \cite{Keling} is that most cases resemble the I2V mode, where the initial frame directly replicates the reference image (as indicated by the red box in Figure \ref{fig:supp_sip}), followed by subject motion generated based on text, thereby limiting the effectiveness of textual descriptions.

Figure \ref{fig:supp_mip} displays some qualitative comparisons of multi-subject consistency generation. Firstly, Kling still reflects the I2V pattern, appearing unnatural transitions in the first few frames of the video. Additionally, in the second example with three reference images of persons, confusion issues are evident in all methods except ours. Vidu shows the first man's clothes and the second man's face, and includes a person unrelated to the reference images. Pika misses one person, and Kling also lacks one person and shows the same issue as Vidu. The final case demonstrates that Vidu and Pika appear more realistic, indicating that their text responsiveness is stronger than their subject consistency.

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.78\textwidth]{figures/supp_sip.pdf} 
	\caption{Comparative results of single reference subject-to-video generation.}
	\label{fig:supp_sip}
\end{figure*}


\begin{figure*}[t]
	\centering
	\includegraphics[width=0.78\textwidth]{figures/supp_mip.pdf} 
	\caption{Comparative results of multi-reference subject-to-video generation.}
	\label{fig:supp_mip}
\end{figure*}


\subsection*{8. Limitations and future work}
\label{sec:supp:limitations}
\textbf{Limitations.} While \textit{Phantom} demonstrates strong performance in subject-consistent video generation, several challenges persist. First, handling uncommon subjects (e.g., rare animals or niche objects) remains difficult due to biases in training data coverage. Second, complex multi-subject interactions (e.g., overlapping movements or fine-grained spatial relationships) often lead to partial confusion or inconsistent relative subject sizes. Third, generating videos that strictly adhere to intricate text responses (e.g., precise spatial layouts or nuanced temporal dynamics) is limited by the current cross-modal alignment mechanism. These issues stem from three core factors: (1) gaps in dataset diversity, particularly for non-human-centric scenarios; (2) the inherent rigidity of the reference image injection strategy, which struggles to disentangle entangled features from multiple subjects; and (3) biases inherited from pre-trained base models and visual encoders, such as CLIP’s semantic oversimplification and VAE’s over-referenced details. 

\noindent \textbf{Future work.} Addressing these limitations will require multi-faceted innovations, and we propose the following directions: 

\begin{itemize}
    \item Enhanced Cross-Modal Alignment: Develop adaptive injection mechanisms that dynamically prioritize text or image conditions based on task requirements, reducing content leakage and improving text responsiveness. 
    
    \item Spatiotemporal Disentanglement: Integrate spatial-aware attention modules and physics-inspired motion priors to better model multi-subject interactions and enforce consistent relative scales. 
    
    \item Bias-Aware Training: Mitigate dataset and model biases through adversarial debiasing techniques and synthetic data augmentation for underrepresented subjects. 
    
    \item Granular Control: Explore auxiliary control signals (e.g., depth maps, segmentation masks) to complement text prompts, enabling precise alignment with complex instructions. 
    
    \item Foundation Model Adaptation: Fine-tune pre-trained encoders on domain-specific data (e.g., medical imaging, animation) to broaden \textit{Phantom}’s applicability while preserving generalization. 
\end{itemize}

By advancing these areas, \textit{Phantom} could evolve into a versatile tool for industrial applications such as virtual try-ons, interactive storytelling, and educational content creation, ultimately narrowing the gap between academic research and real-world demands.