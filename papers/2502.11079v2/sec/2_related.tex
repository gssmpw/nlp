\section{Related Work}
\label{sec:related_work}

\subsection{Video foundation model}
\label{sec:related_work:foundation}

The diffusion algorithm has spurred the rise of video foundation model research, significantly impacting content creation and intelligent interaction. Early latent diffusion models (LDM) \cite{rombach2022high} typically utilized U-Net \cite{ronneberger2015u} architectures, such as the open-source Stable Diffusion 1.5 \cite{rombach2022high}.  Temporal modules were later added to these models, evolving them into video generation models like Make-A-Video \cite{singer2022make}, SVD \cite{blattmann2023stable}, and Animatediff \cite{guo2023animatediff}. The DiT \cite{peebles2023scalable} architecture, guided by scaling laws, has led to the development of more vision foundation models. Among these, Stable Diffusion 3 introduced MMDiT \cite{esser2024scaling}, a dual-stream DiT architecture, which has been adopted in open-source video generation projects such as CogvideoX \cite{yang2024cogvideox}, HunyuanVideo \cite{kong2024hunyuanvideo} and SeedVR \cite{wang2025seedvr}.

\subsection{Subject-consistency image generation}
\label{sec:related_work:image}

In recent years, significant progress has been made in subject-consistent generation for image tasks. Optimization-based methods \cite{ruiz2023dreambooth, gal2022image, hu2021lora, shah2024ziplora, huang2024context} training bind image content with special identifiers for text-to-image generation. A notable work in the training and inference paradigm is IP-Adapter \cite{ye2023ip}, which freezes the base model weights while only training additional adapters to achieve subject-consistent generation. This approach is also widely used in tasks requiring facial ID consistency \cite{guo2024pulid, wang2024instantid, chen2024dreamidentity}. However, these solutions often rely on CLIP \cite{cherti2023reproducible} or DINO \cite{oquab2023dinov2} for extracting image semantics, leading to a trade-off between low-level detail reconstruction and flexible text response. Recent advancements have unified image generation and editing tasks \cite{chen2024unireal, xiao2024omnigen, erwold-2024-qwen2vl-flux, wanx_ace}, enabling various types of editing tasks within a single model, including subject-consistent generation. Compared to adapter-based approaches, this method deeply learns image-text alignment, fully leveraging foundation models and resolving degradation issues from multiple adapters.

\subsection{Subject-consistency video generation}
\label{sec:related_work:video}

From recent research developments, the advancement of video generation capabilities and algorithmic innovations tends to lag behind image tasks. Similar to image consistency techniques, Kling \cite{Keling} has released an optimization-based video generation method for facial ID consistency, which requires uploading multiple videos of the same person for optimization, resulting in significant computational costs. Adapter-based approaches have also been attempted for video ID consistency tasks, such as ID-Animator \cite{he2024id} and ConsisID \cite{yuan2024identity}. However, these works have been validated on small datasets (around 10k), which limits their ability to fully align facial information with text descriptions. Recent works like ConceptMaster \cite{huang2025conceptmaster}, MovieWeaver \cite{liang2025movie}, and VideoAlchemist \cite{chen2025multi} have demonstrated capabilities in generating consistent multi-subject videos in general scenarios. However, there are currently no open-source methods for the S2V task, and commercial software's S2V capabilities \cite{Vidu, Pika, Keling, Hailuo, polyak2024movie} remain state-of-the-art. Therefore, comparing the performance with commercial closed-source solutions is crucial for evaluating the superiority of the proposed method.
