\section{Phantom}
\label{sec:phantom}

This section introduces the specific implementation of \textbf{\textit{Phantom}}. The first subsection describes how to construct cross-modal alignment training data, emphasizing the creation of cross-pair text-image-video triplets to address the "copy-paste" issue. The second subsection presents the design and considerations of the \textit{Phantom} architecture, focusing on how single and multiple subject features are dynamically injected into the framework. The third subsection introduces some key training settings and inference techniques to ensure the efficient implementation of S2V capabilities.

\subsection{Data Pipeline}
\label{sec:method:data_pipeline}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\columnwidth]{figures/attention.pdf} 
	\caption{\textbf{Dynamic injection strategy} and attention calculation for single or multiple reference subjects in each MMDiT block.}
	\label{fig:attention}
\end{figure}

To achieve subject-to-video (S2V) generation, we constructed a triplet data structure of text-image-video for cross-modal learning (Figure \ref{fig:data_pipeline}), ensuring that videos are paired with both images and text. 
First, we sampled long videos from Panda70M \cite{chen2024panda70m} and in-house sources. These videos were cut into single-scene segments using AutoShot \cite{zhu2023autoshot} and PySceneDetect \cite{pyscenedetect}, and any clips with low quality, aesthetics, or motion levels were filtered out.
Next, we used Gemini \cite{team2023gemini} to generate captions for the filtered video clips, focusing on describing the subjects' appearance, behavior, and the scene. 
Further, the LLM \cite{GPT4} is utilized to analyze the caption and extract the subject words with appearance descriptions, which are used as prompts for the VLM \cite{Qwen2.5-VL} to obtain the subject detection boxes of the reference frames.
At this point, the descriptions of the subjects in the captions can be exactly aligned with the subject elements detected in the reference images.

Although the reference images and text are aligned, the reference images are taken from specific frames within the videos. These image-video pairs are termed "in-pair" data. Some existing methods \cite{chen2025multi,huang2025conceptmaster} use in-pair data to train S2V models, ensuring subject consistency between images and videos. However, high visual similarity might cause the model to disregard text prompts, resulting in generated videos that simply copy-paste the input images. To address this issue, we undertake an additional effort to further establish pairings between cross-video clips. We employ the image embedder \cite{shao20221st} of the improved CLIP architecture to score and pair subjects detected across different videos. Pairs with scores that are excessively high (indicating a likelihood of copy-pasting) or too low (indicating different subjects) are eliminated.  
%For face IDs,  we employ the ArcFace \cite {deng2019arcface} features to measure the similarity (see supplementary material). 

After constructing the cross-paired data pipeline, further segmentation is required based on application scenarios. 
These primary elements include people, animals, objects, backgrounds, and more. Additionally, interactions between multiple elements can further categorize scenarios, such as multi-person interactions, human-pet interactions, and human-object interactions. 
By segmenting the data sources according to these application scenarios, we can quantitatively supplement missing data types. For example, virtual try-on applications require specific collections of model images and garment layouts. 
Ultimately, we obtained cross-pair data on the order of one million, among which the data containing human subjects accounted for the largest proportion. In addition, we also added a portion of paired image data to increase diversity. The data sources are Subject200k \cite{chen2024unireal} and OmniGen \cite{xiao2024omnigen}.

\subsection{Framework}
\label{sec:method:framework}

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.85\textwidth]{figures/radar_bar.pdf} 
	\caption{Video quality evaluation (left) and user study results for multi-subject consistency (right).}
	\label{fig:radar}
\end{figure*}

The \textbf{\textit{Phantom}} architecture, shown in Figure \ref{fig:framework}, consists of an untrained input head and a trained MMDiT module. The input head includes a 3D VAE \cite{yang2024cogvideox}  encoder and an LLM \cite{yang2024qwen2} inherited from the video foundation model \cite{lin2025diffusion, wang2025seedvr}, which encode the input video and text, respectively. 
The vision encoder, critically, comprises both a Variational Autoencoder (VAE) \cite{esser2021taming} and CLIP \cite{zhai2023sigmoid, radford2021learning}. The image features $F_{\text{ref\_v}}$ concatenated with the video latents $F_{\text{vid}}$ reuse the 3D VAE to maintain consistency in the visual branch input. Meanwhile, the image CLIP features $F_{\text{ref\_c}}$ concatenated with the text features $F_{\text{text}}$ provide high-level semantic information, compensating for the low-level features from the VAE. Feature merging involves dimensional alignment, as detailed below, 
\begin{equation}
	F_{T}^{l_1 + l_2, c} =F_{\text{text}}^{l_1,c}\oplus F_{\text{ref\_c}}^{l_2,c},
\end{equation}
\begin{equation}
	F_{V}^{t+n,h,w,c} = F_{\text{vid}}^{t,h,w,c}\oplus F_{\text{ref\_v}}^{n,h,w,c},
	\label{eq:fV}
\end{equation}
where $\oplus$ denotes concatenation. The concatenated features $F_T$ and $F_V$ are fed into the visual and text branches of MMDiT, and the model only separates the injected features during the calculation of attention.

Specifically, the MMDiT block is based on \cite{lin2025diffusion, wang2025seedvr} and improved for reference image input, primarily modifying the Attention \cite{vaswani2017attention} block, as shown in Figure \ref{fig:attention}. 
First, the $Q_{\text{vid}}$, $K_{\text{vid}}$, $V_{\text{vid}}$ features calculated from $F_{\text{vid}}$ are divided into windows of size 9. 
Then, the $Q_{\text{ref\_v}}$, $K_{\text{ref\_v}}$, $V_{\text{ref\_v}}$ features calculated from $F_{\text{ref\_v}}$
% of one or more subject reference images encoded by the VAE 
are dynamically concatenated to the end of each window, while the in-situ features are sequentially shifted to the start of the next window. 
This approach maintains the window structure while ensuring interaction between video and subject features within each window, as well as adaptive input for single- or multi-subject. 
Meanwhile, the $Q_{\text{text}}$, $K_{\text{text}}$, $V_{\text{text}}$ features calculated from $F_{\text{text}}$ and the $Q_{\text{ref\_c}}$, $K_{\text{ref\_c}}$, $V_{\text{ref\_c}}$ features calculated from $F_{\text{ref\_c}}$ are dynamically concatenated. After collecting all reference information, self-attention is calculated within each window. Then, the dynamically injected reference image features (including $\text{ref\_v}$ and $\text{ref\_c}$) and the text features within each window are extracted from the output features and averaged. This process ensures that the dimensions of the input and output features within the current block remain consistent, thereby facilitating subsequent block computations.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{figures/ipid_eval.pdf} 
	\caption{Comparison results showing, from top to bottom, single subject, multi-subject, and facial ID-consistent video generation, with four uniformly sampled frames displayed in each case.}
	\label{fig:ipid_eval}
\end{figure*}


\subsection{Training and inference}
\label{sec:method:train}

\textbf{Training setup.} 
We employ rectified flow (RF) \cite{liu2022flow,lipman2022flow} to construct the training objective and adjust the noise distribution sampling \cite{esser2024scaling}. RF aims to learn an appropriate flow field, enabling the model to efficiently and high-quality generate meaningful data samples from noise.
In the forward process of training, noise is added to clean data $x_0$ to generate $x_t = (1-t) \cdot x_0 + t \cdot \epsilon $, where $\epsilon$ is Gaussian noise with the distribution $\mathcal{N}(0, \text{I})$ and $t$ is a randomly sampled step scaled to a value between 0 and 1 based on the total steps (T=1000). 
The model predicts velocity $v_t$ to regress velocity $u_t = dx_t/dt$, and $v_t$ is represented by,
\begin{equation} 
v_t = \mathcal{G}_\theta (x_t, t, F_T, F_V).
\end{equation}
Thus, the RF training loss is given by,
\begin{equation} 
\mathcal{L}_{\text{mse}} = {\left \| v_t  - u_t \right \|}^{2}.
\end{equation} 
%where $x_{\text{T}}^{\text{pred}} = x_\text{t} + (1-t) \cdot x^{\text{pred}} $ and $x_{0}^{\text{pred}} = x_\text{t} - t \cdot x^{\text{pred}}$. 
Notably, $v_t$ includes additional (n)-dimensional features at the tail (refer to Eq.\ref{eq:fV}), which does not participate in the loss calculation.
The model training is conducted in two phases: the first phase trains for 50k iterations at 256p/480p resolution, and the second phase incorporates mixed 720p data, training for an additional 20k iterations to enhance higher resolution generation capabilities. Additionally, since one of the training objectives of VAE is pixel-level reconstruction, the CLIP features can be overshadowed when trained together with VAE features. Therefore, we set a relatively high dropout rate (0.7) for VAE during training to achieve balance. The total computational resources consumed approximately 30,000 GPU-hours on A100.


\noindent \textbf{Inference settings.}
\textit{Phantom} inference can accept 1 to 4 reference images and generate corresponding videos by describing the reference subjects using a given text prompt. Note that generating with more reference subjects may lead to unstable results. 
To align with the training data, the text prompt used in inference must first be adjusted by a rephraser to ensure it accurately describes the appearance and behavior of each reference subject, avoiding confusion between similar subjects (see supplementary materials). 
The Euler method is used for sampling over 50 steps, and the classifier-free guidance \cite{ho2022classifier} separates the image and text conditions. The denoised output at each step is given by,
\begin{equation}
	\begin{split}
		x_{t-1} = & x_{t-1}^{\oslash} + \omega_1 ( x_{t-1}^I - x_{t-1}^{\oslash} ) \\
		& + \omega_2 ( x_{t-1}^{TI} - x_{t-1}^{I} ),
	\end{split}
	\label{eq:inference}
\end{equation}
where $x_{t-1}^{\oslash}$ is the unconditional denoising output, $x_{t-1}^I$ is the image-conditioned denoising output, and $x_{t-1}^{TI}$ is the joint text-image conditioned denoising output. The weights $\omega _1$ and $\omega _2$ are set to 3 and 7.5, respectively.

