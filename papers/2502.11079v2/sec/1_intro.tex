\section{Introduction}
\label{sec:intro}

The rise of diffusion models \cite{peebles2023scalable, ho2020denoising} is rapidly reshaping the field of generative modeling at an astonishing pace. Among them, the advancements in video generation brought by diffusion models are particularly remarkable. 
In the visual domain, video generation requires to pay more attention to the continuity and consistency of multiple frames compared to image generation, which poses additional challenges. 
Inspired by the scaling laws of large language models \cite{GPT4, touvron2023llama, yang2024qwen2}, the focus of video generation has shifted towards investigating foundational large models, similar to Sora \cite{OpenAI2023Sora, polyak2024movie, yang2024cogvideox, kong2024hunyuanvideo, zheng2024open}, which have demonstrated promising visual effects and are paving the way for a new era in Artificial Intelligence Generated Content. 

%Nevertheless, up to now, various vertical applications of video generation tasks still lag behind image generation tasks.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.82\columnwidth]{figures/motivation.pdf}
	\caption{
		Relationship in cross-modal video generation tasks.
	}
	\label{fig:motivation}
\end{figure}

\begin{figure*}[h]
	\centering
	\includegraphics[width=0.98\textwidth]{figures/teaser_half.pdf}
	\caption{
		Subject-consistent video generation examples using our method, with reference images and corresponding generated video frames (text prompts omitted). The last three rows show multiple reference subjects.}
	\label{fig:teaser}
	%	\thispagestyle{empty} % 去除页眉和页脚
\end{figure*}

Currently, foundational video generation models focus mainly on two major tasks: text-to-video\cite{OpenAI2023Sora} and image-to-video \cite{blattmann2023stable}. 
Text-to-video (T2V) leverages language models to understand input text instructions and generate visual content describing the expected characters, movements, and backgrounds. While it allows for creative and imaginative content combinations, it often struggles with generating consistently predictable results due to inherent randomness. 
On the other hand, image-to-video (I2V) typically provides the first frame of an image along with optional text descriptions to transform a static image into a dynamic video. Although it is more controllable, the content richness is often limited by the strict ``copy-paste" \cite{polyak2024movie, chen2025multi} nature of the first frame.
We term the process of subject-consistent video generation as subject-to-video (S2V) \cite{huang2025conceptmaster, chen2025multi, Hailuo}, which involves capturing the subject from an image and flexibly generating a video based on text prompts, while combining the diversity and controllability of joint image and text inputs. As shown in Figure \ref{fig:motivation}, its essence lies in balancing the dual-modal prompts of text and image, requiring the model to simultaneously align text instructions and image contents.

%\clearpage  

However, the research on subject consistency in video generation tasks still lags behind image generation scenarios. 
As text-to-image (T2I) foundation models \cite{flux2024, esser2024scaling} have matured, subject-to-image (S2I) has evolved from parameter optimization methods \cite{ huang2024context, ruiz2023dreambooth} to adapter-based training approaches \cite{ye2023ip, huang2024realcustom}, to unified image editing approaches \cite{chen2024unireal, xiao2024omnigen, wanx_ace}, achieving impressive results (refer to Sec \ref{sec:related_work:image}).
The most straightforward way to implement S2V is to combine S2I with I2V, but there are two main limitations. First, S2I has greater difficulty in learning subject consistency compared to S2V, as the S2V training data naturally include multi-view dynamic variations, allowing for better understanding of the subject.
Second, transitioning from S2I to I2V can lead to information loss. For instance, when generating a back-to-front view motion, the subject's ID information may be lost because the first frame lacks it, which hinders I2V from maintaining ID consistency (see supplementary material).
Therefore, subject-consistent generation requires a specialized video model for unified processing.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{figures/data_pipeline2.pdf} 
	\caption{Data processing pipeline for cross-modal video generation. The process involves filtering, adding captions, detection, and matching stages to extract subjects from video clips and align them with the text prompts, ensuring consistent video generation.}
	\label{fig:data_pipeline}
\end{figure*}

Specifically, the subject-consistent video generation task aims to deeply and simultaneously align the content described in the text and images. To achieve this, we propose a data pipeline for the S2V task, producing training data in the form of text-image-video triplets. Two key issues must be addressed. First, prevent the leakage of image content into the generated video. Some methods \cite{chen2025multi, huang2025conceptmaster, liang2025movie, yuan2024identity, he2024id} sample key frames from a video as image conditions to reconstruct the video, but this allows the model to copy-paste image content, reducing text responsiveness. While some approaches enhance data through transformations \cite{chen2025multi}, they fail to address the rigid properties and overall lighting of the image. We focus on constructing cross-video multi-subject pairs to ensure that subjects exhibit non-rigid deformations and color variations while maintaining content matching. Second, address the issue of confusion arising from multi-subject generation. Specifically, when similar subjects trigger identical textual descriptions, it can lead to content ambiguity. To resolve this, we emphasized distinct descriptions of the subjects' appearances in the video. The appearances of multiple subjects should be distinguishable and precisely match the contents of the sampled reference images. Furthermore, we build a rephraser that rephrases user's input text prompts to include detailed description of the image content.

Our model design is based on two primary considerations: \textbf{(1)} How to simply and effectively extend a video foundation model to support S2V capabilities; \textbf{(2)} How to achieve a unified framework for single- and multi-subject consistency generation. 
Thus, we redesigned the image-text joint injection model based on pre-trained T2V and I2V models \cite{lin2025diffusion} to ensure effective cross-modal learning. 
Specifically, our method is built on the MMDiT \cite{esser2024scaling} architecture. Referring to \cite{wang2025seedvr}, full self-attention is replaced with window attention to reduce computational costs. VAE \cite{esser2021taming} and CLIP \cite{zhai2023sigmoid} are used to encode the reference images, and the encoded results are fed into the video and text branches of MMDiT, respectively.
The VAE latent provides low-level detail information, while CLIP offers high-level semantic information.
Additionally, we introduced a dynamic information injection strategy during attention calculation, allowing the insertion of one or more reference images without affecting the window size and position encoding \cite{su2024roformer}, achieving a unified model architecture for single- and multi-subject consistent video generation.


In addition, for the S2V task, we constructed evaluation datasets for portrait IDs, single subjects, and multiple subjects, and developed corresponding evaluation metrics. Since the performance of some open-source reproducible projects \cite{he2024id, yuan2024identity, huang2025conceptmaster, liang2025movie, chen2025multi} has not yet matched that of closed-source commercial solutions \cite{Vidu, Pika, Keling, Hailuo, polyak2024movie}, our focus is on comparing with commercial methods. Overall, our proposed \textbf{\textit{Phantom}} has the following contributions:

\textbf{Concepts.} (1) We are the first to clearly define the subject-to-video (S2V) task and elucidate its relationship with text-to-video (T2V) and image-to-video (I2V), as in Fig. \ref{fig:motivation}; (2) \textit{Phantom} offers a feasible path for the S2V task, focusing on high-quality alignment of both textual and visual content.

\textbf{Technology.} (1) A new data pipeline constructs cross-modal aligned triplet data, effectively addressing the issues of information leakage (copy-paste) and content confusion (multiple subjects); (2) \textit{Phantom} offers a unified framework for generating videos from both single and multiple subject references, utilizing dynamic injection scheme of various conditions at its core.

\textbf{Significance.} (1) \textit{Phantom} demonstrates superior generation quality, bridging the gap between academic research and proprietary commercialization; (2) the unified consistency generation paradigm covers subtasks such as ID generation and demonstrates significant advantages, indicating that \textit{Phantom}-like solutions have broad prospects in scenarios such as the film industry or advertising production.

% \textbf{(1)} A new data pipeline that adapts to S2V tasks and achieves cross-modal alignment; \textbf{(2)} A unified video generation framework for both single and multi-subject references; \textbf{(3)} Leading performance compared to various closed-source commercial solutions.

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.98\textwidth]{figures/framework.pdf} 
	\caption{Overview of the \textbf{\textit{Phantom}} architecture. Triplet data is encoded into latent space at the input head, and after combination, it is processed through modified MMDiT blocks to learn the alignment of different modalities. }
	\label{fig:framework}
\end{figure*}
