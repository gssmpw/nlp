\section{Experiments}
\label{sec:exps}

\subsection{Evaluation materials}
\label{sec:exp:eval_mat}

\textit{Phantom} can be fine-tuned from any video generation base model \cite{lin2025diffusion, wang2025seedvr}. The T2V and I2V pre-training stages are excluded from this evaluation. We focus on assessing the subject consistency generation capability, with additional independent evaluations for face ID-based video generation. Due to the lack of an established benchmark for subject-to-video, we constructed a specific test set and defined evaluation metrics accordingly.

We collected 50 reference images from different scenarios, covering humans, animals, products, environments, and clothing. Each reference image is paired with 3 different text prompts. To ensure confidence in each case, each text-image pair is generated with three random seeds, resulting in a total of 450 videos. For scenarios with multiple reference images, we mixed the aforementioned reference images and rewrote the text prompts to obtain a test set of 50 groups. Additionally, considering the unique value of portrait scenarios, we collected an additional 50 portrait reference images, including both celebrities and ordinary individuals, for independent evaluation of ID consistency. 

For the S2V task, the existing available state-of-the-art (SOTA) methods are closed-source commercial tools. Therefore, we evaluated and compared the latest capabilities of Vidu \cite{Vidu}, Pika \cite{Pika}, and Kling \cite{Keling}. For the ID-preserving video generation task, the commercial tool Hailuo \cite{Hailuo} demonstrated impressive results. We also evaluated an excellent open-source algorithm ConsisID \cite{yuan2024identity}.

\subsection{Quantitative results}
\label{sec:exp:quant}

We classify the S2V evaluation metrics into three major categories: video quality, text-video consistency, and subject-video consistency. First, the visualization of video quality is shown in the radar chart on the left side of Figure \ref{fig:radar}. We selected six metrics provided by VBench \cite{huang2024vbench++}  for testing and supplemented them with four inner model scores such as structure breakdown score. For text-video consistency, we used ViCLIP \cite{wang2022internvideo} to directly calculate the cosine similarity score between the text and the video. For single subject consistency, we uniformly sampled 10 frames from each video and calculated the CLIP \cite{cherti2023reproducible} and DINO \cite{oquab2023dinov2} feature Direction Scores with the reference image. Additionally, we used grounded-sam to segment the subject part of the video and calculate the CLIP and DINO scores (excluding scene graphs). For ID consistency, we used three facial recognition models to measure similarity \cite{deng2019arcface, huang2020curricularface}. 

The video quality evaluation results, shown on the left side of Figure \ref{fig:radar}, indicate that \textit{Phantom} performs slightly worse \cite{huang2024vbench++}, while excelling in other metrics. As shown in Table \ref{tab:id_eval} and \ref{tab:ip_eval}, \textit{Phantom}  leads in overall metrics for subject consistency (Identity Consistency) and prompt following. For multi-subject video generation, due to high error rates in automated subject detection and matching, we conducted a user study. We surveyed 20 users, who rated the methods on a scale of 1 to 3 (1: unusable, 2: usable, 3: satisfactory). The evaluation results, displayed in the bar chart on the right side of Figure \ref{fig:radar}, show that \textit{Phantom} 's multi-subject performance is comparable to commercial solutions, with some advantages in subject consistency.

\begin{table}[t]
	\centering
	\resizebox{\columnwidth}{!}{
		\small
		\begin{tabular}{lcccc}
			\toprule
			\multirow{2}{*}{Methods} & \multicolumn{3}{c}{Identity Consistency} & \multicolumn{1}{c}{Prompt Following} \\
			\cmidrule(lr){2-4} \cmidrule(lr){5-5}
			& FaceSim-Arc $\uparrow$ & FaceSim-Cur $\uparrow$ & FaceSim-glink $\uparrow$ & ViCLIP-T $\uparrow$ \\
			\midrule
			ConsisID                  & 0.538       & 0.417       & 0.470         & 21.76    \\
			Hailuo-ID                 & 0.542       & 0.504       & 0.557         & 23.31    \\
			Phantom-ID                & \textbf{0.581} & \textbf{0.529} & \textbf{0.590} & \textbf{24.12}   \\
			\bottomrule
		\end{tabular}
	}
	\caption{Comparison of different methods based on identity consistency and prompt following}
	\label{tab:id_eval}
\end{table}

\begin{table}[t]
	\centering
	\resizebox{\columnwidth}{!}{
		\small
		\begin{tabular}{lcccccc}
			\toprule
			\multirow{2}{*}{Methods} & \multicolumn{4}{c}{Subject Consistency} & \multicolumn{1}{c}{Prompt Following} \\
			\cmidrule(lr){2-5} \cmidrule(lr){6-6}
			& CLIP-I $\uparrow$ & DINO-I $\uparrow$ & CLIP-I-Seg $\uparrow$ & DINO-I-Seg $\uparrow$ & ViCLIP-T $\uparrow$ \\
			\midrule
			Vidu2.0            & 0.706 & 0.511 & \underline{0.724} & \textbf{0.544}  & 22.78 \\
			Pika2.1            & 0.697 & 0.498 & 0.712 & 0.534  & \underline{23.05} \\
			Kling1.6          & \textbf{0.732} & \textbf{0.554} & 0.715 & 0.569 & 21.62 \\
			Phantom-IP         & \underline{0.714} & \underline{0.523} & \textbf{0.731} & \underline{0.538} & \textbf{23.41} \\
			\bottomrule
		\end{tabular}
	}
	\caption{Comparison of different methods based on single subject consistency and prompt following. \textbf{Boldface} indicates the highest scores in each column, and \underline{underline} indicates the second-highest scores.}
	\label{tab:ip_eval}
\end{table}


\subsection{Qualitative results}
\label{sec:exp:qual}

We present the comparison results of several typical cases in Figure \ref{fig:ipid_eval}. Each generated video is displayed with four evenly sampled frames, including the first and last frames. The first two rows of Figure \ref{fig:ipid_eval}  respectively show the results of generating single- and multiple subject consistency. It can be seen that Vidu \cite{Vidu} and \textit{Phantom} exhibit balanced performance in subject consistency, visual effect, and text response. Pika \cite{Pika} performs poorly in subject consistency. Kling \cite{Keling} has a notable issue: some cases exhibit characteristics analogous to I2V approaches. For instance, the first frame of character videos almost matches the input reference image, leading to low success rates in virtual try-on scenarios. Additionally, the laptop case shows that the compared methods tend to cause deformations in rigid body movements. The last row of Figure \ref{fig:ipid_eval} shows the results of video generation for facial ID preservation. The open-source method ConsisID \cite{yuan2024identity} tends to exhibit motion blur, and has weak text response. Hailuo \cite{Hailuo} excels in visual aesthetics, but there is some loss in facial similarity. Our results are balanced across all dimensions, with particular advantage in ID consistency. More qualitative analyses are presented in supplementary materials.

\subsection{Ablation study}
\label{sec:exp:ablation}

% \begin{figure}[h]
% 	\centering
% 	\includegraphics[width=0.98\columnwidth]{figures/confused_case.pdf} 
% 	\caption{Examples of multi-subject confusion: On the left are the multi-subject reference images, while the right columns present the cases of confusion and the successful cases after improvement.}
% 	\label{fig:confused}
% \end{figure}

% \begin{table}[h]
% 	\centering
% 	\resizebox{\columnwidth}{!}{
% 	\begin{tabular}{lcc}
% 		\toprule
% 		& \textbf{\textit{w/o}} text-image alignment & \textbf{\textit{w/}} text-image alignment \\
% 		\midrule
% 		Success rate & 65\% & 95\% \\
% 		\bottomrule
% 	\end{tabular}
% 	}
% 	\caption{Success rate of multi-subject generation with and without text-image alignment.}
% 	\label{tab:success_rate}
% \end{table}

% \noindent \textbf{Multi-subject confusion issue.}  When multiple reference subjects are input simultaneously, appearance confusion may occur. Our solution aligns text descriptions with video subjects during training, ensuring distinct descriptions for each subject. During inference, a rephraser adjusts input text prompts to align with the training format. For example, in the first row of Figure \ref{fig:confused}, the original prompt "A family of three is having a meal at the table" caused confusion. The rephrased prompt "a woman in black, a young girl in white, and an elderly man in a suit eating together at the table" resolved this issue. In the second row of Figure \ref{fig:confused}, the original prompt "a girl in casual clothes walking by the beach" failed to match the reference. The rephrased prompt "a girl in a white T-shirt and jeans walking by the beach" successfully matched the reference.
% Quantitative analysis, shown in Table \ref{tab:success_rate}, indicates a significant increase in the success rate of subject-consistent generation with this method.
% Aligning image and text is crucial for multi-subject generation tasks. This approach, which requires no additional complex data structures or model designs, significantly optimizes the multi-subject confusion problem.


\begin{table}[t]
	\centering
	\resizebox{\columnwidth}{!}{
	\small
	\begin{tabular}{lcccccc}
		\toprule
		\multirow{2}{*}{\vtop{\hbox{\strut Methods}}} & \multicolumn{2}{c}{Subject Consistency} & \multicolumn{1}{c}{Prompt Following} & \multicolumn{2}{c}{Video Quality} \\
		\cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-6}
		& CLIP-I $\uparrow$ & DINO-I $\uparrow$ & ViCLIP-T $\uparrow$ & Aes score $\uparrow$ & Clarity score $\uparrow$ \\
		\midrule
		w/o CLIP & 0.693 & 0.519 & \textbf{23.63} & 62.03 & 71.40 \\
		w/o VAE & 0.512 & 0.302 & 22.79 & 48.82 & 70.76 \\
		w/ All & \textbf{0.714} & \textbf{0.523} & \underline{23.40} & \textbf{64.32} & \textbf{71.72} \\
		\bottomrule
	\end{tabular}}
	\caption{The ablation experiment results of VAE and CLIP.}
	\label{tab:vae_clip}
\end{table}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.98\columnwidth]{figures/vae_clip.pdf} 
	\caption{Qualitatively display the ablation of VAE and CLIP.}
	\label{fig:vae_clip}
\end{figure}

\noindent \textbf{Selection of visual encoder.} 
Due to differences in training methods, CLIP aligns image-text pairs and tends to extract semantic information, while VAE aims for lossless reconstruction and focuses on detailed information. As shown in Figure \ref{fig:vae_clip}, faces generated using only CLIP features are smoother and more refined but show decreased similarity. In contrast, faces generated using VAE features are sharper but may amplify undesirable details, making them contain more artifacts.
In general object scenarios, CLIP is inadequate at reproducing details like text and patterns, thus primarily serving to supplement VAE's high-level information. Quantitative results in Table \ref{tab:vae_clip} show that combining VAE and CLIP features is more advantageous. Additional ablation studies are given in supplementary materials.
