%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Preliminaries
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}
In this section, we explain the problem setting and give a brief literature review of MFLD and PoC. See Appendix \ref{sec:related_extra} for additional background information.
\subsection{Problem setting}
For a functional $G:\cP_2(\bR^d) \to \bR$, we say $G$ is differentiable when there exists a functional (referred to as a {\it first variation}): $\frac{\delta G}{\delta \mu}:~\cP_2(\bR^d) \times \bR^d \ni (\mu,x) \mapsto \frac{\delta G(\mu)}{\delta \mu}(x) \in \bR$ such that for any $\mu, \mu' \in \cP_2(\bR^d)$,
\[ \left.\frac{\rd G (\mu+\epsilon (\mu'-\mu))}{\rd\epsilon} \right|_{\epsilon=0} 
= \int  \frac{\delta G(\mu)}{\delta \mu}(x) (\mu'-\mu)(\rd x), \]
and say $G$ is linearly convex when for any $\mu, \mu' \in \cP_2(\bR^d)$,
\begin{equation}\label{eq:convexity}
G(\mu') \geq G(\mu) + \int \frac{\delta G(\mu)}{\delta \mu}(x)  (\mu'-\mu)(\rd x).      
\end{equation}

Given a differentiable and linearly convex functional $F_0 :\cP_2(\bR^d) \to \bR$ and $\lambda>0$, we consider the minimization problem of an entropy-regularized convex functional: %\citep{mei2018mean,hu2019mean,nitanda2022convex,chizat2022mean,chen2022uniform,suzuki2023convergence,kook2024sampling,nitanda2024improved,chewi2024uniform}:
\begin{equation}\label{prob:org}
    \min_{\mu \in \cP_2(\bR^d)} 
    \left\{ 
    \cL(\mu) = F_0(\mu) + \bE_{X \sim \mu}[ r(X)] + \lambda \Ent(\mu)
    \right\},
\end{equation}
where $r: \bR^d \to \bR$ is a $\lambda'$-strongly convex function (e.g., $r(x) = \lambda' \|x\|_2^2$ $(\lambda'>0)$).
We set $F(\mu) = F_0(\mu) + \bE_{\mu}[r(X)]$. 
A typical example of $F_0$ is an empirical risk of the two-layer mean-field neural network (see Example \ref{eg:mean-field-nn}).
Throughout the paper, we assume that the solution $\mu_* \in \cP_2(\bR^d)$ of the problem \eqref{prob:org} exists and make the following regularity assumption \cite{chizat2022mean,nitanda2022convex,chen2023entropic} under which $\mu_*$ is unique and satisfies the optimality condition: $\mu_* \propto \exp\left( -\frac{1}{\lambda} \frac{\delta F(\mu_*)}{\delta \mu}\right)$ (see \citet{hu2019mean,chizat2022mean} for the details).
\begin{assumption}\label{assumption:wg_regularity}
    There exists $C_1, C_2>0$ such that for any $\mu \in \cP_2(\bR^d)$, $x \in \bR^d$, $\left| \nabla \frac{\delta F_0(\mu)}{\delta \mu}(x) \right| \leq C_1$ and for any $\mu, \mu' \in \cP_2(\bR^d)$, $x, x' \in \bR^d$,    
    \begin{align*}
        &\left\| \nabla \frac{\delta F_0(\mu)}{\delta \mu}(x) - \nabla \frac{\delta F_0(\mu')}{\delta \mu}(x') \right\|_2 \\
        &~~~~~~~~~~~~~~~~\leq C_2 \left( W_2(\mu,\mu') + \| x - x'\|_2 \right),
    \end{align*}
    where $W_2$ is the $2$-Wasserstein distance.
\end{assumption}

\subsection{Mean-field Langevin dynamics and uniform-in-time propagation of chaos}
First, consider the finite-particle setting $\rho_\vx=\frac{1}{N}\sum_{i=1}^N \delta_{x^i}$ for $\vx=(x^i)_{i=1}^N \in \bR^{dN}$ and the following noisy gradient descent for $F(\rho_{\vx})$. Given $k$-th iteration $\vX_k = (X_k^1,\ldots,X_k^N)$, for each $i \in \{1,2,\ldots,N\}$, we perform
\begin{equation}\label{eq:discrete_mfld}
    X_{k+1}^i = X_k^i - \eta \nabla \frac{\delta F(\rho_{\vX_k})}{\delta \mu}(X_k^i) + \sqrt{2\lambda \eta} \xi_k^i,
\end{equation}
where $\xi_k^i \sim \cN(0,I_d)~(i\in \{1,2,\ldots,N\})$ are i.i.d. standard normal random variables and the gradient in the RHS is taken for the function: $\frac{\delta F(\rho_{\vX_t})}{\delta \mu}(\cdot): \bR^d \rightarrow \bR$.
The continuous-time representation of Eq.~\eqref{eq:discrete_mfld} is given by the $N$-tuple of SDEs $\{\vX_t\}_{t\geq 0} = \{(X_t^1,\ldots,X_t^N)\}_{t\geq 0}$:
\begin{equation}\label{eq:finite_particle_mfld}
    \rd X_t^i = - \nabla \frac{\delta F (\rho_{\vX_t})}{\delta \mu}(X_t^i)\rd t + \sqrt{2\lambda}\rd W_t^i,
\end{equation}
where $\{W_t^i\}_{t\geq 0},~(i\in \{1,\ldots,N\})$ are independent standard Brownian motions. Note that Eq.~\eqref{eq:finite_particle_mfld} is equivalent to the Langevin dynamics $\rd \vX_t = - N \nabla_{\vX} F(\rho_{\vX_t}) \rd t + \sqrt{2\lambda}\rd \vW_t$ on $\mathbb{R}^{dN}$, where $\{\vW_t\}_{t\geq 0}$ is the standard Brownian motion on $\bR^{dN}$ since $N \nabla_{x^i} F(\rho_\vx) = \nabla \frac{\delta F (\mu_{\vx})}{\delta \mu}(x^i)$ \citep{chizat2022mean}.
Therefore, $\pow[\mu,N]_t = \mathrm{Law}(\vX_t)$ converges to the Gibbs distribution
\begin{equation*}\label{eq:finite_particle_opt}
    \frac{\rd \pow[\mu,N]_*}{\rd \vx}(\vx) \propto \exp\left( - \frac{N}{\lambda}F(\rho_\vx)\right).
\end{equation*}
which minimizes the following entropy-regularized linear functional defined on $\cP_2(\bR^{dN})$: for $\pow[\mu,N]\in \cP_2(\bR^{dN})$,
\begin{equation}\label{prob:finite_particle_opt}
    \pow[\cL,N]( \pow[\mu,N]) 
    = N \bE_{\vX \sim \pow[\mu,N]}[ F(\rho_\vX)] + \lambda \Ent(\pow[\mu,N]).
\end{equation}

Next, we take the mean-field limit: $N\to \infty$ under which Eq.~\eqref{eq:finite_particle_mfld} converges to the MFLD that solves the problem Eq.~\eqref{prob:org}; 
\begin{equation}\label{eq:mfld}
    \rd X_t = - \nabla \frac{\delta F}{\delta \mu}(\mu_t)(X_t)\rd t + \sqrt{2\lambda}\rd W_t,~~~\mu_t = \mathrm{Law}(X_t),
\end{equation}
where $\{W_t\}_{t\geq 0}$ is the $d$-dimensional standard Brownian motion with $W_0=0$. Under the log-Sobolev inequality on the proximal Gibbs distribution $\hat{\mu} \propto \exp\left(-\frac{1}{\lambda}\frac{\delta F}{\delta \mu}\right)$, \citet{nitanda2022convex,chizat2022mean} showed the exponential convergence of the objective gap $\cL(\mu_t) - \cL(\mu_*)$, where $\mu_* = \argmin_{\mu \in \cP_2(\bR^d)}\cL(\mu)$. 

Therefore, $\frac{1}{N}\pow[\cL,N](\pow[\mu,N]_k)$, where $\pow[\mu,N]_k=\mathrm{Law}(\vX_k)$, is expected to approximate $\cL(\mu_*)$ through the time and mean-field limit $k \to \infty, N \to \infty$, leading to the natual question: 
\vspace{-1mm}
\begin{center}
{\it What is the convergence rate of $\frac{1}{N}\pow[\cL,N](\pow[\mu,N]_k)$ to $\cL(\mu_*)$?}
\end{center}
\vspace{-1mm}
This approximation error has been studied in the literature of PoC. Recently, \citet{suzuki2023convergence} proved the following uniform-in-time PoC for Eq.~\eqref{eq:discrete_mfld} by using the techniques in \citet{chen2022uniform}:
\begin{align}\label{eq:discrete_mfld_poc}
    \frac{1}{N}\pow[\cL,N]( \pow[\mu,N]_k) - \cL(\mu_*) 
    \leq \exp\left(-\lambda\alpha \eta k/2\right)\pow[\Delta,N]_0 + \delta_{\eta,N},  
\end{align}
where $\pow[\Delta,N]_0 = \frac{1}{N}\pow[\cL,N]( \pow[\mu,N]_0) - \cL(\mu_*)$ is the initial gap and $\delta_{\eta,N}=\frac{(\lambda \eta + \eta^2)D_1}{\lambda \alpha} + \frac{\lambda D_2}{\alpha N}$ $(\exists D_1,D_2 > 0)$ is the discretization error in time and space.
The continuous-time counterpart ($\eta \to 0$) was proved by \citet{chen2022uniform}. 
The typical estimation of LSI-constant $\alpha \gtrsim \exp(-\Theta(1/\lambda))$ (e.g., Theorem 1 in \citet{suzuki2023convergence}) using Holley and Stroock argument \citep{holley1987logarithmic} or Miclo's trick \citep{bardet2018}) suggests the exponential blow-up of the particle approximation error $\frac{\lambda D_2}{\alpha N}$ in Eq.~\eqref{eq:discrete_mfld_poc} as $\lambda \to 0$.

Afterward, this exponential dependence was removed by \citet{nitanda2024improved,chewi2024uniform} that evaluate the particle approximation error at the solution: $\frac{1}{N}\pow[\cL,N]( \pow[\mu,N]_*) - \cL(\mu_*)$ and optimization error: $\frac{1}{N}\left(\pow[\cL,N]( \pow[\mu,N]_k) - \pow[\cL,N]( \pow[\mu,N]_*) \right)$, respectively.
In the risk minimization problem setting, \citet{nitanda2024improved} proved $\frac{1}{N}\pow[\cL,N]( \pow[\mu,N]_*) - \cL(\mu_*) \leq  \frac{C}{N}$ $(\exists C>0)$ and \citet{chewi2024uniform} proved uniform-in-$N$ LSI on $\pow[\mu,N]_* \in \bR^{dN}$ with the constant estimation $\bar{\alpha} \gtrsim\frac{\lambda'}{\lambda}\exp\left(-O\left( \frac{1}{\lambda'} + \frac{1}{\lambda \lambda'} + \frac{1}{\lambda^2 \lambda'^{3}}\right)\right)$, leading to the $N$-independent convergence rate of $\frac{1}{N}\pow[\cL,N](\pow[\mu,N]_k) - \cL(\mu_*)$ up to the particle approximation error $C/N$ plus time-discretization error.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Main Results I: Improved PoC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Main Result I: Improved Propagation of Chaos for Mean-field Neural Network}\label{sec:main_results}
In this section, we present an improved propagation-of-chaos for the mean-field Langevin dynamics under the uniform directional LSI introduced below.
\begin{definition}\label{eq:conditional_gibbs}
    For $\vx^{-i}=(x^1,\ldots,x^{i-1},x^{i+1},\ldots, x^N)$ $(i \in \{1,2,\ldots,N\})$, we define a {\it conditional Gibbs distribution} $\nu_{i|-i}(\cdot|\vx^{-i})$ on $\bR^d$ by 
    \begin{equation*}
        \frac{\rd \nu_{i|-i}}{\rd x}(x|\vx^{-i})
        = \frac{\exp\left(-\frac{N}{\lambda}F(\rho_{x \cup \vx^{-i}})\right)}{\int \exp\left(-\frac{N}{\lambda}F(\rho_{\tilde{x}\cup \vx^{-i}})\right) \rd\tilde{x}}, 
    \end{equation*}
    where $\rho_{x \cup \vx^{-i}} = \frac{1}{N}\sum_{j\neq i}\delta_{x^{j}} + \frac{1}{N}\delta_{x}$.
\end{definition}

\begin{assumption}[Uniform directional LSI]\label{assumption:uniform_directional_lsi}
    There exists a constant $\alpha>0$ such that for any $\vx \in \bR^{dN}$ and $i \in \{1,2,\ldots,N\}$, $\nu_{i|-i}(\cdot|\vx^{-i})$ satisfies the LSI with the constant $\alpha$; for all $\mu \in \cP_2(\bR^d)$ absolutely continuous w.r.t. $\nu_{i|-i}(\cdot\mid \vx^{-i})$, it follows that
    \[ \KL(\mu\| \nu_{i|-i}(\cdot|\vx^{-i}) ) \leq \frac{1}{2\alpha} \FI(\mu\|\nu_{i|-i}(\cdot|\vx^{-i})). \]
\end{assumption}

We also make the following assumptions.
\begin{assumption}\label{assumption:regularity}
    A functional $F_0(\mu)$ is differentiable and linearly convex. 
\end{assumption}

The nonlinearity of $F_0$ is the key to the PoC analysis for mean-field models, thereby motivating the use of the Bregman divergence associated with $F_0$ \citep{nitanda2024improved}; for distributions $\mu, \mu' \in \cP_2(\bR^d)$,
\[ B_{F_0}(\mu,\mu') = F_0(\mu) - F_0(\mu') - \pd< \frac{\delta F_0(\mu')}{\delta \mu}, \mu-\mu'>. \]

\begin{assumption}\label{assumption:nonlinearity}
    There exists a constant $B>0$ such that for any $\vx \in \bR^{dN}$, $x \in \bR^d$, and $i \in \{1,2,\ldots,N\}$,
    \[ B_{F_0}(\rho_{x\cup \vx^{-i}},\rho_\vx) \leq \frac{B}{N^2}. \]
\end{assumption}
We give an example of training MFNNs, which satisfies Assumptions \ref{assumption:uniform_directional_lsi}, \ref{assumption:regularity}, and \ref{assumption:nonlinearity}.
\begin{example}[Training MFNN]\label{eg:mean-field-nn}
    Let $\cY \subset \bR$ be a label space, $\cZ \subset \bR^{d'}$ be an input data space, $h(x,\cdot): \cZ \to \bR$ be a function parameterized by $x \in \bR^d$, and $\ell(\cdot,\cdot): \bR\times \bR \to \bR$ is a loss function. Given training examples $\{(z_j,y_j)\}_{j=1}^n \subset \cZ \times \cY$, we consider the empirical risk: 
    \[ F_0(\mu) = \frac{1}{n}\sum_{j=1}^n \ell\left(\bE_{X\sim\mu}[h(X,z_j)],y_j\right), \]
    and $L_2$-regularizaton $r(x) = \lambda'\|x\|_2^2$.
    We assume that $\sup_{x\in \bR^d, z\in \cZ}|h(x,z)| \leq R$ and that for any $y\in\bR$, $\ell(\cdot,y)$ is convex and $L$-smooth; there exists $L > 0$ such that for any $a,b \in \bR$, $\ell(b,y) \leq \ell(a,y) + \frac{\partial \ell(a,y)}{\partial a}(b-a) + \frac{L}{2}|b-a|^2$.
    Applying this $L$-smoothness with $a=\bE_{\rho_\vx}[h(X,z_j)], b=\bE_{\rho_{x\cup\vx^{-i}}}[h(X,z_j)], y=y_j$ and taking average over $j\in\{1,2,\ldots,n\}$, we get $B_{F_0}(\rho_{x\cup \vx^{-i}},\rho_\vx) \leq \frac{L}{2n}\sum_{j=1}^n \left| \frac{h(x^i,z_j)}{N}\right|^2 \leq \frac{LR^2}{2N^2}$.
    Moreover, we assume $\sup_{|a|\leq R, y\in\cY, x\in \bR^d, z \in \cZ} \|\partial_1 \ell(a,y) \partial_x  h(x,z)\| \leq R'$.
    Then, by the contraction principle (Proposition 5.4.3 in \citet{bakry2014analysis}) with Theorem 1.4 in \citet{brigati2024heat}, Assumption \ref{assumption:uniform_directional_lsi} holds with $\alpha=\frac{2\lambda'}{\lambda}\exp\left( -\frac{R'^{2}}{2\lambda\lambda'} - \frac{4R'}{\sqrt{2\lambda\lambda'}} \right)$ (see also Lemma 6 in \citet{chewi2024uniform}).
\end{example}
The defective LSI developed by \citet{chen2022uniform} is the key condition to study MFLD in finite-particle setting. We here derive an improved variant by exploiting the problem structure and uniform directional LSI.
The proof can be found in Appendix \ref{subsec:poc_proof}.
\begin{lemma}[Defective LSI]\label{lemma:clsi} 
    Suppose Assumptions \ref{assumption:uniform_directional_lsi}, \ref{assumption:regularity}, and \ref{assumption:nonlinearity} hold. Then, it follows that for any $\pow[\mu,N]\in\cP_2(\bR^{dN})$,
    \begin{align*} 
        &\frac{\lambda}{N}\KL(\pow[\mu,N]\|\tensor[\mu,N]_*) + \bE_{\vX \sim \pow[\mu,N]}[B_{F_0}(\rho_\vX,\mu_*)] \\
        &=
        \frac{1}{N}\pow[\cL,N](\pow[\mu,N]) - \cL(\mu_*) 
        \leq 
        \frac{B}{N} 
        + \frac{\lambda}{2\alpha N} \FI(\pow[\mu,N]\|\pow[\mu,N]_*).
    \end{align*}
    %where $\FI(\pow[\mu,N]\|\pow[\mu,N]_*) = \bE_{\vx \sim \pow[\mu,N]}\left[ \left\| \nabla \log \frac{\rd \pow[\mu,N]}{\rd \pow[\mu,N]_*}(\vx) \right\|_2^2\right]$.
\end{lemma}

Lemma \ref{lemma:clsi} gives an approximation error bound between $\pow[\mu,N]$ and $\tensor[\mu,N]_*$, which shrinks up to $B/N$ error as $\pow[\mu,N] \to \pow[\mu,N]_*$ and shrinks to zero by additionally taking $N\to \infty$, meaning that each particle of the system $(X^1,\ldots,X^N)\sim\pow[\mu,N]$ becomes independent to each other. Compared to the original result \cite{chen2022uniform}, the particle approximation term $B/N$ is independent of $\alpha$, similar to \citet{nitanda2024improved}.
Note that whereas \citet{nitanda2024improved} only consider the case of $\pow[\mu,N]=\pow[\mu,N]_*$, our result allows for any distribution $\pow[\mu,N]$ at the cost of the Fisher information $\FI(\pow[\mu,N]\|\pow[\mu,N]_*)$. % and reproduce a similar result: $\frac{1}{N}\pow[\cL,N](\pow[\mu,N]_*) - \cL(\mu_*) \leq  \frac{B}{N}$, which immediately follows from the theorem with $\pow[\mu,N]=\pow[\mu,N]_*$.
%\footnote{As for the LSI on $\pow[\mu,N]_*$, we refer the reader to \citet{chewi2024uniform} that proved the uniform-in-$N$ LSI in the nonlinear setting.}
Lemma \ref{lemma:clsi} can be indeed regarded as an extended LSI on the finite-particle system and nonlinear mean-field objective, where Fisher information is lower bounded by the optimality gap up to $B/N$ error. In particular, when $F_0$ is the linear functional: $F_0(\mu)= \bE_\mu[f]~(\exists f: \bR^d \to \bR)$, the lemma reproduces the standard LSI on $\pow[\mu,N]_*$: 
\begin{equation*}
    \KL(\pow[\mu,N]\|\pow[\mu,N]_*) \leq \frac{1}{2\alpha}\FI(\pow[\mu,N]\|\pow[\mu,N]_*)
\end{equation*}
because $\pow[\mu,N]_* = \tensor[\mu,N]_*,~B_{F_0}=0$, and $B=0$ in this case. 

Therefore, Lemma \ref{lemma:clsi} is instrumental in the computational complexity analysis of MFLD in the finite-particle setting as shown in the following theorem. 
We set $\pow[\Delta_0,N] = \frac{1}{N}\pow[\cL,N](\pow[\mu,N]_0) - \cL(\mu_*)$.
\begin{theorem}[Propagation chaos for MFLD]\label{theorem:mfld_convergence}
    Suppose Assumptions \ref{assumption:wg_regularity}, \ref{assumption:uniform_directional_lsi},  \ref{assumption:regularity}, and \ref{assumption:nonlinearity} hold and consider the $L_2$-regularization: $r(x)=\lambda' \|x\|_2^2~(\lambda'>0)$. Then, 
    \begin{enumerate}[itemsep=0mm,leftmargin=5mm,topsep=0mm] 
        \item MFLD in the continuous-time \eqref{eq:finite_particle_mfld} satisfies
        \[ 
        \frac{1}{N}\pow[\cL,N](\pow[\mu,N]_t) - \cL(\mu_*) 
        \leq \frac{B}{N} + \exp(-2\alpha \lambda t) \pow[\Delta,N]_0.
        \]
        \item MFLD in the discrete-time \eqref{eq:discrete_mfld} with $\eta \lambda' < 1/2$ satisfies
        \[ 
        \frac{1}{N}\pow[\cL,N](\pow[\mu,N]_k) - \cL(\mu_*) 
        \leq \frac{B}{N} + \frac{ \delta_{\eta}}{\alpha\lambda} 
        + \exp( -\alpha\lambda\eta k )\pow[\Delta,N]_0, \]
        where $\delta_\eta =  8\eta( C_2^2 + \lambda^{\prime 2}) (\eta C_1^2 + \lambda d) 
        +  32 \eta^2 \lambda'^2( C_2^2 + \lambda^{\prime 2}) \left( \frac{\bE\left[ \left\| \vX_0 \right\|_2^2 \right]}{N} + \frac{1}{\lambda'}\left(\frac{C_1^2}{4\lambda'} + \lambda d\right) \right)$. % is the time-discretization error.
    \end{enumerate}
\end{theorem}
\begin{proof}
    We here demonstrate the convergence in the continuous-time setting.
    The distribution $\pow[\mu,N]_t=\mathrm{Law}(\vX_t)$ of Eq.~\eqref{eq:finite_particle_mfld} satisfies the following Fokker-Planck equation:
    \[ \frac{\partial \pow[\mu,N]_t}{\partial t} 
    = \lambda \nabla\cdot \left( \pow[\mu,N]_t \log \frac{\rd \pow[\mu,N]_t}{\rd \pow[\mu,N]_*}\right). \]
    By the standard argument of Langevin dynamics (e.g., \citet{vempala2019rapid}) and Lemma \ref{lemma:clsi}, we get
    \begin{align*}
        &\frac{\rd}{\rd t}(\pow[\cL,N](\pow[\mu,N]_t) - N\cL(\mu_*) - B)
        = -\lambda^2 \FI(\pow[\mu,N]_t \| \pow[\mu,N]_*) \\
        &\leq -2 \alpha \lambda (\pow[\cL,N](\pow[\mu,N]_t) - N\cL(\mu_*) - B).
    \end{align*}
    Then, the statement follows from a direct application of the Gr\"{o}nwall’s inequality. 
    The convergence in the discrete-time is also proved by incorporating one-step iterpolation argument. See Appendix \ref{subsec:poc_proof}.
\end{proof}

From this result, we see that MFLD indeed induces the PoC regarding KL-divergence. In fact, the following inequality, which is a direct consequence of Lemma \ref{lemma:clsi} with Theorem \ref{theorem:mfld_convergence} in the continuous-time,
shows that the particles $(X^i_t)_{i=1}^N \sim \pow[\mu,N]_t$ become independent as $t \to \infty$ and $N \to \infty$:
\begin{equation}\label{eq:mfld_poc}
    \frac{1}{N}\KL(\pow[\mu,N]_t \| \tensor[\mu,N]_*) 
    \leq \frac{B}{\lambda N} + \exp(-2\alpha \lambda t) \frac{\pow[\Delta,N]_0}{\lambda}.
\end{equation}
We note that the particle approximation term $B/N$ in Theorem \ref{theorem:mfld_convergence} is independent of LSI-constants, whereas the error $O(\frac{\lambda}{\alpha' N})$ obtained in \citet{suzuki2023convergence} scales inversely with an LSI constant $\alpha'$ on the proximal Gibbs distribution which can be exponentially small as $\lambda \to 0$. Whereas the term $B/N$ is comparable to \citet{nitanda2024improved,chewi2024uniform}, our convergence rate $\exp(-2\alpha \lambda t)$ in optimization is faster since their results rely on the LSI constant $\bar{\alpha}$ on $\pow[\mu,N]_*$ which is smaller than $\alpha = \frac{\lambda'}{\lambda}\exp\left( -\frac{R'^{2}}{\lambda\lambda'} - \frac{4R'}{\sqrt{\lambda\lambda'}} \right)$ (Example \ref{eg:mean-field-nn}) in general\footnote{However, we note PoC result obtained by \citet{chewi2024uniform} is applicable to non-bounded activation functions such as ReLU.}. For instance, \citet{chewi2024uniform} estimated the LSI constant $\bar{\alpha} \gtrsim\frac{\lambda'}{\lambda}\exp\left(-O\left( \frac{1}{\lambda'} + \frac{1}{\lambda \lambda'} + \frac{1}{\lambda^2 \lambda'^{3}}\right)\right)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Main Result II: Model approximation error
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Main Result II: Model Approximation Error and PoC-based Model Ensemble}\label{sec:main_results2}
In this section, we study how MFNNs trained with MFLD approximate the mean-field limit: $\bE_{X\sim \mu_*}[h(X,z)]$. Moreover, we present a PoC-based model ensemble method that further reduces the error.
Throughout this section, we focus on training MFNNs (Example \ref{eg:mean-field-nn}) and suppose $\sup_{x\in \bR^d, z\in \cZ}|h(x,z)| \leq R$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Point-wise model approximation error
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Point-wise model approximation error}\label{subsec:pw_model_error}
We consider point-wise model approximation error between $\bE_{X\sim \rho_\vX}[h(X,z)] = \frac{1}{N}\sum_{i=1}^N h(X^i,z)$ and $\bE_{X\sim \mu_*}[h(X,z)]$ on each point $z \in \cZ$, where $\vX=(X^1,\ldots,X^N)\sim \pow[\mu,N]$.
The error usually consists of the bias and variance terms where the bias means the difference between $\pow[\mu,N]$ and $\tensor[\mu_*,N]$ and the variance is due to finite-$N$ particles. In general, it is not straightforward to show the variance reduction as $N \to \infty$ since $X_i~(i=1,2,\ldots,N)$ are not independent, and hence can exhibit positive correlation.
However, in our setting, PoC helps to reduce the correlation among particles, resulting in better approximation error via the variance reduction.

Since we are concerned with the correlation between each pair of particles, we reinterpret $\KL(\pow[\mu,N] \| \tensor[\mu,N]_*)$ as the gap between their marginal distributions.
For each index subset $S \subset \{1,\ldots,N\}$, we denote by $\pow[\mu,N]_S$ the marginal distribution of $\pow[\mu,N]$ on $S$ and write $\pow[\mu,N]_{1:s} = \pow[\mu,N]_{\{1,\ldots,s\}}$,  $\pow[\mu,N]_{i} = \pow[\mu,N]_{\{i\}},~\pow[\mu,N]_{i,j} = \pow[\mu,N]_{\{i,j\}}$ for simplicity. We say the distribution $\pow[\mu,N]$ is {\it exchangeable} if the laws of $(X_{\sigma(1)},\ldots,X_{\sigma(N)})$ and $(X_1,\ldots,X_N)$ are identical for all permutation $\sigma: \{1,2,\ldots,N\} \rightarrow \{1,2,\ldots,N\}$.
\begin{lemma}\label{lemma:han_inequality}
For any integers $s, N \in \bN$ such that $s\leq N$, it follows that
\begin{equation*}
    \frac{N}{s\binom{N}{s}} \sum_{|S| = s} \KL( \pow[\mu,N]_S \| \tensor[\mu_*,s]) 
    \leq \KL(\pow[\mu,N] \| \tensor[\mu_*,N]).
\end{equation*}
In particular, if $\pow[\mu,N]$ is exchangeable, we get
\begin{equation*}
    \frac{N}{s}  \KL( \pow[\mu,N]_{1:s} \| \tensor[\mu_*,s]) 
    \leq \KL(\pow[\mu,N] \| \tensor[\mu_*,N]).
\end{equation*}
\end{lemma}
\begin{proof}
    The assertion holds by the direct application of Han's inequality. See Appendix \ref{subsec:pw_model_error_proof}.
\end{proof}
We here give the model approximation bound using $\KL(\pow[\mu,N]\|\tensor[\mu_*,N])$ with the proof to show how the above lemma helps to control the correlation across particles.
\begin{proposition}\label{prop:pw_model_approximation}
    Suppose $\pow[\mu,N]$ is exchangeable. Then, it follows that for any $z \in \cZ$,
    \begin{align*} 
        &\bE_{\vX \sim \pow[\mu,N]}\left[ \left( \bE_{X \sim \rho_\vX}[ h(X,z)] - \bE_{X\sim\mu_*}[h(X,z)]\right)^2 \right]  \\
        &\leq \frac{4R^2}{N} + 8R^2\sqrt{ \frac{\KL(\pow[\mu,N]\|\tensor[\mu_*,N])}{N}}.\
    \end{align*}
\end{proposition}
\begin{proof}
    We here decompose the error as follows.
    \begin{align*}
        &\bE_{\vX \sim \pow[\mu,N]}\left[ \left( \bE_{X \sim \rho_\vX}[ h(X,z)] - \bE_{X\sim\mu_*}[h(X,z)]\right)^2 \right] \\
        &= \frac{1}{N^2}\bE_{\vX \sim \pow[\mu,N]}\left[ \sum_{i=1}^N\left( h(X_i,z) - \bE_{X\sim\mu_*}[h(X,z)]\right)^2 \right] \\
        &+ \frac{1}{N^2}\bE_{\vX \sim \pow[\mu,N]}
        \Bigg[ \sum_{i\neq j}\left( h(X_i,z) - \bE_{X\sim\mu_*}[h(X,z)]\right) \\
        &~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\cdot\left( h(X_j,z) - \bE_{X\sim\mu_*}[h(X,z)]\right) \Bigg].
    \end{align*}
    Using the boundedness of $h$, the first term can be upper bounded by $4R^2/N$. The second term can be evaluated as follows. Set $H(X_i)=h(X_i,z) - \bE_{X\sim\mu_*}[h(X,z)]$. Then,
    \begin{align*}
        &\bE_{\vX \sim \pow[\mu,N]}\left[ H(X_i)H(X_j) \right]\\
        &= \bE_{(X_i,X_j) \sim \pow[\mu,N]_{i,j}}\left[ H(X_i)H(X_j) \right] \\
        &= \bE_{(X_i,X_j) \sim \tensor[\mu_*,2]}\left[ H(X_i)H(X_j) \right] \\
        &+ (\bE_{(X_i,X_j) \sim \pow[\mu,N]_{i,j}} - \bE_{(X_i,X_j) \sim \tensor[\mu_*,2]})\left[ H(X_i)H(X_j) \right] \\
        &\leq 8R^2 \TV (\pow[\mu,N]_{1,2}, \tensor[\mu_*,2]) \\
        &\leq 4R^2 \sqrt{ 2\KL(\pow[\mu,N]_{1,2}\|\tensor[\mu_*,2])}, 
        %&\leq 8R^2 \sqrt{ \frac{\KL(\pow[\mu,N]\|\tensor[\mu_*,N])}{N}}.
    \end{align*}
    where $\TV$ is the TV-norm and we used Pinsker's inequality.
    Applying Lemma \ref{lemma:han_inequality} with $s=2$, we finish the proof.
\end{proof}
In the proof, we see that KL-divergence controls the cross term by absorbing the difference between marginal distributions $\pow[\mu,N]_{i,j}$ and $\tensor[\mu,2]$. By combining this result with the PoC for MFLD (Theorem \ref{theorem:mfld_convergence}), we arrive at the model approximation error achieved by MFLD. 

\begin{theorem}\label{theorem:point_approximation_mfld}
    Under the same conditions as in Theorem \ref{theorem:mfld_convergence}, we run MFLD in the discrete-time, with $\eta \lambda' < 1/2$ and $\vX_0 \sim \tensor[\mu,N]_0$. Then we get
    \begin{align*}
        &\bE_{\vX \sim \pow[\mu,N]_k}\left[ \left( \bE_{X \sim \rho_\vX}[ h(X,z)] - \bE_{X\sim\mu_*}[h(X,z)]\right)^2 \right]  \notag\\
        &\leq \frac{4R^2}{N} + 8R^2 \sqrt{\frac{B}{\lambda N} + \frac{\delta_\eta}{\alpha \lambda^2} + \exp\left(-\alpha \lambda \eta k\right)\frac{\pow[\Delta,N]_0}{\lambda}}. 
    \end{align*}
\end{theorem}
Note that exchangeability of $\pow[\mu,N]_k$ is satisfied at all iterations because of the symmetric structure of problem and initialization with respect to particles. 

\paragraph{Model Ensemble}
We introduce the PoC-based model ensemble to further reduce the approximation error. We first train $M$ MFNNs of $N$-neurons in parallel with the same settings and obtain sets of optimized particles $\vX_j~(j=1,2,\ldots,M)$ where each $\vX_j = (X^1_j\ldots,X^N_j)$ represents each network and they are independent to each other. We then integrate them into a single network of $MN$-neurons as follows:
\begin{equation}\label{eq:merged_network}
    \frac{1}{M}\sum_{j=1}^M \bE_{X\sim \rho_{\vX_j}}[h(X,z)]
    = \frac{1}{MN}\sum_{j=1}^M \sum_{i=1}^N h(X^i_j,z).
\end{equation}

Because of the independence of networks $\{\vX_j\}_{j=1}^M$, variance reduction occurs, resulting in the improved approximation error. Indeed, by extending Proposition \ref{prop:pw_model_approximation} into an ensemble setting (see Proposition \ref{prop:pw_poc_merge}) and using PoC (Theorem \ref{theorem:mfld_convergence}), we get the following bound.
The proof is deferred to Appendix \ref{subsec:pw_model_error_proof}.
\begin{theorem}\label{theorem:point_approximation_multiple_mfld}
    Under the same conditions as in Theorem \ref{theorem:mfld_convergence}, we run $M$-parallel MFLD in the discrete time independently, with $\eta \lambda' < 1/2$ and $\vX_{j,0} \sim \tensor[\mu,N]_0$ $(j=1,2,\ldots,M)$. Then
    \begin{align*} 
        &\bE_{\{\vX_{j,k}\}_{j=1}^M}\left[ \left( \frac{1}{M}\sum_{j=1}^M\bE_{\rho_{\vX_{j,k}}}[ h(X,z)] - \bE_{\mu_*}[h(X,z)]\right)^2 \right] \\
        &\leq \frac{4R^2}{MN} + \frac{8R^2}{M}\sqrt{\frac{B}{\lambda N} + \frac{\delta_\eta}{\alpha \lambda^2} + \exp(-\alpha \lambda \eta k) \frac{\pow[\Delta,N]_0}{\lambda}} \\
        &+2R^2 \left(\frac{B}{\lambda N} + \frac{\delta_\eta}{\alpha \lambda^2} + \exp(-\alpha \lambda k) \frac{\pow[\Delta,N]_0}{\lambda}\right), 
    \end{align*}
    where $\vX_{j,k}$ is the particles at $k$-iteration for $j$-th network.
\end{theorem}
For simplicity of comparison, we consider the bound $\frac{4R^2}{MN} + \frac{8R^2}{M} \sqrt{\frac{B}{\lambda N}} + \frac{2R^2 B}{\lambda N}$ obtained in the limit $k\to \infty,~\eta\to0$. Then, we see that this bound becomes smaller than the error $\frac{4R^2}{N} + 8R^2 \sqrt{\frac{B}{\lambda N}}$ achieved by single network training (Theorem \ref{theorem:point_approximation_mfld}) by using sufficiently large $M,N$ becase of the better dependence on $M,N$.


%Given the same particles of predictors for both methods, that is $s=Ms'$, we see the improvement of the approximation error from $O\left( \frac{1}{Ms'} + \frac{1}{\sqrt{\lambda N}}\right)$ to $O\left( \frac{1}{Ms'} + \frac{1}{M \sqrt{\lambda N}} + \frac{1}{\lambda N}\right)$.
%Moreover, the best achievable error is also improved. In fact, by instantiatin Eq.~\eqref{eq:approx_error_pruning} with $s=\sqrt{\lambda N}$ and Eq.~\eqref{eq:approx_error_merging} with $s'=M=\sqrt{\lambda N}$, we obtain $O\left( \frac{1}{\sqrt{\lambda N}}\right)$ and $O\left( \frac{1}{\lambda N} \right)$ errors, respectively.

\paragraph{Remark.} Our result can extend to randomly pruned networks. That is, we consider randomly pruning $(N-s)$-neurons after training the MFNN of $N$-neurons.
Then, we get the counterpart of Proposition \ref{prop:pw_model_approximation} as follows.
\begin{align*} 
    &\bE_{\vX \sim \pow[\mu,N]_{1:s}}\left[ \left( \bE_{X \sim \rho_\vX}[ h(X,z)] - \bE_{X\sim\mu_*}[h(X,z)]\right)^2 \right]  \\
    &\leq \frac{4R^2}{s} + 8R^2\sqrt{ \frac{\KL(\pow[\mu,N]\|\tensor[\mu_*,N])}{N}},
\end{align*}
where $\pow[\mu,N]_{1:s}$ is the distribution of remaining neurons. Theorem \ref{theorem:point_approximation_multiple_mfld} can also extend to the ensemble model of randomly pruned networks in the same way.
%We train $M$ mean-field neural networks of $N$-neurons in parallel and then randomly prune $(N-s')$-neurons from each network and integrate them into a single network of $Ms'$-neurons.
%\begin{align*} 
%    &\bE_{\{\vX_j\}_{j=1}^M}\left[ \left( \frac{1}{M}\sum_{j=1}^M\bE_{X \sim \rho_{\vX_j}}[ h(X,z)] - \bE_{X\sim\mu_*}[h(X,z)]\right)^2 \right] \\
%    &\leq \frac{4R^2}{Ms'} + \frac{8R^2}{M}\sqrt{ \frac{\KL(\pow[\mu,N]\|\tensor[\mu_*,N])}{N}} 
%    + \frac{2R^2 \KL(\pow[\mu,N] \| \tensor[\mu,N]_*)}{N}. 
%\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Uniform model approximation error
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Uniform model approximation error}\label{subsec:uniform_model_error}
We here consider uniform model approximation error over $z \in \cZ$, which is more useful than point-wise evaluation in the machine learning scenario such as generalization analysis. The uniform bound essentially requires complexity evaluation of the model, and hence we make the additional assumption to control the complexity. 

\begin{assumption}\label{assumption:model_constraint}~
    \begin{itemize}[itemsep=0mm,leftmargin=5mm,topsep=0mm]
        \item The data domain is bounded: $\cZ \subset [-1,1]^{d} \subset \bR^{d}$
        \item There exists $\beta>0$ such that for any $x\in \bR^d$, $z, z' \in \cZ$, 
        \[ | h(x,z) - h(x,z')| \leq \beta \|x\|_2  \| z - z' \|_2. \]
    \end{itemize}
\end{assumption}
For example, $h(x,z)= \frac{R}{3}(\tanh(x^{1\top}z + x^2) + 2 \tanh(x^3)),~(x^1 \in \bR^{d}, (x^2, x^3) \in \bR^2)$, used in \citet{suzuki2023featurelearning} satisfies the above assumption with $\beta=\frac{R}{3}$ due to $1$-Lipschitz continuity of $\tanh$.

Given random variables $\{\vX_j\}_{j=1}^M,~(\vX_j = (X^1_j,\ldots,X^N_j))$, we consider the empirical Rademacher complexity of the function class $\cF = \{ x \mapsto h(x,z) \mid z \in \cZ \}$:
\[ \hat{\cR}_{N,M}(\cF) 
    = \bE_{\sigma}
    \left[ 
        \sup_{f \in \cF} \left|\frac{1}{MN}\sum_{j=1}^M\sum_{i=1}^N \sigma^i_j f(X^i_j)  \right|
    \right], \] 
where the expectation is taken over the Rademacher random variables $\sigma=(\sigma^i_j)$ which are i.i.d. with the probability $\bP[\sigma^i_j=1]=\bP[\sigma^i_j=-1]=1$. Here, we utilize the uniform laws of large numbers to evaluate the approximation error of an ensemble model defined by $\tensor[\mu,N]_*$; note that the result for a single model is obtained as a special case $M=1$.
\begin{lemma}\label{lemma:uniform_lln}
    Let $\vX_j \sim \tensor[\mu,N]_* (j=1,2,\ldots,M)$ be independent random variables.
    For $\delta \in (0,1)$, it follows that with high probability $1-\delta$,
    \begin{align*}
        &\left\| \frac{1}{M}\sum_{j=1}^M\bE_{\rho_{\vX_j}}[h(X,\cdot)] - \bE_{\mu_*}[h(X,\cdot)] \right\|_\infty \\
        &\leq 2\bE_{\{\vX_j\}_{j=1}^M}\left[\hat{\cR}_{N,M}(\cF)\right] 
        + R \sqrt{ \frac{2\log(1/\delta)}{MN}}.
    \end{align*}
\end{lemma}
\begin{proof}
    The lemma follows directly by applying the uniform law of large numbers to the function class $\cF$ (see, for instance, \citet{mohri2012foundations}.
\end{proof}
The complexity $\bE_{\{\vX_j\}_{j=1}^M}\left[\hat{\cR}_{N,M}(\cF)\right]$ can be then evaluated by Dudley's entropy integral (Lemma \ref{lemma:dudley}) under Assumption \ref{assumption:model_constraint} and the boundedness $|h(x,z)| \leq R$. By using the variational formulation of KL-divergence (e.g., Corollary 4.15 in \citet{boucheron2013concentration}), we translate the result of Lemma \ref{lemma:uniform_lln} into the approximation error of an ensemble model obtained by $M$ independent parallel iterates $\vX_{j,k} \sim \pow[\mu,N]_k~(j=1,2,\ldots,M)$  of MFLDs. Combining these techniques with Theorem \ref{theorem:mfld_convergence}, we conclude the following theorem. 

\begin{theorem}\label{theorem:uniform_approximation_multiple_mfld}
    Suppose Assumption \ref{assumption:model_constraint} and the same conditions as in Theorem \ref{theorem:mfld_convergence} hold. Run $M$-parallel MFLD in the discrete time independently, with $\eta \lambda' < 1/2$ and $\vX_{j,0} \sim \tensor[\mu,N]_0 (j=1,2,\ldots,M)$. Then we get
    \begin{align*}
        &\bE_{\{\vX_{j,k}\}}\left[\left\| \frac{1}{M}\sum_{j=1}^M \bE_{X\sim \rho_{\vx_{j,k}}}[h(X,\cdot)] - \bE_{X\sim \mu_*} [h(X,\cdot)]\right\|_\infty\right] \\
        &~~~~=\tilde{O}\left( R\sqrt{\frac{d}{MN} + \frac{dB}{\lambda N}
        + \frac{d\lambda}{MN(\lambda + MB)}} \right) \\
        &~~~~+ O\left( R\sqrt{\frac{d\lambda M N}{\lambda + MB}} \left(\frac{ \delta_{\eta}}{\alpha\lambda^2} 
        + \frac{1}{\lambda}\exp( -\alpha\lambda\eta k )\pow[\Delta,N]_0 \right) \right).
    \end{align*}
\end{theorem}
Here, the $\tilde{O}$-notation hides logarithmic factors. As for the concrete bound and proofs, see Appendix \ref{subsec:uniform_model_error_proof}. The term $\sqrt{\frac{d}{MN} + \frac{B}{\lambda N} + \frac{d\lambda}{MN(\lambda + MB)}}$ represents the particle approximation error due to finite $N$ particles, and even when $M=1$, they improve upon the bound in \citet{suzuki2023convergence,suzuki2023featurelearning} by removing the LSI constant $\alpha$ from the corresponding term. And the upper bound shows the improvement as $M$ increases.

