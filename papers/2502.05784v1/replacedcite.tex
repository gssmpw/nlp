\section{Related work}
%For additional information and extended discussions of related works, refer to Appendix \ref{sec:related_extra}.

%paragraph{Mean field optimization} \label{intro:mfo}____ proved the uniform-in-time PoC for MFLD, suggesting that the finite particle discretization error shrinks at a constant pace as $N \rightarrow \infty$. ____ refined the existing approximation error bound ____ by leveraging on the problem structure in risk minimization, removing the dependence on the LSI constant of the proximal Gibbs measures. A key assumption in this work is that the optimality condition ____ is satisfied. 

%Subseqeuntly, ____ proved the uniform-in-$N$ LSI constant. In particular, ____ reduced the scaling of the LSI constant to a single exponent that is dependent on both the strong regularization and entropy regularization terms. This uniform-in-$N$ LSI constant is then combined with the LSI-constant-free particle approximation error ____ to  derive tighter generalization bounds ____.

%\paragraph{Ensembling and model merging}   Ensemble methods improve predictive performance by combining the outputs of multiple models during inference ____. The average voting method which takes the mean of predictions ____ has been applied to deep learning problems ____. In contrast, model merging consolidates multiple models into a single one by combining individual parameters. ____ refers to averaging the weights of independently fine-tuned models as \textit{model soups}, exhibiting considerable success particularly in the optimization of LLMs ____. While other novel strategies for merging LLMs have been proposed ____, ____ showed that basic weight averaging methods can perform competitively if constituent models are similar.