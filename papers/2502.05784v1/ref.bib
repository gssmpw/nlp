@book{mohri2012foundations,
  author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  title = {Foundations of Machine Learning},
  year = {2012}
}

@inproceedings{koltchinskii2005exponential,
  title={Exponential convergence rates in classification},
  author={Koltchinskii, Vladimir and Beznosova, Olexandra},
  booktitle={International Conference on Computational Learning Theory},
  pages={295--307},
  year={2005}
}

@article{audibert2007fast,
  title={Fast learning rates for plug-in classifiers},
  author={Audibert, Jean-Yves and Tsybakov, Alexandre B},
  journal={The Annals of statistics},
  volume={35},
  number={2},
  pages={608--633},
  year={2007}
}

@article{laurent2000adaptive,
  title={Adaptive estimation of a quadratic functional by model selection},
  author={Laurent, Beatrice and Massart, Pascal},
  journal={The Annals of statistics},
  volume={28},
  number={5},
  pages={1302--1338},
  year={2000}
}

@article{bartlett2006convexity,
  title={Convexity, Classification, and Risk Bounds},
  author={Bartlett, Peter L and Jordan, Michael I and McAuliffe, Jon D},
  journal={Journal of the American Statistical Association},
  volume={101},
  number={473},
  year={2006},
  pages={138--156}  
}

@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={SIAM Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018}
}

@article{dieuleveut2017harder,
  title={Harder, better, faster, stronger convergence rates for least-squares regression},
  author={Dieuleveut, Aymeric and Flammarion, Nicolas and Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={3520--3570},
  year={2017}
}

@article{dieuleveut2016nonparametric,
  title={Nonparametric stochastic approximation with large step-sizes},
  author={Dieuleveut, Aymeric and Bach, Francis},
  journal={The Annals of Statistics},
  volume={44},
  number={4},
  pages={1363--1399},
  year={2016}
}

@inproceedings{pillaud2018exponential,
  title={Exponential Convergence of Testing Error for Stochastic Gradient Methods},
  author={Pillaud-Vivien, Loucas and Rudi, Alessandro and Bach, Francis},
  booktitle={Proceedings of Conference on Learning Theory 31},
  pages={1--47},
  year={2018}
}

@inproceedings{rakhlin2012making,
  title={Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization},
  author={Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  booktitle={Proceedings of International Conference on Machine Learning 29},
  pages={1571--1578},
  year={2012}
}

@article{lacoste2012simpler,
  title={A Simpler Approach to Obtaining an ${O}(1/t)$ Convergence Rate for the Projected Stochastic Subgradient Method},
  author={Lacoste-Julien, Simon and Schmidt, Mark and Bach, Francis},
  journal={arXiv preprint arXiv:1212.2002},
  year={2012}
}

@Article{zhang2004statistical,
 author    ={Zhang, Tong},
 title     ={Statistical Behavior and Consistency of Classification Methods Based on Convex Ris Minimization},
 journal   ={The Annals of Statistics},
 year      ={2004},
 volume    ={32},
 number    ={1},
 pages     ={56--134}
}

@article{pinelis1994optimum,
  title={Optimum bounds for the distributions of martingales in Banach spaces},
  author={Pinelis, Iosif},
  journal={The Annals of Probability},
  pages={1679--1706},
  year={1994}
}

@inproceedings{frostig2015competing,
  title={Competing with the empirical risk minimizer in a single pass},
  author={Frostig, Roy and Ge, Rong and Kakade, Sham M and Sidford, Aaron},
  booktitle={Proceedings of Conference on Learning Theory 28},
  pages={728--763},
  year={2015}
}

@inproceedings{rakhlin2017equivalence,
  title={On Equivalence of Martingale Tail Bounds and Deterministic Regret Inequalities},
  author={Rakhlin, Alexander and Sridharan, Karthik},
  booktitle={Proceedings of Conference on Learning Theory 30},
  pages={1704--1722},
  year={2017}
}

@inproceedings{hardt2016train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={Proceedings of International Conference on Machine Learning 33},
  pages={1225--1234},
  year={2016}
}

@inproceedings{liu2017algorithmic,
  title={Algorithmic Stability and Hypothesis Complexity},
  author={Liu, Tongliang and Lugosi, G{\'a}bor and Neu, Gergely and Tao, Dacheng},
  booktitle={Proceedings of International Conference on Machine Learning 34},
  pages={2159--2167},
  year={2017}
}

@inproceedings{allen2019convergence,
  title={A Convergence Theory for Deep Learning via Over-Parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={Proceedings of International Conference on Machine Learning 36},
  pages={242--252},
  year={2019}
}

@book{nes2004,
 title={Introductory Lectures on Convex Optimization: A Basic Course},
 author={Nesterov, Yurii},
 publisher={Kluwer Academic Publishers},
 year={2004}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@article{robbins1951stochastic,
  author={Robbins, Herbert and Monro, Sutton},
  title={A Stochastic Approximation Method},
  journal={The Annals of Mathematical Statistics},
  volume={22},
  number={3},
  pages={400--407},
  year={1951}
}

@article{ghadimi2013stochastic,
  title={Stochastic First-and Zeroth-Order Methods for Nonconvex Stochastic Programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013}
}

@article{polyak1992acceleration,
  title={Acceleration of Stochastic Approximation by Averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM Journal on Control and Optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992}
}

@techreport{ruppert1988efficient,
  title={Efficient Estimations from a Slowly Convergent {R}obbins-{M}onro Process},
  author={Ruppert, David},
  year={1988},
  institution={Cornell University Operations Research and Industrial Engineering}
}

@article{nemirovski2009robust,
  title={Robust Stochastic Approximation Approach to Stochastic Programming},
  author={Nemirovski, Arkadii S and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on Optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009}
}


@article{bubeck2015convex,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015}
}

@article{nesterov2009primal,
  title={Primal-dual subgradient methods for convex problems},
  author={Nesterov, Yurii},
  journal={Mathematical programming},
  volume={120},
  number={1},
  pages={221--259},
  year={2009}
}

@book{nemirovskii1983problem,
  title={Problem Complexity and Method Efficiency in Optimization},
  author={Nemirovskii, Arkadii and Yudin, David Borisovich},
  year={1983},
  publisher={John Wiley}
}

@article{rosasco2004loss,
  title={Are loss functions all the same?},
  author={Rosasco, Lorenzo and Vito, Ernesto De and Caponnetto, Andrea and Piana, Michele and Verri, Alessandro},
  journal={Neural Computation},
  volume={16},
  number={5},
  pages={1063--1076},
  year={2004},
  publisher={MIT Press}
}

@article{schmidt2017minimizing,
  title={Minimizing Finite Sums with the Stochastic Average Gradient},
  author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal={Mathematical Programming},
  volume={162},
  number={1-2},
  pages={83--112},
  year={2017}
}

@inproceedings{allen2017katyusha,
  title={Katyusha: The First Direct Acceleration of Stochastic Gradient Methods},
  author={Allen-Zhu, Zeyuan},
  booktitle={Proceedings of Annual ACM SIGACT Symposium on Theory of Computing 49},
  pages={1200--1205},
  year={2017},
  organization={ACM}
}

@article{tsybakov2004optimal,
  title={Optimal aggregation of classifiers in statistical learning},
  author={Tsybakov, Alexander B},
  journal={The Annals of Statistics},
  volume={32},
  number={1},
  pages={135--166},
  year={2004}
}

@article{caponnetto2007optimal,
  title={Optimal rates for the regularized least-squares algorithm},
  author={Caponnetto, Andrea and De Vito, Ernesto},
  journal={Foundations of Computational Mathematics},
  volume={7},
  number={3},
  pages={331--368},
  year={2007}
}

@article{smale2006online,
  title={Online learning algorithms},
  author={Smale, Steve and Yao, Yuan},
  journal={Foundations of computational mathematics},
  volume={6},
  number={2},
  pages={145--170},
  year={2006}
}

@article{ying2006online,
  title={Online regularized classification algorithms},
  author={Ying, Yiming and Zhou, D-X},
  journal={IEEE Transactions on Information Theory},
  volume={52},
  number={11},
  pages={4775--4788},
  year={2006}
}

@article{cesa2004generalization,
  title={On the generalization ability of on-line learning algorithms},
  author={Cesa-Bianchi, Nicolo and Conconi, Alex and Gentile, Claudio},
  journal={IEEE Transactions on Information Theory},
  volume={50},
  number={9},
  pages={2050--2057},
  year={2004}
}

@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={Journal of machine learning research},
  volume={2},
  number={Mar},
  pages={499--526},
  year={2002}
}

@inproceedings{rahimi2007random,
  title={Random Features for Large-scale Kernel Machines},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in Neural Information Processing Systems 20},
  pages={1177--1184},
  year={2007}
}

@inproceedings{kakade2009generalization,
  title={On the generalization ability of online strongly convex programming algorithms},
  author={Kakade, Sham M and Tewari, Ambuj},
  booktitle={Advances in Neural Information Processing Systems 22},
  pages={801--808},
  year={2009}
}

@inproceedings{agarwal2009information,
  title={Information-Theoretic Lower Bounds on the Oracle Complexity of Convex Optimization},
  author={Agarwal, Alekh and Wainwright, Martin J and Bartlett, Peter L and Ravikumar, Pradeep K},
  booktitle={Advances in Neural Information Processing Systems 22},
  pages={1--9},
  year={2009}
}

@inproceedings{xiao2009dual,
  title={Dual averaging method for regularized stochastic learning and online optimization},
  author={Xiao, Lin},
  booktitle={Advances in Neural Information Processing Systems 22},
  pages={2116--2124},
  year={2009}
}

@inproceedings{bach2011non,
  title={Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning},
  author={Bach, Francis and Moulines, Eric},
  booktitle={Advances in Neural Information Processing Systems 24},
  pages={451--459},
  year={2011}
}

@inproceedings{bach2013non,
  title={Non-Strongly-Convex Smooth Stochastic Approximation with Convergence Rate ${O}(1/n)$},
  author={Bach, Francis and Moulines, Eric},
  booktitle={Advances in Neural Information Processing Systems 26},
  pages={773--781},
  year={2013}
}

@inproceedings{johnson2013accelerating,
  title={Accelerating Stochastic Gradient Descent using Predictive Variance Reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Advances in Neural Information Processing Systems 26},
  pages={315--323},
  year={2013}
}

@inproceedings{defazio2014saga,
  title={SAGA: A Fast Incremental Gradient Method with Support for Non-Strongly Convex Composite Objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Advances in neural information processing systems 27},
  pages={1646--1654},
  year={2014}
}

@inproceedings{nitanda2014stochastic,
  title={Stochastic Proximal Gradient Descent with Acceleration Techniques},
  author={Nitanda, Atsushi},
  booktitle={Advances in Neural Information Processing Systems 27},
  pages={1574--1582},
  year={2014}
}

@inproceedings{liu2016stein,
  title={Stein variational gradient descent: A general purpose bayesian inference algorithm},
  author={Liu, Qiang and Wang, Dilin},
  booktitle={Advances in neural information processing systems 29},
  pages={2378--2386},
  year={2016}
}

@inproceedings{murata2017doubly,
  title={Doubly Accelerated Stochastic Variance Reduced Dual Averaging Method for Regularized Empirical Risk Minimization},
  author={Murata, Tomoya and Suzuki, Taiji},
  booktitle={Advances in Neural Information Processing Systems 30},
  pages={608--617},
  year={2017}
}

@inproceedings{rudi2017generalization,
  title={Generalization properties of learning with random features},
  author={Rudi, Alessandro and Rosasco, Lorenzo},
  booktitle={Advances in Neural Information Processing Systems 30},
  pages={3215--3225},
  year={2017}
}

@inproceedings{jacot2018neural,
  title={Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in Neural Information Processing Systems 31},
  pages = {8580--8589},
  year={2018}
}

@inproceedings{carratino2018learning,
  title={Learning with sgd and random features},
  author={Carratino, Luigi and Rudi, Alessandro and Rosasco, Lorenzo},
  booktitle={Advances in Neural Information Processing Systems 31},
  pages={10192--10203},
  year={2018}
}

@inproceedings{pillaud2018statistical,
  title={Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes},
  author={Pillaud-Vivien, Loucas and Rudi, Alessandro and Bach, Francis},
  booktitle={Advances in Neural Information Processing Systems 31},
  pages={8114--8124},
  year={2018}
}

@inproceedings{erdogdu2018global,
  title={Global non-convex optimization with discretized diffusions},
  author={Erdogdu, Murat A and Mackey, Lester and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems 31},
  pages={9671--9680},
  year={2018}
}

@inproceedings{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Advances in Neural Information Processing Systems 31},
  pages={3040--3050},
  year={2018}
}

@inproceedings{xu2018global,
  title={Global convergence of Langevin dynamics based algorithms for nonconvex optimization},
  author={Xu, Pan and Chen, Jinghui and Zou, Difan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems 31},
  pages={3122--3133},
  year={2018}
}

@inproceedings{rotskoff2018parameters,
  title={Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks},
  author={Rotskoff, Grant M and Vanden-Eijnden, Eric},
  booktitle={Advances in Neural Information Processing Systems 31},
  pages={7146--7155},
  year={2018}
}

@inproceedings{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={8139--8148},
  year={2019}
}

@inproceedings{wei2019regularization,
  title={Regularization matters: Generalization and optimization of neural nets vs their induced kernel},
  author={Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={9712--9724},
  year={2019}
}

@inproceedings{su2019learning,
  title={On Learning Over-parameterized Neural Networks: A Functional Approximation Perspective},
  author={Su, Lili and Yang, Pengkun},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={2637--2646},
  year={2019}
}

@inproceedings{zou2019improved,
  title={An improved analysis of training over-parameterized deep neural networks},
  author={Zou, Difan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={2053--2062},
  year={2019}
}

@inproceedings{cao2019generalization,
  title={Generalization bounds of stochastic gradient descent for wide and deep neural networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={10836--10846},
  year={2019}
}

@inproceedings{yehudai2019power,
  title={On the power and limitations of random features for understanding neural networks},
  author={Yehudai, Gilad and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={6598--6608},
  year={2019}
}

@inproceedings{bietti2019inductive,
  title={On the inductive bias of neural tangent kernels},
  author={Bietti, Alberto and Mairal, Julien},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={12873--12884},
  year={2019}
}

@inproceedings{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  booktitle={Advances in neural information processing systems 32},
  pages={8570--8581},
  year={2019}
}

@inproceedings{mucke2019beating,
  title={Beating SGD Saturation with Tail-Averaging and Minibatching},
  author={M{\"u}cke, Nicole and Neu, Gergely and Rosasco, Lorenzo},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={12568--12577},
  year={2019}
}

@inproceedings{vempala2019rapid,
  title={Rapid convergence of the unadjusted langevin algorithm: Isoperimetry suffices},
  author={Vempala, Santosh and Wibisono, Andre},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={8094--8106},
  year={2019}
}

@inproceedings{ronen2019convergence,
  title={The convergence rate of neural networks for learned functions of different frequencies},
  author={Ronen, Basri and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={4763--4772},
  year={2019}
}

@inproceedings{ghorbani2019limitations,
  title={Limitations of lazy training of two-layers neural network},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={9111--9121},
  year={2019}
}

@inproceedings{allen2019can,
  title={What Can ResNet Learn Efficiently, Going Beyond Kernels?},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={9017--9028},
  year={2019}
}

@inproceedings{li2019stochastic,
  title={Stochastic runge-kutta accelerates langevin monte carlo and beyond},
  author={Li, Xuechen and Wu, Yi and Mackey, Lester and Erdogdu, Murat A},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={7748--7760},
  year={2019}
}

@inproceedings{suzuki2020generalization,
  title={Generalization bound of globally optimal non-convex neural network training: Transportation map estimation by infinite dimensional Langevin dynamics},
  author={Suzuki, Taiji},
  booktitle={Advances in Neural Information Processing Systems 33},    
  year={2020}
}

@inproceedings{suzuki2019deep,
  title={Deep learning is adaptive to intrinsic dimensionality of model smoothness in anisotropic Besov space},
  author={Suzuki, Taiji and Nitanda, Atsushi},
  booktitle={Advances in Neural Information Processing Systems 34},  
  year={2021}
}

@inproceedings{nitanda2020particle,
  title={Particle dual averaging: Optimization of mean field neural networks with global convergence rate analysis},
  author={Nitanda, Atsushi and Wu, Denny and Suzuki, Taiji},
  booktitle={Advances in Neural Information Processing Systems 34},    
  pages={19608--19621},    
  year={2021}
}

@inproceedings{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  booktitle={Proceedings of the 7th International Conference on Learning Representations},  
  year={2019}
}

@inproceedings{suzuki2018adaptivity,
  title={Adaptivity of deep relu network for learning in besov and mixed smooth besov spaces: optimal rate and curse of dimensionality},
  author={Suzuki, Taiji},
  booktitle={Proceedings of the 7th International Conference on Learning Representations},    
  year={2019}
}

@inproceedings{ba2019generalization,
  title={Generalization of two-layer neural networks: An asymptotic viewpoint},
  author={Ba, Jimmy and Erdogdu, Murat and Suzuki, Taiji and Wu, Denny and Zhang, Tianzong},
  booktitle={Proceedings of the 8th International Conference on Learning Representations},    
  year={2020}
}

@inproceedings{pham2021global,
  title={Global Convergence of Three-layer Neural Networks in the Mean Field Regime},
  author={Huy Tuan Pham and Phan-Minh Nguyen},
  booktitle={Proceedings of the 9th International Conference on Learning Representations},    
  year={2021}
}

@inproceedings{oko2022psdca,
  title={Particle Stochastic Dual Coordinate Ascent: Exponential convergent algorithm for mean field neural network optimization},
  author={Oko, Kazusato and Suzuki, Taiji and Nitanda, Atsushi and Wu, Denny},
  booktitle={Proceedings of the 10th International Conference on Learning Representations},  
  year={2022}
}

@article{chizat2022mean,
  title={Mean-Field Langevin Dynamics: Exponential Convergence and Annealing},
  author={Chizat, L{\'e}na{\"\i}c},
  journal={Transactions on Machine Learning Research},
  year={2022}  
}

@article{zhang2018learning,
  title={Learning one-hidden-layer relu networks via gradient descent},
  author={Zhang, Xiao and Yu, Yaodong and Wang, Lingxiao and Gu, Quanquan},
  journal={arXiv preprint arXiv:1806.07808},
  year={2018}
}

@article{zhang2019fast,
  title={Fast Convergence of Natural Gradient Descent for Overparameterized Neural Networks},
  author={Zhang, Guodong and Martens, James and Grosse, Roger},
  journal={arXiv preprint arXiv:1905.10961},
  year={2019}
}


@article{ramachandran2017,
  title={Searching for activation functions},
  author={Ramachandran, Prajit and Barret Zoph and Quoc V. Le},
  journal={arXiv preprint arXiv:1710.05941},
  year={2017}
}

@inproceedings{nitanda2019stochastic,
  title={Stochastic Gradient Descent with Exponential Convergence Rates of Expected Classification Errors},
  author={Nitanda, Atsushi and Suzuki, Taiji},
  booktitle={Proceedings of International Conference on Artificial Intelligence and Statistics 22},
  pages={1417--1426},
  year={2019}
}

@inproceedings{nitanda2022convex,
  title={Convex analysis of the mean field langevin dynamics},
  author={Nitanda, Atsushi and Wu, Denny and Suzuki, Taiji},
  booktitle={Proceedings of International Conference on Artificial Intelligence and Statistics 25},  
  pages={9741--9757},
  year={2022}
}

@book{nm1994,
 author    ={Newey, Whitney K and McFadden, Daniel},
 title     ={Large Sample Estimation and Hypothesis Testing},
 journal   ={Handbook of Econometrics},
 year      ={1994},
 volume    ={4},
 pages     ={2111--2245} 
}

@article{cucker2002mathematical,
  title={On the mathematical foundations of learning},
  author={Cucker, Felipe and Smale, Steve},
  journal={Bulletin of the American mathematical society},
  volume={39},
  number={1},
  pages={1--49},
  year={2002}
}

@article{bach2017equivalence,
  title={On the equivalence between kernel quadrature rules and random feature expansions},
  author={Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={714--751},
  year={2017}
}

@article{chizat2018note,
  title={A note on lazy training in supervised differentiable programming},
  author={Chizat, Lenaic and Bach, Francis},
  journal={arXiv preprint arXiv:1812.07956},
  year={2018}
}

@article{weinan2019comparative,
  title={A comparative analysis of optimization and generalization properties of two-layer neural network and random feature models under gradient descent dynamics},
  author={Weinan, E and Ma, Chao and Wu, Lei},
  journal={Science China Mathematics},
  pages={1--24},
  year={2019}
}

@article{yashima2019exponential,
  title={Exponential Convergence Rates of Classification Errors on Learning with SGD and Random Features},
  author={Yashima, Shingo and Nitanda, Atsushi and Suzuki, Taiji},
  journal={arXiv preprint arXiv:1911.05350},
  year={2019}
}

@article{cao2019generalization_,
  title={A Generalization Theory of Gradient Descent for Learning Over-parameterized Deep ReLU Networks},
  author={Cao, Yuan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1902.01384},
  year={2019}
}

@article{zou2018stochastic,
  title={Stochastic gradient descent optimizes over-parameterized deep relu networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:1811.08888},
  year={2018}
}

@article{zou2020gradient,
  title={Gradient descent optimizes over-parameterized deep ReLU networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={Machine Learning},
  volume={109},
  number={3},
  pages={467--492},
  year={2020}
}

@article{ji2019polylogarithmic,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1909.12292},
  year={2019}
}

@article{cao2019towards,
  title={Towards Understanding the Spectral Bias of Deep Learning},
  author={Cao, Yuan and Fang, Zhiying and Wu, Yue and Zhou, Ding-Xuan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1912.01198},
  year={2019}
}

@article{rosasco2010learning,
  title={On learning with integral operators},
  author={Rosasco, Lorenzo and Belkin, Mikhail and Vito, Ernesto De},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Feb},
  pages={905--934},
  year={2010}
}

@article{smale2009geometry,
  title={Geometry on probability spaces},
  author={Smale, Steve and Zhou, Ding-Xuan},
  journal={Constructive Approximation},
  volume={30},
  number={3},
  pages={311},
  year={2009}
}

@inproceedings{rahamanspectral2019,
  title={On the Spectral Bias of Neural Networks},
  author={Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred A and Bengio, Yoshua and Courville, Aaron},
  booktitle={Proceedings of International Conference on Machine Learning 36},
  pages={5301--5310},
  year={2019}
}

@inproceedings{arora2019fine,
  title={Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={Proceedings of International Conference on Machine Learning 36},
  pages={322--332},
  year={2019}
}

@inproceedings{du2019gradient,
  title={Gradient Descent Finds Global Minima of Deep Neural Networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={Proceedings of International Conference on Machine Learning 36},
  pages={1675--1685},
  year={2019}
}

@inproceedings{steinwart2009optimal,
  title={Optimal Rates for Regularized Least Squares Regression.},
  author={Steinwart, Ingo and Hush, Don R and Scovel, Clint},
  booktitle={Proceedings of Conference on Learning Theory 22},
  pages={79--93},
  year={2009}
}

@article{bach2017breaking,
  title={Breaking the curse of dimensionality with convex neural networks},
  author={Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={19},
  pages={1--53},  
  year={2017}
}

@book{atkinson2012spherical,
  title={Spherical harmonics and approximations on the unit sphere: an introduction},
  author={Atkinson, Kendall and Han, Weimin},
  year={2012},
  publisher={Springer}
}

@article{blanchard2018optimal,
  title={Optimal rates for regularization of statistical inverse learning problems},
  author={Blanchard, Gilles and M{\"u}cke, Nicole},
  journal={Foundations of Computational Mathematics},
  volume={18},
  number={4},
  pages={971--1013},
  year={2018}
}

@article{nitanda2017stochastic,
  title={Stochastic Particle Gradient Descent for Infinite Ensembles},
  author={Nitanda, Atsushi and Suzuki, Taiji},
  journal={arXiv preprint arXiv:1712.05438},
  year={2017}
}

@article{mei2018mean,
  title={A mean field view of the landscape of two-layer neural networks},
  author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={33},
  pages={E7665--E7671},
  year={2018}
}

@article{durmus2019analysis,
  title={Analysis of Langevin Monte Carlo via Convex Optimization},
  author={Durmus, Alain and Majewski, Szymon and Miasojedow, B{\l}a{\.z}ej},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={73},
  pages={1--46},
  year={2019}
}

@inproceedings{wibisono2018sampling,
  title={Sampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem},
  author={Wibisono, Andre},
  booktitle={Proceedings of Conference on Learning Theory 31},
  pages={2093--3027},
  year={2018}
}

@article{holley1987logarithmic,
  title={Logarithmic Sobolev inequalities and stochastic Ising models},
  author={Holley, Richard and Stroock, Daniel},  
  journal={Journal of statistical physics},
  volume={46},
  number={5-6},
  pages={1159--1194},
  year={1987}
}

@article{bakry1985diffusions,
  title={Diffusions hypercontractives},
  author={Bakry, Dominique and {\'E}mery, Michel},
  journal={S{\'e}minaire de probabilit{\'e}s de Strasbourg},
  volume={19},
  pages={177--206},
  year={1985}
}

@article{otto2000generalization,
  title={Generalization of an inequality by Talagrand and links with the logarithmic Sobolev inequality},
  author={Otto, Felix and Villani, C{\'e}dric},
  journal={Journal of Functional Analysis},
  volume={173},
  number={2},
  pages={361--400},
  year={2000}
}

@article{erdogdu2020convergence,
  title={On the Convergence of Langevin Monte Carlo: The Interplay between Tail Growth and Smoothness},
  author={Erdogdu, Murat A and Hosseinzadeh, Rasa},
  journal={arXiv preprint arXiv:2005.13097},
  year={2020}
}

@article{nesterov2005smooth,
  title={Smooth minimization of non-smooth functions},
  author={Nesterov, Yu},
  journal={Mathematical programming},
  volume={103},
  number={1},
  pages={127--152},
  year={2005}
}

@article{javanmard2019analysis,
  title={Analysis of a two-layer neural network via displacement convexity},
  author={Javanmard, Adel and Mondelli, Marco and Montanari, Andrea},
  journal={arXiv preprint arXiv:1901.01375},
  year={2019}
}

@incollection{jordan199618,
  title={An extended variational principle},
  author={Jordan, Richard and Kinderlehrer, David},
  booktitle={Partial Differential Equations and Applications: Collected Papers in Honor of Carlo Pucci (Lecture Notes in Pure and Applied Mathematics)},
  volume={177},
  chapter={18},
  pages={187--200},
  year={1996},
  publisher={CRC Press}
}

@article{jordan1998variational,
  title={The variational formulation of the Fokker--Planck equation},
  author={Jordan, Richard and Kinderlehrer, David and Otto, Felix},
  journal={SIAM journal on mathematical analysis},
  volume={29},
  number={1},
  pages={1--17},
  year={1998}
}

@article{hu2019mean,
  title={Mean-field Langevin dynamics and energy landscape of neural networks},
  author={Hu, Kaitong and Ren, Zhenjie and Siska, David and Szpruch, Lukasz},
  journal={arXiv preprint arXiv:1905.07769},
  year={2019}
}

@article{jabir2019mean,
  title={Mean-field neural odes via relaxed optimal control},
  author={Jabir, Jean-Fran{\c{c}}ois and {\v{S}}i{\v{s}}ka, David and Szpruch, {\L}ukasz},
  journal={arXiv preprint arXiv:1912.05475},
  year={2019}
}

@article{menz2014poincare,
  title={Poincar{\'e} and logarithmic Sobolev inequalities by decomposition of the energy landscape},
  author={Menz, Georg and Schlichting, Andr{\'e}},
  journal={The Annals of Probability},
  volume={42},
  number={5},
  pages={1809--1884},
  year={2014},
  publisher={Institute of Mathematical Statistics}
}

@article{eberle2019quantitative,
  title={Quantitative Harris-type theorems for diffusions and McKean--Vlasov processes},
  author={Eberle, Andreas and Guillin, Arnaud and Zimmer, Raphael},
  journal={Transactions of the American Mathematical Society},
  volume={371},
  number={10},
  pages={7135--7173},
  year={2019}
}

@inproceedings{xie2017diverse,
  title={Diverse neural network learns true target functions},
  author={Xie, Bo and Liang, Yingyu and Song, Le},
  booktitle={Proceedings of International Conference on Artificial Intelligence and Statistics 20},
  pages={1216--1224},
  year={2017}
}

@article{dalalyan2017further,
  title={Further and stronger analogy between sampling and optimization: Langevin Monte Carlo and gradient descent},
  author={Dalalyan, Arnak S},
  journal={arXiv preprint arXiv:1704.04752},
  year={2017}
}

@inproceedings{rotskoff2019global,
  title={Global convergence of neuron birth-death dynamics},
  author={Rotskoff, Grant M and Jelassi, Samy and Bruna, Joan and Vanden-Eijnden, Eric},
  booktitle={Proceedings of International Conference on Machine Learning 36},
  pages={9689--9698},
  year={2019}
}

@article{ghorbani2020neural,
  title={When Do Neural Networks Outperform Kernel Methods?},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={arXiv preprint arXiv:2006.13409},
  year={2020}
}

@article{bai2019beyond,
  title={Beyond linearization: On quadratic and higher-order approximation of wide neural networks},
  author={Bai, Yu and Lee, Jason D},
  journal={arXiv preprint arXiv:1910.01619},
  year={2019}
}

@inproceedings{li2020learning,
  title={Learning Over-Parametrized Two-Layer Neural Networks beyond NTK},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang R},
  booktitle={Proceedings of Conference on Learning Theory 33},  
  pages={2613--2682},
  year={2020}
}

@article{ge2019mildly,
  title={Mildly overparametrized neural nets can memorize training data efficiently},
  author={Ge, Rong and Wang, Runzhe and Zhao, Haoyu},
  journal={arXiv preprint arXiv:1909.11837},
  year={2019}
}

@article{du2018power,
  title={On the power of over-parametrization in neural networks with quadratic activation},
  author={Du, Simon S and Lee, Jason D},
  journal={arXiv preprint arXiv:1803.01206},
  year={2018}
}

@article{carrillo2001entropy,
  title={Entropy dissipation methods for degenerate parabolicproblems and generalized sobolev inequalities},
  author={Carrillo, Jos{\'e} A and J{\"u}ngel, Ansgar and Markowich, Peter A and Toscani, Giuseppe and Unterreiter, Andreas},
  journal={Monatshefte f{\"u}r Mathematik},
  volume={133},
  number={1},
  pages={1--82},
  year={2001}
}

@article{raginsky2017non,
  title={Non-convex learning via stochastic gradient Langevin dynamics: a nonasymptotic analysis},
  author={Raginsky, Maxim and Rakhlin, Alexander and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1702.03849},
  year={2017}
}

@inproceedings{dai2016provable,
  title={Provable bayesian inference via particle mirror descent},
  author={Dai, Bo and He, Niao and Dai, Hanjun and Song, Le},
  booktitle={Proceedings of International Conference on Artificial Intelligence and Statistics 19},  
  pages={985--994},
  year={2016}
}

@article{chizat2021sparse,
  title={Sparse optimization on measures with over-parameterized gradient descent},
  author={Chizat, Lenaic},
  journal={Mathematical Programming},
  pages={1--46},
  year={2021}
}



@inproceedings{akiyama2021,
  title={On Learnability via Gradient Method for Two-Layer ReLU Neural Networks in Teacher-Student Setting},
  author={Akiyama, Shunta and Suzuki, Taiji},
  booktitle={Proceedings of International Conference on Machine Learning 38},
  pages={152--162},
  year={2021}
}

@article{roberts1996exponential,
  title={Exponential convergence of Langevin distributions and their discrete approximations},
  author={Roberts, Gareth O and Tweedie, Richard L},
  journal={Bernoulli},
  volume={2},
  number={4},
  pages={341--363},
  year={1996},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@article{mattingly2002ergodicity,
  title={Ergodicity for SDEs and approximations: locally Lipschitz vector fields and degenerate noise},
  author={Mattingly, Jonathan C and Stuart, Andrew M and Higham, Desmond J},
  journal={Stochastic processes and their applications},
  volume={101},
  number={2},
  pages={185--232},
  year={2002},
  publisher={Elsevier}
}

@article{cheng2017convergence,
  title={Convergence of Langevin MCMC in KL-divergence},
  author={Cheng, Xiang and Bartlett, Peter},
  journal={arXiv preprint arXiv:1705.09048},
  year={2017}
}

@article{dalalyan2014theoretical,
  title={Theoretical guarantees for approximate sampling from smooth and log-concave densities},
  author={Dalalyan, Arnak S},
  journal={arXiv preprint arXiv:1412.7392},
  year={2014}
}

@article{durmus2017nonasymptotic,
  title={Nonasymptotic convergence analysis for the unadjusted Langevin algorithm},
  author={Durmus, Alain and Moulines, Eric},
  journal={The Annals of Applied Probability},
  volume={27},
  number={3},
  pages={1551--1587},
  year={2017},
  publisher={Institute of Mathematical Statistics}
}

@article{lu2019scaling,
  title={Scaling limit of the Stein variational gradient descent: The mean field regime},
  author={Lu, Jianfeng and Lu, Yulong and Nolen, James},
  journal={SIAM Journal on Mathematical Analysis},
  volume={51},
  number={2},
  pages={648--671},
  year={2019},
  publisher={SIAM}
}

@article{ding2019ensemble,
  title={Ensemble Kalman sampling: Mean-field limit and convergence analysis},
  author={Ding, Zhiyan and Li, Qin},
  journal={arXiv preprint arXiv:1910.12923},
  year={2019}
}

@article{garbuno2020interacting,
  title={Interacting Langevin diffusions: Gradient structure and ensemble Kalman sampler},
  author={Garbuno-Inigo, Alfredo and Hoffmann, Franca and Li, Wuchen and Stuart, Andrew M},
  journal={SIAM Journal on Applied Dynamical Systems},
  volume={19},
  number={1},
  pages={412--441},
  year={2020},
  publisher={SIAM}
}

@article{allen2020backward,
  title={Backward feature correction: How deep learning performs deep learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2001.04413},
  year={2020}
}


@article{nitanda2019gradient,
  title={Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems},
  author={Nitanda, Atsushi and Chinot, Geoffrey and Suzuki, Taiji},
  journal={arXiv preprint arXiv:1905.09870},
  year={2019}
}

@article{lu2020mean,
  title={A mean-field analysis of deep resnet and beyond: Towards provable optimization via overparameterization from depth},
  author={Lu, Yiping and Ma, Chao and Lu, Yulong and Lu, Jianfeng and Ying, Lexing},
  journal={arXiv preprint arXiv:2003.05508},
  year={2020}
}

@article{nguyen2020rigorous,
  title={A rigorous framework for the mean field limit of multilayer neural networks},
  author={Nguyen, Phan-Minh and Pham, Huy Tuan},
  journal={arXiv preprint arXiv:2001.11443},
  year={2020}
}

@article{araujo2019mean,
  title={A mean-field limit for certain deep neural networks},
  author={Ara{\'u}jo, Dyego and Oliveira, Roberto I and Yukimura, Daniel},
  journal={arXiv preprint arXiv:1906.00193},
  year={2019}
}

@article{mei2019mean,
  title={Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={arXiv preprint arXiv:1902.06015},
  year={2019}
}

@article{ghorbani2019linearized,
  title={Linearized two-layers neural networks in high dimension},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={arXiv preprint arXiv:1904.12191},
  year={2019}
}

@article{chizat2020implicit,
  title={Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
  author={Chizat, Lenaic and Bach, Francis},
  journal={arXiv preprint arXiv:2002.04486},
  year={2020}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{chen2020generalized,
  title={A Generalized Neural Tangent Kernel Analysis for Two-layer Neural Networks},
  author={Chen, Zixiang and Cao, Yuan and Gu, Quanquan and Zhang, Tong},
  journal={arXiv preprint arXiv:2002.04026},
  year={2020}
}

@article{daniely2020learning,
  title={Learning Parities with Neural Networks},
  author={Daniely, Amit and Malach, Eran},
  journal={arXiv preprint arXiv:2002.07400},
  year={2020}
}

@article{yang2020feature,
  title={Feature Learning in Infinite-Width Neural Networks},
  author={Yang, Greg and Hu, Edward J},
  journal={arXiv preprint arXiv:2011.14522},
  year={2020}
}

@article{ma2019there,
  title={Is There an Analog of Nesterov Acceleration for MCMC?},
  author={Ma, Yi-An and Chatterji, Niladri and Cheng, Xiang and Flammarion, Nicolas and Bartlett, Peter and Jordan, Michael I},
  journal={arXiv preprint arXiv:1902.00996},
  year={2019}
}
  
@article{chen2020dynamical,
  title={A Dynamical Central Limit Theorem for Shallow Neural Networks},
  author={Chen, Zhengdao and Rotskoff, Grant M and Bruna, Joan and Vanden-Eijnden, Eric},
  journal={arXiv preprint arXiv:2008.09623},
  year={2020}
}

@article{bou2020convergence,
  title={Convergence of unadjusted Hamiltonian Monte Carlo for mean-field models},
  author={Bou-Rabee, Nawaf and Schuh, Katharina},
  journal={arXiv preprint arXiv:2009.08735},
  year={2020}
}

@article{bou2021mixing,
  title={Mixing Time Guarantees for Unadjusted Hamiltonian Monte Carlo},
  author={Bou-Rabee, Nawaf and Eberle, Andreas},
  journal={arXiv e-prints},
  pages={arXiv--2105},
  year={2021}
}

@article{guillin2020uniform,
  title={Uniform long-time and propagation of chaos estimates for mean field kinetic particles in non-convex landscapes},
  author={Guillin, Arnaud and Monmarch{\'e}, Pierre},
  journal={arXiv preprint arXiv:2003.00735},
  year={2020}
}

@article{guillin2021kinetic,
  title={The kinetic Fokker-Planck equation with mean field interaction},
  author={Guillin, Arnaud and Liu, Wei and Wu, Liming and Zhang, Chaoen},
  journal={Journal de Math{\'e}matiques Pures et Appliqu{\'e}es},
  volume={150},
  pages={1--23},
  year={2021},
  publisher={Elsevier}
}

@article{kraskov2004estimating,
  title={Estimating mutual information},
  author={Kraskov, Alexander and St{\"o}gbauer, Harald and Grassberger, Peter},
  journal={Physical review E},
  volume={69},
  number={6},
  pages={066138},
  year={2004},
  publisher={APS}
}

@inproceedings{cheng2018underdamped,
  title={Underdamped Langevin MCMC: A non-asymptotic analysis},
  author={Cheng, Xiang and Chatterji, Niladri S and Bartlett, Peter L and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={300--323},
  year={2018},
  organization={PMLR}
}

@inproceedings{dwivedi2018log,
  title={Log-concave sampling: Metropolis-Hastings algorithms are fast!},
  author={Dwivedi, Raaz and Chen, Yuansi and Wainwright, Martin J and Yu, Bin},
  booktitle={Conference on Learning Theory},
  pages={793--797},
  year={2018},
  organization={PMLR}
}

@article{dalalyan2020sampling,
  title={On sampling from a log-concave density using kinetic Langevin diffusions},
  author={Dalalyan, Arnak S and Riou-Durand, Lionel and others},
  journal={Bernoulli},
  volume={26},
  number={3},
  pages={1956--1988},
  year={2020},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@article{eberle2019couplings,
  title={Couplings and quantitative contraction rates for Langevin dynamics},
  author={Eberle, Andreas and Guillin, Arnaud and Zimmer, Raphael and others},
  journal={Annals of Probability},
  volume={47},
  number={4},
  pages={1982--2010},
  year={2019},
  publisher={Institute of Mathematical Statistics}
}

@article{erdogdu2021convergence,
  title={Convergence of Langevin Monte Carlo in Chi-Squared and R{\'e}nyi Divergence},
  author={Erdogdu, Murat A and Hosseinzadeh, Rasa and Zhang, Matthew S},
  year={2021}
}

@article{toscani2000trend,
  title={On the trend to equilibrium for some dissipative systems with slowly increasing a priori bounds},
  author={Toscani, Guiseppe and Villani, C{\'e}dric},
  journal={Journal of Statistical Physics},
  volume={98},
  number={5},
  pages={1279--1309},
  year={2000}
}

@article{villani2009hypocoercivity,
  title={Hypocoercivity},
  author={Villani, C{\'e}dric},
  year={2009},
  publisher={American Mathematical Society}
}

@article{guillin2019uniform,
  title={Uniform Poincar $\{$$\backslash$'e$\}$ and logarithmic Sobolev inequalities for mean field particles systems},
  author={Guillin, Arnaud and Liu, Wei and Wu, Liming and Zhang, Chaoen},
  journal={arXiv preprint arXiv:1909.07051},
  year={2019}
}

@book{bauschke2011convex,
  title={Convex analysis and monotone operator theory in Hilbert spaces},
  author={Bauschke, Heinz H and Combettes, Patrick L and others},
  volume={408},
  year={2011},
  publisher={Springer}
}

@book{rockafellar,
  address={Princeton},
  author={R. T. Rockafellar},
  publisher={Princeton University Press},
  title={Convex Analysis},
  year={1970},
}

@article{chizat2021convergence,
  title={Convergence Rates of Gradient Methods for Convex Optimization in the Space of Measures},
  author={Chizat, L{\'e}na{\"\i}c},
  journal={arXiv preprint arXiv:2105.08368},
  year={2021}
}

@article{kazeykina2020ergodicity,
  title={Ergodicity of the underdamped mean-field Langevin dynamics},
  author={Kazeykina, Anna and Ren, Zhenjie and Tan, Xiaolu and Yang, Junjian},
  journal={arXiv preprint arXiv:2007.14660},
  year={2020}
}



@book{milstein2013stochastic,
  title={Stochastic numerics for mathematical physics},
  author={Milstein, Grigori Noah and Tretyakov, Michael V},
  year={2013},
  publisher={Springer Science \& Business Media}
}


@article{monmarche2017long,
  title={Long-time behaviour and propagation of chaos for mean field kinetic particles},
  author={Monmarch{\'e}, Pierre},
  journal={Stochastic Processes and their Applications},
  volume={127},
  number={6},
  pages={1721--1737},
  year={2017},
  publisher={Elsevier}
}

@article{kent2021frank,
  title={Frank-Wolfe Methods in Probability Space},
  author={Kent, Carson and Blanchet, Jose and Glynn, Peter},
  journal={arXiv preprint arXiv:2105.05352},
  year={2021}
}

@inproceedings{charles2018stability,
  title={Stability and generalization of learning algorithms that converge to global optima},
  author={Charles, Zachary and Papailiopoulos, Dimitris},
  booktitle={Proceedings of International Conference on Machine Learning 35},
  pages={745--754},
  year={2018}
}


@article{kozachenko1987sample,
  title={Sample estimate of the entropy of a random vector},
  author={Kozachenko, LF and Leonenko, Nikolai N},
  journal={Problemy Peredachi Informatsii},
  volume={23},
  number={2},
  pages={9--16},
  year={1987},
  publisher={Russian Academy of Sciences, Branch of Informatics, Computer Equipment and~â€¦}
}

@article{ying2020mirror,
  title={Mirror descent algorithms for minimizing interacting free energy},
  author={Ying, Lexing},
  journal={Journal of Scientific Computing},
  volume={84},
  number={3},
  pages={1--14},
  year={2020}
}

@inproceedings{tian2022modern,
  title={Modern evolution strategies for creativity: Fitting concrete images and abstract concepts},
  author={Tian, Yingtao and Ha, David},
  booktitle={International Conference on Computational Intelligence in Music, Sound, Art and Design (Part of EvoStar)},
  pages={275--291},
  year={2022}
}

@article{chen2023entropic,
  title={Entropic fictitious play for mean field optimization problem},
  author={Chen, Fan and Ren, Zhenjie and Wang, Songbo},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={211},
  pages={1--36},
  year={2023}
}

@article{brown1951iterative,
  title={Iterative solution of games by fictitious play},
  author={Brown, George W},
  journal={Activity Analysis of Production and Allocation},
  volume={13},
  number={1},
  pages={374--376},
  year={1951}
}

@article{hadikhanloo2019finite,
  title={Finite mean field games: fictitious play and convergence to a first order continuous mean field game},
  author={Hadikhanloo, Saeed and Silva, Francisco J},
  journal={Journal de Math{\'e}matiques Pures et Appliqu{\'e}es},
  volume={132},
  pages={369--397},
  year={2019}
}

@article{cardaliaguet2017learning,
  title={Learning in mean field games: the fictitious play},
  author={Cardaliaguet, Pierre and Hadikhanloo, Saeed},
  journal={ESAIM: Control, Optimisation and Calculus of Variations},
  volume={23},
  number={2},
  pages={569--591},
  year={2017}
}

@article{perrin2020fictitious,
  title={Fictitious play for mean field games: Continuous time analysis and applications},
  author={Perrin, Sarah and P{\'e}rolat, Julien and Lauri{\`e}re, Mathieu and Geist, Matthieu and Elie, Romuald and Pietquin, Olivier},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13199--13213},
  year={2020}
}

@article{lavigne2022generalized,
  title={Generalized conditional gradient and learning in potential mean field games},
  author={Lavigne, Pierre and Pfeiffer, Laurent},
  journal={arXiv preprint arXiv:2209.12772},
  year={2022}
}

@article{chizat2022trajectory,
  title={Trajectory inference via mean-field Langevin in path space},
  author={Chizat, L{\'e}na{\"\i}c and Zhang, Stephen and Heitz, Matthieu and Schiebinger, Geoffrey},
  journal={arXiv preprint arXiv:2205.07146},
  year={2022}
}

@article{laine2020modular,
  title={Modular primitives for high-performance differentiable rendering},
  author={Laine, Samuli and Hellsten, Janne and Karras, Tero and Seol, Yeongho and Lehtinen, Jaakko and Aila, Timo},
  journal={ACM Transactions on Graphics (TOG)},
  volume={39},
  number={6},
  pages={1--14},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@inproceedings{suzuki2023uniformintime,
  title={Uniform-in-time propagation of chaos for the mean field gradient Langevin dynamics},
  author={Taiji Suzuki and Atsushi Nitanda and Denny Wu},
  booktitle={Proceedings of the 11th International Conference on Learning Representations},  
  year={2023}
}

@inproceedings{loshchilovdecoupled,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={Proceedings of the 7th International Conference on Learning Representations},
  year={2019}
}

@article{chen2022uniform,
  title={Uniform-in-Time Propagation of Chaos for Mean Field Langevin Dynamics},
  author={Chen, Fan and Ren, Zhenjie and Wang, Songbo},
  journal={arXiv preprint arXiv:2212.03050},
  year={2022}
}

@inproceedings{nishikawa2022,
  title={Two-layer neural network on infinite dimensional data: global optimization guarantee in the mean-field regime},
  author={Naoki Nishikawa and Taiji Suzuki and Atsushi Nitanda and Denny Wu},
  booktitle={Advances in Neural Information Processing Systems 35},
  year={2022}
}

@inproceedings{ba2022high,
  title={High-dimensional Asymptotics of Feature Learning: How One Gradient Step Improves the Representation},
  author={Ba, Jimmy and Erdogdu, Murat A and Suzuki, Taiji and Wang, Zhichao and Wu, Denny and Yang, Greg},
  booktitle={Advances in Neural Information Processing Systems 35},
  year={2022},
}

@inproceedings{abbe2022merged,
  title={The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks},
  author={Abbe, Emmanuel and Adsera, Enric Boix and Misiakiewicz, Theodor},
  booktitle={Conference on Learning Theory},
  pages={4782--4887},
  year={2022}
}

@inproceedings{chen2018stein,
  title={Stein points},
  author={Chen, Wilson Ye and Mackey, Lester and Gorham, Jackson and Briol, Fran{\c{c}}ois-Xavier and Oates, Chris},
  booktitle={International Conference on Machine Learning},
  pages={844--853},
  year={2018}
}

@article{huang2021distribution,
  title={Distribution dependent stochastic differential equations},
  author={Huang, Xing and Ren, Panpan and Wang, Feng-Yu},
  journal={Frontiers of Mathematics in China},
  volume={16},
  pages={257--301},
  year={2021}
}

@article{kook2024sampling,
  title={Sampling from the Mean-Field Stationary Distribution},
  author={Kook, Yunbum and Zhang, Matthew S and Chewi, Sinho and Erdogdu, Murat A and others},
  journal={arXiv preprint arXiv:2402.07355},
  year={2024}
}

@article{sznitman1991topics,
  title={Topics in propagation of chaos},
  author={Sznitman, Alain-Sol},
  journal={Ecole d'Et{\'e} de Probabilit{\'e}s de Saint-Flour XIXâ€”1989},
  pages={165--251},
  year={1991}
}

@article{mckean1966class,
  title={A class of Markov processes associated with nonlinear parabolic equations},
  author={McKean Jr, Henry P},
  journal={Proceedings of the National Academy of Sciences},
  volume={56},
  number={6},
  pages={1907--1911},
  year={1966}
}

@inproceedings{suzuki2023convergence,
  title={Convergence of mean-field Langevin dynamics: time-space discretization, stochastic gradient, and variance reduction},
  author={Suzuki, Taiji and Wu, Denny and Nitanda, Atsushi},
  booktitle={Advances in Neural Information Processing Systems 36},
  year={2023}
}

@inproceedings{suzuki2023featurelearning,
  title={Feature learning via mean-field Langevin dynamics: classifying sparse parities and beyond},
  author={Suzuki, Taiji and Wu, Denny and Oko, Kazusato and Nitanda, Atsushi},
  booktitle={Advances in Neural Information Processing Systems 37},
  year={2023}
}

@article{bardet2018,
author = {Jean-Baptiste Bardet and Natha{\"e}l Gozlan and Florent Malrieu and Pierre-Andr{\'e} Zitt},
title = {Functional inequalities for Gaussian convolutions of compactly supported measures: Explicit bounds and dimension dependence},
volume = {24},
journal = {Bernoulli},
number = {1},
pages = {333 -- 353},
year = {2018}
}

@article{cott2022,
author = {Patrick Cattiaux and Arnaud Guillin},
title = {Functional inequalities for perturbed measures with applications to log-concave measures and to some Bayesian problems},
volume = {28},
journal = {Bernoulli},
number = {4},
pages = {2294 -- 2321},
year = {2022}
}

@book{bakry2014analysis,
  title={Analysis and geometry of Markov diffusion operators},
  author={Bakry, Dominique and Gentil, Ivan and Ledoux, Michel},
  volume={348},
  year={2014}
}

@article{rotskoff2022trainability,
  title={Trainability and accuracy of artificial neural networks: An interacting particle system approach},
  author={Rotskoff, Grant and Vanden-Eijnden, Eric},
  journal={Communications on Pure and Applied Mathematics},
  volume={75},
  number={9},
  pages={1889--1935},
  year={2022}
}

@article{sirignano2020mean,
  title={Mean field analysis of neural networks: A law of large numbers},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={SIAM Journal on Applied Mathematics},
  volume={80},
  number={2},
  pages={725--752},
  year={2020}
}

@article{sirignano2020meanb,
  title={Mean field analysis of neural networks: A central limit theorem},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={Stochastic Processes and their Applications},
  volume={130},
  number={3},
  pages={1820--1852},
  year={2020}
}

@article{dembo1991information,
  title={Information theoretic inequalities},
  author={Dembo, Amir and Cover, Thomas M and Thomas, Joy A},
  journal={IEEE Transactions on Information theory},
  volume={37},
  number={6},
  pages={1501--1518},
  year={1991}
}

@article{bordelon2024infinite,
  title={Infinite Limits of Multi-head Transformer Dynamics},
  author={Bordelon, Blake and Chaudhry, Hamza Tahir and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2405.15712},
  year={2024}
}


@article{vyas2024feature,
  title={Feature-learning networks are consistent across widths at realistic scales},
  author={Vyas, Nikhil and Atanasov, Alexander and Bordelon, Blake and Morwani, Depen and Sainathan, Sabarish and Pehlevan, Cengiz},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{wortsman2022model,
  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  booktitle={International conference on machine learning},
  pages={23965--23998},
  year={2022}
}

@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle={International Conference on Machine Learning},
  pages={3259--3269},
  year={2020}
}

@inproceedings{neyshabur2020being,
  title={What is being transferred in transfer learning?},
  author={Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
  booktitle={Advances in Neural Information Processing Systems 33},
  pages={512--523},
  year={2020}
}

@inproceedings{mittal2018recovering,
  title={Recovering from random pruning: On the plasticity of deep convolutional neural networks},
  author={Mittal, Deepak and Bhardwaj, Shweta and Khapra, Mitesh M and Ravindran, Balaraman},
  booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={848--857},
  year={2018}
}

@inproceedings{li2022revisiting,
  title={Revisiting random channel pruning for neural network compression},
  author={Li, Yawei and Adamczewski, Kamil and Li, Wen and Gu, Shuhang and Timofte, Radu and Van Gool, Luc},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={191--201},
  year={2022}
}

@inproceedings{shim2021layer,
  title={Layer-wise pruning of transformer attention heads for efficient language modeling},
  author={Shim, Kyuhong and Choi, Iksoo and Sung, Wonyong and Choi, Jungwook},
  booktitle={2021 18th International SoC Design Conference (ISOCC)},
  pages={357--358},
  year={2021},
  organization={IEEE}
}

@article{cordonnier2020multi,
  title={Multi-head attention: Collaborate instead of concatenate},
  author={Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
  journal={arXiv preprint arXiv:2006.16362},
  year={2020}
}

@inproceedings{voita2019analyzing,
  title={Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={5797--5808},
  year={2019}
}

@inproceedings{nitanda2024improved,
  title={Improved particle approximation error for mean field neural networks},
  author={Nitanda, Atsushi},
  booktitle={Advances in Neural Information Processing Systems 37},
  year={2024}
}

@article{chewi2024uniform,
  title={Uniform-in-$ N $ log-Sobolev inequality for the mean-field Langevin dynamics with convex energy},
  author={Chewi, Sinho and Nitanda, Atsushi and Zhang, Matthew S},
  journal={arXiv preprint arXiv:2409.10440},
  year={2024}
}

@inproceedings{nitanda2024statistical,
  title={Improved statistical and computational complexity of the mean-field Langevin dynamics under structured data},
  author={Nitanda, Atsushi and Oko, Kazusato and Suzuki, Taiji and Wu, Denny},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{wang2024uniform,
  title={Uniform log-Sobolev inequalities for mean field particles with flat-convex energyy},
  author={Wang, Songbo},
  journal={arXiv preprint arXiv:2408.03283},
  year={2024}
}

@inproceedings{hu2023llm,
  title={LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models},
  author={Hu, Zhiqiang and Wang, Lei and Lan, Yihuai and Xu, Wanyu and Lim, Ee-Peng and Bing, Lidong and Xu, Xing and Poria, Soujanya and Lee, Roy},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={5254--5276},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{brigati2024heat,
  title={Heat flow, log-concavity, and Lipschitz transport maps},
  author={Brigati, Giovanni and Pedrotti, Francesco},
  journal={arXiv preprint arXiv:2404.15205},
  year={2024}
}

@article{mousavi2024learning,
  title={Learning multi-index models with neural networks via mean-field langevin dynamics},
  author={Mousavi-Hosseini, Alireza and Wu, Denny and Erdogdu, Murat A},
  journal={arXiv preprint arXiv:2408.07254},
  year={2024}
}

@misc{boucheron2013concentration,
  title={Concentration inequalities: a non asymptotic theory of independence},
  author={Boucheron, St{\'e}phane and Lugosi, Gabor and Massart, Pascal},
  year={2013}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{dietterich2000ensemble,
  title={Ensemble methods in machine learning},
  author={Dietterich, Thomas G},
  booktitle={International workshop on multiple classifier systems},
  pages = {1--15},
  year={2000}
}

@inproceedings{hansen1990neural,
  title={Neural network ensembles},
  author={Hansen, Lars Kai and Salamon, Peter},
  booktitle={Proceedings of the IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {993--1001},
  volume = {12},
  year={1990}
}

@article{breiman1996bagging,
  title={Bagging predictors},
  author={Breiman, Leo},
  journal={Machine Learning},
  volume={24},
  pages={123--140},
  year={1996},
  publisher={Kluwer}
}

@inproceedings{li2022revisiting,
  title={Revisiting random channel pruning for neural network compression},
  author={Li, Yawei and Adamczewski, Kamil and Li, Wen and Gu, Shuhang and Timofte, Radu and Van Gool, Luc},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={191--201},
  year={2022}
}

@inproceedings{utans1996weight,
  title={Weight averaging for neural networks and local resampling schemes},
  author={Utans, Joachim},
  booktitle={Association for the Advancement of Artifical Intelligence 13},
  pages = {133--138},
  year={1996}
}

@inproceedings{izmailov2018@averaging,
  title={Averaging weights lead to wider optima and better generalization},
  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  booktitle={Conference on Uncertainty in Artificial Intelligence 34},
  pages = {876--885},
  year={2018}
}

@inproceedings{chronopoulou2023adaptersoup,
  title={Adaptersoup: Weight averaging to improve generalization of pretrained language models},
  author={Chronopoulou, Alexandra and Peters, Matthew E and Fraser, Alexander and Dodge, Jesse},
  booktitle={Findings of the Association for Computational Linguistics 61},
  pages = {2054--2063},
  year={2023}
}

@inproceedings{rame2023model,
title={Model ratatouille: Recyling diverse models for out-of-distribution generalization},
  author={RamÃ©, Alexandre and Ahuja, Khartik and Zhang, Jianyu and Cord, Matthieu and Bottou,LÃ©on and Lopez-Paz, David},
  booktitle={Proceedings of International Conference on Machine Learning 40},
  pages={28656--28679},
  year={2023}
}

@inproceedings{yu2024language,
  title={Language models are super mario: Absorbing abilities from homologous models as a free lunch},
  author={Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin},
  booktitle={Proceedings of International Conference on Machine Learning 41},
  pages = {57755--57775},
  year={2024}
}

@article{gauthier2024merging,
  title={Merging in a bottle: Differentiable adaptive merging (DAM) and the path from averaging to automation},
  author={Gauthier-Caron, Thomas and Siriwardhana, Shamane and Stein, Elliot and Ehghaghi, Malikeh and Goddard, Charles and McQuade, Mark and Solawetz, Jacob and Labonne, Maxime},
  journal={arXiv preprint arXiv:2410.08371},
  year={2024}
}

@article{charles2024acree,
  title={Arceeâ€™s MergeKit: A toolkit for merging large language models},
  author={Goddard, Charles and Siriwardhana, Shamane and Ehghaghi, Malikeh and Meyers, Luke and Karpukhin, Vlad and Benedict, Brian and McQuade, Marc and Solawetz, Jacob},
  journal={arXiv preprint arXiv:2403.13257 },
  year={2024}
}

@article{yang2024model,
  title={Model merging in LLMs, MLLMs, and beyond: Methods, theories, applications and opportunities},
  author={Yang, Enneng and Shen, Li and Guo, Guibing and Wang, Xingwei and Cao, Xiaochun and Zhang, Jie and Tao, Dacheng},
  journal={arXiv preprint arXiv:2408.07666 },
  year={2024}
}

@inproceedings{davari2024model,
  title={Model breadcrumbs: Scaling multi-task model merging with sparse masks},
  author={Davari, Mohammad Reza and Belilovsky},
  booktitle={European Conference on Computer Vision 18},
  year={2024}
}

@inproceedings{jin2023dataless,
  title={Dataless knowledge fusion by merging weights of language models},
  author={Jin, Xisen and Ren, Xiang and Preotiuc-Pietro, Daniel and Cheng, Pengxiang},
  booktitle={Proceedings of the 11th International Conference on Learning Representations},
  year={2023}
}

@inproceedings{ilharco2022patching,
  title={Patching open-vocabulary models by interpolating weights},
  author={Ilharco, Gabriel and Wortsman, Mitchell and Gadre, Samir Yitzhak and Song, Shuran and Kornblith, Simon and Farhadi, Ali and Schmidt, Ludwig},
  booktitle={Advances in Neural Information Processing Systems 35},
  year={2022}
}


@inproceedings{ortiz2023task,
  title={Task arithmetic in the tangent space: Improved editing of pre-trained models},
  author={Ortiz-Jimenez, Guillermo and Favero, Alessandro and Frossard, Pascal},
  booktitle={Advances in Neural Information Processing Systems 36},
  year={2023}
}

@article{jain2018parallelizing,
  title={Parallelizing stochastic gradient descent for least squaers regression: mini-batching, averaging and model misspecification},
  author={Jain, Prateek and Kakade, Sham and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
  journal={Journal of Machine Learning Research}, 
  volume = {223},
  pages = {1--42},
  year={2018}
}

@inproceedings{wang2024generalization,
  title={Generalization analysis of stochastic weight averaging with general sampling},
  author={Wang, Peng and Shen, Li and Tao, Zerui and He, Shuaida and Tao, Dacheng},
  booktitle={Proceedings of the International Conference on Machine Learning 41},
  pages={51442--51464},
  year={2024}
}

@article{romero2020automatic,
  title={Automatic detection of depression in speech using ensemble convolutional neural networks},
  author={Vazquez-Romero, Adrian and Gallardo-Antolin, Ascension},
  journal = {Entropy},
  volume = {22},
  number = {6},
  year={2020}
}

@article{cheng2018the,
  title={The relative performance of ensemble methods with deep convolutional neural networks for image classification},
  author={Cheng, Ju and Bibaut, Aurelian and van der Laan, Mark},
  journal = {Journal of Applied Statistics},
  volume = {45},
  number = {15},
  year={2018},
  pages={2800--2818}
}

@article{mohammed2023comprehensive,
  title={A comprehensive review on ensemble deep learning: Opportunities and challenges},
  author={Mohammed, Ammar and Mohammed, Rania Kora},
  journal = {Journal of King Saud University-Computer and Information Sciences},
  volume = {35},
  number = {2},
  year={2023},
  pages={754--774}
}

@article{ganaie2022ensemble,
  title={Ensemble deep learning: A review},
  author={Ganaie, M.A. and Hu, Minghui and Malik, A.K. and Tanveer, M. and Suganthan, P.N.},
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {115},
  year={2022},
  pages={105--151}
}

@article{kim2003constructing,
  title={Constructing support vector machine ensemble},
  author={Kim, Hyun-Chul and Pang, Shaoning and Je, Hong-Mo and Kim, Daijin and Yang, Sung and Kim, Bang},
  journal = {Pattern Recognition},
  volume = {36},
  year={2003},
  pages={2757â€“-2767}
}

@article{soares2004meta,
  title={A meta-learning method to select the kernel width in support vector regression},
  author={Soares, Carlos and Brazdil, Pavel and Kuba, Peter},
  journal = {Machine Learning},
  volume = {54},
  year={2004},
  pages={195--209}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
