\section{Related work}
%For additional information and extended discussions of related works, refer to Appendix \ref{sec:related_extra}.

%paragraph{Mean field optimization} \label{intro:mfo}\citet{chen2022uniform, suzuki2023convergence} proved the uniform-in-time PoC for MFLD, suggesting that the finite particle discretization error shrinks at a constant pace as $N \rightarrow \infty$. \citet{nitanda2024improved} refined the existing approximation error bound \citep{chen2022uniform, suzuki2023convergence} by leveraging on the problem structure in risk minimization, removing the dependence on the LSI constant of the proximal Gibbs measures. A key assumption in this work is that the optimality condition \citep{mei2018mean, hu2019mean, chizat2022mean} is satisfied. 

%Subseqeuntly, \citet{wang2024generalization, chewi2024uniform, kook2024sampling} proved the uniform-in-$N$ LSI constant. In particular, \citet{chewi2024uniform} reduced the scaling of the LSI constant to a single exponent that is dependent on both the strong regularization and entropy regularization terms. This uniform-in-$N$ LSI constant is then combined with the LSI-constant-free particle approximation error \citep{nitanda2024improved} to  derive tighter generalization bounds \citep{chewi2024uniform}.

%\paragraph{Ensembling and model merging}   Ensemble methods improve predictive performance by combining the outputs of multiple models during inference \citep{hansen1990neural, dietterich2000ensemble}. The average voting method which takes the mean of predictions \citep{breiman1996bagging} has been applied to deep learning problems \citep{cheng2018the, romero2020automatic}. In contrast, model merging consolidates multiple models into a single one by combining individual parameters. \citet{wortsman2022model} refers to averaging the weights of independently fine-tuned models as \textit{model soups}, exhibiting considerable success particularly in the optimization of LLMs \citep{ilharco2022patching, jin2023dataless, davari2024model}. While other novel strategies for merging LLMs have been proposed \citep{rame2023model, chronopoulou2023adaptersoup, yu2024language}, \citet{gauthier2024merging} showed that basic weight averaging methods can perform competitively if constituent models are similar.