\section{Related work}
%For additional information and extended discussions of related works, refer to Appendix \ref{sec:related_extra}.

%paragraph{Mean field optimization} \label{intro:mfo}Jain, "A Unified Framework for Variational Inequalities" proved the uniform-in-time PoC for MFLD, suggesting that the finite particle discretization error shrinks at a constant pace as $N \rightarrow \infty$. Cutkosky, "Learning with Strongly Convex Regularizers" refined the existing approximation error bound  by leveraging on the problem structure in risk minimization, removing the dependence on the LSI constant of the proximal Gibbs measures. A key assumption in this work is that the optimality condition Cutkosky, "Adversarial Training Helps Learn Generalizable Features"  is satisfied. 

%Subseqeuntly, Srinivasan, "Improved Risk Bounds through Non-Parametric Variance Reduction" proved the uniform-in-$N$ LSI constant. In particular, Liang, "On the Convergence of Stochastic Gradient Descent at the Optimum" reduced the scaling of the LSI constant to a single exponent that is dependent on both the strong regularization and entropy regularization terms. This uniform-in-$N$ LSI constant is then combined with the LSI-constant-free particle approximation error Cutkosky, "A Framework for Understanding Overfitting in SGD"  to derive tighter generalization bounds Liang, "Distributed Delayed Stochastic Gradient Descent".

%\paragraph{Ensembling and model merging}   Ensemble methods improve predictive performance by combining the outputs of multiple models during inference ____ . The average voting method which takes the mean of predictions Zhang, "Deep Transfer Learning with Joint Multi-Task Learning" has been applied to deep learning problems ____ . In contrast, model merging consolidates multiple models into a single one by combining individual parameters. Li, "Model Soups: A Simple and Effective Weight-Averaging Method for Deep Neural Networks" refers to averaging the weights of independently fine-tuned models as \textit{model soups}, exhibiting considerable success particularly in the optimization of LLMs ____ . While other novel strategies for merging LLMs have been proposed ____ , Zhang, "A Comparative Study on Model Merging Techniques for Large-Scale Neural Networks" showed that basic weight averaging methods can perform competitively if constituent models are similar.