
\section{Additional Background Information} \label{sec:related_extra}
This section provides supplementary information about past works that are relevant to the paper. While not essential to the primary narrative, it will provide readers with a deeper understanding of previously established MFNN concepts and motivations behind our PoC-based model ensemble strategy.

\subsection{Mean field optimization}
Two layer mean-field neural networks provide a tractable analytical framework for studying infinitely wide neural networks. As $N \rightarrow \infty$, the optimization dynamics is captured by a partial differential equation (PDE) of the parameter distribution, where convexity can be exploited to show convergence to the global optimal solution \citep{chizat2018global, mei2018mean, rotskoff2019global}. If Gaussian noise is added to the gradient, we get MFLD which achieves global convergence to the optimal solution \citep{mei2018mean, hu2019mean}. Under the uniform LSI, \citet{nitanda2022convex, chizat2022mean} show that MFLD converges at an exponential rate by using the proximal Gibbs distribution associated with the dynamics. MFLD has attracted significant attention because of its feature learning capabilities \citep{ suzuki2023featurelearning,mousavi2024learning}.

As the assumption that $N = \infty$ is not applicable to real-world scenarios, a discrete-time finite particle system would align closer to an implementable MFLD i.e. noisy gradient descent. \citet{nitanda2022convex} provides a global convergence rate analysis for the discrete-time update by extending the one-step interpolation argument for Langevin dynamics \citep{vempala2019rapid}. Meanwhile, the approximation error of the finite particle setting is studied in propagation of chaos literature \citep{sznitman1991topics}. For finite MFLD setting, \citet{mei2018mean} first suggested that approximation error grows exponentially with time before \citet{chen2022uniform, suzuki2023convergence} proved the uniform-in-time propagation of chaos with error bound: $O \left( \frac{\alpha}{\lambda N}\right)$, suggesting that the difference between the finite $N$-particle system and mean-field limit shrinks as $N \rightarrow \infty$. However, this also means that particle approximation error blows-up exponentially as $\lambda \rightarrow 0$ due to the exponential relationship between $\alpha$ and $\lambda$ \citep{nitanda2022convex,chizat2022mean,suzuki2023convergence}. \citet{suzuki2023featurelearning} proposes an annealing procedure for classification tasks to remove this exponential dependence in LSI, wihch requires that $\lambda$ be gradually reduced over time and will not work for fixed regularization parameters. \citet{nitanda2024improved, chewi2024uniform} then proved a refined propagation of chaos independent of $\alpha$ at the solution as described in Section \ref{subsec:contributions}.

\subsection{Ensembling and model merging}


%\paragraph{Averaging model weights} For instance, \cite{frankle2020linear,neyshabur2020being,wortsman2022model}. \AN{From Wortsman et al. (2022)} {\it Frankle et al. (2020) find that, when training a pair of models from scratch on harder datasets such as ImageNet with the same hyperparameter configuration and initialization but different data order, interpolating weights achieves no better than random accuracy. However, Frankle et al. (2020) showed that when the two models share a portion of their optimization trajectory, accuracy does not drop when they are averaged. Analogously, Neyshabur et al. (2020) demonstrate that when two models are finetuned with the same pre-trained initialization, the interpolated model attains at least the accuracy of the endpoints. Unlike Nagarajan and Kolter (2019); Frankle et al. (2020); Neyshabur et al. (2020) we consider averaging many models with varied hyperparameter configurations.}
In recent years, efforts to improve predictive capabilities and computational efficiency in machine learning have revived interest in techniques such as ensembling \citep{ganaie2022ensemble, mohammed2023comprehensive} and model merging \citep{yang2024model, charles2024acree}. Ensemble methods improve predictive performance by combining the outputs of multiple models during inference \citep{hansen1990neural, dietterich2000ensemble}. Although several fusion variants exist \citep{kim2003constructing, soares2004meta}, \citet{cheng2018the} shows that simple average voting \citep{breiman1996bagging} does not perform significantly worse while still being highly efficient, with uses in several deep learning applications \citep{cheng2018the, romero2020automatic}. 

In contrast, model merging consolidates multiple models into a single one by combining individual parameters, showing success particularly in the optimization of LLMs \citep{ilharco2022patching, jin2023dataless, davari2024model}. An approach to merging models is to simply average the weights across multiple models \citep{utans1996weight}. Taking the average of weights along a single optimization trajectory has been demonstrated to achieve better generalization \citep{izmailov2018@averaging, frankle2020linear}. Moreover, interpolating any two random weights from models that lie in the same loss basins could produce even more optimal solutions that are closer to the centre of the basin \citep{neyshabur2020being}. These works then form the foundation of \textit{model soups} in \citet{wortsman2022model} which refers to averaging the weights of independently fine-tuned models. Similarly, \citet{gauthier2024merging} showed that basic weight averaging methods can perform competitively if constituent models are similar, despite the emergence of novel LLM merging strategies \citep{rame2023model, chronopoulou2023adaptersoup, yu2024language}.

Despite the widespread use of model merging in the current research landscape, theoretical results concerning the merging of fully trained neural networks are limited. For models trained with stochastic gradient descent, averaging the model weights during different iterations of a single run improves stability bounds \citep{hardt2016train} and variance \citep{jain2018parallelizing} under convex assumptions. Stability bounds in the non-convex settings are then addressed by \citet{wang2024generalization}.  \citet{ortiz2023task} studied weight interpolation techniques for task arithmetic in vision-language models, demonstrating that linearized models under the neural tangent kernel regime can outperform non-linear counterparts.




%\label{intro:mfo}\citet{chen2022uniform, suzuki2023convergence} proved the uniform-in-time PoC for MFLD, suggesting that the finite particle discretization error shrinks at a constant pace as $N \rightarrow \infty$. \citet{nitanda2024improved} refined the existing approximation error bound \citep{chen2022uniform, suzuki2023convergence} by leveraging on the problem structure in risk minimization, removing the dependence on the LSI constant of the proximal Gibbs measures. A key assumption in this work is that the optimality condition \citep{mei2018mean, hu2019mean, chizat2022mean} is satisfied.

%Subseqeuntly, \citet{wang2024generalization, chewi2024uniform, kook2024sampling} proved the uniform-in-$N$ LSI constant. In particular, \citet{chewi2024uniform} reduced the scaling of the LSI constant to a single exponent that is dependent on both the strong regularization and entropy regularization terms. This uniform-in-$N$ LSI constant is then combined with the LSI-constant-free particle approximation error \citep{nitanda2024improved} to  derive tighter generalization bounds \citep{chewi2024uniform}.
