\section{Introduction}\label{sec:introduction}
A two layer \textit{mean-field neural network} (MFNN) with $N$ neurons is defined as an empirical average of $N$ functions: $\bE_{X  \sim \rho_\vx} \left[h\left( X, \cdot\right) \right] = \frac{1}{N} \sum^N_{i=1} h(x^i, \cdot)$, where each $h(x^i,\cdot)$ represents a single neuron with parameter $x^i$ and $\rho_\vx = \frac{1}{N} \sum^N_{i=1} \delta_{x_i}$ is an empirical distribution. As the number of neurons get infinitely large $(N \rightarrow \infty)$, the \textit{mean-field limit} is attained: $\rho_\vx \rightarrow \mu$, leading to MFNN having an infinite number of particles: $\bE_{X\sim \mu} \left[ h(X, \cdot)\right]$. Since a distribution $\mu$ parameterizes the model in this mean-field limit, training can now be formulated as the optimization over the space of probability distributions \citep{nitanda2017stochastic}. Gradient descent for MFNNs exhibits global convergence \citep{chizat2018global, mei2018mean} and adaptivity \citep{yang2020feature, ba2022high}. To improve stability during training, one may consider \textit{noisy} gradient training by adding Gaussian noise, giving rise to \textit{mean-field Langevin dynamics} (MFLD) \citep{mei2018mean, hu2019mean}. MFLD, with $N=\infty$, also achieves global convergence to the optimal solution \citep{hu2019mean, jabir2019mean}, with an exponential convergence rate under the \textit{uniform log-Sobolev inequality} (LSI) \cite{nitanda2022convex, chizat2022mean} in the continuous-time setting. 

However, the mean-field limit attained at $N = \infty$ cannot be accurately replicated in real-life scenarios. When employing a finite-particle system $\rho_\vx$, the approximation error that arises has been studied in the literature on \textit{propagation of chaos} (PoC) \cite{sznitman1991topics}. In the context of MFLD, \citet{chen2022uniform, suzuki2023convergence} proved the \textit{uniform-in-time} PoC for the trajectory of MFLD. In particular, in the long-time limit, they established the bounds $\pow[\cL,N](\pow[\mu,N]_*)-\cL(\mu_*) = O\left(\frac{\lambda}{\alpha N} \right)$, where $\alpha \gtrsim \exp \left(-\Theta \left( \frac{1}{\lambda}\right)\right)$ is the LSI constant on \textit{proximal Gibbs distributions}, $\lambda$ is the regularization coefficient, and $\pow[\cL,N](\pow[\mu,N]_*)$ and $\cL(\mu_*)$ are the optimal values in finite- and infinite-particle systems. Subsequently, \citet{nitanda2024improved} improved upon this result by removing $\alpha$ from the above bound, resulting in $O \left( \frac{1}{N}\right)$. This refinement of the bound is significant as previously, the LSI constant could become exponentially small as $\lambda \rightarrow 0$. While \citet{nitanda2024improved} also established PoC for the MFLD trajectory by incorporating the \textit{uniform-in-N} LSI \cite{chewi2024uniform}: $\pow[\cL,N](\pow[\mu,N]_t) \to \pow[\cL,N](\pow[\mu,N]_*)$, this approach is indirect for showing convergence to the mean-field limit $\cL(\mu_*)$ and results in a slower convergence rate over time. 

In this work, we further aim to improve PoC for MFLD by demonstrating a faster convergence rate in time, while maintaining the final approximation error $O\left(\frac{1}{N}\right)$ attained at $t=\infty$. We then utilize our result to propose a PoC-based ensemble technique by demonstrating how finite particle systems can converge towards the mean-field limit when merging MFNNs trained in parallel.

\subsection{Contributions} \label{subsec:contributions}
The PoC for MFLD \cite{chen2022uniform,suzuki2023convergence} consists of particle approximation error $O\left(\frac{\lambda}{\alpha N}\right)$ due to finite-$N$-particles and optimization error $\exp(-\Theta(\lambda \alpha t))$. This result basically builds upon the defective LSI: $\exists \delta>0$,
\[ \frac{1}{N}\pow[\cL,N](\pow[\mu,N]) - \cL(\mu_*) \leq \frac{\delta}{N} + \frac{\lambda}{2\alpha N}\FI(\pow[\mu,N]\|\pow[\mu,N]_*) \]
implicitly established by \citet{chen2022uniform} under the uniform LSI condition \cite{nitanda2022convex,chizat2022mean}, where $\FI$ is Fisher information. The dependence on LSI-constant $\alpha$ in $O\left(\frac{\lambda}{\alpha N}\right)$ of PoC is basically inherited from $\delta$. In our work, we first remove the dependence on $\alpha$ from $\delta$ by introducing {\it uniform directional LSI} (Assumption \ref{assumption:uniform_directional_lsi}) in training MFNNs setting. Based on this defective LSI, we then derive an improved PoC for MFLD where the particle approximation error is $O\left(\frac{1}{N}\right)$. Similar to \citet{nitanda2024improved}, this improvement exponentially reduces the required number of particles since the constant $\alpha\gtrsim \exp\left(-\Theta(\frac{1}{\lambda})\right)$ can exponentially decrease as $\lambda \to \infty$. Moreover, our result demonstrates a faster optimization speed compared to \citet{nitanda2024improved,chewi2024uniform} due to a different exponent $\alpha$ in the optimization error terms: $\exp(-\Theta(\lambda \alpha t))$. In our analysis, $\alpha$ is a constant of the uniform directional LSI, which is larger than the LSI constant on $\pow[\mu,N]_*$ appearing in the optimization error in \citet{nitanda2024improved,chewi2024uniform} (see the discussion following Theorem \ref{theorem:mfld_convergence}).

Next, we translate the PoC result regarding objective gap into the point-wise and uniform model approximation errors: $|\bE_{X\sim\rho_\vx}[h(X,z)] - \bE_{X\sim\mu_*}[h(X,z)]|$ and $\| \bE_{X\sim\rho_\vx}[h(X,\cdot)] - \bE_{X\sim\mu_*}[h(X,\cdot)]\|_{\infty}$ useful for obtaining generalization error bound on classification task \cite{suzuki2023featurelearning,nitanda2024statistical}. Again, the bound consists of the sum of particle approximation and optimization error terms.
Compared to the previous results \cite{suzuki2023convergence,suzuki2023featurelearning}, our bound is tighter since the particle approximation term is independent of the LSI-constant. This improvement directly eliminates the requirement for an exponential number of neurons with respect to dimension $d$ in their learning setup (e.g., $k$-parity problems \cite{suzuki2023featurelearning}).
We also propose a PoC-based model ensemble method to further reduce the model approximation error and empirically verify its performance on synthetic datasets. To our knowledge, our study is the first to provide a theoretical guarantee for model ensembling of MFNNs using PoC results. Going beyond the scope of the theory, we examine the applicability of our method to merging LoRA parameters for language models.
\begin{itemize}
    \item We demonstrate an improved PoC for MFLD (Theorem \ref{theorem:mfld_convergence}) under uniform directional LSI condition (Assumption \ref{assumption:uniform_directional_lsi}). This improvement removes the dependence on LSI constant $\alpha \gtrsim \exp\left(-\Theta\left(\frac{1}{\lambda}\right)\right)$ from the particle approximation error in \citet{chen2022uniform,suzuki2023convergence} and accelerates the optimization speed in \citet{nitanda2024improved,chewi2024uniform}.
    \item We translate the PoC result regarding objective gap into point-wise and uniform model approximation errors (Theorems \ref{theorem:point_approximation_mfld}, \ref{theorem:point_approximation_multiple_mfld}, and \ref{theorem:uniform_approximation_multiple_mfld}). These results also remove the dependence on the LSI constant from the particle approximation terms in the previous model approximation errors \cite{suzuki2023convergence,suzuki2023featurelearning}.
    \item We propose an ensembling method for MFNNs trained in parallel to reduce approximation error, providing theoretical guarantees (Theorem \ref{theorem:point_approximation_multiple_mfld}, \ref{theorem:uniform_approximation_multiple_mfld}) and empirical verification on synthetic datasets. Moreover, going beyond the theoretical framework, we apply our method to merge multiple LoRA parameters of language models and observe improved prediction performance.
\end{itemize}

%\subsection{Related work}
%For additional information and extended discussions of related works, refer to Appendix \ref{sec:related_extra}.

%paragraph{Mean field optimization} \label{intro:mfo}\citet{chen2022uniform, suzuki2023convergence} proved the uniform-in-time PoC for MFLD, suggesting that the finite particle discretization error shrinks at a constant pace as $N \rightarrow \infty$. \citet{nitanda2024improved} refined the existing approximation error bound \citep{chen2022uniform, suzuki2023convergence} by leveraging on the problem structure in risk minimization, removing the dependence on the LSI constant of the proximal Gibbs measures. A key assumption in this work is that the optimality condition \citep{mei2018mean, hu2019mean, chizat2022mean} is satisfied. 

%Subseqeuntly, \citet{wang2024generalization, chewi2024uniform, kook2024sampling} proved the uniform-in-$N$ LSI constant. In particular, \citet{chewi2024uniform} reduced the scaling of the LSI constant to a single exponent that is dependent on both the strong regularization and entropy regularization terms. This uniform-in-$N$ LSI constant is then combined with the LSI-constant-free particle approximation error \citep{nitanda2024improved} to  derive tighter generalization bounds \citep{chewi2024uniform}.

%\paragraph{Ensembling and model merging}   Ensemble methods improve predictive performance by combining the outputs of multiple models during inference \citep{hansen1990neural, dietterich2000ensemble}. The average voting method which takes the mean of predictions \citep{breiman1996bagging} has been applied to deep learning problems \citep{cheng2018the, romero2020automatic}. In contrast, model merging consolidates multiple models into a single one by combining individual parameters. \citet{wortsman2022model} refers to averaging the weights of independently fine-tuned models as \textit{model soups}, exhibiting considerable success particularly in the optimization of LLMs \citep{ilharco2022patching, jin2023dataless, davari2024model}. While other novel strategies for merging LLMs have been proposed \citep{rame2023model, chronopoulou2023adaptersoup, yu2024language}, \citet{gauthier2024merging} showed that basic weight averaging methods can perform competitively if constituent models are similar.

\subsection{Notations}
We use lowercase letters such as $x$ for vectors and uppercase letters such as $X$ for random variables $\bR^d$, respectively. The boldface is used for tuples of them like $\vx = (x^1,\ldots,x^N) \in \bR^{Nd}$ and $\vX=(X^1,\ldots,X^N)$. Given $\vx=(x^i)_{i=1}^N$, $\vx^{-i}$ denotes $(x^1,\ldots,x^{i-1},x^{i+1},\ldots,x^N)$. 
%$\vX^{-i}$ is also defined in the same way.
$\|\cdot\|_2$ denotes the Euclidean norm. $\cP_2(\bR^d)$ denotes the set of probability distributions with finite second moment on $\bR^d$.
For probability distributions $\mu, \nu \in \cP_2(\bR^d)$, we define 
Kullback-Leibler (KL) divergence (a.k.a. relative entropy) by $\KL(\mu\|\nu) = \int \rd\mu(x) \log \frac{\rd \mu}{\rd \nu}(x)$ and define Fisher information by $\FI(\mu\|\nu) = \int \rd\mu(x) \|\nabla \log \frac{\rd \mu}{ \rd \nu}(x)\|_2^2$. 
$\Ent$ denotes the negative entropy: $\Ent(\mu) = \int  \mu(\rd x)\log \frac{\rd \mu}{\rd x}(x)$. 
%We denote by $\frac{\rd \mu}{\rd x}$ the density function of $\mu$ with respect to the Lebesgue measure if it exists. 
We denote $\pd< f, m>= \int f(x) m(\rd x)$ for a (signed) measure $m$ and integrable function $f$ on $\bR^d$. 
Given $\vx=(x^1,\ldots,x^N) \in \bR^{Nd}$, we write an empirical distribution supported on $\vx$ as $\rho_\vx = \frac{1}{N}\sum_{i=1}^N \delta_{x^i}$.

