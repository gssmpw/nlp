\section{Proofs}\label{sec:proofs}
\subsection{Propagation of chaos for MFLD (Section \ref{sec:main_results})}\label{subsec:poc_proof}
\begin{proof}[Proof of Lemma \ref{lemma:clsi}]
    The first equality of the assertion was proved by \citet{nitanda2024improved}. We here prove the inequality by utilizing the argument of conditional and marginal distribution of $\pow[\mu,N]$ \cite{chen2022uniform}.
    
    For $\vX \sim \pow[\mu,N]$, we denote by $\pow[\mu,N]_{i|-i}(\cdot | \vx^{-i})$ and $\pow[\mu,N]_{-i}$ the conditional distribution of $X^i$ conditioned by $\vX^{-i}=\vx^{-i}$ and the marginal distribution of $\vX^{-i}$, respectively.
    It holds that 
    \begin{align} 
        &\bE_{\vx \sim \pow[\mu,N]}\left[ \left\| \nabla \log \frac{\rd \pow[\mu,N]}{\rd \pow[\mu,N]_*}(\vX) \right\|_2^2\right] \notag\\
        &= \sum_{i=1}^N \bE_{\vx \sim \pow[\mu,N]}\left[ \left\| \nabla_{x^i} \log \frac{\rd \pow[\mu,N]}{\rd \vx}(\vX) + \frac{N}{\lambda}\nabla_{x^i}F(\mu_\vx)\right\|_2^2\right] \notag\\
        &= \sum_{i=1}^N \bE_{\vX^{-i} \sim \pow[\mu,N]_{-i}} \left[ \bE_{X^i \sim \pow[\mu,N]_{i|-i}(\cdot | \vX^{-i})}\left[ \left\| \nabla_{x^i} \log \frac{\rd \pow[\mu,N]}{\rd \vx}(\vX) + \frac{N}{\lambda}\nabla_{x^i}F(\mu_\vx)\right\|_2^2 \right]\right]. \label{eq:fisher_div}
    \end{align}

    We write $p_{-i}(\vx^{-i})=\frac{\rd \pow[\mu,N]_{-i}}{\rd \vx^{-i}}(\vx^{-i})$ and $p_{i|-i}(x|\vx^{-i})=\frac{\rd \pow[\mu,N]_{i|-i}(\cdot|\vx^{-i})}{\rd x}(x)$.
    Since $\frac{\rd \pow[\mu,N]}{\rd \vx}(\vx) = p_{-i}(\vx^{-i}) p_{i|-i}(x^i|\vx^{-i})$, we get the following equation:
    \begin{equation*}
        \nabla_{x^i} \log \frac{\rd \pow[\mu,N]}{\rd \vx}(\vx)
        = \frac{\nabla_{x^i}(p_{-i}(\vx^{-i}) p_{i|-i}(x^{i}|\vx^{-i}))}{p_{-i}(\vx^{-i}) p_{i|-i}(x^{i}|\vx^{-i})}
        = \frac{\nabla_{x^i}p_{i|-i}(x^i|\vx^{-i})}{p_{i|-i}(x^i|\vx^{-i})}
        = \nabla \log p_{i|-i}(x^i|\vx^{-i}).
    \end{equation*}
    Hence, Eq.~\eqref{eq:fisher_div} can be further bounded by the LSI on the conditional Gibbs distribution (Assumption \ref{assumption:uniform_directional_lsi}) as follows: 
    \begin{align}
        &\sum_{i=1}^N \bE_{\vX^{-i} \sim \pow[\mu,N]_{-i}} \left[ \bE_{X^i \sim \pow[\mu,N]_{i|-i}(\cdot |\vX^{-i})}\left[ \left\| \nabla \log p_{i|-i}(X^i|\vX^{-i}) + \frac{N}{\lambda}\nabla_{x^i}F(\mu_\vx)\right\|_2^2 \right]\right] \notag \\
        &=\sum_{i=1}^N \bE_{\vX^{-i} \sim \pow[\mu,N]_{-i}} \left[ \bE_{X^i \sim \pow[\mu,N]_{i|-i}(\cdot|\vX^{-i})}\left[ \left\| \nabla \log \frac{\rd \pow[\mu,N]_{i|-i}}{\rd \nu_{i|-i}}(X^i|\vX^{-i}) \right\|_2^2 \right]\right] \notag \\
        &\geq 2\alpha  \sum_{i=1}^N \bE_{\vX^{-i} \sim \pow[\mu,N]_{-i}} \left[ \KL( \pow[\mu,N]_{i|-i}(\cdot|\vX^{-i}) \| \nu_{i|-i}(\cdot|\vX^{-i})) \right].  \label{eq:fisher_div_eval}
    \end{align}

    Let $\nu$ be the probability distribution on $\bR^d$ with the density $\frac{\rd \nu}{\rd x}(x) \propto \exp(-r(x)/\lambda)$. Here, notice that the conditional Gibbs distribution $\nu_{i|-i}(\cdot|\vx^{-i})$ is the minimizer of the following objective: for $\vx^{-i}$
    \begin{align*} 
        \nu_{i|-i}(\cdot|\vx^{-i}) 
        &= \argmin_{\mu \in \cP_2(\bR^d)}\left\{
        \int NF_0(\rho_{x\cup\vx^{-i}})\mu(\rd x) 
        + \bE_{X\sim \mu}[r(X)] + \lambda \Ent(\mu)
        \right\} \\
        &= \argmin_{\mu \in \cP_2(\bR^d)}\left\{
        \int NF_0(\rho_{x\cup\vx^{-i}})\mu(\rd x) 
        + \lambda\KL(\mu\|\nu)
        \right\}.
    \end{align*}

    Because of the optimality of the conditional Gibbs distribution, we have 
    \begin{align}
        &\lambda\KL( \pow[\mu,N]_{i|-i}(\cdot|\vX^{-i}) \| \nu_{i|-i}(\cdot|\vX^{-i})) \notag\\
        &=\lambda \Ent(\pow[\mu,N]_{i|-i}(\cdot|\vX^{-i}))
        + \int NF(\rho_{x \cup \vX^{-i}}) \pow[\mu,N]_{i|-i}(\rd x|\vX^{-i})
        + \lambda \log \int \exp\left(-\frac{N}{\lambda}F(\rho_{x\cup \vX^{-i}})\right) \rd x \notag \\
        &=\lambda \KL\left(\pow[\mu,N]_{i|-i}(\cdot|\vX^{-i} )\| \nu \right)
        + \int NF_0(\rho_{x \cup \vX^{-i}}) \left(\pow[\mu,N]_{i|-i}(\rd x|\vX^{-i})
        - \nu_{i|-i}(\rd x|\vX^{-i})(\rd x)\right)  - \lambda \KL\left(\nu_{i|-i}(\cdot|\vX^{-i}) \| \nu\right) \notag \\
        &\geq \lambda \KL\left(\pow[\mu,N]_{i|-i}(\cdot|\vX^{-i} )\| \nu \right)
        + \int NF_0(\rho_{x \cup \vX^{-i}}) \left(\pow[\mu,N]_{i|-i}(\rd x|\vX^{-i})
        - \mu_*(\rd x)\right)  - \lambda \KL\left(\mu_* \| \nu\right). \label{eq:fisher_div_eval2}
    \end{align}

    The expectation of the second term of the right-hand side can be futher evaluated as 
    \begin{align}
        &N \sum_{i=1}^N \bE_{\vX \sim \pow[\mu,N]} \left[ \int F_0(\rho_{x \cup \vX^{-i}}) \left(\pow[\mu,N]_{i|-i}(\rd x|\vX^{-i})- \mu_*(\rd x)\right) \right] \notag \\
        &= N \sum_{i=1}^N \bE_{\vX \sim \pow[\mu,N]} \left[ F_0(\rho_\vX) - \int F_0(\rho_{x \cup \vX^{-i}})\mu_*(\rd x) \right] \notag \\
        &= -N \sum_{i=1}^N \bE_{\vX \sim \pow[\mu,N]} \left[\int \left\{ B_{F_0}(\rho_{x \cup \vX^{-i}},\rho_\vX) + \pd< \frac{\delta F_0(\rho_\vX)}{\delta \mu}, \rho_{x \cup \vX^{-i}}-\rho_\vX> \right\} \mu_*(\rd x)\right] \notag \\
        &= -N \sum_{i=1}^N \bE_{\vX \sim \pow[\mu,N]} \left[\int B_{F_0}(\rho_{x \cup \vX^{-i}},\rho_\vX) \mu_*(\rd x)\right] \notag \\
        &~~~~ + \sum_{i=1}^N \bE_{\vX \sim \pow[\mu,N]} \left[\int \left\{ \frac{\delta F_0(\rho_\vX)}{\delta \mu}(X^i) - \frac{\delta F_0(\rho_\vX)}{\delta \mu}(x) \right\} \mu_*(\rd x)\right] \notag \\
        &= -N \sum_{i=1}^N \bE_{\vX \sim \pow[\mu,N]} \left[\int B_{F_0}(\rho_{x \cup \vX^{-i}},\rho_\vX) \mu_*(\rd x)\right] \notag \\
        &~~~~ + N \bE_{\vX \sim \pow[\mu,N]} \left[\int \frac{\delta F_0(\rho_\vX)}{\delta \mu}(x) \left(\rho_{\vX}(\rd x) - \mu_*(\rd x)\right) \right] \notag \\
        &\geq - B 
        + N \left(\bE_{\vX \sim \pow[\mu,N]} \left[ F_0(\rho_\vX)\right] - F_0(\mu_*) \right), \label{eq:fisher_div_eval3}
    \end{align}
    where the last inequality is due to the convexity of $F_0$ and Assumption \ref{assumption:nonlinearity}.

    By the information inequality (Lemma 5.1 of \citet{chen2022uniform}), the first term of Eq.~\eqref{eq:fisher_div_eval2} of the right-hand side can be evaluated as 
    \begin{align}
        \sum_{i=1}^N \bE_{\vX \sim \pow[\mu,N]}\left[ \KL\left(\pow[\mu,N]_{i|-i}(\cdot|\vX^{-i} )\| \nu \right) \right] 
        \geq \KL(\pow[\mu,N] \| \tensor[\nu,N]).
    \end{align}
    
    Combining all of them, we get
    \begin{align*}
        &\bE_{\vx \sim \pow[\mu,N]}\left[ \left\| \nabla \log \frac{\rd \pow[\mu,N]}{\rd \pow[\mu,N]_*}(\vX) \right\|_2^2\right] \\
        &\geq \frac{2\alpha}{\lambda} \left\{ - B + N \bE_{\vX \sim \pow[\mu,N]} \left[ F_0(\rho_\vX) \right] + \lambda \KL(\pow[\mu,N] \| \tensor[\nu,N]) 
        - N(F_0(\mu_*) + \lambda \KL\left(\mu_* \| \nu\right)) \right\} \\
        &= \frac{2\alpha}{\lambda} \left\{ - B + N \bE_{\vX \sim \pow[\mu,N]} \left[ F(\rho_\vX) \right] + \lambda \Ent(\pow[\mu,N]) - N(F(\mu_*) +\lambda\Ent(\mu_*)) \right\} \\
        &= \frac{2\alpha}{\lambda} \left( - B + \pow[\cL,N](\pow[\mu,N]) - N \cL(\mu_*) \right).
    \end{align*}
    This concludes the proof.
\end{proof}

\begin{proof}[Proof of Theorem \ref{theorem:mfld_convergence}]
    We here prove the convergence of MFLD in the discrete-time by using the one-step interpolation argument \cite{nitanda2024improved,suzuki2023convergence}. 
    
    We construct the one-step interpolation for $k$-th iteration: $X_{k+1}^i = X_k^i - \eta \nabla \frac{\delta F(\rho_{\vX_k})}{\delta \mu}(X_k^i) + \sqrt{2\lambda \eta} \xi_k^i,~(i\in \{1,2,\ldots,d\})$.
    as follows: for $i\in \{1,2,\ldots,d\}$,
    \begin{equation}\label{eq:noisy-GD_dynamics}
        \rd Y_t^i = - \nabla \frac{\delta F(\rho_{\vY_0})}{\delta \mu}(Y_0^i)\rd t + \sqrt{2\lambda}\rd W_t,
    \end{equation}
    where $\vY_0 = (Y_0^1,\ldots,Y_0^d) = (X_k^1,\ldots,X_k^d)$ and $W_t$ is the standard Brownian motion in $\bR^d$ with $W_0 = 0$.
    We denote by $\nu_t$ the distributions of $\vY_t$. 
    Then, $\nu_0 = \pow[\mu,N]_k (= \mathrm{Law}(\vX_k))$, $\nu_\eta = \pow[\mu,N]_{k+1} (=\mathrm{Law}(\vX_{k+1}))$ (i.e., $\vY_{\eta} \disteq \vX_{k+1}$).
    In this proof, we identify the probability distribution with its density function with respect to the Lebesgure measure for notational simplicity. For instance, we denote by $\pow[\mu,N]_*(\vy)$ the density of $\pow[\mu,N]_*$. 

    By the proof of Theorem 2 in \citet{nitanda2024improved}, we see for $t \in [0,\eta]$,
    \begin{align}
        \frac{\rd \pow[\cL,N]}{\rd t}(\nu_t)
        &\leq - \frac{\lambda^2}{2} \int \nu_t(\vy) \left\| \nabla \log \frac{\nu_t}{\pow[\mu,N]_*}(\vy) \right\|_2^2 \rd \vy 
        + N \delta_\eta, \label{eq:one_step_decrease}
    \end{align}
    where $\delta_\eta = 8\eta( C_2^2 + \lambda^{\prime 2}) (\eta C_1^2 + \lambda d) 
    + 32 \eta^2 \lambda'^2( C_2^2 + \lambda^{\prime 2}) \left( \frac{1}{N}\bE\left[ \left\| \vX_0 \right\|_2^2 \right] + \frac{1}{\lambda'}\left(\frac{C_1^2}{4\lambda'} + \lambda d\right) \right)$.

    Combining Lemma \ref{lemma:clsi} with the above inequality, we get
    \begin{align*}
        &\frac{\rd \pow[\cL,N]}{\rd t}(\nu_t)
        \leq  - \alpha \lambda \left( \pow[\cL,N](\nu_t) - N \cL(\mu_*) - B \right)
        + N \delta_\eta. \\
        \Longleftrightarrow 
        ~~~~&\frac{\rd}{\rd t}\left( \pow[\cL,N](\nu_t) - N \cL(\mu_*) - B - \frac{N\delta_\eta}{\alpha \lambda} \right)
        \leq  - \alpha \lambda \left( \pow[\cL,N](\nu_t) - N \cL(\mu_*) - B - \frac{N\delta_\eta}{\alpha \lambda} \right).
    \end{align*}    

    Noting $\nu_\eta = \pow[\mu,N]_{k+1}$ and $\nu_0 = \pow[\mu,N]_k$, the Gr\"{o}nwallâ€™s inequality leads to 
    \[ 
        \pow[\cL,N](\pow[\mu,N]_{k+1}) -  N\cL(\mu_*) - B - \frac{ N\pow[\delta,N]_{\eta}}{\alpha \lambda} 
        \leq \exp( -\alpha\lambda\eta )\left(  \pow[\cL,N](\pow[\mu,N]_k) -  N\cL(\mu_*) - B- \frac{N\pow[\delta,N]_{\eta}}{\alpha \lambda} \right). 
    \]
    
This inequality holds at every iteration of (\ref{eq:noisy-GD_dynamics}). Hence, we arrive at the desired result,  
\begin{align*}
    \frac{1}{N}\pow[\cL,N](\pow[\mu,N]_k) - \cL(\mu_*)  
    &\leq \frac{B}{N} + \frac{ \pow[\delta,N]_{\eta}}{\alpha \lambda} 
    + \exp( -\alpha\lambda\eta k )\left( \frac{1}{N}\pow[\cL,N](\pow[\mu,N]_0) - \cL(\mu_*) - \frac{B}{N} - \frac{ \pow[\delta,N]_{\eta}}{\alpha \lambda} \right) \\
    &\leq \frac{B}{N} + \frac{ \pow[\delta,N]_{\eta}}{\alpha \lambda} 
    + \exp( -\alpha\lambda\eta k )\left( \frac{1}{N}\pow[\cL,N](\pow[\mu,N]_0) - \cL(\mu_*) \right).
\end{align*}
\end{proof}

\subsection{Point-wise model approximation error (Section \ref{subsec:pw_model_error})}\label{subsec:pw_model_error_proof}
\begin{proof}[Proof of Lemma \ref{lemma:han_inequality}]
    It follows that by Han's inequality \citep{dembo1991information},
    \[ \frac{1}{s \binom{N}{s}} \sum_{|S|=s} \int \pow[\mu,N]_S(\rd \vx_S) \log \frac{\rd \pow[\mu,N]_S}{\rd \vx_S}(\vx_S) \leq \frac{1}{N}\int \pow[\mu,N](\rd x) \log \frac{\rd \pow[\mu,N]}{\rd \vx}(\vx). \]
    Moreover, we see
    \begin{align*} 
        \sum_{|S|=s} \int \pow[\mu,N]_S(\rd \vx_S) \log \frac{\rd \tensor[\mu_*,k]}{\rd \vx_S}(\vx_S)
        &= \sum_{|S|=s} \sum_{i \in S}\int \pow[\mu,N]_i(\rd x^i) \log \frac{\rd \mu_*}{\rd x}(x^i)  \\
        &= \binom{N-1}{s-1}\sum_{i=1}^N\int \pow[\mu,N]_i(\rd x^i) \log \frac{\rd \mu_*}{\rd x}(x^i) \\
        &= \binom{N-1}{s-1} \int \pow[\mu,N](\rd \vx) \log \frac{\rd \tensor[\mu_*,N]}{\rd \vx}(\vx). 
    \end{align*}
    Noticing $\binom{N-1}{s-1} = \frac{s}{N}\binom{N}{s}$, we conclude the first statement which immediately implies the second statement.
\end{proof}


\begin{proposition}\label{prop:pw_poc_merge}
    Suppose $\pow[\mu,N]$ is exchangeable and $\vX_{j} \sim \tensor[\mu,N]$ $(j=1,2,\ldots,M)$. Then, it follows that for any $z \in \cZ$,
    \begin{align*} 
        \bE_{\{\vX_j\}_{j=1}^M}\left[ \left( \frac{1}{M}\sum_{j=1}^M\bE_{\rho_{\vX_j}}[ h(X,z)] - \bE_{\mu_*}[h(X,z)]\right)^2 \right] 
        \leq \frac{4R^2}{N M} + \frac{8R^2}{M}\sqrt{ \frac{\KL(\pow[\mu,N]\|\tensor[\mu_*,N])}{N}} 
    + \frac{2R^2 \KL(\pow[\mu,N] \| \tensor[\mu,N]_*)}{N}.
    \end{align*}
\end{proposition}
\begin{proof}[Proof of Proposition \ref{prop:pw_poc_merge}]
    \begin{align*}
        \bE_{\{\vX_j\}_{j=1}^M}&\left[ \left( \frac{1}{M}\sum_{j=1}^M\bE_{X \sim \rho_{\vX_j}}[ h(X,z)] - \bE_{X\sim\mu_*}[h(X,z)]\right)^2 \right] \\
        &= \frac{1}{M^2}\bE_{\{\vX_j\}_{j=1}^M}\left[ \sum_{j=1}^M\left(\bE_{X \sim \rho_{\vX_j}}[ h(X,z)] - \bE_{X\sim\mu_*}[h(X,z)]\right)^2 \right] \\
        &+ \frac{1}{M^2}\bE_{\{\vX_j\}_{j=1}^M}\left[ \sum_{j\neq k}\left( \bE_{X \sim \rho_{\vX_j}}[ h(X,z)] - \bE_{X\sim\mu_*}[h(X,z)]\right)\left( \bE_{X \sim \rho_{\vX_k}}[ h(X,z)] - \bE_{X\sim\mu_*}[h(X,z)]\right) \right].
    \end{align*}
    Using Proposition \ref{prop:pw_model_approximation}, we can upper bound the first term by $\frac{4R^2}{Ms'} + \frac{8R^2}{M}\sqrt{ \frac{\KL(\pow[\mu,N]\|\tensor[\mu_*,N])}{N}}$. 
    The second term can be evaluated as follows. Set $H(\vX_j) = \bE_{X \sim \mu_{\vX_j}}[ h(X,z)] - \bE_{X\sim\mu_*}[h(X,z)]$. Then for $j\neq k$,
    \begin{align*}
        \bE_{\{\vX_j\}_{j=1}^M}\left[ H(\vX_j)H(\vX_k) \right]
        &= \left( \bE_{\vX_j}\left[ H(\vX_j) \right] \right)^2 \\
        &= \left( \bE_{\vX_j}\left[  \frac{1}{s'} \sum_{i=1}^{s'} h(X_j^i,z) \right] - \bE_{X\sim\mu_*}[h(X,z)] \right)^2 \\
        &= \left( \bE_{X \sim \pow[\mu,N]_1}\left[  h(X,z) \right] - \bE_{X\sim\mu_*}[h(X,z)] \right)^2 \\
        &\leq 4R^2 \TV^2(\pow[\mu,N]_1,\mu_*) \\
        &\leq 2R^2 \KL(\pow[\mu,N]_1 \| \mu_*) \\
        &\leq \frac{2R^2}{N}\KL(\pow[\mu,N] \| \tensor[\mu,N]_*).
    \end{align*}
    This concludes the proof.
\end{proof}

%%%%%
\subsection{Uniform model approximation error (Section \ref{subsec:uniform_model_error})}\label{subsec:uniform_model_error_proof}
We evaluate the empirical Rademacher complexity $\hat{\cR}_{N,M}(\cF)$ by using Dudley's entropy integral. We define the metric $\|f\|_{N,M,2} = \sqrt{\frac{1}{MN}\sum_{j=1}^M\sum_{i=1}^N |f(X^i_j)|^2}$.
We denote by $\cN(\cF,\epsilon,\|\cdot\|_{N,M,2})$ the $\epsilon$-covering number of $\cF$ with respect to the $\|\cdot\|_{N,M,2}$-norm.
\begin{lemma}[Dudley's entropy integral]\label{lemma:dudley}
    Given a function class $\cF$ on $\bR^d$, we suppose $R = \sup_{f\in\cF}\|f\|_{N,M,2} < \infty$. Then,
    \begin{equation*}
        \hat{\cR}_{N,M}(\cF) 
        \leq \inf_{\delta > 0} 
        \left\{
            4\delta 
            + \frac{12}{\sqrt{MN}} \int_{\delta}^{R} \sqrt{\log 2 \cN(\cF,\epsilon,\|\cdot\|_{N,M,2})} \rd\epsilon
        \right\}.
    \end{equation*}
\end{lemma}

\begin{proposition}\label{proposition:rademacher_complexity_bound}
    Suppose Assumption \ref{assumption:model_constraint} holds and $\vX_j \sim \pow[\mu,N]$ $(j=1,2,\ldots,M)$ are independent. Then, we get
    \[ \bE_{\{\vX_j\}_{j=1}^M} \left[\hat{\cR}_{N,M}(\cF) \right] 
    \leq 4 R\sqrt{\frac{d}{MN}}
        + 12R\sqrt{\frac{1}{MN} \left(\log 2 + d\log \left( 1 + 2\beta MR^{-1}\sqrt{MNd^{-1}}\bE_{\vX\sim\pow[\mu,N]}[\|\vX\|_2]\right)\right)}. \]
\end{proposition}
\begin{proof}
    Since $\| f \|_{N,M,2} \leq \| f \|_{N,M,\infty} = \max_{i,j}|f(X^i_j)|$, it is sufficient evaluate the $\epsilon$-covering number of $\cF$ with respect to $\| \cdot \|_{N,M,\infty}$. 
    We write $r=\max_{i,j}\|X^i_j\|_2$. By Assumption \ref{assumption:model_constraint}, for any $z, z' \in \cZ$,
    \[ \max_{i,j}| h(X^i_j,z) - h(X^i_j,z')| 
    \leq \max_{i,j} \beta \|X^i_j\|_2  \| z - z' \|_2 
    =  \beta r \| z - z' \|_2, \]
    we see $\cN(\cF, \epsilon,\|\cdot\|_{N,M,\infty}) \leq \cN\left(\cZ, \epsilon/(\beta r), \|\cdot\|_{2}\right) = \left( 1 + \frac{2 \beta r}{\epsilon}\right)^{d}$.

    Therefore, by Lemma \ref{lemma:dudley} with $\delta = R\sqrt{d(MN)^{-1}}$, we get
    \begin{align*}
        \hat{\cR}_{N,M}(\cF) 
        &\leq 4R \sqrt{\frac{d}{MN}}
        + 12R\sqrt{\frac{1}{MN}\log 2 \cN(\cF,R\sqrt{d(MN)^{-1}},\|\cdot\|_{N,M,\infty})} \\
        &=4R \sqrt{\frac{d}{MN}}
        + 12R\sqrt{\frac{1}{MN} \left(\log 2 + d\log \left( 1 + 2 \beta r R^{-1}\sqrt{MNd^{-1}}\right)\right)} \\
        &\leq 4R \sqrt{\frac{d}{MN}}
        + 12R\sqrt{\frac{1}{MN} \left(\log 2 + d\log \left( 1 + 2 \beta R^{-1}\sqrt{MNd^{-1}} \sum_{j=1}^M \|\vX_j\|_2\right)\right)},
    \end{align*}
    where we used $r \leq \sum_{j=1}^M \|\vX_j\|_2$.
    Finally, Jensen's inequality yields
    \[ \bE_{\{\vX_j\}_{j=1}^M} \left[\hat{\cR}_{N,M}(\cF) \right] 
    \leq 4R \sqrt{\frac{d}{MN}}
        + 12R\sqrt{\frac{1}{MN} \left(\log 2 + d\log \left( 1 + 2\beta MR^{-1}\sqrt{MNd^{-1}}\bE_{\vX\sim\pow[\mu,N]}[\|\vX\|_2] \right)\right)}.\]
\end{proof}

Here, we give the complete version of the uniform model approximation bound.
\begin{theorem}[Complete version of Theorem \ref{theorem:uniform_approximation_multiple_mfld}]\label{theorem:uniform_approximation_multiple_mfld_complete}
    Suppose Assumption \ref{assumption:model_constraint} and the same conditions as in Theorem \ref{theorem:mfld_convergence} hold. Run $M$-parallel MFLD in the discrete time independently, with $\eta \lambda' < 1/2$ and $\vX_{j,0} \sim \tensor[\mu,N]_0 (j=1,2,\ldots,M)$. Then,
    \begin{align*}
        \bE_{\{\vX_{j,k}\}}&\left[\left\| \frac{1}{M}\sum_{j=1}^M \bE_{X\sim \rho_{\vx_{j,k}}}[h(X,\cdot)] - \bE_{X\sim \mu_*} [h(X,\cdot)]\right\|_\infty\right] \\
        &\leq \frac{5CR}{4}\sqrt{\frac{d}{MN} + \frac{dB}{\lambda N}}
        + CR\sqrt{\frac{d\lambda}{MN(\lambda + MB)}}\log\left( C'\sqrt{\left(\lambda+MB\right)\frac{\pi}{\lambda}} \right) \\
        &+ CR\sqrt{\frac{d\lambda M N}{\lambda + MB}} \left(\frac{ \delta_{\eta}}{\alpha\lambda^2} 
        + \frac{1}{\lambda}\exp( -\alpha\lambda\eta k )\pow[\Delta,N]_0 \right),\\
        %&=\tilde{O}\left( R\sqrt{\frac{d}{MN} + \frac{dB}{\lambda N}
        %+ \frac{d\lambda}{MN(\lambda + MB)}} \right) \\
        %&+ O\left( R\sqrt{\frac{d\lambda M N}{\lambda + MB}} \left(\frac{ \delta_{\eta}}{\alpha\lambda^2} 
        %+ \frac{1}{\lambda}\exp( -\alpha\lambda\eta k )\pow[\Delta,N]_0 \right) \right),
    \end{align*}
    where $C'=1 + 2\beta MR^{-1}\sqrt{MNd^{-1}}\bE_{\vX\sim\tensor[\mu,N]_*}[\|\vX\|_2]$.
\end{theorem}
\begin{proof}
    For $\vx_1,\ldots,\vx_M \in \bR^{dN}$, we set 
    $g(\vx_1,\ldots,\vx_M) = \sup_{z \in \cZ} 
    \left| \frac{1}{M}\sum_{j=1}^M \bE_{X\sim \rho_{\vx_j}}[h(X,z)] - \bE_{X\sim \mu_*}[h(X,z)]\right|$.
    By the variational formulation of KL-divergence (e.g., Corollary 4.15 in \citet{boucheron2013concentration}), we get
    \begin{align}\label{eq:variational_kl}
        \bE_{\mu^{(N)\otimes M}}[g]
        &\leq \frac{1}{\gamma} \log \bE_{\mu^{\otimes N M}_*}[\exp(\gamma g)] + \frac{\KL(\mu^{(N)\otimes M}\|\mu^{\otimes N M}_*)}{\gamma} \notag \\
        &\leq \frac{1}{\gamma} \log \bE_{\mu^{\otimes N M}_*}[\exp(\gamma g)] + \frac{M\KL(\mu^{(N)}\|\mu^{\otimes N}_*)}{\gamma} 
    \end{align}

    For independent random variables $\vX_j \sim \tensor[\mu,N]_* (j=1,2,\ldots,M)$, by Lemma \ref{lemma:uniform_lln} and \ref{proposition:rademacher_complexity_bound}, it follows that with high probability $1-\delta$,
    \begin{align*}
        &g(\vX_1,\ldots,\vX_M) \\
        &\leq 2\bE_{\{\vX_j\}_{j=1}^M}\left[\hat{\cR}_{N,M}(\cF)\right] 
        + R \sqrt{ \frac{2\log(1/\delta)}{MN}} \\
        &\leq 8 R\sqrt{\frac{d}{MN}}
        + 24R\sqrt{\frac{1}{MN} \left(\log 2 + d\log \left( 1 + 2\beta MR^{-1}\sqrt{MNd^{-1}}\bE_{\vX\sim\tensor[\mu,N]_*}[\|\vX\|_2]\right)\right)}
        + R \sqrt{ \frac{2\log(1/\delta)}{MN}}\\
        &\leq CR\sqrt{\frac{d\log(C'/\delta)}{MN}},
    \end{align*}
    where $C$ is a uniform constant and $C'=1 + 2\beta MR^{-1}\sqrt{MNd^{-1}}\bE_{\vX\sim\tensor[\mu,N]_*}[\|\vX\|_2]$.
    This means
    \begin{align*}
        &\bP_{\mu^{\otimes N M}_*}\left[ g(\vX_1,\ldots,\vX_M) > CR\sqrt{\frac{d\log(C'/\delta)}{MN}} \right] \leq \delta \\
        \iff
        &\bP_{\mu^{\otimes N M}_*}\left[ g(\vX_1,\ldots,\vX_M) > t \right] \leq C'\exp\left( - \frac{MNt^2}{dC^2R^2}\right) \\
        \iff
        &\bP_{\mu^{\otimes N M}_*}\left[ g(\vX_1,\ldots,\vX_M) > \frac{1}{\gamma}\log t \right] \leq C'\exp\left( - \frac{MN(\log t)^2}{dC^2R^2\gamma^2}\right).
    \end{align*}
    Using this tail bound, 
    \begin{align*}
        \bE_{\mu^{\otimes N M}_*}[\exp(\gamma g)]
        &=\int_0^\infty \bP_{\mu^{\otimes N M}_*}\left[ \exp(\gamma g(\vX_1,\ldots,\vX_M)) > t \right] \rd t \\
        &=\int_0^\infty \bP_{\mu^{\otimes N M}_*}\left[ g(\vX_1,\ldots,\vX_M) > \frac{1}{\gamma} \log t \right] \rd t \\
        &=\int_0^\infty C'\exp\left( - \frac{MN(\log t)^2}{dC^2R^2\gamma^2} \right) \rd t \\
        &=C'CR\gamma\sqrt{\frac{\pi d}{MN}}\exp\left( \frac{dC^2R^2\gamma^2}{4MN} \right).
    \end{align*}
    Therefore, we get
    \begin{equation*}
        \bE_{\mu^{(N)\otimes M}}[g]
        \leq \frac{dC^2R^2\gamma}{4MN} + \frac{1}{\gamma}\log\left( C'CR\gamma\sqrt{\frac{\pi d}{MN}} \right)
        + \frac{M\KL(\mu^{(N)}\|\mu^{\otimes N}_*)}{\gamma}.
    \end{equation*}
    Moreover, by applying Lemma \ref{lemma:clsi} Theorem \ref{theorem:mfld_convergence} to Eq.~\eqref{eq:variational_kl}, we get
    \begin{align*}
        \bE_{\{\vX_{j,k}\}}&\left[\left\| \frac{1}{M}\sum_{j=1}^M \bE_{X\sim \rho_{\vx_{j,k}}}[h(X,\cdot)] - \bE_{X\sim \mu_*} [h(X,\cdot)]\right\|_\infty\right] \\
        &\leq \frac{dC^2R^2\gamma}{4MN}
        + \frac{1}{\gamma}\log\left( C'CR\gamma\sqrt{\frac{\pi d}{MN}} \right)
        + \frac{M}{\gamma} \left(  \frac{B}{\lambda} + \frac{ N\delta_{\eta}}{\alpha\lambda^2} 
        + \frac{N}{\lambda}\exp( -\alpha\lambda\eta k )\pow[\Delta,N]_0 \right).
    \end{align*}

    Finally, by seting $\gamma = \frac{1}{CR}\sqrt{\frac{MN}{d}\left(1+\frac{MB}{\lambda}\right)}$, we get
    \begin{align*}
        \bE_{\{\vX_{j,k}\}}&\left[\left\| \frac{1}{M}\sum_{j=1}^M \bE_{X\sim \rho_{\vx_{j,k}}}[h(X,\cdot)] - \bE_{X\sim \mu_*} [h(X,\cdot)]\right\|_\infty\right] \\
        &\leq \frac{5CR}{4}\sqrt{\frac{d}{MN} + \frac{dB}{\lambda N}}
        + CR\sqrt{\frac{d\lambda}{MN(\lambda + MB)}}\log\left( C'\sqrt{\left(\lambda+MB\right)\frac{\pi}{\lambda}} \right) \\
        &+ CR\sqrt{\frac{d\lambda M N}{\lambda + MB}} \left(\frac{ \delta_{\eta}}{\alpha\lambda^2} 
        + \frac{1}{\lambda}\exp( -\alpha\lambda\eta k )\pow[\Delta,N]_0 \right).
    \end{align*}
\end{proof}

%\begin{proof}[New proof]
%    Let $\vX_j=(X_j^1,\ldots,X_j^N) \sim \pow[\mu,N]~(j=1,2,\ldots,M)$ be $M$ independent random variables.
%    We set
%    \begin{align*}  
%        &g_N(z) = \frac{1}{MN}\sum_{j=1}^M\sum_{i=1}^N h(X_{j}^i,z) - \bE_{X \sim \pow[\mu,N]_1}[h(X,z)], \\
%        &S_N(z) = |g_N(z)| - \bE_{\{\vX_j\}_{j=1}^M}[ | g_N(z) | ].
%    \end{align*}
%    We define the metric $d$ defined in $\cZ$ as follows: for any $z,z'\in\cZ$,
%    \begin{align*}
%        d(z,z') = \sqrt{ \bE_{\{\vX_j\}_{j=1}^M}\left[ (S_N(z) - S_N(z'))^2 \right] }.
%    \end{align*}


%    Given $\{\vX_j\}_{j=1}^M$, we set 
%    \[ S_{M,N}(z) = \frac{1}{M}\sum_{j=1}^M \bE_{X\sim \rho_{\vx_{j}}}[h(X,z)] - \bE_{X\sim \mu_*} [h(X,z)],~~~~(z \in \cZ). \]

%    We endow the data space $\cZ$ with the metric $d(z,z') = |S_{M,N}(z) - S_{M,N}(z')|$ ($\forall z,z' \in \cZ$). We denote by $\cN(\cZ,\epsilon,d)$ the $\epsilon$-covering number of $\cZ$ with respect to $d$. We also denote by $\cZ_\epsilon \subset \cZ$ a finite subset that provides $\epsilon$-covering of $\cF$ corresponding to $\cN(\cZ,\epsilon,d)$. For any $z \in \cZ$, there exists $\pi_\epsilon(z) \in \cZ_\epsilon$ such that $d(z,z')\leq \epsilon$.

%    Noting that for any $z,z'\in \cZ$,
%    \begin{align*}
%        d(z,z')
%        &= \left| \frac{1}{MN}\sum_{j=1}^M\sum_{i=1}^N \left( h(X_j^i,z) - h(X_j^i,z') \right)
%        + \bE_{X\sim\mu_*}[h(X,z)-h(X,z')] \right| \\
%        &\leq \frac{1}{MN}\sum_{j=1}^M\sum_{i=1}^N \left| h(X_j^i,z) - h(X_j^i,z') \right|
%        + \bE_{X\sim\mu_*}\left[ \left| h(X,z)-h(X,z') \right| \right] \\
%        &\leq \beta \left( \frac{1}{MN}\sum_{j=1}^M\sum_{i=1}^N\|X_j^i\|_2
%        + \bE_{X\sim\mu_*}[\|X\|_2]\right)\|z-z'\|_2 \\
%        &= \beta r \|z-z'\|_2,
%    \end{align*}
%    where we set $r = \frac{1}{MN}\sum_{j=1}^M\sum_{i=1}^N\|X_j^i\|_2$, we see $\cN(\cZ, \epsilon, d) \leq \cN\left(\cZ, \epsilon/(\beta r), \|\cdot\|_{2}\right) = \left( 1 + \frac{2 \beta r}{\epsilon}\right)^{d}$.
    
%    Let $\vX_j=(X_j^1,\ldots,X_j^N) \sim \pow[\mu,N]~(j=1,2,\ldots,M)$ be $M$ independent random variables. Then, we get
%    \begin{align*}
%        \bE_{\{\vX_j\}_{j=1}^M}\left[ 
%            \sup_{z\in\cZ} S_{M,N}^2(z)
%        \right]
%        &= \bE_{\{\vX_j\}_{j=1}^M}\left[ 
%            \sup_{z\in\cZ} (S_{M,N}(z) - S_{M,N}(\pi(z)) + S_{M,N}(\pi(z)))^2
%        \right] \\
%        &\leq 2\bE_{\{\vX_j\}_{j=1}^M}\left[ \sup_{z\in\cZ} \left(S_{M,N}(z) - S_{M,N}(\pi(z))\right)^2 
%            + \sup_{z\in\cZ} S_{M,N}^2(\pi(z))
%        \right]\\
%       &\leq 2\bE_{\{\vX_j\}_{j=1}^M}\left[ \epsilon^2 
%           + \sup_{z\in\cZ} S_{M,N}^2(\pi(z))
%        \right]\\
%        &\leq 2\epsilon^2 + 2\bE_{\{\vX_j\}_{j=1}^M}\left[ 
%            \max_{z\in\cZ_{\epsilon}} S_{M,N}^2(z)
%        \right]
%    \end{align*}
    
%    Because of the boundedness $\sup_{x \in \bR^d, z\in\cZ}|h(x,z)| \leq R$, $S^2_{M,N}(z) - \bE[S^2_{M,N}(z)]$ is a sub-Gaussian with variance factor $4R^4$. Therefore, by the maximal inequality for sub-Gaussian distribution (e.g., see \citet{boucheron2013concentration}), we get
%    \begin{align*}
%        &\max_{z\in\cZ_{\epsilon}} \left\{ S_{M,N}^2(z) - \bE[S^2_{M,N}(z)]\right\}
%        \leq 2\sqrt{2}R^2 \log \cN(\cZ,\epsilon,d)
%        \leq 2\sqrt{2}R^2 d \log \left( 1 + \frac{2\beta r}{\epsilon}\right) \\
%        \iff &\max_{z\in\cZ_{\epsilon}} S_{M,N}^2(z)
%        \leq \max_{z\in\cZ_{\epsilon}} \bE[S^2_{M,N}(z)]
%        + 2\sqrt{2}R^2 d \log \left( 1 + \frac{2\beta r}{\epsilon}\right).
%    \end{align*}
%\end{proof}