\section{Experiments}\label{sec:experiments_extra}
The code used in this work will be made publicly available later.
%{The code to replicate our results can be found in this public Github repositroy:}
%\AL{make a new public github repo and include link later}
%\AN{We should adhere to dual blind policy during review process. Please refer to https://icml.cc/Conferences/2025/AuthorInstructions}
%\AN{This mean it is safe to submit the code as a supplement file (zipped). After the acceptance, we can make the code open on github.}
%\AL{Yes, I will consult you on this tomorrow, thank you.}

\subsection{Pseudocode and training settings for mean-field experiments} \label{subsec:pseudocode}

For experiments concerning MFNNs, the output of a neuron in a two-layer MFNN is modelled by: $h(x_i, z_i) = R\tanh(x_i^3) \tanh(x_i^{1\top} z_i + x_i^2)$, where $x_i = (x_i^1, x_i^2, x_i^3) \in \bR^{d + 1 + 1}$ is its parameter, $z_i$ is the given input and $R$ is a scaling constant. The $\tanh$ activation function is placed on the second layer as boundedness of the model is crucial for our analysis. Noisy gradient descent is then used to train neural networks for $T$ epochs each. We omit the pseudocode for training MFNNs with MFLD since it is identical to the backpropagation with noisy gradient descent algorithm.

\paragraph{Algorithm \ref{alg:circle_data}} Generate the double circle data: $\mathcal{D} = \left( z_i, y_i\right)^n_{i=1}$, $z_i \in \bR^2, y_i \in \bR$ before splitting it into $\mathcal{D}_\text{train}$ and $\mathcal{D}_{\text{test}}$. We set $n=200$, $r_\text{inner}=1$, $r_\text{outer}=2$ and use an 80-20 train-test split for the data.

\paragraph{Algorithm \ref{alg:multi_index_data}} Generate the $k$ multi-index data: $\mathcal{D} = \left( z_i, y_i\right)^n_{i=1}$, $z_i \in \bR^d, y_i \in \bR$. A key step is normalizing and projecting $z_i$ to the inside of a $d$-dimensional hypersphere. We set $n = 500$, $d = 100$, $r=5$, $k=100$ and $\bar{R} = 100$. 

\paragraph{Algorithm \ref{alg:classification}} Describes how we obtain and test the performance of merged MFNNs against (an approximation to) the mean-field limit by computing the sup-norm between both outputs. The relevant results are stored into a dictionary for plotting the heatmaps. The training procedure is identical for both the classification and regression problem. Let $M_\text{max} = 20$ and $N_\text{list} = \{50, 100, \dots ,500 \}$ denote the maximum number of networks to merge and list of neuron settings to train in parallel respectively. We set the hyperparameters for training as follows:
\begin{itemize}
    \item Classification: $R=10$, $N_\infty=10000$, $\eta = 0.1$, $\lambda' = 0.1$, $\lambda = 0.01$, $T=200$ and loss function: logistic loss
    \item Regression: $R=10$, $N_\infty=10000$, $\eta = 0.01$, $\lambda' = 0.1$, $\lambda = 0.01$, $T=100$ and loss function: mean squared error
\end{itemize}


\begin{algorithm} 
\caption{Generate data points along cocentric 2D circles}\label{alg:circle_data}
\begin{algorithmic}[1]
\REQUIRE $n$, $r_{\text{inner}}$, $r_{\text{outer}}$
\ENSURE Dataset $\mathcal{D} = \{(z_i, y_i)\}_{i=1}^n$
\STATE Initialize $\mathcal{D} \gets \emptyset$
\FOR{$i = 1$ to $n$}
    \STATE Sample $\theta \sim \text{Uniform}(0, 2\pi)$
    \STATE Sample $\xi_1, \xi_2 \sim \text{Normal}(0, 0.1)$
    \IF{$i <  n/2$}
        \STATE $r \gets r_{\text{inner}}$
        \STATE $y_i \gets -1$
    \ELSE
        \STATE $r \gets r_{\text{outer}}$
        \STATE $y_i \gets +1$
    \ENDIF
    \STATE Compute Cartesian coordinates: $z_i = (r \cos(\theta) + \xi_1, r \sin(\theta) + \xi_2)$
    \STATE Add $(z_i, y_i)$ to $\mathcal{D}$
\ENDFOR
\STATE Randomly shuffle $\mathcal{D}$
\STATE Split $\mathcal{D}$ into $\mathcal{D}_{\text{train}}$ (80\%) and $\mathcal{D}_{\text{test}}$ (20\%)
\STATE \textbf{return} $\mathcal{D}_{\text{train}}, \mathcal{D}_{\text{test}}$
\end{algorithmic}
\end{algorithm}

\clearpage
\begin{figure}[H]
\vspace{-1.5em}
\begin{algorithm}[H] 
\caption{Generate $k$ multi-index data}
\begin{algorithmic}[1] \label{alg:multi_index_data}
\REQUIRE $n$, $d$, $r$, $k$, $\bar{R}$
\ENSURE Dataset $\mathcal{D} = \{(z_i, y_i)\}_{i=1}^n$
\STATE Initialize $\mathcal{D} \gets \emptyset$
\FOR{$i = 1$ to $n$}
    \STATE Sample $\zeta \sim \text{Normal}(0,1)$
    \STATE $\zeta \gets \zeta^{(1/d)} \times r$ \hfill \COMMENT{Get scaling constant}
    \STATE Sample $z \sim \text{Normal}\left(0, \text{I}_d \right)$
    
    \STATE $z_i \gets z/ |z|$ \hfill \COMMENT{Normalize} 
    \STATE $z_i \gets z_i \times \zeta$ \hfill \COMMENT{Project}
    \STATE $y_i \gets 0$
    \FOR{$j = 1$ to $k$}
        \STATE $y_i \gets y_i + \tanh \left(z_i^j \right)$
    \ENDFOR
\STATE $y_i \gets y_i \times (\bar{R}/k)$
\STATE Add $(z_i, y_i)$ to $\mathcal{D}$
\ENDFOR
\STATE Split $\mathcal{D}$ into $\mathcal{D}_{\text{train}}$ (80\%) and $\mathcal{D}_{\text{test}}$ (20\%)
\STATE \textbf{return} $\mathcal{D}_{\text{train}}, \mathcal{D}_{\text{test}}$
\end{algorithmic}
\end{algorithm}
\end{figure}

\begin{algorithm} 
\caption{Training and merging MFNNs}\label{alg:classification}
\begin{algorithmic}[H]
\REQUIRE $\mathcal{D}_{\text{train}}, \mathcal{D}_{\text{test}} = (z_\text{test}, y_\text{test})$, $N_\infty$, $N_\text{list}$, $M_\text{max}$
\ENSURE Dictionary \textit{sup\_norm\_dic} maps $N$ to the average sup\_norm
\STATE $h_\infty \gets$ Train a MFNN with $N_\infty$ neurons on $\mathcal{D}_{\text{train}}$
\STATE $\hat{y}_\infty \gets$Use $h_\infty$ to predict on $\mathcal{D}_{\text{test}}$
\STATE Initialize \textit{sup\_norm\_dic} $\gets \{\}$
\FOR{$N \in N_\text{list}$}
    \STATE  $\{h_N^{1}, h_N^{2}, \dots h^{M_\text{max}}_N \} \gets$Train $M_\text{max}$ MFNNs with $N$ neurons on $\mathcal{D}_{\text{train}}$
    \STATE Initialize \textit{sup\_norm\_lst} $\gets []$

    \FOR{$M \in \{1, 2, \dots M_\text{max} \}$}
        \STATE \textit{sup\_norm\_total} $\gets 0$
        \FOR{50 iterations}
            \STATE Randomly sample $M$ networks from $\left \{ h_N^1, h_N^2, \dots, h^{M_\text{max}}_N \right \}$
            \STATE $h_{MN} \gets$Merge the $M$ networks to form a new neural network 
            \STATE $\hat{y} \gets$Use $h_{MN}$ to predict on $\mathcal{D}_{\text{test}}$
            \STATE \textit{sup\_norm} $\gets \text{max}\left(|\hat{y} - \hat{y}_\infty |\right)$
            \STATE $\text{\textit{sup\_norm\_total}} \gets \text{\textit{sup\_norm\_total}} + \text{\textit{sup\_norm}}$
        \ENDFOR
        \STATE Append \textit{sup\_norm\_total}/ 50 to \textit{sup\_norm\_lst}
    \ENDFOR
    \STATE \textit{sup\_norm\_dic}[\textit{N}] $\gets$ \textit{sup\_norm\_lst}
\ENDFOR
\STATE \textbf{return} \textit{sup\_norm\_dic}
\end{algorithmic}
\end{algorithm}


\clearpage

\subsection{Additional MFNN experiments} \label{subsec:additional_experiments}
Beyond examining the effect of both $M$ and $N$ on sup norm, we also compare the convergence rate of MFNNs using different $\lambda \in \{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4} \}$ on the multi-index regression problem. Since the training dataset is small and we intend to investigate high $\lambda$, we have to consider the low epoch setting to prevent deterioration of generalization capabilities. We train 20 networks in parallel and average the MSE (in log-scale) at each epoch, repeating this for $N\in \{300, 400, \dots, 800\}$. Figure \ref{fig:experiments_extra} shows that higher $\lambda$ improves the convergence speed of particles and makes training more stable. Finally, we merge networks with the same hyperparameters for comparison across different $\lambda$. A similar trend is observed in Table \ref{table:experiments_extra}, highlighting the efficacy of PoC-based ensembling when training for fewer epochs with a high $\lambda$.

\begin{figure}[H]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{extra_experiment.png}}
\caption{Averaged test ln(MSE) of singular MFNNs, across different $N$ and $\lambda$ for 5 epochs}  
\label{fig:experiments_extra}
\end{center}
\end{figure}

\begin{table*}[th]
    \centering
    \caption{MSE comparison between merging $M=20$ networks across different $N$ and $\lambda$ after 5 epochs.}
    \label{table:experiments_extra}
    \begin{footnotesize}
    \begin{tabular}{ccccccc} 
    \toprule
    & \multicolumn{6}{c}{$\boldsymbol{N}$} \\
    $\boldsymbol{\lambda}$ & 300 & 400 & 500 & 600 & 700 & 800\\
    \midrule
    $10^{-1}$ & \underline{0.9132253} & \underline{0.9040508}& \underline{0.9075238}& \underline{0.9044338}& \underline{0.9030165}&  \underline{0.9022377}\\
    $10^{-2}$ & 1.2325489& 1.2229528& 1.2166352& 1.1978958 & 1.1921849& 1.1654898\\
    $10^{-3}$ & 1.5718020& 1.5668763& 1.5607907& 1.5581368& 1.5282313& 1.5234329\\
    $10^{-4}$ & 1.6987042& 1.6887244& 1.6631799& 1.6135653& 1.5860944& 1.5821924\\
    \bottomrule
    \end{tabular}
    \end{footnotesize}
\end{table*}


\clearpage


\subsection{LoRA for finetuning language models}\label{subsec:experiments_extra_lora}
To examine the effect of $\lambda$, we perform LoRA and PoC-based merging by varying $\lambda \in \{0,10^{-5},10^{-4}\}$ with one-epoch training. We optimize eight LoRA parameters of rank $N=32$ in parallel using noisy AdamW with the speficied $\lambda$. Table \ref{table:LoRA_comparison_1epoch} summarizes the results. For LoRA, the table lists the best result among the eight LoRA parameters based on the average accuracy across all datasets and also provides the average accuracies of the eight parameters for each dataset. We observed that for Llama2-7B with $\lambda=0$ and  $\lambda=10^{-5}$, the chances of the optimization converging are very low. Consequently, both the average accuracy of eight LoRAs and the accuracy of PoC-based merging are also low. This is because the regularization strength $\lambda$ controls the optimization speed as seen in Theorem \ref{theorem:mfld_convergence}. On the other hand, by using a high constant $\lambda=10^{-4}$ the average performance was improved, and PoC-based merging achieved quite high accuracy even with only one-epoch of training. This result suggests using high $\lambda$ to reduce the training costs, provided it does not negatively affect generalization error. For Llama3-8B, one-epoch training is sufficient to converge, and while LoRA performed well and PoC-based merging further improved the accuracies.

\begin{table*}[th]
    \centering
    \caption{Accuracy comparison of LoRA and PoC-based merging for finetuning Llama models (1 epoch).}
    \label{table:LoRA_comparison_1epoch}
    \begin{footnotesize}
    \begin{tabular}{cccccccccccc}
        \toprule
        \textbf{Model} & \textbf{Method} & \textbf{$\lambda$} & \textbf{SIQA} & \textbf{PIQA} & \textbf{WinoGrande} & \textbf{OBQA} & \textbf{ARC-c} & \textbf{ARC-e} & \textbf{BoolQ} & \textbf{HellaSwag} & \textbf{Ave.} \\
        \midrule
        \multirow{11}{*}{\begin{tabular}{c}Llama2\\7B\end{tabular}}
            & LoRA (best) & $0$  & $80.55$ & $82.86$ & $83.19$ & $81.60$ & $71.08$ & $84.51$ & $71.90$ & $90.21$ & $80.74$ \\
            & LoRA (ave.) & $0$ & $64.73$  & $76.31$ & $77.76$ & $68.70$ & $57.02$ & $69.02$ & $69.04$ & $70.63$ & $69.15$ \\
            & \textbf{PoC merge} & $0$ & $32.29$ & $62.57$ & $83.58$ & $22.20$ & $28.41$ & $29.42$ & $61.53$ & $28.50$ & $43.56$ \\
            \cmidrule(lr){2-12}
            & LoRA (best) & $10^{-5}$ & $80.14$ & $82.37$ & $83.43$ & $80.40$ & $68.86$ & $83.42$ & $71.68$ & $89.94$ & $80.03$ \\
            & LoRA (ave.) & $10^{-5}$  & $74.37$ & $74.12$ & $80.55$ & $67.50$ & $58.34$ & $71.98$ & $69.43$ & $66.25$ & $70.32$ \\
            & \textbf{PoC merge} & $10^{-5}$ & $74.56$ & $83.84$ & $85.16$ & $60.00$ & $63.14$ & $78.37$ & $68.72$ & $92.77$ & $75.82$ \\
            \cmidrule(lr){2-12}
            & LoRA (best) & $10^{-4}$ & $78.20$ & $80.90$ & $81.22$ & $78.40$ & $65.19$ & $79.00$ & $69.97$ & $86.50$ & $77.42$ \\
            & LoRA (ave.) & $10^{-4}$  & $74.42$ & $77.70$ & $76.08$ & $75.93$ & $60.93$ & $76.25$ & $65.68$ & $66.71$ & $71.71$ \\
            & \textbf{PoC merge} & $10^{-4}$ & $80.76$ & $82.15$ & $84.85$ & $84.80$ & $71.25$ & $85.35$ & $72.26$ & $91.65$ & $81.63$ \\
        \midrule
        \multirow{11}{*}{\begin{tabular}{c}Llama3\\8B\end{tabular}}
            & LoRA (best) & $0$ & $80.45$ & $88.47$ & $86.82$ & $87.60$ & $82.25$ & $90.87$ & $73.85$ & $95.78$ & $85.76$ \\
            & LoRA (ave.) & $0$  & $80.51$ & $88.87$ & $86.85$ & $87.00$ & $80.78$ & $90.98$ & $73.71$ & $95.84$ & $85.57$ \\
            & \textbf{PoC merge} & $0$ & $81.73$ & $88.96$ & $87.77$ & $88.00$ & $81.40$ & $91.71$ & $74.46$ & $96.45$ & $86.31$ \\
            \cmidrule(lr){2-12}
            & LoRA (best) & $10^{-5}$ & $80.50$ & $88.68$ & $86.98$ & $86.80$ & $81.48$ & $91.12$ & $75.14$ & $95.97$ & $85.83$ \\
            & LoRA (ave.) & $10^{-5}$  & $80.83$ & $88.64$ & $86.85$ & $87.05$ & $80.39$ & $90.76$ & $71.54$ & $95.87$ & $85.24$ \\
            & \textbf{PoC merge} & $10^{-5}$ & $81.53$ & $89.45$ & $87.92$ & $87.80$ & $82.25$ & $91.79$ & $75.54$ & $96.44$ & $86.59$ \\
            \cmidrule(lr){2-12}
            & LoRA (best) & $10^{-4}$ & $80.30$ & $88.57$ & $86.42$ & $87.20$ & $78.07$ & $89.81$ & $73.61$ & $95.14$ & $84.89$ \\
            & LoRA (ave.) & $10^{-4}$ & $80.00$ & $88.20$ & $85.69$ & $86.23$ & $78.86$ & $89.48$ & $73.08$ & $95.05$ & $84.57$ \\
            & \textbf{PoC merge} & $10^{-4}$ & $80.71$ & $89.72$ & $88.08$ & $89.00$ & $82.17$ & $91.79$ & $74.56$ & $96.36$ & $86.55$ \\
        \bottomrule
    \end{tabular}
    \end{footnotesize}
\end{table*}
