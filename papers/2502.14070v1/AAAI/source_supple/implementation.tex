We conduct experiments by integrating our approach with two reward fine-tuning methods: DDPO~\cite{ddpo} and AlignProp~\cite{alignprop}. Each experiment is based on the implementation of the respective baseline.
In the DDPO experiments, we set the number of denoising steps (i.e. inference steps) to 50, the CFG scale to 5.0, and sample 64 images in each iteration to apply the policy-gradient method. For the policy-gradient update, we use importance sampling with a clip range of 0.0001. In the AlignProp experiments, we set the number of inference steps to 50, the CFG scale to 7.5, and sample 64 images in each iteration to directly backpropagate the gradient of the reward model through the denoising process. 
Regarding the training details, the learning rate is set to 3e-4 in the DDPO experiments and 1e-3 in the AlignProp experiments. The model is updated using AdamW optimizer with $\beta_1=0.9$ and $\beta_2 = 0.99$ in both experiments. 
We use 4 NVIDIA RTX A6000 GPUs for all our experiments.