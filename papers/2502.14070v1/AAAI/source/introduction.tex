\begin{figure}[!t]
  \centering
  \includegraphics[width=1.0\linewidth]{AAAI/images/dolphin_exp_v5.pdf}
  \caption{
  (a) Generated images during the reward optimization process with our proposed method called DiffExp given the prompt ``a dolphin riding a bike,'' which is often challenging for existing reward fine-tuning approaches, including our baseline DDPO~\cite{ddpo}. (b) We also provide corresponding reward curves against the number of reward queries, where our method indeed improves its sample efficiency during reward optimization process, capturing good reward signals for reward fine-tuning. 
  }
  \label{fig:dolphin}
\end{figure}

%%% 1. Success of RLHF in T2I models
Reward fine-tuning~\cite{ddpo,dpok,draft,alignprop, align_t2i} has recently emerged as a powerful method for improving text-to-image diffusion models~\cite{sdxl,stable_diffusion}.
Unlike the conventional optimization strategy of likelihood maximization, this framework focuses on maximizing reward scores that measure the quality of model outputs, such as image-text alignment~\cite{pickscore,align_t2i,hps,imagereward} and image fidelity~\cite{aesthetic}.
Several methods including policy gradient~\cite{ddpo,dpok} and direct backpropagation~\cite{draft,alignprop} have been studied for reward maximization.
These methods have shown promising results in improving image-text alignment~\cite{ddpo,dpok}, reducing undesired biases~\cite{dpok}, and removing artifacts from generated images~\cite{hps}. 

%%% 2. Limitations: slow training
Since this reward-based fine-tuning involves online sample generation, the reward optimization depends on what samples are produced during the generation process. 
In other words, if samples with good reward signals are not obtained during the generation process, the model will converge slowly due to the lack of these signals. 
To demonstrate this, we conduct reward-based fine-tuning using the single prompt ``a dolphin riding a bike'', which is known to be challenging for reward optimization~\cite{ddpo}.
In this experiment, we fine-tune Stable Diffusion 1.5~\cite{stable_diffusion} using the policy gradient method (i.e., DDPO~\cite{ddpo}) to maximize image-text alignment based on ImageReward~\cite{imagereward} scores. 
As shown in Figure~\ref{fig:dolphin}, the current method fails to find a good reward signal (i.e., images containing a bicycle) during the optimization process, resulting in trivial solutions that change the style of the image without improving image-text alignment.
This highlights the need for enhanced exploration in the sample generation process to better capture good reward signals.


%%% 3. Our method
In this work, we introduce \metabbr (DiffusionExplore), an exploration method designed to efficiently fine-tune text-to-image diffusion models using rewards.
Our method relies on two main strategies: dynamically controlling the scale of classifier-free guidance (CFG)~\cite{cfg} and randomly weighting certain phrases in the text prompt.
CFG is a sampling technique that balances sample diversity and fidelity.
Unlike conventional approaches that fix the CFG scale during the denoising process, we propose initially setting this scale to an extremely low value and increasing it in the later step.
This dynamic scheduling improves the diversity of the online samples while maintaining their fidelity.
To further enhance diverse sample generation, we perform additional exploration by randomly assigning weights to certain phrases within the text prompt. 
This approach creates different images where specific elements of the text prompt are emphasized randomly, thereby promoting the emergence of valuable reward signals. 
We find that these two key strategies significantly boost the effectiveness of reward-based fine-tuning (see blue curve in Figure~\ref{fig:dolphin}).

%%% 4. Experimental Results summary
To verify the effectiveness of our exploration method, \metabbr, we integrate it with two popular reward fine-tuning techniques: policy gradient method~\cite{ddpo} and direct reward backpropagation method~\cite{alignprop}.
Specifically, we fine-tune the Stable Diffusion model~\cite{stable_diffusion} using our method to optimize various reward functions, including Aesthetic scorer~\cite{aesthetic} and PickScore~\cite{pickscore}. 
Our experiments demonstrate that \metabbr improves the sample-efficiency of both the policy gradient and reward backpropagation methods by encouraging more efficient exploration, which in turn increases the diversity of online samples.
Finally, we show that our method also significantly improves the quality of high-resolution text-to-image models like SDXL~\cite{sdxl}. We summarize our contributions as follows:

\begin{itemize}
    \item We propose \metabbr: an exploration method that improves diversity of online sample generation for efficient reward fine-tuning of text-to-image diffusion models.
    \item We show that our method enhances sample-efficiency of reward optimization in various fine-tuning methods, such as policy gradient and direct backpropagation.
    \item We demonstrate that our method is also effective when applied to the more recent SDXL model, as well as on the more challenging prompt set, DrawBench.
\end{itemize}

\begin{figure*}[tb]
  \centering
  \includegraphics[width=\linewidth]{AAAI/images/aaai_diffexp_main_v3.pdf}
  \caption{An overview of our proposed method called DiffExp, which consists of two main steps: (a) random prompt weighting and (b) dynamic scheduling of the CFG (classifier-free guidance) scale. In (a), word embeddings of the given prompt are randomly and differently weighted, which are then consumed by the image generation process, increasing the diversity of generated images. Further, in (b), the CFG scale of the denoising process is dynamically scheduled to control models to generate high-quality and diverse images, which is often challenging with a constantly set CFG scale. 
  }
  \label{fig:overall_method}
\end{figure*}
