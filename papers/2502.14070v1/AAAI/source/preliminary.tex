\paragraph{Text-to-image diffusion models.} 
Diffusion models~\cite{ddpm} are a class of generative models that represent the data distribution $p(x)$ by iteratively transforming Gaussian noise into data samples through a denoising process.
These models can be extended to model the conditional distribution $p(x|c)$ by incorporating an additional condition $c$.
In this work, we consider text-to-image diffusion models that utilize textual conditions to guide image generation. 
The models consist of parameterized denoising function $\epsilon_{\theta}$ and are trained by predicting added noise $\epsilon$ to the image $x$ at timestep $t$. 
Formally, given dataset $D$ comprising of image-text pairs $(x,c)$, the training objective is as follows:
\begin{equation}
\mathcal{L}_{DM}:={E}_{x, c, \epsilon, t}\left[\left\|\epsilon-\epsilon_\theta\left(x_t, t, c\right)\right\|_2^2\right],
\end{equation}
where $\epsilon$ is a Gaussian noise $\sim \mathcal{N}(0, I)$, $t$ is a timestep sampled from uniform distribution $\mathcal{U}(0, T)$, and $x_t$ is the noised image by diffusion process at a timestep $t$. At inference phase, sampling begins with drawing $x_T \sim \mathcal{N}(0, I)$, and denoising process is iteratively conducted to sample a denoised image using the estimated noise by $\epsilon_{\theta}$. 
Specifically, the estimated noise is utilized to obtain the mean of transition distribution $p(x_{t-1}|x_t, c)$ in the denoising process, as follows:
\begin{equation}
    \mu_\theta\left(x_t, t, c\right)=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta\left(x_t, t, c\right)\right),
    \label{eq:ddpm_mean}
\end{equation}

\begin{equation}
p_\theta\left(x_{t-1} \mid x_{t},c\right)=\mathcal{N}\left(\mu_\theta\left(x_{t}, t, c\right), \Sigma_t\right),
\label{eq:ddpm_transition}
\end{equation}
where $\alpha_t$, $\beta_t$ are pre-defined constants used for timestep dependent denoising, $\Sigma_t$ represents covariance matrix of denoising transition, and $\bar{\alpha}_t:=\prod_{s=1}^t \alpha_s$.

\paragraph{Classifier-free guidance (CFG).} 
\citet{cfg} proposed {\em classifier-free guidance} (CFG), which serves as the basis of recent text-to-images diffusion models. In CFG, the denoising network predicts 
the noise based on the linear combination of conditional and unconditional noise estimates as follows:
\begin{equation}
\label{eq:cfg_normal}
\tilde{\epsilon}_\theta\left(x_t, t, c\right)=\epsilon_\theta\left(x_t, t\right) + w *\left(\epsilon_\theta\left(x_t, t, c\right) - \epsilon_\theta\left(x_t, t\right) \right),
\end{equation}
where $w$ is the guidance scale of CFG.
By incorporating the unconditional noise estimate into the prediction, we can achieve a trade-off between sample quality and diversity. 
Specifically, increasing the guidance weight $w$ enhances the fidelity of the samples but reduces their variety.