 In this section, we introduce \metabbr (DiffusionExplore): an exploration method for efficient fine-tuning of text-to-image diffusion models using rewards. We first describe the problem setup regarding reward optimization of text-to-image diffusion models. Then, we present our approach, which promotes exploration in sample generation by scheduling CFG scale and randomly weighting a phrase of prompt. 

\subsection{Problem Formulation}
\label{sec:method_formulation}

We consider the problem of fine-tuning a text-to-image model using a reward model. The goal is to maximize the expected reward $r(x_0, c)$ for image $x_0$ generated by the model $p_\theta$ using text prompt $c$. Formally, it can be formulated as follows:
\begin{equation}
  \max_\theta E_{p(c)} E_{p_\theta\left(x_0 \mid c\right)}\left[r(x_0, c)\right],
  \label{eq:reward_obj}
\end{equation}
where $p(c)$ is the training prompt distribution and $p_\theta(\cdot|c)$ is the sample distribution for the generated image $x_0$. To achieve this goal, we consider online optimization methods that continuously generate image samples while optimizing models with rewards. Specifically, we utilize two types of reward fine-tuning methods: {\em the policy gradient method}~\cite{dpok,ddpo} and {\em direct reward backpropagation method}~\cite{draft, alignprop}. In the policy gradient method, the denoising process is defined as a multi-step Markov Decision Process, and the transition distribution $p_\theta\left(x_{t-1} \mid x_{t},c\right)$ (described in Equation~\ref{eq:ddpm_transition}) is treated as a policy. Based on this framework, the gradient of the objective in Equation 1 is computed as follows to update the model:
\begin{equation}
E_{p(c),p_\theta(x_0|c)}\left[\sum_{t=1}^T \nabla_\theta \log p_\theta\left(x_{t-1}| x_t, c\right)r\left(x_0, c\right)\right].
    \label{eq:diffusion_policy}
\end{equation}
As an alternative approach, the direct reward backpropagation method assumes that the reward model is differentiable and directly backpropagates the gradient of the reward function through the denoising process to optimize the objective in Equation ~\ref{eq:reward_obj}. 


\subsection{Dynamic Scheduling of CFG Scale}

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{AAAI/images/CFG_example_v5.pdf}
  \caption{Comparison of the generated images with different CFG scale scheduling strategies: (a) constantly high CFG scale, (b) constantly low CFG scale, and (c) dynamically scheduled CFG scale. We observe that a model often shows increased sample diversity with a low CFG scale but suffers from degraded image quality, which is generally the opposite with a high CFG scale. Instead, dynamically scheduling the CFG scale balances sample diversity and image quality. 
  % Note that images in each column are generated with the same random seed.
  }
  \label{fig:cfg_scale_comparison}
\end{figure}

To effectively discover good reward signals, it is crucial to generate diverse image samples through exploration. To achieve this, we propose dynamically scheduling the CFG scale of the denoising process during online optimization. 
As detailed in the preliminaries, the CFG scale controls the trade-off between sample quality and diversity: a high CFG scale yields high-quality but low-diversity samples, whereas a low CFG scale promotes diversity at the cost of reduced quality (see Figure \ref{fig:cfg_scale_comparison}).
To optimize this balance, we start with a low CFG scale during the early stages of the denoising process and switch to a high CFG scale after the $t_{thres}$ denoising step, as follows:
$$\tilde{\epsilon}_\theta\left(x_t, t, c\right) = \epsilon_\theta\left(x_t, t\right) + w(t)\cdot\left(\epsilon_\theta\left(x_t, t, c\right) - \epsilon_\theta\left(x_t, t\right)\right),$$
where
\begin{equation}
    w(t) = \left\{ 
    \begin{array}{lll}
        w_l & \textit{if} &  t > t_{thres}, \\
        w_h & \textit{if} & t \leq t_{thres},
    \end{array}
    \right.
\end{equation}
and $w_l$, $w_h$ ($w_l < w_h$) are the CFG scale values. Note that in the denoising process, we get Gaussian noise at $t=T$ while the final image is at $t=0$. We set $w_l$ to an extremely low value and $w_h$ to an ordinary CFG value (i.e., 5.0 or 7.5).
This dynamic scheduling adaptively balances between image quality and diversity, allowing for generating diverse image samples without sacrificing overall sample quality.


\subsection{Random Prompt Weighting}

To further promote diverse sample generation, we propose an additional exploration method that alters text prompts.
Specifically, we increase the weight of a random word (i.e. token) in the text prompt embedding. 
This adjustment emphasizes the selected word in the generated image, leading to variations where different elements are highlighted.
%Adjusting the weight of a word leads to the corresponding element being emphasized in the generated image.
%Thus, re-weighting random words lets us automatically produce diverse images where different elements in the generated image are emphasized.
Consider the text prompt \texttt{"A dolphin riding a bike"} for example, which typically results in images featuring only a dolphin, with the bike often omitted.
By increasing the weight of the word \texttt{"bike"}, there is a high chance that text-to-image diffusion models generate the image containing a bike.
This leads to more complete representations of the original prompt, thereby achieving higher prompt alignment rewards and enhancing image diversity.
%as well, leading to images that contain both elements (which receive higher prompt alignment reward).}

Formally, given a text embedding $c$ with $N_{words}$ words, we choose a random word with index $0 \leq i < N_{words}$. Then, we increase the weight of the chosen word as follows:
\begin{equation}
c[i] \longleftarrow c_{null}+w_{prompt} *\left(c[i]-c_{null}\right),
\end{equation}
where $c_{null}$ is the text embedding that corresponds to the empty text (``''), and $w_{prompt}$ is the prompt weight. 
We find that sampling $w_{prompt}$ randomly from $\mathcal{U}(1, 1.2)$ every time is generally successful.


\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{AAAI/images/reward_curve_seen_v3.pdf}
  \caption{Reward curves for training prompts. At each checkpoint, we generate 10 images per seen prompt and use the average of their reward scores. Our sampling method is employed only during fine-tuning, not for plotting this curve.}
  \label{fig:seen_curve}
\end{figure*}