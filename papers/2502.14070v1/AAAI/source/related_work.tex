\paragraph{Fine-tuning diffusion models using reward functions.}
Optimizing text-to-image diffusion models on rewards has been proven effective in supervising the final output of a diffusion model by a given reward. They are especially useful when training objectives are difficult to define given a set of images, such as human preference. Early studies~\cite{align_t2i} use supervised approaches to fine-tune the pre-trained models on images, which are weighted according to the reward function. As they are not trained online on examples, recent works~\cite{ddpo, dpok} further explore optimizing rewards using policy gradient algorithms by formulating the denoising process as a multi-step decision-making task. Though these RL approaches can flexibly train models on non-differentiable rewards, they might lose useful signals as analytic gradients of many reward functions are, in practice, available. AlignProp~\cite{alignprop} and DRaFT~\cite{draft} explored optimizing diffusion models with differentiable rewards of human preference~\cite{pickscore, hps}, effectively improving the image generation quality. 
In this work, we explore another strategy by focusing on an exploration technique toward a more efficient reward optimization process.


\paragraph{Efficient reward fine-tuning with exploration.}
In reinforcement learning (RL), where agents learn to make decisions by interacting with the environment, exploration is a crucial aspect. 
Exploration directly impacts the discovery of optimal policies, as it allows agents to gather diverse experiences. 
Intrinsic Curiosity Module (ICM)~\citep{icm} motivates exploration by rewarding actions that lead to unseen states, promoting discovery of unexplored areas of the environment.
Soft Actor-Critic (SAC)~\citep{sac} enhances exploration by employing entropy regularization, encouraging a wider range of actions. 
These works promote exploration by driving the agent to gather new experiences. 
There are works that investigate efficient reward fine-tuning using generative diffusion models as well.
A concurrent work~\citep{sergey1} proposed to endorse exploration by promoting diversity, formulating the fine-tuning of diffusion models as entropy-regularized control against pre-trained diffusion model.
Another work~\citep{sergey2} facilitates exploration by integrating an uncertainty model and KL regularization into the diffusion model tuning process.
Inspired by the success of exploration strategy in RL literature and the reward fine-tuning of diffusion models, we propose an effective exploration method for reward fine-tuning of text-to-image diffusion models.