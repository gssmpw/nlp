We design our experiments to investigate the following questions:
\begin{enumerate}
    \item Can our exploration method enhance the sample-efficiency of reward fine-tuning methods?
    \item Can our exploration method improve the quality of generated samples?
    \item Can our exploration method exhibit generalization ability for unseen prompts that are not used during fine-tuning? 
\end{enumerate}

\begin{figure*}[!t]
  \centering
  \includegraphics[width=\linewidth]{AAAI/images/image_example_v4.pdf}
  \caption{Generated samples from baselines and ours with (a) Aesthetic and (b) PickScore reward models. Notably, ours generates images with comparably high aesthetic quality (see (a)) and produces images with better image-text alignment given the prompts (see (b)). Note that images in the same column are generated with the same random seed. 
  }
  \label{fig:sample_quality}
\end{figure*}

%\subsection{Experimental Setup}
\paragraph{Baselines.}
As discussed in the Problem Formulation section, we evaluate the effectiveness of our exploration method using two types of state-of-the-art reward fine-tuning methods: policy gradient method~\cite{dpok,ddpo} and direct reward backpropagation method~\cite{draft,alignprop}. We combine our approach with each of these methods and compare the performance with the original methods. Specifically, we use (1) DDPO~\cite{ddpo} for the policy gradient method and (2) AlignProp~\cite{alignprop} for direct reward backpropagation method.

\paragraph{Reward functions.}
To evaluate our method across different types of rewards, we conduct experiments using two distinct reward functions. First, we utilize an Aesthetic Score~\cite{aesthetic}, which is trained to predict the aesthetic quality of images. Following the baseline~\cite{ddpo, alignprop}, we use 45 animal names as training prompts for the aesthetic quality task. Second, in order to improve image-text alignment, we employ PickScore~\cite{pickscore}, an open-source reward model trained on a large-scale human feedback dataset. Based on the baseline~\cite{ddpo}, we use a total of 135 prompts for the image-text alignment task, combining 45 different animal names with 3 different activities (e.g., ``a monkey washing the dishes''). We provide the entire set of prompts used for training in the supplementary materials.

\paragraph{Implementation details.}
In our experiment, we use Stable Diffusion v1.5~\cite{stable_diffusion} as the pre-trained text-to-image diffusion model. Following the baselines, we employ Low-Rank Adaptation (LoRA)~\cite{lora} rather than fine-tuning the weights of pre-trained text-to-image model. As for scheduling exploration, we apply our exploration method only up to the three-fourths of the entire fine-tuning. We provide more implementation details in the supplemental material.


\begin{figure*}[!t]
  \centering
  \includegraphics[width=1\linewidth]{AAAI/images/reward_curve_unseen_v3.pdf}
  \caption{Reward curve for unseen prompts. At each checkpoint, we generate 10 images per unseen prompt and use the average of their reward scores. Our sampling method is employed only during fine-tuning, not for plotting this curve.}
  \label{fig:unseen_curve}
\end{figure*}


\subsection{Experimental Results}
\paragraph{Sample efficiency in reward fine-tuning.}
We begin by conducting experiments and comparing our method with DDPO and AlignProp across two reward functions: Aesthetic and PickScore. Figure~\ref{fig:seen_curve} presents the results, highlighting the achieved reward scores and sample efficiency. Notably, our method outperforms the baselines in terms of reward scores, while demonstrating better sample efficiency. Specifically, our approach required approximately 20\% fewer samples to achieve the same reward score as DDPO and AlignProp. 


\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{AAAI/images/imgrwd_score_v4.pdf}
  \caption{
  Comparison of an unseen ImageReward~\cite{imagereward} score after reward fine-tuning with and without our proposed \metabbr on our baselines, such as DDPO and AlignProp. The proposed method \metabbr achieves higher reward compared to all other baselines. 
  Note that models are optimized with either Aesthetic or PickScore as a reward function. 
  %Reward values measured using an unseen reward (ImageReward) after reward-tuning with or without \metabbr. As presented, \metabbr achieves higher reward value in all experiments.
  }
  \label{fig:imgreward_score}
\end{figure}

\paragraph{Qualitative comparison of image quality.}
We provide visual examples generated by each method in Figure~\ref{fig:sample_quality}. 
With the Aesthetic reward function, we observe that our method generates images with relatively high aesthetic quality, which aligns with the intent of reward function (see Figure ~\ref{fig:sample_quality} (a)). Notably, with the PickScore reward function, we observe that our method plays an important role in generating images with high image-text alignment (see Figure ~\ref{fig:sample_quality} (b)). For example, given a text prompt ``A butterfly playing chess,'' all baselines result in generating a human (not a butterfly) playing chess (see 1st column). We provide more diverse generated samples in the supplemental materials.

\paragraph{Quantitative comparison of image quality.}
To further quantitatively evaluate the sample quality, we use an ``unseen'' reward function called ImageReward~\cite{imagereward} (which is trained on a large-scale human preference dataset) that was not used during reward fine-tuning (i.e., not in the optimization objectives). In Figure~\ref{fig:imgreward_score}, we observe that ours clearly achieves higher ImageReward scores compared to baselines, such as DDPO and AlignProp fine-tuned with Aesthetic and PickScore reward functions. This demonstrates that our \metabbr indeed improves the overall image quality rather than merely increasing the reward through reward hacking~\cite{scaling_law_overopt, kim2024confidenceaware}.


\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{AAAI/images/drawbench_img_v3.pdf}
  \caption{Generated image samples with DrawBench prompts, which are known to be challenging for the current text-to-image models. Our method successfully generates images with high sample quality and image-text alignment, capturing contexts of the given prompts such as ``a magnifying glass'' and ``a man tripping over a cat''. 
  % Note that images in the same column are generated with the same random seed.
  }
  \label{fig:drawbench_img}
\end{figure}


\paragraph{Generalization to unseen prompts.}
We also investigate the model's capacity for generalization to new prompts, which were not used during reward fine-tuning. Following the standard evaluation protocol~\cite{ddpo, alignprop}, we consider a novel test set of animal names (for models fine-tuned with Aesthetic reward model) and activities (for models fine-tuned with PickScore reward model) that were not encountered during the training phase. A more detailed explanation of the experimental setup is provided in the supplemental material. As shown in Figure~\ref{fig:unseen_curve}, which shows reward curves to compare ours with baselines, a model with \metabbr consistently outperforms the baseline models in terms of rewards and sample efficiency, confirming its ability to generalize well to unseen prompts.  

\begin{figure*}[!t]
  \centering
  \includegraphics[width=1.0\linewidth]{AAAI/images/analysis_curve3.pdf}
  \caption{
  The results of our ablation study represent (a) the effect of each main module and (b) the impact of different hyper-parameter values of $t_{thres}$. 
  }
  \label{fig:analysis_curve}
\end{figure*}


\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{AAAI/images/application_curve_v3.pdf}
  \caption{Reward curves for our experiments with (a) DrawBench and (b) Stable Diffusion XL (SDXL). The former evaluates the ability to generate images with challenging prompts, while the latter evaluates the adaptability to the current SOTA model.}
  \label{fig:application_curve}
\end{figure}


%\subsection{Applications}
\paragraph{Experiment with challenging prompts.} 
Our proposed DiffExp provides an effective way to encourage the generation of diverse samples during reward fine-tuning. Thus, we argue that DiffExp is advantageous for learning more complicated prompts that are often challenging with conventional approaches, such as AlignProp. To evaluate this, we collect 58 challenging prompts from DrawBench~\cite{imagen}, which is well-known to be challenging for the current text-to-image models. We use AlignProp as our baseline model and fine-tune with an ensemble of HPS~\cite{hps}, Aesthetic, and PickScore reward models, which is reported as effective in previous studies~\cite{draft,prdp}, especially for complicated prompts. As shown in Figure~\ref{fig:drawbench_img}, the pre-trained model and AlignProp struggle to generate images aligned well with the given prompts. For example, such models are often overfitted to certain phrases (e.g., ``Batman''), ignoring other contexts (e.g., ``magnifying glass''). In contrast, our proposed DiffExp successfully generates samples with high image quality and high text fidelity (see the last column), confirming that our exploration method effectively improves reward fine-tuning with complicated (and challenging) prompts. We provide more diverse examples in supplemental material. We further provide reward curves for this experiment in Figure~\ref{fig:application_curve} (a), which is consistent with our previous analysis, demonstrating the effectiveness of our approach even to the challenging scenarios. 


\paragraph{Experiment with Stable Diffusion XL (SDXL).}
In order to verify the usefulness of \metabbr with state-of-the-art models, we fine-tune SDXL~\citep{sdxl} to maximize Aesthetic Score on animal prompts using \metabbr. %Specifically, we fine-tune SDXL with Aesthetic Score as the reward function, with AlignProp + \metabbr or AlignProp. 
As shown in Figure~\ref{fig:application_curve}, using AlignProp with \metabbr achieves a significantly higher maximum reward value as well as better sample efficiency compared to AlignProp without \metabbr. This trend is in accordance with our experiments using Stable Diffusion v1.5, which are presented in Figure \ref{fig:seen_curve}. We include the visual examples in the supplementary material.



\paragraph{Ablation studies.}
We conduct an ablation study to see the effect of each proposed module: (i) random prompt weighting and (ii) dynamic scheduling of the CFG scale. In Figure~\ref{fig:analysis_curve} (a), we provide reward curves with variants of our models with and without those modules. We observe that all our variants generally perform better than our baseline, AlignProp, in terms of rewards and sample efficiency, while using those modules together provides the best results. This trend is consistent with different reward models, such as Aesthetic and PickScore. Further, we experiment with different values of hyper-parameter, $t_{thres}$, which determines how long the CFG scale should be maintained to a low value. In Figure~\ref{fig:analysis_curve} (b), we provide reward curves for variants of our models with $t_{thres}=\{900, 800, 700\}$. Note that in the denoising process, we have Gaussian noise at $t=1000$ while the final image is at $t=0$. We observe that higher $t_{thres}$ generally provides better results in terms of rewards, with the best performance with $t_{thres}=900$, and lowering it degrades the image quality (consistent with our analysis in Figure~\ref{fig:cfg_scale_comparison}). 