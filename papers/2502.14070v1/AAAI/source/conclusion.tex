In this work, we propose \metabbr, an exploration method for promoting diverse reward signals during the reward optimization of text-to-image models. We demonstrate that adjusting the CFG scale of denoising process and randomly weighting certain phrase within prompts can serve as effective exploration strategy by improving the diversity of online sample generation. 
We conduct extensive experiments to verify that \metabbr improves both sample efficiency and generated image quality, and demonstrate this across various reward fine-tuning methods such as DDPO or AlignProp.
Furthermore, we conduct analysis using more advanced prompt sets such as DrawBench, and apply our method to the more advanced diffusion models such as SDXL, both of which result in significant performance improvements.