\section{Related Work}
\label{sec:rel_work}

In this section, we review the related work of \texttt{CRAG}, which includes both conversational recommender systems and research on large language models (LLM) with collaborative filtering.

\subsection{Conversational Recommender Systems}

Conversational recommender systems (CRS) aim to generate recommendations through natural language interactions with users \cite{jannach2021survey,gao2021advances}. Throughout the dialogue with users, there usually exist two types of information, i.e., \textit{item} and \textit{context}, where the latter denotes the non-item words that users utter alongside the items to express their preferences. To handle these two aspects, CRS models generally contain the following two phases: \textbf{\textit{(i)}} \textit{modeling}, which learns to understand both items and context mentioned in the dialogue, and \textbf{\textit{(ii)}} \textit{generation}, which generates items and words in natural language based on the dialogue understanding as the response. 

From the \textit{modeling} perspective, a typical CRS involves three key components: entity modeling, context modeling, and semantic fusion. Various traditional recommendation models, such as factorization machines \citep{rendle2010factorization} and denoising autoencoders \citep{vincent2008extracting}, have been used to model the items mentioned in the dialogues \citep{li2018towards, chen2019towards}. Context modeling, on the other hand, utilizes language models with recurrent neural networks (RNNs) \citep{chung2014empirical} or transformers \citep{vaswani2017attention, ham2020end} to capture the conversational flow and background information. To integrate item and context information, semantic fusion techniques such as mutual information maximization \citep{belghazi2018mutual} and cross-attention mechanisms \citep{vaswani2017attention} have been employed \citep{zhou2020improving, wang2022towards} to comprehensively understand the user preferences. In addition, knowledge databases, such as DBPedia \cite{auer2007dbpedia} and ConceptNet \cite{speer2017conceptnet}, have been used to enhance both item and context modeling with external information. 

For the \textit{generation} phase, early methods introduced a switch mechanism, i.e., a binary predictor, to decide whether the next token to be generated should be a word or an item \citep{li2018towards, chen2019towards}. Afterward, approaches such as copy mechanism \citep{gu2016incorporating} are used to align item and word tokens in the same generation space. Recently, \citet{wang2022towards} introduced an \texttt{<item>} token when generating the context, which enables the system to comprehensively consider the generated context for item recommendations. \textit{The advent of large language models (LLMs) has further blurred the boundaries between items and context, as well as between modeling and generation phases of CRSs}. LLMs possess extensive knowledge and reasoning abilities, allowing them to understand items and context simultaneously in the form of natural language. Moreover, the generation of items and context in the responses can be unified in the textual space, leveraging the LLM's capacity to produce coherent natural language outputs.

However, LLMs are comparatively less effective in the recommendation of more recent items due to fewer relevant documents in the training corpora. In addition, LLMs struggle to leverage collaborative filtering knowledge, which is highly informative for recommendations. These two challenges motivate us to introduce collaborative retrieval with two-step reflection in \texttt{CRAG} to augment the LLM's recommendations with context-aware CF knowledge. 

\subsection{LLM with Collaborative Filtering}

Recently, recommender system researchers have recognized the importance of integrating collaborative filtering (CF) with large language models (LLMs) to further enhance their recommendation abilities \citep{wu2024survey}. Most works focus on the \textit{white-box} LLMs, where the model weights are accessible to the researchers. One promising strategy is to introduce new tokens for users/items to capture the collaborative filtering knowledge. These tokens can be independently assigned for each user/item \citep{zhu2024collaborative,bao2023tallrec} or clustered based on semantic indexing \citep{hua2023index}. The embeddings associated with the user/item tokens can be learned with language modeling on natural language sequences converted from user-item interactions \citep{zhu2024collaborative} or predicted from pretrained CF models based on external neural networks \citep{kim2024large, zheng2024adapting}. While white-box LLMs provide the possibility to introduce CF knowledge through model finetuning, they are generally smaller in scale compared to large proprietary LLMs. Due to the inaccessibility of model weights, combining CF with \textit{black-box} LLMs is less explored. One strategy is to augment CF models with LLMs' analysis of user preferences \citep{ren2024representation,wei2024llmrec,xi2024towards}. To use the LLM itself as the recommender, \citet{wu2024coral} proposed to transform user-item interactions into the prompt for LLMs to understand the user preference and utilize a policy network to reduce the redundancy. However, these approaches focus on traditional symmetric CF settings, which are not suitable for CRS with asymmetric item mention/recommendation and complex contextual information.

\texttt{CRAG} distinguishes itself by effectively combining CF with black-box, state-of-the-art LLMs that comprehensively consider the interaction data and the context in the dialogue for recommendations. By introducing context-aware retrieval and a two-step reflection process, \texttt{CRAG} addresses the limitations of zero-shot LLM-based CRS and substantially enhances the recommendation quality.

\begin{figure}[t]
\centering
\includegraphics[width=0.42\textwidth]{images/attitude.pdf} 
\vspace{-3mm}    
\caption{Distribution of attitudes for movie mentions in user queries and system responses for the Reddit-v2 test set.} 
\vspace{-5mm}   
\label{fig:attitude}
\end{figure}