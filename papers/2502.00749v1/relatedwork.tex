\section{Related Work}
In this related work, we will first give an overview of the latest research around table tennis robots in~\cref{subsec:rl_ttr}.
%
Next, we cover event-based object detection in~\cref{subsec:rel_ebod} and event representations in~\cref{subsec:rel_er}.

\subsection{Table Tennis Robots}\label{subsec:rl_ttr}

% Copied from Jonas dissertation
Ever since Billingsley initiated a robot table tennis competition in 1983~\cite{Billingsley1983robot}, robotic table tennis has been a popular tool for research in computer vision and robot control.
%
A table tennis robot using frame-based cameras with a \ac{CNN}-based ball detection was presented in~\cite{GomezGonzalez2019robotics}.
In~\cite{Buchler2022tor}, the authors have designed a completely new pneumatic robot arm able to attain very high-end-effector speeds.
%
The authors in~\cite{Ji2021iros} introduced a table tennis robot system but focused on trajectory prediction and hitting velocity control.
%
Unfortunately, they do not explain their vision system in detail.

In~\cite{DAmbrosio2023rss}, Google DeepMind presented their table tennis robot system, including perception, planning, and robot control.
%
In a recent update~\cite{Ambrosio2024arxiv}, the same authors have shown that while not yet able to compete with professional players, table tennis robots can compete with amateurs and intermediate-level players.
%
Table tennis robots are also widely used as a use case for reinforcement learning, e.g., in~\cite{DAmbrosio2023rss}\cite{Ding2022iros}\cite{Yang2021ieee}\cite{Gao2020iros}.

All of the above table tennis robot systems use frame-based cameras for their perception system and do not make use of event-based cameras.
%
In this work, we developed a perception pipeline for a system similar to~\cite{Tebbe2019gcpr}, using only event-based cameras.
%
We show the advantages of event-based perception pipelines compared to a perception pipeline using conventional, frame-based cameras.

After the ball detection, the next step in a table tennis robot system is the prediction of the ball's trajectory.
%
Given the observation of the positions of the ball until the current time, the robot system needs to predict the ball's future trajectory to plan the hitting stroke of the robot.
%
Even for humans, this prediction is difficult and requires years to get a good estimation of balls with heavy spin.
%
This is in particular due to the difficulty of measuring spin~\cite{Tebbe2019gcpr}.

Next to the more traditional approaches of curve fitting and an \ac{EKF} in~\cite{Tebbe2019gcpr}, a GRU-based learning approach was used in~\cite{Gao2022ijcnn}.
%
In~\cite{Achterhold2023l4dc}, the authors use an \ac{EKF} and learn the parameters of the dynamics model outperforming two black-box approaches.
%
All these works state the importance of accurate spin and a low spin uncertainty.


% \subsection{Event-Based Cameras}\label{subsec:rw_event_cameras}
%
% Event-based cameras, also known as neuromorphic cameras or dynamic vision sensors, have gained considerable attention in computer vision and robotics due to their unique characteristics and advantages over traditional frame-based cameras~\cite{Lichtsteiner2008jssc}\cite{Gallego2020pami}.
% %
% Event-based cameras operate by detecting logarithmic changes in brightness asynchronously on a per-pixel basis, reporting events only when the brightness change exceeds a specified threshold.
%
% %The output of an event camera is an event stream of tuples of the form $<$$x, y, t, p$$>$ with the location of the event $(x, y)$, the time stamp $t$ and the polarity of the event $p$, indicating if the pixel got brighter or darker.
%
% The asynchronous event-based nature of event-based cameras enables high temporal resolution and low latency, making them particularly suitable for capturing fast-moving objects~\cite{Monforte2020aicas}\cite{Mitrokhin2018iros}\cite{Forrai2023icra} and scenes with a high dynamic range~\cite{Perot2020neurips}\cite{Stoffregen2020eccv}.


\subsection{Event-Based Object Detection}\label{subsec:rel_ebod}

Event-based object detection can be subdivided into model-based and learning-based methods.
%
Most learning-based methods make use of deep learning architectures to learn object detection tasks given a training dataset.
%
In~\cite{Perot2020neurips}, Prophesee presented the first event-based object detector that does not use an intermediate gray scale representation.
%
%The authors collected a big data set by mounting a camera rig with a GoPro and an event camera to a car and driving around.
%
Recurrent layers are used to take the temporal information of the event data into account.
%
An event-based convolutional layer (e-conv) and an event-based max pooling layer (e-max-pool) were introduced in~\cite{Cannici2019cvprw}.
%
Inference time was accelerated using a sparse update method in which only hidden and output neurons changed by incoming events were processed.
%
To make better use of the data structure of the events stream, Graph Neural Networks (GNNs) were used in~\cite{Schaefer2022cvpr}.
%
The incoming events iteratively update the graph rather than fully reconstructing it at each time-step, preserving the asynchronous nature of event-based cameras.
%
Also, more recent network architectures such as Vision Transformers (ViTs) were introduced for event data~\cite{Liu2024wacv}.
%
While learning-based methods tend to achieve higher accuracy, their inference time is often longer than the runtime of model-based methods.

An early model-based approach for object detection made use of the Hough transform to track objects with circular structure~\cite{Glover2016iros}.
%
In later work, the same authors implemented a particle filter with an observation heuristic, which can be adjusted depending on the object to track~\cite{Glover2017iros}.
%
In~\cite{Forrai2023icra} and~\cite{Falanga2020sr}, events are clustered using DBSCAN to differentiate between the background and independently moving objects with a motion-compensated mean time stamp image.
%
In our setup, the cameras are static, and therefore, motion compensation, which tends to be computationally heavy, is not needed.

Since a fast runtime is essential in our table tennis robot setup, we use a model-based approach with an event representation that allows \textit{event-by-event} updates.
%
Event-based cameras produce a lower-latency ball trajectory prediction as information is available in the system earlier than traditional cameras~\cite{Monforte2020aicas}. 


\subsection{Event Representation}\label{subsec:rel_er}

Depending on the algorithm, events are either consumed \textit{event-by-event} or in a \textit{batch-of-events}.
%
The minority of work does \textit{event-by-event} processing, mostly filter-based approaches, like~\cite{Scheerlinck2019accv}, and \acp{SNN}, like~\cite{Seifozzakerini2016bmcv}.
%
If events are processed in batches, there is a variety of choices on how to represent the information present in the event data.
%
For the sake of completeness, we cover the relevant ones here, where we take the definition from~\cite{Gallego2020pami}.

The simplest form is event packets, where events in a spatio-temporal neighborhood are processed together.
%
In event packets, the precise time stamp and polarity information is retained.
%
The next, almost obvious choice is the 2D histogram or event frame.
%
The events in a time window are converted in a simple way into an image/frame that can be fed to image-based computer vision algorithms.
%
The temporal information present in event data is used in the so-called time surface (TS).
%
In a time surface, a 2D map, each pixel stores a single time value.
%
Therefore, the ``intensity'' of the converted image is a function of the motion history at that location, with larger values corresponding to a more recent motion.
%
The concept of TS was further improved in~\cite{Lagorce2017pami} by introducing a hierarchical representation.
%
In~\cite{Sironi2018cvpr}, the authors present Histograms of Averaged Time Surfaces (HATS), which is less sensitive to noise and non-idealities of event-based cameras.

Instead of using hand-crafted event representations, the authors in~\cite{Gehrig2019iccv} introduced a combination of filters and convolutions, which can be learned in an end-to-end fashion when used in \acp{NN}.

In~\cite{Manderscheid2019cvpr}, the authors use a speed invariant time surface for learning to detect corner points with a simple random forest, showing the benefits of a speed invariant representation.
%
A more recent event representation is the threshold-ordinal surface (TOS), introduced in~\cite{Glover2022pami}.
%
TOS is designed to accumulate visual data, so it is compatible with conventional image processing algorithms like, e.g., circle or corner detection.
%
The representation provides a coherent and bound spatial representation of the asynchronous events, partially maintaining the information about their temporal order and attempting to capture the most up-to-date position of edges in the scene.
%
In this work, we make use of an improved version of TOS, the Exponential Reduced Ordinal Surface (EROS), used in~\cite{Glover2024icra}.
%
EROS can be used at any given point in time by downstream processing, with values between $0$ and $255$.