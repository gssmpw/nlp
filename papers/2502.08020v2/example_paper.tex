%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{array}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{ulem}
\usepackage{enumitem}
\usepackage{makecell}
\setlist[itemize]{noitemsep, nolistsep, leftmargin=*}
\setlist[enumerate]{noitemsep, nolistsep, leftmargin=*}

\newcommand{\eg}{\textit{e.g., }}
\newcommand{\ie}{\textit{i.e., }}
\newcommand{\n}{\textsc{CoSD}}

\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Collaborative Speculative Decoding (CoSD)}

\begin{document}

\twocolumn[
\icmltitle{Speculate, then Collaborate:\\ Fusing Knowledge of Language Models during Decoding}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Ziyao Wang}{UMD}
\icmlauthor{Muneeza Azmat}{IBM}
\icmlauthor{Ang Li}{UMD}
\icmlauthor{Raya Horesh}{IBM}
\icmlauthor{Mikhail Yurochkin}{IBM,IBM-MIT}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{UMD}{Department of Electrical and Computer Engineering, University of Maryland, College Park, College Park, MD, USA.}
\icmlaffiliation{IBM}{IBM Research, Yorktown Heights, NY, USA.}
\icmlaffiliation{IBM-MIT}{MIT-IBM Watson AI Lab, Cambridge, MA, USA}

\icmlcorrespondingauthor{Ziyao Wang}{ziyaow@umd.edu}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Language Models, Speculative Decoding, Knowledge Fusion}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{†Work started and partially done during Ziyao’s internship at IBM Research.}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Large Language Models (LLMs) often excel in specific domains but fall short in others due to the limitations of their training. Thus, enabling LLMs to solve problems collaboratively by integrating their complementary knowledge promises to improve their performance across domains. To realize this potential, we introduce a novel \textit{Collaborative Speculative Decoding (CoSD)} algorithm that enables efficient LLM knowledge fusion at test time without requiring additional model training. CoSD employs a draft model to generate initial sequences and an easy-to-learn rule or decision tree to decide when to invoke an assistant model to improve these drafts. CoSD not only enhances knowledge fusion but also improves inference efficiency, is transferable across domains and models, and offers greater explainability. Experimental results demonstrate that CoSD improves accuracy by up to 10\% across benchmarks compared to existing methods, providing a scalable and effective solution for LLM-based applications.

% Although large language models (LLMs) are often considered a step toward artificial general intelligence (AGI), yet they remain far from achieving it. LLMs are not all-knowing but rather have specialized domains of expertise shaped by their training process. Therefore, it is crucial to develop algorithms that enable LLMs to collaboratively solve problems by integrating their complementary knowledge. Existing approaches to this challenge either incur significant computational overhead or reply on overly simplistic techniques such as average decoding, resulting in suboptimal performance. To overcome these limitations, we introduce a novel \textit{Collaborative Speculative Decoding (CoSD)} algorithm that enables efficient LLM knowledge fusion at test time without requiring additional model training. CoSD employs a draft model to generate initial sequences and an assistant model to verify and improve these drafts. We propose efficient rule-based and decision-tree-based verification algorithms to combine the strengths of both models, resulting in improved outputs. CoSD not only enhances knowledge fusion but also improves inference efficiency, is transferable across domains, and offers greater explainability. Experimental results demonstrate that CoSD improves accuracy by up to 10\% across benchmarks compared to existing methods, providing a scalable and effective solution for LLM-based applications.
\end{abstract}

\section{Introduction}
State-of-the-art large language models (LLMs), such as GPT-4~\citep{achiam2023gpt} and Llama-3~\citep{dubey2024llama}, have demonstrated impressive capabilities in generating high-quality text across a variety of domains. These models are trained on vast datasets, allowing them to perform well on a wide range of tasks. However, despite their general effectiveness, no single LLM excels uniformly across all domains. Different models tend to have \textit{complementary knowledge}, with each model specializing in certain areas. For example, one model may be more proficient in technical writing, while another may outperform in creative tasks. This heterogeneity has led to an increasing interest in developing methods that can \textit{fuse the knowledge} of multiple LLMs, enabling users to harness their collective strengths for more robust and versatile applications. 

To address these challenges, recent research has shifted focus to \textit{test-time} knowledge fusion, which eliminates the need for retraining by combining model outputs during inference. This approach allows users to leverage the complementary knowledge of multiple LLMs without the overhead of additional training. For example, \citet{wang2023fusing} proposed a method that selects expert models dynamically at inference time using supervised learning, while \citet{ong2024routellm} introduced a router model that optimizes the selection of models based on performance and cost. Other approaches focus on integrating outputs through the decoding process, such as token-wise decoding~\citep{shen2024learning} and character-wise decoding~\citep{gu2024chared}, which combine outputs at a fine-grained level. Although these methods offer potential, they often struggle to balance strong knowledge integration with efficiency, which limits their practicality in real-world applications.

In response to these limitations, we propose \textit{Collaborative Speculative Decoding} {\n}, a novel algorithm designed to efficiently fuse the knowledge of multiple LLMs at inference time. {\n} builds upon recent developments in \textit{Speculative Decoding}~\citep{leviathan2023fast,xia2023speculative} to create an efficient system where multiple LLMs collaborate during the inference process. As shown in Figure \ref{fig_main}, {\n} consists of two models: a \textit{draft model} that generates an initial sequence of tokens and an \textit{assistant model} that verifies these tokens in parallel. When the assistant model predicts a token different from that of the draft model, a comparison of their token probabilities is used to determine whether to replace the draft token. This decision-making process can be guided by either a predefined rule set (Rule-Based \n) or a pre-trained decision tree (Tree-Based \n). The sequence is then regenerated and re-verified iteratively until accepting all tokens, ensuring both accuracy and computational efficiency.

{\n} presents several notable advantages over existing test-time fusion methods. First, by leveraging speculative decoding, {\n} improves inference efficiency, relying on token probabilities rather than more complex and resource-intensive representations like embeddings or hidden states. Second, {\n} demonstrates superior knowledge fusion due to the carefully designed decision-making process, which can be optimized for specific domains. Third, Rule-Based {\n} is highly transferable across tasks and model pairs; once the rules are established with optimal hyperparameters, they can be applied to a broad range of tasks. Similarly, the decision tree-based approach exhibits strong transferability, even when trained on domain-specific data. Finally, {\n} offers an interpretable framework, i.e., its use of human-readable rules or decision trees provides transparency, making it easier to evaluate, optimize, and understand compared to less transparent deep learning systems.

We validate the effectiveness of {\n} through extensive experiments on standard benchmarks and multiple model pairings. Our results show that {\n} not only significantly enhances the fusion of LLM knowledge but also improves efficiency and transferability across various domains. The key contributions of this work are as follows:
\begin{itemize}
    \item We introduce \n, a novel algorithm that enables efficient test-time fusion of LLM knowledge without requiring retraining.
    \item \n’s efficiency and transferability make it practical for a wide range of users, facilitating its implementation through both models and APIs.
    \item Our experimental results demonstrate that {\n} improves overall accuracy by up to 10\% across benchmarks, surpassing the state-of-the-art methods.
\end{itemize}

\section{Related Work}
\textbf{Language Model Fusion} from multiple LMs aims at enhancing the cross-domain performance of the resulting model and reducing bias. The primary efforts for such integration include model merging~\citep{goddard2024arcee}, such as model weight averaging~\citep{wortsman2022model} and linear mode connectivity~\citep{ainsworth2022git, ito2024analysis, wang2020federated}. Another series of works is called model stacking, which refers to concatenating models along the depth dimension. \citet{wu2024llama} and \citet{kim2023solar} stack the decoder blocks to expand the depth of Llama models. For large language models, some other research proposes knowledge fusion~\citep{wan2024knowledge}. They combine the capabilities of existing LLMs and transfer them into a single LLM. Another important trend of work called Mixture of Expert (MoE)~\citep{zhu2024llama, xue2024openmoe} builds sparse neural networks and only activates a subset of parameters (\ie experts) for each input. However, these methods either require the fused models to have the same structure or require fine-tuning after fusing to achieve the desired model performance. Towards mitigating these flaws, a new wave of works adopt decoding methods to fuse LMs. \citet{gu2024chared} propose a character-wise ensemble decoding method to fuse two LLMs' outputs. \citet{shen2024learning} and \citet{wang2023fusing} fuse model knowledge by training to choose between the generation of different LLMs. In our experiments, we consider several baselines from the latter group of works and observe gains in either efficiency or performance when using our method to merge cross-domain knowledge from different LMs when decoding.
% However, these methods still fail to find both effective and efficient decoding ways to merge cross-domain knowledge from different LMs.

\textbf{Speculative Decoding} is an efficient decoding paradigm for LM inference~\citep{xia2024unlocking,stern2018blockwise,xia2023speculative}. It accelerates the inference process by first generating draft tokens efficiently, and then using an LLM to verify draft tokens in parallel and correct them if needed~\citep{leviathan2023fast}, which avoids the autoregression process. In practice, the draft generator in speculative decoding could be a small LM~\citep{chen2023accelerating,miao2023specinfer,zhou2023distillspec}, a sub-model of an LLM~\citep{zhang2023draft,yang2023predictive,elhoushi2024layer}, or a text database retriever~\citep{he2023rest,li2024nearest}. The final generation of speculative decoding will be similar to the autoregressive generation of the target LLM, which is only acceptable when the target LLM has much better performance but is less efficient than the draft generator. No previous work focuses on using speculative decoding to approach the model fusion problem.

\begin{figure*}[t]
    \centering  \includegraphics[width=6.2in]{cosd_main.pdf}
    \vspace{-5mm}
    \caption{The workflow of collaborative speculative decoding.}
    \label{fig_main}
\end{figure*}

\section{Collaborative Speculative Decoding}
In our Collaborative Speculative Decoding system, our purpose is to fuse the predicted sequences of two LLMs efficiently. We define our problem as follows: given an input sequence $x_1, \dots, x_{t}$, {\n} uses a draft model $\mathcal{M}_p$ and an assistant model $\mathcal{M}_q$ to collaboratively generate an output sequence $x_{t+1}, \dots, x_{t+K}$ that integrates both models’ knowledge and expertise.

As Figure \ref{fig_main} illustrates, the process begins with the draft model $\mathcal{M}_p$ generating a draft sequence $\widetilde{x}_{t+1}, \dots, \widetilde{x}_{t+K}$ in an autoregressive manner. 
Subsequently, the assistant model $\mathcal{M}_q$ verifies the draft tokens by producing an assistant sequence $\hat{x}_{t+1}, \dots, \hat{x}_{t+K}$.
After both sequences are generated, {\n} iterate through the tokens and their corresponding probabilities to verify whether to accept a draft token $\widetilde{x}_{t+i}$ or replace it with the corresponding assistant token $\hat{x}_{t+i}$. 
Both rule-based or tree-based verification strategies, use token probabilities to determine whether a replacement is necessary. 
When a replacement occurs, all subsequent draft tokens are discarded, and a new draft sequence is generated starting from the replaced token. 
This process continues until the output reaches the maximum length or an $<EOS>$ token is generated. 
The full generation and verification process is elaborated in Algorithm \ref{alg_cosd} and described in following sections.

\subsection{Generation.} The generation process follows the principles of Speculative Decoding. First, the draft model $\mathcal{M}_p$ generates a sequence of tokens autoregressively:
\begin{equation}
\begin{aligned}
    \textbf{for}& \ i=1 \ \textbf{to} \ K \ \textbf{do} \\
    &\widetilde{x}_{t+i} \sim \mathcal{M}_p(x|{x}_1, \dots, \widetilde{x}_{t+i-1}),
\end{aligned}
\end{equation}

Here, $\widetilde{x}_{t+i}$ represents the token predicted by the draft model at position $i$, selected as the token with the highest probability. The sequence $\widetilde{x}_{t+1}, \dots, \widetilde{x}_{t+K}$ is generated autoregressively and produced sequentially.

After the draft sequence is generated, the assistant model $\mathcal{M}_q$ is used to verify these tokens. The assistant model generates tokens in parallel:
\begin{equation}
\begin{aligned}
i = & 1, \dots, K \ \textbf{in parallel do} \\
    &\hat{x}_{t+i} \sim \mathcal{M}_q(x|{x}_1, \dots, \widetilde{x}_{t+i-1}),
\end{aligned}
\end{equation}
Note that we already have all the draft tokens $\widetilde{x}_{t+1}, \dots, \widetilde{x}_{t+K}$ when we generate the assistant tokens. Thus, all the $\hat{x}_{t+i}$ in Eq. (2) can be generated in parallel. The process can also handle cases where the draft and assistant models use different tokenizers. In such cases, the draft sequence is first decoded by the draft model’s tokenizer and then encoded by the assistant model’s tokenizer:
\begin{equation}
\begin{aligned}
i = & 1, \dots, K \ \textbf{in parallel do} \\
    &{x}_1, \dots, \widetilde{x}_{t+i-1} \xrightarrow[T_{p}]{decode} \text{Texts} \xrightarrow[T_{q}]{encode} {x}_1^*, \dots, {x}_n^*, \\
    &\hat{x}_{t+i} \sim \mathcal{M}_q(x|{x}_1^*, \dots, {x}_n^*),
\end{aligned}
\end{equation}
where $T_p$ and $T_q$ are the tokenizers of the draft model and the assistant model respectively. The draft sequence is first decoded into texts by $T_p$ and then encoded by $T_q$ to fit the assistant model.

\begin{algorithm*}[tb]
\caption{Workflow of {\n}}
\label{alg_cosd}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithmic}[1] 
\REQUIRE Draft model $\mathcal{M}_p$, assistant model $\mathcal{M}_q$, input sequence $x_1, \dots, x_t$, predefined hyperparameters $\alpha, \beta$ and trained decision tree $\mathcal{T}$;
\ENSURE Output sequence $x_{t+1}, \dots, x_{t+K}$;

\vspace{0.3cm}
\renewcommand{\algorithmicrequire}
{ \textbf{Generation}}
\REQUIRE
\FOR{$i$ in $0,1,\dots, K$}
\STATE $\widetilde{x}_{t+i} \sim \mathcal{M}_p(x|{x}_1, \dots, \widetilde{x}_{t+i-1})$ \textbf{$\quad \#$Generate draft in an auto-regressive manner.}
\ENDFOR
\STATE Verify the draft in parallel:
\STATE $i=1, \dots, K$ \textbf{in parallel do}
\STATE $\quad \hat{x}_{t+i} \sim \mathcal{M}_q(x|{x}_1, \dots, \widetilde{x}_{t+i-1})$, \textbf{$\quad \#$Generate the assistant sequence in parallel.}

\STATE Send both $\widetilde{x}_1, \dots, \widetilde{x}_K$, $\hat{x}_1, \dots, \hat{x}_K$, and all related probabilities $\mathcal{M}_p(\widetilde{x}_i)$, $\mathcal{M}_q(\hat{x}_i)$ to verification.

\vspace{0.3cm}

\renewcommand{\algorithmicrequire}{\textbf{Verification}}
\REQUIRE
\FOR{$i$ in $0,1,\dots, K$}
\STATE \textbf{if} $\widetilde{x}_{t+i} \neq \hat{x}_{t+i}$ \textbf{and} $\mathcal{M}_p(\widetilde{x}_{t+i}) < \alpha$ \textbf{and}
$\mathcal{M}_q(\hat{x}_{t+i}) > \beta \cdot \mathcal{M}_p(\widetilde{x}_{t+i})$ \textbf{then $\quad \quad$  or}

\STATE \textbf{if} $\widetilde{x}_{t+i} \neq \hat{x}_{t+i}$ \textbf{and} $\mathcal{T}(\mathcal{M}_p(\widetilde{x}_{t+i}), \mathcal{M}_q(\hat{x}_{t+i})) = 1$ \textbf{then}

%\IF{$\mathcal{M}_p(\widetilde{x}_i) < \alpha$ \textbf{and} $\mathcal{M}_q(\hat{x}_i) > \beta \cdot \mathcal{M}_p(\widetilde{x}_i)$ }
\STATE $\quad x_{t+i} \leftarrow \hat{x}_{t+i}$
\STATE $\quad t \leftarrow t+i$
\STATE $\quad$\textbf{Exit loop, go to Generation}
%\ENDIF
\ENDFOR

\end{algorithmic} 
\end{algorithm*}

\subsection{Verification}
After the generation, we have a draft sequence $\widetilde{x}_{t+1}, \dots, \widetilde{x}_{t+K}$ and an assistant sequence $\hat{x}_{t+1}, \dots, \hat{x}_{t+K}$, along with the corresponding probabilities $\mathcal{M}_p(\widetilde{x}_{t+i})$ and $\mathcal{M}_q(\hat{x}_{t+i})$. We then use this information to verify whether to keep the draft token $\widetilde{x}_i$ or replace it with the assistant token $\hat{x}_i$ and thus ensemble the model knowledge. In order to make {\n} suitable for a wider range of tasks, we propose two strategies for verification. The first strategy, called Rule-Based Verification, applies clear rules to decide whether to select the draft token or the assistant token. The second strategy, i.e., Tree-Based Verification, involves training a decision tree to classify and select between the draft and assistant tokens.

\paragraph{Rule-Based Verification.} In Rule-Based Verification, the system applies simple yet general rules to determine whether the draft token $\widetilde{x}_{t+i}$ should be replaced by the assistant token $\hat{x}_{t+i}$. The intuition behind these rules is that if the draft model predicts a token with low confidence and the assistant model offers a higher-confidence alternative, the draft token should be replaced. The following rules define the verification process:
\begin{align}
    &\widetilde{x}_{t+i} \neq \hat{x}_{t+i},\\
    &\mathcal{M}_p(\widetilde{x}_{t+i}) < \alpha, \\
    &\mathcal{M}_q(\hat{x}_{t+i}) > \beta \cdot \mathcal{M}_p(\widetilde{x}_{t+i}),
\end{align}

These conditions check whether (1) the draft and assistant tokens differ, (2) the draft token has a probability below a threshold $\alpha$, and (3) the assistant token has a probability sufficiently higher than the draft token’s probability by a factor of $\beta$. If all conditions are met, the draft token is replaced with the assistant token.
% Eq. (4) implies that the draft model and the assistant model predict different tokens. Eq. (5) indicates that the draft token has a probability lower than a threshold $\alpha$. Eq. (6) implies that the assistant token has a relatively higher probability than the production of the draft probability and a coefficient $\beta$. 
% When Eq. (3), (4), and (5) are all satisfied, we replace $\widetilde{x}_{t+i}$ by $\hat{x}_{t+i}$. 

Intuitively, the Rule-Based Verification can be explained as follows: if the draft model is uncertain and the assistant model provides a better alternative, the system opts for the assistant’s prediction. If a replacement is made, the sequence is updated, and the draft model regenerates from that point onward.

% Intuitively, the Rule-Based Verification can be explained as follows: during the generation, if the draft model has low confidence in its predicted next token, and the assistant model predicts a different token with relatively high confidence, we replace the draft token with the assistant token.
% If this token replacement happens, as shown in Alg. \ref{alg_cosd}, we will update the accepted token number $t$ to $t+i$ and set the new $x_1, \dots, x_{t}$ sequence as the input sequence of the draft generation, and proceed the generation process.

\paragraph{Tree-Based Verification.} For domain-specific applications, Rule-Based Verification may not always be optimal. It is necessary to improve performance in specialized domains, such as healthcare~\citep{poonia2024ensemble}, smart home~\citep{amru2024network}, or math~\citep{mazraeh2024novel}. Therefore, we design the Tree-Based Verification method, which involves training a decision tree to decide when to replace a draft token with an assistant token. Training the decision tree on specific domain data allows for a more accurate assessment of knowledge fusion performance within those particular contexts. 
Specifically, our decision tree $\mathcal{T}$ takes two probabilities, $\mathcal{M}_p(\widetilde{x}_{t+i})$ and $\mathcal{M}_q(\hat{x}_{t+i})$, as inputs. The decision tree's output $\mathcal{T}(\mathcal{M}_p(\widetilde{x}_{t+i}), \mathcal{M}_q(\hat{x}_{t+i})) \in \{0, 1\}$ indicates whether to use the draft token ($y_i = 0$) or replace it with the assistant token ($y_i = 1$).

To train a decision tree suitable for specific domains, we first select a commonly used benchmark dataset $D$ for this domain (e.g., GSM8K~\citep{cobbe2021training} in math) with several input and ground-truth output pairs, \ie $x_1, \dots, x_t$ and $x_{t+1}, \dots, x_{t+K}$. 
We iterate through all the tokens in the ground-truth output in each pair. For the $i$-th token, we concatenate the input sequence and the first $i-1$ tokens of output sequences. Then, we feed the concatenated input $x_1, \dots, x_{t+i-1}$ into the two models separately to obtain the predicted next token $\widetilde{x}_{t+i}$, $\hat{x}_{t+i}$ and their corresponding probabilities $\mathcal{M}_p(\widetilde{x}_{t+i})$, $\mathcal{M}_q(\hat{x}_{t+i})$. This probability pair is one training sample of the decision tree. As for the related ground-truth label, we have three rules:
\begin{itemize}
    \item If $\widetilde{x}_{t+i} = x_{t+i}$, we assign the label $y_i = 0$ to encourage the decision tree to select the draft token.
    \item If $\widetilde{x}_{t+i} \neq x_{t+i}$ and $\hat{x}_{t+i}$ = $x_{t+i}$, we assign the label $y_i = 1$ to encourage the decision tree to select the assistant token.
    \item If neither $\widetilde{x}_{t+i}$ nor $\hat{x}_{t+i}$ match the target, we drop the sample and continue the loop with $i \leftarrow i+1$.
\end{itemize}

%Next, we compare the predicted words $\widetilde{x}_i$, $\hat{x}_i$ with the $i$-th token of the ground-truth output. If $\widetilde{x}_i$ matches the ground truth, the ground-truth label $y_i$ for the decision tree is 0. If $\widetilde{x}_i$ is incorrect but $\hat{x}_i$ matches, the label is 1. If neither matches, the sample is discarded. 
We iterate through all the input-output pairs and finally construct the training data sample in the form of $\{[\mathcal{M}_p(\widetilde{x}_i), \mathcal{M}_q(\hat{x}_i)], y_i\}$. In the training process, we aim to train the decision tree classifier $\mathcal{T}: \mathbb{R}^2 \to \{0, 1\}$ to minimize the difference between the predicted label and the ground truth:
\begin{equation}
\begin{split}
\min_\mathcal{T} \sum_{i=1}^{N} [ y_i \log(\mathcal{T}(\mathcal{M}_p(\widetilde{x}_i), \mathcal{M}_q(\hat{x}_i))) \\
+ (1 - y_i) \log(1 - \mathcal{T}(\mathcal{M}_p(\widetilde{x}_i), \mathcal{M}_q(\hat{x}_i))) ].
\end{split}
\end{equation}

%\begin{equation}
%\mathcal{F}(P({draft}), P(assistant)) \rightarrow \{0, 1\}
%\end{equation}
%\begin{equation}
%0 \rightarrow \textbf{Accept;} \quad 1 \rightarrow \textbf{Reject} \qquad \mathcal{F}(\cdot)
%\end{equation}


After training, our decision tree can predict whether to choose the draft token or the assistant token based on the two input probabilities. If the decision tree predicts $1$, the same as the rule-based verification, we replace the token, update the accepted token number, and send the new input sequence back to the generation. 
Since the decision tree is trained on a dataset specific to the corresponding domain, using this decision tree to fuse the model outputs can achieve better results in that domain. 

%where $\mathcal{L}$ is the loss function (e.g., binary cross-entropy) used to measure the dissimilarity between the decision tree’s output and the ground truth.

% The label assignment for training the decision tree is based on the performance of models A and B in relation to the ground truth target. For each instance in the dataset:
% \begin{itemize}
    % \item If model A's prediction matches the target, we assign the label $y_i = 0$ to encourage the decision tree to select model A.
    % \item If model A's prediction is incorrect but model B's prediction matches the target, we assign the label $y_i = 1$ to encourage the decision tree to select model B.
    % \item If neither model A nor model B matches the target, the decision tree may learn from the probabilistic output to make a more informed decision.
% \end{itemize}

% Thus, the training process involves providing the decision tree with truncated predictions from both models A and B. The goal is for the decision tree to learn when to rely on model A and when to switch to model B, based on their relative performance. Specifically, the decision boundary between choosing A or B depends on how closely each model's prediction aligns with the target output.

% \subsection{Training Objective}

% The objective of the decision tree training can be summarized as finding the decision rule $f$ that minimizes the error between the predicted decision (whether to use model A or B) and the optimal decision given the ground truth. Let $\hat{y}_i$ denote the predicted decision from the decision tree, and $y_i$ the true decision, then the overall objective can be written as:

% \[
% \min_f \sum_{i=1}^{N} \mathcal{L}(\hat{y}_i, y_i) = \min_f \sum_{i=1}^{N} \mathcal{L}(f(P_{A_i}, P_{B_i}), y_i).
% \]

% Through iterative training, the decision tree refines its ability to choose between model A and model B, optimizing for accuracy based on the input probabilities $P_A$ and $P_B$. This approach allows the model to handle classification decisions more effectively in scenarios where one model performs better than the other in certain regions of the feature space.
\begin{table*}[t]
 \caption{LLM pairs in the experiments.}
	\label{tab_scenario}
	\centering
	\footnotesize
        \resizebox{6.10in}{!}{
	\begin{tabular}{c|ccc}
		\toprule
		%&\multicolumn{10}{c}{MNIST}   \\
\textbf{Methods} &{\textbf{Draft Model}}&\textbf{Assist. Model} &\textbf{Simulated Scenario} \\
        \midrule
       \textbf{Pair 1}&Mistral 7B DARE&Mistral 7B Mixed&Complementary Knowledge Fusion\\
       \textbf{Pair 2}&Llama 3 Wissenschaft 8B &Llama 3 Bophades 8B&Complementary Knowledge Fusion\\
       \textbf{Pair 3}&Mistral 7B~\citep{jiang2023mistral}&Mistral Math 7B&Catastrophic Forgetting Recovery\\
       \textbf{Pair 4}&TinyLlama~\citep{zhang2024tinyllama}&Llama 2 Chat&Capacity Imbalance\\
       \textbf{Pair 5}&Llama 2 Chat~\citep{touvron2023llama}&WizardMath~\citep{luo2023wizardmath}&Different Tokenizers\\
       \textbf{Pair 6}&Llama 2 Chat&DeepSeek Coder~\citep{guo2024deepseek}&Different Tokenizers\\
	\bottomrule
	\end{tabular}}
\end{table*}

\section{Experiment}
\subsection{Experimental Settings}
\paragraph{Scenarios, Models, and Benchmarks.} We evaluate {\n} and compare it against several baselines in scenarios that reflect common use cases where users may seek to fuse the knowledge of multiple LLMs. These scenarios include: (i) \textbf{Complementary Knowledge Fusion:} The fused LLMs have complementary knowledge, and users hope that the knowledge fusion system can perform as well as the best model for each task across all tasks; (ii) \textbf{Catastrophic Forgetting Recovery:} The fused models are one base model and a model fine-tuned from the base model. Fine-tuning improves performance in certain domains but reduces the performance in other domains due to catastrophic forgetting. Users expect to heal the catastrophic forgetting by fusing the knowledge of the two LLMs; (iii) \textbf{Capacity Imbalance:}  Users use a small draft model and adopt an API of the assistant model with a much larger capacity. The fusion system is expected to perform similarly to the assistant model; (iv) \textbf{Different Tokenizers:} Fuses the LLMs with different tokenizers. To simulate these scenarios, we carefully selected six pairs of LLMs from the HuggingFace repository~\citep{jain2022hugging}, representing each of the four use cases outlined above. Table \ref{tab_scenario} lists the model pairs and the corresponding simulated scenarios.

For all the scenarios and model pairs, we use MMLU \citep{hendrycks2020measuring}, GSM8K~\citep{cobbe2021training}, HumanEval~\citep{chen2021evaluating}, Hellaswag~\citep{zellers2019hellaswag}, and TruthfulQA~\cite{lin2021truthfulqa} as the evaluation benchmark. We use tinyBenchmarks~\citep{polo2024tinybenchmarks} for all the benchmarks except HumanEval to further increase the efficiency of experiments. These benchmarks test general question-answering, mathematical reasoning, and coding capabilities, providing a comprehensive assessment of the models’ abilities across different domains. By using these benchmarks, we can evaluate the effectiveness of {\n} and the baselines in fusing complementary knowledge across diverse tasks and model configurations.

\paragraph{Baselines.}
We use tree baselines in the experiment.
\textbf{Speculative Decoding:} It also uses a draft model and an assistant model to generate the output. However, it adopts a different verification algorithm that replaces the draft token when $\frac{\mathcal{M}_p(\widetilde{x}_i)}{\mathcal{M}_q(\hat{x}_i)} < U(0,1)$.
\textbf{Average Decoding:} It averages the predicted probabilities of the draft model and the assistant model and chooses the final output from the averaged probabilities.
\textbf{Co-LLM~\citep{shen2024learning}:} It trains a single layer to classify the hidden state of a base model. The output probability of the layer decides to use the base model generation or evoke an assistant model to help generation.

\paragraph{Hyperparameters.} We run {\n} with the following settings. For Rule-Based {\n}, we set $\alpha = 0.5$ and $\beta = 0.5$, which were determined to be the optimal and most transferable parameters based on our analysis in Figure \ref{fig_alpha}. For Tree-Based {\n}, we randomly select three samples from the AlpacaEval dataset to train the decision tree. It is important to note that we use MMLU, GSM8K, and HumanEval as our benchmarks. Consequently, the training data for the decision tree do not overlap with the test data, creating a more realistic scenario to evaluate the decision tree’s transferability across different tasks and domains.

\begin{table*}[t] 
\caption{The results of fusing LLMs with complementary knowledge and the same tokenizer. Pair 1 and pair 2 are complementary knowledge fusion results. Pair 3 simulates a catastrophic forgetting healing scenario, and pair 4 is a disparate capacity LLM fusion result.}
	\label{tab_e1}
	\centering
	\footnotesize
        \resizebox{6.2in}{!}{
	\begin{tabular}{c|c|cc|ccccc}
		\toprule
		%&\multicolumn{10}{c}{MNIST}   \\
    \multirow{2}{*}{\textbf{Models}}&\multirow{2}{*}{\textbf{Benchmarks}} &\multirow{2}{*}{\textbf{Draft}}&\multirow{2}{*}{\textbf{Assist.}}&\textbf{Spec.}&\textbf{Avg.}&\multirow{2}{*}{\textbf{Co-LLM}}&\multirow{2}{*}{\textbf{CoSD-Rule}}&\multirow{2}{*}{\textbf{CoSD-Tree}}\\
&&&&\textbf{Decoding}&\textbf{Decoding}&&&\\
       \midrule
        \multirow{6}{*}{\textbf{Pair 1}}&\textbf{MMLU}&65.82&59.26&59.33&62.22&60.40&\textbf{65.06}&63.71\\
       &\textbf{GSM8K}&31.20&42.19&33.36&38.33&\textbf{38.85}&36.81&37.24\\
       &\textbf{HumanEval}&28.66&31.10&14.02&25.60&29.91&\textbf{31.34}&28.29 \\
       &\textbf{Hellaswag}&84.35&79.71&80.25&81.44&82.58&\textbf{85.19}&83.17 \\
       &\textbf{TruthfulQA}&43.92&40.62&\textbf{44.18}&40.52&42.78&43.65&43.77\\

       \cmidrule(r){2-9}&\textbf{AVG.}&50.79&50.58&46.23&49.62&50.90&\textbf{52.41}&51.24\\
       \midrule
        \multirow{6}{*}{\textbf{Pair 2}}&\textbf{MMLU} &54.81&52.02&53.20&52.31&55.25&56.97&\textbf{58.37}\\
       &\textbf{GSM8K}&39.79&51.02&43.85&43.89&41.04&\textbf{45.72}&41.89\\
       &\textbf{HumanEval}&21.34&43.90&39.02&38.41&37.25&\textbf{39.10}&36.22\\
       &\textbf{Hellaswag}&86.41&82.36&82.52&86.39&85.64&\textbf{86.96}&86.84 \\
       &\textbf{TruthfulQA}&56.90&49.54&51.29&54.45&55.07&55.09&\textbf{55.40}\\
       \cmidrule(r){2-9}&\textbf{AVG.}&47.85&55.77&53.98&55.09&54.85&\textbf{56.77}&55.74\\
       \midrule
        \multirow{6}{*}{\textbf{Pair 3}}&\textbf{MMLU}&61.45&46.59&43.39&56.60&58.78&62.41&\textbf{63.87}\\
       &\textbf{GSM8K}&25.01&35.43&33.10&36.61&37.15&\textbf{45.47}&33.85\\
       &\textbf{HumanEval}&27.44&9.76&10.97&18.90&21.88&\textbf{25.61}&23.17\\
       &\textbf{Hellaswag}&82.71&73.86&80.67&75.73&79.03&\textbf{82.81}&81.95 \\
       &\textbf{TruthfulQA}&35.45&48.50&36.23&41.88&44.60&44.49&\textbf{46.01} \\
       \cmidrule(r){2-9}&\textbf{AVG.}&46.37&42.83&40.87&45.94&48.29&\textbf{52.16}&49.77\\
       \midrule
       \multirow{6}{*}{\textbf{Pair 4}}&\textbf{MMLU}&32.13&47.65&47.30&42.62&47.47&47.84&\textbf{48.15}\\
       &\textbf{GSM8K}&3.36&15.63&\textbf{14.63}&12.12&11.97&12.52&12.29\\
       &\textbf{HumanEval}&8.53&12.20&10.39&12.55&11.73&\textbf{12.80}&10.54\\
       &\textbf{Hellaswag}&60.76&77.95&75.94&68.77&73.19&73.23&\textbf{74.08}\\
       &\textbf{TruthfulQA}&30.47&32.09&29.98&32.35&30.64&32.17&\textbf{32.58} \\
       \cmidrule(r){2-9}&\textbf{AVG.}&27.05&37.10&35.65&33.68&35.00&\textbf{35.71}&35.53\\
		\bottomrule
 	\end{tabular}}
    \vspace{-0.3cm}
\end{table*}

\begin{table*}[t]
\caption{Fusing LLMs with different tokenizers.}
	\label{tab_tokenizer}
	\centering
	\footnotesize
        \resizebox{4.8in}{!}{
	\begin{tabular}{c|c|cc|ccc}
		\toprule
		%&\multicolumn{10}{c}{MNIST}   \\
\textbf{Models} &{\textbf{Benchmarks}}&\textbf{Draft}&\textbf{Assist.}&\textbf{Char-ED} &\textbf{CoSD-Rule}&\textbf{CoSD-Tree}\\
        \midrule
        \multirow{2}{*}{\textbf{Pair 5}}&\textbf{MMLU}&47.65&40.61&44.29&50.65&\textbf{52.13}\\
       &\textbf{GSM8K}&15.63&51.13&37.54&\textbf{44.88}&37.01\\
       \midrule
       \multirow{2}{*}{\textbf{Pair 6}}&\textbf{MMLU}&47.65&59.63&52.51&\textbf{57.33}&55.20\\
&\textbf{HumanEval}&8.53&73.17&59.04&\textbf{59.88}&51.42\\
		\bottomrule
	\end{tabular}}
\end{table*}
\subsection{Experimental Results}

\paragraph{Fusing LLMs with Complementary Domain Knowledge.}
We first evaluated the performance of different methods for fusing LLMs with complementary knowledge, with results shown in the pair 1 and pair 2 columns of Table \ref{tab_e1}.
Both CoSD-Rule and CoSD-Tree consistently outperformed the baseline methods in terms of averaging performance. For instance, in pair 1, CoSD-Rule and CoSD-Tree achieved averaged scores of 52.41 and 51.24 on, surpassing all the baselines and both the draft model and the assistant model. Besides, CoSD-Rule also achieves the best performance on MMLU, HumanEval, and Hellaswag. In pair 2, {\n} matches the performance of the better model for each task across all tasks. For example, {\n} achieves a similar MMLU performance to the draft model and a similar performance on GSM8K and HumanEval to the assistant model in pair 2. Compared with {\n}, Speculative Decoding only performs similarly to the assistant model, thus will be more suitable to the scenario when the assistant model is much stronger than the draft model. Average Decoding fuses model knowledge but only achieves an average accuracy across tasks, unlike CoSD, which integrates the strengths of different LLMs. Co-LLM's performance is the closest to {\n}, but since it requires training on specific datasets, its transferability across different datasets is inferior to {\n}.

These results highlight the effectiveness of {\n}, particularly the superior fusion capabilities across multiple benchmarks and model pairs. With the support of experimental results, we state that if users have multiple language models with different specialties but are unsure when to use each model, {\n} always provides better overall performance.

%It is also interesting to see that CoSD-Rule outperforms CoSD-Tree in the averaged scores. We attribute this phenomenon to the fact that the rules exhibit greater generalizability compared to the decision tree. Since our decision tree is trained on AlpacaEval, it performs better on some general QA tasks (\eg MMLU), but does not have an advantage in math (\eg GSM8K) and coding (\eg HumanEval). 
%CoSD-Rule is relatively general and performs well across three domains; however, it is not as effective as the decision tree on MMLU (\eg 56.97 for CoSD-Rule and 58.37 for CoSD-Tree in pair 1).

\paragraph{Catastrophic Forgetting Recovery.} We select a Mistral base model and a fine-tuned math Mistral model for pair 3 in Table \ref{tab_e1} to simulate the catastrophic forgetting recovery. We found that CoSD-Rule performs particularly well on this type of task. It not only recovers from forgetting across all benchmarks but also outperforms both the draft and assistant models on MMLU, GSM8K, and Hellaswag. These results suggest that {\n} can further enhance the original performance of both models by enabling collaboration between them.

\paragraph{Fusing LLMs with disparate capacity.}
When the assistant model has a much larger capacity than the draft model, the model fusion system is supposed to achieve a similar performance to the draft model. Speculative Decoding is more suited for this task because its verification strategy tends to replace more draft tokens with assistant tokens. However, {\n} results in pair 4, Table \ref{tab_e1} are still comparable to Speculative Decoding. For instance, CoSD-Rule has higher HumanEval scores than Speculative Decoding, while CoSD-Tree has higher MMLU, Hellaswag, and TruthfulQA scores than Speculative Decoding.
These results on LLMs with disparate capacities indicate that {\n} is not only applicable to complementary knowledge LLM fusion but also to efficient inference tasks. When the draft model is smaller and the assistant model is larger, our {\n} can achieve performance similar to the assistant model. At the same time, since the assistant model only performs parallel verification, {\n} still has more efficient inference compared to using the assistant model alone.

\paragraph{Fusing LLMs with Different Tokenizers.} Although {\n} needs to decode and then encode the sequences during the verification when the models have different tokenizers, which sacrifices some efficiency, it can still effectively fuse the model knowledge. In the experiments, we fuse a Llama 2 Chat and a WizardMath to evaluate the {\n} performance on MMLU and GSM8K. We fuse a Llama 2 Chat and a Deepseek Coder to evaluate {\n} on MMLU and HumanEval. Results are shown in Table \ref{tab_tokenizer}. {\n} outperforms the character-wise averaging method CharED~\citep{gu2024chared} in both model pairs and benchmarks. We do not include other baselines since they are not applicable to the different tokenizer settings.

\paragraph{Ablation Studies.}
We have several tunable hyperparameters in {\n}. In Rule-Based {\n}, we have $\alpha$ and $\beta$ that determine the rules to replace the draft tokens. In Tree-Based {\n}, the training data and hyperparameters influence the performance of the decision tree. Thus, we use ablation experiments to identify the impact of these hyperparameters on the final model performance, allowing us to determine the optimal and transferable hyperparameter settings.
Figure \ref{fig_alpha} shows the relationship between $\alpha$, $\beta$ values in Rule-Based {\n} and model performance. The x-axis represents the values of $\alpha$, and the y-axis represents the values of $\beta$. The numbers in the small squares represent the sum score of MMLU and GSM8K, which reflect the overall model performance of {\n}. We can see that with $\alpha=0.5, 0.75$ and $\beta=0.5, 0.75$, Rule-Based {\n} perform consistently well in the two model pairs. We ultimately selected $\alpha=0.5, \beta=0.5$ as the general hyperparameters in our experiments. We believe this setting effectively integrates the knowledge of the models.

\begin{figure}[h] 
\includegraphics[width=0.47\textwidth]{alpha.pdf}
    \vspace{-0.5cm}
    \caption{The sum score of MMLU and GSM8K with various $\alpha$, $\beta$ settings on pair 1 (left figure) and pair 2 (right figure).}
    %\luyang{What is the reason not putting this on the right side?}}
    \label{fig_alpha}
\end{figure}

Table \ref{tab_tree} displays the impact of the tree training dataset on Tree-Based {\n}. The decision tree trained on different datasets performs relatively consistently, even when the training set is not in the same distribution with any benchmark (e.g., AlpacaEval achieves good results across all three benchmarks). When the decision tree’s training set shares the same distribution as a particular benchmark, CoSD-Tree tends to perform slightly better on that benchmark. Therefore, if users are aware of the model's application scenario, they can use the corresponding benchmark from that task to train the decision tree. This would result in a domain-adapted tree that is better suited to the specific task. In addition, as mentioned in the table title, we use very few samples to train the decision tree, thus training decision trees introduces almost no additional computational overhead.

\begin{table*}[t]
    \centering
    \caption{An example of how {\n} polish the draft generation in GSM8K dataset. The table shows the outputs for a GSM8K question generated by CoSD-Rule and CoSD-Tree. Output tokens that are not highlighted represent accepted draft tokens, while tokens marked in pink are rejected draft tokens, followed by the assistant tokens that replace the rejected ones highlighted in green.}
    \begin{tabular}{@{}p{1.8cm}p{13cm}@{}}
        \toprule
        \textbf{\makecell[c]{Model}} & \textbf{\makecell[c]{Example}} \\ 
        \midrule
        \textbf{\makecell[c]{Question}} & \makecell[l]{Rory orders 2 subs for \$7.50 each, 2 bags of chips for \$1.50 each and 2 cookies for \$1.00 \\each for delivery.  There’s a 20\% delivery fee added at check out and she wants to add a \\ \$5.00 tip.  What will her delivery order cost? \textcolor{red}{Correct answer: 29}}\\
        \midrule
        \textbf{\makecell[c]{CoSD\\Rule}} & \makecell[l]{The cost of the subs is 2 \colorbox{pink}{\sout{*}}\colorbox{green}{x} \$7.50 = $\ll$2*7.50=15$\gg$15 \\
        The cost of the bags of chips is 2 x \$1.50 = $\ll$2*1.50=3$\gg$3 \\
        \colorbox{pink}{\sout{We}}\colorbox{green}{The} \colorbox{pink}{\sout{delivery}}\colorbox{green}{total} \colorbox{pink}{\sout{final}} \colorbox{green}{sub} \colorbox{pink}{\sout{is}}\colorbox{green}{before} \colorbox{pink}{\sout{tax}}\colorbox{green}{the} \colorbox{pink}{\sout{tax}}\colorbox{green}{delivery} fee is \\
        \colorbox{pink}{\sout{15}}\colorbox{green}{\$} 15 + \$3 + \$2 = \$$\ll$15+3+2=20$\gg$20 \\ The delivery fee is 20\% of \$20 \colorbox{pink}{\sout{=}}\colorbox{green}{,} so the delivery fee is \$20 x 0.20 = $\ll$20*0.20=4$\gg$4 \\ The total \colorbox{pink}{\sout{cost}}\colorbox{green}{before} the tip is \$20 + \$4 = \$$\ll$20+4=24$\gg$24 The tip is \$5.00 \\
        The total \colorbox{pink}{\sout{of}}\colorbox{green}{delivery} \colorbox{pink}{\sout{is}}\colorbox{green}{order} \colorbox{pink}{\sout{will}}\colorbox{green}{cost} is \$24 + \$5 = \$$\ll$24+5=29$\gg$29}\\
        \midrule
        \textbf{\makecell[c]{CoSD \\ Tree}} & \makecell[l]{The cost of the subs is 2 \colorbox{pink}{\sout{*}}\colorbox{green}{x} \$7.50 = $\ll$2*7.5=15$\gg$15 \\
        The cost of the \colorbox{pink}{\sout{bags}}\colorbox{green}{chips} is 2 x \$1.50 = $\ll$2*1.5=3$\gg$3 \\
        The cost of the cookies: 2 x \$1.00 = $\ll$2*1=2$\gg$2 \\
        The \colorbox{pink}{\sout{cost}}\colorbox{green}{subtotal} \colorbox{pink}{\sout{of}}\colorbox{green}{before} the delivery fee: 15 + 3 + 2 = $\ll$15+3+2=20$\gg$20 \\
        The 20\% delivery fee: 20\% of 20 = $\ll$20 \colorbox{pink}{\sout{\%}}\colorbox{green}{*} \colorbox{pink}{\sout{2}}\colorbox{green}{.}2=4$\gg$4 \\
        The total cost of the order before the tip: 20 + 4 = $\ll$20+4=24$\gg$24 \\
        The total cost of the order \colorbox{pink}{\sout{is}}\colorbox{green}{by} adding \colorbox{pink}{\sout{all}}\colorbox{green}{the} tip: 24 + 5 = $\ll$24+5=29$\gg$29}  \\
        \bottomrule
    \end{tabular}
    \label{tab:casestudy}
    \vspace{-0.5cm}
\end{table*}

\begin{table}[t]
\caption{Training the decision tree with different datasets in pair 3. Each column represents a decision tree trained by the dataset in the column header. We use 10 samples of MMLU, 3 samples of each other datasets to train the decision tree.}
	\label{tab_tree}
	\centering
	\footnotesize
        \resizebox{3.2in}{!}{
	\begin{tabular}{c|cccc}
		\toprule
		%&\multicolumn{10}{c}{MNIST}   \\
\textbf{Benchmarks} &{\textbf{MMLU}}&\textbf{GSM8K} &\textbf{HumanEv.}&\textbf{AlpacaEv.}\\
        \midrule
        \textbf{MMLU}&\textbf{63.94}&60.88&61.23&63.87\\
       \textbf{GSM8K}&35.04&\textbf{37.17}&30.08&33.85\\
       \textbf{HumanEv.}&\textbf{25.62}&23.04&23.09&23.17\\
		\bottomrule
	\end{tabular}}
    \vspace{-0.1cm}
\end{table}

\begin{table}[t]
\caption{Efficiency of LLM Knowledge Fusion. Token latency represents the averaged time to generate one token, and acceptance rate refers to the proportion of draft tokens that are not replaced. Typically, the higher the latter, the lower the former, as fewer tokens require regeneration. Experiments are done by pair 3.}
	\label{tab_efficiency}
	\centering
	\footnotesize
        \resizebox{2.6in}{!}{
	\begin{tabular}{c|cc}
		\toprule
		%&\multicolumn{10}{c}{MNIST}   \\
\multirow{2}{*}{\textbf{Methods}} &{\textbf{Token}}&\textbf{Acceptance} \\
&{\textbf{Latency (ms)}}&\textbf{Rate} \\
        \midrule
        \textbf{Spec. Decoding}&131.22&0.89\\
       \textbf{CoSD-Rule}&132.31&0.81\\
       \textbf{CoSD-Tree}&135.82&0.77\\
		\bottomrule
	\end{tabular}}
\end{table}

\paragraph{Case Studies.}
We use an example in GSM8K to demonstrate how CoSD effectively combines the knowledge of two models in Table \ref{tab:casestudy}. CoSD replaces the red tokens generated by the draft model with the green tokens from the assistant model. We display the answer from the single draft model and the assistant model in Table \ref{tab:casestudyapp} in the appendix. Neither the draft model nor the assistant generates the correct result when used alone. The main issue with the draft model is its weak mathematical calculation ability (\eg in the fourth line, it calculates the tax as 20\% of 20 to be 10, instead of the correct answer 4). On the other hand, the assistant model performs well in terms of mathematical calculations but lacks the logical rigor of the draft model. It fails to compute the subtotal without the tip first, leading to the incorrect final calculation. 
{\n} effectively integrates the strengths of both models. For instance, in CoSD-Rule, in the fifth line, the assistant model rejects the draft model’s incorrect computation of 20\% of 20 = 10 and instead uses the correct calculation of 20 * 0.2 = 4, successfully avoiding the error in the draft model’s tax calculation. In the sixth line, the draft model correctly leads to generate the subtotal of \$24, so in the final step, CoSD-Rule computes the simpler 24 + 5 instead of the more complicated 15 + 3 + 2 + 5, resulting in the correct answer.

Also, there are situations that {\n} makes wrong decisions. As shown in Table \ref{tab:casesmmlu} in Appendix \ref{sec:mmlu}, {\n} does not always select the correct answer. In the above example, the draft model made the correct choice with high confidence, so the final generation retained the correct answer. However, in the example below, while the draft model also made the correct choice, the assistant model provided an incorrect answer with higher confidence, leading to the final output being changed to the wrong answer. This demonstrates that using confidence as the criterion does not guarantee selecting the correct option but can only aim to choose the correct answer with a higher probability.

\paragraph{Efficiency.} Since we perform fusion during the inference, efficiency is a major advantage of our approach. We compared the time overhead of {\n} with the baselines. We use token latency and acceptance rate as the metrics for efficiency. As displayed in Table \ref{tab_efficiency}, Speculative Decoding has the lowest latency among all methods, since it makes the least token replacement. However, although {\n} methods replace a few more tokens, the increase in total latency is almost negligible. Considering that {\n} has the best knowledge fusion performance, we have achieved a better balance between efficiency and effectiveness.

\section{Conclusion}
In this paper, we fuse the LLM knowledge in a simple yet effective way. Our algorithm {\n} takes the probabilities of predicted tokens from two LLMs as the feature to verify whether to keep the draft token or adopt the assistant token. The verification strategy can be either a rule-based or a pre-trained decision tree. Our extensive experiments show that {\n} performs better than the state-of-the-art methods across 6 LLM pairs and 5 benchmarks. Compared to previous works, {\n} has superior knowledge fusion ability, a broader range of application scenarios, and comparable efficiency. It works well in scenarios including complementary knowledge fusion, catastrophic forgetting recovery, knowledge fusion with disparate model capacity, and knowledge fusion with different tokenizers. {\n} makes it possible for ordinary users to fuse the LLM knowledge with only the API queries, without any training or fine-tuning of LLMs, or requirements of white-box LLM information such as hidden states. It provides users with better tools to manipulate LLMs in wider application scenarios.


\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Limitations}
While {\n} demonstrates strong performance across various scenarios, it is important to acknowledge its limitations. This section highlights cases where {\n} may not be applicable and tasks that it fails to address. Identifying these constraints provides clarity on its scope of use and helps guide future improvements. Below, we outline two specific limitations:

(1) When the two collaborating models are of similar size and one significantly outperforms the other, {\n} offers no advantage over using only the better model. In this case, using the better model only is sufficient. This also requires the user to have prior knowledge of the performance of the two models on different benchmarks and to determine that one model is significantly better than the other. If the user is uncertain, we still recommend using {\n} to ensure the best results.

(2) Another limitation of {\n} is that it cannot guarantee the replaced assistant token is always better than the discarded draft one. It relies on the confidence scores of the models, which are not always perfectly aligned with token quality. The algorithm selects the output of the more confident model, aiming to maximize the likelihood of choosing a better token, but this approach may occasionally lead to suboptimal results.

\section{Additional Experiments and Discussion}

\textbf{The Average Iterations in {\n}.} The number of iterations required during collaborative decoding depends on the maximum length of the model output. Table~\ref{tab:iterations} reports the average number of iterations on the GSM8K dataset for different maximum lengths.

\begin{table}[h!]
    \centering
    \begin{tabular}{c|ccc}
        \toprule
        \textbf{Max Length} & \textbf{CoSD-Rule} & \textbf{CoSD-Tree} & \textbf{Spec. Dec.} \\
        \midrule
        \textbf{128} & 11.41 & 13.58 & 9.77 \\
        \textbf{256} & 15.29 & 16.01 & 14.20 \\
        \textbf{512} & 21.23 & 21.95 & 18.51 \\
        \bottomrule
    \end{tabular}
    \caption{Average number of iterations for different maximum output lengths.}
    \label{tab:iterations}
\end{table}

Although the number of iterations scales with the output length, it does not directly imply a proportional increase in generation time. As the number of accepted tokens grows, the number of tokens requiring regeneration decreases significantly. For instance, with a maximum output length of 128, the average number of iterations is 11, but the total generated output length remains around 300 tokens. This highlights the efficiency of our approach in reducing redundant generation.

\textbf{Collaborate with More LLMs.} Our {\n} also supports multiple collaborating models. Table~\ref{tab:decision_tree} presents the results when three models are used for collaboration:

\begin{table}[h!]
    \centering
    \begin{tabular}{c|ccccc}
        \toprule
        \textbf{Dataset} & \textbf{Draft} & \textbf{Assist. 1} & \textbf{Assist. 2} & \textbf{CoSD-Rule} & \textbf{CoSD-Tree} \\
        \midrule
        \textbf{MMLU} & 32.13 & 47.65 & 35.62 & 44.14 & 46.48 \\
        \textbf{GSM8K} & 3.36 & 15.63 & 8.33 & 15.85 & 14.02 \\
        \bottomrule
    \end{tabular}
    \caption{Performance of three collaborator LLMs.}
    \label{tab:decision_tree}
\end{table}

In this setup, the draft model is TinyLlama, while the assistant models are Llama 2 Chat 7b and Llama-7b. Our findings demonstrate that involving additional models improves prediction accuracy. Table \ref{tab:decision_tree} demonstrates that when three models collaborate if one significantly outperforms the other two, the final system will achieve performance close to that of the best model. This indicates that our algorithm is effective when applied to more than two models. With sufficient LLMs, we can also better utilize training data, even when certain samples are excluded.

\textbf{The Case Study of MMLU.}
\label{sec:mmlu}
While {\n} is effective in many cases, there are instances where it makes incorrect decisions, highlighting its limitations. As shown in Table \ref{tab:casesmmlu}, {\n} does not always select the correct answer when the draft model and the assistant model disagree. In the first example, the draft model correctly identified the answer with high confidence, which allowed the final output to retain the accurate result. This showcases the potential of {\n} to preserve correct answers when confidence aligns with accuracy.

However, in the second example, the draft model once again made the correct prediction, but the assistant model, despite being incorrect, provided an answer with higher confidence. Consequently, the final output was altered to the wrong answer, overriding the draft model's correct prediction. This illustrates a shortcoming of the {\n} approach: relying solely on confidence scores as the decision-making criterion does not guarantee correctness. Confidence may reflect certainty but not necessarily accuracy, leading to situations where errors from the assistant model dominate the final outcome.

This limitation suggests that while {\n} can improve generation quality by prioritizing higher-confidence predictions, it does so with the assumption that confidence correlates with correctness. In practice, this assumption does not always hold, especially when the assistant model is overconfident in its incorrect predictions. To address this, future improvements could explore additional heuristics or cross-validation mechanisms to better balance confidence with accuracy, ensuring that correct answers are more consistently selected.

\begin{table}[t!]
    \centering
    \begin{tabular}{@{}p{2cm}|p{2.8cm}@{}p{2.8cm}@{}p{2.8cm}@{}p{2.8cm}@{}}
        \toprule
        & \multicolumn{4}{c}{\textbf{Example}} \\ 
        \midrule
        \textbf{\makecell[l]{Question}} & \multicolumn{4}{l}{\makecell[l]{The Yang-shao culture gave way to the Lung-Shan sometime after: \\A. 6,000 B.P. B. 5,000 B.P. C. 4,000 B.P. D. 3,000 B.P. \\
        \textcolor{red}{Correct answer: B}}}\\
        \midrule
       \multirow{2}{*}{\textbf{Answer}} &\textbf{Draft}&\textbf{Assist.}&\textbf{CoSD-Rule}&\textbf{CoSD-Tree}\\
        &B&D (wrong)&B&B\\
        \midrule
        \textbf{\makecell[l]{Question}} & \multicolumn{4}{l}{\makecell[l]{Rowena can paint a room in $14$ hours, while Ruby can paint it in $6$ hours. \\ If Rowena paints for $x$ hours and Ruby paints for $y$ hours, they will finish \\ half of the painting, while if Rowena paints for $y$ hours and Ruby paints \\ for $x$ hours they will paint the whole room. Find the ordered pair $(x,y)$. \\ A. ($\frac{11}{10}, \frac{11}{10}$)
        B. ($\frac{231}{20}, \frac{21}{20}$)
        C. ($\frac{231}{40}, \frac{21}{40}$)
        D. (1,1) \\ \textcolor{red}{Correct answer: C}}}\\
        \midrule
        \multirow{2}{*}{\textbf{Answer}}&\textbf{Draft}&\textbf{Assist.}&\textbf{CoSD-Rule}&\textbf{CoSD-Tree}\\
        &C&D (wrong)&D (wrong)&D (wrong)\\
        \midrule
        \bottomrule
    \end{tabular}
    \caption{Two examples of how {\n} modify the generation in MMLU dataset. The example above demonstrates how {\n} helps improve generation quality, while the example below shows instances where {\n} sometimes selects incorrect answers.}
    \label{tab:casesmmlu}
\end{table}

\begin{table*}[h!]
    \centering
    \begin{tabular}{@{}p{2.0cm}p{14.5cm}@{}}
        \toprule
        \textbf{\makecell[c]{Model}} & \textbf{\makecell[c]{Example}} \\ 
        \midrule
        \textbf{\makecell[c]{Question}} & \makecell[l]{Rory orders 2 subs for \$7.50 each, 2 bags of chips for \$1.50 each and 2 cookies for \$1.00 \\each for delivery.  There’s a 20\% delivery fee added at check out and she wants to add a \\ \$5.00 tip.  What will her delivery order cost? \textcolor{red}{Correct answer: 29}}\\
        \midrule
        \textbf{\makecell[c]{Draft \\Model}} & \makecell[l]{The cost of the subs is 2 * \$7.50 = $\ll$2*7.5=15$\gg$15 \\ The cost of the chips is 2 * \$1.50 = $\ll$2*1.5=3$\gg$3 \\ We add the total cost of the food items by \$15 + \$3 + \$2 = \$$\ll$15+3+2=20$\gg$20 \\ The cost of the delivery fee is 20\% of the total cost of the order, \\so 20\% of 20 = $\ll$20\% of 20=10$\gg$10 \\ The total cost of the food items and delivery fee is \$20 + \$10 = \$$\ll$20+10=30$\gg$30 \\ The cost of the tip is \$5.00 \\ The total cost of the order is 15 + 3 + 2 + 10 + 5 = $\ll$15+3+2+10+5=45$\gg$45.} \\
        \midrule
        \textbf{\makecell[c]{Assist. \\ Model}} & \makecell[l]{The cost of the subs is 2 x \$7.5 = \$$\ll$2*7.5=15$\gg$15 \\ The cost of the chips is 2 x \$1.5 = \$$\ll$2*1.5=3$\gg$3 \\ The total cost of her order without any fees is \$15 + \$3 + \$2 = \$$\ll$15+3+2=20$\gg$20 \\ The delivery fee is 20\% of the total order, so \$20 * 0.20 = \$$\ll$20*0.20=4$\gg$4 \\ The tip is an additional \$5 \\ Therefore, her delivery order will cost \$15 + \$3 + \$2 + \$4 + \$5 = \$$\ll$15+3+2+4+5=35$\gg$35.}  \\
        \bottomrule
    \end{tabular}
    \caption{The answers from the single draft model and assistant model with the question in Table \ref{tab:casestudy}.}
    \label{tab:casestudyapp}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
