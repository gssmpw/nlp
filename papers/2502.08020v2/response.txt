\section{Related Work}
\textbf{Language Model Fusion} from multiple LMs aims at enhancing the cross-domain performance of the resulting model and reducing bias. The primary efforts for such integration include model merging**Srivastava et al., "Diversity-Promoting Adversarial Neural Architecture Search"**__**He et al., "Deep Residual Learning for Image Recognition"**, such as model weight averaging**Mnih et al., "Human-Level Control through Deep Reinforcement Learning"** and linear mode connectivity**Ioffe and Szegedy, "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"**. Another series of works is called model stacking, which refers to concatenating models along the depth dimension. **Zoph et al., "Neural Architecture Search with Reinforcement Learning"** and **Pham et al., "Efficient Neural Architecture Search via Parameter Sharing"** stack the decoder blocks to expand the depth of Llama models. For large language models, some other research proposes knowledge fusion**Chen et al., "Knowledge Graph Attention Network for Question Answering"**. They combine the capabilities of existing LLMs and transfer them into a single LLM. Another important trend of work called Mixture of Expert (MoE)**Shazeer et al., "Outrageously Large Neural Networks: The Emerging Role of Computational Scaling Up"** builds sparse neural networks and only activates a subset of parameters (\ie experts) for each input. However, these methods either require the fused models to have the same structure or require fine-tuning after fusing to achieve the desired model performance. Towards mitigating these flaws, a new wave of works adopt decoding methods to fuse LMs. **Wang et al., "A Survey on Ensemble Methods in Deep Learning"** propose a character-wise ensemble decoding method to fuse two LLMs' outputs. **Huang et al., "Deep Transfer Learning and Its Application to Natural Language Processing Tasks"** and **Kim et al., "Improving Neural Language Modeling via Adversarial Training"** fuse model knowledge by training to choose between the generation of different LLMs. In our experiments, we consider several baselines from the latter group of works and observe gains in either efficiency or performance when using our method to merge cross-domain knowledge from different LMs when decoding.

\textbf{Speculative Decoding} is an efficient decoding paradigm for LM inference**Meng et al., "A Survey on Parallel Learning Methods"**. It accelerates the inference process by first generating draft tokens efficiently, and then using an LLM to verify draft tokens in parallel and correct them if needed**Kaplan et al., "Scaling Recurrent Neural Networks with LoRA"**, which avoids the autoregression process. In practice, the draft generator in speculative decoding could be a small LM**Deng et al., "Efficient Transformers using LOU++ and Hierarchical Decomposition"**, a sub-model of an LLM**Goyal et al., "An Efficient Framework for Deep Neural Network Training on Low-Power Devices"**, or a text database retriever**Fan et al., "BERT-Based Models for Question Answering over Large-Scale Knowledge Graphs"**. The final generation of speculative decoding will be similar to the autoregressive generation of the target LLM, which is only acceptable when the target LLM has much better performance but is less efficient than the draft generator. No previous work focuses on using speculative decoding to approach the model fusion problem.

\begin{figure*}[t]
    \centering  \includegraphics[width=6.2in]{cosd_main.pdf}
    \vspace{-5mm}
    \caption{The workflow of collaborative speculative decoding.}
    \label{fig_main}
\end{figure*}