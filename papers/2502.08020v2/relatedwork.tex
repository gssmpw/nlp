\section{Related Work}
\textbf{Language Model Fusion} from multiple LMs aims at enhancing the cross-domain performance of the resulting model and reducing bias. The primary efforts for such integration include model merging~\citep{goddard2024arcee}, such as model weight averaging~\citep{wortsman2022model} and linear mode connectivity~\citep{ainsworth2022git, ito2024analysis, wang2020federated}. Another series of works is called model stacking, which refers to concatenating models along the depth dimension. \citet{wu2024llama} and \citet{kim2023solar} stack the decoder blocks to expand the depth of Llama models. For large language models, some other research proposes knowledge fusion~\citep{wan2024knowledge}. They combine the capabilities of existing LLMs and transfer them into a single LLM. Another important trend of work called Mixture of Expert (MoE)~\citep{zhu2024llama, xue2024openmoe} builds sparse neural networks and only activates a subset of parameters (\ie experts) for each input. However, these methods either require the fused models to have the same structure or require fine-tuning after fusing to achieve the desired model performance. Towards mitigating these flaws, a new wave of works adopt decoding methods to fuse LMs. \citet{gu2024chared} propose a character-wise ensemble decoding method to fuse two LLMs' outputs. \citet{shen2024learning} and \citet{wang2023fusing} fuse model knowledge by training to choose between the generation of different LLMs. In our experiments, we consider several baselines from the latter group of works and observe gains in either efficiency or performance when using our method to merge cross-domain knowledge from different LMs when decoding.
% However, these methods still fail to find both effective and efficient decoding ways to merge cross-domain knowledge from different LMs.

\textbf{Speculative Decoding} is an efficient decoding paradigm for LM inference~\citep{xia2024unlocking,stern2018blockwise,xia2023speculative}. It accelerates the inference process by first generating draft tokens efficiently, and then using an LLM to verify draft tokens in parallel and correct them if needed~\citep{leviathan2023fast}, which avoids the autoregression process. In practice, the draft generator in speculative decoding could be a small LM~\citep{chen2023accelerating,miao2023specinfer,zhou2023distillspec}, a sub-model of an LLM~\citep{zhang2023draft,yang2023predictive,elhoushi2024layer}, or a text database retriever~\citep{he2023rest,li2024nearest}. The final generation of speculative decoding will be similar to the autoregressive generation of the target LLM, which is only acceptable when the target LLM has much better performance but is less efficient than the draft generator. No previous work focuses on using speculative decoding to approach the model fusion problem.

\begin{figure*}[t]
    \centering  \includegraphics[width=6.2in]{cosd_main.pdf}
    \vspace{-5mm}
    \caption{The workflow of collaborative speculative decoding.}
    \label{fig_main}
\end{figure*}