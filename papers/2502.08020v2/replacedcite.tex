\section{Related Work}
\textbf{Language Model Fusion} from multiple LMs aims at enhancing the cross-domain performance of the resulting model and reducing bias. The primary efforts for such integration include model merging____, such as model weight averaging____ and linear mode connectivity____. Another series of works is called model stacking, which refers to concatenating models along the depth dimension. ____ and ____ stack the decoder blocks to expand the depth of Llama models. For large language models, some other research proposes knowledge fusion____. They combine the capabilities of existing LLMs and transfer them into a single LLM. Another important trend of work called Mixture of Expert (MoE)____ builds sparse neural networks and only activates a subset of parameters (\ie experts) for each input. However, these methods either require the fused models to have the same structure or require fine-tuning after fusing to achieve the desired model performance. Towards mitigating these flaws, a new wave of works adopt decoding methods to fuse LMs. ____ propose a character-wise ensemble decoding method to fuse two LLMs' outputs. ____ and ____ fuse model knowledge by training to choose between the generation of different LLMs. In our experiments, we consider several baselines from the latter group of works and observe gains in either efficiency or performance when using our method to merge cross-domain knowledge from different LMs when decoding.
% However, these methods still fail to find both effective and efficient decoding ways to merge cross-domain knowledge from different LMs.

\textbf{Speculative Decoding} is an efficient decoding paradigm for LM inference____. It accelerates the inference process by first generating draft tokens efficiently, and then using an LLM to verify draft tokens in parallel and correct them if needed____, which avoids the autoregression process. In practice, the draft generator in speculative decoding could be a small LM____, a sub-model of an LLM____, or a text database retriever____. The final generation of speculative decoding will be similar to the autoregressive generation of the target LLM, which is only acceptable when the target LLM has much better performance but is less efficient than the draft generator. No previous work focuses on using speculative decoding to approach the model fusion problem.

\begin{figure*}[t]
    \centering  \includegraphics[width=6.2in]{cosd_main.pdf}
    \vspace{-5mm}
    \caption{The workflow of collaborative speculative decoding.}
    \label{fig_main}
\end{figure*}