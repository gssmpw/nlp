[
  {
    "index": 0,
    "papers": [
      {
        "key": "goddard2024arcee",
        "author": "Goddard, Charles and Siriwardhana, Shamane and Ehghaghi, Malikeh and Meyers, Luke and Karpukhin, Vlad and Benedict, Brian and McQuade, Mark and Solawetz, Jacob",
        "title": "Arcee's MergeKit: A Toolkit for Merging Large Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wortsman2022model",
        "author": "Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others",
        "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "ainsworth2022git",
        "author": "Ainsworth, Samuel K and Hayase, Jonathan and Srinivasa, Siddhartha",
        "title": "Git re-basin: Merging models modulo permutation symmetries"
      },
      {
        "key": "ito2024analysis",
        "author": "Ito, Akira and Yamada, Masanori and Kumagai, Atsutoshi",
        "title": "Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching"
      },
      {
        "key": "wang2020federated",
        "author": "Wang, Hongyi and Yurochkin, Mikhail and Sun, Yuekai and Papailiopoulos, Dimitris and Khazaeni, Yasaman",
        "title": "Federated learning with matched averaging"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wu2024llama",
        "author": "Wu, Chengyue and Gan, Yukang and Ge, Yixiao and Lu, Zeyu and Wang, Jiahao and Feng, Ye and Luo, Ping and Shan, Ying",
        "title": "Llama pro: Progressive llama with block expansion"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "kim2023solar",
        "author": "Kim, Dahyun and Park, Chanjun and Kim, Sanghoon and Lee, Wonsung and Song, Wonho and Kim, Yunsu and Kim, Hyeonwoo and Kim, Yungi and Lee, Hyeonju and Kim, Jihoo and others",
        "title": "Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wan2024knowledge",
        "author": "Wan, Fanqi and Huang, Xinting and Cai, Deng and Quan, Xiaojun and Bi, Wei and Shi, Shuming",
        "title": "Knowledge fusion of large language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zhu2024llama",
        "author": "Zhu, Tong and Qu, Xiaoye and Dong, Daize and Ruan, Jiacheng and Tong, Jingqi and He, Conghui and Cheng, Yu",
        "title": "Llama-moe: Building mixture-of-experts from llama with continual pre-training"
      },
      {
        "key": "xue2024openmoe",
        "author": "Xue, Fuzhao and Zheng, Zian and Fu, Yao and Ni, Jinjie and Zheng, Zangwei and Zhou, Wangchunshu and You, Yang",
        "title": "Openmoe: An early effort on open mixture-of-experts language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "gu2024chared",
        "author": "Gu, Kevin and Tuecke, Eva and Katz, Dmitriy and Horesh, Raya and Alvarez-Melis, David and Yurochkin, Mikhail",
        "title": "CharED: Character-wise Ensemble Decoding for Large Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "shen2024learning",
        "author": "Shen, Shannon Zejiang and Lang, Hunter and Wang, Bailin and Kim, Yoon and Sontag, David",
        "title": "Learning to decode collaboratively with multiple language models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "wang2023fusing",
        "author": "Wang, Hongyi and Polo, Felipe Maia and Sun, Yuekai and Kundu, Souvik and Xing, Eric and Yurochkin, Mikhail",
        "title": "Fusing models with complementary expertise"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "xia2024unlocking",
        "author": "Xia, Heming and Yang, Zhe and Dong, Qingxiu and Wang, Peiyi and Li, Yongqi and Ge, Tao and Liu, Tianyu and Li, Wenjie and Sui, Zhifang",
        "title": "Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding"
      },
      {
        "key": "stern2018blockwise",
        "author": "Stern, Mitchell and Shazeer, Noam and Uszkoreit, Jakob",
        "title": "Blockwise parallel decoding for deep autoregressive models"
      },
      {
        "key": "xia2023speculative",
        "author": "Xia, Heming and Ge, Tao and Wang, Peiyi and Chen, Si-Qing and Wei, Furu and Sui, Zhifang",
        "title": "Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "leviathan2023fast",
        "author": "Leviathan, Yaniv and Kalman, Matan and Matias, Yossi",
        "title": "Fast inference from transformers via speculative decoding"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "chen2023accelerating",
        "author": "Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John",
        "title": "Accelerating large language model decoding with speculative sampling"
      },
      {
        "key": "miao2023specinfer",
        "author": "Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Wang, Zeyu and Zhang, Zhengxin and Wong, Rae Ying Yee and Zhu, Alan and Yang, Lijie and Shi, Xiaoxiang and others",
        "title": "SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification"
      },
      {
        "key": "zhou2023distillspec",
        "author": "Zhou, Yongchao and Lyu, Kaifeng and Rawat, Ankit Singh and Menon, Aditya Krishna and Rostamizadeh, Afshin and Kumar, Sanjiv and Kagy, Jean-Fran{\\c{c}}ois and Agarwal, Rishabh",
        "title": "Distillspec: Improving speculative decoding via knowledge distillation"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "zhang2023draft",
        "author": "Zhang, Jun and Wang, Jue and Li, Huan and Shou, Lidan and Chen, Ke and Chen, Gang and Mehrotra, Sharad",
        "title": "Draft \\& verify: Lossless large language model acceleration via self-speculative decoding"
      },
      {
        "key": "yang2023predictive",
        "author": "Yang, Seongjun and Lee, Gibbeum and Cho, Jaewoong and Papailiopoulos, Dimitris and Lee, Kangwook",
        "title": "Predictive pipelined decoding: A compute-latency trade-off for exact LLM decoding"
      },
      {
        "key": "elhoushi2024layer",
        "author": "Elhoushi, Mostafa and Shrivastava, Akshat and Liskovich, Diana and Hosmer, Basil and Wasti, Bram and Lai, Liangzhen and Mahmoud, Anas and Acun, Bilge and Agarwal, Saurabh and Roman, Ahmed and others",
        "title": "Layer skip: Enabling early exit inference and self-speculative decoding"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "he2023rest",
        "author": "He, Zhenyu and Zhong, Zexuan and Cai, Tianle and Lee, Jason D and He, Di",
        "title": "Rest: Retrieval-based speculative decoding"
      },
      {
        "key": "li2024nearest",
        "author": "Li, Minghan and Chen, Xilun and Holtzman, Ari and Chen, Beidi and Lin, Jimmy and Yih, Wen-tau and Lin, Xi Victoria",
        "title": "Nearest Neighbor Speculative Decoding for LLM Generation and Attribution"
      }
    ]
  }
]