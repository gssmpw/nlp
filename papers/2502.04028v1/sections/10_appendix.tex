
% \section{Deep Meta Coordination Graphs for Multi-Agent Reinforcement Learning}

\noindent\rule{\linewidth}{1pt}

\noindent \textbf{Appendix A}: This section provides further details about evaluation domains used in the study. 

\noindent \textbf{Appendix B}: This section provides further details about algorithm implementations in the study. 

\noindent \textbf{Appendix C}: This section provides details on some of the ablation study conducted in this paper. 

\noindent\rule{\linewidth}{1pt} 

\section*{Appendix A: Task settings} 

\paragraph{Why MACO benchmark?} \cite{wang2022contextaware} It is designed to evaluate multi-agent reinforcement learning algorithms by presenting them with a series of complex and diverse coordination tasks. This benchmark draws from classic problems in the cooperative multi-agent learning literature, enhancing their difficulty. Each task represents a specific type of coordination challenge where agents must learn different coordination strategies. By increasing the complexity of these tasks, the MACO benchmark provides a rigorous framework for analyzing the performance and adaptability of multi-agent learning approaches in various cooperative scenarios. 

\paragraph{Gather} (Figure \ref{fig:envs}a) is an extension of the Climb Game \cite{wei2016lenient}. Gather increases the task's complexity by extending it temporally and adding stochasticity. Agents must navigate to one of three potential goal states: \( g_1 \), \( g_2 \), or \( g_3 \), corresponding to the actions \( a_0 \), \( a_1 \), and \( a_2 \) in Climb. At the start of each episode, one goal is randomly designated as optimal. Agents are randomly spawned, and only those near the optimal goal know its designation. The reward function is:\[R = 
\begin{cases} 
10 & \text{if all agents reach the optimal goal } g_1, \\
5 & \text{if all agents reach a non-optimal goal}, \\
-5 & \text{if only some agents reach the optimal goal}.
\end{cases}
\]

\paragraph{Disperse} (Figure \ref{fig:envs}b) has twelve agents who must select one of four hospitals to work at each timestep. The environment has a dynamic requirement where only one hospital needs a specific number of agents at any given time $t$. The task tests the agents' ability to distribute themselves efficiently according to the hospital's needs, with penalties for understaffing, i.e., when \(y_j^{t+1} < x_j^t\), where $y_j^{t+1}$ is the number of agents that went to hospital $j$ and $x_j^t$ is the required number of agents for that hospital. \[ R = min(y_j^{t+1} - x_j^t, 0) \]

\paragraph{Pursuit or ``Predator and Prey"} (Figure \ref{fig:envs}c) has ten predator agents that must capture randomly walking prey on a $10 \times 10$ grid. The environment is designed to test the agents' ability to coordinate their movements to successfully capture the prey, which requires simultaneous actions by at least two predators. The task is made more challenging by introducing penalties for failed capture attempts. \[ R = 
\begin{cases} 
1 & \text{if prey is captured by two agents} \\
-1 & \text{if only one agent attempts to capture prey}
\end{cases}
\]

\paragraph{Hallway} (Figure \ref{fig:envs}d) is a multi-chain Dec-POMDP \cite{oliehoek2016concise} which extends the original Hallway problem \cite{wang2019learning}. Agents must coordinate their movements through a hallway to reach a goal state simultaneously. Each agent can observe its own position and choose to move left, move right, or stay still. The environment tests the agents' ability to synchronize their actions in the face of limited observability and potential conflicts when multiple groups attempt to reach the goal simultaneously. \[ R = 
\begin{cases} 
1 & \text{if agents in the same group} \\ 
& \text{reach goal $g$ simultaneously,} \\
-0.5 \times n_g & \text{if $n_g > 1$ groups attempt to} \\
                & \text{move to $g$ at the same time,}
\end{cases}
\]
where $n_g$ is the number of groups attempting to reach the goal simultaneously.


\paragraph{StarCraft Multi-Agent Challenge (SMACv2)} (Figure \ref{fig:envs}e) is a challenging benchmark for MARL due to its combination of high-dimensional, partially observable environments, diverse unit types, and dynamic, stochastic elements. SMACv2 \cite{ellis2023smacv} introduces procedurally generated scenarios that require agents to generalize to unseen settings during evaluation, addressing the lack of stochasticity and partial observability in the original SMAC \cite{samvelyan2019starcraft}, which previously allowed near-perfect performance with simpler open-loop policies. 

\section*{Appendix B: Overall experimental setup} 

\textbf{We will make all source code required for conducting and analyzing the experiments publicly available upon publication of the paper with a license that allows free usage for research purposes. } We have shared all our (anonymized) codes here: {\color{blue}\url{https://anonymous.4open.science/r/dmcgmarl}} for reviewers' reference. The conceptual outlines of all \algoabb\ algorithms are discussed in the main paper. All baseline algorithms (IQL, VDN, QMIX, CW-QMIX, OW-QMIX, QTRAN, QPLEX) are implemented in the PyMARL framework \cite{rashid2018qmix} and PyMARL2 \cite{hu2023rethinking}. DCG's implementation was based on the official repository released by the authors, which can be found at \url{https://github.com/wendelinboehmer/dcg}. 
% We list the final hyperparameters for \algoabb\ in \cref{tab:dmcg_hyperparameters,tab:rl_hyperparameters}. 
All experiments were run on four different seeds \{0, 1, 2, 3\} to compute each reported result for statistical significance. We report the mean (dark lines in plots) and standard deviations (light shadows underlying the corresponding curves) of our evaluation metrics to summarize the performances of all reported results. The experiments were run on a computing infrastructure equipped with NVIDIA RTX 6000 Ada Generation GPUs, each with 50GB of memory, running on a Linux operating system.

% \subsection{Hyperparameters} 

% \Cref{tab:gather_env_args,tab:disperse_env_args,tab:pursuit_env_args,tab:hallway_env_args,tab:smacv2_env_args} present the environment configurations used in our experiments. These configurations are crucial for replicating the results and understanding the dynamics of each environment.

% \begin{table*}[!ht]
%     \centering
%     \begin{tabular}{|c|c|c|c|c|c|}
%         \hline
%         \textbf{Parameter} & \textbf{Gather} & \textbf{Disperse} & \textbf{Pursuit} & \textbf{Hallway} & \textbf{SMACv2} \\ \hline
%         \texttt{$|\mathcal{V}| (= n)$}   & 5 & 12 & 10 & 12 & 5\\ \hline
%         \texttt{$K$}   & 5 & 12 & 10 & 12 & 5 \\ \hline
%         \texttt{$d$}   & 4 & 17 & 70 & 2 & 1024 \\ \hline
%         \texttt{$o$}   & 5 & 12 & 10 & 12 & 5 \\ \hline
%         \texttt{topology}      & full & full & full & full & full \\ \hline
%         \texttt{rnn\_hidden\_dim} & 64 & 64 & 64 & 64 & 64 \\ \hline
%         \texttt{seeds} & \{0,1,2,3\} & \{0,1,2,3\} & \{0,1,2,3\} & \{0,1,2,3\} & \{0,1,2,3\} \\ \hline
%     \end{tabular}
%     \caption{DMCG hyperparameters}
%     \label{tab:dmcg_hyperparameters}
% \end{table*}

% % \begin{table}[!h]
% %     \centering
% %     \begin{tabular}{|c|c|c|c|c|c|}
% %         \hline
% %         \textbf{Parameter} & \textbf{Ranges of tried values} \\ \hline
% %         \texttt{$K$}   & \{1,2,3,5, $n/2$, $n$\} \\ \hline
% %         \texttt{$o$}   & \{1,2,3,5, $n/2$, $n$\} \\ \hline
% %         \texttt{topology}      & {\{full, line, cycle, star, none\}} \\ \hline
% %         \texttt{rnn\_hidden\_dim} & \{64\} \\ \hline
% %     \end{tabular}
% %     \caption{DMCG Hyperparameters (ranges tried)}
% %     \label{tab:dmcg_hyperparameters}
% % \end{table}

% \begin{table}[!ht]
%     \centering
%     \begin{tabular}{|c|c|}
%         \hline
%         \textbf{Hyperparameter} & \textbf{Value} \\ \hline
%         \texttt{gamma}          & 0.99 \\ \hline
%         \texttt{batch\_size}    & 32 \\ \hline
%         \texttt{buffer\_size}   & 32 \\ \hline
%         \texttt{lr}             & 0.0005 \\ \hline
%         \texttt{critic\_lr}     & 0.0005 \\ \hline
%         \texttt{optim\_alpha}   & 0.99 \\ \hline
%         \texttt{optim\_eps}     & 0.00001 \\ \hline
%         \texttt{grad\_norm\_clip} & 10 \\ \hline
%     \end{tabular}
%     \caption{RL Hyperparameters}
%     \label{tab:rl_hyperparameters}
% \end{table}

% \begin{table}[!ht]
%     \centering
%     \begin{tabular}{|c|c|}
%         \hline
%         \textbf{Parameter} & \textbf{Value} \\ \hline
%         \texttt{n\_agents} & 5 \\ \hline
%         \texttt{episode\_limit} & 8 \\ \hline
%         \texttt{map\_height} & 3 \\ \hline
%         \texttt{map\_width} & 5 \\ \hline
%         \texttt{catch\_reward} & 10 \\ \hline
%         \texttt{catch\_fail\_reward} & -5 \\ \hline
%         \texttt{other\_reward} & 5 \\ \hline
%     \end{tabular}
%     \caption{Environment Arguments for Gather}
%     \label{tab:gather_env_args}
% \end{table}

% \begin{table}[!ht]
%     \centering
%     \begin{tabular}{|c|c|}
%         \hline
%         \textbf{Parameter} & \textbf{Value} \\ \hline
%         \texttt{n\_agents} & 12 \\ \hline
%         \texttt{n\_actions} & 4 \\ \hline
%         \texttt{initial\_need} & [0, 0, 0, 0] \\ \hline
%         \texttt{episode\_limit} & 10 \\ \hline
%     \end{tabular}
%     \caption{Environment Arguments for Disperse}
%     \label{tab:disperse_env_args}
% \end{table} 

% \begin{table}[!ht]
%     \centering
%     \begin{tabular}{|c|c|}
%         \hline
%         \textbf{Parameter} & \textbf{Value} \\ \hline
%         \texttt{n\_agents} & 10 \\ \hline
%         \texttt{n\_preys} & 5 \\ \hline
%         \texttt{episode\_limit} & 50 \\ \hline
%         \texttt{map\_size} & 10 \\ \hline
%         \texttt{catch\_reward} & 1 \\ \hline
%         \texttt{catch\_fail\_reward} & -1 \\ \hline
%         \texttt{sight\_range} & 2 \\ \hline
%     \end{tabular}
%     \caption{Environment Arguments for Pursuit}
%     \label{tab:pursuit_env_args}
% \end{table}


% \begin{table}[!ht]
%     \centering
%     \begin{tabular}{|c|c|}
%         \hline
%         \textbf{Parameter} & \textbf{Value} \\ \hline
%         \texttt{n\_agents} & 12 \\ \hline
%         \texttt{n\_groups} & 5 \\ \hline
%         \texttt{reward\_win} & 1 \\ \hline
%     \end{tabular}
%     \caption{Environment Arguments for Hallway}
%     \label{tab:hallway_env_args}
% \end{table} 

% \begin{table}[!ht]
%     \centering
%     \begin{tabular}{|c|c|}
%         \hline
%         \textbf{Parameter} & \textbf{Value} \\ \hline
%         \texttt{difficulty} & 7 \\ \hline
%         \texttt{map\_name} & ``10gen\_protoss" \\ \hline
%         \texttt{move\_amount} & 2 \\ \hline
%         \texttt{reward\_death\_value} & 10 \\ \hline
%         \texttt{reward\_defeat} & 0 \\ \hline
%         \texttt{reward\_negative\_scale} & 0.5 \\ \hline
%         \texttt{reward\_only\_positive} & True \\ \hline
%         \texttt{reward\_scale} & True \\ \hline
%         \texttt{reward\_scale\_rate} & 20 \\ \hline
%         \texttt{reward\_sparse} & False \\ \hline
%         \texttt{reward\_win} & 200 \\ \hline
%         \texttt{min\_attack\_range} & 2 \\ \hline
%         \texttt{num\_fov\_actions} & 12 \\ \hline
%         \texttt{obs\_own\_pos} & True \\ \hline
%         \ \ \texttt{n\_units} & 5 \\ \hline
%         \ \ \texttt{n\_enemies} & 5 \\ \hline
%         \ \ \ \ \texttt{dist\_type} & weighted\_teams \\ \hline
%         \ \ \ \ \texttt{unit\_types} & [``stalker", ``zealot", ``colossus"] \\ \hline
%         \ \ \ \ \texttt{weights} & [0.45, 0.45, 0.1] \\ \hline
%         \ \ \ \ \texttt{observe} & True \\ \hline
%         \ \ \ \ \texttt{dist\_type} & surrounded\_and\_reflect \\ \hline
%         \ \ \ \ \texttt{p} & 0.5 \\ \hline
%         \ \ \ \ \texttt{map\_x} & 32 \\ \hline
%         \ \ \ \ \texttt{map\_y} & 32 \\ \hline
%     \end{tabular}
%     \caption{Environment Arguments for SMACv2}
%     \label{tab:smacv2_env_args}
% \end{table}

\section*{Appendix C: Ablation studies} 

\subsection*{(1) Topologies} 

\begin{figure}[!htp]
\centering
\includegraphics[width=\linewidth]{figures/results/maco_topo.png}
\caption{Impact of different coordination graph topologies on \algoabbâ€™s performance in MACO tasks. Fully connected graphs ($E_\text{full}$) provided the best results, underscoring the importance of topology choice in MARL.}
\label{fig:ablation:topologies}
\end{figure} 

\edits{We investigated the impact of different coordination graph (CG) topologies on \algoabb's performance in MACO tasks. Specifically, we examined fully connected (\(E_{\text{full}}\)), cyclic (\(E_{\text{cycle}}\)), linear (\(E_{\text{line}}\)), and star (\(E_{\text{star}}\)) structures, following the approach in \cite{bohmer2020deep}. Each topology was used as the initialization for the input CGs, allowing us to evaluate how different graph structures influence learning. A single structure was initialized to assess learning impact, as no variability exists in such a topology. \algoabb\ dynamically refines these initial topologies during training, leveraging its attention mechanism to focus on the most relevant edges for task-specific coordination. As shown in Figure~\ref{fig:ablation:topologies}, fully connected graphs consistently yielded the best performance, highlighting the importance of initializing with a topology that allows for flexible and rich interactions.} 
% In our first ablation study, we investigated the impact of different topologies in \algoabb\ in all MACO tasks. Following \cite{bohmer2020deep}, we also considered the following edge sets $E$: 
% \begin{itemize}
%     \item $E_\text{full} := \{ \{i, j\} \mid 1 \leq i < n, \ i < j \leq n \}$
%     \item $E_\text{cycle} := \{ \{i, (i \ \text{mod} \ n) + 1\} \mid 1 \leq i \leq n \}$
%     \item $E_\text{line} := \{ \{i, i + 1\} \mid 1 \leq i < n \}$
%     \item $E_\text{star} := \{ \{1, i\} \mid 2 \leq i \leq n \}$
% \end{itemize}
% \edits{
% Our findings, as shown in Figure \ref{fig:ablation:topologies}, reveal that the topology of the coordination graph significantly affects the agents' ability to coordinate effectively. Fully connected coordination graphs generally led to better overall performance. However, the varying and sometimes poor performance of other topologies underscores the importance of choosing an appropriate topology for the coordination graph in MARL. \\ 


\subsection*{(2) Algorithms} 

\begin{figure}[!htp]
\centering
\includegraphics[width=\linewidth]{figures/results/maco_algo.png}
\caption{Impact of using \algoabb\ as a feature learning backbone for VDN, QMIX, CW-QMIX, and OW-QMIX in the Gather task. Notably, \algoabb\ can enhance the performance of these methods.} 
\label{fig:ablation:algorithms}
\end{figure} 

\noindent \edits{In the Gather task, we explored the impact of using \algoabb\ as a representation learning backbone for value decomposition methods, including VDN, QMIX, CW-QMIX, and OW-QMIX. Instead of each agent directly using its local observations, \algoabb\ was used to generate meta coordination graphs and extract feature representations using its GCN module. These enriched features, capturing higher-order and indirect relationships, were then input into the respective algorithms. This approach does not modify the underlying logic of the algorithms but augments their input space. As shown in Figure~\ref{fig:ablation:algorithms}, QMIX and other methods exhibited improved performance when using \algoabb\ as a feature extractor, demonstrating the effectiveness of its learned representations. For example, the learned meta coordination graphs provided richer task-relevant context, even for algorithms that rely on single-agent decomposition. } 
% In the next further ablation study conducted in the Gather task, we explored the impact of using \algoabb's as a representation learning backbone on value decomposition methods, including VDN, QMIX, CW-QMIX, and OW-QMIX. Specifically, \algoabb\ inputs the coordination graph, generates multiple sets of meta coordination graphs capturing intricate multi-agent interactions, and then uses GCN to learn powerful representations for encoding them. We input these learned representations into the aforementioned algorithms and study their performance. Figure \ref{fig:ablation:algorithms} illustrates our findings. QMIX's performance (Figure \ref{fig:ablation:topologies}a) was considerably enhanced with all graph topology. VDN (Figure \ref{fig:ablation:topologies}b) and CW-QMIX (Figure \ref{fig:ablation:topologies}d) perform similar to all corresponding variants. The \algoabb-STAR and \algoabb-CYCLE are slow at first but eventually learn a slightly better policy in VDN and CW-QMIX, respectively. OW-QMIX (Figure \ref{fig:ablation:topologies}c) still outperforms all the corresponding \algoabb\ variants with varying topologies. We speculate that \algoabb\ can be beneficial to MARL algorithms even as only a feature extractor by leveraging the higher-order relationships it learns. This indicates that the representation learning in \algoabb\ effectively enriches the input features for other algorithms, allowing them to perform closer to the level of \algoabb\ itself. This study highlights the robustness and versatility of the representation learning component within \algoabb, demonstrating its value not only in our novel approach but also as an enhancement to existing CTDE algorithms. 