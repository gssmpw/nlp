\context{DecPOMDPs.} 
In this paper, we model cooperative multi-agent tasks as \textit{decentralized partially observable Markov decision processes} (Dec-POMDP) \cite{oliehoek2016concise}. Formally, we define the following tuple for $n$ agents, \[ \langle I, S, \boldsymbol{A}, \{A^i\}^{n}_{i=1}, P, R, \Omega, \boldsymbol{O}, \{O^i\}^{n}_{i=1}, n, \gamma \rangle,\] where $I = \{1, \ldots, n\}$ is the set of $n$ agents and $S$ is the set of environmental states. $\boldsymbol{A} = \times_{i \in I} A^i$ is the set of joint actions, where, $A^i$ are the actions available to agent $i$ which can be different for each agent. Similarly, \edits{$\boldsymbol{O} = \times_{i \in I} O^i$} is the set of joint observations with $O^i$ as the observations for agent $i$. At each timestep $t$, a joint observation $o_t = \langle o^1_t, \ldots, o^n_t \rangle$ is computed using the observation function $\Omega(\cdot\mid s_t)$, where $s_t \in S$ is the state of the environment at time $t$. Each agent $i$ observes only its own component $o^i_t \in o_t$ and selects an action $a^i_t \in A^i$. The joint action $a_t = \langle a^1_t, \ldots, a^n_t \rangle$ induces a transition to the next state $s_{t+1}$ according to the transition function $P(s_{t+1}|s_t,a_t)$. All agents receive a shared reward $r_t = R(s_t, a_t)$ and $\gamma \in [0, 1)$ is the discount factor. Each agent maintains a local action-observation history $\tau^i_t = \langle o^1_0, a^1_0, \ldots, o^n_{t-1}, a^n_{t-1}, o^n_t \rangle$ and $\tau_t = \langle \tau^1_t, \ldots, \tau^n_t \rangle$ is the joint history. We focus on episodic tasks of a varying length $T$. Let $\pi^i$ be the local policy of each agent $i$, then the joint policy $\pi$ can be given by $\pi(a_t \mid \tau_t) = \prod_{i \in I} \pi^i(a^i_t \mid \tau^i_t)$. We use the Q-value function to evaluate $\pi$:\[Q_{\pi}(s_t, a_t) = \mathbb{E}_{\pi}\left[\sum_{t'=t}^{T} \gamma^{t'-t} r_{t'} \Big\rvert s_t, a_t\right], \forall s_t \in S, \forall a_t \in A\] The goal in collaborative MARL is to find an optimal joint policy $\pi^* = \langle \pi^*_0, \ldots, \pi^*_n \rangle$ with $Q_{\pi^*} = Q^* = \max_{\pi} Q^{\pi}$. 

\paragraph{Independent Q Learning (IQL).} 
Each agent \(i\) learns its own policy \(\pi_i(a_i \mid \tau_i)\) independently, optimizing its individual value function \(Q_i(\tau_i, a_i)\) without considering the actions or policies of other agents \cite{tan1993multi}. 

\paragraph{Value function factorization.} The goal here is to decompose the global joint action-value function \( Q_{\text{tot}} \) into individual agent value functions \( Q_i \) in different ways while maintaining consistency with the individual greedy maximization (IGM) principle \cite{son2019qtran}. \textbf{VDN} \cite{sunehag2018value} assumes that \( Q_{\text{tot}} \) is the sum of all \( Q_i \): $Q^{\text{VDN}}_{\text{tot}}(\tau, a) = \sum_{i=1}^{n} Q_i(\tau_i, a_i) $. \textbf{QMIX} \cite{rashid2018qmix} introduces a mixing network to combine \( Q_i \) using a non-linear monotonic function. However, it uses an unweighted projection that treats all joint actions equally. \textbf{Weighted QMIX} \cite{rashid2020weighted} improves on this by using a weighted projection that prioritizes better joint actions. \textbf{QTRAN} \cite{son2019qtran} transforms \( Q_{\text{tot}} \) into a factorizable function, hence not requiring the constraint of additivity or monotonicity anymore. \textbf{QPLEX} \cite{wang2021qplex} takes a duplex dueling network architecture to factorize \( Q_{\text{tot}} \). 

\paragraph{Coordination Graphs.} Instead of fully decomposing \( Q_{\text{tot}} \), a higher value function factorization can be expressed using coordination graphs \cite{guestrin2002coordinated}. Specifically, for an undirected coordination graph $G = \langle V, E \rangle$ with agents as vertices $v_i \in V$ and coordination dependencies among agents as edges in $E$, we have \[ Q_{\text{tot}}(\tau, a) = \frac{1}{|V|} \sum_{v_i \in V} Q_i(\tau_i, a_i) + \frac{1}{|E|} \sum_{\{i,j\} \in E} Q_{ij}(\tau_i, \tau_j, a_i, a_j)\] Here $Q_i$ are the utility functions and $Q_{ij}$ are the pairwise payoff functions for the agents. 


