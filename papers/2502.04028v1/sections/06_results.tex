
In this section, we present the results of our experiments, highlighting \algoabb's performance on all the MARL tasks discussed in the previous section. We focus on the key domain-specific metrics (at test time): number of prey caught in Pursuit, average returns per episode in Disperse, and win rate in Gather, Hallway, and SMACv2. 

\paragraph{MACO benchmark.} Figure \ref{fig:res:maco_sota} shows the performance of all algorithms in the MACO tasks described previously. \algoabb\ outperforms all the other algorithms in the Gather environment significantly ($> 25\%$) and quickly (within $\approx$ 200K episodes). It achieves impressive performance with a stable win rate approaching nearly 100\% (Figure \ref{fig:res:maco_sota}a). DCG and OW-QMIX achieve the next-best win rates. However, OW-QMIX needs a lot more samples ($\approx$ 300K episodes) to get there. DCG makes slightly faster progress than \algoabb\ initially but stabilizes at a much lower win rate ($\approx$ 80\%), hence, suboptimal. In the Disperse environment, \algoabb\ significantly surpasses all independent learning and value function decomposition-based algorithms and slightly outperforms DCG (Figure \ref{fig:res:maco_sota}b). Pursuit and Hallway present notably more challenging problems, and \algoabb\ demonstrates a clear advantage over fully decomposed value-based approaches which fail to learn cooperative policies in these tasks (Figure \ref{fig:res:maco_sota}c and \ref{fig:res:maco_sota}d). DCG also outperforms all other algorithms in these tasks. \algoabb\ does moderately better than DCG in Pursuit but performs significantly better in Hallway. 

These environments strongly exhibit issues such as \textit{relative overgeneralization} and miscoordination due to partial observability and stochasticity. In the Pursuit task, for instance, when an agent, based on identical local observations, decides to capture prey but ends up failing and receiving a penalty because other agents did not take complementary actions. This outcome would have been different had the same actions been coordinated (relative overgeneralization). Similarly, in Hallway, where agents must reach a goal state simultaneously without knowledge of each other's positions, fully decomposed methods are inadequate, especially when agents start from random positions, leading to frequent failures in coordination. \algoabb's success in these environments demonstrates its ability to solve these issues which cause decentralized baselines and centralized training methods to suffer from sample inefficiency or fail completely. The introduction of stochasticity and the temporal extension of Gather and Disperse environments make them a robust test of agents' ability to develop sophisticated coordination strategies. Also, they are non-factored games, i.e., they do not present an explicit decomposition of global rewards. \algoabb\ implicitly allows the decomposition of the global reward into smaller, localized multi-agent interactions among agents. While the global reward remains non-factored in its explicit form, it models the dependencies between agentsâ€™ actions. This allows the learning process to break down the problem and optimize smaller, more manageable parts of the global objective. 

For DCG, we used its default configuration with complete graphs and no low-rank approximation. Despite its strong performance in all environments, DCG underperforms relative to \algoabb\ (significantly in Gather and Hallway; moderately in Disperse and Pursuit). We speculate that this is because DCG relies heavily only on \edits{direct} interactions between agents. This reduces DCG's effectiveness in tasks where higher-order \edits{and indirect} interactions are crucial but not explicitly modeled. In contrast, \algoabb\ effectively incorporates these interactions, enabling more efficient and accurate coordination in such challenging multi-agent environments. 

\paragraph{StarCraft Multi-Agent Challenge (SMACv2).} We now investigate \algoabb's scalability to SMACv2. We implement a 5v5 Protoss scenario with a team consisting of Stalkers, Zealots, and a Colossus. Stalkers and Zealots each have a 45\% chance of being deployed, while the Colossus has a 10\% chance. The scenario takes place on a 32x32 map with random starting positions and is set at difficulty level 7. \algoabb's performance closely align with those of DCG \edits{(see Figure \ref{fig:res:smacv2})}. Interestingly, QMIX outperforms both methods, confirming findings from previous research \cite{yu2022surprising}, which show that several value-based MARL methods excel in this testbed. The success of QMIX suggests that this domain \edits{may} not exhibit miscoordination issues like relative overgeneralization, which causes a less expressive value function (such as in QMIX and VDN) to undervalue potentially successful joint actions when they cannot distinguish between coordinated and uncoordinated actions \cite{bohmer2020deep,gupta2021uneven}. This makes SMACv2 less effective for demonstrating the full potential of advanced coordination graph formulations. Despite this, the comparable performance of \algoabb\ and DCG highlights that the enhanced ability to learn meta coordination graphs - capturing higher-order and indirect relationships - does not negatively impact our method's sample efficiency. The resilience against miscoordination issues, as shown earlier, underscores a key advantage of \algoabb\ over previous coordination graph methods. 

On further analysis of these results, we examined additional metrics, including test mean returns, average number of dead allies, and average number of dead enemies. We observed that \algoabb\ resulted in slightly fewer dead enemies on average compared to DCG, but it also maintained a lower number of dead allies. Moreover, \algoabb\ achieved a higher mean return over the episodes. We speculate that \algoabb\ agents are striking a better balance between aggression and preservation in this scenario. The slightly lower number of enemy casualties might suggest a more cautious or strategic approach, where agents prioritize survival and long-term success over immediate, aggressive tactics. This approach seems to result in fewer casualties among allies, which could contribute to the higher overall returns observed. \algoabb, through its ability to capture higher-order and indirect relationships among agents, is likely leading to a different cooperative strategy here, one which is trying to reduce casualties instead of focusing on winning. A more detailed study may be needed to further analyze \algoabb\ in this context, which we leave to future work. 

In summary, (1) \algoabb's success in these tasks shows its ability to explicitly model the locality of interactions and formal reasoning about joint actions given the coordination graph structure. (2) Its superior performance compared to value decomposition-based methods reassures the need for its design because a full decomposition can fail to learn policies or is much less sample efficient than \algoabb. (3) As the environments are made significantly more challenging to strongly exhibit miscoordination issues such as relative overgeneralization, successfully learning cooperative policies in them demonstrates \algoabb's ability to solve relative overgeneralization. (4) By outperforming DCG (significantly in Gather and Hallway; moderately in Pursuit and Disperse), our results show the need to go beyond encoding only pairwise interactions and capturing higher-order and indirect interactions among agents as well. (5) We show that \algoabb\ also scales to SMACv2 and performs at least as well as typical coordination graph formulations such as DCG. 

