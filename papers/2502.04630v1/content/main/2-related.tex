\section{Related Works}
\label{related_works}


We propose a multi-sensor fusion of RGB, event, and depth data using shared deformable Gaussian splatting representation. Our technique builds on recent advances in neural rendering, sensor fusion, and high-speed imaging. 
We outline these advancements and how our method builds upon or differs from them. 
For further details, we refer readers to resources on neural rendering \cite{xie2022neural, tewari2022advances, fei20243d}, event camera applications \cite{gallego2020event}, and depth sensing \cite{horaud2016overview, szeliski2022computer}.


\subsection{Neural Rendering}

Neural radiance field (NeRF) introduced by \citet{mildenhall2021nerf} and its extensions \cite{barron2021mip,zhang2020nerf++} represent 3D scenes implicitly using multilayer perceptrons (MLPs) for inverse differentiable rendering via analysis-by-synthesis.
This method has been adapted to dynamic scenes \cite{pumarola2020d, liu2022devrf} by conditioning NeRF on time \cite{li2021neural, du2021neural, xian2021space} or learning time-based transformations \cite{pumarola2021d,park2021nerfies,tretschk2021non,lombardi2019neural,gao2021dynamic},
with additional performance enhancements via incorporating explicit representations \cite{muller2022instant, mobilenerf, plenoxels, kulhanek2023tetra}.
More recent methods enhance efficiency through plane projection \cite{cao2023hexplane,fridovich2023k,shao2023tensor4d}, compact hashing \cite{wang2024masked}, neural point clouds \cite{kappel2024d}, and separated static and dynamic scene components \cite{song2023nerfplayer}.
Despite these improvements, \nerf-based methods demand numerous neural network evaluations per ray, especially in the empty regions, limiting training speed, and real-time rendering capability. While acceleration techniques like instant-ngp \cite{muller2022instant, chen2025neural} mitigate some inefficiencies,  
implicit representations remain slow and lack explicit deformability.

Recently, 3D Gaussian Splatting (3DGS) \cite{kerbl20233d} has demonstrated remarkable performance in computing the scene parameters, achieving real-time rendering with state-of-the-art quality. 
This method represents a 3D scene with a set of Gaussians with differentiable attributes such as position, scale, rotation, density, and radiance.
Since its introduction, 3DGS has been extended in several ways,
with improvements targeting anti-aliasing \cite{mip-splatting}, dynamic scene handling \cite{yang2023gs4d, duan20244d}, and optimized rendering for large scenes \cite{hierarchGS}; also see overview provided by \cite{fei20243d}.
Of particular relevance to this work are Gaussian splatting techniques adapted to dynamic scenes, such as per-frame Gaussian models \cite{luiten2024dynamic}, continuous deformations via neural networks \cite{xie2024gaussian, yang2024deformable}, deformation fields on planes \cite{wu20244d}, and trajectory modeling of Gaussians using polynomial and Fourier series fitting \cite{gao2024gaussianflow}. 
In this work, we leverage 3D Gaussians as a shared scene representation across different sensor modalities and utilize Gaussian splatting techniques for 3D reconstruction of deforming scenes from complementary multisensory data.



\subsection{High-speed Imaging}

Event cameras are sensors that transmit binary events per pixel, indicating an increase or decrease of the observed brightness. 
Their high dynamic range and rapid temporal resolution make them ideal for computational photography \cite{han2020neuromorphic, tulyakov2021time, tulyakov2022time}. 
Research on 2D applications of event cameras, including full-frame reconstruction \cite{kim2008simultaneous, rebecq2016evo, scheerlinck2018continuous} and deblurring \cite{haoyu2020learning, sun2022event}, have already seen extensive progress, while 3D imaging applications have started emerging at rapid pace in recent times \cite{hidalgo2020learning,baudron2020e3d,cui2022dense,uddin2022unsupervised,wang2022evac3d,kim2016real,rebecq2018emvs,hwang2023ev,low2023robust}.
Some early works combined event cameras with lasers or structured light projectors for 3D reconstruction \cite{brandli2014adaptive, matsuda2015mc3d}, while more recent research has explored reconstructing 3D scenes by integrating event cameras with stereo setups \cite{uddin2022unsupervised} or LiDAR \cite{cui2022dense}.
Notable progress has also been made in 3D event-based reconstruction using NeRF and 3DGS to achieve high multi-view coherence in dynamic scenes \cite{rudnev2023eventnerf,klenk2023nerf,hwang2023ev,low2023robust,ma2023deformable,xiong2024event3dgs,guo2024spikegs}.
Yet, these systems often struggle with the limited photometric data captured by event cameras, which provide only derivative information based on motion.
We overcome these challenges by fusing event cameras with RGB and depth sensors, and demonstrate high-speed imaging across diverse and challenging scenarios.







\input{content/figures/7-hardward_setup}

\subsection{Sensor Fusion}
To address the limitations inherent to using single sensing modality, sensor fusion and multimodal sensing algorithms have been widely explored.
Combining complementary data sources, such as depth and RGB images, has been extensively studied for refining depth estimation, leveraging the ubiquity of RGB-D cameras \cite{henry2012rgb, bleiweiss2009fusing, boominathan2014improving}. 
Radar fusion with cameras has enabled applications in object detection \cite{nabati2021centerfusion}, imaging in cluttered environments \cite{grover2001low}, and autonomous navigation. 
Lidar and optical time-of-flight sensors have similarly been used to image through scattering media \cite{bijelic2020seeing}, densify lidar scans \cite{kim2009multi}, and improve depth estimation \cite{lindell2018single, nishimura2020disambiguating, attal2021torf}.
To overcome the limitations of optical cameras, researchers have also fused optical sensors with acoustic sensors such as microphone arrays \cite{chakravarthula2023seeing,lindell2019acoustic} and sonars \cite{Ferreira2016,qadri2024aoneus,williams2004simultaneous,Raaj2016,babaee20153}.
Sensor fusion using event-based cameras is relatively a nascent field, with most approaches targeting applications of RGB and event camera fusion \cite{feng2020deep,liang2019multi,gehrig2021combining} for overcoming RGB-only camera limitations. 
Building on these concepts, we develop a sensor fusion approach integrating RGB, event and depth cameras to achieve high-speed capture of rapid real-world motion.







