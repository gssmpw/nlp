\section{Results}
\label{experiments}

\input{content/tables/synthesized_data}


This section evaluates our method on synthetic and real-world data. 
Experimental setup and analysis for simulated data are detailed in \subsectionautorefname{~\ref{syn_exp}}, while hardware prototype details and results for real-world dynamic scenes are in \subsectionautorefname{~\ref{real_world_exp}}. 
In \subsectionautorefname{~\ref{analysis}}, we analyze the impact of scene conditions---such as viewing angles, lighting, motion speed, and depth map quality---on performance and discuss our optimization strategies 
across dynamic scenarios.

\vspace{1mm}
\noindent
\textbf{Optimization Details}
Our method, implemented in PyTorch, is tested on an NVIDIA V100 GPU. 
We train our sensor-fusion \gs~approach using the Adam optimizer \cite{diederik2014adam}, fine-tuning parameters as outlined by \citet{kerbl20233d}. 
For the deformation field in \equationautorefname{~\ref{eq:deformation-field}}, 
we model deformations of $\mubm$, $\sbm$, and $\rbm$, scaling the time dimension of the deformation grid based on the degree of scene motion.
Event supervision (\equationautorefname{~\ref{eq:event-splat}}) uses temporal window length of $l_{min}=1$ ms across all scenes, with $l_{max}$ adjusted for illumination---smaller for bright scenes and larger for dim ones.
To optimize the final objective in \equationautorefname{~\ref{eq:final-objective}}, we initialize with a static vanilla \gs~model trained for 3000 steps, followed by training the full sensor-fusion model for 30,000 steps.


\subsection{Synthetic Evaluations}
\label{syn_exp}



\noindent
\textbf{Simulation and Synthetic Data.}
To evaluate our approach under diverse dynamic scene conditions, particularly those challenging to capture in real-world environments, we developed a simulator combining Blender \cite{blender} and ESIM \cite{rebecq2018esim}. 
Our synthetic dataset includes three custom scenes and one scene from \citet{ma2023deformable},
rendered at 1000fps and a resolution of $400\times 400$ pixels using Blender. Depth information was directly extracted from Blender, while events were simulated using ESIM.

We categorized synthetic scenes based on three characteristics:
baseline (large, small), lighting(bright, dark), and object speed(slow, fast). 
The scenes are:



\begin{itemize}
    \item \textit{\textbf{Hummingbird}}. 
    A hummingbird flaps its wings at very high speeds while the camera moves in a small circle around it. The baseline is small, lighting is bright, and object motion is fast.
    \item \textit{\textbf{Rotating Balls}}. In a dark setting, a red ball orbits a green ball, both self-rotating.
    The baseline is small, lighting is dark, and object motion is slow.
    \item \textit{\textbf{Sculpture}}. A sculpture disintegrates rapidly while the camera orbits around it.
    The baseline is large, lighting is bright, and object motion is fast
    \item \textit{\textbf{Lego}}. From \citet{ma2023deformable}, this scene features a Lego bulldozer with the bucket performing two sweeping motions, while the camera orbits around it.
    The baseline is large, lighting is bright, and object motion is slow.
\end{itemize}






\vspace{1mm}
\noindent
\textbf{Experiments and Assessment}.
We evaluate our experimental results using several metrics, including per-pixel accuracy (PSNR), perceptual quality (LPIPS)\cite{zhang2018unreasonable}, and depth root mean square (DRMS).
We compare our method to Deformable 3DGS\cite{yang2024deformable}, 4DGS\cite{wu20244d}, and Robust e-NeRF\cite{low2023robust}. For Robust e-NeRF\cite{low2023robust}, since we only have monochrome event data, we calculate the metrics in grayscale.
Quantitative results for our synthetic dataset are summarized in \tableautorefname{~\ref{tab:synthesis_data}}, and qualitative results are presented in \figureautorefname{~\ref{fig:general-results}}. As indicated in \tableautorefname{~\ref{tab:synthesis_data}}, our method consistently outperforms other methods across all quality metrics. Notably, in the visual results in \figureautorefname{~\ref{fig:general-results}}, our approach captures fine details in dynamic scenes, such as the accurate reconstruction of surfaces like the specular ball in column 1, and rapidly moving hummingbird's wings in column 2, where other methods falter.

\input{content/figures/depth}

\noindent
\textbf{Structural Accuracy}. Depth supervision enhances the accuracy of the 3D structure in our method. In \figureautorefname{~\ref{fig:depth}}, we present a depth map rendered from a viewpoint interpolated between training cameras. This supervision allows our method to effectively learn the 3D structure, distinguishing the object from the background more clearly than other methods. In \figureautorefname{~\ref{fig:extrapolate}}, we show images rendered from a viewpoint outside the training camera positions, along with a top-down view of the point cloud extracted from the Gaussians. Our method accurately positions the small red ball, whereas other methods either misplace it or represent it incompletely. The point cloud reveals that our method successfully captures the 3D structure of the red ball, unlike other methods that depict it as either a flat circle or merge it with the green ball.


\subsection{Real-World Evaluations}
\label{real_world_exp}

\noindent
\textbf{Hardware Prototype}. To validate our method, we built a hardware prototype consisting of three sensors: a FLIR Blackfly S machine vision camera for high-resolution RGB data, a a Prophesee EVK4 event-based camera for high-speed motion information, and LUCID Helios2+ depth camera for capturing spatial structure. These cameras are mounted around a speed-adjustable turntable, allowing precise control over scene rotation. A programmable LED light is positioned above the turntable to provide adjustable illumination. \figureautorefname{~\ref{fig:hardware_setup}} illustrates our hardware setup and the integration of the cameras with the turntable.

\vspace{1mm}
\noindent
\textbf{Real-World Data}. We capture three distinct scenes using the aforementioned hardware prototype. We capture the scenes for 5 seconds under 30 frame per second. We crop the RGB frames to size of 900$\times$900, the event data to size of 600$\times$600, and the depth data to size of 380$\times$380. The captured scenes are:
\begin{itemize} 
    \item \textit{\textbf{Dancing Toy}}: In a bright setting, a windup toy dances rapidly with its head waving in a dizzying speed. We set the turntable to rotate 72 degrees. The baseline is large, the lighting is bright, and the object speed is fast. 
    \item \textit{\textbf{Dancing Toy (Dark)}}: We repeat the \textit{\textbf{Dancing Toy}} scene, but with a dim illumination.
    \item \textit{\textbf{Newton's Cradle}}: A Newton's cradle bounces in a bright setting. We set the turntable to rotate 37.5 degrees. The baseline is small, the lighting is bright, and the object speed is fast. 
\end{itemize}

\noindent
\textbf{Experiments}. We visualize the reconstructed results in \figureautorefname{~\ref{fig:real_world_visualization}}. In the \textit{\textbf{Dancing Toy}} scene, our method achieves smoother motion interpolation, correctly maintaining the toy's head position, in contrast to other methods that position it incorrectly. For the \textit{\textbf{Dancing Toy (Dark)}} scene, our method consistently reconstructs smooth motion even in low-light conditions, unlike other methods that introduce artifacts around the neck or blur the face. However, we observe a color jitter effect around the eyes, which is a common phenomenon in monochromatic event-based reconstruction, as reported in \cite{xiong2024event3dgs,ma2023deformable}. For the \textit{\textbf{Newton's Cradle}} scene, the reconstruction of the specular surfaces of the balls is a significant challenge for \gs, as discussed in \cite{fan2024spectromotion, yang2024spec}. Despite these difficulties, our method successfully reconstructs the balls and ensures smooth interpolation between frames, whereas the other methods either introduce artifacts or completely omit the balls.\todo{add the dark scene discussion if we have it}

\subsection{Analysis}
\label{analysis}

Imaging quality is constrained by factors such as limited camera baselines, low lighting, or slow capture speeds. Reconstructing scenes under such extreme conditions is critical for many applications. Here, we demonstrate the robustness of our method under these limiting conditions. In \tableautorefname{~\ref{tab:conditions}}, we compare our method with 4DGS \cite{yang2023gs4d} under different conditions. In \figureautorefname{~\ref{fig:baseline_numframes}} and \figureautorefname{~\ref{fig:object_speed_lighting}}, we visualize the comparison across different conditions of our method.
\input{content/tables/conditions}
\input{content/figures/extrapolate}


\noindent
\textbf{Camera Baseline}. 
We evaluate the impact of different camera baselines on scene reconstruction quality using the \textit{\textbf{Rotating Balls}} scene, captured with \textit{large}, \textit{medium}, and \textit{small} baselines.
As shown in \tableautorefname{~\ref{tab:conditions}} and \figureautorefname{~\ref{fig:object_speed}}, our method achieves robust reconstruction quality even with the smallest baseline.




\vspace{1mm}
\noindent
\textbf{Number of Training Samples}. 
We investigate the performance of our method with varying number of training samples on the \textit{\textbf{Lego}} scene.
The results are presented in \tableautorefname{~\ref{tab:conditions}} and \figureautorefname{~\ref{fig:num_frames}}. 
Unlike other methods, which exhibit a significant decline in structural accuracy with fewer frames, our sensor fusion method remains robust even with sparse training samples.

\vspace{1mm}
\noindent
\textbf{Object Speed}. 
We evaluate reconstruction quality for objects moving at different speeds using the \textit{\textbf{Hummingbird}} scene, where the hummingbird flaps its wings at %
$2\times$ faster and $2\times$ slower speeds. 
As shown in \tableautorefname{\ref{tab:conditions}} and \figureautorefname{\ref{fig:object_speed}}, our method outperforms others in image quality, even at faster speeds. 
We observe that the structural accuracy modestly decreases at slower wing speeds due to increased overlap of the wings behind the body, making it more challenging  
for the Gaussians to accurately predict the 3D structure.

\vspace{1mm}
\noindent
\textbf{Lighting}. 
RGB camera performance often deteriorates under extreme lighting conditions, such as low or high dynamic range lighting, making robust reconstruction essential for many applications. 
In the \textit{\textbf{Sculpture}} scene, we evaluate our method under three lighting scenarios: \textit{Bright} (sufficient lighting), \textit{Dark} (low lighting), and \textit{Spotlight} (one side illuminated by a spotlight, the other side in shadow). 
The sculpture's color is unified to marble white across all scenes, with increased metallic appearance in the \textit{Spotlight} scene to enhance HDR effects.
As shown in \tableautorefname{~\ref{tab:conditions}} and \figureautorefname{~\ref{fig:lighting}}, our method demonstrates robust reconstruction across these challenging lighting conditions---see the reconstruction of surface cracks in all three cases. 
Note that in the \textit{Dark} setting, the PSNR for all methods is higher due to much of the sculpture appearing as a uniform black color, reducing detail variations. 
However, as seen in \figureautorefname{~\ref{fig:lighting}}, the visual quality in the \textit{Dark} setting is not necessarily better than under other lighting conditions.
