\section{Sensor Fusion for High-Speed 3D Imaging}\feedbackNeeded
\label{method}


The data provided by our multisensor fusion setup includes 
a set of RGB image frames $\Ical=\{\Ibm\}$, events $\Ecal=\{\ebm\}$, and depth maps $\Dcal=\{\Dbm\}$.
Given these,
our goal is to reconstruct the underlying high-speed, dynamic 3D scene. 
In \subsectionautorefname{~\ref{preliminary}}, we provide a brief overview of event camera data modeling and 3D Gaussian splatting.
In \subsectionautorefname{~\ref{subsec:event-splatting}}, we present our sensor fusion splatting framework, which enables joint optimization of 3D Gaussians and their temporal deformation fields for reconstructing high-speed, dynamic 3D scenes from multi-sensor data.
The details of our hardware imaging prototype are discussed in \sectionautorefname{~\ref{real_world_exp}}.

\subsection{Preliminaries}
\label{preliminary}

\noindent
\textbf{Modeling Event Camera}. 
An event camera operates with a grid of independent pixels, each responding to changes in the log photocurrent. 
In particular, each pixel asynchronously generates an event $\ebm_k = (\xbm_k, t_k, p_k)$ at time $t_k$ when the difference of logarithmic brightness $L$ at the pixel $\xbm_k = (x_k, y_k)^T$ reaches a predefined threshold $C$:
\begin{equation}
    L(\xbm_k, t_k) - L(\xbm_k, t_k - \Delta t_k) = p_k C,
\end{equation}
where 
$p\in\left\{- 1, + 1\right\}$ is the sign (or polarity) of the brightness change, and $\Delta t_k$ is the time since the last event at the pixel $\xbm_k$. 
The result is a sparse sequence of events which are asynchronously triggered by radiance changes.
For simplicity, we refer to an event at position $\xbm$ and time $t$ as $\ebm_{\xbm,t}$, with its corresponding polarity denoted as $p_{\xbm,t}$. 



\noindent
\\
\textbf{3D Gaussian Splatting}. \gs~represents a 3D scene explicitly as a set of anisotropic 3D Gaussians, with 
each Gaussian defined by its mean position $\mubm\in\R^3$ and covariance matrix $\Sigmabm\in\R^{3\times3}$.
To enable differentiable optimization, the covariance matrix is decomposed as $\Sigmabm=\Rbm\Sbm\Sbm^\top\Rbm^\top$, where $\Sbm\in\R^3_+$ is the scaling matrix and $\Rbm\in SO(3)$ is the rotation matrix. 
We simplify these matrices using the 
quaternion $\rbm\in\R^4$ for rotations and the scaling factor $\sbm\in\R^3$. 
For image rendering, the 3D Gaussians are projected (splatted) onto a 2D camera plane, and the mean position $\mubm$ and covariance matrix $\Sigmabm$ are transformed using
the following equations:
\begin{equation}
\mubm^{\prime}=\Pbm \Wbm \mubm,
\end{equation}
\begin{equation}
\Sigmabm^{\prime}=\Jbm\Wbm \Sigmabm \Wbm^\top \Jbm^\top,
\end{equation}
where $\mubm^{\prime}$ and $\Sigmabm^\prime$ are the projected 2D mean position and covariance matrix, and $\Pbm$, $\Wbm$, and $\Jbm$ represent the projective transformation, viewing transformation, and the Jacobian of the affine approximation of $\Pbm$, respectively. 
After projection, we apply alpha blending to calculate the final pixel value $C(\xbm)$ at pixel $\xbm$ by blending the $N$ ordered Gaussian along the ray shot from the pixel:
\begin{equation}
\label{alpha_blending}
C(\xbm)=\sum_{i \in N} \cbm_i \sigma_i \prod_{j=1}^{i-1}\left(1-\sigma_j\right),
\end{equation}
where $\cbm_i$ denotes the view-dependent radiance,
$\sigma_i$ denotes the density of each Gaussian, and $\prod(\cdot)$ denotes the cumulative visibility.
Finally, given a set of target images, Gaussian splatting frames 3D reconstruction as an inverse problem, and iteratively refines the parameters of 3D Gaussians $G(\mubm, \rbm, \sbm, \cbm, \sigma)$ to minimize the difference between projected splats and actual image data, resulting in high-fidelity scene reconstructions.








\subsection{Sensor Fusion Splatting}
\label{subsec:event-splatting}

As illustrated in \figureautorefname{~\ref{fig:overall}}, our method leverages RGB for color, event cameras for high-speed motion, and depth sensors for spatial structure, to
enable robust, high-quality 3D reconstructions of fast-moving, complex scenes. 
Three tailored loss functions ensure precise and consistent optimization across all sensor data.

\vspace{1mm}
\noindent
\textbf{Modeling Deformable Scenes}.
Instead of training separate sets of 3D Gaussians for each time-dependent RGB view and interpolating between them, we capture continuous motion and reconstruct 3D scenes with spatio-temporal consistency.
By decoupling motion from geometry, we map 3D Gaussians into a canonical space, creating time-independent representations guided by geometric priors.
These priors link 3D Gaussian positions to spatio-temporal changes, supervised using multi-sensor data.
To efficiently encode scene dynamics, we employ a grid encoder \cite{cao2023hexplane, wu20244d}, which maps spatial and temporal features onto 2D grids, and thereby accelerating rendering.

Specifically, we sample the encoded features from a given time $t$ and the 3D Gaussian parameters $\mubm$ as inputs, and decode the offset values that deform the 3D Gaussians $\Delta G$ using a multilayer perceptron (MLP) network. 
This deformation is given as:
\begin{equation}
   \Delta G(\mubm,\sbm,\rbm) = M_{\gbm, \thetabm}(\mubm, \gamma(t)),
   \label{eq:deformation-field}
\end{equation}
where $M_{\gbm, \thetabm}$ denotes the MLP producing the deformation field, $\gbm$ denotes the grid features, ${\thetabm}$ represents the MLP parameters and 
where 
$\gamma(t) = (\sin(2^k \pi t), \cos(2^k \pi t))$ denotes positional encoding.
Novel images are then generated by splatting the deformed 3D Gaussians updated with parameters obtained from the deformation field.



\vspace{1mm}
\noindent 
\textbf{Event Supervision}. 
We splat the 3D Gaussians based on the events provided by the event camera and the deformable scene modeling discussed above, to reconstruct high-speed dynamic scenes.
Specifically, integrating the event information, which provides dense log intensity changes over time, in optimizing the scene parameters complement the temporal sparsity of traditional RGB or depth cameras.
To optimize the Gaussians using temporal events, we start by randomly selecting two timestamps, $t_s$ and $t_e$ (with $t_s<t_e$), from a given set of events. 
We model the ground truth log-intensity difference $\Delta\Lbm^*\in\R^{H\times W}$, between $t_s$ and $t_e$ as:
\begin{equation}
\Delta\Lbm_{t_s\veryshortarrow t_e}(\xbm)=\begin{cases}
\int_{t_s}^{t_e} \eta p_{\xbm,t}\diff t & \text{if } \{\ebm_{\xbm,t}\mid t_s<t<t_e\}\ne \emptyset \\
0 & \text{otherwise}
\end{cases},
\end{equation}
where $\eta$ is the contrast threshold, and $p_{\xbm,t}$ represent the event polarity.
For supervising and optimizing temporally consistent Gaussian deformations, we also predict the log-intensity difference between splatted images $\{\widehat{\Ibm}\}$ at the selected times $t_s$ and $t_e$:
\begin{equation}
\Delta \widehat{\Lbm}_{t_s\veryshortarrow t_e} = \log{\widehat{\Ibm}_{t_e}}-\log{\widehat{\Ibm}_{t_s}}.
\end{equation}

A key consideration to note while simulating event data is that the time window $t_e-t_s$ significantly impacts reconstruction quality: shorter windows inhibit propagation of broader lighting changes, while longer windows compromise fine details due to event neutralization -- consecutive events of opposite polarity canceling each other -- inevitably caused during the signal integration time.
This was also observed in some previous studies simulating event data \cite{rudnev2023eventnerf,low2023robust,xiong2024event3dgs}.
To address this constraint, we adopt two strategies:
(1) we randomly sample the window length as $t_e-t_s\sim\Ucal[l_{min},l_{max}]$ to balance temporal resolution with event density ($l_{min}$ and $l_{max}$ being the minimum and maximum window lengths) , and 
(2) to prevent misinterpretation of neutralized events during integration, we apply a mask to exclude these canceled events from loss calculation. This approach ensures they are not mistaken for zero events, avoiding unnecessary penalization for actual intensity variations.
Therefore, the optimization objective for splatting based on event signals is given as:
\begin{equation}
\Lcal_{event} = |\Delta\Lbm_{t_s\veryshortarrow t_e} - \Delta \widehat{\Lbm}_{t_s\veryshortarrow t_e}|_2\odot\mbm,
\label{eq:event-splat}
\end{equation}
where $\mbm$ denotes the event mask.

While directly supervising with raw event data has been widely used for \gs~and \nerf~training \cite{klenk2023nerf, rudnev2023eventnerf, low2023robust, xiong2024event3dgs}, raw events in real-world, multi-sensor, and dynamic scenarios can become corrupted by noise and misaligned due to calibration errors. 
Empirically, we observe that directly training with raw events introduce wavering artifacts and jumpy background, ultimately degrading the rendering performance. 







To address these issues, we additionally supervise our training using reconstructed frames from the E2VID method \cite{rebecq2019high}. 
We observe that naively minimizing MSE or L1 loss between rendered \gs~frames and reconstructed E2VID frames yields temporal inconsistencies and fog-like artifacts in the rendered frames due to inherent calibration errors and misalignments.\todo{may need to show these artifacts in sup and ref here}
To mitigate these artifacts and improve temporal consistency, we employ the Learned Perceptual Image Patch Similarity (LPIPS) metric \cite{zhang2018unreasonable} as a perceptual loss $\Lcal{lpips}$:

\begin{equation} 
\Lcal_{lpips} = \text{LPIPS}(\Ibm_t, \widehat{\Ibm}_t),
\end{equation}
where, at a time $t$, $\Ibm_t$ is reconstructed from events via E2VID, and $\widehat{\Ibm}_t)$ is rendered at the same time $t$ using \equationautorefname{\ref{alpha_blending}}. 
By aligning the \gs~generated frames with E2VID reconstructions, we reduce visual artifacts in renderings and enhance temporal stability.
Since E2VID produces only grayscale images, we integrate RGB camera data into the pipeline to provide the necessary color information.

\vspace{1mm}
\noindent 
\textbf{RGB Supervision}.
Given a ground truth image $\Ibm_t\in\R^{H\times W\times 3}$ at time $t$, an image $\widehat{\Ibm}_t$ is rendered via 3DGS using \equationautorefname{~\ref{alpha_blending}}. The objective function for training 3D Gaussians $G$ and MLP (parameterized by ${\thetabm}$) with given multi-view RGB images is given by

\begin{equation}
\Lcal_{rgb} = |\Ibm_t-\widehat{\Ibm}_t|.
\end{equation}
\noindent
This RGB image supervision provides necessary color information. 





\vspace{1mm}
\noindent
\textbf{Depth Supervision}. 
To ensure structural consistency throughout the spatio-temporal changes in the scene, we incorporate supervision using depth maps from a depth sensor. To this end, we render the depthmap $\widehat{\Dbm}_t$ using \equationautorefname{~\ref{alpha_blending}}, but replace $\cbm$ with the z distance of the gaussians in camera space.
The depth-based loss function is defined as:

\begin{equation}
\Lcal_{depth}=|\Dbm_t-\widehat{\Dbm}_t|.
\end{equation}

where $\Dbm_t\in\R^{H\times W}_{+}$ represents the ground truth depth map at time $t$. 
Incorporating depth splatting encourages accurate depth alignment during reconstruction of fast-moving scenes, reinforcing structural consistency across frames.

\vspace{1mm}
\noindent
\textbf{Overall Loss Function}. 
In addition to the aforementioned objectives, we apply a second-order temporal smoothness regularization \cite{cao2023hexplane} to the deformation grid features $\gbm$ from \equationautorefname{~\ref{eq:deformation-field}}.
This regularization prevents artifacts and enhances temporal smoothness of reconstructed high-speed scenes, and is computed as:
\begin{equation}
    \Lcal_{\gbm} = \|\nabla^2_{t}\gbm\|^2
\end{equation}
The final objective function for our approach is given by:

\begin{equation}
\Lcal=\lambda_1 \Lcal_{rgb}+\lambda_2 \Lcal_{event} + \lambda_3 \Lcal_{lpips} + \lambda_4 \Lcal_{depth} + \lambda_5 \Lcal_{\gbm},
\label{eq:final-objective}
\end{equation}
where the $\lambda$s are the weight balancing factors.



