\section{Introduction}\feedbackNeeded
\label{intro}

Capturing high-speed 3D phenomena---such as the flapping wings of a hummingbird, explosions, or industrial processes---has wide-ranging applications in science and engineering. 
However, designing imaging systems for these scenarios is challenging due to their extremely high bandwidth demands and limited baseline geometries. 
Ultra-high-speed cameras (e.g., Phantom, Photron, IDT), although can achieve remarkable frame rates, are prohibitively expensive (over \$100k), offer short capture durations (only a few seconds), lack depth information, and generate massive data volumes. 
Currently, no single imaging modality can capture dynamic scenes with high spatial, temporal, and depth resolutions. 
In this work, we address this limitation by integrating three complementary imaging modalities---RGB cameras, event cameras, and depth cameras---which collectively provide spatial, temporal, and depth information.

RGB cameras are inexpensive and provide high 2D spatial resolution along with color information about the scene. However multiple RGB cameras struggle to achieve high-resolution depth information in limited baseline scenarios~\cite{qadri2024aoneus, qu2024z}, due to the missing cone problem. 
Additionally, RGB cameras face challenges in low-light conditions and are constrained by bandwidth limitations, resulting in low frame rates. 

Event cameras, in contrast, offer ultra-high temporal resolutions (down to microseconds) by capturing only pixel-level changes in brightness. They transmit asynchronous sparse data, known as events, which significantly reduces data redundancy and storage demands. Furthermore, their high dynamic range (HDR) enables them to capture scenes with extreme lighting contrasts, which would otherwise appear washed out or darkened with conventional RGB cameras. 
These features make event cameras an excellent complement to RGB cameras for capturing high-speed scenes.
However, neither of these cameras provide depth information, which complicates reconstruction in small baseline scenarios.

Depth sensors are shown to sample data within the missing cone, capturing essential information that RGB and event cameras cannot \cite{qu2024z}. 
Active depth cameras, such as SPADs \cite{gupta2019photon, folden2024foveaspad, gupta2019asynchronous, jungerman20223d, po2022adaptive} or CWToF sensors \cite{keetha2024splatam, friday2024snapshot, he2019recent, shrestha2016computational}, measure the distance between the camera and the scene, providing necessary depth information for accurate reconstruction, especially in small baseline conditions. 


In this work, we integrate information from all three complementary sensory modalities to enable reconstruction of high-speed 3D scenes in small baseline scenarios.
As shown in \figureautorefname{~\ref{fig:overall}}, the image representation of each modality differ significantly: RGB cameras provide color images, event cameras provide pixel-level intensity changes at extremely high speed, and depth cameras generate depth maps.
Fusing this sensory information requires a shared scene representation.
To achieve this, we represent the underlying scene using deformable 3D Gaussians that are shared across all imaging modalities.
We render data from RGB, event and depth cameras through a Gaussian splatting framework, and jointly optimize the 3D Gaussian parameters and their temporal deformation model by
minimizing the differences between the rendered splatting outputs and the observed multi-view, multi-sensory data.
This approach enables reconstruction of high-resolution multiview images and depth maps, achieving enhanced spatial, temporal, and angular fidelity. 
\todo{do we show any multiview images? if not we should.}

We rigorously evaluate our approach on diverse synthetic scenes and real-world scenes captured with our sensor fusion prototype (shown in \figureautorefname{~\ref{fig:hardware_setup}}), including challenging scenarios with rapid motion and low-light conditions---scenarios where traditional cameras or single modalities often fail (see \figureautorefname{~\ref{fig:real_world_applications}}). \todo{do we have real evaluations for all three scenarios?}
Our results demonstrate that the proposed multi-sensor fusion approach significantly improves reconstructions, delivering both fidelity and robustness in capturing high-speed, dynamic scenes.

In summary, we make the following contributions:
\begin{enumerate}
    \item We \textit{propose} Sensor Fusion Splatting, a novel deformable 3D Gaussian splatting framework that integrates RGB, event, and depth data streams to enable robust, high-speed 3D scene reconstruction and consistent photorealistic novel view synthesis from multi-sensor inputs.
    
    \item We \textit{develop} a hardware sensor fusion prototype that combines RGB, event, and depth cameras to capture high-speed deformable 3D scenes, supporting our proposed method. \todo{this point seems like a repetition of the above point}

    \item We rigorously evaluate our approach on diverse synthetic and real-world datasets featuring high-speed, deforming 3D scenes, demonstrating significant performance improvements in scene reconstruction.
\end{enumerate}
\noindent
Our code and data will be made public upon acceptance of the manuscript.






    

