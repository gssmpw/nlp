With the rapid expansion of Earth Observation~(EO) satellite missions, deep learning has emerged as a powerful solution for key EO applications, ranging from monitoring natural resources \cite{Chen_etal_2023, Fayad_etal_2024, Li_etal_2023a, Liu_etal_2022, Pauls_etal_2024, schwartz2023forms} and assessing environmental impacts \cite{Dalagnol_etal_2023, Wagner_etal_2023}, to climate monitoring and forecast \cite{Andrychowicz_etal_2023, Schultz_etal_2021}, as well as agricultural and land resource management \cite{Ienco_etal_2019, Kussul_etal_2017, Zhang_etal_2019}.

While research of new deep learning architectures and upcoming remote satellite missions are expected to enhance existing EO applications, 
supervised deep learning approaches have key limitations: First, these models generally depend on labeled data, often requiring large annotated datasets. For example, the estimation of above-ground biomass~(AGB) still heavily relies on traditional methods due to limited ground-based measurements and the need to use other variables as proxies~\cite{Morin_etal_2023, schwartz2023forms}. 

Second, EO models are often tailored to specific tasks, limiting their adaptability and reusability, even when using similar input data sources. For instance, a land cover classification model cannot easily predict canopy height due to differences in input-output relationships, as the network may prioritize learning features that are highly discriminative for a given task but not the other. 

Third, EO models often use optical or radar imagery to predict single-valued targets like canopy height or land cover classes. However, they struggle with more complex outputs like the full vertical structure of vegetation.\footnote{Capturing such vertical structures is essential for understanding biomass allocation and species diversity. Predicting vertical structure from EO imagery is challenging due to its complexity and limited sensor data, requiring cross-modal understanding over direct prediction \cite{Tan_etal_2024}.}

Foundation models (FMs) offer a promising solution to the aforementioned limitations by leveraging self-supervised pre-training on vast amounts of unlabeled data \cite{Brown_etal_2020,Devlin_2018,Kirillov_etal_2023,Minderer_etal_2022,Radford_etal_2021}. For EO applications, such models are being developed to process diverse data types, including time series processing \cite{Yuan_etal_2022}, adaptation to different input satellite sensors \cite{Xiong_etal_2024} or varying Ground Sample Distances (GSD) \cite{Reed_etal_2023}, and multi-modal data fusion \cite{Astruc_etal_2025, astruc2024anysat, Fuller_etal_2024}.
Current foundation models primarily address the adaptability issue, but not label scarcity. In fact, most EO-focused FMs are pre-trained masked auto-encoders (MAEs), which require extensive fine-tuning \cite{Lehner_etal_2024,Singh_etal_2023}. Moreover, existing FMs, even multi-modal ones, only leverage images. 
As such, they excel in EO tasks requiring horizontal structural understanding (e.g., land cover mapping, tree species identification), but struggle with EO applications requiring vertical structure understanding (e.g., canopy height estimation).
Additionally, their patch-based approach limits direct pixel-level prediction capabilities. It also hinders the integration of LiDAR data, such as full-waveform LiDAR data available as labels at higher resolutions than the patch scale. 

In this work, we propose a \emph{Dense Unsupervised Nature Interpretation Algorithm} (DUNIA) that learns to generate pixel-level embeddings by aligning vertical structure information from a space-borne full waveform LiDAR with satellite imagery through contrastive learning. This dual alignment strategy enables both vertical and horizontal structure understanding, supporting diverse EO applications with minimal, if any, training. DUNIA achieves strong cross-modal retrieval performances, excels in forest structure benchmarks, and enables per-pixel vertical and horizontal structure retrieval. In our experiments, we demonstrate the effectiveness of the resulting embeddings for seven key EO applications and show that zero-shot classification operating on these embeddings can outperform specialized supervised EO models. A simplified version of our approach is presented in ~\cref{fig:overview}.

\section{Background \& Contribution}
We sketch the related work along with the contributions made. 

\subsection{Contrastive Learning}
Contrastive learning trains networks to create similar embeddings for related data and dissimilar embeddings for unrelated data, where related samples can be either different augmentations of the same input or inputs from different modalities. In this context, one of the main challenges is to avoid a so-called model collapse, in which case the model outputs become constant. 
This is typically addressed through (i) architectural designs or (ii) specialized objective functions. 

Prominent examples of architectural designs are BYOL~\cite{Grill_etal_2020} and SimSiam~\cite{Chen_etal_2021}, which respectively use asymmetry to prevent collapse, BYOL employing a momentum encoder and SimSiam using a stop-gradient operation with a predictor network. 
Specialized objective functions can be categorized into pair-based contrastive losses, clustering losses, and negative-pair-free losses. Here, pair-based losses, such as those presented in SimCLR~\cite{Chen_etal_2020} and MoCo~\cite{He_etal_2020}, align positive pairs and separate negative pairs, but assume unique positive pairs within a mini-batch, which is not always true for EO data, especially at high resolutions. Clustering methods like SeLa~\cite{Asano_etal_2019} and SwAV~\cite{Caron_etal_2020} relax this constraint by performing the contrastive objective on data clusters. Negative-pair-free methods, such as Barlow Twins~\cite{Zbontar_etal_2021} and VICReg~\cite{bardes2021vicreg}, focus on redundancy reduction in the feature dimension. Zero-CL \cite{Zhang_etal_2021} uses whitening transformations on the embeddings to maximize the trace of the cross-covariance matrix, achieving alignment and redundancy reduction without negative pairs.

\subsection{Multi-Modal Learning} 
Multi-modal self-supervised approaches can be categorized into three main categories. (1) Modality-agnostic models use a shared network for different modalities~\cite{Carreira_etal_2022,Girdhar_etal_2022,Jaegle_etal_2021}, but are typically limited to on one modality at a time during training, restricting cross-modal knowledge sharing \cite{Srivastava_etal_2024}. For EO applications, several models follow this approach. Scale-MAE~\cite{Reed_etal_2023} and DOFA~\cite{Xiong_etal_2024} both process imagery across varying resolutions, the latter also processes different wavelengths using a common encoder. (2) Fusion-encoder models integrate data from multiple modalities through cross-modal attention, effectively combining information from each modality~\cite{Bachmann_etal_2022, Bao_etal_2021, Singh_etal_2022}. OmniSat~\cite{Astruc_etal_2025} and AnySat~\cite{astruc2024anysat} exemplify this by integrating data from diverse EO sources into a unified representation. (3) Finally, multi-encoder approaches use separate encoders for each modality, which are then aligned in a shared latent space. This strategy is highly effective for tasks such as cross-modal retrieval and zero-shot classification across different modalities: image/text~\citep{Radford_etal_2021, Jia_etal_2021, Yuan_etal_2021, Yu_etal_2022}, audio/text~\cite{Guzhov_etal_2022}, and video/text~\citep{Luo_etal_2022, Pei_etal_2023}. 

\subsection{Contribution}
Our work aligns closely with the abovementioned third category, focusing on mono- and cross-modal retrieval within the multi-encoder approach. Unlike prior models that align paired data at the instance level (\eg, matching an image to its textual description), here we perform pixel-level alignment. This is necessary for dense predictions required for EO applications, and preserves the spatial resolution to match full LiDAR waveform footprints.

Specifically, our approach aligns sparse LiDAR waveforms with high-resolution imagery through contrastive learning to understand the vertical structure (i.e., pixel-waveform alignment) and simultaneously performs pixel-pixel alignment for horizontal structure understanding. DUNIA achieves strong cross-modal retrieval, surpasses specialized models in forest structure benchmarks, and enables per-pixel vertical structure retrieval. For land cover understanding, it performs competitively in zero-shot and low-shot evaluations. Additionally, its embeddings can directly generate waveforms representing the forest's vertical structure from pixel inputs â€” a task not possible with existing methods.
