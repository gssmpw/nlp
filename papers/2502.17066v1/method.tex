Our goal is to learn two pixel-scale embedding spaces for two different EO data modalities using a single model, enabling simultaneous horizontal and vertical structural understanding. The vertical data come from GEDI \cite{Dubayah_etal_2020}, a spaceborne instrument that acquires sparse forest vertical structure measurements through 1D LiDAR waveforms and the horizontal data corresponds to 10 m resolution imagery from the Sentinel- 1 \& 2 satellite missions (see \autoref{appendix:experimental_settings} for additional details).
Our framework is composed of two auto-encoders~(AEs) and a third model that aligns, at the pixel-level, its output embeddings to the outputs of the two AEs. The two auto-encoders are, respectively, a multi-temporal image AE, and a full waveform AE. The third model is an encoder-decoder~ type of model, with two similar, but separate decoders. Next, we provide some details on the main building blocks of the three main models, the pre-training objective and the latent diffusion model used for waveform generation. Full descriptions are deferred to \autoref{appendix:implementation_details} and the detailed model is presented in ~\cref{fig:dunia_overview}. 

\begin{figure*}[ht]
\begin{center}
\includegraphics[width=0.95\textwidth]{Figures/diagram.png}
\end{center}
\vskip-0.3cm
\caption{An overview of our framework showcasing the main blocks and connections. The pre-training model, which take as input as single date S-1 \& 2 image mosaic, is shaded in blue. The multi-temporal image AE, which take as input a multi-date S-1\& S2 image mosaic, is shaded in yellow. The vertical structure AE, which take as input the GEDI waveforms, is shaded in green.
}
\label{fig:dunia_overview}
\vskip-0.14cm
\end{figure*}

\subsection{Pre-training Model}
\label{sec:image-encodec}
The pre-training model in DUNIA (\cref{fig:dunia_overview}) takes an image $\mathcal{I} \in \mathbb{R}^{H \times W \times C}$ and produces pixel level embeddings in $\mathbb{R}^{H \times W \times D_p}$. $C$ is the number of input channels, $(H,W)$ is the image resolution, and $D_p$ is the target projection dimension. $\mathcal{I}$ is a median composite image generated from Sentinel- 1 \& 2 observations over several dates. The model comprises five building blocks: a patch embedding, a shared encoder, two similar decoders, neighborhood attention layers and several projection heads. 

Following is a brief description of each of these blocks. 
\vspace{-0.3cm}
\paragraph{Patch Embedding.} The 2D composite image $\mathcal{I}$ is split into $ N = HW/P^2$ patches of dimension $N\times D$ using a convolutional layer, with $P$ and $D$ the patch size and the patch embedding dimension respectively.
\vspace{-0.3cm}
\paragraph{Encoder Architecture.} The encoder consists of 16 standard transformer layers \citep{vaswani}, organized into 4 Transformer Blocks (TB), each containing four layers. Each layer takes and outputs a sequence of image tokens in $\mathbb{R}^{N \times D}$.
\vspace{-0.3cm}
\paragraph{Decoder Architecture.} The two decoders transform the encoder outputs into two sets of pixel-sized embeddings ($\mathcal{O}^\mathcal{V}$, $\mathcal{O}^\mathcal{H} \in \mathbb{R}^{H \times W \times D_p}$) for pixel-waveform and pixel-pixel alignment respectively. They use a hierarchical structure inspired by Ranftl et al. \yrcite{ranftl2021vision} that progressively upsamples features while combining information from different encoder layers to maintain both fine and coarse details. Each decoder block ($d \in \{1,2,3,4\}$, with $d=1$ representing the final decoder layer) produces an image-like feature map of shape $\frac{H}{2^{(d-1)}} \times \frac{W}{2^{(d-1)}} \times D_{p_{d}}$, where \(D_{p_d} = 2^{p-1}D_p\) is the feature dimension out of block $d$. We refer to the output at these stages for a given decoder as $\mathcal{O}_d$. For simplicity, we will henceforth refer to $\mathcal{O}_1$ as $\mathcal{O}$.  
\vspace{-0.3cm}
\paragraph{Neighborhood Attention (NA).} To enhance local spatial relationships, we add two NA layers \cite{hassani2023natten} for each decoder block at the highest resolution ($d=1$), where each pixel attends to its surrounding window of size $w$. This complements the global attention from the transformer layers with explicit element relation modeling mechanism.
\vspace{-0.3cm}
\paragraph{Projection Head.} For the pixel-pixel alignment objective, we follow standard practice and append a projection head for each output of $\mathcal{O}^\mathcal{H}_d$. As the projection head becomes more specialized towards the training objective \cite{xue2024projectionhead}, its output becomes less generalizable when training and downstream objectives are misaligned, and as such it is discarded after training. For the pixel-waveform alignment, $\mathcal{O}^\mathcal{V}$ does not use a projection head, since the downstream tasks (e.g., waveform generation) align well with the training objective.

\subsection{Waveform Processing Model}
\label{sec:waveform_ae}
The waveform AE (\cref{fig:dunia_overview}) is based on a VQ-VAE (Vector Quantized Variational Autoencoder) \citep{oordVQVAE} architecture. It consists of three primary components: a waveform encoder, a residual vector quantizer (RVQ), and a waveform decoder. The input to this model are GEDI LiDAR waveforms ($\mathcal{W}$) available only for certain pixel locations in the composite image $\mathcal{I}$.
\vspace{-0.3cm}
\paragraph{Waveform Encoder.} The waveform encoder ($\mathcal{E}_w$) uses a ResNet-based architecture to process 1D waveform inputs through multiple stages, progressively reducing their length while increasing feature representation capacity. $\mathcal{E}_w$ transforms a waveform ($\mathcal{W}$) of shape $\mathbb{R}^{L \times 1}$ into a latent representation $z_e \in \mathbb{R}^{L/16 \times 16}$. To align $z_e$ with its corresponding pixel embedding from $\mathcal{O}^\mathcal{V}$ we perform a channel-wise average pooling followed by a linear projection. This process transforms $z_e$ into $\mathcal{O}^\mathcal{W} \in \mathbb{R}^{D_p}$.
\vskip-0.1cm
\paragraph{Residual Vector Quantizer (RVQ).}\label{paragraph:rvq_layer} For regularization by enforcing discrete representations, we use an RVQ layer with $Q$ quantizers each containing $V$ codebook vectors. The RVQ iteratively quantizes the latent waveform representation $z_e$ through a sequence of quantizers, with each processing the residual from the previous one to produce the final quantized representation $z$.
\vskip-0.1cm
\paragraph{Waveform Decoder.} The waveform decoder $\mathcal{D}_w$ mirrors the structure of the encoder but operates in reverse order to reconstruct the waveform from its quantized latent representation $z$. In essence, $\mathcal{D}_w(z) = \hat{\mathcal{W}}$. While waveform reconstruction is not necessary for the alignment, reconstructing directly from aligned encoded waveforms ensures that the future generation process operates on a waveform latent space that is already structured based on the shared semantic and structural alignment with the pixel embeddings. This approach is beneficial as we are interested in generating waveforms given pixel inputs.

\subsection{Multitemporal Image Processing Model}\label{sec:multitemproal_ae} The multitemporal image model (\cref{fig:dunia_overview}) is designed to process temporal sequences of median composite images, following an autoencoder (AE) architecture. The input to this model is a sequence of images $\mathcal{M} \in \mathbb{R}^{T \times H \times W \times C}$ covering the same area as the input to the pre-training model. These images come from $T$ Sentinel- 1 \& 2 acquisition dates that overlap with the dates used to generate the composite image $\mathcal{I}$. This design choice, unlike multi-temporal pre-training models that require time series data during training and inference, or mono-temporal models limited to a single date, enables the model to capture some phenological traits without needing time series data during inference.

\paragraph{Multi-temporal Image AE.} This AE follows a UNet structure, but replaces conventional convolutional blocks with ConvLSTM \cite{shi2015convlstm} layers to explicitly model temporal correlations. The model takes a multi-temporal input image $\mathcal{M}$ and produces a feature map $\mathcal{X} \in \mathbb{R}^{T \times H \times W \times D_p}$, which is then reconstructed back to a tensor $\mathcal{\hat M}$ of the same shape as the inputs. Each decoder block produces an image-like feature map of shape $\frac{H}{2^{(d-1)}} \times \frac{W}{2^{(d-1)}}$ denoted as $\mathcal{X}_d$ at the corresponding decoder stage.

\paragraph{Temporal Pooling.} To align the output embeddings from the multi-temporal image AE with those from the pre-training model, we first perform temporal average pooling. This transforms $\mathcal{X}_d \in \mathbb{R}^{T \times \frac{H}{2^{(d-1)}} \times \frac{W}{2^{(d-1)}} \times D_p}$ to $\mathcal{O}^\mathcal{T}_d\in \mathbb{R}^{\frac{H}{2^{(d-1)}} \times \frac{W}{2^{(d-1)}} \times D_p}$, which then passes through a projection head. For the output with the highest resolution (i.e., $\mathcal{O}^\mathcal{T}_1$) we also append two NA layers before the projection head.  For simplicity we refer to $\mathcal{O}^\mathcal{T}_1$ as $\mathcal{O^T}$.

\subsection{Pre-Training Objective}\label{se:pretraining_objective}

DUNIA is pre-trained on pixel-waveform and pixel-pixel alignment, alongside modality reconstruction for the modality-specific AEs. For the alignment task, we rely on two, similarly performing, non-negative-pair contrastive losses, namely VICReg \cite{bardes2021vicreg} and Zero-CL \cite{Zhang_etal_2021}. The selection between either loss functions is determined by the number of available elements within a mini-batch and the requirements of each loss function. VICReg requires large batch sizes to perform well \cite{bardes2021vicreg}, which is a non-issue for the pixel-pixel contrastive objective given the high number of pixels available in each mini-batch of images. However, this is not suitable for the waveforms which are few in number within the mini-batch. On the other hand, Zero-CL performs well even for small batch sizes but faces computational bottlenecks with very large batches. Consequently, we adopt Zero-CL for pixel-waveform alignment and VICReg for pixel-pixel alignment. The formulation of these two objectives are detailed below.

\subsubsection{Pixel-Waveform Alignment} Zero-CL replaces the alignment and uniformity terms in negative-pair-based contrastive losses \cite{Arora_etal_2019} with, respectively, an instance-wise contrastive loss ($\mathcal{L}_{\text{Ins}}$) and a feature-wise contrastive loss ($\mathcal{L}_{\text{Fea}}$). The overall loss $\mathcal{L}_{Zero-CL}$ is $\mathcal{L}_{\text{Fea}} + \mathcal{L}_{\text{Ins}}$. $\mathcal{L}_{Zero-CL}$ is applied on \( Z^A \in \mathbb{R}^{G\times D_p} \), the $L_2$ normalized pixel embeddings from $\mathcal{O}^\mathcal{V}$, and \( Z^B \in \mathbb{R}^{G \times D_p} \) their corresponding $L_2$ normalized waveform embeddings from $\mathcal{O}^\mathcal{W}$, where \( G \) is the number of available GEDI samples in a given mini-batch. The mathematical formulation for this loss can be found in \autoref{appendix:objective_functions}.

\subsubsection{Pixel-Pixel Alignment} VICReg is formulated as three loss terms: variance ($\mathcal{L}_{\text{var}}$), invariance ($\mathcal{L}_{\text{inv}}$), and covariance ($\mathcal{L}_{\text{cov}}$). Let \( Z^\mathcal{H}_{d} \in \mathbb{R}^{M_d\times D_{p_d}} \) represent the $L_2$ normalized pixel embeddings from \( \mathcal{O}^\mathcal{H}_d\), \( M_d = B \times \frac{H}{2^{(d-1)}} \times \frac{W}{2^{(d-1)}} \) with $B$ the mini-batch size, $\frac{H}{2^{(d-1)}}$ and $\frac{W}{2^{(d-1)}}$ the height and width of a decoder's output feature map. \( Z^\mathcal{T}_d \in \mathbb{R}^{M_d \times D_{p_d}} \) is their corresponding $L_2$ normalized pixel embeddings from \( \mathcal{O}^\mathcal{T}_d \). The overall hierarchical pixel-pixel alignment loss is expressed as:
\begin{align}
    \mathcal{L}_\text{VICReg} = \sum_{d=1}^{4} \alpha_v \mathcal{L}_{\text{var}}(Z^\mathcal{H}_{d},Z^\mathcal{T}_{d})\nonumber\\
    + \beta_i\mathcal{L}_{\text{inv}}(Z^\mathcal{H}_{d},Z^\mathcal{T}_{d}) \nonumber\\
    + \gamma_c \mathcal{L}_{\text{cov}}(Z^\mathcal{H}_{d},Z^\mathcal{T}_{d})\label{hiearchical_vicreg}
\end{align}
Following Bardes \yrcite{bardes2021vicreg} we set $\alpha_v$ and $\beta_i$ to 25.0 and $\gamma_c$ to 1.0. The mathematical formulation of VICReg can be found in \autoref{appendix:objective_functions}.

\subsubsection{Reconstruction Losses} In addition to the previously defined contrastive losses, our pre-training objective also has three additional reconstruction losses for the modality-specific AEs (i.e., the waveform AE and the multi-temporal image AE) and on the outputs of $\mathcal{O}^{\mathcal{H}}$ for regularization. The reconstruction losses are simply the mean squared error loss ($mse$) on the reconstructed waveform $\mathcal{\hat{W}} \in \mathbb{R}^{1 \times L}$ given input waveform $\mathcal{W}$, the reconstructed multi-temporal image $\mathcal{\hat{M}} \in \mathbb{R}^{T \times H \times W \times C}$ given input image $\mathcal{M}$, and the reconstructed mono-temporal image $\mathcal{\hat{I}} \in \mathbb{R}^{H \times W \times C}$ given input image $\mathcal{I}$. The reconstruction term is formulated as:
\begin{align}
    \label{rec_losses}
    \mathcal{L}_{\text{rec}} = mse(\mathcal{W},\mathcal{\hat{W}}) + mse(\mathcal{M},\mathcal{\hat{M}}) + mse(\mathcal{I},\mathcal{\hat{I}})
\end{align}

\subsection{Waveform Generation} \label{sec:diffusion}

To generate a full GEDI waveform given a pixel input, we leverage latent diffusion models (LDMs) \cite{rombach2022sd}. LDMs are generative models that iteratively refine a latent representation, starting from a normally distributed prior in a learned latent space. These models can also be conditioned on auxiliary inputs $y$ to model conditional distributions $p(z|y)$. In our case, the conditioning variable consists of pixel embedding $\mathcal{O}^{\mathcal{V}}_{\phi,\lambda}$ at coordinate $(\phi, \lambda)$, while the latent variable $z \in \mathbb{R}^{L/16\times16}$ represents the embedding of the quantized waveform obtained from the RVQ layer (\cref{paragraph:rvq_layer}). During inference, the waveform generation follows a two-step process. First we sample a representation $z$ given condition $\mathcal{O}^{\mathcal{V}}_{\phi,\lambda}$ using the LDM, and then the frozen $D_w(z)$ yields a waveform.

For a detailed description of the diffusion process, including the denoising objective, noise schedule, and latent variable sampling, we refer the reader to Appendix~\ref{appendix:diffusion_math}.