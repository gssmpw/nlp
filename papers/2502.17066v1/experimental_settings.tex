We provide a detailed experimental evaluation showing that DUNIA is able to achieve high performance across a variety of tasks in zero-shot and fine-tuned settings, including land cover classification, crop mapping, and vertical forest structure analysis (cover, height and GEDI waveform retrieval). We leverage diverse datasets to inform the model on horizontal structures--Sentinel- 1 \& 2, and vertical structures with GEDI waveforms.

The following section contains a summary of the experimental setup, with full details in the appendix. Model configuration and optimisation can be found in Appendix \ref{appendix:model_config_and_optim}. For clarity, during the pre-training phase, we set the input image size to $64\times64$ pixels, with 14 channels from stacked Sentinel-1 \& 2 image composites. The embedding dimension is set to 64 (i.e., $D_p=64$). During inference, the input image size can be of any size, but we inferred on inputs of shape $256\times256$ pixels.

\subsection{Experimental Setup}
For our evaluation, we resort to various datasets and tasks. The datasets and experimental details are described next.

\subsubsection{Datasets}

\paragraph{Pre-training Datasets.} \label{seq:pretraining_datasets}

We used Sentinel-2 Level-2A surface reflectance data from Google Earth Engine, including 10 m and 20 m spatial resolution bands with the latter upscaled to 10 m. Two sets of mosaics were created over the entire metropolitan French territory: a single leaf-on season mosaic (April-September 2020) for the pre-trained model, and three four-month mosaics (October 2019-September 2020) for the multitemporal AE. Cloud filtering was applied using the S2 Cloud Probability dataset.

Sentinel-1 data were obtained from the S1A and S1B satellites operating at C-band, collected in Interferometric Wide swath mode with VV and VH polarizations. The data was calibrated, geometrically corrected, and resampled to 10 m resolution. Similar to S2, we created two sets of mosaics with normalized backscattering coefficients.

The Global Ecosystem Dynamics Investigation (GEDI) is a full-waveform LiDAR sensor on the International Space Station (ISS), operational between 51.6$^{\circ}$ N and 51.6$^{\circ}$ S from 2019-2023. We used Level 1B, 2A, and 2B data from April 2019 to December 2021, extracting waveforms, geolocation, height metrics ($\mathcal{W}_{rh}$), canopy cover ($\mathcal{W}_{c}$), and Plant Area Index ($\mathcal{W}_{pai}$). After quality filtering following Fayad et al. \yrcite{Fayad_etal_2024} and seasonal selection, the dataset contained $\approx$ 19 million waveforms; covering less than 1\% of the total surface area of France.

Overall, our pre-training dataset consisted of 836K $64 \times 64$ pixels Sentinel- 1 \& 2 images with, on average, 26 corresponding GEDI waveforms per image.

\paragraph{Evaluation Datasets.}
We evaluated our model on seven downstream tasks using labels from various data products at resolutions matching or lower than our model's output (10 m). 
\emph{PureForest ($PF$)} provides a benchmark dataset of ground truth patches for classifying mono-specific forests in France, featuring high-resolution imagery and annotations for over 135K $50 \times 50$ m (i.e., 5x5 pixels) patches across 13 tree species \cite{gaydon2024pureforest}. \emph{CLC+Backbone ($CLS_+$)} is a pan-European land cover inventory for 2021, utilizing Sentinel-2 time series (2020-2022) and a TempCNN classifier \citep{Pelletier2019} to produce a 10 m raster indicating the dominant land cover among 11 classes. \emph{PASTIS} is a crop mapping dataset by Garnot et al. \yrcite{garnot2021pastis}, covering 18 crop classes and 1 background class with 2433 densely annotated $128 \times 128$ pixels images at 10 m resolution. The \emph{Vertical Structure dataset} assesses model performance in mapping forest heights, canopy cover, plant area index, and waveform retrieval at 10 m resolution, relying on GEDI-derived products as presented earlier. When available, we used the train/val/test split used by the references (i.e., $PF$ and \emph{PASTIS}). For the $CLS_+$ dataset, we used the same split as the unsupervised dataset, which followed a 65/10/25 split. 

\subsubsection{Performance Evaluation}
We evaluated our model's retrieval capacities for both zero-shot dense prediction tasks and its performance in the fine-tuned setting. For both settings, we used the weighted F1 score ($wF1$) for classification tasks, root mean squared error ($rmse$) and Pearson's correlation coefficient ($r$) for regression tasks. To evaluate the similarity between acquired and retrieved/generated waveforms, we used Pearson's correlation coefficient, computed between the time-aligned retrieved/generated waveforms and the acquired waveforms.

For zero-shot classification, we constructed retrieval databases based on the downstream tasks. For vertical structure tasks, outputs from $\mathcal{O}^\mathcal{V}$ served as queries, and we created a single database with $L_2$ normalized waveform embeddings from $\mathcal{O}^\mathcal{W}$ as keys, paired with four target labels: $\mathcal{W}_{rh}$, $\mathcal{W}_c$, $\mathcal{W}_{pai}$, and the complete waveform $(\mathcal{W})$. For horizontal structure tasks, outputs from $\mathcal{O}^\mathcal{H}$ were used as queries, creating a separate database for each target as not all targets were available at all pixels simultaneously. Here, keys are $L_2$ normalized pixel embeddings from $\mathcal{O}^\mathcal{H}$, with targets being a land cover class (i.e., $CLC_+$), crop type (i.e., $PASTIS$), or tree species (i.e., $PF$). For tree species identification, the labels cover a $5 \times 5$ pixel area, so queries and keys are the averaged embeddings over this window. Next, given an input image and its $L_2$ normalized pixel embeddings from $\mathcal{O}^\mathcal{V}$ or $\mathcal{O}^\mathcal{H}$, we retrieve the $k$ nearest neighbors (KNN) for each pixel based on cosine similarity and assign the target class by distance-weighted voting. for the KNN retrieval, keys were obtained from the training split while the queries were obtained from the test split.  

For the low-shot fine-tuning, we froze the entire pre-trained network except for the last two NA layers in each decoder, which were appended with an output head consisting of two sequential $1 \times 1$ convolutional layers. The first layer halves the input channels, and the second projects the reduced representation to the desired output size.

We evaluated zero-shot classification and low-shot fine-tuning in a low-data regime. We define a dataset with a low number of labels based on the type of labels usually available for this dataset. For $CLC_+$ and $PASTIS$, labeled data are available as densely annotated images. For $PF$, $\mathcal{W}$, $\mathcal{W}_{rh}$, $\mathcal{W}_c$ and $\mathcal{W}_{pai}$, labeled data are available as single annotated pixels.

\subsubsection{Competing Models.} 
We compared DUNIA in the fine-tuned setting to two current state-of-the-art Earth Observation FMs: CROMA \cite{fuller2023croma} and AnySat \cite{astruc2024anysat}. CROMA takes as input Sentinel- 1 \& 2 imagery, while Anysat is pre-trained using Sentinel- 1 \& 2 times series as well as very high-resolution imagery. For a fair comparison, we refined the pre-training of both models for 200K steps using our datasets and the training details from the respective papers. For AnySat, we used multi-temporal Sentinel-1 \& 2 mosaics with three time steps and included SPOT images at 1.5 m resolution as an additional input modality during pre-training but fine-tuned using only Sentinel- 1 \& 2. Both models were evaluated on all downstream tasks except waveform generation, as they do not support this task.

\subsection{Results}
Our evaluation shows that embeddings from our proposed framework, combined with simple zero-shot classifiers, often surpass specialized supervised models. Even with minimal labeled data, our model demonstrates strong low-shot performance, rivaling or exceeding state-of-the-art methods.
\input{results}