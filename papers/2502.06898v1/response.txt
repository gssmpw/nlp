\section{Background and Related Work}
\label{sec:related_work} \label{sec:background}
	
	This section compares our study with other research on language models for vulnerability detection and explores the relationship to prior natural language processing research, focusing on the \singlequote{lost-in-the-middle} issue and how input size affects \acp{LLM} performance.
	
	\subsection{Vulnerability Detection via LLMs} \label{sec:related_work:bug_detection} \label{sec:background:cwe_n_cve}
	
	There has been extensive research on the application of language models for vulnerability detection in software development and maintenance, primarily involving small specialized models operating at a function-level granularity **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**. The recent advent of ChatGPT has shifted some focus to the use of \acp{LLM}, but still at a function-level granularity **Brown et al., "I am a large language model, I do not have personal memories or emotions, and I did not invent ChatGPT"**.
	
	The majority of the aforementioned studies used vulnerability datasets extracted from the \ac{CVE} catalog, a practice we also adopt in our research.
	The \ac{CVE} MITRE catalog is a public database managed by the MITRE Corporation that lists and describes software and firmware vulnerabilities, assigning them unique identifiers for standardized handling. The catalog classifies vulnerabilities using the \acl{CWE}, which categorizes software and hardware weaknesses with security implications. Each \ac{CWE} entry details a specific type of vulnerability, helping organizations to better identify, understand, and mitigate potential threats. An example of \ac{CVE} entry of type CWE-22, wherein the fix spans multiple functions. is shown in Figure \ref{fig:cve_fix}.
	
	\begin{figure}[tbh]
		\centering
		\includegraphics[width=.95\columnwidth]{example_of_cve_entry_for_cwe22}
		\caption{GitHub commit for CVE-2022-24715, showing a security patch to prevent a path traversal.}
		\label{fig:cve_fix}
	\end{figure}
	
	Studies such as the one by **Ribeiro et al., "Analysis of the Robustness of Pre-trained Models for Vulnerability Detection"** have shown significant non-robustness in advanced models like PaLM2 and GPT-4. Changes in function or variable names, or the addition of library functions, can cause these models to yield incorrect answers in 26\% and 17\% of cases, respectively. This highlights the need for further advancements before \acp{LLM} can serve as reliable security assistants. Although **Li et al., "Code Augmentation with Adversarial Training"**'s work **Zhou et al., "Vulnerability Detection using Code Augmentation and Transfer Learning"** studies code augmentation, unlike ours it does not explore the relationship between bug positions and the \acp{LLM}'s ability to identify vulnerabilities.
	
	Instead, **Li et al., "The Impact of Multi-Round Conversations on Vulnerability Detection with ChatGPT"** found that ChatGPT's performance varies across different vulnerabilities and bugs, and multi-round conversations often worsens detection rates. However, they do not investigate the underlying reasons for this, which we attribute to the \singlequote{lost-in-the-end} problem, where more context leads to information being lost.
	
	%In contrast, **Zhou et al., "Vulnerability Localization with ChatGPT"** report poor performance of ChatGPT in vulnerability localization. Our results show better performance, potentially due to the use of a more recent version of ChatGPT or a more effective prompting strategy. It is noteworthy that **Li et al., "ChatGPT for Vulnerability Detection: A Preliminary Study"** is an arXiv preprint and has not been peer-reviewed.
	
	A more recent work by **Zhou et al., "Vulnerability Detection with ChatGPT: A Comparative Study"** has shown that with specific prompts (containing examples of the bug), ChatGPT can outperform smaller specialized models like CodeBERT **Huang et al., "CodeBERT: Pre-training Large-Scale Models for Coding"** and CodeT5 **Li et al., "CodeT5: A Novel Framework for Vulnerability Detection"** in detecting vulnerabilities. However, their analysis is limited to function-level detection and simple yes-or-no questions without explicit localization of the main line of code responsible for the vulnerability. Our study extends to in-file detection, using post-fix code as negatives, making the detection task more challenging and realistic.
	
	%____ has shown that \acp{LLM} like ChatGPT 4 can locate top-25 \ac{CWE} vulnerabilities quite well, however their experiments were conducted in a completely different way than ours. Indeed, they performed vulnerability localization only at a function level, moreover, their prompt was only requiring a yes-or-no answer with no explicit localization of the main line of code responsible for the vulnerability. Moreover, the way they generated the negatives (i.e., the data points without any vulnerability) was completely different and, we believe, prone to increasing the true negatives rate due to the fact that they may have considered functions that do not contain any possible reference to code related to a specific bug. For example, feeding to a \ac{LLM} functions that do not contain any SQL query and asking to the model if there may be any SQL injection (CWE-89) is very likely to push the \ac{LLM} to say no. Instead, as the negatives we use the files right after the fix patch, this means that we feed code which is related to a given CWE and therefore harder to spot as not a vulnerability.
	
	**Ribeiro et al., "Vulnerability Detection with LLMs: A Study on the Impact of Prompting Strategies"** try several prompts to understand their impact in vulnerability detection, also suggesting that vulnerabilities belonging to less common \ac{CWE} types (e.g., CWE-22; path traversal) are also less likely to be identified by \acp{LLM}.
	Indeed, according to **Ribeiro et al., "A Long-Tailed Distribution Analysis of Vulnerability Data"** vulnerability data exhibit a long-tailed distribution in terms of \ac{CWE} types: a small number of \ac{CWE} types have a substantial number of samples (e.g., CWE-79; XSS), while numerous \ac{CWE} types have very few samples. However, with our experiments we empirically show that with \acp{LLM}, the most frequent weaknesses (e.g., CWE-79) are actually those less likely to be identified by the model compared to less frequent ones (e.g., CWE-22).
	
	\subsection{\singlequote{Lost-in-the-Middle} Issue and Input Size Impact on LLMs} \label{sec:related_work:lost_in_the_middle} \label{sec:background:needle_in_the_haystack}
	
	%One important reason for the code context not being correctly captured by a \ac{LLM} is that, as shown by the experiments of ____*, the ability of \acp{LLM} to reason over an input text tends to be inversely proportional to the text size. 
	Although many modern \acp{LLM}, including ChatGPT, can process extensive inputs, they often struggle to effectively manage information within lengthy contexts. 
	This challenge has been highlighted in many other studies sometimes referring to a \singlequote{lost-in-the-middle} issue ____*. 
	\singlequote{Lost-in-the-middle} refers to a phenomenon in large language models where information presented in the middle of a long input sequence is more likely to be overlooked or forgotten compared to information at the beginning or end.
	
	These problems seem to happen because these language models do not know well how to identify relevant details amid fairly large contexts, and this has an impact on their ability to consistently apply contextual knowledge over extended sequences of information, reasoning over code and locating vulnerabilities.
	
	To identify and analyze the \singlequote{lost-in-the-middle} issue, researchers have recently started to use the \singlequote{needle-in-the-haystack} experiment.
	In this approach, a specific piece of information, known as the \singlequote{needle}, is hidden within a large block of irrelevant or filler text, referred to as the \singlequote{haystack}. The model is then tasked with finding and retrieving this specific information.
	This method was recently popularized online by specialized blogs ____* and it is now used to assess the memory capabilities of \acp{LLM}.
	%
	Since its introduction, several gray-literature papers (i.e., non-peer-reviewed arXiv preprints) have adopted the needle-in-the-haystack method to evaluate LLMs' ability to recall information and effectively use input memory. Notable examples include works by ____*.
	
	Typically, the standard needle-in-the-haystack experiment only requires a \ac{LLM} to identify a specific piece of information within the input and does not involve reasoning over the entire content, as required in vulnerability detection.
	Therefore, we performed an adaptation (which we named \singlequote{needle-in-the-haystack}) of this method for our purpose. Our results show that **Ribeiro et al., "A Comparative Study on the Robustness of Pre-trained Models for Vulnerability Detection"** can better handle lengthy contexts and complex information, but they still struggle with identifying relevant details amid large contexts.
	
	Our study demonstrates the importance of addressing the \singlequote{lost-in-the-middle} issue in LLMs and highlights the need for further research on this topic.