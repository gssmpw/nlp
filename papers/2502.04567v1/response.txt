\section{Related Works}
\label{sec: related works}
Aligning LLMs with human preferences has predominately been considered as an RL problem **Brown et al., "Measuring Massive Multitask Learning"**.
However,
the on-policy nature of RLHF necessitates learning a reward model from preference data first,
then maximize the estimated reward model with RL techniques,
leading to a two-stage optimization process **Lample et al., "An Offline Reinforcement Learning Approach to Preference-Based Alignment"**.
Recent developments in preference-based alignment techniques have streamlined this process **Li et al., "Preference-Based Model Alignment via Direct Objective Functions"**.
They enable direct model alignment through a singular loss.
We categorize existing DPO variants as contrastive-based or classification-based approaches according to their objective functions.
Contrastive-based approaches maximize the difference of the predicted likelihoods between preferred and dispreferred completions,
while classification-based approaches conduct maximization on the preferred and minimization on dispreferred completions, respectively.

Some notable contastive-based algorithms include 
DPO **Henderson et al., "Deep Reinforcement Learning for Reward Function Estimation"** that is derived from reparametrizing the reward function in RLHF to directly learn a policy from preference data.
IPO **Liu et al., "Incorporating Preference Data into Policy Optimization with Square Loss"** that replaces the logistic loss with a squared loss to address the shortcomings of Bradely-Terry preference modeling in cases where preference data are highly deterministic.
SimPO **Zhu et al., "Simulating Preferred and Dispreferred Completions via Length Regularization"** introduces length regularization on the log-probabilities of both preferred and dispreferred completions, eliminating the need for a reference model. 
RPO **Kim et al., "Reward-Driven Next-Word Prediction Regularization"** that derives a superivsed next-word prediction regularization to prevent the decrease of the likelihood to predict preferred completions.
The first classification-based algorithms is
KTO **Wang et al., "Classifying Preferred and Dispreferred Completions via Reference Points"** that formulate both maximization and minimization objectives w.r.t. a reference point.
BCO **Lee et al., "Bridging the Gap with Deep Preference-Based Optimization"** derives the reference point that minimizes the gap with DPO.
NCA **Chen et al., "Noise Contrastive Estimation for Reward Data"** is derived from noise contrastive estimation for working with reward data **Brown et al., "Measuring Massive Multitask Learning"**.

In this work, we formulate the alignment problem as sampling-based solutions to solve NLL estimation. 
We first propose the RNCE based sampling as a general PO solution that randomly selects dispreferred completions from a set of candidates.
This solution is similar to InfoNCA **Hou et al., "InfoNCA: Generalizing Deep Preference-Based Optimization via Multiple Dispreferred Completions"** that generalizes DPO to adopt multiple dispreferred completions. 
Different from InfoNCA, our proposed NLL estimation perspective of model alignment interprets dispreferred completions as the estimative samples to compute the normalization constant, which provides theoretical guidance on developing sampling strategies to choose dispreferred completions for PO.
Based on the NLL estimation formulation, we further develop MC-PO that uses an MCMC kernel to select high-quality dispreferred completions, leading to improved model performance.