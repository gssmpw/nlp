
\section{Preference Optimization as NLL Estimation}
\label{sec: po as nll estimation}
In this section,
we revisit the PO objective function (Sec. \ref{sec:background preference optimization}) and formulate it as minimizing the NLL of a probability model (Sec. \ref{sec: preference optimization as nll estimation}).
Then we apply a sampling-based approach to solve this (Sec. \ref{sec: preference optimization via sampling-based solution}).

\subsection{Background: Preference Optimization}
\label{sec:background preference optimization}
RLHF aims to align a target policy $\pi_{\theta}$ with human preference based on an reward model $r(\mat{x}, \mat{y})$.
This optimization problem is formulated as
\begin{equation}
\label{eq: rlhf objective function}
\max \limits_{\pi_{\boldsymbol\theta}}
\mathbb{E}_{\substack{\mat{x} \sim \rho,\\ \mat{y} \sim \pi_{\boldsymbol\theta}(\cdot|x)}}
[r(\mat{x}, \mat{y})]
-
\beta \cdot KL[\pi_{\boldsymbol\theta}(\mat{y} | \mat{x}) || \pi_{\rm ref}(\mat{y} | \mat{x})],
\end{equation}
where $\rho$ represents the distribution over input prompts, 
$\beta$ is a hyper-parameter controlling the deviation from the reference policy $\pi_{\rm ref}$.
The reward model $r(\mat{x}, \mat{y})$ is typically estimated from empirically observed data and cannot accurately represent real-world human preference.
The KL-divergence constraint prevents the model from over-fitting the estimated reward model \citep{skalse2022defining},
as well as maintaining the generation diversity and preventing mode-collapse to single high-reward completions.

Following prior works \citep{go2023aligning},
the closed-form solution to the KL-constrained reward maximization in Eq.~\eqref{eq: rlhf objective function} can be derived as,
\begin{equation}
\label{eq: rlhf optimal solution}
\pi^{\ast}(\mat{y} | \mat{x})
=
\frac{1} {Z(\mat{x})}
\pi_{\rm ref}(\mat{y} | \mat{x}) 
\exp 
\Big(
\frac{1} {\beta} r(\mat{x}, \mat{y})
\Big).
\end{equation}
The partition function $Z(\mat{x})$ ensures that $\pi^{\ast}$ is a valid probability conditioned on any $\mat{x}$.
$Z(\mat{x})$ is typically intractable to compute since the output space is combinatorially large,
which makes this representation hard to utilize in practice.

To estimate $\pi^{\ast}$,
DPO reparameterizes the reward model in terms of the target policy, which enables directly optimizing the target policy
from pairwise preference dataset.
DPO is derived from rearranging the closed-form solution of $\pi^{\ast}$ in Eq.~\eqref{eq: rlhf optimal solution} to express the reward function in terms of its corresponding optimal policy,
then substituting this expression into the Bradley-Terry model \citep{bradley1952rank}.
This yields the DPO objective function,
\begin{equation}
\label{eq:dpo}
\min_{r_{\boldsymbol{\theta}}} 
\underset{(\mat{x}, \mat{y}_0, \mat{y}_1) \sim \mathcal{D}}{\mathbb{E}}
\left[ 
    -\log \sigma\left( 
        \beta r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0) 
        - \beta r_{\boldsymbol\theta}(\mat{x}, \mat{y}_1)
    \right)
\right],
\end{equation}
where $r_{\boldsymbol\theta}(\mat{x}, \mat{y}) := \log \frac{\pi_{\boldsymbol\theta}(\mat{y} | \mat{x})} {\pi_{\rm ref}(\mat{y} | \mat{x})}$ is the parameterized reward model,
$\mat{y}_0$ and $\mat{y}_1$ represent preferred and dispreferred completions, respectively,
$\mathcal{D}$ is the distribution of pairwise preference data.
DPO optimizes the target policy to distinguish between preferred and dispreferred completions, conditioned on the same input prompt.

Existing studies on PO have primarily focused on creating pairwise preference data using heuristic approaches.
In the next, we offer theoretical insights into sampling dispreferred completions by framing PO as NLL estimation. 


\subsection{Preference Optimization as NLL Estimation}
\label{sec: preference optimization as nll estimation}
\paragraph{Background: NLL estimation.}
Unnormalized models can be used to approximate complex data distributions.
However,
estimating unnormalized models is not straightforward since the NLL estimation involves the typically intractable normalization constant.
Given some observations from the target distribution,
we seek to approximate it with a parametric probability model,
\begin{equation}
\label{eq: expression for probability distribution}
p_{\boldsymbol\theta}(\mat{y} | \mat{x})
:=
\frac{\tilde{p}_{\boldsymbol\theta}(\mat{y} | \mat{x})}
{Z_{\boldsymbol\theta}(\mat{x})},
\;\;
Z_{\boldsymbol\theta}(\mat{x})
=
\int
\tilde{p}_{\boldsymbol\theta}(\mat{y}' | \mat{x})
d \mat{y}',
\end{equation}
where $\tilde{p}_{\boldsymbol\theta}$ is an unnormalized model and $Z_{\boldsymbol\theta}(\mat{x})$ is its normalization constant.
The NLL estimation minimizes the negative log-likelihood of $p_{\boldsymbol\theta}$ of predicting these observations.
Roughly speaking,
as the number of observations approaches to infinity,
the NLL estimation results in a parametric probability model 
that increasingly approximates the target distribution.


\paragraph{Proposed Formulation: PO as NLL estimation.}
In this work,
we define the following probability model that is closely related to the expression of $\pi^{\ast}$ in Eq.~\eqref{eq: rlhf optimal solution}.
\begin{align}
\label{eq: mle parameterized policy}
& p_{\boldsymbol\theta}(\mat{y} | \mat{x})
: =
\frac{1} {Z_{\boldsymbol\theta}(\mat{x})}
\mu(\mat{y} | \mat{x}) 
\exp\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y})
\Big),
\\ \nonumber
&
Z_{\boldsymbol\theta}(\mat{x})
=
\int
\mu(\mat{y} | \mat{x})
\exp 
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y})
\Big)
d\mat{y}
,
\\ \nonumber
&
r_{\boldsymbol\theta}(\mat{x}, \mat{y}) = \log \frac{\pi_{\boldsymbol\theta}(\mat{y} | \mat{x})} {\pi_{\rm ref}(\mat{y} | \mat{x})}.
\end{align}
where 
$\mu$ is a proposal distribution that we can sample from.
For any set of parameters $\boldsymbol\theta$,
we assume that $p_{\boldsymbol\theta}$ covers the support of $\pi^{\ast}$, such as $p_{\boldsymbol\theta}(\mat{y} | \mat{x}) > 0$ whenever $\pi^{\ast}(\mat{y} | \mat{x}) > 0$, for all $\mat{x} \sim \rho$. 
In this expression,
the unnormalized model is defined as 
$\tilde{p}_{\boldsymbol\theta}(\mat{y} | \mat{x})
:=
\mu(\mat{y} | \mat{x}) 
\exp\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y})
\Big)$.
To estimate $\pi^{\ast}$ with $p_{\boldsymbol\theta}$,
the NLL estimation minimizes the negative log-likelihood of $p_{\boldsymbol\theta}$ to predict observations sampled from $\pi^{\ast}$,
\begin{align}
\label{eq: nll estimation objective function}
& \boldsymbol\theta^{\ast} = \arg \min \limits_{\boldsymbol\theta}
\mathbb{E}_{\mat{x} \sim \rho, \; \mat{y} \sim \pi^{\ast}(\cdot|x)}
\Big[
\mathcal{L}^{NLL}(\boldsymbol\theta, \mat{x}, \mat{y})
\Big],
\\ \nonumber
& {\rm where}\;
\mathcal{L}^{NLL}(\boldsymbol\theta, \mat{x}, \mat{y})
=
-
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y})
+ \log Z_{\boldsymbol\theta}(\mat{x}).
\end{align}
Recall in Eq.~\eqref{eq: mle parameterized policy} that the reward model can be represented in terms of the target policy,
which allows for optimizing the target policy by solving the NLL estimation.


In practice,
the first term of $\mathcal{L}^{NLL}$ in Eq.~\eqref{eq: nll estimation objective function} is typically easy to optimize as the gradient of the target policy (e.g., the LLM) can be computed using existing automatic differentiation software.
However,
optimizing the normalization constant is non-trivial.
In the next,
we focus on sampling-based approaches to estimate the normalization constant.


\subsection{Preference Optimization via Sampling-Based Solution for Its NLL Estimation Formulation}
\label{sec: preference optimization via sampling-based solution}
\paragraph{Proposed: PO via sampling-based solution of NLL estimation.}
Importance sampling applies samples from a proposal distribution to estimate the normalization constant \cite{naesseth2024elementssequentialmontecarlo}.
Ranking noise contrastive estimation (RNCE) \citep{olmin2024connection}, as a more advanced sampling approach, 
utilizes both importance samples and true observations from the target distribution to estimate the intractable term.
Given one observation $\mat{y}_0$ sampled from $\pi^{\ast}$
and $M$ i.i.d. noisy samples from a proposal distribution,
RNCE optimizes to classify as $\mat{y}_0$ coming from the true distribution.

\begin{restatable}{proposition}{rnce}
\label{prop: ranking noise contrastive estimation objective}
Suppose that we have $\mat{y}_0 \sim \pi^{\ast}(\mat{y} | \mat{x})$,
and $M$ noisy samples $\{\mat{y}_i\}_{i=1}^M$,
where each $\mat{y}_i$ is sampled from a proposal distribution, $\mat{y}_i \sim \mu(\mat{y} | \mat{x})$.
Then RNCE approximates the NLL estimation as follows,
\begin{align}
\label{eq: ranking noise contrastive estimation}
& \mathcal{L}^{Sample}(\boldsymbol\theta, \mat{x}, \mat{y}_0)
\\ \nonumber
& =
-
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
+
\log
{\sum_{i=0}^M \exp
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\Big)}.
\end{align}
\end{restatable}
The detailed derivation is in Appendix \ref{sec: RNCE objective function derivation}.
Compared to the NLL estimation in Eq.~\eqref{eq: nll estimation objective function},
RNCE approximates the intractable normalization constant as $Z_{\boldsymbol\theta}(\mat{x})
=
\frac{1}{M+1}
\sum_{i=0}^M \exp
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\Big)$,
with both $\mat{y}_0$ sampled from $\pi^{\ast}$ and noisy samples $\{\mat{y}_i\}_{i=1}^M$ from a proposal distribution (notice that $\frac{1}{M+1}$ is cancelled by taking the gradient of $\log \mathcal{L}^{Sample}$).
Consequently,
$\mathcal{L}^{Sample}$ is equivalent to a cross-entropy loss
that optimizes the model to classify $\mat{y}_0$ as the correct prediction among all ($M+1$) candidates.


\paragraph{Proposed: Existing PO can be formulated as sampling-based solutions of NLL estimation.}
In the sampling-based solution from Eq.~\eqref{eq: ranking noise contrastive estimation},
we consider the true observation $\mat{y}_0$ as the preferred completion,
and noise samples from the proposal distribution as dispreferred completions.
This leads to an expression of PO as follows, with letting $M=1$,
\begin{align*}
& 
-
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
+
\log
{\sum_{i=0}^1 \exp
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\Big)}
\\
&
=
- \log
\frac{
\exp
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
\Big)
}
{
\exp
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
\Big)
+
\exp
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_1)
\Big)
},
\\
& =
- \log \sigma
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
-
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_1)
\Big),
\end{align*}
where $\sigma$ is the logistic function.
This sampling-based solution with one noise sample is equivalent to DPO  where the noise sample acts as a dispreferred completion (\eref{eq:dpo}).
This provides a novel interpretation on dispreferred completions in existing PO:
dispreferred completions can be understood as importance samples used to estimate the normalization constant in NLL estimation.


Building on this connection, we can adapt various sampling-based algorithms from the NLL estimation literature to generate dispreferred samples for PO. 
These algorithms aim to improve the accuracy of estimating the normalization constant, thereby improving PO performance.
For instance,
in Proposition \ref{prop: ranking noise contrastive estimation objective},
RNCE suggests random sampling to construct dispreferred completions. 
In the sampling based NLL estimation literature, there exist more advanced strategy than RNCE \cite{olmin2024connection}. So in the next section, we develop a more advanced sampling strategy for PO.




