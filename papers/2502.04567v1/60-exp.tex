

\section{Numerical Experiments}
\label{sec: numerical experiments}
In this section, we present main results of our experiments, highlighting the effectiveness of MC-PO and OnMC-PO on various benchmarks (Sec. \ref{sec: main results}) and providing an in-depth understanding on the effect of sampling strategies (Sec. \ref{sec: experiments on sampling strategies}).
More extensive results can be found in Appendix \ref{appendix: extensive experimental results}.

\subsection{Experimental Setup}
\label{sec: experimental setup}
We summarize experimental setting here, more details can be found in Appendix \ref{appendix: experimental setup}.
\paragraph{Models and datasets.}
We perform PO under \textbf{three} different setups: \textbf{ (1) The base setup} considers the
\href{https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-SFT}{Llama-3.1-8B-SFT} model, which has been fine-tuned using supervised next-word prediction on the TÃœLU 3 SFT Mix dataset \citep{lambert2024t},
and {\href{https://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta}{Mistral-7B-SFT}}.
We fine-tune these models on the \href{https://huggingface.co/datasets/berkeley-nest/Nectar}{Nectar} dataset \citep{zhu2023starling}.
The Nectar dataset consists of $7$ ranked completions per input prompt generated by different LLMs,
which creates both high-quality and diverse candidate completions for sampling.
For each input prompt,
we consider the rank-$1$ completion as the preferred completion and subsequently eliminate the rank-$2$ completion to minimize noises in preference pairs. 
From the remaining $5$ candidates, we then randomly select a dispreferred completion.
\textbf{ (2) The instruct setup} uses the off-the-shelf instruction-tuned
\href{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}{Llama-3.1-8B-Instruct} model \citep{dubey2024llama} to initialize the target policy $\pi_{\boldsymbol\theta}$.
This model has undergone extensive instruction-tuning processes, making it more expressive compared to the initialization model in the base setup.
We use prompts from the \href{https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized}{UltraFeedback} dataset \citep{cui2023ultrafeedback} to regenerate the preferred and dispreferred completions using the \href{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}{Llama-3.1-8B-Instruct} model.
This makes the instruct setup closer to an on-policy setting \citep{tang2024understanding}.
Specifically,
we generate $6$ completions using temperatures of $0.6$, $0.8$, and $1$ for each input prompt.
Then, we apply the iterative pairwise ranking approach \citep{chen2024towards} with the
\href{https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct}{Llama-3.1-70B-Instruct} to select the most preferred completion and randomly sample a dispreferred completion from remaining candidates.
\textbf{ (3) The batched online setup} is in the middle of the offline and purely online setups \citep{schulman2017proximal, lambert2024t}, striking a balance between efficiency and adaptability.
We equally split the training steps into three batches and regenerate the preference data following the instruct setup using the current model checkpoint.
This approach is more efficient than a purely online setting \citep{qi2024online}, as initializing the inference  is often computationally expensive \citep{kwon2023efficient}.

\paragraph{Training.}
All training jobs are done using full-parameter tuning. 
We fix the effective batch size of $128$ and the number of training epochs of $2$.
Hyperparameter optimization is conducted using $7$ different learning rates.
All results are reported as the average of the final checkpoints across $3$ random seeds, along with the standard deviation,
which can effectively reduce numerical randomness \citep{miller2024adding}.
Each training job is done on a node of $8\cdot$A100 GPUs.

\paragraph{Evaluations.}
To evaluate the performance of aligned models,
we use two popular open-ended instruction-following benchmarks:
AlpacaEval $2$ \citep{dubois2024length} and Arena-Hard \citep{li2024live}.
These benchmarks assess the model's versatile conversational capabilities across a wide range of queries and have been widely adopted by the community.
We focus on winrate as evaluation metric.
Let $N_{\rm cand}$, $N_{\rm base}$ and $N_{\rm tie}$ be the number of candidate wins, baseline wins and ties, respectively.
The adjusted winrate is computed as 
\begin{equation*}
{\rm Winrate}
:=
\frac{N_{\rm cand} + N_{\rm tie} / 2} 
{N_{\rm cand} + N_{\rm base} + N_{\rm tie}}.
\end{equation*}
All winrate-based evaluations are done using \href{https://huggingface.co/mistralai/Mistral-Large-Instruct-2407}{Mistral-Large-Instruct-2407  } as the model judge.


\subsection{Main Results: Comparing with SOTA PO}
\label{sec: main results}
\begin{table*}[h!]
\centering
\begin{tabular}{l|cc|cc|cc}
\hline
Model & \multicolumn{2}{|c|}{\href{https://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta}{Mistral-7B-SFT}} & \multicolumn{2}{|c|}{\href{https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-SFT}{Llama-3.1-8B-SFT}} & \multicolumn{2}{|c}{\href{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}{Llama-3.1-8B-Instruct}} \\
\band Train dataset & \multicolumn{2}{|c|}{Nectar} & \multicolumn{2}{|c|}{Nectar} & \multicolumn{2}{|c}{Ultrafeedback (prompt only)} \\
\band Evaluation & Alpaca & Arena & Alpaca & Arena & Alpaca & Arena \\ \hline
DPO & 25.07($\pm$6.81) & 42.01($\pm$11.88) & 33.74($\pm$2.51) & 60.25($\pm$2.12) & 64.22($\pm$1.01) & 75.88($\pm$0.79) \\
RPO & 15.31($\pm$0.62) & 39.18($\pm$0.49) & 32.50($\pm$0.75) & 59.20($\pm$0.82) & 51.27($\pm$0.50) & 64.74($\pm$0.12) \\
EXO & 21.77($\pm$4.09) & 30.63($\pm$3.55) & 26.48($\pm$3.31) & 52.89($\pm$5.03) & 64.75($\pm$1.72) & 74.93($\pm$0.81) \\
SimPO & 18.62($\pm$2.64) & 48.26($\pm$3.90) & 33.71($\pm$1.41) & 60.69($\pm$1.01) & 54.28($\pm$1.48) & 73.36($\pm$1.38) \\
CPO & 24.27($\pm$0.39) & 49.66($\pm$0.34) & 29.10($\pm$1.01) & 55.25($\pm$0.60) & 65.28($\pm$0.54) & \textcolor{blue}{77.92($\pm$1.78)} \\ \hline
BCO & 23.04($\pm$0.19) & 46.68($\pm$1.62) & 24.96($\pm$1.28) & 58.16($\pm$1.76) & 61.17($\pm$1.27) & 73.45($\pm$0.54) \\
KTO & 22.98($\pm$0.23) & 45.77($\pm$1.85) & 24.50($\pm$1.35) & 53.40($\pm$0.75) & 60.35($\pm$0.67) & 71.19($\pm$0.49) \\
APO & 15.79($\pm$0.78) & 35.94($\pm$0.26) & 21.13($\pm$0.40) & 53.25($\pm$0.82) & 57.54($\pm$0.97) & 70.70($\pm$0.25) \\
SPPO & 12.68($\pm$0.27) & 30.87($\pm$0.67) & 20.26($\pm$0.34) & 53.52($\pm$0.56) & 56.39($\pm$0.58) & 71.73($\pm$0.62) \\
NCA & 17.30($\pm$0.37) & 39.88($\pm$0.80) & 20.46($\pm$0.36) & 53.36($\pm$1.25) & 58.04($\pm$0.42) & 72.40($\pm$0.23) \\ \hline
\band  MC-PO & \textcolor{blue}{30.86($\pm$0.91)} & \textbf{52.75($\pm$2.00)} & 35.84($\pm$0.31) & \textbf{63.77($\pm$0.81)} & 66.90($\pm$0.74) & \textbf{76.71($\pm$0.24)} \\ 
\band OnMC-PO & \textbf{30.52($\pm$0.24)} & \textcolor{blue}{52.90($\pm$0.53)} & \textcolor{blue}{39.70($\pm$0.29)} & \textcolor{blue}{64.21($\pm$0.59)} & \textcolor{blue}{72.63($\pm$0.21)} & \textbf{77.71($\pm$0.85)} \\ \hline
\end{tabular}
\caption{
Performance evaluation of preference-optimized models.
Results are reported as winrate against GPT-4 as baseline.
Each experiment is conducted using three random seeds. 
We report the mean winrate and standard deviation for both \href{https://huggingface.co/datasets/alpaca_eval/alpaca_eval_2}{AlpacaEval $2$} \citep{dubois2024length} and \href{https://huggingface.co/datasets/arena_hard/arena_hard}{Arena-Hard} \citep{li2024live}. 
The models \href{https://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta}{Mistral-7B-SFT} and \href{https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-SFT}{Llama-3.1-8B-SFT} are trained using the \href{https://huggingface.co/datasets/berkeley-nest/Nectar}{Nectar} dataset, 
\href{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}{Llama-3.1-8B-Instruct} is trained using prompts from the \href{https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized}{UltraFeedback} dataset and self-generated completions.
The highest scores are highlighted in blue, 
and scores within one standard deviation of the highest are boldfaced.
}
\label{table: winrate model performance evaluation llama}
\end{table*}

We first compare MC-PO with existing offline PO algorithms,
then demonstrate that OnMC-PO further improves alignment performance.
We categorize existing baselines as contrastive and classification-based approaches based on their objective functions.
Specifically,
contrastive-based algorithms include 
\textbf{DPO} \citep{rafailov2024direct},
\textbf{RPO} \citep{liu2024provably},
\textbf{EXO} \citep{jitowards},
\textbf{SimPO} \citep{meng2024simpo} and
\textbf{CPO} \citep{xucontrastive}.
Classification-based algorithms include
\textbf{BCO} \citep{jung2024binary},
\textbf{KTO} \citep{ethayarajh2024kto},
\textbf{APO} \citep{d2024anchored},
\textbf{SPPO} \citep{wu2024self}
and \textbf{NCA} \citep{chen2024noise}.
Details on baseline algorithms can be found in Appendix \ref{appendix: baseline preference optimization algorithms}.

The main results are summarized in Table \ref{table: winrate model performance evaluation llama}. 
MC-PO outperforms existing baselines in five out of six studies. 
Notably, in the base setup, using the {\href{https://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta}{Mistral-7B-SFT}} model, 
MC-PO outperforms DPO by 
$4.5\%$ and $9\%$ on Alpaca-Eval and Arena, respectively. 
Using the {\href{https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-SFT}{Llama-3.1-8B-SFT}} model, MC-PO leads to winrates of $35.84\%$ and $63.77\%$ on Alpaca-Eval and Arena, respectively. 
In the instruct setup, as all candidate completions are sampled from the {\href{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}{Llama-3.1-8B-Instruct}} model, sampling from these candidates proves less effective due to the low diversity in the candidate set. 
Consequently, MC-PO shows less improvement compared to existing baselines.
When MC-PO is extended to online setting based on the batched online setup,
OnMC-PO results in further improvement.
With the {\href{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}{Llama-3.1-8B-Instruct}} model,
OnMC-PO leads to a winrate of $72.63 \%$ on Alpaca, outperforming existing baselines.





\subsection{Analysis of Sampling Strategies in MC-PO}
\label{sec: experiments on sampling strategies}
\begin{figure}[th]
\centering
\begin{minipage}{.99\columnwidth}
\centering
    \includegraphics[width = \linewidth]{figures/ablation_sampling_strategies_alpaca.png}
    (a): Alpaca-Eval
\end{minipage}
\begin{minipage}{.99\columnwidth}
\centering
    \includegraphics[width = \linewidth]{figures/ablation_sampling_strategies_arena.png}
    (b): Arena
\end{minipage}
\caption{
Winrate evaluation of the optimized \href{https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-SFT}{Llama-3.1-8B-SFT} model using MC-PO, versus its Max, and Min sampling based variants. 
Five modified \href{https://huggingface.co/datasets/berkeley-nest/Nectar}{Nectar} datasets are used for training. 
$x$ negs represents that the training dataset contains $x$ negative candidates for each input prompt. 
For example, the $3$ negs dataset is constructed by removing rank-$2$ and rank-$3$ completions from the \href{https://huggingface.co/datasets/berkeley-nest/Nectar}{Nectar} dataset.
}
\label{fig: ablation sampling strategies}
\end{figure}



We also study how varying the sampling strategies in MC-PO impact the PO performance as the quality of sampled preference dataset gets changed.
We develop Max and Min samplings as variants of MC-PO based on the MCMC kernel defined in Algorithm \ref{alg: contrastive divergence kernel}. Here, 
Max (Min) sampling variant outputs the candidate with maximum (minimum) weight among all candidates,
where the weight is calculated as 
$
w_i = \frac{\exp 
\big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\big)} {\sum_{j=0}^L \exp 
\big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_j)
\big)}
$.
Moreover,
we construct preference datasets with varying candidate qualities. For instance, based on the \href{https://huggingface.co/datasets/berkeley-nest/Nectar}{Nectar} dataset, we progressively remove highly ranked completions for each input prompt. 
The first dataset excludes the rank-$2$ completion, while the second dataset excludes both the rank-$2$ and rank-$3$ completions, resulting in diminished candidate quality.
The results are summarized in Fig.~\ref{fig: ablation sampling strategies},
from which we observe the following insights:


\textbf{(1) Sampling from the MCMC kernel yields optimal performance.}
MC-PO achieves a balance between exploitation (sampling according to the categorical distribution), and exploration  
(retaining probabilities for choosing alternative candidates). As detailed in Sec. \ref{sec: mcmc preference optimization}, this approach accurately estimates the gradient of the log-normalization constant, which in turn, leads to improved  performance.

\textbf{(2) Min-based variant leads to low performance and high variance.}
From the NLL estimation viewpoint of PO, dispreferred samples are used for estimating the gradient of the log-normalization constant. 
CD proves that hard negatives yield a more accurate gradient estimation. 
The Min sampling variant, instead, is deemed as the worst sampling strategy according to CD, 
leading to inaccurate gradient estimations and therefore resulting in lower model performance and increased variance.

\textbf{(3) MC-PO correlates with the quality of candidates proportionally.}
When the preference dataset includes five high-quality candidates for each input prompt (referred to as $5$ negs), both MC-PO and Max strategies yield the best model performance. 
However, as high-quality completions are eliminated from the candidate sets, the performance of models optimized with MC-PO and Max variant declines due to the reduced quality of candidates. 
When there is only one candidate per prompt (referred to as $1$ neg), all three sampling strategies are equivalent.





\subsection{Data and Ablation Analysis of MC-PO}
\label{sec:expdata}

\textbf{MC-PO is robust against noisy samples: }
We demonstrate that MC-PO maintains high model performance even when noisy samples are included in the candidate set. 
Differently, it has been proven that when the edit distance between pairs of completions is low, DPO leads to a reduction in the modelâ€™s likelihood of preferring the desired completions \citep{pal2024smaug}. 

For experimental setup, we consider the processed \href{https://huggingface.co/datasets/berkeley-nest/Nectar}{Nectar} dataset and inject a noise sample into the candidate set by randomly switching two tokens of the preferred completion for each input prompt. 
As shown in Table \ref{table: model performance when noise samples are included},
due to the small edit distance between all preference pairs, DPO($-$), which applies the noise sample as dispreferred completion, leads to a degenerated model. 
DPO, which randomly selects a dispferred completion, is impacted by the noise injection.
MC-PO, however,  samples a dispreferred completion based on the log-probabilities of all candidates, chooses semantically hard negatives instead of syntactically similar negatives with small edit distances.






\paragraph{MC-PO benefits from sampling more negatives.}





\begin{table}[th]
\centering
\begin{tabular}{l|cc}
\hline
Model & \multicolumn{2}{|c}{\href{https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-SFT}{Llama-3.1-8B}} \\
Evaluation & Alpaca & Arena \\ \hline
DPO($-$) & 1.08($\pm$0.6) & 3.17($\pm$0.9)
\\
DPO & 23.62($\pm$2.81) & 50.51($\pm$5.59)
\\
\band MC-PO & \textcolor{blue}{28.98($\pm$1.34)} & \textcolor{blue}{58.09($\pm$2.63)} \\ \hline
\end{tabular}
\caption{
Evaluation of models trained with DPO and MC-PO when noise samples are included in the dispreferred candidate set. 
DPO($-$) uses the noise sample as a dispreferred completion, 
DPO selects a dispreferred completion at random, 
and MC-PO samples from a candidate set that includes the noise sample.
}
\label{table: model performance when noise samples are included}
\end{table}
\begin{table}[th]
\centering
\begin{tabular}{l|ccc}
\hline
\multicolumn{4}{c}{\href{https://huggingface.co/datasets/berkeley-nest/Nectar}{Nectar} ~/~ 
\href{https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-SFT}{Llama-3.1-8B-SFT}
}
\\ \hline
Alpaca & $M=1$ & $M=2$ & $M=3$ \\
RNCE & 33.74(2.51) & 33.73(0.49) & 34.36(0.56)
\\
\band MC-PO & \textcolor{blue}{35.84(0.31)} & \textcolor{blue}{36.73(0.59)} & \textcolor{blue}{37.40(0.13)} \\ 
\hline
Arena & $M=1$ & $M=2$ & $M=3$ \\
RNCE & 60.25(2.12) & 61.53(0.29) & 61.16(0.69)
\\
\band MC-PO & \textcolor{blue}{63.77(0.81)} & \textcolor{blue}{64.53(0.60)} & \textcolor{blue}{66.16(0.13)} \\ 
\hline
\end{tabular}
\caption{
Performance comparison of preference-optimized models using RNCE and MC-PO with multiple dispreferred samples.
}
\label{table: model performance sampling strategy more dispreferred completions}
\end{table}
In Table \ref{table: model performance sampling strategy more dispreferred completions},
we examine the performance of MC-PO and RNCE when the number of dispreferred samples is greater than $1$. 
It is evident that RNCE (who uses random sampling) does not achieve notable improvements when having more  dispreferred samples. Conversely, MC-PO (who utilizes an MCMC kernel for sampling) consistently demonstrates improved performance when the number of dispreferred samples increases.


\textbf{MC-PO versus augmented training dataset.}
MC-PO optimizes models on a specialized data format where each input prompt is paired with a preferred completion alongside multiple dispreferred completions. 
Utilizing this data format, an alternative method can be augmenting the training dataset by pairing each preferred completion with each of its corresponding dispreferred completions. 
For instance, in the processed \href{https://huggingface.co/datasets/berkeley-nest/Nectar}{Nectar} dataset, where each prompt contains $5$ candidate completions, we can increase the dataset size four-fold by this augmentation approach. 
Subsequently, we implement DPO on the augmented dataset, as an alternative to compare with MC-PO. 
Recall in Table \ref{table: winrate model performance evaluation llama} that the {\href{https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-SFT}{Llama-3.1-8B-SFT}} model trained with MC-PO achieves winrates of $35.84(\pm0.31)$ and $63.77(\pm0.81)$ on Alpaca-Eval and Arena, respectively. 
This alternative solution, which increases training time by $4$X, leads to winrates of $34.18(\pm1.26)$ and $59.62(\pm1.04)$ on Alpaca-Eval and Arena, respectively.


\textbf{OnMC-PO versus Online DPO.}
We compare OnMC-PO with online DPO \citep{guo2024direct} that applies random sampling to choose a disprerred completion.
Both algorithms generate completion in an batched online setting.
With the \href{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}{Llama-3.1-8B-Instruct} model.
Online DPO achieves winrates of $72.63 \%$ and $73.28 \%$ on Alpaca-eval and Arena, respectively,
On Arena,
OnMC-PO outperforms online DPO by a large margin, achieve a winrate of $77.71 \%$.
