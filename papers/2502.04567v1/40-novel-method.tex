
\begin{algorithm}[h]
\caption{MC Kernel}
\label{alg: contrastive divergence kernel}
\algorithmicinput \;\; $\mat{x}$, $\mat{y}_0$
\\
\algorithmicdo \;\; Sample $\{\mat{y}_i\}_{i=1}^{L}$ from $\mu$
\\
\algorithmicdo \;\; Compute $\{ w_i \}_{i=0}^{L}$,
\; $w_i = \frac{\exp 
\big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\big)} {\sum_{j=0}^L \exp 
\big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_j)
\big)}$
\\
\algorithmicdo \;\; Sample $z \sim {\rm Categorical}\big([w_0, w_1,...,w_{L}]\big)$
\\
\algorithmicoutput \;\; $\mat{y}_z$
\end{algorithm}

\section{Novel Preference Optimization via CD}
\label{sec: preference optimization via contrastive divergence}
Contrastive divergence (CD) uses MCMC methods to estimate the gradient of the log-normalization constant.
CD starts the MCMC sampling from training data rather than a random state, which allows the sampling to converge faster. 
The sampling process involves a small number of MCMC steps (often just one), making it particularly effective for probability models where the normalization constant cannot be easily computed.
CD represents a class of sampling strategies that can be implemented by developing different MC Kernels.
The aforementioned RNCE is a special case of CD with random sampling.
Based on the theoretical foundation that connects PO with sampling-based solutions for NLL estimation,
we first derive a CD algorithm for PO, referred to as MC-PO (Sec. \ref{sec: mcmc preference optimization}).
We then extend MC-PO to an online setting, developing OnMC-PO (Sec. \ref{sec: omcmc preference optimization}).


\subsection{Preference Optimization with MCMC Sampling}
\label{sec: mcmc preference optimization}
To begin with,
we derive the gradient of the NLL estimation in Eq.~\eqref{eq: nll estimation objective function}.
\begin{align}
\label{eq: gradient of mle objective function}
& \nabla_{\boldsymbol\theta} \mathcal{L}^{NLL} (\boldsymbol\theta, \mat{x}, \mat{y})
\\ \nonumber
& =
- \nabla_{\boldsymbol\theta} 
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
+
\nabla_{\boldsymbol\theta} \log Z_{\boldsymbol\theta}(\mat{x}),
\\ \nonumber
& =
- \nabla_{\boldsymbol\theta} 
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
+
\mathbb{E}_{p_{\boldsymbol{\theta}}(\mathbf{y} | \mathbf{x})}
\Big[
\nabla_{\boldsymbol\theta} 
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y})
\Big].
\end{align}
This gradient term is intractable to compute since it involves an expected value over the probability model $p_{\boldsymbol\theta}$ defined in Eq.~\eqref{eq: mle parameterized policy}.
To address this,
CD applies a MC Kernel $K_{\boldsymbol\theta}(\mat{y}' | \mat{x}, \mat{y})$ to estimate the gradient of the log-normalization constant.
the MC Kernel generates samples with high likelihood from $p_{\boldsymbol\theta}$ via a proposal distribution.

We consider the MC Kernel defined in Algorithm \ref{alg: contrastive divergence kernel}.
Given a proposal distribution $\mu$,
this kernel is initialized at a pair of $\mat{x}$ and $\mat{y}_0$ sampled from $\pi^{\ast}$\footnote{As in Sec. \ref{sec: preference optimization via sampling-based solution}, $\mat{y}_0$ can be replace with a preferred completion.}.
At the initial step of the MCMC chain,
it first generates $L$ samples from the proposal distribution,
then it samples the output $\mat{y}'$ from a softmax distribution computed from the unnormalized model over all $L$ samples.
At the next iteration,
this kernel computation is repeated with an initialization of $\mat{y}'$ being the sampled output from the previous step.
The MC Kernel aims to generate a sample with high estimated reward from a proposal distribution.


\paragraph{Proposed: CD samples hard negatives for PO.}
We first connect CD with RNCE and discuss the sampling strategy suggested by CD.
Then we apply this sampling to PO.

\begin{restatable}{lemma}{cdgeneralizernce}
\label{prop: cd generalizes rnce}
When CD runs the MCMC chain for only a single step,
it shares the same objective function with RNCE in Eq.~\eqref{eq: ranking noise contrastive estimation}.
\end{restatable}
The detailed derivation is in Appendix \ref{sec: cd generalizes rnce}.
Given a true observation $\mat{y}_0$ and noisy samples $\{\mat{y}_i\}_{i=1}^M$, the objective functions of CD and RNCE are equivalent under a special condition. 
However, the MC Kernel from CD, as outlined in Algorithm \ref{alg: contrastive divergence kernel}, suggests to sample in proportion to the reward model. 
This leads to more accurate NLL estimation.
Specifically, the gradient of the log-normalization constant in Eq.~\eqref{eq: gradient of mle objective function} is represented as the expected value over the probability model. 
By sampling in proportion with the reward model, it effectively generates samples with higher probability mass from the probability model, thereby improving the coverage of the distribution in the expected value.


Recall the connection between CD and RNCE established in Sec. \ref{sec: preference optimization via sampling-based solution},
existing PO can be formulated as CD.
CD leads to improved accuracy by sampling proportionally to the reward model,
which suggests to choose hard negatives with high estimated rewards for PO.
Here we show that PO benefits from hard negatives.
\begin{restatable}{lemma}{dpogradient}
\label{prop: dpo gradient}
Let $M=1$,
the gradient of the sampling-based objective in Eq.~\eqref{eq: ranking noise contrastive estimation} can be derived as follows,
\begin{align*}
\nabla_{\boldsymbol\theta} \mathcal{L}^{Sample}(\boldsymbol\theta, \mat{x}, \mat{y}_0)
& =
-
\beta
\sigma
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_1)
-
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
\Big)
\\ \nonumber
& \;\;\;\;
\nabla_{\boldsymbol\theta}
\Big(
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
-
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_1)
\Big),
\end{align*}
where $\mat{y}_0$ and $\mat{y}_1$ are preferred and dispreferred completions, respectively.
\end{restatable}
The derivation is in Appendix \ref{sec: dpo gradient derivation}.
When the estimated reward for a dispreferred completion exceeds that of a preferred one, this results in a larger gradient magnitude, leading a more effective update of the target policy $\pi_{\boldsymbol\theta}$. 
The MC Kernel, as outlined in Algorithm \ref{alg: contrastive divergence kernel}, aims to sample in proportion to the estimated reward model to achieve this.


\paragraph{Practical Implementation for MC-PO.}
We propose MC-PO as an offline PO algorithm.
For efficiency,
MC-PO runs the MCMC chain for only a single step.
To implement the MC Kernel in Algorithm \ref{alg: contrastive divergence kernel},
we consider a preference dataset that consists of $L$ candidate completions for each input prompt.
During training,
the MC Kernel only computes the weights that depend on the target policy $\pi_{\boldsymbol\theta}$ and samples based on the categorical distribution.
The kernel computation is fast as it is independent from computing the gradient for parameter updates.




\subsection{MCMC for Online Preference Optimization}
\label{sec: omcmc preference optimization}
\paragraph{Online MC-PO leads to an unbiased gradient estimator.}
Having an unbiased gradient estimation is a standard condition to guarantee general convergence of stochastic gradient descent \citep{bottou2018optimization}.
We demonstrate that in an online setting where the true observation is sampled from the probability model, rather than from the target distribution, 
then the CD estimation of the gradient of log-normalization in Eq.~\eqref{eq: gradient of mle objective function} is an unbiased estimator.
\begin{restatable}{proposition}{UnbiasedMLE}
\label{prop: unbiased mle estimator}
Let 
$\hat{Z}_{\boldsymbol\theta}(\mat{x})$
be an estimation of the normalization constant,
\begin{equation*}
\hat{Z}_{\boldsymbol\theta}(\mat{x})
=
\frac{1}{M + 1}
{\sum_{i=0}^M \exp
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\Big)}.
\end{equation*}
When $\mat{y}_0$ is sampled from the probability model $p_{\boldsymbol\theta}$,
then
\begin{equation*}
\mathbb{E}_{p_{\boldsymbol\theta}(\mat{y}_0 | \mat{x}) 
\mu(\{\mat{y}_i\}_{i=1}^M | \mat{x}))
}
\Big[
\nabla_{\boldsymbol\theta}
\log
\hat{Z}(\mat{x})
\Big]
=
\nabla_{\boldsymbol\theta} \log Z_{\boldsymbol\theta}(\mat{x}),
\end{equation*}
where $\mu(\{\mat{y}_i\}_{i=1}^M | \mat{x})) = \prod_{i=1}^M \mu(\mat{y}_i | \mat{x})$.
\end{restatable}
The detailed derivation is in Appendix \ref{sec: unbiased mle estimator}.
This explains the clear advantage of online methods over offline methods \citep{tang2024understanding}.
Specifically,
online PO algorithms generate preferred completions from the target policy that is proportional to the probability model $p_{\boldsymbol\theta}$,
intends to have an unbiased gradient estimation.

\paragraph{Practical implementation for OnMC-PO.}
As suggested by Proposition \ref{prop: unbiased mle estimator},
it is desirable to sample the preferred completion from the probability model $p_{\boldsymbol\theta}$.
We implement online MC-PO (OnMC-PO) as an extension of MC-PO.
Given a input prompt,
we sample multiple completions from the target policy $\pi_{\boldsymbol\theta}$ and identify the most preferred one as $\mat{y}_0$.
Moreover,
since the policy update at each step is relatively small,
we consider a batched online algorithm \citep{rosset2024direct} where sampling from $\pi_{\boldsymbol\theta}$ is done after a number of gradient updates.
