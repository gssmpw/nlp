[
  {
    "index": 0,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "schulman2017proximal",
        "author": "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",
        "title": "Proximal policy optimization algorithms"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      },
      {
        "key": "azar2024general",
        "author": "Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele",
        "title": "A general theoretical paradigm to understand learning from human preferences"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "azar2024general",
        "author": "Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele",
        "title": "A general theoretical paradigm to understand learning from human preferences"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "meng2024simpo",
        "author": "Meng, Yu and Xia, Mengzhou and Chen, Danqi",
        "title": "Simpo: Simple preference optimization with a reference-free reward"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "liu2024provably",
        "author": "Liu, Zhihan and Lu, Miao and Zhang, Shenao and Liu, Boyi and Guo, Hongyi and Yang, Yingxiang and Blanchet, Jose and Wang, Zhaoran",
        "title": "Provably mitigating overoptimization in rlhf: Your sft loss is implicitly an adversarial regularizer"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "ethayarajh2024kto",
        "author": "Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe",
        "title": "Kto: Model alignment as prospect theoretic optimization"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "jung2024binary",
        "author": "Jung, Seungjae and Han, Gunsoo and Nam, Daniel Wontae and On, Kyoung-Woon",
        "title": "Binary classifier optimization for large language model alignment"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "chen2024noise",
        "author": "Huayu Chen and Guande He and Lifan Yuan and Ganqu Cui and Hang Su and Jun Zhu",
        "title": "Noise Contrastive Alignment of Language Models with Explicit Rewards"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "gutmann2010noise",
        "author": "Gutmann, Michael and Hyv{\\\"a}rinen, Aapo",
        "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "chen2024noise",
        "author": "Huayu Chen and Guande He and Lifan Yuan and Ganqu Cui and Hang Su and Jun Zhu",
        "title": "Noise Contrastive Alignment of Language Models with Explicit Rewards"
      }
    ]
  }
]