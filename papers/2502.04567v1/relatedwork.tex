\section{Related Works}
\label{sec: related works}
Aligning LLMs with human preferences has predominately been considered as an RL problem \citep{ouyang2022training}.
However,
the on-policy nature of RLHF necessitates learning a reward model from preference data first,
then maximize the estimated reward model with RL techniques,
leading to a two-stage optimization process \citep{schulman2017proximal}.
Recent developments in preference-based alignment techniques have streamlined this process \citep{rafailov2024direct,azar2024general}.
They enable direct model alignment through a singular loss.
We categorize existing DPO variants as contrastive-based or classification-based approaches according to their objective functions.
Contrastive-based approaches maximize the difference of the predicted likelihoods between preferred and dispreferred completions,
while classification-based approaches conduct maximization on the preferred and minimization on dispreferred completions, respectively.

Some notable contastive-based algorithms include 
DPO \citep{rafailov2024direct}
that is derived from reparametrizing the reward function in RLHF to directly learn a policy from preference data.
IPO \citep{azar2024general} that replaces the logistic loss with a squared loss to address the shortcomings of Bradely-Terry preference modeling in cases where preference data are highly deterministic.
SimPO \citep{meng2024simpo} introduces length regularization on the log-probabilities of both preferred and dispreferred completions, eliminating the need for a reference model. 
RPO \citep{liu2024provably} that derives a superivsed next-word prediction regularization to prevent the decrease of the likelihood to predict preferred completions.
The first classification-based algorithms is
KTO \citep{ethayarajh2024kto} that formulate both maximization and minimization objectives w.r.t. a reference point.
BCO \citep{jung2024binary}
derives the reference point that minimizes the gap with DPO.
NCA \citep{chen2024noise} is derived from noise contrastive estimation for working with reward data \citep{gutmann2010noise}.

In this work, we formulate the alignment problem as sampling-based solutions to solve NLL estimation. 
We first propose the RNCE based sampling as a general PO solution that randomly selects dispreferred completions from a set of candidates.
This solution is similar to InfoNCA \citep{chen2024noise} that generalizes DPO to adopt multiple dispreferred completions. 
Different from InfoNCA, our proposed NLL estimation perspective of model alignment interprets dispreferred completions as the estimative samples to compute the normalization constant, which provides theoretical guidance on developing sampling strategies to choose dispreferred completions for PO.
Based on the NLL estimation formulation, we further develop MC-PO that uses an MCMC kernel to select high-quality dispreferred completions, leading to improved model performance.