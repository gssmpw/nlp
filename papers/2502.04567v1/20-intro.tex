

\section{Introduction}
\label{sec: introduction}
While large language models (LLMs) learn broad world knowledge, aligning their behavior precisely with human values is challenging due to the unsupervised nature of their training.
Reinforcement learning from human feedback (RLHF) \citep{ouyang2022training} has emerged as a class of effective algorithms to align LLMs \citep{schulman2017proximal}.
Recent works on direct preference optimization (DPO) \citep{rafailov2024direct} and its variants (e.g., Identity preference optimization \citep{azar2024general}) directly optimize an LLM to adhere to human values, without explicit reward modeling or RL.

The data for these algorithms are often collected in the form of preferences \citep{ziegler2019fine}.
This leads to more consistent labeling across human annotators as it reduces their cognitive load and avoids the need for absolute judgment, which can be more subjective. 
Existing studies on PO have predominately considered creating pairwise preference data via simple heuristics,
such as choosing a dispreferred completion by maximizing the gap with the preferred response in terms of human (AI) ratings \citep{tunstall2023zephyr,lambert2024t}.
However,
none of these heuristics has a full theoretical justification.
Here, we ask the question:
{\em ``How to choose dispreferred completion(s) for PO?"}


To answer this question,
we develop a novel PO framework that provides theoretical guidance on developing effective sampling strategies to select dispreferred completions.
To achieve this, we formulate PO as minimizing the negative log-likelihood (NLL) of a probability model. 
Unfortunately, NLL includes a normalization constant that is in the form of an integral, and thus, usually intractable. 
To address this issue, we propose to estimate the normalization constant using a sampling strategy~\citep{naesseth2024elementssequentialmontecarlo}. 
For instance, the ranking noise contrastive estimation applies conditional importance sampling to generate samples from a proposal distribution and uses them to approximate the integral within the normalization constant~\citep{olmin2024connection}. As we will show, these samples can act as dispreferred completions in PO, thus establishing a connection between the proposed NLL estimation and existing PO algorithms. 
Such a connection enables us to apply advanced sampling strategies, from the literature on estimating the normalization constant in NLL, for generating dispreferred completions in PO.

After formulating PO as an NLL minimization problem, we propose to use an advanced sampling strategy, contrastive divergence (CD), to estimate the normalization constant in this NLL. 
CD applies Markov chain Monte Carlo (MCMC) sampling to approximate the gradient of the log-normalization constant \citep{hinton2002training}. 
The central component in CD is the MC kernel that generates the next sample conditioned on the current one. 
The MC kernel produces samples with high probability mass w.r.t. the probability model, which leads to accurate estimation for the gradient of the log-normalization constant. 
In our PO formulation, we define the probability model to be proportional to the log-likelihood of the target policy. Thus, sampling proportionally to the probability model can be interpreted as selecting a \textit{hard negative}, i.e.,~a dispreferred completion that closely resembles the preferred one, and thus, makes it challenging for the model to distinguish between them. 
We demonstrate the effectiveness of this sampling strategy both theoretically and empirically.

\begin{figure}[t]
\centering
\begin{minipage}{.99\columnwidth}
\centering
    \includegraphics[width = \linewidth]{figures/high_level.png}
\end{minipage}
\caption{
\textbf{Left:} existing studies choose a dispreferred completion as the one that maximizes the gap with the preferred completion based on human (or AI) ranked scores.
\textbf{Right:} we propose theoretical guidance to sample dispreferred completion(s) proportionally to the parameterized reward model.
As the parameters evolve during training, the sampling of dispreferred completion changes.
}
\label{fig: high level demonstration}
\end{figure}

We summarize our main contributions below.
\begin{itemize}
    \item 
    We present a novel PO framework that provides theoretical guidance on developing sampling strategies to generate dispreferred completions.
    This is achieved by formulating the alignment problem as NLL estimation and solve it via a sampling strategy.
    \item 
    We propose MC-PO as an offline PO algorithm.
    Given a preference dataset that consists of multiple dispreferred completions for each input prompt,
    MC-PO applies a MC kernel from CD to select \textit{hard negatives} as dispreferred completions proportionally to the log-likelihood of the target policy.
    \item
    Our theoretical analysis suggests that sampling preferred completions from the target policy leads to an unbiased gradient estimation of the normalization constant. 
    Building on this insight, we propose OnMC-PO, an extension of MC-PO to the online settings.
    \item 
    We demonstrate that MC-PO outperforms existing SOTA baselines, and OnMC-PO leads to further improvement.
    Moreover, we numerically validate the effectiveness of various sampling strategies, prove that the MC kernel leads to the optimal model performance.
\end{itemize}



