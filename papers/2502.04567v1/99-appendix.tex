

\newpage
\appendix
\onecolumn

\section{Theoretical Derivations}
\label{sec: theoretical derivations}

\subsection{Background on Preference Optimization}
\label{sec: background on rlhf}
\paragraph{RLHF formulation.}
Language models have shown impressive capabilities in the past few years by generating diverse and compelling text from human input prompts.
One of the key ingredients behind the success of language models is post-training alignment.
Reinforcement learning from human feedback (RLHF) aims to align a target policy $\pi_{\boldsymbol\theta}$ with human preference based on an reward model $r(\mat{x}, \mat{y})$ that approximates human judgements.
This optimization problem is formulated as 
\begin{equation*}
\max \limits_{\pi_{\boldsymbol\theta}}
\mathbb{E}_{{\mat{x} \sim p,\; \mat{y} \sim \pi_\theta(\cdot|\mat{x})}}
[r(\mat{x}, \mat{y})]
-
\beta \cdot KL[\pi_{\boldsymbol\theta}(\mat{y} | \mat{x}) || \pi_{\rm ref}(\mat{y} | \mat{x})],
\end{equation*}
where $\beta$ is a hyper-parameter controlling the deviation from the base reference policy $\pi_{\rm ref}$.
The added constraint is important,
as it prevents the model from deviating too far from the distribution on which the reward is accurate,
as well as maintaining the generation diversity and preventing mode-collapse to single high-reward answers.

Prior works \citep{go2023aligning, peters2007reinforcement} prove that the optimal solution to the KL-constrained reward maximization objective takes the following form,
\begin{equation*}
\pi^{\ast}(\mat{y} | \mat{x})
=
\frac{1} {Z(x)}
\pi_{\rm ref}(y | x) 
\exp 
\Big(
\frac{1} {\beta} r(x, y)
\Big),
\end{equation*}
where $Z(x)$ is the partition function that normalizes the probability.
\begin{equation*}
Z(x)
=
\sum_{\mat{y}}
\pi_{\rm ref}(y | x) 
\exp 
\Big(
\frac{1} {\beta} r(x, y)
\Big).
\end{equation*}
Since the space of model completions is combinatorilly large, computing $Z(x)$ exactly is often infeasible.

\paragraph{Direct preference optimization \citep{rafailov2024direct}.}
Direct preference optimization (DPO) enables to directly optimize a language model to adhere to human preferences,
without explicit reward modeling or reinforcement learning.
Specifically,
DPO reparameterizes the reward model in terms of the target policy,
which enables directly optimizing the target policy from preference dataset.
To begin with,
we can rearrange the close-form solution of RLHF to express the reward function in terms of its corresponding optimal policy, the reference policy,
and the partition function,
\begin{equation*}
r(\mat{x}, \mat{y})
=
\beta 
\log
\frac{\pi^{\ast}(\mat{y} | \mat{x})}
{\pi_{\rm ref}(\mat{y} | \mat{x})}
+
\beta
\log Z(\mat{x}).
\end{equation*}
Substituting this into the Bradley-Terry preference model \citep{bradley1952rank} yields
\begin{equation*}
    p^{*}(\mat{y}_0 \succ \mat{y}_1 | \mat{x})
    = \frac{1}{1 + \exp\left( 
        \beta \log \frac{\pi^{*}(\mat{y}_1 | \mat{x})}{\pi_{\rm ref}(\mat{y}_1 | \mat{x})}
        - \beta \log \frac{\pi^{*}(\mat{y}_0 | \mat{x})}{\pi_{\rm ref}(\mat{y}_0 | \mat{x})}
    \right)},
\end{equation*}
where $\mat{y}_0$ and $\mat{y}_1$ denote preferred and dispreferred completions, respectively.


DPO reformulates this as a maximum likelihood objective over a preference dataset $\mathcal{D} = \{(\mat{x}^{(i)}, \mat{y}_0^{(i)}, \mat{y}_1^{(i)})\}$, leading to:
\begin{equation*}
    \pi_{\boldsymbol{\theta}}^{*}
    = \arg\min_{\pi_{\boldsymbol{\theta}}} 
    \mathbb{E}_{(\mat{x},\mat{y}_0,\mat{y}_1) \sim \mathcal{D}}
    \left[ 
        -\log \sigma\left( 
            \beta \log \frac{\pi_{\boldsymbol{\theta}}(\mat{y}_0 | \mat{x})}{\pi_{\rm ref}(\mat{y}_0 | \mat{x})} 
            - \beta \log \frac{\pi_{\boldsymbol{\theta}}(\mat{y}_1 | \mat{x})}{\pi_{\rm ref}(\mat{y}_1 | \mat{x})}
        \right)
    \right],
\end{equation*}
where $\sigma$ denotes the logistic function. 
This approach enables direct optimization of language models using preference data while avoiding instabilities associated with RL-based training.


\subsection{Proof of Proposition \ref{prop: ranking noise contrastive estimation objective}}
\label{sec: RNCE objective function derivation}
\rnce*

\begin{proof}
The Ranking Noise Contrastive Estimation (RNCE) is based on a multi-class classification problem with a single observation point and multiple noise ones from a proposal distribution.

Suppose we have $\mat{y}_0 \sim \pi^{\ast}(\mat{y} | \mat{x})$,
and noise samples $\mat{y}_i \sim \mu(\mat{y} | \mat{x})$, for $i = 1,2,...,M$.
We use $\{ \mat{y}_i \}_{i=0}^M$ to denote all samples.
Let the variable $z \in \{0, 1, ..., M \}$ denote the index of the observation sampled from $\pi^{\ast}$,
and assume that all outcomes are equally probable a priori,
e.g.,
$P(z=i) = 1 / (M + 1)$, for $i = 0, 1,...,M$.
Conditioned on $\{ \mat{y}_i \}_{i=0}^M$, we want the probability model to maximize the posterior probability of identifying $z = 0$ (the probability of identifying the index corresponding to the observation sampled from $\pi^{\ast}$).

\begin{equation*}
\min \limits_{\boldsymbol\theta}
- \log P(z = 0 | \mat{x}, \{ \mat{y}_i \}_{i=0}^M).
\end{equation*}

According to the Bayes rule and the law of total probability,
\begin{equation*}
P(z = 0 | \mat{x}, \{ \mat{y}_i \}_{i=0}^M)
=
\frac{P(\{ \mat{y}_i \}_{i=0}^M | \mat{x}, z = 0) P(z = 0)}
{P(\{ \mat{y}_i \}_{i=0}^M | \mat{x})}
=
\frac{P(\{ \mat{y}_i \}_{i=0}^M | \mat{x}, z = 0) P(z = 0)}
{\sum_{j=0}^M P(\{ \mat{y}_i \}_{i=0}^M | \mat{x}, z = j) P(z = j)}.
\end{equation*}

Recall that all outcomes of $z$ are equally probable,
$P(z=0) = P(z=1) = ... = P(z=M)$,
\begin{equation*}
P(z = 0 | \mat{x}, \{ \mat{y}_i \}_{i=0}^M)
=
\frac{P(\{ \mat{y}_i \}_{i=0}^M | \mat{x}, z = 0)}
{\sum_{j=0}^M P(\{ \mat{y}_i \}_{i=0}^M | \mat{x}, z = j)}.
\end{equation*}

Since $z$ is the index corresponding to the observation sampled from $\pi^{\ast}$,
\begin{equation*}
P(\{ \mat{y}_i \}_{i=0}^M | \mat{x}, z = i)
=
\pi^{\ast}(\mat{y}_i | \mat{x})
\prod_{j \neq i}
\mu(\mat{y}_j | \mat{x}).
\end{equation*}

Therefore,
\begin{align*}
P(z = 0 | \mat{x}, \{ \mat{y}_i \}_{i=0}^M)
& =\frac{P(\{ \mat{y}_i \}_{i=0}^M | \mat{x}, z = 0)}
{\sum_{j=0}^M P(\{ \mat{y}_i \}_{i=0}^M | \mat{x}, z = j)},
\\
& =
\frac{\pi^{\ast}(\mat{y}_0 | \mat{x}) \prod_{i=1}^M
\mu(\mat{y}_i | \mat{x})}
{\sum_{i=0}^M \pi^{\ast}(\mat{y}_i | \mat{x}) \prod_{j \neq i} \mu(\mat{y}_j | \mat{x})},
\\
& =
\frac{\pi^{\ast}(\mat{y}_0 | \mat{x}) / \mu(\mat{y}_0 | \mat{x}) \prod_{i=0}^M \mu(\mat{y}_i | \mat{x})}
{\sum_{i=0}^M \pi^{\ast}(\mat{y}_i | \mat{x}) / \mu(\mat{y}_i | \mat{x}) \prod_{j=0}^M \mu(\mat{y}_j | \mat{x})},
\\
& =
\frac{\pi^{\ast}(\mat{y}_0 | \mat{x}) / \mu(\mat{y}_0 | \mat{x})}
{\sum_{i=0}^M \pi^{\ast}(\mat{y}_i | \mat{x}) / \mu(\mat{y}_i | \mat{x})}.
\end{align*}

We use the probability model defined in Eq.~\eqref{eq: mle parameterized policy} to estimate $\pi^{\ast}$,
\begin{equation*}
\pi^{\ast}(\mat{y} | \mat{x})
\approx
p_{\boldsymbol\theta}(\mat{y} | \mat{x})
: =
\frac{1} {Z_{\boldsymbol\theta}(\mat{x})}
\mu(\mat{y} | \mat{x}) 
\exp\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y})
\Big),
\end{equation*}
this leads to the following
\begin{equation*}
P(z = 0 | \mat{x}, \{ \mat{y}_i \}_{i=0}^M)
=
\frac{p_{\boldsymbol\theta}(\mat{y}_0 | \mat{x}) / \mu(\mat{y}_0 | \mat{x})}
{\sum_{i=0}^M p_{\boldsymbol\theta}(\mat{y}_i | \mat{x}) / \mu(\mat{y}_i | \mat{x})}
= 
\frac{\exp\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
\Big)}
{\sum_{i=0}^M \exp\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\Big)}.
\end{equation*}

Finally,
\begin{align*}
\min \limits_{\boldsymbol\theta}
- \log P(z = 0 | \mat{x}, \{ \mat{y}_i \}_{i=0}^M)
& =
\min \limits_{\boldsymbol\theta}
- \log
\Big(
\frac{\exp\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
\Big)}
{\sum_{i=0}^M \exp\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\Big)}
\Big)
\\
& =
\min \limits_{\boldsymbol\theta}
-
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
\Big)
+
\log
{\sum_{i=0}^M \exp
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\Big)}.
\end{align*}
\end{proof}



\subsection{Proof of Lemma \ref{prop: cd generalizes rnce}}
\label{sec: cd generalizes rnce}
\cdgeneralizernce*

\begin{proof}
Let $\mat{y}_0 \sim \pi^{\ast}(\mat{y} | \mat{x})$
and
$\mat{y}_i \sim \mu(\mat{y} | \mat{x})$ for $i \in [1,...,M]$.
Recall the intractable gradient term in Eq.~\eqref{eq: gradient of mle objective function},
\begin{align}
\label{eq: cd estimation gradient of log norm constant}
\mathbb{E}_{p_{\boldsymbol\theta}(\mat{y} | \mat{x})}
\Big[
\nabla_{\boldsymbol\theta} 
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y})
\Big]
& =
\mathbb{E}_{\mu(\mat{y} | \mat{x})} 
\Big[
\frac{p_{\boldsymbol\theta}(\mat{y} | \mat{x})} {\mu(\mat{y} | \mat{x})} 
\nabla_{\boldsymbol\theta} 
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y})
\Big],
\\ \nonumber
& =
\mathbb{E}_{\mu(\mat{y} | \mat{x})} 
\Big[
\frac{
\exp 
\big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y})
\big)
}
{Z_{\boldsymbol\theta}(\mat{x})}
\nabla_{\boldsymbol\theta} 
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y})
\Big],
\\ \nonumber
& \approx
\sum_{i=0}^M
\frac{\exp 
\big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\big)}
{\sum_{j=0}^M \exp 
\big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_j)
\big)}
\nabla_{\boldsymbol\theta}
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
,
\\ \nonumber
& =
\nabla_{\boldsymbol\theta}
\log
{\sum_{i=0}^M \exp
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\Big)}.
\end{align}
Threfore,
CD applies importance sampling with a proposal distribution $\mu$ to estimate the gradient of the log-normalization constant.
\end{proof}


\subsection{Proof of Lemma \ref{prop: dpo gradient}}
\label{sec: dpo gradient derivation}
\dpogradient*
\begin{proof}
Recall the sampling-based solution of the NLL estimation objective function in Eq.~\eqref{eq: ranking noise contrastive estimation},
let $M=1$,
\begin{align*}
\nabla_{\boldsymbol\theta} \mathcal{L}^{Sample}(\boldsymbol\theta, \mat{x}, \mat{y}_0)
& =
\nabla_{\boldsymbol\theta}
\Big[
-
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
+
\log
{\sum_{i=0}^1 \exp
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\Big)}
\Big],
\\ \nonumber
& =
\nabla_{\boldsymbol\theta}
\Big[
-
\log
\frac{
\exp
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
\Big)
}
{
\exp
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
\Big)
+
\exp
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_1)
\Big)
}
\Big],
\\ \nonumber
& =
\nabla_{\boldsymbol\theta}
\Big[
-
\log \sigma
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
-
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_1)
\Big)
\Big],
\\ \nonumber
& =
- \Big(1 - \sigma
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
-
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_1)
\Big)
\Big)
\nabla_{\boldsymbol\theta}
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
-
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_1)
\Big),
\\ \nonumber
& =
- \beta \sigma
\Big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_1)
-
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
\Big)
\nabla_{\boldsymbol\theta}
\Big(
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_0)
-
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_1)
\Big).
\end{align*}
The last equality uses $1 - \sigma(x) = \sigma(-x)$.
\end{proof}



\subsection{Proof of Proposition \ref{prop: unbiased mle estimator}}
\label{sec: unbiased mle estimator}

\begin{lemma}
\label{lemma: appendix probability simplification}
Let $z$ denote the index corresponding to the observation sampled from the probability model $p_{\boldsymbol\theta}$,
\begin{equation*}
\begin{cases} 
    \mat{y}_z \sim p_{\boldsymbol\theta}(\mat{y} | \mat{x}), \\
    \mat{y}_j \sim \mu(\mat{y} | \mat{x}),
    \;\; {\rm for} \; j \neq z, j = 0,1,...,M.
\end{cases}
\end{equation*}
Then the marginal distribution
\begin{equation*}
P(\{\mat{y}_i\}_{i=0}^M)
=
\frac{\hat{Z}_{\boldsymbol\theta}(\mat{x})} {Z_{\boldsymbol\theta}(\mat{x})}
\prod_{j=0}^M \mu(\mat{y}_j | \mat{x}).
\end{equation*}
\end{lemma}

\begin{proof}
Without loss of generality, we consider $z=0$.
Let 
\begin{equation*}
P(z=0, \{\mat{y}_i\}_{i=0}^M) = p_{\boldsymbol\theta}(\mat{y}_0 | \mat{x}) 
\prod_{i=1}^M \mu(\mat{y}_i | \mat{x}).
\end{equation*}
Then,
assume that all outcomes of $z$ are equally probable a priori,
e.g.,
$P(z=i) = 1 / (M + 1)$, for $i = 0, 1,...,M$,
\begin{equation*}
P(z=0, \{\mat{y}_i\}_{i=0}^M)
=
P(z = 0) 
\cdot
P(\mat{y}_0 | \mat{x}, z = 0)
\cdot
\prod_{i=1}^M P(\mat{y}_i | \mat{x}, z = 0)
=
\frac{1}{M + 1}
\cdot
p_{\boldsymbol\theta}(\mat{y}_0 | \mat{x})
\cdot
\prod_{i=1}^M \mu(\mat{y}_i | \mat{x}).
\end{equation*}

The marginal distribution $P(\{\mat{y}_i\}_{i=0}^M)$ can be computed by marginalizing $P(z, \{\mat{y}_i\}_{i=0}^M)$ over the index $z$.
\begin{align*}
P(\{\mat{y}_i\}_{i=0}^M)
& =
\sum_{i=0}^M
P(z=i, \{\mat{y}_i\}_{i=0}^M)
\\ \nonumber
& =
\sum_{i=0}^M
\frac{1}{M + 1}
\cdot
p_{\boldsymbol\theta}(\mat{y}_i | \mat{x})
\cdot
\prod_{j \neq i} \mu(\mat{y}_j | \mat{x}),
\\ \nonumber
& =
\sum_{i=0}^M
\frac{1}{M + 1}
\cdot
\frac{\mu(\mat{y}_i | \mat{x}) 
\exp
\big(\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\big) } {Z_{\boldsymbol\theta}(\mat{x})}
\frac{\mu(\mat{y}_i | \mat{x})} {\mu(\mat{y}_i | \mat{x})}
\cdot
\prod_{j \neq i} \mu(\mat{y}_j | \mat{x}),
\\ \nonumber
& =
\frac{1} {Z_{\boldsymbol\theta}(\mat{x})(M + 1)}
\cdot
\Big(
\prod_{j=0}^M \mu(\mat{y}_j | \mat{x})
\Big)
\Big(
\sum_{i=0}^M
\exp\big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\big)
\Big),
\\ \nonumber
& =
\frac{\hat{Z}_{\boldsymbol\theta}(\mat{x})} {Z_{\boldsymbol\theta}(\mat{x})}
\prod_{j=0}^M \mu(\mat{y}_j | \mat{x}),
\end{align*}

where $\hat{Z}_{\boldsymbol\theta}(\mat{x}) = \frac{1} {M + 1} \sum_{i=0}^M \exp
\big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\big)
$.
\end{proof}


\UnbiasedMLE*



\begin{proof}




We prove that $\nabla_{\boldsymbol\theta}\log\hat{Z}_{\boldsymbol\theta}(\mat{x})$ is an unbiased estimator when $\mat{y}_0$ is sampled from the probability model $p_{\boldsymbol\theta}$.
\begin{align*}
\mathbb{E}_{p_{\boldsymbol\theta}(\mat{y}_0 | \mat{x}) 
\mu(\{\mat{y}_i\}_{i=1}^M | \mat{x})
}
\Big[
\nabla_{\boldsymbol\theta} \log \hat{Z}_{\boldsymbol\theta}(\mat{x})
\Big]
& =
\mathbb{E}_{p_{\boldsymbol\theta}(\mat{y}_0 | \mat{x}) 
\mu(\{\mat{y}_i\}_{i=1}^M | \mat{x})
}
\Big[
\frac{1} {\hat{Z}(\mat{x})}
\nabla_{\boldsymbol\theta} \hat{Z}_{\boldsymbol\theta}(\mat{x})
\Big],
\\ \nonumber
& =
\frac{1} {M + 1}
\sum_{i=0}^M
\mathbb{E}_{p_{\boldsymbol\theta}(\mat{y}_0 | \mat{x}) 
\mu(\{\mat{y}_i\}_{i=1}^M | \mat{x})
}
\Big[
\frac{\nabla_{\boldsymbol\theta}\exp\big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\big)
}
{\hat{Z}_{\boldsymbol\theta}(\mat{x})}
\Big],
\\ \nonumber
& =
\frac{1} {M + 1}
\sum_{i=0}^M
\mathbb{E}_{
P(z, \{\mat{y}_i \}_{i=0}^M)
}
\Big[
\frac{\nabla_{\boldsymbol\theta}\exp
\big( 
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\big)
} 
{\hat{Z}_{\boldsymbol\theta}(\mat{x})}
\Big],
\end{align*}
where 
$p_{\boldsymbol\theta}(\mat{y}_0 | \mat{x}) 
\mu(\{\mat{y}_i\}_{i=1}^M | \mat{x})
=
P(z, \{\mat{y}_i \}_{i=0}^M)
$ by definition.

Since the integrand 
$
\frac{\nabla_{\boldsymbol\theta}\exp\big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\big)
} 
{\hat{Z}_{\boldsymbol\theta}(\mat{x})}
$ does not depend on the index $z$,
\begin{align}
\label{eq: appendix derivation unbiased gradient 2}
\mathbb{E}_{p_{\boldsymbol\theta}(\mat{y}_0 | \mat{x}) 
\mu(\{\mat{y}_i\}_{i=1}^M | \mat{x})
}
\Big[
\nabla_{\boldsymbol\theta} \log \hat{Z}_{\boldsymbol\theta}(\mat{x})
\Big]
=
\frac{1} {M + 1}
\sum_{i=0}^M
\mathbb{E}_{
P(\{\mat{y}_i \}_{i=0}^M)
}
\Big[
\frac{\nabla_{\boldsymbol\theta}\exp\big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\big)
} 
{\hat{Z}_{\boldsymbol\theta}(\mat{x})}
\Big].
\end{align}


Recall that 
$P(\{\mat{y}_i\}_{i=0}^M)
=
\frac{\hat{Z}_{\boldsymbol\theta}(\mat{x})} {Z_{\boldsymbol\theta}(\mat{x})}
\prod_{j=0}^M \mu(\mat{y}_j | \mat{x})$ from Lemma \ref{lemma: appendix probability simplification},

\begin{align*}
\mathbb{E}_{P(\{\mat{y}_i \}_{i=0}^M)
}
\Big[
\frac{\nabla_{\boldsymbol\theta}\exp\big( 
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\big)} 
{\hat{Z}_{\boldsymbol\theta}(\mat{x})
}
\Big]
& =
\int
\frac{\nabla_{\boldsymbol\theta}\exp\big(
\beta
\log 
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\big)}
{\hat{Z}_{\boldsymbol\theta}(\mat{x})}
\cdot
\frac{\hat{Z}_{\boldsymbol\theta}(\mat{x})} {Z_{\boldsymbol\theta}(\mat{x})}
\cdot
\prod_{i=0}^M \mu(\mat{y}_i | \mat{x})
\cdot
d \{ \mat{y}_i \}_{i=0}^M,
\\
& =
\frac{1}{Z_{\boldsymbol\theta}(\mat{x})}
\int
\nabla_{\boldsymbol\theta}\exp\big( 
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\big)
\cdot
\mu(\mat{y}_i | \mat{x})
\cdot
d\mat{y}_i
\cdot
\prod_{j \neq i}
\mu(\mat{y}_i | \mat{x}) 
\cdot
d\{\mat{y}_j\}_{j\neq i},
\end{align*}
where 
$
\prod_{j \neq i}
\mu(\mat{y}_i | \mat{x}) d\{\mat{y}_j\}_{j\neq i} = 1
$.
Then,
\begin{align*}
\mathbb{E}_{P(\{\mat{y}_i \}_{i=0}^M)
}
\Big[
\frac{\nabla_{\boldsymbol\theta}\exp\big( 
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\big)} 
{\hat{Z}_{\boldsymbol\theta}(\mat{x})
}
\Big]
& =
\frac{1}{Z_{\boldsymbol\theta}(\mat{x})}
\int
\nabla_{\boldsymbol\theta}\exp\big( 
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\big)
\cdot
\mu(\mat{y}_i | \mat{x})
\cdot
d\mat{y}_i,
\\
& =
\frac{1}{Z_{\boldsymbol\theta}(\mat{x})}
\int
\exp\big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y})
\big)
\cdot
\nabla_{\boldsymbol\theta}
\log
\exp\big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y})
\big)
\cdot
\mu(\mat{y} | \mat{x}) 
\cdot
d\mat{y}.
\end{align*}

Recall the expression of the probability model,
$
\exp\big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\big)
=
\frac{Z_{\boldsymbol\theta}(\mat{x}) p_{\boldsymbol\theta}(\mat{y} | \mat{x})} {\mu(\mat{y} | \mat{x})}
$.
Then,
\begin{align*}
\label{eq: appendix derivation unbiased gradient 3}
\mathbb{E}_{P(\{\mat{y}_i \}_{i=0}^M)
}
\Big[
\frac{\nabla_{\boldsymbol\theta}\exp\big( 
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\big)} 
{\hat{Z}_{\boldsymbol\theta}(\mat{x})
}
\Big]
& =
\frac{1}{Z_{\boldsymbol\theta}(\mat{x})}
\int
\frac{Z_{\boldsymbol\theta}(\mat{x}) p_{\boldsymbol\theta}(\mat{y} | \mat{x})} {\mu(\mat{y} | \mat{x})}
\cdot
\nabla_{\boldsymbol\theta}
\log 
\frac{Z_{\boldsymbol\theta}(\mat{x}) p_{\boldsymbol\theta}(\mat{y} | \mat{x})} {\mu(\mat{y} | \mat{x})}
\cdot
\mu(\mat{y} | \mat{x}) 
\cdot
d\mat{y},
\\ \nonumber
& =
\int
p_{\boldsymbol\theta}(\mat{y} | \mat{x})
[
\nabla_{\boldsymbol\theta} \log Z_{\boldsymbol\theta}(\mat{x})
+
\nabla_{\boldsymbol\theta} \log p_{\boldsymbol\theta}(\mat{y} | \mat{x}) 
-
\nabla_{\boldsymbol\theta} \log \mu(\mat{y} | \mat{x})
]
d\mat{y},
\\ \nonumber
& =
\int p_{\boldsymbol\theta}(\mat{y} | \mat{x})
\nabla_{\boldsymbol\theta} \log Z_{\boldsymbol\theta}(\mat{x})
d \mat{y}
+
\int p_{\boldsymbol\theta}(\mat{y} | \mat{x})
\nabla_{\boldsymbol\theta} \log p_{\boldsymbol\theta}(\mat{y} | \mat{x})
d \mat{y},
\\ \nonumber
& =
\nabla_{\boldsymbol\theta} \log Z_{\boldsymbol\theta}(\mat{x})
\int
p_{\boldsymbol\theta}(\mat{y} | \mat{x}) d\mat{y}
+
\int
\nabla_{\boldsymbol\theta}
p_{\boldsymbol\theta}(\mat{y} | \mat{x}) d\mat{y},
\\ \nonumber
& =
\nabla_{\boldsymbol\theta} \log Z_{\boldsymbol\theta}(\mat{x})
+
\nabla_{\boldsymbol\theta}
\int
p_{\boldsymbol\theta}(\mat{y} | \mat{x}) d\mat{y},
\\ \nonumber
& =
\nabla_{\boldsymbol\theta} \log Z_{\boldsymbol\theta}(\mat{x}).
\end{align*}

Recall in Eq.~\eqref{eq: appendix derivation unbiased gradient 2},
\begin{align*}
\mathbb{E}_{p_{\boldsymbol\theta}(\mat{y}_0 | \mat{x}) 
\mu(\{\mat{y}_i\}_{i=1}^M | \mat{x})
}
\Big[
\nabla_{\boldsymbol\theta} \log \hat{Z}_{\boldsymbol\theta}(\mat{x})
\Big]
& = 
\frac{1} {M + 1}
\sum_{i=0}^M
\mathbb{E}_{
P(\{\mat{y}_i \}_{i=0}^M)
}
\Big[
\frac{\nabla_{\boldsymbol\theta}\exp\big(
\beta
r_{\boldsymbol\theta}(\mat{x}, \mat{y}_i)
\big)
} 
{\hat{Z}_{\boldsymbol\theta}(\mat{x})}
\Big],
\\ \nonumber
& =
\frac{1}{M + 1}
\sum_{i=0}^M
\nabla_{\boldsymbol\theta} \log Z_{\boldsymbol\theta}(\mat{x}),
\\ \nonumber
& =
\nabla_{\boldsymbol\theta} \log Z_{\boldsymbol\theta}(\mat{x}).
\end{align*}
\end{proof}


\newpage
\section{Extensive Experimental Results}
\label{appendix: extensive experimental results}

\subsection{Experimental Setup}
\label{appendix: experimental setup}
We provide details about experimental setting here.

\paragraph{Models and datasets.}
Our main experiments are conducted under three settings.

\begin{itemize}
\item \textbf{Base setup:}
This setting considers the 
\href{https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-SFT}{Llama-3.1-8B-SFT} model, which has been fine-tuned using supervised next-word prediction on the TÜLU 3 SFT Mix dataset \citep{lambert2024t},
and {\href{https://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta}{Mistral-7B-SFT}}.

The \href{https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-SFT}{Llama-3.1-8B} model is constructed by fine-tuning the \href{https://huggingface.co/meta-llama/Llama-3.1-8B}{Llama-3.1-8B-base} on the TÜLU 3 SFT Mix dataset. 
The TÜLU 3 SFT Mix dataset spans various domains including general instruction following, knowledge recall, mathematical reasoning, coding, safety, non-compliance, and multilingual tasks, with domain mixing ratios determined by thorough experimental analyses and contains approximately $23$M prompt-response pairs. 
We employ the publicly available model checkpoint of the \href{https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-SFT}{Llama-3.1-8B} for further fine-tuning on the \href{https://huggingface.co/datasets/berkeley-nest/Nectar}{Nectar} dataset \citep{zhu2023starling}, which includes $7$ ranked completions per input prompt generated by various LLMs, providing a diverse and high-quality set of candidate completions. 
The Nectar dataset is modified by removing the rank-$2$ completion, leaving each prompt with $5$ ranked completions.
For each prompt, the rank-$1$ completion is considered as the preferred completion, and a dispreferred completion is randomly selected from the remaining $5$ candidates.
\item \textbf{Instruct setup:}
This setup considers the off-the-shelf instruction-tuned
\href{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}{Llama-3.1-8B-Instruct} model to initialize the target policy $\pi_{\boldsymbol\theta}$.
This model has undergone extensive instruction-tuning processes, making it more expressive compared to the initialization model in the base setup.

We utilize prompts from the \href{https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized}{UltraFeedback} dataset \citep{cui2023ultrafeedback} to regenerate both chosen and rejected completions employing the \href{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}{Llama-3.1-8B-Instruct} model. 
This approach aligns the instruct setup more closely with an on-policy framework \citep{tang2024understanding}.
Specifically, for each prompt, we generate two completions at a temperature of $0.6$, two at $0.8$, and two at $1.0$, thereby introducing diversity within the candidate completions. 
Subsequently, we implement the iterative pairwise ranking method \citep{chen2024towards} using the \href{https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct}{Llama-3.1-70B-Instruct} \citep{dubey2024llama} to determine the most preferred completion and randomly select a dispreferred completion from the remaining candidates.
The iterative pairwise ranking algorithm \citep{chen2024towards} relies on two assumptions to identify the winner:
\begin{enumerate}[noitemsep]
    \item
    \textbf{Transitive:}
    $y^{(i, a)} \succ y^{(i, b)}$ and $y^{(i, b)} \succ y^{(i, c)}$ leads to $y^{(i, a)} \succ y^{(i, c)}$ almost surely, where $a, b, c \in \{1, 2, \dots, M\}$.
    \item 
    \textbf{Symmetry:}
    The ordering of two completions does not affect the comparison result $W$,
    $W(x^i, y^{(i, a)}, y^{(i, b)}) = W(x^i, y^{(i, b)}, y^{(i, a)})$.
\end{enumerate}
Given these assumptions, identifying the most preferred completion from $L$ candidates can be accomplished from $(L-1)$ comparisons.
Specifically, the algorithm initiates by comparing the first pair of completions, followed by comparing their winner with the next candidate. 
This iterative process continues until an overall winner is determined.
\item \textbf{Batched online setup:}
This setting is the middle of the offline and purely online setups \citep{schulman2017proximal, lambert2024t}, striking a balance between deployment efficiency and adaptability.
The number of total training steps is calculated as the number of total data divided by the effective batch size (the effective batch size is chosen as $128$ across all experiments).
The training steps are then divided equally into three segments, and we use the model checkpoint from the start of each segment to regenerate the preference data. 
For example, with a total of $450$ training steps, we initiate with the \href{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}{Llama-3.1-8B-Instruct} model to generate preference data for the first $150$ steps. 
At the $\rm 150^{th}$ step, we utilize the current model checkpoint to generate data for the next $150$ steps, continuing this sequence. 
The preference data generation adheres to the Instruct setting. 
This method proves more efficient than a purely online approach \citep{schulman2017proximal,qi2024online}, as starting the inference kernel in an online environment often incurs significant computational costs \citep{kwon2023efficient}.
\end{itemize}


\paragraph{MC-PO implementation details.}
Recall the MC kernel defined in Algorithm \ref{alg: contrastive divergence kernel},
this kernel selects a output based on samples from a proposal distribution $\mu$.
In the instruct setup,
the proposal distribution is considered as the reference policy $\pi_{\rm ref}$,
that is the \href{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}{Llama-3.1-8B-Instruct} model.
In the base setup,
we consider the proposal distribution as a list of LLMs that are used to generate all completions in the \href{https://huggingface.co/datasets/berkeley-nest/Nectar}{Nectar} dataset.
These LLMs include GPT-4, GPT-3.5-turbo, GPT-3.5-turbo-instruct, LLama-2-7B-chat, and Mistral-7B-Instruct, alongside other existing datasets and models.



\paragraph{Training.}
All training jobs are done using full-parameter tuning. 
We fix the batch size as $1$ and gradient accumulation steps as $128$,
which results in an effective batch size of $128$.
We train all models with $2$ epochs.
Hyperparameter optimization is conducted using $7$ different learning rates.
All results are reported as the average performance of the final checkpoints across $3$ random seeds, along with the standard deviation,
which can effectively reduce numerical randomness \citep{miller2024adding}.
Each training job is done on a node of $8\cdot$A100 GPUs and multiple nodes are executed in parallel.






\begin{itemize}
\item \textbf{Justification on $2$ training epochs.}
\begin{table}[h!]
\centering
\begin{tabular}{l|ccc}
\hline
Model & \multicolumn{3}{|c}{\href{https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-SFT}{Llama-3.1-8B-Base} (Alpaca-Eval)} \\
& Epoch $1$ & Epoch $2$ & Epoch $3$ \\
MC-PO & 32.93($\pm$0.39) & 35.84($\pm$0.31) & 35.01($\pm$0.71) \\ 
\hline
Model & \multicolumn{3}{|c}{\href{https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-SFT}{Llama-3.1-8B-Base} (Arena)} \\
& Epoch $1$ & Epoch $2$ & Epoch $3$ \\
MC-PO & 61.70($\pm$0.29) & 63.77($\pm$0.81) & 63.83($\pm$0.75) \\ 
\hline
\end{tabular}
\caption{
Performance of preference-optimized models using MC-PO at each training epoch.
}
\label{table: mc-po on all epochs}
\end{table}
As shown in Table \ref{table: mc-po on all epochs},
the MC-PO training from epoch $1$ to epoch $2$ demonstrates substantial performance improvement.
Extending training to $3$ epochs does not yield additional improvements in performance.

\item 
\textbf{Details on hyperparameter optimization.}
We choose $7$ learning rates for all PO algorithms.
These include 
$8e-7$,
$1e-6$,
$2e-6$,
$5e-6$,
$8e-7$,
$1e-5$,
$2e-5$.
Since each experiment is repeated with three random seeds,
each reported number in the experiment section requires training $3 \times 7 = 21$ models.

\end{itemize}


\paragraph{Evaluation.}
We compute winrate with \href{https://huggingface.co/mistralai/Mistral-Large-Instruct-2407}{Mistral-Large-Instruct-2407  } as the model judge for all evaluations.
The input prompt for the LLM judge is shown as follows,


\begin{codebox}
You are a helpful assistant, that ranks models by the quality of their answers. \
Act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. \
The length of the response generated by each assistant is not a criterion for evaluation. \
Your evaluation should consider correctness, helpfulness, completeness, and clarity of the responses. \
Remember not to allow the length of the responses to influence your evaluation. \
You will be given the question within <question> tags, \
assistant A's answer within <assistant a> tags. \
and assistant B's answer within <assistant b> tags. \
Your job is to evaluate whether assistant A's answer or assistant B's answer is better. \
Avoid any position biases and ensure that the order in which the responses are presented does not \
influence your decision. Be as objective as possible. \
After providing your explanation, output your final verdict within <verdict> tags strictly following this format: \
<verdict>A</verdict> if assistant A is better, <verdict>B</verdict> if assistant B is better, and <verdict>tie</verdict> for a tie.
You must provide your final verdict with the format <verdict>xxx</verdict> once in your response!!!

<question>
{question}
</question>

<assistant a>
{response a}
</assistant a>

<assistant b>
{response b}
</assistant b>
\end{codebox}





\subsection{Details on Baseline Preference Optimization Algorithms}
\label{appendix: baseline preference optimization algorithms}
All baseline algorithms are implemented in the {\href{https://huggingface.co/docs/trl/en/index}{TRL library},
and their objective functions are summarized in Table \ref{table: preference optimization baseline algorithms}.
Here we present the details about their hyper-parameter choices.
The hyper-parameter $\beta$ is chosen as $0.01$ in all PO algorithms (if it appears).
The hyper-parameter $\lambda$ for the supervised next-word prediction is set as $0.1$.
$\gamma$ in SimPO and CPO is fixed as $10$.

\begin{table*}[h!]
\centering
\begin{tabular}{l|cc}
\hline
\multicolumn{1}{c|}{Method}  & Objective Function \\ \hline
DPO & 
$
- \log \sigma
\Big(
\beta
\log
\frac{\pi_{\boldsymbol\theta}(\mat{y}_0 | \mat{x})} 
{\pi_{\rm ref}(\mat{y}_0 | \mat{x})}
-
\beta
\log
\frac{\pi_{\boldsymbol\theta}(\mat{y}_1 | \mat{x})} 
{\pi_{\rm ref}(\mat{y}_1 | \mat{x})}
\Big)
$
\\
RPO & 
$
- \log \sigma
\Big(
\beta
\log
\frac{\pi_{\boldsymbol\theta}(\mat{y}_0 | \mat{x})} 
{\pi_{\rm ref}(\mat{y}_0 | \mat{x})}
-
\beta
\log
\frac{\pi_{\boldsymbol\theta}(\mat{y}_1 | \mat{x})} 
{\pi_{\rm ref}(\mat{y}_1 | \mat{x})}
\Big)
-
\lambda
\log
\frac{\pi_{\boldsymbol\theta}(\mat{y}_0 | \mat{x})} 
{\pi_{\rm ref}(\mat{y}_0 | \mat{x})}
$
\\
EXO &
$
-
\sigma \Big(
\beta 
\cdot
{\rm logits}
\Big)
\log
\sigma \Big(
\beta 
\cdot
{\rm logits}
\Big)
+
\sigma \Big(
\beta 
\cdot
{\rm logits}
\Big)
\log
\sigma \Big(
-
\beta 
\cdot
{\rm logits}
\Big)
$
\\
SimPO &
$
- \log \sigma \Big(\frac{\beta} {|\mat{y}_0|} \log \pi_{\boldsymbol\theta} (\mat{y}_0 | \mat{x}) -  \frac{\beta} {|\mat{y}_1|} \log \pi_{\boldsymbol\theta} (\mat{y}_1 | \mat{x}) - \gamma \Big)
$ 
\\
CPO &
$
- \log \sigma \Big(
\frac{\beta} {|\mat{y}_0|} \log \pi_{\boldsymbol\theta} (\mat{y}_0 | \mat{x}) -  \frac{\beta} {|\mat{y}_1|} \log \pi_{\boldsymbol\theta} (\mat{y}_1 | \mat{x}) - \gamma \Big)
-
\lambda \cdot
\frac{\beta} {|\mat{y}_0|} \log \pi_{\boldsymbol\theta} (\mat{y}_0 | \mat{x})
$
\\
\hline
BCO & 
$
- \log \sigma
\Big(
\beta
\log
\frac{\pi_{\boldsymbol\theta}(\mat{y}_0 | \mat{x})} 
{\pi_{\rm ref}(\mat{y}_0 | \mat{x})} 
- \Delta_{\rm BCO}
\Big)
-
\log \sigma
\Big(
-
\beta
\log
\frac{\pi_{\boldsymbol\theta}(\mat{y}_1 | \mat{x})} 
{\pi_{\rm ref}(\mat{y}_1 | \mat{x})}
-
\Delta_{\rm BCO}
\Big)
$
\\
KTO & 
$
- \log \sigma
\Big(
\beta
\log
\frac{\pi_{\boldsymbol\theta}(\mat{y}_0 | \mat{x})} 
{\pi_{\rm ref}(\mat{y}_0 | \mat{x})} 
- \Delta_{\rm KTO}
\Big)
-
\log \sigma
\Big(
-
\beta
\log
\frac{\pi_{\boldsymbol\theta}(\mat{y}_1 | \mat{x})} 
{\pi_{\rm ref}(\mat{y}_1 | \mat{x})}
-
\Delta_{\rm KTO}
\Big)
$
\\
APO & 
$
- \log \sigma
\Big(
\beta
\log
\frac{\pi_{\boldsymbol\theta}(\mat{y}_0 | \mat{x})} 
{\pi_{\rm ref}(\mat{y}_0 | \mat{x})}
\Big)
+
\log \sigma
\Big(
\beta
\log
\frac{\pi_{\boldsymbol\theta}(\mat{y}_1 | \mat{x})} 
{\pi_{\rm ref}(\mat{y}_1 | \mat{x})}
\Big)
$
\\
SPPO & 
$
\Big(
\log
\frac{\pi_{\boldsymbol\theta}(\mat{y}_0 | \mat{x})} 
{\pi_{\rm ref}(\mat{y}_0 | \mat{x})}
-
\frac{0.5} {\beta}
\Big)^2
+
\Big(
\log
\frac{\pi_{\boldsymbol\theta}(\mat{y}_1 | \mat{x})} 
{\pi_{\rm ref}(\mat{y}_1 | \mat{x})}
+
\frac{0.5} {\beta}
\Big)^2
$
\\
NCA &
$
- \log \sigma
\Big(
\beta
\log
\frac{\pi_{\boldsymbol\theta}(\mat{y}_0 | \mat{x})} 
{\pi_{\rm ref}(\mat{y}_0 | \mat{x})}
\Big)
-
0.5 
\log \sigma
\Big(
-
\beta
\log
\frac{\pi_{\boldsymbol\theta}(\mat{y}_0 | \mat{x})} 
{\pi_{\rm ref}(\mat{y}_0 | \mat{x})}
\Big)
-
0.5 
\log \sigma
\Big(
-
\beta
\log
\frac{\pi_{\boldsymbol\theta}(\mat{y}_1 | \mat{x})} 
{\pi_{\rm ref}(\mat{y}_1 | \mat{x})}
\Big)
$
\\
\hline
MC-PO &
$
-
\beta
\log
\frac{\pi_{\boldsymbol\theta}(\mat{y}_0 | \mat{x})} 
{\pi_{\rm ref}(\mat{y}_0 | \mat{x})}
+
\log
{\sum_{i=0}^M \exp
\Big(
\beta
\log
\frac{\pi_{\boldsymbol\theta}(\mat{y}_i | \mat{x})} 
{\pi_{\rm ref}(\mat{y}_i | \mat{x})}
\Big)}.
$
\\ \hline
\end{tabular}
\caption{
Preference optimization algorithms and their objective function implementations.
In RPO and CPO: $\lambda$ is a hyper-parameter controlling the supervised next-word prediction regularization.
In EXO: 
$\rm{logits} := \log
\frac{\pi_{\boldsymbol\theta}(\mat{y}_0 | \mat{x})} 
{\pi_{\rm ref}(\mat{y}_0 | \mat{x})}$.
In SimPO: $\gamma$ is a hyper-parameter.
In BCO and KTO: $\Delta_{\rm BCO}$ and $\Delta_{\rm KTO}$ are empirically computed.
}
\label{table: preference optimization baseline algorithms}
\end{table*}




























