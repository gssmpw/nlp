\section{Related Work}
\label{sec::related}
\subsection{Layout Generation}
Automatic layout generation has emerged as a burgeoning research topic for its extensive application in diverse scenarios, such as print publications \cite{publaynet2019zhong,magazine2019zheng,read2020patil,vtn2021arroyo}, poster/advertisement design \cite{poster2021guo,poster2021qian}, and graphic user interface design \cite{rico2017deka,ruite2021rahman,layoutganpp2021kikuchi,ganbased2021lis}. Based on generation requirements, layout generation can be broadly classified into conditional and unconditional generation. Conditional generation more specifically encompasses tasks including layout completion \cite{layouttf2021gupta,layoutnuwa2024tang}, relationship control \cite{lee2020neural,layoutganpp2021kikuchi,layoutdm2023Inoue}, and noise refinement \cite{lee2020neural,ruite2021rahman,layoutprompter2023lin}, \emph{etc.}, all of which require generating layouts predicated on specific conditions. In contrast, unconditional generation refers to crafting a new layout from scratch without any prior information. Earlier layout generation methods involve classic optimization on energy-based models \cite{convetion2014donovan,convention2015donovan}, as well as building models based on Generative Adversarial Network (GAN) \cite{layoutgan2018li,magazine2019zheng,nauata2020house,ganbased2021lis,layoutganpp2021kikuchi} and Variational AutoEncoder (VAE) \cite{layoutvae2019jyothi,lee2020neural,read2020patil}. Currently, Transformer-based approaches dominate the state-of-the-art of this field, which utilizes the self-attention mechanism \cite{attention2017vaswani} to learn the contextual relationship between layout objects and enhance generation quality. Contingent on the modeling paradigm, they can be generally grouped into masked modeling-based and generative modeling-based methods.

\textbf{Masked modeling}. In this paradigm, layout sequences undergo a masking process to create partial inputs, requiring the model to predict masked attributes and construct complete layouts. This methodology parallels the principle of masked language modeling epitomized by BERT \cite{bert2019devlin}, thus they typically employ Transformer encoder as the core model component. For example, LayoutGAN++ \cite{layoutganpp2021kikuchi} embeds Transformer into a GAN framework and performs latent optimization on the relational control task. BLT \cite{blt2022kong} discovers the immutable dependency chain problem that prevents autoregressive decoders from conditional generation and leverages a bi-directional Transformer to surmount this issue. More recently, diffusion model \cite{diffusion2020jonathan} has seen an exponential surge in the research community. A deluge of approaches have emerged to exploit this technique in concert with Transformer \cite{layoutdm2023Inoue,LDGM2023hui,layoutdiff2023zhang,layoutdm2023chai,dlt2023levi}. They corrupt the layout attributes through a Markov process to simulate different generation task conditions and perform reverse denoising from timestep $T$ until 0 to derive complete layouts. For instance, LDGM \cite{LDGM2023hui} decouples diffusion forward processes for each attribute parallelly and conducts a joint denoising process with global context to improve generation. LayoutDiffusion \cite{layoutdiff2023zhang} proposes a block-wise transition matrix based on the heterogeneous nature of layout to realize a mild forward process, thus easing the attribute estimation in the reversed process.

\textbf{Generative modeling}. Methods under the generative paradigm involve holistically generating layouts in a predict-next manner, usually exploiting Transformer of either encoder-decoder or decoder-only architectures. Corase-to-Fine \cite{ctf2022jiang} generates layout latent code with an encoder and performs corase-to-fine decoding by a two-stage decoder. LayoutFormer++ \cite{layouttfpp2023jiang} utilizes a bi-directional encoder with a decoder to perform various generation tasks. Parse-Then-Place \cite{lin2023iccv} proposes a two-stage approach to decompose text-to-layout tasks based on a T5 \cite{t52020jmlr} model. In contrast to the above encoder-decoder models, decoder-only models have received less attention until the transformative breakthrough of LLM, with only LayoutTransformer \cite{layouttf2021gupta} and VTN \cite{vtn2021arroyo} having employed this architecture. Propelled by the monumental success of LLM, LayoutPrompter \cite{layoutprompter2023lin} proposes to exploit the hidden layout cognitive ability inside the frozen GPT3 for generation tasks. LayoutNUWA \cite{layoutnuwa2024tang} utilizes LLaMA2 \cite{llama22023touvron} and CodeLLaMA \cite{codellmam2023roziere}, performing layout generation based on code instruction tuning.

\textbf{Task \& Domain Unification.} From the perspective of unification, research efforts have undergone a perceptible transition from building task-specific model \cite{lee2020neural,layouttf2021gupta,vtn2021arroyo,layoutganpp2021kikuchi,ctf2022jiang} to task-generic model \cite{blt2022kong,layoutdm2023Inoue,LDGM2023hui,layouttfpp2023jiang,layoutprompter2023lin,layoutnuwa2024tang}. BLT \cite{blt2022kong} and LayoutNUWA \cite{layoutnuwa2024tang} covers a limited range of tasks, with three and four tasks respectively. LayoutPrompter \cite{layoutprompter2023lin} extends the range to solve five tasks in a training-free manner. LayoutFormer++ \cite{layouttfpp2023jiang} and LayoutDM (Inoue et al.) \cite{layoutdm2023Inoue} address six common layout generation tasks, in which LayoutFormer++ trains the same model for each task separately, whereas LayoutDM trains the model with all tasks simultaneously. LDGM \cite{LDGM2023hui} transcends the limitation of handling six fixed tasks to handling hybrid tasks, which is the combination of various separate tasks. It unifies both separate and hybrid tasks with a joint training procedure and achieves a much broader setting. The scope of research has also expanded in domain coverage. LayoutNUWA \cite{layoutnuwa2024tang} performs joint training with layout data from all three domains (scientific article, App UI, and magazine) under a domain-generic framework, whereas prior methods simply perform generation on a single-type of layout data. However, no research has ever stepped into the broadest yet challenging repertoire of unifying multiple tasks and data domains with a joint training process. For either research or industry applications, building a task-generic as well as domain-generic generation engine indeed holds significant value. Therefore, we propose LGGPT to unify various tasks and domains of layout data, and additionally incorporate a text-to-layout task \cite{lin2023iccv} under the domain-generic setting, further extending the comprehensiveness of unification.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{arch.png}{}
	\caption{Overall architecture of LGGPT, which mainly consists of Arbitrary Layout Instruction (ALI), Universal Layout Response (ULR), the Interval Quantization Encoding (IQE) strategy, and a unified LLM. ALI is utilized for instruction tuning on the LLM, which consolidates a designated prompt for layout type and random layout conditions through \emph{Arbitrary Layout Condition Sequence}. IQE is proposed to compress ALI for a more condensed structure. ULR requires the LLM always to generate a complete, precise layout given arbitrary layout inputs.}
	\label{Fig::arch}
\end{figure*}

\subsection{Large Language Model}
\label{sec::rw_llm}
\textbf{LLM for layout reasoning.} Large Language Models (LLMs) \cite{zhao2023survey}, such as GPT4 \cite{gpt42023}, LLaMA3 \cite{llama32024dubey}, and Qwen2 \cite{qwen22024yang}, have witnessed tremendous progress in the NLP field. The success of LLMs on reasoning tasks, for example, commonsense reasoning \cite{causalreason2011aaai,levesque2012winograd} and logistic reasoning \cite{nijkamp2023codegen,phi12023gunasekar,phi1.52023li}, underscores their potential for structured reasoning more broadly. Since layout generation demands a blend of logical consistency and aesthetic sensibility, it immensely benefits from the sophisticated context understanding and causal reasoning skills of LLMs. This positions them as a compelling foundation for solving complicated layout generation tasks. Several researches have grounded the feasibility of applying LLMs in layout generation through their reasoning abilities. In natural scenarios, VISORGPT \cite{visorgpt2023xie} leverages GPT2 \cite{gpt22019radford} to learn visual layout priors of location, shape, and implicit relations between visual elements. LayoutGPT \cite{layoutgpt2023feng} injects visual commonsense into off-the-shelf ChatGPT \cite{chatgpt2022ouyang} to generate 2D and 3D layouts, then perform text-conditioned image or indoor scene generation. In document scenarios, LayoutPrompter \cite{layoutprompter2023lin} awakens the layout design ability by performing in-context learning on GPT3 with layout data. LayoutNUWA \cite{layoutnuwa2024tang} converts layout generation to code implementation to enhance semantic richness, leveraging LLaMA2 and CodeLLaMA for code instruction tuning. These endeavors inspire us to harness the exceptional reasoning skills of LLMs to tackle the challenging and yet-to-explore unified layout generation, which spans arbitrary tasks and various domains. Nevertheless, their sheer scales require substantial training resources and impede the ease of deployment. In this paper, we turn to smaller LLMs to reduce computational cost. We propose a suite of techniques to ensure respectable performance while optimizing computational efficiency, striving for a satisfactory trade-off between performance and efficiency.

\textbf{Instruction Tuning.} Instruction tuning refers to fine-tuning LLMs on a dataset of instructions and corresponding desired responses. It holds increasing significance in aligning LLMs with human preferences and has now become a key ingredient of LLMs' training recipe \cite{zhang2023instruction}. Conventionally, most LLMs perform instruction tuning on homogeneous data, \emph{i.e.}, fine-tuning the model with data reflective of their pre-training exposure, such as natural language text \cite{peng2023instruction,1_8ktasks2022chung} and code \cite{codellmam2023roziere,phi12023gunasekar}. However, a growing body of research is exploring the potential of heterogeneous data, \emph{i.e.}, data that differs significantly from pre-training content in structure, such as time-series data \cite{llmtimeseries2023hao,yu2023temporal}, to instruct LLMs for domain-specific comprehension and broader task generalization. This further extends the border of instruction tuning and enables more flexible specialties of LLMs. Converse to prior layout LLMs \cite{layoutprompter2023lin,layoutnuwa2024tang} that adapt layout sequences into the HTML code format replete with superfluous code symbols, we formulate layouts into a succinct composition of solely textual classes and numerical geometries. We then encompass them within our ALI and ULR templates for more compact and effective layout representations. Although this is heterogeneous data to mainstream LLMs, their innate reasoning capability learned from pretraining still benefits the understanding of layout context. Hence, building upon the success of trailblazing works \cite{llmtimeseries2023hao,yu2023temporal}, we employ heterogeneous instruction tuning to empower LLMs to understand specialized layout conditions and perform diverse layout generation tasks seamlessly.