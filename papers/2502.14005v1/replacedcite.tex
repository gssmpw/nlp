\section{Related Work}
\label{sec::related}
\subsection{Layout Generation}
Automatic layout generation has emerged as a burgeoning research topic for its extensive application in diverse scenarios, such as print publications ____, poster/advertisement design ____, and graphic user interface design ____. Based on generation requirements, layout generation can be broadly classified into conditional and unconditional generation. Conditional generation more specifically encompasses tasks including layout completion ____, relationship control ____, and noise refinement ____, \emph{etc.}, all of which require generating layouts predicated on specific conditions. In contrast, unconditional generation refers to crafting a new layout from scratch without any prior information. Earlier layout generation methods involve classic optimization on energy-based models ____, as well as building models based on Generative Adversarial Network (GAN) ____ and Variational AutoEncoder (VAE) ____. Currently, Transformer-based approaches dominate the state-of-the-art of this field, which utilizes the self-attention mechanism ____ to learn the contextual relationship between layout objects and enhance generation quality. Contingent on the modeling paradigm, they can be generally grouped into masked modeling-based and generative modeling-based methods.

\textbf{Masked modeling}. In this paradigm, layout sequences undergo a masking process to create partial inputs, requiring the model to predict masked attributes and construct complete layouts. This methodology parallels the principle of masked language modeling epitomized by BERT ____, thus they typically employ Transformer encoder as the core model component. For example, LayoutGAN++ ____ embeds Transformer into a GAN framework and performs latent optimization on the relational control task. BLT ____ discovers the immutable dependency chain problem that prevents autoregressive decoders from conditional generation and leverages a bi-directional Transformer to surmount this issue. More recently, diffusion model ____ has seen an exponential surge in the research community. A deluge of approaches have emerged to exploit this technique in concert with Transformer ____. They corrupt the layout attributes through a Markov process to simulate different generation task conditions and perform reverse denoising from timestep $T$ until 0 to derive complete layouts. For instance, LDGM ____ decouples diffusion forward processes for each attribute parallelly and conducts a joint denoising process with global context to improve generation. LayoutDiffusion ____ proposes a block-wise transition matrix based on the heterogeneous nature of layout to realize a mild forward process, thus easing the attribute estimation in the reversed process.

\textbf{Generative modeling}. Methods under the generative paradigm involve holistically generating layouts in a predict-next manner, usually exploiting Transformer of either encoder-decoder or decoder-only architectures. Corase-to-Fine ____ generates layout latent code with an encoder and performs corase-to-fine decoding by a two-stage decoder. LayoutFormer++ ____ utilizes a bi-directional encoder with a decoder to perform various generation tasks. Parse-Then-Place ____ proposes a two-stage approach to decompose text-to-layout tasks based on a T5 ____ model. In contrast to the above encoder-decoder models, decoder-only models have received less attention until the transformative breakthrough of LLM, with only LayoutTransformer ____ and VTN ____ having employed this architecture. Propelled by the monumental success of LLM, LayoutPrompter ____ proposes to exploit the hidden layout cognitive ability inside the frozen GPT3 for generation tasks. LayoutNUWA ____ utilizes LLaMA2 ____ and CodeLLaMA ____, performing layout generation based on code instruction tuning.

\textbf{Task \& Domain Unification.} From the perspective of unification, research efforts have undergone a perceptible transition from building task-specific model ____ to task-generic model ____. BLT ____ and LayoutNUWA ____ covers a limited range of tasks, with three and four tasks respectively. LayoutPrompter ____ extends the range to solve five tasks in a training-free manner. LayoutFormer++ ____ and LayoutDM (Inoue et al.) ____ address six common layout generation tasks, in which LayoutFormer++ trains the same model for each task separately, whereas LayoutDM trains the model with all tasks simultaneously. LDGM ____ transcends the limitation of handling six fixed tasks to handling hybrid tasks, which is the combination of various separate tasks. It unifies both separate and hybrid tasks with a joint training procedure and achieves a much broader setting. The scope of research has also expanded in domain coverage. LayoutNUWA ____ performs joint training with layout data from all three domains (scientific article, App UI, and magazine) under a domain-generic framework, whereas prior methods simply perform generation on a single-type of layout data. However, no research has ever stepped into the broadest yet challenging repertoire of unifying multiple tasks and data domains with a joint training process. For either research or industry applications, building a task-generic as well as domain-generic generation engine indeed holds significant value. Therefore, we propose LGGPT to unify various tasks and domains of layout data, and additionally incorporate a text-to-layout task ____ under the domain-generic setting, further extending the comprehensiveness of unification.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{arch.png}{}
	\caption{Overall architecture of LGGPT, which mainly consists of Arbitrary Layout Instruction (ALI), Universal Layout Response (ULR), the Interval Quantization Encoding (IQE) strategy, and a unified LLM. ALI is utilized for instruction tuning on the LLM, which consolidates a designated prompt for layout type and random layout conditions through \emph{Arbitrary Layout Condition Sequence}. IQE is proposed to compress ALI for a more condensed structure. ULR requires the LLM always to generate a complete, precise layout given arbitrary layout inputs.}
	\label{Fig::arch}
\end{figure*}

\subsection{Large Language Model}
\label{sec::rw_llm}
\textbf{LLM for layout reasoning.} Large Language Models (LLMs) ____, such as GPT4 ____, LLaMA3 ____, and Qwen2 ____, have witnessed tremendous progress in the NLP field. The success of LLMs on reasoning tasks, for example, commonsense reasoning ____ and logistic reasoning ____, underscores their potential for structured reasoning more broadly. Since layout generation demands a blend of logical consistency and aesthetic sensibility, it immensely benefits from the sophisticated context understanding and causal reasoning skills of LLMs. This positions them as a compelling foundation for solving complicated layout generation tasks. Several researches have grounded the feasibility of applying LLMs in layout generation through their reasoning abilities. In natural scenarios, VISORGPT ____ leverages GPT2 ____ to learn visual layout priors of location, shape, and implicit relations between visual elements. LayoutGPT ____ injects visual commonsense into off-the-shelf ChatGPT ____ to generate 2D and 3D layouts, then perform text-conditioned image or indoor scene generation. In document scenarios, LayoutPrompter ____ awakens the layout design ability by performing in-context learning on GPT3 with layout data. LayoutNUWA ____ converts layout generation to code implementation to enhance semantic richness, leveraging LLaMA2 and CodeLLaMA for code instruction tuning. These endeavors inspire us to harness the exceptional reasoning skills of LLMs to tackle the challenging and yet-to-explore unified layout generation, which spans arbitrary tasks and various domains. Nevertheless, their sheer scales require substantial training resources and impede the ease of deployment. In this paper, we turn to smaller LLMs to reduce computational cost. We propose a suite of techniques to ensure respectable performance while optimizing computational efficiency, striving for a satisfactory trade-off between performance and efficiency.

\textbf{Instruction Tuning.} Instruction tuning refers to fine-tuning LLMs on a dataset of instructions and corresponding desired responses. It holds increasing significance in aligning LLMs with human preferences and has now become a key ingredient of LLMs' training recipe ____. Conventionally, most LLMs perform instruction tuning on homogeneous data, \emph{i.e.}, fine-tuning the model with data reflective of their pre-training exposure, such as natural language text ____ and code ____. However, a growing body of research is exploring the potential of heterogeneous data, \emph{i.e.}, data that differs significantly from pre-training content in structure, such as time-series data ____, to instruct LLMs for domain-specific comprehension and broader task generalization. This further extends the border of instruction tuning and enables more flexible specialties of LLMs. Converse to prior layout LLMs ____ that adapt layout sequences into the HTML code format replete with superfluous code symbols, we formulate layouts into a succinct composition of solely textual classes and numerical geometries. We then encompass them within our ALI and ULR templates for more compact and effective layout representations. Although this is heterogeneous data to mainstream LLMs, their innate reasoning capability learned from pretraining still benefits the understanding of layout context. Hence, building upon the success of trailblazing works ____, we employ heterogeneous instruction tuning to empower LLMs to understand specialized layout conditions and perform diverse layout generation tasks seamlessly.