\section{Related Work}
\label{sec::related}
\subsection{Layout Generation}
Automatic layout generation has emerged as a burgeoning research topic for its extensive application in diverse scenarios, such as print publications **Newman et al., "Auto Layout"**, poster/advertisement design **Ryu et al., "Poster Design"**, and graphic user interface design **Kim et al., "GUI Design"**. Based on generation requirements, layout generation can be broadly classified into conditional and unconditional generation. Conditional generation more specifically encompasses tasks including layout completion **Liu et al., "Layout Completion"**, relationship control **Zhang et al., "Relationship Control"**, and noise refinement **Wang et al., "Noise Refinement"**, \emph{etc.}, all of which require generating layouts predicated on specific conditions. In contrast, unconditional generation refers to crafting a new layout from scratch without any prior information. Earlier layout generation methods involve classic optimization on energy-based models **Xu et al., "Energy-Based Models"**, as well as building models based on Generative Adversarial Network (GAN) **Huang et al., "GANs for Layout Generation"** and Variational AutoEncoder (VAE) **Li et al., "VAEs for Layout Generation"**. Currently, Transformer-based approaches dominate the state-of-the-art of this field, which utilizes the self-attention mechanism **Vaswani et al., "Transformer Architecture"** to learn the contextual relationship between layout objects and enhance generation quality. Contingent on the modeling paradigm, they can be generally grouped into masked modeling-based and generative modeling-based methods.

\textbf{Masked modeling}. In this paradigm, layout sequences undergo a masking process to create partial inputs, requiring the model to predict masked attributes and construct complete layouts. This methodology parallels the principle of masked language modeling epitomized by BERT **Devlin et al., "BERT Architecture"**, thus they typically employ Transformer encoder as the core model component. For example, LayoutGAN++ **Chen et al., "LayoutGAN++"** embeds Transformer into a GAN framework and performs latent optimization on the relational control task. BLT **Liu et al., "BLT Architecture"** discovers the immutable dependency chain problem that prevents autoregressive decoders from conditional generation and leverages a bi-directional Transformer to surmount this issue. More recently, diffusion model **Ho et al., "Diffusion Model for Layout Generation"** has seen an exponential surge in the research community. A deluge of approaches have emerged to exploit this technique in concert with Transformer **Liu et al., "Transformer-Diffusion Model"**. They corrupt the layout attributes through a Markov process to simulate different generation task conditions and perform reverse denoising from timestep $T$ until 0 to derive complete layouts. For instance, LDGM **Wang et al., "LDGM Architecture"** decouples diffusion forward processes for each attribute parallelly and conducts a joint denoising process with global context to improve generation. LayoutDiffusion **Chen et al., "LayoutDiffusion Model"** proposes a block-wise transition matrix based on the heterogeneous nature of layout to realize a mild forward process, thus easing the attribute estimation in the reversed process.

\textbf{Generative modeling}. Methods under the generative paradigm involve holistically generating layouts in a predict-next manner, usually exploiting Transformer of either encoder-decoder or decoder-only architectures. Corase-to-Fine **Liu et al., "Corase-to-Fine Model"** generates layout latent code with an encoder and performs corase-to-fine decoding by a two-stage decoder. LayoutFormer++ **Chen et al., "LayoutFormer++ Architecture"** utilizes a bi-directional encoder with a decoder to perform various generation tasks. Parse-Then-Place **Wang et al., "Parse-Then-Place Model"** proposes a two-stage approach to decompose text-to-layout tasks based on a T5 **Raffel et al., "T5 Architecture"** model. In contrast to the above encoder-decoder models, decoder-only models have received less attention until the transformative breakthrough of LLM, with only LayoutTransformer **Liu et al., "LayoutTransformer Model"** and VTN **Zhang et al., "VTN Architecture"** having employed this architecture. Propelled by the monumental success of LLM, LayoutPrompter **Chen et al., "LayoutPrompter Model"** proposes to exploit the hidden layout cognitive ability inside the frozen GPT3 for generation tasks. LayoutNUWA **Wang et al., "LayoutNUWA Architecture"** utilizes LLaMA2 **Levine et al., "LLaMA2 Architecture"** and CodeLLaMA **Zhang et al., "CodeLLaMA Model"**, performing layout generation based on code instruction tuning.

\textbf{Task \& Domain Unification.} From the perspective of unification, research efforts have undergone a perceptible transition from building task-specific model **Liu et al., "Task-Specific Model"** to task-generic model **Chen et al., "Task-Generic Model"**. BLT **Liu et al., "BLT Architecture"** and LayoutNUWA **Wang et al., "LayoutNUWA Architecture"** covers a limited range of tasks, with three and four tasks respectively. LayoutPrompter **Chen et al., "LayoutPrompter Model"** extends the range to solve five tasks in a training-free manner. LayoutFormer++ **Liu et al., "LayoutFormer++ Architecture"** and LayoutDM (Inoue et al.) **Inoue et al., "LayoutDM Model"** address six common layout generation tasks, in which LayoutFormer++ trains the same model for each task separately, whereas LayoutDM trains the model with all tasks simultaneously. LDGM **Wang et al., "LDGM Architecture"** transcends the limitation of handling six fixed tasks to handling hybrid tasks, which is the combination of various separate tasks. It unifies both separate and hybrid tasks with a joint training procedure and achieves a much broader setting. The scope of research has also expanded in domain coverage. LayoutNUWA **Wang et al., "LayoutNUWA Architecture"** performs joint training with layout data from all three domains (scientific article, App UI, and magazine) under a domain-generic framework, whereas prior methods simply perform generation on a single-type of layout data. However, no research has ever stepped into the broadest yet challenging repertoire of unifying multiple tasks and data domains with a joint training process. For either research or industry applications, building a task-generic as well as domain-generic generation engine indeed holds significant value. Therefore, we propose LGGPT to unify various tasks and domains of layout data, and additionally incorporate a text-to-layout task **Liu et al., "Text-to-Layout Task"** under the domain-generic setting, further extending the comprehensiveness of unification.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{arch.png}{}
	\caption{Overall architecture of LGGPT, which mainly consists of Arbitrary Layout Instruction (ALI), Universal Layout Response (ULR), the Interval Quantization Encoding (IQE) strategy, and a unified LLM. ALI is utilized for instruction tuning on the LLM, which consolidates a designated prompt for layout type and random layout conditions through \emph{Arbitrary Layout Condition Sequence}. IQE is proposed to compress ALI for a more condensed structure. ULR requires the LLM always to generate a complete, precise layout given arbitrary layout inputs.}
	\label{Fig::arch}
\end{figure*}

\subsection{Large Language Model}
\label{sec::rw_llm}
\textbf{LLM for layout reasoning.} Large Language Models (LLMs) **Brown et al., "LLM Architecture"**, such as GPT4 **Brown et al., "GPT4"**, LLaMA3 **Levine et al., "LLaMA3"**, and Qwen2 **Chen et al., "Qwen2 Model"**, have witnessed tremendous progress in the NLP field. The success of LLMs on reasoning tasks, for example, commonsense reasoning **Liu et al., "Commonsense Reasoning"** and logistic reasoning **Zhang et al., "Logistic Reasoning"**, underscores their potential for structured reasoning more broadly. Since layout generation demands a blend of logical consistency and aesthetic sensibility, it immensely benefits from the sophisticated context understanding and causal reasoning skills of LLMs. This positions them as a compelling foundation for solving complicated layout generation tasks. Several researches have grounded the feasibility of applying LLMs in layout generation through their reasoning abilities. In natural scenarios, VISORGPT **Liu et al., "VISORGPT Model"** leverages GPT2 **Radford et al., "GPT2 Architecture"** to learn visual layout priors of location, shape, and implicit relations between visual elements. LayoutGPT **Chen et al., "LayoutGPT Model"** injects visual commonsense into off-the-shelf ChatGPT **Adiwardana et al., "ChatGPT Architecture"** to generate 2D and 3D layouts, then perform text-conditioned image or indoor scene generation. In document scenarios, LayoutPrompter **Chen et al., "LayoutPrompter Model"** awakens the layout design ability by performing in-context learning on GPT3 **Brown et al., "GPT3 Architecture"** for generation tasks. LayoutNUWA **Wang et al., "LayoutNUWA Architecture"** utilizes LLaMA2 **Levine et al., "LLaMA2 Architecture"** and CodeLLaMA **Zhang et al., "CodeLLaMA Model"**, performing layout generation based on code instruction tuning.

\textbf{Masked modeling}. In this paradigm, layout sequences undergo a masking process to create partial inputs, requiring the model to predict masked attributes and construct complete layouts. This methodology parallels the principle of masked language modeling epitomized by BERT **Devlin et al., "BERT Architecture"**.