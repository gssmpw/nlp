[
  {
    "index": 0,
    "papers": [
      {
        "key": "liu2022dynamic",
        "author": "Liu, Liu and Qu, Zheng and Chen, Zhaodong and Tu, Fengbin and Ding, Yufei and Xie, Yuan",
        "title": "Dynamic sparse attention for scalable transformer acceleration"
      },
      {
        "key": "ribar2023sparq",
        "author": "Ribar, Luka and Chelombiev, Ivan and Hudlass-Galley, Luke and Blake, Charlie and Luschi, Carlo and Orr, Douglas",
        "title": "Sparq attention: Bandwidth-efficient llm inference"
      },
      {
        "key": "liu2023deja",
        "author": "Liu, Zichang and Wang, Jue and Dao, Tri and Zhou, Tianyi and Yuan, Binhang and Song, Zhao and Shrivastava, Anshumali and Zhang, Ce and Tian, Yuandong and Re, Christopher and others",
        "title": "Deja vu: Contextual sparsity for efficient llms at inference time"
      },
      {
        "key": "xiao2023efficient",
        "author": "Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike",
        "title": "Efficient streaming language models with attention sinks"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ma2024compressing",
        "author": "Ma, Da and Chen, Lu and Zhang, Situo and Miao, Yuxun and Zhu, Su and Chen, Zhi and Xu, Hongshen and Li, Hanqi and Fan, Shuai and Pan, Lei and others",
        "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "xiao2019sharing",
        "author": "Xiao, Tong and Li, Yinqiao and Zhu, Jingbo and Yu, Zhengtao and Liu, Tongran",
        "title": "Sharing attention weights for fast transformer"
      },
      {
        "key": "bhojanapalli2021leveraging",
        "author": "Bhojanapalli, Srinadh and Chakrabarti, Ayan and Veit, Andreas and Lukasik, Michal and Jain, Himanshu and Liu, Frederick and Chang, Yin-Wen and Kumar, Sanjiv",
        "title": "Leveraging redundancy in attention with reuse transformers"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "mu2024cross",
        "author": "Mu, Yongyu and Wu, Yuzhang and Fan, Yuchun and Wang, Chenglong and Li, Hengyu and He, Qiaozhi and Yang, Murun and Xiao, Tong and Zhu, Jingbo",
        "title": "Cross-layer attention sharing for large language models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "men2024shortgpt",
        "author": "Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng",
        "title": "Shortgpt: Layers in large language models are more redundant than you expect"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "fan2024not",
        "author": "Fan, Siqi and Jiang, Xin and Li, Xiang and Meng, Xuying and Han, Peng and Shang, Shuo and Sun, Aixin and Wang, Yequan and Wang, Zhongyuan",
        "title": "Not all layers of llms are necessary during inference"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zhang2023h2o",
        "author": "Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\\'e}, Christopher and Barrett, Clark and others",
        "title": "H2o: Heavy-hitter oracle for efficient generative inference of large language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "ge2023model",
        "author": "Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng",
        "title": "Model tells you what to discard: Adaptive kv cache compression for llms"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "oren2024transformers",
        "author": "Oren, Matanel and Hassid, Michael and Yarden, Nir and Adi, Yossi and Schwartz, Roy",
        "title": "Transformers are multi-state rnns"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "xiao2023efficient",
        "author": "Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike",
        "title": "Efficient streaming language models with attention sinks"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "ribar2023sparq",
        "author": "Ribar, Luka and Chelombiev, Ivan and Hudlass-Galley, Luke and Blake, Charlie and Luschi, Carlo and Orr, Douglas",
        "title": "Sparq attention: Bandwidth-efficient llm inference"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "tang2024quest",
        "author": "Tang, Jiaming and Zhao, Yilong and Zhu, Kan and Xiao, Guangxuan and Kasikci, Baris and Han, Song",
        "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "douze2024faisslibrary",
        "author": "Matthijs Douze and Alexandr Guzhva and Chengqi Deng and Jeff Johnson and Gergely Szilvasy and Pierre-Emmanuel Mazar\u00e9 and Maria Lomeli and Lucas Hosseini and Herv\u00e9 J\u00e9gou",
        "title": "The Faiss library"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "liu2024retrievalattention",
        "author": "Liu, Di and Chen, Meng and Lu, Baotong and Jiang, Huiqiang and Han, Zhenhua and Zhang, Qianxi and Chen, Qi and Zhang, Chengruidong and Ding, Bailu and Zhang, Kai and others",
        "title": "Retrievalattention: Accelerating long-context llm inference via vector retrieval"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "liu2024retrievalattention",
        "author": "Liu, Di and Chen, Meng and Lu, Baotong and Jiang, Huiqiang and Han, Zhenhua and Zhang, Qianxi and Chen, Qi and Zhang, Chengruidong and Ding, Bailu and Zhang, Kai and others",
        "title": "Retrievalattention: Accelerating long-context llm inference via vector retrieval"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "sun2024shadowkv",
        "author": "Sun, Hanshi and Chang, Li-Wen and Bao, Wenlei and Zheng, Size and Zheng, Ningxin and Liu, Xin and Dong, Harry and Chi, Yuejie and Chen, Beidi",
        "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "chen2024magicpig",
        "author": "Chen, Zhuoming and Sadhukhan, Ranajoy and Ye, Zihao and Zhou, Yang and Zhang, Jianyu and Nolte, Niklas and Tian, Yuandong and Douze, Matthijs and Bottou, Leon and Jia, Zhihao and others",
        "title": "Magicpig: Lsh sampling for efficient llm generation"
      }
    ]
  }
]