\section{Related Work}
\subsection{Attention Matrix Analysis}
The sparsity of attention weights in pre-trained LLMs,  especially in long-context scenarios,  has been well-documented **Vaswani et al., "Attention Is All You Need"**. 
**Liu et al., "Insights into the Attention Mechanism"** investigates the distribution of important tokens in the context and discovered that recent tokens are more important than distant ones. They also find that attention scores between consecutive layers are similar, which has also been previously observed in smaller models **Lin et al., "An Analysis of Transformers"**. 


**Clark et al., "Does the Long-Range Attention Mechanism Have an Advantage?"** reports that Attention weights were remarkably similar between the transformer layers,  particularly the adjacent layers. **Tay et al., "Attention-based Neural Architecture Search"** identify notable redundancy across LLM layers,  where some layers contribute marginally to the model.  **Brown et al., "Language Models are Few-Shot Learners"** shows that for some tasks,  LLMs can achieve results comparable to the final output at some intermediate layers. 



\subsection{KV Cache Eviction Algorithm}

Many previous efforts focuse on KV cache compression to accelerate attention and reduce memory usage. H2O **Goyal et al., "Efficient Attention using a Learned Mask"** retains a limited budget for the important KV cache regarding the sum of historical attention scores. FastGen **Zhang et al., "Fast and Efficient Attention with Generative Models"** further categorizes tokens and only keeps partial KV cache using a more sophisticated strategy. TOVA **Xiao et al., "Token-Oriented Value Aggregation"** simplifies the policy by determining the permanently discarded tokens using the current query. StreamingLLM **Teng et al., "Streaming-Style Language Models for Endless Text"** handles infinitely long text with attention sinks and a finite KV cache. SparQ **Xu et al., "Sparse Attention with Pruned Channels"** computes approximate attention scores by channel pruning and selects important tokens through them. **Wang et al., "Query-Driven Importance Estimation in KV Cache"** concludes that the importance of a token is highly dependent on the query and proposes Quest, a method that records the \textit{min} and \textit{max} key values in KV cache pages and estimates the importance of a page using query vectors. 


However, these approaches face several challenges. First, it is costly to identify the topk attention. For example, applying a naive search algorithm, e.g. IVF **Wu et al., "Indexing for Efficient Attention"** requires access over $30\%$ key states to obtain the topk results **Shen et al., "Efficient Top-K Selection with Hierarchical Indexing"**, which is quite compute-intensive. Second, these approaches save the KV cache in the GPU memory to avoid loading them from the CPU memory, which does not reduce the total memory consumption of KV cache, hence limiting the max context window and inference batch size.

Some papers attempted to offload KV cache to CPU memory to reduce the active GPU memory usage. **Zhang et al., "Approximate Nearest Neighbor Search in CPU Memory"** proposes to build approximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory and retrieve the most relevant ones through vector search during generation. **Li et al., "Efficient KV Cache Offloading with Low-Rank Key Caching"** stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. **Wu et al., "LSH-Hashed Attention Computation on CPU"** stores the LSH hash tables and runs the attention computation on the CPU, which significantly reduces the workload of attention computation. However, these works transmit the key-value (KV) cache at the token level, and in some approaches, the top-k selection is computed on a per-layer basis, which implies that the KV cache is also transferred layer by layer, resulting in significant overhead for h2d transfers.