\section{Related Work}
\subsection{Attention Matrix Analysis}
The sparsity of attention weights in pre-trained LLMs,  especially in long-context scenarios,  has been well-documented \citep{liu2022dynamic,  ribar2023sparq, liu2023deja,  xiao2023efficient}. 
\citet{ma2024compressing} investigates the distribution of important tokens in the context and discovered that recent tokens are more important than distant ones. They also find that attention scores between consecutive layers are similar, which has also been previously observed in smaller models \citep{xiao2019sharing,  bhojanapalli2021leveraging}. 


\citet{mu2024cross} reports that Attention weights were remarkably similar between the transformer layers,  particularly the adjacent layers. \citet{men2024shortgpt} identify notable redundancy across LLM layers,  where some layers contribute marginally to the model.  \citet{fan2024not} shows that for some tasks,  LLMs can achieve results comparable to the final output at some intermediate layers. 



\subsection{KV Cache Eviction Algorithm}

Many previous efforts focuse on KV cache compression to accelerate attention and reduce memory usage. H2O \citep{zhang2023h2o} retains a limited budget for the important KV cache regarding the sum of historical attention scores. FastGen \citep{ge2023model} further categorizes tokens and only keeps partial KV cache using a more sophisticated strategy. TOVA \citep{oren2024transformers} simplifies the policy by determining the permanently discarded tokens using the current query. StreamingLLM \citep{xiao2023efficient} handles infinitely long text with attention sinks and a finite KV cache. SparQ \citep{ribar2023sparq} computes approximate attention scores by channel pruning and selects important tokens through them. \citep{tang2024quest} concludes that the importance of a token is highly dependent on the query and proposes Quest, a method that records the \textit{min} and \textit{max} key values in KV cache pages and estimates the importance of a page using query vectors. 


However, these approaches face several challenges. First, it is costly to identify the topk attention. For example, applying a naive search algorithm, e.g. IVF \citep{douze2024faisslibrary}, requires access over $30\%$ key states to obtain the topk results \citep{liu2024retrievalattention}, which is quite compute-intensive. Second, these approaches save the KV cache in the GPU memory to avoid loading them from the CPU memory, which does not reduce the total memory consumption of KV cache, hence limiting the max context window and inference batch size.

Some papers attempted to offload KV cache to CPU memory to reduce the active GPU memory usage. \cite{liu2024retrievalattention} proposes to build approximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory and retrieve the most relevant ones through vector search during generation. \cite{sun2024shadowkv} stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. \cite{chen2024magicpig} stores the LSH hash tables and runs the attention computation on the CPU, which significantly reduces the workload of attention computation. However, these works transmit the key-value (KV) cache at the token level, and in some approaches, the top-k selection is computed on a per-layer basis, which implies that the KV cache is also transferred layer by layer, resulting in significant overhead for h2d transfers.