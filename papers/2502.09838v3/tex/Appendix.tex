\begin{center}
\Large \textbf{Appendix}
\end{center}
This is the Appendix for ``HealthGPT: A Medical Large Vision-Language Model for Unifying
Comprehension and Generation via Heterogeneous Knowledge Adaptation''. This Appendix is organized as follows: 

\begin{itemize}
\item \textbf{Section A} presents the experimental implementation details, the training process of \ourmethod{}, and the specifics of \texttt{VL-Health}.
\item \textbf{Section B} systematically provides an analysis of Heterogeneous Low-Rank Adaptation.
\item \textbf{Section C} shows supplementary experimental results to validate the effectiveness of \ourmethod{}.
\end{itemize}

\section{Implementation Details}
\subsection{Model Details}
We employ CLIP-L/14~\cite{radford2021learning} as the visual feature extractor, extracting both shallow and deep features to serve as visual tokens. The model uses alignment adapters, implemented with two-layer MLPs, to align shallow features, representing concrete visual granularity, and deep features, representing abstract visual granularity. These visual tokens are concatenated with text tokens and input into the large language models (LLMs).

\ourmethod{} offers two versions: \texttt{HealthGPT-M3} and \texttt{HealthGPT-L14}, which are based on Phi-3-mini~\cite{abdin2024phi} and Phi-4~\cite{abdin2024phi} as the pre-trained LLMs, respectively. In addition, we expand the LLM vocabulary with 8192 VQ indices derived from VQGAN-f8-8192~\cite{esser2021taming}, serving as multi-modal tokens to further augment the model's capacity for understanding both visual and textual input. Figure \ref{tab:model_details} shows the details.
\begin{table}[h!]
    \centering
    \vspace{-1mm}
    \caption{Overview of the Components of \ourmethod{}.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cccccccc}
        \toprule
        \rowcolor[HTML]{E9F3FE}
        \textbf{Model} & \textbf{ViT} & \textbf{Adapter} & \textbf{MLP-dims} & \textbf{Model dims} & \textbf{LLM} & \textbf{Params} & \textbf{Vocab Size} & \textbf{H-LoRA Rank} \\ \hline \hline
        \cellcolor[HTML]{DAE0FB}
        HealthGPT-M3 & CLIP-L/14 & 2-layer MLP & 1024 & 3072 & Phi-3-mini & 3.8B & 40206 & 16(Comp.), 64(Gen.) \\
        \cellcolor[HTML]{DAE0FB}
        HealthGPT-L14 & CLIP-L/14 & 2-layer MLP & 1024 & 5120 & Phi-4 & 14B & 108547 & 8(Comp.), 32(Gen.) \\ 
        \bottomrule
    \end{tabular}
    }
    \label{tab:model_details}
\end{table}
\vspace{-2mm}
\subsection{Training Details}
In this study, we propose a three-stage learning strategy that is compatible with our innovative heterogeneous low-rank adaptation (H-LoRA). We provide a detailed hyperparameter configuration for the model's three-stage training process. The specific hyperparameter settings used are listed in Table \ref{tab:parameters}. These hyperparameters are crucial for ensuring the model's learning efficacy and final performance.
\begin{table}[h!]
    \centering
    \vspace{-1mm}
    \caption{Overview of Hyperparameter Configurations.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cc|cc|cc|cc|cc|cc}
    \toprule
    \rowcolor[HTML]{E9F3FE}
    & \multicolumn{6}{c|}{\texttt{HealthGPT-M3}} & \multicolumn{6}{c}{\texttt{HealthGPT-L14}} \\
    \cline{2-13}
    \rowcolor[HTML]{E9F3FE}
    & \multicolumn{2}{c|}{Stage-1} & \multicolumn{2}{c|}{Stage-2} & \multicolumn{2}{c|}{Stage-3} & \multicolumn{2}{c|}{Stage-1} & \multicolumn{2}{c|}{Stage-2} & \multicolumn{2}{c}{Stage-3} \\
    \cline{2-13}
    \rowcolor[HTML]{E9F3FE}
    \multirow{-3}{*}{Hyperparameter} & Comp. & Gen. & Comp. & Gen. & Comp. & Gen. & Comp. & Gen. & Comp. & Gen. & Comp. & Gen. \\
    \hline \hline
    \cellcolor[HTML]{DAE0FB}
    Optimizer & \multicolumn{2}{c|}{AdamW} & \multicolumn{2}{c|}{AdamW} & \multicolumn{2}{c|}{AdamW} & \multicolumn{2}{c|}{AdamW} & \multicolumn{2}{c|}{AdamW} & \multicolumn{2}{c}{AdamW} \\
    \cellcolor[HTML]{DAE0FB}
    Adapter LR & 1e-3 & 2e-5 & \multicolumn{2}{c|}{2e-5} & \multicolumn{2}{c|}{2e-5} & 1e-3 & 2e-5 & \multicolumn{2}{c|}{2e-5} & \multicolumn{2}{c}{2e-5}\\
    \cellcolor[HTML]{DAE0FB}
    Learning Rate & / & 2e-4 & \multicolumn{2}{c|}{2e-4} & \multicolumn{2}{c|}{2e-4} & / & 1e-4 & \multicolumn{2}{c|}{2e-4} & \multicolumn{2}{c}{2e-4} \\
    \cellcolor[HTML]{DAE0FB}
    Global Batch Size & 256 & 64 & \multicolumn{2}{c|}{32} & 128 & 64 & 256 & 64 & \multicolumn{2}{c|}{32} & 128 & 64 \\
    \cellcolor[HTML]{DAE0FB}
    Weight Decay & \multicolumn{2}{c|}{0} & \multicolumn{2}{c|}{0} & \multicolumn{2}{c|}{0} & \multicolumn{2}{c|}{0} & \multicolumn{2}{c|}{0} & \multicolumn{2}{c}{0} \\
    \cellcolor[HTML]{DAE0FB}
    Dropout Rate & 0 & 0.05 & \multicolumn{2}{c|}{0.05} & \multicolumn{2}{c|}{0.05} & 0 & 0.05 & \multicolumn{2}{c|}{0.05} & \multicolumn{2}{c}{0.05} \\
    \cellcolor[HTML]{DAE0FB}
    LR Scheduler & \multicolumn{2}{c|}{Warm Up} & \multicolumn{2}{c|}{Constant} & \multicolumn{2}{c|}{Warm Up} & \multicolumn{2}{c|}{Warm Up} & \multicolumn{2}{c|}{Constant} & \multicolumn{2}{c}{Warm Up} \\
    \cellcolor[HTML]{DAE0FB}
    Max Sequence Length & \multicolumn{2}{c|}{2048} & \multicolumn{2}{c|}{2048} & \multicolumn{2}{c|}{2048} & \multicolumn{2}{c|}{2048} & \multicolumn{2}{c|}{2048} & \multicolumn{2}{c}{2048} \\ 
    \bottomrule
    \end{tabular}
    }
    \label{tab:parameters}
\end{table}

It is worth noting that we sometimes observe instances of loss spikes during the training of medical visual comprehension and generation tasks. Through repeated validation, we discovered that larger model parameters and learning rates tend to lead to this issue, which is the reason for the slight differences in hyperparameters between \texttt{HealthGPT-M3} and \texttt{HealthGPT-L14}.

\subsection{\texttt{VL-Health}}
The construction of the \texttt{VL-Health} dataset involves two key steps: \textbf{(\rmnum{1}) data collection}, \textbf{(\rmnum{2}) data processing}, as detailed below:

\noindent \textbf{Data Collection:} During the collection phase, we carefully considered the diversity of medical images and the complexity of the tasks, selecting appropriate subsets for comprehension and generation tasks. For comprehension tasks, we selected datasets such as VQA-RAD~\cite{lau2018dataset}, SLAKE~\cite{liu2021slake}, PathVQA~\cite{he2020pathvqa}, and MIMIC-CXR-VQA~\cite{bae2024mimic}, which cover various medical imaging modalities like radiology and pathology, and include professional annotations to assist the model in learning tasks such as lesion detection and disease diagnosis. Additionally, large-scale multi-modal datasets like LLaVA-Med~\cite{li2024llava} and PubMedVision~\cite{chen2024huatuogpt} were included to provide broader medical knowledge support and facilitate the training of complex reasoning tasks. For generation tasks, we focused on four mainstream task categories: super-resolution image generation, modality conversion, text-to-image generation, and image reconstruction. The IXI~\cite{IXI} dataset, containing a large number of healthy brain MRI images, is suitable for training super-resolution models; the MIMIC-CHEST-XRAY~\cite{bae2024mimic} dataset, with X-ray images and their corresponding textual reports, is appropriate for text-to-image generation tasks; the SynthRAD2023~\cite{thummerer2023synthrad2023} dataset provides a large number of paired CT and MRI images, supporting modality conversion model training; for image reconstruction tasks, we rewrote and adjusted the LLaVA-558k~\cite{liu2024improved} dataset.

\noindent \textbf{Data Processing:} After data collection, we performed filtering and processing of the raw data. For VisualQA tasks, we standardized the data entries into two forms: open-ended questions and single-choice questions, enabling flexible training and evaluation. Additionally, considering that multi-image data has a minimal impact on performance but introduces extra padding and training time, we excluded multi-image data. For the scanned image data in generation tasks, we applied slicing extraction, image registration, data augmentation, and normalization to treat 2D images as visual inputs for model training or used VQGAN-generated indices to supervise the generation tasks.
% \subsubsection{Data Construction}
% \textbf{Dataset Collection.}In the data collection phase, we thoroughly considered the diversity of medical images and the complexity of the tasks, ultimately selecting datasets to enhance the model's visual understanding and high-quality medical image generation capabilities.For understanding tasks, we selected datasets such as VQA-RAD, SLAKE, PathVQA, and MIMIC-CXR-VQA. These datasets include radiological images, pathology images, and other modalities, along with professional annotations that assist the model in learning to identify lesions, classify disease types, etc.For generation tasks, we selected the IXI, SynthRAD2023, MIMIC-CHEST-XRAY, and LLaVA-558k datasets for the four mainstream tasks: super-resolution image generation, modality conversion, text-to-image generation, and image reconstruction, respectively.

% \textbf{Data Processing.} After collecting the raw datasets, we performed filtering and processing. For the VisualQA tasks, we standardized the data entries in the dataset into two QA formats: open-ended questions and multiple-choice questions, to facilitate direct and flexible training and evaluation. Our researchers, with extensive experience in vision-language learning, also improved and standardized the prompt language across different datasets to enhance the training quality. The original data in the SynthRAD2023 dataset consists of 3D CT and MRI scan images. To improve model training efficiency and performance, we followed the method proposed by Zhenbin Wang, performing operations such as slice extraction, image registration, data augmentation, and normalization on the original 3D images. The processed 2D images were then used for training the deep learning models.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig/distribution.pdf}
    \caption{VL-Health dataset collection distribution.}
    \label{fig:human_eval}
    \vskip -0.1in
\end{figure}

\subsubsection{Data Statistics}
This section provides detailed statistical information about the \texttt{VL-Health} dataset to offer a more comprehensive understanding.

\noindent \textbf{Data Overview:} To ensure a balanced development of the model's comprehension and generation capabilities, in addition to the LLaVA-558k and PubMedVision-PT datasets used for alignment, the \texttt{VL-Health} dataset ultimately selected 765,802 additional visual question-answering (VQA) training samples (to endow the model with visual comprehension and instruction-following capabilities) and 783,045 generation training samples (to provide the model with reconstruction and visual generation instruction-following abilities). This contributes to the transfer of knowledge between comprehension and generation tasks, enhancing the model's overall performance. For medical image comprehension tasks, images were selected from VQA-RAD (approximately 450 images), SLAKE (approximately 630 images), PathVQA (approximately 2,600 images), MIMIC-CXR-VQA (approximately 52,000 images), LLaVA-Med (approximately 61,000 images), and PubMedVision (approximately 500,000 images). Multiple question-answer pairs were retained for each image to enhance the model's understanding and generalization of the image content. Table \ref{table:medical_task_stages} shows the data distribution of \texttt{VL-Health} for three-stage learning strategy, where mixed-47k is based on the sampling of all data in stage-1.

\noindent \textbf{Diversity and Quality Assessment:} \texttt{VL-Health} covers 11 modalities, including CT, MRI, X-ray, microscopy, OCT, ultrasound, and fundus photography, which aids the model in learning features from various modalities. The dataset also encompasses a wide range of diseases, from common to rare, and from localized lesions to systemic diseases, including pulmonary diseases, skeletal abnormalities, brain lesions, tumors, cardiovascular diseases, and cellular abnormalities. This provides comprehensive training support to the model, enabling it to learn the characteristics and diagnosis of various diseases.

\begin{table}[h]
\centering
\caption{Data distribution of \texttt{VL-Health} in three-stage learning strategy.}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|l|l}
\toprule
\rowcolor[HTML]{E9F3FE}\textbf{Medical Task} & \textbf{Stage-1} & \textbf{Stage-2}  \\ \hline \hline
\cellcolor[HTML]{DAE0FB}Comp. & LLaVA-558k, PubMedVision-PT & \multirow{2}{*}{Mixed-47k} \\
\cellcolor[HTML]{DAE0FB}Gen.  & LLaVA-558k &   \\
\hline
\rowcolor[HTML]{E9F3FE}\textbf{Medical Task} & \multicolumn{2}{l}{\textbf{Stage-3}}\\
\hline \hline
\cellcolor[HTML]{DAE0FB}Comp. & \multicolumn{2}{l}{LLaVA\_Med, MIMIC\_CXR\_VQA, PubMedVision-FT, LLaVA-665k, PathVQA, SLAKE, VQA-RAD} \\
\cellcolor[HTML]{DAE0FB}Gen. & \multicolumn{2}{l}{IXI, SynthRAD2023, MIMIC-CHEST-XRAY} \\
\bottomrule
\end{tabular}
}
\label{table:medical_task_stages}
\end{table}


% This section provides detailed statistics of the \texttt{VL-Health} dataset to offer a more comprehensive understanding.

% \textbf{Overview of Data.} In order to ensure balanced development of the model's comprehension and generation capabilities, the \texttt{VL-Health} dataset ultimately filtered 765,802 extra visual QA training samples and 783,045 generation training samples. This facilitates knowledge transfer between comprehension and generation tasks, improving the overall performance of the model. For the medical image comprehension task, we selected images from VQA-RAD (about 450 images), SLAKE (about 630 images), PathVQA (about 2,600 images), MIMIC-CXR-VQA (about 52,000 images), LLaVA-Med (about 61,000 images), and PubMedVision (about 500,000 images). We retained multiple question-answer pairs for each image to enhance the model's ability to understand and generalize the content of the images.

% \textbf{Diversity and Quality Assessment.} VL-Health covers 11 modalities, including CT, MRI, X-ray, Microscopy, OCT, Ultrasound, and Fundus, helping the model learn features from different modalities. The dataset also spans a wide range, from common diseases to rare diseases, and from localized lesions to systemic diseases, including lung diseases, skeletal abnormalities, brain lesions, tumors, cardiovascular diseases, and cellular abnormalities. This provides comprehensive training support, enabling the model to learn the features and diagnoses of various diseases.

\subsubsection{Data Format.}

All data samples are converted into a unified instruction-response format for training and evaluation. Specifically, the \texttt{VL-Health} dataset consists of the following components:
\begin{itemize}
    \vspace{-2mm}
    \item \texttt{Task Type}: Specifies the granularity of visual features output by the visual encoder and selects the corresponding H-LoRA submodule. For generation tasks, the response also includes multi-modal tokens corresponding to VQ indices.
    \vspace{-2mm}
    \item \texttt{Task Instruction}: Guides the model to interpret the image and generate a response, covering various aspects of the image and specifying the output format.
    \vspace{-2mm}
    \item \texttt{Response}: The textual output generated based on the task instruction and input image, ensuring it meets the question and formatting requirements.
    \vspace{-2mm}
    \item \texttt{Input Image}: Provides the visual signal for the model to process.
    \vspace{-2mm}
    \item \texttt{Target Image Index}: In generation tasks, this is added as a multi-modal token to the response for autoregressive generation.
    \vspace{-2mm}
\end{itemize}

\section{Analysis of Heterogeneous Low-Rank Adaptation}
We propose H-LoRA, which utilizes hard routing selection to allocate plugins for knowledge learning and representation across tasks, thereby preventing conflicts arising from heterogeneous knowledge. Furthermore, within each task, we optimized based on MoELoRA, enhancing performance while reducing computational overhead. The pseudocode is detailed Algorithm \ref{alg:h-lora}.

\begin{algorithm}[h]
   \caption{H-LoRA Algorithm}
   \label{alg:h-lora}
\begin{algorithmic}
   \STATE {\bfseries Input:} concrete-grained visual features $\mathcal{F}^\text{Con}$, abstract-grained visual features $\mathcal{F}^\text{Abs}$, comprehension-based H-LoRA modules $(\{A_i^\text{Comp.}\}_{i=1}^k, \mathcal{R}_\text{outer}^\text{Comp.})$, generation-based H-LoRA modules $(\{A_i^\text{Gen.}\}_{i=1}^k, \mathcal{R}_\text{outer}^\text{Gen.})$, task type $T$ (comprehension or generation), number of LoRA experts $k$, origin linear layer weights $W_0$, text features $\mathcal{T}$, hidden state h
   \STATE {\bfseries Output:} final output $\mathcal{O}$
   \STATE // Select task-specific image features
   \IF{$T = \text{generation task}$} 
       \STATE $\mathcal{F}^{\text{img}} \gets \mathcal{F}^{\text{Con}}$
   \ELSIF{$T = \text{comprehension task}$}
       \STATE $\mathcal{F}^{\text{img}} \gets \mathcal{F}^{\text{Abs}}$
   \ENDIF

   \STATE $\mathcal{U} \gets \text{concat}(\mathcal{F}^{\text{img}}, \mathcal{T})$ // Concatenate image features and text features

   \STATE $\{A_i\}_{i=1}^{k}, \{B_i\}_{i=1}^{k},\mathcal{R}_\text{outer}  \gets \{A_i^T\}_{i=1}^{k}, \{B_i^T\}_{i=1}^{k},\mathcal{R}^T_\text{outer}$ // Assign task-specific H-LoRA submodule

   \STATE // Merge LoRA experts' matrices
   \STATE $\mathbf{A}^{\text{merged}} \gets \text{concat}(\{A_i\}_{i=1}^{k})$
   \STATE $\mathbf{B}^{\text{merged}} \gets \text{concat}(\{B_i\}_{i=1}^{k})$
   \STATE $\mathcal{W} \gets R(\mathbf{h})$ // Generate routing weights based on input hidden state $\mathbf{x}$

   \STATE $\mathcal{W}^\text{expanded} \gets \alpha \times \mathcal{W} / r \otimes \mathbf{1}_r$ // Expand routing weights to match merged matrices

   \STATE $\mathcal{O}^\text{H-LoRA} \gets (x \cdot \mathbf{A}^{\text{merged}} \odot \mathcal{W}^\text{expanded}) \cdot \mathbf{B}^{\text{merged}}$ // Compute H-LoRA output using element-wise multiplication

   \STATE $\mathcal{O} \gets x \cdot W_0 + \mathcal{O}^\text{H-LoRA}$ // Add H-LoRA output to pre-trained weights to get final output

   \STATE \textbf{Return} $\mathcal{O}$

\end{algorithmic}
\end{algorithm}

We further analyzed the computational overhead differences between MoELoRA and H-LoRA. Assuming that both methods use the same number of LoRA experts $k$, we can compare their time complexity from the perspective of the operational steps involved.

\noindent \textbf{Computational Overhead of MoELoRA.} In MoELoRA, the operations involving the expert matrix mainly include the following steps: 
\textbf{(i) Expert Multiplication}: MoELoRA requires $2k$ multiplications with the LoRA experts. 
\textbf{(ii) Router Multiplication}: One multiplication with the Router is required. 
\textbf{(iii) Router Output Expansion}: MoELoRA needs to perform $k$ expansion operations on the Router's output weights to generate the appropriate shapes that match the dimensions of the input and LoRA experts while iterating through the experts. 
\textbf{(iv) Dot Product}: For each expanded Router weight, a dot product with the intermediate state of the expert is required, resulting in $k$ multiplications. 
\textbf{(v) Addition}: Finally, $k$ addition operations are required to accumulate the results from each LoRA expert into the final output. Assuming the time complexity of each operation is the same, the additional time complexity introduced when equipping a fully connected layer with MoELoRA is: 
$O(2k + 1 + k + k + k) = O(5k + 1)$.
Thus, MoELoRA introduces an additional time overhead of $O(5k + 1)$ during computation.

\noindent \textbf{H-LoRA.} In contrast to MoELoRA, H-LoRA reduces the computational overhead by concatenating the LoRA expert matrices. Specifically:
\textbf{(i) Expert Multiplication}: H-LoRA merges all LoRA experts by directly creating a larger A and B matrix, instead of performing independent operations for each expert. This process can be implemented through matrix initialization without additional concatenation operations. Therefore, only $2$ multiplications with the LoRA experts are required. 
\textbf{(ii) Router Multiplication}: H-LoRA still requires one multiplication with the Router. 
\textbf{(iii) Router Output Expansion}: H-LoRA only requires one expansion operation on the Router's output weights. 
\textbf{(iv) Dot Product}: H-LoRA only requires one dot product between the Router's output and the expert's intermediate state. 
\textbf{(v) Addition}: Finally, H-LoRA only requires one addition operation to accumulate the LoRA expert results into the intermediate state. Therefore, the additional time complexity introduced by H-LoRA is:
$O(2 + 1 + 1 + 1 + 1) = O(6)$.

Comparing the two, we see that MoELoRA introduces a linear increase in additional time complexity with respect to the number of experts $k$, resulting in a complexity of $O(5k + 1)$, while H-LoRA’s additional time complexity is fixed at $O(6)$, independent of $k$. We observe that when $k$ is small, the time complexity differences between MoELoRA and H-LoRA are negligible. However, as $k$ increases, MoELoRA’s computational overhead grows linearly, while H-LoRA’s remains constant. This makes H-LoRA significantly more computationally efficient than MoELoRA, particularly in large-scale tasks. We will further demonstrate the significant advantage of H-LoRA in training time in subsequent experiments, validating its efficiency in practical applications.

\section{Supplemental Experimental Results}
In this section, we include additional experiments to demonstrate the superiority of \ourmethod{} and articulate our design philosophy.

\subsection{Results: OmniMedVQA Benchmark} OmniMedVQA~\cite{hu2024omnimedvqa} is a novel, large-scale medical visual question answering (VQA) benchmark designed to encompass various modalities and anatomical regions by collecting diverse images from multiple medical datasets. Our experimental results are presented in Table \ref{tab:omnimedvqa}.

\input{tab/visual_comprehension_part2}

Through our analysis, we make the following observations: (i) \texttt{HealthGPT-M3} outperforms other models in 4 out of 7 sub-tasks, achieving an average score that exceeds cutting-edge medical Large Vision-Language Models (LVLMs) as well as general LVLMs; (ii) the unified model demonstrates relatively weak performance on OmniMedVQA; however, our approach effectively mitigates performance degradation caused by generation tasks, serving as a unified model; (iii) \texttt{HealthGPT-L14} excels across all sub-tasks, achieving optimal or near-optimal results with an average score of 74.4, significantly surpassing other models.


% \section{Instruction Template}
% \subsection{Medical Comprehension Tasks.}
% \subsection{Medical Generation Tasks.}

% \section{Pseudocode}
% \subsection{H-LoRA}
% \subsection{Trhee-stages Learning Stragety}

% \section{Additional experiments and results}
% \subsection{Reconstruction}
% \subsection{Heterogeneous Knowledge Adaptation with H-LoRA Plugs}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\linewidth]{fig/human_eval_v2.pdf}
%     \caption{(a) Proportion of model responses selected as the best in human evaluation. (b) Human Evaluation Dataset.}
%     \label{fig:human_eval}
%     \vskip -0.1in
% \end{figure}
% \subsection{Human Evaluation.}
% We further conduct human evaluation on the VQA-RAD, SLAKE, and PathVQA benchmarks,  which contain 1,000 open-ended questions.  Specifically, we recruit 5 clinicians to rank the randomly shuffled responses from \ourmethod{}, LLaVA-Med, HuatuoGPT-Vision, Llama-3.2, InternVL-2 and Show-o. During the evaluation, questions were randomly selected, and the model-generated responses were anonymized and ranked. The results, as shown in Figure 6, indicate that \ourmethod{} was frequently selected as the best answer. This suggests that \ourmethod{} has further application potential in medical care scenarios.

\subsection{Stability Analysis of Number of Experts} 
We investigated the impact of the number of LoRA experts on model performance within a multi-LoRA architecture, conducting extensive experiments on MoELoRA and H-LoRA with varying numbers of experts. The experimental results are presented in Table \ref{tab:lora_num}. As the number of experts increases, the training time for MoELoRA is significantly prolonged. When \( n = 8 \), the training time for MoELoRA is twice that of LoRA, whereas H-LoRA incurs no additional training delay and performs better. It is estimated that at \( n = 32 \), the training time for MoELoRA could reach eight times that of LoRA, preventing it from completing training and inference. This result aligns with the analysis in Appendix B, indicating that H-LoRA not only avoids introducing additional training delays compared to LoRA but also outperforms MoELoRA.

\input{tab/Ablation_LoRA_expert_num}




\subsection{Impact of Heterogeneous Knowledge Fusion on Performance} 
Traditional unified models often utilize mixed training methods, which may result in performance degradation due to variations in task modes. To address this, we propose a three-phase learning strategy to support H-LoRA, effectively mitigating inter-task conflicts. Specifically, the second phase (Heterogeneous H-LoRA Plugin Adaptation) integrates LLMs with different H-LoRA plugins into a new unified foundation by mixing the training of the embedding layers and output heads for two tasks. Figure \ref{fig:stage-2} illustrates the impact of this phase on the performance of medical comprehension and generation tasks. We observe that the second phase effectively unifies the model with minimal impact on overall performance, significantly alleviating the conflict issues arising from mixed training in medical scenarios.


\subsection{Human Evaluation.}
\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    \includegraphics[width=0.35\textwidth]{fig/stage-2.pdf}
    \caption{Performance changes before and after the stage-2.}
    \label{fig:stage-2}
\end{wrapfigure}
We further conduct human evaluation on the VQA-RAD, SLAKE, and PathVQA benchmarks,  which contain 1,000 open-ended questions.  Specifically, we recruit 5 clinicians to rank the randomly shuffled responses from \texttt{HealthGPT-L14}, LLaVA-Med, HuatuoGPT-Vision, Llama-3.2, InternVL-2 and Show-o. During the evaluation, questions were randomly selected, and the model-generated responses were anonymized and ranked. The results, as shown in Figure \ref{fig:human_eval}, indicate that \ourmethod{} was frequently selected as the best answer. This suggests that \ourmethod{} has further application potential in medical care scenarios.

\subsection{Reconstruction Performance}

Currently, unified models that align visual features based on reconstruction tasks include pre-LVLMs, post-LVLMs, as well as Unified-IO 2~\cite{lu2024unified} and SEED-X~\cite{ge2024seed}. To investigate the controllability of visual generation in rigorous settings such as medical contexts, we evaluated the performance of these models in medical image reconstruction in Table \ref{tab:conversion}. Experimental results demonstrate that \ourmethod{} exhibits the most stable reconstruction performance with a small amount of data.

\subsection{Case Study}
Figures \ref{fig:mt_case} and \ref{fig:sr_case} illustrate examples of modality transformation and super-resolution reconstruction. In Figure \ref{fig:mt_case}, the results generated by our method in the CT (MRI) to MRI (CT) transformation task are highly close to the ground truth, effectively guiding the model in the transformation across different regions. For the MRI super-resolution reconstruction task, Figure \ref{fig:sr_case} demonstrates the accuracy of our method in restoring scan image details, accurately reconstructing the essential details of the image.

\input{tab/reconstruction}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.55\linewidth]{fig/human_eval_v2.pdf}
    \caption{(a) Proportion of model responses selected as the best in human evaluation. (b) Human Evaluation Dataset.}
    \label{fig:human_eval}
    \vskip -0.1in
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{fig/MT.png}
    \caption{Case of modality transfer.}
    \label{fig:mt_case}
    \vskip -0.1in
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\linewidth]{fig/SR_CASE.png}
    \caption{Case of MRI image super-resolution.}
    \label{fig:sr_case}
    \vskip -0.1in
\end{figure}


% \begin{algorithm}[H]
% \caption{Classical MoELoRA with Multiple A and B Matrices}
% \KwIn{\\$x$ : Input features, \\$\mathbf{W}$ : pre-trained weight matrix, \\$r$ : LoRA parameter dimension, \\$lora\_num$ : Number of experts, \\$\mathbf{A_i}$, $\mathbf{B_i}$ : A and B matrices for each expert}
% \KwOut{Final result}

% \textbf{Initialization:}
% \Begin{
%     \For{$i = 1$ \KwTo $lora\_num$}{
%         $\mathbf{A_i} \in \mathbb{R}^{d_i \times r}$, $\mathbf{B_i} \in \mathbb{R}^{r \times d_o}$\;
%     }
%     $\mathbf{route\_weights} \in \mathbb{R}^{d_i \times lora\_num}$\;
% }

% \textbf{Forward Pass:}
% \Begin{
% $\mathbf{output} \gets \mathbf{W}x$\\
% $\mathbf{route\_weight} \gets \text{softmax}(\text{Route\_Layer}(x))$ \\
% \For{$i = 1$ \KwTo $lora\_num$}{
%     $\mathbf{A_i\_output} \gets \mathbf{A_i}(x) \times \alpha$
%     $\text{lora\_output}_i \gets \mathbf{route\_weight_i} \cdot (\mathbf{A_i\_output} \times \mathbf{B_i})$ \\
%     $\mathbf{output} \gets \mathbf{output} + \text{lora\_output}_i$
% }
% }
% \end{algorithm}

% \begin{algorithm}[H]
% \caption{Our Method with Single A and B Matrix}
% \KwIn{\\$x$ : Input features, \\$\mathbf{W}$ : pre-trained weight matrix, \\$r$ : LoRA parameter dimension, \\$lora\_num$ : Number of experts, \\$\mathbf{A}$ : Single A matrix, \\$\mathbf{B}$ : Single B matrix}
% \KwOut{Final result}

% \textbf{Initialization:}
% \Begin{
%     $\mathbf{A} \in \mathbb{R}^{d_i \times r}$, $\mathbf{B} \in \mathbb{R}^{r \times d_o}$ \\
%     $\mathbf{route\_weights} \in \mathbb{R}^{lora\_num \times r}$ 
% }

% \textbf{Forward Pass:}
% \Begin{
%     $\mathbf{output} \gets \mathbf{W}x$ \\
%     $\mathbf{route\_weight} \gets \text{softmax}(\text{Route\_Layer}(x))$ \\
%     $\mathbf{A\_output} \gets \mathbf{A}(x) \times \alpha \times lora\_num $ \\
%     $\mathbf{expanded\_route\_weight} \gets \mathbf{route\_weight}.repeat(\frac{r}{lora\_num},dim=-1)$ \\
%     $\text{lora\_output} \gets \mathbf{expanded\_route\_weight} \times \mathbf{A\_output} \times \mathbf{B}$\\
%     $\mathbf{output} \gets \mathbf{output} + \text{lora\_output}$
% }
% \end{algorithm}