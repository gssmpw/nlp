\section{Experiments}
\label{Experiments}

\subsection{Data and Experimental Setup}
\noindent\textbf{Data Details.} We curate \texttt{VL-Health} dataset~(see Figure~\ref{fig:data}). 
For medical visual comprehension, 
% To enhance the domain-specific knowledge of the HealthCare model for medical visual comprehension tasks,
we leverage multiple medical-specific datasets, including PubMedVision~\cite{chen2024huatuogpt}, LLaVA-Med~\cite{li2024llava}, PathVQA~\cite{he2020pathvqa}, MIMIC-CXR-VQA~\cite{bae2024mimic}, SLAKE~\cite{liu2021slake}, and VQA-RAD~\cite{lau2018dataset}. Additionally, we incorporate high-quality open-world data from LLaVA-1.5~\cite{liu2024improved} to preserve the model's general knowledge and instruction-following capabilities.
% \begin{wrapfigure}{r}{0.3\textwidth}
%   \begin{center}
%   \vskip +5mm
%     \includegraphics[width=1.2\linewidth]{fig/data.pdf}
%     \caption{Data statistics of VL-Health.}
%     \vspace{-2mm}
%     \label{fig:data}
%   \end{center}
% \end{wrapfigure}
For generation tasks, we construct a reconstruction dataset based on LLaVA-558k~\cite{liu2024improved}, and also explore two key tasks in personalized medical image enhancement—super-resolution and modality conversion—using the IXI~\cite{IXI} and SynthRAD2023~\cite{thummerer2023synthrad2023} datasets. Detailed data selection and instruction templates are in the Appendix.

\noindent\textbf{Model Details.} We select CLIP-L/14~\cite{radford2021learning} as the visual encoder and used the hidden states of its second and penultimate layers as concrete-grained and abstract-grained features for model's dynamic hierarchical visual perception. Drawing on the successful experiences of LLaVA, we employ a MLP to align the multi-modal feature embeddings. We choose the parameter-efficient phi-3-mini~\cite{abdin2024phi} and phi-4~\cite{abdin2024phi} as the base model. For visual comprehension and generation tasks, we set the rank of H-LoRA to 16 and 64, with four experts. Additionally, we use the f8-8192 version of VQGAN as the image indexing and upsampling module.
\vspace{-3mm}

\subsection{Main Experiments}
\input{tab/SR}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/Ablation_LoRA_Rank.pdf}
    \caption{Performance comparison of LoRA, MoELoRA, and H-LoRA under different rank settings.}
    \label{fig:6}
    \vskip -0.1in
\end{figure}

\noindent \textbf{Comprehension.}  
% We compare \ourmethod{} with several existing models, including medical-specific LVLMs (e.g., Med-Flamingo, LLaVA-Med, HuatuoGPT-Vision) as well as recent open-world LVLMs (e.g., BLIP-2, LLaVA-v1.5, InstructBLIP, Yi-VL, InternVL2, Llama-3.2) \textcolor{red}{[TODO: Citation]}.
% Additionally, we test several SOTA unified visual comprehension and generation models, including Show-o, Unified-IO 2, and Janus. The experimental results are shown in Table \ref{tab:results}, with the following key observations:
% (i) In medical visual comprehension tasks, \ourmethod{} demonstrate superior performance, achieving scores of 78.7 and 68.5 in the evaluation metrics ``close" and OmniMedVQA, respectively, significantly outperforming both medical-specific models (e.g., HuatuoGPT-Vision) and general-purpose models (e.g., Llama-3.2).  
% (ii) Despite being trained on billions of data points, unified models still exhibit poor generalization performance in medical visual comprehension. For instance, Unified-IO 2 scored only 33.8. In contrast, Yi-VL and InternVL2 achieved scores of 44.9 and 52.5, respectively.  
% (iii) \ourmethod{}, with only 3.8B parameters, scored 61.3 on the medical multi-modal unified task, significantly outperforming existing unified models in medical downstream scenarios.  
% These results underscore the superiority of \ourmethod{} in medical image comprehension tasks.
We compare \ourmethod{} with several existing models, including medical-specific LVLMs (e.g., Med-Flamingo~\cite{moor2023med}, LLaVA-Med~\cite{li2024llava}, HuatuoGPT-Vision~\cite{chen2024huatuogpt}) as well as recent open-world LVLMs (e.g., BLIP-2~\cite{li2023blip}, LLaVA-v1.5~\cite{liu2024improved}, InstructBLIP~\cite{dai2023instructblipgeneralpurposevisionlanguagemodels}, Yi-VL~\cite{young2024yi}, InternVL2~\cite{chen2024far}, Llama-3.2~\cite{dubey2024llama}). Additionally, we test several SOTA unified visual comprehension and generation models, including Show-o~\cite{xie2024show}, Unified-IO 2~\cite{lu2024unified}, and Janus~\cite{wu2024janus}. The experimental results are shown in Table \ref{tab:results}, with the following key observations: \textbf{(i) SOTA Results Compared with LVLMs:} In medical visual comprehension tasks, \ourmethod{} demonstrates superior performance, significantly outperforming both medical-specific models (e.g., HuatuoGPT-Vision) and general-purpose models (e.g., Llama-3.2). \textbf{(ii) Surpassing Current Unified LVLMs:} Despite being trained on billions of data points, unified models still exhibit poor generalization performance in medical visual comprehension. For instance, Unified-IO 2 scored only 33.8. In contrast, \texttt{HealthGPT-M3}, with only 3.8B parameters, scored 61.3 on the medical multi-modal unified task, significantly outperforming existing unified models in medical downstream scenarios. \textbf{(iii) Stable Improvement with Large Base Model:} Our method demonstrates excellent scalability, with \texttt{HealthGPT-L14} achieving a score of 66.4 in the larger model configuration. This result significantly outperforms all other models, highlighting the effectiveness of scaling up the base model for enhanced performance in medical tasks.

\input{tab/Ablation_LoRA}
% \input{tab/Ablation LoRA expert num}
\input{tab/Ablation_three_stages}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.92\linewidth]{fig/Visual_Perception_v2.pdf}
    \caption{ The loss visualization (a) and performance comparison (b) with respect to different visual perceptions. }
    \label{fig:perception}
\end{figure}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/CXR_Case_v2.pdf}
    \caption{Case study of report-to-CXR under different instructions. (a) shows a normal CXR image for comparison. (b) and (c) illustrate generated cases with varying severity and affected regions. The graffiti areas indicate abnormal conditions.}
    \label{fig:CXR_Case}
\end{figure*}

\noindent \textbf{Generation.}  
We study three key tasks in medical imaging.  
\textbf{(i) Modality Conversion:} In this task, we focus on the conversion between CT and MRI modalities for the brain and pelvic regions, designing four specific sub-tasks. All comparative models (Pix2Pix~\cite{isola2017image}, CycleGAN~\cite{zhu2017unpaired}, BBDM~\cite{li2023bbdm}, Vmamba~\cite{liu2024vmamba}, and DiffMa~\cite{wang2024soft}) trained a separate model for each sub-task, while \ourmethod{} unify all tasks into a single training process. 
The experimental results, shown in Table ~\ref{tab:conversion}, demonstrate that our approach outperforms other methods across multiple evaluation metrics. For instance, in the CT2MRI-Brain task, \texttt{HealthGPT-M3} achieves an SSIM of 79.38, significantly surpassing traditional methods like Pix2Pix (71.09) and the recent DiffMa (71.47). 
% Although BBDM slightly outperformed \ourmethod{} in SSIM for the MRI2CT-Brain task, our approach showed better performance in pixel-level evaluations (such as PSNR and MSE), with pixels closer to the ground truth. 
\textbf{(ii) Super-Resolution:} We conduct 4× super-resolution experiments on the IXI dataset, with the results presented in Table ~\ref{tab:sr}. Notably, most existing methods fail to fully leverage the prior knowledge of key structures in medical images, resulting in significant shortcomings in detail recovery. 
In contrast, our method significantly mitigates this issue. Specifically, \texttt{HealthGPT-M3} excels in key metrics such as SSIM, PSNR, and ISE, achieving scores of 78.19, 32.76, and 34.47, respectively. 
% In terms of SSIM, our method surpasses the second-best model, LIIF, by 4.88 points, demonstrating a clear advantage in structural similarity. 
Additionally, \texttt{HealthGPT-M3} achieves the lowest score of 12.34, further validating its exceptional performance in human visual perception. \textbf{(iii) Reconstruction:} 
We compare \texttt{HealthGPT-M3} with unified models with reconstruction capabilities, such as Unified-IO 2 and SEED-X. The results show that our approach performs better controllability for visual reconstruction. We also train \texttt{HealthGPT-L14} with a similar number of trainable parameters to the \texttt{M3} version. Hence, the similar performance between the two models meets our expectations. Details are in the Appendix. 

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.95\linewidth]{fig/modality transfer case.jpg}
%     \caption{\textcolor{red}{[TODO: CAPTION]}}
%     \label{fig:3}
%     \vskip -0.1in
% \end{figure}
% \input{tab/SR}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.95\linewidth]{fig/SR case.jpg}
%     \caption{\textcolor{red}{[TODO: CAPTION]}}
%     \label{fig:5}
%     \vskip -0.1in
% \end{figure}


\subsection{In-Depth Study}

\noindent \textbf{Effect of Heterogeneous Low-Rank Adaptation.}
H-LoRA provides an optimized multi-LoRA architecture for multi-task learning. We conduct extensive validation of this structure, with results presented in Table 4, comparing the performance of LoRA, MoELoRA, and H-LoRA in medical unified comprehension and generation tasks. In the majority of comprehension tasks and all generation tasks, H-LoRA demonstrates superior performance, particularly in the OmniMedVQA benchmark, where it improved from 64.90 to 68.50. Notably, despite some applications of MoELoRA in certain scenarios, it do not show advantages in this task and had a training time approximately 50\% longer than LoRA. 
% Further ablation experiments (see Table 5) expanded the number of LoRA experts, revealing that when \( n = 8 \), the training time of MoELoRA was twice that of LoRA, while H-LoRA exhibited no additional training delay and achieved better performance. Extrapolating this trend to \( n = 32 \), the training time of MoELoRA even reached eight times that of LoRA, rendering it incapable of completing training and inference. 
Figure 5 illustrates the performance of the three PEFT methods in medical visual comprehension and generation tasks across different ranks, with H-LoRA consistently outperforming the other methods in all scenarios, demonstrating significant advantages in handling diverse tasks.

\noindent \textbf{Different Learning Strategy.} 
% We propose a three-stage learning strategy adapting for H-LoRA that facilitates the decoupled handling of comprehension and generation tasks. In contrast to methods that train the two tasks in a mixed manner, our strategy significantly reduces the performance degradation caused by conflicts between task types (see Table 6). In the medical visual comprehension task, mixed training leads to substantial catastrophic forgetting, and results in a decline in the visual reconstruction task as well. In contrast, our approach effectively leverages the medical embedding knowledge contained within pre-trained LLMs, successfully mitigating task conflicts. Additional experiments demonstrate that heterogeneous knowledge fusion aligns the H-LoRA plugs—which incorporates both comprehension and generation knowledge—with pre-trained LLMs, having negligible effects on the model's capabilities for unified tasks. Detailed results can be found in the Appendix.
We propose a three-stage learning strategy for H-LoRA that decouples comprehension and generation tasks. Unlike methods that train both tasks simultaneously, our approach reduces performance degradation from task conflicts (see Table 5). In the medical visual comprehension task, mixed training causes catastrophic forgetting and degrades visual reconstruction, whereas our strategy effectively uses the medical embedding knowledge in pre-trained LLMs to mitigate these conflicts. 
% Additional experiments show that heterogeneous knowledge fusion aligns the H-LoRA plugs—integrating both comprehension and generation knowledge—with pre-trained LLMs, with minimal impact on unified task performance. Detailed results are in the Appendix.
Meanwhile, we examine how fusing heterogeneous H-LoRA plugins in the second training stage results in minimal performance degradation. Detailed results are in the Appendix.


% We conduct an ablation analysis on visual perceptual inputs suitable for comprehension and generation tasks. Figure 6 illustrates that the convergence efficiency for comprehension tasks is significantly higher with abstract-grained visual inputs compared to concrete-grained inputs, whereas generation tasks perform better with concrete-grained inputs. This result further underscores the necessity of the hierarchical visual perception we propose, which suggests that customizing visual inputs at different hierarchies for specific tasks can substantially enhance efficiency.

\noindent \textbf{Hierarchical Visual Perception Analysis.} We conduct an ablation analysis on visual perceptual inputs for comprehension and generation tasks. Figure ~\ref{fig:perception} shows that comprehension tasks converge more efficiently with abstract-grained inputs, while generation tasks perform better with concrete-grained inputs. This highlights the importance of the hierarchical visual perception we propose, suggesting that tailoring visual inputs for specific tasks at different hierarchies can significantly improve efficiency.



% We invited five evaluators with professional backgrounds to conduct human assessments on five LVLMs, including \ourmethod{}. 
% The evaluation covered open visual question answering tasks, namely VQA-RAD, SLAKE, and PathVQA. During the evaluation, questions were randomly selected, and the model-generated responses were anonymized and ranked. The results, as shown in Figure 6, indicate that \ourmethod{} was frequently selected as the best answer. This suggests that \ourmethod{} has further application potential in medical care scenarios.

 
% We further explore the medical image-to-text generation task without reference images, using a small amount of CXR data for instruction fine-tuning. For clarity, we annotate the images with varying degrees and locations of injuries in Figure 7, comparing them with healthy CXR images. We observe that HealthGPT was able to effectively generate CXR images based on the given instructions, demonstrating its potential in healthcare education and training, as well as in auxiliary diagnosis.

\noindent \textbf{Report-to-CXR Task.} We further explore the medical image generation task without reference images, using a small amount of MIMIC-CXR data~\cite{johnson2019mimic} for instruction fine-tuning. Figure ~\ref{fig:CXR_Case} annotates images with varying injury degrees and locations, comparing them to healthy CXR images. We observe that \texttt{HealthGPT} effectively generates CXR images based on the instructions, showcasing its potential in healthcare education and auxiliary diagnosis.

