\section{Introduction}
\label{Introduction}
Large Vision-Language Models (LVLMs)~\cite{liu2023llava, gpt4v, llavanext, chen2024far} have demonstrated outstanding open-world visual comprehension and reasoning abilities through language-based interactive dialogue over the past years, simultaneously opening up new opportunities for applications in specialized domains. Specifically, recent studies~\cite{li2024llava-med,tu2024towards} have utilized pre-trained large language models (LLMs) and visual instruction data to build interactive diagnostic tools and treatment planning systems, revealing the immense potential of LVLMs in medical scenarios. 
However, these studies primarily concentrate on visual comprehension tasks that produce text-based outputs, such as medical visual question answering~\cite{li2024llava-med} or report generation~\cite{nath2024vila}, and deficient the ``drawing'' capability needed for medical visual generation. 
In practice, integrating visual comprehension and generation can significantly enhance the multifunctionality of medical LVLMs.

Recent studies have increasingly focused on developing unified LVLMs capable of comprehending and generating content across diverse visual modalities. Earlier approaches predominantly utilized continuous visual tokens fed into LLMs, using the LLMs themselves as conditional generators for external generative models~\cite{ge2024seed,wu2023next,dong2023dreamllm}. More recent research has explored the use of discrete visual tokens for image representation and generation within a fully autoregressive framework~\cite{team2024chameleon,wang2024emu3,xie2024show}. These methods not only enhance controllability but also demonstrate early success in open-world, any-to-any tasks, highlighting the preliminary potential of a unified autoregressive learning paradigm in multi-modal tasks. 

While unified LVLMs have achieved initial success in general scenarios, such a unified framework remains underexplored in the medical domain.
Adapting the aforementioned general unified model paradigm to the medical domain presents two major challenges:
\textbf{(\rmnum{1}) High-scale and -quality Data Limitations}. Open-world models necessitate extensive pre-training on billions or even more diverse, multi-modal data samples for comprehension and generation tasks~\cite{lu2024unified,team2024chameleon}. However, the accessible medical data significantly lacks in scale and quality compared to natural multi-modal datasets. Its specialized and domain-specific characteristics make it challenging to develop a unified medical model from scratch.
\textbf{(\rmnum{2}) Conflicts between Comprehension and Generation}. 
Comprehension tasks often strip away visual details to focus on abstraction, while generation tasks require detailed preservation, making tokens sensitive to all visual alterations.
As shown in Figure \ref{fig:conflict}, which features experiments conducted on medical images, the performance in comprehension (or generation) tasks steadily decreases as the proportion of generation (or comprehension) data increases, and vice versa. This highlights a dilemma in autoregressive multi-modal training, stemming from the need to maintain consistency between pre- and post-LVLMs. While some methods have explored mutual enhancement between comprehension and generation~\cite{pan2024auto,tong2024metamorph}, improvements still exhibit diminishing returns, with performance degradation remaining a significant issue.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.92\linewidth]{fig/task_conflict.pdf}
    \caption{With a fixed amount of comprehension (generation) data, increasing the proportion of the other type leads to significant performance degradation.}
    \label{fig:conflict}
\end{figure}

To tackle the aforementioned challenges, we propose \ourmethod{} (see Figure \ref{fig:1})
% (refer to Figure \ref{fig:2})
, which progressively adapts a pre-trained LLM as an unified medical multi-modal model with a small amount of visual instruction data. 
We devise innovative Parameter-Efficient Fine-Tuning (PEFT) approach~\cite{ding2023parameter}, called \textbf{Heterogeneous Low-Rank Adaptation (H-LoRA)}, which decouples the learning process of LVLMs for comprehension and generation tasks. Inspired by the plug-and-play nature of LoRA~\cite{hu2021lora}, H-LoRA enables the model to store heterogeneous comprehension and generation knowledge
in independent ``plugins", thus avoiding joint optimization issues caused by conflicts between
comprehension and generation tasks. 
In addition, we also consider the variety of sub-tasks among comprehension or generation tasks.  Qualitative research highlights the limitations of a single LoRA in handling multidimensional task scenarios, mainly due to catastrophic forgetting and interference~\cite{liu2024moe,lin2024teamlora}. To address this, we draw on the concept of Mixture of Experts (MoE)~\cite{masoudnia2014mixture} and introduce LoRA experts. The aim is to dynamically transfer task-shared knowledge to adapt to downstream tasks.
Unlike MoELoRA~\cite{luo2024moelora}, H-LoRA
employs reversible matrix block multiplication to combine LoRA experts, significantly reducing the overhead of multiple matrix multiplications.
\textbf{Notably, when using four experts, it requires only 67\% of the MoELoRA training time.}



To effectively leverage H-LoRA in \ourmethod{}, we further introduce a  \textbf{Hierarchical Visual Perception} (HVP) and devise a corresponding \textbf{Three-stage Learning Strategy} (TLS). \textbf{HVP}: we separate visual details learning from Vision transformer (ViT) for comprehension and generation. As is widely recognized, the ViT encodes visual concepts with increasing abstraction, generally, becoming finer as we progress over levels~\cite{vig2019multiscale}. 
% Thus, we maintain the visual features of anterior and posterior layers that align with the distinct attributes of comprehension and generation, thereby preventing potential task interference. 
Thus, we maintain the visual features of the anterior and posterior layers to accommodate the differing requirements for visual granularity in comprehension and generation tasks while preventing potential task interference.
\textbf{TLS}:
In the first and second stages, given the heterogeneity between comprehension and generation tasks, we first train H-LoRA plugins for \ourmethod{} to incorporate both medical comprehension and generation knowledge, thus endowing the LLMs with capabilities for vision-language alignment and vision-to-vision reconstruction.
Additionally, through minimal mixed-task training, we built fusion embedding layers and output heads that merge text and visual tokens, establishing a unified LVLM foundation for visual instruction fine-tuning.
In the third stage, by only training the H-LoRA plugins, \ourmethod{} is able to rapidly adapt to a wide range of downstream medical tasks, covering various types of medical comprehension and generation tasks.


To effectively implement our approach, we have curated a dataset for training unified medical LVLMs, called \texttt{VL-Health}, including seven comprehension tasks and five generation tasks (Figure~\ref{fig:1}).
% ooibc: this is when Fig 1 is used but no reference so far!
% wq: revised
%
% which includes both comprehension data selection and generation data construction.
Through quantitative analysis and validation on multi-modal tasks, the results demonstrate that \ourmethod{} is capable of unifying medical multi-modal abilities in data-constrained scenarios, achieving performance comparable to or better than existing state-of-the-art (SOTA) models across multiple metrics. Overall, the main contributions of this paper are summarized as follows:
\begin{itemize}
    \item \textbf{Unified Med-LVLM.} We introduce \ourmethod{}, which, to the best of our knowledge, is the first unified framework for multi-modal comprehension and generation in complex medical scenarios.
    \item \textbf{Effective Learning Paradigm.} We present H-LoRA, an optimized multi-LoRA PEFT 
    %%% ooibc: you are assuming that everyone knows what PEFT is
    %% parameter efficient free training is quite specific
    %
     %%% wq: thanks, just mentioned peft above
    architecture based on task-gated decoupling, is designed to effectively mitigate data conflict issues.
    \item \textbf{Holistic Training Dataset.} We curated \texttt{VL-Health}, a comprehensive dataset designed for both comprehension and generation tasks.
    \item  \textbf{Superior Downstream Improvements}: Extensive experiments are conducted and the results confirm
 %highlight
 \ourmethod{}'s effectiveness
in medical vision-language comprehension and generation.
\end{itemize}