\section{Related Work}
\label{Related Work}
\noindent\textbf{Medical Vision Large Language Models.} Recently, medical vision large language models (Med-VLLMs) have made significant progress, demonstrating excellent performance in understanding medical images and responding to human queries based on these images~\cite{zhou2023survey,tian2023role}. XrayGPT~\cite{thawkar2023xraygpt} combines a medical visual encoder (MedClip)~\cite{wang2022medclip} with a fine-tuned LLM , using a simple linear transformation layer to achieve alignment between visual and textual information, significantly enhancing the understanding of medical images. 
On this basis, LLaVA-Med~\cite{li2024llava} further enhances visual-text alignment in medical contexts by selecting high-quality image-text pairs from PubMed papers and synthesized VQA datasets. BiomedGPT~\cite{luo2024biomedgpt} employs a BERT-style encoder and GPT-style decoder architecture, pre-trained on interdisciplinary datasets. Compared to commercial models like Med-PaLM~\cite{singhal2023large}, BiomedGPT significantly reduces model size while maintaining superior performance.
However, issues of language adaptability and dataset specificity still remain. To address these, HuatuoGPT-Vision~\cite{chen2024huatuogpt} introduces the PubMedVision dataset, which contains 1.3 million high-quality medical samples, significantly improving the model's adaptability across diverse medical applications. However, current Med-VLLMs mainly focus on medical comprehension and lack the capability for the  medical vision-language generation. 

\noindent\textbf{Unified Visual Comprehension and Generation Models.}
Recent research has increasingly concentrated on creating unified LVLMs that are adept at understanding and producing content across various visual modalities.
NExT-GPT~\cite{wu2023next} achieves perception and generation for arbitrary combinations of multi-modal inputs and outputs by aligning LLMs. Similarly, SEED~\cite{ge2023planting}, SEED-X~\cite{ge2024seed}, and DreamLLM~\cite{dong2023dreamllm} employ learnable queries and leverage next-token prediction to generate visual tokens, providing conditional inputs to external generation modules. Unlike these methods, which function as external conditioners, Unified-IO~\cite{lu2022unified}, Unified-IO 2~\cite{lu2024unified}, and Chameleon~\cite{team2024chameleon} internalize multi-modal generation tasks within a unified Transformer architecture by extending multi-modal vocabularies, enabling direct generation based on next-token prediction. Building on this concept, Lumina-mGPT~\cite{liu2024lumina} and ANOLE~\cite{chern2024anole} further enhance the generation capabilities of unified models using high-quality data, particularly improving the quality and flexibility of image generation. 