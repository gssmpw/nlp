\begin{abstract}
We present \ourmethod{}, a powerful Medical Large Vision-Language Model (Med-LVLM)
that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. 
Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception approach and a three-stage learning strategy.
To effectively learn the \ourmethod{}, we devise a comprehensive medical domain-specific comprehension and generation dataset called \texttt{VL-Health}. 
Experimental results demonstrate exceptional performance and scalability 
of \ourmethod{} in medical visual unified tasks. 
Our project can be accessed at \url{https://github.com/DCDmllm/HealthGPT}.

\end{abstract}