\section{Related work}
\subsection{Image-to-image generation methods}
Image-to-image (I2I) translation has emerged as a pivotal technique for cross-domain visual representation learning, particularly in cartographic applications where remote sensing imagery requires transformation into structured map formats. This conversion process faces a fundamental challenge: the significant domain gap between raw sensor data and stylized map elements. To bridge this domain discrepancy, modern frameworks employ specialized architectures that explicitly model cross-domain mapping relationships. These architectures typically incorporate multi-scale feature extractors and domain-specific constraints to enhance output fidelity \cite{Pang2021ImagetoImageTranslationMethods}. The objective of these frameworks goes beyond replicating the source imagery at a pixel level; it also involves capturing latent semantic representations and modeling complex feature distributions. Generally, generative models aim to synthesize artificial data instances that approximate the intrinsic probability distribution of authentic samples, based on data conforming to specific distributional constructs \cite{A.Oussidi2018Deepgenerativemodels}.

The initial generative model is derived from variational autoencoder (VAE), introduced by Kingma \cite{Kingma2013Autoencodingvariationalbayes}, pioneered probabilistic modeling for data generation. However, due to limitations in synthesizing high-resolution outputs and inherent instability during adversarial training, VAEs were superseded by GAN-based (Generative Adversarial Networks) architectures \cite{Goodfellow2014GenerativeAdversarialNets}. Current techniques ormulated using Conditional Generative Adversarial Networks (cGANs) \cite{Wang2018HighResolutionImageSynthesis}, such as Pix2Pix \cite{Isola2017ImagetoImageTranslationConditional} and Pix2PixHD \cite{Wang2018HighResolutionImageSynthesis},  achieved supervised cross-domain mapping by incorporating input-output constraints. It effectively visualized semantic labels, reconstructed edges into objects, and synthesized colors. Notably, StarGAN \cite{Choi2018StarGANUnifiedGenerative} provided a paradigmatic solution for multi-domain translation through a unified architecture design, while CycleGAN \cite{Zhu2017UnpairedImagetoImageTranslation} pioneered unpaired translation methodologies, greatly expanding the technological applicability in diverse cartographic scenarios.

Drawing inspiration from thermodynamics, diffusion models employ iterative noise introduction to capture and learn patterns of informational decay caused by noise. In turn, this enables the generation of images based on the learned dynamics \cite{Sohl-Dickstein2015DeepUnsupervisedLearning}. These models have demonstrated exceptional performance in generating high-resolution images, with the Denoising Diffusion Probabilistic Model (DDPM) \cite{Ho2020DenoisingDiffusionProbabilistic} being a prominent representative in the field. DDPM operates by reversing the diffusion process: in the forward process, noise is progressively added to the original data until it transforms into Gaussian noise. The model is trained to reverse this process step-by-step, reconstructing data from noise. Building on this foundation, Nichol et al. \cite{Nichol2021ImprovedDenoisingDiffusion} introduced a learning variance strategy within DDPM, significantly improving the quality of synthesized images while enhancing the speed of the sampling process. To expedite sampling further, Song et al. proposed the Denoising Diffusion Implicit Model (DDIM) \cite{Song2022DenoisingDiffusionImplicit}, which reformulated the diffusion process as non-Markovian. DDIM, without altering DDPM's training methodology, significantly boosted sampling speed while maintaining nearly identical image generation quality. Furthermore, DDIM established a deterministic generation process, ensuring that the resulting images were solely influenced by the initial noise used in the reverse process.

The diffusion models are primarily unconditional generation models, producing images through random sampling without relying on external controls. On the other hand, conditional diffusion models generate images by incorporating additional control signals. Such models allow for the integration of diverse input conditions, spanning classification-to-image generation \cite{Ho2022CascadedDiffusionModels}, image-to-image synthesis \cite{Saharia2022PaletteImagetoImageDiffusion,Saharia2022ImageSuperResolutionIterative,Zhang2023AddingConditionalControl}, text-to-image generation \cite{Rombach2022HighResolutionImageSynthesis,Saharia2022PhotorealisticTexttoImageDiffusion,Wu2024TaiyiDiffusionXLAdvancingBilingual}, and image editing \cite{Avrahami2022BlendedDiffusionTextDriven,Nie2023BlessingRandomnessSDE,Shi2023DragDiffusionHarnessingDiffusion,Hertz2022PrompttoPromptImageEditing}, among others. However, as DDPM's reverse process operates directly in pixel space, predicting high-dimensional noise demands substantial computational resources and memory, posing challenges for high-resolution image generation. Two principal strategies have been devised to surmount this limitation: cascade generation \cite{Saharia2022PhotorealisticTexttoImageDiffusion,Ho2022CascadedDiffusionModels}, which begins with the generation of a low-resolution image and progressively synthesizes higher-resolution images by treating the preceding stages as conditional inputs; and latent space generation \cite{Rombach2022HighResolutionImageSynthesis}, which compresses the image into a lower-dimensional latent space using an encoder-decoder framework and performs the denoising diffusion process within this latent space. Both approaches have proven effective in optimizing the efficiency of high-resolution image synthesis using diffusion models.

Consequently, these approaches frequently fail to adequately extract and represent the intricate features of objects in remote sensing imagery, resulting in maps of subpar quality compared to those produced by methods specifically tailored for this domain.

\subsection{Generation maps from remote sensing image}
The generation of maps from remote sensing imagery has garnered significant attention in recent years, particularly with the advent of deep learning techniques. With the rapid advancement of remote sensing technology, capturing high-resolution remote sensing imagery has become increasingly effortless and efficient. This progress paves the way for adopting intelligent interpretation techniques based on deep learning, which inherently depend on high-quality data for optimal training. The primary tasks associated with the intelligent analysis of remote sensing imagery include semantic segmentation, feature object extraction, target detection, and scene classification. Remote sensing images are composed of many geographical elements, often intricate and diminutive, presenting notable challenges for their semantic segmentation. Existing methods for semantic segmentation in this domain have primarily focused on addressing these difficulties, particularly in the accurate segmentation of geographic elements within remote sensing imagery \cite{Wang2023UPerNetConvNeXtSemantic,Ma2024MultilevelMultimodalFusion,Zhang2024RSAMSegSAMbasedApproach,Toker2024SatSynthAugmentingImageMask,Zhao2023CDDFuseCorrelationDrivenDualBranch,Peng2023DiffusionbasedImageTranslation,Lu2023ScoreSegLeveragingScorebased,Ayala2023Diffusionmodelsremote,Chen2021SelfAttentionReconstructionBias,Cheng2022MaskedAttentionMaskTransformer,Shen2022Highqualitysegmentation,Jiang2020RWSNetsemanticsegmentation}. In terms of the pixel-level classification of land use and land cover within remote sensing imagery, significant attention has been devoted to challenges associated with the classification of large surface areas and heterogeneous land cover types \cite{Yao2023ExtendedVisionTransformer,Wang2023DSViTDynamicallyScalable,Chen2023SparseViTRevisitingActivation,Li2024LearningHolisticallyDetect,Zhao2024RSmambalargeremote,Guo2022ISDNetIntegratingShallow,Shen2022Highqualitysegmentation,Xi2021WideSlidingWindow}. Similarly, research aimed at feature object extraction has explored methods to detect specific elements, such as roads, by leveraging their geometric and topological characteristics for improved precision in extraction \cite{Luc2016SemanticSegmentationusing,Chen2021ReconstructionBiasUNet}. For target detection, existing approaches tailored for remote sensing imagery have primarily concentrated on overcoming the challenges posed by detecting small, densely clustered objects, irrespective of their orientation or rotation \cite{Yang2019SCRDetMoreRobust,Ding2019LearningRoITransformer,Yin2020AirportDetectionBased,Li2021SeMoYOLOMultiscaleObject,Han2021ReDetRotationEquivariantDetector,Xia2024ViTCoMerVisionTransformer}. Meanwhile, scene classification has increasingly integrated multimodal technologies, combining visual data from remote sensing imagery with geographical textual information. This synergy has given rise to foundational image-text models within the remote sensing field \cite{Muhtar2024LHRSBotEmpoweringRemote,Zhang2024EarthGPTUniversalMultimodal,Kuckreja2024GeoChatGroundedLarge,Guo2024SkySenseMultiModalRemote,Mall2023RemoteSensingVisionLanguage}, establishing a robust framework for a wide range of downstream applications. However, the task of generating high-quality maps from remote sensing imagery remains a complex challenge due to the inherent differences between the two domains.

Some researchers have using generate techniques to generate maps from remote sensing imagery: facilitate transitions from remote sensing imagery to map tiles through the adversarial training of generators and discriminators \cite{Isola2017ImagetoImageTranslationConditional}. However, these approaches predominantly emphasize the intrinsic features of images while overlooking the relevance and complementarity of geographical features within remote sensing images, resulting in maps that inadequately represent complex geographical features. Various GAN-based map generation methods have improved map generation to some extent. For instance, GeoGAN \cite{Ganguli2019GeoGANConditionalGAN} employs a Conditional Generative Adversarial Network (cGAN) \cite{Wang2018HighResolutionImageSynthesis} to translate remote sensing imagery into map representations, while SMAPGAN \cite{Chen2021SMAPGANgenerativeadversarial} leverages incompletely paired remote sensing and internet map sample data to produce maps using semi-supervised generative, although it does not fully overcome the issue of map quality. Additionally, CreativeGAN \cite{Fu2021TranslationAerialImage} incorporates semantic information to improve the accuracy of generated maps. These GAN-based methodologies refine the outputs through adversarial training dynamics between generators and discriminators, where discriminators assess image authenticity and penalize deviations from the target appearance. However, these methods predominantly focus on single-level map generation and fail to accommodate the intricate requirements of multi-scale map generation.

In emergency mapping, there is a necessity for small-scale maps to provide an overview with general geographic information, alongside large-scale maps to precisely depict local geographic changes. To tackle the specific challenges associated with multi-scale map generation, Chen et al. \cite{Chen2022GeneratingMultiscaleMaps} proposed a straightforward training approach for producing hierarchical maps. At the highest level (\(k\)-th level, where \(k\) denotes the maximum scale), this strategy generates maps directly from remote sensing imagery, whereas at lower levels, the mappings between the \(k\)-th and \((k-1)\)-th levels are learned through independently trained models. However, this approach exhibits notable limitations: the training process is complex, as each level requires a distinct model, and the accuracy of the highest-level map constrains the quality of lower-level maps. This results in insufficient utilization of the features within remote sensing imagery. In contrast, CscGAN \cite{Liu2021CscGANconditionalscaleconsistent} developed a multi-scale map generation network capable of producing multi-level maps using a single model. This approach introduces a map-level classifier, inspired by StarGAN \cite{Choi2018StarGANUnifiedGenerative}, as an extension to the discriminator. It utilizes preprocessed road morphology data as input. Nevertheless, this method struggles to generate consistent multi-scale maps solely from remote sensing imagery, as its multi-level maps pertain to disparate regions rather than representing varying scales of the same area. This limitation narrows its applicability for multi-scale map generation within a cohesive geographic region.

Building upon these advancements, Fu et al. \cite{Fu2023Levelawareconsistentmultilevel} currently represent the state-of-the-art through their level-aware generation framework. They proposed a multi-level map generation network that employs a level classifier to distinguish and generate map tiles across different levels. Their network introduces a map element extractor to capture significant geographic features from satellite imagery and incorporates a multi-level fusion generator to produce seamless multi-level maps from initial lower-level map inputs. However, their architecture relies on independently generated map tiles, which, when assembled, often exhibit visual discontinuities that detract from the overall readability and aesthetic coherence of the resulting map.

In conclusion, existing methodologies for map generation often focus solely on single-level maps or fail to meet the requirements for consistent multi-scale map production. These approaches frequently overlook the complex relationships between map scales and geographic features, resulting in suboptimal outcomes. Furthermore, many methods rely on independent generation processes for each map tile, which leads to visual discontinuities and a lack of coherence across the generated maps. These limitations impede the practical application of such methods in real-world scenarios, particularly in emergency mapping and large-format cartographic applications. To address these limitations, an end-to-end generative approach for cartography has been proposed, specifically designed to produce high-quality, seamless, and wide-area multi-scale tile maps from remote sensing imagery. This method effectively tackles the challenges of multi-scale tile map generation, ensuring improved consistency and usability for regional map production.

\begin{figure*}[htp]
	\centering
	\includegraphics[width=1.0\linewidth]{method-framework.pdf}
	\caption{
		The overall structure of C2GM encapsulates a hierarchical, self-cascading design aimed at producing multi-scale, large-area tile map for specified spatial scenarios. The generation process is divided into multiple stages, starting with lower-level maps and gradually transitioning to higher-level ones. In each stage, the generation process depends on the lower-level tile previously produced and its scale information. This multi-stage generation strategy not only enhances the quality of map tile generation but also ensures consistency and coherence across different tiles by learning the disparities between various map scales and integrating cartography expertise.
	}
	\label{fig:method-framework}
\end{figure*}