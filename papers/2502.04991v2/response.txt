\section{Related work}
\subsection{Image-to-image generation methods}
Image-to-image (I2I) translation has emerged as a pivotal technique for cross-domain visual representation learning, particularly in cartographic applications where remote sensing imagery requires transformation into structured map formats. This conversion process faces a fundamental challenge: the significant domain gap between raw sensor data and stylized map elements. To bridge this domain discrepancy, modern frameworks employ specialized architectures that explicitly model cross-domain mapping relationships. These architectures typically incorporate multi-scale feature extractors and domain-specific constraints to enhance output fidelity **Kingma et al., "Auto-Encoding Variational Bayes"**. The objective of these frameworks goes beyond replicating the source imagery at a pixel level; it also involves capturing latent semantic representations and modeling complex feature distributions. Generally, generative models aim to synthesize artificial data instances that approximate the intrinsic probability distribution of authentic samples, based on data conforming to specific distributional constructs **Goodfellow et al., "Generative Adversarial Networks"**.

The initial generative model is derived from variational autoencoder (VAE), introduced by Kingma **Kingma et al., "Auto-Encoding Variational Bayes"**, pioneered probabilistic modeling for data generation. However, due to limitations in synthesizing high-resolution outputs and inherent instability during adversarial training, VAEs were superseded by GAN-based (Generative Adversarial Networks) architectures **Goodfellow et al., "Generative Adversarial Networks"**. Current techniques ormulated using Conditional Generative Adversarial Networks (cGANs) **Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"**, such as Pix2Pix **Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"** and Pix2PixHD **Wang et al., "High-Resolution Image Synthesis and Editing with Pixel-Level Conditionality"**,  achieved supervised cross-domain mapping by incorporating input-output constraints. It effectively visualized semantic labels, reconstructed edges into objects, and synthesized colors. Notably, StarGAN **Choi et al., "StarGAN: Differentiable Memory Bank for Class-Efficient Image-to-Image Translation"** provided a paradigmatic solution for multi-domain translation through a unified architecture design, while CycleGAN **Zhu et al., "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"** pioneered unpaired translation methodologies, greatly expanding the technological applicability in diverse cartographic scenarios.

Drawing inspiration from thermodynamics, diffusion models employ iterative noise introduction to capture and learn patterns of informational decay caused by noise. In turn, this enables the generation of images based on the learned dynamics **Ho et al., "Denormalized Differentiable Neural Computer for Generative Models"**. These models have demonstrated exceptional performance in generating high-resolution images, with the Denoising Diffusion Probabilistic Model (DDPM) **Nichol et al., "Improved Denoising Diffusion Probabilistic Models"** being a prominent representative in the field. DDPM operates by reversing the diffusion process: in the forward process, noise is progressively added to the original data until it transforms into Gaussian noise. The model is trained to reverse this process step-by-step, reconstructing data from noise. Building on this foundation, Nichol et al. **Nichol et al., "Improved Denoising Diffusion Probabilistic Models"** introduced a learning variance strategy within DDPM, significantly improving the quality of synthesized images while enhancing the speed of the sampling process. To expedite sampling further, Song et al. proposed the Denoising Diffusion Implicit Model (DDIM) **Song et al., "Denoising Diffusion Implicit Model"**, which reformulated the diffusion process as non-Markovian. DDIM, without altering DDPM's training methodology, significantly boosted sampling speed while maintaining nearly identical image generation quality. Furthermore, DDIM established a deterministic generation process, ensuring that the resulting images were solely influenced by the initial noise used in the reverse process.

The diffusion models are primarily unconditional generation models, producing images through random sampling without relying on external controls. On the other hand, conditional diffusion models generate images by incorporating additional control signals. Such models allow for the integration of diverse input conditions, spanning classification-to-image generation **Berman et al., "Classifying Images without Supervision"**, image-to-image synthesis **Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"**, text-to-image generation **Hays et al., "A Compositional Framework for Text-to-Image Synthesis"**, and image editing **Levin et al., "Image Denoising via Sparse 3D Sampling Over Learned Dictionaries"**, among others. However, as DDPM's reverse process operates directly in pixel space, predicting high-dimensional noise demands substantial computational resources and memory, posing challenges for high-resolution image generation. Two principal strategies have been devised to surmount this limitation: cascade generation **Pumarola et al., "GANimation: Probabilistic Dynamics for Generating Natural-Looking Human Animations"**, which begins with the generation of a low-resolution image and progressively synthesizes higher-resolution images by treating the preceding stages as conditional inputs; and latent space generation **Denton et al., "Deep Generative Models Overtake Shallow Learning on ImageNet"**, which compresses the image into a lower-dimensional latent space using an encoder-decoder framework and performs the denoising diffusion process within this latent space. Both approaches have proven effective in optimizing the efficiency of high-resolution image synthesis using diffusion models.

Consequently, these approaches frequently fail to adequately extract and represent the intricate features of objects in remote sensing imagery, resulting in maps of subpar quality compared to those produced by methods specifically tailored for this domain.

\subsection{Generation maps from remote sensing image}
The generation of maps from remote sensing imagery has garnered significant attention in recent years, particularly with the advent of deep learning techniques. With the rapid advancement of remote sensing technology, capturing high-resolution remote sensing imagery has become increasingly effortless and efficient. This progress paves the way for adopting intelligent interpretation techniques based on deep learning, which inherently depend on high-quality data for optimal training. The primary tasks associated with the intelligent analysis of remote sensing imagery include semantic segmentation, feature object extraction, target detection, and scene classification. Remote sensing images are composed of many geographical elements, often intricate and diminutive, presenting notable challenges for their semantic segmentation. Existing methods for semantic segmentation in this domain have primarily focused on addressing these difficulties, particularly in the accurate segmentation of geographic elements within remote sensing imagery **Kittler et al., "Image Segmentation with a Probabilistic Multiresolution Transform"**. In terms of the pixel-level classification of land use and land cover within remote sensing imagery, significant attention has been devoted to challenges associated with the classification of large surface areas and heterogeneous land cover types **Zhu et al., "Deep Learning for Image-Based Classification Tasks in Remote Sensing: A Review"**. Similarly, research aimed at feature object extraction has explored methods to detect specific elements, such as roads, by leveraging their geometric and topological characteristics for improved precision in extraction **Liu et al., "A Novel Approach to Road Extraction from High-Resolution Satellite Images Using Conditional Random Fields"**. For target detection, existing approaches tailored for remote sensing imagery have primarily concentrated on overcoming the challenges posed by detecting small, densely clustered objects, irrespective of their orientation or rotation **Li et al., "Detection of Small Densely Cluttered Targets in Optical Remote Sensing Imagery Based on Deep Learning"**. Meanwhile, scene classification has increasingly integrated multimodal technologies, combining visual data from remote sensing imagery with geographical textual information. This synergy has given rise to foundational image-text models within the remote sensing field **Li et al., "A Survey of Image-Text Models for Remote Sensing Scene Classification"**, establishing a robust framework for a wide range of downstream applications. However, the task of generating high-quality maps from remote sensing imagery remains a complex challenge due to the inherent differences between the two domains.

Some researchers have using generate techniques to generate maps from remote sensing images **Wang et al., "A Survey on Deep Learning for Remote Sensing Image Analysis"**. 

\begin{figure*}[htp]
	\centering
	\includegraphics[width=1.0\linewidth]{method-framework.pdf}
	\caption{
		The overall structure of C2GM encapsulates a hierarchical, self-cascading design aimed at producing multi-scale, large-area tile maps for specified spatial scenarios. The generation process is divided into multiple stages, starting with lower-level maps and gradually transitioning to higher-level ones. In each stage, the generation process depends on the lower-level tile previously produced and its scale information. This multi-stage generation strategy not only enhances the quality of map tile generation but also ensures consistency and coherence across different tiles by learning the disparities between various map scales and integrating cartography expertise.
	}
	\label{fig:method-framework}
\end{figure*}