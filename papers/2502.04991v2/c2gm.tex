%%
%% Copyright 2019-2021 Elsevier Ltd
%%
%% This file is part of the 'CAS Bundle'.
%% --------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'CAS Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for cas-dc documentclass for
%% double column output.

\documentclass[a4paper,fleqn]{cas-dc}

% If the frontmatter runs over more than one page
% use the longmktitle option.

%\documentclass[a4paper,fleqn,longmktitle]{cas-dc}

\usepackage[numbers]{natbib}
%\usepackage[authoryear]{natbib}
%\usepackage[authoryear,longnamesfirst]{natbib}

%%%Author macros
\def\tsc#1{\csdef{#1}{\textsc{\lowercase{#1}}\xspace}}
\tsc{WGM}
\tsc{QE}
%%%

% Uncomment and use as if needed
%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}[theorem]{Lemma}
%\newdefinition{rmk}{Remark}
%\newproof{pf}{Proof}
%\newproof{pot}{Proof of Theorem \ref{thm}}

\begin{document}
\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}
\let\printorcid\relax % 可去掉页面下方的ORCID(s)

% Short title
% \shorttitle{<short title of the paper for running head>}
\shorttitle{C2GM: Cascading conditional generative cartography framework}

% Short author
% \shortauthors{<short author list for running head>}
\shortauthors{Chenxing Sun et al.}

% Main title of the paper
\title[mode = title]{C2GM: Cascading conditional generative cartography framework for multi-scale tile map generation with geographic feature constraints}

% Title footnote mark
% eg: \tnotemark[1]
% \tnotemark[<tnote number>]
% \tnotemark[1]

% Title footnote 1.
% eg: \tnotetext[1]{Title footnote text}
% \tnotetext[<tnote number>]{<tnote text>}
% \tnotetext[1]{This study was supported in part by the National Natural Science Foundation of China under Grant 42471475; the Key Projects of Foundation Improvement Program; the Opening Fund of Key Laboratory of Geological Survey and Evaluation of Ministry of Education (Grant No. GLAB 2024ZR06) and the Fundamental Research Funds for the Central Universities.}

% First author
%
% Options: Use if required
% eg: \author[1,3]{Author Name}[type=editor,
%       style=chinese,
%       auid=000,
%       bioid=1,
%       prefix=Sir,
%       orcid=0000-0000-0000-0000,
%       facebook=<facebook id>,
%       twitter=<twitter id>,
%       linkedin=<linkedin id>,
%       gplus=<gplus id>]

% \author[<aff no>]{<author name>}[<options>]

% Corresponding author indication
% \cormark[<corr mark no>]

% Footnote of the first author
% \fnmark[<footnote mark no>]

% Email id of the first author
% \ead{<email address>}

% URL of the first author
% \ead[url]{<URL>}

% Credit authorship
% eg: \credit{Conceptualization of this study, Methodology, Software}
% \credit{<Credit authorship details>}

% Address/affiliation
% \affiliation[<aff no>]{organization={},
%             addressline={},
%             city={},
% %          citysep={}, % Uncomment if no comma needed between city and postcode
%             postcode={},
%             state={},
%             country={}}

% \author[<aff no>]{<author name>}[<options>]

% Footnote of the second author
% \fnmark[2]

% Email id of the second author
% \ead{}

% URL of the second author
% \ead[url]{}

% Credit authorship
% \credit{}

% Address/affiliation
% \affiliation[<aff no>]{organization={},
%             addressline={},
%             city={},
% %          citysep={}, % Uncomment if no comma needed between city and postcode
%             postcode={},
%             state={},
%             country={}}

% Corresponding author text
% \cortext[1]{Corresponding author}

% Footnote text
% \fntext[1]{}

% For a title note without a number/mark
%\nonumnote{}

\author[1]{Chenxing Sun}[style=chinese, orcid=0000-0001-7108-2988]
\credit{Conceptualization of this study, Methodology, Software}

\author[2]{Yongyang Xu}[style=chinese, orcid=0000-0001-7421-4915]

\author[3]{Xuwei Xu}[style=chinese]

\author[2]{Jing Bai}[style=chinese]

\author[2]{Xixi Fan}[style=chinese]

\author[4,5]{Xiechun Lu}[style=chinese, orcid=0000-0001-8450-2195]

\author[1,2,6]{Zhanlong Chen}[style=chinese, orcid=0000-0001-6373-3162]
\cormark[1]
\ead{chenzl@cug.edu.cn}

\address[1]{Key Laboratory of Geological Survey and Evaluation of Ministry of Education, China University of Geosciences, Wuhan 430074, China}
\address[2]{School of Computer Science, China University of Geosciences, Wuhan 430074, China}
\address[3]{School of Geographical and Information Engineering, China University of Geosciences, Wuhan 430074, China}
\address[4]{College Of Computer and lnformation Technology, China Three Gorges University, Yichang 443002, China}
\address[5]{Hubei Key Laboratory of Intelligent Vision Based Monitoring for Hydroelectric Engineering, China Three Gorges University, Yichang 443002, China}
\address[6]{Engineering Research Center of Natural Resource Information Management and Digital Twin Engineering Software, Ministry of Education, Wuhan 430074, China}

\cortext[1]{Corresponding author}

% Here goes the abstract
\begin{abstract}
    Multi-scale tile maps are essential for geographic information services, serving as fundamental components of surveying and cartographic workflows. Although existing image generation networks can efficiently produce map tiles from remote sensing images, they tend to emphasize texture features over geospatial characteristics and scale properties. Consequently, these methods are less effective in extracting vital geographic semantic information for cartography. They also fail to generate continuous multi-scale tile maps in large-format cartographic applications. This paper introduces C2GM, a novel framework for generating multi-scale tile maps through conditional guided diffusion and multi-scale cascade generation. Specifically, we develop a scale-driven conditional feature fusion encoder to extract object priors from remote sensing images and implement cascade reference double branch input, ensuring accurate representation of complex features. The generated low-level tiles serve as constraints for high-level map generation, enhancing visual continuity. Additionally, we incorporate map scale modality information using CLIP to simulate the relationship between map scale and cartographic generalization in tile maps. Extensive experimental evaluations show that C2GM consistently achieves state-of-the-art performance across all metrics, facilitating the rapid and effective generation of multi-scale large-format tile maps for emergency response and remote mapping applications. The source code, pre-trained models, and all the experimental results will be publicly available at https://github.com/Magician-MO/C2GM upon the paper's acceptance of this work.
\end{abstract}

% Use if graphical abstract is present
%\begin{graphicalabstract}
%\includegraphics{}
%\end{graphicalabstract}


% Research highlights
% \begin{highlights}
% \item Propose an innovative framework for generating multi-scale tile maps from RS imagery by utilizing conditional diffusion models alongside multi-scale cascade generation principles.
% \item Scale-modality encoding mechanism dynamically achieves data-driven embedding of multi-scale cartographic knowledge priors.
% \item The cascaded generation strategy employs low-level map tiles as spatial constraints to eliminate edge discontinuities while preserving semantic coherence across scales.
% \item Dual-branch scale-adaptation network guarantees cartographic fidelity across various scenarios by aligning cartographic generalization with geographic semantics.
% \end{highlights}

% Keywords
% Each keyword is seperated by \sep
\begin{keywords}
    Generative Cartography \sep
    Cascading Generation \sep
    Multi-Scale Tile Map \sep
    Diffusion Model
\end{keywords}

\maketitle

% Main text

\section{Introduction}
Multi-scale tile maps, valued for their accessibility and readability, are integral to modern society, with extensive applications in emergency response, disaster management, and urban planning \cite{Skidmore1997Useremotesensing,Ezequiel2014UAVaerialimaging}. The proliferation of online map services employing multi-scale tile maps, such as Google Maps, Bing Maps, and OpenStreetMap, has made it easy for individuals to access and utilize these resources. Although traditional mapping methods generally depend on field surveys, modern providers like Google and Microsoft now frequently employ remote sensing and mapping vehicles for data collection. However, these cartographic techniques still heavily rely on the manual acquisition of vector data, cartographic generalization and manual selection, hindering the delivery of real-time map services \cite{Haunold1993keystrokelevelanalysis,Park2011Hybridlinesimplification}. In practical scenarios, such as mapping remote areas or conducting emergency rescue operations, the rapid generation of tiled maps and real-time updates of map services is particularly crucial. Advancements in remote-sensing technology now allow satellites, drones, and aircraft to capture high-resolution images rich in geographical information rapidly. This development makes it feasible to create multi-scale map tiles from real-time remote sensing data (Fig.\ref{fig:introduce-idea} (b)), offering a more efficient alternative to traditional cartographic workflows\cite{Shen2022Highqualitysegmentation,Liu2022RAANetresidualASPP,Zhao2024RSmambalargeremote}. Such an approach significantly accelerates map production and ensures timely updates, particularly in inaccessible or hazardous areas for ground-based surveys.

\begin{figure}[b]
	\begin{center}
		\includegraphics[width=1.0\linewidth]{introduce-idea.pdf}
	\end{center}
    \caption{The generation of multi-scale maps from remote-sensing images can leverage scale information and tile segmentation in multi-scale tile map samples.}
	\label{fig:introduce-idea}
\end{figure}

While previously considered a highly challenging task, recent advancements in image generation have transformed the field of cartographic \cite{Sester2018Buildinggeneralizationusing}. These techniques now present a promising solution for synthesizing map tiles directly from remote sensing imagery \cite{Goodfellow2014GenerativeAdversarialNets,Mirza2014ConditionalGenerativeAdversarial}. They reframing the problem as a generative task within a data-driven framework, aiding in the learning of cartographic knowledge from a wide range of tile maps as cartographic results \cite{Ganguli2019GeoGANConditionalGAN}.

Current map generation model, exemplified by GeoGAN \cite{Ganguli2019GeoGANConditionalGAN}, SMAPGAN \cite{Chen2021SMAPGANgenerativeadversarial}, and CreativeGAN \cite{Fu2021TranslationAerialImage}, utilize image translation \cite{Isola2017ImagetoImageTranslationConditional, Wang2018HighResolutionImageSynthesis} methods to achieve map creation from remote sensing imagery. However, these approaches remain constrained to fixed-scale, single-layer map generation paradigms. Specifically, they exhibit insensitivity to hierarchical scale information inherent in tile-based maps and lack the capacity for multi-scale tile map production. Moreover, Multi-level map generate method can generate multi-scale maps using map-level \cite{Liu2021CscGANconditionalscaleconsistent, Fu2023Levelawareconsistentmultilevel}, but fixed level encoding fails to provide a detailed understanding of map scales and overlooks cartographic knowledge associated with scale modality information. The current flaws of applying generation method in multi-scale cartography from remote sensing images lay in three aspects:

1) Single-scale independent models are insufficient for generating coherent multi-scale maps required for hierarchical decision-making. Utilizing the cartographic expertise embedded in extensive tile map samples (see Fig. \ref{fig:introduce-idea}(a)) to enhance scale expression accuracy in generated maps remains a significant challenge. Current methodologies often rigidly link map levels to generated outputs, overlooking the integration of scale-specific characteristics including spatial resolution, map scale, and visual presentation. This oversight diminishes the model's capacity to leverage cartographic expertise from multi-scale maps, resulting in inaccurate scale representation and compromising the practical utility of generated cartographic products.

2) Tile-by-tile generation frequently introduces visual inconsistencies along tile edges after stitching (Fig. \ref{fig:introduce-idea}(c)), which compromises both spatial readability and practical usability. These artifacts are particularly problematic in emergency mapping scenarios where efficiency and accuracy are critical. Additionally, stylistic inconsistencies across tiles negatively impact the map's overall coherence and aesthetic quality. Furthermore, the absence of comprehensive geographical semantic information in tile samples hinders the effectiveness of generative model training.

3) Single-scale generative models are inadequate for dynamically and accurately representing map features in practical mapping scenarios, failing to seamlessly integrate cartographic features from macro to micro perspectives. Moreover, reliance on isolated geographical semantic extraction modules restricts spatial context understanding, while semantic segmentation samples risk obscuring detailed map information. These limitations impair cross-modal spatial comprehension and reduce the generative model's expressive capabilities.

To address these challenges, we propose a novel framework for generating multi-scale tile maps from remote sensing imagery, termed Cascading Conditional Generative Cartography frameworks (C2GM). This framework leverages the principles of conditional diffusion models and multi-scale cascade generation to enhance the quality of generated maps. By integrating scale information and cascading references, C2GM effectively captures the intricate relationships between map scales and geographic features, ensuring coherent and visually appealing outputs. Unlike prior single-stage map generation methods, our self-cascaded map generation framework recursively produces multi-scale tile maps from a given spatial scene. This unified generation model is applicable across various map scales: during each stage of map generation, variables such as the low-level map from the previous stage, map level, scale, and spatial resolution are integrated as controllable parameters to guide the creation of higher-scale, high-level map tiles. As the stages progress, the produced tile maps exhibit diverse spatial resolutions, feature representations, and map syntheses, enabling the parallel generation of multi-scale map tiles for a given large-format spatial scene.

Additionally, we have developed a real-world remote sensing image dataset CSCMG (Cross-Scale Cascade Map Generation dataset) from Glasgow, UK, to validate the effectiveness of our proposed method and address limitations in existing datasets for multi-scale map generation. This dataset achieves extensive sample area coverage through coordinate sampling and includes continuous regional tile data from the London area to support continuous map range generation testing. The multi-scale tile samples within the dataset enable the verification of cross-scale map cascade generation. Furthermore, map scale information—such as map level, scale, and resolution associated with each remote sensing image-tile map sample pair—provides essential constraints for multi-scale map generation.

In summary, our main contributions are as follows.

1) We have developed a CLIP-based scale-modality encoding mechanism and constructed a multimodal cartographic feature fusion module based on SPADE. This approach dynamically achieves data-driven embedding of multi-scale cartographic knowledge priors, aligns cartographic generalization with geographic semantics, and effectively integrates the expertise of cartographic synthesis specialists into the generative model.

2) We eliminate edge discontinuities while maintaining semantic coherence across scales by employing low-level map tiles as spatial constraints in a cascaded generation strategy. It enables boundless map generation for uncharted regions, where conventional methods produce fragmented outputs unsuitable for emergency coordination.

3) Our dual-branch scale-adaptation network extracts complementary features from remote sensing imagery guided by cascaded references. The architecture guarantees cartographic fidelity across various scenarios by adaptively weighting local textures and global structures, providing a rich representation of intricate geographical features in the resultant map.

\section{Related work}
\subsection{Image-to-image generation methods}
Image-to-image (I2I) translation has emerged as a pivotal technique for cross-domain visual representation learning, particularly in cartographic applications where remote sensing imagery requires transformation into structured map formats. This conversion process faces a fundamental challenge: the significant domain gap between raw sensor data and stylized map elements. To bridge this domain discrepancy, modern frameworks employ specialized architectures that explicitly model cross-domain mapping relationships. These architectures typically incorporate multi-scale feature extractors and domain-specific constraints to enhance output fidelity \cite{Pang2021ImagetoImageTranslationMethods}. The objective of these frameworks goes beyond replicating the source imagery at a pixel level; it also involves capturing latent semantic representations and modeling complex feature distributions. Generally, generative models aim to synthesize artificial data instances that approximate the intrinsic probability distribution of authentic samples, based on data conforming to specific distributional constructs \cite{A.Oussidi2018Deepgenerativemodels}.

The initial generative model is derived from variational autoencoder (VAE), introduced by Kingma \cite{Kingma2013Autoencodingvariationalbayes}, pioneered probabilistic modeling for data generation. However, due to limitations in synthesizing high-resolution outputs and inherent instability during adversarial training, VAEs were superseded by GAN-based (Generative Adversarial Networks) architectures \cite{Goodfellow2014GenerativeAdversarialNets}. Current techniques ormulated using Conditional Generative Adversarial Networks (cGANs) \cite{Wang2018HighResolutionImageSynthesis}, such as Pix2Pix \cite{Isola2017ImagetoImageTranslationConditional} and Pix2PixHD \cite{Wang2018HighResolutionImageSynthesis},  achieved supervised cross-domain mapping by incorporating input-output constraints. It effectively visualized semantic labels, reconstructed edges into objects, and synthesized colors. Notably, StarGAN \cite{Choi2018StarGANUnifiedGenerative} provided a paradigmatic solution for multi-domain translation through a unified architecture design, while CycleGAN \cite{Zhu2017UnpairedImagetoImageTranslation} pioneered unpaired translation methodologies, greatly expanding the technological applicability in diverse cartographic scenarios.

Drawing inspiration from thermodynamics, diffusion models employ iterative noise introduction to capture and learn patterns of informational decay caused by noise. In turn, this enables the generation of images based on the learned dynamics \cite{Sohl-Dickstein2015DeepUnsupervisedLearning}. These models have demonstrated exceptional performance in generating high-resolution images, with the Denoising Diffusion Probabilistic Model (DDPM) \cite{Ho2020DenoisingDiffusionProbabilistic} being a prominent representative in the field. DDPM operates by reversing the diffusion process: in the forward process, noise is progressively added to the original data until it transforms into Gaussian noise. The model is trained to reverse this process step-by-step, reconstructing data from noise. Building on this foundation, Nichol et al. \cite{Nichol2021ImprovedDenoisingDiffusion} introduced a learning variance strategy within DDPM, significantly improving the quality of synthesized images while enhancing the speed of the sampling process. To expedite sampling further, Song et al. proposed the Denoising Diffusion Implicit Model (DDIM) \cite{Song2022DenoisingDiffusionImplicit}, which reformulated the diffusion process as non-Markovian. DDIM, without altering DDPM's training methodology, significantly boosted sampling speed while maintaining nearly identical image generation quality. Furthermore, DDIM established a deterministic generation process, ensuring that the resulting images were solely influenced by the initial noise used in the reverse process.

The diffusion models are primarily unconditional generation models, producing images through random sampling without relying on external controls. On the other hand, conditional diffusion models generate images by incorporating additional control signals. Such models allow for the integration of diverse input conditions, spanning classification-to-image generation \cite{Ho2022CascadedDiffusionModels}, image-to-image synthesis \cite{Saharia2022PaletteImagetoImageDiffusion,Saharia2022ImageSuperResolutionIterative,Zhang2023AddingConditionalControl}, text-to-image generation \cite{Rombach2022HighResolutionImageSynthesis,Saharia2022PhotorealisticTexttoImageDiffusion,Wu2024TaiyiDiffusionXLAdvancingBilingual}, and image editing \cite{Avrahami2022BlendedDiffusionTextDriven,Nie2023BlessingRandomnessSDE,Shi2023DragDiffusionHarnessingDiffusion,Hertz2022PrompttoPromptImageEditing}, among others. However, as DDPM's reverse process operates directly in pixel space, predicting high-dimensional noise demands substantial computational resources and memory, posing challenges for high-resolution image generation. Two principal strategies have been devised to surmount this limitation: cascade generation \cite{Saharia2022PhotorealisticTexttoImageDiffusion,Ho2022CascadedDiffusionModels}, which begins with the generation of a low-resolution image and progressively synthesizes higher-resolution images by treating the preceding stages as conditional inputs; and latent space generation \cite{Rombach2022HighResolutionImageSynthesis}, which compresses the image into a lower-dimensional latent space using an encoder-decoder framework and performs the denoising diffusion process within this latent space. Both approaches have proven effective in optimizing the efficiency of high-resolution image synthesis using diffusion models.

Consequently, these approaches frequently fail to adequately extract and represent the intricate features of objects in remote sensing imagery, resulting in maps of subpar quality compared to those produced by methods specifically tailored for this domain.

\subsection{Generation maps from remote sensing image}
The generation of maps from remote sensing imagery has garnered significant attention in recent years, particularly with the advent of deep learning techniques. With the rapid advancement of remote sensing technology, capturing high-resolution remote sensing imagery has become increasingly effortless and efficient. This progress paves the way for adopting intelligent interpretation techniques based on deep learning, which inherently depend on high-quality data for optimal training. The primary tasks associated with the intelligent analysis of remote sensing imagery include semantic segmentation, feature object extraction, target detection, and scene classification. Remote sensing images are composed of many geographical elements, often intricate and diminutive, presenting notable challenges for their semantic segmentation. Existing methods for semantic segmentation in this domain have primarily focused on addressing these difficulties, particularly in the accurate segmentation of geographic elements within remote sensing imagery \cite{Wang2023UPerNetConvNeXtSemantic,Ma2024MultilevelMultimodalFusion,Zhang2024RSAMSegSAMbasedApproach,Toker2024SatSynthAugmentingImageMask,Zhao2023CDDFuseCorrelationDrivenDualBranch,Peng2023DiffusionbasedImageTranslation,Lu2023ScoreSegLeveragingScorebased,Ayala2023Diffusionmodelsremote,Chen2021SelfAttentionReconstructionBias,Cheng2022MaskedAttentionMaskTransformer,Shen2022Highqualitysegmentation,Jiang2020RWSNetsemanticsegmentation}. In terms of the pixel-level classification of land use and land cover within remote sensing imagery, significant attention has been devoted to challenges associated with the classification of large surface areas and heterogeneous land cover types \cite{Yao2023ExtendedVisionTransformer,Wang2023DSViTDynamicallyScalable,Chen2023SparseViTRevisitingActivation,Li2024LearningHolisticallyDetect,Zhao2024RSmambalargeremote,Guo2022ISDNetIntegratingShallow,Shen2022Highqualitysegmentation,Xi2021WideSlidingWindow}. Similarly, research aimed at feature object extraction has explored methods to detect specific elements, such as roads, by leveraging their geometric and topological characteristics for improved precision in extraction \cite{Luc2016SemanticSegmentationusing,Chen2021ReconstructionBiasUNet}. For target detection, existing approaches tailored for remote sensing imagery have primarily concentrated on overcoming the challenges posed by detecting small, densely clustered objects, irrespective of their orientation or rotation \cite{Yang2019SCRDetMoreRobust,Ding2019LearningRoITransformer,Yin2020AirportDetectionBased,Li2021SeMoYOLOMultiscaleObject,Han2021ReDetRotationEquivariantDetector,Xia2024ViTCoMerVisionTransformer}. Meanwhile, scene classification has increasingly integrated multimodal technologies, combining visual data from remote sensing imagery with geographical textual information. This synergy has given rise to foundational image-text models within the remote sensing field \cite{Muhtar2024LHRSBotEmpoweringRemote,Zhang2024EarthGPTUniversalMultimodal,Kuckreja2024GeoChatGroundedLarge,Guo2024SkySenseMultiModalRemote,Mall2023RemoteSensingVisionLanguage}, establishing a robust framework for a wide range of downstream applications. However, the task of generating high-quality maps from remote sensing imagery remains a complex challenge due to the inherent differences between the two domains.

Some researchers have using generate techniques to generate maps from remote sensing imagery: facilitate transitions from remote sensing imagery to map tiles through the adversarial training of generators and discriminators \cite{Isola2017ImagetoImageTranslationConditional}. However, these approaches predominantly emphasize the intrinsic features of images while overlooking the relevance and complementarity of geographical features within remote sensing images, resulting in maps that inadequately represent complex geographical features. Various GAN-based map generation methods have improved map generation to some extent. For instance, GeoGAN \cite{Ganguli2019GeoGANConditionalGAN} employs a Conditional Generative Adversarial Network (cGAN) \cite{Wang2018HighResolutionImageSynthesis} to translate remote sensing imagery into map representations, while SMAPGAN \cite{Chen2021SMAPGANgenerativeadversarial} leverages incompletely paired remote sensing and internet map sample data to produce maps using semi-supervised generative, although it does not fully overcome the issue of map quality. Additionally, CreativeGAN \cite{Fu2021TranslationAerialImage} incorporates semantic information to improve the accuracy of generated maps. These GAN-based methodologies refine the outputs through adversarial training dynamics between generators and discriminators, where discriminators assess image authenticity and penalize deviations from the target appearance. However, these methods predominantly focus on single-level map generation and fail to accommodate the intricate requirements of multi-scale map generation.

In emergency mapping, there is a necessity for small-scale maps to provide an overview with general geographic information, alongside large-scale maps to precisely depict local geographic changes. To tackle the specific challenges associated with multi-scale map generation, Chen et al. \cite{Chen2022GeneratingMultiscaleMaps} proposed a straightforward training approach for producing hierarchical maps. At the highest level (\(k\)-th level, where \(k\) denotes the maximum scale), this strategy generates maps directly from remote sensing imagery, whereas at lower levels, the mappings between the \(k\)-th and \((k-1)\)-th levels are learned through independently trained models. However, this approach exhibits notable limitations: the training process is complex, as each level requires a distinct model, and the accuracy of the highest-level map constrains the quality of lower-level maps. This results in insufficient utilization of the features within remote sensing imagery. In contrast, CscGAN \cite{Liu2021CscGANconditionalscaleconsistent} developed a multi-scale map generation network capable of producing multi-level maps using a single model. This approach introduces a map-level classifier, inspired by StarGAN \cite{Choi2018StarGANUnifiedGenerative}, as an extension to the discriminator. It utilizes preprocessed road morphology data as input. Nevertheless, this method struggles to generate consistent multi-scale maps solely from remote sensing imagery, as its multi-level maps pertain to disparate regions rather than representing varying scales of the same area. This limitation narrows its applicability for multi-scale map generation within a cohesive geographic region.

Building upon these advancements, Fu et al. \cite{Fu2023Levelawareconsistentmultilevel} currently represent the state-of-the-art through their level-aware generation framework. They proposed a multi-level map generation network that employs a level classifier to distinguish and generate map tiles across different levels. Their network introduces a map element extractor to capture significant geographic features from satellite imagery and incorporates a multi-level fusion generator to produce seamless multi-level maps from initial lower-level map inputs. However, their architecture relies on independently generated map tiles, which, when assembled, often exhibit visual discontinuities that detract from the overall readability and aesthetic coherence of the resulting map.

In conclusion, existing methodologies for map generation often focus solely on single-level maps or fail to meet the requirements for consistent multi-scale map production. These approaches frequently overlook the complex relationships between map scales and geographic features, resulting in suboptimal outcomes. Furthermore, many methods rely on independent generation processes for each map tile, which leads to visual discontinuities and a lack of coherence across the generated maps. These limitations impede the practical application of such methods in real-world scenarios, particularly in emergency mapping and large-format cartographic applications. To address these limitations, an end-to-end generative approach for cartography has been proposed, specifically designed to produce high-quality, seamless, and wide-area multi-scale tile maps from remote sensing imagery. This method effectively tackles the challenges of multi-scale tile map generation, ensuring improved consistency and usability for regional map production.

\begin{figure*}[htp]
	\centering
	\includegraphics[width=1.0\linewidth]{method-framework.pdf}
	\caption{
		The overall structure of C2GM encapsulates a hierarchical, self-cascading design aimed at producing multi-scale, large-area tile map for specified spatial scenarios. The generation process is divided into multiple stages, starting with lower-level maps and gradually transitioning to higher-level ones. In each stage, the generation process depends on the lower-level tile previously produced and its scale information. This multi-stage generation strategy not only enhances the quality of map tile generation but also ensures consistency and coherence across different tiles by learning the disparities between various map scales and integrating cartography expertise.
	}
	\label{fig:method-framework}
\end{figure*}

\section{Method}
\subsection{Overall framework}
We propose a novel self-cascading generative mapping framework, designed to generate tile map across diverse scales. As depicted in Fig. \ref{fig:method-framework}, the overarching design of C2GM enables the recursive reconstruction of large-scale tile map using a unified generative model.

The generation process is structured into sequential stages: initially synthesizing lower-level maps, then progressively creating higher-level ones through iterative refinement. The model incorporates three key inputs at each generative stage: remote sensing images, cascading references, and corresponding scale information. Remote sensing images provide essential geo-object features for cartographic synthesis; lower-level map tiles serve as cascading references, offering pre-existing cartographic characteristics to guide the generation process; and scale information enables the model to adapt to and represent map features at various scales. Unlike auto-regressive models prioritizing resolution enhancement, our cascading framework incorporates scale-specific cartographic knowledge. This design allows the model to learn hierarchical geographic representations, ensuring that higher-level maps generalize features such as road simplification and building aggregation by cartographic standards instead of merely refining pixel-level details.

At the \(k\)th generation stage, let \(x_0^{(k)}\) denote the target map and \(s^{(k)}\) represent the scale information, encompassing elements such as map layer \(s_z^{(k)}\), spatial resolution \(s_r^{(k)}\), scale ratio \(s_s^{(k)}\), and geospatial representation \(s_f^{(k)}\). When generating the map for the \((k+1)\)th stage, the input conditional variables \(x_0^{(k)}\) and \(s^{(k)}\) are embedded into the model. Assuming that during each stage, the model increases the tile level \(s_z^{(k)}\) of the input tile by \(n\) levels (e.g., \(n=1\)), and if the map tile size generated at the \(k\)th stage is \(H \times W\) pixels, the \((k+1)\)th stage output will comprise \(4^n\) map tiles at level \(s_z^{(k)}+n\). These new tiles correspond to the geographic area covered by the \(k\)th stage tile but are represented within a frame of \(2^n \cdot H \times 2^n \cdot W\) pixels.

Through a parallel generation mechanism, the above operation can be iteratively repeated \(m\) times, starting from the \(k\)th stage, eventually resulting in a full tile map consisting of multiple layers and scales. Formally, the resulting tile map can be expressed as:
\[
\mathcal{X} = \{ x_0^{(k)}, x_0^{(k+1)}, \dots, x_0^{(k+m)} \},
\]
where the tile map produced at the \((k+m)\)th stage, \(x_0^{(k+m)}\), corresponds to the map layer \(s_z^{(k)}+m \cdot n\) and has an ultimate size of \(H \cdot 2^{(n^m)} \times W \cdot 2^{(n^m)}\) pixels.

\subsection{Conditional denoising in map generation}
The Denoising Diffusion Probabilistic Models (DDPMs) represent a class of advanced generative models rooted in the principles of the diffusion process. The fundamental mechanism of DDPMs involves the gradual addition and subsequent removal of noise, simulating a diffusion-like transformation of data to generate new samples \cite{Ho2020DenoisingDiffusionProbabilistic}. These two phases—forward diffusion and reverse denoising—correspond to the training and sampling stages of the model, respectively. The traditional DDPM conducts both the forward and reverse processes directly over the entire pixel space, with the diffusion process mathematically described as follows:
\begin{equation}
	\begin{aligned}
		y_{t} = \sqrt{\bar{\alpha_{t}} } y_{0}+\sqrt{1-\bar{\alpha_{t}}}\epsilon, \ \ \ \ \epsilon\sim N\left(0,1\right)
	\end{aligned}
	\label{equ:r1_diffusion_process}
\end{equation}
where \(y_{0}\) denotes the data sampled from the true data distribution, and \(\bar{\alpha}_{t}\) represents a predefined hyperparameter governing the noise schedule. The term \(\epsilon\) corresponds to random noise drawn from the standard Gaussian distribution. The DDPM framework centers on training a denoising function \(\epsilon_{\theta}\), which estimates the noise component embedded in \(y_{t}\), constrained by an \(L_2\) loss function:
\begin{equation}
	\begin{aligned}
		L=\mathrm{\mathbb{E}}_{\left(y_{0},\epsilon, t\right)}\parallel \epsilon-\epsilon_{\theta}\left(y_{t},t\right) \parallel ^{2}
	\end{aligned}
	\label{equ:r1_l2_loss}
\end{equation}
In this equation, \(t\) denotes an integer time step drawn from the interval \([0, T]\), while \(\epsilon_{\theta}\) signifies the noise predictor that undergoes training. Minimizing the aforementioned loss function allows the model to progressively transform random noise into high-quality data throughout the reverse denoising procedure. For more precise and controlled data generation, supplementary conditioning variables can be integrated into the noise predictor, yielding an extended formulation such as \(\epsilon_{\theta}(y_{t}, t, c)\), where \(c\) encapsulates the conditioning information. Based on the underlying principles of DDPM, each incremental step within the denoising operation can be mathematically formulated as follows:
\begin{equation}
	\begin{aligned}
		y_{t-1}=\frac{1}{\sqrt{\alpha_{t}}}(y_{t}-\frac{\beta_{t}}{\sqrt{1-\bar{\alpha_{t}}}}\epsilon_{\theta})+\sigma_{t}\epsilon, \ \ \ \ \epsilon\sim N\left(0,1\right)
	\end{aligned}
	\label{equ:r1_denoising_process}
\end{equation}
where \(\alpha_{t}\), \(\bar{\alpha}_{t}\), \(\beta_{t}\), and \(\sigma_{t}\) are predefined hyperparameters governing the noise schedule, and \(\epsilon\) denotes additional noise injected during the reverse process. After \(T-1\) iterations, this method yields a final sample conditioned on \(c\).

Despite their high-quality results, DDPMs are computationally intensive and time-consuming for training and inference, making them less practical for certain applications. Addressing this limitation, Latent Diffusion Models shift the diffusion and denoising processes from pixel space to a lower-dimensional latent space learned by a pre-trained autoencoder. This paradigm drastically reduces the dimensional complexity while maintaining crucial data details. The encoding and decoding operations of the autoencoder can be defined as follows:
\begin{align}
	z_{0} = \mathcal{E}\left( y \right)
	\label{equ:r1_encoder} \\
	y \approx \mathcal{D}\left( z_{0} \right)
	\label{equ:r1_decoder}
\end{align}
where \(z_{0}\) denotes the latent representation of the target data \(y\), \(\mathcal{E}\) represents the encoder, and \(\mathcal{D}\) is the decoder, both of which are pre-trained. As a result, the entire diffusion and denoising processes are executed in the latent space, where \(z_{0}\) resides. This design achieves an optimal balance between computational efficiency and the preservation of data fidelity.

Map generation models rely solely on remote sensing imagery tiles as inputs to produce corresponding spatial scene map tiles.This process can be mathematically expressed as follows:
\begin{equation}
	\begin{aligned}
			y=\mathop{\arg\min}_{y}-\log\left(p\left(y\mid x\right)\right)
		\end{aligned}
	\label{equ:r1_ddpm_model}
\end{equation}
where \(x\) denotes the remote sensing imagery tiles, while \(y\) represents the generated map tiles. The term \(p(\cdot \mid x)\) refers to the conditional probability density function of the target map given the remote sensing imagery input, typically modeled as either a standard Gaussian distribution (\(L_2\) loss) or a Laplacian distribution (\(L_1\) loss), with the true target map serving as its mean. This approach, which relies exclusively on remote sensing imagery tiles, theoretically learns the many-to-one mapping between remote sensing tiles and the corresponding map tiles. However, when applied to multi-scale map generation, these methods often need to be revised, struggling to extract precise semantic information about geo-objects across multiple scales of remote sensing imagery.

To address these limitations, drawing inspiration from advancements in decoupled feature learning, we propose the following revised modeling approach:
\begin{equation}
	\begin{aligned}
			y=\mathop{\arg\min}_{y}-\log\left(p\left(y\mid x,con,rsm\right)\right)
		\end{aligned}
	\label{equ:r1_sgdm_model}
\end{equation}
where \(con\) and \(rsm\) represent scale information and cascading reference conditions. Both serve as pivotal components for encoding map scale information. In the context of this methodology, the terms are defined as follows:
\begin{itemize}
    \item \textbf{Scale Information}: The cross-modal constraints of scale features correspond to the map's level. These encapsulate the spatial resolution, scale, and textual representation of geographic objects associated with the specified level of the generated map.
    \item \textbf{Cascade Reference}: The visual representation of map sheets and geographic object information from lower levels, acts as cascading feature constraints.
\end{itemize}

During the training phase, any useful prior knowledge can be utilized to provide content guidance. In the testing phase, multi-scale reconstruction results are derived using scale information and cascading reference. By decomposing cartographic reference information into scale and cascading features, this modeling approach enables improved reconstruction guidance, delivering higher-quality generation results with enhanced scale representation and stronger semantic fidelity.

\begin{figure*}[htp]
	\centering
	\includegraphics[width=\linewidth]{method-model.pdf}
	\caption{The SGDM architecture leverages a Variational Autoencoder (VAE) to transition the diffusion and reverse processes from pixel space to latent space. During training, the latent representation of the target map, \(z_0\), undergoes a progressive transformation into \(z_t\) through the diffusion process, followed by denoising achieved via a U-Net network. Two bespoke modules are introduced to further refine the denoising process: the Map Feature Encoder (MFEncoder) and the Scale Feature Adapter (SFAdapter). The MFEncoder integrates information derived from remote sensing imagery and cascading references to construct the conditional feature, \(F_{\mathrm{cond}}\). Expanding on \(F_{\mathrm{cond}}\), the SFAdapter produces multi-scale features that are subsequently merged with the outputs of corresponding U-Net layers through element-wise addition.}
	\label{fig:method-model}
\end{figure*}

\subsection{Scale-guided diffusion model}
In order to address the challenges associated with multi-scale map generation from remote sensing images in real-world scenarios, we propose a novel scale-unified map generation model called the Scale-guided Diffusion Model (SGDM). Unlike previous single-scale map generation models, SGDM can simultaneously process inputs from remote sensing image tiles, target map tiles, and cascading reference tiles. This collaborative approach enables the generation of map tiles customized to specific scales or levels. The core architecture of SGDM is illustrated in Fig. \ref{fig:method-model}. The SGDM framework is composed of four integral components: the pre-trained Variational Auto-Encoder (VAE), the denoising U-shaped network (U-Net) with skip connections, the Map Feature Encoder (MFEncoder), and the Scale Feature Adapter (SFAdapter). The VAE encodes and decodes high-resolution images, thereby transitioning the conventional DDPM diffusion and denoising process from pixel space to latent space, enhancing training stability and reducing computational overhead. Concurrently, the U-Net, functioning as a conditioned noise predictor, iteratively predicts and removes noise to achieve clean data. It is constructed with multiple encoding, middle, and decoding layers, most of which are composed of residual blocks, cross-attention, and self-attention modules. The MFEncoder integrates prior information from the style guide map, remote sensing image, and scale reference map to produce a conditional feature map enriched with semantic and stylistic details. Finally, the Scale Feature Adapter (SFAdapter) converts these conditional features into multi-scale features (about the image domain scale rather than the geographic domain scale) that are compatible with the U-Net architecture. These features are incorporated into the outputs of the U-Net's encoding and decoding layers on an element-wise basis, ensuring seamless feature integration and effective guidance throughout the generation process.

\subsubsection{Scale information guidance}
At each generation stage, the previously generated lower-level tiles, along with time step and scale information embeddings are utilized as conditional input variables. For the lower-level tile input — cascading reference map \(x_0^{(k)}\) — its dimensions do not align with \(x_t^{(k+1)}\). To address this, and inspired by \cite{Saharia2022ImageSuperResolutionIterative} and \cite{Li2022SRDiffSingleimage}, we redesigned the network architecture. First, as illustrated in Fig. \ref{fig:method-model}, the encoder \(E_{lr}\) is applied to encode the lower-level map tiles \(x_0^{(k)}\). Then, a series of upsampling and convolutional layers \(f_{up}\) are employed to align the feature map dimensions with \(x_t^{(k+1)}\). Finally, the features are fused by concatenating \(x_t^{(k+1)}\) and the feature maps along the channel dimension:
\begin{equation}
	\label{equ:mr_id_encoder}
	\tilde{x}_t^{(k+1)}=\text{cat}[x_t^{(k+1)},f_{up}(E_{lr}(x_0^{(k)}))]
\end{equation}
where \(\text{cat}(\cdot)\) represents the concatenation operation along the channel dimension.

For the scale information \(s\) in text modality data, we perform cross-modal encoding using pre-trained CLIP model \cite{Radford2021LearningTransferableVisual,Cherti2023Reproduciblescalinglaws}, with the CLIP scale encoder denoted as \(f_\theta\) (Fig. \ref{fig:method-model}). This approach allows us to obtain the embedding vector of scale information \(e_s^{(k)} = f_\theta(s)\) at each generation stage, capturing the cross-modal representation of scale information corresponding to the spatial context of the generated map. This embedding subsequently guides the map generation process through the cross-attention mechanism \cite{Vaswani2017Attentionallyou} during denoising:
\begin{equation}
	\label{equ:sr_td_encoder}
	e_s^{(k)}=f_\theta(s_z^{(k)}+s_r^{(k)}+s_s^{(k)}+s_f^{(k)})
\end{equation}
Through this process, the scale information embedding \(e_s^{(k)} \in \mathbb{R}^D\) is obtained at the \(k\)-th generation stage. The time step variable \(t \in \{1, \dots, T\}\) is then encoded via a frequency encoding transformation \(f_\omega\):
\begin{equation}
	\label{equ:ts_fd_encoder}
	e_t=f_\omega(t)
\end{equation}
Here, \(\theta_t\) is a learnable parameter.

Finally, \(e_s^{(k)}\) and \(e_t\) are combined to generate the final conditional embedding vector for the \(k\)-th generation stage and the \(t\)-th denoising step:
\begin{equation}
	e_t^{(k)} = e_s^{(k)} + e_t
\end{equation}
Using the conditional embedding vector \(e_t^{(k)}\) and the cascade map feature \(\tilde{x}_t^{(k)}\), we define the conditional variable for the \(k\)-th generation stage and \(t\)-th denoising step as:
\begin{equation}
	c_t^{(k)} = \{e_t^{(k)}, \tilde{x}_t^{(k)}\}
\end{equation}
This condition \(c = c_t^{(k)}\) is then used as input to model the following conditional probability density function \(p_\theta\left(y \mid x, c\right)\):
\begin{align}
    p_\theta(x_{0:T}|c_t^{(k)})&=p(x_T)\prod_{t=1}^{T}{p_\theta(x_{t-1}|x_t, c_t^{(k)}})
    \label{equ:r2_ddpm_pc_0} \\
    p_\theta(x_{t-1}|{x_t},c_t^{(k)})&=\mathcal{N}(x_{t-1};\mu_\theta(x_t,t,c_t^{(k)}),\sigma_t^2{I})
    \label{equ:r2_ddpm_pc_t}
\end{align}

Based on \eqref{equ:r1_sgdm_model}, while keeping the forward process unchanged, we construct a conditional denoising model:
\begin{equation}
	\begin{aligned}
			y=\mathop{\arg\min}_{y}-\log\left(p_\theta(x|c)\right)
		\end{aligned}
	\label{equ:r2_ddpm_model_sc}
\end{equation}

Through this design, we can ultimately generate map tiles of varying scales in a self-cascading manner, encoded with information on resolution, temporal steps, and scale.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{method-blocks.pdf}
	\caption{Detailed structure of the proposed Multi-branch Mapping Feature Encoder (MFEncoder): it can generate the conditioned features \(F_{\mathrm{cond}}\) that contain cartographic semantic information by utilizing the content information of the reference map at multiple scales through the SPADE module.}
	\label{fig:method-blocks}
\end{figure}

\subsubsection{Map feature encoder}
To facilitate the successful amalgamation of cascading information from cascading reference tiles into remote sensing image tiles, we have developed a multi-branched map feature encoder (MFEncoder), illustrated in Fig. \ref{fig:method-model}. This encoder integrates semantic information from cascading reference tiles into remote sensing image tiles across various scales. As illustrated in Fig. \ref{fig:method-blocks}(a) convolutional and down-sampling blocks are employed in one branch to extract multi-scale features from the cascading reference tiles. The multi-scale features are derived from the remote sensing image tiles through the Basic Block in another branch. To achieve the fusion of these features, we have incorporated the SPADE (Spatially-Adaptive Normalization) module at each scale. In our design, the input to the SPADE block comprises feature maps from remote sensing image tiles, abundant in coarse texture and structural information. As illustrated in Fig. \ref{fig:method-blocks}(b), our enhanced SPADE module distinguishes itself from the original design by utilizing features from both remote sensing and cascading reference tiles to compute the scaling factor \(\gamma\) and the offset term \(\beta\). This approach enables a flexible and high-fidelity integration of content-guided information into the features of remote sensing images. The entire process is detailed as follows:
\begin{gather}
	\gamma^{i} = \mathrm{Conv}_{\gamma}\left(\mathrm{Conv_{share}}\left(\left[F_{\mathrm{rs}}^{i},F_{\mathrm{cr}}^{i}\right]\right)\right)
	\label{equ:r1_gamma} \\
	\beta^{i} = \mathrm{Conv}_{\beta }\left(\mathrm{Conv_{share}}\left(\left[F_{\mathrm{rs}}^{i},F_{\mathrm{cr}}^{i}\right]\right)\right)
	\label{equ:r1_beta} \\
	F_{\mathrm{rs}}^{i+1} = \gamma^{i} \frac{F_{\mathrm{rs}}^{i} - \mu^{i}}{\sigma^{i}} + \beta^{i}
	\label{equ:r1_spade}
\end{gather}

In this context, \(F_{\mathrm{rs}}^{i}\) and \(F_{\mathrm{cr}}^{i}\) denote the features of terrestrial objects in remote sensing imagery and the features of the cascading reference map at the \(i\)-th layer, respectively. \(\mathrm{Conv_{share}}\) represents the shared convolutional module, while \(\mathrm{Conv}{\gamma}\) and \(\mathrm{Conv}{\beta}\) are the convolutional layers used to predict the scaling factor \(\gamma^{i}\) and the bias term \(\beta^{i}\). Here, \(\gamma^{i}\) and \(\beta^{i}\) indicate the spatially varying scaling factor and bias term at the \(i\)-th layer, whereas \(\mu^{i}\) and \(\sigma^{i}\) correspond to the mean and standard deviation of all input features within the mini-batch. By stacking content-guiding modules, the features of the cascading reference map are progressively integrated into the terrestrial object features of remote sensing imagery, enabling more precise map reconstruction at a semantic level. Figure \ref{fig:method-blocks}(c) illustrates the network architecture of the basic block, convolutional block, and downsampling block.

\subsubsection{Scale feature adapter}
The MFEncoder excels in integrating texture and structural features from remote sensing image tiles, alongside content features from cascading reference tiles, ultimately producing the conditional feature \(F_{\mathrm{cond}}\). Given that \(F_{\mathrm{cond}}\) encompasses the majority of external information requisite for map generation, the function of the Scale Feature Adapter (SFAdapter), illustrated in Fig. \ref{fig:method-model}, is primarily to generate multi-scale features and align them with the pre-existing knowledge of the pre-trained stable diffusion model.

Consequently, the architecture of the SFAdapter module is inherently straightforward, primarily comprising stacked convolutional layers, residual blocks, and down-sampling blocks (Fig. \ref{fig:method-blocks}(c)). The SFAdapter module progressively increases the channel count of the feature maps while simultaneously diminishing their spatial dimensions, resulting in three distinct sets of conditional features. These features are then integrated into the output of the U-Net network layer by layer, on an element-wise basis, to facilitate the generation process of the stable diffusion model.

\begin{figure*}[!hbp]
    \centering
    \includegraphics[width=\linewidth]{dataset-example.pdf}
    \caption{Examples of RS-Map tile sample pairs from levels 14 to 18 in the CSCMG dataset are provided. The dataset covers a broad range of cross-scale scenarios, including forests, rivers, streets, residential areas, industrial areas, bodies of water, plazas, and more.}
    \label{fig:dataset-example}
\end{figure*}

\subsection{Loss function} \label{sec:loss}
As illustrated in Fig. \ref{fig:method-model}, the trainable parameters of the Scale-Guided Diffusion Model (SGDM) are determined by the following loss function:
\begin{equation}
    \begin{aligned}
    L=\mathrm{\mathbb{E}}_{\left(z_{0},\epsilon,rs,cr,t\right)}\parallel \epsilon-\epsilon_{\theta}\left(z_{t},t,rs,cr\right) \parallel ^{2}
    \end{aligned}
    \label{equ:r3_l2_loss}
\end{equation}
This equation, \(rs\) and \(cr\), represents the input remote sensing image tiles and cascading reference tiles. Here, \(\epsilon\) denotes randomly sampled Gaussian noise, while \(t\) signifies the integer timestamp sampled from the interval \([0, T]\). In this context, \(\epsilon_{\theta}\) refers to the denoising U-Net model.

\subsection{Evaluation metrics} \label{sec:metrics}
We adhere to established practices in generative cartography \cite{Fu2023Levelawareconsistentmultilevel,Fu2021TranslationAerialImage} by employing image quality evaluation metrics to comprehensively assess the performance of map generation methods. The realism of the generated maps is quantified using the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM), calculated on the Y channel within the YCbCr color space \cite{Wang2004Imagequalityassessment}. Furthermore, the Fréchet Inception Distance (FID) is utilized to evaluate the distributional divergence between real and generated maps \cite{Heusel2017GANsTrainedTwo}.

While semantic segmentation metrics (e.g., mIoU) could offer deeper insights, our CSCMG dataset focuses on large-scale, unpaired image-to-image translation without pixel-level labels. To address the limitations of pixel or distribution-based metrics in assessing semantic-level cartographic features, we employ the Map Feature Perception Metric (MFP). This metric complements traditional evaluations by explicitly measuring global semantic alignment and spatial consistency between synthesized and reference maps. The MFP leverages a self-supervised Vision Transformer (ViT) \cite{Dosovitskiy2020imageworth16x16} pretrained with DINO \cite{Caron2021EmergingPropertiesSelfSupervised} to extract deep features, comprising two components:
\begin{itemize}
    \item \textbf{Global Feature Evaluation}: Captures holistic cartographic attributes (e.g., scene categories, element distributions) using the [CLS] token embeddings from the final ViT layer:
    \begin{equation}
        \mathcal{L}_G = 1 - \text{MSE}(\text{CLS}_o, \text{CLS}_t),
    \end{equation}
    where $\text{CLS}_o$ and $\text{CLS}_t$ denote embeddings for generated and target maps, respectively.
    \item \textbf{Spatial Similarity Evaluation}: Quantifies local geometric relationships via self-attention keys from the final ViT layer:
    \begin{equation}
        \mathcal{L}_S = \left\| S(I_o) - S(I_t) \right\|_F,
    \end{equation}
    where $S(I) = \text{cos\_sim}(K^L(I))$ computes the cosine similarity matrix of patch-wise semantic correlations.
\end{itemize}

The integrated MFP is defined as:
\begin{equation}
    \text{MFP} = \lambda_1 \mathcal{L}_G + \lambda_2 \mathcal{L}_S,
\end{equation}
with $\lambda_1=10$ and $\lambda_2=1$ empirically determined. Lower MFP values indicate superior semantic fidelity and spatial coherence.

We combine FID, PSNR, and SSIM with MFP to evaluate structural coherence, style fidelity, and semantic plausibility holistically.

\section{Experiment}
\subsection{Experimental setup}
\subsubsection{Datasets}\label{sec:Datasets}
In this pursuit, we have established a novel benchmark dataset CSCMG to validate the proposed C2GM framework and stimulate future research in large-scale remote sensing image super-resolution. The data were meticulously collected from the Glasgow area in the UK, ensuring that the sampling regions for the test and training tile sets were distinctly delineated and non-overlapping. Representative image pairs are illustrated in Fig. \ref{fig:dataset-example}.

\begin{table}[ht]
    \caption{Detailed information for the CSCMG dataset.}
	\label{tab:cmmgd_dataset}
    \centering
    \renewcommand{\arraystretch}{1.2}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cccccc}
    \hline
    \multirow{2}{*}{Level} & \multicolumn{2}{c}{Tile pair number} & \multirow{2}{*}{Scale} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Resolution\\ (m/pixel)\end{tabular}} & \multirow{2}{*}{Example features} \\
        & Training set & Testing set &  &  &  \\ \hline
    14 & 4870 & 280 & 1:35000 & 9.555 & village, or suburb \\
    15 & 15106 & 292 & 1:15000 & 4.777 & small road \\
    16 & 30040 & 300 & 1:8000 & 2.389 & street \\
    17 & 40555 & 300 & 1:4000 & 1.194 & block, park, addresses \\
    18 & 45000 & 300 & 1:2000 & 0.597 & some buildings, trees \\
    All & 135571 & 1471 & - & - & - \\ \hline
    \end{tabular}
    }
\end{table}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{dataset-cascade.pdf}
    \caption{Examples of cascade references from levels 16 to 18 in the CSCMG dataset. The dataset provides 2X or 4X cascade references, with image resolutions of 128 and 64.}
    \label{fig:dataset-cascade}
\end{figure}

CSCMG comprises pairs of remote sensing and map tiles across various scales, ranging from levels 14 to 18, as detailed in Table \ref{tab:cmmgd_dataset}. The dataset includes 137,042 sample tiles, with 135,571 allocated for training and 1,471 designated for testing. All tiles were sourced from Google Maps' free tile service. The extensive multi-scale tile samples within the dataset enable robust validation of multi-scale map generation capabilities.

Tiles span from level 18 down to level 14, encompassing the entire spatial extent of all sub-level tiles structured through a quadtree division. This design enhances both the training and testing processes of the cascade map generation model. The dataset offers cascade training data at 2X and 4X, providing cascading references covering either one or two levels. As illustrated in Fig. \ref{fig:dataset-cascade}, the image resolutions of the reference tiles are either 128 or 64, upscaled to 256 to ensure uniform tile sizes while preserving the spatial resolution of the original references. This approach facilitates the validation of the efficacy of multi-scale cascading generation. Bi-cubic resampling is employed to maintain a 2X or 4X spatial resolution difference between the map tiles and cascading reference tiles.

To evaluate our method's multi-scale map generation capabilities, we utilize the multi-scale map generation dataset MLMG constructed by the state-of-the-art (SOTA) method LACG \cite{Fu2023Levelawareconsistentmultilevel}. The MLMG dataset assesses multi-scale map generation techniques and includes tiles from diverse urban environments such as forests, rivers, streets, residential areas, industrial zones, water bodies, and squares. The dataset comprises samples collected from cities in the United States and China, reflecting significant differences in their respective images and maps. In China, roads are typically wider with lower distribution density, lacking a clear parallel-vertical relationship, whereas in the United States, roads are generally straight and densely packed, with the ground neatly segmented into blocks. The dataset from the U.S. city is MLMG-US, while the dataset from the Chinese city is MLMG-CN. Both MLMG-US and MLMG-CN contain equal samples and levels, featuring multi-scale remote sensing images paired with corresponding map tiles and element labels.

\subsubsection{Implementation details}
Our proposed C2GM framework is based on the Stable Diffusion 2.1-base model. The SGDM model is implemented using the PyTorch framework, and all experiments are conducted on two NVIDIA GeForce RTX 3090 GPUs. The training process leverages cascaded map tiles spanning levels 14 to 18 from the CSCMG dataset.

The training procedure is divided into two distinct stages. In the first stage, the Stable Diffusion model is fine-tuned using the loss function defined in Section \ref{sec:loss}. During this phase, the parameters of the Map Feature Encoder (MFEncoder), Scale Feature Adapter (SFAdapter), and the linear layers of the U-Net are optimized. The model consists of 152 million trainable parameters, with a total size of 1.4 billion, and employs a hierarchical architecture that reduces computational redundancy through parameter sharing across scales. Training proceeds over 200,000 steps with a batch size of 20, utilizing the AdamW optimizer with a learning rate of \(5 \times 10^{-5}\), and achieves convergence within 480 GPU-hours using dual RTX 3090 GPUs.

C2GM demonstrates enhanced computational efficiency through latent space compression and cascaded generation. A single 256×256 tile is generated in 1.2 seconds on two RTX 3090 GPUs, achieving a total of 3.2 trillion floating-point operations per iteration, a 42\% reduction compared to standard diffusion models. For practical emergency mapping at a 1:50,000 scale, covering zoom levels 10 to 16 and encompassing 441 tiles, the end-to-end generation completes in approximately 9 minutes. Although diffusion models inherently exhibit higher complexity than GANs due to iterative denoising, our architectural optimizations, such as multi-scale feature reuse and CLIP-guided early stopping, effectively mitigate computational overhead.

\begin{figure*}[htp]
    \centering
    \includegraphics[width=\linewidth]{experiment-qualitative.pdf}
    \caption{Visualized results of map generation on the MLMG-US test set using our method and analogous approaches are presented. However, due to the limitation of reference information, only the results for levels 17 and 18 are available in section (k) of the figure. Our results demonstrate superior accuracy in map scale representation and enhanced visual continuity of the combined tile map.}
    \label{fig:experiment-qualitative}
\end{figure*}

\subsection{Experimental results}
To substantiate the efficacy of our technique, we compared our method against established image-to-image conversion methods (Pix2pix \cite{Isola2017ImagetoImageTranslationConditional}, Pix2pixHD \cite{Wang2018HighResolutionImageSynthesis}, CycleGAN \cite{Zhu2017UnpairedImagetoImageTranslation}, SPADE \cite{Park2019SemanticImageSynthesis}), map generation techniques grounded in remote sensing imagery (SMAPGAN \cite{Chen2021SMAPGANgenerativeadversarial}, Creative GAN \cite{Fu2021TranslationAerialImage}), and the current state-of-the-art map generation method, LACG \cite{Fu2023Levelawareconsistentmultilevel}.

\subsubsection{Quantitative evaluation}
We employed the MLMG to evaluate these models individually, with the results presented in Table \ref{tab:comparison}. Regarding the FID and PSNR metrics, our method consistently achieved optimal outcomes on average. Specifically, on the MLMG-CN dataset, the proposed method recorded an FID index of 103.99 and a PSNR index of 29.759, outperforming other methods. Similarly, the MLMG-US dataset achieved an FID index of 104.63 and a PSNR index of 28.012, demonstrating superior performance. These results indicate that our method surpasses others in these image quality evaluation metrics, effectively validating its efficacy. Notably, the FID results highlight a significant advantage, suggesting that the distribution difference between the maps generated by our method and the real maps is minimal, thus reflecting high-quality output.

Furthermore, we identified suboptimal results in the testing outcomes; for instance, the LCAG method exhibited a suboptimal performance in the FID metric, underscoring the substantial advantage of our method in terms of perceived quality compared to LCAG. In the context of the PSNR metric, our method outperformed the suboptimal SMAPGAN method, indicating higher similarity in the generated maps. However, it is important to note that some researchers have pointed out that a higher PSNR does not necessarily correlate with improved visual quality \cite{Ledig2017PhotoRealisticSingleImage,Pei2018DoesHazeRemoval}. We opted to retain this metric as it provides a relative measure of the generated map quality across different methods and is a valuable reference in ablation studies.

We attribute the discrepancies observed in test results between the MLMG-CN and MLMG-US datasets to the inherent differences in the datasets themselves. The MLMG-CN dataset is derived from map data of Chinese cities, while the MLMG-US dataset is based on American city map data. The geographic information within these two datasets varies significantly, resulting in differing test outcomes. Notably, the map data from American cities may exhibit more complex geographic features, whereas the ground features in Chinese cities tend to be more regular, contributing to the lower performance observed in the MLMG-US dataset compared to the MLMG-CN dataset.

\begin{table}[t]
    \caption{Quantitative comparison of disparate \\ methodologies on the MLMG dataset.}
    \label{tab:comparison}
    \centering
    \begin{tabular}{ccccc}
        \hline
        \multirow{2}{*}{Model} & \multicolumn{2}{c}{MLMG-CN} & \multicolumn{2}{c}{MLMG-UN} \\
        &  FID$\downarrow$ & PSNR$\uparrow$  &  FID$\downarrow$ & PSNR$\uparrow$  \\
        \hline
        Pix2Pix & 227.31 &  23.16  &  248.66 &  26.406 \\
        Pix2PixHD   & 195.48 &  23.044 &  188.22 &  25.297 \\
        CycleGAN    & 213.99 &  22.515 &  153  & 24.85  \\
        SPADE  & 332.78 &  23.166 &  304.46 &  25.948 \\
        SelectionGAN  &  261.07 &  23.169 &  260.18 &  25.563 \\
        SMAPGAN & 290.65 &  \underline{24.918} &  347.38 &  \underline{27.564} \\
        CreativeGAN & 172.74 &  23.204 &  149.34 &  25.683 \\
        LCAG   & \underline{134.05} &  23.876 &  \underline{125.8}  &  25.409 \\
        C2GM-2X & 114.21 &  29.204 &  124.46 &  27.318 \\
        C2GM-4X & \textbf{103.99} & \textbf{29.759} & \textbf{104.63} & \textbf{28.012} \\
        \hline
    \end{tabular}
\end{table}

\subsubsection{Qualitative evaluation}
We conducted a visual comparison with all comparative models. Fig. \ref{fig:experiment-qualitative} shows that most methods have difficulty generating accurate and continuous cartographic features, such as roads, buildings, and vegetation. We find that multi-scale results from single-scale methods exhibit more ground object errors, feature breaks, and discontinuous tile noise, particularly at higher levels. Map generation methods designed for multi-scale map generation, such as SMAPGAN, CreativeGAN, and the SOTA method LCAG, perform well in multi-scale scale expression. However, their tile results have obvious visual discontinuity problems after map splicing, and the features of the objects they express also have fractures and noise. Compared with other methods, our proposed method is significantly better in terms of the integrity of geographical features and the visual continuity of map tiles. Our method is closer to the ground truth (GT) than other methods, especially in the high-level tile map generation results.

Vertical comparison of the generation outcomes at different levels reveals that other methods produce similar ground object representations, highlighting a problem of inconsistency between map synthesis and map scale. In contrast, the maps generated by our proposed method demonstrate superior synthesis features across various levels. For instance, in the 18-level and 16-level maps, our method showcases more pronounced differences in ground object representation compared to other methods, effectively conveying more accurate map synthesis scale information. These results underscore that our method significantly outperforms the comparison methods in generating multi-scale maps from remote sensing imagery, thereby validating the effectiveness of our approach.

\begin{table}[t]
    \caption{Comparison results of two groups of indicators of different model generation effects}
    \label{tab:mfp}
    \centering
    \begin{tabular}{ccccc}
        \hline
        Model   & FID↓  & SSIM↑ & PSNR↑& MFP↑   \\
        \hline
        TSIT    & 109.25 & 0.7142 & 24.87 & 0.5910  \\
        C2GM    & 31.60  & 0.7503 & 24.79 & 0.7351  \\
        \hline
    \end{tabular}
\end{table}

\subsubsection{Map features evaluation}
To comprehensively assess the interpretability and spatial consistency of C2GM, we conducted a map feature evaluation using the MFP metric (Section \ref{sec:metrics}) on the CSCMG dataset. The results are shown in Table \ref{tab:mfp}. The MFP metric is specifically designed to evaluate the semantic alignment and spatial consistency of generated maps, offering a more comprehensive assessment than traditional metrics. C2GM achieves an MFP score of 0.7351, surpassing TSIT's score of 0.5910, which indicates enhanced semantic alignment and spatial consistency. The metric's two components—global feature similarity ($\mathcal{L}_G$) and spatial correlation ($\mathcal{L}_S$)—effectively capture C2GM's ability to preserve cartographic semantics, such as road networks and building distributions, and geometric relationships.

C2GM also excels in traditional metrics such as FID (31.60 compared to TSIT's 109.25) and SSIM (0.7503 compared to 0.7142), confirming its structural coherence and alignment with real map distributions. Although the PSNR values are similar (24.79 compared to 24.87), the MFP metric underscores C2GM's semantic advantages that traditional feature-level metrics do not capture. Specifically, the sensitivity of the MFP to spatial-topological errors, quantified through self-attention key correlations, provides critical insights beyond those offered by traditional metrics. It establishes C2GM as a robust solution for high-fidelity map generation, supported by both quantitative rigor and qualitative plausibility.

\subsection{Ablation studies}
The ablation experiments are divided into two parts. Experiment 1 investigates the impact of incorporating scale information (CLIPEncoder) into the Baseline, illustrating how scale information guidance affects the generated map. Experiment 2 examines the effect of adding cascade references (MFEncoder) to the Baseline, emphasizing the role of tile cascade generation in the map output. Results from these ablation experiments, displayed in Table \ref{tab:ablation} and Fig. \ref{fig:experiment-ablation}, compare the generation outcomes of the Baseline method with those of the proposed method under 2X and 4X cascade references for map denoising. These experiments validate the effectiveness of each component in our approach and identify a more effective span for cascade generation.

\begin{figure*}[htp]
    \centering
    \includegraphics[width=\linewidth]{experiment-ablation.pdf}
    \caption{Visual results of the ablation study on the CSCMG dataset are presented. Results of our method are visualized after incrementally adding modules. These results demonstrate that the proposed method can generate high-quality and continuous tile map with the assistance of the cascade reference (MFEncoder) and scale information reference (CLIPEncoder).}
    \label{fig:experiment-ablation}
\end{figure*}

\begin{table}[t]
    \caption{Ablation results for Cascade Reference (MFEncoder) and Scale Information (CLIPEncoder) on the CSCMG dataset.}
    \label{tab:ablation}
    \centering
    \begin{tabular}{ccccc}
        \hline
        +MFEncoder & +CLIPEncoder & PSNR$\uparrow$ & SSIM$\uparrow$ & FID$\downarrow$ \\
        \hline
        $\times$ & $\times$ & 19.962 & 0.873 & 52.543 \\
        $\times$ & \checkmark & 20.898 & 0.882 & 44.217 \\
        2X & $\times$ & 23.096 & 0.934 & 45.512 \\
        2X & \checkmark & 31.069 & 0.941 & 50.052 \\
        4X & $\times$ & 24.886 & 0.915 & 44.329 \\
        4X & \checkmark & \textbf{31.584} & \textbf{0.935} & \textbf{34.369} \\
        \hline
    \end{tabular}
\end{table}

\subsubsection{Ablation study of cascade feature encoder}
Adding cascade references to the multi-branch cartographic feature encoder (MFEncoder) aims to fuse low-level map information, further ensuring the consistency of the main content. As indicated in Table \ref{tab:ablation}, FID scores for generated maps with 2X and 4X cascade references are 45.512 and 44.329, respectively, while PSNR scores are 23.096 and 24.886. Compared to the scenario without cascade references, these FID scores reflect increases of 7.031 and 8.241, and PSNR scores show improvements of 3.134 and 4.924, respectively. Such results demonstrate that cascade generation significantly enhances the quality of the generated maps. Furthermore, an improved SSIM index indicates enhanced structural similarity of the generated maps, leading to more accurate outputs.

From Figures \ref{fig:experiment-ablation} (a), (c) and (e), it is evident that considering multi-scale information mitigates misjudgments of geographic elements that arise when only current-level data is analyzed. Visual discontinuities of geographic elements in the original map have been corrected, bringing the generated map closer to the actual representation. Results across different levels illustrate that employing a multi-branch cartographic feature encoder with cascade references allows for integrating single-level generated maps with information from other levels. This integration enables the cartographic model to better recognize objects' comprehensive features across various scales, thus acquiring more extensive cartographic expertise.

Moreover, Figures \ref{fig:experiment-ablation} (d) and (f) demonstrate that comparing the 2X and 4X cascade generation results from the ablation tests reveals superior outcomes for the 4X cascade generation in terms of FID, PSNR, and SSIM metrics. A broader cascade generation span facilitates the model's ability to learn comprehensive map features more effectively, resulting in more accurate maps. The performance of the 2X cascade generation may be limited by its narrower span, which could restrict the diversity of comprehensive map features the generation model can learn. Additionally, relatively similar cascade references may interfere with the terrain features extracted from the images, thereby diminishing the quality enhancement effect of the map cascade generation.

\subsubsection{Ablation study of scale feature adapter}
The scale feature adapter is designed to utilize scale information, enabling the generator to optimize output mapping at each scale information and accurately represent content differences. In a map with a scale of 1:8000, the scale feature adapter effectively removes erroneous elements such as an inappropriate yellow background and nonexistent roads. As shown in Table \ref{tab:ablation}, incorporating the scale feature adapter results in improvements across all metrics compared to the baseline, particularly an 8.3 increase in the FID score, highlighting the significant impact of scale information guidance on accurately representing map features. The CLIP-based scale information encoder translates the textual scale data of the generated map into image features, integrates these with ground features extracted from remote sensing images, and embeds them into the pre-trained generator. This process enables the model to discern comprehensive differences among maps of varying scales, leading to improved generation outcomes at each scale that more accurately reflect the characteristics specific to those scales.

Figure \ref{fig:experiment-ablation} illustrates the results of the ablation experiment. In particular, Figures \ref{fig:experiment-ablation}(a), (c), and (e) display the baseline map denoising results based on remote sensing images, whereas Figures \ref{fig:experiment-ablation}(b), (d), and (f) show the results after integrating scale information. Maps created with scale information demonstrate significant generalization differences at varying scales compared to those generated without it.

Specifically, in Figure \ref{fig:experiment-ablation}(a), the baseline method at a scale of 1:8000 includes superfluous elements like a yellow background and fictitious roads, which are effectively removed after integrating scale information, as seen in Figure \ref{fig:experiment-ablation}(b). Similarly, in Figures \ref{fig:experiment-ablation}(c) and \ref{fig:experiment-ablation}(e), as well as Figures \ref{fig:experiment-ablation}(d) and \ref{fig:experiment-ablation}(f), the baseline method at a scale of 1:2000 incorporates unnecessary buildings and roads, which are also removed after incorporating scale information. Therefore, maps generated by the baseline method exhibit considerable feature discrepancies across different scales, which are effectively corrected by integrating scale information.

This finding indicates that the model can effectively discern differences in map features across scales. Maps generated without scale information exhibit substantial errors, such as incorrect placement of roads and buildings and the presence of non-existent elements. These inaccuracies result from the model’s inability to accurately capture scale-dependent features, leading to incoherent results. Incorporating scale information rectifies these issues, producing a more accurate map at a given scale.

\begin{figure*}[htp]
    \centering
    \includegraphics[width=\linewidth]{discussion-map.pdf}
    \caption{Continuous Range Tile Map Generation: C2GM demonstrates powerful capabilities on generating wide-ranging, multi-scale, unbounded continuous, and virtually limitless tile map.}
    \label{fig:discussion-map}
\end{figure*}

\section{Discussion}
\subsection{Comparison of the map tile continuous of cascade generated}
Notably, the large-scale tile map generation results achieved through the cascading generation method yield high-quality map images. When spliced together, several high-quality generated map tiles form a continuous tile map (Fig. \ref{fig:discussion-map}). Generative models are highly sensitive to the resolution of input and output images. Leveraging higher resolutions to generate large-scale maps can detrimentally affect generative model performance. A common approach involves stitching together small-scale map tiles to form a larger map \cite{Liu2021CscGANconditionalscaleconsistent,Fu2023Levelawareconsistentmultilevel}. However, this technique often results in visual discontinuities between the tiles, particularly in urban areas where building, road, and other geographic feature boundaries can exhibit noticeable breaks and inconsistencies \cite{Chen2021SMAPGANgenerativeadversarial}. This phenomenon is especially pronounced across different map tiles, as they may result from varying generation processes or possess diverse feature distributions. To mitigate this issue, we have adopted a cascaded generation method for producing large-scale maps.

Although we employed a tile-by-tile generation approach, it is evident that the proposed method produces continuous surface features and maintains stylistic similarities between tiles (Fig. \ref{fig:discussion-map}). Our findings demonstrate that the cascading generation strategy reduces the visual discontinuity typically associated with tile splicing, making large-scale and multi-scale map generation easier. Furthermore, as illustrated in Fig. \ref{fig:experiment-qualitative}(k), the different scale maps generated for the same area exhibit varying degrees of comprehensiveness; for instance, the 18-level generated maps preserve more intricate details compared to the 16-level maps, much like authentic maps.

\subsection{Comparison of the feature generalization of multi-scale generated}
The experimental results of multi-scale map generation indicate that using low-level cascade reference map tiles in model training enhances the model's generalization capability at varying scales (Fig. \ref{fig:discussion-map}). Consequently, the proposed model can use these generated low-level map tiles as input to produce tile maps with higher resolutions and richer details. Figure \ref{fig:experiment-ablation}(f) at map level 16-18 illustrates the multi-scale cascade generation results of C2GM on the CSCMG dataset. Initially, the model is conditioned on existing map tiles at a scale of 1:16000, ultimately generating maps with scales ranging from 1:16000 to 1:2000 based on remote sensing images and cascade references. In contrast to methods that use fixed map layers as constraints, C2GM more effectively captures geographical feature differences between layers through a cascaded reference layers approach, resulting in higher-quality map generation \cite{Liu2021CscGANconditionalscaleconsistent,Fu2023Levelawareconsistentmultilevel}. This distinctive approach facilitates the creation of high-quality, multi-scale, large-scale tile maps that encompass diverse geographical features and scenes.

As a foundation generation model, C2GM unveils new avenues for constructing generative world models within geographic contexts by simulating the cartographic generalization process employed by cartographers during the multi-scale mapping endeavor. The model's ability to generate maps at different scales, such as 1:2000 and 1:8000, demonstrates its potential for producing high-quality maps with varying levels of detail (Fig. \ref{fig:discussion-map}). The generated maps exhibit a high degree of realism and continuity, showcasing the model's effectiveness in capturing the intricate features of the input remote sensing images. This capability is particularly valuable for applications in urban planning, environmental monitoring, and disaster response, where accurate and detailed maps are essential.

\subsection{Limitations and environmental adaptability}
The proposed framework demonstrates distinct advantages in urban cartographic scenarios with significant multi-scale generalization requirements (levels 14–18), where artificial features (e.g., buildings, roads) necessitate explicit cartographic operations like aggregation and simplification. However, limitations emerge in natural landscapes and small-scale mapping (levels 1–12). For natural features (e.g., forests, terrain), scale transitions often involve gradual geometric smoothing rather than structural reorganization, reducing the efficacy of cascade learning due to inherent self-similarity across scales. Moreover, tile-based generation relies on localized spatial coherence—a premise valid for structured urban layouts but less so for expansive natural terrains with long-range dependencies.

At small scales (levels 1–12), extreme geometric abstraction transforms features into symbolic representations, creating a resolution gap that challenges pixel-level cascade learning. The current framework’s dependency on explicit feature discrepancies limits its capacity to address such abstraction. While CLIP-based scale encoding partially mitigates global inconsistencies, terrain-aware attention mechanisms remain absent, hindering geomorphological coherence. Future work should explore hybrid vector-raster paradigms to bridge symbolic-geometric disparities and integrate topographic constraints for enhanced natural scene modeling. These adaptations would extend the framework’s applicability while preserving its strengths in urban multi-scale cartography.

\section{Conclusion}
This paper introduces C2GM, an innovative foundational framework for generative cartography designed to create multi-scale tile map from remote sensing images. C2GM produces high-quality tile map characterized by rich geographical diversity, seamless visual continuity, and multi-scale representations by enhancing existing generative models. It pioneers multi-scale map synthesis through three synergistic innovations: CLIP-guided multi-scale cascade generation, which integrates cartographic expertise into the diffusion process; self-cascading tile generation with cross-tile consistency constraints to resolve edge discontinuities; and dual-branch scale adaptation for geographically-aware feature extraction. Additionally, we developed the CSCMG dataset, which incorporates cascaded reference and scale information to support the training and evaluation of multi-scale map generation models. The quality of the model's map generation was evaluated using the MFP metric through feature-level perception. Comprehensive experiments demonstrated C2GM's exceptional capability in generating multi-scale tile maps.


% Numbered list
% Use the style of numbering in square brackets.
% If nothing is used, default style will be taken.
%\begin{enumerate}[a)]
%\item
%\item
%\item
%\end{enumerate}

% Unnumbered list
%\begin{itemize}
%\item
%\item
%\item
%\end{itemize}

% Description list
%\begin{description}
%\item[]
%\item[]
%\item[]
%\end{description}

% Figure
% \begin{figure}[<options>]
% 	\centering
% 		\includegraphics[<options>]{}
% 	  \caption{}\label{fig1}
% \end{figure}


% \begin{table}[<options>]
% \caption{}\label{tbl1}
% \begin{tabular*}{\tblwidth}{@{}LL@{}}
% \toprule
%   &  \\ % Table header row
% \midrule
%  & \\
%  & \\
%  & \\
%  & \\
% \bottomrule
% \end{tabular*}
% \end{table}

% Uncomment and use as the case may be
%\begin{theorem}
%\end{theorem}

% Uncomment and use as the case may be
%\begin{lemma}
%\end{lemma}

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix



% To print the credit authorship contribution details
% \printcredits

%% Loading bibliography style file
\bibliographystyle{model1-num-names}
%\bibliographystyle{cas-model2-names}

% Loading bibliography database
\bibliography{refs}

% Biography
% \bio{}
% % Here goes the biography details.
% \endbio

% \bio{pic1}
% % Here goes the biography details.
% \endbio

\end{document}

