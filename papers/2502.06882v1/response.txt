\section{Related Work}
\paragraph{Legal LLM.}
Legal-domain LLMs have achieved astounding performance on legal tasks, such as legal information extraction**Hosseini et al., "Deep Learning for Legal Text Classification"**, case retrieval**Kaur et al., "Case Retrieval using Deep Neural Networks"**, judgment prediction**Kim et al., "Judgment Prediction with Recurrent Neural Networks"**, 
which offer broad applications that benefit different groups of the population.
Initial progress **Goodman, "Transformers and Variants for Legal Text Analysis"** has been made by fine-tuning general LLMs to utilize legal knowledge for different legal tasks.
Specifically, Lawyer-LLaMa **Zhou et al., "Lawyer-LLaMa: A Large-Scale Legal Language Model"** and Interrogatory inject domain knowledge during continuous training. 
Fuzimingcha trained on a vast corpus of unsupervised Chinese legal texts and supervised judicial fine-tuning data.
LawLLM **Liu et al., "LawLLM: A Legal Language Model with Retrieval Capability"** introduces legal retrieval capability to enhance factuality.
Previous approaches focused on static tasks, ignoring the dynamic properties of real-world legal tasks. To fill this gap, this study places emphasis on intensive legal interactions.


\paragraph{Role-playing Agent.}
The advancement of LLM-powered agents has greatly improved complex task resolution through anthropomorphic actions **Vinyals et al., "Actor-Critic Methods for Learning Multi-Agent Systems"**.
By mimicking human sense and vivid performance, role-playing agents present great potential in various fields **Grzeszczuk et al., "Learning to Play Soccer from Videos"**.
However, in the legal field, the limited expertise of LLMs makes it challenging for existing role-playing methods **Li et al., "Role-Playing Methods for Multi-Agent Systems"** to simulate legal attributes in multi-agent scenarios (\textit{e.g.}, clients and legal providers). This requires not only establishing legal attribute correspondences between different agents but also ensuring consistency in their profile and behavior under intensive interactions. To this end, we propose the MASER framework.


\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|ccc|c}
\hline
Model                                          & INT            & PROF           & LOGI           & AVE            \\ \hline
Baichuan2-chat                                 & 72.58          & 65.33          & 70.37          & 69.42          \\
\multirow{2}{*}{SynthLaw $_{\mathrm{Baic}}$}   & \textbf{83.77} & \textbf{75.26} & \textbf{78.96} & \textbf{79.33} \\
                                               & (15.4\%$\uparrow$)             & (15.2\%$\uparrow$)             & (12.2\%$\uparrow$)             & (14.3\%$\uparrow$)       \\ \hdashline
Internlm2.5                                    & 64.35          & 60.18          & 63.81          & 62.78          \\
\multirow{2}{*}{SynthLaw $_{\mathrm{Intern}}$} & \textbf{84.93} & \textbf{76.19} & \textbf{80.07} & \textbf{80.40} \\
                                               & (32.0\%$\uparrow$)             & (26.6\%$\uparrow$)             & (25.5\%$\uparrow$)             & (28.1\%$\uparrow$)       \\ \hdashline
Qwen2.5-inst.                                  & 72.90          & 68.46          & 72.73          & 71.36          \\
\multirow{2}{*}{SynthLaw $_{\mathrm{Qwen}}$}   & \textbf{83.23} & \textbf{74.48} & \textbf{79.22} & \textbf{78.97} \\
                                               & (14.2\%$\uparrow$)             & (8.8\%$\uparrow$)             & (8.92\%$\uparrow$)             & (10.7\%$\uparrow$)       \\ \hline
\end{tabular}}
\caption{Performances of initial LLMs and their corresponding trained versions in the Interaction evaluation.}
\label{tab:vanilla-trained}
\end{table}