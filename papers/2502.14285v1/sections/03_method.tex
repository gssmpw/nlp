

\section{Data Consturction}
% In this section, we first introduce the real-word scenario of the prompt stealing and how we build the benchmark to imitate this scenario.
% Based on the fused output prefixes, we propose two jailbreak attack approaches. 
% To identify the security vulnerability of image generation models to the prompt stealer attacks, we propose EvoStealer, a new prompt stealing method, which includes three components of image element extraction, differential evolution, and fitness function.
% The details are introduced below.

% 在这一部分，我们首先介绍prompt template stealing的威胁模型，详细介绍攻击者的攻击基础，限制条件和攻击目标。之后，我们介绍如何构建benchmark去模仿这一个攻击场景。详细的介绍如下。
In this section, we introduce the threat model of prompt template stealing, providing a detailed description of the attacker's existing conditions, constraints, and objectives. We then detail our methodology for developing \data, a comprehensive benchmark designed to realistically simulate this attack scenario.  The specifics are presented below.

\subsection{Threat Model} \label{sec_threat_model}
The attack scenario is grounded in real-world applications. Attackers have access to two pieces of information from the prompt trading platform: 9 sample images and the generative model (e.g., DALL-E 3~\footnote{https://openai.com/index/dall-e-3/} or Midjourney~\footnote{https://www.midjourney.com/home}). While attackers can interact with the model via an API, they are not privy to its internal parameters. Their objectives are twofold: first, to generate images that closely resemble, or even replicate, the sample images by using the same subject with the stolen prompt template; and second, to alter the subject within the template and generate images that retain the same style as the sample images.




\subsection{Benchmark Construction} \label{sec_data_collection}
\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/framework.pdf}
    \caption{The key steps of EvoStealer in differential evolution, including the identification of differences and commonalities, mutation, mutation addition, and crossover operations.}
    \label{fig:framework}
\end{figure*}
% Currently, no specialized benchmark exists for prompt template stealing research. To address this gap, we introduce \data, a novel benchmark comprising 50 freely available prompt templates sourced from PromptBase. These templates are divided into 25 Easy and 25 Hard instances, spanning nine distinct subject areas. We utilize DALL·E 3 as our primary model, combining each prompt template with the 9 subjects to generate 450 unique images.
% To mitigate the inherent stochasticity of text-to-image models, we implement a rigorous manual review process. Image quality assessment focuses on two critical dimensions: (1) the alignment between the generated image subject and the prompt subject, and (2) the stylistic consistency across all 9 images generated from each prompt template. Within each set, five in-domain images evaluate similarity between stolen and original prompts, while four out-of-domain images assess prompt generalization across subjects. For detailed construction information, please refer to Appendix~\ref{app_data}.

Currently, no specialized benchmark exists for prompt template stealing research. To address this gap, we introduce \data, a novel benchmark comprising 50 freely available prompt templates sourced from PromptBase and LaPrompt. These templates are divided into two equal groups of 25 templates each, categorized as "Easy" and "Hard" based on complexity. Each group encompasses 9 distinct subject categories.
We utilize DALL·E 3 as our generation model, combining each prompt template with the 9 subjects to produce 450 unique images.
To ensure quality control, we implemented a comprehensive manual review process focusing on two key criteria: subject-prompt alignment and stylistic consistency across template-generated images. For each group, we designated the first 5 generated images as in-domain data to assess similarity between original and stolen prompt. The remaining 4 images serve as out-of-domain data, enabling evaluation of prompt template generalization capabilities across diverse subjects. For comprehensive details on the benchmark construction methodology, please refer to Appendix~\ref{app_data}.

\section{EvoStealer}
In this section, we introduce the three main steps of EvoStealer: Image Element Extraction, Differential Evolution, and Fitness Function. The details are presented below.
% \subsection{EvoStealer}



\subsection{Image Element Extraction}
High-quality prompts for text-to-image generation typically consist of a subject and several modifiers~\citep{liu2022design,oppenlaender2024taxonomy}. The subject defines the object or scene depicted in the image, such as \textit{"a woman with a flower crown"} or a more intricate description like \textit{"Woodland creatures gather around a shimmering pond, surrounded by trees and glowing flowers, creating a peaceful scene"}. The modifiers specify the style of the image, including aspects such as artistic style and resolution. While multimodal models can accurately identify simple subjects, they often misinterpret complex subjects, mistakenly treating parts of the subject as style modifiers. For instance, in the case of the complex subject "\textit{peaceful scene}", the model may misinterpret "\textit{peaceful}" as a style modifier, contaminating the intended description.

% To mitigate this issue, we define an image element extraction pattern: <\textbf{Subject, Modifiers, Supplements}>. The subject continues to describe the object or scene, while the modifiers are categorized into four distinct groups.
% \begin{enumerate}
%     \item Artistic Style: Include Genre, Era or Historical Style, Cultural and Technological Style.
%     \item Visual Composition and Structure: Include Composition and Layout, Form and Structure, Scale, Movement, Perspective, Pattern and Ornamentation and Detail Level.
%     \item Aesthetic and Emotional Atmosphere: Include Tone and Atmosphere, Emotional Atmosphere, Lighting and Shadow Effects,
%     \item Medium and Material: Include Medium, Material, Technique, Texture, Surface, Color Palette, Brushwork, Line Quality, Strokes, Layering, Transparency, Opacity and Resolution.
% \end{enumerate}

To address this issue, we define an image element extraction pattern: <Subject, Modifiers, Supplements>. The subject describes the object or scene, while the modifiers are categorized into four types: \textit{Artistic Style, Visual Composition and Structure, Aesthetic and Emotional Atmosphere}, and \textit{Medium and Material}. For further details, please refer to Appendix~\ref{app_pattern}. We have imposed the aforementioned constraints on the modifiers to ensure that the model describes the image solely from the relevant perspectives within the four predefined categories. This restriction contributes to the stability and controllability of modifier extraction. However, such a constraint may limit the model’s ability to fully capture the diversity of style features. To mitigate this limitation, we incorporate supplements as a compensatory measure. Supplements encompass descriptions outside the four categories and can include individual words, phrases, or even sentences, such as "\textit{radiating lines suggesting motion}" or "\textit{subtle transitions between colors}".


\input{tables/main_in_domain}
\subsection{Differential Evolution} \label{diffevo}
Firstly, we introduce the theory of differential evolution. The process of generating offspring through the differential evolution algorithm is represented using numerical vectors. Initially, each vector in the population is sequentially selected as the base vector, denoted as $\alpha$. Then, three individuals, $x_{1}, x_{2}, x_{3}$, are randomly chosen from the population to perform the mutation operation. Specifically, the difference between $x_{2}, x_{3}$ is calculated, and this difference undergoes mutation. The mutated difference is then added to $x_{1}$ to produce a new vector, denoted as $\beta$. The mutation operation is mathematically expressed as: $\beta = x_{1} + F(x_{2} - x_{3})$, where $F$ represents the mutation factor, which controls the magnitude of the mutation. Finally, a crossover operation is applied to the vectors $\alpha$ and $\beta$ to generate the offspring.

Figure~\ref{fig:framework} illustrates the differential evolution process implemented in EvoStealer. In Step 1, we differentiate between modifiers and supplements due to their distinct characteristics, particularly in terms of controllability and unpredictability. For modifiers, we focus on identifying the differences between the two sets, while for supplements, we concentrate on their common components. This approach is grounded in the understanding that the uncontrollability of supplements introduces unique features specific to individual images. Additionally, supplements typically contain more tokens than modifiers, which results in a greater influence on the visual representation of the image and, consequently, on the generalization ability of the template. In Step 2, we randomly select an image from the in-domain dataset to influence the mutation process. This strategy serves two purposes: first, it helps filter out modifiers that do not align with the image (e.g., in the case of a surrealistic style image, modifiers such as "cartoon style" are excluded); second, the image, serving as a mutation variable, introduces additional contextual information. As mentioned earlier, the initial version of EvoStealer directly derives image element extraction, which results in an over-reliance on the quality of this extraction. By incorporating the image in Step 2, we enable the population to gain valuable information that may otherwise be overlooked, thus mitigating the drawback of over-dependence on image element extraction. In Step 3, no modifications are made, and the two components are simply combined to generate the mutated description. In Step 4, in contrast to the direct crossover used in genetic algorithms, EvoStealer first identifies the common parts between the two individuals. When generating the new offspring, the common parts are fully inherited, while only the differing parts undergo the crossover operation. This design approach strikes a balance between the exploration and exploitation of the algorithm, facilitating effective exploration while ensuring that generalization constraints are preserved.



\subsection{Fitness Function} \label{section_fitness}
The fitness function is employed to assess the quality of offspring, with those exhibiting higher fitness scores being retained for progression to the next iteration. While the fitness function does not directly influence the offspring generation, it guides the search direction throughout the evolution process. Our fitness function incorporates both the semantic similarity of the text and the style similarity of the image. Specifically, for each offspring (i.e., a prompt template), we sequentially replace the subject within the template and calculate its semantic similarity with the ground truth. Additionally, we randomly select a subject and use the target model (DALL·E 3) to generate the corresponding image, subsequently calculating the similarity between this generated image and the corresponding image from the in-domain dataset. The mathematical formulation is as follows:

\[
\small
\begin{aligned}
F &= \frac{1}{n} \sum_{i=1}^{n} \left( \lambda \left( \frac{\mathbf{T}_{\text{off}}(i) \cdot \mathbf{I}_{\text{gt}}(i)}{\|\mathbf{T}_{\text{off}}(i)\| \|\mathbf{I}_{\text{gt}}(i)\|} \right) \right) \\
  &\quad + (1 - \lambda)  \left( \frac{\mathbf{I}_{\text{off}} \cdot \mathbf{I}_{\text{gt}}}{\|\mathbf{I}_{\text{off}}\| \|\mathbf{I}_{\text{gt}}\|} \right)
\end{aligned}
\]

Where \( \text{off} \) and \( \text{gt} \) denote the offspring and ground truth, respectively, and \( \mathbf{T} \) and \( \mathbf{I} \) represent the text and image embeddings. The parameter \(\lambda\) serves as a balance factor to weight the two similarity measures.



