\section{Experiments}

% We employ \data to evaluate the vulnerability of image generation models to the prompt template stealing.
% We follow the recent work \citep{shen2024prompt, naseh2024iteratively, huang2024vbench} to use the metric subject similarity, style similarity and semantic similarity for measuring the image generation models' safety to prompt template stealing (Section \ref{metric}), which holds higher agreement with human annotations.
% We also conduct the human evaluation to measure the percentage of successful prompt stealing.

We employ \data to evaluate the vulnerability of image generation models to prompt template stealing.
Following recent works~\citep{shen2024prompt, naseh2024iteratively, huang2024vbench}, we employ subject similarity, style similarity, and semantic similarity metrics to evaluate the performance of image generation models against prompt template stealing (Section \ref{metric}). These metrics demonstrate higher agreement with human annotations than previous approaches.
Additionally, we conduct human evaluation to measure the quality of prompt stealing.


% \subsection{Dataset Details}
% \data comprises 50 prompt sets and their corresponding 450 images, with detailed information provided in Section~\ref{sec_data_collection}. To evaluate the two objectives presented in Section~\ref{sec_threat_model}, we divided the 9 sample images into in-domain and out-of-domain categories. The in-domain group consists of 5 images used to assess the similarity between images generated by the stolen prompts and the original images. The out-of-domain group contains 4 images designed to evaluate the generalization capability of the stolen prompts (or prompt templates) to different subjects.

\input{tables/main_out_domain}

\subsection{Baselines}
Our baselines encompass models for both caption generation (BLIP-2) and prompt stealing attack~(CLIP Interrogator and PromptStealer).
\begin{itemize}
    \item \textbf{BLIP-2}: BLIP-2~\citep{li2023blip} is a multimodal model that aligns text with images using a lightweight Querying Transformer to connect a frozen image encoder with LLMs. In this study, we employ the BLIP-2-opt-2.7b model to generate image descriptions.
    \item \textbf{CLIP Interrogator}: CLIP Interrogator~\footnote{https://github.com/pharmapsychotic/clip-interrogator/tree/main} uses CLIP to generate image descriptions, incorporating prompts from preset categories such as artists, flavors, and mediums. It encodes both the image and text with the CLIP model, calculates their similarity, and generates the most matching description.
    \item \textbf{PromptStealer}: PromptStealer~\citep{shen2024prompt} consists of two modules: the Subject Generator, fine-tuned on BLIP to extract image subjects, and the Modifier Detector, a multi-class classifier that selects style modifiers based on similarity to predefined categories. The final prompt is generated by concatenating the subject and selected modifiers.
\end{itemize}


\subsection{Experimental Settings}

Due to the inherent difficulties in subject identification and replacement within BLIP-2-generated prompts, its evaluation is limited to in-domain data only. For both CLIP Interrogator and PromptStealer methods, we first extract subjects and modifiers from 5 in-domain samples and concatenate them to create prompts. We then randomly select a prompt and systematically replace its subject with subjects from the out-of-domain group. For PromptStealer, we maintain a threshold value of 0.6. In EvoStealer's implementation, we extract prompt templates from in-domain data and perform sequential subject substitutions, using 9 different subjects to generate the final prompts. Both the population size and generation count are set to 5, with the temperature parameter set to 0 to ensure consistent results. We employ SigLIP~\citep{zhai2023sigmoid} for fitness score calculations and set \(\lambda\) to 0.5. All image generation is performed using DALL·E 3 with a resolution of 1024×1024 and standard quality settings.

\subsection{Evaluation Metric} \label{metric}
We adopt the evaluation framework proposed by \citet{huang2024vbench} and employ the following metrics to assess the performance of EvoStealer and baseline methods:
\begin{itemize} 
    \item \textbf{Subject Similarity}: To evaluate the similarity between subjects in paired images, we utilize the self-supervised model DINO~\citep{oquab2023dinov2}, as subject comparison is a crucial aspect of image similarity assessment.
    \item \textbf{Style Similarity}: To measure style consistency, we employ CLIP and SigLIP to extract style features from images generated using stolen prompts and compare them with the original images.
    \item \textbf{Semantic Similarity}: To assess prompt similarity, we compute the cosine similarity between embeddings of the stolen and target prompts, generated using CLIP and SigLIP.
    \item \textbf{Human Evaluation}: We recruit 3 external evaluators to rate the similarity between generated and target images on a scale of 1-5, where higher scores indicate greater similarity. For each group, we randomly sample 2 images from both in-domain and out-of-domain categories and calculate average scores. For out-of-domain samples, the evaluation focuses exclusively on style similarity.
\end{itemize}



\subsection{Main Results}
% Table 1 presents the experimental results for four methods. The table clearly shows EvoStealer outperforms the other three methods across all evaluation metrics.

% \paragraph{Performance on in-domain data.} EvoStealer surpasses the second-best method by margins of 5.23\%, 6.58\%, and 18.1\% across three evaluation metrics, achieving an average improvement of 10.1\%. This performance indicates that the templates extracted by EvoStealer generate images with a higher degree of similarity to the originals. Notably, EvoStealer demonstrates exceptional performance in prompt text similarity, significantly outperforming competing methods. This suggests that the stolen prompts closely align with the original prompts, fulfilling a key objective of prompt stealing attacks. In contrast, PromptStealer's performance is comparatively limited, likely due to a mismatch between its fine-tuning data and the experimental data, which underscores a notable limitation of the approach. EvoStealer, on the other hand, leverages MLLMs as image element extractors without requiring fine-tuning, showcasing its broader applicability and robustness.

% \paragraph{Performance on out-of-domain data.} EvoStealer demonstrates superior out-of-domain performance compared to PromptStealer, achieving advantages of 18.37\%, 8.92\%, and 23.48\%, with an average lead of 15.93\%. Furthermore, a comparison of the two methods' average performance on in-domain and out-of-domain data reveals distinct trends: EvoStealer's performance declines by 1.47\%, 1.11\%, 0.77\%, and 1.12\%, whereas PromptStealer's performance drops by 9.17\% and 3.45\%. These results indicate that EvoStealer possesses stronger generalization capabilities than PromptStealer. The primary distinction between the two methods lies in their approaches: PromptStealer extracts common features from in-domain data to construct a prompt template, while EvoStealer focuses on stealing a single image and effectively adapting it. This fundamental difference underscores the divergence in their effectiveness and applicability.

% 表1和表2分别提供了EvoStealer和其他方法在in-domain和out-of-domain数据上的性能比较。总体上看，EvoStealer的整体性能均显著优于其他baselines方法。

% 1. 在in-domain数据上的性能。在Easy和Hard两个类别上，EvoStealer均优于其他类别。以EvoStealer (GPT-4o)为例，EvoStealer在两类数据上分别优于次好的方法CLIP Interrogator 8.63%和6.64%，这说明，EvoStealer能够生成还原度更高的prompt，实现更优秀的窃取。特别是在文本的语义比较上，EvoStealer窃取的prompt均远超其他方法，这主要是因为CLIP Interrogator和PromptStealer仅是粗糙地将[subject]和modifiers拼接的方式合成prompt，缺乏可变性。此外，CLIP的长度限制进一步影响了对修饰词的提取。反之，EvoStealer则没有这种限制，EvoStealer采用迭代的方式去产生多个的候选模板，提高了模板的多样性。这一点和D等人的研究符合。

% 2. 在out-of-domain上的性能。从表2可以清晰地看到EvoStealer的性能均超过其他方法，相对于在in-domain上的表现，EvoStealer在out-of-domain上的优势更大。以EvoStealer (GPT-4o)为例，EvoStealer的领先幅度都在10%以上，这说明EvoStealer窃取的模板能够更好地泛化到其他主体上。此外，对比表1中的结果，一个明显的现象是，在out-of-domain数据上，EvoStealer几乎没有性能衰退。但是CLIP Intergator和PromptStealer分别平均衰退3.60%和2.36%。这是因为EvoStealer提取的多张图片的公共特征，实现的是模板的窃取。

% 3. 不同模型的性能比较。观察表1和表2，整体上，使用GPT-4o性能最好，GPT-4o-mini次之，InternVL2-26B相对差些。但是，3个模型的区别整体不大。这主要是因为EvoStealer主要依靠模型的文本和图像分析能力。这间接说明了EvoStealer具有兼容性，不依赖于一个特定的多模态模型。

Tables~\ref{tb:main_indomain} and~\ref{tb:main_outdomain} present comparative performance evaluations between EvoStealer and baseline methods using both in-domain and out-of-domain data. The results demonstrate that EvoStealer consistently outperforms baseline approaches across all evaluation metrics.

\paragraph{Performance on in-domain data.} EvoStealer outperforms other methods in both the Easy and Hard categories. For example, EvoStealer (GPT-4o) leads the second-best method, CLIP Interrogator, by 8.63\% and 6.64\% on the two datasets, demonstrating its ability to generate more accurate prompt templates and better stealing performance. Notably, EvoStealer excels in textual semantic comparison, as its prompts are significantly more effective. CLIP Interrogator and PromptStealer rely on simple concatenation of [subject] and modifiers, limiting variability. Additionally, CLIP’s length restriction hampers modifier extraction. In contrast, EvoStealer generates diverse templates iteratively, avoiding these limitations. This aligns with the findings of~\citet{naseh2024iteratively}.

\paragraph{Performance on out-of-domain data.} As shown in Table~\ref{tb:main_outdomain}, EvoStealer outperforms other methods, especially on out-of-domain data, where it demonstrates a larger advantage compared to in-domain data. For instance, EvoStealer (GPT-4o) leads by more than 10\% across all data types, indicating better generalization of stolen templates to different subjects. Furthermore, as seen in Table~\ref{tb:main_indomain}, EvoStealer's performance on out-of-domain data shows minimal degradation, while CLIP Interrogator and PromptStealer experience average degradations of 3.60\% and 2.36\%, respectively. This is due to EvoStealer’s effective template stealing by extracting common features across multiple images.

\paragraph{Comparison of performance across different models.} As shown in Tables~\ref{tb:main_indomain} and~\ref{tb:main_outdomain}, GPT-4o outperforms the other models, followed by GPT-4o-mini, with InternVL2-26B performing slightly worse. However, the performance differences among these models are minimal. This is primarily due to EvoStealer's reliance on the models' text and image analysis capabilities, indicating that EvoStealer is highly compatible and not dependent on a specific multimodal model.