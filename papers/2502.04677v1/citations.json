[
  {
    "index": 0,
    "papers": [
      {
        "key": "shen2024fastswitch",
        "author": "Shen, Ao and Li, Zhiyao and Gao, Mingyu",
        "title": "Fastswitch: Optimizing context switching efficiency in fairness-aware large language model serving"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "yu2022orca",
        "author": "Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon",
        "title": "Orca: A distributed serving system for $\\{$Transformer-Based$\\}$ generative models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zhong2024distserve",
        "author": "Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao",
        "title": "Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving"
      },
      {
        "key": "patel2024splitwise",
        "author": "Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, {\\'I}{\\~n}igo and Maleki, Saeed and Bianchini, Ricardo",
        "title": "Splitwise: Efficient generative {LLM} inference using phase splitting"
      },
      {
        "key": "jain2024intelligent",
        "author": "Jain, Kunal and Parayil, Anjaly and Mallick, Ankur and Choukse, Esha and Qin, Xiaoting and Zhang, Jue and Goiri, {\\'I}{\\~n}igo and Wang, Rujia and Bansal, Chetan and R{\\\"u}hle, Victor and others",
        "title": "Intelligent router for {LLM} workloads: Improving performance through workload-aware scheduling"
      },
      {
        "key": "agrawal2024taming",
        "author": "Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and Tumanov, Alexey and Ramjee, Ramachandran",
        "title": "Taming throughput-latency tradeoff in {LLM} inference with sarathi-serve"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zheng2024sglang",
        "author": "Zheng, Lianmin and Yin, Liangsheng and Xie, Zhiqiang and Sun, Chuyue and Huang, Jeff and Yu, Cody Hao and Cao, Shiyi and Kozyrakis, Christos and Stoica, Ion and Gonzalez, Joseph E and others",
        "title": "Sglang: Efficient execution of structured language model programs"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "srivatsa2024preble",
        "author": "Srivatsa, Vikranth and He, Zijian and Abhyankar, Reyna and Li, Dongming and Zhang, Yiying",
        "title": "Preble: Efficient Distributed Prompt Scheduling for {LLM} Serving"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "qin2024mooncake",
        "author": "Qin, Ruoyu and Li, Zheming and He, Weiran and Zhang, Mingxing and Wu, Yongwei and Zheng, Weimin and Xu, Xinran",
        "title": "Mooncake: A kvcache-centric disaggregated architecture for {LLM} serving"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "kim2024effect",
        "author": "Kim, Kyoungmin and Hong, Kijae and Gulcehre, Caglar and Ailamaki, Anastasia",
        "title": "The Effect of Scheduling and Preemption on the Efficiency of {LLM} Inference Serving"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "yang2024queueing",
        "author": "Yang, Yuqing and Xu, Yuedong and Jiao, Lei",
        "title": "A Queueing Theoretic Perspective on Low-Latency LLM Inference with Variable Token Length"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "guldogan2024multi",
        "author": "Guldogan, Ozgur and Kunde, Jackson and Lee, Kangwook and Pedarsani, Ramtin",
        "title": "Multi-Bin Batching for Increasing {LLM} Inference Throughput"
      }
    ]
  }
]