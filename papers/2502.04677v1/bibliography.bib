

@article{yang2024queueing,
  title={A Queueing Theoretic Perspective on Low-Latency LLM Inference with Variable Token Length},
  author={Yang, Yuqing and Xu, Yuedong and Jiao, Lei},
  journal={arXiv preprint arXiv:2407.05347},
  year={2024}
}

@book{garey1979computers,
  title={Computers and intractability},
  author={Garey, Michael R and Johnson, David S},
  volume={174},
  year={1979},
  publisher={freeman San Francisco}
}

@inproceedings{imai2024predicting,
  title={Predicting {LLM} Inference Latency: A Roofline-Driven ML Method},
  author={Imai, Saki and Nakazawa, Rina and Amaral, Marcelo and Choochotkaew, Sunyanan and Chiba, Tatsuhiro},
  booktitle={Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{zheng2024sglang,
  title={Sglang: Efficient execution of structured language model programs},
  author={Zheng, Lianmin and Yin, Liangsheng and Xie, Zhiqiang and Sun, Chuyue and Huang, Jeff and Yu, Cody Hao and Cao, Shiyi and Kozyrakis, Christos and Stoica, Ion and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2312.07104},
  year={2024}
}

@article{liu2024optimizing,
  title={Optimizing {LLM} queries in relational workloads},
  author={Liu, Shu and Biswal, Asim and Cheng, Audrey and Mo, Xiangxi and Cao, Shiyi and Gonzalez, Joseph E and Stoica, Ion and Zaharia, Matei},
  journal={arXiv preprint arXiv:2403.05821},
  year={2024}
}

@article{qin2024mooncake,
  title={Mooncake: A kvcache-centric disaggregated architecture for {LLM} serving},
  author={Qin, Ruoyu and Li, Zheming and He, Weiran and Zhang, Mingxing and Wu, Yongwei and Zheng, Weimin and Xu, Xinran},
  journal={arXiv preprint arXiv:2407.00079},
  year={2024}
}

@article{shen2024fastswitch,
  title={Fastswitch: Optimizing context switching efficiency in fairness-aware large language model serving},
  author={Shen, Ao and Li, Zhiyao and Gao, Mingyu},
  journal={arXiv preprint arXiv:2411.18424},
  year={2024}
}

@inproceedings{yu2022orca,
  title={Orca: A distributed serving system for $\{$Transformer-Based$\}$ generative models},
  author={Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={521--538},
  year={2022}
}

@article{zhong2024distserve,
  title={Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving},
  author={Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao},
  journal={arXiv preprint arXiv:2401.09670},
  year={2024}
}

@inproceedings{patel2024splitwise,
  title={Splitwise: Efficient generative {LLM} inference using phase splitting},
  author={Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, {\'I}{\~n}igo and Maleki, Saeed and Bianchini, Ricardo},
  booktitle={2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)},
  pages={118--132},
  year={2024},
  organization={IEEE}
}

@article{jain2024intelligent,
  title={Intelligent router for {LLM} workloads: Improving performance through workload-aware scheduling},
  author={Jain, Kunal and Parayil, Anjaly and Mallick, Ankur and Choukse, Esha and Qin, Xiaoting and Zhang, Jue and Goiri, {\'I}{\~n}igo and Wang, Rujia and Bansal, Chetan and R{\"u}hle, Victor and others},
  journal={arXiv preprint arXiv:2408.13510},
  year={2024}
}

@article{srivatsa2024preble,
  title={Preble: Efficient Distributed Prompt Scheduling for {LLM} Serving},
  author={Srivatsa, Vikranth and He, Zijian and Abhyankar, Reyna and Li, Dongming and Zhang, Yiying},
  journal={arXiv preprint arXiv:2407.00023},
  year={2024}
}

@article{kim2024effect,
  title={The Effect of Scheduling and Preemption on the Efficiency of {LLM} Inference Serving},
  author={Kim, Kyoungmin and Hong, Kijae and Gulcehre, Caglar and Ailamaki, Anastasia},
  journal={arXiv preprint arXiv:2411.07447},
  year={2024}
}

}

@article{guldogan2024multi,
  title={Multi-Bin Batching for Increasing {LLM} Inference Throughput},
  author={Guldogan, Ozgur and Kunde, Jackson and Lee, Kangwook and Pedarsani, Ramtin},
  journal={arXiv preprint arXiv:2412.04504},
  year={2024}
}

@article{zhang2024personalization,
  title={Personalization of large language models: A survey},
  author={Zhang, Zhehao and Rossi, Ryan A and Kveton, Branislav and Shao, Yijia and Yang, Diyi and Zamani, Hamed and Dernoncourt, Franck and Barrow, Joe and Yu, Tong and Kim, Sungchul and others},
  journal={arXiv preprint arXiv:2411.00027},
  year={2024}
}

@article{zaib2022conversational,
  title={Conversational question answering: A survey},
  author={Zaib, Munazza and Zhang, Wei Emma and Sheng, Quan Z and Mahmood, Adnan and Zhang, Yang},
  journal={Knowledge and Information Systems},
  volume={64},
  number={12},
  pages={3151--3195},
  year={2022},
  publisher={Springer}
}


@book{kirk2022programming,
  title={Programming massively parallel processors: a hands-on approach},
  author={Kirk, David B and Wen-Mei, W Hwu},
  year={2022},
  publisher={Morgan kaufmann}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{wu2024survey,
  title={A survey on large language models for recommendation},
  author={Wu, Likang and Zheng, Zhi and Qiu, Zhaopeng and Wang, Hao and Gu, Hongchao and Shen, Tingjia and Qin, Chuan and Zhu, Chen and Zhu, Hengshu and Liu, Qi and others},
  journal={World Wide Web},
  volume={27},
  number={5},
  pages={60},
  year={2024},
  publisher={Springer}
}

%ttscaling1
@article{snell2024scaling,
  title={Scaling {LLM} test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

%verifier1
@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{360brew,
  title={360Brew : A Decoder-only Foundation Model for Personalized
Ranking and Recommendation},
  author={360Brew Team},
  journal={arXiv preprint arXiv:2501.16450},
  year={2025}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

%deepseekr1
@article{guo2025deepseek,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in {LLM}s via Reinforcement Learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{li2023loogle,
  title={LooGLE: Can Long-Context Language Models Understand Long Contexts?},
  author={Li, Jiaqi and Wang, Mengmeng and Zheng, Zilong and Zhang, Muhan},
  journal={arXiv preprint arXiv:2311.04939},
  year={2023}
}

@article{hao2023toolkengpt,
  title={Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings},
  author={Hao, Shibo and Liu, Tianyang and Wang, Zhen and Hu, Zhiting},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={45870--45894},
  year={2023}
}

@article{agrawal2024taming,
  title={Taming throughput-latency tradeoff in {LLM} inference with sarathi-serve},
  author={Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and Tumanov, Alexey and Ramjee, Ramachandran},
  journal={arXiv preprint arXiv:2403.02310},
  year={2024}
}

@misc{anthropic_prompt_caching,
  author       = {Anthropic},
  title        = {Prompt Caching for Faster, Cheaper {LLM} Inference},
  year         = {2024},
  url          = {https://www.anthropic.com/news/prompt-caching},
  note         = {Accessed: 2025-01-29}
}

@misc{zheng2023efficiently,
      title={Efficiently Programming Large Language Models using SGLang},
      author={Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Jeff Huang and Chuyue Sun and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph E. Gonzalez and Clark Barrett and Ying Sheng},
      year={2023},
      eprint={2312.07104},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{sglang_bench_serving,
  author       = {{SGL}ang},
  title        = {{SGL}ang's serving benchmark utility},
  year         = {2024},
  url          = {https://github.com/sgl-project/sglang/blob/v0.4.1.post1/python/sglang/bench_serving.py},
  note         = {Accessed: 2025-01-29}
}
