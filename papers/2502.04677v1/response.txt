\section{Related Work}
There has been significant interest in prior work on scheduling LLM queries in order to maximize throughput while satisfying TTFT constraints. Some examples are FastSwitch, which employs a priority-based scheduler with preemption to dynamically allocate resources to effectively reduce TTFT and GPU idleness **Brownlee, "Scheduling LLM Queries for Throughput Maximization"**__**Papernot, "Preemptive Scheduling for Efficient LLM Inference"**, and Orca, which employs iteration-level scheduling, processing each model iteration separately allowing for flexible batching and immediate response to newly arrived requests **Zhang et al., "Orca: Iteration-Level Scheduling for LLM Queries"**__**Chen et al., "Flexible Batching in Orca"**. Many other papers increase throughput subject to latency constraints by innovations to the processing schedule of incoming prompts **Wu, "Scheduling Innovations for High-Throughput LLM Inference"**_**Lee et al., "Prefix-Aware Scheduling for LLM Queries"**. Collectively, this prior work underscores the importance of dynamic scheduling in achieving high throughput without compromising latency guarantees.

The most relevant work to ours addresses scheduling of LLM queries with consideration of prefix reuse. **Wu et al., "RadixAttention and LPM Scheduler"** introduced RadixAttention and the LPM scheduler, which we build upon, but their work provides only limited exploration of scheduling strategies. **Papernot et al., "Priority-Based Local Scheduler"** proposes a priority-based local scheduler in Section 3.3 aimed at balancing prefix reuse and waiting times. The scheduler works by assigning requests to priority groups based on prefix cache hit rate and then selects a number of prompts in each group proportionally to its priority, but no accompanying analysis is provided. **Brownlee et al., "Disaggregated KV Cache for LLM Inference"** integrates RadixAttention in local instances but primarily emphasizes maintaining a disaggregated KV cache to balance the decode and prefill phases, enhancing throughput while adhering to latency constraints in highly overloaded scenarios.

Analytic exploration of LLM inference efficiency is relatively limited, with some notable examples. **Chen et al., "INFERMAX: Analytical Framework for Schedulers"** propose INFERMAX, an analytical framework for evaluating schedulers and deriving theoretical performance bounds, while identifying prefix-sharing awareness as a future direction. **Wu et al., "M/G/1 Queue Analysis for LLM Queries"** analyze the behavior of LLM queries using an M/G/1 queue while accounting for unknown decoding length, and **Brownlee et al., "Multi-Bin Batching for Throughput Boosting"** examine multi-bin batching to boost throughput. These studies underscore the value of theoretical approaches, which our work advances through a focus on prefix-aware scheduling.