@article{agrawal2024taming,
  title={Taming throughput-latency tradeoff in {LLM} inference with sarathi-serve},
  author={Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and Tumanov, Alexey and Ramjee, Ramachandran},
  journal={arXiv preprint arXiv:2403.02310},
  year={2024}
}

@article{guldogan2024multi,
  title={Multi-Bin Batching for Increasing {LLM} Inference Throughput},
  author={Guldogan, Ozgur and Kunde, Jackson and Lee, Kangwook and Pedarsani, Ramtin},
  journal={arXiv preprint arXiv:2412.04504},
  year={2024}
}

@article{jain2024intelligent,
  title={Intelligent router for {LLM} workloads: Improving performance through workload-aware scheduling},
  author={Jain, Kunal and Parayil, Anjaly and Mallick, Ankur and Choukse, Esha and Qin, Xiaoting and Zhang, Jue and Goiri, {\'I}{\~n}igo and Wang, Rujia and Bansal, Chetan and R{\"u}hle, Victor and others},
  journal={arXiv preprint arXiv:2408.13510},
  year={2024}
}

@article{kim2024effect,
  title={The Effect of Scheduling and Preemption on the Efficiency of {LLM} Inference Serving},
  author={Kim, Kyoungmin and Hong, Kijae and Gulcehre, Caglar and Ailamaki, Anastasia},
  journal={arXiv preprint arXiv:2411.07447},
  year={2024}
}

}

@inproceedings{patel2024splitwise,
  title={Splitwise: Efficient generative {LLM} inference using phase splitting},
  author={Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, {\'I}{\~n}igo and Maleki, Saeed and Bianchini, Ricardo},
  booktitle={2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)},
  pages={118--132},
  year={2024},
  organization={IEEE}
}

@article{qin2024mooncake,
  title={Mooncake: A kvcache-centric disaggregated architecture for {LLM} serving},
  author={Qin, Ruoyu and Li, Zheming and He, Weiran and Zhang, Mingxing and Wu, Yongwei and Zheng, Weimin and Xu, Xinran},
  journal={arXiv preprint arXiv:2407.00079},
  year={2024}
}

@article{shen2024fastswitch,
  title={Fastswitch: Optimizing context switching efficiency in fairness-aware large language model serving},
  author={Shen, Ao and Li, Zhiyao and Gao, Mingyu},
  journal={arXiv preprint arXiv:2411.18424},
  year={2024}
}

@article{srivatsa2024preble,
  title={Preble: Efficient Distributed Prompt Scheduling for {LLM} Serving},
  author={Srivatsa, Vikranth and He, Zijian and Abhyankar, Reyna and Li, Dongming and Zhang, Yiying},
  journal={arXiv preprint arXiv:2407.00023},
  year={2024}
}

@article{yang2024queueing,
  title={A Queueing Theoretic Perspective on Low-Latency LLM Inference with Variable Token Length},
  author={Yang, Yuqing and Xu, Yuedong and Jiao, Lei},
  journal={arXiv preprint arXiv:2407.05347},
  year={2024}
}

@inproceedings{yu2022orca,
  title={Orca: A distributed serving system for $\{$Transformer-Based$\}$ generative models},
  author={Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={521--538},
  year={2022}
}

@article{zheng2024sglang,
  title={Sglang: Efficient execution of structured language model programs},
  author={Zheng, Lianmin and Yin, Liangsheng and Xie, Zhiqiang and Sun, Chuyue and Huang, Jeff and Yu, Cody Hao and Cao, Shiyi and Kozyrakis, Christos and Stoica, Ion and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2312.07104},
  year={2024}
}

@article{zhong2024distserve,
  title={Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving},
  author={Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao},
  journal={arXiv preprint arXiv:2401.09670},
  year={2024}
}

