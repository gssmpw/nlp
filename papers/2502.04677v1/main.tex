\documentclass{article}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage[numbers]{natbib}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{algorithm,algpseudocode}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[title]{appendix}
\usepackage{bm}
\usepackage{courier}
\usepackage[usenames,dvipsnames]{color}
\usepackage{enumitem}
\usepackage{graphicx}
%\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage[margin=0.9in]{geometry}
\usepackage{url}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage[dvipsnames]{xcolor}
\hypersetup{
	colorlinks,
	linkcolor={red!80!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}
\usepackage{authblk}
\newcommand{\Raj}[1]{\textcolor{red}{Raj: #1}}
\newcommand{\ts}[1]{\textcolor{orange}{#1}}

\input{./Definitions}

\title{LLM Query Scheduling with Prefix Reuse and Latency Constraints}
\date{}

\setlength\parindent{0pt}
\usepackage{parskip}

\hypersetup{
	colorlinks,
	linkcolor={red!80!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}


\author[1]{Gregory Dexter}
\author[1]{Shao Tang}
\author[1]{Ata Fatahi Baarzi}
\author[1]{Qingquan Song}
\author[1]{Tejas Dharamsi}
\author[1]{Aman Gupta}
\affil[1]{LinkedIn Corporation}

\interfootnotelinepenalty=10000

\begin{document}

\maketitle

\newcommand{\st}[1]{{\color{orange} #1}}
\newcommand{\ata}[1]{{\color{brown} #1}}
\newcommand{\qq}[1]{{\color{blue} #1}}
\newcommand{\aman}[1]{{\color{red} #1}}

% Uncomment to hide comments
% \renewcommand{\textcolor}[2]{}
% \renewcommand{\st}[2]{}

\begin{abstract}
The efficient deployment of large language models (LLMs) in online settings requires optimizing inference performance under stringent latency constraints, particularly the time-to-first-token (TTFT) and time-per-output-token (TPOT). This paper focuses on the query scheduling problem for LLM inference with prefix reuse, a technique that leverages shared prefixes across queries to reduce computational overhead. Our work reveals previously unknown limitations of the existing first-come-first-serve (FCFS) and longest-prefix-match (LPM) scheduling strategies with respect to satisfying latency constraints. We present a formal theoretical framework for LLM query scheduling under RadixAttention, a prefix reuse mechanism that stores and reuses intermediate representations in a radix tree structure. Our analysis establishes the NP-hardness of the scheduling problem with prefix reuse under TTFT constraints and proposes a novel scheduling algorithm, $k$-LPM, which generalizes existing methods by balancing prefix reuse and fairness in query processing. Theoretical guarantees demonstrate that $k$-LPM achieves improved TTFT performance under realistic traffic patterns captured by a data generative model. Empirical evaluations in a realistic serving setting validates our findings, showing significant reductions in P99 TTFT compared to baseline methods.

\end{abstract}

\input{main_text}

\bibliographystyle{plainnat}
\bibliography{bibliography}

\onecolumn
\appendix
\input{appendix}

\end{document}
