%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[manuscript,nonacm]{acmart}
\usepackage{tcolorbox}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage[export]{adjustbox}

%\documentclass[sigconf,authordraft]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{1014}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[XXXX 2025]{The Xth International Conference on XXXX}{XX–XX June, 2025}{XXXX, XXXX}

%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}
%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}
%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%
%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Mapping Trustworthiness in Large Language Models: A Bibliometric Analysis Bridging Theory to Practice}

%Mapping Trustworthiness in Large Language Models: A Bibliometric Perspective

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{José Antonio Siqueira de Cerqueira}
\affiliation{%
  \institution{Tampere University}
  \city{Tampere}
  \country{Finland}}
\email{jose.siqueiradecerqueira@tuni.fi}
\orcid{0000-0002-8143-1042}

\author{Kai-Kristian Kemell}
\affiliation{%
  \institution{Tampere University}
  \city{Tampere}
  \country{Finland}}
\email{kai-kristian.kemell@tuni.fi}
\orcid{0000-0002-0225-4560}

\author{Rebekah Rousi}
\affiliation{%
  \institution{University of Vaasa (UWASA)}
  \city{Vaasa}
  \country{Finland}}
\email{rebekah.rousi@uwasa.fi}
\orcid{0000-0001-5771-3528}

\author{Nannan Xi}
\affiliation{%
  \institution{Tampere University}
  \city{Tampere}
  \country{Finland}}
\email{nannan.xi@tuni.fi}
\orcid{0000-0002-9424-8116}

\author{Juho Hamari}
\affiliation{%
  \institution{Tampere University}
  \city{Tampere}
  \country{Finland}}
\email{juho.hamari@tuni.fi}
\orcid{0000-0002-6573-588X}

\author{Pekka Abrahamsson}
\affiliation{%
  \institution{Tampere University}
  \city{Tampere}
  \country{Finland}}
\email{pekka.abrahamsson@tuni.fi}
\orcid{0000-0002-4360-2226}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Siqueira de Cerqueira et al.}

%%
\begin{abstract}

The rapid proliferation of Large Language Models (LLMs) has raised pressing concerns regarding their trustworthiness, spanning issues of reliability, transparency, fairness, and ethical alignment. Despite the increasing adoption of LLMs across various domains, there remains a lack of consensus on how to operationalize trustworthiness in practice. This study bridges the gap between theoretical discussions and implementation by conducting a bibliometric mapping analysis of 2,006 publications from 2019 to 2025. Through co-authorship networks, keyword co-occurrence analysis, and thematic evolution tracking, we identify key research trends, influential authors, and prevailing definitions of LLM trustworthiness. Additionally, a systematic review of 68 core papers is conducted to examine conceptualizations of trust and their practical implications. Our findings reveal that trustworthiness in LLMs is often framed through existing organizational trust frameworks, emphasizing dimensions such as ability, benevolence, and integrity. However, a significant gap exists in translating these principles into concrete development strategies. To address this, we propose a structured mapping of 20 trust-enhancing techniques across the LLM lifecycle, including retrieval-augmented generation (RAG), explainability techniques, and post-training audits. By synthesizing bibliometric insights with practical strategies, this study contributes towards fostering more transparent, accountable, and ethically aligned LLMs, ensuring their responsible deployment in real-world applications.

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
  <concept>
    <concept_id>10010147.10010178</concept_id>
    <concept_desc>Computing methodologies~Artificial intelligence</concept_desc>
    <concept_significance>500</concept_significance>
  </concept>
  <concept>
    <concept_id>10010147.10010178.10010179</concept_id>
    <concept_desc>Computing methodologies~Natural language processing</concept_desc>
    <concept_significance>500</concept_significance>
  </concept>
  <concept>
    <concept_id>10002944.10011122.10002945</concept_id>
    <concept_desc>General and reference~Surveys and overviews</concept_desc>
    <concept_significance>500</concept_significance>
  </concept>
  <concept>
    <concept_id>10011007.10011074.10011092</concept_id>
    <concept_desc>Software and its engineering~Software development techniques</concept_desc>
    <concept_significance>500</concept_significance>
  </concept>
  <concept>
    <concept_id>10003456.10003462</concept_id>
    <concept_desc>Social and professional topics~Computing / technology policy</concept_desc>
    <concept_significance>300</concept_significance>
  </concept>
</ccs2012>
\end{CCSXML}



\ccsdesc[500]{Computing methodologies~Artificial intelligence}
\ccsdesc[500]{Computing methodologies~Natural language processing}
\ccsdesc[500]{General and reference~Surveys and overviews}
\ccsdesc[500]{Software and its engineering~Software development techniques}
\ccsdesc[300]{Social and professional topics~Computing / technology policy}



%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Trustworthiness, AI ethics, Large Language Models, Bibliometric Mapping}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Artificial Intelligence (AI) has become a cornerstone of technological progress, yet remains at the centre of ethical debate. More than 200 guidelines and principles have been proposed to guide its development and use, reflecting the growing recognition of its societal impact \cite{hagendorff2020ethics, Correa2023WorldwideAIethics200}. These AI ethical guidelines, which have been proposed by public organisations, society, academia and private companies, do not provide objective guidance to developers, but rather abstract principles that are not practical for developers to operationalise, e.g, transparency, privacy, justice \& fairness \cite{Vakkuri2021ECCOLAjournal, floridi2022unified, Cerqueira2022Guide}.

As an evolution of AI system that employs advanced deep learning techniques, Large Language Models (LLMs) are trained on vast datasets and capable of generating novel text, images, audio, and video, based on the data it was trained on \cite{sun2024trustllm}. Their rapid adoption has sparked both enthusiasm and concern, as their potential for negative outcomes -- such as spreading misinformation, perpetuating biases, or hallucination -- has become evident \cite{liu2023trustworthy, Wang2023DecodingTrustAC}. Despite efforts like the Paris AI Action Summit \cite{elysee_2025}, which seeks to promote responsible AI development and international cooperation, significant challenges remain. For instance, Google’s recent decision to remove its AI principles prohibiting the use of AI in autonomous weapons and surveillance has raised questions about the commitment of major tech companies to ethical AI \cite{google2025endingaiban}. This shift underscores the tension between ethical ideals and commercial or geopolitical interests, highlighting the need for robust, enforceable legislations.

The swift popularization of LLMs have encouraged researchers in understanding the specific ethical dimensions of this new generative AI (genAI), introducing them mainly as trustworthiness taxonomies, i.e., a rebranded set of similar abstract AI ethical principles \cite{sun2024trustllm, liu2023trustworthy}. Trustworthiness in LLMs can be characterized in terms of reliability, safety, fairness, resistance to misuse, explainability \& reasoning, social norm and robustness \cite{liu2023trustworthy}. Thus, the focus of AI ethics discussions has shifted to the trustworthiness of LLM systems. The current debate on LLM trustworthiness largely focus on providing definitions, benchmarks and evaluation methods \cite{sun2024trustllm, liu2023trustworthy, Wang2023DecodingTrustAC}.

Analogously with AI ethics, the proliferation of trustworthiness taxonomies for LLMs has not led to a clear consensus on definitions or a practical framework to guide the development of these systems \cite{smith2025responsiblegenerativeaiuse}. Despite the growing body of literature on LLM trustworthiness, there is still no unified set of practical guidelines to help developers mitigate risks throughout the development and deployment process. This gap between theory and practice not only hinders the creation of more trustworthy LLM systems but also masks deeper structural problems that have persisted since the early days of AI ethics.

This study aims to bridge the gap between theory and practice by investigating the means of operationalising trustworthiness in LLMs. We conduct a bibliometric mapping study, with 2,006 studies from 2019 to 2025, to identify the most influential authors, research trends and approaches and systematically analyse their similarities and differences. Additionally, by manually reading 68 key documents, we also conduct an analysis of the different definitions of trustworthiness proposed in the literature, providing insights into how different authors conceptualise trust in LLMs. Furthermore, we extend our analysis by identifying practical strategies that developers can use to enhance the trustworthiness of LLMs throughout the development lifecycle -- from pre-training to deployment and real-world use. By bringing these perspectives together, we aim to provide both a theoretical and practical basis for promoting more ethical and trustworthy LLMs.

In order to guide this study, we devised the following Research Questions:
\begin{itemize}
%    \item RQ1: Why has trustworthiness become a central concern in LLM research, and how does it relate to AI ethics, responsible AI, and user acceptance?
    \item RQ1: What are the research trends, key authors, and thematic areas in the study of LLM trustworthiness?
    \item RQ2: What are the LLM trust/trustworthiness definitions proposed by different authors in the literature?
    \item RQ3: What practical techniques and tools can be used to enhance LLM trustworthiness throughout the LLM lifecycle?
\end{itemize}

%Tentative
The main contributions of this study are as follows:
\begin{itemize}
\item \textbf{Bibliometric Mapping Analysis:} Using Bibliometrix, an R-tool, we conduct a bibliometric mapping study to identify the most influential authors, definitions, and approaches in the field. This analysis reveals trends, patterns, and areas of divergence in the literature on LLM trustworthiness.
%Put here some actual data from this analysis

\item \textbf{Systematic Analysis of Trustworthiness Definitions:} We compile and categorize various definitions of trust and trustworthiness in LLMs proposed by different authors. By analyzing these definitions, we identify commonalities, differences, and conceptual overlaps with related terms such as Responsible AI and AI Ethics.
%Put here some actual data from this analysis

\item \textbf{LLM Trustworthiness Enhancing Strategies:} We propose a mapping of practical techniques, technologies and processess that developers can use to enhance LLM trustworthiness at various stages of the development lifecycle, from pre-training to post-training, deployment and end-use. This helps in bridging the gap between abstract taxonomies and actionable steps.
%Put here some actual data from this analysis

%Analyse if this remains, depends on the analyse, I think it can stay bcuz at some point I will talk about policy and education
\item \textbf{Actionable Recommendations:} Based on our findings, we offer actionable recommendations for developers, policymakers, and researchers to operationalize trustworthiness principles in practice. These recommendations aim to address the structural challenges that have hindered the development of trustworthy AI systems.
\end{itemize}

By integrating a systematic review of trustworthiness definitions with bibliometric insights and practical techniques, this work contributes to ongoing efforts to operationalize trustworthiness in LLMs and offers concrete steps toward the development of more trustworthy LLM systems.

%to harness the potential of AI in a more trustworthy way.

This article is organized as follows: in the Section \ref{related_work_section}, we review similar works that conduct literature reviews on AI ethical guidelines and LLM trustworthiness taxonomies, highlighting their similarities and differences. In Section \ref{methodology_section}, we describe the Bibliometrix methodology adopted. In Section \ref{resultsanddiscussion_section}, we present the main findings, including systematic analyses of LLM trustworthiness definitions and trustworthiness-enhancing techniques. In Section \ref{threats_to_validity}, we discuss the study's limitations and potential biases. Finally, in Section \ref{final_remarks_section}, we reflect on the practical and theoretical implications of the results, suggesting future directions for research and development.

%Structure
%1. Introduction
%2. Related Work (Here you would talk about other lit reviews). This either goes here as a 2nd section, or you can put it before or after discussion at the end. Maybe 2nd is more familiar; we put it as second in the SLR by the student: https://helda.helsinki.fi/server/api/core/bitstreams/6996a645-6972-472d-87f7-7b60349e37b9/content
%3. Research Methodology (find out what this should be called. In lit reviews it's often called "Litearture Review Protocol" or similar)
%4. Results. You can have more than one if needed... structure this how you like. Probably have 1 subsection for definition of trustworthiness, 1 subsection for the strategies (that other table), and then other subsections as needed, based on the bibliometric analysis
%5. Discussion. Here discuss the implications; theoretical and practical. The general outline of what you found and what it means. So e.g., you can cite Jobin's guideline review and say that trustworthiness similarly has many definitions and the discussion doesn't have a clear consensus
%6. Validity threats / limitations
%7. Conclusions

\section{Related Work}
\label{related_work_section}

%Here just discuss other literature reviews related to AI ethics. You can just build on "related work" from this and modify slightly: https://helda.helsinki.fi/server/api/core/bitstreams/6996a645-6972-472d-87f7-7b60349e37b9/content

In the context of this study, we examine previous literature reviews related to LLM and genAI trust/trustworthiness and ethics in the context of this study. Among the most relevant works is that of Liu et al. \cite{liu2023trustworthy}, which proposes a taxonomy for evaluating trustworthiness in LLM in seven major categeories: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Although the paper is not a literature review, it does describe each of the major categories with examples. However, the authors fail to provide a clear definition of what is trustworthy and do not provide practical guidance for developers on how to operationalise their taxonomy.

Albahri et al. \cite{Albahri2023} conducted a systematic science mapping analysis in the context of trustworthy and explainable AI (XAI) in healthcare systems, and found various XAI techniques for developers to operationalise. Unlike their approach, we aim to provide means to operationalise more of the trustworthiness categories.

It can be seen that many scholars have provided reviews of AI ethical principles, such as \cite{Mittelstadt2019}, \cite{jobin2019globallandscape}, \cite{Correa2023WorldwideAIethics200}, \cite{hagendorff2020ethics}, \cite{siqueira2021ethical}, also providing an overview of publicly available tools that can help operationalise ethical principles in AI \cite{Morley2019}. However, they are not as relevant to our review as they appeared before the LLM and genAI hype, and we want to explore the notion of trustworthiness in LLM and genAI and how it relates to ethics in AI, also providing practical guidance for developers to operationalise it.

%Eu adicionei esse aqui pq usa Bibliometrix
%A systematic review of trustworthy and explainable artificial intelligence in healthcare: Assessment of quality, bias risk, and data fusion



\section{Methodology}
\label{methodology_section}

This study employs a bibliometric analysis to systematically investigate the landscape of research on trustworthiness in LLMs. Bibliometric analysis is a quantitative research method used to identify trends, influential works, and key research themes in a given domain \cite{Donthu2021HowTCbibliometric}. To achieve this, we utilize Bibliometrix, an open-source R package specifically designed for bibliometric and scientometric studies \cite{aria2017bibliometrix}.

\subsection{Data Collection}

The dataset for this study was extracted from the \href{https://www.webofknowledge.com)}{Web of Science (WoS) Core Collection} during 21st of February 2025, as this database contain high-quality, peer-reviewed publications in Artificial Intelligence, ethics, and Software Engineering, and provides the highest quality of publications and reliability in its indexing of high-ranking journals. \cite{Caputo2021DigitalizationAB}. The search query was formulated using a combination of relevant keywords to ensure a comprehensive dataset:


\begin{tcolorbox}

TS=(("Large Language Model*" OR LLM OR "Generative AI" OR "genAI") AND (Trust* OR Ethic*))


\end{tcolorbox}

Where TS stands for Topic, which encompasses the following fields within a record: Title, Abstract, Author Keywords and Keywords Plus®. The latter is automatically generated by the database, in which its data are words or phrases that frequently appear in the titles of an article's references, but do not appear in the title of the article itself \footnote{\href{https://support.clarivate.com/ScientificandAcademicResearch/s/article/KeyWords-Plus-generation-creation-and-changes?language=en_US}{https://support.clarivate.com/ScientificandAcademicResearch/s/article/KeyWords-Plus-generation-creation-and-changes}}.

The inclusion criteria for the retrieved publications were:
\begin{itemize}
    \item Peer-reviewed journal articles and conference proceedings.
    \item Papers published in the last \textbf{6 years} (2019-2025).
    \item Studies encompassing discussion on trustworthiness or ethics of LLMs or generative AI.
    \item Articles written in \textbf{English}.
\end{itemize}

After filtering out duplicates, irrelevant papers, and LLM as authors abbreviation, a \textbf{final dataset of 2,006 publications} was obtained. This amount of studies discussing LLMs is impressive, since we had to reduce the year range of our search to only 6 years due to the fact that LLM is a new technology.

\textbf{Further manual reading:} After exploring the broader set with Bibliometrix, we conducted a manual reading of a reduced dataset in order to gain insightful information and cross-validate our findings. This manual reading was undertaken to provide answers to research questions 2 and 3. This minimal set of studies was obtained by filtering within Bibliometrix. An objective way to define whether or not there was a focus on trustworthiness was to look at the discussion at the abstract level. Thus, by filtering papers that had "trust*" in the abstract, a set of 68 papers was found for manual reading.


\subsection{Data Processing and Preprocessing}

The raw bibliographic data was exported in \textit{BibTeX format} and processed using \textbf{Bibliometrix} in R. The following preprocessing steps were conducted:
\begin{itemize}
    \item \textbf{Data Cleaning}: Removal of missing entries, duplicate records, and irrelevant fields.
    \item \textbf{Normalization}: Unification of author names and keyword variations to ensure consistency.
\end{itemize}

\subsection{Bibliometric Analysis}

The bibliometric analysis consisted of the following techniques:
%VERIFY THIS AT THE END

\begin{itemize}
    \item \textbf{Descriptive Analysis}: Calculation of publication trends, citation statistics, and author productivity over time.
    \item \textbf{Co-Authorship Network Analysis}: Mapping collaborations between authors and institutions to identify key contributors in LLM trustworthiness research.
    \item \textbf{Co-Citation and Bibliographic Coupling}: Identifying clusters of highly interconnected research papers and their thematic evolution.
    \item \textbf{Keyword Co-Occurrence Analysis}: Investigating the most frequently occurring terms to identify emerging topics and research gaps.
    \item \textbf{Thematic Mapping}: Using \textit{Biblioshiny} (a visualization tool within Bibliometrix) to classify research themes based on centrality and density.
\end{itemize}

\subsection{Visualization and Interpretation}

To facilitate a deeper understanding of the research trends, we generated various \textbf{network visualizations}, including:
\begin{itemize}
    \item \textbf{Co-Authorship Networks} (to highlight key collaborations).
    \item \textbf{Keyword Co-Occurrence Networks} (to reveal thematic clusters).
    \item \textbf{Citation Networks} (to track the evolution of influential studies).
\end{itemize}

The results of these analyses were synthesized to provide insights into the \textbf{state-of-the-art, research gaps, and future directions in LLM trustworthiness research}.



\section{Results and Discussion}
\label{resultsanddiscussion_section}

This study examines the landscape of trustworthiness and ethics research in LLMs and generative AI from 2019 to 2025. A total of 2006 relevant studies, published across 1160 different venues over the past six years, were analyzed. These publications were authored by 7901 researchers, with an average citation count of 8.901 per document. Notably, collaborative research dominates the field, with 96.6\% of the studies involving multiple authors (7,632 in total), while only 3.4\% were produced by single authors (269 individuals).

\subsection{What are the research trends, key authors, and thematic areas in the study of LLM trustworthiness?}
% KEY AUTHORS MESMO?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Anual Publication Per Year
\subsection{Evolution of Publications from 2019-2025}
The study shows a sharp rise in publications on LLM trustworthiness and ethics from 2019 to early 2025, growing from 3 articles in 2019 to 1,434 in 2024, shown in Table \ref{tab:articles_per_year} and in Figure \ref{fig:annualscientific_production}. This growth was driven by advances like GPT-3/4, regulatory debates (e.g., EU AI Act), and societal concerns about AI ethics. Collaborative research dominated, with 96.6\% of studies involving multiple authors. By February 2025, 213 articles were published, suggesting continued momentum (~1,278 projected by year-end). This highlights the increase interest researchers have in trust/trustworthiness and ethical LLM and genAI research.

\begin{table}[ht]
    \centering
    \caption{Number of Articles Published Per Year}
    \label{tab:articles_per_year}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Year} & \textbf{Articles} \\
        \hline
        2019 & 3 \\
        2020 & 1 \\
        2021 & 4 \\
        2022 & 12 \\
        2023 & 339 \\
        2024 & 1434 \\
        2025 & 213 \\
        \hline
    \end{tabular}
\end{table}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/AnnualScientificProduction-2025-02-21.png}
    \caption{Evolution of Documents Per Year - 2019 - 2025.}
    \label{fig:annualscientific_production}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Most Global Cited Documents
\subsection{Most Global Cited Documents}

Figure \ref{fig:most_Global_cited_documents} shows the top documents ranked by total number of citations, without any filtering. Articles with high global citations indicate significant impact and influence across disciplines \cite{Donthu2021HowTCbibliometric}.

The document "ChatGPT for good? On opportunities and challenges of large language models for education" by Kasneci et al. \cite{Kasneci2023} has received 1,135 global citations. The paper explores the opportunities and challenges of using LLMs like ChatGPT in education, highlighting their potential to personalize learning, create educational content, and support both students and teachers. However, it also addresses critical risks such as bias, over-reliance, ethical concerns, and sustainability, emphasizing the need for responsible integration, teacher training, and ongoing research. The authors advocate for a balanced approach that leverages LLMs to enhance education while ensuring ethical use, transparency, and equitable access. Ongoing efforts to perceive the use of LLMs in \textbf{education} highlights how academia is concerned about this disruptive tool in educational settings.

The second most cited document is "Opinion Paper: “So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy" by Dwivedi et al. \cite{Dwivedi2023}, with an equal amount of 1,135 global citations. The paper explores the \textbf{multidisciplinary impact of ChatGPT}, highlighting its potential to enhance productivity across industries while raising concerns about ethics, misinformation, and transparency. It discusses its role in \textbf{research, business, and education}, emphasizing both opportunities and challenges, including AI's effect on academic integrity and hybrid work. The authors propose multiple future research directions, e.g., on AI governance, human-AI collaboration, and responsible implementation.

The third most cited document "ChatGPT Utility in Healthcare Education, Research, and Practice: Systematic Review on the Promising Perspectives and Valid Concerns" by Sallam Malik \cite{Sallam2023}. This study systematically reviews the potential applications and limitations of ChatGPT in \textbf{healthcare}. It highlights the benefits of ChatGPT in \textbf{scientific writing, research efficiency, medical education, and clinical workflows}, while addressing concerns such as misinformation, bias, ethical and legal challenges, and safety risks. The study emphasises the need for responsible use of AI in healthcare and calls for regulatory guidelines and stakeholder collaboration to ensure ethical and effective implementation.

The three highly cited documents analysed present investigations of the specific adoption and impact of ChatGPT in different domains -- \textbf{education} \cite{Kasneci2023}, \textbf{research, business, and education} \cite{Dwivedi2023}, and \textbf{healthcare} \cite{Sallam2023}. This shows how the research community has responded to the rapid emergence of ChatGPT in several domains. Furthermore, all three top documents have in common the need to ensure the ethical use, transparency and responsible implementation and use of AI.


\begin{figure}[htbp!]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/MostGlobalCitedDocuments-2025-02-21.png}
    \caption{Most Globally Cited Documents (2022-2024).}
    \label{fig:most_Global_cited_documents}
\end{figure}


%Global citations = the citations that an article receives as is (without filtration). Local citations = the citations that an article receives from other articles in the review corpus only (with filtration—i.e., the review domain). Local citations are generally lower and can never be higher than global citations, wherein the occurrence of the latter is an indication of an erroneous entry. The comparison of global and local citations can enrich understanding of research impact and influence as they reveal the actual or true state of affairs (e.g., articles with high global citations demonstrate impact and influence across disciplines, whereas articles with high local citations indicate impact and influence within the discipline).--------How to conduct a bibliometric analysis: An overview and guidelines

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Most Local Cited Sources
\subsection{Most Local Cited Sources}

When analyzing the most cited sources in our dataset in Table \ref{tab:top_sources}, we found an overwhelming 10,237 citations to arXiv preprint documents. This highlights a strong reliance on this preprint repository, suggesting that research in this area is rapidly evolving, with many studies referencing early-stage, non-peer-reviewed work. The prominence of arXiv, a widely used open-access platform for sharing preprints in fields like computer science, physics, and artificial intelligence, reflects the fast-paced nature of this research domain, where immediate access to emerging findings is valuable. However, while arXiV is interesting for the open-access, the documents inside are mostly pre-prints, that is, they mostly lack the scientific rigour that draws from the peer-review process.

In second place is ADV NEUR IN with 1,021 citations, which stands for Advances in Neural Information Processing Systems (NeurIPS). This is one of the most prestigious conferences in artificial intelligence and machine learning, attracting top researchers from around the world. It serves as a key platform for presenting groundbreaking advances in deep learning, reinforcement learning, and computational neuroscience that are shaping the future of AI research. The high citation rate of NeurIPS papers reflects its significant impact, as cutting-edge developments presented at the conference often drive innovation across academia and industry.

Nature comes in third with 980 citations. Nature is one of the world's most prestigious and influential scientific journals, publishing ground-breaking research in a wide range of disciplines, including artificial intelligence, medicine and environmental science. Its rigorous peer-review process and high impact factor make it a trusted source for cutting-edge discoveries that shape scientific progress. The high citation rate of Nature articles reflects their far-reaching impact, often setting new research directions and informing policy and industry advances.

The presence of Journal of Medical Internet Research (JMIR) with 633 citations and The Cureus Journal of Medical Science with 448 citations highlights the academia interest in the use of AI, genAI and LLMs in healthcare


\begin{table}[h]
    \centering
    \begin{tabular}{clc}
        \toprule
        \textbf{\#} & \textbf{Sources} & \textbf{Articles} \\
        \midrule
        1 & ARXIV & 10,237 \\
        2 & ADV NEUR IN & 1,021 \\
        3 & NATURE & 980 \\
        4 & J MED INTERNET RES & 633 \\
        5 & IEEE ACCESS & 618 \\
        6 & LECT NOTES COMPUT SC & 464 \\
        7 & COMPUT HUM BEHAV & 454 \\
        8 & CUREUS J MED SCIENCE & 448 \\
        9 & SCIENCE & 448 \\
        10 & INT J INFORM MANAGE & 405 \\
        \bottomrule
    \end{tabular}
    \caption{Most Local Cited Sources}
    \label{tab:top_sources}
\end{table}


co-citation analysis, bibliographic coupling, keyword co-occurrence and other techniques

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Most Relevant Authors

% \begin{figure}[htbp!]
%     \centering
%     \includegraphics[width=0.8\textwidth]{Images/MostRelevantAuthors-2025-02-21.png}
%     \caption{Caption.}
%     \label{fig:most_relevant_authors}
% \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Most Relevant Countries
\subsection{Most Relevant Countries}

The Table \ref{tab:country_production} shows the country distribution of research articles based on corresponding authors, distinguishing between single-country publications (SCP) and multi-country publications (MCP). The USA leads with 610 articles (30.4\%), of which 18\% involve international collaboration (MCP). China (10.5\%) follows, but with a higher proportion of MCPs (33.3\%), indicating a stronger commitment to international research. The United Kingdom (7.4\%) and Canada (2.9\%) have some of the highest MCP percentages (45.9\% and 41.4\% respectively), reflecting their active participation in global collaborations. Other notable contributors include Germany (5.1\%), Australia (4.2\%) and India (3.2\%), all of which also have a significant share of international partnerships, indicating the increasing globalisation of AI and LLM research. For better visualization, we also provide Figue \ref{fig:most_relevant_countries}.

This highlights the trend that the US and China are the leading forces driving innovation in AI and LLM research, reflecting their dominant roles in technological advancements and AI development, with technologies such as OpenAI's ChatGPT in USA and DeepSeek in China. Meanwhile, the European Union (EU) tends to focus on regulation, as seen in policies such as the GDPR and the EU AI Act, emphasising ethical oversight and legal frameworks rather than direct competition in AI innovation.

\begin{table}[h]
    \centering
    \begin{tabular}{clccccc}
        \toprule
        \textbf{\#} & \textbf{Country} & \textbf{Articles} & \textbf{Articles \%} & \textbf{SCP} & \textbf{MCP} & \textbf{MCP \%} \\
        \midrule
        1  & USA             & 610  & 30.4 & 500 & 110 & 18.0  \\
        2  & China           & 210  & 10.5 & 140 & 70  & 33.3  \\
        3  & United Kingdom  & 148  & 7.4  & 80  & 68  & 45.9  \\
        4  & Germany         & 102  & 5.1  & 69  & 33  & 32.4  \\
        5  & Australia       & 85   & 4.2  & 54  & 31  & 36.5  \\
        6  & India           & 65   & 3.2  & 42  & 23  & 35.4  \\
        7  & Canada          & 58   & 2.9  & 34  & 24  & 41.4  \\
        8  & Spain           & 57   & 2.8  & 38  & 19  & 33.3  \\
        9  & Italy           & 56   & 2.8  & 41  & 15  & 26.8  \\
        10 & Netherlands     & 33   & 1.6  & 19  & 14  & 42.4  \\
        \bottomrule
    \end{tabular}
    \caption{Country Contribution Based on Corresponding Authors}
    \label{tab:country_production}
\end{table}


\begin{figure}[htbp!]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/MostRelevantCountries-2.png}
    \caption{Country Contribution Based on Corresponding Authors. Note: MCP represents multi-country publications, while SCP indicates the ratio of single-country publications.}
    \label{fig:most_relevant_countries}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Thematic Evolution
\subsection{Thematic Evolution}

The thematic evolution of AI ethics and trustworthiness in LLM concerns is illustrated in the Figure \ref{fig:thematic_evolution}, which maps the shifts in research focus from 2019 to 2025. Initially, between 2019 and 2022, research focused predominantly on broad terms such as artificial intelligence, ethics, machine learning, language models and AI -- reflecting general concerns about AI development and ethical challenges. 
%This can be seen in the trend in the literature in papers such as \cite{jobin2019globallandscape}, \cite{Guizzardi2020}.

However, in 2023-2024, the focus begins to narrow towards more specialised topics such as large language model, generative artificial intelligence, responsible AI and AI ethics. The emergence of these topics coincides with the surge in popularity of ChatGPT, but studies on the specific ethical or responsible dimensions of large language model and generative artificial intelligence are still in their infancy.

Regarding AI ethics and responsible AI, terms used in this period, they are discussed through theoretical principles \cite{jobin2019globallandscape}, \cite{floridi2022unified}, \cite{Mittelstadt2019}, \cite{Vakkuri2021ECCOLAjournal} and the urge to translate these principles into practice \cite{fromwhattohow2019floridi}, \cite{vakkuri2020prototype}, \cite{Cerqueira2022Guide}, \cite{siqueira2021ethical}.

As AI systems -- especially LLMs -- became more advanced, concerns over trust, transparency, and accountability became even more pronounced. In 2025-2025, Figure \ref{fig:thematic_evolution} reveals a diversification of research topics, branching into AI in education, blockchain, trustworthiness, creativity, chatbots, and disinformation. 

The rise of trustworthiness comes mainly from the term responsible AI, this demonstrates how the discourse has shifted from traditional responsible AI principles in AI ethics to what is trustworthiness in generative AI. This progression highlights the tension between innovation and ethical oversight, reinforcing the need for concrete trustworthiness frameworks rather than abstract trustworthiness taxonomies.

Similarly, the focus on AI ethics has shifted to concerns about disinformation, reflecting the fact that LLMs are rapidly replacing traditional search engines and are primarily used as chatbots, particularly in question-answering (QA) settings. As users increasingly rely on LLMs rather than traditional search engines to retrieve information, the risk of misinformation and manipulated content has become a central issue. This typical type of use of LLMs suggests why discussions of AI ethics have evolved to disinformation.

%%%%%
Figure \ref{fig:thematic_evolution} aligns with the bibliometric mapping approach of this study, revealing how AI ethics has been rebranded from abstract guiding principles into abstract trustworthiness taxonomies for LLMs.

Researchers that have never delved on Artificial Intelligence suddenly started researching about their ethical issues but in the generative AI ChatGPT era, forcing them to come up with new old terminologies that have been around for almost a decade. This sudden interest is, in the opinion of the first author of this paper, hindering the actual advancements towards more ethical AI, as it is masking the debate towards a "new" set of theoretical guidelines as an enforced contribution, but not providing practical guidance for the developers, the ones actual responsible for the development of ethical AI.

In other words, this perceived rebranding of responsible AI and AI ethics to trustworthiness and disinformation is for the most part failing in proposing useful unified terminologies, frameworks, or practical guidance for developers to operationalize principles.

There is a growing need for enforceable guidelines beyond non binding ethical commitments. Even more evident after big tech companies such as Google withdrawing its AI ethical principles in mass surveillance and AI weapons \cite{google2025endingaiban}. Consequently, there is an urge for actual hard laws that can govern the development and use of these systems, that can move beyond the EU AI Act \cite{sillberg2024euaiactgood}, due to the fact that it only propose risk level assessments, and links to the AI HLEG \cite{TrustworthyAIHLEG}. The later has a set of trustworthy AI principles, however, it is non binding. Despite the fact that a binding document links to a non binding one, does not automatically transform the AI HLEG into hard law. On top of that, an unified set of practical guidance are still lacking, after several years of debate -- at least since 2019 \cite{fromwhattohow2019floridi}.

Despite this progress, fragmentation remains, with multiple authors competing with definitions of LLM trustworthiness rather than a single, unified framework. 

The continued proliferation of non binding framework proposals and terminologies by singular researchers highlights the urgency of bridging the gap between theoretical AI ethics and its real-world implementation through unified frameworks and legislations. 

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/ThematicEvolution_Print_Jose.png}
    \caption{Thematic Evolution: 2019-2025.}
    \label{fig:thematic_evolution}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Word cloud

The word cloud in Figure \ref{fig:word_cloud} provides a visual representation of the most common keywords in the literature related to LLM, genAI and its trust and ethical implications. The size of each keyword indicates its frequency, with larger words appearing more frequently in the reviewed studies.

Among the most prominent terms, "ChatGPT", "trust", "AI", "technology", and "information" are the most frequently mentioned, indicating that discussions around trust and AI adoption play a crucial role in ongoing research. It also shows that researchers are mainly making observations based on the ChatGPT platform, which is certainly the main LLM used in academia. 

In addition, "health" and "education" emerge as recurring themes, reflecting concerns about the impact of AI in different domains, and broadening the debates about the responsible use of AI and its impact on different domains, particularly education and healthcare.

The word cloud also highlights other frequently cited themes, such as "performance," "user acceptance," "privacy," "ethics," "impact," "challenges," "bias," "intention," "risk," and "opportunities."  These recurring themes highlight the growing emphasis on trustworthy AI, with concerns such as bias, privacy and ethics closely linked to user acceptance. At the same time, the literature recognises both challenges and opportunities, emphasising the need for responsible AI adoption while exploring its potential benefits.

Overall, Figure \ref{fig:word_cloud} illustrates the multifaceted nature of ChatGPT research, encompassing technical advances, ethical considerations, societal impacts, and real-world applications, particularly in areas such as trust in AI, automation, education, and user interaction. The findings reinforce the importance of ensuring ethical implementation of AI, transparent models and responsible innovation in this rapidly evolving field.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=0.8\textwidth, trim={0cm 30cm 0cm 30cm}, clip]{Images/WordCloud-2.png}
    \caption{Word cloud.}
    \label{fig:word_cloud}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Co-Occurrence Network
\subsection{Co-Occurrence Network by Keyword Plus}

The co-occurrence network visualization in Figure \ref{fig:co_occurrence_network} highlights the central role of "artificial intelligence", with strong associations to key themes like "ChatGPT", "chatbot", "large language models", related to the technologies themselves, and to "medicine", "science", "education", "health", relating to domains of interest; and "ethics" and "bias" emphisizing the ethical concerns. This suggests that AI research is deeply interconnected with societal and ethical challenges, particularly in LLMs and their impact on various domains.

On the other hand, the blue cluster focuses on the concept of "trust", linking it to "acceptance", "user acceptance", "perceptions", "adoption", "quality" and "privacy". This indicates a growing interest in AI trustworthiness, highlighting it as in terms of user acceptance. That is, instead of having previous set of AI ethical guidelines governing AI use, development and deployment in a more holistic way, this suggests a shift in a more simplistic view of trust in AI is simply gauging the acceptance of a LLM by the user.

The co-occurrence network in Figure \ref{fig:co_occurrence_network} highlights the the divide between ethical AI development and its trust/trustworthiness implications.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/co-occurrence-network.png}
    \caption{Co-Occurrence Network.}
    \label{fig:co_occurrence_network}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Coupling Map


The clustering by coupling analysis presented in Figure \ref{fig:clustering} categorizes research themes based on author coupling, using references as a measure. The x-axis (Centrality) represents the extent to which a topic is interconnected with others, while the y-axis (Impact) reflects the influence of each cluster based on global citation scores. The clusters highlight key themes in AI research, with a strong focus on artificial intelligence, ChatGPT, trust, and performance. Notably, the red cluster (AI, trust) has high confidence values, suggesting a strong association between AI discussions and trustworthiness concerns.

The green cluster, positioned near the center, contains artificial intelligence, ChatGPT, and performance, indicating a balance between centrality and impact. This suggests that discussions about ChatGPT’s capabilities and AI performance are well-integrated within broader AI research. The top-right cluster, though smaller, highlights decision-making and AI systems, implying its emerging importance in AI governance. The analysis suggests a growing intersection between AI ethics, trust, and performance, reinforcing the need for responsible AI development and evaluation frameworks.

Our main interest resides in the red cluster, as it discusses trust. Table \ref{tab:top_authors} shows the most important authors ranked by its relevance within the red cluster.

Overall, the five studies highlight the rapid adoption and diverse applications of large language models (LLMs) across multiple domains, while also highlighting their ethical, practical, and evaluative challenges. Martinez-Maldonado et al. systematically review the use of LLMs in education and identify nine key applications, such as grading, content generation and feedback, while cautioning about replicability, transparency and ethical concerns. Park et al. extend this discussion by investigating the impact of misinformation generated by ChatGPT on tourism decision-making, demonstrating how trust in AI can be easily manipulated when incorrect information is prominently presented. Similarly, Sallam examines AI in healthcare, highlighting its potential to improve medical research, education and clinical workflows, but also highlighting risks such as bias, hallucination, plagiarism and cybersecurity threats.

Wang et al. provide a meta-analysis of AI evaluation methods and argue for a more structured approach to assessing their performance, ethical considerations and real-world impact in different domains. This ties in with Herrera et al.'s work on Explainable AI (XAI), which highlights the urgent need for transparency in AI decision-making and proposes 28 open problems for achieving better interpretability and accountability in AI systems. Taken together, these studies show that while LLMs have transformative potential, their responsible use requires robust evaluation methods, ethical guidelines and accountability.

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{clp{10cm}c}
        \toprule
        \textbf{\#} & \textbf{Author} & \textbf{Title} & \textbf{Citation} \\
        \midrule
        1  & Martinez-Maldonado R & Practical and ethical challenges of large language models in education: A systematic scoping review & \cite{Yan2023} \\
        2  & Park J & When ChatGPT Gives Incorrect Answers: The Impact of Inaccurate Information by Generative AI on Tourism Decision-Making & \cite{Kim2023} \\
        3  & Sallam M & ChatGPT Utility in Healthcare Education, Research, and Practice: Systematic Review on the Promising Perspectives and Valid Concerns & \cite{Sallam2023} \\
        4  & Wang C & A Survey on Evaluation of Large Language Models & \cite{chang2023survey} \\
        5  & Herrera F & Explainable Artificial Intelligence (XAI) 2.0: A manifesto of open challenges and interdisciplinary research directions & \cite{Longo2024} \\
        \bottomrule
    \end{tabular}
    \caption{Top Cited Authors within 'trust' Cluster and Their Works}
    \label{tab:top_authors}
\end{table}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/clustering_by_coupling.png}
    \caption{Clustering by Coupling.}
    \label{fig:clustering}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Table LLM Trustworthiness Definition
\subsection{The LLM trust/trustworthiness definitions proposed by different authors in the literature}

In Table \ref{tab:trust_definitions}, we provide a structured overview of various definitions and conceptualizations of trust and trustworthiness in AI, as discussed in the literature. This compilation allows for a comparative analysis of how trust in AI is framed across different domains, emphasizing dimensions such as ability, reliability, integrity, explainability and ethical alignment. From the initial 68 analysed documents, 18 documents are explicitly or implicitly providing useful information to the task of framing what is trust/trustworthiness in LLM or genAI. That is, 26.47\% against 73.53\% of documents that are not providing useful information for this task. Other documents were not taken into consideration because they are either closed access, editorial, on the use of LLM to improve trust, in a different domain e.g., radiology, measuring trust level of a certain group when using certain LLM system, or simply with a different focus. Interesting to note that 5 of the discarded papers were related to fake news, misinformation and deepfake. The complete analysis is available on Zenodo.

Trust is a multidimensional construct that encompasses several key components, including benevolence, competence, and integrity \cite{Mayer1995}. Mayer et al.'s work entitled "An Integrative Model of Organizational Trust" was published in 1995 in a Management venue, and has been adapted in several contexts, one of them being in trust in AI systems. We found a tendency in reviewed studies in utilizing Mayer et al.'s work to build on definitions for what is trustworthiness in LLM -- 33,33\%, one third, of definitions (6/18 entries: 5 \cite{Wang2024InvestigatingTrust}, 6 \cite{Ganapati2024PublicValue}, 8 \cite{Lacity2024}, 12 \cite{Ressel2024}, 15 \cite{JasperJia2025}, 16 \cite{Martell2024}) explicitly draw from Mayer et al.’s \cite{Mayer1995} organizational trust framework. Its core elements -- ABI+ factors of perceived trustworthiness  -- towards AI, are \cite{Martell2024}:

\begin{enumerate}
    \item \textbf{Ability:} AI's capacity to perform tasks effectively,
    \item \textbf{Benevolence:} AI’s intent to act in users' best interests,
    \item \textbf{Integrity:} adherence to ethical standards,
    \item \textbf{Predictability:} consistency in AI responses.
\end{enumerate}

Trust in AI, particularly in situations of uncertainty and vulnerability, is described as the belief that an AI agent will support an individual in achieving his or her goals \cite{Wang2024InvestigatingTrust}. Trust is fundamentally linked to vulnerability and uncertainty, as highlighted by multiple perspectives. Hoy and Tschannen-Moran \cite{Hoy1999} define trust as a willingness to be vulnerable based on confidence in another party’s benevolence, reliability, competence, honesty, and openness, a concept applied to AI-mediated teacher-student relationships. Mayer et al. \cite{Mayer1995} reinforce this by framing trust as a positive expectation of another’s intentions and behavior, emphasizing the risk involved in relying on an entity. Similarly, Lee and See \cite{Lee2004} describe trust as the belief that an agent will assist in achieving one’s goals despite uncertainty and vulnerability, underscoring that trust in AI systems depends not just on their performance but also on users' confidence in their intent and reliability.

In this context, users assume the vulnerable position of relying on genAI models or proprietary platforms with the expectation that this risk will be beneficial to them, i.e. that it will help them achieve their goals despite possible misinformation or bias \cite{Ressel2024}. In the paper "Mitigative Strategies for Recovering From Large Language Model Trust Violations" \cite{Martell2024}, the authors present three different levels of trust calibration, as shown in Table \ref{tab:trust-calibration}.

\begin{table}[ht]
\centering
\caption{Types of Trust Calibration in Human-AI Interaction \cite{Martell2024}}
\label{tab:trust-calibration}
\begin{tabular}{p{11cm}l}
\toprule
\textbf{Concept and Definition} & \textbf{Reference} \\ 
\midrule
\textbf{Appropriate (Calibrated) Trust}: Alignment between an individual's perceived trust level and the AI system's actual capabilities and performance. & \cite{Yang2020} \\
\addlinespace

\textbf{Over-Trust}: Occurs when users: 
\begin{itemize}
\item Perceive the AI as more capable than it truly is
\item Treat the technology as a fully competent teammate
\item Over-rely on AI outputs despite known limitations
\end{itemize} & \cite{Aroyo2021} \\
\addlinespace

\textbf{Under-Trust}: Occurs when users:
\begin{itemize}
\item Underestimate the AI system's capabilities
\item Over-rely on their own judgment
\item Dismiss potentially valuable AI contributions
\end{itemize} & \cite{Hoff2014} \\
\bottomrule
\end{tabular}
\end{table}

The same study highlights the need to prioritise accuracy in LLM deployment, as early errors can irrevocably damage user confidence, calibration and subsequent adoption. In other words, operationalising trustworthiness in AI-LLM systems from the early stages is fundamental, as it is difficult to regain user trust. This can be seen in cases such as Microsoft's Tay Bot \footnote{\href{https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/}{https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/}}. This AI-powered chatbot started tweeting offensive neo-Nazi comments within hours of its release and was subsequently shut down.

The more familiar we are with a tool, the more we tend to trust them \cite{Califano2024}. Other authors state that trust in AI-LLM can be translated as the degree to which users and institutions are confident that the data used by, and the information generated by, AI-LLM is impartial, i.e. unbiased \cite{Jenks2024}.

Two more trust related dimensions are presented in Table \ref{tab:trust_definitions}: cognitive trust (rational evaluation of a system’s reliability and accuracy) and affective trust (emotional confidence in its benevolence and alignment with user interests) \cite{Zhang2014}. Cognitive trust, anchored in perceived credibility, facilitates affective trust through sustained reliance, as users transfer trust from the platform's technical competence to its perceived ethical intent \cite{Zhang2014, Wang2016}. This interdependence highlights how platform reliability shapes users' perceptions of AI-generated content, with technical accuracy ('reliable') and empathetic design ('credible') jointly calibrating holistic trust.

What are the trade-offs of using or adapting a 30 year old organizational trust framework to the AI era? First, as a benefit, it is a well established set of trust taxonomy, that has been widely accepted in the academia. However, it was devised way before wider adoption of different AI technologies, in special LLMs. Similarly to AI ethics, where soft law recommendations did not pose serious changes, should trustworthiness in LLM discussion be on hard laws that can unify the terms and recommendations?

In \cite{Jenks2024}, the authors state three principles of AI-LLM: fair, unbiased, and ethical. On top of that, they relate these principles with the work done within the European Commission on AI "Building Trust in Human-Centric Artificial Intelligence" \cite{EuropeanCommission2019}, which states that trustworthy AI should not only be lawful, ethical, and robust, but also give humans the ability to have control over its use, draw from and provide reliable data, offer security and privacy to its users, and be transparent, accountable, and inclusive. The efforts of the European Commision on AI is better translated in the EU AI Act \cite{sillberg2024euaiactgood, europaActFirst_EUAIAct}, where a risk level assessment for AI systems is provided. We briefly describe the EU AI Act's risk level in Table \ref{tab:euaiact_risk_levels}. From this Table, we pinpoint that most of genAI and LLM generated content falls under the "Limited Risk" level, where there is a need for Transparency requirements, highlighting once again the importance of this trustworthy principle.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|l|}
        \hline
        \textbf{Risk Level} & \textbf{Description} \\ \hline
        \textbf{Unacceptable Risk} & AI systems that are prohibited due to threats to fundamental rights (e.g., social scoring, manipulation). \\ \hline
        \textbf{High Risk} & AI systems that impact safety or fundamental rights (e.g., biometric ID, credit scoring, law enforcement). \\ \hline
        \textbf{Limited Risk} & AI systems with transparency requirements (e.g., chatbots, AI-generated content). \\ \hline
        \textbf{Minimal Risk} & AI systems with no specific regulations (e.g., spam filters, AI-driven recommendations). \\ \hline
    \end{tabular}
    \caption{Risk Levels in the EU AI Act}
    \label{tab:euaiact_risk_levels}
\end{table}


In addition, within the EU AI Act there is reference to a non-binding document, also provided by the European Comission, that aims to provide a set of trustworthy principles in AI. This non-binding document is the 2019 Ethics Guidelines for Trustworthy AI developed by the AI High-Level Expert Group (HLEG) \cite{TrustworthyAIHLEG}. We shortly describe its principles in Table \ref{tab:hleg_principles}. This Table should complement our Table \ref{tab:trustworthiness_terms}, as there are many overlapping terms, such as Transparency (Transparency) and Accountability (Responsible).

\begin{table}[h]
    \centering
    \begin{tabular}{|c|l|}
        \hline
        \textbf{Principle} & \textbf{Description} \\ \hline
        \textbf{Human Agency and Oversight} & AI should respect human autonomy and be controllable. \\ \hline
        \textbf{Technical Robustness and Safety} & AI should be resilient, secure, and function as intended. \\ \hline
        \textbf{Privacy and Data Governance} & AI must ensure data protection and quality. \\ \hline
        \textbf{Transparency} & AI should be explainable and provide clear information. \\ \hline
        \textbf{Diversity, Non-discrimination, and Fairness} & AI should be inclusive and avoid bias. \\ \hline
        \textbf{Societal and Environmental Well-being} & AI should promote sustainability and benefit society. \\ \hline
        \textbf{Accountability} & AI should have mechanisms for responsibility and oversight. \\ \hline
    \end{tabular}
    \caption{AI HLEG Principles for Trustworthy AI}
    \label{tab:hleg_principles}
\end{table}

Finally, in \cite{BaezaYates2024}, there is a critic towards the use of the terms trustworthy and ethical AI, as they can be used to humanize, i.e., anthropomorphise AI. This critic is valid insofar every Information System artefact, such as LLM and AI systems, are devised by humans, and they are programmatically designed to fulfil a set of subjective or objective desires implied by the developers or organizations they are inserted.

\renewcommand{\arraystretch}{1.2} % Adjust row spacing
\begin{longtable}{p{1cm}p{9cm}p{3cm}p{1cm}p{2cm}}
\caption{Trustworthiness Definitions in AI} \label{tab:trust_definitions} \\  % Label goes right after caption
\toprule
\textbf{\#} & \textbf{Trust/trustworthiness definition} & \textbf{Synonyms to trustworthy AI} & \textbf{Ref} & \textbf{Builds on} \\
\midrule
\endfirsthead

\multicolumn{5}{c}{\textit{(Continued from previous page)}} \\
\toprule
\textbf{Index} & \textbf{Includes trust/trustworthiness definition?} & \textbf{Synonyms to trustworthy AI} & \textbf{Citation} & \textbf{Definition builds on} \\
\midrule
\endhead

\bottomrule
\endfoot

% Data Rows
1 & Foundation Models (FMs) and Large Language Models (LLMs) demonstrate strong generalization capabilities and adaptability to specific tasks through fine-tuning or in-context learning. However, they currently lack the predictability and robustness required for deployment 'as is' in adversarial environments. In this context, trustworthiness is characterized by the ability to be directly deployed in real-world scenarios while maintaining resilience against adversarial conditions. & Trustworthy, resilient, and interpretable artificial intelligence (AI) & \cite{Jha2023Challenges} & - \\

2 & To improve trust (in these models) is to understand the behavior of deep neural networks for Information Retrieval (IR). & Explainable AI & \cite{Lucchese2023CanEmbeddings} & - \\

3 & Not explicitly, but states that: LLM’s ability in providing reliable (S\&P) advice. & Trusted, Reliable & \cite{Chen2023CanLLMProvide} & - \\

4 & The authors argue that the perceived trustworthiness of GAN-generated content depends on several factors, including the realism of the content, the context in which it is presented, and the pre-existing trust individuals have in the sources of information. More specifically in the case of fake news and misinformation. The authors suggest that addressing the ethical and political dimensions of GANs is crucial for maintaining trust in democratic societies. & Perceived trustworthiness, disinformation concerns & \cite{Carnevale2023-CARHEF-2} & - \\

5 & Trust in AI is described as the belief that an AI agent will support an individual in achieving their goals, particularly in situations marked by uncertainty and vulnerability. Trust plays a crucial role in user interaction with AI, as its absence can lead to reluctance in adopting AI tools, even when they demonstrate superior performance. Conversely, excessive trust in AI, especially in high-stakes domains like software engineering, can cause users to overlook potential errors or risks. The study explores strategies for fostering appropriate trust in generative AI tools, such as GitHub Copilot, to enhance their usability and reliability. & Ability, Benevolence, Integrity & \cite{Wang2024InvestigatingTrust} & \cite{Lee2004} \cite{Liao2022} \cite{Vereschak2021} \cite{EuropeanCommission2019} \cite{das2020opportunitieschallengesexplainableartificial} \cite{Boubin2017} \cite{OConnor2019} \cite{Pearce2022Asleep} \cite{Perry2023} \cite{Mayer1995}  \\

6 & Trust in AI comprises three key components: ability (ensuring reliable performance), integrity (aligning with ethical and social norms), and benevolence (acting in society’s best interest). Trustworthiness is reinforced by verifying AI’s reliability, ensuring ethical alignment, and strengthening oversight through public scrutiny and regulation, emphasizing governance over technical solutions. & Ability, integrity, benevolence, verifiability & \cite{Ganapati2024PublicValue} & \cite{DesouzaDawson2023} \cite{Mayer1995} \\

7 & While not formally defining trustworthiness, it conceptualizes it as the ability of AI systems to avoid generating false or misleading responses by accurately determining when they lack sufficient evidence to answer a given question. & Reliable, factual & \cite{ajewska2024} & - \\

8 & Trust in IT artifacts is defined by their quality, reliability, accuracy, and security. While organizational trust frameworks, such as Mayer et al. \cite{Mayer1995}, are widely used in IS research, applying concepts like benevolence to IT artifacts risks anthropomorphizing technology. & Ability/competence, integrity, and benevolence & \cite{Lacity2024} & \cite{Mayer1995} \cite{Porra2019} \cite{Belanger2019} \cite{Kim2009} \cite{Lowry2013} \\

9 & Not explicitly, but it describes trustworthy AI in terms of consistency, reliability, explainability, and safety. & Consistency, reliability, explainability, and safety & \cite{Gaur2024} & - \\

10 & It follows a definition of trust in education, which states that: "Trust is an individual’s or group’s willingness to be vulnerable to another party based on the confidence that the latter party is benevolent, reliable, competent, honest, and open." This definition is applied to teacher-student relationships to explore how trust is built and eroded in an AI-mediated assessment environment. & Transparency, reliability, honesty, competence, integrity, fairness, benevolence, openness, trust-building & \cite{LuoJess2024} & \cite{Hoy1999} \\

11 & Believing in the credibility or accuracy of a statement. & Credibility, AI alignment & \cite{Buchanan2024} & - \\

12 & Trust is characterized by a willingness to be vulnerable, accompanied by a positive expectation of the other party’s intentions and behavior. In the context of AI and LLMs, real-world incidents have urged the need for translating high-level trustworthiness rhetoric into domain-specific practices. This raises the question of how trust should be defined in fields such as the insurance industry and whether regulatory bodies should actively promote trust in AI systems. & AI ethics, AI trustworthy & \cite{Ressel2024} & \cite{Mayer1995} \\

13 & While trust in AI-generated content is not explicitly defined, the authors suggest that trust can be enhanced by providing indicators of familiarity, validation, or endorsements from experts. However, they also note that people tend to be more cautious when AI-generated content is explicitly labeled. & Transparency, disclosure, validation, familiarity, experience, task suitability, credibility & \cite{Califano2024} & - \\

14 & Trust in AI-LLM is defined as confidence in the impartiality of its data and generated information. Personalization plays a crucial role in fostering trust in AI-LLM, particularly in intercultural communication. According to the European Commission (2024), trustworthy AI should not only be lawful, ethical, and robust but also empower users with control over its use, ensure data reliability, security, and privacy, and uphold transparency, accountability, and inclusivity. Ultimately, trust in AI-LLM depends on the perception that its outputs are reliable and unbiased, with trustworthiness requiring ethical, inclusive, and accountable AI-generated knowledge. & Confidence, fair, unbiased, ethical, lawful, robust, human oversight, transparency, accountability, security, privacy & \cite{Jenks2024} & \cite{cook2001trust} \cite{Araujo2020} \cite{EuropeanCommission2025} \\

15 & Trust is a multidimensional construct that encompasses several key components, including benevolence, competence, and integrity. Research suggests that users tend to trust AI-generated content less when they are aware of its artificial origin, a trend observed in multiple studies. & Credibility, Transparency & \cite{JasperJia2025} & \cite{Mayer1995} \\

16 & Trust is defined as the belief that an agent will assist in achieving an individual’s goals in situations marked by uncertainty and vulnerability. Appropriate trust, or calibrated trust, occurs when a person’s trust in a system aligns with its actual performance. Over-trust arises when individuals overestimate AI capabilities, while under-trust occurs when users undervalue AI and rely too heavily on themselves. The ABI+ framework outlines four key dimensions of trust: (1) Ability -- AI's capacity to perform tasks effectively, (2) Benevolence -- AI’s intent to act in users' best interests, (3) Integrity -- adherence to ethical standards, and (4) Predictability -- consistency in AI responses. This highlights the importance of ensuring AI systems maintain accuracy from early deployment stages, as initial errors can have long-term consequences on user trust calibration and later adoption. & Transparency, calibrated trust, reliability & \cite{Martell2024} & \cite{Mayer1995} \cite{Lee2004} \cite{Yang2020} \cite{Aroyo2021} \cite{Hoff2014} \cite{Dietz2006} \cite{Toreini2020} \\

17 & Trust in generative AI can be divided into cognitive trust and affective trust. Cognitive trust is based on a user’s evaluation of the AI platform’s ability to deliver accurate and reliable information, while affective trust stems from the user’s belief that the AI system considers their interests and well-being. & Reliable, credible & \cite{Zhou2024} & \cite{Zhang2014} \cite{Wang2016} \cite{Stewart2003} \\

18 & The author actually criticizes the term trustworthy and ethical AI because they can humanize AI. & Responsible AI & \cite{BaezaYates2024} & - \\

\end{longtable}

In Table \ref{tab:trustworthiness_terms}, we present the frequency of key terms associated with trustworthiness in AI, reflecting the most commonly emphasized aspects in the literature. Transparency and Explainability appear as the first and second most frequently mentioned characteristics, respectively, highlighting how they are enabling principles, i.e., without transparency and explainability it is impossible for other trustworthiness or ethical principles to exist \cite{siqueira2021ethical}. Reliability is related to Ability and Predicability in the ABI+ framework \cite{Mayer1995}. Furthermore, Ethics, Privacy, and Fairness highlight concerns related to Integrity, i.e., adherence to ethical standards. This Table \ref{tab:trustworthiness_terms} provides a quantitative perspective on the many trustworthiness aspects identified from the literature.

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Term} & \textbf{Frequency} \\
        \hline
        Transparency    & 12 \\
        Explainability  & 9  \\
        Reliability     & 9  \\
        Ethics         & 6  \\
        Privacy        & 6  \\
        Fairness       & 5  \\
        Responsible    & 5  \\
        Credibility    & 4  \\
        Robustness     & 4  \\
        Trustworthy    & 4  \\
        \hline
    \end{tabular}
    \caption{Frequency of Trustworthiness-Related Terms}
    \label{tab:trustworthiness_terms}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Practical LLM trustworthiness-enhancing techniques found in the literature}

%Most techniques (12/20) target the post-training phase (e.g., fine-tuning, RAG, XAI methods), reflecting a reactive focus on mitigating emergent risks rather than preempting them during pre-training.
From the 68 documents analysed, 19 unique documents contribute to the creation of the Table \ref{tab:trust_enhancing}, which provides strategies for enhancing trustworthiness in LLM. We found 20 different strategies, categorised by their LLM lifecycle phase -- either pre-training, post-training or inference phase -- the actor responsible for their implementation -- either developer or user -- and the trustworthiness attributes associated with them -- e.g. transparency, explainability, reliability.

In the first place, with 6 different documents referring to this strategy, is (supervised) fine-tuning (SFT), in the post-training phase. This strategy is the responsibility of the LLM system developer and focuses mainly on reliability, robustness and fairness. This is justified by the boom of chatGPT, which is a fine-tuned version of the GPT-3 or GPT-4 pre-trained models. SFT has enabled models to become helpful assistants, that is, only the pre-trained models themselves were not the desired chatbot that chatGPT came to be. Nowadays, there is extensive documentation such as \href{https://platform.openai.com/docs/guides/fine-tuning}{OpenAI fine-tuning docs}, and platforms such as \href{https://colab.research.google.com/}{Google Colab}, which have facilitated the availability of hardware and environment to allow fine-tuning by anyone anywhere in the world.

In second place, with 4 different documents referring to this strategy, is Retrieval Augmented Generation (RAG), which also takes place in the post-training phase and under the responsibility of the developer. This technique has proved to be efficient, cost-effective and allows models to base their responses on frequently updated documents. This strategy has demonstrated versatility and requires fewer technical resources to implement. The trustworthiness attributes most associated with RAG are faithfulness, transparency and reliability.

Developers have primary responsibility for trustworthiness (16/20 techniques), while users contribute minimally (4/20, e.g., human-in-the-loop feedback). This raises concerns about scalability, as techniques such as stakeholder feedback (Entry 5) require sustained user engagement, which is rarely seen in real-world deployments.

Users can improve the trustworthiness of AI-LLM systems mainly in the inference phase, through human-in-the-loop (evaluating the output), stakeholder feedback (expert feedback), prompt engineering (implementing prompt techniques) and Reinforcement Learning from Human Feedback (RLHF) (providing feedback/labelling to the generated output).

%Only 3 methods address pre-training (data curation, training, contextual explanation), suggesting underdeveloped preventive approaches.
Most techniques (12/20) target the post-training phase (e.g. fine-tuning, RAG, XAI methods). This overemphasis on post-training strategies rather than pre-training (e.g., bias mitigation through pre-training data curation) should not always suggest that developers are attempting to address reliability risks only after deployment. This is due to the fact that the pre-training phase (e.g., data gathering, training) typically requires millions of dollars and supercomputers. However, it is suggested from previous RQs that trust issues should be addressed before deployment, as early failures can severely damage user confidence and adoption, making it difficult for a platform to regain its image and user trust \cite{Martell2024}. In this study, we suggest that the model has passed the deployment phase only when it is in the inference phase.

Explainability and transparency dominate (14/20 entries), driven by techniques such as contextual explanations (Entry 8) and uncertainty expressions (Entry 11). In contrast, security (Entry 16: blockchain) and ethics (Entry 13: audits) receive limited attention, indicating a misalignment with regulatory priorities such as the EU AI Act’s emphasis on systemic risk management.

The strategies identified suggest the need for integrated frameworks that unify technical interventions (e.g. RAG, knowledge graphs) with governance mechanisms (e.g. post-release audits, human-in-the-loop) across the LLM lifecycle, with a strong adherence to legislations.

Efforts such as the EU AI Act 
From the policy making perspective, it is pressing to enact regulations to mandate transparency and accountability in the development and deployment of LLMs. 

\renewcommand{\arraystretch}{1.2} % Adjust row spacing
\begin{longtable}{p{0.7cm}p{4cm}p{2.0cm}p{2.0cm}p{4.0cm}p{1.0cm}}
\caption{Trustworthiness-Enhancing Strategies in LLMs} \label{tab:trust_enhancing} \\  
\toprule
\textbf{\#} & \textbf{Trustworthiness-Enhancing Strategy} & \textbf{LLM Lifecycle Phase} & \textbf{Responsibility} & \textbf{Trustworthiness Attributes} & \textbf{Citations} \\
\midrule
\endfirsthead

\multicolumn{6}{c}{\textit{(Continued from previous page)}} \\
\toprule
\textbf{\#} & \textbf{Trustworthiness-Enhancing Strategy} & \textbf{LLM Lifecycle Phase} & \textbf{Responsibility} & \textbf{Trustworthiness Attributes} & \textbf{Citations} \\
\midrule
\endhead

\bottomrule
\endfoot

1  & (Supervised) Fine-tuning & Post-training & Developer & Reliability, Robustness, Fairness & \cite{Jha2023Challenges} \cite{Kilhoffer2024} \cite{Barman2024} \cite{Ressel2024} \cite{Chen2024} \cite{Roumeliotis2025} \\
2  & RAG & Post-training & Developer & Faithfulness, Transparency, Reliability & \cite{Jha2023Challenges} \cite{Lan2024} \cite{Ressel2024} \cite{Hannah2025} \\
3  & Training & Pre-training & Developer & Robustness, Accuracy & \cite{Bhattacharya2023} \\
4  & Human-in-the-loop & Inference & User & Fairness, Transparency, User Trust & \cite{Wang2024} \cite{Chen2024} \\
5  & Stakeholder feedback & Inference & User & Trust Calibration, Social Acceptability & \cite{Wang2024} \\
6  & Prompt engineering & Inference & User & Safety, Controllability, Fairness & \cite{Mousavi2024} \cite{Hannah2025} \\
7  & Evaluation & Post-training & Developer & Accountability, Transparency & \cite{Mousavi2024} \cite{Nag2023} \\
8  & Contextual Explanation & Pre-training/Post-training & Developer & Explainability, Transparency & \cite{Chari2023} \\
9  & In-context Learning (ICL) & Post-training & Developer & Adaptability, Efficiency, Fairness & \cite{Zhang2024} \cite{Barman2024} \\
10 & RAG-Ex: a model and language-agnostic explanation framework & Post-training & Developer & Explainability, Transparency & \cite{Sudhi2024} \\
11 & Uncertainty Expression in LLMs & Post-training & Developer & Transparency, Explainability, Reliability & \cite{Kim2024} \\
12 & LLMs’ self-explanations (i.e., explanations of their own answers and behaviors) & Post-training & Developer & Explainability, Interpretability & \cite{Kim2024} \\
13 & Post-release audit tool as an evaluative framework & Post-training & Developer & Transparency, Accountability, Explainability, Ethics & \cite{FernandezNieto2024} \\
14 & Reinforcement Learning from Human Feedback (RLHF) & Inference & Developer/User & Alignment & \cite{Barman2024} \cite{Chen2024} \\
15 & Chain of Thought (CoT) & Inference & Developer & Explainability & \cite{Barman2024} \cite{Chen2024} \\
16 & Blockchain & Post-training & Developer & Security, Transparency, Tamper-resistance & \cite{Fan2024} \\
17 & Knowledge Graph & Post-training & Developer & Transparency, Explainability & \cite{Chen2024} \cite{Hannah2025} \\
18 & (Training) Data curation & Pre-training & Developer & Fairness, Bias Mitigation & \cite{Chen2024} \\
19 & (Access to external) Tools (e.g., access to search engines online) & Inference & Developer & Explainability & \cite{Chen2024} \\
20 & eXplainable AI (XAI) techniques – model simplification (LIME), perturbation-based methods (SHAP), gradient-based approaches (InputXGradient, Grad-CAM), Layer-wise Relevance Propagation (LRP) & Post-training & Developer & Explainability, Interpretability, Transparency & \cite{Mersha2025} \\

\end{longtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%% From paper Communicating the cultural other: trust and bias in generative AI and large language models

% s. Araujo et al. (2020), for instance, discuss how humans tend to view machines as more objective and rational than human judgement. Logg et al. (2019: 90) come to a similar conclusion in their six experiments that show “lay people adhere more to advice when they think it comes from an algorithm than from a person.” The researchers call this tendency “algorithm appreciation,” which reinforces the Communicating the cultural other 5 observation that societies view machines as untethered to the subjectivities and biases of humans. Yet, machines are biased because algorithms come from humans (see Jones, this issue). In other words, algorithmic biases are cultural biases: humans imprint their
% own prejudices and subjectivities onto machines. For example, programmers and
% data scientists may knowingly or unwittingly create programs and applications that
% reflect cultural biases: in one instance, Obermeyer et al. (2019) show that health
% institutions use algorithms that are racially biased, disproportionally viewing Black
% patients as more problematic and expensive.


\section{Limitations}
\label{Limitations}

While bibliometric analysis provides valuable insights into research trends, it has notable limitations. Its emphasis on publication and citation counts does not always capture the depth, quality or maturity of a field of research \cite{Donthu2021HowTCbibliometric}. High citation rates can indicate popularity rather than true intellectual impact, and biases in citation patterns can distort impact \cite{Donthu2021HowTCbibliometric}.

To address these concerns, this study combines bibliometric techniques with qualitative assessments of articles with high focus on discussing trustworthiness (i.e., studies with "trust*" in the abstract). By integrating both perspectives, we aim to provide a more comprehensive understanding of LLM trustworthiness research, including also more detailed, qualitative analysis of select papers in addition to the quantitative bibliometric approach.

Another limitation of this study is that it relies solely on data from the Web of Science database. This results in two limitations. First, given the rapid pace of research in LLMs, a significant proportion of relevant studies are often published on arXiv, a widely used open-access preprint repository maintained by Cornell University. By not including arXiv, this study may not capture the latest developments and trends in the field. However, by not including them, this study aims to focus only on peer-review papers. Second, by only focusing on one database, we will also have excluded some potentially relevant, peer-reviewed and already published research. However, this was done due to technical limitations related with the chosen bibliometric approach.

\section{Final Remarks}
\label{final_remarks}

This study provides a bibliometric analysis of trustworthiness in Large Language Models (LLMs), highlighting key research trends, influential papers and gaps in the literature. The findings reveal a shift from traditional AI ethics discussions towards trustworthiness frameworks, but a lack of consensus and practical implementation strategies persists. While various approaches such as transparency, accountability and governance mechanisms have been proposed, enforceable regulations and standardised frameworks remain necessary for real-world impact.

Future research should focus on bridging the gap between theoretical taxonomies and actionable trustworthiness-enhancing techniques to ensure their practical application in LLM development and deployment. In addition, interdisciplinary collaboration is essential to reconcile ethical principles with technical and regulatory considerations. Given the rapid evolution of LLMs and their increasing societal impact, ongoing efforts must prioritise both responsible innovation and policy-driven accountability. Ultimately, achieving trustworthy AI will require not only technological advances, but also robust governance, public engagement, and industry-wide commitment to ethical AI development.

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
This research was supported by Jane and Aatos Erkko Foundation through CONVERGENCE of Humans and Machines Project under grant No. 220025.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.


\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.
