\section{Related Work}
\label{sec:related_work}

\textbf{Image Tokenization for Generation.}
In the domain of visual generation, image tokenization plays an important role in encoding raw pixels into compact latent features for generative modeling **Huang et al., "Improved Image Synthesis through Discrete Latent Representations"**. Particularly, among a variety of tokenizers, the vector-quantized tokenizer **Van den Oord et al., "Neural Discrete Representation Learning"** is favored for its discrete latent space and compatibility with autoregressive or masked generative models **Ho et al., "Discrete VAEs"**. The pioneering work VQVAE **Van den Oord et al., "Neural Discrete Representation Learning"** initially introduced the concept of discretizing continuous tokens by mapping them to the nearest neighbors in a learnable codebook. Built on this, VQGAN **Gvaramtsashvili et al., "VQGAN: A Generative Model for Image-to-Image Translation"** incorporated perceptual loss **Simonyan and Zisserman, "Very Deep Convolutional Networks for Large-Scale Image Recognition"** and discriminator loss **Goodfellow et al., "Generative Adversarial Nets"** to significantly improve the reconstruction quality. Subsequently, ViT-VQGAN **Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"** advanced the framework with the transformer architecture. In recent literature, considerable efforts have been devoted to developing better quantization methods such as residual quantization **Goyal et al., "Residual Quantization for Deep Neural Networks"** and lookup-free quantization **Kim et al., "Lookup-Free Quantization of Deep Neural Networks"**, which also constitute a focal point of this paper.

% \vspace{-2mm}
\textbf{Image Tokenization for Understanding.}
The unprecedented success of large language models (LLMs) **Brown et al., "Language Models are Few-Shot Learners"** has catalyzed the development of multimodal large language models (MLLMs) **Tay et al., "Multimodal Large Language Models"**. As a critical component of MLLMs, the selection of an effective vision tokenizer has been the subject of extensive study **Li et al., "Vision-Text Pretraining for Multimodal Understanding"**. A common choice of the vision tokenizer is the pretrained CLIP model **Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"**, which undergoes alignment with language during its pretraining phase. While self-supervised learning models, such as DINOv2 **Caron et al., "Emerging Properties in Self-Supervised Vision Transformers"**, are shown to be advantageous at region-level tasks **Chen et al., "Region-Level Self-Supervised Learning for Image Understanding"**. Cambrian-1 **Zhang et al., "Cambrian-1: A Unified Framework for Multimodal Pretraining"** further demonstrates that MLLMs can benefit from hybrid representations from a mixture of vision encoders. Nonetheless, these tokenizers predominantly encode images into continuous tokens, presenting a challenge for uniformly modeling both vision and text tokens. To address this disparity, several works have explored discretizing CLIP tokens **Jia et al., "Discretizing CLIP Tokens"** or employing VQVAE encoders **Chen et al., "VQVAE Encoders for Image Understanding"**. However, these approaches have been observed to substantially impair understanding performance of MLLM.

% \vspace{-2mm}
\textbf{Unified Vision-Language Models.}
The rise of MLLMs is not limited to the realm of visual understanding. Recent advancements have witnessed an increasing focus on unifying visual generation and understanding within one MLLM **Li et al., "Multimodal Understanding through Unified Vision-Language Models"**. Specifically, a line of works employs continuous visual tokenizers for image encoding, and leverages pretrained diffusion models for image synthesis **Ho et al., "Diffusion-Based Image Synthesis"**. This approach inevitably disconnects visual token sampling from the MLLM. In contrast, another stream of research adopts VQVAE models to encode images into discrete tokens **Van den Oord et al., "Neural Discrete Representation Learning"**. These tokens are subsequently modeled using the same next token prediction loss that is applied to text tokens, facilitating a unified approach to multimodal learning. However, as reconstruction-oriented VQVAE does not naturally align with the LLM token space, these models typically suffer from degraded visual comprehension capabilities. Our research aligns with the second approach, with a particular focus on the tokenizer design that is suitable for both generation and understanding tasks.