\section{Related Work}
\label{sec:related_work}

\textbf{Image Tokenization for Generation.}
In the domain of visual generation, image tokenization plays an important role in encoding raw pixels into compact latent features for generative modeling \cite{vqvae, latentdiffusion}. Particularly, among a variety of tokenizers, the vector-quantized tokenizer \cite{vqvae} is favored for its discrete latent space and compatibility with autoregressive or masked generative models \cite{var, llamagen, maskgit, magvit}. The pioneering work VQVAE \cite{vqvae} initially introduced the concept of discretizing continuous tokens by mapping them to the nearest neighbors in a learnable codebook. Built on this, VQGAN \cite{vqgan} incorporated perceptual loss \cite{perceptual} and discriminator loss \cite{discriminator} to significantly improve the reconstruction quality. Subsequently, ViT-VQGAN \cite{vitvqgan} advanced the framework with the transformer architecture. In recent literature, considerable efforts have been devoted to developing better quantization methods such as residual quantization \cite{rqvae} and lookup-free quantization \cite{magvitv2}, which also constitute a focal point of this paper.

% \vspace{-2mm}
\textbf{Image Tokenization for Understanding.}
The unprecedented success of large language models (LLMs) \cite{gpt3, gpt4} has catalyzed the development of multimodal large language models (MLLMs) \cite{llava, vila, mm1}. As a critical component of MLLMs, the selection of an effective vision tokenizer has been the subject of extensive study \cite{wang2023makes, cambrian}. A common choice of the vision tokenizer is the pretrained CLIP model \cite{clip}, which undergoes alignment with language during its pretraining phase. While self-supervised learning models, such as DINOv2 \cite{dinov2}, are shown to be advantageous at region-level tasks \cite{groma}. Cambrian-1 \cite{cambrian} further demonstrates that MLLMs can benefit from hybrid representations from a mixture of vision encoders. Nonetheless, these tokenizers predominantly encode images into continuous tokens, presenting a challenge for uniformly modeling both vision and text tokens. To address this disparity, several works have explored discretizing CLIP tokens \cite{seed} or employing VQVAE encoders \cite{lwm, showo}. However, these approaches have been observed to substantially impair understanding performance of MLLM.

% \vspace{-2mm}
\textbf{Unified Vision-Language Models.}
The rise of MLLMs is not limited to the realm of visual understanding. Recent advancements have witnessed an increasing focus on unifying visual generation and understanding within one MLLM \cite{dreamllm, nextgpt, chameleon, transfusion, showo, metamorph}. Specifically, a line of works employs continuous visual tokenizers for image encoding, and leverages pretrained diffusion models for image synthesis \cite{dreamllm, seedx, emu2}. This approach inevitably disconnects visual token sampling from the MLLM. In contrast, another stream of research adopts VQVAE models to encode images into discrete tokens \cite{chameleon, emu3, showo, vilau, liquid}. These tokens are subsequently modeled using the same next token prediction loss that is applied to text tokens, facilitating a unified approach to multimodal learning. However, as reconstruction-oriented VQVAE does not naturally align with the LLM token space, these models typically suffer from degraded visual comprehension capabilities. Our research aligns with the second approach, with a particular focus on the tokenizer design that is suitable for both generation and understanding tasks.