[
  {
    "index": 0,
    "papers": [
      {
        "key": "vqvae",
        "author": "Van Den Oord, Aaron and Vinyals, Oriol and others",
        "title": "Neural discrete representation learning"
      },
      {
        "key": "latentdiffusion",
        "author": "Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\\\"o}rn",
        "title": "High-resolution image synthesis with latent diffusion models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "vqvae",
        "author": "Van Den Oord, Aaron and Vinyals, Oriol and others",
        "title": "Neural discrete representation learning"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "var",
        "author": "Tian, Keyu and Jiang, Yi and Yuan, Zehuan and Peng, Bingyue and Wang, Liwei",
        "title": "Visual autoregressive modeling: Scalable image generation via next-scale prediction"
      },
      {
        "key": "llamagen",
        "author": "Sun, Peize and Jiang, Yi and Chen, Shoufa and Zhang, Shilong and Peng, Bingyue and Luo, Ping and Yuan, Zehuan",
        "title": "Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation"
      },
      {
        "key": "maskgit",
        "author": "Chang, Huiwen and Zhang, Han and Jiang, Lu and Liu, Ce and Freeman, William T",
        "title": "Maskgit: Masked generative image transformer"
      },
      {
        "key": "magvit",
        "author": "Yu, Lijun and Cheng, Yong and Sohn, Kihyuk and Lezama, Jos{\\'e} and Zhang, Han and Chang, Huiwen and Hauptmann, Alexander G and Yang, Ming-Hsuan and Hao, Yuan and Essa, Irfan and others",
        "title": "Magvit: Masked generative video transformer"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "vqvae",
        "author": "Van Den Oord, Aaron and Vinyals, Oriol and others",
        "title": "Neural discrete representation learning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "vqgan",
        "author": "Esser, Patrick and Rombach, Robin and Ommer, Bjorn",
        "title": "Taming transformers for high-resolution image synthesis"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "perceptual",
        "author": "Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver",
        "title": "The unreasonable effectiveness of deep features as a perceptual metric"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "discriminator",
        "author": "Karras, Tero and Laine, Samuli and Aila, Timo",
        "title": "A style-based generator architecture for generative adversarial networks"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "vitvqgan",
        "author": "Yu, Jiahui and Li, Xin and Koh, Jing Yu and Zhang, Han and Pang, Ruoming and Qin, James and Ku, Alexander and Xu, Yuanzhong and Baldridge, Jason and Wu, Yonghui",
        "title": "Vector-quantized image modeling with improved vqgan"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "rqvae",
        "author": "Lee, Doyup and Kim, Chiheon and Kim, Saehoon and Cho, Minsu and Han, Wook-Shin",
        "title": "Autoregressive image generation using residual quantization"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "magvitv2",
        "author": "Yu, Lijun and Lezama, Jos{\\'e} and Gundavarapu, Nitesh B and Versari, Luca and Sohn, Kihyuk and Minnen, David and Cheng, Yong and Birodkar, Vighnesh and Gupta, Agrim and Gu, Xiuye and others",
        "title": "Language Model Beats Diffusion--Tokenizer is Key to Visual Generation"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "gpt3",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      },
      {
        "key": "gpt4",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "Gpt-4 technical report"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "llava",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      },
      {
        "key": "vila",
        "author": "Lin, Ji and Yin, Hongxu and Ping, Wei and Molchanov, Pavlo and Shoeybi, Mohammad and Han, Song",
        "title": "Vila: On pre-training for visual language models"
      },
      {
        "key": "mm1",
        "author": "McKinzie, Brandon and Gan, Zhe and Fauconnier, Jean-Philippe and Dodge, Sam and Zhang, Bowen and Dufter, Philipp and Shah, Dhruti and Du, Xianzhi and Peng, Futang and Weers, Floris and others",
        "title": "Mm1: Methods, analysis \\& insights from multimodal llm pre-training"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "wang2023makes",
        "author": "Wang, Guangzhi and Ge, Yixiao and Ding, Xiaohan and Kankanhalli, Mohan and Shan, Ying",
        "title": "What Makes for Good Visual Tokenizers for Large Language Models?"
      },
      {
        "key": "cambrian",
        "author": "Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others",
        "title": "Cambrian-1: A fully open, vision-centric exploration of multimodal llms"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "clip",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "dinov2",
        "author": "Oquab, Maxime and Darcet, Timoth{\\'e}e and Moutakanni, Th{\\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and others",
        "title": "Dinov2: Learning robust visual features without supervision"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "groma",
        "author": "Ma, Chuofan and Jiang, Yi and Wu, Jiannan and Yuan, Zehuan and Qi, Xiaojuan",
        "title": "Groma: Localized visual tokenization for grounding multimodal large language models"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "cambrian",
        "author": "Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others",
        "title": "Cambrian-1: A fully open, vision-centric exploration of multimodal llms"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "seed",
        "author": "Ge, Yuying and Ge, Yixiao and Zeng, Ziyun and Wang, Xintao and Shan, Ying",
        "title": "Planting a seed of vision in large language model"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "lwm",
        "author": "Liu, Hao and Yan, Wilson and Zaharia, Matei and Abbeel, Pieter",
        "title": "World model on million-length video and language with ringattention"
      },
      {
        "key": "showo",
        "author": "Xie, Jinheng and Mao, Weijia and Bai, Zechen and Zhang, David Junhao and Wang, Weihao and Lin, Kevin Qinghong and Gu, Yuchao and Chen, Zhijie and Yang, Zhenheng and Shou, Mike Zheng",
        "title": "Show-o: One single transformer to unify multimodal understanding and generation"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "dreamllm",
        "author": "Dong, Runpei and Han, Chunrui and Peng, Yuang and Qi, Zekun and Ge, Zheng and Yang, Jinrong and Zhao, Liang and Sun, Jianjian and Zhou, Hongyu and Wei, Haoran and others",
        "title": "Dreamllm: Synergistic multimodal comprehension and creation"
      },
      {
        "key": "nextgpt",
        "author": "Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng",
        "title": "Next-gpt: Any-to-any multimodal llm"
      },
      {
        "key": "chameleon",
        "author": "Team, Chameleon",
        "title": "Chameleon: Mixed-modal early-fusion foundation models"
      },
      {
        "key": "transfusion",
        "author": "Zhou, Chunting and Yu, Lili and Babu, Arun and Tirumala, Kushal and Yasunaga, Michihiro and Shamis, Leonid and Kahn, Jacob and Ma, Xuezhe and Zettlemoyer, Luke and Levy, Omer",
        "title": "Transfusion: Predict the next token and diffuse images with one multi-modal model"
      },
      {
        "key": "showo",
        "author": "Xie, Jinheng and Mao, Weijia and Bai, Zechen and Zhang, David Junhao and Wang, Weihao and Lin, Kevin Qinghong and Gu, Yuchao and Chen, Zhijie and Yang, Zhenheng and Shou, Mike Zheng",
        "title": "Show-o: One single transformer to unify multimodal understanding and generation"
      },
      {
        "key": "metamorph",
        "author": "Tong, Shengbang and Fan, David and Zhu, Jiachen and Xiong, Yunyang and Chen, Xinlei and Sinha, Koustuv and Rabbat, Michael and LeCun, Yann and Xie, Saining and Liu, Zhuang",
        "title": "MetaMorph: Multimodal Understanding and Generation via Instruction Tuning"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "dreamllm",
        "author": "Dong, Runpei and Han, Chunrui and Peng, Yuang and Qi, Zekun and Ge, Zheng and Yang, Jinrong and Zhao, Liang and Sun, Jianjian and Zhou, Hongyu and Wei, Haoran and others",
        "title": "Dreamllm: Synergistic multimodal comprehension and creation"
      },
      {
        "key": "seedx",
        "author": "Ge, Yuying and Zhao, Sijie and Zhu, Jinguo and Ge, Yixiao and Yi, Kun and Song, Lin and Li, Chen and Ding, Xiaohan and Shan, Ying",
        "title": "Seed-x: Multimodal models with unified multi-granularity comprehension and generation"
      },
      {
        "key": "emu2",
        "author": "Sun, Quan and Cui, Yufeng and Zhang, Xiaosong and Zhang, Fan and Yu, Qiying and Wang, Yueze and Rao, Yongming and Liu, Jingjing and Huang, Tiejun and Wang, Xinlong",
        "title": "Generative multimodal models are in-context learners"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "chameleon",
        "author": "Team, Chameleon",
        "title": "Chameleon: Mixed-modal early-fusion foundation models"
      },
      {
        "key": "emu3",
        "author": "Wang, Xinlong and Zhang, Xiaosong and Luo, Zhengxiong and Sun, Quan and Cui, Yufeng and Wang, Jinsheng and Zhang, Fan and Wang, Yueze and Li, Zhen and Yu, Qiying and others",
        "title": "Emu3: Next-token prediction is all you need"
      },
      {
        "key": "showo",
        "author": "Xie, Jinheng and Mao, Weijia and Bai, Zechen and Zhang, David Junhao and Wang, Weihao and Lin, Kevin Qinghong and Gu, Yuchao and Chen, Zhijie and Yang, Zhenheng and Shou, Mike Zheng",
        "title": "Show-o: One single transformer to unify multimodal understanding and generation"
      },
      {
        "key": "vilau",
        "author": "Wu, Yecheng and Zhang, Zhuoyang and Chen, Junyu and Tang, Haotian and Li, Dacheng and Fang, Yunhao and Zhu, Ligeng and Xie, Enze and Yin, Hongxu and Yi, Li and others",
        "title": "Vila-u: a unified foundation model integrating visual understanding and generation"
      },
      {
        "key": "liquid",
        "author": "Wu, Junfeng and Jiang, Yi and Ma, Chuofan and Liu, Yuliang and Zhao, Hengshuang and Yuan, Zehuan and Bai, Song and Bai, Xiang",
        "title": "Liquid: Language Models are Scalable and Unified Multi-modal Generators"
      }
    ]
  }
]