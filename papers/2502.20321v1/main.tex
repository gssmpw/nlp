%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\usepackage{lipsum}
\usepackage{tcolorbox}
\usepackage{array}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{colortbl}
\usepackage{tabularx}
\usepackage{twemojis}
\usepackage{pifont}
\usepackage{pgfplots}

\definecolor{myblue}{RGB}{108,142,170}
\definecolor{mypurple}{RGB}{132,101,150}
\definecolor{myyellow}{RGB}{247,189,70}
\definecolor{lightgray}{gray}{0.9}

\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\TODO}[1]{\textbf{\color{red}[TODO: #1]}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\newcommand{\xjqi}[1]{{\color{magenta}{{[\textbf{xjqi}: #1]}}}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{UniTok: A Unified Tokenizer for Visual Generation and Understanding}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.

% \icmlsetsymbol{equal}{*}
\icmlsetsymbol{lead}{$\dagger$}

\begin{icmlauthorlist}
\icmlauthor{Chuofan Ma}{1,2}
\icmlauthor{Yi Jiang}{2,lead}
\icmlauthor{Junfeng Wu}{2,3}
\icmlauthor{Jihan Yang}{1}
\icmlauthor{Xin Yu}{1} \\
\vspace{1mm}
\icmlauthor{Zehuan Yuan}{2}
\icmlauthor{Bingyue Peng}{2}
\icmlauthor{Xiaojuan Qi}{1,lead}
\end{icmlauthorlist}

\icmlaffiliation{1}{The University of Hong Kong}
\icmlaffiliation{2}{ByteDance Inc.}
\icmlaffiliation{3}{Huazhong University of Science and Technology}

\icmlcorrespondingauthor{Xiaojuan Qi}{xjqi@eee.hku.hk}
\icmlcorrespondingauthor{Zehuan Yuan}{yuanzehuan@bytedance.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Visual Tokenizer, Multimodal Generation and Understanding}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlProjectLead} % otherwise use the standard text.

\begin{abstract}
The representation disparity between visual generation and understanding imposes a critical gap in integrating these capabilities into a single framework. To bridge this gap, we introduce UniTok, a discrete visual tokenizer that encodes fine-grained details for generation while also capturing high-level semantics for understanding. Despite recent studies have shown that these objectives could induce loss conflicts in training, we reveal that the underlying bottleneck stems from limited representational capacity of discrete tokens. We address this by introducing multi-codebook quantization, which divides vector quantization with several independent sub-codebooks to expand the latent feature space, while avoiding training instability caused by overlarge codebooks. Our method significantly raises the upper limit of \textit{unified discrete tokenizers} to match or even surpass \textit{domain-specific continuous tokenizers}. For instance, UniTok achieves a remarkable rFID of 0.38 (versus 0.87 for SD-VAE) and a zero-shot accuracy of 78.6\% (versus 76.2\% for CLIP) on ImageNet. 
Our code is available at \url{https://github.com/FoundationVision/UniTok}.

% We introduce UniTok, a discrete visual tokenizer that preserves fine-grained details for generative modeling while also capturing high-level semantics for understanding. UniTok is jointly trained with reconstruction (VQVAE) and contrastive (CLIP) supervisions to derive unified visual representations. In contrast to the conventional wisdom, we observe that the two learning objectives do not inherently conflict. Instead, the primary bottleneck in unified training arises from the limited representational capacity of discrete tokens. To address this challenge, we propose to divide the quantization process with multiple independent sub-codebooks, in a manner similar to multi-head attention. This design substantially expands the latent feature space while avoiding training instability incurred by large codebook size. We show that our method  significantly improves the upper limit of \textit{unified discrete tokenizers} to match or even surpass \textit{domain-specific continuous tokenizers} -- 

% UniTok achieves a remarkable rFID of 0.38 (compared to 0.87 for SD-VAE) and a zero-shot accuracy of 78.6\% (versus 76.2\% for CLIP) on ImageNet.

\end{abstract}


\section{Introduction}
\label{sec:intro}

% progress in visual generation and understanding, respectively
% -> the trend to unify generation and understanding within an MLLM
% -> limitations of existing unified paradigms / visual tokenizers
% -> the need for a unified visual tokenizer
% -> the challenge of unified visual tokenizer (slow convergency, suboptimal results)
% -> misunderstanding: generative and discriminative training objectives conflict
% -> our finding: the bottleneck stems from limited representational capacity

% \xjqi{Autoregressive modeling, dominant in natural language processing (NLP), is now transforming vision, driving advances in image and video generation \cite{videopoet, var, mar, llamagen} with scalability and quality comparable to diffusion models \cite{sdxl, dit}. Meanwhile, Multimodal Large Language Models (MLLMs) are integrating diverse visual understanding tasks within an autoregressive framework \cite{minigpt, llava, mm1}. These developments have fueled a growing interest in unifying visual generation and understanding within a single autoregressive framework [add many citations].}

% Autoregressive modeling that dominates natural language processing has recently extended its influence to vision domains. This is characterized by the rapid growth of autoregressive models in image and video generation \cite{videopoet, var, mar, llamagen}, which demonstrate promising scalability and generation quality on par with diffusion-based methods \cite{sdxl, dit}. The trend is also visible in visual understanding, where Multimodal Large Language Models (MLLMs) integrate diverse vision-centric tasks, ranging from basic image captioning to intricate visual reasoning, into the autoregressive framework \cite{minigpt, llava, mm1}.

Autoregressive modeling has recently extended its predominance in natural language processing to visual domains. This is characterized by the rapid growth of autoregressive models in image and video generation \cite{videopoet, var, mar}, as well as the increasing prevalence of Multimodal Large Language Models (MLLMs) for diverse visual understanding tasks \cite{llava, cambrian, visionllm2}. Given these advancements in individual fields, there is a growing interest in developing unified MLLMs by integrating visual generation and understanding within a single autoregressive framework \cite{lwm, chameleon, emu3}.

However, a critical challenge persists in unification: the disparity in visual tokenization for understanding and generation. For instance, the CLIP \cite{clip} tokenizer, widely used in multimodal understanding, does not naturally fit into generative modeling, which requires precise encoding of fine-grained details. Conversely, the VQVAE \cite{vqvae} tokenizer, which is tailored for autoregressive generation, potentially falls short of capturing high-level semantics crucial for visual understanding \cite{showo}. To address this issue, recent studies attempt to use separate tokenizers for each task \cite{janus}, yet this increases model complexity without fundamentally bridging the gap in representation. These limitations underscore the need for a unified visual tokenizer to serve both generation and understanding objectives. 

\begin{figure}[t]
  \centering
   \includegraphics[width=1.0\linewidth]{fig/teaser_2.pdf}
   \vspace{-4mm}
   \caption{(a): The unified tokenizer training paradigm. (b): Comparison with the unified tokenizer VILA-U in terms of ImageNet zero-shot accuracy and reconstruction FID.}
   \label{fig:teaser}
   \vspace{-4mm}
\end{figure}

Recently, VILA-U \cite{vilau} has proposed a promising paradigm for unified tokenizers, which integrates CLIP supervision into VQVAE training to complement VQ tokens with text alignment and rich semantics, as illustrated in \cref{fig:teaser}(a). Despite this innovation, it has been observed that the unified tokenizer struggles to converge in training and tends to underperform domain-specific tokenizers. This is commonly attributed to loss conflicts arising from divergent optimization goals \cite{vilau, janus}. However, recent studies on visual generation suggest that these objectives might not inherently conflict, as evidenced by the alignment behavior in generative and discriminative representation learning \cite{repa}. This contrast raises an important question: \textit{Do reconstruction and contrastive losses truly conflict in unified tokenizer training, or might there be an underlying bottleneck that remains unidentified?}

To answer the question, we conduct a comprehensive ablation study on the current unified tokenizer training paradigm (see \cref{fig:ablation}), which yields several intriguing findings. First, we show that removing reconstruction supervision, which equals training a vector-quantized CLIP model, actually does not lead to better understanding performance over unified tokenizers. This observation indicates that the existing gap between unified and CLIP tokenizers mainly arise from vector quantization rather than the conflict of learning objectives. We further establish that this gap stems from both token factorization -- which projects tokens into a lower-dimensional space for code index lookup \cite{vitvqgan} -- and discretization. These operations are essential for vector quantization but inevitably compromise the expressiveness of visual tokens. We thus identify that \textit{the primary bottleneck of unified tokenizers lies in the limited representational capacity of discrete tokens}.

To address this limitation, an intuitive approach is to expand the codebook size and latent code dimension of the tokenizer, which enables better approximation of the continuous feature space. However, this in practice could result in low codebook utilization \cite{vqlc} and diminished performance gains \cite{magvitv2}, while also making the code lookup process more computationally expensive. Drawing inspiration from the divide-and-conquer algorithm, we introduce multi-codebook quantization to alleviate this problem. Specifically, this involves dividing the visual token into multiple chunks and discretizing each with independent sub-codebooks, similar to the multi-head attention mechanism \cite{attention}. Such design effectively scales up the latent code space by increasing the number of sub-codebooks, circumventing the optimization problem associated with large codebooks.

Building upon multi-codebook quantization, we train a unified tokenizer called UniTok to bridge visual generation and understanding. We conduct a thorough evaluation of UniTok's reconstruction and classification capabilities, and more importantly, its effectiveness in establishing a unified MLLM. Experimental results show that UniTok achieves comparable or even better performance to domain-specific tokenizers, recording an impressive 0.38 reconstruction FID and a 78.6\% zero-shot accuracy at $256 \times 256$ resolution on ImageNet. The foundational capabilities of UniTok further contribute to strong downstream task performance. Our model sets a new state-of-the-art among unified autoregressive MLLMs on both multimodal understanding and generation benchmarks.


\begin{figure*}[t]
  \centering
   \includegraphics[width=1.0\linewidth]{fig/framework_2.pdf}
   \vspace{-4mm}
   \caption{\textbf{An overview of UniTok.} The tokenizer is trained to faithfully reconstruct the input image while aligning its discrete latent features with the text caption. For vector quantization, each visual token is split into multiple chunks, which then undergo code index lookup on corresponding sub-codebooks concurrently.}
   \label{fig:framework}
   % \vspace{-2mm}
\end{figure*}

\section{Related Work}
\label{sec:related_work}

\textbf{Image Tokenization for Generation.}
In the domain of visual generation, image tokenization plays an important role in encoding raw pixels into compact latent features for generative modeling \cite{vqvae, latentdiffusion}. Particularly, among a variety of tokenizers, the vector-quantized tokenizer \cite{vqvae} is favored for its discrete latent space and compatibility with autoregressive or masked generative models \cite{var, llamagen, maskgit, magvit}. The pioneering work VQVAE \cite{vqvae} initially introduced the concept of discretizing continuous tokens by mapping them to the nearest neighbors in a learnable codebook. Built on this, VQGAN \cite{vqgan} incorporated perceptual loss \cite{perceptual} and discriminator loss \cite{discriminator} to significantly improve the reconstruction quality. Subsequently, ViT-VQGAN \cite{vitvqgan} advanced the framework with the transformer architecture. In recent literature, considerable efforts have been devoted to developing better quantization methods such as residual quantization \cite{rqvae} and lookup-free quantization \cite{magvitv2}, which also constitute a focal point of this paper.

% \vspace{-2mm}
\textbf{Image Tokenization for Understanding.}
The unprecedented success of large language models (LLMs) \cite{gpt3, gpt4} has catalyzed the development of multimodal large language models (MLLMs) \cite{llava, vila, mm1}. As a critical component of MLLMs, the selection of an effective vision tokenizer has been the subject of extensive study \cite{wang2023makes, cambrian}. A common choice of the vision tokenizer is the pretrained CLIP model \cite{clip}, which undergoes alignment with language during its pretraining phase. While self-supervised learning models, such as DINOv2 \cite{dinov2}, are shown to be advantageous at region-level tasks \cite{groma}. Cambrian-1 \cite{cambrian} further demonstrates that MLLMs can benefit from hybrid representations from a mixture of vision encoders. Nonetheless, these tokenizers predominantly encode images into continuous tokens, presenting a challenge for uniformly modeling both vision and text tokens. To address this disparity, several works have explored discretizing CLIP tokens \cite{seed} or employing VQVAE encoders \cite{lwm, showo}. However, these approaches have been observed to substantially impair understanding performance of MLLM.

% \vspace{-2mm}
\textbf{Unified Vision-Language Models.}
The rise of MLLMs is not limited to the realm of visual understanding. Recent advancements have witnessed an increasing focus on unifying visual generation and understanding within one MLLM \cite{dreamllm, nextgpt, chameleon, transfusion, showo, metamorph}. Specifically, a line of works employs continuous visual tokenizers for image encoding, and leverages pretrained diffusion models for image synthesis \cite{dreamllm, seedx, emu2}. This approach inevitably disconnects visual token sampling from the MLLM. In contrast, another stream of research adopts VQVAE models to encode images into discrete tokens \cite{chameleon, emu3, showo, vilau, liquid}. These tokens are subsequently modeled using the same next token prediction loss that is applied to text tokens, facilitating a unified approach to multimodal learning. However, as reconstruction-oriented VQVAE does not naturally align with the LLM token space, these models typically suffer from degraded visual comprehension capabilities. Our research aligns with the second approach, with a particular focus on the tokenizer design that is suitable for both generation and understanding tasks.



\section{Method}
\label{sec:method}

\vspace{2mm}
In this section, we introduce UniTok, a unified visual tokenizer well-suited for both generation and understanding tasks. We start with a unified training recipe that integrates reconstruction (VQVAE) and contrastive (CLIP) supervisions (\cref{sec:training}). However, we find that simply combining both training objectives leads to severe performance degradation, which can be mainly attributed to limited representational capacity of discrete tokens (\cref{sec:bottleneck}). To this end, we propose multi-codebook quantization and attention factorization to enhance the latent feature space and derive unified visual representations (\cref{sec:unitok}). An overview of the framework is presented in \cref{fig:framework}.

\subsection{Unified Supervision}
\label{sec:training}
\vspace{1mm}
Visual generation and understanding typically impose distinct demands on the visual tokenizer. For instance, generation emphasizes lossless compression for accurate reconstruction, whereas understanding prioritizes semantically meaningful and discriminative features. To accommodate both requirements, we jointly train the tokenizer with \textit{(i)} a VQVAE-based reconstruction loss to preserve low-level information, and \textit{(ii)} an image-text contrastive loss that enhances high-level semantics of the features.

To be specific, the VQVAE-based loss term $\mathcal{L}_\text{recon}$ consists of a pixel-level reconstruction loss $\mathcal{L}_\text{R}$, a perceptual loss $\mathcal{L}_\text{P}$ based on the LPIPS metric \cite{perceptual}, a discriminator loss $\mathcal{L}_\text{G}$ to enhance reconstruction fidelity \cite{discriminator}, and a vector quantization loss $\mathcal{L}_\text{VQ}$ to minimize distance between the encoder output and its nearest code entry. It is denoted as:
\begin{equation}
    \mathcal{L}_\text{recon} = \mathcal{L}_\text{R} + \lambda_\text{VQ}\mathcal{L}_\text{VQ} + \lambda_\text{P}\mathcal{L}_\text{P} + \lambda_\text{G}\mathcal{L}_\text{G},
\end{equation}
where $\lambda$ is the weight factor for the corresponding loss term. The image-text contrastive loss term $\mathcal{L}_\text{contra}$ is basically the same as in CLIP \cite{clip}. Therefore, the final loss term of UniTok can be written as:
\begin{equation}
    \mathcal{L} =\mathcal{L}_\text{recon} + \lambda_\text{contra}\mathcal{L}_\text{contra}.
\end{equation}
We simply choose $\lambda_\text{contra}=1$ in this paper. 


\subsection{Quantization Bottleneck}
\label{sec:bottleneck}

Despite being augmented with CLIP supervision, we find that the unified tokenizer still exhibits unsatisfactory performance in visual understanding tasks, significantly lagging behind the commonly used CLIP tokenizer. To figure out the underlying causes of this underperformance, we break down the key components involved in training a unified tokenizer, as illustrated in~\cref{fig:ablation}. Starting with the CLIP baseline, we provide a step-by-step walk-through and ablation of all changes in the following paragraphs.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/roadmap_2.pdf}
    \vspace{-4mm}
    \caption{\textbf{Roadmap to build UniTok.} The \textcolor{myblue}{blue bars} illustrate the progressive changes in VQA performance from the CLIP tokenizer to the unified tokenizer, while the \textcolor{mypurple}{purple bars} represent the proposed improvements in UniTok. The VQA score is measured using the average accuracy across the VQAv2, GQA, TextVQA, and POPE benchmarks. All models are trained from scratch on 512m image-text pairs from DataComp.}
    \vspace{-4mm}
    \label{fig:ablation}
\end{figure}

% \vspace{-2mm}
\textbf{Factorization.} Modern VQ-tokenizers typically project continuous tokens to a lower-dimensional latent space for code index lookup (e.g. from 768-d to 16-d), known as token factorization \cite{vitvqgan}. This increases the relative density of codes by compressing the latent code space, thereby reducing quantization error. To evaluate the impact of factorization in CLIP training, we add two linear projection layers on top of the CLIP vision encoder (right before average pooling), which transforms tokens from 768-d to 16-d and then back to 768-d. Notably, vector quantization and reconstruction supervision are not included at this stage. Surprisingly, it turns out that this channel compression operation significantly compromises the expressiveness of tokens, leading to severe performance degradation in downstream VQA tasks.

% \vspace{-2mm}
\textbf{Discretization.} Based on the implementation described above, we further introduce vector quantization to CLIP training, which maps factorized tokens to their nearest code entries. Compared to language tokenizers with vocabularies exceeding 200k entries, the vocabulary size of modern VQ-tokenizers is markedly smaller (i.e., typically ranging from 4k to 16k). Mapping continuous tokens to such a small codebook results in considerable information loss. This is validated in our experiment, which demonstrates that discretizing the factorized tokens with a 16k codebook causes an average accuracy drop of 2.1 in VQA tasks.

% \vspace{-2mm}
\textbf{Reconstruction Supervision.} Finally, we integrate reconstruction losses into the training process to build a unified tokenizer, as outlined in \cref{sec:training}. Previous literature suggests that loss conflict between VQVAE and CLIP is a major cause of performance degradation in joint training \cite{vilau}. We observe a similar phenomenon where joint training results in sub-optimal ImageNet zero-shot classification accuracy and reconstruction FID compared to specialized training. However, surprisingly, we find that this degradation has negligible impacts on downstream understanding performance. Moreover, the degradation in classification accuracy and reconstruction FID diminishes after we improve the quantization methods (detailed in the next section). Based on these observations, we speculate that the perceived loss conflict is only a superficial issue, and the primary cause of the underperformance lies in the limited representational capacity of discrete tokens.


\subsection{UniTok}
\label{sec:unitok}
A straightforward solution to breaking the quantization bottleneck could be increasing the codebook size and the latent code dimension. However, current studies on VQVAE tokenizers suggest that there is diminishing gain in scaling and the performance saturates after the codebook size reaches 16k \cite{magvitv2, llamagen}. Continuing expansion results in a substantial portion of codes being rarely used or becoming `dead' during training, which negatively impacts downstream task performance \cite{vitvqgan}. To address this issue, we propose multi-codebook quantization and attention factorization in the following paragraphs.

% \vspace{-3mm}
\textbf{Multi-codebook quantization (MCQ)} discretizes the latent tokens with a set of independent codebooks. Specifically, the latent vector $f \in \mathbb{R}^{d}$ is first evenly split into $n$ chunks $\left\{ f_{1}, f_{2}, ..., f_{n} \right\}$, where $f_{i} \in \mathbb{R}^{\frac{d}{n}}$. The subsequent quantization process is denoted as:
\begin{equation}
    \hat{f} = \text{Concat}\left(\mathcal{Q}\left( Z_{1}, f_{1} \right), \mathcal{Q}\left( Z_{2}, f_{2} \right), ..., \mathcal{Q}\left( Z_{n}, f_{n} \right) \right)
\end{equation}
where $\hat{f}$ is the discretized latent vector, $\mathcal{Q}$ is the code index lookup operation, and $Z_{i}$ is $i$-th sub-codebook. Compared to conventional quantization methods, the proposed MCQ effectively scales up the vocabulary size. For instance, by increasing the number of sub-codebooks from 1 to 4, and suppose each sub-codebook contains 16k code entries, the theoretical vocabulary size exponentially increases from $2^{14}$ to $2^{56}$ (i.e., there are up to $2^{14\times4}$ possible combinations of codes for each token). As the size of each individual codebook remains constant, it circumvents the optimization problem associated with large codebooks. Besides, the dimensionality of the latent codes also scales proportionally with the number of codebooks (i.e., increasing from 16-d to 64-d in this case), which further enhances the representational capacity of discrete representations.

% \vspace{-2mm}
\textbf{Attention factorization.} Existing VQ methods usually employ linear or convolutional projection layers for token factorization. But as shown in \cref{sec:bottleneck}, this over-simplified design fails to preserve rich semantics in original tokens, leading to degraded understanding performance. To alleviate this problem, we suggest adapting the multi-head attention modules for factorization, as illustrated in \cref{fig:attn_proj}. Despite its simplicity, we find this design effectively strengthens the representational power of factorized tokens. Notably, to ensure compatibility with autoregressive generation, the factorization blocks are configured with causal attention.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/attn_proj.pdf}
    \caption{Modified attention blocks for factorization. Modules in yellow indicate a change in the number of channels.}
    \label{fig:attn_proj}
\end{figure}


\subsection{Unified MLLM}
\label{sec:unified_mllm}
We proceed to develop a unified multimodal model with UniTok. Particularly, we leverage the unified framework introduced in Liquid \cite{liquid}, which models (discrete-valued) vision and language sequences with a universal next-token prediction loss. But instead of learning the visual codebook from scratch, we reuse code embeddings of UniTok by projecting them to the MLLM token space with an MLP projector. Notably, despite UniTok encodes an image into $H \times W \times K$ codes (where $K$ represents the number of sub-codebooks), we simplify this for MLLM input by merging every $K$ consecutive codes into a single visual token. Similarly, when it comes to visual token prediction, we make each token autoregressively predict the next $K$ codes, using a depth transformer head as implemented in RQ-Transformer \cite{rqvae} and VILA-U \cite{vilau}. This design maintains efficiency for visual generation in the context of multi-codebooks.


\section{Experiments}
\label{sec:expriment}

\subsection{Implementation Details}

\textbf{Tokenizer Setup.} Leading VQVAE tokenizers predominantly adopt the CNN architecture, while ViT is preferred in CLIP training for its scalability. To take advantage of both, we choose a hybrid architecture, ViTamin-L/16 \cite{vitamin}, to instantiate UniTok. We configure UniTok with eight sub-codebooks, each containing 4,096 code entries and a latent dimension set to 8-d (the global latent dimension is thus 64-d). The discriminator is initialized with pretrained DINOv2-S \cite{dinov2}. We train the tokenizer for one epoch on the public dataset DataComp-1B \cite{datacomp} consisting of 1.28B image-text pairs, with all images resized to $256 \times 256$ resolution and a global batch size of 16k. The learning rate is set to 1e-3 for the tokenizer and 2e-4 for the discriminator. Besides, we prepare two settings for evaluation: one with pretrained CLIP weight initialization and one with random initialization (the default setting). 

% Following VAR \cite{var}, we instantiate the discriminator with pretrained DINOv2-S \cite{dinov2} and mute the GAN loss for the first 0.375 epochs of training.


\textbf{MLLM Setup.} We instantiate a unified MLLM described in \cref{sec:unified_mllm} with the Llama-2-7B base model \cite{llama2}. Following Liquid, we first pretrain the model on a mix of multimodal data, which is composed of 10M language data from DCLM \cite{dclm}, 30M internal MidJourney-style synthetic data, and 30M re-captioned image-text pairs from COYO \cite{coyo} and Laion \cite{laion}. Subsequently, we finetune the model on 1.5M text-to-image data and 1.5M multimodal instruction tuning data introduced in Mini-Gemini \cite{minigemini}. Specifically, the learning rate is set to 5e-5 in the pretraining stage and 2e-5 in the finetuning stage. For visual understanding evaluation, we report results on standard VQA benchmarks including VQAv2 \cite{vqav2}, GQA \cite{gqa}, TextVQA \cite{textvqa}, POPE \cite{pope}, MME \cite{mme}, and MM-Vet \cite{mmvet}. For visual generation evaluation, we report results on GenAI-Bench \cite{genaibench} and MJHQ-30K \cite{playground}.


\subsection{Tokenizer Comparison}

\begin{table}[h]
    \vspace{-4mm}
    \caption{Comparison with modern tokenizers on ImageNet reconstruction FID and zero-shot classification accuracy. The rFID is measured at $256 \times 256$ resolution with $16 \times$ downsample ratio. $\dagger$ indicates the model uses pretrained CLIP weights for initialization.}
    \vskip 0.1in
    \centering
    \tablestyle{5pt}{1.1}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{l c c c c}
            \toprule
            Method & \#Tokens & rFID $\downarrow$ & Accuracy \\
            \midrule
            \multicolumn{4}{l}{\textit{VQVAE Model}} \\
            \midrule
            VQ-GAN~\cite{vqgan} & 256 & 4.98 & -- \\
            % ViT-VQGAN~\cite{vitvqgan} & $32 \times 32$ & 1.28 & -- \\
            RQ-VAE~\cite{rqvae} & 256 & 1.30 & -- \\
            VAR~\cite{var} & 680 & 0.90 & -- \\
            \midrule
            \multicolumn{4}{l}{\textit{CLIP Model}} \\
            \midrule
            CLIP~\cite{clip} & 256 & -- & 76.2 \\
            SigLIP~\cite{siglip} & 256 & -- & 80.5 \\
            ViTamin~\cite{vitamin} & 256 & -- & 81.2 \\
            \midrule
            \multicolumn{4}{l}{\textit{Unified Model}} \\
            \midrule
            TokenFlow$^{\dagger}$~\cite{tokenflow} & 680 & 1.37 & -- \\
            VILA-U$^{\dagger}$~\cite{vilau} & 256 & 1.80 & 73.3 \\
            % VILA-U$^{\dagger}$~\cite{vilau} & $27 \times 27 \times 16$ & 1.25 & 78.0 \\
            \rowcolor{lightgray}
            UniTok & 256 & 0.39 & 70.5 \\
            \rowcolor{lightgray}
            UniTok$^{\dagger}$ & 256 & 0.38 & 78.6 \\
            \bottomrule
        \end{tabular}
    }
    \vspace{-2mm}
    \label{tab:primary_result}
\end{table}

\begin{table*}[]
    \vspace{-4mm}
    \caption{Comparison with unified multi-modal large language models on VQA benchmarks.}
    \vskip 0.1in
    \centering
    \tablestyle{5pt}{1.1}
    \resizebox{1.0\linewidth}{!}{
        \begin{tabular}{l c c c | c c c c c c}
            \toprule
            Method & LLM & Token Type & Res. & VQAv2 & GQA & TextVQA & POPE & MME & MM-Vet \\
            \midrule
            Emu~\cite{emu} & Llama-13B & Continuous & 224 & 52.0 & - & - & - & - & - \\
            LaVIT~\cite{lavit} & Llama-7B & Continuous & 224 & 66.0 & 46.8 & - & - & - & - \\
            DreamLLM~\cite{dreamllm} & Vicuna-7B & Continuous & 224 & 72.9 & - & 41.8 & - & - & 26.6 \\
            Unified-IO 2~\cite{unifiedio} & 6.8B from scratch & Continuous & 384 & 79.4 & - & - & 87.7 & - & - \\
            Janus~\cite{janus} & DeepSeek-1.3B & Continuous & 384 & 77.3 & 59.1 & - & 87.0 & 1338 & 34.3 \\            
            \midrule
            CM3Leon~\cite{cm3leon} & 7B from scratch & Discrete & 256 & 47.6 & - & - & - & - & - \\
            LWM~\cite{lwm} & Llama-2-7B & Discrete & 256 & 55.8 & 44.8 & 18.8 & 75.2 & - & - \\
            Show-o~\cite{showo} & Phi-1.5-1.3B & Discrete & 256 & 59.3 & 48.7 & - & 73.8 & 948 & - \\
            Chameleon~\cite{chameleon} & 34B from scratch & Discrete & 512 & 69.6 & - & - & - & - \\
            Liquid~\cite{liquid} & Gemma-7B & Discrete & 512 & 71.3 & 58.4 & 42.4 & 81.1 & 1119 & - \\
            VILA-U~\cite{vilau} & Llama-2-7B & Discrete & 256 & 75.3 & 58.3 & 48.3 & 83.9 & 1336 & 27.7 \\
            \rowcolor{lightgray}
            UniTok & Llama-2-7B & Discrete & 256 & 76.8 & 61.1 & 51.6 & 83.2 & 1448 & 33.9 \\
            \bottomrule
        \end{tabular}
    }
    \vspace{-2mm}
    \label{tab:understand_bench}
\end{table*}

\begin{table*}[]
    \vspace{-2mm}
    \caption{Comparison with other visual generation methods on GenAI-Bench (advanced prompts).}
    \vskip 0.1in
    \centering
    \tablestyle{4pt}{1.1}
    \resizebox{1.0\linewidth}{!}{
        \begin{tabular}{lcc | cccccc}
            \toprule
            \multirow{2}{*}{Method} & \multirow{2}{*}{Type} & \multirow{2}{*}{\#Training Images} & \multirow{2}{*}{Count{$\uparrow$}} & \multirow{2}{*}{Differ{$\uparrow$}} & \multirow{2}{*}{Compare{$\uparrow$}} & \multicolumn{2}{c}{Logical{$\uparrow$}} & \multirow{2}{*}{Overall{$\uparrow$}}   \\
            \cmidrule{7-8}
            & & & & & & Negate & Universal \\
            \midrule
            SD v2.1 \cite{latentdiffusion} & Diffusion & 2000M & 0.68 & 0.70 & 0.68 & 0.54 & 0.64 & 0.62 \\
            SD-XL \cite{sdxl} & Diffusion & 2000M & 0.71 & 0.73 & 0.69 & 0.50 & 0.66 & 0.63 \\
            Midjourney v6 \cite{midjourney} & Diffusion & -- & 0.78 & 0.78 & 0.79 & 0.50 & 0.76 & 0.69 \\
            DALL-E 3 \cite{dalle3} & Diffusion & -- & 0.82 & 0.78 & 0.82 & 0.48 & 0.80 & 0.70 \\
            \midrule
            Show-o \cite{showo} & Discrete Diff. & 36M & 0.70  & 0.62 & 0.71 & 0.51 & 0.65 & 0.60 \\
            LWM \cite{lwm} & Autoregressive & -- & 0.59 & 0.58 & 0.54 & 0.49 & 0.52 & 0.53 \\
            VILA-U~\cite{vilau} & Autoregressive & 15M & 0.70 & 0.71 & 0.74 & 0.53 & 0.66 & 0.64 \\
            Liquid~\cite{liquid} & Autoregressive & 30M & 0.76 & 0.73 & 0.74 & 0.46 & 0.74 & 0.65 \\
            \rowcolor{lightgray}
            UniTok & Autoregressive & 30M & 0.76 & 0.76 & 0.79 & 0.46 & 0.73 & 0.67 \\
            \bottomrule
        \end{tabular}
    }
    \label{tab:genai_advanced}
    \vspace{-2mm}
\end{table*}

We benchmark UniTok on ImageNet using two primary metrics: Fréchet Inception Distance (FID) to evaluate reconstruction quality, and top-1 zero-shot accuracy to assess image-text alignment. The results are presented in \cref{tab:primary_result}. Notably, UniTok excels in reconstruction quality compared to both unified and domain-specific tokenizers, recording an impressive 0.38 rFID on ImageNet with $16\times$ downsampling ratio. As a discrete tokenizer, UniTok even surpasses the continuous VAE tokenizer from Stable Diffusion v2.1 \cite{sd21}, showcasing the superiority of the proposed multi-codebook quantization. For the perception performance, we observe that randomly initialized UniTok demonstrates suboptimal zero-shot classification accuracy. This is expected as current training schedule (i.e., one epoch on 1.28B samples) is insufficient for CLIP training to fully converge. It can be seen that initializing the model with pretrained CLIP weights largely alleviates the problem, boosting the zero-shot accuracy from 70.8\% to 78.6\%. However, we would like to point out that higher ImageNet accuracy does not guarantee superior downstream performance. As demonstrated in \cref{tab:llava_eval}, we find that random initialization actually leads to better understanding performance.

In complement to quantitative results, we also provide examples of reconstructed images in \cref{fig:rec_vis}.

\subsection{Visual Understanding Performance}
\vspace{1mm}
We evaluate the understanding performance of UniTok on diverse VQA benchmarks in \cref{tab:understand_bench}. Our unified MLLM showcases clear advantages when compared to other unified models that also utilize a discrete visual tokenizer. Specifically, UniTok significantly outperforms the Chameleon model, which relies on a traditional VQVAE tokenizer, by 7.2\% higher accuracy on VQAv2. Additionally, it surpasses VILA-U, another model with a unified tokenizer, by 3.3\% in accuracy on the TextVQA benchmark and by a notable margin of 112 points on the MME-Perception scores. Furthermore, we can see that UniTok largely narrows the performance gap with MLLMs that incorporate continuous visual tokenizers. These strong results confirm the candidacy of UniTok as a unified visual tokenizer for multimodal models.


\subsection{Visual Generation Performance}

\begin{figure*}[t]
  \centering
   \includegraphics[width=1.0\linewidth]{fig/vis.png}
   \vspace{-4mm}
   \caption{Images generated in a resolution of $256 \times 256$ with our unified MLLM.}
   \label{fig:qualitative_results}
\end{figure*}

\begin{table}[]
    \vspace{-2mm}
    \caption{Results on MJHQ-30K.}
    \vskip 0.1in
    \centering
    \tablestyle{5pt}{1.15}
    \resizebox{1.0\linewidth}{!}{
        \begin{tabular}{lcccc}
        \toprule 
        Method & Type & Res. & FID$\downarrow$ \\
        \midrule
        SD-XL \cite{sdxl} & Diffusion & 1024 & 9.55 \\
        PixArt \cite{pixart} & Diffusion & 1024 & 6.14 \\ 
        Playground \cite{playground} & Diffusion & 1024 & 4.48 \\
        Liquid \cite{liquid} & Autoregressive & 512 & 5.47 \\
        Janus \cite{janus} & Autoregressive & 384 & 10.10 \\
        LWM \cite{lwm} & Autoregressive & 256 & 17.77 \\
        Show-o \cite{showo} & Discrete Diff. & 256 & 15.18 \\
        VILA-U \cite{vilau} & Autoregressive & 256 & 12.81 \\
        \midrule
        UniTok & Autoregressive & 256 & 7.46 \\
        \bottomrule
        \end{tabular}
    }
    \vspace{-2mm}
    \label{tab:mjhq}
\end{table}

\vspace{1mm}
\textbf{Quantitative Results.} \cref{tab:genai_advanced} presents the text-to-image generation performance of our unified MLLM on GenAI-Bench (advanced prompts)\footnote{Due to paper length limit, we put the results on GenAI-Bench (basic prompts) in the Appendix.}, a challenging benchmark that measures the alignment of generated images with text prompts in complex dimensions such as counting, differentiation, comparison, and understanding logical relationships. In this demanding context, our model not only consistently outperforms other autoregressive unified models, but also achieves competitive performance against domain experts (diffusion models) trained on billions of images. The strong results underscore the superior capability of our unified MLLM in complex text-to-image generation tasks.

We further evaluate the quality of images generated by our model on the MJHQ-30K benchmark, details of which are presented in \cref{tab:mjhq}. Notably, as this benchmark primarily relies on the FID score for evaluation, high-resolution images are preferred because they potentially capture more fine-grained details. Despite this makes FID across different resolutions less comparable, we show that our model achieves impressive performance even at the the smallest resolution, showcasing its ability to generate high-quality, detail-rich images.

\vspace{1mm}
\textbf{Qualitative Results.} We present some examples of the images generated by our model in \cref{fig:qualitative_results}, using text prompts sampled from MJHQ-30K. The visualization results demonstrate our model is capable of synthesizing photo-realistic and visually appealing images. Moreover, the model is able to comprehend a wide spectrum of concepts, such as `Vincent van Gogh painting style' and `bitcoin', and flexibly combine these concepts to synthesize creative images. 


\subsection{Ablation Studies}

\begin{table*}[]
    \vspace{-2mm}
    \caption{Impact of different supervision types on downstream generation and understanding performance. The rFID and gFID are measured on the ImageNet ($256\times256$) validation set. LlamaGen-L \cite{llamagen} is adopted as the generator for gFID evaluation.}
    \vskip 0.1in
    \centering
    \tablestyle{6pt}{1.1}
    \resizebox{0.8\linewidth}{!}{
        \begin{tabular}{c c c c c c c c c}
            \toprule
            \multirow{2}{*}{Supervision} & \multicolumn{2}{c}{Generation} & \multicolumn{6}{c}{Understanding} \\
            \cmidrule(lr){2-3} \cmidrule(lr){4-9}
            & rFID $\downarrow$ & gFID $\downarrow$ & VQAv2 & GQA & SciQA & TextVQA & POPE & MME \\
            \midrule
            Contrastive & -- & -- & 68.95 & 56.89 & 65.64 & 49.89 & 82.34 & 1373 \\
            Reconstruction & 0.82 & 3.59 & 56.33 & 47.53 & 63.26 & 43.65 & 77.09 & 902 \\
            Recon. + Contra. & 0.72 & 3.26 & 69.14 & 56.06 & 65.25 & 49.22 & 81.42 & 1333 \\
            \bottomrule
        \end{tabular}
    }
    \label{tab:loss_ab}
\end{table*}

\vspace{1mm}
\textbf{Impact of Supervision Types.} To ablate the impact of contrastive and reconstruction losses in UniTok training, we conduct experiments on tokenizers trained with different supervision types, as shown in \cref{tab:loss_ab}. It is worth noting that all the tokenizers are vector-quantized even though some do not have reconstruction supervision. First, we show that reconstruction-oriented tokenizer significantly lags behind tokenizers with contrastive supervision in visual understanding performance. This observation evidences the limitations of traditional VQVAE. Second, we demonstrate that reconstruction and contrastive training objectives do not inherently conflict, or can be addressed by enhancing discrete feature space. With multi-codebook quantization, the jointly trained tokenizer not only exhibits understanding performance on par with the tokenizer trained solely with contrastive loss, but also slightly improves generation performance over the reconstruction-oriented tokenizer.

\textbf{Number of Sub-Codebooks.} To gain deeper insights into multi-codebook quantization, we evaluate how tokenizer performance changes with the number of sub-codebooks. Specifically, for rFID evaluation, we train the tokenizer solely with reconstruction loss on OpenImages \cite{openimages}, and evaluated it on ImageNet ($256\times256$) validation set. While for ImageNet zero-shot accuracy evaluation, the tokenizer is trained on DataComp-1B 128m subset using only contrastive loss. As shown in \cref{tab:num_codebooks}, given a constant global codebook size, increasing the number of sub-codebooks consistently improves reconstruction FID and classification accuracy. This indicates that multi-codebook quantization generally benefits vector-quantized models, independent of the training objectives.

\begin{table}[]
    \vspace{-2mm}
    \caption{Ablation on the number of sub-codebooks. The size of a codebook is denoted as $A \times B$, where $A$ is the number of sub-codebook and $B$ is the size of sub-codebook.}
    \vskip 0.1in
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{c c c c c}
            \toprule
            Codebook & $1\times16384$ & $2\times8192$ & $4\times4096$ & $8\times2048$ \\
            \midrule
            rFID $\downarrow$ & 1.50 & 0.98 & 0.54 & 0.33 \\
            Accuracy & $41.0\%$ & $43.9\%$ & $44.7\%$ & $46.1\%$ \\
            \bottomrule
        \end{tabular}
    }
    \label{tab:num_codebooks}
\end{table}

\begin{table}[]
    \vspace{-2mm}
    \caption{Comparison of different initialization methods under the LLaVA framework. $\dagger$ indicates the model uses CLIP weights for initialization. We highlight the default setting of UniTok in gray.}
    \vskip 0.1in
    \centering
    \tablestyle{6pt}{1.1}
    \resizebox{1.0\linewidth}{!}{
        \begin{tabular}{l c c c c c}
            \toprule
            Tokenizer & VQAv2 & GQA & TextVQA & POPE & MME \\
            \midrule
            UniTok$^{\dagger}$ & 69.9 & 56.2 & 49.3 & 81.2 & 1331 \\
            \rowcolor{lightgray}
            UniTok & 72.4 & 58.2 & 51.6 & 82.4 & 1392 \\
            \bottomrule
        \end{tabular}
    }
    \label{tab:llava_eval}
\end{table}

\textbf{CLIP Weight Initialization.} In \cref{tab:llava_eval}, we ablate the impact of CLIP weight initialization on visual understanding performance. Specifically, we adopt the classic LLaVA framework for evaluation, replacing the original CLIP tokenizer with UniTok while keeping all other the training settings unchanged. One tokenizer is initialized with the pretrained ViTamin-L-256 \cite{vitamin} weights, while the other is randomly initialized. To our surprise, UniTok that is trained from scratch surpasses the one initialized with pretrained CLIP weights, despite the latter actually achieves better zero-shot classification accuracy. This suggests downstream VQA performance may not be highly correlated with ImageNet classification accuracy. More importantly, it also implies that CLIP weight initialization may serve as a negative prior for unified tokenizers, as the unified visual feature space could drastically differ from CLIP feature space. 


\section{Conclusion}
This paper studies unified visual tokenization for generation and understanding, which serves as the cornerstone of unified multimodal large language models. We investigate the training paradigm of unified tokenizers and identify that the current challenge in unification mainly arises from the limited representational power of discrete tokens. To address this limitation, we introduce multi-codebook quantization and attention-based factorization to build a unified tokenizer called UniTok. We show that UniTok achieves comparable or even superior performance than domain-specific tokenizers, and excels in downstream visual generation and understanding tasks. The ablation study further reveals that discriminative and generative representation learning does not inherently conflict. We hope our findings could inspire future research in this domain.

However, due to limited computational resources, UniTok is only trained for one epoch, which is not sufficient for CLIP-based semantic representation learning. We believe extending the training schedule could further benefit the tokenizer, especially in understanding performance. 

\section*{Impact Statement}
\vspace{1mm}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.


\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Image Reconstruction}

\cref{fig:rec_vis} presents qualitative results on the primary image reconstruction task. It can be seen that UniTok can faithfully reconstruct `hard examples' that contain small texts and human faces. The results further confirm that UniTok preserves detailed information such as texture and shape when encoding images.

\begin{figure*}[h]
  \centering
   \includegraphics[width=1.0\linewidth]{fig/rec.png}
   \vspace{-6mm}
   \caption{Qualitative results on image reconstruction in a resolution of $256 \times 256$.}
   \label{fig:rec_vis}
\end{figure*}

\section{More Generation Results}

We provide the results on GenAI-Bench (basic prompts) in \cref{tab:genai_base}, which measures the basic skills in understanding attributes, scenes, and relations in text inputs. We demonstrate that UniTok consistently delivers superior generation performance on this benchmark.

\begin{table*}[h]
    \caption{Comparison with other visual generation methods on GenAI-Bench (basic prompts).}
    \vskip 0.1in
    \centering
    \tablestyle{4pt}{1.1}
    \resizebox{1.0\linewidth}{!}{
        \begin{tabular}{lcc | cccccc}
            \toprule
            \multirow{2}{*}{Method} & \multirow{2}{*}{Type} & \multirow{2}{*}{\#Training Images} & \multirow{2}{*}{Attribute{$\uparrow$}} & \multirow{2}{*}{Scene{$\uparrow$}} & \multicolumn{3}{c}{Relation{$\uparrow$}} & \multirow{2}{*}{Overall$\uparrow$}   \\
            \cmidrule{6-8}
            & & & & & Spatial & Action & Part \\
            \midrule
            SD v2.1~\cite{latentdiffusion} & Diffusion & 2000M & 0.80 & 0.79 & 0.76 & 0.77 & 0.80 & 0.78 \\
            SD-XL~\cite{sdxl} & Diffusion & 2000M & 0.84 & 0.84 & 0.82 & 0.83 & 0.89 & 0.83\\
            Midjourney v6~\cite{midjourney} & Diffusion & -- & 0.88 & 0.87 & 0.87 & 0.87 & 0.91 & 0.87 \\
            DALL-E 3~\cite{dalle3} & Diffusion & -- & 0.91 & 0.90 & 0.92 & 0.89 & 0.91 & 0.90 \\
            \midrule
            Show-o~\cite{showo} & Discrete Diff.  & 36M & 0.72 & 0.72 & 0.70 & 0.70 & 0.75 & 0.70  \\
            LWM~\cite{lwm} & Autoregressive & -- & 0.63 & 0.62 & 0.65 & 0.63 & 0.70 & 0.63 \\
            VILA-U~\cite{vilau} & Autoregressive & 15M & 0.78 & 0.78 & 0.77 & 0.78 & 0.79 & 0.76 \\
            Liquid~\cite{liquid} & Autoregressive & 30M & 0.84 & 0.86 & 0.81 & 0.83 & 0.91 & 0.83 \\
            \rowcolor{lightgray}
            UniTok & Autoregressive & 30M & 0.85 & 0.87 & 0.86 & 0.86 & 0.89 & 0.85 \\
            \bottomrule
        \end{tabular}
    }    
    \label{tab:genai_base}
\end{table*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
