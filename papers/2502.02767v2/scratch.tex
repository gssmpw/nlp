%Though these activities will be conducted in service of the public interest, they could be understood, in certain lights, to be instances of fraud or deception.

%we read existing literature that discusses exemptions for good faith security research. A good deal of this work addresses 1) the DMCA and its research exemption, or 2) the writing of the CFAA and its ambiguity. 


%In the course of completing both objectives, we expect that our work will implicate at least several of the methodologies described in the previous section. 
%The objective of our study was twofold: 1) to understand \textit{how} commercial age verification systems work, and 2) to understand \textit{how well} these systems work. 
%To accomplish these objectives, we intend to (respectively) 1) reverse engineer these systems, and 2) 
% We created a menu of proposed ID and ``physical redteaming'' attacks on photo- and id-based age verification systems. These evasion methods are intended to be techniques that an average teenager would be able to access, and should not require significant money, resources, or technical expertise to perform. We additionally created draft proposals of 1) a user study of these age verification systems, to be conducted with under-18 and adult participants and 2) a lab-based study of ``ID attacks'' – i.e., attempts to circumvent document verification via modifications to (images of) the submitted ID document. These are currently undergoing IRB approval.

%Our research team ran into two problems almost immediately:
%The study design 
%
%In particular, we're designing a security and privacy analysis of a suite of commercial age verification services, and are in the process of consulting with legal clinics and privacy experts in order to understand how best to 1) negotiate with potential age verification vendors, 2) protect the privacy of potential study participants, and 3) verify the robustness of our findings, in light of the opacity of the black box systems we're investigating. The study will require users to don disguises and present modified ID documents to an age verification system. Though these activities will be conducted in service of the public interest, they could be understood, in certain lights, to be instances of fraud or deception. In the process of seeking out possible remedies to these issues, we read existing literature that discusses exemptions for good faith security research. A good deal of this work addresses 1) the DMCA and its research exemption, or 2) the writing of the CFAA and its ambiguity. Far fewer works discussed anti-fraud law at the state and federal law as it might relate to good-faith security research. 
%
%Though the specific setting for our inquiry was an age verification study, our findings are relevant to most security work that investigates platforms, requires researchers to interact with ML-driven systems, or implicates human study participants as surrogates or subjects. We discuss these motivations and tensions below.


%\sstodo{The research that led to this paper}
%\sstodo{I moved this here out of 3.3 because I think it wanted to be here instead... though I'm also now thinking we might want to minimize this, so maybe we just stick with the stuff in 1.1? I'm not sure. To discuss.}
%\mx{@everyone how much should/can we disclose here?}    
%The objective of our age verification study is twofold: 1) to understand \textit{how} commercial age verification systems work, and 2) to understand \textit{how well} these systems work. To accomplish these objectives, we intend to (respectively) 1) reverse engineer these systems, and 2) conduct a series of ``physical red teaming'' attacks on these systems. In the course of completing both objectives, we expect that our work will implicate at least several of the methodologies described in the previous section. 
% \textbf{Study design.} We created a menu of proposed ID and ``physical redteaming'' attacks on photo- and id-based age verification systems. These evasion methods are intended to be techniques that an average teenager would be able to access, and should not require significant money, resources, or technical expertise to perform. We additionally created draft proposals of 1) a user study of these age verification systems, to be conducted with under-18 and adult participants and 2) a lab-based study of ``ID attacks'' – i.e., attempts to circumvent document verification via modifications to (images of) the submitted ID document. These are currently undergoing IRB approval.

% \textbf{Institutional Review Board (IRB) approvals.} We communicated with two IRBs, at Princeton and MIT, prior to and during this study. On multiple iterations of review and revision, the IRB surfaced (and we addressed) issues of moral hazard to underage participants; and possible privacy issues pertaining to un-sandboxed data collection mechanisms.  

% \textbf{Outreach.} We contacted over a dozen age verification providers to request research access to their services for testing. We have plans to advertise our study to possible participants in the next month or so. \mx{TODO update when more info is available}

% \textbf{Testing.} We have conducted some preliminary tests of the efficacy of these age verification systems using our own (the research team’s) faces, some physical disguises, and some modified ID documents. These findings will be published separately in a technical report. 



\2 Age verification example: What we’d like to do is ask human subjects to (say) attempt to appear older/younger than they actually are (e.g. by putting on a mask or submitting against a real or prop ID to adversarially test an age verification system.
\3 But the whole point is that we don’t know what kind of processing of the ID/face data the companies are doing.
\3 Nightmare scenario: The company secretly monitors and flags any attempted uses that appear to be adversarial, and now the human subject’s face data and/or ID card are flagged as “people who attempt to bypass verif systems.”  Even worse nightmare scenario: Companies share this data with each other and/or the government for improved security.  This potentially means not only the researchers but also the human subjects are relying on the legal analysis we do here, at least unless we 
\2 To mitigate this, we only used human subjects with systems that either (1) worked fully on-device/offline (we kept all that data fully under our control at all times; we deleted all data at the conclusion of the study); (2) or the company agreed to not process, and to delete all information associated with the study participants as soon as reasonable.  Further discussion below, but this means the research participants should be protected from any of these potential harms.  We also (3) had the researchers take on the risk for analyzing systems that we couldn’t make (1) or (2) work for.
\1 IRB
\2 For CS research, the typical ethical analysis guide for protecting human subjects is the 2012 Menlo Report (similar to the 1979 Belmont Report for protecting human subjects in biomedical/behavioral research).  The first two categories roughly align with rights-based/deontological and utility-based/consequentialist criteria (see e.g. Morgan and Henrin, Uncertainty)
\2 Menlo report centers (1) Respect for Persons (informed consent, respect individuals as autonomous agents, etc), (2) Beneficence (benefits vs harms), and (3) Justice+Fairness (try to equally consider benefits+burdens of research esp in subject selection).  These three are the same as in the Belmont report; the Menlo also has a fourth category: The Law+Public Interest (at least try to consider the law, and be transparent+accountable))
\3 Many of the considerations around fraud would have been more in the Respect for Persons / Informed Consent bucket.  (E.g. not revealing what the study was about to the study participants.)  But in our case, the big considerations are more around Beneficence and are more of a risks problem (likely scenario: no harm; unlikely, but nonzero risk: large harm to participants, say, if we got the legal question wrong and a study participant got accused of fraud).  Depending on priors about that harm, also potential fairness issues (e.g. if the likelihood of being accused/punished of fraud doesn’t fall evenly across study participants).
\3 The best case scenario for the study is to eliminate (or mitigate as much as possible) the risk for study participants.  We already removed as much of this as we could (see next bullet) – something would have to go very wrong for the study participants to still be at risk, but in principle it would still be nice to completely remove this issue.  Could be done by (1) making the fraud question for participants airtight; (2) transferring liability to the researchers?????; (3) ???
\3 The next best scenario is what we did, which is only have study participants interact with (1) systems that were fully offline and therefore did not carry risk of harms, or (2) companies that promised to sandbox their tools with us and agree not to process/retain the data, since the only risks were associated with processing the participants’ data.
\3 The alternative avenue is: Just don’t do the study, or have only the researchers do the nonzero risk elements.  This option comes with genuine beneficence and fairness problems, though.  Beneficence: These programs are being deployed with severely under-researched efficacy at scale anyway, study or no study.  Fairness/justice: Without recruitment of many participants, the system is only analyzed wrt the demographic characteristics of the researchers themselves.  In our case this was especially salient because it was about children, so even with a massive group of diverse researchers, having children represented would have been basically impossible.
\2 Participant anonymity
\3 Sometimes we see their IDs, even if we don’t retain any of the info!
\3 Sometimes the company sees the IDs (hence why we make them promise not to retain/process any of the info!)
\3 Same concerns for face data.  (Safe to assume one can find someone’s name given their face)
\3 IRB also has participant anonymity concerns, but would probably be okay with us deleting the info afterward.
\2 We were lucky enough to have the BU/MIT law clinic at our disposal.  A lot of researchers won’t have that or won’t know about it!
\2 Using public datasets rather than live human participants doesn’t actually help this problem.  Then you might have done the exact same thing (landed someone in a weird database) without even having them go through informed consent.
\2 Other 



\begin{outline}

\1 Table 1 shows categories of research, the laws they implicate, and how those fields + laws have modified each other to allow reasonable carveouts (MX)

\1 Since problems X and Y with anti-fraud laws are similar to issues faced in law Z, we generally think similar mitigation strategies L, M, and O would be good ways to address problems X and Y in the anti-fraud context in addition to the Z context. (AS?)

\1 However, fraud also has separate issues that do not appear in most DMCA,CFAA, etc research, especially the possible (though unlikely) legal liability of human study participants.  To address those problems, and guided by ethical research principles, we think it's worth exploring P, Q, or R, as potential avenues.

\1 Some calls on reconsiderations of the laws -- lying to people vs machines, ``obtaining a thing of value'' other than making the system do something it's not, or straight-up charigng policy or safe harbor style thing... 


\1 Table 1 shows categories of research, the laws they implicate, and how those fields + laws have modified each other to allow reasonable carveouts (MX)

\1 Since problems X and Y with anti-fraud laws are similar to issues faced in law Z, we generally think similar mitigation strategies L, M, and O would be good ways to address problems X and Y in the anti-fraud context in addition to the Z context. (AS?)

\1 However, fraud also has separate issues that do not appear in most DMCA,CFAA, etc research, especially the possible (though unlikely) legal liability of human study participants.  To address those problems, and guided by ethical research principles, we think it's worth exploring P, Q, or R, as potential avenues.

\1 Some calls on reconsiderations of the laws -- lying to people vs machines, ``obtaining a thing of value'' other than making the system do something it's not, or straight-up charigng policy or safe harbor style thing... 
\end{outline}


\textbf{Mitigating risk.} Beyond the legal complications inherent to this work, there are also research ethics to consider. 
The foremost ethical concern, in our use case, is that age verification services might perform potentially dangerous data processing without users' informed consent.

In the case of our age verification work, we’d like to ask human subjects to attempt to appear older or younger than they actually are (e.g., by donning a mask or submitting a real or prop ID to adversarially test an age verification system).
While we'll be able to observe the final outcomes of such tests, we won't know what kind of processing of the ID or face data the companies are doing.

A nightmare scenario is one in which the company secretly monitors and flags any attempted uses that appear to be adversarial, and now the human subject’s face data and/or ID card are flagged as “people who attempt to bypass verif systems.”  Even worse nightmare scenario: Companies share this data with each other and/or the government for improved security. This potentially means not only the researchers but also the human subjects are relying on the legal analysis we do here, at least unless we \mx{incomplete sentence?}

To mitigate this, we only used human subjects with systems that either (1) worked fully on-device/offline (we kept all that data fully under our control at all times; we deleted all data at the conclusion of the study); (2) were provided by companies that had expressly agreed to delete all information associated with study participants as soon as reasonable. We also (3) had the research team take on the risk of analyzing systems that we couldn't make (1) or (2) work for.

\sstodo{Mega-condense the following so it's basically a short case study.}
\begin{outline}

% \2 Age verification example: What we’d like to do is ask human subjects to (say) attempt to appear older/younger than they actually are (e.g. by putting on a mask or submitting against a real or prop ID to adversarially test an age verification system.
% \3 But the whole point is that we don’t know what kind of processing of the ID/face data the companies are doing.
% \3 Nightmare scenario: The company secretly monitors and flags any attempted uses that appear to be adversarial, and now the human subject’s face data and/or ID card are flagged as “people who attempt to bypass verif systems.”  Even worse nightmare scenario: Companies share this data with each other and/or the government for improved security.  This potentially means not only the researchers but also the human subjects are relying on the legal analysis we do here, at least unless we 
% \2 To mitigate this, we only used human subjects with systems that either (1) worked fully on-device/offline (we kept all that data fully under our control at all times; we deleted all data at the conclusion of the study); (2) or the company agreed to not process, and to delete all information associated with the study participants as soon as reasonable.  Further discussion below, but this means the research participants should be protected from any of these potential harms.  We also (3) had the researchers take on the risk for analyzing systems that we couldn’t make (1) or (2) work for.
% \1 IRB
% \2 For CS research, the typical ethical analysis guide for protecting human subjects is the 2012 Menlo Report (similar to the 1979 Belmont Report for protecting human subjects in biomedical/behavioral research).  The first two categories roughly align with rights-based/deontological and utility-based/consequentialist criteria (see e.g. Morgan and Henrin, Uncertainty)
% \2 Menlo report centers (1) Respect for Persons (informed consent, respect individuals as autonomous agents, etc), (2) Beneficence (benefits vs harms), and (3) Justice+Fairness (try to equally consider benefits+burdens of research esp in subject selection).  These three are the same as in the Belmont report; the Menlo also has a fourth category: The Law+Public Interest (at least try to consider the law, and be transparent+accountable))
\1 Temp
\2 Temp
\3 Many of the considerations around fraud would have been more in the Respect for Persons / Informed Consent bucket.  (E.g. not revealing what the study was about to the study participants.)  But in our case, the big considerations are more around Beneficence and are more of a risks problem (likely scenario: no harm; unlikely, but nonzero risk: large harm to participants, say, if we got the legal question wrong and a study participant got accused of fraud).  Depending on priors about that harm, also potential fairness issues (e.g. if the likelihood of being accused/punished of fraud doesn’t fall evenly across study participants).
\3 The best case scenario for the study is to eliminate (or mitigate as much as possible) the risk for study participants.  We already removed as much of this as we could (see next bullet) – something would have to go very wrong for the study participants to still be at risk, but in principle it would still be nice to completely remove this issue.  Could be done by (1) making the fraud question for participants airtight; (2) transferring liability to the researchers?????; (3) ???
\3 The next best scenario is what we did, which is only have study participants interact with (1) systems that were fully offline and therefore did not carry risk of harms, or (2) companies that promised to sandbox their tools with us and agree not to process/retain the data, since the only risks were associated with processing the participants’ data.
\3 The alternative avenue is: Just don’t do the study, or have only the researchers do the nonzero risk elements.  This option comes with genuine beneficence and fairness problems, though.  Beneficence: These programs are being deployed with severely under-researched efficacy at scale anyway, study or no study.  Fairness/justice: Without recruitment of many participants, the system is only analyzed wrt the demographic characteristics of the researchers themselves.  In our case this was especially salient because it was about children, so even with a massive group of diverse researchers, having children represented would have been basically impossible.
\2 Participant anonymity
\3 Sometimes we see their IDs, even if we don’t retain any of the info!
\3 Sometimes the company sees the IDs (hence why we make them promise not to retain/process any of the info!)
\3 Same concerns for face data.  (Safe to assume one can find someone’s name given their face)
\3 IRB also has participant anonymity concerns, but would probably be okay with us deleting the info afterward.
\2 We were lucky enough to have the BU/MIT law clinic at our disposal.  A lot of researchers won’t have that or won’t know about it!
\2 Using public datasets rather than live human participants doesn’t actually help this problem.  Then you might have done the exact same thing (landed someone in a weird database) without even having them go through informed consent.
\2 Other 

\end{outline}


\textbf{Interaction beyond study participants}




To take one example, researchers conducting large-scale Internet measurement study may involve interacting with many users' web data.
Research ethics holds the resaerchers to 
Researchers have 
The Menlo report suggests one method for dealing with this, and n this scenario the report suggests interfacing with network services as a proxy responsible for the users' data.
However, 

While the Menlo report provides useful high-level principles that will guide our ethical analysis, it falls short of being a comprehensive ethical guide in many ways.

One shortcoming of the Menlo Report was that its existing descriptions published in 2012 do not map well onto the modern computer science ethics landscape.
\sstodo{Maybe I should just jump straight to fraud...}
For example, a common ethical consideration in modern computer security research is a process for responsibly disclosing newly-discovered vulnerabilities on a deliberate timeline coordinated with other stakeholders to balance the competing goals of bringing the vulnerability to attention (to highlight the need to build and deploy a software patch or mitigation strategy), while still ensuring malicious adversaries cannot immediately exploit the public research until the patch or mitigation is in place
\cite{ISOIEC29147,householderCERTGuideCoordinated,VulnerabilityDisclosureOWASP}.

In computer security research many practices have evolved beyond those described in the Menlo report, including the practices of coordinated vulnerability disclosures \cite{ISOIEC29147,householderCERTGuideCoordinated,VulnerabilityDisclosureOWASP} and analyses of the burdens to non-informed systems and users (see e.g. \cite[App.\ B]{demirSimilarityWebMeasurements2023}).
\sstodo{I think this would be hard to follow if you didn't already know what I meant, but I don't want to go into too much more detail}

%The Menlo report, while it does mention this tension, provides no guidance on how to navigate it.
%The Menlo report also falls short in other ways; 
%\begin{itemize}
%\item First of all, this doesn't really map that well onto the modern computer science ethics landscape. 
    %\begin{itemize}
    %\item Pentesting -- Usually takes great care not to affect normal users... but definitely not getting informed consent most of the time.
    %\textbf{In fact oftentimes the research continues even if there is not consent from the company itself (ToS violations).}  Evolution toward coordinated vulnerability disclosure norms (give 3 months notice).
    %\item Web scraping -- Measurement studies rarely if ever deal with consent of collected info.  Web scrapes for other purposes, similar thing.  The ethics calculation is more about burdens.  See e.g. \cite{demirSimilarityWebMeasurements2023}.
    %\item Scripted user studies and sock puppets and social engineering/interactive -- If on a small scale, might be possible to debrief.  But rare.  \sstodo{Cite phishing lit etc}
    %\item Audits and attacks on ML -- Most assume the prompts entered are not being acted upon in a meaningful way.  \sstodo{Find some examples}
    %\end{itemize}
%\end{itemize}



\sstodo{[Move this] [Is this actually helpful or is this kind of a nothing sentence?]}
To be clear, there is no one-size-fits-all ethical analysis; the methods presented here are suggested guidelines that could be overruled in the context of a specific study.


\sstodo{
Study participants,
Study targets,
Third parties
(Computery and people-y for both.  Not with computers as people, but as lying-to-computers impacting humans)
}
\sstodo{X vs CCDH -- burning fb machine bits}
\sstodo{Debbie Hellman -- justice and fairness.  Fairness concern arises if it further marginalizes an already marginalized group.}

\sstodo{Dignity for persons, but who??}

The first two categories roughly align with rights-based/deontological and utility-based/consequentialist criteria (see e.g. Morgan and Henrin, Uncertainty)




%\sstodo{Things we definitely care about:}
%\begin{itemize}
%\item CFAA \cite{kumarLegalRisksAdversarial,parkResearchersGuideLegal,phishing-book,soghoian2008legal}
%\item DMCA/Copyright \cite{parkResearchersGuideLegal}
%\item ToS/Contracts
%\item (shiny new object) Wire Fraud, Identity Fraud, and Identity Theft :)
%\end{itemize}



\sstodo{Other legal risks we saw in at least one paper but like don't seem like a big deal these days:}
\begin{itemize}
\item ECPA? \cite{parkResearchersGuideLegal}
\item Trade secret law? \cite{parkResearchersGuideLegal}
\item Export controls \cite{parkResearchersGuideLegal}
\item The Federal CAN-SPAM Act \cite{phishing-book}
\item Trademark law
\cite{phishing-book,soghoian2008legal}
\item FTC Act \cite{phishing-book}
\item Being sued by your study participants \cite{phishing-book}
\item California anti-phishing act \cite{soghoian2008legal}
\item Trespass to Chattel \cite{soghoian2008legal}
\end{itemize}



\begin{outline}
\1 The increasingly blurry line between ``hacking'' and ``tricking'': see \cite{calo2018}. In particular, Calo cites United States v. Kane, which found that one user's exploitation of a key sequence in a buggy poker machine did \textit{not} constitute a violation of CFAA, though the poker machine was classified as a ``protected computer.''
\2 This is additionally complicated when users act as surrogates for the researcher in order to observe 
\2 In this case, when both human users and ML-driven systems are implicated, we might argue that there are limits to what informed consent can do: unclear what exactly the blackbox is doing with user-submitted study data, and how e.g. model weights might change in response to data submitted under testing conditions. 

\1 Related work: Connections to similar issues in security research in the line of “we want to do research on this really important problem but the legal question is not super clear cut in our favor”
\2 Some encryption research has historically run into legal barriers in the US
\2 This is to the point where DMCA and some other laws now have carveouts for good-faith security research
\2 But it’s still a real issue in the US and elsewhere
\2 We should probably look into papers by Eran Tromer or Ed Felten to look into this backstory

\1 Related work – other areas where CS research runs into legal issues
\2 Notification + disclosure practices as in typical security vulnerability research
\2 Notifying companies that we are going to be investigating their systems and if the systems are as-a-service, telling them the expected load.
\2 Ideally we want to secure agreements with these companies that they will delete all the data from any studies we use rather than processing it (e.g. as training data) to ensure the privacy of our human subjects.
\2 As vulnerability disclosure: give companies 1-3 months notice before publication of vulnerabilities)
\2 Web crawling (in security research or otherwise)

\1 \textbf{Wins}
\2 DMCA carveout for researchers 
\2 Sandvig v. Barr: 2020 ruling that found that platform accountability research on potential sources of racial, gender, or other discrimination does *not* violate CFAA
\2 
\end{outline}


%AFS: I defer to you all how much you want to discuss prior scholarship, but adding stuff here  as I go to avoid having to dig this up later!
If we want to get into related scholarship:
\begin{outline}
\1 \cite{parkResearchersGuideLegal} provides the most complete and current review of legal risks for security research 
\1 \cite{bhandari2024} connects modern attempts to audit platforms for discrimination with the longstanding auditing done in the offline world.
\1 
\1 \cite{marwick2016} reviews best practices for researchers to avoid harassment related to their work on online platforms
\1 \cite{longpre_safe_2024} reviews legal and practical dangers to researchers examining AI systems, and calls for both legal protection and mechanisms to ensure technical access by researchers to AI systems
\1 \cite{mulligan_cybersecurity_2015} examines the chilling effect of legal uncertainty in cybersecurity research
\end{outline}


The \textit{Digital Millennium Copyright Act} (DMCA) was passed in 1998 to address technology that might run afoul of existing copyright law. In particular, its anti-circumvention provision made it unlawful to circumvent technologies intended to prevent unauthorized access to copyrighted material. The \textit{Computer Fraud and Abuse Act} (CFAA) was passed in 1986 in response to growing awareness of nation-state attacks on U.S. government computer systems.\footnote{Reagan famously initiated drafting of the CFAA after watching \textit{War Games.}} In particular, the CFAA prohibits ``unauthorized access'' to ``protected computers,'' though the definitions of both of these phrases are ambiguous as written. Prior to the enactment of the CFAA, numerous computer fraud cases were prosecuted as mail and wire fraud violations \mx{citation}. \textit{Wire fraud} legislation at the state and federal level penalizes the use of electronic communication to deceive or steal (see~\Cref{sec:methods} for a discussion of our age verification research methods). 



\begin{outline}
\1 Known pain points -- summarize this in the background/motivation, then go into more detail later

\1 Finding the line on what is legal and illegal
% \2 How do we define forgery when 1) these modifications are being undertaken in the service of public interest research, and 2) it’s not always clear what actually constitutes identity documentation?
% \3 etc etc
% \3 etc etc
% \2 How can we disclaim that we’re not attempting to forge IDs / not committing a felony? [many of these insights drawn from conversations with Andy, Madeleine, and Will, and are also summarized in the memo]
% \2 if we’re altering names or address information on a card, we should document our attempts (e.g. any search engine lookups) to verify that fictional identities/locations are, indeed, fictional
% we should note, on the altered document, that it is *not* an official identity document
% \2 if the document has a state seal or state official’s signature, we should remove that seal and/or signature from any altered copies of the document
% \3 we should check each state’s regs regarding document mods – these vary
\1 Finding the line on ethics – see ethics analysis below
\1 Communicating with age verification companies; obtaining research access to their software – this isn’t directly a legal issue, but oftentimes their resistance to our research felt motivated by their legal team (or their PR team)
\2 A shortlist of some of their responses to our requests:
\3 Radio silence / no response
\3 Company directs us to existing demo of their service
\3 Marketing team eagerly follows up with an offer to demo their tech; legal team follows up on that followup with more questions about our methods and target venue
\3 Company requests detailed description of our methods and other services we’ll be testing in advance of access
\3 Company requests review of results/draft before we publish
\3 Company requests that we sign off on terms \& conditions before we proceed any further; some of these agreements have included clauses that specifically address the possibility of `reputational harm’ as a result of our findings
\end{outline}



%\cite{nazzalMultiInstanceAdversarialAttack2024} registered multiple domain names on the public internet to demonstrate how malicious domains can avoid detection; they only evaluate their own algorithm but they speculate that deployed systems would also be affected.
    %\cite{heNurgleExacerbatingResource2024} and \cite{yaishSpeculativeDenialServiceAttacks} both carry out (different) Denial of Service attacks on blockchain ``testnets.''
%\cite{zhangVirtualTouchTesla2024} conduct attacks on ``automated control chains'' such as those between smart glasses and vehicles and remark that penetration of the car could also lead to further attacks by using APIs available to the user within the car.
%\cite{slocumThatDoesntGo} describe attacks in augmented reality that involve several kinds of spoofing and deception.
%\cite{moonPrydeModularGeneralizable2024} evaluate attacks against firewalls primarily in a lab setting, but including use of commercial cloud networks such as Amazon Web Services.
%\cite{lisowskiSIMuraISlicingComplexity} build a software version of a SIM card for use in security research into malicious SIM cards.  They tested their software SIM in mobile networks 
Many papers presenting attacks (and sometimes accompanying defenses) operate by deliberately presenting false, misleading, or random data to a system that does not expect it, often automatically at large scale (e.g. \cite{khodayariGreatRequestRobbery2024,wangBreakWallBottom2024,kleinParseMeBaby2024,wangWhereURLsBecome2024} analyzing web vulnerabilities in URLs and website data or  analyzing clouds).
Some papers perform general security auditing of APIs or applications that, among other techniques, involve sending malformed information to the entities, e.g. \cite{duVulnerabilityorientedTestingRESTful,aliRiseInspectronAutomated}.
Other works e.g. \cite{zhangPredecessorawareDirectedGreybox2024,liuAFGenWholeFunctionFuzzing2024,liuLabradorResponseGuided2024,chenChronosFindingTimeout2024,wangSyzTrustStateawareFuzzing2024,xuSaturnHostGadgetSynergistic2024,yangGenericDatabaseManagement,liSDFuzzTargetStates,xiangCriticalCodeGuided,rongUnbiasedMultipleTargetFuzzing,gulerAtroposEffectiveFuzzing2024} advance the state of the art in fuzzing, the process of automatically manipulating data to see if it causes bad effects when input.
%\cite{guMoreHasteLess2024} measures vulnerabilities in Continuous Integration workflows.
\cite{jingRevisitingAutomotiveAttack2024} perform attacks on vehicles, primarily by exploiting weaknesses in entertainment software in the car.
%\cite{zhangEyeSauronLongRange} test their system to detect spy cameras and hidden recording devices in several real-world private locations including in a hotel.


\cite{pantelaiosFV8ForcedExecution,huangDonapiMaliciousNPM,xieArcanumDetectingEvaluating} measure malicious or privacy-invasive browser extensions.



% https://www.usenix.org/system/files/usenixsecurity24-cornelissen.pdf

% https://www.usenix.org/system/files/usenixsecurity24-gohil.pdf Deception only against selves.


% What about deception for defense purposes? E.g. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646653

% \sstodo{This could involve deception at many levels. Red teaming often involves misrepresenting one's true identity at the technical level, e.g. spoofed network information, incorrect credentials, etc.} 

% \sstodo{Can skip reverse engineering for the paper because I think basically all RE in the research context falls under PT and red teaming}
% \subsection{Reverse-Engineering} 
% The DMCA research exemption lists reverse-engineering and security research as separate activities. Reverse-engineering requires 


\footnote{Notably, the list articulated in \textit{Alvarez} specifically includes "fraud," and cites a prior case concerning commercial speech. \textit{See id.} at 717 (citing Virginia Bd. of Pharmacy v. Virginia Citizens Consumer Council, 425 U.S. 748, 771 (1976). The case does not say all fraud is categorically unprotected; it says instead that "[u]truthful speech, commcercial or otherwise, has never been protected for its own sake." \textit{Virginia Bd. of Pharmacy}, 425 U.S. at 771. The full meaning of that is unclear; it cannot mean that all untruthful speech is unprotected, as \textit{Alvarez} says the opposite. But its application to all claims of fraud is more murky. There \textit{is} a preexisting Supreme Court case that more directly addresses a First Amendment challenge to a claim of fraud and finds fraudulent speech unprotected, Illionis ex rel. Madigan v. Telemarking Assocs., Inc., 538 U.S. 600 (2003). Only the dissent in \textit{Alvarez} engages with that case. \textit{See Alvarez}, 567 U.S. at 746 (Alito, J., dissenting).} 


\emph{Informed consent} is an important aspect of studies that is typically considered under \emph{respect for persons}, allowing individuals to understand what being a study participant means and allow them to choose not to participate if they wish.
Many issues of deceiving study participants are mitigated if those affected by the study can consent to the study ahead of time.
When it is possible to do informed consent, the bulk of ethical questions of fraud under the \emph{respect for persons} considerations  are likely resolved, though \emph{beneficience} and \emph{justice/fairness} analyses would still need to be conducted, and it would not eliminate all legal risks in a study.

However, in some cases informed consent for all human participants and stakeholders is difficult.  One reason this can occur is because the research might involve a technical system that is used by high number of users fairly disonnected from the researchers themselves (e.g. a measurement study involving millions or billions of users' Internet traffic measured at one point) \cite{partridgeEthicalConsiderationsNetwork2016}, which is the focus of the Menlo report's discussion of waivers of informed consent.
However, another potential reason this occurs that is more relevant to this work is because the research itself is measuring a response to a deception.




\cite{narayananNoEncoreEncore2015,jakobssonWhyHowPerform2008,partridgeEthicalConsiderationsNetwork2016,wendlerDeceptionPursuitScience2004,kelmanHumanUseHuman1978,cranorConferenceSubmissionReview,finnEthicsGovernanceDevelopment2023,kohnoEthicalFrameworksComputer}). 
\section{OLD}



Some ethical frameworks suggest that deception is never justified under any circumstances (e.g. \cite{baumrindResearchUsingIntentional1985,meadResearchHumanBeings}).
However others make exceptions for specific uses of deceptions (e.g. \cite{wendlerDeceptionPursuitScience2004,belmontReport}, including the Menlo report itself \cite{menloReport}.  We find it illuminating to discuss the circumstances under which these frameworks describe that using deception in a study.


\cite{} presents a view that deception is never justified in psychology research.


\begin{itemize}
\item The research cannot be conducted without deception \cite{}
\end{itemize}

Some studies measure the response of a system or a person to a deceptive stimulus \sstodo{examples}.  In these studies, similar to some in behavioral science research, informing the target of the upcoming deception would prevent the study from reaching its objectives.  In these cases, deception is a necessary part of study design. 

\sstodo{Back to when is deception justified}

Ethical frameworks sometimes suggest that these exceptions are justified if particluar conditions are met.
The Common Rule's criteria \cite{InformedConsentFAQs} for a waiver of informed consent for general research in the setting of Health and Human Services are that:
\begin{enumerate}
\item The research could not practicably be carried out without the waiver;
\item The research involves no more than minimal risk to the subjects [as applied to computer science research on networks or platforms, this typically involves ensuring that ``bystander'' users whose actions, data, or information are being observed in the study are minimally harmed];
\item The waiver will not adversely affect the rights and welfare of the subjects [similarly, in the computer science context this typically applies to bystander users]
\item Whenever appropriate the subjects will be provided with additional pertinent information after participation.
\end{enumerate}
We see these conditions as reasonable and helpful guidelines for navigating deception in computer science research.
There is also ongoing discussion in the community about how to address these issues
There are past examples of computer security research that violated these principles, as well as significant discussion in the community about how to address these issues (e.g. \cite{narayananNoEncoreEncore2015,jakobssonWhyHowPerform2008,partridgeEthicalConsiderationsNetwork2016,wendlerDeceptionPursuitScience2004,cranorConferenceSubmissionReview,finnEthicsGovernanceDevelopment2023,kohnoEthicalFrameworksComputer}). Suggestions generally either  suggest a stricter set of criteria such as forbidding any deception at all, or encourage clearly stating the ethical reasoning of the work, including how these principles were met.

The details of any particular study design will dominate the decision of whether a particular study is ethically acceptable or not, moreso than the mere presence or absence of deception, however it is clear that the presence of deception raises a flag that must be somehow justified or mitigated in order for the study to reach the bar.
We see these as helpful starting points, but not resolutions; moreover as we have discussed throughout this paper and especially in Section \sstodo{XXX}, the ethical question does not resolve the legal issues.

%\item The deception should be done in a fair and just manner that treats subjects equitably
%\item Particularly at-risk subjects are entitled to extra protections
%\item When reasonably possible, researchers should ``debrief'' subjects at the conclusion of their participation or of the study

\sstodo{yueDarkFleeceProbingDark}


%We think that the guidelines above should apply to legal analyses of research that may be facing fraud charges.


\sstodo{No deception in the research, but might end up with weird fraud accusations anyway (Afghanistan election investigation https://www.usenix.org/system/files/usenixsecurity24-panahi.pdf }

\mx{dropping relevant cits. about research ethics below}

\cite{elovici2014ethical}
\cite{dimkov2010two}



\sstodo{Ends vs means. If the object of your study is means and not ends, then put it in a sandbox as best you can.  Avoids both legal risk and harm.  Age verif and penetration testing.}

\sstodo{Probably ditch this}
This legal governance of ethical standards is relatively new to law, emerging in the context of the strong push to establish ethical practices and review in biomedical and then social research in the 1960s in the context of atrocities of the 20th Century \cite{robertt.bowerEthicsSocialResearch1978,curranGovernmentalRegulationUse1969}.
Ethical review came to ``information and computer technology'' much later, mainly through conversation and controversy among researchers that eventually led to the publication of the ``Menlo Report'' \cite{menloReport}.

\sstodo{Wizard Of Oz Experiments}




% cutting for space: In addition to concerns under the DMCA, researchers can confront issues under "standard" copyright law, for copied of programs and media made as a part of research and analysis. Courts, fortunately, have consistently found such intermediate copying to be a "fair use" under the standard analysis \as{add Corellium, copying for critique cases}