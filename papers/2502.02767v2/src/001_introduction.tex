
\section{Introduction}
\label{sec:introduction}

Effective computer science research often requires a degree of impersonation.  Research into increasingly complex systems necessarily requires the research to emulate the behavior of different types of users, or represent to either machines or their operators that they are someone they are not. In so doing, researchers increasingly confront criminal and civil laws that prohibit certain forms of misrepresentation on online platforms. In a United States (U.S.) context,\footnote{As a lawyer and scholars from the United States, we focus our discussion on U.S. law.} this is most broadly in the domain of federal wire fraud (18 U.S.C. §1343), but also state and federal laws concerning identify fraud or identity theft. Research into  AI systems, security systems, social media platforms, or Internet infrastructure can involve deception, and deception can reintroduce some of the criminal legal risk that had been previously relegated. And because some of the most significant research in this domain raises questions about government or political behavior on online systems---e.g., cybersecurity or authentication
for government services, or research into the spread of political disinformation -- governments may possess both means and motive to use criminal law to deter such research.

Many computer science researchers have found themselves in need of legal guidance in the course of their research.
In the United States, this may be most well known in the field of computer security, where researchers sometimes brush with the Computer Fraud and Abuse Act (18 U.S.C. § 1030)  \cite{parkResearchersGuideLegal, mulligan_cybersecurity_2015}, %\sstodo{cite cfaa examples}, 
the Digital Millennium Copyright Act (17 U.S.C. § 1201; DMCA) \cite{simons_viewpoint_2001,noauthor_thawing_2021},
and perform security analyses that sometimes violate Terms of Service with companies whose products they are analyzing. More recently, social media and other online platforms researchers have experienced similar threats under similar laws, compounded by threats to disable or suspend personal researcher accounts \cite{edelsonUnderstandingEngagementUS2021, longpre_safe_2024}.
Early research into the functions of generative artificial intelligence platforms have begun to receive similar threats. \cite{kumarLegalRisksAdversarial, edwards2024}

Under these commonly-asserted laws, there are now commonly-asserted defenses. While the DMCA prohibits the act of attempting to ``circumvent'' technical tools to access copyright-protected material, there exist both statutory exemptions for reverse engineering (§1201(f)), encryption research (§1201(g)), and security testing (§1201(j)), alongside a more comphrehensive exemption for security research passed as part of the triennial rulemaking conducted by the Library of Congress that modifies the scope of the DMCA (37 C.F.R. § 201.40(b)(1)). In 2021 the United States Supreme Court limited the scope of the CFAA in a useful way in Van Buren v. United States, and the following year the Department of Justice announced a charging policy to avoid charging good-faith security research under the same law. \cite{vanBuren2021, OfficePublicAffairs2022} In more recent litigation, lawyers representing researchers have also found ways to argue against the scope and application of website terms of service, though the full effect of those arguments remains to be seen. 
The reduced application of the CFAA and DMCA has led to a popular belief that most legal risks for software and platform research now are exclusively questions of civil liability.


However, fraud remains relatively under-examined in computer science research.
Although this risk has been mentioned before in research on phishing (see \Cref{sec:related-fraud}), the analysis of how fraud laws apply to existing methods in computer science research is not sufficient for researchers and policymakers to understand the interaction between research and anti-fraud laws. Moreover, as we will discuss in \Cref{sec:methods}, the landscape of computer science research has expanded and changed in such a way that these laws are more relevant than ever before: in particular, the ubiquity and opacity of AI/ML-driven systems will increasingly compel researchers to devise clever and possibly deceptive methods for investigating how these systems work; measuring their impact on platforms and people; and understanding what risks might arise with their continued use. 

In this paper, we study the interactions between end users, researchers, and systems in the context of developments in security research. How can researchers understand and examine these systems in safe, ethical, and legally and technically defensible ways?  