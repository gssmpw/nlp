\section{Research Ethics for Identity and Fraud}
\label{sec:ethics}

In this section we examine fraud-related ethical considerations in computer science research, aided by but not fully addressed by existing research ethics frameworks.
As Albert and Grimmelmann recently cautioned, one should not equate law with ethics: ``[l]aws are not always right, and they are never neutral,'' \cite{albert_right_2023}; similarly, Thomas et al.\ remind us that ``[o]ccasionally research may be illegal but still ethical,'' \cite{thomasEthicalIssuesResearch2017}. Ethics and law are rightly separate, albeit related, inquiries.
Many jurisdictions require by law that scientific work funded by federal agencies be subject to ethical review (e.g. the ``Common Rule'' \cite{protectionsohrpFederalPolicyProtection2009} in the U.S.).


The formal guidance that computer scientists working with IRBs would be familiar with provides only high-level principles relevant to addressing ethics and especially those related to deception. Similarly although most computer security publication venues require an ethics analysis when relevant \cite{finnEthicsGovernanceDevelopment2023}, in practice they supply little practical advice when considering questions of deception or fraud.
Newer discussions about research ethics specific to computer security are ongoing (e.g. \cite{FosteringResponsibleComputing,cranorConferenceSubmissionReview,kohnoEthicalFrameworksComputer}).

In this section we discuss guidance for tackling these questions in significantly more detail, though each study will pose case-by-case questions that we cannot address fully in an abstract analysis.
This ethical analysis dovetails with the legal analysis both to illuminate guidelines for scientists undertaking research where fraud may be relevant, and also suggests potential paths that might change on the legal side -- in other words, this ethical analysis helps us ensure that our legal analysis works with, not against, ethical considerations.

\textbf{The Menlo Report and its limits.} For computer science research, the typical ethical analysis for protecting human subjects in studies reviewed by an IRB is summarized in the 2012 Menlo Report \cite{menloReport}, similar to the 1979 Belmont Report \cite{belmontReport} for protecting human subjects in biomedical and behavioral research.


The Menlo report centers four lenses with which to consider the ethics of a particular study:
\begin{enumerate}
\item \emph{Respect for Persons}: A rights-based deontological approach that focuses on respecting study participants' and other individuals' rights as autonomous agents (especially via informed consent).  
\item \emph{Beneficence}: A utilitarian benefits vs harms analysis regarding research participants. Under this branch researchers are expected to maximize benefits to research participants, to identify and minimize harms, and determine mitigation strategies for harms that remain.
\item \emph{Justice and Fairness}: Researchers should try to consider and distribute the benefits and burdens of the research equally. This can apply to participant selection, the study itself, and the outputs and goals of the study.
\item \emph{Law and Public Interest}: While the Menlo report makes clear that researchers are not expected to be legal experts, it does ask researchers to do their due diligence in trying to ensure their study is fully legal, to be as transparent as possible, and to hold themselves accountable if something goes wrong. Additionally, the study should hold some public value.
\end{enumerate}


While the Menlo report is not a fully comprehensive ethical framework (and in particular ignores several difficult questions of computer security ethics including how to responsibly manage vulnerability disclosures \cite{ISOIEC29147,householderCERTGuideCoordinated,FosteringResponsibleComputing})
we find this to be a good place to start when considering standard informed consent practices and times when the use of deception may be justified.
In many studies, these principles are applied in a relatively straightforward way.
In the best case scenario, researchers gather informed consent from all involved in the study \cite{menloReport,belmontReport,InformedConsentFAQs,cranorConferenceSubmissionReview},
much computer security research involves minimal or no harms, and, with proper participant selection and study design, raises few or no issues of justice and fairness.


Ethical restrictions always require researchers to protect the privacy of those involved with their study \cite{menloReport,ethicsOfOnlineResearch,alllmanIssuesEtiquetteConcerning2007}.
Researchers should also take into account whether users would generally expect collected information to be private or not, especially in the context of social media \cite{ethicsOfOnlineResearch}.
This can pose especially thorny issues when using a public dataset that was the result of a leak \cite{thomasEthicalIssuesResearch2017}.
As identified in the Menlo report, computer security research often needs ``protection of human subjects'' to apply not only to study participants, but also to other humans in the system, and occasionally to computer systems as well.
Taking as an example a hypothetical web scraping study, a typical study design would not have users whose data was scraped sign a consent form.
However,  researchers are expected to protect the interests of the individuals whose data is represented and are expected to avoid taking illegitimate action to obtain datasets \cite{menloReport}. (However, the dataset may still be of illicit origin even if the researchers did nothing illegitimate, for example if the dataset was leaked and made publicly available.) These issues are not directly related to deception and are addressed in other ethical analyses of computer science research, e.g.\ \cite{thomasEthicalIssuesResearch2017,partridgeEthicalConsiderationsNetwork2016,menloReport,cranorConferenceSubmissionReview}.

\textbf{The ethics of deception.}
In the context of this work on fraud, when computer science research involves deception on the part of the researchers or their agents -- whether that deception is directly to a person, to a computer system, or involves ``bystanders'' in the wild -- that poses a major ethical hurdle.

As is true in other research fields like social or biomedical research, the use of deception should not be taken lightly and raises an ethical barrier that the study must overcome if it is justified (and indeed some would argue that deception is never justified in research under any circumstances \cite{baumrindResearchUsingIntentional1985,meadResearchHumanBeings}, although many ethical frameworks in research permit deception under some circumstances \cite{findleyObligatedDeceiveAliases2016,goodeEthicsDeceptionSocial1996,InformedConsentFAQs,belmontReport,wendlerDeceptionPursuitScience2004}).

That said,
we see deception in computer security as  different from deception in social and biomedical research.  Tying in with our discussion of deception in \Cref{sec:def-deception}, this is because:
\begin{enumerate}
\item The deception is often \emph{to a computer system} rather than a person
\item The deception is often done \emph{by a computer system} rather than a person
\item The purpose of the deception is often to \emph{test the security or boundaries} of a system, and therefore often violates written or unwritten rules of the system
\end{enumerate}

This suggests the need to adapt ethical guidelines for dealing with deception into a computer security context.

\subsection{Justified uses of deception in computer security research}

Based upon 
the Common Rule's criteria for a waiver of informed consent \cite{InformedConsentFAQs},
the Menlo and Belmont reports \cite{menloReport,belmontReport},
ethics considerations in computer security and measurement \cite{FosteringResponsibleComputing,partridgeEthicalConsiderationsNetwork2016,kohnoEthicalFrameworksComputer,cranorConferenceSubmissionReview},
the history of laws governing ethical restrictions on experiments \cite{curranGovernmentalRegulationUse1969,finnEthicsGovernanceDevelopment2023},
justifications for deception in social and other research \cite{goodeEthicsDeceptionSocial1996,christensenDeceptionPsychologicalResearch1988,findleyObligatedDeceiveAliases2016},
and post-mortems of prominent ethical controversies in computer security \cite{narayananNoEncoreEncore2015,jakobssonWhyHowPerform2008},
we suggest that the use of deception in computer security can be justified when the following conditions are met:


\textbf{1. The ``least deceptive means'' are used; less deceptive methods are not feasible or effective.}
Given that concerns about deception in research largely derive from \emph{respect for persons} \cite{menloReport,belmontReport,FosteringResponsibleComputing} (and, to some extent, concerns about maintaining trust in science \cite{baumrindResearchUsingIntentional1985,meadResearchHumanBeings,robertt.bowerEthicsSocialResearch1978,curranGovernmentalRegulationUse1969}),
we believe that deception can be justified if researchers are respecting persons as much as the study goals reasonably allow.
Study design plays a big part in this ethical determination, just as it does in measurement studies \cite{partridgeEthicalConsiderationsNetwork2016}.
Controlled lab experiments rather than experiments ``in the wild'' can remove this element of deception, when feasible.
Additionally, in keeping with the respect for persons principle and keeping in mind the blurrier line between ``hacking'' and ``tricking'' machines \cite{calo2018}, we see deception \emph{of systems and machines} as less directly problematic than deception \emph{of people} -- with the caveat that indirect deception of people is still deception of people.

\textbf{2. The study's value warrants the use of deception.}
From a more consequentialist perspective, most computer security studies are intended to measure or improve the security of systems used by many people, and thus provide a justification for performing the research.  The value of the knowledge gained as a result of the study is a necessary but not sufficient condition for the study to be ethical.  In particular, a study that provides minimal value even if the study went perfectly well may not be sufficient justification to overcome the ethical issues arising from deception.  This principle is also found in justifications of deception in psychology research \cite{christensenDeceptionPsychologicalResearch1988,goodeEthicsDeceptionSocial1996,findleyObligatedDeceiveAliases2016}.

\textbf{3. ``Bystanders'' involved with the deceived system will face minimal or no harms, rights violations, or injustices as a result of the deception.}
Inspired by the Common Rule's requirements for a waiver of informed consent \cite{InformedConsentFAQs} and similar concerns about minimizing harms in measurement studies \cite{partridgeEthicalConsiderationsNetwork2016}, we believe that no amount of deception would be justified in a study if ``bystanding'' users were to be significantly harmed as a result.
This principle also underlies existing ethical principles in computer security for ensuring that one does not hack ``live'' systems that will have an impact on users \cite{kohnoEthicalFrameworksComputer} and ensure the privacy and confidentiality of ``bystander'' users is protected \cite{menloReport,alllmanIssuesEtiquetteConcerning2007,cranorConferenceSubmissionReview}.


