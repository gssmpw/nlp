\section{Uses of deception in security research}
\label{sec:methods}

In this section, we present working definitions of ``deception'' and ``deceptive acts.'' We then introduce a taxonomy of security research methodologies and describe how elements of deception have historically been deployed in each (\Cref{sec:taxonomy}). Following, we present notes on identity verification research and discuss how this line of work presents a unique challenge: it implicates multiple subcategories in our methods taxonomy \textit{and} blurs the line between means and ends (\Cref{sec:id_work}). We note that descriptions and examples provided in this section are intended to be representative, and are not exhaustive surveys of the research landscape. 




\subsection{Defining ``deception''}
\label{sec:def-deception}

In the broadest of terms, we define \textit{deception} as \textit{an intentional act of misrepresentation}. 
We separate a ``deceptive act'' into four component parts: the deceiver (``who''), the mode of misrepresentation (``by what means''), the deceived (``whom''), and the purpose of the deception (``to what ends''). In our discussion of the second of these, we also include brief descriptions of what forms of identification or action constitute ``authentic'' or non-deceptive representations in the same setting. 
\newline

\noindent\fbox{%
    \parbox{\linewidth}{ 
    \textit{\textbf{Deception}} in the context of security research occurs when (1) a researcher or system controlled by a researcher (2) makes a false or misleading statement about their identity, their motivations or their intent, or some other aspect of their research methodology to (3) another party or system controlled by a party, in order to (4) obtain information about or access to a system, or to measure the success of, or response to, the deception.}
}
\newline

Here we further explore each of these components (\textit{who} is deceiving \textit{whom}? \textit{By what means}, and \textit{to what ends}?). These real and hypothetical examples are in preparation for the following section (\Cref{sec:taxonomy}) which describes computer security research methods and how they might be viewed as deception.

\subsubsection{Who?} 

\textbf{Human actors}, including researchers and study surrogates acting on behalf of researchers, might employ deceptive techniques in order to understand human and automated responses to the deception in an experimental setting. Employees of companies with in-house or third party red teams might use deception to identify vulnerabilities in an internal computer system. Researchers might enter false information into an AI or other automated system to influence its response. \textbf{Automated systems}, including AI and ML-driven systems deployed by researchers and non-AI/ML driven programs (e.g., automated tenant screening \cite{oyama2009not}), might generate sham data or behavioral nudges in the course of (e.g.) A/B testing or enforcing rate limits. 

\subsubsection{By what means?}

Actors might use \textbf{false names}, or use real names in a misleading way.
They might submit \textbf{false email addresses, phone numbers, or other information} to login processes, or use real email addresses or phone numbers in a misleading way. This includes spoofing (i.e., sending illegitimate messages from) real email addresses or phone numbers and other use of real email addresses or phone numbers that does not involve sending messages from those accounts.

Actors might use \textbf{false account credentials or devices}, or use real account credentials or devices in a misleading way. This includes, but is not limited to, attempting to log in to a computer using compromised credentials and stating the compromised credentials to a person. 
Actors might also use \textbf{false data or spoofed network/device information}, and may also use misleading, altered, or spoofed \textbf{metadata} (e.g. to convince a system they are in a particular physical location).

\textbf{Biometric spoofing} entails use of substitute or fake biological evidence to circumvent biometric verification systems, such as face and fingerprint scanners. Researchers have demonstrated that it is possible to circumvent fingerprint scans by presenting prints lifted from a glass surface using a tacky material (such as the gelatin in a gummy bear) to a scanner \cite{matsumoto2002impact}. \textbf{Fake and modified IDs} can be used (in research and the real world) to evade verification systems that use government-issued identity documents. Depending on jurisdiction and nature of the modification, however, this might constitute a criminal offense \cite{hunt2023kind}. \textbf{Communication of false, misleading, or incomplete information} intersects with many of the previously mentioned modes of deception; this modality additionally includes textual information sent or withheld from online platforms, email communication, and prompts sent to LLMs or other AI models.

\subsubsection{Whom?}

The intended targets of these deceptions (and affected parties, more generally) can include \textbf{human actors}, such as users of an online platform; employees of companies that host web services of interest to researchers; government employees; and participants in a research study. Just as often, \textbf{computer systems} are the direct targets of these deceptions: for instance, hackers might attempt to gain access to login or other credential systems, computer networks, and AI systems. They might adopt any of the approaches mentioned in the previous section in order to spoof credentials, jailbreak guardrails, or overwhelm a distributed network with a 51\% attack \cite{aponte202151}. 


\subsubsection{To what ends?}
In our earlier definition of deception, we broadly summarized the objectives of security research with deceptive elements as follows: 
to obtain a) \textbf{information} about or b) \textbf{access} to a system, or 3) to perform \textbf{measurement} of the success of the deception. Among other activities, a) would include most \textit{platform studies}, measurement and/or social engineering experiments that require researchers to perturb an existing social network to understand how human users or the system might respond to those perturbations. The measurement might encompass a combination of human and system-wide responses (e.g., ``how many users have to click on my fake ads before Facebook takes them down?''). b) includes many activities that might qualify as ``traditional security research'' methodologies: e.g., penetration testing and red teaming. c) comprises identity-spoofing red teaming attacks and a number of newer adversarial ML techniques targeting LLMs; the common thread across these works is that the efficacy of the adversarial technique is generally a primary research outcome because the target system is a black box. 

