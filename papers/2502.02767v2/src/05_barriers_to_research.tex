\subsection{Deception in practice: a taxonomy}
\label{sec:taxonomy}

We present a taxonomy of methods in computer science research that may implicate fraud. We drew on existing taxonomies and analyses in similar areas in order to formulate this section: \cite{sandvig2014} discusses research methods for investigating algorithmic discrimination on platforms; \cite{gilbert2024risks} reports on legal difficulties in modern behavioral science; \cite{parkResearchersGuideLegal} presents a guide to navigating the legal landscape for security researchers; and
\cite{thomasEthicalIssuesResearch2017} discusses works that involve datasets of illicit origin.
Using the taxonomies set forth in these works as a starting point, we queried Google Scholar for all search queries resulting from the Cartesian product of the following sets: $\texttt{methods} = \lbrace$\textit{penetration testing, red teaming, adversarial ML, reverse engineering, social engineering, sock puppets, undercover studies, user studies, web scraping, scripted studies}$\rbrace$ and $\texttt{axes} = \lbrace$\textit{research ethics, fraud, legality}$\rbrace$. We include social science publications alongside works appearing in the proceedings of tier 1 security venues in 2024.

For each taxon (denoted by italic font), we provide 1) a description of the study methodology; 2) a description of potentially deceptive elements of this research methodology; and 3) reference(s) to academic work employing this method in conjunction with some form of deception. Where relevant, we highlight works whose authors responsibly disclose deceptive elements of their work to participants at the conclusion of the study; we also discuss works where an ethics review or responsible disclosure would ideally be included. See Table \ref{tab:research_taxonomy} for a summary.

Our intention is to emphasize the prevalence and importance of deception in good faith computer science research. 
Note, also, that the descriptions and examples we provide in each taxon are intended to be representative summaries and are non-exhaustive. 


\subsubsection{Penetration testing, red teaming, and reverse engineering} 
\label{sec:pentesting}
``Traditional'' security red teaming entails whitehat hacking of a software system in order to understand its vulnerabilities. Oftentimes, the attacker and target are (or are acting on behalf of) the same entity---e.g., a company might employ in-house security red teams in order to test the robustness of its own systems. In particular, red teaming might require an attacker to misrepresent their true identity at the technical level via spoofed network information or incorrect credentials.
\textbf{Example studies:} 
\cite{gallowayPracticalAttacksDNS2024} carries out attacks on Domain Name System reputation systems (i.e., systems that attempt to rate website domains as benign vs. spam vs. phishing, etc). Acting as an attacker controlling a domain, the attacker (researcher) takes actions such as adding unrelated highly-rated benign links to their DNS server to trick the reputation system into giving the attacker (researcher) a higher reputation score, despite the fact that the domain contained binaries that should have been classified as malware (modified by the researchers to remove the actual malware). The researchers conducted this research in concert with a security vendor to ensure the effects were ``isolated,'' but they were interacting with real systems in a deceptive way.
As another example, the first experiment of \cite{maFakeBehalfImperceptibleEmail} involves sending emails from spoofed addresses (e.g. ``admin@google.com'') to several email providers in order to measure how those providers implement various spoofing protections. \cite{maFakeBehalfImperceptibleEmail} disclosed all vulnerabilities to the affected service provider; the authors' study protocol received an exemption from their IRB. 



\subsubsection{Web scraping and measurement}
\label{sec:passive_mmt}
Web scraping refers to the practice of using automated tooling to collect information from Internet webpages; snapshots are static captures of Internet webpages (comprising text, image, and metadata contents). In general, social media companies do not make codebases for their ranking and content moderation algorithms publicly available; it is similarly difficult to compel these companies to disclose their data collection practices to the general public.\footnote{The EU has made some progress in this regard, e.g. in the Digital Services Act.} As such, researchers interested in platform accountability work conduct data scrapes, audits, and qualitative analyses of platform content (among other techniques) in order to evaluate platform transparency. 
\textbf{Example studies:} 
\cite{kreibichSpamCampaignTrail2008} ``infiltrates'' a botnet by scraping and reverse engineering the Storm botnet's distribution platform., and then acting as a worker within the net.
\cite{decary2014policing} deploys bots in IRC chat rooms frequented by hackers in order to record chat logs without disclosing the presence of bot accounts to chat room participants. 

\subsubsection{Scripted user studies}
\label{sec:scripted}
Researchers hire study participants to perform a pre-specified sequence of actions (e.g., an information retrieval task) as described by an \textit{activity script}. Participants might be asked to use their own identity information or social media accounts in order to perform these actions. In general, the motivation for these methods (as opposed to sock puppets) is ecological validity: observer effects resulting from inauthentic actions online or new, historyless accounts might invalidate collected data. 
\textbf{Example studies:} 
\cite{dimkov2010two} records two instances of ``physical'' red teaming that require a study subject to use social engineering to obtain ID credentials to gain entry to a protected room.
\cite{haeder2016secret} uses ``secret shoppers'' to evaluate insurance policy holders' ease of access to healthcare providers. \cite{dimkov2010two} disclosed the deception to employees affected by the social engineering experiment; \cite{haeder2016secret} does not mention a disclosure process.  


\subsubsection{Sock puppets}
\label{sec:sock_puppets}
Researchers devise false identities in order to perform testing at scale, to access systems they would otherwise be unable to access using their own credentials, or to create controlled user histories for interacting with platform algorithms. In most cases, researchers take pains to make sure that these fabricated identities do not coincide with the identifying information of known living persons. Researchers performing platform studies with sock puppets often deliberately obfuscate the fact that they are operating fake accounts in order to preserve the ecological validity of their results. \textbf{Example studies:} We discuss four studies that use sock puppet accounts to interact with their respective social media platforms. \cite{boshmaf2011socialbot}
deploys Socialbot Network, comprising 102 bot accounts, to connect with more than eight thousand (real) Facebook users. These users did not know that they were interacting with fake accounts. \cite{bandy2021more} creates eight sock puppet accounts on Twitter, manipulate each account's actions, then observe changes in source and topic diversity in each account's timeline. \cite{srba2023auditing} and \cite{hosseinmardi2024causally} create sock puppet accounts on Youtube in order to observe how the platform's recommendations evolve to accommodate artificially curated watch histories. For both \cite{bandy2021more} and \cite{srba2023auditing}, the target of the deception was the recommendation algorithm. Work similar to that of \cite{boshmaf2011socialbot}, in which real world users interact with fake accounts, would ideally receive review from an IRB; after each interaction, or at the conclusion of the study, users on the platform that interacted with a bot account should be debriefed about the true nature of those interactions.  

\subsubsection{Social engineering and interactive studies}
\label{sec:soc_eng}
Researchers interact with web users without informing users of their intention to 1) alter elements of the online environment(s) or 2) observe the effects that these changes have on users. Deception is a central element of this work: by design, the research subject is not privy to their own participation in experimental work and, as a result, cannot give informed consent. \textbf{Example studies:} \cite{di2022revisiting} conducts a longitudinal analysis of web administrators' responses to individual user requests for personal information; the requests were sent by the research team under aliases and included modified ID information (see \Cref{sec:id_work}). The study authors received consent from the owners of the credentials used to submit data requests. At the conclusion of the study, the study authors contacted the organizations targeted by their data requests to disclose their study objectives and inform them of their findings. 
\cite{acharyaConningCryptoConman2024} created tweets ``tailored to lure scammers,''  ``bait [scammers] into revealing their payment information,'' and then validate the scam by ``observ[ing] private key theft by scammers.''  These straightforwardly interacted with scammers in the wild in a deceptive way.
Additionally, much phishing research falls into this category: \cite{jakobssonWhyHowPerform2008} describes experimental designs employing deceptive URLs and email addresses; more recently, \cite{royChatbotsPhishbotsPhishing2024} investigates similar questions, but includes LLM-generated phishing attacks.

\subsubsection{Audits} 
\label{sec:audits}
Most social media platforms (and other online services) do not make code and data readily available to the public. Researchers employ a variety of techniques in order to infer under-hood mechanisms for platform operations (e.g., content moderation and ranking): for instance, counterfactual testing, robustness and sensitivity testing, and explainability analyses. 
\textbf{Example studies:}
\cite{kaplanMeasurementAnalysisImplied2022} measures discrimination in Facebook ad distribution by posing as a Facebook advertiser and running job ads to determine potential biases in their distribution. The authors took measures to minimize harm to Facebook users by only creating ads for real job listings. These ads only displayed text germane to the actual job listing, and no personal information was collected about users who clicked on each fake ad. 
\cite{westPictureWorth5002024} analyzes ML tasks in mobile apps; they detected the tasks, reconstructed the pipeline, and then assessed performance.  The performance assessment portion involved a script that would inject data inputs into the model and measure results. 
As AI models have gotten more effective at doing voice and face recognition, researchers evaluating those systems present false data to them in the process of evaluating them (e.g. \cite{jiangCanHearYour,chengALIFLowCostAdversarial2024,kimScoresTellEverything2024}).

\subsubsection{Attacks on machine learning and artificial intelligence} 
\label{sec:adversarial_ml}
In order to understand the robustness of ML-driven systems, researchers use prompt engineering (in the case of LLMs), poisoning attacks, and jailbreaking techniques to identify vulnerabilities at all points in the ML pipeline: training set contents, model weights, and the scope of the reported evaluation. Some of these approaches might require researchers to misrepresent themselves to the system under analysis. For example, researchers might prompt an LLM as though they were someone else to cause a change in the response or to reverse-engineer the model or its training data. In recent times, image forgery attacks have been aided by advanced image generation networks \cite{cox2024}. Researchers might also measure the susceptibility of a model to data poisoning by attempting to cause the model to train on false or misleading data. 

Additionally, researchers might probe the boundaries of watermarks, guardrails, or other restrictions on the outputs of generative AI models (see, for instance, the ``Grandma jailbreak'' for LLMs \cite{zhang2023jailbreak}); toward this end, researchers might feed the AI false information. In the coming years, we expect that attacks, defenses, and models in this space will evolve rapidly and additional types of attacks will become relevant---including those that contain false representations. As well, we expect that research in these fields will become even more important as these systems are used for more social and technical purposes. \cite{carliniAreAlignedNeural} is a recent example of such an attack on ``live'' large language models.

We do not provide an exhaustive taxonomy of ML attacks in this section; see references for a discussion of research on intelligent systems, and see \cite{kumarLegalRisksAdversarial} for a discussion on the CFAA implications of adversarial ML research.
\textbf{Example studies:}
\cite{perez2022red} uses language models to red team LLMs.
\cite{yanLLMAssistedEasyTriggerBackdoor} attacks code completion models to inject vulnerabilities by poisoning the model's data and causing it to yield code in which vulnerabilities are hard to detect. \cite{carta2022video} and \cite{scherhag2017biometric} use video stream injection and facial morphing applications, respectively, to circumvent biometric face scans. 
Several systems investigate jailbreaking of models, often by crafting prompts in such a way that harmful instructions are ``disguised'' to avoid guardrails (e.g. \cite{carliniAreAlignedNeural,liuMakingThemAsk2024,yangSneakyPromptJailbreakingTextimage2024,yuDontListenMe2024}).
Works that generate deepfakes (for example, to build databases of deepfakes for other researchers to use) also mention the need to pay special attention to legal consideration since this is a developing area (e.g. \cite{mengAVAInconspicuousAttribute2024,laytonSokGoodBad}).
Although LLMs are much more widespread now, this line of research to understand ``adversarial'' inputs causing unexpected results in machine learning models started much earlier \cite{huangAdversarialMachineLearning,kurakinAdversarialMachineLearning2017}.

\subsection{Research involving legal identification}
\label{sec:id_work}
\label{sec:ids}
The age verification study we describe in~\Cref{sec:motivation} implicates several of these approaches, including red teaming and auditing.
However, that work and others like it (for instance, \cite{di2022revisiting, di2019personal}) raise additional questions because they require that researchers present or alter identity documents.
To the extent that these images are significantly different from their own identities, these methods constitute a form of deception, undertaken in the service of understanding the mechanism and performance of these verification algorithms. In \Cref{sec:law}, we discuss why research involving modification of legal ID documents is uniquely legally challenging.

\cite{guanIDNetNovelDataset2024} and \cite{lerougeDocXPand25kLargeDiverse2024} both build datasets dataset of synthetic ID cards from a training dataset of 5.8k screenshots of real IDs and photos. Xie et al.\ additionally create fraudulent versions of the synthetic IDs for further training.