\documentclass[12pt]{article}

\usepackage{orcidlink,lmodern}
\usepackage{graphicx, float, caption}
\usepackage{natbib}
\usepackage{authblk} % Manages authors' affiliation
\usepackage{multirow}
\usepackage{hyperref}

\usepackage{amsmath,amssymb,amsfonts,amsthm}%
\usepackage{xcolor}%
\usepackage{manyfoot}%
\usepackage{algorithm2e}%
\usepackage{ulem}
\usepackage{mathtools} % ceiling operators
\usepackage{breqn} % brackets on multiple lines
\usepackage{bbm} % for \mathbbm 1
\usepackage{natbib}
\usepackage{mathrsfs}
\usepackage{array}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{url}
\usepackage{anyfontsize} 
\usepackage{silence} 
% To get rid of the following warnings:
\WarningFilter*{latex}{Text page \thepage\space contains only floats}
% This stopped MANY warnings about fonts
% being unavailable and replaced by others, as well as
%   Warning: Size substitutions with differences
\usepackage{makecell}
\usepackage{algorithmic}

\hyphenation{RaFFLE}
\hyphenation{PILOT}

% Definitions of handy macros can go here
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert} % long abs/norm
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclareMathOperator{\logit}{logit}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\var}{\mbox{Var}}
\newcommand{\Prob}[1]{\mbox{P}\left( #1 \right)}
\newcommand{\thetab}{\boldsymbol{\theta}}
\newcommand\logten{\ensuremath{\log_{10}}}
\newcommand{\eb}{\boldsymbol{e}}
\newcommand{\xb}{\boldsymbol{x}}
\newcommand{\ub}{\boldsymbol{u}}
\newcommand{\bz}{\boldsymbol z}
\newcommand{\yb}{\boldsymbol{y}}
\newcommand{\zb}{\boldsymbol{z}}
\newcommand{\Lb}{\boldsymbol{L}}
\newcommand{\Ub}{\boldsymbol{U}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bR}{\boldsymbol{R}}
\newcommand{\alphab}{\boldsymbol{\alpha}}
\newcommand{\betab}{\boldsymbol{\beta}}
\newcommand{\etab}{\boldsymbol{\eta}}
\newcommand{\gammab}{\boldsymbol{\gamma}}
\newcommand{\mub}{\boldsymbol{\mu}}
\newcommand{\thetabb}{\boldsymbol{\theta}}
\newcommand{\mubt}{\tilde{\boldsymbol{\mu}}}
\newcommand{\thetabt}{\tilde{\boldsymbol{\theta}}}
\newcommand{\mut}{\tilde{\mu}}
\newcommand{\thetat}{\tilde{\theta}}
\newcommand\maT{\mathcal{T}}
\newcommand\maE{\mathbb{E}}
\newcommand\maR{\mathbb{R}}
\newcommand\maI{\mathcal{I}}
\newcommand\maD{\mathcal{D}}
\newcommand\maS{\mathcal{S}}
\newcommand\con{\textnormal{\textsc{con}}}
\newcommand\lin{\textnormal{\textsc{lin}}}
\newcommand\pcon{\textnormal{\textsc{pcon}}} 
\newcommand\blin{\textnormal{\textsc{blin}}} 
\newcommand\plin{\textnormal{\textsc{plin}}} 
\newcommand\pconc{\textnormal{\textsc{pconc}}}

%\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}% meant for a single numbering over all types.
\newtheorem{lemma}{Lemma}%  also.
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence 
%% instead of independent numbers for propositions.
\newtheorem{proposition}[theorem]{Proposition} 
\newtheorem{corollary}[theorem]{Corollary}
%%\newtheorem{proposition}{Proposition}% to get separate
%%numbers for theorems and propositions etc.

% \theoremstyle{thmstyletwo}%
% \newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

% \theoremstyle{thmstylethree}%
% \newtheorem{definition}{Definition}%

\definecolor{orange1}{RGB}{255,128,0}
\definecolor{purple2}{RGB}{102,0,204}
\definecolor{blue}{RGB}{0,0,255}
\definecolor{red}{RGB}{255,0,0}
\providecommand{\blue}[1]{\textcolor{blue}{#1}}
\providecommand{\red}[1]{\textcolor{red}{#1}}

\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
%\addtolength{\topmargin}{-1in}%
\addtolength{\topmargin}{-0.8in}%

\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}
{#1}\small\normalsize} \spacingset{1}

\title{A Powerful Random Forest Featuring Linear Extensions (RaFFLE)}

\author[1]{Jakob Raymaekers}
\author[2]{Peter J. Rousseeuw}
\author[1]{Thomas Servotte}
\author[1]{\\Tim Verdonck}
\author[1,2]{Ruicong Yao}

\affil[1]{Department of Mathematics, University 
          of Antwerp, Belgium}
\affil[2]{Section of Statistics and Data Science, 
          University of Leuven, Belgium}

\setcounter{Maxaffil}{0}
\renewcommand\Affilfont{\itshape\small}
\date{February 14, 2025}           
  \maketitle

\bigskip
\begin{abstract}
Random forests are widely used in regression. However, the decision trees used as base learners are poor approximators of linear relationships. To address this limitation we propose RaFFLE (Random Forest Featuring Linear Extensions), a novel framework that integrates the recently developed PILOT trees (Piecewise Linear Organic Trees) as base learners within a random forest ensemble. PILOT trees combine the computational efficiency of traditional decision trees with the flexibility of linear model trees. To ensure sufficient diversity of the individual trees, we introduce an adjustable regularization parameter and use node-level feature sampling. These modifications boost the accuracy of the forest. We establish theoretical guarantees for the consistency of RaFFLE under weak conditions, and its faster convergence when the data are generated by a linear model. Empirical evaluations on 136 regression datasets demonstrate that RaFFLE outperforms the classical CART and random forest methods, the regularized linear methods Lasso and Ridge, and the state-of-the-art XGBoost algorithm, across both linear and nonlinear datasets. By balancing predictive accuracy and computational efficiency, RaFFLE proves to be a versatile tool for tackling a wide variety of regression problems. 
\end{abstract}

\noindent {\it Keywords:} 
Algorithm; Consistency; Linear Model Trees; Machine Learning; Regression.

\section{Introduction}
Regression random forests are versatile ensemble learning techniques that have gained popularity due to their simplicity and strong predictive performance. 
At the core of a regression random forest, decision trees like CART \cite{cart} act as the base learners. Such a decision tree is a hierarchical method where data is split into subsets at each node, based on the value of a feature, and continuing until terminal nodes (leaves) are reached. Each leaf yields a prediction based on the average response of the training cases in that leaf, and the overall tree is thus a piecewise constant fit where the pieces are (hyper)rectangles in the original predictor space. However, a single decision tree can be sensitive to small changes in the training data, leading to high variance and overfitting.

Random forests, originally introduced in \cite{breiman2001random}, address this limitation by building a collection of decision trees, each trained on a different sample of the data, with the final prediction obtained by aggregating the predictions from all trees. This ensemble approach reduces variance, leading to more stable and reliable predictions, especially when dealing with noisy data or complex relationships.

Two key components make random forests effective: bootstrap aggregating (`bagging') and random feature selection. With bagging, each tree in the forest is trained on a bootstrap sample of the dataset, which is created by random sampling with replacement. This technique introduces diversity among the trees, as each tree receives a slightly different version of the data. Random feature selection means that during the training of each decision tree, only a random subset of features is considered for splitting at each node. This decorrelates the trees, preventing any single feature from dominating the decision-making process and ensuring that the ensemble captures a broad spectrum of data characteristics.

Random forests are particularly well-suited for regression tasks due to their ability to fit complex, non-linear relationships between input variables and the target variable. They can handle high-dimensional data, mixed data types (numerical and categorical), missing values, and interactions between features without explicit specification. Additionally, random forests inherently provide measures of feature importance, giving insights into the factors driving the predictions. While there have been many advances in regression such as Gradient Boosted Trees \citep{xgboost, lightgbm} and Neural Networks \citep{tabnet}, random forests remain popular and are often found to outperform more complex methods over a wide range of tasks \citep{rf_performance_rodriguez2015, FERNANDEZDELGADO201911, zamo2014benchmark}.

Despite their advantages, traditional random forests have limitations, particularly in handling datasets with linear characteristics. This limitation has driven research into new types of decision trees and ensembles that can extend or replace the standard random forest framework, addressing specific challenges or enhancing predictive performance in particular contexts. An overview of ensemble approaches for regression can be found in \cite{mendes2012ensemble}. An ensemble of generalized linear models (GLMs) was proposed by \cite{song2013random}, who found that it performs similarly to a random forest. Ensembles of regression trees with some linear characteristics, also called linear model trees, were investigated by \cite{ao2019linear}, \cite{rodriguez2010experimental} and \cite{li2011learning}, illustrating the potential of random forests composed of more expressive base learners. \citet{freund1996experiments} proposed boosting as a versatile framework, laying the groundwork for Gradient Boosted Decision Trees (GBDT) such as XGBoost \citep{xgboost} and LightGBM \citep{lightgbm}. Later, \citet{shi2019gradient} proposed a GBDT method using piecewise linear model trees as base learners, highlighting the effectiveness of combining boosting with alternative base learners.

In this paper we propose RaFFLE (\textbf{Ra}ndom \textbf{F}orest \textbf{F}eaturing \textbf{L}inear \textbf{E}xtensions), a random forest of linear model trees. The latter are of the PILOT type, which stands for \textbf{PI}ecewise \textbf{L}inear \textbf{O}rganic \textbf{T}ree \cite{raymaekers2024pilot}.  PILOT trees combine the computational efficiency and flexibility of classical decision trees with the expressiveness of linear model trees, making them particularly well-suited to handle datasets where linear relationships play a substantial role. By leveraging the strengths of PILOT, RaFFLE offers several advantages over traditional random forests and other ensemble methods. These include improved handling of both linear and nonlinear data structures and a balance between predictive accuracy and computational efficiency. We develop RaFFLE as a solid framework for regression tasks, and rigorously evaluate its theoretical properties and empirical performance.

The paper is structured as follows. In Section \ref{pilot_sec:methodology} we describe the PILOT algorithm and its properties. We also explain the modifications adapting PILOT for use in a random forest, including the introduction of a regularization parameter, node-level feature sampling, and computational speedups.
 
In Section \ref{pilot_sec:theory} we provide a rigorous theoretical analysis of RaFFLE. We prove its consistency under weak conditions as well as its convergence rate on linear data, and discuss the implications for predictive performance. We also derive its computational complexity.
 
In Section \ref{pilot_sec:results} we present an empirical evaluation of RaFFLE on 136 regression datasets, comparing its performance with that of CART and the traditional random forest, the state-of-the-art XGBoost algorithm, and the Lasso and Ridge regularized linear methods. The results showcase RaFFLE's superior accuracy across datasets with diverse characteristics, highlighting its versatility.
 
Finally, in Section \ref{pilot_sec:conclusion} we summarize the key contributions of RaFFLE and propose directions for future work.

\section{Methodology}\label{pilot_sec:methodology}
We first provide an overview of the PILOT algorithm, and then describe the modifications to make PILOT a suitable base learner in a random forest ensemble.

\subsection{Linear model trees by PILOT}

The popular piecewise constant binary trees require little computational cost, but they yield inefficient approximations of smooth continuous functions. To resolve this issue, linear model trees have been proposed \citep{torgo1997functional, loh2011classification, stulp2015many}. They preserve the tree-like structure for partitioning the predictor space, but in the leaf notes they allow for linear combinations of the variables. These model trees offer greater flexibility while preserving the simple structure of traditional regression trees. Until recently, one of the main drawbacks of linear model trees was their high computational cost. As a result, they were rarely used to analyze large datasets, and  incorporating them into ensemble methods would lead to infeasible computational costs.

The PILOT (\textbf{PI}ecewise \textbf{L}inear \textbf{O}rganic \textbf{T}ree) method \cite{raymaekers2024pilot} is a novel linear model tree algorithm designed to address the limitations of earlier approaches for fitting linear model trees. Most notably, PILOT runs with the same time complexity as the classical CART algorithm for binary decision trees. This gives PILOT the ability to fit very large datasets and opens the door to the integration of linear model trees in ensemble methods such as random forests. In addition to a relatively low computational cost, PILOT improves upon existing algorithms for linear model trees by efficiently guarding against extrapolation, and by incorporating on-the-fly regularization to avoid overfitting. This regularization makes it unnecessary to prune the tree afterward, hence the adjective `organic'. The method enjoys strong theoretical support, as it is consistent on additive data and achieves faster convergence rates when the data are generated by a linear model. Empirically, PILOT outperforms CART when the underlying relationship between predictors and response is roughly continuous and smooth, while performing on par otherwise. 

The PILOT algorithm takes a very similar approach to CART in that it builds the regression tree through a greedy top-down process. More precisely, PILOT starts with all observations in one node, and splits this node if that improves the fit. When split, the observations within a node are divided between the two child nodes, and the process is repeated in each of the child nodes separately. Unlike CART however, PILOT incorporates linear models in this process, in the following way. Instead of only considering a piecewise constant model on each predictor, PILOT considers five different models and selects the best one. 
These five models are shown in Figure~\ref{fig:fivemodels}.
The piecewise constant model (\textsc{pcon}) is that of CART, and it also applies to categorical regressors. A node that isn't split has a constant fit (\textsc{con}). To these models PILOT adds three models with linearity. The first type is the univariate linear model (\textsc{lin}), which does not split the node. Another is the `broken line' (\textsc{blin}) which is continuous, and the most general is the piecewise linear model (\textsc{plin}). 

\begin{figure}[!ht]
    \centering
    \begin{tabular}{ccccc}
        \textbf{p}iecewise & & & \textbf{b}roken & 
        \textbf{p}iecewise \\ 
        \textbf{con}stant & \textbf{con}stant & 
        \textbf{lin}ear & \textbf{lin}e &
        \textbf{lin}ear \\   
        \includegraphics[width = 0.17\textwidth]{pcon.pdf} &
        \includegraphics[width = 0.17\textwidth]{con.pdf} &
        \includegraphics[width = 0.17\textwidth]{lin.pdf} &
        \includegraphics[width = 0.17\textwidth]{blin.pdf} &
        \includegraphics[width = 0.17\textwidth]{plin.pdf}\\
    \end{tabular}
\caption{The five regression models used in PILOT:
  \textsc{pcon}, \textsc{con}, \textsc{lin}, 
  \textsc{blin} and \textsc{plin}.} 
\label{fig:fivemodels}
\end{figure}

After a model is fitted, the residuals of that fit are passed on as the new response for the next step. When  \textsc{con} is fit, the recursion in that node stops. Only \textsc{pcon}, \textsc{blin}, and \textsc{plin} split the node into two child nodes. When \textsc{lin} is fit, the node is not split but the residuals are updated and passed on to the next step. For a given feature, all five models can be evaluated in one pass through the feature, just as in CART. The coefficients of the linear model in a leaf node are obtained by adding the coefficients encountered in the path from root to leaf node, as illustrated in Figure~\ref{fig:treeplot}.

\begin{figure}[!ht]
\center{\includegraphics[scale=0.8]{pilot_example.pdf}}
\caption{An example of a PILOT tree} 
\label{fig:treeplot}
\end{figure}

Of course, as the \textsc{plin} model is strictly more general than the other models, selecting the model type solely based on the largest reduction in residual sum of squares would lead to only selecting \textsc{plin} models. To avoid this, in each node PILOT uses a Bayesian Information Criterion (BIC) to balance model complexity with goodness of fit. It is given by
\begin{equation} \label{eq:BIC}
  \mbox{BIC} = n\log\Big(\frac{\mathrm{RSS}}{n}\Big)+\nu\log(n)
\end{equation}
which depends on the residual sum of squares $\mathrm{RSS}$, the number of cases $n$ in the node, and the degrees of freedom $\nu$ of the model. PILOT uses $\nu = 1, 2, 5, 5, 7$ for the \textsc{con}, \textsc{lin}, \textsc{pcon}, \textsc{blin} and \textsc{plin} models.
When fitting a node, PILOT chooses the combination of a feature and a regression model that yields the lowest BIC value. This avoids overfitting in a computationally efficient way, in contrast to approaches that require costly post-hoc pruning.\\

\subsection{PILOT in a random forest}

While PILOT did very well in empirical studies, its performance as a standalone method does not automatically make it a good base learner for a random forest. In particular, PILOT was designed with a lot of emphasis on avoiding overfitting. This was needed in view of the large flexibility of combining recursive partitioning and linear model fits. Due to this implicit regularization, a PILOT tree typically has a smaller variance than a CART tree trained with the same maximum depth. Random forests leverage base learners with relatively large variance, so the out-of-the-box PILOT is less suited as a base learner.

In order to make PILOT suited as a base learner in a random forest, we thus need to curtail its inherent regularization and allow the linear model trees to have more variability. To achieve this, we propose two major changes. The first is to adjust the BIC criterion used to select the best model fit in a given node, which balances goodness of fit and model complexity. For this we propose to replace the BIC criterion of equation~\eqref{eq:BIC} by
\begin{equation} \label{eq:BICalpha}
  \mbox{BIC}_\alpha = 
  n\log\Big(\frac{\mathrm{RSS}}{n}\Big) +
  \nu_\alpha \log(n)
\end{equation}

\vspace{1mm}
\noindent where $\nu_\alpha \coloneqq 1 + \alpha(\nu - 1)$ and $0 \leqslant \alpha \leqslant 1$ is a tuning parameter. So when $\alpha = 1$, the default $\nu$ values from the standalone PILOT are used. For $\alpha=0$ all model types have equal $\nu$ values of 1, and we would only fit \textsc{plin} models. The lower the value of $\alpha$, the more the tree will tend to overfit the data, which is desired to some extent as we want to increase the variability of the base learner in the random forest.

The second major change to the original PILOT algorithm is that we disable the broken line (\textsc{blin}) model. The broken line is a regularized version of the piecewise linear (\textsc{plin}) model. Removing it typically results in proportionally more \textsc{plin} fits, increasing the variance of the tree. As an additional advantage, the \textsc{blin} model is the most computationally demanding one.  Therefore removing this model results in a significant speed-up during training, which turned out not to lose performance in a random forest. We emphasize that when fitting a standalone PILOT tree, it remains advised to keep \textsc{blin} in the set of models.

In addition to partially curtailing the regularization, we made several small adjustments. In particular, after shifting the response up or down to give it a symmetric range $[-B,B]$, we truncate all predictions to the interval $[-B, B]$ instead of $[-1.5B, 1.5B]$ as originally proposed. This means that the individual trees cannot generate predictions outside of the range of the response in the training data. This option is safer and was found to perform the best in general. 

Overall, our modifications enhance the diversity of the PILOT trees in the random forest, reduce the computational cost of the trees, and safeguard against extrapolation. We now construct RaFFLE as a random forest of linear model trees trained by the modified PILOT algorithm. The two main building blocks of random forests are used. The first is that each tree in the ensemble is trained on a newly generated bootstrap sample of the data. The second is random feature selection. In the classical random forest, each tree is trained on a newly generated random subset of $q$ features where $q \leqslant p$. We go one step further, by newly generating a random subset of $q$ features at every node of every tree. The PILOT trees trained with both bootstrapping the data and this node-level random feature selection we call  \textit{random PILOT trees}, to distinguish them from the deterministic standalone PILOT trees. The pseudocode of RaFFLE is shown in Algorithm~\ref{algo:RaFFLE}.

\begin{algorithm}
\caption{The RaFFLE fitting algorithm} \label{algo:RaFFLE}
\begin{algorithmic}[1]
\REQUIRE \
\vspace{-2mm}
\begin{itemize}
\itemsep -3pt
    \item $X$ (Feature data)
    \item $y$ (Target values)
    \item \textsc{n\_estimators} (Number of PILOT trees)
    \item \textsc{n\_features\_tree} (Number of features used by each tree)
    \item \textsc{n\_features\_node} (Number of features used in the nodes of each tree)
    \item \textsc{alpha} (Tuning parameter for degrees of freedom of each node type)
    \item \textsc{max\_depth} (Maximum depth of trees, excluding \textsc{LIN} nodes)
    \item \textsc{max\_model\_depth} (Maximum depth of trees, including \textsc{LIN} nodes)
    \item \textsc{min\_sample\_fit} (Minimum number of cases required to fit any model)
    \item \textsc{min\_sample\_alpha} (Minimum number of cases required to fit a
    
    piecewise model)
    \item \textsc{min\_sample\_leaf} (Minimum number of cases required in leaf nodes)
\end{itemize}
\STATE $\textsc{n\_features\_node} \leftarrow \min(\textsc{\_features\_tree}, \textsc{n\_features\_node})$
\STATE Initialize $estimators$ as a list of
\textsc{n\_estimators} random PILOT trees that

use this parameter \textsc{n\_features\_node} 
\FOR{$i = 1$ to $n\_estimators$}
    \STATE Take a random bootstrap sample with row indices $r_i$
    \STATE Fit $estimators[i]$ on $(X[r_i], y[r_i])$
    using the above parameters
\ENDFOR
\end{algorithmic}
\end{algorithm}

We have implemented PILOT in \texttt{C++} with a \texttt{Python} interface using \texttt{pybind11}. RaFFLE is built in \texttt{Python} around this interface.

\section{Theoretical results}\label{pilot_sec:theory}

We now focus on the theoretical analysis of RaFFLE, by exploring its consistency and convergence rate.
 
The notations are aligned with \cite{raymaekers2024pilot}. In particular, $||x||_n$ and $\langle x,y\rangle_n$ denote the empirical squared norm and the empirical inner product for $n$-dimensional vectors $x$ and $y$. 
The set $\maT_k^{(m)}$ consists of the tree nodes at depth $k$ plus the leaf nodes of depth lower than $k$, in the $m$-th random PILOT tree of the RaFFLE method $\mathcal{R}_{M,K} = (\maT^{(1)}_{K},\dots,\maT^{(M)}_{K})$ with $M$ trees and maximum depth $K$. 
The prediction of the PILOT tree at depth $k$ is denoted as $\hat f(\maT_k^{(m)})$, and $\hat f(\mathcal R_{M,K})$ is the prediction of RaFFLE. 
The training error at depth $k$ is denoted as $R_k^{(m)}:=||Y-\hat f(\maT_k^{(m)})||^2_n - ||Y-f||^2_n$ where $f$ is the underlying function and $Y$ is the response variable. We occasionally omit $m$ for notational simplicity.

We can rewrite $R_k$ as $\sum_{T\in \mathcal{T}_K}w(T)R_k(T)$ with weights $w(T):=t/n$ where $t$ is the number of cases in node $T$, and $R_k(T):=||Y-\hat f(\mathcal{T}_k)||^2_t-||Y-f||^2_t$. Then we immediately have $R_{k}=R_{k-1}-\sum_{T\in \mathcal{T}_{k-1}}w(T)\Delta^{k}(T)$ where
$$\Delta^{k}(T):=||Y-\hat{f}(\mathcal{T}_{k-1})||^2_t\;-\;t_l||Y-\hat{f}(\mathcal{T}_{k})||^2_{t_l}/t\;-\;t_r||Y-\hat{f}(\mathcal{T}_{k})||^2_{t_r}/t$$
is the impurity gain on $T$, and $t_l$ and $t_r$ denote the number of cases in the left and right child of $T$. In particular, if the $\lin$ model is selected, then by convention we let $t_r=0$ since there is no split. The design matrix of all $n$ cases is denoted as $\bX\in\maR^{n\times p}$. Given some tree node $T$ with $t$ cases, we denote by $\bX_T:=(X_{T_1},\dots,X_{T_t})^\top\in\maR^{t\times p}$ the restricted data matrix and by $X^{(j)}_T$ its $j$-th column. The variance of the $j$-th column is given by $\hat\sigma_{j,T}^2:=\sum_{k=1}^t(X_{T_k}^{(j)}-\overline{X^{(j)}_T})^2/t$. We also let $(\hat\sigma_{j,T}^u)^2$ be the classical unbiased variance estimates with denominator $t-1$. The $T$ is omitted when $T$ is the root node, or if there is no ambiguity. The total variation of a function $f$ on $T$ is denoted as $||f||_{TV(T)}$. For nonzero values $A_n$ and $B_n$ which depend on $n\rightarrow\infty$, we write $A_n\precsim B_n$ if and only if $A_n/B_n\le \mathcal{O}(1)$, and $\succsim$ is analogous. We write $A_n\asymp B_n$ if $A_n/B_n= \mathcal{O}(1)$. The complement of a set $A$ is denoted by $A^c$, and $\# A$ stands for the cardinality of $A$.

RaFFLE trains the $m$-th tree in the forest $\mathcal{R}_{M,K}$ on a bootstrapped sample of data points 
$\maD_{\maI_m}\subset \maD_n$ with indices $\maI_m$. In addition to that, at each node it uses only $q \leqslant p$ predictors that are selected at random. In the following, we use $\Xi_k$ to denote the random variable that selects the sets of predictors $\maS_{T}\subset\{1,\dots,p\}$, in every node $T$ at depth $k$, where $\#\maS_{T} = q$. Note that $\Xi_k$ is independent of the cases and the response but depends on $m$, but for notational simplicity we omit $m$ here. 

\subsection{Consistency}\label{subsec:consistency}
We assume the underlying function $f\in\mathcal{F}\subset L^2([0,1]^p)$ admits an additive form
\begin{equation} \label{eq:additive}
  f(X):=f_1(X^{(1)})+\dots+f_p(X^{(p)})
\end{equation}
where $f_j$ has bounded variation and $X^{(j)}$ is the $j$-th predictor. We define the total variation norm $||f||_{TV}$ of $f\in\mathcal{F}$ as the infimum of $\sum_{j=1}^p||f_j||_{TV}$ over all possible representations of $f$, and assume that the representation in \eqref{eq:additive} attains this infimum.
The consistency of a single PILOT tree was established in \citep{raymaekers2024pilot}.
Following \cite{klusowski2024large} we now extend this consistency result to the forest ensemble. 

We first derive the expected impurity gain of a random PILOT tree at each depth $k$ on the dataset $\maD_{\maI_m}$, and then aggregate the gains over $K$ steps for an error estimate of the entire tree. In the end we will take the expectation over all trees to prove the consistency of RaFFLE.

For the expected gain at depth $k$, it suffices to consider the conditional expectation on $\Xi_k|\Xi_{k-1}$ which captures the randomness of $\maS_T$ for any $T$ at depth $k-1$, i.e.\ $ \maE_{\Xi_k|\Xi_{k-1}}\left[\max_{j\in\maS_T}\widehat\Delta^{k}(\hat s_j,j,T)\right]$. Here, $\hat s_j$ is the optimal split point for feature $j$ in terms of the gain $\widehat\Delta^{k}$ in node $T$. We start with the situation when $\widehat\Delta^{k}$ stems from a  \textnormal{\textsc{pcon}} node and later generalize the results to arbitrary nodes. Lemma \ref{lem:pcon_step_est} gives a lower bound on the expected impurity gain of \textsc{pcon} for a specific node $T$ at depth $k$.

\begin{lemma}\label{lem:pcon_step_est}
Assuming the response variable is bounded in $[-B,B]$ and $R_{k-1}(T)>0$ in some node $T$, then the expected impurity gain of \textnormal{\textsc{pcon}} on this node satisfies
\begin{equation*}
   \maE_{\Xi_k|\Xi_{k-1}}\left[\max_{j\in\maS_T}\widehat\Delta^{k}_{\mbox{\tiny PCON}}(\hat s_j,j,T)\right]\geqslant \frac{qR_{k-1}^2(T)}{p(||f||_{TV}+2B)^2}
\end{equation*}
where $\hat s_j$ is the optimal splitting point of a \textsc{pcon} model fit to the observations in node $T$ using the $j$-th feature.
\end{lemma}

All proofs of this subsection are in Appendix A.

We next derive Lemma \ref{lem:bic} which leverages the BIC model selection to quantify the expected impurity gain of any selected model relative to that of \textsc{pcon}.

\begin{lemma}\label{lem:bic}
Consider some node $T$ with $t$ cases. For any random set of predictors $\maS_T$ induced by $\Xi_k|\Xi_{k-1}$ we let $\Delta_1$, $\Delta_2$ and $\nu_1$, $\nu_2$ be the impurity gains and degrees of freedom of two regression models on $T$ based on $\maS_T$. Let $R_T$ be the initial residual sum of squares in $T$. We have two results.
\begin{itemize}
\item Let $E_1$ be the event under $\Xi_{k}|\Xi_{k-1}$ that model 1 does better than \textnormal{\textsc{con}}, i.e.\ $BIC_{\mbox{\tiny{CON}}}> BIC_1$. Then we have
$t\maE[\widehat\Delta_1|E_1]/R_T> C(\nu_1,t)>0$ for some positive function $C$ depending on $\nu_1$ and $t$. Similarly $t\maE[\widehat\Delta_1|E_1^c]/R_T\leqslant C(\nu_1,t)$. Here, 
$$C(v,t) = \frac{1}{t}\Big(1-\exp\Big((\nu_{\mbox{\tiny CON}}-\nu_1)\frac{\log t}{t}\Big)\Big).$$
\item 
Let $E_2$ be the event that the selected model is not $\con$. Let $\widehat\Delta$ be the impurity gain of the selected non-$\con$ model based on the BIC criterion and the random predictors $\maS_T$ induced by $\Xi_{k}|\Xi_{k-1}$. Then we have
$\maE[\widehat\Delta|E_2] \geqslant \frac{1}{4}\maE[\Delta_{\pcon}|E_2]$.
\end{itemize}
\end{lemma}

\begin{remark}\label{rem:connode}
%\noindent {\bf Remark.}
Note that in a random tree it is not necessarily true that after selecting \textnormal{\textsc{con}} at a node $T$, subsequent steps will also choose \textnormal{\textsc{con}}. This is because the random set $\maS_T$ may not include the optimal predictor $\widehat j_p$ that has an impurity gain using a non-$\con$ model. When the depth increases, $\widehat j_p$ and the corresponding model still have a chance to be selected. In our analysis we will therefore use Lemmas \ref{lem:pcon_step_est} and \ref{lem:bic} to estimate the impurity gain for nodes that include the optimal predictor $\widehat j_p$, otherwise, we lower bound the gain by zero. 
\end{remark}

We now obtain the error estimation of a single random PILOT tree in Theorem \ref{theorem:errorRandomPILOT}.

\begin{theorem}\label{theorem:errorRandomPILOT}
    Let $f\in \mathcal{F}$ with finite\ $||f||_{TV}$ and denote by $\hat f(\mathcal{T}_K)$ the prediction of a $K$-depth random PILOT tree. Suppose that $X\sim P$ on $[0,1]^p$ and the response variable is a.s.\ bounded in $[-B,B]$. Let $q$ denote the number of random predictors out of $p$. Then
\begin{equation}
\maE_{\Xi_k}(||Y-\hat f(\mathcal{T}_K)||^2_n)\, \leqslant ||Y-f||^2_n \, + \frac{4p(||f||_{TV}+2B)^2}{q(K+3)}+\maE_{\Xi_k}(R_{C_K^+})\;.
\end{equation}
Moreover, if we let $K=\log_2(n)/r$ with $r>1$, then $\maE_{\Xi_k}(R_{C_K^+})$ goes to zero at the rate $\mathcal{O}\left(\sqrt{\log (n)/n^{(r-1)/r}}\right)$.
\end{theorem}

Finally, we obtain the universal consistency of RaFFLE on additive data:
\begin{theorem}\label{theorem:consistency}
   Let $f\in \mathcal{F}$ with finite\ $||f||_{TV}$ and denote by $\hat f(\mathcal {R}_{M,K_n})$ the prediction of RaFFLE with M trees and depth $K_n$. Suppose $X\sim P$ on $[0,1]^{p_n}$ and the response variable is a.s.\ bounded. If the depth $K_n$ of each tree, the number of random predictors $q_n$ and the number of total predictors $p_n$ satisfy $q_nK_n/p_n\rightarrow\infty$ and $2^{K_n}p_n\log(np_n)/n\rightarrow0$, then RaFFLE is consistent, that is
\begin{equation}
\lim_{n\rightarrow\infty} \maE[||f-\hat f(\mathcal {R}_{M,K_n})||^2]=0.
\end{equation} 
\end{theorem}


\subsection{Convergence rate on linear data}\label{subsec:convergencelinear}

In this subsection we study the convergence rate of RaFFLE on linear data. As RaFFLE incorporates linear model trees, we expect the convergence rate to be better than the universal rate obtained in the previous section. The key to the proof is to tackle the randomness introduced by $\Xi_k$ and $\maD_{\maI_m}$. We assume Conditions 1 and 2 below.
\begin{itemize}
\item \textbf{Condition 1:} The PILOT algorithm stops splitting a node whenever
\begin{itemize}
\vspace{-0.2cm}
\item the number of cases in the node is less than $n_{\mbox{\tiny min}}=n^\delta$ for some $0<\delta<1$; 
\item the variance of some predictor is less than $2\sigma_0^2$ where $0<\sigma_0<1$; 
\item the volume of the hyperrectangle of the node is less than a threshold $\eta > 0$.
\end{itemize}
\item \textbf{Condition 2:} We assume that $X\sim P$ on $[0,1]^p$ and the error $\epsilon$ has a finite fourth moment. Moreover, for any hyperrectangle $C$ with volume up to $\eta$ we assume that\linebreak $\lambda_{min}(Cor(X|X\in C)) \geqslant 2\lambda_0>0$, where $Cor(X)$ is the correlation matrix.
\end{itemize}

We denote the least squares loss on the node $T$ by $L^*_n(T):=\min_{\hat\beta}||Y-\boldsymbol X_T\hat\beta||^2_T$ and the least squares loss on the full data by $L^*_n:=\min_{\hat\beta}||Y-\boldsymbol X\hat\beta||^2_n$. We further denote by $L^k_n:=||Y-\hat f(\maT_k)||^2_n$ the loss of a $k$-depth PILOT tree, and by $L^k_n(T):=||Y_T-\boldsymbol X_T\beta_T||^2_t$ its loss in node $T$ for some $\beta_T$ (we can write the loss like this because the prediction function is linear on $T$). When the depth $k$ is not important, we omit the superscript. We use the notation  $\boldsymbol{\tilde X}_T$ for the standardized predictor matrix obtained by dividing each (non-intercept) column $j$ in $\bX$ by $\sqrt{n}\hat\sigma^u_j$ where $(\hat\sigma^u_j)^2$ is the unbiased estimate of the variance of column $j$. 

We first derive the expected impurity gain of the $\lin$ model when $q$ predictors are selected randomly out of $p$.
\begin{lemma}\label{lem:lin_step_est}
Let $T$ be a node with $t$ cases with training loss $L^{(k-1)}_n(T)$, and $L^*_n(T)$ the training loss of least squares regression. In addition to Conditions 1 and 2 we assume that the residuals of the response on T have zero mean. 
The features in the node form $\boldsymbol X_T\in\mathbb R^{t\times p}$ which is a subset of $\boldsymbol X\in\mathbb R^{n\times p}$ randomly depending on $\boldsymbol X$, $\Xi_{k-1}$ and the noise. 
If the \textit{random} PILOT tree considers $q\leqslant p$ variables selected by $\Xi_k|\Xi_{k-1}$, then there exists a constant $C_{\lambda_0,\sigma_0,p}$ such that 
 $$P_{\boldsymbol X_T}\Bigg[\maE_{\Xi_k|\Xi_{k-1}}\left(\Delta_{\lin}\right)\geqslant \frac{q^2\lambda_0(L_n^{k-1}(T)-L^*_n(T))}{4p^3}\Bigg]
 \geqslant 1-\exp(C_{\lambda_0,\sigma_0,p}t).$$
\end{lemma}

All proofs of this subsection are in Appendix B.
Since $\maS_T$ includes the optimal predictor for the $\lin$ model, Lemma~\ref{lem:lin_step_est:cond} follows  immediately.

\begin{lemma}\label{lem:lin_step_est:cond}
For any $\Xi_{k-1}$, let $L_n^{k-1}(T)|_{\Xi_{k-1}}$ be the squared error in node $T$ with $t$ cases given $\Xi_{k-1}$. Then there exists $C_{\lambda_0,\sigma_0,p}>0$ such that
\begin{align*}
 & P_{\boldsymbol X,\varepsilon}\Bigg[\max_{\maS_T\sim\Xi_{k}|\Xi_{k-1}}\left(\Delta_{\mbox{\tiny LIN}}\right)\geqslant \frac{\lambda_0(L_n^{k-1}(T)|_{\Xi_{k-1}}-L^*_n(T))}{4p},\;\;\; \forall \Xi_{k-1}\Bigg]\\
 \geqslant& P_{\boldsymbol X,\varepsilon}\left[\lambda_{min}(\tilde {\boldsymbol X}_T^\top\tilde {\boldsymbol X}_T)>\lambda_0, \;\;\; \forall T\in \maT_{k-1}, \forall \Xi_{k-1}\right]\geqslant1-\exp(-C_{\lambda_0,\sigma_0,p}t)\,.
\end{align*}
\end{lemma}

Next, this yields a bound for an entire tree.

\begin{theorem} \label{theorem:recursion2}
Assume the data is generated by the linear model $Y\sim\boldsymbol X\beta + \varepsilon$ with $n$ cases. Under Conditions 1 and 2, the difference between the training loss $L^k_n$ of a \textit{random PILOT tree} (depending on $\Xi_K$) at depth $K$ and the training loss $L^*_n$ of least squares regression satisfies
\begin{equation}
\maE_{\boldsymbol X,\varepsilon,\Xi_k}[L^K_n-L^*_n] \leqslant \gamma^K\sqrt{\maE_{\boldsymbol X,\varepsilon,\Xi_k}[(L^0_n-L^*_n)^2]}+\mathcal{O}(\log(n)/n^{\delta})
\end{equation}
where 
$$\gamma:=1-\frac{q\lambda_0}{4p^2}\;.$$
\end{theorem}

Now we can prove the fast convergence rate of RaFFLE on linear data.

\begin{theorem}[Fast convergence on linear data] \label{theorem:rate}
Assume the conditions of Theorem~\ref{theorem:recursion2} hold and that $|Y|$ is a.s. bounded. Let $K_n= log_\gamma(n)$. Then we have for any $0<\delta<1$ that
\begin{equation}
  \maE_{\maD_n,\varepsilon,\Xi_{K_n}}[||\hat f(\mathcal {R}_{M,K_n}) - \boldsymbol X\beta||^2] \leqslant \mathcal{O}\left(\frac{\log(n)}{n^{\delta}}\right)\;.
\end{equation}
\end{theorem}

To illustrate RaFFLE's favorable convergence rate on linear data, we ran the following simulation. We generate a random feature matrix $X$ with $n = 8000$ cases, $p = 20$ features and an effective rank of 16. The target variable $y$ is subsequently calculated using random coefficients drawn from a uniform distribution in the range $[0, 100]$, plus Gaussian noise with mean zero and a standard deviation of 0, 0.1, 0.5 and 1. From these data we take out a fixed test set of size 2000 and keep it separate. Finally, we let the number of training cases vary between 10 and 6000 in steps of size 200 and fit Ordinary Least Squares regression (OLS), RaFFLE, the classical random forest (RF), and XGBoost (XGB). We validate the performance on the test set using the $R^2$-score. This is repeated 5 times using different random seeds. The $R^2$-scores are then averaged over these 5 repetitions.
Figure \ref{fig:linear_convergence} summarizes the results of this experiment.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{linear_convergence.pdf}
    \caption{Average $R^2$-scores over 5 runs on simulated linear data, for increasing training set size.}
    \label{fig:linear_convergence}
\end{figure}

The first vertical dashed line indicates the point where RaFFLE reaches 97\% of the OLS $R^2$-score, and the second line where RaFFLE reaches 99\%. We see that RaFFLE needs substantially less data to approximate the OLS solution to a given degree than the classical RF and XGBoost. The latter do not even reach the 97\% for a training set size of 6000.

The analysis in this section demonstrates that RaFFLE not only inherits the consistency of its PILOT base learners but also its fast convergence rate on linear data. These results highlight the  reliable performance of RaFFLE across a wide range of regression tasks. The flexibility of RaFFLE makes it a strong candidate for modeling both linear and nonlinear data structures.

\subsection{Time and space Complexity}
Since PILOT has the same time and space complexity as CART as shown in Section 3.3 of \citep{raymaekers2024pilot}, RaFFLE also has the same complexity as the original CART forest. In particular, the presorting procedure on the full dataset requires $\mathcal{O}(n p\log(n))$ time. For each tree, it requires $\mathcal{O}(n)$ time to get one bootstrap sample. At each node, the time complexity of selecting a model is $\mathcal{O}(n q)$ which dominates the $\mathcal{O}(q)$ time for selecting a random set of $q\leqslant p$ predictor variables. It also dominates the  time for splitting nodes after model selection, which is at most $\mathcal{O}(n)$. The number of nodes in a tree is at most $n^K$. Finally, the procedure is repeated $M$ times to build the forest, which in total gives $\mathcal{O}(n p\log(n) + M2^Knq)$. 

The space complexity follows directly, and equals $\mathcal{O}(np + M2^Kp)$. The first term is the size of the dataset, and the second is for storing the fitted parameters of the trees, which includes split points and linear coefficients. 

\section{Empirical study}\label{pilot_sec:results}
\subsection{Datasets and methods}
Now that the theoretical guarantees on RaFFLE are established, we investigate its properties empirically. We compare RaFFLE with both classical and state-of-the-art regression methods, on datasets of different types.

We benchmark RaFFLE on a total of 136 regression datasets from 2 sources: the UCI Machine Learning Repository (UCI) \cite{uciml} and the Penn Machine Learning Benchmark (PMLB) \cite{Olson2017PMLB, romano2021pmlb}. From both sources we downloaded all datasets that are available through their respective Python APIs (\url{https://github.com/uci-ml-repo/ucimlrepo} and \url{https://github.com/EpistasisLab/pmlb}) and are labeled as \textit{Regression} tasks. We found that some of the datasets labeled as \textit{Regression} actually had a non-numeric target variable, so we removed them from the selection. We also removed the three largest regression datasets from PMLB (1191, 1195 and 1595), each with 1 million observations, as running the grid search on them was too computationally expensive. We also removed the Forest Fire dataset \cite{forest_fires_162} (UCI 162) because all methods in our study had a negative $R^2$ on this dataset, due to substantial outliers \cite{Cortez2007ADM}. 

We did very little preprocessing on the datasets: (1) features consisting of dates were dropped, (2) columns with more than 50\% of missing values were dropped, 
and (3) in the Online News Popularity dataset \cite{online_news_popularity_332} we log-transformed the skewed target variable.\\

We compare RaFFLE with the following competitors:
\begin{itemize}
\itemsep -1pt
    \item \textbf{CART}: a single CART decision tree \cite{cart};
    \item \textbf{PILOT}: a single PILOT decision tree \cite{raymaekers2024pilot};
    \item \textbf{RF}:  a classical random forest of CART decision trees \cite{breiman2001random};
    \item \textbf{XGB}: the popular XGBoost method \cite{xgboost}; 
    \item \textbf{Lasso}: $\ell_1$-regularized linear regression;
    \item \textbf{Ridge}: $\ell_2$-regularized linear regression.
\end{itemize}
For CART, RF, Lasso and Ridge we use the scikit-learn \cite{scikit-learn} implementations, and for XGB we use the official python API \url{https://xgboost.readthedocs.io/en/stable/python/index.html}\,.
For CART and PILOT we only consider the default parameters. For Lasso and Ridge the parameter controlling the level of regularization is tuned by searching over a grid of 100 values. Finally, for RF, RaFFLE 
and XGB we perform a grid search on the hyperparameters listed in Table \ref{tab:grid_search_parameters}, where \textsc{n\_features\_node} is the fraction of the total number of features $p$ that is drawn at each level of each tree. For \textsc{n\_features\_node} $=0.7$ this is 70\%, and when it is set to 1 all features are used each time, as in the default random forest for regression in scikit-learn.  

As both RF and XGBoost are often used out-of-the-box without much tuning of hyperparameters, we include a default version of RaFFLE alongside the tuned RaFFLE. In this way we can investigate the sensitivity of RaFFLE to the hyperparameters. Moreover, it allows for a fair comparison between RaFFLE and RF, given that the cross-validated RaFFLE has an extra tuned parameter $\alpha$. The default RaFFLE, which we will denote by \textbf{dRaFFLE}, has parameters \textsc{alpha} = 0.5, \textsc{n\_features\_node} = 1.0 and \textsc{max\_depth} = 20. All other RaFFLE parameters (for both the tuned and the default version, see Algorithm \ref{algo:RaFFLE} for an overview) are set to the following default values: \textsc{n\_estimators} = 100, \textsc{n\_features\_tree} = 1, \textsc{max\_model\_depth} = 100, \textsc{min\_sample\_fit} = 10, \textsc{min\_sample\_alpha} = 5, and \textsc{min\_sample\_leaf} = 5. 

\begin{table}[!ht]
\centering
\caption{Overview of grid search parameters}
\begin{tabular}{|>{\raggedright\arraybackslash}p{3.8cm}|>{\raggedright\arraybackslash}p{3.8cm}|>{\raggedright\arraybackslash}p{3.8cm}|}
\hline
\textbf{Parameter name} & \textbf{Parameter values} & \textbf{Used by} \\
\hline
\textsc{alpha} & \{0.01, 0.5, 1\} & RaFFLE \\
\hline
\textsc{n\_features\_node}$^1$  & \{0.7, 1\} & RF, RaFFLE, XGB \\
\hline
\textsc{max\_depth} & \{6, 20, None$^2$\} & RF, RaFFLE, XGB \\
\hline
\end{tabular}
\label{tab:grid_search_parameters}  

\vspace{1mm}
\footnotesize{1. for RF the parameter is called \textsc{max\_features}, for XGB \textsc{colsample\_bynode}}.\\
\footnotesize{2. \textsc{max\_depth} = None is only used in RF as it is the scikit-learn default.}
\end{table}

\subsection{Results}

In order to compare the methods, we compute the average five-fold cross-validation $R^2$ for each of the methods. In other words, for each fold we train on 80\% of the data, and then compute the $R^2$ on the holdout part containing the remaining 20\% of the data. Then we compute the average of these $R^2$ values over the five folds. This yields a performance measure for all the different methods. 

While the resulting performance measures are comparable for a given dataset, we need to adjust the performance measure for the different degrees of difficulty across datasets. Ranks are often used for this, but they do not capture the amount by which the performance differs between methods. Instead, we carry out the following normalization. For each dataset, we divide the average $R^2$ values by the highest of these values attained by the various methods on the same dataset. The resulting performance measure lies between 0 and 1 (we clip negative $R^2$ values to 0), where 1 indicates it is the best performer, and a value smaller than 1 quantifies the performance of the method relative to the best method on that dataset. The resulting relative $R^2$ values allow comparisons across datasets.

Tables \ref{tab:empirical_results_p1}, \ref{tab:empirical_results_p2} and \ref{tab:empirical_results_p3} in Section \ref{app:fullsimulationresults} of the Supplementary Material show the detailed results of the benchmarks on all datasets. The summary statistics are shown in Table \ref{tab:empirical_results_summary}, from which we can draw several conclusions. RaFFLE with tuned hyperparameters achieves an average relative $R^2$-score of 0.99 with a standard deviation of 0.02, and substantially outperforms all other methods. Interestingly, dRaFFLE performed second best at 0.96, in spite of its fixed hyperparameters. This indicates that the default parameters in dRaFFLE already give a reasonably good performance, making it suitable as an out-of-the-box method that is faster than the full RaFFLE. Next in line are PILOT, RF and XGB with performances around 0.90\,. Finally, we have CART, Lasso and Ridge with average performance about 40\% below the best method per dataset, and whose performance is much more variable.

\begin{table}[ht]
\caption{Summary statistics of the relative $R^2$ values.}
\label{tab:empirical_results_summary}
\centering
\begin{tabularx}{\textwidth}{lrrrrrrrr}
\toprule
    &          CART &         PILOT &            RF &   
    RaFFLE &  dRaFFLE&           XGB &         Ridge &  
    Lasso \\
\midrule
  mean &          0.64 &          0.91 &          0.92 & 
  \textbf{0.99} & 0.96 &           0.90 &         0.59 & 
  0.58 \\
  std &          0.28 &          0.15 &          0.09 & 
  \textbf{0.02} &      0.10 &      0.15 &        0.33 &
  0.31 \\
\bottomrule
\end{tabularx}
\end{table}

Figure \ref{fig:boxplot_overall} shows boxplots of the relative $R^2$ values over all datasets, which confirm the initial conclusions. RaFFLE stands out with the highest relative $R^2$ values, which were always above 80\% and very often above 95\%. dRaFFLE is the second best performer, followed by PILOT, RF and XGB. Note that this instance of PILOT is the default version without tuning any parameters, whereas they were tuned for RF and XGBoost. Finally, we have CART, Lasso and Ridge that do well on some datasets, but have a median performance of about 70\% of the best method, and a substantial number of datasets where the performance is quite poor relative to the other five methods.

\begin{figure}[!ht]
\centering\includegraphics[width=0.8\textwidth]{boxplots_overall_relative_cr.pdf}
\caption{Boxplots of relative $R^2$-scores by method}
\label{fig:boxplot_overall}
\end{figure}

Another analysis of the empirical results takes the presence of strong linear patterns in the data into account. Sometimes CART performs well, and sometimes the linear methods Lasso and Ridge do well. Given the fundamental difference in the structures that these methods aim to fit, we can expect that their relative performance varies a lot over the different datasets. This is indeed confirmed when plotting the unstandardized $R^2$ values of CART against those of Lasso and Ridge, as shown in Figure \ref{fig:pairplot_zoom}. We see that the performances of Lasso and Ridge are relatively similar, and both differ a lot from the performance of CART.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.7\textwidth]{pairplot_zoom.pdf}
\caption{Pairs plot of raw $R^2$ values of CART and the linear methods Lasso and Ridge}
\label{fig:pairplot_zoom}
\end{figure}

We can thus try to categorize the datasets as `linear' and `nonlinear'. The so-called `linear datasets' are those where Lasso or Ridge outperform CART, whereas the others are considered `nonlinear datasets' on which CART outperforms. We can then compare the performance of the methods on both types separately. Figure \ref{fig:boxplots_lin_vs_nonlin} shows the same relative $R^2$ values as Figure \ref{fig:boxplot_overall}, but now split by dataset type. RaFFLE outperforms on both data types, followed by dRaFFLE. We also see that the linear methods typically performed poorly on the nonlinear datasets, whereas CART often still performed well on linear datasets. The relative performances of the remaining nonlinear methods PILOT, RF, and XGB were fairly similar to each other.

\begin{figure}[!ht]
\centering
\includegraphics[width=1.0\textwidth]{boxplots_lin_vs_nonlin_relative_all_cr.pdf}
\caption{Boxplots of relative $R^2$-scores by method and `type' of dataset. The so-called `linear datasets' (blue boxplots) are those where Lasso or Ridge performed better than CART. On the remaining `nonlinear datasets' (orange boxplots) CART performed better than Lasso and Ridge.}
\label{fig:boxplots_lin_vs_nonlin}
\end{figure}

\vspace{-2mm}
\section{Conclusions}\label{pilot_sec:conclusion}

In this paper we introduced RaFFLE, a random forest of linear model trees trained by PILOT. By integrating this  base learner, RaFFLE combines the versatility of random forests with the expressiveness of linear model trees. This hybrid approach enables RaFFLE to handle a wide range of regression problems, from datasets with complex nonlinear relationships to those with strong linear or piecewise linear structures.

To adapt PILOT for use in random forests, we made targeted modifications to increase the variability between the trees by reducing the regularization in the standalone PILOT's design. To this end we introduced the parameter $\alpha$ to tune the degrees of freedom assigned to different node types, added node-level feature sampling, and disabled the \textsc{blin} node type to speed up the computation. These modifications allow individual trees in the ensemble to overfit slightly in order to increase diversity within the forest, a critical factor in improving overall predictive performance. The resulting RaFFLE algorithm strikes a balance between accuracy and computational efficiency.

Our theoretical analysis shows that RaFFLE is consistent at general additive data. We also show that it attains a better convergence rate on data generated by a linear model. Empirical evaluation on 136 regression datasets from the UCI and PMLB repositories demonstrates RaFFLE’s superior performance compared to CART, the traditional random forest, XGBoost, Lasso and Ridge. RaFFLE not only achieves high accuracy across a broad spectrum of datasets, but also consistently outperforms other methods on both linear and nonlinear tasks. Even a non-tuned default version of RaFFLE performs competitively, underscoring the stability of the approach.

Despite its strengths, RaFFLE has some limitations that merit further investigation. The computational complexity of PILOT nodes, while mitigated by our modifications, can still pose challenges for extremely large datasets. Future work could explore more efficient tree-building algorithms or parallelized implementations to improve scalability. Examining its performance in high-dimensional settings with noisy or sparse features, as well as in domains with a large percentage of missing data, could uncover new opportunities for improvement. Moreover, while RaFFLE performs well in regression tasks, extending the framework to handle classification problems would broaden its applicability.

Another promising avenue is the integration of advanced optimization techniques, such as gradient boosting or adaptive sampling, which could further enhance RaFFLE’s accuracy and speed. Lastly, refining interpretability tools, such as feature importance measures or visualizations tailored to PILOT’s piecewise linear structure, could improve RaFFLE's appeal in domains where transparency is critical.

RaFFLE represents a step forward in the evolution of tree-based ensembles, bridging the gap between traditional decision trees and linear model trees. By leveraging PILOT’s speed and regularization within a random forest framework, RaFFLE offers a unique blend of flexibility and accuracy. This hybrid approach enables it to address a wide range of regression challenges.

%\clearpage
%\begin{appendices}
\vspace{5mm}
\noindent {\Large \bf Appendix} 

\section*{A. Proofs of the theoretical results in Section~\ref{subsec:consistency}}
\label{app:consistency}

\noindent {\bf Proof of Lemma~\ref{lem:pcon_step_est}}
%\begin{proof}
Let $\widehat\Delta^{k}_{\mbox{\tiny PCON}}(\hat s_p, \hat j_p, T)$ be the largest impurity gain induced by the optimal predictor $\hat j_p\in\{1,\dots,p\}$ and split point $\hat s_p$. Following the reasoning for equation (E.2) in \cite{klusowski2024large}, we know that $\maE_{\Xi_k|\Xi_{k-1}}\left[\max_{j\in\maS_T}\widehat\Delta^{k-1}_{\mbox{\tiny PCON}}(\hat s_j,j,T)\right]\geqslant q\widehat\Delta^{k-1}_{\mbox{\tiny PCON}}(\hat s_p, \hat j_p, T)/p$. The intuition is that the expected gain should be larger than the optimal gain times the probability that the optimal predictor is included in the set of predictors selected by $\Xi_k$, which equals $q/p$. Combining this with the lower bound on $\widehat\Delta_{\mbox{\tiny PCON}}$ in Lemma 3 of \citep{raymaekers2024pilot} proves the result. Note that the $2B$ in the denominator comes from the fact that a random PILOT tree truncates at $[-B,B]$, and therefore $\sum_{j=1}^{p_n}||\hat f(\maT_k)_j||_{TV}\,=\max \hat f(\maT_k)-\min\hat f(\maT_k)\leqslant 2B$, from which we obtain $\left(\frac{1}{2}||f-\hat f(\maT_k)||_{TV(T)}\right)^2 \leqslant \frac{1}{4}(||f||_{TV}+2B)^2$\,.
%\end{proof}

\newpage
\noindent {\bf Proof of Lemma~\ref{lem:bic}}

\begin{proof}
The first result follows directly from taking the conditional expectation. For the second result, we note that for each realization of $\Xi_{k}|\Xi_{k-1}$, $\widehat\Delta$ and the selected model are deterministic. Therefore, by Proposition 4 of \citep{raymaekers2024pilot}, it holds that for $\alpha > 0$ we have $\widehat\Delta\geqslant\frac{(\nu_{\lin}-1)}{(\nu_{\plin}-1)}\widehat\Delta_{\mbox{\tiny PCON}} = \frac{1}{4}\widehat\Delta_{\mbox{\tiny PCON}}$. When $\alpha = 0$ the BIC is equivalent to the MSE, so we have $\widehat\Delta\geqslant\widehat\Delta_{\mbox{\tiny PCON}}$. Therefore, by taking the conditional expectation we prove the result on $E_2$. 
    
\end{proof}

\noindent {\bf Proof of Theorem~\ref{theorem:errorRandomPILOT}}
\begin{proof}
As we saw in Remark \ref{rem:connode}, selecting $\con$ in a node does not necessarily mean that $\con$ will be selected in subsequent steps. But there may be nodes where $\con$ is the overall the best, that is, no other model on any of the predictors achieves a lower BIC. 
We denote these \textit{final} $\con$ nodes by $\con^*$ and separately control the error on them. 
We follow the notations in the proof of Theorem 3 of \citep{raymaekers2024pilot}, but we define the set $C^+_k$ only for those final $\con^*$ nodes. 
In particular, for each realization of $\Xi_K$, $1\leqslant k\leqslant K$, we let $C_k^+:=\{T|T=\textsc{con}^*, T\in \maT_{k-1}, R_{k-1}(T)>0\}$ be the set of nodes on which $\textsc{con}^*$ is fitted before the \mbox{$k$-th}\linebreak step, $R_{C_k^+}:=\sum_{T\in C^+_k}w(T)R(T)$, and $\widetilde{R}_k:=R_k-R_{C_k^+}$. We want to first evaluate the expected value of $\widetilde R_k - \widetilde R_{k-1}$ which is the impurity gain on the non-$\con^*$ nodes at step $k$. The errors $\maE_{\Xi_k}(R_{C_K^+})$ in the $\con^*$ nodes will be added later. Here we consider two specific situations for each $T$: 

\noindent 1. The optimal predictor $\hat j_p$ among all predictors is included in $\maS_T$. The probability is $q/p$ in each node, as in the proof of Lemma \ref{lem:pcon_step_est}.

\noindent 2. $\hat j_p$ is not included in $\maS_T$ (this probability is $1-q/p$ in each node). In this case, either other suboptimal non-$\con$ models are selected or a $\con$ model is selected which might not necessarily be $\con^*$. 

In the first situation, we have by the proof in \citep{raymaekers2024pilot} that
\begin{equation*}
        \widetilde R_k\leqslant \widetilde R_{k-1} - \frac{1}{F}\widetilde R^2_{k-1}
\end{equation*}
where the constant $F = 4(||f||_{TV}+2B)^2$ comes from the second result in Lemma \ref{lem:bic} which controls the lower bound of the ratio between the gain of $\pcon$ (on all predictors) and the selected model (on $\hat j_p$). In the second situation, the impurity gain is trivially bounded by zero where the worst case corresponds to a $\con$ fit. We now take the expectation over $\Xi_{k}|\Xi_{k-1}$ to get 
\begin{equation*}
    \widetilde R_{k-1} - \maE_{\Xi_{k}|\Xi_{k-1}}[\widetilde R_k]\geqslant \maE_{\Xi_{k}|\Xi_{k-1}} \! \left[ \sum_{T\in \maT_{k-1}\backslash C^+_{k-1},  \tilde R_{k-1}(T)\geqslant0}\mathbbm 1_{\{\hat j_p\in \maS_T\}}\frac{w(T)}{F}\widetilde R^2_{k-1}(T)\right] \geqslant \frac{q}{pF}\widetilde R^2_{k-1}.
\end{equation*}
By taking the expectation with respect to $\Xi_{k-1}$ and applying Jensen's inequality to\linebreak $\maE_{\Xi_{k-1}}[\widetilde R^2_{k-1}]$ we get
 \begin{equation*}
   \maE_{\Xi_k}[\widetilde R_k]\leqslant\maE_{\Xi_k}[\widetilde R_{k-1}] - \frac{q}{pF}(\maE_{\Xi_k}[\widetilde R_{k-1}])^2\,.
\end{equation*}
By an induction argument similar to Lemma 4.1 of \cite{klusowski2024large} we obtain 
\begin{equation*}
    \maE_{\Xi_k}[||Y-\hat f(\mathcal{T}_K)||^2_n]\, \leqslant ||Y-f||^2_n \, + \frac{4p(||f||_{TV}+2B)^2}{q(K+3)} + \maE_{\Xi_k}[R_{C_K^+}]\;.
\end{equation*}
Next we deal with the $\con^*$ nodes. For $\maE_{\Xi_k}(R_{C_K^+})$, we note that for $\forall T\in C_K^+$, we have
    \begin{equation*} \label{eq:conbound}
    \frac{(\maE_{\Xi_k} [R(T)])^2}{FR_0}\leqslant\frac{\maE_{\Xi_k} [R(T)^2]}{FR_0}\leqslant \frac{p\maE_{\Xi_{k}}[\maE_{\Xi_{k+1}|\Xi_k}[\Delta_{\mbox{\tiny PCON}}]]}{qR_0}\leqslant\frac{p}{qt}\Big(1-\exp\Big((\nu_{\mbox{\tiny CON}}-\nu_{\mbox{\tiny PCON}})\frac{\log t}{t}\Big)\Big)
\end{equation*}
where $R_0$ is the initial error before training, and $R(T)$ is the remaining error in $T$. The first inequality follows from Jensen's inequality, the second inequality follows from Lemma \ref{lem:pcon_step_est} which provides an upper bound of the error $R(T)$ using the expected gain of a potential $\pcon$ fit, and the last inequality follows from the first result in Lemma \ref{lem:bic}. Note that by our definition, $\con^*$ is the best model considering all predictors, therefore the conditional expectation in that first result becomes an unconditional expectation, which justifies the last inequality. Note that we have controlled the expected error in a $\con^*$ node by its number of cases $t$. We then apply the Cauchy-Schwarz inequality to get a bound only regarding $n$,
\begin{equation*}
    \maE_{\Xi_k}(R_{C_K^+})\le\sqrt{2^Kn\log n}\precsim \mathcal{O}\left(\sqrt{\log (n)/n^{(r-1)/r}}\,\right)
\end{equation*}
    by choosing $K=\log_2(n)/r$ with $r>1$.
\end{proof}

\noindent {\bf Proof of Theorem~\ref{theorem:consistency}}

\begin{proof}
As in the proof of Theorem 4.3 in  \cite{klusowski2024large}, we will leverage Theorem 11.4, Theorem 9.4, and Lemma 13.1 from \cite{distribution} to show this. For each $m$ we have
\begin{align*}
    P\Big(&\maE_{\Xi_{K_n}}||f-\hat f(\maT_{K_n}^{(m)})||^2\geqslant2(\maE_{\Xi_{K_n}}||Y-\hat f(\maT_{K_n}^{(m)})||^2_n-||Y-f||^2_n)+a+b \Big) \leqslant \\
    &P\Big(\exists f\in\mathcal F,\text{ } ||f-\hat f(\maT_{K_n}^{(m)})||^2\geqslant2(||Y-\hat f(\maT_{K_n}^{(m)})||^2_n-||Y-f||^2_n)+a+b\Big).
\end{align*}
Here $a,b$ are positive numbers that tend to zero as $n\rightarrow\infty$. The above holds because the inequality for the expectation implies the existence of a piecewise linear function and a realization of $\Xi_{K_n}$ for which the inequality holds (i.e., without expectations). Following \cite{klusowski2024large}, we assume that the procedure samples a subset 
$\maD_{\maI_m}\subset \maD_n =\{X_1,\dots,X_n\}$ 
with indices $\maI_m\subset \{1,\dots,n\}$
such that $\#\maI_m = n_m$ and 
$\maE_{\maI_m}(1/n_m)\asymp 1/n$ (e.g. with fixed 
size $n/2$). 
Therefore, we can follow the proof of Theorem 1 of \citep{raymaekers2024pilot} to deduce that on $\maD_{\maI_m}$ we have
\begin{equation} \label{oracle}
\begin{split}
\maE_{\Xi_{K_n}}[\,||f-\hat f(\maT_{K_n}^{(m)})||^2]\leqslant &\,\frac{2p_nF}{q_n({K_n}+3)}+\frac{C_3\sqrt{2^{K_n}n_m\log n_m}}{n_m}\\&\,+\frac{C_4\log(n_m p_n\log n_m)2^{K_n+\log(p_n+1)}}{n_m}\,.
\end{split}
\end{equation}
Using our assumption that $\maE_{\maI_m}(1/n_m)\asymp 1/n$ we have 
\begin{equation*}
    \maE_{\Xi,\maD_n}[||f-\hat f(\mathcal {T}^{(m)}_{K_n})||^2]\leqslant \frac{2p_nF}{q_n(K_n+3)}+\frac{C_3'\sqrt{2^{K_n}n\log n}}{n}+\frac{C_4'\log(n p_n\log n)2^{K_n+\log(p_n+1)}}{n}
\end{equation*}
by taking the expectations over all possible $\maD_{\maI_m}$ and $\Xi_K$. Finally, by Jensen's inequality, we can upper bound the error of the forest by the average error of the random PILOT trees to get
\begin{equation*}
    \maE_{\Xi,\maD_n}[||f-\hat f(\mathcal {R}_M)||^2]\leqslant \frac{2p_nF}{q_n(K_n+3)}+\frac{C_3'\sqrt{2^{K_n}n\log n}}{n}+\frac{C_4'\log(n p_n\log n)2^{K_n+\log(p_n+1)}}{n}
\end{equation*}
which tends to zero if $K_n, p_n$, and $q_n$ satisfy the conditions.
\end{proof}


\section*{B. Proofs of the theoretical results in Section~\ref{subsec:convergencelinear}}
\label{app:linear}

\noindent {\bf Proof of Lemma~\ref{lem:lin_step_est}}

\begin{proof}
We re-parameterize $L_n^k(T)=||Y_T-\boldsymbol{\tilde X}_T\tilde\beta_T||^2_t$ and write its gradient as $\nabla L_n^k(T)|_{\tilde\beta = \tilde\beta_T}\,$. By the definition of the gradient, we have for fixed $\tilde{\boldsymbol{X}}_T$ and noise that
\begin{align}\label{eq:lem1:lin_est}
\begin{split}
  \maE_{\Xi_k|\Xi_{k-1}}\left[\frac{nq||\nabla L_n^{k-1}(T)|_{\tilde\beta = \tilde\beta_T}||_{\infty}}{2p}\right]&=\maE_{\Xi_k|\Xi_{k-1}}\left[q\left|\left|r^\top \tilde{\boldsymbol X}_T\right|\right|_\infty/p\right]\\
  &=\maE_{\Xi_k|\Xi_{k-1}}\left[q\max_{j\in\{1\dots p\}}\{|r^\top \tilde X^{(j)}_T|\}/p\right]\\
  &\leqslant\maE_{\Xi_k|\Xi_{k-1}}\left[\max_{j\in\maS_T}\frac{|r^\top X^{(j)}_T|}{\sqrt n\hat\sigma_j^u}\right]\\
  &\leqslant \maE_{\Xi_k|\Xi_{k-1}}\left[\max_{j\in\maS_T}\frac{|r^\top (X^{(j)}_T-\overline X^{(j)}_T)|}{\sqrt n\hat\sigma_j}\right]\\
  &=\maE_{\Xi_k|\Xi_{k-1}}\left[\sqrt{n\Delta_{\mbox{\tiny LIN}}}\right]
\end{split}
\end{align}
where $r$ denotes the residuals $Y_T-\tilde \bX_T\tilde\beta_T$. Note that $\nabla L_n^{k-1}$ is deterministic given $\tilde{\boldsymbol{X}}_T$, $\Xi_{k-1}$ and the noise. The first inequality follows from the fact that the optimal predictor is included in the sampled predictors with probability $q/p$ (see also Lemma \ref{lem:pcon_step_est}), the second inequality follows from the fact that the residuals of the response on $T$ have zero mean, and the last equality follows from Lemma 1 of \citep{raymaekers2024pilot} which gives an explicit expression of $\Delta_{\lin}$. Therefore, we have that
\begin{align*}
 &P_{\boldsymbol X_T}\Bigg[\maE_{\Xi_k|\Xi_{k-1}}\left(\Delta_{\mbox{\tiny LIN}}\right)\geqslant 
 \frac{q^2\lambda_0(L_n^{k-1}(T)-L^*_n(T))}{4p^3}\Bigg]\\\geqslant &P_{\boldsymbol X_T}\left[\frac{n||\nabla L_n^{k-1}|_{\tilde\beta = \tilde\beta_T}||^2_{\infty}}{4}\geqslant \frac{\lambda_0(L_n^{k-1}(T)-L^*_n(T))}{4p}\right]\\
  \geqslant &P_{\boldsymbol X_T}\left[\lambda_{min}(\tilde {\boldsymbol X}_T^\top\tilde {\boldsymbol X}_T)>\lambda_0\right]
\end{align*}
where the first inequality follows from equations in (\ref{eq:lem1:lin_est}). The second inequality follows from the fact that $L_n^*(T)$ and $ L_n^{k-1}(T)$ are deterministic given $\Xi_{k-1}$, $\tilde{\boldsymbol{X}}_T$ and the noise. Since $\Xi_{k-1}$ only affects the node (hyperrectangle) $T$ in which $\tilde{\boldsymbol{X}}_T$ lies, and Conditions 1 and 2 hold uniformly for any $T$, we can apply the reasoning in the proof of Lemma 4 in \citep{raymaekers2024pilot}.
\end{proof}


\noindent {\bf Proof of Theorem~\ref{theorem:recursion2}}

\begin{proof}
Following the proof of Theorem 4 in \citep{raymaekers2024pilot} and taking the expectation 
with respect to $\Xi_k$ we have with probability at least $\sum_{T\in \maT_k}\exp(-C_{\lambda_0,\sigma_0,p}t)$ under $\boldsymbol{X}, \varepsilon$ that
\begin{align*}
    \maE_{\Xi_{k-1}}[ L_n^{k-1}-L^*_n] - &\maE_{\Xi_k}[(L^{k}_n - L^*_n)]
    \geqslant\maE_{\Xi_{k}}\left[\mathbbm 1_{\{\hat j_p\in\maS_T\}}\sum_{T\in \maT_{k-1}}w(T)\Delta_{\mbox{\tiny LIN}}(T)\right]\\
    &\geqslant \frac{q}{p}\maE_{\Xi_{k-1}}\left[\frac{\lambda_0}{4p}\maE_{\Xi_k|\Xi_{k-1}}\left(\sum_{T\in\maT_{k-1}}w(T)(L^{k-1}_n(T) - L^*_n(T))\right)\right]\\
    & \geqslant  \frac{q\lambda_0}{4p^2}\maE_{\Xi_{k-1}}[L^{k-1}_n-L^*_n].
\end{align*}
Here the first inequality follows from the proof of Theorem \ref{theorem:errorRandomPILOT} where we discuss whether the optimal predictor $\hat j_p$ is included. The second inequality follows from our Lemma~\ref{lem:lin_step_est:cond} and (i)-(iv) of the  proof of Theorem 4 in \citep{raymaekers2024pilot}. Note that $\hat j_p$ need not be the optimal predictor of $\lin$, and the optimal model must have a larger impurity gain than the optimal gain of the $\lin$ model by the BIC criterion. The final inequality follows from (v) of that proof. Therefore,
\begin{equation*}
    \maE_{\Xi_{k}}[L^{k}_n-L^*_n]\leqslant\maE_{\Xi_{k-1}}[L^{k-1}_n-L^*_n]\left(1-\frac{q\lambda_0}{4p^2}\right).
\end{equation*}
Note that here $1-\frac{q\lambda_0}{4p^2}\geqslant1-\frac{\lambda_0}{4p}>0$.

Let $G:=\{(\boldsymbol{X},\varepsilon)|\forall \Xi_K, T\in \maT_k, 1\leqslant k\leqslant K, \lambda_
{min}(\tilde{\boldsymbol X}_T^\top\tilde{\boldsymbol X}_T)\geqslant\lambda_0\}$. By Lemma~\ref{lem:lin_step_est:cond} and a union bound on $K$ (e.g.\ choosing $K\asymp\log n$), we can derive as in \citep{raymaekers2024pilot} that\linebreak $P(G^c)\precsim \exp(-C'_{\lambda_0,\sigma_0,p}n^{\delta})$. We then have for sufficiently large $n$ that
\begin{align*}
\maE_{\boldsymbol X,\varepsilon,\Xi_K}&[L^K_n-L^*_n]=\maE_{\boldsymbol X,\varepsilon}[\mathbbm 1_{G}\maE_{\Xi_{K}}[L^K_n-L^*_n]+\mathbbm 1_{G^c}\maE_{\Xi_{K}}[L^K_n-L^*_n]\\
&\leqslant\sqrt{P(G)}\sqrt{\maE_{\boldsymbol X,\varepsilon}[(\maE_{\Xi_{k}}[L^K_n-L^*_n])^2]} +\sqrt{P(G^c)}\sqrt{\maE_{\boldsymbol X,\varepsilon}[(\maE_{\Xi_{K}}[L^0_n-L^*_n])^2]}\\
&\leqslant \Big( 1-\exp(-C'_{\lambda_0,\sigma_0,p}n^{\delta}) \Big)^{1/2}\left(1-\frac{q\lambda_0}{4p^2}\right)^{K}\sqrt{\maE_{\boldsymbol X,\varepsilon,\Xi_K}[(L^0_n-L^*_n)^2]}\\
&\quad + \exp(-C''_{\lambda_0,\sigma_0,p}n^{\delta})\sqrt{\maE_{\boldsymbol X,\varepsilon,\Xi_K}[(L^0_n-L^*_n)^2]}\\
&\leqslant\left(1-\frac{q\lambda_0}{4p^2}\right)^{K}\sqrt {\maE_{\boldsymbol X,\varepsilon,\Xi_K}[(L^0_n-L^*_n)^2]}+\mathcal{O}(1/n)
\end{align*}
where the second inequality follows from Jensen's inequality.

It remains to control the error in the $\con^*$ nodes. For given $\boldsymbol X,\varepsilon$ we have that if an extra $\lin$ is fitted to the actual $\con$ node, then
\begin{equation*} 
    \frac{\maE_{\Xi_{K+1}|\Xi_{K}}[\Delta_{\mbox{\tiny lin}}]}{R_0}\leqslant\frac{1}{t}\Big(1-\exp\Big((\nu_{\mbox{\tiny CON}}-\nu_{\mbox{\tiny LIN}})\frac{\log t}{t}\Big)\Big)
\end{equation*}
using the first result of Lemma~\ref{lem:bic}. Next, we use Lemma~\ref{lem:lin_step_est} and a similar union bound as we did for $G$ to derive that with probability at least $1-\exp(C_{\lambda_0,\sigma_0,p}t)$ it holds that
\begin{equation*}
   \frac{q^2\lambda_0(L_n^{K}(T)|_{\Xi_{K}}-L^*_n(T))}{4p^3}\leqslant\frac{R_0}{t}\Big(1-\exp\Big((\nu_{\mbox{\tiny CON}}-\nu_{\mbox{\tiny LIN}})\frac{\log t}{t}\Big)\Big),
   \;\;\; \forall \Xi_{K}.
    \end{equation*}
We denote such an event by $H$. Moreover, noting that $\nu_\con\leqslant\nu_\lin$ we have by the integrability of $\varepsilon$ that
\begin{align*}
    \maE_{\boldsymbol X, \varepsilon,\Xi_K}[\mathbbm 1_{H} (L_n^K(T)-L^*_n(T))]&\precsim \maE_{\boldsymbol X,\varepsilon}[Y^2]\maE_{\boldsymbol X, \varepsilon,\Xi_K}{\Big(1-\exp\Big((\nu_{\mbox{\tiny CON}}-\nu_{\mbox{\tiny LIN}})\frac{\log t}{t}\Big)\Big)}\\
    &\precsim\maE_{\boldsymbol X, \varepsilon,\Xi_K}\left({\frac{(\nu_{\mbox{\tiny LIN}}-\nu_{\mbox{\tiny CON}})\log t}{t}}\right).
\end{align*}
Let us denote by $\maT_K^{\textsc{con}^*}$ all the $\textsc{con}^*$ nodes in the tree up to depth $K$. Then, by an argument similar to the case without the $\con^*$ nodes, we control the expectation of the weighted sum of the errors on $\con^*$ nodes by
\begin{align*}
\maE_{\boldsymbol X,\varepsilon,\Xi_K}\Big[\sum_{T\in \maT_K^{\textsc{con}^*}}w(T)&(L_n^K(T)-L^*_n(T))\Big]\\ 
&\leqslant
\maE_{\boldsymbol X,\varepsilon}\Big[\mathbbm 1_{H^c}\sum_{T\in \maT_K^{\textsc{con}^*}}\maE_{\Xi_K}[w(T)(L_n^K(T)-L^*_n(T))]\Big]\\
&\;\;\;\;+\maE_{\boldsymbol X,\varepsilon}\Big[\mathbbm 1_{H}\sum_{T\in \maT_K^{\textsc{con}^*}}\maE_{\Xi_K}[w(T)(L_n^K(T)-L^*_n(T))]\Big]\\
&\leqslant \mathcal{O}\Big(\frac{1}{n}\Big) +\maE_{\boldsymbol X,\varepsilon,\Xi_K}\Big[\mathbbm 1_{H}\sum_{T\in \maT_K^{\textsc{con}^*}}\frac{(\nu_{\mbox{\tiny LIN}}-\nu_{\mbox{\tiny CON}})\log(t)}{n}\Big]\\
&\leqslant \mathcal{O}\Big(\frac{1}{n}\Big) + \mathcal{O}\left(\frac{N_{leaves}\log(n)}{n}\right)\\
&\leqslant \mathcal{O}\Big(\frac{1}{n}\Big) + \mathcal{O}\left(\frac{\log(n)}{n^\delta}\right),
\end{align*}
where the last inequality follows from $N_{leaves}\leqslant n^{1-\delta}$ by Condition 1.

We then combine the two estimations using the same reasoning as in the proof of Theorem 4 in \citep{raymaekers2024pilot}, which concludes the proof.
\end{proof}

\noindent {\bf Proof of Theorem~\ref{theorem:rate}}

\begin{proof}
Using an argument similar to the proof of Theorem 5 in \citep{raymaekers2024pilot} and equation (\ref{oracle}), we first have for any $m$ on $\maD_{\maI_m}$ that
\begin{equation*}
        \maE_{\maD_{I_m},\varepsilon,\Xi_{K_n}}[||\hat f(\mathcal {T}_{K_n}^{(m)}) - \boldsymbol X\beta||^2] \leqslant \mathcal{O}\left(\maE_{I_{m}}\left(\frac{\log(n_m)}{n_m^{\delta}}\right)\right).
\end{equation*}
Since $0 < \delta < 1$ we can apply Jensen's inequality to the concave function $x^\delta$ which yields $\maE_{\maI_m}(1/n_m^\delta)\leqslant(\maE_{\maI_m}(1/n_m))^\delta\precsim1/n^\delta$, and therefore 
\begin{equation*}
        \maE_{\maD_{I_m},\varepsilon,\Xi_{K_n}}[||\hat f(\mathcal {T}_{K_n}^{(m)}) - \boldsymbol X\beta||^2] \leqslant \mathcal O\left(\frac{\log(n)}{n^\delta}\right).
\end{equation*}
Taking the expectation on $\maD_{\maI_m}$ and using Jensen's inequality as in the proof of Theorem \ref{theorem:consistency}  makes the convergence rate hold for the RaFFLE prediction under $\maD_n$.
\end{proof}

%\newpage
\section*{C. Full simulation results per dataset}\label{app:fullsimulationresults}

\renewcommand{\arraystretch}{0.8}
\begin{table}[ht]
\caption{Average $R^2$ score divided by highest average $R^2$ score per dataset (I/III)}
\label{tab:empirical_results_p1}
\centering
\footnotesize
\begin{tabularx}{\textwidth}{llrrrrrrrr}
\toprule
source &  id &          CART &         PILOT &            RF &            RaFFLE &  \makecell{RaFFLE \\Default} &           XGB &         Ridge &         Lasso \\
\midrule
  PMLB & 192 &          0.46 &          0.87 &          0.91 &  \textbf{1.0} &  \textbf{1.0} &          0.64 &          0.89 &          0.89 \\
  PMLB & 195 &          0.87 &          0.91 & \textbf{0.98} &  \textbf{1.0} & \textbf{0.99} & \textbf{0.97} &          0.89 &          0.89 \\
  PMLB & 197 & \textbf{0.98} & \textbf{0.99} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &          0.73 &           0.5 \\
  PMLB & 201 & \textbf{0.98} &          0.89 &  \textbf{1.0} &          0.93 &          0.92 &  \textbf{1.0} &          0.47 &          0.47 \\
  PMLB & 207 &          0.86 &          0.91 & \textbf{0.98} &  \textbf{1.0} & \textbf{0.98} & \textbf{0.97} &          0.89 &          0.89 \\
  PMLB & 210 &          0.71 &          0.83 &          0.92 & \textbf{0.98} & \textbf{0.98} &           0.9 & \textbf{0.99} &  \textbf{1.0} \\
  PMLB & 215 &          0.95 &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &          0.74 &          0.74 \\
  PMLB & 218 &           0.5 &          0.89 & \textbf{0.99} &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.96} &          0.06 &           0.0 \\
  PMLB & 225 &          0.51 & \textbf{0.98} & \textbf{0.98} &  \textbf{1.0} &  \textbf{1.0} &          0.93 &          0.54 &          0.54 \\
  PMLB & 227 & \textbf{0.97} & \textbf{0.99} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &          0.72 &          0.51 \\
  PMLB & 228 &          0.83 &          0.95 & \textbf{0.96} &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.97} &          0.89 &          0.91 \\
  PMLB & 229 &          0.83 &          0.85 &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.98} & \textbf{0.97} &          0.88 &          0.89 \\
  PMLB & 230 &          0.94 &          0.85 &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.97} &  \textbf{1.0} &           0.9 &          0.82 \\
  PMLB & 294 &          0.86 &          0.88 &  \textbf{1.0} & \textbf{0.97} & \textbf{0.96} &  \textbf{1.0} &          0.78 &          0.78 \\
  PMLB & 344 &  \textbf{1.0} & \textbf{0.99} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &          0.81 &          0.81 \\
  PMLB & 485 &          0.44 &          0.76 &  \textbf{1.0} & \textbf{0.99} &          0.88 & \textbf{0.99} &          0.87 &          0.88 \\
  PMLB & 503 &          0.68 & \textbf{0.96} & \textbf{0.98} &  \textbf{1.0} & \textbf{0.99} & \textbf{0.97} & \textbf{0.96} & \textbf{0.96} \\
  PMLB & 505 & \textbf{0.98} & \textbf{0.99} & \textbf{0.99} &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.99} &  \textbf{1.0} &  \textbf{1.0} \\
  PMLB & 519 &           0.9 &  \textbf{1.0} &          0.94 &  \textbf{1.0} & \textbf{0.99} &           0.9 &  \textbf{1.0} &  \textbf{1.0} \\
  PMLB & 522 &           0.0 &          0.39 &          0.84 &  \textbf{1.0} & \textbf{0.98} &          0.81 &          0.32 &          0.33 \\
  PMLB & 523 &  \textbf{1.0} &          0.95 & \textbf{0.99} &          0.95 &          0.95 &  \textbf{1.0} & \textbf{0.98} &  \textbf{1.0} \\
  PMLB & 527 &          0.86 &          0.93 &          0.89 &          0.94 &          0.94 &          0.84 &  \textbf{1.0} &  \textbf{1.0} \\
  PMLB & 529 &          0.66 & \textbf{0.98} &          0.95 & \textbf{0.99} & \textbf{0.99} &          0.92 &  \textbf{1.0} &  \textbf{1.0} \\
  PMLB & 537 &          0.75 &          0.93 & \textbf{0.97} &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.98} &           0.6 &          0.62 \\
  PMLB & 542 &          0.23 &          0.91 &          0.85 &          0.89 &          0.62 &          0.82 &  \textbf{1.0} &          0.94 \\
  PMLB & 547 &          0.35 &          0.77 &  \textbf{1.0} & \textbf{0.99} & \textbf{0.99} & \textbf{0.97} &          0.81 &          0.81 \\
  PMLB & 556 &          0.92 &          0.05 &  \textbf{1.0} & \textbf{0.98} & \textbf{0.96} & \textbf{0.98} &           0.0 &           0.0 \\
  PMLB & 557 &          0.95 &          0.19 &  \textbf{1.0} & \textbf{0.98} & \textbf{0.97} &          0.95 &          0.01 &           0.0 \\
  PMLB & 560 & \textbf{0.97} & \textbf{0.99} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &          0.76 &           0.7 \\
  PMLB & 561 & \textbf{0.96} & \textbf{0.99} & \textbf{0.98} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &          0.91 &          0.85 \\
  PMLB & 562 & \textbf{0.97} & \textbf{0.99} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &          0.72 &          0.51 \\
  PMLB & 564 &          0.88 & \textbf{0.99} & \textbf{0.97} &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.99} &          0.75 &          0.75 \\
  PMLB & 573 & \textbf{0.98} & \textbf{0.99} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &          0.73 &           0.5 \\
  PMLB & 574 &          0.39 &          0.75 &  \textbf{1.0} & \textbf{0.99} & \textbf{0.98} & \textbf{0.99} &           0.0 &          0.01 \\
  PMLB & 579 &          0.53 & \textbf{0.98} &          0.85 &  \textbf{1.0} & \textbf{0.98} &          0.84 &          0.76 &          0.76 \\
  PMLB & 581 &          0.71 &          0.93 &          0.92 &  \textbf{1.0} & \textbf{0.99} &          0.94 &          0.29 &           0.3 \\
  PMLB & 582 &          0.68 & \textbf{0.97} &          0.87 &  \textbf{1.0} & \textbf{0.98} &           0.9 &          0.22 &          0.25 \\
  PMLB & 583 &          0.71 & \textbf{0.98} &          0.91 &  \textbf{1.0} & \textbf{0.99} &          0.94 &          0.26 &           0.3 \\
  PMLB & 584 &          0.63 &          0.94 &          0.88 &  \textbf{1.0} & \textbf{0.99} &          0.91 &          0.24 &          0.26 \\
  PMLB & 586 &           0.8 & \textbf{0.97} &          0.94 &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.96} &          0.28 &          0.28 \\
  PMLB & 588 &          0.76 &          0.95 &          0.92 &  \textbf{1.0} & \textbf{0.99} &          0.95 &          0.23 &          0.28 \\
  PMLB & 589 &           0.8 & \textbf{0.99} &          0.93 &  \textbf{1.0} & \textbf{0.99} &          0.95 &          0.28 &           0.3 \\
  PMLB & 590 &          0.53 & \textbf{0.96} &          0.83 &  \textbf{1.0} & \textbf{0.99} &          0.89 &          0.73 &          0.75 \\
  PMLB & 591 &          0.12 &          0.91 &          0.89 &  \textbf{1.0} & \textbf{0.98} &          0.88 &          0.25 &          0.34 \\
  PMLB & 592 &          0.81 & \textbf{0.98} &          0.93 &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.96} &          0.25 &          0.26 \\
  PMLB & 593 &          0.78 & \textbf{0.99} &          0.93 &  \textbf{1.0} & \textbf{0.99} &          0.95 &          0.29 &           0.3 \\
  PMLB & 594 &           0.0 &          0.94 &          0.87 &  \textbf{1.0} & \textbf{0.99} &          0.75 &           0.0 &           0.0 \\
\bottomrule
\end{tabularx}
\end{table}

\renewcommand{\arraystretch}{0.9}
\begin{table}[ht]
\caption{Average $R^2$ score divided by highest average $R^2$ score per dataset (II/III)}
\label{tab:empirical_results_p2}
\centering
\footnotesize
\begin{tabularx}{\textwidth}{llrrrrrrrr}
\toprule
source &  id &  CART &         PILOT &   RF &   RaFFLE &  \makecell{RaFFLE \\Default} &  XGB & Ridge & Lasso \\
\midrule
  PMLB & 595 &  0.67 & \textbf{0.98} &          0.89 &  \textbf{1.0} &  \textbf{1.0} &          0.93 &  0.75 &  0.76 \\
  PMLB & 596 &  0.72 & \textbf{0.99} &          0.93 &  \textbf{1.0} & \textbf{0.99} &          0.94 &   0.3 &   0.3 \\
  PMLB & 597 &  0.83 & \textbf{0.99} & \textbf{0.96} &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.97} &  0.27 &  0.27 \\
  PMLB & 598 &  0.59 & \textbf{0.97} &          0.86 &  \textbf{1.0} & \textbf{0.99} &          0.91 &  0.74 &  0.74 \\
  PMLB & 599 &  0.85 & \textbf{0.99} & \textbf{0.96} &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.98} &   0.3 &   0.3 \\
  PMLB & 601 &  0.74 & \textbf{0.98} &          0.92 &  \textbf{1.0} & \textbf{0.99} &          0.92 &  0.33 &  0.34 \\
  PMLB & 602 &   0.7 & \textbf{0.98} &          0.88 &  \textbf{1.0} & \textbf{0.99} &          0.88 &  0.24 &  0.24 \\
  PMLB & 603 &  0.28 & \textbf{0.96} &          0.74 &  \textbf{1.0} & \textbf{0.97} &          0.71 &  0.73 &  0.82 \\
  PMLB & 604 &  0.72 & \textbf{0.98} &          0.93 &  \textbf{1.0} &  \textbf{1.0} &          0.95 &  0.26 &  0.26 \\
  PMLB & 605 &  0.49 &          0.91 &          0.87 &  \textbf{1.0} & \textbf{0.97} &          0.91 &  0.33 &  0.36 \\
  PMLB & 606 &  0.79 & \textbf{0.99} &          0.94 &  \textbf{1.0} & \textbf{0.99} & \textbf{0.96} &  0.32 &  0.32 \\
  PMLB & 607 &  0.76 & \textbf{0.97} &          0.93 &  \textbf{1.0} &  \textbf{1.0} &          0.95 &  0.22 &  0.24 \\
  PMLB & 608 &  0.82 & \textbf{0.98} &          0.95 &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.97} &  0.29 &  0.29 \\
  PMLB & 609 &  0.74 & \textbf{0.97} &          0.92 &  \textbf{1.0} &  \textbf{1.0} &          0.95 &  0.77 &  0.77 \\
  PMLB & 611 &  0.54 &  \textbf{1.0} &          0.82 & \textbf{0.97} &          0.94 &          0.82 &  0.17 &  0.17 \\
  PMLB & 612 &  0.83 & \textbf{0.99} & \textbf{0.96} &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.97} &  0.26 &  0.26 \\
  PMLB & 613 &  0.64 &          0.93 &          0.89 &  \textbf{1.0} & \textbf{0.99} &          0.89 &   0.3 &   0.3 \\
  PMLB & 615 &  0.69 &          0.95 &          0.87 &  \textbf{1.0} & \textbf{0.97} &          0.87 &  0.32 &  0.32 \\
  PMLB & 616 &  0.62 &          0.94 &           0.9 &  \textbf{1.0} & \textbf{0.99} &          0.92 &  0.11 &   0.2 \\
  PMLB & 617 &  0.81 &          0.94 &          0.94 &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.97} &  0.23 &  0.23 \\
  PMLB & 618 &  0.74 & \textbf{0.96} &          0.93 &  \textbf{1.0} &  \textbf{1.0} &          0.95 &  0.26 &  0.28 \\
  PMLB & 620 &  0.72 & \textbf{0.99} &          0.91 &  \textbf{1.0} & \textbf{0.99} &          0.94 &  0.26 &  0.27 \\
  PMLB & 621 &  0.35 &          0.95 &          0.71 &  \textbf{1.0} &          0.87 &          0.74 &  0.77 &   0.8 \\
  PMLB & 622 &  0.75 & \textbf{0.97} &          0.93 &  \textbf{1.0} & \textbf{0.99} &          0.95 &  0.27 &  0.29 \\
  PMLB & 623 &  0.82 & \textbf{0.97} &          0.95 &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.97} &  0.29 &  0.29 \\
  PMLB & 624 &  0.37 &           0.9 &          0.82 &  \textbf{1.0} &           0.9 &          0.85 &  0.79 &  0.79 \\
  PMLB & 626 &   0.7 &          0.92 &          0.91 &  \textbf{1.0} & \textbf{0.97} &          0.95 &  0.17 &  0.26 \\
  PMLB & 627 &  0.75 & \textbf{0.96} &          0.95 &  \textbf{1.0} & \textbf{0.99} &          0.95 &  0.28 &  0.29 \\
  PMLB & 628 &  0.86 & \textbf{0.97} & \textbf{0.96} &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.97} &  0.27 &  0.27 \\
  PMLB & 631 &  0.78 & \textbf{0.98} &          0.94 &  \textbf{1.0} & \textbf{0.99} &          0.94 &  0.29 &  0.29 \\
  PMLB & 633 &  0.51 & \textbf{0.96} &          0.83 &  \textbf{1.0} & \textbf{0.97} &          0.87 &  0.76 &  0.77 \\
  PMLB & 634 &  0.46 &          0.89 &          0.93 &  \textbf{1.0} & \textbf{0.97} &          0.87 &  0.42 &  0.42 \\
  PMLB & 635 &  0.44 & \textbf{0.99} &          0.84 &  \textbf{1.0} & \textbf{0.98} &          0.86 &  0.72 &  0.73 \\
  PMLB & 637 &   0.6 & \textbf{0.97} &          0.86 &  \textbf{1.0} & \textbf{0.97} &          0.91 &  0.15 &  0.23 \\
  PMLB & 641 &  0.77 & \textbf{0.99} &          0.92 &  \textbf{1.0} & \textbf{0.99} &          0.93 &  0.25 &  0.26 \\
  PMLB & 643 &  0.65 & \textbf{0.97} &          0.89 &  \textbf{1.0} & \textbf{0.98} &          0.93 &  0.13 &  0.15 \\
  PMLB & 644 &  0.57 & \textbf{0.96} &          0.83 &  \textbf{1.0} & \textbf{0.96} &          0.86 &  0.02 &  0.21 \\
  PMLB & 645 &  0.66 &          0.92 &           0.9 &  \textbf{1.0} & \textbf{0.98} &          0.92 &  0.22 &  0.25 \\
  PMLB & 646 &  0.77 & \textbf{0.96} &          0.93 &  \textbf{1.0} & \textbf{0.99} &          0.94 &  0.33 &  0.33 \\
  PMLB & 647 &   0.6 & \textbf{0.99} &          0.89 &  \textbf{1.0} & \textbf{0.96} &          0.91 &  0.25 &  0.29 \\
  PMLB & 648 &  0.49 & \textbf{0.97} &          0.84 &  \textbf{1.0} & \textbf{0.97} &          0.88 &  0.22 &  0.39 \\
  PMLB & 649 &  0.67 & \textbf{0.99} &           0.9 &  \textbf{1.0} & \textbf{0.99} &          0.92 &  0.78 &  0.78 \\
  PMLB & 650 &  0.47 & \textbf{0.97} &           0.8 &  \textbf{1.0} & \textbf{0.98} &          0.82 &  0.77 &  0.79 \\
  PMLB & 651 &   0.0 &          0.88 &          0.56 &  \textbf{1.0} &          0.81 &          0.59 &  0.72 &  0.76 \\
  PMLB & 653 &  0.37 &          0.87 &           0.8 &  \textbf{1.0} &          0.95 &          0.81 &   0.8 &  0.82 \\
  PMLB & 654 &  0.59 & \textbf{0.97} &          0.86 &  \textbf{1.0} & \textbf{0.98} &          0.89 &  0.74 &  0.74 \\
  PMLB & 656 &  0.51 &          0.86 & \textbf{0.98} &  \textbf{1.0} &          0.95 &  \textbf{1.0} &  0.21 &  0.26 \\
\bottomrule
\end{tabularx}
\end{table}

\renewcommand{\arraystretch}{0.9}
\begin{table}[ht]
\caption{Average $R^2$ score divided by highest average $R^2$ score per dataset (III/III)}
\label{tab:empirical_results_p3}
\centering
\footnotesize
\begin{tabularx}{\textwidth}{llrrrrrrrr}
\toprule
source &   id &          CART &         PILOT &            RF &            RaFFLE &  \makecell{RaFFLE\\Default} &           XGB &         Ridge &         Lasso \\
\midrule
  PMLB &  657 &          0.71 &          0.92 &          0.95 &  \textbf{1.0} & \textbf{0.99} &          0.94 &          0.23 &          0.25 \\
  PMLB &  658 &          0.51 &          0.88 &          0.81 &  \textbf{1.0} & \textbf{0.99} &          0.84 &          0.13 &          0.18 \\
  PMLB &  659 &          0.11 &          0.92 &          0.94 &  \textbf{1.0} & \textbf{0.98} &          0.86 &          0.94 &          0.94 \\
  PMLB &  663 & \textbf{0.98} & \textbf{0.99} & \textbf{0.99} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.97} & \textbf{0.97} \\
  PMLB &  665 &           0.0 &          0.79 &          0.82 &  \textbf{1.0} &          0.95 &           0.0 &          0.87 &          0.89 \\
  PMLB &  666 &          0.21 &          0.94 &          0.94 &  \textbf{1.0} & \textbf{0.99} &          0.81 &          0.88 &          0.88 \\
  PMLB &  678 &           0.0 &          0.91 &          0.19 &          0.87 &          0.52 &          0.22 &  \textbf{1.0} &  \textbf{1.0} \\
  PMLB &  687 &          0.22 &          0.87 & \textbf{0.96} &  \textbf{1.0} &          0.88 &          0.79 &           0.8 &          0.85 \\
  PMLB &  690 & \textbf{0.98} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &          0.92 &          0.92 \\
  PMLB &  695 &          0.86 &          0.94 &          0.95 & \textbf{0.99} & \textbf{0.96} &          0.92 &  \textbf{1.0} &  \textbf{1.0} \\
  PMLB &  706 &          0.25 &          0.95 &          0.92 &          0.92 &          0.81 &          0.63 &  \textbf{1.0} & \textbf{0.99} \\
  PMLB &  712 &           0.7 & \textbf{0.98} & \textbf{0.96} &  \textbf{1.0} & \textbf{0.99} &          0.79 &  \textbf{1.0} &  \textbf{1.0} \\
  PMLB & 1027 &          0.89 & \textbf{0.99} & \textbf{0.98} &  \textbf{1.0} &  \textbf{1.0} &          0.94 &  \textbf{1.0} &  \textbf{1.0} \\
  PMLB & 1028 &          0.89 &          0.75 & \textbf{0.98} & \textbf{0.99} & \textbf{0.98} &          0.89 &  \textbf{1.0} &  \textbf{1.0} \\
  PMLB & 1029 &          0.91 &  \textbf{1.0} &          0.95 &  \textbf{1.0} &  \textbf{1.0} &          0.91 &  \textbf{1.0} &  \textbf{1.0} \\
  PMLB & 1030 &          0.92 & \textbf{0.98} &          0.94 &  \textbf{1.0} &  \textbf{1.0} &          0.92 & \textbf{0.99} & \textbf{0.99} \\
  PMLB & 1089 &          0.76 & \textbf{0.98} &          0.92 &  \textbf{1.0} & \textbf{0.97} &          0.89 &          0.93 &          0.94 \\
  PMLB & 1096 &          0.65 & \textbf{0.96} &          0.92 & \textbf{0.99} & \textbf{0.98} &          0.94 & \textbf{0.96} &  \textbf{1.0} \\
  PMLB & 1193 &          0.35 & \textbf{0.98} & \textbf{0.99} &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.98} &          0.95 &          0.95 \\
  PMLB & 1199 &           0.0 &          0.93 &  \textbf{1.0} & \textbf{0.99} & \textbf{0.98} &          0.93 &          0.93 &          0.93 \\
  PMLB & 1201 &           0.0 &          0.81 &          0.75 & \textbf{0.97} & \textbf{0.97} &  \textbf{1.0} &          0.37 &          0.37 \\
  PMLB & 1203 &          0.77 & \textbf{0.99} & \textbf{0.99} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &          0.85 &          0.85 \\
  PMLB & 4544 &          0.43 &           0.9 &          0.91 & \textbf{0.98} &          0.93 &          0.87 & \textbf{0.99} &  \textbf{1.0} \\
   UCI &    1 &          0.15 &          0.93 & \textbf{0.98} &  \textbf{1.0} & \textbf{0.99} &          0.88 &          0.95 &          0.95 \\
   UCI &    9 &          0.88 &          0.95 & \textbf{0.99} &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.98} &          0.92 &          0.86 \\
   UCI &   10 &          0.71 &          0.58 &  \textbf{1.0} &          0.95 &          0.92 &           0.9 &          0.72 &          0.34 \\
   UCI &   60 &           0.0 &          0.36 &          0.94 &  \textbf{1.0} &          0.92 &           0.0 &          0.42 &          0.51 \\
   UCI &   87 &          0.92 &          0.72 & \textbf{0.98} &          0.94 &          0.93 &  \textbf{1.0} &          0.79 &          0.79 \\
   UCI &  165 &          0.91 &           0.9 & \textbf{0.98} & \textbf{0.99} & \textbf{0.98} &  \textbf{1.0} &          0.65 &          0.65 \\
   UCI &  183 &          0.46 &          0.93 & \textbf{0.97} &  \textbf{1.0} & \textbf{0.99} &          0.91 & \textbf{0.98} & \textbf{0.98} \\
   UCI &  186 &          0.14 &          0.61 & \textbf{0.98} &  \textbf{1.0} &          0.95 &          0.89 &          0.54 &          0.53 \\
   UCI &  275 &          0.91 &          0.95 & \textbf{0.98} &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.99} &          0.73 &          0.73 \\
   UCI &  291 &           0.9 &          0.84 & \textbf{0.98} &  \textbf{1.0} & \textbf{0.96} &  \textbf{1.0} &          0.48 &          0.16 \\
   UCI &  294 & \textbf{0.96} & \textbf{0.98} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} &  \textbf{1.0} & \textbf{0.96} & \textbf{0.96} \\
   UCI &  332 &           0.0 &          0.79 &  \textbf{1.0} &          0.92 &           0.0 &          0.77 &          0.78 &          0.52 \\
   UCI &  368 &          0.88 &          0.92 &          0.89 &          0.92 &          0.92 &           0.9 &  \textbf{1.0} & \textbf{0.97} \\
   UCI &  374 &          0.25 &          0.33 &  \textbf{1.0} & \textbf{0.97} &          0.94 & \textbf{0.97} &           0.3 &           0.3 \\
   UCI &  381 &           0.8 &          0.78 & \textbf{0.99} &  \textbf{1.0} & \textbf{0.98} & \textbf{0.99} &          0.32 &          0.32 \\
   UCI &  409 &          0.68 &          0.92 &          0.87 & \textbf{0.98} &          0.94 &          0.92 &  \textbf{1.0} &          0.83 \\
   UCI &  477 &          0.66 &          0.91 &  \textbf{1.0} & \textbf{0.99} & \textbf{0.98} & \textbf{0.97} &          0.76 &          0.67 \\
   UCI &  492 &           0.0 &          0.63 &          0.82 & \textbf{0.96} &          0.82 &  \textbf{1.0} &           0.0 &          0.23 \\
   UCI &  597 &          0.81 &          0.93 & \textbf{0.99} & \textbf{0.99} & \textbf{0.99} &  \textbf{1.0} &          0.93 &          0.77 \\
\midrule
       & average &     0.64 &          0.91 &          0.92 & \textbf{0.99} & \textbf{0.96} &           0.9 &          0.59 &          0.58 \\
       &  std &          0.28 &          0.15 &          0.09 &          0.02 &           0.1 &          0.15 &          0.33 &          0.31 \\
\bottomrule
\end{tabularx}
\end{table}

%\end{appendices}

\clearpage
%\backmatter

\noindent{\bf Software availability.} The \texttt{Python} and \texttt{C++} code for RaFFLE, and an example script, are at
\url{ https://github.com/STAN-UAntwerp/PILOT/tree/raffle-paper-clean}.

% \bibliographystyle{chicago}
% \setlength{\bibsep}{5pt plus 0.2ex}
% {\small
% \spacingset{1}
% \bibliography{reference.bib}
% }

\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Ao, Li, Zhu, Ali, and Yang}{Ao et~al.}{2019}]{ao2019linear}
Ao, Y., H.~Li, L.~Zhu, S.~Ali, and Z.~Yang (2019).
\newblock The linear random forest algorithm and its advantages in machine learning assisted logging regression modeling.
\newblock {\em Journal of Petroleum Science and Engineering\/}~{\em 174}, 776--789.

\bibitem[\protect\citeauthoryear{Arik and Pfister}{Arik and Pfister}{2021}]{tabnet}
Arik, S.~{\"O}. and T.~Pfister (2021).
\newblock Tabnet: Attentive interpretable tabular learning.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, Volume~35, pp.\  6679--6687.

\bibitem[\protect\citeauthoryear{Breiman}{Breiman}{2001}]{breiman2001random}
Breiman, L. (2001).
\newblock Random forests.
\newblock {\em Machine Learning\/}~{\em 45\/}(1), 5--32.

\bibitem[\protect\citeauthoryear{Breiman, Friedman, Olshen, and Stone}{Breiman et~al.}{1984}]{cart}
Breiman, L., J.~H. Friedman, R.~A. Olshen, and C.~J. Stone (1984).
\newblock {\em Classification and regression trees}.
\newblock Milton Park: Routledge.

\bibitem[\protect\citeauthoryear{Chen and Guestrin}{Chen and Guestrin}{2016}]{xgboost}
Chen, T. and C.~Guestrin (2016).
\newblock Xgboost: A scalable tree boosting system.
\newblock In {\em Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}.

\bibitem[\protect\citeauthoryear{Cortez and Morais}{Cortez and Morais}{2007a}]{Cortez2007ADM}
Cortez, P. and A.~Morais (2007a).
\newblock A data mining approach to predict forest fires using meteorological data.
\newblock In {\em Proceedings of the 13th Portuguese Conference on Artificial Intelligence}, pp.\  512--523.

\bibitem[\protect\citeauthoryear{Cortez and Morais}{Cortez and Morais}{2007b}]{forest_fires_162}
Cortez, P. and A.~Morais (2007b).
\newblock {Forest Fires}.
\newblock UCI Machine Learning Repository.
\newblock {DOI}: https://doi.org/10.24432/C5D88D.

\bibitem[\protect\citeauthoryear{Fernandes, Vinagre, Cortez, and Sernadela}{Fernandes et~al.}{2015}]{online_news_popularity_332}
Fernandes, K., P.~Vinagre, P.~Cortez, and P.~Sernadela (2015).
\newblock {Online News Popularity}.
\newblock UCI Machine Learning Repository.
\newblock {DOI}: https://doi.org/10.24432/C5NS3V.

\bibitem[\protect\citeauthoryear{Fernández-Delgado, Sirsat, Cernadas, Alawadi, Barro, and Febrero-Bande}{Fernández-Delgado et~al.}{2019}]{FERNANDEZDELGADO201911}
Fernández-Delgado, M., M.~Sirsat, E.~Cernadas, S.~Alawadi, S.~Barro, and M.~Febrero-Bande (2019).
\newblock An extensive experimental survey of regression methods.
\newblock {\em Neural Networks\/}~{\em 111}, 11--34.

\bibitem[\protect\citeauthoryear{Freund and Schapire}{Freund and Schapire}{1996}]{freund1996experiments}
Freund, Y. and R.~E. Schapire (1996).
\newblock Experiments with a new boosting algorithm.
\newblock In {\em Proceedings of the Thirteenth International Conference on Machine Learning}, pp.\  148--156. ACM.

\bibitem[\protect\citeauthoryear{Gy{\"o}rfi, Kohler, Krzyzak, and Walk}{Gy{\"o}rfi et~al.}{2002}]{distribution}
Gy{\"o}rfi, L., M.~Kohler, A.~Krzyzak, and H.~Walk (2002).
\newblock {\em A Distribution-{F}ree Theory of Nonparametric Regression}.
\newblock New York: Springer.

\bibitem[\protect\citeauthoryear{Ke, Meng, Finley, Wang, Chen, Ma, Ye, and Liu}{Ke et~al.}{2017}]{lightgbm}
Ke, G., Q.~Meng, T.~Finley, T.~Wang, W.~Chen, W.~Ma, Q.~Ye, and T.-Y. Liu (2017).
\newblock {LightGBM: A Highly Efficient Gradient Boosting Decision Tree}.
\newblock In {\em Neural Information Processing Systems}.

\bibitem[\protect\citeauthoryear{Kelly, Longjohn, and Nottingham}{Kelly et~al.}{2024}]{uciml}
Kelly, M., R.~Longjohn, and K.~Nottingham (2024).
\newblock Uci machine learning repository.

\bibitem[\protect\citeauthoryear{Klusowski and Tian}{Klusowski and Tian}{2024}]{klusowski2024large}
Klusowski, J.~M. and P.~M. Tian (2024).
\newblock Large scale prediction with decision trees.
\newblock {\em Journal of the American Statistical Association\/}~{\em 119\/}(545), 525--537.

\bibitem[\protect\citeauthoryear{Li and Li}{Li and Li}{2011}]{li2011learning}
Li, C. and H.~Li (2011).
\newblock Learning random model trees for regression.
\newblock {\em International Journal of Computers and Applications\/}~{\em 33\/}(3), 258--265.

\bibitem[\protect\citeauthoryear{Loh}{Loh}{2011}]{loh2011classification}
Loh, W.-Y. (2011).
\newblock Classification and regression trees.
\newblock {\em Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery\/}~{\em 1\/}(1), 14--23.

\bibitem[\protect\citeauthoryear{Mendes-Moreira, Soares, Jorge, and Sousa}{Mendes-Moreira et~al.}{2012}]{mendes2012ensemble}
Mendes-Moreira, J., C.~Soares, A.~M. Jorge, and J.~F.~D. Sousa (2012).
\newblock Ensemble approaches for regression: A survey.
\newblock {\em ACM Computing Surveys\/}~{\em 45\/}(1), 1--40.

\bibitem[\protect\citeauthoryear{Olson, La~Cava, Orzechowski, Urbanowicz, and Moore}{Olson et~al.}{2017}]{Olson2017PMLB}
Olson, R.~S., W.~La~Cava, P.~Orzechowski, R.~J. Urbanowicz, and J.~H. Moore (2017, Dec).
\newblock {PMLB: A large benchmark suite for machine learning evaluation and comparison}.
\newblock {\em BioData Mining\/}~{\em 10\/}(36), 1--13.

\bibitem[\protect\citeauthoryear{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay}{Pedregosa et~al.}{2011}]{scikit-learn}
Pedregosa, F., G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel, M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos, D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay (2011).
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock {\em Journal of Machine Learning Research\/}~{\em 12}, 2825--2830.

\bibitem[\protect\citeauthoryear{Raymaekers, Rousseeuw, Verdonck, and Yao}{Raymaekers et~al.}{2024}]{raymaekers2024pilot}
Raymaekers, J., P.~J. Rousseeuw, T.~Verdonck, and R.~Yao (2024).
\newblock Fast linear model trees by {PILOT}.
\newblock {\em Machine Learning\/}~{\em 113}, 6561--6610.

\bibitem[\protect\citeauthoryear{Rodr{\'\i}guez, Garc{\'\i}a-Osorio, Maudes, and D{\'\i}ez-Pastor}{Rodr{\'\i}guez et~al.}{2010}]{rodriguez2010experimental}
Rodr{\'\i}guez, J.~J., C.~Garc{\'\i}a-Osorio, J.~Maudes, and J.~F. D{\'\i}ez-Pastor (2010).
\newblock An experimental study on ensembles of functional trees.
\newblock In {\em Multiple Classifier Systems: 9th International Workshop, MCS 2010, Cairo, Egypt. Proceedings 9}, pp.\  64--73. Springer.

\bibitem[\protect\citeauthoryear{Rodriguez-Galiano, Sanchez-Castillo, Chica-Olmo, and Chica-Rivas}{Rodriguez-Galiano et~al.}{2015}]{rf_performance_rodriguez2015}
Rodriguez-Galiano, V., M.~Sanchez-Castillo, M.~Chica-Olmo, and M.~Chica-Rivas (2015).
\newblock Machine learning predictive models for mineral prospectivity: An evaluation of neural networks, random forests, regression trees and support vector machines.
\newblock {\em Ore Geology Reviews\/}~{\em 71}, 804--818.

\bibitem[\protect\citeauthoryear{Romano, Le, La~Cava, Gregg, Goldberg, Chakraborty, Ray, Himmelstein, Fu, and Moore}{Romano et~al.}{2022}]{romano2021pmlb}
Romano, J.~D., T.~T. Le, W.~La~Cava, J.~T. Gregg, D.~J. Goldberg, P.~Chakraborty, N.~L. Ray, D.~Himmelstein, W.~Fu, and J.~H. Moore (2022).
\newblock {PMLB v1.0: An open source dataset collection for benchmarking machine learning methods}.
\newblock {\em Bioinformatics\/}~{\em 38\/}(3), 878–880.

\bibitem[\protect\citeauthoryear{Shi, Li, and Li}{Shi et~al.}{2019}]{shi2019gradient}
Shi, Y., J.~Li, and Z.~Li (2019).
\newblock Gradient boosting with piece-wise linear regression trees.
\newblock In {\em Proceedings of the 28th International Joint Conference on Artificial Intelligence}, pp.\  3432--3438.

\bibitem[\protect\citeauthoryear{Song, Langfelder, and Horvath}{Song et~al.}{2013}]{song2013random}
Song, L., P.~Langfelder, and S.~Horvath (2013).
\newblock Random generalized linear model: {A} highly accurate and interpretable ensemble predictor.
\newblock {\em BMC Bioinformatics\/}~{\em 14}, 1--22.

\bibitem[\protect\citeauthoryear{Stulp and Sigaud}{Stulp and Sigaud}{2015}]{stulp2015many}
Stulp, F. and O.~Sigaud (2015).
\newblock Many regression algorithms, one unified model: A review.
\newblock {\em Neural Networks\/}~{\em 69}, 60--79.

\bibitem[\protect\citeauthoryear{Torgo}{Torgo}{1997}]{torgo1997functional}
Torgo, L. (1997).
\newblock Functional models for regression tree leaves.
\newblock In {\em Proceedings of the Fourteenth International Conference on Machine Learning}, pp.\  385--393.

\bibitem[\protect\citeauthoryear{Zamo, Mestre, Arbogast, and Pannekoucke}{Zamo et~al.}{2014}]{zamo2014benchmark}
Zamo, M., O.~Mestre, P.~Arbogast, and O.~Pannekoucke (2014).
\newblock A benchmark of statistical regression methods for short-term forecasting of photovoltaic electricity production, {P}art {I}: {D}eterministic forecast of hourly production.
\newblock {\em Solar Energy\/}~{\em 105}, 792--803.

\end{thebibliography}



\end{document}





\end{document}

%=======================================================
\section[Introduction]{Introduction} \label{sec:intro}

Data containing outliers pose severe challenges to data scientists. Outliers can heavily distort the outcome of a statistical analysis, and should thus be handled carefully. Robust statistics develops a collection of tools that are designed to provide reliable results even when outliers are present in the data. The approach taken by robust methodology is to first fit a model to the clean part of the data, and then to detect potential outliers through their deviation from this robust fit \citep{rousseeuw1987bookdata,maronna2019robust}.

Over the last two decades, \proglang{R} has been the dominant programming language for implementing and disseminating robust statistical methods. Packages such as \pkg{Robustbase} \citep{robustbase} and \pkg{rrcov} \citep{rrcov} are widely used for robust statistical analysis, with \pkg{Robustbase} alone having nearly 10 million downloads. More specialized packages like \pkg{robustHD} \citep{robustHD} for high-dimensional data, and \pkg{cellWise} \citep{cellWise} focused on cellwise outliers, have further extended robust statistical methodologies in \proglang{R}. Additionally, some of these algorithms have been implemented in \mbox{\proglang{MATLAB}} through the \pkg{LIBRA} library \citep{LIBRA} and the \pkg{FSDA} library \citep{riani2012fsda}.

Despite the growing popularity of \proglang{Python} in data science, the availability of robust statistical methods in \proglang{Python} has remained limited. Aside from a few implementations, such as the \code{MinCovDet} function
in the \pkg{scikit-learn} library \citep{scikit-learn} and the \code{RLM} function for regression M-estimators in \pkg{statsmodels} \citep{seabold2010statsmodels}, \proglang{Python} users have had little access to the robust tools readily available in \proglang{R}.

To address this gap we created \pkg{RobPy}, a \proglang{Python} package that consolidates the most popular robust statistical algorithms under one umbrella. Built on established libraries like \pkg{NumPy}, \pkg{SciPy}, and \pkg{scikit-learn}, \pkg{RobPy} provides robust tools for data preprocessing, univariate estimation, covariance matrices, regression, and principal component analysis. These tools are complemented by specialized visualization techniques for diagnosing and handling outliers. \pkg{RobPy} aims to bring robust data science algorithms to a wider audience, enabling \proglang{Python} users to perform reliable data analysis even in the presence of outliers.

We will first describe the structure of the \pkg{RobPy} package and explain how it builds on established existing \proglang{Python} libraries for statistics and data science. In Section \ref{sec:softwareusage} we then provide practical examples of the package's functions applied to real-world data.

%======================================================
\section{Structure} \label{sec:structure}

\pkg{RobPy} is a \proglang{Python} package specifically created for robust statistical analysis. It offers an extensive array of modules, classes and functions. The package aims to be user-friendly, while providing a broad spectrum of functionalities. \pkg{RobPy} inherits from the popular libraries \pkg{NumPy} \citep{numpy}, \pkg{SciPy} \citep{SciPy} and \pkg{scikit-learn} \citep{scikit-learn}, ensuring compatibility with these foundational tools. Many of its algorithms mirror or are inspired by implementations found in the \proglang{R}-packages \pkg{robustbase}, \pkg{rrcov} and \pkg{cellWise}. 

In the \pkg{RobPy} package, most base classes are derived from the \pkg{scikit-learn} API \citep{scikit-learn_api}. By using their base classes and adhering to their conventions our implementations are standardized, making them more familiar for users. This also allows users to seamlessly integrate our algorithms with other \pkg{scikit-learn} tools. All of the algorithms are implemented in an object-oriented way and adhere to the \code{fit-predict}/\code{fit-transform} conventions used by \pkg{scikit-learn}.

Most \pkg{RobPy} classes include specialized tools to visualize outliers. In robust statistics, many methods have diagnostic plots to help distinguish between normal observations and various types of outliers. Consequently, we equipped most base classes with a tailored plotting tool specific to the analysis method used. This will be illustrated in Section \ref{sec:softwareusage}.

In the following subsections we will discuss the structure of the various \pkg{RobPy} modules, detailing the classes and functions they implement. We will reference the papers that discuss the robust methods used and their original implementations. Table \ref{tab:allmethods} provides a quick overview of all algorithms implemented in each module and their corresponding visualization tools. For a detailed description of the individual functions and their parameters we refer to the \pkg{RobPy} documentation which can be found at \href{https://robpy.readthedocs.io/en/latest/index.html}{robpy.readthedocs.io}\,.

\begin{center}
\begin{table}[H] \small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{|l||l|l|l|}
\hline
%\rowcolor{grey}
Module & Classes and/or methods & Description & Visualization \\
\hline
\hline
\multirow{3}{*}{Preprocessing} & \code{DataCleaner} & cleans a dataset for analysis & \multirow{3}{*}{} \\
         & \code{RobustPowerTransformer} & robustly transforms features & \\
        & \code{RobustScaler} & robustly scales features &  \\
\cline{1-3}
\multirow{5}{*}{Univariate} & \code{UnivariateMCD} & univariate MCD estimator &  \\
                  & \code{OneStepM} & one-step M-estimator &                   \\
                  & \code{Qn} & $Q_n$ estimator of scale &                   \\
                  & \code{Tau} & $\tau$ estimator of scale&                   \\\cline{4-4}
                  & \code{adjusted\_boxplot} & boxplot for skewed data &  returns the boxplot \\
\hline
\multirow{6}{*}{Covariance} & \code{FastMCD} & fast MCD algorithm &  \multirow{5}{3.7cm}{distance-distance:\\ robust distances versus \\ Mahalanobis distances\\} \\
                 & \code{DetMCD} & deterministic MCD algorithm &  \\
                 & \code{WrappingCovariance} & covariance using wrapping & \\
                 & \code{KendallTau} & covariance via Kendall's $\tau$ &                  \\
                 & \code{OGK} & OGK covariance matrix & \\\cline{4-4}
                 & \code{CellMCD} & cellwise robust MCD & five diagnostic plots                \\ 
\hline
\multirow{3}{*}{Regression} & \code{FastLTSRegression}& fast Least Trimmed Squares & \multirow{3}{3.7cm}{plot of robust\\ residuals versus \\ robust distances of $x$ \\ } \\
 & \code{SRegression} & fast S-regression & \\
 & \code{MMRegression} & MM-regression & \\
 \hline
 \multirow{2}{*}{PCA} & \code{ROBPCA} & ROBust PCA & \multirow{2}{3.7cm}{plot of orthogonal\\
  versus score distances}\\
  & \code{PCALocantore} & Spherical PCA & \\
  \hline
  Outliers & \code{DDC} & detects deviating cells & cellmap \\
  \hline
  \multirow{6}{*}{Utils} & \code{mahalanobis\_distance} & Mahalanobis distances & \multirow{6}{*}{}\\
  & \code{l1median} & spatial median for location & \\
  & \code{weighted\_median} & weighted univariate median & \\
  & \code{stahel\_donoho} & Stahel-Donoho outlyingness& \\
  & \code{Huber} & Huber's $\rho$ and $\psi$ functions& \\
  & \code{TukeyBisquare} & Tukey's $\rho$ and $\psi$ functions & \\
\cline{1-3}
  \multirow{5}{*}{Datasets} & \code{load\_telephone} & load telephone data& \multirow{5}{*}{} \\
  & \code{load\_stars} & load star cluster data & \\
  & \code{load\_animals} & load animals data  & \\
  & \code{load\_topgear} & load TopGear data  & \\
  & \code{load\_glass} & load glass data& \\
  \hline
\end{tabular}
\caption{Overview of the implemented methods per module.}
\label{tab:allmethods}
\end{table}
\end{center}
\normalsize

\subsection{Preprocessing} \label{sec:structure:preprocessing}

The data preprocessing module contains several classes. They are built using functionality from the base classes \code{BaseEstimator, OneToOneFeatureMixin} and \code{TransformerMixin}\linebreak from the \pkg{scikit-learn} API. This means they each implement a \code{.fit}, \code{.transform}, and\linebreak \code{.inverse\_transform} method.

The first class is \code{DataCleaner}, which operates similarly to the \code{checkDataSet} function from the \proglang{R}-package \pkg{cellWise}. The class is used to examine datasets and to exclude specific columns and rows that fail to meet certain conditions, e.g. by containing too many missing values. 
It works with a \code{fit} and a \code{transform} method, and it has two properties, \code{dropped\_columns} and \code{dropped\_rows}, to inspect which columns and rows were removed during the cleaning process. This preprocessor is mostly meant as a convenience tool that combines many conventional preprocessing steps into a single function.

A second class is \code{RobustPowerTransformer}, which robustly transforms skewed variables to approximate normality. For this purpose it employs a Yeo-Johnson or a Box-Cox transform as described in \cite{robustboxcoxyeojohnson}. The \code{fit} method robustly 
estimates a transformation parameter. The user can specify the type of transform (\code{"yeojohnson"} or \code{"boxcox"}). The default is the Yeo-Johnson transform which can handle negative data values. The transformation can then be applied using the \code{transform} method and can be inverted using \code{inverse\_transform}. The implementation is based on the function \code{transfo} in the \proglang{R}-package \pkg{cellWise} and can be considered as a robust alternative to \pkg{scikit-learn}'s \code{PowerTransformer}.

The third class is \code{RobustScaler}, constructed to scale features using a robust scale and/or location estimator of choice. By default the univariate Minimum Covariance Determinant described in the next section is applied, but the user can also pass a different \code{RobustScale}. This preprocessor is similar to \pkg{scikit-learn}'s \code{RobustScaler}, but is more flexible as it allows any robust scale estimator as opposed to only a quantile range.

\subsection{Univariate} \label{sec:structure:univariate}

The univariate module contains several univariate location and scale estimators from the robustness literature. They all use the implemented base class \code{RobustScale}, which has the properties \code{location} and \code{scale} and can be fitted using the \code{fit} method. Each child class is expected to implement a \code{.\_calculate} method where the attributes \code{scale\_} and \code{location\_} are set. Consider a univariate dataset 
$X = \{x_1,\dots,x_n\}$ of sample size $n$. A simple location
estimator is the median of the dataset $\med(X) = \med_i(x_i)$, and the scale can be estimated by the median absolute deviation given by $\mbox{MAD}(X) =
\med_i|x_i - \med(X)|$ (multiplied by 1.4833 if we want the MAD to be consistent for the standard deviation at normally distributed data). 
A minimalist child class that uses the median for location and the MAD for scale would look like this:
% %\begin{CodeChunk}
% \begin{CodeInput}
\begin{verbatim}
import numpy as np
from scipy.stats import median_abs_deviation

class MedianMAD(RobustScale):
    def _calculate(self, X):
        self.location_ = np.median(X)
        self.scale_ = median_abs_deviation(X)
\end{verbatim}
% \end{CodeInput}
% %\end{CodeChunk}

The univariate Minimum Covariance Determinant (MCD) 
estimator \citep{rousseeuw1987bookdata} looks for the $h$-subset (that is, 
a subset of $X$ containing $h$ observations) that has the smallest variance. It then estimates location by the average of that $h$-subset, and estimates scale from its standard deviation. The parameter $h$ must be at least $n/2$ and can be specified by the user as the argument \code{alpha}, which sets $h$ equal to \code{alpha} times~$n$. The algorithm employs a reweighting step and can apply a consistency correction for normal data. More details on the MCD can be found in the next section which describes it multivariate version. 

Also one-step M-estimators are available.
These location and scale estimators are quite %\linebreak
robust and computationally efficient. Their implementation is based on the \proglang{R}-function %\linebreak 
\mbox{\code{estLocScale}} in the package \pkg{cellWise}. The one-step M-estimators start from initial estimates $(\hmu_0,\hsigma_0)$ of location and scale, e.g. the median and the MAD, and afterward perform one reweighting step:
\begin{equation*}
   \hmu_1(X)= \frac{\sum_i w_{1,i} x_i}{\sum_i w_{1,i}} 
   \quad  \text{and} \quad \hsigma_1(X) = 
   \sqrt{\frac{\sigma_0^2}{n \delta}\sum_i w_{2,i} r_i^2}\;.
\end{equation*}
The weights depend on two weight functions $W_1$ and $W_2$ by
\begin{equation*}
   w_{1,i} = W_1(r_i)\quad \text{and} \quad
   w_{2,i} = W_2(r_i) \quad \text{with} \quad
   r_i = \frac{x_i-\hmu_0}{\hsigma_0}\;.
\end{equation*}
The constant $\delta$ above ensures consistency for normally distributed data. 

Also the $Q_n$ scale estimator of \cite{Qn} is included.
It is defined as the first \textbf{Q}uartile of the distances between the points. More formally,
\begin{equation*}
    Q_n = 2.219\,\{|x_i-x_j|:i<j\}_{(k)} 
    \quad \text{with} \quad k = \binom{h}{2} 
    \quad \text{for} \quad 
    h=\left\lfloor\dfrac{n}{2}\right\rfloor + 1.
\end{equation*}
It has much better statistical efficiency than the MAD, and
is computed by the fast algorithm of \cite{fastQn}. 

Lastly, \pkg{RobPy} also contains the $\tau$-estimator from \cite{tau_and_OGK}, which is a special case of the one-step M-estimators given by
\begin{equation*}
    \tau_{location}= \frac{\sum_i w_i x_i}{\sum_i w_i} 
    \quad \text{and} \quad \tau_{scale} = 
    \sqrt{\frac{\MAD^2(X)}{n}\sum_i \rho_{c_2}
    \left(\frac{x_i - \tau_{location}}{\MAD(X)}\right)}
\end{equation*}
where the weights $w_i$ are defined as
\begin{equation*}
    w_i = W_{c_1}\left( \frac{x_i - \med(X)}{\MAD(X)}\right)
    \quad \text{with} \quad 
    W_{c}(u) = \left(1-\left(\frac{u}{c}\right)^2\right)^2 
    I(|u| \leq c)
\end{equation*}
and $\rho_{c}(u) = \min(c^2,u^2)$.
The default values are $c_1 = 4.5$ and $c_2 = 3$, but 
different values can be provided to the class \code{Tau}.

\subsubsection*{Adjusted boxplot}

The standard boxplot is a widely used tool to visualize univariate data. It is based on the median as well as the interquartile range 
$\IQR = Q_3 - Q_1$ in which $Q_1$ is the first quartile and $Q_3$ is the third. It displays the median inside the box that goes from $Q_1$ to $Q_3$\,, and flags points outside the interval $[Q_1 - 1.5\,\IQR\,,\,Q_3 + 1.5\,\IQR]$ as outliers. However, the reasoning behind this choice is restricted to data from a symmetric distribution. Therefore \cite{hubert2008adjustedboxplot} suggested the adjusted boxplot. This plot uses a different boundary to detect outliers, based on the medcouple (MC) of \cite{brys2004medcouple}, a robust skewness measure that is positive when the data has a longer tail on the right and negative when the tail is on the left. It flags points as outliers when they fall outside the interval \vskip-2mm
$$  [Q_1 - 1.5 e^{-4\MC} \IQR,Q_3 + 1.5 e^{3\MC} \IQR] 
    \quad \text{when} \quad \MC \geq 0$$
\vskip-2mm
and outside
\vskip-4mm
$$ [Q_1 - 1.5 e^{-3\MC} \IQR,Q_3 + 1.5 e^{4\MC} \IQR] 
    \quad \text{when} \quad \MC < 0.$$
%\vskip-2mm
This makes the adjusted boxplot more appropriate for asymmetric data. For symmetric data ($\MC=0$) the adjusted boxplot coincides with the classical boxplot. It is included in \pkg{RobPy} as \code{adjusted\_boxplot} and utilizes the \proglang{Python} module \pkg{statsmodels} \citep{seabold2010statsmodels} for the \code{medcouple} function. 

\subsection{Covariance} \label{sec:structure:covariance}

Various robust estimators of covariance matrices (also called `scatter matrices') have been proposed in the past decades, with different properties. The covariance module implements five frequently used scatter estimators. They all use the new base class \code{RobustCovariance} which builds on the \code{EmpiricalCovariance} class in \pkg{scikit-learn}. 

Now the dataset $X = (\bx_1,\dots,\bx_n)^{\top} \in \mathbb{R}^{n \times p}$ is multivariate, with $n$ cases and $p$ numerical variables, and the goal is to estimate a $p \times p$ scatter matrix $\bSigma$ and a central location $\bmu$ which is a $p$-variate point. One of the many uses of such estimates is to compute a statistical distance of each observation $\bx_i$ to the center $\bmu$ relative to the scatter matrix $\bSigma$. When the true $\bmu$ and $\bSigma$ are known, the statistical distance is defined as
\begin{equation*}
    d(\bx_i, \bmu, \bSigma) = \sqrt{\left(\bx_i - \bmu\right)^{\top} \bSigma^{-1} (\bx_i - \bmu)}\;.
\end{equation*}
The classical Mahalanobis distance is given by
\begin{equation*}
    \MD(\bx_i) = d(\bx_i, \boldsymbol{\overline{x}}, \cov(X))
\end{equation*}
where $\boldsymbol{\overline{x}}$ is the empirical mean and $\cov(X)$ is the empirical covariance matrix. The robust distances $\RD(\bx_i)$ are defined analogously, but they plug in a robust location for $\bmu$ and a robust scatter matrix for $\bSigma$.

The \code{RobustCovariance} class includes the distance-distance plot. It shows the robust distances versus the classical Mahalanobis distances, and is equipped with thresholds for outlier detection \citep{rousseeuw1999fastMCD}. By default, it uses the threshold $\sqrt{\chi_{p,0.975}^2}$ to flag points as outliers. The plot is drawn by the \code{distance\_distance\_plot} function, after obtaining a robust covariance estimator by the \code{fit} method.

\subsubsection*{Minimum Covariance Determinant}

The objective of the Minimum Covariance Determinant (MCD) estimator of \cite{rousseeuw1984LTSregression} is to find the $h$-subset whose empirical covariance matrix has the lowest determinant. More formally, we look for a subset $H \subset X$ with $\#H=h$ that minimizes $\text{det}(\cov(H))$, and then put the raw MCD estimates equal to
%This \textit{raw} MCD is defined as follows:
\begin{align*}
    \bmu_{MCD} =&\, \frac{1}{h}\sum_{\bx_i \in H} \bx_i\\
    \bSigma_{MCD} =&\, \frac{1}{h-1}\sum_{\bx_i \in H} (\bx_i - \bmu_{MCD})(\bx_i - \bmu_{MCD})^{\top}\;.
\end{align*}
The estimated scatter matrix $\bSigma_{MCD}$ is often multiplied by a correction factor to make it consistent when the data come from a normal distribution without outliers \citep{Pison:Corfac}. For other properties of the MCD see \cite{Cator2012} and \cite{MCD2018}. \cite{Cuesta2008} applied the MCD to cluster analysis.

Several algorithms exist for the MCD. The \pkg{RobPy} package implements two of them. The first one, \code{FastMCD}, carries out the algorithm of \cite{rousseeuw1999fastMCD}. It starts from a fixed number \code{n\_initial\_subsets} of random initial subsets. To each of them it applies so-called concentration steps (C-steps) that always lower the determinant, and it keeps the fit with the lowest determinant.
A second approach is implemented as \code{DetMCD}, based on the deterministic algorithm of \cite{hubert2012detMCD}. This algorithm is faster because it starts from only six carefully selected preliminary scatter estimators, each of them followed by C-steps until convergence. Both the above algorithms have an argument \code{alpha}, which sets $h$ equal to \code{alpha} times $n$. In practice, the raw MCD is almost always followed by a reweighting step to increase its efficiency. The weights for this are computed as
\begin{equation*}
    w_i = I\left(d(\bx_i, \bmu_{MCD}, \bSigma_{MCD}) \leqslant \sqrt{\chi_{p,0.975}^2}\right),
\end{equation*}
where ($\bmu_{MCD},\bSigma_{MCD}$) are the raw estimates. The reweighted MCD is then given by
\begin{align*}
    \bmu_{RMCD} &= \frac{1}{\sum_{i=1}^n w_i}\,\sum_{i=1}^nw_i \bx_i \\
    \bSigma_{RMCD}& = \frac{1}{\sum_{i=1}^n{w_i-1}}\,\sum_{i=1}^n w_i (\bx_i - \bmu_{RMCD})(\bx_i - \bmu_{RMCD})^{\top}\;.
\end{align*}
The boolean argument \code{reweighting} specifies whether the reweighting step is to be carried out. By default it is.

\subsubsection*{Cellwise MCD}

The algorithms described so far belong to the casewise framework, in which the word outlier refers to a case, that is, a row of the data matrix. But in recent years also a second framework is receiving attention. It can happen that most data cells
(entries) in a row are regular, and just a few of them are
anomalous. The first article to formulate the cellwise paradigm
was \cite{Alqallaf2009}. They noted how outliers propagate:
given a fraction $\varepsilon$ of contaminated cells at random positions, the expected fraction of contaminated rows is
$1 - (1 - \varepsilon)^p$ which grows quickly for increasing $\varepsilon$  and/or increasing dimension $p$. 
The two paradigms are quite different. The casewise 
paradigm is about cases that do not belong in the dataset, for
instance, because they are members of a different population. In contrast, the cellwise paradigm assumes that some cells in a row of the data matrix may deviate
from the values they should have had, perhaps due to measurement
errors, whereas the remaining cells in the same row
still contain useful information. See also \cite{ALYZ2015}.

Both of the above MCD algorithms fall within the casewise framework as they look for an optimal subset containing $h$ cases. However, within the cellwise framework another MCD estimator exists, namely the cellwise MCD estimator of \cite{raymaekers2023cellwiseMCD}. In this method $h$ no longer represents the number of inlying cases, instead it refers to the minimal number of unflagged cells per column (variable). It is implemented in the \pkg{RobPy} package as \code{CellMCD}, which mimics the functionality of the function \code{cellMCD} in the \proglang{R}-package \pkg{cellWise}. For this cellwise method, the usual distance-distance plot does not tell the whole story as it focuses on outlying cases. Instead, this method has its own \code{cell\_MCD\_plot} function that can make 5 different diagnostic plots depending on the argument \code{plottype}. The resulting plots will be illustrated in Section \ref{sec:cellwise_softwareusage}. An advantage of \code{CellMCD} is its built-in mechanism to handle missing values.

\subsubsection*{Wrapping covariance estimator}

As (ultra-) high-dimensional data becomes increasingly common today, \pkg{RobPy} also contains the wrapping covariance estimator of \cite{raymaekers2021froc}. This algorithm robustly standardizes the variables $x_{\bdot j}$ to new variables $z_{\bdot j}$ and then `wraps' them by applying the tailored data transformation $\psi$ given by
\begin{equation*}
    \psi_{b,c}(z) = \begin{cases}
  z  & \text{if} \  0 \leq |z| \leq b \\
  q_1 \tanh(q_2(c-|z|))\text{sign}(z) & \text{if} \ b \leq |z| \leq c \\
  0 & \text{if} \ c \leq |z|\;.
\end{cases}
\end{equation*}
Next it computes the classical covariance of the transformed data and undoes the standardization.
Default values of $b$ and $c$ are 1.5 and 4, with the corresponding $q_1$ and $q_2$ given in Appendix A.6 of \cite{raymaekers2021froc}. The wrapping approach is less sophisticated than the above algorithms, but on the other hand its simplicity allows fast computation of a fairly robust covariance estimator for high-dimensional data, which might have casewise as well as cellwise outliers. It is implemented as the function \code{WrappingCovariance}.

\subsubsection*{Other covariance estimators}

The covariance module contains two additional casewise covariance estimators. One of them is \code{KendallTau}, which estimates the covariance matrix by combining Kendall's $\tau$ rank correlation (implemented in \pkg{SciPy}) with univariate scales obtained by the \code{Qn} estimator, as in \cite{Ollerer2015}. A second covariance estimator is the Orthogonalized Gnanadesikan-Kettenring estimator proposed by \cite{tau_and_OGK}, which is one of the initial estimators used inside \code{DetMCD}.

\subsection{Regression} \label{sec:structure:regression}

Regression is a cornerstone of data science. It aims to predict a numerical response variable $y$ from one or more predictor variables $x_{\bdot j}$ that are also called features or regressors. Naturally, it has received considerable attention in robust statistics, which is why \pkg{RobPy} includes three of the most commonly used robust multiple linear regression algorithms. A base class is used, built on the \code{RegressorMixin} and the \code{BaseEstimator} classes of \pkg{scikit-learn}, employing a \code{fit} and a \code{predict} method. The regression base class is equipped with a tailored plotting tool called \code{outlier\_map}, which plots the standardized robust residuals versus the robust distances of the $x$-points formed by the predictor variables. The thresholds of the robust standardized residuals are $\pm 2.5$ and shown as horizontal lines, whereas the threshold on the robust $x$-distances is at $\sqrt{\chi_{p,0.975}^2}$ yielding a vertical line. This allows us to distinguish between\\ \vspace{-7mm}
\begin{itemize}
\setlength{\parskip}{-1pt}
\setlength{\itemsep}{2pt}
\item regular observations (inlying residual and inlying $x$-distance);
\item vertical outliers (outlying residual and inlying $x$-distance);
\item good leverage points (inlying residual and outlying $x$-distance); % which are fairly harmless;
\item bad leverage points (outlying residual and outlying $x$-distance).
\end{itemize}  
\vspace{-3mm}
The plot is drawn by the function \code{outlier\_map} after fitting a robust regression.

\subsubsection*{Least Trimmed Squares}

A first robust regression estimator is the Least Trimmed Squares (LTS) estimator of \cite{rousseeuw1984LTSregression}. Similar to the MCD covariance estimator, it tries to find an $h$-subset with the smallest sum of squared residuals. This can also be formulated as
\begin{equation*}
    \hbbeta = \underset{\bbeta}{\text{argmin}} \sum_{i=1}^h (r(\bbeta)^2)_{(i)}
\end{equation*}
where $r_i(\bbeta)^2$ are the squared residuals given a vector $\bbeta$ of coefficients, which are then ordered as $(r(\bbeta)^2)_{(1)} \leq \dots \leq (r(\bbeta)^2)_{(n)}$\,. So the objective function is a sum of squared residuals, but it only adds $h$ of them instead of all. The size of this subset can be set by the argument \code{alpha}, which sets $h$ equal to \code{alpha} times $n$. The \code{FastLTSRegression} algorithm starts from \code{n\_initial\_subset} random initial subsets, applies C-steps to each of them, and keeps the result with the lowest objective. In order to increase the statistical efficiency of LTS it is usually followed by a reweighting step, which can be controlled by the boolean argument \code{reweighting}. 

\subsubsection*{S-estimators and MM-estimators}

The regression module also contains two other classes of robust regression estimators. An S-estimator \citep{Sest1984} is designed to minimize an M-estimator of scale of the residuals. It is given by
\begin{equation*}
  \hbbeta = \underset{\bbeta}{\text{argmin}} \ S(\bbeta)
  \quad \text{with} \quad \frac{1}{n} \sum_{i=1}^n \rho \left( \frac{r_i(\bbeta)}{S(\bbeta)} \right) = \frac{1}{2} %\delta
\end{equation*}
where the function $\rho$ is specified by the argument \code{rho} that has a default (Tukey's bisquare) with good properties. 
An MM-estimator \citep{yohai1987MMregr} follows an S-estimator by a second step, similar to reweighting, to increase its statistical efficiency. The S-estimator is implemented as \code{SRegression} and uses the FAST-S algorithm of \cite{2006fastS}, and \code{MMRegression} combines FAST-S with iteratively reweighted least squares.

%%%%%
\newpage
\subsection{Principal component analysis} \label{sec:structure:PCA}

Principal component analysis (PCA) is a popular dimension reduction technique. Its classical version projects the $p$-variate datapoints on a $q$-dimensional subspace while retaining as much variance as possible. However, classical PCA is known to be very sensitive to outliers. Therefore, \pkg{RobPy} includes two robust PCA algorithms. They are built on the base class \code{RobustPCA}, which uses the \code{\_BasePCA} class from \pkg{scikit-learn}. The \code{fit} method learns the robust PCA components, after which the \code{transform} method obtains the $q$-variate scores and the \code{predict} method yields the projected points in $p$ dimensions. The new PCA module also provides a specialized plot \citep{hubert2005robpca} implemented as  \code{plot\_outlier\_map}. It displays the orthogonal distances, which are defined as the Euclidean distance from each point to its projection onto the PCA subspace, versus the score distances, i.e. the robust distances within the PCA subspace. This allows the user to distinguishes between regular cases, orthogonal outliers, good PCA leverage points, and bad PCA leverage points, in a way analogous to the outlier map of robust regression.  

\subsubsection*{ROBPCA}

The first robust PCA algorithm is \code{ROBPCA} from \cite{hubert2005robpca}. It combines the concept of projection pursuit with the idea of using eigenvectors of a robust covariance matrix of the scores. Its main steps are:\\ \vspace{-8mm}
\begin{enumerate}
\setlength{\parskip}{-1pt}
\setlength{\itemsep}{2pt}
\item Find the $h$ least outlying datapoints by selecting those with the lowest Stahel-Donoho outlyingness (to be described in Section \ref{sec:structure:utils});
\item Project the datapoints on the $q$-dimensional subspace spanned by the eigenvectors of the covariance matrix of these $h$ datapoints;
\item Carry out the reweighted MCD on the $q$-variate scores and use the eigendecomposition of its covariance matrix to obtain the final principal components.    
\end{enumerate}

\subsubsection*{Spherical PCA}

Spherical PCA \citep{locantore1999SPCA} is an earlier and less sophisticated PCA algorithm. It computes the principal components as the eigenvectors of the covariance matrix after projecting the data on a sphere, given by
\begin{equation}
    \frac{1}{n-1}\sum_{i=1}^n \frac{(\bx_i-\bhmu)(\bx_i-\bhmu)^{\top}}{||\bx_i-\bhmu||^2}
\end{equation}
where $\bhmu$ is a robust center such as the spatial median, which is the point $\bhmu$ with the lowest total distance
$\sum_{i=1}^n ||\bx_i - \bmu||$ from the datapoints. Spherical PCA is implemented as \code{PCALocantore}. As arguments one can give the desired number of principal components \code{n\_components}, or the fraction of the variance \code{k\_min\_var\_explained} that should be explained. 

\subsection{Detecting cellwise outliers} \label{sec:structure:outliers}

\pkg{RobPy} contains a recent cellwise outlier detection algorithm, the DetectDeviatingCells (DDC) algorithm of \cite{rousseeuw2018DDC}, implemented as the function \code{DDC}. This algorithm is suited even for scenarios with a high number of variables, and can also handle missing values. In addition to detecting outlying cells, it can predict them and impute missing values using the \code{fit}, \code{predict} and \code{impute} methods. \code{DDC} comes with a heatmap plotting tool called \code{cellmap}, used to visualize the standardized residuals of the DDC analysis. The implementation is based on the function \code{DDC} in the \proglang{R}-package \code{cellWise}.

\subsection{Utils} \label{sec:structure:utils}

Several of the methods and algorithms mentioned above utilize fundamental techniques. One of these is the Mahalanobis distance, discussed in Section \ref{sec:structure:covariance}. Another is the Stahel-Donoho outlyingness of a point $\bx$ relative to a dataset $(\bx_1,\dots,\bx_n )^{\top}$ of $p$ dimensions, defined as:
\begin{equation}
    \text{SDO}(\bx) = \sup_{\ba \in \mathbb{R}^p} \frac{|\ba^{\top}\bx - \med_i(\ba^{\top}\bx_i)|}{\MAD_i(\ba^{\top}\bx_i)}.
\end{equation}
Basic functions such as these are bundled in the utils module. For example, there is a base class \code{BaseRho} containing the $\rho$- and $\psi$-functions \code{TukeyBisquare} and \code{Huber}, so they can easily be used. The functions \code{mahalanobis\_distance} and \code{stahel\_donoho} provide straightforward implementations of the above building blocks. The univariate weighted median implemented as \code{weighted\_median} is used in the computation of the $Q_n$ estimator, and the spatial median given by \code{l1median} is used in spherical PCA. 

\subsection{Datasets} \label{sec:structure:datasets}

\pkg{RobPy} includes several datasets that are often encountered in the robustness literature. These datasets serve as standard examples and benchmarks, allowing users to easily test robust algorithms. They are listed in Table \ref{tab:datasets}. The first three datasets consist of bivariate data with small sample sizes, and are commonly employed to illustrate robust covariance and regression estimators. They are also available in the \proglang{R}-package \pkg{robustbase}. The fourth is the well-known TopGear dataset originating from the \proglang{R}-package \pkg{robustHD}. It contains 32 variables of mixed types about 297 cars that were featured in the BBC television show Top Gear until 2014. The TopGear data has been utilized in different settings, including PCA, regression, and cellwise outlier detection. The final dataset is the high-dimensional glass data, consisting of 180 observed intensities at 750 wavelengths. This spectral dataset has frequently been used to illustrate casewise and cellwise outlier detection and is also available in the \proglang{R}-package \pkg{cellWise}.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    data & name in \proglang{R}-package & \proglang{R}-package & n & p & reference\\
    \hline
    telephone & telef & robustbase & 24 & 2 & \cite{rousseeuw1987bookdata}\\
    stars & starsCYG & robustbase & 47 & 2 & \cite{rousseeuw1987bookdata}\\
    animals & Animals2 & robustbase & 65 & 2 & \cite{rousseeuw1987bookdata}\\
    topgear & TopGear & robustHD & 297 & 32 & \cite{robustHD}\\
    glass & data\_glass & cellWise & 180 & 750 & \cite{lemberge2000glassdata}\\
    \hline
    \end{tabular}
    \caption{The datasets available in \pkg{RobPy}.}
    \label{tab:datasets}
\end{table}

The datasets can be loaded using the functions \code{load\_telephone}, \code{load\_stars}, \code{load\_animals}, \code{load\_topgear} and \code{load\_glass}. This returns a \code{Bunch} object from \pkg{scikit-learn}, containing the data, a list of the names of the features, a description of the dataset and a path to the location of the data.

%\newpage
\section{Software usage} \label{sec:softwareusage}

In this section, we demonstrate the implemented robust algorithms by applying them to the TopGear data. This dataset is available in \pkg{RobPy} in the module \code{datasets} and can be loaded as follows:

% %\begin{CodeChunk}
% \begin{CodeInput}
\begin{verbatim}
>>> from robpy.datasets import load_topgear
>>> data = load_topgear(as_frame=True)
>>> print(data.DESCR)
>>> print(data.data.head())
\end{verbatim}
% \end{CodeInput}
% %\end{CodeChunk}

These commands also show the corresponding data description and the first 5 rows.

Prior to showcasing the methods, we remark that notebooks that execute all code in this section are available at \href{https://robpy.readthedocs.io/en/latest/examples.html}{robpy.readthedocs.io}\;.

\subsection{Preprocessing} \label{sec:preprocessing_softwareusage}

Before we subject the TopGear data to various methods we apply some preprocessing steps, as is often done when analyzing data. First we clean the data by removing columns and rows that are not suited for most analysis methods, such as non-numerical columns, columns and rows with too many missing values, discrete columns, columns with a scale of zero, and columns corresponding to the case numbers. We do this with the constructed class \code{DataCleaner}:
%%\begin{CodeChunk}
\begin{verbatim}
>>> from robpy.preprocessing import DataCleaner
>>> cleaner = DataCleaner().fit(data.data)
>>> clean_data = cleaner.transform(data.data)
\end{verbatim} 
%\end{CodeChunk}
To see which columns and rows have been removed, we run the following code: 
%%\begin{CodeChunk}
\begin{verbatim}
>>> print(json.dumps(cleaner.dropped_columns, indent=4))
>>> print(cleaner.dropped_rows)
\end{verbatim}
%\end{CodeChunk}
This gives us:
%\begin{CodeChunk}
%\begin{CodeOutput}
\begin{verbatim} 
{
    "non_numeric_cols": [
        "Make",
        "Model",
        "Type",
        ...
        "Origin"
    ],
    "cols_rownumbers": [],
    "cols_discrete": [
        "Fuel",
        "DriveWheel",
        "AdaptiveHeadlights",
        ...
        "Origin"
    ],
    "cols_bad_scale": [
        "Cylinders"
    ],
    "cols_missings": []
}
{'rows_missings': [69, 95]}
\end{verbatim} 
%\end{CodeOutput}
%\end{CodeChunk}
As a second preprocessing step, we aim to robustly transform variables that are skewed and/or have a long tail, to bring them closer to normality. For example, the variable \code{Price} is clearly right skewed, as we can see in its adjusted boxplot and its histogram in Figure~\ref{fig:price}.
%\begin{CodeChunk}
\begin{verbatim}
>>> adjusted_boxplot(clean_data['Price'],figsize=(2,2))
>>> clean_data['Price'].hist(bins=20, figsize=(4, 2))
\end{verbatim}
%\end{CodeChunk}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.34\linewidth]{price_adjboxplot.png}
    \includegraphics[width=0.5\linewidth]{price.png}
    \captionsetup{skip=0pt}
    \caption{Adjusted boxplot and histogram of the variable \code{Price}.}
    \label{fig:price}
\end{figure}

As this might affect approaches that assume normality or symmetry of the inliers, we now use the class \code{RobustPowerTransformer} to robustly transform the variable so its central part becomes more symmetric:
%\begin{CodeChunk}
\begin{verbatim}
>>> from robpy.preprocessing import RobustPowerTransformer
>>> price_transformer = RobustPowerTransformer(method='auto').fit
        (clean_data['Price'])
>>> clean_data['Price_transformed'] = price_transformer.transform
        (clean_data['Price'])
\end{verbatim}
%\end{CodeChunk}
The \code{fit} method of \code{RobustPowerTransformer} selects the most appropriate transformation, which is either of the Yeo-Johnson type or the Box-Cox type. It estimates the transformation parameter $\lambda$ by minimizing a reweighted maximum likelihood objective function. Afterward the \code{transform} method applies the transformation to the data variable. For \code{Price} the selected transformation and $\lambda$ parameter are:
%\begin{CodeChunk}
\begin{verbatim}
>>> price_transformer.method, price_transformer.lambda_rew
\end{verbatim}
%\end{CodeChunk}
%\begin{CodeChunk}
\begin{verbatim}
('boxcox', -0.42354039562300644)
\end{verbatim}
%\end{CodeChunk}
The method has selected the Box-Cox transformation here. A parameter value of $\lambda=1$ would indicate no transformation, and $\lambda=0$ would correspond to the logarithmic transform. The selected $\lambda \approx -0.42$ thus goes a bit further than the logarithmic transform.
Figure~\ref{fig:pricetransformed} shows the robustly transformed variable. We see that the transformation has made the variable more symmetric and has reduced the long right tail.
%\begin{CodeChunk}
\begin{verbatim}
>>> adjusted_boxplot(clean_data['Price_transformed'],figsize=(2,2))
>>> clean_data['Price_transformed'].hist(bins=20, figsize=(4, 2))
\end{verbatim}
%\end{CodeChunk}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.286\linewidth]{price_transformed_adjboxplot.png}
    \includegraphics[width=0.5\linewidth]{price_transformed.png}
    \caption{Adjusted boxplot and histogram of the variable \code{Price} transformed by the Box-Cox transformation with $\lambda = -0.42354$\,.}
    \label{fig:pricetransformed}
\end{figure}
Other skewed variables that benefit from a robust power transformation are \code{Displacement, BHP, Torque} and \code{TopSpeed}:
%\begin{CodeChunk}
\begin{verbatim}
>>> fig, axs = plt.subplots(2, 2, figsize=(15, 8))
>>> for col, ax in zip(['Displacement', 'BHP', 'Torque', 'TopSpeed'],
        axs.flatten()):
>>>     clean_data[col].hist(ax=ax, bins=20, alpha=0.3)
>>>     transformer = RobustPowerTransformer(method='auto')
            .fit(clean_data[col].dropna())
>>>     clean_data.loc[~np.isnan(clean_data[col]), col] = 
            transformer.transform(clean_data[col].dropna())
>>>     ax2=ax.twiny()
>>>     clean_data[col].hist(ax=ax2, bins=20, label='transformed',
            color='orange', alpha=0.3)
>>>     ax.grid(False)
>>>     ax2.grid(False)
>>>     ax2.legend(loc='upper right')
>>>     ax.set_title(f'{col}: method = {transformer.method}, lambda = 
            {transformer.lambda_rew:.3f}')
>>> fig.tight_layout()
\end{verbatim}
%\end{CodeChunk}
The transformation results for these variables are shown in Figure~\ref{fig:othertransforms}. In each panel the blue histogram shows the original variable, the orange histogram shows the transformed variable, and the overlap has a mixed color.
% The transformed variable appears to be shifted so its smallest value matches that of the original variable. One could also match their medians, but this would make the overlap bigger.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{other_transforms.png}
    \caption{\code{RobustPowerTransformer} applied to
    \code{Displacement, BHP, Torque} and \code{TopSpeed}.}
    \label{fig:othertransforms}
\end{figure}
Another preprocessing class in the \pkg{RobPy} package is the \code{RobustScaler} class, designed to scale and/or center variables using a \code{RobustScale}. This could for instance be the location and the scale from the univariate Minimum Covariance Determinant, implemented as \code{UnivariateMCD}. We will illustrate it in Section \ref{sec:PCA_softwareusage}.

\subsection{Location and scatter estimators} \label{sec:locationscatter_softwareusage}

Estimates of location and scatter play a key role in data analysis. It is widely known that the classical mean and covariance matrix are heavily influenced by outliers. Therefore we implemented several robust alternatives, all building on the base class \code{RobustCovariance}. There are two algorithms for the Minimum Covariance Determinant, \code{FastMCD} and \code{DetMCD}. The Orthogonalized Gnanadesikan-Kettenring estimator is computed by \code{OGK}, Kendall's tau covariance by \code{KendallTau}, and the covariance matrix based on the wrapping function is obtained by \code{WrappingCovariance}. Below we illustrate the functionality of \code{FastMCD} on the TopGear data, but the other methods are run in a similar way.

As the estimators listed above only work on data without missing values, we first remove the cars containing missing values. However, in Section \ref{sec:cellwise_softwareusage} a cellwise robust covariance algorithm will be illustrated that can handle missing values. 
%\begin{CodeChunk}
\begin{verbatim}
>>> clean_data2 = clean_data.dropna()
\end{verbatim}
%\end{CodeChunk}
Next, we calculate the Minimum Covariance Determinant on the data by employing the \code{fit} method. Here we use the transformed \code{Price} variable, so we remove the original \code{Price} variable. To visualize the outliers we draw a distance-distance plot. 
%\begin{CodeChunk}
\begin{verbatim}
>>> from robpy.covariance import FastMCD
>>> mcd = FastMCD().fit(clean_data2.drop(columns=['Price']))
>>> fig = mcd.distance_distance_plot()
\end{verbatim}
%\end{CodeChunk}
\begin{figure}[H]
    \centering
    \includegraphics[width=.5\linewidth]{fastmcd3.png}
    \caption{Distance-distance plot showing the robust 
    distances obtained by FastMCD versus the non-robust
    classical Mahalanobis distances.}
    \label{fig:fastmcd}
\end{figure}
In Figure~\ref{fig:fastmcd}, one extreme outlier stands out in the top right corner, as it has a very large robust distance as well as a large Mahalanobis distance. We can inspect this outlier as follows:
%\begin{CodeChunk}
\begin{verbatim}
>>> data.data.loc[
       clean_data2.index[(mcd._robust_distances > 60) & 
       (mcd._mahalanobis_distances > 12)], ['Make', 'Model']+
       list(set(clean_data2.columns).intersection(set(data.data.columns)))]
\end{verbatim}
%\end{CodeChunk}
%\begin{CodeChunk}
\begin{verbatim}
      Make   Model  Acceleration  Height  ...  Price    Displacement
41    BMW    i3     7.9           1578.0  ...  33830.0  647.0
\end{verbatim}
%\end{CodeChunk}
The outlier is the electric BMW i3, which stands out because this is a dataset featuring vehicles from before 2014, when there were few electric cars.

\subsection{Principal component analysis} \label{sec:PCA_softwareusage}

As principal component analysis is usually based on the classical covariance estimator, it is very prone to outliers. In the \pkg{RobPy} package, we have implemented two robust PCA methods: \code{ROBPCA} and spherical PCA (\code{PCALocantore}). Here we will illustrate \code{ROBPCA}.

When the variables have different measurement units or some variables have very different scales, any PCA analysis will be dominated by the variables with the largest scale. Therefore, it is often recommended to scale the variables first. Here we start by scaling the TopGear data with the \code{RobustScaler} class, using its default scale estimator, which is the univariate MCD for each variable:
%\begin{CodeChunk}
\begin{verbatim}
>>> from robpy.preprocessing import RobustScaler
>>> from robpy.pca import ROBPCA
>>> scaled_data = RobustScaler(with_centering=False).fit_transform(
        clean_data2.drop(columns=['Price']))
>>> pca = ROBPCA().fit(scaled_data)
>>> print(pca.components_)
>>> print(pca.explained_variance_ratio)
\end{verbatim}
%\end{CodeChunk}
%\begin{CodeChunk}
\begin{verbatim}
[[-0.3249863   0.09160325]
 [-0.32181544  0.17999333]
 [-0.33824779 -0.00529083]
 [ 0.25809292 -0.34035504]
 [-0.24901036  0.31572018]
 [ 0.2507995  -0.14444022]
 [-0.36398309 -0.30642778]
 [-0.35399797 -0.24551552]
 [-0.34291378 -0.2203963 ]
 [-0.06770964 -0.71357778]
 [-0.32390731  0.10419143]]
[0.75655623 0.87247218]
\end{verbatim}
%\end{CodeChunk}
The output indicates that ROBPCA has obtained 2 principal components that together  explain 87.25\% of the variance. If more components are wanted, this can be specified by the argument \code{n\_components}. To visualize possible outliers we draw an outlier map, shown in Figure~\ref{fig:robpca}.
%\begin{CodeChunk}
\begin{verbatim}
>>> score_distances, orthogonal_distances, score_cutoff, od_cutoff = 
        pca.plot_outlier_map(scaled_data, return_distances=True)
\end{verbatim}
%\end{CodeChunk}
\begin{figure}[H]
    \centering
    \includegraphics[width=.5\linewidth]{robpca.png}
    \caption{PCA outlier map from ROBPCA.}
    \label{fig:robpca}
\end{figure}
Most points in Figure~\ref{fig:robpca} lie in the region of regular cases, plus a few orthogonal outliers and good leverage points. In the top right panel we see an extreme bad leverage point, that is, a point with an outlying orthogonal distance and an outlying score distance, as well as some borderline cases. We identify these cars as follows:
%\begin{CodeChunk}
\begin{verbatim}
>>> data.data.loc[clean_data2.loc[(score_distances > score_cutoff) &
        (orthogonal_distances > od_cutoff)].index, ['Make', 'Model'] + 
        list(set(clean_data2.columns).intersection(set(data.data.columns)))]
\end{verbatim}
%\end{CodeChunk}
%\begin{CodeChunk}
\begin{verbatim}
    Make           Model    Displacement  TopSpeed  ...  MPG
41  BMW            i3       647.0         93.0      ...  470.0
49  Bugatti        Veyron   7993.0        252.0     ...  10.0
124 Jeep           Wrangler 2777.0        107.0     ...  34.0
135 Land Rover     Defender 2198.0        90.0      ...  25.0
164 Mercedes-Benz  G-Class  2987.0        108.0     ...  25.0
196 Pagani         Huayra   5980.0        230.0     ...  23.0
\end{verbatim}
%\end{CodeChunk}
Here we again observe an electric car (the BMW i3), two ultra-high-performance sports cars (Bugatti Veyron and Pagani Huayra) and three rugged all terrain vehicles (Jeep Wrangler, Land Rover Defender and the Mercedes-Benz G-Class). It makes sense that these stand out.

\subsection{Regression} \label{sec:regression_softwareusage}
We saw that \pkg{RobPy} contains several robust regression algorithms. Here we demonstrate the workings of the \code{MMRegression} class. The algorithms \code{FastLTSRegression} and \code{SRegression} operate similarly. 

Our goal is to predict the transformed price from the other variables. The linear model is fitted as follows, resulting in the coefficients below:
%\begin{CodeChunk}
\begin{verbatim}
>>> from robpy.regression import MMRegression
>>> X = clean_data2.drop(columns=['Price', 'Price_transformed'])
>>> y = clean_data2['Price_transformed']
>>> estimator = MMRegression().fit(X, y)
>>> estimator.model.coef_
\end{verbatim}
%\end{CodeChunk}
%\begin{CodeChunk}
\begin{verbatim}
array([ 2.71338919e-01,  4.24224370e-01,  2.04835077e-01,  3.63925688e-02,
        8.75191584e-02,  3.77330897e-03,  4.65624836e-04, -2.97257613e-04,
        8.37238866e-04, -9.42993337e-04])
\end{verbatim}
%\end{CodeChunk}
Next we draw an outlier map to see whether there are any outliers:
%\begin{CodeChunk}
\begin{verbatim}
>>> resid, std_resid, distances, vt, ht = estimator.outlier_map(X, 
        y.to_numpy(), return_data=True)
>>> bad_leverage_idx = (np.abs(std_resid) > vt) & (distances > ht)
\end{verbatim}
%\end{CodeChunk}
The resulting plot is Figure~\ref{fig:MMregression}. It considers most points as regular cases and good leverage points, plus a few vertical outliers. We also see some points that fall in the `bad leverage point' region, most of which are merely borderline cases.
\begin{figure}[H]
    \centering
    \includegraphics[width=.5\linewidth]{MMregression2.png}
    \caption{TopGear data: outlier map of the MM-regression of the transformed price on the other numerical variables.}
    \label{fig:MMregression}
\end{figure}
As these points have an outlying price, we would like to compare the actual price with the price predicted by the model. However, in section \ref{sec:preprocessing_softwareusage} we transformed the variable \code{Price} by a Box-Cox transform, so the predictions need to be transformed back first:
%\begin{CodeChunk}
\begin{verbatim}
>>> data.data.loc[clean_data2[bad_leverage_idx].index, ['Make', 'Model',
        'Price']].assign(predicted_price=price_transformer.inverse_transform
        (estimator.predict(X.loc[bad_leverage_idx])).round())
\end{verbatim}
%\end{CodeChunk}
%\begin{CodeChunk}
\begin{verbatim}
        Make           Model          Price     predicted_price
2       Aston Martin   Cygnet         30995.0   15326.0
5       Aston Martin   V12 Zagato     396000.0  117938.0
164     Mercedes-Benz  G-Class        82945.0   34576.0
222     Rolls-Royce    Phantom        352720.0  116166.0
223     Rolls-Royce    Phantom Coupe  333130.0  111003.0
253     Toyota         Prius          24045.0   16272.0
\end{verbatim}
%\end{CodeChunk}
We see that these points represent cars from prestigious brands (Aston Martin, Mercedes and Rolls-Royce) and a pioneer in hybrid technology (the Toyota Prius). These characteristics explain why their actual price exceeds their predicted price.


\subsection{Algorithms for cellwise outliers} \label{sec:cellwise_softwareusage}
All algorithms illustrated so far are designed for the casewise outlier paradigm. We now switch to the cellwise outlier paradigm and illustrate the workings of the DetectDeviatingCells and cellMCD algorithms, both of which can also handle missing values.

%\newpage
\subsubsection*{DetectDeviatingCells} \label{sec:ddc_softwareusage}
The DetectDeviatingCells (DDC) algorithm \citep{rousseeuw2018DDC} takes a dataset that may contain missing values, and aims to find cellwise outliers. It computes robust correlations between the variables, and produces a predicted value for each cell based on the other cells in the same row. Each cell thus obtains a cellwise residual, which is the actual cell value minus its prediction. When the absolute value of the standardized cellwise residual is above $\sqrt{\chi^2_{1,0.99}} \approx 2.5$ the cell is flagged as outlying.

The results of DDC can be visualized in a cellmap. This is a kind of heatmap in which data cells are represented by small squares. Cells with inlying residuals are colored yellow, and missing values are white. Cells with a positive outlying residual are shown in red. These are measurements that were higher than predicted.
Cells with negative outlying residual are shown in blue instead. In order to distinguish far outliers from borderline cells, the color changes gradually from yellow over orange to more intense red, and from yellow over light purple to more intense blue.

The code below runs the DDC algorithm and draws such a cellmap. Since the TopGear data contains a lot of cars the full cellmap is very big, and mostly yellow because the percentage of outlying cells is small. For illustration purposes we only show the cellmap for a small subset of 17 cars, some of which have interesting outlying cells.
%\begin{CodeChunk}
\begin{verbatim}
>>> from robpy.outliers import DDC
>>> ddc = DDC().fit(clean_data.drop(columns=['Price']))
>>> row_indices = np.array([ 11,  41,  55,  73,  81,  94,  99, 135, 150, 164,
        176, 198, 209, 215, 234, 241, 277])
>>> ax = ddc.cellmap(clean_data.drop(columns=['Price']), 
        row_zoom=row_indices)
>>> cars = data.data.apply(lambda row: f"{row['Make']} {row['Model']}", 
        axis=1).tolist()
>>> ax.set_yticklabels([cars[i] for i in row_indices], rotation=0)
\end{verbatim}
%\end{CodeChunk}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.85\linewidth]{ddccellmap2.png}
    \caption{TopGear cellmap resulting from the DetectDeviatingCells algorithm.}
    \label{fig:DDCcellmap}
\end{figure}

The resulting cellmap is in Figure~\ref{fig:DDCcellmap}.  
We see for instance that the Audi A4 has no outlying cells, whereas the electric BMW i3 has an abnormally small \code{Displacement} (which is that of its small `range extender' petrol engine that can generate extra electricity) and very high miles per gallon \code{MPG}. Some of the other outlying cells in this cellmap are interpreted in \cite{rousseeuw2018DDC}.

Note that the \code{impute} method from the class \code{DDC} provides a version of the dataset in which the missing values and the outlying cells have been imputed, while leaving the remaining cells unchanged. This imputed dataset can be useful with other analysis methods. Moreover, when we receive new (out of sample) data, even if it is just a single case, the \code{predict} method will return the same type of results as for the training data, such as standardized cellwise residuals, detection of outlying cells, and cell imputation.

%\newpage
\subsubsection*{Cellwise MCD} \label{sec:cellmcd}
In this last section we demonstrate the cellwise MCD algorithm , a robust covariance estimator for the cellwise outlier paradigm. It is implemented as the class \code{CellMCD} and can also handle missing values. Here we apply it to the TopGear data. To reproduce the results in \cite{raymaekers2023cellwiseMCD} we first log-transform the variables \code{Displacement}, \code{BHP}, \code{Torque}, and \code{TopSpeed}.
%\begin{CodeChunk}
\begin{verbatim}
>>> from robpy.covariance.cellmcd import CellMCD

>>> data = load_topgear(as_frame=True)
>>> car_models = data.data['Make'] + data.data['Model']
>>> cleaner = DataCleaner().fit(data.data)
>>> clean_data = cleaner.transform(data.data)
>>> clean_data = clean_data.drop(columns=['Verdict'])
>>> for col in ['Displacement', 'BHP', 'Torque', 'TopSpeed']:
>>>     clean_data[col] = np.log(clean_data[col])
>>> clean_data['Price'] = np.log(clean_data['Price']/1000)
>>> car_models.drop(cleaner.dropped_rows["rows_missings"],inplace=True)
>>> car_models = car_models.tolist()
>>> clean_data.head()

>>> cellmcd = CellMCD()
>>> cellmcd.fit(clean_data.values)
\end{verbatim}
%\end{CodeChunk}

We now illustrate the diagnostic plots available for cellMCD, which 
are geared towards identifying outlying cells instead of outlying cases. The four plots in Figure~\ref{fig:cellmcdprice} focus on a single variable, the log-transformed \code{Price}. The panels of Figure~\ref{fig:cellmcdprice} were obtained by the following commands, in which the log(Price) variable has index 0:
%\begin{CodeChunk}
\begin{verbatim}
>>> cellmcd.cell_MCD_plot(variable=0, variable_name="Price", 
       row_names=car_models, plottype="indexplot", 
       annotation_quantile=0.9999999)
>>> cellmcd.cell_MCD_plot(variable=0, variable_name="Price", 
       row_names=car_models, plottype="residuals_vs_variable",
       annotation_quantile=0.9999999)
>>> cellmcd.cell_MCD_plot(variable=0, variable_name="Price", 
       row_names=car_models, plottype="residuals_vs_predictions",
       annotation_quantile=0.9999999)
>>> cellmcd.cell_MCD_plot(variable=0, variable_name="Price", 
       row_names=car_models, plottype="variable_vs_predictions",
       annotation_quantile=0.99999)
\end{verbatim}
%\end{CodeChunk}

\begin{figure}[H]
    \includegraphics[width=.48\textwidth]{indexplotcellmcd.png}\hfill
    \includegraphics[width=.5\textwidth]{resvsvarcellmcd.png}\hfill
    \\[\smallskipamount]
    \includegraphics[width=.5\textwidth]{resvspredcellmcd.png}\hfill
    \includegraphics[width=.49\textwidth]{pricevspredcellmcd.png}\hfill
    \caption{TopGear data: diagnostic plots from \code{CellMCD}.}
    \label{fig:cellmcdprice}
\end{figure}

The automatically annotated cars in Figure 8 were already flagged in Sections \ref{sec:PCA_softwareusage} and \ref{sec:regression_softwareusage}. They are high-end models from prestigious brands. Their price is not in line with their other characteristics according to the patterns formed by most of the cars. The price of these cars includes an exclusivity premium, as is typical for luxury goods.

We can also look at a scatterplot of two variables with their tolerance ellipse based on the cellwise MCD. Here we do this for \code{Acceleration}, which is the time needed  to accelerate from zero to 60 miles per hour, versus the logarithm of \code{Price}:

%\begin{CodeChunk}
\begin{verbatim}
>>> cellmcd.cell_MCD_plot(second_variable = 4, 
       second_variable_name = "Acceleration", row_names = car_models, 
       variable = 0, variable_name = "Price", plottype = "bivariate", 
       annotation_quantile=0.999999)
\end{verbatim}
%\end{CodeChunk}

The resulting plot is Figure~\ref{fig:cellmcdbivariate}. We see a negative relation between acceleration time and price, and indeed the latter can be seen as a proxy for horsepower. The automatically annotated cars at the top were flagged before, but also two new outliers are detected, the SsangYong Rodius and the Renault Twizy. In the plot we see that these cars (and one other car) have a cell value of zero for \code{Acceleration}, which is physically impossible. The robust analysis has thus detected errors in the data. Presumably the true \code{Acceleration} values of these cars were not measured, and someone filled in zeroes instead of labeling these cells as NA.

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{cellmcdbivariate.png}
    \caption{TopGear data: a bivariate diagnostic plot from \code{CellMCD} with tolerance ellipse.}
    \label{fig:cellmcdbivariate}
\end{figure}


\section*{Coding details}

The \pkg{RobPy} package currently uses the following versions of \proglang{Python} and accompanying \mbox{libraries}:
\begin{itemize}
    \item \proglang{Python} 3.10 as the programming environment;
    \item \pkg{scikit-learn} (v1.3 or higher) for machine learning algorithms and data processing;
    \item \pkg{SciPy} (v1.11.4) for scientific computing, including statistical functions;
    \item \pkg{statsmodels} (v0.14.1) for advanced statistical modeling;
    \item \pkg{Matplotlib} (v3.8.2) for data visualization and plotting \citep{matplotlib};
    \item \pkg{tqdm} (v4.66.1) for progress bar integration during computational tasks \citep{tqdm};
    \item \pkg{seaborn} (v0.13.2) for additional visualization features \citep{seaborn}.
\end{itemize}

\section*{Acknowledgments}

Sarah Leyder is supported by Fonds Wetenschappelijk onderzoek - Vlaanderen (FWO) as a PhD fellow Fundamental Research (PhD fellowship 11K5525N). This research received funding from the Flemish Government under the "Onderzoeksprogramma Artificiële Intelligentie (AI) Vlaanderen” programme.

% \bibliographystyle{chicago}
% \setlength{\bibsep}{5pt plus 0.2ex}
% {\small
% \spacingset{1}
% \bibliography{refs.bib}
% }

\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Agostinelli, Leung, Yohai, and Zamar}{Agostinelli et~al.}{2015}]{ALYZ2015}
Agostinelli, C., A.~Leung, V.~J. Yohai, and R.~H. Zamar (2015).
\newblock Robust estimation of multivariate location and scatter in the presence of cellwise and casewise contamination.
\newblock {\em Test\/}~{\em 24\/}(3), 441--461.

\bibitem[\protect\citeauthoryear{Alfons}{Alfons}{2021}]{robustHD}
Alfons, A. (2021).
\newblock \pkg{robustHD}: An \proglang{R} package for robust regression with high-dimensional data.
\newblock {\em Journal of Open Source Software\/}~{\em 6\/}(67), 3786.

\bibitem[\protect\citeauthoryear{Alqallaf, Van~Aelst, Yohai, and Zamar}{Alqallaf et~al.}{2009}]{Alqallaf2009}
Alqallaf, F., S.~Van~Aelst, V.~J. Yohai, and R.~H. Zamar (2009).
\newblock Propagation of outliers in multivariate data.
\newblock {\em The Annals of Statistics\/}~{\em 37}, 311--331.

\bibitem[\protect\citeauthoryear{Brys, Hubert, and Struyf}{Brys et~al.}{2004}]{brys2004medcouple}
Brys, G., M.~Hubert, and A.~Struyf (2004).
\newblock A robust measure of skewness.
\newblock {\em Journal of Computational and Graphical Statistics\/}~{\em 13\/}(4), 996--1017.

\bibitem[\protect\citeauthoryear{Buitinck, Louppe, Blondel, Pedregosa, Mueller, Grisel, et~al.}{Buitinck et~al.}{2013}]{scikit-learn_api}
Buitinck, L., G.~Louppe, M.~Blondel, F.~Pedregosa, A.~Mueller, O.~Grisel, et~al. (2013).
\newblock Api design for machine learning software: Experiences from the \pkg{scikit-learn} project.
\newblock In {\em ECML PKDD Workshop: Languages for Data Mining and Machine Learning}, pp.\  108--122.

\bibitem[\protect\citeauthoryear{Cator and Lopuha{\"a}}{Cator and Lopuha{\"a}}{2012}]{Cator2012}
Cator, E. and H.~P. Lopuha{\"a} (2012).
\newblock Central limit theorem and influence function for the {MCD} estimators at general multivariate distributions.
\newblock {\em Bernoulli\/}~{\em 18}, 520--551.

\bibitem[\protect\citeauthoryear{Croux and Rousseeuw}{Croux and Rousseeuw}{1992}]{fastQn}
Croux, C. and P.~J. Rousseeuw (1992).
\newblock Time-efficient algorithms for two highly robust estimators of scale.
\newblock In {\em Computational Statistics}, Volume~1, pp.\  411--428. Springer-Verlag.

\bibitem[\protect\citeauthoryear{Cuesta-Albertos, Matr\'{a}n, and Mayo-Iscar}{Cuesta-Albertos et~al.}{2008}]{Cuesta2008}
Cuesta-Albertos, J., C.~Matr\'{a}n, and A.~Mayo-Iscar (2008).
\newblock Robust estimation in the normal mixture model based on robust clustering.
\newblock {\em Journal of the Royal Statistical Society: Series B\/}~{\em 70}, 779--802.

\bibitem[\protect\citeauthoryear{{da Costa-Luis}}{{da Costa-Luis}}{2019}]{tqdm}
{da Costa-Luis}, C.~O. (2019).
\newblock \pkg{tqdm}: A fast, extensible progress meter for \proglang{Python} and cli.
\newblock {\em Journal of Open Source Software\/}~{\em 4\/}(37), 1277.

\bibitem[\protect\citeauthoryear{Harris, Millman, van~der Walt, Gommers, Virtanen, Cournapeau, et~al.}{Harris et~al.}{2020}]{numpy}
Harris, C.~R., K.~J. Millman, S.~J. van~der Walt, R.~Gommers, P.~Virtanen, D.~Cournapeau, et~al. (2020).
\newblock Array programming with \pkg{NumPy}.
\newblock {\em Nature\/}~{\em 585\/}(7825), 357--362.

\bibitem[\protect\citeauthoryear{Hubert, Debruyne, and Rousseeuw}{Hubert et~al.}{2018}]{MCD2018}
Hubert, M., M.~Debruyne, and P.~J. Rousseeuw (2018).
\newblock {Minimum Covariance Determinant and extensions}.
\newblock {\em WIREs Computational Statistics\/}~{\em 10}, e1421.

\bibitem[\protect\citeauthoryear{Hubert, Rousseeuw, and Vanden~Branden}{Hubert et~al.}{2005}]{hubert2005robpca}
Hubert, M., P.~J. Rousseeuw, and K.~Vanden~Branden (2005).
\newblock Robpca: A new approach to robust principal component analysis.
\newblock {\em Technometrics\/}~{\em 47\/}(1), 64--79.

\bibitem[\protect\citeauthoryear{Hubert, Rousseeuw, and Verdonck}{Hubert et~al.}{2012}]{hubert2012detMCD}
Hubert, M., P.~J. Rousseeuw, and T.~Verdonck (2012).
\newblock A deterministic algorithm for robust location and scatter.
\newblock {\em Journal of Computational and Graphical Statistics\/}~{\em 21\/}(3), 618--637.

\bibitem[\protect\citeauthoryear{Hubert and Vandervieren}{Hubert and Vandervieren}{2008}]{hubert2008adjustedboxplot}
Hubert, M. and E.~Vandervieren (2008).
\newblock An adjusted boxplot for skewed distributions.
\newblock {\em Computational Statistics \& Data Analysis\/}~{\em 52\/}(12), 5186--5201.

\bibitem[\protect\citeauthoryear{Hunter}{Hunter}{2007}]{matplotlib}
Hunter, J.~D. (2007).
\newblock \pkg{Matplotlib}: A 2d graphics environment.
\newblock {\em Computing in Science \& Engineering\/}~{\em 9\/}(3), 90--95.

\bibitem[\protect\citeauthoryear{Lemberge, De~Raedt, et~al.}{Lemberge et~al.}{2000}]{lemberge2000glassdata}
Lemberge, P., I.~De~Raedt, et~al. (2000).
\newblock Quantitative analysis of 16--17th century archaeological glass vessels using pls regression of epxma and $\mu$-xrf data.
\newblock {\em Journal of Chemometrics\/}~{\em 14\/}(5-6), 751--763.

\bibitem[\protect\citeauthoryear{Locantore, Marron, Simpson, Tripoli, Zhang, and Cohen}{Locantore et~al.}{1999}]{locantore1999SPCA}
Locantore, N., J.~Marron, D.~Simpson, N.~Tripoli, J.~Zhang, and K.~Cohen (1999).
\newblock Robust principal component analysis for functional data.
\newblock {\em Test\/}~{\em 8}, 1--73.

\bibitem[\protect\citeauthoryear{Maechler, Rousseeuw, Croux, Todorov, Ruckstuhl, Salibian-Barrera, Verbeke, Koller, Conceicao, and {di Palma}}{Maechler et~al.}{2023}]{robustbase}
Maechler, M., P.~J. Rousseeuw, C.~Croux, V.~Todorov, A.~Ruckstuhl, M.~Salibian-Barrera, T.~Verbeke, M.~Koller, E.~L.~T. Conceicao, and M.~A. {di Palma} (2023).
\newblock {\em \pkg{robustbase}: Basic Robust Statistics}.
\newblock CRAN.
\newblock \proglang{R} package version 0.95-1.

\bibitem[\protect\citeauthoryear{Maronna, Martin, Yohai, and Salibi{\'a}n-Barrera}{Maronna et~al.}{2019}]{maronna2019robust}
Maronna, R.~A., R.~D. Martin, V.~J. Yohai, and M.~Salibi{\'a}n-Barrera (2019).
\newblock {\em Robust Statistics: Theory and Methods (with \proglang{R})}.
\newblock John Wiley \& Sons.

\bibitem[\protect\citeauthoryear{Maronna and Zamar}{Maronna and Zamar}{2002}]{tau_and_OGK}
Maronna, R.~A. and R.~H. Zamar (2002).
\newblock Robust estimates of location and dispersion for high-dimensional datasets.
\newblock {\em Technometrics\/}~{\em 44\/}(4), 307--317.

\bibitem[\protect\citeauthoryear{{\"O}llerer and Croux}{{\"O}llerer and Croux}{2015}]{Ollerer2015}
{\"O}llerer, V. and C.~Croux (2015).
\newblock Robust high-dimensional precision matrix estimation.
\newblock In K.~Nordhausen and S.~Taskinen (Eds.), {\em Modern Nonparametric, Robust and Multivariate Methods: Festschrift in Honour of Hannu Oja}, pp.\  325--350. Cham: Springer International Publishing.

\bibitem[\protect\citeauthoryear{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, et~al.}{Pedregosa et~al.}{2011}]{scikit-learn}
Pedregosa, F., G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel, et~al. (2011).
\newblock \pkg{scikit-learn}: Machine learning in \proglang{Python}.
\newblock {\em Journal of Machine Learning Research\/}~{\em 12}, 2825--2830.

\bibitem[\protect\citeauthoryear{Pison, Van~Aelst, and Willems}{Pison et~al.}{2002}]{Pison:Corfac}
Pison, G., S.~Van~Aelst, and G.~Willems (2002).
\newblock Small sample corrections for {LTS} and {MCD}.
\newblock {\em Metrika\/}~{\em 55}, 111--123.

\bibitem[\protect\citeauthoryear{Raymaekers and Rousseeuw}{Raymaekers and Rousseeuw}{2021}]{raymaekers2021froc}
Raymaekers, J. and P.~J. Rousseeuw (2021).
\newblock Fast robust correlation for high-dimensional data.
\newblock {\em Technometrics\/}~{\em 63\/}(2), 184--198.

\bibitem[\protect\citeauthoryear{Raymaekers and Rousseeuw}{Raymaekers and Rousseeuw}{2023a}]{raymaekers2023cellwiseMCD}
Raymaekers, J. and P.~J. Rousseeuw (2023a).
\newblock The cellwise minimum covariance determinant estimator.
\newblock {\em Journal of the American Statistical Association\/}, 1--12.

\bibitem[\protect\citeauthoryear{Raymaekers and Rousseeuw}{Raymaekers and Rousseeuw}{2023b}]{cellWise}
Raymaekers, J. and P.~J. Rousseeuw (2023b).
\newblock {\em \pkg{cellWise}: Analyzing Data with Cellwise Outliers}.
\newblock CRAN.
\newblock \proglang{R} package version 2.5.2.

\bibitem[\protect\citeauthoryear{Raymaekers and Rousseeuw}{Raymaekers and Rousseeuw}{2024}]{robustboxcoxyeojohnson}
Raymaekers, J. and P.~J. Rousseeuw (2024).
\newblock Transforming variables to central normality.
\newblock {\em Machine Learning\/}~{\em 113}, 4953--4975.

\bibitem[\protect\citeauthoryear{Riani, Perrotta, and Torti}{Riani et~al.}{2012}]{riani2012fsda}
Riani, M., D.~Perrotta, and F.~Torti (2012).
\newblock \pkg{FSDA}: A \proglang{MATLAB} toolbox for robust analysis and interactive data exploration.
\newblock {\em Chemometrics and Intelligent Laboratory Systems\/}~{\em 116}, 17--32.

\bibitem[\protect\citeauthoryear{Rousseeuw}{Rousseeuw}{1984}]{rousseeuw1984LTSregression}
Rousseeuw, P.~J. (1984).
\newblock Least median of squares regression.
\newblock {\em Journal of the American Statistical Association\/}~{\em 79\/}(388), 871--880.

\bibitem[\protect\citeauthoryear{Rousseeuw and Croux}{Rousseeuw and Croux}{1993}]{Qn}
Rousseeuw, P.~J. and C.~Croux (1993).
\newblock {Alternatives to the Median Absolute Deviation}.
\newblock {\em Journal of the American Statistical Association\/}~{\em 88}, 1273--1283.

\bibitem[\protect\citeauthoryear{Rousseeuw and Leroy}{Rousseeuw and Leroy}{1987}]{rousseeuw1987bookdata}
Rousseeuw, P.~J. and A.~M. Leroy (1987).
\newblock {\em {Robust Regression and Outlier Detection}}.
\newblock John Wiley \& Sons.

\bibitem[\protect\citeauthoryear{Rousseeuw and {Van Den Bossche}}{Rousseeuw and {Van Den Bossche}}{2018}]{rousseeuw2018DDC}
Rousseeuw, P.~J. and W.~{Van Den Bossche} (2018).
\newblock Detecting deviating data cells.
\newblock {\em Technometrics\/}~{\em 60\/}(2), 135--145.

\bibitem[\protect\citeauthoryear{Rousseeuw and {Van Driessen}}{Rousseeuw and {Van Driessen}}{1999}]{rousseeuw1999fastMCD}
Rousseeuw, P.~J. and K.~{Van Driessen} (1999).
\newblock A fast algorithm for the minimum covariance determinant estimator.
\newblock {\em Technometrics\/}~{\em 41\/}(3), 212--223.

\bibitem[\protect\citeauthoryear{Rousseeuw and Yohai}{Rousseeuw and Yohai}{1984}]{Sest1984}
Rousseeuw, P.~J. and V.~J. Yohai (1984).
\newblock Robust regression by means of {S}-estimators.
\newblock In J.~Franke, W.~H{\"a}rdle, and R.~Martin (Eds.), {\em Robust and Nonlinear Time Series Analysis}, New York, pp.\  256--272. Lecture Notes in Statistics No. 26, Springer-Verlag.

\bibitem[\protect\citeauthoryear{Salibian-Barrera and Yohai}{Salibian-Barrera and Yohai}{2006}]{2006fastS}
Salibian-Barrera, M. and V.~J. Yohai (2006).
\newblock A fast algorithm for s-regression estimates.
\newblock {\em Journal of Computational and Graphical Statistics\/}~{\em 15\/}(2), 414--427.

\bibitem[\protect\citeauthoryear{Seabold and Perktold}{Seabold and Perktold}{2010}]{seabold2010statsmodels}
Seabold, S. and J.~Perktold (2010).
\newblock \pkg{statsmodels}: Econometric and statistical modeling with \proglang{Python}.
\newblock In {\em 9th Python in Science Conference}.

\bibitem[\protect\citeauthoryear{Todorov and Filzmoser}{Todorov and Filzmoser}{2009}]{rrcov}
Todorov, V. and P.~Filzmoser (2009).
\newblock An object-oriented framework for robust multivariate analysis.
\newblock {\em Journal of Statistical Software\/}~{\em 32\/}(3), 1--47.

\bibitem[\protect\citeauthoryear{Verboven and Hubert}{Verboven and Hubert}{2005}]{LIBRA}
Verboven, S. and M.~Hubert (2005).
\newblock \pkg{LIBRA}: a \proglang{MATLAB} library for robust analysis.
\newblock {\em Chemometrics and Intelligent Laboratory Systems\/}~{\em 75\/}(2), 127--136.

\bibitem[\protect\citeauthoryear{Virtanen, Gommers, Oliphant, Haberland, Reddy, Cournapeau, et~al.}{Virtanen et~al.}{2020}]{SciPy}
Virtanen, P., R.~Gommers, T.~E. Oliphant, M.~Haberland, T.~Reddy, D.~Cournapeau, et~al. (2020).
\newblock \pkg{SciPy} 1.0: Fundamental algorithms for scientific computing in \proglang{Python}.
\newblock {\em Nature Methods\/}~{\em 17}, 261--272.

\bibitem[\protect\citeauthoryear{Waskom}{Waskom}{2021}]{seaborn}
Waskom, M.~L. (2021).
\newblock \pkg{seaborn}: Statistical data visualization.
\newblock {\em Journal of Open Source Software\/}~{\em 6\/}(60), 3021.

\bibitem[\protect\citeauthoryear{Yohai}{Yohai}{1987}]{yohai1987MMregr}
Yohai, V.~J. (1987).
\newblock High breakdown-point and high efficiency robust estimates for regression.
\newblock {\em The Annals of Statistics\/}~{\em 15}, 642--656.

\end{thebibliography}

\end{document}
