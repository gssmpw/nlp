\section{Introduction}
\begin{figure}[h]
\centering
\includegraphics[width=1.0\columnwidth]{images/introcution/intro.pdf} 
\caption{Overview of MITracker's multi-view integration mechanism. Given $K$ camera views, our method projects features from views with visible targets into a 3D feature volume space, which is then used to refine tracking in views where the target is occluded.}
\label{fig: intro_fig}
\end{figure}

Visual object tracking, a core computer vision task, involves estimating class-agnostic target positions across video sequences. 
This technique is crucial for applications such as augmented reality and autonomous driving, where it is essential to continuously monitor and predict the trajectories of various objects within dynamic environments. 
Despite notable advances in single-view tracking through Siamese networks \cite{li2018high, danelljan2020probabilistic} and transformers \cite{cui2022mixformer, cai2023robust, zheng2024odtrack}, significant challenges persist – particularly occlusions, appearance changes, and target loss. 
While approaches like RTracker \cite{huang2024rtracker} attempt to address these challenges by determining target loss and detection mechanisms, the inherent limitations of single viewpoint information remain a fundamental constraint.


Multi-camera systems offer a promising solution by leveraging complementary viewpoints to maintain continuous tracking, particularly for handling occlusions through camera overlap \cite{zhu2024muc}. 
However, the development of effective multi-view object tracking (MVOT) faces several critical challenges. 
First, existing multi-view datasets are largely restricted to specific object categories like humans or birds \cite{han2023mmptrack, xiao2023multi}, limiting their applicability for generic object tracking. 
Second, current MVOT approaches \cite{xu2016multi, hou2020multiview, harley2023simple} primarily focus on tracking specific categories of objects using detection and re-identification methods, which are not suitable for class-agnostic object tracking. 
Even when attempting to track generic objects across multiple views, 
researchers have to rely on single-view datasets for training due to the absence of comprehensive multi-view data \cite{wu2020visual}. 
This limitation severely restricts models' ability to understand complex spatial relationships and appearance variations across different viewpoints.



To address these challenges, we first construct a \textbf{M}ulti-\textbf{V}iew object \textbf{Track}ing (MVTrack) dataset.
MVTrack dataset contains 234K frames captured from 3-4 cameras, with precise bounding box (BBox) annotations covering 27 distinct objects across 9 challenging tracking attributes such as occlusion and deformation.
Unlike existing datasets such as GMTD \cite{wu2020visual} which only provides testing data, MVTrack dataset offers both training and evaluation sets, enabling development and validation of MVOT models.


To effectively utilize MVTrack dataset, we propose a novel MVOT method named \textbf{M}ulti-View \textbf{I}ntegration \textbf{Tracker} (MITracker) for tracking any object in video frames of arbitrary length from arbitrary viewpoints.
As illustrated in Figure \ref{fig: intro_fig}, MITracker can integrate multi-view features into a unified 3D feature volume and further refine tracking in occluded views, thus producing robust tracking outcomes.
The framework of MITracker consists of two important modules: \textbf{View-Specific Feature Extraction} and \textbf{Multi-View Integration}.
The first module employs a Vision Transformer (ViT) \cite{dosovitskiy2020image} to extract view-specific features of the target object from the current search frame in a streaming manner, where the target object is indicated by a reference frame.
The second module constructs a 3D feature volume by fusing 2D features from multiple views and leveraging bird’s eye View (BEV) guidance, which significantly enhances the model's spatial understanding. 
This 3D feature volume is then deployed in spatial-enhanced attention to improve tracking accuracy.
MITracker allows for the maintenance of stable tracking results and demonstrates strong recovery capabilities in challenging cases such as occlusions and out of view objects.


In summary, our main contributions are as follows:

\begin{itemize}
    \item We introduce MVTrack, a large-scale multi-view tracking dataset containing 234K frames from 3-4 calibrated cameras. It has precise BBox annotations of 27 object categories across 9 challenging tracking attributes, which provides the first comprehensive benchmark for training class-agnostic MVOT methods and enriches the approaches for evaluating these methods. 
    \item We propose MITracker, a novel multi-view tracking method that constructs BEV-guided 3D feature volumes to enhance spatial understanding and utilize a spatial-enhanced attention mechanism to enable robust recovery from target loss in specific views.
    \item Our extensive experiments demonstrate that MITracker achieves state-of-the-art (SOTA) performance on both MVTrack and GMTD datasets, improving recovery rate from 56.7\% to 79.2\% to reduce target loss in challenging scenarios.
\end{itemize}