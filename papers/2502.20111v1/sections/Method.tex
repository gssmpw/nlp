\section{MITracker}
We propose MITracker, a novel multi-view tracking framework that robustly tracks class-agnostic objects across multiple camera views. As illustrated in Figure \ref{fig: method_pipeline}, MITracker consists of two main components: (1) a \textbf{view-specific feature extraction} module (Sec. \ref{subsec: View-specific Feature Extraction}) that encodes frame features and generates single-view tracking results in a streaming fashion, and (2) a \textbf{multi-view integration} module (Sec. \ref{subsec: Multi-view Integration}) that fuses multi-view features with BEV guidance and refines view-specific feature with a spatial-enhanced attention mechanism. 


\subsection{View-Specific Feature Extraction}
\label{subsec: View-specific Feature Extraction}
As shown in Figure \ref{fig: method_pipeline}a, this module processes the video stream from a specific viewpoint \(k\), and extracts target-aware features in the search frame at a timepoint \(t\) based on the reference frame that indicates the target object.

\textbf{View-Specific Encoder.}
We employ ViT as the backbone of our view-specific encoder.
The visual inputs of the view-specific encoder consist of a search frame $S \in \mathbb{R}^{3 \times H_s \times W_s}$ and a reference frame $R \in \mathbb{R}^{3 \times H_r \times W_r}$. 
As the transformer block processes a series of tokens, we segment the frames into non-overlapping patches with $p \times p$ resolution.
The search and reference frames are individually embedded into a token sequence, represented by $I_S \in \mathbb{R}^{N_s \times D}$ and $I_R \in \mathbb{R}^{N_r \times D}$, where $D$ is the hidden dimension, $N_s = \frac{H_s W_s}{p^2}$ is the number of search tokens, and $N_r = \frac{H_r W_r}{p^2}$ is the number of reference tokens.

To ensure temporal continuity between frames, akin to the method utilized in ODTrack \cite{zheng2024odtrack}, two specialized temporal tokens are also included in the inputs of the view-specific encoder to facilitate the propagation of temporal information. 
Specifically, at any given time \( t \), a learnable token \( T_t \) is randomly initialized, which is designed to capture temporal information of the current frame.
Concurrently, we incorporate a token \( T_{t-1} \) that carries temporal information from the preceding frame, which leverages historical features to enhance tracking accuracy and continuity.
The input token sequence of our view-specific encoder can be formulated as the composition of the visual and temporal tokens $f = [T_{t}, T_{t-1}, I_{R}, I_{S}]$, while the output token sequence is denoted as $f' = [T'_{t}, T'_{t-1}, I'_{R}, I'_{S}]$.

After obtaining $f'$, \( T'_{t} \) is used to compute attention weights in conjunction with \( I'_{S} \) to utilize temporal information for adjustments, which can be described as follows:
\begin{equation}
    I_{U} = I^{\prime}_{S}\cdot(I^{\prime}_{S}\times (T^{\prime}_{t})^\top),
\end{equation}
where \( I_U \) represents the extracted feature that encapsulates attention focused on the target object in the search frame.

\textbf{Single-View Tracking Result.}
We employ a BBox head based on the CenterNet architecture \cite{zhou2019objects} to output tracking results from the extracted feature \( I_U \). 
This head comprises three distinct sub-networks, each designed to compute the classification score map, BBox dimensions, and offset sizes, respectively. The highest-scoring position on the classification score map is identified as the target location. This configuration establishes a robust framework capable of effectively handling single-view visual object tracking tasks.

To facilitate further multi-view integration, we also apply convolutional layers to map \( I_U \) to a 2D feature map with original image size, denoted as \( F_{2D} \in \mathbb{R}^{32 \times H_s \times W_s} \).
This establishes a pixel-wise correspondence between the extracted feature and the search image, which is crucial for reconstructing the 3D feature space in the following section.

\subsection{Multi-View Integration}
\label{subsec: Multi-view Integration}
To effectively integrate 2D feature maps $F_{2D}^{1}, F_{2D}^{2}, ..., F_{2D}^{K}$ from $K$ viewpoints, we project them into a 3D feature space and then aggregate them under the supervision of BEV guidance. 
Finally, we embed the aggregated feature to a 3D-aware token to refine all view-specific features $I_{U}^{1}, I_{U}^{2}, ..., I_{U}^{K}$ via spatial-enhanced attention, thus producing stable tracking results across different viewpoints.


\textbf{3D Feature Projection.}
As illustrated in Figure \ref{fig: method_pipeline}b,  we construct a 3D feature volume of size $X \times Y \times Z$, where $(X, Y)$ represents the horizontal plane and $Z$ axis denotes the vertical direction following \cite{zhang2022voxeltrack, harley2023simple}. 
For a viewpoint $k$, we project the $(u, v)$ coordinates in $F_{2D}^{k}$ to $(x, y, z)$ coordinates in the 3D feature volume by the formula below:
\begin{equation}
\begin{pmatrix} u \\ v \\ 1 \end{pmatrix} = C_K [ C_R | C_t] \begin{pmatrix} x \\ y \\ z \\ 1 \end{pmatrix},
\label{eq:projection}
\end{equation}
where $C_K$ represents the camera’s intrinsic matrix, $C_R$ denotes the rotation matrix describing the camera’s orientation, and $C_t$ is the translation vector specifying the camera’s position in space. 
Upon establishing the mapping matrix, we implement bilinear sampling to populate the 3D feature volume. 
In scenarios that involve multiple viewpoints, we compute the average of the mapped values from each view to ensure consistency. Consequently, we derive a 3D feature volume represented as \( F_{3D} \in \mathbb{R}^{32 \times X \times Y \times Z} \).


\textbf{3D Feature Aggregation.}
To better integrate multi-view spatial information, we apply 1D convolutional layers to aggregate features along the Z-axis of $F_{3D}$, resulting in \( F'_{3D} \in \mathbb{R}^{32 \times X \times Y} \), thereby consolidating spatial information within the \( (X, Y) \) plane.
Subsequently, a classification head (i.e., BEV head) is employed to generate a BEV score map from $F'_{3D}$.
This BEV map delineates the object positions on the horizontal plane, thereby imposing supervision constraints on information fusion across multiple viewpoints. 
This integrative approach allows for precise localization and mapping within multi-view scenarios.

\textbf{Spatial-Enhanced Attention.}
BEV guidance for the aggregated 3D feature $F'_{3D}$ only implicitly constrains the original single-view output, but it is insufficient to address the potential target loss issue due to the lack of direct supervision on tracking results.
To remedy this, we introduce spatial-enhanced attention to explicitly incorporate $F'_{3D}$ into the tracking process as shown in Figure \ref{fig: method_pipeline}c. 

We first use convolutional layers to embed $F'_{3D}$ into a 3D-aware token $T_{3D} \in \mathbb{R}^{1 \times D}$, which inherits multi-view spatial information. 
For all the $K$ viewpoints, we then individually concatenate $T_{3D}$ with their unrefined features $I_{U}^{1},I_{U}^{2},...,I_{U}^{K}$ produced by the view-specific encoder.
For a viewpoint $k$, a series of transformer blocks take in its composite token sequence $(T_{3D}, I_{U}^{k})$ and refine them using attention mechanisms that leverage fused 3D spatial information.
A final BBox head outputs the refined tracking results, where potential errors such as target loss are corrected.
