\section{Experiments}

\input{tables/experiments/compairson}

\subsection{Dataset}
In addition to MVTrack dataset, we use two external datasets for training and evaluation, which are detailed as follows.

\begin{itemize}
    \item \textbf{GOT10K.} GOT-10K \cite{huang2019got} is a large and diverse dataset with a wide range of object categories. Its training set contains 9,335 videos across 480 moving object categories.
    \item \textbf{GMTD.} GMTD \cite{wu2020visual} is a multi-view tracking test set with 10 scenes, captured by 2-3 uncalibrated cameras in indoor and outdoor settings. It includes 6 target types and various tracking challenges.
\end{itemize}

\subsection{Implementation Details}
\textbf{Loss Function.}
For the BBox head, we employ a weighted focal loss \cite{lin2017focal} \(L_{\text{cls}}\)  for classification, along with the generalized intersection over union loss \cite{rezatofighi2019generalized} \(L_{\text{giou}}\) and \(L_1\) loss for BBox regression. Additionally, a focal loss \(L_{\text{bev}}\) is utilized for BEV map supervision.
The overall loss function of the model is formulated as follows:
\begin{equation}
L_{\text{track}} = L_{\text{cls}} + \lambda_{\text{giou}} L_{\text{giou}} + \lambda_{L_1} L_1 + \lambda_{\text{bev}} L_{\text{bev}},
\end{equation}
where \(\lambda_{\text{giou}} = 5\), \(\lambda_{L_1} = 2\), and \(\lambda_{\text{bev}} = 0.1\) are the coefficients that balance the contributions from each loss .

\begin{figure*}[htbp]
	\centering
	\begin{subfigure}{0.66\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{images/experiments/success_plot.pdf}
		\caption{Success plot.}
		\label{fig: Success plot}
	\end{subfigure}
        \begin{subfigure}{0.66\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{images/experiments/Frame_numbers_needed_for_recovery_plot.pdf}
		\caption{Recovery ability plot.}
		\label{fig: recovery_plot}
        \end{subfigure}
        \centering
        \begin{subfigure}{0.65\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{images/experiments/Robust_tracking_plot.pdf}
		\caption{Robust tracking plot.}
		\label{fig: Robust tracking plot}
        \end{subfigure}
        
	\caption{General experiments on the MVTrack  dataset evaluate tracking robustness. MITracker provides multi-view results, while other methods yield single-view results. In (a), methods are ranked by AUC and noted in the legend. For (b), the numbers in the legend represent the method's recovery rate within 10 frames after the target disappears.
 }
	\label{OPE}
\end{figure*}


\textbf{Training Setup.}
We initialize our view-specific encoder with pre-trained DINOv2 \cite{oquab2023dinov2} parameters using the ViT-base model \cite{dosovitskiy2020image}. 
For the visual inputs, we set the reference frame with $182 \times 182$ pixels, and the search frame with $364 \times 364$ pixels. 
We utilize the camera parameters from the dataset for projection, with the 3D feature volume having dimensions \( X = 200 \), \( Y = 200 \), and \( Z = 3 \).

Our training process consists of two stages. In the first stage, we only train the view-specific feature extraction module.
Specifically, we train the view-specific encoder and BBox head using single-view inputs from GOT-10K and MVTrack datasets until convergence.
For each viewpoint, we include one reference frame and two random search frames from 200 frame interval in each iteration, thus promoting temporal information propagation between frames.
In the second stage, we fine-tune the view-specific encoder and train the entire framework using multi-view data from the MVTrack dataset.
For each training sample, we randomly select 2 to 4 viewpoints with one reference and two search frames in each iteration. 
All the training procedures are conducted on 2 NVIDIA A100 80GB GPUs.

For detailed implementation and training specifics, please refer to the Appendix.


\subsection{Evaluation Metrics}
We evaluate our method using three standard performance measures from the single-view tracking benchmark \cite{OTB2015, muller2018trackingnet, fan2019lasot}: Area Under Curve (AUC), Precision (P), and Normalized Precision (P$_{Norm}$):

\begin{itemize} 
    \item \textbf{AUC}: The Intersection over Union (IoU) measures the overlap between predicted and ground truth BBoxes in each frame. The AUC metric is calculated by varying the IoU threshold to evaluate the area error in the tracking region. 
    \item \textbf{P}: Precision is defined as the distance between the predicted and ground truth BBox centers. This metric is used to assess the positional error in tracking. 
    \item \textbf{P$_{Norm}$}: To mitigate biases due to variations in BBox size, we normalize the center point by the width and height of the ground truth BBox. This adjustment provides a more accurate metric. 
\end{itemize}



\subsection{Comparison with Existing Methods}
\textbf{SOTA Performance on Benchmark.}
We evaluate tracking performance with single-view visual object tracking methods, training all models (except SAM2 and SAM2Long) on the GOT10K and MVTrack datasets. The models are tested on both the MVTrack and GMTD datasets under single-view and multi-view settings.

However, single-view methods cannot handle multi-view inputs or generate multi-view predictions. To address this, we employ a post-fusion strategy to obtain multi-view results. Specifically, single-view predictions are first projected into the 3D world coordinate system. The region with maximum overlap is identified as the target position, which is then reprojected onto the 2D image plane of each viewpoint to generate the fused multi-view tracking results.

As shown in Table \ref{tab:compairsion}, MITracker achieves superior performance in both multi- and single-view tracking across different datasets. 
In multi-view scenarios with 3-4 cameras, MITracker outperforms other methods that rely on post-processing for multi-view fusion, surpassing the second-best method OSTrack by approximately 26\% in P$_{Norm}$. In single-view settings, MITracker surpasses SOTA methods on the MVTrack dataset, achieving an AUC of 68.57\%, which outperforms ODTrack by approximately 5\%.



Notably, MITracker exhibits strong generalization capabilities by achieving exceptional performance on the GMTD, despite it not being included in the training data.
This demonstrates the robustness of our multi-view approach even in single-view scenarios. We attribute these improvements to our multi-view training strategy, which enables the model to better understand spatial relationships crucial for precise tracking.
It is also noteworthy that post-processing degrades the performance of all single-view methods.
This indicates a substantial distribution gap in view-independent feature detection across models, making effective fusion through geometric projections challenging.


\textbf{Stable Continuous Tracking Capability.}
To further evaluate tracking robustness, we conducted three comparative experiments on the MVTrack dataset. Only MITracker utilizes multi-view inputs, other methods use single-view inputs and generate BBoxes independently.

First, we analyzed tracking success rates across various IoU thresholds, as shown in Figure \ref{fig: Success plot}. MITracker consistently outperforms competing methods regardless of the threshold value.


Second, we evaluated the recovery capability after the target was invisible by measuring the proportion of successful tracking resumption within given frame intervals \cite{huang2024rtracker}.
As illustrated in Figure \ref{fig: recovery_plot}, with a 10-frame interval, MITracker achieves a high success rate of 79.2\% in these recovery tests.
In comparison, SAM2Long only achieves a 56.7\% recovery rate under the same setting, highlighting our method's exceptional ability to quickly reestablish tracking after the target dissapears.

In practical applications, users can manually intervene to restart the model’s tracking by providing an accurate initial position. 
In this experimental setup, we measured the maximum continuous tracking length of video frames and the average number of restarts (triggered when target loss exceeds 10 frames, using ground truth for repositioning) \cite{zhao2024biodrone}.
As shown in Figure \ref{fig: Robust tracking plot}, MITracker achieves nearly 100 frames longer tracking duration than ODTrack while requiring fewer restart counts.



\subsection{Ablation Study}
Results in Table \ref{tab:ablation_study} demonstrate that BEV Loss, which provides implicit multi-view information feedback, significantly enhances model performance. 
This improvement is attributed to its ability to augment spatial awareness during single-view feature extraction. 
The Spatial attention, which utilizes fused information to adjust outputs from single-view perspectives, also contributes to notable performance improvements in the model.

\input{tables/experiments/ablation}


\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{images/experiments/vis.pdf} 
\caption{Qualitative comparison results. Comparison of our tracker with two SOTA methods on MVTrack dataset (top) and GMTD (bottom). Each frame is cropped for better visualization. IoU curves of each method’s prediction and ground truth are shown above, where IoU reflects tracking quality. MITracker demonstrates superior re-tracking performance upon target reappearance.
}
\label{visulazation}
\end{figure}
\subsection{Visualization Comparison}



Our qualitative evaluation focuses on the influence of occlusion and fast motion. In the upper part of Figure \ref{visulazation}, we select two viewpoints from the MVTrack dataset and evaluate them on MITracker and ODTrack, which has the second-best performance on this dataset. 
The \colorbox{mygray}{gray areas} in the graph represent periods when the object is out of view or fully occluded by other objects. 
We can easily observe that MITracker is able to re-track the object shortly after it reappears, whereas ODTrack tends to continue in a lost state. 
For instances \#405 and \#515 in V2, even when the object reappears in the frame, ODTrack still mistakenly locks onto the wrong object. 
The bottom of Figure \ref{visulazation} presents tests conducted on the GMTD, where we also selected the second-best method, EVPTrack, for comparison with MITracker. 
When a pedestrian reappears after being obscured by a pillar, EVPTrack mistakenly locks onto the wrong target, whereas MITracker is able to maintain stable and continuous tracking.

We also visualize the predicted BEV trajectories from MITracker in Figure \ref{fig: trajectory}. 
Referencing the ground-truth trajectories, MITracker effectively integrates multi-view features and provides accurate 3D spatial information.


\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{images/experiments/bev.pdf} 
\caption{Visualization of BEV trajectories on the MVTrack dataset. Left: scene captured by four cameras. Right: scene captured by three cameras.}
\label{fig: trajectory}
\end{figure}

\section{Discussion}
In previous sections, we provide a detailed introduction to the MVTrack dataset and demonstrate the outstanding performance of MITracker. However, there are some areas that could be improved in future work.

\textbf{Limitations.} Although MVTrack dataset includes a diverse set of scenes, it currently consists of indoor environments only, potentially limiting the generalization of methods trained on it to outdoor settings. Additionally, MITracker relies on camera calibration for multi-view fusion, which may restrict its applicability in scenarios where calibration is challenging or infeasible.

\textbf{Future work.} We plan to extend MVTrack dataset by including outdoor scenes and a wider range of tracking objects to enable the development of more generalizable multi-view tracking algorithms. Furthermore, we aim to enhance MITracker by reducing its dependency on precise camera calibration, making it more adaptable to scenarios where accurate calibration is challenging.