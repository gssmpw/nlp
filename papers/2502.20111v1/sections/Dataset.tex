\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{1.0\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{images/dataset/data-umbrella1-1.png}
        \caption{\textit{umbrella1-1}: Deformation, Aspect Ratio Change and Scale Variation.}
        \label{fig: data-umbrella}
    \end{subfigure}
    
    \begin{subfigure}[b]{1.0\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{images/dataset/data-phone3-3.png}
        \caption{\textit{phone3-3}: Low Resolution, Fully Occlusion and Partial Occlusion.}
        \label{fig: data-phone}
    \end{subfigure}

    \begin{subfigure}[b]{1.0\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{images/dataset/data-tenis5-1.png}
        \caption{\textit{tenis5-1}: Out of View, Motion Blur and Background Clutter.}
        \label{fig: data-tennis}
    \end{subfigure}
    
    \caption{Example sequences, annotations, and their corresponding tracking attributes in the MVTrack dataset.}
    \label{fig:data-vis}
\end{figure}


\section{MVTrack Dataset}
MVTrack dataset is designed to fill the gaps in the field of MVOT and has received approval for data collection from an Institutional Review Board.
As shown in Table \ref{tab: benchmark dataset comparison}, compared to single-view datasets, we maintain competitive class diversity while adding multi-view capabilities. Compared to MVOT datasets, we provide significantly richer object categories (27 vs 1-8 classes) and more videos (260) with practical camera setups (3-4 views). MVTrack dataset is the only dataset that combines multi-view tracking, rich object categories, absent label annotations, and calibration information.


\textbf{Data Collection.} 
We employ a multi-camera system for data collection, consisting of 3 or 4 time-synchronized Azure Kinect cameras. 
All video sequences are recorded at a resolution of 1920$\times$1080 with 30 FPS.
These cameras are positioned to ensure multiple overlapping views, and their intrinsic parameters are provided by the manufacturer. 
The extrinsic parameters are obtained through calibration and finely adjusted using MeshLab \cite{meshlab, LocalChapterEvents:ItalChap:ItalianChapConf2008:129-136}. 
With this calibration information, we set the central point of the scene as the origin of the world coordinate system, aligning all viewpoints to this unified coordinate system.

\textbf{Data Annotation.}
MVTrack dataset provides frame-level annotations, including 2D object BBoxes and ground coordinate annotations in a unified coordinate system (i.e., BEV annotations).
Following an annotation strategy similar to LaSOT \cite{fan2019lasot}, where for each visible frame, an axis-aligned BBox tightly encloses the target, and an `invisible' label is assigned for the invisible target. 
The BBox annotations are generated semi-automatically, with trackers \cite{xie2024autoregressive, chen2023seqtrack, zheng2024odtrack} used for initial labeling. 
The machine-generated annotations are then manually adjusted and double-checked for accuracy.
Subsequently, using camera calibration parameters, the 2D object BBoxes from multiple viewpoints are projected into the unified coordinate system to compute the BEV coordinates.


\begin{figure*}[t]
\centering
\includegraphics[width=1\textwidth]{images/method/pipeline.pdf} 
\caption{The framework of MITracker. (a) The view-specific feature extraction module employs a ViT that utilizes temporal tokens to process each view independently, outputting unrefined results that can be further improved by multi-view information. The multi-view integration module contains (b) 3D feature volume construction that aggregates features into 3D space with BEV guidance and (c) spatial-enhanced attention that refines tracking results by 3D spatial information.}
\label{fig: method_pipeline}
\end{figure*}



\textbf{Challenging Attributes.} 
In our dataset, we particularly focus on 9 common tracking challenges to better assess tracker performance: Background Clutter, Motion Blur, Partial Occlusion, Full Occlusion, Out of View, Deformation, Low Resolution, Aspect Ratio Change, and Scale Variation. 

More specifically, Figure \ref{fig:data-vis} illustrates three challenging samples in the MVTrack dataset. Figure \ref{fig: data-umbrella} shows significant deformation and scale changes of an umbrella being opened. Figure \ref{fig: data-phone} demonstrates the tracking of small, low-resolution objects like a mobile phone under full and partial occlusions. Figure \ref{fig: data-tennis} highlights the impact of fast motion causing blur when tracking a tennis ball. These attributes can significantly aid in training the model to achieve more robust results.




\textbf{Statistical Analysis.} 
MVTrack dataset consists of five indoor scenes, captured with a total of ten sets of calibration parameters. 
It covers 27 everyday objects, ranging from small objects like pens to larger objects such as umbrellas. 
The dataset includes 68 sets of multi-view data, comprising 260 videos and a total of 234,430 frames. 

We divide the dataset into training, validation, and testing sets. The training set consists of 196 videos and 180K frames, while the validation set contains 30 videos and 28K frames. The testing set comprises 34 videos and 26K frames. We include an unseen scene in the validation and testing sets that are distinct from the scenes in the training set. Furthermore, the testing set includes both object categories that appear in the training set and new object categories not present during training. This enables evaluation of the model's performance across various targets and settings.

More details about MVTrack dataset are provided in the Appendix.

