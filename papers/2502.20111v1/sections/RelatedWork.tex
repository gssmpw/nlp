\section{Related Work}

\input{tables/dataset/benchmark}
\subsection{Visual Object Tracking}
Visual object tracking has garnered significant research interest, leading to many breakthroughs. Numerous single-view datasets \cite{OTB2015, kiani2017need, kristan2016novel, muller2018trackingnet, fan2019lasot, huang2019got, wang2021towards, hu2022global, peng2024vasttrack} span a wide range of categories, aimed at enhancing models' ability to track arbitrary objects. With the expansion of these datasets, single-view tracking methods have also advanced rapidly. 
Early approaches based on Siamese networks \cite{li2018high, danelljan2020probabilistic} use CNNs to extract features from reference and search regions, establishing a linear relationship between them. 
More recent works have incorporated transformers for enhanced feature extraction \cite{cui2022mixformer, ye2022joint}, while others introduce attention modules to enable nonlinear relationships \cite{chen2021transformer}. 
However, these methods lack temporal continuity as they process each frame independently.
Algorithms like dynamic template updating \cite{chen2023seqtrack} and spatio-temporal trajectory tracking \cite{wei2023autoregressive, zheng2024odtrack} have shown promising results in addressing this issue.
Despite these advancements, recovering from target loss remains a significant challenge.

To re-track the target after a tracking failure, RTracker \cite{huang2024rtracker} leverages a tree-structured memory system to detect target loss and a dedicated detector for self-recovery. However, this approach is constrained by its complex design and the detector's reliance on specific categories.
Single-view tracking suffers from inherent limitations due to its restricted field of view, which is an inevitable challenge. In contrast, GMT \cite{wu2020visual} incorporates multi-view tracking within a single-view training framework. This limits its capacity to effectively model the intricate relationships between multi-view appearances and background contexts in the real world.



\subsection{Multi-View Object Tracking}
MVOT provides more comprehensive information about the target, effectively addressing issues such as occlusion. 
To leverage multi-view information, various fusion strategies have been developed for target association across viewpoints.
Some approaches establish multi-view relationships by projecting detection results onto a BEV plane \cite{xu2016multi}. 
However, this method is prone to detection errors, especially with occlusion.
% To tackle this issue, MVDet \cite{hou2020multiview} improves it by incorporating multi-view information at the detection stage (known as early fusion) through feature projection onto the ground plane, enabling the model to capture richer interaction information across views. 
To tackle this issue, methods \cite{hou2020multiview, hou2021multiview} improve it by incorporating multi-view information at the detection stage (known as early fusion) through feature projection onto the ground plane, enabling the model to capture richer interaction information across views. 
% Building on this approach, Simple-BEV \cite{harley2023simple} maps features into 3D space at multiple heights, which reduces distortions caused by ground plane projection.
Building on these approaches, methods such as \cite{song2021stacked, harley2023simple, teepe2024earlybird} map features into 3D space at multiple heights, which reduces distortions caused by ground plane projection.

While these methods enhance multi-view tracking, existing multi-view datasets are relatively scarce and often limited to specific target categories, such as pedestrians \cite{chavdarova2018wildtrack, han2023mmptrack, hao2024divotrack}. 
This results in a reliance on detection outcomes, limiting the ability to track arbitrary objects. 
GMTD \cite{wu2020visual} expands the target to multiple categories, but its scale remains small, primarily designed for evaluation purposes. There is a pressing need for a multi-view tracking dataset capable of handling arbitrary objects.