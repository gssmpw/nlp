\section{Results \& Discussion}
\label{sec:results}


\noindent
We evaluate the performance characteristics of BHM~\cite{senanayake2017bayesian} and VSA-OGM~\cite{snyder2024brain} across two distinct RL environments known as \textit{MarsExplorer}~\cite{Koutras2021MarsExplorer} and \textit{RaceCarGym}~\cite{Brunnbauer_racecar_gym}. All RL models are trained using the open-source Stable-Baselines framework~\cite{stable-baselines3} with Open-AI Gym~\cite{1606.01540} and Farama's Gymnasium~\cite{towers2024gymnasiumstandardinterfacereinforcement}. We train models and perform all timing analysis using an NVIDIA DGX compute cluster with 40GB A100 GPUs. It should be noted all experiments are performed on a slice of a single DGX machine with one A100 GPU.

% --------------------------------------------------
% Unknown Environment Exploration with MarsExplorer
% --------------------------------------------------
% \noindent
\textbf{Environment Exploration with \textit{MarsExplorer}:} The \textit{MarsExplorer} environment tasks autonomous agents with exploring unknown environments. We create three environments with varying sizes to increase the difficulty of learning the entire environment. These levels are known as Level 1, Level 5, and Level 10 with shapes [20x20], [30x30], and [40x40] respectively. All levels maintain the same reward coefficients as described in Section~\ref{sec:methods}.

Taking inspiration from the default observation space within \textit{MarsExplorer}, we scale the resulting occupancy grid maps from both BHM~\cite{senanayake2017bayesian} and VSA-OGM~\cite{snyder2024brain} to between 0 and 0.5 with the voxel corresponding to the agent's location having a value of 1.0. 
We chose to scale the resulting occupancy values between a subset of the range and set the location value outside of this range. This process avoids the possibility of an intermediate value being confused with the agent's location.
\textit{MarsExplorer} uses a one-to-one mapping between the number of grid cells and the occupancy grid so the resulting OGM's from both methods have the shape $\mathcal{G}$.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/vsa_vs_sbhm_generalization-mars.png}
    \caption{The generalization capabilities of policy networks trained with VSA-OGM~\cite{snyder2024brain} and BHM~\cite{senanayake2017bayesian} on unseen map layouts within the \textit{MarsExplorer} environment~\cite{Koutras2021MarsExplorer}.}
    \label{fig:mars-generalization}
\end{figure}

\begingroup
\renewcommand{\arraystretch}{1.2}
\begin{table}[]
\centering
\caption{Training results with Proximal Policy Optimization~\cite{schulman2017proximalpolicyoptimizationalgorithms} on different levels of the MarsExplorer~\cite{Koutras2021MarsExplorer} environment}
\label{tab:mars-results}
\begin{tabular}{cccc}
\toprule
\textbf{Algorithm} & \textbf{Level} & \textbf{Mean Reward} & \textbf{OGM Latency} \\
\midrule
                                  & 1     &  117.02  & \textcolor{black}{$0.75\pm0.06\text{ms}$}        \\
BHM~\cite{senanayake2017bayesian} & 5     &  230.25  & \textcolor{black}{$0.76\pm0.05\text{ms}$}        \\
                                  & 10    &  180.80   & \textcolor{black}{$0.93\pm0.04\text{ms}$}        \\
\midrule
                           & 1     & 119.10  & \textcolor{black}{$1.76\pm0.32\text{ms}$}        \\
VSA-OGM~\cite{snyder2024brain} & 5     & 232.93  & \textcolor{black}{$1.73\pm0.31\text{ms}$}        \\
                           & 10    & 184.35   & \textcolor{black}{$1.70\pm0.33\text{ms}$}        \\ 
\bottomrule
\end{tabular}
\end{table}
\endgroup

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/racecar-ppo-results.png}
    \caption{The qualitative results of training a multi-headed policy network with Proximal Policy Optimization~\cite{schulman2017proximalpolicyoptimizationalgorithms} for multiple tracks in \textit{RaceCarGym}~\cite{Brunnbauer_racecar_gym} with VSA-OGM~\cite{snyder2024brain} and BHM~\cite{senanayake2017bayesian}.}
    \label{fig:racar-results}
\end{figure*}





% Both OGM methods are configured following the guidelines specified in their \hl{respective articles}.
BHM is configured with a bandwidth of 6, while VSA-OGM is configured with a vector dimensionality of 4096, a length scale of 3, and 4 tiles per dimension. In both cases, the length scale and bandwidth parameters control the fidelity and smoothness of the map with respect to map size. The vector dimensionality and number of tiles are unique to VSA-OGM and allow the algorithm to scale based on problem requirements.
% \hl{These parameters were chosen to make each algorithm as equal as possible to highlight their unique computational characteristics}.
The policy network utilizes a convolutional feature extractor with a multi-layer perception (MLP).

The convolutional feature extractor receives the modified OGM and passes it through two convolutional layers. The first layer returns 8 features with a kernel size of 3 and a stride of 2. The second convolutional layer accepts the 8 features and returns 16 features with a kernel size of 2 and a stride of 2. The extracted features are flattened and passed through a linear layer returning 256 features. These features are then passed through a MLP using 4 hidden layers with 64 hidden neurons and ReLU activations. To introduce more variability in the training process, we adjust the decay factor and learning rate with unique values (.95, .99, .999) and (.0003, .000198, .000099), respectively. We train our model with Proximal Policy Optimization (PPO)~\cite{schulman2017proximalpolicyoptimizationalgorithms} and each parameter combination, for 100000 time steps, leading to nine training configurations per OGM algorithm and level combination. The averaged performance of these nine combinations are shown in Table~\ref{tab:mars-results} and Figure~\ref{fig:mars-results}. All timing analysis is performed with PyTorch~\cite{paszke2019pytorchimperativestylehighperformance}, all tensors pre-loaded into GPU memory, and include synchronization between the CPU and GPU.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/vsa_vs_sbhm_generalization-racecar.png}
    \caption{The generalizability of trained multi-headed policy networks when trained and evaluated on multiple maps. Austria, Berlin, and Treitlstrasse were used for training with all remaining maps being used for evaluation. All results are averaged over 5 evaluations.}
    \label{fig:generalization-results}
\end{figure*}

As shown in Table~\ref{tab:mars-results}, VSA-OGM performs slightly better in terms of mean reward at the cost of 2.33x increased latency per time step.
% The approximate 2.33x increase in latency versus BHM highlights the \hl{unique complexity scaling} of VSA-OGM \hl{where the baseline computational complexity is higher for smaller problems than required by BHM}. \hl{These results directly parallel existing theoretical works} in probabilistic hyperdimensional computation where improvements in computational complexity arise as problem complexity increases~\cite{furlong2022fractional}.
The final size of the learned memory with VSA-OGM was 0.52MB with BHM having a model size of 0.02MB on the largest map (Level 10). This increase in model size is expected as the GPU compatible version of BHM assumes independence between voxels and doesn't maintain the full covariance matrix~\cite{snyder2024brain, senanayake2017bayesian}. 

\textbf{\textit{Generalizability.}} As shown in Figure~\ref{fig:mars-generalization}, we evaluate the generalizability of all trained policy networks on each level with random obstacle layouts. We evaluate each parameter combination across both OGM methods and 25 levels of the same size but with different obstacle layouts for a total of 2250 evaluations. Our results show that policies trained with VSA-OGM exhibit an approximately 52\% increase in generalizability (measured by the percent increase in reward) across all evaluations. This suggests that policy networks trained with VSA-OGM provide policy networks with adequate information to perform the tasks while also not allowing the policy to over fit and suffer on unknown environments. It should be noted that the increased variance with VSA-OGM is expected as PPO is very sensitive to parameter configurations so many of the configurations fail to generalize. Therefore, the increased variance is caused by the high performance ceiling provided by policy networks trained with VSA-OGM.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figs/maps.png}
    \caption{The training maps used within the \textit{RaceCarGym} environment.}
    \label{fig:maps}
    \vspace{-10pt}
\end{figure}

% --------------------------------------------------
% Path Planning with Race Car Gym
% --------------------------------------------------
\textbf{Autonomous Driving with \textit{RaceCarGym}:} We evaluate the performance attributes of VSA-OGM for autonomous driving with the \textit{RaceCarGym}~\cite{Brunnbauer_racecar_gym} reinforcement learning environment. We utilized three of the tracks (\textit{Berlin}, \textit{Austria}, and \textit{Treitlstrasse}). \textit{Berlin} is the easiest track with multiple gradual corners with \textit{Austria} having more short radius (closer to $90^\circ$) corners. \textit{Treitlstrasse} is the most difficult with extremely sharp turns through narrow corridors. We train all model configurations individually on each track. A visualization of the three training maps is shown in Figure~\ref{fig:maps}.



\begingroup
\renewcommand{\arraystretch}{1.2}
\begin{table}[]
\centering
\caption{Training results with Proximal Policy Optimization~\cite{schulman2017proximalpolicyoptimizationalgorithms} on different tracks of the RaceCarGym~\cite{Brunnbauer_racecar_gym} environment}
\label{tab:racecar-results}
\begin{tabular}{cccc}
\toprule
\textbf{Algorithm} & \textbf{Track} & \textbf{Mean Reward} & \textbf{OGM Latency} \\
\midrule
                                  & Berlin       & 197.99  & $3.39\pm1.27\text{ms}$       \\
BHM~\cite{senanayake2017bayesian} & Austria      & 334.86  & $2.97\pm0.51\text{ms}$         \\
                                  & Treitlstasse & 121.42  & $2.03\pm0.29\text{ms}$         \\
\midrule
                               & Berlin       & 214.36  & \textcolor{black}{$198.83\pm0.52\text{ms}$}  \\
VSA-OGM~\cite{snyder2024brain} & Austria      & 297.31  & \textcolor{black}{$198.73\pm0.68\text{ms}$}  \\
                               & Treitlstasse & 90.62   & \textcolor{black}{$198.02\pm0.94\text{ms}$}  \\ 
\bottomrule
\end{tabular}
\end{table}
\endgroup



\textit{RaceCarGym}'s default observation space doesn't utilize any form of OGM so we augmented the default observation space to include the OGM as a multiple input observation space. The policy network is a multi-headed MLP receiving LiDAR, pose, velocity, and the OGMs from VSA-OGM~\cite{snyder2024brain} and BHM~\cite{senanayake2017bayesian}. Similar to the \textit{MarsExplorer} experiments, the MLP has 4 hidden layers with 64 hidden neurons using ReLU activations along with the augmented OGM's giving the network greater positional awareness with respect to the map. We limit the resolution of each voxel to a 1 meter by 1 meter area. This is much lower than existing literature on OGM methods~\cite{wilson2022convolutional} because the open-source implementation of BHM runs out of GPU memory when querying the learned model to extract denser occupancy grids.

BHM is configured with a kernel length scale of 6, while VSA-OGM is configured with a vector dimensionality of 10000, a binding length scale of 1, and 8 tiles per dimension. We increase the vector dimensionality and number of tiles per dimensional for all \textit{RaceCarGym} experiments to increase the capacity of VSA-OGM with respect to the increased point cloud densities. In both cases, the length scale parameters control the fidelity of the map with respect to map size.
% These parameters were chosen to make each algorithm \hl{as equal as possible} to highlight their unique computational characteristics. 

We utilize the same training process as described for the \textit{MarsExplorer} experiments leading to nine training configurations per OGM algorithm and level combination. Due to the increased complexity of the environment, we increase the number of training steps to 300000. These training results are averaged and form the results presented in Table~\ref{tab:racecar-results} and Figure~\ref{fig:racar-results}.

As shown in Table~\ref{tab:racecar-results}, VSA-OGM stays within 12\% of BHM's mean reward on all tracks with the exception of \textit{Treitlstrasse} at a 33\% drop. However, the mean reward plots suggest that the convergence with VSA-OGM, even on \textit{Treitlstrasse}, was slower than BHM. 
% We shown in our final experiment that this slower convergence This suggests that VSA-OGM is providing the downstream policy network with adequate information to learn a more generalized policy more complex environments} although the \hl{holographic nature}, the noise from combining multiple vectors into a fixed-length representation, of VSA-OGM is slowing down the rate of convergence. We investigate the downstream impact of this increased noise and slower convergence speed 
The final memory size of the learned representation with VSA-OGM was 5.12MB with the final BHM method having a model size of approximately 0.01MB depending upon the map size. As previously stated, this increase in parameters is expected as the GPU version of BHM assumes independence between cells and doesn't maintain the full covariance matrix. Compared to the previous \textit{MarsExplorer} experiments, we see that VSA-OGM's latency is approximately 2 orders of magnitude worse than BHM. While the size of the maps is similar between the environments, \textit{RaceCarGym} uses approximately 2 orders of magnitude more points per individual LIDAR scan. This suggests that the encoding operation of higher density point clouds is hindering the latency of VSA-OGM in this experiment.

\textbf{\textit{Generalizability.}} In our final experiment, we perform multi-map training and evaluation where policy networks are sequentially trained across three maps (\textit{Berlin}, \textit{Austria}, and \textit{Treitlstrasse}) for 300000 time steps each. As in all previous experiments, we perform training for each OGM method and all nine parameter combinations. As shown in Figure~\ref{fig:generalization-results}, we evaluate each of these trained models across all training tracks and nine unseen evaluation tracks. These results are averaged across five random seeds and initial conditions. The major takeaway from these results is that the policy networks trained with VSA-OGM present no decrease in model generalizability to unseen environments and even improve generalizability across all evaluation tracks. Therefore, models trained with VSA-OGM maintain similar reward curves during training and present increased generalization in unseen environments at the cost of increased computational complexity.

% \hl{MP: What happens in cicrle track?! or VSA-OGM in Columbia track? Don't you want to remove these two? it is confusing? or why stochasticity is higher in general in VSA? except columbia}


% % --------------------------------------------------
% % Scaling to higher density maps
% % --------------------------------------------------
% % \noindent
% \textbf{Scaling to Higher Map Densities:} We evaluate the performance attributes of VSA-OGM for autonomous driving with the \textit{RaceCarGym}~\cite{Brunnbauer_racecar_gym} reinforcement learning environment. \lipsum[1] \lipsum[2]