\section{Reinforcement Learning with OGMs}
\label{sec:methods}


% \hl{MP: This is a very nice section. Not much comments on my end. It would be nice to publicly release all codes that facilitates the bridge between RL and different OGM tecniques. I suggest putting a footnote at the first page that codes will be publicly released..}

\noindent
% OGMs directly integrate with RL by \hl{serving as a critical component of the agent's observation space}. Despite \hl{this natural integration} capability, the majority of RL environments lack support for directly creating OGMs from the environment. 
Most OGM frameworks, such as BHM~\cite{senanayake2017bayesian} and VSA-OGM~\cite{snyder2024brain}, require a different data format versus the native LiDAR output of many reinforcement learning environments. OGM methods require formalized training data similar to the feature and target format defined in Scikit-Learn~\cite{pedregosa2011scikit}, where $X$ represents the data feature matrix with shape $\gamma_n \times 2$, where $\gamma_n$ is equal to the number of ray-casts in a LiDAR point cloud $\gamma$ and other 2 dimensions represent a Cartesian coordinate. Likewise, $y$ is a $\gamma_n\times1$ vector representing the labels of each point where $0$ signifies empty and $1$ signifies occupied.
Bridging this gap and extracting OGM training data from LiDAR-based RL environments requires the end user to augment baseline environments with a wrapping layer transforming environmental information into an OGM compatible format. In this section, we describe the RL environments utilized throughout all experiments and the environmental wrappers augmenting their baseline input and output structure into an OGM-friendly format.

\begingroup
\renewcommand{\arraystretch}{1.2}
\begin{table}[h]
\begin{center}
\caption{The mathematical symbols used throughout this paper.} 
\label{tab:symbol-table}  
\begin{tabular}{c|c}
\textbf{Symbol}   & \textbf{Meaning}                         \\ \hline
$\mathcal{G}$     & square grid of cells                     \\
$g$               & an individual grid cell in $\mathcal{G}$ \\
$g_{loc}$         & the agent's location in $\mathcal{G}$    \\
$N$               & the number of cells in $\mathcal{G}$     \\
$M$               & the number of ray-cast interpolations    \\
$\mathcal{M}$     & ground truth map of $\mathcal{G}$ cells  \\
$\gamma$          & a point cloud in polar coordinates       \\
$\gamma_n$        & the number of ray-casts in $\gamma$      \\
$\gamma_{max}$    & maximum cast distance in meters          \\
$\delta$          & normalized steering angle                \\
$\tau$            & normalized motor speed                   \\
$t$               & time                                     \\
$r$               & reward value                             \\
$r_{bonus}$       & reward for target map exploration        \\
$r_{explore}$     & reward for exploring new areas           \\
$r_{invalid}$     & penalty for invalid movement             \\
$r_{move}$        & penalty for movement                     \\
$r_{obstacle}$    & penalty for hitting an obstacle          \\
$r_{steer}$       & penalty for steering magnitude           \\
$r_{velocity}$    & reward for maintaining target velocity   \\
$r_{collision}$   & penalty for collisions                   \\
$v$               & velocity in meters per second            \\
$X$               & Cartesian ray-casts from $\gamma$        \\
$X_{int}$         & interpolated ray-casts from $\gamma$     \\
$y$               & occupancy label vector for $X$           \\
$y_{int}$         & occupancy label vector for $X_{int}$     \\
$\theta$          & angle in radians                         \\
$\Theta$          & angle vector for each ray-cast           \\
$\Delta\Theta$    & step-size between individual ray-casts   \\
$\lambda_x$       & reward scaling parameter                 \\
$T$               & orientation matrix                       \\


\end{tabular}
\end{center}
\end{table}
\endgroup

\textbf{\textit{Mars Explorer (MarsExplorer)}}~\cite{Koutras2021MarsExplorer}: \textit{MarsExplorer}~\cite{Koutras2021MarsExplorer} is an open-source Open-AI Gym~\cite{gym} compatible environment designed to train agents to explore randomized unknown environments. These environments are constrained to be a square grid $\mathcal{G}$ of $N=rows\times cols$ cells defined as:
\begin{equation}
    \mathcal{G} = \{(x, y): x\in[1,rows],y\in[1, cols] \}.
\end{equation}
The ground truth map of a scenario $\mathcal{M}$ is defined as a mapping from each individual grid cell $\mathcal{M}(g)$ such that:
\begin{equation}
    \mathcal{M}(g) = 
        \begin{cases} 
        0.3 & \text{empty} \\
        1 & \text{occupied} \\
        \end{cases} \hspace{8pt} g = (x, y) \in \mathcal{G}.
\end{equation}

The default observation space in \textit{MarsExplorer} is a real-valued matrix, with shape $\mathcal{G}$, indicating whether each cell is occupied or empty. The field of view is determined by propagating circular ray-casts uniformly distributed across the unit circle. These ray-casts simulate a LIDAR sensor, generating a point cloud vector $\gamma$ in polar coordinates with shape $\gamma_n$, where $\gamma_n$ represents the number of ray-casts in the point cloud. We utilized the default $\gamma_n=32$ provided within \textit{MarsExplorer}. The observation matrix is then updated by marking all intersected cells as empty ($g=0.3$) and terminal cells (those where a ray-cast terminates before the max cast distance) as occupied ($g=1.0$). Lastly, the agent's position $loc$ is marked by setting the corresponding cell to an intermediate value where $g_{loc}=0.6$. The action space for \textit{MarsExplorer} is discrete with 4 options representing moving one cell up, down, left, or right.

The reward $r$ is defined as a piecewise function consisting of four parts: $r_{explore}$, $r_{move}$, $r_{invalid}$, and $r_{bonus}$. With agents having the ultimate goal of exploring the entire environment, $r_{explore}$ is defined as the number of newly explored cells from $t-1$ to $t$. Therefore, we can also conclude that the entire grid has been explored when $\sum_{k=0}^Tr_{explore}(k) \rightarrow N$. To encourage the agent to use fewer movements to observe the environment, a fixed penalty $r_{move}$ of 0.5 is applied at every time step. $r_{bonus}$ serves as an encouragement bonus where the agent receives an increased reward of 100 if 95\% or more of the environment has been explored. Lastly, is $r_{invalid}$ which overrides the other pieces if activated. $r_{invalid}$ represents a fixed penalty of -100 if the agent hits an obstacle or tries to move out of bounds.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/lidar_clouds.png}
    \caption{The process of transforming the baseline observation spaces with polar-coordinate LIDAR scans to Cartesian coordinates located and orientated on the global reference frame. (A) The first and last LIDAR scans in polar coordinates. (B) The scans transformed into Cartesian coordinates. (C) The point clouds oriented based on the agent's location and orientation.}
    \label{fig:data-transformation}
\end{figure*}

\textbf{\textit{Racecar Gym (RaceCarGym)}}~\cite{Brunnbauer_racecar_gym}: Inspired by the F1-Tenth driving challenge, \textit{RaceCarGym} tasks agent's with the goal of autonomous driving around flat models of real world racetracks. This environment is developed with PyBullet~\cite{coumans2021} to implement vehicle physics with a differential drive kinematics model, steering on the front wheels, and a ray-casting-based LIDAR sensor. All experiments in this work utilize the default physics parameters. The vehicle is equipped with a 2D LIDAR sensor returning point clouds $\gamma$ with $\gamma_n=1080$ beams across a 273.5$^{\circ}$ field of view. Moreover, the vehicle model also includes pose, velocity, and acceleration sensors with configurable noise. To isolate OGM performance from the variability introduced by SLAM algorithms, we disable all position sensor noise. Additionally, ray-cast measurements are subjected to uniformly distributed noise, constrained within $\pm5\%$ of the true cast length, to simulate realistic sensor imperfections. Controlling the vehicle are two actuators adjusting the steering angle and motor speed. The default observation space is the point cloud $\gamma$ and the corresponding pose and velocity information. Likewise, the action space is a real-valued vector, normalized between -1 and 1, representing the steering angle $\delta$ and motor speed $\tau$.

We create a custom reward function to encourage the agent to maintain a high velocity throughout the tracks whilst minimizing the amount of steering input, the distance to any obstacles, and avoiding collisions. This process results in a final reward value $r$ consisting of four distinct components: $r_{velocity}$, $r_{steering}$, $r_{obstacle}$, and $r_{collision}$. $r_{velocity}$ is designed to encourage the agent to maintain high velocities throughout each timestep such that:
\begin{equation}
    r_{velocity} = \begin{cases}
        \frac{v}{\lambda_0}, & v > \lambda_1 \\
        -1, & \text{else}
    \end{cases}
\end{equation}
where $v$ is the cumulative magnitude of velocities across all dimensions. $\lambda_{0}$ and $\lambda_{1}$ are heuristically chosen to be 3.5 and 0.1 respectively. $r_{steering}$ penalizes very large steering inputs to avoid jerk and is defined as
\begin{equation}
    r_{steering} = \begin{cases}
        0.1, & |\frac{\delta}{\lambda_3}| < \lambda_4 \\
        -\frac{\delta}{\lambda_2}, & else
    \end{cases}
\end{equation}
$\lambda_{2}$, $\lambda_{3}$, and $\lambda_{4}$ are heuristically chosen to be 15, 5, and 0.08, respectively. $r_{obstacle}$ penalizes the agent for being too close to any given hazard and is calculated by subtracting 0.4 from the distance to the closest obstacle. $r_{collision}$ is a fixed penalty of 100 that is applied if the agent hits any obstacle during the course of an episode. Bringing everything together, the final reward value $r$ is calculated as:
\begin{equation}
    r=r_{velocity}+r_{steering}+r_{obstacle}+r_{collision}.
\end{equation}



\textbf{\textit{Environment Wrapping}}: To transform polar coordinate based LIDAR scans to Cartesian space, we create a theta vector $\Theta$ with shape $\gamma_n$ representing the degree of rotation about the unit circle with respect to each ray-cast. For the \textit{MarsExplorer} environment with a 360$^\circ$ field of view and $\gamma_n$ ray-casts, $\theta$ will have a discrete radian step size of $\Delta\Theta=\frac{2\pi}{\gamma_n}$ such that $\Theta=\{0 * \Delta\Theta, 1 * \Delta\Theta, ..., (\gamma_n -1) * \Delta\Theta\}$. Conversely with the \textit{RaceCarGym} having a field of view of 273.5$^{\circ}$, it has a different step size of $\Delta\Theta=\frac{273.5*2\pi}{365*\gamma_n}$ radians.

\begin{algorithm}
    \caption{Polar to Cartesian Coordinates}\label{alg:polar2cartesian}
    \begin{algorithmic}
        \REQUIRE $\gamma_n\gets\text{number of ray-casts}$
        \REQUIRE $\gamma \gets \text{LIDAR scan with } \gamma_n \text{ ray-casts}$
        \REQUIRE $\Theta \gets \text{angle of each ray-cast in radians}$
        \STATE $X\in \mathbb{R}^{N\times 2}$
        \STATE $i \gets 0$
        \WHILE{$i < \gamma_n $}{
            \STATE $X[i, 0] \gets \gamma[i]\sin(\Theta[i])$
            \STATE $X[i, 1] \gets \gamma[i]\cos(\Theta[i])$
            \STATE $i = i + 1$
        }
        \ENDWHILE
        \RETURN $X$
    \end{algorithmic}
\end{algorithm}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/mars-explorer-results.png}
    \caption{The qualitative results of training a convolutional policy network with Proximal Policy Optimization~\cite{schulman2017proximalpolicyoptimizationalgorithms} for multiple levels in \textit{MarsExplorer}~\cite{Koutras2021MarsExplorer} with VSA-OGM~\cite{snyder2024brain} and BHM~\cite{senanayake2017bayesian}.}
    \label{fig:mars-results}
\end{figure*}

Following Algorithm~\ref{alg:polar2cartesian}, we convert each ray-cast length in $\gamma$ and each ray-cast angle in $\Theta$ to a Cartesian coordinate resulting in a matrix $X\in \mathbb{R}^{\gamma_n \times 2}$. We create a corresponding output matrix $y$ with shape $\gamma_n \times1$ and all values defaulting to 1. To account for ray-casts that do not intersect with an obstacle and return the maximum cast distance $\gamma_{max}$, we employ element-wise conditioning and assignment to set all $y$ values, where indices that match $\gamma[i]=\gamma_{max}$ are set to 0 signifying that the ray-cast corresponds to an empty location.

To avoid under-sampling free-space within the environment, we employ linear interpolation to generate $M$ points along each individual ray-cast. This process generates a matrix $X_{int}$ with $M$ evenly distributed points between 0 and the length of the i-th raycast $\gamma[i]$ resulting in a final matrix shape of $M\gamma_n\times2$. A corresponding target vector $y_{int}$, with shape $M\gamma_n$, is initialized with zeros signifying that all points in $X_{int}$ are classified as empty. We concatenate $X$ and $X_{int}$ along with $y$ and $y_{int}$ resulting in vector-matrices with shape $(M+1)\gamma_n\times2$ and $(M+1)\gamma_n$, respectively. We specify $M=1$ and $M=20$ for all \textit{MarsExplorer} and \textit{RaceCarGym} experiments, respectively. These values were empirically chosen based on the environmental complexity. The last step in our data preprocessing is creating a geometric transformation matrix $T$ to translate and rotate the Cartesian coordinates to the global reference frame using the agent's pose. More information on this process can be found in~\cite{cullen2012matrices}. A visualization of the end-to-end data transformation process is shown in Figure~\ref{fig:data-transformation}.

% \hl{MP: It would be nice to increase the font size of the figures legend and axis numbers or make them bold. it's hard to see the numbers or text.}
