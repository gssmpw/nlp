\subsection{\matching Task Results}
\label{subsec:match-results}

%\subsubsection{\linker Model}

Table~\ref{tab:rq1-lnk} reports the result of the \linkerConfigsLetter experimented configurations of the \linker model along with the best variants of the two flavors of \simil technique.
%
The best configuration was the one that augmented the training set with \textsc{JavaTransformer}~\cite{rabin:ist2021:javatransformer} and did not use the CWE information to describe the vulnerability, but only the summary description, achieving a $0.57$ F0.5 score and $0.71$ precision (highlighted in yellow).
Despite having a lower F0.5 score than the highest achieved by the \finder model in the \finding task, the precision value was still in an acceptable range.
The factor that lowered the F0.5 was the recall of $0.31$.
Despite this, this configuration achieved a very high AUC-ROC score of $0.89$---a trait shared with all the other configurations except one.

Similarly to what happened with the \finder model, the training data augmentation had noticeable effects in terms of precision, observing that \textsc{JavaTransformer} always led to an increase.
%\TEMP{(i) augmenting with \textsc{JavaTransformer} always reduced false positives,
%(ii) when the CWE was present, SPAT induced the model to have a greater recall, achieving the highest score of $0.52$.}
On the other hand, the presence of CWE did not cause noteworthy differences in all metrics, making the two groups comparable; yet, we cannot conclude the presence of absence is either beneficial or detrimental.
In any case, we observed that the recall was difficult to increase, just like the \finder model.
%irrespective of the augmentation or the presence of CWE.
%
Between the two flavors of \simil, only \yakeSimil managed to have some true positives without making many false positive classifications.
Still, all \linker's configurations behaved better than both baselines.
On the contrary, the best variant of the \codebertSimil approach ended up behaving like a constant classifier, flagging class \linkerPosClass in almost all the cases. 
%On the other hand, we observed that all configurations could not score a recall higher than $0.5$---achieved by the best configuration.
%This is the only metric that is slightly outperformed by a baseline technique, i.e., \TEMP{Identifier-Vocab}. However, this was only due to the one extra true positive classification at the cost of 288 false positives, which makes this technique unsuitable for this task.

The lower performance of the \linker could be ascribed to the implications of its simplified task, which limited the diversity in its training set (especially when compared to the \finder's training set, seen in Section~\ref{subsec:invitro}).
Considering the difficulty of learning relationships between two textual inputs, the \linker model could only marginally understand the task.
%(i) the amount of data to process, as the \linker must process two different types of sources, i.e., the test code and the vulnerability description.
%Second, the size of the \linker's training set is noticeably smaller than \finder's (see Section~\ref{subsec:invitro}).
%Both aspects could be the reasons that made the linking task more challenging than the retrieval task.
Nevertheless, the sufficient precision and the high AUC-ROC score let us consider the \linker model ready for integration with the \finder model to address the \matching task.

%\subsubsection{Integrated Model}

Table~\ref{tab:rq1-glb} reports the result of the \globalConfigs experimented configurations of the integrated model along with the \fixCommits technique.
%
The best configuration was the one that ran a pre-training of both \finder and \linker models in their specific tasks (using their best configurations seen in Tables~\ref{tab:rq1-fnd} and~\ref{tab:rq1-lnk}) and then ran a fine-tuning for the \matching task without augmentation (highlighted in yellow in Table~\ref{tab:rq1-glb}).
%using the training data augmented with SPAT~\cite{yu:jss2022:spat} .
This configuration achieved a high precision of $0.86$, a good F0.5 score of $0.68$, and a greatly high AUC-ROC score of $0.91$---surpassed by two other configurations by a small margin.
%Despite having the same F0.5 score as the configuration trained without data augmentation, we preferred the one trained with SPAT due to its higher precision.
%\EI{Should we say that the \finder threshold was set to 0.5? Actually, we experimented with both 0.5 and the best one seen in Table~\ref{tab:rq1-fnd}, but 0.5 was the best. In any case, the \finder model is the SAME! Maybe this will confuse the reviewer and focus on this point too much. Maybe we can just omit this.}

\input{tables/rq1-lnk-new}

The main factor that affected the performance of the integrated model was \glbMode, indicating how the \matching task was addressed through the \finder and \linker models.
The setting that used the \linker model directly for the \matching task (i.e., without the \finder's support) achieved $0.70$ precision and $0.63$ F0.5 score without augmented training data.
Such results are better than the best \linker model on the simplified task.
%
After switching to the joint use of \finder and \linker without pre-training (\textsl{FT-Only}), we observed an interesting fact.
When the training data was augmented, there were no noteworthy differences; however, the configuration without data augmentation made no positive classifications at all.
%
At this point, we employed the pre-training for both models independently but removed the fine-tuning (\textsl{PT-Only}), obtaining a noticeable improvement in precision, reaching $0.80$, though with a drop in recall ($0.25$).
Lastly, we introduced the fine-tuning back (\textsl{PT-FT}), achieving a $7.5\%$ and $52\%$ boost in precision and recall, respectively, when no augmentation is used.
This confirms the benefit of employing two models for the \matching task with a two-stage training.
%In the end, the training data augmentation is largely detrimental.

\input{tables/rq1-glb-new}

Just like the \finder and \linker models, the recall score was generally low; this further supports the fact that balancing precision and recall is challenging in all the tasks addressed in this work. 
The heuristic approach \fixCommits, on the other hand, achieved a higher recall of $0.63$.
Nevertheless, its precision and F0.5 were still lower than those achieved by the best integrated model.
Indeed, \vuteco achieved $62\%$ and $26\%$ more precision and F0.5, respectively, than \fixCommits.
Given our preference for precise solutions, the integrated model of \vuteco was preferred over the \fixCommits approach.

At this point, we re-trained the integrated model with the best configuration and ran it on the set of \Java projects in the wild to match the test cases with the vulnerabilities they witnessed.
The model returned \invivoMatchingResults matches from \invivoMatchingResultsProjects projects (\invivoMatchingResultsProjectsPerc of the analyzed ones).
The inspectors then validated all those findings (spending three minutes per entry on average), agreeing on \invivoMatchingAgreement (\invivoMatchingAgreementPerc) cases.
Afterward, they jointly inspected the seven remaining cases where they had conflicting judgments until reaching a common decision.
At the end of the inspection process, the inspector affirmed that none of the matches were valid---due to this, the inspectors never agreed on the presence of valid matches, making the resulting Kappa score insignificant despite the two inspectors agreeing in almost all cases.

The lack of valid matches did not meet the expectations set during the in-vitro validation.
Unfortunately, \vuteco is not ready yet for matching test cases and vulnerabilities in the wild.
%
Nevertheless, we recognized that this task is challenging, apparently much more than the one on \VulforJ---despite both contexts comprising real-world \Java projects.
Hence, we shed more light on these results by asking the two inspectors to make a fine-grained inspection of all the matches to see whether the test cases were, at least, security tests and whether they were matched with a similar vulnerability.
They found that all but one test cases were concerned with security issues.
This accessory behavior provides an alternative way to find security-related tests with a different principle than the one used by the \finder model alone. 
Besides, in \emph{nine} cases, we found that the matched vulnerabilities are similar to the correct ones---in terms of vulnerability type or description.
For instance, in \textsc{Dropwizard}, the test case \texttt{shouldNotBeVulnerableToCVE\_2022\_42889} was matched with CVE-2020-5245, which is an injection vulnerability that opens to remote executions akin to CVE-2022-42889 (the intended one)~\cite{dropwizard:example}.
Hence, the integrated model demonstrated signs that the learning stage had some benefits, though insufficient.
Better modeling, as well as more examples of real matches, are needed to improve the generalizability of the \matching task.
%The reason is that we limit the list of CVEs that VUTECO can choose (from project KB)

\rqanswer{2}{
In the \matching task, \vuteco achieved $0.86$ precision and $0.68$ F0.5 score when tested on a held-out set from \VulforJ without the need for augmenting its training data.
It benefited from the joint use of \finder and \linker, first pre-trained on their tasks and then further fine-tuned for the \matching task.
Unfortunately, \vuteco failed to match test cases and vulnerabilities in the wild.
Nevertheless, all tests involved in the matches were also security-related, and in some cases, the matched vulnerability was not too dissimilar to the exact one.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \vuteco returned \resultsWitTests tests matched with \resultsVulns distinct vulnerabilities in \invivoProjectsWithWitTests (\invivoProjectsWithWitTestsPerc) projects.
% Among all tests returned, \resultsWitTestsSingleVuln (\resultsWitTestsSingleVulnPerc) were matched with one single vulnerability, which is in line with the expected behavior of unit tests focusing on one aspect at a time.
% In the remaining \resultsWitTestsMoreVulns cases, we observed no more than \resultsWitTestsMaxVulns vulnerabilities matched.
% Furthermore, \vuteco matched a median of \resultsMedianTestsPerVuln tests per vulnerability, for a total of \resultsMatches matches.
% These results show how \vuteco can reduce the huge search space by several orders of magnitude, flagging only \resultsWitTestsPerc of the tests as suspects.
% Thus, \vuteco reached its primary goal in simplifying the search of witnessing tests in large collections.

% The sample the two inspectors validated (using $0.8$ probability as cut-off) was made of \inspectedTests tests matched with \inspectedVulns distinct vulnerabilities, for a total of \inspectedMatches matches in \inspectedProjects projects.
% A complete pass on the sample took each inspector almost \inspectionTime hours, with an average of about \inspectionTimePerMatch minutes per match.
% The two inspectors agreed on \inspectionAgreements cases, scoring \inspectionKappa Kappa score, indicating \inspectionKappaWords inter-rater agreement.
% Afterward, they had a joint inspection round on the remaining \inspectionDisagreements (\inspectionDisagreementsPerc) cases where they had conflicting judgments until reaching a final joint decision.

% At the end of the inspection process, the number of valid matches (i.e., the times a test case was correctly matched to a given vulnerability) was \inspectedMatchesValid out of \inspectedMatches (\inspectedMatchesValidPerc).
% The matching capabilities of \vuteco did not perform like in \rqOne, as the number of false positive matches was expected to be close to zero.
% However, with such a huge search space (i.e., \invivoTests tests and \invivoVulns vulnerabilities), the probability of having false positives becomes high.
% For this, \rqTwo aimed to shed doubts about its practical performance, even if this would appear as a drop from the analyses in \rqOne---which only provided a partial view of the \vuteco's real capabilities.
% This manual inspection was only done for the \inspectedMatches matches having the highest probabilities; the remaining matches returned by \vuteco could still be valid.
% The full results can be found in the paper's replication package~\cite{appendix}.

% The somehow limited number of valid matches indicates the intrinsic difficulty of matching witnessing tests.
% Nevertheless, we further analyzed the valid matches marked by the inspectors and reflected on other facts.
% Namely, we asked to what extent \vuteco managed to identify security-related tests, even in case of incorrect matches.
% Hence, the same two inspectors were involved again to check which among the \inspectedTests test cases could be testing security-related aspects, irrespective of the correctness of the match with the vulnerability.
% This was much easier to assess, as this resulted in an \inspectionTestsKappaWords agreement, reaching \inspectionTestsKappa Kappa score.
% As a result, \inspectedTestsSecTests out of \inspectedTests (\inspectedTestsSecTestsPerc) test cases concerned security-related aspects, showing how \vuteco can be used for this secondary task.

% From the perspective of the vulnerabilities, we observed that \inspectedVulnsWithOneValidTest out of \inspectedVulns (more than half) had at least one witnessing test correctly matched---even if they might have been incorrectly matched with other test cases.
% This result marks the importance of \vuteco as a tool for \textbf{mapping unknown witnessing tests to vulnerabilities} that other matching mechanisms might miss.
% As a support to this, we ran the \fixCommits approach on the same set of \invivoProjects projects to check if it could match a valid witnessing test to these \inspectedVulnsWithOneValidTest vulnerabilities.
% Among the \matchesFixCommits matches made by \fixCommits, \testsNotFoundByFixCommits vulnerabilities out of \inspectedVulnsWithOneValidTest did not receive the right match.
% Therefore, considering only the inspected matches, \vuteco successfully matched a witnessing test in four times more vulnerabilities than the heuristic baseline approaches.

%\TEMP{
%For instance, in \texttt{netty}~\cite{netty}, two vulnerabilities, CVE-2019-16869, and CVE-2020-7238, consisted of the same issue concerning the improper handling of whitespace in HTTP headers. In particular, the latter was born after an incomplete fix of the former.
%The patch for the former added the test case \texttt{testWhitespace}. However, the patch for the latter did not modify this test as it was already valid. For this, \fixCommits could not catch this test, while \vuteco could in both cases.
%}
%\EI{If it doesn't take so much space, we can mention a validated case. Possibly, one that show that a witnessing test was added after the fix commit.}
%Two candidates: (1) CVE-2014-0112 affecting \textsc{Apache Struts} was fixed at revision \texttt{2e2da2}, but the witnessing test was added two commits later at revision \texttt{149181}. (2) CVE-2019-0225 of JSPWIKI was fixed at \texttt{88d89d65} but the test was added in the commit after.

% \rqanswer{2}{
% \vuteco matched \resultsVulns historical vulnerabilities to \resultsWitTests test cases in \invivoProjectsWithWitTests out of \invivoProjects projects.
% A manual inspection on \inspectedMatches top matches revealed that \inspectedMatchesValid were valid.
% This resulted in \inspectedVulnsWithOneValidTest vulnerabilities being matched with a real witnessing test, which is four times more than the \fixCommits baseline approach could do.
% }