\section{Introduction}
\label{sec:intro}

Software vulnerabilities are flaws violating certain security requirements~\cite{frei:weis2010:cve,mcgraw:2006:building:security:in}, commonly caused by how the flawed code components handle their inputs~\cite{li:ccs2017:patches}.
Differently from traditional bugs, vulnerabilities are commonly detected via reactive mechanisms, such as Static Application Security Testing (SAST) tools, penetration testing, or fuzzing~\cite{lipp:issta2022,austin:ist2013,Shahriar:csur2012,kaur:coconet2019}, which are often considered enough to detect most security issues~\cite{Elder:emse2022:really,Cruzes:2017:agile:sectests}.

The ``shift-left'' principle encourages the adoption of a \textit{test-first approach}, in which vulnerabilities are detected proactively via automated testing before the code is committed---in a similar fashion to how bugs are intercepted~\cite{mcgraw:2006:building:security:in,Gonzalez:esem2021:challenge:sectests}.
Despite having different characteristics~\cite{Camilo:msr2015:bug:vulns,canfora:cs2020:bug:vulns}, we argue that vulnerabilities should be treated like traditional bugs in this sense: Employ \textit{unit tests} to intercept security issues in earlier stages and reduce the risk of shipping them into production.
Via automated tests, vulnerabilities can be found by stimulating a certain behavior by supplying the application with crafted inputs~\cite{felderer:2016:survey:sectests,Cruzes:2017:agile:sectests,Mohammadi:ase2016:xss:unit}.
For instance, a test case for a method affected by a Cross-site Scripting vulnerability would craft exploitative strings, like \texttt{<script>alert(1)</script>}, and pass them to the method parameters.
Then, it would leverage assertions to ensure the method behaves correctly, i.e., the HTTP response object does not contain the input reflected as-is.
A failed assertion would mean that the vulnerability was present.
A test case behaving like this is called \textbf{vulnerability-witnessing test}~\cite{kang:issta2022:transfer,zhang:2023:llm:sectests} (a.k.a. Proof of Vulnerability, PoV~\cite{pinconschi2021comparative, bui:msr2022:vul4j}), or simply \textsl{``witnessing test''} in this context.

%Listing~\ref{lst:motivating} shows the test witnessing an infinite loop vulnerability (CWE-835) affecting \textsc{Apache Commons Imaging}, disclosed via CVE-2018-17202.
%If this test is executed on the vulnerable version (i.e., the one including the \texttt{while} statement highlighted as red in Listing~\ref{lst:motivating}), it will fail due to taking more than two seconds---set by the test author to decree when the infinite loop happens.  
Listing~\ref{lst:motivating_jspwiki} shows the test witnessing a path traversal vulnerability (CWE-22) affecting \textsc{Apache JSPWiki}, disclosed via CVE-2019-0225.
Suppose this test is executed on the vulnerable version (i.e., the one including the \texttt{request.getPathInfo()} in the \texttt{return} statement highlighted as red).
In that case, it will fail due to not returning to the main wiki page---as intended behavior defined by the developers.
Interestingly, the complete version of the witnessing test was only added in a later commit, not in the vulnerability-fixing commit.

%
%Writing such tests at the unit or integration level is challenging and not sufficiently encouraged~\cite{Cruzes:2017:agile:sectests,Le:msr2024:vuln:pred}.
%In this sense, automated data-driven approaches would support developers in this task, allowing them to intercept vulnerabilities at earlier stages during development.
%For example, AI-based techniques could automatically generate a unit test for a given vulnerable code~\cite{kang:issta2022:transfer,zhang:2023:llm:sectests} or provide examples of tests of historical vulnerability that developers can reuse for testing similar security issues.
%Unfortunately, \textbf{training and validating such approaches require much data, which is currently limited}.

\VulforJ is the primary reference collection of witnessing tests~\cite{bui:msr2022:vul4j}, made of manually-validated \vulforjWitTests unit tests matched with \vulforjVulns vulnerabilities affecting \vulforjProjects \Java projects.
\VulforJ allows reproducing such tests in isolated environments where the project has successfully been built on its vulnerable and patched versions.
%Such a dataset comprises \vulforjVulns reproducible vulnerabilities affecting \Java projects, consisting of isolated environments where the vulnerable project, like \textsc{Jenkins}, has been built successfully on the vulnerable and patched versions.
%Besides, the concept of ``reproducible'' means that the vulnerability can be triggered via at least one test case for a total of \vulforjWitTests distinct witnessing tests.
%\footnote{The original paper refers to them as Proof of Vulnerabilities, PoV.}
To find them, the test suite after the patch had been applied was run on both versions; any test methods passing on the former and failing on the latter were manually inspected and confirmed to be witnessing a vulnerability.
Despite similar datasets with vulnerability data of other programming languages exist~\cite{Bhandari:promise2021:cvefixes,Fan:msr2020:bigvul,Nikitopoulos:fse2021:crossvul,ponta:msr2019:projectkb}, \VulforJ is the only one having working witnessing tests to date.

%Enriching a catalog like \VulforJ is not an easy task. In fact, 
According to what has been described in the \VulforJ original paper~\cite{bui:msr2022:vul4j}, the process of reproducing a vulnerability through its witnessing tests was: (1) Select a vulnerability having access to the commit that patched it; (2) checkout and build the project onto that commit, (3) run the test suite found at that revision, (4) checkout and build the commit before representing the vulnerable version, and (5) run the same test suite from before on this vulnerable revision.
%Any test that passed on the patched version but failed on the vulnerable version was deemed the witness of that vulnerability.
%(further details of this process are described in Section~\ref{sec:background}).
%
Such a process had to face several challenges.
For instance, not all vulnerabilities can be reproduced, as the building must succeed in both the vulnerable and patched versions.
It is also required to observe at least one test case passing on the patched version but failing on the vulnerable one.
Therefore, it was required that (1) the witnessing tests exist in the test suite at the patched version and (2) the fix commit must fully resolve the vulnerability (otherwise, the witnessing test might fail in that version as well).
The whole process worked for only \vulforjVulns out of 899 (8.78\%) vulnerabilities inspected.
%We observe that this process bears its fruits when two main conditions are met.
%First, \textbf{the building must succeed for both the vulnerable and patched versions}, which happened only in 139 out of 417 ($\sim$33\%) candidate patches.
%Then, the two test suite runs must result in tests failing on the vulnerable version and passing on the patched version.
%This requirement can only happen if \textbf{the witnessing tests exist in the test suite exactly the patched version}, which happened only for 69 out of 139 ($\sim$50\%) buildable patches.
%In summary, the whole process was successful only for 17\% of the candidate patches.
%it might not be applicable in most of the cases\COMMENT{The word ``most'' should be backed by some data. If we have time, we can compute the CVEs from CVEDetails affecting Java projects (how?) that are not in ProjectKB. Otherwise, just rephrase}.
%Indeed, \VulforJ requires the vulnerabilities have an explicit reference to a fixing commit.
%Without such information, the approach cannot be reused to find more vulnerability-witnessing tests.
%This leaves the only option to manually inspect the whole content of a test suite to find the (likely) vulnerability-witnessing tests.
%This can easily take up huge amount of time and effort as they test suites can be made of thousands of test methods~\cite{lin:ist2014:test:reduction}.

\iffalse
\begin{listing}[t]
    \inputminted[
    frame=single,
    baselinestretch=1.2,
    fontsize=\tiny,
    escapeinside=||,
    linenos,
    xleftmargin=2em
    ]{java}{listings/motivating.java}
    \caption{Documented fix for CVE-2018-17202 and its related witnessing test in \textsc{Apache Commons Imaging}.}
    \label{lst:motivating}
\end{listing}
\fi

\begin{listing}[t]
    \inputminted[
    frame=single,
    baselinestretch=1.2,
    fontsize=\tiny,
    escapeinside=||,
    linenos,
    xleftmargin=2em
    ]{java}{listings/motivating_jspwiki.txt}
    \caption{Documented fix for CVE-2019-0225 and its related witnessing test in \textsc{Apache JSPWiki}.}
    \label{lst:motivating_jspwiki}
\end{listing}

In other words, the manual search of witnessing tests is likely \textbf{unsuccessful}---like \textit{finding a needle in a haystack}---and \textbf{effort-consuming}.
Thus, an automated solution could reduce the burden of finding real-world examples of witnessing tests in the wild.
In fact, the security researcher would benefit from an automatic tool that \textit{finds} more examples of vulnerability-related tests---which are currently scarce---to be used for developing novel AI-based techniques for generating security tests~\cite{kang:issta2022:transfer,zhang:2023:llm:sectests} or improving the automated repair process~\cite{bui:msr2022:vul4j, liu2019tbar, mohammadi2019automated,gao2021beyond}. At the same time, software engineers also would benefit from an automated \textit{matching} mechanism that relates test cases in their projects with the historical vulnerabilities that affected it, likely because they lost---or never had---track of such relationships explicitly.

Therefore, we present \vuteco (\ul{\textbf{VU}}lnerability \ul{\textbf{TE}}st \ul{\textbf{CO}}llector), a fully-static approach collecting vulnerability-witnessing tests in \Java test suites.
% without requiring to build the analyzed project.
%It inspects an entire \Java project test suite at any revision without requiring the witnessing test to exist since the vulnerability has been patched and matches them with the right vulnerability.
%\vuteco is a fully static solution that does not require building the inspected projects.
\vuteco addresses two tasks: (1) the ``\finding'' task to determine whether a test case is
security-related and (2) the ``\matching'' task to relate a test case to the exact vulnerability it is witnessing.
The former occurs through a deep-learning model based on a pre-trained CodeBERT~\cite{feng:emnlp2020:codebert} and a logistic classifier to return binary predictions, called \finder.
The latter task, instead, requires the joint use of the \finder and another CodeBERT-based model, called the \linker, to validate the match between a test case and a description of a vulnerability with binary predictions.
%The matching occurs via deep-learning models---based on pre-trained CodeBERT models~\cite{feng:emnlp2020:codebert}---and a logistic classifier on top to return binary predictions.
%In this respect, the whole model is made of two distinct sub-models, each consisting of a pre-trained CodeBERT model~\cite{feng:emnlp2020:codebert} with a deep neural network and a logistic classifier on top.
%The former determines if a test case is likely witnessing a vulnerability, while the latter validates the link between a candidate witnessing tests and a vulnerability (via its description) that affected that project.
%The former---a.k.a. the \finder model---has been fine-tuned on a dataset of \JUnit test methods and can determine if a test method is likely witnessing a vulnerability.
%The latter---a.k.a. the \linker model---has been fine-tuned on a dataset of witnessing tests paired with the description of the related vulnerability and can say if a suspect test method is witnessing the given vulnerability.
%Then, once the two models had been trained on their sub-tasks, they were further fine-tuned in an integrated setting to address \vuteco's main downstream task.
%Both datasets have been built from the \vulforjProjects projects appearing in \VulforJ, made of \vulforjTests \JUnit test methods, of which \vulforjWitTests have witnessed a vulnerability (\vulforjVulns in total).
\vuteco has been trained for both tasks using the data in \VulforJ, accounting for \vulforjTests distinct test cases from the \vulforjProjects projects, of which \vulforjWitTests witnessed \vulforjVulns vulnerabilities.

%In their best configuration, the \finder model scored 0.83 precision and 0.71 F1 score, while the \linker model achieved 0.59 precision and 0.61 F1 score.
%After this, the two have been integrated into one \globalModel model to address \vuteco's main downstream task, scoring perfect precision and 0.51 F1 score.
%The three training sessions leveraged \vulforjTests test cases from the \vulforjProjects projects in \VulforJ, of which \vulforjWitTests witnessed \vulforjVulns vulnerabilities.
After finding the best configurations, \vuteco successfully addressed the \finding task with perfect precision and 0.83 F0.5 score, while addressing the \matching with satisfactory results, i.e., 0.86 precision and 0.68 F0.5 score.
%In its best configuration, \TEMP{\vuteco achieved perfect precision and a 0.55 F1 score.}
Afterward, \vuteco was employed on a set of \invivoProjects open-source \Java projects affected by \invivoVulns vulnerabilities, finding a total of 102 out of 145 (70\%) truly security-related test cases.
Unfortunately, the match between the test cases and the vulnerabilities did not bring the expected results, failing to match any test case with the exact vulnerability.
%matched with \resultsVulns vulnerabilities, after processing \invivoTests test cases.
%The top matches were manually inspected by two researchers with experience in software security and unit testing.
%The process took each inspector almost \inspectionTime hours and confirmed that \inspectedVulnsWithOneValidTest vulnerabilities had the correct test matched.
After a deeper inspection of the matching results, we found that in almost all cases, the test case was still related to security aspects; in some cases, the matched vulnerability was similar to the exact one.
%
Hence, we conclude that \vuteco can benefit researchers and practitioners in finding vulnerability-witnessing tests, though the matching remains a challenging task requiring more attention.

%The raw and validated results of \vuteco have been collected into a new dataset and open-sourced.
In summary, this paper:
\begin{itemize}[leftmargin=*]
    \item Introduces \vuteco, the \textit{first ever} approach to find vulnerability\-/witnessing tests in \Java repositories and match them with the related vulnerabilities.
    \item Validates the performance of \vuteco for the \finding and \matching tasks on datasets extracted from \VulforJ, accounting for \vulforjTests test cases, of which \vulforjWitTests witnessed \vulforjVulns vulnerabilities in \vulforjProjects \Java projects.
    \item Employs \vuteco to find security-related tests and match them with vulnerabilities in \invivoProjects open-source \Java projects, succeeding in the former case and failing in the latter.
    \item Releases a replication package containing all the scripts and data connected to the experiments~\cite{appendix}.
\end{itemize}
