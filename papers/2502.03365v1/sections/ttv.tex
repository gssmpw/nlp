\section{Threats to Validity}
\label{sec:ttv}

%\EI{TTV on using only the CVEs from ProjectKB vs all the CVEs for those projects mined from NVD: this would never reduce the false matches, but, in the best case, increase the true matches. Besides, this could backfire... not sure}

%\subsection{Threats to Internal Validity}

%\CB{As I remember, the test can be predicted to be linked to cve $A^*$. The truly associated cve of the test is $A$---not existing in the training dataset, but $A$ and $A^*$ share the similarity such as vuln type, project context, etc.\footnote{For example, in the case of CVE-2018-1000865 and CVE-2018-1000866 in \url{https://docs.google.com/spreadsheets/d/1ZAZ5GWLGZFmtFbuQyIS8k0n_RgZbAW2A2WckpYV5SLE/edit}}}
For the \matching task, we considered the performance of the \finder and \linker models in their tasks independently before integrating them.
%, emulating a bottom-up integration testing scenario (explained in Section~\ref{subsubsec:overview}).
Yet, there was no guarantee that the selected configurations would work well once integrated.
%does not re-use the exact models from their sessions but re-use the best configuration observed.
%and re-train them on a different dataset (the motivations are given in Section~\ref{subsubsec:overview}).
%Therefore, the final behavior might be different than expected.
We mitigated this threat by running a new train-test session to assess the whole integrated model for the actual \matching task.
The final answer to the in-vitro analysis on \rqTwo is only given by the results achieved by the integrated model.
%When evaluating the \globalModel model, we only searched its optimal configuration without tuning the \finder and \linker models again, as this would have required searching among 13 hyper-parameters (Table~\ref{tab:hyperparams}), expanding the search space too much.

Due to computational constraints, we could not thoroughly analyze the impact of many design aspects in the in-vitro analyses.
%, so we prioritized only those aspects that could provide some benefits.
We treated some aspects as factors, like data augmentation, to be evaluated at test time, while others were treated as hyper-parameters to be optimized at development time.
%We did not employ statistical tests to compare the various groups due to the limited number of configurations belonging to each.
%In Section~\ref{sec:results}, we observed how certain configuration values might provide some benefits, e.g., data augmentation.
%helped the \linker model more than \finder model.
%Yet, we did not employ any statistical tests and just compared the values obtained with the aggregated values.
%Despite not employing statistical tests, the analyses conducted aimed to find the most suitable configuration to prepare \vuteco for real-world usage, experimented in the two in-vivo analyses.
%\EI{The fact that we used ``Learn'' could be mentioned}

%\TEMP{While evaluating \vuteco in-vitro, we moved the classification threshold of all models from the default $0.5$ to a different value that maximized the F0.5 scores.
%However, the differences were marginal on a large scale, so we opted to keep the default thresholds.
%The performance of the tuned configurations is in our online appendix~\cite{appendix}.}

For the \matching task, \vuteco accepts a natural language description of the vulnerability.
Hence, \vuteco accepts any text describing the issue, also from other sources like bug reports~\cite{zhou2017automated,le2019automated,bao2022v,nguyen2023multi,iannone:tosem2024:exploit}.
We opted to experiment with CVE, as it is the most comprehensive catalog for obtaining such information and can be easily queried.
%\CB{I think this makes sense, even the CVE description sometimes seems not to be valid, for example, in this case it just contains texts about the CVE duplication \footnote{\url{https://nvd.nist.gov/vuln/detail/CVE-2015-8581}}.}

%\EI{Overfitting risk: Vuteco might mistake two similar vulnerabilities of the same project if they share text. Similarly, the prediction can go well if similar vulnerabilities fall into the training. Example: CVE-2018-1000865 and CVE-2018-1000866. However, we didn't do anything to mitigate this risk... cross-validation would have been good.}

%\EI{TTV for the baselines, they might be too naive. Besides, we can say that other baseline might exists, and in our appendix we did some others}

%\subsection{Threats to External Validity}

\vuteco has been trained and tested on the \JUnit test cases in \VulforJ.
Therefore, all the results observed cannot be generalized to other programming languages, other testing frameworks (like \textsc{TestNG}), or vulnerability types that are not found in \Java code, like memory-related vulnerabilities.
We chose \Java mainly due to the availability of \VulforJ containing examples of witnessing tests to train and validate \vuteco.
At the same time, we remark that \Java is still a relevant language to analyze from a security perspective as it keeps exhibiting vulnerabilities for a long time~\cite{veracode}.

Despite being the only catalog of its kind, the limited number of witnessing tests in \VulforJ does not allow models to learn many patterns that can be generalized to different contexts.
It also lacks diversified examples, e.g., it has eight examples of tests witnessing CWE-835 (Infinite Loop), but just one witnessing CWE-78 (OS Command Injection)~\cite{bui:msr2022:vul4j}.
We partially handled this by trying to augment the training sets involved during the experimentation, though without the hoped effect.
\vuteco was born to contribute to feeding such knowledge bases with new examples, which in turn could be used to re-train \vuteco and return more results.

%While the in-vitro analysis validated \vuteco on a subset of witnessing tests paired with vulnerability descriptions from real-world \Java projects in \VulforJ, the in-vivo showed its applicability on further real-world projects, extending the setting used for the in-vitro analyses.
%Both support \vuteco's ecological validity.

%\subsection{Threats to Construct Validity}

\vuteco flags witnessing tests based on a static approach: No test cases are executed, and no projects are built.
For this, \vuteco cannot provide a definite answer about whether the tests will trigger the matched vulnerabilities.
The dynamic assessment has been taken outside the scope of this work due to technical difficulties in building numerous projects in a reasonable time, as encountered by the authors of \VulforJ~\cite{bui:msr2022:vul4j}.
%\vuteco's goal is to find candidate vulnerability-witnessing tests whose actual behavior must be assessed in separate sessions. 
%\EI{Additional reasons of why we don't do this: so far we have no knowledge of how witnessing tests appear, since we can't have good empirical studies without data. If we had to build and run the tests automatically and on large scale, most would fail (as seen in Vul4J), so losing many precious candidates for expanding our knowledge on the matter. Of course, in the future this can be addressed.}

%We defined \vuteco's general architecture based on internal trials on the development sets used in the in-vivo evaluations.
%The design choices made have certainly influenced its performance.
%We acknowledge that numerous other hyper-parameters and design decisions could have led to better versions of \vuteco; however, their investigation would have required running thousands of train-test sessions and incurring huge computation costs.
%In this work, we focused on the main factors we believed to be impactful, aiming to define a first working tool for collecting vulnerability-witnessing tests.
%The wide range of possible designs makes it difficult to identify the right solution without running thousands of train-test sessions and incurring huge computation costs.
%We followed similar design choices adopted in studies making classification of test cases~\cite{fatima:tse2023:flakify,akli:ast2023:flakycat}.
%Future improvements can be made to enhance \vuteco's retrieval capabilities for this stable point.
%
%Besides, we know many other aspects could influence the model performance, like employing a weighted loss function or different network architectures, requiring deeper experimentation.

%In \rqOne, we relied on the traditional metrics adopted in information retrieval to assess the performance of the experimented models and determine the most suitable for our purpose.
%To mitigate the risk of selecting models with apparently good performance but without practical usefulness, we also considered the absolute numbers of true and false positives and the Inspection Ratio.

%\EI{TTV regarding the limit to 512 tokens... we partially handled it by removing comments before the test method and one-lining, but we still had X\% cases in which the input was longer than 512 tokens. We accepted the risk and just truncated, and there are no approaches that can identify the relevant part in a test (which can be interesting actually to do)}

%\subsection{Threat to Conclusion Validity}
