\section{Related Work}
\label{sec:rw}

\subsection{Test Case Classification}

Fatima et al.~\cite{fatima:tse2023:flakify} presented \textsc{Flakify}, a data-driven approach to detect flaky tests in \Java projects.
%a deep-learning classifier to flag flaky tests in \Java projects~\cite{fatima:tse2023:flakify}.
\textsc{Flakify} leverages a pre-trained CodeBERT and a feed-forward neural network % with one hidden layer 
to predict whether a \JUnit test method had a flaky behavior.
Such architecture resembles the \vuteco's \finder model.
%The model has been fine-tuned on example \JUnit test methods, represented solely with their source code plus \textsc{Javadoc} and other annotations if available.
%The training set was oversampled with bootstrapping until the imbalance ratio was 1:1 (i.e., the number of flaky tests matched those of non-flaky tests).
\textsc{Flakify} achieved 0.79 and 0.73 F1 scores on two different experiments on \textsc{FlakeFlagger} dataset~\cite{Alshammari:icse2021:FlakeFlagger}, while achieving 0.98 and 0.89 F1 score on \textsc{IDoFT} dataset~\cite{lam:icst2019:idflakies}, outperforming state of the art approaches.
Somewhat similarly, \textsc{FlakyCat} exploits few-shot learning to predict the exact category of flakiness of \JUnit tests~\cite{akli:ast2023:flakycat}.
\textsc{FlakyCat} relies on a pre-trained CodeBERT to create the embeddings of test cases and a Siamese Network to project the embeddings into a space where tests of the same flakiness category appear similar---according to the cosine similarity function.
%Tests longer than 512 tokens (max allowed by CodeBERT) are handled separately.
This is enacted by the Triplet Loss function~\cite{Schroff2015:triplet:loss} during the training.
%An input test method is first projected into the vector space, then compared with all the tests in the knowledge base, and flagged with the same class of the most similar test.
%Namely, in addition to the test method to classify, the model is fed with two additional test methods, one belonging to the same class and one not, and is sampled randomly from a knowledge base of labeled test methods.
%Once the three test methods are processed by the CodeBERT and the Siamese Network, the similarity scores to the two support examples are computed and used to compute the triplet loss score, which penalizes the network if the tests of different classes have high similarity and vice versa.
%The actual classification consists of a \textit{1-nearest neighbor} approach.

The design of \vuteco, particularly the \finder model, has been inspired by such approaches due to the similarity of their tasks, particularly those made in \textsc{Flakify}~\cite{fatima:tse2023:flakify}.
Both approaches rely only on the test code for making the classification without requiring access to the production code, running it, or using any human-defined features.
%
%Differently from previous work on similar tasks, \vuteco, through its \finder model, is the first automated approach that flags test cases witnessing vulnerabilities.
%Besides, \vuteco can also pinpoint the exact vulnerability being tested, thanks to the joint use of \finder and \linker models.


\subsection{Vulnerability-witnessing Tests}

Kang et al.~\cite{kang:issta2022:transfer} introduced \textsc{Transfer} to generate security test cases for \Java projects affected by vulnerable library dependencies.
\textsc{Transfer} builds on existing vulnerability-witnessing tests mined from the upstream library project and tries to generate a test case targeting the client project that recreates the same program state generated by the execution of the witnessing test in the original library.
This approach leverages a genetic algorithm, whose goal is to find a client test that ``mimics'' the behavior of the library test.
\textsc{Transfer} successfully generated security tests for 14 known library vulnerabilities in 23 client projects.
Later, \textsc{Transfer} was extended by Chen et al.~\cite{chen:icse2024:vesta} by adding a migration step, which helps guarantee the similarity between generated tests from client projects and the original vulnerability tests.
Their tool, \textsc{Vesta}, outperforms \textsc{Transfer} by 53.4\% in terms of the effectiveness of generating tests on a dataset of 30 \Java vulnerabilities.
%
Zhang et al.~\cite{zhang:2023:llm:sectests} adopted a similar idea but relied on \textsc{ChatGPT}-4 rather than a genetic algorithm.
The library witnessing tests have been used as examples in the prompt to nudge the AI to generate a similar test but targeting a different production method (living in the client code rather than the library).
The proposed approach generated 24 working tests from 55 tasks, encompassing 30 known vulnerabilities and at least one affected client project for each.
%Such tests successfully triggered the vulnerability when run from the client projects.

Antunes et al.~\cite{Antunes:issre2012:recycling} proposed an approach to recycle the payloads from existing test cases of the File Transfer Protocol (FTP) protocol, which were then used to generate new vulnerability test cases for other protocols or new features. The proposed tool achieved better performance in terms of vulnerability coverage compared to two fuzzers and discovered 25 new vulnerabilities in FTP servers.

%\vuteco does not generate vulnerability-witnessing tests but rather gathers those already existing in \Java project test suites.
%In fact, \vuteco supports the expansion of known catalogs of vulnerability-witnessing tests, which will be instrumental in improving existing automated vulnerability test generation techniques.
%For instance, \vuteco can provide \textsc{Transfer} with the starting witnessing tests to start the generation process or few-shot examples for LLM-driven approaches.
