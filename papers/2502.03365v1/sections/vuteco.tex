\section{The \vuteco Approach}
\label{sec:vuteco}

\subsection{Overview}
\label{subsec:vuteco_overview}

\vuteco faces two distinct tasks.
The \finding task accepts a \JUnit test case as input and tells whether it is security-related (i.e., \finderPosClass) or not (i.e., \finderNegClass).
The \matching task accepts a \JUnit test case and a description (in natural language) of a known historical vulnerability (related to the project) and tells whether the test case is witnessing that vulnerability (i.e., \globalPosClass) or not (i.e., \globalNegClass).
%The main downstream task of \vuteco consists of (1) inspecting \JUnit test suites, (2) finding candidate vulnerability-witnessing test cases, and (3) matching them with the right vulnerability.

The \vuteco approach has been wrapped into a tool (having the same name, \vuteco) that automates both tasks.
The tool accepts (i) a \Java project repository, (ii) the project revision (i.e., commit) to inspect, and (iii) a list of descriptions (in natural language) of known historical vulnerabilities related to the project---taken from CVE (Common Vulnerabilities and Exposures).
Any \textsc{Git}-based repository is valid as long as there are \JUnit test cases to process.
%; the only requirement is that it has to describe what the vulnerability consists of via natural language.
%\CB{Finding vulnerability patches~\cite{zhou2017automated, nguyen2023multi}, identifying vulnerable software versions (V-SZZ)~\cite{bao2022v}, vulnerability assessment~\cite{le2019automated}}

Figure~\ref{fig:vuteco-overview} depicts the general functioning of \vuteco.
Once checking out the project repository to the selected revision,
%\vuteco extracts all the valid \JUnit test methods adopting a heuristic approach. Namely,
\vuteco parses all \Java files and marks any class method having the following properties as a test case:
\begin{enumerate}[leftmargin=*]
    \item it is annotated with \texttt{@Test} (\JUnit 4 and 5) or its class extends \textsc{TestCase} or any subclass of it (for \JUnit 3);
    \item it is not overriding a method defined in class \textsc{TestCase}, like \texttt{run()} or \texttt{getName()} (for \JUnit 3);
    \item it is not a ``lifecycle method'', i.e., annotated with \texttt{@BeforeAll}, \texttt{@AfterAll}, \texttt{@BeforeEach}, or \texttt{@AfterEach};
    \item it returns \texttt{void} if not annotated with \texttt{@TestFactory};
    \item it is not \texttt{abstract}, \texttt{static} or \texttt{private};
    \item its class is not \texttt{abstract}.
\end{enumerate}
Such properties have been designed according to how the \JUnit guide describes a test case~\cite{junit:guide} and the official \textsc{Javadoc} of \JUnit beyond version~3.

Each mined test case is sent to the \finder model, which is responsible for implementing the \finding task (i.e., determining whether the test case is security-related).
Then, each test flagged as security-related is sent to the \linker model along with the descriptions of all vulnerabilities supplied via input to determine which of them is witnessed by the test case.
%This joint use of \finder and \linker models implements the \matching task (the reason for using two models rather than one is motivated by the results of the in-vitro experimentation, which are presented in Section~\ref{sec:results}).
%Each candidate test is then paired with all vulnerability descriptions supplied via input and sent to the second model, i.e., the \linker, whose goal is to assess if the candidate is witnessing the vulnerability described by the short text.
%Ultimately, \vuteco returns all the test cases that were successfully linked with a vulnerability.
Sections~\ref{subsec:finding} and~\ref{subsec:matching} describe how the \finding and \matching tasks have been carried out.
The design choices behind the models used there are supported by the experimentation described in Section~\ref{subsec:invitro}.
%and the related results in Section~\ref{subsec:fnd-results} and~\ref{subsec:match-results}.

\begin{figure}[t]
    \centering
    \resizebox{.98\linewidth}{!}{
        \includegraphics{figures/vuteco-overview.pdf}
    }
    \caption{Graphical overview of the functioning of \vuteco.}
    \label{fig:vuteco-overview}
\end{figure}

%\subsection{Technical Detail}

\subsection{The \finding Task}
\label{subsec:finding}

The \finding task consists of determining if a test case is security-related, i.e., it is focused on some security properties of the project where it belongs.
The \finder model addresses this task, trained to classify test cases into two classes, i.e., \finderPosClass (the positive class) and \finderNegClass (the negative class).
%
The upper part of Figure~\ref{fig:vuteco-detail} depicts the architecture of the \finder model, which consists of a deep neural network built on top of a pre-trained CodeBERT~\cite{feng:emnlp2020:codebert}.
The pre-trained CodeBERT starts from the checkpoint \texttt{microsoft/codebert-base} downloaded from \textsc{HuggingFace}~\cite{codebert:base:hf}.
%Essentially, it is a specialized RoBERTa model~\cite{liu2019roberta} made of 125M trainable weights.
CodeBERT has been pre-trained on
%with masked language modeling (MLM) and replaced token detection (RTD) self-supervised tasks for
code examples of six programming languages, including \Java, paired with natural language text.
Hence,
%leveraging the pre-training made from its base RoBERTa model~\cite{liu2019roberta} and how CodeBERT has been trained,
we deemed it suitable for understanding the content of \JUnit test methods.

As a preliminary action, \vuteco transforms the input test case by removing new line characters and consecutive white space characters (including tabs), reducing the whole method into a single line.
Then, the resulting string is tokenized using the WordPiece algorithm~\cite{wu2016googles:wordpiece} (whose vocabulary was fitted during the pre-training of CodeBERT), and each resulting token is replaced with a numeric identifier.
%Such a step is mandatory as Transformer-based models can only interpret sentences as numerical vectors.
Then, the resulting numeric vector is sent to the CodeBERT input layer (supporting up to 512 encoded tokens), which returns an embedded representation of each token.
Besides, due to the underlying BERT-based architecture, an additional embedding for the special token \texttt{[CLS]} is returned, which has the goal of capturing the whole syntax and semantics of the entire sentence (here, the whole test case), making it suitable for sentence classification tasks.
Considering the goal of the \finding task, we only selected the embedded representation of \texttt{[CLS]}, made of 768 values.
%
On top of this, we added a deep neural network with an input layer of 768 neurons, matching the size of the sentence embedding resulting from the CodeBERT model; then, we added two linear hidden layers with 768 and 64 output neurons, respectively, calling GELU activation function~\cite{hendrycks2023gaussian:gelu}.
After this, the final layer returns the two logits needed for the final classification, which is done through the Softmax function to return the probabilities for the two classes.

\subsection{The \matching Task}
\label{subsec:matching}

\begin{figure}[t]
    \centering
    \resizebox{.98\linewidth}{!}{
        \includegraphics{figures/vuteco-detail.pdf}
    }
    \caption{\EI{FIX, broken!} Graphical depiction of the inner working of \vuteco.}
    \label{fig:vuteco-detail}
\end{figure}

The \matching task consists of determining if a test case and a vulnerability are related, i.e., the former witnesses the latter.
This task is carried out by the joint use of the \finder model (as in Section~\ref{subsec:finding}) plus the \linker model.
Indeed, the \linker model is focused on a \textit{simplified} task that assumes the input test case is surely security-related.
The motivation for the joint use of two models---rather than the direct use of the \linker model---is supported by the results of the in-vitro experimentation in Section~\ref{sec:results}.

The \linker model has been trained on its simplified task, i.e., trained to classify pairs of security-related tests and vulnerability descriptions into two classes, i.e., \linkerPosClass (the positive class) and \linkerNegClass (the negative class).
%
The lower part of Figure~\ref{fig:vuteco-detail} depicts the architecture of the \linker model.
Such a model follows a similar architecture to the \finder model, leveraging the same pre-trained CodeBERT model to create the input embeddings.
However, instead of creating the sentence embedding of the sole test case code (still reduced to a single line), the \linker model also adds the vulnerability description as well.
Therefore, the vulnerability description was \textit{concatenated} before the test case with a special token [SEP] in between to indicate the two different sentences (as required by BERT-like models) and then tokenized in the same manner as the \finder model.
%
The resulting sentence embedding is then sent to a deep neural network with an input layer of 768 neurons (to match the embedding size) and a linear hidden layer of 256 output neurons, calling the GELU activation function.
%
Like the \finder model, the final layer returns the logits for the classification, converted into probabilities of the two classes with the Softmax function.

To enable the joint training and use of the \finder and \linker models, \vuteco employs a \textit{decision function} after the \finder made its prediction on the test case---depicted on the right-most side of Figure~\ref{fig:vuteco-detail}.
Indeed, if the probability of the test case belonging to the \finderPosClass class was at least $0.5$, then the \linker model is invoked, and its probabilities are used for the \globalPosClass and \globalNegClass classes.
Otherwise, the probabilities of the \finder are used instead.
Simply put, the \linker's judgment is not considered if the \finder model did not flag the test case as \finderPosClass as the \linker model expects security-related tests as input.
%
Hereafter, the wording ``integrated model'' refers to the joint use of \finder and \linker to address the \matching task.

%\subsubsection{The \finder and \linker Integration}

%The right-most side of Figure~\ref{fig:vuteco-detail} shows how \finder and \linker models have been used in conjunction to address the main downstream task.
%Namely, once the \finder and \linker models made their predictions, \vuteco returns the logits from the \linker model if the probability of \finderPosClass (computed applying the Softmax function on the logits) was more than $0.1$, otherwise \vuteco returns the logits computed by the \finder model.
%Hereafter, the wording ``integrated model,'' refers to the integration of \finder and \linker for the main downstream task.
%This selection is due to the \linker model's inability to make reliable predictions if the test method has not been flagged as a suspect vulnerability-witnessing.
%Consequently, if the \finder model flagged the test as a suspect, the prediction made by the \linker model becomes meaningful, and it will be used to assess if the supposed witnessing test is linked to the given vulnerability description.
%We refer to this integration as the \globalModel model for simplicity.

% \subsection{Training the Deployment Version}
% \label{subsub:train}

% The \finder and \linker models were trained in individual sessions to become more familiar with their sub-tasks before the integration.
% %---i.e., assessing the links between a \JUnit test method and a vulnerability description.
% Afterward, the integrated model was trained further to perform well on the main downstream task.
% As a result, three training sessions were needed: One for the \finder model, one for the \linker, and one for the integrated model.
% From \vuteco's perspective, the first two sessions can be seen as a \textit{pre-training}, while the latter as a \textit{fine-tuning}.
% The three training datasets have been extracted from \VulforJ, the only available knowledge base containing \JUnit test cases with the witnessed vulnerabilities.

% The deployment version of \vuteco relies on the results of the in-vitro evaluation (Section~\ref{subsec:in-vitro-results} to configure the training process.
% %
% The \finder model was trained on \finderTrainingSize \JUnit test cases, of which \finderTrainingPosSize were witnessing a vulnerability ($\sim$\finderTrainingPosPerc).
% %
% Likewise, the \linker model had a training set of \linkerTrainingSize pairs of witnessing test cases and vulnerability descriptions, of which \linkerTrainingPosSize represented valid links ($\sim$\linkerTrainingPosPerc).
% To handle such a class imbalance, this training set has been \textit{augmented} via bootstrapping (i.e., sampling with replacements) the \linkerPosClass pairs until the final class imbalance resulted in 1:2 (i.e., one valid link for every two invalid links), ending up with \linkerTrainingExtra new instances of class \linkerPosClass.
% %
% After training the two models for their respective sub-tasks, the integrated model was trained on a third training set made of \globalTrainingSize pairs of test cases and vulnerabilities, of which \globalTrainingPosSize were valid ($\sim$\globalTrainingPosPerc).

% The \finder, \linker, and the integrated model have been trained for \finderTrainingEpochs, \linkerTrainingEpochs, and \globalTrainingEpochs epochs, respectively.
% In all three sessions, 15\% of the training sets were used as \textit{development sets} to find the best training checkpoint to return at the end.
% Namely, the models were evaluated on their development sets at the end of each epoch, and the best checkpoint was determined by the highest \TEMP{AUC-ROC}~\cite{Junge2018:roc} and lowest cross-entropy loss (in case of ties).
