\subsection{\finding Task Results}
\label{subsec:fnd-results}

%All the performance reported in this Section pertains to the experimented configurations with their optimal hyper-parameters and decision threshold maximizing the \metricForTest.
%The exact values of hyperparameters and decision thresholds are in our online appendix~\cite{appendix}.

%\subsubsection{\finder Model}

Table~\ref{tab:rq1-fnd} reports the result of the \finderConfigsLetter experimented configurations of the \finder model along with the best variants of the two flavors of \vocab technique.
%
The best configuration achieved a $0.83$ F0.5 score and perfect precision without augmenting the training set (highlighted in yellow).
%This model behaved equally to the one trained on the training set without augmentation; still, we selected the one trained with SPAT as the best one due to its slightly higher AUC-ROC, i.e., $0.96$.
It also achieved the highest F1 score of $0.67$ and a very high AUC-ROC---only surpassed by the configuration trained with SPAT~\cite{yu:jss2022:spat} with $0.03$ margin.

%\TEMP{From a broader perspective, all the \finder's configurations achieved very high precision and, consequently, a high F0.5 score, as well as a very high AUC-ROC score.}
From a broader perspective, we observed that the data augmentation mechanism had a negative effect on increasing the number of false positives---while keeping the number of true positives unaffected.
Indeed, improper data augmentation (e.g., bootstrapping) can decrease precision by up to $47\%$, introducing several false positive classifications for at most one extra true positive.
%
At the same time, we observed that all configurations could not score a recall higher than $0.56$---achieved by the least precise configuration.
The best configuration, indeed, could reach $0.50$.
This is the only metric in which a baseline technique, i.e., \identVocab, (slightly) outperforms the best configuration.
However, this was only due to the one extra true positive classification---just as in the bootstrapping-augmented variant---at the cost of 288 false positives, making this baseline unsuitable for this task.

The high F0.5 score (due to the perfect precision) observed in the \textit{None} variant allows the \finder model to be the right tool for finding candidate vulnerability-witnessing tests.
%
Thus, we re-trained the \finder model with this configuration and ran it on the set of \Java projects in the wild to find security-relevant test cases.
The model returned \invivoFindingResults test cases from \invivoFindingResultsProjects projects (\invivoFindingResultsProjectsPerc of the analyzed ones).
The inspectors then validated all those findings (spending two minutes per entry on average), agreeing on \invivoFindingAgreement (\invivoFindingAgreementPerc) cases, obtaining \invivoFindingKappa Kappa score, indicating \textit{quasi-perfect} inter-rater agreement.
Afterward, they jointly inspected the 11 remaining cases where they had conflicting judgments until reaching a common decision.
At the end of the inspection process, the number of valid test cases was \invivoFindingResultsCorrect out of \invivoFindingResults (\invivoFindingResultsCorrectPerc).

This caused \vuteco to score $0.7$ precision in the wild, which is not too far from the perfect precision achieved during the in-vitro validation.
In fact, this result must be contextualized with the much greater set of test cases \vuteco analyzed, which was more than 18 times bigger than the held-out set extracted from \VulforJ (\invivoTests tests vs. \evalFinderTrainTot), inevitably leading to a higher chance of false positives.
%
Zooming into the incorrect findings, we observed that the \finder model was fooled by test cases containing certain keywords related to vulnerabilities but did not actually test any security issues.
For example, in \textsc{SonarQube}, \vuteco flagged the test case \texttt{write\_cve} as security-related~\cite{sonarqube:example}; however, the test checks whether the scanner writes certain information about a CVE in the report, which is definitely not a security issue affecting \textsc{SonarQube} itself.
Hence, the \finder model failed to interpret the context in which the security-related terms, like \texttt{cve} or \texttt{DoS}, have been used.
%
This problem could be addressed by introducing fine-grained semantic analyses and better keyword matching, which would greatly reduce the number of false positives.

\rqanswer{1}{
In the \finding task, \vuteco achieved perfect precision and $0.83$ F0.5 score when tested on a held-out set from \VulforJ without the need for augmenting its training data.
The \finder model is generally conservative, favoring precision more easily than recall; an improper data augmentation gives up too much precision for a marginal, often null, increase in recall.
\vuteco found \invivoFindingResultsCorrect security-related test cases in the wild, i.e., \invivoFindingResultsCorrectPerc of the total.
Most of the false positives were induced by some security-related terms appearing in the test code.
}

\input{tables/rq1-fnd-new}
