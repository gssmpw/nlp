\section{Experimental Design}
\label{sec:design}

%\subsection{Research Goal}

Section~\ref{sec:vuteco} described \vuteco in its best setting, based on two evaluations aimed at understanding whether \vuteco successfully addressed the \finding and \matching tasks.
The first one analyzed how \vuteco performed in an \textit{in-vitro} setting, i.e., by testing it on a held-out set originating from \VulforJ, the same source also used for its training and development.
The second one is a complementary analysis that looks into its performance in an \textit{in-vivo} scenario, i.e., by selecting the best model resulting from the \textit{in-vitro} evaluation and running it on a set of projects outside \VulforJ.

\rqbox{1}{How does \vuteco perform in \textbf{finding} security-related test cases?}

\rqbox{2}{How does \vuteco perform in \textbf{matching} test cases and vulnerabilities?}

%Indeed, our first goal was to understand the right way to configure \vuteco to maximize its performance.

%\rqbox{1}{What is the most effective configuration of \vuteco for matching vulnerabilities and test cases?}

%Afterward, we assessed the usefulness of \vuteco (in its best setting) by employing it on real-world \Java projects to collect instances of test cases witnessing historical vulnerabilities that we were not aware of.

%\rqbox{2}{To what extent can \vuteco collect vulnerability-witnessing tests in open-source \Java projects?}

%To address \rqOne, we trained and tested several configurations.
%Section~\ref{subsec:invitro} explains how we addressed \rqOne, describing how the %training and testing data has been prepared and how we selected the configurations to experiment, while Section~\ref{subsec:invivo} explains how we addressed \rqTwo, describing what we obtained by employing \vuteco in the wild.

\subsection{In-vitro Evaluation Design}
\label{subsec:invitro}

%\subsubsection{Overview}
%\label{subsubsec:overview}

%\vuteco comprises two key components: The \finder and \linker models.
%Considering the criticality of their individual tasks, both models had to be evaluated in separate sessions to ensure they could adequately solve their sub-task before being deemed suitable for integration (following a \textit{bottom-up} approach).

During the in-vitro evaluation, we explored the role of certain \textit{factors}, such as training data augmentation, to observe how they affected the final performance, measured in the standard way for binary classification tasks.
%
\vuteco's architecture and training slightly vary depending on the task, as illustrated in Figure~\ref{fig:vuteco-detail}.
For the \finding task, the \finder model was trained and tested using a collection of test cases extracted from the projects in \VulforJ.
%
On the other hand, the \matching task required three training sessions: one for the \finder model for its \finding task, one for the \linker model for its simplified task (see Section~\ref{subsec:matching}), and an additional one integrating the trained \finder and \linker for the \matching task.
We remark that evaluating the \linker model on its simplified task only aims at assessing its suitability for the integrated model, which is the one addressing the \matching task.

%We experimented with different configurations of the \finder and \linker models in two isolated evaluation sessions, i.e., focusing on their respective sub-tasks, with no interaction with each other.
%The \finder and \linker models were trained and tested with several configurations in two independent evaluation sessions.
%Then, we proceeded with a third evaluation session focused on the integrated model, which leveraged the best configurations of the \finder and \linker models discovered in the previous sessions (see Sections~\ref{subsub:eval} and ~\ref{subsub:configs} for how the best configurations were determined).
%We remark that when encapsulated in the \globalModel model, the \finder and \linker models had to be fitted on new training sets to avoid using data that fell into the test set of the \globalModel model.
%
%To put it simply, we followed an approach resembling \textit{bottom-up} integration testing, where we first evaluated the two units (here the \finder and the \linker models) in isolation, and then we evaluated the \globalModel model in an integrated, and more realistic, context.
%Each session entailed the training and testing of several configurations to find the optimal way to architect and train the models (Section ~\ref{subsub:configs}). 

\subsubsection{Data Selection}
\label{subsubsec:data}

The primary source of data for the in-vitro evaluations was \VulforJ~\cite{bui:msr2022:vul4j}, as it is the only known source having \JUnit test cases matched with the vulnerabilities they are witnessing.
%
At the time of this paper writing, \VulforJ had \vulforjProjects \Java projects, of which \vulforjWitTests \JUnit test cases marked as witnessing tests.
From such projects, we checked out their patched revision (i.e., the project version in which the vulnerability has been fixed and where the witnessing tests have been found) and mined \vulforjNotTests more test cases from the same projects using the heuristic in Section~\ref{subsec:vuteco_overview} to have examples of tests not witnessing any vulnerability.
We fetched the metadata from the \vulforjVulns vulnerabilities in \VulforJ, such as the summary description and the weakness type, using the CVE Search API~\cite{cve:search}.
We ended up with \vulforjVulnsWithMetadata vulnerabilities with metadata, as three were reported via internal bug reports.

For the \finding task, we selected all the \JUnit test cases in \VulforJ, ignoring any vulnerability metadata.
%To evaluate the \finder model in isolation, we selected all the \JUnit test cases in \VulforJ, ignoring any vulnerability metadata.
Hence, all \vulforjWitTests tests have been labeled as \finderPosClass class (the positive class), while the remaining \vulforjNotTests tests have been labeled as \finderNegClass class (the negative class).
Then, we split this dataset using stratified sampling (i.e., keeping the same class distribution), creating a training set $F_{TR}$ (70\%), a development set $F_{DE}$ (15\%), and a test set $F_{TE}$ (15\%).
%The development sets were used to (1) design the general \vuteco's architecture, (2) select the relevant hyper-parameters worthy of investigation, and (3) assess the correctness of the scripts.

For the \matching task, two more datasets were needed, one for the \linker model and one for the integrated model.
For the former, we created pairs of vulnerability\-/witnessing tests and the metadata of the linked vulnerability in \VulforJ.
%, as this model assumes the given test method is witnessing a vulnerability, though without knowing which one.
%Indeed, this model aims to find the right vulnerability to link with the witnessing test (hence the name).
%All \vulforjWitTests tests have been paired with the related \vulforjVulnsWithMetadata descriptions.
Since three witnessing tests were matched to the three vulnerabilities without metadata, we ended up with \evalLinkerTrue valid pairs, marked as \linkerPosClass.
Then, we paired the \evalLinkerTrue tests with all the metadata of the vulnerabilities they do not witness, creating \evalLinkerFalse invalid links to form the \linkerNegClass class.
Then, we split this dataset with stratified sampling, creating a training set $L_{TR}$ (70\%), a development set $L_{DE}$ (15\%), and a test set $L_{TE}$ (15\%).
%
The dataset for the integrated model followed a similar construction.
The \evalGlobalTrue valid pairs of the \linker model also formed the positive class \globalPosClass.
However, in this case, the negative class \globalNegClass was made by pairing all test cases from \VulforJ projects (not just the witnessing tests) with the metadata of unrelated vulnerabilities affecting the same project, forming \evalGlobalFalse invalid pairs.
Again, we split this dataset with stratified sampling, creating a training set $I_{TR}$ (70\%), a development set $I_{DE}$ (15\%), and a test set $I_{TE}$ (15\%).
%
When testing the integrated model for the \matching task, we removed from $F_{TR}$ and $L_{TR}$ any test case and vulnerability appearing in $I_{TE}$ to avoid data leakages that could influence the evaluation results.
%We remark that for this session only, the \finder and \linker models had to be re-trained on their sub-tasks before the integration to avoid data leakages that could influence the evaluation results.
%Hence, we removed from $F_{TR}$ and $L_{TR}$ any data (test cases and/or vulnerabilities) that appeared in $I_{TE}$, creating the altered training sets $F'_{TR}$ and $L'_{TR}$.

\subsubsection{Performance Indicators}
\label{subsub:eval}

%All three model types carried out a \textbf{binary classification task}.
%Namely, the models output two logit scores, which are converted into probabilities through the Softmax function; in our case, the second value is the probability that an instance belongs to the positive class (\finderPosClass, \linkerPosClass, and \globalPosClass).
We relied on traditional metrics to measure the goodness of binary predictions, i.e., \textit{precision} (Pr), \textit{recall} (Re), \textit{F1 score}~\cite{powers:2011:precision:recall,baeza:1999:modern:information:retrieval}, the Area Under the Receiver Operator Characteristic curve (AUC-ROC)~\cite{Junge2018:roc}, and report the absolute number of positive classifications (i.e., True Positives and False Positives) to give them more context to the results.
%, and \textit{Matthews's correlation coefficient} (MCC)~\cite{matthews:1975:mcc}.
%
We argue that the main goal of \vuteco is to \textbf{maximize the number of correct findings and minimize the incorrect ones}, with the purpose of expanding the current knowledge base of witnessing tests to address the challenges described in Section~\ref{sec:intro}.
Given this circumstance, we include the \textit{F0.5 score} in the analysis, which is a varied version of the F1 score where twice as much weight is given to precision compared to recall~\cite{van1979:information:retrieval}.
During the analysis of the performance at test time, we give more attention to the F0.5 score as it captures the trade-off between precision and recall that aligns with the \vuteco's purpose.

%Besides, we also considered the Area Under the Receiver Operator Characteristic curve (AUC-ROC)~\cite{Junge2018:roc}.
%so as to understand if a different decision threshold could improve a model's classification performance.
%Indeed, a high AUC-ROC value indicates that the trade-off between true and false positives could be improved by changing the default decision threshold (i.e., $0.5$) to a different value.
%Hence, we recomputed all performance metrics at all different levels of the decision threshold used to build in the ROC curves and considered the performance related to the threshold that had determined the highest \metricForTest.
%We remark that changing the decision threshold does not change how the model behaves but only affects the metrics relying on the confusion matrix.

%Namely, when a model is run on a given input, it returns (two) logit scores; using the Softmax function, we obtained the probabilities of belonging to the two classes, and we selected the one with the highest value.
%We also selected such metrics for their suitability in measuring the classification performance on imbalanced test sets.
%We relied on the F1 score and MCC metrics as a first-level assessment to determine the most effective configurations.
%However, in case different configurations exhibited similar F1 and MCC scores (i.e., in the range of $\pm$0.05) and there were trade-offs between precision and recall, we \textbf{preferred those with higher precision}.
%Indeed, \vuteco also aims to reduce the search space by a vast amount (i.e., \textit{finding the needle in a haystack}), pinpointing interesting candidates.
%For this, we set the acceptable reduction to roughly three orders of magnitude, i.e., about one matched among 1,000.
%In the paper's replication package, we also reported the values of other metrics of secondary importance, like \textit{Accuracy} or \textit{Specificity}.\footref{fn:link}

\subsubsection{Configurations Experimented}
\label{subsub:configs}

A \textit{configuration} of a model is made of one or more \textbf{factors}, i.e., aspects we hypothesized could notably affect the performance, e.g., the algorithm used to augment the training set.
%On the other hand, with the latter, we refer to secondary aspects that can still affect the performance but to a lesser extent, such as the size of the hidden layers of the model's classification head.
Every time we had to train a model (\finder, \linker, and both together), we did it with different configurations and compared their performance on the related test sets to find the most suitable one.
The set of factors involved for each model type is in Table~\ref{tab:factors}.
%Each of the three evaluation sessions involved a set of factors based on their relevance to the respective models, reported in Table~\ref{tab:factors}.

All three models handled the class imbalance of their training sets by augmenting the data through \fndAug, \lnkAug, and \glbAug factors.
We experimented with three different mechanisms:
(1) \textbf{\JavaTransformer} (JT)~\cite{rabin:ist2021:javatransformer}, a \textsc{Java} command-line tool that transforms a \textsc{Java} method (including test methods) into semantically-equivalent clones by applying nine semantic-preserving transformation rules, like renaming variables or add random logging statements;
(2) \textbf{SPAT}~\cite{yu:jss2022:spat}, similar to \JavaTransformer, it creates semantically\-/equivalent clones of \textsc{Java} methods with 18 transformation rules;
(3) \textbf{Bootstrapping} (BS), a resampling technique creating exact copies of instances of the minority class, a.k.a. random oversampling.
%We relied on the implementation provided by \textsc{imbalanced-learn} \textsc{Python} library.\footnote{\textsc{imbalanced-learn}: \url{https://imbalanced-learn.org/}}
%
%We remark that \JavaTransformer and SPAT were applied only on the \JUnit test methods of the minority classes.
%That is, given a vulnerability-witnessing test $T$ linked with vulnerability $V$, for the \finder's training set, the generated methods $T'$, $T''$, etc. have been labeled as \finderPosClass, while for \linker's and \globalModel's training sets they have been paired with $V$ and labeled as \linkerPosClass and \globalPosClass, respectively.
We also experimented with a fourth case in which the training data were not augmented.

\input{tables/factors}

The \linker model accepts a vulnerability as input, whose metadata typically consists of two elements, i.e., the summary description and an assigned weakness type, identified using the CWE (Common Weakness Enumeration).
The former conveys most of the information about the vulnerability and is present in all valid CVE records, while the latter might be missing or incorrectly assigned~\cite{Pan:icse23:cwe}.
We experimented with two scenarios via the \lnkCwe factor, i.e., whether to prepend the CWE identifier and name (when available) before the summary description or not use it at all.
%\EI{Kill this factor and preserve only Concat-TD. If we want, we can put it as a TTV point, but no one will complain. This will change the nr. of linkers and the time required for it, cut in half, so it should be 50 hours rather than 100}
%Furthermore, we also evaluated the order of concatenation of the two textual inputs involved in the \linker model (i.e., the test case source code and the vulnerability metadata) via \lnkConcat factor; namely, we placed the vulnerability metadata before the test case (\textbf{Cat-DT}) and after (\textbf{Cat-TD}).

To keep the number of experimented configurations reasonable, the integrated model reused the best configurations of \finder and \linker models observed in their independent training and testing sessions.
%The integrated model reuses the best configurations of \finder and \linker models.
However, when used together for the \matching task, they were re-trained again on slightly modified training sets (see Section~\ref{subsubsec:data}).
In a way, this can be seen as a \textit{pre-training} session that we hypothesized could benefit the performance of the \matching task.
%Nevertheless, the benefit resulting from the integration of the two models and the need for prior pre-training is yet to be confirmed.
In this regard, we experimented with several ways of integrating the two models via the \glbMode factor:
(1) fine-tuning on $I_{TR}$ using only the \linker model without prior pre-training (\textbf{Linker-Only});
(2) fine-tuning on $I_{TR}$ using \finder and \linker together without prior pre-training (\textbf{FT-Only});
(3) using \finder and \linker together with prior pre-training but no fine-tuning on $I_{TR}$ (\textbf{PT-Only});
(4) using \finder and \linker together with prior pre-training and fine-tuning on $I_{TR}$ (\textbf{PT-FT}).
In PT-Only mode, the factor \glbAug is ignored as it does not involve $I_{TR}$, resulting in just one configuration with this mode.

In summary, we tested \finderConfigsLetter \finder configurations, \linkerConfigsLetter \linker configurations, and \globalConfigs integration configurations, trained for \finderTrainingEpochs, \linkerTrainingEpochs, and \globalTrainingEpochs epochs, respectively.
All training sessions used a dropout layer with $0.1$ probability between every linear layer added after the CodeBERT to reduce the risk of overfitting~\cite{hinton2012improving:dropout}.
The weights were updated using the AdamW optimizer~\cite{loshchilov2019decoupled:adamw}, with $10^{-5}$ learning rate decaying linearly, and the loss was measured using the cross-entropy function~\cite{Zhang2018:cross:entropy}.
%The configurations of a model were evaluated on the model's related test set, e.g., the \linker configurations were tested on $L_{TE}$, and the best one was chosen according to the \metricForTest.
%Similarly to what happens when training the \vuteco's deployment version (Section~\ref{subsub:train}), the \finder, \linker, and integration configurations have been trained for \finderTrainingEpochs, \linkerTrainingEpochs, and \globalTrainingEpochs epochs, respectively.
At the end of each epoch, we evaluated the model checkpoint on the related development set, returning the one with the highest \metricForDev and lowest loss (in case of ties).
%Only the checkpoints (evaluated at the end of each epoch) with the highest \metricForDev and lowest loss (in case of ties) were returned.
%\TEMP{We optimized according to the \metricForTest as it evaluates a binary classifier at multiple decision thresholds, postponing the search for the right threshold, which can be expensive during training.}
%Training the \finder and \linker configurations took about \TEMP{100 hours}, which were run in parallel as they were independent of each other.
%In comparison, the \globalModel configurations took about \TEMP{40} hours.
%We carried out \finderRuns train-test rounds for the \finder model, \linkerRuns for the \linker model, and \globalRuns for the \globalModel model.
The models have been implemented with \textsc{PyTorch} and trained with the \textsc{HuggingFace} API.
The evaluation has been carried out on a server with an NVIDIA Tesla A100 Core GPU and an Intel Xeon Platinum 8352V CPU.

\subsubsection{Hyper-parameter Tuning}
\label{subsub:hyperparams}

In addition to the factors, we selected other secondary aspects that might have some impact on the performance, such as the size of the hidden layers of a model's classification head.
We considered such aspects as \textbf{hyper-parameters} that we optimized during the model's training; namely, for each configuration, we searched the best hyper-parameter combination by comparing the models on the performance on the development set, selecting the one having the highest \metricForDev and lowest loss.
During the analysis of the in-vitro results for both tasks (Section~\ref{sec:results}), we only report the performance on the test set, leaving the full report in the paper's online appendix~\cite{appendix}.
%
%Therefore, we only considered the configurations with optimized hyper-parameters to address \rqOne.
%The full report of the performance of all the hyper-parameter combinations is in our appendix~\cite{appendix}.

All three models use training data augmentation, which can happen to different ``extents''; we call this hyper-parameter $E$.
\JavaTransformer (JT) and SPAT can be run multiple times to generate further semantically equivalent clones as they are driven by certain randomness elements, e.g., to decide the new variable names.
Bootstrapping (BS), instead, can generate instances of the minority class until a new imbalance ratio is reached; for example, $0.25$ means that the minority instances should be 25\% of the total.
Hence, for JT and SPAT, we set $E$$=$$\{5, 15, 25\}$ (discarding any duplicate instances), while for BS, we set $E$$=$$\{0.25, 0.50, 0.75\}$.
When no data augmentation is done (\textit{None}), this hyper-parameter is ignored.
%
Besides, for the \finder and \linker models, we searched for the right size of the two hidden layers before the output layer, using $H_1$$=$$\{768, 512, 256\}$ and $H_2$$=$$\{256, 128, 64, 0\}$, respectively.
%The size of the two hidden layers in \linker and \finder models can be configured with \fndHOne, \fndHTwo, \lnkHOne, and \lnkHTwo.
%\glbThres determines the minimum required probability of a test case of class \finderPosClass (according to the \finder model) to be sent to the \linker model.
%Removing newlines and consecutive blank characters in test methods can be toggled on or off with \fndOneLine and \lnkOneLine.

%to render them into single representations that could be handled by the deep neural network for the classification.
%Therefore, \lnkMerge assumed two possible values:
%\begin{itemize}[leftmargin=*]
%\item \textbf{Cat-DT} (previously mentioned in Section~\ref{sec:vuteco}), which concatenates vulnerability description and the test method (in this order) and places [SEP] special token in between, which tells the CodeBERT model that the whole input is made of two sentences and returns the combined embeddings in [CLS]. 
%\item \textbf{Cat-TD} is like Cat-DT but concatenates the two inputs in reverse order.
%\item \textbf{Learn}, which uses the CodeBERT model twice to get the separate sentence embeddings of the test methods and vulnerability description and then places an extra linear layer of size 1536 x 768 with GELU as an activation function to transform the two embeddings into one new 768-long representation. This way, the model will learn how to merge the two sentences properly during its training.
%\end{itemize}

% \begin{itemize}[leftmargin=*]
% \item \textbf{PT-FT} (described in detail in Section~\ref{subsub:train}), which trains the \finder and \linker models on their training sets ($F'_{TR}$ and $L'_{TR}$, respectively), before fine-tuning the \globalModel model on its training set $I_{TR}$.
% \item \textbf{PT-Only}, which only pre-train the \finder and \linker models on $F'_{TR}$ and $L'_{TR}$, without additional fine-tuning.
% \item \textbf{FT-Only}, which only fine-tune the \globalModel model on $I_{TR}$, leaving the default weights for \finder and \linker models.
% \end{itemize}

\subsubsection{Baseline Approaches}
\label{subsub:baselines}

To the best of our knowledge, \vuteco is the first solution targeting the problem of matching tests and vulnerabilities; thus, no other approaches can be used for a direct comparison.
Nevertheless, we developed some \textit{baseline approaches} for each model tested, relying on mechanisms more lightweight than those used in \vuteco.
%To this end, we developed a \textit{baseline approach} for each of the three sessions as a means of comparison; they rely on lightweight mechanisms and set the lower bounds that the \finder, the \linker, and the \globalModel models must surpass to be deemed effective.

A baseline for the \finder model should extract facts from the test cases using a different principle than those used by the \finder model.
%The task of the \finder model consists of analyzing a test case and determining if it is witnessing a vulnerability.
%Hence, the baseline approach is supposed to extract facts from the test cases using a different principle than those used by the \finder model.
We developed a heuristic algorithm that extracts \textit{keywords} from the test case and checks (via case-insensitive match) if these appear in a vocabulary of keywords fitted on the test cases in the training set $F_{TR}$.
If the number of keywords found in the vocabulary surpasses an arbitrary threshold $N$, the test case is flagged as \finderPosClass.
We extracted keywords in two ways: (i) using YAKE~\cite{campos:2020:yake}, which extracts the $K$ most relevant keywords from a text in an unsupervised manner, and (ii) using a custom script to retrieve all the identifiers (i.e., variable and method names) in the test case, as they likely indicate the purpose of the test and are not mixed with other irrelevant programming keywords.
We call these two flavors of the approach \yakeVocab and \identVocab, respectively.
We experimented with $N$$=$$[1..10]$ for both baselines; for \yakeVocab we experimented with $K$$=$$\{5,10,15,20,25,30\}$; while for \identVocab we experiment the honoring \texttt{camelCase} and \texttt{snake\_case} notations, e.g., if identifiers like \texttt{user\_Password} must be split into \texttt{user} and \texttt{password} or not.
In total, 80 variants have been evaluated; in Section~\ref{sec:results}, we present only the best variants (in terms of \metricForTest) for both flavors.
%The baseline for the \finder model consisted of a heuristic algorithm called \devNames that checks if a test case contains certain terms based on a vocabulary fitted on $F_{TR}$.
%Namely, all the ``developer-defined terms,'' i.e., method and variable names, are extracted from the tests in $F_{TR}$ and stored in a vocabulary.
%\devNames checks how many terms in a test case appear in the fitted vocabulary (via case-insensitive match); if the number of matched terms surpasses an arbitrary threshold $T_n$, the test case is flagged as \finderPosClass.
%We called this approach \devNames.
%We experimented with two hyper-parameters.
%The first decided whether additional sub-terms must be extracted from identifiers honoring \texttt{camelCase} or \texttt{snake\_case} notations (e.g., from \texttt{user\_Password} it extracts \texttt{user} and \texttt{password} as well).
%The second determined $T_n$, experimenting with integers ranging from $1$ to $5$.
%In total, ten \devNames variants were built. 

A baseline for the \linker model should check whether there is a connection between the test case and the vulnerability description.
%The task of the \linker model consists of checking whether there is a connection between the test case and the vulnerability description.
%This can be done by checking if the two text inputs are somehow similar.
Hence, we developed a heuristic algorithm that checks the \textit{similarity} between the two textual inputs.
If the similarity surpasses an arbitrary threshold $S$, the pair is flagged as \linkerPosClass.
We made two different implementations: (i) extracting the keyword sets from both text inputs using YAKE~\cite{campos:2020:yake} and comparing them using the Jaccard index; (ii) extracting the embeddings from both text inputs using a pre-trained CodeBERT model~\cite{feng:emnlp2020:codebert} and comparing them using the cosine similarity.
We call these two flavors of the approach \yakeSimil and \codebertSimil, respectively.
For the former, we experimented with $S$$=$$\{0.01,0.02,0.03,0.04,0.05,0.1\}$ and $K$$=$$\{5,10,15,20,25,30\}$, while for the latter we experimented with $S$$=$$\{0.9,0.95,0.96,0.97,0.98,0.99\}$.
In total, 42 variants have been evaluated; in Section~\ref{sec:results}, we present only the best variants (in terms of \metricForTest) for both flavors.
%
%The baseline approach for the \linker model is called \keywords and relies on YAKE~\cite{campos:2020:yake} to extract the top $K$ most relevant keywords from a text.
%YAKE was run on the test case and the vulnerability description, obtaining two keyword sets.
%Then, the Jaccard index was computed to measure the similarity across the two keyword sets; if it surpassed an arbitrary threshold $T_k$, the pair was classified as \linkerPosClass.
%We experimented with $K$$=$$\{5,10,15,20,25,30\}$ and $T_k$$=$$\{0.01,0.02,0.05,0.1,0.15,0.2,0.25,0.5\}$, building a total of 48 \keywords variants.

%The baseline approach for the \globalModel model followed a different principle, using a different piece of information from the vulnerability metadata, i.e., the set of known fixing commits.

Regarding the baseline for the integrated model, we followed a different principle by employing an approach called \fixCommits, which relies on the assumption that developers create unit tests alongside the patches to show that the vulnerability was fixed successfully.
\fixCommits does not look at the vulnerability description but rather inspects all fix commits of a vulnerability to gather the set of test cases $TM$ added or modified and flags that vulnerability and all the tests in $TM$ as \globalPosClass pairs.
Any other pair is automatically considered as \globalNegClass.
We observe that there is no sufficient evidence that its core assumption generally holds cases as developers might not create any test when fixing vulnerabilities
%and not all tests created might be related to the vulnerability.
For example, in CVE-2010-0684 of \textsc{ActiveMQ} none of the three fix commits (\texttt{2895197}, \texttt{fed39c3}, and \texttt{9dc43f3}) added any test.
%in CVE-2014-0112 of \textsc{Apache Struts} a witnessing test was added in \texttt{149181a7}, two commits after the fixing commit \texttt{2e2da292}.
For this, \fixCommits is to be considered as a heuristic approach rather than a ground truth.
%; nevertheless, our goal in this experimentation was also to evaluate how it addresses this task.
%In this respect, the \VulforJ dataset shows that in some cases, the vulnerability-witnessing test happens to be added in the context of the fixing commit; still, there are cases in which this does not happen.

%Then, it leverages \textsc{CodeShovel}~\cite{grund:icse2021:codeshovel}---a tool constructing the change history of individual \Java methods---to match the tests in $TM$ to the test cases appearing in the test suite at the inspected revision (e.g., the latest revision).
%The tests correctly matched (i.e., they could be found after the fix commits) are deemed \globalPosClass to the input vulnerability.

\subsection{In-vivo Evaluation Design}
\label{subsec:invivo}

During the in-vivo evaluation, we assessed the \textit{precision} of the results obtained by \vuteco for both tasks due to the lack of ground truth---indeed, this evaluation aims at assessing its usefulness \textit{in the wild}.
Namely, the \finding task returns a list of test cases that \vuteco believes to be security-related, while the \matching task returns a list of pairs of test cases matched with their vulnerabilities.
%
We prepared \vuteco for both tasks using the best configurations resulting from the respective in-vitro evaluation (partially revealed in Section~\ref{sec:vuteco}).
This consisted of re-training the models used in each task by merging the content of the test sets in the related training sets (as there was no use for it anymore).
The rest of the training followed the setting described in Section~\ref{subsub:configs}.
%Once we found the best configuration for the three \vuteco's building blocks, we used it to build \vuteco (the same setting presented in Setting~\ref{sec:vuteco}).
%In this phase, we employed \vuteco---with the setting described in Section~\ref{sec:vuteco} resulting from \rqOne---in real-world \Java projects to collect instances of vulnerability-witnessing tests we are not aware of.
%We were interested in comprehending if \vuteco behaved somehow as it did during the in-vitro evaluation.

We selected open-source \Java projects and their vulnerabilities from the latest version of \projectKB~\cite{ponta:msr2019:projectkb} available in its \textsc{GitHub} repository~\cite{projectkb:github}.
First, we excluded the vulnerabilities in \VulforJ, as they could have been used to train \vuteco---and so biasing the final results---and also since we were not interested in their witnessing tests.
With this step, we discarded 60 vulnerabilities.
We discarded 11 more vulnerabilities whose metadata could not be found in \projectKB or be retrieved with \textsc{CVESearch} API.
Then, we assessed their projects' accessibility, discarding those without a remote URL or no longer accessible, which determined the removal of 92 more vulnerabilities.
%
Since we had no guidance on when the witnessing tests could have been added, we selected the latest revision of each project (i.e., the HEAD of the respective base branches) on June 13, 2024.
We believe witnessing tests addressing past vulnerabilities can still be found in current test suites.
Therefore, the input given to \vuteco for both tasks was a project's repository (remote URL) and its latest revision.
Besides, for the \matching task, \vuteco also requires the list of historical vulnerabilities that affected it (via \projectKB).
%
At this point, we applied the test selection criteria in Section~\ref{subsec:vuteco_overview} to ensure the projects had valid test suites.
This revealed \invivoProjectsWithNoTests projects without any test that \vuteco could process; thus, we discarded the related \vulnsWithNoTests vulnerabilities.
%
After all these steps, we ended up with \invivoProjects projects and \invivoVulns vulnerabilities.
%
The \invivoProjects projects comprised a total of \invivoTests test cases, all given to \vuteco during the \finding task as input.
For the \matching task, instead, we first paired all the test cases with only the vulnerabilities affecting the related project, and then \vuteco processed each pair one by one.

To assess the validity of the results, we relied on two independent researchers with experience in software security and testing and familiarity with \VulforJ.
They autonomously inspected all the findings with a predicted probability of belonging to the positive classes (of the respective tasks) of at least $0.5$.
They were instructed to mark the correct and incorrect ones, allowing the computation of true and false positive predictions and, therefore, the precision.
%Due to the large-scale nature of this evaluation, we still let \vuteco return all the matches according to its default probability threshold (i.e., $0.5$); however, only the matches with at least $0.8$ probability were subject to the manual inspection to provide the two researchers a reasonable workload.
We relied on Cohen's Kappa score~\cite{cohen:1960:kappa,McHugh:2012:cohen} to measure agreement between the two raters; all cases of disagreement were jointly discussed and solved until a full agreement was reached.
%As the first step of the process, the two inspectors ran an alignment phase on a subset of the results to tune their inspection style and select the aspects to focus on.
%The sample was made such that it contained \TEMP{XXX\%} results flagged by \vuteco.
%Afterward, the two inspectors could proceed independently, splitting the remaining tests equally.

%\EI{Corrently, if we use the standard classification threshold to 0.5 we get about 100 tests. In this context, we can use both inspectors to validate ALL tests, without an initial alignment.
%In principle, AUC-ROC is meant to find the optimal threshold. However, in our case we want to maximize the True Positives and have a reasonable work size. We know that we can achieve both things by just increasing the treshold (True Positives cannot decrease if the model is more conservative). In practice, we make the model return all the predicted probabilities and we can validated the Top results beyond the selected threshold (TBD)
%}

%\TEMP{
%It is worth noting that \vuteco returns the probability that a relationship is correct.
%Nevertheless, only the instances with at least $0.95$ probability were manually analyzed for this evaluation to give the inspectors a reasonable workload.
%}
%Furthermore, we were interested in comprehending the collection capabilities of \vuteco, so we opted to return the probability of belonging \globalPosClass class rather than the sharp belonging class, leaving the inspectors to decide how to use this additional information. 
