% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{listings}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% import
\usepackage{xcolor}  % グラフィックス関連
\usepackage{pxrubrica}        % ルビ
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{comment}
\usepackage{makecell}
\usepackage{color}
\usepackage{pifont}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{listings}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{courier}
\usepackage{adjustbox}


\newcommand{\yes}{\ding{55}}%
\newcommand{\no}{\ding{51}}%
\newcommand{\pkg}[1]{\textsf{#1}}
\newcommand{\code}[1]{\texttt{#1}}

\definecolor{red}{rgb}{0.8,0,0}
\definecolor{green}{rgb}{0,0.8,0}
\definecolor{blue}{rgb}{0,0,0.8}
\definecolor{yellow}{rgb}{0.90,0.91,0.75}
\definecolor{purple}{rgb}{0.85,0.80,0.91}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\cyan}[1]{{\color{cyan}#1}}
\newcommand{\green}[1]{{\color{green}#1}}
\newcommand{\gray}[1]{{\color{gray}#1}}
\newcommand{\ToGray}[0]{\color{gray}}
\newcommand{\ToBlack}[0]{\color{black}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{codeblue}{rgb}{0.25,0.5,0.75}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{codeorange}{rgb}{1,0.6,0}
\definecolor{codeyellow}{rgb}{0.7,0.7,0}
\definecolor{codeteal}{rgb}{0,0.5,0.5}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{codeblue},
    numberstyle=\tiny\color{black}, % Set numbers to black
    stringstyle=\color{black}, % Set strings to black
    basicstyle=\footnotesize\ttfamily,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    morekeywords={null,true,false},
    keywordstyle=\color{codeblue}, % Keywords like null, true, false in blue
    morestring=[b]",
    stringstyle=\color{black}, % Strings in black
    literate=
     *{0}{{{\color{black}0}}}{1}
      {1}{{{\color{black}1}}}{1}
      {2}{{{\color{black}2}}}{1}
      {3}{{{\color{black}3}}}{1}
      {4}{{{\color{black}4}}}{1}
      {5}{{{\color{black}5}}}{1}
      {6}{{{\color{black}6}}}{1}
      {7}{{{\color{black}7}}}{1}
      {8}{{{\color{black}8}}}{1}
      {9}{{{\color{black}9}}}{1}
      {"}{{{\color{black}"}}}{1} % Quotes in black
      {:}{{{\color{codeblue}:}}}{1} % Colon in blue
      {,}{{{\color{black},}}}{1} % Comma in blue
      {\{}{{{\color{codeblue}\{}}}{1} % Opening curly brace in blue
      {\}}{{{\color{codeblue}\}}}}{1} % Closing curly brace in blue
      {[}{{{\color{codeblue}[}}}{1} % Opening square bracket in blue
      {]}{{{\color{codeblue}]}}}{1}, % Closing square bracket in blue
}

\lstset{style=mystyle}
\lstset{linewidth=\textwidth}


\newcommand{\TODO}[1]{\emph{\color{red}[TODO: #1]}}
\newcommand{\FIX}[1]{{\color{green}#1}}
\newcommand{\FIXME}[1]{{\color{red}#1}}
\newcommand{\rcOurs}[0]{\rowcolor[rgb]{0.85,0.98,0.87}}
\newcommand{\ccO}[0]{\cellcolor[rgb]{0.85,0.98,0.87}}
\newcommand{\breakInTable}[1]{\begin{tabular}{c} #1 \end{tabular}}
\newcommand{\datasetName}[0]{LegalViz\xspace}
\renewcommand{\paragraph}[1]{\noindent\textbf{#1}.\hspace{2pt}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{LegalViz: Legal Text Visualization by Text To Diagram Generation}


\author{
    Eri Onami$^{1,2}$   
    Taiki Miyanishi$^{3,6,2}$ 
    Koki Maeda$^{4,7}$ 
    Shuhei Kurita$^{5,7,2}$\\
      $^{1}$Nara Institute of Scient and Technology 
      $^{2}$RIKEN AIP 
      $^{3}$The University of Tokyo \\
      $^{4}$Institute of Science Tokyo 
      $^{5}$National Institution of Informatics
      $^{6}$ATR
      $^{7}$NII LLMC\\
      \texttt{onami.eri.ob6@is.naist.jp},
      \texttt{taiki.miyanishi@weblab.t.u-tokyo.ac.jp},\\
      \texttt{koki.maeda@nlp.c.titech.ac.jp},
      \texttt{skurita@nii.ac.jp} \\}


\begin{document}
\maketitle
\begin{abstract}
Legal documents including judgments and court orders require highly sophisticated legal knowledge for understanding.
To disclose expert knowledge for non-experts, we explore the problem of visualizing legal texts with easy-to-understand diagrams and propose a novel dataset of \datasetName with 23 languages and 7,010 cases of legal document and visualization pairs, using the DOT graph description language of Graphviz.
\datasetName provides a simple diagram from a complicated legal corpus identifying legal entities, transactions, legal sources, and statements at a glance, that are essential in each judgment.
In addition, we provide new evaluation metrics for the legal diagram visualization by considering graph structures, textual similarities, and legal contents.
We conducted empirical studies on few-shot and finetuning large language models for generating legal diagrams and evaluated them with these metrics, including legal content-based evaluation within 23 languages. Models trained with LegalViz outperform existing models including GPTs, confirming the effectiveness of our dataset.
\end{abstract}

\section{Introduction}
Driven by the rapid advancements in large language model (LLM) performance~\citep{NEURIPS2020_1457c0d6, OpenAI2023GPT4TR}, 
adaptation to specialized domains in Natural Language Processing (NLP) receives increasing attention in many fields~\citep{lu2022learn,kung2023performance,legalbench2023}.
Specifically, the application of LLMs to the legal field holds the potential to automate significant tasks and support roles traditionally occupied by lawyers and judges~\citep{choi2021chatgpt,frankenreiter2022natural}.
The understanding of legal documents poses unique challenges for NLP applications, as legal reasoning requires not only interpreting the surface utterance but also implicit rules often omitted from the legal documents.
It also requires following legal syllogisms, understanding the implications of related regulations, and applying them to specific case facts to deduce the final consequences.

\begin{figure}[t]
    \centering
    \includegraphics[width=8cm]{Figures/LegalViz_Fig1_v1.pdf}
    \caption{Model input and expected output of legal text visualization drawn by Graphviz.}
    \label{fig:teaser}
\end{figure}

At an early stage of legal NLP, there are several studies applying traditional NLP approaches for legal documents, such as named entity recognition~\citep{Angelidis2018NamedER, 10.1007/978-3-319-99722-3_32, pais-etal-2021-named, de-gibert-bonet-etal-2022-spanish}, summarization~\citep{elaraby-litman-2022-arglegalsumm, aumiller-etal-2022-eur}, text classification~\citep{chalkidis-etal-2019-extreme} and text segmentation~\citep{10.1145/3462757.3466085}.
In addition, some notable studies focus on capturing the structural legal meanings inherent in legal documents, such as learning judgment facts and results~\citep{niklaus-etal-2021-swiss}, assessing the fairness of law~\citep{chalkidis-etal-2022-fairlex}, and using the facts and attributes to predict charges~\citep{hu-etal-2018-shot}.
However, there are still numerous gaps between current legal domain status and whole automation of legal tasks by LLMs, especially judgement generation. 

The main challenges of LLM for legal applications are as follows:
(i) LLMs need to understand which legal entities are involved, the relationships between them, and the relevant legal rules.
They must also interpret the meaning of legal actions in judgments and court decisions.
If models overlook legal entities, their rights, their obligations, or key facts in interpreting the law, they fail to fulfill legal requirements, or deduce inappropriate conclusions.
(ii) LLMs must articulate why quoted rules are interpreted in their view, explaining the requirements and effects of the rules should become as they assert.
This process should adhere to the procedure of legal syllogism, requiring the recognition of potential legal entities, relationships of them, and related rules.
To address these challenges, extensive legal document resources are crucial for effectively tuning LLMs to perform well in legal domains.
However, despite the wide accessibility of plain texts of laws and court judgments on the internet,
there remains a significant lack of legal datasets with professional annotations that can enhance the capability for legal syllogism.
Moreover, LLM technology should enhance legal adaptation capabilities, supporting not only professionals but also non-experts, as everyone has legal rights and should have the opportunity to benefit from the law.

To meet these demands, we explore the novel dataset \datasetName, an automatic visualization task, generating legal diagrams that describe the legal entities, their legal relationship, related rules, and summary of the key legal facts for legal interpretation from input legal plain texts.
We introduced this visualization task with an existing diagram visualization tool of GraphViz because diagrams can succinctly elucidate complex legal relationships, allowing viewers to grasp the essentials at a glance without consulting the original article.
In fact, the visualization of legal concepts is employed in various contexts, such as textbooks for judicial examinations, university classrooms, and TV news segments.
This approach provides non-experts with easy-to-interpret visual and conceptual representations of legal materials, enhancing accessibility and understanding.
By training with our dataset, models can accurately recognize legal rules concerned in the case, identify legal entities capable of exercising rights, and understand legal transactions, and statements from professional legal documents. 
\datasetName consists of 7,010 pair professional legal documents and diagrams of DOT language code of Graphviz, with 23 languages of EUR-LEX.
Figure~\ref{fig:teaser} from the LegalViz dataset illustrates a legal diagram that explains a case where the commission required Germany to recover aid based on the common market principles, and Germany subsequently issued recovery requests.
We assume LegalViz is the first work to utilize LLMs for the visualization of legal documents.\footnote{Our dataset is available at \url{https://github.com/mizuumi/LegalViz}.}

% \url{https://github.com/mizuumi/LegalViz}


\if 0

Our contributions are summarized as follows:
%\vspace{-3pt}
\begin{enumerate}
  \item We introduce a novel dataset of LegalViz, which establishes a new task of generating diagram visualizations from legal documents, covering 23 languages from EUR-LEX.
  \item We proposed an evaluation method to assess accuracy of the legal entities, norms, relations understanding, taking into account quality of diagram formalization, legal structure understanding, and summarization of legal statements of each parties.
  \item We conducted extensive empirical studies on \datasetName and observed the effectiveness of our dataset both quantitatively and qualitatively, especially our finetuned models overperformed GPT models.
\end{enumerate}

\fi

\begin{figure*}[t]
    \centering
    \includegraphics[width=16cm]{Figures/LegalViz_Fig2_v1.pdf}
    \caption{Legal text from EUR-LEX (left) to the resulting legal graph (right).
    }
    \label{fig:task}
\end{figure*}

\section{Related Work}

NLP applications in the legal domain are several core areas~\citep{Katz2023NaturalLP} such as information extraction, classification, summarization, judgment prediction, and resources and benchmarks.

\paragraph{Judgment prediction}
In this task, models predict the outcomes of legal cases based on given facts.
Previous studies provide judgment data from various courts of diverse countries, including decisions from the Supreme Court of the United States~\citep{10.1371/journal.pone.0174698} and the European Court of Human Rights~\citep{Medvedeva2020-MEDUML, Kaur2019ConvolutionalNN}.
Additionally, judgment prediction research has covered Switzerland~\citep{niklaus-etal-2021-swiss}, Chinna~\citep{ye-etal-2018-interpretable}, criminal law~\citep{chen-etal-2019-charge, Xiao2018CAIL2018AL}, and asylum decisions~\citep{10.1145/3086512.3086538, 10.1145/3086512.3086537}.

\paragraph{Legal resources and benchmarks} 
Datasets and benchmarks, covering a broad range of legal domains and languages, have been proposed.
These include English Tax Law~\citep{Holzenberger2020ADF}, European Legislation and the European Court of Human Rights~\citep{chalkidis-etal-2019-extreme}, Corporate and Contract Law~\citep{hendrycks2021cuad, tuggener-etal-2020-ledgar}, Supreme Court cases and US court cases~\citep{10.1145/3462757.3466088}.
The scope extends to German legal cases~\citep{icaart21}, a mixture of Korean legal text summarization, prediction and text classification~\citep{10.5555/3600270.3602628}, and refugee cases~\citep{barale-etal-2023-asylex}. 
Additionally, multilingual and multi-legal domain datasets have been developed, such as a multilingual corpus of English, German, Italian, Polish ~\citep{drawzeski-etal-2021-corpus},
and LEXGLUE~\citep{chalkidis-etal-2022-lexglue} which covers six predictive tasks over five datasets made of English from the US, EU, and Council of Europe.
Furthermore, Lexfiles~\citep{chalkidis-etal-2023-lexfiles} offers a comprehensive dataset of comprised of US, UK, Canada, India, European Court of Human Rights, and Lextreme~\citep{niklaus-etal-2023-lextreme}, which covers wide-range of tasks and countries among EU nations.
However, none of these datasets are designed to support the visualization of legal documents for non-experts.
In contrast, \datasetName offers legal specific annotations in 23 multilingual legal documents, specifically tailored for visualization.
These annotations cover legal entities, their relationship, related rule, related facts of legal texts, thereby enhancing the clarity and interpretation of legal documents for judicial judgments.

\if[]
Our \datasetName contributes to identification of legal entity, their relationship, related rule, related facts of legal texts, providing 
Once those four elements are identified from dispute backgrounds, judges can generate legal decisions using law identified by model and applying entities and facts to the legal requirements.
\fi


\paragraph{Text to graph generation}
Following the iconic successions of the GPT models, LLMs can generate not only contextual texts and program codes~\citep{shi-etal-2022-natural, christopoulou-etal-2024-text} but also visualization codes~\citep{SparksofAGI}, such as creation of scientific vector graphics with TiKZ~\cite{automatikz}
%the TiKZ dataset~\cite{automatikz} 
and diagram generation with refinements and diffusion process~\cite{Zala2023DiagrammerGPT}.
Text-to-code generation studies are predominantly focused on mainstream programming languages like Python and shell scripts, and are typically examined with English text~\citep{shi-etal-2022-natural, christopoulou-etal-2024-text}.
Both text-to-graph generation and graph-to-text generation studies are often conducted for clarifying paragraph structure and summarizing critical issues and relationship between words~\citep{koncel-kedziorski-etal-2019-text, jin-etal-2020-genwiki} of the input plain texts mainly in English.
These text-to-graph approaches are suitable for free drawing based on text instructions, but they sacrifice the visualization of logical relationships within the visualized content. 
In comparison, our graph generation approach utilizes the DOT language of Graphviz, enabling models to focus specifically on visualizing the logical relationships within the content.


\section{Building LegalViz Dataset}

\subsection{Legal Visualization}
\label{sec:legalviz}
The aim of legal visualization tasks is to generate an interpretable graph that clarifies the legal relationships embedded within the input legal texts. 
The constructed graph comprises legal entities and/or rules as nodes, connected by edges representing legal transactions and/or significant facts relevant for judicial determination. 
To effectively visualize these legal relationships, we utilize the DOT language of Graphviz, a widely adopted open-source tool for graph visualization. 
Figure~\ref{fig:task} presents an overview of our proposed task, showcasing both the expected input and output.


\paragraph{Legal entity}
Legal entities are applicants and respondents of judgment, courts, creditors, debtors, criminal suspects, or companies and employees.
Legal entities are represented in \textbf{double octagons}.
In contrast to grammatical general nouns, proper nouns, or objects, we concentrate on persons or organizations capable of exercising legal rights and engaging in transactions.

\paragraph{Legal relationship \& transactions}
Legal relationships encompass various form of relationships between legal entities, including the exercise of legal rights from one to another, legally significant transactions, the interrelations between legal statements made by entities and the underlying norms that support them, and relationships defined under law such as employment, contractual agreements, marriage, and family relationships.
Legal transactions are specific types of relations among legal entities, such as purchases, notifications and any actions exercising rights.
Both legal relationships and transactions are represented in the directed or undirected \textbf{edges} with various styles between legal entities.
Some edges have textual relation labels.

\paragraph{Legal source}
Legal sources are rules applied or referred by court and support legal statements. 
Here we concentrate on legal sources explicitly written in the legal document.
They include constitutions, statutes, ordinances, and case laws.
These extracted rules are represented in \textbf{trapeziums}.
Each trapezium of the legal source is connected to a node interpreting rules supported by the legal source via undirected edges.

\paragraph{Legal statement}
Legal statements are detailed explanations of transactions and factual descriptions of the case notable for the final judgment to summarize.
Adding these summaries to diagrams help non-experts grasp the facts important for final judgments at a glance.
Legal statements are represented in \textbf{squares}, connected to a node by an edge.



\begin{table}[t]
\centering
  \footnotesize
\begin{tabular}{lrrr}
\toprule
\textbf{Split} & \textbf{\# Instances} & \textbf{\# Nodes} & \textbf{\# Relations}\\
\midrule
Train & 4,710 & 12,624 & 16,367 \\
Validation & 1,150 & 3,404 & 2,717 \\
Test & 1,150 & 3,128 & 3,589 \\
\midrule
Total & 7,010 & 19,156 & 22,673 \\
\bottomrule
\end{tabular}
\caption{Dataset splits.}
\label{table:dataset_split}
\vspace{-1em}
\end{table}


\begin{table}[t]
\centering
  \footnotesize
\begin{tabular}{lccccc}
\toprule
\textbf{Lang.} & \textbf{ISO} & \textbf{\# Ins.} & $L_{\mathrm{word}}$ & $L_{\mathrm{char}}$ & $L_{\mathrm{code}}$ \\
\midrule
All & - & 7,010  &  109.0  &  644.2  &  759.8 \\
\midrule
Bulgarian & BG & 290  &  113.4  &  625.5  &  759.8 \\
Spanish & ES & 307  &  133.7  &  693.4  &  708.4 \\
Czech & CS &  307  &  102.8  &  582.9  &  832.5 \\
Danish & DA & 307  &  110.9  &  640.7  &  766.8 \\
German & DE & 312  &  108.9  &  683.0  &  759.1 \\
Estonian & ET & 307  &  83.9  &  588.8  &  809.4 \\
Greek & EL &  307  &  121.4  &  698.6  &  779.2 \\
English & EN & 312  &  122.6  &  629.2  &  623.0 \\
French & FR &  312  &  128.6  &  674.8  &  766.9 \\
Croatian & HR & 263  &  103.2  &  577.7  &  718.7 \\
Italian & IT &  312  &  123.3  &  705.1  &  788.7 \\
Latvian & LV & 307  &  94.4  &  598.8  &  725.7 \\
Lithuanian & LT & 307  &  94.6  &  609.4  &  749.8 \\
Hungarian & HU & 307  &  97.4  &  670.2  &  809.7 \\
Maltese & MT & 305  &  100.4  &  706.3  &  777.7 \\
Dutch & NL & 312  &  122.0  &  687.0  &  784.7 \\
Polish & PL & 307  &  106.7  &  655.0  &  759.2 \\
Portuguese & PT &  307  &  125.2  &  653.1  &  778.0 \\
Romanian & RO & 290  &  118.3  &  672.0  &  791.8 \\
Slovak & SK &  308  &  101.0  &  585.7  &  727.9 \\
Slovenian & SL & 308  &  106.6  &  580.0  &  730.5 \\
Finnish & FI & 308  &  78.5  &  649.6  &  808.7 \\
Swedish & SV & 308  &  108.9  &  639.1  &  748.2 \\
\bottomrule
\end{tabular}
\caption{
Dataset statistics.
$L_{\mathrm{word}}$ and $L_{\mathrm{char}}$  are length of legal text. $L_{\mathrm{code}}$ is character length of Graphviz code. 
}
\label{table:dataset_statistics_1}
\vspace{-1em}
\end{table}



\subsection{Dataset Creation}
\paragraph{Collection of legal document}
To construct the legal graph dataset, we collected legal documents in the following manner:
(i) We sourced legal documents from the EUR-LEX website\footnote{\url{https://eur-lex.europa.eu}}, which provides public access to judgments, orders, and rules of EU countries in official EU languages.
We specifically selected judgments from the years 2006 to 2019, available in translations across 23 languages, to capture the latest legal trends. 
(ii) We extracted the factual sections of the judgments that contain legal facts to be expressed in the graph. 
(iii) Finally, we gathered the corresponding sections of legal documents in 23 languages to ensure consistency across translations.

\paragraph{Graphviz annotation}
We have manually annotated Graphviz code visualization from the legal documents by an annotator with expertise in the legal domain.
The process involved several steps:
(i) We broke down long judgment cases into short paragraphs so that DOT language can draw diagrams in units easily understandable at a glance.
(ii) We extracted the legal entities and rules as nodes of the diagram, legal transactions as edge relations within the diagram, and the summary of statements and explanations as squared nodes.
(iii) We created a Graphviz diagram to represent the extracted relations, using variations in node shape and relations, as defined by the rules in Appendix~\ref{sec:graph_formalism}.

\paragraph{Translation of Graphviz annotation}
To cover the European Union's official languages present at the time the judgment was written, we translated our English annotation to other languages as follows:
(i) We first used GPT-4 to extract the legal words and sentences from the provided English judgements, aiming to save as many terms as possible from the EU's officially translated variations of judgments.
(ii) If GPT-4 fails the extraction task, we then apply GPT-4 translations from English to other languages.
(iii) We manually checked the previously translated sentences and retranslated them using DeepL and the Azure GPT API if any translation errors were detected.
The prompts used in the translation process are described in Appendix~\ref{sec:appendix}.

\paragraph{Dataset statistics}
Table~\ref{table:dataset_split} shows the statistics of the \datasetName dataset.
We build a total of 7,010 pairs of legal texts and graphs, encompassing 23 language variations and more than 300 unique legal texts.
The constructed legal graph consists of 19,159 nodes and 22,673 relations.
We also summarize the average word length, number of characters in legal sentences, and character length of Graphviz code for each language in Table~\ref{table:dataset_statistics_1}.




\section{Evaluation}
\label{sec:eval}
We compare the reference and hypothesis graphs to assess the quality of the generated.
One straightforward way to achieve this is to directly compare two images visualized by GraphViz.
However, this approach clearly ignores the textual structure of the legal documents.
One other approach is directly comparing the DOT language codes in textual manner, ignoring numerous minor differences of the visualization codes that can result in different graphs.
Therefore we propose an approach to first compose graphs, align the graph components, and then compare each component of graphs using textual metrics as described in this section.


\input{tables/main_result}

\subsection{Similarity of two graphs with texts}
Formally, let $\mathcal{G}_r$ and $\mathcal{G}_h$ be the reference and hypothesis graphs.
Each graph is composed of a set of edges $E$ and nodes $V$.
When an edge $e\in E$ connects a starting node $v_s$ and an end node $v_e$, it is represented by a tuple $e=[v_s,v_e,l]$, where $l$ is a label of this edge.
Nodes always include non-empty texts, while edge-label texts can be blank for edges without labels.

\paragraph{DOT code validation}
First, we examine whether the generated Graphviz code forms a valid graph $\mathcal{G}_h$ in terms of the DOT language. This is done by simply processing with the pydot library\footnote{https://github.com/pydot/pydot}.

\paragraph{Nodes alignment by bipartite matching}
Second, we extract nodes $V_h$ from $\mathcal{G}_h$ and align them with nodes from the reference graph: $V_r$ from $\mathcal{G}_r$ using the similarity of the texts in nodes.
For this node alignment, we apply the bipartite matching problem for the two sets of nodes $V_h$ and $V_r$, using the similarity function $\mathrm{sim}(v_r,v_h)$ between two texts in the reference $v_r \in V_r$  and hypothesis nodes $v_h \in V_h$ with a textual similarity metric.
Here we use BERTScore~\cite{bertscore} for the similarity metric because of its robustness and high human correlation.
Given the textual similarity scores between all reference and hypothesis nodes, we apply a bipartite matching solver in NetworkX\footnote{https://networkx.org/} for nodes and obtain the nodes alignment between the reference and hypothesis graphs that are used for later evaluation.

\paragraph{Graph, node \& edge evaluation}
After we determined the node alignment, we performed the comparison of the two graphs based on the nodes, edges and their labels.
We introduce the following three metrics with different depth:
\texttt{Graph}, \texttt{Graph\&Node} and \texttt{Graph\&Node\&Edge}.

\texttt{Graph} is an F1 metrics of the matched edges after the node alignment by bipartite matching. 
This metric is for the similarity measurement of the entire graph structure, ignoring the textual differences of nodes and edges after the alignment.
Let the node set $V_r$ and edge set $E_r$ composes reference graph $G_r$, and the node set $V_h$ and edge set $E_h$ composes hypothesis (generated) graph $G_h$.
Using a node alignment function $a(\cdot): V_h \to \{V_r,\phi \}$ from the generated graph nodes $V_h$ to the reference graph nodes $V_r$ and Kronecker delta $\delta_{\mu \nu} = 1$ iif $\mu=\nu$ otherwise $\delta_{\mu \nu} = 0$, which represents the agreement of the nodes here,
we compute the agreement score of the nodes as
\begin{align}
    \nonumber f_{\mathrm{Graph}}(e_h,e_r)&=\delta_{a(v_{s,h}) v_{s,r}} \delta_{a(v_{e,h}) v_{e,r}}
\end{align}.
$v_{s,h}$ and $v_{s,r}$ are the start nodes of each edge from the hypothesis (generated) and reference graphs, while $v_{e,h}$ and $v_{e,r}$ are the end nodes of it, respectively. 
The generated nodes can be aligned to $\phi$ when they are not aligned to any reference nodes: $v_h \xrightarrow{a(\cdot)} \phi$. Here $\phi$ is a null node, and we assume for any nodes $\nu$, $\delta_{\phi\nu}=0$. From this binary function $f_{\mathrm{Graph}}(e_h,e_r)$, we can compute TP, FP and FN by
\begin{align}
\nonumber 
    \mathrm{TP} &= \sum_{e_h \in E_h,e_r \in E_r} f_{\mathrm{Graph}}(e_h,e_r) \\
    \nonumber 
    \mathrm{FP} &= |E_h|-\mathrm{TP}, \ \ \ \ \ ~~ 
    \mathrm{FN} = |E_r|-\mathrm{TP}
\end{align}
and obtained F1 value from these for \texttt{Graph}.

\texttt{Graph\&Node} is an F1-based metric where BERTScore penalize the dissimilar texts of the two aligned nodes pairs $\{v_{s,h}, v_{s,r} \}$ and $\{v_{e,h}, v_{e,r} \}$ for each edge. TP is calculated as
\begin{align}
\nonumber 
    \mathrm{TP} = \sum_{e_h \in E_h,e_r \in E_r} &f_{\mathrm{Graph}}(e_h,e_r) \ \cdot \\ 
    \nonumber &\mathrm{sim}(v_{s,r},v_{s,h})\mathrm{sim}(v_{e,r},v_{e,h})
\end{align}
while FP and FN are calculated from TP.
Because of the products of the start and end node similarities, the \texttt{Graph\&Node} metric is sensitive to the difference of node texts compared with the \texttt{Graph} metric.


Similarly, \texttt{Graph\&Node\&Edge} is an F1-based metric that considers node and edge text similarity in terms of the BERTScore as following
\begin{align}
\nonumber 
    \mathrm{TP} = &\sum_{e_h \in E_h,e_r \in E_r} f_{\mathrm{Graph}}(e_h,e_r) \ \cdot \\
    \nonumber &~~~~~~\mathrm{sim}(v_{s,r},v_{s,h})\mathrm{sim}(v_{e,r},v_{e,h}) \mathrm{sim}(l_r,l_h)
\end{align}
by penalizing dissimilar texts of the edge texts.

\subsection{Evaluation of Legal Content}
\label{sec:legalvizeval}
We also introduce the evaluation of legal contents in our visualizations. As described in Sec.~\ref{sec:legalviz}, the legal contents in LegalViz are associated with specific diagrams in GraphViz. For diagrams of legal entities (\textbf{double octagon}), Legal source (\textbf{trapeziums}), Legal statement (\textbf{squares}), we extract these nodes from the reference ($v_r$) and hypothesis ($v_h$) graphs and check whether they are properly aligned in the alignment in the previous section. Then we measure the similarity of the node texts with BERTScore for successfully aligned nodes and compute micro averaged F1 score as following $\mathrm{TP}$, $\mathrm{FP}$, $\mathrm{FN}$:
\begin{align}
    \mathrm{TP} &= \sum_{v_h \in V_h, v_r \in V_r} \delta_{v_r,v_h} \mathrm{sim}(v_r,v_h) \nonumber \\ 
    \mathrm{FP} &= |\{v_h\}|-\mathrm{TP} \nonumber \\
    \mathrm{FN} &= |\{v_r\}|-\mathrm{TP}
    \nonumber
\end{align}
where $\delta_{v_r,v_h}=1$ iff $v_r$ and $v_h$ is aligned, and otherwise 0.
For legal relations \& transactions (\textbf{edges}), we extract the aligned edge label texts and compute F1 score from the similarity of labels.

\input{tables/multi_lang_parts}

\section{Experiments}
We evaluate the ability to visualize graphs from legal sentences with \datasetName.
Overall, our finetuned models overperformed fewshot GPT models.

\subsection{Experimental settings}
We conduct the experiments of legal visualization in the manner of the DOT language code generation with the publicly available Llama and Gemma family models and OpenAI GPT APIs via Microsoft Azure. We use the GPT-3.5-Turbo (\texttt{1106}), GPT-4 (\texttt{0613}) and GPT-4o (\texttt{2024-05-13}) models.
For Llama family models, we experimented with the models specialized for code generation of CodeLlama~\citep{Rozire2023CodeLO} and the recently released Llama 3.1 \& 3.2 models~\citep{dubey2024llama3herdmodels} and Gemma 2-9B~\citep{Riviere2024Gemma2I} models.
Experimental settings are two holds: few-shot generation and finetuning of the publicly available models.
In few-shot experiments, we notice not only the GPT models but only publicly available models are capable of producing valid DOT language codes without finetuning to some extent.
We follow the supervised finetuning of Hugging Face with the detailed finetuning parameters in Appendix~\ref{app:param}.
In evaluation, we generate ten different Graphviz codes with each model and examine their quality in evaluation methods of graph and legal contents described in Sec.~\ref{sec:eval}.


\subsection{Result}
\paragraph{Graph-based Evaluation}

The Graph-based Evaluation section of Table~\ref{table:generation_performance} presents the experimental results of each model evaluated by \texttt{Graph}, \texttt{Graph\&Node}, and \texttt{Graph\&Node\&Edge} metrics explained in Section~\ref{sec:eval}.
Among three evaluation metrics, \texttt{Graph\&Node\&Edge} is the most difficult because all three graph elements must be correct as shown in the evaluation.
Most importantly, our finetuned models outperformed few-shot counterparts and even GPT models, which are assumed to be larger than the CodeLlama-13B models, suggesting the effectiveness of our dataset for finetuning. 
Also, finetuned Gemma-2-9B took the highest scores on \texttt{Graph} and \texttt{Graph\&Node}, and \texttt{Graph\&Node\&Edge}. Surprisingly, Gemma-2-9B performed worse than Gemma-2-9B-it before finetuning, suggesting the effectiveness of finetuning with LegalViz.


\begin{figure*}[t]
    \centering
    \includegraphics[width=15.5cm]{Figures/fig3.pdf}
    \caption{Qualitative analysis of diagrams by Graphviz code. Figures are generated by the finetuned modesl of Gemma2-9B, CodeLlama-13B-Instruct, few shot models of GPT-3.5-Turbo, GPT-4 and an annotated diagram.}
    \label{fig:qualitative}
\end{figure*}

\paragraph{Valid DOT code ratio}
The Valid Graph section of Table~\ref{table:generation_performance} presents the success rate of forming a valid DOT language code without code syntax errors.
In the first generation trial, GPT-4 is the most accurate to generate valid DOT language codes among all models in both few-shot and finetuned settings and the second best is Gemma2-9B finetuned model.
When we let models generate ten variations, several finetuned models (Llama-3.1-8B, Gemma2-9B, Gemma2-9B-it) are able to generate valid DOT code in 100.0 percents of the test set.
Comparing the publicly available few-shot and finetuned models, finetuning with our dataset strongly improves the valid graph creation of all models.


\paragraph{Legal Content Evaluation}
The Legal Content section of Table~\ref{table:generation_performance} presents the legal aspect-wise evaluation as described in Sec.~\ref{sec:legalvizeval}.
By nature of the legal entities, \texttt{Entity} can be extracted from input sentences in many conditions, achieving high scores in the table.
However, the other three aspects aren't easily extracted from the input legal text.
\texttt{Statement} includes the text generation for legal facts and tends to be lower scores than others. 
This is because legal statements appear in texts without some remarkable keywords, compared to legal \texttt{Source}, which is often mentioned in texts with terminology such as ``Law'' and ``Act'' and legal \texttt{Relations \& Transactions}, which is found in texts with terminologies such as ``contract,'' ``issue'' with some warrants and orders, ``notification'' with notable as a legal act.
\texttt{Statement} acts for summarizing notable facts related to rule and its interpretation in question, to describe the detail of other nodes especially legal relations and transactions, and to explain the facts applicable to legal requirements.
Finetuned \texttt{Gemma2-9B} achieved highest score in all four aspects of the legal content evaluation.
The scores in \texttt{Statement} are improved by approximately three times compared to the few-shot scores across all models, suggesting the effectiveness of finetuning with our dataset.

\paragraph{Scores by languages}
Table~\ref{table:scores_by_language} presents the results in legal contents by all 23 languages in EUR-LEX.
We present the best performing model of Gemma2-9B in finetuned and fewshot conditions. We also present the averaged results of 10 models in Table~\ref{table:generation_performance} without GPTs to highlight the performance difference before and after finetuning with LegalViz across languages, while minimizing the influence of individual model characteristics.
Among these languages, models perform relatively weakly in languages that have relatively fewer resources~\citep{chalkidis-etal-2021-multieurlex}, such as Maltese, Latvian, Lithuanian, 
 Hungarian. For languages that have relatively more resources such as English and French, models tend to have higher scores than others.
From a linguistic point of view, Hungarian and Finnish, belonging to the same Uralic language group, have low scores in each model. This may reflect their linguistic difference from other languages.
Similarly, for the Romance language group, e.g., Romanian, French, Spanish, Italian, and Portuguese, models have moderate performances, seemingly better than those of the Uralic language group and languages that also have fewer resources than those of English and French.
Among four legal aspects, the source and statement parts include the summarization task of the legal document for visualization. They are considerably difficult parts in the graphs and the performance in some languages becomes 0 without finetuning.
It is also notable that finetuning contributes the performance in all aspects in all of these languages.

\subsection{Qualitative Analysis}
\label{sec:qualitative}
We conduct a qualitative analysis using the best performing model of the finetuned Gemma2-9B and CodeLlama-13B Instruct models, along with the few-shot GPT-3.5-Turbo and GPT-4.
Figure~\ref{fig:qualitative} presents the graphs generated from English legal documents along with the annotated graph.
This legal document used for the model input is on the Appendix~\ref{app:inuput}.
This is a part of a criminal procedural case in which the prosecutor requested the court to declare securing custody where the prosecutor and the court are legal entities.
Square nodes in annotated data are describing intention of request and consequence of the request written in order. 


For fewshot models of GPT-3.5-Turbo and GPT-4, GPT-3.5-Turbo wrongly recognize that all nodes are legal entities and illustrate them in double octagon.
Its description of the legal relationship between the court and the authority is also incorrect because the court didn't take the legal action directly in this article. 
The GPT-4 model assumes that ``JR'' and ``French authorities'' are legal entities but fail to illustrate relationships between those and other entities as these entities lack connections to other graph parts. 
Also, GPT-4 extracted two different court names from given text as different entities. However they are indeed different divisions of the same court and fails to summarize the relationship between them.
Gemma2-9B ft. successfully extracts the detailed description of court's request, while it extracted incorrect entity as double octagon shape and it couldn't extract the court order and its description.
CodeLlama-13B-Instruct ft. model successfully forms the graph structure but the square node mentioned European arrest warrant in the right bottom is inconsistent with input text.


\section{Conclusion}
We have proposed \datasetName, the first manually annotated dataset to visualize legal text with DOT language Graphviz and introduced a novel evaluation method taking into account both diagram visualization quality and sentences of not only graph nodes and relations but also legal contents.
We empirically confirmed the effectiveness of our dataset with wide-range of experiments including comparisons of few-shot and finetuning models and demonstrated trained models outperform the closed models of GPTs in all evaluation metrics.

\section*{Limitation}
\datasetName contains the same number of instances in
23 languages of EUR-LEX. However, this doesn't mean that the models with fintuned or few-shot have the same ability to treat all 23 languages equally. Especially models face difficulties in fewer language resources as we experimented.
We cannot offer any warranty for using our dataset and models for real usages such as legal advice. We also consider that our dataset should be used with appropriate supervision by experts. This can be a \textit{potential risk} when our dataset is misused. We assume that results of automatic visualizations by models are still different from the annotated visualizations in most cases, suggesting the current limitation of the generation.


\section*{Ethic Statements}
The annotation material of this dataset is publicly available EU legal materials including judgments and orders, which do not include personal or sensitive information, with the exception of trivial information presented by consent, e.g., the names of the active presidents of the European Parliament, European Council, or other official administration bodies. The copyright for the editorial content of this website, the summaries of EU legislation, and the consolidated texts, which are owned by the EU, is licensed under the Creative Commons Attribution 4.0 International license.\footnote{https://eur-lex.europa.eu/content/legal-notice/legal-notice.html}


\section*{Acknowledgment}
This work was supported by JSPS Grant-in-Aid for Young Scientists (JP22K17983), 
 JSPS Fostering Joint International Research (A) (JP22KK0184)
 and JST CRONOS JPMJCS24K6.


% Bibliography entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
% Custom bibliography entries only
%\bibliography{custom}


\onecolumn
\appendix


\section{Qualitative analysis input}
\label{app:inuput}
The English legal text used the qualitative analysis in Section~\ref{sec:qualitative} is the following:

\noindent\fbox{%
    \parbox{\textwidth}{%
On 28 May 2019, the procureur d’État du Luxembourg (State Prosecutor, Luxembourg) requested that the chambre du conseil du tribunal d’arrondissement de Luxembourg (Investigation Chamber of the District Court, Luxembourg) declare that JR should be surrendered to the French authorities. By order of 19 June 2019, the chambre du conseil du tribunal d’arrondissement de Luxembourg (Pre-trial Chamber of the District Court, Luxembourg) stated that it had no jurisdiction to hear JR’s application for a declaration that the European arrest warrant was invalid and granted the request for the surrender of JR to the French authorities.
    }%
}


\section{Detailed experimental settings}
\label{app:param}

For training of LLMs, we follow the default setting of Hugging Face supervised finetuning of the trl\footnote{https://github.com/huggingface/trl} library for the optimizers and schedulers. We use the mini-batch size of 32. We use the max token length of 4096 for training as we notice some languages, e.g., Greek, require longer tokens than other languages depending on Llama tokenizers. In finetuning, we use FP32 precision and all trainable parameters are updated. All Llama-family experiments are done on a single node with four NVIDIA A100 GPUs.


\input{tables/multi_lang_parts_appendix}
\input{tables/multi_lang_gne_appendix}
\input{tables/main_result_appendix}


\section{Results of the Validation split}
Table~\ref{table:main_result_appendix} shows the results of the validation split of the main performance table of Table~\ref{table:generation_performance}.


\section{Additional Multilingual Experiments}
We experimented several models listed in the tables below and selected models with great scores are discussed in main paper.

\paragraph{Multilingual results of all models}
We conducted experiments using the following models, namely, Llama3.1-8B, Llama3.1-8B-Instruct, Llama3.2-3B, Llama3.2-3B-Instruct, CodeLlama-7B, CodeLlama-7B-Instruct, CodeLlama-13B, CodeLlama-13B-Instruct, Gemma2-9B, and Gemma2-9B-it.
Firstly, the results evaluated with legal content evaluation point of view are shown in Table~\ref{table:scores_by_language_all_fewshot} and Table~\ref{table:scores_by_language_all_finetuned}.
Table~\ref{table:scores_by_language_all_fewshot} shows few-shot Legal Content evaluation of all models conducted and Table~\ref{table:scores_by_language_all_finetuned} shows finetuning results of Legal Content evaluation.  
Secondly, the results evaluated with graph-based point of view are summarized in Table~\ref{table:scores_by_language_fewshot} and Table~\ref{table:scores_by_language_finetuned}.
Table~\ref{table:scores_by_language_fewshot} shows few-shot results of graph-based evaluation and Table~\ref{table:scores_by_language_finetuned} represents.

For comparison of each language's score, we calculated the average of all few-shot models and the average of few-shot models despite GPT models in Table~\ref{table:scores_by_language_all_fewshot} and Table~\ref{table:scores_by_language_fewshot}.
In the same way, the average score of all finetuned models is calculated in Table~\ref{table:scores_by_language_all_finetuned} and Table~\ref{table:scores_by_language_finetuned}.


\section{Applications of traditional NLP tasks for legal domain}
\paragraph{Legal information extraction}
Named Entity Recognition (NER) is a fundamental information extraction task that has been developed for several languages, including 
Greek~\citep{Angelidis2018NamedER}, Brazilian~\citep{10.1007/978-3-319-99722-3_32}, Romanian~\citep{pais-etal-2021-named}, and Spanish~\citep{de-gibert-bonet-etal-2022-spanish}.
Those NER approaches extract mainly the same objects as those in non-legal domains, while some efforts try to extract legal entities from court documents~\citep{Chapter11LexNLPNaturallanguageprocessingandinformationextractionforlegalandregulatorytexts}.
Once NER identified entities, Relation Extraction in the legal domain~\citep{chalkidis-etal-2021-paragraph} takes this information further by identifying and classifying the relationships between these entities, such as facts and allegedly violated articles, specific articles and paragraphs, and case references, as well as relevant facts and allegations.

\paragraph{Legal classification} 
The classification task of legal texts has been proposed with a focus on practical applications.
For example, to enhance the interpretation of complex legal information, multi-label classification of legal texts assigns multiple conceptual class labels to words appearing in legal sentences \citep{chalkidis-etal-2019-extreme}.
Notably, FairLex~\citep{chalkidis-etal-2022-fairlex} aims to ensure the fair application of the law by classifying attributes such as age, gender, region, and state.


\paragraph{Legal summarization} 
As a more complex and application-oriented task, legal summarization is also prominent in the field, which aims to generate a summary of legal sentences.
Existing summarization studies address Canadian legal cases~\citep{elaraby-litman-2022-arglegalsumm} and EU legislations~\citep{aumiller-etal-2022-eur}.


\section{Legal Diagram Formalism}
\label{sec:graph_formalism}

Here we define several rules to express legal relations within the DOT language grammar.

\paragraph{Graph node rules}
Legal entities are represented by nodes (vertices) in DOT languages with the shape of double octagons except legally deceased persons who are presented in the shape of ellipses.
Legal norms that are effective in the present case are represented by graph nodes with  trapezium shapes.


\paragraph{Graph edge rules}
Legal transactions and the explanatory relationships between legal entities are represented by directed edges.
The family or marital relationships established under civil law are represented by an undirected bold edge.
The legal rights that cannot be exercised are represented by dashed edges. 
Dotted edges denote relationships of the legal succession between legal entities.

To illustrate the equivalent relationship between diagram nodes, undirected edges are used to connect entities and their status explanations, rules and statements, legal transactions, and their explanations.

We also note that legal relations can also be represented by graph nodes when legal relations have some relations with other entities.
Figure~\ref{fig:annotation} explains how to draw graphs when additional description is required for graph relations.
In Graphviz, we cannot draw lines directly to the graph relations.
Hence we change graph labels to nodes and connect to other nodes for adding explanation.
Further details of the DOT language grammar for representations of legal entity relations and an actual dataset example are provided in Appendix~\ref{app:anno rule}~\&~\ref{app:dataset}.

\begin{figure}[t]
    \centering
    \includegraphics[width=7.5cm]{Figures/annotation_rules.pdf}
    \caption{
    Annotation rule when adding explanation to graph relations.
    }
    \label{fig:annotation}
\end{figure}


\section{Graphviz annotation}
\label{app:anno rule}
The following is an example of the Graphviz code annotation rules.

\lstset{
    frame=single,
    numbers=left,
    tabsize=2
}
\begin{lstlisting}
[shape=doubleoctagon]: Entities which are capable to act as legal entity.
[shape=trapezium]: Any kinds of rules which are legally effective, applied to the present case or supporting legal statements.
[style=dotted]: Relationship of succession between 2 entities.
[dir=none]: Equivalent relationship, agreements, or connecting detailed explanation of other nodes.
[dir=none, style=bold]: Marital relationships or family relationships which have been established under civil law.
[style=dashed]: Expressing a legal right that cannot be exercised or not existed.
[shape=ellipse]: Expressing a person who is legally deceased.
\end{lstlisting}

\section{Qualitative Analysis}
Figure~\ref{fig:qualitative_appendix} shows an additional case of qualitative analysis. 
Here, GPT-3.5-Turbo and GPT-4 failed to generate correct relational graph since some nodes (``Public prosecutors in France'' and ``Article 6(1) of Framework Decision 2002/584'') are not connected to other nodes lacking understandings of legal relations.
On the other hand, Gemma2-9B ft. and CodeLlama13B-Instruct ft. models fine-tuned with LegalViz output almost the same diagram as annotated data, with the same legal entities (``JR'' and ``The Court of Appeal in Luxembourg'') and correctly extracted reason of appeal.

\begin{figure}[t]
    \centering
    \includegraphics[width=15cm]{Figures/NAACLAppendix.pdf}
    \caption{
    Additional qualitative analysis.
    }
    \label{fig:qualitative_appendix}
\end{figure}

\section{Prompt}
\label{sec:appendix}
The prompt for LLMs used in training, generation and dataset creation is presented in Table~\ref{tab:app-prompt}.

\input{tables/prompt_table}

\section{Train dataset examples}
\label{app:dataset}
\subsection*{Dataset Example}
\begin{center}
\begin{tabular}{c}
{\scriptsize
\begin{lstlisting}[breakatwhitespace=false]
{'ID': '45',
 'category': 'EU law',
 'diagram_number': '7',
 'case_name': 'Case T-207/02: Nicoletta Falcone v Commission of the\nEuropean Communities',
 'case_number': 'C2005/006/64',
 'document_url': 'https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:C2005/006/64&qid=1713891140330',
 'year': '2004',
 'text': 'In Case T-207/02: Nicoletta Falcone, a candidate in Competition COM/A/10/01, represented by M. Condinanzi, against Commission of the European Communities (Agent: J. Currall, assisted by A. Dal Ferro, with an address for service in Luxembourg) - application for annulment of the decision of 2 May 2002 of the selection board in Competition COM/A/10/01 to exclude the applicant from the written tests on the ground that she did not obtain sufficient marks to be included among the 400 best candidates - the Court of First Instance (Second Chamber), composed of J. Pirrung, President, A.W.H. Meij and N. Forwood, Judges; H. Jung, Registrar, has given a judgment on 26 October 2004, in which it:',
 'Graphviz': 'digraph {\n    rankdir=LR;\n    node [shape=box];\n\n    "Nicoletta Falcone" -> "M. Condinanzi" [label="represent" dir=none];\n    "The Comission of the European Comminities" -> "Nicoletta Falcone" [label="application for annulment of the decision of 2 May 2002 of the selection board in Competition COM/A/10/01 to exclude the applicant from the written tests on the ground that she did not obtain sufficient marks to be included among the 400 best candidates"];\n}',
 'language': 'English'
}
\end{lstlisting}
}
\end{tabular}
\end{center}


\end{document}
