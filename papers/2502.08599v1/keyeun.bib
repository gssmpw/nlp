@incollection{kross2017self,
  title={Self-distancing: Theory, research, and current directions},
  author={Kross, Ethan and Ayduk, Ozlem},
  booktitle={Advances in experimental social psychology},
  volume={55},
  pages={81--136},
  year={2017},
  publisher={Elsevier}
}

@article{mead1934mind,
  title={Mind, self, and society from the standpoint of a social behaviorist.},
  author={Mead, George Herbert},
  year={1934},
  publisher={Chicago}
}


@article{thissen2002quick,
  title={Quick and easy implementation of the Benjamini-Hochberg procedure for controlling the false positive rate in multiple comparisons},
  author={Thissen, David and Steinberg, Lynne and Kuang, Daniel},
  journal={Journal of educational and behavioral statistics},
  volume={27},
  number={1},
  pages={77--83},
  year={2002},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}



@article{naeimi2019validating,
  title={Validating self-reflection and insight scale to measure readiness for self-regulated learning},
  author={Naeimi, Leila and Abbaszadeh, Mahsa and Mirzazadeh, Azim and Sima, Ali Reza and Nedjat, Saharnaz and Hejri, Sara Mortaz},
  journal={Journal of Education and Health Promotion},
  volume={8},
  number={1},
  pages={150},
  year={2019},
  publisher={Medknow}
}

@article{wang2023measuring,
  title={Measuring user competence in using artificial intelligence: validity and reliability of artificial intelligence literacy scale},
  author={Wang, Bingcheng and Rau, Pei-Luen Patrick and Yuan, Tianyi},
  journal={Behaviour \& information technology},
  volume={42},
  number={9},
  pages={1324--1337},
  year={2023},
  publisher={Taylor \& Francis}
}

@misc{sindermann2021assessing,
  title={Assessing the Attitude towards artificial intelligence: introduction of a short measure in German, Chinese, and English language. KI K{\"u}nstliche Intelligenz. 35 (1), 109--118 (2021)},
  author={Sindermann, C and Sha, P and Zhou, M and Wernicke, J and Schmitt, HS and Li, M and Sariyska, R and Stavrou, M and Becker, B and Montag, C},
  year={2021}
}

@article{wang2024patient,
  title={PATIENT-$\{$$\backslash$Psi$\}$: Using Large Language Models to Simulate Patients for Training Mental Health Professionals},
  author={Wang, Ruiyi and Milani, Stephanie and Chiu, Jamie C and Eack, Shaun M and Labrum, Travis and Murphy, Samuel M and Jones, Nev and Hardy, Kate and Shen, Hong and Fang, Fei and others},
  journal={arXiv preprint arXiv:2405.19660},
  year={2024}
}

@article{li2024leveraging,
  title={Leveraging Large Language Model as Simulated Patients for Clinical Education},
  author={Li, Yaneng and Zeng, Cheng and Zhong, Jialun and Zhang, Ruoyu and Zhang, Minhao and Zou, Lei},
  journal={arXiv preprint arXiv:2404.13066},
  year={2024}
}

@article{cohen_does_2018,
	title = {Does {Character} {Similarity} {Increase} {Identification} and {Persuasion}?},
	volume = {21},
	issn = {1521-3269, 1532-785X},
	url = {https://www.tandfonline.com/doi/full/10.1080/15213269.2017.1302344},
	doi = {10.1080/15213269.2017.1302344},
	language = {en},
	number = {3},
	urldate = {2023-12-16},
	journal = {Media Psychology},
	author = {Cohen, Jonathan and Weimann-Saks, Dana and Mazor-Tregerman, Maya},
	month = jul,
	year = {2018},
	pages = {506--528},
}

@inproceedings{cheng_marked_2023,
	address = {Toronto, Canada},
	title = {Marked {Personas}: {Using} {Natural} {Language} {Prompts} to {Measure} {Stereotypes} in {Language} {Models}},
	shorttitle = {Marked {Personas}},
	url = {https://aclanthology.org/2023.acl-long.84},
	doi = {10.18653/v1/2023.acl-long.84},
	language = {en},
	urldate = {2024-06-13},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Cheng, Myra and Durmus, Esin and Jurafsky, Dan},
	year = {2023},
	pages = {1504--1532},
	file = {Full Text:/Users/keyeun/Zotero/storage/YAYURUU9/Cheng et al. - 2023 - Marked Personas Using Natural Language Prompts to.pdf:application/pdf},
}

@misc{santurkar_whose_2023,
	title = {Whose {Opinions} {Do} {Language} {Models} {Reflect}?},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2303.17548},
	doi = {10.48550/ARXIV.2303.17548},
	abstract = {Language models (LMs) are increasingly being used in open-ended contexts, where the opinions reflected by LMs in response to subjective queries can have a profound impact, both on user satisfaction, as well as shaping the views of society at large. In this work, we put forth a quantitative framework to investigate the opinions reflected by LMs -- by leveraging high-quality public opinion polls and their associated human responses. Using this framework, we create OpinionsQA, a new dataset for evaluating the alignment of LM opinions with those of 60 US demographic groups over topics ranging from abortion to automation. Across topics, we find substantial misalignment between the views reflected by current LMs and those of US demographic groups: on par with the Democrat-Republican divide on climate change. Notably, this misalignment persists even after explicitly steering the LMs towards particular demographic groups. Our analysis not only confirms prior observations about the left-leaning tendencies of some human feedback-tuned LMs, but also surfaces groups whose opinions are poorly reflected by current LMs (e.g., 65+ and widowed individuals). Our code and data are available at https://github.com/tatsu-lab/opinions\_qa.},
	urldate = {2024-06-13},
	publisher = {arXiv},
	author = {Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori},
	year = {2023},
	note = {Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Computers and Society (cs.CY)},
}

@inproceedings{kang_values_2023,
	address = {Singapore},
	title = {From {Values} to {Opinions}: {Predicting} {Human} {Behaviors} and {Stances} {Using} {Value}-{Injected} {Large} {Language} {Models}},
	shorttitle = {From {Values} to {Opinions}},
	url = {https://aclanthology.org/2023.emnlp-main.961},
	doi = {10.18653/v1/2023.emnlp-main.961},
	language = {en},
	urldate = {2024-06-14},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Kang, Dongjun and Park, Joonsuk and Jo, Yohan and Bak, JinYeong},
	year = {2023},
	pages = {15539--15559},
	file = {Full Text:/Users/keyeun/Zotero/storage/M7YFA86M/Kang et al. - 2023 - From Values to Opinions Predicting Human Behavior.pdf:application/pdf},
}

@inproceedings{han_meet_2022,
	address = {Seattle, United States},
	title = {Meet {Your} {Favorite} {Character}: {Open}-domain {Chatbot} {Mimicking} {Fictional} {Characters} with only a {Few} {Utterances}},
	shorttitle = {Meet {Your} {Favorite} {Character}},
	url = {https://aclanthology.org/2022.naacl-main.377},
	doi = {10.18653/v1/2022.naacl-main.377},
	language = {en},
	urldate = {2024-06-14},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Han, Seungju and Kim, Beomsu and Yoo, Jin Yong and Seo, Seokjun and Kim, Sangbum and Erdenee, Enkhbayar and Chang, Buru},
	year = {2022},
	pages = {5114--5132},
	file = {Full Text:/Users/keyeun/Zotero/storage/PDV59U8W/Han et al. - 2022 - Meet Your Favorite Character Open-domain Chatbot .pdf:application/pdf},
}

@inproceedings{sang_tvshowguess_2022,
	address = {Seattle, United States},
	title = {{TVShowGuess}: {Character} {Comprehension} in {Stories} as {Speaker} {Guessing}},
	shorttitle = {{TVShowGuess}},
	url = {https://aclanthology.org/2022.naacl-main.317},
	doi = {10.18653/v1/2022.naacl-main.317},
	language = {en},
	urldate = {2024-06-14},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Sang, Yisi and Mou, Xiangyang and Yu, Mo and Yao, Shunyu and Li, Jing and Stanton, Jeffrey},
	year = {2022},
	pages = {4267--4287},
	file = {Full Text:/Users/keyeun/Zotero/storage/XDVV7MT9/Sang et al. - 2022 - TVShowGuess Character Comprehension in Stories as.pdf:application/pdf},
}

@article{premack_does_1978,
	title = {Does the chimpanzee have a theory of mind?},
	volume = {1},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X00076512/type/journal_article},
	doi = {10.1017/S0140525X00076512},
	abstract = {Abstract
            
              An individual has a theory of mind if he imputes mental states to himself and others. A system of inferences of this kind is properly viewed as a theory because such states are not directly observable, and the system can be used to make predictions about the behavior of others. As to the mental states the chimpanzee may infer, consider those inferred by our own species, for example,
              purpose
              or
              intention
              , as well as
              knowledge, belief, thinking, doubt, guessing, pretending, liking
              , and so forth. To determine whether or not the chimpanzee infers states of this kind, we showed an adult chimpanzee a series of videotaped scenes of a human actor struggling with a variety of problems. Some problems were simple, involving inaccessible food – bananas vertically or horizontally out of reach, behind a box, and so forth – as in the original Kohler problems; others were more complex, involving an actor unable to extricate himself from a locked cage, shivering because of a malfunctioning heater, or unable to play a phonograph because it was unplugged. With each videotape the chimpanzee was given several photographs, one a solution to the problem, such as a stick for the inaccessible bananas, a key for the locked up actor, a lit wick for the malfunctioning heater. The chimpanzee's consistent choice of the correct photographs can be understood by assuming that the animal recognized the videotape as representing a problem, understood the actor's purpose, and chose alternatives compatible with that purpose.},
	language = {en},
	number = {4},
	urldate = {2024-06-14},
	journal = {Behavioral and Brain Sciences},
	author = {Premack, David and Woodruff, Guy},
	month = dec,
	year = {1978},
	pages = {515--526},
	file = {Full Text:/Users/keyeun/Zotero/storage/IQWNR9MK/Premack and Woodruff - 1978 - Does the chimpanzee have a theory of mind.pdf:application/pdf},
}

@misc{yuan_evaluating_2024,
	title = {Evaluating {Character} {Understanding} of {Large} {Language} {Models} via {Character} {Profiling} from {Fictional} {Works}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2404.12726},
	doi = {10.48550/ARXIV.2404.12726},
	abstract = {Large language models (LLMs) have demonstrated impressive performance and spurred numerous AI applications, in which role-playing agents (RPAs) are particularly popular, especially for fictional characters. The prerequisite for these RPAs lies in the capability of LLMs to understand characters from fictional works. Previous efforts have evaluated this capability via basic classification tasks or characteristic imitation, failing to capture the nuanced character understanding with LLMs. In this paper, we propose evaluating LLMs' character understanding capability via the character profiling task, i.e., summarizing character profiles from corresponding materials, a widely adopted yet understudied practice for RPA development. Specifically, we construct the CroSS dataset from literature experts and assess the generated profiles by comparing ground truth references and their applicability in downstream tasks. Our experiments, which cover various summarization methods and LLMs, have yielded promising results. These results strongly validate the character understanding capability of LLMs. We believe our constructed resource will promote further research in this field. Resources are available at https://github.com/Joanna0123/character\_profiling.},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Yuan, Xinfeng and Yuan, Siyu and Cui, Yuhan and Lin, Tianhe and Wang, Xintao and Xu, Rui and Chen, Jiangjie and Yang, Deqing},
	year = {2024},
	note = {Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@misc{serapio-garcia_personality_2023,
	title = {Personality {Traits} in {Large} {Language} {Models}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2307.00184},
	doi = {10.48550/ARXIV.2307.00184},
	abstract = {The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant human-like text. As LLMs increasingly power conversational agents used by the general public world-wide, the synthetic personality embedded in these models, by virtue of training on large amounts of human data, is becoming increasingly important. Since personality is a key factor determining the effectiveness of communication, we present a comprehensive method for administering and validating personality tests on widely-used LLMs, as well as for shaping personality in the generated text of such LLMs. Applying this method, we found: 1) personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid; 2) evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles. We discuss application and ethical implications of the measurement and shaping method, in particular regarding responsible AI.},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Serapio-García, Greg and Safdari, Mustafa and Crepy, Clément and Sun, Luning and Fitz, Stephen and Romero, Peter and Abdulhai, Marwa and Faust, Aleksandra and Matarić, Maja},
	year = {2023},
	note = {Version Number: 3},
	keywords = {68T35, Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computers and Society (cs.CY), FOS: Computer and information sciences, Human-Computer Interaction (cs.HC), I.2.7},
}

@inproceedings{bommasani2022,
 author = {Bommasani, Rishi and Creel, Kathleen A. and Kumar, Ananya and Jurafsky, Dan and Liang, Percy S},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {3663--3678},
 publisher = {Curran Associates, Inc.},
 title = {Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome Homogenization?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/17a234c91f746d9625a75cf8a8731ee2-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@article{argyle_out_2023,
	title = {Out of {One}, {Many}: {Using} {Language} {Models} to {Simulate} {Human} {Samples}},
	volume = {31},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Out of {One}, {Many}},
	url = {https://www.cambridge.org/core/product/identifier/S1047198723000025/type/journal_article},
	doi = {10.1017/pan.2023.2},
	abstract = {Abstract
            
              We propose and explore the possibility that language models can be studied as effective proxies for specific human subpopulations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the “algorithmic bias” within one such tool—the GPT-3 language model—is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property
              algorithmic fidelity
              and explore its extent in GPT-3. We create “silicon samples” by conditioning the model on thousands of sociodemographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and sociocultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines.},
	language = {en},
	number = {3},
	urldate = {2023-08-24},
	journal = {Political Analysis},
	author = {Argyle, Lisa P. and Busby, Ethan C. and Fulda, Nancy and Gubler, Joshua R. and Rytting, Christopher and Wingate, David},
	month = jul,
	year = {2023},
	pages = {337--351},
	file = {Submitted Version:/Users/keyeun/Zotero/storage/KGTBEIC9/Argyle et al. - 2023 - Out of One, Many Using Language Models to Simulat.pdf:application/pdf},
}


@misc{petrov_limited_2024,
	title = {Limited {Ability} of {LLMs} to {Simulate} {Human} {Psychological} {Behaviours}: a {Psychometric} {Analysis}},
	shorttitle = {Limited {Ability} of {LLMs} to {Simulate} {Human} {Psychological} {Behaviours}},
	url = {http://arxiv.org/abs/2405.07248},
	abstract = {The humanlike responses of large language models (LLMs) have prompted social scientists to investigate whether LLMs can be used to simulate human participants in experiments, opinion polls and surveys. Of central interest in this line of research has been mapping out the psychological profiles of LLMs by prompting them to respond to standardized questionnaires. The conflicting findings of this research are unsurprising given that mapping out underlying, or latent, traits from LLMs' text responses to questionnaires is no easy task. To address this, we use psychometrics, the science of psychological measurement. In this study, we prompt OpenAI's flagship models, GPT-3.5 and GPT-4, to assume different personas and respond to a range of standardized measures of personality constructs. We used two kinds of persona descriptions: either generic (four or five random person descriptions) or specific (mostly demographics of actual humans from a large-scale human dataset). We found that the responses from GPT-4, but not GPT-3.5, using generic persona descriptions show promising, albeit not perfect, psychometric properties, similar to human norms, but the data from both LLMs when using specific demographic profiles, show poor psychometrics properties. We conclude that, currently, when LLMs are asked to simulate silicon personas, their responses are poor signals of potentially underlying latent traits. Thus, our work casts doubt on LLMs' ability to simulate individual-level human behaviour across multiple-choice question answering tasks.},
	urldate = {2024-06-13},
	publisher = {arXiv},
	author = {Petrov, Nikolay B. and Serapio-García, Gregory and Rentfrow, Jason},
	month = may,
	year = {2024},
	note = {arXiv:2405.07248 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, I.2.7, 68T50},
	file = {arXiv Fulltext PDF:/Users/keyeun/Zotero/storage/4JG8KSPI/Petrov et al. - 2024 - Limited Ability of LLMs to Simulate Human Psycholo.pdf:application/pdf;arXiv.org Snapshot:/Users/keyeun/Zotero/storage/ZR89R4PP/2405.html:text/html},
}

@book{hesse2013demian,
  title={Demian: the story of Emil Sinclair's youth},
  author={Hesse, Hermann},
  year={1919},
  publisher={Boni \& Liveright}
}

@incollection{hall2015cultural,
  title={Cultural Identity and Diaspora},
  author={Hall, Stuart},
  booktitle={Colonial discourse and post-colonial theory},
  pages={392--403},
  year={2015},
  publisher={Routledge}
}

@book{bracken1996handbook,
  title={Handbook of self-concept: Developmental, social, and clinical considerations.},
  author={Bracken, Bruce A},
  year={1996},
  publisher={John Wiley \& Sons}
}

@article{markus1987dynamic,
  title={The dynamic self-concept: A social psychological perspective},
  author={Markus, Hazel and Wurf, Elissa},
  journal={Annual review of psychology},
  volume={38},
  number={1},
  pages={299--337},
  year={1987},
  publisher={Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA}
}

@incollection{oyserman_self-concept_2001,
title={Self-concept and identity},
author={Oyserman, Daphna},
booktitle={The Blackwell Handbook of Social Psychology},
editor={Tesser, Abraham and Schwarz, Norbert},
pages={499--517},
year={2001},
publisher={Blackwell},
address={Malden, MA}
}

@inproceedings{yamashita-etal-2023-realpersonachat,
    title = "{R}eal{P}ersona{C}hat: A Realistic Persona Chat Corpus with Interlocutors{'} Own Personalities",
    author = "Yamashita, Sanae  and
      Inoue, Koji  and
      Guo, Ao  and
      Mochizuki, Shota  and
      Kawahara, Tatsuya  and
      Higashinaka, Ryuichiro",
    editor = "Huang, Chu-Ren  and
      Harada, Yasunari  and
      Kim, Jong-Bok  and
      Chen, Si  and
      Hsu, Yu-Yin  and
      Chersoni, Emmanuele  and
      A, Pranav  and
      Zeng, Winnie Huiheng  and
      Peng, Bo  and
      Li, Yuxi  and
      Li, Junlin",
    booktitle = "Proceedings of the 37th Pacific Asia Conference on Language, Information and Computation",
    month = dec,
    year = "2023",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.paclic-1.85",
    pages = "852--861",
}

@article{roberts2006patterns,
  title={Patterns of mean-level change in personality traits across the life course: a meta-analysis of longitudinal studies.},
  author={Roberts, Brent W and Walton, Kate E and Viechtbauer, Wolfgang},
  journal={Psychological bulletin},
  volume={132},
  number={1},
  pages={1},
  year={2006},
  publisher={American Psychological Association}
}

@article{ozer2006personality,
  title={Personality and the prediction of consequential outcomes},
  author={Ozer, Daniel J and Benet-Martinez, Veronica},
  journal={Annu. Rev. Psychol.},
  volume={57},
  pages={401--421},
  year={2006},
  publisher={Annual Reviews}
}

@inproceedings{clochat_2024,
author = {Ha, Juhye and Jeon, Hyeon and Han, Daeun and Seo, Jinwook and Oh, Changhoon},
title = {CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642472},
doi = {10.1145/3613904.3642472},
abstract = {Large language models (LLMs) have facilitated significant strides in generating conversational agents, enabling seamless, contextually relevant dialogues across diverse topics. However, the existing LLM-driven conversational agents have fixed personalities and functionalities, limiting their adaptability to individual user needs. Creating personalized agent personas with distinct expertise or traits can address this issue. Nonetheless, we lack knowledge of how people customize and interact with agent personas. In this research, we investigated how users customize agent personas and their impact on interaction quality, diversity, and dynamics. To this end, we developed CloChat, an interface supporting easy and accurate customization of agent personas in LLMs. We conducted a study comparing how participants interact with CloChat and ChatGPT. The results indicate that participants formed emotional bonds with the customized agents, engaged in more dynamic dialogues, and showed interest in sustaining interactions. These findings contribute to design implications for future systems with conversational agents using LLMs.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {305},
numpages = {24},
keywords = {Conversational Agents, Large Language Models, Persona, Persona Customization},
location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
series = {CHI '24}
}

@inproceedings{deshpande-etal-2023-toxicity,
    title = "Toxicity in chatgpt: Analyzing persona-assigned language models",
    author = "Deshpande, Ameet  and
      Murahari, Vishvak  and
      Rajpurohit, Tanmay  and
      Kalyan, Ashwin  and
      Narasimhan, Karthik",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.88",
    doi = "10.18653/v1/2023.findings-emnlp.88",
    pages = "1236--1270",
    abstract = "Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Legislation has recognized its significance and recently drafted a {``}Blueprint For An AI Bill Of Rights{''} which calls for domain experts to identify risks and potential impact of AI systems. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to $6\times$, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others ($3\times$ more) irrespective of the assigned persona, reflecting discriminatory biases in the model. Our findings show that multiple provisions in the legislative blueprint are being violated, and we hope that the broader AI community rethinks the efficacy of current safety guardrails and develops better techniques that lead to robust, safe, and trustworthy AI.",
}


@inproceedings{lee_large_2024,
	address = {Rio de Janeiro Brazil},
	title = {Large {Language} {Models} {Portray} {Socially} {Subordinate} {Groups} as {More} {Homogeneous}, {Consistent} with a {Bias} {Observed} in {Humans}},
	isbn = {9798400704505},
	url = {https://dl.acm.org/doi/10.1145/3630106.3658975},
	doi = {10.1145/3630106.3658975},
	language = {en},
	urldate = {2024-06-15},
	booktitle = {The 2024 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Lee, Messi H.J. and Montgomery, Jacob M. and Lai, Calvin K.},
	month = jun,
	year = {2024},
	pages = {1321--1340},
	file = {Submitted Version:/Users/keyeun/Zotero/storage/W3VEQH8R/Lee et al. - 2024 - Large Language Models Portray Socially Subordinate.pdf:application/pdf},
}



@inproceedings{ahn_mpchat_2023,
	address = {Toronto, Canada},
	title = {{MPCHAT}: {Towards} {Multimodal} {Persona}-{Grounded} {Conversation}},
	shorttitle = {{MPCHAT}},
	url = {https://aclanthology.org/2023.acl-long.189},
	doi = {10.18653/v1/2023.acl-long.189},
	language = {en},
	urldate = {2024-06-16},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ahn, Jaewoo and Song, Yeda and Yun, Sangdoo and Kim, Gunhee},
	year = {2023},
	pages = {3354--3377},
	file = {Full Text:/Users/keyeun/Zotero/storage/KIYQBXTN/Ahn et al. - 2023 - MPCHAT Towards Multimodal Persona-Grounded Conver.pdf:application/pdf},
}


@misc{zhou_sotopia_2024,
	title = {{SOTOPIA}: {Interactive} {Evaluation} for {Social} {Intelligence} in {Language} {Agents}},
	shorttitle = {{SOTOPIA}},
	url = {http://arxiv.org/abs/2310.11667},
	abstract = {Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.},
	urldate = {2024-06-16},
	publisher = {arXiv},
	author = {Zhou, Xuhui and Zhu, Hao and Mathur, Leena and Zhang, Ruohong and Yu, Haofei and Qi, Zhengyang and Morency, Louis-Philippe and Bisk, Yonatan and Fried, Daniel and Neubig, Graham and Sap, Maarten},
	month = mar,
	year = {2024},
	note = {arXiv:2310.11667 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Preprint, 43 pages. The first two authors contribute equally},
	file = {arXiv Fulltext PDF:/Users/keyeun/Zotero/storage/I9MIXQVZ/Zhou et al. - 2024 - SOTOPIA Interactive Evaluation for Social Intelli.pdf:application/pdf;arXiv.org Snapshot:/Users/keyeun/Zotero/storage/B7HTF5J9/2310.html:text/html},
}

@misc{zhang_speechagents_2024,
	title = {{SpeechAgents}: {Human}-{Communication} {Simulation} with {Multi}-{Modal} {Multi}-{Agent} {Systems}},
	shorttitle = {{SpeechAgents}},
	url = {http://arxiv.org/abs/2401.03945},
	abstract = {Human communication is a complex and diverse process that not only involves multiple factors such as language, commonsense, and cultural backgrounds but also requires the participation of multimodal information, such as speech. Large Language Model (LLM)-based multi-agent systems have demonstrated promising performance in simulating human society. Can we leverage LLM-based multi-agent systems to simulate human communication? However, current LLM-based multi-agent systems mainly rely on text as the primary medium. In this paper, we propose SpeechAgents, a multi-modal LLM based multi-agent system designed for simulating human communication. SpeechAgents utilizes multi-modal LLM as the control center for individual agent and employes multi-modal signals as the medium for exchanged messages among agents. Additionally, we propose Multi-Agent Tuning to enhance the multi-agent capabilities of LLM without compromising general abilities. To strengthen and evaluate the effectiveness of human communication simulation, we build the Human-Communication Simulation Benchmark. Experimental results demonstrate that SpeechAgents can simulate human communication dialogues with consistent content, authentic rhythm, and rich emotions and demonstrate excellent scalability even with up to 25 agents, which can apply to tasks such as drama creation and audio novels generation. Code and models will be open-sourced at https://github. com/0nutation/SpeechAgents},
	urldate = {2024-06-16},
	publisher = {arXiv},
	author = {Zhang, Dong and Li, Zhaowei and Wang, Pengyu and Zhang, Xin and Zhou, Yaqian and Qiu, Xipeng},
	month = jan,
	year = {2024},
	note = {arXiv:2401.03945 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: work in progress},
	file = {arXiv Fulltext PDF:/Users/keyeun/Zotero/storage/IXA8QN3G/Zhang et al. - 2024 - SpeechAgents Human-Communication Simulation with .pdf:application/pdf;arXiv.org Snapshot:/Users/keyeun/Zotero/storage/BAJBXJIZ/2401.html:text/html},
}
@misc{xu2024character,
      title={Character is Destiny: Can Large Language Models Simulate Persona-Driven Decisions in Role-Playing?}, 
      author={Rui Xu and Xintao Wang and Jiangjie Chen and Siyu Yuan and Xinfeng Yuan and Jiaqing Liang and Zulong Chen and Xiaoqing Dong and Yanghua Xiao},
      year={2024},
      eprint={2404.12138},
      archivePrefix={arXiv},
      primaryClass={id='cs.AI' full_name='Artificial Intelligence' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of AI except Vision, Robotics, Machine Learning, Multiagent Systems, and Computation and Language (Natural Language Processing), which have separate subject areas. In particular, includes Expert Systems, Theorem Proving (although this may overlap with Logic in Computer Science), Knowledge Representation, Planning, and Uncertainty in AI. Roughly includes material in ACM Subject Classes I.2.0, I.2.1, I.2.3, I.2.4, I.2.8, and I.2.11.'}
}

@article{luft1955johari,
  title={The Johari window, a graphic model of interpersonal awareness},
  author={Luft, Joseph and Ingham, Harrington},
  journal={Proceedings of the western training laboratory in group development},
  year={1955}
}