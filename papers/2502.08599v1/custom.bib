% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@incollection{kross2017self,
  title={Self-distancing: Theory, research, and current directions},
  author={Kross, Ethan and Ayduk, Ozlem},
  booktitle={Advances in experimental social psychology},
  volume={55},
  pages={81--136},
  year={2017},
  publisher={Elsevier}
}

@article{mead1934mind,
  title={Mind, self, and society from the standpoint of a social behaviorist.},
  author={Mead, George Herbert},
  year={1934},
  publisher={Chicago}
}


@article{thissen2002quick,
  title={Quick and easy implementation of the Benjamini-Hochberg procedure for controlling the false positive rate in multiple comparisons},
  author={Thissen, David and Steinberg, Lynne and Kuang, Daniel},
  journal={Journal of educational and behavioral statistics},
  volume={27},
  number={1},
  pages={77--83},
  year={2002},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}



@article{naeimi2019validating,
  title={Validating self-reflection and insight scale to measure readiness for self-regulated learning},
  author={Naeimi, Leila and Abbaszadeh, Mahsa and Mirzazadeh, Azim and Sima, Ali Reza and Nedjat, Saharnaz and Hejri, Sara Mortaz},
  journal={Journal of Education and Health Promotion},
  volume={8},
  number={1},
  pages={150},
  year={2019},
  publisher={Medknow}
}

@article{wang2023measuring,
  title={Measuring user competence in using artificial intelligence: validity and reliability of artificial intelligence literacy scale},
  author={Wang, Bingcheng and Rau, Pei-Luen Patrick and Yuan, Tianyi},
  journal={Behaviour \& information technology},
  volume={42},
  number={9},
  pages={1324--1337},
  year={2023},
  publisher={Taylor \& Francis}
}

@misc{sindermann2021assessing,
  title={Assessing the Attitude towards artificial intelligence: introduction of a short measure in German, Chinese, and English language. KI K{\"u}nstliche Intelligenz. 35 (1), 109--118 (2021)},
  author={Sindermann, C and Sha, P and Zhou, M and Wernicke, J and Schmitt, HS and Li, M and Sariyska, R and Stavrou, M and Becker, B and Montag, C},
  year={2021}
}

@article{wang2024patient,
  title={PATIENT-$\{$$\backslash$Psi$\}$: Using Large Language Models to Simulate Patients for Training Mental Health Professionals},
  author={Wang, Ruiyi and Milani, Stephanie and Chiu, Jamie C and Eack, Shaun M and Labrum, Travis and Murphy, Samuel M and Jones, Nev and Hardy, Kate and Shen, Hong and Fang, Fei and others},
  journal={arXiv preprint arXiv:2405.19660},
  year={2024}
}

@article{li2024leveraging,
  title={Leveraging Large Language Model as Simulated Patients for Clinical Education},
  author={Li, Yaneng and Zeng, Cheng and Zhong, Jialun and Zhang, Ruoyu and Zhang, Minhao and Zou, Lei},
  journal={arXiv preprint arXiv:2404.13066},
  year={2024}
}

@article{cohen_does_2018,
	title = {Does {Character} {Similarity} {Increase} {Identification} and {Persuasion}?},
	volume = {21},
	issn = {1521-3269, 1532-785X},
	url = {https://www.tandfonline.com/doi/full/10.1080/15213269.2017.1302344},
	doi = {10.1080/15213269.2017.1302344},
	language = {en},
	number = {3},
	urldate = {2023-12-16},
	journal = {Media Psychology},
	author = {Cohen, Jonathan and Weimann-Saks, Dana and Mazor-Tregerman, Maya},
	month = jul,
	year = {2018},
	pages = {506--528},
}

@inproceedings{cheng_marked_2023,
	address = {Toronto, Canada},
	title = {Marked {Personas}: {Using} {Natural} {Language} {Prompts} to {Measure} {Stereotypes} in {Language} {Models}},
	shorttitle = {Marked {Personas}},
	url = {https://aclanthology.org/2023.acl-long.84},
	doi = {10.18653/v1/2023.acl-long.84},
	language = {en},
	urldate = {2024-06-13},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Cheng, Myra and Durmus, Esin and Jurafsky, Dan},
	year = {2023},
	pages = {1504--1532},
	file = {Full Text:/Users/keyeun/Zotero/storage/YAYURUU9/Cheng et al. - 2023 - Marked Personas Using Natural Language Prompts to.pdf:application/pdf},
}

@misc{santurkar_whose_2023,
	title = {Whose {Opinions} {Do} {Language} {Models} {Reflect}?},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2303.17548},
	doi = {10.48550/ARXIV.2303.17548},
	abstract = {Language models (LMs) are increasingly being used in open-ended contexts, where the opinions reflected by LMs in response to subjective queries can have a profound impact, both on user satisfaction, as well as shaping the views of society at large. In this work, we put forth a quantitative framework to investigate the opinions reflected by LMs -- by leveraging high-quality public opinion polls and their associated human responses. Using this framework, we create OpinionsQA, a new dataset for evaluating the alignment of LM opinions with those of 60 US demographic groups over topics ranging from abortion to automation. Across topics, we find substantial misalignment between the views reflected by current LMs and those of US demographic groups: on par with the Democrat-Republican divide on climate change. Notably, this misalignment persists even after explicitly steering the LMs towards particular demographic groups. Our analysis not only confirms prior observations about the left-leaning tendencies of some human feedback-tuned LMs, but also surfaces groups whose opinions are poorly reflected by current LMs (e.g., 65+ and widowed individuals). Our code and data are available at https://github.com/tatsu-lab/opinions\_qa.},
	urldate = {2024-06-13},
	publisher = {arXiv},
	author = {Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori},
	year = {2023},
	note = {Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Computers and Society (cs.CY)},
}

@inproceedings{kang_values_2023,
	address = {Singapore},
	title = {From {Values} to {Opinions}: {Predicting} {Human} {Behaviors} and {Stances} {Using} {Value}-{Injected} {Large} {Language} {Models}},
	shorttitle = {From {Values} to {Opinions}},
	url = {https://aclanthology.org/2023.emnlp-main.961},
	doi = {10.18653/v1/2023.emnlp-main.961},
	language = {en},
	urldate = {2024-06-14},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Kang, Dongjun and Park, Joonsuk and Jo, Yohan and Bak, JinYeong},
	year = {2023},
	pages = {15539--15559},
	file = {Full Text:/Users/keyeun/Zotero/storage/M7YFA86M/Kang et al. - 2023 - From Values to Opinions Predicting Human Behavior.pdf:application/pdf},
}

@inproceedings{han_meet_2022,
	address = {Seattle, United States},
	title = {Meet {Your} {Favorite} {Character}: {Open}-domain {Chatbot} {Mimicking} {Fictional} {Characters} with only a {Few} {Utterances}},
	shorttitle = {Meet {Your} {Favorite} {Character}},
	url = {https://aclanthology.org/2022.naacl-main.377},
	doi = {10.18653/v1/2022.naacl-main.377},
	language = {en},
	urldate = {2024-06-14},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Han, Seungju and Kim, Beomsu and Yoo, Jin Yong and Seo, Seokjun and Kim, Sangbum and Erdenee, Enkhbayar and Chang, Buru},
	year = {2022},
	pages = {5114--5132},
	file = {Full Text:/Users/keyeun/Zotero/storage/PDV59U8W/Han et al. - 2022 - Meet Your Favorite Character Open-domain Chatbot .pdf:application/pdf},
}

@inproceedings{sang_tvshowguess_2022,
	address = {Seattle, United States},
	title = {{TVShowGuess}: {Character} {Comprehension} in {Stories} as {Speaker} {Guessing}},
	shorttitle = {{TVShowGuess}},
	url = {https://aclanthology.org/2022.naacl-main.317},
	doi = {10.18653/v1/2022.naacl-main.317},
	language = {en},
	urldate = {2024-06-14},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Sang, Yisi and Mou, Xiangyang and Yu, Mo and Yao, Shunyu and Li, Jing and Stanton, Jeffrey},
	year = {2022},
	pages = {4267--4287},
	file = {Full Text:/Users/keyeun/Zotero/storage/XDVV7MT9/Sang et al. - 2022 - TVShowGuess Character Comprehension in Stories as.pdf:application/pdf},
}

@article{premack_does_1978,
	title = {Does the chimpanzee have a theory of mind?},
	volume = {1},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X00076512/type/journal_article},
	doi = {10.1017/S0140525X00076512},
	abstract = {Abstract
            
              An individual has a theory of mind if he imputes mental states to himself and others. A system of inferences of this kind is properly viewed as a theory because such states are not directly observable, and the system can be used to make predictions about the behavior of others. As to the mental states the chimpanzee may infer, consider those inferred by our own species, for example,
              purpose
              or
              intention
              , as well as
              knowledge, belief, thinking, doubt, guessing, pretending, liking
              , and so forth. To determine whether or not the chimpanzee infers states of this kind, we showed an adult chimpanzee a series of videotaped scenes of a human actor struggling with a variety of problems. Some problems were simple, involving inaccessible food – bananas vertically or horizontally out of reach, behind a box, and so forth – as in the original Kohler problems; others were more complex, involving an actor unable to extricate himself from a locked cage, shivering because of a malfunctioning heater, or unable to play a phonograph because it was unplugged. With each videotape the chimpanzee was given several photographs, one a solution to the problem, such as a stick for the inaccessible bananas, a key for the locked up actor, a lit wick for the malfunctioning heater. The chimpanzee's consistent choice of the correct photographs can be understood by assuming that the animal recognized the videotape as representing a problem, understood the actor's purpose, and chose alternatives compatible with that purpose.},
	language = {en},
	number = {4},
	urldate = {2024-06-14},
	journal = {Behavioral and Brain Sciences},
	author = {Premack, David and Woodruff, Guy},
	month = dec,
	year = {1978},
	pages = {515--526},
	file = {Full Text:/Users/keyeun/Zotero/storage/IQWNR9MK/Premack and Woodruff - 1978 - Does the chimpanzee have a theory of mind.pdf:application/pdf},
}

@misc{yuan_evaluating_2024,
	title = {Evaluating {Character} {Understanding} of {Large} {Language} {Models} via {Character} {Profiling} from {Fictional} {Works}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2404.12726},
	doi = {10.48550/ARXIV.2404.12726},
	abstract = {Large language models (LLMs) have demonstrated impressive performance and spurred numerous AI applications, in which role-playing agents (RPAs) are particularly popular, especially for fictional characters. The prerequisite for these RPAs lies in the capability of LLMs to understand characters from fictional works. Previous efforts have evaluated this capability via basic classification tasks or characteristic imitation, failing to capture the nuanced character understanding with LLMs. In this paper, we propose evaluating LLMs' character understanding capability via the character profiling task, i.e., summarizing character profiles from corresponding materials, a widely adopted yet understudied practice for RPA development. Specifically, we construct the CroSS dataset from literature experts and assess the generated profiles by comparing ground truth references and their applicability in downstream tasks. Our experiments, which cover various summarization methods and LLMs, have yielded promising results. These results strongly validate the character understanding capability of LLMs. We believe our constructed resource will promote further research in this field. Resources are available at https://github.com/Joanna0123/character\_profiling.},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Yuan, Xinfeng and Yuan, Siyu and Cui, Yuhan and Lin, Tianhe and Wang, Xintao and Xu, Rui and Chen, Jiangjie and Yang, Deqing},
	year = {2024},
	note = {Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@misc{serapio-garcia_personality_2023,
	title = {Personality {Traits} in {Large} {Language} {Models}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2307.00184},
	doi = {10.48550/ARXIV.2307.00184},
	abstract = {The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant human-like text. As LLMs increasingly power conversational agents used by the general public world-wide, the synthetic personality embedded in these models, by virtue of training on large amounts of human data, is becoming increasingly important. Since personality is a key factor determining the effectiveness of communication, we present a comprehensive method for administering and validating personality tests on widely-used LLMs, as well as for shaping personality in the generated text of such LLMs. Applying this method, we found: 1) personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid; 2) evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles. We discuss application and ethical implications of the measurement and shaping method, in particular regarding responsible AI.},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Serapio-García, Greg and Safdari, Mustafa and Crepy, Clément and Sun, Luning and Fitz, Stephen and Romero, Peter and Abdulhai, Marwa and Faust, Aleksandra and Matarić, Maja},
	year = {2023},
	note = {Version Number: 3},
	keywords = {68T35, Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computers and Society (cs.CY), FOS: Computer and information sciences, Human-Computer Interaction (cs.HC), I.2.7},
}

@inproceedings{bommasani2022,
 author = {Bommasani, Rishi and Creel, Kathleen A. and Kumar, Ananya and Jurafsky, Dan and Liang, Percy S},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {3663--3678},
 publisher = {Curran Associates, Inc.},
 title = {Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome Homogenization?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/17a234c91f746d9625a75cf8a8731ee2-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@article{argyle_out_2023,
	title = {Out of {One}, {Many}: {Using} {Language} {Models} to {Simulate} {Human} {Samples}},
	volume = {31},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Out of {One}, {Many}},
	url = {https://www.cambridge.org/core/product/identifier/S1047198723000025/type/journal_article},
	doi = {10.1017/pan.2023.2},
	abstract = {Abstract
            
              We propose and explore the possibility that language models can be studied as effective proxies for specific human subpopulations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the “algorithmic bias” within one such tool—the GPT-3 language model—is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property
              algorithmic fidelity
              and explore its extent in GPT-3. We create “silicon samples” by conditioning the model on thousands of sociodemographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and sociocultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines.},
	language = {en},
	number = {3},
	urldate = {2023-08-24},
	journal = {Political Analysis},
	author = {Argyle, Lisa P. and Busby, Ethan C. and Fulda, Nancy and Gubler, Joshua R. and Rytting, Christopher and Wingate, David},
	month = jul,
	year = {2023},
	pages = {337--351},
	file = {Submitted Version:/Users/keyeun/Zotero/storage/KGTBEIC9/Argyle et al. - 2023 - Out of One, Many Using Language Models to Simulat.pdf:application/pdf},
}


@misc{petrov_limited_2024,
	title = {Limited {Ability} of {LLMs} to {Simulate} {Human} {Psychological} {Behaviours}: a {Psychometric} {Analysis}},
	shorttitle = {Limited {Ability} of {LLMs} to {Simulate} {Human} {Psychological} {Behaviours}},
	url = {http://arxiv.org/abs/2405.07248},
	abstract = {The humanlike responses of large language models (LLMs) have prompted social scientists to investigate whether LLMs can be used to simulate human participants in experiments, opinion polls and surveys. Of central interest in this line of research has been mapping out the psychological profiles of LLMs by prompting them to respond to standardized questionnaires. The conflicting findings of this research are unsurprising given that mapping out underlying, or latent, traits from LLMs' text responses to questionnaires is no easy task. To address this, we use psychometrics, the science of psychological measurement. In this study, we prompt OpenAI's flagship models, GPT-3.5 and GPT-4, to assume different personas and respond to a range of standardized measures of personality constructs. We used two kinds of persona descriptions: either generic (four or five random person descriptions) or specific (mostly demographics of actual humans from a large-scale human dataset). We found that the responses from GPT-4, but not GPT-3.5, using generic persona descriptions show promising, albeit not perfect, psychometric properties, similar to human norms, but the data from both LLMs when using specific demographic profiles, show poor psychometrics properties. We conclude that, currently, when LLMs are asked to simulate silicon personas, their responses are poor signals of potentially underlying latent traits. Thus, our work casts doubt on LLMs' ability to simulate individual-level human behaviour across multiple-choice question answering tasks.},
	urldate = {2024-06-13},
	publisher = {arXiv},
	author = {Petrov, Nikolay B. and Serapio-García, Gregory and Rentfrow, Jason},
	month = may,
	year = {2024},
	note = {arXiv:2405.07248 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, I.2.7, 68T50},
	file = {arXiv Fulltext PDF:/Users/keyeun/Zotero/storage/4JG8KSPI/Petrov et al. - 2024 - Limited Ability of LLMs to Simulate Human Psycholo.pdf:application/pdf;arXiv.org Snapshot:/Users/keyeun/Zotero/storage/ZR89R4PP/2405.html:text/html},
}

@book{hesse2013demian,
  title={Demian: the story of Emil Sinclair's youth},
  author={Hesse, Hermann},
  year={1919},
  publisher={Boni \& Liveright}
}

@incollection{hall2015cultural,
  title={Cultural Identity and Diaspora},
  author={Hall, Stuart},
  booktitle={Colonial discourse and post-colonial theory},
  pages={392--403},
  year={2015},
  publisher={Routledge}
}

@book{bracken1996handbook,
  title={Handbook of self-concept: Developmental, social, and clinical considerations.},
  author={Bracken, Bruce A},
  year={1996},
  publisher={John Wiley \& Sons}
}

@article{markus1987dynamic,
  title={The dynamic self-concept: A social psychological perspective},
  author={Markus, Hazel and Wurf, Elissa},
  journal={Annual review of psychology},
  volume={38},
  number={1},
  pages={299--337},
  year={1987},
  publisher={Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA}
}

@incollection{oyserman_self-concept_2001,
title={Self-concept and identity},
author={Oyserman, Daphna},
booktitle={The Blackwell Handbook of Social Psychology},
editor={Tesser, Abraham and Schwarz, Norbert},
pages={499--517},
year={2001},
publisher={Blackwell},
address={Malden, MA}
}

@inproceedings{yamashita-etal-2023-realpersonachat,
    title = "{R}eal{P}ersona{C}hat: A Realistic Persona Chat Corpus with Interlocutors{'} Own Personalities",
    author = "Yamashita, Sanae  and
      Inoue, Koji  and
      Guo, Ao  and
      Mochizuki, Shota  and
      Kawahara, Tatsuya  and
      Higashinaka, Ryuichiro",
    editor = "Huang, Chu-Ren  and
      Harada, Yasunari  and
      Kim, Jong-Bok  and
      Chen, Si  and
      Hsu, Yu-Yin  and
      Chersoni, Emmanuele  and
      A, Pranav  and
      Zeng, Winnie Huiheng  and
      Peng, Bo  and
      Li, Yuxi  and
      Li, Junlin",
    booktitle = "Proceedings of the 37th Pacific Asia Conference on Language, Information and Computation",
    month = dec,
    year = "2023",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.paclic-1.85",
    pages = "852--861",
}

@article{roberts2006patterns,
  title={Patterns of mean-level change in personality traits across the life course: a meta-analysis of longitudinal studies.},
  author={Roberts, Brent W and Walton, Kate E and Viechtbauer, Wolfgang},
  journal={Psychological bulletin},
  volume={132},
  number={1},
  pages={1},
  year={2006},
  publisher={American Psychological Association}
}

@article{ozer2006personality,
  title={Personality and the prediction of consequential outcomes},
  author={Ozer, Daniel J and Benet-Martinez, Veronica},
  journal={Annu. Rev. Psychol.},
  volume={57},
  pages={401--421},
  year={2006},
  publisher={Annual Reviews}
}

@inproceedings{clochat_2024,
author = {Ha, Juhye and Jeon, Hyeon and Han, Daeun and Seo, Jinwook and Oh, Changhoon},
title = {CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642472},
doi = {10.1145/3613904.3642472},
abstract = {Large language models (LLMs) have facilitated significant strides in generating conversational agents, enabling seamless, contextually relevant dialogues across diverse topics. However, the existing LLM-driven conversational agents have fixed personalities and functionalities, limiting their adaptability to individual user needs. Creating personalized agent personas with distinct expertise or traits can address this issue. Nonetheless, we lack knowledge of how people customize and interact with agent personas. In this research, we investigated how users customize agent personas and their impact on interaction quality, diversity, and dynamics. To this end, we developed CloChat, an interface supporting easy and accurate customization of agent personas in LLMs. We conducted a study comparing how participants interact with CloChat and ChatGPT. The results indicate that participants formed emotional bonds with the customized agents, engaged in more dynamic dialogues, and showed interest in sustaining interactions. These findings contribute to design implications for future systems with conversational agents using LLMs.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {305},
numpages = {24},
keywords = {Conversational Agents, Large Language Models, Persona, Persona Customization},
location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
series = {CHI '24}
}

@inproceedings{deshpande-etal-2023-toxicity,
    title = "Toxicity in chatgpt: Analyzing persona-assigned language models",
    author = "Deshpande, Ameet  and
      Murahari, Vishvak  and
      Rajpurohit, Tanmay  and
      Kalyan, Ashwin  and
      Narasimhan, Karthik",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.88",
    doi = "10.18653/v1/2023.findings-emnlp.88",
    pages = "1236--1270",
    abstract = "Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Legislation has recognized its significance and recently drafted a {``}Blueprint For An AI Bill Of Rights{''} which calls for domain experts to identify risks and potential impact of AI systems. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to $6\times$, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others ($3\times$ more) irrespective of the assigned persona, reflecting discriminatory biases in the model. Our findings show that multiple provisions in the legislative blueprint are being violated, and we hope that the broader AI community rethinks the efficacy of current safety guardrails and develops better techniques that lead to robust, safe, and trustworthy AI.",
}


@inproceedings{lee_large_2024,
	address = {Rio de Janeiro Brazil},
	title = {Large {Language} {Models} {Portray} {Socially} {Subordinate} {Groups} as {More} {Homogeneous}, {Consistent} with a {Bias} {Observed} in {Humans}},
	isbn = {9798400704505},
	url = {https://dl.acm.org/doi/10.1145/3630106.3658975},
	doi = {10.1145/3630106.3658975},
	language = {en},
	urldate = {2024-06-15},
	booktitle = {The 2024 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Lee, Messi H.J. and Montgomery, Jacob M. and Lai, Calvin K.},
	month = jun,
	year = {2024},
	pages = {1321--1340},
	file = {Submitted Version:/Users/keyeun/Zotero/storage/W3VEQH8R/Lee et al. - 2024 - Large Language Models Portray Socially Subordinate.pdf:application/pdf},
}



@inproceedings{ahn_mpchat_2023,
	address = {Toronto, Canada},
	title = {{MPCHAT}: {Towards} {Multimodal} {Persona}-{Grounded} {Conversation}},
	shorttitle = {{MPCHAT}},
	url = {https://aclanthology.org/2023.acl-long.189},
	doi = {10.18653/v1/2023.acl-long.189},
	language = {en},
	urldate = {2024-06-16},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ahn, Jaewoo and Song, Yeda and Yun, Sangdoo and Kim, Gunhee},
	year = {2023},
	pages = {3354--3377},
	file = {Full Text:/Users/keyeun/Zotero/storage/KIYQBXTN/Ahn et al. - 2023 - MPCHAT Towards Multimodal Persona-Grounded Conver.pdf:application/pdf},
}


@misc{zhou_sotopia_2024,
	title = {{SOTOPIA}: {Interactive} {Evaluation} for {Social} {Intelligence} in {Language} {Agents}},
	shorttitle = {{SOTOPIA}},
	url = {http://arxiv.org/abs/2310.11667},
	abstract = {Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.},
	urldate = {2024-06-16},
	publisher = {arXiv},
	author = {Zhou, Xuhui and Zhu, Hao and Mathur, Leena and Zhang, Ruohong and Yu, Haofei and Qi, Zhengyang and Morency, Louis-Philippe and Bisk, Yonatan and Fried, Daniel and Neubig, Graham and Sap, Maarten},
	month = mar,
	year = {2024},
	note = {arXiv:2310.11667 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Preprint, 43 pages. The first two authors contribute equally},
	file = {arXiv Fulltext PDF:/Users/keyeun/Zotero/storage/I9MIXQVZ/Zhou et al. - 2024 - SOTOPIA Interactive Evaluation for Social Intelli.pdf:application/pdf;arXiv.org Snapshot:/Users/keyeun/Zotero/storage/B7HTF5J9/2310.html:text/html},
}

@misc{zhang_speechagents_2024,
	title = {{SpeechAgents}: {Human}-{Communication} {Simulation} with {Multi}-{Modal} {Multi}-{Agent} {Systems}},
	shorttitle = {{SpeechAgents}},
	url = {http://arxiv.org/abs/2401.03945},
	abstract = {Human communication is a complex and diverse process that not only involves multiple factors such as language, commonsense, and cultural backgrounds but also requires the participation of multimodal information, such as speech. Large Language Model (LLM)-based multi-agent systems have demonstrated promising performance in simulating human society. Can we leverage LLM-based multi-agent systems to simulate human communication? However, current LLM-based multi-agent systems mainly rely on text as the primary medium. In this paper, we propose SpeechAgents, a multi-modal LLM based multi-agent system designed for simulating human communication. SpeechAgents utilizes multi-modal LLM as the control center for individual agent and employes multi-modal signals as the medium for exchanged messages among agents. Additionally, we propose Multi-Agent Tuning to enhance the multi-agent capabilities of LLM without compromising general abilities. To strengthen and evaluate the effectiveness of human communication simulation, we build the Human-Communication Simulation Benchmark. Experimental results demonstrate that SpeechAgents can simulate human communication dialogues with consistent content, authentic rhythm, and rich emotions and demonstrate excellent scalability even with up to 25 agents, which can apply to tasks such as drama creation and audio novels generation. Code and models will be open-sourced at https://github. com/0nutation/SpeechAgents},
	urldate = {2024-06-16},
	publisher = {arXiv},
	author = {Zhang, Dong and Li, Zhaowei and Wang, Pengyu and Zhang, Xin and Zhou, Yaqian and Qiu, Xipeng},
	month = jan,
	year = {2024},
	note = {arXiv:2401.03945 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: work in progress},
	file = {arXiv Fulltext PDF:/Users/keyeun/Zotero/storage/IXA8QN3G/Zhang et al. - 2024 - SpeechAgents Human-Communication Simulation with .pdf:application/pdf;arXiv.org Snapshot:/Users/keyeun/Zotero/storage/BAJBXJIZ/2401.html:text/html},
}
@misc{xu2024character,
      title={Character is Destiny: Can Large Language Models Simulate Persona-Driven Decisions in Role-Playing?}, 
      author={Rui Xu and Xintao Wang and Jiangjie Chen and Siyu Yuan and Xinfeng Yuan and Jiaqing Liang and Zulong Chen and Xiaoqing Dong and Yanghua Xiao},
      year={2024},
      eprint={2404.12138},
      archivePrefix={arXiv},
      primaryClass={id='cs.AI' full_name='Artificial Intelligence' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of AI except Vision, Robotics, Machine Learning, Multiagent Systems, and Computation and Language (Natural Language Processing), which have separate subject areas. In particular, includes Expert Systems, Theorem Proving (although this may overlap with Logic in Computer Science), Knowledge Representation, Planning, and Uncertainty in AI. Roughly includes material in ACM Subject Classes I.2.0, I.2.1, I.2.3, I.2.4, I.2.8, and I.2.11.'}
}

@article{luft1955johari,
  title={The Johari window, a graphic model of interpersonal awareness},
  author={Luft, Joseph and Ingham, Harrington},
  journal={Proceedings of the western training laboratory in group development},
  year={1955}
}

@inproceedings{kim-etal-2020-will,
    title = "Will {I} Sound Like Me? Improving Persona Consistency in Dialogues through Pragmatic Self-Consciousness",
    author = "Kim, Hyunwoo  and
      Kim, Byeongchang  and
      Kim, Gunhee",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.65",
    doi = "10.18653/v1/2020.emnlp-main.65",
    pages = "904--916",
    abstract = "We explore the task of improving persona consistency of dialogue agents. Recent models tackling consistency often train with additional Natural Language Inference (NLI) labels or attach trained extra modules to the generative agent for maintaining consistency. However, such additional labels and training can be demanding. Also, we find even the best-performing persona-based agents are insensitive to contradictory words. Inspired by social cognition and pragmatics, we endow existing dialogue agents with public self-consciousness on the fly through an imaginary listener. Our approach, based on the Rational Speech Acts framework (Frank and Goodman, 2012), can enforce dialogue agents to refrain from uttering contradiction. We further extend the framework by learning the distractor selection, which has been usually done manually or randomly. Results on Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang et al., 2018) dataset show that our approach reduces contradiction and improves consistency of existing dialogue models. Moreover, we show that it can be generalized to improve context-consistency beyond persona in dialogues.",
}

@inproceedings{zhang-etal-2018-personalizing,
    title = "Personalizing Dialogue Agents: {I} have a dog, do you have pets too?",
    author = "Zhang, Saizheng  and
      Dinan, Emily  and
      Urbanek, Jack  and
      Szlam, Arthur  and
      Kiela, Douwe  and
      Weston, Jason",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1205",
    doi = "10.18653/v1/P18-1205",
    pages = "2204--2213",
    abstract = "Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i)condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.",
}

@misc{xie2024largelanguagemodelagents,
      title={Can Large Language Model Agents Simulate Human Trust Behaviors?}, 
      author={Chengxing Xie and Canyu Chen and Feiran Jia and Ziyu Ye and Kai Shu and Adel Bibi and Ziniu Hu and Philip Torr and Bernard Ghanem and Guohao Li},
      year={2024},
      eprint={2402.04559},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.04559}, 
}

@inproceedings{frisch-giulianelli-2024-llm,
    title = "{LLM} Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models",
    author = "Frisch, Ivar  and
      Giulianelli, Mario",
    editor = "Deshpande, Ameet  and
      Hwang, EunJeong  and
      Murahari, Vishvak  and
      Park, Joon Sung  and
      Yang, Diyi  and
      Sabharwal, Ashish  and
      Narasimhan, Karthik  and
      Kalyan, Ashwin",
    booktitle = "Proceedings of the 1st Workshop on Personalization of Generative AI Systems (PERSONALIZE 2024)",
    month = mar,
    year = "2024",
    address = "St. Julians, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.personalize-1.9",
    pages = "102--111",
    abstract = "Agent interaction has long been a key topic in psychology, philosophy, and artificial intelligence, and it is now gaining traction in large language model (LLM) research. This experimental study seeks to lay the groundwork for our understanding of dialogue-based interaction between LLMs: Do persona-prompted LLMs show consistent personality and language use in interaction? We condition GPT-3.5 on asymmetric personality profiles to create a population of LLM agents, administer personality tests and submit the agents to a collaborative writing task. We find different profiles exhibit different degrees of personality consistency and linguistic alignment in interaction.",
}

@inproceedings{jiang-etal-2024-personallm,
    title = "{P}ersona{LLM}: Investigating the Ability of Large Language Models to Express Personality Traits",
    author = "Jiang, Hang  and
      Zhang, Xiajie  and
      Cao, Xubo  and
      Breazeal, Cynthia  and
      Roy, Deb  and
      Kabbara, Jad",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.229",
    doi = "10.18653/v1/2024.findings-naacl.229",
    pages = "3605--3627",
    abstract = "Despite the many use cases for large language models (LLMs) in creating personalized chatbots, there has been limited research on evaluating the extent to which the behaviors of personalized LLMs accurately and consistently reflect specific personality traits. We consider studying the behavior of LLM-based agents which we refer to as LLM personas and present a case study with GPT-3.5 and GPT-4 to investigate whether LLMs can generate content that aligns with their assigned personality profiles. To this end, we simulate distinct LLM personas based on the Big Five personality model, have them complete the 44-item Big Five Inventory (BFI) personality test and a story writing task, and then assess their essays with automatic and human evaluations. Results show that LLM personas{'} self-reported BFI scores are consistent with their designated personality types, with large effect sizes observed across five traits. Additionally, LLM personas{'} writings have emerging representative linguistic patterns for personality traits when compared with a human writing corpus. Furthermore, human evaluation shows that humans can perceive some personality traits with an accuracy of up to 80{\%}. Interestingly, the accuracy drops significantly when the annotators were informed of AI authorship.",
}


@article{kocaballi2023conversational,
  title={Conversational ai-powered design: Chatgpt as designer, user, and product},
  author={Kocaballi, A Baki},
  journal={arXiv preprint arXiv:2302.07406},
  year={2023}
}

@inproceedings{deusex_2024,
author = {Salminen, Joni and Liu, Chang and Pian, Wenjing and Chi, Jianxing and H\"{a}yh\"{a}nen, Essi and Jansen, Bernard J},
title = {Deus Ex Machina and Personas from Large Language Models: Investigating the Composition of AI-Generated Persona Descriptions},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642036},
doi = {10.1145/3613904.3642036},
abstract = {Large language models (LLMs) can generate personas based on prompts that describe the target user group. To understand what kind of personas LLMs generate, we investigate the diversity and bias in 450 LLM-generated personas with the help of internal evaluators (n=4) and subject-matter experts (SMEs) (n=5). The research findings reveal biases in LLM-generated personas, particularly in age, occupation, and pain points, as well as a strong bias towards personas from the United States. Human evaluations demonstrate that LLM persona descriptions were informative, believable, positive, relatable, and not stereotyped. The SMEs rated the personas slightly more stereotypical, less positive, and less relatable than the internal evaluators. The findings suggest that LLMs can generate consistent personas perceived as believable, relatable, and informative while containing relatively low amounts of stereotyping.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {510},
numpages = {20},
keywords = {AI, HCI, LLMs, evaluation, user personas},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{liu2024skepticism,
  title={From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News},
  author={Liu, Yuhan and Chen, Xiuying and Zhang, Xiaoqing and Gao, Xing and Zhang, Ji and Yan, Rui},
  journal={arXiv preprint arXiv:2403.09498},
  year={2024}
}

@inproceedings{chuang-etal-2024-simulating,
    title = "Simulating Opinion Dynamics with Networks of {LLM}-based Agents",
    author = "Chuang, Yun-Shiuan  and
      Goyal, Agam  and
      Harlalka, Nikunj  and
      Suresh, Siddharth  and
      Hawkins, Robert  and
      Yang, Sijia  and
      Shah, Dhavan  and
      Hu, Junjie  and
      Rogers, Timothy",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.211",
    doi = "10.18653/v1/2024.findings-naacl.211",
    pages = "3326--3346",
    abstract = "Accurately simulating human opinion dynamics is crucial for understanding a variety of societal phenomena, including polarization and the spread of misinformation. However, the agent-based models (ABMs) commonly used for such simulations often over-simplify human behavior. We propose a new approach to simulating opinion dynamics based on populations of Large Language Models (LLMs). Our findings reveal a strong inherent bias in LLM agents towards producing accurate information, leading simulated agents to consensus in line with scientific reality. This bias limits their utility for understanding resistance to consensus views on issues like climate change. After inducing confirmation bias through prompt engineering, however, we observed opinion fragmentation in line with existing agent-based modeling and opinion dynamics research. These insights highlight the promise and limitations of LLM agents in this domain and suggest a path forward: refining LLMs with real-world discourse to better simulate the evolution of human beliefs.",
}

@inproceedings{chen_empathy_2024,
author = {Chen, Chaoran and Li, Weijun and Song, Wenxin and Ye, Yanfang and Yao, Yaxing and Li, Toby Jia-Jun},
title = {An Empathy-Based Sandbox Approach to Bridge the Privacy Gap among Attitudes, Goals, Knowledge, and Behaviors},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642363},
doi = {10.1145/3613904.3642363},
abstract = {Managing privacy to reach privacy goals is challenging, as evidenced by the privacy attitude-behavior gap. Mitigating this discrepancy requires solutions that account for both system opaqueness and users’ hesitations in testing different privacy settings due to fears of unintended data exposure. We introduce an empathy-based approach that allows users to experience how privacy attributes may alter system outcomes in a risk-free sandbox environment from the perspective of artificially generated personas. To generate realistic personas, we introduce a novel pipeline that augments the outputs of large language models (e.g., GPT-4) using few-shot learning, contextualization, and chain of thoughts. Our empirical studies demonstrated the adequate quality of generated personas and highlighted the changes in privacy-related applications (e.g., online advertising) caused by different personas. Furthermore, users demonstrated cognitive and emotional empathy towards the personas when interacting with our sandbox. We offered design implications for downstream applications in improving user privacy literacy.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {234},
numpages = {28},
keywords = {empathy, generated personas, privacy awareness, privacy intervention, privacy literacy, sandbox},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{nario-redmond_social_2004,
	title = {The {Social} and {Personal} {Identities} {Scale}: {A} {Measure} of the {Differential} {Importance} {Ascribed} to {Social} and {Personal} {Self}-{Categorizations}},
	volume = {3},
	issn = {1529-8868, 1529-8876},
	shorttitle = {The {Social} and {Personal} {Identities} {Scale}},
	url = {https://www.tandfonline.com/doi/full/10.1080/13576500342000103},
	doi = {10.1080/13576500342000103},
	language = {en},
	number = {2},
	urldate = {2024-06-15},
	journal = {Self and Identity},
	author = {Nario-Redmond, Michelle R and Biernat, Monica and Eidelman, Scott and Palenske, Debra J},
	month = apr,
	year = {2004},
	pages = {143--175},
	file = {Nario-Redmond et al. - 2004 - The Social and Personal Identities Scale A Measur.pdf:/Users/Maria/Zotero/storage/GBB8QNJZ/Nario-Redmond et al. - 2004 - The Social and Personal Identities Scale A Measur.pdf:application/pdf},
}

@article{chen_meta-analysis_2024,
	title = {A {Meta}-{Analysis} {Examining} the {Role} of {Character}-{Recipient} {Similarity} in {Narrative} {Persuasion}},
	volume = {51},
	issn = {0093-6502, 1552-3810},
	url = {http://journals.sagepub.com/doi/10.1177/00936502231204834},
	doi = {10.1177/00936502231204834},
	abstract = {This meta-analysis synthesized 19 empirical articles reporting 123 effect sizes of character-recipient similarity on narrative processing and persuasion outcomes across different contexts, including health, environmental, and social issues. We also aimed to investigate whether the effect magnitude varies depending on how the similarity is operationalized, which perspective is adopted, and what context the narrative persuasion is placed in. The results indicated that, compared to a dissimilar counterpart, a similar character leads to stronger identification (k = 34, d = 0.14, p {\textless} .01) and self-referencing (k = 12, d = 0.16, p {\textless} .01). The effects on transportation (k = 22, d = 0.13, p = .05) and resistance (k = 12, d = −0.16, p = .05) were marginally significant. It was also found that the similarity manipulated on chosen demographic and biographic variables like occupation and living place yields the strongest impact among other variables (i.e., innate demographic and biographic variables like age and sex, psychological and behavioral variables like beliefs and behaviors). Furthermore, the similarity effect in narrative persuasion becomes intensified when combined with a first-person perspective and placed in a social issue context. By presenting a synthesis of the existing research, this meta-analytical study sought to identify areas in need of further refinement and outline future investigation directions for narrative persuasion.},
	language = {en},
	number = {1},
	urldate = {2024-06-15},
	journal = {Communication Research},
	author = {Chen, Meng and Dong, Yujie and Wang, Jilong},
	month = feb,
	year = {2024},
	pages = {56--82},
	file = {Chen et al. - 2024 - A Meta-Analysis Examining the Role of Character-Re.pdf:/Users/Maria/Zotero/storage/W488GXBB/Chen et al. - 2024 - A Meta-Analysis Examining the Role of Character-Re.pdf:application/pdf},
}

@misc{xiao_how_2023,
	title = {How {Far} {Are} {We} from {Believable} {AI} {Agents}? {A} {Framework} for {Evaluating} the {Believability} of {Human} {Behavior} {Simulation}},
	shorttitle = {How {Far} {Are} {We} from {Believable} {AI} {Agents}?},
	url = {http://arxiv.org/abs/2312.17115},
	abstract = {Human behavior simulation of AI agents necessitates the agents to possess a quality of believability, which is crucial as it facilitates users in establishing trust toward the agents and streamlines the fulfillment of the agents' goal. While recent advancements in Large Language Model (LLM) based agents have improved human behavior simulation, challenges inherent to LLMs (e.g., long context modeling) can undermine their believability. Consequently, evaluating AI agent believability becomes imperative. Unfortunately, prior research often neglects the negative impacts of LLM deficiencies. To address these gaps, we introduce two metrics for assessing LLM-based agent believability: consistency, and robustness, together with a benchmark, SimulateBench, with which, we evaluate the consistency and robustness of agents implemented with popular LLMs. We find that agents (i) struggle to accurately depict character information when presented with lengthy profile inputs; (ii) exhibit vulnerability to profile perturbations; and (iii) are significantly affected by certain key factors that impact their overall believability. Code and SimulateBench are public at https://github.com/GAIR-NLP/GPTMan.},
	language = {en},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Xiao, Yang and Cheng, Yi and Fu, Jinlan and Wang, Jiashuo and Li, Wenjie and Liu, Pengfei},
	month = dec,
	year = {2023},
	note = {arXiv:2312.17115 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {Xiao et al. - 2023 - How Far Are We from Believable AI Agents A Framew.pdf:/Users/Maria/Zotero/storage/YEVNJ8YQ/Xiao et al. - 2023 - How Far Are We from Believable AI Agents A Framew.pdf:application/pdf},
}

@inproceedings{cheng_compost_2023,
	address = {Singapore},
	title = {{CoMPosT}: {Characterizing} and {Evaluating} {Caricature} in {LLM} {Simulations}},
	shorttitle = {{CoMPosT}},
	url = {https://aclanthology.org/2023.emnlp-main.669},
	doi = {10.18653/v1/2023.emnlp-main.669},
	abstract = {Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations’ susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.},
	language = {en},
	urldate = {2024-06-15},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Cheng, Myra and Piccardi, Tiziano and Yang, Diyi},
	year = {2023},
	pages = {10853--10875},
	file = {Cheng et al. - 2023 - CoMPosT Characterizing and Evaluating Caricature .pdf:/Users/Maria/Zotero/storage/KAPVZBJ9/Cheng et al. - 2023 - CoMPosT Characterizing and Evaluating Caricature .pdf:application/pdf},
}


@misc{wang_user_2024,
	title = {User {Behavior} {Simulation} with {Large} {Language} {Model} based {Agents}},
	url = {http://arxiv.org/abs/2306.02552},
	abstract = {Simulating high quality user behavior data has always been a fundamental problem in humancentered applications, where the major difficulty originates from the intricate mechanism of human decision process. Recently, substantial evidences have suggested that by learning huge amounts of web knowledge, large language models (LLMs) can achieve human-like intelligence. We believe these models can provide significant opportunities to more believable user behavior simulation. To inspire such direction, we propose an LLM-based agent framework and design a sandbox environment to simulate real user behaviors. Based on extensive experiments, we find that the simulated behaviors of our method are very close to the ones of real humans. Concerning potential applications, we simulate and study two social phenomenons including (1) information cocoons and (2) user conformity behaviors. This research provides novel simulation paradigms for human-centered applications.},
	language = {en},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Wang, Lei and Zhang, Jingsen and Yang, Hao and Chen, Zhiyuan and Tang, Jiakai and Zhang, Zeyu and Chen, Xu and Lin, Yankai and Song, Ruihua and Zhao, Wayne Xin and Xu, Jun and Dou, Zhicheng and Wang, Jun and Wen, Ji-Rong},
	month = feb,
	year = {2024},
	note = {arXiv:2306.02552 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {Wang et al. - 2024 - User Behavior Simulation with Large Language Model.pdf:/Users/Maria/Zotero/storage/XSAH935Y/Wang et al. - 2024 - User Behavior Simulation with Large Language Model.pdf:application/pdf},
}

@inproceedings{adams_sparse_2023,
	address = {Hybrid},
	title = {From {Sparse} to {Dense}: {GPT}-4 {Summarization} with {Chain} of {Density} {Prompting}},
	shorttitle = {From {Sparse} to {Dense}},
	url = {https://aclanthology.org/2023.newsum-1.7},
	doi = {10.18653/v1/2023.newsum-1.7},
	abstract = {Selecting the “right” amount of information to include in a summary is a difficult task. A good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entitysparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries generated by CoD are more abstractive, exhibit more fusion, and have less of a lead bias than GPT-4 summaries generated by a vanilla prompt. We conduct a human preference study on 100 CNN DailyMail articles and find that humans prefer GPT-4 summaries that are more dense than those generated by a vanilla prompt and almost as dense as human written summaries. Qualitative analysis supports the notion that there exists a tradeoff between informativeness and readability. 500 annotated CoD summaries, as well as an extra 5,000 unannotated summaries, are freely available on HuggingFace1.},
	language = {en},
	urldate = {2024-06-15},
	booktitle = {Proceedings of the 4th {New} {Frontiers} in {Summarization} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Adams, Griffin and Fabbri, Alex and Ladhak, Faisal and Lehman, Eric and Elhadad, Noémie},
	year = {2023},
	pages = {68--74},
	file = {Adams et al. - 2023 - From Sparse to Dense GPT-4 Summarization with Cha.pdf:/Users/Maria/Zotero/storage/FBHRUKA5/Adams et al. - 2023 - From Sparse to Dense GPT-4 Summarization with Cha.pdf:application/pdf},
}



@misc{song_identifying_2024,
	title = {Identifying {Multiple} {Personalities} in {Large} {Language} {Models} with {External} {Evaluation}},
	url = {http://arxiv.org/abs/2402.14805},
	abstract = {As Large Language Models (LLMs) are integrated with human daily applications rapidly, many societal and ethical concerns are raised regarding the behavior of LLMs. One of the ways to comprehend LLMs’ behavior is to analyze their personalities. Many recent studies quantify LLMs’ personalities using self-assessment tests that are created for humans. Yet many critiques question the applicability and reliability of these self-assessment tests when applied to LLMs. In this paper, we investigate LLM personalities using an alternate personality measurement method, which we refer to as the external evaluation method, where instead of prompting LLMs with multiple-choice questions in the Likert scale, we evaluate LLMs’ personalities by analyzing their responses toward open-ended situational questions using an external machine learning model. We first fine-tuned a Llama2-7B model as the MBTI personality predictor that outperforms the stateof-the-art models as the tool to analyze LLMs’ responses. Then, we prompt the LLMs with situational questions and ask them to generate Twitter posts and comments, respectively, in order to assess their personalities when playing two different roles. Using the external personality evaluation method, we identify that the obtained personality types for LLMs are significantly different when generating posts versus comments, whereas humans show a consistent personality profile in these two different situations. This shows that LLMs can exhibit different personalities based on different scenarios, thus highlighting a fundamental difference between personality in LLMs and humans. With our work, we call for a re-evaluation of personality definition and measurement in LLMs.},
	language = {en},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Song, Xiaoyang and Adachi, Yuta and Feng, Jessie and Lin, Mouwei and Yu, Linhao and Li, Frank and Gupta, Akshat and Anumanchipalli, Gopala and Kaur, Simerjot},
	month = feb,
	year = {2024},
	note = {arXiv:2402.14805 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Song et al. - 2024 - Identifying Multiple Personalities in Large Langua.pdf:/Users/Maria/Zotero/storage/QFA6YULL/Song et al. - 2024 - Identifying Multiple Personalities in Large Langua.pdf:application/pdf},
}

@misc{jiang_evaluating_2023,
	title = {Evaluating and {Inducing} {Personality} in {Pre}-trained {Language} {Models}},
	url = {http://arxiv.org/abs/2206.07550},
	abstract = {Standardized and quantified evaluation of machine behaviors is a crux of understanding LLMs. In this study, we draw inspiration from psychometric studies by leveraging human personality theory as a tool for studying machine behaviors. Originating as a philosophical quest for human behaviors, the study of personality delves into how individuals differ in thinking, feeling, and behaving. Toward building and understanding human-like social machines, we are motivated to ask: Can we assess machine behaviors by leveraging human psychometric tests in a principled and quantitative manner? If so, can we induce a specific personality in LLMs? To answer these questions, we introduce the Machine Personality Inventory (MPI) tool for studying machine behaviors; MPI follows standardized personality tests, built upon the Big Five Personality Factors (Big Five) theory and personality assessment inventories. By systematically evaluating LLMs with MPI, we provide the first piece of evidence demonstrating the efficacy of MPI in studying LLMs behaviors. We further devise a PERSONALITY PROMPTING (P2) method to induce LLMs with specific personalities in a controllable way, capable of producing diverse and verifiable behaviors. We hope this work sheds light on future studies by adopting personality as the essential indicator for various downstream tasks, and could further motivate research into equally intriguing human-like machine behaviors.},
	language = {en},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Jiang, Guangyuan and Xu, Manjie and Zhu, Song-Chun and Han, Wenjuan and Zhang, Chi and Zhu, Yixin},
	month = oct,
	year = {2023},
	note = {arXiv:2206.07550 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Jiang et al. - 2023 - Evaluating and Inducing Personality in Pre-trained.pdf:/Users/Maria/Zotero/storage/UZRWWHGZ/Jiang et al. - 2023 - Evaluating and Inducing Personality in Pre-trained.pdf:application/pdf},
}

@misc{ha_clochat_2024,
	title = {{CloChat}: {Understanding} {How} {People} {Customize}, {Interact}, and {Experience} {Personas} in {Large} {Language} {Models}},
	shorttitle = {{CloChat}},
	url = {http://arxiv.org/abs/2402.15265},
	abstract = {Large language models (LLMs) have facilitated significant strides in generating conversational agents, enabling seamless, contextually relevant dialogues across diverse topics. However, the existing LLM-driven conversational agents have fixed personalities and functionalities, limiting their adaptability to individual user needs. Creating personalized agent personas with distinct expertise or traits can address this issue. Nonetheless, we lack knowledge of how people customize and interact with agent personas. In this research, we investigated how users customize agent personas and their impact on interaction quality, diversity, and dynamics. To this end, we developed CloChat, an interface supporting easy and accurate customization of agent personas in LLMs. We conducted a study comparing how participants interact with CloChat and ChatGPT. The results indicate that participants formed emotional bonds with the customized agents, engaged in more dynamic dialogues, and showed interest in sustaining interactions. These findings contribute to design implications for future systems with conversational agents using LLMs.},
	language = {en},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Ha, Juhye and Jeon, Hyeon and Han, DaEun and Seo, Jinwook and Oh, Changhoon},
	month = feb,
	year = {2024},
	note = {arXiv:2402.15265 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file = {Ha et al. - 2024 - CloChat Understanding How People Customize, Inter.pdf:/Users/Maria/Zotero/storage/E4D24JCB/Ha et al. - 2024 - CloChat Understanding How People Customize, Inter.pdf:application/pdf},
}

@misc{shao_character-llm_2023,
	title = {Character-{LLM}: {A} {Trainable} {Agent} for {Role}-{Playing}},
	shorttitle = {Character-{LLM}},
	url = {http://arxiv.org/abs/2310.10158},
	abstract = {Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents {\textbackslash}textit\{memorize\} their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.},
	language = {en},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Shao, Yunfan and Li, Linyang and Dai, Junqi and Qiu, Xipeng},
	month = dec,
	year = {2023},
	note = {arXiv:2310.10158 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Shao et al. - 2023 - Character-LLM A Trainable Agent for Role-Playing.pdf:/Users/Maria/Zotero/storage/24QAPHF5/Shao et al. - 2023 - Character-LLM A Trainable Agent for Role-Playing.pdf:application/pdf},
}

@inproceedings{park_social_2022,
	address = {Bend OR USA},
	title = {Social {Simulacra}: {Creating} {Populated} {Prototypes} for {Social} {Computing} {Systems}},
	isbn = {978-1-4503-9320-1},
	shorttitle = {Social {Simulacra}},
	url = {https://dl.acm.org/doi/10.1145/3526113.3545616},
	doi = {10.1145/3526113.3545616},
	language = {en},
	urldate = {2024-06-15},
	booktitle = {Proceedings of the 35th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Park, Joon Sung and Popowski, Lindsay and Cai, Carrie and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
	month = oct,
	year = {2022},
	pages = {1--18},
	file = {Full Text:/Users/Maria/Zotero/storage/3RQ4YWSM/Park et al. - 2022 - Social Simulacra Creating Populated Prototypes fo.pdf:application/pdf},
}

@inproceedings{park_generative_2023,
	address = {San Francisco CA USA},
	title = {Generative {Agents}: {Interactive} {Simulacra} of {Human} {Behavior}},
	isbn = {9798400701320},
	shorttitle = {Generative {Agents}},
	url = {https://dl.acm.org/doi/10.1145/3586183.3606763},
	doi = {10.1145/3586183.3606763},
	language = {en},
	urldate = {2024-06-15},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
	month = oct,
	year = {2023},
	pages = {1--22},
	file = {Full Text:/Users/Maria/Zotero/storage/UDKGBAP6/Park et al. - 2023 - Generative Agents Interactive Simulacra of Human .pdf:application/pdf},
}

@article{boyd_values_2021,
	title = {Values in {Words}: {Using} {Language} to {Evaluate} and {Understand} {Personal} {Values}},
	volume = {9},
	issn = {2334-0770, 2162-3449},
	shorttitle = {Values in {Words}},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/14589},
	doi = {10.1609/icwsm.v9i1.14589},
	abstract = {People’s values provide a decision-making framework that helps guide their everyday actions. Most popular methods of assessing values show tenuous relationships with everyday behaviors. Using a new Amazon Mechanical Turk dataset (N = 767) consisting of people’s language, values, and behaviors, we explore the degree to which attaining “ground truth” is possible with regards to such complicated mental phenomena. We then apply our ﬁndings to a corpus of Facebook user (N = 130, 828) status updates in order to understand how core values inﬂuence the personal thoughts and behaviors that users share through social media. Our ﬁndings suggest that self-report questionnaires for abstract and complex phenomena, such as values, are inadequate for painting an accurate picture of individual mental life. Free response language data and language modeling show greater promise for understanding both the structure and content of concepts such as values and, additionally, exhibit a predictive edge over self-report questionnaires.},
	language = {en},
	number = {1},
	urldate = {2024-06-15},
	journal = {Proceedings of the International AAAI Conference on Web and Social Media},
	author = {Boyd, Ryan and Wilson, Steven and Pennebaker, James and Kosinski, Michal and Stillwell, David and Mihalcea, Rada},
	month = aug,
	year = {2021},
	pages = {31--40},
	file = {Boyd et al. - 2021 - Values in Words Using Language to Evaluate and Un.pdf:/Users/Maria/Zotero/storage/QDLZV9MF/Boyd et al. - 2021 - Values in Words Using Language to Evaluate and Un.pdf:application/pdf},
}

@misc{xie_human_2024,
	title = {Human {Simulacra}: {Benchmarking} the {Personification} of {Large} {Language} {Models}},
	shorttitle = {Human {Simulacra}},
	url = {http://arxiv.org/abs/2402.18180},
	abstract = {Large language models (LLMs) are recognized as systems that closely mimic aspects of human intelligence. This capability has attracted attention from the social science community, who see the potential in leveraging LLMs to replace human participants in experiments, thereby reducing research costs and complexity. In this paper, we introduce a framework for large language models personification, including a strategy for constructing virtual characters’ life stories from the ground up, a Multi-Agent Cognitive Mechanism capable of simulating human cognitive processes, and a psychology-guided evaluation method to assess human simulations from both self and observational perspectives. Experimental results demonstrate that our constructed simulacra can produce personified responses that align with their target characters. Our work is a preliminary exploration which offers great potential in practical applications. All the code and datasets will be released, with the hope of inspiring further investigations.},
	language = {en},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Xie, Qiuejie and Feng, Qiming and Zhang, Tianqi and Li, Qingqiu and Yang, Linyi and Zhang, Yuejie and Feng, Rui and He, Liang and Gao, Shang and Zhang, Yue},
	month = jun,
	year = {2024},
	note = {arXiv:2402.18180 [cs]},
	keywords = {Computer Science - Computers and Society},
	file = {Xie et al. - 2024 - Human Simulacra Benchmarking the Personification .pdf:/Users/Maria/Zotero/storage/Q9IIST8G/Xie et al. - 2024 - Human Simulacra Benchmarking the Personification .pdf:application/pdf},
}

@inproceedings{yang_psycot_2023,
	address = {Singapore},
	title = {{PsyCoT}: {Psychological} {Questionnaire} as {Powerful} {Chain}-of-{Thought} for {Personality} {Detection}},
	shorttitle = {{PsyCoT}},
	url = {https://aclanthology.org/2023.findings-emnlp.216},
	doi = {10.18653/v1/2023.findings-emnlp.216},
	abstract = {Recent advances in large language models (LLMs), such as ChatGPT, have showcased remarkable zero-shot performance across various NLP tasks. However, the potential of LLMs in personality detection, which involves identifying an individual’s personality from their written texts, remains largely unexplored. Drawing inspiration from Psychological Questionnaires, which are carefully designed by psychologists to evaluate individual personality traits through a series of targeted items, we argue that these items can be regarded as a collection of wellstructured chain-of-thought (CoT) processes. By incorporating these processes, LLMs can enhance their capabilities to make more reasonable inferences on personality from textual input. In light of this, we propose a novel personality detection method, called PsyCoT, which mimics the way individuals complete psychological questionnaires in a multi-turn dialogue manner. In particular, we employ a LLM as an AI assistant with a specialization in text analysis. We prompt the assistant to rate individual items at each turn and leverage the historical rating results to derive a conclusive personality preference. Our experiments demonstrate that PsyCoT significantly improves the performance and robustness of GPT-3.5 in personality detection, achieving an average F1 score improvement of 4.23/10.63 points on two benchmark datasets compared to the standard prompting method. Our code is available at https://github.com/TaoYang225/PsyCoT.},
	language = {en},
	urldate = {2024-06-15},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Yang, Tao and Shi, Tianyuan and Wan, Fanqi and Quan, Xiaojun and Wang, Qifan and Wu, Bingzhe and Wu, Jiaxiang},
	year = {2023},
	pages = {3305--3320},
	file = {Yang et al. - 2023 - PsyCoT Psychological Questionnaire as Powerful Ch.pdf:/Users/Maria/Zotero/storage/5CGGJ32D/Yang et al. - 2023 - PsyCoT Psychological Questionnaire as Powerful Ch.pdf:application/pdf},
}

@misc{wang_user_2023,
	title = {User {Behavior} {Simulation} with {Large} {Language} {Model} based {Agents}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2306.02552},
	doi = {10.48550/ARXIV.2306.02552},
	abstract = {Simulating high quality user behavior data has always been a fundamental problem in human-centered applications, where the major difficulty originates from the intricate mechanism of human decision process. Recently, substantial evidences have suggested that by learning huge amounts of web knowledge, large language models (LLMs) can achieve human-like intelligence. We believe these models can provide significant opportunities to more believable user behavior simulation. To inspire such direction, we propose an LLM-based agent framework and design a sandbox environment to simulate real user behaviors. Based on extensive experiments, we find that the simulated behaviors of our method are very close to the ones of real humans. Concerning potential applications, we simulate and study two social phenomenons including (1) information cocoons and (2) user conformity behaviors. This research provides novel simulation paradigms for human-centered applications.},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Wang, Lei and Zhang, Jingsen and Yang, Hao and Chen, Zhiyuan and Tang, Jiakai and Zhang, Zeyu and Chen, Xu and Lin, Yankai and Song, Ruihua and Zhao, Wayne Xin and Xu, Jun and Dou, Zhicheng and Wang, Jun and Wen, Ji-Rong},
	year = {2023},
	note = {Version Number: 3},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Information Retrieval (cs.IR)},
}

@inproceedings{aher_using_2023,
author = {Aher, Gati and Arriaga, Rosa I. and Kalai, Adam Tauman},
title = {Using large language models to simulate multiple humans and replicate human subject studies},
year = {2023},
publisher = {JMLR.org},
abstract = {We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a "hyper-accuracy distortion" present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {17},
numpages = {35},
location = {<conf-loc>, <city>Honolulu</city>, <state>Hawaii</state>, <country>USA</country>, </conf-loc>},
series = {ICML'23}
}

@misc{horton_large_2023,
	title = {Large {Language} {Models} as {Simulated} {Economic} {Agents}: {What} {Can} {We} {Learn} from {Homo} {Silicus}?},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Large {Language} {Models} as {Simulated} {Economic} {Agents}},
	url = {https://arxiv.org/abs/2301.07543},
	doi = {10.48550/ARXIV.2301.07543},
	abstract = {Newly-developed large language models (LLM) -- because of how they are trained and designed -- are implicit computational models of humans -- a homo silicus. These models can be used the same way economists use homo economicus: they can be given endowments, information, preferences, and so on and then their behavior can be explored in scenarios via simulation. I demonstrate this approach using OpenAI's GPT3 with experiments derived from Charness and Rabin (2002), Kahneman, Knetsch and Thaler (1986) and Samuelson and Zeckhauser (1988). The findings are qualitatively similar to the original results, but it is also trivially easy to try variations that offer fresh insights. Departing from the traditional laboratory paradigm, I also create a hiring scenario where an employer faces applicants that differ in experience and wage ask and then analyze how a minimum wage affects realized wages and the extent of labor-labor substitution.},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Horton, John J.},
	year = {2023},
	note = {Version Number: 1},
	keywords = {FOS: Economics and business, General Economics (econ.GN)},
}

@inproceedings{hamalainen_evaluating_2023,
	address = {Hamburg Germany},
	title = {Evaluating {Large} {Language} {Models} in {Generating} {Synthetic} {HCI} {Research} {Data}: a {Case} {Study}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {Evaluating {Large} {Language} {Models} in {Generating} {Synthetic} {HCI} {Research} {Data}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3580688},
	doi = {10.1145/3544548.3580688},
	language = {en},
	urldate = {2024-06-15},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Hämäläinen, Perttu and Tavast, Mikke and Kunnari, Anton},
	month = apr,
	year = {2023},
	pages = {1--19},
	file = {Full Text:/Users/Maria/Zotero/storage/KT8ZL83P/Hämäläinen et al. - 2023 - Evaluating Large Language Models in Generating Syn.pdf:application/pdf},
}

@misc{gao_s3_2023,
	title = {S3: {Social}-network {Simulation} {System} with {Large} {Language} {Model}-{Empowered} {Agents}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {S3},
	url = {https://arxiv.org/abs/2307.14984},
	doi = {10.48550/ARXIV.2307.14984},
	abstract = {Social network simulation plays a crucial role in addressing various challenges within social science. It offers extensive applications such as state prediction, phenomena explanation, and policy-making support, among others. In this work, we harness the formidable human-like capabilities exhibited by large language models (LLMs) in sensing, reasoning, and behaving, and utilize these qualities to construct the S\${\textasciicircum}3\$ system (short for \${\textbackslash}textbf\{S\}\$ocial network \${\textbackslash}textbf\{S\}\$imulation \${\textbackslash}textbf\{S\}\$ystem). Adhering to the widely employed agent-based simulation paradigm, we employ prompt engineering and prompt tuning techniques to ensure that the agent's behavior closely emulates that of a genuine human within the social network. Specifically, we simulate three pivotal aspects: emotion, attitude, and interaction behaviors. By endowing the agent in the system with the ability to perceive the informational environment and emulate human actions, we observe the emergence of population-level phenomena, including the propagation of information, attitudes, and emotions. We conduct an evaluation encompassing two levels of simulation, employing real-world social network data. Encouragingly, the results demonstrate promising accuracy. This work represents an initial step in the realm of social network simulation empowered by LLM-based agents. We anticipate that our endeavors will serve as a source of inspiration for the development of simulation systems within, but not limited to, social science.},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Gao, Chen and Lan, Xiaochong and Lu, Zhihong and Mao, Jinzhu and Piao, Jinghua and Wang, Huandong and Jin, Depeng and Li, Yong},
	year = {2023},
	note = {Version Number: 2},
	keywords = {FOS: Computer and information sciences, Social and Information Networks (cs.SI)},
}

@inproceedings{hidalgo2019mapping,
  title={Mapping uncertainty around research data: a Digital Humanities transdisciplinary perspective adopting the Johari window},
  author={Hidalgo, Enric Senabre and Wandl-Vogt, Eveline and Dorn, Amelie and Souza, Renato Rocha},
  booktitle={Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
  pages={804--809},
  year={2019}
}

@incollection{kuhn2017empirical,
  title={An empirical investigation of self-attitudes},
  author={Kuhn, Manford H and McPartland, Thomas S},
  booktitle={Sociological Methods},
  pages={167--182},
  year={2017},
  publisher={Routledge}
}


@article{Jones2000ACM,
  title={A Conceptual Model of Multiple Dimensions of Identity},
  author={Susan Jones and Marylu K. McEwen},
  journal={Journal of College Student Development},
  year={2000},
  volume={41},
  url={https://api.semanticscholar.org/CorpusID:37394550}
}


@article{allport1937personality,
  title={Personality: A psychological interpretation.},
  author={Allport, Gordon Willard},
  year={1937},
  publisher={Holt}
}

@article{schwartz1994there,
  title={Are there universal aspects in the structure and contents of human values?},
  author={Schwartz, Shalom H},
  journal={Journal of social issues},
  volume={50},
  number={4},
  pages={19--45},
  year={1994},
  publisher={Wiley Online Library}
}

@article{schwartz1987toward,
  title={Toward a universal psychological structure of human values.},
  author={Schwartz, Shalom H and Bilsky, Wolfgang},
  journal={Journal of personality and social psychology},
  volume={53},
  number={3},
  pages={550},
  year={1987},
  publisher={American Psychological Association}
}

@article{frederickx2014role,
  title={The role of personality in the initiation of communication situations},
  author={Frederickx, Sofie and Hofmans, Joeri},
  journal={Journal of Individual Differences},
  year={2014},
  publisher={Hogrefe Publishing}
}

@article{corr&matthews2009,
author = {Corr, Philip and Matthews, Gerald},
year = {2009},
month = {01},
pages = {1-906},
title = {The Cambridge Handbook of Personality Psychology},
doi = {10.1017/CBO9780511596544.002}
}

@book{weinberg_foundations_2019,
	title = {Foundations of {Sport} and {Exercise} {Psychology}, {7E}},
	isbn = {978-1-4925-6114-9},
	url = {https://books.google.co.kr/books?id=ACBwDwAAQBAJ},
	publisher = {Human Kinetics},
	author = {Weinberg, R.S. and Gould, D.},
	year = {2019},
	lccn = {2018012860},
}

@article{schwartzpvq2009basic,
  title={Basic human values},
  author={Schwartz, Shalom H},
  journal={sociologie},
  volume={42},
  pages={249--288},
  year={2009}
}


@article{bfi_short_2017,
	title = {Short and extra-short forms of the {Big} {Five} {Inventory}–2: {The} {BFI}-2-{S} and {BFI}-2-{XS}},
	volume = {68},
	issn = {00926566},
	shorttitle = {Short and extra-short forms of the {Big} {Five} {Inventory}–2},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0092656616301325},
	doi = {10.1016/j.jrp.2017.02.004},
	language = {en},
	urldate = {2024-06-15},
	journal = {Journal of Research in Personality},
	author = {Soto, Christopher J. and John, Oliver P.},
	month = jun,
	year = {2017},
	pages = {69--81},
}

@misc{sandy_21-item_2018,
	title = {21-{Item} {Portrait} {Values} {Questionnaire}--{Modified} {Version}},
	url = {https://doi.apa.org/doi/10.1037/t64425-000},
	doi = {10.1037/t64425-000},
	language = {en},
	urldate = {2024-06-15},
	author = {Sandy, Carson J. and Gosling, Samuel D. and Schwartz, Shalom H. and Koelkebeck, Tim},
	month = jan,
	year = {2018},
	note = {Institution: American Psychological Association},
}

@article{fearon1999identity,
  title={What is identity (as we now use the word)},
  author={Fearon, James D},
  journal={Unpublished manuscript, Stanford University, Stanford, Calif},
  pages={1--43},
  year={1999}
}

@inproceedings{demollm2024,
author = {Lee, Messi H.J. and Montgomery, Jacob M. and Lai, Calvin K.},
title = {Large Language Models Portray Socially Subordinate Groups as More Homogeneous, Consistent with a Bias Observed in Humans},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3658975},
doi = {10.1145/3630106.3658975},
abstract = {Large language models (LLMs) are becoming pervasive in everyday life, yet their propensity to reproduce biases inherited from training data remains a pressing concern. Prior investigations into bias in LLMs have focused on the association of social groups with stereotypical attributes. However, this is only one form of human bias such systems may reproduce. We investigate a new form of bias in LLMs that resembles a social psychological phenomenon where socially subordinate groups are perceived as more homogeneous than socially dominant groups. We had ChatGPT, a state-of-the-art LLM, generate texts about intersectional group identities and compared those texts on measures of homogeneity. We consistently found that ChatGPT portrayed African, Asian, and Hispanic Americans as more homogeneous than White Americans, indicating that the model described racial minority groups with a narrower range of human experience. ChatGPT also portrayed women as more homogeneous than men, but these differences were small. Finally, we found that the effect of gender differed across racial/ethnic groups such that the effect of gender was consistent within African and Hispanic Americans but not within Asian and White Americans. We argue that the tendency of LLMs to describe groups as less diverse risks perpetuating stereotypes and discriminatory behavior.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1321–1340},
numpages = {20},
keywords = {AI Bias, Homogeneity Bias, Large Language Models, Perceived Variability, Stereotyping},
location = {<conf-loc>, <city>Rio de Janeiro</city>, <country>Brazil</country>, </conf-loc>},
series = {FAccT '24}
}



@misc{gupta_bias_2023,
	title = {Bias {Runs} {Deep}: {Implicit} {Reasoning} {Biases} in {Persona}-{Assigned} {LLMs}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Bias {Runs} {Deep}},
	url = {https://arxiv.org/abs/2311.04892},
	doi = {10.48550/ARXIV.2311.04892},
	abstract = {Recent works have showcased the ability of LLMs to embody diverse personas in their responses, exemplified by prompts like 'You are Yoda. Explain the Theory of Relativity.' While this ability allows personalization of LLMs and enables human behavior simulation, its effect on LLMs' capabilities remains unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs to perform basic reasoning tasks. Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse personas (e.g. an Asian person) spanning 5 socio-demographic groups. Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness. While they overtly reject stereotypes when explicitly asked ('Are Black people less skilled at mathematics?'), they manifest stereotypical and erroneous presumptions when asked to answer questions while adopting a persona. These can be observed as abstentions in responses, e.g., 'As a Black person, I can't answer this question as it requires math knowledge', and generally result in a substantial performance drop. Our experiments with ChatGPT-3.5 show that this bias is ubiquitous - 80\% of our personas demonstrate bias; it is significant - some datasets show performance drops of 70\%+; and can be especially harmful for certain groups - some personas suffer statistically significant drops on 80\%+ of the datasets. Overall, all 4 LLMs exhibit this bias to varying extents, with GPT-4-Turbo showing the least but still a problematic amount of bias (evident in 42\% of the personas). Further analysis shows that these persona-induced errors can be hard-to-discern and hard-to-avoid. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs - a trend on the rise - can surface their deep-rooted biases and have unforeseeable and detrimental side-effects.},
	urldate = {2024-06-16},
	publisher = {arXiv},
	author = {Gupta, Shashank and Shrivastava, Vaishnavi and Deshpande, Ameet and Kalyan, Ashwin and Clark, Peter and Sabharwal, Ashish and Khot, Tushar},
	year = {2023},
	note = {Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}
