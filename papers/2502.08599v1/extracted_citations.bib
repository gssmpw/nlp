@inproceedings{aher_using_2023,
author = {Aher, Gati and Arriaga, Rosa I. and Kalai, Adam Tauman},
title = {Using large language models to simulate multiple humans and replicate human subject studies},
year = {2023},
publisher = {JMLR.org},
abstract = {We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a "hyper-accuracy distortion" present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {17},
numpages = {35},
location = {<conf-loc>, <city>Honolulu</city>, <state>Hawaii</state>, <country>USA</country>, </conf-loc>},
series = {ICML'23}
}

@article{argyle_out_2023,
	title = {Out of {One}, {Many}: {Using} {Language} {Models} to {Simulate} {Human} {Samples}},
	volume = {31},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Out of {One}, {Many}},
	url = {https://www.cambridge.org/core/product/identifier/S1047198723000025/type/journal_article},
	doi = {10.1017/pan.2023.2},
	abstract = {Abstract
            
              We propose and explore the possibility that language models can be studied as effective proxies for specific human subpopulations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the “algorithmic bias” within one such tool—the GPT-3 language model—is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property
              algorithmic fidelity
              and explore its extent in GPT-3. We create “silicon samples” by conditioning the model on thousands of sociodemographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and sociocultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines.},
	language = {en},
	number = {3},
	urldate = {2023-08-24},
	journal = {Political Analysis},
	author = {Argyle, Lisa P. and Busby, Ethan C. and Fulda, Nancy and Gubler, Joshua R. and Rytting, Christopher and Wingate, David},
	month = jul,
	year = {2023},
	pages = {337--351},
	file = {Submitted Version:/Users/keyeun/Zotero/storage/KGTBEIC9/Argyle et al. - 2023 - Out of One, Many Using Language Models to Simulat.pdf:application/pdf},
}

@inproceedings{chen_empathy_2024,
author = {Chen, Chaoran and Li, Weijun and Song, Wenxin and Ye, Yanfang and Yao, Yaxing and Li, Toby Jia-Jun},
title = {An Empathy-Based Sandbox Approach to Bridge the Privacy Gap among Attitudes, Goals, Knowledge, and Behaviors},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642363},
doi = {10.1145/3613904.3642363},
abstract = {Managing privacy to reach privacy goals is challenging, as evidenced by the privacy attitude-behavior gap. Mitigating this discrepancy requires solutions that account for both system opaqueness and users’ hesitations in testing different privacy settings due to fears of unintended data exposure. We introduce an empathy-based approach that allows users to experience how privacy attributes may alter system outcomes in a risk-free sandbox environment from the perspective of artificially generated personas. To generate realistic personas, we introduce a novel pipeline that augments the outputs of large language models (e.g., GPT-4) using few-shot learning, contextualization, and chain of thoughts. Our empirical studies demonstrated the adequate quality of generated personas and highlighted the changes in privacy-related applications (e.g., online advertising) caused by different personas. Furthermore, users demonstrated cognitive and emotional empathy towards the personas when interacting with our sandbox. We offered design implications for downstream applications in improving user privacy literacy.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {234},
numpages = {28},
keywords = {empathy, generated personas, privacy awareness, privacy intervention, privacy literacy, sandbox},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{cheng_compost_2023,
	address = {Singapore},
	title = {{CoMPosT}: {Characterizing} and {Evaluating} {Caricature} in {LLM} {Simulations}},
	shorttitle = {{CoMPosT}},
	url = {https://aclanthology.org/2023.emnlp-main.669},
	doi = {10.18653/v1/2023.emnlp-main.669},
	abstract = {Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations’ susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.},
	language = {en},
	urldate = {2024-06-15},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Cheng, Myra and Piccardi, Tiziano and Yang, Diyi},
	year = {2023},
	pages = {10853--10875},
	file = {Cheng et al. - 2023 - CoMPosT Characterizing and Evaluating Caricature .pdf:/Users/Maria/Zotero/storage/KAPVZBJ9/Cheng et al. - 2023 - CoMPosT Characterizing and Evaluating Caricature .pdf:application/pdf},
}

@inproceedings{chuang-etal-2024-simulating,
    title = "Simulating Opinion Dynamics with Networks of {LLM}-based Agents",
    author = "Chuang, Yun-Shiuan  and
      Goyal, Agam  and
      Harlalka, Nikunj  and
      Suresh, Siddharth  and
      Hawkins, Robert  and
      Yang, Sijia  and
      Shah, Dhavan  and
      Hu, Junjie  and
      Rogers, Timothy",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.211",
    doi = "10.18653/v1/2024.findings-naacl.211",
    pages = "3326--3346",
    abstract = "Accurately simulating human opinion dynamics is crucial for understanding a variety of societal phenomena, including polarization and the spread of misinformation. However, the agent-based models (ABMs) commonly used for such simulations often over-simplify human behavior. We propose a new approach to simulating opinion dynamics based on populations of Large Language Models (LLMs). Our findings reveal a strong inherent bias in LLM agents towards producing accurate information, leading simulated agents to consensus in line with scientific reality. This bias limits their utility for understanding resistance to consensus views on issues like climate change. After inducing confirmation bias through prompt engineering, however, we observed opinion fragmentation in line with existing agent-based modeling and opinion dynamics research. These insights highlight the promise and limitations of LLM agents in this domain and suggest a path forward: refining LLMs with real-world discourse to better simulate the evolution of human beliefs.",
}

@inproceedings{deusex_2024,
author = {Salminen, Joni and Liu, Chang and Pian, Wenjing and Chi, Jianxing and H\"{a}yh\"{a}nen, Essi and Jansen, Bernard J},
title = {Deus Ex Machina and Personas from Large Language Models: Investigating the Composition of AI-Generated Persona Descriptions},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642036},
doi = {10.1145/3613904.3642036},
abstract = {Large language models (LLMs) can generate personas based on prompts that describe the target user group. To understand what kind of personas LLMs generate, we investigate the diversity and bias in 450 LLM-generated personas with the help of internal evaluators (n=4) and subject-matter experts (SMEs) (n=5). The research findings reveal biases in LLM-generated personas, particularly in age, occupation, and pain points, as well as a strong bias towards personas from the United States. Human evaluations demonstrate that LLM persona descriptions were informative, believable, positive, relatable, and not stereotyped. The SMEs rated the personas slightly more stereotypical, less positive, and less relatable than the internal evaluators. The findings suggest that LLMs can generate consistent personas perceived as believable, relatable, and informative while containing relatively low amounts of stereotyping.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {510},
numpages = {20},
keywords = {AI, HCI, LLMs, evaluation, user personas},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@misc{gao_s3_2023,
	title = {S3: {Social}-network {Simulation} {System} with {Large} {Language} {Model}-{Empowered} {Agents}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {S3},
	url = {https://arxiv.org/abs/2307.14984},
	doi = {10.48550/ARXIV.2307.14984},
	abstract = {Social network simulation plays a crucial role in addressing various challenges within social science. It offers extensive applications such as state prediction, phenomena explanation, and policy-making support, among others. In this work, we harness the formidable human-like capabilities exhibited by large language models (LLMs) in sensing, reasoning, and behaving, and utilize these qualities to construct the S\${\textasciicircum}3\$ system (short for \${\textbackslash}textbf\{S\}\$ocial network \${\textbackslash}textbf\{S\}\$imulation \${\textbackslash}textbf\{S\}\$ystem). Adhering to the widely employed agent-based simulation paradigm, we employ prompt engineering and prompt tuning techniques to ensure that the agent's behavior closely emulates that of a genuine human within the social network. Specifically, we simulate three pivotal aspects: emotion, attitude, and interaction behaviors. By endowing the agent in the system with the ability to perceive the informational environment and emulate human actions, we observe the emergence of population-level phenomena, including the propagation of information, attitudes, and emotions. We conduct an evaluation encompassing two levels of simulation, employing real-world social network data. Encouragingly, the results demonstrate promising accuracy. This work represents an initial step in the realm of social network simulation empowered by LLM-based agents. We anticipate that our endeavors will serve as a source of inspiration for the development of simulation systems within, but not limited to, social science.},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Gao, Chen and Lan, Xiaochong and Lu, Zhihong and Mao, Jinzhu and Piao, Jinghua and Wang, Huandong and Jin, Depeng and Li, Yong},
	year = {2023},
	note = {Version Number: 2},
	keywords = {FOS: Computer and information sciences, Social and Information Networks (cs.SI)},
}

@misc{gupta_bias_2023,
	title = {Bias {Runs} {Deep}: {Implicit} {Reasoning} {Biases} in {Persona}-{Assigned} {LLMs}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Bias {Runs} {Deep}},
	url = {https://arxiv.org/abs/2311.04892},
	doi = {10.48550/ARXIV.2311.04892},
	abstract = {Recent works have showcased the ability of LLMs to embody diverse personas in their responses, exemplified by prompts like 'You are Yoda. Explain the Theory of Relativity.' While this ability allows personalization of LLMs and enables human behavior simulation, its effect on LLMs' capabilities remains unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs to perform basic reasoning tasks. Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse personas (e.g. an Asian person) spanning 5 socio-demographic groups. Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness. While they overtly reject stereotypes when explicitly asked ('Are Black people less skilled at mathematics?'), they manifest stereotypical and erroneous presumptions when asked to answer questions while adopting a persona. These can be observed as abstentions in responses, e.g., 'As a Black person, I can't answer this question as it requires math knowledge', and generally result in a substantial performance drop. Our experiments with ChatGPT-3.5 show that this bias is ubiquitous - 80\% of our personas demonstrate bias; it is significant - some datasets show performance drops of 70\%+; and can be especially harmful for certain groups - some personas suffer statistically significant drops on 80\%+ of the datasets. Overall, all 4 LLMs exhibit this bias to varying extents, with GPT-4-Turbo showing the least but still a problematic amount of bias (evident in 42\% of the personas). Further analysis shows that these persona-induced errors can be hard-to-discern and hard-to-avoid. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs - a trend on the rise - can surface their deep-rooted biases and have unforeseeable and detrimental side-effects.},
	urldate = {2024-06-16},
	publisher = {arXiv},
	author = {Gupta, Shashank and Shrivastava, Vaishnavi and Deshpande, Ameet and Kalyan, Ashwin and Clark, Peter and Sabharwal, Ashish and Khot, Tushar},
	year = {2023},
	note = {Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@inproceedings{hamalainen_evaluating_2023,
	address = {Hamburg Germany},
	title = {Evaluating {Large} {Language} {Models} in {Generating} {Synthetic} {HCI} {Research} {Data}: a {Case} {Study}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {Evaluating {Large} {Language} {Models} in {Generating} {Synthetic} {HCI} {Research} {Data}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3580688},
	doi = {10.1145/3544548.3580688},
	language = {en},
	urldate = {2024-06-15},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Hämäläinen, Perttu and Tavast, Mikke and Kunnari, Anton},
	month = apr,
	year = {2023},
	pages = {1--19},
	file = {Full Text:/Users/Maria/Zotero/storage/KT8ZL83P/Hämäläinen et al. - 2023 - Evaluating Large Language Models in Generating Syn.pdf:application/pdf},
}

@misc{horton_large_2023,
	title = {Large {Language} {Models} as {Simulated} {Economic} {Agents}: {What} {Can} {We} {Learn} from {Homo} {Silicus}?},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Large {Language} {Models} as {Simulated} {Economic} {Agents}},
	url = {https://arxiv.org/abs/2301.07543},
	doi = {10.48550/ARXIV.2301.07543},
	abstract = {Newly-developed large language models (LLM) -- because of how they are trained and designed -- are implicit computational models of humans -- a homo silicus. These models can be used the same way economists use homo economicus: they can be given endowments, information, preferences, and so on and then their behavior can be explored in scenarios via simulation. I demonstrate this approach using OpenAI's GPT3 with experiments derived from Charness and Rabin (2002), Kahneman, Knetsch and Thaler (1986) and Samuelson and Zeckhauser (1988). The findings are qualitatively similar to the original results, but it is also trivially easy to try variations that offer fresh insights. Departing from the traditional laboratory paradigm, I also create a hiring scenario where an employer faces applicants that differ in experience and wage ask and then analyze how a minimum wage affects realized wages and the extent of labor-labor substitution.},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Horton, John J.},
	year = {2023},
	note = {Version Number: 1},
	keywords = {FOS: Economics and business, General Economics (econ.GN)},
}

@inproceedings{jiang-etal-2024-personallm,
    title = "{P}ersona{LLM}: Investigating the Ability of Large Language Models to Express Personality Traits",
    author = "Jiang, Hang  and
      Zhang, Xiajie  and
      Cao, Xubo  and
      Breazeal, Cynthia  and
      Roy, Deb  and
      Kabbara, Jad",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.229",
    doi = "10.18653/v1/2024.findings-naacl.229",
    pages = "3605--3627",
    abstract = "Despite the many use cases for large language models (LLMs) in creating personalized chatbots, there has been limited research on evaluating the extent to which the behaviors of personalized LLMs accurately and consistently reflect specific personality traits. We consider studying the behavior of LLM-based agents which we refer to as LLM personas and present a case study with GPT-3.5 and GPT-4 to investigate whether LLMs can generate content that aligns with their assigned personality profiles. To this end, we simulate distinct LLM personas based on the Big Five personality model, have them complete the 44-item Big Five Inventory (BFI) personality test and a story writing task, and then assess their essays with automatic and human evaluations. Results show that LLM personas{'} self-reported BFI scores are consistent with their designated personality types, with large effect sizes observed across five traits. Additionally, LLM personas{'} writings have emerging representative linguistic patterns for personality traits when compared with a human writing corpus. Furthermore, human evaluation shows that humans can perceive some personality traits with an accuracy of up to 80{\%}. Interestingly, the accuracy drops significantly when the annotators were informed of AI authorship.",
}

@inproceedings{kang_values_2023,
	address = {Singapore},
	title = {From {Values} to {Opinions}: {Predicting} {Human} {Behaviors} and {Stances} {Using} {Value}-{Injected} {Large} {Language} {Models}},
	shorttitle = {From {Values} to {Opinions}},
	url = {https://aclanthology.org/2023.emnlp-main.961},
	doi = {10.18653/v1/2023.emnlp-main.961},
	language = {en},
	urldate = {2024-06-14},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Kang, Dongjun and Park, Joonsuk and Jo, Yohan and Bak, JinYeong},
	year = {2023},
	pages = {15539--15559},
	file = {Full Text:/Users/keyeun/Zotero/storage/M7YFA86M/Kang et al. - 2023 - From Values to Opinions Predicting Human Behavior.pdf:application/pdf},
}

@inproceedings{lee_large_2024,
	address = {Rio de Janeiro Brazil},
	title = {Large {Language} {Models} {Portray} {Socially} {Subordinate} {Groups} as {More} {Homogeneous}, {Consistent} with a {Bias} {Observed} in {Humans}},
	isbn = {9798400704505},
	url = {https://dl.acm.org/doi/10.1145/3630106.3658975},
	doi = {10.1145/3630106.3658975},
	language = {en},
	urldate = {2024-06-15},
	booktitle = {The 2024 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Lee, Messi H.J. and Montgomery, Jacob M. and Lai, Calvin K.},
	month = jun,
	year = {2024},
	pages = {1321--1340},
	file = {Submitted Version:/Users/keyeun/Zotero/storage/W3VEQH8R/Lee et al. - 2024 - Large Language Models Portray Socially Subordinate.pdf:application/pdf},
}

@article{liu2024skepticism,
  title={From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News},
  author={Liu, Yuhan and Chen, Xiuying and Zhang, Xiaoqing and Gao, Xing and Zhang, Ji and Yan, Rui},
  journal={arXiv preprint arXiv:2403.09498},
  year={2024}
}

@article{mead1934mind,
  title={Mind, self, and society from the standpoint of a social behaviorist.},
  author={Mead, George Herbert},
  year={1934},
  publisher={Chicago}
}

@inproceedings{park_generative_2023,
	address = {San Francisco CA USA},
	title = {Generative {Agents}: {Interactive} {Simulacra} of {Human} {Behavior}},
	isbn = {9798400701320},
	shorttitle = {Generative {Agents}},
	url = {https://dl.acm.org/doi/10.1145/3586183.3606763},
	doi = {10.1145/3586183.3606763},
	language = {en},
	urldate = {2024-06-15},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
	month = oct,
	year = {2023},
	pages = {1--22},
	file = {Full Text:/Users/Maria/Zotero/storage/UDKGBAP6/Park et al. - 2023 - Generative Agents Interactive Simulacra of Human .pdf:application/pdf},
}

@inproceedings{park_social_2022,
	address = {Bend OR USA},
	title = {Social {Simulacra}: {Creating} {Populated} {Prototypes} for {Social} {Computing} {Systems}},
	isbn = {978-1-4503-9320-1},
	shorttitle = {Social {Simulacra}},
	url = {https://dl.acm.org/doi/10.1145/3526113.3545616},
	doi = {10.1145/3526113.3545616},
	language = {en},
	urldate = {2024-06-15},
	booktitle = {Proceedings of the 35th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Park, Joon Sung and Popowski, Lindsay and Cai, Carrie and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
	month = oct,
	year = {2022},
	pages = {1--18},
	file = {Full Text:/Users/Maria/Zotero/storage/3RQ4YWSM/Park et al. - 2022 - Social Simulacra Creating Populated Prototypes fo.pdf:application/pdf},
}

@misc{petrov_limited_2024,
	title = {Limited {Ability} of {LLMs} to {Simulate} {Human} {Psychological} {Behaviours}: a {Psychometric} {Analysis}},
	shorttitle = {Limited {Ability} of {LLMs} to {Simulate} {Human} {Psychological} {Behaviours}},
	url = {http://arxiv.org/abs/2405.07248},
	abstract = {The humanlike responses of large language models (LLMs) have prompted social scientists to investigate whether LLMs can be used to simulate human participants in experiments, opinion polls and surveys. Of central interest in this line of research has been mapping out the psychological profiles of LLMs by prompting them to respond to standardized questionnaires. The conflicting findings of this research are unsurprising given that mapping out underlying, or latent, traits from LLMs' text responses to questionnaires is no easy task. To address this, we use psychometrics, the science of psychological measurement. In this study, we prompt OpenAI's flagship models, GPT-3.5 and GPT-4, to assume different personas and respond to a range of standardized measures of personality constructs. We used two kinds of persona descriptions: either generic (four or five random person descriptions) or specific (mostly demographics of actual humans from a large-scale human dataset). We found that the responses from GPT-4, but not GPT-3.5, using generic persona descriptions show promising, albeit not perfect, psychometric properties, similar to human norms, but the data from both LLMs when using specific demographic profiles, show poor psychometrics properties. We conclude that, currently, when LLMs are asked to simulate silicon personas, their responses are poor signals of potentially underlying latent traits. Thus, our work casts doubt on LLMs' ability to simulate individual-level human behaviour across multiple-choice question answering tasks.},
	urldate = {2024-06-13},
	publisher = {arXiv},
	author = {Petrov, Nikolay B. and Serapio-García, Gregory and Rentfrow, Jason},
	month = may,
	year = {2024},
	note = {arXiv:2405.07248 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, I.2.7, 68T50},
	file = {arXiv Fulltext PDF:/Users/keyeun/Zotero/storage/4JG8KSPI/Petrov et al. - 2024 - Limited Ability of LLMs to Simulate Human Psycholo.pdf:application/pdf;arXiv.org Snapshot:/Users/keyeun/Zotero/storage/ZR89R4PP/2405.html:text/html},
}

@misc{wang_user_2023,
	title = {User {Behavior} {Simulation} with {Large} {Language} {Model} based {Agents}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2306.02552},
	doi = {10.48550/ARXIV.2306.02552},
	abstract = {Simulating high quality user behavior data has always been a fundamental problem in human-centered applications, where the major difficulty originates from the intricate mechanism of human decision process. Recently, substantial evidences have suggested that by learning huge amounts of web knowledge, large language models (LLMs) can achieve human-like intelligence. We believe these models can provide significant opportunities to more believable user behavior simulation. To inspire such direction, we propose an LLM-based agent framework and design a sandbox environment to simulate real user behaviors. Based on extensive experiments, we find that the simulated behaviors of our method are very close to the ones of real humans. Concerning potential applications, we simulate and study two social phenomenons including (1) information cocoons and (2) user conformity behaviors. This research provides novel simulation paradigms for human-centered applications.},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Wang, Lei and Zhang, Jingsen and Yang, Hao and Chen, Zhiyuan and Tang, Jiakai and Zhang, Zeyu and Chen, Xu and Lin, Yankai and Song, Ruihua and Zhao, Wayne Xin and Xu, Jun and Dou, Zhicheng and Wang, Jun and Wen, Ji-Rong},
	year = {2023},
	note = {Version Number: 3},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Information Retrieval (cs.IR)},
}

@misc{xiao_how_2023,
	title = {How {Far} {Are} {We} from {Believable} {AI} {Agents}? {A} {Framework} for {Evaluating} the {Believability} of {Human} {Behavior} {Simulation}},
	shorttitle = {How {Far} {Are} {We} from {Believable} {AI} {Agents}?},
	url = {http://arxiv.org/abs/2312.17115},
	abstract = {Human behavior simulation of AI agents necessitates the agents to possess a quality of believability, which is crucial as it facilitates users in establishing trust toward the agents and streamlines the fulfillment of the agents' goal. While recent advancements in Large Language Model (LLM) based agents have improved human behavior simulation, challenges inherent to LLMs (e.g., long context modeling) can undermine their believability. Consequently, evaluating AI agent believability becomes imperative. Unfortunately, prior research often neglects the negative impacts of LLM deficiencies. To address these gaps, we introduce two metrics for assessing LLM-based agent believability: consistency, and robustness, together with a benchmark, SimulateBench, with which, we evaluate the consistency and robustness of agents implemented with popular LLMs. We find that agents (i) struggle to accurately depict character information when presented with lengthy profile inputs; (ii) exhibit vulnerability to profile perturbations; and (iii) are significantly affected by certain key factors that impact their overall believability. Code and SimulateBench are public at https://github.com/GAIR-NLP/GPTMan.},
	language = {en},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Xiao, Yang and Cheng, Yi and Fu, Jinlan and Wang, Jiashuo and Li, Wenjie and Liu, Pengfei},
	month = dec,
	year = {2023},
	note = {arXiv:2312.17115 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {Xiao et al. - 2023 - How Far Are We from Believable AI Agents A Framew.pdf:/Users/Maria/Zotero/storage/YEVNJ8YQ/Xiao et al. - 2023 - How Far Are We from Believable AI Agents A Framew.pdf:application/pdf},
}

@misc{xie2024largelanguagemodelagents,
      title={Can Large Language Model Agents Simulate Human Trust Behaviors?}, 
      author={Chengxing Xie and Canyu Chen and Feiran Jia and Ziyu Ye and Kai Shu and Adel Bibi and Ziniu Hu and Philip Torr and Bernard Ghanem and Guohao Li},
      year={2024},
      eprint={2402.04559},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.04559}, 
}

@misc{xie_human_2024,
	title = {Human {Simulacra}: {Benchmarking} the {Personification} of {Large} {Language} {Models}},
	shorttitle = {Human {Simulacra}},
	url = {http://arxiv.org/abs/2402.18180},
	abstract = {Large language models (LLMs) are recognized as systems that closely mimic aspects of human intelligence. This capability has attracted attention from the social science community, who see the potential in leveraging LLMs to replace human participants in experiments, thereby reducing research costs and complexity. In this paper, we introduce a framework for large language models personification, including a strategy for constructing virtual characters’ life stories from the ground up, a Multi-Agent Cognitive Mechanism capable of simulating human cognitive processes, and a psychology-guided evaluation method to assess human simulations from both self and observational perspectives. Experimental results demonstrate that our constructed simulacra can produce personified responses that align with their target characters. Our work is a preliminary exploration which offers great potential in practical applications. All the code and datasets will be released, with the hope of inspiring further investigations.},
	language = {en},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Xie, Qiuejie and Feng, Qiming and Zhang, Tianqi and Li, Qingqiu and Yang, Linyi and Zhang, Yuejie and Feng, Rui and He, Liang and Gao, Shang and Zhang, Yue},
	month = jun,
	year = {2024},
	note = {arXiv:2402.18180 [cs]},
	keywords = {Computer Science - Computers and Society},
	file = {Xie et al. - 2024 - Human Simulacra Benchmarking the Personification .pdf:/Users/Maria/Zotero/storage/Q9IIST8G/Xie et al. - 2024 - Human Simulacra Benchmarking the Personification .pdf:application/pdf},
}

@misc{yuan_evaluating_2024,
	title = {Evaluating {Character} {Understanding} of {Large} {Language} {Models} via {Character} {Profiling} from {Fictional} {Works}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2404.12726},
	doi = {10.48550/ARXIV.2404.12726},
	abstract = {Large language models (LLMs) have demonstrated impressive performance and spurred numerous AI applications, in which role-playing agents (RPAs) are particularly popular, especially for fictional characters. The prerequisite for these RPAs lies in the capability of LLMs to understand characters from fictional works. Previous efforts have evaluated this capability via basic classification tasks or characteristic imitation, failing to capture the nuanced character understanding with LLMs. In this paper, we propose evaluating LLMs' character understanding capability via the character profiling task, i.e., summarizing character profiles from corresponding materials, a widely adopted yet understudied practice for RPA development. Specifically, we construct the CroSS dataset from literature experts and assess the generated profiles by comparing ground truth references and their applicability in downstream tasks. Our experiments, which cover various summarization methods and LLMs, have yielded promising results. These results strongly validate the character understanding capability of LLMs. We believe our constructed resource will promote further research in this field. Resources are available at https://github.com/Joanna0123/character\_profiling.},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Yuan, Xinfeng and Yuan, Siyu and Cui, Yuhan and Lin, Tianhe and Wang, Xintao and Xu, Rui and Chen, Jiangjie and Yang, Deqing},
	year = {2024},
	note = {Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@misc{zhang_speechagents_2024,
	title = {{SpeechAgents}: {Human}-{Communication} {Simulation} with {Multi}-{Modal} {Multi}-{Agent} {Systems}},
	shorttitle = {{SpeechAgents}},
	url = {http://arxiv.org/abs/2401.03945},
	abstract = {Human communication is a complex and diverse process that not only involves multiple factors such as language, commonsense, and cultural backgrounds but also requires the participation of multimodal information, such as speech. Large Language Model (LLM)-based multi-agent systems have demonstrated promising performance in simulating human society. Can we leverage LLM-based multi-agent systems to simulate human communication? However, current LLM-based multi-agent systems mainly rely on text as the primary medium. In this paper, we propose SpeechAgents, a multi-modal LLM based multi-agent system designed for simulating human communication. SpeechAgents utilizes multi-modal LLM as the control center for individual agent and employes multi-modal signals as the medium for exchanged messages among agents. Additionally, we propose Multi-Agent Tuning to enhance the multi-agent capabilities of LLM without compromising general abilities. To strengthen and evaluate the effectiveness of human communication simulation, we build the Human-Communication Simulation Benchmark. Experimental results demonstrate that SpeechAgents can simulate human communication dialogues with consistent content, authentic rhythm, and rich emotions and demonstrate excellent scalability even with up to 25 agents, which can apply to tasks such as drama creation and audio novels generation. Code and models will be open-sourced at https://github. com/0nutation/SpeechAgents},
	urldate = {2024-06-16},
	publisher = {arXiv},
	author = {Zhang, Dong and Li, Zhaowei and Wang, Pengyu and Zhang, Xin and Zhou, Yaqian and Qiu, Xipeng},
	month = jan,
	year = {2024},
	note = {arXiv:2401.03945 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: work in progress},
	file = {arXiv Fulltext PDF:/Users/keyeun/Zotero/storage/IXA8QN3G/Zhang et al. - 2024 - SpeechAgents Human-Communication Simulation with .pdf:application/pdf;arXiv.org Snapshot:/Users/keyeun/Zotero/storage/BAJBXJIZ/2401.html:text/html},
}

@misc{zhou_sotopia_2024,
	title = {{SOTOPIA}: {Interactive} {Evaluation} for {Social} {Intelligence} in {Language} {Agents}},
	shorttitle = {{SOTOPIA}},
	url = {http://arxiv.org/abs/2310.11667},
	abstract = {Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.},
	urldate = {2024-06-16},
	publisher = {arXiv},
	author = {Zhou, Xuhui and Zhu, Hao and Mathur, Leena and Zhang, Ruohong and Yu, Haofei and Qi, Zhengyang and Morency, Louis-Philippe and Bisk, Yonatan and Fried, Daniel and Neubig, Graham and Sap, Maarten},
	month = mar,
	year = {2024},
	note = {arXiv:2310.11667 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Preprint, 43 pages. The first two authors contribute equally},
	file = {arXiv Fulltext PDF:/Users/keyeun/Zotero/storage/I9MIXQVZ/Zhou et al. - 2024 - SOTOPIA Interactive Evaluation for Social Intelli.pdf:application/pdf;arXiv.org Snapshot:/Users/keyeun/Zotero/storage/B7HTF5J9/2310.html:text/html},
}

