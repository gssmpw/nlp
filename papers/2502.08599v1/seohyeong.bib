@inproceedings{kim-etal-2020-will,
    title = "Will {I} Sound Like Me? Improving Persona Consistency in Dialogues through Pragmatic Self-Consciousness",
    author = "Kim, Hyunwoo  and
      Kim, Byeongchang  and
      Kim, Gunhee",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.65",
    doi = "10.18653/v1/2020.emnlp-main.65",
    pages = "904--916",
    abstract = "We explore the task of improving persona consistency of dialogue agents. Recent models tackling consistency often train with additional Natural Language Inference (NLI) labels or attach trained extra modules to the generative agent for maintaining consistency. However, such additional labels and training can be demanding. Also, we find even the best-performing persona-based agents are insensitive to contradictory words. Inspired by social cognition and pragmatics, we endow existing dialogue agents with public self-consciousness on the fly through an imaginary listener. Our approach, based on the Rational Speech Acts framework (Frank and Goodman, 2012), can enforce dialogue agents to refrain from uttering contradiction. We further extend the framework by learning the distractor selection, which has been usually done manually or randomly. Results on Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang et al., 2018) dataset show that our approach reduces contradiction and improves consistency of existing dialogue models. Moreover, we show that it can be generalized to improve context-consistency beyond persona in dialogues.",
}

@inproceedings{zhang-etal-2018-personalizing,
    title = "Personalizing Dialogue Agents: {I} have a dog, do you have pets too?",
    author = "Zhang, Saizheng  and
      Dinan, Emily  and
      Urbanek, Jack  and
      Szlam, Arthur  and
      Kiela, Douwe  and
      Weston, Jason",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1205",
    doi = "10.18653/v1/P18-1205",
    pages = "2204--2213",
    abstract = "Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i)condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.",
}

@misc{xie2024largelanguagemodelagents,
      title={Can Large Language Model Agents Simulate Human Trust Behaviors?}, 
      author={Chengxing Xie and Canyu Chen and Feiran Jia and Ziyu Ye and Kai Shu and Adel Bibi and Ziniu Hu and Philip Torr and Bernard Ghanem and Guohao Li},
      year={2024},
      eprint={2402.04559},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.04559}, 
}

@inproceedings{frisch-giulianelli-2024-llm,
    title = "{LLM} Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models",
    author = "Frisch, Ivar  and
      Giulianelli, Mario",
    editor = "Deshpande, Ameet  and
      Hwang, EunJeong  and
      Murahari, Vishvak  and
      Park, Joon Sung  and
      Yang, Diyi  and
      Sabharwal, Ashish  and
      Narasimhan, Karthik  and
      Kalyan, Ashwin",
    booktitle = "Proceedings of the 1st Workshop on Personalization of Generative AI Systems (PERSONALIZE 2024)",
    month = mar,
    year = "2024",
    address = "St. Julians, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.personalize-1.9",
    pages = "102--111",
    abstract = "Agent interaction has long been a key topic in psychology, philosophy, and artificial intelligence, and it is now gaining traction in large language model (LLM) research. This experimental study seeks to lay the groundwork for our understanding of dialogue-based interaction between LLMs: Do persona-prompted LLMs show consistent personality and language use in interaction? We condition GPT-3.5 on asymmetric personality profiles to create a population of LLM agents, administer personality tests and submit the agents to a collaborative writing task. We find different profiles exhibit different degrees of personality consistency and linguistic alignment in interaction.",
}

@inproceedings{jiang-etal-2024-personallm,
    title = "{P}ersona{LLM}: Investigating the Ability of Large Language Models to Express Personality Traits",
    author = "Jiang, Hang  and
      Zhang, Xiajie  and
      Cao, Xubo  and
      Breazeal, Cynthia  and
      Roy, Deb  and
      Kabbara, Jad",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.229",
    doi = "10.18653/v1/2024.findings-naacl.229",
    pages = "3605--3627",
    abstract = "Despite the many use cases for large language models (LLMs) in creating personalized chatbots, there has been limited research on evaluating the extent to which the behaviors of personalized LLMs accurately and consistently reflect specific personality traits. We consider studying the behavior of LLM-based agents which we refer to as LLM personas and present a case study with GPT-3.5 and GPT-4 to investigate whether LLMs can generate content that aligns with their assigned personality profiles. To this end, we simulate distinct LLM personas based on the Big Five personality model, have them complete the 44-item Big Five Inventory (BFI) personality test and a story writing task, and then assess their essays with automatic and human evaluations. Results show that LLM personas{'} self-reported BFI scores are consistent with their designated personality types, with large effect sizes observed across five traits. Additionally, LLM personas{'} writings have emerging representative linguistic patterns for personality traits when compared with a human writing corpus. Furthermore, human evaluation shows that humans can perceive some personality traits with an accuracy of up to 80{\%}. Interestingly, the accuracy drops significantly when the annotators were informed of AI authorship.",
}


@article{kocaballi2023conversational,
  title={Conversational ai-powered design: Chatgpt as designer, user, and product},
  author={Kocaballi, A Baki},
  journal={arXiv preprint arXiv:2302.07406},
  year={2023}
}

@inproceedings{deusex_2024,
author = {Salminen, Joni and Liu, Chang and Pian, Wenjing and Chi, Jianxing and H\"{a}yh\"{a}nen, Essi and Jansen, Bernard J},
title = {Deus Ex Machina and Personas from Large Language Models: Investigating the Composition of AI-Generated Persona Descriptions},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642036},
doi = {10.1145/3613904.3642036},
abstract = {Large language models (LLMs) can generate personas based on prompts that describe the target user group. To understand what kind of personas LLMs generate, we investigate the diversity and bias in 450 LLM-generated personas with the help of internal evaluators (n=4) and subject-matter experts (SMEs) (n=5). The research findings reveal biases in LLM-generated personas, particularly in age, occupation, and pain points, as well as a strong bias towards personas from the United States. Human evaluations demonstrate that LLM persona descriptions were informative, believable, positive, relatable, and not stereotyped. The SMEs rated the personas slightly more stereotypical, less positive, and less relatable than the internal evaluators. The findings suggest that LLMs can generate consistent personas perceived as believable, relatable, and informative while containing relatively low amounts of stereotyping.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {510},
numpages = {20},
keywords = {AI, HCI, LLMs, evaluation, user personas},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{liu2024skepticism,
  title={From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News},
  author={Liu, Yuhan and Chen, Xiuying and Zhang, Xiaoqing and Gao, Xing and Zhang, Ji and Yan, Rui},
  journal={arXiv preprint arXiv:2403.09498},
  year={2024}
}

@inproceedings{chuang-etal-2024-simulating,
    title = "Simulating Opinion Dynamics with Networks of {LLM}-based Agents",
    author = "Chuang, Yun-Shiuan  and
      Goyal, Agam  and
      Harlalka, Nikunj  and
      Suresh, Siddharth  and
      Hawkins, Robert  and
      Yang, Sijia  and
      Shah, Dhavan  and
      Hu, Junjie  and
      Rogers, Timothy",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.211",
    doi = "10.18653/v1/2024.findings-naacl.211",
    pages = "3326--3346",
    abstract = "Accurately simulating human opinion dynamics is crucial for understanding a variety of societal phenomena, including polarization and the spread of misinformation. However, the agent-based models (ABMs) commonly used for such simulations often over-simplify human behavior. We propose a new approach to simulating opinion dynamics based on populations of Large Language Models (LLMs). Our findings reveal a strong inherent bias in LLM agents towards producing accurate information, leading simulated agents to consensus in line with scientific reality. This bias limits their utility for understanding resistance to consensus views on issues like climate change. After inducing confirmation bias through prompt engineering, however, we observed opinion fragmentation in line with existing agent-based modeling and opinion dynamics research. These insights highlight the promise and limitations of LLM agents in this domain and suggest a path forward: refining LLMs with real-world discourse to better simulate the evolution of human beliefs.",
}

@inproceedings{chen_empathy_2024,
author = {Chen, Chaoran and Li, Weijun and Song, Wenxin and Ye, Yanfang and Yao, Yaxing and Li, Toby Jia-Jun},
title = {An Empathy-Based Sandbox Approach to Bridge the Privacy Gap among Attitudes, Goals, Knowledge, and Behaviors},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642363},
doi = {10.1145/3613904.3642363},
abstract = {Managing privacy to reach privacy goals is challenging, as evidenced by the privacy attitude-behavior gap. Mitigating this discrepancy requires solutions that account for both system opaqueness and users’ hesitations in testing different privacy settings due to fears of unintended data exposure. We introduce an empathy-based approach that allows users to experience how privacy attributes may alter system outcomes in a risk-free sandbox environment from the perspective of artificially generated personas. To generate realistic personas, we introduce a novel pipeline that augments the outputs of large language models (e.g., GPT-4) using few-shot learning, contextualization, and chain of thoughts. Our empirical studies demonstrated the adequate quality of generated personas and highlighted the changes in privacy-related applications (e.g., online advertising) caused by different personas. Furthermore, users demonstrated cognitive and emotional empathy towards the personas when interacting with our sandbox. We offered design implications for downstream applications in improving user privacy literacy.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {234},
numpages = {28},
keywords = {empathy, generated personas, privacy awareness, privacy intervention, privacy literacy, sandbox},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{nario-redmond_social_2004,
	title = {The {Social} and {Personal} {Identities} {Scale}: {A} {Measure} of the {Differential} {Importance} {Ascribed} to {Social} and {Personal} {Self}-{Categorizations}},
	volume = {3},
	issn = {1529-8868, 1529-8876},
	shorttitle = {The {Social} and {Personal} {Identities} {Scale}},
	url = {https://www.tandfonline.com/doi/full/10.1080/13576500342000103},
	doi = {10.1080/13576500342000103},
	language = {en},
	number = {2},
	urldate = {2024-06-15},
	journal = {Self and Identity},
	author = {Nario-Redmond, Michelle R and Biernat, Monica and Eidelman, Scott and Palenske, Debra J},
	month = apr,
	year = {2004},
	pages = {143--175},
	file = {Nario-Redmond et al. - 2004 - The Social and Personal Identities Scale A Measur.pdf:/Users/Maria/Zotero/storage/GBB8QNJZ/Nario-Redmond et al. - 2004 - The Social and Personal Identities Scale A Measur.pdf:application/pdf},
}

@article{chen_meta-analysis_2024,
	title = {A {Meta}-{Analysis} {Examining} the {Role} of {Character}-{Recipient} {Similarity} in {Narrative} {Persuasion}},
	volume = {51},
	issn = {0093-6502, 1552-3810},
	url = {http://journals.sagepub.com/doi/10.1177/00936502231204834},
	doi = {10.1177/00936502231204834},
	abstract = {This meta-analysis synthesized 19 empirical articles reporting 123 effect sizes of character-recipient similarity on narrative processing and persuasion outcomes across different contexts, including health, environmental, and social issues. We also aimed to investigate whether the effect magnitude varies depending on how the similarity is operationalized, which perspective is adopted, and what context the narrative persuasion is placed in. The results indicated that, compared to a dissimilar counterpart, a similar character leads to stronger identification (k = 34, d = 0.14, p {\textless} .01) and self-referencing (k = 12, d = 0.16, p {\textless} .01). The effects on transportation (k = 22, d = 0.13, p = .05) and resistance (k = 12, d = −0.16, p = .05) were marginally significant. It was also found that the similarity manipulated on chosen demographic and biographic variables like occupation and living place yields the strongest impact among other variables (i.e., innate demographic and biographic variables like age and sex, psychological and behavioral variables like beliefs and behaviors). Furthermore, the similarity effect in narrative persuasion becomes intensified when combined with a first-person perspective and placed in a social issue context. By presenting a synthesis of the existing research, this meta-analytical study sought to identify areas in need of further refinement and outline future investigation directions for narrative persuasion.},
	language = {en},
	number = {1},
	urldate = {2024-06-15},
	journal = {Communication Research},
	author = {Chen, Meng and Dong, Yujie and Wang, Jilong},
	month = feb,
	year = {2024},
	pages = {56--82},
	file = {Chen et al. - 2024 - A Meta-Analysis Examining the Role of Character-Re.pdf:/Users/Maria/Zotero/storage/W488GXBB/Chen et al. - 2024 - A Meta-Analysis Examining the Role of Character-Re.pdf:application/pdf},
}

@misc{xiao_how_2023,
	title = {How {Far} {Are} {We} from {Believable} {AI} {Agents}? {A} {Framework} for {Evaluating} the {Believability} of {Human} {Behavior} {Simulation}},
	shorttitle = {How {Far} {Are} {We} from {Believable} {AI} {Agents}?},
	url = {http://arxiv.org/abs/2312.17115},
	abstract = {Human behavior simulation of AI agents necessitates the agents to possess a quality of believability, which is crucial as it facilitates users in establishing trust toward the agents and streamlines the fulfillment of the agents' goal. While recent advancements in Large Language Model (LLM) based agents have improved human behavior simulation, challenges inherent to LLMs (e.g., long context modeling) can undermine their believability. Consequently, evaluating AI agent believability becomes imperative. Unfortunately, prior research often neglects the negative impacts of LLM deficiencies. To address these gaps, we introduce two metrics for assessing LLM-based agent believability: consistency, and robustness, together with a benchmark, SimulateBench, with which, we evaluate the consistency and robustness of agents implemented with popular LLMs. We find that agents (i) struggle to accurately depict character information when presented with lengthy profile inputs; (ii) exhibit vulnerability to profile perturbations; and (iii) are significantly affected by certain key factors that impact their overall believability. Code and SimulateBench are public at https://github.com/GAIR-NLP/GPTMan.},
	language = {en},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Xiao, Yang and Cheng, Yi and Fu, Jinlan and Wang, Jiashuo and Li, Wenjie and Liu, Pengfei},
	month = dec,
	year = {2023},
	note = {arXiv:2312.17115 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {Xiao et al. - 2023 - How Far Are We from Believable AI Agents A Framew.pdf:/Users/Maria/Zotero/storage/YEVNJ8YQ/Xiao et al. - 2023 - How Far Are We from Believable AI Agents A Framew.pdf:application/pdf},
}

@inproceedings{cheng_compost_2023,
	address = {Singapore},
	title = {{CoMPosT}: {Characterizing} and {Evaluating} {Caricature} in {LLM} {Simulations}},
	shorttitle = {{CoMPosT}},
	url = {https://aclanthology.org/2023.emnlp-main.669},
	doi = {10.18653/v1/2023.emnlp-main.669},
	abstract = {Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations’ susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.},
	language = {en},
	urldate = {2024-06-15},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Cheng, Myra and Piccardi, Tiziano and Yang, Diyi},
	year = {2023},
	pages = {10853--10875},
	file = {Cheng et al. - 2023 - CoMPosT Characterizing and Evaluating Caricature .pdf:/Users/Maria/Zotero/storage/KAPVZBJ9/Cheng et al. - 2023 - CoMPosT Characterizing and Evaluating Caricature .pdf:application/pdf},
}


@misc{wang_user_2024,
	title = {User {Behavior} {Simulation} with {Large} {Language} {Model} based {Agents}},
	url = {http://arxiv.org/abs/2306.02552},
	abstract = {Simulating high quality user behavior data has always been a fundamental problem in humancentered applications, where the major difficulty originates from the intricate mechanism of human decision process. Recently, substantial evidences have suggested that by learning huge amounts of web knowledge, large language models (LLMs) can achieve human-like intelligence. We believe these models can provide significant opportunities to more believable user behavior simulation. To inspire such direction, we propose an LLM-based agent framework and design a sandbox environment to simulate real user behaviors. Based on extensive experiments, we find that the simulated behaviors of our method are very close to the ones of real humans. Concerning potential applications, we simulate and study two social phenomenons including (1) information cocoons and (2) user conformity behaviors. This research provides novel simulation paradigms for human-centered applications.},
	language = {en},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Wang, Lei and Zhang, Jingsen and Yang, Hao and Chen, Zhiyuan and Tang, Jiakai and Zhang, Zeyu and Chen, Xu and Lin, Yankai and Song, Ruihua and Zhao, Wayne Xin and Xu, Jun and Dou, Zhicheng and Wang, Jun and Wen, Ji-Rong},
	month = feb,
	year = {2024},
	note = {arXiv:2306.02552 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {Wang et al. - 2024 - User Behavior Simulation with Large Language Model.pdf:/Users/Maria/Zotero/storage/XSAH935Y/Wang et al. - 2024 - User Behavior Simulation with Large Language Model.pdf:application/pdf},
}

@inproceedings{adams_sparse_2023,
	address = {Hybrid},
	title = {From {Sparse} to {Dense}: {GPT}-4 {Summarization} with {Chain} of {Density} {Prompting}},
	shorttitle = {From {Sparse} to {Dense}},
	url = {https://aclanthology.org/2023.newsum-1.7},
	doi = {10.18653/v1/2023.newsum-1.7},
	abstract = {Selecting the “right” amount of information to include in a summary is a difficult task. A good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entitysparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries generated by CoD are more abstractive, exhibit more fusion, and have less of a lead bias than GPT-4 summaries generated by a vanilla prompt. We conduct a human preference study on 100 CNN DailyMail articles and find that humans prefer GPT-4 summaries that are more dense than those generated by a vanilla prompt and almost as dense as human written summaries. Qualitative analysis supports the notion that there exists a tradeoff between informativeness and readability. 500 annotated CoD summaries, as well as an extra 5,000 unannotated summaries, are freely available on HuggingFace1.},
	language = {en},
	urldate = {2024-06-15},
	booktitle = {Proceedings of the 4th {New} {Frontiers} in {Summarization} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Adams, Griffin and Fabbri, Alex and Ladhak, Faisal and Lehman, Eric and Elhadad, Noémie},
	year = {2023},
	pages = {68--74},
	file = {Adams et al. - 2023 - From Sparse to Dense GPT-4 Summarization with Cha.pdf:/Users/Maria/Zotero/storage/FBHRUKA5/Adams et al. - 2023 - From Sparse to Dense GPT-4 Summarization with Cha.pdf:application/pdf},
}



@misc{song_identifying_2024,
	title = {Identifying {Multiple} {Personalities} in {Large} {Language} {Models} with {External} {Evaluation}},
	url = {http://arxiv.org/abs/2402.14805},
	abstract = {As Large Language Models (LLMs) are integrated with human daily applications rapidly, many societal and ethical concerns are raised regarding the behavior of LLMs. One of the ways to comprehend LLMs’ behavior is to analyze their personalities. Many recent studies quantify LLMs’ personalities using self-assessment tests that are created for humans. Yet many critiques question the applicability and reliability of these self-assessment tests when applied to LLMs. In this paper, we investigate LLM personalities using an alternate personality measurement method, which we refer to as the external evaluation method, where instead of prompting LLMs with multiple-choice questions in the Likert scale, we evaluate LLMs’ personalities by analyzing their responses toward open-ended situational questions using an external machine learning model. We first fine-tuned a Llama2-7B model as the MBTI personality predictor that outperforms the stateof-the-art models as the tool to analyze LLMs’ responses. Then, we prompt the LLMs with situational questions and ask them to generate Twitter posts and comments, respectively, in order to assess their personalities when playing two different roles. Using the external personality evaluation method, we identify that the obtained personality types for LLMs are significantly different when generating posts versus comments, whereas humans show a consistent personality profile in these two different situations. This shows that LLMs can exhibit different personalities based on different scenarios, thus highlighting a fundamental difference between personality in LLMs and humans. With our work, we call for a re-evaluation of personality definition and measurement in LLMs.},
	language = {en},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Song, Xiaoyang and Adachi, Yuta and Feng, Jessie and Lin, Mouwei and Yu, Linhao and Li, Frank and Gupta, Akshat and Anumanchipalli, Gopala and Kaur, Simerjot},
	month = feb,
	year = {2024},
	note = {arXiv:2402.14805 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Song et al. - 2024 - Identifying Multiple Personalities in Large Langua.pdf:/Users/Maria/Zotero/storage/QFA6YULL/Song et al. - 2024 - Identifying Multiple Personalities in Large Langua.pdf:application/pdf},
}

@misc{jiang_evaluating_2023,
	title = {Evaluating and {Inducing} {Personality} in {Pre}-trained {Language} {Models}},
	url = {http://arxiv.org/abs/2206.07550},
	abstract = {Standardized and quantified evaluation of machine behaviors is a crux of understanding LLMs. In this study, we draw inspiration from psychometric studies by leveraging human personality theory as a tool for studying machine behaviors. Originating as a philosophical quest for human behaviors, the study of personality delves into how individuals differ in thinking, feeling, and behaving. Toward building and understanding human-like social machines, we are motivated to ask: Can we assess machine behaviors by leveraging human psychometric tests in a principled and quantitative manner? If so, can we induce a specific personality in LLMs? To answer these questions, we introduce the Machine Personality Inventory (MPI) tool for studying machine behaviors; MPI follows standardized personality tests, built upon the Big Five Personality Factors (Big Five) theory and personality assessment inventories. By systematically evaluating LLMs with MPI, we provide the first piece of evidence demonstrating the efficacy of MPI in studying LLMs behaviors. We further devise a PERSONALITY PROMPTING (P2) method to induce LLMs with specific personalities in a controllable way, capable of producing diverse and verifiable behaviors. We hope this work sheds light on future studies by adopting personality as the essential indicator for various downstream tasks, and could further motivate research into equally intriguing human-like machine behaviors.},
	language = {en},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Jiang, Guangyuan and Xu, Manjie and Zhu, Song-Chun and Han, Wenjuan and Zhang, Chi and Zhu, Yixin},
	month = oct,
	year = {2023},
	note = {arXiv:2206.07550 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Jiang et al. - 2023 - Evaluating and Inducing Personality in Pre-trained.pdf:/Users/Maria/Zotero/storage/UZRWWHGZ/Jiang et al. - 2023 - Evaluating and Inducing Personality in Pre-trained.pdf:application/pdf},
}

@misc{ha_clochat_2024,
	title = {{CloChat}: {Understanding} {How} {People} {Customize}, {Interact}, and {Experience} {Personas} in {Large} {Language} {Models}},
	shorttitle = {{CloChat}},
	url = {http://arxiv.org/abs/2402.15265},
	abstract = {Large language models (LLMs) have facilitated significant strides in generating conversational agents, enabling seamless, contextually relevant dialogues across diverse topics. However, the existing LLM-driven conversational agents have fixed personalities and functionalities, limiting their adaptability to individual user needs. Creating personalized agent personas with distinct expertise or traits can address this issue. Nonetheless, we lack knowledge of how people customize and interact with agent personas. In this research, we investigated how users customize agent personas and their impact on interaction quality, diversity, and dynamics. To this end, we developed CloChat, an interface supporting easy and accurate customization of agent personas in LLMs. We conducted a study comparing how participants interact with CloChat and ChatGPT. The results indicate that participants formed emotional bonds with the customized agents, engaged in more dynamic dialogues, and showed interest in sustaining interactions. These findings contribute to design implications for future systems with conversational agents using LLMs.},
	language = {en},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Ha, Juhye and Jeon, Hyeon and Han, DaEun and Seo, Jinwook and Oh, Changhoon},
	month = feb,
	year = {2024},
	note = {arXiv:2402.15265 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file = {Ha et al. - 2024 - CloChat Understanding How People Customize, Inter.pdf:/Users/Maria/Zotero/storage/E4D24JCB/Ha et al. - 2024 - CloChat Understanding How People Customize, Inter.pdf:application/pdf},
}

@misc{shao_character-llm_2023,
	title = {Character-{LLM}: {A} {Trainable} {Agent} for {Role}-{Playing}},
	shorttitle = {Character-{LLM}},
	url = {http://arxiv.org/abs/2310.10158},
	abstract = {Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents {\textbackslash}textit\{memorize\} their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.},
	language = {en},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Shao, Yunfan and Li, Linyang and Dai, Junqi and Qiu, Xipeng},
	month = dec,
	year = {2023},
	note = {arXiv:2310.10158 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Shao et al. - 2023 - Character-LLM A Trainable Agent for Role-Playing.pdf:/Users/Maria/Zotero/storage/24QAPHF5/Shao et al. - 2023 - Character-LLM A Trainable Agent for Role-Playing.pdf:application/pdf},
}

@inproceedings{park_social_2022,
	address = {Bend OR USA},
	title = {Social {Simulacra}: {Creating} {Populated} {Prototypes} for {Social} {Computing} {Systems}},
	isbn = {978-1-4503-9320-1},
	shorttitle = {Social {Simulacra}},
	url = {https://dl.acm.org/doi/10.1145/3526113.3545616},
	doi = {10.1145/3526113.3545616},
	language = {en},
	urldate = {2024-06-15},
	booktitle = {Proceedings of the 35th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Park, Joon Sung and Popowski, Lindsay and Cai, Carrie and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
	month = oct,
	year = {2022},
	pages = {1--18},
	file = {Full Text:/Users/Maria/Zotero/storage/3RQ4YWSM/Park et al. - 2022 - Social Simulacra Creating Populated Prototypes fo.pdf:application/pdf},
}

@inproceedings{park_generative_2023,
	address = {San Francisco CA USA},
	title = {Generative {Agents}: {Interactive} {Simulacra} of {Human} {Behavior}},
	isbn = {9798400701320},
	shorttitle = {Generative {Agents}},
	url = {https://dl.acm.org/doi/10.1145/3586183.3606763},
	doi = {10.1145/3586183.3606763},
	language = {en},
	urldate = {2024-06-15},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
	month = oct,
	year = {2023},
	pages = {1--22},
	file = {Full Text:/Users/Maria/Zotero/storage/UDKGBAP6/Park et al. - 2023 - Generative Agents Interactive Simulacra of Human .pdf:application/pdf},
}

@article{boyd_values_2021,
	title = {Values in {Words}: {Using} {Language} to {Evaluate} and {Understand} {Personal} {Values}},
	volume = {9},
	issn = {2334-0770, 2162-3449},
	shorttitle = {Values in {Words}},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/14589},
	doi = {10.1609/icwsm.v9i1.14589},
	abstract = {People’s values provide a decision-making framework that helps guide their everyday actions. Most popular methods of assessing values show tenuous relationships with everyday behaviors. Using a new Amazon Mechanical Turk dataset (N = 767) consisting of people’s language, values, and behaviors, we explore the degree to which attaining “ground truth” is possible with regards to such complicated mental phenomena. We then apply our ﬁndings to a corpus of Facebook user (N = 130, 828) status updates in order to understand how core values inﬂuence the personal thoughts and behaviors that users share through social media. Our ﬁndings suggest that self-report questionnaires for abstract and complex phenomena, such as values, are inadequate for painting an accurate picture of individual mental life. Free response language data and language modeling show greater promise for understanding both the structure and content of concepts such as values and, additionally, exhibit a predictive edge over self-report questionnaires.},
	language = {en},
	number = {1},
	urldate = {2024-06-15},
	journal = {Proceedings of the International AAAI Conference on Web and Social Media},
	author = {Boyd, Ryan and Wilson, Steven and Pennebaker, James and Kosinski, Michal and Stillwell, David and Mihalcea, Rada},
	month = aug,
	year = {2021},
	pages = {31--40},
	file = {Boyd et al. - 2021 - Values in Words Using Language to Evaluate and Un.pdf:/Users/Maria/Zotero/storage/QDLZV9MF/Boyd et al. - 2021 - Values in Words Using Language to Evaluate and Un.pdf:application/pdf},
}

@misc{xie_human_2024,
	title = {Human {Simulacra}: {Benchmarking} the {Personification} of {Large} {Language} {Models}},
	shorttitle = {Human {Simulacra}},
	url = {http://arxiv.org/abs/2402.18180},
	abstract = {Large language models (LLMs) are recognized as systems that closely mimic aspects of human intelligence. This capability has attracted attention from the social science community, who see the potential in leveraging LLMs to replace human participants in experiments, thereby reducing research costs and complexity. In this paper, we introduce a framework for large language models personification, including a strategy for constructing virtual characters’ life stories from the ground up, a Multi-Agent Cognitive Mechanism capable of simulating human cognitive processes, and a psychology-guided evaluation method to assess human simulations from both self and observational perspectives. Experimental results demonstrate that our constructed simulacra can produce personified responses that align with their target characters. Our work is a preliminary exploration which offers great potential in practical applications. All the code and datasets will be released, with the hope of inspiring further investigations.},
	language = {en},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Xie, Qiuejie and Feng, Qiming and Zhang, Tianqi and Li, Qingqiu and Yang, Linyi and Zhang, Yuejie and Feng, Rui and He, Liang and Gao, Shang and Zhang, Yue},
	month = jun,
	year = {2024},
	note = {arXiv:2402.18180 [cs]},
	keywords = {Computer Science - Computers and Society},
	file = {Xie et al. - 2024 - Human Simulacra Benchmarking the Personification .pdf:/Users/Maria/Zotero/storage/Q9IIST8G/Xie et al. - 2024 - Human Simulacra Benchmarking the Personification .pdf:application/pdf},
}

@inproceedings{yang_psycot_2023,
	address = {Singapore},
	title = {{PsyCoT}: {Psychological} {Questionnaire} as {Powerful} {Chain}-of-{Thought} for {Personality} {Detection}},
	shorttitle = {{PsyCoT}},
	url = {https://aclanthology.org/2023.findings-emnlp.216},
	doi = {10.18653/v1/2023.findings-emnlp.216},
	abstract = {Recent advances in large language models (LLMs), such as ChatGPT, have showcased remarkable zero-shot performance across various NLP tasks. However, the potential of LLMs in personality detection, which involves identifying an individual’s personality from their written texts, remains largely unexplored. Drawing inspiration from Psychological Questionnaires, which are carefully designed by psychologists to evaluate individual personality traits through a series of targeted items, we argue that these items can be regarded as a collection of wellstructured chain-of-thought (CoT) processes. By incorporating these processes, LLMs can enhance their capabilities to make more reasonable inferences on personality from textual input. In light of this, we propose a novel personality detection method, called PsyCoT, which mimics the way individuals complete psychological questionnaires in a multi-turn dialogue manner. In particular, we employ a LLM as an AI assistant with a specialization in text analysis. We prompt the assistant to rate individual items at each turn and leverage the historical rating results to derive a conclusive personality preference. Our experiments demonstrate that PsyCoT significantly improves the performance and robustness of GPT-3.5 in personality detection, achieving an average F1 score improvement of 4.23/10.63 points on two benchmark datasets compared to the standard prompting method. Our code is available at https://github.com/TaoYang225/PsyCoT.},
	language = {en},
	urldate = {2024-06-15},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Yang, Tao and Shi, Tianyuan and Wan, Fanqi and Quan, Xiaojun and Wang, Qifan and Wu, Bingzhe and Wu, Jiaxiang},
	year = {2023},
	pages = {3305--3320},
	file = {Yang et al. - 2023 - PsyCoT Psychological Questionnaire as Powerful Ch.pdf:/Users/Maria/Zotero/storage/5CGGJ32D/Yang et al. - 2023 - PsyCoT Psychological Questionnaire as Powerful Ch.pdf:application/pdf},
}

@misc{wang_user_2023,
	title = {User {Behavior} {Simulation} with {Large} {Language} {Model} based {Agents}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2306.02552},
	doi = {10.48550/ARXIV.2306.02552},
	abstract = {Simulating high quality user behavior data has always been a fundamental problem in human-centered applications, where the major difficulty originates from the intricate mechanism of human decision process. Recently, substantial evidences have suggested that by learning huge amounts of web knowledge, large language models (LLMs) can achieve human-like intelligence. We believe these models can provide significant opportunities to more believable user behavior simulation. To inspire such direction, we propose an LLM-based agent framework and design a sandbox environment to simulate real user behaviors. Based on extensive experiments, we find that the simulated behaviors of our method are very close to the ones of real humans. Concerning potential applications, we simulate and study two social phenomenons including (1) information cocoons and (2) user conformity behaviors. This research provides novel simulation paradigms for human-centered applications.},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Wang, Lei and Zhang, Jingsen and Yang, Hao and Chen, Zhiyuan and Tang, Jiakai and Zhang, Zeyu and Chen, Xu and Lin, Yankai and Song, Ruihua and Zhao, Wayne Xin and Xu, Jun and Dou, Zhicheng and Wang, Jun and Wen, Ji-Rong},
	year = {2023},
	note = {Version Number: 3},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Information Retrieval (cs.IR)},
}

@inproceedings{aher_using_2023,
author = {Aher, Gati and Arriaga, Rosa I. and Kalai, Adam Tauman},
title = {Using large language models to simulate multiple humans and replicate human subject studies},
year = {2023},
publisher = {JMLR.org},
abstract = {We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a "hyper-accuracy distortion" present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {17},
numpages = {35},
location = {<conf-loc>, <city>Honolulu</city>, <state>Hawaii</state>, <country>USA</country>, </conf-loc>},
series = {ICML'23}
}

@misc{horton_large_2023,
	title = {Large {Language} {Models} as {Simulated} {Economic} {Agents}: {What} {Can} {We} {Learn} from {Homo} {Silicus}?},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Large {Language} {Models} as {Simulated} {Economic} {Agents}},
	url = {https://arxiv.org/abs/2301.07543},
	doi = {10.48550/ARXIV.2301.07543},
	abstract = {Newly-developed large language models (LLM) -- because of how they are trained and designed -- are implicit computational models of humans -- a homo silicus. These models can be used the same way economists use homo economicus: they can be given endowments, information, preferences, and so on and then their behavior can be explored in scenarios via simulation. I demonstrate this approach using OpenAI's GPT3 with experiments derived from Charness and Rabin (2002), Kahneman, Knetsch and Thaler (1986) and Samuelson and Zeckhauser (1988). The findings are qualitatively similar to the original results, but it is also trivially easy to try variations that offer fresh insights. Departing from the traditional laboratory paradigm, I also create a hiring scenario where an employer faces applicants that differ in experience and wage ask and then analyze how a minimum wage affects realized wages and the extent of labor-labor substitution.},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Horton, John J.},
	year = {2023},
	note = {Version Number: 1},
	keywords = {FOS: Economics and business, General Economics (econ.GN)},
}

@inproceedings{hamalainen_evaluating_2023,
	address = {Hamburg Germany},
	title = {Evaluating {Large} {Language} {Models} in {Generating} {Synthetic} {HCI} {Research} {Data}: a {Case} {Study}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {Evaluating {Large} {Language} {Models} in {Generating} {Synthetic} {HCI} {Research} {Data}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3580688},
	doi = {10.1145/3544548.3580688},
	language = {en},
	urldate = {2024-06-15},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Hämäläinen, Perttu and Tavast, Mikke and Kunnari, Anton},
	month = apr,
	year = {2023},
	pages = {1--19},
	file = {Full Text:/Users/Maria/Zotero/storage/KT8ZL83P/Hämäläinen et al. - 2023 - Evaluating Large Language Models in Generating Syn.pdf:application/pdf},
}

@misc{gao_s3_2023,
	title = {S3: {Social}-network {Simulation} {System} with {Large} {Language} {Model}-{Empowered} {Agents}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {S3},
	url = {https://arxiv.org/abs/2307.14984},
	doi = {10.48550/ARXIV.2307.14984},
	abstract = {Social network simulation plays a crucial role in addressing various challenges within social science. It offers extensive applications such as state prediction, phenomena explanation, and policy-making support, among others. In this work, we harness the formidable human-like capabilities exhibited by large language models (LLMs) in sensing, reasoning, and behaving, and utilize these qualities to construct the S\${\textasciicircum}3\$ system (short for \${\textbackslash}textbf\{S\}\$ocial network \${\textbackslash}textbf\{S\}\$imulation \${\textbackslash}textbf\{S\}\$ystem). Adhering to the widely employed agent-based simulation paradigm, we employ prompt engineering and prompt tuning techniques to ensure that the agent's behavior closely emulates that of a genuine human within the social network. Specifically, we simulate three pivotal aspects: emotion, attitude, and interaction behaviors. By endowing the agent in the system with the ability to perceive the informational environment and emulate human actions, we observe the emergence of population-level phenomena, including the propagation of information, attitudes, and emotions. We conduct an evaluation encompassing two levels of simulation, employing real-world social network data. Encouragingly, the results demonstrate promising accuracy. This work represents an initial step in the realm of social network simulation empowered by LLM-based agents. We anticipate that our endeavors will serve as a source of inspiration for the development of simulation systems within, but not limited to, social science.},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Gao, Chen and Lan, Xiaochong and Lu, Zhihong and Mao, Jinzhu and Piao, Jinghua and Wang, Huandong and Jin, Depeng and Li, Yong},
	year = {2023},
	note = {Version Number: 2},
	keywords = {FOS: Computer and information sciences, Social and Information Networks (cs.SI)},
}

@inproceedings{hidalgo2019mapping,
  title={Mapping uncertainty around research data: a Digital Humanities transdisciplinary perspective adopting the Johari window},
  author={Hidalgo, Enric Senabre and Wandl-Vogt, Eveline and Dorn, Amelie and Souza, Renato Rocha},
  booktitle={Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
  pages={804--809},
  year={2019}
}

@incollection{kuhn2017empirical,
  title={An empirical investigation of self-attitudes},
  author={Kuhn, Manford H and McPartland, Thomas S},
  booktitle={Sociological Methods},
  pages={167--182},
  year={2017},
  publisher={Routledge}
}


@article{Jones2000ACM,
  title={A Conceptual Model of Multiple Dimensions of Identity},
  author={Susan Jones and Marylu K. McEwen},
  journal={Journal of College Student Development},
  year={2000},
  volume={41},
  url={https://api.semanticscholar.org/CorpusID:37394550}
}


@article{allport1937personality,
  title={Personality: A psychological interpretation.},
  author={Allport, Gordon Willard},
  year={1937},
  publisher={Holt}
}

@article{schwartz1994there,
  title={Are there universal aspects in the structure and contents of human values?},
  author={Schwartz, Shalom H},
  journal={Journal of social issues},
  volume={50},
  number={4},
  pages={19--45},
  year={1994},
  publisher={Wiley Online Library}
}

@article{schwartz1987toward,
  title={Toward a universal psychological structure of human values.},
  author={Schwartz, Shalom H and Bilsky, Wolfgang},
  journal={Journal of personality and social psychology},
  volume={53},
  number={3},
  pages={550},
  year={1987},
  publisher={American Psychological Association}
}

@article{frederickx2014role,
  title={The role of personality in the initiation of communication situations},
  author={Frederickx, Sofie and Hofmans, Joeri},
  journal={Journal of Individual Differences},
  year={2014},
  publisher={Hogrefe Publishing}
}

@article{corr&matthews2009,
author = {Corr, Philip and Matthews, Gerald},
year = {2009},
month = {01},
pages = {1-906},
title = {The Cambridge Handbook of Personality Psychology},
doi = {10.1017/CBO9780511596544.002}
}

@book{weinberg_foundations_2019,
	title = {Foundations of {Sport} and {Exercise} {Psychology}, {7E}},
	isbn = {978-1-4925-6114-9},
	url = {https://books.google.co.kr/books?id=ACBwDwAAQBAJ},
	publisher = {Human Kinetics},
	author = {Weinberg, R.S. and Gould, D.},
	year = {2019},
	lccn = {2018012860},
}

@article{schwartzpvq2009basic,
  title={Basic human values},
  author={Schwartz, Shalom H},
  journal={sociologie},
  volume={42},
  pages={249--288},
  year={2009}
}


@article{bfi_short_2017,
	title = {Short and extra-short forms of the {Big} {Five} {Inventory}–2: {The} {BFI}-2-{S} and {BFI}-2-{XS}},
	volume = {68},
	issn = {00926566},
	shorttitle = {Short and extra-short forms of the {Big} {Five} {Inventory}–2},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0092656616301325},
	doi = {10.1016/j.jrp.2017.02.004},
	language = {en},
	urldate = {2024-06-15},
	journal = {Journal of Research in Personality},
	author = {Soto, Christopher J. and John, Oliver P.},
	month = jun,
	year = {2017},
	pages = {69--81},
}

@misc{sandy_21-item_2018,
	title = {21-{Item} {Portrait} {Values} {Questionnaire}--{Modified} {Version}},
	url = {https://doi.apa.org/doi/10.1037/t64425-000},
	doi = {10.1037/t64425-000},
	language = {en},
	urldate = {2024-06-15},
	author = {Sandy, Carson J. and Gosling, Samuel D. and Schwartz, Shalom H. and Koelkebeck, Tim},
	month = jan,
	year = {2018},
	note = {Institution: American Psychological Association},
}

@article{fearon1999identity,
  title={What is identity (as we now use the word)},
  author={Fearon, James D},
  journal={Unpublished manuscript, Stanford University, Stanford, Calif},
  pages={1--43},
  year={1999}
}

@inproceedings{demollm2024,
author = {Lee, Messi H.J. and Montgomery, Jacob M. and Lai, Calvin K.},
title = {Large Language Models Portray Socially Subordinate Groups as More Homogeneous, Consistent with a Bias Observed in Humans},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3658975},
doi = {10.1145/3630106.3658975},
abstract = {Large language models (LLMs) are becoming pervasive in everyday life, yet their propensity to reproduce biases inherited from training data remains a pressing concern. Prior investigations into bias in LLMs have focused on the association of social groups with stereotypical attributes. However, this is only one form of human bias such systems may reproduce. We investigate a new form of bias in LLMs that resembles a social psychological phenomenon where socially subordinate groups are perceived as more homogeneous than socially dominant groups. We had ChatGPT, a state-of-the-art LLM, generate texts about intersectional group identities and compared those texts on measures of homogeneity. We consistently found that ChatGPT portrayed African, Asian, and Hispanic Americans as more homogeneous than White Americans, indicating that the model described racial minority groups with a narrower range of human experience. ChatGPT also portrayed women as more homogeneous than men, but these differences were small. Finally, we found that the effect of gender differed across racial/ethnic groups such that the effect of gender was consistent within African and Hispanic Americans but not within Asian and White Americans. We argue that the tendency of LLMs to describe groups as less diverse risks perpetuating stereotypes and discriminatory behavior.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1321–1340},
numpages = {20},
keywords = {AI Bias, Homogeneity Bias, Large Language Models, Perceived Variability, Stereotyping},
location = {<conf-loc>, <city>Rio de Janeiro</city>, <country>Brazil</country>, </conf-loc>},
series = {FAccT '24}
}



@misc{gupta_bias_2023,
	title = {Bias {Runs} {Deep}: {Implicit} {Reasoning} {Biases} in {Persona}-{Assigned} {LLMs}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Bias {Runs} {Deep}},
	url = {https://arxiv.org/abs/2311.04892},
	doi = {10.48550/ARXIV.2311.04892},
	abstract = {Recent works have showcased the ability of LLMs to embody diverse personas in their responses, exemplified by prompts like 'You are Yoda. Explain the Theory of Relativity.' While this ability allows personalization of LLMs and enables human behavior simulation, its effect on LLMs' capabilities remains unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs to perform basic reasoning tasks. Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse personas (e.g. an Asian person) spanning 5 socio-demographic groups. Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness. While they overtly reject stereotypes when explicitly asked ('Are Black people less skilled at mathematics?'), they manifest stereotypical and erroneous presumptions when asked to answer questions while adopting a persona. These can be observed as abstentions in responses, e.g., 'As a Black person, I can't answer this question as it requires math knowledge', and generally result in a substantial performance drop. Our experiments with ChatGPT-3.5 show that this bias is ubiquitous - 80\% of our personas demonstrate bias; it is significant - some datasets show performance drops of 70\%+; and can be especially harmful for certain groups - some personas suffer statistically significant drops on 80\%+ of the datasets. Overall, all 4 LLMs exhibit this bias to varying extents, with GPT-4-Turbo showing the least but still a problematic amount of bias (evident in 42\% of the personas). Further analysis shows that these persona-induced errors can be hard-to-discern and hard-to-avoid. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs - a trend on the rise - can surface their deep-rooted biases and have unforeseeable and detrimental side-effects.},
	urldate = {2024-06-16},
	publisher = {arXiv},
	author = {Gupta, Shashank and Shrivastava, Vaishnavi and Deshpande, Ameet and Kalyan, Ashwin and Clark, Peter and Sabharwal, Ashish and Khot, Tushar},
	year = {2023},
	note = {Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}
