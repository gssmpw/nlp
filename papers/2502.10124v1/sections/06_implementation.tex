\section{Implementation}
\label{section:implementation}

In this section, we investigated the best combination of gaze behavior features to predict the noticeability of redirection.
We described participants' gaze behaviors with pupil activity, gaze angular distance to users' hands, eye saccade, and fixation.
For each category of participants' gaze behavioral data, we systematically examine different feature combinations to identify those that most accurately characterize the participants' visual responses.
We then develop a regression model to explain the relationship between the selected gaze behavioral features and the noticeability of redirection.

% \delete{
% In this section, we describe our implementation of our prediction model with the collected data from Section~\ref{section:datacollection}, including the participants' pupil activity, gaze movement, eye saccade, and fixation.
% In each category of participants' gaze behavioral data, we enumerate the combination of features to select the features that describe the gaze behavior best.
% Then we combine the four categories and explore the best prediction performance with different prediction models.
% Meanwhile, we leveraged the collected noticeability under each visual attention condition (i.e., the proportion of correct responses in each condition) as the ground truth.
% In this way, we aimed to build a model that takes the user's gaze behavioral patterns as input and predicts the noticeability of the motion offset under different visual attention conditions.
% }

\subsection{Gaze behavioral patterns}
\label{section:featureselection}

Results in \autoref{section:formativestudy} showed that the users' gaze behavioral data was correlated to the noticeability.
We divided the gaze behavioral data into four categories:

\begin{itemize}
    \item \textit{Index of Pupillary Activity}: 
    Index of Pupillary Activity (IPA) has been used to reflect users' cognitive load by analyzing the change of users' pupil dilation~\cite{duchowski2018index, lindlbauer2019context}. 
    While visual stimuli were presented, users' cognitive load might be inadvertently affected and could therefore impact the noticeability results.
    \item \textit{Gaze angular distance to elbow and hand}:  
    We calculated the vector starting from the user's eye to the elbow and hand joint. 
    We then calculated the angular distance between this vector and the gaze vector.
    These metrics reflect whether the participant was looking at the primary task or attracted by the visual stimuli.
    We decided not to calculate the distance between the focus point and the visual stimuli, as in real-world use cases, there is no single visual stimuli but only complicated ones, which make it hard to compute this distance.
    \item \textit{Eye saccade frequency, duration and interval}:
    Eye saccade is a rapid eye movement that shifts the eye from one area to another.
    We leveraged the detection algorithm from \citeauthor{pymovements} to detect the saccade frequency and duration~\cite{pymovements}. 
    The saccade frequency and duration indicate how often and how quickly users shift their eye gaze separately.
    The saccade interval suggests the temporal distribution, which indicates whether the saccades are uniformly distributed across the session.
    \item \textit{Eye fixation frequency, duration and interval}:
    Eye fixations represent when eyes stop scanning the scene and hold the foveal vision on an object of interest. 
    We also used the frequency, duration, and interval of eye fixation to indicate how often and how long users stared at a place and the temporal distribution of fixation.
\end{itemize}

\subsection{Regression model}

To better represent the previous gaze behavioral patterns, we computed \textit{mean, standard deviation, median, maximum and minimum} of IPA and gaze angular distance and combined them with the eye saccade and fixation features.
Therefore, we had 3 behaviors(IPA, gaze angular distance to hand, gaze angular distance to elbow) $\times$ 5 features (mean, standard deviation, median, maximum and minimum) $+$ 3 saccade (saccade frequency, duration and interval) $+$ 3 fixation (fixation frequency, duration and interval) $=~21$ features in total.
% \delete{
% We used \textit{mean, standard deviation, median, maximum and minimum} of IPA and gaze distance to describe the features.
% Combining the eye saccade and fixation features, we had $3~\times~5~+3+3~=~21$ features in total.
% }
However, the search space to determine the combination of features that provides the highest predictive power for noticeability includes as many as $\sum_{i=1}^{21}\frac{21!}{i!(21-i!)}~=~2^{21}-1$ conditions, which means that a grid search is not practical.
Therefore, we adopted a similar method as \cite{maslych2023effective} to select the features.
We first selected the best combination of features within each category and then searched the combination of these categories iteratively to figure out the best combination.

In this process, we used Support Vector Regression (SVR) from scikit-learn package~\footnote{https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html} as the benchmark model since SVR has a stable performance on various data.
The SVR model took the selected features as input, then output a probability ranging from 0 to 1 as the predicted noticeability.
We leveraged the leave-one-user-out cross-validation in the test and the mean squared error (MSE) as the metric.

\autoref{table:featureselection} lists the best combination of features within each of four categories.
Among them, the selected combination in the \textit{gaze angular distance} achieved the best performance, while the other features also demonstrated the potential for predicting noticeability.
Therefore, we combined the features from different categories and further tested them.

\begin{table*}[htb]
  \centering
  \small
    \begin{tabular}[width=\columnwidth]{ccc}
    \toprule
    \textbf{Category} & \textbf{Best combination} & \textbf{MSE}\\
    \midrule
    IPA & mean, maximum, minimum & 0.039 (0.013) \\
    % \midrule
    Gaze angular distance & \makecell{mean(hand), std(hand), median(hand) \\ mean(elbow), std(elbow), maximum(elbow)} & 0.017(0.008) \\
    % \midrule
    Eye saccade & frequency, duration, interval & 0.027(0.009) \\
    % \midrule
    Eye fixation & frequency, duration & 0.040 (0.012) \\
    \bottomrule
    \end{tabular}
    \caption{The best feature combination of each category and the prediction performance. The prediction performance is presented as the average (standard deviation) of MSE.}
    \label{table:featureselection}
\end{table*}

We then tested the regression error of all combinations of the feature category with leave-one-user-out cross-validation.
For each feature combination, we filtered the data with it and then fitted a model with 11 participants' data and tested it on the one remaining participant's data.
After repeating this 12 times, we determined the overall regression error for one feature combination.
\autoref{figure:featureselection} illustrates the regression error of all 15 feature combinations.
The results demonstrate that combining all these four category features achieves the best performance with an MSE of 0.011.

To further understand the best feature combination across the users, we also explored the best feature set for each test user in the leave-one-user-out cross-validation process.
For each test user, we trained a model with each feature combination and selected the best one.
The results showed that for 7 out of the 12 participants, the best feature set was the combination of all four feature categories.
For 3 of the 12 participants, the best set was the combination of IPA, Gaze Angular Distance and Fixation and for the other 2 of the 12 participants, the best set contained IPA, Gaze Angular Distance and Saccade.
% \delete{
% These results show that the selected features were able to indicate the visual stimuli's influence on the noticeability for various users.
% }
The results suggest that each feature captures distinct aspects of gaze behavior that contribute to predicting noticeability. 
Although the gaze angular distance showed a lower correlation with noticeability compared to saccades in \autoref{section:formativestudy}, it performs as the most powerful feature for predicting noticeability. 
This may due to the fact that in~\autoref{section:formativestudy}, we only considered the mean distance, whereas in this study, we included additional numerical features, which could provide more informative insights than the mean alone.
As for eye gaze saccade, it also contributes significantly to the prediction, aligning with the correlation results in~\autoref{section:formativestudy}, as it indicates users' visual focus shift and cognitive activity.
While IPA and fixation also have the potential to predict noticeability, their prediction accuracy is lower compared to the other features. 
This could be because they reflect more general cognitive activity and engagement, rather than specific responses to visual stimuli.
However, combining these features allows us to capture both where users are looking at and the dynamic shifts in focus, which together indicate the noticeability of redirection.
% IPA reflects users' cognitive load, which is correlated with their level of engagement and noticeability. 
% Fixations reveal sustained attention, indicating users' intentional focus on a specific region, while saccades highlight transitions in gaze, suggesting more exploratory or spontaneous actions. 
% Together, these features capture both where users are looking at and the dynamic shifts in focus that influence noticeability.

To further investigate if the selected features could model noticeability, we analyzed the regression error for each individual data point in a per user manner. 
As shown in \autoref{figure:predictionperformance_peruser}, the outputs from our model preserved the relative order of noticeability across the six conditions in 90.3\% data points.
The fitted noticeability in various conditions mostly remained in the range of the ground truth, while most errors came from the two most similar conditions (\textbf{CS} and \textbf{NL}). 
Furthermore, \autoref{figure:predictionperformance_average} illustrates the noticeability average and standard deviation of the data collection results and our model's output.
Our model's output average approximates the participant's results while simultaneously exhibiting a lower standard deviation.
This could be due to the inherent noise introduced from estimating the noticeability using the frequency of participants who reported the noticing of redirection in the study.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/implementation/category.png}
    \caption{The regression error of all combinations of the feature category. The error bars denote the standard deviations.}
    \label{figure:featureselection}
\end{figure}

% \subsection{Model selection}

% \delete{
% Subsequently, we tested different models with the selected features to determine the best prediction models.
% We acknowledge that a different set of features might be optimal for a prediction models other than SVR.
% However, we did not aim to fine tune the feature sets for each model; this is not feasible considering the large search space for feature combinations.
% Instead, we compared the performance of different models on the same set of features that we think are most indicative of the noticeability results.
% We implemented several common regression models from scikit-learn package and reported the MSE with leave-one-user-out cross-validation.
% As shown in \autoref{table:modelselection}, the Adaboost regression model achieved the best performance, while other models also achieved a comparable performance.
% Notably, we did not implement more complicated models (e.g., deep learning models) to prevent the model from overfitting to our relatively small dataset.
% In this paper, we aim to demonstrate the potential of predicting noticeability under various visual effects.
% We regard collecting more data and implementing more sophisticated models as important avenues for future work.
% }

% \begin{table}[htb]
%   \centering
%   \small
%     \begin{tabular}[width=\columnwidth]{cc}
%     \toprule
%     \textbf{Model} & \textbf{MSE}\\
%     \midrule
%     SVR & 0.012(0.008) \\
%     % \midrule
%     Linear Regressor & 0.012(0.008) \\
%     % \midrule
%     Adaboost Regressor & 0.008(0.005) \\
%     Decision Tree Regressor & 0.013 (0.007) \\
%     Random Forest Regressor & 0.018 (0.010) \\
%     % \midrule
%     \bottomrule
%     \end{tabular}
%     \caption{The regression performance of different models, presented as the average (standard deviation) of MSE.}
%     \label{table:modelselection}
% \end{table}

% \subsection{Performance analysis}

% \delete{
% To further investigate if our model and selected features could predict noticeability, we analyzed the prediction percentage error for each individual data point in a per user manner. 
% We performed the tests with the SVR model since it was robust and could represent the average performance of different models.
% As shown in \autoref{figure:predictionperformance_peruser}, the predictions from our model preserved the relative order of noticeability across the six conditions in 90.3\% data points.
% The predicted noticeability in various conditions mostly remained in the range of the ground truth, while most errors came from the two most similar conditions (\textbf{CS} and \textbf{NL}). 
% Furthermore, \autoref{figure:predictionperformance_average} illustrates the noticeability average and standard deviation of the data collection results and our model prediction.
% Our model's prediction average approximates the participant's results while simultaneously exhibiting a lower standard deviation.
% This could be due to the inherent noise introduced from estimating the noticeability using the frequency of participants who reported the offset in the study.
% }

\subsection{Classficiation model}
\label{section:classfication_model}

In our studies, noticeability was measured by the frequency with which participants detected redirection during the trials. Based on this, we developed a regression model that outputs the probability of noticing the redirection as a float value between 0 and 1. While this probability effectively indicates how likely users are to notice the redirection, a classification model providing a simple yes/no result could offer greater practical utility.
To explore this, we trained a classification model by applying various thresholds to the noticeability results and categorizing it into distinct classes. 

\begin{description}
\item[Binary model]
We applied a threshold of 0.5 to transform the collected noticeability results into binary labels: Unnoticeable $(\leq 0.5)$ and Noticeable $(> 0.5)$.
With these, we trained a Support Vector Machine (SVM) classification model with the same features selected in \autoref{section:featureselection}; this model achieved an accuracy of 0.9174 $(SD=0.1126)$ and an F1-score of 0.8968 $(SD=0.1342)$ with leave-one-user-out cross-validation on our collected dataset.
\item[Three-class model]
Then we divided the noticeability into three categories with two thresholds: Low Noticeability $(\leq 0.4)$, Medium Noticeability $(0.4 <$ noticeability $\leq 0.7)$, and High Noticeability $(>0.7)$.
With the same SVM classification model and selected feature, our re-trained model achieved an accuracy of 0.8562 $(SD=0.1240)$ and an F1-score of 0.8478 $(SD=0.1276)$.
To be noted, the prediction accuracy was affected by how we converted the noticeability value to separate labels and might increase with fine-tuned features tailored to the classification task.
This indicates that the selected features from the gaze behavioral pattern have the potential to predict the noticeability as separate categories.
\end{description}

\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.47\linewidth}
        \includegraphics[width=\columnwidth]{figures/implementation/opacity_scatterplot.png}
        \caption{}
        \label{figure:predictionperformance_peruser}
    \end{subfigure}
    \centering
    \begin{subfigure}{0.47\linewidth}
        \includegraphics[width=\columnwidth]{figures/implementation/opacity_boxplot_regression.png}
        \caption{}
        \label{figure:predictionperformance_average}
    \end{subfigure}
    \caption{(a) illustrates the regression results in each condition for each user. (b) illustrates the regression results with leave-one-user-out cross-validation.}
    \label{figure:predictionperformance}
\end{figure*}


