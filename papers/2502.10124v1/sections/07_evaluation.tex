\section{Evaluation: Extend to More Visual Stimuli}
\label{section:evaluation}

To evaluate the performance and extendability of our regression model, we tested our model with the same dual-task experiments and yes/no procedure, but with new visual stimuli and new participants.
We tested scale- and color-based visual stimuli, which were not included in the training set of the proposed model.
As the model only takes the user's gaze behavioral data as input without any prior knowledge about the visual stimuli, we aimed to also investigate whether the gaze behavioral patterns of users remain consistent with different types of stimuli.

\subsection{Design}

We designed two new visual stimuli that the participants were required to monitor and report in the secondary task in this study, as shown in \autoref{figure:visualeffects}.
In a scale-based animation, a red ball increased from an invisible small scale to the normal scale of the same as in the opacity stimuli and reset to the invisible small scale.
As for the color-based animation, the red ball would change from red to yellow and reset to red by altering the hue in the HSV color space.
We note that in the opacity- and scale-based animations, the red ball started at an invisible state and we could adjust the ball's location while it was invisible to users.
However, in the case of the color-based animation, the red ball remained consistently visible.
Thus, we adjusted the ball's location when the color animation finished and waited a random time interval before the start of the color visual effect.
We asked participants to report as soon as the color changed to yellow instead of the location change.

For both visual stimuli, we used the duration and layout of the visual stimuli to control their influence on the noticeability.
Therefore, we had $2~\times~3~=~6$ conditions with the order counterbalanced by a Latin square.
However, to avoid the influence of fatigue, we adopted a between-subject study design for the two visual stimuli conditions, where each participant only tested either color- or scale-based stimuli.
In this way, each participant completed $1$ visual effect $\times~(2~durations~\times~3~layouts)~\times~24$ measurements $=~144$ trials in total. 

We calculated the selected features based on the participant's gaze data, as described in \autoref{section:featureselection}, and leveraged our SVR model to output the noticeability.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/evaluation/visualeffects.png}
    \caption{Demonstrations of the three visual effects (opacity, color, scale) in the formative study and the evaluation study.}
    \label{figure:visualeffects}
\end{figure}


\subsection{Apparatus \& Procedure}
The apparatus and procedure were identical to those of our data collection study (\autoref{section:datacollection}). 
The study lasted around 40 minutes and each participant was compensated with \$15 USD.

\subsection{Participants}

We recruited 24 new participants from a local university for the study and divided them into two groups randomly.
For the color-based visual effects, 12 participants (5 females, 7 males, average age of 22.83 years with $SD=1.27$) were allocated to participate in the scale-based effect study.
These participants reported their familiarity with VR as an average of 3.83 $(SD=1.75)$ on a 7-point Likert-type scale from 1 (not at all familiar) to 7 (very familiar).
For the scale-based visual effects, we had another 12 participants (6 females, 6 males) with an average age of 22.75 $(SD=1.76)$ and a self-reported familiarity with VR of an average of 3.25 $(SD=1.22)$.
All participants were right-handed.

\subsection{Result}
% \delete{
% Since there is no previous model that computes the noticeability of redirection using gaze behaviors, we added a linear function baseline in the evaluation.
% We  use the average of the gaze behaviors from unnoticed trials and the average from noticed trials to fit a linear function with the collected data on the opacity-based visual stimuli.
% }

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}{0.47\linewidth}
        \includegraphics[width=0.8\columnwidth]{figures/evaluation/color_boxplot_regression.png}
        \caption{Study results for color-based visual effects.}
        \label{figure:evaluationresult_clr}
    \end{subfigure}
    \centering
    \begin{subfigure}{0.47\linewidth}
        \includegraphics[width=0.8\columnwidth]{figures/evaluation/scale_boxplot_regression.png}
        \caption{Study results for scale-based visual effects.}
        \label{figure:evaluationresult_scl}
    \end{subfigure}
    \caption{Study results of the model trained with opacity-based visual effects, tested on the color- and scale-based visual effects.}
    \label{figure:evaluationresult}
\end{figure*}

% \textit{Controlling the strengths of visual stimuli with duration and layout was effective.}
As shown in \autoref{figure:evaluationresult}, the participants' noticeability of arm redirection with color-based visual stimuli varied from 12\% to 79\% in six conditions with an average of 49\% and a standard deviation of 17\%.
Similarly, the noticeability with scale-based visual stimuli ranged from 17\% to 83\% with an average of 51\% and a standard deviation of 18\%.
This suggested that the noticeability of redirection was effectively affected by the duration and layout of the color- and scale-based visual stimuli.

We further analyzed the virtual and physical trajectory lengths under different noticeability conditions to confirm whether participants' physical movements were effectively redirected, as the same in~\autoref{section:study1results}.
In the unnoticed condition, participants' physical movement trajectories were significantly shorter than their virtual trajectories: \textit{Physical} $(AVG = 1.17,~SD = 0.05)$ and \textit{Virtual} $(AVG = 1.27,~SD = 0.05,~t(11) = 2.48,~p < 0.05)$.
Similarly, in the noticed condition, participants' physical movement trajectories were also shorter than their virtual trajectories: \textit{Physical} $(AVG = 1.15,~SD = 0.04)$ and \textit{Virtual} $(AVG = 1.28,~SD = 0.04,~t(11) = 4.88,~p < 0.01)$.

\textit{Our regression model could accurately compute the noticeability with new color- and scale-based visual stimuli.}
We calculated the selected features with the recorded eye gaze behavior data.
Our model takes the selected features as input and outputs the noticeability in different conditions with leave-one-user-out cross-validation.
For this study, our model achieved an MSE of 0.014 (SD = 0.006) and 0.012 (SD = 0.005) for noticeability with the color- and scale-based visual stimuli separately.
Compared to the 0.011 MSE result of the opacity visual stimuli, these results indicate that our model has the potential to compute noticeability accurately under new visual stimuli with new participants.
% \delete{
% The linear function method, on the other hand, achieved an MSE of 0.018 $(SD=0.008)$ and 0.015 $(SD = 0.006)$ for noticeability with the color- and scale-based visual stimuli.
% These results showed that our machine learning models outperforms the linear function.
% }
This indicates that although our model was built with the data collected from a limited number of participants, the elicited eye behavioral patterns could be generalized to different visual effects and new users.
We note that the two tested stimuli never appeared in the training dataset of our model, and the color-based stimuli even apply a paradigm of being always visible different from the opacity-based stimuli that the model was trained with.
Our results show that our model has the potential to be applied to scenarios with different visual stimuli, as long as the gaze behavioral patterns of users are consistent across scenarios.

\textit{Participants' gaze behavioral patterns are consistent across three conditions of visual stimuli.}
To further explore if participants exhibit similar gaze behavioral patterns when testing different visual stimuli, we conducted a technical evaluation with the data recorded in the previous study.
We leveraged the selected features to train a regression model with the data from one of the three visual stimuli (opacity, color, and scale) and tested the model on the data from the other two stimuli.
As shown in \autoref{table:evaluationresult}, the regression model of color- and scale-based visual stimuli also achieved a comparable performance when computing the noticeability under the same visual stimuli.
While all three regression models achieved the best performance with the test data from the same visual effect, they also proved the ability to compute the noticeability under other two visual stimuli.
This suggests that the gaze behavioral patterns were consistent across visual stimuli, and can be used to compute the noticeability of redirection.


\begin{table}[htb]
  \centering
  \small
    \begin{tabular}[width=\columnwidth]{cccc}
    \toprule
    \textbf{Train set} & \textbf{MSE of Opacity} & \textbf{MSE of Color} & \textbf{MSE of Scale} \\
    \midrule
    Opacity & 0.011(0.005) & 0.014(0.006) & 0.012(0.005) \\
    Color & 0.018(0.014) & 0.011 (0.005) & 0.015(0.009) \\
    Scale & 0.016(0.009) & 0.022(0.014) & 0.013(0.007) \\
    \bottomrule
    \end{tabular}
    \caption{The regression performance of training the model with data under one visual stimulus and testing on the data from all three visual stimuli. The results are presented as the average (standard deviation) of MSE.}
    \label{table:evaluationresult}
\end{table}
