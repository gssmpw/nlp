\section{Introduction}
\label{section:introduction}

% redirection is unique and important in VR
Virtual Reality (VR) systems enable users to embody virtual avatars by mirroring their physical movements and aligning their perspective with virtual avatars' in real time. 
As the head-mounted displays (HMDs) block direct visual access to the physical world, users primarily rely on visual feedback from the virtual environment and integrate it with proprioceptive cues to control the avatar’s movements and interact within the VR space.
Since human perception is heavily influenced by visual input~\cite{gibson1933adaptation}, 
VR systems have the unique capability to control users' perception of the virtual environment and avatars by manipulating the visual information presented to them.
Leveraging this, various redirection techniques have been proposed to enable novel VR interactions, 
such as redirecting users' walking paths~\cite{razzaque2005redirected, suma2012impossible, steinicke2009estimation},
modifying reaching movements~\cite{gonzalez2022model, azmandian2016haptic, cheng2017sparse, feick2021visuo},
and conveying haptic information through visual feedback to create pseudo-haptic effects~\cite{samad2019pseudo, dominjon2005influence, lecuyer2009simulating}.
Such redirection techniques enable these interactions by manipulating the alignment between users' physical movements and their virtual avatar's actions.

% % what is hand/arm redirection, motivation of study arm-offset
% \change{\yj{i don't understand the purpose of this paragraph}
% These illusion-based techniques provide users with unique experiences in virtual environments that differ from the physical world yet maintain an immersive experience. 
% A key example is hand redirection, which shifts the virtual hand’s position away from the real hand as the user moves to enhance ergonomics during interaction~\cite{feuchtner2018ownershift, wentzel2020improving} and improve interaction performance~\cite{montano2017erg, poupyrev1996go}. 
% To increase the realism of virtual movements and strengthen the user’s sense of embodiment, hand redirection techniques often incorporate a complete virtual arm or full body alongside the redirected virtual hand, using inverse kinematics~\cite{hartfill2021analysis, ponton2024stretch} or adjustments to the virtual arm's movement as well~\cite{li2022modeling, feick2024impact}.
% }

% noticeability, motivation of predicting a probability, not a classification
However, these redirection techniques are most effective when the manipulation remains undetected~\cite{gonzalez2017model, li2022modeling}. 
If the redirection becomes too large, the user may not mitigate the conflict between the visual sensory input (redirected virtual movement) and their proprioception (actual physical movement), potentially leading to a loss of embodiment with the virtual avatar and making it difficult for the user to accurately control virtual movements to complete interaction tasks~\cite{li2022modeling, wentzel2020improving, feuchtner2018ownershift}. 
While proprioception is not absolute, users only have a general sense of their physical movements and the likelihood that they notice the redirection is probabilistic. 
This probability of detecting the redirection is referred to as \textbf{noticeability}~\cite{li2022modeling, zenner2024beyond, zenner2023detectability} and is typically estimated based on the frequency with which users detect the manipulation across multiple trials.

% version B
% Prior research has explored factors influencing the noticeability of redirected motion, including the redirection's magnitude~\cite{wentzel2020improving, poupyrev1996go}, direction~\cite{li2022modeling, feuchtner2018ownershift}, and the visual characteristics of the virtual avatar~\cite{ogawa2020effect, feick2024impact}.
% While these factors focus on the avatars, the surrounding virtual environment can also influence the users' behavior and in turn affect the noticeability of redirection.
% One such prominent external influence is through the visual channel - the users' visual attention is constantly distracted by complex visual effects and events in practical VR scenarios.
% Although some prior studies have explored how to leverage user blindness caused by visual distractions to redirect users' virtual hand~\cite{zenner2023detectability}, there remains a gap in understanding how to quantify the noticeability of redirection under visual distractions.

% visual stimuli and gaze behavior
Prior research has explored factors influencing the noticeability of redirected motion, including the redirection's magnitude~\cite{wentzel2020improving, poupyrev1996go}, direction~\cite{li2022modeling, feuchtner2018ownershift}, and the visual characteristics of the virtual avatar~\cite{ogawa2020effect, feick2024impact}.
While these factors focus on the avatars, the surrounding virtual environment can also influence the users' behavior and in turn affect the noticeability of redirection.
This, however, remains underexplored.
One such prominent external influence is through the visual channel - the users' visual attention is constantly distracted by complex visual effects and events in practical VR scenarios.
We thus want to investigate how \textbf{visual stimuli in the virtual environment} affect the noticeability of redirection.
With this, we hope to complement existing works that focus on avatars by incorporating environmental visual influences to enable more accurate control over the noticeability of redirected motions in practical VR scenarios.
% However, in realistic VR applications, the virtual environment often contains complex visual effects beyond the virtual avatar itself. 
% We argue that these visual effects can \textbf{distract users’ visual attention and thus affect the noticeability of redirection offsets}, while current research has yet taken into account.
% For instance, in a VR boxing scenario, a user’s visual attention is likely focused on their opponent rather than on their virtual body, leading to a lower noticeability of redirection offsets on their virtual movements. 
% Conversely, when reaching for an object in the center of their field of view, the user’s attention is more concentrated on the virtual hand’s movement and position to ensure successful interaction, resulting in a higher noticeability of offsets.

Since each visual event is a complex choreography of many underlying factors (type of visual effect, location, duration, etc.), it is extremely difficult to quantify or parameterize visual stimuli.
Furthermore, individuals respond differently to even the same visual events.
Prior neuroscience studies revealed that factors like age, gender, and personality can influence how quickly someone reacts to visual events~\cite{gillon2024responses, gale1997human}. 
Therefore, aiming to model visual stimuli in a way that is generalizable and applicable to different stimuli and users, we propose to use users' \textbf{gaze behavior} as an indicator of how they respond to visual stimuli.
In this paper, we used various gaze behaviors, including gaze location, saccades~\cite{krejtz2018eye}, fixations~\cite{perkhofer2019using}, and the Index of Pupil Activity (IPA)~\cite{duchowski2018index}.
These behaviors indicate both where users are looking and their cognitive activity, as looking at something does not necessarily mean they are attending to it.
Our goal is to investigate how these gaze behaviors stimulated by various visual stimuli relate to the noticeability of redirection.
With this, we contribute a model that allows designers and content creators to adjust the redirection in real-time responding to dynamic visual events in VR.

To achieve this, we conducted user studies to collect users' noticeability of redirection under various visual stimuli.
To simulate realistic VR scenarios, we adopted a dual-task design in which the participants performed redirected movements while monitoring the visual stimuli.
Specifically, participants' primary task was to report if they noticed an offset between the avatar's movement and their own, while their secondary task was to monitor and report the visual stimuli.
As realistic virtual environments often contain complex visual effects, we started with simple and controlled visual stimulus to manage the influencing factors.

% first user study, confirmation study
% collect data under no visual stimuli, different basic visual stimuli
We first conducted a confirmation study (N=16) to test whether applying visual stimuli (opacity-based) actually affects their noticeability of redirection. 
The results showed that participants were significantly less likely to detect the redirection when visual stimuli was presented $(F_{(1,15)}=5.90,~p=0.03)$.
Furthermore, by analyzing the collected gaze data, results revealed a correlation between the proposed gaze behaviors and the noticeability results $(r=-0.43)$, confirming that the gaze behaviors could be leveraged to compute the noticeability.

% data collection study
We then conducted a data collection study to obtain more accurate noticeability results through repeated measurements to better model the relationship between visual stimuli-triggered gaze behaviors and noticeability of redirection.
With the collected data, we analyzed various numerical features from the gaze behaviors to identify the most effective ones. 
We tested combinations of these features to determine the most effective one for predicting noticeability under visual stimuli.
Using the selected features, our regression model achieved a mean squared error (MSE) of 0.011 through leave-one-user-out cross-validation. 
Furthermore, we developed both a binary and a three-class classification model to categorize noticeability, which achieved an accuracy of 91.74\% and 85.62\%, respectively.

% evaluation study
To evaluate the generalizability of the regression model, we conducted an evaluation study (N=24) to test whether the model could accurately predict noticeability with new visual stimuli (color- and scale-based animations).
Specifically, we evaluated whether the model's predictions aligned with participants' responses under these unseen stimuli.
The results showed that our model accurately estimated the noticeability, achieving mean squared errors (MSE) of 0.014 and 0.012 for the color- and scale-based visual stimili, respectively, compared to participants' responses.
Since the tested visual stimuli data were not included in the training, the results suggested that the extracted gaze behavior features capture a generalizable pattern and can effectively indicate the corresponding impact on the noticeability of redirection.

% application
Based on our model, we implemented an adaptive redirection technique and demonstrated it through two applications: adaptive VR action game and opportunistic rendering.
We conducted a proof-of-concept user study (N=8) to compare our adaptive redirection technique with a static redirection, evaluating the usability and benefits of our adaptive redirection technique.
The results indicated that participants experienced less physical demand and stronger sense of embodiment and agency when using the adaptive redirection technique. 
These results demonstrated the effectiveness and usability of our model.

In summary, we make the following contributions.
% 
\begin{itemize}
    \item 
    We propose to use users' gaze behavior as a medium to quantify how visual stimuli influences the noticebility of redirection. 
    Through two user studies, we confirm that visual stimuli significantly influences noticeability and identify key gaze behavior features that are closely related to this impact.
    \item 
    We build a regression model that takes the user's gaze behavioral data as input, then computes the noticeability of redirection.
    Through an evaluation study, we verify that our model can estimate the noticeability with new participants under unseen visual stimuli.
    These findings suggest that the extracted gaze behavior features effectively capture the influence of visual stimuli on noticeability and can generalize across different users and visual stimuli.
    \item 
    We develop an adaptive redirection technique based on our regression model and implement two applications with it.
    With a proof-of-concept study, we demonstrate the effectiveness and potential usability of our regression model on real-world use cases.

\end{itemize}

% \delete{
% Virtual Reality (VR) allows the user to embody a virtual avatar by mirroring their physical movements through the avatar.
% As the user's visual access to the physical world is blocked in tasks involving motion control, they heavily rely on the visual representation of the avatar's motions to guide their proprioception.
% Similar to real-world experiences, the user is able to resolve conflicts between different sensory inputs (e.g., vision and motor control) through multisensory integration, which is essential for mitigating the sensory noise that commonly arises.
% However, it also enables unique manipulations in VR, as the system can intentionally modify the avatar's movements in relation to the user's motions to achieve specific functional outcomes,
% for example, 
% % the manipulations on the avatar's movements can 
% enabling novel interaction techniques of redirected walking~\cite{razzaque2005redirected}, redirected reaching~\cite{gonzalez2022model}, and pseudo haptics~\cite{samad2019pseudo}.
% With small adjustments to the avatar's movements, the user can maintain their sense of embodiment, due to their ability to resolve the perceptual differences.
% % However, a large mismatch between the user and avatar's movements can result in the user losing their sense of embodiment, due to an inability to resolve the perceptual differences.
% }

% \delete{
% However, multisensory integration can break when the manipulation is so intense that the user is aware of the existence of the motion offset and no longer maintains the sense of embodiment.
% Prior research studied the intensity threshold of the offset applied on the avatar's hand, beyond which the embodiment will break~\cite{li2022modeling}. 
% Studies also investigated the user's sensitivity to the offsets over time~\cite{kohm2022sensitivity}.
% Based on the findings, we argue that one crucial factor that affects to what extent the user notices the offset (i.e., \textit{noticeability}) that remains under-explored is whether the user directs their visual attention towards or away from the virtual avatar.
% Related work (e.g., Mise-unseen~\cite{marwecki2019mise}) has showcased applications where adjustments in the environment can be made in an unnoticeable manner when they happen in the area out of the user's visual field.
% We hypothesize that directing the user's visual attention away from the avatar's body, while still partially keeping the avatar within the user's field-of-view, can reduce the noticeability of the offset.
% Therefore, we conduct two user studies and implement a regression model to systematically investigate this effect.
% }

% \delete{
% In the first user study (N = 16), we test whether drawing the user's visual attention away from their body impacts the possibility of them noticing an offset that we apply to their arm motion in VR.
% We adopt a dual-task design to enable the alteration of the user's visual attention and a yes/no paradigm to measure the noticeability of motion offset. 
% The primary task for the user is to perform an arm motion and report when they perceive an offset between the avatar's virtual arm and their real arm.
% In the secondary task, we randomly render a visual animation of a ball turning from transparent to red and becoming transparent again and ask them to monitor and report when it appears.
% We control the strength of the visual stimuli by changing the duration and location of the animation.
% % By changing the time duration and location of the visual animation, we control the strengths of attraction to the users.
% As a result, we found significant differences in the noticeability of the offsets $(F_{(1,15)}=5.90,~p=0.03)$ between conditions with and without visual stimuli.
% Based on further analysis, we also identified the behavioral patterns of the user's gaze (including pupil dilation, fixations, and saccades) to be correlated with the noticeability results $(r=-0.43)$ and they may potentially serve as indicators of noticeability.
% }

% \delete{
% To further investigate how visual attention influences the noticeability, we conduct a data collection study (N = 12) and build a regression model based on the data.
% The regression model is able to calculate the noticeability of the offset applied on the user's arm under various visual stimuli based on their gaze behaviors.
% Our leave-one-out cross-validation results show that the proposed method was able to achieve a mean-squared error (MSE) of 0.012 in the probability regression task.
% }

% \delete{
% To verify the feasibility and extendability of the regression model, we conduct an evaluation study where we test new visual animations based on adjustments on scale and color and invite 24 new participants to attend the study.
% Results show that the proposed method can accurately estimate the noticeability with an MSE of 0.014 and 0.012 in the conditions of the color- and scale-based visual effects.
% Since these animations were not included in the dataset that the regression model was built on, the study demonstrates that the gaze behavioral features we extracted from the data capture a generalizable pattern of the user's visual attention and can indicate the corresponding impact on the noticeability of the offset.
% }

% \delete{
% Finally, we demonstrate applications that can benefit from the noticeability prediction model, including adaptive motion offsets and opportunistic rendering, considering the user's visual attention. 
% We conclude with discussions of our work's limitations and future research directions.
% }

% \delete{
% In summary, we make the following contributions.
% }
% % 
% \begin{itemize}
%     \item 
%     \delete{
%     We quantify the effects of the user's visual attention directed away by stimuli on their noticeability of an offset applied to the avatar's arm motion with respect to the user's physical arm. 
%     Through two user studies, we identified gaze behavioral features that are indicative of the changes in noticeability.
%     }
%     \item 
%     \delete{We build a regression model that takes the user's gaze behavioral data and the offset applied to the arm motion as input, then computes the probability of the user noticing the offset.
%     Through an evaluation study, we verified that the model needs no information about the source attracting the user's visual attention and can be generalizable in different scenarios.
%     }
%     \item 
%     \delete{We demonstrate two applications that potentially benefit from the regression model, including adaptive motion offsets and opportunistic rendering.
%     }

% \end{itemize}

\begin{comment}
However, users will lose the sense of embodiment to the virtual avatars if they notice the offset between the virtual and physical movements.
To address this, researchers have been exploring the noticing threshold of offsets with various magnitudes and proposing various redirection techniques that maintain the sense of embodiment~\cite{}.

However, when users embody virtual avatars to explore virtual environments, they encounter various visual effects and content that can attract their attention~\cite{}.
During this, the user may notice an offset when he observes the virtual movement carefully while ignoring it when the virtual contents attract his attention from the movements.
Therefore, static offset thresholds are not appropriate in dynamic scenarios.

Past research has proposed dynamic mapping techniques that adapted to users' state, such as hand moving speed~\cite{frees2007prism} or ergonomically comfortable poses~\cite{montano2017erg}, but not considering the influence of virtual content.
More specifically, PRISM~\cite{frees2007prism} proposed adjusting the C/D ratio with a non-linear mapping according to users' hand moving speed, but it might not be optimal for various virtual scenarios.
While Erg-O~\cite{montano2017erg} redirected users' virtual hands according to the virtual target's relative position to reduce physical fatigue, neglecting the change of virtual environments. 

Therefore, how to design redirection techniques in various scenarios with different visual attractions remains unknown.
To address this, we investigate how visual attention affects the noticing probability of movement offsets.
Based on our experiments, we implement a computational model that automatically computes the noticing probability of offsets under certain visual attractions.
VR application designers and developers can easily leverage our model to design redirection techniques maintaining the sense of embodiment adapt to the user's visual attention.
We implement a dynamic redirection technique with our model and demonstrate that it effectively reduces the target reaching time without reducing the sense of embodiment compared to static redirection techniques.

% Need to be refined
This paper offers the following contributions.
\begin{itemize}
    \item We investigate how visual attractions affect the noticing probability of redirection offsets.
    \item We construct a computational model to predict the noticing probability of an offset with a given visual background.
    \item We implement a dynamic redirection technique adapting to the visual background. We evaluate the technique and develop three applications to demonstrate the benefits. 
\end{itemize}



First, we conducted a controlled experiment to understand how users perceived the movement offset while subjected to various distractions.
Since hand redirection is one of the most frequently used redirections in VR interactions, we focused on the dynamic arm movements and manually added angular offsets to the' elbow joint~\cite{li2022modeling, gonzalez2022model, zenner2019estimating}. 
We employed flashing spheres in the user's field of view as distractions to attract users' visual attention.
Participants were instructed to report the appearing location of the spheres while simultaneously performing the arm movements and reporting if they perceived an offset during the movement. 
(\zhipeng{Add the results of data collection. Analyze the influence of the distance between the gaze map and the offset.}
We measured the visual attraction's magnitude with the gaze distribution on it.
Results showed that stronger distractions made it harder for users to notice the offset.)
\zhipeng{Need to rewrite. Not sure to use gaze distribution or a metric obtained from the visual content.}
Secondly, we constructed a computational model to predict the noticing probability of offsets with given visual content.
We analyzed the data from the user studies to measure the influence of visual attractions on the noticing probability of offsets.
We built a statistical model to predict the offset's noticing probability with a given visual content.
Based on the model, we implement a dynamic redirection technique to adjust the redirection offset adapted to the user's current field of view.
We evaluated the technique in a target selection task compared to no hand redirection and static hand redirection.
\zhipeng{Add the results of the evaluation.}
Results showed that the dynamic hand redirection technique significantly reduced the target selection time with similar accuracy and a comparable sense of embodiment.
Finally, we implemented three applications to demonstrate the potential benefits of the visual attention adapted dynamic redirection technique.
\end{comment}

% This one modifies arm length, not redirection
% \citeauthor{mcintosh2020iteratively} proposed an adaptation method to iteratively change the virtual avatar arm's length based on the primary tasks' performance~\cite{mcintosh2020iteratively}.



% \zhipeng{TO ADD: what is redirection}
% Redirection enables novel interactions in Virtual Reality, including redirected walking, haptic redirection, and pseudo haptics by introducing an offset to users' movement.
% \zhipeng{TO ADD: extend this sentence}
% The price of this is that users' immersiveness and embodiment in VR can be compromised when they notice the offset and perceive the virtual movement not as theirs~\cite{}.
% \zhipeng{TO ADD: extend this sentence, elaborate how the virtual environment attracts users' attention}
% Meanwhile, the visual content in the virtual environment is abundant and consistently captures users' attention, making it harder to notice the offset~\cite{}.
% While previous studies explored the noticing threshold of the offsets and optimized the redirection techniques to maintain the sense of embodiment~\cite{}, the influence of visual content on the probability of perceiving offsets remains unknown.  
% Therefore, we propose to investigate how users perceive the redirection offset when they are facing various visual attractions.


% We conducted a user study to understand how users notice the shift with visual attractions.
% We used a color-changing ball to attract the user's attention while instructing users to perform different poses with their arms and observe it meanwhile.
% \zhipeng{(Which one should be the primary task? Observe the ball should be the primary one, but if the primary task is too simple, users might allocate more attention on the secondary task and this makes the secondary task primary.)}
% \zhipeng{(We need a good and reasonable dual-task design in which users care about both their pose and the visual content, at least in the evaluation study. And we need to be able to control the visual content's magnitude and saliency maybe?)}
% We controlled the shift magnitude and direction, the user's pose, the ball's size, and the color range.
% We set the ball's color-changing interval as the independent factor.
% We collect the user's response to each shift and the color-changing times.
% Based on the collected data, we constructed a statistical model to describe the influence of visual attraction on the noticing probability.
% \zhipeng{(Are we actually controlling the attention allocation? How do we measure the attracting effect? We need uniform metrics, otherwise it is also hard for others to use our knowledge.)}
% \zhipeng{(Try to use eye gaze? The eye gaze distribution in the last five seconds to decide the attention allocation? Basically constructing a model with eye gaze distribution and noticing probability. But the user's head is moving, so the eye gaze distribution is not aligned well with the current view.)}

% \zhipeng{Saliency and EMD}
% \zhipeng{Gaze is more than just a point: Rethinking visual attention
% analysis using peripheral vision-based gaze mapping}

% Evaluation study(ideal case): based on the visual content, adjusting the redirection magnitude dynamically.

% \zhipeng{(The risk is our model's effect is trivial.)}

% Applications:
% Playing Lego while watching demo videos, we can accelerate the reaching process of bricks, and forbid the redirection during the manipulation.

% Beat saber again: but not make a lot of sense? Difficult game has complicated visual effects, while allows larger shift, but do not need large shift with high difficulty


