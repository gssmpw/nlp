% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\begin{document}
%
\title{Using Cognitive Models to Improve Training Against Human and GPT-4 Generated Social Engineering Attacks}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Tyler Malloy \and
Maria Jos√© Ferreira \and 
Fei Fang \and
Cleotilde Gonzalez}
%
\authorrunning{Malloy et al.}
\institute{Carnegie Mellon University, Pittsburgh PA 15222, USA}
%
\maketitle
Social engineering attacks are commonly used by cyber criminals to gain access to valuable and sensitive data. Recent Large Language Models (LLMs) such as GPT-4 have demonstrated the ability to produce convincing text that mimics human writing, and code that could be used to create fake emails and websites that appear to be legitimate. Research in cybersecurity has noted the risks of increased proliferation of social engineering attacks through the use of LLMs. However, existing methods of training end users against social engineering attacks are typically based on simple human-designed emails in a classroom-style delivery of instruction. In this work, we propose the use of GPT-4 to write convincing text that mimics real emails, as well as HTML and CSS code to stylize emails. To our knowledge, this is the first study of human anti-phishing training that compares how human students learn when presented with emails and code written by humans or generated by GPT-4. Our research introduces an experimental paradigm to determine whether there is a difference in end-user detection using human-written and GPT-4 generated emails. This was done in a two-by-two design that varied the original author of the email text (Human or GPT-4) as well as the style of the email (Plain-text or HTML/CSS). A pre-experiment quiz on the indicators of phishing emails served as a measure of the base phishing knowledge of participants, and a post-experiment questionnaire had participants indicate what proportion of the content they observed was generated by an AI chat-bot. In reality, participants observed either exclusively human written or GPT-4 generated emails.

Experimentation results show that emails written by humans and stylized using HTML/CSS code generated by GPT-4 are the most challenging for end-users, with a significant interaction effect leading to the GPT-4 written and HTML/CSS stylized emails being the easiest for participants to categorize. Analysis of the performance of participants based on their perception of content as being AI written demonstrates a significant bias whereby participants rate more emails as being phishing if they believe a higher proportion of the emails were generated by AI. This effect represents a novel \textit{AI-writing bias} that leads to participants assuming that AI written emails are phishing attempts. This bias is closely related to the well studied phenomenon of algorithm aversion. Participants that had less initial knowledge of phishing emails performed worse on average across all experiment conditions compared to participants that performed better on the initial phishing quiz. These two groups, participants who have less initial phishing knowledge and those who perceive all AI-written content as being more likely to be phishing, could have their performance improved through a better method of selecting emails to show to participants.

Alongside this experiment, we propose an Instance-Based Learning cognitive model that uses GPT-4 embeddings of emails as attributes to predict behavior in the email categorization task. This cognitive model can be used to both predict participant categorization of emails, and determine the optimal email to show to that participant. This is done by iterating over all possible emails that could be shown to a participant, and selecting the email that has the highest probability of incorrect categorization by that participant at that time in the training. This is inspired by the intuition that more difficult emails will expand participant's ability to categorize a wider variety of emails. We demonstrate simulation results that predict the potential improvement in participant training outcomes through this email selection method.  Finally, we performed a follow-up experiment to confirm the benefits of selecting emails using a cognitive model of participant behavior, with the true improvement over the baseline matching closely with our predictions. Comparisons of the performance of participants in this follow-up experiment shows that selecting emails in this way reduces the effect that low phishing knowledge and high AI-writing bias have on performance.

Overall, the contributions of this work are, first, the outline of some limitations to current social engineering training methods, second, the identification of a potential solution to these limitations through the use of a cognitive model to improve learning outcomes, and third, the confirmation of this potential solution experimentally by testing it using human participants. A novel bias is presented in which participants assumed that AI-written emails are more likely to be phishing, leading to worse categorization performance. We show experimentally that selecting educational example emails using an IBL cognitive model reduces the effect of the AI-writing bias we demonstrate. These results show the usefulness of cognitive models in predicting the learning progress of end-users in training scenarios, and the difficulty of correctly identifying phishing emails that are written by humans and then stylized by GPT-4.


\end{document}
