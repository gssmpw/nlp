% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\bibliographystyle{splncs04}
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\usepackage{url}
\begin{document}
%
\title{Modeling the Training of Human and GPT-4 Social Engineering Attack Recognition}
%\title{Modeling and Evaluating Human and GPT-4 Social Engineering Attack Detection}
%Using Cognitive Models to Improve Training Against Human and GPT-4 Generated Social Engineering Attacks}
% Modeling the Training of Human and GPT-4 Social Engineering Attack Recognition
\titlerunning{Human and GPT-4 Social Engineering Attacks}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Tyler Malloy \and
Maria Jos√© Ferreira \and 
Fei Fang \and
Cleotilde Gonzalez}
%
\authorrunning{Malloy et al.}
\institute{Carnegie Mellon University, Pittsburgh PA 15222, USA}
%
\maketitle

\begin{abstract}
    Social engineering attacks remain a critical tool for cybercriminals seeking to exploit sensitive data. Although the threat of AI-generated content in such attacks is growing, current training methods predominantly rely on simplistic human-designed emails. This research introduces a novel experimental paradigm to investigate differences in the detection of human-generated versus AI-generated phishing emails. Our behavioral results reveal that emails co-created by humans and Generative-AI models pose a greater challenge to end users compared to those emails created by GPT-4 or Human only. We also propose a cognitive model that predicts user behavior during training, which offers the potential to be used in future training frameworks to improve training effectiveness. Our work contributes by (1) identifying critical weaknesses in current social engineering training and (2) proposing a cognitive model-driven solution to better equip users against evolving threats.
\end{abstract}

\section{Introduction}
Social engineering attacks are commonly used by cyber criminals to gain access to valuable and sensitive data. Recent Large Language Models (LLMs) such as GPT-4 have demonstrated the ability to produce convincing text that mimics human writing, and code that could be used to create fake emails and websites that appear to be legitimate. Research in cybersecurity has identified the risks of increased proliferation of social engineering attacks through the use of LLMs \cite{schmitt2024digital}. However, the efficacy of LLM-generated emails in training users against social engineering attacks has not been evaluated. Many training programs are based on simple human-designed emails in classroom-style instruction delivery \cite{wen2019hack}. In this work, we propose the use of GPT-4 to write convincing text that mimics real emails, as well as HTML and CSS code to stylize emails. To our knowledge, this is the first study designed to establish the efficacy of GPT-4-generated emails compared to those written by humans. We also evaluate the efficacy of emails that are co-created by humans and styled by GPT-4. 

Our research introduces an experimental paradigm to determine whether there is a difference in end user detection using human-written and GPT-4 generated emails. This was done in a two-by-two design that varied the original author of the email text (Human or GPT-4) as well as the style of the email (Plain-text or HTML/CSS). A pre-experiment quiz on the indicators of phishing emails served as a measure of the base phishing knowledge of participants, and a post-experiment questionnaire had participants indicate what proportion of the content they observed was generated by AI. Participants observed exclusively human-written or GPT-4 generated emails.

The results of the experiment show that emails written by humans and stylized using HTML/CSS code generated by GPT-4 are the most challenging for end users, with a significant interaction effect leading to the GPT-4 written and HTML/CSS stylized emails being the easiest for participants to categorize. Analysis of the performance of participants based on their perception of content as AI-written demonstrates a significant bias by which participants rate more emails as phishing if they believe a higher proportion of emails were generated by AI. This effect represents a novel \textit{AI-writing bias} that leads participants to assume that AI-written emails are phishing attempts. This bias is closely related to the well-studied phenomenon of algorithm aversion. Participants who had less initial knowledge of phishing emails performed worse on average under all experiment conditions compared to participants who performed better on the initial phishing quiz. These two groups, participants who have less initial knowledge about phishing and those who perceive all AI-written content as being more likely to be phishing, could improve their performance through a better method of selecting emails to show to participants.

Alongside this experiment, we propose an Instance-Based Learning (IBL) cognitive model that uses GPT-4 embeddings of emails as attributes to predict the user's behavior in the email categorization task. 
%This cognitive model can be used to predict the categorization of the emails of the participants and determine the optimal email to show to that participant. 
%This is done by iterating over all possible emails that could be shown to a participant, and selecting the email that has the highest probability of incorrect categorization by that participant at that time in the training. This is inspired by the intuition that more difficult emails will expand participant's ability to categorize a wider variety of emails. 

We demonstrate that the IBL model is capable of accurately predicting the user's classification. We also run a simulation study to demonstrate how the model could be used to predict the categorization of a user and, by this prediction, select an optimal email to show to that participant to optimize their training.

%This is done by iterating over all possible emails that could be shown to a participant, and selecting the email that has the highest probability of incorrect categorization by that participant at that time in the training. This is inspired by the intuition that more difficult emails will expand participant's ability to categorize a wider variety of emails. 
%results that predict the potential improvement in participant training outcomes through this email selection method.  

\section{Background}
Generative Artificial Intelligence (GAI) has the potential to improve education and training in a variety of settings through increased accessibility and reduced costs (for a review, see \cite{baldassarre2023social}. However, there are significant ethical concerns due to the potential negative societal impacts of these models being misused \cite{bommasani2021opportunities}, such as through the generation of social engineering attacks \cite{al2023chatgpt}. One commonly used and widely available class of GAI are pre-trained Large Language Models (LLMs) that can be prompted to produce highly convincing textual outputs that resemble human writing \cite{sejnowski2023large}. While these methods are trained to avoid producing potentially harmful content, they can be repeatedly prompted when changing the initial prompt or continuing with different prompts, in an effort to produce desired outputs \cite{white2023prompt}. The design of the prompts that are input into LLMs to produce text is call \textit{prompt engineering}, and can be used to improve the quality of the LLM output \cite{chen2023unleashing}. The repeated prompting of LLMs has been applied onto predicting how humans may speed up learning through the use of natural language instructions that can be used to inform the predicted value of actions without needing experience of performing those actions in a specific environment state \cite{mcdonald2023exploring}.

LLMs such as the Generative Pretrained Transformer 3 (GPT-3) \cite{brown2020language} have been evaluated in their social engineering ability and have shown lower performance in designing social engineering attacks compared to humans \cite{sharma2023well}. The ability of these models is constantly evolving, putting into question the ability of newer models to design social engineering attacks \cite{kumar2023certifying}. While more advanced models may be able to produce more human-like text, they also have more advanced methods to prevent misuse. This work seeks to evaluate the newer GPT-4 model \cite{achiam2023gpt} in its ability to design phishing emails, as well as to compare the effectiveness of social engineering attacks designed by humans and LLM alone and emails generated by different combinations of the output of the human and LLM model. 

This work introduces an experimental paradigm for evaluating the potential harm of LLM use in one specific area, social engineering attacks. This experimental paradigm is used to compare social engineering attacks in the form of phishing emails that are either fully written by human cybersecurity experts, fully written by GPT-4, or a combination of the two through prompt engineering. Alongside this experiment, we propose a method to mitigate the potential misuse of LLMs in cybersecurity contexts by improving training against social engineering attacks. This is done by using a cognitive model to trace and predict individual learning progress and determine the best educational examples to show to participants. 

Overall, the contributions of this work are, first, the outline of some limitations to current social engineering training methods and, second, the identification of a potential solution to these limitations through the use of a cognitive model to improve learning outcomes. A novel bias is presented, in which participants assumed that AI-written emails are more likely to be phishing, leading to worse categorization performance. We show through simulation that selecting educational example emails using an IBL cognitive model reduces the effect of the AI-writing bias we demonstrate. These results show the usefulness of cognitive models in predicting the learning progress of end users in training scenarios, and the difficulty of correctly identifying phishing emails that are written by humans and then stylized by GPT-4.

\subsection{Large Language Models and Social Engineering Attacks}
The use of LLMs in the production of social engineering attacks demonstrates a significant concern for cybersecurity \cite{gupta2023chatgpt}. The simplicity of Generative AI tools makes them easy to apply to tasks such as writing phishing emails from scratch or stylizing existing phishing emails to look more convincing, potentially increasing their effectiveness \cite{sharma2023well}. Modern LLMs are even capable of producing code \cite{khan2022automatic}, such as Javascript, HTML, and CSS, \cite{lajko2022towards} that can create highly convincing emails that resemble real emails sent from many companies \cite{park2024ai}. This adds an additional layer to the potential misuse of LLMs in social engineering attacks, as hand-writing code for realistic looking emails would normally take minutes or hours, and can be done in seconds with LLMs. These two areas, writing original phishing emails and stylizing emails with HTML and CSS code, are the main focus of our experiment to investigate how users may be susceptible to social engineering attacks from humans and LLMs. 

One method of reducing the potential harm of LLMs is through the use of specific training that can make LLMs less likely to produce harmful content \cite{cao2023defending}. This is typically done using feedback from humans, either machine learning engineers or crowd-sourced participants in user studies \cite{bai2022training}. This can train models to avoid producing content that is designed to trick or scam users, such as phishing emails. However, the effectiveness of these methods in preventing the generation of dangerous content forms is not perfect and can often be worked around with more complex prompt engineering \cite{fredrikson2015model}. More advanced prompting can also train a separate model to adjust the prompt until it is accepted by the LLM and the desired content is produced \cite{zou2023universal}. In this work, we focus on using relatively simple prompt engineering to faithfully replicate what we view as a realistic scenario of a cyber attacker applying an LLM to write a phishing email. 

\subsection{Social Engineering Training}
Training end users to identify social engineering attacks is an important part of cybersecurity \cite{back2021cyber}. Users without experience in security are vulnerable, making them the `weakest link' of cyber defense \cite{vishwanath2022weakest}. Phishing emails are an especially common method of social engineering due to the high volume of emails sent daily and the potential for compromising systems provided by redirecting users to unintended websites, among other methods \cite{gupta2016literature}. Typically, training users to identify phishing emails focuses on specific features of these emails that can indicate that they are phishing attempts, such as the use of urgent language; making requests of confidential information; making an offer; containing a link to a dangerous website; among other features \cite{kumaraguru2009school}. In the past, this has been done using plain text emails written by human cybersecurity experts \cite{weaver2021training}. These training paradigms are a large industry and are commonly required by individuals, universities, companies, and other groups that are interested in improving the ability of end users to identify phishing emails \cite{jampen2020don}. 

\begin{figure}[t!] 
\begin{centering}
  \includegraphics[width=\textwidth]{Figures/Trial.png} 
  \caption{An example of the email identification task shown to participants}\label{fig:Trial}
 \end{centering} 
\end{figure}

Given the ever-updated nature of phishing attempts and the ease of use of LLMs in creating social engineering attacks, it is important to understand how users make decisions and learn from examples of emails written or stylized by LLMs. The intelligence selection of training examples shown to students has been shown to improve their learning outcomes \cite{ferguson2006improving}. This can be done by applying cognitive modeling methods to predict participant learning and decision making \cite{feng2011student}. In this work, these cognitive models are adjusted to reflect human behavior and serve as a baseline that can test various methods to improve end-user training on the identification of phishing emails. 

\subsection{Cognitive Modeling}
Cognitive models have previously been applied to predict human learning in anti-phishing training \cite{singh2023cognitive}. Recently, Generative AI models have been integrated with cognitive models by forming \textit{representations}, of stimuli, such as textual information using LLM embeddings \cite{malloy2024applying}, \cite{malloy2024leveraging}. This approach has demonstrated human-like abilities to recognize new stimuli, even when they are informationally complex, based on past experiences \cite{malloy2024efficient}. We propose the use of LLM embeddings as attributes of a cognitive model to both predict student learning and evaluate them under different experimental conditions. These same models are also used to simulate possible improvements in phishing education that can be afforded by intelligently selecting email examples. 

An Instance-Based Learning (IBL) model is used to both predict human learning in each condition of our experiment, and simulate the potential improvement of human learning afforded by an intelligence selection of example emails. Using LLMs to form representations of emails allows us to use the same representation method in experimental conditions. Comparing the accuracy of the IBL model in predicting human behavior across conditions allows us to assess how effectively it can be used to predict general human behavior. Additionally, we perform a simulation of these IBL models that fit human learning and decision making that allows us to evaluate methods of improving user learning in the identification of phishing emails. These simulation results provide evidence for our proposed method of improving cognitive-based training to make participant learning outcomes as efficient and effective as possible. 

\subsection{Instance Based Learning}
IBL models work by storing instances $i$ in memory $\mathcal{M}$, composed of utility outcomes $u_i$ and options $k$ composed of features $j$ in the set of features $\mathcal{F}$ of environmental decision alternatives. In the case of predicting student learning from phishing emails, these options include labeling an email as being either dangerous (phishing) or benign (ham), the features correspond to the attributes of the email that are relevant for determining if it is a phishing email, in our model the LLM embeddings, and the outcome corresponds to the point feedback provided to students depending on whether they are correct (1 point) or incorrect (-1 points). These options are observed in an order represented by the time step $t$, and the time step in which an instance occurred is given $\mathcal{T}(i)$. When tracing human participant performance, the memory is composed of the options presented to participants, the options that they selected, and the utility reward that was presented to them. 

To model the retrieval of instances in memory when calculating the expected value of different option alternatives, IBL models calculate the activation of each instance in memory based on the current options available. In calculating this activation, the similarity between instances in memory and the current instance is represented by adding the value $S_{ij}$ over all attributes, which is the similarity of the attribute $j$ of instance $i$ to the current state. This gives the activation equation as: 
 
\begin{equation}
A_i(t) = \ln \Bigg( \sum_{t' \in \mathcal{T}_i(t)} (t - t')^{-d}\Bigg) + \mu \sum_{j \in \mathcal{F}} \omega_j (S_{ij} - 1) + \sigma \xi
\label{eq:activation}
\end{equation}
The parameters of the IBL model can either be fit to individual human performance, or set to their default values. These parameters are the decay parameter $d$; the mismatch penalty $\mu$; the attribute weight of each $j$ feature $\omega_j$; and the noise parameter $\sigma$. The default values for these parameters are $(d,\mu,\omega_j,\sigma) = (0.5, 1, 1, 0.25)$. The IBL models in this work use default values to predict individual student behaviors. The value $\xi$ is drawn from a normal distribution $\mathcal{N}(-1,1)$ and multiplied by the noise parameter $\sigma$ to add random noise to the activation. Varying these parameters impacts which instances are retrieved, and ultimately how the predicted utility of option alternatives is calculated.  

When predicting human learning and decision making based on textual information such as phishing emails, it is possible to use LLMs to form embeddings of these emails as attributes of the IBL model \cite{malloy2024applying}. To calculate the similarity metric $S_{ij}$ between two emails, we use the cosine similarity of their embeddings, as is done in \cite{malloy2024leveraging}. In this work, this has the benefit that the same method of forming attributes from emails can be used across experimental conditions. Thus, we can assess the effectiveness of an IBL+LLM cognitive model in predicting human learning and decision making during training. 

The blended value of an option $k$ is calculated at time step $t$ according to the utility outcomes $u_i$ weighted by the probability of retrieval of that instance $P_i$ and summing over all instances in memory $\mathcal{M}_k$ to give the equation:
\begin{equation}
V_k(t) = \sum_{i \in \mathcal{M}_k} P_i(t)u_i
\label{eq:blending}
\end{equation}

Where $P_i(t)$ is the probability of retrieval, calculated by an inverse-temperature weighted soft-max of all available instance activations. 

\begin{figure}[t!] 
\begin{centering}
  \includegraphics[width=0.7\textwidth]{Figures/Emails.png} 
  \caption{Top-Left: The original plain-text email written by human experts Bottom-Left: The GPT-4 stylized version of this original email. Bottom-Right: The fully GPT-4 rewritten and stylized version of the email. Top-Right: The stripped plain-text version of the fully GPT-4 rewritten email.}\label{fig:Emails}
 \end{centering} 
\end{figure}

\section{Experiment}
The recent proliferation of phishing emails written or styled by large language models (LLMs) brings into question our understanding of how users make judgments of phishing emails and how these judgments compare between human and LLM written content. These LLM written emails can either be fully authored by humans, by LLMs, or a combination of the two where a human creates one of either the text body or styling, and the LLM creates the other. To test these different options of generating emails, we use a 2x2 design varying author (Human or GPT-4) or style (plain-text or GPT-4 stylized). We designed an experiment to collect human judgments of phishing (dangerous) and ham (safe) emails and varied the author (Human or GPT-4) and style (Plain-text or Styled) in a between-subjects 2x2 design. 

An example of the experimental interface used to evaluate the identification training of phishing emails is shown in Figure \ref{fig:Trial}. In this example, the email being shown is a human-written and plain-text styled email. Importantly, for each experimental condition, the same set of 360 emails was used, all based on the original dataset of plain-text emails written by human cybersecurity experts that was used in a previous study \cite{singh2023cognitive}. These base emails were then either stylized by GPT-4, or rewritten entirely by prompting GPT-4 to write an email with the same attributes that the experts coded the original emails as having. The fully GPT-4 rewritten email is also stripped of HTML and CSS code and presented as the plain-text version of the GPT-4 written email. This resulted in 4 sets of 360 emails with the same general features and topics in each set. Figure \ref{fig:Emails} shows the same email that is stylized, fully rewritten, and the plain-text version of that email. 

\subsection{Methods}
This experiment compares human learning and decision making when categorizing emails as phishing (dangerous) or ham (safe) depending on the email author (Human or GPT-4) and style (plain-text or GPT-4 stylized). We are interested in determining which condition is the most difficult for humans to make accurate judgments in and whether there is a relationship between participant confidence, reaction time, and accuracy. This is an important potential relationship as it can aid in our overall goal of improving the quality of example emails shown to participants based on their performance.

\begin{figure}[t!] 
\begin{centering}
  \includegraphics[width=0.7\textwidth]{Figures/BarPerformance.png} 
  \caption{Pre and post-training categorization accuracy for ham and phishing emails by experimental condition.}\label{fig:BarPerformance}
 \end{centering} 
\end{figure}

This experiment included 10 pre-training trials without feedback, 40 training trials with feedback, and 10 post-training trials without feedback. During all trials, participants made judgments about emails as phishing or ham and indicated their confidence in their judgment. We recruited 268 participants online through the Amazon Mechanical Turk (AMT) platform. Of these participants, 44 did not complete all 60 trials and were excluded from further analysis. Of the remaining 224 participants, 18 were removed due to poor performance in the categorization task, as predefined in the study preregistration. This predefined criterion removed all participants who performed less than two standard deviations below the mean categorization improvement between pre-training and post-training trials. 

This exclusion resulted in a total of 207 participants used for the following analysis. Participants (69 Female, 137 Male, 1 Non-binary) had an average age of 40.02 with a standard deviation of 10.48 years. Of these participants, 25 had never received a phishing email, 101 had received phishing emails on a few occasions, and 79 had received phishing emails on many occasions.  Participants were compensated with a base payment of \$3 with the potential to earn up to a \$12 bonus payment depending on performance. This experiment was approved by the Carnegie Mellon University Institutional Review Board, and the study was pre-registered on OSF\footnote{\url{https://osf.io/wbg3r/}}. All participant data and analysis code is available on OSF. 

\subsection{Results}
The primary comparison between conditions is done in terms of the improvement in categorization accuracy percentage between the 10 pre-training trials and the 10 post-training trials. These results are shown in Figure \ref{fig:BarPerformance}, with the pre-training performance lightly shaded and the post-training performance a darker shade. The only decrease in performance between pre and post-training was in the Human written and GPT-4 styled ham email categorization. 

A mixed repeated measure analysis of variance of the effect of the author of the email and the style of the email on the improvement of categorization demonstrated no significant variation in author ($F=1.101,p=0.295,\eta_p^2=0.005$) but a significant variation of style ($F=12.261$, $p=0.001$, $\eta_p^2=0.057$) as well as a significant interaction between author and style ($F=14.344$, $p<0.001$, $\eta_p^2=0.066$). A post-hoc multi-comparison Tukey test showed that the improvement of the human subject in the human written and GPT-4-styled condition had a significantly lower improvement from the prior training to the post-training categorization accuracy ($p=0.033$) when compared to the GPT-4-written and GPT-4-styled condition. All other comparisons between conditions did not show a significant difference in the effect. This indicates that the smallest improvement in participant categorization accuracy was the Human written and GPT-4 styled condition ($\mu=0.015$) while the largest improvement was in the GPT-4 written and styled condition ($\mu=0.104$).

These results demonstrate the difficulty of training participants to identify emails that were written by human cybersecurity experts and stylized by GPT-4. Interestingly, the highest accuracy for the detection of phishing emails after training was observed with the written and styled by GPT-4. This is potentially due to the safety methods built into the GPT-4 model which could have hindered the model's ability to write convincing phishing emails. Alternative approaches to the GPT-4 model prompting, such as prompt attack, could produce more convincing phishing emails, though these complex methods may be outside of the skill set of most cybersecurity attackers.    

The results of this analysis indicate that GPT-4 stylized human-written phishing emails present the most challenging learning and decision-making paradigm. There was a strong interaction effect between the author of the email and the style, whereby the author was less relevant in plain-text emails, but became significant in stylized emails. This is crucial to our understanding of phishing email training, since many existing platforms still use plain-text emails in training examples.

\subsection{Participant AI Identification}
\begin{figure}[t!] 
\begin{centering}
  \includegraphics[width=\textwidth]{Figures/AIBias.png} 
  \caption{Linear regression comparing the percentage of emails categorized as being phishing emails and the proportion of emails identified as being AI written. Regressions are split between each of the four experimental conditions. Shaded regions represent 95\% confidence intervals of linear regression with $R^2$ and slope labeled.}\label{fig:PerceptionCondition}
 \end{centering} 
\end{figure}
To capture human participant identification of how emails were created, they were asked four questions at the end of the experiment to estimate the number of emails that they saw that were AI generated. The next comparison we performed was to assess the overall probability of categorizing an email as phishing based on how likely a participant was to categorize an email as being phishing based on their identification of emails as AI-generated or created by humans.  

These results are shown in Figure \ref{fig:PerceptionCondition} which shows a regression of the average percent of emails classified as phishing, since half of all emails shown to the participants were phishing, a correct categorization of all emails would result in 50\% emails being classified as phishing. In general, the participants tended to categorize more than half of the emails they were shown as phishing emails. Additionally, there was an overall trend across each condition that the higher the proportion of emails identified as AI written, the higher the probability of categorizing any email as being phishing.

It may seem surprising that the increased perception of emails as written by an AI model would lead to this bias in categorizing emails as being phishing. However, people generally demonstrate a poor ability to detect AI-written content \cite{kobis2021artificial}, which could interact with general aversion to algorithms \cite{burton2020systematic}) which has been shown to be higher in people who have experience with algorithms making incorrect judgments \cite{dietvorst2015algorithm}. 

We can see from this regression that participants who identified emails as being AI written in both of the GPT-4 styled conditions were more likely to categorize emails as being phishing if they had a higher identification of emails as being AI written. This represents an important bias in participant identification of emails that could potentially be exploited by cybersecurity attackers. This further motivates the improvement of training for detecting social engineering attacks that are designed by both humans and LLMs. 

A comparison of the slopes of these regressions in Figure \ref{fig:PerceptionCondition} demonstrates that this effect of phishing categorization bias is not equal across conditions. Notably, the likelihood of categorizing emails as being phishing has both a higher slope and a higher $R^2$ for emails that were styled by GPT-4. Looking back to the four example emails shown in Figure \ref{fig:Emails}, we can see that both of the GPT-4 styled conditions include banners, logos, bold text and other styled text that may draw the attention of participants. It is likely that participants were attending to these more salient features in the GPT-4 styled conditions, which if perceived as being AI generated could bias participants into believing that emails are phishing. 

These comparisons demonstrate that there is a difference between experimental conditions in how identifying emails as being AI written impacts the likelihood of categorizing emails as being phishing. This has important implications for both understanding how participants make judgments of emails in different contexts, as well as how best to design training when incorporating LLMs into the design of example emails. It is important that participants not over attend to irrelevant features like the perception of content as being AI written, and focus on relevant features like the presence of offers or incorrect sender addresses. 

\begin{figure}[!t] 
\begin{centering}
\includegraphics[width=\textwidth]{Figures/Simulations.png} 
  \caption{All improvement measures refer to the percentage point difference between pre-training and post-training accuracy. Left: Human participants (pastel colors) compared to IBL model training (bright colors) improvement under randomized email selection. Right: Simulated IBL student model improvement under IBL teacher model email selection (dark colors) compared to IBL model training under randomized email selection (bright colors). Color indicates condition, shade indicates training method, error bars indicate standard deviation.}\label{fig:training}
 \end{centering} 
\end{figure}

\subsection{Cognitive Modeling}
Before describing our proposed method for improving phishing training against human and LLM attackers, we must determine the appropriate method of modeling human behavior in this task. The emails used in each condition have the same base email, a plain-text and human-written email that had hand-crafted attributes associated with it. An alternative to using these hand-crafted features is to use LLM embedding representations of emails. However, the complex nature of these embedding representations means they may be difficult to use in a cognitive model that seeks to reflect the realities of human cognition. 

To assess these two different approaches in their ability to model and predict human-like learning in this task, we compared an IBL model that used hand-crafted attributes with one that used LLM model embeddings (IBL+LLM). This was done using a model-tracing approach for each individual participant, which works by setting the IBL model to select the same choice made by an individual participant, and observing the same utility outcome from that choice that the participant observed. This allows us to compare the IBL and the performance of human participants with the same experience.  

\subsection{Proposed Phishing Training supported by IBL}
Our proposed method to improve the learning outcomes of phishing training is based on the use of an IBL model to perform model training during the experiment and select emails to show to participants based on that model. Specifically, this model will be trained on all trials of an experiment based on the emails shown to a participant. During the pre-training and post-training trial blocks, the emails will be selected randomly from all possible emails. Then, during the training block where participants receive feedback, the model will search through all possible emails to find the email with the highest probability of being incorrectly categorized. 

The theory behind this approach is that emails should be selected to show participants when there is a high probability that the participant will misclassify them. This can ensure that participants observe a diverse and challenging set of emails, based on their individual performance on past trials. Since we only have data from human participants in trials in which emails are selected at random, we instead compare these two email selection training approaches using IBL+LLM models. The average percentage point improvement in participant categorization accuracy between the pre-training and post-training trials is shown in the lighter shaded bars on the left column of Figure \ref{fig:training}.

IBL+LLM simulated students have the same training as the experiment, with 10 pre-training trials without feedback, 40 training trials with feedback, and 10 post-training trials without feedback. These IBL models trained with a random sampling of emails are compared to the same IBL models trained with emails selected by a separate IBL teacher model. This teacher model is structured in the same way as the IBL tracing models described in previous results. These IBL teachers predict the behavior of simulated IBL students. After each trial of the main training portion of the experiment, the IBL teacher model iterates over all emails that have not yet been shown to the IBL student, and selects the email that has assigns the highest expected utility to the incorrect categorization for that email. 

Other than this training period with emails selected by the IBL teacher, the same standard pre and post-training periods are performed with randomized emails. Results from this training method are shown on the right column of Figure \ref{fig:training}, and demonstrate a clear and significant improvement between the training outcomes, as measured by pre-post-training improvement in terms of percentage point accuracy, between the random email sampling and the IBL+LLM teacher sampling. This suggests that selecting emails to show students using an IBL teaching model may improve the quality of educational outcomes.  

Overall, this comparison of different methods to train simulated IBL+LLM student models provides support for our planned study that will use a IBL teacher model to select the emails that real human participants will observe. This future study will confirm the benefit afforded by using an IBL teacher model to trace the performance of human students and select emails to show to them that will maximize their learning outcomes. The selection of emails to choose those that are most difficult for an individual student effectively broadens the range of emails they experience in the training block when they are receiving feedback. 

\section{Discussion}
In this work, we present a method for assessing different potential uses of GPT-4 by human cyberattackers interested in crafting phishing emails. Results from this experimentation highlights an issue of current methods of training end users to identify phishing emails and improve cybersecurity. Alongside this, we present a proposed solution to the issues that we highlight, to improve the quality of phishing email identification training through the use of a cognitive model. This is done by using an Instance Based Learning model to select the emails that are shown to participants and improve their learning outcomes.

Several interesting and surprising results from analyses of human behavior were revealed in our experimental result. Firstly, the most significant different between any two conditions of the experiment was in the human-written and GPT-4-styled condition and the GPT-4-written and GPT-4-styled condition. Comparing pre-training performance and improvement in the plain-text styled conditions showed little difference between different email authors. This interaction demonstrates that the GPT-4 model is unlikely to write convincing phishing emails from scratch without more advanced prompt engineering.

Another important result from experimental analysis was the observed bias between the perception of emails as being generated by an AI model. As participants were more likely to perceive emails as being written or stylized by AI, the worse their performance in categorizing ham emails. It is possible that the presence of this bias could be incorporated into improved feedback to students, to point out that AI generated writing does not necessarily indicate that an email is phishing. 

Improving education of AI-generated content is an important step to preventing the misuse of LLMs in the future, by improving the public awareness of the capabilities of LLMs, and how best to detect when they are potentially being used for nefarious purposes. A significant area of research in machine learning is seeking to further the capabilities of LLMs, aligning their outputs to human goals and use cases, and make misuse more difficult. However, it is unlikely that a perfect model will ever be trained, as it is possible to train separate models to learn how to best prompt LLMs to allow for unintended use cases. Thus, proper education and training is a crucial step to reducing the potential harm of LLMs in the future. 

\section*{Acknowledgments}
This research was sponsored by the Army Research Office and accomplished under Australia-US MURI Grant Number W911NF-20-S-000, and the AI Research Institutes Program funded by the National Science Foundation under AI Institute for Societal Decision Making (AI-SDM), Award No. 2229881. Compute resources and GPT model credits were provided by the Microsoft Accelerate Foundation Models Research Program grant ``Personalized Education with Foundation Models via Cognitive Modeling"

\bibliography{springer}

\end{document}
