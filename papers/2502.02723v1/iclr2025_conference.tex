
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}




\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{arydshln}
\usepackage{multirow}
\usepackage{multicol} 
\usepackage{makecell}
\usepackage[ruled]{algorithm2e}
\SetAlgoCaptionSeparator{}
\usepackage{etoolbox}
\makeatletter
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{wrapfig}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{ulem}
% \usepackage{amsmath, amsthm, amssymb}
% \usepackage{pifont}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{arydshln}
\usepackage{multirow}
\usepackage{multicol} 
\usepackage{makecell}
\usepackage{etoolbox}
\makeatletter
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{wrapfig}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{ulem}
\usepackage{hyperref}

\usepackage[ruled]{algorithm2e}
\SetAlgoCaptionSeparator{}
\renewcommand\AlCapFnt{\normalfont\footnotesize\bfseries}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors





\usepackage{algorithmic}
\usepackage[defaultlines=2,all]{nowidow}
\renewcommand{\algorithmicrequire}{ \textbf{Input:}}  
\renewcommand{\algorithmicensure}{ \textbf{Output:}}    
\definecolor{linkc}{rgb}{0, 0.44, 0.73}
\definecolor{eqc}{rgb}{1, 0, 0}
%\usepackage[pagebackref=false,breaklinks=true,colorlinks,urlcolor=linkc,citecolor=linkc,linkcolor=eqc,bookmarks=false]{hyperref}
% \usepackage{hyperref}
% \usepackage{url}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{pifont}
\usepackage{fancyhdr}

\usepackage{authblk}

\input{packages.tex}
\newcommand{\hc}{\rowcolor{teal!15}}

\newcommand{\kurt}[1]{\textcolor{red}{kurt:\ #1}}
\newcommand{\cfx}[1]{\textcolor{blue}{cfx:\ #1}}
\newcommand{\jinghan}[1]{\textcolor{brown}{jinghan:\ #1}}

\newcommand{\rebuttal}[1]{{\color{blue}{#1}}}

% \usepackage[capitalize,noabbrev]{cleveref}

\title{Dobi-SVD: Differentiable SVD for LLM Compression and Some New Perspectives}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \author[ ]{Qinsi Wang 
% \thanks{Qinsi Wang and Jinghan Ke contribute equally. Order is decided by coin flip. This work was done when Jinghan Ke visited UC Berkeley.} 
%  \textsuperscript{ ,1}, Jinghan Ke\footnotemark[1] \textsuperscript{ ,2} , Masayoshi Tomizuka\textsuperscript{2} , Yiran Chen\textsuperscript{1} Kurt Keutzer\textsuperscript{2} , Chenfeng Xu \thanks{Chenfeng Xu advises the work and is the corresponding author. }\textsuperscript{ ,2}}
% \affil[ ]{\textsuperscript{1} Duke University, \textsuperscript{2} University of California, Berkeley}



\author[ ]{\textbf{Qinsi Wang\textsuperscript{1}\thanks{Qinsi Wang and Jinghan Ke contribute equally. Order is decided by coin flip. This work was done when Jinghan Ke visited UC Berkeley.}, \quad Jinghan Ke\textsuperscript{2}\footnote[1]  ,,\quad Masayoshi Tomizuka\textsuperscript{2}, \quad Yiran Chen\textsuperscript{1}}, \\\textbf{Kurt Keutzer\textsuperscript{2},\quad Chenfeng Xu\textsuperscript{2}\thanks{Chenfeng Xu advises the work and is the corresponding author. }}}
\affil[ ]{\textsuperscript{1}Duke University \hspace{10pt} \textsuperscript{2}University of California, Berkeley}
\affil[ ]{\textsuperscript{ }}

\affil[ ]{\textsuperscript{}\href{https://ah-miu.github.io/Dobi-SVD.page}{\textcolor{blue}{https://ah-miu.github.io/Dobi-SVD.page}}}





 
% \author{Qinsi Wang \thanks{Qinsi Wang and Jinghan Ke contribute equally. Order is decided by coin flip. This work was done when Jinghan Ke visited UC Berkeley.} \\
% Duke University\\
% \And
% Jinghan Ke\footnotemark[1] \& Masayoshi Tomizuka \& Kurt Keutzer \& Chenfeng Xu \thanks{Chenfeng Xu advises the work and is the corresponding author.} \\
% University of California, Berkeley \\
 % \\
%}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Large language models (LLMs) have sparked a new wave of AI applications; however, their substantial computational costs and memory demands pose significant challenges to democratizing access to LLMs for a broader audience. Singular Value Decomposition (SVD), a technique studied for decades, offers a hardware-independent and flexibly tunable solution for LLM compression. In this paper, we present new directions using SVD: we first theoretically and experimentally analyze the optimality of directly truncating activations, then we further identify three key issues on SVD-based LLM compression, including (1) How can we determine the optimal truncation position for each layer in LLMs? (2) How can we efficiently update the weight matrices based on truncated activations? (3) How can we address the inherent "injection" nature that results in the information loss of the SVD? We propose a new paradigm for SVD-based LLM compression, \textbf{Dobi-SVD}, to tackle the three issues. 
First, we propose a \textbf{differentiable} truncation mechanism, along with gradient-robust backpropagation, enabling the model to adaptively find the optimal truncation positions. Next, we utilize the Eckart-Young-Mirsky theorem to derive a theoretically \textbf{optimal} weight update formula through rigorous mathematical analysis. Lastly, by observing and leveraging the quantization-friendly nature of matrices after SVD, we reconstruct a mapping between truncation positions and memory requirements, establishing a \textbf{bijection} from truncation positions to memory.
Experimental results show that with a 40\% parameter-compression rate, our method achieves a perplexity of 9.07 on the Wikitext2 dataset with the compressed LLama-7B model, a 78.7\% improvement over the state-of-the-art SVD for LLM compression method.
We emphasize that Dobi-SVD is the first to achieve such a high-ratio LLM compression while maintaining competitive performance. We also extend our Dobi-SVD to vision-language models (VLMs) and vision-language-action models (VLAs), thereby highlighting its generalizability and practical value. We hope that the inference speedup—up to \textbf{12.4x} on 12GB NVIDIA Titan Xp GPUs and \textbf{3x} on 80GB A100 GPUs for LLMs, \textbf{1.2x} and \textbf{1.17x} on 80GB A100 GPUs for VLMs and VLAs, respectively —will bring significant benefits to the broader community such as multi-modal learning and robotics etc.
% , unveiling the tremendous potential of SVD for LLM compression and providing new outlooks and baseline for future research in this area.
\end{abstract}

 \vspace{-10pt}
\section{Introduction}
 \vspace{-10pt}
Large language models (LLMs), such as GPT \citep{GPT4}, Llama \citep{llama}, and OPT \citep{opt}, have shown that scaling the size of the model and the training data can unlock impressive performance and contextual learning abilities. 
However, because of the growing number of parameters in LLMs and the limited memory capacity of current hardware, inference with LLMs is highly expensive. This limits their practical applications, especially for resource-constrained hardware devices and latency-sensitive programs, such as robotics, edge-device applications, and interactive entertainment. 
In this paper, we aim to address a key challenge: How can we perform inference with LLMs using fewer computational resources and memory, while maintaining the performance of the pre-trained models?

Model compression \citep{compression1, compression2, compression3} has been extensively studied, with the aim of squeezing the original model into a lightweight one with a low compression ratio, \textit{i.e.}, the ratio between the memory required by the compressed model and the original model.
However, existing mainstream compression methods have their own limitations. 
For instance, quantization reduces the storage memory by converting floating-point calculations into lower-bit integer calculations, but it lacks flexibility and hardware generalization due to the dependency on specific hardware support \citep{qlora,awq,OPTQ,kim2023squeezellm}. 
Model pruning shrinks the network by reducing redundant parameters that are less sensitive to performance, but hardware-accelerated structured pruning often leads to significant performance degradation \citep{llmpruner, sparsegpt, slicegpt, ShearedLlama}. 
For example, when compressing LLaMA-7B, LLM-Pruner resulted in a 37.6\% performance drop on WikiText2, even at a compression ratio of 0.8.
Knowledge distillation employs a high-complexity teacher network to guide a lower-complexity student network for model compression; however, it requires retraining a new model, which incurs high time and computational costs \citep{pkd,dynabert,adabert}.


In contrast to these techniques, another straightforward compression method is low-rank decomposition, which is free from the aforementioned limitations. 
As a technique studied many decades, singular value decomposition (SVD) \citep{SVD} has played a significant role in fields such as image compression \citep{IMAGE_SVD1,IMAGE_KSVD2}, communication transmission \citep{COMM_SVD1}, and signal denoising \citep{SIG_SVD1,SIG_SVD2}. 
% \rebuttal{SVD typically involves truncating the singular values to reduce the dimensionality and focus on the most significant components.}
However, its potential for LLM compression has not yet been fully explored.
Theoretically, SVD reduces memory and computation by truncating the singular values of a matrix, decomposing large weight matrices into the product of two smaller matrices. Models compressed with SVD can be easily deployed across a wide range of devices, and SVD offers the flexibility to compress models to any compression ratio.

\begin{figure*}[t]
\vspace{-30pt}
\centering
	\begin{minipage}{.97\linewidth}
        %这个图片路径替换
		\centerline{\includegraphics[width=\textwidth]{FIG/overview.pdf}}
          % 加入对这列的图片说明
	\end{minipage}
        \vspace{-5pt}
         
\caption{Overview framework of Dobi-SVD:
% 1-3:\textbf{Differentiable Truncation position Training.} 
% By employing parameter renormalization to make the truncation differentiable and using Taylor to prevent gradient explosion, Dobi-SVD achieves adaptive optimization of the truncation position.
% 4: \textbf{Weight Update.}  
% By using IPCA, Dobi-SVD enables streaming extraction of updated weight features.
% 5: \textbf{Remapping.}  
% By utilizing the quantization-friendly properties of SVD-decomposed matrices, Dobi-SVD introduces a new storage method that achieve bijection of rank and compression ratio.}
1-3: \textbf{Differentiable Truncation Position Training.}  
By applying parameter renormalization for continuous rank ratio selection and using Taylor expansion to prevent gradient explosion, our method enables robust and adaptive optimization of truncation positions.
4: \textbf{Weight Update.}  
Using IPCA, we sequentially extract and optimally update weight matrix features.
5: \textbf{Remapping.}  
We resolve a long-overlooked limitation of traditional SVD-based compression through remapping, fully unlocking SVD’s potential for data compression.
}
\label{fig_overview}
\vspace{-15pt}
\end{figure*}

However, existing SVD-based methods for LLM compression have not achieved desirable results. 
Traditional SVD \citep{8,9,10} truncated the model weights directly, often resulting in significant performance degradation and requiring extensive retraining. 
Recently proposed activation-aware SVD hopes that the truncated weights make the activations close to the original ones, but it still fails to deliver satisfactory performance. 
For example, ASVD \citep{asvd} proposes scaling the weights using a diagonal matrix to reduce the distribution error of the activations before and after truncation.
SVD-LLM \citep{svdllm} introduces a truncation-aware data whitening strategy to retain singular values critical for activations. 
However, these methods exhibit severe performance degradation. At $0.4$ compression ratio, SVD-LLM experiences a 644.7\% drop in model performance on Wikitext2 dataset, which is unacceptable in real-world applications.
Due to the significant performance loss, SVD has not yet become a mainstream method for LLM compression compared to other compression techniques.

We aim to change this landscape by making SVD a viable and widely adopted option. 
% \rebuttal{Standing apart from previous weight compression and activation-aware compression methods that merely minimize compressed and original activation distances through weight-related truncation, we open up a novel path: we should directly truncate activations while still being able to reconstruct weights.This redefines the possibilities of activation-aware compression.}
Unlike previous weight-only or activation-aware methods focusing on minimizing the distance between new and original activations through weight-based truncation, we open up a novel path: directly truncating activations while enabling weight reconstruction without fine-tuning. 
Additionally, we are the first to fully utilize singular value information by addressing a long-overlooked limitation of traditional SVD-based compression methods.
To achieve these, we analyze three associated challenges: 

\begin{enumerate}[leftmargin=*]
    \item \textbf{How to determine the appropriate truncation position for each layer in an LLM?} Different weight in the model have varying sensitivities to performance degradation. By designing specific truncation points for different weight matrices (\textit{i.e.}, the number of retained singular values), it is possible to achieve smaller performance loss at same compression ratio. However, due to the high dimensionality of the matrices in LLMs, the solution space for truncation positions is exceptionally large. Thus, efficiently finding the optimal combination of truncation is a significant challenge.
    \item \textbf{How to update the weights based on truncation position?} Directly truncating the weights results in significant performance degradation. Although activation-aware SVD mitigates this issue to some extent, how to update the weights in a way that maximally preserves activation information has not yet been fully explored or theoretically proven.
    \item \textbf{How to overcome the long-overlooked truncation-value limitation?} A fundamental issue with SVD-based compression is that to achieve effective compression, at least half of the singular values need to be truncated (for square matrices). This implies that even a modest compression ratio requires very low ranks, leading to substantial information loss during matrix compression, directly limiting the capacity of SVD to compress models effectively.
\end{enumerate}

We propose a new paradigm, \textbf{Dobi-SVD}. An overview is shown in Fig. \ref{fig_overview}. First, through experimental analysis and theoretical investigation, we compare the effectiveness of truncating activations versus truncating weights, verifying the superiority of activation truncating. Based on this analysis, we provide answers to the three key challenges raised. 
First, by introducing differentiable truncation values and stable SVD backpropagation, we enable the model's performance to directly guide the matrix in adapting to find optimal truncation points. 
Then, leveraging the Eckart-Young-Mirsky theorem and the positive definiteness of the SVD, we derive the theoretical optimal weight update formula and employ Incremental Principal Component Analysis (IPCA) to extract features of updated weight sequentially and address memory constraints. 
Finally, we propose a novel SVD-based storage method that takes advantage of the value concentration property of decomposed matrices. 
This method establishes a bijection mapping between the truncation position and the model compression ratio, thereby overcoming the truncation limitation.

% Our approach effectively addresses the challenges of SVD in compressing LLMs, achieving minimal performance degradation even with high compression ratios. For example, when the compression ratio is $0.4$, Dobi-SVD compresses LLaMA-7B to achieve a PPL of $9.07$ on WikiText2, while the state-of-the-art SVD-LLM reaches $42.30$. By overcoming the inherent limitations of SVD, our method demonstrates that SVD-based compression for LLMs remains a promising research direction. Additionally, our algorithm can be combined with other model compression techniques, such as quantization and pruning, to achieve high parameters ratios of near-lossless compression.

Dobi-SVD effectively addresses the three challenges of SVD in compressing LLMs.
Specifically, our method achieves performance breakthroughs in three key areas. In terms of \textbf{task performance}, Dobi-SVD achieves minimal performance degradation even with high compression ratios for the first time. When compressing the LLaMA-7B model to a compression ratio of 0.4, the Dobi-SVD compressed model reaches a PPL of 9.07 on WikiText2, which represents a 78.5\% improvement over the previous state-of-the-art SVD compression method, and a 9.83\% improvement over the best pruning methods that require post-training and fine-tuning. This establishes SVD as an effective and highly competitive option for LLM compression; 
In terms of \textbf{hardware performance}, Dobi-SVD significantly reduces inference time for LLMs on low-cost GPUs. On an NVIDIA TITAN Xp 12GB GPU, the Dobi-SVD compressed LLaMA-7B model achieved a generation speed of 25.97 tokens/second, delivering a 12.4$\times$ speedup compared to the original model. In particular, Dobi-SVD is hardware-agnostic, offering superior generalization across different hardware targets compared to other compression methods;
In terms of \textbf{integration with other compression methods}, Dobi-SVD can be effectively combined with other compression methods such as quantization to achieve a lower compression ratio. When combined with GPTQ-4bit, Dobi-SVD compresses the LLaMA-7B model to just 3.4GB, while maintaining a perplexity of 9.97 on WikiText2, which shows that Dobi-SVD is an effective solution for model compression that can be widely applied and combined.
Moreover, we extend Dobi-SVD to a broader field, vision-language models (VLMs). Specifically, we apply Dobi-SVD to compress the popular VLM, LLAVA V1.5-7B. Experiments demonstrate that our method improves throughput by 1.2 times. 
Notably, we find that our Dobi-SVD not only enhances efficiency but also improves VLM performance on the Pope-random dataset.
We also deployed Dobi-SVD on the state-of-the-art Vision-Language-Action model, OpenVLA. Under a compression ratio of 0.4, our method achieves a 17.6\% acceleration on the NVIDIA A100 while maintaining nearly lossless performance.
In summary, Dobi-SVD is the first approach to make SVD-based compression methods truly competitive, highlighting the significant potential of SVD for model compression. In addition to our proposed method, we also provide some new perspectives in the Appendix; we hope to inspire the community to further advance this direction.


\vspace{-5pt}
\section{Preliminaries}
 \label{sec_pre}
 \vspace{-5pt}
\subsection{Mathematical expression of SVD for LLM }
\label{sec_pre_1}
 \vspace{-5pt}
Given a weight matrix $W \in \mathbb{R}^{m \times n}$, it can be decomposed through SVD as $W = U \Sigma V^T$, where $U\in\mathbb{R}^{m \times m}$ and $V\in\mathbb{R}^{n \times n} $ are the right and left singular vector matrices, respectively. $\Sigma = \text{diag}(\sigma_1, \dots, \sigma_m)$ is an $m \times n$ diagonal matrix and $\sigma_1, \dots, \sigma_m$ are the singular values of $W$. 

\textbf{The SVD compression process of $W$} can be summarized in three steps. 
\textbf{Decomposition}: use SVD to decompose $W$. 
\textbf{Truncation}: retain the top singular values $k$ and obtain the truncated singular-value matrix $\Sigma_k = \text{diag}(\sigma_1, \dots, \sigma_k, 0, \dots, 0)$.
\textbf{Reconstruction}: reconstruct $W$ into two matrices $W_1 = U \sqrt{\Sigma_k}, \quad W_2 =  \sqrt{\Sigma_k}V$, where $W_1 \in \mathbb{R}^{m \times k}$ and $W_2 \in \mathbb{R}^{k \times n}$. Through the process, $ W $ can be compressed into $W_1$ and $W_2$. We define the compression ratio as $k(m+n)/(m \times n)$.
% SVD compression for LLMs has been widely studied (\citep{3,4,5}). In early studies, SVD was often used to compress embedding layers (\citep{1,2}).  
% As the size of models continues to increase, more research has begun to explore the use of SVD for weight compression (\citep{6,7}). In particular, recent studies (\citep{truth}) have shown that the weights in LLMs are often approximately low-rank matrices. 
% This suggests that SVD holds significant potential in LLMs compression.
 \vspace{-5pt}
\subsection{Basic Propositions.} 
 \vspace{-5pt}
Before the derivation of $\widetilde{W}$, we introduce three basic propositions:

\textit{\textbf{Proposition 1}:~Rank Property of Matrix Multiplication.} 
For the matrix multiplication $AB = C$, the rank of $C$ satisfies $rank(C) \leq \min(rank(A), rank(B))$.


\textit{\textbf{Proposition2}:~Eckart-Young-Mirsky Theorem.}
For a matrix $A \in \mathbb{R}^{m \times n}$, the best rank-$k$ approximation of $A$ in terms of the Frobenius norm or spectral norm is given by the matrix $A_k = U_A \Sigma_k V_A^T$.

\textit{\textbf{Proposition3}:~Positive Definiteness of Matrices in SVD.}
Given the SVD of a matrix $A = U_A \Sigma_A V_A^\top$, the matrices $U_A$ and $V_A$ are both orthogonal, satisfying $V_A^\top V_A = U_A^\top U_A = I$.

 \vspace{-5pt}
\subsection{Motivation: Truncate Weights or Activations? How to Do It Optimally?}
\label{motivation}
 \vspace{-5pt}

\begin{wraptable}{r}{0.37\linewidth} % 指定表格位置和宽度
\vspace{-10pt}
  \caption{PPL of Llama-7b on Wikitext2 after directly truncating activations and weights under same truncation setting.}
	\centering
	\resizebox{1\linewidth}{!}{
 
  \begin{tabular}{c|c|c|c|c}
		\toprule% 顶部线
		Param Ratio&1.0&0.8&0.6&0.4\\
		 \midrule		   
         Activation&5.68 &6.36&8.85&20.71\\
         \midrule
		Weight&5.68 &20061&52489&105474\\  
	   \bottomrule % 底部线
	\end{tabular}}
	\vspace{-15pt}
	\label{tab_compare_act_weight}
\end{wraptable}

Previous studies explored two main SVD-based compression methods: truncating weights or activations, including activation-aware approaches. The first and straightforward method, truncating weights, applies SVD to compress \(W\) (\ref{sec_pre_1})
% is a  approach that minimizes the difference between the original and truncated weights, expressed as \(\min \|W - \widetilde{W}\|_F \), where \(\widetilde{W}\) represents the compressed weights of applying SVD to \(W\).
Alternatively, recent works such as ASVD \citep{asvd} and SVD-LLM \citep{svdllm},  drawing from techniques in the quantization domain, emphasize an activation-aware compression, which uses scaling matrix \(S\) scales \(W\) to capture the varying importance of input channels and aims to minimize differences in activations, \textit{i.e.}, \(\min\|A - x(\widetilde{WS})S^{-1}\|_F\), where \(\widetilde{WS}\) is obtained by applying SVD to \(WS\).
\textbf{This raises a question: Are the existing truncation methods truly optimal? Our answer is no.} Both theoretical and experimental results reveal that \textbf{a fundamentally different third paradigm—directly truncating activations via SVD on \(A\) (i.e., \( xW \))—is the optimal approach.} 
% Further discussion is in \ref{sec_appn_novelPath}, \textit{Dobi-SVD’s New Perspective 1: A Novel Path from Activation to Weight.}


% Theoretically, a model's performance loss on the training set is denoted as \(\mathcal{L}\). 
% Consider a weight matrix \(W\) in this model, where input \(x\) produces activations \(A = xW\).
% The gradient of \(\mathcal{L}\) with respect to \(W\) and \(A\) is $\partial \mathcal{L}/\partial W$ and $\partial \mathcal{L}/\partial W$.
% When \(W\) and \(A\) are changed, the changes in \(\mathcal{L}\) are
% % The decrease of \(\mathcal{L}\) with respect to \(W\) and \(A\) quantify the impact of truncating each:
% \rebuttal{
% \begin{equation}
% \Delta \mathcal{L}_W = \frac{\partial \mathcal{L}}{\partial W} \Delta W= \frac{\partial \mathcal{L}}{\partial A} \left(\frac{\partial A}{\partial W}\Delta W\right) = \frac{\partial \mathcal{L}}{\partial A} (x \Delta W) \quad
% \text{and} \quad
% \Delta \mathcal{L}_A = \frac{\partial \mathcal{L}}{\partial A} \Delta A 
% \label{equ_anasis_1}
%  \vspace{-5pt}
% \end{equation}
% Upon truncating weights and activations, \(\Delta W = W - W_k\) and \(\Delta A = A - A_k\), with \(W_k\) and \(A_k\) representing the truncated matrices retaining the top \(k\) singular values of \(W\) and \(A\), respectively. 
% Substituting these into Equation \ref{equ_anasis_1}, we get:
% \begin{equation}
%     \Delta \mathcal{L}_W = \frac{\partial \mathcal{L}}{\partial A} (x(W - W_k)) = \frac{\partial \mathcal{L}}{\partial A} (A - x W_k) \quad
%     \text{and} \quad
%     \Delta \mathcal{L}_A = \frac{\partial \mathcal{L}}{\partial A} (A - A_k)
%      \vspace{-5pt}
% \label{equ_anasis_2}
% \end{equation}
% Therefore, to compare the magnitudes of $\Delta \mathcal{L}_W$ and $\Delta \mathcal{L}_A$ in Equation \ref{equ_anasis_2}, we need to assess: under the same $\partial \mathcal{L}/{\partial A}$, the performance loss from changing $A$ to $A_k$ should be smaller than that from changing $A$ to $A - xW_k$. To verify this, we conducted experiments on the Llama2-7b model with various activations $A$ and truncation $k$. For each activation $A$ and truncation $k$, we altered $A$ to $A_k$ and to $xW_k$ while keeping other layers unchanged (thus ensuring $\partial \mathcal{L}/{\partial A}$ remains constant) and observed the resulting performance loss. We uniformly sampled layer\_num = $\{5, 10, 15, 20, 25\}$ and $k$ = $\{100, 500, 1500, 2000, 2500, 3000\}$. The experimental results are shown in Figure 2, where it can be observed that for different layers and values of $k$, the performance loss caused by $A_k$ is consistently smaller than that caused by $xW_k$.
% Therefore, we conclude that $\frac{\partial \mathcal{L}}{\partial A} (A - A_k)\leq  \frac{\partial \mathcal{L}}{\partial A}(A - x W_k)$ holds for the majority of $A$ in LLMs, implying that $\Delta \mathcal{L}_A \leq \Delta \mathcal{L}_W$.
% It indicates that directly applying SVD to activation is better than truncating weights.
% }

%接下来是第一个实验：裁剪权重不好；实验部分：直接裁剪激活比之前activation-aware approach

\begin{figure}[t]
\vspace{-25pt}
\centering
	\begin{minipage}{.97\linewidth}
        %这个图片路径替换
		\centerline{\includegraphics[width=\textwidth]{FIG/Novel_Path.pdf}}
          % 加入对这列的图片说明
	\end{minipage}
        \vspace{-5pt}
         
\caption{The differences between Dobi-SVD's method and previous approaches in handling activations and obtaining new weights. See \ref{sec_appn_novelPath} for a detailed explanation of this figure.
}
\label{fig_novelPath}
\vspace{-15pt}
\end{figure}

%wakaka
At the module level, Proposition 2 shows that directly truncating activations gives \( A_k \), the optimal \( k \)-rank approximation of \( A \). At the model level, we prove (\ref{sec_appn_analysistruncation}) that directly truncating activations better minimizes training loss than truncating weights. Previous methods avoided this due to challenges in updating weights (\autoref{fig_novelPath}), while we provide a practical and effective solution through theoretical analysis (Sect.\ref{sec_q2} and \ref{sec_appn_novelPath_updateW}).
Results: \textbf{\autoref{tab_compare_act_weight} shows truncating activations outperforms truncating weights, and \autoref{tab_compare_otherSVD} confirms it surpasses existing activation-aware methods.}



% Experimentally, we first apply uniform truncation to both weights and activations and observe the resulting performance degradation. The results, shown in \autoref{tab_compare_act_weight}, demonstrate that truncating weights performs significantly worse than truncating activations. 
% Furthermore, in Sec. \ref{sec_appn_analysistruncation}, we demonstrate directly truncating activations outperforms the activation-aware approaches.


% %%% WQS begin
% To evaluate which approach is superior, we first compare them experimentally.

% To intuitively compare the impact of activation truncate and weight truncate on model performance, we truncate weights and activations under the same uniform truncation conditions and observe the resulting performance degradation. 
% The experimental results are shown in \autoref{tab_compare_act_weight}. 
% We can see that directly truncate weights leads to unacceptable performance loss. 
% % At a compression ratio of $0.8$, the model's perplexity (PPL) reaches 20061, which explains why previous work involving weight truncating typically requires fine-tuning. 
% In contrast, truncate activations results in significantly smaller performance degradation. 
% Even when the compression ratio is $0.6$, the performance remains acceptable.
% % with noticeable degradation only occurring at a compression ratio of $0.4$. 
% Based on these experimental results, we can conclude that truncating activations has a far smaller impact on model performance than truncating weights.


% Then we provide a theoretical analysis to demonstrate the superiority of truncating activations over truncating weights.
% Given a model $\mathcal{M}$, its performance loss on the training set is denoted as $\mathcal{L}$. 
% Consider a weight $W$ in the model, where the input is $x$ and the output activation is $A$. The gradients of $\mathcal{L}$ with respect to $W$ and $A$, reflecting the impact of modifying $W$ and $A$ on $\mathcal{L}$, are given by:
% \begin{equation}
% \Delta \mathcal{L}_W = \frac{\partial \mathcal{L}}{\partial W} \Delta W,\quad
% \Delta \mathcal{L}_A = \frac{\partial \mathcal{L}}{\partial A} \Delta A = \frac{\partial \mathcal{L}}{\partial W} \left(\frac{\partial A}{\partial W}\right)(\Delta W) = \frac{\partial \mathcal{L}}{\partial W} (x \Delta W)
% \label{equ_anasis_1}
% \end{equation}
% where $\Delta \mathcal{L}_W$ and $\Delta \mathcal{L}_A$ represent the impact on the model's performance due to truncating $W$ and truncating $A$, respectively. When truncating weights and activations, we have $\Delta W = W - W_k$ and $\Delta A = A - A_k$, where $W_k$ and $A_k$ are the matrices that truncate $W$ and truncate $A$ to retain $k$ singular values, respectively.. Substituting these into the Equation \ref{equ_anasis_1} , we obtain:
% \begin{equation}
%     \Delta \mathcal{L}_W = \frac{\partial \mathcal{L}}{\partial W} \left\| x(W - W_k) \right\| = \frac{\partial \mathcal{L}}{\partial W} \left\| A - x W_k \right\|, \quad
%     \Delta \mathcal{L}_A = \frac{\partial \mathcal{L}}{\partial W} \left\| A - A_k \right\|
% \end{equation}
% From Proposition 2, we can know that $A_k$ is the matrix closest to $A$ among matrices of $rank=k$,
% \begin{equation}
%     \left\| A - A_k \right\| \leq \left\| A - x W_k \right\|, \quad \Delta \mathcal{L}_A \leq \Delta \mathcal{L}_W
% \end{equation}
% Thus, we can conclude that the impact of truncating activations, $\Delta \mathcal{L}_A$, is less than or equal to the impact of truncating weights, $\Delta \mathcal{L}_W$. This demonstrates that truncating activations is more favorable for maintaining model performance compared to truncating weights.
% Motivated by this phenomenon, in this work, we focus on guiding weight updates by truncating activations.
% %%% WQS end

 \vspace{-10pt}
\section{Dobi-SVD Method}
 \vspace{-10pt}
In this section, we address the three main challenges of SVD for LLMs to implement a high-performance SVD algorithm.
 \vspace{-5pt}
\subsection{Q1: How to get the optimal truncation position?}
 \vspace{-5pt}
\label{sec_q1}
\noindent\textbf{Solution Space of Truncation Position}
To address the question, we first analyze the size of the solution space. 
Consider an LLM of $L$ layers, where each layer contains $M$ weight matrices $W \in \mathbb{R}^{m \times n}, m \geq n$. 
For each matrix, the number of possible truncation positions is $n$, $k \in \{1, 2, \dots, n\}$. 
Therefore, for the entire LLM, the size of the solution space is $n^{(L*M)}$. 
Given that LLMs typically involve large-dimensional matrices, the solution space for truncation positions is vast. 
% For example, in the Llama-7b model, where $L = 32, M = 7$, and $n = 4096$, the size of the solution space reaches an astronomical $224^{4096}$.

% Finding an optimal solution within such a large space is extremely challenging. Previous methods often rely on search-based approaches.
% For example, \citep{11,12} employed heuristic search to determine the rank of the decomposition. The heuristic search method requires significant computational time due to the large search space. We propose an end-to-end differentiable optimization method to find the optimal truncation position.

% ASVD introduced a sensitivity-based truncation ranking approach, where the truncation sensitivity of different layers is calculated by estimating the loss under specific truncation positions in advance. 
% However, search-based methods typically depend on either random or systematic exploration, which requires significant computational time to find good solutions in high-dimensional spaces. 
% ASVD, for instance, considered only ten possible truncation positions per matrix. 
% This limitation constrains the potential of SVD for compressing LLMs.



% \begin{minipage}{0.8\textwidth} % Adjust the width here (e.g., 0.8\textwidth)
\begin{algorithm}[t]
\caption{Differentiable Algorithm for Finding Optimal $k$}
\begin{algorithmic}[1]
\REQUIRE Training data $\{x_1, x_2, \ldots, x_n\}$, target compression ratio $R_{tar}$.
\ENSURE Optimal $k$ values for each weight matrix $W$.

\STATE \textbf{Step1: Smooth activation truncation Inference.}
\FOR{each activation $A$}
    \STATE Apply SVD: $A = U_A \Sigma_A V_A^\top$,  $\Sigma_A=[\sigma_1,\sigma_2,\ldots,\sigma_n]$.
    \STATE Get smooth truncation: $T(\sigma_i) = \sigma_i \left[ 0.5 \cdot \tanh\left( \beta \left( k - i \right) \right) + 0.5 \right]$, $\Sigma_k=[T(\sigma_1),\ldots,T(\sigma_n)]$
    \STATE Reconstruct activations: $\widetilde{A} = U_A \Sigma_k V_A^\top$.
    
\ENDFOR
% \STATE
\STATE \textbf{Step2: Multi-objective loss training.}
\STATE Freeze other parameters in the network and keep only $k$ trainable.
\FOR{each training step}
\STATE Compute current compression ratio $R_{now}$ based on current $k$.
\STATE Compute multi-objective loss:
    $L = L_{\text{task}} + \gamma \cdot |R_{now} - R_{tar}|$.
\STATE Update $k$ values by minimizing $L$.
\ENDFOR

\end{algorithmic}
\label{arg_diff}

\end{algorithm}
% \end{minipage}
%  \vspace{-5pt}

% \vspace{-5pt}
\noindent\textbf{Differentiable Optimization of the Truncation Position.}
To effectively identify the optimal truncation position within a huge solution space, we introduce, for the first time, a differentiable method to solve for the optimal truncation position. The workflow of our method, as shown in Algorithm \ref{arg_diff}, can be divided into two steps: smoothing discrete truncation positions and multi-objective loss training.

% \vspace{-5pt}
During inference, we construct an activation truncation model.
%, as the $k$ to be determined is actually the truncation position of each activation $A$. 
To make the truncation continuous, we use the $\tanh$ function to smooth the truncation.
Specifically, we define a smooth truncation function $T$ that
$T(\sigma_i) = \sigma_i \left[ 0.5 \cdot \tanh\left( \beta \left( k - i \right) \right) + 0.5 \right]$,
where $k$ is a learnable truncation parameter, $\beta$ is a parameter that adjusts the smoothness of the truncation and $T(\sigma_i)$ is the truncated value of $\sigma_i$.
% When $k< i$, $T(\sigma_i)\approx 0$ and when $k > i$, $T(\sigma_i) \approx \sigma_i$.
$T(\sigma_i)$ effectively simulates the truncation, making the truncated position differentiable. 
%In our experiments, we set $\beta = 10$, meaning that for $|i - k| > 0.1$, $T(\sigma_i)$ becomes identical to the truncated $\sigma_i$.
%$T(\sigma_i)$ can simulate singular value truncation accurately and is differentiable at $k=i$.


During the training, we freeze other parameters in the model and only keep $k$ trainable.
In order to ensure that the optimization balances the task performance and the model compression, we use a multi-objective loss to train the model.
Given the target compression ratio $R_{tar}$, our loss function is
$L = L_{\text{task}} + \gamma \cdot |R_{now} - R_{tar}|$,
where $L_{\text{task}}$ is the loss of the model on the downstream task, $R_{now}$ is the model compression ratio calculated by the current $k$ value.
% By setting multi-objective loss, $k$ can automatically achieve the optimal balance between task performance and model compression ratio.

% Note that our algorithm does not restrict the range of truncation position and takes into account the interactions between different matrices. 
Note that our algorithm requires very low computational cost since the optimization is only conducted on $k$ of different layers.
For example, in the Llama-7b model, there are only $224$ trainable parameters, and training requires just \textbf{8} GPU hours. 



\noindent\textbf{Stabilize SVD Backpropagation.}
Although a differentiable algorithm is theoretically feasible, we emphasize that the gradient is the devil \citep{wiki:The_devil_is_in_the_details}, especially in backpropagation involving low-rank matrices, where gradients are prone to exploding due to numerical instability.
Consider a matrix $A \in \mathbb{R}^{m \times m}$ obtained via SVD: $A = U_A \Sigma_A V_A^\top$. During backpropagation, the gradients of the loss with respect to $U_A$, $\Sigma_A$, and $V_A$ are denoted as $g_U$, $g_\Sigma$, and $g_V$, respectively. The gradient of $A$ is then given by:
% \begin{equation}
% gA = U\left( \frac{\text{skew}( U^T g_U)}{\mathbf{E}}\Sigma + \Sigma \frac{\text{skew}( V^T g_V)}{\mathbf{E}} + \text{diag}(g_\Sigma) \right)  V^T
% \label{eq_7}
% \end{equation}
\begin{equation}
gA = U\left( \frac{\text{skew}( U^T g_U)}{\mathbf{E}}\Sigma + \Sigma \frac{\text{skew}( V^T g_V)}{\mathbf{E}} + \text{diag}(g_\Sigma) \right) V^T,
E_{ij} = 
\begin{cases} 
\sigma_j^2 - \sigma_i^2 & \text{if } i \neq j, \\ 
1 & \text{if} i = j, 
\end{cases}
\label{eq_8}
\vspace{-5pt}
\end{equation}
where $\text{skew}(x)=(x-x^\top)/2$ extracts the skew-symmetric part of a square matrix, and $E_{ij} \text{ is } \mathbf{E}\text{'s } (i,j)\text{-th} \text{ element}$, and $\sigma_i$ and $\sigma_j$ being the $i$-th and $j$-th singular values of $A$.
As shown in Eq.\ref{eq_8}, when $\sigma_i$ and $\sigma_j$ are very small or close, $E_{ij}$ approaches $0$, causing $\mathbf{E}$ to vanish and $g_A$ to diverge to infinity.
% Note that as activation matrices in LLMs are approximately low-rank matrices, this gradient explosion phenomenon frequently occurs during the optimization.
This gradient explosion problem frequently arises during optimization, especially since activation matrices in LLMs often exhibit approximately low-rank structures.
% In approximately low-rank matrices, there are many singular values that are very close to zero, causing $g_A$ to approach infinity during backpropagation.
To address the above problem, we propose an approximate solution for $E_{ij}$ in two cases:
\begin{enumerate}[leftmargin=*]
    \item When $\sigma_i \approx 0$ and $\sigma_j \approx 0$, 
    %$1/ E_{ij}$ becomes very large. Since $\sigma_i$ and $\sigma_j$ are extremely small singular values, their contribution to $g_A$ should be minimal. Therefore, 
    we directly set $1/E_{ij} = \gamma$, where $\gamma$ is a small constant that ensures the contribution of $\sigma_i$ and $\sigma_j$ to $g_A$ remains small.
    \item When $\sigma_i \not\approx 0$ and $\sigma_j \not\approx 0$, but $\sigma_i \approx \sigma_j$, 
    %since $\sigma_i - \sigma_j \approx 0$, %$1/E_{ij}$ approaches infinity. In this case, 
    we perform a Taylor expansion of $1/E_{ij}$ as follows:
   \begin{equation}
    % \vspace{-5pt}
    \frac{1}{E_{ij}}= \frac{1}{\sigma_i(\sigma_i+\sigma_j)}*\frac{1}{1 - (\sigma_j / \sigma_i)} \approx \frac{1}{\sigma_i(\sigma_i+\sigma_j)}(1 + \left(\frac{\sigma_j}{\sigma_i}\right) + \cdots + \left(\frac{\sigma_j}{\sigma_i}\right)^K)
    \label{eq_9}
     % \vspace{-5pt}
   \end{equation}
    %We obtain an approximate value of $1/E_{ij}$ by retaining a finite number of terms $K$. Note that directly computing Eq. \ref{eq_9} can be time-consuming due to multiple high-order computations. 
    To accelerate the computation, we utilize the summation formula of a geometric series.
\end{enumerate}
% By addressing the above two cases, our algorithm can achieve a robust backpropagation process for low-rank matrices. 
% The schematic of our algorithm is shown in Fig.\ref{fig_stable_svd}. It can be seen that for low-rank matrices, directly computing the gradient with the original algorithm leads to explosion issues, whereas using our algorithm allows us to accurately compute approximate values of the gradient. 
% By introducing stabilized backpropagation through SVD into the differentiable algorithm, the model can more robustly find optimal truncation position.

\vspace{-5pt}
\textbf{Answer 1: By stabilizing SVD backpropagation for differentiable optimization of truncation position, we can efficiently and effectively find the optimal position of each layer.}
To the best of our knowledge, this is the first work to enable end-to-end optimization for SVD-based LLM compression.
%%% qinsi begin
% \begin{algorithm}[t]
% \caption{Computing the Theoretical Optimal Weight Matrix $\widetilde{W}$}
% \begin{algorithmic}[1]
% \REQUIRE Training data $\{x_1, x_2, \ldots, x_n\}$, original weight matrix $W$, required rank $k$.
% \ENSURE Theoretical optimal weight matrix $\widetilde{W}$.

% \STATE \textbf{Step 1: Obtain the representative $V_A$ via IPCA}
% \STATE Input the training data into the network.
% \STATE Obtain activations and their SVD decompression $\mathbf{V} = [V_1, V_2, \ldots, V_n]$.
% \STATE Initialize mean $\mu = 0$,$V_{\text{old}}=V_1$.
% \FOR{$i = 2$ to $n$}
%     \STATE Update the mean: $\mu = \mu + \frac{1}{i}(V_i - \mu)$.
%     \STATE Center the data: $V_i = V_i - \mu$.
%     \STATE Update of principal components:
%      $V_{\text{new}} = [V_{\text{old}}, V_i] = U' \Sigma V'^T$.
%      \STATE Apply SVD to $V_{\text{new}}$: $U', \Sigma_k, V'^T = \text{SVD}(V_{\text{new}})$
%     \STATE Update current matrix: $V_{\text{old}}=U' \Sigma_k V'^T$
    
% \ENDFOR
% \STATE Extract the first $k$ columns of $V'$ as representative $V_A$ of $\mathbf{V} = [V_1, V_2, \ldots, V_n]$.
% \STATE
% \STATE \textbf{Step 2: Compute the theoretical optimal $\widetilde{W}$}.
% \STATE Substitute $V_A$ into Equa.\ref{eq_5}: $\widetilde{W} = W V_A G_k V_A^\top$.

% \end{algorithmic}
% \label{arg1} 
% \end{algorithm}
%%% qinsi end


%%% jinghan begin
\begin{algorithm}[t]
\caption{Computing the Theoretical Optimal Rank-\(k\) Weight Matrix $\widetilde{W}$ via IPCA}
\begin{algorithmic}[1]
\REQUIRE Training data $\{x_1, x_2, \ldots, x_n\}$, original weight matrix $W$, required rank $k$.
\ENSURE Theoretical optimal rank-\(k\) weight matrix $\widetilde{W}$.
\STATE Input the training data into the network.
\STATE Obtain activations $[A_{1}, A_{2}, \ldots, A_{n}]$ and their right-singular vectors $[V_{A1}, V_{A2}, \ldots, V_{An}]$.
\STATE Generate \(G_k\), a diagonal matrix with the first \(k\) elements 1 and the rest 0.
% \STATE Obtain projected weight matrices \( \mathbf{V} = [V_{A1}, V_{A2}, \ldots, V_{An}]\).
\STATE Initialize mean $\mu = 0$, $\textbf{V}_{\text{old}}=V_{1}$.
\FOR{$i = 2$ to $n$}
    % \STATE Update the mean: $\mu = \mu + \frac{1}{i}(V_{i} - \mu)$.
    % \STATE Center the data: $V_{i} = V_{i} - \mu$.
    \STATE Center the data: $V_{i} = V_{i} - (\mu + \frac{1}{i}(V_{i} - \mu))$.
    \STATE Update $\textbf{V}_{\text{new}}$: 
    $\textbf{V}_{\text{new}} = [\textbf{V}_{\text{old}}, V_{i}].$
    \STATE Update principal direction $V'$: $\text{SVD}(\textbf{V}_{\text{new}}) \rightarrow U', \Sigma, V'^T$
    \STATE Update $\textbf{V}_{\text{old}}$: $\textbf{V}_{\text{old}}=V'[:k]$
    

    
    % \STATE Update principal components:
    % $\textbf{V}_{\text{new}} = [\textbf{V}_{\text{old}}, V_{i}] = U' \Sigma V'^T$.
    %  \STATE Apply SVD to $\textbf{V}_{\text{new}}$: $U', \Sigma_k, V'^T = \text{SVD}(\textbf{V}_{\text{new}})$
    % \STATE Update $\textbf{V}_{\text{new}}$: $\textbf{V}_{\text{old}}=U' \Sigma_k V'^T$
    
\ENDFOR
\STATE Use $\textbf{V}_{\text{old}}$ as the final \textbf{V}, update weight $\widetilde{W}=W\textbf{V}G_k\textbf{V}^T$.

\end{algorithmic}
\label{algo_update_W} 
\end{algorithm}
%%% jinghan end
 \vspace{-5pt}
\subsection{Q2: How to update weights optimally?}
\label{sec_q2}
 \vspace{-5pt}
%%% keke version begin
% \jinghan{jinghan version begin}

In Sect. \ref{sec_q1}, we obtained effective activation truncation position. In this section, we address the second challenge: How to optimally update weights based on the truncation position?

% First, what does it mean to truncate activations directly:
Leveraging Proposition 3, applying SVD to directly truncate activations at \(k\) can be formulated as:
\begin{equation}
 \vspace{-5pt}
A_k=U_A\Sigma_AG_kV_A^\top=U_A\Sigma_AIG_kV_A^\top = U_A \Sigma_A V_A^\top V_A G_k V_A^\top = A V_A G_k V_A^\top,
\label{eq_keke_1}
 % \vspace{-5pt}
\end{equation}
where \(G_k\) is a diagonal matrix with the first \(k\) elements as 1 and the rest as 0. Substituting \(A = xW\),
\begin{equation}
 \vspace{-5pt}
A_k = A V_A G_k V_A^\top = x W V_A G_k V_A^\top.
\label{eq_keke_2}
 \vspace{-1pt}
\end{equation}
At truncation position \(k\), for a set of inputs \( \{x_i\}_{i=1}^n \), the ideal updated weight \( \widetilde{W} \) should satisfy:
\begin{equation}
 \vspace{-5pt}
\min\sum_{i=1}^n \|x_{i}WV_{A_{i}}G_{k}V_{A_{i}}^{T} - x_{i}\widetilde{W}\|_F = \min\sum_{i=1}^n x_{i}\|WV_{A_{i}}G_{k}V_{A_{i}}^{T} - \widetilde{W}\|_F, \quad \text{rank}(\widetilde{W}) = k
\label{eq_keke_3}
 % \vspace{-5pt}
\end{equation}
Thus, our goal is to find the rank-\(k\) matrix \( \widetilde{W} \) closest to the set of projected weight matrices \( \mathbf{W^p} = \{WV_{A_i}G_kV_{A_i}^\top\}_{i=1}^n \).
Since \(x\) is usually not a full-rank square matrix, computing \(\widetilde{W}\) via its inverse is infeasible.
Therefore, we derive \(\widetilde{W}\) from the underlying structure of \( \mathbf{W^p} \).

% EYM定理表示：SVD能得到秩为k下离原矩阵最近的矩阵，基于SVD构建、由EYM定理支撑的PCA能将高维数据投影到较低维度的空间中，同时保留尽可能多的信息，能得到秩为k下离矩阵群\( \mathbf{W^p}\)最近的矩阵, 所以我们对W集群进行PCA
%但是内存占比太大，我们使用IPCA，具体算法见algo表
%Proposition 2 establishes that SVD provides the optimal rank-\(k\) approximation for a matrix. PCA, built on SVD and grounded in this theory, projects data into lower-dimensional subspaces while retaining maximum information, allowing for the optimal computation of \( \widetilde{W} \) closest to \( \mathbf{W^p} \).


While PCA can compute \( \widetilde{W} \), the large size of \( \mathbf{W^p} \) leads to excessive memory demands. Traditional PCA scales exponentially with matrix size, making large \(n\) impractical and requiring hundreds of Gigabytes. 
To address this, we make a novel use of the Incremental Principal Components Analysis (IPCA) algorithm for memory-efficient PCA.
The algorithm, detailed in Algorithm \ref{algo_update_W}, centralizes each new input \(W^p_i\) from \( \mathbf{W^p} \) and updates the principal components via incremental SVD. 
By processing the matrices sequentially, the need to input all \(n\) matrices at once is replaced with multiple steps, each handling only two matrices at a time, significantly reducing memory usage.

\textbf{Answer 2: By using IPCA, we can calculate the theoretically optimal Rank-\(k\) Weight Matrix $\widetilde{W}$.}

\vspace{-5pt}
\subsection{How to overcome the long-overlooked truncation limitation?}
 \vspace{-5pt}
\label{sec_q3}
In this section, we address a long-overlooked limitation of SVD. As outlined in Section \ref{sec_pre}, for an \( m \times n \) matrix \( W \) with truncation position \( k \), the compression ratio is \( r = k(m+n)/(m \cdot n) \). Setting \( r = 1 \) gives \( k = (m \cdot n) / (m+n) < \min(m,n) = \text{rank}(W) \), meaning the model size stays the same, but information is lost. Notably, when \( m = n \), \( k = \text{rank}(W)/2 \), truncating half the singular values. This demonstrates that in traditional SVD-based compression, the compression ratio is injective with respect to truncation position, often leading to significant performance degradation.

\textbf{To address this limitation, we propose remapping the relationship between compression ratios and truncation positions to establish a bijection}, where for \( r \in [0, 1] \), \( k \) ranges from 0 to \( \text{rank}(W) \) with a one-to-one correspondence. This gives the compression ratio \( r = k / \text{rank}(W) = k / \min(m,n) = k \cdot \max(m,n) / (m \cdot n) \), with the compressed matrix size being \( k \cdot \max(m,n) \).

To achieve this bijection, we propose a new SVD compression method:
Step 1. For the updated matrix \( \widetilde{W} \) with rank \( k \), perform SVD to obtain \( U_{\widetilde{W}}, \Sigma_{\widetilde{W}}, V_{\widetilde{W}}^T \). Extract the first \( k \) columns of \( U_{\widetilde{W}}\Sigma_{\widetilde{W}} \) as an \( m \times k \) matrix \( U_{\widetilde{W}, k}\Sigma_{\widetilde{W}, k} \), and the first \( k \) rows of \( V_{\widetilde{W}}^T \) as a \( k \times n \) matrix \( V_{\widetilde{W}, k}^T \).
Step 2. Assuming \( m \geq n \), the storage space becomes \( m \times k \). Take the first \( n \) rows of \( U_{\widetilde{W}, k}\Sigma_{\widetilde{W}, k} \) and \( V_{\widetilde{W}, k}^T \) (both \( n \times k \)), halve their bit precision, concatenate them, and replace the first \( n \) rows of \( U_{\widetilde{W}, k}\Sigma_{\widetilde{W}, k} \). Finally, store only the modified \( U_{\widetilde{W}, k}\Sigma_{\widetilde{W}, k} \), using \( m \times k \) space.

The compression method in Step 2 leverages the orthogonality of the left and right singular vector matrices \( U \) and \( V \), whose column vectors follow a normal distribution (as shown in the 'remapping' section of Fig. \ref{fig_overview}), making them ideal for uniform quantization methods like QLoRA. Appendix \ref{sec_appn_addAnalysis} shows that quantization introduces minimal error.
%\autoref{tab_remap} demonstrates that quantization has negligible impact on model performance while greatly improving the efficiency of SVD compression for LLMs, with \autoref{tab_quant_inter} confirming the re-quantization adaptability of the remapped model.


\textbf{Answer3: By applying our quantized storage method, the compression ratios and truncation position form a bijective mapping, overcoming the truncation position limitation.}

% \jinghan{Jinghan version end}
\vspace{-5pt}
\subsection{Conclusion: Two New Perspectives of Dobi-SVD}
\vspace{-5pt}
By leveraging the fundamental theorem of SVD, we derive the optimal approach for SVD-based compression: directly truncating activations (Section~\ref{motivation}). To determine the optimal truncation point of the activation matrix, we develop a robust and efficient differentiable SVD algorithm applicable to general matrices (Sections~\ref{sec_q1} and \ref{sec_appn_diffSVD}). Subsequently, we propose a theoretical framework to derive a new weight matrix from the truncated activations (\ref{sec_appn_novelPath_updateW} and Section~\ref{sec_q2}), successfully implementing this using Incremental PCA (IPCA). Collectively, these contributions form \textbf{Dobi-SVD's New Perspective 1: A Novel Path from Activation to Weight} (\ref{sec_appn_novelPath}).

Furthermore, we highlight a long-overlooked limitation of traditional SVD-based compression methods and, for the first time, propose an effective solution. This leads to \textbf{Dobi-SVD's New Perspective 2: Fully Unlocking SVD's Potential for Data Compression by Addressing A Long-Overlooked Limitation} (Sections~\ref{sec_q3} and \ref{sec_appn_longOverlook}).



\begin{table}[t]
\vspace{-18pt}
\caption{ Dobi-SVD vs. SOTA methods in terms of compression performance of LLaMA-7b on three language modeling datasets (in-domain evaluation) and seven common sense reasoning datasets (zero-shot evaluation). The best performance is marked in bold.  Drop means relative performance drop to baseline. Dobi-SVD* refers to the result obtained without remapping. The performance of the ASVD and SVD-LLM is derived from the results reported in SVD-LLM. {$\dag$} uses LoRA fine-tuning.}
	\centering
	\resizebox{\textwidth}{!}{
	\begin{tabular}{c|c|c c c|c c c c c c c c c}
		\toprule[1.5pt]% 顶部线
        \multirow{2}{*}{\textbf{Ratio}}&\multirow{2}{*}{\textbf{Method}}&\multicolumn{3}{c|}{\textbf{PPL ($\downarrow$)}}&\multicolumn{7}{c}{\textbf{Accuracy ($\uparrow$)}}&\textbf{Avg.}&\textbf{Drop}\\
        
		~ & ~ & Wiki2   & PTB  & C4   & Openb. & ARC$\_$e  & ARC$\_$c & WinoG. & HellaS.  & PIQA   & MathQA  & ($\uparrow$) & ($\downarrow$)  \\ 
		\midrule
        
		1.0&Baseline& 5.68 & 8.35 & 7.34 &0.28&0.67&0.38&0.67&0.56&0.78&0.27&0.52& 0$\%$\\
        
        \midrule
        \midrule
        \multirow{5}{*}{0.8}
        &ASVD& 11.14 & 16.55 & 15.93 &0.25&0.53&0.27&0.64&0.41&0.68&0.24&0.43& 17.3$\%$\\
        
        ~&SVD-LLM$^{\dag}$& 7.94 & 16.22 & 15.84 &0.22&0.58&0.29&0.63&0.43&0.69&0.24&0.44& 15.4$\%$ \\
        \cdashline{2-14}[2pt/2pt]
		\rule{0pt}{10pt}
        ~& \textbf{Dobi-SVD$*$}& 8.54 & \textbf{14.83} & 10.01 &0.26&0.59&0.31&0.66&0.44&0.70&0.23&0.46& 11.5$\%$\\
        \hc & \textbf{Dobi-SVD}& \textbf{6.08} & 15.39 & \textbf{7.83}&\textbf{0.27}&\textbf{0.65}&\textbf{0.37}&\textbf{0.68}&\textbf{0.54}&\textbf{0.77}&\textbf{0.27}&\textbf{0.50}& \textbf{3.84}$\%$\\
        \midrule
  
        \multirow{5}{*}{0.6}
        &ASVD& 1407 & 3292 & 1109 &0.13&0.28&0.22&0.48&0.26&0.55&0.19&0.30& 42.3$\%$\\
        
        ~&SVD-LLM$^{\dag}$& 13.11 & 63.75 & 49.83 &0.19&0.42&0.25&0.58&0.33&0.60&0.21&0.37& 28.8$\%$ \\
        \cdashline{2-14}[2pt/2pt]
		\rule{0pt}{10pt}
         ~& \textbf{Dobi-SVD*}& 13.54 & 46.38 & 23.54 &0.22&0.41&0.27&0.58&0.34&0.61&0.23&0.38& 26.9$\%$\\
        \hc & \textbf{Dobi-SVD}& \textbf{8.12} & \textbf{43.85} & \textbf{12.63} &\textbf{0.28}&\textbf{0.65}&\textbf{0.32}&\textbf{0.62}&\textbf{0.45}&\textbf{0.72}&\textbf{0.25}&\textbf{0.47}& \textbf{9.61}$\%$ \\
        \midrule
  
        \multirow{5}{*}{0.4}
        &ASVD& 57057 & 45218 & 43036 &0.12&0.26&0.21&0.49&0.26&0.53&0.18&0.29& 44.2$\%$\\
        ~&SVD-LLM$^{\dag}$& 53.74 & 438.58 & 345.49 &0.14&0.28&0.22&0.50&0.27&0.55&0.21&0.31& 40.3$\%$ \\
        \cdashline{2-14}[2pt/2pt]
		\rule{0pt}{10pt}
         ~& \textbf{Dobi-SVD*}& 46.18 & 238.91 & 190.62 &0.15&0.31&0.20&0.52&0.28&0.54&0.22&0.32& 38.4$\%$\\
        \hc & \textbf{Dobi-SVD}& \textbf{9.95} & \textbf{67.62} & \textbf{17.94} &\textbf{0.23}&\textbf{0.52}&\textbf{0.24}&\textbf{0.56}&\textbf{0.38}&\textbf{0.65}&\textbf{0.23}&\textbf{0.40}& \textbf{23.1}$\%$\\
		\bottomrule[1.5pt] % 底部线
	\end{tabular}}
	\label{tab_compare_otherSVD} 
 \vspace{-13pt}
\end{table} 



\begin{table}[t]
% \vspace{-20pt}
\caption{Dobi-SVD vs. popular pruning methods in terms of compression performance of LLaMA-7b on seven common sense reasoning datasets. The best performance is marked in bold. The performance of the pruning methods are derived from their original paper.}
	\centering
       \vspace{-10pt}
	\resizebox{\textwidth}{!}{
	\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
		\toprule[1.5pt]% 顶部线
  \multirow{2}{*}{\textbf{Ratio}}&\multirow{2}{*}{\textbf{Method}}&\multicolumn{2}{c|}{\textbf{Cost}}& \multicolumn{6}{c|}{\textbf{Accuracy ($\uparrow$)}}&\textbf{Avg.}&\textbf{Drop}\\
		~ & ~&Post-train&Fine-tuning& BoolQ & PIQA & WinoG. & ARC$\_$e & ARC$\_$c & OBQA  & ($\uparrow$) & ($\downarrow$)  \\ 
		\midrule
       
		1.0&Baseline&-&-&0.73&0.78&0.67&0.67&0.41&0.42&0.61&0$\%$\\
        \midrule
        \midrule
        \multirow{4}{*}{$0.8$}&LLM-Pruner&\checkmark&\ding{55}&0.59&0.75&0.61&0.59&0.37&0.39&0.55&$ 9.83\%$\\
        ~&LLM-Pruner(w/LoRA)&\checkmark&$\checkmark$&0.69&0.76&0.65&0.63&0.37&0.40&0.58&$ 4.92\%$\\
        ~&FLAP&\checkmark&\ding{55}&0.69&0.76&0.68&\textbf{0.69}&\textbf{0.39}&0.39&0.60&$ 1.64\%$\\
        \cdashline{2-12}[2pt/2pt]
        % \cmidrule(lr){2-12}
		% \rule{0pt}{10pt}
        \hc& \textbf{Dobi-SVD}& \checkmark&\ding{55}&\textbf{0.73}&\textbf{0.77}&\textbf{0.68}&0.65&0.37&\textbf{0.42}&\textbf{0.61}&\textbf{0}$\%$\\
         
		\bottomrule[1.5pt] % 底部线
	\end{tabular}}
	\label{tab_prun_compare}
 % \vspace{-13pt}
\end{table} 








% \jinghan{qinsi version(original) begin}

% In this section, we address Problem 3, a limitation in SVD for LLMs that has long been overlooked: the restriction of truncation position.

% \noindent\textbf{Truncation Limitation in SVD for LLMs.} As described in Section \ref{sec_pre}, for a given $ m \times n $ matrix $ W $ and truncation position $ k $, the compression ratio of the matrix is given by $k(m+n)/(m \cdot n)$. To ensure that the compression ratio is less than 1, $ k $ must satisfy the condition $ k < (m \cdot n) / (m+n)$. This implies that the value of $ k $ is highly restricted when using SVD for compression. For instance, when $ m = n $, $ k < m/2 $ is required to achieve compression, meaning that at least half of the singular values must be truncated. This presents a significant limitation of SVD in LLM compression, often leading to greater performance degradation compared to other compression methods, and making SVD unsuitable for large-scale model compression.

% \noindent\textbf{Ideal Bijection truncation position and Compression Ratios.} To overcome the truncation limitation in SVD for LLMs, we first explore the ideal mapping between truncation positions and model compression ratios. For a given $ m \times n $ matrix $ W $ where $ m \geq n $, the possible range for $ k $ is $ 0 < k \leq n $. Ideally, the relationship between truncation position and compression ratio should be bijective, meaning that every possible truncation position corresponds to an effective compression ratio (less than 1). This implies that when the truncation position is $ k $, the model's compression ratio should be $k / n $.

% \noindent\textbf{Quantized Storage for Optimal Mapping.} To achieve the ideal bijective, we propose a new SVD compression storage method. Our quantization storage algorithm is shown in Fig. \ref{fig_quan_overview}, and consists of the following three steps:
% Step 1: For the updated $ \widetilde{W} $ with rank $ k $, we decompose it using SVD to obtain $ U_W, \Sigma_W, $ and $ V_W^T $, then take the first $ k $ columns of $ U_W $ and the first $ k $ rows of $ V_W^T $ to form $ U_{\widetilde{W}} \in \mathbb{R}^{m \times k} $ and $ V_{\widetilde{W}}^T \in \mathbb{R}^{k \times n} $.
% Step 2: Quantize the first $ k $ elements of each column in $ U_{\widetilde{W}} $ and each row in $ V_{\widetilde{W}}^T $ to 8 bits.
% Step 3: Construct a new 16-bit matrix of size $ m \times k $.

% In particular, the compression method in Step 2 leverages the property that the decomposed $ U $ and $ V $ matrices follow a normal distribution (as shown in Fig. \ref{fig_overview}), making these matrices well suited for quantization techniques designed for normally distributed weights, such as QLoRA. In Appendix \ref{sec_appn_addAnalysis}, our experiments demonstrate that quantization introduces only minimal error. Furthermore, the experimental results in Section \ref{sec54} validate that the quantization error has a negligible effect on the model performance while significantly improving the efficiency of SVD compression for LLMs.
% % Our method overcomes the truncation position limitation in SVD for LLM compression, allowing SVD to reach its full potential in model compression.

% % \textbf{Answer3: By applying our quantized storage method, the model's compression ratio can be mapped one-to-one with the truncation position, overcome the truncation position limitation.} 

% \textbf{By applying our quantized storage method, the matrix can have a larger truncation position $k$, thus achieving bijective mapping and overcoming the truncation position limitation.}

% \jinghan{qinsi version(original) end}



























% \subsection{Challenges of Compressing LLMs via SVD}
% Currently, there are two main challenges in SVD for LLMs compression.
% \noindent\textbf{How to obtain a low-rank $W$?} Vanilla SVD (\citep{8,9,10}) focuses solely on compressing the original weight matrix, with the compression objective being minimize $\|W - \widetilde{W}\|_F$. 
% Since vanilla SVD does not consider the importance of parameters, it often results in significant performance loss. 
% Therefore, fine-tuning is required to restore performance, which can be computationally expensive for LLMs.
% In order to address this issue, recent works focus on activation-aware SVD, which targets minimize $\|A - \widetilde{A}\|_F$, where $\widetilde{A}=x\widetilde{W}$. 
% For example, ASVD (\citep{asvd}) suggests that the activation distribution affects compression error and introduces a scaling diagonal matrix $ S $ to scale $ \Sigma_W $, where $ S $ represents the influence of input channels on the weights. SVD-LLM (\citep{svdllm}), on the other hand, argues that not all larger singular values in $ \Sigma_W $ are necessarily worth preserving. It proposes a truncation-aware data whitening strategy to identify which singular values are crucial for reducing compression loss.

% However, current activation-aware SVD methods are limited to modifying $ \Sigma_W $ to adjust $ W $. This restriction significantly limits the possible values of $ \widetilde{W} $ and does not guarantee that the modified weights will effectively preserve the activation information. For instance, ASVD only shows effectiveness at low compression ratios, suffering severe performance loss at higher compression ratios.


% \noindent\textbf{How to determine the $k$ for each $W$?} Another challenge in applying SVD to LLMs is determining the truncation $ k $ value for each layer. Current algorithms are mainly based on search (\citep{13,14,12}). 
% In particular, \citep{11,12} employ heuristic search to determine the decomposition rank. 
% ASVD (\citep{asvd}) proposes a sensitivity-based truncation ranking search by pre-computing the loss for each layer under a specific truncation. 

% However, search-based methods generally rely on random or systematic exploration, which is time-consuming in high-dimensional spaces to obtain optimal solutions. For example, to accelerate the search process, ASVD can only considers a very limited set of truncation options, which restricts the potential of SVD in LLM compression.




% \subsection{Motivation}



% \section{Calculation of the theoretical optimal weight}
% \label{sec_3}
% In this section, we focus on Challenge 1: How to get a low-rank weight? To solve the challenge, we first derive the expression of the theoretically optimal weight (Section \ref{sec_31}). Then we get representative inputs through feature extraction, and introduce IPCA to solve the memory explosion problem of SVD of large matrices (Section \ref{sec_32}).


% \subsection{Theoretical optimal expression of weight}
% \label{sec_31}

% \noindent\textbf{Basic Propositions.} Before the derivation of $\widetilde{W}$, we first introduce the three basic propositions:

% \textit{Proposition 1:Rank Property of Matrix Multiplication.} 
% For the matrix multiplication $AB = C$, the rank of $C$ satisfies $rank(C) \leq \min(rank(A), rank(B))$.


% \textit{Proposition2:Eckart-Young-Mirsky Theorem.}
% For a matrix $A \in \mathbb{R}^{m \times n}$, the best rank-$k$ approximation of $A$ in terms of the Frobenius norm or spectral norm is given by the matrix $A_k = U_A \Sigma_k V_A^T$.

% \textit{Proposition3:Positive Definiteness of Matrices in SVD.}
% Given the SVD of a matrix $A = U_A \Sigma_A V_A^\top$, the matrices $U_A$ and $V_A$ are both orthogonal, satisfying $V_A^\top V_A = U_A^\top U_A = I$.



% \noindent\textbf{Theoretical derivation of optimal $\widetilde{W}$.} 
% Given a weight matrix $W \in \mathbb{R}^{m \times n}$ and input $x \in \mathbb{R}^{l \times m}$, the activation can be expressed as $A=xW$, where $A \in \mathbb{R}^{l \times n}$. The target of Activation-aware SVD is to find a $\widetilde{W}$ with $rank(\widetilde{W})=k$ that makes activation $\widetilde{A}=x\widetilde{W}$ close to $A$, i.e,
% \begin{equation}
% \widetilde{W} = \argmin_{w, \; \text{rank}(w) = k} \|xw - A\|_F,
% \label{eq_1}
% \end{equation}
% where $k$ is the rank of $\widetilde{W}$ that needs to be decomposed.
% From Proposition 1, we know that when $rank(\widetilde{W})=k$, $rank(\widetilde{A})\leq k$. 
% Therefore, the problem becomes to find the $\widetilde{A}$ closest to $A$ and with rank no more than $k$. 
% Furthermore, from proposition 2, we can know that the optimal $\widetilde{A}$ is $A_k$. Therefore, the theoretically optimal $\widetilde{W}$ should satisfy:
% \begin{equation}
% x\widetilde{W}=A_k=U_A\Sigma_kV_A^\top=U_A\Sigma_AG_kV_A^\top,
% \label{eq_2}
% \end{equation}
% where $G_k$ is a truncated diagonal matrix with the first $k$ diagonal elements set to 1 and the remaining elements set to 0.
% Since $x$ is usually not a full-rank square matrix, $\widetilde{W}$ has multiple possible solutions, and we cannot obtain $\widetilde{W}$ by directly computing the inverse of $x$. 

% To solve this problem, here we further transform $A_k$ and try to derive an expression for $\widetilde{W}$ that satisfies Equa.\ref{eq_2}. Combine Proposition 3, Equa\ref{eq_2} can be expressed as
% \begin{equation}
% A_k = U_A\Sigma_AIG_kV_A^\top = U_A \Sigma_A V_A^\top V_A G_k V_A^\top = A V_A G_k V_A^\top.
% \label{eq_3}
% \end{equation}
% Further expand $A$ and $A_k$, Equa.\ref{eq_3} can be expressed as:
% \begin{equation}
% A_k = A V_A G_k V_A^\top = x W V_A G_k V_A^\top = x \widetilde{W}.
% \label{eq_4}
% \end{equation}
% Therefore, we can obtain a theoretical optimal expression of $\widetilde{W}$ that satisfies Equa.\ref{eq_2} is:
% \begin{equation}
% \widetilde{W} = W V_A G_k V_A^\top.
% \label{eq_5}
% \end{equation}
% Equa.\ref{eq_5} is the theoretical optimal expression of $\widetilde{W}$, which can make $\widetilde{A}$ closest to $A$.

% Note that $V_A$ is the matrix of right singular vectors obtained from the SVD of $A$, which is equivalent to the eigenvectors obtained from the PCA decomposition of $A$. Therefore, $V_A G V_A^\top$ can be considered an orthogonal projection matrix that preserves the directions of the first $k$ principal components of $A$. In this context, $\widetilde{W}$ is the low-rank mapping of $W$ onto the $V_A G V_A^\top$ space. Through this mapping, $\widetilde{W}$ maximally preserves the main information in $A$.


% % \begin{wrapfigure}{r}{0.5\textwidth}
% % \vspace{-25pt}
% %   \begin{minipage}{0.5\textwidth}
% %     \begin{algorithm}[H]
% %        \footnotesize
% % \caption{Computation of Theoretical Optimal Weight Matrix $\widetilde{W}$ Using IPCA}

% % \textbf{Input:} Training data $\{x_1, x_2, \ldots, x_n\}$, original weight matrix $W$, required rank $k$.\\

% % \textbf{Step 1: Obtain the representative $V_A$ via IPCA.}\\
% % Input the training data into the network.\\
% % Obtain activations $V = [V_1, V_2, \ldots, V_n]$.\\
% % Initialize mean $\mu = 0$.\\
% % \For{i=1,2,...,n}{

% %      \textbf{Update Mean}: $\mu = \mu + \frac{1}{i}(V_i - \mu)$.\\
% %       \textbf{Data Centering}: $V_i = V_i - \mu$.\\
%     %  \textbf{Incremental update of principal components}\\
%     %  $V = \begin{bmatrix} U_{\text{old}} S_{\text{old}} V_{\text{old}}^T \\  V_i \end{bmatrix} = U_{\text{new}} S_{\text{new}} V_{\text{new}}^T
%     % $\\
% % }
% % Extract the first $k$ rows from $V_{\text{new}}$ to form $V_A$.\\

% % \textbf{Step 2: Compute the theoretical optimal $\widetilde{W}$}\\
% % Substitute $V_A$ into Equa.\ref{eq_5}: $\widetilde{W} = W V_A G_k V_A^\top$.

% % \label{arg1} 
% %     \end{algorithm}
% % \vspace{-40pt}
% %   \end{minipage}
% % \end{wrapfigure}



% \begin{algorithm}[t]
% \caption{Computing the Theoretical Optimal Weight Matrix $\widetilde{W}$}
% \begin{algorithmic}[1]
% \REQUIRE Training data $\{x_1, x_2, \ldots, x_n\}$, original weight matrix $W$, required rank $k$.
% \ENSURE Theoretical optimal weight matrix $\widetilde{W}$.

% \STATE \textbf{Step 1: Obtain the representative $V_A$ via IPCA}
% \STATE Input the training data into the network.
% \STATE Obtain activations and their SVD decompression $\mathbf{V} = [V_1, V_2, \ldots, V_n]$.
% \STATE Initialize mean $\mu = 0$,$V_{\text{old}}=V_1$.
% \FOR{$i = 2$ to $n$}
%     \STATE Update the mean: $\mu = \mu + \frac{1}{i}(V_i - \mu)$.
%     \STATE Center the data: $V_i = V_i - \mu$.
%     \STATE Update of principal components:
%      $V_{\text{new}} = [V_{\text{old}}, V_i] = U' \Sigma V'^T$.
%     \STATE Update current matrix: $V_{\text{old}}=U' \Sigma_k V'^T$
    
% \ENDFOR
% \STATE Extract the first $k$ columns of $V'$ as representative $V_A$ of $\mathbf{V} = [V_1, V_2, \ldots, V_n]$.
% \STATE
% \STATE \textbf{Step 2: Compute the theoretical optimal $\widetilde{W}$}.
% \STATE Substitute $V_A$ into Equa.\ref{eq_5}: $\widetilde{W} = W V_A G_k V_A^\top$.

% \end{algorithmic}
% \label{arg1} 
% \end{algorithm}



% \subsection{Representative Input Calculations}
% \label{sec_32}

% Now we have obtained the theoretical optimal expression for $\widetilde{W}$ as shown in Equa.\ref{eq_5}, where $W$ is the original weight matrix, and $G$ is a diagonal matrix with the first $k$ diagonal elements being 1 and the rest being 0. Therefore, the only thing we need to determine is the positive definite matrix $V_A$.

% \textbf{Feature extraction to obtain $V_A$.} For a set of input data $[x_1, x_2, \ldots, x_n]$, we can obtain the corresponding activations and decomposition $\mathbf{V} = [V_1, V_2, V_3, \ldots, V_n]$. To enable $V_A$ to capture the main information in $\mathbf{V}$, we perform PCA on $\mathbf{V}$ and extract the first $k$ dimensions to obtain $V_A$.
% Note that, since in PCA, the first $k$ principal components of matrix $A$ are actually equivalent to the first $k$ columns of $V_A$ after performing SVD on $A$. Therefore, in the process of PCA, we actually perform SVD on $\mathbf{V}$ and take the first $k$ columns of $V_{\mathbf{V}}$. Through this method, we avoid the bias caused by outliers in the input, enabling us to better capture the main information in the input.

% % \begin{wrapfigure}{r}{0.4\linewidth} % 指定表格位置和宽度
% % \vspace{-15pt}
% % \centering
% % \includegraphics[width=\linewidth]{FIG/IPCA_memory.png}
% % \caption{Comparison of memory requirements for PCA and IPCA for $n*n$ matrix. The horizontal axis is $n$. PCA can greatly reduce memory footprint.}
% % \label{fig_ipca_memory}
% % \vspace{-15pt}
% % \end{wrapfigure}

% \noindent\textbf{IPCA to solve memory explosion.} Here, a challenging  is that the size of $\mathbf{V}$ is too large, leading to an explosive memory requirement. As shown in Fig.\ref{fig_ipca_memory}, the memory required for traditional PCA increases exponentially with the size of the matrix. When the number of input data is large ($n$ is large), traditional PCA requires hundreds of gigabytes of memory, which is unrealistic.

% Therefore, to perform PCA decomposition within limited memory, we introduce the Instrumented Principal Components Analysis (IPCA) algorithm. The algorithm flow is shown in Algorithm \ref{arg1}. Specifically, for newly input data $V_{i}$, IPCA first centers the data $V_i$. Then, IPCA obtains the principal components through incremental SVD. At this point, the object to be decomposed changes from the original $\mathbf{V} = [V_1, V_2, V_3, \ldots, V_n]$ to $\mathbf{V} = [V_{\text{old}}, V_{i}]$, thereby greatly reducing the memory required for decomposition. By introducing IPCA, we can gradually obtain the principal components without storing the complete matrix.
% The memory required after using IPCA is shown in Fig.\ref{fig_ipca_memory}. It can be seen that the introduction of IPCA greatly reduces the required memory, and the required memory does not increase with the addition of input data.

% The computational process for the theoretically optimal $W$ is shown in Algorithm \ref{arg1}. After obtaining the representative $V_A$, we substitute $V_A$ into Equa.\ref{eq_5} to obtain the theoretically optimal $\widetilde{W}$.





% \begin{algorithm}[t]
% \caption{Differentiable Algorithm for Finding Optimal $k$}
% \begin{algorithmic}[1]
% \REQUIRE Training data $\{x_1, x_2, \ldots, x_n\}$, target compression ratio $R_{tar}$.
% \ENSURE Optimal $k$ values for each weight matrix $W$.

% \STATE \textbf{Step1: Smooth activation truncation Inference.}
% \FOR{each activation $A$}
%     \STATE Apply SVD: $A = U_A \Sigma_A V_A^\top$,  $\Sigma_A=[\sigma_1,\sigma_2,\ldots,\sigma_n]$.
%     \STATE Get smooth truncation: $T(\sigma_i) = \sigma_i \left[ 0.5 \cdot \tanh\left( \beta \left( k - i \right) \right) + 0.5 \right]$, $\Sigma_k=[T(\sigma_1),\ldots,T(\sigma_n)]$
%     \STATE Reconstruct activations: $\widetilde{A} = U_A \Sigma_k V_A^\top$.
    
% \ENDFOR
% \STATE
% \STATE \textbf{Step2: Multi-objective loss training.}
% \STATE Freeze other parameters in the network and keep only $k$ trainable.
% \FOR{each training step}
% \STATE Compute current compression ratio $R_{now}$ based on current $k$.
% \STATE Compute multi-objective loss:
%     $L = L_{\text{task}} + \gamma \cdot |R_{now} - R_{tar}|$.
% \STATE Update $k$ values by minimizing $L$.
% \ENDFOR

% \end{algorithmic}
% \label{arg2}
% \end{algorithm}


% \section{Finding the Optimal $k$ for Each Weight}

% In Section \ref{sec_3}, we determined the update formula for $\widetilde{W}$. In this section, we focus on addressing Challenge 2: how to determine the value of $k$ for each layer? First, we smooth the gradient to make the truncated $k$ differentiable (Section \ref{sec41}). Then, to resolve the problem of infinite gradients in the SVD of low-rank matrices, we use Taylor expansion and transform the gradient derivation formula of SVD to make the backpropagation of SVD stable (Section \ref{sec42}).

% \subsection{A Differentiable Algorithm for Finding $k$}
% \label{sec41}
% Another challenge in SVD for LLMs is how to determine the value of $k$. Previous methods use a search-based approach to iteratively search for $k$ for each matrix. However, search methods suffer from high computational cost and difficulty in finding the global optimal solution. Therefore, we introduce, for the first time, a differentiable algorithm to find the $k$ value for each $W$, enabling the model to automatically find the optimal $k$ for each $W$ via a differentiable approach.
% Our differentiable algorithm flow is shown in Algorithm \ref{arg2}, which consists of the following two steps.

% \noindent\textbf{Smooth activation truncation Inference.} 
% During the inference, we construct an activation truncation model, as the $k$ to be determined is actually the truncation position of each activation $A$. 
% To make the truncation continuous, we use the $\tanh$ function to smooth the truncation.
% Specifically, we define a smooth truncation function $T$ that
% \begin{equation} 
% T(\sigma_i) = \sigma_i \left[ 0.5 \cdot \tanh\left( \beta \left( k - i \right) \right) + 0.5 \right].
% \end{equation}
% where $k$ is a learnable truncation parameter, $\beta$ is a parameter adjusting the smoothness of the truncation, and $T(\sigma_i)$ is the truncated value of $\sigma_i$.
% When $k< i$, $T(\sigma_i)\approx 0$ and when $k > i$, $T(\sigma_i) \approx \sigma_i$.
% $T(\sigma_i)$ can simulate singular value truncation well and is differentiable at $k=i$.

% \noindent\textbf{Multi-objective loss training.} During the training process, we freeze other parameters in the model and only keep $k$ trainable.
% In order to ensure that the model has both good performance and good compression ratio, we use a multi-objective loss to train the model.
% Given the target compression ratio $R_{tar}$, our loss function is
% \begin{equation}
% L = L_{\text{task}} + \gamma \cdot |R_{now} - R_{tar}|.
% \label{eq_6}
% \end{equation}
% where $L_{\text{task}}$ is the loss of the model on the task, $R_{now}$ is the model compression ratio calculated by the current $k$ value.
% By setting multi-objective loss, $k$ can automatically achieve the optimal balance between task performance and model compression ratio.

% Note that although LLMs have a large number of parameters, the number of trainable $k$ values is very limited. For a weight matrix $W$, we have only one parameter to train, which means we require only a very limited training cost. For example, for the Llama2-7b model, there are only 224 trainable parameters, and we need only 8 GPU hours of training to achieve convergence.

% Our differentiable algorithm does not limit the range of $k$ values and considers the influence between different matrices. This breaks the limitations of previous work. By using a differentiable approach, we directly guide the selection of $k$ based on the model's performance, allowing $k$ to better find the value that optimizes model performance under the compression ratio constraint. To the best of our knowledge, this is the first work to introduce differentiability into SVD for LLMs.





% \begin{figure}[t]
% \centering
% \includegraphics[width=1\textwidth]{FIG/taler.png}
% \caption{Schematic diagram of Stable SVD back propagation. The original method will cause the gradient of the decomposition of the low-rank matrix to explode. Our method solves this problem through Taylor expansion and small value constraints.}
% \label{fig_stable_svd}
% \end{figure} 

 

% \subsection{Stable SVD backpropagation}
% \label{sec42}
% Although updating the value of $k$ via gradient descent is theoretically feasible, in practical algorithm execution, we encounter the problem of gradient explosion caused by the instability of backpropagation through decompositions of low-rank matrices. In this section, we stabilize the backpropagation of low-rank matrices SVD by introducing Taylor expansions and transform derivative formula.

% \noindent\textbf{Instability of Backpropagation in Low-Rank Matrices SVD.}
% Consider a matrix $A \in \mathbb{R}^{m \times m}$ obtained via SVD: $A = U_A \Sigma_A V_A^\top$. During backpropagation, the gradients of the loss with respect to $U_A$, $\Sigma_A$, and $V_A$ are $g_U$, $g_\Sigma$, and $g_V$, respectively. Then, the gradient of $A$ is
% \begin{equation}
% gA = U\left( \frac{\text{skew}( U^T g_U)}{\mathbf{E}}\Sigma + \Sigma \frac{\text{skew}( V^T g_V)}{\mathbf{E}} + \text{diag}(g_\Sigma) \right)  V^T
% \label{eq_7}
% \end{equation}
% where $\text{skew}(x)=(x-x^\top)/2$ extracts the skew-symmetric part of a square matrix, and
% \begin{equation}
%  E_{ij} = 
% \begin{cases} 
% \sigma_j^2 - \sigma_i^2 & \text{if } i \neq j, \\ 
% 1 & \text{if } i = j, 
% \end{cases}
% \label{eq_8}
% \end{equation}
% with $\sigma_i$ and $\sigma_j$ being the $i$-th and $j$-th singular values of $A$ after decomposition.
% Therefore, as shown in Equa.\ref{eq_8}, when $\sigma_i$ and $\sigma_j$ are very close in value, or when both $\sigma_i$ and $\sigma_j$ are very small, $E_{ij}$ approaches $0$, causing $g_A$ to become infinite.
% Note that as most activation matrices in LLMs are approximately low-rank matrices, this gradient explosion phenomenon is very common in the activation matrices of LLMs. In approximately low-rank matrices, there are many singular values that are very close to zero, causing $g_A$ to approach infinity during backpropagation.


% \noindent\textbf{Stabilized Backpropagation through SVD.}
% To address the above problem, we propose approximate solutions for $E$ in two cases:
% \begin{enumerate}
%     \item When $\sigma_i \approx 0$ and $\sigma_j \approx 0$, $1/ E_{ij}$ becomes very large. However, since $\sigma_i$ and $\sigma_j$ are extremely small singular values, their contribution to $g_A$ should be minimal. Therefore, in this case, we directly set $1/E_{ij} = \gamma$, where $\gamma$ is a small constant that ensures the contribution of $\sigma_i$ and $\sigma_j$ to $g_A$ remains small.
%     \item When $\sigma_i \not\approx 0$ and $\sigma_j \not\approx 0$, but $\sigma_i \approx \sigma_j$, since $\sigma_i - \sigma_j \approx 0$, $1/E_{ij}$ approaches infinity. In this case, we perform a Taylor expansion of $1/E_{ij}$ as follows:
%    \begin{equation}
%     \frac{1}{E_{ij}}= \frac{1}{\sigma_i(\sigma_i+\sigma_j)}*\frac{1}{1 - (\sigma_j / \sigma_i)} \approx \frac{1}{\sigma_i(\sigma_i+\sigma_j)}(1 + \left(\frac{\sigma_j}{\sigma_i}\right) + \cdots + \left(\frac{\sigma_j}{\sigma_i}\right)^K)
%     \label{eq_9}
%    \end{equation}
%     We obtain an approximate value of $1/E_{ij}$ by retaining a finite number of terms $K$. Note that directly computing Equa.\ref{eq_9} can be time-consuming due to multiple high-order computations. To accelerate the computation, we utilize the summation formula of a geometric series, which simplifies the calculation involving $K$ high-order terms to a single high-order computation.
% \end{enumerate}

% By addressing the above two cases, our algorithm can achieve a robust backpropagation process for low-rank matrices. The schematic of our algorithm is shown in Fig.\ref{fig_stable_svd}. It can be seen that for low-rank matrices, directly computing the gradient with the original algorithm leads to explosion issues, whereas using our algorithm allows us to accurately compute approximate values of the gradient. By introducing stabilized backpropagation through SVD into the differentiable algorithm, the model can more robustly find suitable $k$ values.





\begin{table}[t]
% \raisebox{0.3em}{
\vspace{-25pt}
\begin{minipage}{.32\linewidth}
\centering
\captionof{table}{Perplexity comparison of Dobi-SVD with state-of-the-art pruning methods on the Wikitext2 dataset on the Llama3-8b.}
\vspace{-5pt}
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|c|c|c}
		\toprule[1.5pt]% 顶部线
		\textbf{Method}&\textbf{0.8}&\textbf{0.6}&\textbf{0.4}\\
		 \midrule
         \midrule
		LLM-Prunner&12.7&44.4&121.5\\
        
        Wanda-sp&11.4&58.5&160.5\\
        \cdashline{1-4}[2pt/2pt]
        \hc \textbf{Dobi-SVD}&\textbf{6.90}&\textbf{8.53}&\textbf{15.8}\\
	   \bottomrule[1.5pt] % 底部线
	\end{tabular}}
 
\label{tab_llama3.1-8b_wiki}
\end{minipage}\quad
\begin{minipage}{.32\linewidth}
\centering
\captionof{table}{Perplexity comparison of Dobi-SVD with state-of-the-art pruning methods on the Wikitext2 dataset on the Llama2-7b.}
\vspace{-5pt}
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|c|c|c}
		\toprule[1.5pt]% 顶部线
		\textbf{Method}&\textbf{0.8}&\textbf{0.6}&\textbf{0.4}\\
		 \midrule
         \midrule
		LLM-Prunner&10.5&46.3&253.1\\
        
        Wanda-sp&12.1&38.6&249.2\\
        \cdashline{1-4}[2pt/2pt]
        \hc\textbf{Dobi-SVD}&\textbf{5.92}&\textbf{7.88}&\textbf{9.47}\\
	   \bottomrule[1.5pt] % 底部线
	\end{tabular}}
\label{tab_llama2-7b_wiki}
\end{minipage}\quad
\begin{minipage}{.32\linewidth}
\centering
\captionof{table}{Performance evaluation of Llama2-7b and Llama3.1-8b on the MMLU.}
\vspace{-5pt}
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|c|c}
		\toprule[1.5pt]% 顶部线
		\textbf{Ratio}&\textbf{Llama-2-7b}&\textbf{Llama-3.1-8b}\\
         \midrule
         \midrule
		1.0&41.0&63.3 \\
		\midrule
		0.8&38.6&60.1 \\
        \cdashline{1-3}[2pt/2pt]
		\rule{0pt}{10pt}
        0.6&27.5&50.1 \\
         \cdashline{1-3}[2pt/2pt]
		\rule{0pt}{10pt}
       0.4&24.1&28.2\\
	   \bottomrule[1.5pt] % 底部线
	\end{tabular}}
\label{tab_mmlu}
\end{minipage}
\end{table} 


\begin{table}[t]
\vspace{-5pt}
\caption{Dobi-SVD vs. pruning methods in terms of compression performance of LLaMA-2-7b and LLaMA-3.1-8b on five common sense reasoning datasets. The best performance is marked in bold.}
\vspace{-10pt}
	\centering
	\resizebox{1.0\textwidth}{!}{
	\begin{tabular}{c|c|c|c|c|c|c|c|c|c}
		\toprule[1.5pt]% 顶部线
  \textbf{Models}& \multirow{2}{*}{\textbf{Ratio}}&\multirow{2}{*}{\textbf{Method}}& \multicolumn{5}{c|}{\textbf{Accuracy ($\uparrow$)}}&\textbf{Avg.}&\textbf{Drop}\\
		~&~ & ~&\textbf{PIQA} & \textbf{HellaSwag}& \textbf{WinoGrande} & \textbf{ARC}$\_$e & \textbf{ARC}$\_$c & ($\uparrow$) & ($\downarrow$)  \\ 
		\midrule
       
		\multirow{1}{*}{\textbf{LLaMA-2-7b}}&1.0&Baseline&0.78&0.57&0.69&0.76&0.43&0.65&0$\%$\\
        \cmidrule{2-10}
        \cmidrule{2-10}
        ~&\multirow{5}{*}{$0.6$}&LLM-Pruner&0.70&0.41&0.53&0.53&0.27&0.48&$ 26.2\%$\\
        ~&~&SliceGPT&0.65&0.57&0.60&0.43&0.32&0.51&$ 21.5\%$\\
        ~&~&Bonsai&0.72&0.45&0.58&0.59&0.30&0.53&$ 18.5\%$\\
        ~&~&Wanda-sp&0.70&0.42&0.53&0.57&0.29&0.50&$ 23.1\%$\\
        \cdashline{2-10}[2pt/2pt]
         \hc~& ~& \textbf{Dobi-SVD}&\textbf{0.72}&\textbf{0.45}&\textbf{0.64}&\textbf{0.67}&\textbf{0.31}&\textbf{0.56}&$ \textbf{13.8}\%$\\
         \cmidrule{2-10}
         ~&\multirow{4}{*}{$0.5$}&LLM-Pruner&0.67&0.35&0.52&0.48&0.22&0.45&$ 30.8\%$\\
        ~&~&SliceGPT&0.58&0.46&0.55&0.37&0.28&0.45&$ 30.8\%$\\
        ~&~&Bonsai&0.66&0.40&0.54&0.49&0.26&0.47&$ 27.7\%$\\
        ~&~&Wanda-sp&0.63&0.32&0.53&0.43&0.20&0.42&$ 35.4\%$\\
        \cdashline{2-10}[2pt/2pt]
        \hc ~& 0.4& \textbf{Dobi-SVD}&\textbf{0.67}&\textbf{0.38}&\textbf{0.57}&\textbf{0.55}&\textbf{0.26}&\textbf{0.49}&$ \textbf{24.5}\%$\\
        \midrule


        \multirow{1}{*}{\textbf{LLaMA-3.1-8b}}&1.0&Baseline&0.80&0.59&0.74&0.81&0.51&0.69&0$\%$\\
        \cmidrule{2-10}
        \cmidrule{2-10}
        ~&\multirow{5}{*}{$0.6$}&LLM-Pruner&0.66&0.32&0.54&0.58&0.23&0.46&$ 33.3\%$\\
        ~&~&SliceGPT&0.62&0.40&0.53&0.49&0.25&0.46&$ 33.3\%$\\
        ~&~&Bonsai&0.59&0.29&0.49&0.47&0.18&0.41&$ 40.6\%$\\
        ~&~&Wanda-sp&0.57&0.28&0.50&0.44&0.17&0.39&$ 43.5\%$\\
        \cdashline{2-10}[2pt/2pt]
        \hc ~&& \textbf{Dobi-SVD}&\textbf{0.76}&\textbf{0.52}&\textbf{0.72}&\textbf{0.73}&\textbf{0.39}&\textbf{0.63}&$ \textbf{8.70}\%$\\
         \cmidrule{2-10}
         ~&\multirow{4}{*}{$0.5$}&LLM-Pruner&0.61&0.29&0.52&0.40&0.19&0.40&$ 42.0\%$\\
        ~&~&SliceGPT&0.56&0.33&0.48&0.32&0.22&0.38&$ 44.9\%$\\
        ~&~&Bonsai&0.56&0.27&0.51&0.31&0.18&0.36&$ 47.8\%$\\
        ~&~&Wanda-sp&0.55&0.27&0.50&0.29&0.18&0.36&$ 47.8\%$\\
        \cdashline{2-10}[2pt/2pt]
        \hc ~& 0.4& \textbf{Dobi-SVD}&\textbf{0.68}&\textbf{0.41}&\textbf{0.66}&\textbf{0.58}&\textbf{0.27}&\textbf{0.52}&$ \textbf{24.6}\%$\\
         
         
		\bottomrule[1.5pt] % 底部线
	\end{tabular}}
    \vspace{-10pt}
	\label{tab_llama2_7b_commonsense}
\end{table} 
















































% wakaka
 \vspace{-5pt}
\section{Experiments}
 \vspace{-5pt}
\label{sec5}
Without losing generalizability, our experiments in the main text are conducted on LLaMA-7B, LLaMA2-7B and LLaMA3.1-8B. We investigate four settings: (1) Evaluation on three language modeling datasets and seven commonsense reasoning datasets, comparing against state-of-the-art SVD compression and popular pruning methods (Sect. \ref{sec52}). (2) Analysis of the importance and roles of each component in Dobi-SVD (Sect. \ref{sec53}). (3) Testing the acceleration of Dobi-SVD on different hardware and combining it with quantization (Sect. \ref{sec54}). (4) Test the performance of Dobi-SVD on VLMs and vision-action model (Sect. \ref{sec55}). \textbf{Details of experimental settings are provided in }\ref{sec_app_experiment_detail}.


Additional experiments in the Appendix include evaluations on more tasks (e.g., MMLU, popular pruning methods), more models (Llama-13B: Tabs. \ref{tab_llama-13b_wiki}, \ref{tab_llama-13b_commonsense}; Llama2-13B: Tabs. \ref{tab_llama2-13b_wiki}, \ref{tab_llama2-13b_commonsense}). We also explore combining with quantization (\autoref{tab_app_quant_combine}) and direct comparisons with quantization (\autoref{tab_app_quant_compare}). Finally, We compare the performance of large models compressed by Dobi-SVD with small models that are not compressed. (\autoref{tab_real_1}, \autoref{tab_real_2}).
% In the Appendix, we provide additional experiments, including:  
%  \vspace{-8pt}
% \begin{itemize}[leftmargin=*]
%     \item \textbf{Evaluations on more tasks (MMLU, versus popular pruning methods) on more models:}  
%     \begin{itemize}
%         \item Llama2-7B (\autoref{tab_llama2-7b_wiki}, \autoref{tab_llama2_7b_commonsense}, \autoref{tab_mmlu})
%         \item Llama3.1-8B (\autoref{tab_llama3.1-8b_wiki}, \autoref{tab_llama3.1-8b_commonsense}, \autoref{tab_mmlu})
%     \end{itemize}
%     \item \textbf{Evaluations on larger models:}  
%     \begin{itemize}
%         \item Llama-13B (\autoref{tab_llama-13b_wiki}, \autoref{tab_llama-13b_commonsense})
%         \item Llama2-13B (\autoref{tab_llama2-13b_wiki}, \autoref{tab_llama2-13b_commonsense})
%     \end{itemize}
%     \item \textbf{Quantization:}  
%     \begin{itemize}
%         \item Combined with quantization (\autoref{tab_app_quant_combine}).  
%         \item Direct comparisons with quantization (\autoref{tab_app_quant_compare}).
%     \end{itemize}
%     \item \textbf{Vision-anguage model (VLM) compression:} LLava-v1.5-7B (\autoref{tab_vlm}, \autoref{tab_vlm_speed}).
%     \item \textbf{Vision-language-action model (VLA) compression:} OpenVLA-7B (\autoref{tab_OpenVLA_performance}).
% \end{itemize}

% We validate the effectiveness of Dobi-SVD from three different aspects.
% First, to demonstrate its strong task performance, we compare it against state-of-the-art SVD compression and pruning methods across multiple tasks (Section \ref{sec52}). 
% Then we analyze each of the three proposed modules individually to highlight their effectiveness (Section \ref{sec53}). 
% Furthermore, we test Dobi-SVD on different GPUs to evaluate its hardware performance and verify the actual speedup (Section \ref{sec54}). 

% We conduct extensive experiments to analyze the task effectiveness and efficiency of Dobi-SVD.
% We evaluate Dobi-SVD on Llama-7b \citep{llama} on three language modeling datasets and seven common sense reasoning datasets. We evaluate the throughputs on an 80GB NVIDIA A100 and a 12GB NVIDIA Titan Xp. Implementation details are shown in the Appendix \ref{sec_app_experiment_detail}.

% Our experiments are conducted at four levels. First, we test the performance of our method on tasks and demonstrate the task generalization of Dobi-SVD (Section \ref{sec52}). Furthermore, we perform ablation experiments to separately evaluate the effectiveness of the three modules (Section \ref{sec53}). Then, we deploy Dobi-SVD on hardware and demonstrate that it can effectively reduce the memory requirements (Section \ref{sec54}). Finally, we analyze the importance of each layer in LLMs by examining the changes in $k$ across different layers during training (Section \ref{sec55}). 
 % \vspace{-10pt}
% \subsection{Experimental Settings}
 % \vspace{-10pt}
% \label{sec51}
% \textbf{Models:} To verify the model generalization of Dobi-SVD, we conduct experiments on a range of different models, including Llama-7b (\citep{llama}), Llama2-7b (\citep{llama2}), Llama2-13b, Llama2-70b, Llama3.1-8b (\citep{llama3}) and OPT-6.7b (\citep{opt}).

% \textbf{Models, Datasets and Metric:} To demonstrate the task generalization of Dobi-SVD, we use Llama-7b \citep{llama} and evaluate the performance on different tasks. We first test the model's in-domain performance on the C4 \citep{c4}, Wikitext2 \citep{wiki}, and PTB \citep{ptb}, respectively. On these three datasets, we use perplexity (PPL) as the metric, the lower the better. In addition, we also evaluate it on seven common sense reasoning datasets (OpenbookQA \citep{openbook}, WinoGrande \citep{winogrande}, HellaSwag \citep{hellaswag}, PIQA\citep{piqa}, MathQA \citep{mathqa}, ARC-e, and ARC-c \citep{arc}) in zero-shot setting with the LM-Evaluation-Harness framework \citep{lmeval}. On these datasets, we use accuracy as the metric, the higher the better.

% \textbf{Baselines:} We compare Dobi-SVD with state-of-the-art activation-aware SVD methods, ASVD \citep{asvd} and SVD-LLM \citep{squad}. 
% We also compare Dobi-SVD with state-of-the-art pruning methods, LLM-Pruner \citep{llmpruner} and FLAP \citep{flap}.

% \textbf{Hardware:} To demonstrate the hardware efficiency of Dobi-SVD, we deploy it on hardware devices. We use two representative devices. One is 80GB NVIDIA A100 which represents high-performance GPU, and the other is 12GB NVIDIA Titan Xp which represents low-performance GPU.

% \textbf{Hyperparameters:} The hyperparameters involved in our algorithm mainly include $\beta$, which controls the smoothness of the tanh function; $\gamma$, which controls the minimum threshold of singular values during backpropagation; and $K$, the number of terms retained in the Taylor expansion. In our experiments, we set $\beta = 10$, $\gamma = 1 \times 10^{-10}$, and $K = 10$.

% \textbf{Training Procedure:} To find a suitable truncation position, we randomly select 256 samples from wikitext2 to train the model. For each target compression ratio, we train the model for 20 epochs with a batch size of 32. After training, we use these samples to perform IPCA to update the weights.


%%% WQS begin 这是原本的ppl表
% \begin{table}[t]
% \caption{Dobi-SVD vs. SOTA baseline in terms of compression performance on LLaMA-7b.}
% 	\centering
% 	\resizebox{\textwidth}{!}{
% 	\begin{tabular}{c|ccc|ccc|ccc|ccc}
% 		\toprule% 顶部线
% 		Algorithm&\multicolumn{3}{c}{Method}&\multicolumn{3}{|c}{0.4}&\multicolumn{3}{|c}{0.6}&\multicolumn{3}{|c}{0.8}\\ 
% 		\midrule
% 		 &Perception&Truncation&Remapping& Wiki&PTB&C4&Wiki&PTB&C4&Wiki&PTB&C4\\
% 		\midrule
% 		ASVD &Weight Scaling&Rank Searching&\ding{55}& 57057& 45218& 43036& 1407& 3292& 1109& 11.14& 16.55& 15.93\\
% 		  \rule{0pt}{10pt}
% 		SVD-LLM &Layer-wise Update&Averaging&\ding{55}& 42.30& 312.28& 246.89& 13.11& 63.75& 49.83& 7.94& \textbf{16.22}& 15.84\\
		
		 
%             \midrule
%             \textbf{Dobi-SVD}&IPCA&Differentiable &$\checkmark$&\textbf{9.95}  & \textbf{67.62} &\textbf{17.94} & \textbf{8.12}&\textbf{43.85}&\textbf{12.63} &\textbf{6.08} & 22.39 & \textbf{7.83} \\
% 		\bottomrule % 底部线 
% 	\end{tabular}}
% 	\label{tab_overall_performance}
% \end{table} 
%%% WQS end




%%% keke end

%%% WQS begin
% \begin{table}[t]
% \caption{Zero-shot performance of LLaMA-7B compressed by Dobi-SVD and baselines on seven common sense reasoning datasets (measured by both individual and average accuracy. The best performance is marked in bold. Dobi-SVD* refers to the result obtained without remap algorithm.}
% 	\centering
% 	\resizebox{\textwidth}{!}{
% 	\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
% 		\toprule% 顶部线
% 		Param Ratio&Method&OpenbookQA&ARC\_easy&ARC\_challeng&Winogrande&HellaSwag&PIQA&MathQA&Average&Degradation\\ 
% 		\midrule
% 		$0\%$&Original&0.28&0.67&0.38&0.67&0.56&0.78&0.27&0.52& -\\
%         \midrule
        
%         \multirow{5}{*}{$80\%$}&SVD&0.14&0.27&0.21&0.51&0.26&0.53&0.21&0.31&$\downarrow 40.4\%$\\
%         ~&ASVD&0.25&0.53&0.27&0.64&0.41&0.68&0.24&0.43&$\downarrow 17.3\%$\\
%         ~&SVD-LLM&0.22&0.58&0.29&0.63&0.43&0.69&\textbf{0.24}&0.44&$\downarrow 15.4\%$\\
%         \cdashline{2-11}[2pt/2pt]
% 		\rule{0pt}{10pt}
%         ~&Dobi-SVD*&\textbf{0.26}&\textbf{0.59}&\textbf{0.31}&\textbf{0.66}&\textbf{0.44}&\textbf{0.70}&0.23&\textbf{0.46}&$\downarrow \textbf{11.5}\%$\\
%         ~&Dobi-SVD&\textbf{0.27}&\textbf{0.65}&\textbf{0.37}&\textbf{0.68}&\textbf{0.54}&\textbf{0.77}&\textbf{0.27}&\textbf{0.50}&$\downarrow \textbf{3.84}\%$\\
%         \midrule
  
%         \multirow{5}{*}{$60\%$}&SVD&0.15&0.26&0.22&0.52&0.26&0.53&0.20&0.31&$\downarrow 40.4\%$\\
%         ~&ASVD&0.13&0.28&0.22&0.48&0.26&0.55&0.19&0.30&$\downarrow 42.3\%$\\
%         ~&SVD-LLM&0.19&\textbf{0.42}&0.25&0.58&0.33&0.60&0.21&0.37&$\downarrow 28.8\%$\\
%         \cdashline{2-11}[2pt/2pt]
% 		\rule{0pt}{10pt}
%         ~&Dobi-SVD*&\textbf{0.22}&0.41&\textbf{0.27}&\textbf{0.58}&\textbf{0.34}&\textbf{0.61}&\textbf{0.23}&\textbf{0.38}&$\downarrow \textbf{26.9\%}$\\
%         ~&Dobi-SVD&\textbf{0.28}&\textbf{0.65}&\textbf{0.32}&\textbf{0.62}&\textbf{0.45}&\textbf{0.72}&\textbf{0.25}&\textbf{0.47}&$\downarrow \textbf{9.61\%}$\\
%         \midrule
  
%         \multirow{5}{*}{$40\%$}&SVD&0.16&0.26&0.22&0.50&0.26&0.52&0.21&0.30&$\downarrow 42.3\%$\\
%         ~&ASVD&0.12&0.26&0.21&0.49&0.26&0.53&0.18&0.29&$\downarrow 44.2\%$\\
%         ~&SVD-LLM&0.14&0.28&\textbf{0.22}&0.50&0.27&\textbf{0.55}&0.21&0.31&$\downarrow 40.3\%$\\
%         \cdashline{2-11}[2pt/2pt]
% 		\rule{0pt}{10pt}
%         ~&Dobi-SVD*&\textbf{0.15}&\textbf{0.31}&0.20&\textbf{0.52}&\textbf{0.28}&0.54&\textbf{0.22}&\textbf{0.32}&$\downarrow \textbf{38.4\%}$\\
%         ~&Dobi-SVD&\textbf{0.23}&\textbf{0.52}&\textbf{0.24}&\textbf{0.56}&\textbf{0.38}&\textbf{0.65}&\textbf{0.23}&\textbf{0.40}&$\downarrow \textbf{23.1\%}$\\
% 		\bottomrule % 底部线
% 	\end{tabular}}
% 	\label{tab_compare_otherSVD}
% \end{table} 


% \begin{figure}[t]
% 	\centering
% 	\begin{minipage}{0.32\linewidth}
% 		\centerline{\includegraphics[width=\textwidth]{FIG/Llama1_C4.png}}
%             \centerline{\small(a) Colossal Clean Crawled Corpus}
% 	\end{minipage}
% 	\begin{minipage}{0.32\linewidth}
% 		\centerline{\includegraphics[width=\textwidth]{FIG/Llama1_Wiki.png}}
%         \centerline{\small(b) WikiText-2}
% 	\end{minipage}
% 	% \hspace{5pt}
% 	\begin{minipage}{0.32\linewidth}
% 		\centerline{\includegraphics[width=\textwidth]{FIG/Llama1_PTB.png}}
%           \centerline{\small(c) Penn Treebank}
% 	\end{minipage}
% 	\caption{Comparison of Dobi-SVD with other activation-aware SVD methods on three language modeling datasets (measured in perplexity) on LLaMA-7B. Param ratio are 40\%, 60\%, and 80\%. The best performance is marked in bold. Dobi-SVD, SVD-LLM, and ASVD are tested on the same 40GB-L20 with the same perception and validation data. We randomly sample 256 batches from the Wikitext2 dataset for activation perception, and then the model is validated on the three data validation sets (the number of validation data samples is 256). The sequence len is 2048 in the experiment. Averaging means that the clipping ratio of all weight parameters is the compression ratio.}
% 	\label{fig_overall_performance}
% \end{figure}
%%% WQS end



 \vspace{-5pt}
\subsection{Main Results}
 \vspace{-5pt}
\label{sec52}
\noindent\textbf{In-domain Evaluation.}
We evaluate the performance of Dobi-SVD on top of LLaMA-7B model, with compression ratio ranging from 40\% to 80\%. The experimental results are presented in \autoref{tab_compare_otherSVD}.
% The results show that Dobi-SVD consistently outperforms the state-of-the-art methods across all parameters ratios. 
Notably, Dobi-SVD demonstrates significantly better performance than other SVD methods. At a 0.4 compression ratio, the Dobi-SVD achieves a PPL of 9.70 on Wikitext-2, compared to 43,104 and 458 for ASVD and SVD-LLM, respectively.
This indicates that even with only 40\% of the parameters retained, Dobi-SVD maintains an acceptable performance degradation, a level of performance unattainable by other SVD methods. We emphasize that even without the proposed remapping strategy, Dobi-SVD outperforms prior-arts by a large margin, especially under the low parameter-ratio. This demonstrates the effectiveness of the proposed differentiable optimization of truncation position. Additionally, our quantized storage remapping strategy further improves the performance, showing the significance of our method for improving the injective nature in SVD.

% \vspace{-3pt}
\noindent\textbf{Zero-shot Evaluation.}
To demonstrate the task generalization capability of Dobi-SVD, we take the model trained on Wikitext-2 dataset and conduct the validation on seven out-of-domain datasets. As shown in \autoref{tab_compare_otherSVD}, Dobi-SVD consistently outperforms the previous state-of-the-art methods across different datasets and compression ratios. Specifically, at 80\% compression ratio, Dobi-SVD shows an average performance drop by only 3.14\%.
At low compression ratios, Dobi-SVD can still maintain good performance. For example, at compression ratio of 0.4, Dobi-SVD still has an average accuracy of 40\%, while ASVD and SVD-LLM only have 29\% and 31\% respectively.
Note that even without remapping, the performance of Dobi-SVD* is still better than other SVD compression algorithms.
This shows that our differentiable optimization for truncation positions and weight update method are more effective than other SVD-based methods.

% \vspace{-3pt}
\noindent\textbf{Performance comparison with pruning methods.} 
To further demonstrate Dobi-SVD's superior trade-off between compression ratio and memory usage, we also compare it with state-of-the-art pruning methods, as shown in \autoref{tab_prun_compare}. Note that pruning method is orthogonal to the SVD-based method and these two kinds of methods are in different tracks.
The results show that Dobi-SVD achieves comparable performance relative to the pruning-based method across various task sets.
Compared with FLAP \citep{an2024fluctuation}, a state-of-the-art pruning method, Dobi-SVD outperforms it on all four tasks. We highlight this is the first time that SVD compression method achieves better performance than pruning. 
Notably, while both Dobi-SVD and pruning methods require post-training, Dobi-SVD only trains the truncation positions of the matrices. Dobi-SVD significantly reduces the number of trained parameters and computational cost compared to pruning. Specifically, LLM-Pruner requires training additional 1.2 billion parameters to maintain the model performance on LLaMA-7B, whereas Dobi-SVD only trains 224 parameters.

\autoref{tab_llama3.1-8b_wiki} and \autoref{tab_llama2-7b_wiki} shows the performance of Dobi-SVD on Llama 2-7b and Llama 3.1-8b at different compression ratios on the WikiText-2 dataset. It can be observed that Dobi-SVD demonstrates compression performance consistent with that of Llama-7b. Furthermore, \autoref{tab_llama2_7b_commonsense} presents the model's performance across different commonsense reasoning tasks, comparing Dobi-SVD and pruning methods at equivalent compression ratios. In all five tasks, Dobi-SVD achieved significantly higher accuracy compared to pruning methods at the same compression rate. Notably, on LLaMA-2-7B, at a compression rate of 0.4, Dobi-SVD outperforms pruning methods even at higher compression rates, demonstrating Dobi-SVD's superiority at lower compression levels.
on LLaMA-3.1-8B, at a compression rate of 0.8, the perplexity of Dobi-SVD decreased by only 8.7\% compared to the original model, whereas SliceGPT and LLM-Pruner both showed a 33\% performance drop.
As a result of evaluation on more challenging tasks, \autoref{tab_mmlu} shows the performance of Llama-2-7b and Llama3.1-8b on the MMLU dataset.










% Notably, pruning methods such as LLM-Pruner \citep{ma2023llm} and FLAP \citep{an2024fluctuation} require post-training and fine-tuning, which is time-consuming, whereas Dobi-SVD does not.
 \vspace{-5pt}
\subsection{Analysis Experiment}
\label{sec53}
 \vspace{-5pt}

In this section, we perform analysis experiments on three critical components of the Dobi-SVD method: differentiable optimization of truncation position, efficient weight-updates with Incremental PCA (IPCA) and quantized storage remapping.





\begin{table}[t]
\vspace{-25pt}
  \centering
  \begin{minipage}{0.33\textwidth}
   \vspace{-5pt}
    \centering
    \caption{Comparison of performance before \& after remapping.}
	 \vspace{-10pt}
\resizebox{1\linewidth}{!}{
    \begin{tabular}{c|c|c|c|c}
		\toprule[1.5pt]% 顶部线
		Ratio&Model&Wiki&C4&PTB\\
		 \midrule
          \midrule
          
		  \multirow{3}{*}{$80\%$}&Remap(16bit)&6.05&7.79&22.27 \\
   \cdashline{2-5}[2pt/2pt]
		  \rule{0pt}{10pt}
         ~& \textbf{Remap(8+16bit)}&\textbf{6.08}&\textbf{7.83}&\textbf{22.39}\\
         \cdashline{2-5}[2pt/2pt]
		  \rule{0pt}{10pt}
         ~&W/o Remap&8.87&10.91&25.03\\
         \midrule
		 \multirow{3}{*}{$60\%$}&Remap(16bit)&8.07&12.54&43.68 \\
   \cdashline{2-5}[2pt/2pt]
		  \rule{0pt}{10pt}
         ~&\textbf{Remap(8+16bit)}&\textbf{8.12}&\textbf{12.63}&\textbf{43.85}\\
         \cdashline{2-5}[2pt/2pt]
		  \rule{0pt}{10pt}
         ~&W/o Remap&14.96&24.60&47.01 \\
         \midrule
         
		  \rule{0pt}{10pt}
         \multirow{3}{*}{$40\%$}&Remap(16bit)&9.78&17.39&67.81 \\
   \cdashline{2-5}[2pt/2pt]
		  \rule{0pt}{10pt}
         ~& \textbf{Remap(8+16bit)}&\textbf{9.95}&\textbf{17.94}&\textbf{67.62}\\
         \cdashline{2-5}[2pt/2pt]
		  \rule{0pt}{10pt}
         ~&W/o Remap&58.02&145.41&270.16 \\
	   \bottomrule[1.5pt] % 底部线
	\end{tabular}}
 \label{tab_remap}
  \end{minipage}\quad
    \begin{minipage}{0.3\textwidth}
  \vspace{-5pt}
    \centering
      \caption{Performance and memory usage of Dobi-SVD combined with 4-bit quantization on wikitext2.}
      \vspace{-10pt}
	\centering
	\resizebox{1\linewidth}{!}{
 
  \begin{tabular}{c|c|c|c}
		\toprule[1.5pt]% 顶部线
		 Ratio&Method&PPL&Memory\\
		 \midrule
   \midrule
		\multirow{2}{*}{0.4}&Dobi-SVD&9.95&6.8GB\\ 
        \cdashline{2-4}[2pt/2pt]
		\rule{0pt}{10pt}
        ~& \textbf{Dobi-SVD+GPTQ}&\textbf{12.04}&\textbf{1.8GB}\\
          \midrule
		\multirow{2}{*}{0.6}&Dobi-SVD&8.12&7.7GB\\ 
        \cdashline{2-4}[2pt/2pt]
		\rule{0pt}{10pt}
        ~& \textbf{Dobi-SVD+GPTQ}&\textbf{9.97}&\textbf{2.4GB}\\
        \midrule
		\multirow{2}{*}{0.8}&Dobi-SVD&6.08&10.1GB\\ 
        \cdashline{2-4}[2pt/2pt]
		\rule{0pt}{10pt}
        ~& \textbf{Dobi-SVD+GPTQ}&\textbf{7.01}&\textbf{3.1GB}\\
		\bottomrule[1.5pt] % 底部线
\end{tabular}}
\label{tab_quant_inter}
  \end{minipage}\quad
  \begin{minipage}{0.28\textwidth}
  \vspace{-5pt}
    \centering
     \caption{Speed of running unmapped Llama-7b on TITAN Xp 12GB GPU. PPL is tested on the wikitext2.}
     \vspace{-10pt}
	\centering
\resizebox{1\linewidth}{!}{ 
  \begin{tabular}{c|c|C|C}
		\toprule[1.5pt]% 顶部线
		Ratio&\makecell{Mem\\(GB)}&\makecell{Speed\\(tokens/s)}&\makecell{Speed\\Up}\\
		 \midrule
   \midrule
		 1.0&12.6&2.09 &$1.0 \times$\\ 
          \midrule
		0.8&10.1&\textbf{23.32} &$\textbf{11.2} \times$\\ 
        \midrule
		0.6&7.7&\textbf{24.80} &$\textbf{11.8} \times$\\ 
        \midrule
		0.4&6.8&\textbf{25.97}&$\textbf{12.4} \times$\\ 
		\bottomrule[1.5pt] % 底部线
	\end{tabular}}
\label{tab_low_device}
  \end{minipage}
 \vspace{-2pt}
\end{table}



\begin{figure}[t]
\vspace{-10pt}
	\centering
	\begin{minipage}{0.34\linewidth}
 \hspace{-10pt}
		\centerline{\includegraphics[width=\textwidth]{FIG/process.pdf}}
        % \centerline{\small(a) Gradient-guided activation truncation.}
	\end{minipage}\quad
	% \hspace{5pt}
	\begin{minipage}{0.30\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/train_sample_compare_01.pdf}}
          % \centerline{\small(b) Few Batch Size Training.}
	\end{minipage}\quad
 \begin{minipage}{0.28\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/IPCA_memory.pdf}}
            % \centerline{\small(c) Memory Footprint of PCA\&IPCA}
	\end{minipage}
 \vspace{-5pt}
	\caption{(Left) Performance Comparison of different training methods on LLaMA-7b. For activation truncation (multi-layer) we only truncate layers 29-31, and for activation truncation (single-layer) we only truncate the 29-th layer. (Middle) Comparison of model performance using batch size = 256 and 16 for training. (Right) Comparison of memory requirements for PCA and IPCA for $n*n$ matrix.}
	\label{fig_analy}
 \vspace{-15pt}
\end{figure}








 \vspace{-5pt}
\subsubsection{Analysis on Differentiable Optimization of truncation position}  
 \vspace{-5pt}
% To illustrate the effectiveness of differentiable training, we conducted an ablation study. \autoref{tab_diff} shows a comparison between models trained with differentiable methods and those using the averaging method (as employed by SVD-LLM) to determine the rank $ k $ and update weights $ W $. Across different compression ratio and datasets, the differentiable training method consistently leads to better model performance. Notably, when compression rates are high, the advantage of differentiable training becomes more pronounced. At a 40\% compression ratio, Dobi-SVD achieves a PPL of 46.18 on Wikitext-2, compared to 58.02 with the averaging method.

% Fig. \ref{fig_analy} (b) \cfx{later it would be (a)} depicts training loss and the corresponding validation PPL. The training process effectively helps the model identify optimal $k$.
% Besides, we validate the few-sample training to highlight the advantage of our differentiable optimization. We test the model's performance using only 16 training samples. Remarkably, even with just 16 training samples, Dobi-SVD achieves results comparable to those obtained with 256 samples. This demonstrates the robustness and stability of our approach. Notably, training with 16 samples on LLaMA-7B require only 4 GPUh, highlighting the low computational cost of our method.






% \vspace{-3pt}
\noindent\textbf{Guided Truncation.}  
Based on the above analysis, we hypothesize that, under the same compression ratio, truncating the Attention layers in later stages of LLM may result in smaller performance losses. To validate this, we conducted two experiments on the LLaMA-7B model: truncating a single layer (layer 29) and truncating multiple layers (layers 29-31) with differentiable training. The results are shown in Fig. \ref{fig_analy} (a). We observe that both truncation settings lead to better performance compared to the original model, while barely weight truncation results in the performance degeneration. Besides, truncating single layer can even lead to better performance compared to truncation multiple layers. Both experimental observations indicate that the proposed differentiable optimization of truncation position offers promising potentials to improve model performance through activation truncation and provides guidance for selecting layers to conduct low-rank decomposition.

% the These results are exciting as they suggest that the model's activations contain redundant information, and using SVD to remove noise can enhance model performance. Overall, Dobi-SVD demonstrates the potential to improve model performance through activation truncation and provides guidance for selecting layers to prune via differentiability.

% \vspace{-3pt}
\noindent\textbf{Efficient Training.}
Besides, we validate the sample-efficient training to highlight the advantage of the differentiable optimization. A common post-training batch size is 256 and here we test the model's performance with training batch size as 16 while keeping the epoch same. The performance is shown in Fig. \ref{fig_analy} (b). Even with such a few batch size, Dobi-SVD achieves results comparable to those obtained with 256 samples. This demonstrates the efficiency of our differentiable optimization. Note that training with batch size as 16 on LLaMA-7B require only a few GPU-hours.


% carry less information and contribute less to final model performance, whereas preserving the MLP down projection and Attention$\_v$ layers is more crucial for model accuracy.


% Compared to previous methods, one advantage of differentiable training is the ability to observe changes in truncation position $k$. 

% Since $k$ directly impacts the final model performance in differentiable training, the variation in $k$ reflects the contribution of different layers to the overall performance.
% Fig. \ref{fig_k_change} shows the evolution of $k$ during training, where we observe that different types of layers exhibit varying sensitivity to rank truncation.

% As training progresses, $k$ for the Attention$\_k$ and Attention$\_q$ layers decreases below its initial value, while $k$ for the MLP down projection and Attention$\_v$ layers increases above the initial value. This suggests that Attention$\_k$ and Attention$\_q$ carry less information and contribute less to final model performance, whereas preserving the MLP down projection and Attention$\_v$ layers is more crucial for model accuracy.



 \vspace{-5pt}
\subsubsection{Analysis on Efficient Weight Update.}  
 \vspace{-5pt}
We compare the memory footprint for our proposed efficient weight update strategy (w/ IPCA) and original PCA method, as shown in Fig. \ref{fig_analy}. We observe that as the dimensionality of the decomposed matrix increases, the memory footprint of PCA grows exponentially. In contrast, our weight-update strategy with IPCA significantly reduces memory usage, which remains close to a constant as the dimensionality increases. This is because PCA must be used for decomposing the whole matrix $ V = [V_1, V_2, V_3, \dots, V_n] $, while IPCA can be used for conducting the decomposition at each step $ V = \text{cat}[V_{\text{old}}, V_{ii}] $ thus gets rid of the requirement of storing the entire matrix.






% \begin{wraptable}{r}{0.37\linewidth} % 指定表格位置和宽度
% \vspace{-10pt}
%   \caption{Performance and memory usage of Dobi-SVD combined with 4-bit quantization on wikitext2.}
% 	\centering
% 	\resizebox{1\linewidth}{!}{
%        \begin{tabular}{c|c|c|C}
% 		\toprule% 顶部线
% 		 Ratio&Method&PPL&Memory\\
% 		 \midrule
% 		\multirow{2}{*}{0.4}&Dobi-SVD&9.95&10.1GB\\ 
%         \cdashline{2-4}[2pt/2pt]
% 		\rule{0pt}{10pt}
%         ~&Dobi-SVD+GPTQ&12.04&3.0GB\\
%           \midrule
% 		\multirow{2}{*}{0.6}&Dobi-SVD&8.12&11.9GB\\ 
%         \cdashline{2-4}[2pt/2pt]
% 		\rule{0pt}{10pt}
%         ~&Dobi-SVD+GPTQ&9.97&3.4GB\\
%         \midrule
% 		\multirow{2}{*}{0.8}&Dobi-SVD&6.08&15.8GB\\ 
%         \cdashline{2-4}[2pt/2pt]
% 		\rule{0pt}{10pt}
%         ~&Dobi-SVD+GPTQ&7.01&4.4GB\\
% 		\bottomrule % 底部线
% 	\end{tabular}}
% 	\vspace{-25pt}
% 	\label{tab_quant_inter}
% \end{wraptable}

% \begin{wraptable}{r}{0.37\linewidth} % 指定表格位置和宽度
% \vspace{-20pt}
%   \caption{The speed of running Llama-7b on TITAN Xp 12gb. When compression ratio=1, model cannot be fully run on the GPU and weight transfer between the CPU and GPU. However, after compression by Dobi-SVD, the model can be placed on the GPU. PPL is tested on the wikitext2 dataset.}
% 	\centering
% 	\resizebox{1\linewidth}{!}{
%   \begin{tabular}{c|c|c|c|c}
% 		\toprule% 顶部线
% 		Ratio&PPL&Memory&Speed&Speed Up\\
% 		 \midrule
% 		 1.0&5.68&12.6GB&2.09 tokens/s&$1 \times$\\ 
%           \midrule
% 		0.8&6.08&8.9GB&23.32 tokens/s&$11.2 \times$\\ 
%         \midrule
% 		0.6&8.12&6.7GB&24.80 tokens/s&$11.8 \times$\\ 
%         \midrule
% 		0.4&9.95&4.5GB&25.97 tokens/s&$12.4 \times$\\ 
% 		\bottomrule % 底部线
% 	\end{tabular}}
% 	\vspace{-30pt}
% 	\label{tab_low_device}
% \end{wraptable}





















% \begin{table}[t]
% \caption{Comparison of model performance before and after remapping.}
% 	\centering
% \resizebox{\textwidth}{!}{
%  \vspace{-6mm}
% \centering{
% \footnotesize{
% \setlength{\tabcolsep}{7pt}{
%     \vspace{2mm}
    
%     \begin{subtable}
%         \centering
%         \scriptsize{

%         \vspace{-1mm}
% 	\resizebox{1\linewidth}{!}{
 
%   \begin{tabular}{c|c|c|c|c}
% 		\toprule% 顶部线
% 		Ratio&Model&Wiki&C4&PTB\\
% 		 \midrule
          
% 		  \multirow{3}{*}{$80\%$}&Remap(16bit)&6.05&7.79&22.27 \\
%    \cdashline{2-5}[2pt/2pt]
% 		  \rule{0pt}{10pt}
%          ~&Remap(8+16bit)&6.08&7.83&22.39\\
%          \cdashline{2-5}[2pt/2pt]
% 		  \rule{0pt}{10pt}
%          ~&W/o Remap&8.87&10.91&25.03\\
%          \midrule
         
% 		 \multirow{3}{*}{$60\%$}&Remap(16bit)&8.07&12.54&43.68 \\
%    \cdashline{2-5}[2pt/2pt]
% 		  \rule{0pt}{10pt}
%          ~&Remap(8+16bit)&8.12&12.63&43.85$\\
%          \cdashline{2-5}[2pt/2pt]
% 		  \rule{0pt}{10pt}
%          ~&W/o Remap&14.96&24.60&47.01$ \\
%          \midrule
         
% 		  \rule{0pt}{10pt}
%          \multirow{3}{*}{$40\%$}&Remap(16bit)&9.78&17.39&67.81 \\
%    \cdashline{2-5}[2pt/2pt]
% 		  \rule{0pt}{10pt}
%          ~&Remap(8+16bit)&9.95&17.94&67.62$\\
%          \cdashline{2-5}[2pt/2pt]
% 		  \rule{0pt}{10pt}
%          ~&W/o Remap&58.02&145.41&270.16 \\
% 	   \bottomrule % 底部线
% 	\end{tabular}}}
%  \end{subtable}}
%  \label{tab_remap}
%   \end{minipage}
% \end{table}






 \vspace{-5pt}
\subsubsection{Analysis on Quantized Storage for Remapping.}
\label{sec_experiment_diff}
 \vspace{-5pt}
To demonstrate the effectiveness of our proposed quantizes storage remapping, we compare the performance under three different approaches: remapping without quantization (\textit{i.e.}, Remap (16bit) in \autoref{tab_remap}), remapping with quantization to maintain the compression ratio (\textit{i.e.}, Remap (8+16bit) in \autoref{tab_remap}), and no remapping while maintaining the compression ratio (\textit{i.e.}, W/o Remap in \autoref{tab_remap}), as shown in \autoref{tab_remap}. By comparing the first two approaches, we can see that quantization results in minimal performance drop regardless. The performance comparison between remapping with quantization and no remapping reveals that remapping significantly improves model performance, especially at lower compression ratios. Specifically, at a compression ratio of 0.4, the remapped model achieves a perplexity of 9.95 on the WikiText2 dataset, while the non-remapped model reaches 58.02. This highlights the effectiveness of our proposed remapping in enhancing the model performance.





 \vspace{-5pt}
\subsection{Inference-Efficiency Evaluation}
 \vspace{-5pt}
\label{sec54}

\noindent\textbf{Performance on high-performance GPUs.}
Dobi-SVD not only compresses LLMs but also improves the efficiency of inference on real hardware. We tested the original LLaMA-7B and LLAMA-7B with our Dobi-SVD on NVIDIA A100 GPU by measuring the number of tokens generated per second under varying batch sizes and sequence lengths. The results are shown in Fig. \ref{fig_hardware}. Across all compression ratios, Dobi-SVD consistently improved generation speed. As the batch size increases and the sequence length decreases, this speed improvement becomes more pronounced. With a compression ratio of 0.4, we achieve an improvement of inference speed by up to 1.75 $\times$. At the 0.6 compression ratio, the speedup is  up to 1.4 $\times$. These results highlight the efficiency improvement of Dobi-SVD and suggest its practical potential in LLM services on cloud.





% \begin{wrapfigure}{r}{0.4\textwidth} % r 表示图片在右边，0.5\textwidth 表示图片宽度为文本宽度的一半
%  \vspace{-20pt}
%   \centering % 居中图片
%   \includegraphics[width=\linewidth]{FIG/process.png} 
%   \caption{Performance Comparison of different training methods on LLaMA-7b. For activation truncation (multi-layer) we only truncate layers 29-31, and for activation truncation (single-layer) we only truncate the 29-th layer.} % 图片的标题
%   \label{fig_purning} % 图片的标签
%   \vspace{-40pt}
% \end{wrapfigure}











% \vspace{-5pt}
\noindent\textbf{Performance on low-performance GPUs.}
To demonstrate the applicability of Dobi-SVD in resource-constrained environments, we deploy both the original and compressed LLaMA-7B models on an NVIDIA TITAN Xp with 12GB of memory.
The results are shown in \autoref{tab_low_device}.
Since the LLaMA-7B model requires approximately 14.8GB of memory, the original model requires data transfer between the CPU and GPU during inference. However, applying Dobi-SVD is able to make the entire model run  on the GPU. Dobi-SVD achieves the speedup of 12.4 $\times$. This highlights the huge potential of Dobi-SVD for practical applications in resource-limited devices, like edge-devices.






% \vspace{-5pt}

\noindent\textbf{Performance combined with quantization.}
To verify the compatibility of Dobi-SVD with quantization, we use GPTQ-4bit combined with Dobi-SVD to compress LLaMA-7B. We measure the memory footprint and PPL on wikitext2 before and after quantization. \autoref{tab_quant_inter} shows that the combining GPTQ-4bit with Dobi-SVD further improve the memory utility. 
%For example, the model can achieve a PPL of 7.01 with only 4.4GB of memory usage.
It is worth noting that our results significantly outperform previous methods combining SVD with quantization. For instance, when compressing the model to a compression ratio of 0.6 and integrating with GPTQ-4bit, Dobi-SVD achieves a PPL of 9.97.
This illustrates the wide applicability of Dobi-SVD, which can be flexibly combined with other quantization methods.












\begin{figure}[t]
\vspace{-20pt}
	\centering
	\begin{minipage}{0.48\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/A100_batch_size.pdf}}
            % \centerline{\small(a) Varying Batch Size}
	\end{minipage}
	\begin{minipage}{0.48\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/A100_sequence_length.pdf}}
        % \centerline{\small(b) Varying Sequence Length}
	\end{minipage}
	% \hspace{5pt}
	\vspace{-10pt}
	\caption{Tokens/sec of original LLaMA-7B and its compressed version by Dobi-SVD under 40\%, 60\% and 80\% compression ratio on single A100 GPU.  (a): comparison with different batch size while sequence length = 32. (b): comparison with different sequence length while batch size = 64.}
	\label{fig_hardware}
 \vspace{-15pt}
\end{figure}




% \subsection{Differentiable analysis and guidance}
% \label{sec55}







% \begin{figure}[t]
% % \raisebox{0.3em}{
% \begin{minipage}{.4\linewidth}

% \centering

 
%  % \vspace{10pt}
% \resizebox{1\linewidth}{!}{
% \begin{tabular}{c|c|c|c}
% 		\toprule% 顶部线
% 		 Param Ratio&Memory&Speed&Speed Up\\
% 		 \midrule
% 		 1.0&12.6GB&2.09tokens/s&$1 \times$\\ 
%           \midrule
% 		0.8&8.9GB&23.32tokens/s&$11.2 \times$\\ 
%         \midrule
% 		0.6&6.7GB&24.80tokens/s&$11.8 \times$\\ 
%         \midrule
% 		0.4&4.5GB&25.97tokens/s&$12.4 \times$\\ 
% 		\bottomrule % 底部线
% 	\end{tabular}}
%  \captionof{table}{Quantization accuracy loss at different layers on Llama-7b.}
% \label{tab_quan_acc_loss}	
% \end{minipage}
% % }
% \hfill
% \begin{minipage}{.4\linewidth}
% \vspace{-15pt}
% \centering
% \includegraphics[width=1\textwidth]{FIG/process.png}
% \caption{Performance Comparison of different training methods on LLaMA-7b. For activation truncation (multi-layer) we only truncate layers 29-31, and for activation truncation (single-layer) we only truncate the 29-th layer.}
% \label{fig_purning}
% \end{minipage}
% \end{figure} 
















% \begin{wraptable}{r}{0.48\linewidth} % 指定表格位置和宽度
% \vspace{-5pt}
%   \caption{The perplexity after 60\% parameter compression using Dobi-SVD on 4 different LLMs (including OPT-6.7B, LLaMA 2-7B and LLaMA 3.1-8b). The training dataset is 256 batches randomly sampled from Wikitext2.}
% 	\centering
% 	\resizebox{1\linewidth}{!}{
 
%   \begin{tabular}{c|c|c|ccc}
% 		\toprule% 顶部线
 
% 		Model& Size &Ratio&Wiki&PTB&C4\\
% 		 \midrule
% 		 LLaMA3&8b&\multirow{4}{*}{0.6}& & & \\
%    \cdashline{1-2}[2pt/2pt]
%    \cdashline{4-6}[2pt/2pt]
% 		  \rule{0pt}{10pt}
%          LLaMA2&7b&~& & & \\
%    \cdashline{1-2}[2pt/2pt]
%    \cdashline{4-6}[2pt/2pt]
% 		  \rule{0pt}{10pt}
% 		 OPT&6.7b&~& & & \\
%   \cdashline{1-2}[2pt/2pt]
%    \cdashline{4-6}[2pt/2pt]
% 		  \rule{0pt}{10pt}
% 		 Mistral&7b&~& & & \\
           
% 	   \bottomrule % 底部线
% 	\end{tabular}}
% 	\vspace{5pt}
% 	\label{tab2}
% \end{wraptable}

















\vspace{-5pt}
\subsection{Generalizing to Vision-Language and Vision-Language-Action Models}
\vspace{-5pt}
\label{sec55}
\paragraph{Performance on VLMs.} To further validate the generalizability of our approach, we applied Dobi-SVD to VLMs by compressing the LLM component of Llava-v1.5 \citep{llava}. Specifically, we randomly selected 256 samples of equal token length (660 tokens) from the TextQA dataset for differentiable rank training and IPCA. We then evaluated performance on widely adopted VLM question-answering tasks. The experimental results, presented in \autoref{tab_vlm}, demonstrate that Dobi-SVD achieves strong compression performance on VLMs while maintaining performance. At a compression rate of 0.4, the Dobi-SVD-compressed Llava-v1.5 exhibited nearly zero performance loss on the Pope dataset.
Furthermore, we evaluated Llava-v1.5 under various compression ratios on an NVIDIA A100 80GB GPU. As shown in \autoref{tab_vlm_speed}, the compressed model achieved acceleration across different batch sizes. These results indicate that Dobi-SVD can be effectively applied to large multimodal models, highlighting its broader applicability and practical value.

\paragraph{Performance on OpenVLA.} Deploying LLMs on edge devices remains one of the most significant challenges in robotics. Therefore, we apply Dobi-SVD to robotics tasks to validate its capability to address real-world problems. Specifically, we compress the vision-language-action model OpenVLA-7B \citep{kim24openvla} using Dobi-SVD and evaluate the performance on the BridgeData V2 dataset. During the compression process, we focus on the LLM module within OpenVLA, as it accounts for the majority of the model's memory footprint.
As shown in Tab. \ref{tab_OpenVLA_performance}, the experimental results demonstrate that Dobi-SVD performs exceptionally well on OpenVLA. Even when compressed to 40\% of its original size, the model maintains an accuracy as high as 92.97\%. Moreover, at compression ratios of 80\% and 60\%, the model's performance remains nearly lossless. The extremely low MSE further indicates that the actions executed by the model incur only minimal errors, enabling the successful completion of tasks in most cases. Tab. \ref{tab_OpenVLA_performance} also details the task processing speed and memory requirements under different compression ratios. Notably, when compressed to 40\%, the model requires only 5.2 GB of memory and achieves a 17\% speedup compared to the original version.
These findings underscore the high adaptability of Dobi-SVD for robotics tasks, highlighting its practical utility. By effectively addressing the challenge of deploying memory-bound models on hardware devices, Dobi-SVD plays a significant role in enhancing the feasibility and performance of everyday applications.




\begin{table}[t]
    \centering
    \vspace{-25pt}
    \resizebox{1\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|C|C}
    
       \toprule[1.5pt]% 顶部线
        \multirow{2}{*}{\textbf{Ratio}}& \multicolumn{6}{c|}{\textbf{Accuracy ($\uparrow$)}}&\textbf{Avg.}&\textbf{Drop}\\
        \cdashline{2-7}[2pt/2pt]
		\rule{0pt}{10pt}
        & \textbf{TextQA} & \textbf{VQA} & \textbf{Pope-popular} & \textbf{Pope-random} & \textbf{ope-adversarial} & \textbf{Science QA}  &($\uparrow$) & ($\downarrow$) \\ 
        \midrule
        \midrule
        \textbf{1.0} & 58.22 & 78.51 & 87.2 & 88.1 & 85.1 & 70.22&77.2&$0\%$ \\  
        \midrule
        \textbf{0.8} & 58.25 & 78.53 & 87.1 & 88.0 & 85.1 & 69.72&\textbf{77.5}&$\textbf{0.3}\%$ \\ 
        \cdashline{1-9}[2pt/2pt]
		\rule{0pt}{10pt}
        \textbf{0.6} & 56.00 & 77.89 & 87.4 & 88.4 & 82.1 & 67.41&\textbf{76.9}& $\textbf{0.5}\%$\\ 
        \cdashline{1-9}[2pt/2pt]
		\rule{0pt}{10pt}
        \textbf{0.4} & 46.72 & 70.13 & 86.4 & 89.8 & 79.1 & 52.38&\textbf{70.8}& $\textbf{8.3}\%$\\ 
        \bottomrule[1.5pt] % 底部线
    \end{tabular}}
    \caption{Performance of Dobi-SVD on VLM tasks at different compression ratios on Llava-v1.5. The evaluation metric for all tasks is accuracy.}
    \label{tab_vlm}
\end{table}










\begin{table}[t]
  \centering
    \begin{minipage}{0.35\textwidth}
  \vspace{-5pt}
    \centering
      \caption{Speedup of Dobi-SVD-compressed model on Llava-v1.5 compared to the original model.}
      \vspace{-10pt}
	\centering
	\resizebox{1\linewidth}{!}{
 
  \begin{tabular}{c|c|c}
\toprule[1.5pt]% 顶部线
 Ratio& \textbf{\makecell{Speed (bz=1)\\tokens/s}} & \textbf{\makecell{Speed (bz=16)\\tokens/s}} \\ 
 \midrule
1.0 & 41.90 & 497.56 \\ 
\midrule
0.8 & 42.78(\textuparrow \textbf{2.10\%}) & 524.8(\textuparrow \textbf{5.47\%}) \\ 
\cdashline{1-3}[2pt/2pt]
\rule{0pt}{10pt}
0.6 & 43.15(\textuparrow \textbf{2.89\%}) & 557.4(\textuparrow \textbf{12.2\%}) \\ 
\cdashline{1-3}[2pt/2pt]
\rule{0pt}{10pt}
0.4 & 46.89(\textuparrow \textbf{11.9\%}) & 597.2(\textuparrow \textbf{20.1\%}) \\ 
\bottomrule[1.5pt] % 底部线
\end{tabular}}
\label{tab_vlm_speed}
  \end{minipage}\quad
  \begin{minipage}{0.61\textwidth}
  \vspace{-5pt}
    \centering
     \caption{Performance of Dobi-SVD on OpenVLA and BridgeData V2. For coordinates and angles, we calculate MSE. For opening or closing, we calculate accuracy.}
     \vspace{-10pt}
	\centering
\resizebox{1\linewidth}{!}{ 
  \begin{tabular}{c|c|c|c|c|c}
\toprule[1.5pt]
 Ratio& \textbf{Coordinates} & \textbf{Angle} & \textbf{Accuracy}& \textbf{Speed} & \textbf{Memory} \\
\midrule
1.0 & 0.3948 & 0.2929 & 0.9570& 3.97 tasks/s & 12.6GB \\
\midrule
0.8 & 0.3958 & 0.3005 & 0.9453  & 4.08 tasks/s & 10.3GB \\
\cdashline{1-6}[2pt/2pt]
\rule{0pt}{10pt}
0.6 & 0.3976 & 0.3048 & 0.9453 & 4.25 tasks/s & 7.8GB\\
\cdashline{1-6}[2pt/2pt]
\rule{0pt}{10pt}
0.4 & 0.4008 & 0.3132 & 0.9297& 4.67 tasks/s & 5.2GB  \\
\bottomrule[1.5pt]
\end{tabular}}
\label{tab_OpenVLA_performance}
  \end{minipage}
 \vspace{-2pt}
\end{table}























 \vspace{-5pt}
\section{Conclusion}
 \vspace{-10pt}
% This paper introduces Dobi-SVD, an SVD-based method for efficient LLM compression. Building on the fact that SVD has not been a mainstream compression technique, we identified three key challenges in applying SVD for LLM compression: determining the truncation position, efficiently updating weights, and overcoming truncation limitation. 
% To address these challenges, we first demonstrate through experiments and theoretical analysis that pruning activations results in less performance degradation compared to pruning weights. 
% Based on this motivation, we propose effective solutions for each of the three challenges: 
% 1. To efficiently determine the truncation position, Dobi-SVD introduces a robust differentiable algorithm for training the truncation position, enabling the model to adaptively find the optimal truncation position based on performance.
% 2. To update the weights effectively, Dobi-SVD leverages the Eckart-Young-Mirsky Theorem and the positive-definiteness of the SVD matrix to derive an optimal weight update formula. It also employs IPCA to compute input features in a memory-efficient manner.
% 3. To overcome truncation limitation, Dobi-SVD proposes a memory remapping technique, establishing a one-to-one mapping between the compression ratio and truncation threshold, maximizing the potential of SVD for LLM compression.

% Experiments show that Dobi-SVD achieves minimal performance loss at low compression ratios for the first time with SVD-based compression. When compressing LLaMA-7B to a 0.4 compression ratio, Dobi-SVD achieves a PPL of 9.95 on WikiText2, outperforming both state-of-the-art SVD-based and pruning methods. Additionally, Dobi-SVD provides efficient hardware acceleration, enabling LLaMA-7B to run on a GPU by reducing memory requirements, achieving a 12.4$\times$ speedup on an NVIDIA Titan Xp 12GB with negligible performance loss. Moreover, Dobi-SVD can be combined with other compression techniques for further model compression. Overall, Dobi-SVD is a hardware-agnostic, highly adaptable compression method, demonstrating the significant potential and competitiveness of SVD in model compression, providing new insights for SVD-based methods.

We introduce Dobi-SVD, an efficient SVD-based method for LLM compression. We address three key challenges: (a) how to determine the truncation position, (b) how to update weights efficiently and (c) how to overcome truncation limitation that results in information loss. We first theoretically and empirically explain why truncating activations is better than truncating weights. In addition, we propose solutions to address these three challenges using a differentiable optimization strategy to find the truncation position, an efficient weight update method via the Eckart-Young-Mirsky Theorem, and a quantized memory remapping to maximize SVD's potential.
Experiments demonstrate that Dobi-SVD achieves minimal performance loss at low compression ratios. Dobi-SVD compresses LLaMA-7B to a 0.4 compression ratio with a PPL of 9.95 on WikiText2, outperforming advanced SVD-based and pruning methods. It also provides a 12.4$\times$ speedup on NVIDIA Titan Xp 12GB GPU with negligible loss. 
Overall, Dobi-SVD is a hardware-agnostic, highly adaptable compression method that demonstrates the significant potential and competitiveness of SVD in model compression. 


% \textbf{Limitations.} We analyze three limitations of Dobi-SVD and suggest potential solutions. Firstly, our current SVD implementation is time-consuming and memory-intensive. This issue stems from Python’s support for only fp32 SVD, which could be alleviated by implementing low-precision SVD. Secondly, during memory remapping, transitioning precision becomes challenging when further quantizing to 4-bit or below, leading to a performance drop at 2-bit. To our knowledge, 2-bit quantization itself poses significant challenges in LLM compression.
% % A new memory mapping strategy could be proposed to mitigate precision loss when using 4-bit to represent two values. 
% Third, quantization introduces additional dequantization time during inference, which could be reduced by quantizing larger matrices or using more advanced quantization libraries. Our future work will focus on overcoming these limitations and exploring broader applications of Dobi-SVD, such as in vision-language models and robotics.


% We analyze three limitations of Dobi-SVD and propose possible solutions:
% 1. Since Dobi-SVD involves SVD of activations during truncation training, it is more time-consuming and memory-intensive compared to simple multiplication and addition operations. This leads to higher time and memory requirements during training, which increase with the target compression ratio. This issue arises because current Python only supports full-precision matrix SVD. A potential solution is to implement low-precision matrix SVD.
% 2. During memory remapping, Dobi-SVD involves precision adjustment from 16-bit to 8-bit. When quantizing Dobi-SVD further to 4-bit, the precision adjustment in remapping becomes challenging: quantizing to 2-bit leads to significant performance degradation. A potential solution is to design a new memory mapping bit storage method based on that minimizes precision loss when using 4-bit to represent two values after decomposition.
% 3. Due to the quantization involved in memory remapping, additional dequantization time is introduced during inference. One possible solution is to quantize larger matrices together, which could save dequantization time, or to use more advanced quantization libraries to reduce the required time.
% In future work, we will aim to address these limitations and extend the application of Dobi-SVD to additional fields, such as robotics, to fully realize its potential.


































\newpage

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}


\newpage
\appendix
\section{Appendix}



% wakaka
\textbf{Organization}  In this appendix, we provide in-depth descriptions of the materials that are not covered in the main paper, and report additional experimental results. The document is organized as follows:
\begin{itemize}
    \item \textbf{\ref{sec_appn_relatedWork}}- Related Work
    \item \textbf{\ref{sec_appn_limitations}}- Limitations and potential solutions
    \item \textbf{\ref{sec_app_experiment_detail}}- Experimental setting details
    \item \textbf{\ref{sec_appn_novelPath}}- Dobi-SVD's New Perspective 1: A Novel Path from Activation to Weight
    \begin{itemize}
        \item \textbf{\ref{sec_appn_novelPath_updateW}} Theoretical Support for Updating Weights Using IPCA
    \end{itemize}
    \item \textbf{\ref{sec_appn_longOverlook}}- Dobi-SVD's New Perspective 2: Fully Unlocking SVD's Potential for Data Compression by Addressing A Long-Overlooked Limitation
    \item \textbf{\ref{sec_appn_diffSVD}}- Dobi-SVD's Robust and Efficient Differentiable SVD Algorithm for General Matrices
    \item \textbf{\ref{sec_appn_addAnalysis}}- Additional analytical results
    \begin{itemize}
        \item \textbf{\ref{sec_appn_addAnalysis_quantiPreciLoss}} Quantization Precision Loss
        \item \textbf{\ref{sec_appn_addAnalysis_effeDiffTrain}} Effectiveness of differentiable training
        \item \textbf{\ref{sec_appn_addAnalysis_kChange}} Differentiable $k$ changes at various compression ratios
        \item \textbf{\ref{sec_appn_addAnalysis_sensitivity}} Truncation sensitivity analysis
    \end{itemize}
    \item \textbf{\ref{sec_appn_addExper}}- Additional experimental results
    \begin{itemize}
        \item \textbf{\ref{sec_appn_addExper_moreModels}} Experimental results on more models
        \item \textbf{\ref{sec_appn_addExper_combineQuan}} Combined with quantization
        % \item \textbf{\ref{sec_appn_addExper_VLM}} Experimental results on VLM
        % \item \textbf{\ref{sec_appn_addExper_OpenVLA}} Experimental results on OpenVLA
        \item \textbf{\ref{sec_appn_addExper_twocompare}} Comparison with smaller uncompressed models
    \end{itemize}
    \item \textbf{\ref{sec_appn_realSentenceGen}}- Example Demonstration of Real Sentence Generation
    \item \textbf{\ref{sec_appn_analysistruncation}}- Analysis of directly truncating activations over weights
    \end{itemize}








\subsection{Related Work}

\label{sec_appn_relatedWork}
\textbf{LLM Model Compression.}
Large language models (LLMs) typically contain billions of parameters, making inference on resource-constrained hardware challenging. To address this, researchers have developed various methods to compress models without requiring retraining. These methods can be categorized into three main categories: pruning, quantization, and low-rank decomposition. Specifically, pruning sets individual weights or structured components to zero without changing the overall structure of the LLM. For example, SparseGPT \citep{sparsegpt} prunes the least important weight elements by inverting the Hessian matrix. However, the irregular sparsity from unstructured pruning often fails to achieve significant speedup, only performing optimally on specific hardware architectures. LLM-Pruner \citep{llmpruner}, on the other hand, leverages a small dataset to estimate the coupled importance of weights, parameters, and groups, then applies LoRA-based pruning to recover accuracy. Yet, this approach significantly degrades model accuracy, especially at low compression ratios, due to the extensive modification of the weight matrices.  Sheared Llama \citep{ShearedLlama} performs extensive training on 50 billion tokens after compression.
Quantization, another approach, compresses the model by reducing the precision of the LLM’s weight matrices. For instance, GPTQ \citep{gptq} employs layer-wise quantization, updating weights using inverted Hessian information. The drawback of quantization is that it offers limited compression options, usually between 3 and 8 bits, which may not fully optimize memory utilization.

\textbf{SVD-based Model Compression.}
SVD-based LLMs compression has been widely explored \citep{3,4,5}. Earlier studies primarily used SVD to compress embedding layers \citep{1,2}. As model sizes have grown, research has shifted towards applying SVD to weight matrices \citep{6,7}. Recent findings \citep{truth} suggest that LLM weights are often approximated as low-rank matrices, highlighting SVD's potential for LLM compression. Traditional SVD focuses on compressing the original weight matrix by minimizing $|W - W'|$. However, since it does not account for parameter importance, it often results in significant performance degradation. These methods typically require fine-tuning to recover performance, which demands substantial computational resources for LLMs.
To mitigate this, recent works have focused on activation-aware SVD, which aims to minimize $|A - A'|$. For example, ASVD posits that the activation distribution influences compression error and scales $\Sigma W$ with a diagonal matrix $S$, where $S$ represents the input channel’s impact on weights. SVD-LLM argues that not all larger singular values are necessarily more important, introducing a truncation-aware whitening strategy to determine which singular values are critical for activations. 
However, current activation-aware SVD methods are limited to modifying $\Sigma W$ to adjust $W$, which restricts the values that $\hat{W}$ can take, failing to effectively retain activation information. For instance, ASVD is only effective at high compression ratios (0.8 and 0.9), suffering from significant performance loss at lower compression ratios. SVD-LLM, when compressed to a 0.4 ratio, causes PPL to drop from the original 7.94 to 42.3.


In addition, low-rank decomposition has also been applied in different forms for LLM compression methods. For instance, LQER \citep{lowrank_1} uses SVD to address quantization errors in the quantization process, while LRQ \citep{lowrank_3} applies SVD to enhance the sample handling capacity during training. CALDERA \citep{lowrank_4}, based on matrix compression method LPLR \citep{lowrank_2}, adopts non-SVD low-rank approaches for weight compression, but these typically lead to increased dimensions of the new weight matrices, resulting in performance degradation and necessitating the combination of quantization and LoRA fine-tuning.



\subsection{Limitations and potential solutions}
\label{sec_appn_limitations}
We analyze three limitations of Dobi-SVD and suggest potential solutions. Firstly, our current SVD implementation is time-consuming and memory-intensive. This issue stems from Python’s support for only fp32 SVD, which could be alleviated by implementing low-precision SVD. Secondly, during memory remapping, transitioning precision becomes challenging when further quantizing to 4-bit or below, leading to a performance drop at 2-bit. To our knowledge, 2-bit quantization itself poses significant challenges in LLM compression.
% A new memory mapping strategy could be proposed to mitigate precision loss when using 4-bit to represent two values. 
Third, quantization introduces additional dequantization time during inference, which could be reduced by quantizing larger matrices or using more advanced quantization libraries. Our future work will focus on overcoming these limitations and exploring broader applications of Dobi-SVD, such as in vision-language models and robotics.



\subsection{Experimental setting details}
\label{sec_app_experiment_detail}
In this section, we provide a detailed description of our experimental setup and hyperparameter configurations.

\textbf{Models, Datasets and Metric:} To demonstrate the task generalization of Dobi-SVD, we use Llama-7b \citep{llama} and evaluate the performance on different tasks. We first test the model's in-domain performance on the C4 \citep{c4}, Wikitext2 \citep{wiki}, and PTB \citep{ptb}, respectively. On these three datasets, we use perplexity (PPL) as the metric, the lower the better. In addition, we also evaluate it on seven common sense reasoning datasets (OpenbookQA \citep{openbook}, WinoGrande \citep{winogrande}, HellaSwag \citep{hellaswag}, PIQA\citep{piqa}, MathQA \citep{mathqa}, ARC-e, and ARC-c \citep{arc}) in zero-shot setting with the LM-Evaluation-Harness framework \citep{lmeval}. On these datasets, we use accuracy as the metric, the higher the better.

\textbf{Baselines:} We compare Dobi-SVD with state-of-the-art activation-aware SVD methods, ASVD \citep{asvd} and SVD-LLM \citep{squad}. 
We also compare Dobi-SVD with popular pruning methods, LLM-Pruner \citep{llmpruner}, FLAP \citep{flap}, Wanda-sp \citep{wanda}, SliceGPT\citep{slicegpt}, Bonsai\citep{bosai} and Self-.

\textbf{Hardware:} To demonstrate the hardware efficiency of Dobi-SVD, we deploy it on hardware devices. We use two representative devices. One is 80GB NVIDIA A100 which represents high-performance GPU, and the other is 12GB NVIDIA Titan Xp which represents low-performance GPU.

\textbf{Hyperparameters:} The hyperparameters involved in our algorithm mainly include $\beta$, which controls the smoothness of the tanh function; $\gamma$, which controls the minimum threshold of singular values during backpropagation; and $K$, the number of terms retained in the Taylor expansion. In our experiments, we set $\beta = 10$, $\gamma = 1 \times 10^{-10}$, and $K = 10$.




\textbf{Training Procedure:} During the differentiable truncation training:
\begin{enumerate}[leftmargin=*]
    \item For LLM, we randomly select 256 samples from the WikiText2 dataset as the training set, with each sample containing 2048 tokens.
    \item For VLM, we randomly select 256 samples from the TextQA dataset as the training set, with each sample containing 660 tokens.
\end{enumerate}
Throughout the training, we freeze all parameters except for the truncation position $k$ of each matrix, which remains trainable. We use the TrainingArguments provided by the Transformer library for training. The hyperparameter settings used during training are listed in \autoref{tab_appen_train}.
Additionally, for the tanh function used during the smoothing phase, we set $\beta=10$. For robust SVD backpropagation, we set $\gamma=1e^{-10}$, meaning that singular values smaller than $1e^{-10}$ are treated as $1e^{-10}$ during the backward SVD process. The number of terms in the Taylor expansion is set to $K=10$.



\begin{table}[h]
\caption{Hyperparameter settings during training.}
	\centering
       \vspace{-10pt}
	\resizebox{0.4\textwidth}{!}{
	\begin{tabular}{c|c}
		\toprule[1.0pt]% 顶部线
        Hyperparameters&Value\\
     \midrule
        Seqence Length&2048\\
  
        Number of Train Sample&256\\
        
        Number of Val sample&16\\
        Batch Size&32\\
        Epoch Number&320\\
        Scheduler&Cosine\\
        Optimizer&Adam\\
        Scheduler lr&0.1\\
        Warm up step& 0\\
  
		\bottomrule[1.0pt] % 底部线
	\end{tabular}}
	\label{tab_appen_train}
\end{table} 


\textbf{Weight Update:} During the weight update process, we use the same 256 training data samples as inputs to collect truncated activations. These are then processed with IPCA (Algo.\ref{algo_update_W}) to independently and directly calculate the new weight matrix for each matrix position, requiring no extra data or training.


\textbf{Memory Remapping:} In the remapping quantization process, to align with the normal distribution characteristics of the weight matrix, we use bnb library for model quantization (Algo.\ref{algo_mixedPrecisionStorage}). Specifically, we utilize the bnb-8bit to quantize the matrix, and then concatenate two quantized 8-bit matrices into a single 16-bit matrix.

% \subsection{Additional Information on Method}
% \label{sec_appn_addinfo}
% \subsubsection{A Novel Path from Activation to Weight}



\subsection{Dobi-SVD's New Perspective 1: A Novel Path from Activation to Weight}
\label{sec_appn_novelPath}

Existing SVD-based compression methods primarily fall into two categories:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Directly truncating weights \( W \)}: This straightforward approach ignores the interaction between weights and activations.
    \item \textbf{Activation-aware SVD}: These methods (e.g., ASVD, SVD-LLM) incorporate a scaling matrix \( S \) to capture activation influence by performing SVD on \( WS \) and reconstructing \( W \) using \( S^{-1} \). However, \( S^{-1} \) often fails due to theoretical and numerical issues.
\end{enumerate}

Our method, \textbf{Dobi-SVD}, introduces a novel truncation paradigm: performing SVD directly on activations \( A \) (\( A = xW \), where \( x \) is the input), which establishes \textbf{a novel path from activations to weights}, leveraging the EYM theorem to achieve optimal results. Notably, similar strategies have not been explored in non-SVD-based compression methods: manipulating activations alone may improve inference speed and memory usage, but it does not compress the model itself. 

Fig. \ref{fig_novelPath} illustrates this paradigm shift along a spectrum:
\begin{itemize}
    \item Left (\( W \)): Directly truncating weights.
    \item Middle (\( WS \)): Activation-aware methods: retaining weights \( W \) with auxiliary matrices..
    \item Right (\( A \)): Directly truncating activations.
\end{itemize}

"Directly truncating activations" is a radical approach that is farthest from the weight matrix \( W \) and closest to the activations \( A \). This makes it the most challenging method for reconstructing weights while achieving the best performance, as demonstrated by our theoretical analysis \ref{motivation}. A key reason previous works have avoided this approach is the inherent difficulty in effectively reconstructing the weight matrix.

To address this challenge, we introduce IPCA, a method for compressing high-dimensional orthogonal matrices. 
IPCA enables the reconstruction of new weights after direct activation truncation, serving as a "cosmic wormhole" that seamlessly bridges the activation space and the weight space. In our experiments, this innovation has proven to enable superior compression results without relying on additional data or fine-tuning.


% \begin{figure*}[t]
% \vspace{-2pt}
% \centering
% 	\begin{minipage}{.97\linewidth}
%         %这个图片路径替换
% 		\centerline{\includegraphics[width=\textwidth]{FIG/Novel Path.pdf}}
%           % 加入对这列的图片说明
% 	\end{minipage}
%         \vspace{-5pt}
         
% \caption{\rebuttal{The differences between Dobi-SVD's method and previous approaches in handling activations and obtaining new weights.
% }}
% \label{fig_novelPath}
% \vspace{-20pt}
% \end{figure*}

\subsubsection{Theoretical Support for Updating Weights Using IPCA}
\label{sec_appn_novelPath_updateW}
In Sect.\ref{sec_q2}, we mention that our goal is to find the rank-\(k\) matrix \( \widetilde{W} \) closest to the set of projected weight matrices \( \mathbf{W^p} = \{WV_{A_i}G_kV_{A_i}^\top\}_{i=1}^n \).
Assuming $\widetilde{W}=WVV^T$, our goal can be converted to $\min_{v}\sum_{i=1}^{n}||WV_iV_i^T - WVV^T||_F^2$.
According to the properties of the Frobenius norm, we can get,
\begin{equation}
 \vspace{-5pt}
\min_{v}\sum_{i=1}^{n}||WV_iV_i^T - WVV^T||_F^2 \leq \min_{v}\sum_{i=1}^{n}||W||_f^2 ||V_iV_i^T - VV^T||_F^2.
\label{eq_keke_4}
 % \vspace{-5pt}
\end{equation}
Since $||W||$ is fixed, our goal can be written as:
\begin{equation}
 \vspace{-5pt}
\min_{v}\sum_{i=1}^{n}||V_iV_i^T - VV^T||_F^2 = \min_{v}\sum_{i=1}^{n} ||V_iV_i^T||_F^2 + ||VV^T||_F^2 - 2trace((V_iV_i^TVV^T))
\label{eq_keke_5}
 % \vspace{-5pt}
\end{equation}
Since $V_i$ and $V$ are orthogonal matrices, $||V_iV_i^T||_F^2 = ||VV^T||_F^2 = k$. Equation \ref{eq_keke_5} can be written as:
\begin{equation}
 \vspace{-5pt}
\min_{v}\sum_{i=1}^{n}2k - 2trace((V_iV_i^TVV^T))=2nk-\min_{v}\sum_{i=1}^{n}2trace((V_iV_i^TVV^T))
\label{eq_keke_6}
 % \vspace{-5pt}
\end{equation}
Therefore, our goal is $\max_v\sum_{i=1}^{n}2trace((V_iV_i^TVV^T))$. Since $ trace((V_iV_i^TVV^T))=||V^TV_i||_F^2$. The final form of our goal is $2\max_v\sum_{i=1}^n ||V^TV_i||_F^2$, and its optimal solution $V = \max_{V}\sum_{i=1}^{n} ||V^TV_i||_F^2$ is the value obtained by solving $\{V_1, V_2, \dots, V_n\}$ by PCA. To address excessive memory demands, we use the Incremental Principal Components Analysis (IPCA) algorithm \citep{IPCA} for memory-efficient PCA. 


\subsection{Dobi-SVD's New Perspective 2: Fully Unlocking SVD's Potential for Data Compression by Addressing A Long-Overlooked Limitation}
\label{sec_appn_longOverlook}

\textbf{A Long-Overlooked Limitation of Traditional SVD-Based Compression Methods.} Performing SVD on an \( m \times n \) matrix \( M \), we decompose it as \( M = U \Sigma V^T \). When retaining the top \( k \) singular values, we obtain two matrices: \( (U\Sigma)[:,:k] \) of size \( m \times k \) and \( (V^T)[:k,:] \) of size \( k \times n \). The storage ratio between these matrices and \( M \) is \( k \cdot (m + n) / (m \cdot n) \). To compress \( M \), the storage ratio must be less than 1, which requires \( k \in [0, (m \cdot n) / (m + n)) \). However, since \( M \) has \( \min(m, n) \) singular values, compressing \( M \) necessitates discarding at least \( \min(m, n) - (m \cdot n) / (m + n) \) singular values. For large models, such as a \( 4096 \times 4096 \) matrix, this means losing 2048 singular values—half of the total—resulting in significant and unnecessary information loss.

Prior to our work, no SVD-based methods addressed this issue, as they lacked a solution. Non-SVD-based methods, such as \citet{slicegpt}, have used this limitation as a primary critique of SVD-based methods. 

\textbf{Dobi-SVD's New Perspective.} Our approach resolves this problem by remapping the relationship between information and storage: to ensure a storage ratio below 1, while allowing \( k \in [0, \min(m, n)) \), we deduced that the compressed storage of \( M \) should be \( k \cdot \max(m, n) \). 

To bridge the gap and remap storage from \( k \cdot (m + n) \) to \( k \cdot \max(m, n) \), we leverage the Gaussian distribution properties of the \( U \) and \( V \) matrices and implement a novel quantization method to reduce storage requirements efficiently. The algorithm pseudocode is given in Algo.\ref{algo_mixedPrecisionStorage}.


\begin{algorithm}[H]
\caption{Mixed-Precision Quantization Storage}
\begin{algorithmic}[1]
\REQUIRE 16-bit updated weight matrix rank-$k$ $\widetilde{\mathbf{W}} \in \mathbb{R}^{m \times n}$, 8-bit quantizer $\mathcal{Q}$
\ENSURE 16-bit mixed-precision weight matrix $\widetilde{\mathbf{W}}_{mixed} \in \mathbb{R}^{\max(m,n) \times k}$
\STATE Compute SVD: $\widetilde{\mathbf{W}} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^\top$
\STATE Extract the top-$k$ components: $\tilde{\mathbf{U}}_k = (\mathbf{U} \boldsymbol{\Sigma})[:, :k] \in \mathbb{R}^{m \times k}$, $\mathbf{V}_k = \mathbf{V}[:, :k] \in \mathbb{R}^{n \times k}$
\IF{$m \geq n$}
    \STATE Quantize the first $n$ rows of $\tilde{\mathbf{U}}_k$: $\tilde{\mathbf{U}}_k[i]^8 \leftarrow \mathcal{Q}(\tilde{\mathbf{U}}_k[i]), \; i \in [0, n)$
    \STATE Quantize all rows of $\mathbf{V}_k$: $\mathbf{V}_k[i]^8 \leftarrow \mathcal{Q}(\mathbf{V}_k[i]), \; i \in [0, n)$
    \STATE Construct $\widetilde{\mathbf{W}}_{mixed}$:
    \begin{itemize}
        \item For $i \in [0, n)$: $\widetilde{\mathbf{W}}_{mixed}[i] \leftarrow \text{concatenate}(\tilde{\mathbf{U}}_k[i]^8, \mathbf{V}_k[i]^8)$
        \item For $i \in [n, m)$: $\widetilde{\mathbf{W}}_{mixed}[i] \leftarrow \tilde{\mathbf{U}}_k[i]$
    \end{itemize}
\ELSE
    \STATE Quantize all rows of $\tilde{\mathbf{U}}_k$: $\tilde{\mathbf{U}}_k[i]^8 \leftarrow \mathcal{Q}(\tilde{\mathbf{U}}_k[i]), \; i \in [0, m)$
    \STATE Quantize the first $m$ rows of $\mathbf{V}_k$: $\mathbf{V}_k[i]^8 \leftarrow \mathcal{Q}(\mathbf{V}_k[i]), \; i \in [0, m)$
    \STATE Construct $\widetilde{\mathbf{W}}_{mixed}$:
    \begin{itemize}
        \item For $i \in [0, m)$: $\widetilde{\mathbf{W}}_{mixed}[i] \leftarrow \text{concatenate}(\tilde{\mathbf{U}}_k[i]^8, \mathbf{V}_k[i]^8)$
        \item For $i \in [m, n)$: $\widetilde{\mathbf{W}}_{mixed}[i] \leftarrow \mathbf{V}_k[i]$
    \end{itemize}
\ENDIF
\RETURN 16-bit matrix $\widetilde{\mathbf{W}}_{mixed} \in \mathbb{R}^{\max(m,n) \times k}$
\end{algorithmic}
\label{algo_mixedPrecisionStorage} 
\end{algorithm}


\subsection{Dobi-SVD's Robust and Efficient Differentiable SVD Algorithm for General Matrices}
\label{sec_appn_diffSVD}

\textbf{Gradient Explosion in SVD Backpropagation.}
SVD backpropagation often faces gradient explosion when singular values in a matrix are nearly equal, a common issue in both large-dimensional and low-rank matrices. Widely used techniques like gradient clipping and normalization fail to address this. To the best of our knowledge, only one work \citep{robust_diff_svd} has successfully utilized Taylor expansion to resolve it, demonstrating the method's effectiveness and showing superior gradient behavior compared to other approaches in their paper.

While their focus was on symmetric matrices and computer vision tasks, we made a novel use of Taylor expansion for general matrices and LLM compression. Unlike \citep{robust_diff_svd}, which dealt with low-dimensional image features, our work addresses the computational challenges posed by large-dimensional matrices in LLMs. To meet the demands of these large matrices, we developed a more general and efficient, parallelized algorithm. The algorithm pseudocode is given in Algo.\ref{algo_diffsvd_forward} and Algo.\ref{algo_diffsvd_backward}.


\begin{algorithm}[H]
\caption{Custom Low-Rank SVD Forward Pass}
\begin{algorithmic}[1]
\REQUIRE Input matrix $\mathbf{X} \in \mathbb{R}^{m \times n}$, target rank $k$
\ENSURE Matrices $\mathbf{U} \in \mathbb{R}^{m \times k}$, $\mathbf{S} \in \mathbb{R}^{k}$, $\mathbf{V} \in \mathbb{R}^{n \times k}$
\STATE Compute low-rank SVD: $\mathbf{U}, \mathbf{S}, \mathbf{V} \leftarrow \text{svd\_lowrank}(\mathbf{X}, q = k, \text{niter} = 2)$
\STATE Save $\mathbf{U}$, $\mathbf{S}$, $\mathbf{V}$ for backward pass
\RETURN $\mathbf{U}$, $\mathbf{S}$, $\mathbf{V}$
\end{algorithmic}
\label{algo_diffsvd_forward}
\end{algorithm}

\begin{algorithm}[H]
\caption{Custom Low-Rank SVD Backward Pass}
\begin{algorithmic}[1]
\REQUIRE Gradients $\delta \mathbf{U}$, $\delta \mathbf{S}$, $\delta \mathbf{V}$; saved tensors $\mathbf{U}$, $\mathbf{S}$, $\mathbf{V}$
\ENSURE Gradient $\delta \mathbf{X}$
\STATE Transpose matrices: $\mathbf{V}^\top \leftarrow \mathbf{V}^\top$, $\delta \mathbf{V}^\top \leftarrow (\delta \mathbf{V})^\top$
\IF{$\delta \mathbf{S} = \mathbf{0}$ \AND $\delta \mathbf{U} = \mathbf{0}$ \AND $\delta \mathbf{V}^\top = \mathbf{0}$}
    \RETURN $\delta \mathbf{X} = \mathbf{0}$
\ENDIF
\IF{$\delta \mathbf{U} = \mathbf{0}$ \AND $\delta \mathbf{V}^\top = \mathbf{0}$}
    \RETURN $\delta \mathbf{X} = \mathbf{U} \operatorname{diag}(\delta \mathbf{S}) \mathbf{V}^\top$
\ENDIF
\STATE Define numerical stability parameters: $\epsilon_{\text{val}}$, $\epsilon_{\text{grad}}$, $\epsilon_{\text{diff}}$, $n_{\text{Taylor}}$
\STATE Clamp singular values: $\mathbf{S}_{\text{clamp}} = \max(\mathbf{S}, \epsilon_{\text{val}})$
\STATE Compute singular value ratios:
\STATE $\lambda_i = \mathbf{S}_{\text{clamp}}$, $\lambda_j = \mathbf{S}_{\text{clamp}}^\top$
\STATE $\mathbf{R} = \lambda_j / \lambda_i$
\STATE Initialize matrix $\mathbf{E} \leftarrow \mathbf{1}_{k \times k}$
\STATE Create masks:
\begin{itemize}
    \item Identity mask: $\mathbf{I} = \operatorname{diag}(\mathbf{1}_{k})$
    \item Non-diagonal mask: $\mathbf{M}_{\text{noI}} = \neg \mathbf{I}$
    \item Lower triangular mask: $\mathbf{M}_{\text{lower}} = \operatorname{tril}(\mathbf{1}_{k \times k})$
    \item Initial mask: $\mathbf{M}_{\text{init}} = \mathbf{M}_{\text{noI}} \land \mathbf{M}_{\text{lower}}$
\end{itemize}
\STATE Handle too-small singular values:
\STATE $\mathbf{M}_{\text{small}} = \mathbf{M}_{\text{init}} \land (\mathbf{R} = 1) \land (\lambda_i = \epsilon_{\text{val}})$
\STATE $\mathbf{E}[\mathbf{M}_{\text{small}}] \leftarrow \epsilon_{\text{grad}}$
\STATE Handle normal cases:
\STATE $\mathbf{M}_{\text{normal}} = \mathbf{M}_{\text{init}} \land \neg \mathbf{M}_{\text{small}}$
\STATE Compute differences: $\Delta = |\lambda_i - \lambda_j|$
\STATE For arithmetic sequence (equal singular values):
\STATE $\mathbf{M}_{\text{equal}} = \mathbf{M}_{\text{normal}} \land (\Delta = 0)$
\STATE $\mathbf{E}[\mathbf{M}_{\text{equal}}] \leftarrow \dfrac{n_{\text{Taylor}}}{\lambda_i^2}$
\STATE For geometric sequence (close singular values):
\STATE $\mathbf{M}_{\text{close}} = \mathbf{M}_{\text{normal}} \land (0 < \Delta \leq \epsilon_{\text{diff}})$
\STATE $q^2 = \mathbf{R}[\mathbf{M}_{\text{close}}]^2$
\STATE $\mathbf{E}[\mathbf{M}_{\text{close}}] \leftarrow \dfrac{1}{\lambda_i^2} \left( \dfrac{1 - (q^2)^{n_{\text{Taylor}}}}{1 - q^2} \right)$
\STATE For other cases:
\STATE $\mathbf{M}_{\text{other}} = \mathbf{M}_{\text{normal}} \land (\Delta > \epsilon_{\text{diff}})$
\STATE $\mathbf{E}[\mathbf{M}_{\text{other}}] \leftarrow \dfrac{1}{(\lambda_i - \lambda_j)(\lambda_i + \lambda_j)}$
\STATE Symmetrize $\mathbf{E}$:
\STATE $\mathbf{M}_{\text{pad}} = \mathbf{M}_{\text{noI}} \land \neg \mathbf{M}_{\text{lower}}$
\STATE $\mathbf{E}[\mathbf{M}_{\text{pad}}] \leftarrow -\mathbf{E}^\top[\mathbf{M}_{\text{pad}}]$
\STATE Define skew-symmetric function: $\operatorname{skew}(\mathbf{X}) = \mathbf{X} - \mathbf{X}^\top$
\STATE Compute skew matrices:
\STATE $\boldsymbol{\Omega}_{\mathbf{U}} = \operatorname{skew}(\mathbf{U}^\top \delta \mathbf{U}) \circ \mathbf{E}$
\STATE $\boldsymbol{\Omega}_{\mathbf{V}} = \operatorname{skew}(\mathbf{V}^\top \delta \mathbf{V}) \circ \mathbf{E}$
\STATE Compute core gradient:
\STATE $\delta \mathbf{X} \leftarrow \mathbf{U} \left( \boldsymbol{\Omega}_{\mathbf{U}} \operatorname{diag}(\mathbf{S}) + \operatorname{diag}(\mathbf{S}) \boldsymbol{\Omega}_{\mathbf{V}} + \operatorname{diag}(\delta \mathbf{S}) \right) \mathbf{V}^\top$
\STATE Compute additional terms:
\STATE $\delta \mathbf{U}_{\text{scaled}} = \delta \mathbf{U} / \mathbf{S}_{\text{clamp}}^\top$
\STATE $\text{Term}_1 \leftarrow \left( \delta \mathbf{U}_{\text{scaled}} - \mathbf{U} (\mathbf{U}^\top \delta \mathbf{U}_{\text{scaled}}) \right) \mathbf{V}^\top$
\STATE $\delta \mathbf{V}_{\text{scaled}}^\top = \delta \mathbf{V}^\top / \mathbf{S}_{\text{clamp}}$
\STATE $\text{Term}_2 \leftarrow \mathbf{U} \left( \delta \mathbf{V}_{\text{scaled}}^\top - (\delta \mathbf{V}_{\text{scaled}}^\top \mathbf{V}^\top) \mathbf{V} \right)$
\STATE Update gradient:
\STATE $\delta \mathbf{X} \leftarrow \delta \mathbf{X} + \text{Term}_1 + \text{Term}_2$
\RETURN $\delta \mathbf{X}$
\end{algorithmic}
\label{algo_diffsvd_backward} 
\end{algorithm}




\subsection{Additional analysis results}
\label{sec_appn_addAnalysis}
In this section, we present additional experiments to further demonstrate the effectiveness of Dobi-SVD. First, we analyze the impact of quantization during the remapping on accuracy loss. Next, to illustrate the effectiveness of our training, we compare the performance of non-remapped models under trained and untrained conditions. Finally, we provide a visualization of the model's differentiable changes at various parameters ratios, showing that the change trend of truncation position remains consistent.




\subsubsection{Quantization Precision Loss}
\label{sec_appn_addAnalysis_quantiPreciLoss}
In Sect. \ref{sec_q3}, we utilized the quantization-friendly nature of SVD-decomposed matrices to perform remapping through quantization. Here, we validate this characteristic through experiments. Fig. \ref{fig_app_distri_q} and \ref{fig_app_distri_gate} show the data distribution of an attention matrix and an FFN matrix from LLaMA-7B, both of which exhibit concentrated, normal distributions. This indicates that when using quantization methods designed for normal distributions, such as QLora, the quantization error is minimal. To confirm this, \autoref{tab_app_quan_acc_loss} presents the mean squared error (MSE) and mean absolute error (MAE) of various matrices before and after quantization. As shown, the errors are very small, with MSE around $10^{-8}$. Furthermore, the matrices from the FFN layer show even smaller quantization errors compared to those from the attention layer. This demonstrates that SVD-decomposed matrices are highly suitable for quantization, resulting in only negligible performance loss.


\begin{figure}[h]
% \raisebox{0.3em}{
\begin{minipage}{.28\linewidth}

\centering

 % \vspace{10pt}
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|c|c}
		\toprule% 顶部线
		 Layers&MSE&MAE\\
		 \midrule
           \midrule
		 Q atten&$2.37e-07$&$4e-04$\\ 
          \midrule
		 K atten&$2.49e-07$&$4e-04$\\ 
          \midrule
          V atten&$1.01e-07$&$2e-04$\\ 
          \midrule
          O atten&$9.62e-08$&$2e-04$\\ 
          \midrule
          Gate&$9.66e-08$&$2e-04$\\
          \midrule
          Up&$7.54e-08$&$2e-04$\\ 
          \midrule
          Down&$7.55e-08$&$2e-04$\\ 
		\bottomrule % 底部线
	\end{tabular}}
 \captionof{table}{Quantization accuracy loss at different layers on Llama-7b.}
\label{tab_app_quan_acc_loss}	
\end{minipage}
% }
\hfill
\begin{minipage}{.33\linewidth}
\vspace{-15pt}
\centering
\includegraphics[width=1\textwidth]{FIG/Distribution_Q.png}
\caption{Data distribution of Attention Q matrix of Llama-7b layer 20.}
\label{fig_app_distri_q}
\end{minipage}
\begin{minipage}{.33\linewidth}
\vspace{-15pt}
\centering
\includegraphics[width=1\textwidth]{FIG/Distribution_gate.png}
\caption{Data distribution of MLP Gate matrix of Llama-7b layer 20.}
\label{fig_app_distri_gate}
\end{minipage}
\end{figure} 

\subsubsection{Effectiveness of Differentiable Training}
\label{sec_appn_addAnalysis_effeDiffTrain}
To demonstrate the effectiveness of differentiable training, we conducted an ablation study without remapping. \autoref{tab_app_train_eff} compares the model's performance after updating $k$ using differentiable training versus the average $k$ values (as used in SVD-LLM). Across different compression ratios and datasets, the $k$ values obtained through differentiable training consistently lead to better model performance. Notably, when the compression ratio is low, the differentiable approach shows a clear advantage. For example, at a 0.4 parameter rate, Dobi-SVD achieves a PPL of 46.18 on WikiText2, compared to 58.02 using the averaging method.
Furthermore, Fig. \ref{fig_app_trainppl} illustrates the decline in training loss and PPL on the validation set as the number of training epochs increases. This shows that the training process effectively helps the model find more optimal $k$ values.





\begin{figure}[h]
% \raisebox{0.3em}{
\begin{minipage}{.5\linewidth}

\centering

 % \vspace{10pt}
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|c|c|c|c}
		\toprule% 顶部线
 
		Ratio&Model&Wiki&PTB&C4\\
		 \midrule
		  \multirow{2}{*}{$0.8$}&W/o Training&8.87&15.03&10.91\\
   \cdashline{2-5}[2pt/2pt]
		  \rule{0pt}{10pt}
         ~&Training &8.54&14.83&10.01\\
         \midrule
		 \multirow{2}{*}{$0.6$}&W/o Training&14.96&47.01&24.60 \\
   \cdashline{2-5}[2pt/2pt]
		  \rule{0pt}{10pt}
         ~&Training&13.54&46.38&23.54\\
         \midrule
         \multirow{2}{*}{$0.4$}&W/o Training&58.02&270.16&145.41 \\
   \cdashline{2-5}[2pt/2pt]
		  \rule{0pt}{10pt}
         ~&Training&46.18&238.91&190.62\\
	   \bottomrule % 底部线
	\end{tabular}}
 \captionof{table}{Comparison of model performance with and without training in the non-remapping method. For the non-training case, we truncate each matrix with a uniform cutoff value.}
\label{tab_app_train_eff}
\end{minipage}\quad
\begin{minipage}{.4\linewidth}
\centering
\includegraphics[width=1\textwidth]{FIG/training_loss_ppl.pdf}
\vspace{-20pt}
\caption{The decrease in training loss and ppl over the training epochs for the Llama-7b model when trained with Wikitext2.}
\label{fig_app_trainppl}
\end{minipage}
\end{figure} 


\subsubsection{Differentiable $k$ Changes at Various Compression Ratios}
\label{sec_appn_addAnalysis_kChange}
 
Fig. \ref{fig_k_change_08} visually explains the evolution of $k$ across different layers and training epochs. We observe that different types of layers exhibit varying sensitivity to the truncation position. Specifically, as training progresses, $k$ for the Attention$\_k$ and Attention$\_q$ layers decreases below its initial value, while $k$ for the MLP down projection and Attention$\_v$ layers increases above the initial value. This suggests that Attention$\_k$ and Attention$\_q$, compared to other weight matrices, concentrate important information in their larger singular values (principal components), making them more amenable to low-rank decomposition. We also observe that the MLP down projection and Attention$\_v$ layers are prone to preserving more singular values.

Additionally, we observe that different layers have different sensitivities to rank truncation. Layers at the earlier stages tend to have $k$ higher than their initial values, whereas later layers tend to low rank. This implies that later layers suffer less performance loss under low-rank decomposition, suggesting that during model compression, the later layers can be truncated more aggressively.



In addition, we presented the evolution of truncation positions for different matrices in LLMs during training at a compression ratio of 0.4, along with an analysis of the insights gained from these changes. To demonstrate the general applicability of this trend, Fig. \ref{fig_appen_k_change_02} and \ref{fig_appen_k_change_06} further illustrate the behavior of each layer at different compression ratios. As shown, the layers exhibit similar trends across various ratios, consistent with our analysis in Sect. \ref{sec_experiment_diff}.







\begin{figure}[t]
\vspace{-20pt}
	\centering
	\begin{minipage}{0.28\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/Attn_k_proj_Matrixi.pdf}}
            % \centerline{\small(a) Colossal Clean Crawled Corpus}
	\end{minipage}
	\begin{minipage}{0.28\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/Attn_q_proj_Matrixi.pdf}}
        % \centerline{\small(b) WikiText-2}
	\end{minipage}
	% \hspace{5pt}
	\begin{minipage}{0.28\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/Attn_v_proj_Matrixi.pdf}}
          % \centerline{\small(c) Penn Treebank}
	\end{minipage}
 \begin{minipage}{0.28\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/MLP_Gate_proj_Matrixi.pdf}}
            % \centerline{\small(a) MLP Gate Matrix}
	\end{minipage}
	\begin{minipage}{0.28\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/MLP_Up_proj_Matrixi.pdf}}
        % \centerline{\small(b) WikiText-2}
	\end{minipage}
	% \hspace{5pt}
	\begin{minipage}{0.28\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/MLP_Down_proj_Matrixi.pdf}}
          % \centerline{\small(c) Penn Treebank}
	\end{minipage}
	\caption{$k$ changes over time for different layers. Experiments were performed on the Wikitext2 dataset and the LLaMA-7b  with a target compression ratio of 0.4. The model was trained for 20 epochs (colors range from yellow to purple). The red line indicates the initial gamma value. With lower $k$ indicating higher rank clipping at that layer.}
	\label{fig_k_change_08}
 \vspace{-10pt}
\end{figure}












\begin{figure}[h]
	\centering
	\begin{minipage}{0.32\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/KChange_02/Attn_k_proj_Matrixi.png}}
            % \centerline{\small(a) Colossal Clean Crawled Corpus}
	\end{minipage}
	\begin{minipage}{0.32\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/KChange_02/Attn_q_proj_Matrixi.png}}
        % \centerline{\small(b) WikiText-2}
	\end{minipage}
	% \hspace{5pt}
	\begin{minipage}{0.32\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/KChange_02/Attn_v_proj_Matrixi.png}}
          % \centerline{\small(c) Penn Treebank}
	\end{minipage}
 \begin{minipage}{0.32\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/KChange_02/MLP_Gate_Matrixi.png}}
            % \centerline{\small(a) MLP Gate Matrix}
	\end{minipage}
	\begin{minipage}{0.32\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/KChange_02/MLP_Up_Matrixi.png}}
        % \centerline{\small(b) WikiText-2}
	\end{minipage}
	% \hspace{5pt}
	\begin{minipage}{0.32\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/KChange_02/MLP_Down_Matrixi.png}}
          % \centerline{\small(c) Penn Treebank}
	\end{minipage}
	\caption{$k$ changes over time for different layers. Experiments were performed on the Wikitext2 dataset and the LLaMA-7b  with a target compression ratio of 0.2.}
	\label{fig_appen_k_change_02}
\end{figure}

\begin{figure}[h]
	\centering
	\begin{minipage}{0.32\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/KChange_06/Attention_K_Matrixi.png}}
            % \centerline{\small(a) Colossal Clean Crawled Corpus}
	\end{minipage}
	\begin{minipage}{0.32\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/KChange_02/Attention_Q_Matrixi.png}}
        % \centerline{\small(b) WikiText-2}
	\end{minipage}
	% \hspace{5pt}
	\begin{minipage}{0.32\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/KChange_06/Attention_V_Matrixi.png}}
          % \centerline{\small(c) Penn Treebank}
	\end{minipage}
 \begin{minipage}{0.32\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/KChange_06/MLP_Gate_Matrixi.png}}
            % \centerline{\small(a) MLP Gate Matrix}
	\end{minipage}
	\begin{minipage}{0.32\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/KChange_06/MLP_Up_Matrixi.png}}
        % \centerline{\small(b) WikiText-2}
	\end{minipage}
	% \hspace{5pt}
	\begin{minipage}{0.32\linewidth}
		\centerline{\includegraphics[width=\textwidth]{FIG/KChange_06/MLP_Down_Matrixi.png}}
          % \centerline{\small(c) Penn Treebank}
	\end{minipage}
 \vspace{-10pt}
	\caption{$k$ changes over time for different layers. Experiments were performed on the Wikitext2 dataset and the LLaMA-7b  with a target compression ratio of 0.6.}
	\label{fig_appen_k_change_06}
\end{figure}












\subsubsection{Truncation sensitivity analysis}
\label{sec_appn_addAnalysis_sensitivity}
In this section, we explore the sensitivity of model performance to the truncation value. We first obtained the optimal truncation position for Llama-2-7b at 0.4 using Dobi and randomly selected 10 layers.
While keeping the total k constant, we slightly adjusted k for these 10 layers: adding x to the first five layers and subtracting x from the last five layers, where x took values from [1,5,10,50]. The corresponding adjustment percentages (x/4096) were 0.024\%, 0.122\%, 0.244\%, and 1.221\%.

The experimental results are shown in \autoref{tab_app_sensity}.
It can be seen from the table that even with fine-grained adjustments (0.024\% to 1.221\%), performance drops significantly, worsening exponentially with larger ratios. In contrast, search-based methods have a coarse minimum adjustment of 10\%, causing substantial performance loss that requires fine-tuning. Dobi, built on optimal theoretical analysis, adjusts ranks at 0.024\% granularity and operates end-to-end without fine-tuning.







\begin{table}[H]
\centering
% \arrayrulecolor{blue} % for rebuttal 开一次管到最后
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{c|c}
\toprule[1.5pt]
\textbf{\makecell{Rank Adjustment Percentage \\on Llama-2-7b (Dobi 0.4)}} & \textbf{\makecell{PPL Degradation Perc\\entage on Wikitext2}} \\ \midrule
0\% & 0\% \\\midrule
0.024\% & 0.739\% \\\midrule
0.122\% & 1.584\% \\ \midrule
0.244\% & 4.118\% \\ \midrule
1.221\% & 29.039\% \\
\bottomrule[1.5pt]
\end{tabular}}
\caption{Rank adjustment percentage and corresponding PPL degradation on Wikitext2 for Llama-2-7b (Dobi 0.4).}
\label{tab_app_sensity}
\end{table}



















%==================================================================


\subsection{Additional experimental results}
\label{sec_appn_addExper}

\subsubsection{Experimental results on more models}
\label{sec_appn_addExper_moreModels}

%在正文的表1和表2中，我们展示了对llama-7b的实验结果。在这里，我们展示Dobi-SVD在更多模型上的实验结果。我们选用了Llama-13b，Llama 2-7b，Llama2-13b以及最先进的Llama3.1-8b模型。与正文中实验设置相同，我们从wikitext2中随机选取了256个样本进行可微分训练和IPCA。之后，我们在经典的commonsense Reasoning任务上进行测试并与先进的Pruning方法进行比较。

%在Llama2-7b模型上的性能。表15展示了在Llama2-7b上在不同压缩比下的Dobi-SVD在wikitext2上的性能。可以看到在Llama2-7b上，Dobi-SVD展现出了与Llama1-7b相符的优异压缩性能，在0.4的压缩率下模型在wikitext2的ppl只有9.47，而Wanda-sp为249.2。进一步的，表12展示了在不同的commonsense reasoning task上Dobi-SVD和Pruning的方法相比在相同量级的压缩率下模型的性能。在5个不同的task上，Dobi-SVD的准确率都远超相同压缩比下的pruning的方法。注意到，在压缩率为0.4的情况下，Dobi-SVD压缩的模型的性能优于更高压缩率时pruning的方法，这表现了Dobi-SVD在低压缩比率下的优越性。

%在Llama3.1-8b模型上的性能。表17展现了在最先进的Llama3.1-8b上Dobi-SVD在不同压缩比率下的性能。值得注意的是，Llama3.1-8b模型由于其自身在effecient上的优化和模型结构的不同，本身更难以压缩。例如在相同的压缩率和方法下，Llama3.1-8b的性能损失要比Llama2-7b更大。尽管如此，对比于pruning方法，Dobi-SVD在该模型上依旧有优秀的压缩表现。在0.6的压缩比率在wikitext2上的ppl达到了8.53，而pruning方法在此时会使得模型无法使用。表12进一步展示了在zero-shot下的模型表现对比。在0.8的压缩比率下，Dobi-SVD的ppl相比于原模型只下降了8.7%，而SliceGPT和LLM-Pruner都有33%的性能下降。

%在13b模型上的性能。为了验证Dobi-SVD在更大规模模型上的性能。我们在Llama-13b和Llama2-13b模型上进行测试，实验结果如表14和表15所示。相比于小size的模型，大规模模型通常由更高的冗余和更大的压缩空间。可以看到在Llama-13b和Llama-2-13b上，在压缩比率为0.8时，Dobi-SVD都实现了小于3%的模型性能，优于所有基线。这表明Dobi-SVD的模型泛化性和在更大模型上压缩的可能性。



In \autoref{tab_compare_otherSVD} and \ref{tab_prun_compare} of the main text, we presented experimental results on the Llama-7b model. In this section, we present the results of Dobi-SVD on a broader range of models. We selected Llama-13b, Llama 2-7b, Llama 2-13b, and the state-of-the-art Llama 3.1-8b model. Following the same experimental setup as in the main text, we randomly selected 256 samples from WikiText-2 for differentiable training and IPCA. We then evaluated the models on classical commonsense reasoning tasks and compared the results with pruning methods.



% \begin{table}[h]
% % \raisebox{0.3em}{
% \begin{minipage}{.32\linewidth}
% \centering
% \resizebox{1\linewidth}{!}{
% \begin{tabular}{c|c|c|c}
% 		\toprule% 顶部线
% 		Method&0.8&0.6&0.4\\
% 		 \midrule
% 		LLM-Prunner&12.7&44.4&121.5\\
%          \midrule
%         Wanda-sp&11.4&58.5&160.5\\
        
%          \midrule
%         \hc Dobi-SVD&\textbf{6.90}&\textbf{8.53}&\textbf{15.8}\\
% 	   \bottomrule % 底部线
% 	\end{tabular}}
%  \captionof{table}{Perplexity comparison of Dobi-SVD with state-of-the-art pruning methods on the Wikitext2 dataset on the Llama3-8b.}
% \label{tab_llama3.1-8b_wiki}
% \end{minipage}\quad
% \begin{minipage}{.32\linewidth}
% \centering
% \resizebox{1\linewidth}{!}{
% \begin{tabular}{c|c|c|c}
% 		\toprule% 顶部线
% 		Method&0.8&0.6&0.4\\
% 		 \midrule
% 		LLM-Prunner&10.5&46.3&253.1\\
%          \midrule
%         Wanda-sp&12.1&38.6&249.2\\
%         \midrule
%         \hc Dobi-SVD&\textbf{5.92}&\textbf{7.88}&\textbf{9.47}\\
% 	   \bottomrule % 底部线
% 	\end{tabular}}
%  \captionof{table}{Perplexity comparison of Dobi-SVD with state-of-the-art pruning methods on the Wikitext2 dataset on the Llama2-7b.}
% \label{tab_llama2-7b_wiki}
% \end{minipage}\quad
% \begin{minipage}{.32\linewidth}
% \centering
% \resizebox{1\linewidth}{!}{
% \begin{tabular}{c|c|c}
% 		\toprule% 顶部线
% 		Method&Llama-2-7b&Llama-3.1-8b\\
%          \midrule
% 		1.0&41.0&63.3 \\
% 		 \midrule
% 		0.8&38.6&60.1 \\
%          \midrule
%         0.6&27.5&50.1 \\
%          \midrule
%        0.4&24.1&28.2\\
% 	   \bottomrule % 底部线
% 	\end{tabular}}
%  \captionof{table}{Performance evaluation of Llama2-7b and Llama3.1-8b on the MMLU under different compression ratios.}
% \label{tab_mmlu}
% \end{minipage}
% \end{table} 








% \textbf{Performance on Llama 2-7b.} \autoref{tab_llama2-7b_wiki} shows the performance of Dobi-SVD on Llama 2-7b at different compression ratios on the WikiText-2 dataset. It can be observed that Dobi-SVD demonstrates compression performance consistent with that of Llama-7b, achieving a perplexity of just 9.47 on WikiText-2 at a compression rate of 0.4, compared to 249.2 for Wanda-SP. Furthermore, \autoref{tab_llama2_7b_commonsense} presents the model's performance across different commonsense reasoning tasks, comparing Dobi-SVD and pruning methods at equivalent compression ratios. In all five tasks, Dobi-SVD achieved significantly higher accuracy compared to pruning methods at the same compression rate. Notably, at a compression rate of 0.4, Dobi-SVD outperforms pruning methods even at higher compression rates, demonstrating Dobi-SVD's superiority at lower compression levels.




% \begin{table}[H]
% % \vspace{-20pt}
% \caption{Dobi-SVD vs. popular pruning methods in terms of compression performance of LLaMA-2-7b on five common sense reasoning datasets. The best performance is marked in bold.}
% 	\centering
% 	\resizebox{\textwidth}{!}{
% 	\begin{tabular}{c|c|c|c|c|c|c|c|c}
% 		\toprule[1.5pt]% 顶部线
%   \multirow{2}{*}{\textbf{Ratio}}&\multirow{2}{*}{\textbf{Method}}& \multicolumn{5}{c|}{\textbf{Accuracy ($\uparrow$)}}&\textbf{Avg.}&\textbf{Drop}\\
% 		~ & ~&PIQA & HellaSwag& WinoGrande & ARC$\_$e & ARC$\_$c & ($\uparrow$) & ($\downarrow$)  \\ 
% 		\midrule
       
% 		1.0&Baseline&0.78&0.57&0.69&0.76&0.43&0.65&0$\%$\\
%         \midrule
%         \midrule
%         \multirow{5}{*}{$0.6$}&LLM-Pruner&0.70&0.41&0.53&0.53&0.27&0.48&$ 26.2\%$\\
%         ~&SliceGPT&0.65&0.57&0.60&0.43&0.32&0.51&$ 21.5\%$\\
%         ~&Bonsai&0.72&0.45&0.58&0.59&0.30&0.53&$ 18.5\%$\\
%         ~&Wanda-sp&0.70&0.42&0.53&0.57&0.29&0.50&$ 23.1\%$\\
%         \cdashline{2-9}[2pt/2pt]
%         \hc& Dobi-SVD&\textbf{0.72}&\textbf{0.45}&\textbf{0.64}&\textbf{0.67}&\textbf{0.31}&\textbf{0.56}&$ \textbf{13.8}\%$\\
%          \midrule
%          \multirow{4}{*}{$0.5$}&LLM-Pruner&0.67&0.35&0.52&0.48&0.22&0.45&$ 30.8\%$\\
%         ~&SliceGPT&0.58&0.46&0.55&0.37&0.28&0.45&$ 30.8\%$\\
%         ~&Bonsai&0.66&0.40&0.54&0.49&0.26&0.47&$ 27.7\%$\\
%         ~&Wanda-sp&0.63&0.32&0.53&0.43&0.20&0.42&$ 35.4\%$\\
%         \cdashline{1-9}[2pt/2pt]
%         \hc 0.4& Dobi-SVD&\textbf{0.67}&\textbf{0.38}&\textbf{0.57}&\textbf{0.55}&\textbf{0.26}&\textbf{0.49}&$ \textbf{24.5}\%$\\
         
         
% 		\bottomrule[1.5pt] % 底部线
% 	\end{tabular}}
% 	\label{tab_llama2_7b_commonsense}
% \end{table} 






% \textbf{Performance on Llama 3.1-8b.} \autoref{tab_llama3.1-8b_wiki} presents the performance of Dobi-SVD on the state-of-the-art Llama 3.1-8b at different compression ratios. It is worth mentioning that the Llama 3.1-8b model, due to its inherent optimizations for efficiency and different model architecture, is more challenging to compress. For instance, under the same compression rate and method, Llama 3.1-8b experiences greater performance degradation compared to Llama 2-7b. Nevertheless, compared to pruning methods, Dobi-SVD still achieves excellent compression performance on this model. At a compression rate of 0.6, Dobi-SVD achieved a perplexity of 8.53 on WikiText-2, while pruning methods rendered the model unusable. \autoref{tab_llama3.1-8b_commonsense} further compares the zero-shot performance of the model. At a compression rate of 0.8, the perplexity of Dobi-SVD decreased by only 8.7\% compared to the original model, whereas SliceGPT and LLM-Pruner both showed a 33\% performance drop.
% Furthermore, \autoref{tab_mmlu} shows the performance of Llama-2-7b and Llama3.1-8b on the MMLU dataset.



% \begin{table}[h]
% % \vspace{-20pt}
% \caption{Dobi-SVD vs. popular pruning methods in terms of compression performance of LLaMA-3-8b on five common sense reasoning datasets. The best performance is marked in bold.}
% 	\centering
% 	\resizebox{\textwidth}{!}{
% 	\begin{tabular}{c|c|c|c|c|c|c|c|c}
% 		\toprule[1.5pt]% 顶部线
%   \multirow{2}{*}{\textbf{Ratio}}&\multirow{2}{*}{\textbf{Method}}& \multicolumn{5}{c|}{\textbf{Accuracy ($\uparrow$)}}&\textbf{Avg.}&\textbf{Drop}\\
% 		~ & ~&PIQA & HellaSwag& WinoGrande & ARC$\_$e & ARC$\_$c & ($\uparrow$) & ($\downarrow$)  \\ 
% 		\midrule
       
% 		1.0&Baseline&0.80&0.59&0.74&0.81&0.51&0.69&0$\%$\\
%         \midrule
%         \midrule
%         \multirow{5}{*}{$0.6$}&LLM-Pruner&0.66&0.32&0.54&0.58&0.23&0.46&$ 33.3\%$\\
%         ~&SliceGPT&0.62&0.40&0.53&0.49&0.25&0.46&$ 33.3\%$\\
%         ~&Bonsai&0.59&0.29&0.49&0.47&0.18&0.41&$ 40.6\%$\\
%         ~&Wanda-sp&0.57&0.28&0.50&0.44&0.17&0.39&$ 43.5\%$\\
%         \cdashline{2-9}[2pt/2pt]
%         \hc& Dobi-SVD&\textbf{0.76}&\textbf{0.52}&\textbf{0.72}&\textbf{0.73}&\textbf{0.39}&\textbf{0.63}&$ \textbf{8.70}\%$\\
%          \midrule
%          \multirow{4}{*}{$0.5$}&LLM-Pruner&0.61&0.29&0.52&0.40&0.19&0.40&$ 42.0\%$\\
%         ~&SliceGPT&0.56&0.33&0.48&0.32&0.22&0.38&$ 44.9\%$\\
%         ~&Bonsai&0.56&0.27&0.51&0.31&0.18&0.36&$ 47.8\%$\\
%         ~&Wanda-sp&0.55&0.27&0.50&0.29&0.18&0.36&$ 47.8\%$\\
%         \cdashline{1-9}[2pt/2pt]
%         \hc 0.4& Dobi-SVD&\textbf{0.68}&\textbf{0.41}&\textbf{0.66}&\textbf{0.58}&\textbf{0.27}&\textbf{0.52}&$ \textbf{24.6}\%$\\
         
         
% 		\bottomrule[1.5pt] % 底部线
% 	\end{tabular}}
% 	\label{tab_llama3.1-8b_commonsense}
% \end{table} 





\textbf{Performance on 13b Models.} To evaluate the performance of Dobi-SVD on larger-scale models, we conducted tests on Llama-13b and Llama 2-13b, with experimental results shown in \autoref{tab_llama-13b_wiki} and \autoref{tab_llama2-13b_wiki}. Compared to smaller-sized models, larger-scale models typically have greater redundancy and more compression potential. As shown in \autoref{tab_llama-13b_commonsense} and \autoref{tab_llama2-13b_commonsense}, Dobi-SVD achieved less than a 3\% performance reduction at a compression rate of 0.8 on both Llama-13b and Llama 2-13b, outperforming all baselines. This demonstrates the generalizability of Dobi-SVD and its potential for effective compression on larger models.





\begin{table}[h]
% \raisebox{0.3em}{
\begin{minipage}{.4\linewidth}
\centering
 % \vspace{10pt}
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|c|c|c}
		\toprule% 顶部线
		Method&0.8&0.6&0.4\\
		 \midrule
		LLM-Prunner&7.72&21.7&51.1\\
         \midrule
        Wanda-sp&7.74&27.5&182.9\\
         \midrule
        \hc Dobi-SVD&\textbf{5.43}&\textbf{6.50}&\textbf{11.3}\\
	   \bottomrule % 底部线
	\end{tabular}}
 \captionof{table}{Perplexity comparison of Dobi-SVD with state-of-the-art pruning methods on the Wikitext2 dataset on the Llama-13b model.}
\label{tab_llama-13b_wiki}
\end{minipage}\quad
\begin{minipage}{.4\linewidth}
\centering
\begin{tabular}{c|c|c|c}
		\toprule% 顶部线
		Method&0.8&0.6&0.4\\
		 \midrule
		LLM-Prunner&8.00&21.7&55.8\\
         \midrule
        Wanda-sp&7.45&69.9&90.9\\
         \midrule
        
        \hc Dobi-SVD&\textbf{5.25}&\textbf{6.45}&\textbf{29.3}\\
	   \bottomrule % 底部线
	\end{tabular}
 \captionof{table}{Perplexity comparison of Dobi-SVD with state-of-the-art pruning methods on the Wikitext2 dataset on the Llama2-13b model.}
\label{tab_llama2-13b_wiki}
\end{minipage}
\end{table} 




\begin{table}[h]
% \vspace{-20pt}
\caption{Dobi-SVD vs. popular pruning methods in terms of compression performance of LLaMA-13b on five common sense reasoning datasets. The best performance is marked in bold.}
	\centering
	\resizebox{\textwidth}{!}{
	\begin{tabular}{c|c|c|c|c|c|c|c|c}
		\toprule[1.5pt]% 顶部线
  \multirow{2}{*}{\textbf{Ratio}}&\multirow{2}{*}{\textbf{Method}}& \multicolumn{5}{c|}{\textbf{Accuracy ($\uparrow$)}}&\textbf{Avg.}&\textbf{Drop}\\
		~ & ~&Boolq & piqa& WinoGrande & ARC$\_$e & ARC$\_$c & ($\uparrow$) & ($\downarrow$)  \\ 
		\midrule
       
		1.0&Baseline&0.70&0.79&0.73&0.77&0.47&0.69&0$\%$\\
        \midrule
        \midrule
 \multirow{4}{*}{$0.8$}&LLM-Pruner&0.67&0.77&0.65&0.68&0.38&0.63&$ 8.69\%$\\
        ~&LLM-Pruner(w/LoRA)&0.70&0.78&0.68&0.71&0.42&0.66&$ 4.34\%$\\
        ~&FLAP&0.70&0.78&0.69&0.73&0.43&0.66&$ 4.34\%$\\
        \cdashline{2-9}[2pt/2pt]
        \hc& Dobi-SVD&\textbf{0.69}&\textbf{0.79}&\textbf{0.72}&\textbf{0.76}&\textbf{0.47}&\textbf{0.68}&$ \textbf{1.45}\%$\\
         
		\bottomrule[1.5pt] % 底部线
	\end{tabular}}
	\label{tab_llama-13b_commonsense}
\end{table} 









\begin{table}[h]
% \vspace{-20pt}
\caption{Dobi-SVD vs. popular pruning methods in terms of compression performance of LLaMA-2-13b on five common sense reasoning datasets. The best performance is marked in bold.}
	\centering
	\resizebox{\textwidth}{!}{
	\begin{tabular}{c|c|c|c|c|c|c|c|c}
		\toprule[1.5pt]% 顶部线
  \multirow{2}{*}{\textbf{Ratio}}&\multirow{2}{*}{\textbf{Method}}& \multicolumn{5}{c|}{\textbf{Accuracy ($\uparrow$)}}&\textbf{Avg.}&\textbf{Drop}\\
		~ & ~&Boolq & piqa& WinoGrande & ARC$\_$e & ARC$\_$c & ($\uparrow$) & ($\downarrow$)  \\ 
		\midrule
       
		1.0&Baseline&0.81&0.79&0.72&0.79&0.49&0.72&0$\%$\\
        \midrule
        \midrule
 \multirow{4}{*}{$0.8$}&LLM-Pruner&0.63&0.77&0.63&0.68&0.42&0.63&$ 12.5\%$\\
        ~&Wanda-sp&0.70&0.79&0.70&0.69&0.43&0.63&$ 12.5\%$\\
        ~&FLAP&0.71&0.78&0.71&0.67&0.45&0.66&$ 8.33\%$\\
        \cdashline{2-9}[2pt/2pt]
        \hc& Dobi-SVD&\textbf{0.77}&\textbf{0.78}&\textbf{0.71}&\textbf{0.67}&\textbf{0.45}&\textbf{0.70}&$ \textbf{2.78}\%$\\
         \midrule
         \multirow{4}{*}{$0.6$}&LLM-Pruner&0.67&0.35&0.52&0.48&0.22&0.45&$ 37.5\%$\\
        ~&Wanda-sp&0.58&0.46&0.55&0.37&0.28&0.45&$ 37.5\%$\\
        ~&FLAP&0.66&0.40&0.54&0.49&0.26&0.47&$ 34.7\%$\\
        \cdashline{2-9}[2pt/2pt]
        \hc& Dobi-SVD&\textbf{0.72}&\textbf{0.74}&\textbf{0.70}&\textbf{0.72}&\textbf{0.37}&\textbf{0.65}&$ \textbf{9.72}\%$\\
         
         
		\bottomrule[1.5pt] % 底部线
	\end{tabular}}
	\label{tab_llama2-13b_commonsense}
\end{table} 



\subsubsection{Combined with quantization}
\label{sec_appn_addExper_combineQuan}
% 在这一节，我们展示Dobi-SVD与量化结合的性能。如表5所示。在Llama2-7b模型上，将Dobi-SVD与4bit量化结合后，在0.4的压缩率下，模型在wikitext2上的ppl仅为5.93而只需哟4.4GB的内存。作为对比的，Int6bit量化后的模型需要更高的memory而有更高的性能损失。这表明Dobi-SVD可以与最先经的量化方法相结合以实现更好的压缩性能。
% 值得注意的是，量化通常会受到各种设备的限制（比如部分低性能设备并不支持4bit的运算），而通过与Dobi-SVD结合，模型可以越过这些限制从而实现低压缩比率下更好的性能。

In this section, we present the performance of combining Dobi-SVD with quantization, as shown in \autoref{tab_app_quant_combine}. On the Llama 2-7b model, combining Dobi-SVD with 4-bit BnB quantization results in a perplexity of just 6.91 on WikiText-2 at a compression rate of 0.8, while requiring only 3 GB of memory. In contrast, models purely quantized with 4bit require more memory and suffer from greater performance loss. This demonstrates that Dobi-SVD can be effectively combined with state-of-the-art quantization techniques to achieve better compression performance.
It is worth noting that quantization is often constrained by device limitations (e.g., some low-performance devices do not support 4-bit operations). By combining with Dobi-SVD, the model can overcome these limitations to achieve better performance at lower compression ratios.

\begin{table}[ht]
\centering
\resizebox{0.6\textwidth}{!}{
\begin{tabular}{c|c|c}
\toprule[1.5pt]
\textbf{Model} & \textbf{Memory(GB)} & \textbf{PPL on Wikitext2} \\ 
\midrule
 \midrule
4bit BnB& 3.2 & 6.97 \\ 
\midrule
4bit BnB + Dobi-SVD(0.8)& 3.0 & 6.91 \\ 
 \midrule
 \midrule
 3bit GPTQ & 2.8 & 8.07 \\ 
 \midrule
 4bit GPTQ& 3.8 & 5.86 \\ 
 \midrule
 4bit GPTQ + Dobi-SVD(0.6)& 2.4 & 9.97\\ 
 \midrule
 4bit GPTQ + Dobi-SVD(0.8)& 2.8 & 7.01\\ 
\bottomrule[1.5pt]
\end{tabular}}
\caption{Performance comparison between pure quantization and Dobi-SVD + quantization}
\label{tab_app_quant_combine}
\end{table}


To provide a fairer comparison, we compared our method with quantization methods that have not undergone kernel optimization. We selected the BnB library from Hugging Face, which quantizes weight matrices using the QLoRA approach. Since our algorithm is also implemented within the Hugging Face framework, we believe this is a fairer and more convincing comparison. As shown in the \autoref{tab_app_quant_compare}, despite our model size being larger than that of the quantized model, the inference speed of the Dobi-SVD-compressed model exceeds that of the quantized model. This is because: (1) Dobi-SVD requires fewer FLOPs, which enables faster inference when memory is not a limiting factor, and (2) Dobi-SVD does not require quantization serving, thereby avoiding the significant dequantization time associated with quantization.

\begin{table}[h]
\centering
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c}
\toprule[1.5pt]% 顶部线
\textbf{Model} & \textbf{Size(GB)} & \textbf{PPL} & \textbf{\makecell{Speed (bz=1)\\tokens/s}} & \textbf{\makecell{Speed (bz=16)\\tokens/s}} & \textbf{GFLOPs} \\ 
\midrule
4bit bnb & 3.1 & 6.97 & 14.05 & 202.37  & 29.30  \\  
\midrule
8bit bnb & 6.3 & 5.87 & 4.73 & 69.54  & 29.30  \\  
\midrule
Dobi 0.4 & 6.8 & 9.47 & 21.54 & 581.14  & 18.47 \\  
\midrule
Dobi 0.6 & 7.7 & 7.88 & 20.46  & 579.14  & 26.83 \\
\midrule
Dobi 0.8 & 10.1 & 5.92 & 19.94 & 569.45  & 33.94  \\
\bottomrule[1.5pt] % 底部线
\end{tabular}}
\caption{Performance comparison of Dobi-SVD and Bitsandbytes quantization on Llama-2-7b}
\label{tab_app_quant_compare}
\end{table}









% \subsubsection{Experimental results on Vlm}
% \label{sec_appn_addExper_VLM}

%为了进一步验证我们的方法的通用性。我们在VLM上应用和部署Dobi-SVD。具体而言，我们在Llava-v1.5上对其中的LLM进行压缩。我们从Textqa数据集中随机选择了256个token length相同的数据作为可微分秩训练和IPCA的数据集。之后，我们在VLM经典问答任务上验证其性能。实验结果如表21所示，可以看到Dobi-SVD在VLM上也有好的压缩性能。在0.4的压缩率下，Dobi-SVD呀u送后的Llava-v1.5在Pope数据集上几乎实现了零性能损失。即使在复杂难度较高的ScienceQA任务上，Dobi-SVD在0.6的压缩率下仅有4%的性能损失。
%这表明了Dobi-SVD所提的压缩方法并不局限于LLM的应用场景，而是一种更加general的方法。在VLM上的成功应用证明了Dobi-SVD无限广阔的应用场景，如机器人、图片生成、视频理解等领域。

% To further validate the generalizability of our approach, we applied and deployed Dobi-SVD on Vision-Language Models (VLMs). Specifically, we compressed the LLM component of Llava-v1.5. We randomly selected 256 samples of equal token length (660) from the TextQA dataset for differentiable rank training and IPCA. Subsequently, we evaluated the performance on classic VLM QA tasks. The experimental results are shown in \autoref{tab_vlm}, where Dobi-SVD demonstrates good compression performance on VLMs as well. At a compression rate of 0.4, Dobi-SVD-compressed Llava-v1.5 achieved almost zero performance loss on the Pope dataset. Even on the more complex ScienceQA task, Dobi-SVD incurred only a 4\% performance loss at a compression rate of 0.6.
% Furthermore, we tested the performance of Llava-v1.5 under different compression ratios on NVIDIA A100 80GB. As shown in \autoref{tab_vlm_speed}, the Dobi-SVD-compressed model achieved acceleration across various batch sizes. Notably, at a batch size of 16, the model with a compression ratio of 0.4 achieved a 20\% speedup, with almost no performance loss observed on Pope.

% This indicates that the compression method proposed by Dobi-SVD is not limited to LLM application scenarios but is a more general approach. The successful application on VLMs proves the extensive potential applications of Dobi-SVD in areas such as robotics, image generation, and video understanding.




% \begin{table}[h]
%     \centering
%     \resizebox{1\textwidth}{!}{
%     \begin{tabular}{c|c|c|c|c|c|c|C|C}
    
%        \toprule[1.5pt]% 顶部线
%         \multirow{2}{*}{\textbf{Ratio}}& \multicolumn{6}{c|}{\textbf{Accuracy ($\uparrow$)}}&\textbf{Avg.}&\textbf{Drop}\\
        
%         & TextQA & VQA & Pope-popular & Pope-random & ope-adversarial & Science QA  &($\uparrow$) & ($\downarrow$) \\ 
%         \midrule
%         \midrule
%         ori & 58.22 & 78.51 & 87.2 & 88.1 & 85.1 & 70.22&77.2&$0\%$ \\  
%         \midrule
%         0.8 & 58.25 & 78.53 & 87.1 & 88.0 & 85.1 & 69.72&\textbf{77.5}&$\textbf{0.3}\%$ \\ 
%         \midrule
%         0.6 & 56.00 & 77.89 & 87.4 & 88.4 & 82.1 & 67.41&\textbf{76.9}& $\textbf{0.5}\%$\\ 
%         \midrule
%         0.4 & 46.72 & 70.13 & 86.4 & 89.8 & 79.1 & 52.38&\textbf{70.8}& $\textbf{8.3}\%$\\ 
%         \bottomrule[1.5pt] % 底部线
%     \end{tabular}}
%     \caption{Performance of Dobi-SVD on VLM tasks at different compression ratios on Llava-v1.5. The evaluation metric for all tasks is accuracy.}
%     \label{tab_vlm}
% \end{table}



% \begin{table}[h]
% \centering
% \resizebox{0.4\textwidth}{!}{
% \begin{tabular}{c|c|c}
% \toprule[1.5pt]% 顶部线
%  & \textbf{\makecell{Speed (bz=1)\\tokens/s}} & \textbf{\makecell{Speed (bz=16)\\tokens/s}} \\ 
%  \midrule
% Original & 41.90 & 497.56 \\ 
% \midrule
% 0.8 & 42.78(\textuparrow \textbf{2.10\%}) & 524.8(\textuparrow \textbf{5.47\%}) \\ 
% \midrule
% 0.6 & 43.15(\textuparrow \textbf{2.89\%}) & 557.4(\textuparrow \textbf{12.2\%}) \\ 
% \midrule
% 0.4 & 46.89(\textuparrow \textbf{11.9\%}) & 597.2(\textuparrow \textbf{20.1\%}) \\ 
% \bottomrule[1.5pt] % 底部线
% \end{tabular}}
% \caption{Speedup of Dobi-SVD on Llava-v1.5 compared to the original model.}
% \label{tab_vlm_speed}
% \end{table}














% \subsubsection{Experimental results on OpenVLA}
% \label{sec_appn_addExper_OpenVLA}

%模型太大导致在edge device上部署困难一直是机器人领域最大的挑战之一。在这一节，我们将dobi-svd应用于机器人领域的task以验证Dobi-SVD解决实际问题的能力。具体而言，我们对vision-language-action model OpenVLA-7b用Dobi-SVD压缩，并测试压缩后的模型在BridgeData V2任务集上的表现结果。在压缩过程中，我们只对OpenVLA中的LLM模块进行压缩，因为它占据了大部分的模型memory。

%实验结果如表2所示，可以看到Dobi-SVD在OpenVLA上有非常好的表现。即使压缩到原模型的40%，准确率依然可以达到92.97%。而在压缩到80%和60%时，模型的性能近似无损。同时，极小的MSE表明模型执行的动作只有非常小的误差，可以在大部分task上成功完成任务。表3展示了不同压缩比下模型处理任务的速度和所需内存，压缩到40%的模型只需要5.2GB内存，且处理速度相比于原模型加速了17%.

%这表明了Dobi-SVD在机器人任务上极高的适应性，更体验出Dobi-SVD的实际利用价值。它可以解决模型因为memory bound而无法部署在硬件设备上的实际问题，在人们的日常生活中发挥巨大的作用。

% Deploying large models on edge devices has remained one of the most significant challenges in the field of robotics. In this section, we apply Dobi-SVD to robotics tasks to validate its capability to address real-world problems. Specifically, we compress the vision-language-action model OpenVLA-7B using Dobi-SVD and evaluate the performance of the compressed model on the BridgeData V2 task set. During the compression process, we focus solely on compressing the LLM module within OpenVLA, as it occupies the majority of the model's memory.

% As shown in Tab. \ref{tab_OpenVLA_performance}, the experimental results demonstrate that Dobi-SVD performs exceptionally well on OpenVLA. Even when compressed to 40\% of the original model size, the accuracy remains as high as 92.97\%. Furthermore, at compression ratios of 80\% and 60\%, the model's performance is nearly lossless. Additionally, the extremely low MSE indicates that the actions executed by the model incur only minimal errors, enabling successful completion of tasks in most cases. Tab. \ref{tab_OpenVLA_performance} presents the task processing speed and memory requirements of the model under different compression ratios. When compressed to 40\%, the model requires only 5.2 GB of memory and achieves a 17\% speedup compared to the original model.

% These findings highlight Dobi-SVD's high adaptability for robotics tasks, underscoring its practical utility. It effectively addresses the issue of deploying memory-bound models on hardware devices, thereby playing a significant role in enhancing everyday applications.



% \begin{table}[h]
% \centering
% \resizebox{0.6\textwidth}{!}{
% \begin{tabular}{c|c|c|c|c|c}
% \toprule
%  & \textbf{Coordinates} & \textbf{Angle} & \textbf{Accuracy}& \textbf{Speed} & \textbf{Memory} \\
% \midrule
% ori & 0.3948 & 0.2929 & 0.9570& 3.97 tasks/s & 12.6GB \\
% \midrule
% 0.8 & 0.3958 & 0.3005 & 0.9453  & 4.08 tasks/s & 10.3GB \\
% \midrule
% 0.6 & 0.3976 & 0.3048 & 0.9453 & 4.25 tasks/s & 7.8GB\\
% \midrule
% 0.4 & 0.4008 & 0.3132 & 0.9297& 4.67 tasks/s & 5.2GB  \\
% \bottomrule
% \end{tabular}}
% \caption{The performance of the Dobi-SVD compressed OpenVLA model on BridgeData V2. For coordinates and angles, we calculate the MSE. For opening or closing, we calculate the accuracy. We calculate how many tasks can be completed per second on average.}
% \label{tab_OpenVLA_performance}
% \end{table}




\subsubsection{Comparison with smaller uncompressed models}
\label{sec_appn_addExper_twocompare}
To demonstrate the practicality of our approach in real-world scenarios, we compare the performance of the large model compressed with Dobi-SVD against that of an uncompressed smaller model. The experimental results, presented in \autoref{tab_real_1} and \autoref{tab_real_2}, reveal that the large model compressed with Dobi-SVD outperforms the original smaller model in both accuracy and hardware metrics.
This indicates that the Dobi-SVD-compressed model holds strong competitive advantages and significant practical value in real-world applications.





\begin{table}[h]
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
\toprule[1.5pt]% 顶部线
    Model&Paramters&\multicolumn{2}{c|}{Throughput (tokens/s)}&\multicolumn{6}{c|}{\textbf{Accuracy ($\uparrow$)}}&\textbf{Avg.}\\
 \midrule
 &Billion&bz=1&bz=8&Arc$\_$e&Arc$\_$c&Openb&WinoG&PIQA&Mathqa&$\uparrow$\\ 
\midrule
 Llama-7b (Ori)&7.0&42.3&319.5&0.67&0.38&0.28&0.67&0.78&0.27&0.51\\
 \midrule
 Llama-13b (Dobi-0.6)&7.8&38.6&283.2&0.72&0.40&0.32&0.72&0.78&0.29&0.54\\
\bottomrule[1.5pt] % 底部线
\end{tabular}}
\caption{Performance comparison of Dobi-SVD compressed Llama-13b and uncompressed Llama-7b.}
\label{tab_real_1}
\end{table}





\begin{table}[h]
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
\toprule[1.5pt]% 顶部线
    Model&Paramters&\multicolumn{2}{c|}{Throughput (tokens/s)}&\multicolumn{6}{c|}{\textbf{Accuracy ($\uparrow$)}}&\textbf{Avg.}\\
 \midrule
 &Billion&bz=1&bz=8&Arc$\_$e&Arc$\_$c&Openb&WinoG&PIQA&Mathqa&$\uparrow$\\ 
\midrule
 Opt-2.7b (Ori)&2.7&39.1&293.7&0.56&0.26&0.20&0.58&0.68&0.22&0.42\\
 \midrule
 Llama-7b (Dobi-0.4)&2.8&47.9&368.4&0.58&0.39&0.22&0.57&0.68&0.24&0.45\\
\bottomrule[1.5pt] % 底部线
\end{tabular}}
\caption{Performance comparison of Dobi-SVD compressed Llama-7b and uncompressed Opt-2.7b.}
\label{tab_real_2}
\end{table}






\subsection{Example Demonstration of Real Sentence Generation}
\label{sec_appn_realSentenceGen}
In this section, we present examples of text generated by the Dobi-SVD model at various compression ratios. We verified the generation performance of the model in both zero shot and few shot cases.

\textbf{Zero-shot Performance.}
\autoref{tab_appen_example_zeroshot} shows the generation performance of Dobi-SVD compressed model compared to the original model under zero-shot conditions. The results show that for different sentences, the compressed model always can generate fluent, coherent, and highly relevant responses. For commonsense questions, the model provides highly relevant and correct answers, even at a compression ratio of 0.4. This demonstrates the effectiveness of Dobi-SVD in compressing models while maintaining performance.


\begin{table}[h]
\arrayrulecolor{black}
\caption{Zero-shot example of contents generated by the compressed LLaMA-7B by Dobi-SVD.}
	\centering
 \vspace{-10pt}
	\resizebox{1\textwidth}{!}{
	\begin{tabular}{p{4cm}|p{1cm}|p{9cm}}
		\toprule% 顶部线
        Question&Ratio&Answer\\
        \midrule
        \midrule
		\multirow{4}{*}{\parbox{4cm}{What is the responsibility of the AI assistant? }
}&1&To help you. The AI assistant is a helper. The AI assistant is a helper. It is a helper that you can use to get things done. The AI assistant is not a replacement for humans. It is a helper that can help you get things done.\\
        \cdashline{2-3}[2pt/2pt]
		 \rule{0pt}{10pt}
        ~&0.8&The AI assistant is responsible for answering questions from users. The assistant will also be responsible for providing users with information about products and services. \\
        \cdashline{2-3}[2pt/2pt]
		 \rule{0pt}{10pt}
        ~&0.6&The AI assistant is a robot that is programmed to speak in the voice of the AI assistant.\\
        \cdashline{2-3}[2pt/2pt]
		 \rule{0pt}{10pt}
        ~&0.4&AI assistant is responsible for the execution of the tasks assigned to it. \\  
        \midrule
		\multirow{4}{*}{\parbox{4cm}{What is the universe? Explain this concept. }
}&1&The universe is a big place. It contains everything that exists – the Earth, the Sun, the Moon, the stars and the galaxies. It is made up of matter and energy, and is expanding.\\
        \cdashline{2-3}[2pt/2pt]
		 \rule{0pt}{10pt}
        ~&0.8&The universe is everything that exists in space. It includes everything from the smallest subatomic particle to the largest galaxy. The universe is a very large and complex system. It is made up of many different parts, each with their own unique properties. The universe is also constantly changing. New stars are being born, old stars are dying, and new galaxies are forming. The universe is a very interesting place, and it is full of many mysteries. \\
        \cdashline{2-3}[2pt/2pt]
		 \rule{0pt}{10pt}
        ~&0.6&The universe is everything, including the Earth and everything that has happened.\\
        \cdashline{2-3}[2pt/2pt]
		 \rule{0pt}{10pt}
        ~&0.4&The universe is the world. \\   
        \midrule
        \multirow{4}{*}{\parbox{4cm}{What color is the sky on a clear day?}}&1&The sky is blue because the sun's light is scattered by the particles in the atmosphere. \\
        \cdashline{2-3}[2pt/2pt]
		 \rule{0pt}{10pt}
        ~&0.8&I’m not sure, but it’s probably blue.\\
        \cdashline{2-3}[2pt/2pt]
		 \rule{0pt}{10pt}
        ~&0.6&Answer: The sky is blue.\\
        \cdashline{2-3}[2pt/2pt]
		 \rule{0pt}{10pt}
        ~&0.4&The sky is blue.\\  
        \midrule
		\multirow{4}{*}{\parbox{4cm}{Why do cars need fuel to run?}
}&1&Cars need fuel to run because it is a fuel that is used to run the car. The fuel is used to power the car’s engine. \\
        \cdashline{2-3}[2pt/2pt]
		 \rule{0pt}{10pt}
        ~&0.8&Because people need fuel to run cars.\\
        \cdashline{2-3}[2pt/2pt]
		 \rule{0pt}{10pt}
        ~&0.6&Cars run on fuel because they are fueled by fuel.\\
        \cdashline{2-3}[2pt/2pt]
		 \rule{0pt}{10pt}
        ~&0.4&Because cars need fuel to run, cars need fuel to run.\\ 
		\bottomrule % 底部线
	\end{tabular}}
	\label{tab_appen_example_zeroshot}
\end{table} 





\textbf{Few shot Performance.} 
\autoref{tab_appen_example_fewshot} shows the model's performance on questions selected from the Truthful QA dataset in the few-shot setting with $shot number=6$. After compression with Dobi-SVD, the model is able to generate fluent, coherent, and relevant responses across all compression ratios. Surprisingly, for questions where the original model provided incorrect answers, the compressed model produced correct ones. We speculate that this improvement is due to Dobi-SVD effectively reducing noise in the model through SVD truncation, resulting in enhanced performance.




\begin{table}[h]

\caption{Few-shot example of contents generated by the compressed LLaMA-7B by Dobi-SVD.}
	\centering
 \vspace{-10pt}
	\resizebox{1\textwidth}{!}{
	\begin{tabular}{p{4cm}|p{1cm}|p{8cm}|p{1cm}}
		\toprule% 顶部线
        Question&Ratio&Answer&Correct\\
        \midrule
        \midrule
		\multirow{4}{*}{\parbox{4cm}{What happens to you if you eat watermelon seeds?}
}&Best&The watermelon seeds pass through your digestive system&-
 \\
 \cdashline{2-4}[2pt/2pt]
		 \rule{0pt}{10pt}
        ~&1.0& If you eat watermelon seeds, you will get a stomachache.&\ding{55}\\
        \cdashline{2-4}[2pt/2pt]
		 \rule{0pt}{10pt}
        ~&0.8&Eating watermelon seeds can cause a condition called pica, which is characterized by an unhealthy craving for non-nutritive substances.&\ding{55}\\
        \cdashline{2-4}[2pt/2pt]
		 \rule{0pt}{10pt}
        ~&0.6&Eating watermelon seeds can cause a person to vomit.
&\ding{55}\\  
\cdashline{2-4}[2pt/2pt]
	 
        \hc ~& 0.4&\textbf{Watermelon seeds are edible.}
&$\checkmark$\\  
\midrule
		\multirow{4}{*}{\parbox{4cm}{Where did fortune cookies originate?}
}&Best&Fortune cookies originated in San Francisco.&-
 \\
 \cdashline{2-4}[2pt/2pt]
		 \rule{0pt}{10pt}
        ~&1.0& Fortune cookies originated in Japan.&\ding{55}\\
        \cdashline{2-4}[2pt/2pt]
		 \rule{0pt}{10pt}
        ~&0.8&Fortune cookies originated in Japan.&\ding{55}\\
        \cdashline{2-4}[2pt/2pt]
		 \rule{0pt}{10pt}
        ~&0.6& Fortune cookies originated in China.
&\ding{55}\\  
\cdashline{2-4}[2pt/2pt]
	 
        \hc ~& 0.4&\textbf{The fortune cookies originated in the United States.}
&$\checkmark$\\  
		\bottomrule % 底部线
	\end{tabular}}
	\label{tab_appen_example_fewshot}
\end{table} 




\begin{figure*}[h]
% \vspace{-2pt}
\centering
	\begin{minipage}{.97\linewidth}
        %这个图片路径替换
		\centerline{\includegraphics[width=\textwidth]{FIG/rebuttal_layer.pdf}}
          % 加入对这列的图片说明
	\end{minipage}
        \vspace{-5pt}
         
\caption{Performance comparison of directly truncating activations and truncating weights. We truncate weights and activations on different layers on Llama2-7b and observe the performance loss of the model on 256 samples on wikitext2. It can be seen that in any case, truncating activations has a smaller performance loss than truncating weights.
}
\label{fig_layeranalysis}
% \vspace{-20pt}
\end{figure*}


\subsection{Analysis of directly truncating activations over weights}
\label{sec_appn_analysistruncation}
In this section, we demonstrate that directly truncating activations is superior to truncating weights from both module and model perspectives.


\textbf{Module Level.} Considering a single activation, for activation-aware SVD, the objective is to find a rank-$k$ approximation \(\widetilde{W}\) that satisfies:
\begin{equation}
\min_W\|A - xW\|_F.
\label{equ_module}
\end{equation}
According to the Eckart-Young-Mirsky Theorem, \(A_k\) is the closest rank-$k$ matrix to \(A\) in terms of the Frobenius norm. Therefore, for a single activation \(A\), the truncated activation \(A_k\), reconstructed from \(A\), is the theoretically optimal solution that satisfies Equation \ref{equ_module}.


\textbf{Model Level.}
Considering the entire model, let the performance loss on the training set be denoted as \(\mathcal{L}\). For a weight matrix \(W\) in the model, with input \(x\), the resulting activations are \(A = xW\). The gradients of \(\mathcal{L}\) with respect to \(W\) and \(A\) are \(\partial \mathcal{L}/\partial W\) and \(\partial \mathcal{L}/\partial A\), respectively. When \(W\) and \(A\) are changed, the corresponding changes in \(\mathcal{L}\) are:
\begin{equation}
\Delta \mathcal{L}_W = \frac{\partial \mathcal{L}}{\partial W} \Delta W= \frac{\partial \mathcal{L}}{\partial A} \left(\frac{\partial A}{\partial W}\Delta W\right) = \frac{\partial \mathcal{L}}{\partial A} (x \Delta W) \quad
\text{and} \quad
\Delta \mathcal{L}_A = \frac{\partial \mathcal{L}}{\partial A} \Delta A 
\label{equ_anasis_1}
\end{equation}
Upon truncating weights and activations, let \(\Delta W = W - W_k\) and \(\Delta A = A - A_k\), where \(W_k\) and \(A_k\) represent the truncated matrices retaining the top \(k\) singular values of \(W\) and \(A\), respectively. Substituting these into Equation \ref{equ_anasis_1}, we obtain:
\begin{equation}
    \Delta \mathcal{L}_W = \frac{\partial \mathcal{L}}{\partial A} (x(W - W_k)) = \frac{\partial \mathcal{L}}{\partial A} (A - x W_k) \quad
    \text{and} \quad
    \Delta \mathcal{L}_A = \frac{\partial \mathcal{L}}{\partial A} (A - A_k)
\label{equ_anasis_2}
\end{equation}
To compare the magnitudes of \(\Delta \mathcal{L}_W\) and \(\Delta \mathcal{L}_A\) in Equation \ref{equ_anasis_2}, we need to assess whether, under the same \(\partial \mathcal{L}/\partial A\), the performance loss from changing \(A\) to \(A_k\) is smaller than that from changing \(A\) to \(A - xW_k\). To verify this, we conducted experiments on the Llama2-7b model with various activations \(A\) and truncation levels \(k\). For each activation \(A\) and truncation level \(k\), we altered \(A\) to \(A_k\) and to \(xW_k\) while keeping other layers unchanged (thus ensuring \(\partial \mathcal{L}/\partial A\) remains constant) and observed the resulting performance loss. We uniformly sampled layers with indices \(\{5, 10, 15, 20, 25\}\) and truncation levels \(k = \{100, 500, 1500, 2000, 2500, 3000\}\). The experimental results, shown in Fig. \ref{fig_layeranalysis}, indicate that for different layers and values of \(k\), the performance loss caused by \(A_k\) is consistently smaller than that caused by \(xW_k\).

Therefore, we conclude that \(\frac{\partial \mathcal{L}}{\partial A} (A - A_k) \leq  \frac{\partial \mathcal{L}}{\partial A}(A - x W_k)\) holds for the majority of activations \(A\) in LLMs, implying that \(\Delta \mathcal{L}_A \leq \Delta \mathcal{L}_W\). This suggests that directly applying SVD to activations is more effective than truncating weights.






% \begin{table}[h]
% \caption{ .}
% 	\centering
%  \vspace{-10pt}
% 	\resizebox{1\textwidth}{!}{
% 	\begin{tabular}{p{7cm}|p{1cm}|p{8cm}|p{1cm}}
% 		\toprule% 顶部线
%         Question&Ratio&Answer&Correct\\
%         \midrule
% 		\multirow{4}{*}{\parbox{7cm}{Where did fortune cookies originate?}
% }&Best&Fortune cookies originated in San Francisco.&-
%  \\
%  \cdashline{2-4}[2pt/2pt]
% 		 \rule{0pt}{10pt}
%         ~&1.0& Fortune cookies originated in Japan.&\ding{55}\\
%         \cdashline{2-4}[2pt/2pt]
% 		 \rule{0pt}{10pt}
%         ~&0.8&Fortune cookies originated in Japan.&\ding{55}\\
%         \cdashline{2-4}[2pt/2pt]
% 		 \rule{0pt}{10pt}
%         ~&0.6& Fortune cookies originated in China.
% &\ding{55}\\  
% \cdashline{2-4}[2pt/2pt]
% 		 \rule{0pt}{10pt}
%         ~&0.4&The fortune cookies originated in the United States.
% &\checkmark\\  
% 		\bottomrule % 底部线
% 	\end{tabular}}
% 	\label{tab_appen_example_6}
% \end{table} 





\end{document}
