@article{1,
  title={Groupreduce: Block-wise low-rank approximation for neural language model shrinking},
  author={Chen, Patrick and Si, Si and Li, Yang and Chelba, Ciprian and Hsieh, Cho-Jui},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{2,
  title={Online embedding compression for text classification using low rank matrix factorization},
  author={Acharya, Anish and Goel, Rahul and Metallinou, Angeliki and Dhillon, Inderjit},
  booktitle={Proceedings of the aaai conference on artificial intelligence},
  volume={33},
  number={01},
  pages={6196--6203},
  year={2019}
}

@article{3,
  title={Speeding up convolutional neural networks with low rank expansions},
  author={Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1405.3866},
  year={2014}
}

@article{4,
  title={Accelerating very deep convolutional networks for classification and detection},
  author={Zhang, Xiangyu and Zou, Jianhua and He, Kaiming and Sun, Jian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={38},
  number={10},
  pages={1943--1955},
  year={2015},
  publisher={IEEE}
}

@article{5,
  title={Exploiting linear structure within convolutional networks for efficient evaluation},
  author={Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{6,
  title={Language model compression with weighted low-rank factorization},
  author={Hsu, Yen-Chang and Hua, Ting and Chang, Sungen and Lou, Qian and Shen, Yilin and Jin, Hongxia},
  journal={arXiv preprint arXiv:2207.00112},
  year={2022}
}

@article{7,
  title={Pufferfish: Communication-efficient models at no extra cost},
  author={Wang, Hongyi and Agarwal, Saurabh and Papailiopoulos, Dimitris},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  pages={365--386},
  year={2021}
}

@article{ShearedLlama,
  title={Sheared llama: Accelerating language model pre-training via structured pruning},
  author={Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06694},
  year={2023}
}

@article{gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{llmpruner,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}

@article{lowrank_1,
  title={LQER: Low-Rank Quantization Error Reconstruction for LLMs},
  author={Zhang, Cheng and Cheng, Jianyi and Constantinides, George A and Zhao, Yiren},
  journal={arXiv preprint arXiv:2402.02446},
  year={2024}
}

@article{lowrank_2,
  title={Matrix compression via randomized low rank and low precision factorization},
  author={Saha, Rajarshi and Srivastava, Varun and Pilanci, Mert},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{lowrank_3,
  title={Lrq: Optimizing post-training quantization for large language models by learning low-rank weight-scaling matrices},
  author={Lee, Jung Hyun and Kim, Jeonghoon and Yang, June Yong and Kwon, Se Jung and Yang, Eunho and Yoo, Kang Min and Lee, Dongsoo},
  journal={arXiv preprint arXiv:2407.11534},
  year={2024}
}

@article{lowrank_4,
  title={Compressing Large Language Models using Low Rank and Low Precision Decomposition},
  author={Saha, Rajarshi and Sagan, Naomi and Srivastava, Varun and Goldsmith, Andrea J and Pilanci, Mert},
  journal={arXiv preprint arXiv:2405.18886},
  year={2024}
}

@inproceedings{sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}

@article{truth,
  title={The truth is in there: Improving reasoning in language models with layer-selective rank reduction},
  author={Sharma, Pratyusha and Ash, Jordan T and Misra, Dipendra},
  journal={arXiv preprint arXiv:2312.13558},
  year={2023}
}

