@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}


@article{1,
  title={Groupreduce: Block-wise low-rank approximation for neural language model shrinking},
  author={Chen, Patrick and Si, Si and Li, Yang and Chelba, Ciprian and Hsieh, Cho-Jui},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{2,
  title={Online embedding compression for text classification using low rank matrix factorization},
  author={Acharya, Anish and Goel, Rahul and Metallinou, Angeliki and Dhillon, Inderjit},
  booktitle={Proceedings of the aaai conference on artificial intelligence},
  volume={33},
  number={01},
  pages={6196--6203},
  year={2019}
}


@article{3,
  title={Speeding up convolutional neural networks with low rank expansions},
  author={Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1405.3866},
  year={2014}
}


@article{4,
  title={Accelerating very deep convolutional networks for classification and detection},
  author={Zhang, Xiangyu and Zou, Jianhua and He, Kaiming and Sun, Jian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={38},
  number={10},
  pages={1943--1955},
  year={2015},
  publisher={IEEE}
}

@article{5,
  title={Exploiting linear structure within convolutional networks for efficient evaluation},
  author={Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{6,
  title={Language model compression with weighted low-rank factorization},
  author={Hsu, Yen-Chang and Hua, Ting and Chang, Sungen and Lou, Qian and Shen, Yilin and Jin, Hongxia},
  journal={arXiv preprint arXiv:2207.00112},
  year={2022}
}

@article{truth,
  title={The truth is in there: Improving reasoning in language models with layer-selective rank reduction},
  author={Sharma, Pratyusha and Ash, Jordan T and Misra, Dipendra},
  journal={arXiv preprint arXiv:2312.13558},
  year={2023}
}

@article{7,
  title={Pufferfish: Communication-efficient models at no extra cost},
  author={Wang, Hongyi and Agarwal, Saurabh and Papailiopoulos, Dimitris},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  pages={365--386},
  year={2021}
}

@article{8,
  title={Speeding-up convolutional neural networks using fine-tuned cp-decomposition},
  author={Lebedev, Vadim and Ganin, Yaroslav and Rakhuba, Maksim and Oseledets, Ivan and Lempitsky, Victor},
  journal={arXiv preprint arXiv:1412.6553},
  year={2014}
}

@article{9,
  title={Acdc: A structured efficient linear layer},
  author={Moczulski, Marcin and Denil, Misha and Appleyard, Jeremy and de Freitas, Nando},
  journal={arXiv preprint arXiv:1511.05946},
  year={2015}
}

@inproceedings{10,
  title={Low-rank matrix factorization for deep neural network training with high-dimensional output targets},
  author={Sainath, Tara N and Kingsbury, Brian and Sindhwani, Vikas and Arisoy, Ebru and Ramabhadran, Bhuvana},
  booktitle={2013 IEEE international conference on acoustics, speech and signal processing},
  pages={6655--6659},
  year={2013},
  organization={IEEE}
}

@article{asvd,
  title={Asvd: Activation-aware singular value decomposition for compressing large language models},
  author={Yuan, Zhihang and Shang, Yuzhang and Song, Yue and Wu, Qiang and Yan, Yan and Sun, Guangyu},
  journal={arXiv preprint arXiv:2312.05821},
  year={2023}
}

@article{svdllm,
  title={Svd-llm: Truncation-aware singular value decomposition for large language model compression},
  author={Wang, Xin and Zheng, Yu and Wan, Zhongwei and Zhang, Mi},
  journal={arXiv preprint arXiv:2403.07378},
  year={2024}
}

@inproceedings{11,
  title={Efficient neural network compression},
  author={Kim, Hyeji and Khan, Muhammad Umar Karim and Kyung, Chong-Min},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12569--12577},
  year={2019}
}


@article{12,
  title={Compression of deep convolutional neural networks for fast and low power mobile applications},
  author={Kim, Yong-Deok and Park, Eunhyeok and Yoo, Sungjoo and Choi, Taelim and Yang, Lu and Shin, Dongjun},
  journal={arXiv preprint arXiv:1511.06530},
  year={2015}
}


@inproceedings{13,
  title={Coordinating filters for faster deep neural networks},
  author={Wen, Wei and Xu, Cong and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={658--666},
  year={2017}
}

@inproceedings{14,
  title={Losparse: Structured compression of large language models based on low-rank and sparse approximation},
  author={Li, Yixiao and Yu, Yifan and Zhang, Qingru and Liang, Chen and He, Pengcheng and Chen, Weizhu and Zhao, Tuo},
  booktitle={International Conference on Machine Learning},
  pages={20336--20350},
  year={2023},
  organization={PMLR}
}

@article{llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{wiki,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}


@article{c4,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}


@article{ptb,
  title={Building a large annotated corpus of English: The Penn Treebank},
  author={Marcus, Mitch and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  journal={Computational linguistics},
  volume={19},
  number={2},
  pages={313--330},
  year={1993}
}


@article{opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{wanda,
  title={A Simple and Effective Pruning Approach for Large Language Models}, 
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J. Zico},
  year={2023},
  journal={arXiv preprint arXiv:2306.11695}
}



@article{arc,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{truthqa,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}

@article{xsum,
  title={Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  journal={arXiv preprint arXiv:1808.08745},
  year={2018}
}


@article{squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, P},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}


@article{GPT4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}





@inproceedings{compression1,
  title={Model compression},
  author={Bucilu«é, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
  booktitle={Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={535--541},
  year={2006}
}

@article{compression2,
  title={A survey of model compression and acceleration for deep neural networks},
  author={Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
  journal={arXiv preprint arXiv:1710.09282},
  year={2017}
}

@article{compression3,
  title={A comprehensive survey on model compression and acceleration},
  author={Choudhary, Tejalal and Mishra, Vipul and Goswami, Anurag and Sarangapani, Jagannathan},
  journal={Artificial Intelligence Review},
  volume={53},
  pages={5113--5155},
  year={2020},
  publisher={Springer}
}





@article{gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}



@article{qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{awq,
  title={AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={87--100},
  year={2024}
}

@inproceedings{OPTQ,
  title={OPTQ: Accurate quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}


@article{llmpruner,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}


@inproceedings{sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}


@article{slicegpt,
  title={Slicegpt: Compress large language models by deleting rows and columns},
  author={Ashkboos, Saleh and Croci, Maximilian L and Nascimento, Marcelo Gennari do and Hoefler, Torsten and Hensman, James},
  journal={arXiv preprint arXiv:2401.15024},
  year={2024}
}

@article{ShearedLlama,
  title={Sheared llama: Accelerating language model pre-training via structured pruning},
  author={Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06694},
  year={2023}
}

@article{robust_diff_svd,
  title={Robust Differentiable SVD.},
  author={Wang, Wei and Dang, Zheng and Hu, Yinlin and Fua, Pascal and Salzmann, Mathieu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021}
}

@misc{wiki:The_devil_is_in_the_details,
   author = "Wikipedia",
   title = "{The devil is in the details} --- {W}ikipedia{,} The Free Encyclopedia",
   year = "2024",
   howpublished = {\url{http://en.wikipedia.org/w/index.php?title=The\%20devil\%20is\%20in\%20the\%20details&oldid=1233380555}},
   note = "[Online; accessed 24-November-2024]"
 }

@article{pkd,
  title={Polycystic kidney disease},
  author={Bergmann, Carsten and Guay-Woodford, Lisa M and Harris, Peter C and Horie, Shigeo and Peters, Dorien JM and Torres, Vicente E},
  journal={Nature reviews Disease primers},
  volume={4},
  number={1},
  pages={50},
  year={2018},
  publisher={Nature Publishing Group UK London}
}


@article{dynabert,
  title={Dynabert: Dynamic bert with adaptive width and depth},
  author={Hou, Lu and Huang, Zhiqi and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Liu, Qun},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9782--9793},
  year={2020}
}


@article{adabert,
  title={Adabert: Task-adaptive bert compression with differentiable neural architecture search},
  author={Chen, Daoyuan and Li, Yaliang and Qiu, Minghui and Wang, Zhen and Li, Bofang and Ding, Bolin and Deng, Hongbo and Huang, Jun and Lin, Wei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2001.04246},
  year={2020}
}


@article{SVD,
  title={The singular value decomposition: Its computation and some applications},
  author={Klema, Virginia and Laub, Alan},
  journal={IEEE Transactions on automatic control},
  volume={25},
  number={2},
  pages={164--176},
  year={1980},
  publisher={IEEE}
}


@inproceedings{IMAGE_SVD1,
  title={Image compression using SVD},
  author={Prasantha, HS and Shashidhara, HL and Murthy, KN Balasubramanya},
  booktitle={International conference on computational intelligence and multimedia applications (ICCIMA 2007)},
  volume={3},
  pages={143--145},
  year={2007},
  organization={IEEE}
}


@article{IMAGE_KSVD2,
  title={Compression of facial images using the K-SVD algorithm},
  author={Bryt, Ori and Elad, Michael},
  journal={Journal of Visual Communication and Image Representation},
  volume={19},
  number={4},
  pages={270--282},
  year={2008},
  publisher={Elsevier}
}



@article{COMM_SVD1,
  title={MIMO transmission over a time-varying channel using SVD},
  author={Lebrun, Guillaume and Gao, Jason and Faulkner, Mike},
  journal={IEEE Transactions on wireless Communications},
  volume={4},
  number={2},
  pages={757--764},
  year={2005},
  publisher={IEEE}
}



@article{SIG_SVD1,
  title={A novel strategy for signal denoising using reweighted SVD and its applications to weak fault feature enhancement of rotating machinery},
  author={Zhao, Ming and Jia, Xiaodong},
  journal={Mechanical Systems and Signal Processing},
  volume={94},
  pages={129--147},
  year={2017},
  publisher={Elsevier}
}


@article{SIG_SVD2,
  title={An efficient SVD-based method for image denoising},
  author={Guo, Qiang and Zhang, Caiming and Zhang, Yunfeng and Liu, Hui},
  journal={IEEE transactions on Circuits and Systems for Video Technology},
  volume={26},
  number={5},
  pages={868--880},
  year={2015},
  publisher={IEEE}
}




@article{openbook,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}

@article{winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}



@article{hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@inproceedings{piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={7432--7439},
  year={2020}
}

@article{mathqa,
  title={Mathqa: Towards interpretable math word problem solving with operation-based formalisms},
  author={Amini, Aida and Gabriel, Saadia and Lin, Peter and Koncel-Kedziorski, Rik and Choi, Yejin and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:1905.13319},
  year={2019}
}


@article{lmeval,
  title={A framework for few-shot language model evaluation},
  author={Gao, Leo and Tow, Jonathan and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and McDonell, Kyle and Muennighoff, Niklas and others},
  journal={Version v0. 0.1. Sept},
  volume={10},
  pages={8--9},
  year={2021}
}

@inproceedings{flap,
  title={Fluctuation-based adaptive structured pruning for large language models},
  author={An, Yongqi and Zhao, Xu and Yu, Tao and Tang, Ming and Wang, Jinqiao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={10},
  pages={10865--10873},
  year={2024}
}

@article{IPCA-1,
  title={Instrumented principal component analysis},
  author={Kelly, Bryan T and Pruitt, Seth and Su, Yinan},
  journal={Available at SSRN 2983919},
  year={2020}
}

@inproceedings{an2024fluctuation,
  title={Fluctuation-based adaptive structured pruning for large language models},
  author={An, Yongqi and Zhao, Xu and Yu, Tao and Tang, Ming and Wang, Jinqiao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={10},
  pages={10865--10873},
  year={2024}
}

@article{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}

@article{kim2023squeezellm,
  title={Squeezellm: Dense-and-sparse quantization},
  author={Kim, Sehoon and Hooper, Coleman and Gholami, Amir and Dong, Zhen and Li, Xiuyu and Shen, Sheng and Mahoney, Michael W and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2306.07629},
  year={2023}
}



@article{wu2024usable,
  title={Usable XAI: 10 strategies towards exploiting explainability in the LLM era},
  author={Wu, Xuansheng and Zhao, Haiyan and Zhu, Yaochen and Shi, Yucheng and Yang, Fan and Liu, Tianming and Zhai, Xiaoming and Yao, Wenlin and Li, Jundong and Du, Mengnan and others},
  journal={arXiv preprint arXiv:2403.08946},
  year={2024}
}

@misc{pp,
      title={Bypass Back-propagation: Optimization-based Structural Pruning for Large Language Models via Policy Gradient}, 
      author={Yuan Gao and Zujing Liu and Weizhong Zhang and Bo Du and Gui-Song Xia},
      year={2024},
      eprint={2406.10576},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.10576}, 
}



@article{lowrank_1,
  title={LQER: Low-Rank Quantization Error Reconstruction for LLMs},
  author={Zhang, Cheng and Cheng, Jianyi and Constantinides, George A and Zhao, Yiren},
  journal={arXiv preprint arXiv:2402.02446},
  year={2024}
}

@article{lowrank_2,
  title={Matrix compression via randomized low rank and low precision factorization},
  author={Saha, Rajarshi and Srivastava, Varun and Pilanci, Mert},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{lowrank_3,
  title={Lrq: Optimizing post-training quantization for large language models by learning low-rank weight-scaling matrices},
  author={Lee, Jung Hyun and Kim, Jeonghoon and Yang, June Yong and Kwon, Se Jung and Yang, Eunho and Yoo, Kang Min and Lee, Dongsoo},
  journal={arXiv preprint arXiv:2407.11534},
  year={2024}
}


@article{lowrank_4,
  title={Compressing Large Language Models using Low Rank and Low Precision Decomposition},
  author={Saha, Rajarshi and Sagan, Naomi and Srivastava, Varun and Goldsmith, Andrea J and Pilanci, Mert},
  journal={arXiv preprint arXiv:2405.18886},
  year={2024}
}



@article{sheared,
  title={Sheared llama: Accelerating language model pre-training via structured pruning},
  author={Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06694},
  year={2023}
}



@article{bosai,
  title={Everybody prune now: Structured pruning of llms with only forward passes},
  author={Dery, Lucio and Kolawole, Steven and Kagy, Jean-Fran{\c{c}}ois and Smith, Virginia and Neubig, Graham and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:2402.05406},
  year={2024}
}

@article{ipca,
  title={Candid covariance-free incremental principal component analysis},
  author={Weng, Juyang and Zhang, Yilu and Hwang, Wey-Shiuan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={25},
  number={8},
  pages={1034--1040},
  year={2003},
  publisher={IEEE}
}

@article{kim24openvla,
    title={OpenVLA: An Open-Source Vision-Language-Action Model},
    author={{Moo Jin} Kim and Karl Pertsch and Siddharth Karamcheti and Ted Xiao and Ashwin Balakrishna and Suraj Nair and Rafael Rafailov and Ethan Foster and Grace Lam and Pannag Sanketi and Quan Vuong and Thomas Kollar and Benjamin Burchfiel and Russ Tedrake and Dorsa Sadigh and Sergey Levine and Percy Liang and Chelsea Finn},
    journal = {arXiv preprint arXiv:2406.09246},
    year={2024},
} 

@inproceedings{llava,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26296--26306},
  year={2024}
}