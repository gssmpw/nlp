\section{Related Work}
\label{sec_appn_relatedWork}
\textbf{LLM Model Compression.}
Large language models (LLMs) typically contain billions of parameters, making inference on resource-constrained hardware challenging. To address this, researchers have developed various methods to compress models without requiring retraining. These methods can be categorized into three main categories: pruning, quantization, and low-rank decomposition. Specifically, pruning sets individual weights or structured components to zero without changing the overall structure of the LLM. For example, SparseGPT \citep{sparsegpt} prunes the least important weight elements by inverting the Hessian matrix. However, the irregular sparsity from unstructured pruning often fails to achieve significant speedup, only performing optimally on specific hardware architectures. LLM-Pruner \citep{llmpruner}, on the other hand, leverages a small dataset to estimate the coupled importance of weights, parameters, and groups, then applies LoRA-based pruning to recover accuracy. Yet, this approach significantly degrades model accuracy, especially at low compression ratios, due to the extensive modification of the weight matrices.  Sheared Llama \citep{ShearedLlama} performs extensive training on 50 billion tokens after compression.
Quantization, another approach, compresses the model by reducing the precision of the LLM’s weight matrices. For instance, GPTQ \citep{gptq} employs layer-wise quantization, updating weights using inverted Hessian information. The drawback of quantization is that it offers limited compression options, usually between 3 and 8 bits, which may not fully optimize memory utilization.

\textbf{SVD-based Model Compression.}
SVD-based LLMs compression has been widely explored \citep{3,4,5}. Earlier studies primarily used SVD to compress embedding layers \citep{1,2}. As model sizes have grown, research has shifted towards applying SVD to weight matrices \citep{6,7}. Recent findings \citep{truth} suggest that LLM weights are often approximated as low-rank matrices, highlighting SVD's potential for LLM compression. Traditional SVD focuses on compressing the original weight matrix by minimizing $|W - W'|$. However, since it does not account for parameter importance, it often results in significant performance degradation. These methods typically require fine-tuning to recover performance, which demands substantial computational resources for LLMs.
To mitigate this, recent works have focused on activation-aware SVD, which aims to minimize $|A - A'|$. For example, ASVD posits that the activation distribution influences compression error and scales $\Sigma W$ with a diagonal matrix $S$, where $S$ represents the input channel’s impact on weights. SVD-LLM argues that not all larger singular values are necessarily more important, introducing a truncation-aware whitening strategy to determine which singular values are critical for activations. 
However, current activation-aware SVD methods are limited to modifying $\Sigma W$ to adjust $W$, which restricts the values that $\hat{W}$ can take, failing to effectively retain activation information. For instance, ASVD is only effective at high compression ratios (0.8 and 0.9), suffering from significant performance loss at lower compression ratios. SVD-LLM, when compressed to a 0.4 ratio, causes PPL to drop from the original 7.94 to 42.3.


In addition, low-rank decomposition has also been applied in different forms for LLM compression methods. For instance, LQER \citep{lowrank_1} uses SVD to address quantization errors in the quantization process, while LRQ \citep{lowrank_3} applies SVD to enhance the sample handling capacity during training. CALDERA \citep{lowrank_4}, based on matrix compression method LPLR \citep{lowrank_2}, adopts non-SVD low-rank approaches for weight compression, but these typically lead to increased dimensions of the new weight matrices, resulting in performance degradation and necessitating the combination of quantization and LoRA fine-tuning.