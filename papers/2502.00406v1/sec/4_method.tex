
\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{fig/Figure9.png}
    \caption{We observe a significant increase in both Retain ROUGE and Forget ROUGE F-Scores on TOFU 10\% with Llama-3 8B as the number of samples (k) generated by the \emph{AuditErase agent} increases. Scores improve sharply up to $k=5$, supporting our claim that sampling multiple responses enhances unlearning efficacy. However, further increases in $k$ yield diminishing returns due to the associated computational cost.}
    \label{fig:f9}
    \vspace{-1\baselineskip}
\end{figure}

\section{Agentic LLM Unlearning}
\label{sec:4}
We introduce \texttt{ALU} as illustrated in Figure \ref{fig:alu_framework}, the first agentic pipeline for fine-grained unlearning in LLMs.  \texttt{ALU} employs four specialized agents to ensure both effective unlearning and preserved response utility.  Operating as a black box, \texttt{ALU} requires only the user query and a list of unlearning targets at inference time.  The agents process the response sequentially using few-shot prompting.  The four agents comprising \texttt{ALU} are:

\ding{182} \textbf{\emph{Vanilla agent.}} This agent resembles a standard language model without any unlearning framework. Provided with the prompt $Q$, the \emph{vanilla agent} responds with an answer $R_v$ that may potentially contain references to one or multiple subjects from the \emph{unlearning target set}. The inclusion of the \emph{vanilla agent} serves two critical objectives. 

\underline{\textit{Circumventing Jailbreaking:}} The \emph{vanilla agent} acts like a shock-absorber in our framework, nullifying the influences of adversarial prompts \cite{lynch2024eight} or state-of-the-art jailbreaking techniques \cite{anil2024many} due to the inherent design of the framework to not suppress any information in the first place.

\underline{\textit{Improves on Guardrailing:}} The initial state of our framework in the absence of the \emph{vanilla agent} closely resembles guardrailing techniques \cite{thaker2024guardrail}. However, we empirically demonstrate through various experiments in Section \ref{sec:5} that guardrailing is brittle to adversarial prompts. Including the \emph{vanilla agent} makes our framework more robust against such attacks. 

\ding{183} \textbf{\emph{AuditErase agent}} Once we have the response $R_v$ from the \emph{vanilla response}, the \emph{AuditErase agent} $M_f$ takes the first step towards fine-grained unlearning. This agent decomposes the task of unlearning into two phases - identification of the targets $T_v$ within $R_v$ with a potential reference of target $t$ in the \emph{forget set} $T$, and generating the first draft of the unlearned responses $R_f$ by eliminating references of $T_v$ from $R_v$. $$R_f \gets \{ r_i = M_f(R_v, t) \mid t \in T_v, i = 1, \dots, k \}$$
The decomposition of the unlearning task and generating $k$ different unlearned responses allows for fine-grained unlearning, addressing knowledge entanglement \cite{liu2024largelanguagemodelunlearning} and boosts model utility. We choose $k=5$ for our framework and illustrate the effect on unlearning efficacy and model utility with different values of $k$ in Figure \ref{fig:f9}.

\begin{algorithm}[t]
\caption{\texttt{ALU}}
\label{alg:unlearning-llm}
\begin{algorithmic}[1]
\REQUIRE $Q$ (prompt), $T = \{t_1, t_2, \dots, t_n\}$ (unlearning targets), $k$ (variations), $j$ (top responses)
\STATE Initialize $M_v$ (Vanilla Agent), $M_a$ (AuditErase Agent), $M_{cr}$ (Critic Agent), $M_{cp}$ (Composer Agent)
\STATE Define $\varphi$ as a null response.

\STATE \textbf{Step 1:} Generate vanilla response
\STATE $R_v \gets M_v(Q)$

\STATE \textbf{Step 2:} Audit vanilla response and erase target data
\STATE $T_v \gets \{t \in T \mid \text{potential reference of } t \text{ in } R_v\}$ \COMMENT{Identify targets present in $R_v$}
\STATE $R_a \gets \{ r_i = M_a(R_v, t) \mid t \in T_v, i = 1, \dots, k \}$

\STATE \textbf{Step 3:} Critic each response $r_i$ from Step 2 and provide a rating
\STATE For each response $r \in R_a$ and target $t \in T_v$:
\STATE \quad $s_{r,t} \gets M_{cr}(r, t, T)$ \COMMENT{Rate $r$ for target $t$ (1-5 scale)}
\STATE $S \gets \{s_{r,t} \mid r \in R_a, t \in T_v\}$ \COMMENT{Aggregate all ratings}

\STATE \textbf{Step 4:} Select top responses
\STATE $R_t \gets \text{Top-}j\text{ responses from } R_f \text{ based on } S$
\STATE $\bar{S} \gets \frac{1}{j} \sum_{r \in R_t} \text{Rating}(r)$
\vspace{1mm}
\STATE \textbf{Step 5:} Composer creates the final response $R_{\text{final}}$
\vspace{1mm}
\IF{$\bar{S} \geq 4$}
    \STATE $R_{\text{final}} \gets M_{cp}(R_t)$
\ELSE
    \STATE $R_{\text{final}} \gets \varphi$
\ENDIF

\STATE \textbf{Output:} $R_{\text{final}}$
\end{algorithmic}

\end{algorithm}

\ding{184} \textbf{\emph{Critic agent}} Most unlearning frameworks lack a fallback mechanism in cases where the unlearning fails \cite{pawelczyk2023context, thaker2024guardrail, liu2024largelanguagemodelunlearning, liu2024revisitingwhosharrypotter}. To address this limitation, we include a \emph{critic agent} $M_c$ with GPT-4o as the critic to ensure an unbiased and a thorough evaluation of the responses. This agent acts as a safety net, analyzing responses $r_i \in R_f$ and assigning a score $s \in [1, 5]$. The score reflects the effectiveness of the unlearning process, considering both the removal of $T_v$ and preservation of response utility. This discourages the model from responding with passive responses like \enquote{\texttt{I cannot answer that question}} in cases where the vanilla response $R_v$ can be reformatted to remove any reference to $T_v$ while maintaining the relevant information. Hence, for each response $r_i$, we have a score $s_i$ quantifying the unlearning effectiveness of the response. $$S = \{s \mid s = M_c(r, T_v, T), r \in R_f, s \in [1, 5] \}$$ 
\ding{185} \textbf{\emph{Composer agent}} The \emph{critic agent} generates the response-rating pairs ${(r_i, s_i) | i = 1, 2, \cdots, k}$, which now serves as an input to the \emph{composer agent}. The \emph{composer agent} then considers the $\text{Top-}j$ responses based on the corresponding ratings and computes the mean score $\bar{S}$ from the $j$ ratings. If $\bar{S}$ is beyond a predefined threshold, the \emph{composer agent} analyzes the $j$ responses and identifies the best aspects regarding response utility and unlearning efficacy for each of the $j$ responses. These aspects are then leveraged to compose the final response $R_{\text{final}}$. In case $\bar{S}$ does not satisfy the threshold, $R_{\text{final}}$ is set to a passive response $\varphi$ like \enquote{\texttt{I am sorry, I cannot respond to that}}.  This pipeline ensures that $R_{\text{final}}$ leaks no information pertaining to the targets in $T$ while aiming to maximize the response utility. 

