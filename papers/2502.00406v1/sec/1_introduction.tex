\section{Introduction}
The downstream applications of Large Language Models (LLMs) have skyrocketed over the last few years, owing to their capability to learn, and sometimes memorize massive text corpora \cite{grattafiori2024llama3herdmodels}, \cite{qwen2.5}, \cite{achiam2023gpt}, \cite{geminiteam2024gemini15unlockingmultimodal}. However, this also raises concerns of copyright infringement \cite{karamolegkou2023copyrightviolationslargelanguage}, \cite{henderson2023foundationmodelsfairuse}, privacy \cite{staab2024memorizationviolatingprivacyinference}, \cite{ippolito2023preventingverbatimmemorizationlanguage}, and spreading of hazardous information \cite{li2024wmdpbenchmarkmeasuringreducing}, \cite{harandizadeh2024riskresponselargelanguage}, \cite{fang2024llmagentsautonomouslyhack} to name a few. Regulatory guidelines \cite{digital2023dpdpa},
\cite{oag2021ccpa}, \cite{european2016gdpr}, have been introduced to safeguard user data, mandating corporations to remove data on demand. To address this, organizations need methodologies without requiring model retraining on an edited corpus, and machine unlearning \cite{Xu_2024}, \cite{Chundawat_2023}, \cite{Tarun_2024}, \cite{chundawat2023badteachinginduceforgetting} has emerged as a paradigm for its ability to meet this need effectively.\par

Unlearning specific pieces of information in multi-billion parameter LLM models comes with a lot of challenges such as catastrophic forgetting in models due to gradient updates \cite{aghajanyan2020betterfinetuningreducingrepresentational}, \cite{zhang2024negativepreferenceoptimizationcatastrophic}, \cite{gu2024modeleditingharmsgeneral} and vulnerability to adversarial attacks on the model \cite{anil2024many}, \cite{schwinn2024soft}. Moreover, the limited access to model weights in many cases poses significant challenges to the effectiveness of conventional unlearning methods~\cite{neel2020descenttodeletegradientbasedmethodsmachine}.

Effective unlearning in LLMs requires a balance of unlearning efficacy, response quality, robustness, and efficiency. Recent research has explored methods without retraining~\cite{achiam2023gpt,jang2022knowledge} to unlearn certain information in LLMs. A majority of the existing research in LLM unlearning focuses on parameter fine-tuning the base LLM \cite{wang2023kgageneralmachineunlearning,li2024wmdpbenchmarkmeasuringreducing,eldan2023whosharrypotterapproximate,liu2024revisitingwhosharrypotter,jia2024soul} leveraging a \emph{forget dataset} to remove the information present in it, and a \emph{retain dataset} to maintain model utility. \cite{pawelczyk2023context} introduced in-context unlearning without making any changes to the model parameters, and similar attempts \cite{thaker2024guardrail} have since been made towards achieving unlearning efficacy comparable with the gradient-based approaches.\par

The knowledge entanglement arising from the interrelated facts in unlearning requests~\cite{wu2024evaluating} represents another challenge where prior work falls short. The deletion of knowledge on a particular topic requires the model to edit its remaining knowledge that is entangled with the topics to be removed \cite{maini2024tofu, lynch2024eight, eldan2023whosharrypotterapproximate}. In Table \ref{tab:t8}, we demonstrate how careful prompting can be used to exploit the interlacing of knowledge in LLMs and retrieve information that was supposed to have been forgotten.\par

Current unlearning methods struggle to balance the aforementioned aspects of unlearning and often compromise an aspect to improve another. We introduce an agentic LLM-based unlearning framework to unlearn a given set of information in an LLM without having to update model parameters or have an elaborate prompting setup \cite{pawelczyk2023context}. To the best of our knowledge~\cite{liu2024rethinking}, this is the first work introducing an agentic LLM unlearning (\texttt{ALU}), involving multiple LLM agents to curate various steps of the unlearning process. Our method performs \emph{targeted unlearning} \cite{liu2024revisitingwhosharrypotter} on a user-provided set of targets by involving four LLM agents to filter the unlearned vanilla response at each stage. Each agent analyzes the response of the last agent and performs designated actions on those responses, passing its output to the next agent in line. The entire framework is based on localizing each agent to a particular task using few-shot prompting \cite{brown2020languagemodelsfewshotlearners}, ensuring that they are not contaminated with the processing steps employed by the last agent to reach its conclusion. We find that the method of using a single LLM to filter content from prompts \cite{thaker2024guardrail} is brittle to cleverly constructed prompts that can bypass the filter by not referring to the unlearning subjects directly. We evaluate our framework on three leading benchmarks -\textsc{WPU}~\cite{liu2024revisitingwhosharrypotter},
\textsc{TOFU}~\cite{maini2024tofu}, and \textsc{WMDP}~\cite{li2024wmdpbenchmarkmeasuringreducing}, with the results demonstrating the robustness and flexibility of our method. Apart from the standard evaluations on these benchmarks, we demonstrate the robustness of our framework against state-of-the-art jailbreaking techniques \cite{anil2024many, lynch2024eight}, convoluted prompts aimed at extracting sensitive data from the model, translating questions in multiple languages, and scaling the number of unlearning targets to a thousand which has never been explored by any work prior to ours. Our work contributes as follows:

\ding{182} \textbf{Agentic LLM unlearning paradigm}: We are the first to introduce an agentic framework employing multiple dedicated LLM agents to unlearn, achieving performance that is either on par or exceeds the existing state-of-the-art LLM unlearning methods both in forget and retain data performance.

\ding{183} \textbf{Zero-setup framework}: \texttt{ALU} is easy to set up, requiring only the prompt and a list of topics to be unlearned. No part of the framework requires additional fine-tuning or elaborate prompting setups.

\ding{184} \textbf{Flexibility and transparency}: The framework is customizable for practical settings. The choice of base LLM is changeable, and for special needs, the agents and their behaviors can be customized. \texttt{ALU} is model-agnostic, allowing any LLM to be leveraged irrespective of the number of parameters. We test \texttt{ALU} on models as small as 2B parameters \cite{gemmateam2024gemma2improvingopen} as well as proprietary LLMs \cite{achiam2023gpt}, outperforming existing methods in similar model sizes.

\ding{185} \textbf{Scalability}: \texttt{ALU} scales with an increasing number of unlearning targets and maintains its efficacy. This is non-trivial in an unlearning setting due to knowledge entanglement \cite{liu2024largelanguagemodelunlearning,wu2024evaluating} between the unlearning targets and remaining information. We scale the \emph{forget set} up to 1000 unlearning targets and illustrate the superior performance of \texttt{ALU} (Section \ref{sec:5.4}) against other methods.
