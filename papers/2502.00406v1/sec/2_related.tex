\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/figure_ALU.png}
    \caption{\textbf{Using LLM agents for fine-grained post hoc unlearning.} The query \enquote{\textit{How was Victor Krum's experience at the Yule Ball?}} is challenging due to indirect references to the unlearning target \textbf{Hermione Granger} in the response. The \textbf{Vanilla Agent} generates an initial, unmodified response. \textbf{AuditErase} detects the target reference in this response and generates $k$ sanitized variations. The \textbf{Critic} evaluates these responses on a 1â€“5 scale, and the \textbf{Composer} synthesizes the top-$j$ rated outputs into the final response.}
    \label{fig:alu_framework}
\end{figure*}
\section{Related Work}
\textbf{Optimization-based unlearning.} Most studies on unlearning in LLMs target weight manipulation~\cite{yao2024largelanguagemodelunlearning,liu2024revisitingwhosharrypotter,jang2022knowledge,jia2024soul}. This approach revolves around maximizing the negative log-likelihood function over a sequence of tokens. Although this approach is promising, altering weights often compromises other aspects, like model utility in general.~\cite{liu2024revisitingwhosharrypotter} points out how gradient ascent \cite{yao2024largelanguagemodelunlearning} generates gibberish since updating weights does not allow one to have precise control over topics being unlearned and retained. To counter this issue to some extent, recent methods perform the unlearning training on a \emph{forget set}, along with a separate representative \emph{retain set}, which prevents the model from catastrophic forgetting~\cite{liu2024revisitingwhosharrypotter,maini2024tofu,sinha2024unstarunlearningselftaughtantisample}. This approach works in preserving the model utility at the cost of training time and compute. In the case of large LLMs, we don't have access to the full dataset. Thus, these methods are bound to fail in maintaining retain utility after model weight alteration. Altering the prediction loss~\cite{zhang2024negativepreferenceoptimizationcatastrophic,liu2024revisitingwhosharrypotter} with a subset of this paradigm focusing on minimizing some form of KL divergence between a \emph{teacher distribution} and the distribution of a \emph{student model} ~\cite{liu2024revisitingwhosharrypotter} are some other methods.\par

\textbf{Post hoc unlearning.} These methods aim to achieve unlearning without requiring access to LLM weights~\cite{pawelczyk2023context}, significantly reducing time and compute compared to optimization-based approaches.~\citet{pawelczyk2023context,kuwana2024blackboxforgetting,muresanu2024unlearnablealgorithmsincontextlearning} modify the prompt, perturbing it to remove traces of the unlearning knowledge from the LLM response in a post hoc manner.~\citet{thaker2024guardrail} uses a different LLM to guardrail the responses from the base LLM, employing prompt prefixes to analyze and edit responses containing information about the unlearning subjects. These methods are more susceptible to jailbreaking attacks \cite{anil2024many, lynch2024eight, mangaokar2024prppropagatinguniversalperturbations, rao2024trickingllmsdisobedienceformalizing}, decreasing their utility in a practical setting. Moreover, cleverly constructed prompts can bypass the guardrailing as designed in \citet{thaker2024guardrail}. This highlights the need for a more sophisticated post hoc approach that retains the advantages of typical post hoc approaches while remaining fairly robust to adversarial attacks.\par

%While the approach of post hoc unlearning appeals to be a lot more pragmatic for practical usages than most of the gradient-based methods,
%Another line of approach can be observed in federated post hoc unlearning \cite{chundawat2024condafastfederatedunlearning}, where the authors selectively dampen the global parameters most affected by forget clients to retain model utility.

%Each agent can be assigned a specific role with distinct specializations in the context of the task. 

\textbf{LLM agents.} LLMs have demonstrated exceptional reasoning capabilities through few-shot prompting \cite{wei2023chainofthoughtpromptingelicitsreasoning, yao2023treethoughtsdeliberateproblem, Besta_2024, ganguly2024proofthoughtneurosymbolic, wang2023selfconsistencyimproveschainthought, zhang2024diagramthought}. This success has inspired the development of multi-agent LLM frameworks, designed to emulate herd intelligence by functioning as a collective of specialized agents, each contributing unique expertise to complex problem-solving tasks. In gaming related problems~\cite{wang2023voyageropenendedembodiedagent, xu2024exploringlargelanguagemodels, light2023avalonbenchevaluatingllmsplaying}, a common theme is to assign specific roles to LLM agents, letting them explore the virtual environment and leverage their specialties to overcome the different hurdles encountered. Effective multi-agent systems often require inter-agent communication, as seen in multi-agent debates \cite{du2023improvingfactualityreasoninglanguage,Xiong_2023,chan2023chatevalbetterllmbasedevaluators} shedding light on the communication among agents for re-evaluation. \citet{chan2023chatevalbetterllmbasedevaluators} highlights the importance of assigning specific roles to the agents, which boosted performance against a single-agent setting in a debating framework. \cite{wang2024describeexplainplanselect} decomposes a larger problem into smaller sub-problems to navigate through Minecraft. In this paper, we view unlearning in a similar
light, filtering a response with multiple LLM agents to ensure reliable unlearning, even against common jailbreaking attacks.



%which is further inspected by an \emph{explainer agent} and a \emph{planner agent} to locate the errors in the initial plan.