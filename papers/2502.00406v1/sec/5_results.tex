\section{Experiments}
\label{sec:5}

\begin{table}[t]
    \centering
    \scriptsize
    \caption{Multiple-choice accuracy of optimization-based methods against \texttt{ALU} on the forget benchmark (\textbf{WMDP}) and the retain benchmark (\textbf{MMLU}) with Llama-3 8B. \texttt{ALU} achieves close to random guess scores across all splits on WMDP, and maintains utility on MMLU.}
    \begin{tabular}{l|cccc}
        \toprule
        \textbf{Method} & \textbf{Bio} $\downarrow$ & \textbf{Chem} $\downarrow$ & \textbf{Cyber} $\downarrow$ & \textbf{MMLU} $\uparrow$\\
        \midrule
        Original & 64.57 & 48.61 & 43.22 & 58.94 \\
        Grad Ascent & 53.6 & 43.53 & 44.17 & 57.70\\
        SCRUB & 59.76 & 41.66 & 39.42 & 44.85\\
        SSD & 43.72 & 40.72 & 39.57 & 51.33\\
        RMU & 29.70 & 47.24 & 28.39 & \textbf{57.81}\\
        SNAP & 33.42 & 49.78 & 26.31 & 52.46\\
        \texttt{ALU} & \textbf{26.31} & \textbf{25.12} & \textbf{24.76} & 57.64\\
        \midrule
        Random Guessing & 25.0 & 25.0 & 25.0 & 25.0\\
        \bottomrule
    \end{tabular}
    \label{tab:t1}
    \vspace{-1\baselineskip}
\end{table}

\begin{table}[]
    \centering
    % \footnotesize
    \caption{Comparison of post hoc methods using Cosine Similarity and ROUGE Metrics with Qwen-2.5 14B with TOFU 10\%, WMDP-chem, and WPU. \texttt{ALU} outperforms competing methods in both unlearning and retaining knowledge. While Guardrail generally surpasses ICUL, its performance varies across datasets. \texttt{ALU} experiences a minor Retain score decrease on WMDP, likely due to knowledge entanglement.}
    \label{tab:comparison_methods}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{llccc|ccc}
        \toprule
        \textbf{Data}&\textbf{Method} & \multicolumn{3}{c}{\textbf{Cosine Similarity}} & \multicolumn{3}{c}{\textbf{ROUGE}} \\
        \cmidrule(lr){3-5} \cmidrule(lr){6-8}
         & & \textbf{Pre-UL} $\uparrow$ & \textbf{Post-UL} $\downarrow$ & \textbf{Retain} $\uparrow$ & \textbf{Pre-UL} $\uparrow$ & \textbf{Post-UL} $\downarrow$ & \textbf{Retain} $\uparrow$ \\
        \midrule
        &ICUL & 0.935 & 0.837 & 0.860 & 0.853 & 0.478 & 0.492 \\
        TOFU &Guardrail & 0.990 & 0.621 & 0.879 & 0.975 & 0.263 & 0.562 \\
        &\texttt{ALU}  & 0.976 & \textbf{0.134} & \textbf{0.912} & 0.945 & \textbf{0.057} & \textbf{0.761} \\
        \midrule
        &ICUL  & 0.943 & 0.399 & 0.460 & 0.900 & 0.112 & 0.410 \\
        WMDP & Guardrail  & 0.940  & 0.610 & \textbf{0.594} & 0.920 & 0.272 & \textbf{0.609} \\
        &\texttt{ALU} & 1.000  & \textbf{0.045} & 0.572 & 1.000 & \textbf{0.000} & 0.560 \\
        \midrule
        &ICUL  & 1.000 & 0.447 & 0.810 & 1.000 & 0.227 & 0.790 \\
        WPU &Guardrail & 1.000 & 0.381 & 0.656 & 1.000 & 0.115 & 0.553 \\
        &\texttt{ALU} & 1.000 & \textbf{0.076} & \textbf{0.972} & 1.000 & \textbf{0.000} & \textbf{0.986} \\
        \bottomrule
    \end{tabular}
    }
\label{tab:t2}  
\vspace{-1\baselineskip}
\end{table}

We present the findings by comparing our framework with existing optimization-based unlearning methods (Section \ref{sec:5.1}), post hoc methods (Section \ref{sec:5.2}), against various perturbations/attacks (Section \ref{sec:5.3}), scaling the frameworks up to 1000 unlearning targets (Section \ref{sec:5.4}), and highlight the practicality of post hoc unlearning in Section \ref{sec:5.5}.

\textbf{Dataset.} We evaluate the competency of \texttt{ALU} against other leading unlearning methods on three benchmark datasets - TOFU \cite{maini2024tofu}, WPU \cite{liu2024revisitingwhosharrypotter}, and WMDP \cite{li2024wmdpbenchmarkmeasuringreducing}. TOFU is a synthetic dataset of fictional author profiles for unlearning. The dataset is primarily divided into three forget sets - \verb|forget01|, \verb|forget05|, \verb|forget10|, in ascending order of unlearning targets. WPU consists of real historical profiles as unlearning targets. We evaluate the frameworks on the \verb|forget100| portion of the dataset consisting of 100 unlearning targets and question-answer pairs related to them. WMDP is the leading benchmark for evaluating unlearning methods for removing hazardous knowledge, which is critical for a framework deployed in practical settings. We evaluate the frameworks on all three subsets of WMDP, namely \verb|wmdp-chem|, \verb|wmdp-bio|, and \verb|wmdp-cyber|. We also test the model utility of the unlearning frameworks on MMLU \cite{hendrycks2021measuringmassivemultitasklanguage}, which serves as the retain dataset.

% We evaluate the competency of \texttt{ALU} against other leading unlearning methods on three benchmarks - TOFU \cite{maini2024tofu}, WPU \cite{liu2024revisitingwhosharrypotter}, and WMDP \cite{li2024wmdpbenchmarkmeasuringreducing}. TOFU is a synthetic dataset of fictional author profiles for unlearning. The dataset is primarily divided into three forget sets - \verb|forget01|, \verb|forget05|, \verb|forget10|, in ascending order of unlearning targets. Table \ref{tab:t2} and Table \ref{tab:t3} are evaluations made on the \verb|forget10| portion of TOFU since that consists of the largest set of unlearning targets, but evaluations on the other two portions can be found in Table \textbf{30} and Table \textbf{31}. Unlike TOFU, WPU consists of real historical profiles as unlearning targets. We evaluate the frameworks on the \verb|forget100| portion of the dataset consisting of 100 unlearning targets and question-answer pairs related to them. WMDP is the leading benchmark for evaluating unlearning methods for removing hazardous knowledge, which is critical for a framework deployed in practical settings. In Table \ref{tab:t1}, we evaluate the frameworks on all three subsets of WMDP, namely \verb|wmdp-chem|, \verb|wmdp-bio|, and \verb|wmdp-cyber|. For the rest of the evaluations, we mainly use \verb|wmdp-chem|. We also test the model utility of the unlearning frameworks on MMLU \cite{hendrycks2021measuringmassivemultitasklanguage}, which serves as the retain dataset in Table \ref{tab:t1}.

\begin{table*}[t]
\footnotesize
    \centering
    \caption{GPT Privacy score on WHP for GPT-4o, Qwen 2.5 14B, and Llama 3.2 3B against various perturbations for circumventing unlearning frameworks. We observe that the model size matters and the smaller 3B model is compromised for ICUL and Guardrail. For Jailbreak prompts, we notice a drop in the scores for \texttt{ALU}, which can be attributed to the compromise in response quality.}
    \begin{tabular}{l|ccc|ccc|ccc}
        \toprule
        \textbf{Perturbation}&\multicolumn{3}{c}{\textbf{ICUL}}&\multicolumn{3}{c}{\textbf{Guardrail}}&\multicolumn{3}{c}{\texttt{ALU}}\\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
        & \textbf{GPT} & \textbf{Llama} & \textbf{Qwen} & \textbf{GPT} & \textbf{Llama} & \textbf{Qwen} & \textbf{GPT} & \textbf{Llama} & \textbf{Qwen} \\
        \midrule
        None & 5.375 & 3.000 & 3.500 & 8.125 & 4.125 & 6.725 & \textbf{9.500} & \textbf{8.500} & \textbf{9.225}\\
        Target Masking & 3.500 & 1.830 & 2.225 & 4.500 & 2.000 & 2.125 & \textbf{9.500} & \textbf{8.160} & \textbf{9.160}\\
        Jailbreak Prompts & 4.330 & 2.830 & 3.666 & 5.000 & 3.830 & 5.000 & \textbf{8.000} & \textbf{7.330} & \textbf{7.830}\\
        Other Languages & 5.375 & 1.125 & 3.000 & 6.500 & 3.225 & 4.750 & \textbf{9.500} & \textbf{6.000} & \textbf{8.750} \\
        Many-shot jailbreaking & 2.670 & 1.000 & 1.750 & 6.333 & 2.000 & 4.125 & \textbf{9.000} & \textbf{7.830} & \textbf{8.830} \\
        \bottomrule
    \end{tabular}
    \label{tab:t4}
    \vspace{-1\baselineskip}
\end{table*}

\textbf{Large Language models.} To demonstrate the efficacy of \texttt{ALU} across models of different sizes and architectures, we include evaluations on \textbf{31 different LLMs} of a wide range of model sizes (2B, 3.8B, 7B, 8B, 9B, 13B, 14B, 16B, 32B, 40B, 70B, 72B) (Table \ref{tab:t9} - Table \ref{tab:t32}), including Qwen \cite{qwen2.5}, Llama \cite{grattafiori2024llama3herdmodels}, and GPT-4o \cite{achiam2023gpt}, Gemma \cite{gemmateam2024gemma2improvingopen},DeepSeek \cite{guo2024deepseekcoderlargelanguagemodel}, Falcon \cite{almazrouei2023falconseriesopenlanguage}, and Phi \cite{abdin2024phi4technicalreport}. %These three models represent the range of model sizes in which we expect the unlearning frameworks to maintain their efficacy.

\textbf{Metrics.} For evaluations done on WMDP \cite{li2024wmdpbenchmarkmeasuringreducing}, we follow the approach of \citet{li2024wmdpbenchmarkmeasuringreducing}, \citet{liu2024largelanguagemodelunlearning} and chose \textbf{Multiple-choice accuracy} as the primary metric. An unlearned model should have an accuracy close to random guess, that is 0.25 for WMDP, since each question has four options. We evaluate the unlearning and retaining efficacy with \textbf{ROUGE-L} scores~\cite{lin-2004-rouge} that computes the similarities of model responses with the oracle answers and is a standard metric for unlearning evaluation~\cite{liu2024largelanguagemodelunlearning, maini2024tofu, liu2024revisitingwhosharrypotter, sinha2024unstarunlearningselftaughtantisample}. We also consider the \textbf{cosine similarity} scores of the responses with the oracle answers to capture the semantic similarity. We show \textbf{F-Score} of the \textit{retain} ROUGE and the \textit{forget} ROUGE scores to strike a balance between the contrastive ROUGE scores. For a more fine-grained evaluation, we consider the \textbf{GPT-Privacy Score} \cite{liu2024revisitingwhosharrypotter, sinha2024unstarunlearningselftaughtantisample} since GPT-4o \cite{achiam2023gpt} as a judge can capture the presence of indirect references to a target in a response. Refer to Appendix \ref{sec:B1} for more details about the metrics used.

% For evaluations done on WMDP \cite{li2024wmdpbenchmarkmeasuringreducing} in Table \ref{tab:t1}, we follow the approach of \citet{li2024wmdpbenchmarkmeasuringreducing}, \citet{liu2024largelanguagemodelunlearning} and chose \textbf{Multiple-choice accuracy} as the primary metric. An unlearned model should have an accuracy close to random guess, that is 0.25 for WMDP, since each question has four options. In Table \ref{tab:t2} and Table \ref{tab:t3}, we evaluate the unlearning and retaining efficacy of the methods with the \textbf{ROUGE-L} scores \cite{lin-2004-rouge} that computes the similarities of model responses with the oracle answers and is a standard metric for unlearning evaluation \cite{liu2024largelanguagemodelunlearning, maini2024tofu, liu2024revisitingwhosharrypotter, sinha2024unstarunlearningselftaughtantisample}. In Table \ref{tab:t2}, we also consider the \textbf{Cosine similarity} scores of the responses with the oracle answers to capture the semantic similarity. \textbf{F-Score} of the Retain ROUGE and the Forget ROUGE scores have been highlighted in Table \ref{tab:t3} to strike a balance between the contrastive ROUGE scores. For a more fine-grained evaluation in Table \ref{tab:t4}, we consider the \textbf{GPT-Privacy Score} \cite{liu2024revisitingwhosharrypotter, sinha2024unstarunlearningselftaughtantisample} since GPT-4o \cite{achiam2023gpt} as a judge can capture the presence of indirect references to a target in a response. Refer to Appendix \ref{sec:B1} for more details about the metrics used.

\subsection{Comparison with Optimization-Based Methods}
\label{sec:5.1}
Unlearning on a set of targets by optimizing a model on some form of loss makes up most of the literature in machine unlearning \cite{yao2024largelanguagemodelunlearning, fan2024salunempoweringmachineunlearning}, \cite{kurmanji2023unboundedmachineunlearning}, \cite{maini2024tofu}, \cite{zhang2024negativepreferenceoptimizationcatastrophic}, \cite{choi2024optoutinvestigatingentitylevelunlearning}. Despite their computational cost, optimization-based methods have consistently outperformed the post hoc methods in the quality of unlearning the targets. To test the competency of \texttt{ALU} against optimization-based methods, we finetune Llama-2 7B \cite{touvron2023llama2openfoundation} on the TOFU dataset \cite{maini2024tofu} and compare the ROUGE-L \cite{lin-2004-rouge} scores of eight baselines against our framework in Table \ref{tab:t3}. Since \texttt{ALU} is a post hoc framework, we provide a list of the same unlearning targets to the framework at inference, which is defined in the \emph{forget set} of the optimization methods.

\textbf{\texttt{ALU} maximizes unlearning efficacy and model utility.} \texttt{ALU} outperforms the other methods in retaining information not present in the forget set while maintaining competency in unlearning the desired targets. We observe that although methods like Gradient difference \cite{fan2024salunempoweringmachineunlearning}, \cite{kurmanji2023machineunlearninglearneddatabases} and KL minimization \cite{maini2024tofu} are better at forgetting information than \texttt{ALU}, they compromise on retaining the information not present in the \emph{forget set} since weight updation costs fine-grained control on the behavior of the framework. Since a performant unlearning method should reconcile the need to effectively forget the target information and preserve knowledge about other relevant information, we also consider the harmonic mean of the Retain and Forget ROUGE-L scores. \texttt{ALU} performs better than the optimization-based methods in balancing forgetting and retaining efficacy while being a post hoc framework.

\begin{table}[t]
\tiny
\footnotesize
\centering
\caption{Optimization-based methods on TOFU 10\% against \texttt{ALU} with Llama-2 7B, \texttt{ALU} outperforms the other metrics in Retain scores, and achieves the best balance between unlearning efficacy and response utility. This can be attributed to the design of the framework which allows for fine-grained response editing, thus enhancing response utility.}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|ccc}
\toprule
\textbf{Method} & \textbf{Retain ROUGE} $\uparrow$ & \textbf{Forget ROUGE} $\downarrow$ & \textbf{F-Score} $\uparrow$ \\
\midrule
Grad Ascent & 0.0000 & 0.0000 & 0.0000\\
Grad Diff & 0.4906 & \textbf{0.0032} & 0.6581\\
KL Min & 0.0046 & 0.0049 & 0.0097\\
Pref Opt & 0.7528 & 0.0602 & 0.8359\\
NPO & 0.2238 & 0.2010 & 0.3497\\
NPO-KL & 0.3370 & 0.2483 & 0.4665\\
NPO-RT & 0.4502 & 0.2380 & 0.5660\\
SNAP & 0.6378 & 0.1136 & 0.7418\\
\texttt{ALU} & \textbf{0.7718} & 0.0540 & \textbf{0.8500}\\
\bottomrule
\end{tabular}%
}
\label{tab:t3}
\vspace{-2\baselineskip}
\end{table}

\textbf{\texttt{ALU} is consistent in cross-domain performance.}
For a more comprehensive evaluation, we evaluate the multiple-choice accuracy of \texttt{ALU} on WMDP \cite{li2024wmdpbenchmarkmeasuringreducing} against Gradient Ascent \cite{yao2024largelanguagemodelunlearning}, SCRUB \cite{kurmanji2023unboundedmachineunlearning}, SSD \cite{foster2023fastmachineunlearningretraining}, RMU \cite{li2024wmdpbenchmarkmeasuringreducing} and SNAP \cite{sarlin2023snapselfsupervisedneuralmaps}. Ideally, the accuracy of a performant unlearning framework should be close to random guessing,  indicating minimal knowledge of the options provided with the query (refer to Appendix \ref{sec:B1} for more details). Table \ref{tab:t1} demonstrates that \texttt{ALU} yields the scores closest to random guessing scores, while maintaining an almost perfect score on MMLU \cite{hendrycks2021measuringmassivemultitasklanguage}, which serves as our retain dataset. Table \ref{tab:t32} displays the performance of \texttt{ALU} on WMDP and MMLU with 31 different models. 

\subsection{Comparison with Post Hoc Methods}
\label{sec:5.2}
Post hoc methods are motivated by the need for computationally and time-efficient alternatives to optimization-based unlearning techniques. 

\textbf{\texttt{ALU} substantially outperforms existing post hoc methods.} We compare \texttt{ALU} against ICUL \cite{pawelczyk2023context} and Guardrailing \cite{thaker2024guardrail} on TOFU(10\%), WMDP-chem \cite{li2024wmdpbenchmarkmeasuringreducing} and WPU \cite{liu2024revisitingwhosharrypotter} using Qwen-2.5 14B \cite{qwen2.5}. Table \ref{tab:t2} lists the pre-unlearning and post-unlearning ROUGE-L scores and cosine similarity scores, along with the scores on the \emph{retain set}. We observe that \texttt{ALU} consistently outperforms the other two baselines in terms of both unlearning and retaining efficacy across all three datasets. While simple guardrailing is better than ICUL at forgetting information in most cases, retaining efficacy is limited and comparable to ICUL.

\textbf{\texttt{ALU} takes knowledge entanglement into account while unlearning.} It is worth noting that the retained scores of all three methods on WMDP-chem are limited as compared to TOFU and WPU, with the limitation more pronounced for \texttt{ALU} given its performance on the other two datasets. We checked \texttt{ALU}'s retaining responses manually, revealing that its lack of performance can be attributed to \textbf{knowledge entanglement} \cite{MCCLOSKEY1989109}, \cite{liu2024largelanguagemodelunlearning}, \cite{maini2024tofu}, which is not observed in the other two datasets. Including the name of a certain chemical compound in the \emph{forget list} for \texttt{ALU} prevents it from answering questions that are indirectly related to the compound, which is a desirable behavior in an unlearning framework. Instances of such knowledge entanglement in WMDP-chem have been discussed in Appendix \ref{sec:wmdp-chem}. We reproduce the same table with 20 more models of varying sizes in Table \ref{tab:t32} to check for the consistency of our framework across model sizes and architectures. A more rigorous evaluation of the methods across model sizes has been done in Section \ref{sec:5.3}.


% Requires: \usepackage{booktabs}

\begin{figure*}[t]
     \centering
     \includegraphics[width=15cm, height = 4.4cm]{fig/Figure1.png}
     \caption{A comparative analysis of seven unlearning frameworks across five key criteria with Qwen-2.5 14B as the base model on WPU. A higher score is better across all criteria. Methods like GA and NPO display deterioration in Unlearning Efficacy and Adversial Robustness when scaled from 20 to 100 targets, while others like WHP demonstrated a decrease in Response Quality and Model Utility. In contrast, \texttt{ALU} performs consistently across all criteria.}
     \label{fig:f1}
     \vspace{-1\baselineskip}
\end{figure*}

\subsection{Controlled Experiments}
\label{sec:5.3}
Existing unlearning benchmarks like WMDP \cite{li2024wmdpbenchmarkmeasuringreducing} and TOFU \cite{maini2024tofu} may not fully capture real-world data complexities, particularly the intricate relationships between concepts. The targets in these datasets are typically unrelated which does not adequately represent the intricate relationships and overlapping concepts often encountered in the real world. To evaluate \texttt{ALU} in such practical settings, we utilize targets from Harry Potter books which we assess \texttt{ALU} alongside two other post hoc methods - ICUL \cite{pawelczyk2023context} and guardrailing \cite{thaker2024guardrail} on GPT-4o \cite{achiam2023gpt}, Qwen-2.5 14B \cite{qwen2.5}, and Llama-3.2 3B \cite{grattafiori2024llama3herdmodels} demonstrating \texttt{ALU}'s model-agnostic nature.
For these experiments, we have chosen \textbf{GPT privacy scores} as the evaluation metric \cite{liu2024revisitingwhosharrypotter}, \cite{sinha2024unstarunlearningselftaughtantisample}. 

\ding{182} \textbf{None} refers to prompts that make naive attempts at extracting information related to the unlearning targets. These prompts explicitly included target names from the \emph{forget set}, simplifying target identification and removal for the frameworks. ICUL \cite{pawelczyk2023context} exhibits performance limitations on advanced models like GPT-4o, leaking information without explicit mentions, which ROUGE-L may miss. Llama-3.2 being a smaller model exhibits a significant gap in performance across both ICUL and guardrailing \cite{thaker2024guardrail} compared to GPT-4o. Notably, \texttt{ALU} bridges the gap to a large extent, indicating its robustness across model sizes. 

\ding{183} \textbf{Target Masking} attempts to extract the information specified for removal by prompting the model without explicitly naming the targets. The queries either contain clear but indirect references to the targets (e.g. \textit{Was Harry's Occlumency teacher fit for the role of teaching Harry?} with \textit{Severus Snape} as one of the targets) or are so designed that while the query in itself would have no references to any of the targets, they cannot be responded to without mentioning one or multiple targets (e.g. \textit{How was Victor Krum's Yule ball experience?} with \textit{Hermione Granger} as one of the targets). ICUL and guardrailing struggle with these prompts, showing significant performance drops, especially guardrailing with Llama and Qwen. Responses to a few queries directly mention the targets for both ICUL and guardrailing, violating the core unlearning principle.
\texttt{ALU} demonstrates minimal loss in performance compared to the \textbf{None} perturbation. The multi-agent architecture of ALU effectively mitigates leakage since subsequent agents can correct errors introduced by earlier stages.

\begin{figure}[t]
    \centering
    %\includegraphics[width=17.0cm, height = 5.3cm]
    \includegraphics[width=0.5\textwidth]{fig/Figure10.png}
    \caption{Number of responses exhibiting information leakage for five different unlearning methods using Qwen-2.5-14B on the TOFU 10\% dataset. The number of unlearning targets was varied from 100 to 1000, and three different dummy to real target ratios were tested: \textbf{980:20}, \textbf{950:50}, and \textbf{900:100}. Results demonstrate a clear trend of increased leakage with target set sparsity for all methods, notably ICUL and NPO for the 980:20 split. \texttt{ALU}, maintains low leakage even with a large number of sparse targets.}
    \label{fig:f2}
    \vspace{-1\baselineskip}
\end{figure}

\ding{184} \textbf{Jailbreak prompts} have been successful in eliciting information which models should restrict \cite{lynch2024eight} \cite{shah2023scalabletransferableblackboxjailbreaks}, \cite{shen2024donowcharacterizingevaluating}. The ICUL authors do not provide any reference to how their method works against such prompts and the authors of guardrailing acknowledge that their framework is not meant to be robust against such jailbreaking. However, assuming users will not attempt to extract prohibited information is unrealistic. We append the user query in the prompt format as mentioned in \cite{lynch2024eight} and \cite{shen2024donowcharacterizingevaluating} and evaluate the three methods on these modified queries, observing a loss in performance across all three methods. While ICUL and guardrailing tend to leak indirect information related to the unlearning subjects, \texttt{ALU} suffers from a decline in model utility which is penalized by GPT-4o while evaluating the responses. This manifests as \texttt{ALU} often replying with \enquote{I'm sorry, I do not know the answer to the question} even for questions which can be answered by carefully avoiding references to the subjects in the \emph{forget set}. However, we do not observe this behavior in \texttt{ALU} for the other perturbations.

\ding{185} \textbf{Other languages} test the framework's generalizability to general perturbations in the prompts \cite{lynch2024eight} \cite{liu2024revisitingwhosharrypotter}. We randomly sample a language from a set of eight languages (refer to Table \ref{tab:t8}) and translate prompts with \textbf{None} perturbation into the chosen language and evaluate the responses of the frameworks on that prompt. \texttt{ALU} and ICUL do not show any loss in performance with GPT-4o as the base model as compared to their performance on the \textbf{None} perturbation, but a significant loss is observed in the case of the Llama model. The responses are mostly gibberish with Llama, and we posit this is due to smaller models not having extensive language generalizability and hence, this cannot be attributed to inherent flaws in the framework. A list of the languages used can be found in Table \ref{tab:t8}.

\ding{186} \textbf{Many-Shot Jailbreaking} follows the approach as stated in \cite{anil2024many}. For each base LLM, we prepare 128 question-response pairs to prepend them before the user queries and evaluates the responses. Many-shot prompting influences the output distribution of the models, potentially overriding training biases \cite{agarwal2024manyshotincontextlearning}. Many-shot prompting circumvents ICUL and guardrailing, leading to responses with direct and indirect references to multiple subjects from the \textit{forget set}. However, \texttt{ALU} maintains a robust performance due to its initial unperturbed response from the \textit{vanilla} agent, limiting the impact of subsequent stages.

\subsection{Scaling number of unlearning targets}
\label{sec:5.4}
Scalability is crucial for the practical applicability of any unlearning framework. We illustrate how \texttt{ALU} scales with an increasing number of unlearning targets in Figure \ref{fig:f1} alongside other optimization-based and post hoc methods when evaluated on WPU \cite{liu2024revisitingwhosharrypotter}. While the performance of most methods deteriorates with an increasing number of targets, \textit{Prompt} and \texttt{ALU} maintain a robust performance. This can be attributed to the long context windows in the recent models \cite{geminiteam2024gemini15unlockingmultimodal}, \cite{grattafiori2024llama3herdmodels}, \cite{qwen2.5}, enabling the models to identify targets in a long list of targets provided to the model at inference time. Model utility is impacted in the post hoc methods, along with \emph{Gradient Ascent} which has been demonstrated in prior works as well \cite{liu2024largelanguagemodelunlearning}, \cite{liu2024revisitingwhosharrypotter}, \cite{sinha2024unstarunlearningselftaughtantisample}. The decline in model utility for \emph{Prompt} has been discussed in \ref{sec:5.3} under certain perturbations. \emph{WHP} \cite{eldan2023whosharrypotterapproximate} and \emph{Causal Intervention} \cite{liu2024revisitingwhosharrypotter} show competitive performance except \emph{WHP} tending to hallucinate with scaling of the \emph{forget set}. Unlearning with agents proves to be more performant than all existing methods across scale, which is consistent with the results we find in \ref{sec:5.3}.

\textbf{\texttt{ALU} is scalable to target set size and sparsity.}
In realistic scenarios, the number of unlearning targets will not be confined to 20 or 100,  potentially reaching hundreds or even thousands. It is hence crucial for an unlearning framework to maintain its efficacy when confronted with a large scale \emph{forget set}. Since none of the current datasets \cite{maini2024tofu}, \cite{liu2024revisitingwhosharrypotter} provide the necessary scale for evaluating up to a thousand unlearning targets, \textbf{we created three sets of 1000 unlearning targets each}. These sets were synthesized by combining 20, 50, and 100 real targets from WPU \cite{liu2024revisitingwhosharrypotter} with names randomly sampled from the US 2010 census \cite{us2010cencus} to evaluate the effect of target sparsity on the baselines. Figure \ref{fig:f2} illustrates the performance of both post hoc and optimization-based methods with Qwen-2.5 14B \cite{qwen2.5} as the base model on the three mixes of targets with dummy targets, evaluated on 20 target-related questions. Information leakage increases with unlearning target sparsity, a problematic trend given that real-world queries may reference only a small fraction of a large target list. Methods like ICUL \cite{pawelczyk2023context} and NPO \cite{zhang2024negativepreferenceoptimizationcatastrophic} are particularly vulnerable to the sparsity issue, with NPO leaking information about nearly all 20 targets in the 980:20 split. In contrast, \texttt{ALU} demonstrates robustness to sparsity, with a few indirect references around 1000 targets. This highlights the need for developing unlearning methods which are more robust to scale and sparsity.

Further experiments and studies on comparing agentic frameworks with existing non-agentic ones (Appendix \ref{sec:6}), sensitivity of agentic frameworks (Appendix \ref{sec:7}), ablation studies (Appendix \ref{sec:ablation}), run time comparisons \ref{sec:6},  practicality of post hoc unlearning (Appendix \ref{sec:5.5}) can be found in the Appendix.
% \subsection{Agentic vs Non-Agentic Frameworks}
% \textbf{Time efficiency.} Practical unlearning frameworks require low time complexity. \texttt{ALU} offers constant-time inference, as shown in Table \ref{tab:t6}.  In contrast, ICUL \cite{pawelczyk2023context} and SNAP \cite{choi2024optoutinvestigatingentitylevelunlearning} scale with the number of unlearning targets, while NPO \cite{zhang2024negativepreferenceoptimizationcatastrophic} incurs a substantial constant cost per target.  Neither of these scaling behaviors is desirable for real-world deployment.

% \textbf{\texttt{ALU} minimizes target leakage risk.}
% Evaluated across diverse datasets, perturbations, and model sizes, ALU consistently prevents indirect leakage of unlearning targets, confirming its adherence to unlearning principles. By leveraging few-shot chain-of-thought prompting \cite{wei2023chainofthoughtpromptingelicitsreasoning}, the agents facilitate fine-grained unlearning by careful response tuning. Notably, this method remains effective even without direct target references, as the \textit{AuditErase} agent analyzes responses within the user query context.

% \textbf{Agents offer flexibility.}
% Different organizations deploying unlearning frameworks have their own special requirements and conditions which they need the frameworks to adapt to. Agentic unlearning offers maximum flexibility, allowing them to customize the agent pipeline to meet their specific needs.

% Refer to Appendix \ref{sec:6} for a more detailed comparison of agentic and non-agentic frameworks.