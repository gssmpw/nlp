% \section{Preliminaries}
% \textbf{Agentic LLM workflow.} Similar to how humans are more efficient in a well-coordinated group, language models also benefit from having a team of peer agents which contribute to the task and make the overall workflow more efficient. Some common practices involve decomposing the task into multiple subtasks, which are assigned to one or more agents for completion. While this makes the pipeline more involved, it vastly enhances the framework's efficiency. Creating agents specifically prompted to be efficient at that subtask is analogous to having multiple \emph{expert agents} who work in harmony, unlike a single generalized agent for the entire workflow. Moreover, LLM agents can also act as reviewer to process or evaluate responses and actions from other agents~\cite{zhuge2024agentasajudgeevaluateagentsagents}. This alleviates human evaluation in certain scenarios requiring significant time and compute. Agents can also be employed for guardrailing, preventing adversarial attacks on the framework in attempts to extract sensitive information. Section \ref{sec:5.3} highlights the efficacy of multi-agent frameworks against jailbreaking techniques.

% \textbf{LLM Unlearning.} Given a set $S = \{s_1, s_2, \cdots, s_N\}$ of $N$ unlearning targets and a user query $x \in \mathcal{X}$, the principle of an unlearning framework is to ensure that the unlearned model $\pi_{\theta_{\text{ul}}}$ generates responses $y$ which maximize unlearning efficacy and response utility. Hence, an ideal response must answer the user query effectively while obscuring references to the unlearning targets. We formalize this objective as follows 
% \begin{equation*}
% \begin{split}
% \pi^* = \underset{\pi_{\theta_{\text{ul}}}}{\operatorname{argmin}} \Biggl[ & \underbrace{\mathcal{D}_{\text{KL}}(\pi_{\theta_{\text{ul}}}(\cdot|x) || \pi_{\theta}(\cdot|x))}_{\text{Utility preservation}} \\
% & + \lambda \underbrace{\mathbb{E}_{y \sim \pi_{\theta_{\text{ul}}}(\cdot|x)} \left[\mathbbm{1}_{\{\exists s \in S : s \in y\}} \right]}_{\text{Unlearning penalty}} \Biggr]
% \end{split}
% \end{equation*}
% Here, $\mathcal{D}_{\text{KL}}(\cdots)$ measures the Kullback-Leibler divergence between the unlearned model  $\pi_{\theta_{\text{ul}}}$ and the original (non-unlearned) model $\pi_{\theta}$. Minimizing the KL-divergence between the two distributions allows the response from $\pi_{\theta_{\text{ul}}}$ to retain the utility of the response from $\pi_{\theta}$. $\lambda \ge 0$ is a hyperparameter that balances the utility and the unlearning strictness, increasing which will encourage the model to emphasize rigorous unlearning at the cost of response utility.

% $\mathcal{X}$ can contain prompts which are engineered to extract sensitive information from $\pi_{\theta_{\text{ul}}}$ \cite{zou2023universaltransferableadversarialattacks}, and we do not make assumptions about the intention of the user as done in \citet{thaker2024guardrail}. \citet{liu2024revisitingwhosharrypotter} points out that current post hoc unlearning methods are brittle to state-of-the-art adversarial attacks \cite{lynch2024eight, anil2024many}, preventing them from being deployed in practical settings. Section \ref{sec:5.4} highlights the robustness of \texttt{ALU} under adversarial attacks. 
