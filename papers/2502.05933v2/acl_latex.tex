% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% If you use natbib package, activate the following three lines:
% \usepackage[round]{natbib}
% \renewcommand{\bibname}{References}
% \renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
% \bibliographystyle{apalike}
\usepackage{float}     % For using the [H] option for figures
\usepackage{enumitem}
\usepackage{color}
\usepackage{breqn}
\usepackage{multirow}
\usepackage{amsmath,amsfonts}
\usepackage{adjustbox}
\usepackage{multirow}

\usepackage{hyperref}

\newcommand{\rh}[1]{{\bf \color{green}{[rh:#1]}}}
\newcommand{\hl}[1]{{\bf \color{blue}{[hl:#1]}}}

\title{Learning to Substitute Words with Model-based Score Ranking}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Hongye Liu \\
  Duke University \\
  \texttt{hongye.liu@duke.edu} \\\And
  Ricardo Henao \\
  Duke University \\
  \texttt{ricardo.henao@duke.edu} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}

\maketitle

\begin{abstract}
Smart word substitution aims to enhance sentence quality by improving word choices; however current benchmarks rely on human-labeled data.
Since word choices are inherently subjective, ground-truth word substitutions generated by a small group of annotators are often incomplete and likely not generalizable.
To circumvent this issue, we instead employ a model-based score (BARTScore) to quantify sentence quality, thus forgoing the need for human annotations. 
Specifically, we use this score to define a distribution for each word substitution, allowing one to test whether a substitution is statistically superior relative to others.
In addition, we propose a loss function that directly optimizes the alignment between model predictions and sentence scores, while also enhancing the overall quality score of a substitution.
Crucially, model learning no longer requires human labels, thus avoiding the cost of annotation while maintaining the quality of the text modified with substitutions.
Experimental results show that the proposed approach outperforms both masked language models (BERT, BART) and large language models (GPT-4, LLaMA). The source code is available at \href{https://github.com/Hyfred/Substitute-Words-with-Ranking}{https://github.com/Hyfred/Substitute-Words-with-Ranking}. 
%\footnote{Code available at \href{https://github.com/example/repo}{GitHub repository}.}
\end{abstract}

\section{Introduction}
In the current era of AI-driven technologies, the generation of natural language by machines has become a critical area of research within the field of natural language processing (NLP).
The advent of advanced language models such as GPT \citep{ouyang2022training} and LLaMA \citep{touvron2023llama, touvron2023llama2, dubey2024llama} has brought about unprecedented improvements in generating coherent, contextually accurate text across a wide range of applications, including machine translation, summarization, and conversational agents.
These models are revolutionizing industries by enabling machines to produce human-like text at scale, greatly expanding the possibilities for automation and interaction in both professional and creative domains.

As language generation capabilities improve, the need to refine and control the output of these models becomes more pressing.
One key area in this direction is smart word suggestion (SWS) defined by \citet{wang2023smart} as a task that focuses on enhancing writing by improving two aspects of NLP: identifying words (or tokens more generally) that if replaced can improve sentence quality, and suggesting alternatives for such words. 
Complementarily, \citet{wang2023smart} introduced a dataset to benchmark models built for SWS.
This task is essential in applications such as paraphrasing, machine translation, and writing assistance, where accurate word choice directly impacts the clarity and quality of the generated text.
Despite the progress in full-text generation, word substitution presents unique challenges in maintaining grammatical correctness, contextual appropriateness, and semantic fidelity.

Previous SWS work can be divided into {\em task-specific} models and {\em prompt-based} methods.
Task-specific models are trained directly to address the SWS task.
One approach adapts models for lexical substitution (LS) \citep{mccarthy2007semeval, kremer2014substitutes}, allowing them to generate substitutions for each word in a sentence.
If the substitution differs from the original word, it is recognized and recorded as such.
%, and it is recorded as a suggestion.
BERT-based \citep{zhou2019BERT} and the LexSubCon \citep{michalopoulos2021lexsubcon} models were first explored in \citet{wang2023smart}.
They also fine-tuned pretrained models like BERT \citep{devlin2018BERT} and BART \citep{lewis2019bart}, allowing them to handle SWS end-to-end.
Importantly, these models were fine-tuned using supervised learning to provide suggestions for Wikipedia text using ground-truth substitutions obtained from a list of synonyms gathered from a thesaurus.
Orthogonal to these machine learning techniques, rule-based methods that rely on paraphrasing databases ({\em e.g.}, PPDB \citep{pavlick2015ppdb}) or a thesaurus ({\em e.g.}, the Merriam-Webster thesaurus) have also been explored \citep{wang2023smart}.
% \footnote{\url{https://www.merriam-webster.com/thesaurus}.}

Alternatively, prompt-based techniques leveraging large language models (LLMs), such as GPT \citep{ouyang2022training}, have gained popularity in SWS.
Provided that research in language generation has shifted from narrow tasks like substitution to generating coherent, contextually rich text, tasks such as SWS have become less of a priority.
However, in general it is still key to enhance sentence quality, as precise word choices can significantly impact clarity, tone, and overall effectiveness in communication.
Moreover, the success of these methods is highly dependent on prompt design.

From a different perspective, benchmarks for SWS predominantly rely on human-labeled data for both training and evaluating model performance.
However, only a small group of annotators are recruited for data labeling, which can lead to a lack of diversity.
For instance, in the SWS test dataset \citep{wang2023smart}, 12,883 suggestions for target words are available of which 7,827 (60.8\%) were created by a single annotator, only 1632 (12.67\%) of suggestions were created by at least three annotators, and merely 633 (4.91\%) of suggestions were created by more than three annotators.
%
While scaling up the number of annotators could potentially mitigate this issue, it significantly increases the costs of generating the annotations.
In the context of substitution tasks, a large number of annotators is necessary to obtain a comprehensive representation of the substitution space, as evaluating them requires a diverse sample of high-quality alternatives.
As a result, validation sets in such benchmarks may not fully capture model performance.
Conversely, recent research has shown that model-based evaluation metrics align closely with human judgments \citep{lu2022toward, yuan2021bartscore, he2024improving}, offering a viable alternative to alleviate the drawbacks of human evaluations.

To address these challenges, we propose an approach that eliminates the dependency on human-labeled data.
Our contributions are listed below:
%
\begin{itemize}[leftmargin=10pt,itemsep=0pt,topsep=0pt,parsep=0pt]
    \item We identified a gap in the evaluation methods for substitution models, which rely on human annotators.
    We propose a model-based evaluation strategy based on the BARTScore to assess SWS models without human annotations.
    % Specifically, we introduce a model-based statistic based on the BARTScore to evaluate SWS models.
    \item We introduce an approach for generating and ranking token substitutions.
    Our method optimizes both the quality of the substitutions, via the model-based score, and the ranking of the substitutions relative to such a score.
    To the best of our knowledge, this is the first time SWS has been tackled by jointly optimizing the association between predictions and quality scores.
    % Our loss function is based on margin ranking or direct preference optimization (DPO).
    \item We redefined the substitution task by forgoing the need for supervised learning relying on human annotations.
    Thus, our method eliminates the need for costly human-labeled data while maintaining high performance in SWS tasks.
    \item Our experimental results demonstrate that the proposed approach outperforms both masked language models (BERT, BART) and LLMs (GPT-4, LLaMA) in the SWS task.
\end{itemize}

\section{Related Work}
%
Enhancing word usage is a key feature of writing assistance.
The SWS task in \citet{wang2023smart} addresses it by emulating a real-world end-to-end writing scenario.
Unlike traditional LS tasks \citep{mccarthy2007semeval, kremer2014substitutes}, SWS requires systems to identify words that, if replaced, can improve sentence quality and suggest alternatives for such words.
In the LS task, however, the word to be replaced is predefined and the model is only tasked with finding suitable substitutions without altering the overall meaning of the sentence.
Further, in LS substitutions are typically lemmatized to match ground-truth labels, but in SWS they must be in the correct grammatical form.

\citet{zhou2019BERT} proposed a LS method using contextual word embeddings, modifying BERT with a dropout embedding policy to partially mask target words and introduced a validation score based on the top four layers of BERT for candidate evaluation.
\citet{michalopoulos2021lexsubcon} integrated external knowledge from WordNet into BERT for LS, combining scores from BERT, WordNet gloss similarity, and sentence embeddings, along with the validation score from \citet{zhou2019BERT}, to generate substitutes.
ParaLS \citep{qiang2023parals} generated substitutes through a paraphrasing model, using a heuristics-based decoding strategy.

To adapt the substitution capabilities from LS to the SWS task, \citet{wang2023smart} used an approach that allowed models to generate substitutions for each word.
If the substitution differs from the original word, it is recognized as the word the model wants to improve and recorded as a suggestion.
Further, they fine-tuned pretrained models like BERT \citep{devlin2018BERT} and BART \citep{lewis2019bart} to address the task in an end-to-end manner, and as an alternative to machine learning, rule-based methods using dictionaries (thesaurus) were also explored.

Recently, prompt-based methods have emerged as a powerful alternative for SWS \citep{wang2023smart}, by leveraging the capabilities of LLMs such as GPT \citep{ouyang2022training}.
These approaches shift the focus from traditional model fine-tuning to designing prompts that guide the model to produce desired outcomes.
While this flexibility allows for easier adaptation to various tasks, the effectiveness of the method heavily relies on the prompt's structure.
Despite the growing interest in generating fluent, contextually cohesive text with LLMs, prompt-based methods often overlook the importance of improving specific word choices.
%, a critical factor in tasks like word substitution.
In contexts where sentence clarity and precision are key, refining word choices remains an important task that prompt-based approaches need to address.

Compared to traditional methods that rely heavily on human involvement, {\em e.g.}, via manual annotations, in our work, we utilize model-based sentence scoring, specifically, BARTScore to quantify the quality of sentences, thereby eliminating the human-associated costs.
We align the model’s output with the scores through a ranking loss, ensuring that substitutions with higher scores are more likely to be generated by the model, resulting in higher-quality substitutions.
This approach forgoes the need for human-dependent supervised training, thus without the need for human intervention.

\section{Methodology}
%
In SWS, achieving a representative sample of high-quality alternatives requires a broad pool of annotators, as diverse substitution options are essential for accurate evaluation.
However, relying solely on human annotators is thus not only costly but also inefficient. 
Below we introduce a method to train and evaluate token substitution models that does not require human annotations, by instead solely relying on model-based scoring functions.

\paragraph{Problem Definition}
%
Let $X$ denote a sentence composed of $N$ tokens, {\em i.e.}, $X=(x_1,\ldots,x_N)$.
We aim to find a collection of $K$ potential substitutes $\{\tilde{w}_k\}_{k=1}^K$ for token $x_n$, while estimating their likelihood given the sentence ({\em i.e.}, its context) as $p_\theta(x_n=\tilde{w}_k|X)$, where $X$ is the original sentence and $p_\theta(\cdot)$ is a model for the conditional likelihood for token $x_n$ parameterized by $\theta$.
Note that technically, if $p_\theta(\cdot)$ is a masked language model we should write $p_\theta(x_n=\tilde{w}_k|X_{\backslash x_n})$, where $X_{\backslash x_n}$ indicates that $x_n$ has been masked out in $X$, and alternatively, if we use an (autoregressive) language model we should write $p_\theta(x_n=\tilde{w}_k|x_{<n},Z)$, where $x_{<n}$ is the portion of the original sentence up to token $n-1$ and $Z$ is a prompt.
However, we use $p_\theta(x_n=\tilde{w}_k|X)$ in the following for notational simplicity.
Based on these likelihood estimates, the token substitution rule is set as $x_n \leftarrow \tilde{w}_k$ if $p_\theta(x_n=\tilde{w}_k|X)>p_\theta(x_n=w_n|X)$, where $w_n$ is the original value of $x_n$.
Moreover, if multiple replacement candidates satisfy the substitution rule, we select $\tilde{w}_k$ according to ${\rm argmax}_k \ p_\theta(x_n=\tilde{w}_k|X)$, and alternatively, if none satisfy it, we leave the token unchanged, {\em i.e.}, $x_n \leftarrow w_n$.

Importantly, $p_\theta(x_n=\tilde{w}_k|X)$ reflects the likelihood of token $\tilde{w}_k$ given its context $X$, rather than the quality of the sentence.
So motivated, we also seek to align such estimates with a score $M(x_n=\tilde{w}_k|X)$ quantifying the quality of sentence $X$ with $x_n$ substituted with $\tilde{w}_k$.
For simplicity, in the following we denote the sentence $X$ with $x_n$ replaced with the $k$-th candidate, $\tilde{w}_k$, as $\tilde{X}_{k}=(\tilde{x}_1,\ldots,\tilde{x}_N)$.
Further, to make our objective scalable and general, the score $M(x_n=\tilde{w}_k|X)$ ought not be obtained via human feedback, but via a {\em black-box model} with which we can score sentences, but through which we cannot learn, {\em i.e.}, gradients cannot be propagated through it to obtain learning signals.

Below we start by showing how we can use a model-based score to statistically characterize the suitability of a candidate as an alternative to the substitution rule introduced above, and then present an approach to estimate $P(x_n=\tilde{w}_k|X)$ while accounting for scores $M(x_n=\tilde{w}_k|X)$.

% To address these challenges, this paper aims to explores a model-based approach to sentence scoring that quantifies sentence quality without the need for human annotations. This not only reduces the costs associated with human labeling but also allows for the creation of a statistical distribution for each word substitution. With this distribution, we can statistically evaluate whether a substitution is significantly better than others. We further strive to propose a novel loss function that directly optimizes the alignment between model predictions and sentence quality scores, aiming to enhance the overall quality of substitutions.
%
% Let $S$ represent a sentence composed of words $\left \{w_{1}, w_{2}, \dots, w_{n}\right \}$, For each word $w_{i}$  , the model suggests a set of substitution candidates  $\left \{c_{1}, c_{2},\dots, c_{K}\right \}$. Let $P(w_{k}, c_{j})$ denote the probability that candidate $c_{j}$  is a valid substitution for word $w_{k}$  . The quality of each substitution is quantified by a model-based sentence score $M(S,c_{j})$, where $S$ is the original sentence, and $c_{j}$  is the substitution candidate. The goal is to optimize a loss function $L(Q,P)$ that aligns the model's predictions $P$ with the sentence score $Q$.
%
% \subsection{Data}
% % \begin{itemize}
% %   \item SWS \cite{wang2023smart}
% %   \item L07 \cite{mccarthy2007semeval}
% %   \item LS14 \cite{kremer2014substitutes}
% %   \item XSum \cite{narayan2018don}
% % \end{itemize}
%
% Enhancing word usage is a key feature in writing assistance, and the "SmartWord Suggestions" (SWS) task and benchmark address this by simulating a realistic end-to-end writing scenario (\cite{wang2023smart}). Unlike traditional lexical substitution (LS) tasks, where the target word is predefined, SWS requires systems to identify and improve suboptimal words. This makes SWS particularly suited for writing enhancement, where the focus is on improving sentence quality rather than simply providing synonyms.
%
% In addition to SWS, we utilize two traditional LS datasets—LS07 (\cite{mccarthy2007semeval}) and LS14 (\cite{kremer2014substitutes})—which focus on generating synonyms for a given target word. However, while LS emphasizes word sense disambiguation, SWS aims to improve overall writing quality by suggesting better word choices. Additionally, LS tasks rely on lemmatized annotations, while SWS focuses on ensuring that substitutions fit grammatically within the sentence.
%
% To further test model robustness across more diverse contexts, we also use the XSum dataset, widely recognized in the summarization domain \cite{narayan2018don}. XSum contains BBC articles paired with corresponding summaries, offering a rich and diverse set of examples for testing model robustness.

\subsection{Statistic for Model-based Scores}\label{sc:model-based_stat}
%
\paragraph{Model-based Score}
%
Using automated scoring methods to quantify text generation quality without relying on human annotators can reduce costs associated with human labeling. 
Model-based scoring approaches such as BLEURT \citep{sellam2020bleurt}, BERTScore \citep{zhang2019BERTscore}, GPTScore \citep{fu2023gptscore}, and BARTScore \citep{yuan2021bartscore}, have proven to be effective while closely aligning with human evaluation results.
BARTScore is one of the most popular metrics for the evaluation of text generation.
It has been shown that BARTScore outperforms other metrics such as BERTScore and BLEURT \citep{yuan2021bartscore}, while also being more efficient than GPTScore.
Further, it has been used also as a ranking tool for substitution tasks, achieving good results \citep{qiang2023parals}.

The BARTScore is a metric for universal natural language generation evaluation \citep{yuan2021bartscore}.
It leverages the conditional likelihood from a pre-trained BART model \citep{lewis2019bart} to assess the quality of generated text.
Since BART is an autoregressive model, the log-likelihood of tokens in a sentence is obtained one at a time conditioned on tokens before it.
Specifically, we write
%
\begin{equation}\label{eq:bartscore}
M(\tilde{X}_k)=\textstyle{\sum}_{n=1}^{N} \log p_{\hat{\theta}}\left(\tilde{x}_{n} \mid \tilde{x}_{<n}, X\right) \,,
\end{equation}
%
where $\hat{\theta}$ represents the parameters of a pre-trained model (in this case, BART\footnote{We use \href{https://github.com/neulab/BARTScore}{BARTScore (ParaBank2 version)} without incorporating any prompts.}), $\tilde{x}_n$ is the $n$-th token of the modified sentence $\tilde{X}_k$ which we wish to score, $\tilde{x}_{<n}$ represents the first $n-1$ tokens of $\tilde{X}_k$, and $X$ is the original (unmodified) sentence.
In \eqref{eq:bartscore}, the BARTScore $M(x_n=\tilde{w}_k|X)$ has been simplified for notational convenience to $M(\tilde{X}_k)$, {\em i.e.}, the score of modified sentence $\tilde{X}_k$ in which a token has been replaced with the $k$-th candidate from potential substitutes $\{\tilde{w}_k\}_{k=1}^K$.
For now, we assume that such candidates are readily available, however, in the next section we will propose a learning strategy to train a model to generate substitutes given $X$.

\paragraph{Model-based Score Statistic}% \label{sec:p-value}
%
One potential issue with model-based scores including the BARTScore in \eqref{eq:bartscore} is that there is no clear standard to determine what score (or score range) qualifies as ``good''.
It is clear that though scores can be used for pairwise or aggregate comparisons, the magnitude of the difference between two scores is not meaningful without a reference score distribution.

This underscores that numerical scores alone are insufficient to fully capture the significance of a score difference.
Inspired by statistical hypothesis testing \citep{moore1999bootstrapping}, we propose constructing a reference (null) distribution of $M(\tilde{X}_k)$ by sampling a collection of substitutes for a given token and then calculating their model-based score.
By determining where \(M(\tilde{X}_k)\) lies within this empirical distribution, we can more reliably assess the quality of a substitute.
Since sampling candidates uniformly at random from a natural language vocabulary or from empirical token frequencies will be hugely inefficient, instead we use a model to generate substitute candidates.
In this manner, we can evaluate the relative quality of the generated substitutes compared to other candidates from the model’s own distribution.
This constitutes a principled framework for assessing the quality of a model in generating token substitutes.
Next, we introduce a hypothesis test to estimate a $p$-value for the significance of a candidate substitute relative to a reference (empirical) substitute distribution.

Given a model we wish to evaluate, let $\{\tilde{w}_k\}_{k=1}^{K_s}$ be a set of $K_s$ candidates for sentence $X$ at a given position produced by such model, {\em e.g.}, a masked language model like BERT \citep{devlin2018BERT}.
Note that in general, $K_s$ is a number much larger than the size of potential substitutes $K$ described before.
We can evaluate the quality of, for instance, the top candidate $w_1$ (assuming they are ordered) as the proportion of times it produces a score that is larger than using all other candidates, $\{\tilde{w}_k\}_{k=2}^{K_s}$.
This frequency or $p$-value is formally written as
%
\begin{equation}
% \text{p-value}=\frac{\sum_{\hat{k}}^{\hat{K}} I[M(\tilde{X}_{\hat{k}})>M(\tilde{X}_{1})]}{\hat{K}} 
p_{\tilde{X}_1} = \frac{1}{K_s-1} \textstyle{\sum}_{k=2}^{K_s} \ I[M(\tilde{X}_k)>M(\tilde{X}_1)] \,,
\label{eq:p-value}
\end{equation}
%
where $I[\cdot]$ is the indicator function and the $\{w_k\}_{k=1}^{K_s}$ producing modified sentences $\{\tilde{X}_k\}_{k=1}^{K_s}$ are obtained from a model $p_\theta(x_n=\tilde{w}_k|X)$, which defines the reference distribution.
In standard hypothesis testing fashion, we reject the hypothesis that candidate $w_1$ has the same expected score as the reference distribution if $p_{\tilde{X}_1}<\alpha$, for a significance level $\alpha$, which we set to $\alpha=0.01$ in our experiments.
Since calculating model-based scores such as BARTScore is computationally expensive, we trade-off statistical accuracy with computational efficiency and let $K_s=1000$ in the experiments.

\subsection{Preference-Aware Learning}\label{sec:Preference-Aware-Learning}
%
Now that we have constructed a statistic based on model-based scores, in principle, we seek to train a model such that for each token $x_{n}$, the resulting $p_{\tilde{X}_k}$ from \eqref{eq:p-value} for a given substitute candidate $w_k$ producing $\tilde{X}_k$ is as small as possible.
According to \eqref{eq:p-value}, it is desirable for $M(\tilde{X}_{1})$ to be larger than $M(\tilde{X}_{\hat{k}})$ for $\{w_k\}_{k=1}^K$ candidates sampled from model $p_\theta(x_n|X)$.
This means that effectively we require the model being such that its outputs align, in likelihood, with the model-based score, BARTScore here.
Consequently, it is desirable to learn parameters $\theta$ so $p_\theta(x_n=w_k|X)$ is ranked consistent to $M(\tilde{X}_{k})$, which in turn will make $p_{\tilde{X}_k}\to 0$ in \eqref{eq:p-value} for $k\to 1$.
Below we consider two approaches to accomplish this: margin ranking \citep{liu2023learning, chern2023improving, liu2022brio} and direct preference optimization (DPO) loss \citep{rafailov2024direct}.
%
% this means that \(M(\tilde{X}_{1})\) should be higher relative to \(M(\tilde{X}_{\hat{k}})\).
% \rh{comes out of nowhere, why put this here? why were you expecting it? how is this relevant to the text below?}
% Ideally, this can be achieved by guiding the model $\theta$ to output substitutions with higher BARTScore values, effectively aligning the model's output with the scoring metric (BARTScore).
% \rh{write it in more general terms, you are basically saying that you want a model to improve BART. Instead you need to say that you want a model with good quality sentences, where quality is defined via a score to avoid using humans and then you say that in the experiments you consider BART, but other scores are possible because you do not need any assumptions on the scoring function.}
% Thus, we can optimize the model $\theta$ to learn how to rank according to the score's preference. This ensures that \(M(\tilde{X}_{1})\) will be higher relative to \(M(\tilde{X}_{\hat{k}})\).

\paragraph{Ranking Optimization}
%
Ranking has been used as an optimization objective in many tasks including text summarization \citep{chern2023improving, liu2022brio}.
Maximum likelihood estimation based on the standard cross-entropy loss can be effective at satisfying \eqref{eq:p-value} by setting it as a classification problem by defining a vector of labels $y$ where $y_k=1$ if $M(\tilde{X}_{k})$ is maximum among $\{M(\tilde{X}_{k})\}_{k=1}^K$ or $y_k=0$ otherwise.
However, it does not take into account the ordering of the model-based scores or their magnitude differences.
So motivated, we consider the following margin ranking (MR) loss \citep{liu2023learning, chern2023improving, liu2022brio}
%
\begin{equation}
\mathcal{L}_{\rm MR} = \textstyle{\sum}_{k}^{K} \textstyle{\sum}_{j > k}^{K} \ \max(0, s_j - s_k + \lambda_{jk}) \,,
\label{eq:constra}
\end{equation}
%
where $\{s_k\}_{k=1}^K$ are the logits from model $p_\theta(x_n=w_k|X)$, {\em i.e.}, before applying the softmax function, and we have sorted them such that $s_k>s_j$ if $M(\tilde{X}_{k}) > M(\tilde{X}_{j})$ for $i,j=1,\ldots,K$.
Further, we set the {\em margin} $\lambda_{jk}=\lambda\times(j-k)$ for some hyperparameter $\lambda$, which in the experiments is set via cross-validation.
Intuitively, \eqref{eq:constra} encourages the model to make predictions whose outputs $\{p_\theta(x_n=w_k|X_{\backslash x_n})\}_{k=1}^K$ are consistent in order with $\{M(\tilde{X}_{k})\}_{k=1}^K$, by penalizing pairwise order mismatches, while also enforcing predictions to be distanced by a fixed margin to improve robustness.
The latter is justified by extensive results from the margin learning literature \citep{smola2000advances}.
%
% the Maximum Likelihood Estimation (MLE) loss is effective in ensuring that the generated summaries align with reference summaries, it doesn't take into account the actual quality of the generated summaries, which is usually evaluated by some metric. The goal of this new approach is to go beyond simply matching the reference by encouraging the model to rank and generate better-quality summaries. We use the model \(\theta\)'s output \(s_{nk}(X)\) to represent the logit score for the \(k\)-th candidate \(w_{k}\) in the context of the input sentence \(X\) and the target word \(x_{n}\). For simplicity, we will refer to this logit score as \(s_{k}\) going forward. The formulation can be expressed as:
%
% \begin{equation}
% \mathcal{L}_{Contra} = \sum_{k}^{K} \sum_{j > k}^{K} \max(0, s_j - s_k + \lambda_{jk}) 
% \label{eq:constra}
% \end{equation}
%
% where $s$ is the logit value generated by the  model $\theta$ when predicting substitution words, and $\{s_{1},s_{2},\cdots,s_{K}\}$ is descendingly sorted by $M({\tilde{X}_{k}})$, $k$ is candidate index, $K$ is the number of candidate, $\lambda_{jk}=\lambda\times(k-j)$, $\lambda$ is margin hyper-parameter. Intuitively, the contrastive loss penalizes any discoordination between the estimated logit and the BARTScore (i.e., when $s_{j} > s_{k}$ but $M(\tilde{X}_{k}) > M(\tilde{X}_{j})$).
%
% \rh{what is a logit, where does it come from, how is it permuted, how can it be sorted and permuted? what is a candidate? why is $\lambda=0.5$?}

% \begin{figure}[H] % Specify image placement (h=here, t=top, b=bottom, p=separate page)
%     \centering
%     \includegraphics[width=1\textwidth]{figures/Picture 8.png} % Include the image file
%     \caption{Workflow} % Add a caption to the image
%     \label{fig:your_label} % Add a label for referencing in the text
% \end{figure}

% Additionally, we noticed that while directly optimizing the alignment between model predictions and sentence scores, we should also consider the overall quality score of a substitution. Therefore, we designed two penalized terms to enhance the performance. The first term focuses on improving the overall score directly. The second term improves the top-1 candidate's score, and by simultaneously increasing the correlation between model predictions and the score, it also raises the overall quality of the substitutions.
% \rh{when/how did you notice? which model? also consider the overall score relative to doing what? what is the first term? where is the second term? This whole paragraph makes no sense.}

\paragraph{Improving Model-based Scores}
%
One unintended consequence of the loss function in \eqref{eq:constra} is that though it encourages model predictions to be aligned with model-based scores, it does so regardless of their values.
This is so because only the order of $\{M(\tilde{X}_{k})\}_{k=1}^K$ is considered in \eqref{eq:constra}.
In practice, we have observed that \eqref{eq:constra} effectively improves the ranking of predictions from the model, but it does so at the expense of producing predictions that have, on average, lower model-based scores relative to a reference model, {\em e.g.}, the pre-trained model used as initialization for the refinement of $\{p_\theta(x_n=w_k|X)\}_{k=1}^K$.
To address this issue, we consider two approaches, one that seeks to improve the weighted average of model-based scores and another that maximizes the model-based score of the top prediction from the model relative to that of the reference model.
Specifically, we write
%
\begin{align}
    \mathcal{L}_{\rm AS} & =-\textstyle{\sum}_{k}^{K} \ h(s_{k}) M(\tilde{X}_k) \,, \label{eq:as} \\
    \mathcal{L}_{\rm BS} & =\operatorname{max}(0,(M(X)-M(\tilde{X_{1}}))f\left(s_{1}\right)) \,, \label{eq:bs} 
\end{align}
%
where $h(\cdot)$ is the softmax function, $s_k$ is the logit corresponding to $p_\theta(x_n=w_k|X)$, and $M(X)$ and $M(\tilde{X_{k}})$ are the model-based scores of the original and modified sentence, respectively.
Recall that for $M(\tilde{X_{1}})$, the token of interest $x_n$ in $X$ has been modified to the top prediction $w_1$ from the model.
Conceptually, $\mathcal{L}_{\rm AS}$ in \eqref{eq:as} seeks to maximize the (weighted) average of model-based scores from $K$ predictions, while $\mathcal{L}_{\rm BS}$ aims to improve the model-based score of the top prediction with respect to that of a reference model.

We then combine the margin ranking loss in \eqref{eq:constra} with the score-improving losses in \eqref{eq:as} or \eqref{eq:bs} as
%
\begin{align}
    \mathcal{L}_{\rm MR+AS} & =\mathcal{L}_{\rm MR} + \gamma\mathcal{L}_{\rm AS} \,, \label{eq:mr_as}\\
    \mathcal{L}_{\rm MR+BS} & =\mathcal{L}_{\rm MR} + \gamma\mathcal{L}_{\rm BS} \,, \label{eq:mr_bs}
\end{align}
%
where $\gamma$ is a hyperparameter trading off ranking or model-based score improvement.
In the experiments we will compare these two approaches to determine empirically whether is better to attempt to improve the scores of $K$ predictions from the model as opposed to improving the score of one of the predictions from the model (the most likely) relative to a baseline prediction obtained from a reference model.
Note that is tempting to use \eqref{eq:bs} with all predictions, not just the first one, however, we found empirically that it considerably increases the computational cost without significant performance gains.
The cost overhead is caused mainly by the need to evaluate the model-based score $K$ times rather than just $2$ in \eqref{eq:bs}. % or $K$ in \eqref{eq:as}.

% \paragraph{Penalized term (Average of BARTScore)}
% 
% From Equation 4, we observe that when optimizing the alignment between model predictions and sentence scores, the model only optimizes the relative ranking of the substitution words, but cannot guarantee the overall quality of the substitutions $\{M(\tilde{X}_k)\}_{k=1}^K$. This may lead to selecting a top-1 substitution with a relatively high score, but if the overall quality of the compared substitutions is low, the top-1 substitution may also be of low quality. Therefore, we should also consider optimizing the overall quality score of the substitutions. To address this, we designed two penalty terms to improve performance. The Average of BARTScore (Aob) term is below.
% 
% \begin{equation}
% \mathcal{L}_{aob}=-\frac{1}{K}\sum_{k}^{K}\left(M(\tilde{X}_k) \times f\left(s_{k}\right)\right)
% \end{equation}
%
% where $f(\cdot)$ is softmax function. This term focuses on directly improving the overall score.
%
% \rh{Same as above, where does the equation below come from, why do you need it, what does it mean, how do you use it?}
%
% \paragraph{Penalized term (Top1 of BARTScore)}
% 
% Instead of improving the overall quality, the Top-1 BARTScore  term was introduced to specifically enhance the quality of the top-1 substitution's BARTScore.
% 
% \[\mathcal{L}=\operatorname{Max}\left(0,\left(M\left(S_{\text {ori}}\right)-M\left(S_{top1}\right)\right) \times e^{S_{i}}\right)\]
%
% \begin{equation}
% \mathcal{L}_{top1}=\operatorname{Max}\left(0,\left(M(X)-M(\tilde{X_{1}})\right) \times f\left(s_{k}\right)\right) 
% \end{equation}
%
% where $M(X)$ refers BARTScore of the original sentence X, $M(\tilde{X_{1}})$ refers BARTScore of the sentence where has been replaced by the top1 substitution$. 
% 
% \rh{explain the equation, where dos it come from, what it does, how it does it, how do you use it?}
%
% To combine both aspects—the correlation between the model's predictions and BARTScore, as well as the overall quality of BARTScore—we use a weighted sum to link the formulas for these two aspects, as shown below.
%
% \begin{equation}
%     \mathcal{L}_{Contra_-aob}=\beta_{1}* \mathcal{L}_{Contra}+\beta_{2}*\mathcal{L}_{aob}
% \label{eq:contra_aob}
% \end{equation}
%
% \begin{equation}
% \mathcal{L}_{Contra_-top1}=\gamma_{1}*\mathcal{L}_{Contra}+\gamma_{2}*\mathcal{L}_{top1} 
% \label{eq:contra_top1}
% \end{equation}
%
% Formula \ref{eq:contra_aob} shows that we directly optimize overall quality of BARTScore using the $\mathcal{L}_{aob}$ and the correlation between the model’s predictions and the scores using the $\mathcal{L}_{Contra}$ at the same time.
%
% Formula \ref{eq:contra_top1} expresses our expectation that the $\mathcal{L}_{top1}$ enhances the top-1 candidate's score and improves the overall quality of substitutions by using $\mathcal{L}_{Contra}$ to increase the correlation between the model's predictions and the scores.
%
% \rh{so what? why do you have two, how are they different, whay sin't there a term weighting the two loss terms in each equation?}

\paragraph{Direct Preference Optimization}
%
Direct Preference Optimization (DPO) \citep{rafailov2024direct} is an efficient technique for aligning large language models with human feedback, which gained popularity due to its simplicity \citep{miao2024inform}.
For instance, it has demonstrated to be effective in chat benchmarks \citep{tunstall2023zephyr, zheng2023judging}.
In our case, the model-based score serves as proxy for human feedback
DPO (under the Plackett-Luce model) and is formally expressed as
%
\begin{align}\label{eq:dpo}
\mathcal{L}_{\rm DPO} = -\mathbb{E} 
\left[ \log \textstyle{\prod}_{k=1}^{K} 
\frac{\exp ( \delta  r_k )}{ \sum_{j=k}^{K} \exp ( \delta r_j )} \right] \,, %\notag
%
%\mathcal{L}_{\rm DPO} = & -\mathbb{E}_{\tilde{X}_1, \ldots, \tilde{X}_K, X} \label{eq:dpo} \\
%& \left[ \log \prod_{k=1}^{K} 
%\frac{\exp \left( \delta  \log \frac{p_\theta(\tilde{X}_k)}{p_{\hat{\theta}}(\tilde{X}_k)} \right)}{ \sum_{j=k}^{K} \exp \left( \delta  \log \frac{p_\theta(\tilde{X}_j)}{p_{\hat{\theta}}(\tilde{X}_j)} \right)} \right] \,, \notag
\end{align}
%
where $r_k=\log p_\theta(\tilde{X}_k) - \log p_{\hat{\theta}}(\tilde{X}_k)$, the expectation is over $\{\tilde{X}_1, \ldots, \tilde{X}_K, X\}$, and 
% $\log \frac{p_\theta(\tilde{X}_k)}{p_{\hat{\theta}}(\tilde{X}_k)}$
% where $p_\theta(\tilde{X}_k)$ is a shorthand for $p_\theta(x_n=w_k|X)$, 
$\theta$ and $\hat{\theta}$ denote the parameters of the model being trained and that used for reference, respectively.
Accordingly, only $\theta$ are updated while learning while $\hat{\theta}$ are kept fixed.

Since the magnitude of the logits drops significantly as $k\to K$, we found that the sum in the denominator of \eqref{eq:dpo} weakens the loss.
Therefore, we removed the sum and instead compared the $k$-th and the $(k+1)$-th substitution, rather than comparing it with all $K$ values.
Further, we approximate $\log p_\theta(\tilde{X}_k)$ with its logit $s_k$, and let $\delta=1$ for convenience.
Then \eqref{eq:dpo} simplifies (for a single token in sentence $X$) to 
%
\begin{align}
    \hspace{-2mm}\mathcal{L}_{\text{DPO*}} &= -\textstyle{\sum}_{k=1}^{K-1} \left( s_k - \hat{s}_k - s_{k+1} + \hat{s}_{k+1} \right) \,, 
    \label{eq:dpo*}
\end{align}
%
where $s_k$ and $\hat{s}_k$ denote the logit of the $k$-th substitution from the model being trained and the reference model, respectively.
% after inputting the sentence into the training model \(\theta\) and locating the target word. \(\hat{s}\) represents the logit of the \(k\)-th substitution after inputting the sentence into the pre-trained model \(\hat{\theta}\) and locating the target word. 

We also extend DPO (under the Bradley-Terry model) \citep{rafailov2024direct} to multiple substitute candidates, where each candidate is compared with the next in the ordered list of substitute candidates.
We write (for a single token in sentence $X$)
%
\begin{align}
& \mathcal{L}_{\text{$\sigma$DPO*}} = \label{eq:dpo*-sigma} \\
& \hspace{8mm} -\textstyle{\sum}_{k=1}^{K-1} \log\sigma \left( s_k - \hat{s}_k - s_{k+1} + \hat{s}_{k+1} \right) \,, \notag
\end{align}
%
%
%
%
% \begin{figure}[H] % Specify image placement (h=here, t=top, b=bottom, p=separate page)
%     \centering
%     \includegraphics[width=1\textwidth]{figures/Picture 9.png} % Include the image file
%     \caption{Workflow} % Add a caption to the image
%     \label{fig:your_label} % Add a label for referencing in the text
% \end{figure}
from which we see that the only difference between \eqref{eq:dpo*-sigma} and \eqref{eq:dpo*} is that the comparison of logit values in the former is scaled with the log-logistic function.
See Appendix~\ref{sc:dpo_det} for a derivation of both losses.

\section{Experiments}
% \subsection{Data we used}
% The SWS/Lexical substitution task data:
% \begin{itemize}
%   \item SWS \cite{wang2023smart}
%   \item L07 \cite{mccarthy2007semeval}
%   \item LS14 \cite{kremer2014substitutes}
% \end{itemize}
%
% The Summarization task data:
% \begin{itemize}
%   \item XSum \cite{narayan2018don}
% \end{itemize}
% \textcolor{red}{The experimental section consists of two parts. In the first part, we evaluate the BERT model from \citep{wang2023smart} on human-labeled substitution datasets, specifically the SWS test dataset \citep{wang2023smart}. We use \( p_{\tilde{X}_1} \) in \eqref{eq:p-value} to explore the issue of subjectivity in human annotations by comparing the differences between \( p_{\tilde{X}_1} \) and the human labels. In the second part, we optimize the models by aligning model more closely with BARTScore results, aiming to achieve improved $p_{\tilde{X}_1}$ performance.}
%
% We start by illustrating the need for the model-based score statistic introduced in Section~\ref{sc:model-based_stat}.
Below we illustrate the problem with evaluating with human annotations, then we present an ablation study comparing the optimization approaches in Section~\ref{sec:Preference-Aware-Learning} and a benchmark comparing the proposed model to MLM and LLM approaches in terms of model-based score alignment, average score and the statistic in \eqref{eq:p-value}. 
Further, we explore the performance of top-2 predictions, {\em i.e.}, when the top prediction is the original token, and results comparing LLMs with and without prompts encouraging token substitutes to be ranked by quality.
%
%\textcolor{red}{In Section \ref{sec:p-value}, we proposed that combining the p-value with hypothesis testing can measure the overall quality of generated substitution. In Section \ref{sec:Preference-Aware-Learning}, we introduce training model such that the p-value for each token can be as small as possible, indicating better word substitutions. The following experimental section is structured according to these two parts.}
%
%\textcolor{red}{In the first part, we evaluate the BERT model from \citep{wang2023smart} on human-labeled substitution datasets, specifically the SWS test dataset \citep{wang2023smart}. The evaluation results are categorized based on the correctness of the human labels, i.e., whether the correct word was identified for substitution and whether an appropriate substitution was provided. By calculating the p-value, we observe the gap between the p-value and the human labels to explore the issue of subjectivity in human annotations.
%In the second part, we use Equations in \eqref{eq:mr_as}, \eqref{eq:mr_bs}, \eqref{eq:dpo*} and \eqref{eq:dpo*-sigma} to align the model more closely with BARTScore results, aiming to improve the performance of p-value}

\paragraph{Datasets}
We consider four datasets.
The SWS dataset by \citet{wang2023smart} consists of three sets, validation and test labeled by human annotators, and an artificially generated training set.
The training dataset was constructed from Wikipedia sentences, with labels obtained using a combination of PPDB \citep{pavlick2015ppdb} and the Merriam-Webster thesaurus.
We also consider two traditional lexical substitution datasets, LS07 \citep{mccarthy2007semeval} and LS14 \citep{kremer2014substitutes}, which use lemmatized human annotations as ground truth substitutions.
Further, we also consider a general dataset without ground-truth substitutions.
Specifically, we use XSum \citep{narayan2018don}, a summarization dataset consisting of BBC news articles and corresponding summaries.
In the Appendix we show Table~\ref{tb:data_summ} summarizing these four datasets.
%
% \footnote{\label{fn:thesaurus}\url{https://www.merriam-webster.com/thesaurus}.}
%
% The SWS framework includes two types of datasets: a validation and test set labeled by human annotators, and an artificially generated training set. In the labeled datasets, annotators identify target words or phrases within sentences and provide one or more suggestions for each. The training dataset was constructed from Wikipedia sentences, with labels manually generated using thesaurus synonyms. The test dataset was also utilized to examine potential issues with human-labeled suggestions.
%
% However, during both training and testing, we employed an unsupervised learning approach, so the model did not require labels for either phase. 
%
% To further evaluate the model's robustness, during the test stage, we also tested it on the test dataset from three common NLP datasets: LS07 (1,710 sentences) (\cite{mccarthy2007semeval}), LS14 (1,569 sentences) (\cite{kremer2014substitutes}), and XSum (11,334 sentences) (\cite{narayan2018don}). LS07 and LS14 are traditional lexical substitution datasets that focus on generating synonyms for a given word, relying on lemmatized annotations. XSum, recognized in the summarization domain, provides BBC articles and corresponding summaries, offering a diverse set of examples for testing model robustness across broader contexts.
%
% \subsection{Baseline we considered}
%
% \begin{itemize}
%   \item BERT \cite{devlin2018BERT}
%   \item BERTSpsv \cite{zhou2019BERT}
%   \item LexSubCon \cite{michalopoulos2021lexsubcon}
%   \item Bart \cite{lewis2019bart}
%   \item SWS \cite{wang2023smart}
%   \item Chatgpt \cite{}
%   \item Llama \cite{}
% \end{itemize}

\paragraph{Baselines}
We compare our method with three classes of baseline models.
{\em Masked language models (MLMs):} BERT-base-uncased with original MLM head (BERT-naive) \citep{devlin2018BERT}.
BERT-spsv \citep{zhou2019BERT}, as a representative LS model.
BERT-SWS and BART-SWS, both of which were fine-tuned on the SWS training dataset \citep{wang2023smart}.
%
{\em Rule-based models:} The rule-based approach introduced by \citet{wang2023smart}, which leverages a thesaurus.
%(a combination of PPDB \citep{pavlick2015ppdb} and Merriam-Webster thesaurus\footnotemark{\ref{fn:thesaurus}})) for token substitutions.
%
% Unlike traditional lexical substitution (LS) tasks, where the target word is predefined, the SmartWord Suggestions (SWS) task requires systems to identify and improve suboptimal words, which makes traditional LS models incompatible. One approach introduced by SWS assumes substitution attempts for all words in a sentence. If the substituted word differs from the original, the prediction is recorded; otherwise, it is inferred that the model does not wish to make a substitution. However, this method is quite cumbersome, so we only used BERT-spsv \citep{zhou2019BERT} as the representative LS model.
% \rh{I do not understand the purpose of this paragraph.}
% Further, we fine-tune MLM models based on BERT \citep{devlin2018BERT} and BART \citep{lewis2019bart} using the SWS training dataset and also consider a rule-based approach introduced by \citet{wang2023smart} that leverages a thesaurus to produce token substitutions.
%
% In addition, we used BERT-base uncased with an MLM head \citep{devlin2018BERT}, BART-base \citep{lewis2019bart}, and a Rule-based model, as outlined in SWS \citep{wang2023smart}. The first two models were fine-tuned on all SWS training datasets using pre-trained models, while the rule-based model made substitutions based on thesaurus synonyms. Since the original paper did not release model weights, we reproduced the models by following the methodology described in the original work.
%
{\em Prompt-based large language models (LLMs):} We leverage the power of pretrained LLMs via prompts to generate token substitutions.
We consider two popular choices, GPT \citep{ouyang2022training} and LLaMA \citep{touvron2023llama, touvron2023llama2, dubey2024llama},
specifically, GPT-4o and LLaMA-3.1-8B-Instruct.
Prompt-based techniques have gained popularity due to their flexibility and effectiveness in generating high-quality, contextually appropriate substitutions without requiring task-specific tuning \citep{liu2023pre}.
We designed both ranking and non-ranking prompts, which we show in Appendix~\ref{sc:llm_prompts}.
%
% evaluate the performance of LLMs like ChatGPT \citep{ouyang2022training} and LLama \citep{touvron2023llama, touvron2023llama2, dubey2024llama} in the SWS task. Prompt-based techniques have gained popularity due to their flexibility and effectiveness in generating high-quality, contextually appropriate substitutes without requiring task-specific tuning. We designed both ranking and non-ranking prompts. The further details can be found in the appendix.

\paragraph{Evaluation Metrics}
%
We use the cosine similarity (CS) to measure the correlation between model predictions and model-based scores, {\em i.e.}, BARTScores.
Since BARTScore produces log-likelihoods, we log-transform model predictions accordingly.
We also consider the quality of the substitutes $\{\tilde{w}_k\}_{k=1}^K$.
First we get the score ratio between the sentence with and without substitution and then calculate the average of BARTScore ratios as $\text{ABR}=1/K\sum_{k=1}^{K}M(\tilde{X}_{k})/M(X)$.
% $\hat{M}(\tilde{X}_{k}) = M(\tilde{X}_{k})/M(X)$

% \subsubsection{Evaluation metrics} \hspace{0.2cm} To evaluate the model's prediction performance and explore the subjectivity in human annotations, we developed a p-value method inspired by the idea of a permutation test. The hypothesis tested here is that a word sampled from the model's distribution should outperform the word predicted by the trained model. This approach provides a statistical basis to assess the quality of a substitution in relation to the model's prediction. In practice, we sample 1,000 words from the model's distribution and calculate the proportion of sampled words that outperform the top-1 prediction based on BARTScore. If the p-value falls below the significance threshold (set to 0.01 in this study), we reject the hypothesis, concluding that the model's top-1 prediction is significantly aligned with the true label, reducing the likelihood of subjective bias in the prediction.

% To further align the training model with our evaluation metric, BARTScore, we first sort the candidate words by their predicted probabilities and then compute the logarithm of these probabilities. We calculate the cosine similarity between the log probabilities and their corresponding BARTScore values, both of which are logarithmic. This approach allows us to assess the degree of alignment between the model's predictions and the evaluation metric. 

% During model training, candidate words with varying quality, as measured by BARTScore, may be included in the candidate list. While calculating the sum of BARTScores could offer insight into the overall quality, BARTScore values are not standardized, making it difficult to judge individual candidates based on their raw scores. To address this, we calculate the BARTScore ratio, which compares the BARTScore of a sentence with a new candidate to the BARTScore of the original sentence. This standardized ratio provides a more reliable measure of candidate quality. Therefore, instead of summing raw BARTScores, we compute the sum of BARTScore ratios to evaluate the overall quality of the candidate list more accurately.

\paragraph{Implementation}
% To explore the subjectivity in human annotations, we utilized a pre-trained BERT model following the methodology from \citep{wang2023smart} to examine the human-labeled results in the SWS test dataset. Given that the original paper did not release the model weights, we reproduced the model based on the original work's description. The goal was to investigate the gap between the real distribution of answers, ideally labeled by a larger group of annotators, and the smaller set of ten annotators used in the dataset. For hyperparameter settings and further reproduction details, please refer to the appendix.
% \rh{This paragraph seems pointless.}
%
For our model, we fine-tuned the BERT-base-uncased model \citep{wang2023smart} on 100k randomly sampled sentences from the SWS training dataset using one Nvidia A100 GPU.
For each sentence, we randomly selected 5 tokens, and for each token, we chose $K=5$ candidates from the model’s output to form a candidate pool and compute the training loss.
The model was fine-tuned over 5 epochs.
For DPO, we duplicated the same BERT model and weights at initialization, freezing one as the reference while only updating the weights of the other during training.
During testing, we used the same token selection strategy, randomly selecting 5 tokens per sentence and choosing the top 5 candidates for each.
% We then calculated the cosine similarity between the model log probabilities and the candidates' BARTScore, as well as the sum of the candidates' BARTScore ratios.
Hyperparameter settings are described in Appendix~\ref{sc:hyperparams}.

% It is worth noting that the proposed approach trained using \eqref{} or \eqref{} is done without access to human annotations thus unsupervised.
% Moreover, we trained our models on a relatively small (random) subset of the SWS dataset (100k of 3.9M sentences) mainly due to computational cost, however, it is likely that training with the larger dataset will improve upon the performance characteristics presented in the results below.
%
%Due to the high computational cost of our model, which integrates both a pre-trained model and a Bart model for dynamic candidate scoring, it was not feasible to fine-tune on the entire dataset. The scoring process occurs online, with candidate words changing dynamically, further increasing the complexity. As a result, we randomly selected 100,000 sentences from the 3,909,650 available SWS training sentences (\cite{wang2023smart}) and fine-tuned the model using different loss functions. Unlike traditional lexical substitution tasks, the SWS task aims to enhance the overall quality of sentences by identifying and replacing suboptimal words.
%
% \subsection{Result}
% \begin{itemize}
%     \item we employed p-value metric in SWS test data, showing some convince cases.
%     \item cosine+sum of BARTScore performance
%     \item benchmark for p-value metric
%     % \item example
%     \item top2 BARTScore performance
%     % \item ablation study
%     \textcolor{red}{\item gpt/llama (with order and without order)}
%     \item divided test data for ls07/14 dataset (same situation)
% \end{itemize}

% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.
%
% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.

\begin{table}[t]
\centering
\scalebox{0.75}{
%\begin{tabular}{c|c|cc|c}
%           & Not change    & \multicolumn{1}{c|}{Agreement} & Disagreement & Total \\ \hline
%Change     & 4125 (0.738)  & \multicolumn{1}{c}{631 (0.113)}    & 830 (0.149)      & 5587  \\
%Not change & 14138 (0.936) & \multicolumn{2}{c|}{969 (0.064)}                       & 15107
%\end{tabular}
\begin{tabular}{c|ccc}
    & Not change & \multicolumn{1}{c}{Agreement} & Disagreement \\ 
    \hline
    Change & 4125 (0.74) & \multicolumn{1}{c}{631 (0.11)} & 830 (0.15) \\
    Not change & 14138 (0.94) & \multicolumn{2}{c}{969 (0.06)}
\end{tabular}
}
\vspace{-2mm}
\caption{Token changes in SWS test data.
Rows are for annotator changes and columns indicate model changes and if they are in agreement with the annotator.
In parenthesis are proportions relative to the row totals.
}
\label{Sws_test_data_stat}
\vspace{-2mm}
\end{table}

\paragraph{Illustrative Example}% \label{sec:Example}
% \begin{figure}[H] % Specify image placement (h=here, t=top, b=bottom, p=separate page)
%     \centering
%     \includegraphics[width=1\textwidth]{figures/Picture 1.png} % Include the image file
%     \caption{Workflow} % Add a caption to the image
%     \label{fig:your_label} % Add a label for referencing in the text
% \end{figure}
%
% In Section \ref{sec:p-value}, we proposed a model-based statistic to evaluate the quality of token substitutions. 
% Here we evaluate such statistic on a dataset for which human annotations are available.
% For this, we simply use the pre-trained BERT model from SWS \citep{wang2023smart}.
%
% that combining the p-value \eqref{eq:p-value} with hypothesis testing can measure the overall quality of generated substitution.
% By evaluating such statistic ($p$-value) using a human annotated dataset and comparing it with human annotations, we are able to underscore the gap between the statistic and human annotations.
% Here we utilized a pre-trained BERT model from SWS \citep{wang2023smart} to analyze the results from the SWS test dataset. 
%
We stratify tokens in the SWS test set into five groups according to whether they were substituted by human annotators and/or the model.
These groups are:
change agreement (CA: both model and the human annotator agreed on the change),
change disagreement (CD: both model and the annotator suggested a change, but the model substitution did not match that of the annotator),
no change agreement (NCA: the model and the annotator kept the token unchanged),
only model changed (OMC: the model suggested a change, but the annotator did not find it necessary), and
only annotator changed (OAC: the annotator suggested a change, but the model did not).

\iffalse
\begin{table}[t]
\centering
\scalebox{0.48}{
\begin{tabular}{lp{13.6cm}}
% \hline
\multirow{4}{*}{CD} & \begin{tabular}[c]{@{}l@{}}While being saddened by my friend's suffering, I also gradually became aware of the \\ \textcolor{red}{critical} role of law in our daily life.\\ Annotations: Human=\{"integral", "important"\}, Model=\{"crucial"\}\end{tabular} \\
\cline{2-2} 
& \begin{tabular}[c]{@{}l@{}}The residential theory \textcolor{red}{argues} that there are houses in the Chaco structures, and the \\ structure is big enough for hundreds of people. \\ Annotations: Human=\{"discussed", "states",   "reasons", "discusses"\}, Model=\{"claims"\}\end{tabular} \\
\hline
\multirow{5}{*}{OMC} & \begin{tabular}[c]{@{}l@{}}Therefore, the power to \textcolor{red}{resolve} disputes and maintain social order is the psychological \\ restraint of the actor and the education of the elder to the \\young rather than the litigation.\\ Annotations: Human=\{"resolve"\}, Model=\{"settle"\}\end{tabular} \\
\cline{2-2} 
& \begin{tabular}[c]{@{}l@{}}According to recent statistics, there is ample coverage demonstrating that humans \\ who inhabit the hub \textcolor{red}{continually} will undergo severe conditions \\due to the dirty air and stressful surroundings.\\ Annotations: Human =\{"continually"\}, Model=\{"constantly"\}\end{tabular} \\
% \hline
\end{tabular}
}
\vspace{-2mm}
\caption{Original sentences with target token substitution in \textcolor{red}{red}. CD: change disagreement and OMC: only model changed.
}
\label{fig:cases}
\end{table}
\fi
 
Results in Table~\ref{Sws_test_data_stat} show that most tokens remain unchanged (NCA: 94\%) by both model (BERT-SWS) and annotator.
The proportion of annotator changes for which the model agrees or disagrees is similar, CA: 11\% {\em vs}. CD: 15\%, respectively.
Interestingly, most annotator changes are not captured by the model (OAC: 74\%), which is consistent with results in \citet{wang2023smart} for a variety of models, and OMC: 6\% tokens not changed by the annotator are changed by the model.
These results underscore that $i$) a model is unlikely to match human annotations when these are largely subjective and incomplete; and $ii$) cases in which changes made by the model do not match the human annotator cannot be considered errors.
These are consequences of having a few annotators each selecting which tokens need to be changed.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\columnwidth]{figures/cases_v2.png}
%     \caption{Original (\textcolor{mblue}{blue}) and modified (\textcolor{mgreen}{green}) sentences with substitution in \textcolor{mred}{red}.
%     Modifications were done by the BERT-SWS model.
%     % Blue is the original sentence from the SWS test dataset, green is the sentence with the substitution generated from the BERT\_SWS model. Red is the substitution word.
%     } % Add a caption to the image
%     \label{fig:cases} % Add a label for referencing in the text
% \end{figure}

This is further illustrated below in Table~\ref{table_benchmark} where we show that though tokens for which both model and annotator agree have better model-based statistics, there is also a non-insignificant portion for which model and annotator disagree, but the model-based statistic suggests that the model changes are of good quality.
Some of these examples are shown in Table~\ref{tb:cases} of the Appendix.
%(with additional examples in the Appendix).

% In the categories of CIC and NCC, we identified examples where the $p$-value met the significance threshold.
% Upon closer examination, we found that many of these cases were indeed miss-labeled by human annotators.
% Some examples are presented in Figure~\ref{fig:cases} (additional examples are shown in the Appendix).
% This highlights the issue of incomplete ground truth due to a limited number of annotators and further supports the conclusion that the $p$-value metric effectively measures the overall quality of a model in generating suitable token substitutes.

% First, we utilized the BERT model from SWS \citep{wang2023smart} and conducted tests (provide suggestions for each token $x_{n}$) on the SWS test dataset \citep{wang2023smart}.
% For each token, based on human annotations, we determined whether the token should be changed and identified the correct substitution from the annotators' perspective.
% We then divided the test results into five categories based on whether the model correctly identified the token's position and provided an appropriate substitution.
% These categories are: changed correctly (CC: both the model and the human annotator agreed on the change), changed incorrectly (CINC: both the model and the annotator suggested a change, but the model substitution did not match that of the annotator), not changed correctly (NCNC: either the model nor the annotator suggested a change), not changed but changed (NCC: the model suggested a change, but the annotator did not think it was necessary), and changed but not changed (CNC: the annotator suggested a change, but the model did not).
% Results in Table~\ref{Sws_test_data_stat} shows the test performance statistics. Despite some of the model's predictions not matching the ground truth, the possibility of incomplete human labels promoted us to use $p$-value for further exploration.
% The 'cnotc' category indicates cases where . The 'cc' category refers to cases where , and the model correctly predicted the substitution. 'cinc' represents instances where . Tokens where n are categorized as 'notcnotc'. Finally, 'notcc' refers to cases where .

% We calculated the $p$-value metrics for each of these categories.
% Result for each category are shown in the first row  of Table~\ref{table_benchmark}.
% The numbers indicate the proportion of model predictions that achieved a $p$-value below significance threshold ($\alpha=0.01$), representing the proportion of substitutions passing the statistical significance test.
% Higher values are preferable.
% We observed that the model achieved high proportions in the CC and NCNC categories, which is expected, as there is greater reliability when both the model and the human annotators agree on whether a token should be changed or not.
% Interestingly, CNC also had a high proportion, suggesting that some tokens may not require substitution, as the original token was adequate.
% Surprisingly, the CIC and NCC categories reached proportions of 0.731 and 0.628, respectively.
% These findings suggest that, in some cases, the model preferred changes that the human annotators did not agree with.
% This allows us to capture the potential shortcomings of substitution labels provided by a limited number of human annotators.

% In the categories of CIC and NCC, we identified examples where the $p$-value met the significance threshold.
% Upon closer examination, we found that many of these cases were indeed miss-labeled by human annotators.
% Some examples are presented in Figure~\ref{fig:cases} (additional examples are shown in the Appendix).
% This highlights the issue of incomplete ground truth due to a limited number of annotators and further supports the conclusion that the $p$-value metric effectively measures the overall quality of a model in generating suitable token substitutes.

\begin{table}[t]
\centering
\adjustbox{width=\columnwidth}{
\begin{tabular}{c|cc|cc|cc|cc|cc}
            & \multicolumn{2}{c|}{SWS}      & \multicolumn{2}{c|}{LS07}     & \multicolumn{2}{c|}{LS14}     & \multicolumn{2}{c|}{XSum}     & \multicolumn{2}{c}{AVG}                                                                                       \\
            & CS            & ABR           & CS            & ABR           & CS            & ABR           & CS            & ABR           & CS                                                                 & ABR                                      \\ \hline
MR          & \textbf{0.99} & 0.82          & 0.93          & 0.81          & 0.93          & 0.78          & 0.93          & 0.79          & 0.94$\pm$0.024                                                     & 0.8$\pm$\textbf{0.012}  \\
DPO                               & 0.91          & 0.87          & 0.9           & 0.86          & 0.91          & 0.83          & 0.85          & 0.82          & 0.89$\pm$0.025                                                     & 0.85$\pm$0.025                           \\
DPO*                              & 0.92          & 0.87          & 0.91          & 0.86          & 0.91          & 0.82          & 0.87          & 0.83          & 0.9$\pm$0.019                                                      & 0.85$\pm$0.021                           \\
$\sigma$DPO* & 0.93          & 0.89          & 0.93          & 0.86          & 0.92          & 0.84          & 0.88          & 0.84          & 0.92$\pm$0.021  & 0.86$\pm$0.02                            \\
MR+BS       & 0.94          & \textbf{0.90} & 0.95          & \textbf{0.87} & 0.95          & \textbf{0.86} & 0.95          & \textbf{0.86} & 0.94$\pm$0.005                                                     & \textbf{0.87}$\pm$0.015 \\
MR+AS       & 0.98          & 0.890         & \textbf{0.99} & 0.85          & \textbf{0.99} & 0.83          & \textbf{0.99} & 0.84          & \textbf{0.99}$\pm$\textbf{0.002} & 0.86$\pm$0.021                          
\end{tabular}}
\vspace{-2mm}
\caption{
Ablation CS and ABR metrics.
Figures for each dataset are medians over all token predictions.
The last column shows averages over all datasets with standard deviations.
The best results are highlighted in {\bf bold}.
}
\label{table_training}
\end{table}

\paragraph{Ablation Study}
%
% We expected that, for each category, a larger proportion of substitutions would fall below the significance level. Ideally, this can be achieved by guiding the model to output substitutions with higher BARTScore values, effectively aligning the model's output with the scoring metric. A suitable approach to address this is through the use of ranking losses. We drew inspiration from Margin Ranking Loss and Direct Preference Optimization (DPO) loss to achieve this alignment.
%
% We identified the limitations of Margin Ranking Loss and DPO loss, noting that these approaches fail to adequately account for the quality of individual candidates, and averaging across candidates is not suitable for our task. To address this, 
Section~\ref{sec:Preference-Aware-Learning} introduced two approaches to align the predictions of a trained model with model-based scores, namely MR in \eqref{eq:constra} and DPO in \eqref{eq:dpo}, \eqref{eq:dpo*} and \eqref{eq:dpo*-sigma}.
For the former, we consider two ways of improving the model-based scores, via a weighted average in \eqref{eq:mr_as} and improvement relative to a reference model in \eqref{eq:mr_bs}.
Results in Table~\ref{table_training} show results comparing $i$) the alignment of model predictions with model-based scores using median CS, and $ii$) the average model-based scores using median ABR.
Distributions of CS and ABR for all tokens are shown in the Appendix Figures~\ref{fig:bench_cs} and \ref{fig:bench_abr}.
We see that
$i$) the MR loss alone is not sufficient to deliver the best CS and produces the worst ABR; 
$ii$) DPO variants improve the ABR relative to MR, but not in terms of CS;
$c$) MR+BS and MR+AS produce the best overall ABR and CS, respectively, however, MR+AS seems to provide the best trade-off between the two performance metrics, while outperforming both MR and DPO.
Consequently, in the following experiments we will focus on MR+AS.
%
%We can observed from Table \ref{table_training}:
%
%1. When using only Contrastive loss, we achieve a CS score of 0.944, second only to Contra-aob. However, the Avb score is only 0.8, the lowest among the models. This indicates that while Contrastive loss can optimize correlation, it does not ensure the quality of candidate words. When Aob and Top1 terms are added, the Avb score significantly improves, while the CS score remains high or even improves. This demonstrates the effectiveness of these two terms.

%
% 3. DPO, a well-known loss function for optimizing correlation, performs poorly on the CS metric. Interestingly, it performs quite well on the Avb metric. This suggests that during the ranking process, some higher-quality candidates are pushed to the top, leading to an improvement in Avb. Additionally, the version with log-sigma performs better than the one without it.
%
%4. Contra-aob consistently achieves the best results across all four datasets on the CS metric. For Avb, it performs second only to Contra-top1. As a result, we selected Contra-top1 as our best model for further experiments.

% We designed the loss function based on the Contrastive Loss \citep{chern2023improving, liu2022brio} and DPO loss \citep{rafailov2024direct}) and trained the model accordingly. During prediction, we selected the top K candidates for each token to form the candidate list. The evaluation metrics used were the cosine similarity between the model’s probability distribution and the BARTScore across the candidate list, and the average of the BARTScore ratios. The first metric captures the correlation between the model's predictions and the scoring, while the second metric assesses the overall quality of the candidates. Table \ref{table_training} illustrates the performance of different loss variants across different datasets.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \begin{table}[]
% \centering
% \adjustbox{width=\columnwidth}{
% \begin{tabular}{c|cc|cc|cc|cc|cc}
% % \hline
%             & \multicolumn{2}{c}{SWS}                                         & \multicolumn{2}{c}{LS07}   & \multicolumn{2}{c}{LS14}                                      & \multicolumn{2}{c}{XSum}                                      & \multicolumn{2}{c}{AVG}                                                                                                             \\
% % \cline{2-11} 
%                 & \multicolumn{1}{c|}{cs}             & \multicolumn{1}{c|}{avb}   & \multicolumn{1}{c|}{cs}             & \multicolumn{1}{c|}{avb} & \multicolumn{1}{c|}{cs}             & \multicolumn{1}{c|}{avb} & \multicolumn{1}{c|}{cs}             & \multicolumn{1}{c|}{avb} & \multicolumn{1}{c|}{cs}                                                                  & avb                                       \\
% \hline
% MR    & \multicolumn{1}{c|}{\textbf{0.986}} & 0.816                      & \multicolumn{1}{c|}{0.928}          & 0.809                    & \multicolumn{1}{c|}{0.93}           & 0.782                    & \multicolumn{1}{c|}{0.931}          & 0.794                    & \multicolumn{1}{c|}{0.944$\pm$0.024}                                                     & 0.8$\pm$\textbf{0.012}   \\
% % \hline
% $\sigma$DPO  & \multicolumn{1}{c|}{0.917}          & 0.875                      & \multicolumn{1}{c|}{0.916}          & \textbf{0.855}                    & \multicolumn{1}{c|}{0.919}          & 0.833                    & \multicolumn{1}{c|}{0.92}           & \textbf{0.845}                    & \multicolumn{1}{c|}{0.918$\pm$\textbf{0.002}}                           & 0.852$\pm$0.014                           \\
% % \hline
% DPO & \multicolumn{1}{c|}{0.918}          & 0.849                      & \multicolumn{1}{c|}{0.87}           & 0.828                    & \multicolumn{1}{c|}{0.874}          & 0.805                    & \multicolumn{1}{c|}{0.873}          & 0.817                    & \multicolumn{1}{c|}{0.884$\pm$0.02}                                                      & 0.825$\pm$0.014                           \\
% % \hline
% MR+BS    & \multicolumn{1}{c|}{0.935}          & \textbf{0.898}             & \multicolumn{1}{c|}{0.945}          & \textbf{0.872}           & \multicolumn{1}{c|}{0.948}          & \textbf{0.856}           & \multicolumn{1}{c|}{0.945}          & \textbf{0.86}            & \multicolumn{1}{c|}{0.943$\pm$0.005}                                                     & \textbf{0.871}$\pm$0.015 \\
% % \hline
% MR+AS     & \multicolumn{1}{c|}{0.983}          & 0.896                      & \multicolumn{1}{c|}{\textbf{0.988}} & 0.854                    & \multicolumn{1}{c|}{\textbf{0.988}} & 0.834                    & \multicolumn{1}{c|}{\textbf{0.987}} & 0.844                    & \multicolumn{1}{c|}{\textbf{0.987}$\pm$\textbf{0.002}} & 0.857$\pm$0.021                           \\
% % \hline
% \end{tabular}
% }
% \caption{Median value of different methods; cs indicate cosine similarity, avb indicate average of sum of BARTScore (ratio). The $\pm$ represent standard deviation. The best result is highlighted in bold.
% }
% \label{table_training}
% \end{table}

\begin{table}[t]
\centering
\scalebox{0.47}{
\begin{tabular}{c|cc|cc|cc|cc|cc}
                   & \multicolumn{2}{c|}{SWS}      & \multicolumn{2}{c|}{LS07}     & \multicolumn{2}{c|}{LS14}     & \multicolumn{2}{c|}{XSum}     & \multicolumn{2}{c}{AVG}                                                                                        \\
                   & CS            & ABR           & CS            & ABR           & CS            & ABR           & CS            & ABR           & CS                                                                 & ABR                                       \\ \hline
BERT-naive      & 0.92          & 0.87          & 0.92          & 0.83          & 0.92          & 0.82          & 0.93          & 0.81          & 0.92$\pm$0.003                                                     & 0.83$\pm$0.018                            \\
BERT-spsv         & 0.93          & 0.82          & 0.93          & 0.80          & 0.93          & 0.74          & 0.93          & 0.74          & 0.93$\pm$0.002                                                     & \multicolumn{1}{l}{0.78$\pm$0.038}        \\
BERT-SWS          & 0.92          & 0.84          & 0.92          & 0.81          & 0.93          & 0.78          & 0.93          & 0.79          & 0.93$\pm$0.003                                                     & 0.81$\pm$0.02   \\
BART-SWS         & 0.93          & 0.84          & 0.92          & 0.83          & 0.92          & 0.77          & 0.93          & 0.80          & 0.93$\pm$0.003                                                     & \multicolumn{1}{l}{0.81$\pm$0.028}        \\
Rule-based        & 0.93          & \textbf{0.90}          & 0.92          & 0.83          & 0.93          & 0.82          & 0.93          & 0.83          & 0.93$\pm$0.001                                                     & \multicolumn{1}{l}{0.85$\pm$0.024}        \\
GPT-4o             & 0.95          & 0.89          & 0.95          & \textbf{0.86} & 0.96          & 0.82          & 0.96          & \textbf{0.85} & 0.95$\pm$0.002                                                     & \textbf{0.86}$\pm$\textbf{0.025} \\
LLaMA              & 0.93          & 0.86          & 0.94          & 0.84          & 0.93          & 0.76          & 0.94          & 0.79          & 0.94$\pm$0.003                                                     & 0.81$\pm$0.040                             \\
MR+AS & \textbf{0.98} & \textbf{0.90} & \textbf{0.99} & 0.85          & \textbf{0.99} & \textbf{0.83} & \textbf{0.99} & 0.84          & \textbf{0.99}$\pm$\textbf{0.002} & \textbf{0.86}$\pm$\textbf{0.021} 
\end{tabular}
}
\vspace{-2mm}
\caption{
Benchmark median CS and ABR metrics.
The last column shows averages over all datasets with standard deviations.
The best results are highlighted in {\bf bold}.
}
\label{baseline_comparison}
\end{table}

\paragraph{Model Benchmark}
%
% \begin{table}[]
% \centering
% \scalebox{0.43}{
% \begin{tabular}{|c|cc|cc|cc|cc|cc|}
% \hline
%               & \multicolumn{2}{c|}{SWS}                    & \multicolumn{2}{c|}{LS07}                   & \multicolumn{2}{c|}{LS14}                   & \multicolumn{2}{c|}{XSum}                                                         & \multicolumn{2}{c|}{AVG}                                                                                                          \\ \cline{2-11} 
%               & \multicolumn{1}{c|}{cs}             & avb   & \multicolumn{1}{c|}{cs}             & avb   & \multicolumn{1}{c|}{cs}             & avb   & \multicolumn{1}{c|}{cs}                            & avb                          & \multicolumn{1}{c|}{cs}                                                                  & avb                                    \\ \hline
% BERT\_vanilla & \multicolumn{1}{c|}{0.917}          & 0.865 & \multicolumn{1}{c|}{0.923}          & 0.833 & \multicolumn{1}{c|}{0.922}          & 0.821 & \multicolumn{1}{c|}{0.926}                         & 0.812                        & \multicolumn{1}{c|}{$0.922^{\pm0.003}$}                                                     & $0.832^{\pm0.018}$                       \\ \hline
% BERT\_spsv    & \multicolumn{1}{c|}{0.929} & 0.823 & \multicolumn{1}{c|}{0.926}          & 0.803 & \multicolumn{1}{c|}{0.927}          & 0.738 & \multicolumn{1}{c|}{0.931}                         & 0.739                        & \multicolumn{1}{c|}{$0.928^{\pm0.002}$}                                                   & \multicolumn{1}{l|}{$0.776^{\pm0.038}$} \\ \hline
% Bart (SWS)     & \multicolumn{1}{c|}{0.925}          & 0.844 & \multicolumn{1}{c|}{0.923}          & 0.829 & \multicolumn{1}{c|}{0.924}          & 0.771 & \multicolumn{1}{c|}{0.931}                         & 0.797                        & \multicolumn{1}{c|}{$0.926^{\pm0.003}$}                                                   & \multicolumn{1}{l|}{$0.810^{\pm0.028}$} \\ \hline
% Rule\_based   & \multicolumn{1}{c|}{0.926}          & 0.883 & \multicolumn{1}{c|}{0.924}          & 0.834 & \multicolumn{1}{c|}{0.925}          & 0.82  & \multicolumn{1}{c|}{0.927} & 0.83 & \multicolumn{1}{c|}{$0.926^{\pm0.001}$}                                                    & \multicolumn{1}{l|}{$0.842^{\pm0.024}$} \\ \hline
% Gpt-4o        & \multicolumn{1}{c|}{0.953}          & 0.88 & \multicolumn{1}{c|}{0.952}               &   \textbf{0.866}    & \multicolumn{1}{c|}{0.956}               &   0.823    & \multicolumn{1}{c|}{0.955}                              &   \textbf{0.856}                           & \multicolumn{1}{c|}{$0.954^{\pm0.002}$}                                                               & $0.856^{\pm0.021}$                                 \\ \hline
% Llama         & \multicolumn{1}{c|}{0.934}           & 0.862 & \multicolumn{1}{c|}{0.936}               &  0.842     & \multicolumn{1}{c|}{0.933}               &    0.764   & \multicolumn{1}{c|}{0.94}                              &                      0.785        & \multicolumn{1}{c|}{$0.936^{\pm0.003}$}                                                               & $0.813^{\pm0.04}$                                  \\ \hline
% BERT(SWS)     & \multicolumn{1}{c|}{0.92}           & 0.841 & \multicolumn{1}{c|}{0.924}          & 0.807 & \multicolumn{1}{c|}{0.926}          & 0.781 & \multicolumn{1}{c|}{0.928}                         & 0.793                        & \multicolumn{1}{c|}{$0.925^{\pm0.003}$}                                                     & $0.806^{\pm\textbf{0.02}}$                         \\ \hline
% BERT (Margin\_aob)   & \multicolumn{1}{c|}{\textbf{0.983}}          & \textbf{0.896} & \multicolumn{1}{c|}{\textbf{0.988}} & 0.854 & \multicolumn{1}{c|}{\textbf{0.988}} & \textbf{0.834} & \multicolumn{1}{c|}{\textbf{0.987}}                & 0.844                        & \multicolumn{1}{c|}{$\textbf{0.987}^{\pm\textbf{0.002}}$} & $\textbf{0.857}^{\pm0.021}$     \\ \hline                  
% \end{tabular}}
% \caption{Median value of different methods; cs indicate cosine similarity, avb indicate average of sum of BARTScore (ratio). The $\pm$ represent standard deviation. The best result is highlighted in bold.
% }
% % \label{table_training}
% \end{table}
%
Next, we evaluate MLMs (BERT, BART) and LLMs (GPT-4o, LLaMA) on all datasets using median CS and ABR as metrics (distributions are shown in the Appendix Figures~\ref{fig:bench_cs} and \ref{fig:bench_abr}).
Table~\ref{baseline_comparison} shows that MR+AS outperforms both MLMs (BERT, BART) and LLMs (GPT-4o, LLaMA), with top performances in terms of CS in all datasets and top ABR in two of the datasets (SWS and LS14).
Also, when accounting for results variation across datasets, we see that MR+AS significantly outperforms the others in terms of CS, while being comparable with GPT-4o in terms of ABR.
Importantly, for the latter, MR+AS and GPT-4o perform markedly better than other approaches.
%
% excelling in the SWS, LS07, and LS14 datasets with cosine similarity scores of 0.983, 0.896, and 0.988, respectively. This demonstrates the effectiveness of our approach in enhancing sentence quality across multiple datasets. Gpt-4o also shows commendable performance, particularly in XSum, with a cosine similarity score of 0.955, indicating its competitive capabilities. 
%
% BERT vanilla refers to the original pre-trained uncased version. Although the average CS results are similar, the Avb score decreases. One possible explanation is that the SWS training set treats only synonyms in the thesaurus as valid replacements for the target word, with only one substitution per word. This provides insufficient information for training the model on word usage, which is consistent with the conclusion from SWS paper \citep{wang2023smart}.

\begin{table}[t]
\centering
\scalebox{0.7}{
\begin{tabular}{c|ccc}
    & CA             & CD             & OMC           \\
    \hline
    BERT-naive & 0.646          & 0.389          & 0.612         \\
    BERT-spsv    & 0.709          & 0.442          & 0.491         \\
    BERT-SWS     & 0.204          & 0.118          & 0.078         \\
    BART-SWS     & 0.225          & 0.139          & 0.121         \\
    Rule Based   & 0.602          & 0.241          & 0.344         \\
    GPT-4o        & 0.761          & 0.563          & 0.661         \\
    LLaMA         & 0.700          & 0.517          & 0.603         \\
    MR+AS         & \textbf{0.961} & \textbf{0.917} & \textbf{0.810}
\end{tabular}
}
\vspace{-2mm}
\caption{Proportion of $p$-values below the significance threshold ($\alpha=0.01$) on the SWS test data.
}
\label{table_benchmark}
\end{table}

\paragraph{Model-based Statistic Benchmark}
%
After showing the effectiveness of our model in terms of CS and ABR, we now use the model-based statistic introduced in Section~\ref{sc:model-based_stat}.
Table~\ref{table_benchmark} shows the proportion of model predictions that meet the significance threshold ($\alpha=0.01$) for three groups of tokens: CA, CD and OMC.
We do not present results for the other two groups (NCA and OAC) because when using rule- and LLM-based models, we get no candidates for non-substitutions.
Further, for these models we may not always get the same number of substitute candidates per token, thus we use their median (3 candidates) across all tokens in the SWS test set as the number of candidates for all other models when calculating the statistic, {\em i.e.}, $K_s=3$ in \eqref{eq:p-value}.
In the Appendix Table~\ref{tb:pv_all} we show an extended table showing the NCA and OAC groups and $K_s=1000$ for MLM-based models (ours included) and Figures~\ref{fig:benchmark1}-\ref{fig:benchmark4} with $p$-value distributions for all models.
To avoid problems associated with overfitting, thus making the evaluation more reliable, in Appendix Table~\ref{tb:pv_gptscore_GPT2-M} and Table~\ref{tb:pv_gptscore_OPT350M} we extended the evaluation result by using the GPTScore\footnote{Similar to BARTScore, we adapted GPTScore for the paraphrasing task using the following prompt: “Rewrite the following text with the same semantics. \{original sentence\} In other words, \{the modified sentence\}”. We also calculated the Spearman correlation between BARTScore and GPTScore (GPT2-medium and OPT-350M), which yielded results of 0.929 and 0.921, respectively. These values indicate a strong correlation between BARTScore and GPTScore.} \citep{fu2023gptscore} with two backbone models: GPT2 \citep{radford2019language} and OPT \citep{zhang2022opt}. The results show that our MR+AS still outperforms other baseline models.

Ideally, we want proportions in Table~\ref{table_benchmark} to be close to one, indicating that the top candidate is of better quality than the alternatives.
Effectively, results demonstrates that our model (MR+AS) not only produces the largest proportions, but it does so irrespective of the group (CA, CD or OMC).
This suggests that substitutions made by our model are of good quality (using BARTScore as proxy for quality) even if they do not agree with the human annotator.
Moreover, in general, LLMs are better than the other MLM- and rule-based alternatives.

It is worth noting that LLM-based models make considerably more substitutions than our model.
In the Appendix Table~\ref{p-value-proportion-full} we show that GPT-4o makes 24.4\%, 24.8\% and 11.5\% changes in groups CA, CD and OMC, respectively, compared to 5.5\%, 5.2\% and 2.7\% by our MR+AS.
This suggests a trade-off between quality and quantity of substitutions between MR+AS and GPT-4o, respectively.
% In the Appendix we show detailed metrics for all models.

% After illustrating the effectiveness of the model-based statistic, we further applied it to test the MLMs (BERT, BART) and LLMs (GPT-
% 4o, LLaMA) to construct a comprehensive benchmark for evaluating the quality of token suggestions.
%
% Table~\ref{table_benchmark} shows the proportion of model predictions that meet the significance threshold ($\alpha=0.01$).
% When using prompts to instruct the model to make substitutions in a sentence, the model only outputs replacement results, thus lacking candidates for cases where no substitutions are made.
% Consequently, we are unable to capture the model’s preferred candidates when no change is required.
% As a result, we cannot evaluate the cnotc and notcnotc categories (indicated by a dash (-)).
%
% It is important to note that caution is required when utilizing the $p$-value in prompt-based LLMs.
% One of the key assumptions of hypothesis testing is the ability to sample a large number of tokens.
% However, when using prompt-based approaches, especially with closed-source models like GPT-4o, obtaining the model's full distribution becomes challenging.
% Additionally, generating a large number of candidates from such models is often prohibitive.
%
% Ideally, we want the number as high as possible, indicating that a larger proportion of predictions passed the hypothesis testing.
% From Table~\ref{table_benchmark}, we can observe that BERT-SWS, BERT-Spsv, and BART-SWS exhibit decent p-values for the cnot and cc categories.
% However, their $p$-values for cinc, notcc, and notcnotc are relatively low, indicating that many of their predictions did not pass the hypothesis testing.
% In contrast, our model (MR+AS), optimized for both quality and correlation, achieves the highest proportion of significant $p$-values across all categories.
%
% From the Table \ref{table_benchmark}, we can observe the following:
%
% 1. BERT\_SWS, BERT-spsv, and BART\_SWS exhibit decent p-values for cnot and cc. However, their p-values for cinc, notcc, and notcnotc are relatively low, indicating that many of their predictions did not pass the permutation test. 
%
% 2. Our model (MR+AS), after being optimized for both quality and correlation, achieves the highest p-values significance proportion across all categories. 
%
% Due to the space limitation, the detail distribution is put into the Appendix.
%
% At the same time, while ensuring high $p$-values, we hope the proportion of tokens involved for the CC, CINC, and NCC categories are also as high as possible, meaning the model identified more words that could be replaced.
% The statistical result of proportion of token involved is shown in Table~\ref{p-value-proportion}. 
%
% As observed from Table \ref{p-value-proportion}, BERT-Spsv shows notably low replacement rates in both the CC and CINC categories. However, the proportion of replacements in these categories suggests that our model adopts a more conservative replacement strategy compared to BERT-SWS and BART-SWS. This indicates that while our model enhances the quality of substitutions, it does so by reducing the overall number of replacements. We explore this compensatory effect further in the section below.
%
% We observed that BERT-spsv has very low replacement rates in the cc and cinc categories.
%
% However, the proportion for cc and cinc indicate that our model's replacement strategy is more conservative compared to BERT\_SWS and BART\_SWS. This suggests that while our model improves the quality of the substitutions, it does so by reducing the proportion of replacements. We explore the compensatory experiment in the next section. 
% This trade-off presents opportunities for future improvement.

\begin{table}[t]
\centering
\scalebox{0.7}{
\begin{tabular}{c|cccc|r}
    & SWS            & LS07           & LS14           & XSum           & AVG \\
    \hline
    BERT-naive      & 0.88          & 0.84          & 0.82          & 0.81          & 0.84$\pm $ 0.025                                                      \\
    BERT-spsv         & 0.82           & 0.80          & 0.73          & 0.73          & 0.77$\pm $ 0.040                                                       \\
    BERT-SWS         & 0.85          & 0.82          & 0.78           & 0.79          & 0.81$\pm $ 0.028                                                      \\
    BART-SWS         & 0.82          & 0.81          & 0.74          & 0.77           & 0.79$\pm $ 0.035                                                      \\
    Rule Based        & 0.89           & 0.84          & 0.82          & 0.85          & 0.85$\pm $ 0.025                                                      \\
    GPT-4o             & 0.88          & 0.86          & 0.80          & 0.85          & 0.85$\pm $ 0.028                                                      \\
    LLaMA              & 0.86           & 0.83          & 0.77          & 0.79          & 0.81$\pm $ 0.035                                                      \\
    MR+AS & \textbf{0.91} & \textbf{0.87} & \textbf{0.85} & \textbf{0.85} & \textbf{0.87} $\pm $ \textbf{0.024}
\end{tabular}}
\vspace{-2mm}
\caption{Median BARTScore ratios for top-2 candidates.
The last column shows averages over all datasets with SDs.
The best results are highlighted in {\bf bold}.
}
\label{figure_top2}
\end{table}

% \vspace{-1mm}
\paragraph{Top-2 Candidate Performance}
%
% \subsection{Ablation Study: BARTScore performance for Top 2 candidate}
%
% \begin{table}[]
% \centering
% \scalebox{0.7}{
% \begin{tabular}{|c|c|c|c|c|c|}
% \hline
%                 & SWS   & LS07  & LS14  & XSum  & AVG     \\ \hline
% margin\_loss    & 0.831 & 0.812 & 0.775 & 0.792 & $0.803^{\pm 0.021}$ \\ \hline
% dpo(log sigma)  & 0.889 & 0.862 & 0.838 & 0.844 & $0.858^{\pm 0.02}$\\ \hline
% dpo(four terms) & 0.88  & 0.859 & 0.836 & 0.844 & $0.855^{\pm 0.017}$\\ \hline
% margin\_top1    & 0.908 & \textbf{0.886} & \textbf{0.871} & \textbf{0.869} & $\textbf{0.884}^{\pm \textbf{0.016}}$\\ \hline
% margin\_aob     & \textbf{0.909} & 0.868 & 0.849 & 0.852 & $0.870^{\pm 0.024}$\\ \hline
%
% \end{tabular}}
% \caption{The median value of BARTScore (ratio) over top2 candidate. The $\pm$ represent standard deviation. The best result is highlighted in bold.
% }
% \label{figure_top2}
% \end{table}
%
% \begin{table}[H]
% \centering
% \scalebox{0.7}{
% \begin{tabular}{|c|c|c|c|c|c|}
% \hline
%                 & SWS   & LS07  & LS14  & XSum  & AVG     \\ \hline
% margin\_loss    & 0.831 & 0.812 & 0.775 & 0.792 & 0.803 $\pm $ 0.021 \\ \hline
% dpo(log sigma)  & 0.889 & 0.862 & 0.838 & 0.844 & 0.858 $\pm $ 0.02\\ \hline
% dpo(four terms) & 0.88  & 0.859 & 0.836 & 0.844 & 0.855 $\pm $ 0.017\\ \hline
% margin\_top1    & 0.908 & \textbf{0.886} & \textbf{0.871} & \textbf{0.869} & \textbf{0.884}  $\pm$ \textbf{0.016}\\ \hline
% margin\_aob     & \textbf{0.909} & 0.868 & 0.849 & 0.852 & 0.870 $\pm $ 0.024\\ \hline
%
% \end{tabular}}
% \caption{The median value of BARTScore (ratio) over top2 candidate. The $\pm$ represent standard deviation. The best result is highlighted in bold.
% }
% \label{figure_top2}
% \end{table}
%
Provided that the substitution rates by our model are low relative to others as discussed above, we now examine the quality of the top-2 substitute.
Note that in MLM-based models a change is produced only when the top substitute candidate is different than the original token and that for LLM-based models the top-2 candidate is only available is the model considers a substitution has to be made and more than one candidates are provided.
To quantify the quality of the top-2 candidates, we randomly selected five tokens from each sentence where the top-2 candidate is available and calculate the BARTScore ratio between the sentence modified with the top-candidate and that of the original (unchanged) sentence.
We then averaged these ratios provide a summary of the quality of top-2 candidates.
Results in Table~\ref{figure_top2} indicate that MR+AS achieved the best top-2 candidate quality across all four datasets followed (on average) by GPT-4o and the rule-based model.
%
% From Table \ref{p-value-proportion}, we observe that our model's replacement rates for cc and cinc are relatively low. As a compensatory measure, when the top-1 prediction is the original token, users can opt for the second candidate as a substitute. To further investigate this, we applied this strategy by selecting the top-2 candidate when the top-1 was the original token, except for the prompt-based model. For the prompt-based model, we selected the top-2 candidate only when the token had changed, as it doesn't provide a second candidate if no change was made. 
% To evaluate the quality of the top-2 candidates, we randomly selected five tokens from each sentence where the top-2 candidate met the required conditions above. For each selected top-2 candidate, we calculated the BARTScore ratio to assess its quality relative to the original token. We then averaged these BARTScore ratios to provide a general measure of the top-2 candidate's quality. The comparison of top-2 candidates across different models, measured in terms of the BARTScore ratio, is presented in Table \ref{figure_top2}.
% The results indicate that MR+AS achieved the best top-2 candidate quality across all four datasets, with BARTScore ratios of 0.87. The Gpt-4o models also show strong performance in this regard. However, BERT (SWS) and BERT\_spsv yields relatively lower top-2 candidate quality. 

% \vspace{-1mm}
\paragraph{Encouraging ranking in LLMs}
%
It is well known that prompting LLMs to provide ordered responses is challenging \citep{lu2023error}.
We tried prompts with and without a specific request for ordered substitute candidates (see Appendix~\ref{sc:llm_prompts} for details).
We found that both GPT and LLaMA produced slightly better median CS values without the prompt specifying ranked candidates (0.954 and 0.943, respectively) relative to the alternative (0.953 and 0.940, respectively).
Importantly, these are significantly lower than the median CS value for MR+AS (0.983).
CS distributions for all models can be found in the Appendix Figure~\ref{fig:llama/gpt_order}.
We note that it may be possible to optimize the prompt for better ranked results, however, this is beyond the scope of our work.

\paragraph{Human Study}
We recruited five annotators to carry out the study.
First, we randomly sampled 25 cases from the SWS test data and provided the top two candidate replacements proposed by our model. 
We randomly flipped the order of these two candidates before presenting them to the annotators.
Annotators were asked to indicate their preference for the order and whether they preferred to replace the target word with either of the two candidates\footnote{Example of questionnaire 1 can be accessed at \href{https://docs.google.com/spreadsheets/d/1G__gBcVt1bDd5UNVrEgdqmPMItpvOJe3j8mKAdkJrKg/edit?usp=sharing}{https://tinyurl.com/SWSQuestionnaire1}.}.

To assess inter-annotator agreement, we calculated the (unweighted) Cohen’s kappa coefficient for both order agreement and replacement agreement.
The average kappa values for the order agreement and the replacement agreement were 0.40$\pm0.21$ (standard deviation) and 0.13$\pm$0.21, respectively.
These results align with our assumption that asking annotators to suggest replacements for a specific word in a given sentence is a subjective task.
In Appendix Table~\ref{kappa}, we present the complete results. 
Furthermore, we observed that in 83\% of the cases, the annotators did not choose to replace the target word.

To mitigate the potential bias caused by providing the target word in advance, we designed a second questionnaire consisting of 50 cases.
Twenty-five cases were sampled where our model and BARTScore agreed, and twenty-five cases where they disagreed.
In this setting, the target word was hidden and only the top two candidate replacements (with randomly flipped order) were provided. 
Annotators were then asked to choose their preference from three options\footnote{Example of questionnaire 2 can be accessed at \href{https://docs.google.com/spreadsheets/d/1SmkAxzvflaq6Qhm4DB37LQu24pS1B4ATBGwzMSXyqkk/edit?usp=sharing}{https://tinyurl.com/SWSQuestionnaire2}.}:
(0) the top-1 and top-2 candidates are equally good.
(1) the top-1 candidate is better.
(2) the top-2 candidate is better.

Subsequently, we made a comparison between the model predictions and the responses of the annotators.
The results showed that in 38.8\% of the the instances, annotators considered both candidates equally good.
In the remaining cases, when the model and BartScore agreed, 79\% of annotators favored the same choice.
In cases where the model and BartScore disagreed, annotators sided with BartScore 67\% of the time and with the model 33\% of the time. In the SWS test data, the model and BartScore agreed on the ranking of the top two candidates in 67\% of instances.
In Appendix Table \ref{human-study}, we present the complete results.

%one requesting an ordered output and the other without specifying order. Detailed prompts are provided in the Appendix, and the cosine similarity results are shown in Figure \ref{fig:llama/gpt_order}. i) From the median values, it is evident that BERT vanilla and BERT (SWS) exhibit relatively poor performance, while larger models like GPT-4o and LLaMA achieve better CS scores, with GPT-4o outperforming LLaMA. ii) We tested both ordered and unordered configurations. For both LLaMA and GPT-4o, enforcing order led to a decline in CS scores, suggesting that their alignment with BARTScore decreases when order is required, highlighting the challenge large models face in generating ordered substitution candidates. iii) Based on median values, our model significantly outperforms all other models.

% It is worth noting that asking large language models to provide an ordered response through prompts poses a challenge. We experimented with two types of prompts: one requesting an ordered output and the other without specifying order. Detailed prompts are provided in the Appendix. The cosine similarity results, shown in the Figure \ref{fig:llama/gpt_order}. 

% 1. From the median values, it is clear that BERT vanilla and BERT (SWS) exhibit relatively poor performance. On the other hand, the larger models, such as GPT-4o and LLaMA, achieve better CS scores, with GPT-4o performing better than LLaMA.

% 2. We tested both with order and without order configurations. For Both of LLaMA and GPT-4o, enforcing order led to a decline in CS scores, suggesting that its alignment with BARTScore decreased when order was required. This highlights the challenge large models face in generating ordered substitution candidates.

% 3. Our model, based on median values, significantly outperforms all other models. 

%indicate that prompts requesting an order did not offer any significant advantage, whether for GPT-4 or Llama. Furthermore, our model significantly outperforms the others. Detailed prompts are provided in the appendix.

\section{Discussion}
%
This paper addressed the problem of training and evaluating SWS models without requiring human annotations.
Specifically, we leveraged a model-based score (BARTScore) to define evaluation metrics and to serve as a label-proxy for our loss function.
Extensive experimental results demonstrate the effectiveness of our approach relative to both MLM- and LLM-based models.
%
% In this paper, we addressed the Smart Word Suggestion (SWS) task, emphasizing the importance of identifying words within a sentence that require optimization to improve coherence. We also explored the challenge of incomplete labeling caused by a limited number of human annotators. To address this issue, we integrated a statistical approach, leveraging BARTScore to more effectively capture substitution performance from a distributional perspective. Further, we aligned the model with the score using Preference-Aware Learning, which effectively compensates for the shortcomings of incomplete labels and opens up new avenues for exploration. The experimental results validate the feasibility of this approach, demonstrating that it can improve the accuracy and reliability of word suggestions where human annotations may fall short.

% \clearpage

\section*{Limitations}
%
The proposed method leverages BARTScore as a sentence scoring mechanism, essentially acting as a proxy for human annotators in guiding model optimization during preference-aware learning.
However, we cannot eliminate the possibility that BARTScore may not always accurately quantify the quality of token substitutions.
While BARTScore has shown effectiveness in our experiments, it could be readily replaced by alternative scoring metrics deemed more reliable or better suited to other word substitution contexts.
We leave this exploration as interesting future work.

\section*{Ethical Considerations}
%
In the process of developing and deploying smart word suggestion systems it is crucial to consider the broader ethical implications of relying on automated tools for language optimization.
These systems may inadvertently reinforce biases present in training data, leading to unintended or inappropriate word suggestions.
Moreover, the replacement of human judgment with automated scoring models like BARTScore can risk overlooking or misrepresenting nuances in language, particularly in sensitive contexts.
Therefore, we recommend ongoing human oversight and continuous evaluation of the model's output to ensure fairness, inclusivity, and alignment with ethical standards.

% \section*{Acknowledgments}

% \clearpage

% \bibliography{acl_latex}
\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Chern et~al.(2023)Chern, Wang, Das, Sharma, Liu, Neubig et~al.}]{chern2023improving}
I-Chun Chern, Zhiruo Wang, Sanjan Das, Bhavuk Sharma, Pengfei Liu, Graham Neubig, et~al. 2023.
\newblock Improving factuality of abstractive summarization via contrastive reward learning.
\newblock \emph{arXiv preprint arXiv:2307.04507}.

\bibitem[{Devlin(2018)}]{devlin2018BERT}
Jacob Devlin. 2018.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}.

\bibitem[{Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan et~al.}]{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al. 2024.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}.

\bibitem[{Fu et~al.(2023)Fu, Ng, Jiang, and Liu}]{fu2023gptscore}
Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023.
\newblock Gptscore: Evaluate as you desire.
\newblock \emph{arXiv preprint arXiv:2302.04166}.

\bibitem[{He et~al.(2024)He, Wang, Jiao, Zhang, Wang, Shi, and Tu}]{he2024improving}
Zhiwei He, Xing Wang, Wenxiang Jiao, Zhuosheng Zhang, Rui Wang, Shuming Shi, and Zhaopeng Tu. 2024.
\newblock Improving machine translation with human feedback: An exploration of quality estimation as a reward model.
\newblock \emph{arXiv preprint arXiv:2401.12873}.

\bibitem[{Kremer et~al.(2014)Kremer, Erk, Pad{\'o}, and Thater}]{kremer2014substitutes}
Gerhard Kremer, Katrin Erk, Sebastian Pad{\'o}, and Stefan Thater. 2014.
\newblock What substitutes tell us-analysis of an “all-words” lexical substitution corpus.
\newblock In \emph{Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics}, pages 540--549.

\bibitem[{Lewis(2019)}]{lewis2019bart}
M~Lewis. 2019.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.
\newblock \emph{arXiv preprint arXiv:1910.13461}.

\bibitem[{Liu et~al.(2023{\natexlab{a}})Liu, Yuan, Fu, Jiang, Hayashi, and Neubig}]{liu2023pre}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023{\natexlab{a}}.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.
\newblock \emph{ACM Computing Surveys}, 55(9):1--35.

\bibitem[{Liu et~al.(2022)Liu, Liu, Radev, and Neubig}]{liu2022brio}
Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig. 2022.
\newblock Brio: Bringing order to abstractive summarization.
\newblock \emph{arXiv preprint arXiv:2203.16804}.

\bibitem[{Liu et~al.(2023{\natexlab{b}})Liu, Shi, He, Ye, Fabbri, Liu, Radev, and Cohan}]{liu2023learning}
Yixin Liu, Kejian Shi, Katherine~S He, Longtian Ye, Alexander~R Fabbri, Pengfei Liu, Dragomir Radev, and Arman Cohan. 2023{\natexlab{b}}.
\newblock On learning to summarize with large language models as references.
\newblock \emph{arXiv preprint arXiv:2305.14239}.

\bibitem[{Lu et~al.(2022)Lu, Ding, Xie, Zhang, Wong, and Tao}]{lu2022toward}
Qingyu Lu, Liang Ding, Liping Xie, Kanjian Zhang, Derek~F Wong, and Dacheng Tao. 2022.
\newblock Toward human-like evaluation for natural language generation with error analysis.
\newblock \emph{arXiv preprint arXiv:2212.10179}.

\bibitem[{Lu et~al.(2023)Lu, Qiu, Ding, Zhang, Kocmi, and Tao}]{lu2023error}
Qingyu Lu, Baopu Qiu, Liang Ding, Kanjian Zhang, Tom Kocmi, and Dacheng Tao. 2023.
\newblock Error analysis prompting enables human-like translation evaluation in large language models.
\newblock \emph{arXiv preprint arXiv:2303.13809}.

\bibitem[{McCarthy and Navigli(2007)}]{mccarthy2007semeval}
Diana McCarthy and Roberto Navigli. 2007.
\newblock Semeval-2007 task 10: English lexical substitution task.
\newblock In \emph{Proceedings of the fourth international workshop on semantic evaluations (SemEval-2007)}, pages 48--53.

\bibitem[{Miao et~al.(2024)Miao, Zhang, Ding, Bao, Zhang, and Tao}]{miao2024inform}
Yuchun Miao, Sen Zhang, Liang Ding, Rong Bao, Lefei Zhang, and Dacheng Tao. 2024.
\newblock Inform: Mitigating reward hacking in rlhf via information-theoretic reward modeling.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information Processing Systems}.

\bibitem[{Michalopoulos et~al.(2021)Michalopoulos, McKillop, Wong, and Chen}]{michalopoulos2021lexsubcon}
George Michalopoulos, Ian McKillop, Alexander Wong, and Helen Chen. 2021.
\newblock Lexsubcon: Integrating knowledge from lexical resources into contextual embeddings for lexical substitution.
\newblock \emph{arXiv preprint arXiv:2107.05132}.

\bibitem[{Moore(1999)}]{moore1999bootstrapping}
Jason~H Moore. 1999.
\newblock Bootstrapping, permutation testing and the method of surrogate data.
\newblock \emph{Physics in Medicine \& Biology}, 44(6):L11.

\bibitem[{Narayan et~al.(2018)Narayan, Cohen, and Lapata}]{narayan2018don}
Shashi Narayan, Shay~B Cohen, and Mirella Lapata. 2018.
\newblock Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization.
\newblock \emph{arXiv preprint arXiv:1808.08745}.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray et~al.}]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al. 2022.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in neural information processing systems}, 35:27730--27744.

\bibitem[{Pavlick et~al.(2015)Pavlick, Rastogi, Ganitkevitch, Van~Durme, and Callison-Burch}]{pavlick2015ppdb}
Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch, Benjamin Van~Durme, and Chris Callison-Burch. 2015.
\newblock Ppdb 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification.
\newblock In \emph{Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)}, pages 425--430.

\bibitem[{Qiang et~al.(2023)Qiang, Liu, Li, Yuan, and Zhu}]{qiang2023parals}
Jipeng Qiang, Kang Liu, Yun Li, Yunhao Yuan, and Yi~Zhu. 2023.
\newblock Parals: lexical substitution via pretrained paraphraser.
\newblock \emph{arXiv preprint arXiv:2305.08146}.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever et~al.}]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al. 2019.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1(8):9.

\bibitem[{Rafailov et~al.(2024)Rafailov, Sharma, Mitchell, Manning, Ermon, and Finn}]{rafailov2024direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano Ermon, and Chelsea Finn. 2024.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Sellam et~al.(2020)Sellam, Das, and Parikh}]{sellam2020bleurt}
Thibault Sellam, Dipanjan Das, and Ankur~P Parikh. 2020.
\newblock Bleurt: Learning robust metrics for text generation.
\newblock \emph{arXiv preprint arXiv:2004.04696}.

\bibitem[{Smola(2000)}]{smola2000advances}
Alexander~J Smola. 2000.
\newblock \emph{Advances in large margin classifiers}.
\newblock MIT press.

\bibitem[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}.

\bibitem[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}.

\bibitem[{Tunstall et~al.(2023)Tunstall, Beeching, Lambert, Rajani, Rasul, Belkada, Huang, von Werra, Fourrier, Habib et~al.}]{tunstall2023zephyr}
Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl{\'e}mentine Fourrier, Nathan Habib, et~al. 2023.
\newblock Zephyr: Direct distillation of lm alignment.
\newblock \emph{arXiv preprint arXiv:2310.16944}.

\bibitem[{Wang et~al.(2023)Wang, Mao, Ge, Wu, Wang, Xia, Tien, and Zhao}]{wang2023smart}
Chenshuo Wang, Shaoguang Mao, Tao Ge, Wenshan Wu, Xun Wang, Yan Xia, Jonathan Tien, and Dongyan Zhao. 2023.
\newblock Smart word suggestions for writing assistance.
\newblock \emph{arXiv preprint arXiv:2305.09975}.

\bibitem[{Yuan et~al.(2021)Yuan, Neubig, and Liu}]{yuan2021bartscore}
Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.
\newblock Bartscore: Evaluating generated text as text generation.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:27263--27277.

\bibitem[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}.

\bibitem[{Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and Artzi}]{zhang2019BERTscore}
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi. 2019.
\newblock Bertscore: Evaluating text generation with bert.
\newblock \emph{arXiv preprint arXiv:1904.09675}.

\bibitem[{Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing et~al.}]{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al. 2023.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:46595--46623.

\bibitem[{Zhou et~al.(2019)Zhou, Ge, Xu, Wei, and Zhou}]{zhou2019BERT}
Wangchunshu Zhou, Tao Ge, Ke~Xu, Furu Wei, and Ming Zhou. 2019.
\newblock Bert-based lexical substitution.
\newblock In \emph{Proceedings of the 57th annual meeting of the association for computational linguistics}, pages 3368--3373.

\end{thebibliography}



\clearpage

\appendix

\section{DPO Derivation Details}\label{sc:dpo_det}
%
\paragraph{DPO*}
%
Direct Preference Optimization (DPO) \citep{rafailov2024direct} is an efficient technique for aligning large language models (LLMs) with human feedback, which gained popularity due to its simplicity \citep{miao2024inform}.
For instance, it has demonstrated to be effective in chat benchmarks \citep{tunstall2023zephyr, zheng2023judging}.
In our case, the model-based score serving as proxy for human feedback
DPO (under the Plackett-Luce Model) is written as
%
\begin{align}\label{eq:dpo_}
\mathcal{L}_{\rm DPO} = -\mathbb{E} 
\left[ \log \textstyle{\prod}_{k=1}^{K} 
\frac{\exp ( \delta  r_k )}{ \sum_{j=k}^{K} \exp ( \delta r_j )} \right] \,, %\notag
\end{align}
%
where $r_k=\log p_\theta(\tilde{X}_k) - \log p_{\hat{\theta}}(\tilde{X}_k)$, the expectation is over $\{\tilde{X}_1, \ldots, \tilde{X}_K, X\}$, and 
$\theta$ and $\hat{\theta}$ denote the parameters of the model being trained and that used for reference, respectively.
Accordingly, only $\theta$ are updated while learning while $\hat{\theta}$ are kept fixed.

Since the magnitude of the logits drops significantly as $k\to K$, we found that the sum in the denominator of \eqref{eq:dpo_} weakens the loss.
Therefore, we removed the sum and instead compared the $k$-th and the $(k+1)$-th substitution, rather than comparing it with all $K$ values.
The Derivation of DPO* is shown below
%
\begin{align}
\mathcal{L}_{\text{DPO*}} &= - \log \textstyle{\prod}_{k=1}^{K-1}  \frac{\exp \left( \delta  r_k  \right)}{\exp \left( \delta  r_{k+1}  \right)} \label{eq:dpo*_derivation} \\
&= -\textstyle{\sum}_{k=1}^{K-1}\log \frac{\exp \left( \delta  r_k  \right)}{\exp \left( \delta  r_{k+1} \right)} \notag \\
&= -\textstyle{\sum}_{k=1}^{K-1} \left( \delta  r_k  - \delta  r_{k+1}  \right) \notag \\
&= -\textstyle{\sum}_{k=1}^{K-1} \delta\left( \log \frac{p_\theta(\tilde{X}_k)}{p_{\hat{\theta}}(\tilde{X}_k)} - \log \frac{p_\theta(\tilde{X}_{k+1})}{p_{\hat{\theta}}(\tilde{X}_{k+1})} \right) \,. \notag
\end{align}
%
Further, we approximate $\log p_\theta(\tilde{X}_k)$ with its logit $s_k$, and let $\delta=1$ for simplicity.
Then \eqref{eq:dpo*_derivation} simplifies (for a single token in sentence $X$) to
%
\begin{align*}
    \mathcal{L}_{\text{DPO*}} &= -\textstyle{\sum}_{k=1}^{K-1} \left( s_k - \hat{s}_k - s_{k+1} + \hat{s}_{k+1} \right) \,, 
    % \label{eq:dpo*_initial}
\end{align*}
%
where $s_k$ and $\hat{s}_k$ denote the logit of the $k$-th substitution from the model being trained and the reference model, respectively.

\paragraph{$\sigma$DPO*}
%
We also extend DPO (under the Bradley-Terry model) \citep{rafailov2024direct} to multiple substitute candidates, where each candidate is compared with the next in the ordered list of candidates.
We write for a single token in sentence $X$
%
\begin{dmath}
\mathcal{L}_{\sigma\mathrm{DPO*}}=- \textstyle{\sum}_{k=1}^{K-1}\log \sigma\left(\delta  r_k-\delta  r_{k+1} \right) \,.
\label{eq:sigma_dpo*_derivation}
\end{dmath}
%
We approximate $\log p_\theta(\tilde{X}_k)$ with its logit $s_k$, and let $\delta=1$ for simplicity.
Then in \eqref{eq:sigma_dpo*_derivation} simplifies (for a single token in sentence $X$) to
%
\begin{align*}
& \mathcal{L}_{\text{$\sigma$DPO*}} = \\
& \hspace{8mm} -\textstyle{\sum}_{k=1}^{K-1} \log\sigma \left( s_k - \hat{s}_k - s_{k+1} + \hat{s}_{k+1} \right) \,,
% \label{eq:sigma_dpo*_}
\end{align*}
%
from which we see that the only difference between \eqref{eq:dpo*-sigma} and \eqref{eq:dpo*} (in the main paper) is that the comparison of logit values in the former is scaled with the log-logistic function.
See Appendix for a derivation of both losses.

% \section{DPO Derivation}
% \label{sec:appendix}

% \begin{align}
% \mathcal{L}_{\rm DPO} = -\mathbb{E} 
% \left[ \log \textstyle{\prod}_{k=1}^{K} 
% \frac{\exp ( \delta  r_k )}{ \sum_{j=k}^{K} \exp ( \delta r_j )} \right] \,, %\notag
% \end{align}
% \begin{multline}
% \mathcal{L}_{\text{DPO}}= -\mathbb{E}_{\tilde{X}_1, \ldots, \tilde{X}_K, X \sim \mathcal{D}} \\ \Bigg [ \log \prod_{k=1}^{K} 
% \frac{\exp \left( \delta  \log \frac{p_\theta(\tilde{X}_k)}{p_{\hat{\theta}}(\tilde{X}_k)} \right)}{ \sum_{j=k}^{K} \exp \left( \delta  \log \frac{p_\theta(\tilde{X}_j)}{p_{\hat{\theta}}(\tilde{X}_j)} \right)} \Bigg] 
% \tag{8}
% \end{multline}

% When we remove the $\sum$ signal from the denominator, let j=k+1, it will turns to

% \begin{align*}
% \begin{dmath}
% \mathcal{L}_{\text{DPO}} &= -\mathbb{E}_{\tilde{X}_1, \ldots, \tilde{X}_K, X \sim \mathcal{D}} \left[ \log \prod_{k=1}^{K-1} \frac{\exp \left( \delta \log \frac{p_\theta(\tilde{X}_k)}{p_{\hat{\theta}}(\tilde{X}_k)} \right)}{\exp \left( \delta \log \frac{p_\theta(\tilde{X}_{k+1})}{p_{\hat{\theta}}(\tilde{X}_{k+1})} \right)} \right] \\ \\
% &= -\sum_{x \sim \mathcal{D}} \sum_{k=1}^{K-1} \log \frac{\exp \left( \delta \log \frac{p_\theta(\tilde{X}_k)}{p_{\hat{\theta}}(\tilde{X}_k)} \right)}{\exp \left( \delta \log \frac{p_\theta(\tilde{X}_{k+1})}{p_{\hat{\theta}}(\tilde{X}_{k+1})} \right)} \\ \\
% &= -\sum_{x \sim \mathcal{D}} \sum_{k=1}^{K-1} \left( \delta \log \frac{p_\theta(\tilde{X}_k)}{p_{\hat{\theta}}(\tilde{X}_k)} - \delta \log \frac{p_\theta(\tilde{X}_{k+1})}{p_{\hat{\theta}}(\tilde{X}_{k+1})} \right) \\ \\
% &= -\sum_{x \sim \mathcal{D}} \sum_{k=1}^{K-1} \left( \log \frac{p_\theta(\tilde{X}_k)}{p_{\hat{\theta}}(\tilde{X}_k)} -  \log \frac{p_\theta(\tilde{X}_{k+1})}{p_{\hat{\theta}}(\tilde{X}_{k+1})} \right) \tag{5}
% % \end{align*}
% \end{dmath}
% Further, we approximate $\log p_\theta(\tilde{X}_k)$ with its logit $s_k$ and let $\delta=1$ for simplicity. Therefore formula \ref{eq:dpo} could be rewrited as 

% \begin{align*}
% \begin{dmath}
% \mathcal{L}_{\text{DPO*}} &= -\sum_{x \sim \mathcal{D}} \sum_{k=1}^{K-1} \left( s_k - \hat{s}_k - s_{k+1} + \hat{s}_{k+1} \right) \tag{9}
% \end{dmath}

% We also extend DPO (under the Bradley-Terry Model) \citep{rafailov2024direct} to multiple substitute candidates, where each candidate is compared with the next in the list of ordered substitute candidates list.

% In order to keep the equation always be a positive value, we add a $\log\sigma$ function, formula \ref{eq:dpo*} becomes as:

% \begin{align*}
% \begin{dmath}
% \mathcal{L}_{\sigma \text{DPO*}} &= -\sum_{x \sim \mathcal{D}} \sum_{k=1}^{K-1} \log\sigma \left( s_k - \hat{s}_k - s_{k+1} + \hat{s}_{k+1} \right) \tag{10} 
% % \end{align*}
% \end{dmath}

% \begin{dmath}
% \mathcal{L}_{\sigma\mathrm{DPO*}}=-\mathbb{E}_{\left(\tilde{X}_{1}, \dots ,\tilde{X}_{K},X\right) \sim \mathcal{D}} \left[\sum_{k=1}^{K-1}\log \sigma\left(\delta  \log \frac{p_{\theta}\left(\tilde{X}_{k} \right)}{p_{\hat{\theta}}\left(\tilde{X}_{k} \right)}-\delta  \log \frac{p_{\theta}\left(\tilde{X}_{k+1}\right)}{p_{\hat{\theta}}\left(\tilde{X}_{k+1} \right)}\right)\right] \\
% &=-\sum_{x \sim \mathcal{D}}\sum_{k=1}^{K-1}\log \sigma\left(\delta  \log \frac{p_{\theta}\left(\tilde{X}_{k} \right)}{p_{\hat{\theta}}\left(\tilde{X}_{k} \right)}-\delta  \log \frac{p_{\theta}\left(\tilde{X}_{k+1}\right)}{p_{\hat{\theta}}\left(\tilde{X}_{k+1} \right)}\right) \\
% &=-\sum_{x \sim \mathcal{D}}\sum_{k=1}^{K-1}\log \sigma\left(\log \frac{p_{\theta}\left(\tilde{X}_{k} \right)}{p_{\hat{\theta}}\left(\tilde{X}_{k} \right)}- \log \frac{p_{\theta}\left(\tilde{X}_{k+1}\right)}{p_{\hat{\theta}}\left(\tilde{X}_{k+1} \right)}\right) \\
% &= -\sum_{x \sim \mathcal{D}} \sum_{k=1}^{K-1} \log\sigma \left( s_k - \hat{s}_k - s_{k+1} + \hat{s}_{k+1} \right) 
% \end{dmath}

\section{LLM Prompts}\label{sc:llm_prompts}
%
Prompt \underline{without} order:

\textit{In the following sentence, please give some suggestions to improve word usage. Please give the results with the JSON format of {“original word”: [“suggestion 1”, “suggestion 2”]}. The 'original word' should include all words that can be improved in the sentence, directly extracted from the sentence itself. [s]}

\noindent Prompt \underline{with} order:

\textit{In the following sentence, please give some suggestions to improve word usage. Please give the results with the JSON format of {“original word”: [“suggestion 1”, “suggestion 2”]}. The 'original word' should include all words that can be improved in the sentence, directly extracted from the sentence itself, and \textbf{the suggestions should be ranked in order of the degree of improvement, from the most effective to the least}. [s]}

where [s] is the sentence. 

At times, the model may fail to provide the correct JSON format, making it difficult to extract the intended answer.
In such cases, it is often necessary to query the model multiple times to obtain a valid JSON output.

\section{Model Hyperparameters}\label{sc:hyperparams}
%
Both of BERT\_SWS reproduction and model optimization are: $\text{epochs}=5$, $\text{batch size} = 64$, $\text{learning rate} = 0.0007$, $\text{max norm} = 1e-5$ for clipped gradient norm, $\text{dropout rate} = 0.1$, $\lambda = 0.5$, and $\gamma = 1$.

\begin{table}[h]
\centering
\scalebox{0.73}{
\begin{tabular}{lllll}
\hline
Data & Task                 & Training  & Validation & Test   \\ \hline
SWS  & SWS                  & 3,909,650 & 200        & 800    \\
LS07 & Lexical Substitution & -         & 299        & 1710   \\
LS14 & Lexical Substitution & -         & 892        & 1569   \\
XSum & Summarization        & 204045    & 11332      & 11,334 \\ \hline
\end{tabular}}
\caption{Datasets summary.
}
\label{tb:data_summ}
\end{table}

% \subsection{Illustrative Example}
% 
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.8\columnwidth]{figures/cases_smaller.png}
%     % \caption{Original (\textcolor{mblue}{blue}) and modified (\textcolor{mgreen}{green}) sentences with substitution in \textcolor{mred}{red}.
%     % Modifications were done by the BERT-SWS model.
%     % % Blue is the original sentence from the SWS test dataset, green is the sentence with the substitution generated from the BERT\_SWS model. Red is the substitution word.
%     % } % Add a caption to the image
%     % \label{fig:cases} % Add a label for referencing in the text
% \end{figure}

% CD:
\begin{table*}[t]
\centering
\scalebox{0.55}{
\begin{tabular}{lp{23cm}}
% \hline
\multirow{35}{*}{CD} & \begin{tabular}[c]{@{}l@{}}While being saddened by my friend's suffering, I also gradually became aware of the  \textcolor{red}{critical} role of law in our daily life.\\ Annotations: Human=\{"integral", "important"\}, Model=\{"crucial"\}\end{tabular} \\
\cline{2-2} 
& \begin{tabular}[c]{@{}l@{}}The residential theory \textcolor{red}{argues} that there are houses in the Chaco structures, and the structure is big enough for hundreds of people. \\ Annotations: Human=\{"discussed", "states",   "reasons", "discusses"\}, Model=\{"claims"\}\end{tabular} \\
\cline{2-2}
& \begin{tabular}[c]{@{}l@{}}As I planned   well for the whole day, I would not make mistakes such as doing things relating to a single subject in a \textcolor{red}{complete} morning or afternoon, \\which would make my head dizzy and I  could not focus all my attention on my work.\\      Annotations: Human=\{"single", "whole"\}, Model= \{"full"\}\end{tabular} \\  
\cline{2-2} 
& \begin{tabular}[c]{@{}l@{}}By contrast, the professor refutes this idea   and \textcolor{red}{insists} that it will not   be that profitable when the cost is taken into consideration.\\      Annotations: Human=\{"urges", "claims"\} Model=  \{"argues"\}\end{tabular}  \\
\cline{2-2} 
& \begin{tabular}[c]{@{}l@{}}For example, in traditional China, young people   needed to turn down their knees to \textcolor{red}{show} their respect to old people like their parents or their   teachers.\\      Annotations: Human=\{"convey", "display",   "manifest"\} Model= \{"demonstrate"\}\end{tabular}   \\
\cline{2-2} 
& \begin{tabular}[c]{@{}l@{}}As a matter of fact, young people didn't need   to obey too many strict rules and could have a \textcolor{red}{relatively} relaxing life.\\      Annotations: Human=\{"generally", "decidedly",   "approximately"\} Model= \{"fairly"\}\end{tabular}     \\ 
\cline{2-2}
& \begin{tabular}[c]{@{}l@{}}To be more specific, most young men tend to be   couch potatoes during weekends, leading to not \textcolor{red}{concerning}  about social issues.\\      Annotations: Human=\{"updating",    "concerns", "regarding",  "caring"\} Model=  \{"worrying"\}\end{tabular}  \\
\cline{2-2} 
& \begin{tabular}[c]{@{}l@{}}As a result, to make sure that I fulfil the \textcolor{red}{goal} I made every day, I would   prevent myself from doing  unnecessary things like playing for a long time in \\  the playground as some of my classmates would do.\\      Annotations: Human=\{"aim"\} Model= \{"objective"\}\end{tabular}  \\   \cline{2-2} 
& \begin{tabular}[c]{@{}l@{}}When enjoying this piece, people can directly   follow the progress of the piece to feel the picture constructed between the \textcolor{red}{various} instruments \\without   exploring its creative background.\\      Annotations: Human=\{"diverse"\} Model= \{"different"\}\end{tabular}    \\ \cline{2-2}
& \begin{tabular}[c]{@{}l@{}}No one \textcolor{red}{likes} to deal with people who can't adopt other people's advice and   insist that only his or her idea  is correct.\\      Annotations: Human=\{"prefers", "enjoys"\} Model=  \{"wants"\}\end{tabular}  \\ \cline{2-2}
& \begin{tabular}[c]{@{}l@{}}Globalists view the coronavirus as a global   threat that shows the \textcolor{red}{common}   plight of humanity and the need to work together.\\      Annotations: Human=\{"reoccurring", "everyday"\} Model=  \{"shared"\}\end{tabular}      \\ \cline{2-2} 
& \begin{tabular}[c]{@{}l@{}}Second, the reading \textcolor{red}{suggests} that the mining industry on asteroids would be highly   profitable due to the numerous valuable elements and precious\\ metals buried   under the asteroids.\\      Annotations: Human=\{"implies", "proposes"\} Model=  \{"indicates"\}\end{tabular}    \\ \hline
\multirow{35}{*}{OMC} & \begin{tabular}[c]{@{}l@{}}Therefore, the power to \textcolor{red}{resolve} disputes and maintain social order is the psychological  restraint of the actor and the education of the elder to the \\young rather than the litigation.\\ Annotations: Human=\{"resolve"\}, Model=\{"settle"\}\end{tabular} \\
\cline{2-2} 
& \begin{tabular}[c]{@{}l@{}}According to recent statistics, there is ample coverage demonstrating that humans who inhabit the hub \textcolor{red}{continually} will undergo severe conditions due\\ to the dirty air and stressful surroundings.\\ Annotations: Human =\{"continually"\}, Model=\{"constantly"\}\end{tabular} \\ \cline{2-2}
& \begin{tabular}[c]{@{}l@{}}We are capable of making crops which can \textcolor{red}{resist} many kinds of unideal   factors, such as colds, bugs, or even drought.\\      Annotations: Human=\{resist\} Model= \{"withstand"\}\end{tabular}   \\ \cline{2-2}
& \begin{tabular}[c]{@{}l@{}}With the proliferation of schools and private   firms nowadays, it is \textcolor{red}{sometimes}   argued that a company or campus should have strict rules that control\\ the   type of clothing that people wear at work and at school.\\      Annotations: Human=\{sometimes\} Model= \{"occasionally"\}\end{tabular}   \\ \cline{2-2} 
& \begin{tabular}[c]{@{}l@{}}The number of students who have grades over 90   in international exams this year has surprisingly soared roughly about 10   percent, a figure that has \\ \textcolor{red}{nearly} doubled as against that of last year.\\      Annotations: Human=\{nearly\} Model= \{"almost"\}\end{tabular}    \\ \cline{2-2}
& \begin{tabular}[c]{@{}l@{}}As a \textcolor{red}{result}, to make sure that I fulfil the goal I made every day, I would   prevent myself from doing unnecessary things like playing for a long time \\in   the playground as some of my classmates would do.\\      Annotations: Human=\{result\} Model= \{"consequence"\}\end{tabular}     \\ \cline{2-2}
& \begin{tabular}[c]{@{}l@{}}Even though the issue is becoming increasingly   critical, many believe it is not \textcolor{red}{essential} for the young to learn how to plan and organize.\\      Annotations: Human=\{essential\} Model= \{"necessary"\}\end{tabular}   \\ \cline{2-2}
& \begin{tabular}[c]{@{}l@{}}When enjoying this piece, people can directly   follow the progress of the piece to feel the picture constructed between the   various instruments \\without \textcolor{red}{exploring} its creative background.\\      Annotations: Human=\{exploring\} Model= \{"examining"\}\end{tabular}     \\ \cline{2-2} 
& \begin{tabular}[c]{@{}l@{}}There is no doubt that letting students decide   how many and how \textcolor{red}{often} they could do study assignments is more flexible.\\      Annotations: Human=\{often\} Model= \{"frequently"\}\end{tabular}  \\ \cline{2-2} 
& \begin{tabular}[c]{@{}l@{}}This is because the Parliament has the   intention to limit personal liberty identified as fundamental rights by the   unequivocal language without \\ \textcolor{red}{allowing} the diagnosed person to leave the residence or places except   for certain circumstances (Momcilovic; Aboriginal Justice; s4).\\      Annotations: Human=\{allowing\} Model= \{"permitting"\}\end{tabular} \\ \cline{2-2}
& \begin{tabular}[c]{@{}l@{}}Those who bolster the idea might indicate this   operation either is interesting to allow them to engage in a relaxed   atmosphere or spend less time\\ \textcolor{red}{focusing} on solid books.\\      Annotations: Human=\{focusing\} Model= \{"concentrating"\}\end{tabular} \\ \cline{2-2}
& \begin{tabular}[c]{@{}l@{}}What   is more, if young students keep making use of video games in some lessons,   their caliber of watching objects will \textcolor{red}{decrease}, granted that \\they avail themselves of exhibiting sight too   much.\\      Annotations: Human=\{decrease\} Model= \{"decline"\}\end{tabular} \\
% \hline
\end{tabular}
}
\caption{Original sentences with target token substitution in \textcolor{red}{red}. CD: change disagreement and OMC: only model changed.
}
\label{tb:cases}
\end{table*}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \begin{table*}[t]
% \centering
% \scalebox{0.8}{
% \begin{tabular}{|c|p{17cm}|}
% \hline
% \multirow{10}{*}{CD}  & \begin{tabular}[c]{@{}l@{}}As I planned   well for the whole day, I would not make mistakes such as doing things relating to a single \\subject in a \textcolor{red}{complete} morning or afternoon, which would make my head dizzy and I  could not focus all\\ my attention on my work.\\      Annotations: Human=\{"single", "whole"\} Model:   \{"full"\}\end{tabular}                                           \\ \cline{2-2} 
%                       & \begin{tabular}[c]{@{}l@{}}By contrast, the professor refutes this idea   and \textcolor{red}{insists} that it will not   be that profitable when the cost is taken\\ into consideration.\\      Annotations: Human=\{"urges", "claims"\} Model:   \{"argues"\}\end{tabular}                                                                                                                                          \\ \cline{2-2} 
%                       & \begin{tabular}[c]{@{}l@{}}For example, in traditional China, young people   needed to turn down their knees to \textcolor{red}{show} their respect to old \\people like their parents or their   teachers.\\      Annotations: Human=\{"convey", "display",   "manifest"\} Model: \{"demonstrate"\}\end{tabular}                                                                                                     \\ \cline{2-2} 
%                       & \begin{tabular}[c]{@{}l@{}}As a matter of fact, young people didn't need   to obey too many strict rules and could have a \textcolor{red}{relatively} \\relaxing life.\\      Annotations: Human=\{"generally", "decidedly",   "approximately"\} Model: \{"fairly"\}\end{tabular}                                                                                                                                     \\ \cline{2-2} 
%                       & \begin{tabular}[c]{@{}l@{}}To be more specific, most young men tend to be   couch potatoes during weekends, leading to not \textcolor{red}{concerning} \\ about social issues.\\      Annotations: Human=\{"updating",    "concerns", "regarding",  "caring"\} Model:   \{"worrying"\}\end{tabular}                                                                                                                    \\ \cline{2-2} 
%                       & \begin{tabular}[c]{@{}l@{}}As a result, to make sure that I fulfil the \textcolor{red}{goal} I made every day, I would   prevent myself from doing \\ unnecessary things like playing for a long time in   the playground as some of my classmates would do.\\      Annotations: Human=\{"aim"\} Model: \{"objective"\}\end{tabular}                                                                                   \\ \cline{2-2} 
%                       & \begin{tabular}[c]{@{}l@{}}When enjoying this piece, people can directly   follow the progress of the piece to feel the picture constructed\\ between the \textcolor{red}{various} instruments without   exploring its creative background.\\      Annotations: Human=\{"diverse"\} Model: \{"different"\}\end{tabular}                                                                                               \\ \cline{2-2} 
%                       & \begin{tabular}[c]{@{}l@{}}No one \textcolor{red}{likes} to deal with people who can't adopt other people's advice and   insist that only his or her idea \\ is correct.\\      Annotations: Human=\{"prefers", "enjoys"\} Model:   \{"wants"\}\end{tabular}                                                                                                                                                           \\ \cline{2-2} 
%                       & \begin{tabular}[c]{@{}l@{}}Globalists view the coronavirus as a global   threat that shows the \textcolor{red}{common}   plight of humanity and the need to\\ work together.\\      Annotations: Human=\{"reoccurring", "everyday"\} Model:   \{"shared"\}\end{tabular}                                                                                                                                               \\ \cline{2-2} 
%                       & \begin{tabular}[c]{@{}l@{}}Second, the reading \textcolor{red}{suggests} that the mining industry on asteroids would be highly   profitable due to the\\ numerous valuable elements and precious metals buried   under the asteroids.\\      Annotations: Human=\{"implies", "proposes"\} Model:   \{"indicates"\}\end{tabular}                                                                                       \\ \hline
% \multirow{10}{*}{OMC} & \begin{tabular}[c]{@{}l@{}}What   is more, if young students keep making use of video games in some lessons,   their caliber of watching\\ objects will \textcolor{red}{decrease}, granted that they avail themselves of exhibiting sight too   much.\\      Annotations: Human=\{decrease\} Model: \{"decline"\}\end{tabular}                                                                                        \\ \cline{2-2} 
%                       & \begin{tabular}[c]{@{}l@{}}We are capable of making crops which can \textcolor{red}{resist} many kinds of unideal   factors, such as colds, bugs, or\\ even drought.\\      Annotations: Human=\{resist\} Model: \{"withstand"\}\end{tabular}                                                                                                                                                                         \\ \cline{2-2} 
%                       & \begin{tabular}[c]{@{}l@{}}With the proliferation of schools and private   firms nowadays, it is \textcolor{red}{sometimes}   argued that a company or campus \\should have strict rules that control the   type of clothing that people wear at work and at school.\\      Annotations: Human=\{sometimes\} Model: \{"occasionally"\}\end{tabular}                                                                   \\ \cline{2-2} 
%                       & \begin{tabular}[c]{@{}l@{}}The number of students who have grades over 90   in international exams this year has surprisingly soared \\roughly about 10   percent, a figure that has \textcolor{red}{nearly} doubled as against that of last year.\\      Annotations: Human=\{nearly\} Model: \{"almost"\}\end{tabular}                                                                                              \\ \cline{2-2} 
%                       & \begin{tabular}[c]{@{}l@{}}As a \textcolor{red}{result}, to make sure that I fulfil the goal I made every day, I would   prevent myself from doing \\unnecessary things like playing for a long time in   the playground as some of my classmates would do.\\      Annotations: Human=\{result\} Model: \{"consequence"\}\end{tabular}                                                                                \\ \cline{2-2} 
%                       & \begin{tabular}[c]{@{}l@{}}Even though the issue is becoming increasingly   critical, many believe it is not \textcolor{red}{essential} for the young to \\learn how to plan and organize.\\      Annotations: Human=\{essential\} Model: \{"necessary"\}\end{tabular}                                                                                                                                                \\ \cline{2-2} 
%                       & \begin{tabular}[c]{@{}l@{}}When enjoying this piece, people can directly   follow the progress of the piece to feel the picture constructed \\between the   various instruments without \textcolor{red}{exploring} its creative background.\\      Annotations: Human=\{exploring\} Model: \{"examining"\}\end{tabular}                                                                                               \\ \cline{2-2} 
%                       & \begin{tabular}[c]{@{}l@{}}There is no doubt that letting students decide   how many and how \textcolor{red}{often} they could do study assignments is \\more flexible.\\      Annotations: Human=\{often\} Model: \{"frequently"\}\end{tabular}                                                                                                                                                                    \\ \cline{2-2} 
%                       & \begin{tabular}[c]{@{}l@{}}This is because the Parliament has the   intention to limit personal liberty identified as fundamental rights\\ by the   unequivocal language without \textcolor{red}{allowing} the diagnosed person to leave the residence or places \\except   for certain circumstances (Momcilovic; Aboriginal Justice; s4).\\      Annotations: Human=\{allowing\} Model: \{"permitting"\}\end{tabular} \\ \cline{2-2} 
                      
%                       & \begin{tabular}[c]{@{}l@{}}Those who bolster the idea might indicate this   operation either is interesting to allow them to engage in a \\relaxed   atmosphere or spend less time \textcolor{red}{focusing} on solid books.\\      Annotations: Human=\{focusing\} Model: \{"concentrating"\}\end{tabular}                                                                                                           \\ \hline
% \end{tabular}}
% \end{table*}

% Original: To be more specific, most young men tend to be couch potatoes during weekends, leading to not concerning about social issues.

% Replaced: To be more specific, most young men tend to be couch potatoes during weekends, leading to not worrying about social issues.

% ### stop here ###

% Original: Second, the reading suggests that the mining industry on asteroids would be highly profitable due to the numerous valuable elements and precious metals buried under the asteroids.

% Replaced: Second, the reading indicates that the mining industry on asteroids would be highly profitable due to the numerous valuable elements and precious metals buried under the asteroids.

% Original: Globalists view the coronavirus as a global threat that shows the common plight of humanity and the need to work together.

% Replaced: Globalists view the coronavirus as a global threat that shows the shared plight of humanity and the need to work together.

% Original: For example, the government in my city sponsored a high school hockey team to attend an international tournament two years ago.

% Replaced: For example, the government in my city supported a high school hockey team to attend an international tournament two years ago.

% Original: No one likes to deal with people who can't adopt other people's advice and insist that only his or her idea is correct.

% Replaced: No one wants to deal with people who can't adopt other people's advice and insist that only his or her idea is correct.

% Original: When enjoying this piece, people can directly follow the progress of the piece to feel the picture constructed between the various instruments without exploring its creative background.

% Replaced: When enjoying this piece, people can directly follow the progress of the piece to feel the picture constructed between the different instruments without exploring its creative background.

% Original: As a result, to make sure that I fulfil the goal I made every day, I would prevent myself from doing unnecessary things like playing for a long time in the playground as some of my classmates would do.

% Replaced: As a result, to make sure that I fulfil the objective I made every day, I would prevent myself from doing unnecessary things like playing for a long time in the playground as some of my classmates would do.

% Original: As a matter of fact, young people didn't need to obey too many strict rules and could have a relatively relaxing life.

% Replaced: As a matter of fact, young people didn't need to obey too many strict rules and could have a fairly relaxing life.

% Original: For example, in traditional China, young people needed to turn down their knees to show their respect to old people like their parents or their teachers.
% Replaced: For example, in traditional China, young people needed to turn down their knees to demonstrate their respect to old people like their parents or their teachers.

% Original: By contrast, the professor refutes this idea and insists that it will not be that profitable when the cost is taken into consideration.

% Replaced: By contrast, the professor refutes this idea and argues that it will not be that profitable when the cost is taken into consideration.

% Original: As I planned well for the whole day, I would not make mistakes such as doing things relating to a single subject in a complete morning or afternoon, which would make my head dizzy and I could not focus all my attention on my work.

% Replaced: As I planned well for the whole day, I would not make mistakes such as doing things relating to a single subject in a full morning or afternoon, which would make my head dizzy and I could not focus all my attention on my work.







% OMC:

% Original: For example, some parents think a gap year is purely a waste of time and money for their children, so they will never support their kids to have a gap year before they go to college.

% Replaced: For example, some parents believe a gap year is purely a waste of time and money for their children, so they will never support their kids to have a gap year before they go to college.


% Original: Communication, an important method for people to exchange information and develop ideas, has been valued and even triggers a heated discussion over whether it is more valuable for groups to communicate about a project in person rather than through e-mail.

% Replaced: Communication, an essential method for people to exchange information and develop ideas, has been valued and even triggers a heated discussion over whether it is more valuable for groups to communicate about a project in person rather than through e-mail.

% Original: A great number of efforts have been made to improve construction safety, leading to the reportable accidents (every 1.000 workers) decreasing from 10,971 to 5,587, almost half of the accident number of 20 years ago. However, it could not be ignored that the accident rate is still high.

% Replaced: A great number of efforts have been made to enhance construction safety, leading to the reportable accidents (every 1.000 workers) decreasing from 10,971 to 5,587, almost half of the accident number of 20 years ago. However, it could not be ignored that the accident rate is still high.

% Original: Those who bolster the idea might indicate this operation either is interesting to allow them to engage in a relaxed atmosphere or spend less time focusing on solid books.
% Replaced: Those who bolster the idea might indicate this operation either is interesting to allow them to engage in a relaxed atmosphere or spend less time concentrating on solid books.

% ### stop here ###

% Original: This is because the Parliament has the intention to limit personal liberty identified as fundamental rights by the unequivocal language without allowing the diagnosed person to leave the residence or places except for certain circumstances (Momcilovic; Aboriginal Justice; s4).

% Replaced: This is because the Parliament has the intention to limit personal liberty identified as fundamental rights by the unequivocal language without permitting the diagnosed person to leave the residence or places except for certain circumstances (Momcilovic; Aboriginal Justice; s4).

% Original: There is no doubt that letting students decide how many and how often they could do study assignments is more flexible.

% Replaced: There is no doubt that letting students decide how many and how frequently they could do study assignments is more flexible.

% Original: When enjoying this piece, people can directly follow the progress of the piece to feel the picture constructed between the various instruments without exploring its creative background.

% Replaced: When enjoying this piece, people can directly follow the progress of the piece to feel the picture constructed between the various instruments without examining its creative background.

% Original: Even though the issue is becoming increasingly critical, many believe it is not essential for the young to learn how to plan and organize.

% Replaced: Even though the issue is becoming increasingly critical, many believe it is not necessary for the young to learn how to plan and organize.

% Original: As a result, to make sure that I fulfil the goal I made every day, I would prevent myself from doing unnecessary things like playing for a long time in the playground as some of my classmates would do.

% Replaced: As a consequence, to make sure that I fulfil the goal I made every day, I would prevent myself from doing unnecessary things like playing for a long time in the playground as some of my classmates would do.

% Original: The number of students who have grades over 90 in international exams this year has surprisingly soared roughly about 10 percent, a figure that has nearly doubled as against that of last year.

% Replaced:The number of students who have grades over 90 in international exams this year has surprisingly soared roughly about 10 percent, a figure that has almost doubled as against that of last year.

% Original: With the proliferation of schools and private firms nowadays, it is sometimes argued that a company or campus should have strict rules that control the type of clothing that people wear at work and at school.

% Replaced: With the proliferation of schools and private firms nowadays, it is occasionally argued that a company or campus should have strict rules that control the type of clothing that people wear at work and at school.

% Original: We are capable of making crops which can resist many kinds of unideal factors, such as colds, bugs, or even drought.

% Replaced: We are capable of making crops which can withstand many kinds of unideal factors, such as colds, bugs, or even drought.

% Original: What is more, if young students keep making use of video games in some lessons, their caliber of watching objects will decrease, granted that they avail themselves of exhibiting sight too much.

% Replaced: What is more, if young students keep making use of video games in some lessons, their caliber of watching objects will decline, granted that they avail themselves of exhibiting sight too much.

% \subsection{Benchmark (CS+AVB)}

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{figures/cs_box_Distributions.png}
    \caption{CS distribution results for SWS test data.
    }
    \label{fig:bench_cs}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{figures/avb_box_Distributions.png}
    \caption{ABR distribution results for SWS test data.
    }
    \label{fig:bench_abr}
\end{figure}

% \subsection{Benchmark ($p$-value)}

\begin{table}[H]
\centering
\scalebox{0.8}{
\begin{tabular}{c|c|c|c|c|c}
                  & OAC                           & CA                              & CD                          & OMC                           & NCA                    \\ \hline
BERT-naive     & \textbf{0.999} & 0.888                           & 0.339                         & 0.274                           & \textbf{1} \\
BERT-spsv        & 0.994                           & 0.981                           & 0.583                         & 0.486                           & 0.999                       \\
BERT-SWS         & \textbf{0.999} & 0.942                           & 0.731                         & 0.628                           & \textbf{1} \\
BART-SWS         & 0.992                           & 0.905                           & 0.604                         & 0.588                           & 0.969                       \\
Rule Based       & -                               & 0.602                           & 0.241                         & 0.344                           & -                           \\
GPT-4o            & -                               & 0.761                           & 0.563                         & 0.661                           & -                           \\
LLaMA             & -                               & 0.700                           & 0.517                         & 0.603                           & -                           \\
MR+AS & \textbf{0.999} & \textbf{0.994} & \textbf{0.9} & \textbf{0.705} & \textbf{1}
\end{tabular}}
\caption{Proportion of $p$-values below the significance threshold ($\alpha=0.01$) for $K_s=1000$ using BARTScore.
}
\label{tb:pv_all}
\end{table}

\begin{table}[H]
\centering
\scalebox{0.8}{
\begin{tabular}{c|c|c|c|c|c}
           & OAC        & CA             & CD             & OMC            & NCA        \\ \hline
BERT-naive & \textbf{1} & 0.888          & 0.425          & 0.47           & \textbf{1} \\
BERT-spsv  & \textbf{1}          & 0.891          & 0.474          & 0.438          & \textbf{1}          \\
BERT-SWS   & \textbf{1} & 0.948          & 0.717          & 0.621          & \textbf{1} \\
BART-SWS   & \textbf{1}          & 0.869          & 0.6            & 0.595          & \textbf{1}          \\
Rule Based & -          & 0.552          & 0.248          & 0.309          & -          \\
GPT-4o     & -          & 0.734          & 0.557          & 0.655          & -          \\
LLaMA      & -          & 0.654          & 0.518          & 0.624          & -          \\
MR+AS      & \textbf{1} & \textbf{0.963} & \textbf{0.809} & \textbf{0.719} & \textbf{1}
\end{tabular}}
\caption{Proportion of $p$-values below the significance threshold ($\alpha=0.01$) for $K_s=1000$ using GPTScore with GPT2-medium (355M).
}
\label{tb:pv_gptscore_GPT2-M}
\end{table}

\begin{table}[H]
\centering
\scalebox{0.8}{
\begin{tabular}{c|c|c|c|c|c}
           & OAC        & CA             & CD             & OMC            & NCA        \\ \hline
BERT-naive & \textbf{1} & 0.893          & 0.439          & 0.351          & \textbf{1} \\
BERT-spsv  & \textbf{1}          & 0.927          & 0.506          & 0.498          & \textbf{1}          \\
BERT-SWS   & \textbf{1} & 0.975          & 0.749          & 0.655          & \textbf{1} \\
BART-SWS   & \textbf{1}          & 0.897          & 0.618          & 0.625          & \textbf{1}          \\
Rule Based & -          & 0.531          & 0.247          & 0.332          & -          \\
GPT-4o     & -          & 0.745          & 0.554          & 0.673          & -          \\
LLaMA      & -          & 0.694          & 0.531          & 0.625          & -          \\
MR+AS      & \textbf{1} & \textbf{0.981} & \textbf{0.759} & \textbf{0.657} & \textbf{1}
\end{tabular}}
\caption{Proportion of $p$-values below the significance threshold ($\alpha=0.01$) for $K_s=1000$ using GPTScore with OPT-350M (350M).
}
\label{tb:pv_gptscore_OPT350M}
\end{table}

\begin{table}[H]
\centering
\scalebox{0.7}{
\begin{tabular}{c|c|c|c|c|c|c}
              & OAC            & CA    & CD  & OMC   & NCA           & Ratio         \\ \hline
BERT-naive & 0.914          & 0.032 & 0.054 & 0.052 & 0.948          & 4.09          \\
BERT-spsv    & 0.952 & 0.02  & 0.028 & 0.017 & 0.983 & 2.82          \\
BERT-SWS     & 0.738          & 0.113 & 0.149 & 0.064 & 0.936          & 1.65          \\
BART-SWS     & 0.736          & 0.113 & 0.151 & 0.068 & 0.932          & 3.88          \\
Rule Based   & 0.63           & 0.11  & 0.26  & 0.101 & 0.899          & 3.66          \\
GPT-4o        & 0.508          & 0.244 & 0.248 & 0.115 & 0.885          & 4.28 \\
LLaMA         & 0.722          & 0.121 & 0.157 & 0.065 & 0.935          & 4.28 \\
MR+AS         & 0.893          & 0.055 & 0.052 & 0.027 & 0.973          & 3.96         
\end{tabular}}
\caption{The value indicates the proportion of tokens involved. We divided the data into three groups (OAC, CA, CD) for changes made by human annotators, and two groups (OMC, NCA) for tokens the human annotator did not change.
The ratio in the last column is between tokens both the model and the annotator change (CA and CD) and tokens only the model changed (NCA).
}
\label{p-value-proportion-full}
\end{table}

\begin{figure}[H] % Specify image placement (h=here, t=top, b=bottom, p=separate page)
    \centering
    \includegraphics[width=\columnwidth]{figures/benchmark_distri_i.png} % Include the image file
    \caption{$p$-value distributions.} % Add a caption to the image
    \label{fig:benchmark1} % Add a label for referencing in the text
\end{figure}

\begin{figure}[H] % Specify image placement (h=here, t=top, b=bottom, p=separate page)
    \centering
    \includegraphics[width=\columnwidth]{figures/benchmark_distri_ii.png} % Include the image file
    \caption{$p$-value distributions.} % Add a caption to the image
    \label{fig:benchmark2} % Add a label for referencing in the text
\end{figure}

\begin{figure}[H] % Specify image placement (h=here, t=top, b=bottom, p=separate page)
    \centering
    \includegraphics[width=\columnwidth]{figures/benchmark_distri_iii.png} % Include the image file
    \caption{$p$-value distributions.} % Add a caption to the image
    \label{fig:benchmark3} % Add a label for referencing in the text
\end{figure}

\begin{figure}[H] % Specify image placement (h=here, t=top, b=bottom, p=separate page)
    \centering
    \includegraphics[width=\columnwidth]{figures/benchmark_distri_iv.png} % Include the image file
    \caption{$p$-value distributions.} % Add a caption to the image
    \label{fig:benchmark4} % Add a label for referencing in the text
\end{figure}

% \subsection{Top1/Top2 BARscore}
% \begin{figure}[H] % Specify image placement (h=here, t=top, b=bottom, p=separate page)
%     \centering
%     \includegraphics[width=0.4\textwidth]{figures/sws_top2.png} % Include the image file
%     \caption{Top1/Top2 BARscore performance} % Add a caption to the image
%     \label{fig:top2_sws} % Add a label for referencing in the text
% \end{figure}

% \subsection{Cosine Similarity with/without order}

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{figures/gpt_llama_order_correct_name.png}
    \caption{Results for LLaMA and GPT-4o with and without a prompt encouraging ranking.
    }
    \label{fig:llama/gpt_order}
\end{figure}

\begin{table}[H]
\centering
\scalebox{0.57}{
\begin{tabular}{l|rrrrr}
\multicolumn{6}{c}{Order Agreement}                                                                                                               \\
      & \multicolumn{1}{l}{Annotator1} & \multicolumn{1}{l}{Annotator2} & \multicolumn{1}{l}{Annotator3} & \multicolumn{1}{l}{Annotator4} & \multicolumn{1}{l}{Annotator5} \\
      \hline
Annotator1 & -                         & 0.28                      & 0.51                      & 0.75                      & 0.44                      \\
Annotator2 & -                         & -                         & 0.12                      & 0.19                      & 0.2                       \\
Annotator3 & -                         & -                         & -                         & 0.41                      & 0.44                      \\
Annotator4 & -                         & -                         & -                         & -                         & 0.68                      \\
Annotator5 & -                         & -                         & -                         & -                         & -                         \\
Average$\pm$SD   & \multicolumn{1}{l}{}      & \multicolumn{1}{l}{}      & \multicolumn{1}{l}{}      & \multicolumn{1}{l}{}      & 0.40$\pm$0.21                     \\
\multicolumn{6}{c}{Replacement Agreement}                                                                                                         \\
      & \multicolumn{1}{l}{Annotator1} & \multicolumn{1}{l}{Annotator2} & \multicolumn{1}{l}{Annotator3} & \multicolumn{1}{l}{Annotator4} & \multicolumn{1}{l}{Annotator5} \\
      \hline
Annotator1 & -                         & -0.03                     & -0.2                      & 0.28                      & 0.43                      \\
Annotator2 & -                         & -                         & 0.11                      & 0.17                      & 0.4                       \\
Annotator3 & -                         & -                         & -                         & -0.16                     & 0.11                      \\
Annotator4 & -                         & -                         & -                         & -                         & 0.17                      \\
Annotator5 & -                         & -                         & -                         & -                         & -                         \\
Average$\pm$SD   & \multicolumn{1}{l}{}      & \multicolumn{1}{l}{}      & \multicolumn{1}{l}{}      & \multicolumn{1}{l}{}      & 0.13$\pm$0.21                    
\end{tabular}}
\caption{Inter-annotator agreement using (unweighted) Cohen’s kappa coefficient. SD is the standard deviation.
}
\label{kappa}
\end{table}

\begin{table}[H]
\centering
\scalebox{0.66}{
\begin{tabular}{l|rrr}
\multicolumn{4}{c}{Model and BARTScore Agreement (25 cases)}                  \\
           & Select same          & Select different & Select 0    \\
           \hline
Annotator1 & 7                    & 1                & 17          \\
Annotator2 & 12                   & 5                & 8           \\
Annotator3 & 14                   & 9                & 2           \\
Annotator4 & 18                   & 5                & 2           \\
Annotator5 & 8                    & 0                & 17          \\
Average$\pm$SD        & 11.8$\pm$4.5                 & 4$\pm$3.6                & 9.2$\pm$7.5         \\
% Standard deviation        & 4.49          & 3.61      &  \\
\multicolumn{4}{c}{Model and BARTScore Disagreement (25 cases)}               \\
           & Agree with BARTScore & Agree with Model & Select 0    \\
           \hline
Annotator1 & 6                    & 3                & 16          \\
Annotator2 & 9                    & 7                & 9           \\
Annotator3 & 14                   & 7                & 3           \\
Annotator4 & 14                   & 8                & 3           \\
Annotator5 & 4                    & 1                & 20          \\
Average$\pm$SD        & 9.4$\pm$4.6                  & 5.2$\pm$3.0              & 10.2$\pm$7.7        \\
% STD        & 4.56            & 3.03      & 7.66
\end{tabular}}
\caption{Human study results. The Model is our (MR+AS) model. SD is the standard deviation. Based on the last column, we calculated the proportion of cases in which humans selected 0 by dividing the last column’s value by 25 and computing the average, resulting in 38.8\%. For the remaining cases (where the selection is either 1 or 2), we analyzed agreement between the annotator, the model, and BARTScore. In the top panel, the proportion of cases where the annotator agree with model and the BARTScore is calculated as the value in the first column divided by the sum of the first and second columns, resulting in an average of 79\%. In the bottom panel, we computed two proportions: the agreement between the annotator and BARTScore, calculated as the value in the first column divided by the sum of the first and second columns, and the agreement between the annotator and the model, calculated as the value in the second column divided by the sum of the first and second columns. The resulting agreement proportions are 67\% for annotator-BARTScore agreement and 33\% for annotator-model agreement.
}
\label{human-study}
\end{table}

% \begin{table*}[t]
% \centering
% \begin{tabular}{c|ccccc|ccccc}
%            & \multicolumn{5}{c|}{GPTScore (GPT2-M)}                       & \multicolumn{5}{c}{GPTScore (OPT350M)}                       \\
%            & OAC & CA             & CD             & OMC            & NCA & OAC & CA             & CD             & OMC            & NCA \\ \hline
% BERT-naïve & 1   & 0.888          & 0.425          & 0.47           & 1   & 1   & 0.893          & 0.439          & 0.351          & 1   \\
% BERT-spsv  & 1   & 0.891          & 0.474          & 0.438          & 1   & 1   & 0.927          & 0.506          & 0.498          & 1   \\
% BERT-SWS   & 1   & 0.948          & 0.717          & 0.621          & 1   & 1   & 0.975          & 0.749          & 0.655          & 1   \\
% BART-SWS   & 1   & 0.869          & 0.6            & 0.595          & 1   & 1   & 0.897          & 0.618          & 0.625          & 1   \\
% Rule Based & -   & 0.552          & 0.248          & 0.309          & -   & -   & 0.531          & 0.247          & 0.332          & -   \\
% GPT-4o     & -   & 0.734          & 0.557          & 0.655          & -   & -   & 0.745          & 0.554          & 0.673          & -   \\
% LLaMA      & -   & 0.654          & 0.518          & 0.624          & -   & -   & 0.694          & 0.531          & 0.625          & -   \\
% MR+AS      & 1   & \textbf{0.963} & \textbf{0.809} & \textbf{0.719} & 1   & 1   & \textbf{0.981} & \textbf{0.759} & \textbf{0.657} & 1  
% \end{tabular}
% \caption{Proportion of $p$-values below the significance threshold ($\alpha=0.01$) for $K_s=1000$ using GPTScore with GPT2-medium (355M) and OPT-350M (350M) as backbone models.}
% \label{GPTScore}
% \end{table*}

% \begin{table}[t]
% \centering
% \scalebox{0.52}{
% \begin{tabular}{c|c|c|c|c}
%               & CC    & CINC  & NCC   & Ratio         \\ \hline
% BERT\_SWS     & 0.113 & 0.149 & 0.064 & 4.09          \\
% BERT\_vanilla & 0.032 & 0.054 & 0.052 & 1.65          \\
% BERT\_spsv    & 0.02  & 0.028 & 0.017 & 2.82 \\
% BART\_SWS     & 0.113 & 0.151 & 0.068 & 3.88          \\
% Rule\_based   & 0.11  & 0.26  & 0.101 & 3.66          \\
% GPT-4o        & \textbf{0.244} & \textbf{0.248} & \textbf{0.115} & \textbf{4.28}          \\
% Llama         & 0.121 & 0.157 & 0.065 & 4.28          \\
% MR+AS         & 0.055 & 0.052 & 0.027 & 3.96         
% \end{tabular}}
% \caption{The value indicates the proportion of tokens involved. We divided the data into two groups: {CNC, CC, CINC}, labeled as 'change' by the human annotator, and {NCC, NCNC}, labeled as 'not change' by the annotator. These two groups correspond to the row in Table \ref{Sws_test_data_stat}, and the sum of the proportions within each group equals 1. Ratio refers to the proportion between the number of tokens both the model and the human want to change and the number of tokens changed only by the model.}
% \label{p-value-proportion}
% \end{table}


\end{document}
