\section{Related Work}
%
Enhancing word usage is a key feature of writing assistance.
The SWS task in \citet{wang2023smart} addresses it by emulating a real-world end-to-end writing scenario.
Unlike traditional LS tasks \citep{mccarthy2007semeval, kremer2014substitutes}, SWS requires systems to identify words that, if replaced, can improve sentence quality and suggest alternatives for such words.
In the LS task, however, the word to be replaced is predefined and the model is only tasked with finding suitable substitutions without altering the overall meaning of the sentence.
Further, in LS substitutions are typically lemmatized to match ground-truth labels, but in SWS they must be in the correct grammatical form.

\citet{zhou2019BERT} proposed a LS method using contextual word embeddings, modifying BERT with a dropout embedding policy to partially mask target words and introduced a validation score based on the top four layers of BERT for candidate evaluation.
\citet{michalopoulos2021lexsubcon} integrated external knowledge from WordNet into BERT for LS, combining scores from BERT, WordNet gloss similarity, and sentence embeddings, along with the validation score from \citet{zhou2019BERT}, to generate substitutes.
ParaLS \citep{qiang2023parals} generated substitutes through a paraphrasing model, using a heuristics-based decoding strategy.

To adapt the substitution capabilities from LS to the SWS task, \citet{wang2023smart} used an approach that allowed models to generate substitutions for each word.
If the substitution differs from the original word, it is recognized as the word the model wants to improve and recorded as a suggestion.
Further, they fine-tuned pretrained models like BERT \citep{devlin2018BERT} and BART \citep{lewis2019bart} to address the task in an end-to-end manner, and as an alternative to machine learning, rule-based methods using dictionaries (thesaurus) were also explored.

Recently, prompt-based methods have emerged as a powerful alternative for SWS \citep{wang2023smart}, by leveraging the capabilities of LLMs such as GPT \citep{ouyang2022training}.
These approaches shift the focus from traditional model fine-tuning to designing prompts that guide the model to produce desired outcomes.
While this flexibility allows for easier adaptation to various tasks, the effectiveness of the method heavily relies on the prompt's structure.
Despite the growing interest in generating fluent, contextually cohesive text with LLMs, prompt-based methods often overlook the importance of improving specific word choices.
%, a critical factor in tasks like word substitution.
In contexts where sentence clarity and precision are key, refining word choices remains an important task that prompt-based approaches need to address.

Compared to traditional methods that rely heavily on human involvement, {\em e.g.}, via manual annotations, in our work, we utilize model-based sentence scoring, specifically, BARTScore to quantify the quality of sentences, thereby eliminating the human-associated costs.
We align the modelâ€™s output with the scores through a ranking loss, ensuring that substitutions with higher scores are more likely to be generated by the model, resulting in higher-quality substitutions.
This approach forgoes the need for human-dependent supervised training, thus without the need for human intervention.