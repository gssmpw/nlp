\section{Introduction}
\label{sec: intro}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/intro-figure-v2.pdf}
    \caption{Illustration of the double-blind federated adaptation.}
    \label{fig: intro-figure}
    \vspace{-1em}
\end{figure}

In the dynamic world of artificial intelligence (AI), the advent of foundational models (FMs) has marked a transformative era. Characterized by their vast scale, extensive pre-training, and versatility, FMs such as GPT \cite{radford2019language, brown2020language}, CLIP \cite{radford2021learning}, BERT \cite{devlin2018bert, he2020deberta, liu2019roberta}, Stable Diffusion \cite{rombach2022high}, Segment Anything \cite{kirillov2023segment}, and Vision Transformers \cite{dosovitskiy2020image, liu2021swin} have advanced the state-of-the-art (SOTA) in many machine learning, computer vision, and language processing tasks. Though vision and multimodal FMs have demonstrated good zero-shot performance on many image classification tasks, there is often scope for performance improvement when faced with challenging out-of-domain tasks (e.g., medical images or satellite imagery). Hence, it becomes essential to adapt the FM for the downstream task.

Adaptation of FMs for downstream tasks involves two main challenges - \textit{computational complexity} and \textit{data availability}. The simplest way to adapt an FM for a downstream classification task is via \textit{transfer learning} - treat the FM as a feature extractor, append a classification head, and learn only the parameters of this classifier using the available training data (while the FM parameters remain frozen). If the classification head involves a single linear layer, it is referred to as \textit{linear probing}. It is also possible to perform \textit{partial} (only a selected subset of parameters are adapted) or \textit{full finetuning} of the parameters of the FM based on the downstream data. Recently, two other broad categories of parameter-efficient fine-tuning (PEFT) approaches have been proposed for FM adaptation. The first approach is called \textit{prompt learning} \cite{jia2022visual}, where input (or intermediate) prompts are learned instead of modifying the FM parameters. The second approach is referred to as \textit{adapters} \cite{hu2021lora,dettmers2023qlora,mercea2024time}, where additional components are added to the FM model architecture, and only these new parameters are learned. Adapters can be further categorized into sequential (e.g., low-rank adaptation a.k.a. LoRA \cite{hu2021lora}) and parallel (e.g., low-rank side adapter a.k.a. LoSA \cite{mercea2024time}) adapters. Except for transfer learning and parallel adapters, all the other adaptation techniques require partial or complete backpropagation of the gradients through the FM, which is computationally expensive. Finally, when only query access to a black-box FM is allowed, gradients can be estimated through \textit{zeroth-order optimization} (ZOO). While ZOO avoids backpropagation, it is also computationally expensive because it requires numerous forward passes. 

The other major challenge in FM adaptation is that downstream training data required for adaptation may not be available to the learning service provider (LSP) who owns the FM. Moreover, this data may be distributed across multiple data owners (e.g., multiple hospitals or banks) and cannot be collated due to privacy concerns and regulations. This necessitates collaboration between the LSP and data owners to enable FM adaptation. Federated Learning (FL) \cite{mcmahan2017communication} is a paradigm that promises to alleviate this data availability challenge by facilitating collaborative model training across distributed data repositories, while maintaining the privacy of the underlying data. FL encompasses a spectrum of applications, from healthcare \cite{antunes2022federated, xu2021federated} and finance \cite{long2020federated, liu2023efficient} to video surveillance \cite{sugianto2024collaborative} and beyond \cite{ghimire2022recent}. It has two primary scenarios: cross-silo (few data owners) and cross-device FL (large number of data owners) \cite{kairouz2021advances}, each catering to different scales of collaboration, thereby enabling both large and small-scale entities to benefit. 

In this work, we focus on the problem of federated adaptation of a foundation model (for an out-of-domain downstream image classification task) by an LSP (server) through collaboration with multiple data owners (clients) under two core constraints: (i) \textit{Model privacy} - the LSP wants to retain full ownership of the FM and does not want to share the FM with the data owners; and (ii) \textit{Data privacy} - the data owners do not want to leak their data to the LSP or other data owners. We jointly refer to these constraints as \textit{double-blind privacy}. We make the following contributions:

\begin{itemize}
    \item We propose a framework for double-blind federated adaptation of FMs based on well-known cryptographic constructs such as fully homomorphic encryption (FHE) and secure multi-party computation (MPC). 

    \item The proposed framework first distills the knowledge from the FM into a sequence of FHE-friendly transformer attention blocks. It then employs an interactive protocol based on a privacy-preserving permutation scheme to perform encrypted inference without leaking the model and the data. It also leverages a low-rank parallel adapter for local learning and an MPC protocol for FL aggregation.

    \item We demonstrate the practical feasibility of the proposed framework through experiments on four datasets.
\end{itemize}


