\section{Related Work}
\label{section: literature-review}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{images/main-full-diagram.pdf}
    \caption{Illustration of the block decomposition and non-linear functions that need to be approximated (highlighted in red).}
    \label{fig: main-full-diagram}
    \vspace{-0.5em}
\end{figure*}


\noindent\textbf{Foundation models}:  
FMs have been highly successful in computer vision \cite{dosovitskiy2020image, radford2021learning, liu2021swin}, natural language processing \cite{radford2019language, liu2019roberta, yang2019xlnet, he2020deberta, clark2020electra}, and beyond \cite{sharir2021image, ranasinghe2022self}. In particular, the two-stage training strategy has shown to be effective, where FMs are first pre-trained on a large dataset for general understanding, and then fine-tuned on a small downstream dataset to learn task-specific features. However, their vast scale introduces significant challenges, particularly in fine-tuning, that hinder their practical applicability. 

\noindent\textbf{Private inference}: The advent of machine learning as a service (MLaaS) has underscored the critical need for privacy-preserving techniques in ML, particularly in inference tasks. The concept of private inference (PI) has emerged as a pivotal solution to safeguard data and model privacy \cite{mohassel2017secureml, srinivasan2019delphi, shen2022abnn2, tan2021cryptgpu, zhang2023sal, li2022mpcformer, huang2022cheetah}. Vision transformers (ViTs), despite their superior performance in various tasks, pose significant challenges due to their computational complexity, especially when coupled with cryptographic constructs such as FHE \cite{acar2018survey, gentry2009fully, cheon2017homomorphic} and MPC \cite{goldreich1998secure, knott2021crypten} that are required for PI. The literature surrounding the optimization of PI frameworks primarily focuses on reducing computational and communication overheads while maintaining or improving model accuracy. SAL-ViT \cite{zhang2023sal} is a framework designed to enhance the efficiency of PI on ViT models. Iron \cite{hao2022iron} introduced secure protocols aimed at optimizing matrix multiplication and several complex non-linear functions integral to transformer models, such as Softmax, GELU activations, and LayerNorm. An alternative approach is to develop PI-friendly transformer architectures and softmax approximations, such as MPCViT \cite{zeng2023mpcvit} and MPCFormer \cite{li2022mpcformer}. MPCViT introduces an MPC-friendly adaptation of ViTs that aims to balance the trade-off between accuracy and efficiency during inference in MPC settings. MPCFormer utilizes MPC and knowledge distillation to address latency and reduction in inference quality for transformers.  



\noindent\textbf{Adaptation of foundation models}: 
The primary issue in adapting FMs is their massive size, making it impractical for individual users or clients with limited computational resources to fine-tune or even store them. Various PEFT techniques such as adapters \cite{houlsby2019parameter, lin2020exploring}, prompt learning \cite{li2021prefix}, low-rank adaptation (LoRA) \cite{hu2021lora}, and low-rank side adaptation \cite{mercea2024time} have been proposed. Numerous variants of LoRA, such as AdaLoRA \cite{zhang2023adaptive}, Delta-LoRA \cite{zi2023delta}, IncreLoRA \cite{zhang2023increlora}, and QLoRA \cite{dettmers2023qlora} are also available. These methods specifically target the transformer attention blocks that form the core of these models. LoRA \cite{hu2021lora} introduces adjustments to the weight matrices within these blocks, thus enabling efficient fine-tuning with a reduced computational load. However, 
LoRA necessitates backpropagation through the backbone, thus increasing the total time taken to update the model. Examples of studies exploring PEFT techniques for LLMs with federated learning include \cite{zhang2023fedpetuning} and \cite{zhang2023towards}. 


