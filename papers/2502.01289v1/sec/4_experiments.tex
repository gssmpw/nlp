\section{Experiments and Results} 
\label{section: experiments} 


\subsection{Datasets} 

To validate and implement our approach, we will utilize a well-known vision transformer (ViT) pretrained on ImageNet-1K (ViT-Base) as the FM. 
The public auxiliary datasets for distilling the FHE-friendly FM are: 
\begin{itemize}
    \item \textbf{Tiny-Imagenet} \cite{le2015tiny} is a subset of the larger ImageNet dataset, and it contains  200 different classes, each with 500 images, 100 validation/test images, totaling 120000 images. 
    \item \textbf{Fed-ISIC2019} \cite{terrail2022flamby} is a multi-class dataset of dermoscopy images containing $23247$ images across $8$ melanoma classes, with high label imbalance. This dataset provides a valuable opportunity to test our framework within the healthcare domain. For the distillation phase of our experiments, we exclusively use data from client 1. 
\end{itemize}
The private downstream datasets are: 
\begin{itemize} 
    \item \textbf{CIFAR-10 and CIFAR-100} \cite{krizhevsky2009learning} datasets are standard benchmarks for image classification, containing $60000$ color images across $10$ and $100$ classes, respectively. 
    \item \textbf{SVHN} \cite{netzer2011reading} dataset is a benchmark for digit recognition, consisting of over $600000$ color images of house numbers. We utilize $73257$ images for training and $26032$ for testing, disregarding the remainder. 
    \item \textbf{Fed-ISIC2019.} The remaining data points for centers 2-6 are used in the fine-tuning experiments, aligning well with the federated setup, as the dataset is tailored for federated learning. For the centralized setup, all data points are pooled into a single client. 
\end{itemize}


\subsection{Experimental Setup}
We employ the Vision Transformer (ViT) \cite{dosovitskiy2020image}, pre-trained on the ImageNet-1k dataset \cite{russakovsky2015imagenet} (ViT-Base), with a backbone dimension of $384 \times 384$. For the first phase of our framework, obtaining the FHE-friendly FM, we use Adam optimizer with a learning rate of $10^{-4}$ for distilling the transformer blocks for 15 epochs and $10^{-5}$ for distilling the prediction layer for the remaining 15 epochs, totaling 30 epochs. We set the batch size to $16$ due to the substantial memory demands. We use MSE loss for the first phase of the distillation and the combination of cross-entropy loss and Kullback-Leibler (KL) divergence losses for the second phase. We set the polynomial order of exponential approximation to $6$ and the order of the inverse to $7$. 
For the second phase of our framework, federated adaptation, we use SGD optimizer with a learning rate of $0.001$ for Linear probing and our proposed method and $5\cdot10^{-5}$ for full fine-tuning experiments. We set the total number of communication rounds to $T=50$, and we use a learning rate scheduler with a decay factor of $0.1$ at rounds $[25, 40]$. We set the batch size to $16$ unless otherwise specified. We use cross-entropy loss to evaluate the effectiveness of the global model and report balanced accuracy. We use Dirichlet distribution-based splitting for all our experiments except the Fed-ISIC2019 dataset, which is naturally partitioned. 
All our experiments are conducted on NVIDIA A100-SXM4-40GB GPUs on an internal cluster server, with each run utilizing a single GPU. 


\begin{table}[th]
    \centering
    \resizebox{\linewidth}{!}{ 
        \begin{tabular}{llcc}
            \toprule 
            \textbf{Public dataset} & \textbf{Methods} & \textbf{Centralized} & \textbf{Federated} \\ \midrule 
            \multirow{3.5}{*}{\parbox{2.25cm}{\centering \textbf{Fed-ISIC2019 \\ (center=0) \\ (InD)}}} & Linear probing                     & $0.6599$ & $0.5856$ \\ 
                                                                                                        & Full fine-tuning                   & $0.7811$ & $0.6752$ \\ \cmidrule{2-4}
                                                                                                        & \gr \textbf{Ours} & \gr $0.7090$ & \gr $0.6679$ \\ \midrule 
            \multirow{3.5}{*}{\parbox{2.25cm}{\centering \textbf{Tiny-Imagenet \\ (OOD)}}}              & Linear probing                     & $0.6372$ & $0.5789$ \\ 
                                                                                                        & Full fine-tuning                   & $0.7817$ & $0.6985$ \\ \cmidrule{2-4}
                                                                                                        & \gr \textbf{Ours} & \gr $0.7051$ & \gr $0.6481$ \\ \bottomrule 
        \end{tabular}
    }
    \caption{Performance comparison of our method with baseline approaches on the Fed-ISIC2019 dataset with five clients ($K=5$), using two auxiliary datasets: Fed-ISIC2019 (center=0) as an in-distribution (InD) dataset and Tiny-ImageNet as an out-of-distribution (OOD) dataset.} 
    \label{tab: fedisic}
    \vspace{-0.5em}
\end{table}

\subsection{Results} 

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{images/tradeoff-plots-log-v2.pdf}
    \caption{Accuracy vs. no. of trainable parameters trade-off (x-axis in log scale) derived from Table \ref{tab: main-comparison-acc}, illustrating the performance of the methods across three datasets.} 
    \label{fig: tradeoff-plot}
\end{figure*}

\textbf{Main results: } In Table \ref{tab: main-comparison-acc}, the performance of our proposed method and baseline methods across three datasets (CIFAR-10, CIFAR-100, and SVHN) is evaluated under both centralized and federated learning settings. The federated learning experiment uses a Dirichlet partitioning strategy with varying levels of data heterogeneity, controlled by the Dirichlet concentration parameter $\alpha$ (ranging from $100$ to $0.01$). The results demonstrate that full fine-tuning achieves the highest accuracy across all datasets and settings, particularly excelling in more homogeneous federated scenarios, but it is computationally expensive. Linear probing performs well under centralized and homogeneous federated settings. When the task is relatively more straightforward, it fails to work with the SVHN dataset, reaching only $0.5938$ accuracy in the centralized setting, and struggles when there is extreme heterogeneity. Our proposed method delivers robust and competitive results across all settings, showing strong resilience to non-i.i.d data, making it suitable for practical federated learning scenarios (refer to Figure \ref{fig: tradeoff-plot}). 

\noindent\textbf{Fed-ISIC2019 results: } Table \ref{tab: fedisic} compares the performance of our method and baselines on the Fed-ISIC2019 dataset with five centers. The auxiliary datasets used at the LSP include (1) Fed-ISIC2019 with only the first center (treated as an in-distribution dataset) and (2) Tiny-ImageNet (treated as an out-of-distribution dataset). The results demonstrate that knowledge transfer from the OOD dataset is effective for all the methods, highlighting that the public dataset can be any available dataset. Notably, our method effectively learns from the OOD FM, with a minimal drop compared to the InD FM, highlighting its robustness. 

\begin{table}[th]
    \centering
    \resizebox{0.8\linewidth}{!}{ 
    \begin{tabular}{lccc}
        \toprule 
         \textbf{Methods} & $K=10$ & $K=20$ & $K=50$ \\ \midrule 
         Linear probing                        & $0.9167$ & $0.9142$ & $0.9007$ \\
         Full fine-tuning                      & $0.9739$ & $0.9513$ & $N/A$ \\ \midrule 
         \rowcolor{lightgrey} \textbf{Ours}    & $0.9446$ & $0.9422$ & $0.9287$ \\ \bottomrule 
    \end{tabular}
    }
    \caption{Scalability analysis of the proposed method to baseline approaches on the CIFAR-10, with varying number of clients $K \in \{10, 20, 50\}$ under a Dirichlet concentration parameter of $1.0$ for data partitioning. $N/A$ $-$ one GPU is insufficient to run the experiment. } 
    \label{tab: number-of-participants}
    \vspace{-0.5em}
\end{table}

\noindent\textbf{Scalability results:} Table \ref{tab: number-of-participants} illustrates the scalability of our method and the baseline approaches on the CIFAR-10 dataset with increasing number of clients $(N=\{10, 20, 50\})$ using Dirichlet partitioning $(\alpha=1.0)$ and a fixed batch size of 8. Full fine-tuning achieves the highest accuracy for $K = 10$ and $K = 20$ but becomes infeasible for $K = 50$ due to GPU limitations (we use only one GPU for each of our experiments). Linear probing demonstrates stable performance, but our method outperforms linear probing in all the settings, balancing compute efficiency, scalability, and accuracy and demonstrating its suitability for federated setups with a large number of clients. 



