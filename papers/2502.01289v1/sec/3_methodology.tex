\section{Problem Formulation}
\label{section: problem-formulation} 

Suppose that a foundation model (FM) $\mathcal{M}_{\psi}$ that is already pre-trained on a large-scale dataset is available with the learning service provider (LSP). The LSP aims to collaborate with the $K$ data owners to adapt the FM for a downstream image classification task. Each data owner $\mathcal{P}_k$ has access to a local training dataset $\mathcal{D}_k = \{\mathbf{x}_i^k,y_i^k\}_{i=1}^{N_k}$ corresponding to the downstream task. Here, $\mathbf{x}_i^k$ denotes the $i^{\text{th}}$ input image of $\mathcal{P}_k$, $y_i^k$ is the corresponding class label, $N_k$ is the number of training samples with $\mathcal{P}_k$, and $k \in [1,K]$. 

\noindent \textbf{Problem Statement}: A task-specific classification head $\mathcal{H}_{\eta}$ and a side adapter $\mathcal{A}_{\theta}$ are appended to the FM to enable the adaptation. Thus, the adapted model $\widetilde{\mathcal{M}}_{\widetilde{\psi}}$ can be considered a combination of the original FM, the side adapter, and the classifier, i.e., $\widetilde{\mathcal{M}}_{\widetilde{\psi}} = (\mathcal{M}_{\psi} || \mathcal{A}_{\theta}) \circ \mathcal{H}_{\eta}$, where $||$ indicates that these functions operate in parallel and $\circ$ is the composition operator. The goal of the double-blind federated adaptation framework is to collaboratively learn the parameters $\eta$ and $\theta$ under the following constraints: (i) \textit{Data Privacy}: the LSP does not learn anything about the local datasets $\{\mathcal{D}_k\}_{k=1}^{K}$ and data owner $\mathcal{P}_k$ does not learn anything about other local datasets $\mathcal{D}_j$, where $j \neq k$; (ii) \textit{Model Privacy}: the data owners do not learn anything about the FM $\mathcal{M}_{\psi}$.

\noindent \textbf{Assumptions}: To simplify the double-blind federated adaptation problem and make it practically feasible, we make the following assumptions: (i) \textit{Auxiliary dataset for preliminary adaptation}: We assume that the LSP has access to an independent auxiliary dataset $\mathcal{D}_{\text{aux}}$, which allows it to perform preliminary adaptation of the given FM into an image classifier. Note that this public dataset may not even correspond to the target image classification task. (ii) \textit{Modularity of FM}: We further assume that the FM has a modular architecture, which can be represented as a sequence of $L$ blocks, i.e., $\mathcal{M}_{\psi} = \mathcal{B}_{\psi_1} \circ \mathcal{B}_{\psi_2} \cdots \circ \mathcal{B}_{\psi_L}$. Specifically, a transformer architecture is considered in this work. (iii) \textit{Thin client}: The data owners do not have the resources to store the FM (or an encrypted version of it) and perform inference (or encrypted inference) using the FM. However, we assume that the data owners have sufficient computational resources to perform fully homomorphic encryption and decryption operations. (iv) \textit{Powerful server}: The LSP has enough computational resources to perform private inference on encrypted data transmitted by the data owners. Henceforth, we refer to the LSP and data owners as server and clients, respectively. (v) \textit{Semi-honest threat model}: Both the server and the clients are assumed to be semi-honest, i.e., they follow the adaptation protocol honestly, but may attempt to violate the privacy constraints. 

\noindent \textbf{Vanilla Federated Adaptation}: Let $\mathcal{L}_k(\theta,\eta) = \frac{1}{N_k} \sum_{i=1}^{N_k} \mathbb{L}(\hat{y}_i^k,y_i^k)$ be the average loss at client $k$, where $\mathbb{L}$ denotes the per-sample loss and $\hat{y}_i^k = \widetilde{\mathcal{M}}_{\widetilde{\psi}}(\mathbf{x}_i^k)$ is the prediction output by the adapted model. Federated adaptation can be posed as a distributed optimization problem \cite{mcmahan2017communication}, where the goal is to learn the global model parameters $(\theta,\eta)$ such that:
\begin{equation}
    \min_{(\theta,\eta)} ~ \sum_{k=1}^K \alpha_k \mathcal{L}_k(\theta,\eta), \nonumber
\label{eq:fed}
\end{equation}
\noindent where $\alpha_k = \frac{N_k}{\sum_{j=1}^K N_j}$. In each round $t$ of FL adaptation, the server broadcasts the previous model parameters $\left(\theta^{(t-1)},\eta^{(t-1)}\right)$. Each client computes the local model parameters $\left(\theta_k^{(t)},\eta_k^{(t)}\right)$ and these local updates are aggregated by the server to obtain the current global model parameters $\left(\theta^{(t)},\eta^{(t)}\right)$. For example, simple FedAvg aggregation function can be represented as follows:

\begin{equation}
   \left(\theta^{(t)},\eta^{(t)}\right) = \sum_{k=1}^{K} \alpha_k \left(\theta_k^{(t)},\eta_k^{(t)}\right).
   \label{eqn:FedAvg}
\end{equation}

\noindent Note that $t \in [1,T]$, where $T$ is the number of communication rounds and the model parameters $(\theta_k^{(0)},\eta_k^{(0)})$ for the first round are typically initialized randomly by the server. 

\noindent \textbf{Challenges}: The above vanilla federated adaptation is not privacy-preserving because it requires computation of $\hat{y}_i^k = \widetilde{\mathcal{M}}_{\widetilde{\psi}}(\mathbf{x}_i^k)$, where the core FM $\mathcal{M}_{\psi}$ is available only at the server and the local training datasets are available only with the respective clients. Hence, it is essential to design a mechanism for computing $\mathcal{M}_{\psi}(\mathbf{x}_i^k)$ without violating the data and model privacy constraints. Moreover, the sharing of local updates $\left(\theta_k^{(t)},\eta_k^{(t)}\right)$ with the server could also potentially leak information about the local datasets \cite{zhu2019deep}. Hence, the aggregation step in Eq. \ref{eqn:FedAvg} must be performed securely without revealing the local updates to the server.

\section{Proposed Adaptation Framework}

The proposed framework for double-blind federated adaptation consists of the following steps: (1) \textbf{FHE-friendly Distillation}: Prior to federated adaptation, the server performs an offline distillation of the original FM into a FHE-friendly model using the auxiliary dataset. (2) \textbf{Encrypted Inference}: During adaptation, each client encrypts its local data using FHE and sends them to the server. The server and client engage in an interactive protocol to perform inference on the encrypted data. (3) \textbf{Local Learning}:  Based on the intermediate outputs received from the server, each client locally updates the side network and classification head. (4) \textbf{Secure Aggregation}: The server securely aggregates these local updates through a multi-party computation (MPC) protocol with the clients. The overall training workflow of the proposed framework is summarized in Figure \ref{fig: main-full-diagram}. At the end of this process, each client receives a copy of the global parameters of the side network and classifier. At the time of inference, the encrypted inference step (Step 2) is repeated. Based on the intermediate outputs received from the server, the client utilizes the global side adapter and classifier to obtain the final class prediction. 

\subsection{FHE-friendly Distillation}

The first step in the proposed framework is to distill the knowledge available in the given FM into an FHE-friendly model. The server performs this step using the auxiliary dataset $\mathcal{D}_{aux}$. Assuming that the given FM follows a modular transformer encoder architecture as shown in Figure \ref{fig: losa-figure}, we can consider the FM as a sequence of $L$ attention blocks. Let $\mathbf{b}_{\ell-1}$ be the input to the $\ell^{\text{th}}$ attention block $\mathcal{B}_{\psi_\ell}$ and $\mathbf{b}_{\ell}$ be the corresponding output. The objective of FHE-friendly distillation is to learn an FHE-friendly approximation of $\mathcal{B}_{\psi_\ell}$ denoted as $\widehat{\mathcal{B}}_{\widehat{\psi}_\ell}$ so that when the encrypted input $\mathcal{E}(\mathbf{b}_{\ell-1})$ is provided as input, the server can compute the encrypted output $\mathcal{E}(\mathbf{b}_{\ell}) = \widehat{\mathcal{B}}_{\widehat{\psi}_\ell}(\mathcal{E}(\mathbf{b}_{\ell-1}))$. Here, $\mathcal{E}$ denotes the encryption operation.

\noindent \textbf{Approximating Non-linear Functions}: The primary challenge in encrypted inference is approximation of the non-linear functions because most well-known FHE schemes (e.g., CKKS \cite{cheon2017homomorphic}) allow only polynomial operations. A transformer attention block has three key non-linear operations: Softmax, GELU, and LayerNorm. In this work, Softmax is approximated using a Taylor series approximation of the exponential function $(e^x)$: 
\begin{equation}
    e^x = \sum_{i=0}^{\infty} \frac{x^i}{i!} \approx \sum_{i=0}^d \frac{x^i}{i!},  
    \label{eq: exp-approx}
\end{equation}
followed by normalization through division by the sum of the calculated exponential values. The error bound of this approximation is the remainder term, e.g. $\dfrac{e^{\xi}}{(d+1)!} x^{d+1}$, for some $\xi$ between $0$ and $x$. Furthermore, GELU activation is approximated via a simple quadratic function: 
\begin{equation}
    \text{GELU}(x) \approx \text{Quad}(x) = 0.125x^2 + 0.25x + 0.5. 
    \label{eq: gelu-approx} 
\end{equation}
The LayerNorm function and Softmax require a division step, which is implemented based on the iterative protocol described in \cite{cheon2019numerical} following the Goldschmidt's algorithm: 
\begin{eqnarray}
    \frac{1}{x} = \frac{1}{1-(1-x)} &=& \prod_{i=0}^{\infty} \left(1+(1-x)^{2^i}\right) \nonumber \\ 
    &\approx& \prod_{i=0}^d \left( 1+(1-x)^{2^i} \right), \label{eq: division} 
\end{eqnarray}
where $x \in (0, 2)$. 

\noindent \textbf{Distilling the FM}: After approximating all nonlinearities (Softmax, GELU, and Inverse) in the FM and developing an FHE-friendly FM, we enhance the performance of the FHE-friendly model through knowledge distillation. Initially, we apply layerwise distillation by aligning hidden representations at four positions: (i) the embedding layer, (ii) the attention matrix within each transformer block (considering raw values before normalization), (iii) hidden states after each transformer block, and (iv) the final prediction layer \cite{jiao2019tinybert, hinton2015distilling, li2022mpcformer}. During distillation, we dedicate the first half of the training epochs to distilling the embedding and transformer blocks (both the attention matrix and hidden states) and then proceed to distill the prediction layer, following the approach in \cite{jiao2019tinybert}. 



\subsection{Encrypted Inference}
\label{subsec:EncryptedInference}

Since FMs often have a very deep neural network architecture, it is challenging to perform encrypted inference over the full FM based on currently available FHE constructs that support only limited multiplicative depth. Attempting to do so will result in large approximation errors, which may be tolerable for simple inference tasks, but can potentially derail the federated learning process. One potential solution is to apply bootstrapping at regular intervals along the computational path. However, bootstrapping is computationally very expensive and makes the framework practically infeasible (especially under the thin client assumption). Hence, we propose performing encrypted inference only over a single transformer attention block at a time. The output of this block is sent back to the client, which decrypts the intermediate result, performs re-encryption of the intermediate result $\mathcal{E}(\mathcal{F}(\mathcal{E}(\mathbf{b}_{\ell})))$, and returns it to the server. Here, $\mathcal{F}$ denotes the decryption operation. 

The downsides of performing encrypted inference over one attention block at a time are two-fold. Firstly, it increases the communication cost because encrypted intermediate outputs are exchanged between the client and the server after every block in every FL round. Since communication efficiency is not one of our core constraints, we consider the increased communication cost as a limitation, not a red flag. However, since the intermediate representations $\mathbf{b}_{\ell}$ after every attention block are accessible to the client in plaintext form, a malicious client could use $(\mathbf{b}_{\ell-1},\mathbf{b}_{\ell})$ pairs for multiple training samples to mount a model extraction attack \cite{liang2024model} and learn the parameters of each transformer block. This clearly violates the model privacy constraint. 

To circumvent the above problem and preserve model privacy, we leverage the fact that each communication round in the federated adaptation phase involves a batch of samples. Let $\mathcal{E}(\mathbf{B}_{\ell}) = [\mathcal{E}(\mathbf{b}_{\ell}^1),\mathcal{E}(\mathbf{b}_{\ell}^2),\cdots,\mathcal{E}(\mathbf{b}_{\ell}^n)]$ be a batch of encrypted intermediate representations corresponding to a client, where $n$ is the batch size. Before sending these representations to the client, the server applies a $n \times n$ permutation matrix $\mathbf{\Pi}_{\ell}$ and sends only the permuted batch $\mathcal{E}(\mathbf{B}_{\ell})\cdot \mathbf{\Pi}_{\ell} = [\mathcal{E}(\mathbf{b}_{\ell}^{\pi(1)}),\mathcal{E}(\mathbf{b}_{\ell}^{\pi(2)}),\cdots,\mathcal{E}(\mathbf{b}_{\ell}^{\pi(n)})]$ to the client. Here, $[\pi(1),\pi(2),\cdots,\pi(n)]$ represents a random permutation of $[1,2,\cdots,n]$. This permutation matrix $\mathbf{\Pi}_{\ell}$ can be randomly selected for each block $\ell$ in each communication round. Thus, the client never sees corresponding pairs $(\mathbf{b}_{\ell-1}^i,\mathbf{b}_{\ell}^i)$ for any training sample $i$ in the batch, ensuring protection against model extraction attacks.

The overall encrypted inference protocol can be summarized as follows. At the beginning of the collaboration, each client $k$ encrypts (using its public key) each input in its local dataset based on a FHE encryption scheme and sends the encrypted inputs $\{\mathcal{E}(\mathbf{x}_i)\}_{i=1}^{N_k}$ and the corresponding encrypted labels $\{\mathcal{E}(y_i)\}_{i=1}^{N_k}$ to the server. The server applies the embedding function on the encrypted data to obtain the input to the first attention layer $\{\mathcal{E}(\mathbf{b}_0^i)\}_{i=1}^{N_k}$. Subsequently, for each FL round, the server randomly selects a batch of $n$ samples from this set, say $\mathcal{E}(\mathbf{B}_{0}) = [\mathcal{E}(\mathbf{b}_{0}^1),\mathcal{E}(\mathbf{b}_{0}^2),\cdots,\mathcal{E}(\mathbf{b}_{0}^n)]$, and performs encrypted inference using the FHE-friendly first attention block $\widehat{\mathcal{B}}_{\widehat{\psi}_1}$. The output of this block is permuted using randomly selected $\Pi_1$ before being transmitted to the client. The client decrypts these representations (using its private key), re-encrypts again (using its public key), and returns it to the server. These re-encrypted values are used as the input to the next attention block and the same encrypted inference process is repeated for all the other blocks $\ell = 2,\cdots,L$. 

When the client receives the output of the final transformer attention block $\mathcal{E}(\mathbf{B}_{L})$, the decrypted representations are passed through the classification head and the final predictions are again encrypted to get $\mathcal{E}(\mathbf{\hat{Y}}) = [\mathcal{E}(\hat{y}^1),\mathcal{E}(\hat{y}^2),\cdots,\mathcal{E}(\hat{y}^n)]$. These encrypted predictions are sent back to the server for per-sample loss computation in the encrypted domain. The server computes the average (or total) loss over the entire batch and sends this encrypted average loss to the client. The client decrypts this average batch loss and uses it for local learning. During this entire encrypted inference protocol, all the operations on the server are carried out in the encrypted domain. Since the server does not have access to the client's private key, the server learns no information about the client's local data. On the other hand, the client receives a batch of intermediate representations (in plaintext) after each attention block, but samples in each batch are randomly permuted based on $\mathbf{\Pi}_{\ell}$. Since the client cannot access $\mathbf{\Pi}_{\ell}$, it cannot use a model extraction attack to learn any useful information about the FM held by the server, as long as the batch size $n$ is sufficiently large. This ensures that the proposed encrypted inference protocol is double-blind, even though it is interactive in nature. 

\begin{figure}
    \centering
    \resizebox{\linewidth}{!}{ 
        \includegraphics{images/side-adaptation-scheme.pdf} 
    }
    \caption{Parallel adapter illustration.} 
    \label{fig: losa-figure}
    \vspace{-0.5em}
\end{figure}

\subsection{Local Learning}

Though various model adaptation strategies are available, the proposed framework requires an adaptation method that does not require backpropagation of gradients through the FM. This leaves us with only two possible choices: \textit{transfer learning} (where only the classification head is learned) and \textit{parallel adapter} (where both the classification head and a side adapter are learned). In this work, we adopt the low-rank parallel adapter method proposed in \cite{mercea2024time} (see Figure \ref{fig: losa-figure}). This method requires access to intermediate representations after every transformer attention block, which is readily available (in plaintext) through the encrypted inference protocol described in Section \ref{subsec:EncryptedInference}.

The output of the low-rank parallel adapter corresponding to the attention block $\ell$ can be expressed as:
\begin{equation}
    \mathbf{h}_{\ell} = g_{\ell}(\mathbf{b}_{\ell} + \mathbf{h}_{\ell-1}) + \mathbf{h}_{\ell-1},
    \label{eqn:overallAdapterOutput}
\end{equation}

\noindent where $\mathbf{h}_{0} = \mathbf{b}_L$. The adapter function $g_{\ell}$ is given by:

\begin{equation}
    g_{\ell}(\mathbf{z}) = \alpha \mathbf{W}_{\ell}^u \text{GELU}(\mathbf{W}_{\ell}^d\mathbf{z}),
    \label{eqn:adapterfn}
\end{equation}
\noindent where $\mathbf{W}_{\ell}^d$ and $\mathbf{W}_{\ell}^u$ are the down and up-projection matrices and $\alpha_{\ell}$ is the scaling factor at block $\ell$. Since the adapter function in Eq. \ref{eqn:adapterfn} is applied to each individual sample, the permutation of samples within a batch does not affect this computation. However, the adapter output in Eq. \ref{eqn:overallAdapterOutput} depends on values from two consecutive blocks, which have undergone different permutations. Hence, it is necessary to ensure consistent permutation of the inputs. When operating a batch of samples, Eq. \ref{eqn:overallAdapterOutput} can be reformulated as:
\begin{equation}
    \mathbf{H}_{\ell} = g_{\ell}(\mathbf{B}_{\ell} + \mathbf{H}_{\ell-1}) + \mathbf{H}_{\ell-1},
    \label{eqn:overallAdapterOutputBatch}
\end{equation}
\noindent where $\mathbf{H}_{0} = \mathbf{B}_L$. Note that the client receives only a permutation of intermediate representations, i.e., $(\mathbf{B}_{\ell} \cdot \Pi_{\ell})$ and not the original $\mathbf{B}_{\ell}$, $\forall~ \ell \in [1,L]$. Hence, to facilitate the computations associated with the parallel adapter, the server also sends $(\Pi_{\ell-1}^{-1} \cdot \Pi_{\ell})$ for all $\ell \in [2,L+1]$ as well as $(\Pi_{L}^{-1} \cdot \Pi_{1})$ to the client. When the client receives $(\mathbf{B}_{L} \cdot \Pi_{L})$, it can compute $\mathbf{H}^{'}_{0} = (\mathbf{B}_L \cdot \Pi_{L}) \cdot (\Pi_{L}^{-1} \cdot \Pi_{1}) = (\mathbf{B}_L \cdot \Pi_{1}) = (\mathbf{H}_{0} \cdot \Pi_{1})$. This can be directly used in Eq. \ref{eqn:overallAdapterOutputBatch} along with $(\mathbf{B}_{1} \cdot \Pi_{1})$ to compute $\mathbf{H}^{'}_{1} = (\mathbf{H}_{1} \cdot \Pi_{2})$. Following the same logic, it is possible to compute $\mathbf{H}^{'}_{\ell} = (\mathbf{H}_{\ell} \cdot \Pi_{\ell+1}), ~ \forall~ \ell \in [1,L]$. When the server receives the final encrypted predictions from the client, it can permute the encrypted labels of the batch using $\Pi_{L+1}$ before computing the per-sample losses and aggregating them. It must be emphasized that revealing $(\Pi_{\ell-1}^{-1} \cdot \Pi_{\ell})$ for all $\ell \in [2,L+1]$ as well as $(\Pi_{L}^{-1} \cdot \Pi_{1})$ to the client does not leak any information about $\Pi_{\ell}$ as shown in Proposition \ref{lemma: 1}. Finally, the client locally updates the parallel adapter and classification head in the plaintext domain based on the average loss received from the server employing the same procedure as in \cite{poirot2019split}. 

\begin{proposition}
    Let $\mA, \mB,$ and $\mC$ be $n \times n$ permutation matrices. Given only $\mA^{-1}\mB$, $\mB^{-1}\mC$, and $\mC^{-1}\mA$, it is computationally infeasible to uniquely recover the individual matrices $\mA$, $\mB$, and $\mC$ without additional information. 
    \label{lemma: 1}
\end{proposition}

\begin{proof}[Proof of Proposition \ref{lemma: 1}]
    Let $\mP = \mA^{-1}\mB$, $\mQ = \mB^{-1}\mC$, and $\mR=\mC^{-1}\mA$, where $\mP$, $\mQ$, and $\mR$ are known products derived from matrices $\mA$, $\mB$, and $\mC$. Each of $\mA, \mB,$ and $\mC$ is a permutation matrix, defined by: 
    \begin{eqnarray}
        \mA, \mB, \mC &\in& \mathbb{P}_n = \big\{ \mX \in \{0,1\}^{n \times n}: \nonumber \\ 
        && \mX^T\mX=\mI, \mX \vone = \vone \big\} 
    \end{eqnarray}
    where $\mathbb{P}_n$ denotes the set of $n \times n$ permutation matrices, $\mX^T$ is the transpose of $\mX$, $\vone$ is a vector of ones, and $\mX^T=\mX^{-1}$. 

    \noindent\textbf{Non-uniqueness of solutions:} To recover $\mA$, $\mB$, and $\mC$ uniquely, we would need the products $\mP$, $\mQ$, and $\mR$ to uniquely determine a single set of matrices $(\mA, \mB, \mC)$. However, for any valid solution $(\mA, \mB, \mC)$ that satisfies $\mP = \mA^{-1}\mB$, $\mQ = \mB^{-1}\mC$, and $\mR=\mC^{-1}\mA$, we can construct alternative solutions by applying a left-multiplication with any permutation matrix $\mS \in \mathbb{P}_n$. Define alternative matrices $\mA^{\prime} = \mS\mA$, $\mB^{\prime} = \mS\mB$, and $\mC^{\prime} = \mS\mC$, then: 
    \begin{eqnarray}
        \mA^{\prime -1} \mB^{\prime} &=& (\mS\mA)^{-1} (\mS\mB) \nonumber \\ 
        &=& \mA^{-1}\mS^{-1} \mS\mB \nonumber \\ 
        &=& \mA^{-1}\mB = \mP, 
    \end{eqnarray}
    and similarly, $\mB^{\prime -1}\mC^{\prime} = \mQ$ and $\mC^{\prime -1}\mA^{\prime} = \mR$. 
    Thus, the products $\mP$, $\mQ$, and $\mR$ are also satisfied by the set $(\mA^{\prime}, \mB^{\prime}, \mC^{\prime})$. Since there are $n!$ possible choices for $\mS$, there are $n!$ distinct sets of matrices $(\mA^{\prime}, \mB^{\prime}, \mC^{\prime})$ that satisfy the products $\mP$, $\mQ$, and $\mR$. 

    \noindent\textbf{Brute-force as the optimal attack strategy:} Given the non-uniqueness property above, an attacker aiming to recover $\mA$, $\mB$, and $\mC$ must resort to brute force, testing all possible configurations of $(\mA, \mB, \mC)$ that satisfy the products $(\mP, \mQ, \mR)$. The total number of combinations of $(\mA, \mB, \mC)$ the attacker would need to test is $n!$ and for $n=16$, we need $16! \approx 2 \cdot 10^{13}$ combinations. 
    
\end{proof}


\subsection{Secure Aggregation}
To ensure the secure aggregation of model parameter updates from clients, we leverage secure multi-party computation (MPC) for computing averages in a privacy-preserving manner. This approach, commonly referred to in the literature as Secure Aggregation \cite{bonawitz2017practical}, enables the aggregation server to compute the average of client updates without gaining access to the individual updates themselves. 



\begin{table*}[th]
    \centering
    \resizebox{0.875\linewidth}{!}{ 
        \begin{tabular}{llccccc} 
            \toprule 
             \multirow{2.5}{*}{\textbf{Datasets}} & \multirow{2.5}{*}{\textbf{Methods}} & \textbf{Centralized} & & \multicolumn{3}{c}{\textbf{Federated} $(K=5)$} \\ \cmidrule{3-3} \cmidrule{5-7} 
             & & \textbf{Pooled} & & \textbf{Dirichlet} $(\alpha=100)$ & \textbf{Dirichlet} $(\alpha=1)$ & \textbf{Dirichlet} $(\alpha=0.01)$ \\ \midrule 
             \multirow{3.5}{*}{\textbf{CIFAR-10}}  & Linear probing                     & $0.9226$ & & $0.9203$ & $0.9191$ & $0.7447$ \\ 
                                                   & Full fine-tuning                   & $0.9635$ & & $0.9759$ & $0.9725$ & $0.8857$ \\ \cmidrule{2-7} 
                                                   & \gr \textbf{Ours} & \gr $0.9428$ & \gr & \gr $0.9471$ & \gr $0.9413$ & \gr $0.8540$ \\ \midrule 
             \multirow{3.5}{*}{\textbf{CIFAR-100}} & Linear probing                     & $0.7476$ & & $0.7486$ & $0.7414$ & $0.5317$ \\ 
                                                   & Full fine-tuning                   & $0.8361$ & & $0.8684$ & $0.8611$ & $0.7882$ \\ \cmidrule{2-7} 
                                                   & \gr \textbf{Ours} & \gr $0.7930$ & \gr & \gr $0.7929$ & \gr $0.7808$ & \gr $0.6620$ \\  \midrule 
             \multirow{3.5}{*}{\textbf{SVHN}}      & Linear probing                     & $0.5938$ & & $0.5879$ & $0.5732$ & $0.3385$ \\ 
                                                   & Full fine-tuning                   & $0.9680$ & & $0.9763$ & $0.9692$ & $0.7601$ \\ \cmidrule{2-7} 
                                                   & \gr \textbf{Ours} & \gr $0.9232$ & \gr & \gr $0.9329$ & \gr $0.9249$ & \gr $0.7431$ \\ \bottomrule 
        \end{tabular}
    }
    \caption{Comparison of the performance of our method against baseline approaches across three datasets (CIFAR-10, CIFAR-100, and SVHN) in both centralized and federated learning settings. The federated experiments involve five clients $(K=5)$ with data partitioned using a Dirichlet distribution at varying levels of heterogeneity $(\alpha=100, 1, 0.01)$.} 
    \label{tab: main-comparison-acc} 
\end{table*}



\label{section: proposed-approach}

