\clearpage
\setcounter{page}{1}
\maketitlesupplementary


\appendix 

\section{How Does the Distillation Work?} 
\label{section: how-distillation}
We adopt the transformer distillation approach \cite{jiao2019tinybert}, that consists of two key phases: (i) transformer-layer distillation and (ii) prediction-layer distillation. For generality, let the original transformer model be referred to as the teacher model $(\mathcal{T})$ and the approximation-enabled transformer model as the student model $(\mathcal{S})$. We assume that both models share the same architecture but differ only in their non-linear components (Softmax, GELU, LayerNorm). Further, we outline the primary sub-layers of the transformer blocks: 
\begin{itemize}
    \item \textbf{Attention.} The attention function is formulated as follows: 
    \begin{equation}
        \mathbf{A} = \frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}},  
        \label{eq: attention}
    \end{equation}
    followed by a softmax operation. We are specifically interested in an un-normalized attention matrix $\mathbf{A}$. 
    \item \textbf{Feed-forward network} is formulated as follows: 
    \begin{equation}
        \mathbf{H}(x) = \text{GELU}(x\mW_1 + b_1)\mW_2 + b_2. 
        \label{eq: hidd-0}
    \end{equation}
\end{itemize}

In the first stage of distillation, we perform an attention-based distillation (Eq. \ref{eq: attention}) and hidden representations based distillation (Eq. \ref{eq: hidd-0}). More precisely, 
\begin{equation}
    \mathcal{L}_{a} = \frac{1}{h} \sum_{i=1}^h \left\|\mathbf{A}_i^{\mathcal{S}} - \mathbf{A}_i^{\mathcal{T}}\right\|^2, 
    \label{eq: attn}
\end{equation}
where $h$ is the number of attention heads. And, the distillation of the hidden representation is formulated as follows: 
\begin{equation}
    \mathcal{L}_{h} = \|\mathbf{H}^{\mathcal{S}} - \mathbf{H}^{\mathcal{T}}\|^2, 
    \label{eq: hidd}
\end{equation}
where the matrices $\mathbf{H}^{\mathcal{S}}$ and $\mathbf{H}^{\mathcal{T}}$ are hidden representations of the respective models. For a detailed look, please refer to Figure \ref{fig: distillation}, which illustrates how layer-wise distillation works. Then, the total loss of the first stage is simply defined as: 
\begin{equation}
    \mathcal{L} = \mathcal{L}_{a} + \mathcal{L}_{h}. 
\end{equation}
Further, we perform a prediction-layer distillation following the knowledge distillation approach of \cite{hinton2015distilling} by matching logits and using the following objective function: 
\begin{equation}
    \mathcal{L}_{p} = \mathcal{L}_{CE} (\vz^{\mathcal{S}} / \tau, \vz^{\mathcal{T}} / \tau), 
\end{equation}
where $\vz^{\mathcal{S}}$ and $\vz^{\mathcal{T}}$ are logit vectors produced by student and teacher models, respectively. $\mathcal{L}_{CE}$ is a cross-entropy loss and $\tau$ is a temperature parameter to produce softer probability distributions over classes. We set $\tau$ to $5$ in all our experiments. 
Finally, the final loss objective is defined as: 
\begin{equation}
    \mathcal{L} = 
    \begin{cases}
        \mathcal{L}_a + \mathcal{L}_h & \qquad \triangleright\ \text{Stage I}, \\ 
        \mathcal{L}_p & \qquad \triangleright\ \text{Stage II}. 
    \end{cases}
    \label{eq: total-loss}
\end{equation}
As outlined in the main paper, the total number of epochs is set to $30$, with $15$ epochs allocated to each stage. 

% Figure \ref{fig: distillation} illustrates how layer-wise distillation works. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{suppl/distillation.pdf}
    \caption{Schematic illustration of the layer-wise knowledge distillation (Equations \ref{eq: attn} and \ref{eq: hidd}).}
    \label{fig: distillation}
\end{figure}

\section{Experimental Results} 

\begin{table*}[t]
    \centering
    \resizebox{0.75\linewidth}{!}{ 
        \begin{tabular}{llrrrlr}
            \toprule 
             \textbf{Datasets} & \textbf{Methods} & \multicolumn{2}{c}{\textbf{No. of params (M)}} & \multicolumn{2}{c}{\textbf{Latency (ms)}} & \textbf{Memory (GB)} \\ \midrule 

             \multirow{3.5}{*}{\textbf{CIFAR-10 / SVHN}}
             & Linear probing                     & $0.0073$  & $(1.00\ \times)$     & $15.1$ & $(1.00\ \times)$ & $9.39$ \\                                              
             & Full fine-tuning                   & $82.1096$ & $(11247.89\ \times)$ & $47.4$ & $(3.14\ \times)$ & $18.13$ \\ \cmidrule{2-7} 
             & \gr \textbf{Ours} & \gr$0.2536$  & \gr$(34.74\ \times)$    & \gr$15.7$ & \gr$(1.04\ \times)$ & \gr$9.08$ \\ \midrule 

             \multirow{3.5}{*}{\textbf{CIFAR-100}}
             & Linear probing                     & $0.0733$  & $(1.00\ \times)$     & $15.7$ & $(1.00\ \times)$ & $9.40$ \\
             & Full fine-tuning                   & $82.1756$ & $(1121.09\ \times)$  & $47.4$ & $(3.03\ \times)$ & $18.13$ \\ \cmidrule{2-7} 
             & \gr \textbf{Ours} & \gr$0.3196$  & \gr$(4.36\ \times)$     & \gr$16.3$ & \gr$(1.04\ \times)$ & \gr$9.08$ \\ \midrule 

             \multirow{3.5}{*}{\textbf{Fed-ISIC2019}}
             & Linear probing                     & $0.0059$  & $(1.00\ \times)$     & $20.2$ & $(1.00\ \times)$ & $17.32$ \\
             & Full fine-tuning                   & $82.1082$ & $(13916.64\ \times)$ & $51.2$ & $(2.54\ \times)$ & $8.71$ \\ \cmidrule{2-7} 
             & \gr \textbf{Ours} & \gr $0.2522$  & \gr $(42.75\ \times)$    & \gr $21.1$ & \gr $(1.05\ \times)$ & \gr $8.39$ \\ \bottomrule 
        \end{tabular}
    }
    \caption{Comparison of the efficiency of our method with baseline approaches in terms of the number of parameters, latency and the memory requirement across four datasets (CIFAR-10/SVHN, CIFAR-100, and Fed-ISIC2019). Latency is measured per data point and includes both forward and backward passes. The last column (Memory (GB)) indicates the GPU memory required to conduct the experiment.} 
    \label{tab: main-comparison-params} 
\end{table*}

\subsection{Main Results} 
Table \ref{tab: main-comparison-params} provides additional details on the number of trainable parameters, the latency, and the GPU memory required to execute the respective experiments across the given datasets and methods. These results complement Tables \ref{tab: main-comparison-acc} and \ref{tab: fedisic}. As shown in the table, our proposed method closely matches Linear Probing in terms of the number of parameters and the latency required to process a single sample, while full fine-tuning exhibits a significant disparity in effectiveness. Additionally, the GPU memory required for our method is comparable to that of Linear Probing, whereas full fine-tuning demands nearly twice as much memory. 

\subsection{Distillation Results} 

Following the findings discussed in Section \ref{section: how-distillation}, we present performance plots for two key processes: (i) fine-tuning the teacher model $\mathcal{T}$ on a public auxiliary dataset $\mathcal{D}_{aux}$, and (ii) distilling the student model $\mathcal{S}$ on the same dataset $\mathcal{D}_{aux}$ using the trained teacher model $\mathcal{T}$. Figure \ref{fig: teacher-imgnet} illustrates the performance metrics $-$ balanced accuracy and loss $-$ of the fine-tuning process on the Tiny-ImageNet dataset. After completing this training, we proceed with distillation for the approximation-enabled student model. Figure \ref{fig: student-imgnet} shows the performance plot for the distillation process on the Tiny-ImageNet dataset. In the loss behavior plot (on the right), a dashed line marks the start of the second stage. During the first stage, as described in Eq. \ref{eq: total-loss}, transformer-layer distillation is performed (showing higher values due to the high-dimensional attention and hidden representation matrices). Then, the second-stage distillation follows, which is a prediction-layer distillation \cite{hinton2015distilling}. The learning rate scheduler is employed at epochs $15$ and $25$ with a decay factor of $0.1$. Figures \ref{fig: teacher-fedisic} and \ref{fig: student-fedisic} present analogous performance plots for the Fed-ISIC2019 dataset, using center=0 as the public auxiliary dataset $\mathcal{D}_{aux}$. The learning rate scheduler is employed at epochs $20$ and $40$ with a decay factor of $0.1$. 


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{suppl/teacher-imgnet.png}
    \caption{Performance plot of fine-tuning the teacher model on the public auxiliary dataset $\gD_{aux}$ (Tiny-Imagenet). The plot on the left shows the balanced accuracy, while the plot on the right presents the loss performance (in a logarithmic scale). } 
    \label{fig: teacher-imgnet}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{suppl/student-imgnet.png}
    \caption{Performance plot of the distillation process on the Tiny-Imagenet dataset. The plot on the left depicts the balanced accuracy across both stages, while the right plot shows the loss performance, with the dashed line indicating the beginning of Stage 2 distillation (Eq. \ref{eq: total-loss}).} 
    \label{fig: student-imgnet} 
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{suppl/teacher-fedisic.png}
    \caption{Performance plot of fine-tuning the teacher model on the Fed-ISIC2019 dataset with center=0. The plot on the left shows the balanced accuracy, while the plot on the right presents the loss performance (in a logarithmic scale).}
    \label{fig: teacher-fedisic} 
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{suppl/student-fedisic.png}
    \caption{Performance plot of the distillation process on the Fed-ISIC2019 dataset with center=0. The plot on the left depicts the balanced accuracy across both stages, while the right plot shows the loss performance, with the dashed line indicating the beginning of Stage 2 distillation (Eq. \ref{eq: total-loss}).} 
    \label{fig: student-fedisic}
\end{figure}

\subsection{Approximation Results} 
In this section, we revisit and elaborate on the approximations introduced in the main paper. 

\begin{table*}[h]
    \centering
    \resizebox{0.85\linewidth}{!}{
        \begin{tabular}{c|ccccc}
        \toprule 
        \textbf{Degree} $d$ & \textbf{Memory (GB)} & \textbf{Latency (s)} & \textbf{Attention loss $(\mathcal{L}_a)$} & \textbf{Representation loss $(\mathcal{L}_h)$} & \textbf{One-epoch accuracy} \\ \midrule 
        $\mathbf{2}$ & $16.82$ & $0.41$ & $44.37$ & $915.46$ & $75.17$ \\ 
        $\mathbf{4}$ & $19.68$ & $0.48$ & $38.31$ & $748.62$ & $83.85$ \\ 
        \rowcolor{lightgray} $\mathbf{6}$ & $22.54$ & $0.52$ & $33.92$ & $655.26$ & $84.36$ \\ 
        $\mathbf{8}$ & $25.39$ & $0.58$ & $30.91$ & $603.53$ & $84.47$ \\ 
        $\mathbf{10}$& $28.99$ & $0.62$ & $28.17$ & $573.54$ & $84.16$ \\ 
        $\mathbf{16}$ & $36.83$ & $0.81$ & $20.31$ & $527.95$ & $85.08$ \\ \midrule 
        $\infty$ \textbf{(True)} & $15.36$ & $0.35$ & $13.59$ & $508.64$ & $86.91$ \\ 
        \bottomrule
        
        \end{tabular}
    }
    \caption{\textbf{Softmax approximation results.} Latency is computed per a batch of samples, when batch size is set to $8$. One-epoch accuracy refers to the accuracy we obtain when performing one full pass over the data for only one epoch. This experiment is carried on the Fed-ISIC2019(center=0) dataset. }
    \label{tab: softmax-approximations}
\end{table*}

\paragraph{Softmax.} Given a vector $\vz \in \mathbb{R}^n$ with components $z_i$, the softmax function is defined as: 
\begin{equation*}
    \text{softmax}(z_i) = \dfrac{e^{z_i}}{\sum_{j=1}^n e^{z_j}}
\end{equation*}
where $e^{z_i}$ denotes the exponential function applied to the component $z_i$. The goal is to approximate the softmax function using a polynomial approximation of the exponential function, $P_n(\vz_i)$, and to estimate the maximum deviation in softmax values due to this approximation. 

\noindent The Taylor series provides a polynomial approximation of $e^x$ around $x=0$ up to $d$ terms: 
\begin{equation*}
    e^x \approx P_d(x) = \sum_{k=0}^d \dfrac{x^k}{k!} 
\end{equation*}
The error bound of this approximation is given by the remainder term $R_n(x)$, expressed as: 
\begin{equation*}
    R_d(x) = \dfrac{e^{\xi}}{(d+1)!} x^{d+1} 
\end{equation*}
for some $\xi$ between $0$ and $x$, indicating the error in approximating $e^x$ with $P_d(x)$. 

\noindent \textbf{Softmax approximation results.} Table \ref{tab: softmax-approximations} presents softmax approximations while keeping all other non-linear components fixed. Using Eq. \ref{eq: exp-approx}, we vary the polynomial degree $d$ for the approximation during a single epoch of knowledge distillation (Section \ref{section: how-distillation}, Stage I) and compare the results to the true softmax. The table reports key performance metrics, including attention loss $\mathcal{L}_a$ (Eq. \ref{eq: attn}), representation loss $\mathcal{L}_h$ (Eq. \ref{eq: hidd}), and accuracy. Additionally, it provides efficiency metrics such as the GPU memory usage and latency for processing a batch of samples. Balancing the trade-offs between time, memory requirements, and performance drop, we chose $d=6$ for all experiments. 

\begin{table*}[th]
    \centering
    \resizebox{0.85\linewidth}{!}{
        \begin{tabular}{c|ccccc}
            \toprule 
             \textbf{Degree} $d$ & \textbf{Memory (GB)} & \textbf{Latency (ms)} & \textbf{Attention loss $(\mathcal{L}_a)$} & \textbf{Representation loss $(\mathcal{L}_h)$} & \textbf{One-epoch accuracy} \\ \midrule 
                                   \textbf{2} & $22.42$ & $118.70$ & $61.50$ & $1698.19$ & $57.87$ \\ 
                                   \textbf{4} & $22.44$ & $119.64$ & $57.42$ & $1534.97$ & $76.64$ \\ 
                                   \textbf{6} & $22.45$ & $120.27$ & $45.67$ & $1352.91$ & $81.77$ \\ 
              \rowcolor{lightgray} \textbf{7} & $22.46$ & $120.43$ & $44.11$ & $1334.32$ & $82.96$ \\ 
                                   \textbf{8} & $22.47$ & $120.97$ & $45.10$ & $1352.55$ & $83.23$ \\ 
                                  \textbf{10} & $22.48$ & $121.70$ & $43.23$ & $1332.62$ & $80.61$ \\ 
                                  \textbf{12} & $22.50$ & $122.98$ & $40.40$ & $1302.41$ & $82.55$ \\ 
                                  \textbf{16} & $22.53$ & $124.76$ & $38.86$ & $1292.36$ & $83.04$ \\ \midrule
                     $\infty$ \textbf{(True)} & $22.46$ & $\,\,\,10.57$ & $38.77$ & $1289.71$ & $83.15$ \\ 
             \bottomrule
        \end{tabular}
    }
    \caption{\textbf{Approximation of the reciprocal (inverse function).} Latency is computed per a batch of samples (aggregate over $L$ transformer blocks), when batch size is set to $8$. One-epoch accuracy refers to the accuracy we obtain when performing one full pass over the data for only one epoch. This experiment is carried on the Fed-ISIC2019 (center=0) dataset.} 
    \label{tab: inverse-approximations}
\end{table*}

\noindent \textbf{Inverse.} Since homomorphic encryption supports only addition and multiplication operations, it cannot perform division directly. To make it work in the encryption domain, we approximate the inverse function. The details of this approximation are presented in Eq. \ref{eq: division}, and the corresponding algorithm is outlined in Algorithm \ref{algorithm: inverse}. To ensure that  the value of $x$ falls within the range $(0,2)$, we determine the maximum possible value of $x$ in the plaintext domain and scale it accordingly in the encrypted domain using this maximum value. 

\begin{algorithm}[t] 
    \caption{Inverse algorithm}\label{algorithm: inverse} 
    \textbf{Input:} $0 < x < 2, \text{ degree } d \in \mathbb{N}$ \\ 
    \textbf{Output:} an approximation of $1/x$ (Eq. \ref{eq: division})
    \begin{algorithmic}[1]
        \State $a_0 \gets 2 - x$ 
        \State $b_0 \gets 1 - x$ 
        \For{$n \gets 0$ to $d-1$}
            \State $b_{n+1} \gets b_n^2$ 
            \State $a_{n+1} \gets a_n \cdot (1 + b_{n+1})$
        \EndFor 
        \State \Return $a_d$ 
    \end{algorithmic}
\end{algorithm}

\noindent \textbf{Inverse approximation results.} Table \ref{tab: inverse-approximations} summarizes the results of inverse function approximations, keeping all other non-linear components fixed and utilizing the softmax approximation with a $6$th-degree polynomial. Following Algorithm \ref{algorithm: inverse} and Eq. \ref{eq: division}, we varied the polynomial degree $d$ under the same conditions described above. While the memory and time requirements remain nearly identical across different values of $d$, the focus is on performance metrics. Based on these considerations, we selected $d=7$, as it closely approximates the true inverse in terms of losses and accuracy. 