\section{Related Work}
\label{sec:related}

In a pioneering work on mechanistic interpretability of transformers 
\cite{elhage2021mathematical} investigate in-context
learning effects of 0, 1 and 2-layer attention-only transformers trained via
next token prediction on a text corpus. They point out that "One layer
attention-only transformers are an ensemble of bigram and “skip-trigram”
(sequences of the form "A… B C") models.", but they don't investigate the
formation of this ensemble. Their focus is on induction heads, formed by
2-layer attention-only transformers.

An important current in mechanistic interpretability is studying the circuits formed by small transformers while training on formal tasks. A prime example is \cite{Nanda:2023}, where they teach 1-layer transformers on modular addition. They show that the composite of the 4-head self-attention block and the first layer of the MLP together with the activation function map to trigonometric functions of the input. On the other hand, they do not study how the attention heads cooperate to this end.

Programs written in the Restricted Access Sequence Processing Language (RASP)
\cite{weiss2021thinking} can be implemented by transformer models. This yields
readily interpretable transformer implementations of counting tasks such as
Double Histogram and Shuffle-Dyck. Moreover, it is possible to restrict
transformer models to learn easy to implement programs
\cite{friedman2023learning}. We are more interested in the programs
unrestricted transformer models learn.
%\andris{Jo jo, de azert meg kene nezni
%  az o modszerukkel, hogy szerintuk mit tanul meg, van meg erre ido?} 

Minimal architectural hyperparameters for a single head 1-layer transformer to
learn the Histogram task are studied in \cite{behrens2024counting}. Depending
on the hyperparameter configurations, they hypothesize two possible families
of programs: relation- and inventory-based counting, for which they give
example implementations. However, it is not proven via mechanistic
interpretability that the models actually learn the hypothesized programs. Our
problem statement differs from theirs significantly in that the amount of
counting we perform is limited only by input string length, with vocabulary
size kept to 8 (including [BOS] and [EOS]).

In \cite{wen2024transformers} it is shown both theoretically and experimentally that there are in fact numerous ways a single head 2-layer transformer can learn Dyck languages: the second attention block needs only satisfy a mild condition for this. We corroborate this finding with our in-depth study of separation accuracy of attention heads.

% 1.3 page


% We aim to understand the internal reasoning processes and training dynamics of
% transformer neural networks trained on formal languages. This work fits into
% the emerging literature on \emph{mechanistic interpretability} (see
% e.g. \cite{bereska2024mechanistic} for a review) which tries to reverse
% engineer trained models and provide an algorithmic description of the
% computation. Our work also fits into another emerging trend, which is using
% statistical machine learning to solve formal/mathematical problems.

%AK ez most tiszta ismetles, kikommentaltam:
%AK Without doubt, neural networks have made huge breakthroughs in the past decade
%AK and are today being deployed to aid humans in thousands of tasks. Transformer
%AK based language models are the latest and most successful generation of this
%AK evolution. Despite all this success, however,


% One of the often repeated criticisms of language models is the lack of logical
% consistency and formal rigor (ADD REFS). This weakness is easy to overlook
% when systems are evaluated on natural language and informal tasks, raising
% doubts about the system's trustworthiness and safety (ADD FURTHER REFS).
% Studying transformer performance on more formal tasks alleviates these problems
% because the expected output is well defined and precisely computable, and
% because small mistakes immediately render the final output wrong, hence
% inconsistencies are easier to detect.

%AK ezt meg azert hagyom ki mert nem foglalkozunk vele, bar kedves a szivunknek: 
%AK Second, one can try
%AK to incorporate formal methods and language models into a joint system, using
%AK the first to make the second more reliable, while using the second to make the
%AK first more flexible.

% Our primary interest is in understanding when and how the model can achieve
% perfect performance, a question often investigated on formal language tasks
% (for recent summaries see \cite{Strobl:2023,Strobl:2024} ADD REFS). Here we go
% beyond the issue of whether a model can \emph{represent} a solution, and ask
% how models can consistently \emph{learn} it. Unsurprisingly, there is a
% significant size gap between the two. We provide a full interpretation of what
% the model has learned. We pay particular attention to the performance of
% individual attention heads as well as their interaction. One interesting
% pattern that emerges from our experiments is that parallel attention heads
% largely learn independent from each other and some of them solve the entire
% problem very well. However, the model does not learn to exploit some of these
% powerful heads, rather it resorts to a mixture of many heads when computing
% the final output.

%AK nekunk nem kell Nanda:2023-at itt kivonatolni, ICLR 2023-as cikk, 300+ hivatkozassal,
%AK The mechanistic interpretability methodological steps of \cite{Nanda:2023} we follow are implemented like this in their paper:
%\begin{compactenum}
%  \item They train models on the task of modular addition.
%%  \item Using statistical methods, they arrive at the hypothesis that the model solves the task by mapping the data to a Fourier subspace on a few key frequencies and using trigonometric identities.
%  \item This hypothesis is then supported by interventions, where they replace model weights by hardcoded ones that are informed by the claimed algorithm and show that performance does not get much worse or even improves.
%  \item The hypothesis is also supported by progress measures: alternative loss functions that seem to indicate the three phases of grokking: memorization, circuit formation and cleanup.
%\end{compactenum}

% \cite{Nanda:2023} treat the composite of the multi-head attention block and
% the first affine map in the feedforward layer as a single function. Here we
% zoom in on the inner workings of the multi-head attention block.


%% \subsection{Works related to analysing the trained model}

%% \subsection{Works related to analysing learning dynamics}

%% \subsection{Works related to learning mathematical problems}