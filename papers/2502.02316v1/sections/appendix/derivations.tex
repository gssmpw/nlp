\section{Derivations} \label{APDX:DERIVATIONS}
\textbf{Lower-Bound Derivation.} $\Ent(\pi_0(\ac_0|\st)) \geq  \blb(\ac^{0},\st)$


\begin{align}
    \Ent(\pi_0(\ac_0|\st))&=-\E_{\bpi_{0:N}}\left[\log \frac{\bpi_{0:N}(\ac^{0:N}|\st)}{\bpi_{1:N|0}(\ac^{1:N}|\st,\ac^0)}\right] \\
    &=-\E_{\bpi_{0:N}}\left[\log \frac{\bpi_{0:N}(\ac^{0:N}|\st)\fpi_{1:N|0}(\ac^{1:N}|\st,\ac^0)}{\bpi_{1:N|0}(\ac^{1:N}|\st,\ac^0)\fpi_{1:N|0}(\ac^{1:N}|\st,\ac^0)}\right] \nonumber \\
    &= \E_{\bpi_{0:N}}\left[\log \frac{\fpi_{1:N|0}(\ac^{1:N}|\st,\ac^0)}{\bpi_{0:N}(\ac^{0:N}|\st)}\right] + \E_{\bpi_{0:N}}\left[\log\frac{\bpi_{1:N|0}(\ac^{1:N}|\st,\ac^0)}{\fpi_{1:N|0}(\ac^{1:N}|\st,\ac^0)}\right] \\
    &= \E_{\bpi_{0:N}}\left[\log \frac{\fpi_{1:N|0}(\ac^{1:N}|\st,\ac^0)}{\bpi_{0:N}(\ac^{0:N}|\st)}\right] + \E_{\pi_{0}}\left[\KL\left(  \bpi_{1:N|0}(\ac^{1:N}|\st,\ac^0)  \|\ \fpi_{1:N|0}(\ac^{1:N}|\st,\ac^0)\right)\right] \\
    & \geq \E_{\bpi_{0:N}}\left[\log \frac{\fpi_{1:N|0}(\ac^{1:N}|\st,\ac^0)}{\bpi_{0:N}(\ac^{0:N}|\st)}\right] ,
\end{align}
where we have used the relation
\begin{equation}
    \pi_0(\ac_0|\st) = \frac{\bpi_{0:N}(\ac^{0:N}|\st)}{\bpi_{1:N}(\ac^{1:N|0}|\st,\ac^0)}
\end{equation}
and the fact that the KL divergence is always non-negative


\textbf{Approximate Inference Formulation.} Recall the definition of the Q-function 
\begin{align}
    Q^{\bpi}(\st_t,\ac^0_t) = r_t + \sum_{l=1}\gamma^l \E_{\rho_{\pi}}\left[r_{t+l}+ \blb(\ac^{0}_{t+l},\st_{t+l})\right].
\end{align}
and
\begin{equation}
    \blb(\ac^{0},\st) = \E_{\bpi_{0:N}}\left[\log \frac{\fpi_{1:N|0}(\ac^{1:N}|\ac^0,\st)}{\bpi_{0:N}(\ac^{0:N}|\st)}\right].
\end{equation}
We start reformulating the objective 
\begin{align}
    J(\bpi) \geq \bar{J}(\bpi) &= \sum_{t=l}^{\infty} \gamma^{t-l}\E_{\rho_\pi}\left[r_t + \blb(\ac^{0}_t,\st_t)\right]. \\
    &=\sum_{t=l+1}^{\infty} \gamma^{t-l}\E_{\rho_\pi}\left[r_t +\blb(\ac^{0}_t,\st_t)\right] + \E_{\rho^\pi}\left[r_l + \blb(\ac^{0}_l,\st_l)\right] \\
    &=\E_{\rho^\pi}\left[Q^{\bpi}(\st_t,\ac^0_t)\right] + \E_{\rho^\pi}\left[\blb(\ac^{0}_l,\st_l)\right] \\
    &=\E_{\rho^\pi}\left[Q^{\bpi}(\st_t,\ac^0_t) + \blb(\ac^{0}_l,\st_l)\right]\\
    &=\E_{\rho^\pi,\bpi_{0:N}}\left[Q^{\bpi}(\st_t,\ac^0_t) + \log \frac{\fpi_{1:N|0}(\ac^{1:N}|\ac^0,\st)}{\bpi_{0:N}(\ac^{0:N}|\st)}\right] \\
    &=-\E_{\rho^\pi}\left[\KL\left(  \bpi(\ac^{0:N}|\st)  \|\ \fpi(\ac^{0:N}|\st)\right) - \log \Z^{\bpi}(\st)\right],
\end{align}
where we used 
\begin{equation}
    \fpi_{0}(\ac^{0}|\st) = \frac{\exp Q^{\bpi}(\st, \ac^0)}{\Z^{\bpi}(\st)}
\end{equation}
in the last step. When minimizing, the negative sign in front of the KL vanishes. Please note that the expectation over the marginal state distribution was ommited in the main text to avoid cluttered notation.


\section{Proofs} \label{APDX:Proofs}

\textbf{Proof of \cref{prop: policy evaluation} (Policy Evaluation).} Let's define the entropy-augmented reward of a diffusion policy as 
\begin{equation}
    r_{\bpi}(\st_t,\ac^0_t) \triangleq r_t(\st_t,\ac^0_t) + \E_{\bpi_{0:N}}\left[\log \frac{\fpi_{1:N|0}(\ac^{1:N}|\ac^0,\st)}{\bpi_{0:N}(\ac^{0:N}|\st)}\right]
\end{equation}
and the update rule for the Q-function as 
\begin{equation}
    Q(\st_t,\ac^0_t) \leftarrow r_{\bpi}(\st_t,\ac^0_t) + \gamma \E_{\st_{t+1}\sim p,\ac_{t+1}^0\sim\bpi}\left[Q(\st_{t+1}, \ac_{t+1}^0)\right].
\end{equation}
This formulation allows us to apply the standard convergence results for policy evaluation as stated in \cite{sutton1999reinforcement}.


\textbf{Proof of \cref{prop: policy improvement} (Policy Improvement).} 
It holds that 
\begin{equation}
    \bpi^{(i+1)}(\ac^{0:N}|s) = \frac{\exp Q^{\pi^{(i)}}(\st,\ac^N)}{Z^{\pi^{(i)}}(s)} \fpi^{(i)}(\ac^{0:N-1}|\ac^N,\st)
\end{equation}
Moreover, using the fact that the KL divergence is always non-negative, we obtain
\begin{equation}
    0 = \KL\left(\bpi^{(i+1)}(\ac^{0:N}|s)\|\bpi^{(i+1)}(\ac^{0:N}|s)\right) \leq \KL\left(\bpi^{(i)}(\ac^{0:N}|s)\|\bpi^{(i+1)}(\ac^{0:N}|s)\right)
\end{equation}
Rewriting the KL divergences yields 
\begin{align}
    & 
    \E_{\bpi^{(i+1)}}\left[\log \frac{\bpi^{(i+1)}(\ac^{0:N}|s)}{\bpi^{(i+1)}(\ac^{0:N}|s)}\right] \leq \E_{\bpi^{(i)}}\left[\log \frac{\bpi^{(i)}(\ac^{0:N}|s)}{\bpi^{(i+1)}(\ac^{0:N}|s)}\right]
    \\ \iff \quad &
    \E_{\bpi^{(i+1)}}\left[\log {\bpi^{(i+1)}(\ac^{0:N}|s)}\right]- \E_{\bpi^{(i+1)}}\left[\log {\bpi^{(i+1)}(\ac^{0:N}|s)}\right] 
    \\ & \leq  \nonumber
    \E_{\bpi^{(i)}}\left[\log {\bpi^{(i)}(\ac^{0:N}|s)}\right]- \E_{\bpi^{(i)}}\left[\log {\bpi^{(i+1)}(\ac^{0:N}|s)}\right]
    \\ \iff \quad &
    \E_{\bpi^{(i+1)}}\left[\log {\bpi^{(i+1)}(\ac^{0:N}|s)}\right]- \E_{\bpi^{(i+1)}}\left[\log {\frac{\exp Q^{\pi^{(i)}}(\st,\ac^N)}{Z^{\pi^{(i)}}(\st)} \fpi^{(i)}(\ac^{0:N-1}|\ac^N,\st)}\right] 
    \\ & \leq  \nonumber
    \E_{\bpi^{(i)}}\left[\log {\bpi^{(i)}(\ac^{0:N}|s)}\right]- \E_{\bpi^{(i)}}\left[\log {\frac{\exp Q^{\pi^{(i)}}(\st,\ac^N)}{Z^{\pi^{(i)}}(\st)} \fpi^{(i)}(\ac^{0:N-1}|\ac^N,\st)}\right]
    \\ \iff \quad &
    \E_{\bpi^{(i+1)}}\left[Q^{\pi^{(i)}}(\st,\ac^N)\right]+
    \E_{\bpi^{(i+1)}}\left[\log {\frac{\fpi^{(i)}(\ac^{0:N-1}|\ac^N,\st)}{\bpi^{(i+1)}(\ac^{0:N}|s)}}\right] 
    \\ & \geq \nonumber
    \E_{\bpi^{(i)}}\left[ Q^{\pi^{(i)}}(\st,\ac^N)\right]+
    \E_{\bpi^{(i)}}\left[\log {\frac{\fpi^{(i)}(\ac^{0:N-1}|\ac^N,\st)}{\bpi^{(i)}(\ac^{0:N}|s)}}\right] 
\end{align}
To keep the notation uncluttered we use
\begin{equation}
    d^{(i+1)}(\st,\ac^N) =  \E_{\bpi^{(i+1)}}\left[\log {\frac{\fpi^{(i)}(\ac^{0:N-1}|\ac^N,\st)}{\bpi^{(i+1)}(\ac^{0:N}|s)}}\right] \quad \text{and} \quad d^{(i)}(\st,\ac^N) =  \E_{\bpi^{(i)}}\left[\log {\frac{\fpi^{(i)}(\ac^{0:N-1}|\ac^N,\st)}{\bpi^{(i)}(\ac^{0:N}|s)}}\right]
\end{equation}

\begin{align}
    Q^{\pi^{(i)}}(\st,\ac^N) & = r_0 + \E\left[ \gamma \left(d^{(i)}(\st_1,\ac^N_1) + \E_{\bpi^{(i)}}\left[ Q^{\pi^{(i)}}(\st_1,\ac^N_1)\right]\right)\right]
    \\ & \leq  
    r_0 +\E\left[ \gamma \left(d^{(i+1)}(\st_1,\ac^N_1) + \E_{\bpi^{(i+1)}}\left[Q^{\pi^{(i)}}(\st_1,\ac^N_1)\right]\right)\right]
    \\ & =
    r_0 +\E\left[ \gamma \left(d^{(i+1)}(\st_1,\ac^N_1) + r_1\right) + \gamma^2 \left(d^{(i)}(\st_2,\ac^N_2) + \E_{\bpi^{(i)}}\left[Q^{\pi^{(i)}}(\st_2,\ac^N_2)\right]\right)\right]
    \\ & \leq  
    r_0 +\E\left[ \gamma \left(d^{(i+1)}(\st_1,\ac^N_1) + r_1\right) + \gamma^2 \left(d^{(i+1)}(\st_2,\ac^N_2) + \E_{\bpi^{(i+1)}}\left[ Q^{\pi^{(i)}}(\st_2,\ac^N_2)\right]\right)\right]
    \\ & \vdots  
    \\ & \leq
    r_0 +\E\left[\sum_{t=1}^\infty \gamma^t \left(d^{(i+1)}(\st_t,\ac^N_t) + r_t\right)\right] = Q^{\pi^{(i+1)}}(\st,\ac^N)
\end{align}

Since $Q$ improves monotonically, we eventually reach a fixed point $Q^{(i+1)} = Q^{(i)} = Q^*$


\textbf{Proof of \cref{prop: policy iteration} (Policy Iteration).} From \cref{prop: policy improvement} it follows that $Q^{\bpi^{i+1}}(\st,\ac)\geq Q^{\bpi^i}(\st,\ac)$. If for $\lim_{k\rightarrow\infty} \bpi^k = \bpi^*$, then it must hold that $Q^{\bpi^*(\st,\ac)}\geq Q^{\bpi}(\st,\ac)$ for all $\bpi\in \cev{\Pi}$ which is guaranteed by \cref{prop: policy improvement}.

