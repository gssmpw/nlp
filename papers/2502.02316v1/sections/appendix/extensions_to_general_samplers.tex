\section{General Diffusion Policies}\label{appdx:gbs}
DIME's maximum entropy reinforcement learning framework for training diffusion policies is not specifically restricted to denoising diffusion policies but can be extended to general diffusion policies. This can be realized using the General Bridges framework as presented in \cite{richterimproved}.
In this case, we can write the forward and backward process as 
\begin{align}
    \dd \ac_t = \left[f(\ac_t, t) + \beta u(\ac_t,\st, t)\right]\dd t + \sqrt{2\beta_t}\dd B_t, \quad \quad a_0 \sim \fpi_0(\cdot|\st), \\
    \dd \ac_t = \left[f(\ac_t, t) - \beta v(\ac_t,\st, t)\right]\dd t + \sqrt{2\beta_t}\dd B_t, \quad\quad \ac_T \sim \mathcal{N}(0,I),
\end{align}
with the drift and control functions $f, u, v: \R^d \times [0,T] \rightarrow \R^d$, the diffusion coefficient $\beta: [0,T] \rightarrow \R^+$, standard Brownian motion $(B_t)_{t\in[0,T]}$ and some target policy $\fpi_0$. Again we denote the marginal density of the forward process as $\fpi_t$ and the conditional density at time $t$ given $l$ as $\fpi_{t|l}$ for $t,l\in[0,T]$.
The backward process starts from $\bpi_T=\fpi_T\sim\mathcal{N}(0,I)$ and runs backward in time where we denote its density as $\bpi$.

The respective discretization using the Euler Maruyama (EM) \cite{sarkka2019applied} method are given by 
\begin{align}
    \ac^{n+1} = \ac^n +\left[f(\ac^n, n) + \beta u(\ac^n, \st, n)\right]\delta +\epsilon_n, \\
    \ac^{n-1} = \ac^n - \left[f(\ac^n, n) - \beta v(\ac^n,\st, n)\right]\delta + \xi_n,
\end{align}
where $\epsilon_n,\xi_n\sim\mathcal{N}(0,2\beta\delta I)$, with the constant discretization step size $\delta$ such that $N=T/\delta$ is an integer. We have used the simplified notation where we write $\ac^n$ instead of $\ac^{n\delta}$. The discretizations admit the joint distributions

\begin{align}
\fpi_{0:N}(\ac^{0:N}|\st) = \pi_0(\ac^0|s) \prod_{n=0}^{N-1}\fpi_{n+1|n}(\ac^{n+1} \big| \ac^{n},\st), \\
\bpi_{0:N}(\ac^{0:N}|\st) = \bpi_N (\ac^N|s) \prod_{n=1}^{N}\bpi_{n-1|n}(\ac^{n-1} \big| \ac^{n},\st),
\end{align}
with Gaussian kernels
\begin{align}
    \fpi_{n+1|n}(\ac^{n+1} \big| \ac^{n},\st) = \mathcal{N}(\ac^{n+1}| \ac^n +\left[f(\ac^n, n) + \beta u(\ac^n, \st, n)\right]\delta, 2\beta\delta I)\\
    \bpi_{n-1|n}( \ac^{n-1} \big| \ac^{n},\st) = \mathcal{N}(\ac^{n-1}|\ac^{n} - \left[f(\ac^{n}, n) - \beta v(\ac^{n}, \st, n)\right]\delta, 2\beta\delta I)
\end{align}


Following the same framework presented in the main text, we can now optimize the controls $u$ and $v$ using the same objective
\begin{equation}
\bar{\mathcal{L}}(u,v) = D_{\text{KL}}\left(\bpi_{0:N}(\ac^{0:N}|\st)|\fpi_{0:N}(\ac^{0:N}|\st)\right),
\end{equation}

where the target policy at time step $n=0$ is given as 
\begin{equation}
    \pi_{0}(\ac^{0}|\st) = \frac{\exp Q^{\bpi}(\st, \ac^0)}{\Z^{\bpi}(\st)}.
\end{equation}

In practice, we optimize the control functions $u$ and $v$ using parameterized neural networks. We have run preliminary results using the general bridge framework within the maximum entropy objective as suggested in our work. The learning curves can be seen in Fig. \ref{fig::appendix::prel::dbs}.



\begin{figure*}[t!]
    \centering
    \begin{minipage}[b]{0.33\textwidth}
        \centering
       \resizebox{1\textwidth}{!}{\input{figures/appendix/gbs/dog_run}}
       \subcaption[]{DIME and GB on Dog Run}
       \label{fig::appendix_dog_rund_dbs}
    \end{minipage}\hfill
    \begin{minipage}[b]{0.33\textwidth}
        \centering
       \resizebox{1\textwidth}{!}{\input{figures/appendix/gbs/humanoid_run}}
       \subcaption[]{DIME and GB on Humanoid Run}
       \label{fig::appendix_hum_rund_dbs}
    \end{minipage}\hfill
    \begin{minipage}[b]{0.33\textwidth}
        \centering
       \resizebox{1\textwidth}{!}{\input{figures/dmc_task_returns/humanoid_run_bro_long}}
       \subcaption[]{DIME and BRO on Humanoid Run}
       \label{fig::appendix_dime_bro_humanoid_run_long}
    \end{minipage}\hfill
    \caption{\textbf{Preliminary results for the GB sampler on the dog run (a) and humanoid run (b) environments from DMC. 
    Comparison to BRO on the humanoid run for 3 million steps. 
    }
    }
    \label{fig::appendix::prel::dbs}
\end{figure*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t!]
        \centering
        \begin{minipage}[t!]{\textwidth}
            \centering
            \begin{minipage}[t!]{0.32\textwidth}
            \includegraphics[width=\textwidth]{figures/illustrations/small_entropy.pdf}
            \subcaption[]{$\ \alpha < 1$}
            \end{minipage}
            \begin{minipage}[t!]{0.32\textwidth}
            \includegraphics[width=\textwidth]{figures/illustrations/normal_entropy.pdf}
            \subcaption[]{$\ \alpha = 1$}
            \end{minipage}
            \begin{minipage}[t!]{0.32\textwidth}
            \includegraphics[width=\textwidth]{figures/illustrations/big_entropy.pdf}
            \subcaption[]{$\ \alpha > 1$}
            \end{minipage}
        \end{minipage}
        \caption[ ]
        {\textbf{The effect of the reward scaling parameter $\alpha$}. The figures in (a)-(b) show diffusion processes for different $\alpha$ values starting at a prior distribution $\mathcal{N}(0,I)$ and going backward in time to approximate the target distribution $\exp{\left(Q^\pi/\alpha\right)}/Z^\pi$. Small values for $\alpha$ (a) lead to concentrated target distributions with less noise in the diffusion trajectories especially at the last time steps. The higher $\alpha$ becomes (b) and (c), the more the target distribution is smoothed and the distribution of the samples at the last time steps becomes more noisy. Therefore, the parameter $\alpha$ directly controls the exploration by enforcing noisier samples the higher $\alpha$ becomes.}
        \label{fig:entropies}
    \end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

