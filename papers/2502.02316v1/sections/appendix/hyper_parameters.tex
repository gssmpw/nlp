\section{Implementation Details}\label{appdx:implementation_details}
We consider a score network with \textit{3} layers and a \textit{256} dimensional hidden layer with gelu activation function. We use Fourier features to encode the timestep and scale the embedding using a feed-forward neural network with two layers with a hidden dimension of 256. For the diffusion coefficient we use a cosine schedule and additionally optimize a scaling parameter for the diffusion coefficient per dimension end-to-end (i.e. we learn the parameter $\beta$).

We employ distributional Q following \cite{bellemare2017distributional}, where the Q-model outputs probabilities $q$ over $b$ bins. Using the bellman backup operator for diffusion models from Eq. \ref{eq: diffusion bellman operator} and the bin values $b$ we follow \cite{bellemare2017distributional} and calculate the target probabilities $q_{target}$. Using the entropy-regularized cross-entropy loss $\mathcal{L}(\phi) = -\sum q_{target}\log q_\phi -0.005\sum q_\phi\log q_\phi$ we update the parameters $\phi$ of the Q-function. Please note that the entropy regularization was not proposed in the original paper from \cite{bellemare2017distributional}, however, we noticed that a small regularization helps improve the performance in the early learning stages but does not change the asymptotic performance. Additionally, we follow \cite{nauman2024bigger} and use the \textit{mean} of the two Q-values instead of the \textit{min} as it has usually been used in RL so far.

The expected Q-values for updating the actor can be easily calculated using the expectation $Q(\st,\ac_t^0) = \sum_i q_i(\st_t,\ac_t^0) b_i$

\textbf{Action Scaling.} Practical applications have a bounded action space that can usually be scaled to a fixed range. However, the action range of the diffusion policy $\bpi$ is unbounded. Therefore, we follow recent works \cite{haarnoja2018soft} and propose applying the change of variables with a \textit{tanh} squashing function at the last diffusion step $n=0$. For the backward process $\bq_{0:N}(u^{0:N}|\st)$ with unbounded action space $u \in \R^D$ we can squash the action $\ac^0=\tanh{u^0}$ such that $\ac^0 \in (-1,1)$ and its density is given by 
\begin{equation}
    \bpi_{0:N}(\ac^{0:N}|\st) = \bq_{0:N}(u^{0:N}|\st)  \det\Biggl|\left(\frac{\text{d}\ac^0}{\text{d}u^0}\right) \Biggr|^{-1},
\end{equation}
with the corresponding log-likelihood
\begin{equation}
    \log \bpi_{0:N}(a^{0:N}|\st) = \log \bq_N(u^{N}) + \sum_{n=1}^N \log \bq_{n-1}(u^{n-1}|u^n,\st) - \sum_{i=1}^D\log\left(1-\tanh^2\left({u^N_i}\right)\right).
\end{equation}
This means that the Gaussian kernels of the diffusion chain have the same log probabilities except for the correction term of the last step at $n=0$ 


%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H] 
	\caption{DIME: Diffusion-Based Maximum Entropy Reinforcement Learning}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Input:}}
		\renewcommand{\algorithmicensure}{\textbf{Output:}}
		\REQUIRE Initialized parameters $\theta, \phi, \alpha$, learningrates $\lambda$
		% \ENSURE a
		\FOR{$k = 1$ to M}
        \IF{k \% UTD}
		      \STATE $\ac_t^{0:T} \sim \ppi_{0:N}(\ac^{0:N}|\st_t)$
            \STATE $\st_{t+1} \sim p(\st_{t+1}|\ac_t^0,\st_t)$
            \STATE $\mathcal{D} \leftarrow \mathcal{D} \bigcup \{\st_t,\ac_t^0,r_t,\st_{t+1} \}$
        \ENDIF
            \STATE $\phi \leftarrow \phi - \lambda_\phi\nabla_{\phi}J_Q(\phi)$ (Eq. \ref{eq::Bellman_Residual})
            \IF{k \% POLICYDELAY}
                \STATE $\theta \leftarrow \theta - \lambda_\theta \nabla_{\theta}\mathcal{L}(\theta)$ (Eq. \ref{eq: joint control as inference2})
                \STATE $\alpha \leftarrow \alpha -\lambda_\alpha J(\alpha) $ (Eq. \ref{eq:auto_tuning_alpha})
            \ENDIF
		\ENDFOR
	\end{algorithmic}
	\label{algo_training}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
Algorithm \ref{algo_training} shows the learning procedure of DIME. Note that policy delay refers to the number of delayed updates of the policy compared to the critic. UTD is the update to data ratio.

\section{List of Hyperparameters}


\begin{table}[h!]
    \centering
    % Adjust the number of 'c' based on how many columns you need.
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l c c c c c c c c c}
        \toprule
        \textbf{} 
        & \textbf{DIME} 
        & \textbf{BRO} 
        & \textbf{BRO Fast} 
        & \textbf{CrossQ}
        & \textbf{QSM}
        & \textbf{Diff-QL}
        & \textbf{Consistency-AC}
        & \textbf{DIPO}\\
        \midrule
        Polyak weight & N/A & 0.005  & 0.005 & N/A & 0.005 & N/A & N/A & N/A  \\
        Update-to-data ratio & 2 & 10 & 2 & 2 & 1 & 1 & 1 &  1 \\
        Discount & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 \\
        batch size & 256 & 128 & 128 & 256 & 256 & 256 & 256 & 256 \\
        Buffer size & 1e6 & 1e6 & 1e6 & 1e6 & 1e6 & 1e5 & 1e5 & 1e6  \\
        $\mathcal{H}_{target}$ & 4$\text{dim}(\mathcal{A})$ & $\text{dim}(\mathcal{A})$ & $\text{dim}(\mathcal{A})/2$ & $\text{dim}(\mathcal{A})$ & N/A & N/A & N/A &  N/A \\
        Critic hidden depth & 2 & BRONET & BRONET & 2 & 2 & 3 & 3 &  3 \\
        Critic hidden size & 2048 & 512 & 512 & 2048 & 2048 & 256 & 256 & 256  \\
        Actor/Score depth & 3 & BRONET & BRONET & 3 & 3 & 4 & 4 &  4 \\
        Actor/Score size & 256 & 256 & 256 & 256 & 256 & 256 & 256 & 256  \\
        Num. Bins/Quantiles & 100 & 100 & 100 & N/A & N/A & N/A & N/A & N/A  \\
        Temp. Learn. Rate & 1e-3 & 3e-4 & 3e-4 & 3e-4 & N/A & N/A & N/A & N/A  \\
        Learn. Rate Critic & 3e-4 & 3e-4 & 3e-4 & 7e-4 & 3e-4 & 3e-4 & 3e-4 & 3e-4  \\
        Learn. Rate Actor/Score & 3e-4 & 3e-4 & 3e-4 & 7e-4 & 3e-4 & 1e-5 & 1e-5 &  3e-4 \\
        Optimizer & Adam & AdamW & AdamW & Adam & Adam & Adam & Adam & Adam  \\
        Diffusion Steps & 16 & N/A & N/A & N/A & 15 & 5 & N/A &  100 \\
        Prior Distr. & $\mathcal{N}(0,2.5)$ & N/A & N/A & N/A & $\mathcal{N}(0,1)$ & N/A & N/A & N/A \\
        Exploration Steps & 5000 & 2500 & 2500 & 5000 & 1e4 & 1e4 & 1e4 &  1e4 \\
        Score-Q align. factor & N/A& N/A & N/A & N/A & 50 & N/A & N/A& N/A \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Hyperparameters of all algorithms for all benchmark suits. Varying hyperparameters for different benchmark suits are described in the text.}
    \label{tab:hyperparameters}
\end{table}
\textbf{DIME.} 
For DIME we use distributional Q where the maximum values for the bins have been chosen per benchmark suite. We have used $v_{min}=-1600$ and $v_{max}=1600$ for the gym environments, $v_{min}=-200$ and $v_{max}=200$ for the DMC suite and $v_{min}=-3600$ and $v_{max}=3600$ for the myo suite.

\textbf{QSM}. In certain environments, we observed that QSM with default hyperparameters performed poorly, particularly in several DMC tasks and the Gym Ant-v3 tasks. To address this, we fine-tuned the hyperparameters for QSM in each of these underperforming tasks. For the DMC tasks, we found that QSM often requires an $\alpha$ value—representing the alignment factor between the score and the Q-function \cite{psenkalearning}—in the range of 100-200, rather than the default value of 50 reported in QSM's original implementation. In the Ant-v3 task, we determined that $\alpha$ needs to be set to 1. In the original implementation, the number of diffusion steps is set to be 5, however, we found using more steps, such as 10 and 15, can significantly improve the performance in these under performed tasks. 

\textbf{CrossQ.} We used the hyperparameters from the original paper \cite{bhattcrossq} for the gym benchmark suite. However, we used a different set of hyperparameters for the DMC and MYO suites for improved performance. More precisely, we increased the policy size to \textit{3 layers} with \textit{256 hidden size}. Additionally, we reduced the learning rate to \textit{7e-4}.

% \begin{table}[h!]
%     \centering
%     % Adjust the number of 'c' based on how many columns you need.
%     \begin{tabular}{l c c c c c c c c c}
%         \toprule
%         \textbf{} 
%         & \textbf{CrossQ}
%         & \textbf{QSM}
%         & \textbf{Diff-QL}
%         & \textbf{Consistency-AC}
%         & \textbf{DIPO}\\
%         \midrule
%         Polyak weight &N/A & 0.005 &  &  &   \\
%         Update-to-data ratio  & 1 & 1 &  &  &   \\
%         Discount  & 0.99 & 0.99 &  &  &   \\
%         batch size  & 256 & 256 &  &  &   \\
%         Buffer size  & 1e6 & 1e6 &  &  &   \\
%         Target Entr.  & $|\mathcal{A}|$ & N/A &  &  &   \\
%         Critic hidden depth  & 2 & 2 &  &  &   \\
%         Critic hidden size & 2048 & 512 &  &  &   \\
%         Actor/Score depth  & 2 & 2 &  &  &   \\
%         Actor/Score size & 256 & 512 &  &  &   \\
%         Num. Bins/Quantiles & N/A & N/A &  &  &   \\
%         Temp. Learn. Rate & 3e-4 & N/A &  &  &   \\
%         Learn. Rate Critic & 1e-3 & 3e-4 &  &  &   \\
%         Learn. Rate Actor/Score & 1e-3 & 3e-4 &  &  &   \\
%         Optimizer & Adam & Adam &  &  &   \\
%         Diffusion Steps & N/A & 5 &  &  &   \\
%         Prior Distr. & N/A & $\mathcal{N}(0,1)$ &  &  &   \\
%         Exploration Steps  & 5000 & 1e4 &  &  &   \\
%         Score-Q align. factor & N/A & N/A & N/A & N/A & N/A & N/A \\
%         \bottomrule
%     \end{tabular}
%     \caption{Hyperparameters for the GYM benchmark. If no entry is provided for a method, the hyperparameters from Table \ref{tab:hyperparameters} are used.  }
%     \label{tab:hyperparameters_gym}
% \end{table}

% \begin{table}[h!]
%     \centering
%     % Adjust the number of 'c' based on how many columns you need.
%     \begin{tabular}{l c c c c c c c c c}
%         \toprule
%         \textbf{} 
%         & \textbf{QSM}
%         & \textbf{Diff-QL}
%         & \textbf{Consistency-AC}
%         & \textbf{DIPO}\\
%         \midrule
%         Polyak weight & 0.005 &  &  &   \\
%         Update-to-data ratio  & 1 &  &  &   \\
%         Discount  & 0.99 &  &  &   \\
%         batch size  & 256 &  &  &   \\
%         Buffer size   & 1e6 &  &  &   \\
%         Target Entr.  & N/A &  &  &   \\
%         Critic hidden depth  & 2 &  &  &   \\
%         Critic hidden size  & 2048 &  &  &   \\
%         Actor/Score depth   & 3 &  &  &   \\
%         Actor/Score size  & 256 &  &  &   \\
%         Num. Bins/Quantiles  & N/A &  &  &   \\
%         Temp. Learn. Rate  & N/A &  &  &   \\
%         Learn. Rate Critic  & 3e-4 &  &  &   \\
%         Learn. Rate Actor/Score  & 3e-4 &  &  &   \\
%         Optimizer  & Adam &  &  &   \\
%         Diffusion Steps  & 15 &  &  &   \\
%         Prior Distr.  & $\mathcal{N}(0,1)$ &  &  &   \\
%         Exploration Steps   & 1e4 &  &  &   \\
%         Score-Q align. factor & 1 & N/A & N/A & N/A \\
%         \bottomrule
%     \end{tabular}
%     \caption{Hyperparameters for the MYOSUITE benchmark. If no entry is provided for a method, the hyperparameters from Table \ref{tab:hyperparameters} are used.  }
%     \label{tab:hyperparameters_myosuite}
% \end{table}