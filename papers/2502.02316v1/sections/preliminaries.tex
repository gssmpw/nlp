\section{Preliminaries}
\subsection{Maximum Entropy Reinforcement Learning}
\label{sec: Maximum Entropy Reinforcement Learning}
\textbf{Notation} We consider the task of learning a policy $\pi: \St \times \Ac \rightarrow \R^+$,
where $\St$ and $\Ac$ denote a continuous state and action space, respectively using reinforcement learning (RL). We formalize the RL problem using an infinite horizon Markov decision process consisting of the tuple $(\St,\Ac,r,p,\rho_{\pi}, \gamma)$, with bounded reward function $r: \St \times \Ac \rightarrow [r_{\text{min}}, r_{\text{max}}]$ and transition density $p: \St \times \St \times \Ac \rightarrow \R^+$ which denotes the likelihood for transitioning into a state $\st' \in \St$ when being in $\st \in \St$ and executing an action $\ac \in \Ac$. We follow \cite{haarnoja2018soft} and slightly overload $\rho_{\pi}$ which denotes the state and state-action marginals induced by a policy $\pi$. Moreover, $\gamma \in [0, 1)$ denotes the discount factor.
For brevity, we use $r_t\triangleq r(\st_t,\ac_t)$. Lastly, we denote objective functions that we aim to maximize as $J$ and minimize as $\mathcal{L}$.

\textbf{Control as inference.} The goal of maximum entropy reinforcement learning (MaxEnt-RL) is to jointly maximize the sum of expected rewards and entropies of a policy
\begin{equation}
\label{eq: marginal max ent}
    J(\pi) = \sum_{t=l}^{\infty} \gamma^{t-l}\E_{\rho_\pi}\left[r_t + \alpha \Ent(\pi(\ac_t|\st_t))\right],
\end{equation}
where $\Ent(\pi(\ac|\st)) = - \int \pi(\ac|\st) \log \pi(\ac|\st) \dd \ac$ is the differential entropy, and $\alpha \in \R^+$ controls the exploration exploitation trade-off \cite{haarnoja2017reinforcement}. To keep the notation uncluttered we absorb $ \alpha$ into the reward function via $r \leftarrow r/\alpha$. Defining the $Q$-function of a policy $\pi$ as
\begin{equation}
\label{eq: marginal Q soft}
Q^{\pi}(\st_t,\ac_t) = r_t + \sum_{l=1}^\infty\gamma^{l} \E_{\rho_{\pi}}\left[r_{t+l}+ \mathcal{H}\left(\pi(\ac_{t+l}|\st_{t+l})\right)\right],
\end{equation}
with $Q^{\pi}: \St \times \Ac \rightarrow \R$,
the MaxEnt objective can be cast as an approximate inference problem of the form
\begin{equation}
\label{eq: marginal control as inference}
\mathcal{L}(\pi) = D_{\text{KL}}\left(\pi(\ac_t|\st_t)\Big|\frac{\exp Q^{\pi}(\st_t,\ac_t)}{\Z^{\pi}(\st_t)}\right),
\end{equation}
in a sense that 
$
    \max_{\pi} J(\pi) = \min_{\pi} \mathcal{L}(\pi).
$
Here, $D_{\text{KL}}$ denotes the Kullback-Leibler divergence and 
\begin{equation}
\label{eq: normalizer}
\Z^{\pi}(\st) = \int \exp Q^{\pi}(\st,\ac) \dd \ac
\end{equation}
is the state-dependent normalization constant.

\textbf{Policy iteration} is a two-step iterative update scheme that is, under certain assumptions, guaranteed to converge to the optimal policy with respect to the maximum entropy objective. The two steps include policy evaluation and policy improvement. 
%
Given a policy $\pi$, policy evaluation aims to evaluate the value of $\pi$. To that end, \cite{haarnoja2018soft} showed that repeated application of the Bellman backup operator $\mathcal{T}^{\pi} Q^{k}$ with 
\begin{equation}
    \label{eq: bellman operator}
    \mathcal{T}^{\pi} Q(\st_t,\ac_t) \triangleq r_t + \gamma \E\left[Q(\st_{t+1},\ac_{t+1}) +\mathcal{H}(\ac_{t+1}|\st_{t+1})\right],
\end{equation}
converges to $Q^{\pi}$ as $k \rightarrow \infty$, starting from any $Q$.
%
To update the policy, that is, to perform the policy improvement step, the $Q$-function of the previous evaluation step, $Q^{\pi_{\text{old}}}$ is used to obtain a new policy according to 
\begin{equation}
\label{eq: marginal policy improvement}
\pi_{\text{new}} = \argmax_{\pi \in \Pi} D_{\text{KL}}\left(\pi(\ac_t|\st_t)\Big|\frac{\exp Q^{\pi_{\text{old}}}(\st_t,\ac_t)}{\Z^{\pi_{\text{old}}}(\st_t)}\right),
\end{equation}
where $\Pi$ is a set of policies such as a family of parameterized distributions.
Note that $\Z^{\pi_{\text{old}}}(\st_t)$ is not required for optimization as it is independent of $\pi$. \citet{haarnoja2018soft} showed that for all state-action pairs $(\st, \ac) \in \St \times \Ac$ it holds that $Q^{\pi_{\text{new}}}(\st,\ac) \geq Q^{\pi_{\text{old}}}(\st,\ac)$ ensuring that policy iteration converges to the optimal policy $\pi^*$ in the limit of infinite repetitions of policy evaluation and improvement.

\subsection{Denoising Diffusion Policies}
\label{sec: Denoising Diffusion Policies}
For a given state $\st \in \St$, we consider a stochastic process on the time-interval $[0,T]$ given by an Ornstein-Uhlenbeck (OU) process \footnote{Please note, for clarity, we slightly abuse notation by using $t$ to denote the time in the stochastic process. This should not be confused with the time step in RL. The distinction becomes clear when we discretize the processes.} \cite{sarkka2019applied}
\begin{equation}
\label{eq: noising process}
        \dd \ac_t  = -\beta_t \ac_t \dd t + \eta\sqrt{2\beta_t} \dd B_t, \quad a_0 \sim \fpi_0(\cdot|\st),
\end{equation}
with diffusion coefficient $\beta: [0,T]\rightarrow \R^+$, standard Brownian motion $(B_t)_{t\in[0,T]}$, and some target policy $\fpi_0$. 
For $t,l\in [0,T]$, we denote the marginal density of Eq. $\ref{eq: noising process}$ at $t$ as $\fpi_t$
% , the joint density at $s,t$ $\fpi_{s,t}$ 
and the conditional density at time $t$ given $l$ as $\fpi_{t|l}$.
Eq. \ref{eq: noising process} is commonly referred to as \textit{forward} or \textit{noising process} since, for a suitable choice of $\beta$, it holds that $\fpi_{T} \approx \mathcal{N}(0, \eta^2I)$. Denoising diffusion models leverage the fact, that the time-reversed process of Eq. \ref{eq: noising process} is given by 
\begin{equation}
\label{eq: denoising process}
        \dd \ac_t  = \left(-\beta_t \ac_t \dd t - 2\eta^2\beta_t \nabla \log \fpi_t(\ac_t|\st)\right) + \eta\sqrt{2\beta_t} \dd B_t,
\end{equation}
starting from $\bpi_T = \fpi_{T} \approx \mathcal{N}(0, \eta^2I)$ and running backwards in time \cite{nelson2020dynamical,anderson1982reverse,haussmann1986time}. For the \textit{backward}, \textit{generative} or \textit{denoising process} (Eq. \ref{eq: denoising process}), we denote the density as $\bpi$. Here, time-reversal means that the marginal densities align, i.e., $\fpi_t = \bpi_t$ for all $t\in[0, T]$. Hence, starting from $\ac_T \sim \mathcal{N}(0, \eta^2I)$, one can sample from the target policy $\fpi_0$ by simulating Eq. \ref{eq: denoising process}. However, for most densities $\fpi_0$, the scores $\left(\nabla \log \fpi_t(\ac_t|\st)\right)_{t\in[0,T]}$ are intractable, requiring numerical approximations. To address this, denoising score-matching objectives are commonly employed, that is, 
\begin{equation}
\label{eq: score matching}
\mathcal{L}_{\text{SM}}(\theta) = \E\left[\beta_t\|f^{\theta}_t(\ac_t,\st) - \nabla \log \fpi_{t|0}(\ac_t|\ac_0,\st) \|^2\right],
\end{equation}
where $t$ is sampled on $[0,T]$ and $f^{\theta}$ denotes a parameterized score network \cite{hyvarinen2005estimation,vincent2011connection}. For OU processes, the conditional densities $\nabla \log \fpi_{t|0}$ are explicitly computable, making the objective tractable for optimizing $\theta$ \cite{song2021scorebased}. Once trained, the score network $f^{\theta}$ can be used to simulate the denoising process 
\begin{equation}
\label{eq: approximate denoising process}
        \dd \ac_t  = \left(-\beta_t \ac_t \dd t - 2\eta^2\beta_t f^{\theta}_t(\ac_t,\st)\right) + \eta\sqrt{2\beta_t} \dd B_t,
\end{equation}
to obtain samples $\ac_0 \sim \ppi_0$ that are approximately distributed according to $\fpi_0$. Here, $\ppi_t$ denotes the marginal distribution of Eq. \ref{eq: approximate denoising process} at time $t$.
While score-matching techniques work well in practice, they cannot be applied to maximum entropy reinforcement learning. 
This is because the expectation in Eq. \ref{eq: score matching} requires samples $\ac_0 \sim \fpi_0 \propto \exp Q^{\pi}$ which are not available. However, in the next section, we build on recent advances in approximate inference to optimize diffusion models without requiring samples from $\ac_0$, relying instead on evaluations of  $Q^{\pi}$.

%
%
%
\section{Diffusion-Based Maximum Entropy RL}
\label{sec: Diffusion-Based Maximum Entropy RL}
Here, we explain how diffusion models can be used within a maximum entropy RL framework. To that end, we express the maximum entropy objective as an approximate inference problem for diffusion models. We then use these results to introduce a policy iteration scheme that provably converges to the optimal policy. Lastly, we propose a practical algorithm for optimizing diffusion models.
\subsection{Control as Inference for Diffusion Policies}
Directly maximizing the maximum entropy objective
\begin{equation*}
\label{eq: marginal diffusion max ent}
    J(\bpi) = \sum_{t=l}^{\infty} \gamma^{t-l}\E_{\rho_\pi}\left[r_t(\st_t,\ac^0_t) + \alpha \Ent(\bpi_0(\ac^0_t|\st_t))\right], 
\end{equation*}
for a diffusion model is difficult as the marginal entropy $\Ent(\bpi_0(\ac|\st))$ of the denoising process in Eq. \ref{eq: denoising process} is intractable.
Please note that we use superscripts for the actions to indicate the diffusion step to avoid collisions with the time step used in RL. Moreover, we will again absorb $\alpha$ into the reward and use  $r_t\triangleq r(\st_t,\ac^0_t)$.
To overcome this intractability, we propose to maximize a lower bound. We start by discretizing the stochastic processes introduced in \Cref{sec: Denoising Diffusion Policies} and use the results as a foundation to derive this lower bound. Note that while similar results can be derived from a continuous-time perspective (see e.g., \citet{berneroptimal,richterimproved,nusken2024transport}), such derivation would require a background in stochastic calculus, making it less accessible to a broader audience. %We therefore stay with the simpler, discrete-time formulation. 

The Euler-Maruyama (EM) discretization \cite{sarkka2019applied} of the noising (Eq. \ref{eq: noising process}) and denoising (Eq. \ref{eq: denoising process}) process is given by
\begin{align}
\label{eq: em discretized noising process}
        \ac^{n+1}  & = \ac^{n} -\beta_{n} \ac^{n} \delta + \epsilon_{n} \quad \text{and}
        \\ 
\label{eq: em discretized denoising process}
        \ac^{n-1}  & = \ac^{n} + \left(\beta_{n} \ac^{n} + 2\eta^2\beta_{n} \nabla \log \fpi_n(\ac^{n}|\st) \right)\delta + \xi_{n}, 
\end{align}
respectively, with $\epsilon_n,\xi_n \sim \mathcal{N}(0,2\eta^2\beta_{n}\delta I)$. Here, $\delta$ denotes a constant discretization step size such that $N = T / \delta$ is an integer. To simplify notation, we write $\ac^n$, instead of $\ac^{n\delta}$.  Under the EM discretization, the noising and denoising process admit the following joint distributions
\begin{align}
    \fpi_{0:N}(\ac^{0:N}|\st) &= \fpi_0(\ac^0|s) \prod_{n=0}^{N-1}\fpi_{n+1|n}(\ac^{n+1} \big| \ac^{n},\st), \label{eq: forward joint}\\
    \bpi_{0:N}(\ac^{0:N}|\st) &= \bpi_N (\ac^N|s) \prod_{n=1}^{N}\bpi_{n-1|n}(\ac^{n-1} \big| \ac^{n},\st), \label{eq: parameterized joint}
\end{align}
in a sense that $\fpi_{0:N}$ and $\bpi_{0:N}$ converge to the law of $(\ac_t)_{t\in[0,T]}$ in Eq. \ref{eq: noising process} and \ref{eq: denoising process}, as $\delta \rightarrow 0$, respectively \cite{doucet2022score}. Here, $\fpi_{n+1|n}$ and $\bpi_{n-1|n}$ are Gaussian transition densities that directly follow from Eq. \ref{eq: em discretized noising process} and \ref{eq: em discretized denoising process}.

To obtain a maximum entropy objective for diffusion models, we make use of the following lower bound on the marginal entropy, that is, $\Ent(\bpi_0(\ac_0|\st)) \geq \blb(\ac^{0},\st)$, where
\begin{equation}
\label{eq: entropy lower bound}
    \blb(\ac^{0},\st) =  \E_{\bpi_{0:N}}\left[\log \frac{\fpi_{1:N|0}(\ac^{1:N}|\ac^0,\st)}{\bpi_{0:N}(\ac^{0:N}|\st)}\right].
\end{equation}
Please note that similar bounds have been used, e.g., in \cite{agakov2004auxiliary,tran2015variational,ranganath2016hierarchical,maaloe2016auxiliary,arenz2018efficient}, or, more generally, follow from the data processing inequality \cite{cover1999elements}. 
A derivation can be found in Appendix \ref{APDX:DERIVATIONS}. From Eq. \ref{eq: entropy lower bound}, it directly follows that 
\begin{equation}
\label{eq: marginal max ent}
    J(\bpi) \geq \bar{J}(\bpi) = \sum_{t=l}^{\infty} \gamma^{t-l}\E_{\rho_\pi}\left[r_t + \blb(\ac^{0}_t,\st_t)\right].
\end{equation}
Next, we cast Eq. \ref{eq: marginal max ent} as an approximate inference problem to make the objective more interpretable. To that end, let us define the $Q$-function of a denoising policy $\bpi$ with respect to the maximum entropy objective $\bar J$ as
\begin{equation}
\label{eq: diffusion Q soft}
Q^{\bpi}(\st_t,\ac^0_t) = r_t + \sum_{l=1}\gamma^l \E_{\rho_{\pi}}\left[r_{t+l}+ \blb(\ac^{0}_{t+l},\st_{t+l})\right],
\end{equation}
with $Q^{\bpi}: \St \times \Ac \rightarrow \R$. With Eq. \ref{eq: diffusion Q soft} we identify the corresponding approximate inference problem as finding $\bpi$ which minimizes (please see Appendix \ref{APDX:DERIVATIONS} for derivation)
\begin{equation}
\label{eq: joint control as inference}
\bar{\mathcal{L}}(\bpi) = D_{\text{KL}}\left(\bpi_{0:N}(\ac^{0:N}|\st)|\fpi_{0:N}(\ac^{0:N}|\st)\right),
\end{equation}
where the target policy, i.e., the marginal of the noising process in Eq. \ref{eq: forward joint} is given by the exponentiated $Q$-function of the diffusion policy
\begin{equation}
    \fpi_{0}(\ac^{0}|\st) = \frac{\exp Q^{\bpi}(\st, \ac^0)}{\Z^{\bpi}(\st)}.
\end{equation}
Recall from \Cref{sec: Denoising Diffusion Policies} that we aim to time-reverse the noising process, that is, to ensure for all states $\st \in \St$, it holds that $\bpi_{0:N} = \fpi_{0:N}$. Please note that this is precisely what Eq. \ref{eq: joint control as inference} is trying to accomplish, i.e., we aim to learn a diffusion model $\bpi$, such that the denoising process time-reverses the noising process, and, in particular, has a marginal distribution given by $\pi_{0} = \exp Q^{\bpi}/\Z^{\bpi}$. Lastly, 
from the data processing inequality it directly follows that 
\begin{align}
\label{eq: data processing inequality}
  D_{\text{KL}}\bigg(\bpi_0(\ac^0|\st)&\Big|\frac{\exp Q^{\bpi}(\st,\ac^0)}{\Z^{\bpi}(\st)}\bigg) \nonumber
 \\
 & \leq D_{\text{KL}}\left(\bpi(\ac^{0:N}|\st)|\fpi(\ac^{0:N}|\st)\right),
\end{align}
which shows the approximate inference problem in Eq. \ref{eq: joint control as inference} indeed optimizes the same inference problem stated in Eq. \ref{eq: marginal control as inference}.
Next, we will use these results to develop a policy iteration scheme for diffusion models. 

\subsection{Diffusion-based Policy Iteration}
We propose a policy iteration scheme for learning an optimal maximum entropy policy, similar to \cite{haarnoja2018soft}. However, here we restrict the family of stochastic actors to diffusion policies $\bpi \in \cev{\Pi} \subset \Pi$. Throughout this section, we assume finite action spaces to enable theoretical analysis, but relax this assumption in \cref{dime_practical}. All proofs of this section are deferred to Appendix \ref{APDX:DERIVATIONS}. 
\todo[inline]{Do we need to assume finite action spaces? $\bpi$ is still assumed to be optimal. Should be relax the assumption here? FINITE ACTION SPACE}

For policy evaluation, we aim to compute the value of a policy $\bpi$. We define the Bellman backup operator as
\begin{equation}
    \label{eq: diffusion bellman operator}
    \mathcal{T}^{\bpi} Q(\st_t,\ac^0_t) \triangleq r_t + \gamma \E\left[Q(\st_{t+1},\ac^0_{t+1}) +\blb(\ac^{0}_{t+1},\st_{t+1})\right].
\end{equation}
Note that Eq. \ref{eq: diffusion bellman operator} contains the entropy-lower bound $\blb$. By applying standard convergence results for policy evaluation \cite{sutton1999reinforcement} we can obtain the value of a policy by repeatedly applying $\mathcal{T}^{\bpi}$ as established in \Cref{prop: policy evaluation}.

\begin{proposition}[Policy Evaluation]
\label{prop: policy evaluation}
Let $\mathcal{T}^{\bpi}$ be the Bellman backup operator for a diffusion policy $\bpi$ as defined in Eq. \ref{eq: diffusion bellman operator}. Further, let $Q^0: \St \times \Ac \rightarrow \R$ and $Q^{k+1} = \mathcal{T}^{\bpi}Q^k$.
Then, it holds that $\lim_{k\rightarrow \infty} Q^k =  Q^{\bpi}$ where $Q^{\bpi}$ is the $Q$ value of $\bpi$.
\end{proposition}

For the policy improvement step, we seek to improve the current policy based on its value using the $Q$-function. Formally, we need to solve the approximate inference problem
\begin{equation}
\label{eq: policy improvement objective}
\bpi^{\text{new}} = \argmin_{\bpi \in \cev{\Pi}}D_{\text{KL}}\left(\bpi_{0:N}(\ac^{0:N}|\st)|\fpi^{\text{ old}}_{0:N}(\ac^{0:N}|\st)\right),
\end{equation}
for all $\st\in\St$, where $\fpi^{\text{ old}}_{0:N}(\ac^{0:N}|\st)$ is as in Eq. \ref{eq: forward joint} with marginal density
\begin{equation}
\label{eq: old policy}
    \fpi^{\text{ old}}_{0}(\ac^{0}|\st) = \frac{\exp Q^{\bpi_{\text{old}}}(\st, \ac^0)}{\Z^{\bpi_{\text{old}}}(\st)}.
\end{equation}
Indeed, solving Eq. \ref{eq: policy improvement objective} results in a policy with higher value as established below.
\begin{proposition}[Policy Improvement]
\label{prop: policy improvement}
Let $\bpi_{\text{old}}, \bpi_{\text{new}} \in \cev{\Pi}$ be defined as in Eq. \ref{eq: old policy} and \ref{eq: policy improvement objective}, respectively. Then for all $(\st,\ac) \in \St \times \Ac$ it holds that $Q^{\bpi_{\text{new}}}(\st,\ac) \geq Q^{\bpi_{\text{old}}}(\st,\ac).$ 
\end{proposition}

Combining these results leads to the policy iteration method which alternates between policy evaluation (\Cref{prop: policy evaluation}) and policy improvement (\Cref{prop: policy improvement}) and provably converges to the optimal policy in $\cev{\Pi}$ (\Cref{prop: policy iteration}).

\begin{proposition}[Policy Iteration]
\label{prop: policy iteration}
Let $\bpi^0, \bpi^{i+1}, \bpi^i, \bpi_* \in \cev{\Pi}$. Further, let $\bpi^{i+1}$ be the policy obtained from $\bpi^{i}$ after a policy evaluation and improvement step. Then, for any starting policy $\bpi^0$ it holds that $\lim_{i\rightarrow \infty} \bpi^i =  \bpi^*$, with $\bpi^*$ such that for all $\bpi\in \cev{\Pi}$ and $(\st,\ac) \in \St \times \Ac$ it holds that $Q^{\bpi^*}(\st,\ac) \geq Q^{\bpi}(\st,\ac).$ 
\end{proposition}

However, performing policy iteration until convergence is in practice often intractable, particularly for continuous control tasks. As such, we will introduce a practical algorithm next.


\begin{figure*}[t!]
    \centering
    \resizebox{0.95\textwidth}{!}{
    \input{figures/ablations/varying_alpha/legend}}%
    
    \begin{minipage}[b]{0.26\textwidth}
        \centering
       \resizebox{1\textwidth}{!}{\input{figures/ablations/varying_alpha/dog-run}}
       \subcaption[]{}
       \label{fig::exps_new_ablations_vary_alpha::dog_run}
    \end{minipage}\hfill
    \begin{minipage}[b]{0.25\textwidth}
        \centering
       \resizebox{0.875\textwidth}{!}{\input{figures/ablations/varying_alpha/pareto_dog_run}}
       \subcaption[]{}
       \label{fig::exps_new_ablations_vary_alpha::dog_run_pareto}
    \end{minipage}\hfill
    \begin{minipage}[b]{0.24\textwidth}
        \centering
       \resizebox{1\textwidth}{!}{\input{figures/ablations/vsDistrQGauss/humanoid_run}}
       \subcaption[]{}
       \label{fig::exps_new_ablations_gauss_vs_diff_distrq::humanoid_run}
    \end{minipage}\hfill
    \begin{minipage}[b]{0.24\textwidth}
        \centering
       \resizebox{1\textwidth}{!}{\input{figures/ablations/vsDistrQGauss/dog-run}}
       \subcaption[]{}
       \label{fig::exps_new_ablations_gauss_vs_diff_distrq::dog_run}
    \end{minipage}\hfill
    \vspace{-2.0mm}
    \caption{\textbf{Reward Scaling Sensitivity (a)-(b)}. The $\alpha$ parameter controls the exploration-exploitation trade-off. (a) shows the learning curves for varying values on DMC's dog-run task. Too high $\alpha$ values ($\alpha=0.1$) do not incentivize learning whereas too small $\alpha$ values ($\alpha\leq10^{-5}$) converge to suboptimal behavior. (b) shows the aggregated end performance for each learning curve in (a). For increasing $\alpha$ values, the end performance increases until it reaches an optimum at $\alpha=10^{-3}$ after which the performance starts dropping. \textbf{Diffusion Policy Benefit (c) and (d).} We compare DIME to a Gaussian policy with the same implementation details as DIME on the (a) humanoid-run and (b) dog-run tasks. The diffusion-based policy reaches a higher return (a) and converges faster.} \vspace{-2mm}
\end{figure*}


\subsection{DIME: A Practical Diffusion RL Algorithm}\label{dime_practical}
To obtain a practical algorithm, we use a parameterized function approximation for the $Q$-function and the policy, that is, $Q_{\phi}$ and $\ppi$, with parameters $\phi$  and $\theta$, respectively.  Here, $\ppi$ is represented by a parameterized score network, see Eq. \ref{eq: approximate denoising process}. 
%
To perform approximate policy evaluation, we can minimize the Bellman residual,  
\begin{equation}\label{eq::Bellman_Residual}
    J_Q(\phi) = \frac{1}{2}\E\left[\left(Q_{\phi}(s_t,a^0_t) - Q_{\text{target}}(s_t,a^0_t)\right)^2\right],
\end{equation}
using stochastic gradients with respect to $\phi$. We provide implementation details in \Cref{sec:implementation details}. Moreover, the expectation is computed using state-action pairs collected from environment interactions and saved in a replay buffer. 
%
For policy improvement, we solve the approximate inference problem 
\begin{equation}
\label{eq: joint control as inference2}
\mathcal{L}(\theta) = D_{\text{KL}}\left(\ppi_{0:N}(\ac^{0:N}|\st)|\fpi_{0:N}(\ac^{0:N}|\st)\right),
\end{equation}
where the target policy, i.e., the marginal of the noising process in Eq. \ref{eq: forward joint} is given by the approximate $Q$-function
\begin{equation}
    \fpi_{0}(\ac^{0}|\st) = \frac{\exp Q_{\theta}(\st, \ac^0)}{\Z_{\theta}(\st)},
\end{equation}
where states are again sampled from a replay buffer.
Further expanding $\mathcal{L}(\theta)$ yields
\begin{align}
\label{eq: expanded loss}
    \mathcal{L}(\theta) = & \E_{\ppi}\Bigg[ \log \ppi_N(a^N|s) - Q_{\phi}(\st,\ac^0)   \\ \nonumber
    & + \sum_{n=1}^N \log\frac{\ppi_{n|n-1}(\ac^{n} \big| \ac^{n-1},\st)}{\fpi_{n-1|n}(\ac^{n-1} \big| \ac^{n},\st)} \Bigg] + \log \Z_{\phi}(s),
\end{align}
showing that $\Z_{\phi}$ is not needed to minimize Eq. \ref{eq: expanded loss} as it is independent of $\theta$. Moreover, contrary to the score-matching objective (see Eq. \ref{eq: score matching}) that is commonly used to optimize diffusion models, stochastic optimization of $\mathcal{L}(\theta)$ does not need access to samples $\ac_0 \sim \exp Q_{\phi}/\Z_{\phi}$, instead relying on stochastic gradients obtained via reparameterization trick \cite{kingma2013auto} using samples from the diffusion model $\ppi$.

\subsection{Implementation Details}\label{sec:implementation details}
\textbf{Autotuning Temperature.} We follow implementations like SAC \cite{haarnoja2018softimplementations} where the reward scaling parameter $\alpha$ is not absorbed into the reward but scales the entropy term. 
Choosing $\alpha$ depends on the reward ranges and the dimensionality of the action space which requires tuning it per environment. We instead follow prior works \cite{haarnoja2018softimplementations} for auto-tuning $\alpha$ by optimizing 
\begin{equation}\label{eq:auto_tuning_alpha}
    J(\alpha) = \alpha \left( \mathcal{H}_{\text{target}} - \plb \right),
\end{equation}
where $\mathcal{H}_{\text{target}}$ is a target value for the mismatch between the noising and denoising processes measured by the log ratio. 

\textbf{Autotuning Diffusion Coefficient.} Please note that the objective function in Eq. \ref{eq: expanded loss} is fully differentiable with respect to parameters of the diffusion process. As such, we additionally treat the diffusion coefficient $\beta$ as learnable parameter that is optimized end-to-end, further reducing the need for manual hyperparameter tuning. Further details on the parameterization can be found in Appendix \ref{appdx:implementation_details}.
%

\textbf{$Q$-function.} Following \citet{bhattcrossq} we adopt the CrossQ algorithm, i.e., we use Batch Renormalization in the Q-function and avoid a target network for calculating $Q_{\text{target}}$. When updating the Q-function, the values for the current and next state-action pairs are queried in parallel. The next Q-values are used as target values where the gradients are stopped. Additionally, we employ distributional Q learning as proposed by \cite{bellemare2017distributional}. The details are described in Appendix \ref{appdx:implementation_details}.
