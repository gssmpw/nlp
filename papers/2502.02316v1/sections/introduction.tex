\section{Introduction}

The maximum entropy reinforcement learning (MaxEnt-RL) objective augments the task reward in each time step with the entropy of the policy \cite{ziebart2008maximum,toussaint2009robot,haarnoja2017reinforcement,haarnoja2018soft}. 
This objective has several favorable properties among which improved exploration \cite{ziebart2010modeling, haarnoja2017reinforcement} is crucial for RL. 
Recent successful model-free RL algorithms leverage these favorable properties and build upon this framework \cite{bhattcrossq,nauman2024bigger} improving sample efficiency and leading to remarkable results. 
However, the policies are traditionally parameterized using Gaussian distributions, significantly limiting their representational capacity. 
On the other hand, diffusion models \cite{sohl2015deep, ho2020denoising, song2021scorebased, karras2022elucidating} are highly expressive generative models and have proven beneficial in representing complex behavior policies \cite{reuss2023goal, chi2023diffusionpolicy}. 
However, important metrics such as the marginal entropy are intractable to compute \cite{zhou2024variational} which restricts their usage in RL.
Because of this shortcoming, recent methods propose different ways to train diffusion-based methods in off-policy RL. While these methods are discussed in more detail in the related work section, most of them require additional techniques to add artificial (in most cases Gaussian) noise to the generated actions to induce exploration in the behavior generation process. Hence, they do not leverage the diffusion model to generate potentially non-Gaussian exploration patterns but fall back to mainly Gaussian exploration. 
Nonetheless, there have been significant advances in training diffusion-based models for approximate inference \cite{berneroptimal, richterimproved}. Since the policy improvement in MaxEnt-RL can also be cast as an approximate inference problem to the energy-based policy \cite{haarnoja2017reinforcement}, it is a natural step to explore these parallels.

%Based on these insights, 
We propose Diffusion-Based Maximum Entropy Reinforcement Learning (DIME). \textit{DIME} leverages recent advances in approximate inference with diffusion models \cite{richterimproved} to derive a lower bound on the MaxEnt objective. We propose a policy iteration framework with monotonic policy improvement that converges to the optimal diffusion policy. Additionally, building on recent off-policy RL algorithms such as Cross-Q \cite{bhattcrossq} and distributional RL \cite{bellemare2017distributional}, we propose a practical version of DIME that can be used for training diffusion-based RL policies. On 13 challenging continuous high-dimensional control benchmarks, we empirically validate that DIME significantly outperforms other diffusion-based baselines on all environments and consistently outperforms other state-of-the-art RL methods based on a Gaussian policy on 10 out of 13 environments, while being computationally more efficient and requiring less algorithmic design choices as the current state of the art baseline BRO \cite{nauman2024bigger}. 
