\section{Related Work} \label{sec:rel_work}
\textbf{Maximum Entropy RL.} 
The maximum entropy RL framework is using the entropy of the policy at each time step as additional objective, providing a principled way of inducing exploration in the RL policy. It is different from entropy regularized RL \cite{neu2017unified} where the entropy of the policy is maximized only for the current time step.
\citet{haarnoja2017reinforcement} proposed Soft-Q Learning where amortized Stein variational gradient descent \cite{wang2016learning} (SVGD) is used to train a parameterized sampler that can sample from the energy-based policy. SAC \cite{haarnoja2018soft} proposes an actor-critic RL method but frames the policy update as an approximate inference problem to the energy-based policy using a Gaussian policy parameterization. 
SAC has been extended to energy-based policies using SVGD in \cite{messaouds} where the authors also propose a new method to estimate the entropy in closed-form. While SVGD is a powerful method for learning an energy-based policy, it is harder to scale these approaches to high-dimensional control problems. 
Recent advances of SAC also define the state-of-the-art in off-policy RL in many domains such as CrossQ \cite{bhattcrossq} and BRO \cite{nauman2024bigger}. CrossQ proposed removing the target network by leveraging batch renormalization and BRO scales to large networks in RL by using several methods such as optimistic exploration \cite{nauman2023theory}, network resets \cite{nikishin2022primacy}, weight decay and high update to data ratios. 

\textbf{Diffusion-Based Policies in RL.} Early works have researched diffusion models in offline RL \cite{lange2012batch, levine2020offline} as trajectory generators \cite{janner2022planning} or as expressive policy representations \cite{wang2023diffusion, kang2024efficient, hansen2023idql, chenoffline,dingconsistency}. 
More recently, diffusion models in online RL have become more popular. 
DIPO \cite{yang2023policy} proposes training a diffusion-based policy using a behavior cloning loss. The actions in the replay buffer serve as target actions for the policy improvement step and are updated using the gradients of the Q-function $\nabla_{\ac}Q(\st,\ac)$. DIPO has been extended to develop methods for learning multi-modal behaviors\cite{li2024learning} by leveraging hierarchical clustering to isolate different behavior modes. DIPO relies on the stochasticity inherent to the diffusion model for exploration and does not explicitly control it via an objective. QSM \cite{psenkalearning} directly matches the policy's score with the gradient of the Q-function $\nabla_{\ac}Q(\st, \ac)$. While their objective avoids differentiating through the whole diffusion chain, the proposed objective disregards the entropy of the policy and therefore exploration. Consequently, QSM needs to add noise to the final action of the diffusion chain. 
More recently, DACER \cite{wang2024diffusion} proposed using the data-generating process as the policy representation and backpropagating the gradients through the diffusion chain. 
However, they do not consider a backward process as we do and their objective for updating the diffusion model is based on the expected Q-values only. 
To incentivize the exploration, DACER adds diagonal Gaussian noise to the sampled actions, where the variance of this noise is controlled by a scaling term that is updated automatically using an approximation of the marginal entropy by extracting a Gaussian Mixture Model from the diffusion policy. 
Concurrently, QVPO \cite{ding2024diffusionbased} proposed weighting their diffusion loss with their respective Q-values after applying transformations. 
However, QVPO relies on sampling actions from a uniform distribution to enforce exploration. 

DIME distinguishes from prior works in that we use the maximum entropy RL framework for training the diffusion policy which was not considered before. This allows direct control of the exploration-exploitation trade-off arising naturally through this objective without the need for additional approximations. DIME is leveraging the diffusion model to generate non-Gaussian exploration actions which is in contrast to most other diffusion RL approaches that still require including Gaussian or uniform exploration noise. 

\textbf{Approximate Inference with Diffusion Models.} Early works on approximate inference with diffusion models were formalized as a stochastic optimal control problem using Schr\"odinger-F\"ollmer diffusions \cite{dai1991stochastic,tzen2019theoretical,huang2021schrodinger} and only recently realized with deep-learning based approaches \cite{vargas2023bayesian,zhang2021path}. \citet{vargasdenoising,berneroptimal} later extended these results to denoising diffusion models. A more general framework where both, forward and backward processes of the diffusion model are learnable was concurrently proposed by \citet{richterimproved,nusken2024transport}. 
Recently, many extensions have been proposed, see e.g. \cite{akhound2024iterated,noble2024learned,geffner2023langevin,zhang2023diffusion,chen2024sequential}. Our work can be seen as an instance of the sampler presented in \cite{berneroptimal}. However, our formulation allows using different diffusion samplers such as those presented in \cite{richterimproved}, while we restrict ourselves in this work to the sampler presented in \cite{berneroptimal}.
