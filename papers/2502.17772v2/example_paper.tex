%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{multirow} 
\usepackage{tablefootnote}
\usepackage{bbm}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\wz}[1]{{\textcolor{red}{[wz: #1]}}}

\newcommand{\hao}[1]{{\textcolor{blue}{[hao: #1]}}}


\newenvironment{proofsketch}{%
  \renewcommand{\proofname}{Proof sketch}%
  \begin{proof}%
}{%
  \end{proof}%
}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{An Improved Privacy and Utility Analysis of DPSGD with Bounded Domain and Smooth Losses}

\begin{document}

\twocolumn[
\icmltitle{An Improved Privacy and Utility Analysis of \\
Differentially Private SGD with Bounded Domain and Smooth Losses}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Hao Liang}{aff1}
\icmlauthor{Wanrong Zhang}{aff2}
\icmlauthor{Xinlei He}{aff1}
\icmlauthor{Kaishun Wu}{aff1}
\icmlauthor{Hong Xing}{aff1,aff3}
% \icmlauthor{Firstname6 Lastname6}{sch,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{comp,yyy}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{aff1}{Information Hub, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China}
\icmlaffiliation{aff2}{Harvard University}
\icmlaffiliation{aff3}{Department of ECE, The Hong Kong University of Science and Technology, HK SAR}


%\icmlcorrespondingauthor{Hao Liang}{hliang346@connect.hkust-gz.edu.cn}
\icmlcorrespondingauthor{Hong Xing}{hongxing@ust.hk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}


Differentially Private Stochastic Gradient Descent (DPSGD) is widely used to protect sensitive data during the training of machine learning models, but its privacy guarantees often come at the cost of model performance, largely due to the inherent challenge of accurately quantifying privacy loss. While recent efforts have strengthened privacy guarantees by focusing solely on the final output and bounded domain cases, they still impose restrictive assumptions, such as convexity and other parameter limitations, and often lack a thorough analysis of utility. In this paper, we provide rigorous privacy and utility characterization for DPSGD for smooth loss functions in both bounded and unbounded domains. We track the privacy loss over multiple iterations by exploiting the noisy smooth-reduction property and establish the utility analysis by leveraging the projection's non-expansiveness and clipped SGD properties. In particular, we show that for DPSGD with a bounded domain, (i) the privacy loss can still converge without the convexity assumption, and (ii) a smaller bounded diameter can improve both privacy and utility simultaneously under certain conditions. Numerical results validate our results.
% In this paper, we provide rigorous privacy and utility characterization for DPSGD for smooth loss functions with both bounded and unbounded domains. In particular, we track the privacy loss over multiple iterations by analyzing the noisy smooth-reduction of shifted R\'enyi divergence caused by SGD sampling, and establish the utility analysis for DPSGD with bounded domain by leveraging the non-expansiveness of projection and properties of clipped SGD. Numerical results validate the theory.
\end{abstract}

\section{Introduction}
\label{submission}

% \begin{itemize}
%     \item \textcolor{blue}{Abadi’s results and their variants assume all intermediate models are revealed, RDP loss will over-estimate}
%     \item \textcolor{blue}{Under the final-round model release assumption, RDP loss still linearly increases with $T$ (Privacy Amplification by Iteration Paper), need convex assumption}
%     \item \textcolor{blue}{Apple’s results showing more training w/o more DP loss, converged privacy loss, need convex and other parameter assumptions}
%     \item \textcolor{blue}{Recent arXiv work, weakly-convex analysis, but just for cyclic data traversal case}
%     \item \textcolor{blue}{Recent ICLR 2025 work, non-convex analysis, but no analytical form}
%     \item \textcolor{blue}{How to characterize the privacy loss and utility in non-convex case}
% \end{itemize}

Differentially Private Stochastic Gradient Descent (DPSGD) \cite{abadi2016deep} has emerged as the leading defense mechanism to protect personal sensitive data in training of machine learning models. However, achieving good performance with DPSGD often comes with a significant privacy cost. A fundamental question, therefore, is how to precisely quantify the privacy loss associated with DPSGD. 

Previous methods for quantifying privacy loss include strong composition \cite{dwork2010boosting,bassily2014private,kairouz2015composition}, moments accountant \cite{abadi2016deep}, R\'enyi Differential Privacy (RDP) \cite{mironov2017renyi,mironov2019r}, and Gaussian Differential Privacy (GDP) \cite{dong2022gaussian}, along with several numerical composition methods \cite{koskela2020computing,gopi2021numerical}. These methods primarily rely on composition theorems, assuming that all intermediate models are revealed during the training procedure, which leads to an overestimation of privacy loss. While numerical composition methods aim to tightly characterize the privacy loss, they still operate under this same assumption.

To address this overestimation, recent works have focused solely on the privacy guarantees of the final output. For instance, the privacy amplification by iteration \cite{feldman2018privacy} demonstrated that withholding intermediate results significantly enhances privacy guarantees for smooth and convex objectives.  Building upon this, Chourasia et al. \yrcite{chourasia2021differential} suggest that the privacy loss of DPGD, the full batch version of DPSGD, may converge exponentially fast for smooth and strongly convex objectives. Furthermore, results by Ye and Shokri \yrcite{ye2022differentially} as well as Ryffel et al. \yrcite{ryffel2022differential} extended this analysis to assess the privacy loss of DPSGD, although both studies rely on the assumption of strong convexity.

More recently, the work by Altschuler and Talwar \yrcite{altschuler2022privacy} and its extension \cite{altschuler2024privacy} established a constant upper bound on privacy loss after a burn-in period for Lipschitz continuous and smooth convex losses over a bounded domain. However, this analytical result is limited by its reliance on the convexity assumption and strict restrictions on the Rényi parameter $\alpha$, which hinders its broader applicability. In order to relax several strong assumptions, Kong and Ribero \yrcite{kong2024privacy} provided an analysis of weakly-convex smooth losses in the case where data is traversed cyclically. Later, Chien and Li \yrcite{chien2024convergent} suggest precisely tracking the privacy leakage incurred before reaching the constant upper bound by solving an optimization problem. However, this result is formulated as an optimization problem rather than a closed-form expression, making it hard to operationalize. Notably, most recent methods necessitate double clipping of both gradients and parameters due to the additional bounded domain assumption. These methods, however, do not provide a thorough utility analysis or experimental results, leaving their practical performance and trade-offs underexplored.

We outline the main contributions of this paper below and provide a comparison of the key assumptions and theoretical results with the most relevant works in \cref{table:compare}.

\begin{table*}[t]
\caption{Comparison of the $(\alpha, \varepsilon)$-RDP guarantee and assumptions needed by different works for DPSGD, where $b$ is the batch size, $n$ is the dataset size, $\eta$ is the step size, $C$ is the gradient norm bound, $D$ is the diameter of the parameter domain, $\sigma_{\text{DP}}$ is the noise scale, and $T$ is the number of iterations. ``$\ddag$" is exclusively suitable for cyclic data traversal cases. ``$\dag$" indicates that a tighter bound can be obtained under additional assumptions on the R\'enyi parameters. $\alpha^*(q,\sigma)$ is defined as the largest $\alpha$ that satisfies both $\alpha \leq K \sigma^2/2-2\log \sigma$ and $\alpha \leq\left(K^2 \sigma^2 / 2-\log 5-2\log \sigma\right) /\left(K+\log (q \alpha)+1 /(2 \sigma^2)\right)$, where $K=\log(1+1/(q(\alpha-1)))$.}
\label{table:compare}
% \vskip 0.15in
\begin{center}
\resizebox{\textwidth}{!}{
\begin{small}
% \begin{sc}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lccccr}
\toprule
Reference & Assumptions  & Domain & Privacy Guarantee & Utility Analysis?  \\
\midrule
\citealt{feldman2018privacy}   & convex, $L$-smooth & unbounded & $\mathcal{O}\left( \frac{ \alpha C^2}{b^2 \sigma_{\text{DP}}^2} T\right)$ & $\surd$\\ \midrule
\multirow{2}{*}{\citealt{altschuler2022privacy}}   & convex, $L$-smooth, $M$-Lipschitz & \multirow{2}{*}{bounded} & \multirow{2}{*}{$\mathcal{O}\left( \frac{\alpha M^2}{n^2\sigma_{\text{DP}}^2} \min \left\{T, \frac{Dn}{\eta M}\right\}\right)$} & \multirow{2}{*}{$\times$}\\
& $\eta\leq 2/L$, $b\leq n/5$, $\sigma_{\text{DP}}>8\sqrt{2}M/b$, $\alpha\leq \alpha^*(b/n,\frac{b\sigma_{\text{DP}}}{2\sqrt{2}M})$  & \\ \midrule
\multirow{2}{*}{\citealt{kong2024privacy}$^\ddag$}      & $m$-weakly convex, $L$-smooth & \multirow{2}{*}{bounded} &  \multirow{2}{*}{$\mathcal{O}\left(\frac{\alpha}{\sigma_{\text{DP}}^2}(D\sqrt{1+2\eta m[1+\frac{m}{2(L+m)}]}+\frac{\eta C}{b})^2\right)$}  & \multirow{2}{*}{$\times$}      \\
 & $\eta\leq\frac{1}{2(m+L)}$ & \\ \midrule
\citealt{chien2024convergent}   &  $L$-smooth or $(L,\lambda)$-H\"older continuous gradient  & bounded & w/o analytical form & $\times$ \\ \midrule
\textbf{Ours}$^\dag$ & $L$-smooth & unbounded & $\mathcal{O}\left( \frac{ \alpha C^2}{n b\sigma_{\text{DP}}^2} T\right)$& $\surd$ \\[5pt]
% \textbf{Ours} & $L$-smooth, $b\leq n/5$, $\sigma_{\text{DP}}>8\sqrt{2}C/b$, $\alpha\leq \alpha^*(b/n,\frac{b\sigma_{\text{DP}}}{2\sqrt{2}C})$ & unbounded & $\mathcal{O}\left( \frac{ \alpha C^2}{n^2\sigma_{\text{DP}}^2} T\right)$& $\surd$ \\[4pt] 
\textbf{Ours}$^\dag$ & $L$-smooth & bounded & $\mathcal{O}\left(\frac{\alpha}{ \sigma_{\textup{DP}}^2}(\frac{(1+\eta L)^2D^2}{\eta^2}+\frac{C^2}{nb})\right)$
& $\surd$ \\
% [4pt]
% \textbf{Ours} & $L$-smooth, $b\leq n/5$, $\sigma_{\text{DP}}>8\sqrt{2}C/b$, $\alpha\leq \alpha^*(b/n,\frac{b\sigma_{\text{DP}}}{2\sqrt{2}C})$  & bounded & $\mathcal{O}\left(\frac{\alpha}{ \sigma_{\textup{DP}}^2}(\frac{(1+\eta L)^2D^2}{\eta^2}+\frac{C^2}{n^2})\right)$
% & $\surd$ \\
\bottomrule
\end{tabular}
% \end{sc}
}
\end{small}
}
\end{center}
\vskip -0.1in
\end{table*}



\subsection{Contributions}

In this paper, we present a precise analytical 
characterization of privacy bounds for DPSGD that focus on smooth loss functions without relying on convexity assumptions or restrictive R\'enyi parameter conditions. Our general results encompass DPSGD applied to both unbounded and bounded domains. Additionally, we establish utility guarantees based on the derived RDP bounds, offering an intuitive perspective on privacy-utility trade-offs. To demonstrate the practicality and validity of our theoretical findings, we conduct extensive numerical simulations, which confirm the effectiveness and rationality of the proposed bounds. Our contributions are as follows:
\begin{itemize}
    \item We analyze the noisy smooth-reduction behavior of the shifted R\'enyi divergence for smooth objectives. This analysis enables the derivation of closed-form RDP guarantees for DPSGD applied to both unbounded and bounded domains.

    \item We establish the convergence behavior for DPSGD with smooth loss functions in unbounded domains and strongly convex smooth loss functions in bounded domains. 
    %for the unbounded domain DPSGD with smooth losses, as well as for the bounded case with strongly convex smooth losses. 
    Our results provide the privacy-utility trade-offs under the computed RDP bounds.
    \item To validate these theoretical findings, we examine the privacy parameters estimated by the membership inference attack (MIA). Extensive experiments illustrate the effectiveness and rationality of the proposed bounds.
\end{itemize}



\subsection{Other Related Works}

In addition to privacy analysis, the utility (convergence) of private optimization algorithms has been extensively studied. This line of works typically focus on understanding how the number of iterations affects the convergence behavior of the algorithm. Below, we provide a brief review of utility analysis for DPSGD.

In 2014, Bassily et al. \yrcite{bassily2014private} analyzed the optimal utility guarantees of DPSGD under the assumption of Lipschitz continuity, considering both convex and strongly convex cases. Then, based on the additional assumption of the gradient distribution, Chen et al. \yrcite{chen2020understanding} studied the convergence of DPSGD with gradient clipping (DPSGD-GC) and derived a utility bound for the non-convex setting. The work by Song et al. \yrcite{song2021evading} explored the convergence of DPSGD-GC for generalized linear models, noting that, in the worst case, the utility can remain constant relative to the original objective. Later, Fang et al. \yrcite{fang2023improved} further refined this analysis for smooth and unconstrained problems, providing more precise convergence results. However,  many of these studies fix a specific value for the clipping threshold $C$, which may be adjusted due to privacy requirements. 

More recently, Koloskova et al. \yrcite{koloskova2023revisiting} characterized the convergence guarantees for DPSGD-GC across various clipping thresholds $C$ in the non-convex setting. While this work provides valuable convergence insights for DPSGD-GC, recent privacy characterizations have introduced the need for double clipping—clipping both gradients and parameters—due to the additional assumption of bounded domains. The convergence analysis involving double clipping has not been thoroughly explored in the existing literature.

\subsection{Organization}
% \wz{can remove this subsection if we concern about space later}
The rest of this paper is organized as follows: In the next section, we recall the relevant preliminaries. Our main results are presented in \cref{s3}. Numerical results are provided in \cref{s4}. Finally, \cref{s5} concludes with a discussion of future research directions motivated by our findings. Proof details are deferred to Appendices.


\section{Preliminaries}
\label{s2}

% \begin{itemize}
%     \item \textcolor{blue}{Important definitions and lemmas (RDP’s definitions, properties, contraction inequality...)}
%     \item \textcolor{blue}{Google brain’s, Apple’s results, … are based on the shifted R\'enyi divergence}
%     \item \textcolor{blue}{DPSGD with double clipping algorithm}
% \end{itemize}

In this section, we introduce the foundational concepts and definitions relevant to our analysis. We start with our notation, which will be used throughout this paper.

\textbf{Notations.} Let $\operatorname{Pr}[\cdot]$ denote the probability of a random event, and $\mathbb{P}_{\boldsymbol{\mu}}$ be the law of a random variable $\boldsymbol{\mu}$. We refer to two datasets $\mathcal{D}$ and $\mathcal{D}^\prime$ as adjacent if they differ from each other only by adding or removing one data point.

\subsection{R\'enyi Differential Privacy (RDP)}

We first recall the formal definition of differential privacy (DP), which provides a standard framework to ensure that a model's output remains almost unchanged when applied to two adjacent datasets that differ only in a single data entry.

\begin{definition}
\textup{(Differential privacy \cite{dwork2006our}).}
For $\epsilon\geq 0$, $\delta\in[0,1]$, a randomized mechanism $\mathcal{M}:\mathcal{X}\mapsto \mathcal{Y}$ is $(\epsilon,\delta)$-DP if, for every pair of adjacent datasets, $\mathcal{D}, \mathcal{D}^\prime \subseteq \mathcal{X}$, and for any subset of outputs $\mathcal{S}\subseteq \mathcal{Y}$, we have
\begin{equation}
    \operatorname{Pr}[\mathcal{M}(\mathcal{D})\in \mathcal{S}]\leq \exp(\epsilon)\operatorname{Pr}[\mathcal{M}(\mathcal{D}^\prime)\in \mathcal{S}]+\delta.
\end{equation}
\end{definition}

Throughout this paper, we use RDP, a more efficient approach for tracking privacy loss than DP, as our primary framework for privacy analysis. RDP provides a relaxation of DP based on \emph{R\'enyi divergence}, which is defined as follows.

\begin{definition}
\textup{(R\'enyi divergence \cite{renyi1961measures}).} For adjacent datasets $\mathcal{D}$ and $\mathcal{D}^\prime$, a mechanism $\mathcal{M}:\mathcal{X}\mapsto \mathcal{Y}$, and an outcome $s\in \mathcal{Y}$, the R\'enyi divergence of a finite order $\alpha\neq1$ between $\mathcal{M}(\mathcal{D})$ and $\mathcal{M}(\mathcal{D}^\prime)$ is defined as
\begin{equation}
\begin{aligned}
&D_\alpha(\mathbb{P}_{\mathcal{M}(\mathcal{D})}||\mathbb{P}_{\mathcal{M}(\mathcal{D}^\prime)}) 
\\
&=\frac{1}{\alpha-1} \log \mathbb{E}_{s\sim \mathbb{P}_{\mathcal{M}(\mathcal{D}^\prime)}}\left\{\left(\frac{\operatorname{Pr}[\mathcal{M}(\mathcal{D})=s]}{\operatorname{Pr}[\mathcal{M}(\mathcal{D}^\prime)=s]}\right)^\alpha\right\}.
\end{aligned}
\end{equation}
\end{definition}


On the grounds of R\'enyi divergence, RDP is defined by the following definition.
\begin{definition}
\textup{(R\'enyi differential privacy \cite{mironov2017renyi}).} For $\alpha> 1$, $\varepsilon\geq0$, a randomized mechanism $\mathcal{M}:\mathcal{X}\mapsto \mathcal{Y}$ satisfies $(\alpha, \varepsilon)$-RDP if, for any pair of adjacent datasets, $\mathcal{D}, \mathcal{D}^\prime \subseteq \mathcal{X}$, it holds that
\begin{equation}
\begin{aligned}
D_\alpha(\mathbb{P}_{\mathcal{M}(\mathcal{D})}||\mathbb{P}_{\mathcal{M}(\mathcal{D}^\prime)})\leq \varepsilon.
\end{aligned}
\end{equation}
\end{definition}

Note that RDP can be easily transformed into an equivalent characterization in terms of DP, as demonstrated by the following lemma.
\begin{lemma}
\label{lem:RDP2DP}
\textup{(From $(\alpha,\varepsilon)$-RDP to $(\epsilon,\delta)$-DP \cite{mironov2017renyi}).} If $\mathcal{M}$ is an $(\alpha,\varepsilon)$-RDP mechanism, it also satisfies $(\varepsilon+\frac{\log1/\delta}{\alpha-1},\delta)$-DP for any $0<\delta<1$.
\end{lemma}


Based on the assumption that intermediate training models are not revealed, \emph{privacy amplification by iteration} \cite{feldman2018privacy} substantially improved the privacy guarantee analysis. This analytical framework is grounded on the concept of \emph{shifted R\'enyi divergence}, which is also useful for our analysis.

\begin{definition}
\label{def:shifted}
    \textup{(Shifted R\'enyi divergence \cite{feldman2018privacy}).} Let $\boldsymbol{\mu}$, $\boldsymbol{\nu}$ be two random variables. Then, for any $z\geq0$ and $\alpha>1$, the $z$-shifted Rényi divergence is defined as
    \begin{equation}
    \mathcal{D}_\alpha^{(z)}(\mathbb{P}_{\boldsymbol{\mu}} || \mathbb{P}_{\boldsymbol{\nu}})=\inf_{\mathbb{P}_{\boldsymbol{\mu}^\prime}:W_{\infty}(\mathbb{P}_{\boldsymbol{\mu}},\mathbb{P}_{\boldsymbol{\mu}^\prime})\leq z}\mathcal{D}_\alpha(\mathbb{P}_{\boldsymbol{\mu}^\prime}||\mathbb{P}_{\boldsymbol{\nu}}),
    \end{equation}
    where $W_{\infty}(\cdot,\cdot)$ denotes the $\infty$-Wasserstein distance\footnote{See \cref{def:wass}.} between two distributions.
\end{definition}

The privacy amplification by iteration framework relies on two key lemmas, which we restate below, focusing specifically on Gaussian noise, which suffices for our purposes. First, \cref{lemma:shifted-reduct} stated next will prove to be useful for analyzing shifted R\'enyi divergence between two distributions convolved with Gaussian noise.

\begin{lemma}
    \label{lemma:shifted-reduct}
    \textup{(Shift-reduction \cite{feldman2018privacy}).} Let $\boldsymbol{\mu}$, $\boldsymbol{\nu}$ be two $d$-dimensional random variables. Then, for any $a\geq0$ and $z\geq 0$, we have
    \begin{equation}
        \begin{aligned}
        \mathcal{D}_\alpha^{(z)}(\mathbb{P}_{\boldsymbol{\mu}} * \mathbb{P}_{\boldsymbol{\zeta}}  \|  \mathbb{P}_{\boldsymbol{\nu}} * \mathbb{P}_{\boldsymbol{\zeta}}) 
        \leq \mathcal{D}_\alpha^{(z+a)}(\mathbb{P}_{\boldsymbol{\mu}} \|  \mathbb{P}_{\boldsymbol{\nu}})+\frac{\alpha a^2}{2\sigma^2},
        \end{aligned}
    \end{equation}
where $\boldsymbol{\zeta}\sim\mathcal{N}(\boldsymbol{0},\sigma^2\boldsymbol{I}_d)$, and $\mathbb{P}_{\boldsymbol{\mu}} * \mathbb{P}_{\boldsymbol{\zeta}}$ denotes the distribution of the sum $\boldsymbol{\mu}+\boldsymbol{\zeta}$ with $\boldsymbol{\mu}$ and $\boldsymbol{\zeta}$ drawn independently.
\end{lemma}

The contraction-reduction lemma provides an upper bound on the shifted Rényi divergence between the distributions of two pushforwards through similar contraction maps.

\begin{lemma}
    \label{lemma:contract-reduce}
    \textup{(Contraction-reduction \cite{feldman2018privacy}).} 
    Let $\boldsymbol{\mu}$, $\boldsymbol{\nu}$ be two random variables. Suppose $\phi$, $\phi^\prime$ are two contractive functions\footnote{ A function is said to be contractive if it is $1$-Lipschitz.} and let $\sup_{\boldsymbol{x}} \|\phi(\boldsymbol{x})-\phi^\prime(\boldsymbol{x})\| \leq s$. Then, for any $z\geq0$, we have
    \begin{equation}
        \mathcal{D}_\alpha^{(z+s)}(\mathbb{P}_{\phi(\boldsymbol{\mu})}||\mathbb{P}_{\phi^\prime(\boldsymbol{\nu})})\leq\mathcal{D}_\alpha^{(z)}(\mathbb{P}_{\boldsymbol{\mu}}||\mathbb{P}_{\boldsymbol{\nu}}).
    \end{equation}
\end{lemma}

Altschuler et al. \cite{altschuler2022privacy, altschuler2024privacy} provide a mild generalization of this lemma, extending it to be suitable for random contraction maps.

\subsection{DPSGD with Double Clipping}

Here, we consider the DPSGD with both gradient clipping and parameter projection (\cref{alg:DPSGD}), termed as \emph{DPSGD-DC}. This method begins with applying the SGD procedure using Gaussian perturbation for model updates. Then, the updated parameters are projected into a bounded domain $\mathcal{K}$ with diameter $D$. Specifically, the update procedure can be expressed as
\begin{equation}
\boldsymbol{\theta}_{t+1}=\Pi_\mathcal{K}\left(\boldsymbol{\theta}_{t}-\eta (\nabla \mathcal{L}_{B_t}\left(\boldsymbol{\theta}_{t};\mathcal{D}\right)+\boldsymbol{\zeta}_{t})\right),
\end{equation}
with 
\begin{equation}
    \Pi_{\mathcal{K}}(\boldsymbol{\theta})\!=\! \arg\min_{\boldsymbol{x}\in\mathcal{K}}\|\boldsymbol{\theta}-\boldsymbol{x}\|,
\end{equation}
where $\eta$ denotes the learning rate, $\boldsymbol{\zeta}_{t}\sim \mathcal{N}(\boldsymbol{0},\sigma_{\text{DP}}^2\boldsymbol{I}_d)$ with noise scale $\sigma_{\text{DP}}$, and 
$\nabla \mathcal{L}_{B_t}\left(\boldsymbol{\theta}_{t} ; \mathcal{D}\right)$ is the clipped SGD gradient obtained from a mini-batch $\mathcal{B}_t$, i.e., $\nabla \mathcal{L}_{B_t}\left(\boldsymbol{\theta}_{t} ; \mathcal{D}\right)=\frac{1}{b} \sum_{\xi \in \mathcal{B}_t} \operatorname{clip}_C(\nabla l_\xi(\boldsymbol{\theta}_t))$, with $b=\left|\mathcal{B}_t\right|$ and $\operatorname{clip}_C(\boldsymbol{x}) = \boldsymbol{x}\cdot\min(1,\frac{C}{\|\boldsymbol{x}\|})$. 

If $\mathcal{K}=\mathbb{R}^d$, it reduces to the vanilla DPSGD with gradient clipping, and we refer to it as \emph{DPSGD-GC}.

\begin{algorithm}[tb]
   \caption{Differentially Private Stochastic Gradient Descent with Double Clipping (DPSGD-DC)}
   \label{alg:DPSGD}
\begin{algorithmic}
   \STATE {\bfseries Input:} Dataset $\mathcal{D}$, stochastic loss function $l_\xi(\boldsymbol{\theta}):\mathbb{R}^d\times \mathcal{D}\rightarrow \mathbb{R}$, learning rate $\eta$, noise scale $\sigma_{\text{DP}}$, dataset size $n$, batch size $b$, gradient norm bound $C$, parameter domain $\mathcal{K}$ with diameter $D$, number of iterations $T$;
   \STATE Initialize $\boldsymbol{\theta}_0\leftarrow \boldsymbol{0}$ and $t\leftarrow0$; \\
   \REPEAT
   \STATE \textbf{1) batch sampling:} \\
   take a random mini-batch $\mathcal{B}_t$ with sampling probability $q=b/n$;
   \STATE \textbf{2) compute and clip the gradients:} \\
   $\nabla \mathcal{L}_{\mathcal{B}_t}(\boldsymbol{\theta}_{t} ; \mathcal{D})\leftarrow\frac{1}{b} \sum_{\xi \in \mathcal{B}_t} \operatorname{clip}_C\left(\nabla l_{\xi}\left(\boldsymbol{\theta}_{t}\right)\right)$, \\
   where $\operatorname{clip}_C(\boldsymbol{x}) = \boldsymbol{x}\cdot\min(1,\frac{C}{\|\boldsymbol{x}\|})$; \\
   \STATE \textbf{3) update and project the parameters:} \\  $\boldsymbol{\theta}_{t+1}\leftarrow\Pi_{\mathcal{K}}(\boldsymbol{\theta}_{t}-\eta(\nabla \mathcal{L}_{B_t}\left(\boldsymbol{\theta}_{t};\mathcal{D}\right)+\boldsymbol{\zeta}_{t}))$, where\\
   $\Pi_{\mathcal{K}}(\boldsymbol{\theta})\!=\! \arg\min_{\boldsymbol{x}\in\mathcal{K}}\|\boldsymbol{\theta}-\boldsymbol{x}\|$ and $\boldsymbol{\zeta}_{t}\sim \!\mathcal{N}(\boldsymbol{0},\sigma_{\text{DP}}^2\boldsymbol{I}_d)$; \\
   \STATE \textbf{4) update the iteration counter:} \\
   $t \leftarrow t + 1$; \\
   \UNTIL{$t>T$}
   \STATE \textbf{\bfseries Output:} Final-round model parameters $\boldsymbol{\theta}_T$.
\end{algorithmic}
\end{algorithm}




% \subsection{Assumptions}

% Next, we provide the assumptions used in our analysis.

% \begin{assumption}
% \label{A:smooth-stochastic}
% \textup{($L$-smooth of stochastic loss function).} The stochastic loss function $l_\xi(\cdot): \mathbb{R}^d\times \mathcal{D} \mapsto \mathbb{R}$ is smooth with constant $L > 0$. That is, for any $\xi\in\mathcal{D}$, $l_\xi(\boldsymbol{\theta})$, the loss accrued on model $\boldsymbol{\theta}$, it is continuously differentiable, and the gradient $\nabla l_\xi(\cdot)$ is Lipschitz continuous with constant $L$, i.e.,  
% \begin{equation}
%     \left\|\nabla l_\xi(\boldsymbol{\theta})-\nabla l_\xi\left(\boldsymbol{\theta}^{\prime}\right)\right\| \leq L\left\|\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime}\right\|,
% \end{equation}
% for all $\boldsymbol{\theta}, \boldsymbol{\theta}^{\prime} \in \mathbb{R}^d$.
% \end{assumption}

% \begin{assumption}
% \label{A:smooth}
% \textup{($L$-smooth).} The loss function $l:\mathbb{R}^d\rightarrow\mathbb{R}$ is smooth with constant $L > 0$, that is, it is continuously differentiable and the gradient $\nabla l(\boldsymbol{\theta})$ is Lipschitz continuous with constant $L$, i.e.,  
% \begin{equation}
%     \left\|\nabla l(\boldsymbol{\theta})-\nabla l\left(\boldsymbol{\theta}^{\prime}\right)\right\| \leq L\left\|\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime}\right\|,
% \end{equation}
% for all $\boldsymbol{\theta}, \boldsymbol{\theta}^{\prime} \in \mathbb{R}^d$.
% \end{assumption}

% Let $l(\boldsymbol{\theta})=\mathbb{E}_{\xi\sim\mathbb{P}_\mathcal{D}}[l_\xi(\boldsymbol{\theta})]$ be the (expected) loss function, where $\mathbb{P}_\mathcal{D}$ denotes the uniform distribution over training datapoints, and $l_\xi(\boldsymbol{\theta})$ denotes the stochastic loss function, i.e., the loss of model $\boldsymbol{\theta}$ on the datapoint $\xi$. 

% \begin{assumption}
%     \label{A:smooth-loss}
%     \textup{($L$-smooth of loss function).} The loss function $l(\cdot)=\mathbb{E}_{\xi\sim\mathbb{P}_{\mathcal{D}}}[l_{\xi}(\cdot) ]$, where $\mathbb{P}_{\mathcal{D}}$ denotes uniform distribution over training dataset $\mathcal{D}$,  is smooth with constant $L > 0$.
% \end{assumption}
% Note that \cref{A:smooth-stochastic} implies \cref{A:smooth-loss}.



% \begin{assumption}
% \label{A:bound-SGD}
% \textup{(Bounded SGD variance).} The stochastic noise of $\nabla l_{\xi}$ is bounded, that is, for all $\boldsymbol{\theta}\in\mathbb{R}^d$, we have
% %\wz{distinction of $\nabla l_{\xi}$ and $\nabla l$ seems not define before. In algorithm 1, we have $\nabla l$ as input loss function, but we have $\nabla l_{\xi}$ inside the clip function. }

% %\hao{In this paper, we focus on solving the following stochastic optimization problem:
% %\begin{equation}
% %     \min _{\boldsymbol{\theta} \in \mathbb{R}^d}\left\{l(\boldsymbol{\theta}):=\mathbb{E}_{\xi \sim \mathbb{P}_\mathcal{D}}\left[l_{\xi}(\boldsymbol{\theta})\right]\right\}
% % \end{equation}
% % where $l_\xi$ denotes the stochastic loss function, i.e., the loss of model $\boldsymbol{\theta}$ on the datapoint $\xi$. \\
% % In order to clearly distinguish between the stochastic loss function $l_\xi$ and the (expected) loss function $l$, we modified \cref{alg:DPSGD} and added the definition of the loss function in this subsection (see the following sentence of \cref{A:smooth-stochastic}.
% % }
% \begin{equation}
%     \mathbb{E}_{\xi\sim\mathbb{P}_{\mathcal{D}}}\left[\|\nabla l_\xi(\boldsymbol{\theta})-\nabla l(\boldsymbol{\theta})\|^2\right]\leq \sigma_{\text{SGD}}^2.
% \end{equation}
% \end{assumption}

% Note that the bounded variance assumption offers a weaker and more realistic definition of SGD variance \cite{lan2012optimal, gorbunov2020stochastic, koloskova2023revisiting} compared to the uniform boundness employed by many other works \cite{zhang2019gradient,zhang2020improved,yang2022normalized}.

\section{Main Theoretical Results}
\label{s3}

% \begin{itemize}
%     \item \textcolor{blue}{Motivate the DP loss (emphasize the motivation in the introduction)}
%     \item \textcolor{blue}{Main Theorem}
%     \item \textcolor{blue}{Insights of the impact of different training-related parameters on the DP loss, and how does it align with our intuitions with limitations mentioned in Section 2 lifted}
%     \item \textcolor{blue}{Poof ideas: New results on non-convex privacy loss trajectory $\rightarrow$ Contraction reduction $\rightarrow$ Privacy guarantee for noisy SGD}
%     \item \textcolor{blue}{Poof ideas: Convergence of double clipping DPSGD $\rightarrow$ Utility guarantee for noisy SGD}
% \end{itemize}

In this section, we construct RDP bounds to analyze the privacy loss associated with releasing the final-round model of DPSGD-GC and DPSGD-DC, respectively. To complete our analysis, we also provide the convergence analysis and derive the utility bounds. 

\subsection{Privacy Analysis of DPSGD}

% We begin by introducing a key lemma that provides a bound on the shifted Rényi divergence for noisy update of smooth loss functions. 

First, we provide the assumption used throughout our analysis.

\begin{assumption}
\label{A:smooth-stochastic}
\textup{($L$-smooth of stochastic loss function).} The stochastic loss function $l_\xi(\cdot): \mathbb{R}^d\times \mathcal{D} \mapsto \mathbb{R}$ is smooth with constant $L > 0$. That is, for any $\xi\in\mathcal{D}$, $l_\xi(\boldsymbol{\theta})$, the loss accrued on model $\boldsymbol{\theta}$, is continuously differentiable, and the gradient $\nabla l_\xi(\cdot)$ is Lipschitz continuous with constant $L$, i.e.,  
\begin{equation}
    \left\|\nabla l_\xi(\boldsymbol{\theta})-\nabla l_\xi\left(\boldsymbol{\theta}^{\prime}\right)\right\| \leq L\left\|\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime}\right\|,
\end{equation}
for all $\boldsymbol{\theta}, \boldsymbol{\theta}^{\prime} \in \mathbb{R}^d$.
\end{assumption}

%\wz{I like your explanation so I put it in the main body, however, how the other part uses lemma 2.6 is not clear. Maybe add one more sentence?}
Define the noisy update function in DPSGD as $\psi(\boldsymbol{\theta}_t)=\boldsymbol{\theta}_t-\frac{\eta}{b}\sum_{\xi \in \mathcal{B}_t} \operatorname{clip}_C\left(\nabla l_\xi\left(\boldsymbol{\theta}_t\right)\right)+\boldsymbol{\varrho}_t$, where $\boldsymbol{\varrho}_t\sim\mathcal{N}(\boldsymbol{0}, \beta\eta^2\sigma_{\text{DP}}^2\boldsymbol{I}_d)$ denotes the partial zero-mean Gaussian perturbation with a given constant $\beta\in(0,1)$. Then, we divide the original Gaussian noise $\mathcal{N}(\boldsymbol{0},\sigma_{\text{DP}}^2\boldsymbol{I}_d)$ into two parts: $\mathcal{N}(\boldsymbol{0},\beta\sigma_{\text{DP}}^2\boldsymbol{I}_d)$ and $\mathcal{N}(\boldsymbol{0},(1-\beta)\sigma_{\text{DP}}^2\boldsymbol{I}_d)$. The first part together with the stochastic gradient descent constitute the noisy update function, and \cref{lemma:smooth-reduct} presented shortly is used to measure its privacy. The other part aims for reducing the shift amount of the shifted R\'enyi divergence leveraging \cref{lemma:shifted-reduct}, and the privacy loss associated with it can be measured.

% \hao{\\1. $\alpha^*$ is defined in the title of \cref{table:compare}. I feel that defining it again here may be redundant, so I don't define it here. \\
% 2. I also think moving the footnote to the main body would be clearer, but when drafting the paper, I didn't figure out how to properly add this definition to the main body, so I temporarily put it in the footnote.\\
% 3. $\beta\in(0,1)$ is a constant used to divide Gaussian noise into two parts. \\
% 4. The reason why we need $\beta$ is that, in our proof, we will divide the original Gaussian noise $\mathcal{N}(0,\sigma^2\boldsymbol{I}_d)$ into two parts $\mathcal{N}(0,\beta\sigma^2\boldsymbol{I}_d)$ and $\mathcal{N}(0,(1-\beta)\sigma^2\boldsymbol{I}_d)$. The first part together with the stochastic gradient descent constitutes the noisy update function (as defined in the footnote), and \cref{lemma:smooth-reduct} is used to measure its privacy. The other part uses \cref{lemma:shifted-reduct} to measure the privacy level during the iteration process.
% }

Next, we introduce a key lemma that provides an upper bound on the shifted R\'enyi divergence for noisy update with general smooth loss functions. 

\begin{lemma}
    \label{lemma:smooth-reduct}
    \textup{(Noisy smooth-reduction).} Let $\psi$ and $\psi^\prime$ be two noisy update functions of DPSGD based on adjacent datasets $\mathcal{D}$ and $\mathcal{D}^\prime$. If the stochastic loss function is $L$-smooth (\cref{A:smooth-stochastic}), for any random variables $\boldsymbol{\mu}$ and $\boldsymbol{\nu}$, we have
    \begin{equation}
    \begin{aligned}
    &\mathcal{D}_\alpha^{((1+\eta L)z)}(\mathbb{P}_{\psi(\boldsymbol{\mu})}||\mathbb{P}_{\psi^\prime(\boldsymbol{\nu})})\\
    \leq \ & \mathcal{D}_\alpha^{(z)}(\mathbb{P}_{\boldsymbol{\mu}}||\mathbb{P}_{\boldsymbol{\nu}})+\frac{2\alpha C^2}{\beta nb\sigma_{\textup{DP}}^2}.
    \end{aligned}
    \end{equation}
    If we further assume $b\leq \frac{n}{5}$, $\alpha\leq\alpha^*(\frac{b}{n},\frac{b\sqrt{\beta}\sigma_{\textup{DP}}}{2C})$, and $\sigma_{\textup{DP}}>\frac{8C}{b\sqrt{\beta}}$, then
    \begin{equation}
    \begin{aligned}
        &\mathcal{D}_\alpha^{((1+\eta L)z)}(\mathbb{P}_{\psi(\boldsymbol{\mu})}||\mathbb{P}_{\psi^\prime(\boldsymbol{\nu})})\\
        \leq \ &\mathcal{D}_\alpha^{(z)}(\mathbb{P}_{\boldsymbol{\mu}}||\mathbb{P}_{\boldsymbol{\nu}})+\frac{8\alpha C^2}{\beta n^2\sigma_{\textup{DP}}^2}.
        \end{aligned}
    \end{equation}
\end{lemma}
\begin{proofsketch}
    We summarize the proof steps as follows. First, we transform the shifted R\'enyi divergence to the standard R\'enyi divergence utilizing the smoothness of losses and equivalent definitions of $\infty$-Wasserstein distance. Next, the post-processing and partial convexity inequalities allow us to derive the privacy loss associated with SGD sampling. Finally, we apply the strong composition lemma (\cref{lem:compos}) of RDP and obtain the privacy loss associated with two Gaussian distributions. The complete proof can be found in \cref{proof:smooth-reduct}.
\end{proofsketch}

This result generalizes the contraction-reduction lemma (\cref{lemma:contract-reduce}) and its variants \cite{altschuler2022privacy,altschuler2024privacy}, which all rely on the convexity of the loss function to ensure that the update function is contractive. It characterizes the privacy dynamics of shifted R\'enyi divergence for noisy stochastic updates with non-convex and smooth loss functions. Based on this building block, we now present the privacy guarantees for DPSGD-GC and DPSGD-DC, respectively.

\begin{theorem}
\label{thm:privacy-GC}
\textup{(Privacy guarantee for DPSGD-GC).} Given any number of iterations $T$, dataset size $n$, batch size $b$, stepsize $\eta$, constant $\beta\in(0,1)$, $\alpha> 1$, gradient clipping threshold $C$, and noise scale $\sigma_{\textup{DP}}$, if the stochastic loss function is $L$-smooth (\cref{A:smooth-stochastic}), then the DPSGD-GC algorithm satisfies $(\alpha,\varepsilon)$-RDP for
\begin{equation}
    \varepsilon = \frac{2\alpha C^2}{\beta nb\sigma_{\textup{DP}}^2}T.
\end{equation}
If we further assume $b\leq \frac{n}{5}$, $\alpha\leq\alpha^*(\frac{b}{n},\frac{b\sqrt{\beta}\sigma_{\textup{DP}}}{2C})$, and $\sigma_{\textup{DP}}>\frac{8C}{b\sqrt{\beta}}$, then
\begin{equation}
    \varepsilon = \frac{8\alpha C^2}{ \beta n^2\sigma_{\textup{DP}}^2}T.
\end{equation}
\end{theorem}

\begin{proofsketch}
We establish our result utilizing the induction hypothesis from $T$ to $0$ and the flexibility of the shifted R\'enyi divergence. For the base case at $t=0$, we have $\mathcal{D}_\alpha^{(z_0)}(\mathbb{P}_{\boldsymbol{\theta}_0}||\mathbb{P}_{\boldsymbol{\theta}_0^\prime})=0$ since the initializations satisfy $\boldsymbol{\theta}_0=\boldsymbol{\theta}^\prime_0$. For the inductive step, we apply the noisy smooth-reduction lemma (\cref{lemma:smooth-reduct}) and subsequently reduce the shift amount using shift-reduction lemma (\cref{lemma:shifted-reduct}) with auxiliary variables to derive the recurrence relationship. Finally, by tracking the privacy loss across all iterations, we derive an upper bound on the RDP loss. The complete proof can be found in \cref{proof:privacy-GC}.
\end{proofsketch} 


\begin{theorem}
\label{thm:privacy-DC}
\textup{(Privacy guarantee for DPSGD-DC).} Given any number of iterations $T$, dataset size $n$, batch size $b$, stepsize $\eta$, constant $\beta\in(0,1)$, $\alpha>1$, gradient clipping threshold $C$, diameter of parameter domain $D$, and noise scale $\sigma_{\textup{DP}}$, if the stochastic loss function is $L$-smooth (\cref{A:smooth-stochastic}), then the DPSGD-GC algorithm satisfies $(\alpha,\varepsilon)$-RDP for
\begin{equation}
    \varepsilon=\frac{2\alpha C^2}{\beta nb\sigma_{\textup{DP}}^2}+\frac{\alpha (1+\eta L)^2D^2}{2\eta^2\sigma_{\textup{DP}}^2(1-\beta)}.
\end{equation}
If we further assume $b\leq \frac{n}{5}$, $\alpha\leq\alpha^*(\frac{b}{n},\frac{b\sqrt{\beta}\sigma_{\textup{DP}}}{2C})$, and $\sigma_{\textup{DP}}>\frac{8C}{b\sqrt{\beta}}$, then
\begin{equation}
    \varepsilon=\frac{8\alpha C^2}{ \beta n^2\sigma_{\textup{DP}}^2}+\frac{\alpha (1+\eta L)^2D^2}{2\eta^2\sigma_{\textup{DP}}^2(1-\beta)}.
\end{equation}
\end{theorem}

\begin{proofsketch}
    This proof follows a similar approach to \cref{thm:privacy-GC} but terminates the induction early at time step $\tau$. By judiciously setting the shift amount $z_\tau$ as $D$ at $t=\tau$, we obtain $\mathcal{D}_\alpha^{(z_\tau)}(\mathbb{P}_{\boldsymbol{\theta}_\tau}||\mathbb{P}_{\boldsymbol{\theta}_\tau^\prime})=0$ due to  the bounded domain assumption. Finally, by appropriately configuring the values of the auxiliary shift variables, we derive the converged privacy loss under the bounded domain restriction, as detailed in \cref{proof:privacy-DC}.
\end{proofsketch}

\begin{remark}
    Comparing \cref{thm:privacy-GC} and \cref{thm:privacy-DC}, we observe that the privacy loss can still converge to a constant for non-convex smooth losses when the domain is bounded by $D$. However, unlike the convex case \cite{altschuler2022privacy, altschuler2024privacy}, where the upper bound scales linearly with $D$, the non-convex setting results in a bound that scales quadratically with $D$. This suggests that privacy bounds in non-convex scenarios are inherently looser than those in convex cases, consistent with recent findings \cite{kong2024privacy, chien2024convergent}.
\end{remark}




\subsection{Utility Analysis of DPSGD}
Next, we provide the convergence bounds (utility) for DPSGD-GC and DPSGD-DC under $(\alpha,\varepsilon)$-RDP constraint, respectively. All bounds are expressed as expectations over the randomness of SGD and Gaussian noise. Our utility analysis builds upon the results of Bassily et al. \yrcite{bassily2014private}, Zhang et al. \yrcite{zhang2020improved}, and Koloskova et al. \yrcite{koloskova2023revisiting}.

\begin{assumption}
\label{A:bound-SGD}
\textup{(Bounded SGD variance).} The stochastic gradient $\nabla l_{\xi}$ has bounded variance, that is, for all $\boldsymbol{\theta}\in\mathbb{R}^d$, we have
%\wz{distinction of $\nabla l_{\xi}$ and $\nabla l$ seems not define before. In algorithm 1, we have $\nabla l$ as input loss function, but we have $\nabla l_{\xi}$ inside the clip function. }

%\hao{In this paper, we focus on solving the following stochastic optimization problem:
%\begin{equation}
%     \min _{\boldsymbol{\theta} \in \mathbb{R}^d}\left\{l(\boldsymbol{\theta}):=\mathbb{E}_{\xi \sim \mathbb{P}_\mathcal{D}}\left[l_{\xi}(\boldsymbol{\theta})\right]\right\}
% \end{equation}
% where $l_\xi$ denotes the stochastic loss function, i.e., the loss of model $\boldsymbol{\theta}$ on the datapoint $\xi$. \\
% In order to clearly distinguish between the stochastic loss function $l_\xi$ and the (expected) loss function $l$, we modified \cref{alg:DPSGD} and added the definition of the loss function in this subsection (see the following sentence of \cref{A:smooth-stochastic}.
% }
\begin{equation}
    \mathbb{E}_{\xi\sim\mathbb{P}_{\mathcal{D}}}\left[\|\nabla l_\xi(\boldsymbol{\theta})-\nabla l(\boldsymbol{\theta})\|^2\right]\leq \sigma_{\text{SGD}}^2.
\end{equation}
\end{assumption}

Note that \cref{A:smooth-stochastic} implies that the loss function $l(\cdot)=\mathbb{E}_{ \xi}[l_{\xi}(\cdot) ]$ is also smooth with constant $L>0$. Based on \cref{A:smooth-stochastic} and \cref{A:bound-SGD}, the following lemma provides an upper bound on the minimum expected norm of the gradient for DPSGD-GC with smooth loss functions.
\begin{lemma}
\label{lem:converge-GC}
    \textup{(Convergence result of DPSGD-GC \cite{koloskova2023revisiting}).} Assume that the loss function $l(\cdot)$ has smoothness parameter $L$ and SGD variance at most $\sigma_{\textup{SGD}}^2$ (\cref{A:bound-SGD}). When running DP-SGD-GC for $T$ steps with step-size $\eta\leq \frac{1}{9L}$, the minimum expected norm of the gradient $\min_{t\in[0,T]} \mathbb{E}[\|\nabla l(\boldsymbol{\theta}_t)\|]$ is upper bounded by
    \begin{equation}
    \begin{aligned}
    &\min_{t\in[0,T]} \mathbb{E}\left[\|\nabla l(\boldsymbol{\theta}_t)\|\right] \\
    &\leq \mathcal{O} \biggl(\frac{1}{\eta CT}+\frac{1}{\sqrt{\eta T}}+\min \bigl(\sigma_{\textup{SGD}}, \frac{\sigma_{\textup{SGD}}^2}{C}\bigr)\\
    &+\sqrt{\eta L} \frac{\sigma_{\textup{SGD}}}{\sqrt{b}}+\frac{dL \eta}{C} \sigma_{\textup{DP}}^2+\sqrt{d L \eta} \sigma_{\textup{DP}}\biggr),
    \end{aligned}
    \end{equation}
\end{lemma}

\begin{assumption}
\label{A:strong-convex}
\textup{($\mu$-strongly convex).} The loss function $l(\cdot)$ is strongly convex with constant $\mu > 0$ if and only if the following inequality holds
\begin{equation}
    \left[\nabla l(\boldsymbol{\theta})-\nabla l\left(\boldsymbol{\theta}^{\prime}\right)\right]^{\top}\left(\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime}\right) \geq \mu\left\|\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime}\right\|^2, 
\end{equation}
for all $\boldsymbol{\theta}, \boldsymbol{\theta}^{\prime} \in \mathbb{R}^d$.
\end{assumption}

For DPSGD-DC, we focus on smooth and strongly convex losses and obtain the following result.
\begin{theorem}
\label{thm:converge-DC}
\textup{(Convergence result of DPSGD-DC).} Assume that the loss function $l(\cdot)$ has smoothness parameter $L$, strongly convex parameter $\mu$ (\cref{A:strong-convex}), SGD variance at most $\sigma_{\textup{SGD}}^2$ (\cref{A:bound-SGD}), and $\boldsymbol{\theta}^*=\arg\min_{\boldsymbol{\theta}}l(\boldsymbol{\theta})\in \operatorname{int} \mathcal{K}$. When running DPSGD-DC for $T$ steps with step-size $\eta\leq \frac{9}{20L}$, the convergence in terms of $\min_{t\in[0,T]} \mathbb{E}\left[\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}\right]$ is upper bounded by
\begin{equation}
\begin{aligned}
    &\min_{t\in[0,T]} \mathbb{E}\left[\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}\right] \\ 
    &\leq \mathcal{O}\biggl(\frac{\sqrt{L}D^2}{\eta C T}+\frac{D}{\sqrt{\eta  T}}+ \min\bigl(\frac{L^{3/4}}{\mu^{5/4}}\sigma_{\textup{SGD}},\sqrt{\frac{\sigma_{\textup{SGD}}^3}{\mu C}}\bigr) \\
    &+\frac{\sqrt{\eta}\sigma_{\textup{SGD}}}{\sqrt{b}}+\frac{d\eta\sigma_{\textup{DP}}^2\sqrt{L}}{C}+\sqrt{d\eta}\sigma_{\textup{DP}}\biggr),
\end{aligned}
\end{equation}
\end{theorem}
\begin{proofsketch}
    We present an intuitive proof sketch below, with the complete proof provided in \cref{proof:converge-DC}. The primary challenges stem from the gradient clipping operation, the SGD procedure, and the parameter projection step. To address these, we divide the proof into separate cases. For instance, in the case where the clipping threshold $C\leq10\sigma_{\text{SGD}} (\frac{L}{\mu})^{\frac{1}{2}}$ and $\|\nabla l(\boldsymbol{\theta}_t)\|\geq 35\sigma_{\text{SGD}} (\frac{L}{\mu})^{\frac{3}{4}}$, first, we leverage the non-expansiveness of the projection operator to analyze the impact of the parameter projection. Next, we introduce an auxiliary clipping factor $\gamma_\xi=\min (1, \frac{C}{\|\nabla l_\xi(\boldsymbol{\theta}_t)\|})$ and apply the Markov inequality to quantify the effects of clipped SGD. Through careful step-size design and manipulations, we obtain a recurrence relation that characterizes the evolution over two successive time steps. Finally, by averaging over $t$, we derive an upper bound for this case. The proof for other cases follows a similar structure but incorporates different auxiliary variables tailored to the true gradient. By summing up all cases, we derive an upper bound on the error for DPSGD-DC.
\end{proofsketch} 

\begin{remark}
    Note that our results use a different convergence metric, $\min_{t\in[0,T]} \mathbb{E}[\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}]$, instead of the more commonly used $\min_{t\in[0,T]} \mathbb{E}[l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)]$, as it better facilitates our derivation and gaining insights into the analytical results.
\end{remark}

Using our RDP guarantees in \cref{thm:privacy-GC} and \cref{thm:privacy-DC}, we immediately obtain the following results.

\begin{corollary}
\textup{(Privacy-utility trade-off for DPSGD-GC).} Assuming that the conditions in \cref{lem:converge-GC} are satisfied, for $\boldsymbol{\theta}_T$ output by DPSGD-GC with $L$-smooth losses, we have the following results.
\begin{itemize}
    \item For $\sigma_{\textup{DP}}^2=\mathcal{O}\bigl(\frac{\alpha C^2 T}{\varepsilon nb}\bigr)$, we have the following inequality
    \begin{equation}
    \begin{aligned}
        &\min_{t\in[0,T]} \mathbb{E}\left[\|\nabla l(\boldsymbol{\theta}_t)\|\right] \\
        &\leq \mathcal{O} \biggl(\frac{1}{\eta CT}+\frac{1}{\sqrt{\eta T}}+\min \bigl(\sigma_{\textup{SGD}}, \frac{\sigma_{\textup{SGD}}^2}{C}\bigr)\\
        &+\sqrt{\eta L} \frac{\sigma_{\textup{SGD}}}{\sqrt{b}}+\frac{d\alpha CL\eta T}{\varepsilon n b}+\frac{\sqrt{d\alpha L\eta T}C}{\sqrt{\varepsilon nb}}\biggr).
    \end{aligned}
    \end{equation}
    \item If we further assume $b\leq \frac{n}{5}$, $\alpha\leq\alpha^*(\frac{b}{n},\frac{b\sqrt{\beta}\sigma_{\textup{DP}}}{2C})$, $\sigma_{\textup{DP}}>\frac{8C}{b\sqrt{\beta}}$, and let $\sigma_{\textup{DP}}^2=\mathcal{O}\bigl(\frac{\alpha C^2 T}{\varepsilon n^2}\bigr)$, then we have the following inequality
    \begin{equation}
    \begin{aligned}
        &\min_{t\in[0,T]} \mathbb{E}\left[\|\nabla l(\boldsymbol{\theta}_t)\|\right] \\
        &\leq \mathcal{O} \biggl(\frac{1}{\eta CT}+\frac{1}{\sqrt{\eta T}}+\min \bigl(\sigma_{\textup{SGD}}, \frac{\sigma_{\textup{SGD}}^2}{C}\bigr)\\
        &+\sqrt{\eta L} \frac{\sigma_{\textup{SGD}}}{\sqrt{b}}+\frac{d\alpha CL\eta T}{\varepsilon n^2}+\frac{\sqrt{d\alpha L\eta T}C}{\sqrt{\varepsilon}n}\biggr).
    \end{aligned}
    \end{equation}
\end{itemize}
\end{corollary}


\begin{corollary}
\textup{(Privacy-utility trade-off for DPSGD-DC).} Assuming that the conditions in \cref{thm:converge-DC} are satisfied, for $\boldsymbol{\theta}_T$ output by DPSGD-DC with $L$-smooth and $\mu$-strongly convex losses, we have the following results.
\begin{itemize}
\label{coro:utility-DC}
    \item For $\sigma_{\textup{DP}}^2=\mathcal{O}\bigl(\frac{\alpha}{\varepsilon}(\frac{(1+\eta L)^2D^2}{\eta^2}+\frac{C^2}{nb})\bigr)$, we have the following inequality
    \begin{equation}
    \begin{aligned}
        &\min_{t\in[0,T]} \mathbb{E}\left[\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}\right] \\ 
        &\leq \mathcal{O}\biggl(\frac{\sqrt{L}D^2}{\eta C T}+\frac{D}{\sqrt{\eta  T}}+ \min\bigl(\frac{L^{3/4}}{\mu^{5/4}}\sigma_{\textup{SGD}},\sqrt{\frac{\sigma_{\textup{SGD}}^3}{\mu C}}\bigr) \\
        &+\frac{\sqrt{\eta}\sigma_{\textup{SGD}}}{\sqrt{b}}+\frac{d\alpha\eta\sqrt{L}}{\varepsilon C}(\frac{(1+\eta L)^2D^2}{\eta^2}+\frac{C^2}{nb})\\
        &+\sqrt{\frac{d\alpha\eta}{\varepsilon}(\frac{(1+\eta L)^2D^2}{\eta^2}+\frac{C^2}{nb})}\biggr).
    \end{aligned}
    \end{equation}
    \item If we further assume $b\leq \frac{n}{5}$, $\alpha\leq\alpha^*(\frac{b}{n},\frac{b\sqrt{\beta}\sigma_{\textup{DP}}}{2C})$, $\sigma_{\textup{DP}}>\frac{8C}{b\sqrt{\beta}}$, and let $\sigma_{\textup{DP}}^2=\mathcal{O}\bigl(\frac{\alpha}{\varepsilon}(\frac{(1+\eta L)^2D^2}{\eta^2}+\frac{C^2}{n^2})\bigr)$, then we have the following inequality
    \begin{equation}
    \begin{aligned}
        &\min_{t\in[0,T]} \mathbb{E}\left[\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}\right] \\ 
        &\leq \mathcal{O}\biggl(\frac{\sqrt{L}D^2}{\eta C T}+\frac{D}{\sqrt{\eta  T}}+ \min\bigl(\frac{L^{3/4}}{\mu^{5/4}}\sigma_{\textup{SGD}},\sqrt{\frac{\sigma_{\textup{SGD}}^3}{\mu C}}\bigr) \\
        &+\frac{\sqrt{\eta}\sigma_{\textup{SGD}}}{\sqrt{b}}+\frac{d\alpha\eta\sqrt{L}}{\varepsilon C}(\frac{(1+\eta L)^2D^2}{\eta^2}+\frac{C^2}{n^2})\\
        &+\sqrt{\frac{d\alpha\eta}{\varepsilon}(\frac{(1+\eta L)^2D^2}{\eta^2}+\frac{C^2}{n^2})}\biggr).
    \end{aligned}
    \end{equation}
\end{itemize}
\end{corollary}

% \begin{figure*}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{
% \includegraphics[width=\columnwidth]{privacy_level.pdf} 
% \includegraphics[width=\columnwidth]{train_loss.pdf}
% }
% \caption{The evolution of the (a) privacy level and (b) training loss during DPSGD-GC with different batch sizes. The shaded error bars correspond to intervals covering 95\% of the realized values, obtained from the 10 Monte Carlo trials. Note that the privacy and utility bounds in terms of the number of epochs, $E$, can be derived by substituting $T=\lceil \frac{n}{b}\rceil E$ into our main results.}
% \label{fig:batch}
% \end{center}
% \vskip -0.2in
% \end{figure*}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{
\subfigure[]{\includegraphics[width=0.45\textwidth]{privacy_level.pdf}}
\subfigure[]{\includegraphics[width=0.45\textwidth]{train_loss.pdf}}
}
\caption{The evolution of the: (a) privacy level and (b) training loss during DPSGD-GC with different batch sizes. The shaded error bars correspond to intervals covering $95$\% of the realized values, obtained from the $10$ Monte Carlo trials. Note that the privacy and utility bounds in terms of the number of epochs, $E$, can be derived by substituting $T=\lceil \frac{n}{b}\rceil E$ into our main results.}
\label{fig:batch}
\end{center}
\vskip -0.2in
\end{figure*}

Corollary \ref{coro:utility-DC} suggests that the utility bound for DPSGD-DC comprises six terms. The first two terms capture optimization-related factors, reflecting the influence of clipping and projection on convergence behavior. The third term accounts for the inherent bias introduced by gradient clipping, while the fourth term reflects the stochastic noise resulting from SGD sampling. Finally, the last two terms quantify the impact of the injected DP noise. Unlike previous works on DPSGD-DC \cite{altschuler2022privacy,altschuler2024privacy, kong2024privacy, chien2024convergent}, which primarily focused on RDP analysis without considering convergence performance, our results explicitly demonstrate how clipping and projection factors affect the utility of DPSGD-DC.

Note that the utility bounds can also be expressed in terms of the standard DP parameter $(\epsilon, \delta)$ by applying the conversion in \cref{lem:RDP2DP}.

%Specifically, we assume  $0<\epsilon<2\log(1/\delta)$ and $0<\delta<1$. According to \cref{lem:RDP2DP}, $(\alpha, \varepsilon)$-RDP implies $(\epsilon,\delta)$-DP, where $\alpha=1+\frac{2}{\epsilon}\log(1/\delta)$ and $\varepsilon=\epsilon/2$. By applying this conversion, we can derive the utility bounds under the $(\epsilon,\delta)$-DP guarantee.
\begin{remark}
    From \cref{thm:privacy-DC}, we establish a constant upper bound on privacy for DPSGD-DC based on the bounded domain diameter $D$, where smaller value of $D$ yields tighter privacy guarantees. Meanwhile, \cref{coro:utility-DC} demonstrates that when the conditions in \cref{thm:converge-DC} are satisfied, a smaller $D$ also leads to a lower upper bound on the convergence. It thus follows that reducing $D$ can enhance the utility guarantee in terms of the privacy-convergence trade-off under certain conditions.
\end{remark}


\section{Experiment Evaluation}
\label{s4}
In this section, we present empirical results estimating the privacy level via MIA. Following Kairouz et al. \yrcite{kairouz2015composition}, we estimate $(\epsilon,\delta)$-DP using the false positive rate (FPR) and false negative rate (FNR) of its attack model on the test data, applying the following formula:
%Specifically, we follow the approach of Kairouz et al. \yrcite{kairouz2015composition} to estimate the $(\epsilon,\delta)$-DP in terms of the false positive rate (FPR) and false negative rate (FNR). This procedure enables us to estimate the privacy parameter using the formula:
%\footnote{As mentioned before, the equivalent $(\epsilon,\delta)$-DP bound can be derived by using \cref{lem:RDP2DP}.}: 
\begin{equation}
    \hat{\epsilon} = \max \biggl\{\log \frac{1-\delta-\text{FPR}}{\text{FNR}},\log \frac{1-\delta-\text{FNR}}{\text{FPR}}\biggr\}.
\end{equation}

We train a private ResNet-18 network \cite{he2016deep} as the target model using the Opacus library \cite{yousefpour2021opacus} on the standard CIFAR10 dataset \cite{krizhevsky2009learning}. According to standard MIA protocols \cite{shokri2017membership}, the training and test sets for each target and shadow model are randomly selected, equal in size, and mutually disjoint. We set the training set size to $10, 000$. While the target model's dataset does not overlap with those of the shadow models, different shadow models may partially share the same data. The shadow models are trained using the same architectures as the target model. For the attack model, we employ a two-layer multilayer perceptron (MLP) with $50$ hidden nodes and ReLU as activation functions. %The goal is to validate our theoretical results.

We emphasize that the MIA serves primarily as a tool to provide a lower bound on privacy, capturing the trend with privacy level changes and validating the consistency of the theoretical bounds. By comparing experimental results with the theoretical trends, we aim for demonstrating the reasonableness of the derived privacy bounds, rather than offering an exact measure of privacy leakage. This approach allows us to examine how privacy evolves with varying experimental conditions under the same privacy attack.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{privacy_level_with_D.pdf}}
\caption{The evolution of the privacy level during DPSGD-DC with different diameters of bounded domain. The red and blue lines correspond to the cases with batch sizes of $100$ and $400$, respectively.}
\label{fig:D}
\end{center}
\vskip -0.2in
\end{figure}

\textbf{Implementation details.} For the target model, we use the SGD optimizer with the learning rate $\eta=0.1$, the noise level $\sigma_{\text{DP}}=0.002$, the clipping threshold $C=20$, and the confidence level $\delta=10^{-5}$. We train $10$ shadow models to simulate the behavior of the target model. For the attack classifier, we use the SGD optimizer with the initial learning rate of $0.01$, the weight decay of $5\times 10^{-4}$, and the momentum of $0.9$. The mini-batch size during training is set to be $100$. We predict the labels by selecting from the model's output the class with the highest probability. Experimental results are reported by averaging over $10$ Monte Carlo trials. Our experiments are implemented in the PyTorch \cite{paszke2017automatic} framework.

\subsection{Effect of the Batch Size}
We conduct experiments on DPSGD-GC with various batch sizes $b\in \{100, 200, 300, 400, 600, 1000\}$. \Cref{fig:batch} illustrates the evolution of the estimated privacy level and training loss per epoch. As expected, the estimated privacy level 
$\epsilon$ increases with the number of epochs, and larger batch sizes provide stronger privacy protection. These observations align with our theoretical results in \cref{thm:privacy-GC}. Additionally, DPSGD-GC converges more slowly with larger batch sizes, consistent with \cref{lem:converge-GC}, further highlighting the trade-off between privacy and convergence.




\subsection{Effect of the Bounded Domain Diameter}

We then conduct experiments on DPSGD-DC using various diameters for the bounded domain $D\in\{20,60,100\}$ with batch sizes of $100$ and $400$, respectively. The estimated privacy parameter of DPSGD-DC for different bounded domain diameters $D$ is shown in \cref{fig:D}. As can be observed, DPSGD-DC provides stronger privacy guarantees with a smaller $D$, as limiting the domain diameter restricts the range of parameter variations to a narrower interval. Additionally, privacy leakage tends to stabilize as the number of epochs increases. This observation is not surprising, as the bounded domain assumption provides a constant upper privacy bound for DPSGD-DC, as demonstrated in \cref{thm:privacy-DC}.  



\section{Conclusions}
\label{s5}

In this paper, we have rigorously analyzed the privacy and utility guarantee of DPSGD, considering both gradient clipping (DPSGD-GC) and double clipping (DPSGD-DC). Our analysis extends the existing privacy bounds of DPSGD-GC and DPSGD-DC to general smooth and non-convex problems without relying on other assumptions. While previous works have focused solely on privacy characterization, we have also derived utility bounds corresponding to our RDP guarantees. This dual characterization admits a more comprehensive understanding of the privacy-utility trade-offs in DPSGD, providing valuable insights for developing more effective differentially private optimization algorithms.

Our work contributes towards the role that  a deeper understanding of parameter projection and gradient clipping play in DPSGD-based algorithms. This has implications for both current applications and future developments in differentially private optimization.

\textbf{Future work.} Our work implies several promising directions. First, refining the tightness of analytical results remains an open challenge. Another interesting direction is to extend this analysis to non-smooth loss functions and other optimization techniques like Adam and RMSProp, which are used notably in deep learning. Lastly, our analysis may be integrated with other optimization frameworks, such as gradient compression, distributed optimization, and federated learning.


% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}











%restatable



% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\section*{Impact Statement}

The goal of this paper is to advance the understanding of the privacy-utility trade-offs in DPSGD. As DPSGD is an essential component of introducing differential privacy in machine learning, our work strengthens data privacy protection by establishing theoretically guaranteed bounds. 

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. 


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Preliminaries}

\subsection{Standard Facts}

To prove our main results, we first introduce the following standard facts.

\begin{lemma}
\label{lem:markov}
\textup{(Markov inequality).} If $x$ is a non-negative random variable and $a > 0$, then we have
\begin{equation}
    \operatorname{Pr}(x\geq a)\leq \frac{\mathbb{E}(x)}{a}.
\end{equation}
\end{lemma}

\begin{lemma}
\label{lem:project}
\textup{(Projection operator is non-expansive).} For any $\boldsymbol{x}, \boldsymbol{y}\in \mathbb{R}^d$, we have
\begin{equation}
    \|\Pi_\mathcal{K}(\boldsymbol{x})-\Pi_\mathcal{K}(\boldsymbol{y})\| \leq \|\boldsymbol{x}-\boldsymbol{y}\|,
\end{equation}
where $\Pi_{\mathcal{K}}(\boldsymbol{x})= \arg\min_{\boldsymbol{z}\in\mathcal{K}}\|\boldsymbol{x}-\boldsymbol{z}\|$ denotes the projection operator.
\end{lemma}

\begin{lemma}
\label{lem:(a+b)2}
    For any $a,b \in \mathbb{R}$, it holds that
    \begin{equation}
        (a+b)^2\leq 2a^2+2b^2.
    \end{equation}
\end{lemma}

\begin{lemma}
\label{lem:x2x}
    For any $a,b \geq0$, it holds that
    \begin{equation}
        a\leq \frac{a^2}{2b}+\frac{b}{2}.
    \end{equation}
\end{lemma}

\begin{lemma}
\label{lem:sqrt(a+b)}
    For any $a,b \geq0$, it holds that
    \begin{equation}
        \sqrt{a+b}\leq \sqrt{a}+\sqrt{b}.
    \end{equation}
\end{lemma}

\subsection{Lemmas for Privacy Analysis}

We then provide some supporting lemmas which are useful for our proof. First, the following lemma provides equivalent characterizations of the $\infty$-Wasserstein distance:
\begin{definition}
\label{def:wass}
    \textup{($\infty$-Wasserstein distance \cite{villani2009wasserstein}).} The $\infty$-Wasserstein distance between two distributions $\mathbb{P}_{\boldsymbol{\mu}}$ and $\mathbb{P}_{\boldsymbol{\nu}}$ is defined as
\begin{equation}
    W_{\infty}(\mathbb{P}_{\boldsymbol{\mu}}, \mathbb{P}_{\boldsymbol{\nu}})=\inf_{\gamma \in \Gamma(\mathbb{P}_{\boldsymbol{\mu}}, \mathbb{P}_{\boldsymbol{\nu}})} \operatorname{ess \ sup}_{(\boldsymbol{x}, \boldsymbol{y}) \sim \gamma}\|\boldsymbol{x}-\boldsymbol{y}\|,
\end{equation}
where $(\boldsymbol{x},\boldsymbol{y})\sim \gamma$ indicates that the essential supremum is taken with respect to the joint distribution $\gamma$, and $\Gamma(\mathbb{P}_{\boldsymbol{\mu}},\mathbb{P}_{\boldsymbol{\nu}})$ represents the collection of the joint distribution with $\mathbb{P}_{\boldsymbol{\mu}}$ and $\mathbb{P}_{\boldsymbol{\nu}}$ as marginals.
\end{definition} 

\begin{lemma} 
\label{lem:W-distance} 
\textup{(Equivalent definitions of $\infty$-Wasserstein distance \cite{feldman2018privacy}).} The following are equivalent for any distributions $\mathbb{P}_{\boldsymbol{\mu}}$ and $\mathbb{P}_{\boldsymbol{\nu}}$:
\begin{enumerate}
\item $W_\infty(\mathbb{P}_{\boldsymbol{\mu}},\mathbb{P}_{\boldsymbol{\nu}})\leq s$.
\item There exist jointly distributed random variables $(\boldsymbol{u}, \boldsymbol{v})$ such that $\boldsymbol{u}\sim \mathbb{P}_{\boldsymbol{\mu}}$, $\boldsymbol{v}\sim \mathbb{P}_{\boldsymbol{\nu}}$, and $\operatorname{Pr}[\|\boldsymbol{u}-\boldsymbol{v}\|\leq s]=1$.
\item There exist jointly distributed random variables $(\boldsymbol{u},\boldsymbol{w})$ such that $\boldsymbol{u}\sim \mathbb{P}_{\boldsymbol{\mu}}$, $\boldsymbol{u}+\boldsymbol{w}\sim \mathbb{P}_{\boldsymbol{\nu}}$, and $\operatorname{Pr}[\|\boldsymbol{w}\|\leq s]=1$.
\end{enumerate}
\end{lemma}

Further, R\'enyi divergence has several fundamental properties, including non-negative and non-decreasing with respect to $\alpha$. One such key property is the post-processing inequality, which we formalize in the following lemma:
\begin{lemma}
\label{lem:post-process} 
\textup{(Post-processing inequality \cite{van2014renyi}).} Let $\boldsymbol{\mu}$ and $\boldsymbol{\nu}$ be two random variables. Then, for any (possibly random) function $\psi$ and $\alpha\geq0$, we have
\begin{equation}
\mathcal{D}_\alpha(\mathbb{P}_{\psi(\boldsymbol{\mu})}||\mathbb{P}_{\psi(\boldsymbol{\nu})})\leq\mathcal{D}_\alpha(\mathbb{P}_{\boldsymbol{\mu}}||\mathbb{P}_{\boldsymbol{\nu}}).
\end{equation}
\end{lemma}
The following lemma demonstrates the partial convexity property of R\'enyi divergence.
\begin{lemma}
\label{lem:convex}
    \textup{(Partial convexity in the second argument \cite{van2014renyi}).} Let $\boldsymbol{\mu}$, $\boldsymbol{\nu}_1$, and $\boldsymbol{\nu}_2$ be three random variables. Then, for any $\alpha\geq0$, the R\'enyi divergence is convex in its second argument, that is, the following inequality holds
    \begin{equation}
    \mathcal{D}_\alpha(\mathbb{P}_{\boldsymbol{\mu}}||(1-\lambda)\mathbb{P}_{\boldsymbol{\nu}_1}+\lambda\mathbb{P}_{\boldsymbol{\nu}_2})\leq (1-\lambda)\mathcal{D}_\alpha(\mathbb{P}_{\boldsymbol{\mu}}||\mathbb{P}_{\boldsymbol{\nu}_1})+\lambda\mathcal{D}_\alpha(\mathbb{P}_{\boldsymbol{\mu}}||\mathbb{P}_{\boldsymbol{\nu}_2}),
    \end{equation}
    for any $0\leq\lambda\leq 1$.
\end{lemma}
The third property is the composition of two RDP mechanisms, as follows:
\begin{lemma}
\label{lem:compos}
    \textup{(Strong composition for R\'enyi divergence \cite{mironov2017renyi}).} Let $\mathcal{M}=\left[\mathcal{M}_1(\mathcal{D}),\mathcal{M}_2\left(\mathcal{D}\right)\right]$ be a sequence of two randomized mechanisms. Then, for any $\alpha>1$, we have
    \begin{equation}
    \mathcal{D}_\alpha(\mathbb{P}_{\mathcal{M}(\mathcal{D})}||\mathbb{P}_{\mathcal{M}(\mathcal{D}^\prime)})\leq \sup_{\boldsymbol{v}} \mathcal{D}_\alpha(\mathbb{P}_{\mathcal{M}_2(\mathcal{D})|\mathcal{M}_1(\mathcal{D})=\boldsymbol{v}}||\mathbb{P}_{\mathcal{M}_2(\mathcal{D}^\prime)|\mathcal{M}_1(\mathcal{D}^\prime)=\boldsymbol{v}})+\mathcal{D}_\alpha(\mathbb{P}_{\mathcal{M}_1(\mathcal{D})}||\mathbb{P}_{\mathcal{M}_1(\mathcal{D}^\prime)}),
    \end{equation}
    where $\mathcal{D}$ and $\mathcal{D}^\prime$ are two adjacent datasets.
\end{lemma}

Finally, we briefly introduce a concise statement of the RDP analysis for the sampled Gaussian mechanism (SGM), which can simplify our analysis under additional parameter assumptions. The SGM is a composition of subsampling and the additive Gaussian noise, whose formal definition is given as follows:
\begin{definition}
\textup{(Sampled Gaussian mechanism).} Let $f:\mathcal{D}\rightarrow\mathbb{R}^d$ and $\mathcal{S}$ be a sample from $[n]$ where each $i\in [n]$ is chosen independently with probability $0<q\leq1$ and $n=|\mathcal{D}|$. The SGM, parameterized with the noise scale $\sigma>0$, is defined as 
\begin{equation}
\begin{aligned}
\mathcal{M}_{\text{SGM}}(\mathcal{D}) = \sum_{i\in \mathcal{S}} f(\mathcal{D}_i) + \mathcal{N}(\boldsymbol{0},\sigma^2\boldsymbol{I}_d),
\end{aligned}
\end{equation}
where $\mathcal{D}_i$ denotes a single element of the dataset $\mathcal{D}$. For R\'enyi parameter $\alpha>1$, the R\'enyi divergence of SGM is defined as
\begin{equation}
    \mathcal{D}_\alpha^{\text{SGM}}(q, \sigma)=\mathcal{D}_\alpha\left(\mathcal{N}(0,\sigma^2)||(1-q)\mathcal{N}(0,\sigma^2)+q\mathcal{N}(1,\sigma^2)\right).
\end{equation}
\end{definition}

The following lemma provides a closed-form upper bound on $\mathcal{D}_\alpha^{\text{SGM}}(q, \sigma)$:
\begin{lemma}
\label{lem:SGM}
    \textup{(Upper bound on R\'enyi divergence of the SGM).} If $q\leq 1/5$, $\sigma>4$, and $\alpha\leq\alpha^*(q,\sigma)$, i.e.,
    \begin{equation}
    \begin{aligned}
        \alpha &\leq K \sigma^2/2-2\log \sigma, \\
        \alpha &\leq\left(K^2 \sigma^2 / 2-\log 5-2\log \sigma\right) /\left(K+\log (q \alpha)+1 /(2 \sigma^2)\right),
    \end{aligned}
    \end{equation}
    where $K=\log(1+1/(q(\alpha-1)))$, then $\mathcal{D}_\alpha^{\text{SGM}}(q, \sigma)\leq \frac{2\alpha q^2}{\sigma^2}$.
\end{lemma}



\subsection{Lemmas for Utility Analysis}

The following results provide several equivalent characterizations of the $L$-smoothness property for functions that are also convex:
\begin{lemma}
\label{lem:smooth}
\textup{(Implications of smoothness \cite{beck2017first}).} Let $f:\mathbb{R}^d\rightarrow\mathbb{R}$ be a convex function, differentiable over $\mathbb{R}^d$. Then, the following claims are equivalent:
\begin{itemize}
    \item $f$ is $L$-smooth.
    \item $f(\boldsymbol{y})\leq f(\boldsymbol{x})+\nabla f(\boldsymbol{x})^T(\boldsymbol{y}-\boldsymbol{x})+\frac{L}{2}\|\boldsymbol{x}-\boldsymbol{y}\|^2$ for all $\boldsymbol{x}, \boldsymbol{y}\in\mathbb{R}^d$.
    \item $f(\boldsymbol{y})\geq f(\boldsymbol{x})+\nabla f(\boldsymbol{x})^T(\boldsymbol{y}-\boldsymbol{x})+\frac{1}{2L}\|\nabla f(\boldsymbol{x})-\nabla f(\boldsymbol{y})\|^2$ for all $\boldsymbol{x}, \boldsymbol{y}\in\mathbb{R}^d$.
\end{itemize}
\end{lemma}


\cref{lem:strong-convex} below describes some useful properties of strong convexity.
\begin{lemma}
\label{lem:strong-convex}
\textup{(Implications of strong convexity \cite{beck2017first}).} Let $f:\mathbb{R}^d\rightarrow\mathbb{R}$ be a proper closed and convex function, differentiable over $\mathbb{R}^d$. Then, for a given $\mu>0$, the following claims are equivalent:
\begin{itemize}
    \item $f$ is $\mu$-strongly convex.
    \item $f(\boldsymbol{y})\geq f(\boldsymbol{x})+\nabla f(\boldsymbol{x})^T(\boldsymbol{y}-\boldsymbol{x})+\frac{\mu}{2}\|\boldsymbol{y}-\boldsymbol{x}\|^2$ for all $\boldsymbol{x}, \boldsymbol{y}\in\mathbb{R}^d$.
    \item The function $f(\cdot)-\frac{\mu}{2}\|\cdot\|^2$ is convex.
\end{itemize}
\end{lemma}





\section{Proofs for Privacy Analysis}

\subsection{Proof of \cref{lemma:smooth-reduct}}
\label{proof:smooth-reduct}

\begin{proof}
    (1) Based on \cref{lem:W-distance}, for the shifted R\'enyi divergence $\mathcal{D}_\alpha^{(z)}(\mathbb{P}_{\boldsymbol{\mu}}||\mathbb{P}_{\boldsymbol{\nu}})$, there exist jointly distributed random variables ($\boldsymbol{\mu},\boldsymbol{\mu}^\prime)$ such that $\operatorname{Pr}[||\boldsymbol{\mu}-\boldsymbol{\mu}^\prime||\leq z] = 1$ and $\mathcal{D}_\alpha^{(z)}(\mathbb{P}_{\boldsymbol{\mu}}||\mathbb{P}_{\boldsymbol{\nu}})=\mathcal{D}_\alpha (\mathbb{P}_{\boldsymbol{\mu}^\prime}||\mathbb{P}_{\boldsymbol{\nu}})$. Then, one may obtain
    \begin{equation}
    \|\psi(\boldsymbol{\mu})-\psi(\boldsymbol{\mu}^\prime)\| \leq \|\boldsymbol{\mu}-\boldsymbol{\mu}^\prime\|+\eta L\|\boldsymbol{\mu}-\boldsymbol{\mu}^\prime\| \leq (1+\eta L)z, 
    \end{equation}
    where the first step is by the triangle inequality and the $L$-smooth assumption, and the second step is by the definition of $\boldsymbol{\mu}^\prime$. Thus, we have
    \begin{equation}
        \label{eq:proof31-1}
        \begin{aligned}
        \mathcal{D}_\alpha^{((1+\eta L)z)}(\mathbb{P}_{\psi(\boldsymbol{\mu})}||\mathbb{P}_{\psi^\prime(\boldsymbol{\nu})}) &\leq \mathcal{D}_\alpha(\mathbb{P}_{\psi(\boldsymbol{\mu}^\prime)}||\mathbb{P}_{\psi^\prime(\boldsymbol{\nu})}) \\
        &= \mathcal{D}_\alpha(\mathbb{P}_{\psi(\boldsymbol{\mu}^\prime)}||\mathbb{P}_{(1-q)\psi(\boldsymbol{\nu})+q\psi^{\prime\prime}(\boldsymbol{\nu})}) \\
        & \leq (1-q)\mathcal{D}_\alpha(\mathbb{P}_{\psi(\boldsymbol{\mu}^\prime)}||\mathbb{P}_{\psi(\boldsymbol{\nu})})+q\mathcal{D}_\alpha(\mathbb{P}_{\psi(\boldsymbol{\mu}^\prime)}||\mathbb{P}_{\psi^{\prime\prime}(\boldsymbol{\nu})})
        \\
        &\leq (1-q)\mathcal{D}_\alpha(\mathbb{P}_{\boldsymbol{\mu}^\prime}||\mathbb{P}_{\boldsymbol{\nu}})+q\mathcal{D}_\alpha(\mathbb{P}_{\psi(\boldsymbol{\mu}^\prime)}||\mathbb{P}_{\psi^{\prime\prime}(\boldsymbol{\nu})}),
        \end{aligned}
    \end{equation}
    where the first step is by the definition of the shifted R\'enyi divergence, the second step is due to SGD sampling with $\psi^{\prime\prime}(\cdot)\stackrel{\Delta}{=}\psi^{\prime}(\cdot|\bar{\mathcal{D}}\in\mathcal{B}_t)$, $\bar{\mathcal{D}}$ denotes the sole differing entry between adjacent datasets ($\mathcal{D}^\prime=\mathcal{D}\cup \{\bar{\mathcal{D}}\}$), the third step is by \cref{lem:convex}, and the last step is by \cref{lem:post-process}.
    
    Note that for the second term $\mathcal{D}_\alpha(\mathbb{P}_{\psi(\boldsymbol{\mu}^\prime)}||\mathbb{P}_{\psi^{\prime\prime}(\boldsymbol{\nu})})$, we have
    \begin{equation}
    \begin{aligned}
    \mathcal{D}_\alpha(\mathbb{P}_{\psi(\boldsymbol{\mu}^\prime)}||\mathbb{P}_{\psi^{\prime\prime}(\boldsymbol{\nu})})&\leq \mathcal{D}_\alpha(\mathbb{P}_{\psi(\boldsymbol{\mu}^\prime),\boldsymbol{\mu}^\prime}||\mathbb{P}_{\psi^{\prime\prime}(\boldsymbol{\nu}),\boldsymbol{\nu}}) \\
    &\leq \sup_{\boldsymbol{v}}\mathcal{D}_\alpha(\mathbb{P}_{\psi(\boldsymbol{\mu}^\prime)|\boldsymbol{\mu}^\prime=\boldsymbol{v}}||\mathbb{P}_{\psi^{\prime\prime}(\boldsymbol{\nu})|\boldsymbol{\nu}=\boldsymbol{v}})+\mathcal{D}_\alpha(\mathbb{P}_{\boldsymbol{\mu}^\prime}||\mathbb{P}_{\boldsymbol{\nu}}) \\
    &\leq \frac{2\alpha C^2}{\beta b^2\sigma_{\text{DP}}^2} + \mathcal{D}_\alpha(\mathbb{P}_{\boldsymbol{\mu}^\prime}||\mathbb{P}_{\boldsymbol{\nu}}),
    \end{aligned}
    \end{equation}
    where the first step is by \cref{lem:post-process}, the second step is by \cref{lem:compos}, and the last step is by the well-known result $\mathcal{D}_\alpha(\mathcal{N}(0, \sigma^2 \boldsymbol{I}_d) \| \mathcal{N}(\boldsymbol{u}, \sigma^2 \boldsymbol{I}_d))=\alpha\|\boldsymbol{u}\|_2^2 / 2 \sigma^2$.
    Hence,
    \begin{equation}
    \begin{aligned}
    \mathcal{D}_\alpha^{((1+\eta L)z)}(\mathbb{P}_{\psi(\boldsymbol{\mu})}||\mathbb{P}_{\psi^\prime(\boldsymbol{\nu})}) 
    &\leq (1-q)\mathcal{D}_\alpha(\mathbb{P}_{\boldsymbol{\mu}^\prime}||\mathbb{P}_{\boldsymbol{\nu}})+\frac{2\alpha C^2}{\beta nb\sigma_{\text{DP}}^2} + q\mathcal{D}_\alpha(\mathbb{P}_{\boldsymbol{\mu}^\prime}||\mathbb{P}_{\boldsymbol{\nu}})
    \\
    &\leq \mathcal{D}_\alpha^{(z)}(\mathbb{P}_{\boldsymbol{\mu}}||\mathbb{P}_{\boldsymbol{\nu}})+\frac{2\alpha C^2}{\beta nb\sigma_{\text{DP}}^2}.
    \end{aligned}
    \end{equation}

    (2) Assuming $b\leq n/5$, $\alpha\leq\alpha^*(\frac{b}{n},\frac{b\sqrt{\beta}\sigma_{\text{DP}}}{2C})$, and $\sigma_{\text{DP}}>\frac{8C}{b\sqrt{\beta}}$, the proof is similar to the previous case. Starting with \cref{eq:proof31-1}, we obtain
    \begin{equation}
        \begin{aligned}
        \mathcal{D}_\alpha^{((1+\eta L)z)}(\mathbb{P}_{\psi(\boldsymbol{\mu})}||\mathbb{P}_{\psi^\prime(\boldsymbol{\nu})}) &\leq \mathcal{D}_\alpha(\mathbb{P}_{\psi(\boldsymbol{\mu}^\prime),\boldsymbol{\mu}^\prime}||\mathbb{P}_{\psi^\prime(\boldsymbol{\nu}),\boldsymbol{\nu}}) \\
        &\leq\sup_{\boldsymbol{v}}\mathcal{D}_\alpha(\mathbb{P}_{\psi(\boldsymbol{\mu}^\prime)|\boldsymbol{\mu}^\prime=\boldsymbol{v}}||\mathbb{P}_{\psi^{\prime}(\boldsymbol{\nu})|\boldsymbol{\nu}=\boldsymbol{v}}) +\mathcal{D}_\alpha(\mathbb{P}_{\boldsymbol{\mu}^\prime}||\mathbb{P}_{\boldsymbol{\nu}}) \\
        & \leq \mathcal{D}_\alpha^{\text{SGM}}\left(\frac{b}{n},\frac{ b\sqrt{\beta}\sigma_{\text{DP}}}{2C}\right)+\mathcal{D}_\alpha(\mathbb{P}_{\boldsymbol{\mu}^\prime}||\mathbb{P}_{\boldsymbol{\nu}})\\
        &\leq \mathcal{D}_\alpha^{(z)}(\mathbb{P}_{\boldsymbol{\mu}}||\mathbb{P}_{\boldsymbol{\nu}})+\frac{8\alpha C^2}{\beta n^2\sigma_{\text{DP}}^2},
        \end{aligned}
    \end{equation}
    where the first step is by \cref{lem:post-process}, the second step is by \cref{lem:compos}, the last step is by \cref{lem:SGM}.
\end{proof}




\subsection{Proof of \cref{thm:privacy-GC}}
\label{proof:privacy-GC}

\begin{proof}
    We first rewrite the update procedure of DPSGD-GC as follows:
    \begin{equation}
    \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t-\frac{\eta}{b}\sum_{\xi \in \mathcal{B}_t} \operatorname{clip}\left(\nabla l_\xi\left(\boldsymbol{\theta}_t\right)\right)+\boldsymbol{\varrho}_t+\boldsymbol{\varsigma}_t  \stackrel{\Delta}{=} \psi(\boldsymbol{\theta}_t) + \boldsymbol{\varsigma}_t,
    \end{equation}
    where $\psi(\cdot)$ denotes the noisy update function, and $\boldsymbol{\varrho}_t\sim\mathcal{N}(\beta\eta^2\sigma_{\text{DP}}^2\boldsymbol{I}_d)$ and $\boldsymbol{\varsigma}_t\sim\mathcal{N}((1-\beta)\eta^2\sigma_{\text{DP}}^2\boldsymbol{I}_d)$ are both the zero-mean Gaussian perturbation with $\beta\in(0,1)$.

    Then, consider real sequence $\{a_t\}_{i=0}^{T-1}$ such that $z_t=\sum_{i=0}^{t-1}(1+\eta L)^{t-i-1}(-a_i)$ is non-negative for all $t$ and $z_T=0$. By this way, we have $z_0=0$ and $z_{t+1}=(1+\eta L)z_t -a_t$.
    The proof proceeds by induction, leveraging \cref{lemma:shifted-reduct} and \cref{lemma:smooth-reduct}. Specifically, we have
    \begin{equation}
    \begin{aligned}
    \mathcal{D}_\alpha^{(z_{t+1})}(\mathbb{P}_{\boldsymbol{\theta}_{t+1}}||\mathbb{P}_{\boldsymbol{\theta}^\prime_{t+1}})&= \mathcal{D}_\alpha^{(z_{t+1})}(\mathbb{P}_{\psi(\boldsymbol{\theta}_{t})+\boldsymbol{\varsigma}_t}||\mathbb{P}_{\psi^\prime(\boldsymbol{\theta}^\prime_{t})+\boldsymbol{\varsigma}_t})\\
    &\leq \mathcal{D}_\alpha^{(z_{t+1}+a_{t})}(\mathbb{P}_{\psi(\boldsymbol{\theta}_t)}||\mathbb{P}_{\psi^\prime(\boldsymbol{\theta}^\prime_{t})}) + \frac{\alpha a_t^2}{2\eta^2\sigma_{\text{DP}}^2(1-\beta)}\\
    &= \mathcal{D}_\alpha^{((1+\eta L)z_t)}(\mathbb{P}_{\psi(\boldsymbol{\theta}_t)}||\mathbb{P}_{\psi^\prime(\boldsymbol{\theta}^\prime_{t})}) + \frac{\alpha a_t^2}{2\eta^2\sigma_{\text{DP}}^2(1-\beta)}\\
    & \leq \mathcal{D}^{(z_t)}_\alpha(\mathbb{P}_{\boldsymbol{\theta}_t}||\mathbb{P}_{\boldsymbol{\theta}_t^\prime}) + \frac{2\alpha C^2}{\beta nb\sigma_{\text{DP}}^2} + \frac{\alpha a_t^2}{2\eta^2\sigma_{\text{DP}}^2(1-\beta)},
    \end{aligned}
    \end{equation}
    where the second step is by \cref{lemma:shifted-reduct}, the third step is by the definition of $z_{t+1}$, and the last step is by \cref{lemma:smooth-reduct}. By using the induction hypothesis, we have
    \begin{equation}
    \begin{aligned}
    \mathcal{D}_{\alpha}(\mathbb{P}_{\boldsymbol{\theta}_{T}}||\mathbb{P}_{\boldsymbol{\theta}^\prime_{T}})&\leq \mathcal{D}_\alpha^{(z_0)}(\mathbb{P}_{\boldsymbol{\theta}_0}||\mathbb{P}_{\boldsymbol{\theta}_0^\prime})+\sum_{t=0}^{T-1}\frac{\alpha a_t^2}{2\eta^2\sigma_{\text{DP}}^2(1-\beta)} + \sum_{t=0}^{T-1}\frac{2\alpha C^2}{\beta nb\sigma_{\text{DP}}^2} \\
    &= \sum_{t=0}^{T-1}\frac{\alpha a_t^2}{2\eta^2\sigma_{\text{DP}}^2(1-\beta)} + \sum_{t=0}^{T-1}\frac{2\alpha C^2}{\beta nb\sigma_{\text{DP}}^2}.
    \end{aligned}
    \end{equation}  
    Let $a_t=0$ for all $t$, we have
    \begin{equation}
    \mathcal{D}_{\alpha}(\mathbb{P}_{\boldsymbol{\theta}_{T}}||\mathbb{P}_{\boldsymbol{\theta}^\prime_{T}})\leq \frac{2\alpha C^2}{\beta nb\sigma_{\text{DP}}^2}T.
    \end{equation}
    Under additional parameter constraints, the proof follows directly by applying the corresponding version of \cref{lemma:smooth-reduct}. Consequently, we omit the detailed derivations here.
\end{proof}




\subsection{Proof of \cref{thm:privacy-DC}}
\label{proof:privacy-DC}

\begin{proof}
    Similarly, consider the real sequence $\{a_t\}_{i=0}^{T-1}$ and any $\tau\in \{0,1,\cdots,T-1\}$ such that $z_t=(1+\eta L)^{t-\tau}D+\sum_{i=\tau}^{t-1}(1+\eta L)^{t-i-1}(-a_i)$ is non-negative for all $t\geq\tau$ and $z_T=0$. Note that we also have $ z_{t+1}=(1+\eta L)z_t -a_t$, yielding
    \begin{equation}
    \mathcal{D}_\alpha^{(z_{t+1})}(\mathbb{P}_{\boldsymbol{\theta}_{t+1}}||\mathbb{P}_{\boldsymbol{\theta}^\prime_{t+1}})\leq \mathcal{D}^{(z_t)}_\alpha(\mathbb{P}_{\boldsymbol{\theta}_t}||\mathbb{P}_{\boldsymbol{\theta}_t^\prime}) + \frac{2\alpha C^2}{\beta nb\sigma_{\text{DP}}^2} + \frac{\alpha a_t^2}{2\eta^2\sigma_{\text{DP}}^2(1-\beta)}.
    \end{equation}
    By repeating the induction from $T$ to $\tau$, we can obtain
    \begin{equation}
    \begin{aligned}
    \mathcal{D}_{\alpha}(\mathbb{P}_{\boldsymbol{\theta}_{T}}||\mathbb{P}_{\boldsymbol{\theta}^\prime_{T}})&\leq \mathcal{D}_\alpha^{(z_\tau)}(\mathbb{P}_{\boldsymbol{\theta}_\tau}||\mathbb{P}_{\boldsymbol{\theta}_\tau^\prime})+\sum_{t=\tau}^{T-1}\frac{\alpha a_t^2}{2\eta^2\sigma_{\text{DP}}^2(1-\beta)} + \sum_{t=\tau}^{T-1}\frac{2\alpha C^2}{\beta nb\sigma_{\text{DP}}^2} \\
    &= \sum_{t=\tau}^{T-1}\frac{\alpha a_t^2}{2\eta^2\sigma_{\text{DP}}^2(1-\beta)} + \sum_{t=\tau}^{T-1}\frac{2\alpha C^2}{\beta nb\sigma_{\text{DP}}^2},
    \end{aligned}
    \end{equation}
    where the last step is by $z_\tau=D$. Let $a_\tau=(1+\eta L)D$, $a_t=0$ for all $t>\tau$, we have 
    \begin{equation}
    \begin{aligned}
    \mathcal{D}_{\alpha}(\mathbb{P}_{\boldsymbol{\theta}_{T}}||\mathbb{P}_{\boldsymbol{\theta}^\prime_{T}})&\leq \min_{\tau\in\{0,\cdots,T-1\}} \frac{2\alpha C^2}{\beta nb\sigma_{\text{DP}}^2}(T-\tau)+\frac{\alpha (1+\eta L)^2D^2}{2\eta^2\sigma_{\text{DP}}^2(1-\beta)} \\
    &=\frac{2\alpha C^2}{\beta nb\sigma_{\text{DP}}^2}+\frac{\alpha (1+\eta L)^2D^2}{2\eta^2\sigma_{\text{DP}}^2(1-\beta)}.
    \end{aligned}
    \end{equation}
    The proof under additional assumptions is also omitted here.
\end{proof}

\section{Proofs for Utility Analysis}

\subsection{Proof of \cref{thm:converge-DC}}
\label{proof:converge-DC}

\begin{proof} Recall that the update procedure of DPSGD-DC is as follows:
\begin{equation}
\begin{aligned}
\boldsymbol{\theta}_{t+1} &= \Pi_\mathcal{K}\left(\boldsymbol{\theta}_t-\eta g(\boldsymbol{\theta}_t)+\boldsymbol{\zeta}_t\right), 
\end{aligned}
\end{equation}
where $g(\boldsymbol{\theta}_t)=\frac{1}{b}\sum_{\xi \in \mathcal{B}_t} g_\xi(\boldsymbol{\theta}_t)=\frac{1}{b}\sum_{\xi \in \mathcal{B}_t} \operatorname{clip}_C\left(\nabla l_\xi(\boldsymbol{\theta}_t)\right)$ and $\boldsymbol{\zeta}_t\sim\mathcal{N}(\boldsymbol{0}, \eta^2\sigma_{\text{DP}}^2\boldsymbol{I}_d)$ is the Gaussian perturbation.

We now proceed to the proof of \cref{thm:converge-DC}. It is worth noting that the main challenges lie in the gradient clipping operation, the SGD procedure, and the parameter projection step. Therefore, the proof will be divided into cases to address these aspects separately.

(1) $C\leq10\sigma_{\text{SGD}} (\frac{L}{\mu})^{\frac{1}{2}}$



(1.1) $\|\nabla l(\boldsymbol{\theta}_t)\|\geq 35\sigma_{\text{SGD}} (\frac{L}{\mu})^{\frac{3}{4}}$

We start the analysis with the following inequality:
\begin{equation}
\label{eq:converge-1}
\begin{aligned}
\mathbb{E}_{\boldsymbol{\zeta}_t}\left[\|\boldsymbol{\theta}_{t+1}-\boldsymbol{\theta}^*\|^2\right] &\leq \mathbb{E}_{\boldsymbol{\zeta}_t}\left[\|\boldsymbol{\theta}_t-\eta g(\boldsymbol{\theta}_t)+\boldsymbol{\zeta}_t-\boldsymbol{\theta}^*\|^2\right] \\
&\leq \|\boldsymbol{\theta}_t-\eta g(\boldsymbol{\theta}_t)-\boldsymbol{\theta}^*\|^2+d\eta^2\sigma_{\text{DP}}^2 \\
&\leq \|\boldsymbol{\theta}_t-\boldsymbol{\theta}^*\|^2-\frac{1}{b}\sum_{\xi\in\mathcal{B}_t}2\eta [g_\xi(\boldsymbol{\theta}_t)]^T(\boldsymbol{\theta}_t-\boldsymbol{\theta}^*)+\eta^2C^2+d\eta
^2 \sigma_{\text{DP}}^2,
\end{aligned}
\end{equation}
where the first step is by the independence of noise and \cref{lem:project}. Let $\gamma_\xi=\min (1, \frac{C}{\|\nabla l_\xi(\boldsymbol{\theta}_t)\|})$. If $\|\nabla l_\xi(\boldsymbol{\boldsymbol{\theta}}_t)-\nabla l(\boldsymbol{\theta}_t)\|\leq 5\sigma_{\text{SGD}} (\frac{L}{\mu})^{\frac{1}{4}}$, we immediate get $\gamma_\xi \leq \frac{C}{35\sigma_{\text{SGD}} (\frac{L}{\mu})^{3/4}-5\sigma_{\text{SGD}} (\frac{L}{\mu})^{1/4}}$ and $\gamma_\xi\geq \frac{7C}{8\|\nabla l(\boldsymbol{\theta}_t)\|}$. Thus,
\begin{equation}
\begin{aligned}
-2\eta [g_\xi(\boldsymbol{\theta}_t)]^T(\boldsymbol{\theta}_t-\boldsymbol{\theta}^*)&=-2\eta[g_\xi(\boldsymbol{\theta}_t)-\gamma_\xi\nabla l(\boldsymbol{\theta}_t)+\gamma_\xi\nabla l(\boldsymbol{\theta}_t)]^T(\boldsymbol{\theta}_t-\boldsymbol{\theta}^*) \\
&\leq-2\eta\gamma_\xi [\nabla l(\boldsymbol{\theta}_t)]^T(\boldsymbol{\theta}_t-\boldsymbol{\theta}^*)-2\eta[g_\xi(\boldsymbol{\theta}_t)-\gamma_\xi\nabla l(\boldsymbol{\theta}_t)]^T(\boldsymbol{\theta}_t-\boldsymbol{\theta}^*) \\
&\leq - \frac{7\eta C}{4\|\nabla l(\boldsymbol{\theta}_t)\|}[l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)]+2\eta \gamma_\xi \|\nabla l_\xi(\boldsymbol{\theta}_t)-\nabla l(\boldsymbol{\theta}_t)\| \cdot \|\boldsymbol{\theta}_t-\boldsymbol{\theta}^*\| \\
&\leq -\frac{7\eta C}{4\sqrt{2L}}\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}+\frac{2\eta C\sigma_{\text{SGD}} (\frac{L}{\mu})^{\frac{1}{4}}}{7\sigma_{\text{SGD}} (\frac{L}{\mu})^{\frac{3}{4}}-\sigma_{\text{SGD}} (\frac{L}{\mu})^{\frac{1}{4}}}\sqrt{\frac{2}{\mu}}\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)} \\
&\leq-\frac{7\eta C}{4\sqrt{2L}}\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}+\frac{4\eta C}{\sqrt{2}(7\sqrt{L}-\sqrt{\mu})}\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)} \\
&\leq -\frac{13\eta C}{12\sqrt{2L}}\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)},
\end{aligned}
\end{equation}
where the third step is by the convexity and Cauchy–Schwarz inequality, and the fourth step is by \cref{lem:smooth} and \cref{lem:strong-convex}.

Else if $\|\nabla l_\xi(\boldsymbol{\theta}_t)-\nabla l(\boldsymbol{\theta}_t)\|> 5\sigma_{\text{SGD}} (\frac{L}{\mu})^{\frac{1}{4}}$, we have
\begin{equation}
-2\eta [g_\xi(\boldsymbol{\theta}_t)]^T(\boldsymbol{\theta}_t-\boldsymbol{\theta}^*) \leq 2\eta C\|\boldsymbol{\theta}_t-\boldsymbol{\theta}^*\| \leq \frac{4\eta C}{\sqrt{2\mu}}\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)},
\end{equation}
by using Cauchy–Schwarz inequality and \cref{lem:strong-convex}. Here, we define $\kappa_\xi=\mathbbm{1}\{\|\nabla l_\xi(\boldsymbol{\theta}_t)-\nabla l(\boldsymbol{\theta}_t)\|> 5\sigma_{\text{SGD}} (\frac{L}{\mu})^{\frac{1}{4}}\}$. According to \cref{lem:markov}, we have
\begin{equation}
\begin{aligned}
\operatorname{Pr}(\kappa_\xi=1)&=\operatorname{Pr}(\|\nabla l_\xi(\boldsymbol{\theta}_t)-\nabla l(\boldsymbol{\theta}_t)\|^2> 25\sigma_{\text{SGD}}^2 \sqrt{L/\mu})\leq\frac{\sigma_{\text{SGD}}^2}{25\sigma_{\text{SGD}}^2\sqrt{L/\mu}}=\frac{1}{25}\sqrt{\frac{\mu}{L}}, \\
\operatorname{Pr}(\kappa_\xi=0)&\geq 1-\frac{1}{25}\sqrt{\frac{\mu}{L}}\geq \frac{24}{25}.
\end{aligned}
\end{equation}
Hence,
\begin{equation}
\label{eq:converge-2}
\begin{aligned}
\mathbb{E}_\xi\left[-2\eta [g_\xi(\boldsymbol{\theta}_t)]^T(\boldsymbol{\theta}_t-\boldsymbol{\theta}^*)\right]&\leq -\frac{13\eta C}{12\sqrt{2L}}\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}\cdot \frac{24}{25}+\frac{4\eta C}{\sqrt{2\mu}}\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}\cdot \frac{1}{25}\sqrt{\frac{\mu}{L}}\\
&\leq -\frac{22\eta C}{25\sqrt{2L}}\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}.
\end{aligned}
\end{equation}
Then, we choose $\eta\leq\frac{7}{10L}(\frac{L}{\mu})^{1/4}$, yielding
\begin{equation}
\label{eq:converge-3}
\begin{aligned}
\eta^2C^2 &\leq \frac{7\eta C^2}{10L}(\frac{L}{\mu})^{1/4}\leq\frac{\eta C^2}{2L} \frac{4\|\nabla l(\boldsymbol{\theta}_t)\|}{10C} \\
&\leq \frac{\eta C}{\sqrt{2L}}\frac{2\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}}{5},
\end{aligned}
\end{equation}
where the last step is by \cref{lem:smooth}. Substituting \cref{eq:converge-2} and \cref{eq:converge-3} into \cref{eq:converge-1}, we have
\begin{equation}
\begin{aligned}
\mathbb{E}_{\boldsymbol{\zeta}_t,\xi}\left[\|\boldsymbol{\theta}_{t+1}-\boldsymbol{\theta}^*\|^2\right] 
&\leq \|\boldsymbol{\theta}_t-\boldsymbol{\theta}^*\|^2-\frac{22\eta c}{25\sqrt{2L}}\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}+\frac{\eta c}{\sqrt{2L}}\frac{2\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}}{5}+d\eta
^2 \sigma_{\text{DP}}^2 \\
&\leq \|\boldsymbol{\theta}_t-\boldsymbol{\theta}^*\|^2-\frac{12\eta c}{25\sqrt{2L}}\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}+d\eta^2\sigma_{\text{DP}}^2.
\end{aligned}
\end{equation}
Thus, averaging over $t$, we have
\begin{equation}
\frac{1}{T+1}\sum_{t=0}^T \mathbb{E}\left[\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}\right]\leq \mathcal{O}\left(\frac{\sqrt{L}D^2}{\eta C T}+\frac{d\eta\sigma_{\text{DP}}^2\sqrt{L}}{C}\right).
\end{equation}

(1.2) $\|\nabla l(\boldsymbol{\theta}_t)\|< 35\sigma_{\text{SGD}} (\frac{L}{\mu})^{\frac{3}{4}}$

By using \cref{lem:strong-convex}, we immediate obtain $\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}\leq \sqrt{\frac{1}{2\mu}}\|\nabla l(\boldsymbol{\theta}_t)\|\leq \mathcal{O}\left(\frac{L^{3/4}}{\mu^{5/4}}\sigma_{\text{SGD}}\right)$.

(2) $C\geq 10\sigma_{\text{SGD}} (\frac{L}{\mu})^{\frac{1}{2}}$

(2.1)  $\|\nabla l(\boldsymbol{\theta}_t)\|>\frac{C}{2}$

In this case, we start the analysis with the following inequality:
\begin{equation}
\begin{aligned}
\label{eq:converge-4}
\mathbb{E}_{\boldsymbol{\zeta}_t}\left[\|\boldsymbol{\theta}_{t+1}-\boldsymbol{\theta}^*\|^2\right] &\leq \mathbb{E}_{\zeta_t}\left[\|\boldsymbol{\theta}_t-\eta g(\boldsymbol{\theta}_t)+\boldsymbol{\zeta}_t-\boldsymbol{\theta}^*\|^2\right] \\
&\leq \|\boldsymbol{\theta}_t-\boldsymbol{\theta}^*\|^2-\frac{1}{b}\sum_{\xi\in \mathcal{B}_t}2\eta [g_\xi(\boldsymbol{\theta}_t)]^T(\boldsymbol{\theta}_t-\boldsymbol{\theta}^*)+\eta^2\|g(\boldsymbol{\theta}_t)\|^2+d\eta
^2 \sigma_{\text{DP}}^2,
\end{aligned}
\end{equation}
where the first step is by \cref{lem:project}, and the second step is by the independence of noise.
Let $\gamma=\min (1, \frac{C}{\|\nabla l(\boldsymbol{\theta}_t)\|})$, we then have $\gamma\geq\min(1,\frac{C}{2\|\nabla l(\boldsymbol{\theta}_t)\|})=\frac{C}{2\|\nabla l(\boldsymbol{\theta}_t)\|}$. Thus,
\begin{equation}
\begin{aligned}
\mathbb{E}_\xi\left[-2\eta [g_\xi(\boldsymbol{\theta}_t)]^T(\boldsymbol{\theta}_t-\boldsymbol{\theta}^*)\right]&=\mathbb{E}_\xi\left[-2\eta[g_\xi(\boldsymbol{\theta}_t)-\gamma\nabla l(\boldsymbol{\theta}_t)+\gamma\nabla l(\boldsymbol{\theta}_t)]^T(\boldsymbol{\theta}_t-\boldsymbol{\theta}^*)\right] \\
&\leq-2\eta\gamma [\nabla l(\boldsymbol{\theta}_t)]^T(\boldsymbol{\theta}_t-\boldsymbol{\theta}^*)-\mathbb{E}_\xi\left[2\eta[g_\xi(\boldsymbol{\theta}_t)-\gamma\nabla l(\boldsymbol{\theta}_t)]^T(\boldsymbol{\theta}_t-\boldsymbol{\theta}^*)\right] \\
&\leq -\frac{\eta C}{\|\nabla l(\boldsymbol{\theta}_t)\|}[l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)]+\mathbb{E}_\xi\left[2\eta \|\nabla l_\xi(\boldsymbol{\theta}_t)-\nabla l(\boldsymbol{\theta}_t)\| \cdot \|\boldsymbol{\theta}_t-\boldsymbol{\theta}^*\|\right] \\
&\leq -\frac{\eta C}{\sqrt{2L}}\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}+2\eta \sigma_{\text{SGD}} \sqrt{\frac{2}{\mu}}\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)} \\
&\leq-\frac{\eta C}{\sqrt{2L}}\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}+ \frac{2\eta C}{5\sqrt{2L}}\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}\\
&\leq -\frac{3\eta C}{5\sqrt{2L}}\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)},
\end{aligned}
\end{equation}
where the third step is by the convexity and Cauchy-Schwarz inequality, and the fourth step is by \cref{lem:smooth} and \cref{lem:strong-convex}. Combined with \cref{eq:converge-3}, we have
\begin{equation}
\begin{aligned}
\mathbb{E}_{\boldsymbol{\zeta}_t,\xi}\left[\|\boldsymbol{\theta}_{t+1}-\boldsymbol{\theta}^*\|^2\right] 
&\leq \|\boldsymbol{\theta}_t-\boldsymbol{\theta}^*\|^2-\frac{3\eta C}{5\sqrt{2L}}\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}+\frac{\eta C}{\sqrt{2L}}\frac{2\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}}{5}+\eta
^2 \sigma_{\text{DP}}^2 \\
&\leq \|\boldsymbol{\theta}_t-\boldsymbol{\theta}^*\|^2-\frac{\eta C}{5\sqrt{2L}}\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}+d\eta^2\sigma_{\text{DP}}^2.
\end{aligned}
\end{equation}
Thus, averaging over $t$, we have
\begin{equation}
\frac{1}{T+1}\sum_{t=0}^T \mathbb{E}\left[\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}\right]\leq \mathcal{O}\left(\frac{\sqrt{L}D^2}{\eta C T}+\frac{d\eta\sigma_{\text{DP}}^2\sqrt{L}}{C}\right).
\end{equation}

(2.2) $\|\nabla l(\boldsymbol{\theta}_t)\|\leq \frac{C}{2}$

In this case, we employ the triangle inequality and obtain
\begin{equation}
\|\nabla l_\xi(\boldsymbol{\theta}_t)\|\leq\|\nabla l_\xi(\boldsymbol{\theta}_t)-\nabla l(\boldsymbol{\theta}_t)\|+\frac{C}{2}.
\end{equation}
Let $\kappa=\mathbbm{1}\{\|\nabla l_\xi(\boldsymbol{\theta}_t)\|> C\}$ and use \cref{lem:markov}, yielding
\begin{equation}
\operatorname{Pr}\left[\kappa=1\right]\leq \operatorname{Pr}\left[\{\|\nabla l_\xi(\boldsymbol{\theta}_t)-\nabla l(\boldsymbol{\theta}_t)\|^2> \frac{C^2}{4}\}\right]\leq \frac{4\sigma_{\text{SGD}}^2}{C}.
\end{equation}

Hence,
\begin{equation}
\label{eq:converge-5}
\begin{aligned}
\mathbb{E}_\xi\left[-2\eta [g_\xi(\boldsymbol{\theta}_t)]^T(\boldsymbol{\theta}_t-\boldsymbol{\theta}^*)\right]&=-2\eta \left[\mathbb{E}_\xi[g_\xi(\boldsymbol{\theta}_t)]-\nabla l(\boldsymbol{\theta}_t)+\nabla l(\boldsymbol{\theta}_t)\right]^T(\boldsymbol{\theta}_t-\boldsymbol{\theta}^*) \\
&\leq 2\eta \cdot\|\mathbb{E}_\xi[g_\xi(\boldsymbol{\theta}_t)]-\nabla l(\boldsymbol{\theta}_t)\| \cdot\|\boldsymbol{\theta}_t-\boldsymbol{\theta}^*\|-2\eta \nabla l(\boldsymbol{\theta}_t)^T(\boldsymbol{\theta}_t-\boldsymbol{\theta}^*) \\
&\leq 2\eta\cdot\|\mathbb{E}_\xi\left[g_\xi(\boldsymbol{\theta}_t)-\nabla l_\xi(\boldsymbol{\theta}_t)|\kappa=1\right]\cdot\operatorname{Pr}(\kappa=1)\|\cdot \|\boldsymbol{\theta}_t-\boldsymbol{\theta}^*\|-2\eta [l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)] \\
&\leq \frac{8\eta\sigma_{\text{SGD}}^2}{C^2}\|\mathbb{E}_\xi[(1-\frac{C}{\|\nabla l_\xi(\boldsymbol{\theta}_t)\|})\nabla l_\xi(\boldsymbol{\theta}_t)]\|\cdot\|\boldsymbol{\theta}_t-\boldsymbol{\theta}^*\|-2\eta [l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)] \\
&\leq \frac{8\eta\sigma_{\text{SGD}}^2}{C^2} \mathbb{E}_\xi[||\nabla l_\xi(\boldsymbol{\theta}_t)-\nabla l(\boldsymbol{\theta}_t)+\nabla l(\boldsymbol{\theta}_t)||]\cdot\|\boldsymbol{\theta}_t-\boldsymbol{\theta}^*\|-2\eta [l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)] \\
&\leq \frac{4\eta\sigma_{\text{SGD}}^3}{\mu C} + \frac{16\eta\sigma_{\text{SGD}}^2}{C^2}\sqrt{\frac{L}{\mu}} [l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)]-2\eta [l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)] \\
&\leq \frac{4\eta\sigma_{\text{SGD}}^3}{\mu C} - \left(2\eta-\frac{4}{25}\eta\sqrt{\frac{\mu}{L}}\right)[l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)] \\
&\leq \frac{4\eta\sigma_{\text{SGD}}^3}{\mu C} - \frac{46}{25}\eta[l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)],
\end{aligned}
\end{equation}
where the second step is by the convexity and Cauchy-Schwarz inequality, the third step is by the law of total expectation, the fifth step is by Jensen's inequality, and the third to last step is by \cref{lem:smooth} and \cref{lem:strong-convex}.

Further, note that when $\eta\leq\frac{9}{20L}$,
\begin{equation}
\label{eq:converge-6}
\begin{aligned}
\mathbb{E}\left[\eta^2\|g(\boldsymbol{\theta}_t)\|^2\right] &\leq \frac{\eta^2}{b^2}\sum_{\xi\in\mathcal{B}_t}\mathbb{E}_\xi\left[\|g_\xi(\boldsymbol{\theta}_t)-\nabla l(\boldsymbol{\theta}_t)+l(\boldsymbol{\theta}_t)\|^2\right] \\
&\leq \frac{2\eta^2}{b^2}\sum_{\xi\in\mathcal{B}_t}\mathbb{E}_\xi\left[\|g_\xi(\boldsymbol{\theta}_t)-\nabla l(\boldsymbol{\theta}_t)\|^2\right]+2\eta^2\|\nabla l(\boldsymbol{\theta}_t)\|^2 \\
&\leq \frac{2\eta^2\sigma_{\text{SGD}}^2}{b}+\frac{9\eta}{5} [l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)],
\end{aligned}
\end{equation}
where the second step is by \cref{lem:(a+b)2}, and the last step is by \cref{lem:smooth}.

Substituting \cref{eq:converge-5} and \cref{eq:converge-6} into \cref{eq:converge-4}, we have
\begin{equation}
\begin{aligned}
\mathbb{E}_{\boldsymbol{\zeta}_t,\xi}\left[\|\boldsymbol{\theta}_{t+1}-\boldsymbol{\theta}^*\|^2\right] 
&\leq \|\boldsymbol{\theta}_t-\boldsymbol{\theta}^*\|^2+\frac{4\eta\sigma_{\text{SGD}}^3}{\mu C}-\frac{46\eta }{25}[l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)]+ \frac{2\eta^2\sigma_{\text{SGD}}^2}{b}+\frac{9\eta}{5}[l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)] +d\eta^2\sigma_{\text{DP}}^2 \\
&\leq \|\boldsymbol{\theta}_t-\boldsymbol{\theta}^*\|^2+\frac{4\eta\sigma_{\text{SGD}}^3}{\mu C}-\frac{\eta }{25}[l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)]+ \frac{2\eta^2\sigma_{\text{SGD}}^2}{b}+d\eta^2\sigma_{\text{DP}}^2.
\end{aligned}
\end{equation}

Hence, one may obtain
\begin{equation}
\frac{1}{25(T+1)}\sum_{t=0}^T \mathbb{E}\left[l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)\right]\leq \frac{D^2}{\eta  (T+1)}+\frac{4\sigma_{\text{SGD}}^3}{\mu C}+\frac{2\eta\sigma_{\text{SGD}}^2}{b}+d\eta\sigma_{\text{DP}}^2.
\end{equation}
Exploiting \cref{lem:x2x} and \cref{lem:sqrt(a+b)}, one may get
\begin{equation}
\begin{aligned}
\frac{1}{25(T+1)}\sum_{t=0}^T \mathbb{E}\left[\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}\right]&\leq \sqrt{\frac{D^2}{\eta  (T+1)}+\frac{4\sigma_{\text{SGD}}^3}{\mu c}+\frac{2\eta\sigma_{\text{SGD}}^2}{b}+d\eta\sigma_{\text{DP}}^2} \\
&\leq\frac{D}{\sqrt{\eta  (T+1)}}+\frac{2\sigma_{\text{SGD}}^{1.5}}{\sqrt{\mu c}}+\frac{\sqrt{2\eta}\sigma_{\text{SGD}}}{\sqrt{b}}+\sqrt{d\eta}\sigma_{\text{DP}}.
\end{aligned}
\end{equation}
Therefore, we have
\begin{equation}
\frac{1}{T+1}\sum_{t=0}^T \mathbb{E}\left[\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}\right]\leq \mathcal{O}\left(\frac{D}{\sqrt{\eta  T}}+\sqrt{\frac{\sigma_{\text{SGD}}^{3}}{\mu C}}+\frac{\sqrt{\eta}\sigma_{\text{SGD}}}{\sqrt{b}}+\sqrt{d\eta}\sigma_{\text{DP}}\right).
\end{equation}
Finally, summing up all the cases, one may get
\begin{equation}
\begin{aligned}
&\min_{t\in[0,T]} \mathbb{E}\left[\sqrt{l(\boldsymbol{\theta}_t)-l(\boldsymbol{\theta}^*)}\right] \\
&\leq \mathcal{O}\left(\frac{\sqrt{L}D^2}{\eta C T}+\frac{D}{\sqrt{\eta T}}+\min\left(\frac{L^{3/4}}{\mu^{5/4}}\sigma_{\text{SGD}},\sqrt{\frac{\sigma_{\text{SGD}}^3}{\mu C}}\right)+\frac{\sqrt{\eta}\sigma_{\text{SGD}}}{\sqrt{b}}+\frac{d\eta\sigma_{\text{DP}}^2\sqrt{L}}{C}+\sqrt{d\eta}\sigma_{\text{DP}}\right).
\end{aligned}
\end{equation}
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
