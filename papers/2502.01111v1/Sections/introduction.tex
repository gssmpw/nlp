\section{Introduction}
Seismic processing is an essential step for raw data acquisition to produce high-quality subsurface images \citep{yilmaz2001seismic}. It involves a series of complex and diverse procedures aimed at revealing detailed information about subsurface formations and their physical properties. Due to the highly intricate nature of seismic wave propagation in subsurface media and the interference of acquisition environments, raw acquired data are often degraded by various factors. For example, environmental noise reduces the signal-to-noise ratio, making it challenging to extract valuable signals. Damaged geophones can lead to bad traces in the data, compromising the consistency and completeness of subsequent processing. Low-frequency signals are often week, resulting in the loss of crucial signal components that are vital for accurately characterizing subsurface structures \citep{virieux2009overview}. These factors negatively impact the accuracy of subsequent seismic processing, imaging and inversion. Therefore, various seismic processing steps should be performed to enhance data quality, strengthen signals, and eliminate interferences, thereby achieving reliable subsurface imaging results and accurate geological interpretation. 

The conventional seismic processing paradigm generally consists of several key steps designed to address the aforementioned issues and enhance data quality. The first stage is preprocessing, which usually includes denoising to mitigate the impact of noise \citep{abma1995lateral, krohn2008introduction, chen2014random, chen2015random, liu2015signal}. Usually, static correction and normalization are performed to compensate for surface irregularities and variations in amplitude, ensuring consistency in signal phase and amplitude \citep{cox1999static}. Multiple suppression is also often employed to eliminate the impact of multiple reflections, thereby enhancing the clarity of primary reflection signals \citep{verschuur1992adaptive, lopez2015closed}. For areas with incomplete data acquisition, interpolation techniques are used to fill in missing information, thereby improving spatial sampling density and resolution \citep{spitz1991seismic, wang2002seismic, chen2019interpolation}. Moreover, velocity analysis \citep{alkhalifah1995velocity, symes2008migration, fomel2009velocity} and migration \citep{baysal1983reverse, chang1987elastic, zhang2015stable} are central to obtain an image of the subsurface. In this process, accurate velocity models are constructed to reposition seismic reflection events to their true locations, resulting in precise subsurface images \citep{etgen2009overview}. In addition, especially recently, inversion techniques are applied to extract lithological and physical property information from the subsurface, enabling a quantitative description of subsurface formations \citep{tarantola1984inversion, tarantola1986strategy, alkhalifah2014tomography}. These processing steps work together to gradually improve the quality of seismic data, and the resulting image for proper geological interpretation and understanding. 

The advantages of the traditional seismic processing paradigm lie in its rigorous theoretical foundation and its extensive application in geophysical exploration \citep{yilmaz2001seismic}. These processing steps have been validated over time and, also, have demonstrated effectiveness in addressing a variety of complex issues while progressively enhancing the quality and reliability of seismic data. Additionally, traditional methods possess strong physical interpretability, enabling clear imaging of subsurface structures and extraction of crucial lithological and physical properties. However, there are also notable limitations associated with traditional methods. Firstly, conventional seismic processing often relies heavily on expert knowledge and experience, requiring frequent parameter adjustments and expert judgment throughout the various steps, to adapt to the various data. In other words, the processing algorithms, other than certain user-defined parameters, are often fixed and not driven by the data. This results in a high professional threshold and a lengthy processing cycle \citep{yu2021deep}. Secondly, given the increasing volume of data, the efficiency and timeliness of traditional methods struggle to meet practical demands, with the data processing and imaging often consuming considerable time and computational resources \citep{hou2021machine}. Lastly, the performance of traditional methods is often insufficiently robust, making them susceptible to noise and the complexities of subsurface media, which hinders their ability to consistently deliver high-quality results \citep{Li2020DLInversion}. 

To overcome the limitations of traditional methods, neural network (NN)-based seismic processing approaches have gradually gained attention due to their numerous unique advantages \citep{yu2021deep, mousavi2022deep, mousavi2024applications}. For instance, deep learning (DL) methods can automatically learn features from data, thereby reducing the reliance on expert knowledge, while also better meeting the need to handle large volumes of data. Furthermore, NN-based models often exhibit superior performance when processing complex seismic data. Typically, an NN-based seismic processing paradigm involves training a deep NN on a substantial amount of seismic data to approximate the nonlinear relationship between input and target data. Since target data from real-world cases are often inaccessible, a common approach is to train the NN using synthetic data in a supervised learning (SL) manner before applying it to real data \citep{yu2019deep, wang2019deep, dong2019desert, wu2019faultseg3d, wu2020building, zhang2021deep, dong2024can, dong2024seismic}. A significant limitation of this approach arises when the synthetic data distribution poorly represents the real data, leading to considerable performance degradation for the trained network \citep{alkhalifah2022mlreal, zhang2022improving}. Therefore, an alternative method is to use self-supervised learning (SSL) (or unsupervised learning) to eliminate the need for labeled data, enabling the network to be trained directly on real data, which can mitigate the generalization issues \citep{saad2020deep, birnie2021potential, liu2023trace, liu2024self, liu2024gabor, saad2024noise, cheng2024effective, cheng2024self}. However, since the training is performed on each real seismic dataset individually, it is often dataset and task specific and, thus, the overall efficiency is lower compared to networks trained using SL. 

% Additionally, without labeled data, optimizing a network in an unsupervised manner is inherently unstable and often necessitates specific techniques to ensure convergence.

Actually, regardless of whether it is in the SL or SSL paradigm, another major issue is that a trained network is often tailored to a specific seismic processing task (SPT). When switching to another task, the network is often trained again from scratch. As mentioned earlier, seismic processing comprises multiple distinct tasks, and training a network from scratch for each task incurs significant time and the computational cost. Consequently, some recent paradigms based on pre-trained models have been proposed, where these models are first pre-trained on large amounts of seismic data using SSL for reconstruction, and then fine-tuned for downstream tasks to improve training efficiency and reduce computational costs. For example, \cite{harsuko2022storseismic} proposed the StorSeismic framework, in which they pre-trained a Transformer model that takes the sequence of seismic shot gathers as input to extract and store features of the seismic data. The pre-trained model is then fine-tuned for multiple SPTs, such as denoising, velocity estimation, first arrival picking, and normal moveout correction, among other tasks. The fine-tuned model demonstrated excellent performance on field data. Similarly, \cite{sheng2023seismic} introduced the Seismic Foundation Model (SFM), employing the Masked Autoencoders approach to pre-train a Transformer on over 2 million large datasets. After pre-training, they extracted the encoder part and connected a simple decoder network for fine-tuning on downstream tasks. SFM exhibited superior performance across tasks like denoising, interpolation, seismic facies classification, geological body recognition, and inversion. Unlike the previous two paradigms, \cite{cheng2024meta} proposed a Meta-Processing framework for multi-task seismic processing that employs meta-learning to extract shared features of seismic data from very limited datasets, thereby providing a robust initialization. This initialization allows for rapid convergence to optimal performance across various SPTs. 

We can see that, the core of the pre-training strategy lies in leveraging NNs to learn and extract distributional characteristics of seismic data, enabling these pre-trained networks to achieve rapid convergence and outstanding performance across various downstream SPTs. Therefore, it inspires us that if a network model can effectively capture the distribution characteristics of seismic data, it can significantly enhance its performance in seismic processing. Recently, generative diffusion models (GDMs) have shown substantial potential in seismology due to their powerful ability to learn given data distributions, including applications such as denoising \citep{li2024conditional, xiao2024diffusion, trappolini2024cold}, interpolation \citep{wei2023seismic, liu2024generative, wang2024self, wei2024seismic}, resolution enhancement \citep{zhang2024seisresodiff}, waveform separation \citep{zhang2024conditional}, imaging improvement \cite{shi2024generative}, and velocity model building \citep{wang2023prior, wang2024controllable, taufik2024learned}. Notably, \cite{durall2023deep} tested GDMs on various SPTs, including demultiple, denoising, and interpolation. They trained GDMs on synthetic data and evaluated it on synthetic data. They presented results of field data testing for the demultiple task, demonstrating competitive outcomes with traditional DL methods. However, significant signal leakage was still observed, which can be attributed to generalization issues arising from the distributional shift between synthetic and field data. In addition, for different SPTs, they trained different GDMs from scratch to accommodate each specific task, which is time-consuming. 

In this paper, we propose a generative seismic foundation model (GSFM) framework for various SPTs. This framework is based on the GDM's powerful capability to capture and store the distributional characteristics of seismic data, potentially offering greater expressiveness compared to traditional pre-training methods. Due to the GDM's need for target data distributions, as shown by \cite{durall2023deep}, we also train our GDM on synthetic data. However, a significant difference from Durall et al.'s approach is that we train various SPTs simultaneously on a single GDM, such as denoising, backscattered noise attenuation, interpolation, and low-frequency extrapolation. We encode these tasks by introducing different class labels, embedding them into the training process of the GDM, enabling the network to automatically identify and handle various SPTs. Training for multi-task  applications is based on the assumption that ideal seismic data (the target) should be clean, complete, and broadband. By training the GDM to capture this ideal distribution, we enable the model to generate the ideal target output from low-quality seismic data. Additionally, we adopt, within the GDM framework, target prediction instead of noise prediction during training to enhance both training stability and inference efficiency. Predicting the target directly aligns the model output with the ideal seismic data distribution, avoiding the iterative denoising process commonly required in conventional GDMs. This design not only simplifies the training process by reducing optimization complexity but also allows us to achieve high-quality results during inference with just a single sampling step. Nevertheless, due to the feature gap between synthetic and field data, we would still face generalization issues when applying the trained model to field data. To address this problem, we propose a strategy to fine-tune our pre-trained GDM on field data using an SSL approach. Specifically, during the initial stage of fine-tuning for each SPT, we use the pre-trained GDM model to directly predict the field data, which is then added to our training dataset. After the GDM model undergoes several iterations of optimization, we iteratively employ the model trained in the previous stage to predict field data and update the training set at fixed intervals. In this way, we gradually shift the distribution captured by the pre-trained GDM from the synthetic domain to the field data distribution, thereby enhancing the model's performance on field data. 

Furthermore, the inherent randomness in the initial noise used during the sampling process allows us to generate multiple predictions for the same input condition. This provides a natural mechanism to assess prediction variability. By evaluating the standard deviation of these predictions, we can identify regions of higher uncertainty, which often correspond to areas with greater signal leakage or processing errors. This capability not only helps evaluate the reliability of the model's predictions but also provides valuable feedback to guide further optimization during the fine-tuning process, ensuring robust performance on field data. 

The contributions of this paper can be summarized as follows:
\begin{itemize}
   \item We propose a generative seismic foundation model framework capable of simultaneously performing various SPTs.
   \item We introduce the use of class label constraints to guide the NN in jointly optimizing different SPTs.
   \item We propose a strategy to fine-tune the pre-trained foundation model on synthetic data using an SSL approach on field data, thereby overcoming the generalization issues of NNs.
   \item We leverage the probabilistic nature of GDMs to quantify the uncertainty of the processing product, which helps assess its reliability and helps guide the fine-tuning of the pre-trained model.
   \item Examples from synthetic and field data demonstrate that our all-in-one seismic processing framework can achieve good processing performance.
\end{itemize} 






