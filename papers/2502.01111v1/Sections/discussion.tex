\section{Discussion}
This section provides an in-depth analysis of various aspects of our proposed GSFM framework. First, we compare the performance of GSFM models trained to predict $x_0$ directly versus those trained to predict the noise component, illustrating the advantages of our chosen prediction strategy. We, then, examine the impact of different sampling step lengths during inference on the quality and efficiency of the results. Additionally, we evaluate the computational and memory efficiency of our framework, highlighting its scalability for large-scale seismic datasets. Finally, we address the limitations of our approach and propose potential directions for future research to further enhance the capabilities of GSFM in seismic processing. 

\subsection{Comparison of predicting target and noise}
We here compare the performance of GSFM trained to predict target $x_0$ with a GSFM variant trained to predict noise $\epsilon$. Both models utilize the same network architecture and training configurations, but their target outputs differ. The noise-targeted GSFM aims to predict the noise component added during the forward diffusion process, while the $x_0$-targeted GSFM directly predicts the labeled data. We evaluate their respective performance, where we use denoising and interpolation tasks on synthetic data as an example. 

Figures \ref{fig16} and \ref{fig17} illustrate the denoising and interpolation results generated by the noise-targeted pre-trained GSFM under different sampling step sizes, including $T=1$, 100, 500, and 1000. For the denoising task, the labeled and input data are shown in Figures \ref{fig2}a and \ref{fig2}b, respectively. Panels (a) to (d) in Figure \ref{fig16} display the denoised products obtained using the specified sampling step sizes, while panels (e) to (h) present the corresponding residuals compared to the labeled data. Similarly, for the interpolation task, the labeled and incomplete input data are shown in Figures \ref{fig4}a and \ref{fig4}b, respectively. Figure \ref{fig17} follows the same structure, highlighting the interpolation outputs and residuals. 

It is evident that the noise-targeted GSFM produces suboptimal results when using lower sampling step sizes ($T=1$, 100, and 500), with significant residual noise. This behavior highlights the challenges faced by the GSFM trained to predict noise, as it struggles to fully recover clean outputs within limited time steps. Even when the maximum sampling steps ($T=1000$) are used, noticeable signal leakage persist in the outputs. For instance, in the interpolation task, when comparing the results of the noise-targeted GSFM using $T=1000$ to the $x_0$-targeted GSFM using $T=1$ (see Figures \ref{fig4}c and \ref{fig4}f), the superiority of the latter becomes evident. While the $x_0$-targeted GSFM produces clean and coherent interpolated results with negligible residuals, the noise-targeted GSFM still contains significant signal leakage even with the highest sampling steps.  

Tables \ref{tab4} and \ref{tab5} further quantify these findings, where we calculate the MSE metric for denoising and interpolation tasks across varying noise and missing levels, respectively. The $x_0$-targeted GSFM consistently achieves lower MSE values than the noise-targeted GSFM, irrespective of the sampling step size. While the noise-targeted model shows general improvement with increased sampling steps, it requires the high step size ($T=500$ and 1000) to deliver results comparable to the $x_0$-targeted GSFM using step size $T=1$. Moreover, we can see that, the performance of the noise-targeted GSFM reveals further limitations. For certain noise levels (10\%, 20\%, 50\%, and 60\%) in the denoising task and missing levels (20\%) in the interpolation task, even when using the maximum sampling steps ($T=1000$), the resulting products still exhibit high MSE values. These elevated MSE scores, especially when compared to the consistently low MSE values achieved by the $x_0$-targeted GSFM, indicate that the noise-targeted GSFM has not been effectively optimized for these tasks. This lack of optimization becomes particularly issues in scenarios requiring fine-grained accuracy, as the noise-targeted GSFM struggles to converge to solutions that adequately approximate the labeled data. 


\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figure/fig16.png}
\caption{Denoised products from the pre-trained DSFM when the prediction target is noise. The label and input test data can be found in Figures \ref{fig2}a and \ref{fig2}b, respectively. The denoised products using sampling step sizes (a) 1, (b) 100, (c) 500, and (d)
. (e, f, g, and h) The corresponding difference between the denoised results and the labeled data. }
\label{fig16}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figure/fig17.png}
\caption{Interpolation products from the pre-trained DSFM when the prediction target is noise. The label and input incomplete data can be found in Figures \ref{fig4}a and \ref{fig4}b, respectively. The interpolated products using sampling step sizes (a) 1, (b) 100, (c) 500, and (d) 1000. (e, f, g, and h) The corresponding difference between the interpolated results and the labeled data.}
\label{fig17}
\end{figure*}

\begin{table}[htbp]
    \centering
    \caption{Comparison of denoising MSE between $x_0$-targeted and noise-targeted GSFM across different noise levels}
    \label{tab4}
    \begin{tabular}{@{} c c c c c c @{}}
        \toprule
        \multirow{2}{*}{Noise Level} & \multirow{2}{*}{GSFM} & \multicolumn{4}{c}{Predicting noise} \\ \cmidrule(lr){3-6} 
         &  & $T=1$ & $T=100$ & $T=500$ & $T=1000$ \\ 
        \midrule
        10\% & \textbf{3.09e-07} & 1.57e-01 & 2.84e-02 & 1.01e-05 & 2.50e-01 \\
        20\% & \textbf{9.20e-07} & 1.63e-01 & 8.3e-03  & 1.28e-05 & 2.40e-01 \\
        30\% & \textbf{1.67e-06} & 1.64e-01 & 7.1e-03  & 1.58e-05 & 4.46e-06 \\
        40\% & \textbf{2.61e-06} & 1.61e-01 & 3.43e-05 & 1.46e-05 & 7.63e-06 \\
        50\% & \textbf{3.79e-06} & 1.61e-01 & 3.32e-05 & 1.73e-05 & 1.30e-01 \\
        60\% & \textbf{4.60e-06} & 1.60e-01 & 6.40e-03 & 4.50e-03 & 1.53e-01 \\
        \bottomrule
    \end{tabular}
\end{table}


\begin{table}[htbp]
    \centering
    \caption{Comparison of interpolation MSE between $x_0$-targeted and noise-targeted GSFM across different missing levels}
    \begin{tabular}{@{} c c c c c c @{}}
        \toprule
        \multirow{2}{*}{\centering Missing Level} & \multirow{2}{*}{\centering GSFM} & \multicolumn{4}{c}{Predicting noise} \\ \cmidrule(lr){3-6}
         &  & $T=1$ & $T=100$ & $T=500$ & $T=1000$ \\ 
        \hline
        10\% & \textbf{1.45e-08} & 1.58e-01 & 9.20e-03 & 1.28e-05 & 3.88e-06 \\
        20\% & \textbf{1.93e-08} & 1.66e-01 & 9.63e-04 & 1.27e-05 & 2.50e-01 \\
        30\% & \textbf{2.85e-08} & 1.66e-01 & 1.90e-05 & 4.93e-05 & 3.03e-06 \\
        40\% & \textbf{8.54e-08} & 1.62e-01 & 3.20e-05 & 1.13e-05 & 2.39e-06 \\
        50\% & \textbf{4.08e-08} & 1.65e-01 & 1.10e-03 & 2.17e-01 & 1.80e-06 \\
        60\% & \textbf{3.65e-07} & 1.60e-01 & 4.40e-03 & 1.32e-05 & 3.65e-06 \\
        \bottomrule
    \end{tabular}
    \label{tab5}
\end{table}

\subsection{Comparison of different sampling steps}
As stated earlier, the GDMs begin with random noise and gradually reduce the noise through a sequence of iterative steps to generate the final prediction. In the previous subsection, we observed that the performance of the GSFM trained to predict noise generally improves with an increasing number of sampling steps. However, for our GSFM designed to predict $x_0$, the synthetic and field data examples consistently used only one sampling step to maintain alignment with the benchmark methods and, also, to consider computational efficiency. This raises an important question: does using more sampling steps lead to better prediction quality for our $x_0$-targeted GSFM? 

To investigate this, we test the $x_0$-targeted GSFM with different sampling step configurations ($T=1$, 10, 50, 100, 500,and 1000) using synthetic test data. The corresponding MSE metrics for all four tasks are summarized in Table \ref{tab6}. We can see that the performance of our $x_0$-targeted GSFM remains highly stable across varying sampling steps. This finding suggests that our $x_0$-targeted GSFM is highly efficient and robust, achieving optimal performance with minimal sampling steps. This is an important advantage for practical applications, as it reduces the computational burden associated with the iterative nature of GDMs. Unlike noise-targeted models, where longer sampling steps are often necessary to achieve acceptable results, the $x_0$-targeted GSFM demonstrates consistent performance even with a single sampling step, highlighting its practical value in SPTs. 

\begin{table}[htbp]
    \centering
    \caption{The MSE metric of our \(x_0\)-targeted GSFM using different sampling steps}
    \label{tab6}
    \begin{tabular}{@{} c c c c c c c @{}}
        \toprule
        Task & \(T=1\) & \(T=10\) & \(T=50\) & \(T=100\) & \(T=500\) & \(T=1000\) \\ 
        \midrule
        Denoising                        & 1.67e-06 & 1.61e-06 & 1.61e-06 & 1.61e-06 & 1.61e-06 & 1.61e-06 \\
        Backscattered noise attenuation & 9.59e-07 & 9.59e-07 & 9.59e-07 & 9.59e-07 & 9.59e-07 & 9.58e-07 \\
        Interpolation                   & 4.08e-08 & 4.08e-08 & 4.07e-08 & 4.08e-08 & 4.07e-08 & 4.07e-08 \\
        Low-frequency extrapolation     & 6.0e-07  & 5.91e-07 & 6.08e-07 & 6.16e-07 & 6.05e-07 & 6.05e-07 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Computation and memory consumption}
Our GSFM employs an U-Net-based network architecture, which is shared with the two benchmarks provided in this study, differing only in minor aspects. Consequently, the computational time and memory consumption for pre-training our GSFM are nearly identical to those of the benchmarks. This consistency extends to the inference phase as well, where the GSFM demonstrates remarkable efficiency. 

During the fine-tuning stage, the iterative refinement strategy allows for progressive improvement of the GSFM's performance on real data. The time consumed during fine-tuning depends on the decision to terminate refinement based on the model's output at the current stage. In our field data examples, we observed that using the same total number of iterations as the conventional pre-training fine-tuning paradigm already produced satisfactory results with the iterative refinement strategy. This demonstrates that the cost of fine-tuning GSFM remains practical and manageable. 

By adopting an $x_0$-target-based GDM, our GSFM achieves comparable results with a single sampling step to those obtained with multiple sampling steps. This unique feature significantly reduces the computational burden during inference stage. As a result, the time and memory requirements for inference in our GSFM are also comparable to those of the benchmarks. 

Overall, the computational and memory efficiency of our GSFM is comparable to the benchmarks during both pre-training, fine-tuning, and also inference stages. Furthermore, the iterative fine-tuning strategy ensures that the additional time cost associated with refinement remains reasonable, making GSFM a computationally efficient solution for SPTs. 

\subsection{Limitations and Future work}
The core idea behind our GSFM is to leverage GDMs to capture and learn the joint distribution of seismic data. Specifically, we consider seismic shot gathers and define the "perfect" shot gather as one that is clean, complete, and broadband (also for example in the future free of multiples). By pre-training the GSFM on synthetic data and fine-tuning it on field data, we aim for GSFM to capture these desirable distribution characteristics. However, certain SPTs require transformations across fundamentally different domains, which present unique challenges. 

For example, tasks like velocity analysis require transforming seismic shot gathers into a smooth background velocity model, effectively converting data from the time domain to the depth domain. To adapt GSFM for such tasks, we could retain the shot gather as one input channel while providing the target background velocity model as the label in another channel. In this case, the GSFM would need to learn the distribution of background velocity models. However, the distribution of background velocity models is inherently disjoint from the distribution of seismic shot gathers. This divergence poses significant challenges for GSFM optimization, as it becomes difficult for the model to learn and represent such disparate distributions within the same framework. 

This limitation highlights a current constraint of our GSFM: it struggles to handle tasks where the target data is not of the same domain as the input data. For pure seismic preprocesing, this is fine. However, for additional tasks like velocity model building, this is a limitation. Addressing this limitation is a key focus for future research. In subsequent work, we aim to extend the capabilities of GSFM by developing strategies to effectively handle such non-overlapping distributions. This may involve introducing modular designs or hybrid frameworks that allow the model to adaptively learn and represent multiple distinct distributions, enabling broader applicability to a wider range of SPTs.

