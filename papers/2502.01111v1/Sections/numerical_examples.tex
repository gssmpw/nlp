\section{Synthetic data examples}
In this section, we first introduce the pre-training details of the GSFM, including dataset preparation and training configuration. We then evaluate the pre-trained GSFM's performance on denoising, backscattered noise attenuation, interpolation, and low-frequency extrapolation tasks using synthetic test data. 

To assess the effectiveness of the pre-trained GSFM, we provide two comparative experimental benchmarks:
\begin{itemize}
    \item \textbf{Benchmark 1}: Traditional NN-based processing paradigm. \\
     This benchmark utilizes conventional NN-based seismic processing methods, employing the networks to approximate the nonlinear relationship between input data and target data. To ensure a fair comparison, Benchmark 1 adopts the same U-Net-based architecture as GSFM, but excludes the time encoding module used in the diffusion model. The network takes single-channel degraded data as input and outputs the corresponding target data. This comparative benchmark effectively demonstrates the advantages of a diffusion model-guided network over traditional training approaches.

     \item \textbf{Benchmark 2}: Conventional pre-training and fine-tuning strategy. \\
     In this benchmark, we first pre-train a NN on synthetic data using an SSL approach, followed by fine-tuning for denoising, backscattered noise attenuation, and low-frequency extrapolation tasks. Again, to ensure fairness, we use the same U-Net architecture as GSFM but remove both the task encoding and time encoding modules. During the pre-training phase, we use the GSFM dataset for all tasks, constructing the input data using random masking. During fine-tuning phase, the training is conducted on each individual task's dataset to evaluate performance across different SPTs.
\end{itemize}

\subsection{Pre-training configuration}
Creating synthetic subsurface models that represent the real Earth remains a challenge. For our purposes, we closely follow the workflow introduced by \cite{ovcharenko2022multi} to generate random velocity models, which have been shown to effectively generalize to real data. Specifically, first, we randomly create 1D compressional wave velocity ($V_p$) profiles using velocity values within our expected range of 1,500 to 4,500 m/s. These 1D profiles are then spread laterally to build 2D laterally homogeneous layered velocity models. Lastly, we apply random elastic transforms to the velocity models to distort them and introduce structures resembling realistic geological phenomena (folding, intrusion, etc.) 

Since we aim to establish a foundation model applied to seismic waveforms, it is of utmost importance that the synthetic waveform for the training dataset is as close as a realistic waveform, which justifies the need to use an elastic modeling engine. We use a Pytorch-based seismic modeling and inversion package called Deepwave \citep{richardson_alan_2023} to perform 2D elastic forward modeling on the aforementioned velocity models. The shear wave velocity ($V_s$) is obtained through a fixed ratio of $V_p /\sqrt(3)$, while the density ($\rho$) is obtained through Gardner's relation \citep{gardner1974formation}. The discretization of the subsurface parameters for the modeling is detailed in Table \ref{tab1}. For the acquisition setting, we consider a marine environment where the data is acquired through a towed streamer consisting of an array of hydrophones that records a pressure component from an airgun source. The airgun source is represented by a Ricker wavelet with a peak frequency of 7 Hz. We set the number of receivers to 324 for every shot. More details of the acquisition parameters are listed in Table \ref{tab1}.

\begin{table}[]
    \centering
    \caption{Parameters for modeling of the synthetic pre-training dataset.}
    \begin{tabular}{ccc}
         \toprule
         Parameter &  Description & Value \\
         \midrule
         nx & Number of samples in the X axis & 324 \\
         nz & Number of samples in the Z axis & 376 \\
         dx & Sampling step in the X axis & 25 m \\
         dz & Sampling step in the Z axis & 25 m\\
         dt & Recording sampling step & 376 \\
         nt & Number of recording timesteps & 1.6e-2 s \\
         T & Total recording time (nt $\times$ dt) & 6.016 s \\
         % ns & Number of shots & \\
         nr & Number of receivers & 324 \\
         ds & Receiver spacing  & 25 m \\
         % dr & Shot spacing & \\
         \bottomrule
    \end{tabular}
    \label{tab1}
\end{table}

In the pre-training phase, we generate a total of 2456 training samples for each task. As our framework simultaneously trains on four SPTs (denoising, backscattered noise attenuation, interpolation, and low-frequency extrapolation), the overall dataset comprise 9824 training samples. We employ the AdamW optimizer with a fixed learning rate of $1e-4$ and a batch size of 5. To enhance the stability of the diffusion model training process, we apply an exponential moving average (EMA) with a rate of 0.999. The pre-training is conducted over 200,000 iterations. 

For a fair comparison, the two benchmark models use the same training configuration, except for the EMA mechanism, as their training processes are stable and do not require it. Once pre-training is completed, we evaluate the performance of the pre-trained GSFM and benchmarks on synthetic test data. During inference, to ensure consistency and fairness across all models, we use a single sampling step (i.e., predicting directly at the final step) for generating the synthetic test results. This setup allow us to comprehensively compare the performance of our pre-trained GSFM with the benchmarks. 

\subsection{Denoising}
We first test the denoising performance of the pre-trained GSFM on synthetic data contaminated by random noise. Figure \ref{fig2} illustrates the denoising products of the three methods. Panel (a) shows the clean data, while panel (b) represents the noisy test data, which is generated by injecting Gaussian noise with a noise level of 30\%, as follows:
\begin{equation}\label{eq14}
y=x+\epsilon \cdot std(x) \cdot rand(0,1),
\end{equation}
where $\epsilon$ is the noise level, $std(x)$ represents the standard deviation of the clean data $x$, and $rand(0,1)$ is the standard normal distribution. The denoised results for GSFM, Benchmark 1, and Benchmark 2 are displayed in panels (c), (d), and (e) (Figure 2), respectively. The difference between the denoised results and the clean data are presented in panels (f), (g), and (h), respectively. 

Visually, the denoised results from our GSFM and two benchmarks appear very similar, with each method successfully suppressing the random noise and preserving the main seismic reflection events. The differences between the methods are subtle and difficult to evaluate qualitatively, as all methods produce results with comparable reflection continuity and noise suppression. 

To provide a more objective assessment of their performance, Table \ref{tab2} shows a quantitative evaluation of the denoising performance in terms of the MSE metric across different noise levels (10\% to 60\%). The results reveal that GSFM consistently outperforms Benchmark 1 across all noise levels, demonstrating lower MSE values. It is worth noting that Benchmark 1 and GSFM share almost identical architectures, with the only difference being that Benchmark 1 excludes the time encoding module used in the diffusion process. Despite this, GSFM consistently outperforms Benchmark 1 in terms of MSE. This performance gap highlights the significant contribution of GDMs, which contributes to the enhanced denoising performance of the networks. 

Benchmark 2 shows slightly better performance than GSFM at intermediate noise levels (20\% to 50\%), which is likely attributed to one more round of task-specific fine-tuning on labeled data. However, the performance gap is marginal, and GSFM demonstrates superior robustness at the highest noise level. This implies that even without fine-tuning, our pre-trained GSFM has achieved denoising capabilities that match those of the fine-tuned network.


\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figure/fig2.png}
\caption{Denoising performance comparison between our pre-trained DSFM and two benchmarks on synthetic data. (a) The clean and (b) noisy data, where the noisy data is created by injecting random noise with a level of 30\% into the clean data. The denoised products from (c) our GSFM, (d) Benchmark 1, and (e) Benchmark 2. f, g, and h are the corresponding difference between the denoised results and the clean data. }
\label{fig2}
\end{figure*}


\begin{table}[htbp]
    \centering
    \caption{MSE comparison of denoising performance at different noise levels}
    \begin{tabular}{cccc}
        \toprule
        Noise level & GSFM & Benchmark 1 & Benchmark 2 \\
        \midrule
        10\% &   \textbf{3.09e-07} & 3.44e-07 & 3.21e-07 \\
        20\% &  9.20e-07 & 9.50e-07 & \textbf{9.02e-07} \\
        30\% &  1.67e-06 & 1.73e-06 & \textbf{1.65e-06} \\
        40\% &  2.61e-06 & 2.68e-06 & \textbf{2.59e-06} \\
        50\% &  3.79e-06 & 3.91e-06 & \textbf{3.77e-06} \\
        60\% &  \textbf{4.60e-06} & 4.81e-06 & 4.61e-06 \\
        \bottomrule
    \end{tabular}
    \label{tab2}
\end{table}


\subsection{Backscattered noise attenuation}
We, then, evaluate the performance of our pre-trained GSFM in attenuating backscattered noise. Figure \ref{fig3} displays the backscattered noise attenuation results for the three methods. Panel (a) shows the clean seismic data, while panel (b) displays the input data contaminated with backscattered noise. The denoised products from GSFM, Benchmark 1, and Benchmark 2 are presented in panels (c), (d), and (e), respectively, and the corresponding residuals are shown in panels (f), (g), and (h), respectively. 

Similar to the denoising case, the visual differences among the results produced by the three methods are minimal. All methods successfully suppress the backscattered noise and preserve the primary seismic reflections. The residuals reveal that all methods reduce the noise effectively. We further compute the MSE metric for the predicted results of each method. GSFM achieves the lowest MSE of $9.59e-07$, outperforming Benchmark 1 ($1.10e-06$) and Benchmark 2 ($1.26e-06$). This demonstrates GSFM's superior ability to attenuate backscattered noise while preserving the seismic signal.

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figure/fig3.png}
\caption{Backscattered noise attenuation performance comparison between our pre-trained DSFM and two benchmarks on synthetic data. (a) The clean and (b) noisy data contaminated with backscattered noise. The denoised products from (c) our GSFM, (d) Benchmark 1, and (e) Benchmark 2. f, g, and h are the corresponding difference between the denoised results and the clean data. }
\label{fig3}
\end{figure*}

\subsection{Interpolation}
Furthermore, we evaluate the interpolation performance of our pre-trained GSFM. Figure \ref{fig4} shows the interpolation results for synthetic data with 50\% randomly missing traces. Panel (a) displays the complete labeled data, while panel (b) shows the input data with missing traces. The interpolated results from GSFM, Benchmark 1, and Benchmark 2 are presented in panels (c), (d), and (e), respectively. The corresponding differences between the interpolated results and the complete data are shown in panels (f), (g), and (h), respectively. 

We can see that all three methods achieve visually similar interpolated products, successfully reconstructing the missing traces with less signal leakage. More quantitatively, Table \ref{tab3} summarizes the MSE metrics of the interpolated results across different missing data levels (10\% to 60\%). The results reveal the following trends: 1. At low missing data levels (10\%), GSFM achieves the lowest MSE ($1.45e-08$), outperforming both benchmarks. 2. At intermediate missing data levels (20\% to 50\%), Benchmark 2 slightly outperforms GSFM, demonstrating its effectiveness in handling moderate missing levels due to its task-specific learning on pre-training stage. However, GSFM consistently performs better than Benchmark 1, highlighting its robustness. 3. At the highest missing data level (60\%), GSFM significantly outperforms both benchmarks, achieving an MSE of $3.65e-07$. This result demonstrates GSFM's superior ability to handle highly missing input data. 

Once again, these results demonstrate that the diffusion model boosts the performance of the networks, enabling GSFM to outperform Benchmark 1 consistently. This trend was also observed in the denoising tests, confirming the effectiveness of the diffusion-guided training paradigm. 

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figure/fig4.png}
\caption{Interpolation performance comparison between our pre-trained DSFM and two benchmarks on synthetic data. (a) The complete (label) and (b) incomplete data, where the incomplete data is created by randomly removing 50\% of traces from the complete data. The interpolated products from (c) our GSFM, (d) Benchmark 1, and (e) Benchmark 2. f, g, and h are the corresponding difference between the interpolated results and the labeled data. }
\label{fig4}
\end{figure*}

\begin{table}[htbp]
    \centering
    \caption{MSE comparison of interpolation performance at different missing levels}
    \begin{tabular}{cccc}
        \toprule
        Missing level & GSFM & Benchmark 1 & Benchmark 2 \\
        \midrule
        10\% &   \textbf{1.45e-08} & 3.60e-08 & 1.73e-08 \\
        20\% &  1.93e-08 & 4.02e-08 & \textbf{1.88e-08} \\
        30\% &  2.85e-08 & 4.51e-08 & \textbf{2.30e-08} \\
        40\% &  8.54e-08 & 8.69e-08 & \textbf{5.21e-08} \\
        50\% &  4.08e-08 & 6.39e-08 & \textbf{3.45e-08} \\
        60\% &  \textbf{3.65e-07} & 5.16e-07 & 5.53e-07 \\
        \bottomrule
    \end{tabular}
    \label{tab3}
\end{table}

\subsection{Low-frequency extrapolation}
Finally, we focus on assessing the capability of our pre-trained GSFM in low-frequency extrapolation, a critical SPT which is particularly beneficial for full-waveform inversion. The extrapolation results for a test data, which miss low-frequencies below 4 Hz, are illustrated in Figure \ref{fig5}. The reference data, including low frequencies, is shown in panel (a), while panel (b) displays the input without low-frequency information. The extrapolated outputs generated by our pre-trained GSFM, Benchmark 1, and Benchmark 2 are depicted in panels (c), (d), and (e), respectively. Panels (f), (g), and (h) highlight the differences between the extrapolated outputs and the reference data. 

Unlike the previous tasks, the difference figures here clearly showcase the differences among the three methods. We can observe that both our GSFM and Benchmark 2 achieve superior extrapolation quality, with minimal residuals and negligible signal leakage. In contrast, Benchmark 1 exhibits more significant signal leakage, particularly near the lower-right region and close to the near-offsets. To complement the visual analysis, we compute the MSE metric between the extrapolated outputs and the reference data for all three methods. GSFM achieve an MSE of $6.0e-07$, while Benchmark 2 slightly outperforms GSFM with an MSE of $3.11e-07$. Benchmark 1, however, performed significantly worse, with an MSE of $1.80e-03$. 


\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figure/fig5.png}
\caption{Low-frequency extrapolation performance comparison between our pre-trained DSFM and two benchmarks on synthetic data. (a) The labeled and (b) input data, where the input data lacks low frequencies below 4 Hz. The extrapolated products from (c) our GSFM, (d) Benchmark 1, and (e) Benchmark 2. f, g, and h are the corresponding difference between the extrapolated results and the labeled data. }
\label{fig5}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figure/fig6.png}
\caption{Backscattered noise attenuation performance comparison between our fine-tuned DSFM and two benchmarks on field data. (a) The field noisy data contaminated with backscattered noise. The denoised products from (b) our fine-tuned GSFM, (c) Benchmark 1, and (d) Benchmark 2. e, f, and g are he corresponding difference between the denoised results and the field noisy data.}
\label{fig6}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figure/fig7.png}
\caption{Backscattered noise attenuation performance comparison between different fine-tuning strategies. (a) The prediction product from the pre-trained GSFM. The processed products from strategies (b) 1, (c) 2, and (d) 3. e, f, g, and h are the corresponding difference between the processed results and the field noisy data (see Figure \ref{fig6}a). }
\label{fig7}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figure/fig8.png}
\caption{Comparison of denoised products of the pre-trained GSFM and the fine-tuned GSFM at different stages. (a) The prediction product from the pre-trained GSFM. (b, c, and d) The prediction products from the fine-tuned stage 1, 5, and 10. e, f, g, and h are the corresponding difference between the processed results and the field noisy data (see Figure \ref{fig6}a). }
\label{fig8}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figure/fig9.png}
\caption{Interpolation performance comparison between different fine-tuning strategies. (a) The denoised product from the fine-tuned GSFM on backscattered noise attenuation task, which come from Figure \ref{fig6}b. (b) The incomplete data, where we artificially remove 50\% of the seismic traces from the denoised product. The interpolated products from (c) the pre-trained GSFM and the fine-tuned GSFM using strategies (d) 1, (e) 2, and (f) 3, respectively. g, h, i, and j are the corresponding differences between the interpolated results and the denoised data.}
\label{fig9}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figure/fig10.png}
\caption{Comparison of interpolated products of the pre-trained GSFM and the fine-tuned GSFM at different stages. (a) The interpolated product from the pre-trained GSFM. (b, c, and d) The interpolated products from the fine-tuned GSFM at the stages 1, 5, and 10. e, f, g, and h are the corresponding difference between the interpolated results and the denoised data (see Figure \ref{fig9}a). }
\label{fig10}
\end{figure*}

\subsection{Understanding performance differences among the methods}
The evaluations of our pre-trained GSFM, Benchmark 1, and Benchmark 2 across the four SPTs on synthetic data offer insights into their strengths, limitations, and fundamental differences. 

Benchmark 1, based on the conventional NN paradigm, consistently underperforms compared to GSFM and Benchmark 2. While its performance gap is less pronounced in the first three tasks, it becomes significantly evident in the low-frequency extrapolation task. We know that Benchmark 1 approximates the nonlinear relationship between the input and target data. For the first three tasks, the target data (clean, complete seismic data) remains consistent across tasks. This consistency enables the network to learn a more generalized mapping that performs adequately across these tasks. However, in the low-frequency extrapolation task, the target data shifts to clean, full-band seismic data, including low frequencies that are absent in the input. This change introduces a more specific and challenging relationship to learn, which the traditional paradigm struggles to approximate effectively. As a result, the network is biased towards learning a more generalized mapping, leading to insufficient focus on the specific relationship required for accurate low-frequency extrapolation. 

In contrast, GSFM, despite sharing the similar architecture as Benchmark 1, leverages the GDMs to capture and learn a more unified distribution. By modeling the joint distribution of clean, complete, and full-band seismic data, GSFM is able to bridge the gap between the input and target data more effectively, enabling it to achieve more accurate and robust results across a broader range of tasks. 

Benchmark 2 consistently demonstrates strong performance across tasks even slightly outperforms GSFM in terms of MSE for certain tasks. However, this slight advantage is achieved through task-specific fine-tuning, which relies heavily on labeled datasets and requires additional computational resources. Although additional fine-tuning can improve performance on synthetic data, our ultimate goal is field data. Since Benchmark 2 conducts fine-tuning using synthetic data, it still faces generalization challenges. In contrast, our GSFM does not rely on task-specific fine-tuning with labeled data. Instead, it undergoes fine-tuning directly on field data in an SSL manner, enabling it to address the generalization challenges faced by Benchmark 2. In the following section, we will share our field data test to highlight these advantages, showcasing GSFM’s effectiveness in addressing the complexities of diverse SPTs. 

\section{\textbf{Field data examples}}
In this section, we will go forward to fine-tune the pre-trained GSFM on real data and, then, evaluate the performance of our fine-tuned GSFM on field data across denoising, interpolation, and low-frequency extrapolation tasks. Also, we will examine the effectiveness of three different fine-tuning strategies, as outlined in the Fine-tuning subsection of the Method section. Finally, we discuss how GSFM can be leveraged for uncertainty quantification in seismic processing and, also, illustrate how use uncertainty quantification to guide our fine-tuning process.

\subsection{Field data and Fine-tuning configuration}
We use a marine field dataset to test our method. This dataset was acquired using a steamer survey in North West Australia. The original dataset consists of 1824 shot gathers activated with air gun sources, with an approximate horizontal spacing of 18.75 m and a sampling rate of 1 ms. Each shot gather contains 648 receivers, spaced 12.5 m apart. For testing purposes, we select every third shot gather starting from the left, resulting in a total of 200 shot gathers. To reduce the computational burden during training, the number of receivers in the field data was reduced to 324, and the time samples are downsampled from 6016 to 376, following the preprocessing used in \cite{harsuko2024optimizing}. 

During fine-tuning on field data using the three different strategies, we ensure a fair comparison by using the same total number of iterations, set to 30000. However, in Strategy 3, these 30000 iterations are divided into 10 stages. Specifically, in Algorithm \ref{alg1}, the total stages $S$ are set to 10, and the iterations per stage $N_{stage}$ are set to 3000. We emphasize that all additional fine-tuning configurations remain consistent. Specifically, the learning rate is fixed at $5e-5$, the batch size is set to 4, and the EMA rate is configured to 0.999. Furthermore, during the sampling process, the pseudo-label generation utilized a diffusion step respace of 1 from the original 1000 diffusion steps. This means that only a single sampling step is used for field data predictions, which significantly improves fine-tuning and processing efficiency to meet the demands of practical applications. 

We also emphasize that, since the field data does not include random noise, we do not fine-tune the pre-trained GSFM for the denoising task in this case. Instead, we independently optimize the pre-trained GSFM for the backscattered noise attenuation, interpolation, and low-frequency extrapolation tasks. During fine-tuning, we adopt a sequential workflow similar to the traditional seismic processing paradigm. Specifically:
\begin{itemize}
    \item \textbf{1. Backscattered noise attenuation}: Fine-tuning is first applied to this task to address inherent noise in the field data. Here, the noise added to pseudo labels is extracted from the area outside the first arrival.
    \item \textbf{2. Interpolation}: After obtaining denoised results, since the denoised data does not contain bad traces, we artificially remove 50\% of the seismic traces from the data to construct the incomplete seismic data, serving as the original fine-tuning dataset for interpolation task. To better reflect practical scenarios, where certain fixed receivers in a steamer are damaged, the indices of the missing 50\% traces hold same across the selected 200 shot gathers.
    \item \textbf{3. Low-Frequency extrapolation}: Finally, the denoised data is used as our initial training data for fine-tuning the GSFM to perform low-frequency extrapolation.
\end{itemize}

This sequential workflow ensures that each task builds upon the results of the previous task, aligning with real-world seismic processing practices. By fine-tuning GSFM directly on field data in an SSL manner, we aim to demonstrate its capability to address generalization challenges, particularly when labeled data is unavailable. In the subsequent subsections, we present the results for each task. 

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.75\textwidth]{Figure/fig11.png}
\caption{The MSE metric of the interpolation results of the selected 200 incomplete shot gathers from the pre-trained GSFM and the fine-tuned GSFM at different stages. The 200 incomplete shot gathers are obtained by removing a fixed 50\% of the traces from the denoising shot gathers. The Original Prediction legend represents the prediction results of the pre-trained GSFM for 200 incomplete shot gathers. The Fine-tuning stage 1, 5, and 10 legends correspond to the prediction results of the GSFM fine-tuned at stages 1, 5, and 10 for 200 incomplete shot gathers, respectively.}
\label{fig11}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{Figure/fig12.png}
\caption{Low-frequency extrapolation performance comparison between different fine-tuning strategies. (a) The denoised product from the fine-tuned GSFM on backscattered noise attenuation task, which come from Figure \ref{fig6}b. The predicted products using (b) pre-trained GSFM and the fine-tuned GSFM using strategies (c) 1, (d) 2, and (e) 3, respectively. The second and third columns correspond to frequency components less than 4 Hz and 2 Hz, respectively. }
\label{fig12}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{Figure/fig13.png}
\caption{Comparison of low-frequency extrapolation products of the pre-trained GSFM and the fine-tuned GSFM at different stages. (a) The extrapolated product from the pre-trained GSFM. b, c, and d are the extrapolated products from the fine-tuned GSFM at the stages 1, 5, and 10. The second and third columns correspond to frequency components less than 4 Hz and 2 Hz, respectively.}
\label{fig13}
\end{figure*}

\subsection{Backscattered noise attenuation}
we first evaluate the performance of our fine-tuned GSFM for backscattered noise attenuation on field data. We compare the outputs of our method against those from Benchmark 1 and Benchmark 2, which is detailed in last section. Furthermore, we explore the impact of various fine-tuning strategies and highlight the iterative improvements achieved with Strategy 3. 

Figure \ref{fig6} illustrates the comparison of the denoised results between our fine-tuned GSFM and the two benchmarks. Panel (a) shows the field data contaminated with backscattered noise, while panels (b), (c), and (d) present the denoised outputs produced by our fine-tuned GSFM, Benchmark 1, and Benchmark 2, respectively. The corresponding differences between the denoised results and the noisy field data are shown in panels (e), (f), and (g), respectively. It is evident that both benchmarks fail to suppress the backscattered noise effectively, leading to severe signal leakage in their outputs. Specifically, compared with Benchmarks 1, Benchmarks 2 exhibit more significant signal leakage. In contrast, our fine-tuned GSFM demonstrates superior performance, successfully preserving the true signal while significantly reducing the noise. This highlights GSFM’s capability to adapt to field data through SSL fine-tuning, addressing the generalization challenges faced by the benchmarks. 

To evaluate the influence of different fine-tuning strategies, we compare the processed outputs in Figure \ref{fig7}. Panel (a) illustrates the result from the pre-trained GSFM before any fine-tuning, while panels (b), (c), and (d) show the outputs after fine-tuning with strategies 1, 2, and 3, respectively. The corresponding differences are shown in panels (e), (f), (g), and (h). The results reveal distinct differences between the strategies. Strategy 1 shows minimal improvement over the pre-trained GSFM, however in some areas, it even increases signal leakage. Strategy 2 provides moderate enhancements in noise reduction, yet residuals persist. Strategy 3, however, achieves substantial gains, effectively reducing noise and preserving the signal structure. This demonstrates the importance of a progressive fine-tuning process in adapting the model to complex field data. 

To further better understand the iterative improvement offered by Strategy 3, Figure \ref{fig8} presents the outputs at different fine-tuning stages. Panel (a) displays the pre-trained GSFM's output, while panels (b), (c), and (d) show the results from stages 1, 5, and 10 of fine-tuning. The corresponding differences are displayed in panels (e), (f), (g), and (h). The stepwise refinement process is clearly evident in these results. Early in the fine-tuning phase, such as stage 1, signal leakage remain prominent. By stage 5, the model demonstrates reduced signal leakage. At the final stage, stage 10, the processed output is of high quality, with excellent noise attenuation and signal preservation. This progressive refinement process reflects the iterative fine-tuning strategy's core strength: it allows GSFM to gradually capture the distribution of the field data, shifting its understanding from the synthetic data distribution toward the field data distribution. This adaptation is critical for real-world applications, as it ensures that the model generalizes effectively to field data even without labeled ground truth. 

\subsection{Interpolation}
We, then, explore the interpolation capabilities of our pre-trained GSFM and its fine-tuned versions when applied to field data. Different fine-tuning strategies are assessed to identify the most effective approach. Moreover, we analyze how the iterative refinement process enhances interpolation performance at successive stages. 

The interpolation results obtained using the pre-trained GSFM and the fine-tuned GSFM under different strategies are depicted in Figure \ref{fig9}. Panel (a) presents the denoised product from the backscattered noise attenuation task (Figure \ref{fig6}b), while panel (b) displays the incomplete data created by removing 50\% of the seismic traces. Panels (c), (d), (e), and (f) show the outputs of the pre-trained GSFM and the models fine-tuned using strategies 1, 2, and 3, respectively. The differences for each result relative to the original denoised data are illustrated in panels (g), (h), (i), and (j). The results show noticeable variations in performance across the strategies. Strategy 1, which minimally adjusts the pre-trained model, produces results that closely resemble the pre-trained GSFM’s output, leaving significant interpolation gaps unaddressed. Strategy 2 improves interpolation performance by reducing gaps, yet some residual inaccuracies remain. Strategy 3, in contrast, delivers the most refined results, reconstructing the missing traces with superior accuracy and significantly reducing errors. This progression highlights the advantage of iterative fine-tuning for aligning the model with field data. 

To better understand the iterative refinement enabled by Strategy 3, Figure \ref{fig10} displays the interpolated results at different stages of fine-tuning. The initial result from the pre-trained GSFM is presented in panel (a), while panels (b), (c), and (d) show the outputs after stages 1, 5, and 10 of fine-tuning, respectively. The corresponding differences are displayed in panels (e), (f), (g), and (h). With each fine-tuning stage, the quality of interpolation improves. At stage 1, the model addresses some missing traces but leaves substantial residuals. By stage 5, the interpolation becomes more accurate, with reduced signal leakage. Stage 10 achieves optimal results, effectively reconstructing the missing traces with minimal discrepancies. This progression demonstrates the power of Strategy 3 in leveraging iterative updates to adapt the model's predictions to the field data distribution. 

Since the incomplete field data is generated by artificially removing 50\% of the seismic traces from the denoised data, we can obtain corresponding labeled data. As a result, we can provide a quantitative perspective to evaluate the interpolation performance across all 200 shot gathers at different fine-tuning stages. The mean squared error (MSE) for interpolation is plotted in Figure \ref{fig11}. The curve labeled "Original Prediction" represents the pre-trained GSFM, while the curves for stages 1, 5, and 10 correspond to the respective fine-tuning iterations. The MSE trends reveal a clear improvement as fine-tuning progresses. Compared to the pre-trained GSFM, stage 1 achieves a noticeable reduction in MSE. By stage 5, the interpolation accuracy improves further, with MSE values continuing to decline. Stage 10 marks the culmination of the process, yielding the lowest MSE across all shot gathers. This trend underscores the effectiveness of the iterative refinement process in progressively aligning the model with the complexities of field data. 

\subsection{Low-frequency extrapolation}
We, finnaly, assess the performance of our pre-trained and fine-tuned GSFM models on the low-frequency extrapolation task for field data. By analyzing results across different fine-tuning strategies, we demonstrate how the iterative refinement process enables the model to progressively capture missing low-frequency components in real data. 

Figure \ref{fig12} presents a detailed comparison of the low-frequency extrapolation results produced by the pre-trained GSFM and models fine-tuned with Strategies 1, 2, and 3. Panel (a) displays the denoised product obtained from the backscattered noise attenuation task (see Figure \ref{fig6}b), which serves as the input for extrapolation. Panels (b), (c), (d), and (e) show the results from the pre-trained GSFM and the fine-tuned models using Strategies 1, 2, and 3, respectively. To provide a focused view of the low-frequency content, the second and third columns highlight frequency components below 4 Hz and 2 Hz, respectively, where we use the low-pass filter to leave the low-frequency component. The results clearly illustrate the progression in reconstruction quality across the strategies. Strategy 1 offers only negligible improvements over the pre-trained GSFM. Strategy 2 delivers better results, reconstructing slight more of the missing low-frequency details. However, it struggles to effectively recover frequencies below 2 Hz. In contrast, Strategy 3 achieves the most substantial enhancement, recovering nearly complete low-frequency components both below 4 Hz and 2 Hz. This demonstrates that Strategy 3 is effective in adapting the model to field data and recovering challenging low-frequency details. 

Again, to investigate the refinement process under Strategy 3, Figure \ref{fig13} illustrates the extrapolated outputs at different fine-tuning stages. Panel (a) represents the prediction from the pre-trained GSFM, while panels (b), (c), and (d) show the results at fine-tuning stages 1, 5, and 10, respectively. The corresponding frequency components below 4 Hz and 2 Hz are highlighted in the second and third columns. The progression across stages demonstrates the strength of iterative refinement. In the early stages, the model begins to reconstruct low-frequency content, but the frequencies below 2 Hz remain less apparent. By stage 5, improvements become evident. At stage 10, the model delivers its best performance, with nearly complete recovery of low-frequency components, including those below 2 Hz. This gradual enhancement further underscores how Strategy 3 enables the GSFM to iteratively align with the field data distribution and, thus, improve the performance of GSFM on real data. 

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figure/fig14.png}
\caption{The mean of multiple interpolation results under same condition and the corresponding uncertainty, where the condition is the incomplete real data (see Figure \ref{fig9}b). (a) The mean of multiple interpolation results. (b) The differences between the mean of multiple interpolation results and the labeled data, where the labeled data is the denoised data (see Figure \ref{fig6}b). (c) The uncertainty of interpolation result.}
\label{fig14}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{Figure/fig15.png}
\caption{The uncertainty dynamics at fine-tuning process for field data interpolation. (a, b, c, d) The mean of 50 interpolation predictions generated by the pre-trained GSFM, and the fine-tuned GSFM at stages 1, 5, and 10, respectively. (e, f, g, h) The difference between the mean interpolation results and the labeled data (denoised results) at the corresponding stages. (i, j, h, l) The uncertainty calculated as the standard deviation of 50 predictions for each model at the corresponding stages.}
\label{fig15}
\end{figure*}

\subsection{Uncertainty quantification}
Traditional NN-based seismic processing paradigms and, also, pre-training and fine-tuning strategies lack the ability to effectively quantify the uncertainty of their processing products. In practical applications, uncertainty quantification is critical as it provides insight into the reliability of the processing results, aiding in decision-making processes. Due to the inherent probabilistic nature of GDMs, they naturally lend us to estimating the uncertainty of processing results. Although our GSFM employs a deterministic sampling process by targeting $x_0$ and setting $\sigma_t=0$, we emphasize that the sampling process still originates from random noise. Therefore, when the seed for sampling is randomized, the initial random noise varies, leading to slight differences in the generated processing results. Consequently, we can still effectively perform uncertainty quantification. 

The method for uncertainty quantification is straightforward. The input condition, e.g., the incomplete seismic data, is replicated $B$ times along the batch dimension. Then, $B$ different random noise are sampled, and the GSFM processes these inputs to generate $B$ corresponding predictions. By calculating the mean of these $B$ predictions along the batch dimension, we obtain the final predicted result. Simultaneously, the standard deviation of the $B$ predictions along the batch dimension provides an indication of variance that can be used for uncertainty quantification. 

To illustrate this, we conduct a test on the interpolation task using field data. Specifically, the incomplete field data (see Figure \ref{fig9}b) is replicated 50 times along the batch dimension, and 50 different random noise samples are used during the prediction process. As a result, we obtain 50 interpolation results, where we use the GSFM fine-tuned at the 10th stage. The mean and standard deviation of these predictions are shown in Figure \ref{fig14}. Panel (a) shows the mean of 50 interpolation results, while panel (b) depicts the difference between the mean interpolation result and the labeled data (see Figure \ref{fig6}b). Panel (c) denotes the uncertainty of interpolation result. We can see that the regions with significant signal leakage exhibit higher uncertainty values, as indicated by the red arrows. This correlation demonstrates that the calculated uncertainty effectively identifies regions in the processed results with higher errors. We emphasis that for the incomplete data used here, derived by removing 50\% of traces from the denoised data, the labeled data is available for direct comparison. This allows us to identify the signal leakage and assess each model’s performance. However, in practice, for field data without available labels, evaluating the prediction quality becomes challenging. In such cases, the uncertainty quantification (Panel (c)) provides a valuable alternative. Such information is invaluable for assessing the quality and reliability of seismic processing results, significantly enhancing the confidence in subsequent applications. 

Actually, the correlation between reduced uncertainty and improved interpolation performance further inspires us to explore whether such uncertainty measures can guide the fine-tuning process. To demonstrate this, Figure \ref{fig15} further explores the uncertainty quantification across pre-trained and GSFM at different fine-tuning stages (stages 1, 5, and 10). Each row in Figure \ref{fig15} corresponds, from top to bottom, to the pre-trained GSFM and the GSFM fine-tuned at the 1st, 5th, and 10th stages, respectively. The columns, from left to right, represent the mean of the predictions from 50 samples, the difference between the mean prediction and the labeled data, and the uncertainty. 

From Figure \ref{fig15}, we can observe the following trends:
\begin{itemize}
    \item \textbf{Prediction consistency}: Across all stages, the mean prediction results appear visually similar, indicating that noticeable performance differences are not easily discernible without a direct comparison of residuals or uncertainty.
    \item \textbf{Reduction in signal leakage}: The residual figures show that as the fine-tuning progresses (from stage 1 to stage 10), the signal leakage consistently decreases, confirming the improvement in interpolation accuracy.
    \item \textbf{Uncertainty dynamics}: The standard deviation measure figures reveal a steady reduction in uncertainty as fine-tuning progresses. This decrease aligns with the reduced signal leakage observed in the residuals, emphasizing the strong relationship between uncertainty and performance.
\end{itemize}

Therefore, these findings highlight the potential of using uncertainty as a guiding metric during fine-tuning stage. Specifically, we can monitor the reduction in uncertainty to evaluate whether fine-tuning is effectively optimizing the model. If the uncertainty stabilizes and shows no significant reduction across successive fine-tuning iterations, it may indicate convergence, suggesting that further fine-tuning is unnecessary.
