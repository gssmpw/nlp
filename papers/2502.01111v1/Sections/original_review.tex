\section{Review of conventional neural network-based seismic processing}
Traditional seismic processing methods often rely on explicit physical models and assumptions, which may not be fully applicable in complex media. In contrast, the advantage of NNs lies in their ability to automatically approximate such complex mapping relationships by extracting features from data, without relying heavily on prior assumptions.

Commonly, NN-based seismic processing methods can be viewed as a parameterized function approximator, which adjusts its internal weights through a training process to learn the nonlinear relationship that maps seismic data, $x_i$, (such as raw noisy data) to the desired output products, $y_i$, (such as denoised data). During this process, the network optimizes its parameters by minimizing the error between the NN output (prediction) and the target, $y_i$, to capture key features in the seismic data, which can be represented by the following loss function:
\begin{equation}\label{eq1}
L(\theta) = \frac{1}{N} \sum_{i=1}^N \| \text{NN}_\theta(x_i) - y_i \|^2,
\end{equation}
where $\theta$ is the set of parameters of the network, $\text{NN}_\theta(x_i)$ is the predicted output of the network for input $x_i$, $y_i$ is the corresponding target output, and $N$ is the number of training samples. By minimizing the loss function $L(\theta)$, the network continuously optimizes its parameters to make the predicted results as close as possible to the target output. 

To further enhance the performance of conventional NN-based seismic processing, a pre-training and fine-tuning paradigm has been proposed \citep{harsuko2022storseismic, sheng2023seismic}. The pre-training method involves an SSL training by reconstructing masked original seismic data, thus providing a good initial parameter set for downstream tasks. The objective can be expressed as the following loss function minimization problem:
\begin{equation}\label{eq2}
L(\theta) = \frac{1}{N} \sum_{i=1}^N \| \text{NN}_\theta(x_i^{masked}) - x_i \|^2
\end{equation}
where $\text{NN}_\theta(x_i^{masked})$ is the NN's reconstructed output for the masked input $x_i^{masked}$, and $x_i$ is the corresponding original input data. By performing the pre-training phase, the network can learn representations of the basic features of the original seismic data. On this basis, fine-tuning training on labeled datasets allows the network to better adapt to specific task requirements. This paradigm not only significantly improves the generalization performance of the network but also accelerates convergence and reduces the dependence on labeled data, thereby achieving more robust seismic processing under complex conditions. 

Despite the notable progress made by the current pre-training and fine-tuning paradigm in seismic data processing, we still face several challenges. First, the pre-training stage often relies on synthetic data, resulting in limited generalizability when addressing the complexities of real data. Meanwhile, due to the differences between pre-training tasks (e.g., reconstruction) and downstream tasks (e.g., denoising, interpolation, or low-frequency extrapolation), the model's performance may be constrained during task transfer, preventing it from fully leveraging the benefits of pre-training phase. Moreover, the dependency on labeled data during fine-tuning stage further restricts the paradigmâ€™s applicability when labeled data are scarce. 

To address these challenges, we, in the following, present a framework for a generative seismic foundation model (GSFM) based on generative diffusion models (GDM). This framework aims to capture ideal seismic data distribution features through multi-task learning and generative data distribution modeling, while training on synthetic data for various tasks to enable the model to handle various seismic processing tasks (SPTs) effectively. By incorporating task-specific encoding, the model can automatically identify and manage different SPTs during training. Furthermore, we introduce a gradual transfer strategy using an SSL approach to fine-tune the model on real data, progressively shifting its distribution from synthetic data to real data, thereby improving its performance in practical applications. 