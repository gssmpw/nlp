\section{Method}
In this section, we will first introduce the fundamental concepts of GDMs. Following that, we will present the framework for a GSFM based on GDMs. We will provide detailed illustrations on how we perform multi-task encoding, pre-training, fine-tuning, and prediction. Finally, we will introduce our network architecture. 

\subsection{Generative diffusion models}
GDMs are gaining attention for its strong capability to produce highly realistic samples. These models initially convert data into pure noise through a forward process and then progressively denoise it to recover the data in the reverse process. 

Within the denoising diffusion probabilistic model (DDPM), the forward process is defined as \citep{ho2020denoising}:
\begin{equation}\label{eq3}
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t} x_{t-1}, (1 - \alpha_t) \mathbf{I}).
\end{equation}
In the forward process, the data sample \( x_0 \) gradually transforms into a noisy version \( x_t \) at each step \( t \), controlled by the parameter \( \alpha_t \), with \( \alpha_t = 1 - \beta_t \), where \( \beta_t \) is a small constant specifying the noise variance incrementally introduced at each step. Thus, at each step, noise is added according to the Gaussian distribution \( \mathcal{N} \).  This process adds isotropic Gaussian noise, facilitated by the identity matrix \( \mathbf{I} \), and progressively diffuses the original data into a Gaussian noise distribution from step \( t = 1 \) to $t = T$. 

To express \( x_t \) directly in terms of \( x_0 \) and a noise term, we can use the reparameterization trick:
\begin{equation}\label{eq4}
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon,
\end{equation}
where \( \epsilon \sim \mathcal{N}(0, \mathbf{I}) \) is Gaussian noise, and \( \bar{\alpha}_t = \prod_{i=1}^t (1 - \beta_i) \). 

To learn how to reverse this process, the network is trained to predict the noise \( \epsilon \) added to the sample at each step \( t \). Given a noisy sample \( x_t \), the network predicts the noise term \( \epsilon_\theta(x_t, t) \). The objective of the network is to minimize the difference between the actual noise \( \epsilon \) (added during the forward process) and the predicted noise \( \epsilon_\theta(x_t, t) \). This is achieved by minimizing the mean squared error (MSE) between the added noise and the predicted noise:
\begin{equation}\label{eq5}
L(\theta) = \mathbb{E}_{x_0, \epsilon, t} \left[ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 \right]
\end{equation}
This objective trains the network to accurately predict the noise added at each step, and to do so, the network needs to store the features of the signal information diffused by this added noise. 

Once the network is trained, the reverse process, starting for a noise sample drawn from the Gaussian distribution, uses the networkâ€™s noise predictions \( \epsilon_\theta(x_t, t) \) to iteratively inject the stored signal and gradually remove noise to reconstruct a sample from the distribution it was trained on. Specifically, starting from pure noise \( x_T \), each step in the reverse process estimates the previous sample \( x_{t-1} \) from \( x_t \) by subtracting the predicted noise component. The reverse sampling equation can be expressed as:
\begin{equation}\label{eq6}
x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right) + \sigma_t z,
\end{equation}
where \( \epsilon_\theta(x_t, t) \) is the noise predicted by the trained network, \( \sigma_t z \) is an optional noise term for stochastic sampling, and \( z \sim \mathcal{N}(0, \mathbf{I}) \). 

In DDPM, the step-by-step denoising process is implemented through a Markov chain, which requires numerous time steps to gradually remove noise. As a result, the sampling speed of DDPM is relatively slow. In contrast, the denoising diffusion implicit model (DDIM) improves the sampling process of DDPM by removing the dependence on the Markov chain \citep{song2020denoising}. Actually, DDIM and DDPM share the same forward process, which can also control noise introduction through Equation \ref{eq4}. The reverse process of DDIM can be represented by the following sampling equation:
\begin{equation}\label{eq7}
x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \hat{x}_0 + \sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2} \, \epsilon_\theta(x_t, t) + \sigma_t z,
\end{equation}
with
\begin{equation}\label{eq8}
\hat{x}_0 = \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \, \epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}} 
\end{equation}
giving an estimate of the original data \( x_0 \) directly, \( \epsilon_\theta(x_t, t) \) is the noise term predicted by the neural network, and \( z \sim \mathcal{N}(0, \mathbf{I}) \) is a random noise term, and \(\sigma_t\) controls the level of this randomness. When \(\sigma_t = 0\), the sampling process becomes deterministic because the random noise term \( \sigma_t z \) is removed. If \(\sigma_t \neq 0\), it introduces a small random term \( \sigma_t z \) in the sampling, allowing for some variation in the generated samples. Since we aim to apply DDIM in seismic processing, we prefer a deterministic approach rather than producing random solutions. Therefore, we will set parameter \(\sigma_t = 0\) in the following.

In DDIM, an additional improvement involves directly training the network to predict the original clean image $x_0$ rather than focusing on the noise $\epsilon$ added to $x_0$ \citep{bansal2024cold}. This approach benefits from effectively leveraging the spatial coherence and semantic information within the image, enabling faster convergence and higher generation quality. In this case, the network's optimization target shifts to minimizing the difference between the predicted image $x_{0,\theta}(x_t, t)$ and the original image $x_0$, as follows:
\begin{equation}\label{eq9}
L(\theta) = \mathbb{E}_{x_0, \epsilon, t} \left[ \| x_0 - x_{0,\theta}(x_t, t) \|^2 \right]. 
\end{equation}

In the $x_0$-based prediction framework, the reverse sampling equation in DDIM can be simplified to:
\begin{equation}\label{eq10}
x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} x_{0,\theta}(x_t, t) + \sqrt{1 - \bar{\alpha}_{t-1}} \hat{\epsilon}(x_t, t)
\end{equation}
where $\hat{\epsilon}(x_t, t)$ is an estimate of the added noise. Given Equation \ref{eq8} in the forward process, we can estimate the added noise $\hat{\epsilon}(x_t, t)$ in terms of $x_t$ and the network's prediction $x_{0,\theta}(x_t, t)$, as follows:
\begin{equation}\label{eq11}
\hat{\epsilon}(x_t, t) = \frac{x_t - \sqrt{\bar{\alpha}_t} x_{0,\theta}(x_t, t)}{\sqrt{1 - \bar{\alpha}_t}}.
\end{equation}

GDMs demonstrate excellent performance in generating high-quality samples due to its strong capability in capturing distributions and its stepwise denoising approach \citep{rombach2022high}. Specifically, DDIM significantly enhances sampling speed by eliminating the dependency on the Markov chain. Furthermore, by adopting a strategy based on predicting $x_0$, we are able to further improve both generation quality and sampling speed, which is needed to meet the accuracy and efficiency requirements of seismic processing. Based on this, we introduce the GSFM framework in the following section, which incorporates multi-task learning into GDM to handle a variety of SPTs, including denoising, interpolation, and low-frequency extrapolation in a unified manner.

\subsection{Generative seismic foundation model: Pre-training}
Our GSFM is adapted from a GDM and employs multi-task simultaneous pre-training on synthetic data, followed by direct fine-tuning on real data. However, in traditional GDMs, the model's input consists of a noisy version of a clean single-channel image, which is used to train the model for stepwise denoising. 

To accommodate the needs of multi-task seismic processing, we extend the input of our GSFM to a dual-channel structure. During the pre-training and fine-tuning phases, the dual-channel inputs may contain different content. In this section, we first explain how the dual-channel network inputs are configured during the pre-training phase. Since the network is optimized on synthetic data during the pre-training phase, we can access the labels for different tasks. Therefore, in this phase, the first channel contains a noisy version of the labels (the target complete clean data), while the second channel is used for the corresponding data to be processed, i.e., the degraded data specific to the task. The content of the second channel varies depending on the SPT, enabling the model to adapt flexibly to different tasks based on the input data. Specifically, the dual channels for different specified tasks are as follows:
\begin{itemize}
    \item \textbf{Denoising}: The second channel contains the data contaminated with noise we want the network to learn to remove.
    \item \textbf{Backscattered noise attenuation}: As a special case, the second channel contains data contaminated with backscattered noise. 
    \item \textbf{Interpolation}: The second channel contains data with missing traces. 
    \item \textbf{Low-frequency extrapolation}: The second channel contains data lacking low-frequency components. 
\end{itemize}

In the pre-training phase, the forward process of our GSFM shares the noise injection formulation of the conventional diffusion model, as shown in Equation \ref{eq4}, thereby constructing the content for the first channel of the dual-channel input. For the second channel, we can see that we essentially use the same input data as that used in conventional NN-based seismic processing methods. 

To enable simultaneous training for different tasks, we introduce a task encoding label $c$, allowing the network to identify and distinguish between various SPTs. For the tasks considered in this paper, including denoising, backscattered noise attenuation, interpolation, and low-frequency extrapolation, their class labels $c$ are defined as 0, 1, 2, and 3, respectively. The embedding method for the task encoding label $c$ is similar to that used for the step $t$. In the section \ref{network_architecture}, Network architecture, we will detail the specific embedding implementation for the task encoding label $c$. 

As previously mentioned, setting the GDM network's prediction target to $x_0$ can enhance generation quality and efficiency. Therefore, during the pre-training and also the following fine-tuning phase, the prediction target is set to $x_0$. In other words, for different SPTs, the network's prediction target corresponds to their respective labeled data. In this case, our pre-training objective can be expressed as:
\begin{equation}\label{eq12}
L(\theta) = \mathbb{E}_{x_0, x, \epsilon, t, c} \left[ \| x_0 - x_{0,\theta}(x_t, x, t, c) \|^2 \right], 
\end{equation}
where $x$ represents the second channel input serving as the conditional constraint. 

% As reviewed earlier, conventional NN-based seismic processing methods typically achieve specific tasks by learning the nonlinear mapping between input and labeled data, such as mapping noisy data to the corresponding clean data.

% Specifically, the dual channels and class labels for different specified tasks are as follows:
% \begin{itemize}
%     \item \textbf{Denoising}: The first channel contains a noisy version of the clean data, while the second channel contains data contaminated with random noise. The class label is set as $c = 0$.
%     \item \textbf{Backscattered noise attenuation}: The first channel contains a noisy version of the clean data, while the second channel contains data contaminated with backscattered noise. The class label is set as $c = 1$.
%     \item \textbf{Interpolation}: The first channel contains a noisy version of the complete data, while the second channel contains data with missing traces. The class label is set as $c = 2$.
%     \item \textbf{Low-frequency extrapolation}: The first channel contains a noisy version of data including low-frequency signals, while the second channel contains data lacking low-frequency components. The class label is set as $c = 3$.
% \end{itemize}

Here, we consider the four SPTs described above. However, we emphasize that our framework is flexible and can be extended to accommodate additional SPTs by simply defining the appropriate degraded data format for the second channel and assigning a new class encoding label for each added task. This adaptability allows our GSFM to serve as a versatile foundation for a wide range of seismic processing needs. For example, if our objective is for GSFM to remove surface multiples, we simulate shot gathers with free surface boundary condition to serve as input to the second channel, while having our clean data target modeled using absorbing boundary condition \citep{harsuko2024optimizing}.

\subsection{Generative seismic foundation model: Fine-tuning}\label{finetune_section}
After completing pre-training on synthetic data, our GSFM is directly fine-tuned on real data to enhance its generalization capability for practical applications. During the fine-tuning phase, due to the lack of labels, we employ an SSL-based optimization approach, maintaining the model's adaptability and stability across multi-task seismic processing. To ensure consistency, the fine-tuning process retains the embedding methods for the task encoding label $c$ during pre-training, enabling the model to continue supporting multi-task learning on real data and improving task transfer efficiency. The prediction target during fine-tuning remains set to $x_0$ (pseudo-labels), which represents the ideal output for each task. 

To accomplish fine-tuning, we perform this process independently for each SPT, using the pre-trained network as the starting point for each task and setting the task encoding label $c$ to the value corresponding to the desired task. We propose the following three fine-tuning strategies for each SPT:
\begin{itemize}
    \item \textbf{1}. The pre-trained model on synthetic data is used directly on the raw field data to generate preliminary processing products, which are then used as pseudo-labels during the fine-tuning phase. In this case, the first channel of the network input is a noisy version of the predicted pseudo-labels, while the second channel takes in shot gathers from the field data.

    \item \textbf{2}. The second channel of the network input differs from that of strategy 1. Instead of using the field data, we use a corrupted version (similar to the corruptions applied to the synthetic data) of the pseudo-labels. For example, for the denoising task, additional noise is added to the predicted pseudo-labels. For the backscattered noise attenuation task, backscattered noise is added to the pseudo-labels. For the interpolation task, traces are removed from the pseudo-labels. For the low-frequency extrapolation task, the low-frequency components are filtered out from the pseudo-labels.

    \item \textbf{3}. The third strategy is based on strategy 2 and involves iteratively updating the training dataset during the fine-tuning process. 
    The complete workflow is detailed in Algorithm \ref{alg1}. Specifically, the fine-tuning process is divided into multiple stages, with each stage consisting of several iterations. In the first stage, we maintain the configuration of strategy 2. In each subsequent stage, the model fine-tuned from the previous stage is used directly on the field data, generating new pseudo-labels. Diffusion process is added to these pseudo-labels to create the input for the first channel, while the second channel contains a further corrupted version of the newly generated pseudo-labels, consistent with strategy 2. 
\end{itemize} 

In subsequent experiments, we will test these three fine-tuning strategies to determine which one performs better in enhancing the model's generalization ability and processing performance. Based on our test results, the fine-tuned network induced by strategy 3 provides superior performance. This outcome is expected, as the multi-stage strategy with gradual optimization allows the model to achieve a smooth transition between the feature distributions of synthetic and real data. By updating and further degrading the pseudo-labels at each stage, the model progressively shifts from the synthetic domain to the real data domain during the fine-tuning process. This stepwise adjustment not only makes the model more robust in handling the complexity of real data but also effectively reduces the distribution gap between synthetic and real data, thereby improving the model's generalization capability in real-world tasks. 


\begin{algorithm}
\caption{Iterative Fine-Tuning with Progressive Pseudo-Labeling for GSFM}\label{alg1}
\textbf{Input:} Pre-trained GSFM model \\
\textbf{Input:} Raw field data $x$, initial pseudo-labels $x_{\text{pseudo}}$, noise $\epsilon$ \\
\textbf{Input:} Total stages $S$, iterations per stage $N_{\text{stage}}$ \\
\textbf{Input:} Task-specific corruption $\text{COR}[\cdot]$ \\
\textbf{Output:} Fine-tuned GSFM model \\
\textbf{-------------------------------- Fine-Tuning Process -----------------------------} 
\begin{algorithmic}
\State 1: Load pre-trained GSFM model.
\State 2: Set task-specific label $c$ according to the seismic processing task.
\State 3: Initialize pseudo-labels $x_{\text{pseudo}}$ by predicting on field data $x$ with the pre-trained model.
\State 4: \textbf{for} {stage $s = 1$ to $S$} \textbf{do}
\State 5: \quad \quad \textbf{for} {iteration $n = 1$ to $N_{\text{stage}}$} \textbf{do}
\State 6: \quad \quad \quad \quad Sample a step $t$ and add noise to pseudo-labels $x_{\text{pseudo}}$: $x_t = \sqrt{\bar{\alpha}_t} x_{\text{pseudo}} + \sqrt{1 - \bar{\alpha}_t} \epsilon$
\State 7: \quad \quad \quad \quad Apply task-specific corruption to $x_{\text{pseudo}}$: $\hat{x} = \text{COR}[(x_{\text{pseudo}}, c)]$.
\State 8: \quad \quad \quad \quad Forward pass $(x_t, \hat{x}, t, c)$ through the model: $x_{0,\theta}(x_t, \hat{x}, t, c) = \text{GSFM}(x_t, \hat{x}, t, c) $
\State 9: \quad \quad \quad \quad Compute loss with respect to target $x_{\text{pseudo}}$ for this task: \\ 
\quad \quad \quad \quad \quad \quad \quad \quad $L(\theta) = \mathbb{E} \left[ \| x_{\text{pseudo}} - x_{0,\theta}(x_t, \hat{x}, t, c) \|^2 \right]$
\State 10: \quad \quad \quad \quad Backpropagate the loss and update model parameters $\theta$
\State 11: \quad \quad \textbf{end for}
\State 12: \quad \quad After $N_{\text{stage}}$ iterations, use the fine-tuned model to generate the updated pseudo-labels: \\
\quad \quad \quad \quad \quad \quad \quad \quad $x_{\text{pseudo}} = \text{GSFM}(\epsilon, x, t, c)$
\State 13: \textbf{end for}
\State 14:  \textbf{Return:} Fine-tuned GSFM model
\end{algorithmic}
\end{algorithm}

\subsection{Generative seismic foundation model: Predicting}

After completing the fine-tuning process, our GSFM is ready to perform predictions for each seismic processing task independently. For each task-specific fine-tuned network, we obtain predictions tailored to the corresponding task. Unlike conventional NN-based seismic processing methods, which typically use a direct mapping approach, our GSFM leverages a generative prediction process due to its foundation in GDM. 

Specifically, for each SPT, we begin by assigning the corresponding task encoding label $c$ to indicate the target task. The network input is structured as follows: The first channel is initialized with random noise $\epsilon$, while the second channel contains seismic data $x$ that needs processing. Using the reverse process of GDM, we iteratively denoise the input to generate the desired output $x_0$. At each step $t$ in the reverse process, the model estimates $x_0$ based on the current noisy input $x_t$, and the reverse step is given by:
\begin{equation}\label{eq13}
x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \, x_{0,\theta}(x_t, x, t, c) + \sqrt{1 - \bar{\alpha}_{t-1}} \, \hat{\epsilon}(x_t, x, t, c),
\end{equation}
where $x_{0,\theta}(x_t, x, t, c)$ is the model's prediction of the clean data $x_0$ given the noisy input $x_t$ at step $t$ and task label $c$. Here, $\hat{\epsilon}(x_t, x, t, c)$ represents an estimate of the noise component, which can be computed as in Equation \ref{eq11}. 

The conventional prediction process continues iteratively, with the model starting from a high level of noise and gradually refining the input. The final output at last step, $x_0$, represents the processed data for the specified task, having been transformed from noise to the desired form through a series of denoising steps. However, considering the efficiency requirements in actual processing, we will only use one time step for the sampling process here. Specifically, we only use the last sampling step, that is, $t$ is set to 0 to get our final prediction product. \\

\subsection{Network architecture}\label{network_architecture}

Our GSFM adopts an enhanced U-Net-based architecture tailored for multi-task seismic processing. This architecture incorporates multi-scale feature extraction, task-specific embeddings, and attention mechanisms to deliver accurate and robust predictions. The main components of the network are illustrated in Figure \ref{fig1}, including convolutional layers, residual blocks, attention blocks, downsampling and upsampling layers, and embeddings for time and task-specific information. 

The GSFM processes dual-channel inputs $(x_t, x)$, where $x_t$ represents in training the target data input at timestep $t$, and $x$ contains the data to be processed specific to the task. These inputs are first passed through an initial $3 \times 3$ convolutional layer that maps the two input channels to 64 feature channels, preparing the data for hierarchical processing in the encoder-decoder structure. 

The encoder path progressively extracts hierarchical features using a combination of downsampling layers, residual blocks and attention blocks. Each downsampling layer reduces the spatial resolution by a factor of 2 and simultaneously doubles the number of feature channels, enabling the extraction of high-level features at coarser scales. Specifically, after the first downsampling operation, the number of channels increases from 64 to 128. Subsequent downsampling operations further increase the channels to 256 and 512. 

The decoder path restores the spatial resolution and reduces the number of channels in a symmetrical manner with respect to the encoder, combining high-level semantic information from the encoder with low-level spatial details via skip connections. Each upsampling layer doubles the spatial resolution and halves the number of feature channels. For example, the number of channels decreases from 512 to 256 after the first upsampling layer. This process continues until the final layer restores the original spatial resolution and reduces the channels back to 64. At the end of the decoder, a final output layer is applied. This layer consists of group normalization, followed by a sigmoid linear unit (SiLU) activation function, and a $3 \times 3$ convolutional layer that reduces the feature channels to the single-channel prediction $x_{0,\theta}(x_t, y, t, c)$. 

To address the requirements of multi-task processing, GSFM integrates two types of embeddings to guide the network with temporal and task-specific information:
\begin{itemize}
    \item \textbf{Time embedding layer} (Figure \ref{fig1}b): The timestep \(t\) is encoded using a sinusoidal positional encoding scheme \citep{vaswani2017attention}, which represents temporal information as a combination of sine and cosine functions. The resulting encoded vector is passed through a series of linear transformations and SiLU activation functions, producing the time embedding vector $t_{emb}$. This embedding vector is injected into the residual blocks to regulate the denoising process across timesteps.
    \item \textbf{Class embedding layer} (Figure \ref{fig1}c): Task-specific information is provided through a learnable embedding layer implemented using \texttt{torch.nn.Embedding}. The task encoding label $c$ is mapped to a high-dimensional embedding vector, which is further processed by linear transformations and gaussian error linear unit (GELU) activations, producing the class embedding vector $c_{emb}$. This embedding vector is incorporated into residual blocks to enable task-specific adaptability.
\end{itemize}

In our GSFM, feature extraction and refinement rely on the integration of residual and attention blocks:
\begin{itemize}
    \item \textbf{Residual blocks} (Figure \ref{fig1}d): Each residual block processes feature maps using a combination of group normalization, SiLU activation functions, and $3 \times 3$ convolutional layers. Task and time embeddings ($c_{emb}$ and $t_{emb}$) are incorporated by projecting them through linear layers and adding the resulting vectors to the feature maps. This design enables task- and time-aware feature processing.
    \item \textbf{Attention block} (Figure \ref{fig1}e): Attention blocks, which is developed by \cite{vaswani2017attention}, are applied to refine the feature maps further. These blocks compute query, key, and value matrices via $1 \times 1$ convolutional layers and normalize attention scores using a softmax operation. The resulting weighted feature maps are aggregated and processed through another $1 \times 1$ convolutional layer. This mechanism allows the network to focus on task-relevant regions, improving feature representation for seismic data.
\end{itemize}


The GSFM leverages downsampling and upsampling layers to capture features across multiple spatial scales. Downsampling layers reduce the spatial resolution of feature maps, enabling the extraction of high-level semantic features, while upsampling layers restore spatial resolution to match the input dimensions. Skip connections link encoder and decoder layers, combining fine-grained spatial details with deep semantic features, thereby improving the accuracy of task-specific predictions.


\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{Figure/fig1.png}
\caption{An illustration of our network architecture. (a) The overall network structure. (b) Time embedding layer. (c) Class embedding layer. (e) The residual block. (d) The attention block. }
\label{fig1}
\end{figure*}