\section{User Study and Data Collection}\label{sec:userstudy}

This section delineates the methodological approach employed in our user studies and data collection processes. The research protocol was approved by the university ethics committee (reference number CS16900), ensuring adherence to ethical standards in human subject research.


\subsection{Participants}

A cohort of 10 participants (4 females, 6 males) was recruited from the undergraduate and postgraduate communities at our university. The sample's age distribution ranged from 21 to 31 years (M = 26.4, SD = 3.14). All participants possessed either normal or corrected-to-normal vision. Among the participants, 50\% (n = 5) wore corrective lenses, with mean myopia measurements of 299 diopters (SD = 96.45) for the left eye and 334 diopters (SD = 148.40) for the right eye.

Given the study's focus on spatial and motion impacts, we collected additional anthropometric data. Participants' heights ranged from 155 cm to 185 cm (M = 172.5 cm, SD = 10.55). The arm length of the dominant hand varied from 52 cm to 63 cm (M = 57.2 cm, SD = 3.96), while interpupillary distances ranged from 6.02 cm to 6.50 cm (M = 6.32 cm, SD = 0.23).
A demographic questionnaire assessed participants' technology familiarity and usage patterns. On a 5-point Likert scale (1 = least familiar, 5 = most familiar), participants reported high familiarity with mobile phone usage (M = 4.5, SD = 0.81) but low familiarity with gaze technologies (M = 1.5, SD = 0.5). The frequency of mobile phone use while walking was moderate (M = 2.9, SD = 1.04, on a scale where 1 = lowest frequency and 5 = highest frequency).


\subsection{Apparatus and Environmental Setting}

The experimental setup comprised a client-server architecture. The client device was an Apple iPhone 13 Pro Max (6.7-inch display, 240 grams, 256 GB storage, iOS 16). This was paired with a high-performance server (Intel i9-13900HX processor, NVIDIA RTX 4080 GPU, 32GB RAM) for real-time data processing and storage. The system achieved a gaze inference frequency of 25-30 fps.

The user studies take place in a large lecture room measuring 8 m in width and 20 m in length. A rectangular area of 5 m $\times$ 10 m is transformed into a concave-shaped maze and the walk path enclosed by soft cushions. Chairs and loungers are arranged in a 3 m $\times$ 3 m area and placed in a corner on the same side as the maze. The rest of the room is left empty. The lighting in the room is primarily natural light, supplemented by artificial illumination.


\subsection{Experiment Procedure of User Study 1: Understanding User-Device Interaction Patterns}

We briefed the purpose, apparatus and procedures of the user studies to all the participants before the start of experiments. To ensure a natural interaction and mitigate privacy concerns, participants were instructed to use their personal phones for the tasks prompted by cue phrases. Simultaneously, our experimental phone was affixed to the rear of their device to capture relevant data, including IMU and facial images from the camera. However, we recognise a potential drawback of this approach: the combined weight of the two phones might alter participants' handling and mobility of their devices.

The experiment is designed with 5 types of tasks and all of these are guided to be executed by cue phrase as shown in Table~\ref{tab:p1tasks}. We do not have specific requirements for task execution but provide corresponding suggestions. For instance, we recommend using an Instagram-like app for the social media browsing task, and Microsoft Office or Google Docs for the document editing task. In the task of chatting and dialogues, headphones could be worn to protect privacy.

The procedure of the experiment is shown in Figure~\ref{fig:study-outline}. Participants are firstly guided to the lying condition where they can lie on a big soft sofa in the experiment room. Once the participant is lying down, the system begins to record data. Then the participant will be guided through a series of tasks on their mobile phone using short cue phrases. Each task lasts between 4 and 6 minutes, and each condition comprising these 5 tasks takes approximately 15 to 20 minutes. The duration of the user study on each participant is between 60 and 90 minutes. 

With 40 trials of 10 participants and 4 motion conditions, after data cleaning, we have collected 1,232,633 events for each of accelerometer, gyroscope and magnetometer and 739,578 samples for head movement and angle events extracted from image frames. 

\begin{table}[H]
\centering
\begin{tabular}{c|ccccc}
\hline
\diagbox{Sensor}{Motion} & Lying & Sitting & Walking Slowly & Walking in Maze & Total \\
\hline
IMU & 311,438 & 310,802 & 324,690 & 285,703 & 1,232,633 \\
Camera & 186,862 & 186,481 & 194,814 & 171,421 & 739,578 \\
\hline
\end{tabular}
\caption{Distribution of data samples across motion conditions and sensor types in User Study 1}
\label{tab:exp1-sensor-data}
\end{table}


\subsection{Experiment Procedure of User Study 2: Impact and Factor Analysis}

User Study 2 aims to assess the impact of mobility on 2D gaze estimation accuracy in simulation of a real-world usage scenario. Participants are instructed to use our experimental device directly, which is equipped with custom-developed gaze calibration and test tasks. The protocol requires participants to naturally alter their device holding postures for each task under different motion conditions while performing both calibration and test tasks.

We collected 803,241 time-stamped events across 50 trials involving 10 participants under 5 motion conditions. Table~\ref{tab:exp3-traintest-sensor-data} presents the number of samples from both IMU and camera sensors under each motion condition and experimental phase. More specifically, we collected 137,011 IMU and 82,205 camera events during calibration, and 666,230 IMU and 399,736 camera events during the 9-point gaze tests.

\begin{table}[H]
\centering
\begin{tabular}{cc|cccccc}
\hline
\multicolumn{2}{l|}{\diagbox{Phase}{Samples}{Motion}} & Lying & Sitting & Standing & Walking & Walking in Maze & Total \\
\hline
\multirow{2}{*}{Calibration} & IMU  & 27,471 & 27,230 & 27,949 & 27,652 & 26,709 & 137,011 \\
 & Camera  & 16,482 & 16,338 & 16,769 & 16,591 & 16,025 & 82,205 \\
\multirow{2}{*}{9-Point Test} & IMU  & 130,419 & 133,647 & 135,409 & 133,512 & 133,243 & 666,230 \\
 & Camera  & 78,251 & 80,188 & 81,245 & 80,107 & 79,945 & 399,736 \\
\hline
\end{tabular}
\caption{Distribution of data samples across motion conditions, sensor types, and experimental phases in User Study 2}
\label{tab:exp3-traintest-sensor-data}
\end{table}


\subsection{Data Preprocessing and Feature Extraction}
In the following, we illustrate the pre-processing and feature extraction steps on IMU data.

\subsubsection{Preprocessing}\label{subsec:preprocessing}

During preprocessing, IMU sensor events are synchronised with camera frames according to their timestamps. Special care is taken to synchronise their heterogeneous sampling frequencies. Specifically, we use neighbour interpolation~\cite{lepot2017interpolation} to estimate the IMU sensor values at the exact moments when camera frames are captured. This method allows us to accurately pair each camera frame with the corresponding IMU data, providing a synchronised dataset for further analysis. 

We perform two denoising steps. We apply a Butterworth low-pass filter to smooth the IMU sensor data~\cite{anguita2013public}, and filter out unrealistic head distances based on human ergonomic thresholds. That is, the samples of head distance distribution can be modelled as an approximate Gaussian. Lower one-sided confidence interval with 95\% probability suggests that the head distance threshold is around 100 cm. If the value is larger then 100 cm, it means the participant does not look at the screen.


\subsection{IMU Feature Extraction}\label{subsec:feature}
Feature extraction is crucial in analysing complex, high-dimensional time-series data from IMUs. We adopt a classic feature engineering method from human activity recognition \cite{anguita2013public}, focusing on three categories of features: \textit{Frequency Domain}, \textit{Time Domain}, and \textit{Cross-channel} features.

\textbf{Frequency Domain Features} play a critical role in identifying the periodic characteristics and predominant frequencies of sensor signals. These features are derived through the transformation of time-domain data into the frequency spectrum using  Fast Fourier Transform (FFT). Significant frequency domain features include mean (calculated as $\frac{1}{N} \sum_{i=1}^{N} x_i$, where $x_i$ represents the amplitude of the $i^{th}$ frequency component, and $N$ is the number of frequency components), variance, standard deviation, skewness, kurtosis, direct component (DC), and the top 4 amplitudes along with their corresponding frequencies. Such features are particularly effective in recognising cyclical activities like walking or running.

\textbf{Time Domain Features}, extracted directly from the sensor data over a time window, provide fundamental statistical insights. The features include mean (given by $\frac{1}{N} \sum_{i=1}^{N} x_i$, where $x_i$ is the $i^{th}$ time-domain sample and $N$ is the total number of samples in a window), variance, standard deviation, mode, min/max, range, and zero-crossing rate. These features capture the basic motion attributes including range, intensity, and variability, which are vital for detecting general motion patterns and activity intensity.

\textbf{Cross-channel Features} examine the interrelationships between different sensor axes or channels. Considering that smartphone sensors like IMUs typically offer three-dimensional data (X, Y, Z axes), it is crucial to analyse the correlations and interactions among these channels. Features in this category include cross-correlation coefficients, represented by $CC_{xy}(k) = \frac{\sum_{i} (a_{x,i} - \bar{a}_x)(a_{y,i+k} - \bar{a}_y)}{\sqrt{\sum_{i}(a_{x,i} - \bar{a}_x)^2 \sum_{i}(a_{y,i+k} - \bar{a}_y)^2}}$, where $a_{x,i}$ and  $a_{y, i+k}$ represents x- and y-axis sensor data at $i$th and $i+k$th (the lag) timestamps, and $\bar{a}_x$ and $\bar{a}_y$ represents the mean x- and y-axis sensor data. This cross-correlation coefficients quantify the degree of similarity and the lead-lag relationship between the time-series data of different axes. Cosine similarity $\frac{\vec{a}_i \cdot \vec{a}_j}{\|\vec{a}_i\|\|\vec{a}_j\|}$ assesses the directional alignment between two multi-axial data vectors $\vec{a}_i$ and $\vec{a}_j$ at the $i$th and $j$th timestamps, providing insight into their relative orientation. The synthetic mean is a composite metric encapsulating the overall magnitude of motion by combining the squared sum of multi-axial signals: $a_i = \sqrt{(a_{x,i})^2 + (a_{y,i})^2 + (a_{z,i})^2}$. These features are instrumental in understanding complex movements, such as the coordination in limb movements during running or multi-axial activities like climbing stairs.

