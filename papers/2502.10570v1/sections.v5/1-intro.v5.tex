\section{Introduction}
Nowadays digital interfaces are an extension of our sensory input, and eye tracking on mobile devices emerges as a frontier in enhancing interactive experiences~\cite{lei2023end}. Recent advancements in mobile technologies  have led to an expansion of eye-tracking applications across a diverse set of platforms, including virtual reality (VR) and augmented reality (AR) headsets, specialised eyewear, 
and handheld mobile devices~\cite{blignaut2016idiosyncratic, menges2019improving}.


While eye-tracking technologies have reached a high level of usability in near-eye devices like VR/AR headsets~\cite{clay2019eye, palmero2021openeds2020}, the situation is considerably more complex for handheld mobile devices, where eye-tracking is achieved via inferring a gaze point or direction on the devices' screen mainly from their front camera. This is referred to as \textit{appearance-based gaze estimation}, which presents the potential to enhance the accessibility of eye-tracking technologies, eliminating the need for additional hardware. At the same time, it also brings a number of challenges~\cite{khamis2018understanding, lei2023DynamicRead, bace2020quantification}, as the performance of gaze estimation model, can be significantly affected by user mobility and holding postures, and as well as environmental conditions such as lighting~\cite{valliappan2020accelerating, cheng2024benchmark}. 


With eye-tracking technologies, calibration is an essential step to ensure high precision of eye tracking, which is often performed at the beginning of each use~\cite{duchowski2017eye,tobii2022calib}. Once calibrated, users are required to maintain their position, especially steady head position and distance to the screen in case of calibration distortion~\cite{huang2019saccalib, martschinke2019gaze}. In the context of eye-tracking on handheld mobile devices, calibration does not last over an extended period, as the quality of facial images and the relative spatial relationships between the user's head and the screen are not stable due to environmental conditions, natural head movements, and variations in holding postures and movement. For example, the distance between the face and the screen varies when users are in different motion states; e.g., 30-37 cm when sitting at a table whereas 33-42 cm when walking~\cite{huang2017screenglint}. Previous research has studied these factors, however, there is a lack of a comprehensive understanding of how these factors interplay in real-world mobile usage scenarios.


For gaze estimation, methods can be broadly categorised based on their output: \textit{2D} gaze estimation methods predict 2D coordinate points on the device's screen, while \textit{3D} gaze estimation methods predict 3D gaze vectors or directions in space~\cite{sugano2014learning, zhang18revisiting,cheng2024benchmark}. In recent years, there has been a growing body of work using 2D gaze estimation for eye tracking on handheld devices~\cite{krafka2016eye, arakawa2022rgbdgaze, huynh2021imon, valliappan2020accelerating}. As an end-to-end workflow, 2D methods are easy to deploy; however, they are susceptible to changes in head pose, device orientation and camera-screen coplanarity~\cite{huang2017tabletgaze,krafka2016eye}, and typically require a large amount of device-specific training data to tune the camera-screen plane relationship and learn the relationships between facial appearance and on-screen gaze points~\cite{bace2019accurate}. This reliance on device-specific data limits their applicability in some scenarios and can lead to decreased performance when users change their head pose or device orientation out of boundary of the train data~\cite{balim2023efe, lei2023DynamicRead}. Given their popularity, it is essential to measure the dynamic range of changes during interaction between the user and device, and how these changes affect 2D gaze estimation accuracy in real-world use. This will be a stepping stone towards building more robust mobile eye-tracking system.


Our research is therefore driven by the overarching question:
\emph{How does motion impact the accuracy of eye tracking during natural interactions with mobile devices?}
We aim to provide empirical evidence to analyse motion-related factors in such interactions, quantify their impact on gaze estimation, and identify key contributing variables.
By closely examining the challenges inherent in 2D gaze estimation, we seek to highlight areas for improvement and guide future solutions.
Centred around this aim, our contributions are as follows:
\begin{enumerate}
    \item \textbf{A Novel Data Collection Framework.} 
    We introduce a data collection framework specifically designed for mobile gaze estimation.
    This framework synchronizes motion and vision data (accelerometer, gyroscope, magnetometer, camera frames, and ground-truth gaze), making it possible to precisely capture both device movement and corresponding visual information. This will help reveal new insights into the effect of motion on gaze estimation.

    \item \textbf{Quantitative Behavioural Modelling in Semi-Controlled Environments.}
    We report findings from user studies conducted under semi-controlled conditions, providing key quantitative evidence about how users naturally interact with mobile devices while performing daily tasks under varying motion states.
    In particular, we model typical user behaviours during mobile interactions by fusing motion sensor data and image frames, discovering distinct behavioural patterns for both within-task interaction and transitions between tasks. We observe that
        the head-to-screen distance remains relatively stable (0.48--2.73\,cm) during a single task, while it increases significantly (7.42--10.10\,cm) when switching between tasks.

    \item \textbf{Empirical Evaluation of a Camera-to-Application Eye-Tracking System.}
   We deploy a 2D gaze estimation-based eye-tracking system in real-world application scenarios, uncovering the following challenges faced by the current approaches:
    
    \begin{itemize}
        \item \emph{Quantifying motion effects via Lasso regression:} 
        We are the first to systematically measure the impact of various motion types on 2D gaze estimation methods using Lasso regression.
        Our analysis identifies head-to-screen distance, device orientation, and head movements as the most influential factors with the contributing coefficients of  35.87\%,  32.29\%, and 10.18\% respectively.
        \item \emph{Calibration limitations in mobile usage:} 
        Our real-world experiments demonstrate that a one-time (single) calibration at the start of interaction yields an average error of 3.43\,cm on handheld devices.
        This highlights the importance of more frequent or adaptive calibration strategies for robust, accurate gaze tracking in mobile settings.
    \end{itemize}

\end{enumerate}


