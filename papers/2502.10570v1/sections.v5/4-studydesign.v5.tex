\section{Study Design}\label{sec:studydesign}
To systematically investigate the impact of motion patterns and behaviours during daily interaction between user and mobile device on gaze estimation accuracy, we divided our primary research question - \textit{How does motion impact the precision of gaze estimation during natural interactions with mobile devices?}- into three sub-questions:

\begin{itemize}
    \item[Q1] - \textbf{Motion Pattern Exploration}: What distinct motion patterns emerge when users interact with mobile phones, and is there discernible regularity in these patterns?
    \item[Q2] - \textbf{Motion Impact on Performance}: To what extent do various motion conditions and postural changes affect the accuracy of 2D gaze estimation?
    \item[Q3] - \textbf{Factor identification}: Which specific dynamic variables significantly contribute to performance degradation in 2D gaze estimation?
\end{itemize}

To address these questions, we designed the user studies depicted in Figure~\ref{fig:study-outline}. The User Study 1 focuses on understanding motion patterns and behaviours during typical user interaction tasks, while the User Study 2 links these motions to gaze estimation accuracy, providing a foundation for subsequent analysis.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1\textwidth]{images.v5/F3-userstudy-overview.pdf}
    \caption{Schematic overview of the user study design. User Study 1 explores spatial dynamics of mobile interactions across various daily tasks and motion conditions. User Study 2 evaluates 2D gaze estimation performance under simulated real-world scenarios.}
    \label{fig:study-outline}
\end{figure}

\subsection{User Study 1 -- Interaction Pattern}
The objective of user study 1 is to analyse the spatial relationships and motion behaviours exhibited by users while interacting with their mobile devices during everyday tasks.  This involves observing and recording a variety of motion states such as lying, sitting, standing, walking slowly, and navigating through a maze, to simulate real-world scenarios. These conditions are chosen based on an extensive literature review~\cite{huang2017screenglint,arakawa2022rgbdgaze, zhang2015appearance,huang2017tabletgaze, funes2014eyediap, lei2023DynamicRead, team2024screentime} and are further supplemented by our maze navigation condition to simulate complex user movements encountered in daily life. 

To ensure ecological validity, we selected tasks that mimic daily usage patterns on handheld devices. Specifically, we asked participants to perform a series of routine tasks under each condition including, \emph{document editing} simulates note-taking or drafting emails, \emph{social chatting} represents quick message replies via popular apps like WhatsApp, \emph{social media browsing} (e.g., Instagram, Twitter) captures casual content consumption, \emph{video calling} allows for real-time communication or short call simulations, and \emph{news reading} aligns with web-based article consumption, with cue phrases provided to guide task execution as shown in Table~\ref{tab:p1tasks}. These tasks were chosen for their prevalence in everyday life and for covering a spectrum of interactions (e.g., heavy typing, continuous scrolling, periodic glances). 

To further reduce bias, tasks were \emph{randomized in order} for each participant. Participants received a short 1--2 minute training/familiarization session with the experiment application before starting. This ensured that they understood how to hold and interact with the device under different motion conditions. We also encouraged them to use their natural usage posture (e.g., one-handed vs.\ two-handed) to maximize authenticity of the recorded behavioural data.

\begin{table}[!htbp]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|l} 
\hline
Task & Cue Phrase \\
\hline
Document editing & \begin{tabular}[c]{@{}l@{}}``Create a new document using your preferred document editing \\app and write today's diary entry.''\end{tabular} \\ 
\hline
Social chatting & \begin{tabular}[c]{@{}l@{}}``Open your preferred social media app, such as WhatsApp, and respond\\~to any unread messages.''\end{tabular} \\ 
\hline
Social media browsing & ``Open a social media app like Instagram to browse content or like posts.'' \\ 
\hline
Video calling & \begin{tabular}[c]{@{}l@{}}``Use your preferred video calling app to either simulate a video call or \\engage in a brief call with a friend.''\end{tabular} \\ 
\hline
News reading & ``Open your preferred news app to read some articles.'' \\
\hline
\end{tabular}
}
\caption{Cue Phrases for switching tasks}\label{tab:p1tasks}
\end{table}

Data collection utilises motion sensing module of our application, which captures a rich dataset of motion sensor readings (accelerometer, gyroscope, and magnetometer) and camera-based measurements (head pose and distance), as detailed in Table~\ref{tab:motionsensormoduleFunction}.


\subsection{User Study 2 -- Impact and Factors}\label{subsec:impact_factor}
User Study 2 aims to assess the performance of a 2D gaze estimation method under simulated real-world conditions by correlating the frequency and intensity of motion patterns with model prediction errors. This study extends the first by examining how well the current mobile eye-tracking systems perform under typical everyday tasks (see Table~\ref{tab:p1tasks}). We replicate cue-guided tasks from user study 1 to evoke similar postures and behaviours under controlled motion conditions, including lying, sitting, standing, walking, and walking in maze. Each participant produces 25 pairs of calibration and test data, corresponding to 5 motion conditions and 5 cue-guided tasks. For each cue-guided task of a motion condition, participants were subjected to a calibration and 9-point test procedure. This collected dataset enables a systematic evaluation of how factors such as head-to-screen distance, head movements, and device orientation affect gaze estimation accuracy in dynamic, mobile contexts.

\subsection{Comparison of User Study 1 and User Study 2}
While User Study 1 focuses on observing and analysing users' natural interaction patterns with their mobile devices under various motion conditions and tasks, User Study 2 specifically evaluates how these motion patterns impact the performance of a 2D gaze estimation system. In User Study 1, participants interact with their devices naturally, and we collect motion and spatial data to understand typical behaviours and patterns, without introducing any gaze estimation tasks that might alter their natural interaction. In contrast, User Study 2 involves participants performing similar tasks but includes gaze estimation procedures, such as calibration and testing, to measure the accuracy of gaze prediction under different motion conditions. The inclusion of calibration and testing in User Study 2 means that participants may adjust their interaction patterns slightly due to the demands of the gaze estimation tasks. However, by replicating the tasks and conditions from User Study 1, we aim to maintain consistency in the behaviours elicited.

This dual-study design allows us to first characterize the motion patterns in natural usage (addressing Q1) and then directly assess their impact on gaze estimation performance (addressing Q2 and Q3). By comparing the findings from both studies, we can identify which aspects of user motion and behaviour are most critical for gaze estimation accuracy and develop strategies to mitigate their negative effects.



