\section{Eye Tracking System}
\label{sec:systemdesign}

Our work aims to understand interactive impact factors between the prediction of 2D gaze estimation and body motion. To support this analysis by empirical evidence, we design and develop a data collection framework for mobile gaze estimation that simultaneously collects eye tracking and motion data. This section provides an overview of our framework design, which includes the Eye Tracking Module, Calibration Module, Evaluation Module, and Motion Sensing Module, as illustrated in Figure~\ref{fig:system-comp}.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images.v5/F1-systemdesign.pdf}
    \caption{Overview of the experimental mobile eye tracking system.}
    \label{fig:system-comp}
\end{figure}


\subsection{Eye Tracking Module}\label{subsec:eye_tracking_module}

The Eye Tracking Module is the core component of our framework, providing essential signals for gaze estimation. We adopted a server-client architecture, similar to established handheld mobile device eye tracking systems~\cite{lei2023DynamicRead, kong2021eyemu}. The client-side, developed using Flutter~\cite{flutter2024}, captures data, while the server-side handles model real-time inference using PyTorch and scikit-learn~\cite{pytorch2024, sklearn2024}. Clients and servers communicate with each other via Wi-Fi using Transmission Control Protocol (TCP), ensuring efficient data transfer.

Our module interfaces with the device's front-facing camera, capturing video at 30 frames per second (fps). Each frame undergoes processing to detect facial landmarks using Google's ML Kit~\cite{Google2023mlkit}. This step typically takes 10-25ms per frame. As our focus is to study the key impact factors in 2D appearance-based gaze estimation, especially head-to-screen distances, head movement, and device orientation, we intentionally do not rectify head positions in facial images. This decision differs from the common practice in 3D gaze estimation pipeline where head position rectification is often applied to address head movement variability~\cite{zhang18revisiting}. 

The server is equipped with the PyTorch and scikit-learn frameworks~\cite{pytorch2024, sklearn2024} to decouple gaze estimation models that cannot run at high speeds on mobile devices for long periods of time due to battery limitations. The model is based on the ~\cite{krafka2016eye} architecture, pre-trained on GazeCapture~\cite{krafka2016eye} and fine-tuned on device-specific data from the RGBDGaze dataset~\cite{arakawa2022rgbdgaze}. The model has an average Euclidean error of 2.05 cm on the test set and can be used as a reasonable feature extractor for our 2D gaze estimation task.

\subsection{Calibration Module}\label{subsec:calibration_module}
The Calibration Module consists of a user interface for data collection and a backend for training and storing calibration models. Users fixate their gaze on a moving dot that traverses the screen boundary, allowing the system to collect face images and corresponding gaze predictions. Based on pilot testing and previous research~\cite{kong2021eyemu, lei2023DynamicRead}, we implement a 5-second data collection cycle, balancing accuracy and user comfort, typically gathering between 125 and 150 data pairs.

The collected data is processed through the gaze estimation model, extracting high-dimensional visual feature maps from the first fully connected layer. These feature maps are synchronised with the ground truth gaze points to train a Support Vector Regression (SVR) model to perform inference instead of the remaining fully connected layers. We chose SVR for its effectiveness as a lightweight calibration method~\cite{lei2023DynamicRead, krafka2016eye, valliappan2020accelerating}, supported by comparative studies~\cite{alexiev2019enhancing}.


\subsection{Evaluation Module}\label{subsec:evaluation_module}

The Evaluation Module consists of a user interface for data collection and a back-end for data storage and analysis. The evaluation interface displays dots on the screen in a random order, with potential locations illustrated in Figure~\ref{fig:system-comp}. The interface allows for varying the sequence of dot appearances and the number of dots (ranging from 3 to 9) as well as different locations. In this study, we use a setup of 9 points appearing in a random order to evaluate accuracy across different screen areas.

\subsection{Motion Sensing Module}\label{subsec:motion_sensing_module}

The Motion Sensing Module captures the device's real-time motion and tracks changes in the user's head and body movements. This module utilises the built-in IMU and the front camera. By employing the Flutter framework's \texttt{sensor\_plus} package and Google's ML Kit, the module accesses and records a rich set of sensor data, including raw and processed acceleration, device rotation, ambient magnetic fields, and head-related movements, as detailed in Table~\ref{tab:motionsensormoduleFunction}.

\textbf{Device Movement:} The module records device manipulation by tracking changes in orientation and movement using IMU data. This helps understand how device handling affects gaze estimation accuracy.

\textbf{Head Movement:} The Motion Sensing Module captures head movement using the front camera, including the head's distance from the device and its orientation (pose). Head pose is computed from facial landmarks detected by ML Kit, represented in terms of Euler angles (pitch and yaw).

Since no out-of-the-box API measures head distance, we developed a custom method based on the linear proportional relationship between interpupillary distance and the distance from the eyes to the phone screen. We manually measured the physical interpupillary distance ($IOD_{\text{Physical}}$) and the distance from the interpupillary centre to the camera ($D_{\text{Head-Camera}}$) using a ruler. The interpupillary distance within the image frame ($IOD_{\text{Frame}}$) is then calculated based on facial landmarks detected in the images. These measurements are used to establish a regression model, as shown in Eq.~\ref{eq:headdistance}, to learn the coefficients $\beta_0$, $\beta_1$, and the random error $\epsilon$, allowing us to approximate $D_{\text{Head-Camera}}$. This method is implemented in the motion sensing module to provide real-time estimations of head-to-camera distance, as depicted in Figure~\ref{fig:eye-distance}.

\begin{equation}
    D_{\text{Head-Camera}} = \beta_0 + \beta_1 \times \frac{IOD_{\text{Physical}}}{IOD_{\text{Frame}}} + \epsilon
    \label{eq:headdistance}
\end{equation}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{images.v5/F2-headdistancereg.pdf}
    \caption{Head-to-camera distance calculation.}
    \label{fig:eye-distance}
\end{figure}


\begin{table}[!htbp]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{cccl}
\hline
\textbf{Sensor Events} & \textbf{Frequency} & \textbf{Dimension} & \multicolumn{1}{c}{\textbf{Description}} \\
\hline
UserAccelerometer & 50Hz & X,Y,Z & Acceleration (m/s\textsuperscript{2}) without the effects of gravity. \\
Accelerometer & 50Hz & X,Y,Z & Acceleration (m/s\textsuperscript{2}) with the effects of gravity. \\
Gyroscope & 50Hz & X,Y,Z & Device rotation. \\
Magnetometer & 50Hz & X,Y,Z & Ambient magnetic field surrounding the device. \\
Head distance & 30Hz & distance & Distance between the eye center and the device camera. \\
Head pose & 30Hz & Pitch, Yaw & Head orientation relative to the camera. \\
\hline
\end{tabular}
}
\caption{Description of sensor data collected and estimated by the Motion Sensing Module.}
\label{tab:motionsensormoduleFunction}
\end{table}

