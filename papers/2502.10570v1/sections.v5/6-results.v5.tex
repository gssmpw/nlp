\section{Data Analysis and Results}\label{sec:results}

\subsection{User Study 1 -- Interaction Pattern}
To understand various motion patterns and behaviours during daily interaction between user and device, we perform analysis from raw motion data to user behaviour patterns.

\begin{itemize}
    \item \textit{Motion Data Visualisation} to examine their value range and interacting relationships among various motion conditions; 
    \item \textit{Mobility Patterns} to investigate regularity across motion conditions and behaviours of interaction. 
\end{itemize}


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.80\textwidth]{images.v5/F4-exp1-sensor-box.pdf}
    \caption{Box plots of sensor data under each motion condition: \textit{Ly} for lying, \textit{St}  for sitting, \textit{Ws} for walking slowly and \textit{Wm} for walking in maze. }
    \label{fig:overall-US1-sensor-data}
\end{figure}


\subsubsection{Motion Data Visualisation}\label{subsec:visualisemotiondata}
Figure~\ref{fig:overall-US1-sensor-data} presents the box plots of raw motion data under each motion condition, and Figure~\ref{fig:us1tsneallposture} visualises the distribution of extracted features under each motion condition. We illustrate the analysis on each dimension in the following. 


\noindent\textbf{Head distance:} Head distance states the distance between the eyes and the camera, approximately indicating the distance between the eyes and the screen. The values in Figure~\ref{fig:overall-US1-sensor-data} show the mean and variation of distance in different motion conditions when the participant interacts with the device. Throughout user study 1, the largest variation in distance resides in lying ($M=31.37$cm, $SD=10.09$cm), followed by sitting ($M=34.61$cm, $SD=8.44$cm) and two dynamic conditions: walking slowly ($M=31.07$cm, $SD=7.01$cm), and walking in maze ($M=33.13$cm, $SD=7.57$cm). The greater variability in lying and sitting may be due to the fact that sofas and chairs can support the body and there is more room for movement of the arms and hands; therefore, postural changes are more flexible.


\noindent\textbf{Head Euler Angles (Pitch, Yaw):}  The pitch and yaw angles are key indicators of head orientation. All participants are accustomed to placing the device below head level, so the mean of pitch angles is positive and the mean of yaw angle is close to 0; more specifically, lying ($M_{pitch} = 1.23$, $SD_{pitch} = 9.61$, $M_{yaw} = -1.24$, $SD_{yaw} = 10.96$), sitting ($M_{pitch} = 2.15$, $SD_{pitch} = 8.09$, $M_{yaw} = -0.73$, $SD_{yaw} = 7.49$), walking slowly ($M_{pitch} = 1.25$, $SD_{pitch} = 7.68$, $M_{yaw} = -0.57$, $SD_{yaw} = 5.20$), and walking in maze ($M_{pitch} = 2.19$, $SD_{pitch} = 10.72$, $M_{yaw} = 0.35$, $SD_{yaw} = 7.34$). Similar to the head distance, the variation of the Euler Angle of the head under static conditions is larger than that under dynamic conditions. However, the changes of posture during walking in the maze are similar to those of sitting. That is, when users are finding and avoiding obstacles in the maze, they tend to move their phones out of their viewing range. 


\noindent\textbf{User Accelerometer (X, Y, Z):}  As users are required to interact with the device by various tasks, they are less likely to perform high-intensity activities; therefore, their accelerometer readings remain stable: lying ($M_{X} = 0.00$, $SD_{X} =0.49 $, $M_{Y} = -0.05$, $SD_{Y} = 0.38$, $M_{Z} = 0.01$, $SD_{Z} = 0.56$), sitting ($M_{X} = 0.01$, $SD_{X} = 0.32$, $M_{Y} = -0.03$, $SD_{Y} = 0.26$, $M_{Z} = -0.01$, $SD_{Z} = 0.40$), walking slowly ($M_{X} = 0.02$, $SD_{X} = 0.59$, $M_{Y} = -0.10$, $SD_{Y} = 0.55$, $M_{Z} = 0.04$, $SD_{Z} = 0.61$), and walking in maze ($M_{X} = 0.01$, $SD_{X} = 0.72$, $M_{Y} = -0.14$, $SD_{Y} = 0.67$, $M_{Z} = 0.07$, $SD_{Z} = 0.77$). The variation in user accelerometer caused by changing posture or task between sitting and lying is smaller than that under dynamic conditions. When users are sitting and lying down, the change in posture is caused by the deflection of the head and arms, which is less than the acceleration generated by the moving actions.


\noindent\textbf{Gyroscope (X, Y, Z):} The gyroscope can obtain data on how fast the orientation of the device changes, and the greater the absolute value of the reading, the faster the angle changes. The mean value of the gyroscope is close to 0 in the three axes for all the motion conditions. The variation is much greater and the change caused by the two walking conditions is greater than that of sitting and lying down; that is, lying ($M_{X} = 0.00$, $SD_{X} = 0.25$, $M_{Y} = 0.00$, $SD_{Y} = 0.26$, $M_{Z} = 0.00$, $SD_{Z} = 0.21$), sitting ($M_{X} = 0.00$, $SD_{X} = 0.18$, $M_{Y} = 0.00$, $SD_{Y} = 0.18$, $M_{Z} = 0.00$, $SD_{Z} = 0.17$), walking slowly ($M_{X} = 0.00$, $SD_{X} = 0.23$, $M_{Y} = -0.01$, $SD_{Y} = 0.33$, $M_{Z} = -0.01$, $SD_{Z} = 0.39$), and walking in maze ($M_{X} = 0.00$, $SD_{X} = 0.28$, $M_{Y} = 0.01$, $SD_{Y} = 0.40$, $M_{Z} = 0.02$, $SD_{Z} = 0.49$).

\noindent\textbf{Orientation (Pitch, Yaw, Roll):}  The pitch, yaw and roll angles are key indicators which are similar with the head pose to show device orientation. The variation of pitch is similar to that of yaw, and lying has the largest variation, followed by the two walking conditions; more specifically, lying ($M_{Pitch} = 0.74$, $SD_{Pitch} = 0.39$, $M_{Yaw} = 0.02$, $SD_{Yaw} = 1.97$), sitting ($M_{Pitch} = 0.73$, $SD_{Pitch} = 0.29$, $M_{Yaw} = -0.28$, $SD_{Yaw} = 1.47$), walking slowly ($M_{Pitch} = 0.58$, $SD_{Pitch} = 0.30$, $M_{Yaw} = -0.29$, $SD_{Yaw} = 1.80$), and walking in maze ($M_{Pitch} = 0.59$, $SD_{Pitch} = 0.29$, $M_{Yaw} = 0.03$, $SD_{Yaw} = 1.87$). Lying has resulted in a big difference in the mean of Roll and a larger variation than the other conditions; that is, lying ($M_{Roll} =  -0.15$, $SD_{Roll} = 1.68$), sitting ($M_{Roll} = 0.04$, $SD_{Roll} = 0.51$), walking slowly ($M_{Roll} = 0.05$, $SD_{Roll} = 0.28$), and walking in maze ($M_{Roll} = 0.03$, $SD_{Roll} = 0.28$). In addition, lying exhibits the greatest variation in head distance and orientation. The two walking conditions exhibit a greater variation than static conditions on accelerometer and gyroscope readings.


\noindent\textbf{Relationships of Motion Conditions} We present t-SNE (t-Distributed Stochastic Neighbour Embedding) plots of extracted sensor features from all the participants under these motion conditions in Figure~\ref{fig:us1tsneallposture}. The t-SNE is usually applied for data visualisation that reduces the dimensionality of high-dimensional data while preserving the relationships and structures within the data as much as possible. Overall, the t-SNE plots show a distinct separation between lying (purple) and sitting (green) conditions, while walking slowly (blue) and walking in maze (yellow) are overlapping due to their similar motion signatures. Specifically, the features under lying are most spread, followed by sitting, walking slowly and walking in maze. The spreadness suggests the variety of postures and movement. This finding is consistent with the above raw sensor data analysis. 


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.85\textwidth]{images.v5/F5-exp1-tsne-allposture.pdf}
\caption{t-SNE plots of sensor features under each motion condition}
\label{fig:us1tsneallposture}
\end{figure}


\subsubsection{Mobility Patterns}
Our next task is to investigate into regularity of motion patterns; i.e., how many different patterns each participant exhibited under different motion conditions. Towards this end, we run GMM (Gaussian Mixture Models) on each participant's data and present the box plots of the number of clusters under each motion conditions in Figure~\ref{fig:gmmhistogram}. Most of the participants exhibit $\sim$4 clusters for lying and sitting and $\sim$2 clusters for walking conditions. This suggests that there is high regularity in users' motion patterns. The motion patterns under dynamic conditions exhibit greater stability, implying that users actively stabilise themselves while attempting to read on the screen. 

\begin{figure}[!htbp]
     \centering
     % \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=0.6\textwidth]{images.v5/F6-histogram.pdf}
    \caption{Comparison of the number of motion clusters under different motion conditions}
    \label{fig:gmmhistogram}
\end{figure}

Figure~\ref{fig:exp1gmmtimep1static} and Figure~\ref{fig:exp1gmmtimep1dynamic} maps GMM clusters of Participant 1 on their time-series sensor data. The GMM effectively clusters data in static conditions, identifying distinct motion patterns during tasks and transitions indicated by cue phrases. On the other hand, it is difficult for GMM to recognise the transitions of tasks under dynamic conditions. All the movements are clustered to a single group. In the following, we will investigate the in-task and switch-task scenarios further. 

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images.v5/F7-exp1-GMM-time-static.pdf}
    \caption{GMM clustering mapped to time-series sensor data on Participant 1 under static conditions}
    \label{fig:exp1gmmtimep1static}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images.v5/F8-exp1-GMM-time-dynamic.pdf}
    \caption{GMM clustering mapped to time-series sensor data on Participant 1 under dynamic conditions}
    \label{fig:exp1gmmtimep1dynamic}
\end{figure}


\noindent\textbf{In vs. Switch Task} Using task execution and task switching as clues, we delve deeper into the data in section~\ref{subsec:visualisemotiondata}. Often users interact with phones in a dynamic and context-dependent manner. We aim to understand how users' gaze and interaction with their devices change as they shift from active engagement in a task (in-task) to transitioning between tasks (switch-task). 


\begin{table}[!htbp]
\centering
\begin{tabular}{l|cccccccc}
\hline
\multirow{3}{*}{\diagbox{Attribute}{Setting}} & \multicolumn{4}{c}{Lying} & \multicolumn{4}{c}{Sitting} \\
 & \multicolumn{2}{c}{In Task} & \multicolumn{2}{c}{Switch Task} & \multicolumn{2}{c}{In Task} & \multicolumn{2}{c}{Switch Task} \\
 \cline{2-9}
 & Mean & Std & Mean & Std & Mean & Std & Mean & Std \\
  \hline
headDistance & 23.65 & {\cellcolor[rgb]{0.608,0.808,0.494}}2.23 & 31.54 & {\cellcolor[rgb]{0.973,0.412,0.42}}10.10 & 41.25 & {\cellcolor[rgb]{0.388,0.745,0.482}}0.48 & 34.60 & {\cellcolor[rgb]{0.984,0.596,0.455}}8.40 \\
headEulerAnglePitch & 3.96 & {\cellcolor[rgb]{0.388,0.745,0.482}}1.55 & 1.22 & {\cellcolor[rgb]{0.98,0.537,0.447}}9.07 & 4.57 & {\cellcolor[rgb]{0.533,0.784,0.49}}2.31 & 2.19 & {\cellcolor[rgb]{0.988,0.667,0.471}}7.62 \\
headEulerAngleYaw & 0.60 & {\cellcolor[rgb]{0.435,0.757,0.482}}1.80 & -1.35 & {\cellcolor[rgb]{0.973,0.412,0.42}}10.48 & -3.42 & {\cellcolor[rgb]{0.576,0.796,0.49}}2.53 & -0.79 & {\cellcolor[rgb]{0.992,0.729,0.482}}6.90 \\
useraccelerometerX & 0.00 & {\cellcolor[rgb]{0.463,0.765,0.486}}0.08 & 0.01 & {\cellcolor[rgb]{1,0.906,0.518}}0.49 & 0.00 & {\cellcolor[rgb]{0.686,0.827,0.498}}0.19 & 0.01 & {\cellcolor[rgb]{0.949,0.906,0.514}}0.31 \\
useraccelerometerY & 0.00 & {\cellcolor[rgb]{0.388,0.745,0.482}}0.05 & -0.05 & {\cellcolor[rgb]{1,0.918,0.518}}0.38 & 0.05 & {\cellcolor[rgb]{0.463,0.765,0.486}}0.08 & -0.03 & {\cellcolor[rgb]{0.824,0.871,0.506}}0.26 \\
useraccelerometerZ & -0.01 & {\cellcolor[rgb]{0.525,0.784,0.49}}0.11 & 0.01 & {\cellcolor[rgb]{1,0.902,0.514}}0.56 & -0.05 & {\cellcolor[rgb]{0.792,0.859,0.502}}0.24 & -0.01 & {\cellcolor[rgb]{1,0.918,0.518}}0.40 \\
gyroscopeX & 0.00 & {\cellcolor[rgb]{0.388,0.745,0.482}}0.04 & 0.00 & {\cellcolor[rgb]{0.973,0.412,0.42}}0.25 & 0.10 & {\cellcolor[rgb]{0.718,0.839,0.498}}0.09 & 0.00 & {\cellcolor[rgb]{0.992,0.71,0.478}}0.19 \\
gyroscopeY & 0.00 & {\cellcolor[rgb]{0.388,0.745,0.482}}0.04 & 0.00 & {\cellcolor[rgb]{0.973,0.412,0.42}}0.26 & 0.00 & {\cellcolor[rgb]{1,0.894,0.514}}0.19 & 0.00 & {\cellcolor[rgb]{0.98,0.914,0.514}}0.18 \\
gyroscopeZ & 0.00 & {\cellcolor[rgb]{0.388,0.745,0.482}}0.03 & 0.00 & {\cellcolor[rgb]{0.973,0.412,0.42}}0.22 & -0.10 & {\cellcolor[rgb]{0.773,0.855,0.502}}0.09 & 0.00 & {\cellcolor[rgb]{0.992,0.718,0.478}}0.16 \\
orientationRoll & -0.24 & {\cellcolor[rgb]{0.388,0.745,0.482}}0.11 & -0.15 & {\cellcolor[rgb]{0.973,0.412,0.42}}1.70 & 0.00 & {\cellcolor[rgb]{0.404,0.749,0.482}}0.13 & 0.04 & {\cellcolor[rgb]{0.98,0.525,0.443}}1.51 \\
orientationPitch & 0.29 & {\cellcolor[rgb]{0.388,0.745,0.482}}0.04 & 0.01 & {\cellcolor[rgb]{0.973,0.412,0.42}}1.99 & 1.06 & {\cellcolor[rgb]{1,0.898,0.514}}1.21 & -0.28 & {\cellcolor[rgb]{0.992,0.737,0.482}}1.47 \\
orientationYaw & 0.32 & {\cellcolor[rgb]{0.388,0.745,0.482}}0.04 & 0.75 & {\cellcolor[rgb]{0.976,0.914,0.514}}0.39 & 0.53 & {\cellcolor[rgb]{1,0.918,0.518}}0.41 & 0.74 & {\cellcolor[rgb]{0.973,0.412,0.42}}1.59 \\
HoldAngle & 23.29 & {\cellcolor[rgb]{0.388,0.745,0.482}}4.73 & 84.75 & {\cellcolor[rgb]{0.973,0.412,0.42}}36.85 & 30.89 & {\cellcolor[rgb]{0.471,0.769,0.486}}5.69 & 44.03 & {\cellcolor[rgb]{0.996,0.804,0.498}}17.50 \\
Velocity & 0.03 & {\cellcolor[rgb]{0.388,0.745,0.482}}0.02 & 0.10 & {\cellcolor[rgb]{0.973,0.412,0.42}}0.21 & 0.08 & {\cellcolor[rgb]{1,0.894,0.514}}0.14 & 0.09 & {\cellcolor[rgb]{0.976,0.914,0.514}}0.13 \\
\hline
\end{tabular}
\caption{Comparison of mean and std of motion data for in task and switching task under static conditions}\label{tab:exp1-inswitch-task-static}
\end{table}


Tables~\ref{tab:exp1-inswitch-task-static} and~\ref{tab:exp1-inswitch-task-dynamic} present a detailed comparison of mean and standard deviation of raw motion data in in- and switch-tasks under static and dynamic conditions. The head distance and orientation sensors such as gyroscope and accelerometer exhibit small variation in the in-task state, suggesting users maintain a relatively stable distance from their device while engaging in a task, regardless of their motion state. 


\begin{table}[!htbp]
\centering
\begin{tabular}{l|cccccccc}
\hline
\multirow{3}{*}{\diagbox{Attribute}{Setting}} & \multicolumn{4}{c}{Walking Slowly} & \multicolumn{4}{c}{Walking in Maze} \\
 & \multicolumn{2}{c}{In Task} & \multicolumn{2}{c}{Switch Task} & \multicolumn{2}{c}{In Task} & \multicolumn{2}{c}{Switch Task} \\
 \cline{2-9}
 & Mean & Std & Mean & Std & Mean & Std & Mean & Std \\
  \hline
headDistance & 27.94 & {\cellcolor[rgb]{0.388,0.745,0.482}}2.58 & 31.67 & {\cellcolor[rgb]{0.976,0.439,0.427}}7.42 & 31.37 & {\cellcolor[rgb]{0.424,0.753,0.482}}2.73 & 33.12 & {\cellcolor[rgb]{0.973,0.412,0.42}}7.54 \\
headEulerAnglePitch & 0.23 & {\cellcolor[rgb]{0.467,0.765,0.486}}2.86 & 1.45 & {\cellcolor[rgb]{0.984,0.588,0.455}}8.28 & 2.24 & {\cellcolor[rgb]{0.725,0.839,0.498}}3.78 & 2.22 & {\cellcolor[rgb]{0.973,0.412,0.42}}10.14 \\
headEulerAngleYaw & -1.84 & {\cellcolor[rgb]{0.455,0.765,0.486}}2.82 & -0.33 & {\cellcolor[rgb]{0.996,0.851,0.506}}5.51 & 0.97 & {\cellcolor[rgb]{0.78,0.855,0.502}}3.96 & 0.30 & {\cellcolor[rgb]{0.992,0.773,0.49}}6.34 \\
useraccelerometerX & 0.02 & {\cellcolor[rgb]{0.388,0.745,0.482}}0.51 & 0.02 & {\cellcolor[rgb]{0.992,0.918,0.514}}0.60 & -0.02 & {\cellcolor[rgb]{1,0.918,0.518}}0.60 & 0.01 & {\cellcolor[rgb]{0.973,0.412,0.42}}0.72 \\
useraccelerometerY & -0.09 & {\cellcolor[rgb]{0.388,0.745,0.482}}0.52 & -0.11 & {\cellcolor[rgb]{0.914,0.894,0.51}}0.55 & 0.01 & {\cellcolor[rgb]{1,0.898,0.514}}0.57 & -0.14 & {\cellcolor[rgb]{0.973,0.412,0.42}}0.67 \\
useraccelerometerZ & 0.03 & {\cellcolor[rgb]{0.647,0.82,0.494}}0.52 & 0.05 & {\cellcolor[rgb]{0.992,0.769,0.49}}0.62 & -0.04 & {\cellcolor[rgb]{0.388,0.745,0.482}}0.48 & 0.08 & {\cellcolor[rgb]{0.973,0.412,0.42}}0.74 \\
gyroscopeX & 0.00 & {\cellcolor[rgb]{0.388,0.745,0.482}}0.20 & 0.00 & {\cellcolor[rgb]{0.922,0.898,0.51}}0.25 & 0.01 & {\cellcolor[rgb]{0.996,0.8,0.494}}0.26 & 0.00 & {\cellcolor[rgb]{0.973,0.412,0.42}}0.28 \\
gyroscopeY & -0.03 & {\cellcolor[rgb]{0.588,0.8,0.49}}0.34 & -0.02 & {\cellcolor[rgb]{0.388,0.745,0.482}}0.33 & 0.01 & {\cellcolor[rgb]{0.984,0.62,0.463}}0.38 & 0.01 & {\cellcolor[rgb]{0.973,0.412,0.42}}0.40 \\
gyroscopeZ & -0.04 & {\cellcolor[rgb]{0.776,0.855,0.502}}0.42 & -0.01 & {\cellcolor[rgb]{0.388,0.745,0.482}}0.39 & -0.10 & {\cellcolor[rgb]{0.988,0.659,0.467}}0.45 & 0.02 & {\cellcolor[rgb]{0.973,0.412,0.42}}0.47 \\
orientationRoll & -0.02 & {\cellcolor[rgb]{0.388,0.745,0.482}}0.07 & 0.06 & {\cellcolor[rgb]{0.996,0.835,0.502}}0.30 & -0.02 & {\cellcolor[rgb]{0.878,0.886,0.51}}0.22 & 0.03 & {\cellcolor[rgb]{0.973,0.412,0.42}}0.48 \\
orientationPitch & -0.36 & {\cellcolor[rgb]{0.388,0.745,0.482}}1.79 & -0.27 & {\cellcolor[rgb]{0.514,0.78,0.486}}1.80 & -0.70 & {\cellcolor[rgb]{1,0.91,0.518}}1.91 & 0.03 & {\cellcolor[rgb]{0.973,0.412,0.42}}3.88 \\
orientationYaw & 0.46 & {\cellcolor[rgb]{0.388,0.745,0.482}}0.18 & 0.61 & {\cellcolor[rgb]{0.737,0.843,0.502}}0.31 & 0.68 & {\cellcolor[rgb]{0.996,0.835,0.502}}0.51 & 0.60 & {\cellcolor[rgb]{0.973,0.412,0.42}}0.99 \\
HoldAngle & 26.37 & {\cellcolor[rgb]{0.494,0.773,0.486}}10.34 & 35.22 & {\cellcolor[rgb]{0.973,0.412,0.42}}18.43 & 39.04 & {\cellcolor[rgb]{0.388,0.745,0.482}}9.62 & 34.62 & {\cellcolor[rgb]{0.984,0.561,0.451}}17.07 \\
Velocity & 0.22 & {\cellcolor[rgb]{0.388,0.745,0.482}}0.13 & 0.23 & {\cellcolor[rgb]{0.984,0.576,0.455}}0.17 & 0.08 & {\cellcolor[rgb]{0.788,0.859,0.502}}0.15 & 0.29 & {\cellcolor[rgb]{0.973,0.412,0.42}}0.18 \\
\hline
\end{tabular}
\caption{Comparison of mean and std of motion data for in task and switching task under dynamic conditions}\label{tab:exp1-inswitch-task-dynamic}
\end{table}


Intuitively, switching-task reveals significant variability, with increased standard deviations across all motion data; e.g., lying: \(31.54 \pm 10.10\)cm, sitting: \(34.60 \pm 8.40\)cm, walking slowly: \(31.67 \pm 7.42\)cm, and walking in a maze: \(33.12 \pm 7.54\)cm. This indicates greater variability and potentially more head movement or changes in posture. This might reflect the users' momentary distraction or reorientation as they switch tasks, which is more pronounced when they are stationary (lying or sitting).


By further examining Table~\ref{tab:exp1-inswitch-task-static} and ~\ref{tab:exp1-inswitch-task-dynamic}, we can conduct empirical analysis of user postures under different motion conditions. For example, the distance from the head to the screen under lying is the shortest; i.e., 23.65 cm, and the head is lifted upward by 3.96 degrees and deflected to the right by 0.6 degrees with respect to the device screen (Pitch = 3.96, Yaw = 0.60). When users are lying down, their phone maintains a head distance of 23.65 cm and is tilting towards the left side of the head (Pitch = 3.96, Yaw = 0.60) with holding angle of 23.29 degrees. 


\subsection{User Study 2 -- Impact and Factors}
Building upon the insights from User Study 1, this section investigates the impact of motion patterns and behaviours under various motion conditions on the performance of 2D gaze estimation and identifies the key factors contributing to the impact. As outlined in Section~\ref{subsec:impact_factor}, User Study 2 data comprises 5 calibration data and five 9-point testing data for each motion condition, including ground truth points, facial images, and IMU sensor readings, as shown in Table~\ref{tab:exp3-traintest-sensor-data}. 

In the following, we analyse the impact of motion patterns and behaviours on 2D gaze estimation:
\begin{enumerate}
    \item \textit{Motion Data Visualisation} to examine the distribution of each motion variables and relationships between calibrations and tests; 
    \item \textit{Performance Degradation} to assess to what degree mobility decrease the performance of gaze estimation model and calibrator; 
    \item \textit{Impact Factor Identification} to discover what mobility features contribute most to the degradation. 
\end{enumerate}


\subsubsection{Motion Data Visualisation}

To maintain consistency with User Study 1, we visualise the motion data from User Study 2 using box plots in Figure~\ref{fig:overall-US3-sensor-data}. Additionally, we compare the distributions of motion data in calibration and test procedures in Figure~\ref{fig:overall-US3-train-test-motion}, which informs our discussion on calibration performance. We analyse each dimension as follows:

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.80\textwidth]{images.v5/F9-exp3-sensor-box.pdf}
    \caption{Box plots of motion data under each motion condition: \textit{Ly} for lying, \textit{St}  for sitting, \textit{Sd}  for standing, \textit{Wk} for walking and \textit{Wm} for walking in maze. }
    \label{fig:overall-US3-sensor-data}
\end{figure}


\noindent\textbf{Head distance:} The trends in head distance are largely consistent with User Study 1, with the largest distances observed in sitting and the shortest in lying. The difference from User Study 1 is that the variation of two  dynamic conditions becomes larger, reaching a range similar to that of the sitting position; that is, sitting ($M=37.62$cm, $SD=9.47$cm), lying ($M=31.03$cm, $SD=7.43$cm), standing ($M=35.22$cm, $SD=7.13$cm), walking ($M=34.51$cm, $SD=8.77$cm), and walking in maze ($M=35.52$cm, $SD=9.72$cm). The difference in the mean values of two user studies is also small, changing from $M=31.37$cm to $M=31.03$cm in lying, $M = \pm3.01$cm in sitting, $M = \pm4.15$cm in standing, $M = \pm3.44$cm in walking, and $M = \pm2.39$cm in  walking in maze.


\noindent\textbf{Head Euler Angles (Pitch, Yaw):}  The pitch and yaw readings in User Study 2 align closely with those from User Study 1. Mean pitch angles across all conditions remained positive (range: 0.94° to 1.25°), with yaw angles close to 0°: lying ($M_{pitch} = 1.21$, $SD_{pitch} = 4.75$, $M_{yaw} = -0.86$, $SD_{yaw} = 6.84$), sitting ($M_{pitch} = 0.95$, $SD_{pitch} = 4.16$, $M_{yaw} = -0.44$, $SD_{yaw} = 2.89$), standing ($M_{pitch} = 1.24$, $SD_{pitch} = 5.34$, $M_{yaw} = -0.38$, $SD_{yaw} = 3.00$), walking ($M_{pitch} = 1.11$, $SD_{pitch} = 5.67$, $M_{yaw} = -0.01$, $SD_{yaw} = 3.93$), and walking in maze ($M_{pitch} = 1.25$, $SD_{pitch} = 0.01$, $M_{yaw} = 0.35$, $SD_{yaw} = 4.69$). But the variation of angle changes between motion conditions become smaller; for example, the range of $SD_{pitch}$ is between 4.16 and 5.78, while that in user study 1 is between 7.68 and 10.72.  The range of $SD_{yaw}$ is between 2.89 and 6.84 while that in user study 1 is between 5.20 and 10.96. The difference in variation may be due to the difference in the weights (two phones in user study 1 vs. 1 phone in user study 2) and the length of the experiment; i.e. the duration of user study 2 was longer than user study 1, shown in Table~\ref{tab:exp1-sensor-data} and Table~\ref{tab:exp3-traintest-sensor-data}. In addition, user study 2 requires a calibration process and testing, where participants try to keep their device holding posture stable to execute the calibration and 9-point testing. This results in a smaller variation in the IMU readings. 


\noindent\textbf{User accelerometer and gyroscope:} Mean accelerometer and gyroscope readings remained consistent with User Study 1, with values close to 0. Specifically, the changes of $SD$ in user accelerometer and gyroscope, across the three axes and the all motion conditions of two user study experiments, are less than 0.45 and 0.34, respectively. 


\noindent\textbf{Orientation (Pitch, Yaw, Roll):} User Study 2 exhibited larger means and variances in orientation compared to User Study 1, that is, lying ($M_{Pitch} = 0.87$, $SD_{Pitch} = 0.40$, $M_{Yaw} = -0.72$, $SD_{Yaw} = 1.93$, $M_{Roll} = -0.16$, $SD_{Roll} = 2.04$),  sitting ($M_{Pitch} = 0.94$, $SD_{Pitch} = 0.33$, $M_{Yaw} = -0.39$, $SD_{Yaw} = 1.18$), $M_{Roll} = -0.18$, $SD_{Roll} = 0.87$), standing ($M_{Pitch} = 0.85$, $SD_{Pitch} = 0.37$, $M_{Yaw} = 0.13$, $SD_{Yaw} = 2.01$), $M_{Roll} = -0.01$, $SD_{Roll} = 0.45$), walking ($M_{Pitch} = 0.80$, $SD_{Pitch} = 0.39$, $M_{Yaw} = -0.05$, $SD_{Yaw} = 1.78$), $M_{Roll} = 0.09$, $SD_{Roll} = 0.78$), and walking in maze ($M_{Pitch} = 0.77$, $SD_{Pitch} = 0.08$, $M_{Yaw} = 0.03$, $SD_{Yaw} = 1.78$), $M_{Roll} = 0.08$, $SD_{Roll} = 0.66$). This may be attributed to participants more consciously altering their postures in response to cues, particularly evident in the continuous and stable body rolls observed during the lying condition.

To further examine the relationships between motion conditions in User Studies 1 and 2, we present t-SNE plots in Figure~\ref{fig:user-study-1-2-comparison}. The left Figure~\ref{fig:exp13-tsne-all} is an comparison overview of motion data in two user studies. The right Figure~\ref{fig:exp13-tsne-posture} shows the comparison under each motion condition, where two walking conditions in user study 2 are combined and compared to user study 1. These visualisations demonstrate a high degree of similarity in motion features between the two studies, validating the consistency of our experimental design and data collection methods.


\begin{figure}[!htbp]
    \begin{subfigure}{0.45\textwidth}
        \centering
    \includegraphics[width=\textwidth, height = 140pt]{images.v5/F10A-exp13-tsne-all.png}
    \caption{Overall comparison}
    \label{fig:exp13-tsne-all}
    \end{subfigure}
    \begin{subfigure}{0.52\textwidth}
        \centering
         \includegraphics[width=\textwidth, height = 140pt]{images.v5/F10B-exp13-tsne-posture.pdf}
    \caption{Comparison on each of motion conditions}
    \label{fig:exp13-tsne-posture}
    \end{subfigure}
    \caption{Cross-comparison t-SNE plots for User Study 1 and 2}
    \label{fig:user-study-1-2-comparison}
\end{figure}


\subsubsection{Impact of Mobility on 2D Gaze Estimation}
To assess the impact of mobility on 2D gaze estimation pipeline (base model performance and after calibrated performance) accuracy. We designed a series of personal-specific calibration approaches with varying granularities, from tasks to motion conditions. Figure~\ref{fig:train-test-calibrators} presents the overview of person-specific training and test procedure in collected data. 


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{images.v5/F11-train-test-calibrators.pdf}
    \caption{Person-specific training and test regime of calibrations in different granularities for each motion condition}
    \label{fig:train-test-calibrators}
\end{figure}

First we conduct an in-depth analysis by comparing the gaze estimation performance on two settings: (1) \textit{base model performance}; that is, directly inputting the facial landmarks and facial images to the iTracker model in Section~\ref{subsec:eye_tracking_module} and obtaining the gaze points; (2) \textit{one-off calibration approach}; that is, training one calibrator using any of the calibration data following the procedure in Section~\ref{subsec:calibration_module} and then using this calibrator for gaze point prediction. This references the default procedure of commercial eye-trackers~\cite{Tobii2023eyetrackerusage, Gazepoint2022tutorial}, where a calibration is performed once before using the eye-tracker, and then the same calibrator is applied until the entire experiment is completed.


The 2D gaze estimation performance measured by calculating the Euclidean distance error in centimetres (cm), i.e. Root Mean Squared Error (RMSE), to quantify the discrepancy between the predicted gaze points and the ground truth: $\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (gt_i - \hat{p}_i)^2}$, where $gt_i$ and $\hat{p}_i$ are the $i$th ground truth and estimated points and $n$ is the total number of samples. Table~\ref{tab:exp3overallsystem} compares RMSE for base model and one-off calibrator, averaged on all the participants. For the one-off calibrator on each motion condition, we use each of 5 calibration data to train a calibrator and test on all the 5 test data, and then average the 5 test results. 


We conducted a repeated-measures ANOVA to examine the effect of motion condition (\textit{lying, sitting, standing, walking, walking in maze}) on the gaze estimation error. 
Mauchly’s test of sphericity was satisfied ($\chi^2(9) = 6.10, p = 0.19$). 
The ANOVA revealed a \emph{significant main effect} of motion condition on error ($F(4,36) = 7.42, p < .001, \eta_p^2 = .34$). 
Post-hoc pairwise comparisons using Holm-Bonferroni correction confirmed that errors under dynamic conditions 
(\textit{walking, walking in maze}) were significantly higher than in static conditions 
(\textit{lying, sitting}) (all $p<.05$). 
Additionally, to quantify practical significance, we computed Cohen’s \emph{d} for each pairwise comparison, 
with values ranging from 0.60 to 0.95, indicating moderate to large effect sizes for the impact of motion on accuracy.


\begin{table}[!htbp]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|cccccc} 
\hline
\diagbox{System}{RMSE (cm)}{Scenario} & Average & Lying & Sitting & Standing & Walking & Walking in Maze  \\ 
\hline
Base Model & 4.49 (2.76) & 4.13 (2.45) & 4.41 (2.93) & 4.48 (2.77) & 4.54 (2.64) & 4.88 (2.88)\\
One-off Calibration & 3.44 (2.19) & 3.53 (2.18) & 3.07 (1.97) & 3.24 (2.05) & 3.56 (2.29) & 3.79 (2.44)\\
\hline
\end{tabular}
}
\caption{Comparison of RMSE (mean and std in brackets) for base model and one-off calibration} \label{tab:exp3overallsystem}
\end{table}


\noindent\textbf{Base model performance}. The base model of our Eye Tracking Module serves as a benchmark for uncalibrated performance. It operates on the pre-train parameters from the third-party dataset and fine-tuned on experiment device specific data of RGBDGaze dataset~\cite{arakawa2022rgbdgaze}. The results in Table~\ref{tab:exp3overallsystem} show that the base model exhibits significant errors. The highest error is 4.88 cm on walking in a maze condition, and is a considerable deviation from the desired performance level. Therefore, it indicates that the base model, without additional calibration, is impractical for an eye-tracking system, particularly under dynamic conditions.


\noindent\textbf{One-off calibration approach performance}. 
 
The one-off calibration approach offers some improvements, but the error remains sub-optimal. Similar to the base model, the one-off calibrator achieves the best performance on the sitting condition (i.e., 3.07 cm), and the worst performance on the walking in maze condition (i.e., 3.79 cm). The results also indicate higher transferability between postures under the sitting condition; that is, i.e., the range of head movements was similar in these conditions. However, this is not the case on the dynamic conditions. 


\noindent\textbf{Task-specific calibration approach performance}. 
To address the limitations of the one-off calibration approach, we investigated a task-specific calibration approach. This method, illustrated in Figure~\ref{fig:train-test-calibrators}, allows the model to learn task-relevant body and head behaviours, enabling us to further examine the impact of behavioural changes on model accuracy.

We designed experiments to evaluate model on both \textit{within-task} and \textit{between-task} scenarios' performance. For each motion condition, we randomly selected one out of five calibration data to train a calibrator. We then tested this calibrator on its corresponding test data (reported as \textit{within-task} errors) and on the other four test data (reported as \textit{between-task} errors). This process was iterated for each calibration data, and the error resulting in cm were averaged. Figure~\ref{fig:within-between} presents task-specific calibration approach performance on within-task and between-task scenarios.


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.75\textwidth]{images.v5/F12-within-task-cm.pdf}
    \caption{Comparison of RMSE for within-task and between-task scenarios}
    \label{fig:within-between}
\end{figure}

Compared to the one-off calibration approach, task-specific calibration approach on within-task scenario reduce the errors from 3.07 cm to 2.57 cm on sitting, 3.53 cm to 2.96 cm on lying, 3.24 cm to 2.81 cm on standing, 3.56 cm to 3.10 cm on walking slowly, and 3.79 cm to 3.47 cm on walking in maze. On average, the improvement is 0.45 cm on all the conditions; i.e., 13.41\% from the one-ff calibration approach performance. In the sitting condition, the error is close to the test error on GazeCapture dataset; i.e., 2.05 cm. However, most of the conditions have not yet achieved this accuracy, due to the natural difference in postures during calibration and testing. Figure~\ref{fig:overall-US3-train-test-motion} presents t-SNE plots of calibration and test data on each motion condition aggregated on all the participants. There is distribution shift in the motion data between calibration and testing stage indicate head and body behaviour differences between the two phases.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{images.v5/F13-exp3-train-test-tsne-posture.pdf}
    \caption{t-SNE plots of motion data of calibration and test dataset under each motion condition }
    \label{fig:overall-US3-train-test-motion}
\end{figure}

\noindent\textbf{Motion-specific calibration approach performance.} 
To further reduce errors, we implemented a motion-specific calibration approach, which represents the finest-grained calibration method achievable in our study. This approach allows the model to learn a wider range of body and head behaviours within a single training session. For each motion condition, we utilised 30\% of all the 9-point motion condition data to train a calibrator and tested it on the remaining 70\% of the data from the same motion condition reported as \textit{within-motion} errors. We also tested this calibrator on all other test sets of other motion conditions reported as \textit{between-motion} errors. This process was iterated five times for each motion condition, and the performance results were averaged.


The motion-specific calibration approach achieved high accuracy, with errors of less than 2 cm in stable conditions, but under dynamic conditions the error increases to 2.73 cm, dropping by nearly 48.91\%. This represents a 36.15\% reduction in error compared to the one-off calibration approach. These results suggest an upper limit of performance in our study and they indicate that the model can greatly improve its performance by appreciating more body and head movements.


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.75\textwidth]{images.v5/F14-withinbetweenmotion-cm.pdf}
\caption{Comparison of RMSE for within-motion and between-motion conditions}
\label{fig:exp3textlevelacc}
\end{figure}

Table~\ref{tab:exp3motionleveltraintest} illustrates how the motion-specific calibration approach performs across different motion conditions, highlighting the impact of motion variability on gaze estimation accuracy. The diagonal values show the lowest error rates (1.78 cm to 2.91 cm) across all conditions, indicating that models perform best when tested on the same condition they were trained on. However, performance degrades when models are applied across different conditions, even between seemingly similar static conditions like sitting and standing (e.g., sitting model error increases from 1.82 cm to 3.36 cm when applied to standing). This degradation is more pronounced between static and dynamic conditions, and particularly notable for the lying condition. These results demonstrate that the generalisation of the model is strongly influenced by the extent and type of head and body movements captured in the training data, supporting the need for motion-specific approaches in mobile gaze estimation to account for motion pattern variability.

\begin{table}[!htbp]
\centering
\begin{tabular}{c|ccccc} 
\hline
\multicolumn{1}{l|}{\diagbox{{}Test}{RMSE(cm)}{{}Train}} & Lying & Sitting & Standing & Walking & Walking in Maze \\
\hline
Lying                     & {\cellcolor[rgb]{0.737,0.843,0.502}}1.88 (1.57)~ & {\cellcolor[rgb]{0.988,0.69,0.475}}3.89 (2.27)~  & {\cellcolor[rgb]{0.992,0.71,0.478}}3.75 (2.11)~  & {\cellcolor[rgb]{1,0.886,0.514}}3.50 (2.10)~     & {\cellcolor[rgb]{0.996,0.816,0.498}}3.59 (2.06)~  \\
Sitting                   & {\cellcolor[rgb]{1,0.875,0.51}}3.52 (2.04)~      & {\cellcolor[rgb]{0.388,0.745,0.482}}1.82 (1.39)~ & {\cellcolor[rgb]{0.922,0.898,0.51}}3.36 (1.96)~  & {\cellcolor[rgb]{0.851,0.878,0.506}}3.29 (1.94)~ & {\cellcolor[rgb]{0.847,0.875,0.506}}3.31 (1.97)~  \\
Standing                  & {\cellcolor[rgb]{0.98,0.914,0.514}}3.49 (2.01)~  & {\cellcolor[rgb]{0.882,0.886,0.51}}3.31 (2.06)~  & {\cellcolor[rgb]{0.518,0.78,0.486}}1.78 (1.32)~  & {\cellcolor[rgb]{0.855,0.878,0.506}}3.19 (2.01)~ & {\cellcolor[rgb]{0.812,0.867,0.506}}3.21 (1.95)~  \\
Walking                   & {\cellcolor[rgb]{0.988,0.643,0.467}}3.89 (2.41)~ & {\cellcolor[rgb]{0.984,0.573,0.451}}4.05 (2.39)~ & {\cellcolor[rgb]{0.992,0.706,0.478}}3.86 (2.34)~ & {\cellcolor[rgb]{0.761,0.851,0.502}}2.54 (1.42)~ & {\cellcolor[rgb]{1,0.922,0.518}}3.46 (2.51)~      \\
Walking in maze           & {\cellcolor[rgb]{0.976,0.463,0.431}}4.22 (2.33)~ & {\cellcolor[rgb]{0.973,0.412,0.42}}4.31 (2.44)~  & {\cellcolor[rgb]{0.976,0.486,0.435}}4.12 (2.43)~ & {\cellcolor[rgb]{0.992,0.714,0.478}}3.73 (2.33)~ & {\cellcolor[rgb]{0.937,0.902,0.514}}2.91 (1.58)~ 
 \\\hline
\end{tabular}
\caption{Comparison of RMSE (mean and std in brackets) for between-motion-state experiments}\label{tab:exp3motionleveltraintest}
\end{table}

\subsubsection{Impact factor identification.} 
Our analysis has demonstrated that motion conditions and postures significantly impact mobile gaze estimation accuracy. To identify the key factors contributing to performance degradation, we employ a linear regression-based method. This approach allows us to quantify the relationship between various sensor features and gaze estimation error.

We curate a dataset using sensor features as input and the distance between predicted and ground truth gaze points as the target variable. Following the feature extraction procedure outlined in Section~\ref{subsec:feature}, we train calibrators using 30\% of the test data from each motion condition. These calibrators are then used to predict gaze points on the remaining 70\% of test data within the same condition and all test data from other conditions.
Given the high dimensionality of our dataset (552 features across various sensors), we employ the Lasso (Least Absolute Shrinkage and Selection Operator) regression model. Lasso is particularly suited for this analysis as it handles multicollinearity and performs automatic feature selection by assigning zero coefficients to less important features. We use LassoCV from sklearn with cross-validation and optimal alpha parameter selection to enhance the robustness of our results.

Table~\ref{tab:exp3RegMobility} presents the top-ranked features identified by our regression analysis for both between-motion (a) and within-motion (b) conditions. The results reveal the following key points. (1) \textit{Head Distance Dominance:} Across all motion conditions, head distance emerges as the most critical factor influencing gaze estimation accuracy. Its regression coefficient is consistently 4-5 times higher than the second-ranked feature. (2) \textit{Motion-Specific Factors:} For the lying condition, additional important factors include synthetic mean of magnetometer (X, Y, Z), head Euler Angle, and synthetic mean of orientation (X, Y, Z). These factors reflect the unique mobility characteristics of the lying posture. (3) \textit{Common Factors Across Conditions:} For sitting, standing, and walking conditions, common influential factors include magnetometer readings and cross-channel features of head Euler Angle (Y and Z) and orientation (X and Y). This suggests that the relative orientation and angles between the head and phone are more critical than absolute positional data. (4) \textit{Consistency Across Analysis:} There is high consistency in the identified impact factors between the ``within'' and ``between'' motion condition analyses, demonstrating the robustness of our findings.

In addition, we aggregate the total Lasso coefficients to quantify the contribution of three major groups:
\emph{head distance}, \emph{head movements} (sum of headEulerAngle features), and \emph{device orientation deflection} (orientation + magnetometer features). For the \textit{between‐motion} model, the total Lasso‐coefficient sum is 27.29, of which head distance contributes 9.79 (\(\approx35.87\%\)), head movements 2.78 (\(\approx10.19\%\)), and device orientation deflection 10.44 (\(\approx38.26\%\)). Similarly, in the \textit{within‐motion} model (sum = 153.67), head distance is 46.39 (\(\approx30.19\%\)), head movements 24.97 (\(\approx16.25\%\)), and device orientation 49.78 (\(\approx32.39\%\)). These three collectively account for more than 75\% of the total Lasso weight in both conditions, underscoring head and device orientation as the primary drivers of 2D gaze‐estimation errors.


\begin{table}[!htbp]
\centering
\includegraphics[width=\textwidth]{images.v5/F15-regression.pdf}
\caption{Impact factors identified \textit{between} and \textit{within} motion conditions}\label{tab:exp3RegMobility}
\end{table}


