\section{Related Work}\label{sec:relatedwork}

Our research intersects four key areas: (1) gaze estimation on mobile devices, (2) eye-tracking calibration, (3) user behaviour modelling, and (4) evaluation of gaze estimation on mobile devices. This section reviews relevant projects in these areas, drawing comparisons and contrasts with our work.


\subsection{Gaze Estimation on Mobile Devices}

Eye tracking research on mobile devices has gained significant traction in recent years, driven by advancements in processing capabilities and camera quality \cite{khamis2018past, cheng2024benchmark, lei2023end, ghosh2023automatic}. This shift aligns with the broader transition of daily tasks and communications from desktops to mobile platforms. Eye tracking technologies can be categorised into two main types: model-based approaches and appearance-based gaze estimation approaches. 


The model-based gaze estimation approaches utilise the geometric contours of the user's face and eyes to establish a spatial mapping relationship between the eyes and the gaze target. One problem of model based gaze estimation is that some of the parameters, like eye centre, are not directly observable~\cite{hansen2009eye, kaur2022rethinking}.  Additionally, these methods are sensitive to image quality and typically have high requirements for the usage scene~\cite{hansen2009eye, liu20203d}. Mobile device usage scenarios include a variety of complex environments such as indoor-outdoor settings and static and dynamic conditions, which presents challenges for deploying model-based approaches. Consequently, only a few works have adopted them, such as EyeTab~\cite{wood2014eyetab} and ScreenGlint~\cite{huang2017screenglint}. 

The appearance-based gaze estimation approaches are an end-to-end technique that builds user facial images directly to the gaze target from learning on large-scale datasets~\cite{zhang2015appearance, zhang2017fullface}. They can work under various lighting conditions and do not strictly require infrared illumination, making them more adaptable to standard mobile device cameras~\cite{zhang2015appearance, krafka2016eye}. In general, gaze estimation methods can be categorized based on their output into \textit{2D} gaze estimation, which predicts gaze coordinates on the screen, and \textit{3D} gaze estimation, which predicts gaze directions as 3D vectors in space~\cite{sugano2014learning, cheng2024benchmark}.

Methods for appearance-based gaze estimation can be further distinguished by their approach to handling head pose variability. 3D methods often involve head pose estimation and normalization techniques, where facial images are transformed into a normalized space to reduce variability due to head pose and other factors~\cite{zhang2018revisiting, zhang19mpiigaze}. By decoupling head pose from gaze direction, 3D methods can generalize better across different head poses and device configurations. However, applying 3D methods in mobile settings presents challenges, i.e. the data normalisation process is usually performed offline as its cost~\cite{park2019few}, which causes additional latency in mobile setting for real-time use. Recent approach, like EFE~\cite{balim2023efe}, try to reduce the pre-processing expensive cost to compute the gaze vector directly from the frame and then calculate gaze points via ray-plane intersection, leveraging the 3D gaze vector and camera parameters.


With the release of GazeCapture~\cite{krafka2016eye}, a dataset specifically for mobile devices, and subsequent datasets such as RGBDGaze~\cite{arakawa2022rgbdgaze}, 2D gaze estimation has gradually become the one of dominant algorithms behind eye tracking on mobile devices due to its simplicity and ease of deployment~\cite{lei2023end, cheng2024benchmark}. Various variants are derived with different model input settings, from eyes~\cite{valliappan2020accelerating}, to various permutations of eyes, faces, landmarks, etc.~\cite{krafka2016eye, huang2022gazeattentionnet, bao2021adaptive, huynh2021imon}, and various model architectures, e.g., CNN~\cite{zhang2015appearance,zhang2017fullface, krafka2016eye}, CNN variants combined with Recurrent Neural Networks (RNNs)~\cite{palmero2019recurrent,kellnhofer2019gaze360}, and more recent transformers~\cite{cheng2022gaze} to continuously achieve better results on the datasets. 

These ongoing efforts continue to push the boundaries of accuracy and efficiency in mobile gaze estimation. However, the inherent challenges of mobile environments such as variable lighting conditions, diverse usage postures, and constant device movement require for robust solutions that can handle the dynamic nature of mobile interactions. Our work builds upon these foundations, focusing on the limitations of 2D gaze estimation approaches in handling head pose variability, a critical factor in real-world mobile usage scenarios.


\subsection{Eye-tracking Calibration}
Eye tracking systems across desktop, VR headset, glasses and mobile device platforms, need to be calibrated to achieve high precision. The calibration process is to collect the new ground truth data and use them to adjust and customise the gaze output to reflect the current spatial geometry between camera, screen and user face~\cite{lei2023end, duchowski2017eye}. The calibration data collection is through a user interface that guides users' attention and requests them to fixate their gaze on specific points~\cite{Drewes2019timecalibration,krafka2016eye, valliappan2020accelerating, huynh2021imon} or follow a moving target~\cite{lei2023DynamicRead,Drewes2019timecalibration}. 
The data is fed to a calibrator to adjust the output from the original gaze estimation model. Valliappan et al.\cite{valliappan2020accelerating} applied this approach, they used support vector regression (SVR) as one-off calibrator to adjust the base 2D gaze estimation model for mobile eye tracking and achieved similar accuracy, $0.50\pm0.03$ cm, to commercial eye-tracker, the Tobii Pro 2, in the 30-second calibration data and sitting conditions.

Calibration has been extensively explored on desktop-based commercial eye-tracking instruments~\cite{lei2023end}. Sugano et al.~\cite{Sugano08Incremental} designed a incremental learning method to handle large variation of head poses. It uses mouse click as an implicit gaze ground truth and clusters training data by head pose to create pose-specific calibrators. Huang et al.~\cite{huang2019saccalib} applied linearity of saccade to correct gaze prediction errors and reduce the calibration distortion. Pi and Shi~\cite{pi2019task} observed that head motion causes a systematic degradation in performance due to changes in head position. Based on this finding, they propose a position-dependent linear homography-based method to correct the raw gaze estimates acquired from a remote eye tracker. Li et al.~\cite{li2022calibration} explored the correlations between gaze estimation errors and facial action units via Monte Carlo.  This technique based on repeated random sampling is used to predict the variability in eye tracking accuracy, particularly under challenging conditions. By simulating scenarios where accuracy might decrease, they employ Monte Carlo to anticipate gaze estimation errors, enabling timely recalibration. 

These studies have primarily focused on desktop or static scenarios. However, the mobile context introduces additional complexities due to frequent changes in device orientation, head posture, and environmental conditions. Our work addresses this gap by focusing specifically on calibration for appearance-based 2D gaze estimation on handheld mobile devices. We aim to study the challenges caused by mobility and perform empirical analysis to identify the key factors and quantify their impact to the precision of 2D gaze estimation. 


\subsection{User Behaviour Modelling on Handheld Mobile Devices}
The transition to mobile devices necessitates understanding user interaction patterns and their impact on gaze estimation. Mobile devices, equipped with sensors, offer real-time insights into user behaviour.
Previous research has extensively explored aspects of user attention and behaviour in mobile contexts. Studies have investigated attention interruptions in interaction \cite{adamczyk2004if, fogarty2005predicting}, attention switching \cite{steil2018forecasting}, and attention allocation in everyday life \cite{bace2020quantification}. For instance, Adamczyk et al. \cite{adamczyk2004if} and Fogarty et al. \cite{fogarty2005predicting} examined the effects of interruptions and the predictive value of sensor data in gauging interruptibility. Jiang et al. \cite{jiang2016vads} and Steil et al. \cite{steil2018forecasting} demonstrated innovative uses of device cameras and head-worn cameras to track visual attention and predict bidirectional attention shifts.

In gaze estimation, recognising users behaviours is crucial, as they impact on mobility conditions and holding postures. Huang et al.~\cite{huang2017screenglint} used front camera to record face-to-screen distances when users are in various activities. Lei et al.~\cite{lei2023DynamicRead} evaluated the effect of mobility conditions on the usability of gaze interfaces, and conclude that mobility such as walking has a significant knocking effect on the gaze interfaces that requires high eye tracking accuracy such as smooth pursuit. 
However, user behaviour modelling in the context of eye-tracking on handheld mobile devices is under-investigated. There are only a few datasets that record gaze data and facial images, including MPIIGaze~\cite{zhang2017mpiigaze}, EyeDiap~\cite{funes2014eyediap} and TableGaze~\cite{huang2017tabletgaze}.  For example, GazeCapture~\cite{krafka2016eye} dataset and RGBDGaze~\cite{arakawa2022rgbdgaze} dataset records IMU data only while collecting gaze ground truth data. There are no datasets nor studies that record IMU sensor data and eye-tracking data when users are performing daily tasks on their devices. 

Our paper aims to bridge the gap, encouraging users to perform various types of tasks on their phones under different motion conditions. The data will help us understand their behaviours under different tasks and as well as when switching tasks. The understanding will uncover insights on how user behaviours impact gaze estimation. 


\subsection{Robustness Evaluation of Gaze Estimation}
Evaluating gaze estimation on mobile devices presents unique challenges due to the dynamic nature of handheld device usage. Unlike fixed desktop or head-mounted devices, mobile eye-tracking systems must contend with constantly changing spatial relationships between the user's eyes, the device's camera, and the screen. This variability necessitates a comprehensive understanding of the factors affecting gaze estimation accuracy in mobile contexts.

Several key variables have been identified as significant contributors to performance degradation. Image resolution is one such factor; higher resolutions improve accuracy, while lower resolutions can hinder the model's ability to predict gaze points~\cite{zhang19mpiigaze}. Lighting conditions also play a crucial role; uneven illumination, shadows, and glares compromise gaze prediction, and models trained on limited lighting conditions struggle to generalize to new environments~\cite{zhang19mpiigaze, zhang2020eth}. Facial occlusions further exacerbate the problem, with partially occluded faces causing significant decreases in performance~\cite{zhang2017fullface}.

Head pose and head-to-screen distance are critical factors affecting accuracy. Models experience a performance drop of up to 34.6\% when encountering unseen head poses of ±40°, highlighting the sensitivity to head orientation~\cite{zhang2020eth}. Valliappan et al.~\cite{valliappan2020accelerating} demonstrated that precision decreases from 1.35 cm to 3.22 cm as the head deviates up to ±25° in roll, pan, and tilt. They also found that as the face ratio (the size of the face relative to the screen) decreases from 0.75 to 0.10—indicating increased head distance—the error increases from 0.83 cm to 2.63 cm. Similarly, increasing the face-to-screen distance from 25 cm to 40 cm results in an error increase from 1.21 cm to 1.64 cm~\cite{huang2017screenglint}.

Variable device holding patterns introduce additional challenges. Khamis et al.~\cite{khamis2018understanding} found that front cameras capture users' full faces only 29\% of the time during unconstrained use, due to the diversity of holding postures. Mobility further impacts performance; gaze estimation errors increase from 0.95 cm when users are sitting to 2.05 cm when walking, emphasizing the need for motion-adaptive techniques in mobile contexts~\cite{lei2023DynamicRead}.

While these studies provide valuable insights, many were conducted in controlled or static environments, which may not fully capture the range of movements and usage patterns associated with handheld devices in real-world settings. Our research aims to address this gap by focusing specifically on dynamic mobile scenarios, exploring how motion impacts gaze estimation accuracy during natural interactions, and utilized Lasson regression process to quantify the effects of motion in each direction.





