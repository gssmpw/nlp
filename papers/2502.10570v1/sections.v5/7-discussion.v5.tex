\section{Discussion}\label{sec:discussion}
Our studies offer an empirical analysis of user-device interaction under varying motion states, shedding light on the core limitations of 2D gaze estimation in mobile contexts, particularly regarding head-movement adaptation. By examining both low-level sensor signals (e.g.\ accelerometer, gyroscope) and higher-level behavioural patterns (in-task vs.\ switch-task, within- vs.\ between-motion conditions), we highlight the need for more robust and adaptive eye-tracking systems on handheld devices. Below, we discuss the main takeaways and potential paths forward.


\subsection{Behaviour Patterns in Mobile Interaction}

\subsubsection{Sensor data analysis and insights}
The analysis of sensor data, including head distance, Euler angles, accelerometer, gyroscope, and magnetometer readings, reveals the complex nature of mobile interactions. The consistency in head distance across different mobility states suggests that users subconsciously attempt to stabilise their view. However, the variability in gyroscope and accelerometer data reflects the frequent adjustments made during motion.


\subsubsection{Clustering distribution and user behaviour}
Through t-SNE visualisations and clustering, we observe distinct behavioural patterns between static (e.g.\ lying, sitting) and dynamic (e.g.\ walking) activities. This aligns with the findings of Sugano et al.~\cite{Sugano08Incremental} on desktop systems, but our work extends this concept to the more challenging domain of mobile devices. 


\subsubsection{In-Task vs. Switch-Task dynamics}
A key insight is the contrast between \emph{in-task} stability and \emph{switch-task} variability. During continuous engagement with a single activity (e.g.\ reading, scrolling), head pose remains moderately consistent. However, once a task ends or changes, rapid reorientation spikes the error. Head distance can fluctuate by up to 10\,cm, and device angles shift abruptly.


\subsection{Limitations of 2D daze estimation in handling dynamic head poses}

Our study provides empirical evidence that 2D gaze estimation methods, while popular for their simplicity, are particularly vulnerable to changes in head pose. This weakness stems from the fundamental assumption in 2D approaches that the model can implicitly learn to handle head pose variations without explicit normalisation or modelling.

\begin{itemize}
    \item \textit{Between-motion scenario:} When users transition across different motion states (e.g.\ from sitting to walking), head movements, device orientation changes, and increased screen distance collectively account for over 70\% of the variance in gaze error.
    \item \textit{Within-motion scenario:} Even within a single motion state, micro-movements, such as the trembling while typing and the swaying of the wrist, explain significant error. Our Lasso regression shows that head distance, head movements, and orientation deflection comprise about 80\% of the regression‐weight sum, implying that small head or device tilt changes still degrade accuracy.
\end{itemize}
This vulnerability arises largely because most 2D methods lack explicit head pose modelling (e.g.\ no normalisation akin to 3D geometry). Therefore, they cannot seamlessly adapt when the user’s face or device orientation drifts beyond the training sample distribution.


\subsubsection{Implications for mobile eye tracking}
A one-off calibration is often insufficient for highly dynamic daily usage. Our data suggests that small posture shifts or task switches can degrade errors by up to 2 or 3 times. A more adaptive calibration approach to periodically re-estimates the user’s head pose or normalises on‐screen coordinates is crucial for robust performance. Incorporating orientation and head‐distance data directly into the 2D pipeline may help enhance the resilience. This practice of handle head pose explicitly is often used in 3D gaze estimation pipeline. Frequent manual calibrations can disrupt flow, leading to \textit{calibration fatigue}. The approach therefore needs to be combined with natural human-device interaction to explore \emph{implicit calibration cues}; e.g., using known UI elements where the user must look to avoid user burden issues.


\subsection{Limitations}

Our study has several limitations that provide opportunities for future research.

\noindent\textit{Sample size.} Our current participant pool (10 in total) offers preliminary insights but may not capture the full variance in anthropometrics, cultural usage patterns, or visual impairments. Scaling up to more diverse user groups will increase generalizability and validate whether our findings hold for broader populations.


\noindent\textit{Experimental setup.} While we attempted to simulate real-world conditions; e.g., lying, walking in a maze, these remain semi-controlled lab scenarios. Follow-up in-the-wild deployments or longer-term longitudinal studies would better capture unpredictable daily activities, lighting changes, and unstructured user behaviours.


\noindent\textit{Hardware integration.} In User Study 1, we attached our experimental device to participants’ personal phones. Although this approach preserved their familiar interface and protected their privacy, the extra weight could affect posture and stability. Future designs might consider more integrated, lightweight hardware solutions.


\noindent\textit{Motion conditions.} The slight discrepancy in motion conditions between User Studies 1 and 2 (four vs.\ five conditions) may affect cross-study comparison. Standardising conditions across studies would be beneficial in future research to provide a more consistent evaluation of 2D gaze estimation limitations.


\noindent\textit{Gaze estimation approach comparison.} Our use of a classic 2D gaze estimation pipeline~\cite{krafka2016eye, bao2021adaptive}  without data normalisation or explicit head pose handling like 3D methods, exemplifies the limitations of current 2D approaches. Future work should compare the generalization ability of 2D and 3D gaze estimation techniques on a more directly way; i.e., complementing 2D and 3D labels under one dataset and test 2D to 3D, 3D to 2D, as well as test their domain adaptation capabilities on different datasets; i.e. different source and target domains.


\subsection{Future Direction of 2D Gaze Estimation}

\noindent\textit{Continuous calibration for 2D gaze estimation based eye tracking system} 
A single calibration does not allow the 2D gaze estimation model to obtain enough user-device interaction data for generalization performance that supports high-precision prediction in different scenarios of device usage. To obtain sufficient data for generalization, the 2D gaze estimation methods need to take multiple calibrations to incrementally and continuously update the model. 


Some projects are starting to look into this direction; for example, Sugano et al.~\cite{Sugano08Incremental} employ incremental learning for gaze estimation in a desktop environment, which learns and clusters interaction patterns with the desktop to improve performance. However, most existing solutions target desktop or static contexts. Future work must adapt these ideas to address higher variability in mobile device usage; e.g., combining inertial sensor triggers with automatic detection of posture shifts to prompt subtle re‐calibration.


\noindent\textit{Toward a 2D-3D hybrid approach}
Finally, bridging 2D and 3D paradigms may yield a powerful hybrid method that retains 2D’s simplicity while leveraging partial geometric cues. For instance, an algorithm could estimate head orientation in a lightweight fashion, then project or normalise the face region for stable 2D inference, thereby mitigating large off-axis head poses without a full 3D camera normalization. Such a pipeline could drastically cut computational cost relative to full 3D solutions yet still handle bigger pose shifts than conventional 2D methods.

Overall, our results concludes that explicitly modelling head and device orientation remains the largest unmet need in 2D gaze estimation for mobile devices. By incorporating adaptive calibration, domain transfer, and partial 3D normalisation techniques, 2D methods can move closer to the accuracy and robustness required for real‐world mobile eye‐tracking applications.




