\section{Related Work}
\subsection{Irregular Time Series Methods}

Early practices for modeling irregular time series with missing values typically relied on fixed-time discretization. In this context, \cite{choi2016doctor} ignores the timestamp information by treating all intervals as equal, \cite{lipton2016modeling} considers missing data as an effective feature for learning, 
% \cite{futoma2017learning} employs Gaussian processes to model missing data and high uncertainty in real-world situations, 
and \cite{harutyunyan2019multitask} segments the data into evenly spaced time intervals. In contrast, GRU-D \cite{che2018recurrent} employs a gated network and incorporates imputation of missing values into the optimization process. Unlike previous methods, it adopts an additional missing value mask and lag matrix as inputs. Similar strategy have been adopted in \cite{,ma2019learning,ma2020adversarial,miao2021generative}, where adversarial frameworks are utilized to enhance the prediction of imputed values. 

Some recent approaches have leveraged attention mechanisms to improve modeling. For instance, SeFT \cite{horn2020set} introduces a set of differentiable set functions and uses attention mechanisms to aggregate embeddings of different variables. ContiFormer \cite{chen2024contiformer}, on the other hand, combines neural ordinary differential equations (ODEs) with attention mechanisms based on continuous-time dynamics, extending the relationship modeling capabilities of Transformers to the continuous time domain. Besides, DNA-T \cite{huang2024dna} utilizes a deformable attention mechanism to dynamically adjust the receptive field, enabling more effective handling of local features and short-term correlations. Warpformer \cite{zhang2023warpformer} also considers multi-scale features by applying a warping module to achieve multi-grained representations. Unlike previous methods that adopt a sequence modeling perspective, ViTST \cite{li2024time} transforms the signals into RGB images and utilizes a pre-trained Swin Transformer for further classification and regression.

% There are still methods that fall outside the aforementioned categories. One work worth reviewing is Raindrop \cite{zhang2021graph}, which models times series from the perspective of graph neural networks. In this approach, each observation resembles a raindrop hitting a sensor graph, spreading information through a ripple effect. 

% On the other hand, diffusion models \cite{han2022card} are also a growing trend and have been explored in various fields such as energy \cite{xu2024denoising}, finance \cite{daiya2024diffstock}, and microbiology \cite{seki2023imputing}.

\subsection{Modeling Time Series as Images}
Transforming time series data into images has gained significant attention with the advancements in visual detection frameworks. Some approaches \cite{sood2021visual,sangha2022automated,ao2023image, semenoglou2023image, maroor2024image} plot time series directly as time-observation representations and utilize convolutional neural networks (CNNs) for downstream tasks. Generally, they do not apply special processing to the sequences, instead focusing on leveraging visual frameworks to better capture temporal patterns in visualized sequences. ViTST \cite{li2024time} is another similar case that extends further to multivariate sequences and discusses the impact of visualization parameters such as color, markers, and order. 

In contrast, other methods emphasize the modeling of time series, which requires more specialized design and expert knowledge. \cite{tripathy2018use} utilizes an iterative filtering (IF) approach to produce different intrinsic mode functions (IMFs) from EEG signals. Empirically, these transformed features often fit the task better than the original signals. Chong et al. \cite{chong2011signal} and Deng et al. \cite{bs2023_1730} model sequences based on time segmentation, calculating time-invariant features and transforming them into corresponding RGB images. Similarly, frequency domain modeling, as demonstrated by TimesNet \cite{wu2023timesnet}, has also proven effective. By utilizing fast Fourier transform (FFT) to concatenate signal of different time periods, it constructs a 2D representation optimized for CNNs. Finally, other methods model the relative relationships between points in a time series. Examples include Gramian Angular Field (GAF), Markov Transition Field (MTF), and recurrence plot \cite{10.5555/2832747.2832798,hatami2018classification}. Typically, these methods involve applying a reversible time coordinate transformation and calculating the correlations between points, effectively capturing the continuity and periodic characteristics of the sequences.