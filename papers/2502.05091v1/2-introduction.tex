\section*{Introduction}
Deep learning has revolutionized medical imaging, enabling automated disease diagnosis, accurate prognosis, and personalized treatment planning \cite{litjens2017survey, esteva2021deep, shen2017deep, fink2020potential}. With the emergence of Convolutional Neural Networks (CNNs) \cite{fukushima1980neocognitron, lecun1989handwritten} and Vision Transformers (ViTs) \cite{dosovitskiy2020image}, deep learning has achieved state-of-the-art performance in supervised tasks such as lesion detection and organ segmentation \cite{ronneberger2015u, chen2021transunet, cao2022swin, wang2022uctransnet, ates2023dual}. Despite these advancements, supervised models require training on large-scale manually annotated datasets and lack generalization across diverse medical imaging tasks and modalities.

To mitigate the reliance on labeled data, self-supervised learning (SSL) methods such as masked autoencoders \cite{he2022masked} have emerged as powerful techniques for extracting meaningful representations from unlabeled images \cite{tang2022self, chen2022scaling}. Despite their advantages, these methods still require supervised fine-tuning on small labeled datasets. Vision-language models (VLMs) such as CLIP (Contrastive Language-Image Pretraining) \cite{radford2021learning} offer a promising alternative by aligning visual and textual representations in a shared latent space, enabling zero-shot capabilities. Initially successful in natural image domains \cite{lin2022frozen, thengane2022clip}, CLIP has since been adapted for various 2D medical imaging tasks, including zero-shot classification, prompt-driven segmentation, image-text retrieval, radiology report generation, and visual question answering \cite{koleilat2024medclip, lu2024multimodal, tiu2022expert, endo2021retrieval, hu2022x, eslami2023pubmedclip, huang2023visual}.

While CLIP-based approaches have advanced 2D medical imaging, their application to 3D imaging remains underexplored due to several key challenges. A major challenge is the scarcity of large-scale, open-source datasets containing 3D image volumes paired with textual reports. To address this, Hamamci et al. \cite{hamamci2024foundation} introduced CT-RATE, a dataset consisting of 50,188 reconstructed 3D chest computed tomography (CT) volumes from 25,692 scans of 21,304 patients, along with their corresponding radiology reports. They also proposed CT-CLIP, a 3D VLM for chest CT that employs CT-ViT \cite{hamamci2025generatect}, a model incorporating a two-stage spatial and temporal transformer. While CT-CLIP performs well in zero-shot detection and image-text retrieval tasks, its use of self-attention mechanisms leads to significant computational overhead. Additionally, its image encoder follows a ViT-like patching strategy, directly downsampling images with a fixed patch size of 20. This strategy leads to feature loss, as it fails to extract hierarchical representationsâ€”an essential aspect of medical imaging. Prior studies have shown that hierarchical feature extraction is crucial for capturing spatial and contextual information in medical images \cite{chen2021transunet, cao2022swin, hatamizadeh2021swin}. Despite these limitations, CT-CLIP marks a significant step toward adapting foundation models for 3D medical imaging.

Another challenge is the computational complexity of adapting CLIP-like VLMs to 3D imaging modalities such as CT and magnetic resonance imaging (MRI). The high-resolution, volumetric nature of 3D medical data imposes substantial computational demands, particularly with standard ViT-based or 3D CNN-based encoders. While ViTs effectively capture global spatial relationships, their computational complexity scales quadratically with input size, making them inefficient for 3D volumes. Likewise, 3D convolutions, which are well-suited for capturing local spatial features, become prohibitively expensive with large kernel sizes ($k > 3$). To mitigate this, depthwise convolutions \cite{chollet2017xception} have been explored as an alternative, reducing both parameter count and computational cost. However, they can still be inefficient for 3D imaging when large kernels and high output channel dimensions are required. Figure \ref{fig:convflop_merge} compares the parameters and FLOPs of 2D and 3D standard versus depthwise convolutions, showing that standard 3D convolutions demand significantly more resources than their 2D counterparts. Although depthwise convolutions improve efficiency, their computational cost scales cubically with kernel size, posing challenges for large 3D medical datasets where large receptive fields are needed to capture complex anatomical structures. Additionally, their cumulative use throughout the network significantly increases the overall computational burden. \input{figures/conv_flop_merge}

To overcome these challenges of existing 3D VLMs, we propose \dc, a family of architectures designed for 3D medical imaging. \dc\ introduces decomposed convolutions as an alternative to self-attention and conventional 3D convolutions, significantly reducing computational complexity while preserving the ability to extract both local and global spatial features. By factorizing 3D convolutions into three 1D convolutions along each spatial dimension, \dc\ aims to balance efficiency and performance, significantly reducing model parameters and FLOPs (Figure \ref{fig:compdecomp}). We integrate \dc\ into the CLIP framework, enabling efficient joint vision-language learning for 3D medical imaging. Our experiments demonstrate that \dc\ achieves competitive zero-shot performance on CT-RATE dataset while significantly reducing computational costs compared to existing state-of-the-art methods. We anticipate that \dc\ will serve as a stepping stone for advancing scalable and efficient image encoders for vision-language understanding and inspire further research into lightweight, high-performance architectures for clinical applications.\input{figures/compdecomp}



% [old version] First is the limited 3D open-source medical imaging datasets including both images and textual reports. To tackle this issue, a recent remarkable work by Hamamci et al. \cite{hamamci2024foundation} introduced CT-RATE, a dataset that consists of 50,188 reconstructed 3D chest CT volumes from 25,692 scans of 21,304 patients and their corresponding radiology reports. They also proposed CT-CLIP, the first 3D foundation model for chest CT-focused contrastive training framework using both 3D images and radiology reports. As for the image encoder, they used CT-ViT \cite{hamamci2025generatect} which incorporates a two-stage spatial and temporal transformer. Although it performs well on tasks such as zero-shot, finetuning, and retrieval, the self-attention mechanism in these transformers is complex and not very computationally efficient. Additionally, the image encoder directly computes patches similar to ViT by downsampling the images using a patch size of 20. Especially in medical imaging, this approach results in a loss of features, as features are not extracted in a hierarchical manner. Previous studies have shown that for medical images, it is crucial to adopt a hierarchical feature extraction structure to better capture the spatial and contextual information in images \cite{chen2021transunet, cao2022swin, hatamizadeh2021swin}. Nonetheless, their critical contribution has opened the door for advancing 3D medical imaging using foundation models.

% Secondly, as briefly mentioned above, adapting CLIP to medical imaging, particularly 3D modalities like computed tomography (CT), introduces some computational challenges. The high-resolution and volumetric nature of 3D medical data results in substantial computational overhead when using standard ViT or 3D CNN-based image encoders. While ViTs excel in capturing global spatial relationships using the self-attention mechanism, their computational complexity scales quadratically with input size, making them inefficient for 3D volumes. Similarly, 3D convolutions which are very effective in capturing local spatial features, are also computationally expensive, especially for large kernel sizes ($k > 3$). Although depthwise convolutions\cite{chollet2017xception} provide an alternative by reducing the parameter count and computational cost compared to regular convolutions, they can still be inefficient for 3D when large kernel sizes and number of output channels are used. Figure. \ref{fig:convflop_merge}a shows the comparison of parameters and FLOPs for 2D and 3D standard and depthwise convolutions. Both standard 2D and 3D convolutions require significantly more parameters and FLOPs compared to depthwise convolutions. Especially for the 3D case, the use of depthwise convolution becomes much more efficient than standard convolution. Nonetheless, it still may not be very efficient as the computational cost of depthwise convolutions still increases cubically with respect to the kernel size. Given that large kernels are typically required to capture complex spatial features in 3D medical data, this scaling becomes particularly problematic. Moreover, given that these convolutions are typically used to form an end-to-end network, the cumulative effect of such operations can lead to an extensive computational burden.
% \input{figures/conv_flop_merge}
% %\input{tables/paramflop}

% To address these challenges, we propose \dc\, a family of architectures specifically designed for 3D medical imaging. \dc\ introduces decomposed convolutions as an alternative to self-attention and conventional 3D convolutions, significantly reducing computational complexity while preserving the ability to extract both local and global spatial features. By factorizing 3D convolutions into multiple 1D operations along each spatial dimension, \dc\ aims to achieve a better trade-off between efficiency and performance, significantly reducing model parameters and FLOPs (Figure \ref{fig:convflop_merge}b). We integrate \dc\ into the CLIP framework, which enables efficient joint vision-language learning for 3D medical imaging. Our experiments demonstrate that \dc\ achieves competitive performance in zero-shot and transfer learning tasks across various datasets and maintains high accuracy while substantially reducing the computational burden over existing SOTA methods. We hope \dc\ will serve as a stepping stone for advancing scalable and efficient image encoders for joint vision-language learning applications and inspire further research into lightweight, high-performance architectures for practical clinical 3D medical imaging applications.
