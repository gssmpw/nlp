\section*{Results}
We train a CLIP-based joint vision-language learning framework (see Developing a CLIP framework using \dc\ in the Methods section) using the CT-RATE dataset, consisting of 50,188 non-contrast 3D chest CT volumes paired with corresponding radiology text reports. For the text encoder, we use CXR-BERT \cite{boecking2022making}. For the image encoder, we comprehensively evaluate our proposed \dc\ architecture by benchmarking its zero-shot performance against several state-of-the-art models, including ViT \cite{dosovitskiy2020image}, CT-ViT \cite{hamamci2025generatect, hamamci2024foundation}, TransUNet \cite{chen2021transunet}, ConvNeXt \cite{liu2022convnet}, InceptionNeXt \cite{yu2024inceptionnext}, and PoolFormer \cite{yu2022metaformer} (see SOTA image encoders in the Methods section).
\input{tables/results}


\subsection*{Zero-shot multi-abnormality detection results}
Once trained to maximize the similarity between image and text embeddings, the CLIP framework enables multi-abnormality detection by inputting each abnormality as a text prompt (see *Zero-shot multi-abnormality detection with \dc* in the Methods section). Specifically, we use the prompts "{Abnormality} is present." and "{Abnormality} is not present." for each of the 18 distinct abnormalities, following the ablation study by Hamamci et al. \cite{hamamci2024foundation}. We then compute the normalized probability of each abnormality being present in the CT image and evaluate model performance using accuracy, F1 score, precision, and recall.

Table \ref{results} compares the zero-shot performance and computational efficiency of multiple models on the CT-RATE dataset for image volumes of size 512×512×256. \dc\ provides a family of models with configurations ranging from nano to tiny variants, with parameter counts between 920.3K and 15.1M and GFLOPs spanning 34.21G to 168.2G. As shown in Table \ref{results}, DCFormer consistently achieves superior accuracy, F1 score, precision, and recall across all configurations while maintaining computational efficiency.

For instance, the nano variant achieves higher F1 score, precision, and recall than ConvNeXt and PoolFormer, despite having fewer parameters and similar FLOPs. Similarly, the naïve variant outperforms ConvNeXt, PoolFormer, and TransUNet, demonstrating the efficiency and effectiveness of DCFormer's architecture. Notably, DCFormer achieves these results with substantially fewer computational resources compared to models like TransUNet and CTViT, which require much larger parameter counts and FLOPs. For example, the naive variant of \dc\ delivers 63.1\% accuracy, 44.5\% F1 score, 29.5\% precision, and 65.5\% recall using just 5.85M parameters and 49.48 GFLOPs—far fewer parameters and similar FLOPs compared to CTViT’s 101.1M parameters and 160.5 GFLOPs and TransUnet's 23.93M parameters and 207.5 GFLOPs.

This efficiency enables DCFormer to achieve robust performance with a simple yet effective architecture, demonstrating its scalability and suitability for applications requiring both high accuracy and computational efficiency. Overall, for zero-shot implementation, \dc\ outperforms other models in efficiency, with its lightweight variants achieving competitive results while significantly reducing computational overhead. DCFormer’s ability to outperform SOTA models with fewer parameters and FLOPs highlights its potential for applications requiring both high performance and computational efficiency.


% [old version] Once the CLIP framework is trained to maximize the similarity between the embeddings of image and text encoders, it is straightforward to perform zero-shot multi-abnormality detection by inputting each abnormality as a text prompt (see Zero-shot multi-abnormality detection with \dc\ in Methods). Specifically, we utilize '\{\textit{Abnormality}\} is present.' and '\{\textit{Abnormality}\} is not present.' prompts for each of the 18 distinct abnormalities following the ablation study of Hamamci et al.\cite{hamamci2024foundation}. Finally, we compute the normalized probability for each abnormality being present on the CT image and compare the accuracy, f1 score, precision and recall scores with the previous SOTA methods.

% Table.\ref{results} compares the zero-shot performance and computational efficiency of various models, including \dc\, and other SOTA methods including ConvNeXt, PoolFormer, ViT, and TransUNet on the CT-RATE dataset at a resolution of 512×512×256. \dc\ offers a family of models with configurations ranging from nano to tiny variants, with parameter counts spanning from 920.3K to 15.1M and GFLOPs ranging from 34.21G to 168.2G. As shown in Table.\ref{results}, DCFormer consistently outperforms other models in terms of accuracy, F1 score, precision, and recall across all variants, while maintaining significantly fewer parameters and FLOPs. For instance, the nano variant achieves higher accuracy, F1 score, precision, and recall than ConvNeXt and PoolFormer, despite having fewer parameters and similar FLOPs. Similarly, the naïve variant achieves superior performance compared to ConvNeXt, PoolFormer and TransUnet, demonstrating the efficiency and effectiveness of DCFormer's architecture. Note that, DCFormer achieves these results with substantially fewer computational resources compared to models like TransUNet and CTViT, which require much larger parameter counts and FLOPs. For example, the tiny variant delivers 64.0\% accuracy, 46.2\% F1 score, 29.9\% precision, and 69.8\% recall using just 15.1M parameters and 168.2G FLOPs, far fewer parameters and similar FLOPs compared to CTViT’s 101.1M parameters and 160.5 GFLOPs and TransUnet's 23.93M parameters and 207.5 GFLOPs.  Such ability of DCFormer delivers a robust performance with a simple and efficient architecture, showing its scalability and suitability for applications that require both high accuracy and computational efficiency.

% Overall, for zero-shot implementation, \dc\ outperforms other models in terms of efficiency, with its lightweight variants achieving competitive results while significantly reducing computational overhead. DCFormer's ability to outperform state-of-the-art models with fewer parameters and FLOPs highlights its potential for applications requiring high performance and computational efficiency.

%The \dc\ naïve variant achieves an accuracy of 66.2\% and an F1 score of 45.2\%, outperforming all ViT variants. Specifically, ViT naïve achieves 55\% accuracy and an F1 score of 42.5\%, while ViT tiny and small variants achieve 61\% and 62.8\% accuracy with F1 scores of 43.2\% and 44.5\%, respectively. Despite its superior performance, \dc\ is significantly more efficient, requiring only 6.20M parameters and 139.3G FLOPs. In comparison, ViT naïve uses 11.10M parameters and 39.05G FLOPs, ViT tiny requires 26.34M parameters and 86.43G FLOPs, and ViT small demands 45.81M parameters and 142.5G FLOPs.

%Compared to CTViT, which achieves 63\% accuracy and an F1 score of 44.2\%, the \dc\ naïve variant not only delivers better performance but also greatly enhances efficiency. CTViT requires 101.1M parameters and 160.5G FLOPs, whereas \dc\ achieves superior results with 16 times fewer parameters and comparable computational cost ... More results... 



 %\subsection*{Fine-tuning multi-abnormality detection results}
 %In this section, we evaluate the \dc\ image encoder by comparing its performance to other SOTA models through a fine-tuning approach. While zero-shot evaluation has the advantage of requiring no labeled data, fine-tuning offers significant improvements in classification performance by allowing the model to adapt to specific downstream tasks. Here, we extend the trained encoders with a single classification layer and fine-tune them in a supervised manner to classify 18 abnormalities. Binary Cross-Entropy (BCE) loss is used to optimize the models which enables independent probability predictions for each class rather than multi-class predictions.
 
 %\subsection*{Retrieval Tasks}

