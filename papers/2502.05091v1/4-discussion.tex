\section*{Discussion}
In this paper, we introduce DCFormer, a novel vision encoder designed to efficiently process 3D medical images within a CLIP-based vision-language framework. DCFormer addresses key computational challenges in 3D CT analysis, where existing models struggle with high-resolution volumetric data due to the quadratic scaling of self-attention in vision transformers and the high computational cost of 3D convolutions. To mitigate these issues, DCFormer leverages decomposed 3D convolutions as a token mixer, reducing FLOPs and parameter count while preserving strong spatial feature extraction capabilities. This design enables superior performance in zero-shot predictions.

We evaluate DCFormer on CT-RATE, a large-scale dataset containing 50,188 reconstructed 3D chest CT volumes paired with radiology reports. The model is tested under the zero-shot setting, benchmarking its performance against state-of-the-art methods including ViT, ConvNeXt, PoolFormer, and TransUNet. Our primary objective is to develop a model with strong zero-shot capabilities for multi-abnormality detection, minimizing the need for task-specific fine-tuning. As shown in Table \ref{results}, DCFormer consistently outperforms competing models in accuracy, F1 score, precision, and recall, all while requiring significantly fewer parameters and computational resources. This efficiency is driven by decomposed convolutions, which factorize 3D convolutions into multiple 1D operations along each spatial dimension, striking an optimal balance between performance and efficiency. Unlike CT-ViT and other computationally intensive 3D models, DCFormer offers a lightweight and scalable solution for large-scale 3D medical imaging, making it particularly well-suited for clinical applications requiring real-time decision-making.

While DCFormer demonstrates promising results, several future directions could enhance its capabilities. First, advanced prompt engineering may further improve zero-shot performance by better aligning image-text embeddings for classification tasks. Second, incorporating larger and more diverse datasets across different medical imaging modalities—such as MRI and PET—could improve generalization and further validate DCFormer’s effectiveness. Finally, integrating DCFormer with other VLMs, such as LLaVA \cite{liu2024visual} and LISA \cite{lai2024lisa}, could expand its applications to visual question answering and prompt-driven 3D image segmentation.

% [Old Version] In this article, we introduced DCFormer, a novel vision encoder specifically designed to efficiently process 3D medical images within a CLIP-based vision-language framework. Our approach addresses key computational challenges in 3D CT analysis, where existing models struggle with high-resolution volumetric data due to the quadratic scaling of self-attention mechanisms in ViTs and the high parameter and FLOP count of 3D convolutions. By employing the decompositon of 3D convolutions as a token mixer, DCFormer reduces FLOPs and parameter count while maintaining strong spatial feature extraction capabilities, ultimately resulting in better zero-shot and fine-tuned predictions. We evaluated DCFormer on the CT-RATE dataset, which contains 50,188 reconstructed 3D chest CT volumes paired with radiology reports, providing a large-scale foundation for training a multimodal framework. The model was tested under both zero-shot and fine-tuned settings, comparing its performance to state-of-the-art (SOTA) methods such as ViT, ConvNeXt, PoolFormer, and TransUNet.

% One of the primary goals of this work was to develop a model capable of strong zero-shot performance, enabling multi-abnormality detection without task-specific fine-tuning. The results in Table 1 demonstrate that DCFormer consistently outperforms other models across accuracy, F1 score, precision, and recall, while requiring significantly fewer parameters and computational resources. The zero-shot results of DCFormer demonstrated its effectiveness in handling 3D medical imaging tasks, particularly in multi-abnormality detection on the CT-RATE dataset. DCFormer consistently outperformed state-of-the-art models such as ConvNeXt, PoolFormer, and ViT across its variants (nano, naïve, and tiny) in terms of accuracy, F1 score, precision, and recall, while maintaining significantly fewer parameters and FLOPs. The key strength of DCFormer lies in its use of decomposed convolutions, which significantly reduce computational complexity while preserving the ability to capture both local and global spatial features. By factorizing 3D convolutions into multiple 1D operations along each spatial dimension, DCFormer achieves a better trade-off between efficiency and performance compared to other models. Unlike CTViT and other 3D methods, most of which require heavy computational resources, DCFormer provides a practical solution for large-scale 3D medical imaging tasks where its lightweight nature makes it well-suited for clinical applications, where efficiency is crucial for real-time medical decision-making.

% While DCFormer has shown promising results, there are several future research directions to further enhance its capabilities. First, exploring more advanced prompt engineering techniques could improve the zero-shot performance of DCFormer. Experimenting with different domain-specific prompts could lead to better alignment between image and text embeddings, thereby improving classification accuracy. Additionally, incorporating more diverse and larger-scale datasets could help generalize the model's performance across different medical imaging tasks and modalities. Another promising direction is to extend DCFormer's architecture to other 3D medical imaging tasks beyond chest CT, such as MRI or PET. Such an application would involve adapting the decomposed convolution strategy to handle different types of volumetric data in the CLIP-based vision-language framework.