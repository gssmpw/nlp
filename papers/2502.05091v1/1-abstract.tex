\begin{abstract}
Vision-language models (VLMs) align visual and textual representations, enabling high-performance zero-shot classification and image-text retrieval in 2D medical imaging. However, extending VLMs to 3D medical imaging remains computationally challenging. Existing 3D VLMs rely on Vision Transformers (ViTs), which are computationally expensive due to self-attention’s quadratic complexity, or 3D convolutions, which demand excessive parameters and FLOPs as kernel size increases. We introduce DCFormer, an efficient 3D medical image encoder that factorizes 3D convolutions into three parallel 1D convolutions along depth, height, and width. This design preserves spatial information while significantly reducing computational cost. Integrated into a CLIP-based vision-language framework, DCFormer is evaluated on CT-RATE, a dataset of 50,188 paired 3D chest CT volumes and radiology reports, for zero-shot multi-abnormality detection across 18 pathologies. Compared to ViT, ConvNeXt, PoolFormer, and TransUNet, DCFormer achieves superior efficiency and accuracy, with DCFormer-Tiny reaching 62.0\% accuracy and a 46.3\% F1-score while using significantly fewer parameters. These results highlight DCFormer’s potential for scalable, clinically deployable 3D medical VLMs. 
%Our code is available at: \url{https://github.com/mirthAI/DCFormer.}
Our codes will be publicly available.

% [Old version: Wei, 250 words] Vision-language models (VLMs) enable cross-modal understanding by aligning visual and textual representations. A notable example is CLIP (Contrastive Language-Image Pretraining), which employs contrastive learning on large-scale, unlabeled image-text pairs to train image and text encoders. CLIP has shown promise in 2D medical imaging, supporting tasks like zero-shot classification and image-text retrieval. However, extending VLMs to 3D medical imaging remains underexplored due to high computational demands. Existing 3D VLMs often use vision transformers (ViTs) as image encoders, leveraging self-attention to capture long-range dependencies. While self-attention effectively models global features, its quadratic complexity makes it computationally expensive for 3D volumes. An alternative approach, 3D convolutions, extracts features hierarchically but requires significantly more parameters and computational overhead as kernel sizes increase. We propose DCFormer (DeComposed Former), an efficient architecture for 3D medical image encoding. DCFormer replaces conventional convolutions with decomposed convolutions, factorizing 3D convolutions into three parallel 1D convolutions along depth, height, and width. This design preserves spatial information while significantly reducing parameters and FLOPS. We integrate DCFormer into a CLIP-based vision-language framework and evaluate it on CT-RATE, a large-scale dataset of 50,188 paired 3D chest CT volumes and radiology reports, assessing zero-shot multi-abnormality detection across 18 pathologies. Compared to ViT, ConvNeXt, PoolFormer, and TransUNet, DCFormer achieves superior efficiency and accuracy. Notably, DCFormer-Tiny attains 64.0\% accuracy and a 46.2\% F1-score, outperforming larger models while using significantly fewer parameters. 
% These results demonstrate DCFormer’s effectiveness for scalable and accurate vision-language understanding in 3D medical imaging. Our code is publicly available at: \url{https://github.com/mirthAI/DCFormer}.

% [Old version: Gorkem] Foundation models such as CLIP (Contrastive Language-Image Pretraining) have achieved remarkable success in natural image domains by leveraging joint vision-language representation learning and have recently been applied to medical imaging tasks. However, the extension of CLIP to 3D medical imaging introduces significant computational challenges due to the high resolution and volumetric nature of the data. Current implementations often rely on Vision Transformer (ViT) and its variants as image encoders, which use self-attention as the token mixer for spatial feature extraction. Although self-attention is very effective in extracting global interactions, it scales quadratically with input size, making it computationally expensive for 3D volumes. Similarly, 3D convolutions are an alternative for feature extraction but introduce additional parameters and computational overhead, especially with larger kernel sizes. In this article, we propose \dc\ (DeComposed Former), a novel token mixer for CLIP image encoder, specifically designed to handle 3D, high-resolution medical images efficiently. \dc\ employs decomposed convolutions as the token mixer, replacing conventional self-attention and 3D convolutions. By factorizing 3D convolutions into several 1D operations along each spatial dimension, \dc\ significantly reduces model complexity, parameter count, and FLOPs while maintaining the ability to capture both local and global spatial features effectively. Finally, based on the \dc\, we build a family of models for different scales. Our experiments demonstrate that \dc\ achieves competitive performance in zero-shot and transfer learning tasks on various datasets compared to previous SOTA methods and maintains high accuracy while significantly improving computational efficiency. Our codes are available at: \textcolor{blue}{(github link will be added)}.


\end{abstract}