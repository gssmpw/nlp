\section*{Methods}
%We propose DCFormer, a novel token mixer to efficiently process 3D CT images in the context of a CLIP-based framework. DCFormer addresses the computational challenges inherit in volumetric 3D CT scans by decomposing the computationally expensive 3D convolution into several 1D operations across all spatial dimensions. DCFormer utilizes MetaFormer\cite{yu2022metaformer, yu2023metaformer} as the backbone block design and uses decomposed convolutions as the token mixer for feature extraction. It also enjoys a hierarchical structure which ultimately enhances feature extraction capacity for 3D CT images. Finally, the proposed DCFormer-based image encoder is integrated into a CLIP framework to achieve text-image alignment. As we will show later, such a decomposition strategy significantly reduces parameter count and computational costs while preserving high zero-shot and fine-tuning performance. 

%\subsection*{DCFormer}


Processing 3D CT images presents significant computational challenges due to their high resolution and volumetric complexity. 3D convolution is computationally expensive due to its cubic scaling with kernel size, and the self-attention mechanism further amplifies the burden by scaling quadratically with input size. To address these challenges, we propose DCFormer, a hybrid architecture that decomposes 3D convolution into multiple 1D components, significantly reducing computational overhead. DCFormer adopts MetaFormer \cite{yu2022metaformer, yu2023metaformer} as its backbone and employs decomposed convolutions as the token mixer for feature extraction. Its hierarchical structure further enhances feature representation in 3D CT images. Finally, the DCFormer-based image encoder is integrated into a CLIP framework for image-text alignment. As demonstrated in our results, this decomposition strategy substantially reduces parameter count and computational cost while maintaining high performance in both zero-shot and fine-tuning settings.

% [old version] Processing 3D CT images brings significant computational challenges due to their high input resolution and the complexity of volumetric data. Especially in the CLIP framework using 3D vision encoder, the computational burden is amplified since a text encoder must be jointly trained with the vision encoder. Although 3D convolution is very effective in capturing spatial features across three dimensions, it is computationally expensive, especially when applied to high-resolution volumetric images. Similarly, the self-attention mechanism also suffers from quadratic scaling with respect to the input size which further amplifies the computational burden when applied to 3D volumetric data. To address these challenges, we propose DCFormer, a hybrid architectural family that decomposes 3D convolution into several 1D components which ultimately reduces computational overhead. DCFormer utilizes MetaFormer\cite{yu2022metaformer, yu2023metaformer} as the backbone block design and uses decomposed convolutions as the token mixer for feature extraction. It also enjoys a hierarchical structure which ultimately enhances feature extraction capacity for 3D CT images. Finally, the proposed DCFormer-based image encoder is integrated into a CLIP framework to achieve text-image alignment. As we show in the results section, such a decomposition strategy significantly reduces parameter count and computational costs while preserving high zero-shot and fine-tuning performance. 


\subsubsection*{Formulation of DCFormer block} %% may need some updates attention part

The core concept, \textit{MetaFormer} \cite{yu2022metaformer, yu2023metaformer}, is a general architecture abstracted from transformers \cite{vaswani2017attention}. In the Metaformer architecture, the token mixer (e.g. self-attention, depthwise convolution) is not specified, while normalization, a channel mixer (e.g. MLP), and residual connections \cite{he2016deep} are retained. The input \( I \) is first passed into a patch embedding block, such as a convolution:
\begin{align}
X = \text{PatchEmbed}(I)=Flatten(\text{Conv}^{(k, s, p)}_{C_0 \rightarrow C} (I)) \label{eq1}
\end{align}
where \( X \in \mathbb{R}^{N \times C} \) denotes the embedded tokens with sequence length \( N \) and embedding dimension \( C \). Here, $C_0, k, s$ and $p$ represent input image channels, kernel size, stride and padding, respectively. These embedded tokens are then fed into the Metaformer architecture: 
\begin{align}
X' &= X + \text{TokenMixer}(\text{Norm}_1(X)), \label{eq2} \\
X'' &= X' + \sigma(\text{Norm}_2(X')W_1)W_2. \label{eq3}
\end{align}
Here, $\text{Norm}_1$ and $\text{Norm}_2$ are typically batch normalization \cite{ioffe2015batch} or layer normalization \cite{ba2016layer}. The TokenMixer serves as the core module for spatial information interaction, $W_1$ and $W_2$ are learnable weights in a two-layer channel MLP, and $\sigma$ is a non-linear activation \cite{nair2010rectified, hendrycks2016gaussian}. 


% [Old version] The core concept \textit{MetaFormer} \cite{yu2022metaformer, yu2023metaformer} is a general architecture abstracted from transformers \cite{vaswani2017attention}. In a general Metaformer architecture, the token mixer (e.g. self-attention, depthwise convolution) is typically not specified while normalization, channel-mixer (e.g. MLP) and residual connections \cite{he2016deep} are retained. The input \( I \) is first passed into a patch embedding block such as convolution:
% \begin{align}
% X = \text{PatchEmbed}(I)=Flatten(\text{Conv}^{(k, s, p)}_{C_0 \rightarrow C} (X)) \label{eq1}
% \end{align}
% where \( X \in \mathbb{R}^{N \times C} \) denotes the embedded tokens with sequence length \( N \) and embedding dimension \( C \). $C_0, k, s$ and $p$ represent input image channels, kernel size, stride and padding, respectively. Then these embedded tokens are fed into the Metaformer architecture: 
% \begin{align}
% X' &= X + \text{TokenMixer}(\text{Norm}_1(X)), \label{eq2} \\
% X'' &= X' + \sigma(\text{Norm}_2(X')W_1)W_2. \label{eq3}
% \end{align}
% Here, $\text{Norm}_1$ and $\text{Norm}_2$ are typically either batch \cite{ioffe2015batch} or layer \cite{ba2016layer} normalization. TokenMixer is the main block for spatial information interaction, $W_1$ and $W_2$ are learnable weights in a two-layer channel MLP and $\sigma$ is a non-linear activation \cite{nair2010rectified, hendrycks2016gaussian}. 


To further enhance the MetaFormer architecture, we introduce Decomposed Convolution as the token mixer within the DCFormer block. This design leverages the computational efficiency of decomposed 1D convolutional operations along each spatial axis (height, width, and depth). By splitting the 3D convolution into three parallel 1D convolutions, Decomposed Convolution captures spatial features while significantly reducing the number of parameters and computational cost. Thus, the DCFormer block integrates decomposed convolutions as a lightweight yet powerful token mixer. Let \( X \in \mathbb{R}^{B \times C \times H \times W \times D} \) denote the input feature map, where \( B \) is the batch size, \( C \) is the number of channels, and \( H \), \( W \), and \( D \) represent the spatial dimensions (height, width, and depth), respectively. The decomposed convolution consists of three 1D depthwise convolutions, processing the input tensor along each spatial axis:
\begin{align}
X^*_{\text{h}} &= \text{DWConv}^{k_h \times 1 \times 1}_{C \rightarrow C} (X), \\
X^*_{\text{w}} &= \text{DWConv}^{1 \times k_w \times 1}_{C \rightarrow C} (X), \\
X^*_{\text{d}} &= \text{DWConv}^{1 \times 1 \times k_d}_{C \rightarrow C} (X) 
\end{align}
where ($k_h, k_w, k_d$) represents the kernel sizes in height, width and depth dimensions, respectively. In our implementation, we set $k_h = k_w = k_d = k \in \{13, 11, 9, 7\}$ to leverage large kernels while maintaining computational efficiency through decomposition.
After applying decomposed convolutions along each spatial axis, we normalize the resulting features separately and then combine them using elementwise summation to form the DCFormer block: 
\begin{align}
X' &= X + \text{Norm}_h(X^*_{\text{h}}) + \text{Norm}_w(X^*_{\text{w}}) + \text{Norm}_d(X^*_{\text{d}})
\end{align}

An illustration of the DCFormer and its Pytorch-like implementation are shown in Figure \ref{fig:deconvnext} and Algorithm \ref{algo:decompconv3d}, respectively.

% [Old version] To further enhance the MetaFormer architecture, we introduce Decomposed Convolution as the token mixer within the DCFormer block. This design leverages the computational efficiency of decomposed 1D convolutional operations along each spatial axis (height, width, and depth) instead of traditional 3D convolutions. By splitting the 3D convolution into three parallel 1D convolutions, Decomposed Convolution can capture spatial features while significantly reducing the number of parameters and computational costs. Thus, the DCFormer block integrates decomposed convolutions as a lightweight yet powerful token mixer. Let the input tensor \( X \in \mathbb{R}^{N \times C \times H \times W \times D} \) represent the input feature map with batch size \( N \), channels \( C \), height \( H \), width \( W \), and depth \( D \). The decomposed convolution consists of three 1D convolutions, processing the input in each spatial axis:
% \begin{align}
% X^*_{\text{h}} &= \text{DWConv}^{k_h \times 1 \times 1}_{g \rightarrow g} (X), \\
% X^*_{\text{w}} &= \text{DWConv}^{1 \times k_w \times 1}_{g \rightarrow g} (X), \\
% X^*_{\text{d}} &= \text{DWConv}^{1 \times 1 \times k_d}_{g \rightarrow g} (X) 
% \end{align}
% where ($k_h, k_w, k_d$) represent the kernel sizes in height, width and depth dimensions, respectively. Note that in our implementation, $k_h = k_w = k_d = k \in \{13, 11, 9, 7\}$, aiming to leverage large kernels with our decomposition strategy.
% After applying decomposed convolutions along each spatial axis, we normalize each of the resulting components separately and then combine them using elementwise summation to form the DCFormer block: 
% \begin{align}
% X' &= X + \text{Norm}_h(X^*_{\text{h}}) + \text{Norm}_w(X^*_{\text{w}}) + \text{Norm}_d(X^*_{\text{d}})
% \end{align}
% The illustration of the DCFormer and its Pytorch-like code is shown in Figure \ref{fig:deconvnext} and Algorithm \ref{algo:decompconv3d}, respectively.

\input{figures/deconvnext}
\input{algorithm}

\subsubsection*{Complexity analysis}
Given an input tensor \( X \in \mathbb{R}^{1 \times C \times H \times W \times D} \) with batch size \( B=1 \) a 3D depthwise convolution \cite{chollet2017xception} with a kernel size ($k_h, k_w, k_d$) where $k_h = k_w = k_d = k$, the total parameter count and FLOPs can be calculated as $Ck^3$ and $2CHWDk^3$, respectively (bias is omitted for simplicity). Note that both parameters and FLOPS increase cubically as $k$ increases. This introduces extensive computational burden, especially for large kernel sizes and when these convolutions are used repeatedly in an end-to-end network, where the effect of the cubic scaling compounds over multiple layers. 

By decomposing the 3D depthwise convolution into three 1D depthwise convolutions, the parameter count and FLOPS are significantly reduced to $3Ck$ and $6CHWDk$, respectively, as each 1D convolution operates independently along a single spatial dimension rather than all three simultaneously. A comparison of FLOPs and parameter counts for 3D depthwise and decomposed convolutions is shown in Figure \ref{fig:compdecomp}. For the decomposed convolution, the computational cost scales linearly with respect to kernel size $k$, as opposed to the cubic scaling in the 3D depthwise convolution. 

When the kernel size is larger than 3, the complexity increase becomes much more significant. Large kernel sizes such as 7, 11 \cite{liu2022convnet, yu2024inceptionnext} and even up to 51 \cite{ding2022scaling, liu2022more} have been shown to improve model performance. In 3D imaging, our decomposition approach becomes crucial, particularly in deep networks where multiple convolution layers are applied. 

% [Old version] Given an input tensor \( X \in \mathbb{R}^{1 \times C \times H \times W \times D} \) with batch size \( N=1 \) and \( C \), \( H \), \( W \), \( D \) representing channel, height, width and depth and a 3D depthwise convolution \cite{chollet2017xception} with a kernel size ($k_h, k_w, k_d$) where $k_h = k_w = k_d = k$, the total parameter count and FLOPs can be calculated as $Ck^3$ and $2CHWDk^3$, respectively (bias is omitted for simplicity). Note that both parameters and FLOPS increase cubically as $k$ increases. This introduces extensive computational burden, especially for large kernel sizes and when these convolutions are used repeatedly in an end-to-end network, where the effect of the cubic scaling compounds over multiple layers. By decomposing the 3D depthwise convolution into three 1D depthwise convolutions, the parameter count and FLOPS become $3Ck$ and $6CHWDk$, respectively. Such a decomposition strategy significantly reduces both the parameter count and FLOPs, as each 1D convolution is applied in a parallel manner along one spatial dimension rather than across all three dimensions at once. Comparisons of both FLOPS and parameters for 3D depthwise and decomposed convolutions are shared in Figure \ref{fig:convflop_merge}b. For the decomposed convolution, the computational cost scales linearly with respect to $k$, as opposed to the cubic scaling in the 3D depthwise convolution. Especially when the kernel size is larger than 3, the complexity increase becomes much more significant. Given that, large kernel sizes such as 7, 11 \cite{liu2022convnet, yu2024inceptionnext} and up to 51 \cite{ding2022scaling, liu2022more} are shown to improve the performance, the decomposition approach becomes  much more critical, especially when used in deep networks where multiple layers of convolution are applied. 
%\input{figures/flop}
\input{figures/overview}

\subsubsection*{Building a vision encoder with \dc}
Based on the above findings, we formulate DCFormer as the token mixer and develop a family of architectures of varying sizes, using DCFormer as the main building block. Figure \ref{fig:overview} illustrates the general architecture of our proposed DCFormer. Following previous work \cite{he2016deep, liu2021swin, liu2022convnet, yu2022metaformer, wang2023internimage, yu2024inceptionnext}, DCFormer adopts a hierarchical structure with four stages and an initial stem stage following \cite{wang2023internimage, dai2021coatnet}. 

In the stem stage, we use a decomposed convolution with a kernel size of 7 and a stride of 4. The downsampled images are then processed with three additional decomposed convolutions, each with a kernel size of 3 and a stride of 1. The outputs of the stem stage are passed into the hierarchical structure, where each stage produces tokens of sizes \(\frac{H}{8} \times \frac{W}{8} \times \frac{D}{8}\), \(\frac{H}{16} \times \frac{W}{16} \times \frac{D}{16}\), \(\frac{H}{32} \times \frac{W}{32} \times \frac{D}{32}\), and \(\frac{H}{64} \times \frac{W}{64} \times \frac{D}{64}\), respectively, where $H$, $W$, $D$ represent height, width and depth of the input image volume. 

We propose four model variants: nano, naive, tiny, small. The number of layers in each stage are [1, 1, 1, 1] for nano, [2, 2, 2, 2] for naive, [2, 3, 3, 2] for tiny and [2, 3, 6, 2] for small models. Additionally, each stage begins with patch embedding where a 3D max pooling operation with a kernel size of 3 and a stride of 2 is used to reduce the image dimensions \cite{dai2021coatnet}. Following \cite{dosovitskiy2020image, liu2022convnet, yu2024inceptionnext}, we set the MLP ratio to 4, while the kernel size for the decomposed convolution is set to 7, as in \cite{liu2021swin, liu2022convnet}. Further configuration details are provided in Table \ref{config}. 
\input{tables/config}

% [Old version] Based on the above findings, we formulate DCFormer as the token mixer and build a family of architectures for different sizes using DCFormer as the main building block. Figure. \ref{fig:overview} demonstrates the general architecture of our proposed DCFormer. Following previous work \cite{he2016deep, liu2021swin, liu2022convnet, yu2022metaformer, wang2023internimage, yu2024inceptionnext}, DCFormer adapts a hierarchical structure with 4 stages and a stem stage following \cite{wang2023internimage, dai2021coatnet}. As for the initial stem stage, we use a decomposed convolution with a kernel size of 7 and a stride of 4. Then the downsampled images are processed further with a sequence of another three decomposed convolutions with a kernel size of 3 and a stride of 1. Finally, the outputs of the stem stage are passed into the hierarchical structure, each stage having \(\frac{H}{8} \times \frac{W}{8} \times \frac{D}{8}\), \(\frac{H}{16} \times \frac{W}{16} \times \frac{D}{16}\), \(\frac{H}{32} \times \frac{W}{32} \times \frac{D}{32}\), and \(\frac{H}{64} \times \frac{W}{64} \times \frac{D}{64}\) tokens respectively, where $H$, $W$, $D$ represent height, width and depth of the input image. We propose 4 variants including naive, tiny, small and base models. The layer numbers for each stage are [1, 1, 1, 1] for nano, [2, 2, 2, 2] for naive, [2, 3, 3, 2] for tiny and [2, 3, 6, 2] for small models. Moreover, each stage begins with patch embedding where a 3D max-pooling with a kernel size of 3 and a stride of 2 is utilized to reduce the image dimension \cite{dai2021coatnet}. Following \cite{dosovitskiy2020image, liu2022convnet, yu2024inceptionnext}, the MLP ratio is set to 4. Following \cite{liu2021swin, liu2022convnet}, the kernel size for the decomposed convolution is set to 7. More details for the configuration details are shared in Table. \ref{config}. 


\subsubsection*{Developing a CLIP framework using \dc}
The CLIP framework aligns visual and contextual embeddings in a shared latent space through contrastive learning (Figure \ref{fig:clip}(a)). In this paper, we integrate our proposed DCFormer as the vision encoder in CLIP, serving as the primary module for efficient processing of 3D medical images. For the text encoder, we incorporate CXR-BERT \cite{boecking2022making}, following the implementation in Hamamci et al.\cite{hamamci2024foundation}. To compute similarities between images and radiology reports, we project visual and textual features into a shared 512-dimensional embedding space. Specifically, we apply global average pooling, followed by a linear projection to the output of the image encoder, while the text encoder output is transformed using a separate linear layer. Finally, visual and textual embeddings are aligned using a contrastive loss function \cite{radford2021learning}:
\begin{align}
    \mathcal{L}_{\text{CLIP}} = - \frac{1}{B} \sum_{i=1}^B \left[ \log \frac{\exp\left(\text{sim}(z_i^t, z_i^v)/\tau\right)}{\sum_{j=1}^B \exp\left(\text{sim}(z_i^t, z_j^v)/\tau\right)} + \log \frac{\exp\left(\text{sim}(z_i^v, z_i^t)/\tau\right)}{\sum_{j=1}^B \exp\left(\text{sim}(z_i^v, z_j^t)/\tau\right)} \right]
\end{align}
where $\text{sim}(z_i^t, z_j^v)$ represents the cosine similarity between the text embedding $z_i^t$ for the $i-$th radiology report and the visual embedding $z_j^v$ for the $j-th$ 3D image, $\tau$ is the temperature parameter (set to 1), and $B$ is the number of text-image pairs in a training batch. 

Unlike CT-CLIP \cite{hamamci2024foundation}, which lacks explicit multi-scale feature extraction, our DCFormer-based CLIP framework introduces a hierarchical structure that captures multi-scale features across different spatial resolutions. This design enhances representation learning and improves the alignment between image and text embeddings by preserving both global context and fine-grained details—crucial for accurate 3D CT image interpretation. Additionally, integrating DCFormer helps address the computational challenges associated with 3D CT imaging. By utilizing decomposed convolutions, our framework effectively reduces both the parameter count and computational overhead while maintaining robust feature extraction capabilities. As a result, the DCFormer-based CLIP framework is well-suited for large-scale 3D medical imaging applications.

\input{figures/clip}

% [old version] The CLIP framework operates by aligning the embeddings of text and image modalities in a shared latent space, enabling cross-modal tasks such as zero-shot classification and retrieval (Figure. \ref{fig:clip}). In this article, we integrate our proposed DCFormer as the vision encoder into the CLIP framework which acts as the primary module for processing 3D medical images efficiently. As for the text encoder, we integrate CXR-Bert\cite{boecking2022making} by following the implementation of Hamamci et al. \cite{hamamci2024foundation}. To compute the similarities between embedded images \( E_{\text{img}} \in \mathbb{R}^{C_{\text{embed}}} \) and radiology reports \( E_{\text{txt}} \in \mathbb{R}^{C_{\text{embed}}} \), we apply global average pooling and a linear layer with 512 dimensions. The CXR-Bert text encoder tokens which are in 768 dimensions, are linearly transformed into a 512 layer. Finally these two embedding spaces are aligned using a contrastive loss function \cite{radford2021learning}:
% \begin{align}
%     \mathcal{L}_{\text{CLIP}} = - \frac{1}{N} \sum_{i=1}^N \left[ \log \frac{\exp\left(\text{sim}(z_i^t, z_i^v)/\tau\right)}{\sum_{j=1}^N \exp\left(\text{sim}(z_i^t, z_j^v)/\tau\right)} + \log \frac{\exp\left(\text{sim}(z_i^v, z_i^t)/\tau\right)}{\sum_{j=1}^N \exp\left(\text{sim}(z_i^v, z_j^t)/\tau\right)} \right]
% \end{align}
% where $\text{sim}(z_i^t, z_j^v)$ represents the cosine similarity between the text embedding $z_i^t$ and the visual embedding $z_j^v$, $\tau$ is the temperature parameter which is set to 1, and $N$ is the number of text-image pairs. It should be noted that to further enhance the effectiveness of the CLIP framework for 3D CT images, we leverage DCFormer’s decomposed convolutions to handle the large input resolution and volumetric nature of CT images while maintaining computational efficiency. Different from CT-CLIP \cite{hamamci2024foundation}, DCFormer-based CLIP framework is composed of a hierarchical structure to effectively capture multi-scale features across different spatial resolutions, enabling improved representation learning and better alignment between image and text embeddings. Such a structure enables the model to focus on both global context and fine-grained details, which is critical for accurately interpreting 3D CT images. Unlike CT-CLIP \cite{hamamci2024foundation}, which does not explicitly incorporate multi-scale feature extraction, our proposed DCFormer-based CLIP framework processes input images through a hierarchical structure which ensures that features from different scales are represented in the learned embedding space. Furthermore, integrating the DCFormer architecture helps tackle the significant computational challenges associated with 3D CT images as it utilizes decomposed convolutions to extract features which effectively reduces the number of parameters and computational overhead.

\subsubsection*{Zero-shot multi-abnormality detection with DCFormer}
For zero-shot multi-abnormality detection, we follow the same approach as CheXzero\cite{tiu2022expert} and CT-CLIP\cite{hamamci2024foundation}. Specifically, two similarity scores are computed using the cosine similarity between the embedding of the CT image of interest and the embeddings of a positive ('\{\textit{Pathology}\} is present.') prompt and a negative ('\{\textit{Pathology}\} is not present.') prompt. 
A softmax function is then applied to these similarity scores to estimate the likelihood of each abnormality being present in the given CT image (see Figure. \ref{fig:clip} (b) for illustration). It should be noted that the zero-shot performance can potentially be improved by experimenting with different prompt variations. For instance, possible prompt templates including: 'Signs of \{\textit{Pathology}\} are/are not detected in the image.', 'There is/is not \{\textit{Pathology}\}.', '\{\textit{Pathology}\} is/is not observed.' 'Indication/No indication of \{\textit{Pathology}\}.'.  Although prompt engineering plays a critical role in improving image-text alignment, we adopt the prompt structure proposed by Hamamci et al. \cite{hamamci2024foundation} ('\{ \textit{Pathology}\} is/is not present.') to maintain consistency with prior work and ensure a fair comparison.

% [old version] For zero-shot multi-abnormality detection, we follow the same practice as CheXzero\cite{tiu2022expert} and CT-CLIP\cite{hamamci2024foundation}. That is, the output scores are extracted using the CT image of interest as well as both positive: '\{\textit{Pathology}\} is present.' and negative: '\{\textit{Pathology}\} is not present.' prompts. Then, a softmax function is applied to these scores to achieve the likelihood of each abnormality being present in the given CT image (see Figure. \ref{fig:clip}b for the illustration). It should be noted that it is possible to enhance the zero-shot performance by experimenting with different prompts associated with medical imaging domain. For instance, 'Signs of \{\textit{Pathology}\} are/are not detected in the image.', 'There is/is not \{\textit{Pathology}\}.', '\{\textit{Pathology}\} is/is not observed.' 'Indication/No indication of \{\textit{Pathology}\}.' etc., can be some prompt templates. Although such prompt engineering plays a critical role in determining how effectively the model aligns text and image embeddings,  we strictly utilize the prompt structure proposed by Hamamci et al. \cite{hamamci2024foundation} ('\{ \textit{Pathology}\} is/is not present.') to maintain consistency with prior work and enable a fair comparison.


%\subsubsection*{Fine-tuning DCFormer for multi-abnormality detection}
%To further enhance the multi-abnormality detection performance of DCFormer, we fine-tune the DCFormer-based image encoder using a supervised learning approach (see Figure. \ref{fig:clip}(c)). Specifically, fine-tuning is performed in two  modes: (1) Frozen mode, where the pre-trained encoder weights remain unchanged, and only a task-specific classification layer is trained; and (2) Trainable mode, where the image encoder weights are updated alongside the task-specific classification layer, allowing the image encoder to adapt to the multi-abnormality detection task. The supervised model is trained to minimize the Binary Cross-Entropy (BCE) loss for each abnormality.

% [Old Version] To further improve the multi-abnormality detection performance of DCFormer, we fine-tune the DCFormer-based image encoder using a supervised learning approach (see Figure. \ref{fig:clip}c). Specifically, we perform the fine-tuning process in two different modes: (1) with the image encoder weights frozen and (2) with the image encoder weights trainable. In the frozen mode, the pre-trained weights of the encoder remain unchanged and only a task-specific additional classification layer is trained to perform the classification task. In the trainable mode, the image encoder weights are updated alongside the task-specific classification layer, enabling the model to adapt more closely to the multi-abnormality detection task. Then, the supervised model is trained to minimize the Binary Cross-Entropy (BCE) loss for each abnormality.

\subsubsection*{State-of-the-art image encoders}
The pioneering work ViT \cite{dosovitskiy2020image} introduced the concept of processing images as sequences of patches, leveraging the self-attention mechanism for global feature extraction. However, its quadratic complexity imposes a significant computational burden, particularly for 3D medical images. TransUNet \cite{chen2021transunet} was the first model to incorporate ViTs into medical image segmentation, employing a hybrid architecture that combines CNNs with transformers. ConvNeXt \cite{liu2022convnet} demonstrated that 7×7 depthwise convolutions can serve as an effective token mixer, improving performance over self-attention while maintaining computational efficiency. PoolFormer \cite{yu2022metaformer} introduced a more efficient feature extraction mechanism than ConvNeXt, replacing self-attention with a simple average pooling operation to minimize computational overhead while preserving strong performance.

% [old version] The pioneering work ViT introduced the concept of processing images as sequences of patches which then leverages the self-attention mechanism for global feature extraction. However, especially for 3D medical images, its quadratic complexity brings extensive computational burden. Similarly, TransUNet is the first model proposed to use ViTs in medical imaging tasks. It enjoys a hybrid model that combines CNNs with transformers. ConvNeXt shows that 7x7 depthwise convolutions as the token mixer can improve the performance over self-attention while retaining the computational efficiency. PoolFormer introduces efficient implementation for feature extraction compared to ConvNeXt, where it employs simple average pooling operation to minimize computational overhead while aiming to maintain good performance. 


\subsubsection*{Datasets}

For model training and evaluation, we used the open-source CT-RATE dataset \cite{hamamci2024foundation}, which consists of 50,188 reconstructed CT volumes from 25,692 distinct CT experiments involving 21,304 patients. Each CT volume is paired with a radiology report. The dataset also includes 18 distinct abnormalities extracted from medical reports of each CT scan (findings and impressions). Table \ref{ctrate} provides an in-depth overview of CT-RATE, detailing the distribution of abnormalities across training and validation subsets. Each abnormality is associated with the number of samples in both sets, along with their respective ratios within the dataset. These ratios represent the proportion of samples for each abnormality relative to the total dataset. Notably, the ratios for the training and validation sets remain nearly identical across all abnormalities, ensuring that the validation set accurately reflects the training distribution, making it reliable for evaluating model performance.

% [old version] As for our first experimental dataset, we use CT-RATE \cite{hamamci2024foundation} which consists of 50,188 reconstructed CT volumes from 25,692 distinct CT experiments from 21,304 different patients. For zero-shot and fine-tuning purposes, the dataset consists of 18 distinct abnormalities extracted from medical reports of each CT scan (i.e, findings and impressions) Table \ref{ctrate} provides an in-depth view of the CT-RATE dataset, focusing on the distribution of various abnormalities across training and validation datasets. Each abnormality is associated with the number of samples in both sets, as well as their respective ratios within the dataset. These ratios show the proportion of samples for a given abnormality relative to the total samples in each subset. Note that the ratios for the training and validation sets are nearly identical across all abnormalities, which ensures that the validation set adequately reflects the distribution of the training set, making it reliable for evaluating model performance.


%\subsubsection*{RAD-ChestCT}

%\subsubsection*{One more dataset}

\subsubsection*{Implementation details}

For model training, we follow the same data splitting and pre-processing strategy as in \cite{hamamci2024foundation}. Specifically, we use 20,000 patients for training and 1,304 for validation. For pre-processing, we first resize each CT volume to a spacing of 0.75 mm on the x-axis and y-axis and 1.5 mm on the z-axis. Then, the volumes are center-cropped or padded to a fixed size of 512 x 512 x 256. Finally, we clip the Hounsfield Unit (HU) values of each CT image volume to [-1000,1000] and normalize them to [-1,1].  We train models using the AdamW optimizer \cite{kingma2014adam} with an initial learning rate $1^{-5}$. We dot not apply learning rate scheduling or warmup, as we have not observed any significant improvements. All models are trained for 15 epochs.

% [old version] As for the training procedure, we follow the same splitting and pre-processing strategy as Hamamci et al.\cite{hamamci2024foundation}. More specifically, we use 20,000 patients for training and 1304 for validation. For pre-processing operations, we first resize each volume to achieve a uniform spacing of 0.75 mm on the x-axis and y-axis and 1.5 mm on the z-axis. Then, the volumes are center-cropped or padded to achieve a constant resolution of 512 x 512 x 256. Finally, we convert each CT volume to Hounsfield Unit (HU) values using intercept and slope. HU values are clipped to a range of -1000 to 1000 and normalized to have values between -1 and 1 during training. For all experiments, we use the input size 512 x 512 x 256. We use AdamW \cite{kingma2014adam} with an initial learning rate $1^{-5}$ as the optimizer. We did not apply any learning rate scheduling or warmup as we have not seen any significant improvements. All models are trained for 15 epochs.
\input{tables/ctrate}
