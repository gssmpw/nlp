\section{Related Works}
\subsection{Alignment}
% There are many existing studies in LLM alignment. 
% We summarize some common literature about alignment in empirical and theoretical perspectives.
\paragraph{Empirical Studies} Various methods have been proposed to align LLMs with human preferences. For instance, RLHF with the BT and PL models was first introduced in \cite{ziegler2019fine} and \cite{ouyang2022training}, respectively. In RLHF, a reward model is trained and is further used in the alignment of the LLM. In contrast, DPO \cite{rafailov2024direct} designs its loss function (training objective) to avoid the need for a separate reward model. Later, BCO \cite{jung2024binary} and KTO \cite{ethayarajh2024kto} were proposed to further enhance alignment performance. In addition to the alignment methods mentioned above, several others have been developed to enhance performance in various ways. For example, ORPO \cite{hong2024orpo} incorporates the SFT loss into DPO, and \cite{yuan2024advancing} uses a preference tree. Other techniques can be found in \cite{xiong2024building,amini2024direct,lu2024online,wang2024bpo,zhou2024t,zhang2024self, franken2024self,yin2024entropy}.

% BPO \cite{wang2024bpo} enhances alignment by leveraging knowledge breadth and depth, while token-level regularization is explored in \cite{zhou2024t}. Additionally, self-exploring and self-supervised methods \cite{zhang2024self, franken2024self}, as well as sample-efficient algorithms \cite{yin2024entropy, xie2024exploratory}, have been proposed.

% Other studies have conducted comprehensive experiments to better understand alignment. For example, \cite{anwar2024foundational} identifies practical challenges in alignment, while \cite{rafailov2024scaling} explores scaling laws. \cite{li2024predicting} discusses the discrepancies between LLMs and LLM-based agents in the context of RLHF, and \cite{ivison2024unpacking} investigates various factors that impact alignment performance.

% \begin{itemize}
%     \item DPO \cite{rafailov2024direct}, PPO \cite{schulman2017proximal}\yue{please help check this reference}, BCO \cite{jung2024binary}, KTO \cite{ethayarajh2024kto},  RLHF \cite{ziegler2019fine,ouyang2022training}.  RLHF using the BT and PL models was first introduced in \cite{ziegler2019fine} and \cite{ouyang2022training} respectively. 
%     \item Enhancements of existing alignment methods: DPO with offset \cite{amini2024direct}, ORPO \cite{hong2024orpo}, preference tree \cite{yuan2024advancing}, multi-turn \cite{xiong2024building}, merging \cite{lu2024online}, BPO leveraging knowledge breath and depth \cite{wang2024bpo}, token-level regularization \cite{zhou2024t}, self-exploring and self-supervised method,\cite{zhang2024self,franken2024self}, sample-efficient algorithms \cite{yin2024entropy,xie2024exploratory}, 
%     \item Works in identifying practical challenges in alignment: \cite{anwar2024foundational}
%     \item Scaling law: \cite{rafailov2024scaling}
%     \item \cite{li2024predicting}: discuss about the discrepancy between LLMs and LLM-based agents when considering RLHF.
%     \item \cite{ivison2024unpacking} explored different factors which impact alignment performance.
% \end{itemize}
\vspace{-0.1in}
\paragraph{Theoretical Investigations} Beside the empirical studies, some other works focus on the theoretical properties of alignment and develop new algorithms based on their analysis. For example, \cite{xiao2024algorithmic} addresses preference bias in RLHF through preference matching. \cite{he2024accelerated} accelerates convergence by applying momentum, and \cite{liu2024dual} proposes an algorithm that uses active learning to select the appropriate human for RLHF. 
% Additionally, \cite{} explores RLHF in offline, online, and hybrid settings.
Other studies can be found in \cite{wang2024magnetic,xiong2024iterative,wang2023rlhf,du2024exploration}.
% and \cite{wang2023rlhf} compares RLHF with traditional reinforcement learning. 
% From a topological perspective, \cite{qiu2024reward} delves into RLHF, while \cite{du2024exploration} focuses on improving sample efficiency in RLHF.
Different from existing literature, we have a emphasis on the separation effect between aligned and unaligned data. 
% This focus leads to a key difference in our theory: previous literature offers limited understanding of how to connect alignment loss with divergence metrics, and we develop a concept of alignment consistency to connect them.


\subsection{Jailbreak Attack}
Aligned LLMs, despite their intended safety measures, can still produce harmful content, as highlighted in studies like \cite{zhou2023synthetic}, \cite{hazell2023spear}, and \cite{kang2024exploiting}. Jailbreak attacks, which exploit vulnerabilities in these models, have been explored in \cite{wei2024jailbroken} and \cite{carlini2024aligned}. To design effective jailbreak attacks, several methods have been proposed, including GCG \cite{zou2023universal}, AutoDAN \cite{liu2023autodan}, PAIR \cite{chao2023jailbreaking}, and TAP \cite{mehrotra2023tree}. 
% In the real data experiments, we will utilize jailbreak attacks to examin the robustenss of the aligned LLMs.

% Although our theoretical framework connects divergence metrics with alignment methods, we will use jailbreak attacks to numerically relate divergence to the robustness of aligned LLMs. We conduct these experiments because the robustness, rather than divergence, is the ultimate goal of safety alignment.

% \subsection{Related Works}
% \subsection{Contributions}
% 1. Unifying framework for safety alignment in terms of divergence optimization, and explaining clustering phenomenon absed on saafety of the prompt. \\
% 2. Motivated by this framework we introduce KLDO, optimizes to maximize divergence between unaligned and aligned distribution, in effort to better separation/alignment\\
% 3. We introduce a separation metric based on the distance between the distributions that acts as a statistically significant indicator of the robustness of the model.\\
% 4. We advocate the use of Compliance Refusal datasets instead of preference datasets for safety alignment, in line with our theory, supported by experiments.