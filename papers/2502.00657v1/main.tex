\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{Safety alignment and latent representation}
\author{Rajdeep Haldar}
\date{October 2024}
\input{MacrosandSettings}


\newcommand{\yue}[1]{\textcolor{purple}{Yue: #1}}
\newcommand{\raj}[1]{\textcolor{magenta}{Raj: #1}}

\begin{document}

\maketitle

\section{Observations}
\begin{enumerate}
    \item \cite{Lin2024TowardsAnalysis} showed that the prompt space is separated into safe and harmful clusters, after alignment. Jailbreak attacks like AutoDAN \citep{Liu2023AutoDAN:Models}, GCG\citep{Zou2023UniversalModels}, PAIRs \citep{Chao2023JailbreakingQueries} translate the prompts from safe to harmful cluster somewhere in between.
    \item The clusters provide insufficient coverage, suggesting some bias or partially learnt distribution.
    \item When paraphrasing jailbreak attacks return to their original clusters.
    \item Given prompt $x$, if we compute initial response score or confidence $\log \pi(y^{1:32}|x)$, then there is a separation between failed vs successful jailbreak prompts. Lower confidence for successful jailbreak prompts. Very high confidence for original anchor prompts. This separation is seen only for aligned model.
    \item If we do sampling instead of greedy search some safety responses can be recovered even after jailbreak. \yue{[? I could not understand this? What is sampling?]} \raj{When generating a text we can just choose the highest probability of the next token (greedy sample), we can instead consider cumulative highest probabilities using beam search, or use some sampling (multinomial, top k, top p etc) from the given distribution.}
\end{enumerate}
\section{Framework}
\subsection{RLHF Model}
RLHF has two major steps reward modelling and policy maximization.
\begin{enumerate}
    \item (BTL or PL) model, log-likelihood maximization over preference.
    \begin{align}
        P(y_i>y_j)&=\frac{e^{\kappa_i}}{e^{\kappa_i}+e^{\kappa_j}}\\
        P(y_i>y_{j_1},\dots,y_{j_{n-1}})&=\frac{e^{\kappa_i}}{e^{\kappa_i}+\sum_{k=1}^{n-1}e^{\kappa_{j_k}}}
    \end{align}
    \item $\kappa_i\to r_\phi(x,y_i)$.
    \begin{align}
    \arg\max\limits_{\phi}\E\limits_{\sim x, y_w, y_l}\log\sigma(r_\phi(x,y_w)-r_\phi(x,y_l))\\
    \arg\max\limits_{\phi}\E\limits_{\sim x, y_w, \{y_{l_k}\}_{k=1}^{n-1}}\log P(y_i>y_{l_k},\forall k)
    \end{align}
    \item \textbf{Policy optimization} : given reference policy $\pref$, optimize $\pi_{\theta}$ on the following objective.
    \begin{equation}
        \arg\max\limits_{\theta}\E\limits_{\sim x,y} r_\phi(x,y) -\beta\infdiv{\pi_\theta (y|x)}{\pref(y|x)}
    \end{equation}
    \item Optimal policy is given by:
    \begin{equation}
\pi_\theta(y|x)=\frac{\pref(y|x)\exp(\beta^{-1}r_\phi(x,y))}{Z(x)}
    \end{equation}
    $Z(x)=\sum_y\pref(y|x)\exp(\beta^{-1}r_\phi(x,y))$
    .This induces an equivalence class on $r_\phi(x,y)\approx_{\log Z(x)}=\beta\log\frac{\pi_\theta(y|x)}{\pref(y|x)}$.
    \item DPO loss, theoretically same optimal solution as RLHF:
    \begin{align}
         \arg\max\limits_{\theta}\E\limits_{\sim x, y_w, y_l}\log\sigma\left(\beta\log\frac{\pi_\theta(y_w|x)}{\pref(y_w|x)}-\beta\log\frac{\pi_\theta(y_l|x)}{\pref(y_l|x)}\right)
    \end{align}
    Notice that when $\beta=0$, we get the optimization independent of $\theta$, which aligns with our prior arguments, of any distribution with support on maximal reward works.\yue{Everything becomes 0...?} \raj{Yes, optimization is a constant independent of $\theta$} \\
    For the multiple negative responses:
    \begin{equation}
\arg\max\limits_{\theta}\E\limits_{\sim x, y_w, \{y_{l_k}\}_{k=1}^{n-1}}\log \frac{\exp r_\phi(x,y_w)}{\exp r_\phi(x,y_w)+\sum\exp r_\phi(x,y_{l_k})}
    \end{equation}
\end{enumerate}
\subsection{Contrastive loss relation}
$\infdiv{\mathbb{P}}{\mathbb{Q}}=\E_\mathbb{P}\log\frac{P(x,y)}{Q(x,y)}$. When $\mathbb{P}=P_{XY},\mathbb{Q}=P_X\otimes P_Y$ this is mutual information $I(X,Y)$.\\
In generalized contrastive loss given $x_g,x_g'$ some encoded features (using some encoding function $g:\mathcal{X}\to \mathcal{S}^{d-1}$), we have cosine similarity function $s(x,x')=cosim(x_g,x_g')$ and temperature $\tau$ such that we want to maximize the objective \citep{Chen2020IntriguingLosses}:
\begin{equation}
    \cL_{CL}=-n^{-1}\sum_{i,j\in \mathcal{MB}}\log \frac{\exp s(x_i,x_j)/\tau}{\sum_{k\neq i}\exp s(x_i,x_k)/\tau}
\end{equation}
In general we could extend to any similarity function $s(x,x')$, doesn't have to be cosine sim.
\cite{Chen2020IntriguingLosses} promotes the decomposition into \emph{alignment} and \emph{distribution}.
\begin{equation}
    \cL_{CL}=\underbrace{-n^{-1}\sum_{i,j}s(x_i,x_j)/\tau}_{\cL_{align}} + \underbrace{n^{-1}\sum_i\log \sum_{k\neq i}\exp s(x_i,x_k)/\tau}_{\cL_{dist}}
\end{equation}
$\cL_{dist}$ encourages distribution to match a prior distribution of high-entropy. \cite{Wang2020UnderstandingHypersphere} shows in case of $s(x,x')=f(x)^Tf(x')$ , $\cL_{dist}$ encourages uniform distribution over hypersphere.
\begin{remark}
\begin{itemize}
    \item  It is tempting to view $r_\phi(x,y)$, reward in the RLHF framework as a similarity function in the contrastive regime.  
    \item In fact using using DPO equivalence, this similarity is essentially dictated by conditional probability of response given prompt $s(x,y)=r_\phi(x,y)=\beta\log\frac{\pi_\theta(y|x)}{\pref(y|x)}$.
    \item If we use the reward $s(x,y)=r_\phi(x,y)=f(x)^Tf(y)$, this reduces to the contrastive analysis in uniform hypershpere shown by \cite{Wang2020UnderstandingHypersphere}. This is the exactly the case of how I initially defined reward as alignment as a dot product of safety label $r_\phi(x,y)=z_x\cdot z_y$. 
\end{itemize}
   
\end{remark}
\begin{enumerate}
    \item There has been some notion of contrast discussed in preferential models like BTL/PL. \cite{Brookes2023ContrastiveEpistasis,Chen2018ContrastiveMeasurements}  
    \item The infoNCE loss \citep{vandenOordDeepMind2018RepresentationCoding} or in general $\cL_{CL}$ acts as a lower bound to mutual information $I(X,Y)$. However, we require the negative samples to come from independent distribution. Let $r_\phi(x,y)=s(x,y)$ then the infoNCE loss is:
    \begin{align}
    \intertext{Replacing empirical sum with expectation and using similarity as reward}
        \cL_{CL}&=-\E\limits_{(x,y)}\log \frac{\exp r_\phi(x,y)}{\exp r_\phi(x,y)+\sum_{y_{-}}\exp r_\phi(x,y')}\\
        \intertext{InfoNCE will be optimal when $\exp r_\phi(x,y)=p(y|x)/p(y)$}
        &=\E\limits_{(x,y)}\log \left(1+p(y)/p(y|x)\sum_{y_{-}}p(y'|x)/p(y')\right)\\
        \intertext{If negative samples are explicitly independent $y'\perp x$ then}
        &=\E\limits_{(x,y)}\log \left(1+p(y)/p(y|x)(n-1)\right)\geq \E\limits_{(x,y)}\log p(y|x)/p(y)+\log(n-1)\\
        &= -I(X,Y)+\log(n-1)
    \end{align}
    \yue{(12) to (13) seems is approximate when $n$ is large enough.}
    For BTL model $n=2$.
    \item In general $\infdiv{\mathbb{P}}{\mathbb{Q}}$ has an equivalence known as Donsker-Varadhan representation. Where $\infdiv{\mathbb{P}}{\mathbb{Q}}=\sup\limits_{T:\Omega\to\reals}\bE_\mathbb{P}[T]-\log \bE_\mathbb{Q}[e^T]$. \\
    This representation is basis for MINE estimator \citep{Belghazi2018MINE:Estimation}, where $T$ can be a neural network or any other parametrization. With $\mathbb{P}=P_{XY},\mathbb{Q}=P_X\otimes P_Y$ , the divergence is mutual information $I(X,Y)$. It is not hard to see that infoNCE and MINE are related. Negative samples (if from product marginal distribution) control \emph{distribution/uniformity} part is and appear in the log expectation part. Positive samples control the alignment.
    \item \cite{Lu2024f-MICL:Learning} generalizes the contrastive loss to any f-divergence and shows KL, infoNCE etc are special cases.  $\fdiv{P_{XY}}{P_X\otimes P_Y}=\sup\limits_{T:\Omega\to\reals}\bE_{P_{XY}}[T]-\bE_{P_X\otimes P_Y}f^*[T]$
    \item In RLHF we are maximizing the lower bound of the following:
    \begin{equation}
        \infdiv{\mathbb{P}}{\mathbb{Q}}=\sup\limits_{\theta}\bE_\mathbb{P}[\beta\log\frac{\pi_\theta(y|x)}{\pref(y|x)}]-\log \bE_\mathbb{Q}[\frac{\pi_\theta(y|x)}{\pref(y|x)}]^\beta
        \label{eqn: KL_loss_dpo}
    \end{equation}
    This maximizes mutual information between $X,Y$ given $\mathbb{Q}$ is indeed $P_X\otimes P_Y$. Notice that even in this case the standard BTL, has only one negative sample per $x$ to approximate the expectation.
    \item In the case when we ensure that $\mathbb{Q}=P_{XY_-}$ the negative samples are unaligned, then we no longer have maximize $I(X,Y)$, instead the divergence between aligned and unaligned samples $\infdiv{P_{XY_+}}{P_{XY_-}}=\bE_x\infdiv{Y_+|X}{Y_-|X}$.\\
    Using Jensen's inequality $-\log \bE_\mathbb{Q}[e^T]\leq -\bE_\mathbb{Q}[T]$.
    \begin{align}
        \infdiv{\mathbb{P}}{\mathbb{Q}}&=\sup\limits_{T:\Omega\to\reals}\bE_\mathbb{P}[T]-\log \bE_\mathbb{Q}[e^T]\\
        &\leq \sup\limits_{T:\Omega\to\reals}\bE_\mathbb{P}[T]-\bE_\mathbb{Q}[T]\label{eqn:KL_loss_kto}\\
        &=\sup\limits_{T:\Omega\to\reals}\bE_\mathbb{P}[T]-\bE_\mathbb{Q}[T]\pm \log \bE_\mathbb{U}[e^T]\\
        &=\infdiv{\mathbb{P}}{\mathbb{U}}-\infdiv{\mathbb{Q}}{\mathbb{U}}
    \end{align}
    We can choose $\mathbb{U}=P_{X}\otimes P_{Y},\mathbb{P}=P_{XY_+},\mathbb{Q}=P_{XY_-}$. RLHF can be seen as difference of two divergences in this case. 
    \begin{remark}
        KTO \cite{Ethayarajh2024KTO:Optimization} seems to maximize the difference in divergence representation and implicitly $\bE_x\infdiv{Y_+|X}{Y_-|X}$, for a suitable $T$ dependent on $\frac{\pi_\theta(y|x)}{\pref(y|x)}$
    \end{remark}
    
\end{enumerate}
\subsection{Optimal solution}
\textbf{DPO/PPO}
Eqn \ref{eqn: KL_loss_dpo} gives version KL based rlhf loss when $T=\beta\log\nicefrac{\pi_\theta(y|x)}{\pref(y|x)}$. But we know the optimal solution of $T$ that attains the supremum is $$T^*=\log \nicefrac{p_{real}(y^+|x)}{p_{real}(y_-|x)}+\log \underbrace{\E_\mathbb{Q}\exp T^*}_{constant}$$\cite{Park2021DeepRepresentation}. 
\begin{align}
    \beta\log\nicefrac{\pi_{\theta^*}(y|x)}{\pref(y|x)}= \log c\cdot\nicefrac{p_{real}(y^+|x)}{p_{real}(y_-|x)}\\
    \pi_{\theta^*}(y|x)=\pref(y|x)\cdot(\nicefrac{p_{real}(y^+|x)}{p_{real}(y_-|x)})^{\beta^{-1}}\cdot c'
    \label{eqn:optimal_dpo}
\end{align}
\textbf{KTO} Eqn \ref{eqn:KL_loss_kto} gives us a KL lower bound in terms of KTO loss where $T=\sigma(\beta\log\sfrac{\pi_\theta(y|x)}{\pref(y|x)}-z_0)$. Using similar proof based on gibbs distribution argument, we can show that the optimal $T^*$ is as follows:
$$T^*=\log \nicefrac{p_{real}(y^+|x)}{p_{real}(y_-|x)}+ \underbrace{\E_\mathbb{Q}T^*}_{constant}$$
Hence, the optimal policy will be:
\begin{align}
    \sigma(\beta\log\sfrac{\pi_\theta(y|x)}{\pref(y|x)}-z_0)=\log c\cdot\nicefrac{p_{real}(y^+|x)}{p_{real}(y_-|x)}\\
    \intertext{Simplifying}
    \pi_{\theta^*}(y|x)\propto\pref(y|x) \cdot e^{\sfrac{z_0}{\beta}}\cdot\left(\frac{1}{1-\log \nicefrac{p_{real}(y_+|x)}{p_{real}(y_-|x)}} -1\right)^{\beta^{-1}}
    \label{eqn:optimal_kto}
\end{align}
\subsection{Segregation of Harmful/Harmless space}
Every prompt $x\in \cx$ belongs to either harmless or harmful cluster characterized by the latent variable $z=0,1$.\\ In terms of response we can either accept a question to answer or reject/refuse to answer. Both of these cases induce complementary response distribution given a question, $P_{real}(y|x,\text{Accept to answer})=1-P_{real}(y|x,\text{refuse to answer})$.\textcolor{red}{(Not sure whether this assumption is needed)}\\
Now for harmful questions aligned/positive responses are essentially samples from refusal distribution, and for harmless question aligned/positive response are from acceptance distribution. Vice versa for unaligned responses. Hence, 
\begin{align}
    p_{real}(y_{+}|x,z)=p_{A}(y|x)^{1-z}\cdot p_{R}(y|x)^{z}\\
    p_{real}(y_{-}|x,z)=p_{A}(y|x)^{z}\cdot p_{R}(y|x)^{1-z}
\end{align}
In eqn \ref{eqn:optimal_dpo}, \ref{eqn:optimal_kto}  substituting $\nicefrac{p_{real}(y_+|x)}{p_{real}(y_-|x)}=p_{A}(y|x)^{1-2z}\cdot p_{R}(y|x)^{2z-1}$ we can see that based on value of $z$, $\pi_\theta$ will favor either acceptance or rejection probability of the responses.
Let's think of the following model:
\begin{equation}
x\to z \to y|x,z = \left\{
\begin{array}{ll}
      P_A (y|x) &, z=0 \\
      P_R(y|x) &, z=1
\end{array} 
\right.
    \label{eqn:generation_model}
\end{equation}
In case of training $x\leftrightarrow z$, given the question we know whether it is harmful or not.  But suppose we want to see given an uninformative prior to the safety $\pi(z|x)\sim p_0(x),p_1(x) $ what would be the estimated probability of the safety given the question, response pair, at optimality?
\begin{align}
    P(z=0|x,y)&=\frac{\pi_{\theta^*}(y|x,z=0)\cdot p_0(x)}{\pi_{\theta^*}(y|x,z=0)\cdot p_0(x)+\pi_{\theta^*}(y|x,z=1)\cdot p_1(x)}\\
    &=\left(1+g(y|x)^{\sfrac{2}{\beta}}\cdot \sfrac{p_1}{p_0}\right)^{-1}
    \intertext{Where $g(y|x)=\sfrac{P_A(y|x)}{P_R(y|x)}, 1-\frac{1}{\log \nicefrac{P_A(y|x)}{P_R(y|x)}}$ for DPO/PPO and KTO resp}
\end{align}
At optimality we can build a naive's bayes classifer for safety label using the LLM distribution.

\section{Experiments}
For latest tables refer to the google sheet \href{https://docs.google.com/spreadsheets/d/1haVI6s-2i3GIRRVsFl85PRDXd-qo14iMigE8cUOyd_k/edit?usp=sharing}{tables}
\subsection{Separation}
\input{tabs/separation_table_train}
\subsection{Alpaca Eval}

\input{tabs/alpacaeval}
\subsection{Attack Success Rate}
\input{tabs/asr_adv_bench}

\subsection{Ablation Study}
Effect of the compliance-refusal type vs. preference type data set for safety alignment.
\bibliographystyle{apalike}
\bibliography{mendley}
\end{document}
