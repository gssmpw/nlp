%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

%\allowdisplaybreaks
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2025}
\usepackage{icml2025}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% custom settings added by user

\input{shorthands}
\newcommand{\yue}[1]{\textcolor{orange}{Yue: #1}}
% \newcommand{\todo}[1]{\textcolor{blue}{TODO: #1}}
\newcommand{\raj}[1]{\textcolor{magenta}{Raj: #1}}
\newcommand{\ZW}[1]{\textcolor{cyan}{ZW: #1}}

\usepackage{colortbl}
\usepackage[table]{xcolor}
\usepackage{color}
\definecolor{linecolor}{gray}{.895}

\allowdisplaybreaks

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{LLM Safety Alignment is Divergence Estimation in Disguise}

\begin{document}

\twocolumn[
\icmltitle{LLM Safety Alignment is Divergence Estimation in Disguise}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Rajdeep Haldar}{purdue}
\icmlauthor{Ziyi Wang}{purdue}
\icmlauthor{Qifan Song}{purdue}
\icmlauthor{Guang Lin}{purdue}
\icmlauthor{Yue Xing}{msu}

\end{icmlauthorlist}

\icmlaffiliation{purdue}{Department of Statistics, Purdue University}
\icmlaffiliation{msu}{Department of Statistics, Michigan State University}


\icmlcorrespondingauthor{Rajdeep Haldar}{rhaldar@purdue.edu}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
 ]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We propose a theoretical framework demonstrating that popular Large Language Model (LLM) alignment methods, including Reinforcement Learning from Human Feedback (RLHF) and alternatives, fundamentally function as divergence estimators between aligned (preferred or safe) and unaligned (less-preferred or harmful) distributions. This explains the separation phenomenon between safe and harmful prompts in the model hidden representation after alignment. 
Inspired by the theoretical results, we identify that some alignment methods are better than others in terms of separation and, introduce a new method, KLDO, and further demonstrate the implication of our theories. We advocate for compliance-refusal datasets over preference datasets to enhance safety alignment, supported by both theoretical reasoning and empirical evidence. Additionally, to quantify safety separation, we leverage a distance metric in the representation space and statistically validate its efficacy as a statistical significant indicator of LLM resilience against jailbreak attacks.
\end{abstract}

% \yue{Unify wording:
% \begin{itemize}
%     \item Safe/unsafe? Safe/harmful? Harmful/harmless?
% \end{itemize}}

\section{Introduction}

Large language models (LLMs) are powerful generative tools capable of understanding human language and performing specific tasks. After the pre-training and supervised fine-tuning stages, alignment methods are used to align the LLMs' outputs with human preferences or ethics. In the literature, methods such as reinforcement learning with human feedback (RLHF), direct preference optimization (DPO), and their variants serve as common approaches. %Building on these, new techniques have been proposed. For instance, ORPO \cite{hong2024orpo} integrates the loss from supervised fine-tuning (SFT) with preference loss to better regulate LLM behavior. Other methods focus on improving the sample efficiency of alignment, e.g., \cite{yin2024entropy,xie2024exploratory}.

Within alignment research, a significant focus is on safety alignment, which aims to ensure that LLMs avoid responding to malicious user inputs and generate only safe responses. Studies, such as \cite{lin2024towards,xu2024uncovering}, observe that for aligned LLMs, the hidden representations of harmful and safe input prompts fall in different clusters (Fig. \ref{fig: separation_motivation}). These observations point to a potential \emph{separation effect} caused by alignment. However, these studies focus on developing stronger attacks via moving the jailbreak attack closer to the cluster of safe prompts. 
% However, previous works have not analyzed this phenomenon from the perspective of alignment itself. Instead, they focus on jailbreak attacks, showing their effectiveness when attacked prompts overlap with safe prompt clusters, and examine only existing open-source aligned models without investigating the source of separation.

\begin{figure}[!ht]
    \centering
    \begin{subfigure}
    \centering
    \includegraphics[width=0.48\linewidth,trim=0 0 0 85, clip]{figs/base_Qwen2.5-1.5B_intro.png}
    \end{subfigure}
    \begin{subfigure}
        \centering \includegraphics[width=0.48\linewidth,trim=0 0 0 85, clip]{figs/Llama-2-7b-chat-hf_anchor.png}
    \end{subfigure}\vspace{-0.15in}
    \caption{Hidden representation space is clustered and separated by prompt safety in the aligned model (\emph{Right: LLama2-chat}) compared to the model without alignment (\emph{Left: Qwen2.5-Base}).}
    \label{fig: separation_motivation}
\end{figure}
%However, there is a gap in the aforementioned analysis. While the LLMs studied in \cite{lin2024towards,xu2024uncovering} employ different alignment methods, there is no comprehensive analysis on how the different alignment methods impact the separation effect. Addressing this gap is crucial for helping practitioners select appropriate alignment methods given their specific task and data. 
In this work, we explain and comprehensively analyze how standard alignment methods fundamentally induce separation and can be incorporated into a unified theoretical framework based on \emph{divergence estimation}. Our contributions are summarized as follows:
%In this work, we aim to analyze the loss functions of various alignment methods and explain the underlying causes of the separation effect and compare the differences among these methods. To achieve this, we develop a theoretical framework which unifies various alignment methods. Our main contributions and observations are summarized as follows:
% \yue{the storyline in Sec 1 and Sec 4 have some discrepancy for now. will discuss with raj on Monday. It is about the order of the four contributions.}
% \subsection{Contributions:}

First, we establish that commonly used alignment methods correspond to specific divergence metrics (Sec \ref{subsec: loss as estimators}): KTO \cite{ethayarajh2024kto} estimates total variation (TV) distance, BCO \cite{jung2024binary} estimates Jensen-Shannon (JS) divergence, and DPO or standard RLHF estimates a non-parametric divergence. Since divergences measure distributional separation, this acts as initial validation of alignment methods inducing separation.
% First, by developing the theoretical framework, we identify that many commonly used alignment methods inherently induce separation. We find that the loss functions in these methods are designed to maximize specific divergence metrics between aligned and unaligned samples. For instance, KTO \cite{ethayarajh2024kto} corresponds to total variation (TV) divergence, while BCO \cite{jung2024binary} aligns with Jensen-Shannon (JS) divergence. To formalize this relationship, we introduce the concept of alignment consistency, which connects these divergence metrics to the loss functions used in alignment.
%Second, observing the connections between alignment methods and divergence metrics, we further compare the different divergence metrics. Simple simulations reveal that DPO is less effective than BCO, as its corresponding divergence metric struggles to quantify differences when the two distributions are far away from each other. While DPO is widely used, it may underperform than BCO in specific tasks, e.g., safety alignment. For KTO, the discrete nature of total variation (TV) introduces potential numerical challenges during alignment. 

Second, inspired by the above finding together with the advancement of KL divergence as a divergence metric,
% the desirable properties of KL divergence as a divergence, 
we propose a new alignment method: KLDO based on KL-divergence estimation (Sec \ref{subsec: kldo}). Moreover, we introduce a formulation for a general class of alignment methods estimating $f$-divergences; we call them FDO (Sec \ref{subsec: fdo}).

%Given that KL divergence increases rapidly when distributions are far apart, KLDO outperforms DPO in safety alignment and performs comparably to BCO. We also extend from KL to the general f-divergence and its corresponding alignment formulation.
% Real-data experiments support this hypothesis, validating KLDO’s effectiveness.
% Simple simulations reveal that DPO is less effective than BCO, as its corresponding divergence metric struggles to quantify differences when the two distributions are far away from each other. While DPO is widely used, it may underperform than BCO in specific tasks, e.g., safety alignment. For KTO, the discrete nature of total variation (TV) introduces potential numerical challenges during alignment. We fine-tune LLMs and validate these findings through real-data experiments. The observations in the experiments are consistent with our theoretical understanding.
%Third, our theoretical framework allows us to link alignment methods to divergence metrics, enabling the development of new alignment methods based on any available metric. Beyond TV and JS, we consider the Kullback-Leibler (KL) divergence and propose a new alignment method, KLDO. Given that KL divergence increases rapidly when distributions are far apart, we hypothesize that KLDO outperforms DPO in safety alignment and performs comparably to BCO. Real-data experiments support this hypothesis, validating KLDO’s effectiveness.
%Fourth, while the above focuses on the divergence metrics and the divergence implicitly induces separation between aligned and unaligned data, we further fill the gap between the divergence metrics and the exact separation. In particular, we identify the key factor and name it as \textbf{alignment consistency}. We theoretically demonstrate that, if a policy if alignment consistent, then the separation happens. 

Third, we show that alignment methods acting as divergence estimators satisfy a property called \textbf{alignment consistency}, enabling aligned models to redistribute their probabilities based on the true likelihood of the response being favorable or not (Sec. \ref{sec:alignment_consistency}). Furthermore, if an alignment method satisfies consistency, we theoretically show that it induces separation, and the separation is amplified using compliance-refusal data instead of preference data (Sec. \ref{sec: separation}).

%Finally, using our theoretical framework, we further analyze how the distributions of aligned and unaligned data affect the separation. We find that compliance refusal datasets are more effective than preference datasets in inducing separation. 
% Real-data experiments also validate this finding.
Finally, we validate our findings via experiments (Sec \ref{sec: experiments}). All alignment methods used in our paper induce significant visual separation in hidden representation from non-aligned base models. 
% We propose a distance metric to quantify the separation and numerically validate the separation phenomenon. 
We also solidify the relation between separation and robustness by showing that the separation 
% distance metric 
is a significant indicator of robustness (Attack Success Rate) \hyperref[phantom: sep vs robustbess]{(Separation Vs Robustness)}.
% Among the alignment methods, KLDO often exhibits superior or competitive performance based on various metrics. 
Additionally, separation/robustness is improved when we use compliance-refusal (CR) instead of preference (Pref) data type (Tab: \ref{tab:compliance_preference_compare}). 
% We advocate the use of CR data type for safety alignment.
%In addition to the theoretical justifications, we also conduct comprehensive experiments to validate the above findings. \yue{Please fill in after updating the experiment section.}
% To summarize, this study develops a theoretical framework to analyze the separation effect induced by alignment methods. We connect divergence metrics to alignment methods and explore how the distributions of aligned and unaligned data influence the separation.
% A final note is that, although our paper mainly focuses on the safety alignment and separate harmful and safe prompts, the theoretical framework can be used for other tasks. 

% paragraph 2: Overview of current practice of alignment; general principle/objective in the alignment stage; 

% paragraph 3: observation of separation phenomenon in practice in harmfulness. \cite{lin2024towards,xu2024uncovering}. However, this line of research mainly focuses on the jailbreak perspective to better understand the vulnerability of LLMs and propose stronger jailbreak attacks. There is limited understanding in the alignment perspective.

% paragraph 4: Lack of universal theoretical understanding, hindering development of alignment approaches with theoretical guarantees. 

% paragraph 5: This work aims to understand the theoretical behavior of LLMs alignment from the perspective of model safety (i.e., the framework of safe correct refusal/compliance model response, and two types of data to be used for alignment). Our contributions are summarized as follows:

% paragraph 6: First, Summarizing from our theoretical derivations on xxx, xxx, xxx (different methods), we introduce the concept of alignment consistency which (what does it do). Informal def of alignment consistency and separation. Claim that this proposed framework \yue{not sure what is a "framework", did not mention it before} enables us to examine the behavior of many existing algorithms (state our main theoretical claims here) and to explain the clustering phenomenon. In particular, we use our framework and connect KTO and BCO with xxx diveregence metrics, which explain their success.  \yue{Do we want to mention that we developed a concept called ``alignment consistency"? If so, please also mention its advantage}

% paragraph 7: Second, Motivated by this framework, we propose two things: (1) while xxx divergence are implied in KTO and BCO, we use KL to develop KLDO. Since all the three methods optimizes to maximize divergence between unaligned and aligned distribution in some divergence metric, we conduct extensive numerical experiments to study their real performance. To evaluate the separation effect, (2) we further introduce a metric. State the empirical evidence that KLDO has good metrics values. 

% Note that our final goal is not to promote KLDO then other alignment methods. Our focus is the effectiveness of our framework, i.e., we use our proposed theoretical framework and develop a method, and the resulting method demonstrates a performance as good as KTO and BCO.

% paragraph 7.5: TBD, depending on how we justify DPO, we may need to revise some place to mention it in the introduction.

% paragraph 8: Finally, A byproduct of our theory further suggests the use of Compliance Refusal datasets instead of preference datasets for safety alignment tasks. We have empirical evidence. Calls for community to build new and more benchmark data for LLMs safety alignment.

\section{Related Works}

\subsection{Alignment}
% There are many existing studies in LLM alignment. 
% We summarize some common literature about alignment in empirical and theoretical perspectives.
\paragraph{Empirical Studies} Various methods have been proposed to align LLMs with human preferences. For instance, RLHF with the BT and PL models was first introduced in \cite{ziegler2019fine} and \cite{ouyang2022training}, respectively. In RLHF, a reward model is trained and is further used in the alignment of the LLM. In contrast, DPO \cite{rafailov2024direct} designs its loss function (training objective) to avoid the need for a separate reward model. Later, BCO \cite{jung2024binary} and KTO \cite{ethayarajh2024kto} were proposed to further enhance alignment performance. In addition to the alignment methods mentioned above, several others have been developed to enhance performance in various ways. For example, ORPO \cite{hong2024orpo} incorporates the SFT loss into DPO, and \cite{yuan2024advancing} uses a preference tree. Other techniques can be found in \cite{xiong2024building,amini2024direct,lu2024online,wang2024bpo,zhou2024t,zhang2024self, franken2024self,yin2024entropy}.

% BPO \cite{wang2024bpo} enhances alignment by leveraging knowledge breadth and depth, while token-level regularization is explored in \cite{zhou2024t}. Additionally, self-exploring and self-supervised methods \cite{zhang2024self, franken2024self}, as well as sample-efficient algorithms \cite{yin2024entropy, xie2024exploratory}, have been proposed.

% Other studies have conducted comprehensive experiments to better understand alignment. For example, \cite{anwar2024foundational} identifies practical challenges in alignment, while \cite{rafailov2024scaling} explores scaling laws. \cite{li2024predicting} discusses the discrepancies between LLMs and LLM-based agents in the context of RLHF, and \cite{ivison2024unpacking} investigates various factors that impact alignment performance.

% \begin{itemize}
%     \item DPO \cite{rafailov2024direct}, PPO \cite{schulman2017proximal}\yue{please help check this reference}, BCO \cite{jung2024binary}, KTO \cite{ethayarajh2024kto},  RLHF \cite{ziegler2019fine,ouyang2022training}.  RLHF using the BT and PL models was first introduced in \cite{ziegler2019fine} and \cite{ouyang2022training} respectively. 
%     \item Enhancements of existing alignment methods: DPO with offset \cite{amini2024direct}, ORPO \cite{hong2024orpo}, preference tree \cite{yuan2024advancing}, multi-turn \cite{xiong2024building}, merging \cite{lu2024online}, BPO leveraging knowledge breath and depth \cite{wang2024bpo}, token-level regularization \cite{zhou2024t}, self-exploring and self-supervised method,\cite{zhang2024self,franken2024self}, sample-efficient algorithms \cite{yin2024entropy,xie2024exploratory}, 
%     \item Works in identifying practical challenges in alignment: \cite{anwar2024foundational}
%     \item Scaling law: \cite{rafailov2024scaling}
%     \item \cite{li2024predicting}: discuss about the discrepancy between LLMs and LLM-based agents when considering RLHF.
%     \item \cite{ivison2024unpacking} explored different factors which impact alignment performance.
% \end{itemize}
\vspace{-0.1in}
\paragraph{Theoretical Investigations} Beside the empirical studies, some other works focus on the theoretical properties of alignment and develop new algorithms based on their analysis. For example, \cite{xiao2024algorithmic} addresses preference bias in RLHF through preference matching. \cite{he2024accelerated} accelerates convergence by applying momentum, and \cite{liu2024dual} proposes an algorithm that uses active learning to select the appropriate human for RLHF. 
% Additionally, \cite{} explores RLHF in offline, online, and hybrid settings.
Other studies can be found in \cite{wang2024magnetic,xiong2024iterative,wang2023rlhf,du2024exploration}.
% and \cite{wang2023rlhf} compares RLHF with traditional reinforcement learning. 
% From a topological perspective, \cite{qiu2024reward} delves into RLHF, while \cite{du2024exploration} focuses on improving sample efficiency in RLHF.
Different from existing literature, we have a emphasis on the separation effect between aligned and unaligned data. 
% This focus leads to a key difference in our theory: previous literature offers limited understanding of how to connect alignment loss with divergence metrics, and we develop a concept of alignment consistency to connect them.


\subsection{Jailbreak Attack}
Aligned LLMs, despite their intended safety measures, can still produce harmful content, as highlighted in studies like \cite{zhou2023synthetic}, \cite{hazell2023spear}, and \cite{kang2024exploiting}. Jailbreak attacks, which exploit vulnerabilities in these models, have been explored in \cite{wei2024jailbroken} and \cite{carlini2024aligned}. To design effective jailbreak attacks, several methods have been proposed, including GCG \cite{zou2023universal}, AutoDAN \cite{liu2023autodan}, PAIR \cite{chao2023jailbreaking}, and TAP \cite{mehrotra2023tree}. 
% In the real data experiments, we will utilize jailbreak attacks to examin the robustenss of the aligned LLMs.

% Although our theoretical framework connects divergence metrics with alignment methods, we will use jailbreak attacks to numerically relate divergence to the robustness of aligned LLMs. We conduct these experiments because the robustness, rather than divergence, is the ultimate goal of safety alignment.

% \subsection{Related Works}
% \subsection{Contributions}
% 1. Unifying framework for safety alignment in terms of divergence optimization, and explaining clustering phenomenon absed on saafety of the prompt. \\
% 2. Motivated by this framework we introduce KLDO, optimizes to maximize divergence between unaligned and aligned distribution, in effort to better separation/alignment\\
% 3. We introduce a separation metric based on the distance between the distributions that acts as a statistically significant indicator of the robustness of the model.\\
% 4. We advocate the use of Compliance Refusal datasets instead of preference datasets for safety alignment, in line with our theory, supported by experiments.
\section{Preliminaries}
% In the following, we introduce the basic notations and alignment methods to be considered, as well as the data format.
\subsection{Notation}
Let \(x\) and \(y\) represent the prompt and response, respectively. Specifically, \(y_w\) denotes an aligned response, while \(y_l\) corresponds to an unaligned response. 
The joint distribution of prompts, aligned responses, and unaligned responses is represented by $(x,y_w,y_l)\sim\cd$. $(x,y)\sim\cd^{+}$ represents samples of the \textbf{ aligned distribution}, which is obtained by marginalizing the joint \(\cd\) over unaligned responses \(y_l\), while the \textbf{unaligned distribution}, \(\cd^{-}\), is obtained by marginalizing $\cd$ over aligned responses $y_w$. Note that the meaning of aligned and unaligned distributions can vary across different contexts (please refer to Sec. \ref{sec:data}).

The trainable model policy for generating a response given a prompt is represented by \(\ppol\), parameterized by \(\theta\), with \(\theta^*\) denoting the parameters at convergence. The reference model policy before alignment training is denoted by \(\pref\).  

For a probability distribution \(\cg\), the probability mass function (p.m.f.) or probability density function (p.d.f.) is represented by \(p_{\cg}\), and the corresponding conditional probability of \(y \mid x\) is written as \(p_{\cg}(y \mid x)\). The sigmoid function is defined as \(\sigma(u) = (1 + \exp(-u))^{-1}\). Finally, $\Omega$ denotes the usual asymptotic notation such that $f(u)=\Omega(g(u))$ if there exists $c>0$, with $f(u)\geq c\cdot g(u)$ for all $u$.

\subsection{Alignment Methods}
\label{sec: rlhf methds}

\paragraph{RLHF:} 
RLHF is one of the foundational methods for model alignment. It consists of the following two steps:\\
1. \textbf{Reward Modeling}: A reward function \( r_\phi(x, y) \) is learned to encode human preferences, where \(\phi\) represents the parameters (usually neural networks). Reward modeling requires paired alignment data of the form of \((x, y_w, y_l)\sim \cd\).

The reward function is trained to maximize the likelihood under the Bradley-Terry (BT) model \cite{bradley1952rank}:  
   \begin{equation}
       p(y_w \succ y_l \mid x) = \frac{\exp{r_\phi(x, y_w)}}{\exp{r_\phi(x, y_w)} + \exp{r_\phi(x, y_l)}}.
       \label{eqn: pairwise model}
   \end{equation}  

2. \textbf{Reward Maximization}: The learned reward function \(r_\phi(x, y)\) is then used to guide the policy \(\ppol\) by maximizing the following objective:  
   \begin{equation}
       \sup\limits_{\theta} \underset{x,y}{\bE}\left( r_\phi(x, y) - \beta \infdiv{\ppol}{\pref}\right),
       \label{eqn: reward max equation}
   \end{equation}  
   where \(\pref\) denotes the reference policy, and \(\infdiv{\ppol}{\pref}\) is a regularization term (e.g., KL divergence), with \(\beta\) controlling the trade-off between reward maximization and deviation of $\ppol$ from \(\pref\). 
   
   
\paragraph{DPO:} Direct Preference Optimization (DPO) \cite{rafailov2024direct} reduces the reward modeling and reward maximization steps of RLHF into a single minimization objective w.r.t. \(\theta\). This is achieved by substituting the theoretically optimal reward of (\ref{eqn: reward max equation}) in terms of the $\ppol$ policy, i.e., \(r_\phi(x,y) = r_\theta(x,y) = \beta \cdot \ln [\sfrac{\ppol(y|x)}{\pref(y|x)}\)], into (\ref{eqn: pairwise model}). The DPO loss is as follows:
%This allows us to express the traditional 2-step RL approach in terms of a single optimization problem under $\theta$ as follows: \yue{How can we can the following from Eqn 3, based on the previous sentence?}:  
\begin{equation*}
\cL_{\text{DPO}}(\theta) = -\underset{x,y_w,y_l \sim \cd}{\bE} \ln \sigma \big(r_\theta(x,y_w) - r_\theta(x,y_l)\big).  
\label{eqn: dpo}
\end{equation*}  

% For superlative preferences, the loss generalizes to:  
% \begin{equation}
% \resizebox{0.91\linewidth}{!}{$\cL_{\text{DPO}_s}(\theta) = -\underset{x,y_w,\{y_{l_i}\}^k_1 \sim \cd}{\bE} \ln \frac{\exp{r_\theta(x,y_w)}}{\exp{r_\theta(x,y_w)} + \sum_{i=1}^k \exp{r_\theta(x,y_{l_i})}}$}  
% \label{eqn: dpo-s}
% \end{equation}  

Theoretically, DPO is equivalent to RLHF \citep{rafailov2024direct}, and our main results will focus on DPO, which implicitly applies to RLHF as well. Alignment under DPO maximizes the likelihood of aligned responses while minimizing it for unaligned ones. 

Unlike RLHF and DPO, which rely on pairwise data, methods such as KTO \cite{ethayarajh2024kto} and BCO \cite{jung2024binary} reformulate alignment as a binary classification problem (aligned vs. unaligned) using unpaired data of the form $(x,y)$.  In our paper, $r_\theta(x,y) = \beta \cdot \ln \sfrac{\ppol(y|x)}{\pref(y|x)}$ is defined as reward unless stated otherwise.

\paragraph{KTO:} Given data in the form \((x, y)\), along with a binary signal indicating whether \(x,y\) is sampled from aligned ($\cd^+$) or unaligned $(\cd^-)$, the KTO loss with reference constant \(z_0\) (Defition \ref{defn: kto ref point}) is defined as follows:  
\begin{equation*}
\resizebox{\linewidth}{!}{$\cL_{\text{KTO}}(\theta) =\underset{x,y \sim \cd^{+}}{\bE} \big[1 - \sigma(r_\theta(x,y) - z_0)\big] + \underset{x,y \sim \cd^{-}}{\bE} \big[1 - \sigma(z_0 - r_\theta(x,y))\big]$}.
\label{eqn: kto}
\end{equation*}  
%where \(z_0\) is a reference constant (Defn. \ref{defn: kto ref point}), 
%and \(r_\theta(x,y)\) represents the reward function, identical to the one used in DPO.  

\paragraph{BCO:} Using the same data format as KTO, the Binary Classification Optimizer (BCO) loss with reference constant \(\delta\) (Defn. \ref{defn: bco ref point}) is defined as follows:  
\begin{equation*}
\resizebox{\linewidth}{!}{$\cL_{\text{BCO}}(\theta) = -\underset{x,y \sim \cd^{+}}{\bE} \ln \big[\sigma(r_\theta(x,y) - \delta)\big] 
- \underset{x,y \sim \cd^{-}}{\bE} \ln \big[\sigma(\delta - r_\theta(x,y))\big]$}.
\end{equation*}  
%where \(r_\theta(x,y)\) represents the reward function, identical to the one used in DPO. The reference value \(\delta\) is the total expected reward (Defn. \ref{defn: bco ref point}).  
% \begin{equation}
% \delta = \frac{1}{2} \left(\underset{x,y \sim \cd^{+}}{\bE} r_\theta(x,y) + \underset{x,y \sim \cd^{-}}{\bE} r_\theta(x,y)\right)
% \end{equation}  

\subsection{Data}
\label{sec:data}
In this section, we define the data generation process to relate LLM alignment and safety classification, which separates harmful and safe prompts in the hidden representation.

Each prompt \(x\) has a binary latent variable \(z_x\), denoting its safety: \(z_x = 1\) corresponds to a safe prompt, while \(z_x = 0\) indicates a harmful prompt. For any prompt, responses could be compliant or refusing, leading to two fundamental distributions: samples \(x, y \sim \cc\) represent the \textbf{compliance distribution}, where \(y\) is a compliant response for \(x\), and samples \(x, y \sim \calr\) correspond to the \textbf{rejection distribution}, where \(y\) is a refusal response. 

In the following, we introduce two data models: \textbf{compliance-refusal (CR)} and \textbf{preference (Pref)} data, which relate aligned $\cd^+$ and unaligned $\cd^-$ distributions to $\cc$ and $\calr$ depending on the context. For \textbf{compliance-refusal (CR) data}, given \(z_x = 0\) (harmful prompts), the aligned distribution corresponds to the rejection distribution, while the unaligned distribution corresponds to the compliance distribution. Conversely, when \(z_x = 1\) (safe prompts), the aligned distribution corresponds to the compliance distribution, and the unaligned distribution corresponds to the \textbf{rejection distribution}. 
% \begin{equation}
% x, y \overset{\text{CR}}{\sim} \begin{cases} 
% \cc &, \cd^{+} \mid z_x = 1 \text{ or } \cd^{-} \mid z_x = 0, \\  
% \calr &, \cd^{-} \mid z_x = 1 \text{ or } \cd^{+} \mid z_x = 0.
% \end{cases}
% \label{eqn: comp-ref}
% \end{equation}
% \yue{I think I understand the above formula, but I feel it is not rigorous?}\raj{Updated the presentation}
In practice, most RLHF applications  \textbf{preference (Pref) data} is used, where given safe prompts (\(z_x = 1\)), both aligned and unaligned responses are compliant, but the aligned response is more preferred. For harmful prompts (\(z_x = 0\)), the aligned response is a refusal, while the unaligned response is a compliant response, with the refusal being preferred. Formally, Table \ref{tab: data_model} summarizes the data generation process. We do not specify the dependency between \( y_w \) and \( y_l \) in our data generation process, as our results are independent of it.

\input{tabs/data_model}


% \vspace{-0.8cm}
%Abbreviations CR and Pref will denote indicate the use of each data respectively\yue{grammar}.
\section{Main Results}
\label{sec: theory}
%In this section, we present our main findings. First, we present variational representations for common divergences, framing them as optimization problems over arbitrary functionals (Sec. \ref{subsec: loss as estimators}). Using this, we link popular alignment losses to the divergence between aligned and unaligned distributions (Thm \ref{thm: divergence convergence}) and propose KLDO, a novel alignment method based on KL divergence, more generally $f$-divergence optimizers (FDO). Next, we establish that most alignment losses satisfy \textbf{alignment consistency}, allowing models to favor responses from the true aligned distribution (Thm \ref{thm: alignment consistent}). Finally, we show alignment-consistent methods enhance separation by enabling LLMs to classify prompts by safety labels (Thm \ref{thm: separation}).
%This section presents our main theoretical results. In particular, in Sec \ref{subsec: loss as estimators}, we revise some existing divergence metrics and connect them with some alignment methods. Sec \ref{subsec: dpo_div}, we discuss DPO and explain why DPO can be potentially worse than other alignment methods for safety alignment. In Sec \ref{sec: general losses}, we introduce the alignment methods inspired from KL divergence as well as f-divergence. Sec \ref{sec:alignment_consistency} and \ref{sec:separation} introduces the concept of alignment consistency, how it helps us on the connection between divergence metrics and the exact separation between aligned and unaligned data, and how the separation is affected by CR and Pref data.

% \subsection*{Outline}
% In this section, we present our main results. First, we introduce variational representations for common divergences, allowing us to express these divergences as optimization problems with respect to an arbitrary functional (Sec. \ref{subsec: loss as estimators}). Using this representation, we establish a connection between popular alignment losses and the divergence between the true aligned and unaligned distributions (Thm \ref{thm: divergence convergence}). Building on this relationship, we propose KLDO, a new alignment method based on KL divergence, and theoretically extend it to a general class of $f$-divergence optimizers (FDO). Next, we demonstrate that most alignment losses satisfy a property called \textbf{alignment consistency}, which enables models to favor or disfavor responses based on the true aligned or unaligned distribution (Thm \ref{thm: alignment consistent}). Finally, to explain the separation phenomenon, we show that alignment-consistent methods enable LLMs to solve a classification problem of predicting the safety label from the underlying prompt (Thm \ref{thm: separation}).

\subsection{Alignment Losses as Divergence Estimators}
\label{subsec: loss as estimators}
To connect alignment losses and divergence metrics, we first layout some details about divergences. Popular divergences, such as KL, TV, and JS,
% like Kullback-Leiber (KL), Total Variation (TV), Jensen-Shannon (JS) 
measure the difference between any two probability distributions $\cp,\cq$ over a random variable $v\in \mathcal{V}$. They can be expressed in terms of an optimization problem over an arbitrary functional $T(v)$ as follows:
\begin{equation}
\footnote{\text{Donskar-Varadhan (DV) representation. \citep{donsker1975asymptotic} }}\infdiv{\cp}{\cq}=\sup\limits_{T}\underset{v\sim \cp}\bE T(v)-\ln \underset{v\sim\cq}{\bE} e^{T(v)},
    \label{eqn: kl rep}
\end{equation}
\begin{equation}
\tvdiv{\cp}{\cq}=\sup\limits_{T:|T|\leq \sfrac{1}{2}}\underset{v\sim \cp}\bE T(v)-\underset{v\sim\cq}{\bE} T(v),
    \label{eqn: tv rep}
\end{equation}
\begin{equation}
    \resizebox{\linewidth}{!}{$2\cdot\jsdiv{\cp}{\cq}-\ln 4= \sup\limits_{T:0\leq T\leq 1}\underset{v\sim \cp}\bE \ln T(v)-\underset{v\sim\cq}{\bE} \ln\left(1-T(v)\right)$}.
    \label{eqn: js rep}
\end{equation}
For the general class of $f$-divergences (Defn. \ref{defn: f-div}) we have the crude variational bound:
\begin{equation}
\fdiv{\cp}{\cq}=\sup\limits_{T:\mathcal{V}\to \effdom (f^*)}\underset{v\sim \cp}\bE T(v)-\underset{v\sim\cq}{\bE} f^*\circ T(v),
    \label{eqn: f-div rep}
\end{equation}
where $f^*$ is the convex conjugate (Defn. \ref{defn: convex conjugate}) of $f$, and $\effdom(f^*)=\{u: f^*(u)<\infty\}$. 

Given the above formulation of divergences, the following theorem connects these metrics with different alignment methods. In short, KTO and BCO exactly correspond to TV and JS, and DPO is lower bounded by the negative TV.
\begin{theorem}
% If the rewards are finite, i.e. there exists a constant $m>0$ such that $r_\theta(x,y)\leq m\, , \forall x,y$, then for any $\theta$, there exists $T_{\theta}$ 
% % $\forall \theta\, , \exists \, T_\theta$ 
% such that
% \begin{align}
%     &\cL_{\text{K}}(\theta)\geq_{\text{lin}}-\cf_{T_{\theta}}^{\text{TV}} (\cd^+,\cd^-),\\
%     &\cL_{\text{DPO$_s$}}(\theta)\geq-\cf_{T_{\theta}}^{\text{DV}} (\cd^+,\cd^-)+\ln\left(1+o(k^{-\sfrac{1}{2}})\right),
% \end{align}
% where K$\in$\{DPO, KTO, BCO\}. Moreover at convergence \yue{What is at convergence for DPO, KTO, BCO?} and the number of less-preferred responses $k\to \infty$ in superlative preferences:
Alignment losses in Sec \ref{sec: rlhf methds} satisfy:
\begin{align*}
   &\cL_{\text{KTO}}(\theta^*)=-\tvdiv{\cd^{+}}{\cd^-}+1,\\
   &\cL_{\text{BCO}}(\theta^*)=\ln4-2\cdot\jsdiv{\cd^+}{\cd^-},\\
   &\cL_{\text{DPO}}(\theta^*)=\Omega(-\tvdiv{\cd^+}{\cd^-}),
\end{align*}
where $\theta^*=\arg\inf \cL(\theta)$ for respective alignment loss $\cL$. %\textcolor{purple}{do we mention that we implicitly assume the model $\pi_\theta$ cover the whole functional space?}
\label{thm: divergence convergence}
\end{theorem}
Thm \ref{thm: divergence convergence} shows that for any model \(\theta\), all alignment losses in Sec \ref{sec: rlhf methds} are bounded below by a negative divergence of the true aligned/unaligned distributions. At convergence (\(\theta = \theta^*\)), BCO and KTO optimally estimate the TV and JS divergences between \(\mathcal{D}^+\) and \(\mathcal{D}^-\). 
% \yue{Intuitively, since the divergence metrics directly measure the difference between $\cd^+$ and $\cd^-$, the globally optimal policy $\theta^*$ aims to capture how the two distributions differ from each other from the perspective of the corresponding divergence metric. }
% \textcolor{purple}{Please refer to Section \ref{sec:alignment_consistency} for a rigorous analysis.}
Since divergences quantify distributional separation, this serves as preliminary evidence that alignment methods promote separation.
% \yue{Need an additional paragraph to talk about the issue in KTO. For now, we mention its problem in the introduction, but not here in this section.}

In terms of DPO, while DPO is bounded by \(-\text{TV}\), its estimated quantity lacks a closed-form solution to connect to any known divergence metric formulation. In the next section, we define a new divergence based on DPO and compare it with existing divergences. 
% \yue{To confirm, Theorem 4.1 applies to both CR and Pref data?}\raj{Yes it is independent of data type}
\subsubsection{Analyzing DPO induced Divergence}
\label{subsec: dpo_div}
Based on the DPO loss, we define a variational representation of a non-parametric candidate divergence as follows:
\begin{equation*}
    \mathbb{D}_{\text{DPO}}\infdivx{\cp}{\cq}=\sup\limits_{T}\underset{v_1\sim \cp,v_2\sim \cq }{\bE} \ln \sigma \big(T(v_1) - T(v_2)\big). 
    \label{eqn: dpo induced divergence}
\end{equation*}
To analyze the behavior of this candidate divergence, we conduct simulation to compare it with other divergences. We compute $\mathbb{D}_{\text{DPO}}$ and $\mathbb{D}_{\text{TV}},\mathbb{D}_{\text{JS}}$ and $\mathbb{D}_{\text{KL}}$ for range of normal distribution pairs $\cp=\cn(0,1),\cq=\cn(\mu,1)$ where we vary $\mu$. The results are summarized in Fig. \ref{fig:dpo_sensitivity}.
% , where \yue{please mention what is normalized divergence.}
% We also calculate . 

% $\mathbb{D}_{\text{DPO}}$ has been appropriately shifted to enforce $0$ at $\mu=0$.
\begin{figure}[!ht]
    \centering
    \vspace{-0.1in}
    \includegraphics[width=0.8\linewidth]{figs/DivergencesvsAcc.png}\vspace{-0.15in}
    \caption{Normalized Divergence vs Accuracy}
    \label{fig:dpo_sensitivity}
\end{figure}

Fig. \ref{fig:dpo_sensitivity} shows the normalized divergences (i.e., the divergence is rescaled to have a maximum value of 1) against the "Accuracy = 1 - Overlap" coefficient (Sec \ref{subsec: overlap defn}), which quantifies the ability to distinguish samples from each Gaussian. Accuracy increases from \(0\to1\) as \(\mu\) grows from \(0\to\infty\). 

From Fig. \ref{fig:dpo_sensitivity}, all divergence curves, including \(\mathbb{D}_{\text{DPO}}\), are non-decreasing with accuracy, confirming \(\mathbb{D}_{\text{DPO}}\) as a valid divergence. However, while TV, JS, and KL are convex and increase steadily with accuracy,
% maintaining sensitivity to distinguish both far and very far distributions. In contrast, 
\(\mathbb{D}_{\text{DPO}}\) follows an S-shaped curve, saturating at the extremes, which limits its sensitivity when distinguishing very far distributions. This distinction is critical, as \(\mathcal{D}^+\) and \(\mathcal{D}^-\) are expected to be well-separated, 
% and other divergences capture this due to their convexity, 
while \(\mathbb{D}_{\text{DPO}}\) cannot due to its tail saturation.

\begin{figure}[!ht]
    \centering
    \vspace{-0.1in}
    \includegraphics[width=0.8\linewidth]{figs/Divergences_vs_mu.png}\vspace{-0.15in}
    \caption{Divergence vs Mean Separation}
    % \vspace{-0.1in}
    \label{fig:dpo_divergence}
\end{figure}
\vspace{-0.1in}
As a supplement, we also plot the unnormalized divergences vs $\mu$ in Fig. \ref{fig:dpo_divergence}. To compare with the other divergences, \(\mathbb{D}_{\text{DPO}}\) is the first to saturate compared to the rest of the divergences, limiting its ability to capture huge separations. 


To summarize, although one can rewrite the loss of DPO as a distance metric, its behavior is different from the other divergences in an unwanted way. Also, we expect DPO to be limiting compared to other methods for safety alignment.
% \vspace{-0.4cm}

% Figure \ref{fig:dpo_divergence}, plots unnormalized divergences vs $\mu$. 

\subsection{Alignment Losses from General Divergences}
\label{sec: general losses}

While Sec \ref{subsec: loss as estimators} discusses KTO, BCO, and DPO, there are some other divergences (e.g., KL in Fig. \ref{fig:dpo_sensitivity}) which are not included in Thm \ref{thm: divergence convergence}. In the following, we use KL to design a new method, KLDO, to demonstrate how the insights in Sec \ref{subsec: loss as estimators} can further inspire new methods. Similarly, we also consider f-divergence in addition to KL.

\subsubsection{KLDO}
\label{subsec: kldo}
We introduce a new alignment method to connect to the KL divergence. On one hand, from Fig. \ref{fig:dpo_sensitivity}, KL is the most sensitive in capturing farthest distribution separation. 
% It seems to be a good divergence to enforce separation in models. 
On the other hand, none of the standard optimizers estimate it (Thm \ref{thm: divergence convergence}). Therefore, we borrow KL and design KLDO: the KL-Divergence Optimizer.
% since many other alignment methods are connected with various divergence a we introduce a KL based optimizer. 
% this is supported by Figure \ref{fig:dpo_divergence}, as well, where KL is unbounded and never saturates. 
% KL seems to be a good divergence to enforce separation in models, and none of the standard optimizers estimate it (Thm \ref{thm: divergence convergence}). Motivated by this we introduce a KL based optimizer.

To derive the loss objective for alignment, we parameterize the functional \(T_\theta(x, y) = r_\theta(x, y)\) in the variational representation (\ref{eqn: kl rep}), resulting in the following loss function:  
\begin{equation}
    \cL_{\text{KLDO}}(\theta) = -\underset{x, y \sim \cd^{+}}{\bE} r_\theta(x, y) + \ln \underset{x, y \sim \cd^{-}}{\bE} e^{r_\theta(x, y)}  
    \label{eqn: kldo loss}.
\end{equation}  
By construction, KLDO estimates the KL divergence, such that \(\cL_{\text{KLDO}}(\theta^*) = -\infdiv{\cd^{+}}{\cd^{-}}\).

\paragraph{Dealing with Biased Gradient Updates} 
% \yue{Give a sentence why we need the gradient formula.}
The gradient of the loss is given by:
\begin{equation}
\nabla_\theta \cL_{\text{KLDO}}(\theta) = -\underset{x, y \sim \cd^{+}}{\bE} \nabla_\theta r_\theta + \frac{\underset{x, y \sim \cd^{-}}{\bE} \nabla_\theta r_\theta e^{r_\theta}}{\underset{x, y \sim \cd^{-}}{\bE} e^{r_\theta}}.
\label{eqn: kldo gradient}
\end{equation}
The second term of (\ref{eqn: kldo gradient}) involves a scaling factor in the denominator, which is an expectation over the complete unaligned distribution. In practice, directly applying SGD updates with a minibatch \( B = B^+ \cup B^- \) of aligned and unaligned samples will induce bias. To mitigate this, we estimate the denominator by computing a moving average over the unaligned minibatch, inspired by \cite{belghazi2018mine}, where mutual information is estimated using a DV representation in a contrastive learning framework.

\subsubsection{FDO}
\label{subsec: fdo}
%Given a variational representation of a divergence, we can derive a corresponding alignment loss. 
Extending from KLDO, in theory, we can also construct an alignment loss based on general class of \(f\)-divergences. We parametrize \(T_\theta(x,y) = g(r_\theta(x,y))\) in variational representation (\ref{eqn: f-div rep}), where \(g: \mathbb{R} \to \effdom(f^*)\) is a strictly increasing and invertible function. We refer to this class of alignment losses as \(\text{FDO}(f, g)\), or the \(f\)-divergence optimizer with the link function \(g\), defined as follows:
\begin{equation}
\resizebox{\linewidth}{!}{$\cL_{\underset{(f, g)}{\text{FDO}}}(\theta) = -\underset{x, y \sim \cd^{+}}{\mathbb{E}} g\left(r_\theta(x, y)\right) + \underset{x, y \sim \cd^{-}}{\mathbb{E}} f^* \circ g\left(r_\theta(x, y)\right)$}.
\label{eqn: fdo}
\end{equation}
To align with Thm \ref{thm: divergence convergence}, by definition, \(\cL_{\text{FDO}(f, g)}(\theta^*) = \fdiv{\cd^+}{\cd^-}\).

\subsection{Alignment Consistency}\label{sec:alignment_consistency}
While the above connects alignment methods with divergence metrics, and divergence intuitively induces separation of the aligned and unaligned data, there is still a missing piece on how to explicitly show the separation in theory. Therefore, in this section, we introduce the concept of alignment consistency, and utilize this concept in the later Sec \ref{sec:separation} to show the separation. 

Before alignment, the probabilities of responses given prompts are governed by the reference policy \(\pref\). An effective alignment method should learn a policy \(\ppolo\) that appropriately adjusts the reference policy based on the likelihood of a response being aligned or unaligned. The following definition formalizes this concept, which we refer to as \emph{alignment consistency}.

\begin{definition}[Alignment Consistent] An alignment method is ``consistent" if the optimal policy follows%$\ppolo(y|x)$ is a non-constant, non-decreasing function of $R(x,y)=\sfrac{p_{\cd^{+}}(y|x)}{p_{\cd^{-}}(y|x)}$.
\begin{equation*}\label{eqn:alignment_consistent}
    \ppolo (y|x)=Z(x)^{-1}\cdot\pref(y|x) \cdot h(R(x,y)),
\end{equation*}
where $R(x,y)=\sfrac{p_{\cd^{+}}(y|x)}{p_{\cd^{-}}(y|x)}$, $h:\reals\to\reals$ is a non-decreasing, non-constant function, and $Z(x)$ is a normalizing constant so that the total probability is 1.
% \begin{align*}
%     \sfrac{\pi_{\theta^*}(y|x)}{\pref(y|x)}> 1 \iff p_{\cd^{+}}(y|x)>p_{\cd^{-}}(y|x)\\
%     \sfrac{\pi_{\theta^*}(y|x)}{\pref(y|x)}< 1 \iff p_{\cd^{+}}(y|x)<p_{\cd^{-}}(y|x)
% \end{align*}   
\label{def: alignment consistent}
\end{definition}
To explain Defn. \ref{def: alignment consistent}, $R(x,y)$ is larger when a response is more likely to be aligned. With increases in $R(x,y)$, the nondecreasing, nonconstant function $h$ ensures that the policy puts more probability mass on aligned responses.
% \yue{sometimes mass and sometimes density... Do we want to unify the wording?}. \yue{I adjusted the wording of this paragraph, is this consistent to your meaning? Just want to expalin $R$ first and then $h$.}\raj{I made it more concise and we can decide on a single word, in our notation we use mass and density interchangabley, given the distribution. we can decide on one word.}

%The following theorem shows alignment consistency for most methods along with the corresponding $\ppolo$.

Given the above definition of alignment consistency, in the following we rewrite the alignment methods to match its definition and figure out $h$.
\begin{theorem}
The following alignment methods are `consistent' (Defn. \ref{def: alignment consistent}) with corresponding $h(u)$:
\begin{align*}
    &h(u)_{\text{KTO}}=\left[\frac{1+\sign{(u-1)}}{1-\sign{(u-1)}}\right]^{\frac{1}{\beta}},\\
    &h(u)_{\text{KLDO, BCO}}=u^{\frac{1}{\beta}}, \quad h(u)_{\text{FDO}}= e^{\frac{g^{-1}\circ f'(u)}{\beta}}.
\end{align*}
\label{thm: alignment consistent}
\end{theorem}
\vspace{-0.25in}
From Thm \ref{thm: alignment consistent}, all the methods enforce \(\ppolo \to \pref\) as \(\beta \to \infty\). This behavior aligns with Equation \ref{eqn: reward max equation}, where large values of \(\beta\) heavily penalize deviations from \(\pref\) during the reward maximization step in RL. Conversely, as \(\beta \to 0\) (i.e., no regularization), \(\ppolo \propto \infty\) or \(0\) depending on whether \(R(x, y) > 1\) or \(R(x, y) \leq 1\). In this regime, the optimal policy eliminates all probability mass from unaligned responses and distributes it uniformly among aligned responses.

For KTO, the function \(h(u)\) is discrete in \(R(x, y)\), which is a characteristic of TV divergence. In contrast, KLDO and BCO exhibit smoother dependencies on \(R(x, y)\), allowing these methods to capture subtle discrepancies between aligned and unaligned distributions, if present.

Finally, as a sanity check, the FDO framework recovers the formulations for KTO, BCO, and KLDO with appropriate choices of \(f\) and \(g\). For example, expressing KTO as an FDO with \(f(u) = \frac{1}{2}|u - 1|\) and \(g(u) = \sigma(u - z_0) - \frac{1}{2}\) reproduces the same result.

\begin{remark}[Is DPO Alignment Consistent?] Theoretically, proving is challenging due to the lack of a closed-form solution for the divergence it estimates (Sec \ref{subsec: dpo_div}). However, as a valid divergence, we believe its consistency arises as a by-product of divergence estimation, supported heuristically and by DPO's empirical success as an alignment method.
\end{remark}
\subsection{Alignment Consistent Loss induces Separation}\label{sec:separation}
%\subsection*{Naive Bayes Classifer for safety label based on $\pi_{\theta^*}$}
To understand the clustering or separation of prompts based on safety by LLMs after alignment, we model this behavior as a classification problem. Specifically, we aim to predict the safety label \(z\) given the prompt \(x\) and the optimal parameter \(\theta^*\) obtained after alignment.  
Using a Bayesian framework with no prior bias towards the safety label, we define the probability of a response being safe, given the prompt-response pair and the optimal policy, as:
\begin{equation}
    p(z=1\mid x,y,\theta^*)=\frac{\ppolo(y\mid x,z=1)}{\sum_{t\in\{0,1\}}\ppolo(y\mid x,z=t)}.
    \label{eqn: z|xy}
\end{equation}
To eliminate the dependence on \(y\) in the conditional model, we normalize over the set of all feasible responses, \(\text{FR}(x) = \{y : \left(\sfrac{p_{\mathcal{C}}(y|x)}{p_{\mathcal{R}}(y|x)}\right)^{2z_x - 1} \geq 1\}\), for a given prompt \(x\). This set consists of all responses likely to comply or refuse based on whether the prompt is safe or harmful.  
\begin{equation}
    p(z = t \mid x, \theta^*) = \sfrac{\sum\limits_{y \in \text{FR}(x)} p(z = t \mid x, y, \theta^*)}{|\text{FR}(x)|}.
    \label{eqn: z|x}
\end{equation}
Using this conditional model, we define a Naive Bayes Classifier for safety, $\hat{z}(x, \theta^*)$ as:
\begin{equation}
    \hat{z}(x, \theta^*) = \arg\max_{t \in \{0,1\}} p(z = t \mid x, \theta^*).
    \label{eqn: naive bayes classifier}
\end{equation}
The following theorem demonstrates how alignment consistency is related to separation.
\begin{theorem}[Separation]
\label{thm:separation}
If an alignment method is alignment consistent, then the Naive Bayes Classifier (\ref{eqn: naive bayes classifier}) is always accurate, $\hat{z}(x,\theta^*)=z_x,\, \forall x $. Moreover, conditional probability $p(z=t\mid x,\theta^*)$ in Equation (\ref{eqn: z|x}) is improved for `CR' vs `Pref' type data, i.e.,
\begin{equation*}
   p^{\text{CR}}(z=z_x \mid x, \theta^*) \geq \,p^{\text{Pref}}(z=z_x \mid x, \theta^*) > 0.5.
\end{equation*}
\label{thm: separation}
\end{theorem}
\vspace{-0.25in}
The above theorem asserts that the Naive Bayes Classifier based on \(\ppolo\) accurately classifies the true safety label for all given prompts. Thus, 
% as a by-product of a consistent alignment method, 
the model learns to distinguish safe and harmful prompts. Moreover, the separation between safe and harmful clusters is stronger when using compliance-refusal data compared to preference data.

% \yue{Something remember to add:
% \begin{itemize}
%     \item Why DPO is less preferred than others?
%     \item While KTO is consistent in TV(or which one?), why it is less preferred than BCO and KLDO?
% \end{itemize}
% }


\section{Experiments}
\label{sec: experiments}
%{In this section, we present real-data experiments linking LLM safety alignment to the theoretical analysis in Sec. \ref{sec: theory}. We begin by detailing the data and experimental setup in Sec. \ref{sec:exp:setup}. In addition to empirically validating the separation of harmful and safe prompts induced by alignment methods, Sec. \ref{sec: separation} establishes a statistically significant relationship between separation and attack success rate (ASR). In Sec. \ref{sec: rob vs utility}, we analyze the trade-off between utility and robustness across alignment methods, showing that each method offers unique benefits. Finally, in Sec. \ref{sec:exp:CR}, we demonstrate the critical role of compliance refusal datasets in increasing separation and in turn improving robustness.}

% In this section, we begin by analyzing the relationship between an LLMs' tendency to generate harmful content and its capacity to distinguish between safe and unsafe prompts.
% Next, we evaluate the effectiveness of KLDO in preventing LLMs from generating harmful content, by addressing two critical questions: 1) Can LLMs, after being aligned with KLDO on both harmful and non-harmful datasets, reliably distinguish between harmful and non-harmful prompts? 2) Do KLDO-aligned LLMs remain robust against harmful prompts and jailbreak attempts without significant performance degradation?
% % Finally, we examine the role of answers to harmful and un-harmful questions in alignment training data in enhancing an LLM’s ability to distinguish between harmful and non-harmful inputs.
% Finally, we examine and demonstrate the crucial role of compliance refusal datasets in preventing LLMs from generating harmful content.

% To answer the first question, 
% following the approach outlined in \citep{lin2024towards}, we visualize the outputs of the LLMs' final hidden layer, reduced to two dimensions using PCA, to represent the model's understanding of each prompt.
% As for the second question, we use AdvBench \citep{zou2023universal}


\subsection{Experiments Setup}\label{sec:exp:setup}
\paragraph{Model}In our experiments, we leverage various LLMs to showcase the universality of our theoretical insights across different models. Specifically, we use 
% the 1-billion-parameter version of 
Llama3.2-1B \citep{dubey2024llama}, 
% the supervised fine-tuned 7-billion-parameter version of 
Llama2-7B \citep{touvron2023llama, ethayarajh2024kto} trained on SHP \citep{ethayarajh2022understanding, ethayarajh2024kto, touvron2023llama}, Anthropic HH \citep{bai2022training, ganguli2022red}, OASST1 \citep{kopf2024openassistant}, 
% the 2-billion-parameter 
Gemma2-2B \citep{team2024gemma}, 
% the 7-billion-parameter 
Mistral-7B-v0.1 \citep{jiang2023mistral}, and 
% the 1.5-billion-parameter 
Qwen2.5-1.5B \citep{yang2024qwen2}.

\paragraph{Data and Training} We leverage the SafeAligner~\citep{huang2024safealigner} and Alpaca-GPT4-Data~\citep{peng2023instruction} datasets to create two versions based on CR and Pref (Sec \ref{sec:data}). We train LLMs using different alignment methods (DPO, KTO, BCO, KLDO). Detailed data and training description can be found in Appendix \ref{appex: experiment_details}. 
% As described in \citep{huang2024safealigner}, the SafeAligner dataset includes $628$ unsafe prompts sourced from open platforms, with safe responses generated by GPT-4 and unsafe responses created by a fine-tuned Llama-3-8B-Instruct model designed to produce harmful content in response to unsafe prompts. The Alpaca-GPT4-Data dataset consists of 52,000 safe prompts from Alpaca\ citep{alpaca}, paired with aligned responses generated by GPT-4. We randomly sample $628$ prompts from Alpaca-GPT4-Data, and combined with the $628$ unsafe prompts from SafeAligner, we create a half-safe and unsafe set of prompts. 

\paragraph{Compliance Refusal Dataset}
For the responses in the compliance refusal dataset, we handle safe and unsafe prompts differently. For safe prompts, the aligned responses are from the Alpaca-GPT4-Data, while the unaligned responses are the predefined refusal answer ``I'm sorry, but I cannot assist with that request." 
Details can be found in Appendix \ref{appex: experiment_details}.
For unsafe prompts, the aligned responses are also the predefined refusal answers, whereas the unaligned responses consist of harmful content generated by the unaligned model Mistral-7B-v0.1 \citep{jiang2023mistral}. Our dataset is available on \href{https://anonymous.4open.science/r/KLDO-84F5/dataset_generation/Base_accept_reject.jsonl}{here}.
% The dataset is available on \href{https://huggingface.co/datasets/rhaldar97/Safety_Accept_Reject}{HuggingFace}.  

\vspace{-0.1in}
\paragraph{Preference Dataset}
For the preference dataset, we use the same prompts as those in the compliance refusal dataset. % and the aligned and unaligned responses remain consistent with those in the compliance refusal dataset.
The key difference lies in the responses to safe prompts. For safe prompts, the aligned responses are the preferred responses from Alpaca-GPT4-Data, as determined by human preference. In contrast, the unaligned responses are not refusals but instead compliant responses that are less preferred from a human perspective. These unaligned responses are generated from GPT3.5-turbo from the same questions. Our dataset is publicly available on \href{https://anonymous.4open.science/r/KLDO-84F5/dataset_generation/Base_preference.jsonl}{here}.

% For the compliance refusal dataset, we collected all the unsafe prompts from SafeAligner and used the 
% \ZW{TODO}

\begin{figure*}[ht]
    \centering
    \subfigure[Base]{
        \includegraphics[width=0.18\textwidth, trim=0 0 0 85, clip]{figs/base_Qwen2.5-1.5B.png}
        \label{fig:sub1}
    }\hspace{-1em}
    \subfigure[DPO]{
        \includegraphics[width=0.18\textwidth, trim=0 0 0 85, clip]{figs/dpo_qwen.png}
        \label{fig:sub2}
    }\hspace{-1em}
    \subfigure[KTO]{
        \includegraphics[width=0.18\textwidth, trim=0 0 0 85, clip]{figs/kto_qwen.png}
        \label{fig:sub3}
    }\hspace{-1em}
    \subfigure[KLDO]{
        \includegraphics[width=0.18\textwidth, trim=0 0 0 85, clip]{figs/kl-ma_qwen.png}
        \label{fig:sub4}
    }\hspace{-1em}
    \subfigure[BCO]{
        \includegraphics[width=0.18\textwidth, trim=0 0 0 85, clip]{figs/bco_qwen.png}
        \label{fig:sub5}
    }
    \vspace{-0.1in}
    \caption{Latent Space Visualization after various alignment methods for \emph{Qwen-2.5-1.5B}.}
    \label{fig: viz qwen}
\end{figure*}

\subsection{Separation and robustness}
\label{sec: separation}
%In this section, we first validate that all alignment methods induce separation in latent space w.r.t. safety from a non-aligned base model as suggested by our theory. We compare various alignment methods and further establish a statistically significant relation between robustness against harmful prompts and separation. 

%In this section, we explore the relationship between the separation of safe and unsafe prompt in the LLMs' hidden representation space and the models' robustness to unsafe prompts. {Different from \cite{lin2024towards} which only uses existing aligned LLMs such as Llama-3-8B-Instruct, we align the LLMs using different alignment methods by ourselves to provide a more comprehensive analysis with respect to these alignment methods.}
% Our analysis reveals a strong correlation between the separation of s and the LLMs' resilience to adversarial prompts, suggesting that improving the separation of safe and unsafe prompts in the representation space can enhance the robustness of LLMs against adversarial attacks. 
% Furthermore, our empirical results demonstrate that LLMs aligned using KLDO achieve a distinction between unsafe and safe prompts that is comparable or superior to other methods.
Unlike \cite{lin-etal-2024-towards-understanding}, which focuses on analyzing already aligned models from a jailbreak perspective, our work examines how alignment impacts separation in latent space. Nevertheless, we adopt the visualization methodology from \cite{lin-etal-2024-towards-understanding} (details in Appendix \ref{subsec: viz methd}) to illustrate separation in aligned models. Our findings reveal that post-alignment, models consistently exhibit separation in latent space, across all methods. For instance, Fig. \ref{fig: viz qwen} demonstrates this for Qwen-2.5-1.5B. Additional visualizations for other models can be found in Appendix \ref{subsec: latent space viz}. However, visualizations are qualitative, and we propose numerical metrics to evaluate separation, clustering, and robustness for detailed quantification.
\subsubsection{Metrics}
\label{subsec: exp metric defn}
\textbf{Separation \& Clustering:} To quantify the separation between safe and harmful prompts in the hidden representation space, we employ the \textit{Bhattacharyya Distance} (\(D_B \in \mathbb{R}^+\)), a metric that measures the distance between two distributions or clusters. This distance directly reflects the extent of overlap between clusters, making it particularly suited to our context, where unaligned base models exhibit significant overlap, while aligned models show minimal or no overlap. Additionally, we assess clustering quality using the \textit{Silhouette Score} (\(s \in [-1,1]\)), which measures how well an object matches its own cluster compared to others. A higher \(s\) indicates denser and more distinct clusters. Together, \(D_B\) and \(s\) serve as sufficient metrics to quantitatively capture the separation and clustering quality observed in visualizations such as Fig. \ref{fig: viz qwen}. Details on these metrics and their implementation are provided in Appendix \ref{subsec: metrics}.

\textbf{Robustness:} We formally assess robustness using the \textbf{Attack Success Rate (ASR)}, defined as the proportion of adversarial prompts that successfully elicit an undesired or misaligned response from the model. A higher ASR indicates greater susceptibility to adversarial attacks, while a lower ASR reflects improved resistance.  We compute the ASR using the AdvBench benchmark~\citep{zou2023universal}.


% To quantify the separation of safe and harmful prompts in the hidden representation space, we use the \textit{Bhattacharyya Distance} and the \textit{Silhouette Score} as metrics for measuring the distinction between safe and harmful prompts. 
% For detailed definition and introduction of 
% Bhattacharyya Distance and Silhouette Score, please refer to the Appendix \ref{appex: experiment_details}.
% While separation intuitively relates to the robustness of LLMs, we formally evaluate the robustness using the Attack Success Rate (ASR), which is the proportion of adversarial prompts that successfully trigger an undesired or misaligned response from the model. A higher ASR indicates greater vulnerability, while a lower ASR suggests enhanced resistance to adversarial attacks. In our case, the ASR is computed on the AdvBench benchmark ~\citep{zou2023universal}.

% For simplicity and visualization purposes, following the methodology in \citep{lin-etal-2024-towards-understanding}, we reduce the dimensionality of the hidden representation to the top two dimensions using PCA. We defer the details of the visualization procedure in the Appendix \ref{appex: experiment_details}.
% To assess the separation of safe and unsafe prompts in the hidden representation space, we introduce two metrics: Bhattacharyya Distance and Silhouette Score.

% The Bhattacharyya Distance measures the distance between two probability distributions and is particularly useful for quantifying the overlap between distributions.
% In addition to the Bhattacharyya Distance, we employ the Silhouette Score to evaluate the separation of safe and unsafe prompts in the LLM’s representation space from a clustering perspective. Prior research~\citep{lin2024towards} has shown that well-aligned LLMs tend to cluster safe prompts together and unsafe prompts together. Building on this observation, the Silhouette Score is introduced to assess the quality of this clustering by measuring both compactness within clusters and separation between clusters.
% By definition, if the Silhouette Score is close to $1$ indicates that points are well-clustered, points being close to its own cluster and far from others. A score close to $-1$, on the other hand, indicates points are closer to another cluster than to their own, suggesting poor clustering.

% By definition, the $s(i)$ ranges from $[-1, 1]$. A score close to $1$ indicates that the point is well-clustered, being close to its own cluster and far from others. A score close to $-1$, on the other hand, indicates that the point is closer to another cluster than to its own, suggesting poor clustering.

% With the tools of measuring separations and robustness in hand, we are now able to analyze the relationship between separations and robustness. We computed the Bhattacharya distance $D_B$ and Silhouette score $s$ for different models after aligning by different alignment methods and computed the ASR with respective to the adversarial prompts from the AdvBench datase~\citep{zou2023universal}. From the Fig. \ref{fig:correlation}, we can tell that the  Bhattacharya distance and  Silhouette score are negativily correlated with the ASR with the person correlation $-0.44$ and $-0.13$, indicating that LLMs that better distinguish safe and unsafe prompts are less vulnerable to adversarial prompts. 
\subsubsection{Comparing Alignment Methods}
%We further investigate how different alignment methods influence an LLM's ability to distinguish safe and harmful prompts. Bhattacharyya Distances and Silhouette Scores are again used to measure the separation of safe and unsafe prompts within the hidden representation space.
In this section, we compare how different alignment methods perform on the separation, clustering, and robustness metrics. The results are summarized in Table \ref{tab:separation}.

\textbf{Separation and Clustering :} KLDO and BCO consistently outperform KTO and DPO in terms of both the Bhattacharyya Distance ($D_B$) and the Silhouette Score ($s$). For instance, KLDO achieves the highest $D_B$ and $s$ for \emph{Llama2-7B-SFT} and \emph{Gemma2-2B}, while BCO leads for \emph{Llama3.2-1B} and \emph{Mistral-7B}, with KLDO as a close second. The persistent underperformance of DPO can be attributed to the less sensitive divergence it induces, as discussed in Sec. \ref{subsec: dpo_div}. For KTO, its reliance on the TV distance, which lacks strict convexity with respect to separation, limits its effectiveness. In contrast, the JS and KL divergences induced by BCO and KLDO (Fig. \ref{fig:dpo_sensitivity}) are more sensitive to larger separations between aligned and unaligned distributions, making BCO and KLDO better suited for improving alignment.

\textbf{Robustness (ASR):} KLDO and BCO generally achieve lower ASR percentages, indicating stronger robustness. For example, KLDO demonstrates the lowest ASR for \emph{Qwen2.5-1.5B} and \emph{Mistral-7B-v0.1}, as well as the second-lowest ASR for \emph{Gemma2-2B}. Even in cases where KLDO does not achieve the lowest ASR, it consistently demonstrates superior utility and win rates, as detailed in Sec. \ref{sec: rob vs utility}. This highlights KLDO's ability to strike a favorable balance between robustness and utility.

\textbf{Omission of Base Model ASR:} We omit ASR percentages for base models since they are text-completion models, not designed for instructions or Q\&A. However, our aligned models, fine-tuned with LoRA and high learning rates, successfully enable Q\&A capabilities. To illustrate this, we compare base and aligned models (DPO, KTO, KLDO, BCO) on adversarial (Appendix Table \ref{tab:sample_advbench}) and clean prompts (Appendix Table \ref{tab:sample_alpaca}). Base models merely reiterate prompts, making ASR percentages irrelevant for evaluation.
%\paragraph{Trends in Robustness} We also observe a general trend of lower ASR correlating with higher $D_B$, indicating that better separation between safe and unsafe prompts leads to improved robustness. This relationship is formalized in Section \ref{subsec: exp sep vs robust}.

%It is evident that BCO and KLDO exhbit an edge compared to KTO, DPO, achieve overall better separation and ASR. For instance, the KLDO-aligned Gemm2-2b model achieves a Bhattacharyya Distance of $10.13$ and a Silhouette Score of $0.61$, significantly outperforming DPO and KTO.
%This supports our earlier findings in Subsection \ref{subsec: dpo_div}, where we observed that $\mathbb{D}{\text{DPO}}$ is less sensitive to the separation between the two distributions due to its early saturation property. In contrast, $\mathbb{D}{\text{TV}}$, $\mathbb{D}{\text{KL}}$, and $\mathbb{D}{\text{JS}}$, which are associated with other alignment methods, exhibit greater sensitivity.
%As shown in Fig. \ref{fig:dpo_sensitivity}, this also explains why KTO demonstrates less separation. The normalized divergence plot of $\mathbb{D}{\text{TV}}$ shows smaller gradients at larger Bhattacharyya Distance scales compared to $\mathbb{D}{\text{KL}}$ and $\mathbb{D}{\text{JS}}$. This indicates that $\mathbb{D}{\text{TV}}$ is inherently less sensitive to separation than the other metrics.
%Similarly, for the Llama2-7b model, KLDO achieves a Bhattacharyya Distance of $4.42$ and a Silhouette Score of $0.61$. For BCO, it leads to the largest separation for Llama-3.2-1B and Mistral-7B-v0.1. By explicitly maximizing the KL divergence and JS divergence, KLDO and BCO effectively separates the aligned ($\cd^{+}$) and unaligned ($\cd^{-}$) distributions, which in turn results in a clearer distinction between safe and unsafe prompts in the LLM's hidden representation space.}

% From Table \ref{tab:separation}, it is evident that KLDO provides superior separation compared to DPO and KTO. For instance, the KLDO-aligned Gemm2-2b model achieves a Bhattacharyya Distance of $10.13$ and a Silhouette Score of $0.61$, significantly outperforming DPO, KTO, and BCO. Similarly, for the Llama2-7b model, KLDO achieves a Bhattacharyya Distance of $4.42$ and a Silhouette Score of $0.61$, again surpassing all other alignment methods. 
% This outcome can be attributed to KLDO's objective of directly optimizing the KL divergence between the aligned and unaligned distributions, $\infdiv{\cd^{+}}{\cd^{-}}$. By explicitly maximizing this divergence, KLDO effectively separates the aligned ($\cd^{+}$) and unaligned ($\cd^{-}$) distributions, which in turn results in a clearer distinction between safe and unsafe prompts in the LLM's representation space.

%However, for models like Llama-3.2-1B, Mistral-7B-v0.1, and Qwen2.5-1.5B, KLDO and BCO demonstrates better separation than DPO and KTO. Despite BCO achieving superior Bhattacharyya Distances, indicative of better separation and lower Attack Success Rates, BCO-aligned models are observed to hamper utility. This observation is discussed in more details in the Sec.~\ref{sec: rob vs utility}.
%Moreover, KLDO- and BCO-aligned models demonstrate lower ASR compared to DPO- and KTO-aligned models. Among them, BCO achieves the lowest ASR on Llama3.2-1B, Llama2-7B-sft, and Gemma2-2B, while on Mistral-7B-v0.1 and Qwen2.5-1.5B, KLDO achieves the lowest ASR. The general theme seems to be KLDO and BCO demonstrate superiority compared to DPO, KTO.
% \ZW{Added some sentences.}\todo{Do we want to discuss the ASR here? Just one short paragraph.}
\input{tabs/separation}

\subsubsection{Relationship between separation and ASR} \label{subsec: exp sep vs robust}
Using the values from Table \ref{tab:separation} across various configurations, we evaluate the Pearson correlation between the Bhattacharyya Distance (\(D_B\)) or Silhouette Score (\(s\)) and the Attack Success Rate (ASR). As shown in Fig. \ref{fig:correlation}, both metrics are negatively correlated with ASR. Specifically, \(D_B\) exhibits a Pearson correlation of \(-0.44\) (p-value = 0.027), while \(s\) shows a weaker correlation of \(-0.13\) (p-value = 0.52). These results indicate that LLMs with better separation between safe and unsafe prompts are generally more robust to adversarial prompts, aligning with our initial hypothesis. However, the clustering metric \(s\) is not statistically significant, which is expected since robustness is dictated by the separation between clusters rather than their density.

\phantomsection\label{phantom: sep vs robustbess}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figs/correlation.pdf} 
    \vspace{-4mm}
    \caption{Correlation between Bhattacharyya Distance/Silhouette Score and the logarithm of Attack Success Rate $\ln(ASR)$ .% The correlation between Bhattacharyya Distance and $\ln(ASR)$ is $-0.44$ with p-value $0.027$, while the correlation between Silhouette Score and $\ln(ASR)$ is $-0.13$ with $0.52$. The observed trends indicate that LLMs with smaller Bhattacharyya distances and Silhouette scores tend to suffer from higher Attack Success Rates.
    }
    \label{fig:correlation}
    % \vspace{-2mm}
\end{figure}

%From the above observations, there is a negative correlation between separation and robustness: with more separation between the safe and harmful prompts, the more likely the LLM rejects harmful prompts.

% Now that we have established a negative correlation between separation and robustness, we can infer that the separation of safe and unsafe prompts in the representation space can, to some extent, indicate the robustness of LLMs to adversarial prompts.


\subsection{Balance between Utility \& Robustness}
\label{sec: rob vs utility}

An effective alignment method must balance robustness and utility. To evaluate this, we measure win rates on the Alpaca Eval~\citep{dubois2024length} benchmark, comparing outputs from DPO, KTO, and BCO-aligned LLMs against KLDO-aligned models. Win rate reflects the proportion of responses preferred over KLDO-aligned outputs, as assessed by GPT-4o. Robustness is measured using ASR on the AdvBench~\citep{zou2023universal}.

\input{tabs/asr_winrate}

From Table \ref{tab:asr_winrate}, KLDO consistently balances low ASR (robustness) and high winrate (utility). For example, in \emph{Gemma2-2B}, KLDO achieves the second-lowest ASR while maintaining a significantly higher winrate. In \emph{Mistral-7B} and \emph{Qwen2.5-1.5B}, where KLDO achieves the lowest ASR, the winrate trade-off is minimal. Other methods often fail to achieve this balance. DPO shows the worst ASR and winrate in \emph{Llama2-7B-SFT}, while BCO, despite a slightly lower ASR in \emph{Gemma2-2B}, suffers a major winrate drop. KTO underperforms in both metrics for \emph{Mistral-7B}.

Overall, KLDO consistently strikes an optimal balance between robustness and utility, outperforming other methods across most configurations.
% Previously, we discussed the separation and robustness aspect of alignment. However, an alignment method that is highly robust at the expense of tremendous hit to utility is useless. In this section, we see the balance between robustness and utility across different alignment methods.

% To evaluate the utility of LLMs, we measure the win rates on prompts from the Alpaca Eval~\citep{dubois2024length} benchmark, a dataset designed to assess alignment fidelity of language models. Specifically, we compare the outputs of DPO, KTO, and BCO-aligned LLMs vs. KLDO-aligned outputs on the Alpaca Eval prompts.

% The win rate is calculated as the proportion of responses where the outputs from DPO, KTO, or BCO-aligned methods are preferred over those of the KLDO-aligned LLMs. GPT-4o serves as the evaluator, comparing both responses and assigning the probability that a given output from one model is more likely to be preferred by humans over the baseline (KLDO-aligned) model's output. And ASR is computed on the AdvBench~\citep{zou2023universal, wei2023jailbreak, huang2024trustllm, dubey2024llama} as in the previous section Sec.\ref{sec: separation}.

% From Table \ref{tab:asr_winrate}, we observe that KLDO consistently balances robustness (low ASR) and utility (high winrate), even in scenarios where it does not achieve the lowest ASR. For instance, in \emph{Gemma2-2B}, KLDO achieves the second-lowest ASR while maintaining a significantly higher winrate. In cases like \emph{Mistral-7B} and \emph{Qwen2.5-1.5B}, where KLDO achieves the lowest ASR, the trade-off in winrate is minimal, only a few percentage points. 

% This balance is unique to KLDO, as other methods often exhibit extreme compromises. For example, DPO shows the worst ASR and winrate for \emph{Llama2-7B-SFT}, while BCO, despite achieving a slightly lower ASR than KLDO in \emph{Gemma2-2B}, suffers from a significant drop in winrate. Similarly, KTO performs poorly in both ASR and winrate for \emph{Mistral-7B}. 

% These observations highlight KLDO's ability to consistently strike an optimal balance between robustness and utility, outperforming other methods in most configurations.
%From Table~\ref{tab:asr_winrate}, we observe a balance between utility and robustness in KLDO- and BCO-aligned models. For Llama3.2-1B, Llama2-7B, and Gemma2-2B, BCO demonstrates higher robustness to adversarial prompts (i.e., lower ASR) but lower utility (i.e., lower win rate) compared to KLDO. Conversely, for Mistral-7B-v0.1 and Qwen2.5-1.5B, KLDO exhibits greater robustness to adversarial prompts but slightly reduced utility. This highlights an inherent trade-off between robustness and utility across alignment methods. 


%Moreover from Table~\ref{tab:asr_winrate}, the KLDO-aligned Mistral-7B-v0.1 achieves an ASR of $1.92\%$, demonstrating significantly higher robustness compared to DPO's $87.69\%$, KTO's $40.38\%$. This improvement comes with only a minimal degradation in win rate, and the KLDO-aligned model even surpasses the utility of the KTO-aligned Mistral-7B-v0.1. Similarly, for Qwen2.5-1.5B, the KLDO-aligned model achieves an ASR of $0.19\%$, outperforming DPO ($4.62\%$), KTO ($0.96\%$), while also achieving a higher win rate than DPO. Aslo the BCO-aligned Mistral-7B-v0.1 and Qwen2.5-1.5B achieve a higher win rate but still a lower ASR compared to DPO and KTO.

%The advantage of KLDO and BCO arises from their optimal policies provide a smoother dependence on the reward, as demonstrated in Theorem~\ref{thm: alignment consistent}. The smoother dependence on the reward \(R(x,y)\) making KLDO and BCO particularly effective in aligning LLMs to produce high-quality responses, as it enables KLDO and BCO to distinguish between safe but less-preferred responses and safe, more-preferred responses, further enhancing its alignment capabilities.

% From Table~\ref{tab:asr_winrate}, we can tell KLDO-aligned Mistral-7B-v0.1 achieved the $1.92\%$ ASR which much robust compared to DPO's $87.69\%$ and KTO $40.38\%$ also KTO's $3.08\%$ with acceptable degradation in win rate, the KLDO-aligned Mistral-7B-v0.1 even achieves better utility than KTO-aligned Mistral-7B-v0.1. Also for Qwen2.5-1.5B, the KLDO-aligned model achieves $0.19\%$ ASR less than DPO's $4.62\%$, KTO's $0.96\%$ and BCO's $0.58\%$, even more the KLDO achieves better win rate that defeating DPO. 
% The advantage stems from the nature of the optimization objective of KLDO. KLDO is optimizing the $\infdiv{\cp}{\cq}$, which also shown in Thm \ref{thm: alignment consistent} and Table \ref{tab: optimal policy} is smoother depend on the reward \(R(x, y)\). This property is especially useful for aligning the LLM to high quality response. As the loss smoother dependence of the reward \(R(x, y)\) gives the KLDO ability to distinguish the safe but less preferred response and safe also more preferred response. 





% \input{tabs/alpaca}

\subsection{Compliance Refusal vs Preference Data}\label{sec:exp:CR}

In this section, we evaluate the impact of the Compliance Refusal Dataset versus the Preference Dataset in improving LLMs' ability to distinguish safe and unsafe prompts, as suggested by Theorem \ref{thm: separation}.

\input{tabs/preference_compliance}

We aligned \emph{Llama3.2-1B} on both datasets and measured the Bhattacharyya distance between safe and unsafe prompt representations, along with the ASR. Results in Table \ref{tab:compliance_preference_compare} show that alignment with the Compliance Refusal Dataset significantly increases the Bhattacharyya distance, achieving better separation between safe and unsafe prompts while imporoving robustness across the board (decreasing the ASR \%). This aligns with Theorem \ref{thm: separation} and highlights the advantages of Compliance Refusal Dataset for safety alignment.

%These findings suggest that incorporating the Compliance Refusal Dataset into the training process significantly enhances the model's ability to distinguish safe prompts from unsafe ones, as evidenced by the substantial increase in Bhattacharyya distance. This improved separation in the representation space is critical for reducing the Attack Success Rate (ASR), as observed in the previous section. By training on the Compliance Refusal Dataset, the model learns to better identify and handle unsafe prompts, thereby improving its robustness to harmful prompts. This highlights the importance of using Compliance Refusal Data as a key component in achieving effective safety alignment.

\section{Conclusion \& Future Direction}

We established a theoretical framework linking LLM alignment methods to divergence estimation, explaining the separation between safe and harmful prompts in latent space after alignment. This insight also led to development of KL divergence based alignment: KLDO, and a broader class of \( f \)-divergence optimizers (FDO) in theory. We also addressed weakness of DPO compared to other alignment methods from a divergence perspective. Empirical results validated our theory, and we demonstrated the importance of compliance-refusal datasets for safety alignment compared to the common preference based datasets used in RLHF. 

One possible future direction is that, the FDO class enables plug-and-play optimizers for custom choices of $f,g$ (Sec. \ref{subsec: fdo}). Moving forward, one can analyze the empirical properties of FDO-based methods, refining their effectiveness and robustness in LLM alignment.
\newpage
\section*{Impact Statement}
This paper presents work whose goal is to advance the field of alignment of LLMs. LLM alignment is crucial to encode human ethics, moral and preference into the models we train and interact with. This work, progresses our fundamental understanding towards alignment and potential guidelines to enhance safety. %There could be several other potential societal consequences of our work, none which we feel must be specifically highlighted here.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
%\section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Definitions}
\subsection{Basic Definitions}
\begin{definition}[KTO reference constant $z_0$]
    The reference constant \(z_0\) in KTO is defined as $\beta\cdot \infdiv{\ppol}{\pref}$. In practice it is estimated for each batch $B=B^+\cup B^-$ of aligned and unaligned samples as follows:
    \begin{equation*}
        \hat{z}_0=\beta\cdot \max\left(0,\frac{1}{m}\sum\limits_{(x,y)\in B}\ln\frac{\ppol(y|x)}{\pref(y|x)}\right)
    \end{equation*}
\label{defn: kto ref point}
\end{definition}
\begin{definition}[BCO reference constant $\delta$]
    The reference constant in BCO is defined as $$\delta = \frac{1}{2} \left(\underset{x,y \sim \cd^{+}}{\bE} r_\theta(x,y) + \underset{x,y \sim \cd^{-}}{\bE} r_\theta(x,y)\right)$$ In practice, the above is estimated by taking moving averages over batches $B_t=B_t^+\cup B_t^-$.
    \begin{equation*}
        \hat{\delta}_t= \hat{\delta}_{t-1}\cdot(1-\alpha) +\alpha\cdot \frac{1}{2}\left(\sum_{(x,y)\in B_t}r_\theta(x,y)\right)
    \end{equation*}
    \label{defn: bco ref point}
\end{definition}
\begin{definition}[$f$-Divergence]
    \[
D_f(\mathcal{P} \|\mathcal{Q}) = \mathbb{E}_{v \sim \mathcal{Q}} \left[ f\left( \frac{p_{\mathcal{P}}(v)}{p_{\mathcal{Q}}(v)} \right) \right],
\]
where \(f: \mathbb{R}_+ \to \mathbb{R}\) is a convex function with \(f(1) = 0\).
\label{defn: f-div}
\end{definition}

\begin{definition}[Convex Conjugate]
    \[
f^*(t) = \sup_{u \in \mathbb{R}} \{ ut - f(u) \},
\]
where \(f^*(t)\) is the convex conjugate of \(f\). Note that $f^*$ is also a convex function.
\label{defn: convex conjugate}
\end{definition}

\subsection{Overlap Coefficient for Gaussian}
\label{subsec: overlap defn}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{figs/overlap.png}
    \caption{Overlap between Gaussian Distributions}
    \label{fig:overlap}
\end{figure}
For two Gaussian distributions \(\mathcal{N}(0, 1)\) and \(\mathcal{N}(\mu, 1)\), the \textbf{overlap} can be expressed as:
\[
\text{Overlap} = \int_{-\infty}^\infty \min \left( p(x), q(x) \right) dx,
\]
where \(p(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2 / 2}\) and \(q(x) = \frac{1}{\sqrt{2\pi}} e^{-(x-\mu)^2 / 2}\).

This simplifies to a closed-form expression using the cumulative distribution function (CDF) \(\Phi(\cdot)\) of the standard normal distribution:
\[
\text{Overlap} = 2 \Phi\left(-\frac{\mu}{2}\right),
\]
where \(\Phi(z) = \frac{1}{2} \left(1 + \text{erf}\left(\frac{z}{\sqrt{2}}\right)\right)\).

The \textbf{Accuracy} is then defined as:
\[
\text{Accuracy} = 1 - \text{Overlap} = 1 - 2 \Phi\left(-\frac{\mu}{2}\right).
\]

\textbf{Interpretation:}
\begin{itemize}
    \item When \(\mu = 0\), the distributions are identical, giving \(\text{Overlap} = 1\) and \(\text{Accuracy} = 0\).
    \item As \(\mu \to \infty\), the distributions become perfectly separable, leading to \(\text{Overlap} \to 0\) and \(\text{Accuracy} \to 1\).
\end{itemize}

\section{Proofs}
% \begin{lemma}
%     Given $|r_\theta(x,y)|\leq m$ for some finite $m>0$, then for any $\theta$ the following inequalities hold:
%     \begin{align}
%         -\cl_{\text{DPO}}(\theta)&\leq \underset{x,y\sim \cd^{+}} {\bE}r_\theta(x,y) -  \underset{x,y\sim \cd^{-}} {\bE}{r_\theta(x,y)}\\
%         -\cl_{\text{DPO}_s}(\theta)&\leq \underset{x,y\sim \cd^{+}}{\bE} r_\theta(x,y) - \ln \left[\underset{x,y\sim \cd^{-}}{\bE}\exp{r_\theta(x,y)}+o\left(k^{-\sfrac{3}{2}}\right)\right]\\
%         -\cl_{\text{KTO}}(\theta)&= -1 +\underset{x,y\sim \cd^{+}}{\bE}\sigma(r_\theta(x,y)-z_0)-\underset{x,y\sim \cd^{-}}{\bE}\sigma(r_\theta(x,y)-z_0)\\
%         -\cl_{\text{BCO}}(\theta)&< \underset{x,y\sim \cd^{+}}{\bE}\ln (\sigma(r_\theta(x,y)-\delta))
%     -\underset{x,y\sim \cd^{-}}{\bE}\ln \sigma(r_\theta(x,y)-\delta)
%     \end{align}
%     \label{lem: loss bounds}
% \end{lemma}
% \begin{proof}
%     \textbf{DPO}
%     \begin{align*}
%     -\cL_{\text{DPO}}(\theta)&=\underset{x,y_w,y_l\sim \cd}{\bE}\ln\sigma (r_\theta(x,y_w)-r_\theta(x,y_l))\\
%     &= \underset{x,y_w,y_l\sim \cd} {\bE} \left(r_\theta(x,y_w) - \ln \left(e^{r_\theta(x,y_w)}+ e^{r_\theta(x,y_l)}\right)\right)\\
%     &\leq  \underset{x,y_w,y_l\sim \cd} {\bE}\left(r_\theta(x,y_w) - {r_\theta(x,y_l)}\right)\\
%     &= \underset{x,y\sim \cd^{+}} {\bE}r_\theta(x,y) -  \underset{x,y\sim \cd^{-}} {\bE}{r_\theta(x,y)}
%     \end{align*}
%     \textbf{DPO}
%     \begin{align*}
%         -\cL_{\text{DPO}}\leq \underset{x,y\sim \cd^{+}} {\bE}r_\theta(x,y) -  \underset{x,y\sim \cd^{-}} {\bE}{r_\theta(x,y)}\leq \infdiv{\cd^{+}}{\cd^{-}}
%     \end{align*}
%     \textbf{DPO$_s$}
%     \begin{align*}
%         -\cL_{\text{DPO}_s}(\theta)&= \underset{x,y_w,\{y_{l_i}\}^k_1\sim \cd}{\bE}\ln\frac{\exp{r_\theta(x,y_w)}}{\exp{r_\theta(x,y_w)}+\sum_{i=1}^k\exp{r_\theta(x,y_{l_i})}}\\
%     &= \underset{x,y_w,\{y_{l_i}\}^k_1\sim \cd}{\bE} \left[r_\theta(x,y_w) - \ln\left(\exp{r_\theta(x,y_w)}+\sum_{i=1}^k\exp{r_\theta(x,y_{l_i})}\right)\right] \\
%     &\leq \underset{x,y\sim \cd^{+}}{\bE} r_\theta(x,y) - \underset{x,y_1\sim \cd^{-}, \{y_i\sim \cd^{-}|x\}}{\bE}\ln\left(\sum_{i=1}^k\exp{r_\theta(x,y_{l_i})}\right)\\
%     \intertext{\center Using Strong Law of large numbers }
%     &= \underset{x,y\sim \cd^{+}}{\bE} r_\theta(x,y) - \ln \left[k\left(\underset{x,y\sim \cd^{-}}{\bE}\exp{r_\theta(x,y)}+o\left(k^{-\sfrac{1}{2}}\right)\right)\right]\\
%     &\leq\underset{x,y\sim \cd^{+}}{\bE} r_\theta(x,y) - \ln \left[\underset{x,y\sim \cd^{-}}{\bE}\exp{r_\theta(x,y)}+o\left(k^{-\sfrac{1}{2}}\right)\right]-\ln k\\
%     &\leq \underset{x,y\sim \cd^{+}}{\bE} r_\theta(x,y) - \ln \left[\underset{x,y\sim \cd^{-}}{\bE}\exp{r_\theta(x,y)}+o\left(k^{-\sfrac{1}{2}}\right)\right]
%     \end{align*}
%     \textbf{KTO}
%     \begin{align*}
%          -\cL_{\text{KTO}}(\theta)&= -\left[\underset{x,y\sim \cd^{+}}{\bE}\left(1-\sigma(r_\theta(x,y)-z_0)\right)+\underset{x,y\sim \cd^{-}}{\bE}\left(1-\sigma(z_0-r_\theta(x,y))\right)\right]\\
%     &=-1 +\left[\underset{x,y\sim \cd^{+}}{\bE}\sigma(r_\theta(x,y)-z_0)-\underset{x,y\sim \cd^{-}}{\bE}\left(1-\sigma(z_0-r_\theta(x,y))\right)\right]\\
%     &=-1 + \left[\underset{x,y\sim \cd^{+}}{\bE}\sigma(r_\theta(x,y)-z_0)-\underset{x,y\sim \cd^{-}}{\bE}\sigma(r_\theta(x,y)-z_0)\right]
%     \end{align*}
%     \textbf{BCO}
%     \begin{align*}
%         -\cL_{\text{BCO}}(\theta)&=\underset{x,y\sim \cd^{+}}{\bE}\ln (\sigma(r_\theta(x,y)-\delta))
%     +\underset{x,y\sim \cd^{-}}{\bE}\ln (\sigma(\delta-r_\theta(x,y)))\\
%     &=\underset{x,y\sim \cd^{+}}{\bE}\ln (\sigma(r_\theta(x,y)-\delta))
%     +\underset{x,y\sim \cd^{-}}{\bE}\ln (1-\sigma(r_\theta(x,y)-\delta))\\
%     \intertext{\center by the fact that $\ln(1-a)<-\ln a$ for $a\in [0,1]$}
%     &<\underset{x,y\sim \cd^{+}}{\bE}\ln (\sigma(r_\theta(x,y)-\delta))
%     -\underset{x,y\sim \cd^{-}}{\bE}\ln \sigma(r_\theta(x,y)-\delta)
%     \end{align*}
% \end{proof}

\begin{proof}[Proof of Thm \ref{thm: divergence convergence}]  
\textbf{DPO}
\begin{align*}
    -\cL_{\text{DPO}}(\theta^*)=-\inf\limits_{\theta}\cL_{\text{DPO}}(\theta)&=\sup\limits_{\theta}-\cL_{\text{DPO}}(\theta)\\&=\underset{x,y_w,y_l\sim \cd}{\bE}\ln\sigma (r_\theta(x,y_w)-r_\theta(x,y_l))\\
    &= \underset{x,y_w,y_l\sim \cd} {\bE} \left(r_\theta(x,y_w) - \ln \left(e^{r_\theta(x,y_w)}+ e^{r_\theta(x,y_l)}\right)\right)\\
    &\leq  \underset{x,y_w,y_l\sim \cd} {\bE}\left(r_\theta(x,y_w) - {r_\theta(x,y_l)}\right)\\
    &= \underset{x,y\sim \cd^{+}} {\bE}r_\theta(x,y) -  \underset{x,y\sim \cd^{-}} {\bE}{r_\theta(x,y)}\\
    &=2m\cdot \sup\limits_{\theta} \underset{x,y\sim \cd^{+}} {\bE}\frac{r_\theta(x,y)}{2m} -  \underset{x,y\sim \cd^{-}} {\bE}\frac{r_\theta(x,y)}{2m}
    \text{ (Suppose $|r_\theta(x,y)|\leq m$ for some $m>0$.)}\\
    &=2m\cdot \tvdiv{\cd^{+}}{\cd^{-}},
    \end{align*}
    where the last step is because  $|r_\theta(x,y)|\leq m$, and we can use TV representation Eqn \ref{eqn: tv rep}, with $v=(x,y), \,T(x,y)=r_\theta(x,y)\cdot (2m)^{-1}, \, \cp=\cd^+,\cq=\cd^-$.
    
    Given the above, we have $-\cL_{\text{DPO}}(\theta^*)\leq 2m \cdot \tvdiv{\cd^{+}}{\cd^{-}} $. Or equivalently:
    $$\cL_{\text{DPO}}(\theta^*)=\Omega\left(- \tvdiv{\cd^{+}}{\cd^{-}} \right).$$

% \textbf{DPO$_s$}

% \begin{align*}
%     -\cL_{\text{DPO}_s}^*=-\inf\limits_{\theta}\cL_{\text{DPO}_s}(\theta)&=\sup\limits_{\theta}-\cL_{\text{DPO}_s}(\theta)
%     \\& \underset{\text{Lemma \ref{lem: loss bounds}}}{\leq} 
%    \sup\limits_{\theta}\underset{x,y\sim \cd^{+}}{\bE} r_\theta(x,y) - \ln \left[\underset{x,y\sim \cd^{-}}{\bE}\exp{r_\theta(x,y)}+o\left(k^{-\sfrac{1}{2}}\right)\right]\\
%    &=\sup\limits_{\theta}\underset{x,y\sim \cd^{+}}{\bE} r_\theta(x,y) - \ln \underset{x,y\sim \cd^{-}}{\bE}\exp{r_\theta(x,y)}-\ln \left[1+o\left(k^{-\sfrac{1}{2}}\right)\cdot \underset{x,y\sim \cd^{-}}{\bE}\exp{r_\theta(x,y)}\right]\\
%    &=\sup\limits_{\theta}\underset{x,y\sim \cd^{+}}{\bE} r_\theta(x,y) - \ln \underset{x,y\sim \cd^{-}}{\bE}\exp{r_\theta(x,y)}-\ln \left[1+o\left(k^{-\sfrac{1}{2}}\right)\right]
%     \intertext{\center given a sufficiently large $k$, and using KL representation Eqn \ref{eqn: kl rep} with $T(x,y)=r_\theta(x,y)$ }%where possible range of $r_\theta(x,y)$ is $(-\infty,\infty)$}
%     &=\infdiv{\cd^{+}}{\cd^{-}}
% \end{align*}
\textbf{KTO}
\phantomsection\label{proof: kto divergence}
\begin{align*}
    -\cL^{*}_{\text{KTO}}= \inf\limits_{\theta}\cL_{\text{KTO}}(\theta)&=\sup\limits_{\theta}-\cL_{\text{KTO}}(\theta)\\ 
    &= -\left[\underset{x,y\sim \cd^{+}}{\bE}\left(1-\sigma(r_\theta(x,y)-z_0)\right)+\underset{x,y\sim \cd^{-}}{\bE}\left(1-\sigma(z_0-r_\theta(x,y))\right)\right]\\
    &=-1 +\left[\underset{x,y\sim \cd^{+}}{\bE}\sigma(r_\theta(x,y)-z_0)-\underset{x,y\sim \cd^{-}}{\bE}\left(1-\sigma(z_0-r_\theta(x,y))\right)\right]\\
    &=-1 + \left[\underset{x,y\sim \cd^{+}}{\bE}\sigma(r_\theta(x,y)-z_0)-\underset{x,y\sim \cd^{-}}{\bE}\sigma(r_\theta(x,y)-z_0)\right]\\
    &=-1 +\sup\limits_\theta \left[\underset{x,y\sim \cd^{+}}{\bE}\sigma(r_\theta(x,y)-z_0)-\underset{x,y\sim \cd^{-}}{\bE}\sigma(r_\theta(x,y)-z_0)\right]\\
    &=-1 +\sup\limits_\theta \left[\underset{x,y\sim \cd^{+}}{\bE}\left(\sigma(r_\theta(x,y)-z_0)-\frac{1}{2}\right)-\underset{x,y\sim \cd^{-}}{\bE}\left(\sigma(r_\theta(x,y)-z_0)-\frac{1}{2}\right)\right]\\
    \intertext{\center Using TV representation Eqn \ref{eqn: tv rep}, with $v=(x,y), \cp=\cd^+, \cq=\cd^-, T(x,y)=\sigma(r_\theta(x,y)-z_0)-\frac{1}{2}$}
    &=-1 +\tvdiv{\cd^{+}}{\cd^{-}}.
\end{align*}

\textbf{BCO}
\phantomsection\label{proof: bco divergence}
\begin{align*}
    -\cL_{\text{BCO}}(\theta^*)= \inf\limits_{\theta}\cL_{\text{BCO}}(\theta)&=\sup\limits_{\theta}-\cL_{\text{BCO}}(\theta)\\
    &=\sup\limits_{\theta} \underset{x,y\sim \cd^{+}}{\bE}\ln (\sigma(r_\theta(x,y)-\delta))
    +\underset{x,y\sim \cd^{-}}{\bE}\ln (\sigma(\delta-r_\theta(x,y)))\\
    &=\sup\limits_{\theta} \underset{x,y\sim \cd^{+}}{\bE}\ln (\sigma(r_\theta(x,y)-\delta))
    +\underset{x,y\sim \cd^{-}}{\bE}\ln (1-\sigma(r_\theta(x,y)-\delta))\\
    &=-\ln4 +2\cdot \jsdiv{\cd^+}{\cd^-},
    \end{align*}
    where the last step if obtained using the variational representation Eqn. \ref{eqn: js rep} with $v=(x,y), \cp=\cd^+,\cq=\cd^-$ and $T(x,y)=\sigma(r_\theta(x,y)-\delta)$. Also, by Lemma \ref{lem: optimal T (i)} we know the optimality is reached.
\end{proof}
\begin{lemma}[Optimal $T$ for variational rep.] The variational representations mentioned in Eqns \ref{eqn: kl rep}, \ref{eqn: tv rep}, \ref{eqn: js rep}, \ref{eqn: f-div rep} , converge to their correponding divergences. Furthermore the optimal functionals $T^*$ are as follows:
\begin{align}
T^*_{\text{KL}}(v)&=\ln \frac{p_{\cp}(v)}{p_{\cq}(v)}+\const
    \label{eqn: optimal t kl}\\
    T^*_{\text{TV}}(v)&=\frac{1}{2}\cdot\sign \left(\frac{p_{\cp}(v)}{p_{\cq}(v)}-1\right) + \const
    \label{eqn:optimal t tv}\\
T^*_{\text{JS}}(v)&= \frac{p_{\cp(v)}}{p_{\cp}(v)+p_{\cq}(v)} \label{eqn: optimal t js}\\
T^*_f(v)&=f'\left(\frac{p_\cp(v)}{p_\cq(v)}\right).
\label{eqn: optimal t f-div}
%T^*_{\text{DPO}}(v)&=\ln \frac{p_{\cp}(v)}{p_{\cq}(v)}+\const
\end{align}
\label{lem: optimal T (i)}
\end{lemma}
   \begin{proof}
       \textbf{KL}\\
       We need to show the following:
       \begin{equation*}
           \infdiv{\cp}{\cq}=\sup\limits_{T}\underset{v\sim \cp}\bE T(v)-\ln \underset{v\sim\cq}{\bE} e^{T(v)}.
       \end{equation*}
       Define a Gibbs distribution $\cg$ with $ p_{\cg}(v)=\frac{p_{\cq}(v)\exp{T(v)}}{\underset{v\sim\cq}{\bE} \exp{T(v)}}$. 
       \begin{align*}
           0\leq\infdiv{\cp}{\cg}&=\underset{v\sim\cp}{\bE}\ln \frac{p_{\cp}(v)}{p_{\cg}(v)}\\
           &=\underset{v\sim\cp}{\bE}\ln \frac{p_{\cp}(v)}{p_{\cq}(v)}-\left[\underset{v\sim\cp}{\bE} T(v) - \ln \underset{v\sim\cq}{\bE} e^{T(v)}\right]\\
           &= \infdiv{\cp}{\cq}- \left[\underset{v\sim\cp}{\bE} T(v) - \ln \underset{v\sim\cq}{\bE} e^{T(v)}\right],
       \end{align*}
       where equality is attained when $\cg=\cp$ i.e., $T(v)=\ln \frac{p_{\cp}(v)}{p_{\cq}(v)}+\underbrace{\ln \underset{v\sim\cq}{\bE} e^{T(v)}}_{\const}$. Note that variational representation Eqn \ref{eqn: kl rep} is equivalent for any $T(v)$ up to a constant, i.e., RHS is equivalent for any $T(v)+\const$.\\
       
    \textbf{TV}
    \begin{align*}
        \tvdiv{\cp}{\cq}&=\sup\limits_{T:|T|\leq \sfrac{1}{2}}\underset{v\sim \cp}\bE T(v)-\underset{v\sim\cq}{\bE} T(v)\\
        &=\sup\limits_{T:|T|\leq \sfrac{1}{2}}\int T(v)\cdot\left(p_{\cp}(v)-p_{\cq}(v)\right)\, dv.
    \end{align*}
    The integral is maximized when $T^*(v)=\frac{1}{2}\cdot\sign{(p_{\cp}(v)-p_{\cq}(v))}$ or equivalently $\frac{1}{2}\cdot\sign \left(\frac{p_{\cp}(v)}{p_{\cq}(v)}-1\right)$ 
    To see it is indeed the total variation distance:
    \begin{align*}
        &\int T^*(v)\cdot\left(p_{\cp}(v)-p_{\cq}(v)\right)\, dv\\
        &=\int \frac{1}{2}\cdot\sign{(p_{\cp}(v)-p_{\cq}(v))}\cdot\left(p_{\cp}(v)-p_{\cq}(v)\right)\, dv\\
        &=\frac{1}{2}\int \left|p_{\cp}(v)-p_{\cq}(v)\right|\, dv\\
        &=\tvdiv{\cd^+}{\cd^-}.
    \end{align*}
\textbf{JS}
\begin{align*}
    &\sup\limits_{T:0\leq T\leq 1}\underset{v\sim \cp}\bE \ln T(v)-\underset{v\sim\cq}{\bE} \ln\left(1-T(v)\right)\\
    =&\sup\limits_{T:0\leq T\leq 1}\int \left[p_{\cp}(v)\ln T(v)+ p_{\cq}(v)\ln (1- T(v))\right]\,dv\text{ (The inner integral is maximized for $T^*(v)=\frac{p_{\cp}(v)}{p_{\cp}(v)+p_{\cq}(v)}$.)}\\
    &=\underset{v\sim \cp}{\bE}\ln \frac{p_{\cp}(v)}{p_{\cp}(v)+p_{\cq}(v)}
    +\underset{v\sim \cq}{\bE}\ln \frac{p_{\cq}(v)}{p_{\cp}(v)+p_{\cq}(v)}\\
    &=-\ln 4 + \infdiv{\cp}{\frac{\cp+\cq}{2}}+\infdiv{\cq}{\frac{\cp+\cq}{2}}\\
    &=-\ln 4 + 2\cdot\jsdiv{\cp}{\cq}  .
\end{align*}
\textbf{$f$-Divergence}
\begin{align*}
    \fdiv{\cp}{\cq}&=\sup\limits_{T:\Omega\to \effdom (f^*)}\underset{v\sim \cp}\bE T(v)-\underset{v\sim\cq}{\bE} f^*\circ T(v)\\
    T^*(v) &=\arg\sup\limits_{T:\Omega\to \effdom (f^*)} \underset{v\sim \cp}\bE T(v)-\underset{v\sim\cq}{\bE} f^*\circ T(v)\text{ ($T^*$ must satisfy the stationary condition.)}\\
    0&=\nabla_{T} \left[\underset{v\sim \cp}\bE T(v)-\underset{v\sim\cq}{\bE} f^*\circ T(v)\right]\\
    % \intertext{Under some standard regularity conditions, and dominated convergence theorem we can take derivative inside the expectation.}
 &\implies p_{\cp}(v)-{f^*}'(T(v))\cdot p_{\cq}(v)=0\\
 &\implies {f^*}'(T(v))=\frac{p_{\cp}(v)}{p_{\cq}(v)} \text{  (Convex conjugate, ${f^*}'(f'(u))=u$ for any $u$.)}\\
 &\implies T^*(v)=f'\left(\frac{p_{\cp}(v)}{p_{\cq}(v)}\right).
\end{align*}    
Due to the convexity of $f^*$ we know ${f^*}''\geq 0$ , inturn $-{f^*}''\leq 0$ making the second order condition negative.  Hence this stationary point is indeed a supremum.\\
To see that the representation is valid we substitute $T^*(v)$ back in the RHS and see if we get back $\fdiv{\cp}{\cq}$.
\begin{align*}
    &\underset{v\sim \cp}\bE T^*(v)-\underset{v\sim\cq}{\bE} f^*\circ T^*(v)=\underset{v\sim \cp}\bE f'\left(\frac{p_{\cp}(v)}{p_{\cq}(v)}\right) -\underset{v\sim\cq}{\bE} f^*\circ f'\left(\frac{p_{\cp}(v)}{p_{\cq}(v)}\right).
    \end{align*}
    Using definition of convex conjugate, we know $f^*\circ f'(u)=u\cdot f'(u)-f(u)$. Hence, we have:
    \begin{align*}
    \underset{v\sim \cp}\bE T^*(v)-\underset{v\sim\cq}{\bE} f^*\circ T^*(v)&=\underset{v\sim \cp}\bE f'\left(\frac{p_{\cp}(v)}{p_{\cq}(v)}\right) -\underset{v\sim\cq}{\bE}\left[  \frac{p_{\cp}(v)}{p_{\cq}(v)}\cdot f'\left(\frac{p_{\cp}(v)}{p_{\cq}(v)}\right)-f\left(\frac{p_{\cp}(v)}{p_{\cq}(v)}\right)\right]\\
    &=\underset{v\sim\cq}{\bE}f\left(\frac{p_{\cp}(v)}{p_{\cq}(v)}\right)=\fdiv{\cp}{\cq}.
\end{align*}
   \end{proof} 

\begin{proof}[Proof of Thm \ref{thm: alignment consistent}]
From Lemma \ref{lem: optimal T (i)} we know the optimal $T^*$ for each divergence. For each alignment method BCO, KTO, KLDO, FDO ($f,g$) we know what is the corresponding functional $T$ in terms of the reward $r_\theta(x,y)$. Hence, simplifying we can get closed form expressions of $\ppolo$ using Lemma \ref{lem: optimal T (i)}.

    % \textbf{DPO}
    
    % \begin{align*}
    %     -\cL_{\text{DPO}}^*=\sup\limits_{\theta}-\cL_{\text{DPO}}(\theta)&\underset{\text{Lemma \ref{lem: loss bounds}}}{\leq} 2m\cdot \sup\limits_{\theta} \underset{x,y\sim \cd^{+}} {\bE}\underbrace{\frac{r_\theta(x,y)}{2m}}_{T(x,y)} -  \underset{x,y\sim \cd^{-}} {\bE}\frac{r_\theta(x,y)}{2m}\\
    % &=2m\cdot \tvdiv{\cd^{+}}{\cd^{-}}=2m\cdot \underset{x}{\bE}\tvdiv{\cd^{+}\mid x}{\cd^{-} \mid x}\\
    % \intertext{From Lemma \ref{lem: optimal T (i)} we know optimal $T^*(y)=\frac{r_{\theta^*}(x,y)}{2m}\underset{\const}{=}\frac{1}{2}\cdot\sign \left(\frac{p_{\cd^{+}}(y|x)}{p_{\cd^{-}}(y|x)}-1\right)$}\\
    % \implies r_{\theta^*}(x,y)&\underset{\const}{=}m\cdot\sign \left(\frac{p_{\cd^{+}}(y|x)}{p_{\cd^{-}}(y|x)}-1\right)=m\cdot\sign\left(R(x,y)-1\right)\\
    % \implies \ppolo(y|x)&\propto\pref(y|x)\cdot\exp{\left(\beta^{-1}\cdot m\cdot\sign\left(R(x,y)-1\right)\right)} 
    % \end{align*}
    % \textbf{DPO$_s$}
    % \begin{align*}
    %     -\cL_{\text{DPO}_s}^*=\sup\limits_{\theta}-\cL_{\text{DPO}_s}(\theta)&\underset{\text{Lemma \ref{lem: loss bounds}}}{\leq}  \sup\limits_{\theta}\underset{x,y\sim \cd^{+}}{\bE} r_\theta(x,y) - \ln \left[\underset{x,y\sim \cd^{-}}{\bE}\exp{r_\theta(x,y)}+o\left(k^{-\sfrac{3}{2}}\right)\right]\\
    % \intertext{\center With sufficiently large $k$, and using KL representation Eqn \ref{eqn: kl rep}, with $T(x,y)=r_\theta(x,y)$ }%where possible range of $r_\theta(x,y)$ is $(-\infty,\infty)$}
    % &=\infdiv{\cd^{+}}{\cd^{-}}=\underset{x}{\bE}\infdiv{\cd^{+}\mid x}{\cd^{-} \mid x}\\
    % \intertext{From Lemma \ref{lem: optimal T (i)} we know optimal $T^*(y)=r_{\theta^*}(x,y)\underset{\const}{=}\ln\frac{p_{\cd^{+}}(y|x)}{p_{\cd^{-}}(y|x)}$}\\
    % \implies r_{\theta^*}(x,y)&\underset{\const}{=}\ln\frac{p_{\cd^{+}}(y|x)}{p_{\cd^{-}}(y|x)}=\ln R(x,y)\\
    % \implies \ppolo(y|x)&\propto\pref(y|x)\cdot {R(x,y)}^{\frac{1}{\beta}}
    % \end{align*}
    % If $|r_{\theta^*}(x,y)|\leq m$ then , it forces a correponding bound of $\ppolo$ with a clip of $\pref\cdot e^{-\sfrac{m}{\beta}},\pref\cdot e^{\sfrac{m}{\beta}}$.\\
    \textbf{KTO:}
    From  \hyperref[proof: kto divergence]{proof of KTO} in Thm \ref{thm: divergence convergence}. We know that:
    \begin{align*}
    \intertext{\center TV representation Eqn \ref{eqn: tv rep} with , $v=(x,y)$ and $T^*(x,y)=\sigma(r_{\theta^*}(x,y)-z_0)-\frac{1}{2}$ implies}
        -\cL_{\text{KTO}}(\theta^*)
    &=-1 +\tvdiv{\cd^{+}}{\cd^{-}}\\
    &=-1+\underset{x}{\bE}\tvdiv{\cd^{+}\mid x}{\cd^{-} \mid x}\text{ (conditional property)}.
    \end{align*}
    From Lemma \ref{lem: optimal T (i)} (Eqn. \ref{eqn:optimal t tv}) we know optimal $T^*(y)=\sigma(r_{\theta^*}(x,y)-z_0)-\frac{1}{2}$ that maximizes the conditional divergence is $\frac{1}{2}\cdot\sign \left(\frac{p_{\cd^{+}}(y|x)}{p_{\cd^{-}}(y|x)}-1\right)=\frac{1}{2}\cdot\sign \left(R(x,y)-1\right)$ up to a constant, thus
    \begin{align*}
    &\implies \sigma(r_{\theta^*}(x,y)-z_0)\underset{\const}{=}\frac{1}{2}\left[\sign{(R(x,y)-1)}+1\right] \text{( $\sigma^{-1}(u)=\ln\frac{u}{1-u}$)}\\
    &\implies r_{\theta^*}(x,y)-z_0 \underset{\const}{=} \ln \left(\frac{1+\sign{[R(x,y)-1]}}{1-\sign{[R(x,y)-1]}}\right)\\
    &\implies \ppolo(y|x) = Z(x)^{-1}\cdot\pref(y|x)\cdot e^{\sfrac{z_0}{\beta}}\cdot \left(\frac{1+\sign{[R(x,y)-1]}}{1-\sign{[R(x,y)-1]}}\right)^{\frac{1}{\beta}}.
    \end{align*}
    \textbf{BCO:} From the \hyperref[proof: bco divergence]{proof of BCO} in Thm \ref{thm: divergence convergence}. We know that JS rep. Eqn \ref{eqn: js rep} with, $v=(x,y)$ and $T^*(x,y)=\sigma(r_{\theta^*}(x,y)-\delta)$ implies
    \begin{align*}
    -\cL_{\text{BCO}}(\theta^*)&= -\ln 4 + 2\cdot\jsdiv{\cd^+}{\cd^-}
    \end{align*}
     Using the conditional property, we have 
     \begin{align*}
    -\cL_{\text{BCO}}(\theta^*)&=-\ln 4 +2\cdot \underset{x}{\bE}\jsdiv{\cd^+\mid x}{\cd^- \mid x},
    \end{align*}
    From Lemma \ref{lem: optimal T (i)} (Eqn. \ref{eqn: optimal t js}) we know optimal $T^*(y)=\sigma(r_{\theta^*}(x,y)-\delta)$ that maximizes the conditional divergence is $\frac{p_{\cd^+}(y|x)}{p_{\cd^+}(y|x)+p_{\cd^-}(y|x)}=\frac{R(x,y)}{R(x,y)+1}$. Thus we get
    \begin{align*}
    &\sigma(r_{\theta^*}(x,y)-\delta) =\frac{R(x,y)}{R(x,y)+1}\\
    &\implies r_{\theta^*}(x,y)-\delta = \ln R(x,y)\text{ ($\sigma^{-1}(u)=\ln\frac{u}{1-u}$)}\\
    &\implies \ppolo(y|x) =Z(x)^{-1}\cdot\pref(y|x)\cdot e^{\sfrac{\delta}{\beta}}\cdot {R(x,y)}^{\frac{1}{\beta}}
    \end{align*}

\textbf{KLDO:} By Eqn. \ref{eqn: kldo loss}, $-\cL_{\text{KLDO}}(\theta^*)$ is equivalent to the DV representation of KL-divergence (Eqn. \ref{eqn: kl rep}), with $v=(x,y), \cp=\cd^+,\cq=\cd^-$ and $T(x,y)=r_\theta(x,y)$ resulting in:
    \begin{align*}
        -\cL_{\text{KLDO}}(\theta^*)&=\infdiv{\cd^+}{\cd^-} \\
        &=\underset{x}{\bE}\infdiv{\cd^+\mid x}{\cd^-\mid x} \text{ (Conditional property)}.
        \end{align*}
        From Lemma \ref{lem: optimal T (i)} (Eqn. \ref{eqn: optimal t kl}) we know optimal $T^*(y)=r_{\theta^*}(x,y)$ that maximizes the conditional divergence is $\ln \frac{p_{\cd^+}(y|x)}{p_{\cd^+}(y|x)}=\ln R(x,y)$. Thus
        \begin{align*}
        \implies r_{\theta^*}(x,y)&= \ln R(x,y)\\
        \implies \ppolo(y|x) &=Z(x)^{-1}\cdot\pref(y|x)\cdot {R(x,y)}^{\frac{1}{\beta}}
    \end{align*}
\textbf{FDO:} By Eqn. \ref{eqn: fdo}, $-\cL_{\underset{(f, g)}{\text{FDO}}}(\theta^*)$ is equivalent to the $f$-divergence representation  (Eqn. \ref{eqn: f-div rep}), with $v=(x,y), \cp=\cd^+,\cq=\cd^-$ and $T(x,y)=g(r_\theta(x,y))$ resulting in:
    \begin{align*}
        -\cL_{\underset{(f, g)}{\text{FDO}}}(\theta^*)&=\fdiv{\cd^+}{\cd^-} \\
        &=\underset{x}{\bE}\fdiv{\cd^+\mid x}{\cd^-\mid x}\text{ (Conditional property)}.
        \end{align*}
        From Lemma \ref{lem: optimal T (i)} (Eqn. \ref{eqn: optimal t f-div}) we know optimal $T^*(y)=r_{\theta^*}(x,y)$ that maximizes the conditional divergence is $f'\left( \frac{p_{\cd^+}(y|x)}{p_{\cd^+}(y|x)}\right)=f'(R(x,y))$. thus,
        \begin{align*}
        &\implies g(r_{\theta^*}(x,y))= f' (R(x,y))\\
        &\implies \ppolo(y|x) =Z(x)^{-1}\cdot\pref(y|x)\cdot e^{\frac{g^{-1}\circ f'(R(x,y))}{\beta}}.
    \end{align*}
    To see the above probability is actually non-decreasing in $R(x,y)$, note that ${g^{-1}}'(u)>0$ as $g\circ g^{-1}(u) =u\implies {g^{-1}}'(u)=\frac{1}{g'(g^{-1}(u))}>0$ (monotonicity of $g(u)$). Combined with the fact that $f''\geq0$ as $f$ is convex. We know that $h(R(x,y))=\exp(\beta^{-1}\cdot g^{-1}\left(f'(R(x,y))\right)$ is non-decreasing.

  
\end{proof}

\begin{proof}[Proof of Thm \ref{thm: separation}]
    Given an alignment method is consistent, we have $\ppolo(y|x)=Z(x)^{-1}\cdot\pref(y|x)\cdot h(R(x,y))$ where $h:\reals\to\reals$ is a non-constant and non-decreasing function.\\
    %We will explicitly prove $p^{\text{CR}}(z=1 \mid x: z_x=1, \theta^*) \geq \,p^{\text{Pref}}(z =1\mid x: z_x=1, \theta^*) > 0.5$, when $x$ is a safe response, the proof for $t,z_x=0$ when $x$ is harmful is identical.\\
    It is enough to show that $p^{\text{CR}}(z=t\mid x:z_x=t, y, \theta^*)\geq p^{\text{Pref}}(z=t\mid x:z_x=t, y, \theta^*)>0.5$ for all $y\in \text{FR}(x)$. As if the prior is true then: \begin{align*}
        \sum\limits_{y\in \text{FR}(x)}p^{\text{CR}}(z=t\mid x:z_x=t, y, \theta^*)\cdot |\text{FR}(x)|^{-1}&\geq \sum\limits_{y\in \text{FR}(x)}p^{\text{Pref}}(z=t\mid x:z_x=t, y, \theta^*)\cdot |\text{FR}(x)|^{-1}>0.5\\
        p^{\text{CR}}(z=t \mid x: z_x=t, \theta^*) &\geq \,p^{\text{Pref}}(z =t\mid x: z_x=t, \theta^*) > 0.5\\
        \implies \hat{z}(x,\theta^*)&=z_x ,\, \forall x
    \end{align*} 
  Note that the conditional can be expressed as follows:   
    \begin{align*}       
    p(z=t\mid x:z_x=t, y, \theta^*)&=\frac{\ppolo(y\mid x,z=t)}{\sum\limits_{t'\in\{0,1\}}\ppolo(y\mid x,z=t')}\\
    &=\left(1+\frac{\ppolo(y\mid x,z=1-t)}{\ppolo(y\mid x,z=t)}\right)^{-1}
    \end{align*}
    \textbf{Classification of Safe Responses}
    If $x$ is a safe response, i.e. $z_x=1$ then:
    $$\text{FR}(x)=\{y:\sfrac{\mathcal{C}(y|x)}{\mathcal{R}(y|x)}\geq 1\}.$$
%    $$\text{FR}^{\text{Pref}}(x)=\{y:\sfrac{\cd^{+}(y|x)}{\cd^{-}(y|x)}\geq 1\mid z_x=1\}=\{y:\sfrac{\mathcal{C}(y|x)}{\mathcal{C}(y|x)}\geq 1\mid z_x=1\}=\{y\in \cy\}$$
In addition,
    \begin{align}
        p^{\text{CR}}(z=1\mid x:z_x&=1, y, \theta^*)=\left(1+\frac{\ppolo(y\mid x,z=0)}{\ppolo(y\mid x,z=1)}\right)^{-1}\underset{\text{CR-Data, Tab:} \ref{tab: data_model}}{=}\left(1+\frac{h(\sfrac{\mathcal{R}(y|x))}{\mathcal{C}(y|x)})}{h(\sfrac{\mathcal{C}(y|x)}{\mathcal{R}(y|x)})}\right)^{-1}\label{eqn: cr bound z=1}
        \geq\left(1+1\right)^{-1}=0.5
    \end{align}
    as $h$ is non-decreasing and $\sfrac{\mathcal{C}(y|x)}{\mathcal{R}(y|x)}\geq 1$ for $y\in \text{FR}(x)$. And
     \begin{align}
        p^{\text{Pref}}(z=1\mid x:z_x&=1, y, \theta^*)=\left(1+\frac{\ppolo(y\mid x,z=0)}{\ppolo(y\mid x,z=1)}\right)^{-1}\underset{\text{Pref-Data, Tab:} \ref{tab: data_model}}{=}\left(1+\frac{h(\sfrac{\mathcal{R}(y|x))}{\mathcal{C}(y|x)})}{h(1)}\right)^{-1}\label{eqn: pref bound z=1}\geq\left(1+1\right)^{-1}=0.5
    \end{align}
    as $h$ is non-decreasing and $\sfrac{\mathcal{C}(y|x)}{\mathcal{R}(y|x)}\geq 1$ for $\text{FR}(x)$.
    
    Also,  Eqn $\ref{eqn: pref bound z=1}\leq \ref{eqn: cr bound z=1}$ as $h$ is non-decreasing. Hence, $p^{\text{CR}}(z=1\mid x:z_x=1, y, \theta^*)\geq p^{\text{Pref}}(z=1\mid x:z_x=1, y, \theta^*)>0.5$.

    \textbf{Classification of Harmful Responses}
    If $x$ is a safe response, i.e. $z_x=0$ then:
    $$\text{FR}(x)=\{y:\sfrac{\mathcal{R}(y|x)}{\mathcal{C}(y|x)}\geq 1\}.$$
    In addition,
%    $$\text{FR}^{\text{Pref}}(x)=\{y:\sfrac{\cd^{+}(y|x)}{\cd^{-}(y|x)}\geq 1\mid z_x=1\}=\{y:\sfrac{\mathcal{C}(y|x)}{\mathcal{C}(y|x)}\geq 1\mid z_x=1\}=\{y\in \cy\}$$
    \begin{align}
        p^{\text{CR}}(z=1\mid x:z_x&=1, y, \theta^*)=\left(1+\frac{\ppolo(y\mid x,z=1)}{\ppolo(y\mid x,z=0)}\right)^{-1}\underset{\text{CR-Data, Tab:} \ref{tab: data_model}}{=}\left(1+\frac{h(\sfrac{\mathcal{C}(y|x))}{\mathcal{R}(y|x)})}{h(\sfrac{\mathcal{R}(y|x)}{\mathcal{C}(y|x)})}\right)^{-1}\label{eqn: cr bound z=0}\geq\left(1+1\right)^{-1}=0.5
    \end{align}
    as $h$ is non-decreasing and $\sfrac{\mathcal{C}(y|x)}{\mathcal{R}(y|x)}\leq 1$ for $y\in \text{FR}(x)$. And
     \begin{align}
        p^{\text{Pref}}(z=1\mid x:z_x&=1, y, \theta^*)=\left(1+\frac{\ppolo(y\mid x,z=1)}{\ppolo(y\mid x,z=0)}\right)^{-1}\underset{\text{Pref-Data, Tab:} \ref{tab: data_model}}{=}\left(1+\frac{h(1)}{h(\sfrac{\mathcal{R}(y|x))}{\mathcal{C}(y|x)})}\right)^{-1}\label{eqn: pref bound z=0}
        \geq\left(1+1\right)^{-1}=0.5
    \end{align}
    as $h$ is non-decreasing and $\sfrac{\mathcal{C}(y|x)}{\mathcal{R}(y|x)}\leq 1$ for $\text{FR}(x)$.
    
    Also,  Eqn $(\ref{eqn: pref bound z=0})\leq (\ref{eqn: cr bound z=0})$ as $h$ is non-decreasing. Hence, $p^{\text{CR}}(z=0\mid x:z_x=0, y, \theta^*)\geq p^{\text{Pref}}(z=0\mid x:z_x=0, y, \theta^*)>0.5$.
\end{proof}

% \begin{proof}[Proof of Thm \ref{thm: FDO}]
%     $\cl_{\text{FDO}(f,g)}(\theta)$ at optimality ($\theta=\theta^*$) Eqn. \ref{eqn: fdo} is essentially the variational representation of $-\fdiv{\cd^+}{\cd^-}$ Eqn. \ref{eqn: f-div rep} with $T(x,y)=g(r_\theta(x,y))$. Where $g'(u)>0$ and $g^{-1}$ is well defined. $-\fdiv{\cd^+}{\cd^-}=-\underset{x}{\bE}\fdiv{\cd^+\mid x}{\cd^- \mid x}$ is optimized for the following $T^*(y)$\\
%     Using Lemma \ref{lem: T optimal (ii)} we know $T^*(y)=g(r_{\theta^*}(x,y))=f'\left(\frac{p_{\cd^+}(y|x)}{p_{\cd^-}(y|x)}\right)=f'\left(R(x,y)\right)$.
%     \begin{align*}
%         \implies r_{\theta^*}(x,y) = g^{-1}\left(f'(R(x,y))\right)\\
%         \implies \ppolo(y|x)\propto\pref(y|x)\exp(\beta^{-1}\cdot g^{-1}\left(f'(R(x,y))\right))
%         \intertext{Normalization doesn't break optimality just shifts the $T$, and due to convexity of $f^*$, the normalized probability is an optimal solution after the shift as well.}
%     \end{align*}
    
%\end{proof}
\newpage
\section{Experimental Details}
Link to our \href{https://anonymous.4open.science/r/KLDO-84F5/}{anonymous github repo} for implementation. 
\label{appex: experiment_details}


\paragraph{Data} We utilize the SafeAligner~\citep{huang2024safealigner} and Alpaca-GPT4-Data~\citep{peng2023instruction} datasets in our experiments. As described in \citep{huang2024safealigner}, the SafeAligner dataset includes $628$ unsafe prompts sourced from open platforms, with safe responses generated by GPT-4 and unsafe responses created by a fine-tuned Llama-3-8B-Instruct model designed to produce harmful content in response to unsafe prompts. The Alpaca-GPT4-Data dataset consists of 52,000 safe prompts from Alpaca\ citep{alpaca}, paired with aligned responses generated by GPT-4. We randomly sample $628$ prompts from Alpaca-GPT4-Data, and combined with the $628$ unsafe prompts from SafeAligner, we create a half-safe and unsafe set of prompts.

\paragraph{Training} We train LLMs using different alignment methods (DPO, KTO, BCO, KLDO) on the above data. The training spans 5 epochs with a learning rate of \(5 \times 10^{-5}\), a batch size of 32, \(\beta = 0.1\), and the Adam optimizer \citep{DBLP:journals/corr/KingmaB14, 8624183}. We apply Low-Rank Adaptation (LoRA) \citep{hu2022lora, Zhang_2023_ICCV, NEURIPS2023_1feb8787} with \(\alpha = 256\), rank = 64, and dropout = 0.05. Combining LoRA with a high learning rate proved highly effective for achieving strong alignment while requiring less computation compared to full parameter training. We perform all our training on 2 Nvidia A100-80 GB Gpus.

% In this section, we 
\subsection{Visalization Methodology \citep{lin-etal-2024-towards-understanding}}
\label{subsec: viz methd}
For each model, both safe and unsafe prompts are fed into the model, and the last-layer embeddings of the full prompts are extracted. These embeddings are then reduced to two dimensions using PCA for visualization purposes for each model separately.
\subsection{Metrics for Separation \& Clustering}
\label{subsec: metrics}
\textbf{Bhattacharyya Distance: }The Bhattacharyya Distance between two probability distributions $P, Q$  can be mathematically expressed as $D_B(P, Q) = -\ln\left(BC(P, Q)\right)$, where $BC(P, Q)$ is the Bhattacharyya coefficients and quantifies the overlap between the two distributions $P, Q$.

In our case, we estimate the hidden representations clusters by Gaussian distributions. Then use the closed form solution of $D_B$ between two gaussian clusters. The Gaussian fit qualitatively seems justified, looking at the visualizations (Fig. \ref{fig: viz qwen}, Sec. \ref{subsec: latent space viz}). %\todo{Even without assuming Gaussian distribution, we can do the following calculation. Is there any benefit in the Bhattacharyya Distance when using Gaussian? Or can we remove this sentence? Or do we need normality test first?}. 
Let $\widehat{\mu}_s$ and $\widehat{\Sigma}_s$ denote the sample mean and variance for safe prompts, and $\widehat{\mu}_u$ and $\widehat{\Sigma}_u$ represent the same for unsafe prompts. Then the Bhattacharyya Distance between safe and unsafe prompts LLM representations can be defined as the following:
\begin{align*}
    D_B = \frac{1}{8}\left(\widehat{\mu}_s-\widehat{\mu}_u\right)^{\top}\widehat{\Sigma}^{-1}\left(\widehat{\mu}_s-\widehat{\mu}_u\right)+\frac{1}{2}\ln{\left(\frac{|\widehat{\Sigma}|}{|\widehat{\Sigma}_s|*|\widehat{\Sigma}_u|}\right)}, 
\end{align*}
where $\widehat{\Sigma} = \frac{1}{2}\left( \widehat{\Sigma}_s+\widehat{\Sigma}_u\right)$. 

\textbf{Silhouette Score:} The Silhouette Score quantifies the relative distance of a data point to its own cluster compared to the other cluster. 
For the $i$-th data point, it is calculated as:
\begin{equation*}
    s(i) = \frac{b(i)-a(i)}{\max\left(a(i), b(i)\right)},
\end{equation*}
where:
\begin{align*}
    a(i) &= \frac{1}{|C_i|-1} \sum_{j \in C_i,  j\neq i}d(i, j), \\
    b(i) &= \min_{C \neq C_i}\frac{1}{|C|} \sum_{j\in C} d(i, j),
\end{align*}
and $d(i, j)$ is the $L_2$ distance between points $i$ and $j$.
Here, $a(i)$ represents the average distance between the $i$-th data point and all other points within its own cluster $C_i$,  while $b(i)$ is the minimum average distance from the $i$-th point to points in any other cluster $C$. The overall Silhouette Score $s$ is computed as the mean of $s(i)$ across all $N$ data points: $s = \sfrac{1}{N}\sum_{i=1,\dots,N} s(i).$

\newpage
\subsection{Sample Responses pre \& post Alignment:}
\label{subsec: sample responses post alignment}
\input{tabs/sample_alpaca}
\input{tabs/sample_advbench}
\newpage
\subsection{Latent Space Visualization}
\label{subsec: latent space viz}
Separation across harmful/safe prompts in latent space after alignment for different methods across various models:
\begin{figure*}[!ht]
    \centering
    \subfigure[Base]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/base_archangel_llama7b.png}
    }\hspace{-1em}
    \subfigure[DPO]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/dpo_llama7b_sft.png}
    }\hspace{-1em}
    \subfigure[KTO]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/kto_llama7b_sft.png}
    }\hspace{-1em}
    \subfigure[KLDO]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/kl-ma_llama7b_sft.png}
    }\hspace{-1em}
    \subfigure[BCO]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/bco_llama7b_sft.png}
    }
    \caption{\emph{Llama2-7b-sft}}
    \label{fig:viz llama-7b-sft}
\end{figure*}
\begin{figure*}[!ht]
    \centering
    \subfigure[Base]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/base_gemma.png}
    }\hspace{-1em}
    \subfigure[DPO]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/dpo_gemma.png}
    }\hspace{-1em}
    \subfigure[KTO]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/kto_gemma2.png}
    }\hspace{-1em}
    \subfigure[KLDO]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/kl-ma_gemma2.png}
    }\hspace{-1em}
    \subfigure[BCO]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/bco_gemma.png}
    }
    \caption{\emph{Gemma 2-2b}}
    \label{fig:viz gemma2}
\end{figure*}
\begin{figure*}[!ht]
    \centering
    \subfigure[Base]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/base_Mistral.png}
    }\hspace{-1em}
    \subfigure[DPO]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/dpo_mistral.png}
    }\hspace{-1em}
    \subfigure[KTO]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/kto_mistral.png}
    }\hspace{-1em}
    \subfigure[KLDO]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/kl-ma_mistral.png}
    }\hspace{-1em}
    \subfigure[BCO]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/bco_mistral.png}
    }
    \caption{\emph{Mistral-7B-v0.1}}
    \label{fig:viz mistral}
\end{figure*}
\begin{figure*}[!ht]
    \centering
    \subfigure[Base]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/base_Llama-3.2-1B.png}
    }\hspace{-1em}
    \subfigure[DPO]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/dpo_llama-3.2-1b.png}
    }\hspace{-1em}
    \subfigure[KTO]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/kto_llama-3.2-1b.png}
    }\hspace{-1em}
    \subfigure[KLDO]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/kl-ma_llama-3.2-1b.png}
    }\hspace{-1em}
    \subfigure[BCO]{
        \includegraphics[width=0.19\textwidth, trim=0 0 0 85, clip]{figs/bco_llama-3.2-1b.png}
    }
    \caption{\emph{Llama3.2-1B}}
    \label{fig:viz llama3.2-1b}
\end{figure*}

\end{document}



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Souepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
