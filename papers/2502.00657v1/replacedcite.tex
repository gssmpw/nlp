\section{Related Works}
\subsection{Alignment}
% There are many existing studies in LLM alignment. 
% We summarize some common literature about alignment in empirical and theoretical perspectives.
\paragraph{Empirical Studies} Various methods have been proposed to align LLMs with human preferences. For instance, RLHF with the BT and PL models was first introduced in ____ and ____, respectively. In RLHF, a reward model is trained and is further used in the alignment of the LLM. In contrast, DPO ____ designs its loss function (training objective) to avoid the need for a separate reward model. Later, BCO ____ and KTO ____ were proposed to further enhance alignment performance. In addition to the alignment methods mentioned above, several others have been developed to enhance performance in various ways. For example, ORPO ____ incorporates the SFT loss into DPO, and ____ uses a preference tree. Other techniques can be found in ____.

% BPO ____ enhances alignment by leveraging knowledge breadth and depth, while token-level regularization is explored in ____. Additionally, self-exploring and self-supervised methods ____, as well as sample-efficient algorithms ____, have been proposed.

% Other studies have conducted comprehensive experiments to better understand alignment. For example, ____ identifies practical challenges in alignment, while ____ explores scaling laws. ____ discusses the discrepancies between LLMs and LLM-based agents in the context of RLHF, and ____ investigates various factors that impact alignment performance.

% \begin{itemize}
%     \item DPO ____, PPO ____\yue{please help check this reference}, BCO ____, KTO ____,  RLHF ____.  RLHF using the BT and PL models was first introduced in ____ and ____ respectively. 
%     \item Enhancements of existing alignment methods: DPO with offset ____, ORPO ____, preference tree ____, multi-turn ____, merging ____, BPO leveraging knowledge breath and depth ____, token-level regularization ____, self-exploring and self-supervised method,____, sample-efficient algorithms ____, 
%     \item Works in identifying practical challenges in alignment: ____
%     \item Scaling law: ____
%     \item ____: discuss about the discrepancy between LLMs and LLM-based agents when considering RLHF.
%     \item ____ explored different factors which impact alignment performance.
% \end{itemize}
\vspace{-0.1in}
\paragraph{Theoretical Investigations} Beside the empirical studies, some other works focus on the theoretical properties of alignment and develop new algorithms based on their analysis. For example, ____ addresses preference bias in RLHF through preference matching. ____ accelerates convergence by applying momentum, and ____ proposes an algorithm that uses active learning to select the appropriate human for RLHF. 
% Additionally, ____ explores RLHF in offline, online, and hybrid settings.
Other studies can be found in ____.
% and ____ compares RLHF with traditional reinforcement learning. 
% From a topological perspective, ____ delves into RLHF, while ____ focuses on improving sample efficiency in RLHF.
Different from existing literature, we have a emphasis on the separation effect between aligned and unaligned data. 
% This focus leads to a key difference in our theory: previous literature offers limited understanding of how to connect alignment loss with divergence metrics, and we develop a concept of alignment consistency to connect them.


\subsection{Jailbreak Attack}
Aligned LLMs, despite their intended safety measures, can still produce harmful content, as highlighted in studies like ____, ____, and ____. Jailbreak attacks, which exploit vulnerabilities in these models, have been explored in ____ and ____. To design effective jailbreak attacks, several methods have been proposed, including GCG ____, AutoDAN ____, PAIR ____, and TAP ____. 
% In the real data experiments, we will utilize jailbreak attacks to examin the robustenss of the aligned LLMs.

% Although our theoretical framework connects divergence metrics with alignment methods, we will use jailbreak attacks to numerically relate divergence to the robustness of aligned LLMs. We conduct these experiments because the robustness, rather than divergence, is the ultimate goal of safety alignment.

% \subsection{Related Works}
% \subsection{Contributions}
% 1. Unifying framework for safety alignment in terms of divergence optimization, and explaining clustering phenomenon absed on saafety of the prompt. \\
% 2. Motivated by this framework we introduce KLDO, optimizes to maximize divergence between unaligned and aligned distribution, in effort to better separation/alignment\\
% 3. We introduce a separation metric based on the distance between the distributions that acts as a statistically significant indicator of the robustness of the model.\\
% 4. We advocate the use of Compliance Refusal datasets instead of preference datasets for safety alignment, in line with our theory, supported by experiments.