[
  {
    "index": 0,
    "papers": [
      {
        "key": "ziegler2019fine",
        "author": "Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey",
        "title": "Fine-tuning language models from human preferences"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "jung2024binary",
        "author": "Jung, Seungjae and Han, Gunsoo and Nam, Daniel Wontae and On, Kyoung-Woon",
        "title": "Binary classifier optimization for large language model alignment"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ethayarajh2024kto",
        "author": "Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe",
        "title": "Kto: Model alignment as prospect theoretic optimization"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "hong2024orpo",
        "author": "Hong, Jiwoo and Lee, Noah and Thorne, James",
        "title": "Orpo: Monolithic preference optimization without reference model"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "yuan2024advancing",
        "author": "Yuan, Lifan and Cui, Ganqu and Wang, Hanbin and Ding, Ning and Wang, Xingyao and Deng, Jia and Shan, Boji and Chen, Huimin and Xie, Ruobing and Lin, Yankai and others",
        "title": "Advancing llm reasoning generalists with preference trees"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "xiong2024building",
        "author": "Xiong, Wei and Shi, Chengshuai and Shen, Jiaming and Rosenberg, Aviv and Qin, Zhen and Calandriello, Daniele and Khalman, Misha and Joshi, Rishabh and Piot, Bilal and Saleh, Mohammad and others",
        "title": "Building math agents with multi-turn iterative preference learning"
      },
      {
        "key": "amini2024direct",
        "author": "Amini, Afra and Vieira, Tim and Cotterell, Ryan",
        "title": "Direct preference optimization with an offset"
      },
      {
        "key": "lu2024online",
        "author": "Lu, Keming and Yu, Bowen and Huang, Fei and Fan, Yang and Lin, Runji and Zhou, Chang",
        "title": "Online merging optimizers for boosting rewards and mitigating tax in alignment"
      },
      {
        "key": "wang2024bpo",
        "author": "Wang, Sizhe and Tong, Yongqi and Zhang, Hengyuan and Li, Dawei and Zhang, Xin and Chen, Tianlong",
        "title": "Bpo: Towards balanced preference optimization between knowledge breadth and depth in alignment"
      },
      {
        "key": "zhou2024t",
        "author": "Zhou, Wenxuan and Zhang, Shujian and Zhao, Lingxiao and Meng, Tao",
        "title": "T-REG: Preference Optimization with Token-Level Reward Regularization"
      },
      {
        "key": "zhang2024self",
        "author": "Zhang, Shenao and Yu, Donghan and Sharma, Hiteshi and Zhong, Han and Liu, Zhihan and Yang, Ziyi and Wang, Shuohang and Hassan, Hany and Wang, Zhaoran",
        "title": "Self-exploring language models: Active preference elicitation for online alignment"
      },
      {
        "key": "franken2024self",
        "author": "Fr{\\\"a}nken, Jan-Philipp and Zelikman, Eric and Rafailov, Rafael and Gandhi, Kanishk and Gerstenberg, Tobias and Goodman, Noah D",
        "title": "Self-supervised alignment with mutual information: Learning to follow principles without preference labels"
      },
      {
        "key": "yin2024entropy",
        "author": "Yin, Mingjia and Wu, Chuhan and Wang, Yufei and Wang, Hao and Guo, Wei and Wang, Yasheng and Liu, Yong and Tang, Ruiming and Lian, Defu and Chen, Enhong",
        "title": "Entropy law: The story behind data compression and llm performance"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wang2024bpo",
        "author": "Wang, Sizhe and Tong, Yongqi and Zhang, Hengyuan and Li, Dawei and Zhang, Xin and Chen, Tianlong",
        "title": "Bpo: Towards balanced preference optimization between knowledge breadth and depth in alignment"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhou2024t",
        "author": "Zhou, Wenxuan and Zhang, Shujian and Zhao, Lingxiao and Meng, Tao",
        "title": "T-REG: Preference Optimization with Token-Level Reward Regularization"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhang2024self",
        "author": "Zhang, Shenao and Yu, Donghan and Sharma, Hiteshi and Zhong, Han and Liu, Zhihan and Yang, Ziyi and Wang, Shuohang and Hassan, Hany and Wang, Zhaoran",
        "title": "Self-exploring language models: Active preference elicitation for online alignment"
      },
      {
        "key": "franken2024self",
        "author": "Fr{\\\"a}nken, Jan-Philipp and Zelikman, Eric and Rafailov, Rafael and Gandhi, Kanishk and Gerstenberg, Tobias and Goodman, Noah D",
        "title": "Self-supervised alignment with mutual information: Learning to follow principles without preference labels"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "yin2024entropy",
        "author": "Yin, Mingjia and Wu, Chuhan and Wang, Yufei and Wang, Hao and Guo, Wei and Wang, Yasheng and Liu, Yong and Tang, Ruiming and Lian, Defu and Chen, Enhong",
        "title": "Entropy law: The story behind data compression and llm performance"
      },
      {
        "key": "xie2024exploratory",
        "author": "Xie, Tengyang and Foster, Dylan J and Krishnamurthy, Akshay and Rosset, Corby and Awadallah, Ahmed and Rakhlin, Alexander",
        "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "anwar2024foundational",
        "author": "Anwar, Usman and Saparov, Abulhair and Rando, Javier and Paleka, Daniel and Turpin, Miles and Hase, Peter and Lubana, Ekdeep Singh and Jenner, Erik and Casper, Stephen and Sourbut, Oliver and others",
        "title": "Foundational challenges in assuring alignment and safety of large language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "rafailov2024scaling",
        "author": "Rafailov, Rafael and Chittepu, Yaswanth and Park, Ryan and Sikchi, Harshit and Hejna, Joey and Knox, Bradley and Finn, Chelsea and Niekum, Scott",
        "title": "Scaling laws for reward model overoptimization in direct alignment algorithms"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "li2024predicting",
        "author": "Li, Margaret and Shi, Weijia and Pagnoni, Artidoro and West, Peter and Holtzman, Ari",
        "title": "Predicting vs. acting: A trade-off between world modeling \\& agent modeling"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "ivison2024unpacking",
        "author": "Ivison, Hamish and Wang, Yizhong and Liu, Jiacheng and Wu, Zeqiu and Pyatkin, Valentina and Lambert, Nathan and Smith, Noah A and Choi, Yejin and Hajishirzi, Hannaneh",
        "title": "Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "schulman2017proximal",
        "author": "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",
        "title": "Proximal policy optimization algorithms"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "jung2024binary",
        "author": "Jung, Seungjae and Han, Gunsoo and Nam, Daniel Wontae and On, Kyoung-Woon",
        "title": "Binary classifier optimization for large language model alignment"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "ethayarajh2024kto",
        "author": "Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe",
        "title": "Kto: Model alignment as prospect theoretic optimization"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "ziegler2019fine",
        "author": "Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey",
        "title": "Fine-tuning language models from human preferences"
      },
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "ziegler2019fine",
        "author": "Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey",
        "title": "Fine-tuning language models from human preferences"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "amini2024direct",
        "author": "Amini, Afra and Vieira, Tim and Cotterell, Ryan",
        "title": "Direct preference optimization with an offset"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "hong2024orpo",
        "author": "Hong, Jiwoo and Lee, Noah and Thorne, James",
        "title": "Orpo: Monolithic preference optimization without reference model"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "yuan2024advancing",
        "author": "Yuan, Lifan and Cui, Ganqu and Wang, Hanbin and Ding, Ning and Wang, Xingyao and Deng, Jia and Shan, Boji and Chen, Huimin and Xie, Ruobing and Lin, Yankai and others",
        "title": "Advancing llm reasoning generalists with preference trees"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "xiong2024building",
        "author": "Xiong, Wei and Shi, Chengshuai and Shen, Jiaming and Rosenberg, Aviv and Qin, Zhen and Calandriello, Daniele and Khalman, Misha and Joshi, Rishabh and Piot, Bilal and Saleh, Mohammad and others",
        "title": "Building math agents with multi-turn iterative preference learning"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "lu2024online",
        "author": "Lu, Keming and Yu, Bowen and Huang, Fei and Fan, Yang and Lin, Runji and Zhou, Chang",
        "title": "Online merging optimizers for boosting rewards and mitigating tax in alignment"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "wang2024bpo",
        "author": "Wang, Sizhe and Tong, Yongqi and Zhang, Hengyuan and Li, Dawei and Zhang, Xin and Chen, Tianlong",
        "title": "Bpo: Towards balanced preference optimization between knowledge breadth and depth in alignment"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "zhou2024t",
        "author": "Zhou, Wenxuan and Zhang, Shujian and Zhao, Lingxiao and Meng, Tao",
        "title": "T-REG: Preference Optimization with Token-Level Reward Regularization"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "zhang2024self",
        "author": "Zhang, Shenao and Yu, Donghan and Sharma, Hiteshi and Zhong, Han and Liu, Zhihan and Yang, Ziyi and Wang, Shuohang and Hassan, Hany and Wang, Zhaoran",
        "title": "Self-exploring language models: Active preference elicitation for online alignment"
      },
      {
        "key": "franken2024self",
        "author": "Fr{\\\"a}nken, Jan-Philipp and Zelikman, Eric and Rafailov, Rafael and Gandhi, Kanishk and Gerstenberg, Tobias and Goodman, Noah D",
        "title": "Self-supervised alignment with mutual information: Learning to follow principles without preference labels"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "yin2024entropy",
        "author": "Yin, Mingjia and Wu, Chuhan and Wang, Yufei and Wang, Hao and Guo, Wei and Wang, Yasheng and Liu, Yong and Tang, Ruiming and Lian, Defu and Chen, Enhong",
        "title": "Entropy law: The story behind data compression and llm performance"
      },
      {
        "key": "xie2024exploratory",
        "author": "Xie, Tengyang and Foster, Dylan J and Krishnamurthy, Akshay and Rosset, Corby and Awadallah, Ahmed and Rakhlin, Alexander",
        "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "anwar2024foundational",
        "author": "Anwar, Usman and Saparov, Abulhair and Rando, Javier and Paleka, Daniel and Turpin, Miles and Hase, Peter and Lubana, Ekdeep Singh and Jenner, Erik and Casper, Stephen and Sourbut, Oliver and others",
        "title": "Foundational challenges in assuring alignment and safety of large language models"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "rafailov2024scaling",
        "author": "Rafailov, Rafael and Chittepu, Yaswanth and Park, Ryan and Sikchi, Harshit and Hejna, Joey and Knox, Bradley and Finn, Chelsea and Niekum, Scott",
        "title": "Scaling laws for reward model overoptimization in direct alignment algorithms"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "li2024predicting",
        "author": "Li, Margaret and Shi, Weijia and Pagnoni, Artidoro and West, Peter and Holtzman, Ari",
        "title": "Predicting vs. acting: A trade-off between world modeling \\& agent modeling"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "ivison2024unpacking",
        "author": "Ivison, Hamish and Wang, Yizhong and Liu, Jiacheng and Wu, Zeqiu and Pyatkin, Valentina and Lambert, Nathan and Smith, Noah A and Choi, Yejin and Hajishirzi, Hannaneh",
        "title": "Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "xiao2024algorithmic",
        "author": "Xiao, Jiancong and Li, Ziniu and Xie, Xingyu and Getzen, Emily and Fang, Cong and Long, Qi and Su, Weijie J",
        "title": "On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization"
      }
    ]
  },
  {
    "index": 37,
    "papers": [
      {
        "key": "he2024accelerated",
        "author": "He, Jiafan and Yuan, Huizhuo and Gu, Quanquan",
        "title": "Accelerated Preference Optimization for Large Language Model Alignment"
      }
    ]
  },
  {
    "index": 38,
    "papers": [
      {
        "key": "liu2024dual",
        "author": "Liu, Pangpang and Shi, Chengchun and Sun, Will Wei",
        "title": "Dual active learning for reinforcement learning from human feedback"
      }
    ]
  },
  {
    "index": 39,
    "papers": [
      {
        "key": "",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 40,
    "papers": [
      {
        "key": "wang2024magnetic",
        "author": "Wang, Mingzhi and Ma, Chengdong and Chen, Qizhi and Meng, Linjian and Han, Yang and Xiao, Jiancong and Zhang, Zhaowei and Huo, Jing and Su, Weijie J and Yang, Yaodong",
        "title": "Magnetic Preference Optimization: Achieving Last-iterate Convergence for Language Models Alignment"
      },
      {
        "key": "xiong2024iterative",
        "author": "Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong, Han and Ji, Heng and Jiang, Nan and Zhang, Tong",
        "title": "Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint"
      },
      {
        "key": "wang2023rlhf",
        "author": "Wang, Yuanhao and Liu, Qinghua and Jin, Chi",
        "title": "Is rlhf more difficult than standard rl? a theoretical perspective"
      },
      {
        "key": "du2024exploration",
        "author": "Du, Yihan and Winnicki, Anna and Dalal, Gal and Mannor, Shie and Srikant, R",
        "title": "Exploration-driven policy optimization in rlhf: Theoretical insights on efficient data utilization"
      }
    ]
  },
  {
    "index": 41,
    "papers": [
      {
        "key": "wang2023rlhf",
        "author": "Wang, Yuanhao and Liu, Qinghua and Jin, Chi",
        "title": "Is rlhf more difficult than standard rl? a theoretical perspective"
      }
    ]
  },
  {
    "index": 42,
    "papers": [
      {
        "key": "qiu2024reward",
        "author": "Qiu, Tianyi and Zeng, Fanzhi and Ji, Jiaming and Yan, Dong and Wang, Kaile and Zhou, Jiayi and Han, Yang and Dai, Josef and Pan, Xuehai and Yang, Yaodong",
        "title": "Reward Generalization in RLHF: A Topological Perspective"
      }
    ]
  },
  {
    "index": 43,
    "papers": [
      {
        "key": "du2024exploration",
        "author": "Du, Yihan and Winnicki, Anna and Dalal, Gal and Mannor, Shie and Srikant, R",
        "title": "Exploration-driven policy optimization in rlhf: Theoretical insights on efficient data utilization"
      }
    ]
  },
  {
    "index": 44,
    "papers": [
      {
        "key": "zhou2023synthetic",
        "author": "Zhou, Jiawei and Zhang, Yixuan and Luo, Qianni and Parker, Andrea G and De Choudhury, Munmun",
        "title": "Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions"
      }
    ]
  },
  {
    "index": 45,
    "papers": [
      {
        "key": "hazell2023spear",
        "author": "Hazell, Julian",
        "title": "Spear phishing with large language models"
      }
    ]
  },
  {
    "index": 46,
    "papers": [
      {
        "key": "kang2024exploiting",
        "author": "Kang, Daniel and Li, Xuechen and Stoica, Ion and Guestrin, Carlos and Zaharia, Matei and Hashimoto, Tatsunori",
        "title": "Exploiting programmatic behavior of llms: Dual-use through standard security attacks"
      }
    ]
  },
  {
    "index": 47,
    "papers": [
      {
        "key": "wei2024jailbroken",
        "author": "Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob",
        "title": "Jailbroken: How does llm safety training fail?"
      }
    ]
  },
  {
    "index": 48,
    "papers": [
      {
        "key": "carlini2024aligned",
        "author": "Carlini, Nicholas and Nasr, Milad and Choquette-Choo, Christopher A and Jagielski, Matthew and Gao, Irena and Koh, Pang Wei W and Ippolito, Daphne and Tramer, Florian and Schmidt, Ludwig",
        "title": "Are aligned neural networks adversarially aligned?"
      }
    ]
  },
  {
    "index": 49,
    "papers": [
      {
        "key": "zou2023universal",
        "author": "Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt",
        "title": "Universal and transferable adversarial attacks on aligned language models"
      }
    ]
  },
  {
    "index": 50,
    "papers": [
      {
        "key": "liu2023autodan",
        "author": "Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei",
        "title": "Autodan: Generating stealthy jailbreak prompts on aligned large language models"
      }
    ]
  },
  {
    "index": 51,
    "papers": [
      {
        "key": "chao2023jailbreaking",
        "author": "Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric",
        "title": "Jailbreaking black box large language models in twenty queries"
      }
    ]
  },
  {
    "index": 52,
    "papers": [
      {
        "key": "mehrotra2023tree",
        "author": "Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin",
        "title": "Tree of attacks: Jailbreaking black-box llms automatically"
      }
    ]
  }
]