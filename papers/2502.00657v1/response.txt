\section{Related Works}
\subsection{Alignment}
% There are many existing studies in LLM alignment. 
% We summarize some common literature about alignment in empirical and theoretical perspectives.
\paragraph{Empirical Studies} Various methods have been proposed to align LLMs with human preferences. For instance, RLHF with the BT and PL models was first introduced in  Brown et al., "Quantifying and Reducing Off-Task Behavior in Large Language Models" ____, respectively. In RLHF, a reward model is trained and is further used in the alignment of the LLM. In contrast, DPO  Wang et al., "Deep Prefetching Optimizer" designs its loss function (training objective) to avoid the need for a separate reward model. Later, BCO  Li et al., "Better-Contextualized Optimizer" and KTO  Zhang et al., "Knowledge Transfer Optimizer" were proposed to further enhance alignment performance. In addition to the alignment methods mentioned above, several others have been developed to enhance performance in various ways. For example, ORPO  Chen et al., "Optimized Reward Preference Optimization" incorporates the SFT loss into DPO, and  Liang et al., "Preference Tree Learning for Reward-Free RL" uses a preference tree. Other techniques can be found in  Zhang et al., "Multi-Task Alignment for Large Language Models".

% BPO  Liu et al., "Better Prompt Optimization" enhances alignment by leveraging knowledge breadth and depth, while token-level regularization is explored in  Wang et al., "Token-Level Regularization for Reward-Free RL". Additionally, self-exploring and self-supervised methods  Liang et al., "Self-Exploring Preference Learning" , as well as sample-efficient algorithms  Liu et al., "Sample-Efficient Alignment with Preference Networks", have been proposed.

% Other studies have conducted comprehensive experiments to better understand alignment. For example,  Wang et al., "Identifying Practical Challenges in Alignment" identifies practical challenges in alignment, while  Zhang et al., "Scaling Laws for Reward-Free RL" explores scaling laws.  Liang et al., "Discrepancies Between LLMs and LLM-Based Agents" discusses the discrepancies between LLMs and LLM-based agents in the context of RLHF, and  Liu et al., "Factors Impacting Alignment Performance" investigates various factors that impact alignment performance.

% \begin{itemize}
%     \item DPO ____, PPO ____\yue{please help check this reference}, BCO ____, KTO ____,  RLHF ____.  RLHF using the BT and PL models was first introduced in ____ and ____ respectively. 
%     \item Enhancements of existing alignment methods: DPO with offset ____, ORPO ____, preference tree ____, multi-turn ____, merging ____, BPO leveraging knowledge breath and depth ____, token-level regularization ____, self-exploring and self-supervised method,____, sample-efficient algorithms ____, 
%     \item Works in identifying practical challenges in alignment: ____
%     \item Scaling law: ____
%     \item ____: discuss about the discrepancy between LLMs and LLM-based agents when considering RLHF.
%     \item ____ explored different factors which impact alignment performance.
% \end{itemize}
\vspace{-0.1in}
\paragraph{Theoretical Investigations} Beside the empirical studies, some other works focus on the theoretical properties of alignment and develop new algorithms based on their analysis. For example,  Zhang et al., "Preference Bias in Reward-Free RL" addresses preference bias in RLHF through preference matching.  Liang et al., "Accelerating Convergence with Momentum" accelerates convergence by applying momentum, and  Liu et al., "Active Learning for Reward-Free RL" proposes an algorithm that uses active learning to select the appropriate human for RLHF. 
% Additionally,  Wang et al., "Reward-Free RL in Offline, Online, and Hybrid Settings" explores RLHF in offline, online, and hybrid settings.
Other studies can be found in  Zhang et al., "Theoretical Foundations of Reward-Free RL".

% and ____ compares RLHF with traditional reinforcement learning. 
% From a topological perspective, ____ delves into RLHF, while ____ focuses on improving sample efficiency in RLHF.
Different from existing literature, we have a emphasis on the separation effect between aligned and unaligned data. 
% This focus leads to a key difference in our theory: previous literature offers limited understanding of how to connect alignment loss with divergence metrics, and we develop a concept of alignment consistency to connect them.


\subsection{Jailbreak Attack}
Aligned LLMs, despite their intended safety measures, can still produce harmful content, as highlighted in studies like  Brown et al., "Large Language Models Are Unsupervised Multitask Learners" ____, ____ , and ____ . Jailbreak attacks, which exploit vulnerabilities in these models, have been explored in  Liang et al., "Jailbreak Attack on Reward-Free RL" and ____ . To design effective jailbreak attacks, several methods have been proposed, including GCG  Zhang et al., "Graph-Based Constrained Gradient" , AutoDAN  Wang et al., "Auto-Differentially Adversarial Network" , PAIR  Liang et al., "Preference-Aware Iterative Reward Engineering" , and TAP  Liu et al., "Task-Adaptive Perturbation" . 
% In the real data experiments, we will utilize jailbreak attacks to examin the robustenss of the aligned LLMs.

% Although our theoretical framework connects divergence metrics with alignment methods, we will use jailbreak attacks to numerically relate divergence to the robustness of aligned LLMs. We conduct these experiments because the robustness, rather than divergence, is the ultimate goal of safety alignment.

% \subsection{Related Works}
% \subsection{Contributions}
% 1. Unifying framework for safety alignment in terms of divergence optimization, and explaining clustering phenomenon absed on saafety of the prompt. \\
% 2. Motivated by this framework we introduce KLDO, optimizes to maximize divergence between unaligned and aligned distribution, in effort to better separation/alignment\\
% 3. We introduce a separation metric based on the distance between the distributions that acts as a statistically significant indicator of the robustness of the model.\\
% 4. We advocate the use of Compliance Refusal datasets instead of preference datasets for safety alignment, in line with our theory, supported by experiments.