
\appendices


\section{Training Details}
\label{sec:details}
This section provides more details about the implementation and training of our method.

\subsection{Stage I}
\subsubsection{Canonical Initialization}
We unwrap the T-pose body onto a UV map, where each pixel stores a 3D position vector. The positional UV map, with a resolution of $(512 \times 512 \times 3)$, is used to initialize Gaussians in the canonical space, ensuring proper alignment with the bodyâ€™s structure. Additionally, a downsampled $(128 \times 128 \times 3)$ version of the positional UV map serves as input to the Gaussian decoder, aiding in reconstructing and refining the 3D representation. 
Furthermore, we use blend
\subsubsection{Training}
The training objectives in this stage focus on image losses and optimizations about Gaussian parameters.We set weights for each objective as $\lambda_{rgb} = 0.8$, $\lambda_{n} = 0.8$, $\lambda_{ssim} = 0.2$, $\lambda_{lpips} = 0.2$, $\lambda_{\Delta x} = 0.85$, $\lambda_{s} = 0.03$, $\lambda_{S} = 1$. 

\subsubsection{Pose Optimization} Our method leverages pose optimization from GaussianAvatar~\cite{hu2023gaussianavatar} for the In-the-wild dataset as a correction for fitted SMPL~\cite{SMPL:2015} pose parameters. We have omitted this functionality for the ZJU-Mocap dataset, as their ground truth pose is accurate. However, GausianAvatar keeps optimizing pose parameters for ZJU-Mocap dataset, which leads to inaccurate poses, especially for invisible parts. Please check the accompanying video results for more details. 
\subsection{Stage II} 
\subsubsection{Dual-space Optimization}
In this stage, we apply Dual-space optimization on top of visible appearance reconstruction to predict the invisible appearance. During training, each epoch is divided into three parts: 50\% for given view training and 50\% for Dual-space optimization. In Dual-space optimization, the weight of canonical optimization is treated as a hyperparameter, defaulting to 50\%. the The fine-tuning losses are added upon $\mathcal{L}_{StageI}$. We set $\lambda_{p} = 0.5 $ and  $\lambda_{SDS} = 0.3 $ initially. 

\subsubsection{Progressive Training}
We design a progressive training strategy in this stage, gradually diminishing the weight of SDS loss. This strategy is employed to enhance further the effectiveness and efficiency of the visible appearance reconstruction. Based on this strategy, the $\lambda_{SDS}$ is reduced gradually by following:
\begin{equation}
\lambda_{SDS}(t) = \lambda_{SDS,0} \cdot \frac{1}{2^{\lfloor \frac{t - t_{\text{0}}}{k} \rfloor}}
  \label{eq:prog.train}
\end{equation}
where $t$ and $t_{0}$ are the current epoch and starting epoch respectively, $k$ is the interval step of changing the weight. We set $t_{0} = 100$ and $k = 100$.

\subsection{Resolution}
The video resolution for the ZJU-Mocap (revised)\cite{liu2023zero1to3} and Monocap datasets is consistently maintained at $1024\times1024$ pixels, while MVHumanNet\cite{xiong2024mvhumannet} has a resolution of $2048\times1500$ pixels. For videos collected from the internet, the resolution ranges from 720p to 1080p. However, in Stage II, Zero123 only accepts $256\times256$ as input. Therefore, for SDS loss calculation, we crop the ground truth images based on their masks and resize them to $256\times256$.

 


% \subsection{Potential Social Impact} The creation and manipulation of highly realistic avatars raise ethical questions regarding privacy, consent, and misrepresentation. There is a risk of misuse, such as creating deceptive content or impersonating individuals without their permission. The widespread availability of realistic avatars could complicate identity verification processes in online environments. It may become more challenging to distinguish between real individuals and avatar representations, potentially undermining trust and security.
\begin{figure}[!t]
  \centering
  \includegraphics[width=1\linewidth]{Figures/Appendix_2.2.pdf}
    % \vspace{-1mm}
  \caption{Qualitative comparison results with SIFU~\cite{Zhang2024SIFU} and SITH~\cite{ho2024sith}.}
    % \vspace{-3mm}
  \label{fig:SIFU}
\end{figure}
\section{More Experiments}



\subsection{Comparison with Image-based Methods \label{resultImagebased}}

In this section, we compare our method with SIFU~\cite{Zhang2024SIFU}, SITH~\cite{ho2024sith}, and ELICIT~\cite{huang2022elicit}, all of which are single-image reconstruction techniques designed to synthesize unseen parts of human avatars.

SIFU proposes an approach to reconstruct clothed human avatars from single images. Qualitatively, as shown in Fig.~\ref{fig:SIFU}, this method can reconstruct decent geometry but fails to synthesize the texture of unseen parts of humans. SITH, similar to SIFU, is a method for single-image reconstruction. SITH can predict the texture of unseen parts of humans, but their generated textures contain unrealistic artifacts.
\begin{figure}[!t]
  \centering
  \includegraphics[width=1\linewidth]{Figures/Appendix_1.1.pdf}
    % \vspace{-2mm}
  \caption{Qualitative comparison results with ELICIT~\cite{huang2022elicit} on novel poses.}
    % \vspace{-2mm}
  \label{fig:ELICIT}
\end{figure}

\begin{table}[!t]
\centering
\renewcommand{\arraystretch}{1.5}

\begin{tabular}{c|c|ccc}
\hline
Dataset                    & Method & PSNR$\uparrow$&SSIM$\uparrow$&LPIPS$\downarrow$  \\ \hline
\multirow{3}{*}{MVHumanNet}   & SIFU   &19.29&0.9486&0.0706\\
                            & SITH   &19.68&0.9462&0.0699\\
                        &\textbf{Ours} & \textbf{20.98} & \textbf{0.9517} & \textbf{0.0553} \\ \hline  
\multirow{3}{*}{Monocap}   & SIFU   &18.96&0.9406&0.0659\\
                            & SITH   &19.06&0.9428&0.0.0673\\
                        &\textbf{Ours} & \textbf{21.16} & \textbf{0.9532} & \textbf{0.0549} \\ \hline  
\multirow{2}{*}{ZJU-Mocap(revised)} & ELICIT &19.23&0.9456&0.0.0689\\
                           &\textbf{Ours} & \textbf{20.82} & \textbf{0.9552} & \textbf{0.0569}  \\ \hline
\end{tabular}
\caption{Quantitative evaluation on MVHumanNet, ZJU-Mocap(revised), and Monocap datasets.}
\vspace{-3mm}
  \label{tab:exp}
\end{table}
ELICIT is a generative model that takes one image and a motion sequence as input to generate an animatable avatar. Qualitative results are shown in Fig.~\ref{fig:ELICIT}. For a fair comparison, since our method takes an image sequence as input, we are comparing the quality by synthesizing a novel pose that is not included in our inputs. Even though ELICIT can predict the unseen parts of humans, it shows blurred edges and floating artifacts while applying motions. Because only one image is used as input for ELICIT, the texture cannot be adapted to novel poses dynamically. In contrast, our method associates texture to different body parts across frames and can predict the correct texture for unseen parts robustly.

In Tab.~\ref{tab:exp}, we present the quantitative evaluation results. SIFU and SITH were tested on the Monocap dataset, while ELICIT was evaluated on the ZJU-Mocap(revised) dataset. The results demonstrate that our method consistently achieves superior performance compared to the state-of-the-art approaches, underscoring its efficacy and robustness.



% }