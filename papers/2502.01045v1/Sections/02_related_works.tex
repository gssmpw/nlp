
\section{Related work}



\subsection{Video-based Human Avatar Reconstruction}
Recently, video-based avatar reconstruction methods primarily rely on regression-based approaches~\cite{he2020geo,he2021arch++,huang2020arch,saito2019pifu,saito2020pifuhd,xiu2022icon,xiu2023econ,zheng2021pamir} or the explicit tracking of human bodies~\cite{habermann2020deepcap,habermann2019livecap,xu2018monoperfcap,alldieck2018video,casado2022pergamo,guo2021human,moon20223d}.
%However,\ZY{why however?} the performance of PIFu-based methods \ZY{why you only mention Pifu here??} is significantly constrained by the scarcity and difficulty of obtaining high-quality 3D training datasets. 
Since the prosperity of Neural Rendering~\cite{mildenhall2021nerf}, many works~\cite{li2022tava,li2023posevocab,liu2021neural,peng2021neural,peng2024animatable,wang2022arah,zheng2022structured,jiang2022selfrecon,jiang2022neuman,su2021nerf,su2022danbo,su2023npc,weng2022humannerf,guo2023vid2avatar,yu2023monohuman} try to combine neural representations with human reconstructions. These methods associate implicit neural fields on human templates like SMPL~\cite{SMPL:2015}. While neural representations have strong representation ability, they are slow in training. But other works~\cite{zhao2022high,huang2023efficient,zheng2023pointavatar,instant_nvr} additionally introduced explicit representations like meshes~\cite{zhao2022high,huang2023efficient}, and points~\cite{zheng2023pointavatar} to improve its efficiency. Yet, achieving high-quality reconstruction results using neural radiance fields still requires neural networks that are expensive to train and render. Recently, Gaussian Splatting~\cite{kerbl3Dgaussians} has emerged as a prominent technology, as it efficiently represents and renders complex scenes with reduced training time, without compromising quality for speed. Many recent works~\cite{lei2023gart,li2023animatable,yin2023humanrecon,zheng2023gps,hu2023gauhuman,jiang2023hifi4g,li2023human101,hu2023gaussianavatar,qian20233dgs,qian20233dgsavatar,zheng2024gpsgaussian,liu24-GVA,reloo,kim2024moconerf,pramishp2024iHuman} try to combine Gaussian Splatting in the avatar reconstruction, which allows efficient avatar rendering in real-time. In this paper, we focus on reconstructing human avatars from monocular video. In GaussianAvatar~\cite{hu2023gaussianavatar}, 3D Gaussians are integrated with SMPL~\cite{SMPL:2015} to explicitly represent humans in various poses and clothing styles. SplattingAvatar~\cite{shao2024splattingavatar} embeds Gaussians onto human triangle meshes, forming a hybrid representation that significantly enhances rendering speed. Furthermore, ExAvatar~\cite{moon2024exavatar} extends this representation to reconstruct animatable hand poses and facial expressions.
However, those methods require the input video to have full-view coverage of the human body and failed to generate unseen parts in the monocular video.
%\ZY{here need to mention some papers for single video reconstruction like Guess the Unseen?}

% In this paper, we also aim at the task of reconstructing human avatars from a monocular video. The most relevant work is GaussianAvatar~\cite{hu2023gaussianavatar}. While we adopt a similar structure, our key innovation is using a diffusion model to hallucinate unseen parts. In comparison, baseline methods can only reconstruct visible parts of the humans.

\subsection{Diffusion Models for Human Avatars}

Pioneer works in avatar generation~\cite{hong2022avatarclip} resort to generate avatars from CLIP features~\cite{radford2021learning}. Recently, diffusion models~\cite{ho2020denoising} show strong ability in learning complex data distributions for data generation. Some works~\cite{zhang2023avatarverse,huang2024dreamwaltz,kolotouros2024dreamhuman,zeng2023avatarbooth,cao2023dreamavatar,jiang2023avatarcraft,zhang2023styleavatar3d,kim2023chupa,cao2023guide3d,mendiratta2023avatarstudio,liao2023tada,wang2023disentangled,liu2023humangaussian,xu2023seeavatar,wang2023humancoser} directly extend the SDS loss~\cite{poole2022dreamfusion} to generate human avatars from text prompts. MVHuman~\cite{jiang2023mvhuman} extends this framework to generate human avatars through multiview diffusion, while HumanNorm~\cite{huang2023humannorm} integrates it with normal map generation. Additionally, HumanNorm~\cite{hu2023humanliff} directly enables 3D human generation, benefiting from tri-plane features. Some other works~\cite{huang2023tech,svitov2023dinar,alldieck2022photorealistic,zhang2023humanref,albahar2023single,ho2024sith,chen2024ghgsingleview,pan2024hsgsingleimagehuman,sun2024occfusion} generate a completed human avatar from a single-view image using diffusion models. 
While these single-view avatar generation techniques produce avatars from single images, directly extending them to generate dynamic humans from monocular videos results in poor rendering quality for dynamic human actions. In contrast, our approach leverages the SDS loss to inpaint the unseen parts of the dynamic human body from a monocular video, with careful consideration of time coherence, consistency, and dynamics. % Instead, our method can more robustly capture details and achieve better rendering quality for dynamic humans.

