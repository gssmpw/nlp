\newcommand{\centered}[1]{\begin{tabular}{l} #1 \end{tabular}}


\begin{figure*}[!t]
\centering
\includegraphics[width=1\textwidth]{Figures/exp_other.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\caption{Qualitative comparison on four datasets. We compare the novel view synthesis quality with HumanNeRF~\cite{weng2022humannerf}, Instant-NVR~\cite{instant_nvr},  SplattingAvatar~\cite{shao2024splattingavatar}, ExAvatar~\cite{moon2024exavatar} and GaussianAvatar~\cite{hu2023gaussianavatar}.}
\label{fig: qualitative1}
\vspace{-3mm}
\end{figure*}


\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{Figures/exp_gtu.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\vspace{-3mm}
\caption{Qualitative comparison on three datasets. We compare the novel view synthesis quality with GuessTheUnseen~\cite{lee2024gtu}.}
\label{fig: qualitative2}
\vspace{-3mm}
\end{figure*}


\begin{table*}[!t]
 
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{c|ccc|ccc|ccc}
\hline
\multirow{2}{*}{} & \multicolumn{3}{c|}{ZJU-Mocap(revised)} & \multicolumn{3}{c|}{MVhumanNets} & \multicolumn{3}{c}{Monocap} \\ 
                  & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ \\ \hline
HumanNeRF         &---&---&---&19.21&0.9456&0.0715&19.38&0.9416&0.0706\\ 
Instant-NVR       &19.90&0.9458&0.0630&---&---&---&---&---&---\\
SplattingAvatar   &19.37&0.9436&0.0702&19.49&0.9467&0.0689&19.51&0.9444&0.0697\\
EXAvatar          &19.65&0.9449&0.0678&19.69&0.9470&0.0671&19.65&0.9475&0.0676\\
GaussianAvatar    &19.50&0.9434&0.0687&19.70&0.9471&0.0645&19.67&0.9489&0.0635\\
GuessTheUnseen    &20.06&0.9493&0.0615&---&---&---&20.56&0.9502&0.0598\\
 \textbf{Ours}    &\textbf{20.82}&\textbf{0.9552}&\textbf{0.0569}&\textbf{20.98}&\textbf{0.9517}&\textbf{0.0553}& \textbf{21.16}& \textbf{0.9532}& \textbf{0.0549}\\ \hline
\end{tabular}
\caption{Quantitative evaluation on ZJU-Mocap(revised), MVHumanNet, and Monocap datasets (unseen view only).\label{Q_table}}
\vspace{-3mm}
\end{table*}

\section{Experiments}



\subsection{Datasets}
% We conduct experiments on three datasets:


% \TODO{considering add MVHumanNets dataset as main test base}

\hspace*{1.5em}\textbf{ZJU-Mocap(revised) dataset~\cite{peng2021neural}.} This dataset is a multi-view dataset. We train and test using this dataset following Instant-NVR~\cite{instant_nvr}. One specific camera capture is used as monocular training input and six cameras, evenly distributed around the object, are reserved for a comprehensive evaluation.

\textbf{Monocap dataset.} Similar to ZJU-Mocap(revised), the Monocap dataset contains multi-view videos collected by AnimatableNeRF~\cite{peng2024animatable} from the DeepCap dataset~\cite{habermann2020deepcap} and the DynaCap dataset~\cite{habermann2021}. The dataset setting follows the ZJU-Mocap(revised) dataset.

\textbf{MVHumanNet dataset~\cite{xiong2024mvhumannet}.} The dataset is a large-scale collection of multi-view human images, encompassing human masks, camera parameters, 2D and 3D keypoints, SMPL/SMPLX parameters. The dataset setting follows the ZJU-Mocap(revised) dataset as well.  


\textbf{In-the-wild dataset.} This dataset contains YouTube videos collected by HumanNeRF~\cite{weng2022humannerf} and Dance Dance Generation~\cite{zhou2019dance}. We employ BEV~\cite{BEV} to estimate camera parameters and the SMPL bodies, then utilize Xmem~\cite{cheng2022xmem} along with Segment-anything~\cite{kirillov2023segany} to extract foreground segmentation of video frames.

\subsection{Implementation Details} 
\label{implementaion}
All videos from all datasets are clipped to 3-5 seconds (100-150 frames) and exclusively capture front views of the subjects. During stage I, training is conducted on a single RTX-3090 GPU with a batch size of 2, requiring approximately 1 hour for 200 training epochs. In Stage II, the entire framework is trained on two RTX-3090 GPUs with a batch size of 1, while the diffusion model is loaded exclusively on the second GPU. The training process requires approximately 2â€“3 hours for 400 epochs, depending on the resolution.


% \subsection{Comparisons}

% \begin{figure*}[t]
% \centering
% \includegraphics[width=1\textwidth]{zju_exp.pdf} % Reduce the figure size so that it is slightly narrower than the column.
% \caption{Qualitative comparison on ZJU-Mocap dataset~\cite{peng2021neural}. We compare the novel view synthesis quality with Instant-NVR~\cite{jiang2023instantavatar} and GaussianAvatar~\cite{hu2023gaussianavatar}.}
% \label{fig_zju}
% \end{figure*}


\subsection{Comparisons with Video-based Methods}

We conduct comparisons of our method with HumanNeRF~\cite{weng2022humannerf}, Instant-NVR~\cite{instant_nvr}, SplattingAvatar~\cite{shao2024splattingavatar}, ExAvatar~\cite{moon2024exavatar}, GaussianAvatar~\cite{hu2023gaussianavatar}, and GuessTheUnseen~\cite{lee2024gtu}.

For a fair comparison, Instant-NVR~\cite{instant_nvr} is trained on the revised version of the ZJU-Mocap dataset, which offers refined camera parameters, SMPL fittings, and more accurate instance masks with body-part segmentation, crucial for the execution of their method. However, HumanNeRF is not adapted to this dataset, and MVHumanNet and Monocap are applied to evaluate this method. Additionally, Instant-NVR lacks a pose refinement technique akin to HumanNeRF, which assists in addressing inaccurate fitting issues in the in-the-wild dataset. GuessTheUnseen is evaluated on the ZJU-Mocap(revised) and Monocap datasets but not on MVHumanNet, as the original images in this dataset contain black bounding boxes that significantly hinder the human motion detection process performed by GuessTheUnseen. Therefore, we will discuss the comparison results based on the type of dataset utilized.


% \begin{figure}[tb]
%   \centering
%   \includegraphics[width=\linewidth]{exp.drawio.pdf}
%   \caption{Qualitative comparison on ZJU-Mocap dataset~\cite{peng2021neural}. We compare the novel view synthesis quality with Instant-NVR~\cite{jiang2023instantavatar} and GaussianAvatar~\cite{hu2023gaussianavatar}.}
%   \label{fig:zju_exp}
% \end{figure}


% \ZY{In both qualitative and quantitative comparisons, we  focus on evaluating the invisible parts of generated human avatars, as these typically represent the most challenging aspects across all viewing directions. Consequently, the quantitative results are generally lower compared to visible parts. This emphasis enables a comprehensive assessment of the models' performance, particularly in scenarios with limited visibility.}
% For both qualitative and quantitative comparisons, our emphasis lies in evaluating the invisible parts of generated human avatars, which often represent the "worst" aspects across all viewing directions. As a consequence, the quantitative results tend to be relatively lower compared to the visible parts. This focus allows us to provide a comprehensive assessment of the models' performance, particularly in challenging scenarios where visibility is limited.

\begin{table}[!t]
    \centering
     \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{c|c|c|c}
    \hline
         & PSNR$\uparrow$& SSIM$\uparrow$ &LPIPS$\downarrow$ \\ \hline
        HumanNeRF & 30.23 & 0.9756 & 0.0314\\ \hline
         SplattingAvatar & 28.28 & 0.9693 & 0.0286\\ \hline
         ExAvatar & 29.46 & 0.9709 & 0.0253\\ \hline
         GaussianAvatar & 29.96 & 0.9716 & 0.0220\\ \hline
        GuessTheUnseen & 29.35 & 0.9685 & 0.0256\\ \hline
       Ours & 29.76 & 0.9712 & 0.0222\\ \hline
    \end{tabular}
    \caption{Quantitative evaluation on In-the-wild datasets (seen view only)\label{tab:exp_youtube}}
    % \vspace{-2mm}
\end{table}
\subsubsection{ZJU-Mocap(revised), MVHumanNet, and Monocap Datasets}
These three datasets serve as the primary testbeds for our experiments due to the availability of ground truths for invisible parts. The quantitative results are presented in Tab.~\ref{Q_table}, where our method surpasses all evaluation techniques in all three metrics, indicating its efficacy in reconstructing both geometry and appearance for invisible parts. Qualitatively, as shown in Fig.~\ref{fig: qualitative1}, the limitations of all compared methods become more evident when visualizing invisible parts. Methods such as GaussianAvatar, ExAvatar, and SplattingAvatar, which are based on Gaussian Splatting, exhibit noticeable artifacts and inconsistencies, including noisy textures and blank spots. Instant-NVR and HumanNeRF, due to their NeRF-based ray-shooting geometric reconstruction technique, not only struggle with appearance consistency but also suffer from geometric issues like floating artifacts and penetrating holes, diminishing the realism of the synthesized avatars. While GuessTheUnseen can infer unseen-view appearances, it introduces noisy textures and multi-face `Janus' artifacts, as shown in Fig.~\ref{fig: qualitative2}.
% Qualitatively, as depicted in Fig.~\ref{fig: qualitative}, the shortcomings of Instant-Avatar and GaussianAvatar become more pronounced when considering invisible parts visualization. GaussianAvatar exhibits visible artifacts and inconsistencies, e.g. noisy textures and blank spots. Meanwhile, caused by the NeRF-based ray-shooting geometric reconstruction technique, Instant-Avatar not only struggles to maintain appearance consistency but exhibits geometric failures, such as floating artifacts and penetrating holes, which detract from the realism of the synthesized avatars. GuessTheUnseen, even though it can guess unseen-view appearances, conducts noise texture and has 'Januse' artifacts.

\begin{table}[!t]
   
     
    \centering
     \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{l|l|l|l}
    \hline
         & PSNR$\uparrow$& SSIM$\uparrow$ &LPIPS$\downarrow$ \\ \hline
        full model  & \textbf{21.06} &\textbf{0.9536} & \textbf{0.0551}\\ \hline
         full model w/o Prog. & 20.98 & 0.9523 & 0.0559\\ \hline
         full model w/o Canonical. & 20.16 & 0.9503 & 0.0586\\ \hline
        full model w/o Opt. & 19.78 & 0.9463 & 0.0626\\ \hline
       full model w/o Observ. & 19.56 & 0.9434 & 0.0685\\ \hline
    \end{tabular}
 \caption{Quantitative results for ablation study. Opt. includes view selection and pose feature injection. Prog. is short for progressive training strategy.}\label{tab:ablation}
 % \vspace{-3mm}
\end{table}
% \vspace{-1mm}
\subsubsection{In-the-wild Dataset} 

The in-the-wild dataset comprises various monocular dancing videos sourced from the internet. For quantitative evaluation, our method demonstrates performance comparable to the baselines for the seen parts of reconstructed humans, as shown in Tab.~\ref{tab:exp_youtube}. However, due to the lack of novel view references, our primary focus is on qualitative evaluation results in comparison to other methods for the unseen parts of humans. As shown in Fig.~\ref{fig: qualitative1}\&\ref{fig: qualitative2}, we observe that HumanNeRF faces similar challenges to Instant-NVR. The generative networks struggle to effectively fuse sampling points for novel view rendering due to a lack of supervision. This shortcoming results in floating points and ``foggy" artifacts in the rendered outputs. Additionally, the Gaussian-based methods continue to produce unrealistic ``tattoo-like" appearances on the backs of synthesized avatars, highlighting its limitations in preserving overall appearance fidelity. For GuessTheUnseen, the `Janus' artifacts become more pronounced, and it even fails to correctly infer the appearance of some subjects. In contrast, our method consistently demonstrates superior performance in addressing the challenges of invisible parts synthesis, excelling in both geometry and appearance reconstruction. The invisible parts of the synthesized avatars show not only enhanced geometric precision but also significantly improved appearance fidelity, with fewer artifacts and smoother textures.
% In contrast, our method \ZY{consistently} demonstrates superior performance by effectively addressing the challenges associated with invisible parts synthesis. By leveraging advanced techniques such as viewpoint-conditioned diffusion models and pose feature injection, our pipeline achieves remarkable results in both geometry and appearance reconstruction. The invisible parts of synthesized avatars not only exhibit enhanced geometry precision but also showcase a significant improvement in appearance fidelity, with fewer artifacts and smoother textures.

% The in-the-wild dataset consists of a variety of monocular dancing videos sourced from the internet. Given the characteristics of this dataset, our main emphasis is placed on qualitative evaluation results when compared against other methods. As illustrated in Fig.~\ref{fig: qualitative}, we observe that HumanNeRF encounters similar issues to Instant-NVR. The network struggles to effectively fuse sampling points along the rays for novel view rendering due to the lack of supervision. This deficiency leads to the presence of floating points and "foggy" artifacts in the results. Additionally, GaussianAvatar continues to exhibit unrealistic and "tattoo" appearances on the back of synthesized avatars, indicating its limitations in preserving overall appearance fidelity.\ZL{ For GuessTheUnseen, the 'Januse' artifacts are getting severe and it even fails to guess the correct appearance for some subjects.}



% \begin{figure}[!t]
%   \centering
%   \includegraphics[width=1\linewidth]{Figures/Appendix_2.2.pdf}
%     \vspace{-1mm}
%   \caption{Qualitative comparison results with SIFU~\cite{Zhang2024SIFU} and SITH~\cite{ho2024sith}.}
    
%   \label{fig:SIFU}
% \end{figure}
% \begin{figure}[!t]
%   \centering
%   \includegraphics[width=1\linewidth]{Figures/Appendix_1.1.pdf}
%     \vspace{-2mm}
%   \caption{Qualitative comparison results with ELICIT~\cite{huang2022elicit} on novel poses.}
%     \vspace{-2mm}
%   \label{fig:ELICIT}
% \end{figure}

% \begin{table}[!t]
% \centering
% \renewcommand{\arraystretch}{1.5}

% \begin{tabular}{c|c|ccc}
% \hline
% Dataset                    & Method & PSNR$\uparrow$&SSIM$\uparrow$&LPIPS$\downarrow$  \\ \hline
% \multirow{3}{*}{MVHumanNet}   & SIFU   &19.29&0.9486&0.0706\\
%                             & SITH   &19.68&0.9462&0.0699\\
%                         &\textbf{Ours} & \textbf{20.98} & \textbf{0.9517} & \textbf{0.0553} \\ \hline  
% \multirow{3}{*}{Monocap}   & SIFU   &18.96&0.9406&0.0659\\
%                             & SITH   &19.06&0.9428&0.0.0673\\
%                         &\textbf{Ours} & \textbf{21.16} & \textbf{0.9532} & \textbf{0.0549} \\ \hline  
% \multirow{2}{*}{ZJU-Mocap(revised)} & ELICIT &19.23&0.9456&0.0.0689\\
%                            &\textbf{Ours} & \textbf{20.82} & \textbf{0.9552} & \textbf{0.0569}  \\ \hline
% \end{tabular}
% \caption{Quantitative evaluation on MVHumanNet, ZJU-Mocap(revised), and Monocap datasets.}
%   \label{tab:exp}
% \end{table}

% \subsection{Comparison with Image-based Methods \label{resultImagebased}}

% In this section, we compare our method with SIFU~\cite{Zhang2024SIFU}, SITH~\cite{ho2024sith}, and ELICIT~\cite{huang2022elicit}, all of which are single-image reconstruction techniques designed to synthesize unseen parts of human avatars.

% SIFU proposes an approach to reconstruct clothed human avatars from single images. Qualitatively, as shown in Fig.~\ref{fig:SIFU}, this method can reconstruct decent geometry but fails to synthesize the texture of unseen parts of humans. SITH, similar to SIFU, is a method for single-image reconstruction. SITH can predict the texture of unseen parts of humans, but their generated textures contain unrealistic artifacts.

% ELICIT is a generative model that takes one image and a motion sequence as input to generate an animatable avatar. Qualitative results are shown in Fig.~\ref{fig:ELICIT}. For a fair comparison, since our method takes an image sequence as input, we are comparing the quality by synthesizing a novel pose that is not included in our inputs. Even though ELICIT can predict the unseen parts of humans, it shows blurred edges and floating artifacts while applying motions. Because only one image is used as input for ELICIT, the texture cannot be adapted to novel poses dynamically. In contrast, our method associates texture to different body parts across frames and can predict the correct texture for unseen parts robustly.

% In Tab.~\ref{tab:exp}, we present the quantitative evaluation results. SIFU and SITH were tested on the Monocap dataset, while ELICIT was evaluated on the ZJU-Mocap(revised) dataset. The results demonstrate that our method consistently achieves superior performance compared to the state-of-the-art approaches, underscoring its efficacy and robustness.





\subsection{Ablation Study}
\subsubsection{Dual-space Optimization} 
Next, we evaluate the effectiveness of Dual-Space Optimization, with ablation results presented in Fig.~\ref{fig:ablation_DS}. Observation optimization is crucial for reconstructing the invisible parts of the avatar, but it often encounters challenges and may not converge effectively, resulting in rough and less satisfactory appearances. In such cases, the canonical optimization step becomes essential. Leveraging the canonical space, the optimization converges more effectively, yielding smoother and more visually pleasing results. Nevertheless, observation optimization can mitigate the quality degradation issues that arise when relying solely on SDS optimization in the canonical space, as shown in Fig.~\ref{fig:ablation1-a},\ref{fig:ablation1-b}\&\ref{fig:ablation1-c}. This iterative approach highlights the importance of both observation and canonical optimization for achieving optimal reconstruction outcomes.

% The fine-tuning components are essential for tackling the challenges posed by partial views, specifically in synthesizing invisible views. The ablation results are illustrated in Fig.~\ref{fig:ablation_exp}.
% The observation fine-tuning process plays a crucial role in the reconstruction of the invisible parts of the avatar. However, its training often encounters challenges and may not converge effectively, resulting in the synthesis of rough and less satisfactory appearances. In such cases, the canonical fine-tuning step becomes essential. By leveraging canonical space, the fine-tuning process can effectively converge and generate smoother and more visually pleasing results. This iterative approach highlights the importance of both observation and canonical fine-tuning in achieving optimal reconstruction outcomes.


\subsubsection{View Selection and Pose Feature Injection} 
We investigate the influence of View Selection and Pose Feature Injection in the following. As shown in Fig.~\ref{fig:ablation_V&P}, view selection filters out visible views, preserving the alignment between visible and canonical appearances, thereby reducing potential disruptions from observation optimization. Additionally, pose feature injection plays a crucial role in further enhancing dynamic appearance, allowing for the capture of finer details, especially in facial regions and cloth textures. These improvements significantly contribute to the overall fidelity and realism of the synthesized avatars.In Tab.~\ref{tab:ablation}, we present a quantitative evaluation of all components in the ZJU-Mocap(revised) dataset, MVHumanNet dataset, and Monocap dataset. Our full model, coupled with progressive training, achieves the best results in this evaluation.

% These components are specifically designed to enhance visible appearance. As depicted in Fig.~\ref{fig:ablation_exp}, view selection filters out visible views, thereby preserving the visible appearance in alignment with the canonical appearance, thus mitigating the potential disruptions caused by observation fine-tuning. Furthermore, pose feature injection continues to play a pivotal role in further enhancing dynamic appearance, enabling the capture of finer details, particularly in facial regions and cloth textures. These enhancements contribute significantly to the overall fidelity and realism of the synthesized avatars.
\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.20\linewidth}  % Adjust width to 20%
        \includegraphics[trim=18 70 18 50, clip, width=\linewidth]{Figures/ablation_2.1.1.pdf}
        \caption{}
        \label{fig:ablation_DS-a}
    \end{subfigure}
    \hspace{0.02\linewidth}  % Adjust space between subfigures (small value)
    \begin{subfigure}[b]{0.20\linewidth}
        \includegraphics[trim=24 40 684 0, clip, width=\linewidth]{Figures/ablation_2.1.pdf}
        \caption{}
        \label{fig:ablation_DS-b}
    \end{subfigure}
    \hspace{0.02\linewidth}  % Adjust space between subfigures (small value)
    \begin{subfigure}[b]{0.20\linewidth}
        \includegraphics[trim=121 40 587 0, clip, width=\linewidth]{Figures/ablation_2.1.pdf}
        \caption{}
        \label{fig:ablation_DS-c}
    \end{subfigure}
    \hspace{0.02\linewidth}  % Adjust space between subfigures (small value)
    \begin{subfigure}[b]{0.20\linewidth}
        \includegraphics[trim=214 40 493 0, clip, width=\linewidth]{Figures/ablation_2.1.pdf}
        \caption{}
        \label{fig:ablation_DS-d}
    \end{subfigure}

    \caption{\textbf{Ablation study about Dual-pace Optimization} (a) conditioning image for SDS. (b) w/o dual space optimization. (c) w/ observation optimization only. (d) full model novel view.}
    \vspace{-6mm}
    \label{fig:ablation_DS}
\end{figure}


\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.32\linewidth}  % Adjust width to 33%
        \includegraphics[trim=33 0 340 0, clip, width=\linewidth]{Figures/ablation_2.2.pdf}
        \caption{}
        \label{fig:ablation_V&P-a}
    \end{subfigure}
    % \hspace{0.02\linewidth}  % Adjust space between subfigures (small value)
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[trim=201 0 170 0, clip, width=\linewidth]{Figures/ablation_2.2.pdf}
        \caption{}
        \label{fig:ablation_V&PS-b}
    \end{subfigure}
    % \hspace{0.02\linewidth}  % Adjust space between subfigures (small value)
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[trim=370 0 0 0, clip, width=\linewidth]{Figures/ablation_2.2.pdf}
        \caption{}
        \label{fig:ablation_V&P-c}
    \end{subfigure}

    \caption{\textbf{Ablation study about View Selection and Pose Feature Injection} (a) w/o both. (b) w/ view selection and w/o pose feature injection. (c) full model novel view.}
    \label{fig:ablation_V&P}
    \vspace{-3mm}
\end{figure}

% \subsubsection{The effectiveness of progressive training:} The full model adequately addresses most cases but may not suffice for low-resolution videos. As depicted in \cref{fig:ablation_exp}, progressive training enhances visible views by gradually reducing the training weight of the SDS loss. This adaptive approach ensures improved performance, particularly in scenarios with lower-resolution videos.




\subsection{Novel Poses Animation}
Our method aligns the generated Gaussian human avatars with the SMPL model, enabling us to animate the reconstructed avatar with novel poses, as shown in Fig.~\ref{fig:anim}. Please refer to the accompanying video for dynamic results.
\begin{figure}[tb]
  \centering
  \includegraphics[width=0.9\linewidth]{Figures/Appendix_animation1.pdf}

  \caption{Avatar animation with novel poses.}
     \vspace{-5mm}
  \label{fig:anim}
\end{figure}

\subsection{Efficiency} As shown in Fig.~\ref{fig:pipline}, our method, with its two-stage training process, requires approximately 3 training hours, significantly outperforming HumanNeRF, which demands about 10 hours of training on the same device. Furthermore, our method can achieve almost real-time rendering speed at 18 fps. But Instant-NVR and HumanNeRF can only render with 2 fps and 7 fps respectively.

