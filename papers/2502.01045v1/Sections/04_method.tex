

\section{Method}
\label{sec:method}

\begin{figure*}
\centering
\includegraphics[width=1\textwidth]{Figures/Pipline4.2.pdf} % Reduce the figure size so that it is slightly narrower than the column.

\caption{Overview of WonderHuman. (1) In stage I, we reconstruct 3D Gaussians and appearances for visible human parts from partial-view videos. We start with optimizable feature vectors named canonical features capturing human geometry and appearance in a canonical space. Then, we use a Gaussian Decoder to predict Gaussian parameters and combine the Linear Blend Skinning (LBS) function with the Gaussian Splatting to render the dynamic 3D human in the observation space. 
%\ZL{The training details are described in Sec.~\ref{trainingLosses}.}
(2) In Stage II, we hallucinate the invisible parts of the avatar using a Dual-space Optimization technique. 
% (2) In Stage II, we fine-tune the networks and trainable avatar features to reconstruct the invisible parts of the avatar using a diffusion model. 
We render images of the human avatar from various novel viewpoints and apply an SDS loss to learn the unseen appearances. Additionally, a normal predictor is utilized to generate normal maps that guide geometry reconstruction, while View Selection and Pose Feature Injection strategies are employed to ensure consistent appearance fusion.}
\vspace{-3mm}
\label{fig:pipline}

\end{figure*}


Given a monocular video as the input, our goal is to reconstruct a high-quality animatable 3D human avatar including both visible and invisible parts. In \name, we employ a dynamic 3D human Gaussian representation, equipped with a generative diffusion model as hallucination prior, which produces a controllable 3D human avatar viewable from any angle. An overview of our method can be found in Fig.~\ref{fig:pipline}.

\subsection{Stage I: Visible Appearance Reconstruction}
\subsubsection{Prediction of Gaussian Parameters} 
\label{gaussianDecoder}
In the first stage, we reconstruct the visible geometry and appearance of an animatable human avatar from a partial-view monocular video. To achieve detailed and high-fidelity reconstructions, building on GaussianAvatar~\cite{hu2023gaussianavatar}, we propose integrating normal information into the Gaussian decoder~\cite{hu2023gaussianavatar}. This improved decoder is used to establish a functional mapping from the underlying geometry of the human to various attributes of 3D Gaussians. And those Gaussians are initialized on the surfaces of SMPL~\cite{SMPL:2015} body in canonical space. Then, we have:

\begin{equation}
  (\Delta x,\Delta n, c,s)=G_{\theta}([S,S]),
  \label{eq:gaussiandecoderstage1}
\end{equation}
where $\theta$ represents optimizable parameters for the Gaussian decoder $G_{\theta}$, and \( S \) represents the features in the canonical space. 
% \TODO{give definition and explanation of this S, channel...}
The canonical feature $S$ is an optimizable tensor, randomly initialized and optimized during training to capture texture and geometry features in canonical space. The size of $S$ is $(128 \times 128)$, and it is concatenated with itself as input of Gaussian decoder $G$. This ensures that the input channel of $G$ remains $(2 \times 128 \times 128)$ during pose feature injection in Stage II (Sec.~\ref{poseInject}).
This decoder $G$ predicts 3D center offset $\Delta{x}$, along with color and scale factors, denoted as $c$ and $s$ respectively. Additionally, it predicts normal offset \(\Delta n\) that is applied to the initial SMPL normals, to capture the intrinsic geometric details. We set the opacity $\alpha$ and 3D rotation $q$ are set to fixed values of $1$ and $(1,0,0,0)$ respectively, to make the network focus more on the geometry information. %enhance geometry reconstruction accuracy.

\subsubsection{Dynamic Human Rendering} 
\label{LBS&rendering}
To render the avatar in observation space, we seamlessly combine the Linear Blend Skinning function with the Gaussian Splatting~\cite{kerbl3Dgaussians} process to deform the avatar from canonical space to observation space:
\begin{equation}
  I_{rgb}=Splatting(x_o,c,Q,r),
  \label{eq:splatting}
\end{equation}
\begin{equation}
  x_o = T_{lbs}(x_c,p,w),
  \label{eq:LBS}
\end{equation}
where $I_{rgb}$ represents the final rendered image. The final canonical Gaussian position $x_c$ is the sum of the initial position $x$ and the predicted offset $\Delta x$. The LBS function $T_{lbs}$ applies the SMPL skeleton pose $p$ and blending weights $w$ to deform $x_c$ into observation space as $x_o$, where $w$ is provided by SMPL~\cite{SMPL:2015}. $Q$ here denotes the remaining parameters of the Gaussians, including scale $s$, opacity $\alpha$, and rotation $q$. For more details on canonical initialization, see Appendix A.1.1.


\subsubsection{Normal Map Rendering of Seen View} 
\label{frontNormalRendering}

We aim to faithfully capture the detailed surface geometry of dynamic human bodies from partial-view videos. Central to this process is the rendering of predicted normal maps, where the predicted \(\Delta n\) is applied to the initial SMPL normals \( n \) to compute \( n_c \) in canonical space. \( n_c \) is then transformed into the observation space \( n_o \) and rendered as normal maps \(I_n\). The Eq.~\eqref{eq:LBS}\&\eqref{eq:splatting} are modified for normals as:
\begin{equation}
  I_{n}=Splatting(x_o,n_o,Q,r),
  \label{eq:normalsplatting}
\end{equation}
\begin{equation}
  n_o = T_{lbs}(n_c,p,w).
  \label{eq:normalLBS}
\end{equation}
This transformation maps 3D Gaussians from the canonical space to the observation space, enabling the preservation of detailed geometry encoded by the normals and appearance.


We supervise the normal vectors using high-quality normal maps derived from ground truth RGB images. For this purpose, we leverage Sapiens~\cite{khirodkar2024sapiens} as a normal predictor to predict normal maps from video frames, using them as supervision for normal maps rendered in our observation space, expressed as:

\begin{equation}
\mathcal{L}_n = MSE(I_n,I^{\text{gt}}_{n}),
  \label{eq:normalloss}
\end{equation}
where $I^{\text{gt}}_{n}$ denotes the predicted normal map from Sapiens. The normal loss $\mathcal{L}_n$ is defined as the MSE loss between $I_{n}$ and $I^{\text{gt}}_{n}$. By aligning the predicted normal maps with those renderings, we achieve a high-fidelity representation of surface geometry that accurately captures both global and fine-grained details.


\subsection{Stage II: Invisible Appearance Reconstruction}
\label{fine-tuning}
Stage I produces an animatable 3D human model with visible appearances learned from partial-view video data, but the unobserved regions of the body typically suffer from relatively low visual quality. To ensure multi-view consistency for the unseen parts, we introduce a viewpoint-conditioned diffusion model as supervision, leveraging generative priors to predict the unseen views from the given inputs. Subsequently, we optimize the Gaussian decoder $G_{\theta}$ to reconstruct a fully renderable 3D human model from any viewpoint. To effectively utilize the observations and improve consistency between observed and hallucinated results, we introduce Dual-space Optimization, View Selection and Pose Feature Injection techniques in the following.


\begin{figure}[tb]
    \centering
    \begin{subfigure}{0.15\linewidth}
        \includegraphics[trim=30 50 710 0, clip, width=\linewidth]{Figures/ablation_1.1.pdf}
        \caption{}
        \label{fig:ablation1-a}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.15\linewidth}
        \includegraphics[trim=170 50 570 0, clip, width=\linewidth]{Figures/ablation_1.1.pdf}
        \caption{}
        \label{fig:ablation1-b}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.15\linewidth}
        \includegraphics[trim=300 50 440 0, clip, width=\linewidth]{Figures/ablation_1.1.pdf}
        \caption{}
        \label{fig:ablation1-c}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.15\linewidth}
        \includegraphics[trim=450 50 290 0, clip, width=\linewidth]{Figures/ablation_1.1.pdf}
        \caption{}
        \label{fig:ablation1-d}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.15\linewidth}
        \includegraphics[trim=730 50 10 0, clip, width=\linewidth]{Figures/ablation_1.1.pdf}
        \caption{}
        \label{fig:ablation1-e}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.15\linewidth}
        \includegraphics[trim=590 50 152 0, clip, width=\linewidth]{Figures/ablation_1.1.pdf}
        \caption{}
        \label{fig:ablation1-f}
    \end{subfigure}

    \caption{\textbf{Left side: Dual-pace Optimization} (a) w/o dual space optimization; (b) w/ canonical optimization only; (c) w/ dual-space optimization; \textbf{Right side: Pose Feature Injection} (d) ground truth; (e) w/o pose feature injection; (f) w/ pose feature injection.}
    \vspace{-3mm}
    \label{fig:ablation1}
\end{figure}

\subsubsection{Dual-space Optimization} 

% \TODO{rephrase this section and emphasize the difference with GTU}
Zero123~\cite{liu2023zero1to3}, a viewpoint-conditioned diffusion model, is used to hallucinate full-body views from partial video frames, using reference frames from the monocular video and target view camera parameters as conditioning inputs.
Its explicit view control enables precise multi-view predictions for 3D reconstruction.  We leverage Score Distillation Sampling~(SDS)~\cite{poole2022dreamfusion} loss for predicting the unseen parts of our 3D Gaussian human model in the observation space. Unfortunately, naively combining Zero123 using SDS for dynamic human reconstruction leads to unrealistic reconstruction results. For instance, directly applying SDS in canonical space often results in degenerated quality issues in avatars—when generating 3D models with 2D diffusion models (See Fig.~\ref{fig:ablation1-b}).

%For instance, directly applying SDS in canonical space often leads to the multi-face `Janus' problem on avatars—when generating 3D models with 2D diffusion models (See Fig.~\ref{fig:ablation1} (2)).

% As a naive approach that trains the network in canonical space, canonical space optimization leverages renderings from Stage I in the canonical T-pose as conditioning references. Solely optimization in canonical space, however, results in 'Januse' artifacts. 'Januse' artifact is used when avatars have multiple faces, which is a common issue in generating 3d avatars using 2d diffusion models. See Fig.~\ref{fig:ablation1}.
To address this, we introduce Dual-Space Optimization, which performs SDS optimization in both canonical and observation spaces. When conducting optimization in the canonical space, we use the rendering in the canonical space from Stage I as a conditioning reference for the 2D generative diffusion model. When conducting optimization in the observation space, we utilize the selected input images from the partial-view video as conditioning references.

The SDS optimization process, combining Zero123 with the dual-space strategy, is thus expressed as:


\begin{equation}
  \mathcal{L}_{SDS}(I_{\theta})\triangleq \mathbb{E}_{t,\epsilon}[w(t)(\epsilon_{\phi}(z_t,y,R,T,t)-\epsilon)\frac{\partial I_{\theta}}{\partial\theta}],
  \label{eq:SDSObservOur}
\end{equation}
where $I_{\theta}$ represents a generated image from an unseen view in observation or canonical space.  $\epsilon_{\phi}$ is the predicted noise by Zero123 conditioned on the image $y$ and the target view camera parameters $(R, T)$.

Since $\mathcal{L}_{SDS}$ is applied in both canonical and observation spaces, we take the observed frames from the input video as $y_{image}$ when optimizing in observation space, and take the canonical rendering from Stage I as $y_{image}$ when optimizing in canonical space.
This approach allows us to more effectively associate features across frames for the reconstruction of unseen parts.
 



During dual-space optimization, we found that appropriately balancing the training processes of the two spaces improves performance. As mentioned earlier, diffusion models face degeneration issues when optimizing in the canonical space. %Additionally, 
For instance, they struggle to predict accurate appearances for complex human poses in the observation space, often producing unrealistic 'tattoo-like' appearances, as shown in Fig.~\ref{fig:ablation_DS-c}. To address this, we set the weight between canonical and observation optimization as a hyperparameter in Stage II to enhances the overall process performance. This refined balance ensures better alignment of the model with the desired objectives, leading to more accurate and reliable outcomes in Stage II.




\subsubsection{View Selection}
\label{viewSelection}


The aforementioned Dual-Space  Optimization with SDS aids in synthesizing the unseen appearance of human avatars. Next, we introduce view selection to analyze which regions of the avatar are poorly observed.

% Utilizing the viewpoint-conditioned SDS loss facilitates the synthesis of the unseen appearance of the human. Nevertheless, it is imperative to acknowledge that the SDS loss may compromise the appearance of visible parts. As a remedial measure, we have implemented view selection and pose feature injection techniques to mitigate this issue.
In both canonical and observation space optimizations, we identify the invisible views that require refinement. By utilizing the differentiable rasterization of Gaussian Splatting~\cite{kerbl3Dgaussians}, we determine the first intersecting Gaussian for each ray, marking these as visible points. Subsequently, visibility maps are rendered to differentiate between the visible and invisible regions of the human avatar as defined in Stage I. Specificly, we first estimate the visibility of each Gaussain. During the training of Stage I with seen views, given a ray $r$, the first Gaussian hit by the ray, $x$, is marked as a seen Gaussian, and its visibility $\psi$ is set to 1. Formally:
\begin{equation}
    \psi(x, r) = \begin{cases} 1, & x \text{ is the first Gaussian on } r \\ 0, & \text{otherwise} \end{cases},
    \label{eq:viewSelection1}
\end{equation}
where $\psi(x, r) = 1$ indicates that the Gaussian $x$ is visible in Stage I, while $\psi(x, r) = 0$ represents the opposite. 
After Stage I training is completed, a visibility map $I_v$ of a random viewpoint $v$ is rendered given $\psi(x, r)$, $r$, and the remaining attributes $Q$. In $I_v$, if the visible region $VR(I_v)$ covers less than 50\% of the foreground region $FR(I_v)$, this viewpoint is marked for refinement in Stage II. Then the view selection is expressed as:
\begin{equation}
    I_v = Splatting \left( x, \psi(x, r) , Q, r \right),
    \label{eq:viewSelection2}
\end{equation}
\begin{equation}
    Visibility(I_v) = \frac{VR(I_v)}{FR(I_v)},
    \label{eq:viewSelection3}
\end{equation}
\begin{equation}
    \mathbb{I}_v = \begin{cases} 0, & Visibility(I_v) \leq 50\% \\ 1, & \text{otherwise} \end{cases}, 
    \label{eq:viewSelection4}
\end{equation} 
where $\mathbb{I}_v = 0$ signifies an unseen viewpoint, indicating that the avatar needs to be refined from viewpoint $v$ in Stage II.


\begin{figure}[tb]
    \centering
    \begin{subfigure}[b]{0.4\linewidth}
        \includegraphics[trim=25 50 30 20, clip, width=\linewidth]{Figures/visibility1.pdf}
        \caption{}
        \label{fig:visibility1}
    \end{subfigure}
    \hspace{0.05\linewidth}
    \begin{subfigure}[b]{0.35\linewidth}
        \includegraphics[trim=70 70 10 50, clip, width=\linewidth]{Figures/visibility2.pdf}
        \caption{}
        \label{fig:visibility2}
    \end{subfigure}

    \caption{\textbf{View Selection based on visibility map} (a) Seen view: Visible region (\textbf{\textcolor{myorange}{orange}}) covers more than 50\% of the foreground region; (b) Unseen view: Invisible region (\textbf{\textcolor{myblue}{blue}}) covers more than 50\% of the foreground region.}
    \vspace{-3mm}
    \label{fig:visibility}
\end{figure}

\subsubsection{Pose Feature Injection}
\label{poseInject}
Furthermore, during dual-space optimization, while SDS is applied across diverse poses in observation spaces, the Gaussian decoder is trained in the canonical space. To capture pose-dependent appearances in the observation space, such as garment wrinkles in Fig.~\ref{fig:ablation1-d}, we leverage the pose encoder similar to GaussianAvatar~\cite{hu2023gaussianavatar} to extract pose-related features, which are then injected into the decoder network. Consequently,  we have:
% Moreover, while the SDS is applied across diverse pose observation spaces, the network is trained in canonical space. To capture pose-dependent appearance, we leverage the pose encoder of GaussianAvatar to extract pose-related features and inject them into the Gaussian parameter network. The \cref{eq:gaussiandecoderstage1} is accordingly modified in this fine-tuning stage as:
\begin{equation}
  (\Delta x, \Delta n,c,s)=G_{\theta}([S,P]),
  \label{eq:stage2G_decoder}
\end{equation}
\begin{equation}
  P = Encoder(P_{uv}),
  \label{eq:poseencoder}
\end{equation}
where $P_{uv}$ is the UV positional map of SMPL for each pose, and $P$ denotes the extracted pose feature, which is concatenated with the canonical features $S$ as input of the Gaussian decoder $G_{\theta}$. And $Encoder(.)$ maps $P_{uv}$ to $P$. All the outputs of $G_{\theta}$ remain the same as in Stage I.


\subsubsection{Normal Map Supervision of Unseen View} 
\label{BackNormalRendering}
For the reconstruction of unseen-view geometry, we extend the rendering process described in Sec.~\ref{frontNormalRendering} to generate normal maps for unseen views. Specifically, the normal maps of given views are treated as front normal maps. To compute the back normal maps from their corresponding front normal map, we utilize a depth-aware, silhouette-consistent bilateral normal integration (d-BiNI) method~\cite{bini2022cao}. These back normal maps are then combined with pretrained SMPL-aware IF-Nets~\cite{chibane20ifnet}, which inpaint the geometry of the remaining body regions. The resulting output is a complete set of normal maps, which serves as full-body normal supervision in Eq.~\ref{eq:normalloss}.


% \subsubsection{Dual Space Fine-tuning}
% Experimentally, the injection of pose features enhances the visible appearance but weakens the learning of invisible appearance during fine-tuning. The rationale behind the observed phenomenon is that pose feature injection renders invisible appearances highly pose-dependent. Consequently, the network is trained individually for each pose observation space rather than jointly, leading to a less efficient fine-tuning process, particularly concerning invisible appearances. To address this issue, we introduce a dual space fine-tuning. Alongside observation fine-tuning, which relies on ground truth, canonical space fine-tuning utilizes renderings under the canonical T-pose space from stage I as conditioning references. Moreover, the input image $\widehat{I_{\theta}}$ is not only rendered in canonical space but also incorporates various pose features, thereby collectively training human appearance across all pose observation spaces.
% \begin{equation}
%   \mathcal{L}_{SDS-c}(\widehat{I_{\theta}})\triangleq E_{t,\epsilon}[w(t)(\epsilon_{\phi}(\widehat{z_t},y_{tpose},R,T,t)-\epsilon)\frac{\partial \widehat{I_{\theta}}}{\partial\theta}],
%   \label{eq:SDSTpose}
% \end{equation}
% where $y_{tpose}$ represents the front-view rendering in the canonical T-pose space. Through this dual space fine-tuning, the network becomes capable of sufficiently learning invisible appearances within a reasonable timeframe.

\subsection{Training Losses}
% \subsection{Optimization}
\label{trainingLosses}
In Stage I, we are modeling a dynamic avatar from partial-view videos using a Gaussian decoder. Additionally, we refine the input pose to correct inaccuracies from SMPL fitting. This stage utilizes MSE loss, SSIM loss~\cite{SSIM}, and perceptual LPIPS loss~\cite{zhang2018unreasonable} between the predicted RGB images and ground truth, as $\mathcal{L}_{rgb}$, $\mathcal{L}_{ssim}$, and $\mathcal{L}_{lpips}$, respectively.
We also apply Frobenius Norm loss as regularization terms for optimizable canonical features $S$, offset $\Delta x$, and scale $s$:
\begin{equation}
\begin{aligned}
\mathcal{L}_{f}^{S} = \sqrt{\sum_{k=1}^{n} |S_{k}|^{2}}, 
\mathcal{L}_{f}^{\Delta x} = \sqrt{\sum_{k=1}^{n} |\Delta x_{k}|^{2}},
\mathcal{L}_{f}^{s} = \sqrt{\sum_{k=1}^{n} |s_{k}|^{2}},
\end{aligned}
\label{eq:lossL2}
\end{equation}
where $\mathcal{L}_{f}^{S}, \mathcal{L}_{f}^{\Delta x},\mathcal{L}_{f}^{s}$ denotes the loss of the $S$, $\Delta x$, and $s$, respectively.
Combining with the normal loss $\mathcal{L}_{n}$ from Eq.~\ref{eq:normalloss}, the total loss function for Stage I is as follows:
\begin{equation}
\begin{aligned}
\mathcal{L}_{StageI}= &\lambda_{rgb}\mathcal{L}_{rgb}+\lambda_{n}\mathcal{L}_{n}+\lambda_{ssim}\mathcal{L}_{ssim}\\
&+\lambda_{lpips}\mathcal{L}_{lpips}+\lambda_{\Delta x}\mathcal{L}_{f}^{\Delta x}+\lambda_{s}\mathcal{L}_{f}^{s}+\lambda_{S}\mathcal{L}_{f}^{S}.
\end{aligned}
  \label{eq:lossstage1}
\end{equation}
% We set weights for each objective as $\lambda_{rgb} = 0.8$, $\lambda_{n} = 0.8$, $\lambda_{ssim} = 0.2$, $\lambda_{lpips} = 0.2$, $\lambda_{\Delta x} = 0.85$, $\lambda_{s} = 0.03$, $\lambda_{S} = 1$. 

%Based on our experiment, SDS generates reasonable textures but struggles with unseen geometry, often resulting in flat surfaces. To address this, we first obtain normal maps from ECON~\cite{xiu2023econ} as supervision and compare them with predicted normal images using MES loss, denote as $\mathcal{L}_{normal}$. This normal supervision helps us capture detailed geometry in invisible areas, as shown in Fig.~\ref{fig: qualitative1}. The total loss function for Stage I is as follows:
% In our overall approach, we undertake a two-stage training process employing distinct loss functions tailored to specific optimization objectives. During stage I, we aim to merge canonical geometry and the visible appearance from partial-view videos into optimizable canonical features using a Gaussian parameter decoder. Additionally, we optimize the input pose to mitigate potential inaccuracies during SMPL fitting. We initially employ MSE loss, SSIM loss~\cite{SSIM}, and perceptual loss~\cite{zhang2018unreasonable} between the predicted image and the ground truth video frames. Furthermore, we apply L2-norm loss on predicted Gaussian parameters and optimizable input features:

% where $\mathcal{L}_{MSE}$ is the sum of MSE losses for predicted RGB images and predicted normal maps. 

In Stage II, the pose encoder and Gaussian decoder are optimized using SDS losses. To prevent degradation of visible appearance and geometry, $\mathcal{L}_{StageI}$ is incorporated. Additionally, $\mathcal{L}_{f}^{p}$ is added with Frobenius Norm loss to regularize the pose feature map. The total loss function for Stage II is expressed as:
% In the fine-tuning stage, the optimization of the pose encoder and Gaussian decoder is achieved through SDS losses. $\mathcal{L}_{StageI}$ is also incorporated in this stage to prevent the degradation of visible appearance and geometry. Additionally, $\mathcal{L}_{feature}$ is substituted with L2-norm loss for the pose feature in regularizing the pose in observation space. The fine-tuning loss function is expressed as:
\begin{equation}
\begin{aligned}
\mathcal{L}_{StageII}=&\mathcal{L}_{StageI}+\lambda_{p}\mathcal{L}_{f}^{p}\\
&+\lambda_{SDS}(\mathcal{L}_{SDS}^{o}+\mathcal{L}_{SDS}^{c}),
\end{aligned}
  \label{eq:lossstage2}
\end{equation}
where $\mathcal{L}_{SDS}^{o}$ and $\mathcal{L}_{SDS}^{c}$ represent the SDS loss for observation space and canonical space, respectively, as defined in Eq.~(\ref{eq:SDSObservOur}). 
%We set $\lambda_{p} = 0.5 $ and $\lambda_{SDS} = 0.3$.
% \subsubsection{Progressive training strategy}

Furthermore, we design a progressive training strategy in this stage, gradually diminishing the weight of SDS loss. This strategy is employed to enhance further the effectiveness and efficiency of the visible appearance reconstruction. More details on progressive training are in Appendix A.2.2.

% Based on this strategy, the $\lambda_{SDS}$ is reduced gradually by following:
% \begin{equation}
% \lambda_{SDS}(t) = \lambda_{SDS,0} \cdot \frac{1}{2^{\lfloor \frac{t - t_{\text{0}}}{k} \rfloor}}
%   \label{eq:prog.train}
% \end{equation}
% where $t$ and $t_{0}$ are the current epoch and starting epoch respectively, $k$ is the interval step of changing the weight. We set $t_{0} = 100$ and $k = 100$. 


% \TODO{more details can be found in xx}
