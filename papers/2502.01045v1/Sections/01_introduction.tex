
\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
% In contemporary digital environments, the quest for lifelike and interactive virtual experiences has driven notable advancements in computer graphics and virtual reality technologies. A pivotal domain of interest lies in the synthesis and manipulation of virtual avatars, which hold critical relevance across gaming, entertainment, virtual communication, and telepresence applications. 
\IEEEPARstart{V}{irtual} avatars have been a key focus in computer vision, graphics, and VR/AR technologies due to their wide applications such as gaming, entertainment, communication, and telepresence. However, reconstructing high-fidelity avatars that faithfully represent human appearance, shape, and dynamics remains a formidable challenge, particularly when confronted with ubiquitous monocular video with highly limited viewpoints.

Existing avatar reconstruction methods have difficulty in reconstructing unseen parts of the human body.
Previous methods~\cite{peng2021neural,peng2024animatable,su2022danbo,wang2022arah} typically rely on dense, synchronized multi-view inputs for the avatar reconstruction task. Recent advancements in implicit neural radians fields~\cite{mildenhall2021nerf,peng2021neural,su2022danbo,wang2022arah} and 3D Gaussian Splatting~\cite{kerbl3Dgaussians,lei2023gart,yin2023humanrecon,zheng2023gps} have explored the high-fidelity reconstruction of both geometry and appearance of dynamic human bodies from relatively sparse multi-view videos. %\ZY{However,} these synchronized multiview videos are often inaccessible in reality, which greatly limits the application scope of these works. 
To reconstruct from monocular videos, other recent methods~\cite{weng2022humannerf,su2021nerf,su2023npc,yu2023monohuman,huang2023efficient,instant_nvr,hu2023gaussianavatar} reconstruct dynamic avatars by animating them within a canonical space derived from observation spaces using video frames. These works enable learning the inter-frame deformation to reconstruct a completed human avatar from the monocular videos. However, these methods still require the video to %cover all the possible viewpoints 
have full-view coverage of the human body, and typically fail to reconstruct unseen parts in the monocular video. Unfortunately, one often only has access to partial-view videos with limited viewpoints, such as front-view videos, leaving most parts of the human body unseen. Reconstructing these occluded parts thus poses a significant challenge for current methodologies.







% we may only have access to partial-view videos with limited viewpoints, like front-view videos, and most parts of the human body are unseen. How to reconstruct these unseen parts remains a challenge for existing methods.

% This approach is ingenious because even though each observation space only captures a partial view, after deformation, these captures can be combined as multi-view supervision in the canonical space. However, these methods require the input video to be a full-view capture that includes all aspects of the human body. As a result, they struggle to synthesize high-quality free-view rendering when provided with a partial-view video.

To address this challenge, we introduce \name to achieve high-quality avatar reconstruction from partial-view monocular videos. 
% \ZY{monocular videos.}
The key idea of \name is to hallucinate the unseen parts of the human using the generative prior encoded by large-scale image diffusion models such as Zero123~\cite{liu2023zero1to3}. The hallucinations are then combined with a Gaussian Splatting~\cite{kerbl3Dgaussians}-based dynamic human reconstruction framework to get a full-body avatar.  % Thus, Wonder-Human is able to render high-quality images of the reconstructed human body from arbitrary viewpoints even though these parts do not show up in the input video.
% \begin{figure}[tb]
%   \centering
%   \includegraphics[width=\linewidth]{teaser.pdf}
%   \caption{Given a front view video, Wonder-Human can synthesize unseen parts of humans comparing with GaussianAvatar~\cite{hu2023gaussianavatar}}

%   \label{fig:intro}
% \end{figure}
However, combining diffusion-generative priors in dynamic human reconstruction is not a trivial task with two outstanding challenges. First, the existing image diffusion generative models are designed mainly to produce single-view \textit{ static} images. Thus, maintaining visually accurate generated content and consistency across frames for \textit{dynamic} human bodies using these generative priors is challenging; For instance, unrealistic artifacts such as blurs often appear when animating the generated bodies. Some existing works~\cite{ho2024sith,huang2023tech,albahar2023single} can produce human bodies from single-view images using diffusion models, but they fail to handle dynamic cases (See Appendix B.1 for more details). Second, it's challenging to ensure that the occluded or invisible portions of the human body generated by diffusion models are consistent with the observed visible parts. Any inconsistency between these generated and visible segments can significantly deteriorate the rendering quality of the human avatar, leading to visually incoherent results. 

%The second challenge is that we need to guarantee that the generated invisible parts of the human body from diffusion models are compatible with the visible parts in the monocular video. Otherwise, the inconsistency between the generated parts and visible parts will severely degenerate the rendering quality when we render this human avatar. 

% First, those image diffusion generative models are only able to produce static images. Then, maintaining the visually correct generated contents from the diffusion model is crucial for dynamic human bodies. For example, some existing works~\cite{ho2024sith,huang2023tech,albahar2023single} can produce human bodies from single-view images powered by diffusion models, but unrealistic artifacts appear when we animate the generated human bodies. The second challenge is that we need to guarantee that the generated invisible parts of the human body from diffusion models are compatible with the visible parts in the monocular video. Otherwise, the inconsistency between the generated parts and visible parts will severely degenerate the rendering quality when we render this human avatar.
% Wonder-Human addresses the above two challenges with two strategies. First, 
\IEEEpubidadjcol
To tackle these issues, we present \name for high-quality dynamic human reconstruction from monocular videos. We propose leveraging the generative priors embedded in a 2D diffusion model, trained on condensed images, to infer the unseen parts of the 3D human through distillation during reconstruction. We further introduce a novel Dual-Space Optimization method to ensure visual plausibility and consistency for dynamic human representations. 
% \ZY{This approach leverages a dynamic reconstruction technique to establish a canonical space, which is then warped to match various human poses across observation spaces.} 
Our Dual-Space Optimization utilizes Score Distillation Sampling (SDS)~\cite{poole2022dreamfusion} in both the canonical and observation spaces. This approach ensures that the generated content remains natural and complete by accounting not only for the information in a canonical pose but also for the dynamics across poses in the observation space. This significantly enhances the rendering quality when animating the reconstructed human avatar. Moreover, a view selection strategy and a pose feature injection approach are employed to reconcile conflicts between the SDS predictions and the given information and fuse pose-dependent effects, enhancing dynamic synthesis and overall avatar fidelity.

We conduct extensive experiments to validate the effectiveness of our method across broad benchmarks including ZJU-Mocap dataset~\cite{peng2021neural}, Monocap dataset~\cite{peng2024animatable}, MVHumanNet~\cite{xiong2024mvhumannet} and In-the-wild dataset~\cite{zhou2019dance}. 
Compared to state-of-the-art methods~\cite{weng2022humannerf,instant_nvr,shao2024splattingavatar,moon2024exavatar,lee2024gtu}, \name produces higher-quality photorealistic renderings of reconstructed human avatars, particularly in rendering visually plausible content for previously unseen parts of the human body. To summarize, our contributions are as follows:


% Experimental Results show that our method achieves photorealistic renderings on the reconstructed human avatars and can correctly hallucinate visually plausible contents on these unseen parts of human bodies, which is not possible for baseline dynamic human reconstruction methods like HumanNeRF~\cite{weng2022humannerf}, Instant-NVR~\cite{jiang2023instantavatar} and GaussianAvatar~\cite{hu2023gaussianavatar}. 

% Wonder-Human leverages the 3D Gaussian representation as an explicit model for reconstructing human avatars with complete appearances, even when provided with partial view inputs. 3D Gaussian representation allows human avatars to be easily reposed from the canonical space to the motion space through a forward skinning process, facilitating animatable 3D Gaussian representation reconstruction with reasonable geometry and visible appearance.

% Building upon the animatable 3D Gaussian representation, our method integrates a  viewpoints-conditioned Score Distillation Sampling (SDS)\cite{poole2022dreamfusion} to fuse invisible appearances, enabling the synthesis of novel view renderings based on given view captures. While the diffusion model aids in the synthesis of novel view renderings based on given view capture, its application has traditionally been limited to static objects. To extend its capabilities to dynamic avatars, we have adapted it using a dual space fine-tuning technique. This approach involves applying SDS loss in both observation and canonical space jointly to stabilize and improve invisible appearance reconstruction. 

%  As demonstrated in our experiments, these strategies significantly improve the quality and realism of the synthesized avatars. 


\begin{itemize}

    \item 
    We propose a novel framework named \name that leverages 2D generative diffusion priors to achieve high-quality, photorealistic reconstruction of dynamic humans from monocular videos, including accurate rendering of unseen body parts. 
    \item 
    We introduce Dual-Space Optimization to ensure visual consistency and enhance realism throughout the dynamic reconstruction process.
    \item 
   We present a view selection strategy alongside pose feature injection to resolve conflicts between SDS predictions and observed data, ensuring pose-dependent effects and higher fidelity in the reconstructed avatar.
   % \item Our method achieves SOTA performance in producing photorealistic renderings for dynamic human reconstruction, particularly for unseen parts absent from input video.
   %\item Our method achieves SOTA performance in photorealistic renderings for dynamic human reconstruction, especially for unseen parts missing from the video.
\end{itemize}
