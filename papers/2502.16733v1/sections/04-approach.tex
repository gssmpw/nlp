\section{Methodology}
\label{sec:approach}
% Here we discuss the key components for computing our concept-based score for CS 
%(which does not rely on knowing/training a model of interest on the large dataset) 
% followed by discussing how to compute it in absence of dataset's labels.
% \AM{change transpose notation}

% \begin{figure}[t]
%   \centering{\includegraphics[width=\columnwidth]{imgs/concept-extraction.png}
%   \caption{{\bf: Extracting Concepts using LLaVA:} We use LLaVA to extract appropriate concepts for dataset classes in an efficient manner. In this figure we show that to retrieve concepts for a particular class of imagenet, \textit{golden retriever}, we prompt the pretrained LLaVA model as follows. 
%   \AM{I used this picture early on. We can create a different one showing the concepts for two similar classes}}
%   }
%   \label{fig:concept-extraction}
% \end{figure}


\subsection{Generating the concept bottleneck via LLMs}
\label{sec:concept_extraction}
Since obtaining data with concept annotation is costly we use LLMs to generate concept annotation for the samples. 
However, generating attributes (word-level concepts) for all the samples in the dataset via LLMs is still costly, hence we generate attribute-level concepts only for class label names. 
This approach was recently shown to be effective at generating the concept bottleneck for interpretable image classification ~\cite{yan2023learning,yang2023language}. 
%every sample in a large dataset for attributes is 
% CBMs require data to be annotated for conceptsManually labeling and designing these attribute concepts can be costly, and does not scale to large numbers of classes.
% The first step of our approach is to extract appropriate attribute-level concepts for different categories/classes of the dataset. 
% To ensure this step is efficient, we extract concepts class-wise and not for every image in a dataset, keeping it consistent with prior work~\cite{yan2023learning,yang2023language}. 
% Furthermore, we preferred to use an open-source LLM over GPT.
In Figure~\ref{fig:overview} (block 1), we present the prompts provided to the LLMs to extract the concepts for various class label names. 
The responses of the LLM are then processed to remove formatting errors to obtain the concept sets. 
%(see App.~\ref{app:concept_extraction} for further details). 
Details of our prompt design, robustness check of different prompts, and examples of the generated attributes are mentioned in App.~\ref{app:concept_extraction}.
Once the per-class concepts are extracted, we select $k$ discriminative concepts per class (concepts that are unique to a class) to form the concept bottleneck. 
% Since some concepts may appear in more than one class, we only keep concepts unique to a particular class. 
Our final list of $k$ concepts for a class contains its name and $k-1$ attributes generated by the LLM. 
In Sec.~\ref{sec:ablation} we show an ablation study using different methods and LLMs to obtain the concept sets, and the number of concepts $k$.  

% While 
% \AM{Mention the computational complexity of extracting via class labels (We note that this step is significantly cheaper than training the model of interest on the entire dataset as required by many previously proposed CS scores.), why LLAVA and not GPT. Mention if there is anything special we did for Imagenet.}
% \AM{Create an appendix to show some class-level concepts and make a picture.}

% \AM{Mention about the use/need of class name here. Discuss some way of extending this to other modalities a little, if possible.}

% \AM{Run the experiment with mean image and 1-shot concepts}


\subsection{Concept-based score for CS}
\label{sec:concept_score_based_CS}
Next, we describe how to use the concept bottleneck to produce a difficulty score for the samples in the dataset. %informing us about each training sample's difficulty in our dataset. 
We start by discussing how we learn the functions $f$ and $g$ described in Sec.~\ref{sec:preliminaries_cbm}
%Then we discuss how to sample a coreset from the dataset based on the proposed score. 
(see Fig.~\ref{fig:overview}(block 2) for an overview of the method).
% \AM{we do not use architecture info at all}
We use the dot product between the visual embeddings of an image $x$ denoted as $\mathcal{V}_{enc}(x)$ and the bottleneck embedding matrix $E_C$ to measure the alignment between the visual features and textual features of the concepts \cite{yang2023language,yan2023learning}. Concretely, 
the concept similarity scores for a sample $x$ is computed as 
\begin{equation}
\label{eq:concept_similarity}
g(x, E_C) := \mathcal{V}_{enc}(x) \cdot E_C^\intercal. 
\end{equation}
To map the concept similarity scores to predictions in the label space $\mathcal{Y}$, we propose to use a linear (concept bottleneck layer) predictor as $f$.
Concretely, the function $f$ with parameters $W \in \mathbb{R}^{N \times N_C}$ is given by
\(f(x; W) := g(x, E_C) \cdot W^\intercal\). 
We learn the parameters $W$ for the function $f$ using %Using these definitions for $f$ and $g$, we learn $W$ for the function $f$ as
\begin{equation}
\label{eq:ce_loss}
W^* = \arg \min_{W} \frac{1}{n} \sum_{i=1}^{n} \ell(f(x_i; W), y_i), %\sum_{j=1}^N - y_i^j \log(f(x_i; W)^j).
\end{equation}
where $\ell(f(x; W), y) = -\log(f(x; W)_y)$ is the cross-entropy loss.
We define the concept bottleneck layer's output, $h(x):=f(g(x, E_C); W^*).$ % $$= g(\mathcal{V}(x), E_C) \cdot (W^*)^T$.
In practice, we learn the optimal $W$ using mini-batch gradient descent by running the optimization for $T$ epochs.
To gauge the difficulty of each training sample we use the area under the margin (AUM) \cite{pleiss2020identifying} while solving Eq.~\ref{eq:ce_loss} which quantifies the data difficulty as the margin of a training sample averaged over $T$ training epochs.  
Concretely, the margin of a sample $(x, y)$ at a training epoch $t$ is
\(M^t(x,y) = h^t_y(x) - \max_{y' \neq y} h^t_{y'}(x), \) where $h^t_{y'}(x)$ is the prediction likelihood of the bottleneck layer at epoch $h^t$ for class $y'$.
Thus, AUM (concept-based score) is computed as
\begin{equation}
\label{eq:aum_true_label}
\mathrm{AUM}(x,y) = \frac{1}{T} \sum_{t=1}^T M^t(x,y).
\end{equation}
Recent works \cite{pleiss2020identifying, zheng2022coverage, zheng2024elfs} have demonstrated the effectiveness of AUM for gauging the sample's difficulty for coreset selection. 
However, \cite{zheng2022coverage,zheng2024elfs} requires computing AUM for the downstream model by training it on the entire dataset once, which is computationally costly. 
On the other hand, our method integrates AUM with the training of the linear concept bottleneck layer $h$, which is computationally cheaper (training a linear layer takes only $7$ minutes for Imagenet compared to $8$ hours for training a ResNet-34) than training the downstream model ($\theta$).
Moreover, since our method does not use the downstream model in the computation of the difficulty score, the coresets are independent of the model architecture, unlike previous training dynamics-based approaches.
% Moreover, as we show in Sec.~\ref{sec:experiments} the coreset obtained through our method (which is independent of the knowledge/training dynamics information of the model of interest $\theta$) is transferable to various model architectures at different pruning rates.


{\bf Sampling training examples to form a coreset.} 
After obtaining data difficulty scores, a crucial step is choosing the samples to form the coreset. 
While many previous works \cite{toneva2018empirical, coleman2019selection} have reported encouraging results keeping the most challenging samples (for our concept-based score this means samples with the smallest margin), recent works \cite{zheng2022coverage,sorscher2022beyond} have shown that this could lead to a catastrophic drop in accuracies after training the downstream model on the coreset, especially when the size of the coreset is small.
This is mainly due to poor sample coverage and potentially mislabeled data in the datasets.
To remedy this, we use Coverage-centric Coreset Selection (CCS) proposed by \cite{zheng2022coverage} (see Alg.~\ref{alg:ccs} in App.~\ref{app:CCS_sampling}) which uses a stratified sampling approach and filters out (potentially) mislabeled samples to form the coreset.
This technique has been shown to achieve consistently superior results to the random baselines for various coreset sizes. 
% We provide a brief overview of this method in App.~\ref{app:CCS_sampling}. % for completeness. 
% \AM{Mention why we chose CCS and not D2 pruning.}
% \AM{Rerun sampling with D2 pruning for rebuttal.}


% \begin{table*}[tb]
%   \caption{Comparison of the model's test accuracy after training on coresets found by various approaches on different datasets shows that our approach leads to coresets with significantly better performance than random and achieves results similar to those found by methods dependent on the training dynamics, even for high pruning rates. 
%   }
%   \label{Table:coreset_performance}
%   \centering
%   \small
%   \resizebox{\textwidth}{!}{
%     \begin{tabular}{c|c|cccc|cccc|cccc|}
%     \toprule
%    \multirow{3}[4]{*}{Method} & \multirow{3}[4]{*}{\makecell{Need \\ Training \\ Dynamics?}} & \multicolumn{12}{c|}{\makecell{Datasets and Pruning Rates}} \\ 
%    \cmidrule(lr){3-14}
%    & & \multicolumn{4}{c|}{\makecell{CIFAR-10}} & \multicolumn{4}{c|}{\makecell{CIFAR-100}} & \multicolumn{4}{c|}{\makecell{Imagenet}} \\
%     \cmidrule(lr){3-6} \cmidrule(lr){7-10} \cmidrule(lr){11-14}
%     & & 30\% & 50\% & 70\% & 90\% & 30\% & 50\% & 70\% & 90\% & 30\% & 50\% & 70\% & 90\% \\
   
%     \midrule
%     Random & No & 94.33 & 93.4 & 90.94 & 79.08 & 74.59 & 71.07 & 65.30 & 44.76 & 72.18 & 70.34 & 66.67 & 52.34 \\
%     Random$_{\mathrm{FFCV}}$ & No & - & - & - & - & - & - & - &- & 73.37 & 71.71 & 67.85 & 51.29 \\
%     \midrule
%     Entropy  & Yes & 94.44 & 92.11 & 85.67 & 66.52 & 72.26 & 63.26 & 50.49 & 28.96 & 72.34 & 70.76 & 64.04 & 39.04 \\
%     Forgetting  & Yes & 95.40 & 95.04 & 92.97 & 85.70 & 77.14 & 74.45 & 68.92 & 55.59 & 72.60 & 70.89 & 66.51 & 52.28 \\
%     AUM  & Yes & 95.27 & 94.93 & 93.00 & 86.08 & 76.84 & 73.77 & 68.85 & 55.03 & 72.29 & 70.52 & 67.78 & 57.36 \\
%     \midrule
%     Ours & No & 94.77 & 93.44 & 91.80 & 84.63 & 75.98 & 72.22 & 66.53 & 51.85 & 73.39 & 72.34 & 69.44 & 55.92 \\
%     \bottomrule
%     \end{tabular}
%     }
% \end{table*}



\subsection{Concept-based score for label-free CS}
\label{sec:label_free_CS}
Recently, there has also been an interest \cite{zheng2024elfs, maharana2023d2, griffin2024zero} in identifying the representative samples from an unlabeled dataset such that 1) we reduce the samples that need to be labeled by humans and 2) we can improve the efficiency of model training by only training the model on a subset of data. 
Our concept-based score can also be effectively utilized for this task with a simple modification. 
Similar to previous works \cite{maharana2023d2,zheng2024elfs,sorscher2022beyond}, we assume that we know the number of classes in the datasets. Additionally, we assume that we also know the names of the classes in the datasets. %(\AM{revisit this} this may be different from dataset class names). 
Previous works have demonstrated that VLMs such as CLIP \cite{radford2021learning} achieve excellent zero-shot performance without requiring fine-tuning on specific datasets. 
We leverage this capability of CLIP models to obtain pseudo-labels for the images in our unlabeled dataset and use them to obtain our difficulty score for each sample % as %as follows 
\begin{equation}
\label{eq:aum_pseudo_label}
\mathrm{AUM}(x, y_{\texttt{pseudo}}) = \frac{1}{T} \sum_{t=1}^T M^t(x,y_{\texttt{pseudo}}),
\end{equation}
where for an image $x$ in the dataset, $y_{\texttt{pseudo}} = \arg \max_{j \in \mathcal{Y}} \mathcal{V}_{enc}(x) \cdot \mathcal{W}^\intercal_{\texttt{zeroshot}}$ where $\mathcal{W}_{\texttt{zeroshot}} \in \mathbb{R}^{N \times d}$ is a matrix with columns defined as $\mathcal{T}_{enc}(s_j)$ and $s_j = $ ``a photo of a \{\texttt{$j^{th}$ \texttt{class name}}\}" for each class $j \in \mathcal{Y}$ \cite{radford2021learning, wortsman2022robust}.
We use these scores along with CCS to produce the coreset. 
Similar to \cite{zheng2024elfs, maharana2023d2}, this coreset is then assumed to be annotated by humans and downstream models ($\theta$) are trained on this annotated coreset. % and their performance is reported in Sec.~\ref{sec:experiments}. 
