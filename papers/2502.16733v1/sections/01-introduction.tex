\vspace{-0.5cm}
\section{Introduction}
\label{sec:introduction}
Machine learning (ML) pipelines are becoming increasingly intensive regarding their data and compute requirements~\cite{touvron2023llama, achiam2023gpt}. 
While in line with the empirical neural scaling laws~\cite{kaplan2020scaling, hestness2017deep, henighan2020scaling, rosenfeld2019constructive} where a model's performance improves with the increasing model and training data size, these improvements come at an unsustainable cost of compute/energy. 
Recently,~\cite{sorscher2022beyond} showed analytically that data pruning can enable models to achieve an exponential reduction in the test error with an increasing dataset size.

Coreset Selection~(CS)~\cite{mirzasoleiman2020coresets, guo2022deepcore,paul2021deep,xia2022moderate,maharana2023d2,zheng2022coverage,choi2024bws} aims to improve the efficiency of training large ML models by pruning a large dataset and retaining only a small subset of representative samples.
Most CS methods, use metrics to estimate the difficulty/importance of every training sample and then use a sampling strategy to produce a coreset, that leads to similar performance on a downstream model as training it on the entire dataset. 
%when used for training a model of interest, produces the same performance as when this model is trained on the entire dataset. 
Many state-of-the-art (SOTA) CS methods use the downstream model's training dynamics --- the
model's behavior on a sample during training, to generate a difficulty score for each sample. 
This enables accurate estimation of the data difficulty but requires training the downstream model on the entire dataset at least once, which can be costly when training a large model on a large dataset.
% prohibitive 
% since training a large model on a large dataset even once is computationally challenging.
While~\cite{coleman2019selection} showed that a coreset selected using training dynamics of a mid-sized proxy model (eg., ResNet-18) is effective for a larger downstream model (eg., ResNet-50), training even such a model is not feasible for large datasets. 
% While \cite{coleman2019selection,zheng2022coverage} showed that a mid-sized proxy model (eg., ResNet-18) could be used to approximate the training dynamics of the actual model of interest (eg., ResNet-50).
% While a mid-sized proxy model may be easier to train than the large model of interest, training even such a model once is still prohibitive when using large datasets. 
% While a mid-a proxy model may be easier to train than the large model of interest, training such a model on a large dataset is still computationally costly. 
% Moreover, the difficulty score computed from these methods is tied to the actual model and may not to estimate the importance of each training sample. 
% However, obtaining information about the training dynamics requires training the model of interest on the full dataset at least once. 
Moreover, these scores are difficult to interpret as they are downstream model-dependent and cannot inform if the sample will be hard or easy to learn for a different downstream model (without training it first).
Thus, we aim to answer the following question
\emph{``How to efficiently estimate a sample's difficulty in an interpretable and model-agnostic way i.e. without the knowledge of the architecture or training dynamics of the downstream model?"}
% How to make such scores more interpretable and aligned to human intuition. 
% training the model of interest on the dataset even once? How can such scores be designed to reflect a sample's difficulty independently of the model?
% "}

% \AM{such scores are interpretable in terms of the model of interest only. what if we don't have the model of interest?}
% \begin{figure}[t]
%   \centering{\includegraphics[width=\columnwidth]{imgs/concept-extraction.png}
%   \caption{{\bf: Automatically generating concept annotation using a language model (LM):} We show the prompt sent to a LM and the concepts it generated for the \textit{golden retriever} class of Imagenet.
%   \AM{Replace the video LLAVA pic with a generic language model.}}
%   }
%   \label{fig:concept-extraction-1}
% \end{figure}

% \begin{figure*}[t]
%   \centering{\includegraphics[width=0.90\textwidth]{imgs/Approach.pdf}
%   \caption{{\bf: Overview of our approach:} We start by learning a linear model that uses concept similarity scores, computed as the alignment between the visual information of a sample and the $k$ concepts for each class (obtained via a language model), to produce a prediction for a training sample. 
%   Based on this, the difficulty score for a training sample is computed as its average margin (i.e., the difference between the likelihood of the correct and the remaining classes) over $T$ training epochs.}
%   }
%   \label{fig:overview}
% \end{figure*}


\begin{figure*}[t]
  \centering{\includegraphics[width=0.92\textwidth]{imgs/overview-tm.pdf}
  \vspace{-0.15cm}
  \caption{\small{\bf: Overview of our approach:} 
  We start by prompting the language models to generate concept annotation for $N$ class label names in the dataset and select $k$ most discriminative attributes (per class) to form the concept bottleneck, which are passed through a text encoder ($\mathcal{T}_{enc}$) to obtain the bottleneck embedding matrix ($E_{C}$). 
  The visual information of a training sample $x$ extracted via the visual encoder  ($\mathcal{V}_{enc}$) is then aligned to the embedding matrix using a linear concept bottleneck layer trained for $T$ epochs. 
  Our difficulty score for $x$ is then computed as the average margin (i.e., the difference between the likelihood of the correct and other classes) over $T$ epochs. 
  Finally, a coreset is selected using stratified sampling which is then used to train an unknown downstream model. 
  %\AM{update stratified}
  % \AM{Add coreset and multiple models of interest}
  % \emph{Step 1:}   
  %  Automatically generate a concept annotation for all the categories in the dataset using a language model (LM). These concepts are then used as a bottleneck in the CBM. \emph{Step 2:} Learn a linear layer using the concept similarity scores, computed as the alignment between the visual information of a sample and the $k$ concepts for each category, to produce a prediction for a training sample. 
  %  Using this, our concept-based difficulty score for a training sample is computed as the average margin (i.e., the difference between the likelihood of the correct and the remaining classes) over $T$ training epochs. \AM{make it smaller}
   %\AM{Typo in score}
  % \AM{Can we put more discriminative concepts in the picture. They need not be what we are getting from the model.}
  }
  }
  \label{fig:overview}
\end{figure*}

To address this question, we use Concept Bottleneck Models (CBMs)~\cite{koh2020concept,yuksekgonul2022post} which are effective at making ML models more interpretable. 
%and providing a handle for intervening with their predictions. 
CBMs map a model's input on to a set of human-understandable concepts, referred to as the ``bottleneck'' and then use them to make a prediction. 
However, CBMs require concept annotation for every sample in the training data, which can be costly to obtain. 
Recently~\cite{yang2023language,yan2023learning} showed that this limitation of CBMs can be overcome by leveraging the advances in Large Language Models (LLMs) and Vision Language Models (VLMs) that can be prompted to generate concept annotation for the training samples without requiring any task-specific fine-tuning.  
Our Fig.~\ref{fig:overview} (block 1) shows the prompt we used to obtain the concept bottleneck using an LLM.
%to obtain concept bottleneck for all the class labels 
% categories 
%in the dataset. 
Once the bottleneck is formed, we use a VLM (such as CLIP \cite{radford2021learning}) to measure the alignment between the visual features and the concept bottleneck (denoted as concept similarity in Fig.~\ref{fig:overview} (block 2)). 
A simple linear concept bottleneck layer is then trained to align the visual and concept features to make predictions on the training samples. 
We use the average margin of a training sample during the training of this bottleneck layer as our concept-based difficulty score. % for each training sample. % training dynamics of this bottleneck layer is then utilized to identify the difficulty of each training sample (Fig.~\ref{fig:overview} (right)).
% alignment score is then used to train a linear layer which makes a prediction for the training sample. 
% While training this linear layer we keep track of how a sample's margin changes and use this concept-based score to measure a sample's difficulty (Fig.~\ref{fig:overview} (right)).
We use a stratified sampling \cite{zheng2022coverage} (block 3) using our score to form the coreset which can then be used to train various downstream models Fig.~\ref{fig:overview} (block 4). 
% We then use this score along a stratified sampling strategy to 
% Fig.~\ref{fig:overview} provides an overview of our approach. 
Our method speeds up the computation of the coreset by $15$ times compared to approaches based on the downstream model's training dynamics across three benchmark datasets. %Training the concept bottleneck layer is significantly cheaper than training the downstream model to compute the difficulty score of the samples.
Moreover, compared to coresets found by other training dynamics-free approaches our coresets lead to downstream models with significantly better accuracy (see Sec.~\ref{sec:experiments}). % shows that our method achieves significantly better results showing its effectiveness for CS.
% to find the coreset s $15$ times faster than training a ResNet-34 model on the Imagenet dataset).
% Thus, unlike previous CS methods, our score which is computable without the knowledge of the architecture and avoids training the downstream model on the entire dataset (even once), makes CS efficient and model agnostic.

We empirically evaluate the effectiveness of our score on three benchmark datasets: CIFAR-10/100 \cite{krizhevsky2009learning} and Imagenet-1K \cite{deng2009imagenet}.
Our results show that downstream models trained on our coresets consistently achieve better accuracy than randomly sampled subsets, especially at high data pruning rates, and achieve performance close to SOTA CS methods. % dependent on the training dynamics of models of interest. 
We also show that our approach is effective for the label-free CS problem where the dataset is unlabeled and leads to models with superior performance compared to SOTA label-free CS methods on Imagenet.
% Our results show that models trained on our coresets exceed performance than random subsets and can even outperform SOTA label-free CS on Imagenet.
Since our CS method is model agnostic, we show that our coreset leads to high performance regardless of the architecture of the downstream model, unlike other methods which require training the downstream model on the dataset first to identify the coresets for a new model architecture. 
%suggesting that our coresets capture representative samples from the dataset with ease. 
% also show that our coresets are transferable to many model architectures showing that our score helps identify difficult samples regardless of the model information. 
%M
% methods based on training dynamics of the models of interest.
Lastly, we show that our concept-based difficulty score is aligned with human intuition, allowing us to interpret why examples are easy/hard, independent of the downstream model. 
Our main contributions are as follows:
\vspace{-0.15cm}
\begin{itemize}
    \item We propose a concept-based score for CS that gauges sample difficulty in a model-agnostic way without training a downstream model on the full dataset, speeding up CS by $\approx$15x compared to SOTA methods.
    % and speeding up CS by approximately 15x compared to SOTA methods.
    
    %We propose a concept-based score for CS for gauging a sample's difficulty in a model agnostic way eliminating the need to train a downstream model on the entire dataset, speeding up CS by $\approx$15 times compared to SOTA methods. 
    
    % We propose a concept-based score for interpretable and efficient CS that eliminates the need for training the downstream model on the entire dataset even once.

    % downstream model agnostic, efficient + 15% speedup
    % 5% on average
    % interpreatbility, mislabeled data detection
    % data centric and emphasize that concepts are also a major contributionn for CS. Mention VLMs will get better at concept extraction helping CS
    
    % We propose a concept-based and model-agnostic data difficulty score for improving the interpretability and efficiency of CS that avoids training a large model on the entire dataset even once.

    % We propose an interpretable and efficiently computable data difficulty score for CS using concept bottlenecks generated by pre-trained LMs that avoids training a large model on the entire dataset even once.

    \item Our coresets improve model accuracy by $5$\% on average over random subsets at high pruning rates, are competitive to SOTA methods, generalize to various architectures, and are computable without labels.
    
    %Coresets found by our approach are transferable to various model architectures 
    % Our method finds coresets leading to high performance on various downstream model architectures and can do so without even requiring dataset's labels.

    \item We show that using LLM-generated concepts makes our score human-understandable, enabling a generalizable and data-centric solution for CS. 
    %is aligned with human perceived difficulty of samples  
    
    %Our coresets improve accuracy by $5$\% on average over random subsets at high pruning rates, match or outperform SOTA CS methods, particularly on Imagenet at lower pruning rates, and achieve a $15$x speed-up for CS compared to training dynamics-based approaches.
    % We show that our coresets lead to a performance improvement of 5\% on average over random subsets at high pruning rates, is at par or better than SOTA CS methods especially on Imagenet for lower pruning rates, and gives a 15x speed-up over CS by training dynamics based approaches. 
    % achieve better performance than random subsets on three benchmark datasets and achieve performance similar to coresets found by SOTA training dynamics-based CS methods.

\end{itemize}






