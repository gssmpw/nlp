\section{Preliminaries}
\label{sec:preliminaries}
% Here we formulate the CS problem, discuss some metrics for it and provide an overview of CBMs. % for the it, followed by an overview of the concept bottleneck models.

% \vspace{-0.1cm}
\subsection{Coreset selection (CS) problem formulation}
\label{sec:preliminaries_cs}
Consider a classification task and data distribution $P$. 
Let $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^{n}$ denote the dataset of $n$ training examples sampled i.i.d. from the distribution $P$ where $x_i$ denotes the data and $y_i \in \mathcal{Y}$ denotes the label from a set of $N$ classes.
CS \cite{coleman2019selection,zheng2022coverage} aims to find a subset $\mathcal{S}$ of $\mathcal{D}$ consisting of $m \leq n$ samples such that the models trained on $\mathcal{S}$ achieve performance comparable to models trained on $\mathcal{D}$. 
Formally, the CS problem is %defined as: % follows,
\begin{equation}
\label{eq:coreset_selection}
\min_{\mathcal{S}:|\mathcal{S}|=m} \mathbb{E}_{(x,y) \sim P}[\ell(x,y|\theta_{\mathcal{S}})] - \mathbb{E}_{(x,y) \sim P}[\ell(x,y|\theta_{\mathcal{D}})],
\end{equation}
where $\theta_{\mathcal{D}}$ and $\theta_{\mathcal{S}}$ denote the ``\emph{downstream model}"  trained on $\mathcal{D}$ and $\mathcal{S}$ (coreset), respectively and $\ell$ is the loss function.

To find this subset $\mathcal{S}$, previous works have proposed scores to gauge a sample's difficulty for a model, and are later used to form the coreset. 
Approaches such as max entropy uncertainty sampling \cite{lewis1994heterogeneous, Settles_2012}, and least confidence \cite{culotta2005reducing} estimate difficulty using the uncertainty of the model's predictions on a sample. %, to gauge its difficulty.
Another set of approaches such as $k$-center greedy \cite{sener2017active} uses  geometric information of the data to filter out redundant samples. 
Yet, another set of approaches uses information from the training dynamics of the downstream model to estimate the a difficulty score. 
Scores such as the forgetting score \cite{toneva2018empirical} which is computed as the number of times a sample gets misclassified after being correctly classified earlier during model training, the area under the margin (AUM) \cite{pleiss2020identifying} which identifies mislabeled/difficult samples, fall in this category.
% Based on these score, a sampling strategy is then used to identify $\mathcal{S}$.
While approaches based on the training dynamics of the downstream model have achieved SOTA results, the requirement of knowledge/training the downstream model or a relatively big proxy model on the entire dataset at least once is inefficient for large datasets/models, even if done only once. 
This motivates the need of data centric approaches for sample's difficulty independent of the downstream model.
% \AM{Sampling strategy}

\subsection{Concept bottleneck models (CBMs)}
\label{sec:preliminaries_cbm}
Recent advances in language model-guided CBMs utilize an LLM to obtain concept bottlenecks which are then used to predict the labels. 
These works rely on a pre-trained multi-modal models (such as CLIP \cite{radford2021learning}) which consists of a visual encoder $\mathcal{V}_{enc}$ and a text encoder $\mathcal{T}_{enc}$ that can map images and text to a $d$-dimensional representation space. % with dimension $d$. 
Let $C = \{c_1, c_2, \cdots, c_{N_C}\}$ be the set of $N_C$ concepts (bottleneck) generated via a LLM, we can then construct a bottleneck embedding matrix $E_C \in \mathcal{R}^{N_C \times d}$ such that each row of the matrix is mapping of the concept $c \in C$ after passing it through textual encoder $\mathcal{T}_{enc}$.
Based on this, a CBM  \cite{yang2023language} produces a prediction $h(x) = f(g(\mathcal{V}_{enc}(x), E_C))$ for a sample $x$ where $g:\mathbb{R}^{d} \rightarrow \mathbb{R}^{N_C}$ computes the similarity of the visual features to each concept in the bottleneck and $f:\mathbb{R}^{d} \rightarrow \Delta$ outputs the probability of each class in the label set $\mathcal{Y}$, where $\Delta$ is a $N$ simplex.
We discuss details of $f$ and $g$ in Sec.~\ref{sec:approach}.