\begin{abstract}
Coreset Selection (CS) identifies a subset of training data that achieves model performance comparable to using the entire dataset. 
Many state-of-the-art CS methods, select coresets using scores whose computation requires training the downstream model on the entire dataset and recording changes in its behavior on samples as it trains (training dynamics). 
These scores are inefficient to compute and hard to interpret as they do not indicate whether a sample is difficult to learn in general or only for a specific model. 
Our work addresses these challenges by proposing an interpretable score that gauges a sample's difficulty using human-understandable textual attributes (concepts) independent of any downstream model. 
Specifically, we measure the alignment between a sample's visual features and concept bottlenecks, derived via large language models, by training a linear concept bottleneck layer and compute the sample's difficulty score using it. 
We then use this score and a stratified sampling strategy to identify the coreset.
Crucially, our score is efficiently computable without training the downstream model on the full dataset even once, leads to high-performing coresets for various downstream models, and is computable even for an unlabeled dataset.
Through experiments on CIFAR-10, CIFAR-100, and ImageNet-1K, we show our coresets outperform random subsets, even at high pruning rates, and achieve model performance comparable to or better than coresets found by training dynamics-based methods.

%However, many state-of-the-art CS methods, use computationally inefficient scores requiring training of the downstream model on the entire dataset and recording changes in its behavior on samples as it trains (training dynamics). 
%Moreover, such scores are difficult to interpret as they do not indicate whether a sample is hard to learn in general or only for a specific model. 
%, hurting their practical utility. 
%to obtain concept bottlenecks and then align the visual features of a training sample, which are then aligned to the visual features of a training sample forming a concept bottleneck layer. 
% and its training dynamics.
%based on how a downstream model's behavior for a sample changes as the model trains on the entire dataset (training dynamics). 
% This hurts the practical utility of such CS methods especially for large datasets/models. 
%changes in the behavior of a downstream model as it trains on the entire dataset (training dynamics), reducing their practical utility.
% and concept bottlenecks generated via a language model for the categories in the dataset. 
% We then train a linear classifier on these alignment scores to estimate difficulty scores for the samples, which are used to form the coreset.
% on how a specific model's behavior changes  training dynamics of the models trained on the entire dataset, r
% universally difficult or only challenging for a particular model.
% Using experiments on CIFAR-10/100 and Imagenet-1K, we show that the coresets found by our approach outperform the random subsets, even at high pruning rates, achieving performance similar to approaches dependent on the model's training dynamics.
%into a sample level actual difficulty  selected by these approaches lack interpretability, since they are model dependent making it unclear why certain samples are easy/hard for a model.
% bottlenecks concepts extracted for different labels from language models.
% Coreset selection (CS) seeks a subset of the training data such that a model trained on this subset matches the performance of the model trained on the entire dataset.
% Moreover, such difficulty scores are hard to interpret since they are model dependent and do not tell if the sample is difficult for any model or just this model.
% Specifically, we first measure the alignment between a sample's visual information and language model generated concept sets for the labels in the dataset.
% Next, we train a linear classifier on these alignment scores to estimate the difficulty score for the training samples and use it to form the coreset. 
% Coreset selection (CS) identifies a training data
% subset that achieves model performance compara-
% ble to using the full dataset. However, many state-
% of-the-art CS methods select the coreset based on
% data difficulty scores dependent on the training dy-
% namics of the models trained on the entire dataset,
% reducing their practical utility. Moreover, these
% scores are hard to interpret as they do not indicate
% whether a sample is universally difficult or only
% challenging for a particular model. In this work,
% we tackle these challenges by proposing a simple
% and interpretable metric to estimate a sample’s
% difficulty in terms of human-understandable con-
% cepts, independent of the model and it’s training
% dynamics. Specifically, we measure the alignment
% between a sample’s visual features and concept
% bottlenecks generated via a language model for
% the categories in the dataset. We then train a linear
% classifier on these alignment scores to estimate
% difficulty scores for the samples, which are used
% to form the coreset. Crucially, our metric avoids
% training the model of interest on the entire dataset
% even once, produces transferable corsets, is in-
% terpretable, and can be computed even without
% dataset labels. Through experiments on CIFAR-
% 10, CIFAR-100, and ImageNet-1K, we demon-
% strate that our approach identifies coresets that
% outperform random subsets, even at high pruning
% rates, and achieves performance comparable to
% methods dependent on the training dynamics
\end{abstract}