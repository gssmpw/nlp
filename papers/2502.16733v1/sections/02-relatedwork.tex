\section{Related Work}
\label{sec:related-work}
{\bf Coreset selection (CS):} 
CS improves the efficiency of model training by selecting a subset of influential samples. % and discarding less influential ones.
Various approaches have been proposed to generate such a subset \cite{guo2022deepcore}. 
A popular approach uses influence functions \cite{koh2017understanding,chatterjee1986influential, liu2021influence,schioppa2022scaling} which measures the influence of a sample by considering the effect of removing it from the model's training.
While effective, these approaches are computationally costly due to their dependence on higher-order derivatives. 
Another set of approaches selects a subset by either matching the gradients to those computed on the entire dataset \cite{mirzasoleiman2020coresets, killamsetty2021grad} or uses training dynamics of a model \cite{toneva2018empirical, pleiss2020identifying,lewis1994heterogeneous,culotta2005reducing,paul2021deep} to compute the importance of a sample. 
However, such approaches require repeated training of the downstream model to produce accurate importance scores.
In comparison, our approach avoids using any knowledge of the downstream model for computing the difficulty scores.
Finally, using the dataset's geometric properties via clustering is another popular choice for CS \cite{sener2017active,sorscher2022beyond,feldman2020turning,feldman2011unified, huang2019coresets}. 
However, the high computational complexity due to their dependence on pairwise distances between the samples prohibits their use on large datasets. 
% efficiently and in an interpretable way.


{\bf Concept-based interpretability approaches: }
Concepts are defined as high-level semantics that refers to the abstract and human-interpretable meanings of the visual data, such as objects, actions, and scenes, as opposed to low-level features like edges or textures~\cite{wu2016valueexplicithighlevel}. 
% These high-level semantic features help interpret visual data in a manner consistent with human understanding. 
Concepts have been used in interpretable computer vision to bridge the gap between human understanding and machine perception in various tasks such as image classification. 
Such interpretability methods can be broadly classified as \textit{post-hoc methods} (do not impose any model constraints) or \textit{by-design} methods. 
Concept Bottleneck Models (CBMs) extend interpretable-by-design approaches by using human-understandable attributes as an intermediate layer for predictions, as used in few-shot learning~\cite{lampert2013attribute} and attribute learning~\cite{xu2020attribute,russakovsky2012attribute}.
While interpretable, CBMs reliance on costly annotations and lower accuracy compared to end-to-end models limit their usage.
Post-hoc Concept Bottleneck Models (PCBMs) address these issues by incorporating static knowledge bases (e.g., ConceptNet~\cite{speer2017conceptnet}) and residual connections to boost accuracy~\cite{yuksekgonul2022post}.
Recently~\cite{yang2023language,yan2023learning} incorporated LLMs to identify the concept bottleneck to make classification more explainable. 
We build on this literature and use CBMs for CS. 
% unlike our work which uses them for CS.
% However, unlike our work, these works focus on making classification more explainable rather than which use the concepts for the CS problem, their task was to make classification more explainable. 
% For additional related work.
See App.~\ref{App:additional_rw} for other related work.



