\appendix
\onecolumn
\begin{center}
{\LARGE \bf Appendix}
\end{center}

We present additional related work in Appendix~\ref{App:additional_rw}. Then we describe the details of our methodology for extracting the concepts from LLaVA in Appendix~\ref{app:concept_extraction} and present additional experiments and implementation details of our experiments in Appendix~\ref{app:implementation_details} including the algorithm for stratified sampling used in our work in Appendix~\ref{app:CCS_sampling}.


\if0
\section{TODO (ignore)}
\begin{itemize}
    \item Make concept-extraction few-shot and more efficient. (Check if class-level concepts are good. If not go for a few-shot image level.)
    \begin{itemize}
        \item Top-k concepts extracted with BERT/LLAVA with class names.
        \item Labo concept selection.
    \end{itemize}

    \item Scale the method to Imagenet and other Image datasets. 

    \item Ablation studies
     \begin{itemize}
        \item Number of top concepts k to keep.
        \item Linear probing vs Non-linear model training with MLPs.
        \item Submodular optimization for top-k concept extraction. 
        \item Evaluate coresets on other model architectures.
        \item Evaluate concept quality with GPT, LLAVA, etc. (Use concise + LABO)
        \item Add additional baselines of Kmeans and other methods.
        \item Show results with image-level concepts compared to class-wise concepts.
     \end{itemize}

     \item Label-free approach: Use CLIP zero-shot predictions as pseudo-labels for the label-free approach. (Additionally, Label noise + feature noise in images and show the Robustness of concepts to these noises.

     \item Interpretability of concept/weight scores + Why does our score not match the AUM score exactly?

     \item Find coreset for a target task from a dataset without labels. Cats and Dogs from a general dataset.

     \item Comparison of vit B-32 and L-14?

     \item Run CS via D2 Pruning.

     \item Compare coresets selected by different methods.
     
\end{itemize}
\fi

\section{Additional related work on concept-based interpretability}
\label{App:additional_rw}
Interpretability methods can be broadly classified as \textit{post-hoc methods} (do not impose any model constraints) or \textit{by-design} methods. 
Post-hoc methods include Gradient-weighted Class Activation Mapping approaches~\cite{bau2017network,selvaraju2017grad,mu2020compositional,hernandez2021natural} that trace network gradients to identify the input areas that guide predictions and Explanation Generation methods~\cite{singh2023explaining,nishida2022improving,kim2018textual,hendricks2016generating} that require models to produce explanations for visual tasks by conditioning their predictions on captioning models or incorporating visual evidence to ground explanations~\cite{hendricks2018grounding,park2018multimodal}.
Interpretable-by-design methods, such as Prototype methods, optimize a metric space where classifications are based on distances to class prototypes, identifying important input regions but often obscuring their semantic content~\cite{nauta2021neural,chen2019looks,snell2017prototypical,satorras2018few,vinyals2016matching}. 

Concept Bottleneck Models (CBMs) are a part of interpretable-by-design approaches that use human-understandable attributes as an intermediate layer for predictions.
A recent advancement, Computational Derivation Learning (CompDL), utilizes a CBM architecture by applying a linear layer over CLIP scores between expert-designed concepts and images, improving evaluation of how well CLIP grounds concepts~\cite{yun2022vision}.  
Post-hoc Concept Bottleneck Models (PCBMs) were recently proposed to ease the requirement of CBMs to rely on costly concept annotations and improve their accuracy compared to end-to-end models. 
However, PCBMs are limited by the coverage of knowledge bases, making them unsuitable for large-scale or domain-specific tasks or fine-grained classification, and their residual predictors can undermine interpretability by blending CBMs with end-to-end models.

High-level semantic-driven descriptions are also used to guide data augmentation to build an informative set~\cite{wickramanayake2021explanation} to make model training efficient with a good enough training set. 
Prior works use external knowledge bases to obtain these textual semantic concepts to guide vision models~\cite{bujwid2021large,kil2021revisiting,roth2022integrating,shen2022k}. 
Thus, the use of concepts has been shown to improve interpretability in various domains. However, to the best of our knowledge, we are the first ones to propose a concept-based score for the CS problem and show its competitiveness to SOTA model training dynamics-dependent approaches.



\section{Details for concept set generation}
\label{app:concept_extraction}
\noindent\textbf{Prompt Selection: }To extract concepts for our approach, we only use the class labels in the prompt as can be seen in Figure~\ref{fig:overview}. 
The prompt, ``Can you give distinct attributes for $ \left< \text{class\;name}\right>$. Give the output separated by a comma in the line.'' instructs the VLM not only to provide distinct keywords but also adds formatting instructions. 
However, despite the instructions included in the prompt, LLaVA outputs are not always formatted well, often containing duplicate entries, mismatched commas and braces, and sometimes having a detailed explanation before the keywords. 
To remedy this we run the LLaVA output through a simple post-processing script and use regular expressions to clean the LLaVA outputs. 
For our experiments where we perform ablation of various concept-bottleneck generation methods~(Table~\ref{Table:ablation_concept_extraction}), we also use two more concept generation methods, one is one-shot image-based class concepts and the second is image-level concept generation. 
For the former, where we select one representative image per class via clustering, we prompt LLaVA as follows, ``$ \left< \text{image}\right>$ Can you give distinct attributes for such an image of $ \left< \text{class\;name}\right>$. Give the output separated by a comma in the line.''
And, to get concepts for every image of a class, we use a similar prompt as follows, ``$ \left< \text{image}\right>$ Can you give distinct visual attributes for this image of $ \left< \text{class\; name}\right>$. Give the output separated by a comma in the line.'' Each LLaVA prompt request on a single A-100 GPU takes approximately 3 seconds. 

\noindent\textbf{Alternative VLMs for Concept Generation: } We leverage LLaVA as our choice of VLM for concept generation, however in Table~\ref{Table:ablation_concept_extraction}, we also compare against concepts extracted from GPT~(column 2)~\cite{yan2023learning}. 
We see comparable performance with those extracted from LLaVA. Moreover, LLaVA is an open-source model whereas GPT prompting is not. We also experimented with retrieving concepts via SpLiCE~\cite{bhalla2024interpreting}.
However, a major limitation of SpLiCE is that similar to image-level concepts it is a costly approach. SpLiCE uses a linear optimization for sparse concept decomposition and can take up to 3 hours for $50,000$ images which is slower than generating class-level concepts from LLaVA.  
% \AM{TM, Can you explain more why it is a costly approach and we did not use it?}. 

% Talk about the prompts used (one-shot, image-level)
% Talk about alternatives to concept extraction; e.g. SPLICE, and why we did not select that. 
% 
%
\section{Additional experiments and implementation details}
\label{app:implementation_details}

\subsection{Visualizing easy/challenging samples based on concept-based score}
\label{app:visualization}
Similar to Fig~\ref{fig:easy_difficult_examples} in Sec.~\ref{sec:visualization} of the main paper, we visualize easy and challenging examples in Fig.~\ref{fig:easy_difficult_examples_all} for CIFAR-10 and subset of classes from CIFAR-100. 
As observed the easy images (the ones that get high scores in our approach) are more canonical images of the class labels whereas the challenging ones are images that can potentially be assigned another class in the same dataset or are mislabeled in the dataset.
The clear distinctions between these images show that our concept-based score aligns well with human intuition on the difficulty of the samples.

\subsection{Algorithm for stratified sampling using CCS \cite{zheng2022coverage}}
\label{app:CCS_sampling}
Here we present the algorithm for sampling the training examples to form the coreset based on the coverage-based selection methodology proposed by \cite{zheng2022coverage}.
A crucial component of the algorithm is the cutoff rate $\beta$ which controls how many challenging samples should be removed from consideration when selecting the coreset. 
This is done to eliminate misclassified samples from the dataset since they can hurt the performance of the model trained on coreset, especially at high pruning rates.
Previous works \cite{zheng2022coverage,zheng2024elfs} ablate the values of this cutoff ratio by training the downstream model on a range of values. 
In our work, we simply use the values proposed by the previous works and find that they work well for our score as well. 
The cutoff rates $\beta$ for different pruning rates $\alpha$ are as follows ($\alpha$, $\beta$).
For CIFAR-10: (30\%, 0), (50\%, 0), (70\%, 10\%), (90\%, 30\%), 
for CIFAR-100: (30\%, 10\%), (50\%, 20\%), (70\%, 20\%), (90\%, 50\%), 
for Imagenet: (30\%, 0), (50\%, 10\%), (70\%, 20\%), (90\%, 30\%).
We used CCS for label-free CS as well and the cutoff rates used were
for CIFAR-10: (30\%, 0), (50\%, 0), (70\%, 20\%), (90\%, 40\%), 
for CIFAR-100: (30\%, 0), (50\%, 20\%), (70\%, 40\%), (90\%, 50\%), 
for Imagenet: (30\%, 0), (50\%, 10\%), (70\%, 20\%), (90\%, 30\%).
% \AM{TM, can you fill the numbers for Imagenet?}

\begin{algorithm}[t] 
\caption{Coverage-centric Coreset Selection (CCS) \cite{zheng2022coverage}} 
\label{alg:ccs}
\textbf{Input}: Dataset with difficulty scores: $\mathbb{D} = \{(x,y,s)\}_{i=1}^n$, pruning ratio: $\alpha$, cutoff rate: $\beta$, number of bins: $b$. \\
\textbf{Output}: Coreset: $\mathcal{S}$
\begin{algorithmic}
\STATE{\# Prune hardest examples}
\STATE{$\mathbb{D}' \leftarrow \mathbb{D} \; \symbol{92} \; \{{\floor*{n \times \beta} \; \mathrm{hardest \; examples}}\}$}
\STATE{$A_1, A_2, \cdots, A_b \leftarrow$ Split scores in $\mathbb{D}'$ into $b$ bins.}
\STATE{$\mathcal{B} \leftarrow \{ B_i: B_i\; \mathrm{ consists \; of \; samples \; with \; scores \; in \; } A_i \; for \; i = 1, \cdots, b\}$.}
\STATE{\# Define the size of the coreset}
\STATE{$m \leftarrow n \times \alpha $.}
\STATE{}
\WHILE{$\mathcal{B} \neq \varnothing$}
    \STATE{\# Select the bin with the fewest examples}
    \STATE{$B_{min} \leftarrow \arg \min_{B \in \mathcal{B}} |B|$}.
    \STATE{\# Compute the budgets for this bin}
    \STATE{$m_{B} \leftarrow \min\{|B|, \floor*{\frac{m}{|\mathcal{B}|}}\}$}.
    \STATE{$\mathcal{S}_B \leftarrow $ randomly sample $m_B$ samples from $B_{min}$.}
    \STATE{$\mathcal{C} \leftarrow \mathcal{C} \bigcup \mathcal{S}_B.$}
    \STATE{$\mathcal{B} \leftarrow \mathcal{B} \; \symbol{92} \; \{B_{min}\}.$} 
    \STATE{$m \leftarrow m - m_{B}.$}
\ENDWHILE
\STATE{return $\mathcal{C}$.}
\end{algorithmic}
\end{algorithm}

\input{tables/affectnet}
\input{tables/bloodmnist}

\subsection{Concept-based CS for emotion recognition and biomedical image recognition}
To validate the generalizability of our concept-based coreset selection method beyond image recognition tasks, we apply our concept-based CS approach to the task of emotion recognition and biomedical image recognition. 

For emotion recognition, we use the Affectnet dataset~\cite{mollahosseini2017affectnet} for our experiments. 
AffectNet is a large-scale facial expression dataset designed for training and evaluating affective computing models~\cite{wang2022systematic}. 
It contains facial images collected from the internet using web search queries for emotion-related keywords in multiple languages. 
Each image is manually annotated for eight discrete emotion categories: \texttt{neutral, happiness, sadness, surprise, fear, disgust, anger, contempt}. For our experiments, we utilize an openly available version of this dataset~\footnote{https://www.kaggle.com/datasets/noamsegal/affectnet-training-data}, containing roughly $16000$ training and $14000$ testing samples. 

According to our approach we first use LLaVA to extract concepts for the 8 emotion classes, using the following prompt, \textit{``What are the facial features that distinguish {emotion class name} from other emotion types. Focus on changes in eyes, nose, lips, eyebrows, mouth. Give the output separated by commas in a line.''}. 
We get $5-10$ distinctive facial feature concepts for every emotion, for instance for emotion class \textit{happy}, we get the following concepts, \textit{``wide open eyes'', ``sparking eyes'', ``smiling lips'', ``open mouth'', ``raised eyebrows'', ``flushed cheeks'', ``teeth barred''}. 

To test coreset performance, we use the EfficientNet model~\cite{tan2019efficientnet} and report F1 scores for our coresets in Table~\ref{Table:affectnet-results}. When compared against randomly selected coresets for the various pruning ratios, coresets selected via our concept-based approach achieve better performance. 


For biomedical image recognition, we use the BloodMNIST \cite{acevedo2020dataset} dataset from the MedMNIST \cite{medmnistv1,medmnistv2} which comprises of images of normal blood cells, captured from individuals without infection, hematologic or oncologic disease and free of any pharmacologic treatment at the moment of blood collection. 
It consists of a total of $17,092$ images and is organized into 8 classes (\texttt{basophil,eosinophil,erythroblast,immature granulocytes(myelocytes, metamyelocytes and promyelocytes),lymphocyte,monocyte,neutrophil,platelet}).

For this dataset, we first extract concepts for the 8 blood cell types via GPT using the following prompt, \textit{``What are the features that can distinguish {blood cell class name} from rest of the blood cell types on their size, shape, nucleus appearance, and the presence of granules in their cytoplasm''}.
We obtain 10 concepts for every blood cell type, for instance, for \textit{platelets}, we get the following concepts, \textit{``Smallest blood component'', ``No nucleus'', ``Granules present'', ``Irregular shape'', ``Cytoplasmic fragments'', ``Variable granule distribution'', ``Oval to round shape'', ``Small dense granules'', ``Lacks chromatin'', ``Compact cytoplasmic body''}. 

To test the coreset performance, we use a ResNet-18 model and report accuracy of our coresets in Table~\ref{Table:bloodmnist-results}. 
Similar to other results, our method achieves better performance than randomly selected coresets for higher pruning rates and is competitive at lower pruning ratios.
This is attributes to the difficulty of calculating concept similarity is the representation space of the CLIP model which is potentially unaware of the terminology used in the medical domain. While replacing CLIP with a VLM that is trained on medical domain can boost the performance of our method, our results highlight that even with access to such a model our approach is able to find better coresets than random subsets. 

Our results on these two tasks highlight the versatility of our approach for model-agnostic approach to coreset selection, which is able to find coreset without requiring training the the downstream models on the entire dataset even once.


\subsection{Experimental details}
For generating the importance score we pre-compute the concept similarity scores for the entire dataset and then train the concept-bottleneck layer (in block 2 of Fig.~\ref{fig:overview}) for $100$ epochs across all experiments. 
This training only requires 800 seconds for Imagenet which is significantly more efficient than training the ResNet-34 model on Imagenet (requires roughly 8 hours without FFCV and about 4 hours with FFCV on two A-100 GPUs).

After the coresets are selected, we use the setting and code from \cite{zheng2022coverage} for training a ResNet-18 model for $40000$ iterations with a batch size of $256$ on the coresets for all pruning ratios for CIFAR-10/CIFAR-100.
For Imagenet, we train ResNet-18, ResNet-34, and ResNet-50 models for $100$ epochs on the coresets identified by our method using the training code based on FFCV \cite{leclerc2023ffcv}.

The performance of the label-free CS is dependent on the quality of the pseudo-labels. 
Compared to the clustering-based approach used by ELFS \cite{zheng2024elfs}, our approach of using the zero-shot classification ability of CLIP models yields significantly better pseudo-label quality along with being simpler and more efficient to compute. 
Specifically, for CIFAR-10/100, pseudo-labels of the training set are computed using the CLIP L-14 model trained on the DataComp-1B dataset \cite{ilharco_gabriel_2021_5143773} yields an accuracy of $98.52$\% and $87.28$\% whereas for Imagenet it achieves an accuracy of $79.47$\%
% 61.7\% \AM{72.89, 79.47} 
which are better than the best pseudo-label accuracy obtained by the clustering approach in ELFS ($92.5$\% and $66.3$\% on CIFAR-10/100 and $58.8$\% on Imagenet).

\begin{figure*}
\small
\centering
\subfigure[Easy images from CIFAR-10]
{
\includegraphics[width=0.45\columnwidth]{imgs/cifar10_easy_images_full.pdf}
}
\hfill
\subfigure[Challenging images from CIFAR-10]
{
\includegraphics[width=0.45\columnwidth]{imgs/cifar10_difficult_images_full.pdf}
}
\subfigure[Easy images from CIFAR-100]
{
\includegraphics[width=0.45\columnwidth]{imgs/cifar100_easy_full.pdf}
}
\hfill
\subfigure[Challenging images from CIFAR-100]
{
\includegraphics[width=0.45\columnwidth]{imgs/cifar100_difficult_full.pdf}
}
\caption{Class-wise easy and challenging images for the 10 classes (\texttt{airplane, car, bird, cat, deer, dog, frog, horse, ship, truck}) in CIFAR-10 and for a subset of 10 classes (\texttt{boy, bridge, camel, cloud, crab, kangaroo, lamp, rose, tiger, train}) from CIFAR-100. Similar to the results in Fig.~\ref{fig:easy_difficult_examples}, easy images (a,c) are more canonical images associated with the class labels whereas challenging images (b,d) are images that are confused between two or more classes in the dataset.}
\label{fig:easy_difficult_examples_all}
\end{figure*}


 %misratios
% write_stratified_dataset () {

    % if [ $5 == 0.1 ]
    % then 
    %     misratio=0.3
    % elif [ $5 == 0.3 ]
    % then
    %     misratio=0.2
    % elif [ $5 == 0.5 ]
    % then
    %     misratio=0.1
    % else
    %     misratio=0.
    % fi
% }