\section{Experiments}
\label{sec:experiments}
Here, we compare the performance of the models trained on coresets generated by our concept-based score to models trained on coresets selected by various SOTA CS methods for the standard and label-free CS problems. 

% \begin{table*}[tb]
%   \caption{Comparison of the model's test accuracy after training on coresets, found in a {\bf label-free} manner, shows that our approach leads to coresets with better performance than random subsets and those found by Prototypicality and is only slightly worse than the coresets found by ELFS which is dependent on the training dynamics of the model.
%   }
%   \label{Table:label_free_coreset_performance}
%   \centering
%   \small
%   \resizebox{\textwidth}{!}{
%     \begin{tabular}{c|c|cccc|cccc|cccc|}
%     \toprule
%    \multirow{3}[4]{*}{Method} & \multirow{3}[4]{*}{\makecell{Need \\ Training \\ Dynamics?}} & \multicolumn{12}{c|}{\makecell{Datasets and Pruning Rates}} \\ 
%    \cmidrule(lr){3-14}
%    & & \multicolumn{4}{c|}{\makecell{CIFAR-10}} & \multicolumn{4}{c|}{\makecell{CIFAR-100}} & \multicolumn{4}{c|}{\makecell{Imagenet}} \\
%     \cmidrule(lr){3-6} \cmidrule(lr){7-10} \cmidrule(lr){11-14}
%     & & 30\% & 50\% & 70\% & 90\% & 30\% & 50\% & 70\% & 90\% & 30\% & 50\% & 70\% & 90\% \\
   
%     \midrule
%     Random & No & 94.33 & 93.4 & 90.94 & 79.08 & 74.59 & 71.07 & 65.30 & 44.76 & 72.18 & 70.34 & 66.67 & 52.34 \\
%     Random$_{\mathrm{FFCV}}$ & No & - & - & - & - & - & - & - &- & 73.37 & 71.71 & 67.85 & 51.29 \\
%     \midrule
%     Prototypicality  & No & 94.70 & 92.90 & 90.10 & 70.90 & 74.50 & 69.80 & 61.10 & 32.10 & 70.90 & 60.80 & 54.60 & 30.60 \\
%     ELFS (SwAV)  & Yes & 95.00 & 94.30 & 91.80 & 82.50 & 76.10 & 72.10 & 65.50 & 49.80 & 73.20 & 71.40 & 66.80 & 53.40 \\
%     ELFS (DINO)  & Yes & 95.50 & 95.20 & 93.20 & 87.30 & 76.80 & 73.60 & 68.40 & 54.90 & 73.50 & 71.80 & 67.20 & 54.90 \\
%     \midrule
%     Ours & No & 94.76 & 93.47 & 91.60 & 84.18 & 74.75 & 71.32 & 65.52 & 49.27 & & & &  \\
%     \bottomrule
%     \end{tabular}
%     }
% \end{table*}

% \begin{table}[tb]
%   \caption{Transferability of coreset
%   }
%   \label{Table:transferability_of_coresets}
%   \centering
%   \small
%   \resizebox{\columnwidth}{!}{
%     \begin{tabular}{c|c|cccc|}
%     \toprule
%     \multirow{2}[1]{*}{Arch.} & \multirow{2}[1]{*}{Method} &  \multicolumn{4}{c|}{\makecell{Pruning Rates}}\\ 
%     & & 30\% & 50\% & 70\% & 90\% \\
%     \midrule
%     \multirow{3}[2]{*}{RN-18} & Random & 71.15$_{0.23}$ & 68.48$_{0.10}$ & 63.15$_{0.19}$ & 44.96$_{0.50}$  \\
%     \cmidrule(lr){2-6}
%     & Ours & 70.94$_{0.19}$ & 69.30$_{0.08}$ & 65.16$_{0.04}$ & 49.57$_{0.16}$  \\
%     & Ours (LF) & & & & \\
%     \midrule
%     \multirow{3}[2]{*}{RN-50}&  Random  & 76.06$_{0.11}$ & 74.44$_{0.04}$ & 70.50$_{0.02}$ & 53.56$_{0.13}$  \\
%     \cmidrule(lr){2-6}
%     & Ours & 76.29$_{0.10}$ & 75.12$_{0.09}$ & 72.05$_{0.20}$ & 58.26$_{0.46}$  \\
%     & Ours (LF) & & & & \\
%     \bottomrule
%     \end{tabular}
%     }
% \end{table}

\input{tables/coreset-performance-tm}
\input{tables/labelfree-coreset-performance-tm}

{\bf Datasets, models, and training:}
We focus on CS for classification tasks on three benchmark datasets. 
Specifically, we use CIFAR-10, CIFAR-100 \cite{krizhevsky2009learning}, and Imagenet-1K \cite{deng2009imagenet} datasets consisting of $50000$, $50000$, and $1.28$ million samples spread across $10$, $100$, and $1000$ classes, respectively. 
For CIFAR-10/CIFAR-100, we train a ResNet-18 model and for Imagenet we train ResNet-18, ResNet-34, and ResNet-50 models on the coresets for all pruning ratios.
% For Imagenet, we consider training  for $100$ epochs on the coresets identified by our method. 
To accelerate training on Imagenet, we utilize the training code based on FFCV \cite{leclerc2023ffcv}.
We run CS for three trials with different random seeds for all experiments and report the average of these runs in our tables.
We report the results for different pruning rates where a pruning rate of $90$\% refers to removing $90$\% of the samples from the original training dataset.

For generating the concept annotation we use a recently proposed open source model LLaVA~\cite{liu2023llava,liu2023improvedllava}. Extracting the concepts for each class using this model takes a mere 3 seconds per prompt. In Sec.~\ref{sec:ablation}, we present an ablation study using concepts extracted from different models/methods. 
For computing the concept similarity scores between the visual and concept bottleneck features we used the CLIP \cite{radford2021learning} model following the previous works \cite{yun2022vision,yang2023language,yan2023learning} which showed its effectiveness for this task.  Specifically, we used the ViT B-32 CLIP model \cite{radford2021learning}. 
For computing the pseudo-labels in Sec.~\ref{sec:label_free_CS} we used a ViT L-14 CLIP model trained on the DataComp-1B dataset \cite{ilharco_gabriel_2021_5143773}.
The accuracies of the models trained on the entire training set are $95.44$\% and $78.74$\% for ResNet(RN)-18 on CIFAR-10/100 and $72.4$\% for RN-18, $75$\% for RN-34, and $78.4$\% for RN-50 on Imagenet. Further experimental details are mentioned in App.~\ref{app:implementation_details}. % \AM{Fill these}.

% \begin{table}[tb]
%   \caption{Performance of models with different architecture trained on the selected coreset for Imagenet at different pruning rates. We show that results of standard CS (Ours) and label-free CS (Ours-LF) perform better than the models trained on a random subset of data. 
%   }
%   \label{Table:transferability_of_coresets}
%   \centering
%   \small
%   \resizebox{\columnwidth}{!}{
%     \begin{tabular}{c|c|cccc|}
%     \toprule
%     \multirow{2}[1]{*}{Arch.} & \multirow{2}[1]{*}{Method} &  \multicolumn{4}{c|}{\makecell{Pruning Rates}}\\ 
%     & & 30\% & 50\% & 70\% & 90\% \\
%     \midrule
%     \multirow{3}[2]{*}{RN-18} & Random & 71.15 & 68.48 & 63.15 & 44.96 \\
%     \cmidrule(lr){2-6}
%     & Ours & 70.94 & 69.30 & 65.16 & 49.57  \\
%     & Ours (LF) & & & & \\
%     \midrule
%     \multirow{3}[2]{*}{RN-34}&  Random  & 73.37 & 71.71 & 67.85 & 51.29  \\
%     \cmidrule(lr){2-6}
%     & Ours & 73.39 & 72.34 & 69.44 & 55.92  \\
%     & Ours-LF & & & & \\
%     \midrule
%     \multirow{3}[2]{*}{RN-50}&  Random  & 76.06 & 74.44 & 70.50 & 53.56  \\
%     \cmidrule(lr){2-6}
%     & Ours & 76.29 & 75.12 & 72.05 & 58.26  \\
%     & Ours-LF & & & & \\
%     \bottomrule
%     \end{tabular}
%     }
% \end{table}



{\bf CS baselines and methods:}
We compare our method against various baselines and SOTA methods proposed by previous works. 
1) {\bf Random:} Uniformly select samples from the datasets to form the coreset. Random$_{\mathrm{FFCV}}$ denotes the performance of the models trained on random subsets of Imagenet using the training code based on FFCV \cite{leclerc2023ffcv}.
2) {\bf Entropy \cite{coleman2019selection}:} Selects samples based on entropy which is computed as the uncertainty in the model's prediction on a sample.
3) {\bf Forgetting Score \cite{toneva2018empirical}:} Selects samples based on the forgetting score which is computed as the number of times an example is misclassified after being correctly classified earlier during training of the downstream model. A higher forgetting score indicates a more challenging sample.
4) {\bf AUM \cite{pleiss2020identifying}:} Selects samples based on their average margin during training of the downstream model i.e., the difference between the target class and the next highest class across the training epochs. Lower AUM indicates a more challenging sample. 
For the forgetting score, AUM, and our method, we use CCS \cite{zheng2022coverage} for sampling to form the coreset whereas for entropy we select the samples with the highest entropy in the coreset as done in previous works \cite{coleman2019selection,zheng2022coverage}.

For label-free CS, we use 
1) {\bf Prototypicality \cite{sorscher2022beyond}:} which first performs k-means clustering in the embedding space of SwAV \cite{caron2020unsupervised} model and ranks samples based on their Euclidean distance to the cluster centers. 
Samples further away from the cluster center are then used to form the coreset. 
2) {\bf ELFS \cite{zheng2024elfs}:} estimates the pseudo-labels of the unlabeled samples using a deep clustering approach (using the embedding space of SwAV \cite{caron2020unsupervised} and DINO \cite{caron2021emerging}) and forms the coreset using the training dynamics of the downstream model trained on the pseudo-labeled data. 

Crucially, SOTA methods such as forgetting score, AUM, and ELFS require training the downstream model on the entire dataset (with true/pseudo labels) first, for CS, unlike our method which is independent of the downstream model. 
While the Random and the Prototypicality don't require the downstream model for CS, we show in the following sections that our results are significantly better than these.
% We report the results for these methods from


\subsection{Evaluating performance on CS}
Table~\ref{Table:coreset_performance} shows the accuracy of models trained on coresets found by various approaches on the test sets of the three datasets for the standard CS problem (where the dataset is labeled). 
We find that coresets found by our approach lead to significantly better performance, even at higher pruning rates, compared to the random subsets.
Moreover, our method which does not use any information about the training dynamics of the downstream models provides competitive performance to coresets found by the SOTA approaches based on forgetting score and AUM, and even outperforms them on Imagenet for smaller pruning rates. 
% Since our approach requires roughly \AM{fill number} minutes compared to \AM{four} hours to generate the coreset via Forgetting Score/AUM on Imagenet, it is able to identify a well performing coreset much more efficiently.

% \begin{figure*}
% \small
% \centering
% \subfigure[easy cifar10]
% {
% \includegraphics[width=0.98\columnwidth]{imgs/cifar10_easy_images.pdf}
% }
% \hfill
% \subfigure[easy cifar100]
% {
% \includegraphics[width=0.98\columnwidth]{imgs/cifar100_easy.pdf}
% }
% \subfigure[challenging cifar10]
% {
% \includegraphics[width=0.98\columnwidth]{imgs/cifar10_difficult_images.pdf}
% }
% \hfill
% \subfigure[challenging cifar100]
% {
% \includegraphics[width=0.98\columnwidth]{imgs/cifar100_difficult.pdf}
% }
% \caption{c}
% \label{fig:easy_difficult_examples}
% \end{figure*}



% \begin{figure*}[t]
%   \centering{\includegraphics[width=\textwidth]{imgs/vizanalysis-tm.pdf}
%   \caption{}
%   \label{fig:easy_difficult_examples}
%   }
% \end{figure*}

% Mention computational advantage to justify efficiency via a table.

\subsection{Evaluating performance on label-free CS}
Table~\ref{Table:label_free_coreset_performance} shows the accuracy of models trained on coresets when the training set lacks labels 
(we report the numbers presented by \cite{zheng2024elfs} for all the methods and baselines). 
The results show that the random subsets are a competitive baseline and even it outperforms Prototypicality \cite{sorscher2022beyond}.
Our results also show that our coresets outperform the random subsets for all pruning rates with the improvements being the most significant at higher pruning rates.
Compared to ELFS \cite{zheng2024elfs}, our method provides competitive performance and even surpasses it for lower pruning rates on Imagenet, without using any information about the downstream model's architecture or its training dynamics. % trained on the entire dataset.  
% This highlights the strength of our approach which can be used as an efficient way of estimating the coreset in the label-free scenario, without many modifications. 
% Thus, unlike ELFS \cite{zheng2024elfs} 
Thus, our method is effective for this problem with a simple modification of utilizing the zero-shot predictions from CLIP as pseudo-labels. %This highlights the effectiveness of our approach for CS. % with CCS is both simple and effective for this task. 
% For CIFAR-10/100, pseudo-labels of the training set computed via our approach achieves an accuracy of $98.52$\% and $87.28$\% where as for Imagenet it achieves an accuracy of $79.47$\%
% % 61.7\% \AM{72.89, 79.47} 
% which is better than the best pseudo-label accuracy obtained by the clustering approach in ELFS ($92.5$\% and $66.3$\% on CIFAR-10/100 and $58.8$\% on Imagenet). 
% Moreover, as shown by ELFS, a better stratified sampling approach than CCS (used throughout the paper) can further improve our performance on this problem. 



% \begin{table}[tb]
%   \caption{Comparison of the coreset performance on CIFAR-100 with 90\% pruning rate for concept sets generated using different techniques. 
%   }
%   \label{Table:ablation_concept_extraction}
%   \centering
%   \small
%   \resizebox{\columnwidth}{!}{
%     \begin{tabular}{c|c|cccc|}
%     \toprule
%    \makecell{Random} & \makecell{LF?} & \makecell{CW-A} & \makecell{CW-D} & \makecell{1-S}  & \makecell{IL} \\ 
%    \midrule
%     \multirow{2}[1]{*}{44.76} & No &  51.58 &  51.05 & 51.68 & 52.47 \\
%     & Yes & 49.27 & 49.33 & 49.40 & 49.33 \\
%     \bottomrule
%     \end{tabular}
%     }
% \end{table}

\begin{figure*}[t]
  \centering{\includegraphics[width=0.95\textwidth]{imgs/concept-viz-tm.pdf}
  \vspace{-0.35cm}
  \caption{\small{Visualizing samples according to our concept-based score for a subset of classes in CIFAR-10/100 showing that easy (challenging) samples are aligned (unaligned) with their assigned label.
  Image-level concepts (in boxes) extracted via LLaVA confirm that easy (challenging) examples are aligned (unaligned) with concepts of their labels, explaining the reason for a high (low) concept-based score.}}
  \label{fig:easy_difficult_examples}
  }
\end{figure*}

\subsection{Evaluating the transferability and efficiency of CS}
Here we evaluate the performance of training downstream models with three different architectures to highlight the effectiveness of our approach for model-agnostic CS.
% first show that our coresets are transferable and lead to accuracy better than training on random subsets for various architectures of the model of interest. 
Table~\ref{Table:transferability_of_coresets} shows superior performance than random for ResNet-18, ResNet-34, and ResNet-50 models trained on our coresets for standard and label-free CS for various pruning rates. 

% and show 
% This highlights the effectiveness of our concept-based score at producing high performing coresets regardless of the architecture or training dynamics of the model of interest. 

Next, we compare the efficiency of our approach at finding coresets compared to approaches relying on training the downstream model on the entire dataset to obtain the training dynamics-based score. 
Using two A-100 GPUs, our approach can find the coreset in approximately $30$ minutes for the Imagenet dataset giving a $15$x speed up over training dynamics-based approaches. 
We obtain a similar speed up for CIFAR-10/100 where our method finds the coreset in less than $2$ minutes.
% our approach takes 2hr 12 mins to get concept similarity + 13 mins for LF. First step can be done parallel (16.5 min in 8 jobs). 16.5 +13 ~ 30 mins. CCS takes 8 hrs to train the imagenet model. 8*60/30 = 16
Moreover, since our method is agnostic to the architecture of the downstream model we do not need to repeat the CS step for different architectures, unlike other methods which need to train the downstream model with new model architecture to find the best coreset for it.
% \AM{Add time of prompt selection in our approach.}

\input{tables/transferability-coreset-tm.tex}



\subsection{Visualizing samples with their concept-based scores}
\label{sec:visualization}
Here we show the advantage of our concept-based score for assessing the sample's difficulty and how using concepts aids its interpretability. %of our difficulty score. 
We start by visualizing the easiest and the most challenging images (per class) for CIFAR-10/100.
In Fig.~\ref{fig:easy_difficult_examples}, the top row shows the images with the highest concept-based scores (easiest) and the bottom shows the images with the lowest scores (challenging) for a subset of classes in CIFAR-10/100. 
As observed the easiest images are typical images associated with the label where as the challenging images are confusing (and even potentially mislabeled) as they look like images from a different class. % in the same dataset. 
For example, some challenging images in the class ``boy'' from CIFAR-100 are actually images of a baby which is also a class in CIFAR-100. 
Similarly, some challenging images from the class ``cat'' in CIFAR-10 look like images of a dog. 
More examples of such images are presented in Fig.~\ref{fig:easy_difficult_examples_all} in App.~\ref{app:visualization}.
Since the challenging examples are confusing for humans too, {\bf any} ML model will find them hard to learn as well. 
This shows that our score is well aligned with the human-perceived difficulty of the samples. % and can estimate this difficulty without requiring any information from the training dynamics of the model of interest. 
 
Next, we demonstrate why certain samples get low/high concept-based scores in our approach by extracting attributes specific to these images using LLaVA (note that these attributes are different from the per-class concepts used in the concept bottleneck). 
To generate these, we prompt LLavA to produce concepts using both the sample image and its class label (see image-level concept extraction in App.~\ref{app:concept_extraction}).
These image-level attributes are shown in the boxes in Fig.~\ref{fig:easy_difficult_examples}.
% of the ass the images along with a text prompt to the LLaVA model and evaluate how well the concepts provided by LLaVA describe the label assigned to the image in the dataset.
As observed in Fig.~\ref{fig:easy_difficult_examples}, image-level attributes provided by LLaVA are related to the class label for easy images whereas they are unrelated for challenging images. 
%This implies that our concept-based score in Eq.~\ref{eq:aum_true_label}, can correctly identify when visual features are aligned (unaligned) with the class labels.
% A high concept-based score as per Eq.~\ref{eq:aum_true_label}, suggests that the visual features of a sample are well aligned with the features of the concepts of the associated class in the CLIP space.
% Since our score in Eq.~\ref{eq:aum_true_label}, is based on similarity between visual features of a sample and those of the  concept sets as per the CLIP model, intuitively, a sample with high score indicates that the LLaVA model describes the image to be similar to the concepts of the sample's category. 
% We indeed find this to be the case as shown in Fig.~\ref{fig:easy_difficult_examples} which shows that for easy images the concepts provided by LLaVA are well aligned with the associated category where as for challenging images there is a mismatch. 
For example, attributes provided by LLaVA for the challenging images of ``airplane'' align more with those of a ship (both of which are classes in CIFAR-10), and concepts provided for challenging images of ``bridges'' align more with those of castles (both of which are classes in CIFAR-100). 
Since our score also assigns a small value for these images, our concept-based score in Eq.~\ref{eq:aum_true_label} can correctly capture when the visual information in the sample is not aligned with the associated label of the sample and vice-versa. 
Thus, explaining why certain examples should be included/excluded from the coreset in a human-understandable way and at a dataset level independent of the downstream model.
% Moreover, our score can aid in filtering of the datasets by identifying mislabeled/confusing samples which can either be removed or flagged for human evaluation.
% Thus, our score which captures the similarity between the visual embeddings of an images and the concept bottleneck embeddings 

% \AM{It is a cheap way to identify mislabeled examples. We can correct it before model training. poisoning example filtering. Sample is challenging for any ML model}

\subsection{Ablation studies}
\label{sec:ablation}
Here, we present an ablation to study the effect of different methods for concept generation and the effect of keeping different numbers of concepts per class ($k$) in the bottleneck.  
% attribute vs description-level concepts, and finally a comparison between per class and per-image concepts.  



{\bf Comparison of different techniques to generate concepts via LLaVA.}
% {\bf Comparison of attribute-level vs. description based concepts} 
% techniques to generate concepts via LLaVA.}
Table~\ref{Table:ablation_concept_extraction} shows how the performance of models trained on our coresets change when different methodologies are used to generate the concept sets. 
Since our method uses LLaVA \cite{liu2023llava}, which is a vision language model, we compare the performance of models trained on the random subsets and coresets obtained using class-wise concepts (only textual information) and concepts extracted using both visual and textual information. 
For concepts generated using only textual information, we consider two alternatives, namely {\bf class-wise attributes} (CW-A) and {\bf class-wise descriptions} (CW-D). 
While CW-A considers concepts formed by a single or a few words, CW-D consists of longer, more descriptive concepts (eg., a descriptive concept for the class butterfly is ``\emph{a beautiful insect with colorful wings}").
For CW-D, we use a subset of $k$ concepts provided by \cite{yang2023language}, generated via the GPT-3 model. 
Our results show that CW-A performs better than CW-D for both the standard and label-free CS problems. 
% However, generating concepts via GPT-3 model is more expensive than using an open source model. 
% However, attribute-level concepts allow for selecting discriminative concepts which lead to better performance compared to random concepts (see Table~\ref{Table:ablation_k}).
Thus, we use attribute-level concepts in our work.
% Thus, we Since longer descriptions were not significantly better in our approach, we consider using attribute level concepts everywhere.
%Hence, considering the simplicity and efficiency of extracting attribute-level for every class, we used concepts sets based only on the textual attributes of classes in our paper.  
\input{tables/ablation-concept-extraction.tex}

Next, for generating concepts using both visual and textual information, we consider two alternatives.
The first is a {\bf class-wise one-shot image attribute} approach where we first cluster all images of a class in the embedding space of the CLIP's visual encoder and identify the image whose embedding is the closest to the cluster center (for the label-free setting we use the pseudo-labels of the images during clustering 
%\AM{we have not done this. Need to do it before rebuttal}
), then we prompt LLaVA to generate attributes using this single image and the class name. 
Once generated we use $k$ discriminative concepts to form the bottleneck.
The second is {\bf image-wise} attribute approach, where we use each image in the training set and prompt LLaVA to generate per image attributes describing the image. 
Once generated we sort the concepts based on their frequency of occurrence in a class and use the most frequently occurring discriminative concepts to form the bottleneck.
While the image-level concepts lead to the best coresets, it is slow and costly to prompt LLaVA to generate attributes for all the images in a large dataset such as Imagenet. 
For CIFAR-100, this process took about nine hours to complete (in comparison CW-A can be extracted in 5 minutes for CIFAR-100, without parallel computation) 
%seconds per class)which may be reduced with exploiting parallel computation) 
which is very costly compared to the small performance gains it provides over other approaches. 
% simpler approaches to obtain concepts.
%\AM{We did not test with single image without clustering since it is not possible to do that without labels. Plus the image we used is in the dataset so result should not be different.}
Lastly, while the one-shot approach is better than CW-A in most cases, the additional step of clustering can be costly for larger datasets such as Imagenet. 
Based on these results, we used CW-A for concept generation using LLaVA.
% Lastly, we compared the performance of our concepts sets ({\bf class-wise attributes (CW-A)}) to concepts sets which contained more descriptive text ({\bf class-wise descriptions (CW-D)}) rather than attributes for every class (eg., a descriptive concept for the class butterfly is ``\emph{a beautiful insect with colorful wings}").
% For this, we used a subset of $k$ concepts provided by \cite{yang2023language}, which have been extracted using a GPT-3 model. 
% Our results show that the two class-wise concepts perform very similarly on the CS problem. 
% Hence, considering the simplicity and efficiency of extracting attributes for every class, we used concepts sets based only on the textual attributes of classes in our paper.  
% We compare the performance to random subsets and the class-level concepts extracted as described in Sec.~\ref{sec:concept_extraction}. 

{\bf Effect of $k$.} In Table~\ref{Table:ablation_k}, we show how the number of concepts extracted per class label affects the selection of coresets. 
Once the list of attribute-level concepts is generated by LLaVA, we can select $k$ concepts per class either randomly or choose concepts unique to a class (discriminative). 
Our results show that using even $k$=$1$ is sufficient to surpass the performance using a random subset (Random baseline). 
This performance increases when we keep discriminative concepts in our concept bottleneck, with $k=5$ achieving the best results.
While the size of the concept bottleneck need not be very large to find good coresets, it is helpful to take a sample's visual similarity with a set of concepts rather than a single concept per class. % (or naively the sample's label). 
Thus, we selected $5$ discriminative concepts per class to form the concept bottleneck. % in our experiments.
% \AM{add citation of concept set size}
\input{tables/ablation-k-tm.tex}





% {\bf Comparison of word vs sentence level concepts}



