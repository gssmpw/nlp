\section{Conclusion}
\label{sec:conclusion}
CS finds representative samples from the training data, training models on which leads to models with accuracy similar to the models trained on the entire dataset. 
In this work, we proposed a score based on concept bottlenecks that allow us to gauge the difficulty of a sample in terms of human-understandable concepts and is independent of the downstream model. % of interest.
%which will eventually be trained on the coreset. 
Our experiments show that training downstream models on coresets selected using our score and a stratified sampling approach leads to better performance than random subsets and achieves accuracy similar to or better than the SOTA approaches based on training dynamics of the downstream model, for both the standard and label-free CS problem. 
Moreover, our score provides an intuitive explanation of a sample's difficulty independent of the downstream model and leads to coresets that achieve high accuracy for various model architectures.
%of the downstream model.

{\bf Limitations:}
In our work, the concept extraction from LLM is treated as a pre-processing step which is independent of the data difficulty score computation and coreset selection. 
However, LLMs can produce very noisy and non-discriminative concepts for various classes (eg., a concept ``blue'' can be associated with several classes in the Imagenet dataset), leading to poor concept-similarity scores. 
Moreover, while class-wise concept extraction is efficient, image-level concepts could be much more informative for a sample's difficulty. 
Thus, improving the efficiency of concept extraction and tuning the prompt for LLMs/VLMs to incorporate the feedback from score computation or CS can help generate better concept bottlenecks leading to a better estimate of a sample's difficulty.
While these are important research directions we leave them for future work.


% before selecting the coreset i focused on extracting concepts from a pre-trained LLM for all classes in our dataset 
% Mention that we could use prompt tuning to incorporate the feedback of coreset performance back into generation of concept bottlenecks. But since LLAVA concepts require post processing it was hard to do it in the current work. 