\begin{table*}[tb]
  \caption{\small{Comparison of the model's (RN-18 for CIFAR10/100 and RN-34 for Imagenet) test accuracy after training on coresets found by various approaches shows that our coresets lead to significantly better performance than Random achieving competitive results compared to the methods using the downstream model's training dynamics, even for high pruning rates. (Best in each category is highlighted).
  }}
  \label{Table:coreset_performance}
  \centering
  \small
  \resizebox{0.98\textwidth}{!}{
    \begin{tabular}{crcccccccccccc}
    \toprule[1.25pt]
   &\multirow{3}[4]{*}{\textbf{Method}} &  \multicolumn{12}{c}{\makecell{\textbf{Datasets and Pruning Rates}}} \\ 
   \cmidrule(lr){3-14}
   &&  \multicolumn{4}{c}{\makecell{\textit{CIFAR-10}}} & \multicolumn{4}{c}{\makecell{\textit{CIFAR-100}}} & \multicolumn{4}{c}{\makecell{\textit{Imagenet}}} \\
    \cmidrule(lr){3-6} \cmidrule(lr){7-10} \cmidrule(lr){11-14}
    & & $30\%$ & $50\%$ & $70\%$ & $90\%$ & $30\%$ & $50\%$ & $70\%$ & $90\%$ & $30\%$ & $50\%$ & $70\%$ & $90\%$ \\
   
    \midrule
    \multirow{3}{*}{\makecell{\textbf{Needs Training} \\ \textbf{Dynamics}}} & Entropy   & $94.44$ & $92.11$ & $85.67$ & $66.52$ & $72.26$ & $63.26$ & $50.49$ & $28.96$ & $72.34$ & $70.76$ & $64.04$ & $39.04$ \\
    & Forgetting & $\mathbf{95.40}$ & $\mathbf{95.04}$ & $92.97$ & $85.70$ & $\mathbf{77.14}$ & $\mathbf{74.45}$ & $\mathbf{68.92}$ & $\mathbf{55.59}$ & $\mathbf{72.60}$ & $\mathbf{70.89}$ & $66.51$ & $52.28$ \\
    & AUM & $95.27$ & $94.93$ & $\mathbf{93.00}$ & $\mathbf{86.08}$ & $76.84$ & $73.77$ & $68.85$ & $55.03$ & $72.29$ & $70.52$ & $\mathbf{67.78}$ & $\mathbf{57.36}$ \\
    \midrule
    \multirow{3}[2]{*}{\makecell{\textbf{Doesn't Need} \\ \textbf{Training} \\ \textbf{Dynamics}}} & Random & $94.33$ & $93.40$ & $90.94$ & $79.08$ & $74.59$ & $71.07$ & $65.30$ & $44.76$ & $72.18$ & $70.34$ & $66.67$ & $52.34$ \\
    
    &Random$_{\mathrm{FFCV}}$ & - & - & - & - & - & - & - &- & \makecell[tl]{$ 73.37$\\ \tiny{\textcolor{denim}{$\pm 0.08$}}} & \makecell[tl]{$ 71.71$\\ \tiny{\textcolor{denim}{$\pm 0.10$}}} & \makecell[tl]{$ 67.85$\\ \tiny{\textcolor{denim}{$\pm 0.04$}}} & \makecell[tl]{$51.29$\\ \tiny{\textcolor{denim}{$\pm 0.20$}}} \\
    \cmidrule(ll){2-14}
% &\textbf{Ours}  & $\mathbf{94.77}$ & $\mathbf{93.44}$ & $\mathbf{91.80}$ & $\mathbf{84.63}$ & $\mathbf{75.98}$ & $\mathbf{72.22}$ & $\mathbf{66.53}$ & $\mathbf{51.85}$ & $\mathbf{73.39}$ & $\mathbf{72.34}$ & $\mathbf{69.44}$ & $\mathbf{55.92}_{\textcolor{blue}{\pm 17.4}}$ \\
&\textbf{Ours}  
& \makecell[tl]{$\mathbf{94.77}$\\ \tiny{\textcolor{denim}{$\pm 0.09$}}} 
& \makecell[tl]{$\mathbf{93.44}$\\ \tiny{\textcolor{denim}{$\pm 0.61$}}} 
& \makecell[tl]{$\mathbf{91.80}$\\ \tiny{\textcolor{denim}{$\pm 0.21$}}} 
& \makecell[tl]{$\mathbf{84.63}$\\ \tiny{\textcolor{denim}{$\pm 0.24$}}} 
& \makecell[tl]{$\mathbf{75.98}$\\ \tiny{\textcolor{denim}{$\pm 0.26$}}} 
& \makecell[tl]{$\mathbf{72.22}$\\ \tiny{\textcolor{denim}{$\pm 0.22$}}} 
& \makecell[tl]{$\mathbf{66.53}$\\ \tiny{\textcolor{denim}{$\pm 0.42$}}} 
& \makecell[tl]{$\mathbf{51.85}$\\ \tiny{\textcolor{denim}{$\pm 0.29$}}} 
& \makecell[tl]{$\mathbf{73.39}$\\ \tiny{\textcolor{denim}{$\pm 0.12$}}} 
& \makecell[tl]{$\mathbf{72.34}$\\ \tiny{\textcolor{denim}{$\pm 0.13$}}} 
& \makecell[tl]{$\mathbf{69.44}$\\ \tiny{\textcolor{denim}{$\pm 0.17$}}} 
& \makecell[tl]{$\mathbf{55.92}$\\ \tiny{\textcolor{denim}{$\pm 0.02$}}}\\
    \bottomrule[1.25pt]
    \end{tabular}
    }
    \vspace{-0.2cm}
\end{table*}