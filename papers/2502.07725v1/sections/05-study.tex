\section{Preliminary User Study}
\label{sec:study}

Using \pluto~as a design probe, we conducted a preliminary user study to assess the utility of the proposed interactive experience for authoring semantically aligned text and charts for data-driven communication.
Specifically, we focused on two goals: 1) understand if and how the system suggestions aid the joint authoring of text and charts, and 2) gather feedback on \pluto's current recommendations and features.

\subsection{Participants and Setup}

We recruited ten participants ($P1$-$P10$) through mailing lists at a data analytics software company.
\new{The recruitment call sought for individuals who use a combination of charts and text for data-driven communication.
Participation in the study was voluntary and} participants were recruited on a first-come, first-serve basis.
Seven participants rated themselves as visualization experts, and three participants had a moderate level of expertise in visualization. 
Regarding participants' professional backgrounds, six participants were solution architects, two participants worked as visualization consultants, and two were data analysts.
Of the ten participants, six participants reported that they used a combination of text and charts to communicate data once in a few weeks, two frequently used text and charts as part of their jobs, and two participants had only recently started using charts and text for communication as part of their new roles.
This mix of participant backgrounds w.r.t. chart and text authoring ensured that the study captured holistic feedback on \pluto~from varying users, including novices, moderate-level users, and experts.

Speaking about their existing authoring experience, all participants noted they manually inspected the chart and wrote a corresponding text blurb while sharing.
Five participants said they sometimes annotated a chart's screenshot with text during communication.
The two participants who frequently communicated using text and charts also noted using the dashboard authoring features in tools like Tableau.

All sessions were conducted remotely via the Zoom video conferencing software. The prototype was hosted on a local server on the experimenter's laptop. Participants were granted control over the experimenter's screen during the session, and all studies followed a think-aloud protocol. The audio, video, and on-screen actions were recorded for all sessions with permission from the participants.

\subsection{Procedure}

We initially considered an evaluation of \pluto~against an existing chart-and-text authoring tool. However, we did not find a freely available baseline that provided equivalent features to \pluto~in terms of supporting multimodal authoring, bidirectional editing of text and charts, and using the semantic structure of descriptions~\cite{lundgard2021accessible} during text recommendation.
We also considered an ablation study to compare the system's output to that from the underlying GPT-4 model.
However, we did not see value in this approach as \pluto's utility stems from the integration of multimodal interactions and recommendations, and not one standalone feature focused on text generation.

We ultimately decided on a qualitative study where all participants interact with \pluto~and perform the same set of tasks as this would allow us to observe usage patterns and assess the utility of the recommendations.
Sessions lasted between 44-57 minutes ($\mu$: 53 min., \new{$\sigma$: 4 min.}) and were organized as follows:

\vspace{.5em}
\noindent\textbf{Introduction} [$\sim$10min]:
Participants were given an overview of the study and asked to share their background information. This briefing was followed by an introduction to \pluto's interface and key features. We used a simple bar chart about US college costs as a running example for the introduction.

\vspace{.5em}
\noindent\textbf{Tasks} [$\sim$30min]:
Participants were presented with three charts and were asked to write complementary text (including title, description, and annotations) for the chart so they could share their findings with others who may be interested in the data.
To ensure the tasks were realistic and of appropriate difficulty, we consulted the two experts from the formative study (Section~\ref{sec:design-goals}) and conducted three pilot studies with participants from academic and journalism backgrounds.

The three charts included a multi-series line chart about movie earnings by genre, a stacked bar chart about costs incurred by airplane bird strikes in the US, and a grouped bar chart about US housing prices shown in Figures~\ref{fig:teaser}A, \ref{fig:scenario-2}A, and \ref{fig:interface}D, respectively.
\new{We selected these chart types based on their frequency of use in prior chart-and-text authoring systems (e.g.,~\cite{sultanum2023datatales,chen2022crossdata,choi2022intentable,kim2023emphasischecker,lin2023inksight}) as well as the general prevalence of bar and line charts across visualization platforms (e.g.,~\cite{battle2018beagle,purich2023toward}) that are commonly leveraged for data-driven communication.}
The order of charts was randomized across participants. 
We asked the participants to use the system as they saw fit (e.g., start with auto-generated text, manually write text and use the suggestions to edit, or manually compose the chart and text without using system recommendations).

\vspace{.5em}
\noindent\textbf{Debrief} [$\sim$10min]:
Sessions concluded with a semi-structured interview discussing the overall experience, support for different authoring tasks, and areas for improvement. Participants also filled out a questionnaire rating the quality and utility \pluto's recommendations and features.

\subsection{Results}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/pdf/results-wo-means.pdf}
    \caption{Participant responses to post-session questions about \pluto's recommendations.}
    \Description[Participant responses to post-session questionnaire]{Overall, participants were generally positive about Pluto's recommendations, particularly commending the use of selection to guide suggestions and the recommendations for updating the chart design based on the text. Detailed responses are as follows. Question: Overall, how likely are you to use such a system in practice for authoring text and charts for data-driven communication? Responses: 4 participants said "very likely", 4 said "likely", 1 said "moderately likely" and 1 said "unlikely".
    Question: How helpful were the text generation suggestions? Responses: 3 participants said "very helpful", 6 said "helpful", and 1 said "unhelpful". Question: How helpful was it to use selections on the chart to guide the text generation features? Responses: 3 participants said "very helpful", 5 said "helpful", and 2 said "moderately helpful". Question: How helpful were the system recommendations for editing the description? Responses: 1 participant said "very helpful", 2 said "helpful", 6 said "moderately helpful", and 1 said "unhelpful". Question: How helpful were the system recommendations for updating the chart? Responses: 4 participants said "very helpful", 4 said "helpful", and 2 said "moderately helpful".}
    \label{fig:responses}
\end{figure}

Overall, participants were receptive to \pluto's features and recommendations, noting they would use such a system in practice for data-driven communication (Figure~\ref{fig:responses}). We summarize general themes from participant feedback.


\vspace{.5em}
\noindent\textbf{Full-text recommendations are helpful for bootstrapping.}
Participants generally found the text generation recommendations useful, with nine participants rating them as `helpful' or `very helpful' (Figure~\ref{fig:responses}).
Participants noted that these suggestions were particularly useful for bootstrapping the authoring process. For instance, commenting on the generated description, $P1$ said, ``\textit{I would love to use these as a first draft. Just helps get my juices flowing.}''
However, participants' feedback on the quality of generated descriptions was mixed, with some participants (P2, P4, P7) finding the suggested text too verbose.
Both $P4$ and $P7$, for instance, noted that they would prefer the system generate a bullet list of key ``insights," allowing them to use the insights to manually craft a narrative.
$P2$ and $P4$ (both frequent users of text and charts for communication) also stated they would like more control over the generated text in terms of its verbosity and writing style (e.g., configuring the text to have a more ``casual'' versus ``formal'' tone depending on the communication context).

Feedback on the suggested titles was unanimously positive, however.
For example, commending a suggested title for the house pricing chart, $P8$ said, ``\textit{Terms like `Influence of' really makes chart feel less templated and more informative.}''
Upon seeing the title ``Action and Adventure Dominate:...", $P5$ noted, ``\textit{I am very impressed with the title. It's almost like it took all my changes and gave me a summary."}
Participants were also pleasantly surprised by the quality of text completions for individual sentences (e.g., Figure~\ref{fig:scenario-2}A) stating ``\textit{the text completion followed my lead very well}" ($P6$).


\vspace{.5em}
\noindent\textbf{Using selections to guide recommendations saves time and instills confidence.}
Overall, participants appreciated the ability to directly select items on the chart to drive text recommendations, with 8/10 participants noting this feature was `helpful' or `very helpful' (Figure~\ref{fig:responses}).
The positive feedback for the multimodal text generation feature often stemmed from its ability to assist in faster writing and to adjust the scope of the generated text.

$P8$, for instance, particularly appreciated the ability to use chart selections to drive auto-complete and annotations (e.g., Figures~\ref{fig:teaser}B and \ref{fig:scenario-2}C) and said, ``\textit{It was nice to be able to point at things and have the system give the text. It saved me a lot of time.}"
P2 and P4, who found the description generation feature too verbose, switched to using selection-based text generation, with P4 stating that ``\textit{the selection at least ensured the generated text was about something I want to talk about.}''

\vspace{.5em}
\noindent\textbf{Description editing recommendations are primarily useful for validation.}
Participants' reactions to recommendations for verifying and editing an entered description (e.g., Figures~\ref{fig:scenario-1}A and \ref{fig:interface}G) were more neutral (Figure~\ref{fig:responses}, Q4).

Seven participants explicitly noted that they appreciated that the system flagged text that needed verification.
P6, for instance, said, ``\textit{Humans are lazy. Having that extra step is going to save a lot of people a lot of embarrassment.}''
Noting that the verification suggestions made him more critical, $P3$ said, ``\textit{It's important that we think about what charts are saying...and it's referring back and making sure. It's definitely a step in the right direction.}''
The remaining participants either felt the suggestions should not appear for manually written text ($P4$) or that statements could be flagged more conservatively, reducing the work on the authors' part ($P7$, $P8$).

Participants used recommendations to add or reorder text in the description only three times across all sessions. Participants commented that these recommendations were too ``obvious'' (particularly referring to the \schemaSecondary{encoding} statement suggestions).
$P9$, for instance, ``\textit{This [\schemaSecondary{encoding} statement] suggestion is too generic and not as interesting as the others that tell me key points about the data.}''
However, all participants noted that the recommendations were appropriately placed on the side and did not impede their workflow (\textbf{DG5}).

\vspace{.5em}
\noindent\textbf{Chart annotation and design recommendations foster an integrated reading experience.}
Participants were generally very impressed by \pluto's suggestions to annotate or sort the chart based on an entered description, with 8/10 participants rating this feature as `helpful' or `very helpful.'

For example, even $P7$, who was generally critical about the other features, exclaimed, ``\textit{Loved that just loved that! It's easy to forget the chart when writing because I know what I should be focusing on, but someone else looking at the chart may not.}''
Appreciating the ability to adjust the system-suggested annotations further (\textbf{DG5}), P1 commented, ``\textit{It's great that it highlighted some elements in the chart based on my text. It made me see things from a reader's perspective and go back and make additional changes.}''

\vspace{.5em}
\noindent\textbf{Usage patterns.}
As typical with mixed-initiative interfaces, there was a constant back and forth between the participants' authoring actions and the system recommendations. We observed four high-level usage strategies around how participants started the authoring process by writing descriptions. We summarize these strategies below, as they can help inform the user experience of future systems\footnote{Note that some participants adopted different strategies across tasks, resulting in the participant count across strategies adding up to more than 10}.

\begin{tightItemize}
    \item \textit{Generate then edit.}
    The most common strategy across participants (7/10) was to start with auto-generated descriptions ({\small\faIcon{feather-alt}} \textbf{Generate}).
    Once the description text was generated, participants would either first peruse through it and make edits or directly request suggestions for improvements to the text and the chart ({\small{\faIcon[regular]{lightbulb}}} \textbf{Suggest}).
    
    \vspace{.5em}
    \item \textit{Guide text completion.}
    Four participants started writing their descriptions leveraging the in-place text competition suggestions (via the \key{Tab} key).
    Participants would typically mention a data entity (e.g., New York) or a narrative hook (e.g., ``\textit{however},'' ``\textit{but},'' ``\textit{sadly}'') in their text and have the system suggest the remaining statement that they would use as-is or edit further.
    The {\small{\faIcon[regular]{lightbulb}}} \textbf{Suggest} feature was primarily used to verify the statements and get chart annotation recommendations to complement the text.

    \vspace{.5em}
    \item \textit{Clipboard text generation.}
    On two occasions, participants used the system text suggestions to create a clipboard of ideas.
    Specifically, participants started by selecting visually salient entities on the chart and asking \pluto~to generate a series of text annotations. Subsequently, the participants went through these annotations and either edited and kept them on the chart, moved them to the description, or deleted them.
    The {\small{\faIcon[regular]{lightbulb}}} \textbf{Suggest} feature was used after this initial drafting of the description and text annotations to get further editing suggestions for the chart.

    \vspace{.5em}
    \item \textit{Manual writing with chart design recommendations.}
    Two participants (both experts at communicating data using text and charts) typically started their process by manually drafting the description and then asking the system to {\small{\faIcon[regular]{lightbulb}}} \textbf{Suggest} improvements. These participants also noted that they utilized the suggest feature to obtain chart annotations and design suggestions based on their input text (\textbf{DG1}).
\end{tightItemize}