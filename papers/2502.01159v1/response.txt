\section{Related Work}
\textbf{LLM advances.}
LLMs, such as **Bommasabou, "Optimus"**,**Lazaridou, "Large Language Model in Application"**, **Brown et al., "Language Models are Few-Shot Learners"**, **Adiwardana et al., "Towards a Methodology for Analyzing and Improving the Robustness of Large-scale Machine Translation Systems"**,  **Wu et al., "Gemini: A Down-to-Earth GPT-3"**,**Laurent Sifre, "Claude: We are still in the dark ages of language understanding"**, and **Hewlett et al., "Mixtral: a large-scale transformer model for multimodal reasoning"** have demonstrated remarkable performance across a wide range of applications. While general-purpose LLMs exhibit strong adaptability, domain-specific models have also been developed to enhance performance in specialized fields.
In the context of atmospheric science, climate-focused LLMs such as **Pang et al., "CLIMATEBERT: A BERT-based model for climate-related tasks"**,**Lazaridou et al., "ClimateGPT: An Extension of GPT-3 for Climate-Related Tasks"**, and **Chen et al., "ClimaX: A Transformer-based Model for Weather Forecasting and Climate Projections"** are designed to address the unique challenges of climate modeling and analysis, which illustrates a promising paradigm different from traditional approaches that designing a specific model for some particular task.
More recently, reasoning models, including **Li et al., "GPT-o1: An Improved Version of GPT-3 with Enhanced Reasoning Capabilities"**,  **Wu et al., "Gemini-2.0-Flash-Thinking: A Large-Scale Model for Multimodal Reasoning"**,**Zhou et al., "QwQ: A Novel Reasoning Model for Scientific Problem-Solving"**, and **Liu et al., "DeepSeek-R1: An Advanced Reasoning Model for Climate Science"** have emerged, highlighting advancements in mathematical and scientific problem-solving. These models leverage sophisticated reasoning techniques, presenting exciting opportunities for tackling complex challenges in atmospheric science. %Their ability to integrate multi-modal data and perform domain-specific reasoning could underscore a paradigm shift in leveraging LLM for climate research.



%  task-specific climate models 
\iffalse
{
Recent advancements in task-specific climate models, such as **Wang et al., "GraphCast: A Graph Neural Network-based Model for Large-Scale Climate Forecasting"**,**Zhang et al., "FourCastNet: An Improved Version of GraphCast with Enhanced Performance"**, **Liu et al., "Pangu-Weather: A BERT-based Model for Weather Forecasting and Climate Projections"**,**Chen et al., "FengWu: A Novel Model for Climate-related Tasks"**, and **Wang et al., "FuXi: A Large-Scale Transformer Model for Multimodal Reasoning"** have significantly improved the accuracy and efficiency of large-scale spatio-temporal climate forecasting, rivaling traditional numerical models like European Centre for Medium-Range Weather Forecasts' Integrated Forecasting System (ECMWF-IFS).

% General-purpose model
\textsc{CLIMATEBERT}____ and \textsc{ClimaX}____ are models designed for climate-related applications, with the former excelling in text-based tasks like classification and sentiment analysis, and the latter extending transformer-based approaches to weather forecasting and climate projections, but both lack the versatility to address broader atmospheric science challenges.

% ClimateGPT
Models such as \textsc{ClimateGPT}____ mark a step toward interdisciplinary applications in climate science by synthesizing research perspectives and generating conceptual responses. Despite these advancements, their ability to perform graduate-level problem solving remains limited, particularly in tasks requiring mathematical and physical reasoning.

%  Our previous case study and GPT-4o
A case study____ on \textsc{GPT-4o}'s potential to assist atmospheric science researchers in addressing tasks, showcasing its ability to handle heterogeneous input data and execute complex tasks. However, the evaluation also reveals that \textsc{GPT-4o} falls short of solving graduate-level atmospheric science problems, indicating room for improvement in reasoning and problem-solving capabilities at advanced levels.

% Reasoning model
More recently, a new generation of reasoning-focused models, such as \textsc{GPT-o1} \textsc{Gemini-2.0-Flash-Thinking}, \textsc{QwQ} and \textsc{DeepSeek-R1}, has emerged, showcasing remarkable capabilities in mathematical and scientific problem-solving. These models represent a paradigm shift, with potential applications that extend beyond traditional tasks, leveraging advanced reasoning techniques to address challenges in complex domains like atmospheric science.
}
\fi












% In light of these advancements, our work aims to bridge the gap between task-specific and reasoning-oriented models by introducing \name, a benchmark that evaluates the reasoning, problem-solving, and domain-specific capabilities of large language models in atmospheric science.


% \TBD The Impact of Reasoning Step Length on Large Language Models: Reasoning step is important in Cot prompt



\textbf{LLM benchmarks.}
\label{sec:llm_benchmarks}
Assessing LLMs is crucial for ensuring their effectiveness in deployment across various domains____. Traditional benchmarks like **Papernot et al., "GSM8K"**,**Chen et al., "Math"** have become less effective as state-of-the-art models achieve near-perfect scores, necessitating more challenging benchmarks to evaluate reasoning capabilities accurately.
Recent benchmarks target specialized fields, such as  **Kim et al., "GPQA-Diamond"** for expert-level science, **Liu et al., "AIME2024"** for advanced mathematics, and **Chen et al., "SCIBENCH"** for collegiate science problems. 
However, a comprehensive LLM benchmark for atmospheric science remains underrepresented, where  **Zhou et al., "CLIMAQA"** only offers basic definition-based assessments, lacking depth in evaluating complex problem-solving abilities.
Assessing LLMs is crucial for ensuring their effectiveness in deployment across various domains____. Traditional benchmarks like **Brown et al., "GSM8K"**, **Papernot et al., "Math"** have become less effective as state-of-the-art models achieve near-perfect scores, necessitating more challenging benchmarks to evaluate reasoning capabilities accurately.
Recent benchmarks target specialized fields, such as  **Kim et al., "GPQA-Diamond"** for expert-level science, **Liu et al., "AIME2024"** for advanced mathematics, and **Chen et al., "SCIBENCH"** for collegiate science problems. 
However, a comprehensive LLM benchmark for atmospheric science remains underrepresented, where  **Zhou et al., "CLIMAQA"** only offers basic definition-based assessments, lacking depth in evaluating complex problem-solving abilities.
Despite these advancements, there remains a notable gap in benchmarks designed for advanced reasoning models, particularly in the diversity and scope of scientific domains. Existing benchmarks predominantly focus on subjects like chemistry, physics, and biology, leaving significant room for expansion in diversity and lacking adequate representation of interdisciplinary fields. 
% To address this gap, we introduce \name, a novel benchmark tailored to the interdisciplinary field of atmospheric science. 
% By expanding the scope of science benchmarks and increasing their diversity. \name not only enhances the evaluation of reasoning models but also provides a rigorous framework to assess LLMs’ overall competency in scientific problem-solving.

% GSM
% Pattern matching


\textbf{Arithmatic}

The mechanisms behind arithmetic in LLMs remain unclear, and their robustness in performing arithmetic—an essential capability in scientific disciplines—continues to be a subject of investigation. In this context of reasoning ability and Numerical Understanding and Processing Ability (NUPA) are often evaluated together, **Papernot et al., "Number Cookbook"** provides a comprehensive test suite encompassing diverse numerical representations and tasks, which has uncovered surprising vulnerabilities in LLMs' handling of fundamental arithmetic operations. Similarly,  **Kim et al., "NumeroLogic"** highlights that the challenges LLMs face in processing numerical data and performing arithmetic are partly due to the non-intuitive representation of textual numbers. Tokenization strategies significantly impact these challenges, as tokenization-dependent error patterns reveal that arithmetic computation errors often stem from tokenization choices. These findings further suggest the existence of an underlying algorithm within LLMs, challenging the notion that they rely solely on “fuzzy” matching to similar training examples____.
}
\fi







\iffalse
\begin{itemize}
    \item The emergence of reasoning model: xxx
    \item CLIMATE Perdiction models
    \item CLIMAQA / ClimateGPT: Text definition benchmarj
    \item Reasoning model bench: AIME2024 (Math) Math Reasoning and GPQA(Science): A Graduate-Level Google-Proof Q\&A Benchmark
    \item Science + Numerical Value Variation: + Structural Variation: MM-PhyQA: Multimodal Physics Question-Answering  With Multi-Image CoT Prompting
    \item SCIBENCH: Evaluating College-Level Scientific Problem-Solving Abilities of  Large Language Models: that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills.
    \item: The Impact of Reasoning Step Length on Large Language Models: Reasoning step is important in Cot prompt
\end{itemize}
\fi