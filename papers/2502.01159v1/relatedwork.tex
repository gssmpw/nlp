\section{Related Work}
\textbf{LLM advances.}
LLMs, such as \textsc{OPT}~\cite{zhang2022opt}, \textsc{LLaMA}~\cite{touvron2023llama}, \textsc{GPT}~\cite{gpt4o}, \textsc{Gemini}~\cite{reid2024gemini}, \textsc{Claude}~\cite{claude3}, and \textsc{Mixtral}\cite{jiang2024mixtral}, have demonstrated remarkable performance across a wide range of applications. While general-purpose LLMs exhibit strong adaptability, domain-specific models have also been developed to enhance performance in specialized fields.
In the context of atmospheric science, climate-focused LLMs such as \textsc{CLIMATEBERT}~\cite{webersinke2021climatebert}, \textsc{ClimateGPT}~\cite{thulke2024climategpt}, and \textsc{ClimaX~\cite{nguyen2023climax}} are designed to address the unique challenges of climate modeling and analysis, which illustrates a promising paradigm different from traditional approaches that designing a specific model for some particular task~\cite{lam2022graphcast, pathak2022fourcastnet, bi2022pangu, chen2023fengwu, chen2023fuxi}.
More recently, reasoning models, including \textsc{GPT-o1}~\cite{openai_learning_to_reason_with_llms}, \textsc{Gemini-2.0-Flash-Thinking}~\cite{deepmind_gemini_flash_thinking}, \textsc{QwQ}~\cite{qwq-32b-preview}, and \textsc{DeepSeek-R1}~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}, have emerged, highlighting advancements in mathematical and scientific problem-solving. These models leverage sophisticated reasoning techniques, presenting exciting opportunities for tackling complex challenges in atmospheric science. %Their ability to integrate multi-modal data and perform domain-specific reasoning could underscore a paradigm shift in leveraging LLM for climate research.



%  task-specific climate models 
\iffalse
{
Recent advancements in task-specific climate models, such as \textsc{GraphCast}~\cite{lam2022graphcast}, \textsc{FourCastNet}~\cite{pathak2022fourcastnet}, \textsc{Pangu-Weather}~\cite{bi2022pangu}, \textsc{FengWu}~\cite{chen2023fengwu}, and \textsc{FuXi}~\cite{chen2023fuxi}, have significantly improved the accuracy and efficiency of large-scale spatio-temporal climate forecasting, rivaling traditional numerical models like European Centre for Medium-Range Weather Forecasts' Integrated Forecasting System (ECMWF-IFS).

% General-purpose model
\textsc{CLIMATEBERT}~\cite{webersinke2021climatebert} and \textsc{ClimaX}~\cite{nguyen2023climax} are models designed for climate-related applications, with the former excelling in text-based tasks like classification and sentiment analysis, and the latter extending transformer-based approaches to weather forecasting and climate projections, but both lack the versatility to address broader atmospheric science challenges.

% ClimateGPT
Models such as \textsc{ClimateGPT}~\cite{thulke2024climategpt} mark a step toward interdisciplinary applications in climate science by synthesizing research perspectives and generating conceptual responses. Despite these advancements, their ability to perform graduate-level problem solving remains limited, particularly in tasks requiring mathematical and physical reasoning.

%  Our previous case study and GPT-4o
A case study~\cite{zhang2024opportunities} on \textsc{GPT-4o}'s potential to assist atmospheric science researchers in addressing tasks, showcasing its ability to handle heterogeneous input data and execute complex tasks. However, the evaluation also reveals that \textsc{GPT-4o} falls short of solving graduate-level atmospheric science problems, indicating room for improvement in reasoning and problem-solving capabilities at advanced levels.

% Reasoning model
More recently, a new generation of reasoning-focused models, such as \textsc{GPT-o1} \textsc{Gemini-2.0-Flash-Thinking}, \textsc{QwQ} and \textsc{DeepSeek-R1}, has emerged, showcasing remarkable capabilities in mathematical and scientific problem-solving. These models represent a paradigm shift, with potential applications that extend beyond traditional tasks, leveraging advanced reasoning techniques to address challenges in complex domains like atmospheric science.
}
\fi












% In light of these advancements, our work aims to bridge the gap between task-specific and reasoning-oriented models by introducing \name, a benchmark that evaluates the reasoning, problem-solving, and domain-specific capabilities of large language models in atmospheric science.


% \TBD The Impact of Reasoning Step Length on Large Language Models: Reasoning step is important in Cot prompt



\textbf{LLM benchmarks.}
\label{sec:llm_benchmarks}
Assessing LLMs is crucial for ensuring their effectiveness in deployment across various domains~\cite{liang2022holistic}. Traditional benchmarks like \texttt{GSM8K}~\cite{cobbe2021training} and \texttt{MATH}~\cite{hendrycks2021measuring} have become less effective as state-of-the-art models achieve near-perfect scores, necessitating more challenging benchmarks to evaluate reasoning capabilities accurately.
Recent benchmarks target specialized fields, such as \texttt{GPQA-Diamond}~\cite{rein2023gpqa} for expert-level science, \texttt{AIME2024}~\cite{MAAInvitational2024} for advanced mathematics, and \texttt{SCIBENCH}~\cite{wang2023scibench} for collegiate science problems. 
However, a comprehensive LLM benchmark for atmospheric science remains underrepresented, where \texttt{CLIMAQA}~\cite{manivannan2024climaqa} only offers basic definition-based assessments, lacking depth in evaluating complex problem-solving abilities.
Designing a good LLM benchmark requires principled guidance to ensure robust, accurate, and meaningful evaluation. For example, A notable advancement is the introduction of symbolic extensions in benchmarking, as seen in \texttt{GSM-Symbolic}~\cite{mirzadeh2024gsm}, \texttt{VarBench}~\cite{qian2024varbench}, and \texttt{MM-PhyQA}. These benchmarks introduce question variants by altering numerical values or modifying problem structures, improving robustness, and mitigating contamination risks. Notably, \texttt{GSM-Symbolic} highlights that even minor perturbations can significantly impact model performance, revealing fragilities in LLM reasoning.
Additionally, numerical reasoning plays a fundamental role in evaluating LLMs, especially for scientific applications. Papers like NumberCookbook~\cite{yang2024number} and NumeroLogic~\cite{schwartz2024numerologic} uncover weaknesses in LLMs' ability to process numerical information accurately, emphasizing that tokenization strategies and internal number representation significantly affect arithmetic performance~\cite{singh2024tokenization}.
Despite advancements in benchmarking, a rigorous climate-focused evaluation framework is still missing. %We address this gap by proposing a benchmark that integrates domain-specific challenges with principled evaluation methods, ensuring LLMs are tested on reasoning, robustness, and numerical accuracy in climate science.

\iffalse
{
% Show frontier models1 do so well on MATH2 and GSM8K that these benchmarks are no longer effective at differentiating models. Using some example to show these. For example gpt o4 can got 90 something on MATH2, o1 got 9x on Math2, not effective to differentiate
% \cite{Recent frontier models1 do so well on MATH2 and GSM8K that these benchmarks are no longer effective at differentiating models. https://openai.com/index/learning-to-reason-with-llms/}
Benchmarks like the GSM8K~\cite{cobbe2021training} and MATH~\cite{hendrycks2021measuring} have long been used to evaluate instruction-tuned models. However, recent advancements in frontier models have rendered these benchmarks less effective at distinguishing model performance, as state-of-the-art models now achieve near-perfect scores on them. 

Benchmarks with higher-quality and extremely challenging questions are regarded as the primary evaluation tools for reasoning models due to their proven effectiveness. Among these, \texttt{GPQA-Diamond}~\cite{rein2023gpqa} represents the science domain, featuring PhD-level questions in chemistry, physics, and biology to assess expert-level understanding beyond general knowledge. Similarly, the American Invitational Mathematics Examination 2024 (\texttt{AIME2024})~\cite{MAAInvitational2024} stands as a representative benchmark for the mathematics domain, focusing on advanced topics and complex reasoning.
% \texttt{MMMU} and \texttt{MMMU-Pro} assess the true understanding and reasoning capabilities of multimodal models through rigorous testing, with \texttt{MMMU-Pro} introducing visual-only settings and augmented candidate options to ensure robust evaluation of vision-language integration.

% Beyond these general-purpose benchmarks, science-specific benchmarks have emerged, offering unique perspectives and challenges. \texttt{MM-PhyQA} is a multimodal physics question-answering benchmark incorporating Multi-Image Chain-of-Thought (MI-CoT) prompting. It evaluates high school-level physics problems, focusing on models’ abilities to integrate visual and textual information. \texttt{SCIBENCH} is a collegiate-level benchmark designed to evaluate scientific problem-solving abilities across mathematics, chemistry, and physics, highlighting limitations in current LLMs and showing that even top-performing non-reasoning models achieve only below 50\% accuracy.
More science benchmarks such as \texttt{SCIBENCH}~\cite{wang2023scibench} and \texttt{MM-PhyQA}~\cite{anand2024mm} have emerged focusing on collegiate-level problem-solving.

For climate science, \texttt{CLIMAQA}~\cite{manivannan2024climaqa} provides a domain-specific evaluation framework, primarily focusing on definition-based questions.
% and lacks assessments requiring the selection of appropriate physical models or advanced mathematical reasoning. Consequently, it does not adequately test models' complex reasoning capabilities.


\texttt{GSM-Symbolic}~\cite{mirzadeh2024gsm},\texttt{VarBench}~\cite{qian2024varbench}, and \texttt{MM-PhyQA} they come up with new paradigm of benchmark, for each question in dataset, they can question variants by altering the variables. This approach provides a more accurate and robust assessment of the true capabilities of language models, effectively mitigating the contamination problem. 
Notably, \texttt{GSM-Symbolic} reveals that minor changes, such as altering numerical values or adding irrelevant clauses, can significantly impact model performance. This highlights the fragility of language models in mathematical reasoning and underscores their current limitations.

% Similarly, \texttt{GSM-Symbolic} addresses the limitations of existing mathematical reasoning evaluations by introducing a benchmark based on symbolic templates that generate diverse and controllable questions. This approach reveals significant performance variability in LLMs when faced with different instantiations of similar problems and highlights the fragility of their reasoning capabilities. \texttt{GSM-Symbolic} demonstrates that even minor changes, such as altering numerical values or adding irrelevant clauses, can lead to drastic performance drops, providing critical insights into the current limitations of LLMs in mathematical reasoning.


Despite these advancements, there remains a notable gap in benchmarks designed for advanced reasoning models, particularly in the diversity and scope of scientific domains. Existing benchmarks predominantly focus on subjects like chemistry, physics, and biology, leaving significant room for expansion in diversity and lacking adequate representation of interdisciplinary fields. 
% To address this gap, we introduce \name, a novel benchmark tailored to the interdisciplinary field of atmospheric science. 
% By expanding the scope of science benchmarks and increasing their diversity. \name not only enhances the evaluation of reasoning models but also provides a rigorous framework to assess LLMs’ overall competency in scientific problem-solving.

% GSM
% Pattern matching


\textbf{Arithmatic}

The mechanisms behind arithmetic in LLMs remain unclear, and their robustness in performing arithmetic—an essential capability in scientific disciplines—continues to be a subject of investigation. In this context of reasoning ability and Numerical Understanding and Processing Ability (NUPA) are often evaluated together, Number Cookbook~\cite{yang2024number} provides a comprehensive test suite encompassing diverse numerical representations and tasks, which has uncovered surprising vulnerabilities in LLMs' handling of fundamental arithmetic operations. Similarly, NumeroLogic~\cite{schwartz2024numerologic} highlights that the challenges LLMs face in processing numerical data and performing arithmetic are partly due to the non-intuitive representation of textual numbers. Tokenization strategies significantly impact these challenges, as tokenization-dependent error patterns reveal that arithmetic computation errors often stem from tokenization choices. These findings further suggest the existence of an underlying algorithm within LLMs, challenging the notion that they rely solely on “fuzzy” matching to similar training examples~\cite{singh2024tokenization}.
}
\fi







\iffalse
\begin{itemize}
    \item The emergence of reasoning model: xxx
    \item CLIMATE Perdiction models
    \item CLIMAQA / ClimateGPT: Text definition benchmarj
    \item Reasoning model bench: AIME2024 (Math) Math Reasoning and GPQA(Science): A Graduate-Level Google-Proof Q\&A Benchmark
    \item Science + Numerical Value Variation: + Structural Variation: MM-PhyQA: Multimodal Physics Question-Answering  With Multi-Image CoT Prompting
    \item SCIBENCH: Evaluating College-Level Scientific Problem-Solving Abilities of  Large Language Models: that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills.
    \item: The Impact of Reasoning Step Length on Large Language Models: Reasoning step is important in Cot prompt
\end{itemize}
\fi