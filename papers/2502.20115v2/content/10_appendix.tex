
\appendix
\clearpage

\section*{Appendix}

The paper and appendix are organized as follows.

\renewcommand{\contentsname}{}
\vspace{-1cm}
\tableofcontents
\newpage

\section{Proofs of Theorems} 
\label{app:sec:multiview_lingam}

\paragraph{Defining a domain connecting ICA and SEM parameters}
In the following proofs, we define the set
\begin{align}
    \label{eq:domain_unmixing_matrix}
\begin{split}
    \mathcal{W} 
    = 
    \{ 
    &\mW \in \mathbb{R}^{p \times p} \mid \text{there exist a permutation matrix } 
    \mP \text{ and a strictly lower triangular matrix } \mT \\
    &\text{ such that } \mW = \mP^\top (\mI - \mT) \mP \}
\end{split}
\end{align}
as the domain of the ``unmixing matrices" $\mW=\mA^{-1}$ in the ICA model from~\eqref{eq:ica_model}, that are compatible with the DAG structure in the structural equation model (SEM) from~\eqref{eq:causal_model}.
As noted in the main text, unmixing matrices are not constrained in the ICA model, except by invertibility (and possibly by normalizing their rows).
However, in the context of SEM estimation, the requirement that $\mW = \mI - \mB$ belongs to the set $\mathcal{W}$ is a key structural constraint.
Furthermore, it should be noted that finding $\mW$ is equivalent to finding $\mB$, since they are related by $\mB=\mI-\mW$.

\subsection{Technical lemmas}

The following two lemmas are about matrices in the context of DAGs. They are used in the proofs of Theorems~\ref{theorem:lingam}, \ref{theorem:identifiability_B_tilde_i}, \ref{theorem:identifiability_multiview_different_Pi}, and~\ref{theorem:identifiability_multiview_shared_P}, and are proven in Appendix~\ref{app:ssec:lemma}.

The basic identifiability results for the causal matrices $\mB^i$ use the following:
\begin{lemma}
    \label{lemma:identifiability_lingam}
    Consider $\mW, \mW' \in \mathcal{W}$ defined in \eqref{eq:domain_unmixing_matrix},  $\mK$ a permutation matrix and $\mD$ a diagonal matrix (of signs and scalings in the ICA context). Then 
    \begin{align}
        \mW' = \mD \mK \mW \implies 
        \mK = \mD = \mI
        \enspace.
    \end{align}
\end{lemma}
On the other hand, the identifiability results for the causal ordering $\mP$ (or causal orderings $\mP^i$) use the following:
 \begin{lemma}
 \label{app:lemma:Bi_fully_connected}
        Let $\mP$ and $\mP'$ be permutation matrices and $\mT$ and $\mT'$ be strictly lower triangular matrices, such that $\mP^\top \mT \mP = \mP'^\top \mT' \mP'$.
        Assume that $\mT$ contains only non-zero elements in its strictly lower triangular part.
        Then, $\mP = \mP'$ and $\mT = \mT'$.
    \end{lemma}


\subsection{Proof of Theorem~\ref{theorem:lingam}}
\label{app:ssec:lingam}

This proof is based on the one from~\citet{shimizu2006linear}, which we attempt to make a bit more rigorous. 
Identifiability was also shown in~\citet[Section $2.3$]{shimizu2022statistical} in the case with $2$ components.

Let us transform the SEM
\begin{align}
    \label{app:eq:causal_model_original}
    \vx = \mB \vx + \vs
\end{align}
into an ICA model
\begin{align}
    \label{app:eq:ica_model}
    \vx = \mA \vs
\end{align}
where the mixing matrix $\mA$ is constrained as $\mA^{-1}=\mI-\mB$, and where $\mB$ is a DAG. The assumptions on $\vs$ are identical in SEM and ICA. Given that $\mB$ is a DAG, we have that $\mW=\mA^{-1}$ belongs to the set $\mathcal{W}$ defined in \eqref{eq:domain_unmixing_matrix}.

Consider two matrices $\mW, \mW' \in \mathcal{W}$ that parameterize the same statistical model in~\eqref{app:eq:causal_model_original} (here, we parameterize by $\mW=\mI-\mB$ instead of $\mB$).
Since $\mathcal{W}$ is included in the space of invertible matrices, we deduce that $\mW$ and $\mW'$ are invertible, which makes them valid unmixing matrices for the same ICA model in \eqref{app:eq:ica_model}.
So, from the identifiability theory of ICA~\cite{comon1994identifiability}, we know that there exist a permutation matrix $\mK$ and a sign-scaling matrix $\mD$ (i.e. a diagonal matrix) such that 
\begin{align}
    \label{eq:lingam_indeterminacy}
    \mW' 
    &= 
    \mD \mK \mW
    \enspace.
\end{align}
Then, we apply Lemma~\ref{lemma:identifiability_lingam},  which shows that being in the domain $\mathcal{W}$ imposes $\mK = \mD = \mI$. Thus, only a single possible solution of the ICA problem is valid, and $\mW$ is fully identifiable (``fully" identifiable is in contrast to conventional ICA theory where some indeterminacies always remain). Since $\mB=\mI-\mW$, we deduce that $\mB$ is fully identifiable as well.


\subsection{Proof of Theorem~\ref{theorem:identifiability_B_tilde_i}}
\label{app:ssec:identifiability_B_tilde_i}

Consider two sets $\Theta = (\mSigma^1, \dots, \mSigma^m, \mW^1, \dots, \mW^m)$ 
and $\Theta' = (\mSigma'^1, \dots, \mSigma'^m, \mW'^1, \dots, \mW'^m)$ that parameterize the same statistical model in~\eqref{eq:multiview_causal_model}, with the $\mW^i$ and $\mW'^i$ necessarily belonging to $\mathcal{W}$.
Note that, as in Appendix~\ref{app:ssec:lingam}, we parameterize by $\mW^i$ instead of $\mB^i$, but it all boils down to the same thing.
First, we use the fact that the domain $\mathcal{W}$ is included in the space of invertible matrices. Thus, the two sets of $\mW^i$ and $\mW'^i$ matrices are valid sets of unmixing matrices for the same multi-view ICA model in \eqref{eq:multiview_ica_model}.
So, from the identifiability theory of multi-view ICA~\citep[Theorem $1$]{richard2021sharedica}, we know that there exist a permutation matrix $\mK$ and a sign-scaling matrix $\mD$, such that for any view $i \in \llbracket 1, m \rrbracket$, we have
\begin{align}
\begin{split}
    \mW'^i
    &=
    \mD \mK \mW^i
    \\
    \mSigma'^i
    &=
    (\mD \mK) \mSigma^i (\mD \mK)^\top \enspace.
\end{split}
\end{align}
Then, we apply Lemma~\ref{lemma:identifiability_lingam} (even for one view is enough), which shows that being in the domain $\mathcal{W}$ imposes $\mK = \mD = \mI$.


\subsection{Proof of Theorem~\ref{theorem:identifiability_multiview_different_Pi}}
\label{app:ssec:identifiability_multiview_different_Pi}

In the context of view-specific causal orderings $\mP^i$, we prove that, under Assumptions~\ref{assump:noise_diversity} and~\ref{assump:total_order_different_Pi}, the decomposition of matrices $\mB^i$ into matrices $\mT^i$ and $\mP^i$ is unique.

Consider two sets of parameters $\Theta = (\mSigma^1, \dots, \mSigma^m, \mP^1, \dots, \mP^m, \mT^1, \dots, \mT^m)$ and $\Theta' = (\mSigma'^1, \dots, \mSigma'^m, \allowbreak \mP'^1, \dots, \mP'^m, \mT'^1, \dots, \mT'^m)$ that parameterize the same statistical model in~\eqref{eq:multiview_causal_model}, where $\mP^i, \mP'^i$ are permutation matrices, and $\mT^i, \mT'^i$ are strictly lower triangular matrices.
Note that here we parameterize by $\mP^i$ and $\mT^i$ rather than $\mB^i$ or $\mW^i$.
From Theorem~\ref{theorem:identifiability_B_tilde_i}, we know that $\mSigma^i = \mSigma'^i$ and that the resulting causal matrices must be equal:
\begin{equation}\label{app:eq:PBP_prime}
    (\mP^i)^\top \mT^i \mP^i
    =
    (\mP'^i)^\top \mT'^i \mP'^i
    \enspace.
\end{equation}

Assumption~\ref{assump:total_order_different_Pi} states that, for each view $i$, the matrix $\mB^i = (\mP^i)^\top \mT^i \mP^i = (\mP'^i)^\top \mT'^i \mP'^i$ contains exactly $\frac{p (p-1)}{2}$ non-zero elements, so it is also the case for $\mT^i$ and $\mT'^i$ which thus represent fully connected graphs.
So, from Lemma~\ref{app:lemma:Bi_fully_connected}, we deduce that, for each view $i$, we have $\mP^i = \mP'^i$ and $\mT^i = \mT'^i$, which concludes the proof.


\subsection{Proof of Theorem~\ref{theorem:identifiability_multiview_shared_P}}
\label{app:ssec:identifiability_multiview_shared_P}

In the context of shared causal ordering $\mP$, we prove that, under Assumptions~\ref{assump:noise_diversity} and~\ref{assump:total_order_shared_P}, the decomposition of matrices $\mB^i$ into matrices $\mT^i$ and $\mP$ is unique.

Consider the two sets $\Theta = (\mSigma^1, \dots, \mSigma^m, \mP, \mT^1, \dots, \mT^m)$ and $\Theta' = (\bar{\mSigma}^1, \dots, \bar{\mSigma}^m, \bar{\mP}, \bar{\mT}^1, \dots, \bar{\mT}^m)$ that parameterize the same statistical model in~\eqref{eq:multiview_causal_model}, where $\mP$ is some permutation matrix, $\mT^i$ are strictly lower triangular matrices, and $\bar{\mP}$ and $\bar{\mT}^i$ are the particular matrices given by Assumption~\ref{assump:total_order_shared_P}.
Note that no assumption on the sparsity of the $\mT^i$ is made.
From Theorem~\ref{theorem:identifiability_B_tilde_i}, we know that $\mSigma^i = \bar{\mSigma}^i$.

Assumption~\ref{assump:total_order_shared_P} amounts to saying that the ``reunion" of the $\bar{\mT}^i$ represents a fully connected graph.
In other words, if we define the reunion as $\bar{\mT}^U = \sum_{i=1}^m \operatorname{abs}(\bar{\mT}^i)$, where $\operatorname{abs}$ denotes the element-wise absolute value function, then we are assuming that the strictly lower triangular part of $\bar{\mT}^U$ only has non-zero elements.
In a similar way to the definition of $\bar{\mT}^U$, let us define
\begin{align}
    \bar{\mW}^U = \sum_{i=1}^m \operatorname{abs}(\bar{\mW}^i)
\end{align}
where $\bar{\mW}^i = \mI - \bar{\mP}^\top \bar{\mT}^i \bar{\mP}$.
The non-zero elements of $\bar{\mP}^\top \bar{\mT}^i \bar{\mP}$ are outside of the diagonal, so matrices $\mI$ and $\bar{\mP}^\top \bar{\mT}^i \bar{\mP}$ contain non-zero elements at different locations.
Thus, we have
\begin{align}
    \operatorname{abs}(\mI - \bar{\mP}^\top \bar{\mT}^i \bar{\mP})
    =
    \mI + \operatorname{abs}(\bar{\mP}^\top \bar{\mT}^i \bar{\mP})
    \enspace.
\end{align}
Furthermore, applying $\bar{\mP}$ to the rows and columns of $\bar{\mT}^i$ only shuffles its entries, without modifying their values. So we have
\begin{align}
    \operatorname{abs}(\bar{\mP}^\top \bar{\mT}^i \bar{\mP})
    =
    \bar{\mP}^\top \operatorname{abs}(\bar{\mT}^i) \bar{\mP} 
    \enspace.
\end{align}
Consequently,
\begin{equation}
    \bar{\mW}^U
    =
    \sum_{i=1}^m \mI + \bar{\mP}^\top \operatorname{abs}(\bar{\mT}^i) \bar{\mP}
    =
    \sum_{i=1}^m \bar{\mP}^\top (\mI + \operatorname{abs}(\bar{\mT}^i)) \bar{\mP}
    =
    \bar{\mP}^\top (m\mI + \bar{\mT}^U) \bar{\mP}
    \enspace.
\end{equation}
So, dividing both sides by $m$ gives
\begin{align}
    \frac1m \bar{\mW}^U
    =
    \bar{\mP}^\top \left( \mI - \left( -\frac1m \bar{\mT}^U \right) \right) \bar{\mP}
\end{align}
where $-\frac1m \bar{\mT}^U$ is strictly lower triangular, and its strictly lower triangular part only has non-zero elements.
    
Next, we apply the same reasoning to the alternative set of parameters, given by $\mW^i = \mI - \mP^\top \mT^i \mP$, and we consider $\mW^U = \sum_{i=1}^m \operatorname{abs}(\mW^i)$.
The proof of Theorem~\ref{theorem:identifiability_B_tilde_i} already implied that $\bar{\mW}^i = \mW^i$ for all $i$. Thus, we have $\bar{\mW}^U = \mW^U$ and
\begin{align}
    \label{app:eq:identifiability_P_reunion_Bi}
    \bar{\mP}^\top \left( \mI - \left( -\frac1m \bar{\mT}^U \right) \right) \bar{\mP}
    = 
    \mP^\top \left( \mI - \left( -\frac1m \mT^U \right) \right) \mP
\end{align}
where $\mT^U$ is defined in a similar way as $\bar{\mT}^U$, except that its strictly lower triangular part can be sparse.
Using Lemma~\ref{app:lemma:Bi_fully_connected} on~\eqref{app:eq:identifiability_P_reunion_Bi},
we obtain that $\mP = \bar{\mP}$, and thus $\mT^i = \bar{\mT}^i$ for all~$i$.
In conclusion, all the sets of DAG decompositions that parameterize the same model are equal, as soon as for one of these decompositions, the reunion of the $\mT^i$ is dense.
We conclude that matrices $\mP$ and $\mT^i$ are unique, and thus identifiable in our terminology.
This proof also implies that there can be only one matrix $\bar{\mP}$ that fulfills Assumption~\ref{assump:total_order_shared_P}.


\newpage
\subsection{Proofs of technical lemmas}
\label{app:ssec:lemma}

\subsubsection{Proof of Lemma~\ref{lemma:identifiability_lingam}}

Consider $\mW, \mW' \in \mathcal{W}$, $\mK$ a permutation matrix, and $\mD$ a diagonal matrix. Suppose that
\begin{align}\label{eq:lingam_indeterminacy_inappendix}
    \mW' = \mD \mK \mW \enspace.
\end{align}

\paragraph{A permutation inequality on $\mK$}
We use the following decompositions from~\eqref{eq:domain_unmixing_matrix},
\begin{align}
    \mW 
    = 
    \mP^\top (\mI- \mT) \mP
    \;, \quad
    \mW' 
    = 
    \mP'^\top (\mI - \mT') \mP' \;,
\end{align}
where $\mP, \mP'$ are permutation matrices and $\mT, \mT'$ are strictly lower triangular matrices. Denote $\mL = \mI - \mT$ and $\mL' = \mI - \mT'$; these are lower triangular matrices that have unit diagonals. We plug the decompositions into~\eqref{eq:lingam_indeterminacy_inappendix}, and get
\begin{align}
    \label{eq:lemma_6_decomposition}
    \mP'^\top \mL' \mP'
    =
    \mD \mK
    \mP^\top \mL \mP
    \enspace.
\end{align}
To exploit the structure of the lower triangular matrices $ \mL$ and $\mL'$ and show how they constrain $\mK$, we now switch notations from permutation matrices $(\mP, \mP', \mK)$, to their corresponding permutation functions $(\phi, \phi', \psi)$. This yields
\begin{align}
    \mL'_{\phi'(i), \phi'(j)}
    =
    \mD_{ii} \mL_{\phi(\psi(i)), \phi(j)} \;,
    \quad \forall i, j \in \llbracket 1, p \rrbracket \;.
\end{align}
In particular, $\mL'$ has unit diagonal, so
\begin{align}
    1 =
    \mL'_{\phi'(i), \phi'(i)}
    =
    \mD_{ii} \mL_{\phi(\psi(i)), \phi(i)} \;,
    \quad \forall i \in \llbracket 1, p \rrbracket \;,
\end{align}
and $\mL$ is lower triangular, so that its non-zero entries must satisfy
\begin{align}
    \phi(i) \leq \phi(\psi(i)) \;,
    \quad \forall i \in \llbracket 1, p \rrbracket \;.
\end{align}
More generally, we can replace $i$ with $\psi(i)$, and so on, and obtain
\begin{align}
    \label{eq:chained_inequalities}
    \phi(i)
    \leq 
    \phi(\psi(i))
    \leq
    \phi(\psi^{(2)}(i))
    \leq
    \ldots
    \quad \forall i \in \llbracket 1, p \rrbracket
\end{align}
where the superscript denotes composition and where we can apply the permutation $\psi$ an arbitrary number of times.

\paragraph{$\mK$ must be the identity}
Suppose that $\psi$ is not the identity. We can pick an index $k \in \llbracket 1, p \rrbracket$ where $k \neq \psi(k)$. Because $\phi$ and $\psi$ are injective, we can apply them to the inequality any number of times $n \in \mathbb{N}$ and get
\begin{align}
    \phi(\psi^{(n)}(k)) \neq \phi(\psi^{(n+1)}(k)) \enspace,
\end{align}
which together with~\eqref{eq:chained_inequalities} implies
\begin{align}
    \phi(k)
    < \phi(\psi(k)) 
    < \phi(\psi^{(2)}(k)) 
    < \ldots
\end{align}
which can be applied any number of times.
However, each inequality increases the index by at least one, while the index cannot go above $p$. Thus, applying the chain at least $p$ times, we have a contradiction.

\paragraph{$\mD$ must be the identity} Now that we know that $\mK = \mI$, \eqref{eq:lemma_6_decomposition} becomes
\begin{align}
    \mP'^\top \mL' \mP'
    =
    \mD
    (\mP^\top \mL \mP)
    \enspace.
\end{align}
The matrix $\mP'^\top \mL' \mP'$ has a diagonal of ones and so too does $\mP^\top \mL \mP$. It follows that $\mD = \mI$. This concludes the proof of the Lemma.


\newpage
\subsubsection{Proof of Lemma~\ref{app:lemma:Bi_fully_connected}}
\label{app:sssec:lemma:Bi_fully_connected}

From the equality $\mP^\top \mT \mP = \mP'^\top \mT' \mP'$, we deduce that
\begin{align}
    \label{app:eq:B_Bprime}
    \mT
    =
    \tilde{\mP}^\top \mT' \tilde{\mP}
    \enspace,
\end{align}
where $\tilde{\mP} = \mP' \mP^\top$ is the permutation matrix represented by function $\phi$, which is defined such that: $\forall k \in [\![1, p]\!]$, $\tilde{\mP}_{k, \phi(k)} = 1$ and $\forall l \neq \phi(k)$, $\tilde{\mP}_{k, l} = 0$.
Using the notation $\phi$ instead of $\tilde{\mP}$, \eqref{app:eq:B_Bprime} can be rewritten as, $\forall k, l \in [\![1, p]\!]$,
\begin{align}\label{app:eq:B^i_kl}
    \mT_{k, l}
    =
    \mT'_{\phi(k), \phi(l)}
    \enspace.
\end{align}
We proceed by a proof by contradiction: assume that $\tilde{\mP} \neq \mI$.
As a consequence, there exists an index $k$ such that $\phi(k) \neq k$. 
Let us fix such an index as $k$.
We can assume without loss of generality that 
\begin{align}
    \phi(k) > k
    \enspace,
\end{align}
otherwise we just have to invert signs and switch rows and columns in the following.
By assumption, the strictly lower triangular part of $\mT$ only has non-zero elements, so we have
\begin{align}
    \mT_{\phi(k), k} 
    \neq 
    0 \enspace.
\end{align}
Yet, from~\eqref{app:eq:B^i_kl}, we know that $\mT_{\phi(k), k} = \mT'_{\phi(\phi(k)), \phi(k)}$ and all the non-zero elements of $\mT'$ are in its strictly lower triangular part, which implies
\begin{align}\label{app:eq:phi_phi>phi}
    \phi(\phi(k))
    >
    \phi(k)
    \enspace.
\end{align}
This logic can be applied repeatedly as in the preceding lemma's proof, and we see that
\begin{align}
    \phi(k)
    <
    \phi^{(2)}(k)
    < \phi^{(3)}(k)
    <
    \dots
    \enspace.
\end{align}
Now, this leads to an infinite strictly increasing sequence, which is contradictory since the index cannot grow greater than $p$.
So, $\tilde{\mP} = \mI$, which means that $\mP' \mP^\top \mP = \mP$, and thus, using the orthogonality of $\mP$,
\begin{align}\label{app:eq:P'_P}
     \mP' = \mP \enspace.
\end{align}
It also follows that $\mT' = \mT$, which concludes the proof of Lemma~\ref{app:lemma:Bi_fully_connected}.


\newpage
\section{Additional experiments}
\label{app:sec:additional_experiments}

\subsection{Additional simulations}
\label{app:ssec:additional_simulations}

In the two following simulations, we study a third version of the MICaDo algorithm where the multi-view ICA algorithm ShICA~\cite{richard2021sharedica} is replaced with MVICA~\cite{richard2020modeling}. 
This version is called MICaDo-MVICA and is displayed in purple.
Although MVICA was not specifically designed to handle Gaussian disturbances, experiments proved that it was quite robust against Gaussianity~\cite{richard2020modeling}.

\paragraph{Multiple causal orderings}
We explore in this paragraph the case where causal orderings are view-specific. To do so, we use the experiment setup of Fig.~\ref{fig:simulation_study} but use multiple matrices $\mP^i$ instead of a shared matrix $\mP$ and we display the results in Fig.~\ref{app:fig:simulation_multiple_Pi}.
One can observe that this setting is not favourable to MultiGroupDirectLiNGAM~\citep{shimizu2012joint}, which still tries to estimate a shared causal ordering, hence the bad performance of the red curve. On the other hand, MICaDo-ML still achieves state-of-the-art results, even when estimating multiple causal orderings.
The results of MICaDo-ML, MICaDo-J, and ICA-LiNGAM~\cite{shimizu2006linear} are comparable to the ones in Fig.~\ref{fig:simulation_study}.
In addition, we can observe that MICaDo-MVICA seems to achieve the best results in the pure non-Gaussian scenario, very close to MICaDo-ML and ICA-LiNGAM, but it fails when all disturbances are Gaussian. 
Interestingly, we can see that its performance increases as the number of samples increases.
This experiment confirms that MICaDo-ML is the only one of the studied algorithms that performs well in all three scenarios.

\begin{figure*}[h]
\centerline{\includegraphics[width=0.95\columnwidth]{image/simulation_multiple_Pi.pdf}}
\caption{
Separation performance of ICA-LiNGAM~\citep{shimizu2006linear}, MultiGroupDirectLiNGAM~\citep{shimizu2012joint} and three versions of our method in the \emph{multiple causal orderings} context and while varying the number of samples. Left column: all disturbances are Gaussian. Middle column: all disturbances are non-Gaussian and the noise variances are equal. Right column: half of the disturbances are Gaussian and the other half are non-Gaussian.
There are three different metrics: the $\ell_2$-distance between true and estimated causal effect matrices $\mB^i$ (upper row), the $\ell_2$-distance between true and estimated matrices $\mT^i$ (middle row), and the rate of failed estimated causal orderings $\mP^i$ over all repetitions (lower row; lower is better).
We used 50 different seeds.}
\label{app:fig:simulation_multiple_Pi}
\end{figure*}

\paragraph{Varying noise level}
It is well-known in ICA theory that algorithms find it increasingly difficult to estimate components as noise increases.
Since our method relies on multiview ICA estimation, studying how performance varies when noise level increases is important.
In Fig.~\ref{app:fig:simulation_noise}, we reproduce the simulation of Fig.~\ref{fig:simulation_study} and~\ref{app:fig:simulation_multiple_Pi} but set the number of samples $n$ to 1000 and vary the noise level (previously set to 1) from $10^{-2}$ to $10^2$. As in Fig.~\ref{fig:simulation_study}, we use the shared causal ordering setting.

\begin{figure*}[h]
\centerline{\includegraphics[width=0.95\columnwidth]{image/simulation_noise.pdf}}
\caption{
Separation performance of ICA-LiNGAM~\citep{shimizu2006linear}, MultiGroupDirectLiNGAM~\citep{shimizu2012joint} and three versions of our method in the shared causal ordering context and while varying the noise level.
The three scenarios and three metrics are the same as in Fig.~\ref{fig:simulation_study} and~\ref{app:fig:simulation_multiple_Pi}.}
\label{app:fig:simulation_noise}
\end{figure*}

We can observe that MICaDo-ML achieves the best results (just ahead of MICaDo-J) when disturbances are Gaussian and the noise level is between $10^{-1}$ and 10.
The drop in performance of MICaDo-ML and MICaDo-J when the noise level goes below $10^{-1}$ can probably be explained by the reduction in noise diversity.
Below $10^{-1}$ and above 10, none of the methods gives good results.

In the context of non-Gaussian disturbances, MICaDo-MVICA and MultiGroupDirectLiNGAM~\citep{shimizu2012joint} outperform the other methods, especially when the noise level is lower than 1.
The original ICA-LiNGAM~\citep{shimizu2006linear} produces decent results, whereas MICaDo-ML is only competitive when the noise level is in $[1, 10]$ and MICaDo-J is bad.
When the noise level exceeds 10, none of the methods retrieve the parameters.

When half of the disturbances are Gaussian and the other half are non-Gaussian, results are more mixed.
We can observe that MICaDo-ML achieves the best results when the noise level is close to 1 (i.e. sources and noises are of similar orders) but MultiGroupDirectLiNGAM outperforms other methods as soon as the noise level is below a threshold somewhere in $[10^{-1}, 1]$.
As for the other scenarios, none of the methods give good results when the noise goes beyond 10.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\columnwidth]{image/average_envelope.pdf}
    \caption{Example of time series obtained after the preprocessing. We averaged the data across subjects and trials, resulting in 10 time series, one for each brain region.}
    \label{app:fig:average_envelope}
\end{figure}

\subsection{Additional experiments on real data}
\label{app:ssec:additional_expes_real_data}

In the following, we extend the experiments conducted in Section~\ref{sec:experiments}.
First of all, Fig.\ref{app:fig:average_envelope} represents the typical time series obtained after the preprocessing.
For visualization purposes, we averaged these time series across subjects and trials.
We see a peak happening before the button press (at $t=0$\,s) in the ``superiortemporal" auditory regions, due to the stimulus. 
We also see a clear rebound effect in the motor regions, as the corresponding time series drop around $t=0$\,s (as the beta waves desynchronize) and then rise from $t=0.3$\,s (as they synchronize again).
This figure also gives the names of the 10 selected parcels.

Figure~\ref{app:fig:six_brains} shows additional brain plots, as a supplement to Fig.~\ref{fig:two_brains}.
For each of the six runs, we randomly selected 50\% of the participants and applied MICaDo-ML to their data.
The same observations apply to the four additional plots, namely that many arrows point from the motor cortex of one hemisphere to the motor cortex of the other hemisphere.

\begin{figure*}[h]
\centerline{\includegraphics[width=0.95\columnwidth]{image/six_brains.pdf}}
\caption{
Top ten most important causal effects on six different runs. Each run was performed on the data of 49 randomly chosen subjects. We took the median of the causal effect matrices and picked the 10 highest entries in absolute value.
Red arrows represent positive effects and blue arrows negative effects.
The two runs at the top correspond to the ones in Fig.~\ref{fig:two_brains}.}
\label{app:fig:six_brains}
\end{figure*}

Fig.~\ref{app:fig:pearson_coefs_B} shows the Pearson correlations between the median matrices obtained for each of the 50 runs.
More specifically, for each run, we computed the element-wise median of the $\mB^i$ matrices, and then we calculated the Pearson correlation between flattened median matrices, for all pairs of runs.
This procedure gives an average correlation of 0.39.
This result tends to show that the medians of the $\mB^i$ matrices are consistent across runs.

Finally, Fig.~\ref{app:fig:spearmanr_coefs_P} shows the Spearman's rank correlation matrix whose entries are used in Fig.~\ref{fig:histogram_spearmanr_coefs_P}. 
This matrix studies the correlations between the 50 estimated causal orderings. 
We can see that, apart from a few particular runs (2, 16, 35, and 47), the orderings are often positively correlated.

\begin{figure}
    \centering
    \begin{minipage}{.45\textwidth}
        \centering
        \includegraphics[width=.8\textwidth]{image/pearson_coefs_B_shica_ml.pdf}
        \captionof{figure}{Pearson correlations between the flattened median causal effect matrices obtained for each of the 50 runs.}
        \label{app:fig:pearson_coefs_B}
    \end{minipage}%
    \hfill
    \begin{minipage}{.45\textwidth}
        \centering
        \includegraphics[width=.8\textwidth]{image/spearmanr_coefs_P_shica_ml.pdf}
        \captionof{figure}{Spearman's rank correlations between estimated orderings $\mP$. This matrix corresponds to the histogram in Fig.~\ref{fig:histogram_spearmanr_coefs_P}.}
        \label{app:fig:spearmanr_coefs_P}
    \end{minipage}
\end{figure}
