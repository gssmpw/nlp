
\section{Background}
\label{sec:background}

\paragraph{Causal ordering}
In the following, we consider a matrix $\mB \in \R^{p \times p}$ that encodes the structure of a directed acyclic graph (DAG), which means that the non-zero entries of $\mB$ represent the edges of the graph.
It follows that there exist a strictly lower triangular matrix $\mT$ and a permutation matrix $\mP$, referred to as the \emph{causal ordering}, such that $\mB = \mP^\top \mT \mP$.
In general, such an ordering is not unique.

\paragraph{LiNGAM, a model for causal discovery}
Let $\vx \in \R^p$ be a random vector of observations and let $\mB \in \R^{p \times p}$ represent a DAG.
We consider a structural equation model, also known as a functional causal model, where the data follows
\begin{align}
    \label{eq:causal_model_original}
    \vx = \mB \vx + \vs
\end{align}
where the entries $s_1,\ldots,s_p$ of the vector $\vs \in \R^p$ are independent noise terms, called disturbances.
Causal discovery consists in inferring the parameters $\mB$ of the model, from observations of $\vx$.
Yet, the identifiability of such a model and therefore the uniqueness of the inferred $\mB$ is not straightforward.
In fact, it is well-known that Gaussian noises make the model unidentifiable, in general~\citep{richardson2002identifiability,genin2021identifiability}.
A major advance was that \citet{shimizu2006linear} assumed the noise variables to be
\textit{non-Gaussian} leading to their Linear Non-Gaussian Acyclic Model (LiNGAM), which leads to identifiability, as discussed later.
In the model, we can then interpret $\mP$ as representing a reordering of the observations, such that the permuted entries $\mP \vx$ satisfy
\begin{align}
    \label{eq:causal_model}
    \mP \vx = \mT \mP \vx + \mP \vs
\end{align}
where the causal matrix between the $\mP\vx$ is now $\mT$ and thus strictly lower triangular.
This means any entry $(\mP \vx)_j$ is a weighted sum of previous entries $(\mP \vx)_{<j}$ and noise $(\mP \vs)_{j}$.
Because $(\mP \vx)_j$ does not depend on future entries $(\mP \vx)_{>j}$, we say that $\vx$ follows a causal ordering given by $\mP$.

\paragraph{Relation of LiNGAM to ICA}
The LiNGAM model, or any similar model based on~\eqref{eq:causal_model_original}, can be rewritten as a latent variable model, in particular an Independent Component Analysis (ICA) model \citep{Hyvabook} as
\begin{align}
    \label{eq:ica_model}
    \vx = \mA \vs
\end{align}
where, again, the entries in $\vs$ are independent and non-Gaussian, and the ``mixing" matrix $\mA$ expresses how the data is generated from the latents, and is given by $\mA = (\mI - \mB)^{-1}$, where $\mB$ is a DAG. Now, many methods developed for ICA can be used to estimate the matrix $\mA$, but it is important to take this special structure into account~\citep{shimizu2006linear}.
In particular, any ICA algorithm does not directly return the correct matrix $\mA$ but rather a related matrix where the columns of $\mA$ may appear in an arbitrary order.

\paragraph{Identifiability of LiNGAM} 
\citet{shimizu2006linear} showed that the LiNGAM model is identifiable in terms of the matrix $\mB$, with no indeterminacies unlike in basic ICA. A rigorous re-statement of this result is given in the following theorem which we prove for completeness in Appendix~\ref{app:ssec:lingam}.
\begin{theorem}[Identifiability of LiNGAM]
\label{theorem:lingam}
In the statistical model defined by~\eqref{eq:causal_model_original}, the parameter $\mB$ is identifiable, provided that the entries in $\vs$ are mutually independent, that at most one of them is Gaussian, and that $\mB$ is a DAG.
\end{theorem}
A further question that has received less attention is whether the model is identifiable in terms of the causal ordering $\mP$. In fact, it is not in general: specifically, there may exist many permutation matrices $\mP$ and strictly lower triangular matrices $\mT$ such that the generated data has the same distribution and the generating permutation cannot be identified.
For instance, in the degenerate case $\mB = \mT = \vzero$, any permutation matrix $\mP$ is equally valid and gives the same data distribution. As is well-known, a DAG in general defines only a partial order in the sense that for some pairs of variables, we cannot necessarily say which is ``earlier" and which is ``later". Thus, to make the causal ordering well-defined, we need further assumptions, as will be considered below.\footnote{One would argue that in some cases, $\mP$ is not even well-defined, and thus it cannot be identifiable, and it is not appropriate to use the concept of identiability. We prefer here to confound the concepts in the sense that we talk about identifiability of $\mP$ if it is uniquely defined and can be uniquely recovered from the data.
A more rigorous justification could be developed by assuming a hierarchical data generation process, where the $\mP$ and $\mT$ are generated first, and the $\mB$ is generated based on them. In that case, if the decomposition of $\mB$ is unique, and $\mB$ is identifiable, also $\mP$ is identifiable in the conventional statistical sense.\label{footnote:1}}


\paragraph{Multi-view ICA}
A multi-view version of ICA is of great practical interest.  One might obtain a number of views of the same data that might be, for example, different subjects in a biomedical context, different users in more technological applications, or different measurement systems of the same physical phenomenon. 
A multi-view extension of ICA
can then be defined in various ways. Here, we consider the case where the components are, at least partly, shared over views, while the mixing matrices (as well as optional noise terms) are view-dependent. 

This leads to the definition of Shared ICA \citep{richard2021sharedica,anderson2013multiviewidentifiability}:
\begin{align}
    \label{eq:multiview_ica_model}
    \vx^i = \mA^i (\vs + \vn^i)
\end{align}
where $\vx^i$ are the different views indexed by the view index $i$.
Recently, identifiability conditions have been explored for such multi-view ICA models. 
On the one hand, it is obvious that if the components are non-Gaussian, the Shared ICA model reduces to a standard ICA model when stacking the different views in a single vector, and hence identifiable. But \citet{richard2021sharedica,anderson2013multiviewidentifiability} showed the surprising result that even if more than one component is Gaussian, the model can still be identifiable if the variances of the noises $\vn^i$ are sufficiently diverse.
