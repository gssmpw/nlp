\section{Introduction}

Existing researches have challenged the prevailing assumption that images are best represented as uniform pixel grids given the continuous nature of real visual world. Traditional signal processing methods, such as Discrete Cosine Transform (DCT)~\cite{khayam2003discrete}, which transfers the spatial signal into the frequency domain, have been effectively applied to lossy image compression techniques, \textit{e.g.}, JPEG~\cite{rabbani2002overview}. With the rapid advancement of neural networks, which have demonstrated remarkable efficiency in function approximation~\cite{lecun2015deep}, researchers are increasingly turning to neural representations for a wide range of fitting-based applications. This shift is central to the field of representation learning~\cite{bengio2013representation}. In the realm of image representation, a notable example is the Local Implicit Image Function~\cite{chen2021learning}, which employs Implicit Neural Representations (INRs)~\cite{chen2019learning, sitzmann2020implicit, park2019deepsdf} to map continuous coordinates to their corresponding signals at any resolution. INR-based methods typically use a compact neural network to produce an implicit continuous mapping, preserving intricate image details and opening up new possibilities for applications such as image compression and super-resolution~\cite{dupont2021coin,chen2021learning, ma2022recovering, strumpler2022implicit}.

\begin{figure}[t]
\centering
\includegraphics[width=1.00\columnwidth]{figures/varying_points.pdf}
\caption{\textbf{Comparison of LIG and GaussianImage on large image fitting quality.} GaussianImage performs badly when optimizing a large number of Gaussian points on images of high resolutions, whereas ours consistently delivers quality improvements as the number of Gaussian points increases. The phenomenon is observed in multiple datasets.}
\label{varying_points}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=1.00\textwidth]{figures/teaser.pdf}
\caption{\textbf{LIG is capable of representing large images with high quality.} We show cases including a histopathology image and a satellite image, showing multi-resolution patches with PSNR values displayed at the bottom-right corner of each image.} 
\label{teaser}
\end{figure*}

Despite their potential, most INR-based methods suffer from substantial training memory and slow decoding speed, largely due to their reliance on grid-based features~\cite{sitzmann2020implicit, saragadam2023wire, ramasinghe2022beyond, liu2024finer}. Specifically, the coordinates are of grid-like structures and mapped to neural features for Multi-Layer Perception (MLP) processing. This approach presents two significant drawbacks: 1) the grid-based structures, which grow quadratically, result in dramatically large equivalent batch sizes, bringing substantial or even prohibitive training memory requirements; 2) the decoding process, which necessitates parallel processing of neural networks, despite ongoing advancements in this area, can be slow for large batches. These limitations become particularly critical when dealing with larger target signals, \textit{e.g.,} large images, which is the primary focus of our work. 

3D Gaussian Splatting~\cite{kerbl20233d} (3DGS), designed for 3D scene reconstruction, has emerged as a novel representation known for its high-quality, real-time rendering capabilities. This is largely due to its use of explicit 3D Gaussians and differentiable tile-based rasterization~\cite{lassner2021pulsar}. In an effort to address the aforementioned challenges and explore the potential of Gaussian Splatting (GS) for image representation, GaussianImage~\cite{zhang2024gaussianimage} introduces a 2D Gaussian Splatting representation for images, advocating an image-space tailored rasterization method for efficient training and rendering. Similarly, Image-GS~\cite{zhang2024image} adaptively allocates and progressively optimizes 2D Gaussians for efficient representation learning. These pioneering works have demonstrated the effectiveness of GS for image fitting, achieving comparable quality and higher efficiency than INR-based methods for small images, while maintaining a satisfactory signal-to-noise ratio.

However, existing GS-based fitting methods have yet to demonstrate their potential for higher fidelity and their adaptability to large images. In our work, we delve deeper into the application of 2DGS representation for large images, which naturally require a larger number of Gaussian points compared to smaller images. We introduce \textbf{L}arge \textbf{I}mages are \textbf{G}aussians (\textbf{LIG}). As shown in Fig.~\ref{varying_points}, LIG is capable of fitting large images with an increasing number of Gaussian points, a task where GaussianImage may fall short. We examine the optimization difficulties encountered by GaussianImage when dealing with a large number of Gaussian points and, in response, we make two distinct modifications to overcome these challenges. Firstly, we optimize the Gaussian parameters using a slightly different 2DGS representation, aided by re-implemented CUDA kernels.
Specifically, we directly optimize the covariance matrix without decompositions which are managed in 3DGS and GaussianImage~\cite{kerbl20233d,zhang2024gaussianimage} and maintain semi-definiteness via post-processing. Secondly, we harness the concept of Level of Detail (LOD) from computer graphics, proposing a Level-of-Gaussian approach for hierarchically fitting large images. This shares similarities with research adopting LOD in Neural Field and Gaussian Splatting, including MINER, BungeeNeRF, Octree-GS, and Hierarchical 3D Gaussian~\cite{saragadam2022miner, ren2024octree, xiangli2022bungeenerf, kerbl2024hierarchical}. With the Level-of-Gaussian mechanism, we can allocate a small ratio of points for initializing coarse low-frequency structures, leaving high-frequency details for second-stage fitting with the majority of Gaussian points. Unlike MINER, which is related to the sensitivity of grid features and fixed resolution for input coordinates, we splatter all Gaussian points simultaneously for different Gaussian number configurations, and select the number of levels as 2 for images of any resolution. 

Our designs ease the training of numerous Gaussian points and enable GS-based representation for large images. As demonstrated in Fig.~\ref{teaser}, LIG can perform high-quality fitting for large images, where we select medical images and remote sensing images for illustration, highlighting the potential for applications in telemedicine and satellite communication~\cite{mittermaier2023digital, de2015satellite}. In summary, our main contributions are highlighted as follows:
\begin{itemize}
    \item We are the first to delve into applying GS-based representation for large images, aiming at reconstructing large images with Gaussian points.
    \item We make two designs on 2DGS for images, namely, 2DGS representation, and the utilization of a two-stage Level-of-Gaussian approach, which mitigate the training obstacles of a large number of Gaussians.
    \item We compare our method with baselines on various types of large images, including general visual high-resolution images, high-resolution histopathology images, and satellite images.
\end{itemize}