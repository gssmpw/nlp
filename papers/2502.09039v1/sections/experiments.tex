\section{Experiments}

\subsection{Experimental Setup}
\textbf{Dataset.} 
We assess our method across three diverse datasets, encompassing medical, remote sensing, and general visual tasks. These datasets vary in resolution where we utilize 15 9K histopathology images of human heart sourced from STimage~\cite{chen2024stimage1k4m}, 4 4K satellite images from the Full-resolution Gaofen-2 (FGF2)~\cite{wang2024cross}, and 2K DIV-HR dataset, comprising 100 images, with low-resolution counterparts evaluated in~\cite{zhang2024gaussianimage}.

\noindent \textbf{Evaluation Metrics.} We use three metrics to evaluate our method against GS-based state-of-the-art and INR-based counterparts. We employ PSNR for image quality, which quantify the distortion between reconstructed images and original images. As one merit of LIG is the fewer training memory for large images compared to GaussinImage and INRs, we provide training memory for reference. Benefiting from GS representation, our method achieves high rendering speed and FPS is used for benchmark.

\noindent \textbf{Implementation Details.} Since we present a new 2DGS representation for images, CUDA kernels are incorporated and we build the packages upon gsplat~\cite{ye2023mathematical}. The training steps for $L_0$ and $L_1$ are set to the same, 30,000 steps in our implementation. The learning rate is 0.018 and Adam optimizer is used~\cite{kingma2014adam}.

\subsection{Main Results}

\begin{table*}[ht]
  \centering
\resizebox{\textwidth}{!}{
  \begin{tabular}{lcccccccccc}
    \toprule
     & & \multicolumn{3}{c}{STimage (9K)} &  \multicolumn{3}{c}{FGF2 (4K)} & \multicolumn{3}{c}{DIV-HR (2K)} \\
    \cmidrule(r){3-5} \cmidrule(r){6-8} \cmidrule(r){9-11}
    Method & Reference & PSNR $\uparrow$ & Tr. Mem. (GB) $\downarrow$ & FPS $\uparrow$ & PSNR $\uparrow$ & Tr. Mem. (GB) $\downarrow$ & FPS $\uparrow$ & PSNR $\uparrow$ & Tr. Mem. (GB) $\downarrow$ & FPS $\uparrow$  \\
    \midrule
     \textbf{INR-based} & & & & & & & & & & \\
    \midrule
    SIREN & NeurIPS'20 & - & - & - & - & - &  - & 28.61 & 28.05 & 38.55 \\
    Gauss & ECCV'22 & - & - & - & - & - & - & 25.39 & 38.56 & 25.12 \\
    WIRE & CVPR'23 & - & - & - & - & - & - & 24.42 & 52.40 & 8.92 \\
    FINER & CVPR'24 & 17.74 & 71.27 & 12.81 & 21.91 & 74.82 & 12.35 & 34.42 & 80.01 & 9.84 \\
    \midrule
    \textbf{GS-based} & & & & & & & & & & \\
    \midrule
    GaussianImage + (3.5e7, 1e7, 5e5) & ECCV'24  & 29.86 & 20.47 & 19.86 & 27.50 & 5.39 & \textbf{78.19} & 40.09 & 1.03 & \textbf{745.01} \\
    GaussianImage + (4.5e7, 1.2e7, 7e5) & ECCV'24  & 29.33 & 23.56 & \textbf{18.95} & 27.53 & 5.64 & \textbf{69.24} & 35.12 & 1.09 & \textbf{635.30} \\
    GaussianImage + (5.5e7, 1.4e7, 9e5) & ECCV'24  & 29.28 & 25.89 & \textbf{16.51} & 27.48 & 6.04 & \textbf{67.20} & 29.45 & 1.17 & \textbf{525.73} \\
\rowcolor[rgb]{ .863,  .922,  1}  \textbf{LIG (Ours)} + (3.5e7, 1e7, 5e5) & - & \textbf{37.47} & \textbf{16.67} & \textbf{20.19} & \textbf{51.81} & \textbf{4.21 }& 74.38 & \textbf{44.89} & \textbf{1.01} & 541.84 \\
\rowcolor[rgb]{ .863,  .922,  1}   \textbf{LIG (Ours)} + (4.5e7, 1.2e7, 7e5) & - & \textbf{39.82} & \textbf{17.75} & 17.89 & \textbf{53.90} & \textbf{4.26} & 63.62 & \textbf{49.07} & \textbf{1.02} & 491.37 \\
\rowcolor[rgb]{ .863,  .922,  1}   \textbf{LIG (Ours)} + (5.5e7, 1.4e7, 9e5) & - & \textbf{42.19} & \textbf{20.26} & 15.72 & \textbf{56.05} & \textbf{4.39} & 58.09 & \textbf{52.22} & \textbf{1.05} & 441.76 \\
    \bottomrule
  \end{tabular}
}
  \caption{\textbf{Quantitative results on three datasets.} We report the PSNR, Training Memory, and FPS for all methods. For GS-based methods, we present the number of Gaussian points for each dataset, denoted in the form of tuples. Please note that for INR-based methods, the fitting may be infeasible for large image datasets due to the large memory. ``3.5e7'' denotes $3\times 10^7$.}
  \label{quantitative}
\end{table*}


\begin{figure*}[t]
\centering
\includegraphics[width=1.00\textwidth]{figures/qualitative.pdf}
\caption{\textbf{Qualitative comparison between LIG and GaussianImage on STimage and FGF2 samples.} We show small patches from the rendered images and the GT images. The difference images are shift to 0.5 for visualization.} 
\label{qualitative}
\end{figure*}

The quantitative results on three datasets are reported in Table~\ref{quantitative}.
We mainly compare our method with GaussianImage~\cite{zhang2024gaussianimage} focusing on large images, and also select INR-based methods for baselines. SIREN~\cite{sitzmann2020implicit}, Gauss~\cite{ramasinghe2022beyond}, WIRE~\cite{saragadam2023wire}, and Finers~\cite{liu2024finer} are selected for comparison. We report the quantitative results, including PSNR, Training Memory, and FPS, on three datasets. The FPS results are tested on the same environment. Note that these INRs are based on grid features, which suffer from large training memory requirements for large images since the batch sizes are too large. Therefore, we leave blank for those infeasible experiments. Additionally, while we can use tiny networks for running, the performances can be poor, as seen in the results of FINER where different network sizes are used for different datasets with training memory smaller than 80G. It is clear that compared with GS-based methods, INRs suffer from large training memory and low FPS.

Compared with GaussianImage, our method performs better on image quality, enabling GS for large signal fitting, especially for larger images on 4K and 9K. Regarding training memory, given the same number of Gaussian points, LIG does not propagate all the gradients but optimizes the levels in two stages, therefore reducing the training memory compared with our single level variant. From the Table ~\ref{quantitative}, we can also see that LIG consumes less memory than GaussianImage. Since the training and inference require two levels, the rendering speed can be slower than the GS-based baseline. However, as the additional level $L_0$ comprises fewer points and the final level $L_1$ has a reduced number of Gaussians, the FPS is not necessarily lower. For instance, on 9K images of STimage, with a total of 3.5e7 Gaussians, the FPS achieved is higher compared to GaussianImage. For other LIG results of the same point number, the reduction in FPS is mild considering the quality and training memory requirements. A qualitative comparison between LIG and GaussianImage is illustrated in Fig.~\ref{qualitative}. Note that for the histopathology image, the abundance of rich details may obscure the weaknesses of the baseline. Please refer to the difference images.


\subsection{Ablation Studies}
We present the ablation studies in Table~\ref{ablation}, evaluating the effectiveness of our two key components across different Gaussian point numbers on various datasets. All models are optimized for the same number of iterations. We utilize a variant representation of 2DGS and introduce a Level-of-Gaussian (LOG) mechanism. In this context, "w/o LOG" indicates the use of only the 2DGS variant with optimization performed at a single level, while "w/ LOG" refers to the full LIG implementation. The results clearly demonstrate that both components consistently yield performance improvements as the number of Gaussian points increases.

\begin{table}[t]
  \centering
  \resizebox{.48\textwidth}{!}{
  \begin{tabular}{lccccc}
    \toprule 
    Method & PSNR & & & & \\
    \midrule
    \textbf{STimage (9K)} & 2.5e7 & 3.5e7 & 4.5e7 & 5.5e7 & 6.5e7 \\
    \midrule
    GaussianImage & 31.86 & 29.86 & 29.33 & 29.28 & 29.26 \\
    Ours w/o LOG & 34.01 & 34.02 & 33.45 & 32.68 & 33.27 \\
    Ours w/ \enspace LOG & \textbf{35.03} & \textbf{37.47} & \textbf{39.82} & \textbf{42.19} & \textbf{44.49} \\
    \midrule
    \textbf{FGF2 (4K)} & 6e6 & 8e6 & 1e7 & 1.2e7 & 1.4e7 \\
    \midrule
    GaussianImage & 28.05 & 27.44 & 27.50 & 27.53 & 27.48 \\
    Ours w/o LOG & 47.07 & 48.54 & 49.65 & 50.50 & 51.35 \\
    Ours w/ \enspace LOG & \textbf{47.17} & \textbf{49.74} & \textbf{51.81} & \textbf{53.90} & \textbf{56.05} \\
    \midrule
    \textbf{DIV-HR (2K)} & 5e5 & 6e5 & 7e5 & 8e5 & 9e5 \\
    \midrule
    GaussianImage & 40.09 & 38.13 & 35.12 & 32.00 & 29.45 \\
    Ours w/o LOG & 43.21 & 43.66 & 43.47 & 42.97 & 42.30 \\
    Ours w/ \enspace LOG & \textbf{44.89} & \textbf{47.07} & \textbf{49.07} & \textbf{50.82} & \textbf{52.22} \\
    \bottomrule
  \end{tabular}
  }
  \caption{\textbf{Ablation studies on three datasets.} We evaluate the effectiveness of our two distinct designs compared to GaussianImage. Across various settings of Gaussian points, our designs consistently bring performance improvements.}
  \label{ablation}
\end{table}

\begin{table}[t]
  \centering
  \resizebox{0.48\textwidth}{!}{
  \begin{tabular}{lcccccc}
    \toprule 
   Setting & PSNR & FPS \\
    \midrule
    $|\mathcal{N}_0|, |\mathcal{N}_1| = 0, 30625000$ & 34.12 & \textbf{25.78} \\
    $|\mathcal{N}_0|, |\mathcal{N}_1| = 4375000, 30625000$ & \textbf{37.47} & 20.19 \\
    \midrule
    $|\mathcal{N}_0|, |\mathcal{N}_1| = 0, 39375000$ & 33.81 & \textbf{21.04} \\
    $|\mathcal{N}_0|, |\mathcal{N}_1| = 5625000, 39375000$ & \textbf{39.86} & 17.89 \\
    \midrule
    $|\mathcal{N}_0|, |\mathcal{N}_1| = 0, 48125000$ & 33.22 & \textbf{17.34} \\
    $|\mathcal{N}_0|, |\mathcal{N}_1| = 6875000, 48125000$ & \textbf{42.19} & 15.72 \\
    \bottomrule
  \end{tabular}
}
  \caption{\textbf{Effectiveness of low-frequency initialization for the second level on STimage.} We show comparison results with only using the number of Gaussian points in the second level for training a single level version of LIG.}
 \label{two-stage}
\end{table}

\begin{table}[h]
  \centering
  \resizebox{0.48\textwidth}{!}{
  \begin{tabular}{lcccccc}
    \toprule 
    Metrics / Iterations & 1e5 & 2e5 & 3e5 & 4e5 & 5e5 \\
    \midrule
    \textbf{STimage (9K)} & & & & &  \\
    \midrule
    PSNR (dB) $\uparrow$ & 37.68 & 39.12 & 39.82 & 40.38 & 40.76 \\
    Training Time (s) $\downarrow$ & 1825.68 & 3446.74 & 4777.65 & 6523.39 & 8028.46\\
    \midrule
    \textbf{FGF2 (4K)} & & & & &  \\
    \midrule
    PSNR (dB) $\uparrow$  & 53.22 & 55.11 & 56.05 & 56.70 & 57.20 \\
    Training Time (s) $\downarrow$ & 670.15 & 1306.56 & 1926.32 & 2562.91 & 3186.11 \\
    \bottomrule
  \end{tabular}
}
  \caption{\textbf{Performances with different training iterations.} The iteration number can be used to strike a balance between image quality and training time. The number for Gaussian points are (4.5e7, 1.4e7) for two datasets.}
  \label{iterations}
\end{table}

\begin{table}[h]
  \centering
  \resizebox{0.35\textwidth}{!}{
  \begin{tabular}{lccc}
    \toprule 
   Method & PSNR & Parameter Number & FPS \\
    \midrule
    LIG & 41.31 & 16000000 & 168.25\\
    NeuRBF & 44.69 & 17631486 & 156.38\\
    \bottomrule
  \end{tabular}
}
  \caption{\textbf{Comparison between LIG and NeuRBF on FGF2.} Given the similar total parameters for optimization, NeuRBF performs higher in PSNR while remains high FPS.}
 \label{neurbf}
\end{table}

\subsection{Further Analysis}
\textbf{Effectiveness of low-frequency initialization.} Our two-stage LOG approach benefits from easier training for large images due to the low-frequency initialization at the first level. In Table~\ref{two-stage}, we compare results with different point assignments to demonstrate the effectiveness of this initialization. We allocate additional points to the first level in the single-stage setting, with the only difference being the training target of the second level, denoted as $L_1$. This initialization results in higher PSNR, indicating that the training of the final target is facilitated. The two-stage process incurs extra inference time, leading to a slight drop in FPS.

\noindent \textbf{Trade-off between quality and training time.} One key factor affecting training time is the number of iterations. In Table~\ref{iterations}, we present results for different training iterations. The linearly increasing training time leads to higher fitting accuracy, with 3e5 iterations representing a compromise point adopted in our experiments.

\noindent \textbf{Comparison with NeuRBF.} We compare our method with the state-of-the-art INRs method, NeuRBF~\cite{chen2023neurbf}. In Table~\ref{neurbf}, we present performance metrics on the FGF2 dataset to illustrate the differences between our method and NeuRBF. We use 2e6 Gaussian points, comprising 1.6e7 parameters, to maintain a similar total number of parameters as NeuRBF for this dataset. It is observed that on 4K data, NeuRBF achieves higher PSNR while maintaining a similar FPS. With its radial bases and the solid foundation of Instant-NGP~\cite{muller2022instant}, NeuRBF advances INRs for large image fitting. While it remains unclear whether recent efficient techniques in INRs, such as hash coding, can be applied to 2DGS, existing and ongoing advancements in 3DGS may positively impact the development of novel 2DGS-based image representations. Given that GS-based image representation is still under-explored, we consider it to be in a very early stage.