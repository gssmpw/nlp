\section{Related Works}

\noindent\textbf{Implicit Neural Representations.}
Since early works focused on representing the signed distance field for 3D shapes~\cite{chen2019learning, park2019deepsdf, xu2019disn, michalkiewicz2019implicit}, Implicit Neural Representations (INRs) have been applied to a variety of applications involving different types of representations, including 3D scenes~\cite{mildenhall2021nerf, barron2021mip}, images~\cite{saragadam2022miner, dupont2021coin, strumpler2022implicit, chen2021learning}, and videos~\cite{chen2021nerv, chen2023hnerv, li2022nerv}. A notable example is NeRF~\cite{mildenhall2021nerf}, which uses a Multilayer Perceptron (MLP) to represent geometry and view-dependent appearance, sparking a surge of interest in 3D vision and graphics. Most INRs use grid-based input processing for spatial signals and focus more on activation functions like sinusoids~\cite{sitzmann2020implicit}, Gabor wavelets~\cite{saragadam2023wire}, and variable-periodic activation functions~\cite{liu2024finer} to capture high-quality details for fitting. However, these methods suffer from inefficient training and inference, making them unsuitable for high-resolution images. To handle large-scale signals, multi-resolution signal representations that rely on efficient feature querying or hierarchical processing have been widely adopted~\cite{muller2022instant, saragadam2022miner, martel2021acorn, chen2023neurbf}. Among them,  while hierarchical architectures, such as MINER~\cite{saragadam2022miner}, can facilitate large image fitting at the cost of sequential training, they do not address the fundamental issues associated with the grid-based nature of INRs, leading to prolonged training times and slow decoding speeds. Recent attempts to improve INRs have included the use of multi-resolution hash encoding or radial basis functions~\cite{muller2022instant, chen2023neurbf}, which have achieved high accuracy with fewer parameters. These advancements have significantly improved INRs, making them the current leading approaches.

\noindent\textbf{3D Gaussian Splatting.} 
3D Gaussian Splatting (3DGS)~\cite{kerbl20233d} becomes a trend in the computer graphics and computer vision communities due to its ability to compactly represent complex 3D scenes while enabling high-speed rendering. The scene is modeled as 3D Gaussian primitives, each of which is defined by position, scale, rotation, opacity, and appearance attributes. The parameters of the Gaussians are optimized to align observations via differentiable rendering.
3DGS shows remarkable potential across a wide range of downstream tasks. These include novel view synthesis and scene reconstruction~\cite{kerbl20233d,kerbl2024hierarchical,yu2024mip,huang20242d,zhu2024deformable,zhao2024hfgs,liu2024endogaussian,li2024endosparse}, 3D reconstruction and generation~\cite{tang2024lgm, xu2024grm, szymanowicz2024splatter, tang2023dreamgaussian, liu2024humangaussian, chen2024generalizable}, and applications in robotics~\cite{yugay2023gaussian, lu2024manigaussian,matsuki2024gaussian}.
Recent advancements in 3DGS further incorporate Level-of-Detail (LOD) techniques to enhance rendering efficiency and enable adaptive scene representation~\cite{ren2024octree,kerbl2024hierarchical,yan2024multi}, particularly crucial for large-scale scene reconstruction which requires visual quality with real-time rendering.

\noindent\textbf{Gaussian Splatting Based Image Representation.} 
Recently, GaussianImage~\cite{zhang2024gaussianimage} is the first to adapt 3DGS for image representations. Specifically, it adapted Gaussian points to image spaces with fewer characterizing parameters and employed alpha blending to merge color and opacity attributes. After per-sample image fitting, the attributes can be compressed using quantization-aware fine-tuning. As a result, GaussianImage can represent images with 2DGS and compress them while maintaining high quality. Image-GS~\cite{zhang2024image} also utilizes the eight parameters for 2D Gaussian points and fits a target image by adaptively allocating and progressively optimizing a set of 2D Gaussians. GaussianSR~\cite{hu2024gaussiansr}, assigns a learnable Gaussian kernel to each pixel for super-resolution.
Another related work, Splatter Image~\cite{szymanowicz2024splatter}, embeds Gaussian attributes at the image level, achieving ultra-efficient 3D reconstruction. In the field of image fitting, despite their groundbreaking success, it remains unclear whether GS can be used to fit large images at a quality that can compete with INRs. Given the nature of large fitting targets, it is necessary to allow for more Gaussian points to be optimized simultaneously. We demonstrate that GaussianImage falls short in this regard due to difficulties in optimizing their representations and the lack of multi-resolution mechanisms for capturing high-frequency information.