%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{xspace}
\newcommand{\code}[0]{\url{https://github.com/BlueWhaleLab/DCScore}}
\newcommand{\ours}[0]{\texttt{DCScore}\xspace}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{titlesec}
\definecolor{Gray}{rgb}{0.93,0.93,0.93}
\newcolumntype{g}{>{\columncolor{Gray}}c}
\usepackage{makecell}
% \usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage{float}

% \newcommand{\yb}[1]{\textcolor{blue}{YB: #1}}
% \definecolor{commentcolor}{RGB}{110,154,155} 
% \newcommand{\PyComment}[1]{\ttfamily\textcolor{commentcolor}{\# #1}}  % add a "#" before the input text "#1"
% \newcommand{\PyCode}[1]{\ttfamily\textcolor{black}{#1}} % \ttfamily is the code font
% \newcommand{\pink}[1]{\textcolor{pink}{#1}}
% \usepackage{subfig}
\usepackage[utf8]{inputenc}
\usepackage{ulem}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{titletoc}


% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Preprint}

\begin{document}

% A diversity measurement approach for synthetic datasets
\twocolumn[
\icmltitle{Measuring Diversity in Synthetic Datasets}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{internship}{†}
\icmlsetsymbol{Corresp}{*}

\begin{icmlauthorlist}
\icmlauthor{Yuchang Zhu}{internship,sysucs}
\icmlauthor{Huizhe Zhang}{sysucs}
\icmlauthor{Bingzhe Wu}{tx}
\icmlauthor{Jintang Li}{sysuse}
\icmlauthor{Zibin Zheng}{sysuse}
\icmlauthor{Peilin Zhao}{tx}
\icmlauthor{Liang Chen}{Corresp,sysucs}
%\icmlauthor{}{sch}
\icmlauthor{Yatao Bian}{Corresp,tx}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}


\icmlaffiliation{sysucs}{School of Computer Science
and Engineering, Sun Yat-sen University, Guangzhou, China}
\icmlaffiliation{sysuse}{School of Software Engineering,
Sun Yat-sen University, Zhuhai, China}
\icmlaffiliation{tx}{Tencent AI Lab, Shenzhen, China}

\icmlcorrespondingauthor{Yatao Bian}{yatao.bian@gmail.com}
\icmlcorrespondingauthor{Liang Chen}{chenliang6@mail.sysu.edu.cn}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\internship \correspondingauthor} % otherwise use the standard text.

\begin{abstract}
 Large language models (LLMs) are widely adopted to generate synthetic datasets for various natural language processing (NLP) tasks, such as text classification and summarization. However, accurately measuring the diversity of these synthetic datasets—an aspect crucial for robust model performance—remains a significant challenge. In this paper, we introduce \ours, a novel method for measuring synthetic dataset diversity from a classification perspective. Specifically, \ours formulates diversity evaluation as a sample classification task, leveraging mutual relationships among samples. We further provide theoretical verification of the diversity-related axioms satisfied by \ours, highlighting its role as a principled diversity evaluation method. Experimental results on synthetic datasets reveal that \ours enjoys a stronger correlation with multiple diversity pseudo-truths of evaluated datasets, underscoring its effectiveness. Moreover, both empirical and theoretical evidence demonstrate that \ours substantially reduces computational costs compared to existing approaches. 
 Code is available at: \code.
\end{abstract}

\section{Introduction}
% background
Large language models (LLMs) have shown exceptional performance across a range of fields, such as chatbots~\citep{achiam2023gpt}, computer programming~\citep{gu2023llm}, and reasoning~\citep{yuan2024advancing}. Inspired by their remarkable capacities, some research~\citep{ye2022zerogen,abdullin2024synthetic,ding2024data} employs LLMs as dataset generators to mitigate the shortage of training data. Although generated data facilitates model optimization, recent studies~\citep{yu2024large,lee2023beyond} suggest that a lack of diversity within the dataset—measured by the variation between samples~\citep{long2024llms}—may lead to performance degradation in some scenarios. 
Although previous studies~\citep{yu2024large,wang2022self} leverage well-designed generation strategies to create highly diverse synthetic datasets, a crucial factor for measuring the diversity of these datasets has been overlooked. 
% a key element measuring the diversity of these datasets remains neglected. 
A principled diversity evaluation metric serves not only to guide LLM generators in creating more diverse data but also extends its utility to data selection~\citep{cao2023instruction}, quantifying augmentation performance~\citep{yang2024investigating}, and assessing mode collapse~\citep{dan2023vendi}. Thus, a diversity evaluation method for synthetic datasets is becoming increasingly important.

% A principled diversity evaluation metric can provide feedback to LLM generators and guide them to generate data with higher diversity. Beyond evaluating the diversity of synthetic datasets, these methods can also be used for data selection~\citep{cao2023instruction}, quantifying augmentation performance~\citep{yang2024investigating,gontijo2020affinity}, and assessing mode collapse~\citep{dan2023vendi}.

% related work summarization and problem
Since the diversity evaluation of synthetic datasets remains under-explored, a natural solution is to directly employ diversity evaluation methods from the fields of natural language processing (NLP)~\citep{khurana2023natural} and machine learning (ML)~\citep{jordan2015machine}. Specifically, efforts to measure diversity within these domains can be summarized into three categories: \textit{N-gram-based method}~\citep{zhu2018texygen,mishra2020dqi}, \textit{Reference-based method}~\citep{heusel2017gans,cifka2018eval}, and \textit{Transformation-based method}~\citep{du2019boosting,zhang2024improving}. The n-gram-based method assesses diversity through n-gram statistics, e.g., the distinct n-grams metric~\citep{li2015diversity} calculates the proportion of unique n-grams out of the total number of n-grams. This approach primarily focuses on the form differences of evaluated texts, often overlooking semantic aspects and offering limited flexibility for evaluators. To align the diversity criteria with human judgment, the reference-based method has emerged as a promising alternative. This approach employs a reference distribution or data as an approximation of human judgment and calculates the similarity between the evaluated data and the reference data~\citep{holtzman2019curious}. 
% However, the process of collecting reference data can be both time-consuming and potentially bias-inducing.
However, collecting reference data can be both time-consuming and may introduce potential biases.


Drawing inspiration from deep representation learning~\citep{butepage2017deep,zhang2021bootstrapped}, the transformation-based method evaluates diversity by first mapping the data into the representation space and then performing diversity summarization~\citep{tevet2020evaluating}. For data mapping, various embedding functions can be employed to facilitate the transformation, with a popular approach being the use of sentence transformers such as Sentence-Bert~\citep{reimers2019sentence} and SimCSE~\citep{gao2021simcse}. Owing to the versatility of embedding functions, transformation-based methods can simultaneously consider extensive aspects, such as semantics and style, to encode data representations, thereby providing a more comprehensive evaluation of diversity. However, this type of method is hindered by suboptimal computational efficiency caused by the high-complexity diversity summarization, such as eigenvalue computation~\cite{dan2023vendi}.
% 简单地计算相似度只是考虑从两两样本之间的关系，没有考虑其他样本对计算结果的影响，因此难以应用到数据集的整体diversity评估当中。

% 公理满足；复杂度 n方；

% challenge of dataset evaluation
In a nutshell, existing diversity evaluation methods in NLP and ML suffer from inherent limitations in the synthetic dataset evaluation scenario. To effectively evaluate the diversity of synthetic datasets, the following challenges must be tackled: (1) \textit{Holistic Analysis}. The diversity evaluation of synthetic datasets is a holistic analysis task, necessitating consideration of the impact of each sample on the final evaluation results. 
% If a new sample is added, the diversity evaluation of the newly augmented dataset should completely account for the relationship between each original sample and the new one.
(2) \textit{Axiomatic Requirements}. Prior research has suggested several axioms that diversity metrics should ideally satisfy. To ensure reasonable evaluation, a well-established diversity evaluation should exhibit properties corresponding to these axioms. (3) \textit{Lower Computational Costs}. As a growing volume of synthetic datasets, a diversity evaluation method with lower computational costs is highly desirable to ensure data quality and model performance.

% our work and contribution
To sum up, the essence of diversity is associated with the identification of differences between samples, and the ability to distinguish these differences is a key element in the classification process~\citep{quine1969ontological}. Motivated by this observation, we propose a synthetic dataset diversity evaluation method from a classification perspective, namely, \ours. Notably, \ours tackles the three aforementioned challenges. Firstly, \ours treats the evaluation of each sample in the synthetic dataset as a distinct classification task, providing a holistic analysis. Secondly, theoretical verification demonstrates that \ours satisfies four axioms outlined in~\cite{leinster2012measuring}, including effective number, identical samples, symmetry, and monotonicity. Lastly, both empirical and theoretical evidence indicates that \ours incurs lower computational costs compared to existing methods.


Our contributions can be summarized as follows:
\begin{itemize}
  \item We propose \ours, a classification-based diversity evaluation method for synthetic datasets. The core idea behind \ours is to treat diversity evaluation as a sample classification task, enabling the capture of mutual relationships among samples.
  \item We theoretically validate that \ours adheres to several intuitive axioms suggested by~\cite{leinster2012measuring}, evidencing the superiority of \ours. 
  \item Extensive experiments show that \ours exhibits a stronger correlation with diversity parameters and human judgment compared to baseline metrics. We also perform a computational cost experiment to confirm the lower computational expense of \ours. 
\end{itemize}

\section{Related Work}
We give a brief literature review of diversity evaluation methods. Moreover, limited by the space, further related works on LLM dataset generators can be found in Appendix~\ref{apd:add_related_work}. 
% prompt-guided
% 1.Identifying Citizen-Related Issues from Social Media Using LLM-Based Data Augmentation
% 2.Mini-DA: Improving Your Model Performance through Minimal Data Augmentation using LLM （选取训练时分类错误的样本增强）
% 3.Data Augmentation for Text-based Person Retrieval Using Large Language Models
% 4.LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition （multi-level）
% 5.Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations
% 6.Zero-Shot Stance Detection using Contextual Data Generation with LLMs
% 7.AugGPT: Leveraging ChatGPT for Text Data Augmentation
% 8.Is GPT-3 a Good Data Annotator?
% 9.GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation
% 10.Large Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching
% 11.Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dataset-guided
% 1.MixSumm: Topic-based Data Augmentation using LLMs for Low-resource Extractive Text Summarization
% 2.ZEROGEN: Efficient Zero-shot Learning via Dataset Generation
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%instruct-guided
% 1.Can LLMs Augment Low-Resource Reading Comprehension Datasets? Opportunities and Challenges
% 2.Generative Data Augmentation using LLMs improves Distributional Robustness in Question Answering
% 3.Empowering Large Language Models for Textual Data Augmentation
% 4.Learning Preference Model for LLMs via Automatic Preference Data Generation
% 5.Mixture of Soft Prompts for Controllable Data Generation
% 6.SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions
% 7.TarGEN: Targeted Data Generation with Large Language Models
% 8.A Synthetic Data Approach for Domain Generalization of NLI Models
% 9.CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP

%I2O:
% 1.CHATGPT OUTPERFORMS CROWD-WORKERS FOR TEXT-ANNOTATION TASKS
% 2.GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation

% Generally speaking, a dataset is a set of input-output samples, where a sample consists of a input text and a output text. According to the generation task, existing methods using LLMs as a dataset generator can be summarized into three categories: InputToOutput (\textit{I2O}), OutputToInput (\textit{O2I}), and SeedToSample (\textit{S2S}). 

% Specifically, I2O methods take the input text (e.g., question) as the input of LLMs generator and obtain the output text (e.g., answer) corresponding to the input text. In text classification, I2O can be referred to as label annotation, which is the most popular way for LLMs to be used as the dataset generator. For example,~\citep{ding2022gpt} designs task-specific prompts to annotate unlabeled data. To further exert the capacity of LLMs, ZeroGen~\citep{ye2022zerogen} generates a training set using LLMs and then trains a task-specific classifier to annotate data labels. AugGPT~\citep{dai2023auggpt} has the same framework as ZeroGen but given samples and their labels in the training set generation stage. conversely, O2I methods aim to generate the input text, e.g., leveraging LLMs to generate diverse input texts related to the given label. For example, xxx. In most cases, we always have a portion of samples with labels, which is known as seed samples. In this regard, given several seed samples as example, S2S methods directly generate input-output samples, which is employed to generate datasets for text classification, story completion and so on. Specifically, xxx. 

% s2s:
% 1.TarGEN: Targeted Data Generation with Large Language Models
% 2.SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions
% 3.A Synthetic Data Approach for Domain Generalization of NLI Models
% 4.CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP

%其他：Empowering Large Language Models for Textual Data Augmentation 

\subsection{Diversity Evaluation Methods}
\textbf{N-gram-based Methods.} With the development of LLMs as dataset generators, the diversity evaluation of synthetic datasets has become a challenging task and remains under-explored in recent evaluation studies~\citep{liang2022holistic,AlpacaEval}. The most comparable diversity evaluation research can be traced back to studies in NLP and ML, which can be summarized into the n-gram-based method~\citep{mishra2020dqi}, reference-based method~\citep{heusel2017gans}, and transformation-based method~\citep{lai2020diversity}. The n-gram-based method is the most popular lexical diversity evaluation method, leveraging n-grams to capture differences in sentence form~\citep{yu2024large}. Commonly used n-gram-based diversity metrics include distinct n-grams (\textit{distinct-n})~\citep{song2024scaling}, self-BLEU~\citep{shu2019generating}, and ROUGE-L~\citep{wang2022self,padmakumar2023does}. However, this method has limitations, as it overlooks differences in other aspects such as semantics and style.

\textbf{Reference-based Methods.} Diversity evaluation is inherently subjective, leading to a reliance on human judgment. Consequently, the reference-based method evaluates diversity by comparing the distribution of the evaluated data to that of a reference dataset~\citep{heusel2017gans}. MAUVE~\citep{pillutla2021mauve} exemplifies this idea by employing a divergence-based metric to capture correlations with human judgment. Regarding the natural language inference (NLI) training set as the reference dataset,~\citep{stasaski2022semantic} first trains an NLI model to infer the relationship between pairs of generated texts and then calculates diversity based on these inference results. Due to the challenges in collecting reference datasets, a recent study~\citep{le2024exploring} proposes evaluating diversity through precision and recall. Despite these advancements, the reference-based method remains significantly constrained because of the need for reference datasets.

\textbf{Transformation-based Methods.} The transformation-based~\citep{lee2023beyond} method leverages well-designed models to generate representations of the evaluated data. Then, the diversity of these representations is summarized using techniques such as eigenvalue computation~\citep{dan2023vendi} and clustering~\citep{du2019boosting}. 
Owing to the superior performance of representation learning, this method considers various aspects of the evaluated data, including semantics, form, and style, offering greater flexibility compared to the other two methods. However, its dependence on high-complexity summarization techniques, such as eigenvalue computation, limits its scalability for comprehensive evaluation tasks, such as evaluating the diversity of synthetic datasets. 

In summary, existing methods primarily focus on NLP and ML fields and are challenging to apply directly to overall dataset diversity evaluation. Different from the above-mentioned studies, our work is dedicated to the holistic diversity evaluation of synthetic datasets. Additionally, to ensure flexible evaluation, our work aims to evaluate diversity-sensitive components that impact the performance of trained models in terms of diversity.

% \begin{CJK}{UTF8}{gbsn}
% \textbf{场景设定与我们相似的论文}
% \begin{itemize}
%     \item Diversity, Density, and Homogeneity: Quantitative Characteristic Metrics for Text Collections
%     \item Beyond Scale: The Diversity Coefficient as a Data Quality Metric for Variability in Natural Language Data
%     \item Exploring Precision and Recall to assess the quality and diversity of LLMs
%     \item Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification Prompting
% \end{itemize}
% \end{CJK}

% \begin{CJK}{UTF8}{gbsn}
% \textbf{可参考的一些论文}
% \begin{itemize}
%   \item Evaluating the Evaluation of Diversity in Natural Language Generation：多样性指标评估的论文（实验设置可参考）
%   \item Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning：提升LLMs生成内容多样性的方法，文中提到了三种diversity测量方法，可以分别归类到n-gram和transformation方法
%   \item Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation：研究文本多样性激励措施对于生成数据集训练出来的模型性能的影响(确定具体场景可参考)
% \end{itemize}
% \end{CJK}

% \begin{CJK}{UTF8}{gbsn}
% \textbf{关于LLMs作为dataset generator的文章：}
% \begin{itemize}
% \item 1. Synthetic dialogue dataset generation using llm agents
% \item 2. Zerogen: Efficient zero-shot learning via dataset generation
% \item 3. \textbf{TarGEN: Targeted Data Generation with Large Language Models}
% \item 4. Data augmentation using llms: Data perspectives, learning paradigms and challenges
% \item 5. Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions
% \end{itemize}
% \end{CJK}


% \subsubsection{N-gram-based method}
% \begin{CJK}{UTF8}{gbsn}
% \begin{itemize}
%   \item DQI: Measuring Data Quality in NLP
%   \item Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias
%   \item Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment
%   \item Does Writing with Language Models Reduce Content Diversity
%   \item A Diversity-Promoting Objective Function for Neural Conversation Models
%   \item SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions
% \end{itemize}
% \end{CJK}


% \subsubsection{Reference-based method}
% \begin{CJK}{UTF8}{gbsn}
% \begin{itemize}
%   \item Semantic Diversity in Dialogue with Natural Language Inference (训练NLI模型的数据集是否可以看做Reference或者NLI分类器本身是否就可以看做是Reference)
%   \item Exploring Precision and Recall to assess the quality and diversity of LLMs
%   \item GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium
% \end{itemize}
% \end{CJK}


% \subsubsection{Transformation-based method}
% \begin{CJK}{UTF8}{gbsn}
% \begin{itemize}
%   \item Diversity, Density, and Homogeneity:
% Quantitative Characteristic Metrics for Text Collections
%   \item Boosting Dialog Response Generation
%   \item Beyond Scale: The Diversity Coefficient as a Data Quality Metric for Variability in Natural Language Data
%   \item Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries
%   \item Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification Prompting
% \end{itemize}
% \end{CJK}

\section{Prelinimaries}

\subsection{LLM as a Dataset Generator}
Since the exceptional performance of LLMs, previous works~\citep{dai2023auggpt,yoo2021gpt3mix} employ LLMs as a dataset generator or for data augmentation purposes. LLMs significantly reduce the cost of label annotation and data collection~\citep{tan2024large}, and in several tasks, even outperform human annotators~\citep{gilardi2023chatgpt}. While some studies attempt to use LLMs to generate datasets from scratch, it is a challenging task for LLMs. In most cases, a pre-trained LLM, denoted as $\mathcal{M}$, takes the data $\mathcal{D}_{sup}$ to be augmented and the generation task $T$ as input, and outputs the augmented dataset $\mathcal{D}$. Formally, this process can be formulated as follows:
\begin{equation}
    \mathcal{D}\leftarrow \mathcal{M}(T, \mathcal{D}_{sup})
\end{equation}
where $T$ can be texts describing the generation task, such as annotation. $\mathcal{D}_{sup}$, which comprises a small number of seed samples or unlabeled inputs, serves as supplementary materials to facilitate data augmentation. For example, we want LLMs to perform an annotation task for sentiment labeling, such as determining whether the sentiment is positive or negative. If we assume $\mathcal{D}_{sup}$ to be ``\textit{It's a boring movie.}'', the description of $T$ could be ``\textit{The sentiment of the movie review is}''. $\mathcal{D}=\{\mathcal{T}_{i}\}_{i=1}^{n}=\{({x}_{i},{y}_{i})\}_{i=1}^{n}$ is the generated dataset with $n$ samples, where $\mathcal{T}_{i}=({x}_{i},{y}_{i}), {x}_{i}$, and ${y}_{i}$ are the input-output sample, the input text, and the output text, respectively. Let $T_{\mathcal{D}}$ denote the downstream task of $\mathcal{D}$, when $T_{\mathcal{D}}$ is the question-answering task, ${x}_{i}$ and ${y}_{i}$ represent the question and answer, respectively. 

It is worth noting that not all components in $\mathcal{D}$ are generated by $\mathcal{M}$, which is related to the category of $\mathcal{D}_{sup}$. As shown in Table~\ref{tab:data_sup}, $\mathcal{D}_{sup}$ can be divided into three categories, namely input text, output text, and seed samples. In Table~\ref{tab:data_sup}, ``$\rightarrow$'' and $\mathcal{T}_{seed}$ represent the direction of generation and seed samples, respectively. For example, ``$x_{i} \rightarrow y_{i}$'' signifies that, given input text $x_{i}$ denoted as $\mathcal{D}_{sup}$, $\mathcal{M}$ processes $\mathcal{D}_{sup}$ and $T$, generating the output text $y_{i}$.

\begin{table}[!t]
    \centering
    \caption{Categories of $\mathcal{D}_{sup}$. In the column of ``Examples'', texts belonging to $\mathcal{D}_{sup}$ are highlighted in \colorbox{gray!30}{gray}, while texts associated with $T$ are marked using an \underline{underline}.}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{ccp{9cm}}
        \toprule
        Generations                                                        & $\mathcal{D}_{sup}$                                                 & Examples \\
        \midrule
        $\{x_{i}\} \rightarrow \{y_{i}\}$      & $\{x_{i}\}$                               &  \makecell[l]{\textbf{Question:} \colorbox{gray!30}{It’s a boring movie.} \underline{The sentiment of the movie}\\ \underline{review is}\\
        \textbf{Answer:} Negative.}       \\ \cmidrule(l){3-3}
        $\{y_{i}\} \rightarrow \{x_{i}\}$              & $\{y_{i}\}$                               &   \makecell[l]{\textbf{Question:} \underline{The movie review in} \colorbox{gray!30}{positive} \underline{sentiment is} \\
        \textbf{Answer:} Good film!}       \\ \cmidrule(l){3-3}
        $\{\mathcal{T}_{seed}\} \rightarrow \{\mathcal{T}_{i}\}$             & $\{\mathcal{T}_{seed}\}$                  &   
        \makecell[l]{\textbf{Question:} \underline{Following are examples of movie review and their} \\ \underline{sentiment labels. Generate samples according to these examples.} \\ \colorbox{gray!30}{Example 1: A boring movie. (Negative);} \\ \colorbox{gray!30}{Example 2: Oh, wonderful movie! (Positive).} \\
        \textbf{Answer:} A meaningful movie. (Positive)}\\
        \bottomrule
        \end{tabular}
    }
    \label{tab:data_sup}
\end{table}

\subsection{Problem Formulation}
\label{subsec:problem}
 % According to generation scenarios in Table~\ref{tab:data_sup}, we observe that in certain downstream tasks, some components of samples in the synthetic dataset are not critical in terms of their diversity and do not impact the performance of the trained models. Conversely, some components of samples in the synthetic dataset significantly affect model performance, which we refer to as diversity-sensitive components $\mathcal{T}_{i}$.

The diversity evaluation of the dataset is a sample richness evaluation problem. Based on the generation scenarios presented in Table~\ref{tab:data_sup}, we find that in certain downstream tasks, the diversity of some components in the synthetic dataset does not influence the performance of the trained models. Conversely, the diversity of other components significantly impacts model performance. We refer to these components, whose diversity influences performance, as \textit{diversity-sensitive components}, denoted as $\mathcal{T}_{i}$. For example, the input text $x_{i}$ in sentiment classification tasks is the diversity-sensitive component. Conversely, the output text $y_{i}$, which represents the sentiment label of the sample and is typically a numerical label (e.g., 0 or 1), does not influence model performance in terms of diversity. Thus, the output text cannot be considered as the diversity-sensitive component. It should be underscored that diversity-sensitive components vary across downstream tasks. The diversity evaluation of synthetic datasets can be transformed into the diversity evaluation of diversity-sensitive components. 

% For example, in a sentiment classification task, the input text $x_{i}$ is the diversity-sensitive component, whereas in a story completion task, the output text $y_{i}$ is identified as the diversity-sensitive component.

Given a synthetic dataset $\mathcal{D}=\{\mathcal{T}_{i}\}_{i=1}^{n}$, we define $\{\mathcal{\tilde{T}}_{i}\}_{i=1}^{n}$ as a collection of diversity-sensitive components. The problem of diversity evaluation of $\mathcal{D}$ can be defined as follows: 
\begin{equation}
\label{eq:problem}
    \operatorname{Diversity Score}\leftarrow \operatorname{Eval}(\{\mathcal{\tilde{T}}_{i}\}_{i=1}^{n})
\end{equation}
where $\operatorname{Eval}$ is the diversity evaluation function, which takes $\{\mathcal{\tilde{T}}_{i}\}_{i=1}^{n}$ as input and outputs the diversity score $\operatorname{DiversityScore}$ of $\mathcal{D}$.
\section{Present Work}
In this section, we first introduce our proposed method \ours from a classification perspective. Then, we present the properties of \ours followed by theoretical proofs. Finally, we provide a detailed complexity analysis of \ours and the transformation-based counterpart.

% semantic entropy的讨论  uncertainy （先不提）

\begin{figure}[!tb]
    \centering
    \includegraphics[width=\linewidth]{./images/overview.pdf}
    \caption{Illustration of the computation of \ours. \ours consists of three stages: text representation, pairwise similarity, and diversity summarization.}
    \label{fig:overview}
\end{figure}

\subsection{\ours: Measuring  Diversity from a Classification Perspective}
\label{subsec:dcscore}
Due to the intrinsic nature of measuring sample differences in diversity evaluation, it is natural to evaluate diversity as a classification task. Consequently, we propose \ours, which formulates diversity evaluation as a sample classification task. Specifically, the difference between samples can be measured through a $n$-classification task, where evaluating $n$ sample datasets involves $n$ $n$-classification tasks, with each sample corresponding to a distinct category. As shown in Figure~\ref{fig:overview}, \ours consists of three stages: text representation, pairwise similarity, and diversity summarization. According to the problem formulation in section~\ref{subsec:problem}, \ours outputs the diversity of synthetic datasets by evaluating diversity-sensitive components. 

% for the $n$-classification task, where $f$ takes $\mathcal{\tilde{T}}_{i}$ as input and predicts its class label $\hat{c}$. This classical $n$-classification task can be formally formulated as follows:

Let $\mathcal{D}=\{\mathcal{T}_{i}\}_{i=1}^{n}=\{({x}_{i},{y}_{i})\}_{i=1}^{n}$ denote a synthetic dataset comprising $n$ input-output samples, and $\{\mathcal{\tilde{T}}_{i}\}_{i=1}^{n}$ represents the diversity-sensitive components. \ours adheres to the paradigm of the transformation-based method to evaluate the diversity of $\mathcal{D}$. Specifically, given $\mathcal{\tilde{T}}_{i}$, \ours first applies an embedding function $\Phi$ to extract the sample representation $\textbf{h}_{i}=\Phi(\mathcal{\tilde{T}}_{i})$. For all samples in $\mathcal{D}$, we obtain the sample representation matrix $\textbf{H}\in \mathbb{R}^{n\times d}$ across all samples, where $d$ denotes the dimension of sample representations. Subsequently, \ours utilizes a kernel function $\operatorname{Kernel}$ to calculate a kernel matrix $\textbf{K}$, where $\textbf{K}\in \mathbb{R}^{n\times n}$ and entry $\textbf{K}[i,j]$ represents similarity between $\mathcal{\tilde{T}}_{i}$ and $\mathcal{\tilde{T}}_{j}$. From a classification perspective, $\textbf{K}[i,j]$ can be considered as the logit of $\mathcal{\tilde{T}}_{i}$ being classified into category $c_{j}$. where $c_{j}$ corresponds to $\mathcal{\tilde{T}}_{j}$. Formally, the aforementioned process can be formulated as follows:
\begin{equation}
\label{eq:rep_sim}
    \textbf{H}=\Phi(\{\mathcal{\tilde{T}}_{i}\}_{i=1}^{n}), \\
    \textbf{K}=\operatorname{Kernel}(\textbf{H}),
\end{equation}
where $\operatorname{Kernel}(\cdot)$ calculates pairwise similarity, with viable options including inner product and RBF kernel. For $\Phi$, a more expressive embedding function can be employed, such as one trained using a well-designed framework like Sentence-Bert~\citep{reimers2019sentence}.

Based on the kernel matrix $\textbf{K}$, \ours leverages a classification function with $\textbf{K}$, denoted as $f_{\textbf{K}}$, to compute the classification probability matrix $\textbf{P}\in \mathbb{R}^{n\times n}$. There are several potential choices for $f_{\textbf{K}}$, among which a natural option is the Softmax function. For $\mathcal{\tilde{T}}_{i}$, the probability that $\mathcal{\tilde{T}}_{i}$ is classified as category $c_{j}$ can be formulated as follows: 
\begin{equation}
\label{eq:cls_probability}
\begin{aligned}
    P(c=c_{j}|\mathcal{\tilde{T}}_{i}) = \textbf{P}[i,j] &= f_{\textbf{K}}(\textbf{K}[i,j])\\ &= \frac{\exp{(\textbf{K}[i,j]}/\tau)}{\sum_{j}{\exp{(\textbf{K}[i,j]}}/\tau)},
\end{aligned}
\end{equation}
where $\tau$ is a temperature hyperparameter to control the classification resolution. A smaller $\tau$ amplifies sample similarity differences, implying a higher classification resolution, while a larger value yields the opposite effect.

% To enhance classification performance, we can employ  Formally, for the $i$-th diversity-sensitive component $\mathcal{\tilde{T}}_{i}$ in $\mathcal{D}$, the embedding function $\Phi$ takes $\mathcal{\tilde{T}}_{i}$ as input and outputs the sample representation $\textbf{h}_{i}$. Based on $\textbf{h}_{i}$, the classification function $f$ predicts the class label of $\mathcal{\tilde{T}}_{i}$. Thus, the classification task shown in~\eqref{eq:cls_ori} can be summarized as follows:
% \begin{equation}
% \label{eq:cls_diver}
%     P(\hat{c}|\mathcal{\tilde{T}}_{i}) = \operatorname{Softmax}(f(\Phi(\mathcal{\tilde{T}}_{i}))),
% \end{equation}

According to~\eqref{eq:cls_probability}, if the evaluated dataset exhibits high sample richness, indicating greater diversity, each sample is likely to be classified into its own category. Conversely, if diversity is low, all samples may be classified into a single category. Based on $\textbf{P}$, \ours calculates diversity of $\mathcal{D}$ as the trace of $\textbf{P}$. The definition of diversity measurement from the classification perspective is as follows:

% For notation simplicity, we denote $P(\hat{c}|\mathcal{\tilde{T}}_{i})$ as $P_{i}$. For each sample in $\mathcal{D}$, we calculate the classification probability $P_{i}$ according to~\eqref{eq:cls_diver}. This results in a probability matrix $\textbf{P}=[P_{1}, P_{2},..., P_{n}]$, where $\textbf{P}[i,j]$ represents the probability that the $i$-th sample $\mathcal{\tilde{T}}_{i}$ is classified into the $j$-th category. 

\begin{definition}[\ours]
\label{defin:cls_score}
 Let $\mathcal{D}=\{\mathcal{T}_{i}\}_{i=1}^{n}$ denote the synthetic dataset with $n$ samples, and let $\{\mathcal{\tilde{T}}_{i}\}_{i=1}^{n}$ represent a set of diversity-sensitive components within $\{\mathcal{T}_{i}\}_{i=1}^{n}$. Denote $P_{i}$ as the classification probability vector of $\mathcal{\tilde{T}}_{i}$. By conducting the classification task for all $\mathcal{\tilde{T}}_{i}$ and obtaining the probability matrix $\textbf{P}=[P_{1}, P_{2},..., P_{n}]$, \ours for $\mathcal{D}$ is defined as the trace of $\textbf{P}$:
\begin{equation}
\label{eq:trace_diver}
    \operatorname{DCScore}(\mathcal{D}) = \operatorname{tr}(\textbf{P})=\sum_{i=1}^{n}{\textbf{P}[i,i]}.
\end{equation}
\end{definition}

\textit{Notably, the process described above is one way to implement of \ours. Building on the core concept of \ours, we have designed an alternative implementation, which is detailed in Appendix~\ref{apd:other_imple}.}

\subsection{Properties of \ours}
We provide theoretical proof that \ours satisfies several axioms~\citep{leinster2012measuring} defined for a principled diversity metric. Specifically, \ours meets four axioms: effective number, identical samples, symmetry, and monotonicity axioms. These guarantees ensure a reasonable and robust diversity evaluation. The matched axioms of our proposed method are outlined below, while their proofs are detailed in Appendix~\ref{apd:proof} due to space constraints. 
\begin{itemize}
    \item \textbf{Effective number}: Diversity should be defined as the effective number of samples in a dataset, ranging from 1 to $n$. \ours meets this axiom, as evidenced by its behavior: \ours equals 1 when all samples in $\mathcal{D}$ are identical and equals $n$ when all samples are distinct. 
    \item \textbf{Identical samples}: Given two identical datasets $\mathcal{D}_{1}$ and $\mathcal{D}_{2}$, the diversity of $\mathcal{D}^{'}$ generated by merging these two datasets remains unchanged. The values of \ours are the same across $\mathcal{D}_{1}$, $\mathcal{D}_{2}$, and $\mathcal{D}^{'}$, i.e.,
    \begin{equation}
    \small
    \label{eq:identical_sample}
        \operatorname{DCScore}(\mathcal{D}_{1}) = \operatorname{DCScore}(\mathcal{D}_{2}) = \operatorname{DCScore}(\mathcal{D}^{'}).
    \end{equation}
    \item \textbf{Symmetry}: Diversity remains constant regardless of the order of the samples, exhibiting permutation invariance. Let $\pi$ denote the permutation function for the sample order, \ours remains unchanged for any sample permutation of $\mathcal{D}$, i.e., 
    \begin{equation}
    \label{eq:dcscore_sum}
        \operatorname{DCScore}(\mathcal{D}) = \operatorname{DCScore}(\pi(\mathcal{D})).
    \end{equation}
    \item \textbf{Monotonicity}: The diversity of a dataset decreases as the similarity between its samples increases. Given two datasets $\mathcal{D}_{1}$ and $\mathcal{D}_{2}$, and a new sample $\mathcal{T}_{n+1}$, where the samples in $\mathcal{D}_{1}$ and $\mathcal{D}_{2}$ are entirely different, and $\operatorname{DCScore}(\mathcal{D}_{1})=\operatorname{DCScore}(\mathcal{D}_{2})=n$. If $\mathcal{T}_{n+1}$ is more similar to the samples in $\mathcal{D}_{2}$ than to those in $\mathcal{D}_{1}$ and is added to both datasets, then for the merged datasets $\mathcal{D}_{1}^{'}$ and $\mathcal{D}_{2}^{'}$, \ours satisfies the following equation.
    \begin{equation}
        \operatorname{DCScore}(\mathcal{D}_{1}^{'}) > \operatorname{DCScore}(\mathcal{D}_{2}^{'}).
    \end{equation}
\end{itemize}

\subsection{Complexity Analysis}
\label{subsec:complexity_ana}

\begin{table}[!t]
\centering
\caption{Comparison of Complexity analysis between \ours and VendiScore. $\mathcal{O}_{kernel}$ represents the complexity of the kernel function.}
\label{tab:complexity_analysis}
\renewcommand\arraystretch{1.3}
\resizebox{\linewidth}{!}{
\begin{tabular}{cccc}
\toprule\
                         &                      & \textbf{General Kernels}    & \textbf{Inner Product}                                     \\
\midrule
\multirow{2}{*}{\textbf{Pairwise Similarity}} & \textbf{Vendi Score} & \multirow{2}{*}{$\mathcal{O}(n^2\cdot\mathcal{O}_{kernel})$}  & \multicolumn{1}{c}{$\mathcal{O}(d^2n)$} \\
                                              & \textbf{\ours}     &                          & $\mathcal{O}(n^2d)$                                                             \\
                                              \cmidrule(l){2-4}
\multirow{2}{*}{\textbf{Summarization}}       & \textbf{Vendi Score} & $\mathcal{O}(n^3)$    & \multicolumn{1}{c}{$\mathcal{O}(d^{3})$}                                           \\
                                              & \textbf{\ours}     & $\mathcal{O}(n^2)$                           & $\mathcal{O}(n^2)$                                          \\
                                              \cmidrule(l){2-4}
\multirow{2}{*}{\textbf{Total}}               & \textbf{Vendi Score} & $\mathcal{O}(n^2\cdot\mathcal{O}_{kernel}+n^3)$ & $\mathcal{O}(d^2n+d^{3})=\mathcal{O}(d^2n)$             \\
                                              & \textbf{\ours}     & $\mathcal{O}(n^2\cdot\mathcal{O}_{kernel}+n^2)$                     & $\mathcal{O}(n^2d+n^2)$   \\
                                              \bottomrule
\end{tabular}}
\end{table}

To better understand \ours, we provide a brief analysis of its time complexity in Table~\ref{tab:complexity_analysis}. Since \textit{VendiScore} is quite similar to \ours, we compare the computational complexities between these two methods. Our analysis reveals that \ours exhibits lower computation complexity under non-linear kernels compared to \textit{VendiScore}. 

Denoting $\mathcal{O}_{kernel}$ as the complexity associated with general kernels (i.e., kernels other than linear kernels), we analyze the complexity in the pairwise similarity and summarization stages. In the pairwise similarity stage, the computation of pairwise similarities results in a complexity of $\mathcal{O}(n^2)$ for \ours. When combined with the complexity $\mathcal{O}_{kernel}$ of the general kernel computation, \ours exhibits a total complexity of $\mathcal{O}(n^2\cdot \mathcal{O}_{kernel})$ in this stage. In the summarization stage, \ours has a complexity of $\mathbf{O}(n^2)$ due to the softmax operation. Consequently, the overall complexity of \ours for general kernels is $\mathcal{O}(n^2\cdot \mathcal{O}_{kernel}+n^2)$. In contrast, \textit{VendiScore} has a total complexity of $\mathcal{O}(n^2\cdot \mathcal{O}_{kernel}+n^3)$, where the pairwise similarity stage is identical to that of \ours, while the summarization stage incurs a complexity of $\mathcal{O}(n^3)$ due to the eigenvalue computation. Thus, for general kernels, \ours demonstrates lower complexity than \textit{VendiScore}.

However, when the inner product is employed as the kernel function and $n\gg d$, \textit{VendiScore} can significantly reduce the complexity by replacing the pairwise similarity $XX^T$ with $X^TX$, where $X\in \mathbb{R}^{n\times d}$. This results in complexities of $\mathcal{O}(d^2n)$ for the pairwise similarity stage and $\mathcal{O}(d^3)$ for the summarization stage. In this regard, \ours has a complexity of $\mathcal{O}(n^2d+n^2)$, which is slightly worse than that of \textit{VendiScore}. We can leverage efficient techniques, such as those proposed in \cite{shim2017svd}, \cite{milakov2018online} and \cite{wen2023pairwise}, to reduce the computational complexity of \ours. Overall, compared to \textit{VendiScore}, \ours maintains lower complexity in most cases, as empirically validated in Section~\ref{subsec:computation_cost} and Appendix~\ref{subsec:cost_larger}. Although \textit{VendiScore} has a lower complexity when the inner product is used as the kernel, experiments in the Appendix~\ref{subsec:cost_larger} show that the computation times of \textit{VendiScore} and \ours are quite similar. In practical applications, different kernels keep computational complexity low, making them more useful than a single kernel.


% \subsection{A Unified Modeling for Existing Methods}
% \label{sec:unify_modeling}
% \textcolor{blue}{To further understand \ours, we compare it with existing methods, including Distinct-n~\citep{li2015diversity}, K-means inertia~\citep{du2019boosting}, and VendiScore~\citep{dan2023vendi}. We find that these methods can be integrated into the \ours framework, which consists of three stages: \textit{Text Representation}, \textit{Pairwise Similarity}, and \textit{Diversity Summarization}. In the text representation stage, sample representations are extracted, which then serve as the basis for measuring similarity between samples in the pairwise similarity stage. Finally, the diversity summarization stage evaluates the dataset's diversity by aggregating the pairwise similarities of all samples. Based on~\eqref{eq:rep_sim}-\ref{eq:trace_diver}, \ours is structured into these three stages as follows:}
% \begin{align}
%     \begin{split}
%     \label{eq:dcscore_unified}
%          &\textnormal{\textit{Text Representation: }} \textbf{H} = \Phi(\{\mathcal{\tilde{T}}_{i}\}_{i=1}^{n}), \\
%          &\textnormal{\textit{Pairwise Similarity: }} \textbf{K} = \operatorname{Kernel}(\textbf{H}), \\
%          & 
%          \begin{aligned}
%             \textnormal{\textit{Diversity Summarization: }} &\textbf{P}[i,j] = f_{\textbf{K}}(\textbf{K}[i,j]),\\
%             &\operatorname{DCScore}(\mathcal{D}) = \operatorname{tr}(\textbf{P})=\sum_{i=1}^{n}{\textbf{P}[i,i]}.
%          \end{aligned}
%     \end{split}
% \end{align}

% \begin{table}[!t]
% \centering
% \caption{\textcolor{blue}{Existing methods are modeled into the framework of \ours.}}
% \label{tab:unified_modeling}
% \small 
% \renewcommand{\arraystretch}{1.6}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{cccc}
%     \toprule
%     \multicolumn{1}{l}{\raisebox{0.1ex}{}} & \raisebox{0.1ex}{\textcolor{blue}{\textbf{Text Representation}}}  & \raisebox{0.1ex}{\textcolor{blue}{\textbf{Pairwise Similarity}}}  & \raisebox{0.1ex}{\textcolor{blue}{\textbf{Diversity Summarization}}} \\
%     \midrule
%     \textcolor{blue}{\textbf{Distinct-n}~\citep{li2015diversity}}           & \multicolumn{1}{c}{\textcolor{blue}{$\operatorname{n-grams}(\operatorname{Concat}(\mathcal{D}))$}} & \multicolumn{1}{c}{\scriptsize \textcolor{blue}{$\operatorname{Unique}(\operatorname{n-grams}(\operatorname{Concat}(\mathcal{D})))$}} & \textcolor{blue}{$\frac{|\operatorname{Unique}(\operatorname{n-grams}(\operatorname{Concat}(\mathcal{D})))|}{|\operatorname{n-grams}(\operatorname{Concat}(\mathcal{D}))|}$}                                            \\
%     \textcolor{blue}{\textbf{K-means inertia}~\citep{du2019boosting}}      & \textcolor{blue}{$\textbf{H} = \Phi(\{\mathcal{\tilde{T}}_{i}\}_{i=1}^{n})$}    & \multicolumn{1}{c}{\textcolor{blue}{$\mathcal{C} = \operatorname{K-means}(\textbf{H})$}} & \textcolor{blue}{$\sum_{\textbf{c}_{k}\in \mathcal{C}, \textbf{h}_{j}\in \textbf{H}_{\textbf{c}_{k}}}{(\textbf{h}_{j}-\textbf{c}_{k})^2}$}                                            \\
%     \textcolor{blue}{\textbf{VendiScore}~\citep{dan2023vendi}}          & \textcolor{blue}{$\textbf{H} = \Phi(\{\mathcal{\tilde{T}}_{i}\}_{i=1}^{n})$}                     & \textcolor{blue}{$\textbf{K} = \operatorname{Kernel}(\textbf{H})$}    & \textcolor{blue}{$\exp{(-\sum_{i=1}^{n}\lambda_{i}\log\lambda_{i})}$}                                            \\
%     % \rule{0pt}{6ex}
%     % DCScore              &                      &                      & \makecell[l]{$\textbf{P}[i,j] = f_{\textbf{K}}(\textbf{K}[i,j])$ \\
%     % $\operatorname{DCScore}(\mathcal{D}) = \operatorname{tr}(\textbf{P})$}                                            \\
%     \bottomrule
% \end{tabular}}
% \end{table}

% \textcolor{blue}{In this regard, three existing methods are summarized into three stages in Table~\ref{tab:unified_modeling}, with further details provided in Appendix~\ref{sec:detailed_modeling}. \textit{Distinct-n} calculates the proportion of unique n-grams to all n-grams within a concatenated dataset, where the concatenation, unique operation, and n-grams are denoted as $\operatorname{Concat}(\cdot)$, $\operatorname{Unique}(\cdot)$, and $\operatorname{n-grams}(\cdot)$, respectively. \textit{K-means inertia} and \textit{VendiScore} are transformation-based methods that perform clustering and eigenvalue computation in the representation space. For \textit{K-means inertia}, $\operatorname{K-means}(\cdot)$, $\mathcal{C}$, and $\textbf{c}_{k}\in \mathcal{C}$ represent k-means clustering, the cluster centroid set, and the $k$-th cluster centroid. For \textit{VendiScore}, $\lambda_{i}$ is the $i$-th eigenvalue of $\textbf{K}/n$.} 

% \textcolor{blue}{In summary, existing methods can be incorporated into the \ours framework, indicating that \ours operates as a higher-level method. \ours also follows a transformation-based approach, resulting in similar processes in the text representation and pairwise similarity stages as \textit{K-means inertia} and \textit{VendiScore}. However, \ours generally exhibits lower complexity, as detailed in Appendix~\ref{subsec:complexity_ana}. It is worth noting that~\eqref{eq:dcscore_unified} is not the only implementation of \ours from a classification perspective, and other implementation methods will be explored in future work.}


\section{Experiments}
We conduct experiments to verify the effectiveness of \ours by examining correlation, computational cost, hyperparameter sensitivity, and further probing. Additionally, we provide additional experiments in Appendix~\ref{apd:addi_exp}.

\subsection{Experimental Settings}
\begin{figure}[tbp]
    \centering
    % \vspace{-3mm}
    \includegraphics[width=0.8\linewidth]{images/exp_setting.pdf}
    % \vspace{-4mm}
    \caption{Experimental settings of correlation evaluation.} 
    % Diversity pseudo-truth represents proxy ground truth, such as the temperature $\tau_{g}$ of dataset generation and human judgment.}
    % \vspace{-4mm}
    \label{fig:exp_settings}
\end{figure}

To verify the effectiveness of \ours, we conduct a series of correlation experiments following the setup in \cite{tevet2020evaluating}. As shown in Figure~\ref{fig:exp_settings}, the core idea of our experimental evaluation is to correlate the diversity measurement results of \ours with diversity pseudo-truths, such as the softmax temperature $\tau_{g}$ of dataset generation, human judgment, and LLMs evaluation. Specifically, we evaluate $l$ generated datasets to obtain $l$ diversity scores and then calculate the correlation with diversity pseudo-truths. To calculate the correlation between measured diversity scores and diversity pseudo-truths, we employ Spearman’s $\rho$~\citep{spearman1961proof}, a measure of rank correlation ranging from -1 to 1, with higher absolute values indicating stronger correlations. Due to space limitations, we present detailed experimental settings in Appendix~\ref{apd:exp_settings}. 

\textbf{Datasets.} We utilize two categories of datasets in our experiments: self-generated datasets and publicly available generated datasets. Self-generated datasets are generated through two data generation strategies~\citep{li2023synthetic}: \textit{zero-shot} and \textit{few-shot} settings. Additionally, we generate datasets for two natural language generation tasks: \textit{text classification} and \textit{story completion}. We utilize three publicly available existing datasets, including SST2~\citep{socher2013recursive}, Yelp~\citep{zhang2015character}, and AG News~\citep{zhang2015character}, and their AttrPrompt-augmented version~\cite{yu2024large}. Detailed information about these datasets can be found in Appendix~\ref{apd:datasets}.  

\textbf{Generation Models.} To generate datasets through zero-shot and few-shot settings, we employ two commonly used LLMs as our dataset generators, including Llama2-13B (13B) and Llama2-70B (70B)~\citep{touvron2023llama}.

\textbf{Baseline Methods.} We compare \ours with three baseline methods detailed in Appendix~\ref{sec:detailed_modeling}, i.e., \textit{Distinct-n}~\citep{li2015diversity}, \textit{K-means inertia}~\citep{du2019boosting}, and \textit{VendiScore}~\citep{dan2023vendi}.

\subsection{Correlation Evaluation}
\label{tab:corre_eval}
We investigate the correlation between the diversity evaluation of \ours and diversity pseudo-truth, such as $\tau_{g}$, human judgment, and LLMs evaluation. We compare \ours with all baselines on self-generated datasets.  

\subsubsection{Correlation with Generation Temperature $\tau_{g}$}
\label{subsec:corre_tau}
\textbf{Evaluation on self-generated datasets.} Previous works~\citep{caccia2018language,tevet2020evaluating,chung2023increasing} have demonstrated a positive correlation between $\tau_{g}$ and the diversity of generated texts, making $\tau_{g}$ as a reasonable diversity pseudo-truth. LLMs with lower $\tau_{g}$ generate less diverse content,  whereas higher $\tau_{g}$ values yield more diverse content. Thus, we evaluate the performance of \ours on self-generated datasets with varying $\tau_{g}$, ranging from 0.2 to 1.2 at 0.05 intervals. We present more information about self-generated datasets in Appendix~\ref{subsubsec:dataset_details}. Table~\ref{tab:correlation_tau} displays the correlation results of all methods. All methods accurately capture the true diversity of generated datasets, as demonstrated by high Spearman’s $\rho$ values. \ours performs on par with VendiScore while providing better scalability for larger synthetic datasets, as discussed in Section~\ref{subsec:computation_cost}. \ours outperforms all baseline methods under the few-shot setting across all datasets, highlighting its effectiveness. \textit{K-means Inertia} exhibits the weakest correlation on the text classification dataset generated by the 13B model under the zero-shot setting, potentially due to its sensitivity to the number of cluster centroids. Overall, \ours outperforms all baselines in most cases, and its evaluation results exhibit a strong correlation with the diversity pseudo-truth according to~\cite{akoglu2018user}.
% performs the worst correlation on the text classification dataset generated by the 13B model under zero-shot setting. We suspect that this phenomenon is related to the sensitivity of \textit{K-means Inertia} to the number of cluster centroids.

\begin{table}[!t]
    \centering
    \caption{Correlation (Spearman’s $\rho$) between $\tau_{g}$ and diversity evaluation methods on datasets generated by different settings (\textit{Zero-shot} or \textit{Few-shot}). Spearman’s $\rho$ varies between -1 and +1 with 0 implying no correlation. Best results are indicated in \textbf{bold}.}
    \label{tab:correlation_tau}
    \renewcommand\arraystretch{1.1}
    \resizebox{0.98\linewidth}{!}{
    \begin{tabular}{l|cccc|cccc}
    \toprule
    \multirow{3}{*}{\textbf{Methods}} & \multicolumn{4}{c|}{\textbf{\textit{Zero-shot setting}}} & \multicolumn{4}{c}{\textbf{\textit{Few-shot setting}}} \\ 
    % \cmidrule(l){2-9}
                                      & \multicolumn{2}{c}{\textbf{Text classification}} & \multicolumn{2}{c|}{\textbf{Story completion}} & \multicolumn{2}{c}{\textbf{Text classification}} & \multicolumn{2}{c}{\textbf{Story completion}} \\
    \cmidrule(l){2-9}
                                      & 13B & 70B & 13B & 70B & 13B & 70B & 13B & 70B \\ \midrule
    Distinct-n                        & 0.9909 & \textbf{0.9870} & 0.9766 & 0.9701 & 0.9857 & 0.9766 & 0.9779 & 0.9935 \\
    K-means Inertia                   & -0.1143 & 0.9688 & 0.9454 & 0.8727 & 0.7104 & 0.7273 & 0.9662 & 0.9662 \\
    VendiScore                        & \textbf{0.9961} & 0.9818 & \textbf{0.9870} & \textbf{0.9922} & \textbf{0.9909} & 0.9857 & \textbf{0.9857} & 0.9961 \\
    \ours                             & \textbf{0.9961} & 0.9779 & 0.9844 & 0.9792 & \textbf{0.9909} & \textbf{0.9883} & \textbf{0.9857} & \textbf{0.9974} \\ \bottomrule
    \end{tabular}}
\end{table}


\begin{figure}[tbp]
    \centering
    % \vspace{-4mm}
    \includegraphics[width=\linewidth]{images/chart_tau-vs-dcscore_combined.pdf}
    % \vspace{-5mm}
    \caption{Diversity evaluation (\ours) w.r.t dataset generated using different temperatures ($\tau_{g}$). \ours's evaluation shows a strong correlation with $\tau_{g}$, indicating its effectiveness in evaluating the intrinsic diversity of the dataset.}
    % \vspace{-4mm}
    \label{fig:dcscore_tau}
\end{figure}

\textbf{Visualization.} We further provide a visualization of the diversity evaluation results for \ours. For each generated dataset, we prompt LLMs to produce 10 distinct answers corresponding to a single prompt, forming an evaluation batch. We then evaluate diversity using the batch evaluation protocol outlined in Appendix~\ref{apd:eval_protocol}. Ideally, a completely diverse dataset may yield a diversity score of 10. 

As shown in Figure~\ref{fig:dcscore_tau}, \ours exhibits a strong positive correlation with $\tau_{g}$, consistent with its impact on content diversity. In most cases, when $\tau_{g} > 0.75$, \ours scores a generated dataset with a diversity value of approximately 10. For text classification, the 13B generation model under the few-shot setting demonstrates a distinct diversity change pattern compared to others. This phenomenon arises from the 13B generation model's inability to follow more complex instructions, resulting in limited diversity improvement.

% \begin{wrapfigure}{r}{0.32\linewidth}
% \begin{figure}[!t]
%     \centering
%     % \vspace{-2mm}
%     \includegraphics[width=0.8\linewidth]{images/results_existing_datasets.pdf}
%     % \vspace{-4mm}
%     \caption{Diversity evaluation on existing generated datasets. A.P. represents the abbreviation of AttrPrompt.}
%     % \vspace{-4mm}
%     \label{fig:correlation_exist_data}
% \end{figure}
% \end{wrapfigure}

% \textbf{Evaluation on existing datasets.} Additionally, we present the diversity evaluation results on three publicly available datasets in Figure~\ref{fig:correlation_exist_data}. By dividing each result by the maximum diversity evaluation value across three datasets, we normalize all results to the range [0,1]. 

% We observe that, compared to baselines, the evaluation results of \ours exhibit a similar changing trend across three datasets, highlighting its effectiveness. Additionally, both \ours and VendiScore demonstrate a greater ability to discern diversity between different datasets compared to the other two methods.

% \begin{table}[!t]
%     \centering
%     \caption{Correlation (Spearman’s $\rho$) between temperature and diversity evaluation methods on datasets generated by \textit{zero-shot setting}. Spearman’s $\rho$ varies between -1 and +1 with 0 implying no correlation. Best results are indicated in \textbf{bold}.}
%     % \renewcommand\arraystretch{0.9}
%     \begin{tabular}{ccccc}
%     \toprule
%     \multirow{2}{*}{\textbf{Methods}} & \multicolumn{2}{c}{\textbf{Text classification}} & \multicolumn{2}{c}{\textbf{Story completion}} \\ \cmidrule(l){2-5}
%                                       & 13B               & 70B             & 13B            & 70B            \\ \midrule
%     Distinct-n                        & 0.9909                   & \textbf{0.9870}         & 0.9766                & 0.9701                \\
%     K-means Inertia                   & -0.1143                  & 0.9688                 & 0.9454                & 0.8727                \\
%     VendiScore                       & \textbf{0.9961}          & 0.9818                 & \textbf{0.9870}        & \textbf{0.9922}       \\
%     \ours                    & \textbf{0.9961}          & 0.9779                 & 0.9844                & 0.9792                \\ \bottomrule
%     \end{tabular}
%     \label{tab:zero_shot}
% \end{table}

% \begin{table}[!t]
%     \centering
%     \caption{Correlation (Spearman’s $\rho$) between temperature and diversity evaluation methods on datasets generated by \textit{few-shot setting}. Spearman’s $\rho$ varies between -1 and +1 with 0 implying no correlation. Best results are indicated in \textbf{bold}.}
%     \begin{tabular}{ccccc}
%     \toprule
%     \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Methods}}} & \multicolumn{2}{c}{\textbf{Text classification}}                & \multicolumn{2}{c}{\textbf{Story   completion}}                 \\
%     \cmidrule(l){2-5}
%     \multicolumn{1}{c}{}                         & \multicolumn{1}{c}{13B} & \multicolumn{1}{c}{70B} & \multicolumn{1}{c}{13B} & \multicolumn{1}{c}{70B} \\ \midrule
%     Distinct-n                             & 0.9857                         & 0.9766                         & 0.9779                         & 0.9935                         \\
%     K-means Inertia                              & 0.7104                         & 0.7273                         & 0.9662                         & 0.9662                         \\
%     VendiScore                                  & \textbf{0.9909}                         & 0.9857                         & \textbf{0.9857}                         & 0.9961                         \\
%     \ours                               & \textbf{0.9909}                         & \textbf{0.9883}                         & \textbf{0.9857}                         & \textbf{0.9974}   \\ \bottomrule                     
%     \end{tabular}
%     \label{tab:few_shot}
% \end{table}

\subsubsection{Correlation with Human Judgment}
\label{subsec:corre_human}
Diversity evaluation is a subjective task, and an ideal method should align well with human judgment. Therefore, we investigate the correlation between \ours and human judgment. To mitigate evaluators' bias, we enlist three individuals to perform pairwise diversity comparisons among datasets with varying $\tau_{g}$ values and report the diversity ranking by averaging the win rate across evaluators. We conduct all evaluations five times to report average results. 
% Moreover, we exclusively use the 70B model for dataset generation to ensure better content creation. 

Table~\ref{tab:human_correlation} presents pairwise correlation between human judgment, $\tau_{g}$, and \ours. Table~\ref{tab:human_correlation} indicates a strong correlation between human judgment and $\tau_{g}$, supporting the use of human judgment as a diversity pseudo-truth. Based on this observation, \ours performs better in two settings: \textbf{Story-Few} (story completion task generation under the few-shot setting) and \textbf{Text-Zero} (text classification task generation under the zero-shot setting). This is confirmed by higher human-\ours correlation in these two settings. In contrast, for \textbf{Story-Zero} and \textbf{Text-Few} settings, we observe more identical content in the initial portions of the diversity-sensitive components within an evaluation batch. In these cases, human evaluators tend to disregard the identical content and base their judgments on the latter sections. However, \ours is affected by the identical content, resulting in a lower pairwise correlation. Despite this, the correlation remains strong, as demonstrated by previous studies~\citep{akoglu2018user}.

% \begin{wraptable}{r}{0.6\linewidth}
\begin{table}[t]
    \centering
    \caption{Pairwise correlation (Spearman’s $\rho$) between human, temperature ($\tau_{g}$), and \ours. \ours indicates a strong correlation with human judgment.}
    \label{tab:human_correlation}
    \renewcommand\arraystretch{1.1}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{c|cccc}
            \toprule
            % & \multicolumn{2}{c|}{\textbf{\textit{Zero-shot setting}}} & \multicolumn{2}{c}{\textbf{\textit{Few-shot setting}}} \\
            & \textbf{Story-Few} & \textbf{Story-Zero} & \textbf{Text-Few} & \textbf{Text-Zero} \\
            % & \multicolumn{2}{c}{\textbf{Text classification}} & \multicolumn{2}{c|}{\textbf{Story completion}} & \multicolumn{2}{c}{\textbf{Text classification}} & \multicolumn{2}{c}{\textbf{Story completion}} \\
            \midrule
            Human-\ours & 0.9040$_{\pm 0.04}$ & 0.7870$_{\pm 0.10}$ & 0.7915$_{\pm 0.16}$ & 0.8798$_{\pm 0.10}$ \\
            $\tau_{g}$-\ours & 0.9086$_{\pm 0.07}$ & 0.7829$_{\pm 0.16}$ & 0.8400$_{\pm 0.16}$ & 0.8971$_{\pm 0.07}$ \\
            $\tau_{g}$-Human & 0.9276$_{\pm 0.02}$ & 0.9194$_{\pm 0.06}$ & 0.9770$_{\pm 0.02}$ & 0.9255$_{\pm 0.08}$ \\
            \bottomrule
        \end{tabular}}
\end{table}
% \end{wraptable}

\subsubsection{Correlation with LLM Evaluator}
\label{subsubsec:gpt_evaluation}
\begin{table}[t]
    \centering
    \caption{Pairwise correlation (Spearman’s $\rho$) between GPT-4, temperature ($\tau_{g}$), and \ours. \ours indicates a strong correlation with GPT-4 evaluation results.}
    \label{tab:gpt_correlation}
    \renewcommand\arraystretch{1.1}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{c|cccc}
            \toprule
            & \textbf{Story-Few} & \textbf{Story-Zero} & \textbf{Text-Few} & \textbf{Text-Zero} \\
            \midrule
            GPT-4-\ours & 0.6057$_{\pm 0.30}$ & 0.9010$_{\pm 0.04}$ & 0.6131$_{\pm 0.18}$ & 0.9052$_{\pm 0.09}$ \\
            $\tau_{g}$-\ours & 0.6757$_{\pm 0.30}$ & 0.8782$_{\pm 0.08}$ & 0.5714$_{\pm 0.27}$ & 0.9336$_{\pm 0.06}$ \\
            $\tau_{g}$-GPT-4 & 0.9086$_{\pm 0.07}$ & 0.7829$_{\pm 0.16}$ & 0.8400$_{\pm 0.16}$ & 0.8971$_{\pm 0.07}$ \\
            \bottomrule
        \end{tabular}}
\end{table}

To further verify the effectiveness of \ours, we investigate the evaluation correlation between \ours and LLMs. Following the setting in Section~\ref{subsec:corre_human}, we employ GPT-4 to conduct pairwise comparisons between two generated datasets with different $\tau_{g}$. These generated datasets are identical to those used in Section~\ref{subsec:corre_human}. Based on the pairwise comparison results, we obtain the diversity ranking outcomes. Regarding GPT-4 evaluation results as the diversity pseudo-truth, we report the pairwise evaluation correlation between \ours, GPT-4, and $\tau_{g}$ in Table~\ref{tab:gpt_correlation}. We observe that \ours exhibits strong correlations with GPT-4 and $\tau_{g}$ in zero-shot settings. By comparing the results of ``$\tau_{g}$-\ours'' and ``$\tau_{g}$-GPT-4'', we find that \ours outperforms the GPT-4 evaluator in terms of correlation with $\tau_{g}$ in zero-shot settings. Regarding the correlation performance in few-shot settings, we notice lower correlations of all baseline methods compared to zero-shot settings. We guess that this phenomenon is related to the distributions of the generated datasets. Although \ours exhibits lower correlations (about 0.6) with GPT-4, this result can still be considered a strong correlation according to~\cite{akoglu2018user}.

\textit{In summary, \ours exhibits a strong correlation (Spearman’s $\rho$ $\geq$ 0.6), with three diversity pseudo-truths: $\tau_{g}$, human judgment, and LLMs evaluation, thereby verifying the effectiveness of \ours.}

\subsection{Computational Cost}
\label{subsec:computation_cost}
The computational cost is crucial in diversity evaluation methods, especially with the increasing sample sizes of synthetic datasets. For a fair comparison, we only present the computation times of transformation-based methods: \ours, \textit{K-means Inertia}, and \textit{VendiScore}. We truncate the text length of three datasets to 50 tokens and record the computation times of three transformation-based methods with varying sample sizes in the range of $\{100, 500, 1000, 2000, 4000\}$.

\begin{figure}[!tbp]
    \centering
    \includegraphics[width=\linewidth]{images/computation_cost_combined.pdf}
    \caption{Computation times under different sample sizes. \ours outperforms all baselines in computational cost.}
    \label{fig:computation_cost}
\end{figure}

As shown in Figure~\ref{fig:computation_cost}, we repeat the experiments five times to report the final results. \ours and \textit{K-means Inertia} exhibit nearly identical computation times. However, \ours significantly outperforms \textit{K-means Inertia} in correlation with $\tau_{g}$, as evidenced in Section~\ref{subsec:corre_tau}. Compared to \textit{VendiScore}, \ours demonstrates a speed advantage of approximately 16\%, or more than one second, when processing 4000 samples. Analyzing the time complexity of these two methods, and disregarding the selection of the kernel function, we find that for a dataset with $n$ samples, where $n\gg d$ is not satisfied, the computational complexity of \ours in diversity summarization is $\mathcal{O}(n^{2})$ due to the softmax computation. In contrast, \textit{VendiScore} requires finding the eigenvalues of an $n\times n$ matrix, resulting in a computational complexity of $\mathcal{O}(n^{3})$. Consequently, \ours offers significantly lower time complexity than \textit{VendiScore} while sacrificing little in diversity evaluation performance. However, as detailed in the complexity analysis shown in Section~\ref{subsec:complexity_ana}, when $n\gg d$ and inner products are used as the kernel function, the total complexity of \textit{VendiScore} can be reduced to $\mathcal{O}(d^2n)$. Thus, we evaluate computational costs on larger datasets, i.e., satisfying $n\gg d$. As shown in Table~\ref{tab:cost_sst2}, \ours exhibits a notable advantage in computational efficiency when using non-linear kernels, e.g., RBF and Poly kernel. We present additional experimental results and a more in-depth analysis in Appendix~\ref{subsec:cost_larger}.

\begin{table}[!t]
\centering
\caption{Comparison of computation time between \ours and VendiScore on SST2.}
\label{tab:cost_sst2}
\small
\renewcommand{\arraystretch}{1.2}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccc}
\toprule
\multirow{2}{*}{\textbf{Kernels}} & \multicolumn{1}{l}{} & \multicolumn{5}{c}{\textbf{SST2}}                                                                                              \\
\cmidrule{2-7}
                                 & \textbf{Sample num}           & \textbf{4k}                   & \textbf{8k}                    & \textbf{16k}                   & \textbf{32k}                   & \textbf{64k}                    \\
                                 \midrule
\multirow{2}{*}{Inner product}   & VendiScore           & 4.65$_{\pm 0.28}$          & \textbf{9.84$_{\pm 0.26}$}  & \textbf{19.02$_{\pm 0.70}$} & \textbf{37.31$_{\pm 1.88}$} & \textbf{76.19$_{\pm 1.91}$}  \\
                                 & \ours              & \textbf{4.58$_{\pm 0.29}$} & 10.03$_{\pm 0.17}$          & 20.42$_{\pm 0.39}$          & 42.91$_{\pm 1.59}$          & 112.47$_{\pm 2.43}$          \\
\multirow{2}{*}{RBF kernel}      & VendiScore           & 5.86$_{\pm 0.06}$          & 12.41$_{\pm 0.49}$          & 32.94$_{\pm 0.40}$          & 100.36$_{\pm 1.44}$         & 449.14$_{\pm 10.35}$         \\
                                 & \ours              & \textbf{5.22$_{\pm 0.33}$} & \textbf{9.94$_{\pm 0.42}$}  & \textbf{21.20$_{\pm 0.75}$} & \textbf{46.57$_{\pm 1.47}$} & \textbf{117.06$_{\pm 1.91}$} \\
\multirow{2}{*}{Poly kernel}     & VendiScore           & 5.73$_{\pm 0.06}$          & 12.72$_{\pm 0.41}$          & 31.47$_{\pm 0.97}$          & 98.31$_{\pm 0.25}$          & 453.11$_{\pm 2.53}$          \\
                                 & \ours              & \textbf{5.09$_{\pm 0.28}$} & \textbf{10.27$_{\pm 0.12}$} & \textbf{20.12$_{\pm 1.02}$} & \textbf{46.25$_{\pm 1.82}$} & \textbf{123.51$_{\pm 3.40}$} \\
                                 \bottomrule
\end{tabular}}
\end{table}


\subsection{Hyperparameters Sensitivity}
\label{subsec:paras_sens}
% temperature 
% \begin{wrapfigure}{r}{0.6\linewidth}
\begin{figure}[!tbp]
    \centering
    % \vspace{-4mm}
    \includegraphics[width=\linewidth]{images/chart_paras_sens.pdf}
    % \vspace{-5mm}
    \caption{Hyperparameter sensitivity analysis w.r.t $\tau$ on self-generated datasets.}
    % \vspace{-3mm}
    \label{fig:paras_sens}
\end{figure}
% \end{wrapfigure}
According to~\eqref{eq:cls_probability}, the temperature ($\tau$) in the Softmax function is a critical hyperparameter that affects classification resolution. To investigate this, we conduct a hyperparameter sensitivity analysis of \ours w.r.t. $\tau$ on self-generated datasets used in Section~\ref{subsec:corre_tau}. We vary $\tau$ within the range of $\{0.0001, 0.001, 0.1, 0.5, 1, 10\}$. Figure~\ref{fig:paras_sens} presents hyperparameter sensitivity results for two natural language generation tasks: text classification and story completion. Overall, lower $\tau$ values result in lower Spearman’s $\rho$, even indicating a negative correlation, while higher $\tau$ values do the opposite. From~\eqref{eq:cls_probability}, a higher $\tau$ reduces pairwise similarity differences, leading to a more uniform distribution of classification probabilities for each sample. This phenomenon can be regarded as a lower classification resolution, i.e., the classification function $f_{\textbf{K}}$ has poorer discrimination power. Furthermore, the correlation result of the 13B generation model under the few-shot setting for the text classification task remains stable despite variations in $\tau$. This phenomenon has the same explanation as in Figure~\ref{fig:dcscore_tau}.

\subsection{Further Probe: Downstream Task Training}
\label{subsubsec:corre_task}

\begin{table}[]
    \centering
    \caption{Downstream task training performance and diversity evaluation on self-generated datasets with $\tau_{g}=\{0.2, 0.7, 1.2\}$.}
    \label{tab:task_train}
    \renewcommand\arraystretch{1.0}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{c|ccc|ccc}
        \toprule
        \multirow{2}{*}{} & \multicolumn{3}{c|}{\textbf{Accuracy}} & \multicolumn{3}{c}{\textbf{DCScore}} \\
                          & \textbf{$\tau_{g}$=0.2}    & \textbf{$\tau_{g}$=0.7}   & \textbf{$\tau_{g}$=1.2}   & \textbf{$\tau_{g}$=0.2}   & \textbf{$\tau_{g}$=0.7}   & \textbf{$\tau_{g}$=1.2}   \\
                          \midrule
        \textbf{Zero-shot}         & 89.10     & 89.70    & 90.37   & 481.76  & 1745.42 & 2082.42 \\
        \textbf{Few-shot}         & 70.07    & 73.19   & 73.41   & 1376.43 & 1958.16 & 2047.90 \\
        \bottomrule
        \end{tabular}
    }
\end{table}

To investigate the correlation between \ours and downstream task training, we train text classification models using self-generated datasets under zero-shot and few-shot settings. We vary $\tau_{g}$ of self-generated datasets within the range of $\{0.2, 0.7, 1.2\}$. More details of training datasets and hyperparameters are presented in Appendix~\ref{subsubsec:dataset_details} and~\ref{subsubsec:para_setting_task}, respectively. As shown in Table~\ref{tab:task_train}, models trained on more diverse datasets achieve better accuracy, likely due to their improved generalization capabilities~\cite{gontijo2020affinity}. Notably, the increased training data diversity makes model fitting more difficult, necessitating additional epochs to achieve optimal accuracy. Additionally, the diversity evaluated by \ours has a similar trend to the accuracy performance in the zero-shot setting, further demonstrating the effectiveness of \ours. Detailed experimental results and further analysis are provided in Appendix~\ref{apd:corre_task}.

\section{Conclusion}
In this work, we investigate the diversity evaluation of synthetic datasets, a topic systematically under-explored in existing research. To this end, we present \ours, a diversity evaluation method from a classification perspective. \ours regards the holistic diversity evaluation as the classification task at the sample level, thereby facilitating the capture of mutual relationships between samples. We provide theoretical guarantees demonstrating that \ours meets the axiom requirements~\citep{leinster2012measuring} for a principled diversity evaluation method. Furthermore, we show that existing methods can be unified within the \ours framework. Experiments on synthetic datasets reveal that \ours exhibits better correlations with various diversity pseudo-truths, such as $\tau_{g}$ and human judgment. Meanwhile, \ours exhibits significantly lower computational cost compared to transformation-based counterparts. Finally, we hope our work encourages future research to pay more attention to the diversity of synthetic datasets and promotes the wider application of these datasets.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Additional Related Work}
\label{apd:add_related_work}
Limited by the space, we provide a literature review of the LLM dataset generator and application of diversity evaluation methods as follows.
\subsection{LLM Dataset Generator}
\textbf{Prompt-guided and Dataset-guided Strategies.} Recent studies~\citep{ding2022gpt,chung2023increasing} leverage LLMs to augment existing datasets or generate a dataset from scratch, demonstrating the effectiveness in improving dataset quality and reducing data collection costs. Generally, efforts to employ LLMs as dataset generators can be categorized into three strategies: \textit{Prompt-guided}~\citep{li2023synthetic}, \textit{Dataset-guided}~\citep{ye2022zerogen}, and \textit{Instruct-guided}~\citep{samuel2023can}. The prompt-guided strategy, a prominent data augmentation approach using LLMs, involves designing task-specific prompts to guide LLMs to augment data in a few-shot~\citep{yoo2021gpt3mix} or zero-shot~\citep{mahmoudi2024zero} manner. Due to its simplicity and effectiveness, subsequent works extend this strategy to various scenarios, such as medical~\citep{yuan2023large}, person retrieval~\citep{li2024data}, and social media scenario~\citep{dos2024identifying}. However, simple prompt engineering has limitations in fully exploiting the capabilities of LLMs, leading to the development of multi-level prompt designs~\citep{ye2024llm} and targeted sample augmentation~\citep {yang2024mini}. To further harness the potential of LLMs, the dataset-guided strategy employs LLMs to generate a training set and then trains a task-specific model to annotate unlabeled data~\citep{sahu2024mixsumm}. The dataset-guided strategy aims to approximate the distribution of targeted scenarios, but it is currently only applicable to text classification tasks. 

\textbf{Instruct-guided Strategy.} Previous studies~\citep {white2023prompt} indicate that the design of prompts significantly impacts the performance of LLMs, spurring research into the instruct-guided strategy. Generally speaking, the instruct-guided strategy leverages LLMs to generate instructions that guide another LLM in dataset generation~\citep{evuru2024coda}. These instructions typically relate to context~\citep{samuel2023can}, criteria~\citep{huang2023learning}, and tasks~\citep{wang2022self}. 
To further improve the quality of instructions, efforts have been concentrated on selecting optimal instructions~\citep{li2024empowering}, integrating soft instructions~\citep{chen2023mixture}, and implementing self-correction mechanisms~\citep{gupta2023targen}. In a nutshell, LLMs are employed to generate or augment datasets through prompt engineering and multi-step strategies, which encompass various application scenarios and downstream tasks. Meanwhile, the diversity of synthetic datasets emerges as a critical factor in measuring data quality. In our work, we focus on the diversity evaluation of synthetic datasets derived from any dataset generation strategies.

\subsection{Application of Diversity Evaluation Methods}
\textbf{Quantifying Augmentation Performance.}
As data augmentation becomes an essential component in the training of deep neural networks~\citep{zhang2017mixup,park2019specaugment}, researchers gradually explore a better quantification of the quality of data augmentation. Some studies~\citep{cubuk2019randaugment} suggest that the effectiveness of data augmentation arises from the increased diversity of the data. Inspired by this observation, a series of studies have introduced diversity evaluation metrics into the performance assessment of data augmentation strategies. Specifically, they consider diversity as one aspect of evaluating the quality of augmented data, thereby determining the effectiveness of data augmentation. For instance,~\cite{gontijo2020affinity} utilize the fundamental idea that models find it more challenging to fit more diverse data, comparing metrics such as training loss and training time before and after augmentation to assess diversity. Similarly,~\cite{yang2024investigating} evaluate diversity by examining the eigenvalues and eigenvectors of the similarity matrix of samples before and after augmentation.

\textbf{Evaluating Mode Collapse.}
Generative adversarial networks (GANs)~\citep{goodfellow2020generative} suffer from a well-known phenomenon called mode collapse, which can result in a lack of diversity in the generated samples~\citep{dieng2019prescribed}. Consequently, existing studies assess mode collapse by evaluating the diversity of the generated samples. For instance, a common approach is to train an MNIST classifier and then count the number of unique classes predicted for the generated samples. Following this paradigm, VendiScore~\citep{dan2023vendi} compares the generation diversity of PresGAN~\citep{dieng2019prescribed} and Self-conditioned GAN~\citep{liu2020diverse}. Additionally, some studies~\citep{yu2017seqgan,zhu2018texygen,caccia2018language} employ different metrics to evaluate the diversity of generated samples from GANs.

\textbf{Other Applications.}
In addition to the aforementioned applications, diversity evaluation metrics have valuable applications in various areas, including sample selection for datasets~\citep{cao2023instruction}, enhancing robustness~\citep{lee2022graddiv}, and eliminating biases within datasets~\citep{huber2024bias}.

\section{Proof of Properties of \ours}
\label{apd:proof}
We theoretically confirm that \ours satisfies several intuitive axioms pointed out by previous studies~\citep{leinster2012measuring}, thereby demonstrating its superiority.  
\begin{itemize}
    \item \textbf{Effective number (Restated)}: Diversity should be defined as the effective number of samples in a dataset, ranging from 1 to $n$. \ours meets this axiom, as evidenced by its behavior: \ours equals 1 when all samples in $\mathcal{D}$ are identical and equals $n$ when all samples are distinct. 
    \begin{proof}
        For \ours, if all samples in a dataset are the same, the probability of any given sample being classified into all categories is the same, i.e., for all $i,j=\{1, 2, ..., n\}$, $\textbf{P}[i,i]=\textbf{P}[i,j]=\frac{1}{n}$. Then, we have $\operatorname{DCScore} = \sum_{i=1}^{n}{\frac{1}{n}}=1$. If all samples in the dataset are distinct, for all $i,j=\{1, 2, ..., n\}$, $\textbf{P}[i,i]=1$. In other words, the classification function confidently predicts that $\mathcal{\tilde{T}}_{i}$ belongs to the $i$-th category. Then, we have $\operatorname{DCScore}$ tending to $n$.
    \end{proof}
    
    \item \textbf{Identical samples (Restated)}: Given two identical datasets $\mathcal{D}_{1}$ and $\mathcal{D}_{2}$, the diversity of the synthetic dataset $\mathcal{D}^{'}$ generated by merging these two datasets remains unchanged. The values of \ours are the same across $\mathcal{D}_{1}$, $\mathcal{D}_{2}$, and $\mathcal{D}^{'}$, i.e.,
    \begin{equation}
    \label{eq:identical_sample}
        \operatorname{DCScore}(\mathcal{D}_{1}) = \operatorname{DCScore}(\mathcal{D}_{2}) = \operatorname{DCScore}(\mathcal{D}^{'}).
    \end{equation}
    \begin{proof}
        Assuming that $\mathcal{D}_{1}$ and $\mathcal{D}_{2}$ are completely identical, and the samples within each dataset are entirely different, i.e., $\operatorname{DCScore}(\mathcal{D}_{1})=\operatorname{DCScore}(\mathcal{D}_{2})=n$. Let $\textbf{P}=[P_{1},..., P_{n},...,P_{2n}]$ denote the probability matrix of the merged dataset $\mathcal{D}^{'}=\mathcal{D}_{1}\cup \mathcal{D}_{2}=\{\mathcal{T}_{i}\}_{i=1}^{2n}$. For $1 \leq i \leq n$, $\mathcal{T}_{i}=\mathcal{T}_{2i}$, where $\mathcal{T}_{i}\in \mathcal{D}_{1}$, $\mathcal{T}_{2i}\in \mathcal{D}_{2}$. Consequently, for each diversity-sensitive component $\mathcal{\tilde{T}}_{i}$ in $\mathcal{D}^{'}$, $\textbf{P}[i,i]=\textbf{P}[i,2i]=\frac{1}{2}$. Finally, $\operatorname{DCScore}(\mathcal{D}^{'}) = \sum_{i=1}^{2n}{\frac{1}{2}}=n$.
        
        However, the assumption that all samples in the dataset are completely different may be too stringent. We further provide a proof with a more relaxed assumption. Suppose that $\mathcal{D}_{1}$ and $\mathcal{D}_{2}$ are completely identical, with $\textbf{K}_{1}$ and $\textbf{K}_{2}$ denoting the kernel matrices for $\mathcal{D}_{1}$ and $\mathcal{D}_{2}$, respectively. In this case, we have $\textbf{K}_{1}=\textbf{K}_{2}$ as follows:
        \begin{equation}
        \label{eq:kernel_identical}
            \textbf{K}_{1} = \textbf{K}_{2} = 
            \begin{bmatrix}
                k_{1,1}^{1} & k_{1,2}^{1} & \cdots & k_{1,n}^{1} \\
                k_{2,1}^{1} & k_{2,2}^{1} & \cdots & k_{2,n}^{1} \\
                \vdots     & \vdots     & \ddots & \vdots \\
                k_{n,1}^{1} & k_{n,2}^{1} & \cdots & k_{n,n}^{1} 
                \end{bmatrix}
                =
                \begin{bmatrix}
                k_{1,1}^{2} & k_{1,2}^{2} & \cdots & k_{1,n}^{2} \\
                k_{2,1}^{2} & k_{2,2}^{2} & \cdots & k_{2,n}^{2} \\
                \vdots     & \vdots     & \ddots & \vdots \\
                k_{n,1}^{2} & k_{n,2}^{2} & \cdots & k_{n,n}^{2} 
            \end{bmatrix}
            .
        \end{equation}
        According to~\eqref{eq:cls_probability}, for the $i$-th diversity-sensitive component in $\mathcal{D}_{1}$ and $\mathcal{D}_{2}$, the probability of being classified as category $c_{i}$ can be computed as follows:
        \begin{equation}
        \label{eq:prob_identical}
            \textbf{P}_{1}[i,i]=\textbf{P}_{2}[i,i]=\frac{k_{i,i}^{1}}{\sum_{j}{k_{i,j}^{1}}}=\frac{k_{i,i}^{2}}{\sum_{j}{k_{i,j}^{2}}}. 
        \end{equation}
        For a merged dataset $\mathcal{D}^{'}=\mathcal{D}_{1}\cup \mathcal{D}_{2}=\{\mathcal{T}_{i}\}_{i=1}^{2n}$, when $1 \leq i \leq n$, we have $\mathcal{T}_{i}=\mathcal{T}_{2i}$, where $\mathcal{T}_{i}\in \mathcal{D}_{1}$, $\mathcal{T}_{2i}\in \mathcal{D}_{2}$. Since the newly added data samples do not affect the pairwise similarity, the kernel matrix $\textbf{K}^{'}$ for $\mathcal{D}^{'}$ can be formulated as follows: 
        \begin{equation}
        \label{eq:kernel_merged}
            \textbf{K}^{'} = 
            \begin{bmatrix}
                k_{1,1}^{1} & \cdots  & k_{1,n}^{1} & k_{1,1}^{2} & \cdots & k_{1,n}^{2} \\
                \vdots      & \ddots  & \vdots      & \vdots      & \ddots & \vdots      \\
                k_{n,1}^{1} & \cdots  & k_{n,n}^{1} & k_{n,1}^{2} & \cdots & k_{n,n}^{2} \\
                k_{1,1}^{2} & \cdots  & k_{1,n}^{2} & k_{1,1}^{1} & \cdots & k_{1,n}^{1} \\
                \vdots      & \ddots  & \vdots      & \vdots      & \ddots & \vdots      \\
                k_{n,1}^{2} & \cdots  & k_{n,n}^{2} & k_{n,1}^{1} & \cdots & k_{n,n}^{1} \\
            \end{bmatrix}
            .
        \end{equation}
        Analogous to~\eqref{eq:prob_identical}, for $1 \leq i \leq n$, the probability of the $i$-th diversity-sensitive component in $\mathcal{D}^{'}$ being classified as category $c_{i}$ can be computed as follows:
        \begin{equation}
        \label{eq:prob_merged}
            \begin{aligned}
                \textbf{P}^{'}[i,i]&=\frac{k_{i,i}^{1}}{\sum_{j}{k_{i,j}^{1}}+\sum_{j}{k_{i,j}^{2}}}\\
                &=\frac{k_{i,i}^{1}}{2\sum_{j}{k_{i,j}^{1}}}=\frac{k_{i,i}^{1}}{2\sum_{j}{k_{i,j}^{2}}}\\
                &=\frac{1}{2}\textbf{P}_{1}[i,i]=\frac{1}{2}\textbf{P}_{2}[i,i]. 
            \end{aligned}
        \end{equation}
        For $n+1\leq i \leq 2n$, we obtain the same result as depicted in~\eqref{eq:prob_merged}. Consequently, the diversity of $\mathcal{D}^{'}$ can be computed as follows:
        \begin{equation}
        \label{eq:dcs_merged}
            \begin{aligned}
                \operatorname{DCScore}(\mathcal{D}^{'})&=\sum_{i}^{2n}\textbf{P}^{'}[i,i]\\
                &=\frac{1}{2}\sum_{i}^{n}\textbf{P}_{1}[i,i]+\frac{1}{2}\sum_{i}^{n}\textbf{P}_{2}[i,i]\\
                &=\sum_{i}^{n}\textbf{P}_{1}[i,i]=\sum_{i}^{n}\textbf{P}_{2}[i,i]\\
                &=\operatorname{DCScore}(\mathcal{D}_{1})=\operatorname{DCScore}(\mathcal{D}_{2}). 
            \end{aligned}
        \end{equation}
    \end{proof}
    
    % \item \textbf{Partitioning}: For a dataset partitioned into several sub-datasets, the diversity of this dataset is only determined by the diversity and size of sub-datasets.
    % \begin{proof}
    % Given
    % \end{proof}

    \item \textbf{Symmetry (Restated)}: Diversity remains constant regardless of the order of the samples, exhibiting permutation invariance. Let $\pi$ denote the permutation function for the sample order, \ours remains unchanged for any sample permutation of $\mathcal{D}$, i.e., 
    \begin{equation}
    \label{eq:dcscore_sum}
        \operatorname{DCScore}(\mathcal{D}) = \operatorname{DCScore}(\pi(\mathcal{D})).
    \end{equation}
    \begin{proof}
        According to~\eqref{eq:cls_probability}, the order of samples does not affect the classification task. Thus, the diagonal elements of \textbf{P} remain unchanged, indicating the symmetry property of \ours.
    \end{proof}

    \item \textbf{Monotonicity (Restated)}: The diversity of a dataset decreases as the similarity between its samples increases. Given two datasets $\mathcal{D}_{1}$ and $\mathcal{D}_{2}$, and a new sample $\mathcal{T}_{n+1}$, where the samples in $\mathcal{D}_{1}$ and $\mathcal{D}_{2}$ are entirely different, and $\operatorname{DCScore}(\mathcal{D}_{1})=\operatorname{DCScore}(\mathcal{D}_{2})=n$. If $\mathcal{T}_{n+1}$ is more similar to the samples in $\mathcal{D}_{2}$ than to those in $\mathcal{D}_{1}$ and is added to both datasets, then for the merged datasets $\mathcal{D}_{1}^{'}$ and $\mathcal{D}_{2}^{'}$, \ours satisfies the following equation.
    \begin{equation}
        \operatorname{DCScore}(\mathcal{D}_{1}^{'}) > \operatorname{DCScore}(\mathcal{D}_{2}^{'}).
    \end{equation}
    \begin{proof}
        For $\mathcal{D}_{1}^{'}=\{\mathcal{T}_{1}^{1}, \mathcal{T}_{2}^{1}, ..., \mathcal{T}_{n}^{1},\mathcal{T}_{n+1}\}$ and $\mathcal{D}_{2}^{'}=\{\mathcal{T}_{1}^{2}, \mathcal{T}_{2}^{2}, ..., \mathcal{T}_{n}^{2}, \mathcal{T}_{n+1}\}$, we have $\operatorname{S}(\mathcal{T}_{i}^{1}, \mathcal{T}_{n+1}) < \operatorname{S}(\mathcal{T}_{j}^{2}, \mathcal{T}_{n+1})$ for any $i,j=\{1,2,...,n\}$. Here, $\operatorname{S}$ is the similarity function. In this regard, the classification function $f$ exhibits lower confidence when classifying dataset $\mathcal{D}_{2}^{'}$, resulting in a lower probability that the $i$-th sample is classified into the $i$-th class, thereby leading to $\textbf{P}_{\mathcal{D}_{1}^{'}}[i,i] > \textbf{P}_{\mathcal{D}_{2}^{'}}[i,i]$. Then, the following formula is satisfied:
        % For ease of understanding, we assume $\mathcal{D}_{1}$, $\mathcal{D}_{2}$ have $n-1$ identical samples, i.e., $\mathcal{D}_{1}=\{\mathcal{T}_{1}, \mathcal{T}_{2}, ..., \mathcal{T}_{n-1},\mathcal{T}_{n}\}$ and $\mathcal{D}_{1}=\{\mathcal{T}_{1}, \mathcal{T}_{2}, ..., \mathcal{T}_{n-1},\mathcal{T}_{n}^{'}\}$, where $\operatorname{S}(\mathcal{T}_{i}, \mathcal{T}_{n})$ \textgreater $\operatorname{S}(\mathcal{T}_{i}, \mathcal{T}_{n}^{'})$ for all $i=\{1,2,...,n-1\}$. Here, $\operatorname{S}$ is the similarity function. In this regard, for the classification of the $i$-th sample, the classification function $f$ assigns more evenly distributed logits for $\mathcal{D}_{2}$. According to~\eqref{eq:cls_diver}, in softmax, the denominator of the diagonal elements of the probability matrix for $\mathcal{D}_{2}$ is greater than that for $\mathcal{D}_{1}$, while their numerators remain the same. Therefore, the following formula is satisfied:
        \begin{equation}
            \textbf{P}_{\mathcal{D}_{1}^{'}}[i,i] > \textbf{P}_{\mathcal{D}_{2}^{'}}[i,i] \rightarrow \operatorname{DCScore}(\mathcal{D}_{1}^{'}) > \operatorname{DCScore}(\mathcal{D}_{2}^{'}),
        \end{equation}
        where $\textbf{P}_{\mathcal{D}_{1}^{'}}$, $\textbf{P}_{\mathcal{D}_{2}^{'}}$ are the probability matrix of $\mathcal{D}_{1}^{'}$, $\mathcal{D}_{2}^{'}$, respectively.
    \end{proof}
    
\end{itemize}


\section{Algorithm and Other Implementations for a Classification Perspective}
% \subsection{Algorithm of \ours}
% \label{apd:algorithm}
% We further provide the PyTorch style pseudocode for implementing \ours, as shown in algorithm~\ref{algo:dcscore}. Our proposed method includes three stages: text representation, pairwise similarity, and diversity summarization. It is worth noting that the Softmax operation can be replaced with other Softmax like additive margin Softmax~\citep{wang2018additive}. 

% \begin{figure}[h]
%     \begin{minipage}{\textwidth}
%         \vspace{-6mm}
%         \begin{algorithm}[H]
%             \caption{PyTorch style pseudocode for \ours.}
%             \label{algo:dcscore}
%             \begin{algorithmic}[0]
%                 \State \PyComment{t:\ diversity-sensitive components within the evaluated dataset}
%                 \State \PyComment{H:\ embeddings of t}
%                 \State \PyComment{K:\ kernel matrix}
%                 \State \PyComment{P:\ probability matrix}
%                 \State
%                 \State \PyComment{text representation}
%                 \State \PyCode{H = \pink{embedder}(t)}
%                 \State
%                 \State \PyComment{pairwise similarity}
%                 \State \PyCode{K = \pink{kernel}(H)}
%                 \State
%                 \State \PyComment{diversity summarization}
%                 \State \PyCode{P = \pink{softmax}(K)}
%                 \State \PyCode{\ours = \pink{Trace}(P)}
%             \end{algorithmic}
%         \end{algorithm}
%     \end{minipage}
%     \vspace{-3mm}
% \end{figure}

\subsection{Other Implementations}
\label{apd:other_imple}
% 公式9所描述的只是DCScore实现分类视角的一种方式。除此之外，DCScore还有其他具体实施方式，拥有更低的复杂度。在n远大于d的时候，对于待测数据集中的任一样本来说，其分类概率建模主要由与其比较相似的样本决定，剩余大多数样本都属于尾部概率的样本。基于这一点，我们可以仅为相似的样本之间计算逐对相似度，以此来减少计算量。因此，我们在下面给出了一个潜在的可行方案。具体地，该方法通过聚类得到聚类中心以及每个样本点的归属，而后计算同一聚类中心的样本互相之间的逐对相似度。通过聚类操作，我们筛选出了与样本点相似度最高的TOP-k个样本进行逐对相似度的计算，避免了全局逐对相似度的计算，进一步降低了方法的计算复杂度。基于上述过程，该方法可以被建模成下式：

% \eqref{eq:dcscore_unified} 
The process in Section~\ref{subsec:dcscore} represents one way to implement the classification perspective of DCScore. There are other potential implementations of DCScore with lower complexity. When $n\gg d$, the classification probability modeling for any sample within the evaluated dataset is primarily determined by samples that are relatively similar to it, while the majority of other samples fall into the tail probability category. Based on this observation, we can reduce computational cost by calculating pairwise similarity only among similar samples. Therefore, we propose a feasible approach named $\ours_{cluster}$ below.

$\ours_{cluster}$ defines similar samples as those within the same cluster. Specifically, $\ours_{cluster}$ involves obtaining cluster centers and the membership of each sample point through clustering, followed by calculating pairwise similarities among samples within the same cluster. By performing clustering, we identify similar samples for each sample point to facilitate pairwise similarity calculation, thereby avoiding the computation of global pairwise similarities and further reducing the computational complexity of the method. Based on this process, the method can be formulated as follows:
\begin{align}
    \begin{split}
    \label{eq:dcscore_other}
         &\textnormal{\textit{Text Representation: }} \textbf{H} = \Phi(\{\mathcal{\tilde{T}}_{i}\}_{i=1}^{n}), \\
         & \begin{aligned}
             \textnormal{\textit{Pairwise Similarity: }} &\mathcal{C} = \operatorname{K-means}(\textbf{H}),\\
             & \textbf{K}[i,j] = 
                \begin{cases} 
                \operatorname{Kernel}(\textbf{h}_i, \textbf{h}_j), & \text{if } \textbf{h}_i, \textbf{h}_j \in \textbf{C}_k \\
                0, & \text{if } \textbf{h}_i \in \textbf{C}_k \text{ and } \textbf{h}_j \in \textbf{C}_l \text{ with } k \neq l
                \end{cases}
         \end{aligned}
         \\
         & 
         \begin{aligned}
            \textnormal{\textit{Diversity Summarization: }} & \textbf{P}[i,j] = 
                \begin{cases} 
                f_{\textbf{K}}(\textbf{K}[i,j]), & \text{if } \textbf{h}_i, \textbf{h}_j \in \textbf{C}_k \\
                0, & \text{if } \textbf{h}_i \in \textbf{C}_k \text{ and } \textbf{h}_j \in \textbf{C}_l \text{ with } k \neq l
                \end{cases} \\
            &\operatorname{DCScore}(\mathcal{D}) = \operatorname{tr}(\textbf{P})=\sum_{i=1}^{n}{\textbf{P}[i,i]}.
         \end{aligned}
    \end{split}
\end{align}
where $\textbf{C}_k, \textbf{C}_l \in \mathcal{C}$ are clusters.

\section{Experimental Settings}
\label{apd:exp_settings}

\subsection{Datasets}
\label{apd:datasets}
Two types of generated datasets, including self-generated datasets and publicly available generated datasets, are employed in our experiments. We provide detailed information on these datasets below.

\subsubsection{Self-generated Datasets}
\label{subsubsec:dataset_details}
% dataset: set1:\tau correlation experiment and hyperparameter sensitivity (0.2-1.2, interval 0.05, total 21groups, 4 settings), set2:human evaluation （0.2-1.2, interval 0.2, total 6 groups, 4 setting, 5 seeds）,
We utilize two different self-generated datasets in three subsections: Section~\ref{subsec:corre_tau}, Section~\ref{subsec:corre_human}, and Section~\ref{subsec:paras_sens}. It is worth noting that Section~\ref{subsec:corre_tau} and Section~\ref{subsec:paras_sens} share the same experimental datasets. We employ two commonly used LLMs as our dataset generator, including Llama2-13B (13B) and Llama2-70B (70B)~\citep{touvron2023llama}. To prompt LLMs to generate datasets, we design two prompts corresponding to \textit{Zero-shot} and \textit{Few-shot} generation settings, respectively. Additionally, self-generated datasets involve two natural language generation (NLG) tasks: \textit{text classification} and \textit{story completion}. We set the maximum number of generated tokens to 100 and 30 for text classification and story completion tasks, respectively. The detailed generation information is offered as follows.

\textbf{Generation Settings.} We use different generation settings for generated datasets used in Section~\ref{subsec:corre_tau}, Section~\ref{subsec:corre_human}, and Section~\ref{subsec:paras_sens}. 
\begin{itemize}
  \item \textbf{Datasets on Section~\ref{subsec:corre_tau}, Section~\ref{subsec:paras_sens}, Appendix~\ref{subsec:impact_embedding}, and Appendix~\ref{subsec:impact_kernel}}. We generate 21 sub-datasets corresponding to different $\tau_{g}$ by varying $\tau_{g}$ from 0.2 to 1.2 with 0.05 intervals. 
  For each sub-dataset, we employ LLMs (13B or 70B) to generate sets of 10 responses per context. Specifically, each sub-dataset consists of 100 samples.
  \item \textbf{Datasets on Section~\ref{subsec:corre_human}.} We employ the 70B model to generate 6 sub-datasets corresponding to different $\tau_{g}$ by varying $\tau_{g}$ from 0.2 to 1.2 with 0.2 intervals. Each sub-dataset includes 5 samples corresponding to a context. To repeat experiments five times, we use five different contexts to prompt the 70B model to generate 5 sub-datasets for each $\tau_{g}$.
  \item \textbf{Datasets on Appendix~\ref{subsubsec:corre_task}.} In zero-shot or few-shot settings, we utilize the 70B model to generate three sub-datasets for the text classification task, corresponding to $\tau_{g}=\{0.2, 0.7, 1.2\}$, respectively. Unlike other settings that provide only one example, in this experiment, we adopt a few-shot setting where four examples and their corresponding labels are given, including two positive examples and two negative examples. Each sub-dataset contains 3,000 samples, and a context is employed to prompt the 70B model to generate five samples. To train text classification models on each sub-dataset, we randomly split 2,100 samples to the training set for each sub-dataset and gather the remaining 900 samples into the testing set across all three sub-datasets. Consequently, we construct a test set comprising 1,800 samples.
\end{itemize}

\textbf{Prompt Settings.} Following the setting of~\cite{li2023synthetic}, we design different prompts for zero-shot and few-shot settings, respectively. For the text classification task under the zero-shot setting, we require LLMs to generate movie reviews with Sci-fi/Action/Drama/Comedy/Romance topics. Each movie review contains a single sentiment of either positive or negative, which is regarded as the text label. For the story completion task, we require LLMs to complete the story according to the given context. The detailed prompt setting is provided in Table~\ref{tab:prompt}.

\begin{table}[!t]
\centering
\caption{Prompt settings for \textit{zero-shot} and \textit{few-shot} settings. Contents that need to be replaced are highlighted in \colorbox{gray!30}{gray}.}
\label{tab:prompt}
\begin{tabular}{p{2cm}|p{4cm}|p{5cm}}
\toprule
\textbf{NLG Tasks} & \textbf{Zero-shot} & \textbf{Few-shot} \\
\midrule
\textbf{Text Classification} & Now you are a movie critic. You are given a movie genre/style and a length requirement. You must come up with a movie that corresponds to the genre/style and write a review that meets the length requirement. Write a film review for a \colorbox{gray!30}{\{style\}} movie to express \colorbox{gray!30}{\{pos\_or\_neg\}} feedback. Each review should have \colorbox{gray!30}{\{num\_of\_words\}} words. Be sure to express your personal insights and feelings. Please be creative and write unique movie reviews. & Now you are a movie critic. You are given a movie genre/style and a length requirement. You must come up with a movie that corresponds to the genre/style and write a review that meets the length requirement. Write a film review according to the given example. Make sure your review expresses the same sentiment (positive or negative) as the example. Each review should have \colorbox{gray!30}{\{num\_of\_words\}} words. Be sure to express your personal insights and feelings. Please be creative and write unique movie reviews. The following is the example:\newline \colorbox{gray!30}{\#An example from IMDB}~\citep{imdb}\# \\
\midrule
\textbf{Story Completion} & Question:\colorbox{gray!30}{\{story\_q\}}\newline Answer: & Complete the story according to the given example.\newline Example: \newline\colorbox{gray!30}{\#An example from ROC Stories}~\citep{mostafazadeh2016corpus}\#\newline Question:\colorbox{gray!30}{\{story\_q\}}\newline Answer: \\
\bottomrule
\end{tabular}
\end{table}

In Table~\ref{tab:prompt}, ``\{style\}'' will be replaced with one topic within \{Sci-fi, Action, Drama, Comedy, Romance\} and ``\{pos\_or\_neg\}'' will be replaced with one label within \{Positive, Negative\}. ``\{num\_of\_words\}'' will be replaced with ``50''. ``\{story\_q\}'' will be replaced by the first three sentences of each sample in the ROC Stories dataset.

% \textbf{Natural Language Generation Task.}

\subsubsection{Publicly Available Generated Datasets}
We use SST2~\citep{socher2013recursive}, Yelp~\citep{zhang2015character}, and AG News~\citep{zhang2015character}, and their augmented version based on AttrPrompt~\citep{yu2024large}. For three original datasets, we randomly sample data from training sets and apply this to the computational cost analysis in Section~\ref{subsec:computation_cost} and Appendix~\ref{subsec:cost_larger}. For three augmented datasets, each dataset has 6000 samples. We sample different sub-datasets based on these three datasets, applied to Section~\ref{subsec:corre_tau} and Section~\ref{subsec:computation_cost}, respectively. The details are as follows.
% SST2-AttrPrompt, Yelp-AttrPrompt, and AG News-AttrPrompt
% set1:correlation experiment (sampling 1000), set2:computation cost (50tokens)
\begin{itemize}
  % \item \textbf{Datasets on Section~\ref{subsec:corre_tau}.} We randomly sample 1000 samples from each of the three datasets.
  \item \textbf{Datasets on Section~\ref{subsec:computation_cost}.} We remove samples with text token lengths less than 50 in the three datasets and then truncate each sample to a length of 50 tokens. Based on the above, we set up sub-datasets with randomly sampled samples of 100, 500, 1000, 2000, and 4000 according to experimental settings.
\end{itemize}


\subsection{Implementation details}
For three transformation-based methods, including \ours, \textit{VendiScore}, and \textit{K-means Inertia}, we employ \textit{unsup-simcse-bert-base-uncased}~\citep{sen_bert} as the embedding function. For all dataset generation language models, we set the top-p and top-k parameters to 1 and -1, respectively. Additionally, we limit the maximum number of newly generated tokens to 100 for the text classification task and 30 for the story completion task. All experiments are conducted on 8$\times$ NVIDIA Tesla V100 GPU with 32GB memory.

\subsubsection{Hyperparameter Settings of Diversity Evaluation}
% parameter settings of all diversity evaluation methods
For \ours, \textit{VendiScore}, and \textit{K-means Inertia}, we fix the batch size of generating sample representation at 128 across all experiments. Given the varying hyperparameters for each diversity evaluation method, we provide the detailed settings for each method below:

\begin{itemize}
  \item \textbf{\ours.} We employ the cosine similarity as $\operatorname{Kernel}(\cdot)$, and Softmax as $f_{\textbf{K}}(\cdot)$. Except for hyperparameter sensitivity experiments, we set $\tau$ in~\eqref{eq:cls_probability} to 1 for all other experiments. 
  \item \textbf{Distinct-n.} We use 5-grams to calculate distinct-n.
  \item \textbf{K-means Inertia.} We set the number of clusters to 10 for all experiments.
  \item \textbf{VendiScore.} We employ the cosine similarity as $\operatorname{Kernel}(\cdot)$.
\end{itemize}

\subsubsection{Hyperparameter Settings of Downstream Task Training}
\label{subsubsec:para_setting_task}
To train text classification models, we employ RoBERTa~\citep{liu2019roberta} as the encoder and utilize the representations from the last layer of the encoder as the classifier's input. We employ LoRA~\citep{hu2021lora} to finetune the encoder and the classifier on self-generated datasets. Specifically, we fix the LoRA scaling factor to 32 and the rank of the update matrices to 8. We use AdamW~\citep{loshchilov2017decoupled} with an initial learning rate of 5e-5 and linear learning rate decay as our optimizer. Additionally, we set the batch size per GPU as 32 and epochs as 120. For the number of training GPUs, we employ 8 GPUs for zero-shot settings and 4 GPUs for few-shot settings. Therefore, the different training steps for zero-shot and few-shot settings are shown in Figure~\ref{fig:loss_curve}.

\subsubsection{Evaluation protocol}
\label{apd:eval_protocol}
% Batch evaluation and Overall evaluation
In our experiments, we employ diversity evaluation methods to score the diversity of sub-datasets using two evaluation protocols: \textit{overall evaluation} and \textit{batch evaluation}. While \textit{K-means Inertia} uses the overall evaluation protocol, all other methods utilize the batch evaluation protocol. The detailed settings for the two evaluation protocols are as follows:

\begin{itemize}
    \item \textbf{Batch evaluation.} Due to a context or prompt associated with several samples in a sub-dataset, the batch evaluation protocol requires that evaluation methods treat samples generated from the same context as a single batch. The evaluation results are then averaged across all batches of the entire sub-dataset.
    \item \textbf{Overall evaluation.} We consider each sample in a sub-dataset as independent, meaning each sample is generated by a distinct context or prompt. Based on this assumption, the overall evaluation protocol requires evaluation methods to directly measure the diversity of the entire sub-dataset.
\end{itemize}

\subsubsection{Prompt Settings of LLM Evaluation}
In Section~\ref{tab:gpt_correlation}, we use GPT-4 to perform pairwise diversity comparisons. To guide GPT-4 in making these comparisons, we employ a well-designed prompt, as illustrated in Figure~\ref{fig:gpt_prompt}. The prompt for GPT-4 evaluations includes the task definition, diversity definition, general prompt, and sentence sets to be compared.

\begin{figure*}[!tb]
    \centering
    \includegraphics[width=0.95\linewidth]{./images/gpt_prompt.pdf}
    \caption{Prompt settings for GPT-4 evaluations.}
    \label{fig:gpt_prompt}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%待完成的实验（重要）%%%%%%%%%%%%%%%
%用最新的llama还有其他家的模型重新生成数据

\section{Additional Experiments}
\label{apd:addi_exp}

% \subsection{Correlation with LLM Evaluator}
% \label{subsec:gpt_evaluation}
% \begin{table}[t]
%     \centering
%     \caption{Pairwise correlation (Spearman’s $\rho$) between GPT-4, temperature ($\tau_{g}$), and \ours. \ours indicates a strong correlation with GPT-4 evaluation results.}
%     \label{tab:gpt_correlation}
%     \renewcommand\arraystretch{1.1}
%     \resizebox{0.8\textwidth}{!}{
%         \begin{tabular}{c|cccc}
%             \toprule
%             & \textbf{Story-Few} & \textbf{Story-Zero} & \textbf{Text-Few} & \textbf{Text-Zero} \\
%             \midrule
%             GPT-4-\ours & 0.6057$_{\pm 0.30}$ & 0.9010$_{\pm 0.04}$ & 0.6131$_{\pm 0.18}$ & 0.9052$_{\pm 0.09}$ \\
%             $\tau_{g}$-\ours & 0.6757$_{\pm 0.30}$ & 0.8782$_{\pm 0.08}$ & 0.5714$_{\pm 0.27}$ & 0.9336$_{\pm 0.06}$ \\
%             $\tau_{g}$-GPT-4 & 0.9086$_{\pm 0.07}$ & 0.7829$_{\pm 0.16}$ & 0.8400$_{\pm 0.16}$ & 0.8971$_{\pm 0.07}$ \\
%             \bottomrule
%         \end{tabular}}
% \end{table}

% To further verify the effectiveness of \ours, we investigate the evaluation correlation between \ours and LLMs. Following the setting in Section~\ref{subsec:corre_human}, we employ GPT-4 to conduct pairwise comparisons between two generated datasets with different $\tau_{g}$. It is worth noting that these generated datasets are identical to those used in Section~\ref{subsec:corre_human}. Based on the pairwise comparison results, we obtain the diversity ranking outcomes. Regarding GPT-4 evaluation results as the diversity pseudo-truth, we report the pairwise evaluation correlation between \ours, GPT-4, and $\tau_{g}$ in Table~\ref{tab:gpt_correlation}. We observe that \ours exhibits strong correlations with GPT-4 and $\tau_{g}$ in zero-shot settings. By comparing the results of ``$\tau_{g}$-\ours'' and ``$\tau_{g}$-GPT-4'', we find that \ours outperforms the GPT-4 evaluator in terms of correlation with $\tau_{g}$ in zero-shot settings. Regarding the correlation performance in few-shot settings, we notice lower correlations of all baseline methods compared to zero-shot settings. We guess that this phenomenon is related to the distributions of the generated datasets. Although \ours exhibits lower correlations (about 0.6) with GPT-4, this result can still be considered a strong correlation according to~\cite{akoglu2018user}.

\subsection{Computational Cost for larger datasets}
\label{subsec:cost_larger}
In the case where the inner product is used as the kernel function and $n\gg d$, \textit{VendiScore} can significantly reduce computational complexity. To ensure a fair comparison, we compare the computation times of \textit{VendiScore} and \ours on larger datasets, as well as under different kernel functions. Specifically, we employ SST2, Yelp, and AG News as the evaluated datasets. We randomly sample {4k, 8k, 16k, 32k, 64k} samples and record the computation times for both methods across these different sample sizes. We repeat the experiments 5 times to report the mean and standard deviation.

As shown in Tables~\ref{tab:cost_sst2},~\ref{tab:cost_Yelp}, and \ref{tab:cost_AG_News}, \ours has a shorter computation time than \textit{VendiScore} in most cases, with \textit{VendiScore} only exhibiting a computational advantage when the inner product is used and $n\gg d$. Furthermore, as the sample size increases, the efficiency advantage of \ours becomes more pronounced. When using a polynomial kernel, on the SST2 dataset, \ours requires only one-third of the computation time of \textit{VendiScore} when the sample size reaches 64k. In contrast, although \textit{VendiScore} has a computational advantage in the case of the inner product, the difference compared to \ours is not significant. The experimental results are consistent with our complexity analysis presented in Section~\ref{subsec:complexity_ana}. Overall, \ours outperforms \textit{VendiScore} in terms of computation time across most kernel functions. \textit{VendiScore} exhibits a computational time advantage only when the inner product is used as the kernel function, which will limit its applicability. As shown in Chapter 4 of~\cite{seeger2004gaussian}, it is essential to employ different kernel functions to accommodate a wider range of scenarios.

% \begin{table}[!t]
% \centering
% \caption{Comparison of computation time between \ours and VendiScore on SST2.}
% \label{tab:cost_sst2}
% \small
% \renewcommand{\arraystretch}{1.2}
% \resizebox{0.95\textwidth}{!}{
% \begin{tabular}{ccccccc}
% \toprule
% \multirow{2}{*}{\textbf{Kernels}} & \multicolumn{1}{l}{} & \multicolumn{5}{c}{\textbf{SST2}}                                                                                              \\
% \cmidrule{2-7}
%                                  & \textbf{Sample num}           & \textbf{4k}                   & \textbf{8k}                    & \textbf{16k}                   & \textbf{32k}                   & \textbf{64k}                    \\
%                                  \midrule
% \multirow{2}{*}{Inner product}   & VendiScore           & 4.65$_{\pm 0.28}$          & \textbf{9.84$_{\pm 0.26}$}  & \textbf{19.02$_{\pm 0.70}$} & \textbf{37.31$_{\pm 1.88}$} & \textbf{76.19$_{\pm 1.91}$}  \\
%                                  & \ours              & \textbf{4.58$_{\pm 0.29}$} & 10.03$_{\pm 0.17}$          & 20.42$_{\pm 0.39}$          & 42.91$_{\pm 1.59}$          & 112.47$_{\pm 2.43}$          \\
% \multirow{2}{*}{RBF kernel}      & VendiScore           & 5.86$_{\pm 0.06}$          & 12.41$_{\pm 0.49}$          & 32.94$_{\pm 0.40}$          & 100.36$_{\pm 1.44}$         & 449.14$_{\pm 10.35}$         \\
%                                  & \ours              & \textbf{5.22$_{\pm 0.33}$} & \textbf{9.94$_{\pm 0.42}$}  & \textbf{21.20$_{\pm 0.75}$} & \textbf{46.57$_{\pm 1.47}$} & \textbf{117.06$_{\pm 1.91}$} \\
% \multirow{2}{*}{Poly kernel}     & VendiScore           & 5.73$_{\pm 0.06}$          & 12.72$_{\pm 0.41}$          & 31.47$_{\pm 0.97}$          & 98.31$_{\pm 0.25}$          & 453.11$_{\pm 2.53}$          \\
%                                  & \ours              & \textbf{5.09$_{\pm 0.28}$} & \textbf{10.27$_{\pm 0.12}$} & \textbf{20.12$_{\pm 1.02}$} & \textbf{46.25$_{\pm 1.82}$} & \textbf{123.51$_{\pm 3.40}$} \\
%                                  \bottomrule
% \end{tabular}}
% \end{table}

\begin{table}[!t]
\centering
\caption{Comparison of computation time between \ours and VendiScore on Yelp.}
\label{tab:cost_Yelp}
\small
\renewcommand{\arraystretch}{1.2}
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccc}
\toprule
\multirow{2}{*}{\textbf{Kernels}} & \multicolumn{1}{l}{} & \multicolumn{5}{c}{\textbf{Yelp}}                                                                                              \\
\cmidrule{2-7}
                                 & \textbf{Sample num}           & \textbf{4k}                   & \textbf{8k}                    & \textbf{16k}                   & \textbf{32k}                   & \textbf{64k}                    \\
                                 \midrule
\multirow{2}{*}{Inner product}   & VendiScore           & 57.96$_{\pm 0.35}$          & \textbf{114.64$_{\pm 1.63}$} & \textbf{227.76$_{\pm 7.04}$} & \textbf{451.49$_{\pm 19.73}$} & \textbf{912.60$_{\pm 25.69}$} \\
                                 & \ours              & \textbf{57.95$_{\pm 0.31}$} & 115.35$_{\pm 1.16}$          & 232.49$_{\pm 1.34}$          & 448.98$_{\pm 23.94}$          & 961.29$_{\pm 2.86}$           \\
\multirow{2}{*}{RBF kernel}      & VendiScore           & 59.31$_{\pm 0.06}$          & 118.15$_{\pm 0.91}$          & 242.06$_{\pm 7.60}$          & 527.99$_{\pm 2.89}$           & 1272.93$_{\pm 21.15}$         \\
                                 & \ours              & \textbf{58.49$_{\pm 0.14}$} & \textbf{116.29$_{\pm 0.92}$} & \textbf{232.94$_{\pm 3.09}$} & \textbf{471.18$_{\pm 7.80}$}  & \textbf{953.62$_{\pm 17.21}$} \\
\multirow{2}{*}{Poly kernel}     & VendiScore           & 59.48$_{\pm 0.05}$          & 118.94$_{\pm 0.95}$          & 234.08$_{\pm 11.72}$         & 522.82$_{\pm 3.04}$           & 1313.55$_{\pm 12.64}$         \\
                                 & \ours              & \textbf{58.73$_{\pm 0.08}$} & \textbf{117.02$_{\pm 0.90}$} & \textbf{227.72$_{\pm 9.51}$} & \textbf{462.45$_{\pm 13.91}$} & \textbf{988.53$_{\pm 1.10}$} \\
                                 \bottomrule
\end{tabular}}
\end{table}

\begin{table}[!t]
\centering
\caption{Comparison of computation time between \ours and VendiScore on AG News.}
\label{tab:cost_AG_News}
\small
\renewcommand{\arraystretch}{1.2}
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccc}
\toprule
\multirow{2}{*}{\textbf{Kernels}} & \multicolumn{1}{l}{} & \multicolumn{5}{c}{\textbf{AG News}}                                                                                              \\
\cmidrule{2-7}
                                 & \textbf{Sample num}           & \textbf{4k}                   & \textbf{8k}                    & \textbf{16k}                   & \textbf{32k}                   & \textbf{64k}                    \\
                                 \midrule
\multirow{2}{*}{Inner product}   & VendiScore           & \textbf{14.56$_{\pm 1.16}$} & \textbf{30.20$_{\pm 1.15}$} & 63.70$_{\pm 1.39}$ & \textbf{127.25$_{\pm 1.13}$} & \textbf{254.20$_{\pm 11.71}$} \\
                                 & \ours              & 14.61$_{\pm 1.15}$ & 30.61$_{\pm 1.77}$          & \textbf{63.57$_{\pm 2.68}$} & 129.70$_{\pm 4.17}$          & 284.76$_{\pm 12.30}$          \\
\multirow{2}{*}{RBF kernel}      & VendiScore           & 16.69$_{\pm 1.54}$          & 33.69$_{\pm 1.47}$          & 80.09$_{\pm 2.34}$          & 185.79$_{\pm 6.44}$          & 617.06$_{\pm 12.51}$          \\
                                 & \ours              & \textbf{16.01$_{\pm 1.53}$} & \textbf{31.06$_{\pm 0.96}$} & \textbf{69.15$_{\pm 1.32}$} & \textbf{129.36$_{\pm 5.56}$} & \textbf{297.29$_{\pm 3.67}$}  \\
\multirow{2}{*}{Poly kernel}     & VendiScore           & 17.60$_{\pm 0.62}$          & 36.16$_{\pm 1.27}$          & 79.34$_{\pm 1.57}$          & 190.96$_{\pm 2.75}$          & 632.69$_{\pm 10.14}$          \\
                                 & \ours              & \textbf{16.88$_{\pm 0.59}$} & \textbf{33.78$_{\pm 1.28}$} & \textbf{68.18$_{\pm 1.66}$} & \textbf{138.18$_{\pm 3.82}$} & \textbf{303.06$_{\pm 11.40}$} \\
                                 \bottomrule
\end{tabular}}
\end{table}

\subsection{Correlation with Downstream Task Training}
\label{apd:corre_task}

% \begin{table}[]
%     \centering
%     \caption{Downstream task training performance and diversity evaluation on self-generated datasets with $\tau_{g}=\{0.2, 0.7, 1.2\}$.}
%     \label{tab:task_train}
%     \renewcommand\arraystretch{1.0}
%         \begin{tabular}{c|ccc|ccc}
%         \toprule
%         \multirow{2}{*}{} & \multicolumn{3}{c|}{\textbf{Accuracy}} & \multicolumn{3}{c}{\textbf{DCScore}} \\
%                           & \textbf{$\tau_{g}$=0.2}    & \textbf{$\tau_{g}$=0.7}   & \textbf{$\tau_{g}$=1.2}   & \textbf{$\tau_{g}$=0.2}   & \textbf{$\tau_{g}$=0.7}   & \textbf{$\tau_{g}$=1.2}   \\
%                           \midrule
%         \textbf{Zero-shot}         & 89.10     & 89.70    & 90.37   & 481.76  & 1745.42 & 2082.42 \\
%         \textbf{Few-shot}         & 70.07    & 73.19   & 72.44   & 1376.43 & 1958.16 & 2047.90 \\
%         \bottomrule
%         \end{tabular}
% \end{table}

To investigate the correlation between \ours and downstream task training, we train text classification models using self-generated datasets under zero-shot and few-shot settings. We vary the generation temperature $\tau_{g}$ of self-generated datasets within the range of $\{0.2, 0.7, 1.2\}$. More details of training datasets and hyperparameters are presented in Appendix~\ref{subsubsec:dataset_details} and~\ref{subsubsec:para_setting_task}, respectively. Figure~\ref{fig:loss_curve} shows the loss curves of these trained classification models. In the zero-shot setting, we observe increasing optimal loss values as $\tau_{g}$ varied from 0.2 to 1.2, indicating that the model is more easily fitted to datasets with limited diversity. However, as shown in Table~\ref{tab:task_train}, models trained on more diverse datasets achieve better accuracy, which can be attributed to their enhanced generalization capabilities. From Table~\ref{tab:task_train}, the diversity evaluated by \ours has a similar trend to the accuracy performance in the zero-shot setting, further demonstrating the effectiveness of \ours in diversity evaluation.

% By analyzing the training sets under the two settings, we find that the differences between samples within the dataset under the few-shot setting are greater, the quality of the data is also higher, and the model requires more training epochs to fit the training dataset.
% \begin{wrapfigure}{r}{0.6\linewidth}
%     \centering
%     \vspace{-4mm}
%     \includegraphics[width=\linewidth]{images/loss_curve_1125.pdf}
%     \vspace{-5mm}
%     \caption{\textcolor{blue}{Loss curves of the downstream task training.}}
%     \vspace{-4mm}
%     \label{fig:loss_curve}
% \end{wrapfigure}

\begin{figure*}[!tb]
    \centering
    \includegraphics[width=0.7\linewidth]{images/loss_curve_1125.pdf}
    \caption{Loss curves of the downstream task training.}
    \label{fig:loss_curve}
\end{figure*}
In the few-shot setting, we observe a trend in optimal loss variation similar to that in the zero-shot setting, as shown in Figure~\ref{fig:loss_curve}. However, as shown in Table~\ref{tab:task_train}, the performance of the model trained on the dataset generated with $\tau_{g}=1.2$ is inferior to that of the model trained on the dataset generated with $\tau_{g}=0.7$, which contrasts with the findings in the zero-shot setting. This phenomenon can be attributed to the higher diversity of datasets generated at a higher $\tau_{g}$, resulting in increased fitting difficulty. Under the current settings, the number of training epochs for the dataset generated at a temperature of 1.2 is insufficient, preventing the trained model from achieving optimal performance. To validate this hypothesis, we increase the number of epochs to 240 and 360 and train models on the dataset generated at a temperature of 1.2. The final training loss and accuracy of these models are shown in Figure~\ref{fig:loss_curve_acc}. We observe that as the number of epochs increases, the model's loss gradually decreases, and its performance improves progressively. Ultimately, the model's accuracy outperforms that of models trained on datasets generated at temperatures of 0.2 and 0.7. Moreover, from Table~\ref{tab:task_train}, models trained on datasets from the zero-shot setting outperform those trained on datasets from the few-shot setting. However, this discrepancy arises from the different test sets used in the two settings, making direct performance comparisons inappropriate.
% \begin{wrapfigure}{r}{0.6\linewidth}
%     \centering
%     \vspace{-4mm}
%     \includegraphics[width=\linewidth]{images/loss_curve_acc_1104.pdf}
%     \vspace{-5mm}
%     \caption{Loss curves and accuracy of models trained on generated dataset with $\tau_{g}=1.2$.}
%     \vspace{-4mm}
%     \label{fig:loss_curve_acc}
% \end{wrapfigure}

\begin{figure*}[!tb]
    \centering
    \includegraphics[width=0.7\linewidth]{images/loss_curve_acc_1104.pdf}
    \caption{Loss curves and accuracy of models trained on generated dataset with $\tau_{g}=1.2$.}
    \label{fig:loss_curve_acc}
\end{figure*}

\subsection{Impact of Embedding Functions $\Phi$}
\label{subsec:impact_embedding}
The paradigm of the transformation-based method enables \ours to utilize various embedding functions tailored to different scenarios. Consequently, we investigate the impact of embedding functions on self-generated datasets used in Section~\ref{subsec:corre_tau}. As shown in Table~\ref{tab:embedding_impact}, we compare the correlation of the diversity evaluation results of \ours across 4 different embedding functions with diversity pseudo-truths, where the model names in parentheses within the embedding function refer to those available on Hugging Face. Our findings indicate that \ours exhibits strong correlations with diversity pseudo-truths across various embedding functions. Notably, \ours utilizing the BGE embedding function achieves the best results in half of the cases. Additionally, the minimum correlation in Table~\ref{tab:embedding_impact} exceeds 0.96, which is classified as a strong correlation according to~\cite{akoglu2018user}. This result also supports the following two conclusions: (1) the embedding function used effectively captures the differences among samples in the dataset from multiple perspectives, and (2) \ours is sufficiently adaptable to different embedding functions while maintaining stable performance.

\begin{table}[!t]
    \centering
    \caption{Correlation (Spearman’s $\rho$) results of \ours with various embedding models. Spearman’s $\rho$ varies between -1 and +1 with 0 implying no correlation. Best results are indicated in \textbf{bold}.}
    \label{tab:embedding_impact}
    \renewcommand\arraystretch{1.2}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cccc|cccc}
    \toprule
    \multirow{3}{*}{\textbf{Embedding models}} & \multicolumn{4}{c|}{\textbf{\textit{Zero-shot setting}}} & \multicolumn{4}{c}{\textbf{\textit{Few-shot setting}}} \\ 
    % \cmidrule(l){2-9}
                                      & \multicolumn{2}{c}{\textbf{Text classification}} & \multicolumn{2}{c|}{\textbf{Story completion}} & \multicolumn{2}{c}{\textbf{Text classification}} & \multicolumn{2}{c}{\textbf{Story completion}} \\
    \cmidrule(l){2-9}
                                      & 13B & 70B & 13B & 70B & 13B & 70B & 13B & 70B \\ \midrule
    \textbf{SimCSE (unsup-simcse-bert-base-uncased)} & \textbf{0.9961}         & 0.9779                 & 0.9844                & 0.9792                & \textbf{0.9909}         & 0.9883                 & 0.9857                & \textbf{0.9974}       \\
\textbf{SimCSE (sup-simcse-roberta-large)}       & 0.9909                  & 0.9753                 & 0.9883                & 0.9883                & 0.9792                  & \textbf{0.9935}        & 0.9779                & 0.9623                \\
\textbf{Sentence BERT (all-mpnet-base-v2)}       & 0.9896                  & 0.9740                 & 0.9870                & 0.9909                & 0.9766                  & 0.9870                 & 0.9857                & 0.9870                \\
\textbf{BGE (bge-large-en-v1.5)}                 & 0.9909                  & \textbf{0.9896}        & \textbf{0.9922}       & \textbf{0.9948}       & 0.9857                  & 0.9922                 & \textbf{0.9870}       & 0.9922 \\ \bottomrule
    \end{tabular}}
\end{table}


\subsection{Impact of Kernel Functions}
\label{subsec:impact_kernel}

Similar to Appendix~\ref{subsec:impact_embedding}, we investigate the impact of different kernel functions on the performance of \ours. Specifically, this experimental setup is identical to that in Appendix~\ref{subsec:impact_embedding}. As shown in Table~\ref{tab:kernel_impact}, we find that \ours demonstrates stable performance across various kernel functions. However, the influence of the kernel function is slightly more pronounced than that of the embedding function, as indicated by the greater fluctuations in correlation among the different kernel functions. Furthermore, we observe that \ours achieves optimal performance in the case of the inner product. Overall, \ours consistently maintains strong diversity evaluation performance across different kernel functions.

\begin{table}[!t]
    \centering
    \caption{Correlation (Spearman’s $\rho$) results of \ours with various kernel functions. Spearman’s $\rho$ varies between -1 and +1 with 0 implying no correlation. Best results are indicated in \textbf{bold}.}
    \label{tab:kernel_impact}
    \renewcommand\arraystretch{1.2}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cccc|cccc}
    \toprule
    \multirow{3}{*}{\textbf{Embedding models}} & \multicolumn{4}{c|}{\textbf{\textit{Zero-shot setting}}} & \multicolumn{4}{c}{\textbf{\textit{Few-shot setting}}} \\ 
    % \cmidrule(l){2-9}
                                      & \multicolumn{2}{c}{\textbf{Text classification}} & \multicolumn{2}{c|}{\textbf{Story completion}} & \multicolumn{2}{c}{\textbf{Text classification}} & \multicolumn{2}{c}{\textbf{Story completion}} \\
    \cmidrule(l){2-9}
                                      & 13B & 70B & 13B & 70B & 13B & 70B & 13B & 70B \\ \midrule
    \textbf{Inner product}                     & \textbf{0.9961}         & 0.9779                 & 0.9844                & \textbf{0.9792}       & \textbf{0.9909}         & \textbf{0.9883}        & \textbf{0.9857}       & \textbf{0.9974}       \\
\textbf{laplacian kernel}                  & 0.9935                  & \textbf{0.9831}        & 0.9883                & 0.9727                & 0.9597                  & 0.9649                 & 0.9701                & 0.9922                \\
\textbf{RBF kernel}                        & 0.9935                  & 0.9818                 & \textbf{0.9896}       & 0.9753                & 0.9740                  & 0.9727                 & 0.9792                & 0.9922                \\
\textbf{polynomial kernel}                 & 0.9870                  & 0.9584                 & 0.9714                & 0.9506                & 0.9182                  & 0.9182                 & 0.9857                & 0.9896 \\ \bottomrule
    \end{tabular}}
\end{table}


% \section{Complexity Analysis}
% \label{subsec:complexity_ana}

% \begin{table}[!t]
% \centering
% \caption{Comparison of Complexity analysis between \ours and VendiScore. $\mathcal{O}_{kernel}$ represents the complexity of the kernel function.}
% \label{tab:complexity_analysis}
% \renewcommand\arraystretch{1.3}
% \resizebox{0.9\textwidth}{!}{
% \begin{tabular}{cccc}
% \toprule\
%                          &                      & \textbf{General Kernels}    & \textbf{Inner Product}                                     \\
% \midrule
% \multirow{2}{*}{\textbf{Pairwise Similarity}} & \textbf{Vendi Score} & \multirow{2}{*}{$\mathcal{O}(n^2\cdot\mathcal{O}_{kernel})$}  & \multicolumn{1}{c}{$\mathcal{O}(d^2n)$} \\
%                                               & \textbf{\ours}     &                          & $\mathcal{O}(n^2d)$                                                             \\
%                                               \cmidrule(l){2-4}
% \multirow{2}{*}{\textbf{Summarization}}       & \textbf{Vendi Score} & $\mathcal{O}(n^3)$    & \multicolumn{1}{c}{$\mathcal{O}(d^{3})$}                                           \\
%                                               & \textbf{\ours}     & $\mathcal{O}(n^2)$                           & $\mathcal{O}(n^2)$                                          \\
%                                               \cmidrule(l){2-4}
% \multirow{2}{*}{\textbf{Total}}               & \textbf{Vendi Score} & $\mathcal{O}(n^2\cdot\mathcal{O}_{kernel}+n^3)$ & $\mathcal{O}(d^2n+d^{3})=\mathcal{O}(d^2n)$             \\
%                                               & \textbf{\ours}     & $\mathcal{O}(n^2\cdot\mathcal{O}_{kernel}+n^2)$                     & $\mathcal{O}(n^2d+n^2)$   \\
%                                               \bottomrule
% \end{tabular}}
% \end{table}

% To enhance our understanding of \ours, we provide a brief analysis of its time complexity in Table~\ref{tab:complexity_analysis}. Denoting $\mathcal{O}_{kernel}$ as the complexity associated with general kernels (i.e., kernels other than linear kernels), we analyze the complexity in the pairwise similarity and summarization stages. In the pairwise similarity stage, the computation of pairwise similarities results in a complexity of $\mathcal{O}(n^2)$ for \ours. When combined with the complexity $\mathcal{O}_{kernel}$ of the general kernel computation, \ours exhibits a total complexity of $\mathcal{O}(n^2\cdot \mathcal{O}_{kernel})$ in this stage. In the summarization stage, \ours has a complexity of $\mathbf{O}(n^2)$ due to the softmax operation. Consequently, the overall complexity of \ours for general kernels is $\mathcal{O}(n^2\cdot \mathcal{O}_{kernel}+n^2)$. In contrast, \textit{VendiScore} has a total complexity of $\mathcal{O}(n^2\cdot \mathcal{O}_{kernel}+n^3)$, where the pairwise similarity stage is identical to that of \ours, while the summarization stage incurs a complexity of $\mathcal{O}(n^3)$ due to the eigenvalue computation. Thus, for general kernels, \ours demonstrates lower complexity than \textit{VendiScore}.

% However, when the inner product is employed as the kernel function and $n\gg d$, \textit{VendiScore} can significantly reduce the complexity by replacing the pairwise similarity $XX^T$ with $X^TX$, where $X\in \mathbb{R}^{n\times d}$. This results in complexities of $\mathcal{O}(d^2n)$ for the pairwise similarity stage and $\mathcal{O}(d^3)$ for the summarization stage. In this regard, \ours has a complexity of $\mathcal{O}(n^2d+n^2)$, which is slightly worse than that of \textit{VendiScore}. Fortunately, we can leverage efficient techniques, such as those proposed in \cite{shim2017svd} and \cite{wen2023pairwise}, to reduce the computational complexity of \ours. Overall, compared to \textit{VendiScore}, \ours maintains lower complexity in most cases, as empirically validated in Section~\ref{subsec:computation_cost} and Appendix~\ref{subsec:cost_larger}. Although \textit{VendiScore} has a lower complexity when the inner product is used as the kernel, experiments in the Appendix~\ref{subsec:cost_larger} show that the computation times of \textit{VendiScore} and \ours are quite similar. Additionally, in practical applications, different kernels can maintain low computational complexity, which is more useful than being restricted to a single kernel.


\section{Detailed Modeling of Existing Methods}
\label{sec:detailed_modeling}
We present the detailed modeling of existing methods into \ours as follows:

\textbf{Distinct-n.} \textit{Distinct-n}~\citep{li2015diversity} is a prevalent diversity metric depending on n-grams, where \textit{n} signifies the number of successive items. \textit{Distinct-n} calculates the proportion of unique n-grams to all n-grams. The n-grams operation falls under the text representation stage, while the step of obtaining a unique set of n-grams corresponds to the pairwise similarity stage. Typically, a high form similarity among samples in the evaluated dataset results in a smaller unique n-grams set. Finally, ratio calculations belong to the diversity summarization stage.
\begin{align}
    \begin{split}
    \label{eq:distinctn_unified}
         &\textnormal{\textit{Text Representation: }} \operatorname{n-grams}(\operatorname{Concat}(\mathcal{D})), \\
         &\textnormal{\textit{Pairwise Similarity: }} \operatorname{Unique}(\operatorname{n-grams}(\operatorname{Concat}(\mathcal{D}))), \\
         &\textnormal{\textit{Diversity Summarization: }} \operatorname{Distinct-n}(\mathcal{D}) = \frac{|\operatorname{Unique}(\operatorname{n-grams}(\operatorname{Concat}(\mathcal{D})))|}{|\operatorname{n-grams}(\operatorname{Concat}(\mathcal{D}))|},
    \end{split}
\end{align}
where $\operatorname{n-grams}$, $\operatorname{Unique}$, and $\operatorname{Concat}$ represent the n-grams, de-overlap process, and concatenate operation, respectively.

\textbf{K-means inertia.} \textit{K-means inertia}~\citep{du2019boosting}, a transformation-based method, performs clustering in sample representation and then calculates inertia as diversity outcomes. Here, inertia refers to the square summarization of the distance between samples and cluster centroids.
\begin{align}
    \begin{split}
    \label{eq:inertia_unified}
         &\textnormal{\textit{Text Representation: }} \textbf{H} = \Phi(\{\mathcal{\tilde{T}}_{i}\}_{i=1}^{n}), \\
         &\textnormal{\textit{Pairwise Similarity: }} \mathcal{C} = \operatorname{K-means}(\textbf{H}), \\
         &\textnormal{\textit{Diversity Summarization: }} \operatorname{Inertia}(\mathcal{D}) = \sum_{\textbf{c}_{k}\in \mathcal{C}, \textbf{h}_{j}\in \textbf{H}_{\textbf{c}_{k}}}{(\textbf{h}_{j}-\textbf{c}_{k})^2},
    \end{split}
\end{align}
where \textbf{H} is the representation of all samples and $\textbf{h}_{i}$ is the representation of the $i$-th sample, $\mathcal{C}$ denotes the cluster centroid set, and $\textbf{c}_{k} \in \mathcal{C}$ represents the $k$-th cluster centroid. The sample representation associated with the $k$-th cluster centroid is expressed as $\textbf{h}_{j}\in \textbf{H}_{\textbf{c}_{k}}$, while $\textbf{H}_{\textbf{c}_{k}}$ denotes the sample representations within the $k$-th cluster centroid.

\textbf{VendiScore.} \textit{VendiScore}~\citep{dan2023vendi} is a recently proposed diversity metric that falls under the category of the transformation-based method. Based on sample representations, VendiScore utilizes a kernel function to calculate similarity matrix $\textbf{K}$. Subsequently, VendiScore summarizes diversity as the exponential of the Shannon entropy of the eigenvalues of $\textbf{K}/n$.
\begin{align}
    \begin{split}
    \label{eq:vendi_unified}
         &\textnormal{\textit{Text Representation: }} \textbf{H} = \Phi(\{\mathcal{\tilde{T}}_{i}\}_{i=1}^{n}), \\
         &\textnormal{\textit{Pairwise Similarity: }} \textbf{K} = \operatorname{Kernel}(\textbf{H}), \\
         &\textnormal{\textit{Diversity Summarization: }} \operatorname{VS}(\mathcal{D}) = \exp{(-\sum_{i=1}^{n}\lambda_{i}\log\lambda_{i})},
    \end{split}
\end{align}
where $\operatorname{Kernel}(\cdot)$ is the kernel function, such as the cosine similarity, $\lambda_{i}$ is the $i$-th eigenvalue of $\textbf{K}/n$.

% \subsubsection{Correlation with Other Diversity Pseudo-truth}


% \subsubsection{Impact of Additive Margin Softmax}


% \subsubsection{Impact of Evaluation Protocol}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
