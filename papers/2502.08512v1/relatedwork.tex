\section{Related Work}
We give a brief literature review of diversity evaluation methods. Moreover, limited by the space, further related works on LLM dataset generators can be found in Appendix~\ref{apd:add_related_work}. 
% prompt-guided
% 1.Identifying Citizen-Related Issues from Social Media Using LLM-Based Data Augmentation
% 2.Mini-DA: Improving Your Model Performance through Minimal Data Augmentation using LLM （选取训练时分类错误的样本增强）
% 3.Data Augmentation for Text-based Person Retrieval Using Large Language Models
% 4.LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition （multi-level）
% 5.Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations
% 6.Zero-Shot Stance Detection using Contextual Data Generation with LLMs
% 7.AugGPT: Leveraging ChatGPT for Text Data Augmentation
% 8.Is GPT-3 a Good Data Annotator?
% 9.GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation
% 10.Large Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching
% 11.Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dataset-guided
% 1.MixSumm: Topic-based Data Augmentation using LLMs for Low-resource Extractive Text Summarization
% 2.ZEROGEN: Efficient Zero-shot Learning via Dataset Generation
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%instruct-guided
% 1.Can LLMs Augment Low-Resource Reading Comprehension Datasets? Opportunities and Challenges
% 2.Generative Data Augmentation using LLMs improves Distributional Robustness in Question Answering
% 3.Empowering Large Language Models for Textual Data Augmentation
% 4.Learning Preference Model for LLMs via Automatic Preference Data Generation
% 5.Mixture of Soft Prompts for Controllable Data Generation
% 6.SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions
% 7.TarGEN: Targeted Data Generation with Large Language Models
% 8.A Synthetic Data Approach for Domain Generalization of NLI Models
% 9.CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP

%I2O:
% 1.CHATGPT OUTPERFORMS CROWD-WORKERS FOR TEXT-ANNOTATION TASKS
% 2.GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation

% Generally speaking, a dataset is a set of input-output samples, where a sample consists of a input text and a output text. According to the generation task, existing methods using LLMs as a dataset generator can be summarized into three categories: InputToOutput (\textit{I2O}), OutputToInput (\textit{O2I}), and SeedToSample (\textit{S2S}). 

% Specifically, I2O methods take the input text (e.g., question) as the input of LLMs generator and obtain the output text (e.g., answer) corresponding to the input text. In text classification, I2O can be referred to as label annotation, which is the most popular way for LLMs to be used as the dataset generator. For example,~\citep{ding2022gpt} designs task-specific prompts to annotate unlabeled data. To further exert the capacity of LLMs, ZeroGen~\citep{ye2022zerogen} generates a training set using LLMs and then trains a task-specific classifier to annotate data labels. AugGPT~\citep{dai2023auggpt} has the same framework as ZeroGen but given samples and their labels in the training set generation stage. conversely, O2I methods aim to generate the input text, e.g., leveraging LLMs to generate diverse input texts related to the given label. For example, xxx. In most cases, we always have a portion of samples with labels, which is known as seed samples. In this regard, given several seed samples as example, S2S methods directly generate input-output samples, which is employed to generate datasets for text classification, story completion and so on. Specifically, xxx. 

% s2s:
% 1.TarGEN: Targeted Data Generation with Large Language Models
% 2.SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions
% 3.A Synthetic Data Approach for Domain Generalization of NLI Models
% 4.CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP

%其他：Empowering Large Language Models for Textual Data Augmentation 

\subsection{Diversity Evaluation Methods}
\textbf{N-gram-based Methods.} With the development of LLMs as dataset generators, the diversity evaluation of synthetic datasets has become a challenging task and remains under-explored in recent evaluation studies~\citep{liang2022holistic,AlpacaEval}. The most comparable diversity evaluation research can be traced back to studies in NLP and ML, which can be summarized into the n-gram-based method~\citep{mishra2020dqi}, reference-based method~\citep{heusel2017gans}, and transformation-based method~\citep{lai2020diversity}. The n-gram-based method is the most popular lexical diversity evaluation method, leveraging n-grams to capture differences in sentence form~\citep{yu2024large}. Commonly used n-gram-based diversity metrics include distinct n-grams (\textit{distinct-n})~\citep{song2024scaling}, self-BLEU~\citep{shu2019generating}, and ROUGE-L~\citep{wang2022self,padmakumar2023does}. However, this method has limitations, as it overlooks differences in other aspects such as semantics and style.

\textbf{Reference-based Methods.} Diversity evaluation is inherently subjective, leading to a reliance on human judgment. Consequently, the reference-based method evaluates diversity by comparing the distribution of the evaluated data to that of a reference dataset~\citep{heusel2017gans}. MAUVE~\citep{pillutla2021mauve} exemplifies this idea by employing a divergence-based metric to capture correlations with human judgment. Regarding the natural language inference (NLI) training set as the reference dataset,~\citep{stasaski2022semantic} first trains an NLI model to infer the relationship between pairs of generated texts and then calculates diversity based on these inference results. Due to the challenges in collecting reference datasets, a recent study~\citep{le2024exploring} proposes evaluating diversity through precision and recall. Despite these advancements, the reference-based method remains significantly constrained because of the need for reference datasets.

\textbf{Transformation-based Methods.} The transformation-based~\citep{lee2023beyond} method leverages well-designed models to generate representations of the evaluated data. Then, the diversity of these representations is summarized using techniques such as eigenvalue computation~\citep{dan2023vendi} and clustering~\citep{du2019boosting}. 
Owing to the superior performance of representation learning, this method considers various aspects of the evaluated data, including semantics, form, and style, offering greater flexibility compared to the other two methods. However, its dependence on high-complexity summarization techniques, such as eigenvalue computation, limits its scalability for comprehensive evaluation tasks, such as evaluating the diversity of synthetic datasets. 

In summary, existing methods primarily focus on NLP and ML fields and are challenging to apply directly to overall dataset diversity evaluation. Different from the above-mentioned studies, our work is dedicated to the holistic diversity evaluation of synthetic datasets. Additionally, to ensure flexible evaluation, our work aims to evaluate diversity-sensitive components that impact the performance of trained models in terms of diversity.

% \begin{CJK}{UTF8}{gbsn}
% \textbf{场景设定与我们相似的论文}
% \begin{itemize}
%     \item Diversity, Density, and Homogeneity: Quantitative Characteristic Metrics for Text Collections
%     \item Beyond Scale: The Diversity Coefficient as a Data Quality Metric for Variability in Natural Language Data
%     \item Exploring Precision and Recall to assess the quality and diversity of LLMs
%     \item Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification Prompting
% \end{itemize}
% \end{CJK}

% \begin{CJK}{UTF8}{gbsn}
% \textbf{可参考的一些论文}
% \begin{itemize}
%   \item Evaluating the Evaluation of Diversity in Natural Language Generation：多样性指标评估的论文（实验设置可参考）
%   \item Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning：提升LLMs生成内容多样性的方法，文中提到了三种diversity测量方法，可以分别归类到n-gram和transformation方法
%   \item Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation：研究文本多样性激励措施对于生成数据集训练出来的模型性能的影响(确定具体场景可参考)
% \end{itemize}
% \end{CJK}

% \begin{CJK}{UTF8}{gbsn}
% \textbf{关于LLMs作为dataset generator的文章：}
% \begin{itemize}
% \item 1. Synthetic dialogue dataset generation using llm agents
% \item 2. Zerogen: Efficient zero-shot learning via dataset generation
% \item 3. \textbf{TarGEN: Targeted Data Generation with Large Language Models}
% \item 4. Data augmentation using llms: Data perspectives, learning paradigms and challenges
% \item 5. Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions
% \end{itemize}
% \end{CJK}


% \subsubsection{N-gram-based method}
% \begin{CJK}{UTF8}{gbsn}
% \begin{itemize}
%   \item DQI: Measuring Data Quality in NLP
%   \item Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias
%   \item Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment
%   \item Does Writing with Language Models Reduce Content Diversity
%   \item A Diversity-Promoting Objective Function for Neural Conversation Models
%   \item SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions
% \end{itemize}
% \end{CJK}


% \subsubsection{Reference-based method}
% \begin{CJK}{UTF8}{gbsn}
% \begin{itemize}
%   \item Semantic Diversity in Dialogue with Natural Language Inference (训练NLI模型的数据集是否可以看做Reference或者NLI分类器本身是否就可以看做是Reference)
%   \item Exploring Precision and Recall to assess the quality and diversity of LLMs
%   \item GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium
% \end{itemize}
% \end{CJK}


% \subsubsection{Transformation-based method}
% \begin{CJK}{UTF8}{gbsn}
% \begin{itemize}
%   \item Diversity, Density, and Homogeneity:
% Quantitative Characteristic Metrics for Text Collections
%   \item Boosting Dialog Response Generation
%   \item Beyond Scale: The Diversity Coefficient as a Data Quality Metric for Variability in Natural Language Data
%   \item Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries
%   \item Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification Prompting
% \end{itemize}
% \end{CJK}