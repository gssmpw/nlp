\section{Related Work}
We give a brief literature review of diversity evaluation methods. Moreover, limited by the space, further related works on LLM dataset generators can be found in Appendix~\ref{apd:add_related_work}. 
% prompt-guided
% 1.Identifying Citizen-Related Issues from Social Media Using LLM-Based Data Augmentation
% 2.Mini-DA: Improving Your Model Performance through Minimal Data Augmentation using LLM （选取训练时分类错误的样本增强）
% 3.Data Augmentation for Text-based Person Retrieval Using Large Language Models
% 4.LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition （multi-level）
% 5.Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations
% 6.Zero-Shot Stance Detection using Contextual Data Generation with LLMs
% 7.AugGPT: Leveraging ChatGPT for Text Data Augmentation
% 8.Is GPT-3 a Good Data Annotator?
% 9.GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation
% 10.Large Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching
% 11.Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dataset-guided
% 1.MixSumm: Topic-based Data Augmentation using LLMs for Low-resource Extractive Text Summarization
% 2.ZEROGEN: Efficient Zero-shot Learning via Dataset Generation
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%instruct-guided
% 1.Can LLMs Augment Low-Resource Reading Comprehension Datasets? Opportunities and Challenges
% 2.Generative Data Augmentation using LLMs improves Distributional Robustness in Question Answering
% 3.Empowering Large Language Models for Textual Data Augmentation
% 4.Learning Preference Model for LLMs via Automatic Preference Data Generation
% 5.Mixture of Soft Prompts for Controllable Data Generation
% 6.SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions
% 7.TarGEN: Targeted Data Generation with Large Language Models
% 8.A Synthetic Data Approach for Domain Generalization of NLI Models
% 9.CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP

%I2O:
% 1.CHATGPT OUTPERFORMS CROWD-WORKERS FOR TEXT-ANNOTATION TASKS
% 2.GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation

% Generally speaking, a dataset is a set of input-output samples, where a sample consists of a input text and a output text. According to the generation task, existing methods using LLMs as a dataset generator can be summarized into three categories: InputToOutput (\textit{I2O}), OutputToInput (\textit{O2I}), and SeedToSample (\textit{S2S}). 

% Specifically, I2O methods take the input text (e.g., question) as the input of LLMs generator and obtain the output text (e.g., answer) corresponding to the input text. In text classification, I2O can be referred to as label annotation, which is the most popular way for LLMs to be used as the dataset generator. For example, Gu et al., "1.Identifying Citizen-Related Issues from Social Media Using LLM-Based Data Augmentation" designs task-specific prompts to annotate unlabeled data. To further exert the capacity of LLMs, ZeroGen Liu et al., "2.ZEROGEN: Efficient Zero-shot Learning via Dataset Generation" generates a training set using LLMs and then trains a task-specific classifier to annotate data labels. AugGPT Zhang et al., "7.AugGPT: Leveraging ChatGPT for Text Data Augmentation" has the same framework as ZeroGen but given samples and their labels in the training set generation stage. conversely, O2I methods aim to generate the input text, e.g., leveraging LLMs to generate diverse input texts related to the given label. For example, xxx. In most cases, we always have a portion of samples with labels, which is known as seed samples. In this regard, given several seed samples as example, S2S methods directly generate input-output samples, which is employed to generate datasets for text classification, story completion and so on. Specifically, xxx. 

% s2s:
% 1.TarGEN: Targeted Data Generation with Large Language Models
% 2.SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions
% 3.A Synthetic Data Approach for Domain Generalization of NLI Models
% 4.CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP

%其他：Empowering Large Language Models for Textual Data Augmentation 

\subsection{Diversity Evaluation Methods}
\textbf{N-gram-based Methods.} With the development of LLMs as dataset generators, the diversity evaluation of synthetic datasets has become a challenging task and remains under-explored in recent evaluation studies Gu et al., "1.Identifying Citizen-Related Issues from Social Media Using LLM-Based Data Augmentation". The most comparable diversity evaluation research can be traced back to studies in NLP and ML, which can be summarized into the n-gram-based method Liu et al., "Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment", reference-based method Zhang et al., "Exploring Precision and Recall to assess the quality and diversity of LLMs", and transformation-based method Li et al., "Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries". The n-gram-based method is the most popular lexical diversity evaluation method, leveraging n-grams to capture differences in sentence form Gu et al., "1.Identifying Citizen-Related Issues from Social Media Using LLM-Based Data Augmentation". Commonly used n-gram-based diversity metrics include distinct n-grams (\textit{distinct-n}) Liu et al., "Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment", self-BLEU Zhang et al., "Exploring Precision and Recall to assess the quality and diversity of LLMs", and ROUGE-L Li et al., "Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries". However, this method has limitations, as it overlooks differences in other aspects such as semantics and style.

\textbf{Reference-based Methods.} Diversity evaluation is inherently subjective, leading to a reliance on human judgment. Consequently, the reference-based method evaluates diversity by comparing the distribution of the evaluated data to that of a reference dataset Zhang et al., "Exploring Precision and Recall to assess the quality and diversity of LLMs". 

\textbf{Transformation-based method} Li et al., "Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries"