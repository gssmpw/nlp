@misc{AlpacaEval,
      author = "tatsu-lab",
      title = "AlpacaEval : An Automatic Evaluator for Instruction-following Language Models",
      year = "2023",
      url = "https://github.com/tatsu-lab/alpaca_eval?tab=readme-ov-file#evaluators"
}

@article{dai2023auggpt,
  title={Auggpt: Leveraging chatgpt for text data augmentation},
  author={Dai, Haixing and Liu, Zhengliang and Liao, Wenxiong and Huang, Xiaoke and Cao, Yihan and Wu, Zihao and Zhao, Lin and Xu, Shaochen and Liu, Wei and Liu, Ninghao and others},
  journal={arXiv preprint arXiv:2302.13007},
  year={2023}
}

@article{dan2023vendi,
  title={The vendi score: A diversity evaluation metric for machine learning},
  author={Dan Friedman, Dan and Dieng, Adji Bousso},
  journal={Transactions on machine learning research},
  year={2023}
}

@article{ding2022gpt,
  title={Is gpt-3 a good data annotator?},
  author={Ding, Bosheng and Qin, Chengwei and Liu, Linlin and Chia, Yew Ken and Joty, Shafiq and Li, Boyang and Bing, Lidong},
  journal={arXiv preprint arXiv:2212.10450},
  year={2022}
}

@inproceedings{du2019boosting,
  title={Boosting dialog response generation},
  author={Du, Wenchao and Black, Alan W},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}

@article{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{lai2020diversity,
  title={Diversity, density, and homogeneity: Quantitative characteristic metrics for text collections},
  author={Lai, Yi-An and Zhu, Xuan and Zhang, Yi and Diab, Mona},
  journal={arXiv preprint arXiv:2003.08529},
  year={2020}
}

@inproceedings{le2024exploring,
  title={Exploring Precision and Recall to assess the quality and diversity of LLMs},
  author={Le Bronnec, Florian and V{\'e}rine, Alexandre and Negrevergne, Benjamin and Chevaleyre, Yann and Allauzen, Alexandre},
  booktitle={62nd Annual Meeting of the Association for Computational Linguistics},
  year={2024}
}

@article{lee2023beyond,
  title={Beyond scale: the diversity coefficient as a data quality metric demonstrates llms are pre-trained on formally diverse data},
  author={Lee, Alycia and Miranda, Brando and Sundar, Sudharsan and Koyejo, Sanmi},
  journal={arXiv preprint arXiv:2306.13840},
  year={2023}
}

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}

@article{mishra2020dqi,
  title={Dqi: Measuring data quality in nlp},
  author={Mishra, Swaroop and Arunkumar, Anjana and Sachdeva, Bhavdeep and Bryan, Chris and Baral, Chitta},
  journal={arXiv preprint arXiv:2005.00816},
  year={2020}
}

@article{padmakumar2023does,
  title={Does Writing with Language Models Reduce Content Diversity?},
  author={Padmakumar, Vishakh and He, He},
  journal={arXiv preprint arXiv:2309.05196},
  year={2023}
}

@article{pillutla2021mauve,
  title={Mauve: Measuring the gap between neural text and human text using divergence frontiers},
  author={Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan and Thickstun, John and Welleck, Sean and Choi, Yejin and Harchaoui, Zaid},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4816--4828},
  year={2021}
}

@inproceedings{shu2019generating,
  title={Generating diverse translations with sentence codes},
  author={Shu, Raphael and Nakayama, Hideki and Cho, Kyunghyun},
  booktitle={Proceedings of the 57th annual meeting of the association for computational linguistics},
  pages={1823--1827},
  year={2019}
}

@article{song2024scaling,
  title={Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment},
  author={Song, Feifan and Yu, Bowen and Lang, Hao and Yu, Haiyang and Huang, Fei and Wang, Houfeng and Li, Yongbin},
  journal={arXiv preprint arXiv:2403.11124},
  year={2024}
}

@article{stasaski2022semantic,
  title={Semantic diversity in dialogue with natural language inference},
  author={Stasaski, Katherine and Hearst, Marti A},
  journal={arXiv preprint arXiv:2205.01497},
  year={2022}
}

@article{wang2022self,
  title={Self-instruct: Aligning language models with self-generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@article{ye2022zerogen,
  title={Zerogen: Efficient zero-shot learning via dataset generation},
  author={Ye, Jiacheng and Gao, Jiahui and Li, Qintong and Xu, Hang and Feng, Jiangtao and Wu, Zhiyong and Yu, Tao and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2202.07922},
  year={2022}
}

@article{yu2024large,
  title={Large language model as attributed training data generator: A tale of diversity and bias},
  author={Yu, Yue and Zhuang, Yuchen and Zhang, Jieyu and Meng, Yu and Ratner, Alexander J and Krishna, Ranjay and Shen, Jiaming and Zhang, Chao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

