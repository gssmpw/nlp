%%%%%%%% arXiv LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

\usepackage{multirow}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{arxiv2025} with \usepackage[nohyperref]{arxiv2025} above.
\usepackage{hyperref}

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{arxiv2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{arxiv2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\NewDocumentCommand{\shuiwang}
{ mO{} }{\textcolor{blue}{\textsuperscript{\textit{Shuiwang Ji}}\textsf{\textbf{\small[#1]}}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \arxivtitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\arxivtitlerunning{A Materials Foundation Model via Hybrid Invariant-Equivariant Architectures}

\begin{document}

\twocolumn[
\arxivtitle{A Materials Foundation Model via Hybrid Invariant-Equivariant Architectures}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the arxiv2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\arxivsetsymbol{equal}{*}

\begin{arxivauthorlist}
\arxivauthor{Keqiang Yan}{cs,equal}
\arxivauthor{Montgomery Bohde}{cs,equal}
\arxivauthor{Andrii Kryvenko}{cs,equal}
\arxivauthor{Ziyu Xiang}{ece}
\arxivauthor{Kaiji Zhao}{mse}
\arxivauthor{Siya Zhu}{mse}
\arxivauthor{Saagar Kolachina}{mse}
\arxivauthor{Doğuhan Sarıtürk}{mse}
\arxivauthor{Jianwen Xie}{lambda}
\arxivauthor{Raymundo Arróyave}{mse,meen,isen}
\arxivauthor{Xiaoning Qian}{cs,ece,bnl}
\arxivauthor{Xiaofeng Qian}{mse,ece,phys}
\arxivauthor{Shuiwang Ji}{cs}
\end{arxivauthorlist}

\arxivaffiliation{cs}{Department of Computer Science \& Engineering, Texas A\&M University, College Station, TX, USA}
\arxivaffiliation{mse}{Department of Materials Science \& Engineering, Texas A\&M University, College Station, TX, USA}
\arxivaffiliation{ece}{Department of Electrical \& Computer Engineering, Texas A\&M University, College Station, TX, USA}
\arxivaffiliation{lambda}{Lambda Inc., USA}
\arxivaffiliation{meen}{J. Mike Walker '66 Department of Mechanical Engineering, Texas A\&M University, College Station, TX, USA}
\arxivaffiliation{isen}{Wm Michael Barnes '64 Department of Industrial and Systems Engineering, Texas A\&M University, College Station, TX, USA}
\arxivaffiliation{phys}{Department of Physics \& Astronomy, Texas A\&M University, College Station, TX, USA}
\arxivaffiliation{bnl}{Computing \& Data Sciences, Brookhaven National Laboratory, Upton, NY, USA}

\arxivcorrespondingauthor{Keqiang Yan}{keqiangyan@tamu.edu}
\arxivcorrespondingauthor{Shuiwang Ji}{sji@tamu.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\arxivkeywords{Machine Learning, Materials, Foundation Models, Equivariance}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \arxivEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\arxivEqualContribution} % otherwise use the standard text.

\begin{abstract}
Materials foundation models can predict energy, force, and stress of materials and enable a wide range of downstream discovery tasks. A key design choice involves the trade-off between invariant and equivariant architectures. Invariant models offer computational efficiency but may not perform well when predicting high-order outputs. In contrast, equivariant models can capture high-order symmetries, but are computationally expensive. In this work, we propose HIENet, a \underline{h}ybrid \underline{i}nvariant-\underline{e}quivariant foundation model that integrates both invariant and equivariant message passing layers. HIENet is designed to achieve superior performance with considerable computational speedups over prior models. Experimental results on both common benchmarks and downstream materials discovery tasks demonstrate the efficiency and effectiveness of HIENet.
\end{abstract}

\section{Introduction}
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.77\textwidth]{dive_figures/overview-v2.pdf}
    % \vspace{-0.1in}
    \caption{HIENet overview and materials property prediction pipeline. The model converts material structures into graph representations and processes them through a hybrid architecture combining invariant and equivariant message passing networks to predict physical properties. The model supports accurate dynamic simulations (bottom) and enables diverse materials science applications (right).}
    \label{fig:overview}
    %\vspace{-0.15in}
\end{figure*}

The discovery of materials with desired properties underpins a wide range of technological advancements~\cite{de2019new,stach2021autonomous}, including improving semiconductor~\cite{shafian2025development}, enabling more efficient renewable energy storage~\cite{lv2022machine}, developing new sensing technologies~\cite{zheng2021smart}, and enabling many other engineering applications~\cite{miracle2024autonomous}. However, traditional materials discovery relies heavily on the expertise, intuition and innovation of materials scientists, often employing time-consuming and costly trial-and-error experimental methods. Over the past two decades, computational approaches, particularly those leveraging advanced quantum mechanical methods like first-principles density functional theory (DFT), have accelerated this process~\cite{zhang2023artificial}. Despite their benefits, these methods come with significant computational costs, scaling from $O(n_e^3)$ to $O(n_e^7)$, where $n_e$ is number of electrons in a materials system. This steep scaling renders high-throughput screening across the vast space of materials challenging. Simulating systems with a large number of atoms with quantum mechanical methods becomes extremely expensive with current approaches.

Recent progress in materials foundation models offers a promising path forward by enabling the prediction of energies, forces, and stresses of materials, achieving significant speedups compared to traditional DFT methods. However, these existing foundation models still face a fundamental trade-off: invariant models are computationally efficient but struggle with high-order property predictions and incorporating physical constraints, while equivariant models capture high-order interactions better but are computationally expensive, making scaling to larger model sizes challenging and expensive. Additionally, model predictions must adhere to key physical constraints: energy should remain invariant under global symmetry operations such as translation, rotation, and reflection, while forces and stresses must be equivariant under the same operations. Moreover, physical laws impose additional requirements, including force equilibrium $\sum \mathbf{F}_i = \mathbf{0}$ when no external influences applied, force conservation $\mathbf{F} = - \nabla E$ ($E$ is total energy), and stress tensor symmetry $\sigma_{ij} = \sigma_{ji}$.

In this work, we propose HIENet, a materials foundation model that satisfies key physical constraints for energy, force, and stress predictions while integrating invariant and equivariant designs to achieve state-of-the-art (SOTA) performance with significant computational speedups compared to existing models. An overview of HIENet is provided in Figure~\ref{fig:overview}. Unlike prior approaches that rely exclusively on either invariant or equivariant layers, HIENet balances these strategies to improve both performance and efficiency. Specifically, HIENet leverages the scalability of invariant designs to increase model size and capacity while utilizing equivariant designs to effectively capture high-order interactions and symmetries, which are critical for learning forces accurately, as demonstrated by our experiments. Moreover, in contrast to existing models like EquiformerV2~\cite{liao2024equiformerv} and Orb~\cite{neumann2024orb}, HIENet rigorously satisfies physical constraints, including O(3) equivariance for force and stress, and adheres to physical conservation laws through physics-informed derivative-based methods. Experimental results on common benchmarks including Materials Project Trajectory and Matbench Discovery, and downstream materials discovery tasks including evaluations on phonon band structures, bulk modulus, \textit{ab initio} molecular dynamics, and alloys as detailed in Sec.~\ref{sec:phonons}, \ref{sec:Ab_initio}, and \ref{sec:HEA} demonstrate the efficiency and effectiveness of HIENet.


\section{Preliminaries and Related Work}
The development of machine learning force fields (MLFFs) requires carefully balancing physical constraints, computational efficiency, and model expressivity. Key challenges arise from the need to preserve symmetries while enabling fast predictions of material properties. In this section, we first formalize the fundamental prediction tasks and outline the unique challenges of crystal structures and their dynamics, followed by an examination of existing MLFF approaches and their limitations.


\subsection{Problem Definition} 

The core task in developing MLFFs is to learn a mapping from atomic structures to quantum mechanical properties while preserving fundamental physical symmetries. Given a crystal structure, we aim to predict three quantities; the total energy $E$, indicating system stability, atomic forces $\bm{F} = \{\bm{F}_i \in \mathbb{R}^3, 1 \le i \le n\}$, where $n$ denotes number of atoms in a cell, driving structural evolution, and the stress tensor $\bm{\sigma} \in \mathbb{R}^{3 \times 3}$ governing cell deformation. These predictions must satisfy key physical constraints: energy should be invariant under global symmetry operations such as translation, rotation, and reflection, while forces and stress must be equivariant under these operations. Additionally, physical conservation laws require force equilibrium $\sum \bm{F}_i = \mathbf{0}$ when no influences applied, force conservation $\bm{F} = - \nabla E$, and stress tensor symmetry $\sigma_{ij} = \sigma_{ji}$.


\textbf{3D crystal structures}.
Unlike regular molecules, crystals are periodic in nature and are characterized as three-dimensional lattices with indefinitely repeating unit cells. Adopting the notation of \citet{yan2024complete}, a crystal structure can be described as a triple $\mathcal{M} = (\bm{Z}, \bm{P}, \bm{L})$, which represents both atomic and geometric information. The atomic composition is denoted by $\bm{Z} = [{z}_1, {z}_2,\cdots, {z}_n] \in \mathbb{R}^n$, where each ${z}_i$ represents the atomic number of $i$-th atom in the unit cell. The arrangement of these atoms in the Euclidean space is given by 3D coordinates  $\bm{P} = [\bm{p}_1, \bm{p}_2,\cdots, \bm{p}_n] \in \mathbb{R}^{3 \times n}$. The periodicity of the unit cell is specified by the lattice matrix $\bm{L} = [\bm{\ell}_1, \bm{\ell}_2, \bm{\ell}_3] \in \mathbb{R}^{3 \times 3}$, whose columns are the three lattice vectors. These vectors can be used to form an infinite crystal structure that can be expressed formally as a pair of two infinite sets:
$\hat{\bm{P}} = \{\hat{\bm{p}}_i|\ \hat{\bm{p}}_i = \bm{p}_i + k_1\boldsymbol{\ell}_1 + k_2\bm{\ell}_2 + k_3\bm{\ell}_3,
k_1, k_2, k_3 \in \mathbb{Z}, i \in \mathbb{Z}, 1 \leq i \leq n\}, 
\hat{\bm{Z}} = \{\hat{{z}}_i|\hat{z}_i = {z}_i, i \in \mathbb{Z}, 1 \leq i \leq n\},$
where $\hat{\bm{P}}$ represents atomic positions in the infinite crystal structure, and $\hat{\bm{Z}}$ defines their corresponding atomic numbers.


\textbf{Molecular dynamics simulation and structural optimization for materials}.
MLFFs can be applied to perform molecular dynamics simulation and optimize the structure of materials. Molecular dynamics simulation is an important method in computational materials science which provides insights about structural, chemical, and thermodynamic properties and allows for in-depth mechanistic understanding and materials discovery. Molecular dynamics simulation solves Newton's equations of motion for both atomic positions and cell parameters of a material system under a specific thermodynamic ensemble. Specifically, the simulation workflow relies on iterative computation of the total system energy $E$, atomic forces $\bm{F}_i$, and stress tensor $\bm{\sigma}$. MLFFs can also be applied to optimize structures through iterative energy minimization. Details on the molecular dynamics simulation and the structural optimization are provided in Appendix~\ref{app:dynamics}. 

%and accurate computation of the total system energy $E$, atomic forces through spatial derivatives $\mathbf{F}_i = -\nabla_{\mathbf{p}_i} E$, and stress components through strain derivatives $\sigma_{ij} = \frac{1}{V}\frac{\partial E}{\partial \epsilon_{ij}}$, where $V$ and $\bm{\epsilon}$ denote the volume and strain tensor of the simulation cell, respectively. MLFFs can also be applied to optimize structure through iterative energy minimization. Details on molecular dynamics simulations and structural optimization are provided in Appendix~\ref{app:dynamics}. 


\subsection{Related Work}

In this section, we focus on machine learning-based interatomic potentials and provide related works in conventional computation methods in Appendix~\ref{app:related_traditional}.


Recent advances in materials property prediction models~\cite{xie2018crystal, choudhary2021atomistic, yan2022periodic,lin2023efficient,choudhary2024jarvis,yan2024complete} and the availability of high-quality materials dynamics datasets~\cite{chen2022universal,deng2023chgnet, barroso2024open} generated using DFT-based algorithms have facilitated the development of materials foundation models capable of predicting energy, force, and stress. Among these models, purely invariant designs, such as M3GNet~\cite{chen2022universal}, CHGNet~\cite{deng2023chgnet}, Orb~\cite{neumann2024orb}, and EScAIP~\cite{quimportance}, are computationally efficient, but typically do not perform as well, particularly for predicting higher order tensors such as force and stress. While some invariant models such as Orb~\cite{neumann2024orb}, and EScAIP~\cite{quimportance} are able to achieve competitive performance on benchmarks, this does not not necessarily translate to downstream tasks, as shown in Sec. \ref{sec:HEA}. In contrast, equivariant designs, including MACE~\cite{batatia2023foundation}, SevenNet~\cite{park2024scalable}, and EquiformerV2~\cite{barroso2024open}, respect physical constraints like equivariance. However, they are computationally expensive due to the extensive use of tensor product operations, which limits their scalability. Additionally, some equivariant models still do not obey other important physical laws, undermining their performance on realistic material system simulations as seen in Sec. \ref{section:experiments}.

Different from these prior foundation models, our proposed HIENet detailed in Sec.~\ref{sec:HIENet} satisfies all key physical constraints while combining the scalability and efficiency of invariant designs with the robustness and symmetry-capturing capabilities of equivariant designs. This novel integration offers a promising direction for the next generation of materials foundation model design.


\section{Hybrid Invariant-Equivariant Networks}
\label{sec:HIENet}
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{dive_figures/IENet.pdf}
    \vspace{-0.25in}
    \caption{HIENet Model Architecture. We construct an O(3) equivariant crystal graph representation and embed node and edge features. We then apply an invariant message passing layer followed by several equivariant message passing layers before predicting the total energy, $\hat{E}$ and using physical laws to compute $\bm{\hat{F}}$, $\bm{\hat{\sigma}}$.}
    \label{fig:model}
    \vspace{-0.15in}
\end{figure*}

Our HIENet model architecture is based on the ComFormer model ~\cite{yan2024complete}, with several important changes to the model architecture and crystal graph construction. We emphasize that our purpose in this work is to develop a materials foundation model and demonstrate the importance of including both invariant and equivariant message passing layers, not to analyze the specific designs of these layers, as this has been extensively studied by previous works~\cite{gasteiger2021gemnet, Batatia2022Design, liao2024equiformerv}. A detailed diagram of the HIENet architecture can be seen in Figure \ref{fig:model}. In this section we only provide an overview of the key differences from ComFormer and justifications for these changes. See Appendix \ref{appendix:model_overview} for a more detailed description of our HIENet model architecture.

\subsection{Physical Constraints} 

In order for MLFF models to be practical and robust for downstream tasks, they need to ensure various physical symmetries. Specifically, for force prediction, models need to ensure forces form a conservative vector field and that the forces on each atom sum to zero excluding friction or external forces. For stress prediction, they must ensure the stress tensor $\bm{\sigma}$ is symmetric. Finally, models need to ensure that each of these predictions transforms appropriately under physical transformations of the crystal system. %Some models such as EquiformerV2 \citep{barroso2024open} and EScAIP \citep{qu2024importance} do not enforce these physical symmetries, and while these models perform well on benchmarks, they underperform on downstream applications, as shown in Sec. \ref{section:experiments}.

While models such as EquiformerV2 \citep{barroso2024open} and Orb \citep{neumann2024orb} are able to achieve good performance on benchmarks, these results do not translate to downstream molecular dynamics stimulation tasks because the model predictions do not obey basic physical laws, as show in Sec. \ref{section:experiments}.

\textbf{Gradient-Based Calculations.} In order to ensure that our force and stress predictions obey the aforementioned physical constraints, we use gradient-based methods to compute force and stress. Specifically, our model directly predicts the total energy, $\hat{E}$ and we compute the force acting on atom $i$ as:

\begin{equation}
    \bm{\hat{F}}_i = -\nabla_{\bm{p}_i}\hat{E}
\end{equation}

where $\nabla_{\bm{p}_i}$ represents the gradient with respect to the position vector $\bm{p}_i$.

\begin{proposition} \label{prop:conserve}
    HIENet predictions $\bm{\hat{F}}_i$ form a conservative vector field. 
\end{proposition}

\begin{proposition} \label{prop:equilibrium}
    HIENet predictions satisfy force equilibrium $\sum_{i=1}^N\bm{\hat{F}}_i = \bm{0}$ when no external influences applied.
\end{proposition}

Proofs of Props. \ref{prop:conserve}, \ref{prop:equilibrium} are in Appendix \ref{appendix:proofs}. We compute the stress tensor as:
\begin{equation}
    \hat{\sigma}_{ij} = \frac{1}{V} \frac{\partial \hat{E}}{\partial \epsilon_{ij}}
\end{equation}
where $\bm{\epsilon}$ is the lattice strain tensor and $V$ is the volume of the unit cell. We ensure that $\bm{\hat{\sigma}}$ will be symmetric by first symmetrizing the strain matrix $\bm{\epsilon}_\text{sym} = \frac{1}{2}(\bm{\epsilon} + \bm{\epsilon}^\top)$. 

\textbf{O(3) Equivariance.} Beyond physical constraints on forces and stress, we also want force and stress tensors to transform appropriately under rotations and reflections of the input material. HIENet achieves O(3) equivariance by using the gradient-based approach to compute force and stress. Prior works \cite{chen2022universal, deng2023chgnet} have shown that even invariant models can produce equivariant force and stress predictions using this method. By relying on these physics-informed calculations rather than directly predicting forces and stress, our model strictly satisfies all required physical constraints.

Importantly, we use a different graph construction than ComFormer, and exclude additional periodic encodings that cause ComFormer to be SO(3) equivariant. O(3) equivariance is important for model predictions to be physically meaningful as the model predictions should rotate accordingly under reflections of the crystal system. Additionally, the underlying DFT algorithm is O(3) equivariant. We provide an ablation study on O(3) vs SO(3) equivariance in Appendix \ref{appendix:ablations}. 

\subsection{Hybrid Invariant-Equivariant Design}

\textbf{Scalability.} The key difference of HIENet compared to previous works is that we apply both E(3) invariant and O(3) equivariant message passing layers. We find that using both invariant and equivariant layers improves performance and enables more efficient scaling of MLFF models. 
%Specifically, by replacing some equivariant message passing layers with invariant message passing, we can actually improve prediction accuracy \emph{and} model speed. 
In Sec. \ref{section:combining_layers} we provide a thorough analysis and ablations to support this observation. 

\textbf{Computational Efficiency.} To further improve the scalability of our model, we remove several operations from ComFormer that we found did not significantly impact performance. Specifically, we found that edge convolution layers did not impact the performance. We also redesigned the eComFormer equivariant message passing layer to use only one tensor product operation instead of two and added skip connections to enable easier optimization of deeper models. Additional ablations of our model compared to ComFormer can be found in Appendix \ref{appendix:ablations}. 

\section{Experimental Evaluations} \label{section:experiments}
In this section, we evaluate HIENet through a series of comprehensive experiments. We first assess its performance on the Materials Project Trajectory (MPtrj) dataset~\citep{deng2023chgnet} and Matbench Discovery benchmark~\citep{riebesell2023matbench} in Sec.~\ref{section:mptrj} and \ref{section:matbench}. We then analyze its computational efficiency compared to existing approaches in Sec.~\ref{section:efficiency}. To demonstrate practical utility, we evaluate HIENet on phonon band structure and bulk modulus calculations in Sec.~\ref{sec:phonons}, \textit{ab initio} molecular dynamics simulations in Sec.~\ref{sec:Ab_initio}, and alloy systems in Sec.~\ref{sec:HEA}. Finally, we provide detailed ablation studies on the benefits of combining invariant and equivariant layers in Sec.~\ref{section:combining_layers}. In these experiments we show that HIENet achieves superior performance while significantly improving computational efficiency compared to competing models. Detailed model settings and training details can be found in Appendix~\ref{appendix:implementation}.

\subsection{Materials Project Trajectory Dataset} \label{section:mptrj}

We train and evaluate our HIENet foundation model on the MPtrj dataset ~\citep{deng2023chgnet}, which contains 1.58M crystal structures. We split the dataset and use 95\% of the data for training and 5\% for validation following ~\citep{batatia2023foundation}. In order to have a fair comparison, we only compare with models trained on this dataset and without any auxiliary losses. As seen in Table \ref{table:mptrj}, HIENet achieves state-of-the-art performance across train and validation splits. Notably, HIENet reduces the energy mean absolute error~(MAE) by nearly 50\% and the force MAE by 23\% compared to the previous state-of-the-art EquiformerV2.

\begin{table}[h!]
\renewcommand{\arraystretch}{1.1}
\caption{\small Mean absolute errors on train and validation splits for models trained on the MPtrj dataset. Inv. and Eqv. denote whether the model uses invariant or equivariant message passing layers, respectively. Best performing model in \textbf{bold} and second best \underline{underlined}. MACE-MP-0 does not report stress performance.}
\label{table:mptrj}
\vspace{0.1in}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cc|ccc}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{Inv.} & \multirow{2}{*}{Eqv.} & Energy $\downarrow$ & Forces $\downarrow$ & Stress $\downarrow$ \\
 & & & (meV/atom) & (meV/Å) & (kBar) \\
\midrule
Train & & & & \\
\midrule 
SevenNet-0 & \xmark & \cmark & 11.5 & 41 & 2.78\\
SevenNet-l3i5 & \xmark & \cmark & \underline{8.3} & \underline{29} & \underline{2.33} \\
%HIENet-l2 & \cmark & \cmark & \textbf{5.86} & \underline{21.64} & \textbf{1.90} \\
HIENet & \cmark & \cmark & \textbf{5.91} & \textbf{20.76} & \textbf{1.95}\\
\midrule
Validation & & & & \\
\midrule
CHGNet & \cmark & \xmark & 33 & 79 & 3.51 \\
MACE-MP-0 & \xmark & \cmark & 20 & 45 & - \\
eqV2 & \xmark & \cmark & \underline{12.4} & \underline{32.22} & \underline{2.48} \\
%HIENet-l2 & \cmark & \cmark & \underline{6.94} & \underline{26.47} & \underline{2.41} \\
HIENet & \cmark & \cmark & \textbf{6.77} & \textbf{24.82} & \textbf{2.31} \\
\bottomrule
\end{tabular}}
\end{table}


\subsection{Evaluation on Matbench Discovery} \label{section:matbench}

We further evaluate our model on the Matbench Discovery benchmark \citep{riebesell2023matbench}, a comprehensive testbed to benchmark model performance on crystal stability predictions and structure optimizations. Notably, the Matbench Discovery benchmark structures come from a different distribution from the MPtrj training dataset, thus posing an out-of-distribution (OOD) generalization problem. To have a fair comparison with other methods, we only compare with models trained on the MPtrj dataset and without auxiliary losses, referred to as 'compliant models' on the Matbench Discovery leaderboard. HIENet performance can be seen in Table \ref{table:matbench}. HIENet performs best or second best on six out of seven metrics and has the best DAF, Precision, RMSE and $R^2$ predictions. 

\begin{table}[h!]
\renewcommand{\arraystretch}{1.1}
\caption{\small Model performance on the Unique Prototype split of the Matbench Discovery benchmark. MAE and RMSE are in meV/atom. Best model in \textbf{bold} and second best \underline{underlined}.}
\label{table:matbench}
\vspace{0.1in}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|c|cccc}
\toprule
Model & HIENet & eqV2 & ORB & SevenNet-l3i5 & MACE \\
\midrule
F1 $\uparrow$ & 0.761 &\textbf{0.77} & \underline{0.765} & 0.76 & 0.669 \\
DAF $\uparrow$ & \textbf{4.75} & 4.64 & \underline{4.70} & 4.63 & 3.78 \\
Precision $\uparrow$ & \textbf{0.726} & 0.709 & \underline{0.719} & 0.708 & 0.577 \\
%Recall $\uparrow$ & 0.801 & \textbf{0.841} & 0.817 & 0.818 & 0.796 \\
Accuracy $\uparrow$ & \underline{0.922} & \textbf{0.926} & \underline{0.922} & 0.92 & 0.878 \\
\midrule
MAE $\downarrow$ & \underline{44} & \textbf{42} & 45 & 48 & 57 \\
RMSE $\downarrow$ & \textbf{86} & \underline{87} & 91 & \underline{87} & 101 \\
$R^2$ $\uparrow$ & \textbf{0.781} & \underline{0.778} & 0.756 & 0.776 & 0.697 \\
\bottomrule
\end{tabular}}
\vspace{0.1in}
\end{table}

\subsection{Efficiency Evaluation} \label{section:efficiency}
In addition to demonstrating improved performance on MPtrj and Matbench discovery datasets, we show that HIENet is more computationally efficient than competing models. This is highly important for downstream materials discovery applications such as structural relaxation and random structure search, which require thousands of forward passes of the model. As seen in Table \ref{table:efficiency}, HIENet, is 90\% faster than SeveNet-l3i5 and over 140\% faster than eqV2, while having better performance than both models. Both EquiformerV2 and SevenNet use equivariant message passing layers only, limiting throughput and scalability. 

\begin{table}[h!]
\caption{\small Number of parameters and inference throughput of HIENet compared with baseline methods. Throughput evaluated using random samples from the MPtrj dataset on a single Nvidia A100 GPU with batch size 1.}
\label{table:efficiency}
\vspace{0.1in}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|c|c}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{Number of Parameters} & Throughput $\uparrow$\\
& & (Samples / sec.) \\
\midrule
eqV2 & 31,207,434 & 9.4 \\
SevenNet-l3i5 & 1,171,327 & 11.9 \\
HIENet & 7,860,155 & 22.6 \\
\bottomrule
\end{tabular}}
\vspace{-3mm}
\end{table}


\subsection{Evaluation on Phonons and Bulk Modulus }
\label{sec:phonons}

% \subsection{Section by Saagar and Kaji and Dr. Qian \textcolor{red}{Saagar, Kaiji, Dr. Qian}}

% \textcolor{red}{We can also have two sections if needed.-Keqiang}


To demonstrate a more realistic task, we evaluate the ability of HIENet to calculate phonon frequencies and phonon band structures. Phonons are collective excitations of atomic vibrations in crystal structures with translational symmetry, playing a crucial role in determining the dynamical stability and thermal conductivity of materials. Understanding the behavior of phonons is important for condensed matter physics, materials science, mechanical engineering, etc. The calculation of phonon band structure relies on the atomic forces upon displacement of atoms in different phonon modes along high-symmetry paths in the first Brillouin zone. These atomic forces can be efficiently calculated with foundation models. Here we perform a phonon band structure calculation workflow using Phonopy \cite{phonopy-phono3py-JPCM, phonopy-phono3py-JPSJ} on a set of 78 materials, derived from a list of Materials Project (MP) structures from~\cite{Riebesell_ffonons}. These MP structures have reference phonon band structures documented in both the Materials Project Database~\cite{jain2013commentary} and the Togo PhononDB Database~\cite{phonopy-phono3py-JPCM, phonopy-phono3py-JPSJ}. The reason for using the PhononDB Database as well as further details regarding the Phonopy workflow is explained in Appendix \ref{appendix:phonon}.

Figure \ref{fig:saagar_result_Phonon_diagram} shows the results for four materials systems: Si, CdTe, Cs$_2$KInF$_6$, and GaAgS$_2$. It shows that HIENet-predicted phonon band structure of Si exhibits reasonable accuracy, and the phonon band structures for CdTe, Cs$_2$KInF$_6$, and GaAgS$_2$ are in very good agreement with the DFT results from the PhononDB database across the entire frequency range and high-symmetry k-paths. Furthermore, the phonon band structure of Cs$_2$KInF$_6$ contains negative phonon frequencies, indicating the dynamical instability of this crystal structure despite that it is locally stable. Impressively, the result from our HIENet model agrees with the reference PhononDB data extremely well even in this negative frequency regime across all high-symmetry pathways. Table \ref{phonon_error_table} depicts the MAE, MSE, and RMSE of frequency calculation among the models trained on the MPtrj dataset. For all metrics, we see that HIENet exhibits the lowest error. 
% Overall, the results suggest our 
HIENet model can be highly valuable for predicting a material's thermal conductivity and structure stability.

Model efficacy on zero-shot prediction of material properties  was further evaluated through calculations of the fourth-order elastic tensor and the corresponding VRH average bulk modulus $K_{VRH}$~\cite{hill1952elastic}, compared against the data in the Materials Project. A sample validation set was generated by first querying the Materials Project Database~\cite{jain2013commentary} for entries that had elasticity reference with number of atom sites ranging from 1 to 6. This query resulted in a total of 7,601 MP entries, of which the first 2,000 were evaluated as part of this task. Elastic tensors and bulk moduli were computed using the MatCalc's Elasticity module~\cite{Liu_MatCalc_2024} with our HIENet model and other models including MACE-MP-0 ~\cite{batatia2023foundation}, SevenNet-0, SevenNet-l3i5~\cite{park2024scalable},  CHGNet~\cite{deng2023chgnet}, and EquiformerV2 \citep{barroso2024open}. Calculation details of $K_{VRH}$ are provided in Appendix \ref{appendix:bulk_mod}. 
\begin{figure*}[h]
    \centering
    % \vspace{-5mm}
    \includegraphics[width=1.00\linewidth]{saagar_figures/Figure_3_HIENET_127-4_NEW_XQ.pdf}
    \vspace{-9mm}
    \caption{Phonon band structures for a) Si, b) CdTe, c) Cs$_2$KInF$_6$, and d) GaAgS$_2$ calculated using HIENet compared with the reference data in the PhononDB database.}
    \label{fig:saagar_result_Phonon_diagram}\vspace{-3mm}
\end{figure*}

\begin{table}[h!]
\renewcommand{\arraystretch}{1.1}
\caption{\small Error in phonon frequency prediction of various models from target values in the PhononDB Database. MAE and MSE are computed against each q-point, and RMSE is taken as the root of the MSE over all q-points and bands. Reported values are averaged across all 78 materials. Best performing model in \textbf{bold} and second best \underline{underlined}.}
\centering
\vspace{0.1in}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cccccc}
\toprule
Model  & HIENet & MACE & SevenNet-l3i5 & CHGNet & eqV2 \\
\midrule
MAE (THz) & \textbf{0.316} & 0.529 & \underline{0.325} & 1.359 & 1.359 \\
MSE (THz$^2$) & \textbf{0.340} & 0.837 & \underline{0.358} & 4.21 & 4.65 \\
RMSE (THz) & \textbf{0.447} & 0.699 & \underline{0.455} & 1.604 & 1.657 \\
\bottomrule
\end{tabular}}
\label{phonon_error_table}
\end{table}


Parity plots for each model was shown in Figure~\ref{fig:saagar_result_Elastic_parity} using Pymatviz~\cite{Riebesell_Pymatviz_2022}. We find that HIENet exhibits generally stronger capability, with the lowest overall MAE and highest overall $R^2$ compared to other models trained on the MPtrj dataset.

Additionally, in both the phonon band and the bulk modulus calculations, we observe that EquiformerV2 underpforms other models despite having good performance on the MPtrj and Matbench Discovery benchmarks. This aligns with our intuitions as EquiformerV2 does not enforce important physical constraints.

\subsection{Evaluation on \textit{Ab Initio} Molecular Dynamics}
\label{sec:Ab_initio}
% \textcolor{red}{\textbf{Kaiji}, please work on this part.}

To further examine the ability of HIENet to predict energy, force, and stress, we evaluated on \textit{ab initio} molecular dynamics (AIMD) simulations and compared the results with those of SevenNet-l3i5, MACE-MP-0, CHGNet, and EquiformerV2 models. Here we evaluate HIENet and other foundation models by comparing the energy, force, and stress predicted by foundation models with the reference data from AIMD simulations. In this section, we generate the dataset for diamond cubic silicon (Si). Details on AIMD dataset generation are provided in Appendix~\ref{app:AIMD}.
%
\begin{figure*}[h]
    \centering
    % \vspace{-30mm}
    \includegraphics[width=1.0\linewidth]{saagar_figures/Figure_4_hienet_updated_titles_2-2_NEW_XQ.pdf}
    \vspace{-10mm}
    \caption{Comparison of bulk modulus $K_{VRH}$ calculated by a) HIENet, b) MACE-MP-0, c) SevenNet-l3i5, d) SevenNet-0, e) CHGNet, and f) EquiformerV2 with the reference data in the Materials Project database.}
    \label{fig:saagar_result_Elastic_parity}\vspace{-3mm}
\end{figure*}

%% Comment from Xiaofeng: we hav already introduced VASP and PBE etc. in Saagar's section. So, the following introduction section by Kaiji can be shortened (see above).
% The AIMD simulations were conducted using density functional theory (DFT)\cite{kaiji_kohn1965self} \cite{kaiji_hohenberg1964density} as implemented in the Vienna Ab Initio Simulation Package (VASP)\cite{kaiji_kresse1996efficient}. The calculations employed the Perdew-Burke-Ernzerhof (PBE) \cite{kaiji_perdew1996generalized}exchange-correlation functional within the framework of the generalized gradient approximation (GGA) \cite{kaiji_langreth1983beyond}. A plane-wave basis set with a cutoff energy of 520 eV was used to ensure numerical accuracy in the simulations. To ensure consistency between training and evaluation, all input settings were generated using the MPRelaxSet class, with the exception of the MD-related settings.

For each configuration, we collect energy, forces, and stress. We then use our model and other models to predict these properties and compare with reference data. It is worth noting that CHGNet uses corrected energies as its training target \cite{jain2011formation, wang2021framework}. Therefore, when comparing CHGNet's predictions with DFT results, it is necessary to apply Materials Project energy corrections to the DFT results to ensure consistency. 


% \begin{figure}
%     \centering
%     \includegraphics[scale=0.35]{kaiji_figures/result_AIMD_Li6PS5Cl.pdf}
%     \caption{Evaluation of energy, force, and stress predictions for 416-atom Li$_{6}$PS$_{5}$Cl system calculated by foundation models: a) HIENet, b) SevenNet-l3i5, c) MACE-MP-0, d) CHGNet, and e) eqV2\_31M\_mp with respect to the DFT results.}
%     \label{fig:kaiji_result_AIMD_Li6PS5Cl}
% \end{figure}


\begin{figure*}[htpb]
    \centering
    \includegraphics[width=0.8\linewidth]{kaiji_figures/Figure_5_result_AIMD_Si_NEW_XQ.pdf}
    \vspace{-6mm}
    \caption{Evaluation of energy, force, and stress predictions for 64-atom Si system calculated by foundation models: a) HIENet, b) SevenNet-l3i5, c) MACE-MP-0, d) CHGNet, and e) eqV2 with respect to the DFT results.}
    \label{fig:kaiji_result_AIMD_Si}
    \vspace{-3mm}
\end{figure*}

% \vspace{-7mm}
Figure \ref{fig:kaiji_result_AIMD_Si} presents the evaluation results for the Si system comprising 4,000 configurations. For energy prediction, HIENet significantly outperforms other models, which exhibit noticeable shifts in predictions for either high-energy configurations or low-energy configurations. In contrast, HIENet achieves the highest accuracy across the entire energy range. For force prediction, HIENet performs slightly worse than EquiformerV2, but outperforms all other models. While MACE-MP-0 and CHGNet introduce errors in large-force scenarios, SevenNet-l3i5 shows slightly lower overall accuracy compared to HIENet. For stress prediction, HIENet exhibits a more pronounced advantage over all other models. Furthermore, unlike the other models, HIENet maintains high accuracy even for high-stress configurations. In summary, HINet has robust and powerful performance on \textit{ab initio} molecular dynamics (AIMD) simulations and outperforms existing foundation models.

% Figure 6 presents the evaluation results on Li$_{6}$PS$_{5}$Cl systems with 2,000 configurations. Both HIENet and SevenNet-l3i5 demonstrate exceptionally high accuracy in predicting energy, forces, and stress. However, SevenNet-l3i5 exhibits slight prediction shifts for high-temperature configurations. In contrast, MACE-MP-0 and CHGNet show significant errors in high-temperature scenarios, resulting in marginally lower overall accuracy.

% In summary, HIENet and SevenNet-l3i5 outperform the other two models in energy, force, and stress predictions. Notably, HIENet achieves higher accuracy in energy predictions compared to SevenNet-l3i5, while maintaining comparable performance in force and stress predictions. Furthermore, HIENet excels under high-temperature and high-pressure configurations, highlighting its potential as a reliable universal machine learning force field.

\subsection{Evaluations on Alloys}
\label{sec:HEA}
% \textcolor{red}{Siya and Dogu, important dates: Thursday is the official deadline, and nothing can be changed. I need a ready version by Monday afternoon. - Keqiang}

We also evaluate our model by deploying it within the Alloy Theoretic Automated Toolkit (ATAT)~\cite{van2002alloy} framework to calculate phase diagrams, following the approach outlined in~\cite{zhu2025accelerating}. Phase diagrams are graphical representations of the state of materials under arbitrary conditions and accurately predicting them is a necessary condition for the further development of complex materials~\cite{arroyave2022phase}.

% \vspace{-5mm}


Starting with the simple Au-Pt binary systems, we first generate Special Quasirandom Structures (SQS)~\cite{zunger1990special} of FCC Au-Pt with different compositions using ATAT, with 32 atoms in a $2\times2\times2$ supercell---the SQS structures are designed to mimic disordered alloys within a certain precision. Then, the relaxation and free energy calculations are carried out using ab initio calculations and various foundation models. Specifically, we used the following foundation models: CHGNet~\cite{deng2023chgnet}, MACE~\cite{batatia2022mace,Batatia2022Design}, GRACE~\cite{PhysRevX.14.021036}, ORB~\cite{neumann2024orb}, and SevenNet (SevenNet-l3i5)~\cite{batzner20223, park2024scalable}. All of these foundation models were trained exclusively on the MPtraj dataset to ensure consistency in training data across models, allowing for a fair comparison of their predictive performance. For all \textit{ab initio} calculations, VASP~\cite{kresse1993ab,kresse1994ab,kresse1996efficiency,kresse1996efficient} is used with the PBE exchange-correlation functional and PAW pseudopotentials at the level of GGA~\cite{blochl1994projector, perdew1996generalized}. The k-point density is set to 8,000 k-points per reciprocal atom for all calculations.

\begin{figure}[h!]
    \centering
    %\vspace{-3mm}
    \includegraphics[width=1\linewidth]{arroyave_figures/AuPt.png}
    \vspace{-5mm}
    \caption{The formation energies per atom of the Au-Pt binary FCC system calculated using various foundation models trained on the MPtrj dataset.}
    \label{fig:arroyave_AuPt}
    \vspace{-1mm}
\end{figure}

% \begin{table}[h!]
% \renewcommand{\arraystretch}{1.1}
% \caption{\small Energies of FCC Au-Pt SQS calculated with VASP and foundation models (eV/atom).}
% \label{tab:arroyave_formation_energies}
% \centering
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|c|c|c|c|c}
% \toprule
% Model & Au & Au$_{0.75}$Pt$_{0.25}$ & Au$_{0.5}$Pt$_{0.5}$ & Au$_{0.25}$Pt$_{0.75}$ & Pt \\
% \midrule
% VASP & -3.228 & -3.922 & -4.612 & -5.337 & -6.102 \\
% CHGNet & -3.273 & -3.928 & -4.635 & -5.356 & -6.092 \\
% MACE & -3.252 & -3.927 & -4.616 & -5.329 & -6.031\\
% SevenNet & -3.270 & -3.925 & -4.596 & -5.311 & -6.046\\
% ORB & -3.274 & -3.914 & -4.624 & -5.329 & -6.059\\
% GRACE & -3.269 & -3.940 & -4.629 & -5.337 & -6.058\\
% HIENet & -3.270 & -3.938 & -4.615 & -5.321 & -6.051\\
% \bottomrule
% \end{tabular}
% }
% \vspace{0.1in}
% \end{table}

In Figure~\ref{fig:arroyave_AuPt}, we plot the formation energies of the Au-Pt FCC binary systems calculated by all the foundation models. Unlike the results previously reported, we find that models like ORB exhibit much larger errors in formation energy when relying solely on the MPtrj dataset. In contrast, our model shows strong agreement with the first-principles results, even trained with only the MPtrj dataset. In addition, although all the models successfully give a positive formation energy for the SQS's, when predicting the miscibility gap in the phase diagram, most of the models including CHGNet, MACE, ORB, and GRACE fail to reproduce the correct ordering of the formation energies: $\Delta G\left(x_\text{Au}=0.5\right) > \Delta G\left(x_\text{Au}=0.25\right)>\Delta G\left(x_\text{Au}=0.75\right)$, as shown in Table~\ref{tab:arroyave_formation_energy_order}. Such ordering of formation energies is highly important in thermodynamics and materials science, as it governs the stability of the phases, as a necessary (albeit not sufficient) condition for a topologically correct phase diagram is for a model to produce the correct ordering in the energetics of the structures competing for equilibrium~\cite{Ober2024thermodynamically}.

\begin{table}[t!]
\renewcommand{\arraystretch}{1.1}
\caption{\small Ordering of Au-Pt formation energies and Error (\%) calculated with different foundation models (1 indicates lowest formation energy and 3 indicates the highest). Best performing model---relative to ordering and (\% error) given by VASP---in \textbf{bold}.}
\label{tab:arroyave_formation_energy_order}
\vspace{0.1in}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|c|c|c}
\toprule
Model & $\Delta G\left(x_\text{Au}=0.25\right)$ & $\Delta G\left(x_\text{Au}=0.5\right)$ & $\Delta G\left(x_\text{Au}=0.75\right)$ \\
\midrule
CHGNet   & 1 (-31.4\%)   & 2 (-9.6\%)   & 3 (109.1\%) \\
MACE     & 1 (-84.4\%)   & 3 (-52.2\%)   & 2 (-19.2\%) \\
SevenNet & 2 (-12.2\%)   & 3 (18.6\%)    & 1 (65.3\%) \\
ORB      & 1 (-28.6\%)   & 2 (-19.0\%)   & 3 (132.5\%) \\
GRACE    & 1 (-48.4\%)   & 3 (-33.4\%)   & 2 (8.0\%) \\
HIENet & \textbf{2 (-24.4\%)} & \textbf{3 (-12.6\%)} & \textbf{1 (16.1\%)} \\
\midrule
VASP     & 2            & 3            & 1 \\
\bottomrule
\end{tabular}
}
%\vspace{-6mm}
\end{table}

%\vspace{-2mm}
\begin{table}[b!]
\renewcommand{\arraystretch}{1.1}
\caption{\small Comparison of errors in formation energies for different binary systems (Au-Pt, Ag-Pt, Cr-Mo, and Nb-V) using various foundation models trained on the MPtrj dataset. The error is computed as a percentage deviation from DFT-calculated formation energy. Best performing model(s) in \textbf{bold}.} 
\label{tab:arroyave_formation_energy_errors}
\vspace{0.1in}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|c|c|c|c}
\toprule
Model & Au-Pt (FCC) & Ag-Pt (FCC) & Cr-Mo (BCC) & Nb-V (BCC) \\
\midrule
CHGNet & 37.169\% & 98.039\% & 107.154\% & 52.308\%\\
MACE & 57.946\% & 59.359\% & 106.197\% & 30.973\%\\
SevenNet & 25.281\% & 73.840\% & 83.470\% & 53.351\%\\
ORB & 44.695\% & \textbf{40.664\%} &  142.348\% & 33.071\%\\
GRACE & 34.134\% & 68.213\% & 26.548\% & 29.756\%\\
HIENet & \textbf{17.717\%} & 60.434\% & \textbf{15.419\%} & \textbf{11.146\%}\\
\bottomrule
\end{tabular}
}
\end{table}

To better evaluate the difference of formation energies, we define the error as:
\begin{equation}
    \text{Error} = \frac{\sum\limits_x{\lvert \Delta G_\text{MLIP}\left(x\right)} - \Delta G_\text{FP}\left(x\right)\rvert}{\sum\limits_x \lvert \Delta G_\text{FP}\left(x\right)\rvert},
\end{equation}
where $\Delta G_\text{MLIP}\left(x\right)$ and $\Delta G_\text{FP}\left(x\right)$ represent the formation free energy at composition $x$ at 0 K, calculated using foundation models and DFT, respectively. In Table~\ref{tab:arroyave_formation_energy_errors}, we show the errors of formation energies of Au-Pt, Ag-Pt, Cr-Mo, and Nb-V binary systems and observe that HIENet achieves the best overall performance.

Additionally, we can also use our model for multi-element systems. In Figure~\ref{fig:arroyave_CrMoV}, we present a ternary phase diagram for the Cr-Mo-V system at 1,000 K calculated with ATAT and HIENet. 

\subsection{Combining Invariant and Equivariant Layers} \label{section:combining_layers}

\begin{figure}
    % \vspace{-5mm}
    \centering
    \includegraphics[width=0.7\linewidth]{arroyave_figures/CrMoV-ienet.png}
    \vspace{-5mm}
    \caption{Cr-Mo-V ternary phase diagram at 1,000 K calculated with ATAT and HIENet. Only the BCC phase is included in the calculation. The phase diagram is plotted with the Pandat~\cite{chen2002pandat} software package.}
    \label{fig:arroyave_CrMoV}
    \vspace{-1mm}
\end{figure}

To demonstrate the performance and speed advantage of using both invariant and equivariant layers, we train 3 models, each with 4 layers. HIENet uses one invariant layer and 3 equivariant layers, I-Net uses 4 invariant layers, and E-Net uses 4 equivariant layers. The results in Table \ref{table:hybrid_ablation} align with our intuitions about combining both types of layers. Specifically, I-Net is fast, but has poor prediction accuracy, E-Net has better accuracy than I-Net, but is much slower, and HIENet outperforms both models while still being faster than E-Net.

% \vspace{-3mm}
\begin{table}[t!]
\renewcommand{\arraystretch}{1.1}
\caption{\small Mean absolute errors on MPtrj validation set for HIENet compared to invariant-only and equivariant-only baseline models. Models trained for 25 epochs on the MPtrj dataset. Best performing model in \textbf{bold} and second best model \underline{underlined}.}
\label{table:hybrid_ablation}
\vspace{0.1in}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|ccc|c}
\toprule
\multirow{2}{*}{Model} & Energy $\downarrow$ & Force $\downarrow$ & Stress $\downarrow$ & Throughput $\uparrow$\\
 & (meV/atom) & (meV/Å) & (kBar) & (Samples / sec.) \\
\midrule
I-Net & 28.00 & 86.05 & 4.74 & 98.0 \\
E-Net & \underline{14.22} & \underline{44.12} & \underline{3.16} & 24.3 \\
HIENet & \textbf{12.59} & \textbf{42.12} & \textbf{3.09} & 30.2 \\
\bottomrule
\end{tabular}}
\vspace{-1mm}
\end{table}

Additionally, we find that the order in which message passing layers are applied has a significant impact on performance. In Table \ref{table:hybrid_ablation_2} we investigate different orders of layers and find that the only combination that performs well is applying invariant layer(s) followed by equivariant layers. Equivariant message passing layers are limited in terms of the non-linearities they can apply, so we hypothesize that applying invariant layers before equivariant layers builds more informative node representations that enable the equivariant layers to be more powerful than in equivariant-only models. 

\vspace{-3mm}
\begin{table}[h!]
\renewcommand{\arraystretch}{1.1}
\caption{\small Mean absolute errors on MPtrj validation set for HIENet compared to models trained with different orders of message passing layers. 'Inv. first' represents the baseline HIENet architecture of applying one invariant layer followed by several equivariant layers, 'Equiv. First' applies several equivariant layers followed by one invariant layer, and 'Mixed' applies alternating invariant and equivariant layers. Models trained for 20 epochs on the MPtrj dataset. Best performing model in \textbf{bold}.}
\label{table:hybrid_ablation_2}
\vspace{0.1in}
\centering
\begin{tabular}{l|ccc}
\toprule
MP Layer & Energy $\downarrow$ & Force $\downarrow$ & Stress $\downarrow$ \\
Ordering & (meV/atom) & (meV/Å) & (kBar) \\
\midrule
Mixed & 50.35 & 94.62 & 6.41 \\
Equiv. First & 32.26 & 77.22 & 4.94 \\
Inv. First & \textbf{16.26} & \textbf{49.29} &\textbf{3.48} \\
\bottomrule
\end{tabular}
\vspace{-0.05in}
\end{table}

%To demonstrate the importance of including both invariant and equivariant message passing layers in designing high-performing \textit{and} efficient models, we provide several ablations on our model design. As HIENet contains 4 total message passing layers (1 invariant layer and 3 equivariant layers) we train two baseline models: I-Net, with 4 invariant layers, and E-Net, with 4 equivariant layers. As shown in Table \ref{table:ablation}, HIENet ...... \textcolor{red}{waiting for performance numbers to finish analysis.}


%\textcolor{red}{Monte: Would like to provide some intuitive explanation for why Inv + Equiv is better than Equiv. Can we show that for a fixed model throughput, equiv + inv is better than inv and equiv separately?} \textcolor{blue}{You can show a figure with two axis, one for throughput and one for accuracy, and use circle with larger radius to mark the model size, and list all these models in this figure to show the difference. -- Keqiang}

\section{Conclusion}

We propose HIENet, a foundation model for materials simulations and discovery, and demonstrate the importance of incorporating both invariant and equivariant message passing layers to build powerful and efficient MLFF models. We show that HIENet achieves state-of-the-art performance on a variety of benchmarks and downstream applications, while being significantly faster than competing models. Finally, we provide several ablation studies to further demonstrate the significance of our contributions and provide insights on designing powerful hybrid invariant-equivariant networks. 


\section*{Acknowledgments}

K.Y., M.B., A.K. and S.J. acknowledge partial support from
National Science Foundation (NSF) under grant IIS-2243850,
ARPA-H under grant 1AY1AX000053, and National Institutes of Health under grant U01AG070112. K.Z., S.K. and X.F.Q. acknowledge partial support from NSF under awards CMMI-2226908 and DMR-2103842 and the donors of ACS Petroleum Research Fund under Grant 65502-ND10. R.A. and D.S. acknowledge support from ARO through Grant No. W911NF-22-2-0117. R.A., X.N.Q. and S.Z. acknowledge NSF through Grant No. 2119103 (DMREF). Z. X. and X.N.Q. acknowledge support from NSF through grants SHF-2215573 and IIS-2212419. Some computations were carried out at the Texas A\&M High-Performance Research Computing (HPRC) facility. We gratefully acknowledge the support of Lambda Inc. for providing computing resources.



%Authors are \textbf{required} to include a statement of the potential 
%broader impact of their work, including its ethical aspects and future 
%societal consequences. This statement should be in an unnumbered 
%section at the end of the paper (co-located with Acknowledgments -- 
%the two may appear in either order, but both must be before References), 
%and does not count toward the paper page limit. In many cases, where 
%the ethical impacts and expected societal implications are those that 
%are well established when advancing the field of Machine Learning, 
%substantial discussion is not required, and a simple statement such 
%as the following will suffice:

%``This paper presents work whose goal is to advance the field of 
%Machine Learning. There are many potential societal consequences 
%of our work, none which we feel must be specifically highlighted here.''

%The above statement can be used verbatim in such cases, but we 
%encourage authors to think about whether there is content which does 
%warrant further discussion, as this statement will be apparent if the 
%paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\bibliography{dive,arroyave,other,kaiji,saagar}
\bibliographystyle{arxiv2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Molecular Dynamics Simulation}

\subsection{Molecular dynamics simulation and structural optimization of materials}
\label{app:dynamics}

\textbf{Molecular dynamics simulation}. Molecular dynamics (MD) simulation~\cite{alder1959md} is an important computational method to compute structural, chemical, and thermodynamic properties, which allows for in-depth mechanistic understanding and materials discovery. MD simulation essentially solves Newton's equations of motion for both atomic positions and cell parameters of a material system under a specific thermodynamic ensemble. Specifically, the simulation workflow relies on iterative computation of the total system energy $E$, atomic forces $\bm{F}_i$, and stress tensor $\bm{\sigma}$. For a given starting structure configuration, $E$, $\bm{F}_i$, and $\bm{\sigma}$ can be calculated by the force field such as classical force field or foundation models. The acceleration, velocity, and position of atoms can be subsequently determined over a time step through numerical integration methods such as the Velocity-Verlet algorithm under a thermodynamic ensemble. The atomic forces of the new structure will then be updated for the next time step. By iterative numerical integration, the system will evolve under the thermodynamic ensemble and interatomic interactions determined by the force field. Stress also plays a crucial role in MD simulations when controlling pressure, such as in an NPT ensemble (i.e. under the constant number of particle, constant pressure, and constant temperature condition). In order to obtain statistically averaged physical quantities, such calculation needs to be performed iteratively for many time steps, hence the computational efficiency becomes critical.

% there are two key formulas in the Velocity-Verlet algorithm, which involve position updates: $r_i(t + \Delta t) = r_i(t) + v_i(t)\Delta t + \frac{1}{2} a_i(t)\Delta t^2$ and velocity updates: $v_i\left(t + \Delta t \right) = v_i(t) + \frac{1}{2} (a_i(t) + a_i(t+\Delta t) ) \Delta t$ where $\Delta t$ is the time step, $r_i$ is the position, $v_i$ is the velocity and $a_i(t)$ is the acceleration. 

% In this scenario, a barostat is introduced to regulate the system's pressure. This involves dynamically adjusting the size and shape of the simulation cell based on the computed stress tensor, ensuring the system evolves towards and maintains the desired pressure conditions. 

% Construct the potential energy surface (PES) which is a high-dimensional function depending on the positions of atoms. 

\textbf{Structural optimization}.
Different from molecular dynamics, structural optimization usually aims to relax the structure and/or cell parameters to their ground state or metastable state. It also involves the calculation of energy, force and stress, which are subsequently used by optimization algorithms or optimizers to update the structure, such as Conjugate Gradient algorithm (CG)~\cite{hestenes1952methods} and Broyden–Fletcher–Goldfarb–Shanno algorithm (BFGS)~\cite{fletcher2000practical}. This process is repeated until the final convergence criteria is reached, 


% Similar to molecular dynamics simulation, structural optimization also needs to calculate the energy, force and stress. However, their workflows and operational methods are different. Based on the gradient of the potential energy surface (force), find the minimum point of energy surface where the corresponding configuration is considered the ground-state configuration. 

% To achieve this, various optimization algorithms (optimizers) can be used, such as Conjugate Gradient algorithm (CG) ~\cite{hestenes1952methods} and Broyden–Fletcher–Goldfarb–Shanno algorithm (BFGS) \cite{fletcher2000practical}. Take BFGS as an example, it is a quasi-Newton method that updates the position of atoms using $\mathbf{r}_{k+1} = \mathbf{r}_k - \mathbf{H}_k^{-1} \cdot \mathbf{f}_k$ where $\mathbf{r}_{k}$ represents the positions of all atoms, $\mathbf{f}_k$ represents the forces acting on all atoms and $\mathbf{H}_k$ is Hessian matrix which is the second derivative of energy with respect to atomic coordinates. BFGS does not actually calculate this matrix in the actual operation, as this would be computationally expensive. Instead, it uses an iterative approach to approximate the inverse Hessian matrix. Despite this approximation, accurate force calculations remain necessary to update the matrix.

% Take BFGS as an example, it is a quasi-Newton method that updates the position of atoms using $\mathbf{r}_{k+1} = \mathbf{r}_k - \mathbf{H}_k^{-1} \cdot \mathbf{f}_k$ where $\mathbf{r}_{k}$ represents the positions of all atoms, $\mathbf{f}_k$ represents the forces acting on all atoms and $\mathbf{H}_k$ is Hessian matrix which is the second derivative of energy with respect to atomic coordinates. BFGS does not actually calculate this matrix in the actual operation, as this would be computationally expensive. Instead, it uses an iterative approach to approximate the inverse Hessian matrix. Despite this approximation, accurate force calculations remain necessary to update the matrix.


\subsection{Conventional computation methods}
\label{app:related_traditional}

Several kinds of simulation techniques are widely used in computational materials science at various scales, such as Density Functional Theory (DFT)~\cite{hohenberg1964density,kohn1965self}, MD simulations~\cite{alder1959md}, and Monte Carlo (MC) simulations~\cite{metropolis1953equation}. DFT is a quantum mechanical method that can be used to simulate material systems at the electronic level. Its key principle is that the ground-state energy of a system can be expressed as a functional of electron density, which reduces $3N_e$-dimensional interacting many-body system down to a fictitious 3-dimensional non-interacting system. However, DFT is computationally expensive and is limited to small systems. MD simulation method has already been elaborated in Appendix~\ref{app:dynamics} where a force field is required for calculating energy, force, and stress. There are two types of MD simulations depending on the underlying force field: \textit{ab initio } MD (AIMD) simulations where atomic forces are calculated by quantum mechanical method such as DFT, and classical MD simulations where empirical force fields are used to calculate atomic forces.  AIMDs are relatively more accurate but computationally expensive, limiting its application to small systems. Classical MD simulations are computationally efficient and can handle large systems, but very often they either lack the accuracy required for highly precise simulations, or cannot be transferred to different simulation conditions. MC simulations are based on statistical mechanics which rely on iterative energy calculations and configuration sampling and updates. Another key challenge is that both classical MD and MC simulations depend on the availability of empirical force fields for the system of interest. Therefore, it is highly desirable to develop foundation models that can provide accurate and efficient calculations of energy, force, and stress of arbitrary materials system, which will significantly advance materials science, physics and chemistry and allow for studying fundamental mechanism and discovering new materials. 

% Several kinds of simulation techniques are widely used in computational materials science, such as Density Functional Theory (DFT)~\cite{hohenberg1964density,kohn1965self}, Molecular Dynamics (MD)~\cite{alder1959md}. DFT is a quantum mechanical method that can be used to simulate material systems at the electronic level. Its key principle is that the ground-state energy of a system can be expressed as a functional of the electron density, rather than directly depending on the positions of electrons, which can help to avoid solving many-body problem. The results of DFT can achieve electronic-level accuracy. However, DFT is computationally expensive and is limited to small systems. MD method simulates material systems by calculating the forces acting on each atom and determining motion of each atom. There are two main types of force fields in traditional MD: empirical force fields and quantum mechanics-based force field. Empirical force fields are computationally efficient and can handle large-scale systems but lack the accuracy required for highly precise simulations. Quantum mechanics-based force fields, while more accurate, are computationally demanding and unsuitable for large systems. Other methods, such as Monte Carlo simulations (MC)~\cite{metropolis1953equation} and Finite Element Methods (FEM)~\cite{zienkiewicz1977finite}, also face limitations in balancing accuracy and efficiency. MC simulations are valid for studying statistical properties, but assumptions are usually required. FEM simulations are better at simulating macroscopic properties rather than atomic-scale phenomena. Therefore, we are eager to develop a force field that can describe the interactions among atoms quickly and accurately, which is why the machine learning-based interatomic potentials are introduced.


\subsection{\textit{Ab initio} molecular dynamics simulation}
\label{app:AIMD}
\textit{Ab initio} molecular dynamics (AIMD) simulations were conducted using DFT as implemented in VASP with the PBE exchange-correlation energy functional. A plane-wave basis set with a cutoff energy of 520 eV was used to ensure numerical accuracy in the simulations. To ensure consistency between training and evaluation, all input settings were generated using the MPRelaxSet class, with additional AIMD-related settings as detailed below.

The dataset was generated for silicon (Si) system containing 64 atoms in a $2\times2\times2$ supercell. A $\Gamma$-centered Monkhorst–Pack k-point sampling grid of $2\times2\times2$ \cite{monkhorst1976special} was used. AIMD simulations were performed in the NVT ensemble with a Nosé-Hoover thermostat at four temperatures of 300, 500, 700, and 900 K with time step of 1 fs for 1,000 steps at each temperature. In total, 4,000 configurations were generated for model evaluation.


\section{HIENet Architechture} \label{appendix:model_overview}

\subsection{Crystal Graph Construction}

We construct a crystal graph that is O(3) equivariant and invariant to all unit cell transformations. Specifically, each node $\mathbf{z}_i$ in the constructed graph represents atom $i$ and all of its infinite duplicates in repeating unit cells. We then build edges using a radius-based graph construction such that there exists an edge $\bm{r}_{ji}$ between nodes $i$ and $j$ if there is a periodic duplicate $j'$ such that:
\begin{equation}
    \|\bm{p}_j + k'_1\bm{l}_1 + k'_2\bm{l}_2 + k'_3\bm{l}_3 - \bm{p}_i\|_2 \leq R_{\text{cut}}
\end{equation}

where $R_{\text{cut}}$ is a fixed cutoff radius. For our model we set $R_{\text{cut}} = 5$Å.

\subsection{Embedding Layer}

We create initial node embeddings $\bm{h}_i$ with a linear projection of atomic number one-hot encodings: $\bm{h}_i = \mathbf{W}_{\text{emb}} \bm{z}_i$. We create edge embeddings $\bm{h}_{ij}$ for edge $\bm{e}_{ij}$ using 8 radial Bessel basis functions:
\begin{equation}
    \bm{h}_{ij} = \frac{2\sin\left(\frac{n\pi}{R_{\text{cut}}}\|\bm{r}_{ij}\|_2\right)}{R_{\text{cut}}\|\bm{r}_{ij}\|_2}f_{poly}\left(\|\bm{r}_{ij}\|_2, R_{\text{cut}}\right)
\end{equation}

where $f_{poly}$ is the polynomial envelope of \citet{gasteigerdirectional}.

\subsection{E(3) Invariant Message Passing}

For our invariant message passing, we use a graph transformer layer to update node features $\bm{h}_i$. Specifically, we form key $\bm{k}_{ji}$, and query $\bm{q}_{ji}$ vectors as

\begin{equation}
    \bm{k}_{ji} = \mathbf{W}_K\left(\bm{h}_i || \bm{h}_j || \bm{h}_{ij}\right)\text{,} \quad \bm{q}_{ji} = \mathbf{W}_Q\left(\bm{h}_i || \bm{h}_j || \bm{h}_{ij}\right) 
\end{equation}

where $||$ denotes vector concatenation. We then build value vectors $\bm{v}_{ji}$ and compute attention weights $\bm{\alpha}_{ji}$:

\begin{equation}
    \bm{v}_{ji} = \Phi\left(\bm{h}_i || \bm{h}_j || \bm{h}_{ij}\right), \quad \bm{\alpha}_{ji} =  \sigma\left(\frac{\bm{q}_{ji}\odot \bm{k}_{ji}}{\sqrt{d}}\right)
\end{equation}

where $\Phi$ is an MLP and $\odot$ represents the Hadamard (elementwise) product. Finally, we compute updated node features:

\begin{equation}
    \bm{h}'_i = \varphi\left(\bm{h}_i\right) + \left(1 - \varphi\left(\bm{h}_i\right)\right)\sum_{j \in \mathcal{N}_i} \bm{\alpha}_{ji}\odot\bm{v}_{ji}
\end{equation}

Here $\varphi\left(\bm{h}_i\right)$ is an MLP with a sigmoid activation that acts as a learnable gating mechanism. 

\subsection{O(3) Equivariant Message Passing}

We first embed edge vectors $\bm{r}_{ji}$ using spherical harmonics $Y^l(\frac{r_{ji}}{||r_{ji}||})$ for each rotation order $l$ up to $L_{\text{max}}$. We then build equivariant features as:

\begin{equation}
    \bm{f}_i = \frac{1}{\left|\mathcal{N}_i\right|}\sum_{j \in \mathcal{N}_i}\textbf{TP}_0\left(\mathbf{W}\bm{h}_j, Y^0\left(\frac{r_{ji}}{||r_{ji}||}\right)\right) \\
    + \sum_{l=1}^{L_{\text{max}}}\sum_{j \in \mathcal{N}_i} \textbf{TP}_0\left(\mathbf{W}\bm{h}_j, Y^l\left(\frac{r_{ji}}{||r_{ji}||}\right)\right)
\end{equation}

where $\textbf{TP}_0$ is a tensor product operation yielding outputs with rotation order $l=0$. We further add a skip connection and gate mechanism to output updated node features:

\begin{equation}
    \bm{h}'_i = \psi\left(\mathbf{W}_{\text{skip}}\bm{h}_i + \mathbf{W}_E\bm{f}_i\right)
\end{equation}

where $\psi$ is an equivariant gate activation function. In practice, we sequentially apply several of our equivariant message passing layers.

\subsection{Model Optimization}

We optimize the model using Huber loss functions for energy, force, and stress: 

\begin{equation}
    \mathcal{L} = L_{\text{huber}}\left(\frac{E}{N}, \frac{\hat{E}}{N}\right) + \lambda_{F} \sum_{i=1}^N \sum_{k=1}^3 L_{\text{huber}}\left(F_{i, k}, \hat{F}_{i, k}\right) \\
    + \lambda_S \sum_{l=1}^6 L_{\text{huber}}\left(\sigma_l, \hat{\sigma}_l\right)
\end{equation}

where $\lambda_{F}$, $\lambda_{S}$ control the relative weight of force and stress losses. In practice, we set $\lambda_{F} = 1$ and $\lambda_{S} = 0.1$. We compute per-atom energy and decompose the $3 \times 3$ stress matrix into virial stress components. We train the model using the AdamW optimizer \citep{loshchilov2018decoupled} and Cosine Annealing learning rate schedule \citep{loshchilov2022sgdr}.

\section{Proofs of Propositions} \label{appendix:proofs}

\subsection{Proof of Prop. \ref{prop:conserve}}

\begin{proposition}
    HIENet predictions $\bm{\hat{F}}_i$ form a conservative vector field. 
\end{proposition}

By definition, a vector field $\mathbf{v}: \mathbb{R} \to \mathbb{R}^n$ is conservative if there exists a continuously differentiable scalar field $\varphi$ such that:

\begin{equation*}
    \mathbf{v} = \nabla\varphi
\end{equation*}

Importantly, we use the polynomial envelope function of \citet{gasteiger2021gemnet} and continuously differentiable activation functions throughout the model to ensure that the potential energy predicted by our model $\hat{E}$ is continuously differentiable with respect to atom positions. 


And because we compute forces using: 

\begin{equation*}
    \bm{\hat{F}} = -\nabla_\mathbf{P}\hat{E}
\end{equation*}

the resulting vector field $\bm{\hat{F}}$ is conservative. \hfill $\square$\\



\subsection{Proof of Prop. \ref{prop:equilibrium}}

\begin{proposition}
    HIENet predictions satisfy force equilibrium $\sum_{i=1}^N\bm{\hat{F}}_i = \bm{0}$ when no external influences applied.
\end{proposition}

We define edge force $\bm{\hat{F}}_{ji}$ as:

\begin{equation*}
    \bm{\hat{F}}_{ji} = -\frac{\partial \hat{E}}{\partial \bm{r}_{ji}}
\end{equation*}

We can then decompose the forces acting on each atom as:

\begin{align*}
    \bm{\hat{F}}_i & = -\frac{\partial \hat{E}}{\partial \bm{p}_i} \\
    & = - \sum_{j \in \mathcal{N}_i}\left(\frac{\partial \hat{E}}{\partial \bm{r}_{ji}}\frac{\partial \bm{r}_{ji}}{\partial \bm{p}_i} + \frac{\partial \hat{E}}{\partial \bm{r}_{ij}}\frac{\partial \bm{r}_{ij}}{\partial \bm{p}_i}\right)\\
    & = - \sum_{j\in \mathcal{N}_i}\left(\frac{\partial \hat{E}}{\partial \bm{r}_{ji}} - \frac{\partial \hat{E}}{\partial \bm{r}_{ij}}\right)\\
    & =\sum_{j \in \mathcal{N}_i}\left(\bm{\hat{F}}_{ji} - \bm{\hat{F}}_{ij}\right)
\end{align*}

Summing over all atoms we get:

\begin{align*}
    \sum_{i=1}^N\bm{\hat{F}}_i & = \sum_{i=1}^N\sum_{j \in \mathcal{N}_i}\left(\bm{\hat{F}}_{ji} - \bm{\hat{F}}_{ij}\right) \\
    & = \sum_{(i,j) \in \mathcal{E}} \left(\bm{\hat{F}}_{ij} - \bm{\hat{F}}_{ij}\right) \\
    & = 0
\end{align*}

therefore the forces acting on each atom sum to 0 as desired. \hfill $\square$

%\subsection{Proof of Prop. \ref{prop:equiv}}

%\begin{proposition}
%    HIENet predictions are O(3) equivariant.
%\end{proposition}

%Model predictions $f\left(\mathbf{Z}, \mathbf{P}, \mathbf{L}\right)$ are O(3) equivariant if, for any rotation transformation $\mathbf{R}\in \mathbb{R}^{3\times 3}, |\mathbf{R}| = \pm 1$, applied to the input crystal, the predictions will rotate accordingly:

%\begin{equation*}
%    \mathbf{R}f\left(\mathbf{Z}, \mathbf{P}, \mathbf{L}\right) = f\left(\mathbf{A}, \mathbf{R}\mathbf{P}, \mathbf{R}\mathbf{L}\right)
%\end{equation*}

%For HIENet force predictions:

%\begin{equation*}
%    \bm{\hat{F}} = -\nabla_\mathbf{P}\bm{\hat{E}} = -\nabla_\mathbf{P} \phi_\theta\left(\mathbf{Z}, \mathbf{P}, \mathbf{L}\right)
%\end{equation*}

%Because HIENet energy predictions are E(3) invariant (and therefore O(3) invariant), $\phi_\theta\left(\mathbf{Z}, \mathbf{R}\mathbf{P},  \mathbf{R}\mathbf{L}\right) = \phi_\theta\left(\mathbf{Z}, \mathbf{P}, \mathbf{L}\right)$, so:

%\begin{equation*}
%    \mathbf{R}\bm{\hat{F}} = -\nabla_{\mathbf{R}\mathbf{P}}\bm{\hat{E}} = -\nabla_{\mathbf{R}\mathbf{P}}\phi_\theta\left(\mathbf{Z}, \mathbf{R}\mathbf{P},  \mathbf{R}\mathbf{L}\right) = -\nabla_{\mathbf{R}\mathbf{P}}\phi_\theta\left(\mathbf{Z}, \mathbf{P}, \mathbf{L}\right)
%\end{equation*}

%Because rotation $\mathbf{R}$ is a linear transformation, $\nabla_{\mathbf{R}\mathbf{P}}\phi_\theta\left(\mathbf{Z}, \mathbf{P}, \mathbf{L}\right) = \mathbf{R}\nabla_{\mathbf{P}}\phi_\theta\left(\mathbf{Z}, \mathbf{P}, \mathbf{L}\right)$, therefore:

%\begin{equation*}
%    \mathbf{R}\bm{\hat{F}} = -\mathbf{R}\nabla_{\mathbf{P}}\phi_\theta\left(\mathbf{Z}, \mathbf{P}, \mathbf{L}\right) = -\mathbf{R}\nabla_{\mathbf{P}}\bm{\hat{E}}
%\end{equation*}

%as desired. Similarly for stress,

%\begin{equation*}
%    \mathbf{R}\bm{\hat{\sigma}}\mathbf{R}^\top = \nabla_{\mathbf{R}\bm{\epsilon}\mathbf{R}^\top}\bm{\hat{E}} = \mathbf{R}\left(\nabla_{\bm{\epsilon}}\bm{\hat{E}}\right)\mathbf{R}^\top
%\end{equation*}

%Therefore both force $\bm{\hat{F}}$ and stress $\bm{\hat{\sigma}}$ predictions are O(3) equivariant. \hfill $\square$

%\section{Additional Matbench Discovery Results} \label{appendix:matbench}

%Here we provide a comparison with more baseline methods on the Unique Prototypes split and we provide results on the 10k Most Stable split and the Full Test Set split from Matbench Discovery, as shown in Table~\ref{table:appendix_matbench_unique}, \ref{table:appendix_matbench_1}, and \ref{table:appendix_matbench_2}.

%\begin{table}[h!]
%\caption{Model performance on the Unique Prototype split of the Matbench Discovery Benchmark. MAE and RMSE are in meV/atom. Best performing model in \textbf{bold} and second best \underline{underlined}.}
%\label{table:appendix_matbench_unique}
%\vspace{0.1in}
%\centering
%\begin{tabular}{l|c|ccccccc}
%\toprule
%Model & HIENet & eqV2 & ORB & SevenNet-l3i5 & SevenNet-0 & GRACE-2L & MACE & CHGNet \\
%\midrule
%F1 $\uparrow$ & 0.761 &\textbf{0.77} & \underline{0.765} & 0.76 & 0.724 & 0.691 & 0.669 & 0.613\\
%DAF $\uparrow$ & \textbf{4.75} & 4.64 & \underline{4.70} & 4.63 & 4.25 & 4.16 & 3.78 & 3.36 \\
%Precision $\uparrow$ & \textbf{0.726} & 0.709 & \underline{0.719} & 0.708 & 0.65 & 0.636 & 0.577 & 0.514\\
%Accuracy $\uparrow$ & \underline{0.922} & \textbf{0.926} & \underline{0.922} & 0.92 & 0.904 & 0.896 & 0.878 & 0.851\\
%\midrule
%MAE $\downarrow$ & \underline{44} & \textbf{42} & 45 & 48 & 48 & 52 & 57 & 63 \\
%RMSE $\downarrow$ & \textbf{86} & \underline{87} & 91 & \underline{87} & 92 & 94 & 101 & 103\\
%R2 $\uparrow$ & \textbf{0.781} & \underline{0.778} & 0.756 & 0.776 & 0.75 & 0.741 & 0.697 & 0.689\\
%\bottomrule
%\end{tabular}
%\vspace{0.1in}
%\end{table}

%\begin{table}[h!]
%\caption{Model performance on the full Matbench Discovery Benchmark test set. MAE and RMSE are in meV/atom. Best performing model in \textbf{bold} and second best \underline{underlined}.}
%\label{table:appendix_matbench_1}
%\vspace{0.1in}
%\centering
%\begin{tabular}{l|c|ccccccc}
%\toprule
%Model & HIENet & eqV2 & ORB & SevenNet-l3i5 & SevenNet-0 & GRACE-2L & MACE & CHGNet \\
%\midrule
%F1 $\uparrow$ & 0.754 & \textbf{0.758} & \underline{0.755} & 0.751 & 0.719 & 0.687 & 0.668 & 0.612\\
%DAF $\uparrow$ & \textbf{4.197} & 4.053 & \underline{4.188} & 4.112 & 3.804 & 3.714 & 3.4 & 3.038 \\
%Precision $\uparrow$ & \textbf{0.720} & 0.696 & \underline{0.719} & 0.706 & 0.653 & 0.637 & 0.583 & 0.521\\
%Accuracy $\uparrow$ & \underline{0.911} & \textbf{0.912} & \underline{0.911} & 0.909 & 0.893 & 0.884 & 0.864 & 0.839\\
%\midrule
%MAE $\downarrow$ & \underline{42} & \textbf{41} & 43 & \underline{42} & 46 & 50 & 55 & 61 \\
%RMSE $\downarrow$ & \textbf{85} & \textbf{85} & 90 & 86 & 90 & 92 & 99 & 100\\
%R2 $\uparrow$ & \textbf{0.779} & \underline{0.777} & 0.752 & 0.773 & 0.75 & 0.74 & 0.698 & 0.69\\
%\bottomrule
%\end{tabular}
%\vspace{0.1in}
%\end{table}

%\begin{table}[h!]
%\caption{Model performance on the 10k Most Stable split of the Matbench Discovery Benchmark. MAE and %RMSE are in meV/atom. Best performing model in \textbf{bold} and second best \underline{underlined}.}
%\label{table:appendix_matbench_2}
%\vspace{0.1in}
%\centering
%\begin{tabular}{l|c|ccccccc}
%\toprule
%Model & HIENet & eqV2 & ORB & SevenNet-l3i5 & SevenNet-0 & GRACE-2L & MACE & CHGNet \\
%\midrule
%F1 $\uparrow$ & 0.966 &\textbf{0.974} & \underline{0.971} & 0.952 & 0.945 & 0.914 & 0.888 & 0.92\\
%DAF $\uparrow$ & \underline{6.112} & \textbf{6.21} & 6.173 & 5.945 & 5.857 & 5.503 & 5.221 & 5.567 \\
%Precision $\uparrow$ & 0.934 & \textbf{0.949} & \underline{0.944} & 0.909 & 0.895 & 0.841 & 0.798 & 0.851\\
%Accuracy $\uparrow$ & 0.934 & \textbf{0.949} & \underline{0.944} & 0.909 & 0.895 & 0.841 & 0.798 & 0.851\\
%\midrule
%MAE $\downarrow$ & \underline{38} & \textbf{37} & \textbf{37} & 50 & 54 & 65 & 87 & 63 \\
%RMSE $\downarrow$ & \textbf{77} & \underline{94} & 98 & 114 & 124 & 133 & 165 & 95\\
%R2 $\uparrow$ & \textbf{0.872} & \underline{0.812} & 0.801 & 0.745 & 0.7 & 0.641 & 0.508 & 0.816\\
%\bottomrule
%\end{tabular}
%\vspace{0.1in}
%\end{table}



\section{Additional Ablations} \label{appendix:ablations}

\begin{table}[h!]
\caption{Mean absolute errors on MPtrj validation set for HIENet compared without architechtural changes from ComFormer. HIENet + RBF denotes replacing the HIENet edge embeddings with the RBF embeddings from ComFormer. HIENet + EdgeConv denotes unclude the Edge-wise transformer layer from iComFormer. Models trained for 20 epochs on the MPtrj dataset. Best performing model in \textbf{bold}.}
\label{table:appendix_ablations_2}
\vspace{0.1in}
\centering
\begin{tabular}{l|ccc}
\toprule
\multirow{2}{*}{Model} & Energy $\downarrow$ & Force $\downarrow$ & Stress $\downarrow$ \\
 & (meV/atom) & (meV/Å) & (kBar) \\
\midrule
HIENet + RBF & 18.87 & 55.58 & 3.97 \\
HIENet + EdgeConv & 17.97 & 54.27 & 3.69 \\
HIENet & \textbf{16.26} & \textbf{49.29} &\textbf{3.48} \\
\bottomrule
\end{tabular}
\vspace{0.05in}
\end{table}

To justify some of the design choices of HIENet compared to ComFormer, we provide several additional ablation studies in Tables \ref{table:appendix_ablations_2} and \ref{table:appendix_ablations}. Using RBF kernels to embed edge vectors negatively impacts performance, and the inclusion of an edge convolution layer both negatively impacts the performance and reduces the model efficiency. As such, both were removed in the final HIENet design.

\begin{table}[h!]
\caption{Mean absolute errors on MPtrj validation set for HIENet with O(3) and SO(3) equivariant crystal graphs. Models trained for 20 epochs on the MPtrj dataset. Best performing model in \textbf{bold}.}
\label{table:appendix_ablations}
\vspace{0.1in}
\centering
\begin{tabular}{c|ccc}
\toprule
\multirow{2}{*}{Equivariance} & Energy $\downarrow$ & Force $\downarrow$ & Stress $\downarrow$ \\
 & (meV/atom) & (meV/Å) & (kBar) \\
\midrule
SO(3) & 19.13 & 56.12 & 3.98 \\
O(3) & \textbf{16.26} & \textbf{49.29} &\textbf{3.48} \\
\bottomrule
\end{tabular}
\vspace{0.05in}
\end{table}

To empirically justify why we use O(3) equivariant crystal graph representations instead of the geometrically complete but SO(3) equivariant crystal graphs from ComFormer, we provide an ablation study where we include the additional periodic encodings of \citet{yan2024complete}. As expected, we observe that SO(3) equivariant HIENet does not perform as well.



\section{Phonon and Bulk Modulus Workflows} \label{appendix:phonon_kvrh}
\subsection{Phonon Frequency Evaluation} \label{appendix:phonon}
As the calculations of the Material Project phonon dataset were performed using the PBEsol exchange-correlation energy functional, it would be inconsistent to compare them with the models trained on the data using the Perdew-Burke-Ernzerhof~(PBE)~\cite{perdew1996generalized} exchange-correlation energy functional. PhononDB, a database of phonon calculations including band structure, DOS, and thermal properties for over 10,000 materials evaluated using the PBE functional, provides a more effective reference for comparison, hence was used as the reference for the evaluation as detailed below. 
Phonon frequencies and corresponding band structures were computed using the Phonopy package via the finite displacement method~\cite{phonopy-phono3py-JPCM, phonopy-phono3py-JPSJ} where foundation models were employed to compute the dynamical matrices and corresponding phonon band structures of each crystal structure. To ensure direct comparison between PhononDB and calculated data, the Phonopy objects were initialized with the same unit cell and supercell matrices as used in PhononDB calculations. Additionally, the primitive cell matrix was included if defined. Displaced supercells were generated using a default displacement of 0.01 {\AA} and the corresponding forces were evaluated with our model. High-symmetry k-path in the Brillouin zone was computed using SeeK-Path~\cite{hinuma2017band, togo2024spglib}. Using this workflow, the high-symmetry k-paths and the sampling grids were identical between the reference phonon band structure from PhononDB and the predicted band structure from our model.

\subsection{Bulk Modulus Evaluation} \label{appendix:bulk_mod}
To compute bulk modulus, we need to calculate the elastic tensor for each crystal. The latter is calculated by first relaxing the input structure to the default force tolerance of 0.1 {eV/\AA} using different models. The relaxed structure is then deformed with strains of ($\pm 0.005$, $\pm 0.01$) applied to normal modes and strains of ($\pm 0.06$, $\pm 0.03$) applied to shear modes for a total of 4 strain magnitudes for each of the 6 strain modes. The resulting stress-strain values are fit linearly to compute the elastic tensor. The reference elastic constants in the Materials Project were calculated using DFT with the PBE functional in the generalized gradient approximation (GGA)~\cite{langreth1983beyond} as implemented the Vienna Ab-initio Simulation Package (VASP)~\cite{kresse1996efficient}. For metallic entries, a plane wave cutoff energy of 700 eV with k-point density of 7,000 per reciprocal atom was used. For non-metallic entries such as insulators or semiconductors, a plane wave cutoff energy of 700 eV was once again used with a k-point density of 10,000 per reciprocal atom \cite{de2015charting}. 
We then calculate elastic tensors for all materials in the validation set using our HIENet model as well as MACE-MP-0, CHGNet SevenNet-l3i5, SevenNet-0, and EquiformerV2~\cite{batatia2023foundation, deng2023chgnet, park2024scalable, barroso2024open}. Of the 2,000 structures computed, 1,763 were ultimately plotted after filtering the structures where $K_{VRH}$ was not reported or where reported $K_{VRH}$ values were extreme. Following~\citet{batatia2023foundation}, $K_{VRH}$ values between -50 GPa to 600 GPa were considered.




\section{Model Settings and Experimental Details} \label{appendix:implementation}

HIENet consists of 1 invariant and 3 equivariant message passing layers. For the invariant message passing layers, we use a hidden dimension of 512 for node features and a single attention head. The equivariant layers use a representation that consists of $512$ scalar channels ($l=0$), 128 vector channels ($l=1$), 64 tensor channels ($l=2$), and 32 higher order-tensor channels ($l=3$). We use 8 radial Bessel basis functions for distance encoding and a polynomial envelope~\cite{gasteigerdirectional} with $p = 6$. We use SiLU activation functions~\cite{elfwing2018sigmoid} throughout the network to ensure smooth and continuously differentiable gradients. Additionally, we scale the input energies  by the root mean square (RMS) of forces from the training dataset and shifted by element-wise reference energies from the same dataset. 


Following \citet{batatia2023foundation}, we split the Materials Project Trajectory (MPtrj) Dataset~\cite{deng2023chgnet} into training (95\%) and validation (5\%) sets. The model is trained for 250 epochs using AdamW optimizer~\cite{loshchilov2018decoupled} with a Cosine Annealing learning rate scheduler~\cite{loshchilov2022sgdr}. We use Huber loss functions for each of the targets with $\delta = 0.01$. To prevent overfitting, we use model checkpoint from epoch 221 for materials discovery applications. For the Matbench Discovery benchmark, we additionally fine-tune the model on energy for 50 epochs. Training is performed on a platform with ten CPUs, Intel Xeon 6248R (Cascade Lake), 3.0GHz, 24-core, 384GB DDR4 memory, and twenty GPUs, NVIDIA A100 40GB GPU accelerator. The total batch size is 440 (22 per GPU) and requires 55 GPU minutes per training epoch and 30 GPU seconds per validation epoch. The training hyperparameters for both the training and fine-tuning stages are summarized in Table~\ref{tab:train_params}.

\begin{table}[h!]
\caption{Hyperparameters for training and fine-tuning stages.}
\label{tab:train_params}
\vspace{0.1in}
\centering
\begin{tabular}{l|cc}
\toprule
Hyper-parameters & Training & Fine-tuning \\
\midrule
Optimizer & AdamW & AdamW \\
Learning rate scheduling & Cosine & Cosine \\
Maximum learning rate & 0.01 & 0.0005 \\
Minimum learning rate & 0.0001 & 0.0 \\
Warmup epochs & 0.1 & 0.1 \\
Warmup factor & 0.2 & 0.1 \\
Number of epochs & 250 & 50 \\
Batch size & 22 & 64 \\
Weight decay & 0.001 & 0.001 \\
Energy loss weight, $\lambda_{E}$ & 1.0 & 1.0  \\
Force loss weight, $\lambda_{F}$ & 1.0 & 0.1  \\
Stress loss weight, $\lambda_{\sigma}$ & 0.01 & 0.001 \\
Model EMA Decay & 0.999 & 0.999 \\
\bottomrule
\end{tabular}
\end{table}





\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
