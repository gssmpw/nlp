\section{ADDITIONAL RELATED WORK}
In this section, we present additional related works that we could not include in the main paper for the readers' reference. We also point to the papers presenting the LLMs/LMMs used in the experiment.

\subsection{Text-to-Vector generation}
\label{subsec:T2V}
Along with the advancements of methods to generate images representing the input text prompt in the raster domain (Text-to-Image; T2I), research on Text-to-Vector (T2V) has become increasingly popular. The mainstream of T2V methods is to define a loss function in the raster domain and run an optimization loop with a differentiable rasterizer~\cite{diffvg}. CLIPDraw~\cite{CLIPDraw} aims to generate drawings from a text description. They realize this by comparing the input text and the image under editing with the CLIP~\cite{clip} encoder. 
VectorFusion~\cite{vectorfusion} generates vector images by integrating Stable Diffusion~\cite{stable-diffusion} and using SDS loss~\cite{dreamfusion} instead of CLIP similarity. Zhang \etal~\cite{T2V-NPR} first learns the embeddings of paths using a VAE and then generates an initial image by optimizing this path representation.

NIVeL~\cite{NIVeL} questions these methods involving path optimization. They point out that SDS loss cannot model the graphic-dependent representation structure. Hence, they propose using an MLP that takes a point in 2D space and outputs the probability that a shape includes the point. With this process, it is possible to obtain an interpretable SVG representation by generating shapes for each layer corresponding to different colors.

\subsection{Utilizing LLMs for Vector Graphics Processing}
\label{subsec:LLM-for-SVG}
Vector graphics, including SVG, are described in a text file. Therefore, we can directly input an image as code into a recently evolving L\textbf{L}Ms---not just L\textbf{M}Ms. Several prior research exist that make LLMs process vector graphics, especially SVG.

Prior work has attempted to solve various SVG processing tasks using LLMs. For vectorizing raster images~\cite{S2VG2, starvector}, researchers focus on the generated SVGs being unreadable and restricted to paths. Therefore, they aim to generate SVG code with language models. In Text-to-Vector generation~\cite{gpt4-experiments, SVG-LLM}, experiments show that LLMs understand the concept of shapes and colors and can perform visual tasks. Papers on SVG understanding~\cite{SVG-LLM, svg-visualizations} demonstrate that the LLM can perform low-level visual analytic tasks. They showed this by giving LLMs input images as SVG code and asking questions about them.

\subsection{LLMs and LMMs used in the experiment}
The following is the list of LLMs and LMMs evaluated in Sec. 4 and its references.
\begin{itemize}
  \item \textbf{Open-sourced General-purpose LLMs}: Gemma 1.1~\cite{gemma-kaggle}, Gemma 2~\cite{gemma2-paper, gemma-kaggle}, Llama 2~\cite{llama2}, Llama 3, Llama 3.1, Llama 3.2~\cite{llama3}, Mistral v0.2, Mistral v0.3~\cite{mistral}, Phi-3.5-mini~\cite{phi3}
  \item \textbf{Open-sourced Code-specific LLMs}: CodeGemma~\cite{codegemma}, Code Llama~\cite{codellama}
  \item \textbf{LMMs}: LlaVA-NeXT~\cite{llava-next}, Phi-3.5-vision~\cite{phi3}, Qwen2-VL~\cite{qwen2-vl}
  \item \textbf{Close-sourced Models}: Gemini 1.0 Pro~\cite{gemini}, Gemini 1.5 Flash, Gemini 1.5 Pro~\cite{gemini-1.5}, GPT-3.5, GPT-4o, GPT-4o mini, o1-mini
\end{itemize}
