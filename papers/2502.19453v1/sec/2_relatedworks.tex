\section{RELATED WORK}
This section introduces prior work that inspired our dataset creation and model evaluation pipelines. We focus on two aspects: benchmarks in vector graphics processing and datasets on image editing.

\subsection{Benchmarks on vector graphics processing}
Several benchmarks have been proposed for vector graphics processing. SVGBench~\cite{starvector} proposes an evaluation method for image vectorization by extracting images from existing datasets and defining metrics. VGBench~\cite{vgbench} is a benchmark for quantitatively analyzing the performance of VQA and generation with three vector graphics formats: SVG, TikZ, and GraphViz. The existing research closest to ours is SVGEditBench~\cite{SVGEditBench}. SVGEditBench is a dataset built to evaluate the SVG editing capabilities of LLMs. 

Our benchmark is different from these previous works in the following points. 
\begin{itemize}
  \item SVGEditBench V2 assesses the performance of SVG \textbf{editing}. To our knowledge, no other benchmark exists besides SVGEditBench.
  \item SVGEditBench V2 offers a wider range of complex editing tasks than SVGEditBench. SVGEditBench is restricted to six predefined and easy tasks. Also, the benchmark is not applicable for other models than LLMs since straightforward heuristics can address these tasks. However, our dataset is not confined to a specific set of tasks. These tasks require a semantic understanding of the input image. For example, a task may require identifying which part of the SVG code corresponds to the object specified.
  \item SVGEditBench V2 evaluates SVG editing from a broader perspective. SVGEditBench primarily uses Mean Squared Error (MSE) as the evaluation metric. However, semantic and geometric aspects are also important when assessing SVG editing. Therefore, we utilize four distinct metrics to encompass these areas during the evaluation phase.
\end{itemize}

\subsection{Similar works in the raster domain}
Datasets for image editing for arbitrary tasks also exist in the raster domain. The representative dataset is the one InstructPix2Pix (IP2P)~\cite{InstructPix2Pix} used to train. In IP2P, a fine-tuned GPT-3 generated an appropriate editing instruction and the image caption after the edit. Then, they employ Prompt-to-Prompt~\cite{prompt-to-prompt} to generate a corresponding image consistent with the original, and these images collectively constitute a dataset suitable for training an image editing model.

Subsequent research has proposed image editing techniques and datasets~\cite{magicbrush, SmartEdit} based on IP2P. They pointed out that the images may not correspond to the instruction since IP2P uses automatically generated images~\cite{magicbrush} and that IP2P cannot handle complex instructions requiring reasoning~\cite{SmartEdit}. The papers address these problems with manual annotation or filtering~\cite{magicbrush, SmartEdit}, or with segmentation of the objects in the image~\cite{SmartEdit}. In our dataset, we generated the editing prompt with an LLM inspired by IP2P.
