\begin{table*}
  \small
  \renewcommand{\arraystretch}{0.7}
  \centering
  \begin{tabular}{@{}llcc|cccc|cc@{}}
      \toprule
       & & & & \multicolumn{4}{c}{Metrics} & \multicolumn{2}{c}{Error Rate} \\
       & Model & Type & \# Params & MSE \textdownarrow & DINO \textuparrow & CLIPScore \textuparrow& Chamfer \textdownarrow & Extraction \textdownarrow & Parsing \textdownarrow \\ \midrule
       & Original & & & $0.061$ & $0.897$ & $26.882$ & $2.584\times 10^3$ & & \\
       & Ground Truth & & & $0$ & $1$ & $27.492$ & $0$ & & \\ \midrule
      %  \multirow{15}{*}{\rotatebox[origin=c]{90}{Open}} 
      \multirow[t]{15}{*}{Open}
       & CodeGemma& Code & 8.54B & $0.084$ & $0.828$ & $26.281$ & $2.596\times 10^3$ & $9.7\%$ & $2.6\%$\\
       & Code Llama& Code & 6.74B & $0.103$ & $0.775$ & $25.714$ & $3.941\times 10^3$ & $14.1\%$ & $5.3\%$\\
       & Gemma 1.1 & & 8.54B & $0.077$ & $0.853$ & $26.542$ & $3.109\times 10^3$ & $1.6\%$ & $9.7\%$\\
       & Gemma 2& & 2.61B & $0.087$ & $0.837$ & $26.072$ & $2.521\times 10^3$ & $33.6\%$ & $7.4\%$\\
       & Gemma 2 & & 9.24B & $0.076$ & $0.866$ & $26.369$ & $\mathbf{2.411\times 10^3}$ & $37.3\%$ & $1.8\%$\\
       & Llama 2& & 6.74B & $0.102$ & $0.791$ & $25.686$ & $2.946\times 10^3$ & $33.1\%$ & $14.7\%$\\ 
       & Llama 3& & 8.03B & $0.099$ & $0.808$ & $26.144$ & $3.548\times 10^3$ & $6.5\%$ & $13.0\%$\\ 
       & Llama 3.1& & 8.03B & $0.108$ & $0.792$ & $26.015$ & $5.413\times 10^{27}$ & $7.1\%$ & $14.5\%$\\ 
       & Llama 3.2& & 3.21B & $0.119$ & $0.764$ & $25.762$ & $2.999\times 10^3$ & $11.6\%$ & $34.8\%$\\ 
       & LlaVa-NeXT& MM & 7.57B & $0.110$ & $0.742$ & $25.401$ & $1.476\times 10^4$ & $20.7\%$ & $22.3\%$\\ 
       & Mistral v0.2& & 7.24B & $0.115$ & $0.736$ & $25.474$ & $5.500\times 10^3$ & $9.4\%$ & $18.8\%$\\
       & Mistral v0.3& & 7.25B & $0.109$ & $0.771$ & $25.760$ & $4.311\times 10^3$ & $4.1\%$ & $10.2\%$\\
       & Phi-3.5-mini& & 3.82B & $0.128$ & $0.701$ & $25.295$ & $7.873\times 10^4$ & $66.2\%$ & $25.3\%$\\ 
       & Phi-3.5-vision& MM & 4.15B & $0.165$ & $0.570$ & $24.326$ & $3.446\times 10^3$ & $89.9\%$ & $61.8\%$\\ 
       & Qwen2-VL& MM & 8.29B & $0.104$ & $0.770$ & $25.592$ & $2.935\times 10^3$ & $21.7\%$ & $21.1\%$\\ \midrule
       \multirow[t]{7}{*}{Closed}
       & Gemini 1.0 Pro& & & $0.076$ & $0.857$ & $26.287$ & $2.876\times 10^3$ & $39.5\%$ & $\mathbf{0.6\%}$\\
       & Gemini 1.5 Flash& MM & & $\mathbf{0.070}$ & $0.869$ & $26.724$ & $2.566\times 10^3$ & $3.4\%$ & $0.8\%$\\ 
       & Gemini 1.5 Pro& MM & & $0.077$ & $0.854$ & $26.480$ & $3.370\times 10^3$ & $0.1\%$ & $0.8\%$\\ 
       & GPT-3.5 & & & $0.073$ & $0.867$ & $26.678$ & $3.733\times 10^3$ & $1.5\%$ & $1.1\%$\\ 
       & GPT-4o & MM & & $\mathbf{0.070}$ & $0.874$ & $26.784$ & $2.930\times 10^3$ & $0.1\%$ & $\mathbf{0.6\%}$\\
       & GPT-4o mini & MM & & $\mathbf{0.070}$ & $\mathbf{0.875}$ & $26.761$ & $3.482\times 10^3$ & $\mathbf{0.0\%}$ & $2.1\%$\\ 
       & o1-mini & & & $0.073$ & $0.872$ & $\mathbf{26.788}$ & $4.943\times 10^3$ & $0.3\%$ & $0.9\%$\\
      \bottomrule
  \end{tabular} %
  \caption{Results of evaluating current LLMs/LMMs' SVG editing ability with our proposed benchmark dataset. ``MM'' refers to multimodal models, and ``Code'' refers to code-specific models. The ``\# Params'' column for the open models shows the number of parameters stated on the corresponding HuggingFace page.}
  \label{tab:main-results}
\end{table*}

\section{EXPERIMENT}
In this section, we utilized current LLMs and LMMs as SVG editors and assessed their performance using SVGEditBench V2. These models can serve as a valuable baseline for SVG editing. While editing vector graphics requires professional skills, the communication ability of these models can simplify the process by offering a text-based interface, thus lowering the barrier to entry.

\subsection{Experimental setting}
We performed evaluations using various LLMs and LMMs, as shown in \Tref{tab:main-results}. The purpose of using LMMs is to confirm whether their vision capabilities can help with SVG editing. To measure the pure capabilities of LLMs and LMMs, we used simple prompts for inference, and we did not perform any prompt engineering, such as showing examples. Even for LMMs, we did not input images. Please refer to the supplementary material for the actual prompt. For open-sourced models, we used the versions with instruction-following capabilities with under 10B parameters. When running these models, we used the vLLM library~\cite{vllm} for efficient inference.

The LlaVa-NeXT model here leverages Mistral v0.2. The GPT-3.5, GPT-4o, GPT-4o mini, and o1-mini models are the ones available as \texttt{gpt-3.5-turbo-0125}, \texttt{gpt-4o-2024-08-06}, \texttt{gpt-4o-mini-2024-07-18} and \texttt{o1-mini-2024-09-12} in the OpenAI API.

\subsection{Results}
\Tref{tab:main-results} shows the results of evaluating the LLMs and LMMs with SVGEditBench V2. These are the metrics averages calculated for all the triplets whose metrics were successfully calculated. On the top of \Tref{tab:main-results}, we show the metric values when we regard the original image and the ground truth image as the model output. If the models succeeded in editing the images according to the editing prompt, the metric averages for the LLM/LMMs should have improved than the ``Original'' values. Also, we report the percentage of images that caused an error during the evaluation pipeline in the rows on the right. Processing failures occur in two cases: when our pipeline cannot extract the SVG code from the LLM/LMM output and when rasterization fails due to grammatical errors in the SVG code or other reasons. In \Tref{tab:main-results}, we report the former case as ``Extraction'' and the latter as ``Parsing''.

\subsection{Qualitative analysis}
We can draw the following conclusions from the metrics and the error rates in \Tref{tab:main-results}.
\begin{enumerate}
  \item In general, \textbf{it is difficult for the current LLMs/LMMs to solve image editing tasks that require a semantic understanding of the images}. We can see that most averages are worse than the ``Original''. These values suggest that the editing models made the images farther from the ground truth.
  \item There was no clear tendency indicating the effect of the vision capabilities of LMMs with SVG editing (\eg, Mistral v0.2 MSE: $0.115$ vs. LlaVa-NeXT MSE: $0.110$, Mistral v0.2 CLIPScore: $25.474$ vs. LlaVa-NeXT CLIPScore: $25.401$). Finely adjusting the parameters is crucial when editing SVG images, and only having vision capabilities is insufficient to improve the performance.
  \item By training LLMs on code, the models could generate SVG code more accurately, improving the error rate (\eg, Llama 2 Extraction: $33.1\%$ vs. Code Llama Extraction: $14.1\%$). This result suggests that having SVG code as training data helps precisely generate SVG code. However, it does not seem to impact the evaluation metrics significantly (\eg, Llama 2 MSE: $0.102$ vs. Code Llama MSE: $0.103$).
  \item The ability to generate SVG code correctly depends on the model series. For instance, the failure rate exceeds 60\% for Phi-3.5 while it is under 10\% for Mistral. This trend is more apparent between Open and Closed models; the error rate is near zero for most Closed models.

\end{enumerate}

\subsection{Quantitative analysis}
In this section, we will look closer at the editing results and clarify when and how the current LLMs/LMMs succeed or fail to help future research on SVG editing.

There were instances where the models could output images similar to the ground truth for some relatively simple tasks. We can classify the successful cases into two categories: deleting and applying basic transformations to elements. In the transforming tasks, the LLMs could properly utilize functionalities of SVG (\texttt{transform} attribute \etc). Also, the LLM/LMM could sometimes accurately calculate the coordinates of the control points, taking the entire drawing range into account. These examples show that current LLMs/LMMs understand basic SVG grammar and can recognize the meaning of each object from the code.

Looking at the LLM/LMM outputs that the evaluation pipeline regarded as an error, we found these errors prominent in the \texttt{<path>} element. In some examples, the language models repeated similar number sequences, resulting in an incomplete code. The \texttt{<path>} elements often include a complex array of numbers. In other cases, the \texttt{d} attribute of an output \texttt{<path>} element contained undefined line commands, or the number of arguments was too many or insufficient.

LLMs/LMMs also seemed likely to avoid or fail processing \texttt{<path>} elements. In some cases, the model had only added an \texttt{<text>} element when it had to reform a shape. Even when the model attempted to edit \texttt{<path>} elements, the output had messy and corrupted shapes.