\section{INTRODUCTION}
\label{sec:introduction}

\begin{figure*}
  \centering
  \includegraphics[width=0.9\hsize]{img/teaser.pdf}
  \caption{SVGEditBench V2 is a collection of triplets consisting of SVG graphics before and after editing and the editing prompt. We extracted the images from multiple publicly available emoji datasets and employed GPT-4o to generate the prompts. This straightforward pipeline can produce a variety of editing tasks. The tasks range from transforming elements to changing the overall style. A high-level understanding of the graphics is necessary to perform these tasks.}
  \label{fig:teaser}
\end{figure*}
With the advancements of Diffusion models, recent research~\cite{stable-diffusion,InstructPix2Pix} attempts to generate or edit raster images according to an input text. Also, generating vector graphics---an alternative way to represent images---has become increasingly popular~\cite{vectorfusion}.

Despite this, not much research exists in text-to-vector editing. Zhang \etal ~\cite{svg-customization} proposes a method to customize an SVG image with a text prompt. However, the method requires fine-tuning a Diffusion model for each exemplar image, which is costly. Also, some research~\cite{SVG-LLM, vgbench, SVGEditBench} aims to generate or understand vector images with Large Language Models (LLMs). They focus on the fact that vector graphics are human-readable text data. However, none of these proposed a new vector graphics editing method.

We believe the limited research on vector graphics editing is due to the absence of unified benchmarks and datasets. Thus, we propose SVGEditBench V2, a benchmark dataset for evaluating methods for editing SVGs. SVGEditBench V2 comprises 1,683 triplets (examples in \Fref{fig:teaser}). Each triplet includes the original SVG image before editing, the ground truth SVG image after editing, and the editing text prompt. The editing prompt describes the changes from the original image to the ground truth. We extracted the images from public SVG emoji datasets and asked GPT-4o to generate the prompt using those images. When evaluating SVG editing methods, we first edit the original image using the editing prompt. We then compare the output image from the method with the ground truth using raster-based, contour-based, and description-based metrics. We will make the created dataset publicly available on GitHub.

We applied our benchmark to existing 15 LLMs and 7 Large Multimodal Models (LMMs) to examine their ability for SVG editing. Our experiments revealed that, generally, these models struggle to edit images following the prompts. Still, there were a few instances where the models succeeded in their editing tasks when they were simple.