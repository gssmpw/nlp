\section{SVGEDITBENCH V2}
\label{sec:method}
\begin{figure*}
  \centering
  \includegraphics[width=0.9\hsize]{img/method.pdf}
  \caption{Overview of our method. Our benchmarking pipeline comprises two main parts: dataset creation and model evaluation. Refer to \Sref{sec:method} and the supplementary material for more details.}
  \label{fig:overview}
\end{figure*}

This section briefly describes each component in the dataset creation and evaluation pipelines in SVGEditBench V2. \Fref{fig:overview} shows the overview of our proposed approach. We provide further details in the supplementary material. Note that although we used emojis as the images, similar pipelines are applicable when creating editing benchmarks for other vector graphics domains.

\subsection{Selection of original images}
Inspired by SVGBench~\cite{starvector} and SVGEditBench~\cite{SVGEditBench}, we built the dataset from existing SVG emoji datasets: Noto Emoji~\cite{noto-emoji}, Twemoji~\cite{twemoji}, and flat and high-contrast versions of Fluent Emoji~\cite{fluent-emoji}. The reasons for choosing emojis are as follows.
\begin{itemize}
    \item Emoji datasets are suited for generating editing tasks. We can obtain images of similar layouts but in different styles when we pick images with the same Unicode codepoint from different datasets. This fact leads to the inclusion of style-transfer tasks. Also, emojis contain multiple images of the same type, such as facial expressions and clocks. With this feature, the editing tasks can better align with real-life editing tasks.
    \item We can use emoji names defined in Unicode to describe the images. These descriptions can help extract suitable image pairs and generate accurate editing instructions in subsequent steps.
\end{itemize}

We filtered out some images from those datasets. Specifically, we excluded images that may be inappropriate for editing and images we could not obtain the description. The number of remaining images was 5,766.

\subsection{Extraction of image pairs}
We extracted an original image (before editing) and a ground truth image (after editing) from these selected image sets. The image pair should look similar and have close meanings. This characteristic makes the tasks realistic and allows the editing prompt to describe the differences clearly. Also, the dataset as a whole should contain diverse images and editing tasks.

We calculated a distance metric for all combinations of two images in the image set and picked image pairs with the closest distance. However, to prevent the same image from being used too frequently, we allowed the same image to be in the dataset only up to three times. The metric here is a sum of LPIPS~\cite{lpips} distance and the similarity of the descriptions measured by the CLIP~\cite{clip} text embeddings. This metric is suitable for obtaining image pairs with the attributes mentioned earlier because it considers both the visual and semantic aspects of the images. We extracted 3,000 image pairs with this process.

\subsection{Generation of editing instruction}
The next step is to create the editing instructions for each image pair. Inspired by InstructPix2Pix~\cite{InstructPix2Pix}, we generated the instructions using GPT-4o (\texttt{gpt-4o-2024-08-06}). We provided GPT-4o the rasterized versions of the original and the ground truth images. We also included the descriptions for each image in the prompt. This strategy differs from IP2P since they made the instructions only from text input.

\subsection{Filtering the triplets}
Up to this point, we gained triplets of an original image, a ground truth image, and an editing instruction that links them. However, these instructions may be inaccurate. Hence, we manually checked them and removed inappropriate ones to reduce noise. We filtered the triplets from two aspects. Firstly, the prompt should mention all the elements in the image that should be changed. Secondly, the prompt should not contain errors that would significantly impact the editing results.

Additionally, we filtered out the triplets if the prompt in the evaluation phase ($\approx$ instruction + original SVG code) would exceed 5,000 characters. Since the LLMs used in the following experiment may not fit long prompts within their context, this process was necessary to reduce the impact of differences in context length on the evaluation results. As we used existing SVG data, we believe this process should not significantly affect the insights on LLMs' ability to edit SVG images. The number of the remaining triplets became 1,683.

\subsection{Analysis of the created dataset}
\Fref{fig:teaser} lists some triplets created by the above pipeline. Compared to SVGEditBench, our tasks are much more complex and diverse. Firstly, the editing prompt specifies the objects the editing model should modify by their names, not color, as in SVGEditBench. It is not straightforward to find the specified object from the SVG code. Also, the difficulty of the tasks varies remarkably from one another. Some are simple change-color tasks, as in the middle-right example of \Fref{fig:teaser}. In contrast, others require generating objects that match the style of the original image and placing it appropriately (top-left example of \Fref{fig:teaser}).

\subsection{Evaluation metrics}
We now evaluate SVG editing techniques (including LLMs) with our created dataset. Given a target SVG editing method, we edit the original image from each triplet using the associated editing prompt. We compare the output with the ground truth image in the triplet. 

We used four metrics in this benchmark. We chose these metrics so that the evaluation reflects the various aspects of SVG editing. Firstly, the Mean Squared Error (MSE) of pixel values and cosine similarity of DINOv2~\cite{dinov2} features compare the images in raster format. These measure how the images are visually similar. Next, we used CLIPScore~\cite{clipscore} between the description text of the ground truth and the rasterized output image to judge the semantic closeness.

Additionally, we used the Chamfer distance metric to evaluate the geometrical closeness of the images. We used a two-step Chamfer distance to account for both the position of the shapes and the difference of the shapes themselves. In this metric, we first calculate the Chamfer distance $d_\mathrm{shape}$ between shapes in the output and ground truth images. We define the distance between the images by summing $d_\mathrm{shape}$ between a shape in one image and its closest match. More details are in the supplementary material.

We must accurately determine the contour from the SVG code to calculate this distance. Therefore, we excluded images that contain elements other than \texttt{<path>}, basic shape elements (\texttt{<rect>} \etc), and elements representing color gradients (ignored in calculating this distance).