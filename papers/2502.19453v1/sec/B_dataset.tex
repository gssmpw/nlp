\section{DETAILS OF THE DATASET CREATION PIPELINE}
This section explains the dataset creation pipeline in more detail than the main paper.

\subsection{Method of retrieving descriptions of emojis}
\label{subsec:emoji-names}
As we explained in Sec. 3.1, emojis have a name associated with them. We use this information in both dataset creation and evaluation. Please refer to Sec. 3.2 and Sec. 3.6 for how we used the descriptions in each pipeline.

The following algorithms show how we gained the descriptions of the images used in our dataset:

For images in Fluent Emoji, the SVG image is under the folder with a descriptive name. Therefore, we use this folder name as the description.

For images in Twemoji and Noto Emoji, the filename of the SVG image indicates the associated Unicode codepoints. We use the following steps to obtain the description from these codepoints:
\begin{enumerate}
    \item If the number of codepoints used to express the emoji is one, we use the \texttt{unicodedata} library ~\cite{unicodedata} in Python 3.12 to retrieve the emoji name. This library can retrieve the character properties in the Unicode Character Database version 15.0.0. This step returns an error if the emoji is in the Private Use Area. For instance, Twemoji contains an emoji in the Private Use Area: Shibuya109 (U+E50A). If this happens, we exclude the image from the pipeline.
    \item If the number of codepoints is more than two, we first search it on the list of ZWJ sequences available on the \textit{Unicode, Inc.} website~\cite{zwj-sequences}. In ZWJ sequences, a single glyph uses multiple emojis with a ZERO WIDTH JOINER (U+200D) in between. If the sequence of codepoints is present in the list, we use the description in the list.
    \item If the sequence is not on the list, we look at the final codepoint. If it is \texttt{U+20E3}, the emoji is a keycap (\ie, a number or a symbol on a square with rounded corners). We run the previous steps except for the final codepoint and prepend \texttt{Keycap:} to the returned description. If the last codepoint is \texttt{U+FE0F}, we simply run the steps without the final codepoint. We can do this because \texttt{U+FE0F} is called the ``emoji variation selector'' and does not change the meaning of the emoji.
    \item If we cannot obtain the description with these steps, we regard it as an unknown ZWJ sequence. We split the sequence with \texttt{U+200D} and get the description for each fragment. We concatenate all the descriptions with a \texttt{+} sign between them and use this to describe the image as a whole.
\end{enumerate}

\subsection{Details of image pair extraction}
\label{subsec:image-pair-extraction}
\begin{figure*}
  \begin{minipage}{0.33\textwidth}
    \includegraphics[width=\linewidth]{img/image_pairs_skin-color.pdf}
    \caption{Examples of image pairs created with emojis with modified skin color included. The tasks are biased to tasks involving changing color.}
    \label{fig:ablation-skin-color}    
  \end{minipage}
  \hspace{0.03\linewidth}
  \begin{minipage}{0.63\linewidth}
      \includegraphics[width=0.50\linewidth]{img/dist_no_restrictions.pdf}
      \includegraphics[width=0.50\linewidth]{img/dist_with_restrictions.pdf}
      \caption{The distribution of the number of times the same image appears in the dataset. The left histogram shows the distribution when we applied no restrictions, and the right shows the distribution in our proposed dataset (\ie, the same image can be in the dataset only up to three times).}
      \label{fig:ablation-restriction}
  \end{minipage}
\end{figure*}

Before extracting image pairs, we excluded the images that seemed inappropriate for the editing tasks. Specifically, we did not use flag images and emojis whose codepoints have skin color modifiers (U+1F3FB to U+1F3FF). The reason is that incorrectly editing these images may offend some people. 

Ensuring the diversity of tasks is another reason for excluding images with modified skin color. \Fref{fig:ablation-skin-color} shows the example image pairs built when those emojis are present. This figure reveals that the dataset contains a lot of Change Color tasks if we used emojis with those emoji modifiers. This trend is because the image pairs with only differences in skin color will be close in structure and semantics. Our distance metric used here regards these pairs as near. Note that some Change Color tasks remain even if we exclude such kinds of emojis (e.g., the middle-right example of Fig. 1).

We extracted the image pairs using the following algorithm:
\begin{enumerate}
    \item We calculate the distance metric discussed in Sec. 3.2 for all combinations of two images in the datasets. Since the images are SVG, we rasterized them at $64\times 64$ resolution before calculating LPIPS.
    \item We sort the combinations with the metric so that the pair closer in distance is processed earlier.
    \item If the metric value is under 0.1, we exclude the image pairs. This step disregards pairs with identical images. Pairs of identical images occur partly because we used two versions for the Fluent Emoji datasets: the flat and high-contrast versions. The emojis in the high-contrast version are black-and-white variants of the flat ones. Therefore, if those in the flat version already include only black and white (\eg, club or spade suits), the corresponding high-contrast version will be identical to the flat version.
    \item If either of the images is already used more than three times, we skip the image pair.
    \item We randomly choose which image is the original and which is the ground truth for each pair.
    \item We end the extraction process if we obtain 3000 image pairs.
\end{enumerate}


\begin{figure*}
  \centering
  \subfloat[LPIPS only]{
    \includegraphics[width=0.3\linewidth]{img/image_pairs_lpips.pdf}
    \label{fig:ablation-creation-metrics-LPIPS}
  }
  \hspace{0.03\linewidth}
  \subfloat[CLIP similarity only]{
    \includegraphics[width=0.3\linewidth]{img/image_pairs_clip-cos.pdf}
    \label{fig:ablation-creation-metrics-CLIP}
  }
  \hspace{0.03\linewidth}
  \subfloat[LPIPS + CLIP similarity]{
    \includegraphics[width=0.3\linewidth]{img/image_pairs.pdf}
    \label{fig:ablation-creation-metrics-both}
  }
  \caption{Examples of image pairs extracted using different comparison metrics. The numbers above the image pairs are assigned in order of the distance metric (closer to farther). The image pairs on the left look similar but differ semantically, while the opposite is true in the middle pairs. Using both image- and text-based metrics leads to image pairs with realistic and easily describable editing tasks.}
  \label{fig:ablation-creation-metrics}
\end{figure*}

We show an ablation study on the effectiveness of restricting the frequency at which the same image can appear (Step 4 in the above algorithm). We introduced this step to ensure the variety of the emojis. \Fref{fig:ablation-restriction} shows the distribution of how many times a single image is in the dataset with and without the restriction. We see that the dataset without this restriction includes some emojis many times, 36 times at maximum. In addition, we could consist of 52\% more images (2,246 without the limit and 3,409 with the limit) by setting this limit.

Additionally, we show how the image comparison metric contributes to gaining appropriate image pairs. We used the sum of LPIPS of the rasterized images and the cosine distance of CLIP text embeddings of the descriptions in our dataset creation pipeline. We aim to extract image pairs that are similar both visually and semantically by using this metric. This characteristic is vital since we believe editing occurs between images similar in both appearance and meaning.

\Fref{fig:ablation-creation-metrics} shows some image pairs extracted using different metrics. We obtained \Fref{fig:ablation-creation-metrics}\subref{fig:ablation-creation-metrics-LPIPS} and \Fref{fig:ablation-creation-metrics}\subref{fig:ablation-creation-metrics-CLIP} solely with LPIPS and CLIP text distance, respectively. \Fref{fig:ablation-creation-metrics}\subref{fig:ablation-creation-metrics-both} is from the generated dataset before filtering (thus, it may contain pairs not used in experiments discussed in Sec. 4). The image pairs in \Fref{fig:ablation-creation-metrics}\subref{fig:ablation-creation-metrics-LPIPS} look similar in shapes and colors. However, the meanings of the images may not be related. For example, the images in pair 008 in \Fref{fig:ablation-creation-metrics}\subref{fig:ablation-creation-metrics-LPIPS} mean ``congratulations'' and ``no littering,'' respectively. Conversely, the image pairs in \Fref{fig:ablation-creation-metrics}\subref{fig:ablation-creation-metrics-CLIP} express similar concepts (\eg, images in pair 016 represent ``identification card'' and ``name badge'', respectively). However, the images may look totally different. Differences in these image pairs will be too complex to describe concisely in text. We obtain image pairs that look and mean similar by summing them, as in \Fref{fig:ablation-creation-metrics}\subref{fig:ablation-creation-metrics-both}.

\subsection{Prompt for Retrieving Editing Instructions}
\label{subsec:appendix-instruction-prompt}
We provided GPT-4o with the following prompt to generate the editing instructions. The rasterized versions of the original and ground truth images come after this text prompt.
\begin{itembox}[l]{The prompt for generating editing instrucions}
\begin{lstlisting}
The first image is an emoji of "<b#(Caption of the original image)#>" and the second is an emoji of "<b#(Caption of the ground truth image)#>." Describe how the first image should be edited to look like the second image. Do not just say "Change to match the second image/emoji," but specify the the expected result. Also, make the instruction as clear and as short as possible.

For example, if a plane is landing towards the runway in the first image and taking off in the second, you could say "Make the plane take off."
\end{lstlisting}
\end{itembox}

The first sentence states what the subsequent two images represent. We used the descriptions obtained as explained in \Sref{subsec:emoji-names} for the captions. The second sentence asks GPT-4o to create the editing instruction. The following two sentences suggest that the editing instruction should not mention the second image (ground truth image), and GPT-4o should provide clear and concise instructions. The instruction should avoid referencing the second image because the target SVG editing model will not see the ground truth image while editing. The final sentence shows an example of captions and their instruction. Showing this example aims to ensure variety in the expression of the prompts (\ie, not just ``Change \textit{A} into \textit{B}'').

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{img/clock.pdf}
    \caption{Example of image captions helping generation of precise editing instructions. The time mentioned in the prompt is wrong in the top example, while the instruction is accurate in the bottom.}
    \label{fig:clock}
\end{figure}
The biggest motivation for using captions in this prompt is that GPT-4o occasionally cannot understand the image's content accurately, especially for clocks. \Fref{fig:clock} shows one example of how these captions contribute to generating precise prompts. In this example, GPT-4o recognized the time in the ground truth image as 4:35 instead of 10:30 when it did not see the caption. In contrast, it could accurately describe where the clock hands should be when considering the two images' captions.

\subsection{Details of the Filtering Stage}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{img/label-studio.png}
    \caption{The interface for filtering the triplets. We used Label Studio to filter out inaccurate editing prompts or inappropriate images.}
    \label{fig:label-studio}
\end{figure}
We manually filtered the generated triplets since image pairs contain inappropriate images or GPT-4o may produce inaccurate prompts. This section looks into the filtering procedure in detail. 

We looked at the image pairs and subjectively classified all the triplets into the following seven categories. We included triplets classified as 1., 2., or 3. in our dataset. We used Label Studio~\cite{labelstudio} to label the triplets smoothly. \Fref{fig:label-studio} shows the interface we created on Label Studio.

\begin{enumerate}
    \item Perfect: The generated editing prompt mentions every component different between the original and ground truth images.
    \item Nothing wrong but not complete: The prompt contains no false statements but does not mention some subtle components different between the two images.
    \item Something wrong but acceptable: The prompt contains slight errors that seemingly do not affect the output.
    \item Too incomplete to be acceptable: The prompt does not mention the main change between the images.
    \item Too wrong to be acceptable: The prompt has critical false statements.
    \item Not comprehensible: The prompt is not in proper English.
    \item Inappropriate triplet: The content of the images seems inappropriate to include in the dataset.
\end{enumerate}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{img/filtering-success.pdf}
    \caption{The example triplets included in our dataset}
    \label{fig:filtering-success}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{img/filtering-failure.pdf}
    \caption{Some notable examples excluded in the filtering process}
    \label{fig:filtering-failure}
\end{figure*}
We show some example triplets in each category to show interesting prompts generated by GPT-4o. \Fref{fig:filtering-success} shows some successful triplets, and \Fref{fig:filtering-failure} shows some failure cases. No triplets fell into Category 6. 

We first describe the successful cases and why we classified those triplets into each category. The top-left of \Fref{fig:filtering-success} shows an example of GPT-4o looking at the images and accurately telling the differences between them, not just by comparing the prompts. The two images have the same caption: ``swan''. However, the prompt generated by GPT-4o could tell that the outline in the ground truth image is gray and the beak is orange. The bottom-left triplet shows how the example in the prompt (see \Sref{subsec:appendix-instruction-prompt}) affected the results. The image pair of this example is just the opposite of the example in the prompt. Therefore, one may expect the generated prompt to be ``Make the plane land.'' However, GPT-4o could invent a different way of telling the same thing. The top-right is one of the triplets classified as ``Nothing wrong but not complete.'' The generated prompt here could indicate that the hair and shirt color differ, but it could not tell that the mouth color changed to red. However, we included this in our dataset since the mouth does not cover much of the image, and the prompt could mention the main differences between the emojis. The bottom-right example was classified as ``Something wrong but acceptable.'' The prompt in this triplet mentioned the raindrops that are not present in the original image. We included this triplet because removing the raindrops is impossible, meaning complying with this part would not change the outcome.

We'll now describe the failure cases in \Fref{fig:filtering-failure}. The top-left is one example of a triplet in Category 4. The editing instruction could not pinpoint the main difference: the tree's shape. The bottom two examples in the left column are from Category 5: Too wrong to be acceptable. One notable example here is that the prompt mentioned the second image, although we asked not to do so in the prompt (see \Sref{subsec:appendix-instruction-prompt}). We excluded such triplets since the editing model could not know how to edit the image. Another type of failure is that GPT-4o could be wrong in visual concepts, as in the bottom-left example. It instructs to turn the arrow clockwise, while it should say counter-clockwise. In another generated prompt, GPT-4o mistook gray for light blue, which would significantly affect the editing result. Therefore, we excluded these triplets from our dataset.

There were four types of inappropriate triplets (Category 7), and we illustrated three of them in the right row of \Fref{fig:filtering-failure}. 
\begin{enumerate}
    \item Some image pairs had identical original and ground truth images. These two characters had different Unicode codepoints and, thus, different captions. The original image was ``FATHER CHRISTMAS'' (U+1F385), while the ground truth image was captioned as ``MAN + CHRISTMAS TREE'' (U+1F468, U+200D, U+1F384). Since the images have different captions, the distance metric explained in \Sref{subsec:image-pair-extraction} was big enough not to be excluded in Step 1. The prompt also suggests that the captions of the images greatly influenced the generated instructions.
    \item Some images were strongly related to religious concepts, as in the middle row of \Fref{fig:filtering-failure}. We considered these images unsuitable for our dataset for the same reason for excluding flags. 
    \item We considered the bottom-right example as promoting gender bias in the evaluation. To successfully edit the original image, the editing model should recognize what the ``gender attributes'' point to. 
    \item The emoji datasets included images that directly mean something offending, such as the ``REVERSED HAND WITH MIDDLE FINGER EXTENDED'' (U+1F595). We excluded triplets with those images.
\end{enumerate}

\subsection{Additional analysis of the created dataset}
\begin{figure*}
  \centering
  \begin{minipage}{0.57\hsize}
      \centering
      \includegraphics[height=5cm]{img/heatmap_before.pdf}
      \includegraphics[height=5cm]{img/heatmap_after.pdf}
      \caption{Distribution of the SVG emoji datasets to which the images in the created dataset belong. The left figure shows the distribution of triplets before filtering, and the right shows the distribution after filtering.}
      \label{fig:distribution}
  \end{minipage}
  \hspace{1em}
  \begin{minipage}{0.4\hsize}
    \centering
    \includegraphics[height=4cm]{img/dataset-comparison-fewer.pdf}
    \caption{Images from Twemoji and their corresponding images from Noto Emoji. Emojis in Twemoji are mostly flat and simpler, while emojis in Noto Emoji tend to be more detailed. These image pairs are the ones actually included in the dataset.}
    \label{fig:dataset-comparison}
  \end{minipage}
\end{figure*}
This section shows the statistics of the dataset. We focus on which dataset the images in the generated dataset originated. \Fref{fig:distribution} shows the distribution of the origin dataset. The figure shows that the images before and after editing mainly come from the same datasets, but image pairs from distinct datasets are also present.

This result verifies the effectiveness of using multiple datasets as the source data. Emojis have a unique style depending on the dataset to which they belong. We show the style differences between datasets in \Fref{fig:dataset-comparison}. In this figure, the emojis in each column have the same Unicode codepoint. The above images are from Twemoji, and the bottom are from Noto Emoji. These image pairs are in the built dataset. We could obtain such style transfer tasks only by using multiple source datasets. 
% Analysis on the SVG Code Lengthはカット（あまりSVGの長さに注目を向けたくないので）