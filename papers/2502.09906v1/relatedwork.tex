\section{Related Work}
This section will first review the current available public insect datasets. Then, we present the current approaches to developing foundation models and large-vision assistant models.

\subsection{Insect Datasets}

Previous studies have presented insect datasets on a small scale for recognition tasks.
\cite{venugoban2014image} introduced a dataset containing $20$ species, with $10$ samples per species.
Later, \cite{xie2015automatic} presented a dataset with $1,440$ samples from $24$ species. 
More recently, larger datasets suitable for deep learning have been developed. \cite{xie2018multi} created a dataset of $4,500$ images spanning $40$ species for insect classification, and \cite{liu2016localization} introduced a dataset with over 5,000 samples focused on insect recognition and localization. 
PestNet \cite{liu2019pestnet}, and AgriPest \cite{wang2021agripest} were specifically developed for small pest detection tasks.
Additionally, \cite{wu2019ip102} introduced IP102, a large-scale dataset containing over $75,000$ insect samples from $102$ species for classification and detection tasks. 
Meanwhile, \cite{van2021benchmarking} presented a dataset with more than $723,000$ samples representing $2,752$ species from the Arthropoda phylum.
Although prior efforts promoted the development of vision and machine intelligence in precision agriculture, no dataset has a large volume of samples and diverse species for insect-related foundation model training. 
Therefore, this work introduces a novel dataset that not only contains a large number of samples, i.e. 1M images, but also has hierarchical labels from the high to the low taxonomy level, including class, order, family, genus, and species.
Table \ref{tab:data_comparison}  compares our proposed dataset with the prior ones.
In comparison with prior datasets, the number of images in our proposed Insect-1M dataset is $13\times$ higher than the prior IP102 dataset, and the number of species is $335\times$ higher than IP102 \cite{wu2019ip102}.
To preserve the rights of datasets and authors of images, instead of publishing images, we only provide labels and links to download images.

\subsection{Self-supervised Pre-training}

Self-supervised pre-training has gained significant research interest as a strategy for solving various visual recognition tasks, i.e., classification, localization, segmentation, video recognition, tracking, and many other tasks \cite{he2022masked, truong2022direcformer, nguyen2021clusformer, truong2021bimal, truong2024fairness, truong2023fredom, truong2023liaad,nguyen2023brainformer,nguyen2023fairness,sefl_supervised_medical,nguyen2023micron,nguyen2022two}.
SimCLR \cite{chen2020simple} learned visual representations through a contrastive learning framework applying various image augmentations.
MoCo \cite{he2020momentum} presented momentum updating to improve the encoder learning for image representations using contrastive learning. 
This framework was later refined to enhance the performance of SimCLR without needing a large batch size \cite{chen2020improved}.
The later method MoCo-V3 \cite{chen2021empirical} further improved by removing the memory queue, ensuring training stability for greater batch sizes.
DINO \cite{caron2021emerging} introduced a self-supervised learning method based on knowledge distillation, which was later extended to DINO-V2 \cite{oquab2023dinov2}, providing improved stability when scaling the size of models and data.
BEiT \cite{bao2021beit} presented a masked image modeling task where discrete visual tokens from the original image were used as prediction targets.
MAE \cite{he2022masked} and SimMIM \cite{xie2022simmim} used a decoder to directly reconstruct pixel values in masked image regions. 
Jigsaw-ViT \cite{chen2023jigsaw} proposed a pre-training task for transformer models by finding spatial positions from shuffled image patches.
This approach was also applied to temporal data to enhance video modeling robustness \cite{truong2022direcformer}.
Micron-BERT \cite{nguyen2023micron} explored subtle changes in facial videos by learning to detect minor differences in images where regions had been swapped between frames.

\subsection{Joint Vision-Language Pre-training}
Recent advantages of joint vision-language pre-training models have been introduced.
CLIP \cite{radford2021learning} and ALIGN \cite{jia2021scaling} demonstrated that dual-encoder models trained on image-text pairs with contrastive objectives can learn strong representations for cross-modal alignment and zero-shot image recognition tasks.
LiT \cite{zhai2022lit} and BASIC \cite{pham2023combined} introduced zero-shot transfer learning techniques by training the text model to learn from the pre-trained image model through contrastive losses on large-scale datasets.
SimVLM \cite{wang2021simvlm}, OFA \cite{wang2022ofa}, and BLIP \cite{li2022blip} employed an encoder-decoder architecture trained with language generative losses, achieving high performance on vision-language benchmarks.
CoCa \cite{yu2022coca} combined contrastive learning with generative image captioning to improve global representation learning and fine-grained image-text alignment.
Subsequent research \cite{zhai2023sigmoid} utilized sigmoid loss to calculate image-text similarity, enabling batch size scaling.
LexLIP \cite{luo2023lexlip} mapped images into a lexicon space to facilitate sparse image-text matching, while EQSIM \cite{wang2023equivariant} computed similarity via equivariant changes between images and text.

\subsection{Large Language-Vision Assistant Models}

The development of large-scale data processing and large language models (LLMs) has provided a new vehicle to solve complex problems, including multimodal data. Some of them can be accounted including large vision-language models \cite{liu2024improved, liu2024visual}, large video-language models \cite{weng2024longvlm, zhao2023learning}, or large audio-language models \cite{hussain2023m, ghosal2023text}.
By incorporating the power of LLMs, the large-scale multimodal models (LMMs) have revolutionized the research of large-scale multimodal.
In the design of LMMs, different input modalities, i.e., images/videos and languages, are connected to LLMs via the project modules \cite{liu2024visual}. Then, the alignment across modalities is performed via cross-attention \cite{liu2024improved}, Q-Formers \cite{li2023blip}, or MLP \cite{liu2024visual}.
The training procedure of LMMs typically has two major steps: pre-training and instruction-tuning.
While the first stage learns the alignment of features across modalities, the second stage enables reasoning about concepts in multimodal inputs and tasks.
Recently, Chen and Zhang \cite{chenfedmbridge} improved LMM learning via multimodal federated learning.
\blue{Other studies enhanced the performance of LMM by introducing high-resolution, Fine-grained, and Pixel-level Vision approaches \cite{ren2024pixellm, yuan2024osprey, zhang2024omgllava, fei2024vitron, wu24next}. However, these prior studies have not been specifically designed to address the challenges of modeling micro-features of insects.}
Several benchmarks were introduced to evaluate the LMM performance, e.g., MMMU \cite{yue2024mmmu}, and
MM-SpuBench \cite{ye2024mm}.



\begin{figure*}[!t]
\begin{center}
\includegraphics[width=\linewidth]{figures/treemap.pdf}
\end{center}
\caption{\textbf{Treemap of the Multimodal Dataset.} Nested boxes represent classes, orders, and families. The size of the boxes represents the relative number of samples.}
\label{fig:treemap}
\vspace{-5mm}
\end{figure*}