\section{Related Work}
This section will first review the current available public insect datasets. Then, we present the current approaches to developing foundation models and large-vision assistant models.

\subsection{Insect Datasets}

Previous studies have presented insect datasets on a small scale for recognition tasks.
____ introduced a dataset containing $20$ species, with $10$ samples per species.
Later, ____ presented a dataset with $1,440$ samples from $24$ species. 
More recently, larger datasets suitable for deep learning have been developed. ____ created a dataset of $4,500$ images spanning $40$ species for insect classification, and ____ introduced a dataset with over 5,000 samples focused on insect recognition and localization. 
PestNet ____, and AgriPest ____ were specifically developed for small pest detection tasks.
Additionally, ____ introduced IP102, a large-scale dataset containing over $75,000$ insect samples from $102$ species for classification and detection tasks. 
Meanwhile, ____ presented a dataset with more than $723,000$ samples representing $2,752$ species from the Arthropoda phylum.
Although prior efforts promoted the development of vision and machine intelligence in precision agriculture, no dataset has a large volume of samples and diverse species for insect-related foundation model training. 
Therefore, this work introduces a novel dataset that not only contains a large number of samples, i.e. 1M images, but also has hierarchical labels from the high to the low taxonomy level, including class, order, family, genus, and species.
Table \ref{tab:data_comparison}  compares our proposed dataset with the prior ones.
In comparison with prior datasets, the number of images in our proposed Insect-1M dataset is $13\times$ higher than the prior IP102 dataset, and the number of species is $335\times$ higher than IP102 ____.
To preserve the rights of datasets and authors of images, instead of publishing images, we only provide labels and links to download images.

\subsection{Self-supervised Pre-training}

Self-supervised pre-training has gained significant research interest as a strategy for solving various visual recognition tasks, i.e., classification, localization, segmentation, video recognition, tracking, and many other tasks ____.
SimCLR ____ learned visual representations through a contrastive learning framework applying various image augmentations.
MoCo ____ presented momentum updating to improve the encoder learning for image representations using contrastive learning. 
This framework was later refined to enhance the performance of SimCLR without needing a large batch size ____.
The later method MoCo-V3 ____ further improved by removing the memory queue, ensuring training stability for greater batch sizes.
DINO ____ introduced a self-supervised learning method based on knowledge distillation, which was later extended to DINO-V2 ____, providing improved stability when scaling the size of models and data.
BEiT ____ presented a masked image modeling task where discrete visual tokens from the original image were used as prediction targets.
MAE ____ and SimMIM ____ used a decoder to directly reconstruct pixel values in masked image regions. 
Jigsaw-ViT ____ proposed a pre-training task for transformer models by finding spatial positions from shuffled image patches.
This approach was also applied to temporal data to enhance video modeling robustness ____.
Micron-BERT ____ explored subtle changes in facial videos by learning to detect minor differences in images where regions had been swapped between frames.

\subsection{Joint Vision-Language Pre-training}
Recent advantages of joint vision-language pre-training models have been introduced.
CLIP ____ and ALIGN ____ demonstrated that dual-encoder models trained on image-text pairs with contrastive objectives can learn strong representations for cross-modal alignment and zero-shot image recognition tasks.
LiT ____ and BASIC ____ introduced zero-shot transfer learning techniques by training the text model to learn from the pre-trained image model through contrastive losses on large-scale datasets.
SimVLM ____, OFA ____, and BLIP ____ employed an encoder-decoder architecture trained with language generative losses, achieving high performance on vision-language benchmarks.
CoCa ____ combined contrastive learning with generative image captioning to improve global representation learning and fine-grained image-text alignment.
Subsequent research ____ utilized sigmoid loss to calculate image-text similarity, enabling batch size scaling.
LexLIP ____ mapped images into a lexicon space to facilitate sparse image-text matching, while EQSIM ____ computed similarity via equivariant changes between images and text.

\subsection{Large Language-Vision Assistant Models}

The development of large-scale data processing and large language models (LLMs) has provided a new vehicle to solve complex problems, including multimodal data. Some of them can be accounted including large vision-language models ____, large video-language models ____, or large audio-language models ____.
By incorporating the power of LLMs, the large-scale multimodal models (LMMs) have revolutionized the research of large-scale multimodal.
In the design of LMMs, different input modalities, i.e., images/videos and languages, are connected to LLMs via the project modules ____. Then, the alignment across modalities is performed via cross-attention ____, Q-Formers ____, or MLP ____.
The training procedure of LMMs typically has two major steps: pre-training and instruction-tuning.
While the first stage learns the alignment of features across modalities, the second stage enables reasoning about concepts in multimodal inputs and tasks.
Recently, Chen and Zhang ____ improved LMM learning via multimodal federated learning.
\blue{Other studies enhanced the performance of LMM by introducing high-resolution, Fine-grained, and Pixel-level Vision approaches ____. However, these prior studies have not been specifically designed to address the challenges of modeling micro-features of insects.}
Several benchmarks were introduced to evaluate the LMM performance, e.g., MMMU ____, and
MM-SpuBench ____.



\begin{figure*}[!t]
\begin{center}
\includegraphics[width=\linewidth]{figures/treemap.pdf}
\end{center}
\caption{\textbf{Treemap of the Multimodal Dataset.} Nested boxes represent classes, orders, and families. The size of the boxes represents the relative number of samples.}
\label{fig:treemap}
\vspace{-5mm}
\end{figure*}