\section{Introduction}\label{sec:introduction}

Generative modeling has become a fundamental task in machine learning, with different frameworks achieving remarkable success across various data modalities \citep{ramesh2022hierarchical, rombach2022high}. While diffusion models have shown exceptional effectiveness \citep{ho2020denoising, song2020score}, they rely on constrained probability paths requiring specialized techniques. Continuous normalizing flows (CNFs) \citep{chen2018neural} offer greater flexibility \citep{song2021maximum}, but remain computationally expensive due to solving high-dimensional ODEs during training and sampling \citep{ben2022matching, rozen2021moser, grathwohl2018scalable}. Flow Matching (FM) \citep{lipman2023flow} addresses these challenges by expressing the transport field through conditional fields, enabling simulation-free learning of ODEs through assumed dynamics towards specific endpoints.

Recent developments have extended flow matching in two key directions: Variational Flow Matching (VFM) \citep{eijkelboom2024variational} reframes the problem as posterior inference over trajectories, providing a probabilistic perspective with flexible modeling choices, while other work has demonstrated flow matching's potential on geometric structures \citep{chen2024flow}. This geometric extension is particularly relevant for data on non-Euclidean spaces, where underlying geometry crucially influences probability paths.

In this paper, we develop Riemannian Gaussian VFM, bridging these directions by extending VFM to Riemannian manifolds with closed-form metrics. We derive a variational objective for Gaussian distributions in these spaces, establishing a geometric extension of VFM, comparable yet fundamentally different from RFM.
Through experiments on a spherical checkerboard dataset, we demonstrate that Riemannian VFM effectively leverages manifold structure, outperforming geometry-unaware methods. 