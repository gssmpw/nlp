\section{Riemannian Gaussian Variational Flow Matching}\label{sec:method}

The geometric generalization of the VFM framework stems from the observation that learning the posterior probability $p_t(x_1\mid x)$ implicitly encodes the geometry of the distribution’s support. For example, in CatFlow \citep{eijkelboom2024variational}, defining $q_t^{\theta}(x_1\mid x)$ as a categorical distribution ensures that the velocities point towards the probability simplex.  This raises the question of whether other geometric information about the support of $p_1$ can be similarly encoded in $q_t^{\theta}(x_1\mid x)$.

To investigate this, we consider the case where $p_t(x_1\mid x)$ is defined as a Gaussian distribution, but with its support on the manifold $\mathcal{M}:= \operatorname{supp}(p_1)$ instead of the ambient Euclidean space. In this setting, the Riemannian Gaussian distribution naturally arises as a generalization of the Gaussian to a Riemannian manifold. We will refer to CFM and RFM as \textit{vanilla} models, minimizing velocity differences, while VFM and the proposed model are \textit{variational}, matching endpoints in their loss.

\subsection{Model Definition}
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.90\textwidth]{figures/r-vfm-diagram.png} 
    \caption{\textbf{Left}: Overview of the main Flow Matching models relevant to our proposed approach, illustrating their relationships. \textbf{Right}: Visualization of each model’s predictions (color-coded to match the corresponding model on the left) for a target distribution \(p_1\) supported on the sphere \(\mathbb{S}_1\).
    }
    \label{fig:schema}
\end{figure}

\paragraph{Riemannian Gaussian.} Let \(\mathcal{M}\) be a Riemannian manifold with metric \(\mathbf{g}\). The Riemannian Gaussian (RG) distribution \citep{pennec2006intrinsic} is defined as
\begin{equation}
    \mathcal{N}_{\text{Riem}}(z \mid \sigma, \mu) 
= \frac{1}{C} \exp\!\left(-\frac{\text{dist}_{\mathbf{g}}(z, \mu)^2}{2\sigma^2}\right),
\end{equation}
where \(z,\mu\in\mathcal{M}\) (with \(\mu\) as the mean), \(\sigma>0\) is a scale parameter, and \(\text{dist}_{\mathbf{g}}(z, \mu)\) denotes the geodesic distance determined by \(\mathbf{g}\). The constant $C$ depends on both $z$ and $\mu$, 
and it normalizes the distribution over \(\mathcal{M}\). A more detailed geometric explanation can be found in Appendix \ref{app:riem_gauss}. 


\paragraph{RG-VFM objective.} We define the Riemannian Gaussian-VFM (RG-VFM) objective by using the Riemannian Gaussian as our variational approximation, i.e.
\begin{equation}
\mathcal{L}_{\text{RG-VFM}} (\theta) = -\mathbb{E}_{t,x_1,x}\left[\log \mathcal{N}_{\text{Riem}}(x_1 \mid \mu_t^{\theta}(x), \sigma_t(x))\right].
\end{equation} 
In the Gaussian VFM case, this setting reduces to a straightforward MSE optimization, so it is natural to wonder whether a similar simplification holds here. In fact, such a simplification exists under two assumptions: (1) the manifold is homogeneous -- that is, any point can be transformed into any other by a distance-preserving symmetry (a formal definition is provided in \cref{app:riemannian}); and (2) we have access to a closed-form expression for its geodesics. Formally, the following holds (see \cref{sec:rvfm} for details):
\begin{restatable}{restatable_proposition}{rgvfm}
Let $\mathcal{M}$ be a homogeneous manifold with closed-form geodesics. Then, the VFM objective reduces to
\begin{equation} \label{eq:rgrfm}
    \mathcal{L}_{\text{RG-VFM}} (\theta) = \mathbb{E}_{t,x_1,x}\left[ || \log_{x_1}(\mu_t^{\theta}(x)) ||_{\mathbf{g}}^2\right].
\end{equation}
\end{restatable}

Minimizing this loss is equivalent to computing the Fréchet mean of the distribution, that is, compute 
$\mu^{\star}= \argmin_{\mu_{\theta}\in \mathcal{M}} \int_M \text{dist}_{\mathbf{g}}(x_1,\mu_{\theta})^2 \,d x_1,$ i.e. the point $\mu_{\theta}$ that minimizes the expected squared geodesic distance to the data points \citep{frechet1948elements}. Intuitively, this can be viewed as a generalization of the mean squared error from the Euclidean setting to a Riemannian framework.

The RG-VFM objective (eq. \ref{eq:rgrfm}) minimizes the geodesic distance on $\mathcal{M}$ between target and predicted endpoints, so it only needs to learn the local geometry of $\mathcal{M}$ around $p_1$. In contrast, vanilla RFM requires that $\operatorname{supp}(p_0)$ lie entirely on $\mathcal{M}$ because its vector fields depend on the manifold’s intrinsic geometry; off-manifold points can lead to unstable estimates of tangents and vector fields. 

For this study, we consider two versions: (1) RG-VFM-$\mathbb{R}^3$, where $p_0$ is Euclidean, $\mathcal{M} \subsetneq \operatorname{supp}(p_0)$, and we use linear interpolation for conditional velocities, and (2) RG-VFM-$\mathcal{M}$, where $\operatorname{supp}(p_0) = \mathcal{M}$, we use geodesic interpolation, and define velocities on the tangent spaces to $\mathcal{M}$.

\subsection{How does RG-VFM fit in the existing Flow Matching framework?}

Figure \ref{fig:schema} (left) illustrates how RG-VFM fits within the framework of related FM models. In VFM, a \textit{probabilistic} generalization of CFM is obtained by making the posterior distribution explicit and customizable, obtaining standard CFM under the choice of a specific Gaussian (see \cite{eijkelboom2024variational}). In contrast, RFM serves as a \textit{geometric} generalization of CFM, where the model and its objective take into account the intrinsic properties and metric of the underlying Riemannian manifold. The same happens for the variational models: VFM with a Gaussian posterior is a particular instance of RG-VFM when the geometry is Euclidean. In Euclidean space,
$
\|\log_{x_1}(\mu_t^{\theta}(x))\|_{\mathbf{g}}^2
$
reduces to
$
\|\mu_t^{\theta}(x) - x_1\|_2^2,
$
thereby recovering the VFM objective.

A further comparison can be made between the simplified version of RFM and RG-VFM-$\mathcal{M}$, where $\mathcal{M}$ is a homogeneous manifold with closed-form geodesics. The variational model (RG-VFM) is \textit{not} a direct generalization of vanilla RFM because, unlike in Euclidean space, \textit{tangent spaces at different points on a manifold do not coincide}. This difference is reflected in the models' outputs (\cref{fig:schema}): vanilla models predict velocity fields, which are integrated as ODEs to construct flows, whereas variational models predict endpoint distributions, ideally aligning with the target distribution $p_1$. 

In Euclidean space, the difference between two vectors starting at $x$ and pointing to different endpoints is simply the vector between those endpoints, leading to identical $L_2$ terms in the objectives, i.e. $\|\mu_t^{\theta}(x) - x_1\|_2^2$ for VFM and $\|u_t(x \mid x_1) - v_t^\theta(x)\|^2$ for CFM. However, since $T_{x}\mathcal{M} \neq T_{x_1}\mathcal{M}$ in general, in their geometric counterparts this equivalence no longer holds: indeed, the difference vector in the RFM objective,  
$
v_t^{\theta}(x) - \frac{\log_{x}(x_1)}{(1-t)},
$
is in $T_{x}\mathcal{M}$, while $\log_{x_1}(\mu_t^{\theta}(x))$ is in $T_{x_1}\mathcal{M}$. This fundamental distinction separates RG-VFM from RFM. More details are in \cref{app:rgvfm_rfm}.
