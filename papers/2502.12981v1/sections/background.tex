\section{Background}\label{sec:background}

\paragraph{Flow Matching.} Modern generative modeling interprets sampling from a target distribution $p_1$ as transporting a base distribution $p_0$ by learning dynamics. Typically, $p_0$ is a standard Gaussian, and the transformation follows a time-dependent mapping $\varphi_t \colon [0, 1] \times \mathbb{R}^D \to \mathbb{R}^D$ where $\varphi_0$ is the identity and $\varphi_1$ pushes $p_0$ onto $p_1$, e.g. normalizing flows \citep{chen2018neural} use an ODE governed by some time-dependent velocity field $u_t$. Though likelihood training is possible through the change of variables formula, solving an ODE  during training is expensive. Flow Matching (FM) \citep{lipman2023flow} bypasses this by directly learning the velocity field:
\begin{equation}
    \mathcal{L}_{\mathrm{FM}}(\theta) =
\mathbb{E}_{t,x}\bigl[
|| u_t(x) - v_t^\theta(x) ||^2
\bigr].
\end{equation}
This is made computationally feasible by reformulating $u_t$ with a conditional velocity field (i.e. assumed dynamics towards a given $x_1$), giving rise to Conditional Flow Matching (CFM):
\begin{equation}
\label{eq:cfmloss}
\mathcal{L}_{\mathrm{CFM}}(\theta) =
\mathbb{E}_{t,x_1,x}
\left[ || u_t(x \mid x_1) - v_t^\theta(x) ||^2 \right].
\end{equation}
Minimizing \cref{eq:cfmloss} provides an unbiased  estimate of $\nabla_\theta \mathcal{L}_{\mathrm{FM}}$, allowing efficient per-sample training. 

\paragraph{Riemannian Flow Matching.} Riemannian Flow Matching (RFM) \citep{chen2024flow} extends FM to Riemannian manifolds. Given a smooth Riemannian manifold $\mathcal{M}$ with closed-form geodesics and metric $\mathbf{g}$, RFM learns a vector field $v_t$:
\begin{equation}\label{eq:rfm}
\mathcal{L}_{\text{RFM}} (\theta) = \mathbb{E}_{t,x_1,x}\left[\left\|v_t^{\theta}(x) - {\log_{x}(x_1)} / {(1-t)}\right\|_\mathbf{g}^2\right],
\end{equation}  
with $\log_{x}(x_1)$ denoting the Riemannian log map, which returns the initial velocity vector of the geodesic connecting $x$ to $x_1$ (more details on Riemannian manifolds are in \cref{app:riemannian}). 
Unlike Euclidean Flow Matching, RFM respects the curvature and geodesics of the underlying space $\mathcal{M}$. Through geodesic or spectral distances, it enables simulation-free training when manifold operations are available, and can utilize approximate distances when closed-form geodesics are intractable, maintaining theoretical guarantees while enabling efficient generative modeling.

\paragraph{Variational Flow Matching.}
Variational Flow Matching (VFM) \citep{eijkelboom2024variational} reformulates FM by introducing a variational distribution $q_t^\theta(x_1 \mid x)$ to approximate the unknown posterior $p_t(x_1 \mid x)$, where the learned velocity $v_t^{\theta}$ is expressed as the expectation of the condition velocity under this variational approximation over trajectories. 
Then, the VFM objective is to minimize the KL divergence between joint distributions, i.e.:
\begin{equation}
    \mathcal{L}_{\mathrm{VFM}}(\theta) = \mathbb{E}_t \left[\mathrm{KL}\left(p_t(x_1, x) ~||~ q_t^{\theta}(x_1, x) \right)\right] = -\mathbb{E}_{t, x_1, x} \bigl[ \log q_t^\theta(x_1 \mid x) \bigr] + \text{const.}
\end{equation}
When $u_t(x \mid x_1)$ is linear in $x_1$ -- e.g. a straight-line interpolation -- the expectation depends only on marginal distributions, implying this objective reduces to a series of $D$ univariate tasks:
\begin{equation}\label{eq:vfm}
\mathcal{L}_{\mathrm{VFM}}(\theta) =
-\mathbb{E}_{t, x_1, x} \left[
\sum_{d=1}^{D} \log q_t^{\theta}(x_1^d \mid x) \right], \text{ e.g. }  \mathcal{L}_{\mathrm{VFM}}(\theta) =\mathbb{E}_{t,x_1,x}\left[\,\|\mu_t^{\theta}(x) - x_1\|^2\right],
\end{equation}
if $q_t^{\theta}$ is Gaussian, relating VFM directly back to FM (see \citet{eijkelboom2024variational} for details). A key feature of VFM is its flexibility in choosing $q_t^\theta$, as different choices of $q_t^\theta$ allow adaptation to various geometries and data types, improving efficiency and expressiveness.
