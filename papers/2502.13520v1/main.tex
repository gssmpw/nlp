% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{contour}

\newcommand{\barec}{\textbf{\textsc{Barec}}}
%\newcommand{\barec}{\textbf{\textsc{Carar}}}
% Carar Corpus of Arabic Readability 

\input{macros}
\usepackage{arabtex}
%\def\endabstract{\egroup}

\usepackage{utf8}
\usepackage{epstopdf}




%\newcommand{\BlevAlif}{{\sc c1-alif}}
%\newcommand{\BlevBa}{{\sc c2-ba}}
%\newcommand{\BlevJim}{{\sc c3-jim}}
%\newcommand{\BlevDal}{{\sc c4-dal}}
%\newcommand{\BlevHe}{{\sc c5-ha}}
%\newcommand{\BlevWaw}{{\sc c6-waw}}
%\newcommand{\BlevZay}{{\sc c7-zay}}
%\newcommand{\BlevHa}{{\sc c8-\underline{h}a}}
%\newcommand{\BlevTa}{{\sc c9-\underline{t}a}}
%\newcommand{\BlevYa}{{\sc c10-ya}}
%\newcommand{\BlevKaf}{{\sc c20-kaf}}
%\newcommand{\BlevLam}{{\sc c30-lam}}
%\newcommand{\BlevMim}{{\sc c40-mim}}
%\newcommand{\BlevNun}{{\sc c50-nun}}
%\newcommand{\BlevSin}{{\sc c60-sin}}
%\newcommand{\BlevAyn}{{\sc c70-ayn}}
%\newcommand{\BlevFa}{{\sc c80-fa}}
%\newcommand{\BlevSad}{{\sc c90-sad}}
%\newcommand{\BlevQaf}{{\sc c100-qaf}}

\newcommand{\BlevAlif}{{\bf 1-alif}}
\newcommand{\BlevBa}{{\bf 2-ba}}
\newcommand{\BlevJim}{{\bf 3-jim}}
\newcommand{\BlevDal}{{\bf 4-dal}}
\newcommand{\BlevHe}{{\bf 5-ha}}
\newcommand{\BlevWaw}{{\bf 6-waw}}
\newcommand{\BlevZay}{{\bf 7-zay}}
\newcommand{\BlevHa}{{\bf 8-\underline{h}a}}
\newcommand{\BlevTa}{{\bf 9-\underline{t}a}}
\newcommand{\BlevYa}{{\bf 10-ya}}
\newcommand{\BlevKaf}{{\bf 20-kaf}}
\newcommand{\BlevLam}{{\bf 30-lam}}
\newcommand{\BlevMim}{{\bf 40-mim}}
\newcommand{\BlevNun}{{\bf 50-nun}}
\newcommand{\BlevSin}{{\bf 60-sin}}
\newcommand{\BlevAyn}{{\bf 70-ayn}}
\newcommand{\BlevFa}{{\bf 80-fa}}
\newcommand{\BlevSad}{{\bf 90-sad}}
\newcommand{\BlevQaf}{{\bf 100-qaf}}

%\usepackage[latin1]{inputenc}
%\usepackage[T1]{fontenc}
%\usepackage{xstring}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{A Large and Balanced Corpus \\ for Fine-grained Arabic Readability Assessment}
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

%\author{First Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\\And
%  Second Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\}

\author{Khalid N. Elmadani,\textsuperscript{\textdagger} Nizar Habash,\textsuperscript{\textdagger} Hanada Taha-Thomure\textsuperscript{$\ddagger$}
  \\
  \textsuperscript{\textdagger}Computational Approaches to Modeling Language Lab, New York University Abu Dhabi\\
   \textsuperscript{$\ddagger$}Zai Arabic Language Research Centre, Zayed University\\
  \texttt{\{khalid.nabigh,nizar.habash\}@nyu.edu, Hanada.Thomure@zu.ac.ae}
  }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\hide{
https://docs.google.com/spreadsheets/d/1FqHeX5hHs6mi2wrrOL9xEE_CmgZxCQvN25zXRBoQfbQ/edit?gid=1721996297#gid=1721996297

1. Intro >1 page (Nizar

2. Related > 1 page (Nizar)

(3 pages)  
3. Readability Annotation > 1 page (Nizar)
3.1 Guidelines
3.2 Annotation Process (IAA in resuls)
   examples from levels + appendix
%[[[[[[[[[[]]]]]]]]]]
4. Barec Corpus > 1 page (Khalid)
4.1 Corpus Selection
4.2 Corpus Statistics + Readbility statistics
4.3 Data splits

====
5. Evaluation  (3) (Khalid)
5.1 Metrics  ++ Acc@753
5.2 IAA Results

5.3 Experimental Settings 
     Bert zoo + Input variants.
    Loss results
    Ensembles
    Oracle
5.4 Dev results

5.4 Test results

6. Conclusion+Future (Khalid)

Limitations
Ethical Considerations













}


\begin{document}
\maketitle


\setcode{utf8}
\vocalize

%\textbf{\textsc{Hello}}
%\textsc{Hello} 

\begin{abstract}
%This paper presents the foundational framework and initial findings of the Corpus of Arabic Readability ({\barec}) project,\footnote{\<قرار> \textit{qrAr} is Arabic for `decision'.} designed to address the  need for comprehensive Arabic language resources aligned with diverse readability levels.
%This paper presents the foundational framework and initial findings of the Balanced Arabic Readability Evaluation Corpus ({\barec}) project,\footnote{\<بارق> \textit{bAriq} is Arabic for `very bright and glittering'.} designed to address the need for comprehensive Arabic language resources aligned with diverse readability levels.
%Inspired by the \textbf{Taha/Arabi21} readability reference \cite{Taha:2017:guidelines}, {\barec} aims to provide a standardized reference for assessing sentence-level Arabic text readability across 19 distinct levels, ranging in targets from kindergarten to postgraduate comprehension. 
%Our ultimate goal with {\barec} is to create a comprehensive and balanced corpus that represents a wide range of genres, topics, and regional variations through a multifaceted approach combining manual annotation with AI-driven tools.
%This paper focuses on our meticulous annotation guidelines, demonstrated through the analysis of 10,631 sentences/phrases (113,651 words). 
%The moderate inter-annotator agreement highlights both the task's complexity and our methodology's robustness, with an average pairwise 79.87\%  Quadratic Weighted Kappa. 
%The average pairwise inter-annotator agreement, measured by Quadratic Weighted Kappa, is 79.9\%, reflecting a high level of substantial agreement.
%We also report competitive results for benchmarking automatic readability assessment. 
%
%We will make the {\barec} corpus and guidelines openly accessible to support Arabic language research and education.

This paper introduces the Balanced Arabic Readability Evaluation Corpus (\barec),\footnote{\<بارق> \textit{bAriq} is Arabic for `very bright and glittering'.} a large-scale, fine-grained dataset for Arabic readability assessment. {\barec} consists of 68,182 sentences spanning 1+ million words, carefully curated to cover 19 readability levels, from kindergarten to postgraduate comprehension. 
%
The corpus balances genre diversity, topical coverage, and target audiences, offering a comprehensive resource for evaluating Arabic text complexity. The corpus was fully manually annotated by a large team of annotators. The average pairwise inter-annotator agreement, measured by Quadratic Weighted Kappa, is 81.3\%, reflecting a high level of substantial agreement.
%
Beyond presenting the corpus, we benchmark automatic readability assessment across different granularity levels, comparing a range of techniques. Our results highlight the challenges and opportunities in Arabic readability modeling, demonstrating competitive performance across various methods.
%
To support research and education, we will make {\barec} openly available, along with detailed annotation guidelines and benchmark results.

\end{list}
\end{abstract}






%-----------------------------------------------------------------------
\section{Introduction}

Text readability impacts understanding, retention, reading speed, and engagement \cite{DuBay:2004:principles}. Texts above a student's readability level can lead to disengagement \cite{klare1963measurement}. \newcite{Nassiri:2023} highlighted that readability and legibility depend on both external features (e.g., production, fonts) and content. Text leveling in classrooms helps match books to students' reading levels, promoting independent reading and comprehension \cite{allington2015research}. Developing readability models is crucial for improving literacy, language learning, and academic performance.

Readability levels have long been a key component of literacy teaching and learning.
One of the most widely used systems in English literacy is Fountas and Pinnell \cite{fountas2006leveled}, which employs qualitative measures to classify texts into 27 levels (A to Z+), spanning from kindergarten to adult proficiency. 
Similarly, \newcite{Taha:2017:guidelines}'s system for Arabic has 19 levels from Arabic letters \<أ> A to \<ق> Q.
%
%These fine-grained levels are designed for pedagogical effectiveness, ensuring that young readers experience gradual, measurable progress, especially in early education (K–6)  \cite{BarberKlauda2020}.  
%While such detailed systems are essential for teaching, broader applications like readability research and automated assessments may benefit from simplified leveling systems with fewer categories for efficiency.
%
These fine-grained levels are designed for pedagogical effectiveness, ensuring young readers experience gradual, measurable progress, particularly in early education (K–6) \cite{BarberKlauda2020}. A key advantage is that they can be easily mapped to coarser levels with fewer categories, which may be more efficient for broader applications in readability research and automated assessments.

In this paper we present the Balanced Arabic Readability Evaluation Corpus ({\barec}) -- a large-scale fine-grained readability assessment corpus across a broad space of genres and readability levels.
Inspired by the Taha/Arabi21 readability reference \cite{Taha:2017:guidelines}, which has been instrumental in tagging over 9,000 children's books, {\barec} seeks to establish a standardized framework for evaluating sentence-level\footnote{We segment paragraphs down to syntactic sentences, and use the term \textit{sentence} even for small standalone text segments such as phrases and single words (e.g. book titles).} Arabic text readability across 19 distinct levels, ranging from kindergarten to postgraduate comprehension.  
 



%Barec has 19 levels orchestrated similarly to the earlier systems and developed for practical teaching and learning purposes that can be useful to teachers in the field, children’s and young adults' publishing houses,  and curriculum designers. Despite that, there might be different ways to think about leveling, possibly depending on the purpose they will be used for. 
%A profusion of the number of levels is necessary for teaching and learning, especially in lower grades (up to grade 6). However, for research purposes and other instances where a simplified system is more logical to use, those levels can be reduced and bunched together. An example would be to take the 19 levels in Barec and collapse them into seven levels, or five or even three. These reductions would match other proposal such as  SAMER 5 readability level \cite{AlKhalil:2018:leveled} or  \newcite{Al-Khalifa:2010:automatic} used a 3-level scale.

% Readability, the measure of how easily a reader can understand a written text, is essential for effective communication across diverse audiences. It is closely associated with text leveling, which categorizes texts into readability levels based on factors like orthography, morphology, syntax, and vocabulary complexity.
%
%Developing readability models is vital for improving literacy rates, aiding language learning, and enhancing academic achievement. However, in Arabic language education and research, there is a significant lack of standardized resources for assessing text readability across various proficiency levels. This challenge is compounded by Arabic's intricate linguistic features, such as rich morphology and lexicon, and its highly ambiguous orthography.% , which includes optional diacritics.

Our contributions are:
(a) \textbf{a large-scale curated corpus} with 68K+ sentences (1M+ words) spanning diverse genres; and 
(b) \textbf{benchmarking of automatic readability assessment} models across multiple granularities, including both fine-grained (19 levels) and collapsed tiered systems (e.g., five-level and three-level scales) to support various research and application needs, aligning with previous Arabic readability frameworks \cite{AlKhalil:2018:leveled,Al-Khalifa:2010:automatic}.
 
%Next, \S\ref{sec:related} reviews related work on readability, and \S\ref{sec:desiderata} highlights our guidelines' design desiderata.
%the considerations guiding the development of our guidelines. 
%\S\ref{sec:guidelines} presents the {\barec} guidelines, and \S\ref{sec:annotation} discusses  data selection and annotation. Finally, in \S\ref{sec:eval}, we present inter-annotator agreement results and insights from the annotated corpus.

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
%-------------------------------------------------------------------------\newpage

\section{Related Work}
\label{sec:related}

%Notes:
%https://www.englishprofile.org/
% https://www.cl.cam.ac.uk/~ejb1/epp.html 
%cite cefr readme paper + laila's paper + zaebuc + samer...

% https://arxiv.org/pdf/2405.02144v1
%AARI: Automatic Arabic readability index

%Creating a CEFR Arabic Vocabulary Profile: A frequency-based multi-dialectal
%approach
%https://www.researchgate.net/publication/373237489_Creating_a_CEFR_Arabic_Vocabulary_Profile_A_frequency-based_multi-dialectal_approach

%Approaches, Methods, and Resources for Assessing the Readability of Arabic Texts \cite{Nassiri:2023}
%* leveling in different languages
%* techniques - formulas vs lexicons
%* granularity: books, words (sentences)
%* refer to references in and of SAMER, Juan+Bashar work, Taha Book \cite{habash-etal-2022-camel}


%There has been a lot of interest in the area of Arabic Readability, resulting in a sizeable and diverse body of literature. For a recent survey, see \newcite{Nassiri:2023}. 
%There has been a lot of interest in the area of Arabic Readability, resulting in a sizeable and diverse body of literature \cite{Nassiri:2023}. 
  
%We present a number of related concepts and efforts next. We discuss relevant Arabic linguistic facts as part of the discussion of {\barec} guidelines (\S\ref{sec:guidelines}).




 
\paragraph{Automatic Readability Assessment}
Automatic readability assessment has been widely studied, resulting in numerous datasets and resources \cite{collins-thompson-callan-2004-language,pitler-nenkova-2008-revisiting,feng-etal-2010-comparison,vajjala-meurers-2012-improving,xu-etal-2015-problems,xia-etal-2016-text,nadeem-ostendorf-2018-estimating,vajjala-lucic-2018-onestopenglish,deutsch-etal-2020-linguistic,lee-etal-2021-pushing}. Early English datasets were often derived from textbooks, as their graded content naturally aligns with readability assessment \cite{vajjala-2022-trends}. However, copyright restrictions and limited digitization have driven researchers to crowdsource readability annotations from online sources \cite{vajjala-meurers-2012-improving,vajjala-lucic-2018-onestopenglish} or leverage CEFR-based L2 assessment exams \cite{xia-etal-2016-text}.
 

\paragraph{Arabic Readability Efforts}
Arabic readability research has focused on text leveling and assessment across various frameworks. \newcite{Taha:2017:guidelines} proposed a 19-level system for children's books based on qualitative and quantitative criteria. Other efforts applied CEFR leveling to Arabic, including the KELLY project’s frequency-based word lists, manually annotated corpora \cite{habash-palfreyman-2022-zaebuc, naous-etal-2024-readme}, and vocabulary profiling \cite{soliman2024creating}.
\citet{el-haj-etal-2024-dares} introduced DARES, a readability assessment dataset collected from Saudi school materials.
The SAMER project \cite{al-khalil-etal-2020-large} developed a lexicon with a five-level readability scale, leading to the first manually annotated Arabic parallel corpus for text simplification \cite{alhafni-etal-2024-samer}.
%
Automated readability assessment has also been explored through rule-based and machine learning approaches. Early models relied on surface-level features like word and sentence length \cite{al2004assessment, Al-Khalifa:2010:automatic}, while later work incorporated POS-based and morphological features \cite{Forsyth:2014:automatic, Saddiki:2018:feature}. The OSMAN metric \cite{el-haj-rayson-2016-osman} leveraged script markers and diacritization, and recent efforts \cite{liberato-etal-2024-strategies} achieved strong results using pretrained models on the SAMER corpus. 

Building on these efforts, we curated the {\barec} corpus across genres and readability levels, and manually annotated it at the sentence-level based on an adaptation of Taha/Arabi21 guidelines \cite{Taha:2017:guidelines}, offering finer-grained control and a more objective assessment of textual variation.



%We discuss next our design criteria and then present the guidelines and annotation process in detail.

%\textbf{BAREC Levels: The system we follow to determine the readability level of sentences and phrases. Inspired by Hanada Taha's levels to determine the readability of texts, BAREC levels are five levels that mark the readability of vocabulary (specifically lexical entries), while Barec levels are for the readability of sentences and phrases.
%%%%
%Texts (from paragraphs to chapters and books) may contain sentences and phrases of different levels.}

%%%%%%%%%%%%%%
%https://docs.google.com/spreadsheets/d/1mAhPQGG_snQRetsXvWLTiz5iNhF6R_zfrdFfRgGB1F4/edit#gid=1490799958
\begin{figure*}[t]
\centering
 \includegraphics[width=0.9\textwidth]{Barec-Pyramid-new.pdf}
    \caption{ The {\barec} \textit{Pyramid}  illustrates the relationship across {\barec} levels and linguistic dimensions, three collapsed variants, and education grades.}
\label{fig:barec-pyramid}
\end{figure*}


%\section{Readability Annotation Desiderata}
%\label{sec:desiderata}
%
%We outline below the key desiderata and guiding principles for the {\barec} project guidelines. 
%
%\paragraph{Comprehensive Coverage}  We will ensure that the annotation guidelines cover a wide range of readability levels, from kindergarten (Easy) to postgraduate (Hard) comprehension. The introductory educational levels with easier readability will be more fine-grained relatively compared with the advanced harder readability level.
%
%\paragraph{Detailed Objective Standardization}  We will minimize subjective decisions by developing standardized guidelines that provide a consistent framework for assessing sentence-level Arabic text readability across 19 distinct levels. We will define different dimensions to identify the many ways a text can be easy or hard. The guidelines should nuance such aspects with insights from dialects, syntax, morphology, orthophonology, semantics, and content, recognizing the inherent flaws in methods like simple word or sentence length.
%
%\paragraph{Bias Mitigation}  We will be mindful that the Arab world has a mix of religions, ethnicities, socioeconomic backgrounds, cultural outlooks, and dialects. Therefore, guidelines and data selections should strive to be representative of diversity and eliminate anti-inclusive assumptions. We will design with awareness of different Arab world regions and their dialects' common uniting core, yet recognize limitations that may require future localizations, especially in the lower (easy readability) levels.
%
%\paragraph{Balanced Coverage}  We will balance the annotated data for different readability levels as well as genres, domains, and topics. We acknowledge the inherent imbalance in the amount of text for different levels, such as the scarcity of children's books compared to adult reading materials and their shorter length, which poses a double penalty.
%
%
%\paragraph{Enriching Annotations}  We will select texts to annotate that already have other annotations to maximize the benefits of studying the interplay of readability with other annotations, such as part-of-speech tagging, syntactic treebanking, named-entity recognition, and sentiment analysis.
%
%\paragraph{Quality Control} We will select annotators with experience in Arabic language education and train them while working with them to fine-tune the guidelines. We will strive for high inter-annotator agreement to validate the robustness of the methodology. We will conduct additional rounds of consistency checks.
%
%\paragraph{Open Accessibility} We are committed to making the {\barec}  corpus and guidelines openly accessible to the research and education community, fostering advancements in Arabic language research, technology development, and education.
%
%
%\paragraph{Ethical Considerations}  Regarding the annotated texts, we will limit annotations to fair-use-sized samples of copyrighted data but allow larger sizes of out-of-copyright data that happen to have additional annotations. For annotators, we will pay fair wages and be mindful of the stressful nature of extended hours of annotation. We will strive to simplify interfaces and provide online feedback mechanisms to minimize errors and alleviate tedious subtasks.
 
\hide{
\section{Readability Annotation Desiderata}
\label{sec:desiderata}

We outline below the key principles for the {\barec} project guidelines:


\paragraph{Comprehensive Coverage} Annotation guidelines will span a wide range of readability levels, from kindergarten (Easy) to postgraduate (Hard), with finer distinctions at lower levels.

\paragraph{Objective Standardization} Standardized guidelines will minimize subjectivity, covering 19 readability levels based on factors like dialect, syntax, morphology, semantics, and content, avoiding oversimplifications like word or sentence length.

\paragraph{Bias Mitigation} Guidelines will reflect the diversity of the Arab world’s religions, ethnicities, and dialects, ensuring inclusivity and considering regional variations, especially in easier levels.

\paragraph{Balanced Coverage} Data annotation will try to balance readability levels, genres, and topics, acknowledging the scarcity of certain texts, like children's books, and their inherent shorter length.

\paragraph{Enriching Annotations} Texts with existing annotations (e.g., part-of-speech tagging, named-entity recognition) will be prioritized to support exploring readability in relation to other linguistic features in the future.

\paragraph{Quality Control} Trained annotators will ensure high inter-annotator agreement, with additional consistency checks for methodology robustness.

\paragraph{Open Accessibility} The {\barec} corpus and guidelines will be openly available to support Arabic language research and education.

\paragraph{Ethical Considerations} Annotation will respect fair-use copyright, and annotators will be fairly compensated, with measures in place to reduce task-related fatigue.

%\newpage
}

\begin{table*}[t]
\centering
 \includegraphics[width=\textwidth]{Example-small.pdf}
    \caption{Representative subset of examples of the 19 {\barec} readability levels, with English translations, and readability level reasoning. Underlining is used to highlight the main keys that determined the level.  }
\label{tab:barec-examples}
\end{table*}

%%%%%%%%%%%%%%%%


\section{{\barec} Corpus Annotation} \label{sec:annotation}
In this section, we discuss the guidelines and annotation process. In the next section, we discuss corpus selection and statistics.

\subsection{{\barec} Guidelines} 
We present below a summarized version of the {\barec} annotation guidelines. A detailed account of the  adaptation process from \newcite{Taha:2017:guidelines}'s guidelines is in \newcite{anonymous:2024:barec-guidelines}.

\paragraph{Readability Levels}
The readability level system of \newcite{Taha:2017:guidelines} uses the Abjad order of Arabic letters for 19 levels: {\BlevAlif}, {\BlevBa}, {\BlevJim}, through to {\BlevQaf}. This system emphasizes a finer distinction in the lower levels, where readability is more varied. The {\barec} pyramid (Figure~\ref{fig:barec-pyramid}) illustrates the scaffolding of these levels and their mapping to, guidelines components, school grades, and three collapsed versions of level size 7, 5, and~3. All four level types (19-7-5-3) are fully aligned to easy mapping from fine-grained to coarse-grained levels. We present results for these levels in Section~\ref{sec:eval}. 

\paragraph{Readability Annotation Principles}
The guidelines focus on readability and comprehension, considering the ease of reading and understanding for independent readers. The evaluation does not depend on grammatical analysis or rhetorical depth but rather on understanding basic, literal meanings. Larger texts may contain sentences at different readability levels, but we focus on sentence-level evaluation, ignoring context and author intent. %We define the target reader as one who has studied in non-religious private schools, with no assumption of prior religious knowledge.

\paragraph{Textual Features}
Levels are assessed in six key dimensions.  Each of these specify numerous linguistic phenomena that are needed to qualify for being ranked in a harder level. The Cheat~Sheet used by the annotators in Arabic and its translation in English are included in Appendix~\ref{app:guidelines}. 

\begin{enumerate}
    \item \textbf{Spelling}: Word length and syllable count affect difficulty.
    \item \textbf{Word Count}: The number of unique words determines the highest level for easier levels.
    \item \textbf{Morphology}: We distinguish between simple and complex morphological forms including the use of clitics and infrequent inflectional features, such as the dual.
    \item \textbf{Syntax}: Specific sentence structure and syntactic relation constructions are identified as pivotal for certain levels.
    \item \textbf{Vocabulary}: The complexity of word choices is key, with higher levels introducing more technical and classical literature vocabulary.
    \item \textbf{Content}: The required prior knowledge and abstraction levels are considered for higher levels.
\end{enumerate}
The {\barec} pyramid (Figure~\ref{fig:barec-pyramid}) illustrates which aspects are used (broadly) for which levels. For example, Spelling criteria are only used up to level {\BlevZay}, while syntax is used until level {\BlevSin}, and word count is not used beyond level {\BlevKaf}.


\paragraph{Problems and Difficulties}
Annotators are encouraged to report any issues like spelling errors, colloquial language, or problematic topics. Difficulty is noted when annotations cannot be made due to conflicting guidelines.

A few representative examples for each level are provided in Table~\ref{tab:barec-examples}. A full set of examples with explanations of leveling choices is in Appendix~\ref{fullexample}. 




% ======================================
 
 

%* include annotation interface screenshot
%*pilot; rounds; feedback
%* who they are... 
%ethics



\begin{table*}[t]
\centering
 \includegraphics[width=1.5\columnwidth]{New/Corpus-Stats.pdf}
    \caption{Summary statistics of the {\barec} Corpus in sentences.}
\label{tab:corpus-stats}
\end{table*}

\subsection{Annotation Team and Process}
 
\paragraph{Annotation Team} The {\barec} annotation team comprised six native Arabic speakers, all of whom are experienced Arabic language educators. Among the team members, one individual (A0) brought prior experience in computational linguistic annotation projects, while the remaining five (A1-5) possessed extensive expertise in readability leveling, gained through their involvement in the Taha/Arabi21 project.

\paragraph{Annotation Process} 
The annotation process began with A0, who led sentence-level segmentation and initial text flagging and selection. We followed the Arabic sentence segmentation guidelines by \newcite{habash-etal-2022-camel}.
%
Subsequently, A1-5 were tasked with assigning readability labels to the individually segmented texts.
The annotation  was done through a simple Google Sheet interface. 
%
A1-5 received folders containing  annotation sets, comprising 100 randomly selected sentences each. 
The average annotation speed was around 2.5 hours per batch (1.5 minutes/sentence).
%To assess the quality of annotations and ensure inter-annotator agreement, shared annotation sets were included without informing the annotators.


Before starting the annotation, all annotators received rigorous training, including three pilot rounds.  These rounds provided opportunities for detailed discussions of the guidelines, helping to identify and address any issues. 
16 shared annotation sets (100 sentence each) were included covertly to ensure quality and measure inter-annotator agreement (IAA).
Finally, we conducted a thorough second review of the corpus data, resulting in every sentence being checked twice for the first phase (10,658 sentences) before continuing to finish the 68,182K sentences (1M words). 

In total, the annotators annotated 90K sentences, 24.2\%  of which is not in the final corpus: 3.6\% were deemed problematic (typos and offensive topics); 11.8\% were part of the second round of first phase annotation; and   8.9\% were part of the IAA efforts, not including their unification. 
%
We report on IAA in Section~\ref{sec:iaa-results}.

\hide{
\subsection{{\barec} Dataset}
%This section describes the collection and balancing of {\barec} readability dataset.
%To collect the dataset, we opted for 
%We curated the {\barec} dataset with an eye towards having  a variety of genres, topics, and resources. We ended up with a set $274$ documents.
%For balancing, we categorized our documents into four groups of Arabic readerships:
%
We curated the {\barec} dataset to include diverse genres and topics, resulting in 274 documents, categorized into four intended readership groups:  \textbf{Children}, \textbf{Young Adults}, \textbf{Adult Modern Arabic}, and \textbf{Adult Classical Arabic}.
%
The distribution of data for each group is shown in Table~\ref{tab:dataset}.
%
We aimed to balance the total word count across these groups.
As a result, children's documents have more sentences due to the typically shorter sentence length in that genre.
%Balancing at the word level led to children's documents containing more sentences, given that content designed for children typically have shorter sentences. 
%
On average the length of sentences in the \textbf{Children} group is 7.0 words, whereas it is 13.7 for \textbf{Adult Classical Arabic}. On average we selected 419 words/document, although there is a lot of variation among \textit{documents}, which range from complete books to chapters, sections, or ad hoc groupings. All selected texts are either out of copyright, or are within fair-use representative sample sizes.
%
%The resources we used for the data collection include: {the Emarati Curriculum}, {books from Hindawi Publishers}, {Wikipedia documents}, {manually verified ChatGPT-generated texts}, a {collection of children poems}, {United Nations documents}, {movie subtitles}, {the pre-islamic Suspended Odes}, {Quran}, {Hadith}, {One Thousand and One Nights}, the philosophical novel {Hayy ibn Yaqdhan}, {Old Testament}, {New Testament}, a modern Arabic novel ({Sara}) and the {WikiNews} collection.
We collected data from various sources, including educational curriculum, books, Wikipedia, manually verified ChatGPT texts, children's poems, UN documents, movie subtitles, classical and religious texts, literary works, and news articles.
All details %and citations of these resources
are available in Appendix~\ref{app:full-data}.

%Summary: Khalid to provide (1) data set selections and statistics; 
%* balancing
%* collection
%* high level design

%#* Example of Annoatation? - 19 examples from every level....
%#? appendix? with why this level? (key / floor/ ceiling)
%? English of comparable level...
}





\section{{\barec} Corpus}
\paragraph{Corpus Selection}

In the process of corpus selection, we aimed to cover a wide educational span as well as different domains and topics. We collected the corpus from $1,362$ documents, which we manually categorized into three domains: \textbf{Arts \& Humanities}, \textbf{Social Sciences}, and \textbf{STEM} (details in Appendix~\ref{app:domains}) and three readership groups: \textbf{Foundational}, \textbf{Advanced}, and \textbf{Specialized}  
(details in  Appendix~\ref{app:reader}). Table~\ref{tab:corpus-stats} shows the distribution of the sentences across domains and groups (see Appendix~\ref{app:corpus-stats} for document and word level distributions).  
%
The distribution across readership levels aligns with the corpus's educational focus, with a higher-than-usual proportion at foundational levels. Variations across domains reflect differences in the availability of texts and reader interest (more Arts \& Humanities, less STEM).
%
The corpus uses documents from $32$ different resources. All selected texts are either out of copyright, within the fair-use limit, or obtained in agreement with publishers. The decision of selecting some of these resources is influenced by the fact that other annotations exist for them. Around 26\% of all sentences came from completely new sources that were manually typed to make them digitally usable. 
%Kalima+majed+green
%17,667
%0.259115309
%
All details about the resources are available in Appendix~\ref{app:full-data}.


\begin{figure*}[t]
\centering
 \includegraphics[width=1.4\columnwidth]{New/chart.pdf}
    \caption{The distribution of the readership groups across {\barec} levels.}
\label{tab:readability-stats}
\end{figure*}


\begin{table}[t]
\centering
 \includegraphics[width=\columnwidth]{New/Splits.pdf}
    \caption{{\barec} Corpus splits.}
\label{tab:corpus-splits}
\end{table}

\paragraph{Readability Statistics}
Figure \ref{tab:readability-stats} shows the distribution of the three readership groups across all readability levels. As expected, foundational texts strictly dominate the lower levels up to {\BlevYa}, then the presence of advanced and specialized texts starts increasing gradually till the highest level. Specialized texts dominate the highest levels, while the middle levels ({\BlevKaf} to {\BlevMim}) include a mix of the three groups with a slight advantage for advanced texts.



\paragraph{Corpus Splits}
%We split the corpus into train, dev, and test sets at the document level. For each resource, we manually select which documents are assigned to which set, so that we end up with 80\% of the sentences in the Training set, 10\% in the Dev set, and 10\% in the Test set.
We split the corpus into \textbf{Train ($\simeq$80\%)}, \textbf{Dev ($\simeq$10\%)}, and \textbf{Test ($\simeq$10\%)} at the document level. 
%However, we divide all sentences used in the IAA studies between the Dev and Test sets equally. These sentences could be used as a special evaluation set as they provide multiple references from different annotators for each example. 
Sentences from IAA studies are evenly divided between Dev and Test as a special evaluation set as they provide multiple references from different annotators for each example. 
%
Also, if other annotations exist for a resource (e.g., CamelTB \citep{CamelTB} and ReadMe++ \citep{naous-etal-2024-readme}), we follow the existing splits. Table \ref{tab:corpus-splits} shows the corpus splits in the level of documents, sentences, and words. 
We didn’t end up with perfect 80-10-10 splits due to the IAA and existing corpora exceptions.
More details about the splits across readability levels, domains, and readership groups are available in Appendix \ref{app:splits}. 



\section{Experiments}

\subsection{Metrics}
In this paper, we define the task of Readability Assesment as an ordinal classification task. We use the following metrics for evaluation.

\paragraph{\textbf{Accuracy (Acc)}} 
The percentage of cases where reference and prediction classes match in the 19-level scheme. We addition consider three variants, \textbf{Acc@7}, \textbf{Acc@5}, \textbf{Acc@3}, that respectively collapse the 19-levels into the 7, 5, and 3-level schemes discussed in Section~\ref{sec:annotation}.

%While easy to interpret and compute, \textbf{Acc} 
%%is not well-suited for ordinal classification as it 
%does not account for the ordered structure of labels. 
%Next, we introduce other metrics that account for the severity of misclassifications.

%\paragraph{\textbf{Accuracy at n levels (Acc@n)}}
%We map both the predicted and true labels from 19 levels to 7, 5, or 3 levels. Introduce the mapping here? What to say?\\

\paragraph{\textbf{Adjacent Accuracy (Acc~$\pm$1)}}
Also known as off-by-1 accuracy. It allows some tolerance for predictions that are close to the true labels. It measures the proportion of predictions that are either exactly correct or off by at most one level.

\paragraph{\textbf{Average Distance (Dist)}}
Also known as Mean Absolute Error (MAE), it measures the average absolute difference between predicted and true labels.  
 


\paragraph{\textbf{Quadratic Weighted Kappa (QWK)}}
An extension of Cohen’s Kappa \citep{Cohen:1968:weighted,2023.EDM-long-papers.9} measuring the agreement between predicted and true labels, but applies a quadratic penalty to larger misclassifications, meaning that predictions farther from the true label are penalized more heavily.



We consider Accuracy, Adjacent Accuracy, Average Distance, Quadratic Weighted Kappa as the primary metrics for selecting the best system.

\begin{table}[t]
\centering
 \includegraphics[width=\columnwidth]{New/variant-example.pdf}
    \caption{An example of a sentence and the corresponding input variants.}
\label{tab:variants-example}
\end{table}

\subsection{Input Variants}
%Languages with rich morphology often convey essential linguistic information through affixation, compounding, and inflection, which can significantly impact readability. Human annotators rely on morphological features —such as the complexity of word formation, the presence of derivational and inflectional morphemes, and the degree of lexical simplification— when assessing readability levels. However, standard tokenization approaches may obscure these morphological cues, making it harder for models to learn relevant patterns. By segmenting sentences into meaningful morphological units, we aim to better capture the structural complexity that annotators consider when assigning readability levels.
In morphologically rich languages, affixation, compounding, and inflection convey key linguistic information that influences readability. Human annotators consider morphological complexity when assessing readability, but standard tokenization may obscure these cues. Segmenting sentences into morphological units helps preserve structural patterns relevant to readability prediction.

\hide{
To generate all input variants, we start by running all sentences through the state-of-the-art MSA morphological disambiguator \citep{inoue-etal-2022-morphosyntactic}, then selecting that top1 set of tags for all words. We use the predicted morphological features to craft the following input variants:

\paragraph{\textbf{Word}}
We tokenize the sentences and remove diacritics and kashida using CAMeL Tools \citep{obeid-etal-2020-camel}.

\paragraph{\textbf{Lex}}
We replace each word with its predicted Lemma.

\paragraph{\textbf{D3Tok}}
We replace each word with its detokenized form.

\paragraph{\textbf{D3Lex}}
Same as D3tok but the basic stem of each word is replaced with the Lemma.
\\

Table \ref{tab:variants-example} shows an example of a sentence and the corresponding input variants.
}

We generate four input variants using CamelTools  morphological disambiguation to identify top choice analysis in context \citep{obeid-etal-2020-camel}.\footnote{CamelTools v1.5.5: Bert-Disambig\textbf{+}calima-msa-s31 db.}  For the \textbf{Word} variant, we simply tokenize the sentences and remove diacritics and kashida using CAMeL Tools \citep{obeid-etal-2020-camel}.  For \textbf{Lex}, we replace each word with its predicted Lemma. For \textbf{D3Tok}, we tokenize the word into its base and clitics form; and for \textbf{D3Lex}, we replace the base form in D3Tok with the lemma. All variants are dediacritized. 
%
Table \ref{tab:variants-example} shows an example of a sentence and the corresponding input variants.


\subsection{Fine-Tuning}
We fine-tuned the top three Arabic BERT-based models according to \citet{inoue-etal-2021-interplay} (AraBERTv02 \citep{antoun-etal-2020-arabert}, MARBERTv2 \citep{abdul-mageed-etal-2021-arbert}, CamelBERT-msa \citep{inoue-etal-2021-interplay}). We also added AraBERTv2 to our experiments due to the possible matching between its pre-training data (morphologically segmented sentences by Farasa \citep{darwish-mubarak-2016-farasa}) and the different input variants.

\subsection{Loss Functions}
Since readability levels exhibit a natural ordering, we explore loss functions that account for the distance between predicted and true labels \citep{heilman-etal-2008-analysis}. In addition to standard cross-entropy loss (\textbf{CE}), we experiment with Mean Squared Error (\textbf{Regression}), Ordinal Log Loss (\textbf{OLL}) \citep{castagnos-etal-2022-simple}, Soft Labels Loss (\textbf{SOFT}) \citep{9156542}, and Earth Mover’s Distance-based loss (\textbf{EMD}) \citep{HouNIPSW17}, as these have been previously used for ordinal classification tasks. OLL, SOFT, and EMD incorporate a distance matrix $D$ into their formulations to penalize predictions proportionally to their distance from the true label. For simplicity, we define the distance between any two adjacent levels as one, setting $D(i,j) = |i-j|$ for labels i and j. For regression, we round the final output to the nearest readability level to ensure predictions align with the 19 levels.

\subsection{Hyper-parameters}
For all experiments, we use a learning rate of \(5 \times 10^{-5}\), a batch size of $64$, and train for six epochs on an NVIDIA V100 GPU. After training, we select the best-performing epoch based on evaluation loss. For Ordinal Log Loss (OLL), we experiment with different values of the weighting parameter \(\alpha\), choosing from \{0.5, 1, 1.5, 2\}. Similarly, for Soft Labels Loss (SOFT), we evaluate different values of the smoothing parameter \(\beta\), selecting from \{2, 3, 4, 5\}. The training of the models in this paper took approximately 20 hours.

\subsection{Procedure}
Our experiments involve three main variables: the pretrained model, the input variant, and the loss function. Our goal is to determine the optimal combination of these three factors. Due to the large number of experiments required, we divide the process into two stages.  
In \textbf{Stage 1}, we train all combinations of pretrained models and input variants using cross-entropy loss. We then select the best combination based on a majority vote from our primary evaluation metrics (Acc, Acc~$\pm$1, Dist, and QWK).  
In \textbf{Stage 2}, we take the best combination of pretrained model and input variant from the first stage and train models using all the different loss functions. 

\begin{table*}[t]
\centering
 \includegraphics[]{New/IAA-results.pdf}
    \caption{Inter-Annotator Agreement (IAA) results  comparing initial annotations to unified labels. While exact agreement is 72.4\% on average, disagreements are within a small range since average ACC$\pm1$ is 94.6\%.}
\label{tab:iaa_results}
\end{table*}

\section{Results}
\label{sec:eval}
\subsection{Inter-Annotator Agreement (IAA)}
\label{sec:iaa-results}

In this section, we report on  14   IAA studies, excluding the three pilots and first two IAAs, which overlapped with annotator training.

\paragraph{Pairwise Agreement}
The average pairwise exact-match over 19 {\barec} levels between any two annotators is only 61.4\%, which reflects the task's complexity. Allowing a fuzzy match distance of up to one level raises the match to 74.6\%.
The overall average pairwise level difference is $0.94$ levels.
The average pairwise Quadratic Weighted Kappa 81.3\% 
(substantial agreement) confirms most disagreements are minor \cite{Cohen:1968:weighted,2023.EDM-long-papers.9}.  

\paragraph{Unification Agreement}
After each IAA study, the annotators discussed and determined a unified readability level for each sentence in the study. Table~\ref{tab:iaa_results} presents how each annotator (A1–5) performs against the unified annotations. 
The results show promising agreement between annotators and the unified annotations. While the average  accuracy (Acc) is relatively lower (72.4\%), reflecting the difficulty of the task, the other metrics, average Acc~$\pm$1 (82.5\%), Distance (0.639), and Quadratic Weighted Kappa (88.2\%), indicate that differences between individual annotators and the unified labels are minor. Additionally, Acc@7, Acc@5, and Acc@3 results suggest that these differences often occur near the boundaries of the 3, 5, and 7 readability groups, rather than being large deviations.
 

\begin{table}[t]
\centering
 \includegraphics[width=\columnwidth]{New/model-selection-results.pdf}
    \caption{Results comparing different combinations of models and input variants on {\barec} Dev set. \textbf{Bold} are the best results on each matric.}
\label{tab:model-selection}
\end{table} 


\begin{table*}[t]
\centering
 \includegraphics[width=0.7\textwidth]{New/Dev-Test-results.pdf}
    \caption{Results comparing different loss function, ensemble methods, and oracle performance on {\barec} Dev and Test sets. \textbf{Bold} are the best results across individual models and across ensembles.}
\label{tab:dev-test-results}
\end{table*}

\subsection{Stage 1 Results}  

Table~\ref{tab:model-selection} presents the results of stage 1, where we evaluate different combinations of pretrained models and input variants using cross-entropy loss. Based on the primary metrics (Acc, Acc~$\pm$1, Dist, and QWK), we observe that the AraBERTv02 and AraBERTv2 models generally achieve higher performance across multiple input variants.  

Among input variants, the Word and D3Tok representations tend to yield better results compared to Lex and D3Lex. Specifically, AraBERTv02 with the Word input achieves the highest Acc (53.6\%) and Dist (1.18).
Notably, AraBERTv2 is the only model that benefits from the D3Tok input compared to the Word input, showing an improvement across all four primary metrics. We argue that this occurs because AraBERTv2 is the only model in this set that was pretrained on segmented data, making it more compatible with morphologically segmented input.
These results suggest that both the choice of input variant and the pretrained model significantly impact performance.  

Based on a majority vote across the four primary metrics, we select AraBERTv02 with the Word input as the best-performing combinations for stage 2, where we evaluate different loss functions.


\hide{

\begin{table}[t]
\centering
 \includegraphics[width=\columnwidth]{New/Dev-results.pdf}
    \caption{Dev Results}
\label{tab:dev-results}
\end{table}





\begin{table}[t]
\centering
 \includegraphics[width=\columnwidth]{New/Test-results.pdf}
    \caption{Test Results}
\label{tab:test-results}
\end{table}
}

\subsection{Stage 2 and Ensemble Results} 
Table~\ref{tab:dev-test-results} presents the results of stage 2, where we evaluate different loss functions using the best-performing pretrained model and input variant combinations from stage 1. To reduce the table size, we only report results for the best loss functions, while the full results are available in Appendix \ref{app:loss}.

\paragraph{Stage 2}
Among individual models, the best-performing model is AraBERTv02 with Word input using the OLL15 loss function. However, when considering accuracy-based metrics (Acc, Acc@7, Acc@5, Acc@3), the cross-entropy (CE) model performs better, suggesting that CE is more effective in producing exact matches, whereas OLL15 is better at capturing ordinal relationships.

\paragraph{Ensemble}
To further improve performance, we experiment with ensemble methods. We define the \textbf{Average ensemble}, where the final prediction is the rounded average of the levels predicted by the six models, and the \textbf{Most Common ensemble}, where the final prediction is the mode of the predicted levels. The results show that the Average ensemble performs better in terms of Acc~$\pm$1, Dist, and QWK, indicating that it tends to stay closer to the correct label. However, it struggles with exact accuracy (Acc), as averaging can blur distinctions between classes. On the other hand, the Most Common ensemble achieves higher Acc but can sometimes be misled by an incorrect majority, leading to greater deviation from the correct label.

\paragraph{Oracle}
We also report an \textbf{Oracle combination}, where we assume access to the best possible prediction from the six models for each sample. This serves as an upper bound on model performance. The Oracle results are significantly higher than those of individual models and are comparable to human annotators’ agreement with the unified labels (see section \ref{sec:iaa-results}). This suggests that while individual models are still far from human-level performance, ensembling has the potential to push results closer to human agreement. More oracle combinations are provided in Appendix \ref{app:ensemble}.
We also include more results on the impact of training granularity on readability level prediction in Appendix \ref{app:gran}

Finally, we note that the trends observed in the development set persist in the test set, further validating our findings. 

\hide{

\section{Results}
\label{sec:eval}
%%Summary:   (2) Annotation statistics; (3) IAA results,

\begin{figure*}[ht!]
% Figure 2: The distribution of annotated sentences among BAREC levels and Arabic readers groups
%Cells Q96:U107 in https://docs.google.com/spreadsheets/d/1mAhPQGG_snQRetsXvWLTiz5iNhF6R_zfrdFfRgGB1F4/edit?gid=257362477#gid=257362477
\centering
 \includegraphics[width=0.85\textwidth]{annotation-distribution-chart.pdf}
    \caption{The distribution of annotated sentences among {\barec} levels and Arabic readers groups}
\label{fig:annotation-distribution}
\end{figure*}

%- Figure 3: Charts comparing the average sentence length (left) and the distribution of lengths (right) per level
%https://docs.google.com/spreadsheets/d/1TB8x2Hh1SNMbJZy-TiF-i7GfXnxgAyzByjfU9ZLg0Ec/edit?gid=1382815757#gid=1382815757
\begin{figure}[ht!]
\centering
 \includegraphics[width=\columnwidth]{level-average.pdf}\\
  \includegraphics[width=\columnwidth]{level-distrib.pdf}
    \caption{Charts comparing the average sentence length (left) and the distribution of lengths (right) per level}
\label{fig:annotation-level-avg}
\end{figure}




\subsection{Inter-Annotator Agreement}
%We conducted four inter-annotator agreement (IAA) studies during training and annotation.
%Three 100-sentence pilot studies were carried out during \textit{training} to enhance agreement.
%The last official IAA study used a common set of $200$ sentences.
%
We conducted four inter-annotator agreement (IAA) studies: three 100-sentence pilots during \textit{training} to enhance agreement, and a final official study using $200$ sentences, which we report on next.
%
The average pairwise exact-match over 19 {\barec} levels between any two annotators is only 49.2\%, which reflects the task's complexity. Allowing a fuzzy match distance of up to 1, 2, 3, or 4 levels raises the match to 64.6\%, 77.1\%, 87.2\%, and 93.2\%, respectively. 
%
The overall average pairwise level difference is $1.38$ levels. 
The average pairwise Quadratic Weighted Kappa 79.9\% 
(substantial agreement) confirms most disagreements are minor \cite{Cohen:1968:weighted,2023.EDM-long-papers.9}.
%The average pairwise agreement's Cohen Kappa is 0.44 (moderate agreement) \cite{Cohen:1968:weighted,2023.EDM-long-papers.9}, which reflects the task's complexity. 
%

%Average Pairwise Kappa None: 43.82%
%Average Pairwise Weighted Kappa Linear: 66.84%
%Average Pairwise Weighted Kappa Quadratic: 79.87%
%
%Average of annotators vs Combined Result RX
%RX Average Pairwise Kappa None: 55.72%
%RX Average Pairwise Weighted Kappa Linear: 76.46%
%RX Average Pairwise Weighted Kappa Quadratic: 87.17%

\paragraph{Second Round QC}
After the above-mentioned IAA, we made some minor guideline clarifications and did some continued training.
Then we conducted a \textbf{second round of full annotation quality check} where every example was checked by a different annotator from the first round. In total  40\% of the labels changed with an average level distance of 0.97; the average pairwise Quadratic Weighted Kappa between the two rounds is 85.5\%.

\input{tables/dataset}


%https://datatab.net/tutorial/cohens-kappa
 
%\#\#\#\# add - unification; distance to unification

%\#\#\#\#  full revision; distances with full revision.


\subsection{Analysis of Annotation Distributions}
%Also -- what percent is problems? did we exclude?

%Problem	114	1.05%
%Difficulty	13	0.12%
%	127	1.17%
\paragraph{Flagged Segments} The actual number of annotated segments is 10,896; but 2.3\% were excluded for flagged problems, and 0.13\% excluded for flagged difficulties. 

\paragraph{Readership Groups and Readability Levels} 
 Figure~\ref{fig:annotation-distribution} visualizes the annotation distributions across the four readership groups identified based on educated guesses and self-declared target readers. Full details are in Appendix~\ref{app:annotation-stats}. Children's texts dominate the easier levels ({\BlevAlif} to {\BlevHa}), while Classical texts dominate the harder levels ({\BlevSad} and {\BlevQaf}), as expected. The middle levels contain a mix of all groups. Interestingly, some Children texts include advanced materials, which may need revision, or can be arguably justified for educational purposes.

\paragraph{Readability Level Patterns} 
%Figure~\ref{fig:annotation-distribution} has an imperfect bell-shaped distribution, due to the out-of-place counts for {\BlevTa} (low) and {\BlevNun} (high).  This pattern may simply be the result of the limited sample size or a hidden bias in the text selections.  From the guidelines point of view we note that {\BlevTa} has a combination of specific features keys that are not common, e.g., dual command form, the vocative, emotional vocabulary, and the Hamza interrogative particle.   
In terms of total counts, Figure~\ref{fig:annotation-distribution} exhibits a slightly skewed distribution, notably with lower counts for {\BlevTa} and higher counts for {\BlevNun}. This pattern could stem from the limited sample size or potential biases in text selections. Notably, the guidelines for {\BlevTa} feature specific uncommon linguistic elements like the dual command form, vocative, emotional vocabulary, and the Hamza interrogative particle.

\paragraph{Readability Level and Text Length} 
%Figure~\ref{fig:annotation-level-avg} presents two charts that contrast  the readability levels with lengths of the segments that are assigned to. In terms of overall averages, we not the generally expected  linear pattern from \BlevAlif to \BlevYa/\BlevKaf. The increasing pattern  continues to \BlevAyn before it drops off. Most of the texts in the highest levels are poetry verses which are generally shorter than common prose. The length distribution chart (Figure~\ref{fig:annotation-level-avg}(right) highlights the variability in lengths within each readability level, confirming that the annotators did not rely on segment lengths as a strict feature for readability level annotation. 

Figure~\ref{fig:annotation-level-avg} presents two charts comparing readability levels with segment lengths. The overall averages show a generally expected linear pattern from {\BlevAlif} to \BlevYa/\BlevKaf, continuing to {\BlevAyn} before dropping off, as higher-level texts, often poetry, are shorter than prose. The length distribution chart, in Figure~\ref{fig:annotation-level-avg}(right), highlights variability within each readability level, confirming that annotators did not strictly use segment lengths for readability level annotation.

%Link: https://docs.google.com/spreadsheets/d/1mAhPQGG_snQRetsXvWLTiz5iNhF6R_zfrdFfRgGB1F4/edit?gid=1528265379#gid=1528265379
%\begin{table}[ht!]
%\centering
% \includegraphics[width=0.9\textwidth]{ai-exp.pdf}
  %\includegraphics[width=0.9\textwidth]{AI-exp-reduced.pdf}
 % \includegraphics[width=0.9\columnwidth]{ai-exp-small.pdf}
  %  \caption{Results of automatic readability assessment. \textbf{Bold} represents the best performance for each metric. CL Rank is the average rank of the correct label, CL Distance is the average distance from the correct label, and QWK is the Quadratic Weighted Kappa.}
%\label{tab:ai-exp}
%\end{table}
\input{tables/ai-exp.tex}

\subsection{Automatic Readability Assessment}
We train sentence-level classifiers by finetuning CAMeLBERT-MIX \cite{inoue-etal-2021-interplay}, MARBERT \cite{abdul-mageed-etal-2021-arbert} and AraBERTv2 \cite{antoun-etal-2020-arabert} to benchmark the baseline performance given the dataset.
We split the dataset into $90\%$ for training and $10\%$ for testing.
We finetune the models using the Transformers library \cite{Wolf:2019:huggingfaces} on a NVIDIA T4 GPU for three epochs with a learning rate of 5e-5, and a batch size of 16.
Table~\ref{tab:ai-exp} shows the results of finetuning the three models for readability prediction as a text classification task.
%
% We report the following metrics for evaluation:
%\begin{itemize}
%    \item accuracy: measures the exact matching between the correct label and the model's prediction.
%    \item accuracy with margin of one level: allows the matching if the difference between the correct label and the prediction is one readability level or less.
%    \item accuracy @$n$: allows the matching if the correct label is within the top $n$ predictions.
%    \item average rank of the correct label: the rank equals $n$ if the correct label is the $n$th in the order of model's predictions.
%    \item average distance from correct label: average distance - in terms of readability levels - between the correct label and the prediction.
%    \item Quadratic Weighted Kappa: motivated by \citep{2023.EDM-long-papers.9}, we use the Quadratic Weighted Kappa between the labels and the predictions to provide scoring for each model. 
%\end{itemize}
%
We report with the following metrics:
 \textbf{Accuracy@n} (correct label is within the top~$n$ predictions),
 \textbf{Average Rank of the Correct Label},
 \textbf{Average Distance from Correct Label}, and \textbf{Quadratic Weighted Kappa}.
 %
The performance of the compared systems is generally similar. Their results are comparable with the IAA numbers, showing a robust Quadratic Weighted Kappa score of 84\%. We anticipate that performance will improve further with additional data.

}


\section{Conclusions and Future Work}
%We introduced the Balanced Arabic Readability Corpus ({\barec}) project, its guidelines, and corpus. 
%We introduced the  {\barec} project, its guidelines, and corpus. 
%which addresses the critical need for comprehensive Arabic language resources catering to diverse readability levels.
%We defined the set of detailed guidelines and activated their use through training a team of annotators and labeling over 10,000 sentences. 
%
%The guidelines and corpus will be made publicly available.

%In future work, we plan to expand the {\barec} project scope to increase the size and diversity of its corpus. We also plan to continue to improve the guidelines to identify sources of disagreements.
%encompassing a broader range of genres, and topics.
%Additionally, 
%We also plan to use our annotation to train and evaluate models for Readability Assessment.
%We introduced the {\barec} project addressing the need for comprehensive Arabic language resources across various readability levels. We developed detailed guidelines, trained annotators, and labeled  68,000+ sentences (1M+ words). The guidelines and corpus will be publicly available.
%We also demonstrated the application of the corpus in automatic leveling, achieving promising results. Future work will expand the corpus's size and diversity, refine the guidelines to address sources of disagreement, and enhance automatic readability models.

%In this paper, we presented the Balanced Arabic Readability Evaluation Corpus (\barec), a large-scale, finely annotated dataset designed to assess Arabic text readability across 19 distinct levels. To our knowledge, it is the largest Arabic corpus for readability assessment, containing over 68,000 sentences and 1 million words. The corpus includes a wide range of genres, topics, and target audiences, making it a comprehensive tool for evaluating the complexity of Arabic texts. The high inter-annotator agreement underscores the reliability and consistency of the annotations. Through benchmarking various readability assessment techniques, we highlighted both the challenges and opportunities in Arabic readability modeling, demonstrating promising performance across different methods.

This paper presented the \textbf{Balanced Arabic Readability Evaluation Corpus (BAREC)}, a large-scale, finely annotated dataset for assessing Arabic text readability across 19 levels. With over 68,000 sentences and 1 million words, it is the largest Arabic corpus for readability assessment, covering diverse genres, topics, and audiences, to our knowledge. High inter-annotator agreement ensures reliable annotations. Through benchmarking various readability assessment techniques, we highlighted both the challenges and opportunities in Arabic readability modeling, demonstrating promising performance across different methods.


Looking ahead, we plan to expand the corpus, enhancing its size and diversity to cover additional genres and topics. We also aim to add annotations related to vocabulary leveling and syntactic treebanks to study less-explored genres in syntax. Future work will include analyzing readability differences across genres and topics. Additionally, the tools we have developed will be integrated into a system to help children's story writers target specific reading levels.

The {\barec} dataset, its annotation guidelines, and benchmark results, will be made publicly available to support future research and educational applications in Arabic readability assessment.

\newpage

%\section*{Acknowledgments}
%The {\barec} project is supported by the Abu Dhabi Arabic Language Centre / Department of Culture and Tourism, UAE.

\section*{Limitations}
One notable limitation is the inherent subjectivity associated with readability assessment, which may introduce variability in annotation decisions despite our best efforts to maintain consistency. Additionally, the current version of the corpus may not fully capture the diverse linguistic landscape of the Arab world. Finally, while our methodology strives for inclusivity, there may be biases or gaps in the corpus due to factors such as selection bias in the source materials or limitations in the annotation process. We acknowledge that readability measures   can be used with malicious intent to profile people; this is not our intention, and we discourage it.

 
 
\section*{Ethics Statement}

All data used in the corpus curation process are sourced responsibly and legally.
The annotation process is conducted with transparency and fairness, with multiple annotators involved to mitigate biases and ensure reliability. All annotators are paid fair wages for their contribution.  The corpus and associated guidelines are made openly accessible to promote transparency, reproducibility, and collaboration in Arabic language research.
 

 %ALC annotators other team members

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
%\bibliographystyle{acl_natbib}
\bibliography{custom,camel-bib-v3,anthology}

\onecolumn
\appendix
\section{{\barec} Annotation Guidelines Cheat Sheet and Examples}
\label{app:guidelines}
\subsection{Arabic Original}
\begin{center}
  %  \includegraphics[scale=1.1]{CheatSheet-BAREC.pdf}
  % LINK Arabic: https://docs.google.com/spreadsheets/d/1mAhPQGG_snQRetsXvWLTiz5iNhF6R_zfrdFfRgGB1F4/edit?gid=1720043463#gid=1720043463
  \includegraphics[width=\textwidth]{BAREC-Guidelines-AR.pdf}
  \end{center} 
%\includegraphics[trim={3cm 3cm 3cm 3cm},scale=0.9]{CheatSheet-BAREC.pdf}
\newpage 
\subsection{English Translation}
\label{fullexample}
%\vspace{1cm}
\begin{center}
  % LINK English: https://docs.google.com/spreadsheets/d/1mAhPQGG_snQRetsXvWLTiz5iNhF6R_zfrdFfRgGB1F4/edit?gid=1850432971#gid=1850432971
  \includegraphics[width=\textwidth]{BAREC-Guidelines-EN.pdf}
\end{center} 

\newpage 
\subsection{Annotation Examples}
Representative examples of the 19 {\barec} readability levels, with English translations, and readability level reasoning. Underlining is used to highlight the main keys that determined the level. 

%\begin{table*}[h]
%\centering
  \includegraphics[width=\textwidth]{Examples.pdf}
 %   \caption{ }
%\label{tab:barec-examples}
%\end{table*}
\newpage 

\section{{\barec} Corpus Splits}
\label{app:splits}
\subsection{Sentence-level splits across readability levels}
\begin{center}
  %  \includegraphics[scale=1.1]{CheatSheet-BAREC.pdf}
  % LINK Arabic: https://docs.google.com/spreadsheets/d/1mAhPQGG_snQRetsXvWLTiz5iNhF6R_zfrdFfRgGB1F4/edit?gid=1720043463#gid=1720043463
  \includegraphics[]{New/sent-splits.pdf}
  \end{center} 
%\includegraphics[trim={3cm 3cm 3cm 3cm},scale=0.9]{CheatSheet-BAREC.pdf}
\newpage 
\subsection{Sentence-level splits across domains and readership groups}
%\vspace{1cm}
\begin{center}
  % LINK English: https://docs.google.com/spreadsheets/d/1mAhPQGG_snQRetsXvWLTiz5iNhF6R_zfrdFfRgGB1F4/edit?gid=1850432971#gid=1850432971
  \includegraphics[]{New/3x3-splits.pdf}
\end{center} 


\twocolumn
\section{{\barec} Corpus Details}
\label{app:full-data}

\subsection{Resources}
\paragraph{Emarati Curriculum} The first five units of the UAE curriculum textbooks for the 12 grades in three subjects: Arabic language, social studies, Islamic studies \cite{Khalil:2018:leveled}.

\paragraph{Hindawi} A subset of 277 books from Hindawi accross different different genres. \footnote{\url{https://www.hindawi.org/books/categories/}}

\paragraph{Wikipedia} A subset of 175 Arabic wikipedia articles covering Culture, Figures, Geography, History, Mathematics, Sciences, Society, Philosophy, Religions and Technologies.\footnote{\url{https://ar.wikipedia.org/}}

\paragraph{ChatGPT} To add more children's materials, we ask Chatgpt to generate 200 sentences ranging from 2 to 4 words per sentence, 150 sentences ranging from 5 to 7 words per sentence and 100 sentences ranging from 8 to 10 words per sentence.\footnote{\url{https://chatgpt.com/}} Not all sentences generated by ChatGPT were correct. We discarded some sentences that were flagged by the annotators. Appendix \ref{app:chatgpt} shows the prompts and the percentage of discarded sentences for each prompt.

\paragraph{Collection of Children poems} Example of the included poems: My language sings (\<لغتي تغني>), Poetry and news (\<أشعار وأخبار>), and The cat and the Eid's hat (\<القطة وقبعة العيد>) \cite{kashkol, poetry-and-news}.

\paragraph{UN} The Arabic translation of the Universal Declaration of Human Rights.\footnote{\url{https://www.un.org/ar/about-us/universal-declaration-of-human-rights}}

\paragraph{Subtitles} A subset of the Arabic side of the OpenSubtitles %parallel 
dataset \cite{Lison:2016:opensubtitles2016}.

\paragraph{The Suspended Odes (Odes)}  The first and the last ten verses of the ten most celebrated poems from Pre-Islamic Arabia (\<المعلقات> Mu’allaqat).
All texts were extracted from Wikipedia.\footnote{\url{https://ar.wikipedia.org/wiki/}\<المعلقات>}

\paragraph{Quran} The first Surah, the last 14 Surahs, the first 106 verses from the second Surah and the first 108 verses from the third Surah from the Holy Quran. We selected the text from the Quran Corpus Project \cite{Dukes:2013:supervised}.\footnote{\url{https://corpus.quran.com/}}

\paragraph{Hadith} The first 75 Hadiths from Sahih Bukhari \cite{bukhari}.  We selected the text from the LK Hadith Corpus\footnote{\url{https://github.com/ShathaTm/LK-Hadith-Corpus}} \cite{Altammami:2019:Arabic}.

\paragraph{Arabian Nights} The openings and endings of the opening narrative and the first eight nights from the Arabian Nights \cite{ArabianNights}. We extracted the text from an online forum.\footnote{\url{http://al-nada.eb2a.com/1000lela\&lela/}}

\paragraph{Hayy ibn Yaqdhan}  A subset of the philosophical novel and allegorical tale written by Ibn Tufail \cite{tufail:hayy}.
We extracted the text from the Hindawi Foundation website.\footnote{\url{https://www.hindawi.org/books/90463596/}}
 
\paragraph{Old Testament} The first 225 words from each of the first 20 chapters of the Book of  Genesis \cite{arabicOldTestament}.\footnote{\url{https://www.arabicbible.com/}\label{biblefoot}}

\paragraph{New Testament} The first 280 words from each of the first 16 chapters of the Book of Matthew \cite{arabicNewTestament}.$^{\ref{biblefoot}}$ %\footnote{\url{https://www.arabicbible.com/}}

\paragraph{Sara} The first 1000 words of {\it Sara}, a novel by Al-Akkad first published in 1938 \cite{akkad:sarah}. We extracted the text from the Hindawi Foundation website.\footnote{\url{https://www.hindawi.org/books/72707304/}}

\paragraph{WikiNews} 70 Arabic WikiNews articles covering politics, economics,
health, science and technology, sports, arts, and culture \cite{Abdelali:2016:farasa}.

\paragraph{Arabic Learner Corpus (ALC)}
20 L2 articles from the Arabic Learner Corpus \citep{phdthesis}

\paragraph{ArabicMMLU} 6093 question and answer pairs from the ArabicMMLU benchmark dataset \cite{koto-etal-2024-arabicmmlu}.

\paragraph{Basic Travel Expressions Corpus (BTEC)}
20 documents from the MSA translation of the Basic Traveling Expression Corpus \cite{eck-hori-2005-overview,takezawa-etal-2007-multilingual}.

\paragraph{Constitutions}
The first 2000 words of the Arabic constitutions from 17 Arabic speaking countries, collected from MCWC dataset \cite{el-haj-ezzini-2024-multilingual}.

\paragraph{Green Library}
61 manually typed books from the Green Library.\footnote{\url{https://archive.org/details/201409_201409}}

\paragraph{Kalima}
The first 500 words of 145 books from Kalima project.\footnote{\url{https://alc.ae/publications/kalima/}}

\paragraph{Majed}
10 manually typed editions of Majed magazine for children from 1983 to 2019.\footnote{\url{https://archive.org/details/majid_magazine}}

\paragraph{Qatar Arabic Language Bank (QALB)}
200 online comments from the Qatar Arabic Language Bank (QALB) Corpus \cite{mohit-etal-2014-first, zaghouani-etal-2014-large}.

\paragraph{ReadMe++}
The Arabic split of the ReadMe++ dataset \cite{naous-etal-2024-readme}.

\paragraph{Spacetoon Songs}
The opening songs of 56 animated children series from Spacetoon channel.

\paragraph{Zayed Arabic-English Bilingual Undergraduate Corpus (ZAEBUC)}
100 student-written articles from the Zayed University Arabic-English Bilingual Undergraduate Corpus \cite{habash-palfreyman-2022-zaebuc}.

Some datasets are chosen because they already have annotations available for other tasks.
For example, dependency treebank annotations exist for \textbf{Odes}, \textbf{Quran}, \textbf{Hadith}, \textbf{1001}, \textbf{Hayy}, \textbf{OT}, \textbf{NT}, \textbf{Sara},\textbf{WikiNews}, \textbf{ALC}, \textbf{BTEC}, \textbf{QALB}, and \textbf{ZAEBUC} \cite{habash-etal-2022-camel}.

\subsection{Domains}
\label{app:domains}
\paragraph{Arts \& Humanities}
The Arts and Humanities domain comprised the following subdomains.
\begin{itemize}
\item  \textit{Literature and Fiction:} Encompasses novels, short stories, poetry, and other creative writing forms that emphasize narrative and artistic expression.	
\item  \textit{Religion and Philosophy: }Contains religious texts, philosophical works, and related writings that explore spiritual beliefs, ethics, and metaphysical ideas.	
\item  \textit{Education and Academic Texts (on Arts and Humanities):} Includes textbooks, scholarly articles, and educational materials that are often structured for learning and academic purposes.	
\item  \textit{General Knowledge and Encyclopedic Content (on Arts and Humanities): } Covers reference materials such as encyclopedias, almanacs, and general knowledge articles that provide broad information on various topics.	
\item  \textit{News and Current Affairs (on Arts and Humanities):} Includes newspapers, magazines, and online news sources that report on current events and issues affecting society.	
\end{itemize} 



\paragraph{Social Sciences}
The Social Sciences domain comprised the following subdomains.
\begin{itemize}
    \item  \textit{Business and Law:} Encompasses legal texts, business strategies, financial reports, and corporate documentation relevant to professional and legal contexts.	
    \item  \textit{Social Sciences and Humanities:} Covers disciplines like sociology, anthropology, history, and cultural studies, which explore human society and culture.	
    \item  \textit{Education and Academic Texts (on Social Sciences):} Includes textbooks, scholarly articles, and educational materials that are often structured for learning and academic purposes.	
    \item  \textit{General Knowledge and Encyclopedic Content (on Social Sciences):} Covers reference materials such as encyclopedias, almanacs, and general knowledge articles that provide broad information on various topics.	
    \item  \textit{News and Current Affairs (on Social Sciences):} Includes newspapers, magazines, and online news sources that report on current events and issues affecting society.	
\end{itemize} 



\paragraph{STEM}
 	
 The Science, Technology, Engineering and Mathematics domain comprised the following subdomains.

\begin{itemize}
    \item \textit{Science and Technology:} Includes scientific research papers, technology articles, and technical manuals that focus on advancements and knowledge in science and tech fields.	
    \item \textit{Education and Academic Texts (on STEM):} Includes textbooks, scholarly articles, and educational materials that are often structured for learning and academic purposes.	
    \item \textit{General Knowledge and Encyclopedic Content (on STEM):} Covers reference materials such as encyclopedias, almanacs, and general knowledge articles that provide broad information on various topics.	
    \item \textit{News and Current Affairs (on STEM):} Includes newspapers, magazines, and online news sources that report on current events and issues affecting society.	
\end{itemize} 



\subsection{Readership Groups}
\label{app:reader}
\paragraph{Foundational}
This level includes learners, typically up to 4th grade or age 10, who are building basic literacy skills, such as decoding words and understanding simple sentences.	

\paragraph{Advanced}
Refers to individuals with average adult reading abilities, capable of understanding a variety of texts with moderate complexity, handling everyday reading tasks with ease.	

\paragraph{Specialized}
Represents readers with advanced skills, typically starting in 9th grade or above in specialized topics, who can comprehend and engage with complex, domain-specific texts in specialized fields.	
  


\onecolumn


% Appendix B
%Cells J11:O63 in https://docs.google.com/spreadsheets/d/1mAhPQGG_snQRetsXvWLTiz5iNhF6R_zfrdFfRgGB1F4/edit?gid=257362477#gid=257362477
\begin{table*}[ht!]
\begin{center}
    \includegraphics[scale=0.96]{New/resources.pdf}
    \caption{{\barec} Corpus Details: the texts used to build the dataset, and the number of documents, sentences, and words extracted from each text.}
\end{center}
  
\label{tab:dataset-details}
\end{table*}


\newpage

\section{ChatGPT Prompts}
\label{app:chatgpt}
%\includegraphics[trim={3cm 3cm 3cm 3cm},scale=0.9]{CheatSheet-BAREC.pdf}
\begin{table*}[h!]
\centering
  \includegraphics[width=1\columnwidth]{prompts.pdf}
    \caption{ChatGPT Prompts. \% Discarded is the percentage of discarded sentences due to grammatical errors.}
\label{tab:chatgpt}
\end{table*}
%\includepdf[pages=1,scale=0.9]{CheatSheet-BAREC.pdf}


\twocolumn
\section{Additional Results}
\subsection{All Loss Functions}
\label{app:loss}

\begin{table}[h!]
  \includegraphics[width=1\columnwidth]{New/loss-func-results.pdf}
    \caption{Loss functions comparisons on {\barec} Dev set. For SVM and Decision Tree classifiers, we used count vectorizer.}
\label{tab:loss-functions}
\end{table}

\begin{table*}[]
\begin{center}
  \includegraphics[]{New/gran-results.pdf}
    \caption{Comparison between training on 19 levels then mapping to the target granularity vs. training directly on the target granularity.}
    \label{tab:gran}
    \end{center}

\end{table*}

\subsection{Impact of Training Granularity on Readability Level Prediction}
\label{app:gran}

To analyze the effect of training granularity on readability level prediction, we compare two approaches: (1) training on all 19 levels and then mapping predictions to lower levels (7, 5, or 3), and (2) training directly on the target granularity (7, 5, or 3 levels).
Table \ref{tab:gran} presents the results of this comparison.
Overall, the results show that training on 19 levels and mapping slightly outperforms the direct training on the target granularity.

\newpage
\onecolumn
\subsection{Ensembles \& Oracles}
\label{app:ensemble}

\begin{table*}[h!]
\begin{center}
    

  \includegraphics[]{New/oracle-ensemble.pdf}
    \caption{Comparison between individual models, ensembles and oracles on {\barec} Dev set.}
    
\end{center}
\label{tab:oracle-ensemble}
\end{table*}

\newpage
\section{Corpus Statistics}
\label{app:corpus-stats}
\subsection{Summary statistics of the {\barec} Corpus in documents}

\begin{center}
  \includegraphics[]{New/corpus-stats-docs.pdf}
\end{center} 

\vspace{1cm}

\subsection{Summary statistics of the {\barec} Corpus in words}
\begin{center}
  \includegraphics[]{New/corpus-stats-words.pdf}
\end{center} 


\hide{
\section{Detailed Annotation Stats}
\label{app:annotation-stats}
%- Appendix D
%Cells Y93:AE113 in %https://docs.google.com/spreadsheets/d/1mAhPQGG_snQRetsXvWLTiz5iNhF6R_zfrdFfRgGB1F4/edit?gid=257362477#gid=257362477
\centering
\begin{table*}[h!]
\centering
  \includegraphics[scale=0.7]{annotation_stats.pdf}
    \caption{Detailed Annotation Statistics across Readability Levels and Reading Groups.}
\label{tab:annotation-stats}
\end{table*}
}


\end{document}
