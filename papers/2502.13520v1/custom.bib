% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").


@article{soliman2024creating, 
title={Creating a CEFR Arabic Vocabulary Profile: A frequency-based multi-dialectal approach},
author={Soliman, Rasha and Familiar, Laila}, 
journal={Critical Multilingualism Studies}, 
volume={11},
number={1}, 
pages={266--286}, 
year={2024}}

@article{Cohen:1960,
  title={A coefficient of agreement for nominal scales},
  author={Cohen, Jacob},
  journal={Educational and psychological measurement},
  volume={20},
  number={1},
  pages={37--46},
  year={1960},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}


@misc{jiang2024medreadme,
      title={MedReadMe: A Systematic Study for Fine-grained Sentence Readability in Medical Domain}, 
      author={Chao Jiang and Wei Xu},
      year={2024},
      eprint={2405.02144},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@book{fountas2006leveled,
  title={Leveled books (k-8): Matching texts to readers for effective teaching},
  author={Fountas, Irene C and Pinnell, Gay Su},
  year={2006},
  publisher={Heinemann Educational Books}}

@inproceedings{Habash:2022:zaebuc,
    title = {{ZAEBUC}: An Annotated {Arabic-English} Bilingual Writer Corpus},
    author = {Nizar Habash and David Palfreyman},
    booktitle = {Proceedings of the Language Resources and Evaluation Conference (LREC)},
    year = {2022},
    address = {Marseille, France}
}


@book{CEFR,
  title={Common European Framework of Reference for Languages: learning, teaching, assessment},
  author={Council~of~Europe, Council of Europe},
  year={2001},
  publisher={Cambridge University Press}
}

@article{al2004assessment,
  title={The assessment of readability books content (boys-girls) of the first grade of intermediate school according to readability standards},
  author={Al-Dawsari, M},
  journal={Sultan Qaboos University, Muscat},
  year={2004}
}



@article{allington2015research,
  title={What research says about text complexity and learning to read},
  author={Allington, Richard L and McCuiston, Kimberly and Billen, Monica},
  journal={The Reading Teacher},
  volume={68},
  number={7},
  pages={491--501},
  year={2015},
  publisher={Wiley Online Library}
}

@book{klare1963measurement,
  title={The Measurement of Readability},
  author={Klare, G.R.},
  isbn={9780608129785},
  lccn={62009125},
  url={https://books.google.ae/books?id=a8ESAQAAMAAJ},
  year={1963},
  publisher={Iowa State University Press}
}


@book{Taha:2017:guidelines,
  title={Arabic Language Text Leveling~({\footnotesize \<معايير هنادا طه لتصنيف مستويات النصوص>}
{\footnotesize \< العربية>})},
  author={Taha-Thomure, Hanada},
  isbn={9786038147313},
  url={https://books.google.ae/books?id=tIeqDwAAQBAJ},
  year={2017},
  publisher={Educational Book House ({\footnotesize \<دار الكتاب التربوي للنشر والتوزيع>)}}
}


@book{kashkol,
    author = {Bayan Al-Safadi},
    title = {Al-Kashkoul: selection of poetry and prose for children 
({\footnotesize \<الكشكول: مختارات من الشعر والنثر للأطفال>})},
    publisher = {Al-Sa'ih Library ({\footnotesize \<مكتبة السائح>})},
    year = {2005}
}


@book{poetry-and-news,
    author = {Taha-Thomure, Hanada},
    title = { Poems and News ({\footnotesize \<أشعار وأخبار>})},
    publisher={Educational Book House ({\footnotesize \<دار الكتاب التربوي للنشر والتوزيع>)}},
    year = {2007}
}

@book{sulaiman-eisa,
    author = {Sulaiman Al-Issa},
    title = {\small \<ديوان الأطفال>},
    publisher = {\small \<دار الفكر المعاصر>},
    year = {2005}
}

@article{Nassiri:2023,
author = {Nassiri, Naoual and Cavalli-Sforza, Violetta and Lakhouaja, Abdelhak},
title = {Approaches, Methods, and Resources for Assessing the Readability of Arabic Texts},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3571510},
doi = {10.1145/3571510},
abstract = {Text readability assessment is a well-known problem that has acquired even more importance in today’s information-rich world. In this article, we survey various approaches to measuring and assessing the readability of texts. Our specific goal is to provide a perspective on the state-of-the-art in readability assessment research for Arabic, which differs significantly from other languages on which readability studies have tended to focus. We provide background on readability assessment research and tools for English, for which readability studies are the most advanced. We then survey approaches adopted for Arabic, both classical formula-based approaches and studies that combine Machine Learning (ML) with Natural Language Processing (NLP) techniques. The works we cover target text corpora for different audiences: school-age first language readers (L1), foreign language learners (L2), and adult readers in non-academic contexts. Therefore, we explore differences between reading in L1 and L2 and consider how they play out specifically in Arabic after describing language characteristics that may impact readability. Finally, we highlight challenges for Arabic readability research and propose multiple future directions to improve readability assessment and related applications that would benefit from more attention.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
articleno = {95},
numpages = {30},
keywords = {features, Arabic, Machine Learning (ML), readability formulas, computational linguistics, text complexity, reading difficulty, Readability}
}
@misc{naous_readme_2023,
	title = {{ReadMe}++: {Benchmarking} {Multilingual} {Language} {Models} for {Multi}-{Domain} {Readability} {Assessment}},
	shorttitle = {{ReadMe}++},
	url = {http://arxiv.org/abs/2305.14463},
	doi = {10.48550/arXiv.2305.14463},
	abstract = {We present a systematic study and comprehensive evaluation of large language models for automatic multilingual readability assessment. In particular, we construct ReadMe++, a multilingual multi-domain dataset with human annotations of 9757 sentences in Arabic, English, French, Hindi, and Russian collected from 112 different data sources. ReadMe++ offers more domain and language diversity than existing readability datasets, making it ideal for benchmarking multilingual and non-English language models (including mBERT, XLM-R, mT5, Llama-2, GPT-4, etc.) in the supervised, unsupervised, and few-shot prompting settings. Our experiments reveal that models fine-tuned on ReadMe++ outperform those trained on single-domain datasets, showcasing superior performance on multi-domain readability assessment and cross-lingual transfer capabilities. We also compare to traditional readability metrics (such as Flesch-Kincaid Grade Level and Open Source Metric for Measuring Arabic Narratives), as well as the state-of-the-art unsupervised metric RSRS (Martinc et al., 2021). We will make our data and code publicly available at: https://github.com/tareknaous/readme.},
	urldate = {2024-04-28},
	publisher = {arXiv},
	author = {Naous, Tarek and Ryan, Michael J. and Lavrouk, Anton and Chandra, Mohit and Xu, Wei},
	month = nov,
	year = {2023},
	note = {arXiv:2305.14463 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\juand\\Zotero\\storage\\5JDS67MZ\\Naous et al. - 2023 - ReadMe++ Benchmarking Multilingual Language Model.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\juand\\Zotero\\storage\\QBHAKUX4\\2305.html:text/html},
}



@misc{yousef2019learning,
      title={Learning meters of {A}rabic and {E}nglish poems with Recurrent Neural Networks: a step forward for language understanding and synthesis}, 
      author={Waleed A. Yousef and Omar M. Ibrahime and Taha M. Madbouly and Moustafa A. Mahmoud},
      year={2019},
      eprint={1905.05700},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{rashwan-etal-2009-hybrid,
  author = {Mohsen Rashwan and Mohammad Al-Badrashiny and Mohamed Attia and Sherif Abdou},
  title = {A Hybrid System for Automatic Arabic Diacritization},
  booktitle = {Proceedings of the Second International Conference on Arabic Language Resources and Tools},
  year = {2009},
  month = {April},
  date = {22-23},
  address = {Cairo, Egypt},
  editor = {Khalid Choukri and Bente Maegaard},
  publisher = {The MEDAR Consortium},
  isbn = {2-9517408-5-9},
  language = {english}
  }

@inproceedings{Obeid:2022:camelira,
    title = "Camelira: An {A}rabic Multi-Dialect Morphological Disambiguator",
    author = "Obeid, Ossama  and
      Inoue, Go  and
      Habash, Nizar",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, UAE",

    url = "https://aclanthology.org/2022.emnlp-demos.32",
    doi = "10.18653/v1/2022.emnlp-demos.32",
    pages = "319--326",
    abstract = "We present Camelira, a web-based Arabic multi-dialect morphological disambiguation tool that covers four major variants of Arabic: Modern Standard Arabic, Egyptian, Gulf, and Levantine.Camelira offers a user-friendly web interface that allows researchers and language learners to explore various linguistic information, such as part-of-speech, morphological features, and lemmas.Our system also provides an option to automatically choose an appropriate dialect-specific disambiguator based on the prediction of a dialect identification component. Camelira is publicly accessible at http://camelira.camel-lab.com.",
}


@inproceedings{alhafni-etal-2023-advancements,
    title = "Advancements in {A}rabic Grammatical Error Detection and Correction: An Empirical Investigation",
    author = "Alhafni, Bashar  and
      Inoue, Go  and
      Khairallah, Christian  and
      Habash, Nizar",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",

    url = "https://aclanthology.org/2023.emnlp-main.396",
    doi = "10.18653/v1/2023.emnlp-main.396",
    pages = "6430--6448",
    abstract = "Grammatical error correction (GEC) is a well-explored problem in English with many existing models and datasets. However, research on GEC in morphologically rich languages has been limited due to challenges such as data scarcity and language complexity. In this paper, we present the first results on Arabic GEC using two newly developed Transformer-based pretrained sequence-to-sequence models. We also define the task of multi-class Arabic grammatical error detection (GED) and present the first results on multi-class Arabic GED. We show that using GED information as auxiliary input in GEC models improves GEC performance across three datasets spanning different genres. Moreover, we also investigate the use of contextual morphological preprocessing in aiding GEC systems. Our models achieve SOTA results on two Arabic GEC shared task datasets and establish a strong benchmark on a recently created dataset. We make our code, data, and pretrained models publicly available.",
}

@phdthesis{halabi2016modern,
  title={Modern standard Arabic phonetics for speech synthesis},
  author={Halabi, Nawar},
  year={2016},
  school={University of Southampton}
}

@inproceedings{Abdelali:2022:natiq,
    title = "{N}ati{Q}: An End-to-end Text-to-Speech System for {A}rabic",
    author = "Abdelali, Ahmed  and
      Durrani, Nadir  and
      Demiroglu, Cenk  and
      Dalvi, Fahim  and
      Mubarak, Hamdy  and
      Darwish, Kareem",
    editor = "Bouamor, Houda  and
      Al-Khalifa, Hend  and
      Darwish, Kareem  and
      Rambow, Owen  and
      Bougares, Fethi  and
      Abdelali, Ahmed  and
      Tomeh, Nadi  and
      Khalifa, Salam  and
      Zaghouani, Wajdi",
    booktitle = "Proceedings of the The Seventh Arabic Natural Language Processing Workshop (WANLP)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",

    url = "https://aclanthology.org/2022.wanlp-1.38",
    doi = "10.18653/v1/2022.wanlp-1.38",
    pages = "394--398",
    abstract = "NatiQ is end-to-end text-to-speech system for Arabic. Our speech synthesizer uses an encoder-decoder architecture with attention. We used both tacotron-based models (tacotron- 1 and tacotron-2) and the faster transformer model for generating mel-spectrograms from characters. We concatenated Tacotron1 with the WaveRNN vocoder, Tacotron2 with the WaveGlow vocoder and ESPnet transformer with the parallel wavegan vocoder to synthesize waveforms from the spectrograms. We used in-house speech data for two voices: 1) neu- tral male {``}Hamza{''}- narrating general content and news, and 2) expressive female {``}Amina{''}- narrating children story books to train our models. Our best systems achieve an aver- age Mean Opinion Score (MOS) of 4.21 and 4.40 for Amina and Hamza respectively. The objective evaluation of the systems using word and character error rate (WER and CER) as well as the response time measured by real- time factor favored the end-to-end architecture ESPnet. NatiQ demo is available online at \url{https://tts.qcri.org}.",
}


@inproceedings{Inoue:2022:morphosyntactic,
    title = "Morphosyntactic Tagging with Pre-trained Language Models for {A}rabic and its Dialects",
    author = "Inoue, Go  and
      Khalifa, Salam  and
      Habash, Nizar",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",

    url = "https://aclanthology.org/2022.findings-acl.135",
    doi = "10.18653/v1/2022.findings-acl.135",
    pages = "1708--1719",
    abstract = "We present state-of-the-art results on morphosyntactic tagging across different varieties of Arabic using fine-tuned pre-trained transformer language models. Our models consistently outperform existing systems in Modern Standard Arabic and all the Arabic dialects we study, achieving 2.6{\%} absolute improvement over the previous state-of-the-art in Modern Standard Arabic, 2.8{\%} in Gulf, 1.6{\%} in Egyptian, and 8.3{\%} in Levantine. We explore different training setups for fine-tuning pre-trained transformer language models, including training data size, the use of external linguistic resources, and the use of annotated data from other dialects in a low-resource scenario. Our results show that strategic fine-tuning using datasets from other high-resource dialects is beneficial for a low-resource dialect. Additionally, we show that high-quality morphological analyzers as external linguistic resources are beneficial especially in low-resource settings.",
}

@inproceedings{Obeid:2020:camel,
    title = "{CAM}e{L} Tools: An Open Source Python Toolkit for {A}rabic Natural Language Processing",
    author = "Obeid, Ossama  and
      Zalmout, Nasser  and
      Khalifa, Salam  and
      Taji, Dima  and
      Oudah, Mai  and
      Alhafni, Bashar  and
      Inoue, Go  and
      Eryani, Fadhl  and
      Erdmann, Alexander  and
      Habash, Nizar",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",

    url = "https://aclanthology.org/2020.lrec-1.868",
    pages = "7022--7032",
    abstract = "We present CAMeL Tools, a collection of open-source tools for Arabic natural language processing in Python. CAMeL Tools currently provides utilities for pre-processing, morphological modeling, Dialect Identification, Named Entity Recognition and Sentiment Analysis. In this paper, we describe the design of CAMeL Tools and the functionalities it provides.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{Bahar:2023:Take,
  author={Parnia Bahar and Mattia {Di Gangi} and Nick Rossenbach and Mohammad Zeineldeen},
  title={{Take the Hint: Improving Arabic Diacritization with Partially-Diacritized Text}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
  pages={3949--3953},
  doi={10.21437/Interspeech.2023-903}
}

@article{esmail-etal-2022-much,
    title = "How Much Does Lookahead Matter for Disambiguation? Partial {A}rabic Diacritization Case Study",
    author = "Esmail, Saeed  and
      Bar, Kfir  and
      Dershowitz, Nachum",
    journal = "Computational Linguistics",
    volume = "48",
    number = "4",
    month = dec,
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.cl-4.20",
    doi = "10.1162/coli_a_00456",
    pages = "1103--1123",
    abstract = {We suggest a model for partial diacritization of deep orthographies. We focus on Arabic, where the optional indication of selected vowels by means of diacritics can resolve ambiguity and improve readability. Our partial diacritizer restores short vowels only when they contribute to the ease of understandability during reading a given running text. The idea is to identify those uncertainties of absent vowels that require the reader to look ahead to disambiguate. To achieve this, two independent neural networks are used for predicting diacritics, one that takes the entire sentence as input and another that considers only the text that has been read thus far. Partial diacritization is then determined by retaining precisely those vowels on which the two networks disagree, preferring the reading based on consideration of the whole sentence over the more na{\"\i}ve reading-order diacritization. For evaluation, we prepared a new dataset of Arabic texts with both full and partial vowelization. In addition to facilitating readability, we find that our partial diacritizer improves translation quality compared either to their total absence or to random selection. Lastly, we study the benefit of knowing the text that follows the word in focus toward the restoration of short vowels during reading, and we measure the degree to which lookahead contributes to resolving ambiguities encountered while reading. L{'}Herbelot had asserted, that the most ancient Korans, written in the Cufic character, had no vowel points; and that these were first invented by Jahia{--}ben Jamer, who died in the 127th year of the Hegira. {``}Toderini{'}s History of Turkish Literature,{''} Analytical Review (1789)},
}



@article{azmi2015survey,
  title={A survey of automatic {Arabic} diacritization techniques},
  author={Azmi, Aqil M and Almajed, Reham S},
  journal={Natural Language Engineering},
  volume={21},
  number={3},
  pages={477--495},
  year={2015},
  publisher={Cambridge University Press}
}

@article{ZITOUNI2009257,
title = {Arabic diacritic restoration approach based on maximum entropy models},
journal = {Computer Speech \& Language},
volume = {23},
number = {3},
pages = {257-276},
year = {2009},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2008.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0885230808000351},
author = {Imed Zitouni and Ruhi Sarikaya},
keywords = {Arabic diacritic restoration, Vowelization, Maximum entropy, Finite state transducer},
abstract = {In modern standard Arabic and in dialectal Arabic texts, short vowels and other diacritics are omitted. Exceptions are made for important political and religious texts and in scripts for beginning students of Arabic. Scripts without diacritics have considerable ambiguity because many words with different diacritic patterns appear identical in a diacritic-less setting. In this paper we present a maximum entropy approach for restoring short vowels and other diacritics in an Arabic document. The approach can easily integrate and make effective use of diverse types of information; the model we propose integrates a wide array of lexical, segment-based and part-of-speech tag features. The combination of these feature types leads to a high-performance diacritic restoration model. Using a publicly available corpus (LDC’s Arabic Treebank Part 3), we achieve a diacritic error rate of 5.1%, a segment error rate 8.5%, and a word error rate of 17.3%. In case-ending-less setting, we obtain a diacritic error rate of 2.2%, a segment error rate of 4.0%, and a word error rate of 7.2%. We also show in this paper a comparison of our approach to previously published techniques and we demonstrate the effectiveness of this technique in restoring diacritics in different kind of data such as the dialectal Iraqi Arabic scripts.}
}

@article{hamed:2017:survey,
  title={A Survey and Comparative Study of {Arabic} Diacritization Tools},
  author={Osama Hamed and Torsten Zesch},
  journal={J. Lang. Technol. Comput. Linguistics},
  year={2017},
  volume={32},
  pages={27-47},
  url={https://api.semanticscholar.org/CorpusID:59347172}
}

@article{darwish:2021:arabic,
author = {Darwish, Kareem and Abdelali, Ahmed and Mubarak, Hamdy and Eldesouki, Mohamed},
title = {Arabic Diacritic Recovery Using a Feature-Rich BiLSTM Model},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3434235},
doi = {10.1145/3434235},
abstract = {Diacritics (short vowels) are typically omitted when writing Arabic text, and readers have to reintroduce them to correctly pronounce words. There are two types of Arabic diacritics: The first are core-word diacritics (CW), which specify the lexical selection, and the second are case endings (CE), which typically appear at the end of word stems and generally specify their syntactic roles. Recovering CEs is relatively harder than recovering core-word diacritics due to inter-word dependencies, which are often distant. In this article, we use feature-rich recurrent neural network model that use a variety of linguistic and surface-level features to recover both core word diacritics and case endings. Our model surpasses all previous state-of-the-art systems with a CW error rate (CWER) of 2.9\% and a CE error rate (CEER) of 3.7\% for Modern Standard Arabic (MSA) and CWER of 2.2\% and CEER of 2.5\% for Classical Arabic (CA). When combining diacritized word cores with case endings, the resultant word error rates are 6.0\% and 4.3\% for MSA and CA, respectively. This highlights the effectiveness of feature engineering for such deep neural models.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
articleno = {33},
numpages = {18},
keywords = {text tagging, Arabic, diacritization}
}


@inproceedings{sutskever:2014:sequence,
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
title = {Sequence to Sequence Learning with Neural Networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3104–3112},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@misc{shatnawi2023automatic,
      title={Automatic Restoration of Diacritics for Speech Data Sets}, 
      author={Sara Shatnawi and Sawsan Alqahtani and Hanan Aldarmaki},
      year={2023},
      eprint={2311.10771},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{azmi:2021:light,
author = {Azmi, Aqil M. and Alnefaie, Rehab M. and Aboalsamh, Hatim A.},
title = {Light Diacritic Restoration to Disambiguate Homographs in Modern Arabic Texts},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3486675},
doi = {10.1145/3486675},
abstract = {Diacritic restoration (also known as diacritization or vowelization) is the process of inserting the correct diacritical markings into a text. Modern Arabic is typically written without diacritics, e.g., newspapers. This lack of diacritical markings often causes ambiguity, and though natives are adept at resolving, there are times they may fail. Diacritic restoration is a classical problem in computer science. Still, as most of the works tackle the full (heavy) diacritization of text, we, however, are interested in diacritizing the text using a fewer number of diacritics. Studies have shown that a fully diacritized text is visually displeasing and slows down the reading. This article proposes a system to diacritize homographs using the least number of diacritics, thus the name “light.” There is a large class of words that fall under the homograph category, and we will be dealing with the class of words that share the spelling but not the meaning. With fewer diacritics, we do not expect any effect on reading speed, while eye strain is reduced. The system contains morphological analyzer and context similarities. The morphological analyzer is used to generate all word candidates for diacritics. Then, through a statistical approach and context similarities, we resolve the homographs. Experimentally, the system shows very promising results, and our best accuracy is 85.6\%.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {dec},
articleno = {60},
numpages = {14},
keywords = {homographs, Arabic language, disambiguation, morphological analysis, automatic diacritization}
}

@misc{pourander:2021:unicode,
  author = {Pournader, Roozbeh and Hallissy, Bob and Evans, Lorna},
  title = {{U}nicode {A}rabic {M}ark {R}endering},
  year = 2021,
  url= {https://www.unicode.org/reports/tr53/tr53-6.html#AMTRA_Specification},
  urldate = {2023-12-11}
}

@misc{whistler:2023:unicode,
  author = {Whistler, Ken},
  title = {{U}nicode {N}ormalization {F}orms},
  year = 2023,
  url = {https://www.unicode.org/reports/tr15/tr15-54.html},
  urldate = {2023-12-11}
}

@misc{whistler:2023:unicodecharacter,
  author = {Whistler, Ken},
  title = {{U}nicode {C}haracter {D}atabase},
  year = 2023,
  url = {http://www.unicode.org/reports/tr44/#Canonical_Combining_Class_Values},
  urldate = {2023-12-11}
}

@ARTICLE{madhfar:2021:effective,
  author={Madhfar, Mokthar Ali Hasan and Qamar, Ali Mustafa},
  journal={IEEE Access}, 
  title={Effective Deep Learning Models for Automatic Diacritization of Arabic Text}, 
  year={2021},
  volume={9},
  number={},
  pages={273-288},
  doi={10.1109/ACCESS.2020.3041676}}



@article{Zerrouki:2017:tashkeela,
  title={Tashkeela: Novel corpus of Arabic vocalized texts, data for auto-diacritization systems},
  author={Zerrouki, Taha and Balla, Amar},
  journal={Data in brief},
  volume={11},
  pages={147--151},
  year={2017},
  publisher={Elsevier}
}

@misc{Elnokrashy:2024:partial,
      title={Partial Diacritization: A Context-Contrastive Inference Approach}, 
      author={Muhammad ElNokrashy and Badr AlKhamissi},
      year={2024},
      eprint={2401.08919},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{aldarmaki:2023:diacritic,
  author={Hanan Aldarmaki and Ahmad Ghannam},
  title={{Diacritic Recognition Performance in Arabic ASR}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
  pages={361--365},
  doi={10.21437/Interspeech.2023-2344}
}

@article{ungurean:2008:automatic,
  title={Automatic diacritic restoration for a TTS-based e-mail reader application},
  author={Ungurean, Catalin and Burileanu, Dragos and Popescu, Vladimir and Negrescu, Cristian and Dervis, Aurelian},
  journal={UPB Scientific Bulletin, Series C},
  volume={70},
  number={4},
  pages={3--12},
  year={2008},
  publisher={Politehnica Press}
}

@misc{abdelali:2019:diacritization,
      title={Diacritization of Maghrebi Arabic Sub-Dialects}, 
      author={Ahmed Abdelali and Mohammed Attia and Younes Samih and Kareem Darwish and Hamdy Mubarak},
      year={2019},
      eprint={1810.06619},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Dukes:2013:supervised,
  title={Supervised collaboration for syntactic annotation of Quranic Arabic},
  author={Dukes, Kais and Atwell, Eric and Habash, Nizar},
  journal={Language resources and evaluation},
  volume={47},
  number={1},
  pages={33--62},
  year={2013},
  publisher={Springer}
}

@article{Altammami:2019:Arabic,
  title={The Arabic--English Parallel Corpus of Authentic Hadith},
  author={Altammami, Shatha and Atwell, Eric and Alsalka, Ammar},
  journal={International Journal on Islamic Applications in Computer Science And Technology-IJASAT},
  year={2019},
  publisher={Leeds}
}

@book{tufail:hayy,
   author = {Ibn~Tufail},
   year = {1150},
   title = {{Hayy ibn Yaqdhan}},
   publisher = {Hindawi},
   url = {https://www.hindawi.org/books/90463596/}
}

@book{ArabianNights,
   author = {Unknown},
   year = {12th century},
   title = {One Thousand and One Nights}
}

@book{bukhari,
   author = {Imam Muhammad al-Bukhari},
   year = {846},
   title = {Sahih al-Bukhari},
   publisher = {Dar Ibn Khathir}
}

@book{akkad:sarah,
   author = {Abbas Mahmoud Al-Akkad},
   year = {1938},
   title = {Sarah},
   publisher = {Hindawi},
   url = {https://www.hindawi.org/books/72707304/}
}

@book{quran, title={The Quran}}

@book{arabicNewTestament, 
title={{New Testament (Arabic Translation)}},
author = {Smith, Eli  and  Van~Dyck, Cornelius },
   year = {1860},
}

@book{arabicOldTestament, 
title={{Old Testament (Arabic Translation)}},
author = {Smith, Eli  and  Van~Dyck, Cornelius },
   year = {1865},
}

@inproceedings{CamelTB,
    title = "Camel Treebank: An Open Multi-genre {A}rabic Dependency Treebank",
    author = "Habash, Nizar  and
      AbuOdeh, Muhammed  and
      Taji, Dima  and
      Faraj, Reem  and
      El Gizuli, Jamila  and
      Kallas, Omar",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",

    url = "https://aclanthology.org/2022.lrec-1.286",
    pages = "2672--2681"
}
 

@inproceedings{2023.EDM-long-papers.9,
 address = {Bengaluru, India},
 author = {Afrizal Doewes and Nughthoh Arfawi Kurdhi and Akrati Saxena},
 booktitle = {Proceedings of the 16th International Conference on Educational Data Mining},
 doi = {10.5281/zenodo.8115784},
 editor = {Mingyu Feng and Tanja KÃ¤ser and Partha Talukdar},
 isbn = {978-1-7336736-4-8},
 month = {July},
 pages = {103--113},
 publisher = {International Educational Data Mining Society},
 title = {Evaluating Quadratic Weighted Kappa as the Standard Performance Metric for Automated Essay Scoring},
 year = {2023}
}

@book{AboAmsha2022:ArabicCEFR,
  author    = {Khaled Hassan {Abo~Amsha}
  and Raed Abdul Rahim
  and Hidaya Sheikh Ali
  and Al-Sayed Ezzat Abo Al-Wafa
  and Mohamed Ismaili Aloui
  and Belkacem Al-Youbi
  and Mohamed Al-Jarrah
  and Saleh Al-Hajouri
  and Mohamed Mutlaq
  and Bashir Al-Obaidi
  and Ibrahim Al-Shafie
  and Mohamed Haqqi Sawatchen
  and Riwaya Mohamed Jamous
  and Suhad Nairat},
  title     = {Applications of the Common European Framework of Reference (CEFR) in Teaching Arabic to Non-Native Speakers. Part One.},
  year      = {2022},
  publisher = {Dar Kunooz Al-Ma'arifah for Publishing and Distribution},
  address   = {Amman, Jordan}
}

@INPROCEEDINGS {9156542,
author = { Bertinetto, Luca and Mueller, Romain and Tertikas, Konstantinos and Samangooei, Sina and Lord, Nicholas A. },
booktitle = { 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) },
title = {{ Making Better Mistakes: Leveraging Class Hierarchies With Deep Networks }},
year = {2020},
volume = {},
ISSN = {},
pages = {12503-12512},
abstract = { Deep neural networks have improved image classification dramatically over the past decade, but have done so by focusing on performance measures that treat all classes other than the ground truth as equally wrong. This has led to a situation in which mistakes are less likely to be made than before, but are equally likely to be absurd or catastrophic when they do occur. Past works have recognised and tried to address this issue of mistake severity, often by using graph distances in class hierarchies, but this has largely been neglected since the advent of the current deep learning era in computer vision. In this paper, we aim to renew interest in this problem by reviewing past approaches and proposing two simple methods which outperform the prior art under several metrics on two large datasets with complex class hierarchies: tieredImageNet and iNaturalist'19. },
keywords = {Standards;Measurement;Vegetation;Machine learning;Art;Visualization;Pipelines},
doi = {10.1109/CVPR42600.2020.01252},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR42600.2020.01252},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jun}

@inproceedings{HouNIPSW17, 
Author = {L. Hou, C.P. Yu, D. Samaras}, 
Booktitle = {NIPS workshop on Learning on Distributions, Functions, Graphs and Groups}, 
Title = {Squared Earth Mover's Distance Loss for Training Deep Neural Networks on Ordered-Classes}, 
Year = {2017}} 


@article{BarberKlauda2020,
  author    = {Barber, Amelia T. and Klauda, Susan L.},
  title     = {How Reading Motivation and Engagement Enable Reading Achievement: Policy Implications},
  journal   = {Policy Insights from the Behavioral and Brain Sciences},
  volume    = {7},
  number    = {1},
  pages     = {27--34},
  year      = {2020},
  doi       = {10.1177/2372732219893385},
  url       = {https://doi.org/10.1177/2372732219893385}
}

@misc{naous2023readme,
      title={ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment}, 
      author={Tarek Naous and Michael J. Ryan and Anton Lavrouk and Mohit Chandra and Wei Xu},
      year={2023},
      eprint={2305.14463},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@phdthesis{phdthesis,
author = {Alfaifi, A.},
year = {2015},
school = {University of Leeds},
month = {11},
pages = {},
title = {Building the Arabic Learner Corpus and a System for Arabic Error Annotation},
doi = {10.13140/RG.2.2.32081.53608}
}



@misc{anonymous:2024:barec-guidelines,
  author = {Anonymous},
  title = {Guidelines - Anonymized for ACL 2025 Submission.},
  year = 2024
}