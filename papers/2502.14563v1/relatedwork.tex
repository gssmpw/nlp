\section{Related Work}
This section introduces the background of agent planning and graph understanding of LLMs.

\subsection{Planning for LLM-based Agents}
% https://arxiv.org/pdf/2402.02716#page=2.22
% https://openreview.net/pdf?id=bmoS6Ggw4j
% https://openreview.net/pdf?id=6LNTSrJjBe
Autonomous agents that interact with an environment to solve complex tasks \cite{yao2022webshop,fan2022minedojo}. Planning involves developing an action sequence before execution, leveraging global knowledge of the task and environment to suggest a logically consistent trajectory \cite{huang2024understanding}. 
% A good plan provides guidance for each step of execution and pushes the agents' performance closer to the global optimum. 

Planning decomposes a complex task and selects a feasible trajectory based on global knowledge \cite{valmeekam2023on, wang2023plan, wu2024oscopilot}.
Searching strategies are applied to explore the optimal plan, such as depth-/breadth-first search and Monte Carlo tree search \cite{yao2023tree, zhou2024language,qi2024mutual, zhao2024large}.
When environmental feedback supplements perception heavily and updates the task knowledge, planning involves reflection to refine the trajectory incrementally \cite{shinn2024reflexion}.
A world model or a reward model integrates global knowledge and predicts the environment states or estimates rewards \cite{hao-etal-2023-reasoning, qiao2024agent}.

\subsection{LLM on Graphs}
% https://openreview.net/pdf?id=CkKEuLmRnr
% https://arxiv.org/pdf/2409.03258
As realistic intricate challenges can be formed as graphs, recent studies explore the LLMs' capabilities for reasoning with graphs.
Graph-of-Thought \cite{besta2024got, Ning2024DGoTDG} first proposes to transform the problem thinking into an arbitrary graph to enable the generation, aggregation, and refining of sub-tasks.
\citet{yao-etal-2024-got} also extract deductive triplets from contexts and build graphs.
Knowledge graphs support faithfulness and inference transparency for knowledge-intensive tasks \cite{Luo2023ReasoningOG, sun2024thinkongraph, Wen2023MindMapKG}. \citet{lin2024graphenhanced} combines graphs with natural language prompts for reasoning about asynchronous plans in real-life tasks, instructing model to either reason based on a given graph or to generate a graph themselves and then reason about it.

%=============TODO: graph for planning========

% 感觉这章还是不太会写，不知道该说多少合适
% Prior work on \textit{naturalistic asynchronous planning} assumes a pre-decomposed set of subtasks with explicitly defined dependencies, focusing on computing minimal execution time under parallelization. 

% \cite{} only considered all necessary steps with a small total. However, in reality, planning may involve many unnecessary steps, leading to the emergence of some feasible solutions.

However, it is demonstrated that the capabilities of graph reasoning and understanding decrease as the scale and complexity of graphs increase. 
Empirical studies have observed a ``comprehension collapse'' phenomenon as the graph size increases \cite{Sui2023TableML, Cao2024GraphInsightUI}.
DARG \cite{zhang2024dargdynamicevaluationlarge} evaluates LLMs' reasoning capability on graphs and also reports a performance decrease with increasing complexity of graphs. 

% 这一句还是感觉不太会写，也不清楚要写多少
% Different from existing work, our paper further focus on the planning of models on tasks and corresponding graphs with more complex dependencies and multiple feasible solutions.

Different from existing work, our paper further provides a more formal and scalable definition of the planning task's graph structure, which captures the inherent complexities and dependencies of the task. Our \textit{planning-over-graph} offers a general framework, independent of the specific nature of the task. Additionally, we demonstrate the effectiveness of this approach by training models on these graphs, achieving significant improvements in performance.