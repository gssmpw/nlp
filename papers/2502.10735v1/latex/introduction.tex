\section{Introduction}
\label{sec:introduction}
% Large language models (LLMs) \citep{achiam2023gpt, touvron2023llama, le2023bloom} have recently shown remarkable performance in a range of complex language benchmarks in the field of language understanding and generation \citep{bubeck2023sparks, wei2022emergent, wei2022chain}. 
% Despite their impressive performance, their extensive model size causes significant computational demands, making LLM inference and deployment a big challenge. 
% One notable advancement in model compression has centered on model pruning \citep{lecun1989optimal, hassibi1993optimal, han2015learning}, which shrinks model sizes by removing specific weights from the model â€“ essentially setting them to zero. Traditional model pruning methods, typically involve retraining \citep{liu2018rethinking, blalock2020state} or iterative training to recover performance \citep{frankle2018lottery, renda2019comparing}, which are less feasible when scaling to large LLMs with billions of parameters. Recently, there has been a growing effort in post-training pruning (PTP) due to its minimal resource demands. PTP methods develop pruning metrics to evaluate the importance of weights, thus the weights with lower importance can be removed. \citep{frantar2023sparsegpt, sun2023simple, zhangplug}.

Large language models (LLMs) \cite{achiam2023gpt, touvron2023llama, le2023bloom} have demonstrated exceptional capabilities in language understanding and generation across various complex benchmarks \cite{bubeck2023sparks, wei2022emergent, wei2022chain}. However, their massive size poses significant challenges for inference and deployment due to extensive computational requirements.
Model pruning has emerged as a promising compression technique, which reduces model size by setting specific weights to zero. While traditional pruning approaches rely on retraining or iterative training to maintain performance \cite{lecun1989optimal, hassibi1993optimal, han2015learning, liu2018rethinking, blalock2020state, frankle2018lottery, renda2019comparing}, these methods become impractical for billion-parameter LLMs. Consequently, post-training pruning (PTP) has gained increasing attention due to its resource efficiency. PTP methods work by developing metrics to assess weight importance, allowing for the removal of less critical weights without the need for retraining \cite{frantar2023sparsegpt, sun2023simple, zhangplug}.

However, as shown in Figure \ref{fig:weight_distribution}, we observe a significant performance drop when applying recent well-established pruning metrics \citep{frantar2023sparsegpt, sun2023simple, zhangplug} to the LLaMA-3 \citep{llama3} model. To analyze the reason for the performance drop, we demonstrate the distributions of input activation norms and weight magnitudes, two main components considered by recent pruning metrics. 
Despite the past success of SparseGPT \citep{frantar2023sparsegpt}, Wanda \citep{sun2023simple}, and RIA \citep{zhangplug} on LLaMA-1 \citep{touvron2023llama} and LLaMA-2 \citep{touvron2023llama2} models, the distinct weight distribution of LLaMA-3 underscores the limitations of using a fixed pruning metric across LLMs with varying weight distributions. 

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{figures/LLM_weight_distribution.pdf}
  \caption{
  \textbf{Performance of existing pruning metrics on different LLMs.} Existing pruning metrics show significant performance drops on the LLaMA-3 model (bar charts in the upper part), influenced by its distinct weight distribution compared to LLaMA-1/2 and Mistral models (the lower part).
  }
  \vspace{-1em}
  \label{fig:weight_distribution}
\end{figure*}

In this paper, we study the essential adaption of pruning strategy across different LLMs, and propose an efficient evolutionary optimization framework, named \textsc{OptiShear}, to automatically search for the pruning strategy for different LLMs, including optimization of both the pruning metric and the layerwise sparsity ratios. 
In particular, we design an effective search space built on our Meta pruning metric. 
% we introduce a meta pruning metric for adaptive LLM pruning. 
Unlike prior pruning metrics \citep{frantar2023sparsegpt, sun2023simple, zhangplug} that consider weights and activations rely on fixed heuristics, our meta pruning metric dynamically balances the relationship between weights and activations, to mitigate diverse weight distributions among different LLMs. 
Moreover, we also consider a better way for post-training pruning evaluations of each search trial. We show that prior evaluations based on perplexity \citep{dongpruner} are more time-consuming and establish limited generalizability across different downstream tasks. Instead, we propose a lightweight search evaluation, model-wise reconstruction error, to speed up the evaluation.
Finally, we apply NSGA-III \citep{deb2013evolutionary, jain2013evolutionary} as our search algorithm, handling both the single-objective problem of pruning metric search and the multi-objective problem of layerwise sparsity ratio search in a unified framework.

We empirically evaluate \textsc{OptiShear} on the widely adopted LLaMA-1, LLaMA-2, LLaMA-3 and Mistral models across multiple benchmarks. Our results demonstrate that, without any retraining or weight update, our \textsc{OptiShear}-derived pruning metrics consistently outperform all established pruning metrics. 
Additionally, our \textsc{OptiShear}-derived layerwise sparsity ratios could also boost the effectiveness of other pruning metrics that consider both weight and activation, such as Wanda and RIA.
Furthermore, we verify the generalizability of our \textsc{OptiShear}-derived pruning metrics through cross-task and cross-model evaluations, showing that metrics developed for complex arithmetic reasoning tasks also perform well on simpler tasks like language modeling, and remain effective when applied to models of different configurations.
%Thus we provide a cost-effective alternative to streamline the adaptive search process.
% , showing that metrics developed for complex arithmetic reasoning tasks excel on simpler tasks like commonsense reasoning and language modeling, and remaining effective when applied to models of different configurations. 

% showing that metrics developed for 
% % complex arithmetic reasoning tasks also show strong performance on simpler tasks like commonsense reasoning and language modeling.
% % Building upon this, within models that share similar weight distributions, the pruning metric derived from models that are notable for their superior performance, matches or even exceeds the performance of the pruning metrics that are originally developed for these specific models. 
% complex arithmetic reasoning tasks also perform well on simpler tasks like commonsense reasoning and language modeling. Additionally, the pruning metrics derived from high-performing models match or even surpass the performance of metrics originally developed for those specific models.