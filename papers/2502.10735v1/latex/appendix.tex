\newpage
\section{Appendix}

% \subsection{NSGA-III Algorithm}

% Detailed process of NSGA-III algorithm is shown in Algorithm \ref{alg:nsga3}. The algorithm starts with an initial population $P_0$ and iterates for a fixed number of generations. In each generation, it combines the parent $P_t$ and offspring populations $Q_t$, performs non-dominated sorting to rank solutions, and selects the best solutions to form the next generation $P_{t+1}$. The niching procedure ensures diversity by favoring solutions close to under-represented reference points.

% \begin{algorithm}[ht]
% \small
% \begin{algorithmic}[1]
% \State Initialize population $P_0$ of size $N$
% \For{$t = 1$ to $T$}
%     \State $Q_t \gets \text{CreateOffspring}(P_t)$
%     \State $R_t \gets P_t \cup Q_t$
%     \State $\mathcal{F} \gets \text{NonDominatedSort}(R_t)$
%     \State $P_{t+1} \gets \text{SelectPopulation}(\mathcal{F}, N)$
% \EndFor
% \State \Return $P_T$
% \end{algorithmic}
% \vspace{-1mm}
% \caption{NSGA-III for Searching Adaptive Pruning Strategy}
% \label{alg:nsga3}
% \end{algorithm}



\subsection{Effectiveness on Super Large Language Models}
\label{sec:70b}
We also explore the effectiveness of \textsc{OptiShear} when scaling up the model size. As shown in Table \ref{tab:70b}, \textsc{OptiShear} consistently outperforms the existing methods across the board.

\begin{table*}[t!]
\resizebox{\textwidth}{!}{%
\fontsize{6.5}{8} \selectfont
\begin{tabular}{ll|c|cccccccc}
\toprule
Model & Method & WikiText & BoolQ & RTE & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average \\
  \hline
\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}LLaMA\\ 30B\end{tabular}} &
  Dense & 4.77 & 82.69 & 66.79 & 63.35 & 75.69 & 80.30 & 52.82 & 36.00 & 65.38 \\
  \cdashline{2-11}
 &
  Magnitude & 7.55 & 64.34 & 50.18 & 50.59 & 66.54 & 72.39 & 43.77 & 29.00 & 53.83 \\
 &
  SparseGPT &5.32&82.32&62.45&59.15&75.22&78.96&48.56&\textbf{35.00}&63.09
   \\
 &
  Wanda &5.98&81.90&65.34&\textbf{60.93}&\textbf{73.48}&\textbf{79.29}&49.66&34.60&63.60
   \\
 &
  RIA &5.16&\textbf{83.36}&67.15&60.01&72.85&78.70&48.29&33.60&63.42
   \\
 &
  \textsc{OptiShear} &\textbf{5.10}&\textbf{83.36}&\textbf{67.51}&\textbf{60.93}&72.61&78.91&\textbf{49.74}&34.20&\textbf{63.89}
   \\
   \hline
\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}LLaMA2\\ 70B\end{tabular}} &
  Dense &3.12&83.40 & 67.87& 66.10& 78.06& 82.55& 54.44& 37.20&67.08
  \\
  \cdashline{2-11}
 &
  Magnitude &4.98&70.55 & 60.65& 61.50& 73.48& 75.70& 49.23& 35.40&60.93 \\
 &
  SparseGPT &3.98&\textbf{83.55} & 70.40& 63.80& \textbf{78.85}& \textbf{82.40}& \textbf{53.75}& 38.20&67.28 \\
 &
  Wanda &3.99&82.50 & \textbf{73.65}& \textbf{64.10}& 78.14& 80.80& 52.65& 37.40&67.03 \\
 &
  RIA &3.91&83.25 & 71.49& 64.05& 77.74& 81.20& 53.16& 36.60&66.77 \\
 &
  \textsc{OptiShear} &\textbf{3.86}&83.25 & 73.21& 64.00& 78.48& 81.25& 53.07& \textbf{38.40}&\textbf{67.38} \\
  \bottomrule
\end{tabular}%
}
\caption{WikiText perplexity and mean zero-shot accuracies(\%) on the LM Harness of 50\% unstructured pruned LLaMA-1 30B and LLaMA-2 70B models. }
\label{tab:70b}
\vspace{-0.5em}
\end{table*}


\subsection{Ablation Study on Search Algorithms and Robustness Analysis}
\label{sec:robustness}

We conduct a robustness analysis using five search algorithms, including random search \citep{bergstra2012random}, which randomly samples hyperparameter values from a predefined search space and does not take into account any information about the performance of previous trials, the Tree-structured Parzen Estimator (TPE) \citep{bergstra2011algorithms, bergstra2013making, ozaki2022multiobjective}, a Bayesian optimization algorithm that uses a tree structure to model the relationship between hyperparameters and the objective function, and Quasi-Monte Carlo (QMC) \citep{bergstra2012random} sampler, which generates a sequence of points that cover the search space more evenly compared to random sampling, for more efficient exploration. Additionally, we utilize the state-of-the-art multi-objective optimization algorithms Non-dominated Sorting Genetic Algorithm II (NSGA-II) \citep{deb2002fast} and NSGA-III \citep{deb2013evolutionary, jain2013evolutionary}, which are based on genetic algorithms and optimize multiple conflicting objectives simultaneously. NSGA-II uses a non-dominated sorting approach to rank solutions based on their dominance relationship, while NSGA-III extends NSGA-II by incorporating reference points to guide the search toward the Pareto front. 

\begin{table*}[t!]
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccc}
\toprule
Dataset        & Random & TPE & QMC  & NSGA-II & NSGA-III \\
\hline
WikiText & 6.89 ( $\pm$ 0.0671)   & \textbf{6.33} ( $\pm$ 0.0714) & 6.39 ( $\pm$ 0.0700) & 6.44 ( $\pm$ 0.0632)   &  6.35 ( $\pm$ 0.0640)        \\
GSM8K          & 7.96 ( $\pm$ 0.2406)  & 8.33 ( $\pm$ 0.2498) & 8.08 ( $\pm$ 0.2220) & 8.47 ( $\pm$ 0.2479)  &  \textbf{8.49} ( $\pm$ 0.2646)        \\
MMLU           &   31.11 ( $\pm$ 0.3962)    &   31.06 ( $\pm$ 0.4017)  &   31.80 ( $\pm$ 0.4400)   &  32.43 ( $\pm$ 0.4701)       &   \textbf{33.06} ( $\pm$ 0.4687)       \\
LM-harness & 55.32 ( $\pm$ 0.5300) & 55.74 ( $\pm$ 0.5367) & 56.19 ( $\pm$ 0.5234) & 56.59 ( $\pm$ 0.5689) & \textbf{57.47} ( $\pm$ 0.5718) \\
\bottomrule
\end{tabular}%
}
\caption{Statistical results of different search algorithms on LLaMA-2 7B model. 
We report the mean and standard deviation under 3 search process runs.
}
\label{tab:robust}
\end{table*}

\begin{table*}[t!]
    \centering
    \setlength{\tabcolsep}{5pt}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccc}
    \toprule
        Seed & 2 Ops \& 2 Coes & 3 Ops \& 3 Coes & 4 Ops \& 4 Coes & 5 Ops \& 5 Coes & 6 Ops \& 6 Coes & Full Search Space\\
        \hline
        0 & 870.16 & 753.32 & 6.43 & 6.51 & 6.57 & \textbf{6.35}\\
        42 & 6.47 & 6.39 & 6.39 & 6.39 & 6.35 & \textbf{6.35}\\
        100 & 6.43 & 6.43 & 6.43 & 6.58 & 6.58 & \textbf{6.35}\\
        \bottomrule
    \end{tabular}
    }
    \caption{WikiText perplexity for random subspaces of the search space on LLaMA2-7B model in searching for optimal pruning metric. }
    \label{tab:subspace}
\end{table*}

Table \ref{tab:robust} presents the statistical analysis, specifically the mean and standard deviation, of the performance of pruned LLaMA-2 7B models across four distinct benchmarks. To validate the robustness and reliability of our results, each model was evaluated using three different search processes, each initialized with different random seeds. We report the performance outcomes of the NSGA-III search method in the main paper, as it generally outperforms other algorithms. 

\subsection{Ablation Study on Search Space}
\label{sec:space}
In Table \ref{tab:subspace}, we construct the sub-search spaces by randomly selecting 2-6 coefficients/operations from our candidate pool. We test three different subspaces using random seeds 0, 42, and 100. The evaluations are conducted on WikiText, and the perplexity scores are reported below. We can see that the search performed on the full set consistently yields the best results. An interesting observation is that using a very small subspace may lead to extremely poor outcomes. This occurs because the candidate coefficients/operations in the subspace are all unsuitable for the target model.




\subsection{Search Cost}
\label{sec:appendix_cost}
In Table \ref{tab:time}, we provide the detailed search time consumed on a single Nvidia RTX A6000 GPU. We report the total time of 350 search trials, as we empirically found that the optimal value is generally obtained within these rounds, as illustrated in Figure 4(c). As shown in the table, the time of an optimal search is within 2.5 GPU hours. With multiple GPUs, the search process can generally be finished within 1 hour. Therefore, we believe that the search cost of our method is moderate and acceptable.


\begin{table*}[htbp]
    \centering
    \setlength{\tabcolsep}{4.5pt}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccccc}
    \toprule
         Search & L1-7B & L1-13B & L2-7B & L2-13B & L3-8B & L3-8B-it & M-7B & M-7B-it \\
         \hline
         Metric& 1h10m28s & 2h13m6s & 1h6m14s & 2h11m55s & 1h30m47s & 1h31m51s & 1h14m22s & 1h15m54s \\
         Ratio& 1h13m44s & 2h19m28s & 1h9m34s & 2h22m45s & 1h31m59s & 1h32m51s & 1h17m8s & 1h18m12s\\
         \bottomrule
    \end{tabular}
    }
    \caption{Cost of searching for optimal pruning metric and layerwise sparsity ratios on LLaMA-1/2/3 and Mistral models. }
    \label{tab:time}
\end{table*}



\subsection{Hyperparameter Analysis}
\label{appendix:hyper}
We evaluate the impact of various hyperparameters applied in the layerwise sparsity ratios search procession the performance of WikiText perplexity. We use the LLaMA-2 7B model and prune to unstructured 50\% sparsity. 


\begin{figure*}[!b]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sparsity_step.png}
        \label{fig:sparsity}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/nratios.png}
        \label{fig:nsample}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/ntrials.png}
        \label{fig:ntrials}
    \end{subfigure}
    \caption{Sensitivity evaluation on sparsity step, number of sparsity ratios, and the number of trials in layerwise sparsity ratio search. }
    \label{fig:search_param}
\end{figure*}

\paragraph{Sparsity step.}

In layerwise sparsity ratio search, we identify the optimal sparsity ratio for each layer by selecting from a predefined sparsity ratio set: [\texttt{target sparsity - sparsity step}, \texttt{target sparsity}, \texttt{target sparsity + sparsity step}]. Here, target sparsity is the pre-defined sparsity ratio for pruning the overall model. The sparsity step allows for adjustments to achieve slightly higher or lower sparsity ratios.

In Figure \ref{fig:search_param}(a), we vary the
sparsity step ranging between 3\% and 10\%.
We empirically find that a 5\% sparsity step usually performs better than other sparsity steps, such as lower 3\% or higher 8\% and 10\%. This is possibly because smaller steps might not significantly reduce redundancy, while larger steps might overly simplify the layers, leading to a loss of important features and a decrease on overall model performance.




\paragraph{Number of sparsity ratios.}

In Figure \ref{fig:search_param}(b), we fix the sparsity step as 5\% and vary the number of sparsity ratios in the predefined sparsity ratio set, which ranges from 3 to 9. Specifically, one sparsity ratio in the sparsity ratio set corresponds to uniform pruning across layers. For example, a predefined sparsity ratio set with 5 sparsity ratios is defined as [\texttt{target sparsity - 2*sparsity step}, \texttt{target sparsity - sparsity step}, \texttt{target sparsity}, \texttt{target sparsity + sparsity step}, \texttt{target sparsity + 2*sparsity step}]. 

We empirically find that, for LLaMA2-7B model that contains 32 layers, a discrete sparsity set of 3 sparsity ratios is able to search for better results than larger sets of sparsity ratios. This possibly because a larger number of sparsity ratios significantly expands the search space, making it challenging to find the optimal solution within a limited number of search trials.


\begin{table*}[t!]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lc|lc}
        \toprule
        coefficient & Equation & coefficient & Equation\\
        \hline
         no coe & $\alpha=\beta=1$ & F norm &  $\alpha = \beta = 1/{\sqrt{\sum_{i=1}^m \sum_{j=1}^n A_{ij}^2}}$ \\
         to sum & $\alpha = \beta = 1/{\sum_{i=1}^m \sum_{j=1}^n A_{ij}}$ & to mean & $ \alpha = \beta = mn/{\sum_{i=1}^m \sum_{j=1}^n A_{ij}} $\\
         row sum & $\alpha=\beta=1/{\sum_{j=1}^n A_{ij}} $ & column sum & $\alpha=\beta=1/{\sum_{i=1}^m A_{ij}}$\\
         relative sum & $\alpha=\beta=$ row sum ($A_{ij}$)+ column sum ($A_{ij}$) \\
         \bottomrule
    \end{tabular}
    }
    \caption{Detailed calculations for the coefficient sets in meta pruning metric.}
    \label{tab:coe_eqs}
\end{table*}

\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccc}
\toprule
\multirow{2}{*}{Metric} &
  \multicolumn{2}{c}{LLaMA-1} &
  \multicolumn{2}{c}{LLaMA-2} &
  \multicolumn{2}{c}{LLaMA-3} &
  \multicolumn{2}{c}{Mistral} \\      & 7B    & 13B    & 7B    & 13B   
          & 8B    & 8B-Inst
           & 7B    & 7B-Inst \\
           \hline
$\alpha$       & relative sum & relative sum & relative sum & relative sum & relative sum & no coe & relative sum & F norm   \\
$\beta$   & to mean &  to mean & to mean & no coe & F norm & F norm & to mean & to mean   \\
$\tau_{1}$  & no op &  square & no op & square & no op & no op & square & sqrt   \\
$\tau_{2}$  & sqrt & no op  & sqrt & sqrt & no op & no op & no op & sqrt   \\
\bottomrule
\end{tabular}%
}
\caption{Optimal coefficients and operations for pruning metrics on C4 calibration data. }
\label{tab:zero-metrics}
\end{table*}


\begin{table*}[htbp]
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccc}
\toprule
\multirow{2}{*}{Metric} &
  \multicolumn{2}{c}{LLaMA-1} &
  \multicolumn{2}{c}{LLaMA-2} &
  \multicolumn{2}{c}{LLaMA-3} &
  \multicolumn{2}{c}{Mistral} \\      & 7B    & 13B    & 7B    & 13B   
          & 8B    & 8B-Inst
           & 7B    & 7B-Inst \\
           \hline
$\alpha$       & row sum & to mean & F norm & column sum & to mean & relative sum & row sum & relative sum  \\
$\beta$   & relative sum &  F norm & to sum & relative sum & to sum & no coe & no coe & to mean  \\
$\tau_{1}$  & no op & no op  & no op & square & square & no op & square & square   \\
$\tau_{2}$  & sqrt & sqrt  & sqrt & sqrt & sqrt & no op & no op & no op   \\
\bottomrule
\end{tabular}%
}
\caption{Optimal coefficients and operations for pruning metrics on GSM8K calibration data. }
\label{tab:gsm8k-metric}
\end{table*}

\paragraph{Number of search trials.} In Figure \ref{fig:search_param}(c), we investigate the influence of varying the number of trials on the performance of the NSGA-III search algorithm. The trials evaluated range from an initial count of 0 up to a maximum of 500. The results reveals that the perplexity stabilizes and reaches an optimal value at the point where the number of search trials is set to 350. Based on this empirical evidence, we select this specific number of trials for the NSGA-III algorithm in our experiments discussed in the main paper. 


\subsection{Optimal Pruning Metric and Layerwise Sparsity Ratios}
\label{sec:appendix_coe}
Our meta pruning metric adjusts the relationship between weight and activation magnitudes by applying specific coefficients and operations to both weight and activation magnitudes. 
The operation sets include (1) no op, which leaves the matrix unchanged, (2) sqrt, which computes the square root of each matrix element, and (3) square, which raises each element to the power of two.  
The coefficient sets include (1) no coe, which leaves the scaling of the matrix elements unchanged, (2) F norm, using the reciprocal of the Frobenius norm of the matrix, (3) to sum, and (4) to mean, setting the coefficients as the reciprocal of the total sum and the average of the matrix elements, respectively. (5) row sum and (6) column sum, using the reciprocal of the sums of specific rows or columns, respectively. Finally, (7) relative sum calculates coefficients as the sum of the row sums and column sums for each matrix element. 
The detailed calculation equations are illustrated in Table \ref{tab:coe_eqs}, using matrix $A=A_{ij}$ with $m$ rows and $n$ columns as input for demonstration.

The detailed calculations for the coefficient sets utilized in our pruning metric are comprehensively illustrated in Table \ref{tab:coe_eqs}. For these calculations, we use a matrix 
$A=A_{ij}$ that consists of $m$ rows and $n$ columns as input demonstration.





\paragraph{Optimal pruning metrics.}

In Table \ref{tab:zero-metrics}, we present the optimal coefficients and operations for pruning metrics using samples from the C4 dataset as calibration data. Table \ref{tab:gsm8k-metric} displays the optimal coefficients and operations for pruning metrics using samples from the GSM8K dataset as calibration data. Compared to the results based on the C4 dataset, the metrics derived from the GSM8K dataset show a greater divergence from RIA metric \citep{zhangplug}. Notably, most of these metrics do not incorporate the relative sum as a weight coefficient.




\paragraph{Optimal layerwise pruning ratio.}
In Figure \ref{fig:layerwise_ratio}, we report the optimal layerwise sparsity ratios for LLaMA-1/2/3 and Mistral models. The results generally indicate that the upper layers contain more redundant parameters compared to the lower layers, as higher sparsity ratios are more common in the top layers, while lower sparsity ratios are more frequent in the lower layers. 

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/prune_ratio.pdf}
  \caption{Searched layerwise sparsity ratios for LLaMA-1/2/3 and Mistral models.}
  \label{fig:layerwise_ratio}
\end{figure*}

\subsection{Relationship between Transformed Wights and Activations}
\label{sec:w_x}
In our analysis of the optimal searched pruning metrics, We find that the differences between transformed weights and transformed activations may affect the effectiveness of different pruning metrics. Specifically, we analyze each pruning metric, such as Wanda, RIA, and OptiShear, by decomposing them into two distinct components: the transformed weights and the transformed activations, each defined by specific coefficients or operations.
As the SparseGPT metric combines weights and the Hessian matrix, and the Wanda metric serves as a simpler approximation of the SparseGPT metric. Due to this relationship, we omit the weight and activation analysis for SparseGPT.


We measure the difference between transformed weights and activations as the layer-wise absolute difference, which is calculated by summing the average absolute differences across all linear sub-modules in each layer. We report the average layer-wise differences between the operated weights and the operated activations across the Wanda, RIA, and OptiShear pruning metrics in Table \ref{tab:average_difference}, with detailed layer-wise difference curves available in Figures \ref{fig:llama2_7b_distance}, \ref{fig:llama2_13b_distance}, \ref{fig:llama3_8b_distance}, 
\ref{fig:mistral_7b_distance}.

\begin{table*}[htbp]
    \centering
    \centering
    \setlength{\tabcolsep}{22pt} % Adjusts the padding between columns
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccc}
        \toprule
        Method & LLaMA-2 7B & LLaMA-2 13B & LLaMA-3 8B & Mistral 7B \\
        \midrule
        Wanda  & 82.66  & 78.30  & 79.31  & 392.13 \\
        RIA    & 22.34  & 21.91  & 21.15  & 44.77  \\
        OptiShear  & 1.09   & 0.0001 & 0.1263 & 0.0304 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Average absolute difference between operated weights and operated activations for Wanda, RIA and OptiShear on C4 Calibration Data.}
    \label{tab:average_difference}
\end{table*}

Table \ref{tab:average_difference} shows that the RIA pruning metric reduces the absolute difference compared to Wanda, while the OptiShear searched metric further minimizes this difference, bringing it close to zero. The weighted transformation operation in the OptiShear pruning metric effectively scales both weights and activations into a similar numerical range, facilitating a balanced evaluation of each weight relative to its corresponding activation.

Coupled with the performance results of each pruning metric presented in Table \ref{tab:all_res}, the difference analysis in Table \ref{tab:average_difference} suggests that pruning metrics with smaller absolute differences between transformed weights and activations are more likely to achieve effective pruning. Thus, the performance of Wanda and other methods may be influenced by how well they account for these differences regarding different models with different weight magnitudes and distributions.

\begin{figure*}[htbp]
    \centering
    \begin{minipage}{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llama2_wanda_distance.png}
    \end{minipage}\hfill
    \begin{minipage}{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llama2_ria_distance.png} 
    \end{minipage}\hfill
    \begin{minipage}{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llama2_mecon_distance.png} 
    \end{minipage}
    \caption{Layerwise absolute distance between transformed weights and transformed activations for Wanda, RIA, and OptiShear metrics on LLaMA-2 7B models.}
    \label{fig:llama2_7b_distance}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \begin{minipage}{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llama2_13b_wanda_distance.png}
    \end{minipage}\hfill
    \begin{minipage}{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llama2_13b_ria_distance.png} 
    \end{minipage}\hfill
    \begin{minipage}{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llama2_13b_mecon_distance.png} 
    \end{minipage}
    \caption{Layerwise absolute distance between transformed weights and transformed activations for Wanda, RIA, and OptiShear metrics on LLaMA-2 13B models.}
    \label{fig:llama2_13b_distance}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \begin{minipage}{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llama3_8b_wanda_distance.png}
    \end{minipage}\hfill
    \begin{minipage}{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llama3_8b_ria_distance.png} 
    \end{minipage}\hfill
    \begin{minipage}{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llama3_8b_mecon_distance.png} 
    \end{minipage}
    \caption{Layerwise absolute distance between transformed weights and transformed activations for Wanda, RIA, and OptiShear metrics on LLaMA-3 8B models.}
    \label{fig:llama3_8b_distance}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \begin{minipage}{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/mistral_7b_wanda_distance.png}
    \end{minipage}\hfill
    \begin{minipage}{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/mistral_ria_distance.png} 
    \end{minipage}\hfill
    \begin{minipage}{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/mistral_mecon_distance.png} 
    \end{minipage}
    \caption{Layerwise absolute distance between transformed weights and transformed activations for Wanda, RIA, and OptiShear metrics on Mistral 7B models.}
    \label{fig:mistral_7b_distance}
\end{figure*}


\subsection{Task-wise Results on LM Harness}
\label{appendix:lm-harness}
For LM-harness results, the 7 evaluated zero-shot tasks are: BoolQ \citep{clark2019boolq},
RTE \citep{wang2018glue}, HellaSwag \citep{zellers2019hellaswag}, WinoGrande \citep{sakaguchi2021winogrande}, ARC Easy and Challenge \citep{clark2018think}, and OpenbookQA \citep{mihaylov2018can}. For
reproducibility, we used v0.4.0 release. All tasks were evaluated on task version 0 except for BoolQ on task version 1. We show the task-wise performance of mean zero-shot accuracies of pruned LLaMA-1/2/3 and Mistral models in Tables \ref{tab:zero-1-7}, \ref{tab:zero-1-13}, \ref{tab:zero-2-7}, \ref{tab:zero-2-13}, \ref{tab:zero-3-8}, \ref{tab:zero-3-8-it}, \ref{tab:zero-m-1}, \ref{tab:zero-m-2}. 

\begin{table*}[htbp]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccccc}
         \toprule
         Method & BoolQ & RTE & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average\\
         \hline
         Dense & 75.06 & 66.23 & 56.93 & 69.54 & 74.82 & 41.02 & 34.30 & 59.70\\
         Magnitude & 55.10 & 54.51 & 45.49 & 59.10 & 58.65 & 32.97 & 22.40 & 46.89 \\
         SparseGPT & 72.03 & 54.15 & 51.43 & 67.87 & 71.39 & 37.54 & 29.60 & 54.86 \\
         Wanda & 71.04 & 54.51 & 51.93 & 65.90 & 69.40 & \textbf{36.95} & 28.80 & 54.08\\
         RIA & 72.84 & 57.76 & 51.93 & 66.85 & \textbf{70.50} & 36.43 & 29.40 & 55.10\\
         Pruner-Zero & 70.28 & 56.68 & 47.27 & 64.96 & 66.92 & 33.25 & 26.80 & 52.31\\
         \hdashline
         Our Metric & \textbf{72.87} & 57.40 & 51.91 & \textbf{67.25} & 70.33 & 36.35 & 29.60 & 55.10\\
         GSM8K Metric & 71.04 & \textbf{58.48} & 52.39 & 67.17 & 69.91 & 37.46 & \textbf{30.20} & \textbf{55.24}\\
         LLaMA2 Metric & 70.73 & 57.63 & \textbf{53.24} & 67.01 & 70.24 & 37.97 & \textbf{30.20} & 55.15\\
         \bottomrule
    \end{tabular}
    }
    \caption{Accuracies (\%) of LLaMA-1 7B model for 7 zero-shot tasks with unstructured 50\% sparsity.}
    \label{tab:zero-1-7}
\end{table*}


\begin{table*}[htbp]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccccc}
         \toprule
         Method & BoolQ & RTE & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average\\
         \hline
         Dense & 78.03 & 70.51 & 59.63 & 72.89 & 77.28 & 46.55 & 33.20 & 62.58\\
         Magnitude & 55.19 & 52.23 & 43.65 & 63.36 & 57.82 & 32.53 & 26.60 & 47.34 \\
         SparseGPT & \textbf{76.89} & 60.95 & 54.99 & 71.46 & 72.15 & 42.17 & 31.20 & 58.54 \\
         Wanda & 75.73 & 62.48 & 55.70 & 71.68 & 72.91 & 43.45 & 32.20 & 59.18\\
         RIA & 76.44 & 62.34 & 56.13 & 72.73 & 72.42 & \textbf{43.87} & 32.20 & 59.45\\
         Pruner-Zero & 73.91 & 62.36 & 52.65 & 69.41 & 70.83 & 41.62 & 28.80 & 57.08\\
         \hdashline
         Our Metric & 76.67 & 62.45 & 56.11 & \textbf{73.63} & 73.25 & 43.62 & \textbf{32.40} & \textbf{59.73}\\
         GSM8K Metric & 76.62 & \textbf{62.89} & 55.48 & 72.79 & 72.58 & 43.78 & 32.00 & 59.45\\
         LLaMA2 Metric & 76.51 & 62.32 & \textbf{56.43} & 71.82 & \textbf{73.39} & 43.84 & \textbf{32.40} & 59.53\\
         \bottomrule
    \end{tabular}
    }
    \caption{Accuracies (\%) of LLaMA-1 13B model for 7 zero-shot tasks with unstructured 50\% sparsity.}
    \label{tab:zero-1-13}
\end{table*}


\begin{table*}[htbp]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccccc}
         \toprule
         Method & BoolQ & RTE & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average\\
         \hline
         Dense & 77.74 & 62.82 & 57.14 & 69.14 & 76.35 & 43.43 & 31.40 & 59.72\\
         Magnitude & 62.57 & 52.35 & 52.99 & 65.35 & 67.97 & 37.20 & 28.40 & 52.40 \\
         SparseGPT & \textbf{75.78} & 57.75 & 52.90 & \textbf{69.14} & 71.34 & 37.97 & 26.60 & 55.90 \\
         Wanda & 75.35 & 53.43 & 52.63 & 67.25 & \textbf{72.35} & 39.42 & 30.80 & 55.89\\
         RIA & 75.66 & 53.79 & 52.25 & 67.25 & 72.05 & 37.71 & \textbf{31.00} & 55.67\\
         Pruner-Zero & 73.48 & 53.29 & 49.18 & 65.83 & 69.92 & 38.36 & 26.60 & 53.81\\
         \hdashline
         Our Metric & 74.62 & \textbf{62.82} & \textbf{57.14} & 68.03 & 71.00 & 38.91 & 29.80 & \textbf{57.47}\\
         GSM8K Metric & 75.11 & 53.79 & 53.55 & 67.25 & 72.31 & \textbf{39.93} & 30.40 & 56.05\\
         LLaMA2 Metric & 75.11 & 53.79 & 53.55 & 67.25 & 72.31 & \textbf{39.93} & 30.40 & 56.05\\
         \bottomrule
    \end{tabular}
    }
    \caption{Accuracies (\%) of LLaMA-2 7B model for 7 zero-shot tasks with unstructured 50\% sparsity.}
    \label{tab:zero-2-7}
\end{table*}

\begin{table*}[htbp]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccccc}
         \toprule
         Method & BoolQ & RTE & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average\\
         \hline
         Dense & 80.52 & 65.34 & 60.33 & 71.95 & 79.38 & 48.47 & 35.20 & 63.03\\
         Magnitude & 57.62 & 55.87 & 54.53 & 65.85 & 70.47 & 38.13 & 27.80 & 52.90 \\
         SparseGPT & 81.42 & 65.26 & 55.83 & 72.64 & 74.91 & 42.23 & \textbf{32.60} & 60.70 \\
         Wanda & 81.86 & 64.08 & 56.92 & 71.37 & 76.12 & 43.81 & 32.00 & 60.88\\
         RIA & \textbf{81.93} & 64.02 & 57.73 & 71.89 & 76.24 & 43.46 & 32.00 & 61.03\\
         Pruner-Zero & 77.86 & 61.22 & 56.89 & 67.90 & 74.16 & 39.81 & 29.40 & 58.18\\
         \hdashline
         Our Metric & 80.97 & \textbf{66.17} & 59.68 & \textbf{72.35} & 76.29 & 43.68 & 30.80 & 61.42\\
         GSM8K Metric & 81.56 & 64.06 & 58.41 & 72.23 & 76.98 & 43.73 & 32.00 & 61.28\\
         LLaMA2 Metric & 80.25 & 66.14 & \textbf{59.73} & 71.57 & \textbf{77.36} & \textbf{43.85} & 32.00 & \textbf{61.56}\\
         \bottomrule
    \end{tabular}
    }
    \caption{Accuracies (\%) of LLaMA-2 13B model for 7 zero-shot tasks with unstructured 50\% sparsity.}
    \label{tab:zero-2-13}
\end{table*}


\begin{table*}[htbp]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccccc}
         \toprule
         Method & BoolQ & RTE & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average\\
         \hline
         Dense & 81.44 & 69.68 & 60.17 & 72.85 & 80.09 & 50.43 & 34.80 & 64.21\\
         Magnitude & 49.14 & 53.43 & 38.55 & 55.09 & 60.69 & 32.42 & 24.80 & 44.87 \\
         SparseGPT & 74.80 & \textbf{54.15} & \textbf{49.90} & 68.35 & 67.05 & 36.43 & 26.40 & 53.87 \\
         Wanda & 73.43 & 52.71 & 41.80 & 63.22 & 64.86 & 29.78 & 21.80 & 49.66 \\
         RIA & 75.20 & 53.12 & 43.00 & 64.56 & 65.87 & 30.55 & 23.00 & 50.76 \\
         Pruner-Zero & 72.32 & 54.51 & 45.78 & 65.19 & 70.58 & 35.41 & 23.60 & 52.48 \\
         \hdashline
         Our Metric & \textbf{79.54} & 53.07 & 43.24 & \textbf{70.24} & \textbf{72.05} & \textbf{41.13} & \textbf{29.20} & 55.50 \\
         GSM8K Metric & 73.88 & \textbf{63.90} & 49.68 & 68.90 & 70.37 & 37.80 & 24.60 & \textbf{55.59} \\
         LLaMA3 Metric & 73.88 & \textbf{63.90} & 49.68 & 68.90 & 70.37 & 37.80 & 24.60 & \textbf{55.59} \\
         \bottomrule
    \end{tabular}
    }
    \caption{Accuracies (\%) of LLaMA-3 8B model for 7 zero-shot tasks with unstructured 50\% sparsity.}
    \label{tab:zero-3-8}
\end{table*}

\begin{table*}[htbp]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccccc}
         \toprule
         Method & BoolQ & RTE & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average\\
         \hline
         Dense & 83.06 & 67.51 & 57.68 & 71.98 & 81.61 & 52.99 & 34.20 & 64.15\\
         Magnitude & 68.84 & 60.65 & 36.31 & 53.75 & 49.83 & 26.19 & 21.80 & 45.31 \\
         SparseGPT & 77.00 & 60.65 & \textbf{49.61} & 66.46 & 70.92 & 40.19 & 26.40 & 55.89 \\
         Wanda & 76.57 & 54.51 & 41.18 & 63.61 & 67.63 & 33.70 & 22.20 & 51.34 \\
         RIA & 78.17 & 54.51 & 42.29 & 64.25 & 68.35 & 34.13 & 22.80 & 50.64 \\
         Pruner-Zero & 76.88 & 54.51 & 45.32 & 65.67 & 69.44 & 36.95 & 25.00 & 55.60 \\
         \hdashline
         Our Metric & \textbf{81.56} & 54.15  & 42.32 & \textbf{68.11} & \textbf{74.28} & \textbf{41.55} & \textbf{29.60} & \textbf{55.94}\\
         GSM8K Metric & 78.17 & 54.51 & 42.29 & 64.25 & 68.35 & 34.13 & 22.80 & 50.64 \\
         LLaMA3 Metric & 76.82 & \textbf{62.45} & 48.18 & 66.30 & 71.34 & 39.59 & 26.80 & 55.93 \\
         \bottomrule
    \end{tabular}
    }
    \caption{Accuracies (\%) of Instruction-tuned LLaMA-3 8B model for 7 zero-shot tasks with unstructured 50\% sparsity.}
    \label{tab:zero-3-8-it}
\end{table*}

\begin{table*}[htbp]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccccc}
         \toprule
         Method & BoolQ & RTE & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average\\
         \hline
         Dense & 81.44 & 69.68 & 60.17 & 72.85 & 80.09 & 50.43 & 34.80 & 64.21\\
         Magnitude & 75.87 & 55.60 & 56.74 & 68.35 & 74.20 & 42.15 & 27.80 & 57.24 \\
         SparseGPT & 76.73 & \textbf{61.01} & 54.52 & 67.72 & 74.24 & 41.64 & 26.60 & 57.49 \\
         Wanda & 76.12 & 55.60 & 48.95 & 65.59 & 72.69 & 37.46 & 23.00 & 54.20\\
         RIA & 76.48 & 56.68 & 49.05 & 66.30 & 72.47 & 37.12 & 22.60 & 54.39 \\
         Pruner-Zero & 77.46 & 60.65 & 50.25 & 68.90 & 71.84 & 37.46 & 22.40 & 55.57 \\
         \hdashline
         Our Metric & \textbf{82.35} & 56.68 & 55.77 & \textbf{70.88} & \textbf{76.18} & \textbf{45.22} & 28.22 & \textbf{59.33} \\
         GSM8K Metric & 81.53 & 55.60 & 54.43 & 69.38 & 74.16 & 42.15 & 26.40 & 57.66 \\
         LLaMA3 Metric & 80.52 & 56.32 & \textbf{55.94} & 69.53 & 75.00 & 42.41 & \textbf{28.40} & 58.30 \\
         \bottomrule
    \end{tabular}
    }
    \caption{Accuracies (\%) of Mistral 7B model for 7 zero-shot tasks with unstructured 50\% sparsity.}
    \label{tab:zero-m-1}

    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccccc}
         \toprule
         Method & BoolQ & RTE & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average\\
         \hline
         Dense & 83.06 & 67.51 & 57.68 & 71.98 & 81.61 & 52.99 & 34.20 & 64.15\\
         Magnitude & 79.09 & 65.06 & 59.31 & 67.43 & 77.96 & 49.77 & 31.80 & 62.34 \\
         SparseGPT & 81.56 & \textbf{72.92} & 58.77 & 70.01 & 76.85 & 48.72 & 28.40 & 62.46 \\
         Wanda & 83.73 & 66.79 & 55.68 & 67.48 & 77.06 & 48.12 & 28.40 & 61.04\\
         RIA & 83.88 & 66.79 & 55.61 & 67.32 & 77.78 & 47.95 & 27.60 & 60.48 \\
         Pruner-Zero & 83.18 & 68.95 & 56.17 & 68.27 & 76.43 & 47.44 & 29.40 & 61.41 \\
         \hdashline
         Our Metric & \textbf{84.40} & 66.79 & 58.75 & \textbf{70.24} & \textbf{80.13} & \textbf{51.45} & \textbf{32.80} & \textbf{63.51} \\
         GSM8K Metric & 84.59 & 67.87 & 58.97 & 68.90 & 78.11 & 51.11 & 31.80 & 63.05 \\
         LLaMA3 Metric & 84.13 & 66.06 & \textbf{59.87} & 69.14 & 78.79 & 51.37 & 32.00 & 63.05 \\
         \bottomrule
    \end{tabular}
    }
    \caption{Accuracies (\%) of Instruction-tuned Mistral 7B model for 7 zero-shot tasks with unstructured 50\% sparsity.}
    \label{tab:zero-m-2}
\end{table*}