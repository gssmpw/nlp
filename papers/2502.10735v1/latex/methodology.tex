\section{\textsc{OptiShear}}
\label{method}
\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\textwidth]{figures/model.pdf}
  \caption{\textbf{Illustration of our proposed \textsc{OptiShear} method.} \textsc{OptiShear} consists of two phases: searching for the optimal pruning metric and the optimal layerwise sparsity ratios. 
  % We present the distribution of metric scores for the optimal metric searched in the LLaMA2-7B model, along with the sparsity ratio for each layer in the right column. 
  % Typically, higher sparsity ratios are common in the upper layers, and lower ratios are more prevalent in the lower layers. 
  }
  % \vspace{-5mm}
  \label{fig:model}
\end{figure*}

% \vspace{-0.25em}

\begin{table*}[t!]
\small
\centering
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{l|l}
\toprule
coefficient candidates for $\alpha, \beta$ & no coe, F norm, to sum, to mean, row sum, column sum, relative sum \\
operation candidates for $F_{1}, F_{2}$ & no op, sqrt, square, sigmoid, softmax, exp, log   \\
\bottomrule
\end{tabular}%
}
\caption{Predefined coefficient and operation candidates for Meta Pruning Metric.}
\vspace{-8pt}
\label{coe-ope}
\end{table*}

\subsection{Method Overview}
% \textsc{OptiShear} focuses on the essential adaption of pruning strategy across different LLMs. As shown in Figure \ref{fig:model}, \textsc{OptiShear} automatically searches for the adaptive pruning strategy through evolutionary optimization, optimizing both the pruning metric and layerwise sparsity ratios. 
\textsc{OptiShear} adapts pruning strategies to different LLMs through evolutionary optimization, automatically determining optimal pruning metrics and layer-specific sparsity ratios (Figure \ref{fig:model}).
% to prune the model into the pre-defined sparsity ratio while preserving the highest performance of its dense counterpart.
% Our OptiShear framework automatically refines the post-training pruning strategy using evolutionary optimization, including optimization of both the pruning metric and the layerwise sparsity ratios, as shown in Figure\ref{fig:model}, to prune the model into the pre-defined sparsity ratio while preserving the highest performance of its dense counterpart.  
% Specifically, as shown in Figure\ref{fig:model}, it includes two phases, Pruning Metric Search and Layerwise Sparsity Ratio Search, aiming to prune the model into the pre-defined sparsity ratio while preserving the highest performance of its dense counterpart. 
% In each phase of the search, \textsc{OptiShear} iteratively samples pruning metrics or layerwise sparsity ratios from a predefined search space. Each sample is then evaluated, producing a numerical measure of its quality. The evaluation results are fed back into the search algorithm to improve future samplings. The details of these two phases are as follows:
During each search phase, \textsc{OptiShear} samples pruning metrics or layerwise sparsity ratios from a predefined search space and evaluates their effectiveness. These evaluation results guide subsequent sampling decisions. 
1) \textbf{Pruning Metric Search} - involves identifying the most effective metric for scoring the importance of model weights. 
% Similar to Wanda \citep{sun2023simple}, we compare the weights on a per-output basis, where weight importance is assessed locally within each output neuron.
2) \textbf{Layerwise Sparsity Ratio Search} -  determines the optimal proportion of weights to remove from each layer using the identified pruning metric.
% determines the optimal sparsity ratios across different model layers. Using the identified pruning metric, we remove less important weights according to layer-specific sparsity ratio.
% \vspace{-0.4em}
% In Section \ref{sec:search space}, we detail the Search Space for each phase in OptiShear, which includes the range of possible pruning metrics and sparsity ratios that can be explored. Section \ref{sec:evaluation measurement} discusses the Evaluation Measurement, outlining how to evaluate the sampled pruning metric or layerwise sparsity ratios efficiently. Finally, in Section \ref{sec:search algorithm}, we describe the Search Algorithm, detailing how we employ an evolutionary algorithm to identify the optimal pruning metric and layerwise sparsity ratio.
% Section \ref{sec:search space} describes Search Space, encompassing the range of pruning metrics and sparsity ratios explored by \textsc{OptiShear}. Section \ref{sec:evaluation measurement} details Evaluation Measurement methodology for efficiently assessing sampled configurations. Section \ref{sec:search algorithm} presents Evolutionary Search Algorithm for identifying optimal pruning metrics and layerwise sparsity ratios.

% \vspace{-0.25em}
\subsection{Search Space}
\label{sec:search space}
% \vspace{-0.25em}
\paragraph{Meta Pruning Metric.}
% The pruning metric identifies the importance of weights in preserving model performance. 
Recent studies have revealed emergent properties in LLMs, including large weight magnitudes \citep{puccetti2022outliers, wei2022outlier, dettmers2022gpt3} and massive input activations \citep{sun2024massive}. Building on these findings, researchers \citep{sun2023simple, zhangplug} have improved pruning effectiveness by combining weight magnitudes with input activations. We extend this approach by proposing a meta pruning metric that dynamically balances weights and activations based on each LLM's weight distribution.
Equation \ref{eq:meta_metric} defines our scoring mechanism for each weight $W_{ij}$ using a weighted combination of weight magnitude and input activation:
\begin{equation}
    S_{ij} = \alpha F_{1} (|W_{ij}|) \cdot \beta F_{2} (\|X_{j}\|_{2}).
\label{eq:meta_metric}
\end{equation}
The score combines weight magnitude $|W_{ij}|$ and input activation norm $|X_{j}|_{2}$ (calculated as the $l_2$ norm of feature $j$ across tokens) through coefficients ($\alpha$, $\beta$) and transformation functions ($F_1$, $F_2$). Table \ref{coe-ope} lists the candidate coefficients and operations, with detailed calculations provided in Appendix \ref{sec:appendix_coe}.


% As shown in Equation \ref{eq:meta_metric}, we score each weight $W_{ij}$ by applying a weighted transformation to the element-wise product between the weight magnitude $|W_{ij}|$ and the norm of input activations $\|X_{j}\|_{2}$.  
% The weighted transformation is conducted by specific coefficients ($\alpha$, $\beta$) and operations ($F_1$, $F_2$).

% Here, we use the absolute value of weights $|W_{ij}|$ to calculate weight magnitudes and $\|X_{j}\|_{2}$ to measure the norm of input activations, which computes the $l_2$ norm of the $j_{th}$ features aggregated across different tokens.
% The predefined coefficients and operation candidates are listed in Table \ref{coe-ope}, with further details and calculation equations provided in Appendix \ref{sec:appendix_coe}.

% Notably, our meta pruning metric is able to encompasses and extends beyond existing pruning metrics like Wanda and RIA.
% For instance, the Wanda metric \citep{sun2023simple}, expressed as 
% \begin{equation}
%     S_{ij}=|W_{ij}|\cdot\|X_{j}\|_{2},
% \end{equation}
% % $S_{ij}=|W_{ij}|\cdot\|X_{j}\|_{2}$, 
% does not set coefficients and operations, providing a uniform weighting between weight magnitude and norm of input activations. 
% Building on this, the RIA metric \citep{zhangplug}, denoted as 
% \begin{equation}
%     S_{ij}=\rm{RI}(|W_{ij}|)\cdot\|X_{j}\|_{2}^{1/2}, 
% \end{equation}
% % $S_{ij}=\rm{RI}(|W_{ij}|)\cdot\|X_{j}\|_{2}$, 
% modifies the coefficient of the weight magnitude as relative sum $\rm{RI}$, and sets the operation of the input activation norm as a square function.

Our meta pruning metric generalizes existing approaches like Wanda and RIA. Wanda \citep{sun2023simple} uses a direct product of weight magnitude and input activation norm: $S_{ij}=|W_{ij}|\cdot\|X_{j}\|_{2}$,
RIA \citep{zhangplug} introduces relative importance (RI) for weights and applies a square root to the activation norm:
$S_{ij}=\rm{RI}(|W_{ij}|)\cdot\|X_{j}\|_{2}^{1/2}$, 
Our framework extends these metrics by allowing flexible coefficients and transformations for both terms, enabling more adaptive pruning strategies.



% \vspace{-0.5em}
\paragraph{Layerwise Sparsity Ratios.}

Research has shown that neurons in different layers of Transformer architectures capture distinct types of information \citep{wang2020rethinking, zhang2021moefication}. We leverage this insight by applying non-uniform pruning across layers: layers containing more critical neurons receive lower pruning ratios, while those with less essential neurons undergo more aggressive pruning.
% Specifically, we identify the optimal sparsity ratio for each layer by selecting from a predefined sparsity ratio set, including \texttt{target sparsity - sparsity step}, \texttt{target sparsity} and \texttt{target sparsity + sparsity step}. Here, target sparsity is the pre-defined sparsity ratio for pruning the overall model. The sparsity step allows for adjustments to achieve slightly higher or lower sparsity ratios, facilitating non-uniform sparsity across different layers.
% We empirically find that for LLMs with more than 32 layers, using a discrete set of three sparsity ratios outperforms larger sets when searching within a limited number of trials.
We determine each layer's sparsity ratio from three options: {target sparsity - sparsity step}, {target sparsity}, or {target sparsity + sparsity step}. The target sparsity represents the model's overall pruning goal, while the sparsity step enables fine-tuned adjustments for layer-specific pruning. For LLMs with over 32 layers, our experiments show that using these three discrete sparsity options yields better results than larger sets when operating under limited search trials.

\subsection{Search Evaluation.}
\label{sec:evaluation measurement}
% \vspace{-0.3em}
% Search Evaluation measures each sampling from the search space, thus guiding the evolutionary process toward finding the optimal pruning strategy \citep{back1993overview}. 
% The primary goal of model pruning is to remove a subset of network weights while aiming to preserve performance \citep{lecun1989optimal, han2015learning}. Following this goal, \textsc{OptiShear} leverages model-wise reconstruction error to evaluate each sampled pruning metric. Furthermore, we introduce a secondary measurement to assess the overall sparsity ratio of the pruned model to evaluate sampled layerwise sparsity ratios.

The Search Evaluation phase guides the evolutionary process by assessing each candidate pruning strategy \citep{back1993overview}. Aligned with the fundamental goal of model pruning—removing weights while maintaining performance \citep{lecun1989optimal, han2015learning}—\textsc{OptiShear} employs two evaluation criteria: primary model-wise reconstruction error to assess pruning metric effectiveness, and secondary overall sparsity measurement to evaluate the layerwise sparsity ratios.

\vspace{-0.5em}
\paragraph{Model-wise Reconstruction Error.}
% We propose a lightweight search evaluation, model-wise reconstruction error, for the sampled pruning strategy. Existing automatic framework, like Pruner-Zero \citep{dongpruner}, use perplexity as the evaluation measure. However, we demonstrate that using perplexity requires more evaluation time and tends to generalize poorly across different downstream tasks. Toward that end, the proposed model-wise reconstruction error admits a faster evaluation of each search trial while preserving generalizability.
% by quantifying the difference between the final layer output of the pruned model from its dense counterpart.  
We introduce model-wise reconstruction error as a lightweight search evaluation metric for pruning strategies. While existing frameworks like Pruner-Zero \citep{dongpruner} use perplexity for evaluation, our experiments show that perplexity-based evaluation is more time-consuming and exhibits poorer generalization across downstream tasks. Our proposed reconstruction error metric enables faster strategy evaluation while maintaining strong generalization capabilities.
The model-wise reconstruction error $f_{rec}$ quantifies the output discrepancy between the dense model $\theta$ and pruned model $\theta^*$ at the final layer. 
Given final layer input activations $X_{l}$, weight matrix $W_{l}\in \mathbb{R}^{r\times c}$ with $r$ output and $c$ input channels, and layer-specific sparsity masks ${M_{i}}$ derived from importance scores, the reconstruction error is computed as:
\begin{equation}
    f_{rec} (\theta, \theta^{*}) = \| W_{l}X_{l} - ({M}_{l} \odot W_{l})\cdot X_{l}\|_{Frob},
    \label{eq:norm}
\end{equation}
where $|\cdot|_{Frob}$ denotes the Frobenius norm \citep{golub1996matrix}, ensuring the pruned model's outputs closely match the dense model's outputs.

% Mathematically, the model-wise reconstruction error, denoted as $f_{rec}$, measures the discrepancy norm in the final layer outputs between the dense model $\theta$ and the pruned model $\theta^*$. Specifically, we denote the input activations for the final layer $l$ as $X_{l}$, and weight with $r$ output channels and $c$ input channels as $W_{l}\in \mathbb{R}^{r\times c}$. 
% % Considering the importance of weights is measured by a sampled pruning metric, 
% A layerwise sparsity mask ${M_{i}}\in{0, 1}^{r\times c}, i\in\{0, ..., l\}$ removes a certain degree of model weights with lower importance scores, which are measured by the sampled pruning metric. 
% Therefore, the model-wise reconstruction error can be formally expressed as:
% \begin{equation}
%     f_{rec} (\theta, \theta^{*}) = \| W_{l}X_{l} - ({M}_{l} \odot W_{l})\cdot X_{l}\|_{Frob},
%     \label{eq:norm}
% \end{equation}
% where $||\cdot||_{Frob}$ is the Frobenius norm \citep{golub1996matrix}, ensuring the final output of pruned model $\theta^*$ closely matches that of dense model $\theta$.

\paragraph{Sparsity Ratio Discrepancy.}
% Layerwise sparsity search assigns each layer a sampled sparsity ratio which is slightly higher or lower than the pre-defined sparsity ratio. As a result, the overall sparsity of the pruned model may deviate from the pre-defined ratio to prune the dense model. Thus, we introduce a secondary measurement, sparsity ratio discrepancy, to evaluate the numerical difference between the sparsity ratio of the pruned model and the pre-defined sparsity ratio. The sparsity ratio discrepancy $f_{ratio}$ is mathematically defined as:
We introduce sparsity ratio discrepancy as a secondary evaluation measure since the layerwise sparsity search may result in overall model sparsity that deviates from the pre-defined target. This occurs because the search assigns each layer a sampled sparsity ratio that can be slightly higher or lower than the pre-defined value. The sparsity ratio discrepancy $f_{ratio}$ quantifies the difference between the achieved model sparsity and the pre-defined target ratio: 
\begin{equation}
    % \small
    f_{ratio} (\theta, \theta^{*}) = |R_{d} - \frac{\rm{p}(\theta) - \rm{p}(\theta^{*}|\mathcal{R})}{\rm{p}(\theta)}|.
    \label{eq:ratio}
\end{equation}
% Here, $R_{d}$ denotes the pre-defined sparsity ratio for pruning the dense model, and $\mathcal{R}$ is the layerwise sparsity ratios applied to the pruned model $\theta^*$. The function $\rm{parameters}(\cdot)$ measures the total number of parameters in the model.
% Therefore, the sparsity ratio of the pruned model is thus calculated by comparing the number of removed parameters to the total number of parameters in the dense model.
where $R_{d}$ is the pre-defined sparsity ratio, $\mathcal{R}$ represents the layerwise sparsity ratios applied to the pruned model $\theta^*$, and $\rm{p}(\cdot)$ counts the total number of model parameters. The actual sparsity ratio is calculated as the fraction of removed parameters relative to the total parameters in the dense model.


\begin{table*}[t!]
\resizebox{\textwidth}{!}{
\fontsize{9}{11} \selectfont
\begin{tabular}{lccccccccccccc}
\toprule
\multirow{2}{*}{Method} &
  \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Weight\\ Update\end{tabular}} &
  \multirow{2}{*}{Sparsity} &
  \multicolumn{2}{c}{LLaMA-1} & &
  \multicolumn{2}{c}{LLaMA-2} & &
  \multicolumn{2}{c}{LLaMA-3} & &
  \multicolumn{2}{c}{Mistral} \\
  \cline{4-5} \cline{7-8} \cline{10-11} \cline{13-14}
  &  &  & 7B & 13B & & 7B & 13B & & 8B & 8B-Inst & & 7B & 7B-Inst \\ \hline
  \rowcolor{gray!15}
\multicolumn{14}{c}{LM Harness} \\
% \rowcolor{gray!30}
Dense & - & 0\%  & 59.70 & 62.58 & & 59.72 & 63.03 & & 64.21 & 64.15 & & 60.06 & 66.69  \\
\hdashline
Magnitude   & \ding{55} & 50\% & 46.89 &  47.34 & & 52.40 & 52.90 & & 44.87 & 45.31 &  & 57.24 &  63.34  \\
SparseGPT   & $\checkmark$ & 50\% &  54.86 & 58.54  && 55.90 & 60.70 & & 53.87  &  55.89 &  & 57.49 &  62.46  \\
Wanda       & \ding{55} & 50\% & 54.08 & 59.18 & & 55.89 & 60.88 && 49.66 & 51.34 & &  54.20  &  61.04  \\
RIA         & \ding{55} & 50\% &   \textbf{55.10}  &   59.45   & & 55.67 &   61.03 & &  50.76     &  50.64 &  &  54.39   &  60.48  \\
Pruner-Zero        & \ding{55} & 50\% &   52.31  &   57.08   & & 53.81 &   58.18 & &  52.48     &  55.60 &  &  55.57   &  61.41  \\
\hdashline
\textsc{OptiShear} & \ding{55} & 50\% &   \textbf{55.10}    &   \textbf{59.73}  &  &  \textbf{57.47} &  \textbf{61.42}  &  &  \textbf{55.50}     &  \textbf{55.94} & & \textbf{59.33} & \textbf{63.51} \\
\hline
\rowcolor{gray!15}
\multicolumn{14}{c}{WikiText Perplexity} \\
Dense & - & 0\%  & 5.37 & 4.80 & &  5.04 & 4.56 & & 5.80 & 7.91 & & 5.23 & 4.90   \\
\hdashline
Magnitude  & \ding{55} & 50\% & 13.27 & 13.55  & & 11.96 & 6.16 & & 73.93 & 5.5E2 & & 7.14 & 6.59   \\
SparseGPT  & $\checkmark$ & 50\% & 6.92 & 5.87 &  & 6.59 & 5.72 &&  10.89 & 13.27&  & 6.42 & 7.02   \\
Wanda  & \ding{55} & 50\% & 6.90 & 5.82 &  & 6.47 & 5.64 & & 10.57 & 16.37 & & 7.24 & 7.22   \\
RIA & \ding{55} & 50\% & 6.81 & 5.83  & & 6.43 & 5.63 & & 12.56 & 15.57 & & 7.27 & 7.21   \\
Pruner-Zero & \ding{55} & 50\% & 7.13 & 6.02  & & 6.86 & 5.88 & & 12.68 & 15.45 & & 7.84 & 7.50   \\
\hdashline
\textsc{OptiShear} & \ding{55} & 50\% & \textbf{6.78} & \textbf{5.74}   &&  \textbf{6.35} & \textbf{5.51} & & \textbf{9.23} & \textbf{11.37} && \textbf{6.22} &  \textbf{6.55}   \\
\hline
\rowcolor{gray!15}
\multicolumn{14}{c}{GSM8K} \\
Dense & - & 0\%  & 11.07 & 17.82 & & 14.59 & 19.86 & & 52.39 & 74.45 & & 40.11  &  47.76  \\
\hdashline
Magnitude  & \ding{55} & 50\% & 1.52 & 5.99 && 2.05 & 6.22  & & 1.97 & 1.29 &  & 15.53  &  27.37  \\
SparseGPT  & $\checkmark$ & 50\% & 8.19 & 15.60 & & 8.11 & 13.42  & & 21.46 & 49.20 & & 25.40 &  33.97  \\
Wanda & \ding{55} & 50\% & 7.96 & 11.52  & & 7.43  & 9.10   & & 10.16 & 32.68 & & 22.74 &  33.59  \\
RIA & \ding{55} & 50\% & 8.04 & 11.14 & & 7.96 & 9.25  & & 15.85 & 52.39 & & 24.18 &  32.15  \\
Pruner-Zero & \ding{55} & 50\% & 6.41 & 9.22 & & 7.32 & 8.58  & & 17.25 & 43.63 & & 21.16 &  32.24  \\
\hline
\textsc{OptiShear} & \ding{55} & 50\% & 8.14 & 15.37 && 8.13 & 13.79 &  & 41.17 & \textbf{52.39} & & 25.31 & \textbf{35.25}   \\
% Incr vs. non-update  & - & - & +0.10 & +3.85 & &  +0.17  & +4.54 &  &  &  +1.13 & +1.66  \\
\quad w/ eval. & \ding{55} & 50\% & \textbf{8.22} & \textbf{15.62}  & & \textbf{8.47} & \textbf{15.03} & & \textbf{43.07}  & 52.15 & & \textbf{25.78} &   35.14   \\
\hline
\rowcolor{gray!15}
\multicolumn{14}{c}{MMLU} \\
Dense & - & 0\%  & 35.28 & 46.98 & & 41.97 & 51.47 & & 65.23 & 66.35 & & 58.92 & 62.54   \\
\hdashline
Magnitude  & \ding{55} & 50\% & 26.24 & 30.12 & & 26.04 & 43.83 & & 4.36 & 12.03 & & 50.83 & 49.52   \\
SparseGPT  & $\checkmark$ & 50\% & 29.48 & 38.29  & & 33.03 & 47.14 & & 49.50 & 52.27 & & 50.95 & 52.04   \\
Wanda & \ding{55} & 50\% & 29.81 & 37.84 & & 32.09 & 48.06 & & 49.05 & 53.15 & & 53.05 & 53.62   \\
RIA & \ding{55} & 50\% & 30.37 & 37.79 & & 31.46 & 47.39 & & 48.99 & 54.02 & & 52.67 & 53.14   \\
Pruner-Zero & \ding{55} & 50\% & 28.57 & 35.51 & & 30.26 & 45.24 & & 41.39 & 46.32 & & 51.75 & 53.15   \\
\hline
\textsc{OptiShear} & \ding{55} & 50\% & 30.93 & 38.80  & & 32.24 & 48.15 & & 50.65 & 55.11 & & 53.10 & 53.77  \\
\quad w/ eval. & \ding{55} & 50\% & \textbf{31.05} & \textbf{39.76} & & \textbf{33.06} & \textbf{48.38} & & \textbf{51.22} & \textbf{55.60} &  & \textbf{53.87} &   \textbf{54.36}  \\
\bottomrule
\end{tabular}
}
\caption{Mean zero-shot accuracies(\%) on the LM Harness, WikiText perplexity, GSM8K and MMLU accuracies(\%) of pruned LLaMA-1/2/3 and Mistral models.}
\label{tab:all_res}
\vspace{-5mm}
\end{table*}


\subsection{Search Algorithm}
\label{sec:search algorithm}
% Our search begins with an initial population of candidate attention modules and each candidate’s fitness is evaluated based on performance across ViT models. Non-dominated sorting ranks the candidates and assigns them to fronts based on how they compare to others.
% Then. we use crowding distance [48] calculations to maintain diversity within the population. The selection process chooses candidates for the next generation, considering sorting and distance. Genetic operations like crossover and mutation are applied to the selected candidates. This introduces diversity and explores new areas of the search space. The least fit candidates are replaced with offspring, ensuring continuous gains. The algorithm iterates through these steps
% until it meets a termination criterion. During the search process, our algorithm takes FLOPs as the complexity constraint. Finally, we consider the real speed results in selecting final found attention from the Pareto-optimal solutions.

% NSGA-III allows for an accurate approximation of the Pareto front and adapts well to both single and multi-objective problems \citep{seada2015u}. 

%% Shuqi version
%We use the Non-dominated Sorting Genetic Algorithm III (NSGA-III) \citep{deb2013evolutionary, jain2013evolutionary} as our search algorithm, since NSGA-III is capable of handling both single and multi-objective optimization problems. This flexibility allows us to address both pruning metric search and layerwise sparsity ratio search scenarios within a unified framework. For single-objective problems, NSGA-III can effectively explore the search space and converge toward the global optimum. For multi-objective problems, NSGA-III employs non-dominated sorting and crowding distance to maintain diversity and generate a diverse set of Pareto optimal solutions \citep{deb2013evolutionary, jain2013evolutionary}.

%OptiShear follows a two-phase search process: the first phase focuses on finding the optimal pruning metric, while the second phase targets the optimal layerwise sparsity ratios.
%In the first phase of the pruning metric search, we minimize the model-wise reconstruction error $f_{norm}$, illustrated in Eq.\ref{eq:norm}, to identify the optimal coefficients and operations for weight magnitudes and activation norms in the meta pruning metric. This phase presents a single-objective problem where NSGA-III identifies a pruning metric that achieves the minimal model-wise reconstruction error. 
%In the second phase of the layerwise sparsity ratios search, we minimize both the model-wise reconstruction error $f_{norm}$ in Eq.\ref{eq:norm}, to maintain similar functionality between the pruned and dense models, and the sparsity ratio discrepancy $f_{ratio}$ in Eq.\ref{eq:ratio}, to ensure the pruned model's overall sparsity matches the pre-defined sparsity ratio. This phase involves multi-objective optimization, where NSGA-III assists in converging towards diverse Pareto optimal solutions and facilitating the identification of optimal trade-offs between the two objectives.

%% Bowei
% We employ the Non-dominated Sorting Genetic Algorithm III (NSGA-III) \cite{deb2013evolutionary} as our search algorithm. NSGA-III is capable of handling both single and multi-objective optimization problems, making it suitable for addressing both pruning metric search and layerwise sparsity ratio search scenarios within a unified framework. 
We adopt the Non-dominated Sorting Genetic Algorithm III (NSGA-III) \cite{deb2013evolutionary} as our search algorithm due to its ability to handle both single and multi-objective optimization problems, making it suitable for unified pruning metric and layerwise sparsity ratio searches.
% Given a search space $\mathcal{S} = {s_1, s_2, ..., s_n}$ where each $s_i$ represents a candidate pruning strategy or candidate sparsity ratios, we define the objective function $f(s): \mathcal{S} \rightarrow \mathbb{R}$ to be minimized. 
\textsc{OptiShear} operates in two phases: first, it searches for optimal pruning metrics by minimizing the model-wise reconstruction error $f_{\text{rec}}$ (Eq. \ref{eq:norm}). Second, it determines optimal layerwise sparsity ratios by jointly minimizing both $f_{\text{rec}}$ and the sparsity ratio discrepancy $f_{\text{ratio}}$ (Eq. \ref{eq:ratio}).
For the single-objective problem of pruning metric search, we aim to find:
\begin{equation}
    s^* = \text{argmin}_{s \in \mathcal{S}} f(s)
\end{equation}
where $s$ represents a candidate pruning strategy. 
% We define the objective function as $f(s): \mathcal{S} \rightarrow \mathbb{R}$, where $f(s)$ is to be minimized.
For the multi-objective problem of layerwise sparsity ratio search, we define a vector of objective functions $\mathbf{F}(s) = (f_1(s), f_2(s), ..., f_k(s))$. The goal is to find the Pareto optimal set:
\begin{equation}
    \mathcal{S}^* = \{s \in \mathcal{S} \mid \nexists s' \in \mathcal{S} : \mathbf{F}(s') \prec \mathbf{F}(s)\}
\end{equation}
where $\prec$ denotes Pareto dominance. 
% The dominance relation is defined as: A solution $x_1$ dominates $x_2$ ($x_1 \prec x_2$) if and only if:
% \begin{align*}
%     \small
%     &\forall i \in \{1, ..., k\}: f_i(x_1) \leq f_i(x_2) \\
%     & \wedge \exists j \in \{1, ..., k\}: f_j(x_1) < f_j(x_2)
% \end{align*}

% Specifically, \textsc{OptiShear} follows a two-phase search process. In the first phase, we minimize the model-wise reconstruction error $f_{\text{rec}}$ (Eq. \ref{eq:norm}) to find the optimal pruning metrics. During the next phase, we aim to find the optimal layerwise sparsity ratios by minimizing both $f_{\text{rec}}$ and the sparsity ratio discrepancy $f_{\text{ratio}}$ (Eq. \ref{eq:ratio}).
% The detailed process of NSGA-III algorithm is provided in Appendix A.1. 
% The algorithm starts with an initial population $P_0$ and iterates for a fixed number of generations. In each generation, it combines the parent $P_t$ and offspring populations $Q_t$, performs non-dominated sorting to rank solutions, and selects the best solutions to form the next generation $P_{t+1}$. The niching procedure ensures diversity by favoring solutions close to under-represented reference points.
