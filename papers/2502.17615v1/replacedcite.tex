\section{Related Works}
\vspace{-0.2cm}
\noindent \textbf{Eigenvector-based approaches.} PCA has been fundamental to statistical data analysis since its introduction by Pearson in 1901 [cite]. The method was later formalized within a multivariate analysis framework by Hotelling ____, establishing its theoretical foundations. In its classical form, PCA involves computing an empirical covariance matrix from the data, followed by its eigendecomposition. This formulation allows the application of numerous efficient numerical methods, including QR decomposition ____, xLAHQR ____, the Lanczos method ____, and ARPACK ____, some of which are implemented in numerical linear algebra packages such as ScaLAPACK ____. 
These methods are effective but often require complete knowledge of the covariance matrix prior to computation.

\noindent\textbf{Centralized stochastic approaches.} With large datasets, iterative and gradient-based methods for PCA have gained prominence.
Krasulina and Oja \& Karhunen proposed two of the earliest stochastic gradient descent methods for online PCA ____. 
The application of the
least square minimization to the PCA has also received attention ____.
More recently, ____ and ____ have proposed efficient stochastic optimization methods that adapt to the streaming model of data (stochastic) and focus on the theoretical guarantees of gradient-based methods in such non-convex scenarios; see also ____.
Other approaches include manifold methods ____, Frank-Wolfe methods ____, Gauss-Newton methods ____, coordinate descent methods ____, accelerated methods ____, as well as variants of the PCA problem itself ____.
Nevertheless, these methods are primarily designed as centralized algorithms.

\noindent \textbf{Data-parallel distributed approaches.}
Prior distributed PCA approaches span several key directions. One line of work utilizes randomized linear algebra and SVD projections in distributed settings, yielding strong theoretical guarantees ____. For distributed subspace computation, recent methods combine FedAvg with Orthogonal Procrustes Transformations ____. Approaches for computing leading principal components leverage both convex ____ and Riemannian optimization for improved efficiency ____. Notable recent advances include an asynchronous Riemannian gradient method that achieves low computational and communication costs ____. The field has also expanded to address specialized scenarios, including Byzantine-robust computation ____, streaming data analysis ____, shift-and-invert preconditioning ____, and coreset-based approaches ____.

\textbf{Model-parallel distributed approaches.} While most prior work focuses on data-parallel approaches, where each machine computes all principal components using local data, DeepMind's EigenGame ____ introduced a novel model-parallel framework. Their approach reformulates PCA as a collaborative game, where each principal component computation acts as a player maximizing its utility through Riemannian gradient ascent. Though initially presented as a sequential process with proved convergence guarantees, EigenGame extends to a distributed setting where components are optimized simultaneously across machines. While this parallel extension offers practical benefits, its theoretical convergence properties remain unanalyzed, a limitation that persists in subsequent improvements ____.

Our work complements existing literature mostly theoretically, but also practically. By eliminating the requirement for sequential completion of principal components, our algorithmic framework achieves comparable empirical performance to EigenGame ____ on large-scale datasets. Crucially, we establish rigorous convergence guarantees for parallel computation, providing the theoretical foundation that has been missing in existing model-parallel approaches.

\vspace{-0.1cm}