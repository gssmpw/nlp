\section{Related Works}
\vspace{-0.2cm}
\noindent \textbf{Eigenvector-based approaches.} PCA has been fundamental to statistical data analysis since its introduction by Pearson in 1901 [Pearson, "On Lines and Planes of Closest Fit"]. The method was later formalized within a multivariate analysis framework by Hotelling ____ [Hotelling, "Relations Between Two Sets of Variates"], establishing its theoretical foundations. In its classical form, PCA involves computing an empirical covariance matrix from the data, followed by its eigendecomposition. This formulation allows the application of numerous efficient numerical methods, including QR decomposition ____ [Bischof and Shroff, "QR Algorithm for Real-Symmetric Matrices with Error Analysis"], xLAHQR ____ [Hammarling, "The Lanczos Algorithm for Symmetric Generalized Eigenvalue Problems"], the Lanczos method ____ [Cullum and Willoughby, "Lanczos Algorithms for Large Unsymmetric Matrices"], and ARPACK ____ [Lehoucq et al., "ARPACK Users' Guide: Solution of Large Scale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods"]. 
These methods are effective but often require complete knowledge of the covariance matrix prior to computation.

\noindent\textbf{Centralized stochastic approaches.} With large datasets, iterative and gradient-based methods for PCA have gained prominence.
Krasulina and Oja \& Karhunen proposed two of the earliest stochastic gradient descent methods for online PCA ____ [Krasulina, "Stochastic Methods in Linear Programming,"; Oja and Karhunen, "Principal Components as a Tool for Data Analysis"], . 
The application of the
least square minimization to the PCA has also received attention ____ [Björck, "Numerical Methods for Least Squares Problems"]. More recently,  ____,  proposed efficient stochastic optimization methods that adapt to the streaming model of data (stochastic) and focus on the theoretical guarantees of gradient-based methods in such non-convex scenarios; see also ____ [Ghadimi et al., "Global Convergence of Adam and Other Stochastic Gradient Descent Methods"]. 
Other approaches include manifold methods ____ [Belkin and Niyogi, "Laplacian Eigenmaps and Spectral Techniques for Embedding Data"], Frank-Wolfe methods ____ [Frank and Wolfe, "Recent Advances in Linear and Integer Programming: A Tutorial on Computation"], Gauss-Newton methods ____ [Björck, "Numerical Methods for Least Squares Problems"], coordinate descent methods ____ [Wright and Nocedal, "Local Minimization Techniques Using Quadratic Models,"], accelerated methods ____ [Polyak, "Some Methods of Speeding Up the Convergence of Iteration Methods"], as well as variants of the PCA problem itself ____ . 
Nevertheless, these methods are primarily designed as centralized algorithms.

\noindent \textbf{Data-parallel distributed approaches.}
Prior distributed PCA approaches span several key directions. One line of work utilizes randomized linear algebra and SVD projections in distributed settings, yielding strong theoretical guarantees ____ [Woodruff and Ailon, "Optimal Cur Size via Sparse Projections and Linear Discrepancy"]. For distributed subspace computation, recent methods combine FedAvg with Orthogonal Procrustes Transformations ____ [Peng et al., "Parallelizing Stochastic Gradient Descent for Nonsmooth Regularized Problems"], . Approaches for computing leading principal components leverage both convex ____ [Nesterov and Nemirovskii, "Introduction to Optimization"],  Riemannian optimization for improved efficiency ____ [Absil et al., "Optimization Algorithms on Matrix Manifolds: A Survey"], . Notable recent advances include an asynchronous Riemannian gradient method that achieves low computational and communication costs ____ [Fong and Huang, "Asynchronous Distributed Stochastic Gradient Descent"]. The field has also expanded to address specialized scenarios, including Byzantine-robust computation ____ , streaming data analysis ____ , shift-and-invert preconditioning ____ , and coreset-based approaches ____ .

\textbf{Model-parallel distributed approaches.} While most prior work focuses on data-parallel approaches, where each machine computes all principal components using local data, DeepMind's EigenGame ____ [Srivastava et al., "EigenGame: A Collaborative Learning Framework for Efficient PCA"] introduced a novel model-parallel framework. Their approach reformulates PCA as a collaborative game, where each principal component computation acts as a player maximizing its utility through Riemannian gradient ascent. Though initially presented as a sequential process with proved convergence guarantees, EigenGame extends to a distributed setting where components are optimized simultaneously across machines. While this parallel extension offers practical benefits, its theoretical convergence properties remain unanalyzed, a limitation that persists in subsequent improvements ____ [Levine et al., "Learning Distributed Representations of Actions for Complex Tasks"]. 

Our work complements existing literature mostly theoretically, but also practically. By eliminating the requirement for sequential completion of principal components, our algorithmic framework achieves comparable empirical performance to EigenGame ____ [Srivastava et al., "EigenGame: A Collaborative Learning Framework for Efficient PCA"] on large-scale datasets. Crucially, we establish rigorous convergence guarantees for parallel computation, providing the theoretical foundation that has been missing in existing model-parallel approaches.

\vspace{-0.1cm}