\section{Introduction}
\label{sec_introduction}

Deep neural networks (DNNs) have become integral to modern computer vision applications, particularly in autonomous driving, where traffic sign recognition plays a crucial role in decision-making. However, these models remain highly susceptible to adversarial attacks, where subtle, carefully placed perturbations can mislead classification systems~\cite{szegedy2013intriguing, goodfellow2014explaining}. Such attack strategies focus on minimizing the visual perturbation for humans so they are not confused while maximizing the misclassification for machine learning algorithms.

In an orthogonal attack strategy, growing number of adversarial image attacks focuses on exploiting image perturbations that are visible, but not confusing, to humans, while they make machine learning algorithms to fail to correctly recognize images. In particular, researchers have focused on naturally occurring artifacts such as leaves, shadows, and lighting variations that can be exploited to create adversarial perturbations to images, subtly altering a model’s perception without drawing human suspicion~\cite{zhong2022shadows, hsiao2024natural,etim2024fallleafadversarialattack}. These physical attacks are particularly concerning because they can also be executed in natural environments by physically modifying the objects being captured with digital cameras, without requiring direct access to a machine learning model or digital input data.

These attacks highlight the fragility of deep learning models in uncontrolled environments, where even minor environmental changes can cause misclassification. Building upon this idea, we introduce the Snowball Adversarial Attack, a novel physical attack that simulates snow accumulation on traffic signs to induce misclassification. This attack leverages the idea that naturally occurring snowfall or snowballs stuck to a traffic sign are naturally ignored by drivers, yet they can correctly recognize the street signs -- but machine learning models are confused. Unlike traditional adversarial attacks that require small, precisely crafted perturbations, the Snowball Adversarial Attack utilizes a progressive occlusion strategy, where obvious patches of snow are placed at strategic locations over street signs. A search algorithm is utilized to find the best location and is shown to be effective for many street sign types and various snowballs or snow patches.

This attack is particularly dangerous in real-world winter conditions, where snowfall already introduces visual obstructions and reduces the contrast of traffic signs against their backgrounds. Because autonomous vehicles and traffic monitoring systems rely heavily on visual perception, an attacker can exploit this vulnerability by carefully various amounts of snow in key regions of a sign, disrupting classification while maintaining a seemingly natural appearance. This makes the Snowball Adversarial Attack both stealthy and highly effective, as it blends seamlessly into common winter weather conditions, making it difficult to detect even through human oversight.

In addition to presenting the new attack, a secondary contribution of this work is a digital framework used to evaluate the attacks. It makes the evaluation more scalable, but also removes need to physically alter the street signs -- which could be impractical, dangerous or even illegal. The digital framework is used in our evaluation and analysis of the attack's effectiveness.

The Snowball Adversarial Attack poses a significant threat to autonomous vehicles and intelligent transportation systems. Since self-driving cars rely heavily on computer vision-based classifiers for real-time decision-making, an attack that misclassifies a stop sign as a speed limit sign could have severe consequences. Given the stealthy and naturally occurring nature of snow this attack may be hard to detect and there is need for creating more robust traffic sign recognition models.

\subsection{Contributions}

The contributions of this work are as follows:

\begin{enumerate}

    \item We propose the Snowball Adversarial Attack, a novel adversarial attack that leverages snow-like obstructions to manipulate traffic sign classification.
    
    \item We demonstrate that strategic snow placement on specific regions of traffic signs maximizes misclassification rates across different sign types, showing a universal effect across various traffic sign classes.
    
    \item We evaluate the attack’s effectiveness using our digital framework, leveraging real street signs from Street View and various realistic snowballs or patches created using generative machine learning models.

    \item We demonstrate the ability of the attack to cause misclassification of all the street signs we have tested.
    
\end{enumerate}
