\section{Related Work}
\label{sec:related_work}

This section provides an overview of recent work on   adversarial example attacks on traffic sign classification.

 RP2 (Robust Physical Perturbations)~\cite{eykholt2018robust} was a pioneering approach that generated physically realizable adversarial examples capable of misleading deep neural networks under real-world conditions, including changes in lighting and angles. Unlike digital attacks, RP2 optimized perturbations that can be physically applied to traffic signs using carefully designed stickers, ensuring transferability across models while maintaining adversarial effectiveness in diverse environments. Subsequent works such as~\cite{liu2023adversarial} have extended RP2 by leveraging generative models to enhance robustness and stealth, further highlighting the security risks in deep learning-based traffic sign recognition systems.

 In~\cite{zhong2022shadows}, the authors introduced a novel optical adversarial attack that leveraged naturally occurring shadows to generate imperceptible yet effective perturbations. This approach exploited a common natural phenomenon to create stealthy adversarial examples in the physical world. The attack operated under a black-box setting and was evaluated extensively in both simulated and real-world environments, demonstrating its ability to mislead machine learning-based vision systems without drawing human attention. 

 The authors in~\cite{hsiao2024natural} explored the vulnerabilities of machine learning systems introduced by common illumination sources such as sunlight and flashlights. They proposed a novel attack method that simulated these lighting conditions to deceive machine learning models without requiring conspicuous physical modifications. Unlike traditional physical adversarial attacks, their approach employed a model-agnostic black-box attack, leveraging the Zeroth-Order Optimization (ZOO) algorithm~\cite{chen2017zoo} to identify deceptive patterns in a physically realizable space. They showcased the attackâ€™s effectiveness, successfully misleading traffic sign classifiers in both digital and real-world settings.

 In~\cite{etim2024fallleafadversarialattack}, the authors introduced a new class of adversarial attacks that exploited fallen leaves to induce misclassification in street sign recognition systems. Unlike traditional adversarial attacks, these perturbations offer plausible deniability, as a leaf obstructing a sign could plausibly originate from a nearby tree rather than being deliberately placed by an attacker. Their results demonstrated the efficacy of naturally occurring perturbations in inducing misclassification, underscoring the potential to degrade the reliability of AI-driven traffic sign recognition systems in real-world 
 ~conditions.
