\section{Threat Model and Assumptions}

In this work, we consider an adversary who seeks to manipulate a deep learning-based traffic sign classification system by leveraging snow accumulation as a natural physical adversarial perturbation. The adversary exploits the fact that machine learning models heavily rely on visual features, making them vulnerable to occlusions that alter key sign characteristics. By strategically placing snow-like obstructions on traffic signs, the adversary aims to induce misclassification, leading to potential safety risks in autonomous driving systems.

The adversary operates under a black-box setting, meaning they do not have access to the modelâ€™s architecture, parameters, or training data. The adversary has physical access to traffic signs and can either rely on naturally accumulating snow or manually apply snow patches at strategic locations on the signs. The attack is particularly dangerous in winter conditions, where snow accumulation is common, making it difficult to distinguish between intentional and unintentional obstructions.

Despite its practicality, the attack is subject to several constraints. Natural weather conditions, such as wind or melting snow, may reduce its effectiveness over time. Additionally, excessive occlusion may be noticeable to human drivers, potentially limiting its impact in mixed human-autonomous driving environments.
