\section{Discussion and Conclusions}
\label{sec:conclusions}

IARs are an emerging competitor to DMs, matching or surpassing them in image quality at a higher generation speed. However, our comprehensive analysis demonstrates that IARs \textit{empirically} exhibit significantly higher privacy risks than DMs, given the current state of privacy attacks against the respective model types. 
Concretely, we develop novel MIA for IARs that leverages components of the strongest MIAs from LLMs and DMs to reach an extremely high \textbf{86.38\%} \tprat, as opposed to merely 6.38\% for the strongest \textit{DM-specific} MIAs in respective DMs. 
Our DI method further confirms the high privacy leakage from IARs by showing that only 6 samples are required to detect dataset membership, compared to at least 200 for reference DMs of comparable image generation utility. We also create a new data extraction attack for IARs that reconstructs even up to 698 training images from \varlarge, while previous work showed only 50 images extracted from DMs. Our results indicate the fundamental privacy-utility trade-off for IARs, where their higher performance comes at the cost of more severe privacy leakage. 
We explore preliminary mitigation strategies inspired primarily by diffusion-based approaches, however, the initial results indicate that dedicated privacy-preserving techniques are necessary. Our findings highlight the need for stronger safeguards in the deployment of IARs, especially in sensitive applications.



\section*{Broader Impact}
Image autoregressive models (IARs) have rapidly gained popularity for their strong image generation abilities. 
However, the privacy risks that come associated to these advancements have remained unexplored.
This work makes a first step towards identifying and quantifying these risks. Through our findings, we highlight that IARs 
 \textit{empirically} experience significant leakage of private data.
These findings are relevant to raise awareness of the community and to steer efforts towards designing dedicated defenses.
This enables a more ethical deployment of these models.


\section*{Acknowledgments}
This work was supported by the German Research Foundation (DFG) within the framework of the Weave Programme under the project titled "Protecting Creativity: On the Way to Safe Generative Models" with number 545047250. We also gratefully acknowledge support from the Initiative and Networking Fund of the Helmholtz Association in the framework of the Helmholtz AI project call under the name "PAFMIM", funding number ZT-I-PF-5-227. Responsibility for the content of this publication lies with the authors. This research was also supported by the Polish National Science Centre (NCN) within grant no. 2023/51/I/ST6/02854 and 2020/39/O/ST6/01478 and by Warsaw University of Technology within the Excellence Initiative Research University (IDUB) programme.