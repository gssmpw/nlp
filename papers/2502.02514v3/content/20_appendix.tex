\appendix
\onecolumn

\section{Why IARs (seem to) leak more privacy than DMs?}
\label{app:why_iars_leak_more}

{In the following we provide insights explaining the higher leakage observed in IARs. First, we focus on differences in architectures and models' internals. Then, we switch to explore architecture-agnostic factors like model size.}

\subsection{Inherent differences between IARs and DMs}
{We note that DMs have inherently different characteristics than IARs, and we link them to the privacy risks they exhibit. We identify three key factors:
\begin{enumerate}
    \item \textbf{Access to $p(x)$ boosts MIA}~\citep{rmia}. We note that IARs inherently expose the full information about $p(x)$ at the output (per-token logits, see~\cref{eq:ar_task}). In contrast, DMs do not, as they learn to transform $\mathcal{N}(0,I)$ to the data distribution $q(x)$ by iterative denoising process. This difference is expressed with varying MIA designs for DMs and IARs--the former exploit the predicted noise, while the latter work with $p(x)$, by focusing on the logits. Our results confirm this premise--MAR is less prone to all privacy risks, and it does not output $p(x)$. It outputs continuous tokens, sampled from a diffusion module.
    \item \textbf{AutoRegressive training exposes IARs to more data per update}. For each training sample passed through the IAR, the model "sees" $N$ different sequences to predict. Conversely, DMs only "see" a single, noisy image. This influences two factors: a) training time of the model--DMs require to be trained two times longer than IARs, on average. b) privacy leakage--IARs are exposed to more information per each update step, which translates to increased vulnerability for privacy attacks like MIAs, DI, and data extraction. VAR outputs 10 sequences of tokens, and is less prone to MIA than RAR, which outputs 256 sequences, \eg VAR-\textit{d}-20 vs. RAR-L (models of similar sizes).
    \item \textbf{Multiple \textit{independent} signals amplify leakage}. Previous works~\citep{maini2024llmdatasetinferencedid,dubinski2024cdicopyrighteddataidentification} aggregate signal from many MIAs to yield a stronger attack. Notably, each token predicted by IARs leaks unique information from the model, as it is generated from a (slightly) different prefix. Thus, per-token losses/logits that IAR-specific MIAs use, when aggregated, add up to a more informative signal, which in turn yields stronger MIAs. In contrast, DMs' outputs provide a general direction for the denoising process, and are strongly correlated. In effect, predictions at different timesteps do not provide enough \textit{novel} information to the MIA to boost its strength.
\end{enumerate}
}

{We believe that these reasons are behind greater privacy leakage that we observe for IARs than for DMs.}

\subsection{Architecture-agnostic differences between the models}

{The models evaluated in our work differ in many factors. Two of them, model size and training duration, are mostly architecture-agnostic, which means they are less related to the design choices of the specific models. As the efficacy of privacy attacks is directly related to these factors~\citep{shokri2017membershipinference}, we want to assess if our results \textit{really} show that IARs leak more than DMs. To this end, we collect five variables: TPR@FPR=1\% (MIA), $P$ (DI metric), model size, training duration, and \textit{Is IAR} for every model we evaluate in the paper (11 IARs, 8 DMs). For the first two (MIA, DI) we take them directly from~\cref{tab:mia_naive_vs_ours,tab:di_naive_vs_ours,tab:tpr_dm}. We obtain the model sizes from~\cref{tab:iar_model_details,tab:dm_details}. Training duration is expressed by a number of data points passed through the model at training, e.g., for RAR-B we have 400 epochs of ImageNet-1k train set, which amounts to $400\times1.27$M $\approx0.5$B samples seen. \textit{Is IAR} factor is a 1 if the model is IAR, 0 otherwise. We take these variables and compute pairwise Pearsonâ€™s correlation between them, using values for all the models.}

{In~\cref{tab:factors_correlation} we show correlations between factors (columns) and privacy metrics (rows). We identify the following insights:
\begin{enumerate}
    \item \textbf{Training duration} is a factor that increases vulnerability for MIA and DI for DMs the most.
    \item \textbf{Model size} influences leakage more for IARs than for DMs.
    \item \textbf{\textit{Is IAR}} factor plays the most significant role for the DI performance. It also correlates with MIA performance.
\end{enumerate}
}

{Our results show that while these two factors--model size and training duration--influence the performance of our attacks against the models, the results strengthen our notion that IARs tend to leak more privacy than IARs due to their inherent characteristics.}

\begin{table}[]
    \scriptsize
    \caption{{\textbf{Correlation between different factors and privacy leakage.} Our results show that while the model-agnostic factors correlate with the performance, the fact that the model is IAR or not also correlates with the leakage.}}
    \centering
    \begin{tabular}{ccccc}
        \toprule
        & Architecture & Training Duration & Model Size & Is IAR \\ 
         \midrule
        {$P$ (DI)} & IAR & 0.24 & -0.39 &  \\ 
        {$P$ (DI)} & DM & -0.58 & -0.32 &  \\ 
        {$P$ (DI)} & All & -0.04 & -0.28 & -0.46 \\ 
        \midrule
         {$TPR@FPR=1\%$}& IAR & 0.17 & 0.93 &  \\ 
         {$TPR@FPR=1\%$}& DM & 0.31 & 0.11 &  \\ 
         {$TPR@FPR=1\%$} & All & -0.2 & 0.87 & 0.38 \\ 
        \bottomrule
    \end{tabular}
    \label{tab:factors_correlation}
\end{table}

\section{Limitations}
\label{app:limitations}

{We acknowledge our privacy analysis of the novel IARs, and comparison to DMs suffers from two limitations. We do not evaluate our attacks on the biggest available models (like Infinity~\citep{han2024infinityscalingbitwiseautoregressive}) trained on massive (over 1B samples), messy datasets. Secondly, there are many factors crucial for MIA and DI performance, which differ in values between almost all the models. The following explains these issues in more detail.}

\subsection{On the infeasibility of high-scale experiments on extremely big models}

{We do not assess how our attacks perform when applied to models trained on datasets of the scale higher than 1M samples. It may raise concerns about the scalability of the attacks and the insights they provide to the real-world applications. Unfortunately, IARs trained on bigger datasets than ImageNet-1k (Infinity~\citep{han2024infinityscalingbitwiseautoregressive}, HART~\citep{tang2024hartefficientvisualgeneration}) do not disclose fully what their training data \textit{exactly} is. Because of that, we are unable to perform a sound evaluation of the privacy attacks. We lack the ability to assess MIA's and DI's performance correctly, as these methods rely on two assumptions: (1) we know a part of the training data (members), (2) we have access to non-members that are \textit{independent and identically distributed} (IID) with members. When we fail to satisfy (2) the methods would collapse to dataset detection~\citep{das2024blind}. Moreover, without satisfying (1) we cannot run MIA and DI at all.}

{While a \textit{methodologically correct} evaluation of the cutting-edge models is out of our reach, we aim to provide more insight into text-to-images IARs, and see how much they leak. To this end, we run our attacks on VAR-CLIP \citep{zhang2024varcliptexttoimagegeneratorvisual}, a VAR-\textit{d}16 model trained on a captioned ImageNet-1k. Our results in~\cref{tab:varclip} show that this model leaks significantly more data than its class-to-image counterpart of the same size. Moreover, the leakage is on a level similar to VAR-\textit{d}20's--a model of double the size of VAR-CLIP. We argue that the increased leakage stems from the model overfitting more to the conditioning information, which is richer for textual data than for the class labels.}

\begin{table}[h]
    \centering
    \scriptsize
    \caption{{\textbf{Leakage of VAR-CLIP compared to class-conditional VARs.} We observe increased privacy leakage over class-conditioned models, expressed by a stronger performance of our attacks.}}
    \begin{tabular}{ccc}
    \toprule
        \textbf{Model} & \textbf{TPR@FPR=1\%} & \textbf{$P$ (DI)} \\ 
        \midrule
         {VAR-CLIP} & 6.30 & 60 \\ 
         {VAR-\textit{d}16} & 2.18 & 200 \\ 
         {VAR-\textit{d}20} & 5.92 & 40 \\ 
        \bottomrule
    \end{tabular}
    \label{tab:varclip}
\end{table}

\subsection{On the impossibility of a fully standardized experimental setup between the models}

{In the ideal scenario we are able to isolate only the factors inherent to the models' architecture, and consequently, are able to draw insights which design choices lead to what privacy risks. We would call such setup \textit{standardized}, meaning that the models are \textit{almost identical}, and differ only in factors we want to explore (like architecture). However, in reality we deal with too few models, each one being trained differently, which allows only for limited insights.}

{We note the models vary in the following ways:
\begin{enumerate}
    \item \textbf{Training duration}, expressed by number of data points seen during training, \eg RAR-B sees $400\times1.27$M $\approx0.5$B samples. In DMs we evaluate the training duration varies between 0.21B to 1.79B samples seen, whereas IARs are trained with between 0.26B and 0.51B samples.
    \item \textbf{Training objectives}. DMs minimize~\cref{eq:dm_loss}, while IARs--~\cref{eq:ar_loss}. Importantly, DMs minimize the expected error \textit{over timesteps and data}, which necessitates a twice as long training duration for DMs than IARs (on average) to achieve comparable FID.
    \item \textbf{Model sizes}. IARs benefit from scaling laws~\citep{kaplan2020scaling}, and that allows them to be scaled up to sizes greater than DMs, before their performance plateaus. DMs cannot be scaled that well--the performance gains diminish faster with the increase of size. In effect, the biggest IARs we evaluate--\varbig and RAR-XXL-- are on average 2-3 times bigger than DMs. Since the size of the model impacts its vulnerability to privacy attacks, our analyses do not fully accommodate for that factor.
    \item \textbf{Two stage architectures}. All models incorporate an encoder-decoder network for training and inference, \eg VQ-VAE~\citep{esser2020taming}. Importantly, these encoders differ between models. VAR's next-scale prediction paradigm requires training of a specialized encoder that understands how to process residual token maps, used during encoding an image to the sequence of discrete tokens. Moreover, VAR and RAR work with \textit{discrete} tokens, \ie the encoder-decoder network additionally contains a quantizer module, which translates the continuous latent representations of the images to a 2D integer-only maps.
\end{enumerate}}

{Unfortunately, these factors directly prohibit a \textit{standardized comparison} of the privacy risks between DMs and IARs. We are not able to fix the training duration for all models--the generation quality of DMs would be significantly subpar than IARs (as DMs require twice the training time of IARs), and thus the results would be unsound. We incorporate the size of the models in~\cref{fig:pareto,fig:pareto_di,fig:pareto_app}, however, we acknowledge that the sizes vary between the models, and this limits our ability to fully disentangle this factor from the privacy results.}

{However, we are able to fix one factor for all the models: utility. We know the models we source are trained to the maximum of the potential each architecture allows, as we utilize models from papers that aim for exactly that--the best performance. We compare models that are the \textit{upper boundary} of what is possible within the inherent limitations and trade-offs each architecture has to offer. We are deeply aware that privacy vs utility is a balancing act: better models tend to be less private. \textbf{Thus, our study fixes one of these parameters--utility--to be the highest possible for a given model}, and under that condition we evaluate how much it leaks. We believe our results provide strong empirical evidence that DMs constitute a Pareto optimum when it comes to image generation--they are comparable in FID, while being significantly more private than the novel IAR models.}

\section{Privacy leakage under a unified attack}
\label{app:unified_mia}

{We acknowledge that the field of privacy attacks against image generative models like IARs or DMs is constantly evolving. Since our work aims to provide the current empirical insights into differences in privacy leakage between these architectures, we use \textit{the strongest available} attacks to provide an upper boundary on the privacy leakage, following literature on privacy auditing~\citep{nasr2023tight,dwork2006differential}}. 

{However, IARs and DMs are two different classes of models. In consequence, the attacks we employ are \textit{tailored} to their inherent properties, and thus the attacks vary. This might raise concerns of the following nature: what if the field progresses and a new, very potent attack is designed for DMs? Will our current empirical results hold, \ie can we \textit{really} claim IARs leak more privacy than DMs, or is it just the current MIAs against DMs that are less powerful than for IARs?}

{We believe our insights in~\cref{app:why_iars_leak_more} provide reasons why IARs \textit{inherently} leak more than DMs. To strengthen our results, we perform an \textit{architecture-agnostic}, unified attack against all models--Loss Attack~\citep{yeom2018lossmia}}.

\subsection{Loss Attack}

\begin{table}[]
    \centering
    \scriptsize
    \caption{{\textbf{Unified attack results.} We employ Loss Attack~\citep{yeom2018lossmia}, discarding any model-specific modifications that might strengthen the signal, to ensure a fair comparison between different model classes and architectures. The results strongly support our notion that IARs leak more privacy than DMs.}}
\begin{tabular}{ccccccc}
\toprule
Model & Architecture & $P$ (Dataset Inference) & TPR@FPR=1\% (MIA) & AUC (MIA) & Accuracy (MIA) \\
\midrule
VAR-$\mathit{d}$16 & IAR & 3000 & 1.50{\tiny $\pm$0.18} & 52.35{\tiny $\pm$0.40} & 50.08{\tiny $\pm$0.03} \\
VAR-$\mathit{d}$20 & IAR & 1000 & 1.67{\tiny $\pm$0.20} & 54.54{\tiny $\pm$0.40} & 50.11{\tiny $\pm$0.03} \\
VAR-$\mathit{d}$24 & IAR & 300 & 2.19{\tiny $\pm$0.20} & 59.56{\tiny $\pm$0.39} & 50.15{\tiny $\pm$0.04} \\
VAR-$\mathit{d}$30 & IAR & 40 & 4.95{\tiny $\pm$0.40} & 75.46{\tiny $\pm$0.35} & 50.32{\tiny $\pm$0.05} \\
MAR-B & IAR & 6000 & 1.43{\tiny $\pm$0.17} & 51.31{\tiny $\pm$0.30} & 50.48{\tiny $\pm$0.16} \\
MAR-L & IAR & 3000 & 1.52{\tiny $\pm$0.16} & 52.35{\tiny $\pm$0.30} & 50.70{\tiny $\pm$0.18} \\
MAR-H & IAR & 2000 & 1.61{\tiny $\pm$0.17} & 53.66{\tiny $\pm$0.30} & 51.07{\tiny $\pm$0.20} \\
RAR-B & IAR & 800 & 1.77{\tiny $\pm$0.25} & 54.92{\tiny $\pm$0.41} & 50.25{\tiny $\pm$0.06} \\
RAR-L & IAR & 400 & 2.10{\tiny $\pm$0.27} & 58.03{\tiny $\pm$0.40} & 50.39{\tiny $\pm$0.07} \\
RAR-XL & IAR & 80 & 3.40{\tiny $\pm$0.40} & 65.58{\tiny $\pm$0.38} & 50.81{\tiny $\pm$0.10} \\
RAR-XXL & IAR & 40 & 5.73{\tiny $\pm$0.52} & 74.44{\tiny $\pm$0.34} & 51.64{\tiny $\pm$0.19} \\
\midrule
LDM & DM & $>20000$ & 1.08{\tiny $\pm$0.13} & 50.13{\tiny $\pm$0.05} & 50.13{\tiny $\pm$0.11} \\
U-ViT-H/2 & DM & $>20000$ & 0.85{\tiny $\pm$0.13} & 50.11{\tiny $\pm$0.09} & 50.07{\tiny $\pm$0.18} \\
DiT-XL/2 & DM & $>20000$ & 0.84{\tiny $\pm$0.14} & 50.09{\tiny $\pm$0.05} & 50.15{\tiny $\pm$0.14} \\
MDTv1-XL/2 & DM & $>20000$ & 0.85{\tiny $\pm$0.13} & 50.05{\tiny $\pm$0.05} & 50.08{\tiny $\pm$0.14} \\
MDTv2-XL/2 & DM & $>20000$ & 0.87{\tiny $\pm$0.12} & 50.14{\tiny $\pm$0.05} & 50.16{\tiny $\pm$0.14} \\
DiMR-XL/2R & DM & $>20000$ & 0.89{\tiny $\pm$0.13} & 49.55{\tiny $\pm$0.06} & 49.70{\tiny $\pm$0.14} \\
DiMR-G/2R & DM & $>20000$ & 0.85{\tiny $\pm$0.12} & 49.54{\tiny $\pm$0.06} & 49.69{\tiny $\pm$0.13} \\
SiT-XL/2 & DM & 6000 & 0.95{\tiny $\pm$0.16} & 48.22{\tiny $\pm$0.26} & 49.97{\tiny $\pm$0.09} \\
\bottomrule
\end{tabular}
\label{tab:unified_mia_result}
\end{table}

\noindent {Loss Attack is defined as follows: (1) For each sample we perform a forward pass through the model as it would be during the training (2) We compute the model loss (specific to each model) for the samples. (3) We use the losses to perform MIA (as in~\cref{app:mias_full}), and we use the losses to perform Dataset Inference (see~\cref{app:di_section})}.

{Loss Attack differs from MIAs against DMs in the following way: instead of fixing the timestep to the most optimal one ($t=100$~\citep{carlini2023extracting}), and averaging the loss over 5 different input noises~\citep{carlini2023extracting}, we sample $t\sim\mathcal{U}[0, 1000]$, and compute the per-sample loss for \textit{a single random noise}}.

{For MAR, we roll back the modifications to the diffusion module, explained in~\cref{app:mias_on_mar}. We do not fix the timestep to the most optimal one ($t=500$), we compute the loss over 5 (default for training), instead of 64 (optimal) input noises, and we sample the masking ratio for each sample following the distribution used during training, instead of fixing it to 0.95--the optimal value.}

{For VAR and RAR, this attack is identical to the one in~\cref{tab:tpr_baseline_mias} (first row).}

{Since the DI framework relies on features obtained from different MIAs, we run DI only with the single feature--Loss Attack. We unify DI to be the same for DMs and IARs by removing the scoring function $s$ for DM-specific DI--CDI~\citep{dubinski2024cdicopyrighteddataidentification}. In effect, the procedure is \textit{identical} for DMs and IARs.}

\subsection{IARs are empirically more prone to the unified attack than DMs}

{Our results in~\cref{tab:unified_mia_result} are consistent with the results achieved with DM- and IAR-specific attacks (\cref{tab:mia_naive_vs_ours,tab:di_naive_vs_ours}) Empirical data shows IARs are more vulnerable to MIAs and DI. Loss Attack does not yield \tprat greater than random guessing (1\%) for DMs, whereas all IARs perform above random guessing. Moreover, with such a weak signal, DI ceases to be successful for DMs, requiring above 20,000 samples ($P$) to reject the null hypothesis (no significant difference between members and non-members), with one exception: SiT. Conversely, IARs retain their high vulnerability to DI, with the most private IAR--MAR-B--being similarly vulnerable to the least private DM--SiT.}

{We believe results obtained under the unified attack strengthen our message that current IARs leak more privacy than DMs.}

\section{Additional Background}

{In the following we provide additional background on Diffusion Models used for comparison to IARs, details on MIAs, and precise definition of the DI procedure, as well as a description of the sampling strategies used by IARs during generation.}

\subsection{Diffusion Models}
\label{app:dms_full}

\begin{table*}[h!]
    \scriptsize
        \newcommand{\tightcolsep}{\setlength{\tabcolsep}{3.5pt}} %
    \tightcolsep %
    \centering
        \caption{\textbf{DM details.} We report the training details for the DM  models used in this work.}
    \begin{tabular}{ccccccccc}
    \toprule
\textbf{} & LDM & {U-ViT-H/2} & {DiT-XL/2} & {MDTv1-XL/2} & {MDTv2-XL/2} & {DiMR-XL/2R} & {DiMR-G/2R} & {SiT-XL/2} \\
    \midrule
\textbf{Model parameters} & 395M & 501M & 675M & 700M & 742M & 505M & 1056M & 675M \\

    \textbf{Training steps} & 178k & 500k & 400k & 2M & 6.5M  & 1M & 1M &  7M \\
    \textbf{Batch size} & 1200 & 1024 & 256 & 256 & 256 & 1024 & 1024 & 256  \\
    \textbf{FID} & 3.60 & 2.29 & 2.27 & 1.79 & 1.58 & 1.70 & 1.63  & 2.06 \\
    \bottomrule
    \end{tabular}
    \label{tab:dm_details}
\end{table*}


{We provide a brief overview of DMs used in our experiments. All models are class-conditioned latent DMs trained on the ImageNet dataset at 256Ã—256 resolution.}
{Except for LDM, all models utilize Vision Transformers (ViT) \citep{dosovitskiy2021imageworth16x16words} as their diffusion backbones. LDM instead employs the UNet architecture \cite{unet}, being a prior work. We refer the reader to the original publications for more details about their architectures and training strategies.}

{\textit{LDM (Latent Diffusion Model)} by \citet{rombach2022high} first propose running diffusion in a learned latent space rather than in pixel space, using a U-Net as the denoising backbone.}

{\textit{DiT-XL/2 (Diffusion Transformer)} by \citet{peebles2022dit} replaces the conventional U-Net with a ViT backbone.}

{\textit{U-ViT-H/2} by \citet{bao2022uvit} adopts a ViT-based architecture with skip connections inspired by U-Nets. It treats image patches, class labels, and diffusion timesteps as input tokens in a unified transformer space.}

{\textit{MDTv1-XL} and \textit{MDTv2-XL (Masked Diffusion Transformer)} by \citet{gao2023masked} apply a masked latent modeling strategy during training to enhance contextual learning. The model predicts missing latent tokens, improving training efficiency and sample quality. MDTv2 introduces architectural refinements that lead to further gains in fidelity and performance.}

{\textit{DiMR-XL/2R} and \textit{DiMR-G/2R} by \citet{liu2024alleviating} propose a multi-resolution diffusion framework that processes features across different spatial scales. This design improves detail preservation and reduces distortions, especially when using large patch sizes. The models also incorporate time-aware normalization to enhance temporal conditioning.}

{\textit{SiT-XL/2 (Scalable Interpolant Transformer)} by \citet{ma2024sit} extends the DiT architecture with an interpolant mechanism that decouples the noise schedule from the model. This allows for greater flexibility in diffusion dynamics without architectural changes.}

{Besides these models, we additionally evaluate emerging DMs: LFM~\citep{dao2023flow}--a flow-matching model, and DiT-MoE~\citep{fei2024scalingdiffusiontransformers16}--a mixture-of-experts DM, based on DiT~\citep{peebles2022dit}. We do not include these models for the final comparison for three reasons: (1) the released models are significantly smaller (130M parameters each) than all other models, (2) the released models achieve subpar FID scores (4.46 for LFM, unknown FID for DiT-MoE), (3) unknown details of training (number of iterations for DiT-MoE). For completeness, we perform MIA and DI, and report the values in~\cref{tab:extra_dms}.}

\begin{table}[!ht]
    \centering
    \scriptsize
    \caption{{\textbf{Results for novel DM architectures.} We see the leakage is similar to the rest of DMs.}}
    \begin{tabular}{ccc}
        \toprule
        Model & TPR@FPR=1\% & $P$ (DI) \\
        \midrule
        LFM & 1.79 & 2000\\
        DiT-MoE & 1.70 & 2000 \\
        \bottomrule
    \end{tabular}
    \label{tab:extra_dms}
\end{table}

\subsection{Membership Inference Attacks}
\label{app:mias_full}


MIAs attempt to identify whether a given input $x$, drawn from distribution $\mathcal{X}$, was part of the training dataset $\mathcal{D}_{\text{train}}$ used to train a target model $f_\theta$. We explore several MIA strategies under a gray-box setting, where the adversary has access to the modelâ€™s loss but no information about its internal parameters or gradients. The goal is to construct an \textit{attack} function $A_{f_\theta}: \mathcal{X} \rightarrow \{0, 1\}$ that predicts membership. 

\textbf{Threshold-Based attack.} 
Threshold-based attack is a key method of establishing membership status of a sample. 
It relies on a metric such as Loss~\citep{yeom2018lossmia} to determine membership. An input $x$ is classified as a member if value of the metric falls below a predefined threshold:
\begin{equation}
A_{f_\theta}(x) = \mathbb{1}[\mathcal{M}(f_\theta, x) < \gamma],
\label{eq:mia_thr}
\end{equation}
where $\mathcal{M}$ is the metric function, and $\gamma$ is the threshold. 

\textbf{\mink Metric.} 
To address the limitations of predictability in threshold-based attacks, \citet{shi2024detecting} introduced the \mink metric. This approach evaluates the least probable $ K\% $ of tokens in the input $ x $, conditioned on preceding tokens{, where $K$ is a hyperparameter, selected from $\{10,20,30,40,50\}$}. By focusing on less predictable tokens, \mink avoids over-reliance on highly predictable parts of the sequence. Membership is determined by thresholding the average negative log-likelihood of these low-probability tokens:
\[
A_{\fmodelm}(x) = \mathbb{1}[\mink(x) < \gamma].
\]

{The final value is reported for the best $K$.}

\textbf{\mink++.}  
\mink++ refines the \mink method by leveraging the insight that training samples tend to be local maxima in the modeled probability distribution. Instead of simply thresholding token probabilities, \mink++ examines whether a token forms a mode or has relatively high probability compared to other tokens in the vocabulary.

Given an input sequence $x = (x_1, x_2, \dots, x_T)$ and an autoregressive language model $f_\theta$, the \mink++ score is computed as:
\begin{equation}
\mathcal{S}_{\text{Min-K\%++}}(x) = \frac{1}{|S|} \sum_{t \in S} \frac{\log p(x_t | x_{<t}) - \mu_{x<t}}{\sigma_{x<t}},
\end{equation}
where $S$ consists of the least probable $K\%$ tokens in $x$, and $\mu_{x<t}$ and $\sigma_{x<t}$ are the mean and standard deviation of log probabilities across the vocabulary. Membership is determined by thresholding:
\begin{equation}
A_{f_\theta}(x) = \mathbb{1}[\mathcal{S}_{\text{Min-K\%++}}(x) \geq \gamma].
\end{equation}

{Similarly to \mink, \mink++ sweeps over $K\in\{10,20,30,40,50\}$, and the final result is reported for the best hyperparameter $K$.}


\textbf{zlib Ratio Attacks.}
A simple baseline attack leverages the compression ratio computed using the \textit{zlib library}~\citep{zlib2004}. This method compares the modelâ€™s perplexity with the sequenceâ€™s entropy, as determined by its zlib-compressed size. The attack is formalized as:
\[
A_{\fmodelm}(x) = \mathbb{1}\left[\frac{\mathcal{P}_{\fmodelm}(x)}{zlib(x)} < \gamma \right].
\]
The intuition is that samples from the training set tend to have lower perplexity for the model, while the zlib compression, being model-agnostic, does not exhibit such biases.

\textbf{CAMIA} introduces several context-aware signals to enhance membership inference accuracy. The \textit{slope signal} captures how quickly the per-token loss decreases over time, as members typically exhibit a steeper decline. \textit{Approximate entropy} quantifies the regularity of the loss sequence by measuring the frequency of repeating patterns, while \textit{Lempel-Ziv complexity} captures the diversity of loss fluctuations by counting unique substrings in the loss trajectoryâ€”both of which tend to be higher for non-members. The loss thresholding \textit{Count Below} approach computes the fraction of tokens with losses below a predefined threshold, exploiting the tendency of members to have more low-loss tokens. \textit{Repeated-sequence amplification} measures how much the loss decreases when an input is repeated, as non-members often show stronger loss reductions due to in-context learning.

\textbf{Surprising Tokens Attack (SURP).}  
SURP detects membership by identifying \textit{surprising tokens}, which are tokens where the model is highly confident in its prediction but assigns a low probability to the actual ground truth token. Seen data tends to be less surprising, meaning the model assigns higher probabilities to these tokens in familiar contexts.

For a given input $x = (x_1, x_2, \dots, x_T)$, surprising tokens are those where the Shannon entropy is low and the probability of the ground truth token is below a threshold:
\begin{equation}
S = \{t \mid H_t < \epsilon_e, \quad p(x_t | x_{<t}) < \tau_k \},
\end{equation}
where $H_t$ is the entropy of the modelâ€™s output at position $t$, {$\tau_k$ is the probability of the bottom $k\%$-th token. $k\in\{10,20,30,40,50\}$, and $\epsilon_e\in\{2,4,8,16\}$ are hyperparameters. }The SURP score is the average probability assigned to these surprising tokens:
\begin{equation}
\mathcal{S}_{\text{SURP}}(x) = \frac{1}{|S|} \sum_{t \in S} p(x_t | x_{<t}).
\end{equation}
Membership is determined by thresholding:
\begin{equation}
A_{f_\theta}(x) = \mathbb{1}[\mathcal{S}_{\text{SURP}}(x) \geq \gamma].
\end{equation}

{The SUPR's result for the best combination of $k$ and $\epsilon_e$ is selected as the final performance.}

\subsection{Dataset Inference}
\label{app:di_section}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/di_schema.pdf}
\caption{\textbf{Dataset Inference for IARs Procedural Steps.} The process consists of four main steps: 
\encircle{1}~\textit{Data Preparation:} Prepare the data to verify whether the (suspected) member samples \P were used to train the IAR. The (confirmed) nonmember samples \U, from the same distribution as \P, serve as the validation set.
\encircle{2}~\textit{Feature Extraction:} Run each individual MIA on all inputs from $\{\P,\U\}$ to extract membership features for all data samples. We use our MIAs tailored to IAR models. 
\encircle{3}~\textit{Score Computation:} Normalize the extracted features using MinMaxScaler to scale them into the [0,1] range and compute a membership score for each sample by summing its normalized feature values.
\encircle{4}~\textit{Statistical Testing:} Apply a statistical t-test to verify whether the scores obtained for the public suspect data points \P are statistically significantly higher than those for \U. If so, \P is marked as being part of the IAR's training set. Otherwise, the test is inconclusive and the IAR's training set is considered independent of \P.}
    \label{fig:di_schema}
\end{figure}


Scaling IARs to larger datasets raises concerns about the unauthorized use of proprietary or copyrighted data for training. With the growing adoption and increasing scale of IARs, this issue is becoming more pressing. In our work, we use DI to quantify the privacy leakage in IAR models. However, DI can  be additionaly used to establish a dispute-resolution framework for resolving illicit use of data collections in model training, ie. \textit{determine if a specific dataset was used to train a IAR.}  
 
 The framework involves three key roles. First, the \textit{victim} ($\mathcal{V}$) is the content creator who suspects that their proprietary or copyrighted data was used to train a IAR without permission. The victim provides a subset of samples ($\mathcal{P}$) they believe may have been included in the model's training dataset. Second, the \textit{suspect} ($\mathcal{A}$) refers to the IAR provider accused of using the victim's dataset during training. The suspect model ($f_\theta$) is examined to determine whether it demonstrates evidence of having been trained on $\mathcal{P}$. Finally, the \textit{arbiter} acts as a trusted third party, such as a regulatory body or law enforcement agency, tasked with conducting the dataset inference procedure. 
For instance, consider an artist whose publicly accessible but copyrighted artworks have been used without consent to train a IAR. The artist, acting as the victim ($\mathcal{V}$), provides a small subset of suspected training samples ($\mathcal{P}$). The IAR provider ($\mathcal{A}$) denies any infringement. An arbiter intervenes and obtains gray-box or white-box access to the suspect model. Using DI methodology, the arbiter determines whether the IAR demonstrates statistical evidence of training on $\mathcal{P}$. 

\subsection{Sampling Strategies}

The greedy approach selects the token with the highest probability. In the top-$k$ sampling, the highest $k$ token probabilities are retained, while all others are set to zero. The remaining non-zero probabilities are then re-normalized and used to determine the next token. Notably, when $k=1$, this method reduces to greedy sampling.
\section{Model Details}
\label{app:model_details}
In our experiments, we use a range of models from VAR~\cite{var_tian2024visualautoregressivemodelingscalable}, RAR\cite{rar_yu2024randomizedautoregressivevisualgeneration}, and MAR~\cite{mar_li2024autoregressiveimagegenerationvector} architectures, each varying in model size and architecture. The details of these models, including the number of parameters, training epochs, and FID scores, are summarized in ~\cref{tab:iar_model_details}.
The models were trained on the class-conditioned image generation on the ImageNet dataset~\cite{deng2009imagenet}.

\begin{table*}[h!]
    \centering
        \newcommand{\tightcolsep}{\setlength{\tabcolsep}{2pt}} %
    \tightcolsep %
    \caption{\textbf{Model details.} We report the training details for IAR the models used in this work.}
    \label{tab:iar_model_details}
        \tiny
    \begin{tabular}{l c c c c c c c c c c c}
        \toprule
        & \multicolumn{4}{c}{\textbf{VAR Models}} & \multicolumn{4}{c}{\textbf{RAR Models}} & \multicolumn{3}{c}{\textbf{MAR Models}} \\
        \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule(r){10-12}
        & VAR-\textit{d}16 & VAR-\textit{d}20 & VAR-\textit{d}24 & VAR-\textit{d}30 & RAR-B & RAR-L & RAR-XL & RAR-XXL & MAR-B & MAR-L & MAR-H \\
        \midrule
        \textbf{Model parameters} & 310M & 600M  &  1.0B & 2.1B & 261M & 462M & 955M & 1.5B & 208M & 478M & 942M \\
        \textbf{Training epochs} & 200  & 250  & 300  & 350 & 400 & 400 & 400 & 400 & 400 & 400 &  400 \\
        \textbf{FID} & 3.55  & 2.95  & 2.33  & 1.92 & 1.95 & 1.70 & 1.50 & 1.48 & 2.31 & 1.78 & 1.55 \\
        \bottomrule
    \end{tabular}
\end{table*}


\section{Training and Inference Cost Estimation}
\label{app:pareto_how}
\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/pareto_full.pdf}
    \caption{\textbf{Comprehensive comparison of the trade-offs between IARs and DMs.}}
    \label{fig:pareto_app}
\end{figure}

Here we describe the comprehensive process of training and generation cost estimation of IARs and DMs, which results in the plot~\cref{fig:pareto_app}. We use \textit{torchprofile}~\citep{torchprofile} Python library to measure GFLOPs used for generation and training. 

In order to compute the training cost, the procedure is as follows. (1) We perform a single forward pass through the model. (2) We multiply the obtained GFLOPs cost by two, to accommodate for the backward pass cost. (3) We multiply the resulting cost of a single forward and backward pass by the amount of training samples passed through the model during training. The amount of samples is based on the numbers reported in the papers for each of the evaluated models. DMs and IARs use a different reporting methodology, with the former reporting training steps and a batch size, and the latter reporting the number of epochs. For the latter, we assume that a full pass through the ImageNet-1k training set is performed, thus we multiply the number of epochs by $1,281,167$.

Time to generate a single sample (referred to as latency) is computed by generating 640 images using code from the original models' repositories. We use the maximum batch size that fits on a single NVIDIA  RTX A4000 48GB GPU, to utilize our hardware to the maximum, in order to ensure a fair comparison. For DMs and IARs we follow the settings reported by authors of the respective papers that give the lowest FID score, \ie we use classifier-free guidance for all the models. For MAR we perform 64 steps of patches sampling. For all DMs but U-ViT we perform 250 steps of denoising, while for U-ViT the reported number is 50, which explains low latency of this model in comparison to others. We acknowledge that, in case of DMs, there are ways to lower the cost of the inference, \eg by lowering the number of denoising steps. However, we use the default, yet more costly setup for these models, as there is an inherent trade-off between generation quality and cost for DMs, which we want to avoid to make our results sound.

Single generation cost in GFLOPs is computed in a similar fashion. We utilize code provided by the authors of the respective papers for the inference, wrap it using \textit{torchprofile}, and perform a generation of a single sample. Note that here we do not measure time, and we can ignore the parallelism of hardware, as the total cost would stay the same. As we observe in~\cref{fig:pareto}, there is a discrepancy between latency and cost of generation, especially in case of RAR, where we observe an order of magnitude higher generation time than the GFLOPs cost would suggest. This phenomenon originates from the KV-Cache mechanism that is used in case of VAR and RAR during sampling. While the compute cost is lower thanks to the mechanism, the reading operation of the cache mechanism is not effectively parallelized, which results in hardware-incurred latency. We, however, acknowledge that this trade-off might become more beneficial in cases of low-power edge devices, as the computational power of these devices is more limited than the speed of memory operations.



\section{MIAs for MAR}
\label{app:mias_on_mar}




\paragraph{Adjusting Binary Mask}
\label{app:adjusting_mask}

MAR extends the IAR framework by incorporating masked prediction strategies, where masked tokens are predicted based on the visible ones. This design choice is inspired by Masked Autoencoders~\citep{he2022masked}, where selectively removing and reconstructing parts of the input allows models to learn better representations. Given that MIAs rely on detecting subtle differences in how models process known and unknown data, we hypothesize that adjusting the masking ratio during inference can amplify membership signals. By increasing the masking ratio from 0.86 (the training average) to 0.95, we create conditions where fewer tokens are available to reconstruct the original image, potentially exposing membership information more prominently.  

Our experimental results, reported in \cref{tab:adjusting_mask}, confirm that this strategy enhances MIAs' effectiveness. Specifically, \tprat for MAR-H increases from 2.18 to 2.88 (+0.70), and MAR-L sees an improvement from 1.89 to 2.25 (+0.36), demonstrating that a higher masking ratio strengthens membership signals. Notably, setting the mask ratio too high (e.g., 0.99) leads to a slight drop in  MIA performance, suggesting a balance must be struck between revealing more membership signal and overly degrading the modelâ€™s ability to generate images effectively.

\begin{table}[h!]
    \centering
    \scriptsize
    \caption{\textbf{Impact of varying mask ratio on MIAs for MAR.} We report \tprat. Higher values indicate stronger membership signals. The best-performing setting is highlighted in bold.}
    \label{tab:adjusting_mask}
    \begin{tabular}{llll}
\toprule
Mask Ratio & MAR-B & MAR-L & MAR-H \\
\midrule
0.75 & 1.64 (-0.05) & 1.65 (-0.24) & 1.81 (-0.37) \\
0.80 & 1.74 (+0.05) & 1.76 (-0.13) & 1.85 (-0.33) \\
0.85 & 1.68 (-0.01) & 1.83 (-0.06) & 2.00 (-0.18) \\
0.86  (default) & 1.69 (0.00) & 1.89 (0.00) & 2.18 (0.00) \\
0.90 & 1.65 (-0.04) & 1.88 (-0.01) & 2.22 (+0.05) \\
\textbf{0.95} & \textbf{1.88 (+0.19)} & \textbf{2.25 (+0.36)} & \textbf{2.88 (+0.70)} \\
0.99 & 1.77 (+0.08) & 1.86 (-0.03) & 2.14 (-0.04) \\

\bottomrule
\end{tabular}
\end{table}



\paragraph{Fixed Timestep}
\label{app:fixed_timestep}

MIAs on DMs have been shown to be most effective when conducted at a specific denoising step $t$~\citep{carlini2023extracting}. Since MAR utilizes a small diffusion module for token generation, we hypothesize that targeting MIAs at a fixed timestep $t$ rather than a randomly chosen one can similarly enhance MIA effectiveness. Unlike full-scale diffusion models, where the most discriminative timestep is typically around $t = 100$, our experiments reveal that for MAR models, the optimal timestep is $t = 500$.  

\cref{tab:fixed_timestep} illustrates the impact of this adjustment. When MIAs are performed at $t = 500$, MAR-H achieves a \tprat{} of 3.30, improving by +0.42 over the baseline random timestep approach. Similarly, MAR-L and MAR-B also see noticeable gains at this timestep. Notably, selecting timestep $t = 100$ significantly reduces the attack's effectiveness, with a drop of -0.38 for MAR-H. 


\begin{table}[h!]
    \centering
    \scriptsize
    \caption{\textbf{Impact of  using a fixed denoising timestep on MIAs for MAR performance.} We report \tprat. The most discriminative timestep is highlighted in bold.}
    \label{tab:fixed_timestep}
\begin{tabular}{llll}
\toprule
Timestep & MAR-B & MAR-L & MAR-H \\
\midrule
random & 1.88 (0.00) & 2.25 (0.00) & 2.88 (0.00) \\
100 & 1.60 (-0.27) & 1.90 (-0.34) & 2.50 (-0.38) \\
\textbf{500} & \textbf{1.88 (+0.00)} & \textbf{2.41 (+0.17)} & \textbf{3.30 (+0.42)} \\
700 & 1.85 (-0.03) & 2.35 (+0.10) & 3.20 (+0.32) \\
900 & 1.65 (-0.22) & 2.14 (-0.10) & 2.97 (+0.09) \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Reducing Diffusion Noise Variance}
\label{app:mul}

The MAR loss function, as defined in \cref{eq:dm_loss}, exhibits certain variance due to its dependence on randomly sampled noise $\epsilon$. During training, MAR uses four different noise samples per image. We hypothesize that increasing the number of noise samples can provide a more stable loss signal, thereby improving the performance of MIAs.  

Our results, summarized in \cref{tab:mul}, confirm that increasing the number of noise samples has a positive effect on attack performance. 






\begin{table}[h!]
    \centering
    \scriptsize
    \caption{\textbf{Impact of R
    reducing diffusion noise variance} on MIAs for MAR performance. We report \tprat. Obtaining loss for random noise sampled multiple times generally improves attack effectiveness. The best-performing setting is highlighted in bold.}
    \label{tab:mul}
\begin{tabular}{llll}
\toprule
Repeats & MAR-B & MAR-L & MAR-H \\
\midrule
4 (default) & 1.88 (0.00) & 2.41 (0.00) & 3.30 (0.00) \\
8 & 1.98 (+0.10) & 2.59 (+0.18) & 3.32 (+0.03) \\
16 & 2.01 (+0.13) & 2.50 (+0.09) & 3.19 (-0.11) \\
32 & 2.00 (+0.11) & 2.56 (+0.15) & 3.35 (+0.06) \\
\textbf{64} & \textbf{2.09 (+0.21)} &\textbf{ 2.61 (+0.20)} & \textbf{3.40 (+0.10)} \\

\bottomrule
\end{tabular}
\end{table}

\section{Full MIA Results}
\label{app:full_mia}
We report \tprat and AUC for each baseline MIA (\cref{tab:tpr_baseline_mias}, \cref{tab:auc_baseline_mias}, each improved MIA for IAR (\cref{tab:tpr_our_mias}, \cref{tab:auc_our_mias}) and each MIA for DMs (\cref{tab:tpr_dm}, \cref{tab:auc_dm}). Results are randomized over 100 experiments.

\begin{table}[h!]
    \centering
    \tiny
\caption{\textbf{\tprat for baseline MIAs.}}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{cccccccccccc}
\toprule
        Model & VAR-$\mathit{d}$16 & VAR-$\mathit{d}$20 & VAR-$\mathit{d}$24 & VAR-$\mathit{d}$30 & MAR-B & MAR-L & MAR-H & RAR-B & RAR-L & RAR-XL & RAR-XXL \\
\midrule

Loss~\citep{yeom2018lossmia} & 1.50{\tiny $\pm$0.16} & 1.67{\tiny $\pm$0.20} & 2.19{\tiny $\pm$0.21} & 4.95{\tiny $\pm$0.38} & 1.42{\tiny $\pm$0.21} & 1.48{\tiny $\pm$0.19} & 1.60{\tiny $\pm$0.21} & 1.76{\tiny $\pm$0.24} & 2.10{\tiny $\pm$0.27} & 3.38{\tiny $\pm$0.42} & 5.70{\tiny $\pm$0.55} \\
Zlib~\citep{carlini2021extractLLM} & 1.55{\tiny $\pm$0.20} & 1.74{\tiny $\pm$0.20} & 2.24{\tiny $\pm$0.24} & 5.77{\tiny $\pm$0.59} & 1.41{\tiny $\pm$0.22} & 1.49{\tiny $\pm$0.21} & 1.59{\tiny $\pm$0.22} & 1.91{\tiny $\pm$0.23} & 2.45{\tiny $\pm$0.26} & 4.21{\tiny $\pm$0.31} & 7.52{\tiny $\pm$0.57} \\
Hinge~\citep{bertran2024scalable} & 1.62{\tiny $\pm$0.19} & 1.72{\tiny $\pm$0.22} & 2.14{\tiny $\pm$0.23} & 4.09{\tiny $\pm$0.40} & --- & --- & --- & 1.81{\tiny $\pm$0.17} & 1.99{\tiny $\pm$0.19} & 2.94{\tiny $\pm$0.36} & 5.16{\tiny $\pm$0.63} \\
Min-K\%~\citep{shi2024detecting} & 1.58{\tiny $\pm$0.16} & 2.04{\tiny $\pm$0.25} & 3.22{\tiny $\pm$0.38} & 12.23{\tiny $\pm$1.13} & 1.69{\tiny $\pm$0.18} & 1.89{\tiny $\pm$0.16} & 2.18{\tiny $\pm$0.23} & 2.09{\tiny $\pm$0.24} & 2.86{\tiny $\pm$0.32} & 5.83{\tiny $\pm$0.52} & 13.48{\tiny $\pm$0.98} \\
SURP~\citep{zhang2024adaptive} & 1.53{\tiny $\pm$0.17} & 1.70{\tiny $\pm$0.20} & 2.23{\tiny $\pm$0.23} & 5.02{\tiny $\pm$0.43} & --- & --- & --- & 1.84{\tiny $\pm$0.18} & 2.12{\tiny $\pm$0.30} & 3.46{\tiny $\pm$0.46} & 5.82{\tiny $\pm$0.53} \\
Min-K\%++~\citep{zhang2024min} & 1.34{\tiny $\pm$0.18} & 2.21{\tiny $\pm$0.28} & 3.73{\tiny $\pm$0.34} & 14.90{\tiny $\pm$0.96} & --- & --- & --- & 2.36{\tiny $\pm$0.29} & 3.26{\tiny $\pm$0.30} & 6.27{\tiny $\pm$0.65} & 14.63{\tiny $\pm$0.87} \\
CAMIA~\citep{chang2024context} & 1.33{\tiny $\pm$0.18} & 1.76{\tiny $\pm$0.19} & 3.07{\tiny $\pm$0.35} & 16.69{\tiny $\pm$1.16} & 1.35{\tiny $\pm$0.19} & 1.38{\tiny $\pm$0.19} & 1.44{\tiny $\pm$0.23} & 1.51{\tiny $\pm$0.17} & 1.78{\tiny $\pm$0.15} & 1.99{\tiny $\pm$0.34} & 4.34{\tiny $\pm$0.51} \\


\bottomrule
\end{tabular}
\label{tab:tpr_baseline_mias}
\end{table}


\begin{table}[h!]
\centering
\tiny
\caption{\textbf{AUC for baseline MIAs.}}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{cccccccccccc}
\toprule
        Model & VAR-$\mathit{d}$16 & VAR-$\mathit{d}$20 & VAR-$\mathit{d}$24 & VAR-$\mathit{d}$30 & MAR-B & MAR-L & MAR-H & RAR-B & RAR-L & RAR-XL & RAR-XXL \\
\midrule
Loss~\citep{yeom2018lossmia} & 52.35{\tiny $\pm$0.35} & 54.53{\tiny $\pm$0.34} & 59.55{\tiny $\pm$0.35} & 75.45{\tiny $\pm$0.30} & 51.92{\tiny $\pm$0.36} & 53.33{\tiny $\pm$0.36} & 55.06{\tiny $\pm$0.34} & 54.92{\tiny $\pm$0.37} & 58.04{\tiny $\pm$0.37} & 65.59{\tiny $\pm$0.34} & 74.45{\tiny $\pm$0.30} \\
Zlib~\citep{carlini2021extractLLM} & 52.38{\tiny $\pm$0.38} & 54.59{\tiny $\pm$0.38} & 59.65{\tiny $\pm$0.37} & 75.67{\tiny $\pm$0.34} & 51.91{\tiny $\pm$0.39} & 53.32{\tiny $\pm$0.39} & 55.05{\tiny $\pm$0.38} & 55.27{\tiny $\pm$0.36} & 58.68{\tiny $\pm$0.35} & 66.85{\tiny $\pm$0.34} & 76.17{\tiny $\pm$0.30} \\
Hinge~\citep{bertran2024scalable} & 53.29{\tiny $\pm$0.39} & 56.83{\tiny $\pm$0.39} & 62.89{\tiny $\pm$0.39} & 77.36{\tiny $\pm$0.33} & --- & --- & --- & 57.07{\tiny $\pm$0.44} & 61.41{\tiny $\pm$0.44} & 71.48{\tiny $\pm$0.39} & 82.14{\tiny $\pm$0.29} \\
Min-K\%~\citep{shi2024detecting} & 53.77{\tiny $\pm$0.40} & 57.84{\tiny $\pm$0.44} & 65.49{\tiny $\pm$0.40} & 83.55{\tiny $\pm$0.30} & 51.87{\tiny $\pm$0.38} & 53.29{\tiny $\pm$0.38} & 55.05{\tiny $\pm$0.38} & 56.53{\tiny $\pm$0.38} & 61.21{\tiny $\pm$0.36} & 71.35{\tiny $\pm$0.32} & 82.33{\tiny $\pm$0.28} \\
SURP~\citep{zhang2024adaptive} & 50.46{\tiny $\pm$0.25} & 54.54{\tiny $\pm$0.38} & 59.60{\tiny $\pm$0.40} & 75.46{\tiny $\pm$0.34} & --- & --- & --- & 52.21{\tiny $\pm$0.40} & 58.02{\tiny $\pm$0.42} & 65.58{\tiny $\pm$0.41} & 74.50{\tiny $\pm$0.33} \\
Min-K\%++~\citep{zhang2024min} & 54.52{\tiny $\pm$0.41} & 57.93{\tiny $\pm$0.38} & 65.76{\tiny $\pm$0.38} & 85.33{\tiny $\pm$0.27} & --- & --- & --- & 57.82{\tiny $\pm$0.41} & 62.48{\tiny $\pm$0.38} & 75.61{\tiny $\pm$0.32} & 85.16{\tiny $\pm$0.26} \\
CAMIA~\citep{chang2024context} & 52.44{\tiny $\pm$0.44} & 55.12{\tiny $\pm$0.44} & 61.37{\tiny $\pm$0.42} & 80.16{\tiny $\pm$0.34} & 51.08{\tiny $\pm$0.42} & 51.96{\tiny $\pm$0.43} & 53.20{\tiny $\pm$0.38} & 51.40{\tiny $\pm$0.36} & 51.83{\tiny $\pm$0.39} & 59.28{\tiny $\pm$0.39} & 66.07{\tiny $\pm$0.36} \\

\bottomrule
\end{tabular}
\label{tab:auc_baseline_mias}
\end{table}



\begin{table}[h!]
    \centering
    \tiny
\caption{\textbf{\tprat for our improved MIAs for IARs.}}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{cccccccccccc}
\toprule
        Model & VAR-$\mathit{d}$16 & VAR-$\mathit{d}$20 & VAR-$\mathit{d}$24 & VAR-$\mathit{d}$30 & MAR-B & MAR-L & MAR-H & RAR-B & RAR-L & RAR-XL & RAR-XXL \\
\midrule
Loss~\citep{yeom2018lossmia} & 2.16{\tiny $\pm$0.26} & 5.95{\tiny $\pm$0.54} & 24.03{\tiny $\pm$1.91} & 86.38{\tiny $\pm$0.92} & 1.54{\tiny $\pm$0.22} & 1.81{\tiny $\pm$0.21} & 2.26{\tiny $\pm$0.26} & 2.86{\tiny $\pm$0.20} & 5.50{\tiny $\pm$0.39} & 16.58{\tiny $\pm$0.97} & 40.76{\tiny $\pm$1.87} \\
Zlib~\citep{carlini2021extractLLM} & 1.75{\tiny $\pm$0.17} & 4.87{\tiny $\pm$0.41} & 20.37{\tiny $\pm$1.19} & 83.99{\tiny $\pm$0.87} & 1.51{\tiny $\pm$0.21} & 1.80{\tiny $\pm$0.23} & 2.23{\tiny $\pm$0.27} & 2.52{\tiny $\pm$0.31} & 4.56{\tiny $\pm$0.39} & 13.91{\tiny $\pm$1.02} & 41.03{\tiny $\pm$1.96} \\
Hinge~\citep{bertran2024scalable} & 0.00{\tiny $\pm$0.00} & 0.00{\tiny $\pm$0.00} & 0.00{\tiny $\pm$0.00} & 0.00{\tiny $\pm$0.00} & --- & --- & --- & 2.50{\tiny $\pm$0.20} & 4.34{\tiny $\pm$0.39} & 10.59{\tiny $\pm$0.88} & 20.23{\tiny $\pm$1.85} \\
Min-K\%~\citep{shi2024detecting} & 0.05{\tiny $\pm$0.02} & 0.06{\tiny $\pm$0.02} & 0.14{\tiny $\pm$0.04} & 1.63{\tiny $\pm$0.13} & 2.09{\tiny $\pm$0.23} & 2.6{\tiny $\pm$0.28} & 3.40{\tiny $\pm$0.30} & 4.30{\tiny $\pm$0.33} & 8.66{\tiny $\pm$0.79} & 26.14{\tiny $\pm$1.22} & 49.80{\tiny $\pm$2.15} \\
Min-K\%++~\citep{zhang2024min} & 0.39{\tiny $\pm$0.06} & 1.40{\tiny $\pm$0.11} & 4.88{\tiny $\pm$0.20} & 37.90{\tiny $\pm$0.44} & --- & --- & --- & 4.19{\tiny $\pm$0.40} & 8.24{\tiny $\pm$0.66} & 23.04{\tiny $\pm$1.14} & 43.67{\tiny $\pm$2.32} \\
CAMIA~\citep{chang2024context} & 1.83{\tiny $\pm$0.25} & 5.46{\tiny $\pm$0.52} & 20.92{\tiny $\pm$1.14} & 72.77{\tiny $\pm$1.04} & 1.00{\tiny $\pm$0.17} & 0.97{\tiny $\pm$0.13} & 1.06{\tiny $\pm$0.15} & 1.63{\tiny $\pm$0.21} & 2.60{\tiny $\pm$0.27} & 6.77{\tiny $\pm$0.47} & 17.85{\tiny $\pm$1.20} \\
\bottomrule
\end{tabular}
\label{tab:tpr_our_mias}
\end{table}


\begin{table}[h!]
\centering
\tiny
\setlength{\tabcolsep}{3pt}
\caption{\textbf{AUC for our improved MIAs for IARs.}}
\begin{tabular}{cccccccccccc}
\toprule
        Model & VAR-$\mathit{d}$16 & VAR-$\mathit{d}$20 & VAR-$\mathit{d}$24 & VAR-$\mathit{d}$30 & MAR-B & MAR-L & MAR-H & RAR-B & RAR-L & RAR-XL & RAR-XXL \\
\midrule
Loss~\citep{yeom2018lossmia} & 61.73{\tiny $\pm$0.33} & 76.26{\tiny $\pm$0.30} & 92.20{\tiny $\pm$0.15} & 98.95{\tiny $\pm$0.05} & 52.25{\tiny $\pm$0.42} & 54.60{\tiny $\pm$0.41} & 57.35{\tiny $\pm$0.40} & 65.61{\tiny $\pm$0.35} & 75.83{\tiny $\pm$0.32} & 89.64{\tiny $\pm$0.21} & 96.17{\tiny $\pm$0.12} \\
Zlib~\citep{carlini2021extractLLM} & 57.91{\tiny $\pm$0.39} & 70.86{\tiny $\pm$0.33} & 88.69{\tiny $\pm$0.24} & 98.51{\tiny $\pm$0.07} & 52.23{\tiny $\pm$0.39} & 54.57{\tiny $\pm$0.39} & 57.33{\tiny $\pm$0.39} & 62.22{\tiny $\pm$0.42} & 72.19{\tiny $\pm$0.37} & 87.51{\tiny $\pm$0.22} & 95.46{\tiny $\pm$0.13} \\
Hinge~\citep{bertran2024scalable} & 52.67{\tiny $\pm$0.36} & 56.11{\tiny $\pm$0.36} & 62.48{\tiny $\pm$0.36} & 74.63{\tiny $\pm$0.30} & --- & --- & --- & 59.66{\tiny $\pm$0.39} & 68.09{\tiny $\pm$0.35} & 81.56{\tiny $\pm$0.29} & 90.62{\tiny $\pm$0.21} \\
Min-K\%~\citep{shi2024detecting} & 59.78{\tiny $\pm$0.34} & 70.43{\tiny $\pm$0.34} & 83.10{\tiny $\pm$0.25} & 90.16{\tiny $\pm$0.27} & 53.31{\tiny $\pm$0.40} & 56.34{\tiny $\pm$0.39} & 59.98{\tiny $\pm$0.38} & 66.81{\tiny $\pm$0.38} & 78.73{\tiny $\pm$0.32} & 91.36{\tiny $\pm$0.20} & 96.97{\tiny $\pm$0.10} \\
Min-K\%++~\citep{zhang2024min} & 57.10{\tiny $\pm$0.30} & 65.44{\tiny $\pm$0.29} & 78.74{\tiny $\pm$0.25} & 93.18{\tiny $\pm$0.16} & --- & --- & --- & 65.20{\tiny $\pm$0.36} & 75.37{\tiny $\pm$0.34} & 88.29{\tiny $\pm$0.23} & 95.84{\tiny $\pm$0.14} \\
CAMIA~\citep{chang2024context} & 56.37{\tiny $\pm$0.38} & 68.18{\tiny $\pm$0.31} & 84.83{\tiny $\pm$0.24} & 96.95{\tiny $\pm$0.09} & 50.86{\tiny $\pm$0.41} & 51.15{\tiny $\pm$0.41} & 51.75{\tiny $\pm$0.41} & 57.95{\tiny $\pm$0.40} & 63.17{\tiny $\pm$0.43} & 70.43{\tiny $\pm$0.39} & 83.55{\tiny $\pm$0.31} \\
\bottomrule
\end{tabular}
\label{tab:auc_our_mias}
\end{table}




\begin{table}[h!]
    \centering
    \tiny
    \setlength{\tabcolsep}{3pt}
    \caption{\textbf{\tprat of MIAs for DMs.}}
    \begin{tabular}{ccccccccc}
    \toprule
     & LDM & U-ViT-H/2 & DiT-XL/2 & MDTv1-XL/2 & MDTv2-XL/2 & DiMR-XL/2R & DiMR-G/2R & SiT-XL/2 \\
    \midrule
    Denoising Loss~\citep{carlini2023extracting} & 1.35{\tiny $\pm$0.14} & 1.30{\tiny $\pm$0.17} & 1.42{\tiny $\pm$0.17} & 1.55{\tiny $\pm$0.18} & 1.64{\tiny $\pm$0.17} & 0.91{\tiny $\pm$0.15} & 0.88{\tiny $\pm$0.15} & 1.02{\tiny $\pm$0.13} \\
    SecMI$_{stat}$~\citep{dm2_duan23bSecMI} & 1.30{\tiny $\pm$0.20} & 1.31{\tiny $\pm$0.19} & 1.49{\tiny $\pm$0.22} & 1.35{\tiny $\pm$0.17} & 1.52{\tiny $\pm$0.22} & 1.15{\tiny $\pm$0.21} & 1.05{\tiny $\pm$0.15} & 0.00{\tiny $\pm$0.00} \\
    PIA~\citep{kong2023efficient} & 1.25{\tiny $\pm$0.16} & 1.25{\tiny $\pm$0.19} & 1.59{\tiny $\pm$0.20} & 1.72{\tiny $\pm$0.20} & 2.07{\tiny $\pm$0.24} & 1.07{\tiny $\pm$0.11} & 1.09{\tiny $\pm$0.12} & 1.14{\tiny $\pm$0.14} \\
    PIAN~\citep{kong2023efficient} & 1.03{\tiny $\pm$0.14} & 1.17{\tiny $\pm$0.16} & 0.92{\tiny $\pm$0.12} & 1.22{\tiny $\pm$0.15} & 1.50{\tiny $\pm$0.20} & 1.04{\tiny $\pm$0.13} & 1.01{\tiny $\pm$0.12} & 1.09{\tiny $\pm$0.14} \\
    GM~\citep{dubinski2024cdicopyrighteddataidentification} & 1.25{\tiny $\pm$0.17} & 1.26{\tiny $\pm$0.17} & 1.34{\tiny $\pm$0.17} & 1.18{\tiny $\pm$0.16} & 1.47{\tiny $\pm$0.19} & 1.13{\tiny $\pm$0.15} & 1.16{\tiny $\pm$0.16} & 1.38{\tiny $\pm$0.18} \\
    ML~\citep{dubinski2024cdicopyrighteddataidentification} & 1.41{\tiny $\pm$0.16} & 1.36{\tiny $\pm$0.20} & 1.50{\tiny $\pm$0.18} & 1.70{\tiny $\pm$0.16} & 1.98{\tiny $\pm$0.26} & 1.01{\tiny $\pm$0.15} & 1.10{\tiny $\pm$0.14} & 1.14{\tiny $\pm$0.12} \\
    CLiD~\citep{zhai2024clid} & 1.55{\tiny $\pm$0.19} & 1.75{\tiny $\pm$0.22} & 2.08{\tiny $\pm$0.28} & 2.72{\tiny $\pm$0.39} & 4.91{\tiny $\pm$0.44} & 0.96{\tiny $\pm$0.14} & 0.90{\tiny $\pm$0.13} & 6.38{\tiny $\pm$0.64} \\
    \bottomrule
    \end{tabular}
    \label{tab:tpr_dm}
\end{table}



\begin{table}[h!]
    \centering
    \tiny
    \setlength{\tabcolsep}{3pt}
    \caption{\textbf{AUC for MIAs for DMs.}}
    \begin{tabular}{ccccccccc}
    \toprule
     & LDM & U-ViT-H/2 & DiT-XL/2 & MDTv1-XL/2 & MDTv2-XL/2 & DiMR-XL/2R & DiMR-G/2R & SiT-XL/2 \\
    \midrule
    Denoising Loss~\citep{carlini2023extracting} & 50.53{\tiny $\pm$0.41} & 50.36{\tiny $\pm$0.42} & 51.77{\tiny $\pm$0.43} & 51.25{\tiny $\pm$0.37} & 51.65{\tiny $\pm$0.37} & 46.25{\tiny $\pm$0.40} & 46.01{\tiny $\pm$0.40} & 47.25{\tiny $\pm$0.34} \\
    SecMI$_{stat}$~\citep{dm2_duan23bSecMI} & 49.84{\tiny $\pm$0.44} & 53.15{\tiny $\pm$0.43} & 55.15{\tiny $\pm$0.46} & 54.44{\tiny $\pm$0.38} & 56.80{\tiny $\pm$0.36} & 48.73{\tiny $\pm$0.45} & 48.73{\tiny $\pm$0.44} & 50.00{\tiny $\pm$0.00} \\
    PIA~\citep{kong2023efficient} & 48.97{\tiny $\pm$0.43} & 51.77{\tiny $\pm$0.44} & 53.18{\tiny $\pm$0.42} & 52.60{\tiny $\pm$0.44} & 54.68{\tiny $\pm$0.45} & 47.31{\tiny $\pm$0.42} & 47.16{\tiny $\pm$0.41} & 49.13{\tiny $\pm$0.44} \\
    PIAN~\citep{kong2023efficient} & 49.56{\tiny $\pm$0.43} & 50.99{\tiny $\pm$0.46} & 50.14{\tiny $\pm$0.43} & 49.96{\tiny $\pm$0.42} & 51.52{\tiny $\pm$0.38} & 49.85{\tiny $\pm$0.41} & 49.79{\tiny $\pm$0.43} & 50.17{\tiny $\pm$0.37} \\
    GM~\citep{dubinski2024cdicopyrighteddataidentification} & 51.51{\tiny $\pm$0.40} & 51.19{\tiny $\pm$0.42} & 50.46{\tiny $\pm$0.46} & 50.72{\tiny $\pm$0.39} & 48.85{\tiny $\pm$0.37} & 45.97{\tiny $\pm$0.45} & 45.86{\tiny $\pm$0.45} & 50.94{\tiny $\pm$0.38} \\
    ML~\citep{dubinski2024cdicopyrighteddataidentification} & 50.36{\tiny $\pm$0.41} & 51.16{\tiny $\pm$0.41} & 52.53{\tiny $\pm$0.45} & 50.42{\tiny $\pm$0.19} & 54.65{\tiny $\pm$0.38} & 46.26{\tiny $\pm$0.38} & 49.37{\tiny $\pm$0.41} & 49.83{\tiny $\pm$0.17} \\
    CLiD~\citep{zhai2024clid} & 52.50{\tiny $\pm$0.39} & 54.27{\tiny $\pm$0.41} & 56.16{\tiny $\pm$0.41} & 57.43{\tiny $\pm$0.41} & 62.54{\tiny $\pm$0.40} & 46.20{\tiny $\pm$0.38} & 45.95{\tiny $\pm$0.41} & 78.65{\tiny $\pm$0.30} \\
    \bottomrule
    \end{tabular}
    \label{tab:auc_dm}
\end{table}



\vspace{5cm}
\section{Full DI Results}
\label{app:full_di}

We report the outcome of DI for DMs in \cref{tab:di_dm}. As an additional observation, we note that contrary to DI for IARs, shifting from the classifier to an alternative feature aggregation increases the number of samples needed to reject $H_0$. This suggests, that the linear classifier remains
necessary for DMs.


\begin{table}[h!]
\caption{\textbf{DI for DMs.} We report the minimal number of samples needed to successfully reject $H_0$.}
    \centering
    \scriptsize
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{ccccccccc}
    \toprule
     & LDM & U-ViT-H/2 & DiT-XL/2 & MDTv1-XL/2 & MDTv2-XL/2 & DiMR-XL/2R & DiMR-G/2R & SiT-XL/2 \\
    \midrule
     DI for DM & 4000 & 700 & 400 & 300 & 200 & 2000 & 200 & 300 \\
    \midrule
     No Classifier & 5000 & 4000 & 3000 & 600 & 400 & 2000 & 2000 & 500 \\
    \bottomrule
    \end{tabular}
    \label{tab:di_dm}
\end{table}









\section{Mitigation Strategy}
\label{app:mitigation}


In this section we detail our privacy risk mitigation strategy.


\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/defense_eval.pdf}
    \caption{\textbf{Privacy-utility trade-off of our mitigation strategy.} We show that successfully defending VAR and RAR against MIA and DI requires adding noise that severely harms the performance. Interestingly, we are able to limit the extent of memorization for VAR, and fully defend MAR against MIA and DI.}
    \label{fig:defense}
\end{figure}


\subsection{Method}
Given an input sample $x$, we perturb the output of the IAR according to a noise scale $\sigma$, which we can adjust to balance privacy-utility trade-off. During inference, we add noise sampled from $\mathcal{N}(0,\sigma)$ to the output. For VAR and RAR, we add it to the logits, and for MAR we add them to the sampled continuous tokens.

We measure privacy leakage with our methods from~\cref{sec:our_priv_eval}. Specifically, we perform MIAs, DI, and the extraction attack. To quantify utility, we generate 10,000 images from the IARs, and compute FID~\citep{heusel2017gans} between generations and the validation set. Lower FID means better quality of the generations.

\subsection{Results}

Our results in~\cref{fig:defense} show that we can effectively lower the privacy loss by applying our mitigation strategy, however, this comes at a cost of significantly decreased utility, as highlighted by substantially increasing FID score. 

We are able to lower the MIAs success by more than half (Fig.~\ref{fig:defense}, left), with the biggest relative drop observed for RAR-XL, for which the \tprat drops from 26\% to \textbf{4.4\%}. Moreover, \textit{all} MAR models become immune to MIAs after noising their tokens, as \tprat drops to 1\% (random guessing) with $\sigma=0.001$. 
When we apply our defense to DI (Fig.~\ref{fig:defense}, second from the left), we have to increase $P$, the minimum number required to perform a successful DI attack, by an order of magnitude, with the biggest relative difference for the smallest models: VAR-16, and RAR-B, with an increase from 80 to 3000, and 200 to 8000, respectively. Such an increase means that the models are harder to attack with DI, \ie their privacy protection is boosted. Similarly to MIA, DI stops working for MAR models immediately.

Our method achieves limited success in mitigating extraction (Fig.~\ref{fig:defense}, third from the left). We are lowering the success of extraction attack only when adding significant amount of noise. However, for \varbig, which exhibits the biggest memorization, with $\sigma=1.0$ we successfully protect \textbf{93} out of 698 samples from being extracted without significantly harming the utility.
Our method, similarly to all defenses, suffers from lowered performance (Fig.~\ref{fig:defense}, right), as signal-to-noise ratio during generation gets worse when $\sigma$ increases.

\subsection{Discussion}

We show that we can mitigate privacy risks by adding noise to the outputs of IARs, at a cost of utility. Notably, all MARs become \textit{fully} immune to MIAs and DI with noise scale as small as $0.001$. This result supports previous insights from~\cref{sec:our_priv_eval}, in which we show that MARs are significantly less prone to privacy risks than VARs and RARs. We argue that logits leak significantly more information than continuous tokens, and thus, adding noise to the latter yields significantly higher protection, at a lower performance cost.

We acknowledge that our privacy leakage defense is a heuristic, and more theoretically sound approaches should be explored, \eg in the domain of Differential Privacy~\citep{dwork2006differential}. To the best of our knowledge, we make the first step towards private IARs.

\section{More About Memorization}
\label{app:more_memorization}

In this section we provide an extended analysis of memorization phenomenon in IARs. We show more examples of memorized images, highlight the relation between the prefix length $i$ and the number of extracted samples, and shed more light on our efficient extraction method, described in~\cref{sec:memorization}.

\subsection{More Memorized Images}
\label{app:more_memorization_images}
In~\cref{fig:more_memorization} we show a non-cherry-picked set of images memorized by IARs. In~\cref{fig:var30_mem_zero} we show an example of an image memorized verbatim by \varbig \textbf{without any prefix}, \ie only from the class label token. In~\cref{fig:mem_uni1} we show an image that has been memorized by both \varbig and RAR-XXL.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/mem_0_var30.png}
    \caption{\textbf{Image extracted from VAR-\textit{d}30 \textit{without prefix}.} (Left) memorized image, (right) generated image.}
    \label{fig:var30_mem_zero}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.975\linewidth]{figures/mem_uni1.png}
    \caption{\textbf{Images extracted from both VAR-\textit{d}30, and RAR-XXL.}}
    \label{fig:mem_uni1}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.975\linewidth]{figures/mem_uni2.png}
    \caption{\textbf{An image extracted from both VAR-\textit{d}30, and MAR-H.}}
    \label{fig:mem_uni2}
\end{figure}


\subsection{Prefix Length vs. Number of Extracted Images}
\label{app:more_memorization_i}

We analyze the effect of the prefix length on the number of extracted samples. As our method leverages conditioning on a part of the input sequence, in~\cref{fig:mem_prefix} we show an increase of extraction success with the increase in the length of the prefix. Notably, we start experiencing false-positives once the prefix length surpasses $30$ for \varbig and RAR-XXL, and $5$ for MAR-H. In effect, the results in~\cref{tab:mem_how_many} provide an upper bound of the success of our extraction method.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/prefix_length.pdf}
    \caption{\textbf{Prefix length and the number of extracted samples.} We show that with an increase of the prefix length, the success of our extraction method increases.}
    \label{fig:mem_prefix}
\end{figure}

\begin{table}[h]
    \centering
    \scriptsize
    \caption{Prefix length $i$ for our data extraction attack. We note that appending longer sequences leads to false positives, \ie the IARs start to generate images from the validation set.}
    \begin{tabular}{cccc}
    \toprule
        Model & \varbig & MAR-H & RAR-XXL \\
    \midrule
        Prefix length $i$ & 30 & 5 & 30 \\
    \bottomrule
    \end{tabular}
    \label{tab:prefix_length_models}
\end{table}

\subsection{Approximate distance vs. SSCD Score}
\label{app:more_memorization_distance}

In this section we underscore the effectiveness of our filtering approach.~\cref{fig:distance_vs_sscd} shows that the distances we design for the candidate selection process indeed correlates with the SSCD score. By focusing only on the top-$5$ samples for each class we effectively narrow our search to just $0.5\%$ of the training set, significantly speeding up the whole process.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/memorization_distance_score.png}
    \caption{\textbf{Distance function $d$ and the SSCD score.} We show that $d$ correlates with the final memorization score. This result makes our candidate selection process sound, and reduces the cost of extracting memorized samples.}
    \label{fig:distance_vs_sscd}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/mem_appendix.png}
    \caption{\textbf{Non-cherry-picked extracted images.} Odd columns from the left correspond to the original image, even to extracted. From left, the images are for VAR-\textit{d}30, RAR-XXL, and MAR-H.}
    \label{fig:more_memorization}
\end{figure}
