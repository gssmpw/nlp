

\begin{figure*}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/pareto_teaser.pdf}
    \caption{\textbf{Privacy-utility and generation speed-performance trade-off for IARs compared to DMs.} 1) IARs achieve better and faster image generation, but reveal more information to potential training data identification attacks.
2) In particular, large IAR models are most vulnerable.
3) In case of large IARs, even the identification of individual training samples (MIAs) has a high success rate.
4) MAR models are more private than other IARs. We attribute it to the inclusion of a diffusion module in this architecture.
}
    \vspace{-10pt}
    \label{fig:pareto}

\end{figure*}

\section{Introduction}
\label{sec:introduction}

The field of visual generative modeling has seen rapid advances in recent years, primarily due to the rise of Diffusion Models (DMs)~\citep{sohl2015deep} that achieve impressive performance in generating highly detailed and realistic images.
For this ability, they currently act as the backbones of commercial image generators~\citep{rombach2022high,midjourney,saharia2022photorealistic}.
Yet, recently, their performance was closely matched or further surpassed through novel image autoregressive models (IARs).
Over the last months, IARs have been achieving new state-of-the-art performance for class-conditional~\citep{var_tian2024visualautoregressivemodelingscalable,rar_yu2024randomizedautoregressivevisualgeneration,mar_li2024autoregressiveimagegenerationvector}
and text-conditional~\citep{han2024infinityscalingbitwiseautoregressive,tang2024hartefficientvisualgeneration,fan2024fluidscalingautoregressivetexttoimage} generation.
The crucial improvement of their training cost and generation quality results from the \textit{scaling laws} that previously were observed for large language models (LLMs) \citep{kaplan2020scaling} with which they share both a training paradigm and architectural foundation.
As a result, with more compute budget, and larger datasets, IARs can achieve better performance than their DM-based counterparts.



At the same time, the privacy risks of IARs remain largely unexplored, posing challenges for their responsible deployment. While privacy risks, such as the leakage of training data points at inference time, have been demonstrated for DMs and LLMs~\citep{carlini2021extractLLM,carlini2023extracting,huang2024demystifying,wen2024detecting}, no such evaluations currently exist for IARs. As a result, the extent to which IARs may similarly expose sensitive information remains an open question, underscoring the necessity for rigorous privacy investigations in this context.



To address this gap and investigate the privacy risks associated with IARs, we conduct a comprehensive analysis using multiple perspectives on privacy leakage. First, we develop a new membership inference attack (MIA)~\citep{shokri2017membershipinference}, which aims to determine whether a specific data point was included in an IAR's training set—a widely used approach for assessing privacy risks. We find that existing MIAs developed for DMs \citep{carlini2023extracting, dm2_duan23bSecMI, kong2023efficient, zhai2024clid} or LLMs~\citep{mattern2023membershipLLM, shi2024detecting}, are ineffective for IARs, as they rely on signals specific to their target model.
We combine elements of MIAs from DMs and LLMs into our new MIA based on the shared properties between the models. For example, we leverage the fact that IARs, similarly to LLMs, perform per-token prediction to obtain signal from every predicted token. However, while LLMs' training is fully self-supervised (e.g., by predicting the next word), the training of IARs can be conditional (based on a class or prompt) as in DMs. We exploit this property, previously leveraged for DMs by \cite{zhai2024clid}, and compute the difference in outputs between
conditional and unconditional inputs as an input to MIAs. This approach allows us to achieve a remarkably strong performance of \textbf{86.38\%} \tprat.

We employ our novel MIA to provide an efficient dataset inference (DI)~\citep{maini2021dataset} method for IARs. DI generalizes MIAs by assessing membership signals over entire datasets, providing a more robust measure of privacy leakage. Additionally, we optimize DI for IARs by eliminating the stage of MIA selection for a given dataset, which was necessary for prior DIs on LLMs~\citep{maini2024llmdatasetinferencedid} and DMs~\citep{dubinski2024cdicopyrighteddataidentification}. Since our MIAs for IARs consistently produce higher scores for members than for non-members, all MIAs can be utilized without any selection. This optimizations reduced the number of samples required for DI in IARs to as few as 6 samples, which is significantly fewer than at least 200 samples required for DI in DMs. 
Finally, we examine the privacy leakage from IARs through the lens of memorization~\citep{feldman2020does,wen2024detecting,huang2024demystifying}. Specifically, we assess the IARs’ ability to reproduce verbatim outputs from their training data during inference.  We experimentally demonstrate that the evaluated IARs have a substantial tendency to verbatim memorization by extracting 698 training samples from \varbig, 36 from RAR-XXL, and 5 from MAR-H. These results highlight the varying degrees of memorization across models and reinforce the importance of mitigating privacy risks in IARs. Together, these approaches form a comprehensive framework for empirically evaluating the privacy risks of IARs.




Our empirical analysis of state-of-the-art IARs and DMs across various scales suggests that IARs that match their DM-counterparts in image generative capabilities are notably more susceptible to privacy leakage. We also explore the trade-offs between privacy risks and other model properties. Specifically, we find that, while IARs are more cost-efficient, faster, and more accurate in generation than DMs, they empirically exhibit significantly greater privacy leakage (see \Cref{fig:pareto}) measured against SOTA privacy attacks tailored against the respective model types. These findings highlight a critical trade-off between performance, efficiency, and privacy in IARs.

In summary, we make the following contributions: %
\begin{itemize}[itemsep=0pt, topsep=0pt]
    \item Our new MIA for IARs achieves extremely strong performance of even \textbf{86.38\%} \tprat, improving over naive application of MIAs by up to \textbf{69\%} %
    \item We provide a potent DI method for IARs, which requires as few as \textbf{6} samples to assess dataset membership signal. 
    \item We propose an efficient method of training data extraction from IARs, and successfully extract up to \textbf{698} images.
    \item  IARs can outperform DMs in generation efficiency and quality but suffer \textbf{order-of-magnitude} higher privacy leakage in MIAs, DI, and data extraction compared to DMs that demonstrate similar FID.
\end{itemize}













