
\section{Background and Related Work}
\label{sec:background}


\textbf{Notation.} We first introduce the notation used throughout the remainder of this paper. Let \( C, H, W, N \) denote the number of channels, height, width, and sequence length, respectively. We represent an \textit{original image} as \( x \in \mathbb{R}^{C \times H \times W} \) and a \textit{generated image} as \( \hat{x} \in \mathbb{R}^{C \times H \times W} \). The \textit{tokenized image} is denoted as \( t \in \mathbb{N}^N \), while the \textit{generated token sequence} is represented as \( \hat{t} \in \mathbb{N}^N \).


\textbf{Image AutoRegressive modeling.}
Originally,~\citet{chen2020generative} defined image autoregressive modeling as: 
\begin{equation}
p(x) = \prod_{n=1}^N p(t_n \mid t_1, t_2, \ldots, t_{n-1}),
\label{eq:ar_task}
\end{equation}
where $N$ is the number of pixels in the image, $t_i$ is the value of $i^{th}$ pixel of image $x\sim\mathcal{D}_{\text{train}}$ (training data), where pixels follow raster-scan order, row-by-row, left-to-right. 
During training, the goal is to minimize negative log-likelihood:
\begin{equation}
L_{AR} = \mathbb{E}_{x\sim\mathcal{D}_{\text{train}}}\left[-\text{log}\left(p\left(x\right)\right)\right].
\label{eq:ar_loss}
\end{equation}
However, learning pixel-level dependencies directly is computationally expensive. 
To address the issue, VQ-GAN~\citep{esser2020taming} transforms the task from next-pixel to next-token prediction. First, the VQ-GAN's encoder maps an image into (lower resolution) latent feature vector, which is then quantized into a sequence of tokens, by a learnable codebook. In effect, the sequence length is short, which enables higher-resolution and high-quality generation. Then, tokens are generated and projected back to the image space by VQ-GAN's decoder. All the subsequent IARs we introduce, utilize tokens from VQ-GAN.
This token-based formulation aligns image generation more closely with natural language processing. Additionally, similarly to autoregressive language models such as GPT-2~\citep{radford2019language}, which generate text by sequentially predicting tokens, modern IARs also employ transformer-based~\citep{vaswani2017attention} architectures to model dependencies between image tokens.
We focus on the recent state-of-the-art IARs. 





\textbf{VAR}
~\citep{var_tian2024visualautoregressivemodelingscalable} is a novel approach to image generation, which shifts the focus of traditional autoregressive learning from next-token to next-scale prediction. Unlike classical IARs, which generate 1D token sequences from images by raster-scan orders, VAR introduces a coarse-to-fine multi-scale approach, encoding images into hierarchical 2D token maps and predicting tokens progressively from lower to higher resolutions. This preserves spacial locality and significantly improves scalability and inference speed.






\textbf{RAR}
~\citep{rar_yu2024randomizedautoregressivevisualgeneration} introduces bidirectional context modeling into IAR. 
Building on findings from language modeling, specifically BERT~\citep{devlin2019bertpretrainingdeepbidirectional}, RAR highlights the limitations of unidirectional approach, and enhances training by randomly permuting token sequences and utilizing bidirectional attention. RAR optimizes~\cref{eq:ar_loss} over all possible permutations, enabling the model to capture bidirectional dependencies, resulting in higher quality generations. 

\textbf{MAR}
~\citep{mar_li2024autoregressiveimagegenerationvector}  uses a small DM to model $p(x)$ from \cref{eq:ar_task}, and samples tokens from it during inference. MAR is trained with the following loss objective:
\begin{equation}
    L_{DM}=\mathbb{E}_{\epsilon,s}\left[||\epsilon-\epsilon_{\theta}\left(t_n^s\mid s,z\right)||^2\right],
    \label{eq:dm_loss}
\end{equation}
where $\epsilon\sim\mathcal{N}(\mathbf{0}, \mathbf{I})$, $\epsilon_{\theta}$ is the DM, $t_n^s=\sqrt{\bar{\alpha_s}}t_n+\sqrt{1-\bar{\alpha_t}}\epsilon$ and $\bar{\alpha_s}$ is DDIM's~\citep{songdenoising} noise schedule, $s$ is the timestep for diffusion process, and $z$ is conditioning input, obtained from the autoregressive backbone, from the previous tokens. This loss design allows MAR to operate with continuous-valued tokens, contrary to VAR and RAR, which use discrete tokens. 
MAR also integrates masked prediction strategies from MAE~\citep{he2022masked}, into the IAR paradigm. Specifically, MAR predicts masked tokens, based on unmasked ones, formulated as $p(x\cdot\neg M \mid x\cdot M)$, where $M\in[0,1]^{N}$ is random binary mask. Like to RAR, MAR utilizes bidirectional attention during training. Its autoregressive backbone differs from other IARs, as MAR employs a ViT~\citep{dosovitskiy2021imageworth16x16words} backbone.

\textbf{Sampling} for IARs is based on $p(x)$, which models the distribution of the next token conditioned on the previous ones in the sequence.\todo{needs explanation, for example, "the generating function $p(x)$ or something like that.} 
For VAR and RAR, operating on discrete tokens, the next token can be predicted via greedy or top-$k$ sampling. 
In contrast, MAR samples tokens by the DM module, which performs $100$ DDIM~\citep{songdenoising} denoising steps. During a single sampling step, VAR outputs a 2D token map, RAR predicts a single token, and MAR generates a batch of tokens.


