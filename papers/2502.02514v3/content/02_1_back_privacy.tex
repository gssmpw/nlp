\section{Privacy Evaluation Frameworks}
\label{sec:priv_eval}


We assess IARs' privacy risks from the three perspectives of membership inference, dataset inference, and memorization. 

\subsection{Membership Inference}
Membership Inference Attacks (MIAs)~\citep{shokri2017membershipinference} aim to identify whether a specific data point was part of the training dataset for a given machine learning model. 
Many MIAs have been proposed for DMs~\citep{dm2_duan23bSecMI,zhai2024clid,carlini2023extracting,kong2023efficient}, but these methods are tailored to DM-specific properties and do not transfer easily to IARs. For instance, some directly exploit the denoising loss~\citep{carlini2023extracting}, while others~\citep{kong2023efficient}, leverage discrepancies in noise prediction between clean and noised samples. CLiD~\citep{zhai2024clid} sources membership signal from the difference between conditional and un-conditional prediction of the DM. Since IARs are also trained with conditioning input, we leverage CLiD to design our MIAs in~\cref{sec:membership}.



MIAs are also popular against LLMs~\citep{mattern2023membershipLLM, shi2024detecting} where they often work with per-token logit outputs of the model.
For example, \citet{shi2024detecting} introduce the \mink metric, which computes the mean of lowest $k\%$-log-likelihoods in the sequence, where $k$ is a hyper-parameter.
Zlib~\citep{carlini2021extractLLM} leverages the compression ratio of predicted tokens using the \textit{zlib library}~\citep{zlib2004} to adjust the metric to the level of complexity of the input sequence. 
Hinge~\citep{bertran2024scalable} metric computes the mean distance between tokens' log-likelihood and the maximum of the remaining log-likelihoods. 
SURP~\citep{zhang2024adaptive} computes the mean of log-likelihood of the tokens with the lowest $k\%$-log-likelihoods in the sequence, where $k$ is some pre-defined threshold. 
\minkpp~\citep{zhang2024min} is based on \mink, but the per-token log-likelihoods are normalized by the mean and standard deviation of the log-likelihoods of preceding tokens. 
CAMIA~\citep{chang2024context} computes the mean of log-likelihoods that are smaller than the mean log-likelihood, and the mean of log-likelihoods that are smaller than the mean of the log-likelihoods of preceding tokens, as well as the slope of log-likelihoods. More detailed description of MIAs can be found in~\cref{app:mias_full}. While LLM MIAs seem to be a natural choice for membership inference on IARs, it is completely unclear whether approaches from the language domain transfer to IARs. In our work we show that the success of this transferability is limited (see~\cref{sec:membership}), 
hence, we design novel MIAs, by exploiting unique properties of IARs. 
Our methods achieve significant improvements over initial MIAs with up to \textbf{69\%} higher \tprat compared to the baselines.

\subsection{Dataset Inference}
Dataset Inference (DI)~\citep{maini2021dataset} aims to determine whether a specific dataset was included in a model's training set. 
Therefore, instead of focusing on individual data points like MIAs, DI aggregates the membership signal across a larger set of training points. With this strong signal, it can uniquely identify whether a model was trained on a given (private) dataset, leveraging strong statistical evidence. 
Similarly to MIAs, DI can serve as a proxy for estimating privacy leakage from a given machine learning model: DI provides insight into how easily one can determine which datasets were used to train a model, for instance, by analyzing the effect size from statistical tests. A higher success rate in DI indicates greater potential privacy leakage.

\textbf{Previous DI Methods.} For supervised models, DI involves the following three steps: (1) obtaining specific features from data samples, based on the observation that training data points are further from decision boundaries than test samples, then (2) aggregating the extracted information through a binary classifier, and (3) applying statistical tests to identify the model's train set.
This approach was later extended to self-supervised learning models~\citep{datasetinference2022neurips}, where training data representations differ from test data, and then to LLMs \citep{maini2024llmdatasetinferencedid} and DMs \cite{dubinski2024cdicopyrighteddataidentification} for identify the training datasets in large generative models. 
Since DI relies on model-specific properties, it is unclear how it can be applied to IARs. We propose how to make DI applicable and effective for IARs.\todo{I changed that sentence because this felt like wastly overselling. Among all, this was for me not a contribution--maybe the framing was still from an old version of paper.} 




\textbf{Setup for DI.}  
DI relies on two data sets: (suspected) member and (confirmed) non-member sets. First, the method extracts features for each sample using MIAs. Next, it aggregates the features for each sample, and obtains the final score, which is designed so that it should be higher for members. Then, it formulates the following hypothesis test: $H_0:\text{mean(scores of suspected member samples)}\leq\text{mean(scores of non-members)}$, and uses the Welch's t-test for evaluation. If we reject $H_0$ at a confidence level $\alpha=0.01$, we claim that we confidently identified suspected members as actual members of the training set. 

Since the strength of the t-test depends on the size of both sample sets, the goal is to reject \( H_0 \) \textit{with as few samples as possible}. Intuitively, as the difference in a model’s behavior between member and non-member samples increases, rejecting \( H_0 \) becomes easier. A larger difference also indicates greater information leakage, allowing us to use DI to compare models in terms of privacy risks. For instance, if model A allows rejection of \( H_0 \) with 100 samples, while model B requires 1000 samples, model A exhibits higher leakage than model B. Throughout this paper, we refer to the minimum number of samples required to reject the null hypothesis as \( P \).

\textbf{Assumptions about Data.} For the hypothesis test to be sound, the suspected member set and non-member set must be independently and identically distributed. Otherwise, the result of the t-test will be influenced by the distribution mismatch between these two sets, yielding a false positive prediction.


\subsection{Memorization}
\label{sec:memorization}

Memorization in generative models refers to the models' ability to reproduce training data exactly or nearly indistinguishably at inference time. While MIAs and DI assess if given samples were used to train the model, memorization enables extracting training data directly from the model~\citep{carlini2021extractLLM,carlini2023extracting}---highlights an \textit{extreme} privacy risk.

In the vision domain, a data point $x$ is memorized, if the distance $l(x,\hat{x})$ from the original $x$ and the generated $\hat{x}$ image is smaller than a pre-defined threshold $\tau$~\citep{carlini2023extracting}. We use the same definition when evaluating our extraction attack in~\Cref{sec:memorization}. 

Intuitively, in LLMs, memorization can be understood as the model’s ability to reconstruct a training sequence $t$ when given a prefix $c$~\citep{carlini2021extractLLM}. 
Specifically, $t=\text{argmax}_{t'\in\mathbb{N}^{N}}p_\theta(t'|c)$, where $p_\theta$ is the probability distribution of the sequence $t'$, parameterized by the LLM's weights $\theta$, akin to~\cref{eq:ar_task}. 
This formulation states we can extract the training sequence $t$ by constructing a prefix $c$ that makes the model output $t$, with greedy sampling. 

Similarly to LLMs, IARs complete an image given an initial portion of it (a prefix), which we leverage for designing our data extraction attack. In contrast, extraction from DMs can rely only on the conditioning input (class label or text prompt), which is both costly and highly inefficient, \eg work by~\citet{carlini2023extracting} requires to generate \textbf{175M} images in order to find just $50$ memorized images, and no memorization has been shown for other large DMs. In contrast, we extract up to \textbf{698} training samples from IARs by conditioning them on a part of the tokenized image, requiring only \textbf{5000} generations.




\todo{\franzi{@Antoni,we need a few paragraphs more pragraphs on background for how to measure memorization, the carlini extraction based on a prefix etc. I took a first stab, but we need asically everything that you build on.}
\antoni{How about now?}}



























