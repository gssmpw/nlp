\section{Our Methods for Assessing Privacy in IARs}
\label{sec:our_priv_eval}
\subsection{Tailoring Membership Inference for IARs}
\label{sec:membership}









\begin{table*}[h!]
    \centering
    \newcommand{\tightcolsep}{\setlength{\tabcolsep}{3pt}} %
    \tightcolsep %
    \scriptsize
    \caption{\textbf{Performance of our MIAs vs baselines.} We report the standard \tprat for best MIAs per model. \textit{Baselines} refers to a unmodified naive application of LLM-specific MIAs to IARs.}
    \begin{tabular}{cccccccccccc}
        \toprule
        \textbf{Model} & \textbf{VAR-\textit{d}16} & \textbf{VAR-\textit{d}20} & \textbf{VAR-\textit{d}24} & \textbf{VAR-\textit{d}30} & \textbf{MAR-B} & \textbf{MAR-L} & \textbf{MAR-H} & \textbf{RAR-B} & \textbf{RAR-L} & \textbf{RAR-XL} & \textbf{RAR-XXL} \\
        \midrule
        Baselines       & 1.62  & 2.21  & 3.72  & 16.68  & 1.69 & 1.89 & 2.18 & 2.36  & 3.25  & 6.27  & 14.62  \\
        Our Methods  & \textbf{2.16}  & \textbf{5.95}  & \textbf{24.03}  & \textbf{86.38}  & \textbf{2.09}  & \textbf{2.61}  & \textbf{3.40}  & \textbf{4.30}  & \textbf{8.66}  & \textbf{26.14}  & \textbf{49.80}  \\
        \midrule
        Improvement    & \textbf{+0.54} & \textbf{+3.73} & \textbf{+20.30} & \textbf{+69.69} & \textbf{+0.40} & \textbf{+0.73} & \textbf{+1.22} & \textbf{+1.94} & \textbf{+5.41} & \textbf{+19.87} & \textbf{+35.17} \\
        \bottomrule
    \end{tabular}
    \label{tab:mia_naive_vs_ours}
\end{table*}




\textbf{Baselines.} We comprehensively analyze how existing MIAs designed for LLMs transfer to IARs. Our results in \cref{tab:mia_naive_vs_ours} (detailed in \cref{app:full_mia}
) indicate that off-the-shelf MIAs for LLMs perform poorly when directly applied to IARs. 
We report the \tprat metric to measure the true positive rate at a fixed low false positive rate, which is a standard metric to evaluate MIAs~\citep{carlini2022lira}. For smaller models, such as \varsmall and MAR-B, all MIAs exhibit performance close to random guessing ($\sim1\%$). %
As model size and the number of parameters increase, the membership signal strengthens, improving MIAs' performance in identifying member samples. Even in the best case (CAMIA with \tprat of 16.69\% on the large {\varbig}), the results indicate that the problem of reliably identifying member samples remains far from being solved.
These findings align with results reported for other types of generative models, as demonstrated by \citet{maini2024llmdatasetinferencedid,zhang2024membership,duan2024membership} in their evaluation of MIAs on LLMs and by \citep{dubinski2024towards,zhai2024clid} for DMs, where the utility of MIAs for models trained on large datasets was shown to be severely limited.

\textbf{Our MIAs for VARs and RARs.} To provide powerful MIAs for IARs, we leverage the models' key properties. Specifically, we exploit the fact that IARs utilize classifier-free guidance~\citep{ho2022classifier} during training, \ie in the forward pass, images are processed both with and without conditioning information, such as class label. This distinguishes IARs from LLMs, which are trained without explicit supervision (no conditioning). Consequently, MIAs designed for LLMs fail to take advantage of this additional conditioning information present in IARs. We build on CLiD~\citep{zhai2024clid}, and compute $p(x|c)-p(x|c_{null})$, where $c$--class label, $c_{null}$--null class, and use this difference as an input to MIAs, instead of per-token logits. We differ from CLiD in the following way: (1) Our method works directly on $p(x)$, whereas CLiD uses model loss to perform the attack. (2) Our attack is parameter free--CLiD requires hyperparameter search and a set of samples to fit a Robust-Scaler to stabilize the MIA signal. We provide a more generalized approach, moreover our results in \cref{tab:mia_naive_vs_ours} demonstrate even up to a \textbf{69.69\%} increase in the \tprat for the VAR-\textit{d}30 model.

\textbf{Our MIAs for MARs.} Many MIAs for LLMs (Hinge, \minkpp, SURP) require logits to compute their membership scores. However, we cannot apply these MIAs to MAR since MAR predicts continuous tokens instead of logits. We instead use per-token loss values obtained from \cref{eq:dm_loss} to adapt other LLM MIAs (Loss, Zlib, \mink, CAMIA). 
As the tokens for MAR are generated using a small diffusion module, we can apply insights from MIAs designed for DMs and target the diffusion module directly in our attack. We detail our MIA improvements for MAR, which counter randomness from the diffusion process and binary masks.






\textit{Improvement 1: Adjusted Binary Masks.}
MAR extends the IAR framework by incorporating masked prediction strategies, where masked tokens are predicted based on visible ones. 
We hypothesize that adjusting the masking ratio during inference can amplify membership signals. We increase this parameter from 0.86 (training average) to 0.95, which improves MIA and suggests that an optimal masking rate exposes more membership information.


\textit{Improvement 2: Fixed Timestep.}~\citet{carlini2023extracting} reported that MIAs on DMs perform best when executed for a specific denoising step \( t \). Since tokens in MAR are generated using a small diffusion module, we can take advantage of this by executing MIAs at a fixed timestep \( t \) rather than a randomly chosen one. Interestingly, we find that \( t = 500 \) is the most discriminative, differing from the findings for full-scale DMs, for which $t=100$ gives the strongest signal~\citet{carlini2023extracting}.

\textit{Improvement 3: Reduced Diffusion Noise Variance.}  
The MAR loss in \cref{eq:dm_loss} exhibits high variance due to its dependence on randomly sampled noise $\epsilon$. To mitigate this, we increase the noise sampling count from the default 4 used during training to 64, computing the mean loss to obtain a more stable signal.

More detailed description of these improvements can be found in~\cref{app:mias_on_mar}. Our results in~\cref{tab:mar_mia_improv_abl} highlight the importance of our changes to evaluate MAR's privacy leakage correctly. Thanks to our improved MIAs we do not under-report the privacy leakage they exhibit.\todo{Antoni: The improvement is too small to state that imo}

\begin{table}[t!]
    \scriptsize
    \centering
    \caption{\textbf{Ablation of improvements to MAR MIAs.} Each modification further strengthens the membership signal. We report \tprat values and gains.}
    \begin{tabular}{cccc}
        \toprule
        \textbf{Method} & \textbf{MAR-B} & \textbf{MAR-L} & \textbf{MAR-H} \\
        \midrule
Baseline & 1.69 & 1.89 & 2.18 \\
+ Adjusted Binary Mask & 1.88 (+0.19) & 2.25 (+0.36) & 2.88 (+0.70) \\
+ Fixed Timestep & 1.88 (+0.00) & 2.41 (+0.17) & 3.30 (+0.42) \\
+ Reduced Noise Variance & \textbf{2.09 (+0.21)} & \textbf{2.61 (+0.20)} & \textbf{3.40 (+0.10)} \\
        \bottomrule
    \end{tabular}
    \label{tab:mar_mia_improv_abl}
\end{table}

\textbf{Overall Performance and Comparison to DMs}  
We present our results in \Cref{fig:pareto}, evaluate overall privacy leakage and compare IARs to DMs based on the \tprat of MIAs. For DMs we use the strongest attack available at the time of writing this paper--CLiD~\citep{zhai2024clid}. In general, smaller and less performant models exhibit lower privacy leakage, which increases with model size. Notably, \varbig and RAR-XXL achieve \tprat values of 86.38\% and 49.80\%, respectively, indicating a substantially higher privacy risk in IARs compared to DMs. In contrast, the highest \tprat observed for DMs is only 6.38\% for SiT-XL/2 (see also \cref{tab:tpr_dm}). \textit{Is it due to the MIAs being stronger for IARs, or are there inherent causes for the increased leakage?}~\cref{app:why_iars_leak_more} explains why IARs, by design, should suffer more leakage, and~\cref{app:unified_mia} gives evidence that DMs leak less under a \textit{unified} attack--an identical MIA for DMs and IARs. Furthermore, MIAs are significantly less effective at identifying member samples in MARs. We attribute this to MARâ€™s use of a diffusion loss function (\cref{eq:dm_loss}) for modeling per-token probability, which replaces categorical cross-entropy loss and eliminates the need for discrete-valued tokenizers.











