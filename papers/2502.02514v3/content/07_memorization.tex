\subsection{Extracting Training Data from IARs}
\label{sec:memorization}

To analyze memorization in IARs, we design a novel training data extraction attack for IARs.
This attack builds on elements of data extraction attacks for LLMs~\citep{carlini2021extractLLM} and DMs~\citep{carlini2023extracting}. 
Integrating elements from both domains is required since IARs operate on tokens (similarly to LLMs), which are then decoded and returned as images (similarly to DMs).
In particular, we make the observation that, on the token level, IARs exhibit a similar behavior that was previously observed for LLMs~\citep{carlini2021extractLLM}.
Namely, for memorized samples, they tend to complete the \textit{correct ending} of a token sequence when prompted with the sequence's prefix.
We exploit this behavior and 1) identify candidate samples that might be memorized, 2) generate them by starting from a prefix in their token space, and sampling the remaining tokens from the IAR, and finally 3) compare the generated image with the original candidate image.
We report a sample as memorized when the generated image is near identical to the original image.
In the following, we detail the individual building blocks of the attack.



\textbf{1) Candidate Identification.} 
To reduce the computational costs, we do not simply generate a large pool of images, but identify promising candidate samples that might be memorized, before generation.
Specifically, we feed an entire tokenized image $t$ into the IAR, which predicts the full token sequence $\hat{t}$ in a \textit{single step}. Then, we compute the distance between original and predicted sequence, $d(t,\hat{t})$, which we use to filter promising candidates.
This approach is efficient, since for IARs the entire token sequence can be processed at once, significantly faster than if we sampled them iteratively. For VAR and RAR we use per-token logits, and apply greedy sampling, with $d(t,\hat{t})=100-\frac{100\cdot\sum_{i=1}^{N}\mathbb{1}\left(t_i=\hat{t_i}\right)}{N}$--an average prediction error. 
For MAR, we sample $95\%$ of the tokens from the remaining $5\%$ unmasked in a single step, and set $d(t,\hat{t})=||t-\hat{t}||^2_2$, as MAR's tokens are continuous. 
Following the intuition that $\hat{t}$ is memorized if $\hat{t}=t$, for each model, for each class we select top-$5$ samples with the smallest $d$, and obtain $5000$ candidates per model. Our candidate identification steps greatly improves the extraction efficiency over previous approaches ~\cite{carlini2023extracting}.
We show the success of our filtering in~\cref{app:more_memorization_distance}.





\textbf{2) Generation.} Then, following the methodology established for LLMs by \citep{carlini2021extractLLM}. for each candidate we select the first $i$ tokens as a prefix. The parameter $i$ is a hyperparameter and we present our best choices for the models in \Cref{tab:prefix_length_models}. We perform iterative greedy sampling of the remaining tokens in the sequence for VAR and RAR, and for MAR we sample from the DM batch by batch. We do not use classifier-free guidance during generation. We note that our method \textit{does not} produce false positives, \ie we do not generate samples from the validation set.


\textbf{3) Assessment.} Finally, we decode the obtained $\hat{t}$ into images, and assess the similarity to the original $t$.
Following~\citet{wen2024detecting}, we use SSCD~\citep{pizzi2022self} score to calculate the similarity, and set the threshold $\tau=0.75$ such that every sample with a similarity $\geq\tau$ will be considered as memorized.















\begin{wraptable}{r}{0.4\linewidth} %
    \centering
    \vspace{-5pt}
    \small
    \newcommand{\tightcolsep}{\setlength{\tabcolsep}{2.3pt}} %
    \tightcolsep %
    \caption{\textbf{Count of Extracted Training Samples per IAR.}}
    \vspace{-5pt}
    \begin{tabular}{cccc}
        \toprule
        \textbf{Model} & \textbf{VAR-\textit{d}30} & \textbf{MAR-H} & \textbf{RAR-XXL} \\
        \midrule
        Count & 698 & 5 & 36 \\
        \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \label{tab:mem_how_many}
\end{wraptable}

\textbf{Results.}
In~\cref{fig:mem_main} we show example memorized samples from \varbig, RAR-XXL, and MAR-H. We are not able to extract memorized images from smaller versions of these IARs. In~\cref{tab:mem_how_many} we see that the extent of memorization is severe, with \varbig memorizing \textbf{698} images. We observe lower memorization for MAR-H and RAR-XXL, which is intuitive, as results from Sections~\ref{sec:membership}, and~\ref{sec:di} show that \varbig is the most vulnerable to MIA and DI. Surprisingly, there is no memorization in token space, \ie $t\neq\hat{t}$, we observe it only in the pixel space. 
We provide more examples of memorized images in~\cref{app:more_memorization_images}.






\label{sec:mem_results}
\begin{figure}[h]
\centering
    \includegraphics[width=1\linewidth]{figures/mem_teaser.png}
    \caption{\textbf{Extracted Training Samples.} We note that IARs can reconstruct verbatim images from their training data. The first row shows the original training samples and the second one presents the extracted images.}
    \label{fig:mem_main}
\end{figure}


\textbf{Memorization Insights.} Many memorized samples follow a pattern: their backgrounds deviate from the “default” or typical scene, as shown in~\cref{fig:mem_uni1} and~\cref{app:more_memorization_images}.
We hypothesize that when a prefix contains part of this “unusual” background, the IAR is conditioned to reproduce the specific training image that originally featured it.
Additionally, several extracted images appear as poorly executed center crops with skewed proportions--see, for instance, the wine bottle in \cref{fig:var30_mem_zero}. These findings suggest memorization is driven by distinct visual cues in the prefix and can lead to the generation of replicas of its training data.
Moreover, the same \textbf{5} samples were extracted from both \varbig and RAR-XXL, \ie the same 5 training images are memorized by both models. One sample is memorized by both \varbig and MAR-H (Fig. \ref{fig:mem_uni1} and \ref{fig:mem_uni2}),suggesting some images are more prone to memorization across architectures.

Our results contrast with findings on DMs~\citep{carlini2023extracting}, where extracting training data requires far more computation. The high memorization in IARs likely stems from their size, as \varbig has 2.1B parameters--more than twice the number of parameters in DMs investigated in prior work. Importantly, our results also show a link between IAR size and memorization, with bigger IARs memorizing more.
Scaling laws suggest that as IARs grow larger, their performance improves, but so does their tendency to memorize, making privacy risks more severe in high-capacity models.

