% CVPR 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\definecolor{cellcol}{gray}{.92}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{9343} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{GaussianMotion: End-to-End Learning of Animatable \\ Gaussian Avatars with Pose Guidance from Text}
%\title{Animate the Human: 3D Human Generation from Text \\ for Rendering Dynamic Scenes }
%\title{Generate and Animate 3D Humans from Text \\ with Deformable Gaussian Splatting}

% GaussPose: Combines "Gaussian" and "pose," suggesting dynamic, pose-dependent human models.a
% AniGauss: Animation + Gaussian의 결합으로, 애니메이션 가능한 Gaussian 모델임을 직관적으로 전달.
% GaussMotion: Gaussian 기반의 동작 가능성을 강조하며, 움직임을 표현.


%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{Gyumin Shim\\
Korea Advanced Institute of Science and Technology\\
{\tt\small shimgyumin@kaist.ac.kr}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Sangmin Lee\\
Sungkyunkwan University \\
{\tt\small sangmin.lee@skku.edu}
\and
Jaegul Choo\\
Korea Advanced Institute of Science and Technology\\
{\tt\small  jchoo@kaist.ac.kr}
}

\begin{document}
%\maketitle
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\centering
\includegraphics[width=1\linewidth]{Figures/teaser_2.pdf}
% \vspace{-2em}
\captionof{figure}{{\bf Examples of 3D human models generated by \ourmodel.}
%Our approach is based on deformable 3D Gaussian Splatting to render animated scenes from user-specified pose inputs.
Our method is able to generate high-quality Gaussian-based avatars from text and render animated scenes from user-specified pose inputs.
}
\label{fig:teaser}
}]


%\input{sec/0_abstract}    
%\input{sec/1_intro}
%\input{sec/2_formatting}
%\input{sec/3_finalcopy}
\begin{abstract}

In this paper, we introduce \ourmodel, a novel human rendering model that generates fully animatable scenes aligned with textual descriptions using Gaussian Splatting.
Although existing methods achieve reasonable text-to-3D generation of human bodies using various 3D representations, they often face limitations in fidelity and efficiency, or primarily focus on static models with limited pose control.
In contrast, our method generates fully animatable 3D avatars by combining deformable 3D Gaussian Splatting with text-to-3D score distillation, achieving high fidelity and efficient rendering for arbitrary poses.
By densely generating diverse random poses during optimization, our deformable 3D human model learns to capture a wide range of natural motions distilled from a pose-conditioned diffusion model in an end-to-end manner.
%Furthermore, we propose Adaptive Score Distillation, which effectively balances the challenges of over-saturation and high variance to achieve optimal results that generate realistic details while appropriately aligning with text.
Furthermore, we propose Adaptive Score Distillation that effectively balances realistic detail and smoothness to achieve optimal 3D results.
%Experimental results demonstrate that our approach surpasses existing baselines, producing high-quality textures in novel-pose animations and generating both diverse and realistic 3D human models from textual input.
Experimental results demonstrate that our approach outperforms existing baselines by producing high-quality textures in both static and animated results, and by generating diverse 3D human models from various textual inputs.


%In this paper, we introduce GaussianMotion, a novel human rendering model that generates fully animatable scenes aligned with textual descriptions using Gaussian Splatting. Although existing methods achieve reasonable text-to-3D generation of human bodies using various 3D representations, they often face limitations in fidelity and efficiency, or primarily focus on static models with limited pose control. In contrast, our method generates fully animatable 3D avatars by combining deformable 3D Gaussian Splatting with text-to-3D score distillation, achieving high fidelity and efficient rendering for arbitrary poses. By densely generating diverse random poses during optimization, our deformable 3D human model learns to capture a wide range of natural motions distilled from a pose-conditioned diffusion model in an end-to-end manner. Furthermore, we propose Adaptive Score Distillation that effectively balances realistic detail and smoothness to achieve optimal 3D results. Experimental results demonstrate that our approach outperforms existing baselines by producing high-quality textures in both static and animated results, and by generating diverse 3D human models from various textual inputs.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}



%3dhuman의 중요성   
The demand for reconstructing and rendering 3D avatars has surged with advancements in computer graphics, enabling a wide range of applications, including virtual reality and metaverse content.
Building on diverse 3D representations~\cite{wang2021neus, shen2021deep, mildenhall2021nerf, kerbl20233d}, numerous studies have explored the reconstruction of 3D human avatars from various data sources, including 3D scans~\cite{saito2019pifu, saito2020pifuhd, zheng2021pamir}, video sequences~\cite{weng2022humannerf, peng2021animatable, hu2024gauhuman}, single images~\cite{huang2022one, huang2024tech}, and even text~\cite{kolotouros2024dreamhuman, cao2024dreamavatar, liao2024tada, liu2024humangaussian}.
In particular, generating 3D human models from text is significantly challenging, as textual descriptions provide limited information about the 3D structure of the human body, which has complex articulations.

% diffusion 과 sds의 등장. human 모델에 적용 
As text-to-image diffusion models~\cite{rombach2022high, saharia2022photorealistic, podell2023sdxl} have emerged, capable of synthesizing diverse and realistic images from textual information, their applicability has significantly expanded across various research fields.
The introduction of score distillation by DreamFusion~\cite{poole2022dreamfusion} marked a breakthrough, enabling text-to-3D synthesis using 2D diffusion models without requiring labeled 3D data. 
Building upon this technique, subsequent studies~\cite{wang2024prolificdreamer,tang2023dreamgaussian,yi2023gaussiandreamer,lin2023magic3d} have further explored the generation of detailed 3D models from text in an unsupervised manner. 

%% 
The text-to-3D task includes the generation of 3D {\it human} models, demonstrating the potential of this approach to create realistic and detailed representations of human models based on textual descriptions.
%However, most existing approaches, particularly those using neural representations, often encounter challenges related to fidelity and efficiency when rendering high-resolution images. They also tend to focus on static models, which limits their applicability in dynamic scenarios where complex pose variation and animation are essential.
Recently, several approaches leveraging neural representations~\cite{mildenhall2021nerf} or mesh representations~\cite{shen2021deep} have been proposed~\cite{kolotouros2024dreamhuman, cao2024dreamavatar, liao2024tada, huang2024dreamwaltz, huang2024humannorm}. 
%However, these methods often face challenges in maintaining both fidelity and efficiency, particularly when rendering high-resolution images.
% 어떤 limitation인지 에 대한 추가설명 
However, these methods often face challenges in balancing both fidelity and efficiency. Neural representations typically incur high computational costs due to the large number of point samples along each ray, particularly when rendering high-resolution images, while mesh-based methods struggle to preserve fine-grained details due to limitations in resolution and structure.


% In contrast, HumanGaussian~\cite{liu2024humangaussian}, based on 3D Gaussian Splatting, has demonstrated unprecedented generation quality by capitalizing on the real-time and memory-efficient properties of Gaussian Splatting. These strengths also extend to the training process, enabling faster rendering of full-body human images. However, HumanGaussian focuses on static models during training, limiting its applicability in dynamic scenarios where handling complex pose variations and animations is critical.
% %as it struggles with poses not seen during training.
% GAvatar~\cite{yuan2024gavatar} is a concurrently proposed Gaussian-based method that further addresses multi-pose handling using a primitive-based 3D representation.
% However, GAvatar heavily relies on mesh representation to regularize the geometry of 3D Gaussian points, which compromises its flexibility and ability to represent complex geometries. \todo{fine grained, 머리카락,등등 예시, gavat 먼저 서술하고 humangaussian .. fully gaussian based ' ... }This reliance leads to difficulties in capturing fine details that are not adequately modeled by the mesh, resulting in suboptimal performance and an overly smoothed geometric structure.
With the emergence of 3D Gaussian Splatting~\cite{kerbl20233d}, a powerful 3D model that surpasses neural representations in terms of efficiency, several Gaussian-based human rendering methods have also emerged.
Yuan et al.\cite{yuan2024gavatar} proposed a hybrid method that combines mesh and Gaussian representations, using Signed Distance Functions (SDF)~\cite{park2019deepsdf} to regularize the opacity of the Gaussian.
%However, this is a hybrid method combining mesh and Gaussian representations, and cannot be considered a fully Gaussian-based approach.
HumanGaussian~\cite{liu2024humangaussian} introduced a fully Gaussian-based approach, demonstrating competitive generation quality and enabling efficient rendering of full-body human images during training and inference.
%by leveraging the inherent strengths of Gaussian Splatting. 
%These strengths also extend to the training process, enabling faster rendering of full-body human images. 
However, HumanGaussian focuses on static models, which limits its applicability in dynamic scenarios where handling complex pose variations and animations is critical.

 
%우리 방법 소개 
%In this paper, we present a novel human rendering model that addresses these limitations by generating fully animatable scenes aligned with textual descriptions using Gaussian Splatting.
%In this paper, we present a novel human rendering model that addresses these limitations by generating fully animatable scenes aligned with textual descriptions, leveraging the inherent strengths of Gaussian Splatting. 

%In this paper, we address these limitations with a novel human rendering model that generates fully animatable scenes from textual descriptions, leveraging the inherent strengths of Gaussian Splatting. 
%Our approach enables end-to-end training, successfully producing realistic and animatable Gaussian avatars without relying on regularization from other representations, thereby ensuring fine-grained geometric details.

%As a result, our method allows for the creation of diverse 3D human models based on the provided text, allowing for the rendering of high-fidelity images in dynamic motion sequences while maintaining computational efficiency.

To address the limitations of fidelity, efficiency, and pose variation, we propose \ourmodel, a novel human rendering model that generates realistic and animatable scenes from textual descriptions. Our approach enables end-to-end training of animatable Gaussian avatars without relying on regularization from other representations, capturing fine-grained geometric details purely through the strengths of Gaussian Splatting.

%technical 한 내용 뒤로 따로 뺌 
As depicted in \Figref{fig:framework}, our method integrates deformable 3D human Gaussian Splatting with pose-aware text-to-3D score distillation.
%Starting from random locations on the canonical SMPL mesh surfaces as initialization, 
Gaussian points in the canonical space are optimized to capture both the appearance aligned with the given text and pose articulation as the input pose changes.
A key innovation of our model is the use of densely generated random poses with explicit pose guidance during optimization. 
% This enables the deformable 3D human model to learn a wide range of poses distilled from a pre-trained diffusion model in an end-to-end manner.
% We utilize a pose-conditioned diffusion model~\cite{rombach2022high} as a prior model, where ControlNet~\cite{zhang2023adding} takes the pose image as input to generate pose-consistent images.
% This provides strong pose-aware guidance when complex pose images are rendered during training.
This allows the deformable 3D human model to effectively learn a diverse range of poses distilled from a pre-trained pose-conditioned diffusion model in an end-to-end manner, providing robust pose-aware guidance when rendering complex poses during training. 
Additionally, to achieve optimal results with realistic details, we propose Adaptive Score Distillation as an alternative to naive score distillation sampling (SDS)~\cite{poole2022dreamfusion},
%This approach addresses both the over-saturation issues of naive SDS and the high variance problems associated with improved score distillation methods.
%This approach achieves a balanced result that preserves fine details while minimizing undesired noise, balancing smoothness and high uncertainty.
which balances the preservation of fine details while minimizing undesired noise, effectively handling the trade-off between smoothness and high uncertainty.


%High variance can destabilize training, resulting in distorted outcomes due to high uncertainty.
% that significantly deviate from the text due to high uncertainty.
%The Adaptive Score Distillation effectively achieves optimal results by generating realistic details while ensuring appropriate alignment with text.

As a result, our method enables the creation of diverse 3D human models based on the provided text, capturing intricate texture details and supporting realistic animations according to user-specified input poses, all while maintaining computational efficiency. We validate our approach through extensive experiments, demonstrating that it significantly outperforms existing baselines.
%especially in random pose animation scenarios. 

\noindent\ In summary, our contributions are as follows:
\begin{itemize}
\setlength\itemsep{-0em}
    \item A novel human rendering model with deformable Gaussian Splatting to create 3D human models aligned with textual descriptions, capable of exhibiting a wide variety of motions.
    \item An innovative framework that optimizes Gaussian points by generating random poses during training, allowing the model to learn both detailed appearances from text descriptions and complex pose articulations through explicit pose guidance.
    %distilled from a pose-conditioned diffusion model.
    \item Adaptive Score Distillation, an improved strategy over naive SDS, effectively balancing the issues of over-saturation and high variance to achieve optimal results with realistic details.
\end{itemize}

%단순결합은 왜 어려운지... 

\begin{figure*}[t]
\centering
\begin{tabular}{@{}c}
\includegraphics[width=1\linewidth]{Figures/architecture-v8.pdf}
\end{tabular}
\caption{{\bf Overview of our proposed framework.}
Given a text prompt as input, we generate animatable 3D humans by modeling deformable Gaussian Splatting, where Gaussian points adapt their positions based on input poses. 
The points are defined in a canonical space and shared across different poses (observation spaces). Random poses are sampled to deform the Gaussian points and rendered as pose images to provide pose-aware guidance for the rendered images $\mathbf{x}$ through score distillation. 
After optimizing the Gaussian points to reflect the appearances described by the text prompt, fully animatable scenes are rendered based on user-specified input poses during inference.
}
\label{fig:framework}
\end{figure*}


\section{Related Works}
\subsection{3D Gaussian Avatar}

%Recently, numerous studies in 3D avatar modeling have increasingly leveraged Gaussian representations~\cite{kerbl20233d} to achieve high-quality, animatable human models that can respond dynamically to various movements and expressions.
%Drawing inspiration from various human deformation concepts derived from deformable neural representations~\cite{gao2023neural, peng2021neural, peng2021animatable, weng2022humannerf}, numerous methods~\cite{hu2024gauhuman, hu2024gaussianavatar, lei2024gart, qian20233dgs, zielonka2023drivable} have emerged, proposing innovative approaches to reconstructing human avatars using Gaussian representations.
Recently, with the emergence of 3D Gaussian Splatting~\cite{kerbl20233d}, which has demonstrated powerful performance in various 3D applications, numerous studies in 3D avatar modeling have increasingly leveraged this technique to create high-quality human models. A range of methods~\cite{hu2024gauhuman, hu2024gaussianavatar, lei2024gart, qian20233dgs, zielonka2023drivable} proposes innovative approaches for reconstructing human avatars using Gaussian representations that can respond dynamically to various movements and expressions. 
These studies draw inspiration from human deformation concepts derived from deformable neural representations~\cite{gao2023neural, peng2021neural, peng2021animatable, weng2022humannerf}, which address how 3D coordinates on a human model are deformed across different poses.
Furthermore, more sophisticated forms of 3D human avatars have been developed, such as ExAvatar~\cite{moon2024expressive}, which incorporates facial and hand expressions, and UV Gaussian~\cite{jiang2024uv}, a hybrid form of animatable avatar that jointly learns mesh deformation and 2D Gaussian textures.
After reconstructing an avatar from monocular or calibrated multi-view videos, these methods facilitate the rendering of scenes from arbitrary viewpoints and poses using the trained 3D Gaussian points during inference, leveraging the computational efficiency of Gaussian representations. 
%This capability makes them not only effective but also scalable for real-time applications.
In this work, we introduce a novel method that can produce an animatable Gaussian avatar from text without requiring any image ground truths.

%Gauhuman: Articu-lated gaussian splatting from monocular human video
%GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians
%Gart: Gaussian articulated template models
%“3DGSAvatar: Animatable Avatars via Deformable 3D Gaussian Splatting,
%Drivable 3D Gaussian Avatars,
%Expressive whole-body 3d gaussian avatar,
%PhysAvatar: Learning the Physics of Dressed 3D Avatars from Visual Observations
%Human Performance Modeling and Rendering via Neural Animated Mesh


\subsection{Text-to-3D Human Generation}

Text-to-3D is a popular task which is to generate 3D models from input textual descriptions without relying on text-3D paired data.
Early work utilizing CLIP~\cite{radford2021learning} embeddings to optimize 3D shapes~\cite{sanghi2022clip} or neural representations~\cite{jain2022zero} has successfully demonstrated that 3D objects can be generated solely from textual descriptions.
As DreamFusion~\cite{poole2022dreamfusion} introduces a method to distill priors from pre-trained 2D diffusion models for targeting 3D models, numerous text-to-3D methods~\cite{tang2023dreamgaussian, yi2023gaussiandreamer, wang2024prolificdreamer} have emerged, aiming to generate high-quality 3D models from input textual descriptions by leveraging various diffusion models.
These methodologies can be directly extended to the task of generating 3D {\it humans}, with DreamHuman~\cite{kolotouros2024dreamhuman} and DreamAvatar~\cite{cao2024dreamavatar} being among the first works in this area that incorporate score distillation to optimize the human neural radiance field (NeRF) model. 
They utilize a deformable human NeRF model to render animatable scenes generated from diverse input texts.
TADA~\cite{liao2024tada} leverages SMPL-X~\cite{SMPL-X:2019} for modeling shape and UV texture, allowing for the rendering of more detailed 3D avatars. 
Recently, HumanNorm~\cite{huang2024humannorm} and Deceptive-Human~\cite{kao2023deceptive} have pushed the boundaries of 3D quality by incorporating additional 3D priors, including depth, normals, and pose information of human shapes.
HumanGaussian~\cite{liu2024humangaussian} successfully integrates Gaussian representations into the text-to-3D human task by training Gaussian splats with score distillation in a stable manner. However, it lacks animation capabilities, as it is designed exclusively for training static poses.


\section{Preliminaries}

\subsection{3D Gaussian Splatting}

3D Gaussian Splatting~\cite{kerbl20233d} is a powerful 3D modeling technique that enables real-time rendering by representing objects or scenes as collections of Gaussian splats.
Each splat $\mathcal{G}$ is characterized by its position $\mathbf{x}$, opacity $\alpha$, color $c$, and covariance matrix $\Sigma$, which defines its shape and spread through a scaling matrix $\boldsymbol{S}$ and rotation matrix $\boldsymbol{R}$. 
The overall image can be rendered by projecting each 3D Gaussian splat $\mathcal{G}$ onto the image plane. The pixel color is determined by accumulating the alpha values of the Gaussian splats as follows:

\begin{equation}
 C=\sum_i\left(\alpha_i^{\prime} \prod_{j=1}^{i-1}\left(1-\alpha_j^{\prime}\right)\right) c_i,
\end{equation}

\noindent where $\alpha_i^{\prime}$ represents the visibility $\alpha_i$ of the $i$-th splat, weighted by the probability density of $i$-th projected 2D Gaussian at the target pixel, and $c$ denotes the color value computed from spherical harmonics coefficients. 
%The probability density of i-th projected 2D Gaussian is defined as 

It is mostly known for its real-time rendering while maintaining image quality compared to implicit neural representations.
While the original 3D Gaussians are optimized using a photometric loss based on the provided ground-truth pixels, our proposed method learns from the distillation loss derived from the diffusion model.


%%%%
% $G(\mathbf{x}; \mathbf{p}_i, \Sigma_i)$ is the Gaussian function given by:
% \begin{equation}
% G(\mathbf{x}; \mathbf{p}_i, \Sigma_i) = \frac{1}{(2\pi)^{d/2} |\Sigma_i|^{1/2}} \exp\left(-\frac{1}{2} (\mathbf{x} - \mathbf{p}_i)^\top \Sigma_i^{-1} (\mathbf{x} - \mathbf{p}_i)\right)
% \end{equation}
% where $d$ is the dimensionality of the space. This representation allows for efficient rendering and can capture complex geometries with varying scales.


\subsection{Score Distillation}

Numerous powerful text-to-image diffusion models~\cite{rombach2022high, saharia2022photorealistic, ramesh2022hierarchical} have been proposed, demonstrating unprecedented image quality achieved through training on billions of text-image pairs.
Building on these diffusion models, Score Distillation Sampling (SDS) was introduced by DreamFusion~\cite{poole2022dreamfusion}, which distills prior knowledge from pre-trained 2D diffusion models to optimize the target 3D model.
When provided with a pre-trained diffusion model $\epsilon_\phi$, SDS optimizes the parameters of the 3D model $\theta$ using the gradient of the loss, which is represented as:

\begin{equation}
\nabla_\theta \mathcal{L}_{\mathrm{SDS}}=\mathbb{E}_{t, \epsilon, y}\left[w(t)\left(\epsilon_\phi\left(\mathbf{x}_t ; y, t\right)-\epsilon\right) \frac{\partial \mathbf{x}}{\partial \theta}\right],
\label{eq:sds}
\end{equation}

\noindent where $\mathbf{x}$ denotes the image rendered by the 3D model, $\mathbf{x}_t$ represents the rendered image with Gaussian noise $\epsilon$ added, and $y$ is the textual input encoded by the text encoder.
At each training iteration, different values of $t$ are sampled steering the rendered images closer to the distribution of real images.
While SDS produces successful distillation results by generating diverse 3D renderings that align with the given text inputs, it faces an over-saturation problem that critically impacts the creation of realistic details in 3D objects.

% \begin{equation}
% \delta=\underbrace{\left[\epsilon_\phi\left(\mathbf{x}_t ;t\right)-\boldsymbol{\epsilon}\right]}_{\text {generative score } \delta_g}+s \cdot \underbrace{\left[\epsilon_\phi\left(\mathbf{x}_t ; y, t\right)-\epsilon_\phi\left(\mathbf{x}_t ; t\right)\right]}_{\text {classifier score } \delta_c}
% \end{equation}

% \begin{equation}
%  \nabla_\theta \mathcal{L}_{\mathrm{CSD}}=\mathbb{E}_{t, \epsilon, \mathbf{c}}\left[w(t)\left(\epsilon_\phi\left(\mathbf{x}_t ; y, t\right)-\epsilon_\phi\left(\mathbf{x}_t ; t\right)\right) \frac{\partial \mathbf{x}}{\partial \theta}\right] 
% \end{equation}

\section{Proposed Method}

%over view 
An overview of our proposed method is described in \Figref{fig:framework}. 
%We propose the human rendering model that can generate both the text-aligned 3D human model when given input textual description and fully animatable scenes with random motion sequences by optimizing the deformable 3D human model with score distillation. 
We introduce \ourmodel, a human rendering model that generates text-aligned 3D human avatars from input textual descriptions and creates fully animatable scenes with random motion sequences by optimizing the deformable 3D Gaussian points using score distillation.

\subsection{Pose Deformable 3DGS}
%non-rigid 얘기... 
% \begin{equation}
% \left\{\mathcal{G}_o\right\}=\mathcal{F}_{\theta_r}\left(\left\{\mathcal{G}_c\right\} ;\left\{\mathbf{B}_{\mathbf{b}}\right\}_{b=1}^B\right)
% \end{equation}

% \begin{equation}
%  \mathbf{x}_o=L B S_{\sigma_w}\left(\mathbf{x}_c ;\left\{\mathbf{B}_b\right\}\right)=\sum_{b=1}^B f_{\sigma_w}\left(\mathbf{x}_c\right)_b \mathbf{B}_b \mathbf{x}_c 
% \end{equation}

%4.1 구성 고민해보기 3dgs avatar 의 느낌을 너무 주는것이 맞는지 

%Inspired by 3DGS-avatar~\cite{qian20233dgs}, which creates animatable human avatars using 3D Gaussian Splatting, we adopted the rigid transformation method for Gaussian splats from this study. 
To create animatable human avatars using 3D Gaussian Splatting, Gaussian points are defined in the canonical space and shared across all different poses. 
Starting from random locations on the canonical SMPL mesh surfaces as initialization, these Gaussian points are optimized to learn both the appearance consistent with the provided text and the ability to articulate various poses as the input pose varies.
%We adopt a rigid transformation method~\cite{qian20233dgs} that deforms each Gaussian splat according to the pose input.
To model different poses, each Gaussian splat is deformed according to the pose input using a rigid transformation method~\cite{qian20233dgs}.
Specifically, the Linear Blend Skinning (LBS) function is applied to transform 3D Gaussian splats from the canonical space to the observation space, where the target pose is specified as input.
As the human skeleton consist of $B$ joints, the transformation $\mathbf{T}$ is represented as the weighted sum of rigid bone transformations as: 

% \begin{equation}
% \mathbf{x}^{c}=T(\mathbf{x}, \mathbf{p}) =\left(\sum_{k=1}^K w_k(\mathbf{x}) M^{\text{trg2can}}_k\right) \mathbf{x},
% \end{equation}

\begin{equation}
\mathbf{x}_o=\mathbf{T} \mathbf{x}_c = (\sum_{b=1}^B \mathbf{w}_b(\mathbf{x_c}) \mathbf{B}_b) \mathbf{x}_c,
\label{eq:transform}
\end{equation}

\noindent where $\mathbf{x}_c$ represents the position of Gaussian splats defined in the canonical space, and $\mathbf{x}_o$ denotes the position in the observation space.
$\mathbf{w}_b$ represents the blend weight for the $b$-th bone in the canonical space, which is further optimized.

$\mathbf{B}_b\in SE(3)$ is the transformation matrix of $b$-th skeleton part, mapping the bone's coordinates from the canonical space to the observation space.
Along with the transformation of positions, the rotation of Gaussian splats is also adjusted according to the equation $\mathbf{R}_o=\mathbf{T}_{1: 3,1: 3} \mathbf{R}_c$.
Note that the rigid bone transformation $\mathbf{B}$ can be computed from the given body pose (see Supplementary Material for details).
%blend weights are optimized by a neural skinning field using a MLP network which take the position coordinates of Gaussian splats as input. 

In 3DGS-avatar~\cite{qian20233dgs}, the blend weight $\mathbf{w}_b$ is optimized from scratch, as the ground truth provides clear guidance for determining the optimal values. 
In our method, on the other hand, the deformation must also be learned through distillation, resulting in weak guidance for optimizing the blend weights. 
Therefore, we propose to learn the residual blend weight relative to the SMPL blend weight $\mathbf{w}^{\mathbf{S}}$ as:

\begin{equation}
\mathbf{w}_b(\mathbf{x}_c)=\operatorname{norm}\left(f_{\theta_r}\left(\mathbf{x}_c\right)_b+\mathbf{w}^{\mathbf{S}}_{b}(\mathbf{x}_c)\right),
\label{eq:blend_weight}
\end{equation}

\noindent where $f_{\theta_r}$ is the MLP network which take the position coordinates of Gaussian splats as input to output residual blend weight values. 
The initial blend weight value $\mathbf{w}^{\mathbf{S}}$ can be computed based on the nearest vertex on the SMPL mesh by calculating the distance from the position coordinates of the Gaussian splats to each vertex.
The residual blend weight values are regularized by minimizing the $l2$-difference $\mathcal{L}_{\text{skinning}}$ between the predicted blend weight $\mathbf{w}_b$ and the initial SMPL blend weight $\mathbf{w}^{\mathbf{S}}$ across all positions of the Gaussian splats $\mathbf{x}_c$. 
By adopting this approach, we ensure that the pose transformation of SMPL is preserved from the initial stage, which effectively aids in converging to the appropriate blend weight.

\begin{figure*}[t]
\centering
%\def\arraystretch{0.2}
\begin{tabular}{@{}c}
\includegraphics[width=1\linewidth]{Figures/quali_static_4.pdf} \\
\end{tabular}
\caption{{\bf Qualitative comparison of 3D human models in a static A-pose.} We evaluate our approach against recent state-of-the-art baselines using different prompts. For each method, two images are rendered from frontal and side views, respectively.
%viewpoints of 0 and 60 degrees,
}
\label{fig:static}
\end{figure*}

%\subsection{Random Pose Sampling with Pose Guidance}
\subsection{Dynamic Pose Guidance}
Previous methods~\cite{huang2024humannorm, liu2024humangaussian} have limited pose control, as they are primarily trained on static poses. 
%To generate fully animatable scenes across random motion sequences, randomly posed images must be learned during the optimization of our 3D human model.
To guarantee natural animatable scenes across diverse motion sequences, randomly posed images must be learned during the optimization of our 3D human model.
At each training step, we randomly sample poses from a normal distribution, where the mean pose is represented by a star pose.
By observing the randomly posed images during training, the deformable 3D human model learns a wide range of poses distilled from a pre-trained diffusion model in an end-to-end manner.

To provide robust pose-aware guidance when the complex pose images are rendered, we utilize ControlNet~\cite{zhang2023adding} which takes a pose image $p$ to generate pose-consistent images. 
As shown in \Figref{fig:framework}, the random pose sampled in each training iteration not only transforms the 3D Gaussian splats but is also rendered as pose images to be input into ControlNet, which adds spatial conditioning controls to the pre-trained diffusion models.
This integration facilitates more coherent distillation, ensuring that the generated outputs align with the sampled poses.
%and enhances the overall realism and dynamism of the animations.




\begin{figure*}[t]
\centering
\def\arraystretch{0.2}
\begin{tabular}{@{}c}
\includegraphics[width=1\linewidth]{Figures/quali_pose_3.pdf} \\
\end{tabular}
\caption{{\bf Qualitative comparison of 3D human models in animated scenes.} We evaluate our approach against recent state-of-the-art baselines in a one-to-one manner. For each method, four images are rendered in different poses corresponding to each text prompt.}
\label{fig:animation}
\end{figure*}

\subsection{Adaptive Score Distillation}
%adaptive 용어 더 고민하기 weight 텀이 낫지 않나... 

By integrating pose guidance into score distillation, the score function in the gradient of the loss~\eqnref{eq:sds} is reformulated as $\epsilon_\phi\left(\mathbf{x}_t; y, t, p\right)$, where $p$ denotes the pose image conditioned on the diffusion model.
We then apply classifier-free guidance (CFG)~\cite{ho2022classifier} to score function and decompose the score matching difference into two components: the denoising score $\delta_n$ and the classifier score $\delta_c$, defined as:
%building on previous studies~\cite{yu2023text, katzir2023noise} that aim to enhance the output quality of SDS

% \begin{equation}
% \delta= \delta_n + s \cdot \delta_c \\
% = \left[\epsilon_\phi\left(\mathbf{x}_t ;t, p\right)-\boldsymbol{\epsilon}\right] +s \cdot \left[\epsilon_\phi\left(\mathbf{x}_t ; y, t, p\right)-\epsilon_\phi\left(\mathbf{x}_t ; t, p\right)\right],
% \end{equation} 

\begin{equation}
\resizebox{.9\hsize}{!}{$
\begin{split}
\delta &= \delta_n + s \cdot \delta_c \\
       &= \left[\epsilon_\phi\left(\mathbf{x}_t ;t, p\right)-\boldsymbol{\epsilon}\right] 
       %&\quad + s \cdot \left[\epsilon_\phi\left(\mathbf{x}_t ; y, t, p\right) - %\epsilon_\phi\left(\mathbf{x}_t ; t, p\right)\right]
       + s \cdot \left[\epsilon_\phi\left(\mathbf{x}_t ; y, t, p\right)-\epsilon_\phi\left(\mathbf{x}_t ; t, p\right)\right],
\end{split}
$}
\end{equation}
\noindent where $s$ is the guidance scale for CFG sampling.
%$p$ is the pose image conditioned on the diffusion model.

%While the classifier score ideally directs toward a local maximum in the probability density of noisy real images conditioned on $y$, the denoising score term introduces significant noise, which can lead to blurry outputs due to an averaging effect, as discussed in \cite{katzir2023noise}.
While $\delta_c$ aims to direct the model towards high-density regions of real images conditioned on $y$, the denoising score $\delta_n$ often introduces excessive noise, resulting in blurry outputs due to averaging effects~\cite{katzir2023noise}. 
%and associated with high uncertainty.
Previous methods~\cite{yu2023text, katzir2023noise} attempted to mitigate this by either removing the denoising score entirely or using a negative classifier score~\cite{katzir2023noise, liu2024humangaussian}. 
%However, we empirically found that simply removing the denoising score results in undesired effects in the output, such as noise or shadows.
%We also observed that several results deviated significantly from the original text descriptions, which can be attributed to the high uncertainty of the classifier score, as shown in \figref{fig:abl_ads}.
However, we empirically found that completely omitting the denoising score can produce artifacts, such as noise or shadow-like distortions, and that the generated outputs may deviate from the intended text description due to the high uncertainty of the classifier score (see \figref{fig:abl_ads}).


%The denoising scores are less noisy at higher timesteps, allowing higher-level semantics to remain relatively unaffected. However, they become very noisy at smaller timesteps, resulting in blurry outputs due to an averaging effect.
%We propose a simple yet effective technique: adaptive score distillation, where only the classifier score is applied for specific timesteps below $\tau$, where the denoising score causes an averaging effect.  
Based on the observation~\cite{katzir2023noise} that the denoising score  $\delta_n$ becomes overly noisy at smaller timesteps, while contributing to smoothness at larger timesteps,
we propose a simple yet effective technique called Adaptive Score Distillation (ASD). 
% In this approach, the denoising score is removed for timesteps below a threshold $\tau$ to mitigate its adverse effects. For timesteps beyond $\tau$, the denoising score is reintroduced to capture smooth high-level semantics. 
In this approach, the denoising score is selectively removed for timesteps below a threshold $\tau$ to mitigate noise, while being reintroduced for timesteps beyond $\tau$.
The score matching difference in ASD is defined as follows:

% \begin{equation}
% \resizebox{.9\hsize}{!}{$
% \delta = \begin{cases}
% \epsilon_\phi\left(\mathbf{x}_t ; y, t, p\right) - \epsilon_\phi\left(\mathbf{x}_t ; t, p\right), & \text{if } t < \tau \\
% \left[\epsilon_\phi\left(\mathbf{x}_t ; t, p\right) - \boldsymbol{\epsilon}\right] \\
% \quad + s \cdot \left[\epsilon_\phi\left(\mathbf{x}_t ; y, t, p\right) - \epsilon_\phi\left(\mathbf{x}_t ; t, p\right)\right], & \text{otherwise.}
% \end{cases}
% $}
% \label{eq:ads}
% \end{equation}

% \begin{equation}
% \resizebox{.5\hsize}{!}{$
% \delta = \begin{cases}
% \delta_c, & \text{if } t < \tau \\
% \delta_n + s \cdot \delta_c, & \text{otherwise.}
% \end{cases}
% $}
% \label{eq:ads}
% \end{equation}

\begin{equation}
\delta = \delta_c \cdot \mathbb{1}_{t < \tau} + (\delta_n + s \cdot \delta_c) \cdot \mathbb{1}_{t \geq \tau},
\label{eq:ads}
\end{equation}

\noindent where $\tau$ can be adaptively defined to balance realistic details and smoothness. 
%450 is used for our experiment. 

%With this straightforward technique, high-level semantics are smoothly aligned with text descriptions through the denoising score at higher timesteps, while low-level details are generated more aggressively solely by the classifier score at lower timesteps. This approach ensures both output quality and clarity, effectively minimizing undesired noise in the output.
This adaptive approach allows our model to leverage the strengths of both score components: ensuring smoother high-level semantic alignment through the denoising score at higher timesteps, while maintaining sharp, detailed features by relying solely on the classifier score at lower timesteps. This ensures both output quality and clarity, effectively minimizing undesired noise in the 3D output.
%consistency with the input text.


\subsection{Training Objective}

\noindent {\bf Scale Regularization} 
The proposed adaptive score distillation successfully generates highly detailed objects; however, we observed some blurry results in certain samples, which were caused by Gaussian splats with large scales.
The blurriness observed around the surface arises from the SDS-based supervision, which is significantly more stochastic than photometric loss, as also noted in HumanGaussian~\cite{liu2024humangaussian}.
To overcome this problem, we propose to apply scale regularization loss during the optimization. 
Typically, the scales of Gaussian splats are adjusted through pruning techniques in the adaptive density control process, which involves removing Gaussian points that exceed a specified scale.
However, this often results in excessive pruning of Gaussian points, leading to a decrease in resolution and challenges in maintaining a balance with densification.
Additionally, points may be pruned at the boundaries, which can lead to a collapse of the human shape as training progresses. We found that imposing a regularization loss to limit the scale size yields the best output quality, preserving both the resolution of the Gaussian points and clear boundaries.

The scale regularization loss is defined as follow: 
\begin{equation}
\mathcal{L}_{\text {scale}}=\frac{1}{|\mathcal{P}|} \sum_{p \in \mathcal{P}} \max \left\{\max \left(\boldsymbol{S}_p\right), r\right\}-r
\label{eq:scale}
\end{equation}
\noindent where $\boldsymbol{S}_p$ represents the scalings of the 3D Gaussians. This loss regularizes the size of the 3D Gaussian points to ensure they do not exceed $r$.

% {\bf Depth Smoothness} In addition to the scale regularization, we propose to apply depth smoothness loss ~\cite{}, which is widely used technique for many 3D tasks to acquire smooth depths. This loss can be applied optionally, and it stabilizes the trainingg the human body in the early stages. 
% vaporous bodies which leads to failure. 
% body 표면의 큰 면적이 opacity 가 낮아지거나 point들이 이동하면서 소실되는 현상이 있음. 이런경우 학습이 매우 불안정하게 학습됨. 

% ads 는 어떤 기준에서 비교하는게 맞는거지? 
% \begin{equation}
% \begin{split}
% \mathcal{L}_{TV} = \frac{1}{N} \sum_{i,j} \Bigg( \min\left( |D_{i+1,j} - D_{i,j}|, 0.2 \right) \\
%  + \min\left( |D_{i,j+1} - D_{i,j}|, 0.2 \right) \Bigg),
% \end{split}
% \end{equation}
% where $D_{i,j}$ is the pixel value of depth map of renderd image.
% N is the total number of pixels in the image 

\noindent {\bf Total Training Objective} Along with the proposed Adaptive Score Distillation $\mathcal{L}_{\text{ASD}}$, our total training objective functions for our method are written as: 
\begin{equation}
\begin{aligned}
&\mathcal{L}_{\text{total}}=\mathcal{L}_{\text{ASD}}+\lambda_{\text{scale}}\mathcal{L}_{\text{scale}} +\lambda_{\text{skinning}}\mathcal{L}_{\text{skinning}},
\end{aligned}
\end{equation}
\noindent where $\lambda_{\text{scale}}$ and $\lambda_{\text{skinning}}$ are hyperparameters determining the importance of each loss.





% \subsection{Training Details}
% \noindent {\bf Rendering Detail in the Training} To facilitate convergence, we render the front and back views of the canonical pose with a probability of 0.2, while images with random poses are rendered at a probability of 0.8 at each iteration. Additionally, we crop 256$\times$256 patches from the full-body rendered images to capture finer details in the local regions of the generated output. During training, the pose is randomly sampled using random noise scaled by 0.3 as the pose parameters, and the viewing angle is randomly selected from azimuth range of $[-180^\circ, 180^\circ]$

% \noindent {\bf Adaptive Density Control} 3DGS adaptively controls the resolution of Gaussian points through Adaptive Density Control (ADC). It periodically densifies or prunes points based on predefined criteria, such as position gradients, scaling, or opacity.  Beginning with an initial set of 70,000 sparse points on the SMPL mesh, We perform ADC every 100 iterations.
% We follow the same strategy as the original 3DGS, which clones and splits Gaussians with an average magnitude of view-space position gradients exceeding a threshold, which we set to 0.02 in our method. Additionally, we clone points that exceed 0.02 in mean distance from the $k$ nearest Gaussian points to avoid vaporous regions.
% Pruning is conducted at a scaling factor threshold of 0.1, and we also prune the 3D Gaussian points that deviate too far from the surface of the SMPL model. This is achieved by computing the distance between the nearest SMPL vertex and the positions of the Gaussian splats, which we set to a threshold of 0.1.

% gs22.5_asd450
% _anneal1000
% _whitebg
% _frontback0.2
% _th0.02
% _scale0.1
% _scaleloss0.01_1000
% _patch256face0.4a
% _randompose_0.3
% _rigid1
% _smplprune0.1
% _clone0.02_
% init0.8 
% {\bf View prompting} The Janus problem, characterized by the presence of multiple heads in the output, is a well-known phenomenon in many text-to-3D tasks. This issue arises because the diffusion model tends to generate outputs from a canonical viewpoint, influenced by the distribution of the training data. In the case of the human domain, this specifically refers to the canonical pose. We address this problem through view prompting, which augments the input prompt with back, side, and front views, and interpolates the text embeddings based on the azimuth sampled during training.

%-------------------------------------------------------------------------


\section{Experimental Results}

\subsection{Qualitative Evaluation}

%\noindent {\bf Qualitative Evaluation} 
We compare our method with recent state-of-the-art approaches in two settings: static poses and animations generated from motion sequences.
For the static pose setting, we compare our method with DreamHuman~\cite{kolotouros2024dreamhuman}, TADA~\cite{liao2024tada}, HumanNorm~\cite{huang2024humannorm}, and HumanGaussian~\cite{liu2024humangaussian}, which are based on neural~\cite{mildenhall2021nerf}, mesh~\cite{shen2021deep}, and Gaussian representations~\cite{kerbl20233d}, respectively. For the animation results, we compare our method with DreamHuman, GAvatar~\cite{yuan2024gavatar}, and HumanGaussian.
Note that the official code of DreamHuman and GAvatar are not released, we use the results from their project pages.
%and we used the official training code and pretrained models for TADA, HumanNorm, and HumanGaussian. \todo{here}

As shown in \Figref{fig:static}, our method generates the most realistic 3D human results in terms of fine geometry and texture details when compared to the other baselines.
DreamHuman suffers from overly smoothed results, as it is represented with solid colors only, lacking details such as wrinkles on the clothing.
TADA produces more detailed textures than DreamHuman, but its human shapes appear unrealistic.
In the case of HumanNorm, areas like the face are generated with more detail through their refinement process, but the overall quality of the full body remains too simplistic, similar to DreamHuman.
HumanGaussian delivers the most plausible performance overall among comparison baselines, but it struggles to capture fine details like hair (see row 1), and leaves behind blurry artifacts between the arms (see row 3).
In contrast, our method generates high-frequency details and delivers clean results without any blurry artifacts. Moreover, as we will show shortly, the difference becomes even clearer in animation results.

We present the animation results in \Figref{fig:animation}. 
For each example, images with 4 different poses are visualized for each method.
DreamHuman leverages the structure of deformable human NeRF during training, enabling it to exhibit natural pose transformations. However, it still lacks realistic details, resulting in overly smooth and simplistic texture quality.
GAvatar also produces natural animation results by handling multiple poses through a primitive-based transformation. However, it struggles to capture complex geometric details, such as hair or loose clothing, which is a persistent issue associated with mesh representations.
%Although it shows fine texture details, it shows misalignment between the geometry an apperance, where some geometry details are just embedded in the trexture. 
%However, GAvatar heavily relies on mesh representation to regularize the geometry of 3D Gaussian points, which compromises its flexibility and ability to represent complex geometries. \todo{fine grained, 머리카락,등등 예시, gavat 먼저 서술하고 humangaussian .. fully gaussian based ' ... }This reliance leads to difficulties in capturing fine details that are not adequately modeled by the mesh, resulting in suboptimal performance and an overly smoothed geometric structure.
HumanGaussian shows critical weaknesses in animation, as it is not designed to be animatable. 
While it provides a heuristic animation function by mapping Gaussian points to the SMPL-X~\cite{SMPL-X:2019} body mesh, 
%and as the body pose changes, the points' positions are updated using body surface normals and distance.
it suffers from severe artifacts during pose changes (e.g., around the arms) due to mapping errors.
%between the Gaussian points and the mesh surface. 
Additionally, it exhibits artifacts in occluded areas in the static pose, such as under the arms (see the 3rd example).
In contrast, our method produces realistic results in both geometry and texture for novel poses by capturing fine-grained details.
%exclusively using a Gaussian representation that captures fine-grained details.
%Additionally, our approach can generalize to novel poses without introducing any artifacts. 
%Note that we synchronized the poses only for HumanGaussian with ours, as the official code for DreamHuman and GAvatar has not been released. 



\begin{table}[t]
\centering
\renewcommand{\tabcolsep}{2.2mm}
\renewcommand{\arraystretch}{1.4}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{lccc}
\bottomrule
\bf Methods & \bf CLIP Score ↑ & \bf FID ↓ & \bf HPSv2 ↑\\ 
\toprule
HumanNorm~\cite{huang2024humannorm} & 27.81 & 5.41 & 0.256 \\
HumanGaussian~\cite{liu2024humangaussian} & 28.45 & 4.18 & 0.263\\ \hline
\rowcolor{cellcol} % 음영 추가
\bf GaussianMotion (ours) & \textbf{29.26} & \textbf{4.05} & \textbf{0.268} \\ 
\toprule
\end{tabular}
}
\caption{{\bf Quantitative comparisons with state-of-the-art text-to-3D human methods.} We evaluate CLIP score, FID, and HPS scores on rendered images.}
%\vspace{-1mm}
\label{tb:quanti}
\end{table}



\begin{figure}[t]
\centering
\def\arraystretch{0.2}
\begin{tabular}{@{}c}
\includegraphics[width=1\linewidth]{Figures/ablation_pose_2.pdf} \\
\end{tabular}
\caption{{\bf Ablation studies on pose guidance.} 
We present rendered images from 3D models trained with and without pose guidance, with the input pose shown in the first column. Additionally, we show generated images sampled from noised rendered images, with and without pose conditioning. }
\label{fig:abl_pose}
\end{figure}


\subsection{Quantitative Evaluation}
%\noindent {\bf Quantitative Evaluation} 
Following HumanNorm~\cite{huang2024humannorm} and HumanGaussian~\cite{liu2024humangaussian}, we conducted a quantitative evaluation to assess the quality of the 3D rendered images produced by our method. 
We selected 30 text prompts from the list provided by HumanGaussian (see Supplementary Material for details) to constitute our test set.
% First, we measured the Fréchet Inception Distance (FID)~\cite{heusel2017gans}, which quantifies the distance between two image datasets in feature space, typically defined as the datasets of real images and generated images.
% For the real image dataset, we sampled 10 images for each prompt using Stable Diffusion V1.5~\cite{rombach2022high}, while the multiview images rendered from 10 views within an azimuth range of $[-180^\circ, 180^\circ]$ were used for the generated image set.
First, we measured the Fréchet Inception Distance (FID)\cite{heusel2017gans}, which quantifies the similarity between feature distributions of real and generated images. We sampled 10 images per prompt using Stable Diffusion V1.5\cite{rombach2022high} for the real images, and used multiview images rendered from 10 azimuth angles in the range of $[-180^\circ, 180^\circ]$ for the generated image set.
Second, we evaluated the CLIP~\cite{hessel2021clipscore} and HPSv2~\cite{wu2023human} scores on the frontal views, which measure the similarity between embeddings encoded from the rendered images and the corresponding text. 
As detailed in \tabref{tb:quanti}, our method achieves the best score across all metrics, demonstrating that our method exhibits the best visual quality and consistency with the input text.


\noindent {\bf User Study} 
We conducted a user study to compare our method with recent state-of-the-art approaches~\cite{huang2024humannorm, liu2024humangaussian}. In this study, we presented pairs of multiview and animated scenes rendered by our method and one of the comparison methods. Using the same set of text prompts as in the quantitative evaluation, we created 30 A$\vs$B pairs and collected responses from 17 participants. Users were asked to assess three criteria: 1) Geometric Quality, 2) Texture Quality, and 3) Text Alignment. The results, summarized in \tabref{tb:user}, indicate that our method consistently outperforms the comparison methods across all criteria.


\begin{table}[t]
\centering
\renewcommand{\tabcolsep}{0.6mm}
\renewcommand{\arraystretch}{1.4}

% \resizebox{0.9\linewidth}{!}{
% \begin{tabular}{cccc}
% \hline
% Method & Texture Quality & Geometric Quality & Text Alignment \\ \hline
% HumanNorm & 1.312 & 1.312 & 1.312 \\
% HumanGaussian & 1.312 & 1.312 & 1.312 \\ \hline
% Ours &  &  & 1.312 \\ \hline
% \end{tabular}
% }
% \resizebox{0.5\linewidth}{!}{
% \begin{tabular}{cc}
% \hline
% Method & Animation Quality  \\ \hline
% HumanGaussian & 1.312 \\ \hline
% Ours &  a \\ \hline
% \end{tabular}
% }

\resizebox{0.999\linewidth}{!}{
\begin{tabular}{lccc}
\bottomrule
\bf Competitors & \bf Geometry & \bf Texture & \bf Text Consistency \\ \toprule
\vs. HumanNorm~\cite{huang2024humannorm} & 65.78 & 65.49 & 72.65 \\ 
\vs. HumanGaussian~\cite{liu2024humangaussian} & 84.51 & 89.31 & 67.45 \\ \toprule
\end{tabular}
}
\caption{{\bf User study with state-of-the-art text-to-3D human methods.} We present the preference percentage of our method compared to state-of-the-art methods.}
%\vspace{-1mm}
\label{tb:user}
\end{table}


\begin{figure}[t]
\centering
\def\arraystretch{0.2}
\begin{tabular}{@{}c}
\includegraphics[width=1\linewidth]{Figures/ablation_ads.pdf} \\
\end{tabular}
\caption{{\bf Ablation studies on $\tau$ changes in Adaptive Score Distillation.} Adjusting the $\tau$ value achieves a balanced result, preserving fine details while minimizing noise.}
\label{fig:abl_ads}
\end{figure}

\subsection{Ablation Studies}
%residual blend weight?
%We conduct some ablation studies on our two technical novelties: Adaptive Score Distillation (ADS) and scaling regularization. 
\noindent {\bf Pose Guidance}
We demonstrate the impact of incorporating pose guidance, which significantly enhances our method's performance, by comparing it with a setting where the model is trained without pose guidance (using only text input for the diffusion model). As shown in \figref{fig:abl_pose}, when trained without pose guidance, the model fails to capture the correct pose and orientation (for example, no face is generated in row 1). 
In the right part of the figure, we also show a generated image sampled from a rendered image with Gaussian noise added at $t=600$.  
The image generated without pose conditioning does not accurately reflect the input pose shown in the reference, implying that the diffusion model fails to produce pose-consistent scores during distillation, significantly hindering our 3D model from learning the correct poses.

\noindent {\bf Adaptive Score Distillation} 
Here, we demonstrate the effectiveness of Adaptive Score Distillation and show how the generation results vary with different $\tau$ values in \eqnref{eq:ads}. First, we follow the annealed distillation time schedule from~\cite{wang2024prolificdreamer}, reducing the maximum distillation timestep to 500 from the middle of the training process, which improves visual quality in the later stages.
We then decrease $\tau$ from 500 (corresponding to training with only the classifier score) in intervals of 100.
As shown in \Figref{fig:abl_ads}, the smoothness derived from the denoising score and the low-level details from the classifier score are interpolated as $\tau$ changes. 
With the highest $\tau$ value, our method fails to produce clean results, incorporating undesired noise such as floating artifacts and shadows. Additionally, we observe that some samples deviate significantly from the original text description (see row 1).
On the other hand, with lower value of $\tau$, which approach the naive SDS as it decrease, it shows results that are oversaturated and far from realistic.
In contrast, by adjusting the $\tau$ value (around 400), we were able to achieve the most balanced results, satisfying both fine details and a lack of noise.


% metric 도 보여줘야 되나?? 
\noindent {\bf Scale Regularization} 
We empirically found that scale regularization significantly improves generation quality by reducing blurriness and enhancing fine details through small Gaussian points. In our baseline setting (a), Gaussian points are pruned if their scaling factor exceeds 0.1. To evaluate the effect of scale regularization versus pruning, we also tested settings (b) and (c), where points with scaling factors above 0.02 and 0.01 are pruned, respectively. Lastly, setting (d) applies scale regularization without altering the pruning threshold.
While setting (a) suffers from blurriness that severely harms visual quality, settings (b) and (c) successfully remove it but at the cost of excessive pruning, which reduces resolution and causes loss of shape as training progresses. In contrast, as shown in (d), applying a regularization loss with $r$ set to 0.01 in \eqnref{eq:scale} yields the best visual quality, preserving  both the resolution of Gaussian points and clear boundaries.


\begin{figure}[t]
\centering
\def\arraystretch{0.2}
\begin{tabular}{@{}c}
\includegraphics[width=1\linewidth]{Figures/ablation_scale.pdf} \\
\end{tabular}
\caption{{\bf Ablation studies on scale regularization against scaling-based pruning.} We present frontal views rendered using each trained 3D model, which are trained with scaling-based pruning and scale regularization, respectively.}
\label{fig:abl_scale}
\end{figure}

%-------------------------------------------------------------------------
\section{Conclusion}

%In this paper, we introduced a novel approach for generating animatable 3D human models from text descriptions using Gaussian Splatting. Our method successfully addresses the limitations of existing approaches, such as limited fidelity, efficiency, and dynamic pose control, by leveraging deformable Gaussian Splatting combined with pose-aware score distillation. By densely sampling random poses during training, our model effectively learns diverse pose variations and intricate appearance details, resulting in high-quality, realistic renderings. The proposed Adaptive Score Distillation further enhances output quality, balancing detail and smoothness to mitigate issues such as over-saturation and high variance. Experimental results confirm that our method consistently outperforms state-of-the-art baselines, offering a powerful solution for creating efficient, detailed, and pose-flexible 3D avatars from a wide range of textual descriptions.

In this paper, we present \ourmodel, a novel approach for generating animatable 3D human models from text descriptions using Gaussian Splatting. Our method overcomes the limitations of existing approaches, including fidelity, efficiency, and dynamic pose control, by combining deformable Gaussian Splatting with pose-aware score distillation. By densely sampling random poses during training, our model learns diverse pose variations and fine details, resulting in high-quality renderings. The proposed Adaptive Score Distillation further refines output quality, balancing detail and smoothness. Experimental results demonstrate that our method outperforms state-of-the-art baselines, offering an efficient, detailed, and pose-flexible solution for creating 3D avatars from text.

{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

% WARNING: do not forget to delete the supplementary pages from your submission 
%\input{sec/supple}

\end{document}
