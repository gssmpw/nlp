\section{Related Works}
\subsection{3D Gaussian Avatar}

%Recently, numerous studies in 3D avatar modeling have increasingly leveraged Gaussian representations~\cite{kerbl20233d} to achieve high-quality, animatable human models that can respond dynamically to various movements and expressions.
%Drawing inspiration from various human deformation concepts derived from deformable neural representations~\cite{gao2023neural, peng2021neural, peng2021animatable, weng2022humannerf}, numerous methods~\cite{hu2024gauhuman, hu2024gaussianavatar, lei2024gart, qian20233dgs, zielonka2023drivable} have emerged, proposing innovative approaches to reconstructing human avatars using Gaussian representations.
Recently, with the emergence of 3D Gaussian Splatting~\cite{kerbl20233d}, which has demonstrated powerful performance in various 3D applications, numerous studies in 3D avatar modeling have increasingly leveraged this technique to create high-quality human models. A range of methods~\cite{hu2024gauhuman, hu2024gaussianavatar, lei2024gart, qian20233dgs, zielonka2023drivable} proposes innovative approaches for reconstructing human avatars using Gaussian representations that can respond dynamically to various movements and expressions. 
These studies draw inspiration from human deformation concepts derived from deformable neural representations~\cite{gao2023neural, peng2021neural, peng2021animatable, weng2022humannerf}, which address how 3D coordinates on a human model are deformed across different poses.
Furthermore, more sophisticated forms of 3D human avatars have been developed, such as ExAvatar~\cite{moon2024expressive}, which incorporates facial and hand expressions, and UV Gaussian~\cite{jiang2024uv}, a hybrid form of animatable avatar that jointly learns mesh deformation and 2D Gaussian textures.
After reconstructing an avatar from monocular or calibrated multi-view videos, these methods facilitate the rendering of scenes from arbitrary viewpoints and poses using the trained 3D Gaussian points during inference, leveraging the computational efficiency of Gaussian representations. 
%This capability makes them not only effective but also scalable for real-time applications.
In this work, we introduce a novel method that can produce an animatable Gaussian avatar from text without requiring any image ground truths.

%Gauhuman: Articu-lated gaussian splatting from monocular human video
%GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians
%Gart: Gaussian articulated template models
%â€œ3DGSAvatar: Animatable Avatars via Deformable 3D Gaussian Splatting,
%Drivable 3D Gaussian Avatars,
%Expressive whole-body 3d gaussian avatar,
%PhysAvatar: Learning the Physics of Dressed 3D Avatars from Visual Observations
%Human Performance Modeling and Rendering via Neural Animated Mesh


\subsection{Text-to-3D Human Generation}

Text-to-3D is a popular task which is to generate 3D models from input textual descriptions without relying on text-3D paired data.
Early work utilizing CLIP~\cite{radford2021learning} embeddings to optimize 3D shapes~\cite{sanghi2022clip} or neural representations~\cite{jain2022zero} has successfully demonstrated that 3D objects can be generated solely from textual descriptions.
As DreamFusion~\cite{poole2022dreamfusion} introduces a method to distill priors from pre-trained 2D diffusion models for targeting 3D models, numerous text-to-3D methods~\cite{tang2023dreamgaussian, yi2023gaussiandreamer, wang2024prolificdreamer} have emerged, aiming to generate high-quality 3D models from input textual descriptions by leveraging various diffusion models.
These methodologies can be directly extended to the task of generating 3D {\it humans}, with DreamHuman~\cite{kolotouros2024dreamhuman} and DreamAvatar~\cite{cao2024dreamavatar} being among the first works in this area that incorporate score distillation to optimize the human neural radiance field (NeRF) model. 
They utilize a deformable human NeRF model to render animatable scenes generated from diverse input texts.
TADA~\cite{liao2024tada} leverages SMPL-X~\cite{SMPL-X:2019} for modeling shape and UV texture, allowing for the rendering of more detailed 3D avatars. 
Recently, HumanNorm~\cite{huang2024humannorm} and Deceptive-Human~\cite{kao2023deceptive} have pushed the boundaries of 3D quality by incorporating additional 3D priors, including depth, normals, and pose information of human shapes.
HumanGaussian~\cite{liu2024humangaussian} successfully integrates Gaussian representations into the text-to-3D human task by training Gaussian splats with score distillation in a stable manner. However, it lacks animation capabilities, as it is designed exclusively for training static poses.