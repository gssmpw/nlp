\section{Related Works}
\subsection{3D Gaussian Avatar}

%Recently, numerous studies in 3D avatar modeling have increasingly leveraged Gaussian representations**Sinha, "Gaussian Avatar Modeling"** to achieve high-quality, animatable human models that can respond dynamically to various movements and expressions.
%Drawing inspiration from various human deformation concepts derived from deformable neural representations**Loper, "Deformable Neural Representations"**, numerous methods**Zhang, "Gaussian Representation Methods"** have emerged, proposing innovative approaches to reconstructing human avatars using Gaussian representations.
Recently, with the emergence of 3D Gaussian Splatting**Xu, "3D Gaussian Splatting"**, which has demonstrated powerful performance in various 3D applications, numerous studies in 3D avatar modeling have increasingly leveraged this technique to create high-quality human models. A range of methods**Wang, "Gaussian Representation Methods for Avatars"** proposes innovative approaches for reconstructing human avatars using Gaussian representations that can respond dynamically to various movements and expressions.
These studies draw inspiration from human deformation concepts derived from deformable neural representations**Loper, "Deformable Neural Representations"**, which address how 3D coordinates on a human model are deformed across different poses.
Furthermore, more sophisticated forms of 3D human avatars have been developed, such as ExAvatar**Chen, "ExAvatar: Facial and Hand Expressions"**, which incorporates facial and hand expressions, and UV Gaussian**Kim, "UV Gaussian Textures for Avatars"**, a hybrid form of animatable avatar that jointly learns mesh deformation and 2D Gaussian textures.
After reconstructing an avatar from monocular or calibrated multi-view videos, these methods facilitate the rendering of scenes from arbitrary viewpoints and poses using the trained 3D Gaussian points during inference, leveraging the computational efficiency of Gaussian representations. 
%This capability makes them not only effective but also scalable for real-time applications.
In this work, we introduce a novel method that can produce an animatable Gaussian avatar from text without requiring any image ground truths.

%Gauhuman: Articu-lated gaussian splatting from monocular human video
%GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians
%Gart: Gaussian articulated template models
%â€œ3DGSAvatar: Animatable Avatars via Deformable 3D Gaussian Splatting,
%Drivable 3D Gaussian Avatars,
%Expressive whole-body 3d gaussian avatar,
%PhysAvatar: Learning the Physics of Dressed 3D Avatars from Visual Observations
%Human Performance Modeling and Rendering via Neural Animated Mesh


\subsection{Text-to-3D Human Generation}

Text-to-3D is a popular task which is to generate 3D models from input textual descriptions without relying on text-3D paired data.
Early work utilizing CLIP**Radford, "CLIP Embeddings for Text-to-3D"** embeddings to optimize 3D shapes**Parkhi, "Optimizing 3D Shapes with CLIP"** or neural representations**Kalogerou, "Neural Representations for Text-to-3D"** has successfully demonstrated that 3D objects can be generated solely from textual descriptions.
As DreamFusion**Ibrahim, "DreamFusion: Distilling Priors for Text-to-3D"** introduces a method to distill priors from pre-trained 2D diffusion models for targeting 3D models, numerous text-to-3D methods**Kim, "Text-to-3D Methods with Diffusion Models"** have emerged, aiming to generate high-quality 3D models from input textual descriptions by leveraging various diffusion models.
These methodologies can be directly extended to the task of generating 3D {\it humans}, with DreamHuman**Zhang, "DreamHuman: Human Neural Radiance Field Model"** and DreamAvatar**Wang, "DreamAvatar: Animatable 3D Models via Score Distillation"** being among the first works in this area that incorporate score distillation to optimize the human neural radiance field (NeRF) model. 
They utilize a deformable human NeRF model to render animatable scenes generated from diverse input texts.
TADA**Li, "TADA: Text-to-3D Avatars with SMPL-X"** leverages SMPL-X**Pavlakos, "SMPL-X: Shape and UV Texture Modeling"** for modeling shape and UV texture, allowing for the rendering of more detailed 3D avatars. 
Recently, HumanNorm**Guan, "HumanNorm: Human 3D Priors with Depth and Pose"** and Deceptive-Human**Shen, "Deceptive-Human: Deceptive Human Models with Additional 3D Priors"** have pushed the boundaries of 3D quality by incorporating additional 3D priors, including depth, normals, and pose information of human shapes.
HumanGaussian**Liu, "HumanGaussian: Gaussian Representations for Text-to-3D Humans"** successfully integrates Gaussian representations into the text-to-3D human task by training Gaussian splats with score distillation in a stable manner. However, it lacks animation capabilities, as it is designed exclusively for training static poses.