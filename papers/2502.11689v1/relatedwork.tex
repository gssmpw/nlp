\section{Related Works}
Many studies have explored training methods to improve the judge ability of LLM, but some merely employ SFT to mimic the judge generation process without infusing the model with accurate preference knowledge \citep{kim2024prometheus,zhang2024generative} . Others directly utilize untrained base models to generate judge pairs, which may fail to produce high-quality responses due to the limitations of base models \citep{ye2024beyond,wang2024direct}. Additionally, nearly all related works lack an efficient data synthesis method, resulting in excessively large training datasets (600-900k entries) \citep{wang2024direct,cao2024compassjudger}. Finally, some work \cite{cao2024compassjudger} raised that that enhancing the judge capability of LLMs may concurrently benefit the improvement of their general abilities. However, this viewpoint has not been further validated. Other works \cite{ye2024beyond,liu2024skywork} focusing on LLM-as-a-judge just concentrate on metrics related to the judge capability of LLMs, and ignore the connections between judge and general abilities.

\begin{figure*}[t]
  \includegraphics[width=\linewidth]{./figures/main.pdf}
  \caption {Data synthesis and model training pipeline. Our pipeline contains 4 stages in order. $q$ :The question in preference dataset. $a_c$, $a_r$: The chosen and rejected answer to $q$ in preference dataset. $inst$: Judge instructions with $q$, $a_c$, $a_r$ merged in. $j_{CoT}$: The reasoning process when giving out a judge. $j_{res}$: Judge result towards judge instruction. $j_c$, $j_r$: The chosen and rejected judge answer in DPO training process.}
    \label{fig:main}
\end{figure*}