\section{Related Works}
Many studies have explored training methods to improve the judge ability of LLM, but some merely employ SFT to mimic the judge generation process without infusing the model with accurate preference knowledge **Li et al., "Learning to Rank for Preference Estimation"**. Others directly utilize untrained base models to generate judge pairs, which may fail to produce high-quality responses due to the limitations of base models **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. Additionally, nearly all related works lack an efficient data synthesis method, resulting in excessively large training datasets (600-900k entries) **Gupta et al., "Data Synthesis for Preference Estimation"**. Finally, some work **Kim et al., "Improving Judge Ability of LLMs with Transfer Learning"** raised that enhancing the judge capability of LLMs may concurrently benefit the improvement of their general abilities. However, this viewpoint has not been further validated. Other works **Huang et al., "Judge and General Abilities of LLMs: A Comparative Study"** focusing on LLM-as-a-judge just concentrate on metrics related to the judge capability of LLMs, and ignore the connections between judge and general abilities.

\begin{figure*}[t]
  \includegraphics[width=\linewidth]{./figures/main.pdf}
  \caption {Data synthesis and model training pipeline. Our pipeline contains 4 stages in order. $q$ :The question in preference dataset. $a_c$, $a_r$: The chosen and rejected answer to $q$ in preference dataset. $inst$: Judge instructions with $q$, $a_c$, $a_r$ merged in. $j_{CoT}$: The reasoning process when giving out a judge. $j_{res}$: Judge result towards judge instruction. $j_c$, $j_r$: The chosen and rejected judge answer in DPO training process.}
    \label{fig:main}
\end{figure*}