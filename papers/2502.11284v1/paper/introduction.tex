\section{Introduction}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{images/experiment.png}
    \caption{An illustration of the choices that introduce the data-allocation trade-off in LLM post-training. Given a fixed limited budget, one has to decide how much to allocate for annotating SFT data and how much for preference annotation (PFT data).}
    \label{fig:experiment_setup}
\end{figure}


The effectiveness of Large Language Models (LLMs) is largely driven by post-training methods after their initial pre-training phase \citep{achiam2023gpt, team2023gemini, chung2024scaling}. Two prominent approaches have emerged: Supervised Fine-Tuning (SFT), which uses direct instruction-response pairs, and Reinforcement Learning (RL) based methods like RLHF \citep{christiano2017deep, wei2022finetunedlanguagemodelszeroshot, bai2022training, bai2022constitutional}. SFT and RLHF are usually applied sequentially after pre-training, as part of the post-training pipeline for building many state-of-the-art models \cite{dubey2024llama, lambert2025tulu3pushingfrontiers}. However, recent work in alignment methods \citep{liu2024understandingreferencepoliciesdirect, ethayarajh2024ktomodelalignmentprospect} show that models can be aligned for some tasks without extensive SFT. Recent reasoning models  \citep{guo2025deepseek} have also improved a pre-trained model's analytical abilities with RL without SFT, raising questions about the necessity, role and extent of SFT in the post-training pipeline. Hence, the optimal allocation of limited training resources between SFT and RL-based approaches in post-training is an open question. Many RL methods involve training a separate reward model to iteratively generate preferences online \citep{christiano2017deep, christiano2023deepreinforcementlearninghuman}. However, approaches based on preference finetuning (PFT) \citep{rafailov2024direct, ethayarajh2024kto} can be used with offline preference-annotated data without training a reward model, while being competitive in performance. \citep{lambert2025tulu3pushingfrontiers, allal2025smollm2}. Understanding how these methods perform under resource constraints is crucial for researchers and practitioners seeking to maximize their performance under strict finetuning budgets. In particular, we focus on measuring their effectiveness under fixed data-annotation budgets, since the cost of collecting and annotating human data is substantial as compared to the compute costs of efficiently finetuning models on the collected data. See appendix \ref{compute_costs} for an estimate of this from our experiments. Efforts aimed at data collection for SFT and PFT methods also present an interesting practical trade-off. SFT typically requires expert demonstrations, which can be expensive and time-consuming to collect but provide a direct signal of the desired model behavior. PFT requires preference annotated data that consists of comparative judgments, which may be easier to obtain but potentially offer less direct supervision \citep{bai2022traininghelpfulharmlessassistant}. We outline this decision-making process in Figure \ref{fig:experiment_setup} and study it from the lens of the theory of consumer choice \citep{0458bef5-f1a5-33b9-8a87-02cfef6aa798, 721839b0-7fd6-3496-8a6f-a05763670acd}. Based on this, we formulate three research questions to understand this trade-off:

\begin{enumerate}
    \item \RQOne
    \item \RQTwo
    \item \RQThree
\end{enumerate}

We examine four tasks commonly used to evaluate finetuning methods in prior works: Summarization, Helpfulness, Instruction Following, and Grade School Math. Our analysis spans training budgets from 100 to 20,000 examples per task, with different allocation ratios between supervised and preference data, over multiple model families and sizes, to examine the Pareto-optimal frontier of performance versus the budget. This comprehensive evaluation allows us to identify optimal strategies for allocating resources between these fine-tuning approaches. Key takeaways from our extensive analysis (which includes over 1,000 unique finetuning runs) across different tasks, model sizes and training data budgets:

\begin{itemize}
    \item SFT demonstrates superior performance in low-data regimes (under 1,000 examples), while the benefits of combining it with preference tuning become more apparent in large-data regimes. 

    \item Across various configurations of model sizes, budgets and tasks, we find that devoting a higher ratio of the budget towards preference data (e.g., 3:1) with respect to SFT data yields the best performance while being cost effective. 

    \item We also analyze the cold-start problem while running preference finetuning directly on the base model, and find that the presence of even minimal SFT data (<10\% of the data-budget), is highly beneficial. 
    
\end{itemize}

These findings have important implications for researchers and practitioners developing LLMs under budget constraints, suggesting that while devoting most resources to preference finetuning is quite cost effective, a hybrid approach with a small amount of high-quality SFT data complemented by larger amounts of preference data may be optimal. 
