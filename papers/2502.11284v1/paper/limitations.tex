\section{Limitations} 

In this work, we compared the effectiveness and interplay of SFT and RL-based methods, under fixed data constraints. In particular, we chose offline methods like DPO and KTO as the baseline implementation of the RL method because it eliminates the need for reward modeling or iterative finetuning. This means that the process of development is limited to collecting an offline dataset and fientuning it - making it the most fair comparable to SFT in terms of implementation effort, compute costs and annotation efforts. Since this baseline RL method shows optimal performance over SFT, we hope that this motivates future work to study more complex RL-based methods and their interplay with SFT. In addition, we used GPT4o annotation for synthetic data generation, and also for evaluating Summarization and Helpfulness, which could include potential biases inherited from the model. 

In addition, we limited the size of the model to under 10 Billion parameters, to keep the finetuning cost low enough to ignore as compared to the data annotation costs. In addition, it would be extremely compute resource intensive to run thousands of finetuning runs with larger model sizes like 70B parameters. We hope that future work would study the scaling trends of RL-based methods against different model sizes, and also study the compute-data trade-off in-depth.
