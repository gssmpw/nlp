\section{Conclusion}

LLM post-training is a complex, multi-faceted approach, where trade-offs between various practical choices under resource constraints are often empirically determined. In this work, we comprehensively studied Supervised Finetuning (SFT) and Preference Finetuning (PFT) approaches, and their relative performance merits under fixed resource training data budget constraints. Our analysis involving various tasks, model sizes, and algorithm choices reveals a nuanced relationship between SFT and PFT across different data budget regimes. In low data scenarios ($n < 1,000$), SFT proves superior, while larger data budgets ($n > 10,000$) benefit from a pipeline of SFT followed by PFT, with higher proportions of preference data allocation. Notably, we identified a "cold-start problem" while applying PFT directly on the base model on every task we studied. However, allocating even a minimal amount of data budget to SFT before transitioning to PFT (even as low as $10\%$) can sometimes provide substantial benefits. We also note that in some analytical reasoning tasks such as mathematics, even the most optimal SFT-PFT allocation provides modest improvements over just SFT. These insights are particularly valuable for researchers and practitioners working with limited resources, as they provide clear guidance to optimize data budget allocation. 

