\section{Related Work}

Several works have studied the trade-offs that arise when choosing different strategies for language model training under a fixed budget, like pretraining v/s finetuning  \citep{bai-etal-2021-pre} and finetuning v/s distillation from a larger teacher model \citet{kang-etal-2023-distill, busbridge2025distillationscalinglaws}. However, large-scale pre-training of modern LLMs is extremely resource intensive, and post-training for specific tasks on top of pre-trained models has achieved state-of-the art results \citep{dubey2024llama, guo2025deepseek, lambert2025tulu3pushingfrontiers} This step involves SFT and RL-based Preference Finetuning, whose data requirements represent significant costs, and we study the trade-offs that arise when data budget for this is constrained. 


Other works study the data and compute costs associated with two methods in isolation \citep{Li_2023, Gilardi_2023, liu2024bestpracticeslessonslearned, tan-etal-2024-large, raghavendra2024revisitingsuperficialalignmenthypothesis, chan2024balancingcosteffectivenesssynthetic}. In addition, \citet{wang2023selfinstructaligninglanguagemodels}  study cost-efficiency in human and synthetic data generation for Supervised Finetuning, and \citet{pmlr-v235-lee24t} study this for Preference Finetuning respectively. In addition, \citet{ivison2024unpackingdpoppodisentangling} study the effects of high-quality preference data on different RL-methods like DPO and PPO. There is also research studying the interplay between these two methods on forgetfulness \citep{fernando2025mitigatingforgettingllmsupervised}, generalization \citep{kirk2024understanding}, alignment \citep{saeidi2025insightsalignmentevaluatingdpo} and factors like the strength of the reference model \citet{liu2024understandingreferencepoliciesdirect}. In our work, we contextualize and study the SFT-PFT trade-offs under a fixed data-budget constraint to find the optimal strategy to allocate between the two.   
