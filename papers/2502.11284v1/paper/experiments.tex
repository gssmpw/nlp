\section{Experimental Design}



\begin{table*}[!ht]
\centering
\small{
\begin{tabular}{@{}p{0.2\textwidth} p{0.33\textwidth} p{0.4\textwidth}@{}}
\toprule
\textbf{Category} & \textbf{SFT} & \textbf{PFT} \\ 
\midrule
\textbf{Summarization} & Reddit TL/DR \cite{volske-etal-2017-tl} & Reddit comparison dataset \cite{stienon2020learning} \\ 
\midrule
\multirow{2}{*}{\textbf{Helpfulness}} 
    & HelpSteer \cite{wang2023helpsteer} & HelpSteer2 \cite{wang2024helpsteer2} \\ 
    & HelpSteer \cite{wang2023helpsteer} & HelpSteer2 \cite{wang2024helpsteer2} \\ 
\midrule
\textbf{Instruction Following} 
    & Tülu3 Persona IF \cite{lambert2025tulu3pushingfrontiers} & Tülu3 Persona IF \cite{lambert2025tulu3pushingfrontiers} \\ 
\midrule
\textbf{Grade School Math} 
    & GSM8k \citep{cobbe2021training} $^{\downarrow}$ & Tülu3 Grade School Math \cite{lambert2025tulu3pushingfrontiers} $^{\downarrow}$\\ 
\bottomrule
\end{tabular}
}
\caption{Overview of the task-specific datasets used. Datasets marked with $^{\downarrow}$ use prompts from the original datasets, but the annotations are synthetically created by us. Refer to appendix \ref{datasets} for more information on how the datasets were curated and processed. }
\label{tab:datasets_tasks}
\end{table*}

\begin{table}[!ht]
\centering
\small{
\begin{tabular}{@{}p{0.15\textwidth} p{0.25\textwidth}@{}}
\toprule
\textbf{Task} & \textbf{Benchmark} \\ 
\midrule
Summarization & Win Rate vs Reference \\ 
\midrule
Helpfulness & Win Rate vs Reference \\ 
\midrule
Instruction Following & IFEval Instruction Accuracy \cite{zhou2023instruction} \\ 
\midrule
Grade School Math & GSM8k Test Accuracy \\ 
\bottomrule
\end{tabular}
}
\caption{Overview of the benchmarks and evaluation metrics for the tasks.}
\label{tab:tasks_benchmarks}
\end{table}



In this section, we formulate our problem of studying resource allocation under a fixed budget (described in Section \ref{problem}), using the two methods detailed in Sections \ref{sft} and \ref{rlhf}, using the experimental setup from Section \ref{exp_setup}.

\subsection{Problem formulation}

\label{problem}

The primary focus of this study is to understand and evaluate two finetuning stages SFT and PFT used in the post-training of text-only LLMs, in terms of their data-annotation costs. Given a fixed budget, the goal is to identify which method is cost-effective, and in what ratio should the resources be allocated between the two methods. We limit our focus to models under 10 Billion parameters and leave the study of larger models, which might incur compute-data annotation cost trade-offs \cite{bai-etal-2021-pre} for future work. We also assume that training prompts are available and annotation involves generating responses, and other resources like researcher/engineer labor costs for model training are equal for both methods, for simplicity.

Data budget is a common variable along which we compare post-training methods. This can be measured in terms of the number of training examples or the monetary cost of annotating the dataset. We assume that no task-specific labeled data is available at the start, and simulate the creation of SFT and PFT annotations using open-source datasets for each target task/skill when available, or synthetically curating data ourselves. Since we intend to study improvements on a task using targeted data annotation, we leave out task-agnostic general-purpose conversational datasets that are commonly used for Preference Finetuning, like UltraFeedback \citep{cui2023ultrafeedback} and Chatbot Arena preferences \citep{chiang2024chatbot}.  

\subsection{Supervised Finetuning}

\label{sft}

The loss function for SFT is defined as: 

\vspace{-7mm}

\begin{equation*}
\mathcal{L}_{\text{SFT}}(\pi) = - \mathbb{E}_{(x, y) \sim \mathcal{D}_{\text{SFT}}} \left[ \log \pi(y | x) \right]
\end{equation*}

\noindent
where $\mathcal{L}_{\text{SFT}}(\pi)$ is parameterized by the model as $\pi$ (the model's parameters), and ${\pi}(y | x)$ is the probability of generating the response $y$ by the model $\pi$ given the prompt $x$, where $(x, y)$ is a prompt-response pair sampled from the SFT dataset $\mathcal{D}_{\text{SFT}}$. 

\subsection{Preference Finetuning}

\label{rlhf}

For RLHF, we use offline Preference Finetuning (PFT) methods, as this simplifies our analysis without the need for iterative training or reward models.  This also allows for a more controlled comparison of various methods, as the data used for post-training is exactly the same in each fine-tuning run.  We use Direct Preference Optimization (DPO) \citep{rafailov2024direct} in our main experiments, which has been used extensively in building state-of-the-art LLMs \cite{dubey2024llama, lambert2025tulu3pushingfrontiers}. The objective of DPO is defined as: 

% \begin{equation*}
% \begin{split}
% \mathcal{L}_{\text{DPO}} &= - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D_\text{PFT}}} \\
% &\left[ \log \sigma \left( \beta \log \frac{\pi_{\theta}(y_w | x)}{\pi_{\text{ref}}(y_w | x)}  - \beta \log \frac{\pi_{\theta}(y_l | x)}{\pi_{\text{ref}}(y_l | x)} \right) \right]
% \end{split}
% \end{equation*}

\vspace{-4mm}

\begin{equation*}
\begin{split}
\mathcal{L}_{\text{DPO}}(\pi_{\theta}; \pi_{\text{ref}}) &= - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D_\text{PFT}}} \\
&\left[ \log \sigma \left( \beta \triangle \right) \right]
 \end{split}
\end{equation*}
% where

\begin{equation*}
    \triangle = \log \frac{\pi_{\theta}(y_w | x)}{\pi_{\text{ref}}(y_w | x)}
    - \log \frac{\pi_{\theta}(y_l | x)}{\pi_{\text{ref}}(y_l | x)}
\end{equation*}
% \begin{equation*}
%     r_{w} = \log \frac{\pi_{\theta}(y_w | x)}{\pi_{\text{ref}}(y_w | x)}
% \end{equation*}
% \begin{equation*}
%     r_{l} = \log \frac{\pi_{\theta}(y_l | x)}{\pi_{\text{ref}}(y_l | x)}
% \end{equation*}

$\mathcal{L}_{\text{DPO}}(\pi_{\theta}; \pi_{\text{ref}})$ is the DPO objective, $\pi_\theta$ is the model policy $\pi_\theta$, $\pi_{\text{ref}}$ is the reference policy, $\beta$ is a scalar parameter controlling the strength of the preference modeling and ($x$, $y_w$, $y_l$) is a triplet of prompt, chosen response and rejected response sampled from an offline preference dataset $\mathcal{D}_{\text{PFT}}$.

\subsection{Models and Post-Training Data}

\label{exp_setup}


We primarily finetune the Llama3.1 8B and Qwen2.5-7B model families for our experiments. Both SFT and PFT are run for 2 epochs each. Since the study involves hundreds of finetuning runs, we use LoRA \cite{hu2021lora} to accelerate finetuning. Additional hyperparameter details are in appendix \ref{hyperparameters}.  
We consider four tasks that represent a diverse variety of LLM post-training goals - \textit{Helpfulness}, \textit{Summarization}, \textit{Instruction Following}, and \textit{Grade School Mathematics}. These test models for capabilities like elementary reasoning, precise response generation, and alignment to human preferences. Table \ref{tab:datasets_tasks} and \ref{tab:tasks_benchmarks} describe the datasets used in finetuning for each task and the evaluation metrics considered. Since human-annotated data and synthetically generated data could have significant differences in quality and content, we ensured that when we compared models finetuned using the two methods on a task, the training data used by both methods were either both human-generated or both synthetically generated data. More details about dataset curation, processing and the conversational chat template used are in appendix \ref{datasets}. 



