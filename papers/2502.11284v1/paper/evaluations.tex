\section{Analysis of SFT and PFT}

\subsection{\RQOne}

\label{optimal_ratio}

\begin{figure*}[!h]
    \centering
    \includegraphics[width=1.0\linewidth]{images/llama_qwen.png}
    
    \caption{Effect of varying the of SFT-PFT data mix on the performance of Llama3.1-8B (top) and Qwen2.5-7B (bottom) base models. The x-axis represents the number of training examples (data budget), and the y-axis represents performance, measured using different task-specific metrics. The ratios represent the fraction of the training data allocated for SFT, and the rest is for Preference Finetuning. The orange line shows the performance when trained using only SFT data (1.0 ratio). The subsequent darkening red-shaded lines indicate decreasing proportions of SFT data in the training set, all the way till using only PFT data directly on the base model (0.0 ratio).}
    \label{fig:llama_qwen}
\end{figure*}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/kto.png}
    \caption{Comparision of SFT against KTO method of PFT. We notice similar relative performance scaling patterns as SFT vs DPO highlighting the general trends in preference data-based finetuning against SFT.}
    \label{fig:kto}
\end{figure}


In this section, we study the following challenge that an NLP researcher or practitioner would face - to improve performance on a \textit{target task or skill}, given a fixed budget to spend on post-training data annotation, what would be an optimal budget allocation between SFT and PFT. Prior work has shown that improvements by scaling up SFT data follows a power law, indicating that training data requirements increase drastically for subsequent performance gains through SFT \citep{raghavendra2024revisitingsuperficialalignmenthypothesis}. We study how this changes when we shift some of the data annotation efforts into Preference Finetuning. To eliminate the unobserved effect of multitask finetuning, we simulate annotating, finetuning, and evaluating the improvements for each skill separately. 

Given a pretrained model and a fixed data-budget, our objective is to finetune using a pipeline of SFT followed by PFT, and empirically determine the optimal performance while scaling the post-training data size. We increase data-budgets from $0$ to $20,000$ instances, covering over four orders of magnitude. We experimented with SFT data allocation ratios $\in\{1.0, 0.75, 0.5, 0.25, 0.0\}$, with the rest allocated for preference data annotation. For instance, for a data budget of 10,000 if the SFT ratio is 0.25, then the SFT and PFT training data have 2,500 and 7,500 examples each. Note that an SFT ratio of $1.0$ and $0.0$ denotes pure SFT and pure PFT on the base model respectively. By measuring data budgets and allocation in terms of number of examples, this section also implicitly assumes that SFT and PFT data have equal data annotation costs. This is because data annotation costs vary substantially - the price of human annotation used in the training data of leading LLMs is usually not publicly disclosed, and synthetic annotation uses LLM API whose prices change significantly over time.\footnote{https://www.nebuly.com/blog/openai-gpt-4-api-pricing} So, defining the data budget in terms of the number of training examples makes the results agnostic to the exact dollar cost at any point in time. In section \ref{cost_ratio}, we relax this equal cost assumption and investigate the sensitivity of the results under different SFT-PFT data annotation costs using publicly available information in the literature as well as our estimated annotation costs.

We show results for Llama3.1 8B and Qwen2.5 7B in Figure \ref{fig:llama_qwen} using DPO as the Preference Finetuning method. We also show results using an alternate PFT method of Kahneman-Tversky Optimization (KTO) \citep{ethayarajh2024kto} in Figure \ref{fig:kto}. Additional results to test for generality, using other model sizes like Llama3.2 $\{3B, 1B\}$ and Qwen2.5 $\{3B, 1.5B\}$ are in Figure \ref{fig:llama_all} and \ref{fig:qwen} in appendix \ref{qwen}.

Our experiments reveal key insights about the interplay between SFT and PFT across different data regimes. 
In several cases, finetuning on a dataset with optimal SFT-PFT budget allocation ratio performed better than finetuning with \textbf{2-5X larger} suboptimally allocated training data budget. For instance, finetuning with 5000 examples with a 25\% SFT allocation is on par with training with 20,000 examples with 75\% SFT allocation on Summarization, Helpfulness and Grade School Math. 


\takeaway{In low-data scenarios (n < 1000), pure SFT demonstrates superior performance compared to other mixed allocation approaches. As the data budget increases, we observe a consistent trend of higher proportions of preference data in the training set leading to improved performance. (Figure \ref{fig:llama_qwen}, \ref{fig:kto})}
 

\subsection{\RQTwo}


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{images/sft_pft_limit_all.png}
    \caption{Scaling patterns of SFT and PFT (using DPO) directly on the Llama3 models - 8B\textbf{(top)}, 3B \textbf{(middle)} and 1B\textbf{(bottom)}. We observe that SFT shows a consistent improvement on the task across all model sizes. However, directly applying PFT shows improvements only in large data-regimes, and only in larger model sizes.}
    
    \label{fig:sft_pft_limit}
\end{figure*}

In the previous section, we observed that lowering the proportion of SFT gave better results. However, the complete absence of SFT data led to suboptimal performance. In Figure \ref{fig:sft_pft_limit}, we study the how model performance scales with training data size when PFT is run on the base model directly on the base model across model sizes, and contrast it with SFT on the base model. We observe that SFT on the base model shows a consistent improvement on the task across all model sizes with more data, unlike Preference Finetuning.

\takeaway{Directly applying preference finetuning on the base model shows little or no improvements on smaller models across all tasks. As model sizes grow,  stylistic tasks like Instruction Following and TL;DR Summarization improve eventually with more data, but mathematics only shows modest improvements. (Figure \ref{fig:sft_pft_limit})} 

This hints at the \textit{cold-start problem} in applying preference-based RL methods directly on the base models for tasks like math, similar to what was recently observed in other RL methods like GRPO in \citet{shao2024deepseekmathpushinglimitsmathematical, guo2025deepseek}. To study this further, we finetune Llama3.1-8B model with $10,000$ examples on Instruction Following and Math data, with diminishing SFT ratios of $0.1$ (1000 SFT examples), $0.01$ (100 SFT examples), and $0.0$ (0 SFT examples), and try both DPO and KTO on these SFT checkpoints. The results from this experiment in Figure \ref{fig:push_limit} show the effects of diminishing SFT data on PFT results. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{images/push_limit.png}
    \caption{Performance of DPO and KTO models with decreasing SFT data ratio of $0.1$, $0.01$ (highlighted in red dotted lines) and $0$ (Pure PFT), for \textit{the same total data budget}. We see that even a minimal amount of SFT can have outsized benefits in both cases, with the improvements being more drastic in analytical tasks like math compared to gradual improvements in stylistic tasks like instruction following.}
    \label{fig:push_limit}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{images/response_analysis.png}
    \caption{Manual analysis of model responses finetuned with "cold-start" SFT data, on the GSM8k test set. Directly performing PFT even with 10,000 examples, although encourages some multistep reasoning, does not help it precisely follow the format in the instruction. However, investing just 10\% of this budget into SFT fixes this problem and yields substantial performance improvements.  }
    \label{fig:response_analysis}
\end{figure}


We see that allocating even a minimal amount of budget to SFT data in tasks like math (as low as $<10\%$) provides a substantial performance boost, whereas the improvements in stylistic tasks are more gradual.

We also analyze this cold-start phenomenon qualitatively by manually annotating and studying a random subset of 50 responses from the GSM8k test set. We check for correct answer, whether the response follows \textit{any} multistep reasoning (loose evaluation), and whether it follows the \textit{exact} format (strict evaluation) specified in appendix \ref{math_data}, which is present in both the training data and the system message. Note that we are not annotating the correctness of the reasoning steps, but only their presence and adherence to the expected format. 

From Figure \ref{fig:response_analysis} we see that the desired structured response style of the math training data deviates significantly from the response style used by the reference base model for the prompts. Even when the system message instructs the model to answer step-by-step in a specified format, the base model often responds with no multistep reasoning (one-word/phrase answers). Performing PFT directly on the base model, although encourages some multistep reasoning style, does not teach it the exact format nor meaningfully improve performance. 

However, investing just \(10\%\) of the data into SFT data before preference finetuning fixes this problem and significantly improves performance. Our analysis shows that it effectively aligns the model's response style to the required format. This could be because the step-by-step reasoning format deviates significantly from the base model's behavior to often respond without any reasoning, which DPO tends to penalize. However, even minimal SFT data quickly aligns the model to the expected response style, and such an SFT model acts as a compatible reference model for DPO to finetune over, without having to deviate too much from it, as described by \citet{liu2024understandingreferencepoliciesdirect}. We analyze the average length of the responses in Table \ref{tab:average_length} and also show example annotated responses from these model that illustrate this better in appendix \ref{response_analysis_examples}. 

We also investigate the effect of different values of the $\beta$ parameter that regulates the penalty for deviating from the base reference model, while running DPO directly on the base model. The results in Figure \ref{fig:beta_test} are in line with the observations above. Lower values of $\beta$ (that penalize deviation from the reference base model less) lead to slightly better math performance since it allows it to explore the step-by-step reasoning style with less penalty.


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/beta_test.png}
    \caption{The effect of reducing $\beta$ in DPO when running directly on the base model, for 1000 and 10000 examples (The value in parentheses over each bar represents the improvement over $\beta=0.1$).
        }
    \label{fig:beta_test}
\end{figure}

\takeaway{In cases where the expected response style deviates significantly from the base model's behavior (like math), allocating even a minimal amount ($<10\%$ of the budget) to perform SFT significantly improves subsequent preference finetuning, by aligning the reference model response style. (Figure \ref{fig:push_limit}, \ref{fig:response_analysis})}


\subsection{\RQThree}

\label{cost_ratio}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{images/costs_plot_main.png}
    \caption{Change in performance patterns with varying SFT-PFT data annotation costs for Grade School Math (Top), and Instruction Following (Bottom) on the Llama3.1 8B model. The plot on the left side is with the ground-truth SFT-PFT cost ratio that we incurred from our synthetic SFT and PFT Grade School Math data generated using GPT4o. On the right side, we scale the plot to simulate different SFT-PFT costs.}
    \label{fig:cost_plots}
\end{figure*}

In this section, we examine how the optimal budget allocation recommendations vary under different cost calculations of annotating SFT and PFT data points. Most open-source datasets do not have cost estimates, making it hard to calculate the exact costs associated with annotation. However, for our Grade School Math examples, we generated synthetic data ourselves using GPT4o for existing SFT and PFT prompts, and thus have an estimate of what it costs to produce this data. Based on Microsoft Azure OpenAI API pricing for GPT4o of $\$2.5/1M$ tokens as of the writing of this in February 2025, it cost us $\$0.0023$ per SFT datapoint annotation and $\$0.0011$ per PFT datapoint preference annotation. This cost per example is in line with previously reported costs from synthetic data creation using GPT4o, adjusted to the change in OpenAI API pricing  \cite{wang2023selfinstructaligninglanguagemodels, honovich2022unnatural, pmlr-v235-lee24t}. This gives us an SFT-PFT ratio of $2:1$ when data is synthetically generated. In the case of human annotation \citet{kiela-etal-2021-dynabench} estimate \$0.5-\$1.0 per example for SFT human data annotation for a prompt and \citet{pmlr-v235-lee24t} report that it cost them \$0.67 per preference annotation example using Google Cloud’s human annotation service, giving an estimated SFT-PFT ratio between $2:1$ to $1:1$. 

In Figure \ref{fig:cost_plots}  we look at the performance curves of the Llama 3.1 8B under our calculated SFT-PFT costs of $(\$0.002, \$0.001)$ i.e. a $(2:1)$ ratio, as well as other plausible SFT-PFT data annotation cost ratio simulations $\in\{(10:1), (5:1), (1:1), (1:5)\}$. For example, a $5:1$ ratio is simulated using SFT-PFT costs as $(\$0.005, \$0.001)$. We see that SFT is beneficial only under smaller data-budgets, or when the costs of annotating an SFT response is much lower than making a preference judgement.


\takeaway{Under most annotation cost structures, it is beneficial to spend more data budgets on Preference data after some initial SFT. (Figure \ref{fig:cost_plots})}  




 








