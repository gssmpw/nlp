\section{Appendix}

\subsection{Ablation studies}

\label{qwen}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{images/llama_all.png}
    \caption{Performance of different SFT-PFT ratios on various models of Llama3 model family.}
    \label{fig:llama_all}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{images/qwen.png}
    \caption{Performance of different SFT-PFT ratios on various models of Qwen2.5 model family.}
    \label{fig:qwen}
\end{figure*}

To test if our findings hold for different models, we run the experiments for different size of models in the Llama3 $\in{8B, 3B, 1B}$ in Figure \ref{fig:llama_all} and Qwen2.5 $\in{7B, 3B, 1.5B}$ model families in Figure \ref{fig:qwen}. 


\subsection{Data Processing and Benchmarks}

\label{datasets}

For each task, whenever possible, we used open-source datasets and benchmarks for each task. We ensured that prompts were never re-used between SFT and PFT datasets in any of the experiments, since we saw diminishing performance doing this during our early experiments. 

We used the Unsloth framework \cite{unsloth} to fine-tune models. All training and evaluation is done using the chat-ml chat template \footnote{https://github.com/vllm-project/vllm/blob/main/examples/template\_chatml.jinja} using single-turn dialogues and evaluations are done in a 0-shot setting. 

Links to the datasets and benchmarks we used - (we use the provided train split for all our training and subsample from it for different budgets):

\paragraph{Summarization}
\begin{enumerate}
    \item \textbf{SFT}: Reddit TL/DR - The dataset has a CC-BY-4.0 license. \href{https://huggingface.co/datasets/UCL-DARK/openai-tldr-filtered}{https://huggingface.co/datasets/UCL-DARK/openai-tldr-filtered}
    \item \textbf{PFT}: Reddit comparison dataset - The dataset has a CC-BY-4.0 license. \href{https://huggingface.co/datasets/UCL-DARK/openai-tldr-summarisation-preferences}{https://huggingface.co/datasets/UCL-DARK/openai-tldr-summarisation-preferences}

\end{enumerate}

\paragraph{Instruction Following}
\begin{enumerate}
    \item \textbf{SFT}: Tulu3 Instruction Following - The dataset has a ODC-By license. \href{https://huggingface.co/datasets/allenai/tulu-3-sft-personas-instruction-following}{}
    \item \textbf{PFT}: Tulu3 Instruction Following - The dataset has a ODC-By license. \href{https://huggingface.co/datasets/allenai/tulu-3-pref-personas-instruction-following}{Tulu3 Instruction Following}
    \item \textbf{Evaluation}: IFEval \href{https://github.com/google-research/google-research/tree/master/instruction_following_eval}{https://github.com/google-research/google-research/tree/master/instruction\_following\_eval}
\end{enumerate}

\paragraph{Alignment}
\begin{enumerate}
    \item \textbf{SFT}: HelpSteer1, HelpSteer2. The datasets have a CC-BY-4.0 license. \href{https://huggingface.co/datasets/nvidia/HelpSteer}{https://huggingface.co/datasets/nvidia/HelpSteer}, \href{https://huggingface.co/datasets/nvidia/HelpSteer2}{https://huggingface.co/datasets/nvidia/HelpSteer2}
    \item \textbf{PFT}: HelpSteer1, HelpSteer2. The datasets have a CC-BY-4.0 license. \href{https://huggingface.co/datasets/nvidia/HelpSteer}{https://huggingface.co/datasets/nvidia/HelpSteer}, \href{https://huggingface.co/datasets/nvidia/HelpSteer2}{https://huggingface.co/datasets/nvidia/HelpSteer2}

\end{enumerate}


\paragraph{Grade School Math}
\begin{enumerate}
    \item \textbf{SFT}: GSM8k (for the prompts) - The dataset has an MIT license. \href{https://huggingface.co/datasets/openai/gsm8k}{https://huggingface.co/datasets/openai/gsm8k}, 
    \item \textbf{PFT}: Tulu3 Grade School Math (for the prompts) - The dataset has ODC-By license. \href{https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math-grade}{https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math-grade}

\end{enumerate}

Further details about the curation and processing of each dataset is described in detail lbelow. 

\subsubsection{Summarization}

We used Reddit TL;DR dataset from \citet{volske-etal-2017-tl} for SFT and Reddit Summarization Comparision dataset from \citet{stienon2020learning} for Summarization. Specifically, we used filtered versions of both the datasets from \citet{kirk2024understandingeffectsrlhfllm} for our experiments. For evaluation, we chose a random subset of 500 examples from the SFT dataset, and calculated win-rate of the fine-tuned models against the reference summary as the metric using GPT4o, following \citet{rafailov2024direct}. 

\begin{tcolorbox}[breakable, title=System message]
    You are a helpful assistant who is an expert at summarizing content into TL;DR sentences. Summarize the following text
\end{tcolorbox}

\paragraph{Evaluation} Given below is the prompt used for evaluating win rate. 

\begin{tcolorbox}[breakable]

Which of the following summaries does a better job of summarizing the most
important points in the given forum post, without including unimportant
or irrelevant details?

\bigbreak
Precise as in it captures the most important points in the post.
Concise as in it does not include unimportant or irrelevant details. It should be a good TL;DR of the post - so a good summary should be no more than 2-3 sentences.
\bigbreak
Example:
\bigbreak
Post: I [18M] know her [15F] now over a Year (since I know my friend [17M]). We haven't really talked to much I saw her from time to time when I was at his place over the weekend. About 2 months ago we came a little closer (I was at his place again and we played on his Xbox One when he went to bed early so we played Dance Central [Dancing Game] until 3am). Since then we've talked more often, she hang out with us and 2 Weeks ago we began texting. Now I would really like to ask her out on a date. But I still haven't talked to my friend about this... I mean she is still his sister, wouldn't it be wierd, plus she is a little young in my opinion(she's her age way ahead in mind)."
\bigbreak
Summary A: I'd like to date the sister of my best friend. But I don't know if she's to young and don't how to start all of this.
\bigbreak
Summary B: You've known a 15-year-old girl for over a year through your 17-year-old friend. Recently, you've been texting and getting closer. You're considering asking her out on a date, but you're unsure if it's appropriate since she's your friend's sister and she's younger than you expect.

\bigbreak
In this example, summary A is better since it is more concise and captures the most important points in the post.
\bigbreak
Post: \{post\}
\bigbreak
Summary A:
\{\{
    'model': 'model\_1',
    'summary': \{output\_1\}
\}\}
\bigbreak
Summary B: 
\{\{
    'model': 'model\_2',
    'summary': \{output\_2\}
\}\}
\bigbreak
Now please rank the models by the quality of their summaries, so that the
model with rank 1 has the best summary. Then return a dictionary of the form
\bigbreak
\{\{

    "reasoning": short justification of ranking, 
    
    "model\_1": rank\_1,
    
    "model\_2": rank\_2
    
\}\}
\bigbreak
Your response must be a valid json dictionary and should contain nothing else because we will directly execute it in Python.

\end{tcolorbox}


\subsubsection{Helpfulness}

For helpfulness, we used Helpsteer \citep{wang2023helpsteer} and Helpsteer 2 \citep{wang2024helpsteer2} datasets. To keep the comparison fair, we mixed the prompts from both the datasets and then randomly split it into SFT and PFT mixes. For SFT, we chose the response with the highest cumulative average, and for PFT, we chose pairs with the highest and lowest average scores as the chosen and rejected, respectively.  

We evaluated the responses using win-rate against 500 randomly chosen prompts and responses from the test set of Anthropic's Helpfulness and Harmlessness dataset \cite{bai2022traininghelpfulharmlessassistant} GPT4o, following \citet{rafailov2024direct}. 


\begin{tcolorbox}[breakable, title=System message]
    You are a helpful assistant who is an expert at providing helpful and harmless responses to questions
\end{tcolorbox}


\paragraph{Evaluation} We include the prompt used for evaluation below:


\begin{tcolorbox}[breakable]

Which of the following responses from a chatbot that was
designed to be helpful and harmless does a better job? 

A good response tries to help answer the user's question or provide useful information, while avoiding harmful or inappropriate
language. You are given a message history between a user and a chatbot, that ends with a 
user question, and two possible responses from the chatbot.
\bigbreak
Message turns: \{messages\}
\bigbreak
Response A:

\{\{

    'model': 'model\_1',
    
    'response': \{output\_1\}
    
\}\}
\bigbreak
Response B: 


\{\{

    'model': 'model\_2',
    
    'response': \{output\_2\}
    
\}\}
\bigbreak
Now please rank the models by the quality of their responses, so that the model with rank 1 has the best response. Then return a dictionary of the form
\bigbreak
\{\{

    "reasoning": short justification of ranking,
    
    "model\_1": rank\_1,
    
    "model\_2": rank\_2
    
\}\}
\bigbreak
Your response must be a valid json dictionary and should contain nothing else
because we will directly execute it in Python. Please provide the ranking that
the majority of humans would give.
    
\end{tcolorbox}

\subsubsection{Instruction Following}

We used Tulu3's Persona-guided Instruction Following SFT and Preference datasets \citep{lambert2025tulu3pushingfrontiers}. 


\begin{tcolorbox}[breakable, title=System message]
    You are a helpful assistant who is an expert at responding to prompts by carefully following the given instructions
\end{tcolorbox}


\paragraph{Evaluation} We evaluated the models on IFEval \citep{zhou2023instruction}. 

\subsubsection{Grade School Math}

\label{math_data}
For Grade School Math, there was a lack of high-quality preference datasets that is similar in distribution to the original GSM8k dataset. So we synthetically generated preference dataset for this. To ensure fairness, we also had to synthetically annotate SFT data. We took prompts from the original GSM8k train set and synthetically generated multiple responses using GPT4o, and only trained on the correct responses after verifying their final answer.


Here is the prompt template used for synthetic SFT data generation for GSM8k. 


\begin{tcolorbox}[]

\textbf{You are an expert mathematician who responds to mathematical questions with precise step-wise solutions. }

\bigbreak
You are given a math problem. Solve it in the following steps:


Step 1: <step 1> 

Step 2: <step 2> 

..

Step n: <step n> 
\bigbreak

\#\#\#\# <Final numerical answer>

Example:
\bigbreak
Question - 

Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?

\bigbreak
Solution - 

Step 1: How many clips did Natalia sell in May? ** Natalia sold 48/2 = <<48/2=24>>24 clips in May.

Step 2: How many clips did Natalia sell altogether in April and May? ** Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.

\#\#\#\# 72
\bigbreak
Now solve this and return just the solution in the specified format (dont repeat the question or add any extra information):

Question - \{question\}

Solution - 

\end{tcolorbox}

We studied how finetuning performance scales with both the original dataset and our synthetic variant, to ensure that our generated data doesn't lead to drastic performance drops as compared to the original dataset. 

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/human_v_synth.png}
    \caption{Comparing the scaling of performance of datasets that were human-generated versus our synthetically generated responses to the GSM8k training prompts. We note that while there is a small difference in the performance, the performance scales similarly.}
    \label{fig:human_v_synth}
\end{figure}


For PFT, we chose prompts from Tulu3's persona-driven grade school math dataset, and generated multiple responses using a llama3.1-8B checkpoint finetuned with 1000 SFT math responses. Since the idea is to build an offline dataset that can be used across all models, we chose this checkpoint which has some mathematical ability instead of generating data from the base model. We then scored each response using GPT4o. Other works like \citet{jiao2024preferenceoptimizationreasoningpseudo} use pseudo labels for generating preferences for DPO, and run them on top of SFT checkpoints. However, we wanted to simulate how data would be annotated by humans, similar to other tasks - the annotators a make preference judgement on the responses to a prompt, with an optional rubric explaining the scoring. We use a style and correctness-based rubric, inspired by similar rubrics used by Deepseek-R1 \cite{guo2025deepseek}, awarding points for both following the format and arriving at the correct final answer. 

Here is the prompt template used for synthetic Preference annotation for GSM8k. 

\begin{tcolorbox}[breakable]

\textbf{Your role is to evaluate text quality based on given criteria. You'll receive an instructional description and text output. Understand and interpret instructions to evaluate effectively. Provide annotations for each text with a rating and rationale. }

\bigbreak
<Instructions>
\bigbreak
Evaluate the quality of the step-by-step solution to the given mathematical problem + reference answer and score the responses between 1 and 5:
\bigbreak
The solution has to be in the following format:

Step 1: <step 1> 

Step 2: <step 2> 

..

Step n: <step n> 
\bigbreak

\#\#\#\# <Final numerical answer>
\bigbreak
Return the scores for each solution in the following json format. Return just the json object and nothing else:
\{\{
    "rationale": "<rationale>",
    "score": "<score>"
\}\}

\bigbreak
Scoring Rubric along with the description of each score:
\bigbreak
1. The solution is completely irrelevant to the given problem - 1.

2. The solution is incorrect and doesn't follow the format - 2.

3. The solution perfectly follows the format, but the steps or the final answer are incorrect - 3.

4. The solution and the steps are correct, but it doesn't follow the expected format perfectly - 4.

5. The solution and the steps are correct and follow the expected format perfectly - 5.

\bigbreak
Note that if the final step is something like "Thus, the answer is 16", or "\#\#\#\# 9+3+2+2=16|<<", or "\#\#\#\# <Final numerical answer>16", it is not perfectly following the format and the score should be 4, not 5.
It is 5 only if the final step only contains the final numerical answer like "\#\#\#\# 16" and nothing else.

\bigbreak
Here is an example annotation:

\bigbreak
<Input>

Problem: Tim rides his bike back and forth to work for each of his 5 workdays.  His work is 20 miles away.  He also goes for a weekend bike ride of 200 miles.    If he can bike at 25 mph how much time does he spend biking a week?
\bigbreak
Reference Answer: 16
\bigbreak

Solution: Let the time Tim spends biking to work for one day be represented as ( t ). Then the round-trip distance for one day is ( 2 times 20 = 40). Since Tim can bike at 25 mph, the time to cover that distance is given by the formula ( t = frac\{40\}\{25\} = 1.6 ). Thus, the time to bike to work for five days is ( 5 times 1.6 = 8 ). The weekend bike ride distance of 200 miles takes time given by ( t = frac\{200\}\{25\} = 8 ). The total biking time for the workdays and weekend is ( 8 + 8 = 16 )

\bigbreak

\#\#\#\# 16

\bigbreak

<Output>

\{\{

    "rationale": "The solution and the steps are correct, but it doesn't follow the expected format perfectly.",
    
    "score": "4"
    
\}\}

\bigbreak
Now annotate this example:

<Input>
\bigbreak
\{input\}

\bigbreak
<Output>



\end{tcolorbox}

\begin{tcolorbox}[breakable, title=System message]

You are a helpful assistant who is an expert at solving math problems. Solve the following math problem and return the solution in the following format:
\bigbreak
Step 1: <step 1> 

Step 2: <step 2> 

..

Step n: <step n> 
\bigbreak

\#\#\#\# <Final numerical answer>

    
\end{tcolorbox}

\paragraph{Evaluation} We evaluated the model on the GSM8k test set. 


\subsection{Finteuning compute v/s data annotation costs}

\label{compute_costs}

Based on the prices from GPU rental companies \footnote{https://coreweave.com/pricing} for L40S GPUs we used in our experiments, as of the writing of this, is about \$1.75/hour. In our experiments, finetuning Llama3.1-8B using LoRA on the gsm8k dataset took about 1 hour for 10000 examples, or about \$0.00001 per example. For comparision, \citet{kiela-etal-2021-dynabench} estimate the cost of human annotation of a single example to be \$0.5 to \$1.0. \citet{honovich-etal-2023-unnatural} and our own estimates suggest that LLM-annotation of an example is about \$0.01 to \$0.001. This implies that the cost of obtaining high-quality human or LLM annotation is significantly higher than the cost of finetuning a reasonable-sized capable model on that task. 



\subsection{Training hyperparameters}

\label{hyperparameters}

\begin{table}[h]
\centering
\caption{SFT Hyperparameter Settings}
\begin{tabular}{lcc}
\toprule
Parameter & Value \\
\midrule
Learning Rate & $5e-5$ \\ % Add your value here
Batch Size &  16 \\ % Add your value here
Number of Epochs & 2 \\ % Add your value here
Warmup Ratio & 0.1 \\ % Add your value here
Learning Rate Scheduler & cosine \\ % Add your value here
Weight decay & 0.01 \\
\bottomrule
\end{tabular}
\label{tab:sft_hyperparameters} % Optional: Add a label for referencing
\end{table}


\begin{table}[h]
\centering
\caption{PFT Hyperparameter Settings}
\begin{tabular}{lcc}
\toprule
Parameter & Value \\
\midrule
Learning Rate & $5e-6$ \\ % Add your value here
Batch Size &  16 \\ % Add your value here
Number of Epochs & 2 \\ % Add your value here
Warmup Ratio & 0.1 \\ % Add your value here
Learning Rate Scheduler & cosine \\ % Add your value here
Weight decay & 0.01 \\
$\beta$ & 0.1 \\
\bottomrule
\end{tabular}
\label{tab:pft_hyperparameters} % Optional: Add a label for referencing
\end{table}


\begin{table}[h]
\centering
\caption{LoRA settings}
\begin{tabular}{lcc}
\toprule
Parameter & Value \\
\midrule
rank & 32 \\ % Add your value here
$\alpha$ &  32 \\ % Add your value here
\bottomrule
\end{tabular}
\label{tab:pft_hyperparameters} % Optional: Add a label for referencing
\end{table}


Hyperparameter settings for SFT and PFT we used are in Table \ref{tab:sft_hyperparameters} and Table \ref{tab:pft_hyperparameters}. 

\subsection{Average model response length from cold-start models}

\begin{table}[h]
\centering
\caption{Average Response Length (\#Characters) for 50 random GSM8k test set examples when finetuning Llama3-8B on Grade School Math data }
\begin{tabular}{lcc}
\toprule
Model (+ finetuning examples) & Length \\
\midrule
Base Model & 288.76 \\ % Add your value here
Base Model + SFT 1,000 &  426.48 \\ % Add your value here
Base Model + SFT 1,000 + PFT 9,000 & 445.74 \\ % Add your value here
Base Model + PFT 10,000 & 567.91 \\ % Add your value here
\bottomrule
\textit{Average training data length} & 456.82 
\end{tabular}
\label{tab:average_length} % Optional: Add a label for referencing
\end{table}


From Table \ref{tab:average_length} we observe that the DPO directly on the base model develops much longer response length which appears to be reasoning attempts, but are often multi-step rambling responses, as illustrated in section \ref{response_analysis_examples}. 

However, running DPO after a minimal amount of SFT model leads to more focused reasoning in-line with the expected format and the average training data response length. 

\subsection{Illustrative example responses of models}


\label{response_analysis_examples}


\begin{tcolorbox}[breakable, title=Question]
Gloria is shoe shopping when she comes across a pair of boots that fit her shoe budget. However, she has to choose between the boots and two pairs of high heels that together cost five dollars less than the boots. If one pair of heels costs \$33 and the other costs twice as much, how many dollars are the boots?

\textbf{Ground truth answer} - 104
\end{tcolorbox}

\begin{tcolorbox}[breakable, title=Base Model]

The boots cost \$200

{\color{red}→ Follows multistep reasoning (loose)} \\
{\color{red}→ Follows exact format (strict)}\\
{\color{red}→ Answer} \\

    
\end{tcolorbox}


\begin{tcolorbox}[breakable, title=Base Model + SFT 1000 examples]

Step 1: How much does the second pair of heels cost? ** The second pair of heels costs 33 * 2 = \$66.

Step 2: How much do the two pairs of heels together cost? ** The two pairs of heels together cost 33 + 66 = \$99.

Step 3: How much do the boots cost? ** The boots cost \$99 - 5 = \$94.

Step 4: How many dollars are the boots? ** The boots cost \$94.

\#\#\#\# 94

{\color{teal}→ Follows multistep reasoning (loose)} \\
{\color{teal}→ Follows exact format (strict)}\\
{\color{red}→ Answer} \\
    
\end{tcolorbox}


\begin{tcolorbox}[breakable, title=Base Model + 10000 PFT examples]

1: Let x be the cost of the more expensive pair of heels.

2: The total cost of the two pairs of heels is 33 + 2x.

3: The total cost of the boots is 38.

4: The total cost of the boots and the two pairs of heels is 38 + 0.

5: The total cost of the boots and the two pairs of heels is 38 + 0.

6: The total cost of the boots and the two pairs of heels is 38 + 0.

7: The total cost of the boots and the two pairs of heels is 38 + 0.

8: The total cost of the boots and the two pairs of heels is 38 + 0.

{\color{teal}→ Follows multistep reasoning (loose)} \\
{\color{red}→ Follows exact format (strict)}\\
{\color{red}→ Answer} \\

    
\end{tcolorbox}


\begin{tcolorbox}[breakable, title=Base Model + 1000 SFT + 9000 PFT examples]

Step 1: How much does the second pair of heels cost? ** The second pair of heels costs 2 * 33 = <<2*33=66>>66 dollars.

Step 2: How much do the two pairs of heels cost together? ** The two pairs of heels cost 33 + 66 = <<33+66=99>>99 dollars.

Step 3: How much do the boots cost? ** The boots cost 99 + 5 = <<99+5=104>>104 dollars.

\#\#\#\# 104

{\color{teal}→ Follows multistep reasoning (loose)} \\
{\color{teal}→ Follows exact format (strict)}\\
{\color{teal}→ Answer} \\
    
\end{tcolorbox}