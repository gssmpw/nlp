\section{Related Work}
\paragraph{High-dimensional BO}
While our work focuses on the setting where a large population of agents cooperates to optimise an unknown black-box function, we can also think about this problem from the perspective of high-dimensional BO, where we wish to optimise a function over a set of high-dimensional inputs. Standard BO struggles in high-dimensional spaces and, in general, the regret bound of the standard GP-UCB algorithm scales exponentially with the number of dimensions ____. A number of different algorithms have been presented to tackle this problem. Methods such as REMBO ____ or HeSBO ____ project the original space into a space of lower dimensionality and conduct the optimisation process there. Decomposition methods ____ assume the function can be additively decomposed into subfunctions, operating on spaces of lower dimensionalities. SaaSBO ____ removes the dimensions that do not seem to be impacting the function value too much, thus reducing the difficulty of the problem. The famous TuRBO ____ defines a trust region to which the optimisation process is restricted, effectively reducing the volume of the search space. Within this work, we exploit certain invariant properties of mean-field functions to achieve higher sample efficiency. The recent work of ____ studied invariances in BO, in general, but without considering mean-field systems specifically. Our mean-field assumption could be considered a permutation-invariance from their work's point of view. However, applying their result directly to our case will result in a regret bound scaling as $\mathcal{O}(\beta_T\sqrt{T\log T^M} / M!)$, which is better than naive BO, but still exhibits dependence on $M$, especially for large $T$. In contrast, the bound of our algorithm is completely independent of $M$. 
\looseness=-1

\paragraph{Mean-Field Control and MARL}
Closest to our setting are mean-field control (MFC) and mean-field multi-agent reinforcement learning (MF-MARL) formulations. We refer to MFC as the setting with a large population of cooperative agents under a known environment, while we reserve MF-MARL for an unknown environment.
____ are the first to analyse MFC as a Mean-Field Markov Decision Process (MF-MDP) and show that the optimal reward converges to the solution of a continuous Hamilton-Jacobi-Bellman equation under some conditions.
____ introduce MF-MDP under mean-field interaction both on state and actions and show the existence of $\epsilon$-optimal policies. 
____ define MFC as an MF-MDP  over a limiting distribution of continuous agents' states and show an optimal policy exists. They further introduce a discretisation strategy for MFQ-learning.  
____ formulate MFC as an MF-MDP where reward and transition functions depend on the empirical measure of the agents' states and show the existence of $\epsilon$-optimal policy under some conditions. 
____ introduce and analyse a sub-optimality gap of SAFARI--an offline MF-MARL algorithm--for settings where the interaction with the environment during training can be prohibitive or even unethical (e.g., social welfare or other societal systems).
____ first set dynamic programming principles for MFC and then show that model-free kernel-based Q-learning has a linear convergence rate for MFC. They further show that the MFC approximation of cooperative MARL has an approximation error $\bigO(\nicefrac{1}{\sqrt{N}})$ with the number of agents $N$. ____ show that a graphon MFC also achieves the discussed approximation error. 
____ introduce M\textsuperscript{3}-UCRL--an algorithm with a sublinear cumulative regret--for MF-MARL where the goal is to simultaneously optimise for the rewards and learn the dynamics from the online experience.
____ build on top of ____ and introduce Safe-M\textsuperscript{3}-UCRL, an online algorithm for constrained MF-MARL that learns underlying dynamics while satisfying constraints throughout the execution. \looseness=-1