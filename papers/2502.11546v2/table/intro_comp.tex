\begin{table*}[!thp]
\resizebox{\textwidth}{!}{
\begin{tabular}{l|ccccccc}
\toprule
\textbf{Dataset} & \textbf{CC Version} & \begin{tabular}[c]{@{}l@{}}\textbf{\#Langs}\\ \textbf{(total)}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{\#Langs}\\ \textbf{(high)}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{\#Langs}\\ \textbf{(medium)}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{\#Langs}\\ \textbf{(low)}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{\#Langs}\\ \textbf{(very low)}\end{tabular} & \textbf{Training-Ready} \\
\midrule
mC4~\cite{raffel2020exploring} & CC-MAIN-2020-34 & 101 & 0  & 43  & 52  & 6 & \ding{55}            \\
% CC100\cite{conneau-etal-2020-unsupervised}    & CC-MAIN-2018-51                              & 100                                                       &                                                          &                                                            &                                                         &                                                              & Yes            \\
% ROOTS\cite{laurenccon2022bigscience}          & CC-MAIN-2021-10                              & 59                                                        &                                                          &                                                            &                                                         &                                                              & Yes            \\
OSCAR 23.01~\cite{abadji2022towards} & CC-MAIN-2022-49 & 153 & 6  & 42  & 25  & 80 & \ding{55}            \\
Glot500~\cite{imanigooghari-etal-2023-glot500} & \underline{CC-MAIN-2020-34} & 511 & 0  & 108 & 79  & 324 & \ding{55}            \\
CulturaX~\cite{nguyen-etal-2024-culturax}      & \underline{CC-MAIN-2022-49} & 167 & 11 & 47  & 27  & 82 & \ding{55}            \\
Madlad-400~\cite{kudugunta2024madlad}          & CC-MAIN-2022-33 & 419 & 7 & 46  & 39  & 327 & \ding{55}            \\
MaLA~\cite{ji2024emma}                         & \underline{CC-MAIN-2022-49} & 939 & 1  & 125 & 78  & 735 & \ding{55}            \\
Glotcc~\cite{kargaran2024glotcc}               & CC-MAIN-2023-50 & 1331 & 0  & 10  & 52  & 1269  & \ding{55}            \\
HPLT-v1.2~\cite{de-gibert-etal-2024-new}            & \underline{CC-MAIN-2022-40} & 191 & 12 & 53  & 38  & 88 & \ding{55}            \\
Fineweb-2~\cite{penedo2024fineweb-2} & CC-MAIN-2024-18 & 1915  &10 & 62  & 49  & 1794 & \ding{55}            \\
\midrule
DCAD-2000 & CC-MAIN-2024-46 & 2282 & 13 & 142 & 124 & 2003 & \ding{51}            \\
\bottomrule
\end{tabular}}
\vspace{-0.5em}
\caption{\label{tab:intro_comp}
Comparison of multilingual datasets constructed from Common Crawl (CC) and our constructed \dcad, focusing on the latest CC version used, the total number of languages supported, distribution across resource categories (high, medium, low, very low), and training readiness. The CC version marked with \underline{underline} indicates an inferred version due to the lack of explicit specification in the original paper. The ``Training-Ready'' column indicates whether the dataset is ready for training LLMs without requiring further data cleaning.
}
\vspace{-1em}
\end{table*}