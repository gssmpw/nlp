%% figure
\input{figure/app_sta_analysis}

\section{Statistical Analysis of the Datasets}
\label{appex:feature_analysis}
In this section, we explore the statistical characteristics of the dataset through visual analysis, focusing on the distribution of data across different languages and the variations observed across different shards.
We highlight the limitations of existing data cleaning methods that rely on fixed thresholds, particularly in the imbalanced data distribution scenarios.
Specifically, when there are substantial discrepancies in word count distributions, these threshold-based cleaning methods are prone to errors, which fail to accurately distinguish between high-quality and low-quality data.

Figure~\ref{fig:diff_langs} illustrates the average word count distribution across different languages in the New CC dataset (CC-MAIN-2024-38).
We observe substantial variation in the average word count across languages within the same dataset.
For instance, some languages exhibit an average word count as high as 4,000, indicating that their texts are generally longer, while others have an average word count ranging from 50 to 100, suggesting that their texts are typically shorter.
This imbalanced distribution complicates the application of traditional fixed-threshold data cleaning methods across all languages.
For example, setting a word count threshold of 800 (e.g., the median word count) may be suitable for many languages, but it would still misclassify a significant portion of data as low-quality.

Figure~\ref{fig:diff_sources} illustrates the average word count distribution for Chinese across different data sources (MaLA, Fineweb-2, and New CC). We observe significant variation in the word count distribution for the same language across these sources.
For example, the average word count for Chinese in the MaLA corpus is 690, while in New CC (CC-MAIN-2024-33), the average word count increases to 1,975.
This discrepancy highlights the inadequacy of a single fixed threshold for data from different sources.
Applying a uniform threshold could lead to incorrect cleaning of Chinese text from certain data sources, potentially compromising the representativeness and quality of the data.
Consequently, it is essential to adopt flexible cleaning strategies tailored to the characteristics of each data source.

Figure~\ref{fig:diff_shards} illustrates the variation in word count for Chinese across different shards in the Fineweb-2 dataset.
We observe imbalanced word count distributions between shards, which further complicates the data cleaning process.
For instance, some shards contain texts with word counts concentrated between 700 and 1,000, while others have texts primarily between 1,000 and 1,200.
This shard-level variation suggests that fixed-threshold cleaning methods may perform inconsistently across different shards, fails to account for the unique characteristics of the data within each shard.
Therefore, in the presence of such imbalanced distributions, it is crucial to implement a more flexible data cleaning approach.

\section{\dcad Grouped by Writting Scripts}
\label{app:script}
As mentioned in Section~\ref{sec:analysis}, \dcad contains a total of 159 writing scripts.
To provide a comprehensive overview, we list each of these scripts and their corresponding statistical information in Table~\ref{tab:app_script_1} and Table~\ref{tab:app_script_2}.
By presenting this information, we aim to highlight the broad range of writing systems represented by DCAD and emphasize its potential in various linguistic research and applications.

\section{Data Cleaning Statistics}
\label{app:data_clean_statistic}
In this section, we provide detailed data cleaning statistics (Table~\ref{tab_app:data_clean_1},~\ref{tab_app:data_clean_2},~\ref{tab_app:data_clean_3} and~\ref{tab_app:data_clean_4}) for high-resource, medium-resource, and low-resource languages. For the data cleaning statistics of very low-resource languages, please refer to the open-source data statistics we released.

\section{Experimental Setup}
\label{app:exp_setup}
In this section, we introduce the experimental setup used to assess the effectiveness of our proposed data cleaning method and the quality of the \dcad dataset.
Our evaluation is conducted on the FineTask benchmark\footnote{\url{https://huggingface.co/spaces/HuggingFaceFW/blogpost-fine-tasks}}, which covers tasks in nine languages: \textit{Chinese}, \textit{French}, \textit{Arabic}, \textit{Russian}, \textit{Thai}, \textit{Hindi}, \textit{Turkish}, \textit{Swahili}, and \textit{Telugu}.
The benchmark spans a wide range of NLP tasks including reading comprehension, common-sense reasoning, natural language understanding, and text generation.

\subsection{Task Formulations and Evaluation Metrics}
Evaluation tasks in FineTask are implemented using two formulations:
\begin{itemize}
    \item \textbf{Cloze Formulation (CF):} In this setup, answer choices are not provided in the prompt, and the model directly generates each option. CF is used during the task selection phase, as it yields an early and reliable training signal.
    \item \textbf{Multi-Choice Formulation (MCF):} Here, answer options are embedded in the prompt (e.g., using A/B/C/D prefixes), and the model is tasked with selecting the correct prefix. MCF is employed for later evaluations of pretrained models, typically using a few-shot (e.g., 5-shot) approach.
\end{itemize}
    

FineTask provides four evaluation metrics: \textit{Accuracy}, \textit{Accuracy normalized over character length}, \textit{Accuracy normalized over token length}, and \textit{PMI Accuracy}.
However, according to statistical data, none of these metrics consistently perform well across all languages.
Therefore, we chose to use normalized accuracy (norm accuracy) in our evaluation process.
While base accuracy performs well in multiple-choice tasks when the option variations are small, and PMI performs best in complex reasoning and knowledge tasks, overall, length-normalized metrics (such as token- or character-based metrics) are the most reliable.
However, the most suitable choice still depends on the language rather than the specific task.
Therefore, we recommend selecting the maximum of character accuracy (acc\_char) and token accuracy (acc\_token) during evaluation.
For generation tasks, unless exact matches are required, we recommend using F1 score, as it is more robust to small variations in generated outputs and less sensitive to noise.

\subsection{Pre-training and Evaluation Protocol}
We continue pre-training the \texttt{LLaMA-3.2-1B} model~\cite{dubey2024llama} on datasets spanning the nine languages included in FineTask. Model evaluations are performed at regular intervals (approximately every 1B tokens) in a 0-shot setting (i.e., without additional prompt instructions) to capture the raw model capabilities. Normalized accuracy is used consistently as the evaluation metric.
We use NVIDIA A100 40GB GPU server for our experiments and all settings are the same as the Fineweb-2 Task.

%%
\input{table/group_by_script}
%%
%% Table
\input{table/data_clean_sta}
%%