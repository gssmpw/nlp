\section{Evaluation}
\label{sec:eval}
To validate the effectiveness of the proposed data cleaning method and the quality of the~\dcad dataset, we conduct a series of experiments on FineTask benchmark\footnote{\url{https://huggingface.co/spaces/HuggingFaceFW/blogpost-fine-tasks}}.
FineTask includes tasks in nine languages: \textit{Chinese}, \textit{French}, \textit{Arabic}, \textit{Russian}, \textit{Thai}, \textit{Hindi}, \textit{Turkish}, \textit{Swahili}, and \textit{Telugu}, covering diverse NLP tasks such as reading comprehension, common-sense reasoning, natural language understanding, and text generation.
We continue pretrain the \texttt{LLaMA-3.2-1B} model~\cite{dubey2024llama} in these nine languages and analyze the impact of different data cleaning methods, anomaly detection algorithms, and existing datasets.
We use normalized accuracy as the evaluation metric.
Detailed experimental settings and results are provided in Appendix~\ref{app:exp_setup}.

%% table
\input{table/diff_data_clean}
%%

\noindent\textbf{Impact of Different Data Cleaning Strategies.}
We compare the performance of models trained using various data cleaning strategies to evaluate the effectiveness of our proposed anomaly detection-based cleaning framework.
As shown in Table~\ref{tab:diff_data_clean}, the baseline method (raw, uncleaned data) consistently underperforms all cleaning strategies, demonstrating the critical importance of data quality in multilingual training.
This poor performance can be attributed to the presence of noisy, irrelevant, or inconsistent data, which hampers model generalization. 
Traditional threshold-based filtering\footnote{We use the implementation from~\url{https://github.com/bigscience-workshop/data-preparation}} applies fixed rules based on features such as language identification scores and document complexity to remove low-quality data.
In contrast, our anomaly detection-based cleaning approach dynamically identifies and filters noisy or anomalous data, resulting in significantly better model performance.
Models trained on cleaned data using our framework achieved a normalized accuracy improvement of approximately 5–20\% over the baseline and outperformed the threshold-based method by 3–10\%.
In terms of scalability, while the threshold-based filtering method is computationally efficient due to its simple, rule-based nature, our anomaly detection-based approach requires more computation. However, it is highly effective at capturing nuanced data issues that fixed rules might miss, making it a better choice for high-quality multilingual training in the long term.

%% table
\input{table/diff_detect_algo}
%% table

\noindent\textbf{Comparison of Anomaly Detection Algorithms.}
We evaluate several classical anomaly detection algorithms to determine the optimal method for constructing \dcad.
The evaluated detection approaches include Isolation Forest (ISO\_Forest;~\citealp{liu2008isolation}), One-Class SVM (OC\_SVM;~\citealp{manevitz2001one}), Local Outlier Factor (LOF;~\citealp{breunig2000lof}), and K-Means~\cite{hartigan1979k}, using implementations from scikit-learn.\footnote{\url{https://scikit-learn.org}}
Table~\ref{tab:diff_detect_algo} illustrates the performance of these algorithms in cleaning the dataset.
While all anomaly detection-based methods outperform the baseline, the performance of OC\_SVM, LOF, and K-Means is inconsistent.
% and highly sensitive to hyperparameter tuning.
These methods often require extensive fine-tuning of parameters like the number of neighbors (for LOF) or the kernel choice (for OC\_SVM), which significantly affects results and increases computational overhead.
In contrast, ISO\_Forest demonstrates more stable and robust performance across experiments, largely due to its ability to efficiently handle noisy and high-dimensional multilingual data.
Unlike other methods, ISO\_Forest does not require intensive hyperparameter tuning to achieve reliable results, making it ideal for large-scale multilingual datasets.
However, there are trade-offs with ISO\_Forest: it can be computationally expensive compared to simpler algorithms like K-means, especially when working with very high-dimensional data (Our feature vectors has 8 dimentions as mentioned in Section~\ref{sec:ad}).
Nevertheless, its robustness and scalability make it the most suitable choice for cleaning multilingual data in \dcad.

\input{figure/diff_corpus}

\noindent\textbf{Comparison with Other Multilingual Datasets.}
To validate the quality of \dcad, we compare it against existing multilingual corpora on the FineTask benchmark.
These corpora include datasets constructed from \textit{New CC}, \textit{MaLA}, and \textit{Fineweb-2} as described in Section~\ref{sec:data_collect}.
As shown in Figure~\ref{fig:diff_corpus}, models trained on \dcad consistently outperform those trained on other datasets, achieving higher normalized accuracy.
The improvements can be attributed to the enhanced data quality, diversity, and reduced noise resulting from our comprehensive cleaning pipeline.
Specifically, \dcad provides greater linguistic diversity and a more balanced representation of low-resource languages, leading to improved performance on tasks involving underrepresented languages like Swahili and Telugu.
This demonstrates that \dcad not only offers a stronger foundation for multilingual NLP research but also makes it directly suitable for training large-scale multilingual models.