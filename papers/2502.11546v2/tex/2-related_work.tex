\section{Related Work}
\label{sec:related_work}
\subsection{Multilingual Dataset for Pretraining}
\label{sec:related_multi_dataset}
Enhancing the multilingual capabilities of LLMs often involves continuing pretraining on large-scale multilingual datasets~\cite{lin2024mala,ji2024emma}.
These datasets can be broadly categorized into curated corpora, domain-specific corpora, and web-crawled corpora.

\noindent\textbf{Curated Corpora.}
Curated datasets are carefully gathered by experts from high-quality sources such as books~\cite{laurenccon2022bigscience}, academic publications~\cite{clement2019use}, and encyclopedia entries~\cite{brown2011wikipedia,lehmann2015dbpedia,kuo2024wikibench}.
While providing highly accurate and reliable content, these datasets often cover only a small subset of languages, which limits their utility for training multilingual models.

\noindent\textbf{Domain-Specific Corpora.}
In addition to general-domain data, fine-tuning LLMs on domain-specific datasets is crucial for improving performance in specialized domains like finance~\cite{zhang2023xuanyuan}, healthcare~\cite{wu2024pmc}, legal~\cite{colombo2024saullm}, and education~\cite{xu2024large,lozhkov2024fineweb-edu}.
However, these datasets often narrow in scale and language diversity, typically supporting only a small group of high-resource languages, which limits their broader applicability.

\noindent\textbf{Web-Crawled Corpora.}
Web-crawled datasets, particularly those derived from Common Crawl, provide large-scale multilingual coverage by leveraging an open repository of over 250 billion web pages.
These datasets include mC4~\cite{raffel2020exploring}, CC-100~\cite{conneau-etal-2020-unsupervised}, OSCAR~\cite{abadji2022towards}, Glotcc~\cite{kargaran2024glotcc}, Fineweb~\cite{penedo2024fineweb}, and Fineweb-2~\cite{penedo2024fineweb-2}.
However, these datasets typically only have basic data cleaning applied, and they still contain a significant amount of noisy or low-quality text, which makes them unsuitable for direct use in LLMs training.
Moreover, the imbalance in language distribution is another issue, with particularly low coverage of high-resource and medium-resource languages.

\subsection{Data Cleaning}
\label{sec:related_data_clean}
Data cleaning is an essential step in preparing high-quality datasets for training robust LLMs.
It involves filtering noisy, irrelevant, or harmful content and can be broadly classified into model-based and heuristic-based approaches~\cite{liu2024datasets}.

\noindent\textbf{Model-Based Methods.}
Model-based approaches employ classifiers or LLMs to distinguish between high-quality and low-quality data.
For instance, content safety models~\cite{li-etal-2024-salad} filter out explicit or gambling-related content, while quality classifiers remove low-relevance text~\cite{jiang2024more}.
LLM-based methods focus on generating prompts for cleaning~\cite{narayan2022can} or integrating error detection and correction into the pipeline~\cite{chen2023seed,ni2024iterclean}.
Although these methods are effective on small-scale datasets, they face scalability challenges due to the high computational cost of processing large-scale data.
In particular, training a separate classifier for each language in a multilingual dataset is impractical.

\noindent\textbf{Heuristic-Based Methods.}
Heuristic approaches apply predefined rules to filter content at both document and sentence levels.
At the document level, strategies include filtering by language identification scores or scoring documents with language models~\cite{laurenccon2022bigscience,nguyen-etal-2024-culturax}.
At the sentence level, rules are applied to remove incomplete or irrelevant content, such as HTML tags or excessively short sentences~\cite{raffel2020exploring,abadji2022towards}.
Although computationally efficient, heuristic methods often require manual tuning and struggle to adapt to the diverse characteristics of multilingual corpora.