\section{Related Work}
% \label{sec: realted}

% \subsection{ECG Representation Learning}
% Recently, ECG self-supervised learning (eSSL) has shown promise in learning ECG representations from unannotated signals **Liu et al., "Unsupervised ECG Classification with Self-Supervised Learning"**. Contrastive methods such as CLOCS **Sarkar et al., "Contrastive Learning for ECG Signal Analysis"** and ASTCL **Kim et al., "ASTCL: Temporal and Spatial Invariance in ECG Representation Learning"** explore temporal and spatial invariance, while generative techniques **Li et al., "Generative Adversarial Networks for ECG Data Augmentation"** focus on masked segment reconstruction. However, both approaches often lack clinical domain knowledge and are limited to single-modality settings, restricting the quality of learned representations.

% Multimodal learning has shown success in multiple biomedical applications **Rajpurkar et al., "MedMNIST: A Large-Scale Medical Image Classification Benchmark"**. However, ECG signals pose unique challenges due to their complex spatial-temporal structure, necessitating well-tailored modeling. As a result, few studies have explored multimodal ECG learning.  **Haque et al., "Multimodal ECG and EHR Data for Clinical Decision Support Systems"** demonstrated the effectiveness of combining ECG and EHR data using large language models (LLMs) to rewrite textual reports. However, their work is restricted to private datasets, making reproducing and comparisons challenging. Other works such as **Xu et al., "Multimodal ECG Learning for Zero-Shot Classification"** explored multimodal ECG learning for zero-shot classification. However, their methods were over simplistic: They align signals with text without sufficiently capturing the distinctiveness of individual ECG leads, and rely on naive category names as prompts, which fail to capture relative patterns, leading to suboptimal performance. Their limited evaluations on small datasets also fall short of fully assessing multimodal ECG learning in real-world scenarios.
% Additionally, works such as **Chen et al., "ECG-to-Text Generation for Clinical Diagnosis"** focus on ECG-to-text generation tasks, but their results are not publicly accessible, making reproducing and comparisons difficult.

% MERL**Kim et al., "Multimodal ECG Learning for Zero-Shot Classification"** is the first open-source study to demonstrate the potential of ECG multimodal learning in zero-shot classification and linear probing across diverse datasets. Therefore, we mainly compare our work to MERL. However, like other methods, MERL relies on all 12 ECG leads as input and cannot handle arbitrary lead combinations, limiting its applicability in real-world clinical scenarios where all 12 leads may not always be available ____

% \subsection{Knowledge Enhanced Medical Multimodal Learning}
% Leveraging medical knowledge to improve medical multimodal learning has advanced significantly, particularly in the radiograph domain, with methods like MedKLIP **Liu et al., "Medical Knowledge-Incorporated Graph Convolutional Network"**, KAD **Kim et al., "Knowledge-Augmented Deep Network for Medical Image Analysis"** and MAVL **Xu et al., "Multimodal Attention-Based Vision-Language Learning"** ____ . These approaches focus on extracting structured knowledge, such as clinical entities from free-text radiology reports, and using this information as an additional supervisory signal to guide multimodal learning. Many models mimic radiological practices or modify structures based on diagnostic routines ____ . However, they rely heavily on well-annotated knowledge graphs, such as RadGraph **Haque et al., "Radiograph Knowledge Graph Construction for Medical Diagnosis"** and Chest ImaGenome **Rajpurkar et al., "Chest ImaGenome: A Large-Scale Chest X-Ray Dataset"** ____ , which require substantial human annotation and are limited to the radiology domain.
% Due to the distinct nature of ECG signals compared to radiographs, the above pipelines cannot be directly adapted for ECG multimodal learning. 
% Furthermore, CVD has a clear hierarchical structure because conditions can have multiple subtypes, such as myocardial infarction, which can be further classified as inferior or anterior myocardial infarction ____ . Unlike lung diseases, typically categorized by morphological or pathological patterns rather than distinct region based subtypes ____ , directly using only the entity from an ECG report can lead to information loss by ignoring the superclass or subtypes.

% \subsection{Challenge in Partial Leads ECG Input}
% Currently, full 12 leads ECG data dominates publicly accessible ECG datasets ____ . However, in real clinical scenarios, obtaining a standard 12 leads ECG can be excessive and often requires advanced clinical knowledge, which may not always be readily available ____ . This makes partial-lead ECG data both crucial and common for practical applications. Despite its importance, partial leads issue is often overlooked and remain unaddressed in existing ECG multimodal representation learning studies. 
% To handle partial lead inputs across various downstream tasks, in this work, we design lead-specific processing and dynamic lead masking strategies that enable our model to accept any combination of ECG leads as input.  adaptable to various clinical scenarios ____ . We evaluate our model on extensive downstream tasks with partial lead inputs, demonstrating its ability to recognize and adapt to the lead-specific nature of ECG signals.

\begin{figure*}[t!]
    \centering
    \vspace{-5pt}
    \includegraphics[width=0.95\linewidth]{figures/framework.png}
    \vspace{-8pt}
    \caption{
    Comparison between classical ECG multimodal learning and our K-MERL framework.  
\textbf{(a):} The classical approaches (e.g., MERL **Kim et al., "Multimodal ECG Learning for Zero-Shot Classification"**) are suboptimal: they processes all leads in a lead-agnostic manner and naively align ECG signals directly free-text reports.  
\textbf{(b):} K-MERL introduces lead-specific processing and lead \& segment masking to capture spatial-temporal patterns unique to each lead. It also extracts cardiac-related entities from reports as structured knowledge and aligns them with ECG features to enhance multimodal learning, thereby reducing the complexity introduced by the grammatical structure of free-text reports.
    }
    \label{fig: framework}
    \vspace{-15pt}
\end{figure*}