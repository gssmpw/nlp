@Article{bao19,
AUTHOR = {Bao, Haojing and Tan, Qingchang and Liu, Siyuan and Miao, Jianwei},
TITLE = {Computer Vision Measurement of Pointer Meter Readings Based on Inverse Perspective Mapping},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3729},
URL = {https://www.mdpi.com/2076-3417/9/18/3729},
ISSN = {2076-3417},
DOI = {10.3390/app9183729}
}

@ARTICLE{cai20,
       author = {{Cai}, Weidong and {Ma}, Bo and {Zhang}, Liu and {Han}, Yongming},
        title = "{A pointer meter recognition method based on virtual sample generation technology}",
      journal = {Measurements},
     keywords = {Pointer meter recognition, Virtual sample generation, Convolutional neural networks, End-to-end model, Computer vision},
         year = 2020,
        month = oct,
       volume = {163},
          eid = {107962},
        pages = {107962},
          doi = {10.1016/j.measurement.2020.107962},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020Meas..16307962C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@InProceedings{Howells_2021_CVPR,
  author       = "Howells, B. and Charles, J. and Cipolla, R.",
  title        = "Real-time analogue gauge transcription on mobile phone",
  booktitle    = "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops",	
  year         = "2021",
}

@inproceedings{dial1,
  title={Deep learning for image-based automatic dial meter reading: Dataset and baselines},
  author={Salomon, Gabriel and Laroca, Rayson and Menotti, David},
  booktitle={IJCNN},
  year={2020},
}

@article{dial2,
  title={Towards image-based automatic meter reading in unconstrained scenarios: A robust and efficient approach},
  author={Laroca, Rayson and Araujo, Alessandra B and Zanlorensi, Luiz A and De Almeida, Eduardo C and Menotti, David},
  journal={IEEE Access},
  volume={9},
  pages={67569--67584},
  year={2021},
  publisher={IEEE}
}

@article{dial3,
  title={{A fully AI-based system to automate water meter data collection in Morocco country}},
  author={Naim, Ayman and Aaroud, Abdessadek and Akodadi, Khalid and El Hachimi, Chouaib},
  journal={Array},
  volume={10},
  pages={100056},
  year={2021},
  publisher={Elsevier}
}

@article{Alexeev20,
  title={A highly efficient neural network solution for automated detection of pointer meters with different analog scales operating in different conditions},
  author={Alexeev, Alexey and Kukharev, Georgy and Matveev, Yuri and Matveev, Anton},
  journal={Mathematics},
  volume={8},
  number={7},
  pages={1104},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{5673a1d18644461d895f9b7c6ecfe2e1,
    title = "Executive Dysfunction and the Prefrontal Cortex",
    author = "Jones, {David T.} and Jonathan Graff-Radford",
    note = "Publisher Copyright: {\textcopyright} Lippincott Williams \& Wilkins.",
    year = "2021",
    month = dec,
    day = "1",
    doi = "10.1212/CON.0000000000001009",
    language = "English (US)",
    volume = "27",
    pages = "1586--1601",
    journal = "CONTINUUM Lifelong Learning in Neurology",
    issn = "1080-2371",
    publisher = "Lippincott Williams and Wilkins",
    number = "6"
}

@article{DBLP:journals/corr/abs-2410-11756,
  author       = {Isaac R. Galatzer{-}Levy and
                  Jed McGiffin and
                  David Munday and
                  Xin Liu and
                  Danny Karmon and
                  Ilia Labzovsky and
                  Rivka Moroshko and
                  Amir Zait and
                  Daniel McDuff},
  title        = {Evidence of Cognitive Deficits and Developmental Advances in Generative
                  {AI:} {A} Clock Drawing Test Analysis},
  journal      = {CoRR},
  volume       = {abs/2410.11756},
  year         = {2024}
}

@inproceedings{yang2022its,
  author       = {Charig Yang and
                  Weidi Xie and
                  Andrew Zisserman},
  title        = {It's About Time: Analog Clock Reading in the Wild},
  booktitle    = {{CVPR}},
  pages        = {2498--2507},
  publisher    = {{IEEE}},
  year         = {2022}
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya et al.},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}
@misc{geminiteam2024,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={Gemini Team and Rohan Anil and Sebastian Borgeaud and Jean-Baptiste Alayrac and Jiahui Yu and Radu Soricut et al.},
      year={2024},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.11805}, 
}
@misc{anthropic_claude3_5_sonnet,
  author       = {Anthropic},
  title        = {Claude 3.5 - Sonnet},
  howpublished = {\url{https://www.anthropic.com/news/claude-3-5-sonnet}},
  note         = {Accessed: 2024-12-06},
  year         = {2024}
}
@misc{lama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle et al.},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}
@misc{qwen2technicalreport,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou et al.},
      year={2024},
      eprint={2407.10671},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.10671}, 
}
@article{yao2024minicpm,
  title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},
  journal={arXiv preprint arXiv:2408.01800},
  year={2024},
  url = {https://arxiv.org/abs/2408.01800}
    
}

@InProceedings{Yue_2024_CVPR,
    author    = {Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and Wei, Cong and Yu, Botao and Yuan, Ruibin and Sun, Renliang and Yin, Ming and Zheng, Boyuan and Yang, Zhenzhu and Liu, Yibo and Huang, Wenhao and Sun, Huan and Su, Yu and Chen, Wenhu},
    title     = {MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {9556-9567},
    url = {https://openaccess.thecvf.com/content/CVPR2024/html/Yue_MMMU_A_Massive_Multi-discipline_Multimodal_Understanding_and_Reasoning_Benchmark_for_CVPR_2024_paper.html}
}

@inproceedings{
lu2024mathvista,
title={MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},
author={Pan Lu and Hritik Bansal and Tony Xia and Jiacheng Liu and Chunyuan Li and Hannaneh Hajishirzi and Hao Cheng and Kai-Wei Chang and Michel Galley and Jianfeng Gao},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=KUNzEQMWU7}
}

@inproceedings{
kazemi2024remi,
title={Re{MI}: A Dataset for Reasoning with Multiple Images},
author={Mehran Kazemi and Nishanth Dikkala and Ankit Anand and Petar Devic and Ishita Dasgupta and Fangyu Liu and Bahare Fatemi and Pranjal Awasthi and Sreenivas Gollapudi and Dee Guo and Ahmed Qureshi},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
url={https://openreview.net/forum?id=930e8v5ctj}
}

@inproceedings{
kazemi2024geomverse,
title={GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning},
author={Mehran Kazemi and Hamidreza Alvari and Ankit Anand and Jialin Wu and Xi Chen and Radu Soricut},
booktitle={AI for Math Workshop @ ICML 2024},
year={2024},
url={https://openreview.net/forum?id=1AUbiBrOF1}
}
@inproceedings{DetToolChain,
author = {Wu, Yixuan and Wang, Yizhou and Tang, Shixiang and Wu, Wenhao and He, Tong and Ouyang, Wanli and Torr, Philip and Wu, Jian},
title = {DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM},
year = {2024},
isbn = {978-3-031-73410-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-73411-3_10},
doi = {10.1007/978-3-031-73411-3_10},
abstract = {We present DetToolChain, a novel prompting paradigm, to unleash the zero-shot object detection ability of multimodal large language models (MLLMs), such as GPT-4V and Gemini. Our approach consists of a detection prompting toolkit inspired by high-precision detection priors and a new Chain-of-Thought to implement these prompts. Specifically, the prompts in the toolkit are designed to guide the MLLM to focus on regional information (e.g., zooming in), read coordinates according to measure standards (e.g., overlaying rulers and compasses), and infer from the contextual information (e.g., overlaying scene graphs). Building upon these tools, the new detection chain-of-thought can automatically decompose the task into simple subtasks, diagnose the predictions, and plan for progressive box refinements. The effectiveness of our framework is demonstrated across a spectrum of detection tasks, especially hard cases. Compared to existing state-of-the-art methods, GPT-4V with our DetToolChain improves state-of-the-art object detectors by +21.5\%AP50 on MS COCO Novel class set for open-vocabulary detection, +24.23\%Acc on RefCOCO val set for zero-shot referring expression comprehension, +14.5\%AP on D-cube describe object detection FULL setting. The codes shall be released upon acceptance.},
booktitle = {Computer Vision – ECCV 2024: 18th European Conference, Milan, Italy, September 29–October 4, 2024, Proceedings, Part XXXII},
pages = {164–182},
numpages = {19},
keywords = {Mulitmodal Large Language Model, Prompting, Detection},
location = {Milan, Italy}
}
@inproceedings{MM1,
author = {McKinzie, Brandon and Gan, Zhe and Fauconnier, Jean-Philippe and Dodge, Sam and Zhang, Bowen and Dufter, Philipp and Shah, Dhruti and Du, Xianzhi and Peng, Futang and Belyi, Anton and Zhang, Haotian and Singh, Karanjeet and Kang, Doug and H\`{e}, Hongyu and Schwarzer, Max and Gunter, Tom and Kong, Xiang and Zhang, Aonan and Wang, Jianyu and Wang, Chong and Du, Nan and Lei, Tao and Wiseman, Sam and Lee, Mark and Wang, Zirui and Pang, Ruoming and Grasch, Peter and Toshev, Alexander and Yang, Yinfei},
title = {MM1: Methods, Analysis and Insights from Multimodal LLM Pre-training},
year = {2024},
isbn = {978-3-031-73396-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-73397-0_18},
doi = {10.1007/978-3-031-73397-0_18},
abstract = {In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published multimodal pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models, including both dense variants up to 30B and mixture-of-experts (MoE) variants up to 64B, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.},
booktitle = {Computer Vision – ECCV 2024: 18th European Conference, Milan, Italy, September 29–October 4, 2024, Proceedings, Part XXIX},
pages = {304–323},
numpages = {20},
location = {Milan, Italy}
}
@misc{fu2024mmecomprehensiveevaluationbenchmark,
      title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models}, 
      author={Chaoyou Fu and Peixian Chen and Yunhang Shen and Yulei Qin and Mengdan Zhang and Xu Lin and Jinrui Yang and Xiawu Zheng and Ke Li and Xing Sun and Yunsheng Wu and Rongrong Ji},
      year={2024},
      eprint={2306.13394},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2306.13394}, 
}
@inproceedings{
zhang2025mmerealworld,
title={{MME}-RealWorld: Could Your Multimodal {LLM} Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?},
author={YiFan Zhang and Huanyu Zhang and Haochen Tian and Chaoyou Fu and Shuangqing Zhang and Junfei Wu and Feng Li and Kun Wang and Qingsong Wen and Zhang Zhang and Liang Wang and Rong Jin},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=k5VHHgsRbi}
}
@misc{deitke2024molmopixmoopenweights,
      title={Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models}, 
      author={Matt Deitke and Christopher Clark and Sangho Lee and Rohun Tripathi and Yue Yang and Jae Sung Park and Mohammadreza Salehi and Niklas Muennighoff and Kyle Lo and Luca Soldaini and Jiasen Lu and Taira Anderson and Erin Bransom and Kiana Ehsani and Huong Ngo and YenSung Chen and Ajay Patel and Mark Yatskar and Chris Callison-Burch and Andrew Head and Rose Hendrix and Favyen Bastani and Eli VanderBilt and Nathan Lambert and Yvonne Chou and Arnavi Chheda and Jenna Sparks and Sam Skjonsberg and Michael Schmitz and Aaron Sarnat and Byron Bischoff and Pete Walsh and Chris Newell and Piper Wolters and Tanmay Gupta and Kuo-Hao Zeng and Jon Borchardt and Dirk Groeneveld and Crystal Nam and Sophie Lebrecht and Caitlin Wittlif and Carissa Schoenick and Oscar Michel and Ranjay Krishna and Luca Weihs and Noah A. Smith and Hannaneh Hajishirzi and Ross Girshick and Ali Farhadi and Aniruddha Kembhavi},
      year={2024},
      eprint={2409.17146},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.17146}, 
}
@misc{ghosal2024languagemodelspuzzleprodigies,
      title={Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning}, 
      author={Deepanway Ghosal and Vernon Toh Yan Han and Chia Yew Ken and Soujanya Poria},
      year={2024},
      eprint={2403.03864},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.03864}, 
}