%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{verbatim}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{algpseudocode}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\DeclareMathOperator{\polylog}{polylog}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{What makes a good feedforward computational graph?}

\begin{document}

\twocolumn[
\icmltitle{What makes a good feedforward computational graph?}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Alex Vitvitskyi}{gdm}
\icmlauthor{Jo\~{a}o G.M. Ara\'{u}jo}{gdm}
\icmlauthor{Marc Lackenby}{ox}
\icmlauthor{Petar Veli\v{c}kovi\'{c}}{gdm}
\end{icmlauthorlist}

\icmlaffiliation{gdm}{Google DeepMind}
\icmlaffiliation{ox}{University of Oxford}

\icmlcorrespondingauthor{Alex Vitvitskyi}{avlife@google.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
As implied by the plethora of literature on graph rewiring, the choice of \emph{computational graph} employed by a neural network can make a significant impact on its downstream performance. Certain effects related to the computational graph, such as under-reaching and over-squashing, may even render the model incapable of learning certain functions. Most of these effects have only been thoroughly studied in the domain of \emph{undirected} graphs; however, recent years have seen a significant rise in interest in \emph{feedforward} computational graphs: directed graphs without any back edges. In this paper, we study the desirable properties of a feedforward computational graph, discovering two important complementary measures: \emph{fidelity} and \emph{mixing time}, and evaluating a few popular choices of graphs through the lens of these measures. Our study is backed by both theoretical analyses of the metrics' asymptotic behaviour for various graphs, as well as correlating these metrics to the performance of trained neural network models using the corresponding graphs.
\end{abstract}

\section{Introduction}

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{ffg.pdf}
    \caption{In this paper, we propose two measures which can be used to assess suitability of \emph{feedforward} computational graphs for neural networks -- 
    \textbf{mixing time} (lower is better; Sec. \ref{sec:mix}) and (normalised) \textbf{minimax fidelity} (higher is better; Sec. \ref{sec:fide}). Here we plot these metrics as attained by various graph generators---at each point in a sequence, the number of nodes doubles, starting from $16$. Using FunSearch \citep{FunSearch2023}, we discover the \textbf{FS graphs}, which have favourable $O(\polylog n)$ mixing time, while retaining higher fidelity compared to a fully connected graph (Sec. \ref{sec:fs}). Several graphs may be viewed in App. \ref{app:gallery}.}
    \label{fig:ffg}
\end{figure}

Modern deep learning workloads frequently necessitate processing of \emph{sequential} inputs, such as words in a sentence \citep{NIPS2014_a14ac55a}, samples of an audio recording \citep{van2016wavenet}, partially ordered nodes in a graph \citep{thost2021directed}, execution steps of an algorithm \citep{velivckovic2022clrs}, or snapshots of edits made to temporal graphs \citep{rossi2020temporal}. In addition, many important self-supervised learning tasks require efficiently predicting future evolution of such inputs, with examples ranging from temporal link prediction \citep{huang2024temporal} to next token prediction \citep{radford_improving_2018}. 

In regimes like this, it is important to be able to train such models \emph{scalably} and \emph{without leaking} ground-truth data about the future input parts that need to be predicted. As such, many modern architectures resort to \emph{feedforward} computational graphs, wherein information may only flow from older samples towards newer ones---never in reverse! Applying such a graph -- also known as a ``causal mask'' \citep{srambical2024going} -- allows for scalable training over entire chunks of the input at the same time.

This naturally invites the question: \emph{what makes a good feedforward computational graph?} Alternately worded, which considerations need to be taken into account when deciding which feedforward graph to use (see Figure \ref{fig:ffg})?

Here we propose two suitable and complementary ways to measure suitability of feedforward graphs: \textbf{mixing time}---the speed at which input data converges towards a stationary distribution---and \textbf{minimax fidelity}---the sharpness of the data is as it propagates through the graph. We supplement our measures with thorough theoretical derivations across several graph generators, and correlate them to empirical performance, paving the way to future studies in the area.

\section{Motivation}

We were inspired to seek an answer to this question, given that there is already a rich, extensive body of work on studying undirected computational graphs (commonly referred to as \emph{graph rewiring}). Therein, important issues such as oversmoothing \citep{li2018deeper,oono2019graph,keriven2022not}, oversquashing \citep{alon2021on,giovanni2024how} and under-reaching \citep{Barcelo2020The} have all been identified, and related to the input graph topology. In response, a substantial amount of methods have been proposed to modify the provided input graph for improved diffusion \citep{gasteiger2019diffusion}, curvature \citep{topping2022understanding,fesser2024mitigating}, effective resistance \citep{arnaiz2022diffwire}, or reducing smoothing \citep{azabou2023half} and commute time \citep{sterner2024commute}.

Eventually, the focus of study expanded beyond ``how to modify an input graph to be better?'' towards ``what makes a good input graph?'', propelling the discovery of \emph{task-agnostic} graphs that are guaranteed to have good topological properties. This question is well-studied in mathematics, where it had led to the advent of \emph{expander graphs} \citep{kowalski2019introduction}; graphs with highly sparse topologies but incredibly good information propagation. Once expanders have been discovered in the context of graph machine learning, they have seen equal application among graph neural networks \citep{deac2022expander,christie2023higher,wilson2024cayley} and graph Transformers \citep{shirzad2023exphormer,shirzad2024even}.

Unfortunately, to the best of our knowledge, the state-of-the-art is not as rich in the domain of feedforward graphs. This includes the domain of mathematics, wherein many important concepts have not been generalised to the directed case as they may rely on spectral properties of the graph structure \citep{chung1997spectral}, and many spectral properties are ill-defined on a feedforward graph. In terms of practical usage, most of the heavy lifting is done either by \emph{fully connected} feedforward graphs or \emph{locally-connected} sliding-window graphs. And while recent work has identified limitations of high-indegree feedforward graphs through over-squashing \citep{barbero2024transformersneedglassesinformation} and dispersion \citep{velickovic2024softmaxforsharpoutofdistribution}, most such works do not actively offer a different computational graph structure, nor do they offer any principles that can be used to derive one. We seek to fill this gap.

{\bf What's in a good metric?} Both of the above papers make it clear---at least if size generalisation is a desirable property---that it is beneficial to \emph{limit the in-degree of each node in the feedforward graph.}

A good metric should be able to help us, in the very least, \textbf{compare against different graph distributions with \emph{same asymptotic in-degree budget}.}

It might also be helpful if the metric would be able to hint to practitioners at what point the in-degree budgets become \textbf{problematic}---specifically, this means they shouldn't be optimised for the fully connected graph. This is not strictly necessary, as the papers above already provide ample proof.

\section{Theoretical foundations}

In this work, we study \textbf{feedforward graphs}, $\mathcal{G}=(\mathcal{V},\mathcal{E})$, i.e., graphs where nodes are given by an ordered set of $n$ integers ($\mathcal{V}=\mathbb{Z}_n$) and edges are constrained to only go \emph{forwards}; that is, $(a,b)\in\mathcal{E}\implies a\leq b$. We will denote by $\tau\in\mathcal{V}$ the \emph{sink vertex} of that graph ($\tau = n - 1$), a node which is used to drive decision-making, and which is not allowed to have any outward edges (except to itself).

Much like a degree in an undirected graph, a node $i$ in a feedforward graph has an \emph{indegree} $\delta_{i\leftarrow}$ and an \emph{outdegree} $\delta_{i\rightarrow}$, counting the number of incoming and outgoing edges to/from node $i$.

As an illustrative example, two types of feedforward graph are commonly used for machine learning tasks today:
\begin{itemize}
    \item The \textbf{fully connected} feedforward graph, which draws an edge between every allowed pair of nodes; $\mathcal{E} = \{(a, b)\ |\ a \leq b\}$. Indegrees and outdegrees in this graph are $\delta_{i\leftarrow} = i + 1, \delta_{i\rightarrow} = (n - i)$.
    \item The \textbf{locally connected} feedforward graph, which draws an edge between all allowed pairs of nodes up to a distance $\kappa\geq 0$ apart: $\mathcal{E} = \{(a, b)\ |\ b - \kappa \leq a\leq b\}$. A special case of $\kappa = 1$ is known as a \textbf{line graph}. Indegrees and outdegrees are $\delta_{i\leftarrow}=\delta_{i\rightarrow}=\kappa + 1$.
\end{itemize}

We now outline three desirable properties which we will typically assume during our exploration:

\paragraph{Self-edges.} We will always assume all self-edges are present in the graph -- that is, $(i, i)\in\mathcal{E}$ for all $i\in\mathcal{V}$, unless otherwise stated. This is generally an important design decision for improving data retention.

\paragraph{Unique sinks.} As we will be tracking how easily information from each node can reach the designated sink node, $\tau$, it is highly desirable that our graph generator does not create additional sink nodes of any kind (beyond the final node, of course). Any additional sink nodes imply an impossibility to reach the final sink from such nodes, and they are hence effectively excluded from decision making.

\paragraph{Self-similarity.} While most of our theory will concern the flow of information specifically into $\tau$, it is important to note that in real workloads, \emph{any} node might be used as a sink node (over an appropriately sampled sub-input). As such, we focus our attention on \emph{self-similar} feedforward graphs, wherein we can expect similar flow properties to all nodes (including ones in the middle).

\section{Mixing time: Tracking path complexity}\label{sec:mix}

We are now ready to work our way towards defining the first of our two measures, the \emph{averaged mixing time} of the feedforward graph. This measure tracks how quickly information travels towards the sink, by carefully analysing the expected path length from each node to the sink. Clearly, the aim is to keep this value within reasonable upper bounds.

\subsection{Basic notions}

For a feedforward graph of $n$ nodes, $\mathcal{G}=(\mathbb{Z}_n,\mathcal{E})$, let $\mathbf{A}\in\mathbb{R}^{n\times n}$ be its adjacency matrix, defined in the usual way:
\begin{equation}
    a_{ij} = \begin{cases}
        1 & (j, i) \in \mathcal{E}\\
        0 & (j, i) \notin \mathcal{E}
    \end{cases}
\end{equation}
keeping in mind that $\mathbf{A}$ must be lower-triangular due to the feedforward constraint.

Its \emph{walk matrix} $\mathbf{W}$ is given by 
$$w_{ij} = 
\begin{cases}
1/\delta_{j\rightarrow} & (j,i)\in\mathcal{E} \\
0 & (j,i)\notin\mathcal{E}.
\end{cases}
$$
So $\mathbf{W}$ is the transition matrix for the \emph{(lazy) random walk} where there is an equal probability of leaving vertex $j$ along any of its outgoing edges. This will be well defined as long as $\delta_{j\rightarrow} > 0$ for all $j\in\mathbb{Z}_n$; we are guaranteeing this condition by requiring all self-edges within the graph.

%There are various natural notions of lazy random walks and their associated matrices:
%$$\overline W_{ij} = 
%\begin{cases}
%1/(2d_{out}(j)) & \text{if there is a directed edge from } j %\text { to } i \\
%1/2 & \text{if } i=j \\
%0 & \text{otherwise.}
%\end{cases}
%$$
%$$\tilde W_{ij} = 
%\begin{cases}
%1/(d_{out}(j)+1) & \text{if there is a directed edge from } j \text{ to } i \\
%1/(d_{out}(j)+1) & \text{if } i=j \\
%0 & \text{otherwise.}
%\end{cases}
%$$
%The matrix $\overline W$ corresponds to the random walk where there is $1/2$ probability of staying put at any vertex. Again this is not defined when there is a sink vertex. However, $\tilde W$ is always defined and seems most natural. It corresponds to the random walk on the graph where a loop is added at every vertex.

Note that for graphs where, for all $j\in\mathbb{Z}_n$, $\delta_{j\rightarrow} = \kappa$ for some constant, $\kappa$, the walk matrix is related to the usual adjacency matrix $\mathbf{A}$. Specifically
$$\mathbf{W} = \frac{1}{\kappa}\mathbf{A}.$$
Hence, in this case, the spectrum of the walk matrix is obtained from the spectrum of $\mathbf{A}$ by scalar multiplication.

\subsection{Stationary distributions}

A probability distribution $\boldsymbol{\pi}\in\mathbb{R}^n$ on the vertices is \emph{stationary} if $\mathbf{W}\boldsymbol{\pi} = \boldsymbol{\pi}$. In other words, $\boldsymbol{\pi}$ is unchanged by one step of the random walk.

It is a well-known result that a strongly connected graph has a unique stationary distribution. Here, a graph is \emph{strongly connected} if, for any ordered pair of vertices, there is an oriented path joining them. However, the graphs we will consider will be feedforward and hence not strongly connected. In that case, we have the following useful result.

\begin{lemma}
Let $\mathcal{G}$ be a feedforward graph with a unique sink vertex $\tau$. Then there is a unique stationary distribution for $\mathbf{W}$, namely $\mathbf{1}_\tau$, the probability distribution taking the value $1$ at $\tau$ and $0$ elsewhere.
\end{lemma}
\begin{proof} Certainly $\mathbf{1}_\tau$ is stationary. To see that it is unique, consider a distribution $\boldsymbol{\pi}$ other than $\mathbf{1}_\tau$. Let $i < n$ be its smallest vertex with $\pi_i \not=0$. When applying the random walk matrix $\mathbf{W}$, we can see that the probability of being at $i$ after one step is $\pi_i/\delta_{i\rightarrow}$. Since $\delta_{i\rightarrow} > 0$, we deduce that $\boldsymbol{\pi}$ is not stationary.
\end{proof}

%\begin{lemma}
%For any finite directed acyclic graph $G$ with $n$ vertices, there is a labelling of its vertices by $\{1, \dots, n \}$ such that whenever an oriented edge runs from $i$ to $j$, then $i < j$. Furthermore when $G$ has a sink vertex $\tau$, then this may be labelled $n$.
%\end{lemma}

%\begin{proof} We induct on the number of vertices. Since $G$ has no cycles, it has a source vertex. Label this $1$. Let $G'$ be the graph obtained by removing this vertex. By induction, its vertices may labelled from $\{2, \dots, n \}$, such that whenever an oriented edge runs from $i$ to $j$, then $i < j$. This gives the required labelling of the vertices of $G$.
%\end{proof}

\subsection{Mixing times}

There are several definitions of mixing time in the literature. Here is one. The \emph{mixing time} is the smallest value of $t$ such that for any starting distribution $\mathbf{x}$,
$$\|\mathbf{W}^t \mathbf{x} - \boldsymbol{\pi}\|_1 < 1/4.$$
Here, the $L^1$ norm is used on probability distributions, i.e., for two probability distributions $\mathbf{x}$ and $\mathbf{y}$,
$$\|\mathbf{x}-\mathbf{y}\|_1 = \sum_i |x_i - y_i|.$$
There is nothing special in the use of $1/4$ here. Any fixed $0 < \epsilon < 1/2$ would work. 
In our set-up, it is reasonable not consider a minimum over starting distributions $\mathbf{x}$ but an `averaged' version:
$$\frac{1}{n} \sum_i \|\mathbf{W}^t\mathbf{e}_i - \boldsymbol{\pi}\|_1,$$
where $\mathbf{e}_i$ is the probability distribution concentrated on vertex $i$. If we let $\boldsymbol{\Pi}$ be the square matrix that has $\mathbf{\pi}$ in each column, then this is 
$$\frac{1}{n} \|\mathbf{W}^t - \boldsymbol{\Pi} \|_1,$$
where now we are taking the $L^1$ norm on matrices: 
$$\|\mathbf{B}\|_1 = \sum_{ij} |b_{ij}|.$$ 
We call this quantity the \emph{averaged mixing time} \citep{diaz2024speeding}. We will now demonstrate its behaviour is as expected on two commonly used graph distributions.

\subsubsection{Line graphs}
Let $\mathcal{G}$ be the line graph from vertex $0$ to vertex $n - 1$, as previously defined. Suppose that we start a random walk at vertex $n-i-1$, i.e., the initial probability distribution is $\mathbf{e}_{n-i-1}$. We want to consider the probabilty of not reaching the terminal vertex after $t$ steps. We can think of flipping a coin $t$ times and we step right only if we get a head. We reach the terminal vertex if and only if we get at least $i$ heads. So the probability of not reaching it in $t$ steps is
$$\sum_{j<i} \left ({t \atop j} \right )2^{-t}.$$
The average of this value, over all $i$ between $0$ and $n-1$, is
$$\frac{1}{n} \sum_{i=0}^{n-1} \sum_{j<i} \left ({t \atop i} \right )2^{-t}
= \frac{1}{n} \sum_{i=0}^{n-1} (n-i-1) \left ({t \atop i} \right )2^{-t}.$$
When $n \geq t$,
$$\sum_{i=0}^{n-1} (n-1) \left ({t \atop i} \right )2^{-t} = (n-1)$$
$$\sum_{i=0}^{n-1} i \left ({t \atop i} \right )2^{-t} = \sum_{i=0}^t i \left ({t \atop i} \right )2^{-t} = t/2$$
So, the average value of the probability of not reaching the terminal vertex is
$$\frac{1}{n} \left( n - 1 - \frac{t}{2} \right ) = 1 - \frac{t + 1}{2n}.$$
This is less than $1/4$ when $t \geq 3n/2$. Hence, the averaged mixing time is $3n/2$, which is $O(n)$.

\subsubsection{Fully connected graphs}

Now consider the case when $\mathcal{G}$ is the fully connected feedforward graph. For convenience, we will label the graph with vertices $\{ 0, \dots, n \}$ where every pair of vertices $i$ and $j$ with $i < j$ are joined by an oriented edge from $j$ to $i$. (Hence, in this scenario, $0$ is the terminal vertex.) Then the average mixing time is of the order $O(\log n)$.

To prove this, say that we start at position $i_0$ and then move to position $i_1$, ending at $i_t$ after $t$ steps. Then the expected value of the vertex after $t$ steps is
$$\frac{1}{n+1} \sum_{i_0 = 0}^n \frac{1}{i_0+1} \sum_{i_1 = 0}^{i_1} \dots \sum_{i_{t-1} =0}^{i_{t-2}} \frac{1}{i_{t-1}+1} \sum_{i_t = 0}^{i_{t-1}} i_t$$
$$= \frac{1}{n+1} \sum_{i_0 = 0}^n \frac{1}{i_0+1} \sum_{i_1 = 0}^{i_1} \dots \sum_{i_{t-1} =0}^{i_{t-2}} \frac{1}{i_{t-1}+1} i_{t-1} (i_{t-1}+1)/2$$
and inductively, we can show that this is 
$$  \frac{n}{2^t}.$$
When $t > \log_2 4n^2$, this expectation is at most $1/(4n)$. This implies that the probability of being on the terminal vertex $0$ is at least $3/4$. Hence, the mixing time is  at most $\log_2 4n^2 = 2 + 2\log n$.

%Now consider the case $\mathcal{G}$ is the fully connected feedforward graph. If we start at vertex $n - i - 1$, then after a single step, we have an equal probability at being at any of the later vertices. On average, we will end with $n - (i/2)$. So after $\log_2 n$ steps, we will almost surely be at the terminal vertex. Surely, the mixing time is going to be of the order $O(\log n)$.

%\subsection{Expander graphs}
%It seems reasonable to conjecture that these also have mixing time of the order $\log n$.

\subsection{Discussion about mixing time as a \emph{good} measure}
When the graph has fixed out-degree $\kappa$, then as observed before, 
$$\mathbf{W} = \frac{1}{\kappa}\mathbf{A}.$$ 
Hence $\kappa \mathbf{W}$ is the adjacency matrix $\mathbf{A}$ for the graph. Mixing time measures how close $\mathbf{W}^t \mathbf{e}_i$ is to the stationary distribution. In other words, it measures the size of $\mathbf{W}^t_{n-1,i}$. This is $1/\kappa^t$ times the $a_{n-1,i}$ entry of $\mathbf{A}^t$. Now this entry of $\mathbf{A}^t$ counts the \emph{number of oriented paths} of length $t$ in $\mathcal{G}$ from vertex $i$ to the terminal vertex. So, we have the conjectured equivalence: mixing time is small (e.g. $O(\log n)$) if and only if there are `lots' of paths from `most' vertices to $\tau$. As having many paths seems useful for efficient data propagation, mixing time seems a good measure of how efficient our network will be under this computational graph. 

Typically, we will treat mixing time as a \emph{cutoff}, determining which graphs would require too many layers---much like the \emph{under-reaching problem} \citep{Barcelo2020The}. It is also interesting to note that the fully connected graph does not have optimal mixing time---in fact, the additional paths create a \emph{distraction}, and the mixing time is optimised by the ``feedforward star'' graph with edges $\mathcal{E} = \{(i, \tau)\ |\ i\in\mathcal{V}\}$. This relates to an observation by \citet{giovanni2024how}, showing how fully-connected \emph{undirected} graphs do not have optimal commute time due to distracting additional edges. However, we will not favour such solutions due to a lack of self-similarity.

\subsection{Discussion about the spectrum}
In the usual theory of random walks on strongly connected graphs, mixing time is related to $\lambda_1$. This is the maximal eigenvalue of the normalised adjacency matrix (other than the eigenvalue $1$).

However, in our case, there is \emph{not} an obvious interpretation of mixing time in terms of the spectrum of $\mathbf{W}$. This is because $\mathbf{ W}$ is lower-triangular. Hence, its spectrum is equal to its diagonal entries. Hence, as a set, the spectrum is just
$$\{ 1/\delta_{i\rightarrow}: 0 \leq i \leq n-1 \}.$$
In general, these matrices are not diagonalisable, and so the total multiplicity of these eigenvalues may be less than $n$.
For example, suppose that $\mathcal{G}$ has constant out-degree $\kappa$, apart from the terminal vertex. Then $\mathbf{W}$ just has two eigenvalues: $1$ and $1/\kappa$. 

\section{Minimax fidelity: Information sharpness}\label{sec:fide}

Mixing time provides an excellent estimate of how long will information need to travel in a graph---it is generally a good idea to keep it low. However, it is not the whole story: it is irrelevant if information from a given node travels fast to the sink if only a small proportion of it makes it through. To quantify the extent to which information sharply reaches the sink, we will be using the \emph{minimax fidelity} metric.

Additionally, we require that every node must have a positive in-degree; that is, $\delta_{i\leftarrow} > 0$ for all $i\in\mathcal{V}$. Note that this will be guaranteed by the self-edge property.

We want to track how ``pre-disposed'' this graph is to allowing information to travel freely in it. This relates to the over-squashing theorem in \citet{barbero2024transformersneedglassesinformation}, but unlike them, we do not assume \emph{any} degree of sharpness in choosing how information travels: specifically, at every step, we assume each node intakes the \emph{average} value of all of the nodes over its incoming edges.

\subsection{Fidelity}

Starting from our adjacency matrix as before, we now derive a ``diffusion'' process specified by the following matrix $\boldsymbol{\Delta}$:
\begin{equation}
    \Delta_{ij} = \begin{cases} 1/\delta_{i\leftarrow} & (j,i)\in\mathcal{E}\\
    0 & (j,i)\notin\mathcal{E}
    \end{cases}
\end{equation}
Note that this is a \emph{complementary} computation to the mixing time metric: while one normalises by \emph{row}, the other normalises by \emph{column}.
And, further, we are interested in how sharply represented can a particular node be in the diffused representations, especially ones that reach the sink vertex.

To simulate this, we start with an idealised setting where all the mass concentrates in a specific vertex, $i$. The vector representing its weight may be expressed as $\mathbf{e}_i\in\mathbb{R}^n$, which is a one-hot vector that is one in position $i$ and zero elsewhere.

One step of diffusion corresponds to a matrix multiplication by ${\boldsymbol{\Delta}}$:
\begin{equation}
(\boldsymbol{\Delta}\mathbf{x})_j = \sum_j{\Delta_{ij}x_j}
\end{equation}
(NB: this does not always yield a probability distribution!)

We can simultaneously estimate the diffusion properties for all $n$ possible initial vertices by stacking the $\mathbf{e}_i$ vectors, recovering the identity matrix, $\mathbf{I}$. Accordingly, after $t\geq 0$ layers of propagation, we can read off the coefficients of each item in each receiver by computing $\boldsymbol{\Delta}^{t}$.

Since we take particular care on how much information has reached \emph{the sink vertex}, we can define the \emph{\textbf{fidelity} of node $i$ at $t$ steps} as $\boldsymbol{\Delta}^{t}_{\tau i}$. This can be interpreted as: ``what is the coefficient of node $i$ in the weighted sum within $\tau$ after $t$ steps of averaging diffusion?''

An important point about the fidelity measure is that it does not always grow with increasing $t$---this is easy to observe in nodes near the end of the sequence:
\begin{proposition}\label{prop:sinkvul}
    Let $\mathcal{G}=(\mathbb{Z}_n,\mathcal{E})$ be a feedforward graph of $n > 1$ nodes with all self-edges and a unique sink vertex $\tau = n - 1$; that is, every node $i\in\mathbb{Z}_n$ is connected to $\tau$ by a path in $\mathcal{E}$. Then, if $\delta_{(n - 2)\leftarrow} > 1$, as $t\to\infty$, $\boldsymbol\Delta_{\tau, n-2}^{t}\rightarrow 0$, that is, the fidelity of node adjacent to the sink eventually vanishes if it has at least one nontrivial incoming edge.
\end{proposition}
\begin{proof}
    Initially, $\boldsymbol\Delta_{n-2,n-2}^{0} = 1$, and $\boldsymbol\Delta_{j,n-2}^{0} = 0$ for all $j\neq n-2$. Since information can only travel forwards in the graph, we can conclude that, if $j < i$,  $\boldsymbol\Delta_{ji}^{t} = 0$ for all $t$. Hence, it will be sufficient to track $\boldsymbol\Delta_{n-2,n-2}^{t}$ and $\boldsymbol\Delta_{\tau,n-2}^{t}$ over time for the purpose of this proof.
    
    Following the formula, we conclude $\boldsymbol\Delta_{n-2,n-2}^{t+1} = \Delta_{n-2,n-2}\boldsymbol\Delta_{n-2,n-2}^{t}=(\delta_{(n-2)\leftarrow})^{-1}\boldsymbol\Delta_{n-2,n-2}^{t}=(\delta_{(n-2)\leftarrow})^{-t}$. Since we assumed $\delta_{(n-2)\leftarrow} > 1$, this value will certainly decay towards zero as $t\rightarrow\infty$.
    
    Since there is only one sink vertex, $\tau$, it must have an in-degree $\delta_{\tau\leftarrow} > 1$: it must have a self-edge (by assumption) and it must have a direct incoming connection from at least one other preceding node. Further, at least one of those nodes must be $n-2$, otherwise it would introduce another sink. The fidelity of node $n-2$ at $t+1$ layers can hence be expressed as $\boldsymbol\Delta_{\tau,n-2}^{t+1}=\Delta_{\tau,n-2}\boldsymbol\Delta_{n-2,n-2}^{t} + \Delta_{\tau\tau}\boldsymbol\Delta_{\tau,n-2}^{t} = (\delta_{\tau\leftarrow})^{-1}\left((\delta_{(n-2)\leftarrow})^{-t} + \boldsymbol\Delta_{\tau,n-2}^{t}\right)$.
    
    Now, note that this expression is maximal when $\delta_{\tau\leftarrow}=\delta_{(n-2)\leftarrow}=2$. As such, we can bound $\boldsymbol\Delta_{\tau,n-2}^{t+1}\leq 2^{-t-1} + \boldsymbol\Delta_{\tau,n-2}^{t} / 2$. From this, we can prove by induction that $\boldsymbol\Delta_{\tau, n-2}^{t}\leq t/ 2^{t}$. The base case ($t=0$) clearly holds, as $\boldsymbol\Delta_{\tau,n-2}^{0} = 0 \leq 0$. From there, assuming the bound holds for $t$, we can show it holds for $t+1$ by substituting $\boldsymbol\Delta_{\tau,n-2}^{t+1}\leq 2^{-t-1} + (t/2^{t})/2 = (t+1)/2^{t+1}$.
    
    This upper bound on $\boldsymbol\Delta_{\tau,n-2}^{t}$ also decays towards zero as $t\rightarrow\infty$, settling the proof.
\end{proof}

\subsection{(Mini)max fidelity}

Given this result, it does not make sense to study long-term dynamics of fidelity for most nodes. Rather, we can track the ``best-case'' scenario for node $i$, as the \emph{maximal} fidelity it achieves over the lifetime of the diffusion; that is, $\phi_i = \max_{t} \boldsymbol\Delta^{t}_{\tau i}$. This is the most sharp we can ever extract node $i$'s features in the sink node\footnote{It is assumed implicitly that neural networks deeper than optimal may still be able to preserve this level of information of node $i$ by leveraging their residual connections \citep{he2016deep}.}.

Out of all of these sharpness measures, we are hunting for the ``weakest points'', where the model is particularly vulnerable to extracting a particular node's features, at any layer. As such, we compute the \textbf{minimax fidelity} as 
\begin{equation}
    \min_i\phi_i = \min_i\max_t\boldsymbol\Delta^{t}_{\tau i}
\end{equation}
We will now deploy this measure on some graph generators of interest, just as we did for mixing time.

It is worth noting that tracking how the signal evolves over time in this way is similar to the Dirichlet energy, which has been used extensively to study the over-smoothing effect in GNNs \citep{cai2020note,zhou2021dirichlet,rusch2022graph,di2022understanding}. One key difference is in the fact that the measure at hand here involves taking maxima and minima and hence is not as smooth to work with.

\subsubsection{Fully connected graphs (and normalised minimax fidelity)}

While fully connected graphs had highly favourable logarithmic mixing time, they immediately fully average information, and therefore their minimax fidelity is always $1/n$. This is potentially problematic, especially if hit with dispersion issues at inference time \citep{velickovic2024softmaxforsharpoutofdistribution}.

Taking this into account, and given the ubiquitous use of the fully connected feedforward graphs in contemporary architectures, we will often opt to report the \emph{normalised minimax fidelity}:
\begin{equation}
    n\min_i\phi_i = n\min_i\max_t\boldsymbol\Delta^t_{\tau i}
\end{equation}
which ensures that the fidelity of the fully connected graph is always $1$ (see Figure \ref{fig:ffg}), and provides an intuitive threshold for whether the fidelity is more or less favourable than fully connected graphs.

\subsubsection{Line graphs}

What line graphs sacrifice in mixing time (and hence tractability of certain problems) they make up for in fidelity. Indeed, line graphs are among the graph families with highest minimax fidelity. Their minimax fidelity can be expressed as follows\footnote{Diffusing a $1$ along the line graph basically involves generating normalised entries of a (trimmed) Pascal's triangle, which is why the binomial coefficients appear in this formula.}, for a graph of $n$ elements:
\begin{equation}
    \max_a \frac{\binom{a}{n - 1}}{2^a}
\end{equation}
This quantity also decays to zero as $n\rightarrow\infty$ (see Appendix \ref{appendix:minmax_line} for a proof using Stirling's approximation), but its normalised minimax fidelity consistently grows as $\sqrt{n / \pi}$ (see Fig. \ref{fig:ffg}---indeed, line graphs have the highest initial fidelity of all considered graphs).

\subsubsection{Erd\H{o}s-R\'{e}nyi graphs and oriented expanders}

So far, we have mainly examined line graphs and fully connected graphs -- which in ways correspond to two extremes with respect to the metrics we proposed. 

We may further see from Figure 1 that creating oriented versions of Erd\H{o}s-R\'{e}nyi \citep{erdos1960evolution} graphs (keeping indegrees fixed) as well as orienting graphs that are known to be undirected expanders (by assigning a random permutation to their nodes) results in negative performance on \emph{both} of our metrics compared to the fully connected graph. 

We will not study these graphs in more depth here, though it is worth remarking that fixed-indegree Erd\H{o}s-R\'{e}nyi graphs are very likely to introduce additional sinks, breaking one of our theory's key assumptions. Another reason why both of these distributions are likely to fail is that they are \emph{effectively unbiased} in terms of which incoming edges they sample -- an earlier node is equally likely to be assigned as an indegree neighbour as a latter one. As we've seen in Proposition \ref{prop:sinkvul}, latter nodes are particularly vulnerable to losing fidelity quickly. This motivates our next distribution under study.

\subsubsection{Poisson($p$) graphs}

Guided by this observation, we set out to construct a graph generator which is \emph{biased} to create edges towards the end of the sequence (so it preserves fidelity better) while still having a chance of generating an edge which is further away from the target node (so it shortens mixing time).

This combination of constraints led us to consider a graph which samples edges by going right-to-left, simulating a Poisson process with probability $p$. It samples in-degree neighbours of node $i$ (given an indegree budget) as follows:
\begin{algorithmic}
\State $\mathcal{E}\gets\mathcal{E}\cup\{(i, i)\}$
\State $\mathrm{edges}\gets 1$
\State $j \gets i - 1$
\While{$j \geq 0\wedge \mathrm{edges}< \mathrm{budget}$} 
    \State $\rho\sim U(0, 1)$
    \If {$\rho > p$}
        \State $\mathcal{E}\gets\mathcal{E}\cup\{(j, i)\}$
        \State $\mathrm{edges}\gets \mathrm{edges} + 1$
    \EndIf
    \State $j \gets j-1$
\EndWhile 
\end{algorithmic}
Different values of $p$ will yield graphs with different levels of locality ($p=0$ yields a local graph generator). In our experiments, we found that $p=0.2$ struck the best balance between improving fidelity and penalising mixing time, though the mixing time is still not nearly as controllable as some of the other graphs -- the profile of the metrics achieved by $\mathrm{Poisson}(0.2)$ is provided in Figure \ref{fig:ffg}.

\section{The FunSearch (FS) graph generator}\label{sec:fs}

\begin{figure*}
    \centering
    \includegraphics[width=0.33\linewidth]{fs_1.pdf}
    \hfill
    \includegraphics[width=0.33\linewidth]{fs_2.pdf}
    \hfill
    \includegraphics[width=0.33\linewidth]{fs_3.pdf}
    \caption{The evolution of the adjacency matrices generated by the FS graph generator. ({\bf Left:}) The original solution found by FunSearch, when asked to optimise minimax fidelity while keeping the mixing time under $2\log_2 n$. ({\bf Middle:}) Adapting the initial solution to bound the in-degree within each cluster. Each pair of successive blocks are now connected by a random bipartite expander. ({\bf Right:}) The final FS graph generator, leveraging a recursive construction to ensure self-similarity and sufficient sparsity.}\label{fig:FSG}
\end{figure*}

Apparently, even after trying out many hand-crafted sparse graph generators, we have not been able to strike the right balance between mixing time and fidelity. So we turned our attention to \emph{evolutionary methods}: as there's no clear method of constructing graphs which optimise the fidelity while keeping the mixing time within reasonable bounds, we used the FunSearch \citep{FunSearch2023} algorithm to produce graphs with good values for both metrics. The most promising result generated by FunSearch may be found in Figure \ref{fig:FSG} (Left), and it turns out to be an impactful motif. 

The core idea of this graph is to divide the nodes into $O(\log n)$ groups, then applying full bipartite graphs across successive chunks. Since there aren't almost any intra-cluster edges, random walks almost certainly always move from one cluster to the next, mixing in logarithmic time. And since each chunk only attends over a smaller number of nodes $O(n / \log n)$, this improves fidelity.

There are two core issues with this graph left to fix from an asymptotics point of view. First, the indegree of each node is $O(n)$ as $n\rightarrow\infty$, which is undesirable. Second, the graph does not exhibit self-similarity: it will only work well if the predictions will take place in the final node block, which will be exposed to the ``final triangle'' of full edges.
\begin{figure*}
    \includegraphics[width=0.33\linewidth]{plots_maxitem/graphs_first_max2.pdf}
    \includegraphics[width=0.33\linewidth]{plots_maxitem/graphs_second_max2.pdf}
    \includegraphics[width=0.33\linewidth]{plots_parity/parity-transformer.pdf}
    \caption{Performance profiles of GATs trained with various computational graphs, on the max retrieval task ({\bf left}), second max retrieval task ({\bf middle}), and parity task ({\bf right}), when trained up to size 256. The $x$-axis corresponds to test accuracy on sizes from $16$ to $1,024$.}
    \label{fig:empirical_results}
\end{figure*}

In order to fix these issues, we first note that each filled chunk of the matrix is essentially corresponding to edges of a bipartite graph. And constructions of bipartite expander graphs (which have excellent neighbourhood coverage) are well understood in mathematics. In Figure \ref{fig:FSG} (Middle), we provide one such construction, built by concatenating $\kappa$ random perfect matchings \citep{lubotzky1994discrete,sarnak1990some}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{to_take_scaling.pdf}
    \caption{Fitted scaling laws of log-minimax fidelity for different indegrees (of the form $k\log n$ for different values of $k$ over $n$ nodes) of sampled expander graphs in the FS graph.}
    \label{fig:scaling}
\end{figure}


To fix self-similarity, it is evident that all intra-cluster edges (the ``blank triangles'' in the adjacency matrix) need to be filled equivalently. However, this would result in $O(n)$ asymptotic indegree per node. As such, we opt to fill the intra-cluster edges \emph{recursively}, by constructing smaller copies of the FS graph, within each triangle. By doing so, we recover the graph from Figure \ref{fig:FSG} (Right), which is now both suitably sparse and self-similar!

As can be seen in Figure \ref{fig:ffg}, the FS graph generator indeed has a desirable tradeoff between solid normalised minimax fidelity and contained mixing time. Further, the fidelity can be meaningfully controlled by varying the chosen indegree of each expander graph; for example, out of all choices of $O(\log n)$ indegree, we find that $4\log n$ offers a favourable ``fidelity scaling law'' (see Figure \ref{fig:scaling}).

\subsection{Analysing the mixing time of the FS graph}

Having demonstrated that the FS graph generator has high fidelity and low mixing time in an empirical setting, we will now prove one of these properties---specifically, that its mixing time is favourable.

Informally, the idea of the proof is that the partitioning of the nodes into $O(\log n)$ blocks allows for quickly traversing long distances in the graph, via the bipartite edges. Making this construction recursive means that, once we reach the final block of size $O(n / \log n)$, the progression towards the sink will remain fast within that block, avoiding a linear mixing time as $n\rightarrow\infty$.

However, we also need to take care that the recursively introduced edges (which stay in the same block) do not overly impede the random walker's trajectory across blocks. Formally, we prove the following:

\begin{theorem}
Consider a FS graph generator with $\lceil\log n\rceil$ blocks per level for a graph of $n$ nodes. Further, assume that for every node, the proportion of its outgoing edges going across blocks is lower bounded by a constant $\alpha > 0$. Then the mixing time of graphs produced by this generator is $O(\polylog n)$. 
\end{theorem}

For reasons of brevity, we provide a full proof of this Theorem (along with supplementary remarks concerning how to construct the graph to match the assumptions of the Theorem) in Appendix \ref{app:rmk}.


\section{Results}


% \begin{figure*}
%     \includegraphics[width=0.33\linewidth]{plots_maxitem/graphs_const.png}
%     \includegraphics[width=0.33\linewidth]{plots_maxitem/graphs_log.png}
%     \includegraphics[width=0.33\linewidth]{plots_maxitem/graphs_sqrt.png}
%     \caption{Performance profiles of GATs trained with various computational graphs, on the max retrieval task up to size 256.}
%     \label{fig:maxtask}
% \end{figure*}

% \begin{figure*}
%     \includegraphics[width=0.33\linewidth]{plots_parity/parity-fixed_in_degree-transformer.pdf}
%     \includegraphics[width=0.33\linewidth]{plots_parity/parity-log_in_degree-transformer.pdf}
%     \includegraphics[width=0.33\linewidth]{plots_parity/parity-sqrt_in_degree-transformer.pdf}
%     \caption{Performance profiles of GATs trained with various computational graphs, on the parity task up to size 256, and tested on sizes 128, 256, 512, and 1024.}
%     \label{fig:parity}
% \end{figure*}

To supplement our findings with empirical performance metrics, we evaluate a graph attention network \citep{velickovic2018graph}-style model on three representative tasks:
\begin{itemize}
    \item finding the highest valued node in a sequence \citep{velickovic2024softmaxforsharpoutofdistribution};
    \item finding the second highest valued node in a sequence \citep{ong2022learnable};
    \item computing the parity value of a bitstring \citep{hahn2020theoretical}.
\end{itemize}
using adjacency matrices sampled from our graph generators. The tasks are chosen to represent opposite extremes of sharpness required: for finding the maximum, exactly one node needs to propagate its input features to the sink -- for parity, \emph{all} of them need to. The performances of different computational graphs can be seen in Figure \ref{fig:empirical_results}.

`(Second) Maximum retrieval' tasks consist in finding the category of the (second) maximum element in the set of items. Categories are provided one-hot encoded, together with the values. For each set the categories are consistent, but are randomised between different sets. Ten categories are used, so chance accuracy is $\sim10\%$.

The `Parity' task consists of computing the parity of the number of $1$s in a bitstring. Since there are only two possible parities, odd and even, random guessing should lead to around $50\%$ accuracy on the task.

For details on the models' architectures and optimisation, please see Appendix \ref{app:models_training}. As one can see in Figure \ref{fig:empirical_results}, the FS graph leads in general to the best performance in distribution and best out of distribution generalisation when compared to other sparse graphs---matching the intuition found by our metrics. More strongly, in the Parity task the FS graph manages to effectively match the fully connected graph, while having a substantially lower number of edges, and while its performance tends to be similar to the Poisson($0.8$) graph inthe Maximum task, in the Second Maximum and Parity tasks there's a significant gap in performance between the FunSearch graph and all the other sparse graphs.

Additionally, we may note that the results are aligned with the recommendations given in Figure \ref{fig:scaling}, which concerns the problem sizes up to $1,024$ studied here. Namely, leveraging an in-degree of $4\log n$ for the random expanders used to sparsify the FS graph has a more graceful performance profile with increasing problem size across all three tasks compared to using an in-degree of $\log n$.

\section{Conclusions}

In this work, we have embarked on a detailed study of feedforward computational graphs, attempting to chart a novel path that could enable for a principled discovery of useful feedforward blueprints. While it is apparent that analysing these graphs is substantially less straightforward than doing so with their undirected counterparts, we believe the outcomes to have been fruitful. Namely, we proposed two well-justified metrics of feedforward information propagation, used them to automatically discover an interesting graph generator, and demonstrated its strong performance on several carefully crafted benchmarks. We are hopeful that our work will inspire targeted follow-up studies in this space, which may usher in a new paradigm of graph rewiring.

% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}

We would like to extend deep thanks to Steven Kapturowski for detailed assistance with the parity experiment, as well as Alexander Novikov for kindly supporting us as we were setting up our FunSearch experiments. We are also grateful to Razvan Pascanu and Simon Osindero for diligently reviewing this paper and providing valuable comments.

%\textbf{Do not} include acknowledgements in the initial version of
%the paper submitted for blind review.

%If a paper is accepted, the final camera-ready version can (and
%usually should) include acknowledgements.  Such acknowledgements
%should be placed at the end of the section, in an unnumbered section
%that does not count towards the paper page limit. Typically, this will 
%include thanks to reviewers who gave useful comments, to colleagues 
%who contributed to the ideas, and to funding agencies and corporate 
%sponsors that provided financial support.

\section*{Impact Statement}

This paper presents a novel framework for improving the information propagation quality in feedforward models. Such models are highly prevalent, and innovations in this space could eventually lead to improvements in model sparsity (allowing for serving more powerful models on edge devices, for example) and model out-of-distribution generalisation (alowing more capable reasoning and scientific advances). Any societal consequences of our work could hence be equated to the societal consequences of accelerating these flavours of research.

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Proof that minimax fidelity in a line graph decays to zero with increasing size}
\label{appendix:minmax_line}
First let's determine an expression for $\max_a \frac{\binom{a}{n - 1}}{2^a}$ for a fixed $n$.

Notice that this is equivalent to finding an expression for $\max_a \frac{\binom{a}{k}}{2^a}$ and then replacing $k = n - 1$.

With a fixed n we know that $a \geq k$, otherwise the binomial term would simply be $0$. Now let $f(a) = \frac{\binom{a}{k}}{2^a}$, and let's compute $\frac{f(a+1)}{f(a)}$:

\begin{equation*}
    \begin{aligned}
        \frac{f(a+1)}{f(a)} & = \frac{2^a}{2^{a+1}} \frac{\binom{a+1}{k}}{\binom{a}{k}} \\
        \frac{f(a+1)}{f(a)} & = \frac{1}{2}\frac{(a+1)!}{k!(a-k+1)!}\frac{k!(a-k)!}{a!} \\
        & = \frac{1}{2} \frac{(a+1)}{a-k+1}
    \end{aligned}
\end{equation*}
From that we know that $\frac{f(a+1)}{f(a)} \geq 1 \iff (a+1) \geq 2(a-k+1) \iff 2k-1 \geq a$. As such $f(a)$ increases for a in $[k, 2k-1]$ and decreases for $a > 2k$. We can thus say that $$\max_a \frac{\binom{a}{k}}{2^a} = \frac{\binom{2k}{k}}{2^{2k}}$$

Now let's compute our limit using Stirling's Approximation:
\begin{equation*}
    \begin{aligned}
&\lim_{k \to \infty} \frac{\binom{2k}{k}}{2^{2k}}  = \lim_{k \to \infty} \frac{2k!}{k!^2} \cdot \frac{1}{2^{2k}} = \\
 &\lim_{k \to \infty} \frac{(2k)^{2k}}{e^{2k}}\sqrt{2\pi(2k)} \frac{e^{2k}}{k^{2k} \cdot (2\pi k)} \cdot \frac{1}{2^{2k}} = \\
 & \lim_{k \to \infty} 2^{2k} \sqrt{\frac{1}{\pi k}} 2^{-2k} = 0
\end{aligned}
\end{equation*}

\section{Proof of Theorem 6.1.}\label{app:rmk}

\begin{proof}
In order to help us reason about the mixing time, it will be very useful to compute it as a function of the starting level and block the random walker is in. That is, we will track $T(d, k)$ as the expected number of steps needed for a random walker to reach the sink node, assuming the walker is currently $d$ depth levels away from the deepest recursive level, and $k$ blocks away from reaching the final block in the current depth level.

We already know that $k$'s maximal value will be $\lceil \log n \rceil$ for each depth level, due to the generator's parameters. For the maximal value of $d$, i.e. the total number of depth levels, we note that after each level, the number of nodes being considered is further subdivided by $\lceil\log n\rceil$. This means that, after $l$ levels, the block size is $\frac{n}{\lceil\log n\rceil^l}$. No further subdivisions are possible once the block size reaches $1$.

Since we are mainly interested in the asymptotic number of depth levels, we make a simplifying assumption that both $\log n$ is an integer and that $n$ is divisible by each power of $\log n$. In this situation, the total number of levels $D$ is such that $n = \log^D n$. This implies $D=\log_{\log n} n = \frac{\log n}{\log \log n}$, i.e., the number of depth levels is $O\left(\frac{\log n}{\log \log n}\right)$.

With this in mind, we aim to quantify $T\left(\frac{\log n}{\log \log n}, \log n\right)$, once again under the simplifying assumption that all of these variables are integers. To do this, we will establish several upper bounds on $T(d, k)$, assuming pessimistic behaviour from the random walker.

Firstly, when the walker is in the final level ($d=0$), the number of nodes considered is $1$, hence there are are no blocks left to traverse, and therefore $T(0, k) = 0$ for all $k$.

Then, once the walker hits the final block in its current depth level ($k=0$), it automatically transitions into the next level, which is further subdivided into $\log n$ blocks. In the worst-case scenario, the walker will have landed in the very first block of the next depth level, and will need to traverse them all. Hence, $T(d, 0) \leq T(d - 1, \log n)$ for all $d > 0$.

Finally, in all other cases, we leverage the assumption on the outgoing edge ratio to remark that the walker will transition into the next block at its current depth level with probability at least $\alpha$. We make a pessimistic assumption that, if the walker does not transition into the next block, it certainly stays exactly put within its current block. This leads to the upper bound of $T(d, k)\leq 1 + \alpha T(d, k - 1) + (1 - \alpha) T(d, k)$.

We can now solve for all of these upper bounds as if they are equalities, yielding the following recurrence relations:
\begin{align}
    T(d, k) &= 1 + \alpha T(d, k - 1) + (1 - \alpha) T(d, k)\\
    T(d, 0) &= T(d - 1, \log n)\\
    T(0, k) &= 0
\end{align}
The key observation is, since $\alpha$ is assumed identical in every depth level, and $k$ is always reset to $\log n$ after each depth level, we can equivalently represent the total walk time as a direct sum of walk times on each depth level. 

Then, since each depth level is represented as a chain of $\log n$ states, $T(d, k)$ may be observed as the expected time of traversing a line graph of $k + (d - 1)\log n$ steps and transition probability $\alpha$. The total number of such steps is $D\log n$, for $D$ being the number of depth levels.

To compute the mixing time of such a Markov chain, we assume that the walker starts in the first node of the first block, and we want to know the smallest value of $t$ such that
$$\sum_{j\leq D \log n} \left ( {t \atop j} \right ) \alpha^j (1-\alpha)^{t-j} < 1/4.$$
For ease of notation, set $N = D \log n$.
The left hand side is the cumulative distribution function for the binomial random variable $X(t,\alpha)$. So we must find the smallest value of $t$ such that $\mathbb{P}(X(t,\alpha) \leq N) < 1/4$. Now, Hoeffding's inequality gives that
$$\mathbb{P}(X(t,\alpha) \leq N) < \exp \left (-2t \left (\alpha - \frac{N}{t} \right)^2 \right ).$$
Hence, when $t = 2N /\alpha $, 
$$\mathbb{P}(X(t,\alpha) \leq N) < \exp \left (- N \alpha \right ).$$
When $\alpha$ is fixed and $N$ is large, this is certainly less than $1/4$. This establishes that the mixing time is at most $O(N) = O(D \log n)$. Note that when $t = D \log n$, then 
$$\sum_{j\leq D \log n} \left ( {t \atop j} \right ) \alpha^j (1-\alpha)^{t-j} =1.$$
So the mixing time is certainly at least $D\log n$.

Therefore, plugging in the appropriate values of $D$, we can conclude that the final mixing time will be $O\left(\frac{\log^2 n}{\log\log n}\right)$, which is $O(\polylog n)$, completing the proof.
\end{proof}

\begin{remark}
An important assumption for our mixing time derivation is that the outdegree ratio of each node's cross-block outgoing edges can be lower bounded by a constant (which we can use as the  value of $\alpha$ for our analysis).

Since there will be $\frac{\log n}{\log\log n}$ depth levels in total, we cannot maintain a fixed outdegree of $\delta_{i\rightarrow}$ at each depth level, as the ratio is then $\frac{\delta_{i\rightarrow}}{\delta_{i\rightarrow}\frac{\log n}{\log\log n}} = \frac{\log\log n}{\log n}$, which will decay to zero as $n\rightarrow\infty$ and cannot be lower-bounded by a constant. This implies that the relative out-degree of each node in each block must decay with increasing depth level.

One simple way to support such a decay is to assume a \emph{geometric} decay with ratio $r < 1$; that is, if the amount of cross-block edges from a given node, $i$, at a given depth level, $d$, is $\delta_{i\rightarrow}^{d}$, the amount of cross-block edges from that node at the next depth level would be $\delta_{i\rightarrow}^{d-1} = \delta_{i\rightarrow}^{d} r$.

This process upper-bounds the total number of outgoing edges of every node in the final graph by a geometric series $\delta^d_{i\rightarrow} + \delta^d_{i\rightarrow} r + \delta^d_{i\rightarrow} r^2 + \dots = \frac{\delta^d_{i\rightarrow}}{1 - r}$. As such, the proportion of the edges which are cross-block at the current level would correspond to $\frac{\delta^d_{i\rightarrow}}{\frac{\delta^d_{i\rightarrow}}{1-r}} = 1 - r$. When moving to the next depth level, the number of outgoing edges will be $\delta^d_{i\rightarrow} r$, and since the previous level has already been fully crossed, there will be no edges from the previous level, and the total outdegree is $\frac{\delta^d_{i\rightarrow} r}{1 - r}$, also leading to a $1-r$ ratio. This trend continues with increasing depth level, and we can hence use $1-r$ as our pessimistic estimate of $\alpha$.

\end{remark}

\section{Adjacency matrix gallery}\label{app:gallery}
In Figures \ref{fig:attn_matr}--\ref{fig:attn_matr_3} we provide visualisations of several adjacency matrix samples for a variety of graph generators we studied in this paper.
\begin{figure*}[!htbp]
    \includegraphics[width=0.33\linewidth]{graphs/cayley.png}
    \includegraphics[width=0.33\linewidth]{graphs/fixed_in_degree_const4.png}
    \includegraphics[width=0.33\linewidth]{graphs/n_const_poisson_graph08.png}
    \caption{Generated samples of adjacency matrices for the graph generators of Oriented Expander ({\bf left}), Erd\H{o}s-R\'{e}nyi with constant indegree ({\bf middle}) and Poisson($0.2$) with constant indegree ({\bf right}).}
    \label{fig:attn_matr}
\end{figure*}
\begin{figure*}
    \includegraphics[width=0.33\linewidth]{graphs/line_graph.png}
    \includegraphics[width=0.33\linewidth]{graphs/n_sqrt_n_poisson_graph08.png}
    \includegraphics[width=0.33\linewidth]{graphs/random_graph_fixed_in_degree_sqrt.png}
     \caption{Generated samples of adjacency matrices for the graph generators of Line Graphs ({\bf left}), Poisson($0.2$) with $\sqrt{n}$ indegree ({\bf middle}) and Erd\H{o}s-R\'{e}nyi with $\sqrt{n}$ indegree ({\bf right}).}
    \label{fig:attn_matr_2}
\end{figure*}
\begin{figure*}
    \includegraphics[width=0.33\linewidth]{graphs/n_log_n_poisson_graph08.png}
    \includegraphics[width=0.33\linewidth]{graphs/random_graph_fixed_in_degree_log.png}
    \includegraphics[width=0.33\linewidth]{graphs/full.png}
    \caption{Generated samples of adjacency matrices for the graph generators of Poisson($0.2$) with $\log_2 n$ indegree ({\bf left}), Erd\H{o}s-R\'{e}nyi with $\log_2 n$ indegree ({\bf middle}) and the Fully Connected graph ({\bf right}).}
    \label{fig:attn_matr_3}
\end{figure*}

\clearpage

\section{Models and Training} 
\label{app:models_training}
We test graphs on two different tasks: bit set parity, which is well known to be hard for transformer architectures without Chain Of Thought \citep{kim2024transformersprovablysolveparity} and finding the category of the maximum or second maximum element in the set. We are interested in length generalisation properties of the tasks. So while we train on length up to $256$ elements, we test sequences of up to $1,024$ elements.

\subsection{Maximum retrieval}
For 'maximum' tasks we utilise simple transformer architecture. It consists of one attention block, which is a single attention layer  followed by two feedforward layers with GELU  activations and pre-norm in between. This attention block is repeated (with shared parameters) for $\log n$ steps for an input of $n$ nodes. We use the cross-entropy loss function and the AdamW optimiser with $10^{-3}$ learning rate and $256$ batch size. Training is performed over $10,000$ steps.  We utilise casual attention mask in conjunction with mask provided by graphs.
\subsection{Parity}
We use a model similar to Gemma 2 \citep{gemmateam2024gemma2improvingopen}, but with only only 8 layers and using standard multi-head attention with 8 heads, the vocabulary size was 2, as there are only 2 symbols in a bitstring, and the embedding dimension was $256$. The loss is a crossentropy and is computed over all positions in the sequence, so the model must predict the parity for all prefixes of the input bitstring. 
To train the model we used the LaProp optimizer \citep{ziyin2021lapropseparatingmomentumadaptivity} with weight decay and RMSClip \cite{shazeer2018adafactoradaptivelearningrates} for 1 million steps, with batch size of $128$ sequences. Our hyperparameters were:
\begin{table}[h]
\centering
\caption{LaProp Hyperparameters}
\begin{tabular}{cc}
\toprule
Hyperparameter & Value \\ \midrule
Learning Rate & $1\times10^{-3}$ \\
$\beta_1$ & 0.9 \\
$\beta_2$ & 0.9 \\
Weight Decay & $5\times10^{-4}$ \\
RMSClip's d & 1 \\
\bottomrule
\end{tabular}
\label{table:laprop_hparams}
\end{table}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
