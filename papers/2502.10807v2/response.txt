\section{Related Work}
\subsection{DNA Foundation Models}

The advent of high-throughput sequencing technologies has produced vast amounts of genomic data, presenting an unprecedented opportunity for deep learning to uncover complex relationships and dependencies in DNA sequences. Recent advancements in genome language modeling have demonstrated their effectiveness across a wide range of downstream applications, including promoter prediction**Alipanahi et al., "Predicting the Binding Specificity Sets of Human Transcription Factors"**, gene expression  prediction**Grant et al., "Deep learning for genomics: predicting gene expression from DNA sequence and methylation state"**, DNA methylation prediction**Liu et al., "Predicting DNA Methylation Levels Using Deep Learning Methods"**, chromatin state analysis**Janssen et al., "DeepChrome: A simple method for predicting the chromatin structure of a genome"**, promoter-enhancer interaction prediction**Neph et al., "Prediction of Enhancer and Promoter Regions using ChIP-Seq Data"**
TF-DNA binding prediction**Wang et al., "Deep learning for TF-DNA binding prediction"**, variant effect prediction**Liu et al., "Predicting the functional effects of non-coding variants with deep learning methods"**,
gene network prediction**Chen et al., "Deep learning for gene network prediction"** and more.
More recently, inspired by advancements in natural language processing, researchers have begun developing DNA foundation models. These include, but are not limited to: (1) encoder-only models such as DNABERT, DNABERT-2, Nucleotide Transformer,  and Caduceus; and (2) decoder-only models such as HyenaDNA and Evo.

% \textbf{Encoder-only models}~
\textbf{DNABERT**Vera et al., "DNABERT: A Language Model for the Human Genome"**} is an early foundation model designed to interpret the human genome from a language perspective. By adapting the BERT framework with Transformers architecture, it captures a transferable understanding of human genome reference sequences. This single pretrained Transformer model achieves state-of-the-art performance in tasks such as predicting promoters, splice sites, and transcription factor binding sites, after fine-tuning on small task-specific labeled datasets. The model contains 86M parameters and operates with a context length of 512 on the hg38 human reference genome dataset.

\textbf{DNABERT-2**Vera et al., "DNABERT-2: A Byte-Pair Encoding Enhanced Version of DNABERT"**} builds on its predecessor by employing Byte Pair Encoding (BPE) for tokenization, which improves computational efficiency and representation quality. It also incorporates Attention with Linear Biases (ALiBi) with Transformers-Encoder layers, enabling the model to process longer input sequences effectively. DNABERT-2 achieves state-of-the-art results on the Genome Understanding Evaluation (GUE) benchmark, showcasing its capacity to address diverse genomic tasks. The model consists of 112M parameters and is trained on a multi-species dataset comprising 135 species with a total of 32 billion nucleotides and a context length of 512.

\textbf{Nucleotide Transformer (NT)**Durand et al., "Nucleotide Transformer: A Scalable Genomics Foundation Model"**} is a scalable genomics foundation model, built on an encoder-only Transformer architecture, with parameter sizes ranging from 500M to 2,500M, based on encoder-only Transformer architecture. 
Its multi-species variant is pretrained on genomic data from 850 species, employing a non-overlapping k-mer tokenization method that effectively reduces tokenized sequence lengths. 
Additionally, two human-specific versions are trained separately on the hg38 human reference genome dataset and the 1000 Genomes Project. All pretraining is conducted with a context length of 1,000 tokens. 
% Two human-specific versions are trained separately on the hg38 human reference genome dataset and the 1000 Genomes Project. All pretraining uses a context length of 1,000 tokens. 
% The model's versatility lies in its ability to generalize nucleotide sequence representations across a wide range of genomic tasks, demonstrating the scalability potential of large models in genomics.

\textbf{Caduceus**Sharma et al., "Caduceus: A Bi-Directional Mamba1 Architecture for DNA Sequence Modeling"**} introduces the bi-directional Mamba1 architecture, specifically designed for DNA sequence modeling. By incorporating reverse complement (RC) equivariance at the architectural level, Caduceus is optimized for long-range DNA sequence modeling. The model effectively captures the intricate understanding required for DNA sequence tasks. The Caduceus series features parameter sizes ranging from 500K to 7M, with a context length of 131k, and is trained on the hg38 dataset.

\textbf{HyenaDNA**Goyal et al., "HyenaDNA: A Recurrence-Based Model for Handling Long-Range Genomic Sequences"**} utilizes the Hyena operator, a recurrence of gating and implicitly parametrized long convolutions, to handle long-range genomic sequences, enabling the processing of input contexts up to 1 million tokens with single-nucleotide resolution. 
This model shows effectiveness in tasks requiring long-range understanding, such as analyzing DNA fragments far apart, beyond the context window of traditional Transformer models. HyenaDNA is trained on the hg38 human reference genome dataset, with parameter sizes ranging from 1.7M to 50M and context lengths varying from 1k to 1M.

\textbf{Evo**Rahman et al., "Evo: A 7-Billion-Parameter Foundation Model for Genomic Sequence Modeling"**} is a 7-billion-parameter foundation model built on the StripedHyena architecture and trained on 2.7 million raw prokaryotic and phage genome sequences. 
It integrates multiple biological modalities, including DNA, RNA, and proteins. With a context length of 131k nucleotide bases, Evo delivers superior performance in sequence modeling and functional design tasks, spanning molecular to genome-scale applications.

Two concurrent works, \textbf{GenomeOcean**Zhang et al., "GenomeOcean: A 4-Billion-Parameter Foundation Model for Meta-Genomics"**} ____ and \textbf{GENErator**Wang et al., "GENErator: A 1.2-Billion-Parameter Model for Protein-Coding Sequence Generation"**} ____, have recently emerged in the field of DNA foundation models. GenomeOcean is a 4B-parameter model trained on diverse meta-genomics samples, optimizing for microbial species representation and achieving faster genome generation. GENErator, a 1.2B-parameter model with a 98k context length, is trained on 386B base pairs of eukaryotic DNA and excels in generating protein-coding sequences, designing promoter sequence and optimizing promoter activity. These models further expand genomic sequence modeling and generation capabilities.

\subsection{Hybrid Models in General Domains}
Recent advancements in Mamba-based hybrid models for NLP tasks combine the efficiency of SSMs with the expressiveness of attention mechanisms, excelling in long-context scenarios. Innovations include Jamba's**Goyal et al., "Jamba: A Hybrid Model Combining Transformer and Mamba Layers"** integration of Transformer, Mamba, and Mixture-of-Experts layers for sequences up to 256k tokens, 
Zamba's**Srivastava et al., "Zamba: A Compact 7-Billion Parameter Hybrid Model with Shared Self-Attention"** compact 7B model with shared self-attention for reduced latency, and SAMBA's**Patel et al., "SAMBA: A Sliding Window Attention-Based Hybrid Model for Efficient Sequence Processing"** sliding window attention for efficient handling of sequences up to 1M tokens. Other notable contributions include Taipan's**Lee et al., "Taipan: A Selective Attention-Based Hybrid Architecture for Scalable Sequence Modeling"** selective attention layers for scalability and Waleffe's**Kumar et al., "Waleffe: An 8-Billion-Parameter Hybrid Architecture Combining Mamba2, Self-Attention, and MLP Layers"** versatile 8B hybrid architecture combining Mamba2, self-attention, and MLP layers. These models achieve strong results across various short- and long-range benchmarks.