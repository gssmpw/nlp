% GFM

@article{ji2021dnabert,
  title={DNABERT: a comprehensive predictor for DNA sequences based on deep transfer learning},
  author={Ji, Yu and Zhou, Zhiqiang and Liu, Han and Davuluri, Ramana V},
  journal={Bioinformatics},
  volume={37},
  number={24},
  pages={4776--4783},
  year={2021},
  publisher={Oxford University Press}
}


@article{zhou2023dnabert2,
  title={DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome},
  author={Zhihan Zhou and Yanrong Ji and Weijian Li and Pratik Dutta and Ramana Davuluri and Han Liu},
  journal={arXiv preprint arXiv:2306.15006},
  year={2023}
}

@article{dalla2023nucleotide,
  title={The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics},
  author={Dalla-Torre, Lorenzo and Benegas, Nicolás and Grechishnikova, Daria and others},
  journal={Nature Methods},
  year={2023}
}

@article{schiff2023caduceus,
  title={Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling},
  author={Schiff, Yair and Kao, Chia-Hsiang and Gokaslan, Aaron and others},
  journal={arXiv preprint arXiv:2306.15006},
  year={2023}
}

@article{poli2023hyenadna,
  title={HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution},
  author={Poli, Michael and Dao, Tri and others},
  journal={arXiv preprint arXiv:2306.15006},
  year={2023}
}
%decoder-only model

@article{meier2023evo,
  title={Sequence modeling and design from molecular to genome scale with Evo},
  author={Meier, Joshua and others},
  journal={Science},
  volume={382},
  number={6667},
  pages={eado9336},
  year={2023},
  publisher={American Association for the Advancement of Science}
}

@misc{wu2025generator,
    title={GENERator: A Long-Context Generative Genomic Foundation Model},
    author={Wei Wu and Qiuyi Li and Mingyang Li and Kun Fu and Fuli Feng and Jieping Ye and Hui Xiong and Zheng Wang},
    year={2025},
    eprint={2502.07272},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@article {Zhou2025genomeocean,
	author = {Zhou, Zhihan and Riley, Robert and Kautsar, Satria and Wu, Weimin and Egan, Rob and Hofmeyr, Steven and Goldhaber-Gordon, Shira and Yu, Mutian and Ho, Harrison and Liu, Fengchen and Chen, Feng and Morgan-Kiss, Rachael and Shi, Lizhen and Liu, Han and Wang, Zhong},
	title = {GenomeOcean: An Efficient Genome Foundation Model Trained on Large-Scale Metagenomic Assemblies},
	year = {2025},
	doi = {10.1101/2025.01.30.635558},
	URL = {https://www.biorxiv.org/content/early/2025/02/05/2025.01.30.635558},
	journal = {bioRxiv}
}


% benchmark

@article{lal2024reglm,
  title={regLM: Designing Realistic Regulatory DNA with Autoregressive Language Models},
  author={Lal, Avantika and Garfield, David and Biancalani, Tommaso and Eraslan, Gokcen},
  journal={bioRxiv preprint},
  year={2024}
}

@article{poli2023genomics,
  title={Genomics Long-Range Benchmark (LRB): Evaluating Long-Context Models on Genomic Data},
  author={Poli, Michael and others},
  journal={arXiv preprint arXiv:2306.00971},
  year={2023}
}

@article{marin2024bend,
  title={BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks},
  author={Marin, Frederikke I. and Teufel, Felix and others},
  journal={arXiv preprint arXiv:2306.15006},
  year={2024}
}


% architect


@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423/",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}

@article{radford2018improving,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal={OpenAI Blog},
  year={2018},
  url={https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}
}




@misc{gu2023mamba,
  title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author={Gu, Albert and Dao, Tri},
  year={2023},
  eprint={2312.00752},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@inproceedings{gu21lssl,
  author       = {Albert Gu and
                  Isys Johnson and
                  Karan Goel and
                  Khaled Saab and
                  Tri Dao and
                  Atri Rudra and
                  Christopher R{\'{e}}},
  editor       = {Marc'Aurelio Ranzato and
                  Alina Beygelzimer and
                  Yann N. Dauphin and
                  Percy Liang and
                  Jennifer Wortman Vaughan},
  title        = {Combining Recurrent, Convolutional, and Continuous-time Models with
                  Linear State Space Layers},
  booktitle    = {Advances in Neural Information Processing Systems 34: Annual Conference
                  on Neural Information Processing Systems 2021, NeurIPS 2021, December
                  6-14, 2021, virtual},
  pages        = {572--585},
  year         = {2021}
}

@misc{fu2023hungryhungryhipposlanguage,
      title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models}, 
      author={Daniel Y. Fu and Tri Dao and Khaled K. Saab and Armin W. Thomas and Atri Rudra and Christopher Ré},
      year={2023},
      eprint={2212.14052},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.14052}, 
}

% echo embedding

@misc{springer2024repetitionimproveslanguagemodel,
      title={Repetition Improves Language Model Embeddings}, 
      author={Jacob Mitchell Springer and Suhas Kotha and Daniel Fried and Graham Neubig and Aditi Raghunathan},
      year={2024},
      eprint={2402.15449},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.15449}, 
}



% Hybrid Mamba Models
@article{waleffe2024empirical,
  title={An Empirical Study of Mamba-based Language Models},
  author={Waleffe, Roger and Byeon, Wonmin and Riach, Duncan and Norick, Brandon and Korthikanti, Vijay and Dao, Tri and Gu, Albert and Hatamizadeh, Ali and Singh, Sudhakar and Narayanan, Deepak and others},
  journal={arXiv preprint arXiv:2406.07887},
  year={2024}
}

@misc{lieber2024jambahybridtransformermambalanguage,
      title={Jamba: A Hybrid Transformer-Mamba Language Model}, 
      author={Opher Lieber and Barak Lenz and Hofit Bata and Gal Cohen and Jhonathan Osin and Itay Dalmedigos and Erez Safahi and Shaked Meirom and Yonatan Belinkov and Shai Shalev-Shwartz and Omri Abend and Raz Alon and Tomer Asida and Amir Bergman and Roman Glozman and Michael Gokhman and Avashalom Manevich and Nir Ratner and Noam Rozen and Erez Shwartz and Mor Zusman and Yoav Shoham},
      year={2024},
      eprint={2403.19887},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.19887}, 
}

@misc{ren2024sambasimplehybridstate,
      title={Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling}, 
      author={Liliang Ren and Yang Liu and Yadong Lu and Yelong Shen and Chen Liang and Weizhu Chen},
      year={2024},
      eprint={2406.07522},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.07522}, 
}

@misc{glorioso2024zambacompact7bssm,
      title={Zamba: A Compact 7B SSM Hybrid Model}, 
      author={Paolo Glorioso and Quentin Anthony and Yury Tokpanov and James Whittington and Jonathan Pilault and Adam Ibrahim and Beren Millidge},
      year={2024},
      eprint={2405.16712},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.16712}, 
}


@misc{vannguyen2024taipanefficientexpressivestate,
      title={Taipan: Efficient and Expressive State Space Language Models with Selective Attention}, 
      author={Chien Van Nguyen and Huy Huu Nguyen and Thang M. Pham and Ruiyi Zhang and Hanieh Deilamsalehy and Puneet Mathur and Ryan A. Rossi and Trung Bui and Viet Dac Lai and Franck Dernoncourt and Thien Huu Nguyen},
      year={2024},
      eprint={2410.18572},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.18572}, 
}


@article{dao2024transformers,
  title={Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality},
  author={Dao, Tri and Gu, Albert},
  journal={arXiv preprint arXiv:2405.21060},
  year={2024}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@misc{brooks2024video,
  title={Video generation models as world simulators},
  author={Brooks, Tim and Peebles, Bill and Holmes, Connor and DePue, Will and Guo, Yufei and Jing, Li and Schnurr, David and Taylor, Joe and Luhman, Troy and Luhman, Eric and others},
  journal={2024-03-03]. https://openai. com/research/video-generation-modelsas-world-simulators},
  year={2024}
}


@article{le2022bert,
  title={BERT-Promoter: An improved sequence-based predictor of DNA promoter using BERT pre-trained model and SHAP feature selection},
  author={Le, Nguyen Quoc Khanh and Ho, Quang-Thai and Nguyen, Van-Nui and Chang, Jung-Su},
  journal={Computational Biology and Chemistry},
  volume={99},
  pages={107732},
  year={2022},
  publisher={Elsevier}
}
@article{zhang2022ipro,
  title={iPro-WAEL: a comprehensive and robust framework for identifying promoters in multiple species},
  author={Zhang, Pengyu and Zhang, Hongming and Wu, Hao},
  journal={Nucleic Acids Research},
  volume={50},
  number={18},
  pages={10278--10289},
  year={2022},
  publisher={Oxford University Press}
}
@article{avsec2021effective,
  title={Effective gene expression prediction from sequence by integrating long-range interactions},
  author={Avsec, {\v{Z}}iga and Agarwal, Vikram and Visentin, Daniel and Ledsam, Joseph R and Grabska-Barwinska, Agnieszka and Taylor, Kyle R and Assael, Yannis and Jumper, John and Kohli, Pushmeet and Kelley, David R},
  journal={Nature methods},
  volume={18},
  number={10},
  pages={1196--1203},
  year={2021},
  publisher={Nature Publishing Group US New York}
}
@article{jin2022idna,
  title={iDNA-ABF: multi-scale deep biological language learning model for the interpretable prediction of DNA methylations},
  author={Jin, Junru and Yu, Yingying and Wang, Ruheng and Zeng, Xin and Pang, Chao and Jiang, Yi and Li, Zhongshen and Dai, Yutong and Su, Ran and Zou, Quan and others},
  journal={Genome biology},
  volume={23},
  number={1},
  pages={219},
  year={2022},
  publisher={Springer}
}
@article{lee2022learning,
  title={Learning the histone codes with large genomic windows and three-dimensional chromatin interactions using transformer},
  author={Lee, Dohoon and Yang, Jeewon and Kim, Sun},
  journal={Nature Communications},
  volume={13},
  number={1},
  pages={6678},
  year={2022},
  publisher={Nature Publishing Group UK London}
}
@article{chen2022capturing,
  title={Capturing large genomic contexts for accurately predicting enhancer-promoter interactions},
  author={Chen, Ken and Zhao, Huiying and Yang, Yuedong},
  journal={Briefings in Bioinformatics},
  volume={23},
  number={2},
  pages={bbab577},
  year={2022},
  publisher={Oxford University Press}
}
@article{ni2022epi,
  title={Epi-mind: identifying enhancer--promoter interactions based on transformer mechanism},
  author={Ni, Yu and Fan, Linqi and Wang, Miao and Zhang, Ning and Zuo, Yongchun and Liao, Mingzhi},
  journal={Interdisciplinary Sciences: Computational Life Sciences},
  volume={14},
  number={3},
  pages={786--794},
  year={2022},
  publisher={Springer}
}
@article{wang2022towards,
  title={Towards a better understanding of TF-DNA binding prediction from genomic features},
  author={Wang, Zixuan and Gong, Meiqin and Liu, Yuhang and Xiong, Shuwen and Wang, Maocheng and Zhou, Jiliu and Zhang, Yongqing},
  journal={Computers in Biology and Medicine},
  volume={149},
  pages={105993},
  year={2022},
  publisher={Elsevier}
}
@article{rozowsky2023tex,
  title={The EN-TEx resource of multi-tissue personal epigenomes \& variant-impact models},
  author={Rozowsky, Joel and Gao, Jiahao and Borsari, Beatrice and Yang, Yucheng T and Galeev, Timur and G{\"u}rsoy, Gamze and Epstein, Charles B and Xiong, Kun and Xu, Jinrui and Li, Tianxiao and others},
  journal={Cell},
  volume={186},
  number={7},
  pages={1493--1511},
  year={2023},
  publisher={Elsevier}
}
@article{theodoris2023transfer,
  title={Transfer learning enables predictions in network biology},
  author={Theodoris, Christina V and Xiao, Ling and Chopra, Anant and Chaffin, Mark D and Al Sayed, Zeina R and Hill, Matthew C and Mantineo, Helene and Brydon, Elizabeth M and Zeng, Zexian and Liu, X Shirley and others},
  journal={Nature},
  volume={618},
  number={7965},
  pages={616--624},
  year={2023},
  publisher={Nature Publishing Group UK London}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}
@article{fournier2023practical,
  title={A practical survey on faster and lighter transformers},
  author={Fournier, Quentin and Caron, Ga{\'e}tan Marceau and Aloise, Daniel},
  journal={ACM Computing Surveys},
  volume={55},
  number={14s},
  pages={1--40},
  year={2023},
  publisher={ACM New York, NY}
}
@article{zhuang2023survey,
  title={A survey on efficient training of transformers},
  author={Zhuang, Bohan and Liu, Jing and Pan, Zizheng and He, Haoyu and Weng, Yuetian and Shen, Chunhua},
  journal={arXiv preprint arXiv:2302.01107},
  year={2023}
}
@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International conference on machine learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}
@article{child1904generating,
  title={Generating long sequences with sparse transformers. arXiv 2019},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}
@misc{gu2022efficiently,
      title={Efficiently Modeling Long Sequences with Structured State Spaces}, 
      author={Albert Gu and Karan Goel and Christopher Ré},
      year={2022},
      eprint={2111.00396},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2111.00396}, 
}
@article{gu2021combining,
  title={Combining recurrent, convolutional, and continuous-time models with linear state space layers},
  author={Gu, Albert and Johnson, Isys and Goel, Karan and Saab, Khaled and Dao, Tri and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={572--585},
  year={2021}
}
@article{gu2022parameterization,
  title={On the parameterization and initialization of diagonal state space models},
  author={Gu, Albert and Goel, Karan and Gupta, Ankit and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={35971--35983},
  year={2022}
}
@article{gupta2022diagonal,
  title={Diagonal state spaces are as effective as structured state spaces},
  author={Gupta, Ankit and Gu, Albert and Berant, Jonathan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22982--22994},
  year={2022}
}
@misc{smith2023simplified,
      title={Simplified State Space Layers for Sequence Modeling}, 
      author={Jimmy T. H. Smith and Andrew Warrington and Scott W. Linderman},
      year={2023},
      eprint={2208.04933},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2208.04933}, 
}
@article{fu2022hungry,
  title={Hungry hungry hippos: Towards language modeling with state space models},
  author={Fu, Daniel Y and Dao, Tri and Saab, Khaled K and Thomas, Armin W and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2212.14052},
  year={2022}
}
@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{adam,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1412.6980},
  timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{schulman2022chatgpt,
  title={ChatGPT: Optimizing language models for dialogue},
  author={Schulman, John and Zoph, Barret and Kim, Christina and Hilton, Jacob and Menick, Jacob and Weng, Jiayi and Uribe, Juan Felipe Ceron and Fedus, Liam and Metz, Luke and Pokorny, Michael and others},
  journal={OpenAI blog},
  volume={2},
  number={4},
  year={2022}
}
@article{dao2023flashattention,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}
@article{lin2023evolutionary,
  title={Evolutionary-scale prediction of atomic-level protein structure with a language model},
  author={Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Smetanin, Nikita and Verkuil, Robert and Kabeli, Ori and Shmueli, Yaniv and others},
  journal={Science},
  volume={379},
  number={6637},
  pages={1123--1130},
  year={2023},
  publisher={American Association for the Advancement of Science}
}
@article{brandes2022proteinbert,
  title={ProteinBERT: a universal deep-learning model of protein sequence and function},
  author={Brandes, Nadav and Ofer, Dan and Peleg, Yam and Rappoport, Nadav and Linial, Michal},
  journal={Bioinformatics},
  volume={38},
  number={8},
  pages={2102--2110},
  year={2022},
  publisher={Oxford University Press}
}
@article{he2024sfm,
  title={SFM-Protein: Integrative Co-evolutionary Pre-training for Advanced Protein Sequence Representation},
  author={He, Liang and Jin, Peiran and Min, Yaosen and Xie, Shufang and Wu, Lijun and Qin, Tao and Liang, Xiaozhuan and Gao, Kaiyuan and Jiang, Yuliang and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2410.24022},
  year={2024}
}
@misc{poli2023hyena,
      title={Hyena Hierarchy: Towards Larger Convolutional Language Models}, 
      author={Michael Poli and Stefano Massaroli and Eric Nguyen and Daniel Y. Fu and Tri Dao and Stephen Baccus and Yoshua Bengio and Stefano Ermon and Christopher Ré},
      year={2023},
      eprint={2302.10866},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2302.10866}, 
}
@misc{
trop2025the,
title={The Genomics Long-Range Benchmark: Advancing {DNA} Language Models},
author={Evan Trop and Yair Schiff and Edgar Mariano Marroquin and Chia Hsiang Kao and Aaron Gokaslan and McKinley Polen and Mingyi Shao and Aymen Kallala and Bernardo P de Almeida and Thomas PIERROT and Yang I Li and Volodymyr Kuleshov},
year={2025},
url={https://openreview.net/forum?id=8O9HLDrmtq}
}

