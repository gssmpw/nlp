\documentclass[12pt]{iopart}
\expandafter\let\csname 
equation*\endcsname=\relax
\expandafter\let\csname endequation*\endcsname=\relax
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage[
singlelinecheck=false % <-- important
]{caption}
% \usepackage{algorithm}
\usepackage{subfigure}
\usepackage{float}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{color}
\usepackage{bm}
\usepackage{amsopn}
\usepackage{stmaryrd}
\usepackage{multirow} 
\usepackage{enumitem}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{caption}
\usepackage{epstopdf}

\DeclareMathOperator*{\argmin}{argmin}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}


\SetKwInput{KwInput}{Input}                % Set the Input

\SetKwInput{KwInitialization}{Initialization} 
\SetKwInput{KwOutput}{Output}              % set the Output
\SetKwInput{KwCondition}{Condition}

\SetKwRepeat{Do}{do}{while}%
% \newtheorem{algorithm}[theorem]{Algorithm}
% \renewcommand{\labelitemi}{Step}
%\renewcommand{\labelitemii}{$\bullet$}
\newcommand{\gguide}{{\it Preparing graphics for IOP Publishing journals}}
%Uncomment next line if AMS fonts required
%\usepackage{iopams}

\begin{document}

\setlength{\abovedisplayskip}{4pt}
\setlength{\belowdisplayskip}{4pt}

\title[Inverse Problems]{Parameter Choices for Sparse Multi-Parameter Regularization with the $\ell_1$ Norm}

\author{Qianru Liu$^{1}$, Rui Wang$^{2}$\footnote{Author to whom any correspondence should be addressed.} 
and Yuesheng Xu$^{3}$ }

\address
{$^{1}$ School of Mathematics and Statistics, Henan University of Technology, Zhengzhou, 450001, People’s Republic of China\\
$^{2}$ School of Mathematics, Jilin University, Changchun 130012, People’s Republic of China\\
$^{3}$ Department of Mathematics and Statistics, Old Dominion University, Norfolk, VA 23529, United States of America}
\eads{\mailto{liuqr23@haut.edu.cn}, \mailto{rwang11@jlu.edu.cn} and \mailto{y1xu@odu.edu}}

\begin{abstract}
This paper introduces a multi-parameter regularization approach using the $\ell_1$
norm, designed to better adapt to complex data structures and problem characteristics while offering enhanced flexibility in promoting sparsity in regularized solutions. As data volumes grow, sparse representations of learned functions become critical for reducing computational costs during function operations. We investigate how the selection of multiple regularization parameters influences the sparsity of regularized solutions. Specifically, we characterize the relationship between these parameters and the sparsity of solutions under transform matrices, enabling the development of an iterative scheme for selecting parameters that achieve prescribed sparsity levels. Special attention is given to scenarios where the fidelity term is non-differentiable, and the transform matrix lacks full row rank. In such cases, the regularized solution, along with two auxiliary vectors arising in the sparsity characterization, are essential components of the multi-parameter selection strategy. To address this, we propose a fixed-point proximity algorithm that simultaneously determines these three vectors. This algorithm, combined with our sparsity characterization, forms the basis of a practical multi-parameter selection strategy. Numerical experiments demonstrate the effectiveness of the proposed approach, yielding regularized solutions with both predetermined sparsity levels and satisfactory approximation accuracy.

\end{abstract}

\section{Introduction}
Multi-parameter regularization is a widely used technique for addressing ill-posed problems \cite{Belkin2006,Brezinski2003,Chen2008,ding2019,Duvelmeyer2006,Lu2013,Lu2007,wang2013}. Motivated by the challenges posed by big data in practical applications, sparse multi-parameter regularization using the $\ell_1$ norm has become a prominent area of research \cite{Afonso2010,Friedman2007,Rapaport2008,Selesnick2014Simultaneous,Shen2024Sparse,Tibshirani2005}. Empirical studies have shown that, compared to single-parameter regularization with the $\ell_1$ norm, the multi-parameter approach offers greater flexibility in promoting sparsity in the regularized solutions while effectively mitigating the ill-posedness inherent in practical problems.

This paper aims to develop a practical parameter selection strategy for multi-parameter regularization with the $\ell_1$ norm, enabling the regularized solution to achieve a specified level of sparsity. To achieve this, we first establish a theoretical understanding of how the regularization parameters affect the sparsity of the solution under transform matrices. Unlike single-parameter regularization, the multi-parameter framework allows independent control over the sparsity of the solution corresponding to each transform matrix.
Using tools from convex analysis, we characterize the relationship between the choice of multiple regularization parameters and the sparsity of the solution under the transform matrices. In the special case where the transform matrices are degenerate identities and the fidelity term exhibits block separability, this characterization provides a direct multi-parameter selection strategy that ensures each sub-vector of the solution achieves a desired level of block sparsity.
In the general case, where such direct adoption is not feasible, we propose an iterative scheme to select the multiple regularization parameters. This scheme ensures the regularized solution attains the prescribed sparsity levels under the transform matrices. Unlike single-parameter regularization models, where only one parameter is optimized, our iterative algorithm considers the interplay between multiple parameters, thereby enhancing the flexibility and effectiveness of the parameter selection process.

%%%%%%%%%%%%







% The second issue involves the error analysis of the solution for the multi-parameter regularization problem. To this end, we  consider a special multi-parameter regularization model of practical importance. In the single-parameter regularization problem, the error is
% compared to the minimal norm solution which is independent of the single regularization parameter. However, in the multi-parameter regularization problem, we contemplate the minimal regularization term solution which generally depends on the multiple  regularization parameters. Under a source condition, we put forward an error estimate between the regularized solution and the minimal regularization term solution. Combining the error estimate and the characterization of the sparsity for the regularized solution, we propose parameter choice strategy so that the regularization term can alleviate the ill-posedness and
% promote sparsity of the resulting regularized solution.


In our recent papers \cite{Liu2023parameter} and \cite{Shen2024Sparse}, we investigated parameter selection strategies for sparse regularization using the $\ell_1$ norm. Compared to these studies, this paper makes two significant contributions:
First, we have established strategies for selecting multiple regularization parameters in a model where the objective function comprises a convex fidelity term and multiple 
$\ell_1$-based regularization terms, each composed with a linear transform. Unlike the iterative algorithm in \cite{Liu2023parameter}, which focused on selecting a single parameter, the algorithm proposed here addresses multiple parameters and explicitly incorporates their interdependencies. While \cite{Shen2024Sparse} also explored iterative schemes for multi-parameter selection in a highly nonconvex optimization problem with numerous network parameters and extensive training data, the model in this paper facilitates a more sophisticated iterative algorithm.
Second, we have developed iterative schemes for selecting multiple regularization parameters in two distinct scenarios: (1) when the fidelity term is differentiable, and the transform matrix has full row rank, and (2) when the fidelity term is nondifferentiable, and the transform matrix lacks full row rank. In contrast, the approaches in \cite{Liu2023parameter} and \cite{Shen2024Sparse} address parameter selection only for the case of a differentiable fidelity term and a full-rank transform matrix.

For the nondifferentiable fidelity term and non-full-rank transform matrix, each iteration requires determining not only the regularized solution but also two auxiliary vectors. To address this, we have characterized these three components using a system of fixed-point equations involving proximity operators. Based on this characterization, we have devised a fixed-point proximity algorithm capable of simultaneously computing all three vectors. The convergence of this algorithm is rigorously analyzed in this paper.
By combining the fixed-point proximity algorithm with the sparsity characterization of the regularized solution, we propose a robust multi-parameter selection strategy that ensures the solution achieves a specified level of sparsity.

%%%%%%%%%%%%%%%%%
%In our recent works \cite{Liu2023parameter} and \cite{Shen2024Sparse}, we studied parameter choices for sparse regularization with the $\ell_1$ norm. Compared with these two papers, the major
%contributions made in this paper can be summarized in the following two aspects. Firstly, we have established choice strategies of the multiple regularization parameters for a regularization model, whose objective function consists of a
%convex fidelity term and multiple regularization terms. Each of these regularization terms is determined by the $\ell_1$ norm composed with a linear transform. In contrast to the iterative algorithm put forward in \cite{Liu2023parameter} for 
%picking the single parameter, the iterative algorithm in this paper is designed to select the multiple parameters and incorporates the interplay between the parameters. 
%The iterative algorithm for choosing multiple parameters was also explored in \cite{Shen2024Sparse}, where we considered a highly nonconvex minimization
%problem with a large number of network parameters and large amount of training
%data. The multi-parameter regularization model considered in this paper enables us to present a more sophisticated iterative algorithm compared to the one devised in \cite{Shen2024Sparse}. 

%Secondly, we have developed iterative schemes to select multiple regularization parameters in both of the following situations: when the fidelity term is differentiable and the transform matrix has full row rank, as well as when the fidelity term is not differentiable and the transform matrix does not have full row rank. However, in \cite{Liu2023parameter} and \cite{Shen2024Sparse}, the iterative schemes were devised to choose the single parameter or the multiple parameters only for the case that the fidelity term is differentiable and the transform matrix has full row rank. Unlike in the case that the fidelity term is differentiable and the transform matrix has full row rank, at each step of the iteration scheme, we are required to determine not only the solution of the regularization problem but also two additional vectors. To address this issue, we have characterized these three vectors in terms of a system of fixed-point equations via proximity operators. Based upon the fixed-point equations, a fixed-point proximity algorithm that can simultaneously determine these three vectors has been devised.  The  convergence analysis of the resulting algorithm has also been provided in this paper. Combining the fixed-point proximity algorithm with the characterization of the sparsity of the regularized solution, we have developed a multi-parameter choice strategy with which the regularized solution has a sparsity of a certain level.
This paper is organized into seven sections and two appendices. Section 2 introduces the general multi-parameter regularization problem under investigation and highlights several examples of practical significance. In Section 3, we establish a characterization of how each regularization parameter affects the sparsity of the regularized solution under its associated transform matrix. This characterization is further specialized in Section 4 to the case where the transform matrices are degenerate identities.
Building on the insights from Sections 3 and 4, Section 5 presents iterative schemes for simultaneously determining multiple parameters and solutions with prescribed sparsity levels under the transform matrices. Two scenarios are addressed: (1) when the fidelity term is differentiable and the transform matrix has full row rank, and (2) when the fidelity term is nondifferentiable and the transform matrix does not have full row rank.
Section 6 demonstrates the effectiveness of the proposed parameter selection strategies through a series of numerical examples. Section 7 concludes the paper by summarizing the findings.
Appendix A contains the proof of the convergence analysis for the fixed-point proximity algorithm introduced in Section 5, while Appendix B provides closed-form expressions for the proximity operators used in the algorithm.

%%%%%%%%%%%%%%%%%%
%We organize this paper in seven sections and an appendix. We describe in Section 2 the general multi-parameter regularization problem under investigation and identify several examples of practical importance. In Section 3, we establish a characterization regarding how each regularization parameter  
%influences the sparsity of the regularized solution under corresponding transform matrix. This 
%characterization is specialized in Section 4 to the particular case that the transform matrices are degenerated identities. In Section 5, motivated by the characterizations established in the previous two sections, we develop iterative schemes that simultaneously determines multiple parameters 
%and solution having a prescribed sparsity levels under the transform matrices. We consider two cases: when the fidelity term is differentiable and the transform matrix has full row rank, as well as when the fidelity term is not differentiable and the transform matrix does not have full row rank. Section 6 is dedicated to demonstrating the effectiveness of the regularization
%parameter choice strategy through several numerical
%examples. We draw a conclusion in Section 7. Finally, the proof for the convergence analysis of the fixed-point
%proximity algorithm devised in Section 5 is included in Appendix A and the closed form formulas for the proximity operators involved in the fixed-point
%proximity algorithm are provided in Appendix B.

\section{Multi-parameter regularization with the \texorpdfstring{$\ell_1$}{} norm}

In this section, we introduce the multi-parameter regularization problem considered in this paper. Additionally, we review several optimization models of practical significance and illustrate how they can be formulated within this general framework.

We begin with describing the multi-parameter regularization problem.  For each $s\in\mathbb{N}$, let  $\mathbb{N}_s:=\{1, 2, \ldots,s\}$ and set $\mathbb{N}_0:=\emptyset$. Suppose that $\bm{\psi}:\mathbb{R}^n\to\mathbb{R}_+:=[0,+\infty)$ is a convex function and for each $j\in\mathbb{N}_d$, $m_j\in\mathbb{N}$ and  $\mathbf{B}_{j}$ is an $m_j\times n$ real matrix. For each $m\in\mathbb{N}$ and $\mathbf{x}:=[x_j:j\in\mathbb{N}_m]\in\mathbb{R}^m$, we define its $\ell_1$ norm by $\|\mathbf{x}\|_1:=\sum_{j\in \mathbb{N}_m}|x_j|$. We consider the multi-parameter regularization problem 
\begin{equation}\label{optimization_problem_under_Bj}
\min
\left\{\bm{\psi}(\mathbf{u})+\sum_{j\in\mathbb{N}_d}\lambda_j\|\mathbf{B}_{j}\mathbf{u}\|_{1}:\mathbf{u}\in\mathbb{R}^n\right\},
\end{equation}
% \begin{equation}\label{optimization_problem_under_B}
% \min
% \left\{\bm{\psi}(\mathbf{u})+\sum_{j\in\mathbb{N}_d}\lambda_j\|\mathbf{B}_j\mathbf{u}_j\|_{1}:\mathbf{u}\in\mathbb{R}^n\right\},
% \end{equation}
where $\lambda_j>0,$ $j\in\mathbb{N}_d,$ is a sequence of positive regularization parameters. 

The multi-parameter regularization problem \eqref{optimization_problem_under_Bj} appears in many application areas. Below, we present several examples of it. In imaging reconstruction problems, a combination of multiple regularizers
was used to encourage their solution to simultaneously exhibit the characteristics enforced by each of them. For example, a combination of frame-based synthesis and analysis $\ell_1$ norm regularizers was proposed in \cite{Afonso2010} for an imaging deblurring problem. Specifically, for $m,n,s\in\mathbb{N}$, we assume that $\mathbf{A}\in\mathbb{R}^{m\times s}$ represents a periodic convolution, $\mathbf{W}\in \mathbb{R}^{s\times n}$ is a synthesis operator whose columns contain the elements of a frame, and $\mathbf{P}\in \mathbb{R}^{m\times s}$ is an analysis operator of a tight Parseval frame satisfying $\mathbf{P}^{\top}\mathbf{P}=\mathbf{I}_s$, where $\mathbf{I}_s$ denotes the identity matrix of order $s$. Let $\mathbf{y}\in\mathbb{R}^m$ be  observed data. The regularization problem combining the synthesis and analysis $\ell_1$ norm regularizers has the form
\begin{equation}\label{CSA}
\min\left\{\frac{1}{2}\|\mathbf{AW}\mathbf{u}-\mathbf{y}\|_2^2+\lambda_1\|\mathbf{u}\|_1+\lambda_2\|\mathbf{PW}\mathbf{u}\|_1:\mathbf{u}\in\mathbb{R}^n\right\}.
\end{equation}
Clearly, problem \eqref{CSA} may be identified as a special case of \eqref{optimization_problem_under_Bj} with $$
\bm{\psi}(\mathbf{u}):=\frac{1}{2}\|\mathbf{AW}\mathbf{u}-\mathbf{y}\|_2^2 \ \ \mbox{for}\  \mathbf{u}\in\mathbb{R}^n, \ \ \mathbf{B}_{1}:=\mathbf{I}_n,\  \ \mathbf{B}_{2}:=\mathbf{PW}.
$$


As a generalization of the lasso regularized model \cite{tibshirani1996regression}, the fused lasso regularized model was proposed in \cite{Tibshirani2005} for problems with features that can be ordered in some meaningful way. Let $p,n\in\mathbb{N}$. Suppose that a prediction problem with $p$ cases has outcomes $y_i$, $i\in\mathbb{N}_p$ and features $x_{ij}$, $i\in\mathbb{N}_p$, $j\in\mathbb{N}_n$. Let $\mathbf{X}:=[x_{ij}:i\in\mathbb{N}_p,j\in\mathbb{N}_n]$ be the $p\times n$ matrix of features and $\mathbf{y}:=[y_i:i\in\mathbb{N}_p]\in\mathbb{R}^p$,  $\mathbf{u}\in\mathbb{R}^n$ be the vectors of outcomes and coefficients, respectively. The fused lasso regularized model is formulated as 
\begin{equation}\label{fused_lasso}
\min\left\{\frac{1}{2}\|\mathbf{X}\mathbf{u}-\mathbf{y}\|_2^2+\lambda_1\|\mathbf{u}\|_1+\lambda_2\|\mathbf{Du}\|_1:\mathbf{u}\in\mathbb{R}^n\right\},
\end{equation}
where $\mathbf{D}:=[d_{ij}:i\in \mathbb{N}_{n-1},j\in\mathbb{N}_n]$ is the $(n-1)\times n$ first order difference matrix with $d_{ii}=-1,$ $d_{i,i+1}=1$ for $i\in\mathbb{N}_{n-1}$ and $0$ otherwise. By introducing 
$$
\bm{\psi}(\mathbf{u}):=\frac{1}{2}\|\mathbf{X}\mathbf{u}-\mathbf{y}\|_2^2, \ \mbox{for}\ \mathbf{u}\in\mathbb{R}^n, \ \mathbf{B}_{1}:=\mathbf{I}_{n}, \ \mathbf{B}_{2}:=\mathbf{D},
$$ 
the fused lasso model \eqref{fused_lasso} can be rewritten in the form of \eqref{optimization_problem_under_Bj}. By penalizing the
$\ell_1$ norm of both the coefficients and their successive differences, the fused lasso regularized model encourages the sparsity
of the coefficients and also the sparsity of their differences. As a special case, the fused lasso signal approximation \cite{Friedman2007} has the form \eqref{fused_lasso} with the feature matrix $\mathbf{X}$ being the identity matrix.

Filtering noisy data was considered in \cite{Selesnick2014Simultaneous} for the case where the underlying signal comprises a low-frequency component and  a sparse or sparse-derivative component. Specifically, assume that the noisy data $y(t)$ can be modeled as 
\begin{equation}\label{noisy-data-form}
y(t)=f(t)+u(t)+\eta(t),
\ t \in\mathbb{N}_n,
\end{equation}
where $f$ is a low-pass signal, $u$ is a sparse and sparse-derivative signal and $\eta$ is stationary white Gaussian noise. Given noisy data of the form \eqref{noisy-data-form}, one seeks the estimate of $f$ and $u$ individually. For this purpose, we first solve the compound sparse denoising problem
\begin{equation}\label{CSD}
\min\left\{\frac{1}{2}\|\mathbf{\mathbf{H}}(\mathbf{y}-\mathbf{u})\|_2^2+\lambda_1\|\mathbf{u}\|_1+\lambda_2\|\mathbf{D}\mathbf{u}\|_1:\mathbf{u}\in\mathbb{R}^{n}\right\}  
\end{equation}
to obtain the estimate $\mathbf{u}^*$ of $u$. Here, $\mathbf{y}:=[y(t):t\in\mathbb{N}_n]$ and  $\mathbf{H}$ is the high-pass filter matrix with the form $\mathbf{H}:=\mathbf{A}^{-1}\mathbf{C}$, where $\mathbf{A}$ and $\mathbf{C}$ are banded matrices. We then get the estimate $\mathbf{f}^*$ of $f$ as $\mathbf{f}^*:=(\tilde{\mathbf{I}}-\mathbf{A}^{-1}\mathbf{C})(\mathbf{y}-\mathbf{u}^*)$. It is clear that the compound sparse denoising model \eqref{CSD} has the form \eqref{optimization_problem_under_Bj} with 
\begin{equation}\label{fidelity-CSD}
\bm{\psi}(\mathbf{u}):=\frac{1}{2}\|\mathbf{\mathbf{H}}(\mathbf{y}-\mathbf{u})\|_2^2,\ \  \mbox{for all}\  \ \mathbf{u}\in\mathbb{R}^n,
\end{equation} 
and $\mathbf{B}_{1}:=\mathbf{I}_n$,  $\mathbf{B}_{2}:=\mathbf{D}$.

Using a technique similar to that used in the fused lasso regularized model, the fused SVM was proposed for classification of array-based comparative genomic hybridization
(arrayCGH) data \cite{Rapaport2008,Tolosi2011}.  Given training data $\{(\mathbf{x}_j,y_j):j\in\mathbb{N}_p\}$ composed of sample points $\{\mathbf{x}_j:j\in\mathbb{N}_p\}\subset \mathbb{R}^n$ and labels $\{y_j:j\in\mathbb{N}_p\}\subset\{-1,1\}$. The aim of binary classification is to find a decision function $f(\mathbf{x}):=\mathbf{u}^{\top}\mathbf{x}$, $\mathbf{x}\in\mathbb{R}^n$ predicting the class $y_j=-1$ or $y_j=1$. The class prediction for a profile
$\mathbf{x}$ is then $1$ if $f(\mathbf{x})\geq 0$ and $-1$ otherwise. The fused SVM based on the hinge loss function has the form
\begin{equation}\label{fused-SVM}
\min\left\{\sum_{j\in\mathbb{N}_p}\mathrm{max}(0,1-y_j\mathbf{u}^{\top}\mathbf{x}_j)+\lambda_1\|\mathbf{u}\|_1+\lambda_2\|\mathbf{D}\mathbf{u}\|_1:\mathbf{u}\in\mathbb{R}^n\right\}.  
\end{equation}
We rewrite model \eqref{fused-SVM} in the form \eqref{optimization_problem_under_Bj} as follows. We define the matrix $\mathbf{X}:=[\mathbf{x}_j:j\in\mathbb{N}_p]^{\top}$, the matrix $\mathbf{Y}:=\mathrm{diag}(y_j:j\in\mathbb{N}_p)$ and the function $\bm{\phi}(\mathbf{z}):=\sum_{j\in\mathbb{N}_p}\mathrm{max}\{0,1-z_j\}$, for all $\mathbf{z}:=[z_j:j\in\mathbb{N}_p]\in\mathbb{R}^p$. Then by introducing 
$\bm{\psi}(\mathbf{u}):=\bm{\phi}(\mathbf{YXu})$, for $\mathbf{u}\in\mathbb{R}^{n}$,  $\mathbf{B}_{1}:=\mathbf{I}_n$, and $\mathbf{B}_{2}:=\mathbf{D}$,
the fused SVM model \eqref{fused-SVM} can be represented in the form \eqref{optimization_problem_under_Bj}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Choices of the multiple regularization parameters}
In this section, we
characterize the relation between the multiple regularization parameters and the sparsity
of the regularized solution under the transform matrices $\mathbf{B}_{j}$, $j\in\mathbb{N}_d$, of the regularization problem \eqref{optimization_problem_under_Bj}. Unlike the single-parameter regularization problem, the use of multiple regularization parameters in problem \eqref{optimization_problem_under_Bj} allows us to separately consider the sparsity of the solution under each transform matrix $\mathbf{B}_{j}$.

We begin by recalling the definition of the level of sparsity for a vector in $\mathbb{R}^s$. For $s\in\mathbb{N}$, we set $\mathbb{Z}_s:=\{0,1,\ldots,s-1\}$. A vector $\mathbf{x}\in\mathbb{R}^s$ is said to have sparsity of level $l\in\mathbb{Z}_{s+1}$ if it has exactly $l$ nonzero components. 

We now reformulate the regularization problem \eqref{optimization_problem_under_Bj} into an equivalent form to facilitate the characterization of the sparsity of the solution under the transform matrices $\mathbf{B}_{j}$, $j\in\mathbb{N}_d$. Let $p_0:=0$ and $
p_j:=\sum_{i\in\mathbb{N}_j}m_i$, $j\in\mathbb{N}_d$.  We decompose a vector $\mathbf{z}:=[z_k:k\in\mathbb{N}_{p_d}]\in\mathbb{R}^{p_d}$ into $d$ sub-vectors by setting \begin{equation*}\label{z_j}
\mathbf{z}_j:=[z_{p_{j-1}+i}: i\in\mathbb{N}_{m_j}]\in\mathbb{R}^{m_j}\ \mbox{for all}\ j\in\mathbb{N}_d.
\end{equation*}
By introducing a column block matrix 
\begin{equation}\label{block-matrix-B}
\mathbf{B}:=[\mathbf{B}_{j}:j\in\mathbb{N}_d]\in\mathbb{R}^{p_d\times n}, 
\end{equation}
we write $\mathbf{B}\mathbf{u}$ for the block column vector $[\mathbf{B}_j\mathbf{u}: j\in\mathbb{N}_d]$.
Thus, we may rewrite the regularization problem \eqref{optimization_problem_under_Bj} as 
\begin{equation}\label{optimization_problem_under_Bj_1}
\min
\left\{\bm{\psi}(\mathbf{u})+\sum_{j\in\mathbb{N}_d}\lambda_j\|(\mathbf{B}\mathbf{u})_{j}\|_{1}:\mathbf{u}\in\mathbb{R}^n\right\}.
\end{equation}

We further convert problem \eqref{optimization_problem_under_Bj_1} into an equivalent form by utilizing a change of variables. To achieve this, we consider inverting the linear system
\begin{equation}\label{LinearSystem}
    \mathbf{B}\mathbf{u}=\mathbf{z}, \ \ \mbox{for each}\ \ \mathbf{z}\in\mathcal{R}(\mathbf{B}).
\end{equation}
Here, $\mathcal{R}(\mathbf{B})$ denotes the range of $\mathbf{B}$. It is known from \cite{Bjork1996,horn2012matrix} that the general solution of the linear system \eqref{LinearSystem} can be represented by the pseudoinverse of  $\mathbf{B}$. An alternative form of the general solution was provided in \cite{Liu2023parameter}. To describe this result, we recall that if $\mathbf{B}$ has the rank $r$ satisfying $0<r\leq \mathrm{min}\{p_d,n\}$, then $\mathbf{B}$ has the SVD as
$\mathbf{B}=\mathbf{U}\mathbf{\Lambda} \mathbf{V}^{\top}$, where $\mathbf{U}$ and $\mathbf{V}$ are $p_d\times p_d$ and $n\times n$ orthogonal matrices, respectively, and $\mathbf{\Lambda}$ is a $p_d\times n$ diagonal matrix with the nonzero diagonal entries $\sigma_1\geq\cdots\geq\sigma_r>0$, which are the nonzero singular values of $\mathbf{B}$. In order to represent the general solution of linear system \eqref{LinearSystem}, we define  
an $n\times(p_d+n-r)$ matrix by employing the SVD of $\mathbf{B}$. Specifically, we denote by $\widetilde{\mathbf{U}}_{r}\in\mathbb{R}^{p_d\times r}$ the matrix composed of the first $r$ columns of $\mathbf{U}$ and define an $n\times(p_d+n-r)$ block diagonal matrix by setting
$$
\mathbf{U}':=\mathrm{diag}\left(\widetilde{\mathbf{U}}_{r}^{\top}, \mathbf{I}_{n-r}\right).
$$ 
We also introduce a diagonal matrix of order $n$ by 
$$
\mathbf{\Lambda}':=\mathrm{diag}\left(\sigma_1^{-1},\sigma_2^{-1},\ldots,\sigma_r^{-1},1,\ldots,1\right).
$$
Using these matrices, we define an $n\times(p_d+n-r)$ matrix by $\mathbf{B}'
:=\mathbf{V}\mathbf{\Lambda}'\mathbf{U}'.$ As has been shown in \cite{Liu2023parameter}, for each solution $\mathbf{u}$ of system  \eqref{LinearSystem}, there exists a unique vector $\mathbf{v}\in\mathbb{R}^{n-r}$ such that 
\begin{equation}\label{solution-Bu=z}
\mathbf{u}=\mathbf{B}'{\small\begin{bmatrix}
\mathbf{z}\\
\mathbf{v}\end{bmatrix}}.
\end{equation}
As a result, the mapping $\mathcal{B}$, defined for each $\mathbf{u}\in\mathbb{R}^n$ by 
$\mathcal{B}\mathbf{u}:=\scriptsize{\begin{bmatrix}
\mathbf{z}\\
\mathbf{v}\end{bmatrix}},$
where $\mathbf{z}:=\mathbf{B}\mathbf{u}$ and $\mathbf{v}\in\mathbb{R}^{n-r}$ satisfies equation \eqref{solution-Bu=z}, is bijective from $\mathbb{R}^n$ onto $\mathcal{R}(\mathbf{B})\times\mathbb{R}^{n-r}$. 

By making use of the change of variables defined by equation \eqref{solution-Bu=z}, we reformulate problem \eqref{optimization_problem_under_Bj_1} as an equivalent multi-parameter regularization problem with for each $j\in\mathbb{N}_d$,  $\mathbf{B}_{j}$ being a degenerated identity. We set
$\mathbb{M}:=\mathcal{R}(\mathbf{B})\times\mathbb{R}^{n-r}$
and let $\iota_{\mathbb{M}}:\mathbb{R}^{p_d+n-r}\rightarrow\overline{\mathbb{R}}:=\mathbb{R}\cup\{+\infty\}$ denote the indicator function of $\mathbb{M}$, that is, $\iota_{\mathbb{M}}(\mathbf{w})=0$ if $\mathbf{w}\in \mathbb{M}$, and $+\infty$ otherwise. For each $j\in\mathbb{N}_d$, we  introduce a degenerated identity matrix by
\begin{equation}\label{I-j}
\mathbf{I}^{'}_{j}:=[\mathbf{0}_{m_j\times p_{j-1}}\  \mathbf{I}_{m_j}\ \mathbf{0}_{m_j\times (p_d-p_j)} \  \mathbf{0}_{m_j\times (n-r)}]\in\mathbb{R}^{m_j\times (p_d+n-r)},
\end{equation}
where $\mathbf{0}_{s\times t}$ denotes the zero matrix of order $s\times t$. 
We show in the following lemma that the regularization problem \eqref{optimization_problem_under_Bj} is equivalent to the regularization problem 
\begin{equation}\label{optimization_problem_under_Bj_3}
\min
\left\{\bm{\psi}\circ\mathbf{B}'(\mathbf{w})+\iota_{\mathbb{M}}(\mathbf{w})+\sum_{j\in\mathbb{N}_d}\lambda_j\|\mathbf{I}^{'}_{j}\mathbf{w}\|_{1}:\mathbf{w}\in\mathbb{R}^{p_d+n-r}\right\}.
\end{equation}
% \begin{equation}\label{optimization_problem_under_Bj}
% \min
% \left\{\bm{\psi}(\mathbf{u})+\sum_{j\in\mathbb{N}_d}\lambda_j\|\mathbf{B}_{(j)}\mathbf{u}\|_{1}:\mathbf{u}\in\mathbb{R}^n\right\},
% \end{equation}

\begin{lemma}\label{equi_mini}
If matrix $\mathbf{B}$ has the form \eqref{block-matrix-B} and $\mathbf{B}'$,  $\mathcal{B}$ are defined as above, then $\mathbf{u}^*$ is a solution of the regularization problem \eqref{optimization_problem_under_Bj} if and only if $\mathcal{B}\mathbf{u}^*$ is a solution of the regularization problem  \eqref{optimization_problem_under_Bj_3}.
\end{lemma}
\begin{proof}
We first prove that $\mathbf{u}^*$ is a solution of problem \eqref{optimization_problem_under_Bj} if and only if $\mathcal{B}\mathbf{u}^*$ is a solution of the constrained optimization problem  \begin{equation}\label{optimization_problem_under_Bj_2}
\min
\left\{\bm{\psi}\circ\mathbf{B}'(\mathbf{w})+\sum_{j\in\mathbb{N}_d}\lambda_j\|\mathbf{I}^{'}_{j}\mathbf{w}\|_{1}:\mathbf{w}\in\mathbb{M}\right\}.
\end{equation} 
As has been shown in \cite{Liu2023parameter}, $\mathcal{B}$ is a bijective mapping from $\mathbb{R}^n$ to $\mathbb{M}$. It suffices to verify that for all $\mathbf{u}\in\mathbb{R}^n$ there holds 
$$
\bm{\psi}(\mathbf{u})+\sum_{j\in\mathbb{N}_d}\lambda_j\|(\mathbf{B}\mathbf{u})_{j}\|_{1}=\bm{\psi}\circ\mathbf{B}'(\mathcal{B}\mathbf{u})+\sum_{j\in\mathbb{N}_d}\lambda_j\|\mathbf{I}^{'}_{j}(\mathcal{B}\mathbf{u})\|_{1}.
$$
By the definition of mapping $\mathcal{B}$, we get that 
$\mathbf{B}'\mathcal{B}\mathbf{u}=\mathbf{u}$ and $\mathbf{I}^{'}_{j}(\mathcal{B}\mathbf{u})=(\mathbf{B}\mathbf{u})_j$, which confirm the validity of the equation
above.

We next show that problem  \eqref{optimization_problem_under_Bj_2} as a constrained optimization problem is equivalent to the unconstrained optimization problem \eqref{optimization_problem_under_Bj_3}. By the definition of the indicator function $\iota_{\mathbb{M}}$,  the minimum of problem  \eqref{optimization_problem_under_Bj_3} will be assumed at an element $\mathbf{w}\in\mathbb{M}$. Thus, problem  \eqref{optimization_problem_under_Bj_3} can be
rewritten as problem \eqref{optimization_problem_under_Bj_2}.
\end{proof}


Below, we consider how the regularization parameters $\lambda_j$, $j\in\mathbb{N}_d$ influence the sparsity of the solution of problem  \eqref{optimization_problem_under_Bj_3}. To characterize the solution of problem \eqref{optimization_problem_under_Bj_3}, we need the notion of the subdifferential of a convex function. Suppose that $f: \mathbb{R}^s\to \overline{\mathbb{R}}$ is a proper convex function. The subdifferential of $f$ at $\mathbf{x}\in{\rm dom}(f):=\{\mathbf{y}\in\mathbb{R}^s:f(\mathbf{y})<+\infty\}$ is defined by
$$
\partial f(\mathbf{x}):=\{\mathbf{y}\in\mathbb{R}^s: \ f(\mathbf{z})\geq f(\mathbf{x})+\langle \mathbf{y},\mathbf{z}-\mathbf{x}\rangle, \ \mathrm{for} \ \mathrm{all} \ \mathbf{z}\in\mathbb{R}^s\}.
$$
It is known \cite{zalinescu2002convex} that for two convex functions $f$ and $g$ on $\mathbb{R}^s$, if $g$ is continuous on $\mathbb{R}^s$ then 
$\partial (f+g)(\mathbf{x})
=\partial f(\mathbf{x}) +\partial g(\mathbf{x})$, for all $\mathbf{x}\in\mathbb{R}^s$. 
We also describe the chain rule of the subdifferential \cite{Showalter1997}. Suppose
that $f:\mathbb{R}^s\rightarrow\overline{\mathbb{R}}$ is a convex function and
$\mathbf{M}$ is an $s\times t$ matrix. If $f$ is continuous at some point of the range of
$\mathbf{M}$, then for all $\mathbf{x}\in\mathbb{R}^t$ 
\begin{equation}\label{chain-rule}
\partial (f\circ \mathbf{M})(\mathbf{x})=\mathbf{M}^{\top}\partial f(\mathbf{M}(\mathbf{x})).
\end{equation}
The Fermat rule \cite{zalinescu2002convex} states that if a proper convex function
$f:\mathbb{R}^s\rightarrow\overline{\mathbb{R}}$ has a minimum at $\mathbf{x}\in\mathbb{R}^s$ if and only if  $\mathbf{0}\in \partial f(\mathbf{x})$. 

For the purpose of characterizing sparsity of vectors in $\mathbb{R}^s$, we also recall the notion of the sparsity partition of $\mathbb{R}^s$ introduced in \cite{xu2023}. Specifically, by using the canonical basis $\mathbf{e}_{s,j},j\in\mathbb{N}_s,$ for $\mathbb{R}^s$, we introduce $s+1$ numbers of subsets of $\mathbb{R}^s$ by
\begin{align*}
&\Omega_{s,0}:=\{\mathbf{0}\in\mathbb{R}^s\}, \nonumber\\
&\Omega_{s,l}:=\left\{\sum_{j\in\mathbb{N}_l}x_{k_j}\mathbf{e}_{s,k_j}
:x_{k_j}\in\mathbb{R}\setminus{\{0\}},\ \mathrm{for} \
1\leq k_1<k_2<\cdots< k_l\leq s\right\},
\ \mathrm{for} \ l\in\mathbb{N}_s.
\end{align*}
It is clear that the sets $\Omega_{s,l},l\in \mathbb{Z}_{s+1}$, form a partition for $\mathbb{R}^s$ and for each $l\in\mathbb{Z}_{s+1}$, $\Omega_{s,l}$ coincides with the set of all vectors in $\mathbb{R}^s$ having sparsity of level $l$. 
% We denote by $\mathbb{K}_{s,l}$ an ordered subset $\{k_i: i\in\mathbb{N}_l\}$ of $\mathbb{N}_s$ with cardinality $l\in\mathbb{N}_s$ in the order 
% $k_1<k_2<\cdots<k_l.$}
% We denote by $\mathbb{K}_{s,l}$ an ordered subset of $\mathbb{N}_s$ with cardinality $l\in\mathbb{N}_s$, that is, 
% $$
% \mathbb{K}_{s,l}:=\{k_i\in \mathbb{N}_{s}: i\in\mathbb{N}_l, \ \mbox{with}\ 1\leq k_1<k_2<\cdots<k_l\leq s\}.
% $$
% In this notation, for each $\mathbf{x}\in \Omega_{s,l}$, $l\in\mathbb{N}_s$, there exists an ordered subset $\mathbb{K}_{s,l}$ of $\mathbb{N}_{s}$ with cardinality $l$ such that
% $$
% \mathbf{x}=\sum_{i\in \mathbb{K}_{s,l}}x_i\mathbf{e}_{s,i}, \ \mbox{with}\ x_i\in\mathbb{R}\setminus{\{0\}}, \ \mbox{for all}\ i\in \mathbb{K}_{s,l}.
% $$
% We also set $\mathbb{K}_{s,0}:=\emptyset$.

We provide in the next lemma a characterization of the sparsity of the solution of problem \eqref{optimization_problem_under_Bj_3}. For an $s\times t$ matrix $\mathbf{M}$, we denote by $\mathcal{N}(\mathbf{M})$ the null space of $\mathbf{M}$ and for each $i\in\mathbb{N}_{t}$, denote by $\mathbf{M}_{(i)}$ the $i$th column of $\mathbf{M}$.

\begin{lemma}\label{sparsity-equi-mini}
Suppose that $\bm{\psi}:\mathbb{R}^n\to\mathbb{R}_+$ is a convex function  and for each $j\in\mathbb{N}_d$, $\mathbf{B}_{j}$ is an $m_j\times n$ matrix. Let $\mathbf{B}$ be defined as in \eqref{block-matrix-B}.  Then problem \eqref{optimization_problem_under_Bj_3}
with $\lambda_j>0$, $j\in \mathbb{N}_d$, has a solution $\mathbf{w}^{*}:=\scriptsize{\begin{bmatrix}
\mathbf{z^*}\\
\mathbf{v^*}\end{bmatrix}}$ with for each $j\in \mathbb{N}_d,$ $\mathbf{z}_j^{*}=\sum_{i\in\mathbb{N}_{l_j}}z_{p_{j-1}+k_i}^*\mathbf{e}_{m_j,k_i}\in \Omega_{m_j,l_j},$ for some $l_{j}\in\mathbb{Z}_{m_j+1}$ if and only if there exist  $\mathbf{a}\in\partial\bm{\psi}(\mathbf{B}'\mathbf{w}^*)$ and $\mathbf{b}:=[b_j:j\in\mathbb{N}_{p_d}]\in\mathcal{N}(\mathbf{B}^{\top})$ such that 
\begin{equation}\label{sparsity-equi-mini-1}
(\mathbf{B}'_{(i)})^{\top}\mathbf{a}={0}, \ i\in\mathbb{N}_{p_d+n-r}\setminus\mathbb{N}_{p_d}, 
\end{equation}
and for each $j\in\mathbb{N}_d$
\begin{equation}\label{sparsity-equi-mini-2}
\lambda_j=-\left( (\mathbf{B}_{(p_{j-1}+k_i)}')^\top\mathbf{a}+b_{p_{j-1}+k_i}\right)\mathrm{sign}(z_{p_{j-1}+k_i}^*),\  i\in \mathbb{N}_{l_j},
\end{equation}
\begin{equation}\label{sparsity-equi-mini-3}
\lambda_j\geq\left|(\mathbf{B}_{(p_{j-1}+i)}')^\top\mathbf{a}+b_{p_{j-1}+i}\right|, \ i\in \mathbb{N}_{m_j}\setminus\{k_i:i\in \mathbb{N}_{l_j}\}.
\end{equation}
\end{lemma}

\begin{proof}
By the Fermat rule, we have that  
$\mathbf{w}^*:=
  \scriptsize{\begin{bmatrix}
\mathbf{z}^*\\
\mathbf{v}^*\end{bmatrix}}$ is a solution of problem \eqref{optimization_problem_under_Bj_3} if and only if  
$$
\mathbf{0}\in\partial\left(\bm{\psi}\circ\mathbf{B}'+\iota_{\mathbb{M}}+\sum_{j\in\mathbb{N}_d}\lambda_j\|\cdot\|_{1}\circ\mathbf{I}^{'}_{j}\right)(\mathbf{w}^*),
$$
which by the continuity of the $\ell_1$ norm and the chain rule \eqref{chain-rule} of the subdifferential is equivalent to
\begin{equation}\label{Fermat-rule-chain-rule}
\mathbf{0}\in(\mathbf{B}')^\top\partial\bm{\psi}(\mathbf{B}'\mathbf{w}^*)+\partial\iota_{\mathbb{M}}(\mathbf{w}^*)+\sum_{j\in\mathbb{N}_d}\lambda_j(\mathbf{I}^{'}_{j})^\top\partial \|\cdot\|_1(\mathbf{z}_j^*).
\end{equation}
Let $\mathbb{M}^{\bot}$ denote the  orthogonal complement of $\mathbb{M}$. It is known that    $\partial\iota_{\mathbb{M}}(\mathbf{w})=\mathbb{M}^{\bot}$ for all $\mathbf{w}\in \mathbb{M}$. Recalling that $\mathbb{M}:=\mathcal{R}(\mathbf{B})\times\mathbb{R}^{n-r}$, we get that 
$$
\mathbb{M}^{\bot}=(\mathcal{R}(\mathbf{B}))^{\bot}\times(\mathbb{R}^{n-r})^{\bot}=\mathcal{N}(\mathbf{B}^{\top})\times\{\mathbf{0}\}.
$$
As a result, $\partial\iota_{\mathbb{M}}(\mathbf{w})=\mathcal{N}(\mathbf{B}^{\top})\times\{\mathbf{0}\}$, for all $\mathbf{w}\in \mathbb{M}$.
Substituting the above equation with $\mathbf{w}:=\mathbf{w}^*$ into the inclusion relation \eqref{Fermat-rule-chain-rule}, we conclude that  $\mathbf{w}^*:=
  \scriptsize{\begin{bmatrix}
\mathbf{z}^*\\
\mathbf{v}^*\end{bmatrix}}$ is a solution of problem \eqref{optimization_problem_under_Bj_3} if and only if there exist  $\mathbf{a}\in\partial\bm{\psi}(\mathbf{B}'\mathbf{w}^*)$ and $\mathbf{b}\in\mathcal{N}(\mathbf{B}^{\top})$ such that 
\begin{equation}\label{Fermat-rule-chain-rule-1}
  -(\mathbf{B}')^\top\mathbf{a}- 
  \small{\begin{bmatrix}\mathbf{b}\\ \mathbf{0}\end{bmatrix}}\in\sum_{j\in\mathbb{N}_d}\lambda_j(\mathbf{I}^{'}_{j})^\top\partial \|\cdot\|_1(\mathbf{z}_j^*).  
\end{equation}
Note that for each $j\in\mathbb{N}_d$
$$
\mathbf{z}_j^{*}=\sum_{i\in\mathbb{N}_{l_j}}z_{p_{j-1}+k_i}^*\mathbf{e}_{m_j,k_i}\in \Omega_{m_j,l_j},\ \mbox{with}\ z_{p_{j-1}+k_i}^*\in\mathbb{R}\setminus{\{0\}}, \ i\in \mathbb{N}_{l_j},
$$ 
with which we obtain that
{\small
\begin{equation*}\label{subdiff-1norm-z*}
\partial\|\cdot\|_1(\mathbf{z}_j^{*})
=\left\{\mathbf{x}\in\mathbb{R}^{m_j}: x_{k_i}=\mathrm{sign}(z_{p_{j-1}+k_i}^*), i\in\mathbb{N}_{l_j}\ \mbox{and}\ |x_i|\leq 1, i\in\mathbb{N}_{m_j}\setminus\{k_i:i\in \mathbb{N}_{l_j}\}\right\}.
\end{equation*}}
Combining inclusion relation \eqref{Fermat-rule-chain-rule-1} with the above equation, we have that $(\mathbf{B}'_{(i)})^{\top}\mathbf{a}={0}$ for all $i\in\mathbb{N}_{p_d+n-r}\setminus\mathbb{N}_{p_d}$, which coincides with equation \eqref{sparsity-equi-mini-1} and for each $j\in\mathbb{N}_d$
$$
-(\mathbf{B}_{(p_{j-1}+k_i)}')^\top\mathbf{a}-b_{p_{j-1}+k_i}=\lambda_j\mathrm{sign}(z_{p_{j-1}+k_i}^*),\ i\in \mathbb{N}_{l_j},
$$
$$
-(\mathbf{B}_{(p_{j-1}+i)}')^\top\mathbf{a}-b_{p_{j-1}+i}\in[-\lambda_j,\lambda_j], \ i\in \mathbb{N}_{m_j}\setminus\{k_i:i\in \mathbb{N}_{l_j}\},
$$
which are equivalent to equation \eqref{sparsity-equi-mini-2} and inequality \eqref{sparsity-equi-mini-3}, respectively.
%
\end{proof}

Combining Lemmas \ref{equi_mini} with \ref{sparsity-equi-mini}, we establish the relation between the regularization parameters and the sparsity of the solution of problem \eqref{optimization_problem_under_Bj}.
\begin{theorem}\label{sparsity_original}
Suppose that $\bm{\psi}:\mathbb{R}^n\to\mathbb{R}_+$ is a convex function  and for each $j\in\mathbb{N}_d$, $\mathbf{B}_{j}$ is an $m_j\times n$ matrix. Let $\mathbf{B}$ be defined as in \eqref{block-matrix-B}. Then problem \eqref{optimization_problem_under_Bj}
with $\lambda_j>0, j\in \mathbb{N}_d,$ has a solution $\mathbf{u}^{*}\in\mathbb{R}^n$ with for each $j\in \mathbb{N}_d,$ $\mathbf{B}_{j}\mathbf{u}^{*}=\sum_{i\in\mathbb{N}_{l_j}}z_{p_{j-1}+k_i}^*\mathbf{e}_{m_j,k_i}\in \Omega_{m_j,l_j},$ for some $l_{j}\in\mathbb{Z}_{m_j+1}$ if and only if there exist  $\mathbf{a}\in\partial\bm{\psi}(\mathbf{u}^*)$ and $\mathbf{b}:=[b_j:j\in\mathbb{N}_{p_d}]\in\mathcal{N}(\mathbf{B}^{\top})$ such that \eqref{sparsity-equi-mini-1}, \eqref{sparsity-equi-mini-2} and \eqref{sparsity-equi-mini-3} hold. 
\end{theorem}
\begin{proof}    
It follows from Lemma \ref{equi_mini} that $\mathbf{u}^{*}\in\mathbb{R}^n$ with for each $j\in \mathbb{N}_d,$ $\mathbf{B}_{j}\mathbf{u}^{*}=\sum_{i\in\mathbb{N}_{l_j}}z_{p_{j-1}+k_i}^*\mathbf{e}_{m_j,k_i}\in \Omega_{m_j,l_j}$ is a solution of problem \eqref{optimization_problem_under_Bj} if and only if $\mathcal{B}\mathbf{u}^*:=
\scriptsize{\begin{bmatrix}
\mathbf{z}^*\\
\mathbf{v}^*\end{bmatrix}}$ with for each $j\in \mathbb{N}_d,$ $\mathbf{z}_j^{*}=\sum_{i\in\mathbb{N}_{l_j}}z_{p_{j-1}+k_i}^*\mathbf{e}_{m_j,k_i}\in \Omega_{m_j,l_j},$ is a solution of problem \eqref{optimization_problem_under_Bj_3}. The latter guaranteed by Lemma \ref{sparsity-equi-mini} is equivalent to that there exist $\mathbf{a}\in\partial\bm{\psi}(\mathbf{B}'\mathcal{B}\mathbf{u}^*)$ and $\mathbf{b}\in\mathcal{N}(\mathbf{B}^{\top})$ such that \eqref{sparsity-equi-mini-1}, \eqref{sparsity-equi-mini-2} and \eqref{sparsity-equi-mini-3} hold. It suffices to show that $\mathbf{a}\in\partial\bm{\psi}(\mathbf{u}^*)$. 
This is done by noting that $\mathbf{B}'\mathcal{B}\mathbf{u}^*=\mathbf{u}^*$. 
\end{proof}

Theorem \ref{sparsity_original}  provides
a characterization of the multiple regularization parameter $\lambda_j$, $j\in\mathbb{N}_d$, with which problem \eqref{optimization_problem_under_Bj} has a solution with
sparsity of a certain level $l_j$ under each transform matrix $\mathbf{B}_{j}$. By specifying the sparsity level of the solution under each transform matrix $\mathbf{B}_{j}$ to be $l_j^*$, our goal is
to find regularization parameters $\lambda_j^*$, $j\in\mathbb{N}_d$, satisfying
conditions \eqref{sparsity-equi-mini-1}, \eqref{sparsity-equi-mini-2} and \eqref{sparsity-equi-mini-3}. However, since these conditions depend on the corresponding
solution, the characterization stated in Theorem \ref{sparsity_original} can not be used directly as a multi-parameter
choice strategy. Motivated by Theorem \ref{sparsity_original}, an iterative scheme to be developed in section 5 will enable us to choose multiple regularization parameters with which a minimizer of problem \eqref{optimization_problem_under_Bj} has a prescribed sparsity level under each transform matrix.


The next result concerns the special case that matrix $\mathbf{B}$ defined by \eqref{block-matrix-B} has full row rank, that is, $\mathrm{rank}(\mathbf{B})=p_d$.
\begin{corollary}\label{sparsity-rank}
    Suppose that $\bm{\psi}:\mathbb{R}^n\to\mathbb{R}_+$ is a convex function  and for each $j\in\mathbb{N}_d$, $\mathbf{B}_{j}$ is an $m_j\times n$ matrix. If $\mathbf{B}$ defined by \eqref{block-matrix-B} satisfies $\mathrm{rank}(\mathbf{B})=p_d$, then problem \eqref{optimization_problem_under_Bj} with $\lambda_j>0, j\in \mathbb{N}_d,$ has a solution $\mathbf{u}^{*}\in\mathbb{R}^n$ with for each $j\in \mathbb{N}_d,$ $\mathbf{B}_{j}\mathbf{u}^{*}=\sum_{i\in\mathbb{N}_{l_j}}z_{p_{j-1}+k_i}^*\mathbf{e}_{m_j,k_i}\in \Omega_{m_j,l_j},$ for some $l_{j}\in\mathbb{Z}_{m_j+1}$ if and only if there exists $\mathbf{a}\in\partial\bm{\psi}(\mathbf{u}^*)$ such that 
\begin{equation}\label{sparsity-equi-mini-1-rank}
(\mathbf{B}'_{(i)})^{\top}\mathbf{a}={0}, \ i\in\mathbb{N}_{n}\setminus\mathbb{N}_{p_d}, 
\end{equation}
and for each $j\in\mathbb{N}_d$
\begin{equation}\label{sparsity-equi-mini-2-rank}
\lambda_j=-(\mathbf{B}_{(p_{j-1}+k_i)}')^\top\mathbf{a}\mathrm{sign}(z_{p_{j-1}+k_i}^*),\  i\in \mathbb{N}_{l_j},
\end{equation}
\begin{equation}\label{sparsity-equi-mini-3-rank}
\lambda_j\geq\left|(\mathbf{B}_{(p_{j-1}+i)}')^\top\mathbf{a}\right|, \ i\in \mathbb{N}_{m_j}\setminus\{k_i:i\in \mathbb{N}_{l_j}\}.
\end{equation}
\end{corollary}
\begin{proof}
Theorem \ref{sparsity_original} ensures that $\mathbf{u}^{*}\in\mathbb{R}^n$ with for each $j\in \mathbb{N}_d,$ $\mathbf{B}_{j}\mathbf{u}^{*}=\sum_{i\in\mathbb{N}_{l_j}}z_{p_{j-1}+k_i}^*\mathbf{e}_{m_j,k_i}\in \Omega_{m_j,l_j}$ is a solution of problem \eqref{optimization_problem_under_Bj} if and only if there exist  $\mathbf{a}\in\partial\bm{\psi}(\mathbf{u}^*)$ and $\mathbf{b}\in\mathcal{N}(\mathbf{B}^{\top})$ such that \eqref{sparsity-equi-mini-1}, \eqref{sparsity-equi-mini-2} and \eqref{sparsity-equi-mini-3} hold. By the assumption that $\mathrm{rank}(\mathbf{B})=p_d$, we rewrite equation \eqref{sparsity-equi-mini-1} as equation \eqref{sparsity-equi-mini-1-rank}. It follows from $\mathrm{rank}(\mathbf{B})=p_d$ that $\mathcal{N}(\mathbf{B}^\top)=\left(\mathcal{R}(\mathbf{B})\right)^\bot=\{\mathbf{0}\}$. Then vector $\mathbf{b}$ in \eqref{sparsity-equi-mini-2} and \eqref{sparsity-equi-mini-3} is the zero vector. Thus, \eqref{sparsity-equi-mini-2-rank} and \eqref{sparsity-equi-mini-3-rank} can be obtained directly.
\end{proof}

Note that if $\bm{\psi}$ is differentiable, then the subdifferential of  $\bm{\psi}$ at $\mathbf{u}^*$
is the singleton $\nabla\bm{\psi}(\mathbf{u}^*)$. In this case, Theorem \ref{sparsity_original} and Corollary \ref{sparsity-rank} have the following simple form.

\begin{corollary}\label{sparsity-diff-rank}
Suppose that $\bm{\psi}:\mathbb{R}^n\to\mathbb{R}_+$ is a differentiable and convex function and for each $j\in\mathbb{N}_d$, $\mathbf{B}_{j}$ is an $m_j\times n$ matrix. Let $\mathbf{B}$ be defined as in \eqref{block-matrix-B}. Then problem \eqref{optimization_problem_under_Bj} with $\lambda_j>0, j\in \mathbb{N}_d,$ has a solution $\mathbf{u}^{*}\in\mathbb{R}^n$ with for each $j\in \mathbb{N}_d,$ $\mathbf{B}_{j}\mathbf{u}^{*}=\sum_{i\in\mathbb{N}_{l_j}}z_{p_{j-1}+k_i}^*\mathbf{e}_{m_j,k_i}\in \Omega_{m_j,l_j},$ for some $l_{j}\in\mathbb{Z}_{m_j+1}$ if and only if there exist $\mathbf{b}:=[b_j:j\in\mathbb{N}_{p_d}]\in\mathcal{N}(\mathbf{B}^{\top})$ such that     \begin{equation*}\label{sparsity-equi-mini-1-diff}
(\mathbf{B}'_{(i)})^{\top}\nabla\bm{\psi}(\mathbf{u}^*)={0}, \ i\in\mathbb{N}_{p_d+n-r}\setminus\mathbb{N}_{p_d}, 
\end{equation*}
and for each $j\in\mathbb{N}_d$
\begin{equation*}\label{sparsity-equi-mini-2-diff}
\lambda_j=-\left( (\mathbf{B}_{(p_{j-1}+k_i)}')^\top\nabla\bm{\psi}(\mathbf{u}^*)+b_{p_{j-1}+k_i}\right)\mathrm{sign}(z_{p_{j-1}+k_i}^*),\  i\in \mathbb{N}_{l_j},
\end{equation*}
\begin{equation*}\label{sparsity-equi-mini-3-diff}
\lambda_j\geq\left|(\mathbf{B}_{(p_{j-1}+i)}')^\top\nabla\bm{\psi}(\mathbf{u}^*)+b_{p_{j-1}+i}\right|, \ i\in \mathbb{N}_{m_j}\setminus\{k_i:i\in \mathbb{N}_{l_j}\}.
\end{equation*}
In particular, if $\mathrm{rank}(\mathbf{B})=p_d$, then the conditions reduce to
\begin{equation*}\label{sparsity-equi-mini-1-diff-rank}
(\mathbf{B}'_{(i)})^{\top}\nabla\bm{\psi}(\mathbf{u}^*)={0}, \ i\in\mathbb{N}_{n}\setminus\mathbb{N}_{p_d}, 
\end{equation*}
and for each $j\in\mathbb{N}_d$
\begin{equation*}\label{sparsity-equi-mini-2-diff-rank}
\lambda_j=-(\mathbf{B}_{(p_{j-1}+k_i)}')^\top\nabla\bm{\psi}(\mathbf{u}^*)\mathrm{sign}(z_{p_{j-1}+k_i}^*),\  k_i\in \mathbb{N}_{l_j},
\end{equation*}
\begin{equation*}\label{sparsity-equi-mini-3-diff-rank}
\lambda_j\geq\left|(\mathbf{B}_{(p_{j-1}+i)}')^\top\nabla\bm{\psi}(\mathbf{u}^*)\right|, \ i\in \mathbb{N}_{m_j}\setminus\{k_i:i\in \mathbb{N}_{l_j}\}.
\end{equation*}
\end{corollary}

%%%%%%%%%%%%%%%%%%%%%%%
\section{A special model with degenerated transform matrices}
In this section, we consider the special case where $p_d=n$ and for each $j\in \mathbb{N}_d,$ the transform matrix $\mathbf{B}_{j}$ takes the form
\begin{equation}\label{B-j-special}\mathbf{B}_{j}:=[\mathbf{0}_{m_j\times p_{j-1}}\  \mathbf{I}_{m_j}\ \mathbf{0}_{m_j\times (n-p_j)}]\in\mathbb{R}^{m_j\times n}.
\end{equation}
In this scenario, the multi-parameter regularization problem \eqref{optimization_problem_under_Bj} assumes the special form
\begin{equation}\label{optimization_problem}
\min
\left\{\bm{\psi}(\mathbf{u})+\sum_{j\in\mathbb{N}_d}\lambda_j\|\mathbf{u}_j\|_{1}:\mathbf{u}\in\mathbb{R}^n\right\}.
\end{equation}
We specialize the characterizations of the sparsity of regularized solutions established in the previous section to this special case. Moreover, particular attention is given to scenarios where the fidelity term $\bm{\psi}$ is block-separable. 

We first characterize the sparsity of the solution of problem \eqref{optimization_problem}. It follows from equation \eqref{B-j-special} that matrix $\mathbf{B}$ defined by \eqref{block-matrix-B} coincides with the identity matrix of order $n$. Given that matrix $\mathbf{B}$ has full row rank, we specialize Corollary \ref{sparsity-rank} to the regularization problem \eqref{optimization_problem}. In addition, the transform matrices $\mathbf{B}_{j}$, $j\in\mathbb{N}_d$, with the form \eqref{B-j-special} enable us to consider the sparsity of each sub-vector of the regularized solution separately. 

\begin{theorem}\label{sparsity-special-case}
Suppose that $\bm{\psi}:\mathbb{R}^n\to\mathbb{R}_+$ is a convex function. Then problem \eqref{optimization_problem}
with $\lambda_j>0$, $j\in \mathbb{N}_d$, has a solution $\mathbf{u}^{*}$ with for each $j\in \mathbb{N}_d$, $\mathbf{u}_j^{*}=\sum_{i\in\mathbb{N}_{l_j}}u^*_{p_{j-1}+k_i}\mathbf{e}_{m_j,k_i}\in \Omega_{m_j,l_j}$, for some $l_{j}\in\mathbb{Z}_{m_j+1}$ if and only if there exists $\mathbf{a}:=[a_j:j\in\mathbb{N}_{n}]\in\partial\bm{\psi}(\mathbf{u}^{*})$ such that for each $j\in \mathbb{N}_d$
\begin{equation}\label{sparsity-equi-mini-1-special}
\lambda_{j}=-a_{p_{j-1}+k_i}\mathrm{sign}(u^*_{p_{j-1}+k_i}), \
i\in\mathbb{N}_{l_j},
\end{equation}
\begin{equation}\label{sparsity-equi-mini-2-special}
\lambda_j\geq|a_{p_{j-1}+i}|,\ i\in \mathbb{N}_{m_j}\setminus\{k_i:i\in \mathbb{N}_{l_j}\}.
\end{equation}
In particular, if $\bm{\psi}$ is differentiable, then the conditions reduce to for each $j\in \mathbb{N}_d$
\begin{equation}\label{sparsity-equi-mini-1-special-diff}
\lambda_{j}=-\frac{\partial{\bm{\psi}}}{\partial{u_{p_{j-1}+k_i}}}(\mathbf{u}^{*})\mathrm{sign}(u^*_{p_{j-1}+k_i}), \
i\in\mathbb{N}_{l_j},
\end{equation}
\begin{equation}\label{sparsity-equi-mini-2-special-diff}
\lambda_j\geq\left|\frac{\partial{\bm{\psi}}}{\partial{u_{p_{j-1}+i}}}(\mathbf{u}^{*})\right|, \ i\in \mathbb{N}_{m_j}\setminus\{k_i:i\in \mathbb{N}_{l_j}\}.
\end{equation}
\end{theorem}
\begin{proof}
Since matrix $\mathbf{B}$ defined by equation \eqref{block-matrix-B} has full row rank, Corollary \ref{sparsity-rank} ensures that problem \eqref{optimization_problem}
with $\lambda_j>0$, $j\in \mathbb{N}_d$, has a solution $\mathbf{u}^{*}$ with for each $j\in \mathbb{N}_d$, $\mathbf{u}_j^{*}=\sum_{i\in\mathbb{N}_{l_j}}u^*_{p_{j-1}+k_i}\mathbf{e}_{m_j,k_i}\in \Omega_{m_j,l_j}$, for some $l_{j}\in\mathbb{Z}_{m_j+1}$ if and only if for each $j\in \mathbb{N}_d$ there exists $\mathbf{a}:=[a_j:j\in\mathbb{N}_{n}]\in\partial\bm{\psi}(\mathbf{u}^{*})$ such that conditions \eqref{sparsity-equi-mini-1-rank}, \eqref{sparsity-equi-mini-2-rank} and \eqref{sparsity-equi-mini-3-rank} hold. Note that index $i$ such that equation \eqref{sparsity-equi-mini-1-rank} holds belongs to an empty set since $p_d=n$. It is clear that matrix $\mathbf{B}'$ appearing in conditions \eqref{sparsity-equi-mini-2-rank} and \eqref{sparsity-equi-mini-3-rank} is also an identity matrix. Hence, conditions \eqref{sparsity-equi-mini-2-rank} and \eqref{sparsity-equi-mini-3-rank} reduce to \eqref{sparsity-equi-mini-1-special} and \eqref{sparsity-equi-mini-2-special}, respectively. If $\bm{\psi}$ is differentiable,
then the subdifferential of $\bm{\psi}$ at $\mathbf{u}^{*}$ is the singleton  $\nabla\bm{\psi}(\mathbf{u}^{*})$. Substituting $a_j=\frac{\partial{\bm{\psi}}}{\partial{u_{j}}}(\mathbf{u}^{*})$ into \eqref{sparsity-equi-mini-1-special} and \eqref{sparsity-equi-mini-2-special} lead directly to \eqref{sparsity-equi-mini-1-special-diff} and \eqref{sparsity-equi-mini-2-special-diff}, respectively.
\end{proof}

As a specific example, we consider the regularization problem
\begin{equation}\label{lasso_multi-parameter}
\min\left\{\frac{1}{2}\left\|\mathbf{A}\mathbf{u}-\mathbf{x}\right\|_2^2
+\sum_{j\in\mathbb{N}_d}\lambda_j\|\mathbf{u}_j\|_1:\mathbf{u}\in\mathbb{R}^{n}\right\}.
\end{equation}
In this model, the fidelity term \begin{equation}\label{fidelity_term}
\bm{\psi}(\mathbf{u}):=\frac{1}{2}\|\mathbf{A}\mathbf{u}-\mathbf{x}\|_2^2, \ \mathbf{u}\in\mathbb{R}^n,
\end{equation}
is convex and differentiable. As a result, we apply Theorem \ref{sparsity-special-case} to this model.

\begin{corollary}\label{sparsity-special-case-example}
Suppose that $\mathbf{x}\in\mathbb{R}^t$ and $\mathbf{A}\in\mathbb{R}^{t\times n}$ are given. Then the regularization problem \eqref{lasso_multi-parameter} with $\lambda_j>0,j\in\mathbb{N}_d$, has a solution $\mathbf{u}^{*}$ with for each $j\in\mathbb{N}_d$, $\mathbf{u}_j^{*}=\sum_{i\in\mathbb{N}_{l_j}}u_{p_{j-1}+k_i}^*\mathbf{e}_{m_j,k_i}\in \Omega_{m_j,l_j}$, for some $l_{j}\in\mathbb{Z}_{m_j+1}$ if and only if for each $j\in\mathbb{N}_d$ 
\begin{equation}\label{sparsity-equi-mini-1-special-diff-example}
\lambda_{j}=(\mathbf{A}_{(p_{j-1}+k_i)})^{\top}(\mathbf{x}-\mathbf{Au^*})\mathrm{sign}(u^{*}_{p_{j-1}+k_i}),\ i\in \mathbb{N}_{l_j}
\end{equation}
\begin{equation}\label{sparsity-equi-mini-2-special-diff-example}
\lambda_j\geq\big|(\mathbf{A}_{(p_{j-1}+i)})^{\top}(\mathbf{Au^*}-\mathbf{x})\big|,\  i\in \mathbb{N}_{m_j}\setminus\{k_i:i\in \mathbb{N}_{l_j}\}.
\end{equation}
\end{corollary}
\begin{proof}
Since the fidelity term $\bm{\psi}(\mathbf{u})$ defined by \eqref{fidelity_term} is convex and differentiable, Theorem \ref{sparsity-special-case} confirms 
that problem \eqref{lasso_multi-parameter} with $\lambda_j>0$, $j\in \mathbb{N}_d$, has a solution $\mathbf{u}^{*}$ with for each $j\in \mathbb{N}_d$, $\mathbf{u}_j^{*}=\sum_{i\in\mathbb{N}_{l_j}}u^*_{p_{j-1}+k_i}\mathbf{e}_{m_j,k_i}\in \Omega_{m_j,l_j}$, for some $l_{j}\in\mathbb{Z}_{m_j+1}$ if and only if \eqref{sparsity-equi-mini-1-special-diff} and \eqref{sparsity-equi-mini-2-special-diff} hold. 
Note that the gradient of $\bm{\psi}$ at $\mathbf{u}^*$ has the form $\nabla\bm{\psi}(\mathbf{u}^*)=\mathbf{A}^\top(\mathbf{A}\mathbf{u}^*-\mathbf{x})$. As a result, there holds for each $j\in\mathbb{N}_n$ and each $i\in\mathbb{N}_{m_j}$ 
$$
\frac{\partial{\bm{\psi}}}{\partial{u_{p_{j-1}+i}}}(\mathbf{u}^{*})=(\mathbf{A}_{(p_{j-1}+i)})^{\top}(\mathbf{Au^*}-\mathbf{x}).
$$
According to the above representations of the partial derivatives of $\bm{\psi}$, conditions \eqref{sparsity-equi-mini-1-special-diff} and \eqref{sparsity-equi-mini-2-special-diff} reduce to \eqref{sparsity-equi-mini-1-special-diff-example} and \eqref{sparsity-equi-mini-2-special-diff-example}, respectively.
  
\end{proof}

We next study the case that the fidelity term $\bm{\psi}$ involved in problem \eqref{optimization_problem} has special structure, that is, $\bm{\psi}$ is block separable. To describe the block separability of a function on $\mathbb{R}^s$, we introduce a partition of the index set $\mathbb{N}_s$. Let $q\in\mathbb{N}$ with $q\leq s$. We suppose that $\mathcal{S}_{s,q}:=\left\{S_{s,1},S_{s,2},\ldots, S_{s,q}\right\}$ is a partition of $\mathbb{N}_s$ in the sense that $S_{s,k}\neq\emptyset$, for all $k\in\mathbb{N}_q$, $S_{s,k}\cap S_{s,l}=\emptyset$ if $k\neq l$, and $\cup_{k\in\mathbb{N}_q}S_{s,k}=\mathbb{N}_s$. For each $k\in \mathbb{N}_q$ we denote by $s_k$ the cardinality of $S_{s,k}$ and regard $S_{s,k}$ as an {\it ordered set} in the natural order of the elements in $\mathbb{N}_s$. That is, 
$$
S_{s,k}:=\{i_{k,1}, \dots, i_{k,s_k}\}, \ \mbox{with}\  i_{k,l}\in \mathbb{N}_s, \ l\in \mathbb{N}_{s_k}\ \mbox{and}\ i_{k,1}<\dots<i_{k,s_k}.
$$
Associated with partition $\mathcal{S}_{s,q}$, we decompose $\mathbf{w}:=[w_k:k\in\mathbb{N}_s]\in\mathbb{R}^s$
into $q$ sub-vectors by setting 
$$
\mathbf{w}_{S_{s,k}}:=[w_{i_{k,1}}, \dots, w_{{i}_{k,s_k}}]\in\mathbb{R}^{s_k},\ k\in\mathbb{N}_q.
$$
A function  $\bm{\phi}:\mathbb{R}^s\rightarrow\mathbb{R}$ is called $\mathcal{S}_{s,q}$-block separable if there exist functions $\bm{\phi}_k:\mathbb{R}^{s_k}\rightarrow\mathbb{R}$, $k\in\mathbb{N}_q$ such that
$$
    \bm{\phi}(\mathbf{w})=\sum\limits_{k\in\mathbb{N}_q}\bm{\phi}_{k}(\mathbf{w}_{S_{s,k}}),\ \mathbf{w}\in\mathbb{R}^s.
$$

We now describe the block separablity of the fidelity term $\bm{\psi}$. Recall that $p_d=n$. If the partition  $\mathcal{S}_{n,d}:=\left\{S_{n,1},S_{n,2},\ldots, S_{n,d}\right\}$ for $\mathbb{N}_{n}$ is chosen with $S_{n,j}:=\{p_{j-1}+i:i\in\mathbb{N}_{m_j}\},\  j\in\mathbb{N}_d,$ then for each $j\in\mathbb{N}_d$ the sub-vector $\mathbf{u}_{S_{n,j}}$ of $\mathbf{u}\in\mathbb{R}^{n}$ coincides with $\mathbf{u}_j$. It is clear that the regularization term in problem \eqref{optimization_problem} is $\mathcal{S}_{n,d}$-block separable. We also assume that $\bm{\psi}$ is $\mathcal{S}_{n,d}$-block separable, that is, there exist functions $\bm{\psi}_{j}:\mathbb{R}^{m_j}\rightarrow\mathbb{R}_+$, $j\in\mathbb{N}_d$ such that 
\begin{equation}\label{block_separable_psi}
\bm{\psi}(\mathbf{u})=\sum\limits_{j\in\mathbb{N}_d}\bm{\psi}_{j}(\mathbf{u}_{j}),\ \mathbf{u}\in\mathbb{R}^n.
\end{equation} 
Combining the block separability of the fidelity term $\bm{\psi}$ and the norm function $\|\cdot\|_1$, the multi-parameter regularization problem \eqref{optimization_problem} can be
reduced to the following lower dimensional single-parameter regularization problems
\begin{equation}\label{optimization_problem-single-parameter}
\min
\left\{\bm{\psi}_{j}(\mathbf{u}_{j})+\lambda_j\|\mathbf{u}_{j}\|_{1}:\mathbf{u}_{j}\in\mathbb{R}^{m_{j}}\right\}, \ j\in\mathbb{N}_d.
\end{equation}
Note that the sparsity of the solution of each single parameter regularization problem \eqref{optimization_problem-single-parameter} was characterized in \cite{Liu2023parameter}. This characterization can also be derived from Theorem \ref{sparsity-special-case}.  
We further assume that functions $\bm{\psi}_{j}$, $j\in\mathbb{N}_d$ has block separability. For each $j\in\mathbb{N}_d$, let $\mathcal{S}_{m_j,q_j}:=\left\{S_{m_j,1},S_{m_j,2},\ldots, S_{m_j,q_j}\right\}$ be a partition of $\mathbb{N}_{m_j}$ and for each $k\in\mathbb{N}_{q_j}$, $s_{j,k}$ be the cardinality of $S_{m_j,k}$. For each $\mathbf{u}\in\mathbb{R}^n$, we set $\mathbf{u}_{j,k}:=(\mathbf{u}_j)_{S_{m_j,k}}$ for all $j\in\mathbb{N}_d$ and $k\in\mathbb{N}_{q_j}$. Suppose that for each $j\in\mathbb{N}_d$, $\bm{\psi}_j$ has the form
\begin{equation}\label{block_separable_psi-j}
 \bm{\psi}_j(\mathbf{u}_j)=\sum\limits_{k\in\mathbb{N}_{q_j}}\bm{\psi}_{j,k}(\mathbf{u}_{j,k}),\ \mathbf{u}_j\in\mathbb{R}^{m_j},
\end{equation} 
with $\bm{\psi}_{j,k}$ being functions from $\mathbb{R}^{s_{j,k}}$ to $\mathbb{R}_+$, $k\in\mathbb{N}_{q_j}$.  

We are ready to characterize the block sparsity of each sub-vector of the solution of problem \eqref{optimization_problem} when $\bm{\psi}$ has block separability described above. Here, we say that a vector $\mathbf{x}\in\mathbb{R}^s$ has $\mathcal{S}_{s,q}$-block
sparsity of level $l\in\mathbb{Z}_{q+1}$ if $\mathbf{x}$ has exactly $l$ number of nonzero sub-vectors with respect to partition $\mathcal{S}_{s,q}$. 
\begin{theorem}\label{sparsity-special-case-block}
Suppose that for each $j\in\mathbb{N}_d$ and each $k\in\mathbb{N}_{q_j}$, $\bm{\psi}_{j,k}:\mathbb{R}^{s_{j,k}}\to\mathbb{R}_+$ is a convex function and $\bm{\psi}_{j}$ is an $\mathcal{S}_{m_j,q_j}$-block separable function having the form \eqref{block_separable_psi-j}. Let $\bm{\psi}$ be the function with the form \eqref{block_separable_psi}. Then problem \eqref{optimization_problem}
with $\lambda_j>0$, $j\in \mathbb{N}_d$, has a solution $\mathbf{u}^{*}$ with for each $j\in \mathbb{N}_d$, $\mathbf{u}_j^{*}$ having the $\mathcal{S}_{m_j,q_j}$-block sparsity of level $l'_j\leq l_j$ for some $l_j\in\mathbb{Z}_{q_j+1}$ if and only if for each $j\in \mathbb{N}_d$ there exist distinct
$k_{j,i}\in\mathbb{N}_{q_j}$, $i\in\mathbb{N}_{l_j}$, such that
\begin{equation}\label{sparsity-equi-mini-special-block}
\lambda_j\geq\mathrm{min}\left\{\|\mathbf{y}\|_{\infty}:\mathbf{y}\in\partial\bm{\psi}_{j,k}(\mathbf{0}) \right\}, \ \mbox{for all}\ k\in\mathbb{N}_{q_j}\setminus{\{k_{j,i}:i\in\mathbb{N}_{l_j}\}}.
\end{equation}
In particular, if $\bm{\psi}_{j,k}$, $j\in\mathbb{N}_d$, $k\in\mathbb{N}_{q_j}$ are differentiable, then
condition \eqref{sparsity-equi-mini-special-block} reduces to
\begin{equation}\label{sparsity-equi-mini-special-block-diff}
    \lambda_j\geq\|\nabla\bm{\psi}_{j,k}(\mathbf{0})\|_\infty,\ \ \mbox{for all}\ \ k\in\mathbb{N}_{q_j}\setminus{\{k_{j,i}:i\in\mathbb{N}_{l_j}\}}.
\end{equation}
\end{theorem}
\begin{proof}
   Observing from the block separability of functions $\bm{\psi}$ and  $\|\cdot\|_1$, we conclude that $\mathbf{u}^*\in\mathbb{R}^n$ is a solution of problem \eqref{optimization_problem} if and only if for each $j\in\mathbb{N}_d$, $\mathbf{u}^*_j\in\mathbb{R}^{m_j}$ is a solution of problem
 \eqref{optimization_problem-single-parameter}. Theorem 3.2 in \cite{Liu2023parameter} ensures that for each $j\in\mathbb{N}_d$, $\mathbf{u}^*_j\in\mathbb{R}^{m_j}$ is a solution of problem
 \eqref{optimization_problem-single-parameter} and has the $\mathcal{S}_{m_j,q_j}$-block sparsity of level $l'_j\leq l_j$ for some $l_j\in\mathbb{Z}_{q_j+1}$ if and only if there exist distinct
$k_{j,i}\in\mathbb{N}_{q_j}$, $i\in\mathbb{N}_{l_j}$, such that
\eqref{sparsity-equi-mini-special-block} holds. For the case that $\bm{\psi}_{j,k}$, $j\in\mathbb{N}_d$, $k\in\mathbb{N}_{q_j}$ are all differentiable, it suffices to notice that the subdifferential of $\bm{\psi}_{j,k}$ at zero are the singleton $\nabla\bm{\psi}_{j,k}(\mathbf{0})$. 
This together with inequality \eqref{sparsity-equi-mini-special-block} leads to inequality 
 \eqref{sparsity-equi-mini-special-block-diff}.
\end{proof}

Unlike in Theorems \ref{sparsity_original} 
 and \ref{sparsity-special-case}, the characterization stated in Theorem \ref{sparsity-special-case-block} can be taken as a multi-parameter choice strategy. That is, when the fidelity
term is block separable, if for each $j\in\mathbb{N}_d$, the regularization parameter
$\lambda_j$ is chosen so that inequality \eqref{sparsity-equi-mini-special-block} (or \eqref{sparsity-equi-mini-special-block-diff}) holds, then the regularization problem  \eqref{optimization_problem}
has a solution with each sub-vector having a block sparsity of a prescribed level. The choice of the parameters depends on the subdifferentials
or the gradients of the functions $\bm{\psi}_{j,k}$, $j\in\mathbb{N}_d$, $k\in\mathbb{N}_{q_j}$. 

We also specialize Theorem \ref{sparsity-special-case-block} to the regularization problem
\eqref{lasso_multi-parameter}. For this purpose, we require that the fidelity
term $\bm{\psi}$ defined by \eqref{fidelity_term}
is block separable. Associated with the partition  $\mathcal{S}_{n,d}:=\left\{S_{n,1},S_{n,2},\ldots, S_{n,d}\right\}$ for $\mathbb{N}_{n}$ with $S_{n,j}:=\{p_{j-1}+i:i\in\mathbb{N}_{m_j}\}$,  $j\in\mathbb{N}_d,$ we decompose matrix $\mathbf{A}\in \mathbb{R}^{t\times n}$ into $d$ sub-matrices by setting 
$$
\mathbf{A}_{[j]}:=[\mathbf{A}_{(i)}:i\in S_{n,j}]\in\mathbb{R}^{t\times m_j},\ j\in\mathbb{N}_d.
$$
By lemma 3.4 of \cite{Liu2023parameter},
the fidelity term $\bm{\psi}$ defined by \eqref{fidelity_term}
is $\mathcal{S}_{n,d}$-block separable if and only if there holds 
\begin{equation}\label{S_nd_block_separable}
(\mathbf{A}_{[j]})^{\top}\mathbf{A}_{[k]}=\mathbf{0},\ \mbox{for all}\ j,k\in\mathbb{N}_d \ \mbox{and}\ j\neq{k}.
\end{equation}
It follows from the decomposition of $\mathbf{A}$ and that of each vector $\mathbf{u}$ in $\mathbb{R}^n$ with respect to $\mathcal{S}_{n,d}$ that $\mathbf{A}\mathbf{u}=\sum_{j\in\mathbb{N}_d}\mathbf{A}_{[j]}\mathbf{u}_j$, for all $\mathbf{u}\in\mathbb{R}^n$.
According to this equation and condition \eqref{S_nd_block_separable}, we represent  $\bm{\psi}$ defined by \eqref{fidelity_term} as in \eqref{block_separable_psi} with $\bm{\psi}_{j}$,
$j\in\mathbb{N}_d$, being defined by
\begin{equation}\label{psi_j_u_j}
\bm{\psi}_{j}(\mathbf{u}_{j})
:=\frac{1}{2}\|\mathbf{A}_{[j]}
\mathbf{u}_j\|_2^2-\mathbf{x}^{\top}\mathbf{A}_{[j]}
\mathbf{u}_j+\frac{1}{2d}\mathbf{x}^{\top}
\mathbf{x},
\  \mathbf{u}_j\in\mathbb{R}^{m_j}.
\end{equation}
To describe the block separability of functions $\bm{\psi}_{j}$, $j\in\mathbb{N}_d$, we recall that for each $j\in\mathbb{N}_d$, $\mathcal{S}_{m_j,q_j}:=\left\{S_{m_j,1},S_{m_j,2},\ldots, S_{m_j,q_j}\right\}$ is a partition of $\mathbb{N}_{m_j}$. 
Associated with the partition $\mathcal{S}_{m_j,q_j}$, matrix $\mathbf{A}_{[j]}$ can be decomposed into $q_j$ sub-matrices by setting 
$$
\mathbf{A}_{[j,k]}:=[(\mathbf{A}_{[j]})_{(i)}:i\in S_{m_j,k}]\in\mathbb{R}^{t\times s_{j,k}},\ k\in\mathbb{N}_{q_j}.
$$
It is clear that the last two terms in the right hand side of equation \eqref{psi_j_u_j} are both $\mathcal{S}_{m_j,q_j}$-block separable.
Hence, again by lemma 3.4 of \cite{Liu2023parameter}, we conclude that the functions $\bm{\psi}_{j}$ with the form \eqref{psi_j_u_j} is $\mathcal{S}_{m_j,q_j}$-block separable if and only if there holds 
\begin{equation}\label{S_mj_qj_block_separable}
(\mathbf{A}_{[j,k]})^{\top}\mathbf{A}_{[j,l]}=\mathbf{0},\ \mbox{for all}\ k,l\in\mathbb{N}_{q_j} \ \mbox{and}\ k\neq{l}.
\end{equation}
We represent $\bm{\psi}_{j}$, $j\in\mathbb{N}_d$, as in \eqref{block_separable_psi-j} when condition \eqref {S_mj_qj_block_separable} holds. For each $j\in\mathbb{N}_d$, the decomposition of $\mathbf{A}_{[j]}$ and that of each vector $\mathbf{u}_j$ in $\mathbb{R}^{m_j}$ with respect to $\mathcal{S}_{m_j,q_j}$ lead to $\mathbf{A}_{[j]}\mathbf{u}_j=\sum_{k\in\mathbb{N}_{q_j}}\mathbf{A}_{[j,k]}\mathbf{u}_{j,k}$, for all $\mathbf{u}_j\in\mathbb{R}^{m_j}$.
Substituting the above equation into definition \eqref{psi_j_u_j} with noting that condition \eqref {S_mj_qj_block_separable} holds, we represent $\bm{\psi}_{j}$ as in \eqref{block_separable_psi-j} with $\bm{\psi}_{j,k}$,
$k\in\mathbb{N}_{q_j}$, having the form
\begin{equation}\label{psi_jk_u_jk}
\bm{\psi}_{j,k}(\mathbf{u}_{j,k})
:=\frac{1}{2}\|\mathbf{A}_{[j,k]}
\mathbf{u}_{j,k}\|_2^2-\mathbf{x}^{\top}\mathbf{A}_{[j,k]}
\mathbf{u}_{j,k}+\frac{1}{2dq_j}\mathbf{x}^{\top}
\mathbf{x},
\  \mathbf{u}_{j,k}\in\mathbb{R}^{s_{j,k}}.
\end{equation}

We now apply Theorem \ref{sparsity-special-case-block} to the regularization problem \eqref{lasso_multi-parameter} when the matrix $\mathbf{A}$ satisfies conditions \eqref{S_nd_block_separable} and \eqref{S_mj_qj_block_separable}.
\begin{corollary}\label{block_separabel_parameter_choice}
Suppose that $\mathbf{x}\in\mathbb{R}^{t}$ and $\mathbf{A}\in\mathbb{R}^{t\times n}$ satisfies conditions \eqref{S_nd_block_separable} and \eqref{S_mj_qj_block_separable}. Then the regularization problem \eqref{lasso_multi-parameter} with $\lambda_j>0$, $j\in\mathbb{N}_{d}$, has a solution $\mathbf{u}^*$ with for each $j\in\mathbb{N}_{d}$, $\mathbf{u}^{*}_j$ having the $\mathcal{S}_{m_j,q_j}$-block sparsity of level $l'_j\leq l_j$ for some $l_j\in \mathbb{Z}_{q_j+1}$
if and only if for each $j\in\mathbb{N}_d$, there exist distinct $k_{j,i}\in\mathbb{N}_{q_j}$, $i\in\mathbb{N}_{l_j}$, such that $\lambda_j\geq\big\|(\mathbf{A}_{[j,k]})^{\top}\mathbf{x}\big\|_{\infty}$, for all $k\in\mathbb{N}_{q_j}\setminus{\{k_{j,i}:i\in\mathbb{N}_{l_j}\}}$.
\end{corollary}
\begin{proof}
As pointed out before, condition \eqref{S_nd_block_separable} ensures that the fidelity term $\bm{\psi}$ defined by \eqref{fidelity_term} is $\mathcal{S}_{n,d}$-block separable and has the form  \eqref{block_separable_psi} with $\bm{\psi}_{j}$,
$j\in\mathbb{N}_d$, being defined by
\eqref{psi_j_u_j}. Moreover, condition \eqref{S_mj_qj_block_separable} guarantees that for each $j\in\mathbb{N}_d$, the function $\bm{\psi}_{j}$ is $\mathcal{S}_{m_j,q_j}$-block separable and can be represented as in \eqref{block_separable_psi-j} with $\bm{\psi}_{j,k}$,
$k\in\mathbb{N}_{q_j}$, having the form \eqref{psi_jk_u_jk}. Clearly, $\bm{\psi}_{j,k},$ $j\in\mathbb{N}_d$, $k\in\mathbb{N}_{q_j}$, are all convex and differentiable functions.
Consequently, we conclude by Theorem \ref{sparsity-special-case-block} that the regularization problem \eqref{lasso_multi-parameter} with $\lambda_j>0$, $j\in\mathbb{N}_{d}$, has a solution $\mathbf{u}^*$ with for each $j\in\mathbb{N}_{d}$, $\mathbf{u}^{*}_j$ having the $\mathcal{S}_{m_j,q_j}$-block sparsity of level $l'_j\leq l_j$ for some $l_j\in \mathbb{Z}_{q_j+1}$
if and only if for each $j\in\mathbb{N}_d$ there exist distinct $k_{j,i}\in\mathbb{N}_{q_j}$, $i\in\mathbb{N}_{l_j}$, such that inequality \eqref{sparsity-equi-mini-special-block-diff} holds. Note that for each $j\in\mathbb{N}_d$, $\nabla\bm{\psi}_{j,k}(\mathbf{0})=-(\mathbf{A}_{[j,k]})^{\top}\mathbf{x}$ for all $k\in\mathbb{N}_{q_j}$. Substituting this equation into inequality \eqref{sparsity-equi-mini-special-block-diff} leads directly to the desired inequality.
\end{proof}  
% We next consider the multi-parameter regularized problem \eqref{optimization_problem}  with $d=n$.

% \begin{theorem}\label{choice_sparsity_general_2}
% Suppose that $\bm{\psi}:\mathbb{R}^n\to\mathbb{R}_+:=[0,+\infty)$ is a convex function. Then problem 
% \begin{equation}\label{optimization_problem_n}
% \min
% \left\{\bm{\psi}(\mathbf{u})+\sum_{j\in\mathbb{N}_n}\lambda_j|u_j|:\mathbf{u}\in\mathbb{R}^n,{u}_j\in\mathbb{R}\right\}
% \end{equation}
% with $\lambda_j>0, j\in \mathbb{N}_n,$ has a solution  $\mathbf{u}^{*}=\sum_{i\in\mathbb{K}_{n,l}}u_{i}^*\mathbf{e}_{n,i}\in \Omega_{n,l}$ for some $l\in\mathbb{Z}_{n+1}$ if and only if there exists $\mathbf{a}:=[a_j:j\in\mathbb{N}_n]\in\partial\bm{\psi}
% (\mathbf{u}^{*})$ such that
% \begin{equation}\label{lambda_general_nonsmooth2}
% \lambda_{j}=-a_{j}\mathrm{sign}({u}_{j}^{*}), \
% j\in\mathbb{K}_{n,l} \ \ \mbox{and}\ \ \lambda_j\geq|a_j|,\ j\in \mathbb{N}_n\setminus\mathbb{K}_{n,l}.
% \end{equation}
% In particular, if $\bm{\psi}$ is a differentiable, then condition \eqref{lambda_general_nonsmooth2} is equivalent to
% \begin{equation}\label{lambda_general_smooth2}
% \lambda_{j}=-\bm{\psi}'_{j}(\mathbf{u}^{*})\mathrm{sign}({u}_{j}^{*}), \
% j\in \mathbb{K}_{n,l} \ \ \mbox{and}\ \ \lambda_j\geq|\bm{\psi}'_j(\mathbf{u}^{*})|, \ j\in \mathbb{N}_n\setminus\mathbb{K}_{n,l}.
% \end{equation}
% \end{theorem}

% We now apply Theorem \ref{choice_sparsity_general_2} to the following regularized model 
% \begin{equation}\label{lasso_transform}
% \min\Big\{\frac{1}{2}\left\|\mathbf{A}\mathbf{u}-\mathbf{x}\right\|_2^2
% +\sum_{j\in\mathbb{N}_n}\lambda_j|{u}_j|:\mathbf{u}\in\mathbb{R}^{n},{u}_j\in\mathbb{R}\Big\}.
% \end{equation}
% Here, the fidelity term $\bm{\psi}(\mathbf{u}):=\frac{1}{2}\left\|\mathbf{A}\mathbf{u}-\mathbf{x}\right\|_2^2$ is differentiable but neither additively separable nor block separable.

% \begin{corollary}\label{general-smooth-example1}
% Suppose that $\mathbf{x}\in\mathbb{R}^p$  and $\mathbf{A}\in\mathbb{R}^{p\times n}$ are given. Then the regularized model \eqref{lasso_transform} with $\lambda_j>0,j\in\mathbb{N}_n$ has a solution $\mathbf{u}^{*}=\sum_{i\in\mathbb{K}_{n,l}}u_{i}^*\mathbf{e}_{n,i}\in \Omega_{n,l}$ for some $l\in\mathbb{Z}_{n+1}$ if and only if there hold
% $\lambda_{j}=(\mathbf{A}_{j})^{\top}(\mathbf{x}-\mathbf{Au^*})\mathrm{sign}({u}_{j}^{*})$, for all $j\in\mathbb{K}_{n,l}$ and $
% \lambda_j\geq\big|(\mathbf{A}_j)^{\top}(\mathbf{Au^*}-\mathbf{x})\big|$, for all $j\in \mathbb{N}_n\setminus\mathbb{K}_{n,l}.$
% \end{corollary}

% As a special case of corollary \ref{general-smooth-example1}, the regularized model \eqref{lasso_transform} has $\mathbf{u}^*=\mathbf{0}$ as a solution if and only if there holds $\lambda_j\geq |(\mathbf{A}_j)^{\top}\mathbf{x}|$ for all $j\in \mathbb{N}_n$.

\section{Iterative schemes for parameter choices}

Theorem \ref{sparsity_original} characterizes the influence of each regularization parameter $\lambda_j$ on the sparsity of the solution to problem \eqref{optimization_problem_under_Bj} under the transform matrix  $\mathbf{B}_{j}$.
Based on this characterization, we develop iterative schemes in this section for selecting multiple regularization parameters that achieve prescribed sparsity levels in the solution of problem \eqref{optimization_problem_under_Bj} under different transform matrices. We consider two cases: when the fidelity term term $\bm{\psi}$  is differentiable and the transform matrix  $\mathbf{B}$  has full row rank, as well as when  $\bm{\psi}$ is non-differentiable and  $\mathbf{B}$  does not have full row rank.


Theorem \ref{sparsity_original} shows that if $\mathbf{u}^*\in\mathbb{R}^n$ is a solution of problem \eqref{optimization_problem_under_Bj} with $\lambda_j^*>0,$ $j\in \mathbb{N}_d,$ and for each $j\in\mathbb{N}_d$, $\mathbf{u}^*$ has sparsity of level $l_j^*\in\mathbb{Z}_{m_j+1}$ under $\mathbf{B}_{j}$, then there exist  $\mathbf{a}\in\partial\bm{\psi}(\mathbf{u}^*)$ and $\mathbf{b}:=[b_j:j\in\mathbb{N}_{p_d}]\in\mathcal{N}(\mathbf{B}^{\top})$ such that for each $j\in\mathbb{N}_d$, $\lambda_j^*$ satisfies conditions \eqref{sparsity-equi-mini-2} and  \eqref{sparsity-equi-mini-3}. According to these conditions, we introduce for each $j\in\mathbb{N}_d$, a sequence $\gamma_{j,i}(\mathbf{u}^{*}),$ $i\in\mathbb{N}_{m_j}$ by  
\begin{equation}\label{rji}
\gamma_{j,i}(\mathbf{u}^{*}):=\left|(\mathbf{B}_{(p_{j-1}+i)}')^\top\mathbf{a}+b_{p_{j-1}+i}\right|, \ i\in\mathbb{N}_{m_j},
\end{equation}
and rearrange them in a nondecreasing order: 
\begin{equation}\label{nondecreasing order}
\gamma_{j, i_1}(\mathbf{u}^{*})\leq \gamma_{j, i_2}(\mathbf{u}^{*})\leq 
\cdots \leq \gamma_{j, i_{m_j}}(\mathbf{u}^{*}), \ \mbox{with}\ \{i_1,  i_2, \ldots, i_{m_j}\}=\mathbb{N}_{m_j}.
\end{equation}
The equality \eqref{sparsity-equi-mini-2} and the inequality \eqref{sparsity-equi-mini-3} that the parameter $\lambda_j^*$ needs to satisfy corresponds the non-zero components
and the zero components of $\mathbf{B}_{j}\mathbf{u}^*$,  respectively. Thus, if $\lambda_j^*>\gamma_{j, i}(\mathbf{u}^{*})$, then $(\mathbf{B}_{j}\mathbf{u}^*)_{i}$ must be zero and if $\lambda_j^*=\gamma_{j, i}(\mathbf{u}^{*})$, then $(\mathbf{B}_{j}\mathbf{u}^*)_{i}$ may be zero or nonzero. With the help of the observation above, we present the following result. 
% \begin{proposition}\label{necessary}
% Suppose that $\bm{\psi}:\mathbb{R}^n\to\mathbb{R}_+$ is a convex function  and for each $j\in\mathbb{N}_d$, $\mathbf{B}_{(j)}$ is an $m_j\times n$ matrix. Let $\mathbf{B}$ be defined by \eqref{block-matrix-B}. If $\mathbf{u}^*$ is a solution of problem \eqref{optimization_problem_under_Bj} with $\lambda_j^*>0,$ $j\in \mathbb{N}_d,$ and for each $j\in\mathbb{N}_d$, $\mathbf{u}^*$ has sparsity of level $l_j^*\in\mathbb{Z}_{m_j+1}$ under $\mathbf{B}_{(j)}$,  then there exist  $\mathbf{a}\in\partial\bm{\psi}(\mathbf{u}^*)$ and $\mathbf{b}:=[b_j:j\in\mathbb{N}_{p_d}]\in\mathcal{N}(\mathbf{B}^{\top})$ such that for each $j\in\mathbb{N}_d$, $\lambda_j^*$ satisfies
% inequality \eqref{Order}.
% \end{proposition}
 
% \begin{proposition}\label{necessary—new}
% Let $\bm{\psi}:\mathbb{R}^n\to\mathbb{R}_+$ be a convex function, for each $j\in\mathbb{N}_d$, $\mathbf{B}_{(j)}$ be an $m_j\times n$ matrix and  $\mathbf{B}$ be defined by  \eqref{block-matrix-B}. Suppose that $\mathbf{u}^*$ is a solution of problem \eqref{optimization_problem_under_Bj} with $\lambda_j^*>0,$ $j\in \mathbb{N}_d,$ and for each $j\in\mathbb{N}_d$, $\mathbf{u}^*$ has sparsity of level $l_j^*\in\mathbb{Z}_{m_j+1}$ under $\mathbf{B}_{(j)}$. Let $\gamma_{j,i},$ $j\in\mathbb{N}_d$, $i\in\mathbb{N}_{m_j}$ be defined as above.

% (a) For each $j\in\mathbb{N}_d$, there exists $l_j\in\mathbb{Z}_{m_j+1}$ with $l_j\geq l_j^*$ such that $\lambda_j^*$ satisfies
% \begin{equation}\label{Order-new}
%    \gamma_{j, i_1}(\mathbf{u}^{*})\leq \cdots \leq \gamma_{j, i_{m_j-l_j}}(\mathbf{u}^{*})<\lambda_j^*=\gamma_{j, i_{m_j-l_j+1}}(\mathbf{u}^{*})=\cdots=\gamma_{j, i_{m_j}}(\mathbf{u}^{*}).
% \end{equation}

% (b) If for each $j\in\mathbb{N}_d$, there exists $l_j\in\mathbb{Z}_{m_j+1}$
% such that $\lambda_j^*$ satisfies
% equation \eqref{Order-new}, then for each $j\in\mathbb{N}_d$, $\mathbf{u}^*$ has sparsity of level $l_j^*\leq l_j$ under $\mathbf{B}_{(j)}$.
% \end{proposition}

\begin{theorem}\label{necessary—new}
Let $\bm{\psi}:\mathbb{R}^n\to\mathbb{R}_+$ be a convex function, for each $j\in\mathbb{N}_d$, $\mathbf{B}_{j}$ be an $m_j\times n$ matrix and  $\mathbf{B}$ be defined by \eqref{block-matrix-B}. Suppose that $\mathbf{u}^*$ is a solution of problem \eqref{optimization_problem_under_Bj} with $\lambda_j^*>0,$ $j\in \mathbb{N}_d$, and for each $j\in\mathbb{N}_d$, $\gamma_{j,i}(\mathbf{u}^{*}),$ $i\in\mathbb{N}_{m_j}$, defined by \eqref{rji}, are ordered as in \eqref{nondecreasing order}. Then the following statements hold true. 

(a) If for each $j\in\mathbb{N}_d$, $\mathbf{u}^*$ has sparsity of level $l_j^*\in\mathbb{Z}_{m_j+1}$ under $\mathbf{B}_{j}$, then for each $j\in\mathbb{N}_d$, $\lambda_j^*$ satisfies
\begin{equation}\label{Order}
   \gamma_{j, i_1}(\mathbf{u}^{*})\leq \cdots \leq \gamma_{j, i_{m_j-l^*_j}}(\mathbf{u}^{*})\leq\lambda_j^*=\gamma_{j, i_{m_j-l^*_j+1}}(\mathbf{u}^{*})=\cdots=\gamma_{j, i_{m_j}}(\mathbf{u}^{*}).
\end{equation} 

(b) If for each $j\in\mathbb{N}_d$, $\mathbf{u}^*$ has sparsity of level $l_j^*\in\mathbb{Z}_{m_j+1}$ under $\mathbf{B}_{j}$, then for each $j\in\mathbb{N}_d$, there exists $l_j\in\mathbb{Z}_{m_j+1}$ with $l_j\geq l_j^*$ such that $\lambda_j^*$ satisfies
\begin{equation}\label{Order-new}
   \gamma_{j, i_1}(\mathbf{u}^{*})\leq \cdots \leq \gamma_{j, i_{m_j-l_j}}(\mathbf{u}^{*})<\lambda_j^*=\gamma_{j, i_{m_j-l_j+1}}(\mathbf{u}^{*})=\cdots=\gamma_{j, i_{m_j}}(\mathbf{u}^{*}).
\end{equation}

(c) If for each $j\in\mathbb{N}_d$, there exists $l_j\in\mathbb{Z}_{m_j+1}$
such that $\lambda_j^*$ satisfies
inequality \eqref{Order-new}, then for each $j\in\mathbb{N}_d$, $\mathbf{u}^*$ has sparsity of level $l_j^*\leq l_j$ under $\mathbf{B}_{j}$.
\end{theorem}

\begin{proof}
We first prove Item (a). If $\mathbf{u}^*$ is a solution of problem \eqref{optimization_problem_under_Bj} with $\lambda_j^*>0,$ $j\in \mathbb{N}_d$, and for each $j\in\mathbb{N}_d$, $\mathbf{u}^*$ has sparsity of level $l_j^*\in\mathbb{Z}_{m_j+1}$ under $\mathbf{B}_{j}$, then for each $j\in\mathbb{N}_d$, the parameter $\lambda_j^*$, guaranteed by Theorem \ref{sparsity_original}, satisfies equality \eqref{sparsity-equi-mini-2} and inequality \eqref{sparsity-equi-mini-3}. Noting that the subset $\{k_i:i\in \mathbb{N}_{l_j^*}\}$ of $\mathbb{N}_{m_j}$ has the cardinality $l_j^*$, there are exactly $l_j^*$
elements of $\{\gamma_{j, i}(\mathbf{u}^{*}): i\in\mathbb{N}_{m_j}\}$ equal
to $\lambda_j^*$ and the remaining $m_j-l_j^*$ elements less than or equal to $\lambda_j^*$. This together with the order of  $\gamma_{j, i}(\mathbf{u}^{*})$, $i\in\mathbb{N}_{m_j}$ as in \eqref{nondecreasing order} leads to the desired inequality \eqref{Order}.

We next verify Item (b). As has been shown in Item (a), for  each $j\in\mathbb{N}_d$, $\lambda_j^*$ satisfies
inequality \eqref{Order}. 
If there is no element of the
sequence $\{\gamma_{j, i}(\mathbf{u}^{*}): i\in\mathbb{N}_{m_j}\}$ being smaller than $\lambda_j^*$, then inequality \eqref{Order} reduces to $\lambda^*_j=\gamma_{j, i_k}(\mathbf{u}^{*})$, $k\in\mathbb{N}_{m_j}$. We then get inequality \eqref{Order-new} with $l_j:=m_j$. Otherwise, we choose $k_j\in\mathbb{N}_{m_j-l^*_j}$ such that $\gamma_{j, i_{k_j}}(\mathbf{u}^{*})<\lambda_j^*=\gamma_{j, i_{k_j+1}}(\mathbf{u}^{*})$. We then rewrite inequality \eqref{Order} as inequality \eqref{Order-new} with $l_j:=m_j-k_j$. It is clear that $l_j\geq m_j-(m_j-l_j^*)=l_j^*$.

It remains to show Item (c). If $l_j=m_j,$ clearly, the sparsity level $l^*_j$ of $\mathbf{u}^*$ under $\mathbf{B}_{j}$ satisfies $l_j^*\leq l_j$. We now consider the case when $l_j<m_j$. According to Theorem \ref{sparsity_original}, the relation $\gamma_{j, i_1}(\mathbf{u}^{*})\leq \cdots \leq \gamma_{j, i_{m_j-l_j}}(\mathbf{u}^{*})<\lambda_j^*$ leads to  $(\mathbf{B}_{j}\mathbf{u}^{*})_{i_k}= 0,$ for all $k\in\mathbb{N}_{m_j-l_j}$. Hence, $\mathbf{B}_{j}\mathbf{u}^{*}$ has at least $m_j-l_j$ zero components. In other words, the number of nonzero components of $\mathbf{B}_{j}\mathbf{u}^{*}$ is at most $l_j$, that is, $\mathbf{u}^*$ has sparsity of level $l_j^*\leq l_j$ under $\mathbf{B}_{j}$. 
\end{proof}

Our goal is to find regularization parameters $\lambda^*_j$, $j\in\mathbb{N}_d$ that ensures the resulting solution $\mathbf{u}^*$ of problem \eqref{optimization_problem_under_Bj} achieves a prescribed sparsity level $l^*_j$ under each $\mathbf{B}_{j}$. According to Item (a) of Theorem \ref{necessary—new}, for each $j\in\mathbb{N}_d$, the parameter $\lambda_j^*$ satisfies inequality \eqref{Order}. Since the sequence $\{\gamma_{j, i}(\mathbf{u}^{*}): i\in\mathbb{N}_{m_j}\}$  depends on the corresponding
solution, inequality \eqref{Order} can not be used directly as a parameter
choice strategy. Instead, it motivates us to propose an iterative scheme. The iteration begins with
initial regularization parameters $\lambda_j^0$, $j\in\mathbb{N}_d$ which are large enough so that for each $j\in\mathbb{N}_d$, the
sparsity level $l_j^0$ of the corresponding solution $\mathbf{u}^0$ of problem \eqref{optimization_problem_under_Bj} under $\mathbf{B}_{j}$ is smaller than the given target sparsity level $l^*_j$. Suppose that at step $k$, we
have $\lambda_j^k$, $j\in\mathbb{N}_d$
and the corresponding solution $\mathbf{u}^k$ with the sparsity level $l_j^k<l^*_j$, $j\in\mathbb{N}_d$ under the transform matrices $\mathbf{B}_{j}$, $j\in\mathbb{N}_d$, respectively. Item (a) of Theorem \ref{necessary—new} ensures that for each $j\in\mathbb{N}_d$, parameter $\lambda_j^k$  satisfies \begin{equation}\label{Order-algorithm}
   \gamma_{j, i^{k}_1}(\mathbf{u}^{k})\leq \cdots \leq \gamma_{j, i^{k}_{m_j-l^k_j}}(\mathbf{u}^{k})\leq\lambda_j^k=\gamma_{j, i^{k}_{m_j-l^k_j+1}}(\mathbf{u}^{k})=\cdots=\gamma_{j, i^{k}_{m_j}}(\mathbf{u}^{k}).
\end{equation} 
We choose parameter $\lambda_j^{k+1}$ at step $k+1$ from the elements among the ordered sequence in \eqref{Order-algorithm}. Motivated by inequality \eqref{Order}, we choose $\lambda_j^{k+1}$ as the $(m_j-l^*_j+1)$-th element of the ordered sequence in \eqref{Order-algorithm}, that is,
\begin{equation}\label{iteration-update}
\lambda_j^{k+1}
    :=\gamma_{j, i^{k}_{m_j-l^*_j+1}}(\mathbf{u}^{k}).
\end{equation}
As a result, for each $j\in\mathbb{N}_d$,  parameter $\lambda_j^{k+1}$ satisfies 
\begin{align}\label{Order-algorithm_1}
    \gamma_{j, i^{k}_1}(\mathbf{u}^{k})\leq \cdots \leq \gamma_{j, i^{k}_{m_j-l^*_j}}(\mathbf{u}^{k})\leq\lambda_j^{k+1}
    &=\gamma_{j, i^{k}_{m_j-l^*_j+1}}(\mathbf{u}^{k})\nonumber\\
    &\leq\cdots\leq\gamma_{j, i^{k}_{m_j-l^k_j+1}}(\mathbf{u}^{k})=\cdots=\gamma_{j, i^{k}_{m_j}}(\mathbf{u}^{k}).
\end{align}

Below, we claim that if the algorithm is convergent, then the parameters obtained by this algorithm satisfy  inequality \eqref{Order} of Theorem \ref{necessary—new} which is a necessary condition for the resulting solution to have  the given target sparsity levels. 
To this end, we state the following assumptions about the convergence of the algorithm. The convergence analysis of the proposed algorithm
for choosing the multiple regularization parameters will be our future research projects. 

(A1) For each $j\in\mathbb{N}_d$, the sequence $l_j^k$, $k\in\mathbb{N}$,
generated by the iteration scheme satisfies that $l_j^k\leq l^*_j$ for all $k\in\mathbb{N}$.

(A2) For each $j\in\mathbb{N}_d$, the sequences $\lambda_j^k$, $k\in\mathbb{N}$  generated by the iteration scheme satisfies that $\lambda_j^k\rightarrow\lambda_j$ as $k\rightarrow+\infty$ for some $\lambda_j>0$.

(A3) The solution $\mathbf{u}\in\mathbb{R}^n$ of problem \eqref{optimization_problem_under_Bj} with $\lambda_j$, $j\in\mathbb{N}_d$ satisfies that for each $j\in\mathbb{N}_d$ and each $s\in\mathbb{N}_{m_j}$, $\gamma_{j, i^{k}_s}(\mathbf{u}^{k})\rightarrow\gamma_{j, i_s}(\mathbf{u})$ as $k\rightarrow+\infty$.
\begin{proposition}
    If assumptions (A1), (A2) and (A3) hold, then for each $j\in\mathbb{N}_d$,  $\lambda_j$ satisfies
    \begin{equation}\label{Order-algorithm_2}
    \gamma_{j, i_1}(\mathbf{u})\leq \cdots \leq \gamma_{j, i_{m_j-l^*_j}}(\mathbf{u})\leq\lambda_j
    =\gamma_{j, i_{m_j-l^*_j+1}}(\mathbf{u})=\cdots=\gamma_{j, i_{m_j}}(\mathbf{u}).
\end{equation}
\end{proposition}
\begin{proof}
Note that assumption (A1)   allows us to choose parameter $\lambda_j^{k+1}$ 
 as in \eqref{iteration-update} at step $k+1$. This together with inequality \eqref{Order-algorithm} leads to inequality 
\eqref{Order-algorithm_1}. 
By taking $k\rightarrow+\infty$ on each item of inequality \eqref{Order-algorithm_1} and assumptions (A2) and (A3), we get that 
\begin{equation}\label{Order-algorithm_3}
 \gamma_{j, i_1}(\mathbf{u})\leq \cdots \leq \gamma_{j, i_{m_j-l^*_j}}(\mathbf{u})\leq\lambda_j
    =\gamma_{j, i_{m_j-l^*_j+1}}(\mathbf{u})\leq\cdots\leq\gamma_{j, i_{m_j}}(\mathbf{u}).
\end{equation}
It follows from inequality \eqref{Order-algorithm} that $\lambda_j^k=\gamma_{j, i^{k}_{m_j}}(\mathbf{u}^{k})$ for all $k\in\mathbb{N}$. Taking $k\rightarrow+\infty$ on both sides of this equation, we get that $\lambda_j=\gamma_{j, i_{m_j}}(\mathbf{u}).$  
This together with  inequality \eqref{Order-algorithm_3} leads to the desired inequality \eqref{Order-algorithm_2}.
\end{proof}

We note that three issues need to be considered in the iterative scheme. First, if $\lambda_j^{k}
=\gamma_{j, i^{k}_{m_j-l^*_j+1}}(\mathbf{u}^{k})$ for some $k\in\mathbb{N}$ and $j\in\mathbb{N}_d$, the choice of $\lambda_j^{k+1}$ as in \eqref{iteration-update} leads to $\lambda_j^{k+1}=\lambda_j^{k}$ and thus is invalid. To address this issue, we choose $\lambda^{k+1}_j$, motivated by inequality \eqref{Order-new}, among the sequence $\{\gamma_{j,i}(\mathbf{u}^{k}):\gamma_{j,i}(\mathbf{u}^{k})<\lambda_j^{k}\}$. We set $\Gamma_j(\mathbf{u}^{k}):=\max\{\gamma_{j,i}(\mathbf{u}^{k}):\gamma_{j,i}(\mathbf{u}^{k})<\lambda_j^{k}\}$ and choose  $\lambda^{k+1}_j$ as 
\begin{equation}\label{iteration-update-1}
\lambda^{k+1}_j:=\min\{\gamma_{j, i^{k}_{m_j-l^*_j+1}}(\mathbf{u}^{k}),\Gamma_j(\mathbf{u}^{k})\}.
\end{equation}
Second, assumption (A1) may not always hold true. If  
 $l_j^k> l^*_j$ for some $k\in\mathbb{N}$ and $j\in\mathbb{N}_d$, this indicates that $\lambda_j^k$
is too small and thus we
should choose $\lambda_j^{k+1}$ at step $k+1$ greater than $\lambda_j^k$. Since as shown in inequality \eqref{Order-algorithm},   all the elements   $\gamma_{j, i^{k}_s}(\mathbf{u}^{k})$, $s\in\mathbb{N}_{m_j}$,
are less than or equal to
$\lambda_j^k$, the choice \eqref{iteration-update-1} cannot provide a desired parameter greater than $\lambda_j^k$. In this case, we should go back to the sequence in step $k-1$ to choose an appropriate parameter $\lambda_j^k$. Finally, due to  the interplay between the 
multiple regularization parameters, we do not require exact match of sparsity levels and instead, we allow them to have a tolerance error. For each $j\in\mathbb{N}_d$, let $l_j$ denote the sparsity level of a solution of problem \eqref{optimization_problem_under_Bj} under $\mathbf{B}_j$. With a given tolerance $\epsilon>0$, we say
that the solution achieves target sparsity levels $l^*_j$, $j\in\mathbb{N}_d$ if 
$$
\sum_{j\in\mathbb{N}_d}|l_j-l_j^*|\leq \epsilon.
$$

% We describe the iterative scheme as follows. For each $j\in\mathbb{N}_d$, we evaluate $\gamma_{j,i}(\mathbf{u}^{0})$ for all $i\in\mathbb{N}_{m_j}$ and rearrange the resulting sequence
% in a nondecreasing order
% \begin{equation}\label{Order-0}
%    \gamma_{j, i^0_1}(\mathbf{u}^{0})\leq \gamma_{j, i^0_2}(\mathbf{u}^{0})\leq 
% \cdots \leq \gamma_{j, i^0_{m_j}}(\mathbf{u}^{0}).
% \end{equation} 
% We update the regularization parameter $\lambda_j$ according to \eqref{Order-0}. Motivated by inequality \eqref{Order}, we choose $\lambda^1_j$ as the $(m_j-l_j^*+1)$th element among the ordered sequence in \eqref{Order-0}. That is, we set $\lambda^1_j:=\gamma_{j, i^0_{m_j-l_j^*+1}}(\mathbf{u}^{0})$, $j\in\mathbb{N}_d$. We solve problem \eqref{optimization_problem_under_Bj} with $\lambda^1_j$, $j\in\mathbb{N}_d$ and
% obtain its solution $\mathbf{u}^1$ with sparsity levels $l^1_j$, $j\in\mathbb{N}_d$. If $l^1_j=l^*_j$, $j\in\mathbb{N}_d$, then the iteration terminates with the desired parameters and
% minimizer. Otherwise, for each $j\in\mathbb{N}_d$, we evaluate $\gamma_{j,i}(\mathbf{u}^{1})$ for all $i\in\mathbb{N}_{m_j}$ and rearrange the resulting sequence
% in a nondecreasing order
% \begin{equation}\label{Order-1}
%    \gamma_{j, i^1_1}(\mathbf{u}^{1})\leq \gamma_{j, i^1_2}(\mathbf{u}^{1})\leq 
% \cdots \leq \gamma_{j, i^1_{m_j}}(\mathbf{u}^{1}).
% \end{equation} 
% In a way similar to
% that for $\lambda_j^1$, $j\in\mathbb{N}_d$, 
% we choose $\lambda_j^2$ as the $(m_j-l_j^*+1)$th element among the ordered sequence in \eqref{Order-1}. We
% repeat the above procedure to obtain an iterative algorithm, with which we obtain
% desirable regularization parameters and a sparse solution achieving the target sparsity levels under the transform matrices simultaneously. 

At step $k+1$ of the iterative scheme, the parameter $\lambda_j^{k+1}$ is chosen from the elements among the ordered sequence in \eqref{Order-algorithm}. To compute  $\gamma_{j, i}(\mathbf{u}^{k})$, $i\in\mathbb{N}_{m_j}$, according to definition \eqref{rji}, we need to obtain the solution $\mathbf{u}^{k}$ of problem \eqref{optimization_problem_under_Bj} with $\lambda_j^k>0,$ $j\in \mathbb{N}_d,$ and then determine the vectors $\mathbf{a}\in\partial\bm{\psi}(\mathbf{u}^k)$ and $\mathbf{b}\in\mathcal{N}(\mathbf{B}^\top)$ in Theorem \ref{sparsity_original}. 

We first consider the case when $\bm{\psi}$ is differentiable and $\mathbf{B}$ has full row rank. In this case, the subdifferential of $\bm{\psi}$ at  $\mathbf{u}^k$
is the singleton $\nabla\bm{\psi}(\mathbf{u}^k)$ and $\mathcal{N}(\mathbf{B}^\top)=\{\mathbf{0}\}$. That is, $\mathbf{a}=\nabla\bm{\psi}(\mathbf{u}^k)$ and $\mathbf{b}=\mathbf{0}$. Accordingly, for each $j\in\mathbb{N}_d$, the sequence $\gamma_{j,i}(\mathbf{u}^k)$, $i\in\mathbb{N}_{m_j}$, has the form 
\begin{equation*}\label{rji-differentiable}
\gamma_{j,i}(\mathbf{u}^k):=\left|(\mathbf{B}_{(p_{j-1}+i)}')^\top\nabla\bm{\psi}(\mathbf{u}^k)\right|, \ i\in\mathbb{N}_{m_j}.
\end{equation*} 
As a result, we merely need to solve problem \eqref{optimization_problem_under_Bj} with $\lambda_j^k>0,$ $j\in \mathbb{N}_d,$ to obtain the solution $\mathbf{u}^{k}$. This  can be done by employing the Fixed Point Proximity Algorithm (FPPA) which was developed in \cite{argyriou2011efficient,li2015multi,micchelli2011proximity}.

We describe the FPPA as follows. Let $\mathbb{S}_{+}^s$ denote the set of symmetric and positive definite matrices. For $\mathbf{H}\in\mathbb{S}_{+}^s$, we define the weighted inner product of $\mathbf{x}$, $\mathbf{y}\in\mathbb{R}^s$ by $\langle \mathbf{x},\mathbf{y}\rangle_{\mathbf{H}}:=\langle \mathbf{x},\mathbf{H}\mathbf{y}\rangle$ and the weighted $\ell_2$-norm of $\mathbf{x}\in\mathbb{R}^s$ by $\|\mathbf{x}\|_{\mathbf{H}}:=\langle \mathbf{x},\mathbf{x}\rangle_{\mathbf{H}}^{1/2}$. Suppose that $f:\mathbb{R}^s\to \overline{\mathbb{R}}$ is a convex function, with $\mathrm{dom}(f)\neq{\emptyset}.$ The proximity operator $\text{prox}_{f,\mathbf{H}}:\mathbb{R}^s\to\mathbb{R}^s$ of $f$ with respect to $\mathbf{H}\in\mathbb{S}_{+}^s$ is defined for $\mathbf{w}\in\mathbb{R}^s$ by 
\begin{equation}\label{proximity operator}
\text{prox}_{f,\mathbf{H}}(\mathbf{w}):=\argmin\left\{\frac{1}{2}\|\mathbf{u}-\mathbf{w}\|_{\mathbf{H}}^2+f(\mathbf{u}):\mathbf{u}\in\mathbb{R}^s\right\}.
\end{equation}
In the case that $\mathbf{H}$ coincides with the $s\times s$ identity matrix $\mathbf{I}_s$, $\text{prox}_{f,\mathbf{I}_s}$ will be
abbreviated as  $\text{prox}_{f}$. 
Suppose that $\Phi:\mathbb{R}^s\to\overline{\mathbb{R}}$ and $\Psi:\mathbb{R}^t\to\overline{\mathbb{R}}$ are two convex functions which may not be differentiable, and $\mathbf{C}\in\mathbb{R}^{t\times s}$. The optimization problem 
$$
\min\{\Phi(\mathbf{u})+\Psi(\mathbf{C}\mathbf{u}):\mathbf{u}\in\mathbb{R}^s\}
$$
can be solved by the FPPA:
For given positive constants $\alpha$, $\rho$ and initial points $\mathbf{u}^0\in\mathbb{R}^s$, $\mathbf{v}^0\in\mathbb{R}^t$, 
\begin{equation}\label{FPPA}
\left\{\begin{array}{l}
\mathbf{u}^{k}=\operatorname{prox}_{\alpha\Phi}\left(\mathbf{u}^{k-1}-\alpha\mathbf{C}^{\top} \mathbf{v}^{k-1}\right), \\
\mathbf{v}^{k}=\rho\left(\mathbf{I}_t-\operatorname{prox}_{\frac{1}{\rho} \Psi}\right)\left(\frac{1}{\rho} \mathbf{v}^{k-1}+\mathbf{C}\left(2 \mathbf{u}^{k}-\mathbf{u}^{k-1}\right)\right).
\end{array}\right.
\end{equation}
In Algorithm \eqref{FPPA}, positive constants $\alpha$ and $\rho$ may be selected to satisfy $\alpha \rho<1/\|\mathbf{C}\|_2^2$ so that the algorithm converges. When $\bm{\psi}$ is differentiable and $\mathbf{B}$ has full row rank, in each step of the iterative algorithm for choosing the multiple regularization
parameters, we solve problem \eqref{optimization_problem_under_Bj} by Algorithm \eqref{FPPA} with specific choices of functions $\Phi$, $\Psi$ and matrix $\mathbf{C}$. 

We summarize the iterative scheme for choosing the multiple regularization
parameters when $\bm{\psi}$ is differentiable and $\mathbf{B}$ has full row rank in Algorithm \ref{algo: iterative scheme picking lambda}. Numerical experiments to be presented in Section 6 demonstrate the effectiveness of this algorithm in identifying the desired regularization parameters. 
% \begin{algorithm}
%   \caption{Iterative scheme selecting multiple regularization parameters for problem \eqref{optimization_problem_under_Bj}}
  
%   \label{algo: iterative scheme picking lambda}
  
%   \KwInput{$\mathbf{B}$, $\bm{\psi}$, $\{l_j^*\}_{j\in\mathbb{N}_{d}}$}
  
%   \KwInitialization{Choose $\{\lambda^0_j\}_{j\in\mathbb{N}_{d}}$ large enough that guarantees $l^0_j\leq l_j^*$ for all $j\in\mathbb{N}_{d}$.}
%   \For{$k = 0,1,2,\ldots$}
%   {Solve \eqref{optimization_problem_under_Bj} with $\{\lambda_j^k\}_{j\in\mathbb{N}_d}$ and count the sparsity level $l^{k}_j$ of $\mathbf{B}_{(j)}\mathbf{u}^{k}$.\\
%   \If{$l^k_j=l^*_j$, for all $j\in\mathbb{N}_d$} {
%     \textbf{break}  % This exits the loop early
%   }
  
%   \For{$j = 1,2,\ldots,d$}
%   {
%   \uIf{$l^k_j<l^*_j$}{Compute $\gamma_{j,i}:=\big|(\mathbf{B}_{p_{j-1}+i}')^\top\nabla\bm{\psi}(\mathbf{u}^{k})\big|$, $i\in\mathbb{N}_{m_j}$.\\
%   Sort: $\gamma_{j,i_1}\leq \cdots \leq \gamma_{j,i_{m_j}}$ with $\{i_1,i_2,\cdots,i_{m_j}\}=\mathbb{N}_{m_j}$.\\
%   Compute $a_j:=\mathrm{max}\big\{\gamma_{j,i}:\gamma_{j,i}< \lambda^{k}_j, i\in\mathbb{N}_{m_j}\big\}$.\\
%   Update $\lambda^{k+1}_j:=\min\big\{\gamma_{j,i_{m_j-{l_j^*}+1}},a_j\big\}$.\\
%   Set $s_j=0$. 
%   }
%   \uElseIf{$l^k_j>l^*_j$}
%   {Update $s_j:=s_j+l^{k}_j-l^{*}_j$ and $\lambda^{k+1}_{j}:=\min\big\{\gamma_{j,i_{m_j-{l_j^*}+1+s_j}},a_j\big\}$.\\
%   }
%   \Else{Update 
%  $\lambda^{k+1}_j$ as $\lambda^{k}_j$.\\
%  Set $s_j=0$.}
%   }
%   }
  
%   \KwOutput{$\{\lambda_j^k\}_{j\in\mathbb{N}_d}$,  $\mathbf{u}^{k}$.}  
% \end{algorithm}

% \begin{algorithm}
%   \caption{Iterative scheme selecting multiple regularization parameters for problem \eqref{optimization_problem_under_Bj}}
  
%   \label{algo: iterative scheme picking lambda}
  
%   \KwInput{$\mathbf{B}$, $\bm{\psi}$, $\{l_j^*:j\in\mathbb{N}_{d}\}$, $\epsilon$}
  
%   \KwInitialization{Choose $\{\lambda^0_j:j\in\mathbb{N}_{d}\}$ large enough that guarantees $l^0_j\leq l_j^*$ for all $j\in\mathbb{N}_{d}$.}
%   \For{$k = 0,1,2,\ldots$}
%   {Solve \eqref{optimization_problem_under_Bj} with $\lambda_j^k$, $j\in\mathbb{N}_d$, and count the sparsity level $l^{k}_j$ of $\mathbf{B}_{(j)}\mathbf{u}^{k}$.\\
%   \If{$\sum_{j\in\mathbb{N}_d}|l_j^k-l_j^*|\leq \epsilon$} {
%     \textbf{break}  % This exits the loop early
%   }
  
%   \For{$j = 1,2,\ldots,d$}
%   {
%     \uIf{$l^k_j<l^*_j$}{Compute $\gamma_{j,i}:=\big|(\mathbf{B}_{p_{j-1}+i}')^\top\nabla\bm{\psi}(\mathbf{u}^{k})\big|$, $i\in\mathbb{N}_{m_j}$.\\
%   Sort: $\gamma_{j,i_1}\leq \cdots \leq \gamma_{j,i_{m_j}}$ with $\{i_1,i_2,\cdots,i_{m_j}\}=\mathbb{N}_{m_j}$.\\
%   Compute $a_j:=\mathrm{max}\big\{\gamma_{j,i}:\gamma_{j,i}< \lambda^{k}_j, i\in\mathbb{N}_{m_j}\big\}$.\\
%   Update $\lambda^{k+1}_j:=\min\big\{\gamma_{j,i_{m_j-{l_j^*}+1}},a_j\big\}$.
%   }
%  \uElseIf{$l^k_j>l^*_j$}
%   {Set $s_j=0$.\\
%   \For{$i = 1,2,\ldots$}
%   {Update $s_j:=s_j+l^{k}_j-l^{*}_j$.\\ Update $\lambda_{j}^{k}:=\min\big\{\gamma_{j,i_{m_j-{l_j^*}+1+s_j}},a_j\big\}$.\\
%   Solve \eqref{optimization_problem_under_Bj} with $\lambda_j^{k}$, $j\in\mathbb{N}_d$, and count the sparsity level $l_j^{k}$ of $\mathbf{B}_{(j)}\mathbf{u}^{k}$.\\
%   \If{$l_j^{k}\leq l_j^*$} { \textbf{break}}
%   % This exits the loop early
  
  
%   }{Update 
%  $\lambda^{k+1}_j$ as $\lambda^{k}_j$ for all $j\in\mathbb{N}_d$.\\}{\textbf{break}}
%   }


%   \Else{Update 
%  $\lambda^{k+1}_j$ as $\lambda^{k}_j$.}
%   }
%   }
  
%   \KwOutput{$\{\lambda_j^k:j\in\mathbb{N}_d\}$,  $\mathbf{u}^{k}$.}  
% \end{algorithm}

\begin{algorithm}
  \caption{Iterative scheme selecting multiple regularization parameters for problem \eqref{optimization_problem_under_Bj} when $\bm{\psi}$ is differentiable and $\mathbf{B}$ has full row rank}
  
  \label{algo: iterative scheme picking lambda}
  
  \KwInput{$\mathbf{B}$, $\bm{\psi}$, $\{l_j^*:j\in\mathbb{N}_{d}\}$, $\epsilon$.}
  
  \KwInitialization{Choose $\{\lambda^0_j:j\in\mathbb{N}_{d}\}$ large enough that guarantees $l^0_j\leq l_j^*$ for all $j\in\mathbb{N}_{d}$.}
  \For{$k = 0,1,2,\ldots$}
  {Solve \eqref{optimization_problem_under_Bj} with $\lambda_j^k$, $j\in\mathbb{N}_d$ by Algorithm \eqref{FPPA} and count the sparsity level $l^{k}_j$ of $\mathbf{B}_{j}\mathbf{u}^{k}$.\\
  \If{$\sum_{j\in\mathbb{N}_d}|l_j^k-l_j^*|\leq \epsilon $} {
    \textbf{break}  % This exits the loop early
  }
  
  \For{$j = 1,2,\ldots,d$}
  {
    \uIf{$l^k_j<l^*_j$}{Compute $\gamma_{j,i}:=\big|(\mathbf{B}_{(p_{j-1}+i)}')^\top\nabla\bm{\psi}(\mathbf{u}^{k})\big|$, $i\in\mathbb{N}_{m_j}$.\\
  Sort: $\gamma_{j,i_1}\leq \cdots \leq \gamma_{j,i_{m_j}}$ with $\{i_1,i_2,\cdots,i_{m_j}\}=\mathbb{N}_{m_j}$.\\
  Compute $\Gamma_j:=\mathrm{max}\big\{\gamma_{j,i}:\gamma_{j,i}< \lambda^{k}_j, i\in\mathbb{N}_{m_j}\big\}$.\\
  Update $\lambda^{k}_j:=\min\big\{\gamma_{j,i_{m_j-{l_j^*}+1}},\Gamma_j\big\}$.
  }
 \ElseIf{$l^k_j>l^*_j$}
  {Set $s_j=0$.\\
  \For{$i = 1,2,\ldots$}
  {Update $s_j:=s_j+l^{k}_j-l^{*}_j$.\\ Update $\lambda_{j}^{k}:=\min\big\{\gamma_{j,i_{m_j-{l_j^*}+1+s_j}},\Gamma_j\big\}$.\\
  Solve \eqref{optimization_problem_under_Bj} with $\lambda^{k}_j$, $j\in\mathbb{N}_d$ by Algorithm \eqref{FPPA}, and count the sparsity level $l^k_j$ of $\mathbf{B}_{j}\mathbf{u}^k$.\\
  \If{$l^k_j\leq l_j^*$} {\textbf{break}}
  % This exits the loop early
  }
  }
  }
  {Update 
 $\lambda^{k+1}_j$ as $\lambda^{k}_j$ for all $j\in\mathbb{N}_d$.}
  }
  
  \KwOutput{$\{\lambda_j^k:j\in\mathbb{N}_d\}$,  $\mathbf{u}^{k}$.}  
\end{algorithm}

We next consider the case when $\bm{\psi}$ is non-differentiable and $\mathbf{B}$ does not have full row rank. In this case, at each step of the iterative
scheme, we should not only find the solution $\mathbf{u}^k$ of problem \eqref{optimization_problem_under_Bj} but also determine the vectors $\mathbf{a}\in\partial\bm{\psi}(\mathbf{u}^k)$ and $\mathbf{b}\in\mathcal{N}(\mathbf{B}^\top)$ satisfying \eqref{sparsity-equi-mini-1}, \eqref{sparsity-equi-mini-2} and \eqref{sparsity-equi-mini-3} simultaneously. To this end, we establish a characterization of vectors $\mathbf{a}$ and $\mathbf{b}$ by using a fixed-point
formulation via the proximity operators of the functions appearing in the objective functions of problem \eqref{optimization_problem_under_Bj_3}.

We begin with recalling some useful results about the proximity operator. It is known \cite{micchelli2011proximity} that the proximity operator of a convex function is intimately related to its subdifferential. Specifically, if $f$ is a convex function from $\mathbb{R}^s$ to $\overline{\mathbb{R}}$, then for all $\mathbf{x}\in\mathrm{dom}(f)$, $\mathbf{y}\in\mathbb{R}^s$ and  $\mathbf{H}\in\mathbb{S}_{+}^s$
\begin{equation}\label{relation_prox_subdiff}
\mathbf{Hy}\in\partial f(\mathbf{x}) \ 
\text{if and only if} \ \mathbf{x}=\mathrm{prox}_{f,\mathbf{H}}(\mathbf{x}+\mathbf{y}).    
\end{equation}
The conjugate function of a convex function $f:\mathbb{R}^s\to \overline{\mathbb{R}}$ is defined as $f^*(\mathbf{y}):=\sup\{\langle \mathbf{x},\mathbf{y}\rangle-f(\mathbf{x}):\mathbf{x}\in\mathbb{R}^s\}$ for all $\mathbf{y}\in\mathbb{R}^s$. There is a relation between the subdifferential of a convex function $f$ and that of its conjugate
function $f^*$. Specifically,  for all $\mathbf{x}\in \mathrm{dom}(f)$ and all $\mathbf{y}\in\mathrm{dom}(f^*)$, there holds
\begin{equation}\label{sub_conjugate}
\mathbf{x}\in \partial f^*(\mathbf{y}) \  \text{if and only if} \ \mathbf{y}\in\partial f(\mathbf{x}).
\end{equation}
This leads to the relation between the proximity operators of $f$ and $f^*$:
$\mathrm{prox}_{f}=\mathbf{I}_s-\mathrm{prox}_{f^*}.$
    
% We also need the identities 
% $$
% \mathrm{prox}_{\frac{1}{\alpha} f^*}
% =\frac{1}{\alpha} (\mathcal{I}-\mathrm{prox}_{\alpha f})(\alpha\mathcal{I}) \ \
% \text{and} 
% \ \ 
% \mathrm{prox}_{f^*,\alpha\mathcal{I}}=\mathrm{prox}_{\frac{1}{\alpha}f^*}.
% $$

In the following proposition, we establish the
fixed-point equation formulation of the solution of problem \eqref{optimization_problem_under_Bj_3}.
\begin{proposition}\label{FPPA_deformation}
Suppose that $\bm{\psi}:\mathbb{R}^n\to\mathbb{R}_+$ is a convex function and for each $j\in\mathbb{N}_d$, $\mathbf{B}_{j}$ is an $m_j\times n$ matrix. Let $\mathbf{B}$ be defined as in \eqref{block-matrix-B}. Then the following statements hold true.

(a) If $\mathbf{w}\in\mathbb{R}^{p_d+n-r}$ is a solution of problem \eqref{optimization_problem_under_Bj_3} with $\lambda_j$, $j\in\mathbb{N}_d$, then there exist vectors $\mathbf{a}\in\mathbb{R}^n$ and $\mathbf{c}\in\mathbb{R}^{p_d+n-r}$ satisfying 
\begin{equation}\label{FPPA_nondiff1}
\ \quad\quad\quad
\mathbf{w}=\mathrm{prox}_{\sum_{j\in\mathbb{N}_d}\lambda_j\|\cdot\|_{1}\circ\mathbf{I}^{'}_{j},\mathbf{O}}\left(\mathbf{w}-\mathbf{O}^{-1}(\mathbf{B}')^{\top}\mathbf{a}-\mathbf{O}^{-1}\mathbf{c}\right),
\end{equation}
\begin{equation}\label{FPPA_nondiff2}
\mathbf{a}=\mathrm{prox}_{\bm{\psi}^*,\mathbf{P}}(\mathbf{P}^{-1}\mathbf{B}'\mathbf{w}+\mathbf{a}), \qquad\quad\qquad\quad
\end{equation}
\begin{equation}\label{FPPA_nondiff3}
\mathbf{c}=\mathrm{prox}_{\iota_{\mathbb{M}}^*,\mathbf{Q}}(\mathbf{Q}^{-1}\mathbf{w}+\mathbf{c}).
 \quad\quad \quad\qquad\qquad
\end{equation}
for any matrices $\mathbf{O}\in\mathbb{S}_{+}^{p_d+n-r}$, $\mathbf{P}\in\mathbb{S}_{+}^{n}$ and $\mathbf{Q}\in\mathbb{S}_{+}^{p_d+n-r}$.

(b) If there exist vectors $\mathbf{w}\in\mathbb{R}^{p_d+n-r}$, $\mathbf{a}\in\mathbb{R}^n$, $\mathbf{c}\in\mathbb{R}^{p_d+n-r}$ and matrices $\mathbf{O}\in\mathbb{S}_{+}^{p_d+n-r}$, $\mathbf{P}\in\mathbb{S}_{+}^{n}$, $\mathbf{Q}\in\mathbb{S}_{+}^{p_d+n-r}$ satisfying equations \eqref{FPPA_nondiff1}, \eqref{FPPA_nondiff2} and \eqref{FPPA_nondiff3}, then $\mathbf{w}$ is a solution of problem \eqref{optimization_problem_under_Bj_3} with $\lambda_j$, $j\in\mathbb{N}_d$.
\end{proposition}
\begin{proof}
According to Fermat rule and the chain rule \eqref{chain-rule} of the subdifferential, we have that 
$\mathbf{w}\in\mathbb{R}^{p_d+n-r}$ is a solution of problem \eqref{optimization_problem_under_Bj_3} if and only if  
\begin{equation*}\label{nondiff-Fermat-rule-chain-rule}
\mathbf{0}\in(\mathbf{B}')^\top\partial\bm{\psi}(\mathbf{B}'\mathbf{w})+\partial\iota_{\mathbb{M}}(\mathbf{w})+\partial\left(\sum_{j\in\mathbb{N}_d}\lambda_j\|\cdot\|_{1}\circ\mathbf{I}^{'}_{j}\right)(\mathbf{w}).
\end{equation*}
The latter is equivalent to that there exist  $\mathbf{a}\in\partial\bm{\psi} (\mathbf{B}'\mathbf{w})$ and $\mathbf{c}\in\partial\iota_{\mathbb{M}}(\mathbf{w})$ such that 
\begin{equation}\label{nondiff-Fermat-rule-chain-rule1}
-(\mathbf{B}')^{\top}\mathbf{a}-\mathbf{c}\in\partial\left(\sum_{j\in\mathbb{N}_d}\lambda_j\|\cdot\|_{1}\circ\mathbf{I}^{'}_{j}\right)(\mathbf{w}).
\end{equation}

We first prove Item (a). If $\mathbf{w}\in\mathbb{R}^{p_d+n-r}$ is a solution of problem \eqref{optimization_problem_under_Bj_3}, then there exist $\mathbf{a}\in\mathbb{R}^n$ and $\mathbf{c}\in\mathbb{R}^{p_d+n-r}$ satisfying inclusion relation \eqref{nondiff-Fermat-rule-chain-rule1}, which further leads to  $\mathbf{O}(-\mathbf{O}^{-1}(\mathbf{B}')^{\top}\mathbf{a}-\mathbf{O}^{-1}\mathbf{c})\in\partial\left(\sum_{j\in\mathbb{N}_d}\lambda_j\|\cdot\|_{1}\circ\mathbf{I}^{'}_{j}\right)(\mathbf{w})$ for any $\mathbf{O}\in\mathbb{S}_{+}^{p_d+n-r}$. Relation \eqref{relation_prox_subdiff} ensures the equivalence between the inclusion relation above and equation \eqref{FPPA_nondiff1}. According to relation \eqref{sub_conjugate}, we rewrite the inclusion relation $\mathbf{a}\in\partial\bm{\psi}(\mathbf{B}'\mathbf{w})$ as $\mathbf{B}'\mathbf{w}\in\partial\bm{\psi}^*(\mathbf{a})$, which further leads to $\mathbf{P}(\mathbf{P}^{-1}\mathbf{B}'\mathbf{w})\in\partial\bm{\psi}^*(\mathbf{a})$ for any $\mathbf{P}\in\mathbb{S}_{+}^{n}$. This guaranteed by relation \eqref{relation_prox_subdiff} is equivalent to equation \eqref{FPPA_nondiff2}. Again by relation \eqref{sub_conjugate}, the inclusion relation   $\mathbf{c}\in\partial\iota_{\mathbb{M}}(\mathbf{w})$ can be rewritten as $\mathbf{w}\in\partial\iota_{\mathbb{M}}^*(\mathbf{c})$. Hence, for any $\mathbf{Q}\in\mathbb{S}_{+}^{p_d+n-r}$, we obtain that $\mathbf{Q}(\mathbf{Q}^{-1}\mathbf{w})\in\partial\iota_{\mathbb{M}}^*(\mathbf{c})$ which guaranteed by relation \eqref{relation_prox_subdiff} is equivalent to equation \eqref{FPPA_nondiff3}.

We next verify Item (b). Suppose that vectors $\mathbf{w}\in\mathbb{R}^{p_d+n-r}$, $\mathbf{a}\in\mathbb{R}^n$, $\mathbf{c}\in\mathbb{R}^{p_d+n-r}$ and matrices $\mathbf{O}\in\mathbb{S}_{+}^{p_d+n-r}$, $\mathbf{P}\in\mathbb{S}_{+}^{n}$, $\mathbf{Q}\in\mathbb{S}_{+}^{p_d+n-r}$ satisfying equations \eqref{FPPA_nondiff1}, \eqref{FPPA_nondiff2} and \eqref{FPPA_nondiff3}. As pointed out in the proof of Item (a), equations \eqref{FPPA_nondiff2} and \eqref{FPPA_nondiff3} are equivalent to inclusion relations $\mathbf{a}\in\partial\bm{\psi} (\mathbf{B}'\mathbf{w})$ and $\mathbf{c}\in\partial\iota_{\mathbb{M}}(\mathbf{w})$, respectively. Moreover, equation \eqref{FPPA_nondiff1} are equivalent to inclusion relation \eqref{nondiff-Fermat-rule-chain-rule1}. Consequently, we conclude that $\mathbf{w}$ is a solution of problem \eqref{optimization_problem_under_Bj_3}.
\end{proof}

Proposition \ref{FPPA_deformation} provides the
fixed-point equation formulations not only for the solution of problem \eqref{optimization_problem_under_Bj_3} but also for another two vectors $\mathbf{a}$ and $\mathbf{c}$. It follows from Lemma \ref{equi_mini} that the solutions of problems \eqref{optimization_problem_under_Bj} and \eqref{optimization_problem_under_Bj_3} are closely related. Below, we show that $\mathbf{a}$ and a subvector of  $\mathbf{c}$ just coincide with the desired two vectors appearing in Theorem \ref{sparsity_original}, respectively. 
\begin{theorem}\label{a-b}
Suppose that $\bm{\psi}:\mathbb{R}^n\to\mathbb{R}_+$ is a convex function and for each $j\in\mathbb{N}_d$, $\mathbf{B}_{j}$ is an $m_j\times n$ matrix. Let $\mathbf{B}$ be defined as in \eqref{block-matrix-B}. Suppose that $\mathbf{w}:=\scriptsize{\begin{bmatrix}
\mathbf{z}\\
\mathbf{v}\end{bmatrix}}\in\mathbb{R}^{p_d+n-r}$ with $\mathbf{v}\in\mathbb{R}^{n-r}$, $\mathbf{z}\in\mathbb{R}^{p_d}$ and for each $j\in \mathbb{N}_d,$ $\mathbf{z}_j=\sum_{i\in\mathbb{N}_{l_j}}z_{p_{j-1}+k_i}^*\mathbf{e}_{m_j,k_i}\in \Omega_{m_j,l_j}$ for some $l_{j}\in\mathbb{Z}_{m_j+1}$, and in addition $\mathbf{a}\in\mathbb{R}^n$,  $\mathbf{c}:=[c_j:j\in\mathbb{N}_{p_d+n-r}]\in\mathbb{R}^{p_d+n-r}$. If vectors $\mathbf{w}$, $\mathbf{a}$ and $\mathbf{c}$ satisfy equations \eqref{FPPA_nondiff1}, \eqref{FPPA_nondiff2} and \eqref{FPPA_nondiff3} for some matrices $\mathbf{O}\in\mathbb{S}_{+}^{p_d+n-r}$, $\mathbf{P}\in\mathbb{S}_{+}^{n}$, $\mathbf{Q}\in\mathbb{S}_{+}^{p_d+n-r}$, then $\mathbf{u}^*:=\mathbf{B}'\mathbf{w}$ is a solution of problem \eqref{optimization_problem_under_Bj} with $\lambda_j$, $j\in\mathbb{N}_d$ and for each $j\in\mathbb{N}_d$, $\mathbf{u}^*$ has sparsity of level $l_{j}$ under $\mathbf{B}_{j}$. Moreover, $\mathbf{a}\in\partial\bm{\psi}(\mathbf{u}^*)$ and $\mathbf{b}:=[c_j:j\in\mathbb{N}_{p_d}]\in\mathcal{N}(\mathbf{B}^{\top})$ satisfy \eqref{sparsity-equi-mini-1}, \eqref{sparsity-equi-mini-2} and \eqref{sparsity-equi-mini-3}. 
\end{theorem}
\begin{proof}
Item (b) of Proposition \ref{FPPA_deformation} ensures that if $\mathbf{w}$, $\mathbf{a}$ and $\mathbf{c}$ satisfy equations \eqref{FPPA_nondiff1}, \eqref{FPPA_nondiff2} and \eqref{FPPA_nondiff3} for some matrices $\mathbf{O}\in\mathbb{S}_{+}^{p_d+n-r}$, $\mathbf{P}\in\mathbb{S}_{+}^{n}$, $\mathbf{Q}\in\mathbb{S}_{+}^{p_d+n-r}$, then $\mathbf{w}$ is a solution of problem \eqref{optimization_problem_under_Bj_3} with $\lambda_j$, $j\in\mathbb{N}_d$. According to Lemma \ref{equi_mini}, we get that $\mathbf{u}^*:=\mathcal{B}^{-1}\mathbf{w}$ is a solution of problem \eqref{optimization_problem_under_Bj} with $\lambda_j$, $j\in\mathbb{N}_d$. By definition of mapping $\mathcal{B}$, we get that $\mathbf{u}^*=\mathbf{B}'\mathbf{w}$ and $\mathbf{B}\mathbf{u}^*=\mathbf{z}$. It follows from definition of matrix $\mathbf{B}$ that for each $j\in \mathbb{N}_d,$ $\mathbf{B}_j\mathbf{u}^*=\mathbf{z}_j$, which shows that $\mathbf{u}^*$ has sparsity of level $l_{j}$ under the transform matrix $\mathbf{B}_{j}$. 
 
 It suffices to verify that $\mathbf{a}\in\partial\bm{\psi}(\mathbf{u}^*)$, $\mathbf{b}\in\mathcal{N}(\mathbf{B}^{\top})$ and there hold \eqref{sparsity-equi-mini-1}, \eqref{sparsity-equi-mini-2} and \eqref{sparsity-equi-mini-3}. As pointed out in the proof of Item (a) of Proposition \ref{FPPA_deformation}, equations \eqref{FPPA_nondiff2} and \eqref{FPPA_nondiff3} are equivalent to inclusion relations $\mathbf{a}\in\partial\bm{\psi} (\mathbf{B}'\mathbf{w})$ and $\mathbf{c}\in\partial\iota_{\mathbb{M}}(\mathbf{w})$, respectively. By noting that $\mathbf{u}^*=\mathbf{B}'\mathbf{w}$, we get that $\mathbf{a}\in\partial\bm{\psi} (\mathbf{u}^*)$. Recalling that $\partial\iota_{\mathbb{M}}(\mathbf{w})=\mathcal{N}(\mathbf{B}^{\top})\times\{\mathbf{0}\}$ leads to $\mathbf{b}\in\mathcal{N}(\mathbf{B}^{\top})$. Note that equation \eqref{FPPA_nondiff1} are equivalent to inclusion relation \eqref{nondiff-Fermat-rule-chain-rule1}. The latter holds if and only if inclusion relation \eqref{Fermat-rule-chain-rule-1} holds. As pointed out in the proof of Lemma \ref{sparsity-equi-mini}, inclusion relation \eqref{Fermat-rule-chain-rule-1} yields that $\mathbf{a}$ and $\mathbf{b}$ satisfy \eqref{sparsity-equi-mini-1}, \eqref{sparsity-equi-mini-2} and \eqref{sparsity-equi-mini-3}. 
\end{proof}

Theorem \ref{a-b} shows that obtaining the solution $\mathbf{u}^*$ of problem \eqref{optimization_problem_under_Bj} and the vectors $\mathbf{a}$, $\mathbf{b}$ satisfying \eqref{sparsity-equi-mini-1}, \eqref{sparsity-equi-mini-2} and \eqref{sparsity-equi-mini-3} can be done by
solving fixed-point equations \eqref{FPPA_nondiff1}, \eqref{FPPA_nondiff2} and \eqref{FPPA_nondiff3}. These three fixed-point equations are coupled
together and they have to be solved simultaneously by iteration. It is convenient to write equations \eqref{FPPA_nondiff1}, \eqref{FPPA_nondiff2} and \eqref{FPPA_nondiff3} in a compact form. To this end, we utilize the three column vectors $\mathbf{w}$, $\mathbf{a}$ and $\mathbf{c}$ to form a block column vector $\mathbf{v}\in \mathbb{R}^{2p_d+3n-2r}$ having $\mathbf{w}$, $\mathbf{a}$ and $\mathbf{c}$ as its three blocks. That is, $\mathbf{v}^\top:=[\mathbf{w}^\top,
\mathbf{a}^\top,
\mathbf{c}^\top]$.
% \begin{equation*}\label{vector_v}
%\mathbf{v}:=
%\begin{bmatrix}
%\mathbf{w}\\
%\mathbf{a}\\
%\mathbf{c}
%\end{bmatrix}.
%\in \mathbb{R}^{p_d+n-r}\times\mathbb{R}^{n}\times\mathbb{R}^{p_d+n-r}.
%\end{equation*}
By integrating together the three proximity operators involved in equations \eqref{FPPA_nondiff1}, \eqref{FPPA_nondiff2} and \eqref{FPPA_nondiff3}, we introduce an operator from $\mathbb{R}^{2p_d+3n-2r}$ to itself by
\begin{equation*}\label{proximity_operator_T}
\mathcal{P}:= 
\begin{bmatrix}
\mathrm{prox}_{\sum_{j\in\mathbb{N}_d}\lambda_j\|\cdot\|_{1}\circ\mathbf{I}^{'}_{j},\mathbf{O}}\\
\mathrm{prox}_{\bm{\psi}^*,\mathbf{P}}\\
\mathrm{prox}_{\iota_{\mathbb{M}}^*,\mathbf{Q}}
\end{bmatrix}.
\end{equation*}
We also define a block matrix by
\begin{equation*}\label{matrix_E}
\mathbf{E}:=\begin{bmatrix}
&\mathbf{I}_{p_d+n-r} &-\mathbf{O}^{-1}(\mathbf{B}')^{\top} &-\mathbf{O}^{-1}\\
&\mathbf{P}^{-1}\mathbf{B}' &\mathbf{I}_{n} 
&\mathbf{0}&\\
&\mathbf{Q}^{-1} &\mathbf{0}
& \mathbf{I}_{p_d+n-r}  
\end{bmatrix}.    
\end{equation*}
In the above notions, we rewrite equations \eqref{FPPA_nondiff1}, \eqref{FPPA_nondiff2} and \eqref{FPPA_nondiff3} in the following compact form
\begin{equation}\label{fixed-point-problem}
\mathbf{v} =(\mathcal{P}\circ \mathbf{E})(\mathbf{v}). 
\end{equation}

Since equations \eqref{FPPA_nondiff1}, \eqref{FPPA_nondiff2} and \eqref{FPPA_nondiff3} are represented in the compact form \eqref{fixed-point-problem}, one may define the Picard iteration based on \eqref{fixed-point-problem} to solve the fixed-point $\mathbf{v}$ of the operator $\mathcal{P}\circ \mathbf{E}$, that is
$$
\mathbf{v}^{k+1} = (\mathcal{P}\circ \mathbf{E})(\mathbf{v}^{k}),\ k=0,1,\cdots
$$
When it converges, the Picard sequence $\mathbf{v}^{k}$, $k=0,1,\cdots$, generated by the Picard iteration above,
converges to a fixed-point of the operator $\mathcal{P}\circ \mathbf{E}$. It is known \cite{Byrne2003} that the convergence of the Picard sequence requires the firmly non-expansiveness of the operator $\mathcal{P}\circ \mathbf{E}$. However, by arguments similar to those used in the proof of  Lemmas 3.1 and 3.2 of \cite{li2015multi}, we can prove that the operator  $\mathcal{P}\circ \mathbf{E}$ is not firmly non-expansive. We need to reformulate the fixed-point equation \eqref{fixed-point-problem} by appropriately split the matrix $\mathbf{E}$ guided
by the theory of the non-expansive map.

% It has been proved in \cite{Combettes2005} that the proximity operators are firmly non-expansive with respect to a certain positive definite matrix $\mathbf{R}$. In the following lemma, we show that the operator $\mathcal{T}$ is the proximity operator of a new convex function
% \begin{equation*}
% g(\mathbf{v}):=  \sum_{j\in\mathbb{N}_d} \lambda_j\|\mathbf{I}'_{(j)}\mathbf{w}\|_1+\bm{\psi}^*(\mathbf{a}) +
% \iota_{\mathbb{M}}^*(\mathbf{c})
% \end{equation*}
% for $\mathbf{v}$ defined by \eqref{vector_v} with respect to the matrix $\mathbf{R}:=\text{diag}(\mathbf{O},\mathbf{P},\mathbf{Q}).$
% Here, we omit the proof since one can complete it by referring to Lemma 3.1 of \cite{li2015multi}.

% \begin{lemma}\label{T_prox_g_R}
% If operator $\mathcal{T}$ is defined by \eqref{proximity_operator_T}, then $\mathcal{T}$ is the proximity operator of the function $g$, that is, $\mathcal{T}=\mathrm{prox}_{g,\mathbf{R}}.$  
% \end{lemma}
% \begin{proof}
% Let $\mathbf{v}$ be defined by \eqref{vector_v}. By the definition of the proximity operator and the structure of convex function $g$, we have that
% \begin{equation}\label{prox_g_R}
% \text{prox}_{g,\mathbf{R}}
% =\argmin\left\{\frac{1}{2}\|\widetilde{\mathbf{v}}-\mathbf{v}\|^2_{\mathbf{R}}+g(\widetilde{\mathbf{v}}):\widetilde{\mathbf{v}}\in\mathbb{R}^{p_d+n-r}
% \times\mathbb{R}^{n}
% \times\mathbb{R}^{p_d+n-r}\right\}.
% \end{equation}
% Noting that 
% \begin{align*}
% \frac{1}{2}\|\widetilde{\mathbf{v}}-\mathbf{v}\|^2_{\mathbf{R}}+g(\widetilde{\mathbf{v}})
% =&\frac{1}{2}\|\widetilde{\mathbf{w}}-\mathbf{w}\|^2_{\mathbf{O}}+\sum_{j\in\mathbb{N}_d}\lambda_j\|\mathbf{I}'_{(j)}\widetilde{\mathbf{w}}\|_1+\frac{1}{2}\|\widetilde{\mathbf{a}}-\mathbf{a}\|^2_{\mathbf{P}}\\
% &+\bm{\psi}^*(\widetilde{\mathbf{a}}) +\frac{1}{2}\|\widetilde{\mathbf{c}}-\mathbf{c}\|^2_{\mathbf{Q}}+\iota_{\mathbb{M}}^*(\widetilde{\mathbf{c}}).
% \end{align*}
% Substituting the equation above into the right hand side of equation \eqref{prox_g_R} leads to the desired result.
% \end{proof}

% Lemma \ref{T_prox_g_R} ensures that the operator $\mathcal{T}$ is firmly non-expansive with respect to the matrix $\mathbf{R}$. If $\mathbf{E}$ were also non-expansive, then the composition $\mathcal{T}\circ \mathbf{E}$ would be non-expansive \cite{Byrne2003}. However, as we can see in the following result, the matrix $\mathbf{E}$ in \eqref{matrix_E} is not non-expansive. We first show the expansivity of the operator $\mathbf{E}$. To this end, for an $s\times s$ matrix $\mathbf{M}$, we define 
% \begin{equation*}
% \|\mathbf{M}\|_{\mathbf{R}}:=\sup\left\{\|\mathbf{Mx}\|_{\mathbf{R}}:\|\mathbf{x}\|_{\mathbf{R}}=1, \mathbf{x}\in\mathbb{R}^s\right\}.   
% \end{equation*}

% \begin{proposition}
% If $\mathbf{E}$ is the operator defined in \eqref{matrix_E}, there holds that $\|\mathbf{E}\|_{\mathbf{R}}>1$.
% \end{proposition}

% \begin{proof}
% Let $\mathbf{v}$ be defined by \eqref{vector_v} with $\|\mathbf{v}\|_{\mathbf{R}}=1$. By the definition of the matrix $\mathbf{E}$, we have that
% \begin{equation*}
% \mathbf{Ev}
% =\begin{bmatrix}
% \mathbf{w}-\mathbf{O}^{-1}(\mathbf{B}')^{\top}\mathbf{a}-\mathbf{O}^{-1}\mathbf{c}\\
% \mathbf{P}^{-1}\mathbf{B}'\mathbf{w}+\mathbf{a}\\
% \mathbf{Q}^{-1}\mathbf{w}+\mathbf{c}
% \end{bmatrix}. 
% \end{equation*}
% Noting that 
% \begin{align*}
% \big\|\mathbf{w}-\mathbf{O}^{-1}(\mathbf{B}')^{\top}\mathbf{a}-\mathbf{O}^{-1}\mathbf{c}
% \big\|^2_{\mathbf{O}}
% = &\|\mathbf{w}\|^2_{\mathbf{O}}
% +\big\|\mathbf{O}^{-1}(\mathbf{B}')^{\top}\mathbf{a}+\mathbf{O}^{-1}\mathbf{c}\big\|^2_{\mathbf{O}}\\
% &-2\big\langle \mathbf{B}'\mathbf{w},\mathbf{a}\big\rangle-2\langle\mathbf{w},\mathbf{c}\rangle,
% \end{align*}
% \begin{align*}
% \big\|\mathbf{P}^{-1}\mathbf{B}'\mathbf{w}+\mathbf{a}\big\|^2_{\mathbf{P}}
% = \|\mathbf{a}\|^2_{\mathbf{P}} +  \|\mathbf{P}^{-1}\mathbf{B}'\mathbf{w}\|^2_{\mathbf{P}} + 2 \big\langle \mathbf{a}, \mathbf{B}'\mathbf{w} \big\rangle,
% \end{align*}
% \begin{align*}
% \big\|\mathbf{Q}^{-1}\mathbf{w}+\mathbf{c}\big\|^2_{\mathbf{Q}}=
% \big\|\mathbf{c}\big\|^2_{\mathbf{Q}} + \big\|\mathbf{Q}^{-1}\mathbf{w}\big\|^2_{\mathbf{Q}} + 2 \langle \mathbf{c}, \mathbf{w} \rangle
% \end{align*}
% and $$\|\mathbf{w}\|^2_{\mathbf{O}}+\|\mathbf{a}\|^2_{\mathbf{P}}+\sum_{j\in\mathbb{N}_d}\big\|\mathbf{c}_j\big\|^2_{\mathbf{Q}_j}=\|\mathbf{v}\|_{\mathbf{R}},$$
% we observe that
% \begin{align*}
% \|\mathbf{Ev}\|_{\mathbf{R}}^2
% =  &\big\|\mathbf{w}-\mathbf{O}^{-1}(\mathbf{B}')^{\top}\mathbf{a}-\mathbf{O}^{-1}\mathbf{c}\big\|^2_{\mathbf{O}}+\big\|\mathbf{P}^{-1}\mathbf{B}'\mathbf{w}+\mathbf{a}\big\|^2_{\mathbf{P}} +\big\|\mathbf{Q}^{-1}\mathbf{w}
% +\mathbf{c}\big\|^2_{\mathbf{Q}}\\
% =& 1 + \big\|\mathbf{O}^{-1}(\mathbf{B}')^{\top}\mathbf{a}
% +\mathbf{O}^{-1}\mathbf{c}\big\|^2_{\mathbf{O}} + \|\mathbf{P}^{-1}\mathbf{B}'\mathbf{w}\|^2_{\mathbf{P}} +\big\|\mathbf{Q}^{-1}\mathbf{w}\big\|^2_{\mathbf{Q}}. 
% \end{align*}
% Since $\mathbf{B}'$ is non-zero matrices there exist a vector $\mathbf{v}$ with $\|\mathbf{v}\|_{\mathbf{R}}=1$ such that the sum of the last three terms on the right-hand side of the above equation is positive. This implies that the norm $\|\mathbf{E}\|_{\mathbf{R}}$ is strictly greater than 1.
% \end{proof}

We describe the split of $\mathbf{E}$ as follows. Set $\mathbf{R}:=\text{diag}(\mathbf{O},\mathbf{P},\mathbf{Q})$. By  introducing three $(2p_d+3n-2r)\times(2p_d+3n-2r)$ matrices $\mathbf{M}_{0},\mathbf{M}_{1},\mathbf{M}_{2}$ satisfying $\mathbf{M}_{0}=\mathbf{M}_{1}+\mathbf{M}_{2}$, we split the expansive matrix $\mathbf{E}$ as 
$$
\mathbf{E}
=(\mathbf{E}-\mathbf{R}^{-1}\mathbf{M}_{0})
+\mathbf{R}^{-1}\mathbf{M}_{1}+\mathbf{R}^{-1}\mathbf{M}_{2}.
$$
Accordingly, the fixed-point equation \eqref{fixed-point-problem} can be  rewritten as
$$
\mathbf{v}
=\mathcal{P}((\mathbf{E}-\mathbf{R}^{-1}\mathbf{M}_{0})\mathbf{v}
+\mathbf{R}^{-1}\mathbf{M}_{1}\mathbf{v}
+\mathbf{R}^{-1}\mathbf{M}_{2}\mathbf{v}).
$$
Based upon the above equation, we define a two-step iteration to solve the fixed-point $\mathbf{v}$ of the operator $\mathcal{P}\circ \mathbf{E}$ as
\begin{equation}\label{two-step_iterative}
\mathbf{v}^{k+1}
=\mathcal{P}((\mathbf{E}-\mathbf{R}^{-1}\mathbf{M}_{0})\mathbf{v}^{k+1}
+\mathbf{R}^{-1}\mathbf{M}_{1}\mathbf{v}^{k}
+\mathbf{R}^{-1}\mathbf{M}_{2}\mathbf{v}^{k-1}).    
\end{equation}

To develop an efficient iterative algorithm, we need to consider two issues: The first issue is the solvability of equation \eqref{two-step_iterative}. The second issue is the convergence of the iterative algorithm. In fact, these two issues may be addressed by choosing appropriate matrices $\mathbf{M}_{0}$, $\mathbf{M}_{1}$, $\mathbf{M}_{2}$.
Specifically, by introducing a real number $\theta$, we choose $\mathbf{M}_{0}$, $\mathbf{M}_{1}$, $\mathbf{M}_{2}$ as
\begin{equation}\label{matrix_M0}
\mathbf{M}_{0}:=\begin{bmatrix}
\mathbf{O} &-(\mathbf{B}')^{\top} &-\mathbf{I}_{p_d+n-r}\\
-\theta\mathbf{B}' &\mathbf{P} 
&\mathbf{0}\\
-\theta\mathbf{I}_{p_d+n-r} &\mathbf{0}
& \mathbf{Q}
\end{bmatrix},    
\end{equation}
\begin{equation}\label{matrix_M1}
\mathbf{M}_{1}:=\begin{bmatrix}
\mathbf{O} &(\theta-2)(\mathbf{B}')^{\top} 
&(\theta-2)\mathbf{I}_{p_d+n-r}\\
-\theta\mathbf{B}' &\mathbf{P} 
&\mathbf{0}\\
-\theta\mathbf{I}_{p_d+n-r} &\mathbf{0}
& \mathbf{Q}
\end{bmatrix},    
\end{equation}
and 
\begin{equation}\label{matrix_M2}
\mathbf{M}_{2}:=\begin{bmatrix}
\mathbf{0} &(1-\theta)(\mathbf{B}')^{\top} 
&(1-\theta)\mathbf{I}_{p_d+n-r}\\
\mathbf{0}  &\mathbf{0}
&\mathbf{0}\\
\mathbf{0} &\mathbf{0} &\mathbf{0}
\end{bmatrix}.    
\end{equation}
It is clearly that $\mathbf{M}_{0}=\mathbf{M}_{1}+\mathbf{M}_{2}$. Associated with these matrices, we have that
\begin{equation}\label{matrix_M0_1}
(\mathbf{E}-\mathbf{R}^{-1}\mathbf{M}_{0})
=\begin{bmatrix}
\mathbf{0}&\mathbf{0}&\mathbf{0}\\
(1+\theta)\mathbf{P}^{-1}\mathbf{B}' &\mathbf{0}&\mathbf{0}\\
(1+\theta)\mathbf{Q}^{-1}&\mathbf{0}&\mathbf{0}
\end{bmatrix},
\end{equation}
\begin{equation}\label{matrix_M1_1}
\mathbf{R}^{-1}\mathbf{M}_{1} 
=\begin{bmatrix}
\mathbf{I}_{p_d+n-r}&(\theta-2)\mathbf{O}^{-1}(\mathbf{B}')^{\top}&(\theta-2)\mathbf{O}^{-1}\\
-\theta\mathbf{P}^{-1}\mathbf{B}'&\mathbf{I}_{n}&\mathbf{0}\\
-\theta\mathbf{Q}^{-1}&\mathbf{0}&\mathbf{I}_{p_d+n-r}
\end{bmatrix}
\end{equation}
and 
\begin{equation}\label{matrix_M2_1}
\mathbf{R}^{-1}\mathbf{M}_{2}
=\begin{bmatrix}
\mathbf{0}&(1-\theta )\mathbf{O}^{-1}(\mathbf{B}')^{\top}& (1-\theta )\mathbf{O}^{-1}\\
\mathbf{0}&\mathbf{0}&\mathbf{0}\\
\mathbf{0}&\mathbf{0}&\mathbf{0}
\end{bmatrix}.
\end{equation}
% \begin{equation*}
% (\mathbf{E}-\mathbf{R}^{-1}\mathbf{M}_{(0)})\mathbf{v}^{k+1} 
% =\begin{bmatrix}
% \mathbf{0}\\
% (1+\theta)\mathbf{P}^{-1}\mathbf{B}' \mathbf{w}^{k+1}\\
% (1+\theta)\mathbf{Q}^{-1}\mathbf{w}^{k+1}
% \end{bmatrix},
% \end{equation*}
% \begin{equation*}
% \mathbf{R}^{-1}\mathbf{M}_{(1)}\mathbf{v}^{k} 
% =\begin{bmatrix}
% \mathbf{w}^{k}+(\theta-2)\left(\mathbf{O}^{-1}(\mathbf{B}')^{\top} \mathbf{a}^{k} +\mathbf{O}^{-1}\mathbf{c}^k\right)\\
% -\theta\mathbf{P}^{-1}\mathbf{B}'\mathbf{w}^{k}+\mathbf{a}^{k}\\
% -\theta\mathbf{Q}^{-1}\mathbf{w}^{k}+ \mathbf{c}^k
% \end{bmatrix}
% \end{equation*}
% and 
% \begin{equation*}
% \mathbf{R}^{-1}\mathbf{M}_{(2)}\mathbf{v}^{k-1} 
% =\begin{bmatrix}
% (1-\theta )\left(\mathbf{O}^{-1}(\mathbf{B}')^{\top} \mathbf{a}^{k-1} +\mathbf{O}^{-1}\mathbf{c}^{k-1}\right)\\
% \mathbf{0}\\
% \mathbf{0}
% \end{bmatrix}.
% \end{equation*}
We then rewrite the iterative scheme \eqref{two-step_iterative} in terms of the vector $\mathbf{w}^k$, $\mathbf{a}^k$ and $\mathbf{c}^k$ as
\begin{equation}\label{FPPA_w}
\left\{\begin{array}{l}
\mathbf{w}^{k+1}=\mathrm{prox}_{\sum_{j\in\mathbb{N}_d}\lambda_j\|\cdot\|_{1}\circ\mathbf{I}^{'}_{j},\mathbf{O}}
\big(\mathbf{w}^{k}+\mathbf{O}^{-1}(\mathbf{B}')^{\top}((\theta-2)\mathbf{a}^{k}\\
\ \quad\qquad+(1-\theta)\mathbf{a}^{k-1})+\mathbf{O}^{-1}((\theta-2)\mathbf{c}^{k}+(1-\theta)\mathbf{c}^{k-1})\big), \\
\ \mathbf{a}^{k+1}=\mathrm{prox}_{\bm{\psi}^*,\mathbf{P}}
\big(\mathbf{a}^{k}+\mathbf{P}^{-1}\mathbf{B}'(\mathbf{w}^{k+1}+\theta(\mathbf{w}^{k+1}-\mathbf{w}^{k}))\big),\\
\ 
\mathbf{c}^{k+1}=\mathrm{prox}_{\iota_{\mathbb{M}}^*,\mathbf{Q}}\big(\mathbf{c}^{k}+\mathbf{Q}^{-1}(\mathbf{w}^{k+1}+\theta(\mathbf{w}^{k+1}-\mathbf{w}^{k})\big). 
\end{array}\right.
\end{equation}
We first note that since matrix $\mathbf{E}-\mathbf{R}^{-1}\mathbf{M}_{0}$ is strictly block lower triangular, the two-step iteration scheme \eqref{two-step_iterative}, as an implicit scheme in general, reduces to an explicit scheme \eqref{FPPA_w}. We next establish the convergence of Algorithm \eqref{FPPA_w} in the following theorem. The convergence can be obtained by using the arguments similar to those in \cite{li2015multi, Li2017two} and we provide its complete proof in Appendix A for the convenience of readers. To this end, we introduce two block matrices by 
\begin{equation}\label{F-G}
\mathbf{F}:=\begin{bmatrix}
\mathbf{P} &\mathbf{0} \\
\mathbf{0} &\mathbf{Q} 
\end{bmatrix} \ \text{and} \ 
\mathbf{G}:=\begin{bmatrix}
\mathbf{B}' \\
\mathbf{I}_{p_d+n-r} 
\end{bmatrix}.    
\end{equation}


\begin{theorem}\label{convergence_FPPA}
 If matrices $\mathbf{O}\in\mathbb{S}_{+}^{p_d+n-r}$, $\mathbf{P}\in\mathbb{S}_{+}^{n}$, $\mathbf{Q}\in\mathbb{S}_{+}^{p_d+n-r}$ and $\theta\in\mathbb{R}$ satisfy 
\begin{equation}\label{satisfy-M-condition}
|\theta|\big\| 
\mathbf{F}^{-\frac{1}{2}}\mathbf{G}
\mathbf{O}^{-\frac{1}{2}}
\big\|_2<1 
\end{equation}
and 
\begin{equation}\label{satisfy-M-condition-H}
\frac{|1-\theta|\|\mathbf{G}\|_2\max\big\{\|\mathbf{O}^{-1}\|_2,
\left\|\mathbf{F}^{-1}
\right\|_2\big\}}{1-|\theta|\big\| 
\mathbf{F}^{-\frac{1}{2}}\mathbf{G}
\mathbf{O}^{-\frac{1}{2}}
\big\|_2}<\frac{1}{2},   
\end{equation}
then the sequence 
$\mathbf{v}^k:=\scriptsize{
\begin{bmatrix}
\mathbf{w}^k\\
\mathbf{a}^k\\
\mathbf{c}^k
\end{bmatrix} }$, $k\in\mathbb{N}$, generated by Algorithm \eqref{FPPA_w} for any given $\mathbf{v}^0, \mathbf{v}^1\in\mathbb{R}^{2p_d+3n-2r}$, converges to a fixed-point of operator $\mathcal{P}\circ\mathbf{E}$.
\end{theorem}


To end this section, we summarize the iterative scheme for parameter choices when $\bm{\psi}$ is non-differentiable and $\mathbf{B}$ does not satisfy full row rank in Algorithm \ref{algo: iterative scheme picking lambda psi non-differentiable}. Note that the computation of the proximity operators involved in Algorithm \eqref{FPPA_w} is essential for the implementation of Algorithm \ref{algo: iterative scheme picking lambda psi non-differentiable}. In Appendix B, we will provide the closed-form formulas for these proximity operators.

\begin{algorithm}
  \caption{Iterative scheme selecting multiple regularization parameters for problem \eqref{optimization_problem_under_Bj}}
  
  \label{algo: iterative scheme picking lambda psi non-differentiable}
  
  \KwInput{$\mathbf{B}$, $\bm{\psi}$, $\{l_j^*:j\in\mathbb{N}_{d}\}$, $\epsilon$.}
  
  \KwInitialization{Choose $\{\lambda^0_j:j\in\mathbb{N}_{d}\}$ large enough that guarantees $l^0_j\leq l_j^*$ for all $j\in\mathbb{N}_{d}$.}
  \For{$k = 0,1,2,\ldots$}
  {Solve \eqref{optimization_problem_under_Bj_3} with $\lambda_j^k$, $j\in\mathbb{N}_d$ by Algorithm \eqref{FPPA_w}, get the vectors $\mathbf{w}^{k}=\scriptsize{\begin{bmatrix}
\mathbf{z}^{k}\\
\mathbf{v}^{k}\end{bmatrix}}$, $\mathbf{a}^{k}$ and $\mathbf{c}^{k}$, and count the sparsity level $l^{k}_j$ of $\mathbf{z}^{k}_j$.\\
  \If{$\sum_{j\in\mathbb{N}_d}|l_j^k-l_j^*|\leq \epsilon $} {
    \textbf{break}  % This exits the loop early
  }
  
  \For{$j = 1,2,\ldots,d$}
  {
    \uIf{$l^k_j<l^*_j$}{Compute $\gamma_{j,i}:=\big|(\mathbf{B}_{(p_{j-1}+i)}')^\top\mathbf{a}^{k}+c^{k}_{p_{j-1}+i})\big|$, $i\in\mathbb{N}_{m_j}$.\\
  Sort: $\gamma_{j,i_1}\leq \cdots \leq \gamma_{j,i_{m_j}}$ with $\{i_1,i_2,\cdots,i_{m_j}\}=\mathbb{N}_{m_j}$.\\
  Compute $\Gamma_j:=\mathrm{max}\big\{\gamma_{j,i}:\gamma_{j,i}< \lambda^{k}_j, i\in\mathbb{N}_{m_j}\big\}$.\\
  Update $\lambda^{k}_j:=\min\big\{\gamma_{j,i_{m_j-{l_j^*}+1}},\Gamma_j\big\}$.
  }
 \ElseIf{$l^k_j>l^*_j$}
  {Set $s_j=0$.\\
  \For{$i = 1,2,\ldots$}
  {Update $s_j:=s_j+l^{k}_j-l^{*}_j$.\\ Update $\lambda_{j}^{k}:=\min\big\{\gamma_{j,i_{m_j-{l_j^*}+1+s_j}},\Gamma_j\big\}$.\\
  Solve \eqref{optimization_problem_under_Bj_3} with $\lambda_j^k$, $j\in\mathbb{N}_d$ by Algorithm \eqref{FPPA_w}, get the vectors $\mathbf{w}^{k}$, $\mathbf{a}^{k}$ and $\mathbf{c}^{k}$, and count the sparsity level $l^{k}_j$ of $\mathbf{z}^{k}_j$.\\
  \If{$l^k_j\leq l_j^*$} {\textbf{break}}
  % This exits the loop early
  }
  }
  }
  {Update 
 $\lambda^{k+1}_j$ as $\lambda^{k}_j$ for all $j\in\mathbb{N}_d$.}
  }
  
  \KwOutput{$\{\lambda_j^k:j\in\mathbb{N}_d\}$, $\mathbf{w}^{k}$.}  
\end{algorithm}

\section{Numerical experiments}
In this section, we present four numerical experiments to demonstrate the efficiency of the proposed multi-parameter choice strategies. 

% In the numerical experiments presented in this section, we solve the multi-parameter regularization problems by employing the Fixed Point Proximity Algorithm (FPPA) which was developed in \cite{argyriou2011efficient,li2015multi,micchelli2011proximity}. We describe the FPPA as follows. Suppose that $f:\mathbb{R}^n\to \overline{\mathbb{R}}:= \mathbb{R}\cup\{+\infty\}$ is a convex function, with $\mathrm{dom}(f):=\{\mathbf{w}\in\mathbb{R}^n:f(\mathbf{w})<+\infty\}\neq{\emptyset}$. The proximity operator $\text{prox}_{f}:\mathbb{R}^n\to\mathbb{R}^n$ of $f$ is defined for $\mathbf{w}\in\mathbb{R}^n$ by
% $$
% \text{prox}_{f}(\mathbf{w}):=\argmin\left\{\frac{1}{2}\|\mathbf{u}-\mathbf{w}\|_2^2+f(\mathbf{u}):\mathbf{u}\in\mathbb{R}^n\right\}.
% $$
% Suppose that $\Phi:\mathbb{R}^n\to\overline{\mathbb{R}}$  and $\Psi:\mathbb{R}^s\to\overline{\mathbb{R}}$ are two convex functions which may not be differentiable, and $\mathbf{C}\in\mathbb{R}^{s\times n}$. We solve the optimization problem 
% $$
% \min\{\Phi(\mathbf{u})+\Psi(\mathbf{C}\mathbf{u}):\mathbf{u}\in\mathbb{R}^n\}
% $$
% by the FPPA:
% For given positive constants $\alpha$, $\rho$ and initial points $\mathbf{u}^0\in\mathbb{R}^n$, $\mathbf{v}^0\in\mathbb{R}^s$, 
% \begin{equation}\label{FPPA}
% \left\{\begin{array}{l}
% \mathbf{u}^{k}=\operatorname{prox}_{\alpha\Phi}\left(\mathbf{u}^{k-1}-\alpha\mathbf{C}^{\top} \mathbf{v}^{k-1}\right), \\
% \mathbf{v}^{k}=\rho\left(\mathcal{I}-\operatorname{prox}_{\frac{1}{\rho} \Psi}\right)\left(\frac{1}{\rho} \mathbf{v}^{k-1}+\mathbf{C}\left(2 \mathbf{u}^{k}-\mathbf{u}^{k-1}\right)\right),
% \end{array}\right.
% \end{equation}
% where $\mathcal{I}$ denotes the $s\times s$ identity matrix. In Algorithm \eqref{FPPA}, we choose positive constants $\alpha$ and $\rho$ satisfying $\alpha \rho<1/\|\mathbf{C}\|_2^2$ so that the algorithm converges. In the numerical examples, we obtain the solution after iteration \eqref{FPPA} converges. 

In presentation of the numerical results, we use “TSLs” and $``\mathrm{SLs}"$ to represent
the target sparsity levels $\{l^*_j:j\in\mathbb{N}_d\}$ and the sparsity levels $\{l_j:j\in\mathbb{N}_d\}$ of the solution obtained from the parameter choice strategies, respectively. 
For the single parameter regularization model, we use 
“TSL” and $``\mathrm{SL}"$ to denote the target sparsity level $l^*$ and the sparsity level $l$ of the solution obtained from the parameter choice strategies, respectively. In the first two experiments related to the regularization problem \eqref{optimization_problem}, we use ``Ratio'' to denote the ratio $r$ of the number of nonzero components
to the total number of components of the obtained solution. In the last two experiments associated with the general regularization problem \eqref{optimization_problem_under_Bj}, we use ``Ratios'' to represent the ratios $\{r_j:j\in\mathbb{N}_d\}$. Specifically, for each $j\in\mathbb{N}_d$, $r_j$ refers to the ratio of the number of nonzero components
to the total number of components of the obtained solution under the transform matrix $\mathbf{B}_{j}$.

\subsection{Parameter choices: a block separable case}\label{NM_block_separable}
In this experiment, we validate the parameter choice strategy proposed in
Corollary \ref{block_separabel_parameter_choice} by considering the multi-parameter regularization model \eqref{lasso_multi-parameter} for signal denoising. The single-parameter regularization model for signal denoising is also considered for the comparison purpose.

We employ model \eqref{lasso_multi-parameter} to recover the Doppler signal function 
\begin{equation}\label{Doppler_signal}
f(t):=\sqrt{t(1-t)}\sin((2.1\pi)/(t+0.05)),
\ t\in[0,1],
\end{equation}
from its noisy data. Let $n:=4096$ and $t_j$, $j\in\mathbb{N}_n$, be the sample points on a uniform grid in $[0,1]$ with step size $h:={1}/{(n-1)}$. We recover the signal  $\mathbf{f}:=[f(t_j):j\in\mathbb{N}_n]$ from the noisy signal  $\mathbf{x}:=\mathbf{f}+\bm{\eta}$, where $\bm{\eta}$ is an additive white Gaussian noise with the signal-to-noise ratio $\text{SNR}=80$.

We describe the multi-parameter regularization model \eqref{lasso_multi-parameter} as follows. In this model, we let $d:=7$ and choose the matrix 
$\mathbf{A}\in\mathbb{R}^{n\times n}$ as the Daubechies wavelet transform with the vanishing moments $\mathrm{N}:=6$ and the coarsest resolution level $\mathrm{L}:=6$.  Let $m_1:=2^6$, $m_j:=2^{j+4}$, $j\in\mathbb{N}_{d}\setminus \{1\}$ and set $p_0:=0$, $
p_j:=\sum_{i\in\mathbb{N}_j}m_i$, $j\in\mathbb{N}_d$. We  choose the partition 
$\mathcal{S}_{n,d}:=\left\{S_{n,1},S_{n,2},\ldots,S_{n,d}\right\}$ for $\mathbb{N}_{n}$ with $S_{n,j}:=\{p_{j-1}+k:k\in\mathbb{N}_{m_j}\}$, $j\in\mathbb{N}_d$.
Associated with this partition, we decompose $\mathbf{u}:=[u_k:k\in\mathbb{N}_n]\in\mathbb{R}^n$ into $d$ sub-vectors $\mathbf{u}_j:=[u_{p_{j-1}+k}: k\in\mathbb{N}_{m_j}]$, $j\in\mathbb{N}_d$, and decompose $\mathbf{A}$ into $d$ sub-matrices $\mathbf{A}_{[j]}:=[\mathbf{A}_{(p_{j-1}+k)}:k\in \mathbb{N}_{m_j}]\in\mathbb{R}^{n\times m_j},\ j\in\mathbb{N}_d$. 
Moreover, for each $j\in\mathbb{N}_d$, we choose 
the nature partition $\mathcal{S}_{m_j,m_j}:=\left\{S_{m_j,1},S_{m_j,2},\ldots,S_{m_j,m_j}\right\}$ for $\mathbb{N}_{m_j}$. That is, $S_{m_j,k}:=\{p_{j-1}+k\}$, $ k\in\mathbb{N}_{m_j}$. Accordingly, we decompose vector $\mathbf{u}_j$ into $m_j$ sub-vectors $\mathbf{u}_{j,k}:=u_{p_{j-1}+k}$ and decompose matrix $\mathbf{A}_{[j]}$ into $m_j$ sub-matrices $
\mathbf{A}_{[j,k]}:=\mathbf{A}_{(p_{j-1}+k)}$, $ k\in\mathbb{N}_{m_j}$. 
It follows from the orthogonality of matrix $\mathbf{A}$ that conditions \eqref{S_nd_block_separable} and \eqref{S_mj_qj_block_separable} are satisfied. This allows us to choose the regularization parameters according to the strategy stated in Corollary \ref{block_separabel_parameter_choice}. We note that if $\mathbf{u}_{\bm{\lambda}}$ is a solution of problem \eqref{lasso_multi-parameter} with $\bm{\lambda}:=[\lambda_j:j\in\mathbb{N}_d]$, then for each $j\in\mathbb{N}_d$, the $\mathcal{S}_{m_j,m_j}$-block sparsity level of $(\mathbf{u}_{\bm{\lambda}})_j$ coincides with its sparsity level. 


In this experiment, we solve model \eqref{lasso_multi-parameter} by Algorithm \eqref{FPPA} with $\Phi(\mathbf{u}):=\sum_{j\in\mathbb{N}_d}\lambda_j\|\mathbf{u}_j\|_1$, $\Psi:=\frac{1}{2}\|\cdot-\mathbf{x}\|_2^2$ and $\mathbf{C}:=\mathbf{A}$.
The proximity operator $\mathrm{prox}_{\alpha\Phi}$ at $\mathbf{z}\in\mathbb{R}^n$ has the form $\mathrm{prox}_{\alpha\Phi}(\mathbf{z}):=\bm{\mu}$ with $\mu_{p_{j-1}+i}$, $j\in\mathbb{N}_d$, $i\in\mathbb{N}_{m_j}$ being defined by \eqref{proximity-operator-sum_lambda_j_mu}.
The proximity operator $\mathrm{prox}_{\frac{1}{\rho}\Psi}$ at $\mathbf{z}\in\mathbb{R}^n$ has the form  $\mathrm{prox}_{\frac{1}{\rho}\Psi}(\mathbf{z}):=\mathbf{w}$ with $\mathbf{w}:=[w_j:j\in\mathbb{N}_n]$ and  $w_j$, $j\in\mathbb{N}_n$ being defined by \eqref{2-norm-1}.

We first validate the  parameter choice strategy stated in 
Corollary \ref{block_separabel_parameter_choice}. We set three prescribed TSLs values $[64,41,50,58,61,66,60]$, $[64,51,68,86,108,116,107]$ and $[64,55,91,128,206,226,230]$. According to the strategy stated in Corollary \ref{block_separabel_parameter_choice}, we select the parameter $\bm{\lambda}^*:=[\lambda_j^*:j\in\mathbb{N}_d]$ with which model \eqref{lasso_multi-parameter} has solutions having the target sparsity levels. For each $j\in\mathbb{N}_{d}$, we set $\gamma_{j,k}:=\big|(\mathbf{A}_{[j,k]})^{\top}\mathbf{x}\big|$, $k\in\mathbb{N}_{m_j}$ and rearrange them in a nondecreasing order: $\gamma_{j,k_1}\leq \gamma_{j,k_2}\leq \cdots\leq \gamma_{j,k_{m_j}}$
with $\{k_1,  k_2, \ldots, k_{m_j}\}=\mathbb{N}_{m_j}$. We then choose $\lambda^*_j:=\gamma_{j,k_{m_j-l_j^*}}$ if $l^*_j<m_j$ and $\lambda^*_j:=0.001\gamma_{j,k_1}$ if $l^*_j=m_j$. We solve model \eqref{lasso_multi-parameter} with each selected value of $\bm{\lambda}^*$ for the corresponding solution $\mathbf{u}_{\bm{\lambda}^*}$ and determine the actual sparsity level SLs of $\mathbf{u}_{\bm{\lambda}^*}$.

We report in Table \ref{Multi_parameter_signal_DWT_block} the numerical results of this experiment: the targeted sparsity levels TSLs, the selected values of parameter $\bm{\lambda}^*$, the actual sparsity levels SLs of  $\mathbf{u}_{\bm{\lambda}^*}$, the Ratio values of $\mathbf{u}_{\bm{\lambda}^*}$ and the $\mathrm{MSE}$ values of the denoised signals $\mathbf{A}\mathbf{u}_{\bm{\lambda}^*}$. Here and in the next subsection, $\mathrm{MSE}:=\frac{1}{n}\|\mathbf{A}\mathbf{u}_{\bm{\lambda}^*}-\mathbf{x}\|_2^2$. Observing from Table \ref{Multi_parameter_signal_DWT_block}, the SLs values match with the TSLs values.  The numerical results in Table \ref{Multi_parameter_signal_DWT_block} demonstrate the efficacy of the strategy stated in Corollary \ref{block_separabel_parameter_choice} in selecting regularization parameters with which the corresponding solution achieves a desired sparsity level and preserves the approximation accuracy.

% Observing from the numerical results, the target sparsity levels in table \ref{Multi_parameter_signal_DWT_block} satisfying $l_1^*=m_1$, that is, all elements of $(\mathbf{u}^{\delta}_{\lambda^*})_1$ are nonzero. We remark that the sequence $(\mathbf{A}_{[j,k]})^{\top}\mathbf{x}^{\delta}$, $k\in\mathbb{N}_{m_1}$ stands for low-frequency wavelet coefficients. However, the parameter choices dependent on the sequence. If $(\mathbf{u}^{\delta}_{\lambda^*})_1$ is sparse, the signal will lose some low-frequency information in the process of denoising.

\begin{table}[ht]
% \vspace{-0.4cm}
\caption{\label{Multi_parameter_signal_DWT_block} Multi-parameter choices $\lambda^*_j:=\gamma_{j,k_{m_j-l_j^*}}$ for signal denoising (block separable)}
% \vspace{0.2cm}
\begin{indented}
\item[]\begin{tabular}{@{}ll|c|c|c}
\hline\hline
&TSLs &$[64,41,50,58,$  &$[64,51,68,86,$ &$[64,55,91,128,$  \\  
&  &$61,66,60]$ &$108,116,107]$ &$206,226,230]$  \\  
&$\bm{\lambda}^*$
&$[1.18,3.12,5.20,6.80,$  
&$[1.18,1.98,3.05,4.05,$ &$[1.18,1.26,1.79,2.62,$  \\ 
&  &$7.42,7.06,7.17]\times 10^{-5}$ &$4.82,5.60,6.15]\times 10^{-5}$
&$2.99,4.12,4.91]\times 10^{-5}$  \\
&$\mathrm{SLs}$  &$[64,41,50,58,$  &$[64,51,68,86,$ &$[64,55,91,128,$  \\  
&  &$61,66,60]$ &$108,116,107]$ &$206,226,230]$\\ &Ratio  &$9.77\% $ &$14.65\%$ &$24.41\%$ \\
&$\mathrm{MSE}$  &$3.50\times10^{-10}$ &$2.33\times10^{-10}$ &$2.04\times10^{-10}$  \\  
\hline\hline
\end{tabular}
\end{indented}
% \vspace{-0.3cm}
\end{table}

For the comparison purpose, we also consider the single-parameter regularization model \eqref{lasso_multi-parameter} with $d:=1$. Let $\gamma_k:=|(\mathbf{A}_{(k)})^{\top}\mathbf{x}|$, $k\in\mathbb{N}_n$, rearranged in a nondecreasing order: $\gamma_{k_1}\leq \gamma_{k_2}\leq \cdots \leq \gamma_{k_n}$ with $\{k_1,k_2,\cdots,k_n\}=\mathbb{N}_n$. For three TSL values $400$, $600$ and $1000$, we choose $\lambda^*:=\gamma_{k_{n-l^*}}$ according to Corollary \ref{block_separabel_parameter_choice} with $d:=1$. By solving model   \eqref{lasso_multi-parameter} with $d:=1$ by Algorithm \eqref{FPPA} with $\Phi(\mathbf{u}):=\lambda\|\mathbf{u}\|_1$, $\Psi:=\frac{1}{2}\|\cdot-\mathbf{x}\|_2^2$ and $\mathbf{C}:=\mathbf{A}\in\mathbb{R}^{n\times n}$, we obtain the solution $\mathbf{u}_{\lambda^*}$ and determine
the actual sparsity level SL of $\mathbf{u}_{\lambda^*}$. The targeted sparsity levels TSL,
the selected values of parameter $\lambda^*$, the actual sparsity levels $\mathrm{SL}$ of  $\mathbf{u}_{\lambda^*}$, the Ratio values of $\mathbf{u}_{\bm{\lambda}^*}$ and the $\mathrm{MSE}$ values of the denoised signals $\mathbf{A}\mathbf{u}_{\lambda^*}$ are reported in Table \ref{Single_parameter_signal_DWT_block}.
% Here, the $\mathrm{ERR}$ error is compared to the minimal norm solution 
% \begin{equation}\label{mini_norm}
% \mathbf{u}^{\dag}:=\argmin\{\|\mathbf{u}\|_1:\mathbf{A}\mathbf{u}
% =\mathbf{x},\mathbf{u}\in\mathbb{R}^n\}
% \end{equation}
% of the prediction problem. 
Observing from Tables \ref{Multi_parameter_signal_DWT_block} and   \ref{Single_parameter_signal_DWT_block}, compared with the single-parameter regularization model, the multi-parameter regularization model may provide a solution having better approximation
accuracy with the same sparsity level.

\begin{table}[ht]
% \vspace{-0.4cm}
\caption{\label{Single_parameter_signal_DWT_block} Single-parameter choices $\lambda^*:=\gamma_{k_{n-l^*}}$ for signal denoising (block separable)}
% \vspace{0.2cm}
\begin{indented}
\setlength{\tabcolsep}{7.2mm
\item[]\begin{tabular}{@{}cl|c|c|c}
\hline\hline
&TSL &$400$  &$600$ &$1000$  \\   
&${\lambda}^*$  &$6.99\times 10^{-5}$ &$5.34\times 10^{-5}$ &$4.02\times 10^{-5}$  \\   
&$\mathrm{SL}$ &$400$ &$600$ &$1000$  \\ 
&Ratio  &$9.77\% $ &$14.65\%$ &$24.41\%$ \\
&$\mathrm{MSE}$ &$4.64\times10^{-10}$  &$3.19\times10^{-10}$ &$2.57\times10^{-10}$  \\  
\hline\hline
\end{tabular}}
\end{indented}
% \vspace{-0.3cm}
\end{table}



\subsection{Parameter choices: a nonseparable case}
This experiment is devoted to validating the efficacy of Algorithm \ref{algo: iterative scheme picking lambda}. We again consider recovering
the Doppler signal function defined by \eqref{Doppler_signal} from its noisy data by the multi-parameter regularization model \eqref{lasso_multi-parameter}. The original signal $\mathbf{f}$ and the noisy signal $\mathbf{x}$ are chosen in the same way as in Subsection  \ref{NM_block_separable}.

In this experiment, the matrix $\mathbf{A}\in\mathbb{R}^{n\times n}$ is determined by the biorthogonal wavelet `bior2.2' available in Matlab with the coarsest resolution level $\mathrm{L}:=6$. In both the analysis and synthesis filters, `bior2.2' possesses 2 vanishing moments. Such  a matrix does not satisfies conditions \eqref{S_nd_block_separable} and \eqref{S_mj_qj_block_separable}. As a result, we choose the regularization parameters by employing Algorithm \ref{algo: iterative scheme picking lambda}. The number $d$ of the regularization parameters, the sub-vectors $\mathbf{u}_j$, $j\in\mathbb{N}_d$ of 
a vector $\mathbf{u}\in\mathbb{R}^n$ and the sub-matrix $\mathbf{A}_{[j]}$, $j\in\mathbb{N}_d$ of $\mathbf{A}$ are all defined as in Subsection  \ref{NM_block_separable}.
% We solve model \eqref{error_regur_model_q=2} using FPPA with $\mathrm{prox}_{\alpha\Phi}$ and $\mathrm{prox}_{\frac{1}{\rho}\Psi}$ at $\mathrm{z}\in\mathbb{R}^n$, as described in subsection \ref{NM_block_separable}, to obtain a numerical solution $\mathbf{u}_{\bm{\lambda}}^{\delta}$.

We set three prescribed TSLs values $[64,64,105,123,145,157,142]$, $[64,64,115,143,$
$191,208,215]$, $[64,64,124,237,295,389,427]$. We choose the parameters $\lambda_j^*$, $j\in\mathbb{N}_d$ by Algorithm \ref{algo: iterative scheme picking lambda} with $\bm{\psi}(\mathbf{u}):=\frac{1}{2}\|\mathbf{Au}-\mathbf{x}\|_2^2$, $\mathbf{B}=\mathbf{I}_n$, $\mathbf{B}'=\mathbf{I}_n$ and $\epsilon=7$. In Algorithm \ref{algo: iterative scheme picking lambda}, we choose the initial parameter $\lambda_j^0=\big\|\mathbf{A}_{[j]}^{\top}{\mathbf{x}}\big\|_{\infty}$, $j\in\mathbb{N}_d$ and solve model \eqref{lasso_multi-parameter} by Algorithm \eqref{FPPA} with $\Phi(\mathbf{u}):=\sum_{j\in\mathbb{N}_d}\lambda_j\|\mathbf{u}_j\|_1$, $\Psi:=\frac{1}{2}\|\cdot-\mathbf{x}\|_2^2$ and $\mathbf{C}:=\mathbf{A}$. The targeted sparsity levels TSLs, the selected values of parameter $\bm{\lambda}^*$ chosen by Algorithm \ref{algo: iterative scheme picking lambda}, the actual sparsity levels $\mathrm{SLs}$ of $\mathbf{u}_{\bm{\lambda}^*}$, the Ratio values of $\mathbf{u}_{\bm{\lambda}^*}$, the numbers $\mathrm{NUM}$ of iterations for $\bm{\lambda}^*$ and the $\mathrm{MSE}$ values of the denoised signals $\mathbf{A}\mathbf{u}_{\lambda^*}$ are reported in Table \ref{Multi_parameter_signal_DWT}. For the three TSLs values, the algorithm reaches the stopping criteria within $11$, $4$ and $15$ iterations, respectively. The SLs values obtained
by Algorithm \ref{algo: iterative scheme picking lambda} match with the TSLs values within tolerance error $\epsilon=7$. The numerical results in
Table \ref{Multi_parameter_signal_DWT}  validate the
efficacy of Algorithm \ref{algo: iterative scheme picking lambda} for obtaining regularization parameters leading to a solution with
desired sparsity level and approximation error.

\begin{table}[ht]
\caption{\label{Multi_parameter_signal_DWT} Multi-parameter choices by Algorithm \ref{algo: iterative scheme picking lambda} for
signal denoising (nonseparable)}
\begin{indented}
\item[]\begin{tabular}{@{}ll|c|c|c}
\hline\hline
&$\mathrm{TSLs}$ &$[64,64,105,123,$  &$[64,64,115,143,$ &$[64,64,124,237,$  \\  
&  &$145,157,142]$ &$191,208,215]$ &$295,389,427]$  \\     
&${\bm{\lambda}}^*$  &$[5.94,0.56,1.91,2.27,$ &$[5.94,0.56,1.06,1.40,$ &$[59.39,5.59,2.19,2.44,$  \\ 
& &$2.57,3.36,6.68]\times 10^{-4}$ &$1.12,1.54,2.82]\times 10^{-4}$ &$3.69,4.87,7.72]\times 10^{-5}$ \\
&$\mathrm{SLs}$  &$[64,64,105,124,$  &$[64,64,116,143,$ &$[64,64,124,237,$  \\  
&  &$147,156,141]$ &$194,210,216]$ &$294,388,426]$\\ 
&Ratio  &$19.56\% $ &$24.58\% $ &$38.99\% $ \\
&$\mathrm{NUM}$  &$11$ &$4$ &$15$  \\
&$\mathrm{MSE}$  &$4.81\times10^{-8}$ &$1.98\times10^{-8}$ &$9.93\times10^{-9}$  \\  
\hline\hline
% &$\mathrm{TSLs}$ &$[64,64,125,249,$  &$[64,64,127,253,$ &$[64,64,126,252,$  \\  
% &  &$409,508,581]$ &$483,812,997]$ &$505,966,1423]$  \\    
% &$\bm{\lambda}^*$
% &$[59.39,5.82,0.72,1.09,$  
% &$[59.39,5.80,0.55,0.52,$ &$[59.39,5.65,0.81,0.89,$  \\ 
% &  &$1.51,3.30,5.69]\times 10^{-5}$ &$0.38,1.12,3.34]\times 10^{-5}$
% &$0.10,0.28,1.80]\times 10^{-5}$  \\
% &$\mathrm{SLs}$  &$[64,64,127,249,$  &$[64,64,127,253,$ &$[64,64,126,251,$  \\  
% &  &$405,508,580]$ &$480,812,994]$ &$506,970,1423]$\\ 
% &Ratio  &$48.75\% $ &$68.21\% $ &$83.11\% $ \\
% &$\mathrm{NUM}$  &$15$ &$5$ &$8$  \\
% &$\mathrm{MSE}$  &$9.38\times10^{-9}$ &$9.13\times10^{-9}$ &$9.16\times10^{-9}$  \\  
% \hline\hline
\end{tabular}
\end{indented}
% \vspace{-0.3cm}
\end{table}

% We compare the numerical results in Table \ref{Multi_parameter_signal_DWT} with the single-parameter regularization model defined by \eqref{error_regur_model_q=2} with $d=1$. We set six TSL values $800$, $1000$, $1600$, $2000$, $2800$ and $3400$ and choose the parameter $\lambda^*$
% by Algorithm  \ref{algo: iterative scheme picking lambda} with $d=1$ and $\epsilon=1$. In this case, the initial parameter $\lambda^0$ is chosen as $\lambda^0:=\|\mathbf{A}^\top \mathbf{x}^{\delta}\|_{\infty}$. The targeted sparsity levels TSL,
% the selected values of parameter $\lambda^*$, the actual sparsity levels $\mathrm{SL}$ of  $\mathbf{u}^{\delta}_{\lambda^*}$, the numbers $\mathrm{NUM}$ of iterations for $\lambda^*$, the $\mathrm{MSE}$ values of the denoised signals $\mathbf{A}\mathbf{u}^{\delta}_{\lambda^*}$ and the $\mathrm{ERR}$ values of $\mathbf{u}^{\delta}_{\lambda^*}$ are listed in Table \ref{Single_parameter_signal_DWT}.

% \begin{table}[ht]
% % \vspace{-0.4cm}
% \caption{\label{Single_parameter_signal_DWT} Single-parameter choices by Algorithm \ref{algo: iterative scheme picking lambda} balancing sparsity and accuracy for
% signal denoising}
% % \vspace{0.2cm}
% \begin{indented}
% \setlength{\tabcolsep}{0.8mm
% \item[]\begin{tabular}{@{}cl|c|c|c|c|c|c}
% \hline\hline
% &$\mathrm{TSL}$
% &$800$  &$1000$ &$1600$ &$2000$  &$2800$ &$3400$ \\   
% &${\lambda}^*$  &$3.68\times 10^{-4}$ &$1.81\times 10^{-4}$ &$5.83\times 10^{-5}$ &$4.20\times 10^{-5}$ &$2.17\times 10^{-5}$ &$1.08\times 10^{-5}$ \\   
% &$\mathrm{SL}$ &$800$ &$999$ &$1600$ &$1999$ &$2800$ &$3400$  \\  
% &$\mathrm{NUM}$ &$6$ &$4$ &$5$ &$5$ &$4$ &$4$ \\  
% &$\mathrm{MSE}$ &$5.32\times10^{-8}$  &$1.61\times10^{-8}$ &$2.52\times10^{-9}$ &$1.58\times10^{-9}$  &$8.55\times10^{-10}$ &$7.36\times10^{-10}$  \\  
% &$\mathrm{ERR}$ &$0.7040$  &$0.4216$ &$0.1816$ &$0.1442$ &$0.1030$ &$0.0918$  \\  \hline\hline
% \end{tabular}}
% \end{indented}
% % \vspace{-0.3cm}
% \end{table}


\subsection{Compound sparse denoising}\label{Compound sparse denoising}
We consider in this experiment the parameter choice of the compound sparse denoising regularization model \eqref{CSD} implemented by Algorithm \ref{algo: iterative scheme picking lambda psi non-differentiable}. In this case, the fidelity term $\bm{\psi}$ defined by \eqref{fidelity-CSD} is differentiable and the transform matrix $\mathbf{B}:=\scriptsize{\begin{bmatrix}
\mathbf{I}_n\\
\mathbf{D}\end{bmatrix}}$ satisfies $rank(\mathbf{B})=n$. Clearly, $\mathbf{B}$ does not have full row rank.


In this experiment, the noisy data $\mathbf{y}:=[y(t):t\in\mathbb{N}_{n}]$ with $n=300$ is generated by adding a low-frequency sinusoid signal $f(t):=\sin(0.021\pi t)$, two additive step discontinuities $u(t)=2$ for  $t<0.3n$, $1$ for $t>0.6n$ and $0$ otherwise, and additive white Gaussian noise $\eta(t)$ with 0.3 standard deviation and zero mean. Set $\mathbf{f}:=[f(t):t\in\mathbb{N}_{n}]$ and $\mathbf{u}:=[u(t):t\in\mathbb{N}_{n}]$. We estimate $\mathbf{f}$ and $\mathbf{u}$ from the noisy data $\mathbf{y}$ by using simultaneous low-pass filtering and compound
sparse denoising. The estimate $\mathbf{u}^*$ of $\mathbf{u}$ is obtained by solving the compound
sparse denoising model \eqref{CSD}, where the $(n-4)\times n$ high-pass filter matrix $\mathbf{H}$ 
is chosen as in Example A of \cite{Selesnick2014Simultaneous}.
% the fourth-order high-pass filter $\mathbf{H}\in\mathbb{R}^{(n-2m)\times n}$ with $m=2$ and cut-off frequency $\omega_c=0.044\pi$. is chosen as in \cite{Selesnick2014Simultaneous}. 
The estimate $\mathbf{f}^*$ of  $f$ is then obtained by  $\mathbf{f}^*=(\tilde{\mathbf{I}}-\mathbf{H})(\mathbf{y}-\mathbf{u}^*)$, where $\tilde{\mathbf{I}}$ is the identity matrix of order $n$ with the first $2$ and last $2$ rows removed. To measure the filtering effect, we define the $\mathrm{MSE}$ by
$$
\mathrm{MSE}:=\frac{1}{n-4}\big\|\tilde{\mathbf{I}}(\mathbf{f}+\mathbf{u})-\mathbf{f}^*-\tilde{\mathbf{I}}\mathbf{u}^*\big\|_2^2.
$$
 
We employ Algorithm \ref{algo: iterative scheme picking lambda psi non-differentiable} to select multiple regularization parameters that ensures
the resulting solution of model \eqref{CSD} achieves given TSLs. In Algorithm \ref{algo: iterative scheme picking lambda psi non-differentiable}, we solve model \eqref{optimization_problem_under_Bj_3} by Algorithm \eqref{FPPA_w} where $d=2$, $\bm{\psi}$ has the form \eqref{fidelity-CSD}, $\mathbf{B}_{1}$ is the identity matrix of order $n$ and $\mathbf{B}_{2}$ is the $(n-1)\times n$ first order difference matrix. Recall that $p_d=2n-1$. We choose the real number $\theta:=1$ and the three matrices $\mathbf{O}:=\frac{1}{\alpha}\mathbf{I}_{p_d}$, $\mathbf{P}:=\frac{1}{\rho}\mathbf{I}_{n}$ and $\mathbf{Q}:=\frac{1}{\beta}\mathbf{I}_{p_d}$ with $\alpha$, $\rho$ and $\beta$ being positive real numbers. According to Theorem \ref{convergence_FPPA}, to ensure the convergence of Algorithm \eqref{FPPA_w}, we choose the real numbers $\alpha$, $\rho$ and $\beta$ satisfying $\left\|\scriptsize{\begin{bmatrix}
\sqrt{\alpha\rho}\mathbf{B}'\\
\sqrt{\alpha\beta}\mathbf{I}_{p_d}
\end{bmatrix}}\right\|_2<1$. In this case, Algorithm \eqref{FPPA_w} reduces to \eqref{Reduce_FPPA_w}. The closed-form formulas of the proximity operators $\mathrm{prox}_{\alpha\sum_{j\in\mathbb{N}_d}\lambda_j\|\cdot\|_{1}\circ\mathbf{I}^{'}_{j}}$, $\mathrm{prox}_{\frac{1}{\beta}\iota_{\mathbb{M}}}$ and $\mathrm{prox}_{\frac{1}{\rho}\bm{\psi}}$ with $\bm{\psi}$ being defined by  \eqref{fidelity-CSD} are given in \eqref{proximity-operator-sum_lambda_j}, \eqref{indicator_prox} and \eqref{2-norm-2}, respectively.


We set five prescribed $\mathrm{TSLs}$ values $[10,5]$, $[20,20]$, $[20,30]$, $[50,40]$ and $[80,60]$ and choose the regularization parameters $\lambda_j^*$, $j\in\mathbb{N}_d$ by employing Algorithm \ref{algo: iterative scheme picking lambda psi non-differentiable} with $\epsilon=2$. We report in Table \ref{filtering_problem} the targeted sparsity levels $\mathrm{TSLs}$, the initial values of $\bm{\lambda}^0$, the selected values of parameter $\bm{\lambda}^*$ chosen by Algorithm \ref{algo: iterative scheme picking lambda psi non-differentiable}, the actual sparsity levels $\mathrm{SLs}$ of $\mathbf{u}^*$, the Ratios values of $\mathbf{u}^*$, the numbers $\mathrm{NUM}$ of iterations for $\bm{\lambda}^*$ and the $\mathrm{MSE}$ values of the filtered signal. For the five TSLs values, the algorithm meets the stopping criteria after $8$, $8$, $6$, $2$, and $12$ iterations, respectively. The SLs values obtained by Algorithm \ref{algo: iterative scheme picking lambda psi non-differentiable} match
with the TSLs values within tolerance error $\epsilon=2$. The numerical results in
Table \ref{filtering_problem} demonstrate the efficacy of Algorithm \ref{algo: iterative scheme picking lambda psi non-differentiable} in selecting multiple regularization parameters
to achieve desired sparsity levels of the solution.

\begin{table}[ht]
\caption{\label{filtering_problem} Multi-parameter choices by Algorithm \ref{algo: iterative scheme picking lambda psi non-differentiable} for model \eqref{CSD}}
% \vspace{0.2cm}
\begin{indented}
\setlength{\tabcolsep}{0.1mm
\item[]\begin{tabular}{@{}ll|c|c|c|c|c}
\hline\hline
&TSLs  &$[10,5]$ &$[20,20]$
&$[20,30]$  &$[50,40]$ &$[80,60]$\\  
& $\bm{\lambda}^0$ &$[0.6,1.0]$ &$[0.5,0.5]$
&$[0.5,0.08]$  &$[0.5,1.0]$ &$[0.5,1.0]$\\ 
&$\bm{\lambda}^*$
&$[0.47,0.62]$  
&$[0.36,0.21]$ &$[0.49,0.07]$  
&$[0.24,0.20]$  
&$[0.20,0.17]$ \\ 
&$\mathrm{SLs}$  &$[9,6]$  &$[20,19]$ &$[20,29]$ 
&$[50,38]$ &$[80,60]$\\ 
&Ratios   &$[3.00\%, 2.01\%] $ &$[6.67\%, 6.35\%] $ &$[6.67\%, 9.70\%]$ &$[16.67\%, 12.71\%]$ &$[26.67\%, 20.07\%] $\\
&$\mathrm{NUM}$  &$8$  &$8$ &$6$ 
&$2$ &$12$\\ 
&$\mathrm{MSE}$  &$3.23\times10^{-2}$ &$2.18\times10^{-2}$ &$2.51\times10^{-2}$  
&$1.65\times10^{-2}$ &$1.57\times10^{-2}$ \\  
\hline\hline
\end{tabular}}
\end{indented}
% \vspace{-0.3cm}
\end{table}

\subsection{Fused SVM}\label{Fused SVM}
The goal of this experiment is to validate the parameter choice of the
fused SVM model \eqref{fused-SVM} implemented by Algorithm \ref{algo: iterative scheme picking lambda psi non-differentiable}. In model \eqref{fused-SVM}, the fidelity term  $\bm{\psi}(\mathbf{u}):=\bm{\phi}(\mathbf{YXu})$ is not differential and the transform matrix $\mathbf{B}:=\scriptsize{\begin{bmatrix}
\mathbf{I}_n\\
\mathbf{D}\end{bmatrix}}$ does not have full row rank.

The dataset utilized for this experiment is the set of handwriting digits sourced from the modified national institute of standards and technology (MNIST) database \cite{lecun1998gradient}. The original MNIST database consists of 60000 training samples and 10000 testing samples of the digits `0' through `9'. We consider the binary classification problem with
two digits `7' and `9', by taking 8141 training samples and 2037 testing samples of these two digits from the database. Let $p:=8141$ be the number of training samples and $\{y_j:j\in\mathbb{N}_p\}\subset\{-1,1\}$ be the labels of training data in which -1 and 1 represent the digits `7' and `9', respectively. In addition, let $n:=784$ be the the number of pixels in each sample. 


We implement Algorithm  \ref{algo: iterative scheme picking lambda psi non-differentiable} to select multiple regularization parameters with which the resulting solution of model \eqref{fused-SVM} achieves given TSLs. By choosing the real number $\theta$ and the matrices $\mathbf{O}$, $\mathbf{P}$, $\mathbf{Q}$ in the same way as in Subsection 6.3, we solve model \eqref{optimization_problem_under_Bj_3} by Algorithm \eqref{Reduce_FPPA_w} where $d=2$, $\bm{\psi}(\mathbf{u}):=\bm{\phi}(\mathbf{YXu})$, $\mathbf{B}_{1}$ is the identity matrix of order $n$ and $\mathbf{B}_{2}$ is the $(n-1)\times n$ first order difference matrix. It is worth noting that the proximity operator of function $\frac{1}{\rho}\bm{\psi}$ can not be expressed explicitly. Instead, we solve model \eqref{optimization_problem_under_Bj_3} by Algorithm \eqref{Reduce_FPPA_w} with $\bm{\psi}$ and $\mathbf{B}'$ being replaced by $\bm{\phi}$ and $\mathbf{YX}\mathbf{B}'$, respectively, and obtain at step k three vectors $\widetilde{\mathbf{w}}^k$, $\widetilde{\mathbf{a}}^k$ and $\widetilde{\mathbf{c}}^k$. Then the vectors $\mathbf{w}^k$, $\mathbf{a}^k$ and $\mathbf{c}^k$ emerging in Algorithm \ref{algo: iterative scheme picking lambda psi non-differentiable} can be obtained by $\mathbf{w}^k:=\widetilde{\mathbf{w}}^{k}$, $\mathbf{a}^k:=(\mathbf{YX})^{\top}\widetilde{\mathbf{a}}^k$ and $\mathbf{c}^k:=\widetilde{\mathbf{c}}^k$. Again by Theorem \ref{convergence_FPPA}, to guarantee the convergence of Algorithm \eqref{Reduce_FPPA_w}, we choose the real numbers $\alpha$, $\rho$ and $\beta$ in Algorithm \eqref{Reduce_FPPA_w} satisfying $\left\|\scriptsize{\begin{bmatrix}
\sqrt{\alpha\rho}\mathbf{YX}\mathbf{B}'\\
\sqrt{\alpha\beta}\mathbf{I}_{p_d}
\end{bmatrix}}\right\|_2<1$. 
The closed-form formulas of the proximity operators $\mathrm{prox}_{\alpha\sum_{j\in\mathbb{N}_d}\lambda_j\|\cdot\|_{1}\circ\mathbf{I}^{'}_{j}}$, $\mathrm{prox}_{\frac{1}{\beta}\iota_{\mathbb{M}}}$ and $\mathrm{prox}_{\frac{1}{\rho}\bm{\phi}}$ are given in \eqref{proximity-operator-sum_lambda_j}, \eqref{indicator_prox} and \eqref{max-function}, respectively.  

We set five prescribed TSLs values $[80,80]$, $[90,110]$, $[150,120]$, $[280,200]$ and $[360,400]$ and use Algorithm \ref{algo: iterative scheme picking lambda psi non-differentiable} with $\epsilon=2$ to select the regularization parameters $\lambda_j^*$,  $j\in\mathbb{N}_d$. The targeted sparsity levels $\mathrm{TSLs}$, the initial values of $\bm{\lambda}^0$, the selected values of parameter $\bm{\lambda}^*$ chosen by Algorithm \ref{algo: iterative scheme picking lambda psi non-differentiable}, the actual sparsity levels $\mathrm{SLs}$ of $\mathbf{u}^*$, the Ratios values of $\mathbf{u}^*$, the numbers $\mathrm{NUM}$ of iterations for $\bm{\lambda}^*$, the accuracy on the training datasets ($\mathrm{TrA}$) and the accuracy on the testing datasets ($\mathrm{TeA}$) are reported in Table \ref{fused_SVM_experiment}.  Algorithm  \ref{algo: iterative scheme picking lambda psi non-differentiable} meets the stopping criteria $\epsilon=2$ within 10, 13, 15, 15 and 8 iterations
for the five TSLs values, respectively. The SLs values obtained by Algorithm \ref{algo: iterative scheme picking lambda psi non-differentiable} match
with the TSLs values within tolerance error $\epsilon=2$. These results validate the
effectiveness of Algorithm \ref{algo: iterative scheme picking lambda psi non-differentiable} for obtaining multiple regularization parameters leading to a solution with 
desired sparsity levels.


\begin{table}[ht]
\caption{\label{fused_SVM_experiment} Multi-parameter choices by Algorithm \ref{algo: iterative scheme picking lambda psi non-differentiable} for model \eqref{fused-SVM}}
% \vspace{0.2cm}
\begin{indented}
{\scriptsize
\setlength{\tabcolsep}{1.5mm
\item[]\begin{tabular}{@{}l|c|c|c|c|cc}
\hline\hline
TSLs  &$[80,80]$ &$[90,110]$
&$[150,120]$  &$[280,200]$ &$[360,400]$\\  
$\bm{\lambda}^0$ &$[120,10]$ &$[70,2]$
&$[28,7]$  &$[2.5,3.0]$ &$[0.6,0.2]$\\ 
$\bm{\lambda}^*$
&$[119.01,9.99]$  
&$[68.80,1.99]$ &$[27.88,6.99]$  
&$[2.30,2.61]$  
&$[0.48,0.14]$ \\ 
$\mathrm{SLs}$  &$[80,78]$  &$[88,110]$ &$[148,120]$ 
&$[281,199]$ &$[360,399]$\\ 
Ratios   &$[10.20\%, 9.96\%] $ &$[11.22\%,14.05\%] $ &$[18.88\%, 15.33\%]$ &$[35.84\%, 25.42\%]$ &$[45.92\%, 50.96\%] $\\
$\mathrm{NUM}$  &$10$  &$13$ &$15 $ 
&$15$ &$8$\\ 
$\mathrm{TrA}$  &$93.58\%$ &$94.72\%$ &$95.23 \%$  
&$96.72\%$ &$ 97.75\%$ \\
$\mathrm{TeA}$  &$94.80\%$ &$95.43\%$ &$96.02\%$  
&$96.37\%$ &$96.27\%$ \\
\hline\hline
\end{tabular}}}
\end{indented}
% \vspace{-0.3cm}
\end{table}



%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

In this paper, we have explored strategies for selecting multiple regularization parameters in the multi-parameter regularization model with the $\ell_1$ norm. We established the relationship between the regularization parameters and the sparsity of the regularized solution under transform matrices. Leveraging this relationship, we developed an iterative algorithm to determine multiple regularization parameters, ensuring that the resulting regularized solution achieves prescribed sparsity levels under the transform matrices.
For scenarios where the fidelity term is nondifferentiable and the transform matrix lacks full row rank, we introduced a fixed-point proximity algorithm capable of simultaneously determining the regularized solution and two auxiliary vectors arising in the sparsity characterization. This algorithm served as a critical component in constructing the iterative parameter selection scheme.
Finally, numerical experiments demonstrated the effectiveness of the proposed multi-parameter selection strategies, confirming their ability to produce solutions with the desired sparsity and performance.


%We have investigated choice strategies of the multiple regularization parameters for the multi-parameter regularization
%model with the $\ell_1$ norm. We have revealed the relation between the multiple regularization parameter and the sparsity of the regularized solution under transform matrices. By employing
%the resulting relation, we have developed an iterative algorithm to determine the multiple
%regularization parameters such that the corresponding regularized solution attains prescribed sparsity levels under the transform matrices. For the case that the fidelity term lacks differentiability and the transform matrix fails to possess full row rank, we have devised a fixed-point proximity algorithm that can
%simultaneously determine the regularized solution together with two additional vectors emerging in
%the sparsity characterization of the regularized solution. Such an algorithm has played an important role in developing the iterative algorithm employed for the selection of the multiple regularization parameters. Numerical experiments have been performed to demonstrate the efficacy of the the proposed multi-parameter choice strategies.  



\bigskip
\noindent{\bf Acknowledgments.}
Q. Liu is supported in part by the Doctor Foundation of Henan University
of Technology, China (No.2023BS061), and the Innovative
Funds Plan of Henan University of Technology (No.2021ZKCJ11).
R. Wang is supported in part by the Natural Science Foundation of China under grant 12171202. Y. Xu is supported in part by the US National Science Foundation under grant DMS-2208386, and the US National Institutes of Health under grant R21CA263876.


\appendix
\section{Proof of Theorem \ref{convergence_FPPA}}
\renewcommand{\thetheorem}{A.\arabic{theorem}}

In this appendix, we present a complete proof for Theorem \ref{convergence_FPPA}, which shows the convergence of Algorithm \eqref{FPPA_w}.

We start with reviewing the notion of weakly firmly non-expansive operators introduced in \cite{li2015multi}. An operator $T:\mathbb{R}^{2s}\to \mathbb{R}^{s}$ is called weakly firmly non-expansive with respect to a set $\mathcal{M}:=\{\mathbf{M}_{0},\mathbf{M}_{1},\mathbf{M}_{2}\}$ of $s\times s $ matrices if for any $(\mathbf{s}^{i},\mathbf{z}^{i},\bm{\omega}^{i})\in\mathbb{R}^s\times\mathbb{R}^s\times\mathbb{R}^s$ satisfying $\bm{\omega}^{i}=T(\mathbf{s}^{i},\mathbf{z}^{i})$ for $i=1,2,$ there holds
\begin{equation}\label{weakly-firmly-non-expansive}
\big\langle\bm{\omega}^{2}-\bm{\omega}^{1}, \mathbf{M}_{0}(\bm{\omega}^{2}-\bm{\omega}^{1})
\big\rangle
\leq\big\langle\bm{\omega}^{2}-\bm{\omega}^{1},\mathbf{M}_{1}(\mathbf{s}^{2}-\mathbf{s}^{1})+\mathbf{M}_{2}(\mathbf{z}^{2}-\mathbf{z}^{1})\big\rangle.
\end{equation}
The graph ${\rm gra}(T)$ of operator $T$ is defined by 
$$
{\rm gra}(T):=\{(\mathbf{s},\mathbf{z},\bm{\omega}):(\mathbf{s},\mathbf{z},\bm{\omega})\in\mathbb{R}^s\times\mathbb{R}^s\times\mathbb{R}^s,\bm{\omega}=T(\mathbf{s},\mathbf{z})\}.
$$
We say the graph ${\rm gra}(T)$ of $T$
is a closed set if for any sequence $\{(\mathbf{s}^k,\mathbf{z}^k,\bm{\omega}^k)\in{\rm gra}(T):k\in\mathbb{N}\}$ converging
to $(\mathbf{s},\mathbf{z},\bm{\omega})$, there holds $(\mathbf{s},\mathbf{z},\bm{\omega})\in{\rm gra}(T)$.
Following \cite{li2015multi}, we also need the notion of Condition-$\mathbf{M}$. 
We say a set $\mathcal{M}:=\{\mathbf{M}_{0},\mathbf{M}_{1},\mathbf{M}_{2}\}$ of $s\times s $ matrices satisfies Condition-$\mathbf{M}$, if the following three hypotheses are satisfied :
(i) $\mathbf{M}_{0}=\mathbf{M}_{1}+\mathbf{M}_{2}$;
(ii) $\mathbf{H}:=\mathbf{M}_{0}+\mathbf{M}_{2}$ is in $\mathbb{S}^s_{+}$,
(iii) $\big\|\mathbf{H}^{-1/2}\mathbf{M}_{2}\mathbf{H}^{-1/2}\big\|_2<1/2$. 
% We next give a brief review of the definition and some properties of firmly non-expansive operators.
% Recall that an operator $\mathcal{J}:\mathbb{R}^s\to \mathbb{R}^s$ is called firmly non-expansive(non-expansive) with respect to a given matrix $\mathbf{H}\in\mathbb{S}_{+}^{s}$ if for all $\mathbf{x}$ and $\mathbf{y}\in\mathbb{R}^s$,
% \begin{equation*}
% \|\mathcal{J}\mathbf{x}-\mathcal{J}\mathbf{y}\|^2 _{\mathbf{H}} \leq \langle \mathcal{J}\mathbf{x}-\mathcal{J}\mathbf{y}, \mathbf{x}-\mathbf{y} \rangle_{\mathbf{H}} \ \ 
% (\|\mathcal{J}\mathbf{x}-\mathcal{J}\mathbf{y}\|_{\mathbf{H}} \leq \|\mathbf{x}-\mathbf{y}\|_{\mathbf{H}}).
% \end{equation*} 

The next result established in \cite{li2015multi} demonstrates that if a  operator $T$ with closed graph is weakly firmly non-expansive with respect to a set $\mathcal{M}:=\{\mathbf{M}_{0},\mathbf{M}_{1},\mathbf{M}_{2}\}$ of $s\times s $ matrices  satisfies Condition-$\mathbf{M}$, then a
sequence $\{\bm{\omega}^{k}:k\in\mathbb{N}\}$ generated by 
\begin{equation}\label{fixed-point_T}
\bm{\omega}^{k+1}=T(\bm{\omega}^{k},\bm{\omega}^{k-1})
\end{equation}
converges to a fixed point $\bm{\omega}$ of $T$, that is, $\bm{\omega}=T(\bm{\omega},\bm{\omega})$.
\begin{lemma}\label{converges_weakly-T}
Suppose that a set $\mathcal{M}:=\{\mathbf{M}_{0},\mathbf{M}_{1},\mathbf{M}_{2}\}$ of $s\times s $ matrices  satisfies Condition-$\mathbf{M}$,  the operator $T:\mathbb{R}^{2s}\to \mathbb{R}^{s}$ is weakly firmly non-expansive with respect to $\mathcal{M}$,  the set of fixed-points of $T$ is nonempty and $\mathrm{dom}(T)=\mathbb{R}^{2s}$. If the sequence $\{\bm{\omega}^{k}:k\in\mathbb{N}\}$ is generated by 
\eqref{fixed-point_T} for any given $\bm{\omega}^{0}$, $\bm{\omega}^{1}\in\mathbb{R}^{s}$, then $\{\bm{\omega}^{k}:k\in\mathbb{N}\}$ converges. Moreover, if the graph ${\rm gra}(T)$ of $T$
is closed, then $\{\bm{\omega}^{k}:k\in\mathbb{N}\}$ converges to a fixed-point of $T$.    
\end{lemma}

Below, we establish the convergence of Algorithm \eqref{FPPA_w} by employing Lemma \ref{converges_weakly-T}. For this purpose, we construct a new operator from operator $\mathcal{P}\circ\mathbf{E}$. Let $\mathbf{M}_{0}$, $\mathbf{M}_{1}$, $\mathbf{M}_{2}$ be $(2p_d+3n-2r)\times(2p_d+3n-2r)$ matrices defined by \eqref{matrix_M0}, \eqref{matrix_M1} and \eqref{matrix_M2}, respectively. Associated with the set  $\mathcal{M}:=\{\mathbf{M}_{0},\mathbf{M}_{1},\mathbf{M}_{2}\}$, we define an operator  $\mathcal{T}_{\mathcal{M}}:\mathbb{R}^{2p_d+3n-2r}\times\mathbb{R}^{2p_d+3n-2r}\to\mathbb{R}^{2p_d+3n-2r}$ for any  $(\mathbf{s},\mathbf{z})\in\mathbb{R}^{2p_d+3n-2r}\times\mathbb{R}^{2p_d+3n-2r}$ by $\bm{\omega}:=\mathcal{T}_{\mathcal{M}}(\mathbf{s},\mathbf{z})$ with $\bm{\omega}$ satisfying 
\begin{equation}\label{iterative_TMtu}
\bm{\omega}
=\mathcal{P}((\mathbf{E}-\mathbf{R}^{-1}\mathbf{M}_{0})\bm{\omega}
+\mathbf{R}^{-1}\mathbf{M}_{1}\mathbf{s}
+\mathbf{R}^{-1}\mathbf{M}_{2}\mathbf{z}).    
\end{equation}
We note that the operator $\mathcal{T}_{\mathcal{M}}$ is well-defined. To see this, we decompose any  $\mathbf{x}\in\mathbb{R}^{2p_d+3n-2r}$ as $\mathbf{x}:=\scriptsize{\begin{bmatrix}
\mathbf{x}_1\\
\mathbf{x}_2\\
\mathbf{x}_3
\end{bmatrix}}$ with $\mathbf{x}_1, \mathbf{x}_3\in\mathbb{R}^{p_d+n-r}$ and  $\mathbf{x}_2\in\mathbb{R}^{n}$. By using  representations  \eqref{matrix_M0_1},\eqref{matrix_M1_1} and \eqref{matrix_M2_1}, we rewrite equation \eqref{iterative_TMtu}  as
\begin{equation}\label{well-defined}
\left\{\begin{array}{l}
\bm{\omega}_{1}=\mathrm{prox}_{\sum_{j\in\mathbb{N}_d}\lambda_j\|\cdot\|_{1}\circ\mathbf{I}^{'}_{j},\mathbf{O}}
\big(\mathbf{s}_{1}+(\theta-2)\mathbf{O}^{-1}(\mathbf{B}')^{\top}\mathbf{s}_{2}+(\theta-2)\mathbf{O}^{-1}\mathbf{s}_{3}\\
 \quad\qquad+(1-\theta)\mathbf{O}^{-1}(\mathbf{B}')^{\top}\mathbf{z}_{2}+(1-\theta)\mathbf{O}^{-1}\mathbf{z}_{3}, \\
\bm{\omega}_{2}=\mathrm{prox}_{\bm{\psi}^*,\mathbf{P}}
\big(\mathbf{s}_{2}+(1+\theta)\mathbf{P}^{-1}\mathbf{B}'\bm{\omega}_{1}-\theta\mathbf{P}^{-1}\mathbf{B}'\mathbf{s}_{1}\big),\\
\bm{\omega}_{3}=\mathrm{prox}_{\iota_{\mathbb{M}}^*,\mathbf{Q}}\big(\mathbf{s}_{3}+(1+\theta)\mathbf{Q}^{-1}\bm{\omega}_{1}-\theta\mathbf{Q}^{-1}\mathbf{s}_{1}\big). 
\end{array}\right.
\end{equation}
Clearly, for any $(\mathbf{s},\mathbf{z})\in\mathbb{R}^{2p_d+3n-2r}\times\mathbb{R}^{2p_d+3n-2r}$, there exists a unique $\bm{\omega}\in\mathbb{R}^{2p_d+3n-2r}$ satisfying equation \eqref{well-defined}.

Observing from equation \eqref{iterative_TMtu}, we have that operator $\mathcal{T}_{\mathcal{M}}$ has the same fixed-point set as $\mathcal{P}\circ\mathbf{E}$. With the help
of operator $\mathcal{T}_{\mathcal{M}}$, we represent equation  \eqref{two-step_iterative} in an explicit form as
\begin{equation}\label{iterative_TM}
\mathbf{v}^{k+1}
=\mathcal{T}_{\mathcal{M}}(\mathbf{v}^{k},
\mathbf{v}^{k-1})  
\end{equation}
and obtain a fixed-point of $\mathcal{T}_{\mathcal{M}}$ by this iteration. 
According to Lemma \ref{converges_weakly-T}, in order to obtain the convergence of iterative scheme \eqref{iterative_TM}, it suffices to prove that operator $\mathcal{T}_{\mathcal{M}}$ defined by \eqref{iterative_TMtu} is weakly firmly non-expansive with respect
to $\mathcal{M}$ and the graph ${\rm gra}(\mathcal{T}_{\mathcal{M}})$ of $\mathcal{T}_{\mathcal{M}}$
is closed, and in addition, the set $\mathcal{M}$ satisfies Condition-$\mathbf{M}$. 
We first show the properties of operator $\mathcal{T}_{\mathcal{M}}$. For this purpose, we introduce a $(2p_d+3n-2r)\times(2p_d+3n-2r)$ skew-symmetric matrix $\mathbf{S}_{\mathbf{B}}$ as
\begin{equation*}
\mathbf{S}_{\mathbf{B}}:=\begin{bmatrix}
\mathbf{0} &-(\mathbf{B}')^{\top} &-\mathbf{I}_{p_d+n-r}\\
\mathbf{B}' &\mathbf{0} 
&\mathbf{0}\\
\mathbf{I}_{p_d+n-r} 
& \mathbf{0} & \mathbf{0} 
\end{bmatrix}.    
\end{equation*}
and then represent matrix $\mathbf{E}$ as $\mathbf{E}=\mathbf{I}_{2p_d+3n-2r}+\mathbf{R}^{-1}\mathbf{S}_{\mathbf{B}}$. 

\begin{proposition}\label{TM-continuous}
Let $\mathbf{M}_{0}$, $\mathbf{M}_{1}$, $\mathbf{M}_{2}$ be $(2p_d+3n-2r)\times(2p_d+3n-2r)$ matrices defined by \eqref{matrix_M0}, \eqref{matrix_M1} and \eqref{matrix_M2}, respectively, and  $\mathcal{M}:=\{\mathbf{M}_{0},\mathbf{M}_{1},\mathbf{M}_{2}\}$. If  $\mathcal{T}_{\mathcal{M}}$ is defined by equation \eqref{iterative_TMtu}, then $\mathcal{T}_{\mathcal{M}}$ is weakly firmly non-expansive with respect to $\mathcal{M}$ and the graph ${\rm gra}(\mathcal{T}_{\mathcal{M}})$ of $\mathcal{T}_{\mathcal{M}}$
is closed.
\end{proposition}
\begin{proof}
We first prove that $\mathcal{T}_{\mathcal{M}}$ is weakly firmly non-expansive with respect to $\mathcal{M}$. It suffices to prove that for any $(\mathbf{s}^{i},\mathbf{z}^{i},\bm{\omega}^{i})\in\mathbb{R}^{2p_d+3n-2r}\times\mathbb{R}^{2p_d+3n-2r}\times\mathbb{R}^{2p_d+3n-2r}$ satisfying $\bm{\omega}^{i}=\mathcal{T}_{\mathcal{M}}(\mathbf{s}^{i},\mathbf{z}^{i})$ for $i=1,2,$ there holds equation \eqref{weakly-firmly-non-expansive}. It follows from definition \eqref{iterative_TMtu} of $\mathcal{T}_{\mathcal{M}}$ that 
\begin{equation}\label{w-s-z}
\bm{\omega}^{i}=\mathcal{P}\big((\mathbf{E}-\mathbf{R}^{-1}\mathbf{M}_{0})\bm{\omega}^{i}
+\mathbf{R}^{-1}\mathbf{M}_{1}\mathbf{s}^{i}
+\mathbf{R}^{-1}\mathbf{M}_{2}\mathbf{z}^{i}\big),\ i=1,2.
\end{equation}
By arguments similar to those used in the proof of Lemma 3.1 of \cite{li2015multi}, we have that operator $\mathcal{P}$ is firmly non-expansive with respect to $\mathbf{R}$, that is, for all $\mathbf{x}$, $\mathbf{y}\in\mathbb{R}^{2p_d+3n-2r}$,
\begin{equation*}
\|\mathcal{P}\mathbf{x}-\mathcal{P}\mathbf{y}\|^2 _{\mathbf{R}} \leq \langle \mathcal{P}\mathbf{x}-\mathcal{P}\mathbf{y}, \mathbf{x}-\mathbf{y} \rangle_{\mathbf{R}}.
\end{equation*} 
As a result, we get by equation \eqref{w-s-z} that  
\begin{equation*}
\|\bm{\omega}^{2}-\bm{\omega}^{1}\|^2_{\mathbf{R}}
\leq \big\langle\bm{\omega}^{2}-\bm{\omega}^{1},(\mathbf{R}\mathbf{E}-\mathbf{M}_{0})(\bm{\omega}^{2}-\bm{\omega}^{1})+\mathbf{M}_{1}(\mathbf{s}^{2}-\mathbf{s}^{1})+\mathbf{M}_{2}(\mathbf{z}^{2}-\mathbf{z}^{1}) \big\rangle. 
\end{equation*}
Substituting $\mathbf{R}\mathbf{E}=\mathbf{R}+\mathbf{S}_{\mathbf{B}}$ into the right hand side of the inequality above, we obtain that 
\begin{equation*}
\big\langle\bm{\omega}^{2}-\bm{\omega}^{1},
\mathbf{M}_{0}(\bm{\omega}^{2}-\bm{\omega}^{1})
\big\rangle
\leq \big\langle\bm{\omega}^{2}-\bm{\omega}^{1},\mathbf{S}_{\mathbf{B}}(\bm{\omega}^{2}-\bm{\omega}^{1})+\mathbf{M}_{1}(\mathbf{s}^{2}-\mathbf{s}^{1})+\mathbf{M}_{2}(\mathbf{z}^2-\mathbf{z}^1) \big\rangle. 
\end{equation*}
This together with the fact that  $\langle\bm{\omega}^{2}-\bm{\omega}^{1},
\mathbf{S}_{\mathbf{B}}(\bm{\omega}^{2}-\bm{\omega}^{1})\rangle=0$ leads to equation \eqref{weakly-firmly-non-expansive}. 
Consequently, we conclude that $\mathcal{T}_{\mathcal{M}}$ is weakly firmly non-expansive with respect to $\mathcal{M}$.

It remains to show the closedness of the graph of operator  $\mathcal{T}_{\mathcal{M}}$. For any sequence 
$\big\{(\mathbf{s}^{k},
\mathbf{z}^{k},\bm{\omega}^{k})\in{\rm gra}(\mathcal{T}_{\mathcal{M}}):k\in\mathbb{N}\big\}
$ converging to $(\mathbf{s},\mathbf{z},\bm{\omega})\in\mathbb{R}^{2p_d+3n-2r}\times\mathbb{R}^{2p_d+3n-2r}\times\mathbb{R}^{2p_d+3n-2r}$, we obtain from definition \eqref{iterative_TMtu} of $\mathcal{T}_{\mathcal{M}}$ that 
\begin{equation*}
\bm{\omega}^{k}=\mathcal{P}((\mathbf{E}-\mathbf{R}^{-1}\mathbf{M}_{0})\bm{\omega}^{k}
+\mathbf{R}^{-1}\mathbf{M}_{1}\mathbf{s}^{k}
+\mathbf{R}^{-1}\mathbf{M}_{2}\mathbf{z}^{k}). 
\end{equation*}
Associated with $(\mathbf{s},\mathbf{z},\bm{\omega})$, we introduce a vector $\widetilde{\bm{\omega}}\in\mathbb{R}^{2p_d+3n-2r}$ as   
$$
\widetilde{\bm{\omega}}:=\mathcal{P}((\mathbf{E}-\mathbf{R}^{-1}\mathbf{M}_{0})\bm{\omega}
+\mathbf{R}^{-1}\mathbf{M}_{1}\mathbf{s}
+\mathbf{R}^{-1}\mathbf{M}_{2}\mathbf{z}). 
$$
Since $\mathcal{P}$ is firmly non-expansive with respect $\mathbf{R}$, there holds for any $k\in\mathbb{N}$
\begin{equation*}
\|\bm{\omega}^{k}-\widetilde{\bm{\omega}}\|^2_{\mathbf{R}}
\leq \big\langle \bm{\omega}^{k}-\widetilde{\bm{\omega}},(\mathbf{R}-\mathbf{M}_{0})(\bm{\omega}^{k}-\bm{\omega})+\mathbf{M}_{1}(\mathbf{s}^{k}-\mathbf{s})+\mathbf{M}_{2}(\mathbf{z}^{k}-\mathbf{z})\big\rangle.
\end{equation*}
By letting $k\rightarrow+\infty$ with noting that $\mathbf{s}^k\rightarrow\mathbf{s}$ and $\mathbf{z}^k\rightarrow\mathbf{z}$ as $k\rightarrow+\infty$, we get $\lim_{k\to+\infty}\bm{\omega}^{k}=\widetilde{\bm{\omega}}$. As a result, $\bm{\omega}=\widetilde{\bm{\omega}}$. This together with the definition of $\widetilde{\bm{\omega}}$ leads directly to 
$\bm{\omega}=\mathcal{T}_{\mathcal{M}}(\mathbf{s},\mathbf{z})$. Therefore, the graph ${\rm gra}(\mathcal{T}_{\mathcal{M}})$ of $\mathcal{T}_{\mathcal{M}}$
is closed.
\end{proof}

The next proposition reveals that the set $\mathcal{M}$ satisfies Condition-$\mathbf{M}$.

\begin{proposition}\label{satisfy-M-condition-proposition}
Let $\mathbf{M}_{0}$, $\mathbf{M}_{1}$, $\mathbf{M}_{2}$ be $(2p_d+3n-2r)\times(2p_d+3n-2r)$ matrices defined by \eqref{matrix_M0}, \eqref{matrix_M1} and \eqref{matrix_M2}, respectively. If
conditions \eqref{satisfy-M-condition} and \eqref{satisfy-M-condition-H} are satisfied,
then the set $\mathcal{M}:=\{\mathbf{M}_{0}$, $\mathbf{M}_{1}$, $\mathbf{M}_{2}\}$ satisfies Condition-$\mathbf{M}$.
\end{proposition}
\begin{proof}
It is clear  that  $\mathbf{M}_{0}=\mathbf{M}_{1}+\mathbf{M}_{2}$, that is, Item (i) of Condition-$\mathbf{M}$ holds. To show the validity of Item (ii), we set $\mathbf{H}:=\mathbf{M}_{0}+\mathbf{M}_{2}$. By introducing block matrices $\mathbf{F}$ and $\mathbf{G}$ as in \eqref{F-G}, we represent $\mathbf{H}$ as 
\begin{equation*}
\mathbf{H}
% =\begin{bmatrix}
% \mathbf{O} &-\theta(\mathbf{B}')^{\top} &-\theta\mathbf{I}_{p_d+n-r} \\
% -\theta\mathbf{B}' &\mathbf{P} 
% &\mathbf{0}\\
% -\theta\mathbf{I}_{p_d+n-r} &\mathbf{0}
% & \mathbf{Q} 
%\end{bmatrix}
=\begin{bmatrix}
&\mathbf{O} &-\theta\mathbf{G}^{\top}\\
&-\theta\mathbf{G} &\mathbf{F}
\end{bmatrix}.
\end{equation*}
It follows from condition \eqref{satisfy-M-condition} that
\begin{equation}\label{inequality-norm-FGO}
\big\| 
\mathbf{F}^{-\frac{1}{2}}(-\theta\mathbf{G})
\mathbf{O}^{-\frac{1}{2}}
\big\|_2
% =|\theta|\big\| 
% \mathbf{F}^{-\frac{1}{2}}\mathbf{G}
% \mathbf{O}^{-\frac{1}{2}}
% \big\|_2
<1.
\end{equation}
which guaranteed by Lemma 6.2 in \cite{li2015multi} is equivalent to 
$\mathbf{H}\in\mathbb{S}_{+}^{2p_d+3n-2r}$. Hence, we get Item (ii) of Condition-$\mathbf{M}$.

It remains to verify Item (iii) of Condition-$\mathbf{M}$. Lemma 6.2 in \cite{li2015multi} ensures that if inequality \eqref{inequality-norm-FGO} holds, the norm of matrix $\mathbf{H}^{-1}$ can be estimated by  
\begin{equation}\label{norm_H_inverse}
\|\mathbf{H}^{-1}\|_2\leq 
\frac{\max\big\{\|\mathbf{O}^{-1}\|_2,
\left\|\mathbf{F}^{-1}
\right\|_2\big\}}{1-|\theta|\big\| 
\mathbf{F}^{-\frac{1}{2}}\mathbf{G}
\mathbf{O}^{-\frac{1}{2}}
\big\|_2}.
\end{equation}
We observe that 
$$
\mathbf{M}_{2}^{\top}\mathbf{M}_{2}
=(1-\theta)^2\begin{bmatrix}
&\mathbf{0} &\mathbf{0} \\
&\mathbf{0} &\mathbf{G}\mathbf{G}^{\top} 
\end{bmatrix},
$$
which yields that \begin{equation}\label{norm_M_2}
\|\mathbf{M}_{2}\|_2
=|1-\theta|\|\mathbf{G}\|_2.
\end{equation} 
It follows that 
\begin{equation}\label{AAequation}
    \big\|\mathbf{H}^{-1/2}\mathbf{M}_{2}\mathbf{H}^{-1/2}
\big\|_2 \leq \big\|\mathbf{H}^{-1/2}
\big\|_2^2\big\|\mathbf{M}_{2}\big\|_2.
\end{equation}
Substituting inequality \eqref{norm_H_inverse} and equation \eqref{norm_M_2} into inequality \eqref{AAequation} yields that \begin{equation*}
\big\|\mathbf{H}^{-1/2}\mathbf{M}_{2}\mathbf{H}^{-1/2}
\big\|_2 \leq \frac{|1-\theta|\|\mathbf{G}\|_2\max\big\{\|\mathbf{O}^{-1}\|_2,
\left\|\mathbf{F}^{-1}
\right\|_2\big\}}{1-|\theta|\big\| 
\mathbf{F}^{-\frac{1}{2}}\mathbf{G}
\mathbf{O}^{-\frac{1}{2}}
\big\|_2},
\end{equation*}
which together with condition  \eqref{satisfy-M-condition-H} further leads to Item (iii) of Condition-$\mathbf{M}$.
\end{proof}

Combining Lemma \ref{converges_weakly-T}, Propositions \ref{TM-continuous} and \ref{satisfy-M-condition-proposition}, we are ready to provide in the following proof of Theorem \ref{convergence_FPPA}.

\noindent {\bf Proof of Theorem \ref{convergence_FPPA}:}\ \,
Proposition \ref{TM-continuous} ensures that $\mathcal{T}_{\mathcal{M}}$ is weakly firmly non-expansive with respect to $\mathcal{M}$ and the graph ${\rm gra}(\mathcal{T}_{\mathcal{M}})$ of $\mathcal{T}_{\mathcal{M}}$
is closed. Moreover, the set $\mathcal{M}$, guaranteed by Proposition \ref{satisfy-M-condition-proposition}, satisfies Condition-$\mathbf{M}$. That is, the hypotheses of Lemma  \ref{converges_weakly-T} are satisfied. By Lemma  \ref{converges_weakly-T}, we get that  
the sequence $\{\mathbf{v}^{k}:k\in\mathbb{N}\}$,   generated by 
\eqref{iterative_TM} for any given $\mathbf{v}^{0}$, $\mathbf{v}^{1}\in\mathbb{R}^{2p_d+3n-2r}$,  converges to a fixed-point of $\mathcal{T}_{\mathcal{M}}$. Note that Algorithm \eqref{FPPA_w} has the equivalent form \eqref{iterative_TM} and operator $\mathcal{T}_{\mathcal{M}}$ has the same fixed-point set as $\mathcal{P}\circ\mathbf{E}$. Consequently, we conclude that the sequence $\{\mathbf{v}^{k}:k\in\mathbb{N}\}$, generated by Algorithm \eqref{FPPA_w} for any given $\mathbf{v}^0, \mathbf{v}^1\in\mathbb{R}^{2p_d+3n-2r}$, converges to a fixed-point of operator $\mathcal{P}\circ\mathbf{E}$.

\section{The closed-form formulas of proximity operators}
\renewcommand{\thetheorem}{B.\arabic{theorem}}
In this appendix, we provide the closed-form formulas for the proximity operators corresponding to the functions utilized in Algorithm \eqref{FPPA_w}.

As in numerical experiments, the real number $\theta$ and the matrices $\mathbf{O}$, $\mathbf{P}$, $\mathbf{Q}$ are chosen as 
$$
\theta:=1,\ \mathbf{O}:=\frac{1}{\alpha}\mathbf{I}_{p_d+n-r},\ \mathbf{P}:=\frac{1}{\rho}\mathbf{I}_{n},\ \mathbf{Q}:=\frac{1}{\beta}\mathbf{I}_{p_d+n-r}
$$ 
with $\alpha$, $\rho$ and $\beta$ being positive real numbers. It is known \cite{Bauschke2011} that for any convex function $f$ from $\mathbb{R}^s$ to $\overline{\mathbb{R}}$ and $\mu>0$
$$
\mathrm{prox}_{f^*,\mu\mathbf{I}_s}=\mathrm{prox}_{\frac{1}{\mu}f^*}
\ \
\text{and} 
\ \ 
\mathrm{prox}_{\frac{1}{\mu} f^*}
=\frac{1}{\mu} (\mathbf{I}_s-\mathrm{prox}_{\mu f})\circ(\mu\mathbf{I}_s).
$$
Accordingly,  Algorithm \eqref{FPPA_w} reduces to
\begin{equation}\label{Reduce_FPPA_w}
\left\{\begin{array}{l}
\mathbf{w}^{k+1}=\mathrm{prox}_{\alpha\sum_{j\in\mathbb{N}_d}\lambda_j\|\cdot\|_{1}\circ\mathbf{I}^{'}_{j}}
\big(\mathbf{w}^{k}-\alpha(\mathbf{B}')^{\top}\mathbf{a}^{k}-\alpha\mathbf{c}^{k}\big), \\
\ \mathbf{a}^{k+1}=\rho\big(\mathbf{I}_n-\mathrm{prox}_{\frac{1}{\rho}\bm{\psi}}\big)
\big(\frac{1}{\rho}\mathbf{a}^{k}+\mathbf{B}'(2\mathbf{w}^{k+1}-\mathbf{w}^{k})\big),\\
\ 
\mathbf{c}^{k+1}=
\beta\big(\mathbf{I}_{p_d+n-r}-\mathrm{prox}_{\frac{1}{\beta}\iota_{\mathbb{M}}}\big)\big(\frac{1}{\beta}\mathbf{c}^{k}+2\mathbf{w}^{k+1}-\mathbf{w}^{k}\big).
\end{array}\right.
\end{equation}

We consider computing the three proximity operators in \eqref{Reduce_FPPA_w} explicitly. We begin with the proximity operator $\mathrm{prox}_{\alpha\sum_{j\in\mathbb{N}_d}\lambda_j\|\cdot\|_{1}\circ\mathbf{I}^{'}_{j}}$ with $\alpha>0$, $\lambda_j>0$, $j\in\mathbb{N}_d$   and $\mathbf{I}^{'}_{j}$, $j\in\mathbb{N}_d$ being defined by \eqref{I-j}.

\begin{proposition}
If $\alpha>0$, $\lambda_j>0$, $j\in\mathbb{N}_d$   and $\mathbf{I}^{'}_{j}$, $j\in\mathbb{N}_d$ being defined by \eqref{I-j}, then the proximity operator $\mathrm{prox}_{\alpha\sum_{j\in\mathbb{N}_d}\lambda_j\|\cdot\|_{1}\circ\mathbf{I}^{'}_{j}}$ at $
\scriptsize{\begin{bmatrix}
\mathbf{z}\\
\mathbf{v}\end{bmatrix}}\in\mathbb{R}^{p_d+n-r}$ with $\mathbf{z}:=[z_{k}:k\in\mathbb{N}_{p_d}]\in\mathbb{R}^{p_d}$ and $\mathbf{v}\in\mathbb{R}^{n-r}$ has the form 
\begin{equation}\label{proximity-operator-sum_lambda_j}
\mathrm{prox}_{\alpha\sum_{j\in\mathbb{N}_d}\lambda_j\|\cdot\|_{1}\circ\mathbf{I}^{'}_{j}}\left({\scriptsize{\begin{bmatrix}
\mathbf{z}\\
\mathbf{v}\end{bmatrix}}}\right):={\scriptsize{\begin{bmatrix}\bm{\mu}\\
\mathbf{v}\end{bmatrix}}}
\ \mbox{with}\ \bm{\mu}:=[\mu_{k}: k\in\mathbb{N}_{p_d}]\in\mathbb{R}^{p_d},
\end{equation}
where for each $j\in\mathbb{N}_d$ and $i\in\mathbb{N}_{m_j}$
\begin{equation}\label{proximity-operator-sum_lambda_j_mu}
\mu_{p_{j-1}+i}=
\left\{\begin{array}{ll}
z_{p_{j-1}+i}-\alpha\lambda_j, \ &\mathrm{if}\ z_{p_{j-1}+i}>\alpha\lambda_j,\\
z_{p_{j-1}+i}+\alpha\lambda_j, \ & \mathrm{if}\ z_{p_{j-1}+i}<-\alpha\lambda_j,\\
 0, \ & \mathrm{if}\ z_{p_{j-1}+i}\in[-\alpha\lambda_j,\alpha\lambda_j].
\end{array}\right.
\end{equation} 
\end{proposition}
\begin{proof}
For each $\mathbf{z}:=[z_{k}:k\in\mathbb{N}_{p_d}]\in\mathbb{R}^{p_d}$ and each $\mathbf{v}\in\mathbb{R}^{n-r}$, we set $\mathrm{prox}_{\alpha\sum_{j\in\mathbb{N}_d}\lambda_j\|\cdot\|_{1}\circ\mathbf{I}^{'}_{j}}\left({\scriptsize{\begin{bmatrix}
\mathbf{z}\\
\mathbf{v}\end{bmatrix}}}\right):={\scriptsize{\begin{bmatrix}\bm{\mu}\\
\bm{\nu}\end{bmatrix}}}$ with $ \bm{\mu}:=[\mu_{k}: k\in\mathbb{N}_{p_d}]\in\mathbb{R}^{p_d}$ and $\bm{\nu}\in\mathbb{R}^{n-r}$. It follows from definition \eqref{proximity operator} of the proximity operator that 
\begin{equation*}\label{alpah_prox_argmin}
{\scriptsize{\begin{bmatrix}\bm{\mu}\\
\bm{\nu}\end{bmatrix}}}=\argmin\left\{\frac{1}{2}\left\|\scriptsize{\begin{bmatrix}
\widetilde{\mathbf{z}}\\
\widetilde{\mathbf{v}}\end{bmatrix}}-\scriptsize{\begin{bmatrix}
\mathbf{z}\\
\mathbf{v}\end{bmatrix}}\right\|_2^2+\alpha \sum_{j\in\mathbb{N}_d}\lambda_j\left\|\mathbf{I}^{'}_{j}\scriptsize{\begin{bmatrix}
\widetilde{\mathbf{z}}\\
\widetilde{\mathbf{v}}\end{bmatrix}}\right\|_1:\scriptsize{\begin{bmatrix}
\widetilde{\mathbf{z}}\\
\widetilde{\mathbf{v}}\end{bmatrix}}\in\mathbb{R}^{p_d+n-r}\right\}   
\end{equation*}
which further leads to 
$$
{\scriptsize{\begin{bmatrix}\bm{\mu}\\
\bm{\nu}\end{bmatrix}}}=\argmin\left\{\sum_{j\in\mathbb{N}_d}\left(\frac{1}{2}\|\widetilde{\mathbf{z}}_j-
\mathbf{z}_j\|_2^2+\alpha\lambda_j\|\widetilde{\mathbf{z}}_j\|_1\right)+\frac{1}{2}\|
\widetilde{\mathbf{v}}-
\mathbf{v}\|_2^2:\scriptsize{\begin{bmatrix}
\widetilde{\mathbf{z}}\\
\widetilde{\mathbf{v}}\end{bmatrix}}\in\mathbb{R}^{p_d+n-r}\right\}.
$$
As a result, we have that 
\begin{equation}\label{mu_j}
\bm{\mu}_j=\argmin \left\{\frac{1}{2}\|\widetilde{\mathbf{z}}_j-
\mathbf{z}_j\|_2^2+\alpha\lambda_j\|\widetilde{\mathbf{z}}_j\|_1:\widetilde{\mathbf{z}}_j\in\mathbb{R}^{m_j}\right\}, \ j\in\mathbb{N}_d,   
\end{equation}
and 
\begin{equation}\label{nu}
\bm{\nu}=\argmin\left\{\frac{1}{2}\|
\widetilde{\mathbf{v}}-
\mathbf{v}\|_2^2:\widetilde{\mathbf{v}}\in\mathbb{R}^{n-r}\right\}. 
\end{equation}
According to Examples 2.3 and 2.4 in \cite{micchelli2011proximity} and noting that $\bm{\mu}_j:=[\mu_{p_{j-1}+i}:i\in\mathbb{N}_{m_j}]$ and $\mathbf{z}_j:=[z_{p_{j-1}+i}:i\in\mathbb{N}_{m_j}]$, $j\in\mathbb{N}_d$, we obtain from equation \eqref{mu_j} that
$$
\mu_{p_{j-1}+i}=\max\{|z_{p_{j-1}+i}|-\alpha\lambda_j,0
\}{\rm sign}(z_{p_{j-1}+i}), \ j\in\mathbb{N}_d,\  i\in\mathbb{N}_{m_j}.
$$
That is, equation \eqref{proximity-operator-sum_lambda_j} holds. Moreover,
equation \eqref{nu} leads directly to $\bm{\nu}=\mathbf{v}$.
\end{proof}

We also consider the special case that $n=r$. In this case, we present a closed-form formula for the proximity operator $\mathrm{prox}_{\alpha\sum_{j\in\mathbb{N}_d}\lambda_j\|(\cdot)_j\|_{1}}$ with $\lambda_j>0$, $j\in\mathbb{N}_d$ and $\alpha>0$.
\begin{corollary}
If $\lambda_j>0$, $j\in\mathbb{N}_d$ and $\alpha>0$, then the proximity operator 
$\mathrm{prox}_{\alpha\sum_{j\in\mathbb{N}_d}\lambda_j\|(\cdot)_j\|_{1}}$ at $\mathbf{z}:=[z_{k}:k\in\mathbb{N}_{p_d}]\in\mathbb{R}^{p_d}$ has the form 
$$
\mathrm{prox}_{\alpha\sum_{j\in\mathbb{N}_d}\lambda_j\|(\cdot)_j\|_{1}}(
\mathbf{z}):=\bm{\mu}
\ \mbox{with}\ \bm{\mu}:=[\mu_{k}: k\in\mathbb{N}_{p_d}]\in\mathbb{R}^{p_d},
$$ 
where for each $j\in\mathbb{N}_d$ and $i\in\mathbb{N}_{m_j}$, $\mu_{p_{j-1}+i}$ is defined by \eqref{proximity-operator-sum_lambda_j_mu}.
\end{corollary}

The next closed-form formula concerns the proximity operator $\mathrm{prox}_{\frac{1}{\beta}\iota_{\mathbb{M}}}$ with $\mathbb{M}:=\mathcal{R}(\mathbf{B})\times\mathbb{R}^{n-r}$ and $\beta>0$. For a matrix $\mathbf{M}$, we denote by $\mathbf{M}^{\dag}$ the pseudoinverse of $\mathbf{M}$.
\begin{proposition}\label{prox-indicator}
If $\mathbb{M}:=\mathcal{R}(\mathbf{B})\times\mathbb{R}^{n-r}$ and $\beta>0$, then the proximity operator $\mathrm{prox}_{\frac{1}{\beta}\iota_{\mathbb{M}}}$ at $
\scriptsize{\begin{bmatrix}
\mathbf{z}\\
\mathbf{v}\end{bmatrix}}\in\mathbb{R}^{p_d+n-r}$ with $\mathbf{z}:=[z_{k}:k\in\mathbb{N}_{p_d}]\in\mathbb{R}^{p_d}$ and $\mathbf{v}\in\mathbb{R}^{n-r}$ has the form 
\begin{equation*}
\mathrm{prox}_{\frac{1}{\beta}\iota_{\mathbb{M}}}\left(\scriptsize{\begin{bmatrix}
\mathbf{z}\\
\mathbf{v}\end{bmatrix}}\right)=\scriptsize{\begin{bmatrix}\mathbf{B}(\mathbf{B}^{\top}\mathbf{B})^{\dag}\mathbf{B}^{\top}\mathbf{z}\\\mathbf{v}\end{bmatrix}}.
\end{equation*}
\end{proposition}
\begin{proof}
According to definition \eqref{proximity operator} of the proximity operator, we obtain for each $\mathbf{z}\in\mathbb{R}^{p_d}$ and each $\mathbf{v}\in\mathbb{R}^{n-r}$ that
$\mathrm{prox}_{\frac{1}{\beta}\iota_{\mathbb{M}}}\left({\scriptsize{\begin{bmatrix}
\mathbf{z}\\
\mathbf{v}\end{bmatrix}}}\right)
:={\scriptsize{\begin{bmatrix}\bm{\mu}\\
\bm{\nu}\end{bmatrix}}}$ with $ \bm{\mu}:=[\mu_{k}: k\in\mathbb{N}_{p_d}]\in\mathbb{R}^{p_d}$ and $\bm{\nu}\in\mathbb{R}^{n-r}$, where
$$
{\scriptsize{\begin{bmatrix}\bm{\mu}\\
\bm{\nu}\end{bmatrix}}}=\argmin\left\{\frac{1}{2}\left\|\scriptsize{\begin{bmatrix}
\widetilde{\mathbf{z}}\\
\widetilde{\mathbf{v}}\end{bmatrix}}-\scriptsize{\begin{bmatrix}
\mathbf{z}\\
\mathbf{v}\end{bmatrix}}\right\|_2^2+\frac{1}{\beta}\iota_{\mathbb{M}}\left({\scriptsize{\begin{bmatrix}
\widetilde{\mathbf{z}}\\
\widetilde{\mathbf{v}}\end{bmatrix}}}\right):\scriptsize{\begin{bmatrix}
\widetilde{\mathbf{z}}\\
\widetilde{\mathbf{v}}\end{bmatrix}}\in\mathbb{R}^{p_d+n-r}\right\}.
$$
By noting that $\mathbb{M}:=\mathcal{R}(\mathbf{B})\times\mathbb{R}^{n-r}$, we rewrite the above equation as 
\begin{equation}\label{mu1}
\bm{\mu}=\argmin\left\{\frac{1}{2}\|\widetilde{\mathbf{z}}-
\mathbf{z}\|_2^2:\widetilde{\mathbf{z}}\in\mathcal{R}(\mathbf{B})\right\}
\end{equation}
and 
\begin{equation}\label{nu1}
\bm{\nu}=\argmin\left\{\frac{1}{2}\|
\widetilde{\mathbf{v}}-
\mathbf{v}\|_2^2:
\widetilde{\mathbf{v}}\in\mathbb{R}^{n-r}\right\}.
\end{equation}
Equation \eqref{mu1} shows that $\bm{\mu}$ is the best approximation  to $\mathbf{z}$ from the subspace $\mathcal{R}(\mathbf{B})$. Hence, we get that $\bm{\mu}=\mathbf{B}\mathbf{x}$ with vector $\mathbf{x}\in\mathbb{R}^n$ satisfying
$$
(\mathbf{B}\mathbf{y})^{\top}(\mathbf{z}-\mathbf{B}\mathbf{x})=0,\ \mbox{for all}\ \mathbf{y}\in\mathbb{R}^n.
$$
By rewriting the above equation as 
$$
\mathbf{y}^{\top}\mathbf{B}^{\top}(\mathbf{z}-\mathbf{B}\mathbf{x})=0,\ \mbox{for all}\ \mathbf{y}\in\mathbb{R}^n,
$$
we have that vector $\mathbf{x}$ is a solution of the linear system 
$$
\mathbf{B}^{\top}\mathbf{B}\mathbf{x}=\mathbf{B}^{\top}\mathbf{z}.
$$
By using the pseudoinverse of $\mathbf{B}^{\top}\mathbf{B}$, we represent  $\mathbf{x}$ as
$$
\mathbf{x}=(\mathbf{B}^{\top}\mathbf{B})^{\dag}\mathbf{B}^{\top}\mathbf{z}+\mathbf{x}_0,
$$
where $\mathbf{x}_0\in\mathbb{R}^n$  satisfying $\mathbf{B}^{\top}\mathbf{B}\mathbf{x}_0=0$. Note that $\mathbf{B}^{\top}\mathbf{B}\mathbf{x}_0=0$ if and only if $\mathbf{B}\mathbf{x}_0=0$. Thus, $\bm{\mu}=\mathbf{B}\mathbf{x}=\mathbf{B}(\mathbf{B}^{\top}\mathbf{B})^{\dag}\mathbf{B}^{\top}\mathbf{z}$. Moreover, we can obtain directly from equation \eqref{nu1} that $\bm{\nu}=\mathbf{v}$.
\end{proof}

In the case when $n=r$, the subspace $\mathbb{M}$ reduces to $\mathcal{R}(\mathbf{B})$. Since  $\mathbf{B}$ has full column rank, the matrix $\mathbf{B}^{\top}\mathbf{B}$ is nonsingular. As a direct consequence of Proposition \ref{prox-indicator}, we describe the closed-form formula of the proximity operator $\mathrm{prox}_{\frac{1}{\beta}\iota_{\mathbb{M}}}$ for this special case as follows.
\begin{corollary}
If matrix $\mathbf{B}$ has full column rank, $\mathbb{M}:=\mathcal{R}(\mathbf{B})$ and $\beta>0$, then the proximity operator $\mathrm{prox}_{\frac{1}{\beta}\iota_{\mathbb{M}}}$ at  $\mathbf{z}:=[z_{k}:k\in\mathbb{N}_{p_d}]\in\mathbb{R}^{p_d}$ has the form
\begin{equation}\label{indicator_prox}
    \mathrm{prox}_{\frac{1}{\beta}\iota_{\mathbb{M}}}(\mathbf{z})=\mathbf{B}(\mathbf{B}^{\top}\mathbf{B})^{-1}\mathbf{B}^{\top}\mathbf{z}.
\end{equation}   
\end{corollary}

Finally, we give closed-form formulas for the proximity operators
of some loss functions, which will be used in numerical experiments. The first loss function is the $\ell_2$ norm composed with a matrix.
\begin{proposition}\label{2-norm}
If $\mathbf{A}$ is an $m\times n$ matrix, $\mathbf{x}\in\mathbb{R}^m$ and $\rho>0$, then the proximity operator $\mathrm{prox}_{\frac{1}{2\rho}\|\mathbf{A}\cdot-\mathbf{x}\|_2^2}$ at $\mathbf{z}\in\mathbb{R}^n$ has the form  
$$
\mathrm{prox}_{\frac{1}{2\rho}\|\mathbf{A}\cdot-\mathbf{x}\|_2^2}(\mathbf{z})=\left(\rho\mathbf{I}_n+\mathbf{A}^{\top}\mathbf{A}\right)^{-1}\left(\rho\mathbf{z} + \mathbf{A}^{\top}\mathbf{x}\right).
$$
\end{proposition}
\begin{proof}
By setting $\mathrm{prox}_{\frac{1}{2\rho}\|\mathbf{A}\cdot-\mathbf{x}\|_2^2}(\mathbf{z}):=\bm{\mu}$, we obtain from definition \eqref{proximity operator} of the proximity operator that 
$$
\bm{\mu}=\argmin\left\{\frac{1}{2}\|\mathbf{u}-\mathbf{z}\|_2^2+\frac{1}{2\rho}\|\mathbf{A}\mathbf{u}-\mathbf{x}\|_2^2:\mathbf{u}\in\mathbb{R}^n\right\},
$$
which together with the Fermat rule leads to 
\begin{equation*}
\bm{\mu}-\mathbf{z}+\frac{1}{\rho}\mathbf{A}^{\top}
(\mathbf{A}\bm{\mu}-\mathbf{x})=\mathbf{0}.
\end{equation*}
By rewriting the above equation as 
$$
\left(\rho\mathbf{I}_n+\mathbf{A}^{\top}\mathbf{A}\right)\bm{\mu}
=\rho\mathbf{z} + \mathbf{A}^{\top}\mathbf{x}
$$
and noting that $\rho\mathbf{I}_n+\mathbf{A}^{\top}\mathbf{A}$ is nonsingular, we obtain that 
$$
\bm{\mu}=\left(\rho\mathbf{I}_n+\mathbf{A}^{\top}\mathbf{A}\right)^{-1}\left(\rho\mathbf{z} + \mathbf{A}^{\top}\mathbf{x}\right),
$$
which completes the proof of this proposition.
\end{proof}

Two special cases of Proposition \ref{2-norm} are used in numerical experiments. In Subsection  \ref{NM_block_separable}, the loss function is chosen as $\Psi:=\frac{1}{2}\|\cdot-\mathbf{x}\|_2^2$. According Proposition \ref{2-norm}, the proximity operator $\mathrm{prox}_{\frac{1}{\rho}\Psi}$ at $\mathbf{z}:=[z_j:j\in\mathbb{N}_n]\in\mathbb{R}^n$ has the form  $
\mathrm{prox}_{\frac{1}{\rho}\Psi}(\mathbf{z}):=\mathbf{w}$ with $\mathbf{w}:=[w_j:j\in\mathbb{N}_n]$, where 
\begin{equation}\label{2-norm-1}
    w_j:=(\rho {z_j}+x_j)/(\rho+1), \ \mbox{for all}\ j\in\mathbb{N}_n.
\end{equation}
In Subsection \ref{Compound sparse denoising}, the loss function $\bm{\psi}$ is defined by \eqref{fidelity-CSD} and the proximity operator $\mathrm{prox}_{\frac{1}{\rho}\bm{\psi}}$ at $\mathbf{z}\in\mathbb{R}^n$ can be represented by 
\begin{equation}\label{2-norm-2}
\mathrm{prox}_{\frac{1}{\rho}\bm{\psi}}(\mathbf{z}):=(\rho\mathbf{I}_n+\mathbf{H}^{\top}\mathbf{H})^{-1}(\rho\mathbf{z}+\mathbf{H}^{\top}\mathbf{Hy}).
\end{equation}

The loss function is chosen in Subsection \ref{Fused SVM} as $\bm{\phi}(\mathbf{z}):=\sum_{j\in\mathbb{N}_p}\mathrm{max}\{0,1-z_j\}$ for $\mathbf{z}:=[z_j:j\in\mathbb{N}_p]\in\mathbb{R}^p$. The closed-form formula of the proximity operator $\mathrm{prox}_{\frac{1}{\rho}\bm{\phi}}$ with $\rho>0$ has been given in \cite{Li2019a}. Specifically, the proximity operator
$\mathrm{prox}_{\frac{1}{\rho}\bm{\phi}}$ at $\mathbf{z}\in\mathbb{R}^p$ has the form $\mathrm{prox}_{\frac{1}{\rho}\bm{\phi}}(\mathbf{z}):=[\mu_j:j\in\mathbb{N}_p]$, where for all $j\in\mathbb{N}_p$,
\begin{equation}\label{max-function}
\mu_j:=
\left\{\begin{array}{ll}
z_{j}+\frac{1}{\rho}, \ &\mathrm{if}\ z_{j}<1-\frac{1}{\rho},\\
z_{j}, \ & \mathrm{if}\ z_{j}>1,\\
 1, \ & \mathrm{if}\ z_j\in[1-\frac{1}{\rho}, 1].
\end{array}\right.   
\end{equation}
\section*{References}
%\bibliographystyle{iopart-num}
%\bibliographystyle{plain}
%\bibliography{Bibliography}
\begin{thebibliography}{02}

\bibitem{Afonso2010}
Afonso M V, Bioucas-Dias J M and Figueiredo M A T 2010 An augmented Lagrangian approach to linear inverse problems with compound regularization {\it IEEE International Conference on Image Processing} 4169--72

\bibitem{argyriou2011efficient}
Argyriou A, Micchelli C A, Pontil M, Shen L and Xu Y 2011 Efficient first order methods for linear composite regularizers
(arXiv:1104.1436)

\bibitem{Bauschke2011}
Bauschke H H and Combettes P L 2011 {\it Convex Analysis and Monotone Operator Theory in Hilbert Spaces} (Springer, New York)


\bibitem{Belkin2006}
Belkin M, Niyogi P and Sindhwani V 2006 Manifold regularization: a geometric frame work for learning from labeled and unlabeled examples J. Mach. Learn. Res. 7 2399--434

\bibitem{Bjork1996}
Bj\"{o}rck Å 1996
{\it Numerical methods for least squares problems}
(Philadelphia, PA: SIAM)

\bibitem{Brezinski2003}
Brezinski C, Redivo-Zaglia M, Rodriguez G and Seatzu S 2003 Multi-parameter regularization techniques for ill-conditioned linear system Numer. Math. 94 203--28

\bibitem{Byrne2003}
Byrne C 2003 A unified treatment of some iterative algorithms in signal processing and image reconstruction {\it Inverse problems} {\bf 20} 103

\bibitem{Chen2008}
Chen Z, Lu Y, Xu Y and Yang H 2008 Multi-parameter Tikhonov regularization for ill-posed
operator equations J. Comput. Math. 26 37--55

% \bibitem{Combettes2005}
% Combettes P L and Wajs V R 2005 Signal recovery by proximal forward-backward splitting {\it SIAM Multiscale Model. Simul.} {\bf 4} 1168--200

% \bibitem{Deutsch2001}
% Deutsch F 2001 {\it Best Approximation in Inner Product Spaces} (New York: Springer)

\bibitem{ding2019}
Ding L and Han W 2019 $\alpha\ell_1$-$\beta\ell_2$ regularization for sparse recovery {\it Inverse Problems} {\bf 35} 125009

\bibitem{Duvelmeyer2006}
D\"{u}velmeyer D and  Hofmann B 2006 A multi-parameter regularization approach for estimating parameters in jump diffusion process J. Inverse Ill-posed Probl. 14 861--80.


\bibitem{Friedman2007}
Friedman J, Hastie T, Höfling H and Tibshirani R 2007 Pathwise coordinate optimization {\it Ann. Appl. Stat.} {\bf 1} 302--32

% \bibitem{grasmair2008sparse} 
% Grasmair M, Haltmeier M and Scherzer O 2008 Sparse regularization with $l^q$ penalty term {\it Inverse Problems} {\bf 24} 055020

% \bibitem{hein2008}
% Hein T 2008 Convergence rates for regularization of ill-posed
% problems in Banach spaces by approximate source conditions {\it Inverse Problems} {\bf 24} 045007

\bibitem{horn2012matrix} 
Horn R A and Johnson C R 2012 {\it Matrix Analysis} (Cambridge: Cambridge University Press)

\bibitem{lecun1998gradient} LeCun Y, Bottou L, Bengio Y and Haffner P 1998  Gradient-based learning applied to document recognition {\it  Proc. IEEE} {\bf 86} 2278--324

\bibitem{li2015multi} 
Li Q, Shen L, Xu Y and Zhang N 2015 Multi-step fixed-point proximity algorithms for solving a
class of optimization problems arising from image processing {\it Adv. Comput. Math.} {\bf 41} 387--422

\bibitem{Li2017two}
Li Q, Xu Y and Zhang N 2017 Two-step fixed-point proximity algorithms for multi-block separable convex problems {\it J. Sci. Comput.} {\bf 70} 1204--28

\bibitem{Li2019a}
Li Z, Song G and Xu Y 2019 A two-step fixed-point proximity algorithm for a class of non-differentiable optimization models in machine learning {\it J. Sci. Comput.} {\bf 81} 923--40

\bibitem{Liu2023parameter} Liu Q, Wang R, Xu Y and Yan M 2023 Parameter Choices for Sparse Regularization with the $\ell_1$ Norm {\it Inverse Problems} {\bf 39} 025004


% \bibitem{Lorenz2008}
% Lorenz D A 2008 Convergence rates and source conditions for Tikhonov regularization with sparsity constraints
% {\it J. Inverse Ill-Posed Problems} {\bf 16 } 463--78

\bibitem{Lu2013}
Lu S and  Pereverzev S V 2013 {\it Regularization Theory for Ill-Posed Problems. Selected Topics} (De Gruyter, Berlin, Boston)

\bibitem{Lu2007}
Lu Y, Shen L and Xu Y 2007 Multi-parameter regularization methods for high-resolution image
reconstruction with displacement errors IEEE Trans. Circuits Systems I 54 1788--99

\bibitem{micchelli2011proximity} 
Micchelli C A, Shen L and Xu Y 2011 Proximity algorithms for image models:
denoising {\it Inverse Problems} {\bf 27} 045009

\bibitem{Rapaport2008}
Rapaport F, Barillot E and Vert J P 2008 Classification of arrayCGH data using fused SVM {\it Bioinformatics} {\bf 24} i375--82.

\bibitem{Selesnick2014Simultaneous}
Selesnick I W, Graber H L, Pfeil D S and Barbour R L 2014 Simultaneous low-pass filtering and total variation denoising {\it IEEE Trans. Image Process.} {\bf 62} 
1109--24

\bibitem{Shen2024Sparse} Shen L, Wang R, Xu Y and Yan M 2024 Sparse Deep Learning Models with the $\ell_1$ Regularization (arXiv:2408.02801)


\bibitem{Showalter1997} Showalter R E 1997 {\it Monotone Operators in Banach Spaces and Nonlinear Partial Differential Equations} (American Mathematical Society, Providence)

\bibitem{tibshirani1996regression}
Tibshirani R 1996 Regression shrinkage and selection via the lasso {\it J. Roy. Statist. Soc. Ser. B} {\bf 58} 267--88.

\bibitem{Tibshirani2005}
Tibshirani R, Saunders M, Rosset S, Zhu J and Knight K 2005 Sparsity and smoothness via the
fused lasso {\it J. R. Stat. Soc. B} {\bf 67} 91–108

\bibitem{Tolosi2011} 
Tolosi L and Lengauer T
2011 Classification with correlated features: unreliability of feature
ranking and solutions {\it Bioinformatics} {\bf 27} 1986--94.

\bibitem{wang2013}
Wang W, Lu S, Mao H and Cheng J 2013 
Multi-parameter Tikhonov regularization with the $\ell_0$ sparsity constraint {\it Inverse Problems} {\bf 29} 065018

\bibitem{xu2023} 
Xu Y 2023 Sparse regularization with the $\ell_0$ norm {\it Anal. Appl.} {\bf 21}  901--29

\bibitem{zalinescu2002convex} Z\u{a}linescu C 2002 {\it Convex Analysis in General Vector Spaces} (River Edge, NJ: World Scientific)




\end{thebibliography}


\end{document} 

