
\documentclass{article} % For LaTeX2e
\usepackage[dvipsnames, table, xcdraw]{xcolor}
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}

% \usepackage{hyperref}
\usepackage[hidelinks, breaklinks, colorlinks, linkcolor=Red, citecolor=RoyalBlue]{hyperref}
\usepackage{url}

\usepackage{multirow}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage{eqparbox}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{array}
\usepackage{pifont}
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{colortbl}
\usepackage{fontenc}
\usepackage{cleveref}
\usepackage{tikz}
\usepackage{xspace}
%\usepackage{caption}

\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\tabref}[1]{Tab.~\ref{#1}}
\newcommand{\Figref}[1]{Figure~\ref{#1}}
\newcommand{\Tabref}[1]{Table~\ref{#1}}
\newcommand{\eqnref}[1]{Eq.~(\ref{#1})}
\newcommand{\secref}[1]{Sec.~\ref{#1}}
\newcommand{\Secref}[1]{Section~\ref{#1}}
\newcommand{\Appref}[1]{Appendix~\ref{#1}}

\definecolor{baselinecolor}{rgb}{0.9, 0.9, 1.}
\definecolor{graycolor}{gray}{0.9}
\definecolor{Green}{rgb}{0.0, 0.5, 0.0}
\definecolor{Green}{rgb}{0.0, 0.5, 0.0}
\definecolor{rebuttal}{rgb}{0.6, 0.6, 1.}
\newcommand{\baseline}[1]{\cellcolor{baselinecolor}{#1}}
\newcommand{\graybase}[1]{\cellcolor{graycolor}{#1}}


\newcommand{\plus}[1]{\scriptsize\bf\textcolor{Green}{#1}}
\newcommand{\tinyplus}[1]{\tiny\bf\textcolor{Green}{#1}}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

% \title{Formatting Instructions for ICLR 2025 \\ Conference Submissions}
\title{UNIP: Rethinking Pre-trained Attention Patterns for Infrared Semantic Segmentation}
% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

\author{Tao Zhang\textsuperscript{1,2} \ \
Jinyong Wen\textsuperscript{1,2} \ \
Zhen Chen\textsuperscript{3} \ \
Kun Ding\textsuperscript{1}\thanks{Corresponding author} \ \
Shiming Xiang\textsuperscript{1,2} \ \
Chunhong Pan\textsuperscript{1} \\
\textsuperscript{1}MAIS, Institute of Automation, Chinese Academy of Sciences, China \\
\textsuperscript{2}School of Artificial Intelligence, University of Chinese Academy of Sciences, China \\
\textsuperscript{3}School of Automation Science and Electrical Engineering, Beihang University, China \\ 
\texttt{\{zhangtao2021,wenjinyong2019,kun.ding\}@ia.ac.cn;} \\ \texttt{\{czhen\}@buaa.edu.cn;\{smxiang, chpan\}@nlpr.ia.ac.cn}\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle


\begin{abstract}
Pre-training techniques significantly enhance the performance of semantic segmentation tasks with limited training data. However, the efficacy under a large domain gap between pre-training (\eg RGB) and fine-tuning (\eg infrared) remains underexplored. In this study, we first benchmark the infrared semantic segmentation performance of various pre-training methods and reveal several phenomena distinct from the RGB domain. Next, our layerwise analysis of pre-trained attention maps uncovers that: (1) There are three typical attention patterns (local, hybrid, and global); (2) Pre-training tasks notably influence the pattern distribution across layers; (3) The hybrid pattern is crucial for semantic segmentation as it attends to both nearby and foreground elements; (4) The texture bias impedes model generalization in infrared tasks. Building on these insights, we propose \textbf{UNIP}, a \textbf{UN}ified \textbf{I}nfrared \textbf{P}re-training framework, to enhance the pre-trained model performance. This framework uses the hybrid-attention distillation NMI-HAD as the pre-training target, a large-scale mixed dataset InfMix for pre-training, and a last-layer feature pyramid network LL-FPN for fine-tuning. Experimental results show that UNIP outperforms various pre-training methods by up to \textbf{13.5\%} in average mIoU on three infrared segmentation tasks, evaluated using fine-tuning and linear probing metrics. UNIP-S\footnote{We use the term \textit{method-size} to denote the vision transformer (ViT) of a specific \textit{size} pre-trained by a specific \textit{method}. T, S, B, and L refer to the ViT-Tiny, ViT-Small, ViT-Base, and ViT-Large, respectively.} achieves performance on par with MAE-L while requiring only \textbf{1/10} of the computational cost. Furthermore, UNIP significantly surpasses state-of-the-art (SOTA) infrared or RGB segmentation methods and demonstrates broad potential for application in other modalities, such as RGB and depth. Our code is available at \href{https://github.com/casiatao/UNIP}{https://github.com/casiatao/UNIP}.

\end{abstract}

\input{sec/introduction}
\input{sec/benchmark_setup}
\input{sec/investigation}
\input{sec/distill}




\section{Conclusion and Discussion}
In this work, we comprehensively benchmark the infrared segmentation performance of different pre-training methods and uncover several valuable insights. We further analyze the pre-trained attention maps and identify the importance of \textit{hybrid} patterns for semantic segmentation. Finally, we propose the UNIP framework to improve the performance of small ViT models. Extensive experimental results demonstrate the effectiveness of our dataset and method. UNIP presents a viable approach for selective knowledge distillation in domain transfer settings. We hope our analysis can provide meaningful insights into the characteristics and differences among pre-training methods, ultimately contributing to the advancements of visual pre-training and downstream transfer learning.

\textbf{Limitations and Future Work.} Due to limited computing resources, we validate UNIP's effectiveness only in the infrared domain for semantic segmentation. However, we believe UNIP can be effectively extended to other modalities, such as RGB and depth images, as the superiority of \textit{hybrid} patterns in these modalities is demonstrated in \tabref{tab:rgb_llp}. Exploring its potential in other dense prediction tasks, like object detection and depth estimation, is also worthwhile. Moreover, combining \textit{hybrid} patterns from different pre-trained methods could 
be a promising avenue.




% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.

\section*{Reproducibility Statement}
Reproducibility is a priority in our research. In this statement, we outline the measures taken to ensure our work can be reproduced.

\textbf{Source Code.} The source code of our work is available at this \href{https://github.com/casiatao/UNIP}{link}. Researchers can access and utilize our code to reproduce the experimental results in this paper. The source code and pre-trained model weights will be made publicly available.

\textbf{Experimental Setup and Details.} In the main paper, the basic experimental configurations are presented in \secref{sec:benchmark_setup} (benchmark) and in \secref{sec:unip_experiments} (UNIP). In \Appref{app:experimental_details}, we provide the detailed settings, including the implementation details of the benchmark (\Appref{app:benchmark_details}) and UNIP (\Appref{app:pre-training}), the comparisons of different evaluation metrics and their hyperparameter settings (\Appref{app:appendix_evaluation}), and the evaluation datasets usage (\Appref{app:evaluation_datasets}).

\textbf{Datasets.} We outline the construction steps of our InfMix dataset in \secref{sec:unip}. In \Appref{app:pretraining_dataset}, we further present more details about the dataset collection and preprocessing.

By highlighting these references, we intend to improve the reproducibility of our work, helping other researchers verify and build on our findings. We're open to any questions or requests for more information about our methods, as we aspire to ensure our research is transparent and reliable.



\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\section*{Appendix}
In \secref{app:related_work}, we discuss the related works. In \secref{app:relationship}, we review the relationship between our motivations and proposed methods. In \secref{app:experimental_details}, we provide detailed descriptions of the experimental settings, including the complete benchmark results, evaluation metrics and datasets, and the experimental specifics of UNIP. Further analysis is conducted in \secref{app:appendix_analysis}, covering (1) the relationship between NMI and attention patterns, and (2) the CKA analysis of feature representation. Additional experimental results are presented in \secref{app:more_experiments}. Finally, we provide more details of the pre-training dataset in \secref{app:pretraining_dataset} and offer additional visualization results in \secref{app:more_visualization}.

\input{sec/related_work}
\input{sec/relationship}
\input{sec/appendix_evaluation}
\input{sec/appendix_analysis}
\input{sec/appendix_experiment}
\input{sec/appendix_infpre}
\input{sec/appendix_visualization}

\end{document}
