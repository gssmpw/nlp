\section{More experiments.}
\label{app:more_experiments}

% \input{tab_tex/rgb_llp}

% \input{tab_tex/data_ratio}
% \input{tab_tex/topk}
\input{tab_tex/data_ratio_and_topk}

\begin{wraptable}{r}{0.4\textwidth}
    \centering
    \vspace{-7.5mm}
    \caption{Comparison of initialization.}
    \label{tab:wo_pretrain}
    % \vspace{-2mm}
    \centering
    \scriptsize
    \setlength{\tabcolsep}{1.0mm}{
    \scalebox{1.0}{
    \begin{tabular}{c c c c c}
        \toprule
        Initialization & Tiny & Small & Base & Large \\
        \midrule
        Random & 30.64 & 36.82 & 39.14 & 39.31  \\
        Pre-training & \textbf{51.53} & \textbf{59.65} & \textbf{62.53} & \textbf{64.47}
        \\
        & \plus{+20.89} & \plus{+22.83} & \plus{+23.39} & \plus{+25.16} \\
        \bottomrule
    \end{tabular}}}
    \vspace{-3.5mm}
\end{wraptable}

\textbf{Pre-training is important.} We compare the average FT performance of pre-trained and randomly initialized models. For pre-trained models, the performance is averaged across six different methods in \tabref{tab:benchmark}. As shown in \tabref{tab:wo_pretrain}, models without pre-training consistently fall behind by 20.89\% to 25.03\%, regardless of model size. This gap widens with larger models, highlighting the importance of pre-training and the necessity of studying different pre-training approaches on infrared tasks.

\textbf{Impact of the Size of the Pre-training Dataset.} \tabref{tab:data_ratio} illustrates the fine-tuning performance with varying ratios of the InfMix dataset. A clear data scaling law is observed, where the performance consistently improves as the pre-training dataset size increases. This demonstrates the necessity of constructing the InfMix dataset, a much larger dataset than other infrared pre-training datasets like MSIP \citep{pad} and Inf30 \citep{infmae}. As we continue to expand the InfMix dataset, we can anticipate even greater advancements in model performance, potentially enabling breakthroughs in applications that rely on infrared data, such as autonomous driving \citep{mcnet}, and surveillance \citep{birdsai}.

\input{tab_tex/two_stage_training}

\textbf{Impact of the Training Strategy on the Pre-training Dataset.} In \tabref{tab:two_stage}, we conduct experiments involving a two-stage training process using MAE-L as the teacher model. In the first stage, the model is distilled using the RGB component of InfMix. In the second stage, the model is subsequently distilled using the infrared component of InfMix. As indicated in \tabref{tab:two_stage}, benefiting from the hybrid pattern distillation, the model of the RGB training stage surpasses MAE-S by a large margin. After the infrared training stage, the model's average segmentation performance improves further. However, we observe a slight decline in performance on the SODA dataset. We attribute this to the problem of data distribution mismatch. Notably, half of the images in SODA depict indoor scenes, which are scarce in our infrared pre-training dataset InfPre. In contrast, such scenes are more prevalent in the ImageNet and COCO datasets. We believe this discrepancy also accounts for the inferior performance of the two-stage training compared to joint training. Joint training benefits from a wider data distribution, which contributes to improved generalization performance.

\textbf{Multi-layer Distillation.} In \tabref{tab:topk}, we examine the use of attention maps from multiple layers of the teacher model for distillation. Interestingly, performance declines as more layers are included. We hypothesize that requiring a single student layer to mimic multiple teacher layers' attention maps introduces excessive complexity and noise, which impedes the distillation process. An adaptive selection of attention maps to minimize noise and redundancy could be a promising direction.

\input{tab_tex/head_wise}
\textbf{Head-wise Distillation.} In \tabref{tab:head_wise}, we explore the head-wise distillation, a more fine-grained distillation method. Compared to the layer-wise distillation in NMI-HAD, it directly utilizes different heads for distillation. The experimental setup involves using MAE-L (16 attention heads for each layer) as the teacher to distill UNIP-S (6 attention heads for each layer). First, we calculate the NMI for attention maps of each attention head in MAE-L and observe that not all attention heads within the same layer exhibit the same attention pattern. Therefore, we categorize these attention heads into three patterns: local (l), hybrid (h), and global (g). We then select six attention heads (the total number of heads in UNIP-S) as distillation targets. For the 18th layer of MAE-L, there are 5 global heads, 4 local heads, and 7 hybrid heads. We experiment with three different combinations: one containing only hybrid patterns (row 2), one containing only local and global patterns (row 3), and one containing all three patterns (row 4). The average NMI values for these combinations are comparable. Notably, the combination containing only hybrid attention patterns achieves the best performance, demonstrating the effectiveness of hybrid attention patterns even in head-wise distillation. Furthermore, using just 6 hybrid attention heads for distillation even surpasses the performance of distilling all 16 heads in the 18th layer (row 1). This phenomenon is also observed in the 24th layer. This suggests that there may be redundancy in the attention maps within a single layer. Therefore, we believe that more fine-grained distillation, such as head-wise distillation, is a highly promising research direction.