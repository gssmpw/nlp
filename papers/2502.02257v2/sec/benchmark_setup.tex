\begin{figure}[t]
    \centering
    \begin{tikzpicture}
       \node at (0,0) {
        \includegraphics[width=\linewidth]{figures/benchmark_v6.pdf}
      };
      % Draw the table
      \node[anchor=south west, xshift=-5.4cm, yshift=-1.85cm] at (current page.south west) {
        \scriptsize
        \setlength{\tabcolsep}{0.8mm}{
        \scalebox{0.9}{
        \begin{tabular}{lcc}
          \hline
          Model & Param & FLOPs \\
          \hline
          ViT-T & 11.0M & 36.7G \\
          ViT-S & 41.9M & 146.2G \\
          ViT-B & 163.7M & 584.0G \\
          ViT-L & 441.3M & 1192.6G \\
          \hline
        \end{tabular}}}
      };
    \end{tikzpicture}
    \vspace{-20pt}
    \caption{The performance of pre-trained models across various methods and sizes. \textit{Left}: The average fine-tuning (FT) performance on three infrared semantic segmentation datasets, along with the associated computational cost. \textit{Middle}: The average linear probing (LP) performance on three infrared datasets. \textit{Right}: The fine-tuning performance on ImageNet \citep{imagenet}. The \textcolor{gray}{gray} dotted lines and corresponding values highlight the performance gains of UNIP over other methods. Detailed results for each dataset are presented in \tabref{tab:benchmark}.}
    \label{fig:benchmark}
    \vspace{-4pt}
\end{figure}

\section{How do pre-training methods perform on infrared tasks?}
\label{sec:benchmark}
In this section, we benchmark six pre-training methods on three infrared semantic segmentation datasets and discuss several key phenomena.

\subsection{Infrared Segmentation Benchmark of RGB Pre-trained Models}
\label{sec:benchmark_setup}
\textbf{Pre-trained Backbone.} Pre-training of the Vision Transformer (ViT) \citep{vit} has gained widespread attention and demonstrated powerful performance in various fields. Many recent pre-training methods \citep{deit,mae,iBOT,dinov2} use ViT for experiments, making pre-trained ViT models readily available. Therefore, ViT models of various sizes are set as the evaluation backbone.

\textbf{Pre-training Methods.} Both supervised and self-supervised methods are investigated. For supervised approaches, we use \textbf{DeiT} \citep{deit} and \textbf{DeiT III} \citep{deit3}, which perform image classification on ImageNet for pre-training. In self-supervised methods, we study contrastive learning (CL) and masked image modeling (MIM). CL methods like \textbf{DINO} \citep{dino} encourage features from different views of the same image to be close, while keeping features from different images distinct. MIM methods like \textbf{MAE} \citep{mae} and \textbf{CrossMAE} \citep{crossmae} focus on reconstructing masked image patches by learning context relations. Although \textbf{iBOT} \citep{iBOT} combines CL with masked feature prediction, we classify it as a CL method due to its similar characteristics to DINO. \textbf{The above methods are selected because they all pre-train vanilla ViT models on ImageNet}, without additional pre-trained tokenizers like BeiT \citep{beit} or MILAN \citep{milan}, or larger datasets like EVA \citep{eva} and DINOv2 \citep{dinov2}. This allows us to focus on the impact of the pre-training tasks alone.

\textbf{Evaluation Datasets.} The evaluation is conducted on three infrared semantic segmentation datasets: SODA \citep{soda}, MFNet-T \citep{mfnet}, and SCUT-Seg \citep{mcnet}. Notably, MFNet is an RGB-Thermal paired dataset. The thermal part MFNet-T is used for benchmarks while the RGB part MFNet-RGB is employed in further investigations. Additionally, RGB datasets like ImageNet-1K \citep{imagenet} and ADE20K \citep{ade20k} are also used for comparison. Details about these datasets can be found in \Appref{app:evaluation_datasets}.

\textbf{Evaluation Metrics.} We employ two metrics: \textit{fine-tuning} (FT) and \textit{linear probing} (LP). FT (\figref{fig:ft_lp}\textcolor{red}{a}) is the primary metric, where both the pre-trained model and the decoder are tuned with the labeled datasets. In LP (\figref{fig:ft_lp}\textcolor{red}{b}), only a linear head is updated while all other parameters remain frozen. \textbf{Average (Avg) FT or LP performance in subsequent sections denotes the mean mIoU across three infrared semantic segmentation datasets.} More details are available in \Appref{app:appendix_evaluation}.

\textbf{Benchmark Results.} In the benchmark, all models are trained for 100 epochs for both evaluation metrics. Typical results are illustrated in \figref{fig:benchmark}, with ImageNet \citep{imagenet} fine-tuning performance included for comparison. The complete results of each dataset are detailed in \tabref{tab:benchmark}. 



\subsection{What insights can we gain from this benchmark?}
\label{sec:benchmark_multi_layer}

\begin{wraptable}{r}{0.36\textwidth}
    \vspace{-7.5mm}
    \caption{Pearson coefficients between average FT and other metrics.}
    \label{tab:correlation_coeff}
    \vspace{1mm}
    \centering
    \scriptsize
    \setlength{\tabcolsep}{0.8mm}{
    \scalebox{1.0}{
    \begin{tabular}{l c c c c}
        \toprule
        Metric & Small & Base & Large & Mean \\
        \midrule
        Avg FT \& Avg LP & \textbf{0.89} & \textbf{0.93} & \textbf{0.81} & \textbf{0.88}  \\
        Avg FT \& IN1K FT & 0.78 & 0.12 & -0.66 & 0.08 \\
        \bottomrule
    \end{tabular}}}
    \vspace{-4mm}
\end{wraptable}

\textbf{The infrared FT performance is strongly positively correlated with LP, but has no clear relationship with ImageNet FT.} \tabref{tab:correlation_coeff} presents the Pearson \citep{pearson} correlation coefficients between different metrics. For each metric pair, the coefficients are calculated across six pre-training methods in \figref{fig:benchmark}. Notably, the coefficients between average infrared LP and FT are close to 1, indicating that models with better LP performance generally exhibit better FT performance. Conversely, the ImageNet FT performance does not consistently correlate with infrared FT results across various model sizes, likely due to domain and task differences. Therefore, using ImageNet accuracy to predict transfer performance on infrared segmentation datasets is not reliable, underscoring the importance of benchmarking on infrared segmentation datasets.

\textbf{Supervised and CL methods outperform MIM methods, especially for small models.} As depicted in \figref{fig:benchmark} and \tabref{tab:benchmark}, the performance of supervised and CL methods like DeiT, DeiT III, DINO, and iBOT is similar across both metrics, except for the LP of DeiT-S. For the LP metric, MIM methods of various sizes consistently lag behind supervised and CL methods by a significant margin, matching observations in the RGB domain \citep{mae} that MIM representations are less linearly separable. In terms of FT, smaller MIM models (ViT-T, S, and B) still underperform supervised and CL methods, while larger models (ViT-L) are more competitive. For instance, MAE-S is far behind iBOT-S (55.39\% vs 62.09\%), but MAE-L performs comparably to iBOT-L (64.35\% vs 64.97\%). As we will discuss in \secref{sec:investigation}, the discrepancy in the attention pattern distribution and texture bias between different models accounts for their distinct infrared segmentation performance.

\textbf{Larger models perform better, but their computational cost increases sharply.}
As illustrated in \figref{fig:benchmark}, larger MIM models bring considerable performance gains over smaller models. However, for supervised and CL methods, small models are already well-trained, and the performance improvement from larger models is marginal compared to the significant increase in computational cost. For example, iBOT-L surpasses iBOT-S by only 2.85\% (64.97\% vs 62.09\%), while the parameter count and FLOPs increase by 10.5$\times$ (441M vs 42M) and 8.2$\times$ (1193G vs 146G), respectively. Given that infrared images are often processed on edge devices with limited computing budgets, using large models to pursue better performance is not cost-effective. Therefore, we believe improving small models is a more effective approach. In \secref{sec:distill}, we propose several strategies to elevate the performance of small models to be on par with larger models.
