\section{How to improve the performance on infrared segmentation?}
\label{sec:distill}
As discussed in \secref{sec:benchmark_multi_layer}, scaling up model sizes for better performance is impractical for resource-constrained scenarios. Therefore, we focus on enhancing small pre-trained models by introducing a comprehensive framework, UNIP, and validating its effectiveness through extensive experiments.

\subsection{UNIP: a unified infrared pre-training framework}
\label{sec:unip}
UNIP improves small pre-trained models by optimizing the pre-training task, constructing an appropriate pre-training dataset, and refining the fine-tuning architecture, as depicted in \figref{fig:architecture}.

\textbf{NMI-Guided Hybrid Attention Pattern Distillation (NMI-HAD).} Compressing knowledge from large models into smaller ones is an effective strategy to enhance performance without increasing parameter count. Previous works use various distillation targets like logits \citep{dino} and features \citep{efficient_sam}. However, they often overlook the relationship between distillation targets and attention patterns. As revealed in \secref{sec:lg_matter}, the \textit{hybrid} attention pattern is crucial for semantic segmentation, with NMI values linked to attention patterns. Therefore, we propose using \textit{hybird} patterns as the distillation target and introduce the NMI-guided \textit{hybrid} attention pattern distillation. First, the NMI value $\text{NMI}(A_l)$ of each teacher model's layer is calculated on ImageNet-1K:
\begin{equation}
    \text{NMI}(A_l)=\frac{1}{M}\sum_{m=1}^M\text{NMI}(A_l^m),  \quad A_l^m=\text{softmax}\left(\frac{Q_l^m (K_l^m)^T}{\sqrt{d}}\right), \quad l=\frac{L}{2}+1,...,L,
    \label{eq:nmi_cal}
\end{equation}
where $A_l^m$ denotes the $m$-th head attention matrix in the $l$-th layer. $L$ and $d$ are the number of layers and the dimension of the teacher model. The method for calculating NMI is detailed in \Appref{app:NMI}. Note that we only consider layers in the latter half of the model, as shallow layers do not capture sufficient knowledge. Next, NMI values are used to identify the location of the \textit{hybrid} attention pattern. The attention map of the layer whose NMI is closest to an empirical value $s$ is utilized as the distillation target $A_T$. Finally, the attention map $A_S$ in the last layer of the student model is forced to imitate $A_T$ by employing the Kullback-Leibler (KL) divergence constraints:
\begin{equation}
    A_T = \mathop{\arg\max}_{A_l}\Delta \text{NMI}(A_l), \quad \Delta \text{NMI}(A_l) =-\left\vert \text{NMI}(A_l)-s \right\vert, \quad
    \mathcal{L}=\frac{1}{M}\sum_{m=1}^M \text{KL}(A_{T}^m||A_{S}^m), 
    \label{eq:distill}
\end{equation}
Empirically, the NMI values of \textit{hybrid} patterns range between 0.06 and 0.12. We find that setting $s$ within this range yields good results. In all our experiments, we set it to 0.09 by default.


\begin{wraptable}{r}{0.47\textwidth}
    \vspace{-3.5mm}
    \centering
    \caption{Comparisons of infrared pre-training datasets. \#Subset denotes the number of datasets from which the images are collected.}
    \label{tab:dataset_comparison}
    \vspace{1mm}
    \scriptsize
    \setlength{\tabcolsep}{1.0mm}{
    \scalebox{1.0}{
    \begin{tabular}{l c c c c}
        \toprule
        Dataset & \#Image & \#Subset & Width & Height \\
        \midrule
        MSIP \citep{pad} & 178,756 & 8 & 844 & 596 \\
        Inf30 \citep{infmae} & 305,241 & - & 700 & 562 \\
        InfPre (ours) & \textbf{541,088} & \textbf{23} & \textbf{1,075} & \textbf{686} \\
        % InfMix (ours) & 859,375 & 25 & & \\
        \bottomrule
    \end{tabular}}}
    \vspace{-3.5mm}
\end{wraptable}

\textbf{InfMix Dataset.} To alleviate the distribution shift and reduce texture bias when distilling RGB pre-trained models for infrared tasks, we develop InfMix, a mixed dataset for distillation. InfMix comprises \textbf{859,375} images from both RGB and infrared modalities, constructed through four steps. (1) Infrared images play a key role in mitigating the distribution shift. However, existing datasets often lack diversity and sufficient images, so we collect a large and unlabelled infrared pre-training dataset called \textbf{InfPre}. It consists of \textbf{541,088} images from \textbf{23} infrared-related datasets. Compared to the other two datasets in \tabref{tab:dataset_comparison}, InfPre offers a larger number of higher-resolution images sourced from more diverse datasets. Importantly, three segmentation datasets used in the benchmark are excluded from InfPre for fair comparison. Details on data collection and deduplication can be found in \Appref{app:infpre}. (2) A subset of ImageNet-1K \citep{imagenet} is used, comprising \textbf{200,000} images evenly sampled from 1,000 classes. Since these images are part of the teacher model's pre-training data, they can anchor the student representation space close to the teacher's, thereby aiding in transferring the teacher's general feature extraction capabilities to the student. (3) The training set of COCO \citep{coco}, with \textbf{118,287} images, is also included to further enrich the pre-training dataset. Unlike single-object-centric images in ImageNet, COCO images typically depict larger scenes with multiple objects, making them more similar to infrared images, as indicated in \tabref{tab:domain_gap} in the appendix. (4) Images from ImageNet and COCO are converted to grayscale (three identical channels) to resemble infrared images more closely, as noted in \tabref{tab:domain_gap}.

\textbf{Last-Layer Feature Pyramid Network (LL-FPN).} To adapt the non-hierarchical ViT to multi-scale decoders in dense prediction tasks, previous works \citep{mae,iBOT} typically generate multi-scale feature maps from different layers of ViT, as shown in \figref{fig:ft_lp}\textcolor{red}{a}. \textbf{However, we find this multi-layer design unnecessary for our distilled models.} In these models, the \textit{hybrid} patterns in later layers equip the final features with both local and global information, making them suitable for multi-scale feature map generation. Inspired by ViTDet \citep{vitdet}, we propose using the last-layer feature pyramid network during fine-tuning. It constructs all feature maps of different scales upon the last layer's features, as illustrated in \figref{fig:architecture} and \figref{fig:ft_lp}\textcolor{red}{c}. As a bonus, this approach enhances the representation capacity of each scale branch compared to the configuration in \figref{fig:ft_lp}\textcolor{red}{a}, since they go through the entire backbone, leading to improved fine-tuning performance.

\input{tab_tex/distill_main} 


\subsection{Experiments}
\label{sec:unip_experiments}

The MAE-L, DINO-B, and iBOT-L are utilized as teacher models for distillation, and the 18th, 9th, and 21st layers are used as the target layer, according to \eqnref{eq:nmi_cal} and \eqnref{eq:distill}. Unless otherwise specified, the distillation, fine-tuning, and linear probing processes are each conducted for 100 epochs. For ablation studies, we mainly focus on the fine-tuning metric as it reflects the model's highest achievable performance. More details about experimental settings can be found in \Appref{app:pre-training}.

\input{tab_tex/sota_pretrain_feature}

\textbf{Improvements of UNIP.} As shown in \tabref{tab:main_distill}, UNIP significantly enhances the performance of small models across both metrics, often exhibiting comparable or even better performance than teacher models. With MAE-L as the teacher, UNIP-T, UNIP-S, and UNIP-B achieve average mIoU gains of \textbf{13.57\%}, \textbf{8.98\%}, and \textbf{4.34\%} in fine-tuning, and \textbf{13.58\%}, \textbf{12.98\%}, and \textbf{12.79\%} in linear probing. Notably, UNIP-S performs comparably to MAE-L with only \textbf{1/10} of the computational cost. UNIP-B even outperforms MAE-L by \textbf{0.93\%} in FT and \textbf{5.06\%} in LP. Using iBOT-L as the teacher, UNIP-S transcends iBOT-S by \textbf{2.61\%} in FT and \textbf{2.77\%} in LP. Meanwhile, UNIP-B shows gains of \textbf{1.27\%} in FT and \textbf{3.74\%} in LP, exceeding its teacher iBOT-L. Even with a smaller teacher like DINO-B, UNIP-S still enhances performance by as least \textbf{1.12\%}. \tabref{tab:sota_comparison} compares the fine-tuning performance of UNIP with other RGB or infrared segmentation methods. With fewer than half the parameters, UNIP-S, distilled from MAE-L, surpasses the universal segmentation method Mask2Former \citep{mask2former} by \textbf{3.77\%} on SODA and \textbf{2.46\%} on MFNet-T. It also outperforms TINN \citep{tinn}, specially designed for infrared semantic segmentation, by \textbf{1.9\%} on SODA and \textbf{1.83\%} on MFNet-T. A larger model UNIP-B further widens this performance gap, indicating that UNIP can greatly unleash the potential of the vanilla ViT for infrared semantic segmentation.


\begin{wrapfigure}{r}{0.51\textwidth}
\vspace{-10pt}
\centering
\includegraphics[width=1.0\linewidth]{figures/ft_nmi_v3.pdf}
\vspace{-22pt}
\caption{The average FT and NMI of each target layer. Each model is distilled for 20 epochs.}
\label{fig:ft_nmi}
\vspace{-11pt}
\end{wrapfigure}

\textbf{Impact of Distillation Target Layers.} \figref{fig:ft_nmi} displays the average fine-tuning performance using different layers of MAE-L and DINO-B as the distillation target layer. Notably, both models exhibit a strong positive correlation between average FT performance and $\Delta$NMI in \eqnref{eq:distill}, as indicated by a large Pearson coefficient. Furthermore, the peaks of FT and $\Delta$NMI occur in the same layer, highlighting the effectiveness of the NMI-HAD.


\begin{wraptable}{r}{0.51\textwidth}
    \vspace{-7.5mm}
    \caption{Ablations for components of the InfMix dataset. The teacher and student models are MAE-L and UNIP-S. All datasets are distilled for the same number of iterations for fair comparison.}
    \label{tab:dataset_composition_ablation}
    \centering
    \scriptsize
    \setlength{\tabcolsep}{0.8mm}{
    \scalebox{1.0}{
    \begin{tabular}{l c c c c c}
        \toprule
        Dataset & \#Images & SODA & MFNet-T & SCUT-Seg & Avg FT \\
        \midrule
        \rowcolor{cyan!15} InfMix & 859,375 & \textbf{70.99} & \textbf{51.32} & 70.79 & \textbf{64.37} \\
        -- w/o IN1K & 659,375 & 69.41 & 51.13 & 70.21 & 63.58 \\
        -- w/o COCO & 741,088 & 69.62 & 51.29 & 69.58 & 63.50 \\
        -- w/o Grayscale & 859,375 & 69.73 & 50.71 & \textbf{71.09} & 63.84 \\
        \midrule
        ImageNet-1K & 1,281,167 & 69.39	& 49.11 & 69.63 & 62.71 \\ 
        InfPre & 541,088 & 68.45 & 51.27 & 67.87 & 62.53 \\
        \bottomrule
    \end{tabular}}}
    \vspace{-4mm}
\end{wraptable}
\textbf{Impact of Pre-training Datasets.} \tabref{tab:dataset_composition_ablation} illustrates the performance of different datasets. As anticipated, all components of the InfMix dataset are necessary, including the infrared dataset InfPre, the ImageNet subset \citep{imagenet}, the COCO training set  \citep{coco}, and the grayscale operation. Remarkably, InfMix significantly outperforms single-modality datasets like ImageNet and InfPre. This improvement can be attributed to the complementary strengths of both modalities: infrared images help mitigate the distribution shift issue, while RGB images enhance general feature extraction capabilities. The mixed dataset effectively balances these two aspects. Moreover, \tabref{tab:data_ratio} in the appendix displays the scaling characteristics of pre-training data, demonstrating the necessity of constructing the larger InfMix dataset.


\begin{wrapfigure}{r}{0.25\textwidth}
\vspace{-11pt}
\centering
\includegraphics[width=1.0\linewidth]{figures/layer_select_s_v2.pdf}
\vspace{-22pt}
\caption{The average FT when employing different $s$ in \eqnref{eq:distill}.}
\label{fig:para_s}
\vspace{-15pt}
\end{wrapfigure}
\textbf{Impact of the Hyperparameter $s$.} The parameter $s$ in \eqnref{eq:distill} determines the layer chosen for distillation. As presented in \figref{fig:para_s}, when $s$ ranges from 0.06 to 0.12, the selected layer remains nearly constant: the 18th layer for MAE-L and the 9th layer for DINO-B. Therefore, the performance of UNIP is relatively stable with respect to $s$.


\textbf{Comparison with the Feature Distillation.} As compared in \tabref{tab:target_ablation}, the performance of feature distillation consistently lags behind attention distillation across different layers, implying the latter's superiority. We believe this is because attention distillation only restricts the relationship between tokens, whereas feature distillation imposes direct constraints on each token's features. Excessive constraints on features may intensify the distribution shift and hinder the generalization of distilled models. Additionally, the performance of feature distillation across different layers is similar, likely due to the skip connections in ViT, which enhance feature similarities between layers. In contrast, the attention maps of different layers differ significantly, as revealed in \secref{sec:attention_pattern}.


\begin{wraptable}{r}{0.48\textwidth}
    \vspace{-7.5mm}
    \centering
    \caption{Comparisons of pre-training methods.}
    \label{tab:other_methods}
    \scriptsize
    \setlength{\tabcolsep}{1.0mm}{
    \scalebox{1.0}{
    \begin{tabular}{l c c}
        \toprule
        Method & Avg FT & Training Time (h) \\
        \midrule
        Continual Pre-trained (MAE-S) & 58.53 & 75.0 (1x RTX3090) \\
        UNIP-S (MAE-L distilled) & \textbf{64.37} & \textbf{72.5} (1x RTX3090)  \\
        \bottomrule
    \end{tabular}}}
    \vspace{-4mm}
\end{wraptable}
\textbf{Comparison with Contiunal Pre-training on Target Domain.}
We initialize MAE-S with RGB pre-trained weights and further pre-train it on InfMix for 100 epochs. As shown in \tabref{tab:other_methods}, this continually pre-trained MAE-S (58.53\%) exceeds the RGB pre-trained one (55.39\% in \tabref{tab:main_distill}). However, it still underperforms UNIP-S by 5.84\% and requires more training time, highlighting the efficiency of UNIP over continual pre-training.

\textbf{Impact of the LL-FPN.} \tabref{tab:fpn_ablation} shows the performance of models distilled from various teachers. While LL-FPN enhances performance for all models, the improvements are much greater when using \textit{hybrid} patterns as distillation targets than \textit{local} or \textit{global} patterns. This demonstrates LL-FPN's superiority and good compatibility with the \textit{hybrid} pattern, supporting the analysis in \secref{sec:unip}.


\begin{wraptable}{r}{0.6\textwidth}
    \vspace{-8mm}
    \caption{The LLP performance on RGB and depth datasets. Training epochs are 30 for ADE20K and 100 for others.}
    \label{tab:rgb_llp}
    \vspace{1mm}
    \centering
    \scriptsize
    \setlength{\tabcolsep}{0.8mm}{
    \scalebox{0.93}{
    \begin{tabular}{l c c c c c}
        \toprule
        \multirow{3}{*}{\textcolor{gray}{(Modality)} Dataset} & \multicolumn{2}{c}{DINO-S} & \multicolumn{2}{c}{DeiT-S} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & Layer 9 & Layer 12  & Layer 9 & Layer 12 \\
        & \textcolor{gray}{(\textit{Hybrid})} & \textcolor{gray}{(\textit{Global})} & \textcolor{gray}{(\textit{Hybrid})} & \textcolor{gray}{(\textit{Global})} \\
        \midrule
        \textcolor{gray}{(RGB)} ADE20K \citep{ade20k} & \textbf{26.11} & 23.15 & \textbf{24.35} & 22.68  \\
        \textcolor{gray}{(RGB)} MFNet-RGB \citep{mfnet} & \textbf{38.94} & 37.53  & \textbf{30.43} & 29.44 \\
        \midrule
        \textcolor{gray}{(Depth)} NYUDepthv2 \citep{nyuv2} & \textbf{17.25} & 15.29 & \textbf{5.55} & 5.15 \\
        \textcolor{gray}{(Depth)} SUN-RGBD \citep{sunrgbd} & \textbf{13.17} & 11.41 & \textbf{5.61} & 4.94 \\
        \bottomrule
    \end{tabular}}}
    \vspace{-4mm}
\end{wraptable}

\textbf{Applicability to Other Modalities.} We extend the LLP experiments in \secref{sec:lg_matter} to the RGB and depth modalities. As shown in \tabref{tab:rgb_llp}, for both DINO-S and DeiT-S, the LLP performance of middle layers (the \textit{local} pattern) surpasses that of deep layers (the \textit{global} pattern) across all RGB and depth semantic segmentation datasets. This mirrors the phenomenon in the infrared domain discussed in \secref{sec:lg_matter}, underscoring the importance of \textit{hybrid} patterns for semantic segmentation tasks, regardless of dataset size or modality. Therefore, we believe that UNIP can be effectively extended to other modalities.

\textbf{Visualizations.} We present visualizations of distilled models in the appendix. As shown in \figref{fig:distill_query_attn}\textcolor{red}{c}, the deep layers of UNIP-S exhibit \textit{hybrid} patterns, indicating that UNIP effectively transfers these patterns from the teacher to the student. The CKA alignment between DION-S and UNIP-S in shallow and middle layers, shown in \figref{fig:cka}, further demonstrates this from a feature representation perspective. Additionally, compared to MAE-L, attention maps in UNIP-S focus more on shape information than textures, as evident in \figref{fig:texture_query_attn}. The emergence of \textit{hybrid} patterns in deep layers and the reduced bias towards texture both contribute to the excellent performance of UNIP.