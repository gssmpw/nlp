\section{Introduction}
\label{sec:introduction}

Pre-training is essential in computer vision, equipping models with fundamental feature extraction capabilities. Supervised methods \citep{deit, deit3} and self-supervised methods, such as contrastive learning (CL) \citep{mocov3,dino} and masked image modeling (MIM) \citep{mae, crossmae}, have demonstrated great potential in various visual tasks, particularly for small-scale datasets. Infrared images, widely used in road surveillance \citep{birdsai}, autonomous driving \citep{mcnet}, and unmanned aerial vehicle \citep{dronevehicle}, often lack labeled data for tasks like object detection and semantic segmentation \citep{soda}. Therefore, having a strong pre-trained backbone is vital for these data-limited scenarios.


\textbf{However, the transfer performance on infrared segmentation of different pre-training methods remains considerably underexplored}. Previous works \citep{mcnet, tinn} aim to improve performance by designing specific architectures for infrared segmentation tasks, without assessing the impact of various pre-training methods on model performance. Additionally, mainstream pre-training methods \citep{mae, iBOT} usually evaluate performance on large-scale RGB datasets like ImageNet \citep{imagenet} and ADE20K \citep{ade20k}. Given the significant domain differences between RGB and infrared datasets, 
further study is necessary to evaluate the transfer performance of different pre-training methods on infrared visual tasks.
and the validity of phenomena observed in RGB datasets for the infrared domain.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/architecture_v10.pdf}
    \vspace{-20pt}
    \caption{The Chain-of-Thought (CoT) of our work. \textit{Step1} (\secref{sec:benchmark}): We benchmark the infrared segmentation performance of various pre-trained models and derive several insights. \textit{Step2} (\secref{sec:investigation}): We explore the reasons for the varying behaviors of these models by analyzing the pre-trained attention maps. \textit{Step3} (\secref{sec:distill}): Based on these findings, we propose UNIP, a unified framework aimed to enhance the performance of small pre-trained models, focusing on three aspects: the pre-training dataset (InfMix), the pre-training task (NMI-HAD), and the fine-tuning architecture (LL-FPN).}
    \label{fig:architecture}
    \vspace{-4pt}
\end{figure}


To this end, we benchmark six popular supervised and self-supervised (CL and MIM) pre-training methods on three infrared semantic segmentation datasets, across different model sizes and evaluation metrics (see \secref{sec:benchmark}, Step1 in \figref{fig:architecture}). Some valuable phenomena are discovered: (1) The ImageNet accuracy of models does not necessarily correlate with their performance on infrared segmentation tasks; (2) Supervised and CL methods exhibit better generalization than MIM methods, especially for small models like ViT-T and ViT-S;  (3) The performance improvement of larger models is marginal compared to the substantial increase in computational cost, making them unsuitable for infrared-related tasks that require fast processing speeds with limited computing resources.

To understand the distinct performance of these methods, we conduct a thorough analysis of attention maps (see \secref{sec:investigation}, Step2 in \figref{fig:architecture}). Three attention patterns--\textit{local}, \textit{hybrid}, and \textit{global}--are identified in different layers of pre-trained models. As shown in \figref{fig:query_attn}, \textit{local} patterns focus on nearby tokens\footnote{In this work, \textit{token} is used to denote the 16$\times$16 patch in the image.},  while \textit{global} patterns prefer foreground tokens. \textit{Hybrid} patterns attend to both types. The pre-training tasks significantly influence the pattern distributions: \textbf{Supervised and CL models exhibit \textit{all} patterns, whereas MIM models show only \textit{local} and \textit{hybrid} patterns.} \textbf{Importantly, the \textit{hybrid} attention pattern is found to be crucial for semantic segmentation as it can effectively capture both local and global information}. To quantitatively distinguish these patterns, we introduce the normalized mutual information (NMI) between query and key tokens as an indicator, which aligns well with pattern distributions. Additionally, we find that the bias towards texture observed in attention maps can exacerbate distribution shifts and hinder model generalization in infrared tasks.

Based on the above analysis, a UNified Infrared Pre-training framework called \textbf{UNIP} is proposed to enhance the infrared segmentation performance of small models (see \secref{sec:distill}, Step3 in \figref{fig:architecture}). First, we introduce the \textbf{NMI}-guided \textbf{H}ybrid \textbf{A}ttention pattern \textbf{D}istillation (\textbf{NMI-HAD}) as the pre-training target, which uses NMI to select the distillation layer and compresses \textit{hybrid} patterns from teacher models to randomly initialized student models. Second, to bridge the gap between pre-training and infrared data and mitigate distribution shifts, we construct a large mixed dataset called \textbf{InfMix} as the pre-training dataset. It comprises \textbf{859,375} images from \textbf{25} datasets, ensuring no overlap with the segmentation datasets used in our benchmark. Third, to utilize \textit{hybrid} patterns in the last layer of distilled modes, we propose the \textbf{L}ast-\textbf{L}ayer \textbf{F}eature \textbf{P}yramid \textbf{N}etwork (\textbf{LL-FPN}) for fine-tuning to enhance performance further. 
With these enhancements, the average segmentation mIoU of UNIP significantly surpasses their counterparts, as shown in \figref{fig:benchmark} and \tabref{tab:main_distill}. When using MAE-L \citep{mae} as the teacher, UNIP achieves improvements of \textbf{13.57\%} (T), \textbf{8.98\%} (S), and \textbf{4.34\%} (B) in fine-tuning, and at least \textbf{12.79\%} in linear probing. With iBOT-L \citep{iBOT} as the teacher, UNIP-S exceeds iBOT-S by \textbf{2.61\%} in fine-tuning, while UNIP-B surpasses iBOT-B by \textbf{3.74\%} in linear probing. Notably, the distilled models even outperform their teacher models across different pre-training methods. UNIP also substantially outperforms other SOTA infrared or RGB segmentation methods, such as TINN \citep{tinn} and Mask2Former \citep{mask2former}, and exhibits effectiveness and application potential in other modalities like RGB and depth images.


Our main contributions consist of (1) A comprehensive benchmark of six pre-training methods on three infrared semantic segmentation datasets, highlighting several key phenomena; (2) A detailed investigation of pre-trained attention patterns, emphasizing the critical importance of the \textit{hybrid} pattern for semantic segmentation; (3) A unified infrared pre-training framework UNIP,  including the NMI-HAD method, the InfMix dataset, and the LL-FPN architecture; (4) Extensive experimental results, demonstrating the effectiveness and efficiency of our method and dataset.