\section{Additional analysis}
\label{app:appendix_analysis}

\subsection{Normalized Mutual Information}
\label{app:NMI}
The Normalized Mutual Information (NMI) is employed in \secref{sec:nmi} to measure the attention patterns. Let $p(q_i)$ denote the marginal probability of the $i$-th query token and $p(k_j)$ denote the marginal probability of the $j$-th key token. Since query tokens are evenly distributed across every spatial coordinate, $p(q_i)$ can be formulated as:
\begin{equation}
    p(q_i)=\frac{1}{N}, \quad i=1,2,...,N.
    \label{eq:marginal_q}
\end{equation}
Assume $A^m\in \mathbb{R}^{N\times N}$ represents the $m$-th head of the attention matrix after the softmax operation without the \textit{class} token, where $N$ is the number of spatial tokens. The attention scores from each query token to all key tokens sum to 1, \ie, $\sum_{j=1}^N A_{i,j}^m=1, i=1,2,...,N. $
Thus, each row of $A$ can be viewed as the conditional probability distribution of key tokens given the query token:
\begin{equation}
    p(k_j\vert q_i)=A_{i,j}^m.
\end{equation}
Then the joint probability of $q_i$ and $k_j$ can be calculated as:
\begin{equation}
    p(q_i,k_j)=p(k_j\vert q_i)p(q_i)=\frac{1}{N}A_{i,j}^m.
    \label{eq:joint_qk}
\end{equation}
The marginal probability of $k_j$ is:
\begin{equation}
    p(k_j)=\sum_{i=1}^Np(q_i,k_j)=\frac{1}{N}\sum_{i=1}^NA_{i,j}^m.
    \label{eq:marginal_k}
\end{equation}
The mutual information of query and key tokens can be formulated as:
\begin{equation}
\begin{aligned}
    I^m(Q;K)&=\sum_{i=1}^N\sum_{j=1}^Np(q_i,k_j)\log{\frac{p(q_i,k_j)}{p(q_i)p(k_j)}} \\
    &=\sum_{i=1}^N\sum_{j=1}^N\frac{1}{N}A_{i,j}^m\log{\frac{NA_{i,j}^m}{\sum_{i=1}^NA_{i,j}^m}}.
\end{aligned}
\end{equation}
The entropy of query and key tokens can be calculated as:
\begin{gather}
    H^m(Q)=-\sum_{i=1}^N p(q_i)\log{p(q_i)}=-\sum_{i=1}^N\frac{1}{N}\log{\frac{1}{N}}, \\
    H^m(K)=-\sum_{i=1}^N p(k_j)\log{p(k_j)}=-\sum_{j=1}^N\left(\frac{1}{N}\sum_{i=1}^NA_{i,j}^m\log{\frac{1}{N}\sum_{i=1}^NA_{i,j}^m}\right).
\end{gather}
Therefore, the NMI of the $m$ head is:
\begin{equation}
    \text{NMI}^m(Q;K)=\frac{I^m(Q;K)}{\sqrt{H^m(Q)H^m(K)}}.
\end{equation}
The final NMI is calculated by averaging on all heads:
\begin{equation}
    \text{NMI}(Q;K)=\frac{1}{M}\sum_{m=1}^M \text{NMI}^m(Q;K).
\end{equation}

The value of NMI ranges from 0 to 1. It reaches the maximum value of 1 when the joint probability of the query and key tokens is the same as their marginal probability:
\begin{equation}
    p(q_i,k_i)=p(q_i)=p(k_i), \quad i=1,2,...,N.
    \label{eq:nmi_1}
\end{equation}
According to \eqnref{eq:marginal_q}, \eqnref{eq:joint_qk}, \eqnref{eq:marginal_k}, and \eqnref{eq:nmi_1}, it can be derived that
\begin{equation}
A^m_{i,j}=
\begin{cases} 
1, & \mbox{if }i=j, \\
0, & \mbox{if }i\neq j,
\end{cases}
\end{equation}
which implies that the attention matrix of each head is an identity matrix. This indicates that each query token focuses only on the key token at the same spatial position, which is a particular case of the \textit{local} attention pattern.

On the other hand, the NMI has a value of 0 when the query and key tokens are independent:
\begin{equation}
    p(q_i,k_j)=p(q_i)p(k_j), \quad i=1,2,...,N, j=1,2,...,N.
    \label{eq:nmi_0}
\end{equation}
According to \eqnref{eq:marginal_q}, \eqnref{eq:joint_qk}, \eqnref{eq:marginal_k}, and \eqnref{eq:nmi_0}, we can derive that
\begin{equation}
    A^m_{i,j}=A^m_{k,j}, \quad i=1,2,...,N, k=1,2,...,N, j=1,2,...N,
\end{equation}
which indicates that every row of the attention matrix is the same. This means that each query token has the same attention maps for all key tokens, which is a particular case of the \textit{global} attention pattern.
\textbf{Therefore, a higher NMI value indicates a stronger relationship between the query and key tokens and a more local attention pattern. Conversely, a lower NMI value means that different query tokens have more similar and global attention patterns for key tokens.}




\subsection{Centered Kernel Alignment}
\label{app:CKA}
In this section, we extend the analysis in \secref{sec:attention_pattern} from the attention pattern to the feature representation. Let $x^l$ denote the input features of the $l$-th block of the ViT model. The features of the next block $x^{l+1}$ can be formulated as:
\begin{align}
    &x_{tmp} = x^l + \text{Attention}(\text{LN}(x^l), \\
    &x^{l+1} = x_{tmp} + \text{FFN}(\text{LN}(x_{tmp}),
\end{align}
where $\text{Attention},\text{FFN},\text{LN}$ refer to the self-attention module, the feedforward module, and the LayerNorm layer, respectively. Obviously, the self-attention module plays a crucial role in transforming the feature representation. The \textit{global} attention pattern will bring the features of different tokens closer since different query tokens interact similarly with all key tokens. In contrast, the \textit{local} attention pattern will make the features of different tokens further apart.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/cka_v3.pdf}
\vspace{-22pt}
\caption{CKA representation analysis of different models. UNIP-S aligns well with DINO-S in the shallow and middle layers, indicating that the \textit{hybrid} patterns are effectively distilled from MAE-L.}
\label{fig:cka}
\vspace{-15pt}
\end{figure}

To investigate the relationships between features of different layers and models, we use the centered kernel alignment (CKA), a metric that measures the similarity between two feature maps. The details of CKA can refer to \citet{CKA}. As shown in \figref{fig:cka}, features in the later layers of MAE-S, \eg, the 10th and 11th layers, are similar to features in the shallow layers of DINO-S, \eg, the 4th, 5th, and 6th layer, implying that the features of MAE-S are relatively lower level compared to DINO-S. \textbf{This is consistent with observations in \secref{sec:attention_pattern} that the \textit{local} attention patterns are distributed in the shallow layers of DINO-S, but are present in all layers of MAE.}

For MAE-L, the features in the middle layers (13th to 20th) exhibit high similarity with the middle-layer features of DINO-S (9th and 10th), due to the \textit{hybrid} patterns in these layers. On the contrary, the features in the later layers (the 22nd and 23rd layers) gradually resemble the shallow layers of DINO-S, which can be attributed to the \textit{local} patterns in the later layers of MAE-L.

It is noteworthy that the UNIP model effectively imitates the features in the middle layers of MAE-L. Its features align more closely with DINO-S than those of MAE-S, especially in the shallow and middle layers, demonstrating that attention distillation can implicitly change the features of distilled models like what feature distillation explicitly does.
