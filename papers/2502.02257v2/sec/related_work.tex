\section{Related work}
\label{app:related_work}

\textbf{Visual pre-training} aims to equip models with fundamental feature extraction capabilities using large-scale pre-training data, aiding their fine-tuning on downstream tasks. Supervised pre-training \citep{resnet, vit}, one of the earliest methods, typically involves image classification on labeled datasets like ImageNet \citep{imagenet}. However, its reliance on labeled data limits its scalability, prompting the development of self-supervised pre-training. This approach utilizes various pretext tasks, such as contrastive learning and masked image modeling, to pre-train models, achieving results competitive with supervised counterparts. These methods are detailed in \secref{sec:benchmark_setup}. In the infrared domain, \citet{pad} proposes the patchwise-scale adapter to adapt RGB pre-trained models for infrared tasks, and \citet{infmae} constructs a hierarchical model for infrared pre-training. However, previous works have not thoroughly analyzed the transfer performance of different pre-training methods on infrared tasks. Our work aims to fill this gap.

\textbf{Knowledge distillation (KD)} is a widely used technique to improve the performance of small models by extracting knowledge from well-trained large models. Initially developed for supervised learning \citep{distill}, it has recently gained popularity in self-supervised learning. \citet{DMAE}, \citet{dbot}, and \citet{efficient_sam} focus on feature KD, while \citet{dino}, \citet{iBOT}, and \citet{dinov2} employ self-relational KD. Similar to our work, \citet{close_look} and \citet{tinymim} explore attention KD, but they only conduct empirical explorations on MAE in the RGB domain and do not explore the underlying mechanism of using different layers for distillation. In contrast, our research systematically investigates which attention patterns are most advantageous for distillation in domain transfer settings and proposes the NMI metric to guide the process, demonstrating effectiveness across various pre-training methods.

\textbf{Semantic segmentation} is a widely investigated visual task that aims to classify each pixel into different semantic categories. As one of the fundamental works, FCN \citep{fcn} employs a fully convolutional neural network for pixel-to-pixel classification. The following works \citep{deeplabv3+,pspnet,upernet} enhance FCN by constructing the feature pyramid network and improving the context fusion module. With the advancements of transformer-based architectures in visual tasks, \citet{segformer} proposes the powerful SegFormer, featuring a hierarchical transformer encoder and a lightweight decoder. Mask2Former \citep{mask2former} further unifies semantic segmentation with other segmentation tasks following the framework of DETR \citep{detr}. For infrared semantic segmentation, \citet{mcnet} develops a multi-level correction network (MCNet) to capture the context in infrared images, while TINN \citep{tinn} focuses on preserving the inherent radiation characteristic within the thermal imaging process. However, these methods do not explore the impact of different pre-trained models on segmentation performance. Our study utilizes semantic segmentation as a representative downstream visual task and systematically investigates the influence of various pre-trained models on this task.
