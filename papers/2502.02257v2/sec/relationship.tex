\section{Relationships between motivations and methods}
\label{app:relationship}

Our primary motivation is to enhance the performance of models on infrared semantic segmentation tasks. From the model perspective, factors affecting the performance on specific tasks include not only the design of the model architecture but also the quality of the model's pre-training. Previous works \citep{soda, tinn} have aimed to improve performance by designing specific network architectures for infrared semantic segmentation tasks. However, in the infrared domain, where labeled data is limited, the quality of the pre-trained model is also crucial. \textbf{Therefore, our work explores an alternative approach by emphasizing the optimization of pre-trained models specifically for infrared semantic segmentation tasks to enhance performance.} To facilitate this exploration, our work is organized into three stages: benchmark establishment, cause analysis, and method proposal, as illustrated in \figref{fig:architecture}.

\textbf{Benchmark Establishment (\secref{sec:benchmark}).} We establish a benchmark for the transfer performance of six RGB pre-training methods, encompassing a total of 18 pre-trained models, on three infrared semantic segmentation datasets (\secref{sec:benchmark_setup}). Our findings reveal several key phenomena (\secref{sec:benchmark_multi_layer}), such as the lack of correlation between model performance on ImageNet and infrared segmentation datasets (\tabref{tab:correlation_coeff}), and the superior generalization of supervised and contrastive learning methods over masked image modeling methods in the context of infrared segmentation tasks (\figref{fig:benchmark}). 

\textbf{Cause Analysis (\secref{sec:investigation}).} To analyze the performance discrepancies among various pre-training methods in infrared segmentation tasks, we conduct an in-depth analysis of the attention maps from the pre-trained models. Our findings indicate that the degree of focus on local and global information (\secref{sec:attention_pattern} - \secref{sec:lg_matter}), as well as on shape and texture information (\secref{sec:texture}), significantly impacts the performance of infrared semantic segmentation tasks. We further validate through corresponding experiments that the existence of hybrid attention patterns (\figref{fig:query_attn} - \figref{fig:layerwise_lp}) and the reduced bias towards texture (\tabref{tab:rgb_infrared}) both play crucial roles in enhancing the performance of pre-trained models in infrared segmentation tasks.

\textbf{Method Proposal (\secref{sec:unip}).} Based on the observations and analyses from the previous two sections, we propose UNIP, a framework designed to improve the infrared segmentation performance of pre-trained models through three key aspects: the pre-training objective (NMI-HAD), the pre-training data (InfMix), and the fine-tuning architecture (LL-FPN). Both NMI-HAD and LL-FPN enhance performance by effectively leveraging hybrid attention patterns, while InfMix enhances performance by reducing the pre-trained model's bias toward texture information. \textbf{Importantly, our approach does not alter the structure of the backbone model or the decoder; instead, we focus on targeted pre-training specifically designed for infrared segmentation tasks. We believe this is one way in which the proposed method is specific to infrared semantic segmentation tasks.} As a result, our pre-trained models significantly outperform RGB pre-trained models of comparable or even larger sizes in infrared semantic segmentation tasks (\tabref{tab:main_distill}), and also achieve superior performance compared to other models specifically designed for infrared segmentation (\tabref{tab:sota_comparison}).